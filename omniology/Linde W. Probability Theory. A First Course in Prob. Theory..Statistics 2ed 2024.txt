Werner Linde
Probability Theory

Also of Interest
Brownian Motion. A Guide to Random Processes and Stochastic Calculus
Ren√© L. Schilling, 2021
ISBN 978-3-11-074125-4, e-ISBN (PDF) 978-3-11-074127-8,
e-ISBN (EPUB) 978-3-11-074149-0
Probability Theory and Statistics with Real World Applications. Univariate and
Multivariate Models Applications
Peter Z√∂rnig, 2024
ISBN 978-3-11-133220-8, e-ISBN (PDF) 978-3-11-133227-7,
e-ISBN (EPUB) 978-3-11-133232-1
Bitcoin: A Game-Theoretic Analysis
Micah Warren, 2023
ISBN 978-3-11-077283-8, e-ISBN (PDF) 978-3-11-077284-5,
e-ISBN (EPUB) 978-3-11-077305-7
Loss Data Analysis. The Maximum Entropy Approach
Henryk Gzyl, Silvia Mayoral, Erika Gomes-Gon√ßalves, 2023
ISBN 978-3-11-104738-6, e-ISBN (PDF) 978-3-11-104818-5,
e-ISBN (EPUB) 978-3-11-104970-0
Fractional Deterministic and Stochastic Calculus
Giacomo Ascione, Yuliya Mishura, Enrica Pirozzi, 2023
ISBN 978-3-11-077981-3, e-ISBN (PDF) 978-3-11-078001-7,
e-ISBN (EPUB) 978-3-11-078022-2

Werner Linde
Probability Theory
‡±©
A First Course in Probability Theory and Statistics
2nd edition

Mathematics Subject Classification 2020
Primary: 60-01, 62-01; Secondary: 60A05
Author
Prof. Dr. Werner Linde
Friedrich-Schiller-Universit√§t Jena
Fakult√§t f√ºr Mathematik & Informatik
Institut f√ºr Stochastik
D-07737 Jena
Germany
Werner.Linde@mathematik.uni-jena.de
ISBN 978-3-11-132484-5
e-ISBN (PDF) 978-3-11-132506-4
e-ISBN (EPUB) 978-3-11-132517-0
Library of Congress Control Number: 2024931814
Bibliographic information published by the Deutsche Nationalbibliothek
The Deutsche Nationalbibliothek lists this publication in the Deutsche Nationalbibliografie;
detailed bibliographic data are available on the Internet at http://dnb.dnb.de.
¬© 2024 Walter de Gruyter GmbH, Berlin/Boston
Cover image: Density Diagram created by Werner Linde
Typesetting: VTeX UAB, Lithuania
Printing and binding: CPI books GmbH, Leck
www.degruyter.com

‡±©
To my wife Karin


Preface
This book is intended as an introductory course for students in mathematics, physical
sciences, engineering, or in other related fields. It is based on the experience of proba-
bility lectures taught during the past 25 years, where the spectrum reached from two-
hour introductory courses, over Measure Theory and advanced probability classes, to
such topics as Stochastic Processes and Mathematical Statistics. Until 2012 these lectures
were delivered to students at the University of Jena (Germany), and since 2013 to those
at the University of Delaware in Newark (USA).
The book is the completely revised version of the German edition ‚ÄúStochastik f√ºr das
Lehramt,‚Äù which appeared in 2014 at De Gruyter. At most universities in Germany, there
exist special classes in Probability Theory for students who want to become teachers of
mathematics in high schools. Besides basic facts about Probability Theory, these courses
are also supposed to give an introduction into Mathematical Statistics. Thus, the original
main intention for the German version was to write a book that helps those students
understand Probability Theory better. But soon the book turned out to also be useful as
introduction for students in other fields, e. g., in mathematics, physics, and so on. Thus
we decided, in order to make the book applicable for a broader audience, to provide a
translation in the English language.
During numerous years of teaching, I learned the following:
‚Äì
Probabilistic questions are usually easy to formulate, generally have a tight relation
to everyday problems, and therefore attract the interest of the audience. Every stu-
dent knows the phenomena that occur when one rolls a die, plays cards, tosses a
coin, or plays a lottery. Thus, an initial interest in Probability Theory exists.
‚Äì
In contrast, after a short time many students have very serious difficulties with un-
derstanding the presented topics. Consequently, a common opinion among students
is that Probability Theory is a very complicated topic, causing a lot of problems and
troubles.
Surely there exist several reasons for the bad image of Probability Theory among stu-
dents. But, as we believe, the most important one is as follows. In Probability Theory, the
type of problems and questions considered, as well as the way of thinking, differs con-
siderably from the problems, questions, and thinking in other fields of mathematics, i. e.,
from fields with which the students became acquainted before attending a probability
course. For example, in Calculus a function has a well-described domain of definition;
mostly it is defined by a concrete formula, has certain properties as continuity, differen-
tiability, and so on. A function is something very concrete which can be made vivid by
drawing its graph. In contrast, in Probability Theory functions are mostly investigated
as random variables. They are defined on a completely unimportant, nonspecified sam-
ple space, and they generally do not possess a concrete formula for their definition. It
may even happen that only the existence of a function (random variable) is known. The
only property of a random variable which really matters is the distribution of its values.
https://doi.org/10.1515/9783111325064-201

VIII
‡±™
Preface
This and many other similar techniques make the whole theory something mysterious
and not completely comprehensible.
Considering this observation, we organized the book in a way that tries to make
probabilistic problems more understandable and that puts the focus more onto expla-
nations of the definitions, notations, and results. The tools we use to do this are exam-
ples; we present at least one before a new definition, in order to motivate it, followed
by more examples after the definition to make it comprehensible. Here we act upon the
maxim expressed by Einstein‚Äôs quote:1
Example isn‚Äôt another way to teach, it is the only way to teach.
Presenting the basic results and methods in Probability Theory without using results,
facts, and notations from Measure Theory is, in our opinion, as difficult as to square the
circle. Either one restricts oneself to discrete probability measures and random vari-
ables or one has to be imprecise. There is no other choice! In some places, it is possible
to avoid the use of measure-theoretic facts, such as the Lebesgue integral, or the exis-
tence of infinite product measures, and so on, but the price is high.2 Of course, I also
struggled with the problem of missing facts from Measure Theory while writing this
book. Therefore, I tried to include some ideas and results about œÉ-fields, measures, and
integrals, hoping that a few readers become interested and want to learn more about
Measure Theory. For those, we refer to the books [Coh13, Dud02], or [Bil12] as good
sources.
In this context, let us make some remark about the verification of the presented re-
sults. Whenever it was possible, we tried to prove the stated results. Times have changed;
when I was a student, every theorem presented in a mathematical lecture was proved ‚Äì
really every one. Facts and results without proof were doubtful and soon forgotten. And
a tricky and elegant proof is sometimes more impressive than the proven result (at least
to us). Hopefully, some readers will like some of the proofs in this book as much as we
did.
One of most used applications of Probability Theory is Mathematical Statistics.
When I met former students of mine, I often asked them which kind of mathematics
they are mainly using now in their daily work. The overwhelming majority of them
answered that one of their main fields of mathematical work is statistical problems.
Therefore, we decided to include an introductory chapter about Mathematical Statis-
tics. Nowadays, due to the existence of good and fast statistical programs, it is very
easy to analyze data, to evaluate confidence regions, or to test a given hypothesis. But
1 See http://www.alberteinsteinsite.com/quotes/einsteinquotes.html
2 For example, several years ago, to avoid the use of the Lebesgue integral, I introduced the expected
value of a random variable as a Riemann integral via its distribution function. This is mathematically
correct, but at the end almost no students understood what the expected value really is. Try to prove that
the expected value is linear using this approach!

Preface
‡±™
IX
do those who use these programs also always know what they are doing? Since we
doubt that this is so, we stressed the focus in this chapter to the question of why the
main statistical methods work and on what mathematical background they rest. We
also investigate how precise statistical decisions are and what kinds of errors may
occur.
The organization of this book differs a little bit from those in many other first-course
books about Probability Theory. Having Measure Theory in the back of our minds causes
us to think that probability measures are the most important ingredient of Probability
Theory; random variables come in second. On the contrary, many other authors go ex-
actly the other way. They start with random variables, and probability measures then
occur as their distribution on their range spaces (mostly ‚Ñù). In this case, a standard
normal probability measure does not exist, only a standard normal distributed random
variable. Both approaches have their advantages and disadvantages, but as we said, for
us the probability measures are interesting in their own right, and therefore we start
with them in Chapter 1, followed by random variables in Section 3.
The book also contains some facts and results that are more advanced and usu-
ally not part of an introductory course in Probability Theory. Such topics are, for exam-
ple, the investigation of product measures, order statistics, and so on. We have assigned
those more involved sections with a star. They may be skipped at a first reading without
loss in the following chapters.
At the end of each chapter, one finds a collection of some problems related to the
contents of the section. Here we restricted ourselves to a few problems in the actual task;
the solutions of these problems are helpful to the understanding of the presented topics.
The problems are mainly taken from our collection of homeworks and exams during the
past years. For those who want to work with more problems we refer to many books,
e. g., [GS01, Gha19, Pao06], or [Rss14], which contain a huge collection of probabilistic
problems, ranging from easy to difficult, from natural to artificial, from interesting to
boring.
Finally, I want to express my thanks to those who supported my work at the trans-
lation and revision of the present book. Many students at the University of Delaware
helped me improve my English and correct wrong phrases and expressions. To men-
tion all of them is impossible. But among them were a few students who read whole
chapters and, without them, the book would have never been finished (or readable).
In particular, I want to mention Emily Wagner and Spencer Walker. They both really
did a great job. Many thanks! Let me also express my gratitude to Colleen McInerney,
Rachel Austin, Daniel Atadan, and Quentin Dubroff, all students in Delaware and at-
tending my classes for some time. They also read whole sections of the book and cor-
rected my broken English. Finally, my thanks go to Professor Anne Leucht from the
Technical University in Braunschweig (Germany); her field of work is Mathematical
Statistics, and her hints and remarks about Chapter 8 in this book were important to
me.

X
‡±™
Preface
And last but not least, I want to thank the Department of Mathematical Sciences at
the University of Delaware for the excellent working conditions after my retirement in
Germany.
Newark, Delaware, June 6, 2016
Werner Linde
Changes in the second edition: The first edition of my textbook was well received by
scholars and students alike, and I would like to thank all of them for their comments
and positive criticisms.
There are a few changes to the second edition. For instance, I added more than 40
new examples, so that now the book contains about 280 of them. Among the new ones
are some classical examples as, e. g., the ‚ÄúBoy or Girl Paradox,‚Äù the ‚ÄúSecretary Problem,‚Äù
the ‚ÄúTwo-Envelope Paradox,‚Äù or ‚ÄúGambler‚Äôs Ruin,‚Äù which may be found in Sections 5.4
or 5.5, respectively. Other examples have been included for better understanding of the
general, sometimes quite abstract, topics.
Section 8 about Mathematical Statistics contains now three tables which summarize
the main tests and confidence regions for normal distributed populations. I hope that
these r√©sum√©s help to get a quick overview about the most used techniques in Math-
ematical Statistics. Furthermore, there is a new Section 8.6.4 about confidence regions
for hypergeometric distributed samples, an important topic missing in the first edition.
A completely new ingredient in this edition are short summaries at the end of almost
every section. Here I give a compressed overview about basic notions and results pre-
sented in the preceding section. The aim of these abstracts is to tell the reader what were
the most important statements and what can possibly be omitted from the first reading.
I believe that graphical presentations of abstract mathematical statements are a
very helpful aid for better understanding, not only for beginners. Therefore, I added
more than 80 new figures, so that now the book contains more than 100 of them. The
increase is mainly due to the fact that there exist now powerful tools as, e. g., TikZ for
drawing convincing figures, tools which either did not yet exist or which I was not aware
of when writing the first edition.
Besides I added several new problems, updated the list of references, and completed
it by adding a few classical books about Measure Theory and Probability as, for example,
[Hal14, Kal21] or [Par05].
Some minor misprints or incorrect arguments have been eliminated, a few parts
were rewritten in order to make them, as I hope, clearer and better understandable.
Finally, I would like to thank my former student Frank Aurzada, TU Darmstadt, for
the fruitful discussions about some newly added examples. Last but not least, I want to
express my gratitude to Nadja Schedensack from De Gruyter for her advice and helpful
remarks concerning layout and TEX problems.
Jena, Germany, January 8, 2024
Werner Linde

Contents
Preface ‡±©VII
1
Probabilities ‡±©1
1.1
Probability spaces ‡±©1
1.1.1
Sample spaces ‡±©1
1.1.2
œÉ-fields of events‚àó‡±©3
1.1.3
Probability measures ‡±©6
1.2
Basic properties of probability measures ‡±©10
1.3
Discrete probability measures ‡±©15
1.4
Special discrete probability measures ‡±©20
1.4.1
Dirac measure ‡±©20
1.4.2
Uniform distribution on a finite set ‡±©21
1.4.3
Binomial distribution ‡±©27
1.4.4
Multinomial distribution ‡±©30
1.4.5
Poisson distribution ‡±©34
1.4.6
Hypergeometric distribution ‡±©36
1.4.7
Geometric distribution ‡±©41
1.4.8
Negative binomial distribution ‡±©43
1.5
Continuous probability measures ‡±©48
1.6
Special continuous distributions ‡±©52
1.6.1
Uniform distribution on an interval ‡±©52
1.6.2
Normal distribution ‡±©55
1.6.3
Gamma distribution ‡±©58
1.6.4
Exponential distribution ‡±©61
1.6.5
Erlang distribution ‡±©63
1.6.6
Chi-squared distribution ‡±©64
1.6.7
Beta distribution ‡±©65
1.6.8
Cauchy distribution ‡±©68
1.7
Distribution function ‡±©69
1.8
Multivariate continuous distributions ‡±©81
1.8.1
Multivariate density functions ‡±©81
1.8.2
Multivariate uniform distribution ‡±©83
1.9
Products of probability spaces‚àó‡±©90
1.9.1
Product œÉ-fields and measures ‡±©90
1.9.2
Product measures: discrete case ‡±©95
1.9.3
Product measures: continuous case ‡±©99
1.10
Problems ‡±©103
2
Conditional probabilities and independence ‡±©111
2.1
Conditional probabilities ‡±©111

XII
‡±™
Contents
2.2
Independence of events ‡±©120
2.3
Problems ‡±©127
3
Random variables and their distribution ‡±©131
3.1
Transformation of random values ‡±©131
3.2
Probability distribution of a random variable ‡±©133
3.3
Special random variables ‡±©143
3.4
Random vectors ‡±©145
3.5
Joint and marginal distributions ‡±©147
3.5.1
Marginal distributions: discrete case ‡±©149
3.5.2
Marginal distributions: continuous case ‡±©154
3.6
Independence of random variables ‡±©157
3.6.1
Independence of discrete random variables ‡±©161
3.6.2
Independence of continuous random variables ‡±©165
3.7
Order statistics‚àó‡±©169
3.8
Problems ‡±©176
4
Operations on random variables ‡±©180
4.1
Mappings of random variables ‡±©180
4.2
Linear transformations ‡±©185
4.3
Coin tossing versus uniform distribution ‡±©189
4.3.1
Binary fractions ‡±©189
4.3.2
Binary fractions of random numbers ‡±©191
4.3.3
Random numbers generated by coin tossing ‡±©193
4.4
Simulation of random variables ‡±©196
4.5
Addition of random variables ‡±©202
4.5.1
Sums of discrete random variables ‡±©204
4.5.2
Sums of continuous random variables ‡±©207
4.6
Sums of certain random variables ‡±©210
4.7
Products and quotients of random variables ‡±©222
4.7.1
Student‚Äôs t-distribution ‡±©226
4.7.2
F-distribution ‡±©228
4.8
Problems ‡±©231
5
Expected value, variance, and covariance ‡±©235
5.1
Expected value ‡±©235
5.1.1
Expected value of discrete random variables ‡±©235
5.1.2
Expected value of certain discrete random variables ‡±©238
5.1.3
Expected value of continuous random variables ‡±©242
5.1.4
Expected value of certain continuous random variables ‡±©245
5.1.5
Properties of the expected value ‡±©249
5.2
Variance ‡±©257

Contents
‡±™
XIII
5.2.1
Higher moments of random variables ‡±©257
5.2.2
Variance of random variables ‡±©261
5.2.3
Variance of certain random variables ‡±©263
5.3
Covariance and correlation ‡±©269
5.3.1
Covariance ‡±©269
5.3.2
Correlation coefficient ‡±©276
5.4
Some paradoxes and examples ‡±©279
5.4.1
Boy or girl paradox ‡±©279
5.4.2
Randomly chosen entries ‡±©282
5.4.3
Secretary problem ‡±©283
5.4.4
Two-envelope paradox ‡±©287
5.5
Gambler‚Äôs ruin ‡±©293
5.6
Problems ‡±©304
6
Normally distributed random vectors ‡±©310
6.1
Representation and density ‡±©310
6.2
Expected value and covariance matrix ‡±©318
6.3
Problems ‡±©324
7
Limit theorems ‡±©327
7.1
Laws of large numbers ‡±©327
7.1.1
Chebyshev‚Äôs inequality ‡±©327
7.1.2
Infinite sequences of independent random variables‚àó‡±©330
7.1.3
Borel‚ÄìCantelli lemma‚àó‡±©333
7.1.4
Weak law of large numbers ‡±©340
7.1.5
Strong law of large numbers ‡±©341
7.2
Central limit theorem ‡±©347
7.3
Problems ‡±©367
8
Mathematical statistics ‡±©370
8.1
Statistical models ‡±©370
8.1.1
Nonparametric statistical models ‡±©370
8.1.2
Parametric statistical models ‡±©374
8.2
Statistical hypothesis testing ‡±©376
8.2.1
Hypotheses and tests ‡±©376
8.2.2
Power function and significance tests ‡±©379
8.3
Tests for binomial distributed populations ‡±©386
8.4
Tests for normally distributed populations ‡±©391
8.4.1
Fisher‚Äôs theorem ‡±©392
8.4.2
Quantiles ‡±©395
8.4.3
Z-tests or Gauss tests ‡±©400
8.4.4
t-tests ‡±©404

XIV
‡±™
Contents
8.4.5
œá2-tests for the variance ‡±©406
8.4.6
Two-sample Z-tests ‡±©408
8.4.7
Two-sample t-tests ‡±©410
8.4.8
F-tests ‡±©412
8.5
Point estimators ‡±©414
8.5.1
Maximum likelihood estimation ‡±©416
8.5.2
Unbiased estimators ‡±©423
8.5.3
Risk function ‡±©427
8.6
Confidence regions and intervals ‡±©432
8.6.1
Construction of confidence regions ‡±©432
8.6.2
Normally distributed samples ‡±©435
8.6.3
Binomial distributed populations ‡±©438
8.6.4
Hypergeometric distributed populations ‡±©444
8.7
Problems ‡±©447
A
Appendix ‡±©451
A.1
Notations ‡±©451
A.2
Elements of set theory ‡±©451
A.2.1
Set operations ‡±©451
A.2.2
Preimages of sets ‡±©454
A.2.3
Problems ‡±©455
A.3
Combinatorics ‡±©456
A.3.1
Binomial coefficients ‡±©456
A.3.2
Drawing balls out of an urn ‡±©461
A.3.3
Multinomial coefficients ‡±©465
A.3.4
Problems ‡±©467
A.4
Vectors and matrices ‡±©468
A.5
Some analytic tools ‡±©472
Bibliography ‡±©479
Index ‡±©481

1 Probabilities
1.1 Probability spaces
The basic concern of Probability Theory is to model experiments involving randomness,
that is, experiments with nondetermined outcome, shortly called random experiments.
The Russian mathematician A. N. Kolmogorov established the modern Probability The-
ory in 1933 by publishing his book (cf. [Kol33]) Grundbegriffe der Wahrscheinlichkeit-
srechnung. In it, he postulated the following:
Random experiments are described by probability spaces (Œ©, ùíú, ‚Ñô).
The triple (Œ©, ùíú, ‚Ñô) comprises a sample space Œ©, a œÉ-field ùíúof events, and a mapping ‚Ñô
from ùíúto [0, 1], called probability measure or probability distribution.
Let us now explain the three different components of a probability space in detail.
We start with the sample space.
1.1.1 Sample spaces
Definition 1.1.1. The sample space Œ© is a nonempty set that contains (at least) all possible outcomes of
the random experiment.
Remark 1.1.2. Due to mathematical reasons, sometimes it can be useful to choose Œ©
larger than necessary. It is only important that the sample space contains all possible
results.
Example 1.1.3. When rolling a die one time, the natural choice for the sample space is
Œ© = {1, . . . , 6}. However, it would also be possible to take Œ© = {1, 2, . . . } or even Œ© = ‚Ñù. In
contrast, Œ© = {1, . . . , 5} is not suitable for the description of the experiment.
Example 1.1.4. Roll a die until the number ‚Äú6‚Äù shows up for the first time. Record the
number of necessary rolls until the first appearance of ‚Äú6‚Äù. The suitable sample space in
this case is Œ© = {1, 2, . . .}. Any finite set {1, 2, . . . , N} is not appropriate because, even if we
choose N very large, we can never be 100 % sure that the first ‚Äú6‚Äù really appears during
the first N rolls.
Example 1.1.5. Suppose in a lottery 6 numbers out of 49 are chosen. If we record the 6
numbers in the way they are chosen, then a suitable sample space is
Œ© = {(œâ1, . . . , œâ6) : 1 ‚â§œâi ‚â§49 , œâi
Ã∏= œâj, i
Ã∏= j} .
https://doi.org/10.1515/9783111325064-001

2
‡±™
1 Probabilities
But, in general, the chosen numbers are published ordered by their size. If this is so, as
corresponding sample space we may choose
Œ© = {(œâ1, . . . , œâ6) : 1 ‚â§œâ1 < ‚ãÖ‚ãÖ‚ãÖ< œâ6 ‚â§49} .
Example 1.1.6. Say we have three urns, each containing white and black balls. Choose
one urn at random and take out a ball. Register if the chosen ball is white or black. Letting
Œ© = {w, b} as sample space is not appropriate. It does not take into account which urn
we had chosen. One suitable sample space would be Œ© = {(w, i), (b, i), i = 1, 2, 3}. Then,
for instance, (w, 1) occurs if we choose urn 1, take out a ball of this urn, and the chosen
ball is a white one.
Example 1.1.7. A light bulb is switched on at time zero and burns for a certain period
of time. At some random time t > 0, it burns out. To describe this experiment, we have
to take into account all possible times t > 0. Therefore, a natural choice for the sample
space in this case is Œ© = (0, ‚àû), or, if we do not exclude that the bulb is defective from
the very beginning, then Œ© = [0, ‚àû).
Example 1.1.8. Customers arrive at the counter of a bank at certain random times. So,
for example, the first customer shows up at time t1, the second at time t2 > t1, and so
on. Then the sample space consists of infinite sequences t1 < t2 < ‚ãÖ‚ãÖ‚ãÖof positive real
numbers assuming hat at least one customer enters the bank that day. But, in fact, be-
cause the number of customers per day is finite, one may also choose
Œ© = {(t1, . . . , tn) : 0 < t1 < ‚ãÖ‚ãÖ‚ãÖ< tn, n ‚àà‚Ñï} ‚à™{0},
where {0} occurs if no customer arrives at that day.
Subsets of the sample space Œ© are called events. In other words, the powerset ùí´(Œ©)
is the collection of all possible events. For example, when we roll a die once there are
exactly 26 = 64 possible events, as, for example,
0, {1}, . . . , {6}, {1, 2}, . . . , {1, 6}, {2, 3}, . . . , {2, 6}, . . . , {1, 2, 3, 4, 5}, Œ© .
Among all events, there are some of special interest, the so-called elementary events.
These are events containing exactly one element. In Example 1.1.3, the elementary events
are
{1}, {2}, {3}, {4}, {5}, and {6} .
Remark 1.1.9. Never confuse the elementary events with the points that they contain.
Look at Example 1.1.3. There we have 6 ‚ààŒ© and for the generated elementary event
holds {6} ‚ààùí´(Œ©).
Let A ‚äÜŒ© be an event. After executing the random experiment, one observes a
result œâ ‚ààŒ©. Then two cases are possible:

1.1 Probability spaces
‡±™
3
1.
The outcome œâ belongs to A. In this case, we say that the event A occurred.
2.
If œâ is not in A, that is, if œâ ‚ààAc, then the event A did not occur.
Example 1.1.10. Roll a die once and let A = {2, 4}. Say the outcome was number ‚Äú6‚Äù.
Then A did not occur. But, if we obtained number ‚Äú2,‚Äù then A occurred.
Example 1.1.11. Suppose we roll a die twice. The describing sample space consists of all
36 pairs of numbers from 1 to 6. If
A = {(1, 1), . . . , (6, 6)} ,
then A occurs if and only if the outcome of the first roll equals the that of the second roll.
Example 1.1.12. In Example 1.1.7, the occurrence of an event A = [T, ‚àû) tells us that the
light bulb burned out after time T or, in other words, at time T it was still shining.
Let us formulate some easy rules for the occurrence of events.
1.
By the choice of the sample space, the event Œ© always occurs. Therefore, Œ© is also
called the certain event.
2.
The empty set never occurs. Thus it is called the impossible event.
3.
An event A occurs if and only if the complementary event Ac does not, and vice
versa, A does not occur if and only if Ac does.
4.
If A and B are two events, then A ‚à™B occurs if at least one of the two events occurs.
Hereby we do not exclude that A and B may both occur.
5.
The event A ‚à©B occurs if and only if A and B both occur.
1.1.2 œÉ-fields of events‚àó
The basic aim of Probability Theory is to assign to each event A a number ‚Ñô(A) in [0, 1],
which describes the likelihood of its occurrence. If the occurrence of an event A is very
likely, then ‚Ñô(A) should be close to 1 while ‚Ñô(A) close to zero suggests that the appear-
ance of A is very unlikely.1 The mapping A Û≥®É‚Üí‚Ñô(A) must possess certain natural prop-
erties. Unfortunately, by mathematical reason it is not always possible to assign to each
event A a number ‚Ñô(A) such that A Û≥®É‚Üí‚Ñô(A) has the desired properties. The solution is
ingenious and one of the key observations in Kolmogorov‚Äôs approach: one chooses a sub-
set ùíú‚äÜùí´(Œ©) such that ‚Ñô(A) is only defined for A ‚ààùíú. If A ‚àâùíú, then ‚Ñô(A) does not exist.
Of course, ùíúshould be chosen as large as possible and, moreover, at least ‚Äúordinary‚Äù
sets should belong to ùíú.
1 If the weather forecast predicts a 70 % chance of rain, you will surely take your umbrella with you. On
the contrary, if the forecast is only 20 %, you will probably not do so. Why? In this case the probability
of the occurrence of the event A, ‚Äúit rains,‚Äù is significantly less likely.

4
‡±™
1 Probabilities
In the case of ‚Äúlarge‚Äù sample spaces, it is in general impossible to assign to each event a meaningful likelihood
of its occurrence. Consequently, for ‚Äúlarge‚Äù sample spaces, as, for example, ‚Ñùor ‚Ñùn, the probability ‚Ñô(A) is
defined only for certain special events A.
The collection ùíúof events for which ‚Ñô(A) is well defined has to satisfy some algebraic
conditions. More precisely, the following properties are supposed.
Definition 1.1.13. A collection ùíúof subsets of Œ© is called a œÉ-field if
(1)
0 ‚ààùíú,
(2)
if A ‚ààùíúthen Ac ‚ààùíú, and
(3)
for countably many A1, A2, . . . in ùíú, it follows that ‚ãÉ‚àû
j=1 Aj ‚ààùíú.
Let us verify some easy properties of œÉ-fields.
Proposition 1.1.14. Let ùíúbe a œÉ-field of subsets of Œ©. Then the following are valid:
(i)
Œ© ‚ààùíú.
(ii) If A1, A2, . . . , An are finitely many sets in ùíú, then ‚ãÉn
j=1 Aj ‚ààùíú.
(iii) If A1, A2, . . . belong to ùíú, then so does ‚ãÇ‚àû
j=1 Aj.
(iv) Whenever A1, . . . , An ‚ààùíú, then ‚ãÇn
j=1 Aj ‚ààùíú.
Proof. Assertion (i) is a direct consequence of 0 ‚ààùíúcombined with property (2) of
œÉ-fields.
To verify (ii), let A1, . . . , An be in ùíú. Set An+1 = An+2 = ‚ãÖ‚ãÖ‚ãÖ= 0. Then for all j = 1, 2, . . . ,
we have Aj ‚ààùíúand, by property (3) of œÉ-fields, also ‚ãÉ‚àû
j=1 Aj ‚ààùíú. But note that we have
‚ãÉ‚àû
j=1 Aj = ‚ãÉn
j=1 Aj, hence (ii) is valid.
To prove (iii), we first observe that Aj ‚ààùíúyields Ac
j ‚ààùíú, hence ‚ãÉ‚àû
j=1 Ac
j ‚ààùíú. Another
application of (2) implies (‚ãÉ‚àû
j=1 Ac
j )c ‚ààùíú. De Morgan‚Äôs rule asserts
(
‚àû
‚ãÉ
j=1
Ac
j )
c
=
‚àû
‚ãÇ
j=1
Aj,
which completes the proof of (iii).
Assertion (iv) may be derived from an application of (ii) to the complementary sets,
as we did in the proof of (iii). Or one can use the method in the proof of (ii), but this time
we choose An+1 = An+2 = ‚ãÖ‚ãÖ‚ãÖ= Œ©.
Corollary 1.1.15. If sets A and B belong to a œÉ-field ùíú, then so do A ‚à™B, A ‚à©B, A \ B, and
AŒîB.
The easiest examples of œÉ-fields are either ùíú= {0, Œ©} or ùíú= ùí´(Œ©). However, the
former œÉ-field is much too small for applications while the latter is generally too big,
at least if the sample space is uncountably infinite. We will shortly indicate how one
constructs suitable œÉ-fields in the case of ‚Äúlarge‚Äù sample spaces as, for example, ‚Ñùor ‚Ñùn.

1.1 Probability spaces
‡±™
5
Proposition 1.1.16. Let ùíûbe an arbitrary nonempty collection of subsets of Œ©. Then there
is a œÉ-field ùíúpossessing the following properties:
1.
It holds that ùíû‚äÜùíúor, verbally, each set C ‚ààùíûbelongs to the œÉ-field ùíú.
2.
The œÉ-field ùíúis the smallest one possessing this property. That is, whenever ùíú‚Ä≤ is
another œÉ-field with ùíû‚äÜùíú‚Ä≤, then ùíú‚äÜùíú‚Ä≤.
Proof. Let Œ¶ be the collection of all œÉ-fields ùíú‚Ä≤ on Œ© for which ùíû‚äÜùíú‚Ä≤, that is,
Œ¶ := {ùíú‚Ä≤ ‚äÜùí´(Œ©) : ùíû‚äÜùíú‚Ä≤ , ùíú‚Ä≤ is a œÉ-field} .
The collection Œ¶ is nonempty because it contains at least one element, namely the pow-
erset of Œ©. Of course, ùí´(Œ©) is a œÉ-field and ùíû‚äÜùí´(Œ©) trivially, hence ùí´(Œ©) ‚ààŒ¶.
Next define ùíúby
ùíú:= ‚ãÇ
ùíú‚Ä≤‚ààŒ¶
ùíú‚Ä≤ = {A ‚äÜŒ© : A ‚ààùíú‚Ä≤ , ‚àÄùíú‚Ä≤ ‚ààŒ¶} .
It is not difficult to prove that ùíúis a œÉ-field with ùíû‚äÜùíú. Indeed, if C ‚ààùíû, then C ‚ààùíú‚Ä≤ for
all ùíú‚Ä≤ ‚ààŒ¶, hence, by construction of ùíú, we get C ‚ààùíú.
Furthermore, ùíúis also the smallest œÉ-field containing ùíû. To see this, take an arbi-
trary œÉ-field
ÃÉ
ùíúcontaining ùíû. Then
ÃÉ
ùíú‚ààŒ¶, which implies ùíú‚äÜ
ÃÉ
ùíúbecause ùíúis the inter-
section over all œÉ-fields in Œ¶. This completes the proof.
Definition 1.1.17. Let ùíûbe an arbitrary nonempty collection of subsets of Œ©. The smallest œÉ-field con-
taining ùíûis called the œÉ-field generated by ùíû. It is denoted by œÉ(ùíû).
Remark 1.1.18. The œÉ-field œÉ(ùíû) is characterized by the three following properties:
1.
œÉ(ùíû) is a œÉ-field.
2.
ùíû‚äÜœÉ(ùíû).
3.
If ùíû‚äÜùíú‚Ä≤ for some œÉ-field ùíú‚Ä≤, then œÉ(ùíû) ‚äÜùíú‚Ä≤.
Example 1.1.19. Let Œ© be an arbitrary nonempty set. If ùíû= {C1, . . . , Cn} where
Ci ‚à©Cj = 0
if i
Ã∏= j
and
n
‚ãÉ
j=1
Cj = Œ© ,
then œÉ(ùíû) consists of the empty set and any finite unions of the Cjs.
For example, if Œ© = {1, . . . , 6} and C1 = {1, 2}, C2 = {3}, and C3 = {4, 5, 6}, then the
elements of the generated œÉ-field are the empty set 0 and C1, C2, C3, C1 ‚à™C2, C1 ‚à™C3,
C2 ‚à™C3, and Œ©.

6
‡±™
1 Probabilities
Definition 1.1.20. Let ùíû‚äÜùí´(‚Ñù) be the collection of all finite closed intervals in ‚Ñù, that is,
ùíû= {[a, b] : a < b , a, b ‚àà‚Ñù} .
The œÉ-field generated by ùíûis denoted by ‚Ñ¨(‚Ñù) and is called the Borel œÉ-field. If B ‚àà‚Ñ¨(‚Ñù), then it is said
to be a Borel set.
Remark 1.1.21. By construction, every closed interval in ‚Ñùis a Borel set. Furthermore,
the properties of œÉ-fields also imply that complements of such intervals, their countable
unions, and intersections are Borel sets. One might believe that all subsets of ‚Ñùare Borel
sets. This is not the case; for the construction of a non-Borel set, we refer to [Gha19,
Example 1.21] or [Dud02, pages 105‚Äì108].
Remark 1.1.22. There exist many other systems of subsets in ‚Ñùgenerating ‚Ñ¨(‚Ñù). Let us
only mention two of them:
ùíû1 = {(‚àí‚àû, b] : b ‚àà‚Ñù}
or
ùíû2 = {(a, ‚àû) : a ‚àà‚Ñù} .
Summary: There is a œÉ-field of subsets in ‚Ñù(also in ‚Ñùn, as we will see later on), the collection of Borel sets,
for which we may always define their probability of occurrence. All sets of interest are Borel sets. Thus, in
fact, knowing probabilities only for those sets is necessary from a mathematical point of view, but for our
purposes this is only a theoretical restriction.
1.1.3 Probability measures
The occurrence of an event in a random experiment is not completely haphazard. Al-
though we are not able to predict the outcome of the next trial, the occurrence or nonoc-
currence of an event follows certain rules. Some events are more likely to occur, others
less. The degree of likelihood of an event A is described by a number ‚Ñô(A), called the
probability of the occurrence of A (in short, probability of A). The most common scale
for probabilities is 0 ‚â§‚Ñô(A) ‚â§1, where the larger ‚Ñô(A), the more likely A is to occur.
One could also think of other scales as 0 ‚â§‚Ñô(A) ‚â§100. In fact, this is even quite often
used; in this sense, a chance of 50 % equals a probability of 1/2.
What does it mean that an event A has probability ‚Ñô(A)? For example, what does it
tell us that an event occurs with probability 1/2? Does this mean a half-occurrence of A?
Surely not.
To answer this question, we have to assume that we execute an experiment not
only once2 but several, say n, times. Thereby we have to ensure that the conditions of
2 It does not make sense to speak of the probability of an event that can be executed only once. For
example, it is (mathematically) absurd to ask for the probability that the Eiffel Tower will be in Paris for
yet another 100 years.

1.1 Probability spaces
‡±™
7
the experiment do not change and that the single results do not depend on each other.
Let
an(A) := Number of trials where A occurs.
The quantity an(A) is called the absolute frequency of the occurrence of A in n trials.
Observe that an(A) is a random number with 0 ‚â§an(A) ‚â§n. Next we set
rn(A) := an(A)
n
(1.1)
and name it the relative frequency of the occurrence of A in n trials. This number is
random as well, but now 0 ‚â§rn(A) ‚â§1.
Example 1.1.23. At www.westlotto.de/lotto-6aus49 one finds a summary of the 6223
drawings in the German lottery between 10/09/1955 and 06/21/2023. Every time there
were 6 numbers chosen out of 49. To describe the experiment, we take as sample space
Œ© = {A ‚äÇ{1, . . . , 49} : |A| = 6} .
Then we get |Œ©| = (49
6 ). Given 1 ‚â§j ‚â§49, define the event ùíÆj as
ùíÆj = {A ‚ààŒ© : j ‚ààA}.
That is, ùíÆj occurs provided that the number j was among the chosen ones. Since
|ùíÆj| = (48
5 ), we obtain
‚Ñô(ùíÆj) =
(48
5 )
(49
6 ) = 6
49 ‚âà0.1224 .
In the above cited summary, one finds the absolute frequency of all events ùíÆj with
j = 1, . . . , 49. For example, the frequencies ùíÆ6, ùíÆ20, and ùíÆ45 equal 825, 719, and 699, re-
spectively. That is, during the past n = 6223 drawings, the numbers 6, 20, and 45 appeared
825, 719, and 699 times, respectively. Thus, their relative frequencies are
rn(ùíÆ6) = 825
6223 ‚âà0.1326 ,
rn(ùíÆ20) = 719
6223 ‚âà0.1155,
and
rn(ùíÆ45) = 699
6223 ‚âà0.1123 .
One should compare these relative frequencies with the expected one (assumed that all
numbers are equally likely), given by
‚Ñô(ùíÆj) = 6
49 = 0.1224 ,
j = 1, . . . , 49 .

8
‡±™
1 Probabilities
It is somehow intuitively clear3 that the relative frequencies of an event A converge
to a (nonrandom) number as n ‚Üí‚àû. And this limit is exactly the desired probability
of the occurrence of the event A. Let us express this in a different way: say we execute
an experiment n times for some large n. Then, on average, we will observe n ‚ãÖ‚Ñô(A)
occurrences of A. For example, when rolling a fair die many times, an even number will
happen in approximately half the cases.
Or, in the setting of Example 1.1.23, on average each number from 1 to 49 should
approximately show up 6223‚ãÖ6
49 = 762 times. Thus, the observed frequencies tell us that
either the drawings were not fair (some numbers are more likely than others) or that
the number n = 6223 of drawings is still too small.
Which natural properties of A Û≥®É‚Üí‚Ñô(A) may be deduced from rn(A) ‚Üí
n‚Üí‚àû‚Ñô(A)?
1.
Since 0 ‚â§rn(A) ‚â§1, we conclude 0 ‚â§‚Ñô(A) ‚â§1.
2.
Because of rn(Œ©) = 1 for each n ‚â•1, we get ‚Ñô(Œ©) = 1.
3.
The property rn(0) = 0 yields ‚Ñô(0) = 0.
4.
Let A and B be two disjoint events. Then rn(A ‚à™B) = rn(A) + rn(B), hence the limits
should satisfy a similar relation, that is,
‚Ñô(A ‚à™B) = ‚Ñô(A) + ‚Ñô(B) .
(1.2)
Definition 1.1.24. A mapping ‚Ñôfulfilling eq. (1.2) for disjoint A and B is called finitely additive.
Remark 1.1.25. Applying eq. (1.2) successively leads to the following. If A1, . . . , An are
disjoint, then
‚Ñô(
n
‚ãÉ
j=1
Aj) =
n
‚àë
j=1
‚Ñô(Aj) .
Finite additivity is a very useful property of probabilities, and in the case of finite
sample spaces, it completely suffices to build a fruitful theory. But as soon as the sample
space is infinite, it is too weak. To see this, let us come back to Example 1.1.4. Assume we
want to evaluate the probability of the event A = {2, 4, 6, . . .}, that is, the first ‚Äú6‚Äù appears
in an even number of trials. Then we have to split A into (infinitely) many disjoint events
{2}, {4}, . . . The finite additivity of ‚Ñôdoes not suffice to get ‚Ñô(A) = ‚Ñô({2}) + ‚Ñô({4}) + ‚ãÖ‚ãÖ‚ãÖ. In
order to evaluate ‚Ñô(A) in this way, we need the following stronger property of ‚Ñô.
Definition 1.1.26. A mapping ‚Ñôis said to be œÉ-additive provided that for countably many disjoint
A1, A2, . . . in Œ© we get
‚Ñô(
‚àû
‚ãÉ
j=1
Aj) =
‚àû
‚àë
j=1
‚Ñô(Aj).
3 We will discuss this question more precisely in Section 7.1.

1.1 Probability spaces
‡±™
9
Let us summarize what we have until now: a mapping ‚Ñôassigning each event its prob-
ability should possess the following natural properties:
1.
For all A, one has 0 ‚â§‚Ñô(A) ‚â§1.
2.
We have ‚Ñô(0) = 0 and ‚Ñô(Œ©) = 1.
3.
The mapping ‚Ñôis œÉ-additive.
Thus, given a sample space Œ©, we look for a function ‚Ñôdefined on ùí´(Œ©) satisfying the
previous properties. But, as already mentioned, if Œ© is uncountable, for example, if Œ© =
‚Ñù, then only very special4 ‚Ñôwith these properties exist.
To overcome these difficulties, in such cases we have to restrict ‚Ñôto a œÉ-field ùíú‚äÜ
ùí´(Œ©).
Definition 1.1.27. LetŒ©beasamplespaceandletùíúbeaœÉ-fieldofsubsetsofŒ©.Afunction‚Ñô: ùíú‚Üí[0, 1]
is called a probability measure or probability distribution on (Œ©, ùíú) if
1.
‚Ñô(0) = 0 and ‚Ñô(Œ©) = 1.
2.
‚Ñôis œÉ-additive, that is, for each sequence of disjoint sets Aj ‚ààùíú, j = 1, 2, . . . , it follows that
‚Ñô(
‚àû
‚ãÉ
j=1
Aj) =
‚àû
‚àë
j=1
‚Ñô(Aj) .
(1.3)
Remark 1.1.28. Note that the left-hand side of eq. (1.3) is well defined. Indeed, since ùíú
is a œÉ-field, Aj ‚ààùíúimplies ‚ãÉ‚àû
j=1 Aj ‚ààùíúas well.
Now we are in a position to define probability spaces in the exact way.
Definition 1.1.29. A probability space is a triple (Œ©, ùíú, ‚Ñô), where Œ© is a sample space, ùíúdenotes a
œÉ-field consisting of subsets of Œ©, and ‚Ñô: ùíú‚Üí[0, 1] is a probability measure.
Remark 1.1.30. Given A ‚ààùíú, the number ‚Ñô(A) describes its probability or, more pre-
cisely, its probability of occurrence. Subsets A of Œ© with A ‚àâùíúdo not possess a proba-
bility.
Let us demonstrate a simple example on how to construct a probability space for a
given random experiment. Several other examples will follow soon.
Example 1.1.31. We ask for a probability space that describes rolling a fair die once. Of
course, Œ© = {1, . . . , 6} and ùíú= ùí´(Œ©). The mapping ‚Ñô: ùí´(Œ©) ‚Üí[0, 1] is given by
‚Ñô(A) = |A|
6 ,
A ‚äÜ{1, . . . , 6} .
Recall that |A| denotes the cardinality of the set A.
4 Discrete ones as we will investigate in Section 1.3.

10
‡±™
1 Probabilities
Remark 1.1.32. Suppose we want to find a model for some concrete random experi-
ment. How do we do this? In most cases, the sample space is immediately determined
by the results we expect. If the question about Œ© is settled, the choice of the œÉ-field de-
pends on the size of the sample space. If Œ© is finite or countably finite, then we may
choose ùíú= ùí´(Œ©). If Œ© = ‚Ñùor even ‚Ñùn, we take the corresponding Borel œÉ-fields. The
challenging task is the determination of the probability measure ‚Ñô. Here the following
approaches are possible:
1.
Theoretical considerations quite often lead to the determination of ‚Ñô. For example,
since the faces of a fair die are all equally likely, this already describes ‚Ñôcompletely.
Similar arguments can be used for certain games or also for lotteries.
2.
If theoretical considerations are neither possible nor available then statistically
investigations may help. This approach is based on the fact that the relative fre-
quencies rn(A) converge to ‚Ñô(A). Thus, one executes n trials of the experiment and
records the relative frequency of the occurrence of A. For example, one may ques-
tion n randomly chosen persons or do n independent measurements of the same
item. Then rn(A) may be used to approximate the value of ‚Ñô(A).
3.
Sometimes also subjective or experience-based approaches can be used to find ap-
proximate probabilities. These may be erroneous, but they might give some hint for
the correct distribution. For example, if a new product is on the market, the distri-
bution of its lifetime is not yet known. At the beginning one uses data of an already
existing similar product. After some time, data about the new product become avail-
able, the probabilities can be determined more accurately.
Summary: A probability space (Œ©, ùíú, ‚Ñô) is a triple where Œ© is a nonempty set, called sample space, ùíúde-
notes a œÉ-field of subsets of Œ©, and, finally, ‚Ñô: ùíú‚Üí[0, 1] is a probability measure (or probability distri-
bution). The probability measure ‚Ñôpossesses the following properties: it is normalized so that ‚Ñô(0) = 0
and ‚Ñô(Œ©) = 1, and it is œÉ-additive. Given an event A ‚ààùíú, the number ‚Ñô(A) describes the likelihood of its
occurrence.
1.2 Basic properties of probability measures
Probability measures obey many useful properties. Let us summarize the most impor-
tant ones in the next proposition.
Proposition 1.2.1. Let (Œ©, ùíú, ‚Ñô) be a probability space. Then the following are valid:
(1) The measure ‚Ñôis also finitely additive.
(2) If A, B ‚ààùíúsatisfy A ‚äÜB, then ‚Ñô(B \ A) = ‚Ñô(B) ‚àí‚Ñô(A).
(3) We have ‚Ñô(Ac) = 1 ‚àí‚Ñô(A) for A ‚ààùíú.
(4) Probability measures are monotone, that is, if A ‚äÜB for some A, B ‚ààùíú, then
‚Ñô(A) ‚â§‚Ñô(B).

1.2 Basic properties of probability measures
‡±™
11
(5) Probability measures are subadditive, that is, for all (not necessarily disjoint)
events5 Aj ‚ààùíú,
‚Ñô(
‚àû
‚ãÉ
j=1
Aj) ‚â§
‚àû
‚àë
j=1
‚Ñô(Aj) .
(1.4)
(6) Probability measures are continuous from below, that is, whenever Aj ‚ààùíúsatisfy
A1 ‚äÜA2 ‚äÜ‚ãÖ‚ãÖ‚ãÖ,
‚Ñô(
‚àû
‚ãÉ
j=1
Aj) = lim
j‚Üí‚àû‚Ñô(Aj) .
(7) In a similar way, each probability measure is continuous from above: if Aj ‚ààùíúsat-
isfy A1 ‚äáA2 ‚äá‚ãÖ‚ãÖ‚ãÖ, then
‚Ñô(
‚àû
‚ãÇ
j=1
Aj) = lim
j‚Üí‚àû‚Ñô(Aj) .
Proof. To prove (1), choose disjoint A1, . . . , An in ùíúand set An+1 = An+2 = ‚ãÖ‚ãÖ‚ãÖ= 0. Then
A1, A2, . . . are infinitely many disjoint events in ùíú, hence the œÉ-additivity of ‚Ñôimplies
‚Ñô(
‚àû
‚ãÉ
j=1
Aj) =
‚àû
‚àë
j=1
‚Ñô(Aj) .
Observe that ‚ãÉ‚àû
j=1 Aj = ‚ãÉn
j=1 Aj and ‚Ñô(Aj) = 0 if j > n, so the previous equation reduces
to
‚Ñô(
n
‚ãÉ
j=1
Aj) =
n
‚àë
j=1
‚Ñô(Aj) ,
and ‚Ñôis finitely additive.
To prove (2), write B = A ‚à™(B \ A) and observe that this is a disjoint decomposition
of B. Hence, by the finite additivity of ‚Ñô, we obtain
‚Ñô(B) = ‚Ñô(A) + ‚Ñô(B \ A) .
Relocating ‚Ñô(A) to the left-hand side proves (2).
An application of (2) to Œ© and A leads to
‚Ñô(Ac) = ‚Ñô(Œ© \ A) = ‚Ñô(Œ©) ‚àí‚Ñô(A) = 1 ‚àí‚Ñô(A) ,
which proves (3).
5 Estimate (1.4) is also known as Boole‚Äôs inequality.

12
‡±™
1 Probabilities
The monotonicity is an easy consequence of (2). Indeed,
‚Ñô(B) ‚àí‚Ñô(A) = ‚Ñô(B \ A) ‚â•0,
implying ‚Ñô(B) ‚â•‚Ñô(A).
To prove inequality (1.4), choose arbitrary A1, A2, . . . in ùíú. Set B1 := A1 and, if j ‚â•2,
then
Bj := Aj \ (A1 ‚à™‚ãÖ‚ãÖ‚ãÖ‚à™Aj‚àí1) .
Then B1, B2, . . . are disjoint subsets in ùíúwith ‚ãÉ‚àû
j=1 Bj = ‚ãÉ‚àû
j=1 Aj. Furthermore, by the
construction, Bj ‚äÜAj, hence ‚Ñô(Bj) ‚â§‚Ñô(Aj). An application of all these properties yields
‚Ñô(
‚àû
‚ãÉ
j=1
Aj) = ‚Ñô(
‚àû
‚ãÉ
j=1
Bj) =
‚àû
‚àë
j=1
‚Ñô(Bj) ‚â§
‚àû
‚àë
j=1
‚Ñô(Aj) .
Thus (5) is proved.
Let us turn now to the continuity from below. Choose A1, A2, . . . in ùíúsatisfying
A1 ‚äÜA2 ‚äÜ‚ãÖ‚ãÖ‚ãÖ. With A0 := 0, set
Bk := Ak \ Ak‚àí1 ,
k = 1, 2, . . .
The Bks are disjoint and, moreover, ‚ãÉ‚àû
k=1 Bk = ‚ãÉ‚àû
j=1 Aj. Furthermore, because of Ak‚àí1 ‚äÜAk,
from (2) we get ‚Ñô(Bk) = ‚Ñô(Ak) ‚àí‚Ñô(Ak‚àí1). When putting this all together, it follows that
‚Ñô(
‚àû
‚ãÉ
j=1
Aj) = ‚Ñô(
‚àû
‚ãÉ
k=1
Bk) =
‚àû
‚àë
k=1
‚Ñô(Bk) = lim
j‚Üí‚àû
j
‚àë
k=1
‚Ñô(Bk)
= lim
j‚Üí‚àû
j
‚àë
k=1
[‚Ñô(Ak) ‚àí‚Ñô(Ak‚àí1)] = lim
j‚Üí‚àû[‚Ñô(Aj) ‚àí‚Ñô(A0)] = lim
j‚Üí‚àû‚Ñô(Aj),
where we used ‚Ñô(A0) = ‚Ñô(0) = 0. This proves the continuity from below.
Thus it remains to prove (7). For this, choose Aj ‚ààùíúwith A1 ‚äáA2 ‚äá‚ãÖ‚ãÖ‚ãÖ. Then the
complementary sets satisfy Ac
1 ‚äÜAc
2 ‚äÜ‚ãÖ‚ãÖ‚ãÖ. The continuity from below lets us conclude
that
‚Ñô(
‚àû
‚ãÉ
j=1
Ac
j ) = lim
j‚Üí‚àû‚Ñô(Ac
j ) = lim
j‚Üí‚àû[1 ‚àí‚Ñô(Aj)] = 1 ‚àílim
j‚Üí‚àû‚Ñô(Aj) .
(1.5)
But
‚Ñô(
‚àû
‚ãÉ
j=1
Ac
j ) = 1 ‚àí‚Ñô((
‚àû
‚ãÉ
j=1
Ac
j )
c
) = 1 ‚àí‚Ñô(
‚àû
‚ãÇ
j=1
Aj) ,

1.2 Basic properties of probability measures
‡±™
13
and plugging this into eq. (1.5) gives
‚Ñô(
‚àû
‚ãÇ
j=1
Aj) = lim
j‚Üí‚àû‚Ñô(Aj),
as asserted.
Remark 1.2.2. Property (2) becomes false without the assumption A ‚äÜB. But because of
B \ A = B \ (A ‚à©B) and A ‚à©B ‚äÜB, we always have
‚Ñô(B \ A) = ‚Ñô(B) ‚àí‚Ñô(A ‚à©B) .
(1.6)
Another useful property of probability measures is as follows.
Proposition 1.2.3. Let (Œ©, ùíú, ‚Ñô) be a probability space. Then for all A1, A2 ‚ààùíú, it follows
that
‚Ñô(A1 ‚à™A2) = ‚Ñô(A1) + ‚Ñô(A2) ‚àí‚Ñô(A1 ‚à©A2) .
(1.7)
Proof. Write the union of the two sets as
A1 ‚à™A2 = A1 ‚à™[A2 \ (A1 ‚à©A2)]
and note that the two sets on the right-hand side are disjoint. Because of A1 ‚à©A2 ‚äÜA2,
property (2) of Proposition 1.2.1 applies and leads to
‚Ñô(A1 ‚à™A2) = ‚Ñô(A1) + ‚Ñô(A2 \ (A1 ‚à©A2)) = ‚Ñô(A1) + [‚Ñô(A2) ‚àí‚Ñô(A1 ‚à©A2)] .
This completes the proof.
Given A1, A2, A3 ‚ààùíú, an application of the previous proposition to A1 and A2 ‚à™A3
implies
‚Ñô(A1 ‚à™A2 ‚à™A3) = ‚Ñô(A1) + ‚Ñô(A2) + ‚Ñô(A3)
‚àí[‚Ñô(A1 ‚à©A2) + ‚Ñô(A1 ‚à©A3) + ‚Ñô(A2 ‚à©A3)]
+ ‚Ñô(A1 ‚à©A2 ‚à©A3) .
Another application of eq. (1.7) to the second and third terms in the right-hand sum
proves the following result (compare Figure 1.1).
Proposition 1.2.4. Let (Œ©, ùíú, ‚Ñô) be a probability space and let A1, A2, and A3 be in ùíú. Then
‚Ñô(A1 ‚à™A2 ‚à™A3) = ‚Ñô(A1) + ‚Ñô(A2) + ‚Ñô(A3)
‚àí[‚Ñô(A1 ‚à©A2) + ‚Ñô(A1 ‚à©A3) + ‚Ñô(A2 ‚à©A3)]
+ ‚Ñô(A1 ‚à©A2 ‚à©A3).

14
‡±™
1 Probabilities
Figure 1.1: The inclusion‚Äìexclusion formula for three sets.
Remark 1.2.5. A generalization of Propositions 1.2.3 and 1.2.4 from 2 or 3 to an arbitrary
number of sets can be found in Problem 1.7. It is the so-called inclusion‚Äìexclusion for-
mula. For example, if n = 4, this formula says, given events A1, A2, A3, A4, that
‚Ñô(
4
‚ãÉ
i=1
Ai) =
4
‚àë
i=1
‚Ñô(Ai) ‚àí
‚àë
1‚â§i<j‚â§4
‚Ñô(Ai ‚à©Aj) +
‚àë
1‚â§i<j<k‚â§4
‚Ñô(Ai ‚à©Aj ‚à©Ak) ‚àí‚Ñô(
4
‚ãÇ
i=1
Ai) .
Let us explain two easy examples of how the properties of probability measures
apply.
Example 1.2.6. Let (Œ©, ùíú, ‚Ñô) be a probability space. Suppose two events A and B in ùíú
satisfy
‚Ñô(A) = 0.5 ,
‚Ñô(B) = 0.4,
and
‚Ñô(A ‚à©B) = 0.2 .
Which probabilities do A ‚à™B, A \ B, Ac ‚à™Bc, and Ac ‚à©B possess?
Answer: An application of Proposition 1.2.4 gives
‚Ñô(A ‚à™B) = ‚Ñô(A) + ‚Ñô(B) ‚àí‚Ñô(A ‚à©B) = 0.4 + 0.5 ‚àí0.2 = 0.7 .
Furthermore, by eq. (1.6), one gets
‚Ñô(A \ B) = ‚Ñô(A) ‚àí‚Ñô(A ‚à©B) = 0.5 ‚àí0.2 = 0.3 .
Finally, by De Morgan‚Äôs rules and another application of eq. (1.6), we get
‚Ñô(Ac ‚à™Bc) = 1 ‚àí‚Ñô(A ‚à©B) = 0.8
and
‚Ñô(Ac ‚à©B) = ‚Ñô(B \ A) = ‚Ñô(B) ‚àí‚Ñô(A ‚à©B) = 0.2 .
In summary, say one has to take two exams A and B. The probability of passing exam
A is 0.5, the probability of passing B equals 0.4, and to pass both it is 0.2. Then with
probability 0.7 one passes at least one of the exams, with 0.3 exam A, but not B, with
probability 0.8 one fails at least once, and, finally, the probability to pass B but not A
is 0.2.

1.3 Discrete probability measures
‡±™
15
Example 1.2.7. Choose at random (all numbers are equally likely) a number from 1
to 1000. How likely is it that the chosen number is neither divisible by 3, nor by 5,
nor by 7?
Answer: Let D be the set of numbers in {1, . . . , 1000} which are neither divisible by
3, nor by 5, nor by 7. Then it follows that
Dc = A ‚à™B ‚à™C
where A consists of multiples of 3, B contains numbers divisible by 5, and C comprises
multiples of 7. Moreover, the numbers in A‚à©B are multiples of 15, A‚à©C contains multiples
of 21, and B ‚à©C consists of multiples of 35. Finally, we note that A ‚à©B ‚à©C includes only
numbers divisible by 105. Easy calculations lead to
‚Ñô(A) = 0.333,
‚Ñô(B) = 0.2,
P(C) = 0.142,
‚Ñô(A ‚à©B) = 0.066,
‚Ñô(A ‚à©C) = 0.047,
‚Ñô(B ‚à©C) = 0.028,
and
‚Ñô(A ‚à©B ‚à©C) = 0.009.
This implies
‚Ñô(Dc) = ‚Ñô(A) + ‚Ñô(B) + ‚Ñô(C)
‚àí[‚Ñô(A ‚à©B) + ‚Ñô(A ‚à©C) + ‚Ñô(B ‚à©C)] + ‚Ñô(A ‚à©B ‚à©C)
= 0.333 + 0.2 + 0.142 ‚àí0.066 ‚àí0.047 ‚àí0.028 + 0.009 = 0.543.
So we finally arrive at
‚Ñô(D) = 1 ‚àí‚Ñô(Dc) = 1 ‚àí0.543 = 0.457 .
1.3 Discrete probability measures
We start with the investigation of finite sample spaces. They describe random experi-
ments where only finitely many different results may occur, as, for example, rolling a
die n times, tossing a coin finitely often, and so on. Suppose the sample space contains
N different elements. Then we may enumerate these elements as follows:
Œ© = {œâ1, . . . , œâN} .
As œÉ-field we choose ùíú= ùí´(Œ©).
Given an arbitrary probability measure ‚Ñô: ùí´(Œ©) ‚Üí‚Ñù, set
pj := ‚Ñô({œâj}) ,
j = 1, . . . , N .
(1.8)
In this way we assign to each probability measure ‚Ñônumbers p1, . . . , pN. Which proper-
ties do they possess? The answer to this question gives the following proposition.

16
‡±™
1 Probabilities
Proposition 1.3.1. If ‚Ñôis a probability measure on ùí´(Œ©), then the numbers pj defined by
eq. (1.8) satisfy
0 ‚â§pj ‚â§1
and
N
‚àë
j=1
pj = 1 .
(1.9)
Proof. The first property is an immediate consequence of having ‚Ñô(A) ‚â•0 for all A ‚äÜŒ©.
The second property of the pjs follows from
1 = ‚Ñô(Œ©) = ‚Ñô(
N
‚ãÉ
j=1
{œâj}) =
N
‚àë
j=1
‚Ñô({œâj}) =
N
‚àë
j=1
pj .
Conclusion. Each probability measure ‚Ñôgenerates a sequence (pj)N
j=1 of real numbers
satisfying the properties (1.9). Moreover, if A ‚äÜŒ©, then we have
‚Ñô(A) =
‚àë
{j:œâj‚ààA}
pj .
(1.10)
In particular, the assignment ‚Ñô‚Üí(pj)N
j=1 is one-to-one.
Property (1.10) is an easy consequence of A = ‚ãÉ{j:œâj‚ààA}{œâj}. Furthermore, it tells us
that ‚Ñôis uniquely determined by the pjs. Note that two probability measures ‚Ñô1 and ‚Ñô2
on (Œ©, ùíú) coincide if ‚Ñô1(A) = ‚Ñô2(A) for all A ‚ààùíú.
Now let us look at the reverse question. Suppose we are given an arbitrary sequence
(pj)N
j=1 of real numbers satisfying the conditions (1.9).
Proposition 1.3.2. Define ‚Ñôon ùí´(Œ©) by
‚Ñô(A) =
‚àë
{j:œâj‚ààA}
pj .
(1.11)
Then ‚Ñôis a probability measure satisfying ‚Ñô({œâj}) = pj for all j ‚â§n.
Proof. The map ‚Ñôhas values in [0, 1] and ‚Ñô(Œ©) = 1 by ‚àëN
j=1 pj = 1. Since the summation
over the empty set equals zero, ‚Ñô(0) = 0.
Thus it remains to show that ‚Ñôis œÉ-additive. Take disjoint subsets A1, A2, . . . of Œ©.
Since Œ© is finite, there are at most finitely many of the Ajs which are nonempty. Say, for
simplicity, these are the first n sets A1, . . . , An. Then we get
‚Ñô(
‚àû
‚ãÉ
k=1
Ak) = ‚Ñô(
n
‚ãÉ
k=1
Ak) =
‚àë
{j:œâj‚àà‚ãÉn
k=1 Ak}
pj
=
n
‚àë
k=1
‚àë
{j:œâj‚ààAk}
pj =
‚àû
‚àë
k=1
‚àë
{j:œâj‚ààAk}
pj =
‚àû
‚àë
k=1
‚Ñô(Ak) ,

1.3 Discrete probability measures
‡±™
17
hence ‚Ñôis œÉ-additive.
By the construction, ‚Ñô({œâj}) = pj, which completes the proof.
Summary: If Œ© = {œâ1, . . . , œâN}, then probability measures ‚Ñôon ùí´(Œ©) can be identified
with sequences (pj)N
j=1 satisfying the conditions (1.9).
{Probability measures ‚Ñôon ùí´(Œ©)}
‚áê‚áí
{Sequences (pj)N
j=1 for which (1.9) hold}
Hereby the assignment from the left- to the right-hand side goes via pj = ‚Ñô({œâj}) while
in the other direction ‚Ñôis given by eq. (1.11).
Example 1.3.3. Assume Œ© = {1, 2, 3}. Then each probability measure ‚Ñôon ùí´(Œ©) is
uniquely determined by the three numbers p1 = ‚Ñô({1}), p2 = ‚Ñô({2}), and p3 = ‚Ñô({3}).
These numbers satisfy p1, p2, p3 ‚â•0 and p1 + p2 + p3 = 1. Conversely, any three numbers
p1, p2, and p3 with these properties generate a probability measure on ùí´(Œ©) via (1.11).
For example, if A = {1, 3}, then ‚Ñô(A) = p1 + p3.
Next we treat countably infinite sample spaces, that is, Œ© = {œâ1, œâ2, . . . }. Also here we
may take ùí´(Œ©) as œÉ-field and, as in the case of finite sample spaces, given a probability
measure ‚Ñôon ùí´(Œ©), we set
pj := ‚Ñô({œâj}) ,
j = 1, 2, . . .
Then (pj)‚àû
j=1 obeys the following properties:
pj ‚â•0
and
‚àû
‚àë
j=1
pj = 1 .
(1.12)
The proof is the same as in the finite case. The only difference is that here we have to use
the œÉ-additivity of ‚Ñôbecause this time Œ© = ‚ãÉ‚àû
j=1{œâj}. By the same argument, it follows
for A ‚äÜŒ© that
‚Ñô(A) =
‚àë
{j‚â•1 : œâj‚ààA}
pj .
Hence, again the pjs determine ‚Ñôcompletely.
Conversely, let (pj)‚àû
j=1 be an arbitrary sequence of real numbers with properties
(1.12).
Proposition 1.3.4. The mapping ‚Ñôdefined by
‚Ñô(A) =
‚àë
{j‚â•1 : œâj‚ààA}
pj
(1.13)
is a probability measure on ùí´(Œ©) with ‚Ñô({œâj}) = pj, 1 ‚â§j < ‚àû.

18
‡±™
1 Probabilities
Proof. The proof is analogous to that of Proposition 1.3.2 with one important exception.
In the case |Œ©| < ‚àû, we used that there are at most finitely many disjoint nonempty
subsets. This is no longer valid. Thus a different argument is needed.
Given disjoint subsets A1, A2, . . . , in Œ© set
Ik = {j ‚â•1 : œâj ‚ààAk} .
Then Ik ‚à©Il = 0 if k
Ã∏= l, thus,
‚Ñô(Ak) = ‚àë
j‚ààIk
pj
and
‚Ñô(
‚àû
‚ãÉ
k=1
Ak) = ‚àë
j‚ààI
pj,
where I = ‚ãÉ‚àû
k=1 Ik. Use that i ‚ààI if and only if there is some k ‚â•1 with i ‚ààIk or,
equivalently, with œâi ‚ààAk, that is, if and only if œâi ‚àà‚ãÉ‚àû
k=1 Ak.
Since pj ‚â•0, Remark A.5.6 applies and leads to
‚Ñô(
‚àû
‚ãÉ
k=1
Ak) = ‚àë
j‚ààI
pj =
‚àû
‚àë
k=1
‚àë
j‚ààIk
pj =
‚àû
‚àë
k=1
‚Ñô(Ak) .
Thus ‚Ñôis œÉ-additive.
The equality ‚Ñô({œâj}) = pj, 1 ‚â§j < ‚àû, is again a direct consequence of the definition
of ‚Ñô.
Summary: If Œ© = {œâ1, œâ2, . . . }, then probability measures ‚Ñôon ùí´(Œ©) can be identified
with (infinite) sequences (pj)‚àû
j=1 possessing the properties (1.12).
{Probability measures ‚Ñôon ùí´(Œ©)}
‚áê‚áí
{Sequences (pj)‚àû
j=1 satisfying (1.12)}
Again, the assignment from the left- to the right-hand side goes via pj = ‚Ñô({œâj}) while
the other direction rests upon eq. (1.13).
Example 1.3.5. For Œ© = ‚Ñïand j ‚â•1, let pj = 2‚àíj. These pjs satisfy conditions (1.12) (check
this!). The generated probability measure ‚Ñôon ùí´(‚Ñï) is then given by
‚Ñô(A) := ‚àë
j‚ààA
1
2j .
For example, if A = {2, 4, 6, . . .}, then we get
‚Ñô(A) = ‚àë
j‚ààA
1
2j =
‚àû
‚àë
k=1
1
22k =
1
1 ‚àí1/4 ‚àí1 = 1
3 .
Or, if B = {N + 1, N + 2, . . .} for a certain N ‚àà‚Ñï, then one obtains

1.3 Discrete probability measures
‡±™
19
‚Ñô(B) =
‚àû
‚àë
j=N+1
1
2j = 2‚àíN‚àí1
‚àû
‚àë
j=0
1
2j = 2‚àíN‚àí1 ‚ãÖ2 = 2‚àíN .
Another way to evaluate the probability of B is
‚Ñô(B) = 1 ‚àí‚Ñô(Bc) = 1 ‚àí
N
‚àë
j=1
1
2j = 1 ‚àí[1 ‚àí2‚àíN‚àí1
1 ‚àí1
2
‚àí1] = 2‚àíN .
Example 1.3.6. Let Œ© = ‚Ñ§\ {0}, that is, Œ© = {1, ‚àí1, 2, ‚àí2, . . . }. With c > 0 specified later
on, assume
pk = c
k2 ,
k ‚ààŒ© .
The number c > 0 has to be chosen so that the conditions (1.12) are satisfied, hence it has
to fulfill
1 = c
‚àë
k‚àà‚Ñ§\{0}
1
k2 = 2 c
‚àû
‚àë
k=1
1
k2 .
But, as is well known,6
‚àû
‚àë
k=1
1
k2 = œÄ2
6 ,
which implies c =
3
œÄ2 . Thus ‚Ñôon ùí´(Œ©) is uniquely described by
‚Ñô({k}) = 3
œÄ2
1
k2 ,
k ‚àà‚Ñ§\ {0} .
For example, if A = ‚Ñï, then
‚Ñô(A) = 3
œÄ2
‚àû
‚àë
k=1
1
k2 = 3
œÄ2
œÄ2
6 = 1
2 .
Or if A = {2, 4, 6, . . . }, it follows that
‚Ñô(A) = 3
œÄ2
‚àû
‚àë
k=1
1
(2k)2 = 1
4 ‚Ñô(‚Ñï) = 1
8 .
For later purposes, we want to combine the two cases of finite and countably infinite
sample spaces and thereby introduce a slight generalization.
6 We refer to [Mor16], where one can find an easy proof of this fact. The problem to compute the value
of the sum is known as ‚ÄúBasel problem.‚Äù The first solution was found in 1734 by Leonhard Euler. Note
that ‚àëk‚â•1 1/k2 = Œ∂(2) with Riemann‚Äôs Œ∂-function.

20
‡±™
1 Probabilities
Let Œ© be an arbitrary sample space. A probability measure ‚Ñôis said to be discrete
if there is an at most countably infinite set D ‚äÜŒ© (i. e., either D is finite or countably
infinite) such that ‚Ñô(D) = 1. Then for A ‚äÜŒ©,
‚Ñô(A) = ‚Ñô(A ‚à©D) = ‚àë
œâ‚ààD
‚Ñô({œâ}) .
Since ‚Ñô(Dc) = 0, this says that ‚Ñôis concentrated on D. Of course, all previous results
for a finite or countably infinite sample space carry over to this more general setting.
Discrete probability measures ‚Ñôare concentrated on an at most countably infinite set D. They are uniquely
determined by the values ‚Ñô({œâ}), where œâ ‚ààD.
Of course, if the sample space is either finite or countably infinite, then all probability
measures on this space are discrete. Nondiscrete probability measures will be intro-
duced and investigated in Section 1.5.
Example 1.3.7. We once more model a single rolling of a die, but now we take as sample
space Œ© = ‚Ñù. Define ‚Ñô({œâ}) = 1
6 if œâ = 1, . . . , 6 and ‚Ñô({œâ}) = 0 otherwise. If D = {1, . . . , 6},
then ‚Ñô(D) = 1, hence ‚Ñôis discrete. Given A ‚äÜ‚Ñù, it follows that
‚Ñô(A) = |A ‚à©D|
6
.
For example, we have ‚Ñô([‚àí2, 2]) = 1
3 and ‚Ñô([3, ‚àû)) = 2
3.
Another, maybe a little artificial, example is as follows.
Example 1.3.8. It is known that the set ‚Ñöof rational numbers is countably infinite.
Hence ‚Ñö= {q1, q2, . . .} with rational numbers qk. Take any sequence (pk)‚àû
k=1 of positive
numbers with ‚àë‚àû
k=1 pk = 1. Then ‚Ñôdefined by
‚Ñô(A) =
‚àë
{k:qk‚ààA}
pk ,
A ‚äÜ‚Ñù,
is a discrete probability measure on ‚Ñùwith ‚Ñô(‚Ñö) = 1. The problem with this probability
measure is that it is completely impossible to evaluate ‚Ñô(B) for almost all B ‚äÜ‚Ñù, even if
the pks are known.
1.4 Special discrete probability measures
1.4.1 Dirac measure
The simplest discrete probability measure is concentrated at a single point. That is, there
exists an œâ0 ‚ààŒ© such that ‚Ñô({œâ0}) = 1. This probability measure is denoted by Œ¥œâ0.

1.4 Special discrete probability measures
‡±™
21
Consequently, for each A ‚ààùí´(Œ©), one has
Œ¥œâ0(A) = {1
if œâ0 ‚ààA,
0
if œâ0 ‚àâA.
(1.14)
Definition 1.4.1. The probability measure Œ¥œâ0 defined by eq. (1.14) is called the Dirac measure or point
measure at œâ0.
Which random experiment does (Œ©, ùí´(Œ©), Œ¥œâ0) model? It describes the experiment
where, with probability one, the value œâ0 occurs. Thus, in fact it is a deterministic
experiment, not random.
Dirac measures are useful tools to represent general discrete probability measures.
Assume ‚Ñôis concentrated on D = {œâ1, œâ2, . . . } and let pj = ‚Ñô({œâj}). Then we may write
‚Ñô=
‚àû
‚àë
j=1
pj Œ¥œâj .
(1.15)
Conversely, if a measure ‚Ñôis represented as in eq. (1.15) with certain œâj ‚ààŒ© and numbers
pj ‚â•0, ‚àë‚àû
j=1 pj = 1, then ‚Ñôis discrete with ‚Ñô(D) = 1, where D = {œâ1, œâ2, . . .}.
1.4.2 Uniform distribution on a finite set
The sample space is finite, say Œ© = {œâ1, . . . , œâN}, and we assume that all elementary
events are equally likely, that is,
‚Ñô({œâ1}) = ‚ãÖ‚ãÖ‚ãÖ= ‚Ñô({œâN}) .
A typical example is a fair die, where Œ© = {1, . . . , 6}.
Since 1 = ‚Ñô(Œ©) = ‚àëN
j=1 ‚Ñô({œâj}), we immediately get ‚Ñô({œâj}) = 1/N for all j ‚â§N. If
A ‚äÜŒ©, an application of eq. (1.11) leads to
‚Ñô(A) = |A|
N = |A|
|Œ©| .
(1.16)
Definition 1.4.2. The probability measure ‚Ñôdefined by eq. (1.16) is called the uniform distribution or
Laplace distribution on the finite set Œ©.
The following formula may be helpful for remembrance. If ‚Ñôis the uniform distribution
on a finite sample space Œ©, then
‚Ñô(A) = Number of cases favorable for A
Number of possible cases
,
A ‚äÜŒ© .

22
‡±™
1 Probabilities
Example 1.4.3. In a lottery, 6 numbers are chosen out of 49 and each number appears
only once. What is the probability that the chosen numbers are exactly the six marked
on my lottery coupon?
Answer: Let us give two different approaches to answer this question.
Approach 1: We record the chosen numbers in the order they show up. As a sample
space, we may take
Œ© := {(œâ1, . . . , œâ6) : œâi ‚àà{1, . . . , 49}, œâi
Ã∏= œâj if i
Ã∏= j} .
Then the number of possible cases is
|Œ©| = 49 ‚ãÖ48 ‚ãÖ47 ‚ãÖ46 ‚ãÖ45 ‚ãÖ44 = 49!
43! .
Let A be the event that the numbers on my lottery coupon appear. Which cardinality
does A possess?
Say, for simplicity, that in our coupon the numbers 1, 2, . . . , 6 are marked. Then
it is favorable for A if these numbers appear in this order. But it is also favorable if
(2, 1, 3, . . . , 6) shows up, that is, any permutation of 1, . . . , 6 is favorable. Hence |A| = 6!,
which leads to7
‚Ñô(A) =
6!
49 ‚ãÖ‚ãÖ‚ãÖ44 =
1
(49
6 ) = 7.15112 √ó 10‚àí8 .
Approach 2: We assume that the chosen numbers are already ordered by their size
(as they are published in a newspaper). In this case our sample space is
Œ© := {(œâ1, . . . , œâ6) : 1 ‚â§œâ1 < ‚ãÖ‚ãÖ‚ãÖ< œâ6 ‚â§49},
and now
|Œ©| = (49
6 ) .
Why? Any set of six different numbers may be written exactly in one way in increas-
ing order and thus choosing six ordered numbers is exactly the same as choosing a
(nonordered) set of six numbers. And there are (49
6 ) possibilities to choose six numbers.
In this setting, we have |A| = 1, thus also here we get
‚Ñô(A) =
1
(49
6 ) .
7 To get an impression about the size of this number, assume we buy lottery coupons with all possible
choices of the six numbers. If each coupon is 0.5-mm thick, then all coupons together have a size of
6.992 km, which is about 4.3 miles. And in this row of 4.3 miles there exists exactly one coupon with the
six numbers chosen in the lottery.

1.4 Special discrete probability measures
‡±™
23
Example 1.4.4. A fair coin is labeled with ‚Äú0‚Äù and ‚Äú1‚Äù. Toss it n times and record the
sequence of zeroes and ones in the order of their appearance. Thus,
Œ© := {0, 1}n = {(œâ1, . . . , œân) : œâi ‚àà{0, 1}} ,
and |Œ©| = 2n. The coin is assumed to be fair, hence each sequence of zeroes and ones is
equally likely. Therefore, whenever A ‚äÜŒ©, one has
‚Ñô(A) = |A|
2n .
Take, for example, the event A where for some fixed i ‚â§n the ith toss equals ‚Äú0‚Äù,
that is,
A = {(œâ1, . . . , œân) : œâi = 0} .
Then |A| = 2n‚àí1 leads to the (not surprising) result
‚Ñô(A) = 2n‚àí1
2n
= 1
2 .
Or let A occur if we observe for some given k ‚â§n exactly k times the number ‚Äú1‚Äù. Then
|A| = (n
k), and we get
‚Ñô(A) = (n
k) ‚ãÖ1
2n .
Suppose n ‚â•2. How likely is it that the first and the last toss coincide? There are 2n‚àí2
possibilities that the first and the last toss are both 0 and also 2n‚àí2 ways for both tosses
to be 1. Hence, the probability of this event equals
2n‚àí2 + 2n‚àí2
2n
= 1
2 .
Example 1.4.5. We have k particles that we distribute randomly into n boxes. All possi-
ble distributions of the particles are assumed to be equally likely. How do we get ‚Ñô(A)
for a given event A?
Answer: In this formulation, the question is not asked correctly because we did not
fix when two distributions of particles coincide. Compare Figures 1.2 and 1.3 to under-
stand why it is important whether or not the particles are anonymous or distinguishable.
Let us illustrate this problem in the case of two particles and two boxes. If the parti-
cles are not distinguishable (anonymous) then there are three different ways to distribute
the particles into the two boxes. Thus, assuming that all distributions are equally likely,
each elementary event has probability 1/3.

24
‡±™
1 Probabilities
On the other hand, if the particles are distinguishable, that is, they carry names, here
1 and 2, then there exist four different ways of distributing them, hence each elementary
event has probability 1/4.
Figure 1.2: Distributing two distinguishable particles into two boxes. Each event has probability 1/4.
Figure 1.3: Distributing two anonymous particles into two boxes. Each event occurs with probability 1/3.
Let us answer the above question in the two cases (distinguishable and anonymous)
separately.
Distinguishable particles: Recall that we have k particles and n boxes. So we may
enumerate the particles from 1 to k and each distribution of particles is uniquely de-
scribed by a sequence (a1, . . . , ak), where aj ‚àà{1, . . . , n}. For example, a1 = 3 means that
particle one is in box 3. Hence, a suitable sample space is
Œ© = {(a1, . . . , ak) : 1 ‚â§ai ‚â§n} .
Since |Œ©| = nk, for events A ‚äÜŒ©, it follows that
‚Ñô(A) = |A|
nk .
Anonymous particles: We record how many of the k particles are in box 1, how many
are in box 2, and so on up to box n. Thus, as sample space we may choose
Œ© = {(k1, . . . , kn) : 0 ‚â§kj ‚â§k, k1 + ‚ãÖ‚ãÖ‚ãÖ+ kn = k} .
The sequence (k1, . . . , kn) occurs if box 1 contains k1 particles, box 2 contains k2, and so
on. From the results in case 3 of Section A.3.2, we derive
|Œ©| = (n + k ‚àí1
k
) .
Hence, if A ‚äÜŒ©, then
‚Ñô(A) = |A|
|Œ©| = |A| k! (n ‚àí1)!
(n + k ‚àí1)!.

1.4 Special discrete probability measures
‡±™
25
Summary: If we distribute k particles into n boxes and assume that all distributions are equally likely,8 then
in the case of distinguishable or of anonymous particles for any set A of distributions,
‚Ñô(A) = |A|
nk
or
‚Ñô(A) = |A| k! (n ‚àí1)!
(n + k ‚àí1)!,
respectively.
Let us evaluate ‚Ñô(A) for some concrete event A in both cases. Suppose k ‚â§n and select
k of the n boxes. Set
A := {In each of the chosen k boxes, there is exactly one particle}.
(1.17)
To simplify the notation, assume that the first k boxes have been chosen. The general
case is treated in a similar way. Then in the ‚Äúdistinguishable case‚Äù the event A occurs if
and only if for some permutation œÄ ‚ààSk the sequence (œÄ(1), . . . , œÄ(k), 0, . . . , 0) appears.
Thus |A| = k! and
‚Ñô(A) = k!
nk .
(1.18)
In the ‚Äúanonymous case,‚Äù it follows that |A| = 1 (why?). Hence here we obtain
‚Ñô(A) = k! (n ‚àí1)!
(n + k ‚àí1)! .
(1.19)
Additional question: For k ‚â§n, define the event B by
B := {Each of the n boxes contains at most 1 particle}.
Find ‚Ñô(B) in both cases.
Answer: The event B is the (disjoint) union of the following events: the k particles are
distributed in a given collection of k boxes. The probability of this event was calculated
in eqs. (1.18) and (1.19), respectively. Since there are (n
k) possibilities to choose k boxes
out of n, we get ‚Ñô(B) = (n
k)‚Ñô(A), with A as defined by (1.17), that is,
‚Ñô(B) = (n
k) ‚ãÖk!
nk =
n!
(n ‚àík)! nk
and
‚Ñô(B) = (n
k) ‚ãÖk! (n ‚àí1)!
(n + k ‚àí1)! =
n! (n ‚àí1)!
(n ‚àík)! (n + k ‚àí1)!,
(1.20)
respectively.
8 Compare Example 1.4.20 and the following remark.

26
‡±™
1 Probabilities
Example 1.4.6. Suppose we distribute 6 particles into 3 boxes such that all distributions
are equally likely. What is the probability that each of the three boxes contains exactly
two particles?
Answer: Let us first assume that the particles are distinguishable. If A is the event
that each box contains 2 particles, then it follows that
|A| = (
6
2, 2, 2) = 6!
23 = 90 .
Consequently, we obtain
‚Ñô(A) = |A|
36 = 10
81.
Let us turn now to the case of indistinguishable particles. Here we have |A| = 1 (why?),
hence in this case it follows that
‚Ñô(A) = 6!(3 ‚àí1)!
(3 + 6 ‚àí1)! = 2 ‚ãÖ6!
8!
= 1
28 .
Example 1.4.7. Let us check the validity of formula (1.20) by an easy test. In Exam-
ple A.3.15, we evaluate the number of tiles of a domino in the following way. There are 7
boxes and we place two particles into these boxes. Hereby, the particles are anonymous
and all distributions are equally likely. Then the event B defined by ‚Äúat most one particle
in each box‚Äù corresponds to tiles with different numbers of dots. Now choose one of
the 28 tiles at random. Since there are 21 tiles with different numbers of dots, it follows
that
‚Ñô(B) = 21
28 = 3
4 .
On the other hand, formula (1.20) leads to (recall that n = 7 and k = 2)
‚Ñô(B) =
n! (n ‚àí1)!
(n ‚àík)! (n + k ‚àí1)! = 7! ‚ãÖ6!
5! ‚ãÖ8! = 6
8 = 3
4 .
So we see, in this case formula (1.20) gives the correct value.
Example 1.4.8. Suppose the sample space Œ© is given by
Œ© = {(k1, . . . , k6) : k1 + ‚ãÖ‚ãÖ‚ãÖ+ k6 = 4, kj ‚àà‚Ñï0}
and endowed with the uniform distribution. That is, all possible representations of the
number 4 by six nonnegative integers are equally likely. So, for example, the occurrence
of (1, 2, 0, 0, 1, 0) is as likely as that of (0, 0, 0, 0, 0, 4) or that of (4, 0, 0, 0, 0, 0).
Questions:
(1) What is the cardinality of Œ©?

1.4 Special discrete probability measures
‡±™
27
(2) How likely is it to observe those (k1, . . . , k6) ‚ààŒ© for which 0 ‚â§kj ‚â§1?
Answers: An equivalent model is as follows: one has six boxes and places four anony-
mous particles into these boxes. In this way, kj describes the number of particles in box
j with 1 ‚â§j ‚â§6.
In the setting of case 3 of Section A.3.2, we have k = 4 and n = 6. Hence, the cardi-
nality of the sample space equals
|Œ©| = (n + k ‚àí1
n ‚àí1 ) = (9
5) = 126 .
To answer the second question, note that all numbers kj satisfy kj ‚â§1 if and only if each
of the six boxes contains at most one particle. According to eq. (1.20), the probability of
this event is given by
n! (n ‚àí1)!
(n ‚àík)! (n + k ‚àí1)! = 6! ‚ãÖ5!
2! ‚ãÖ9! = 5
42 ‚âà0.119048 .
Another (more direct) way to obtain the last result is as follows: there are (6
4) ways to
write 4 as a sum of six numbers which are either 0 or 1. Hence, the desired probability
equals
(6
4)
|Œ©| = 15
126 = 5
42 .
Summary: The uniform distribution ‚Ñôon a sample space Œ© = {œâ1, . . . , œâN} is characterized by
‚Ñô({œâ1}) = ‚ãÖ‚ãÖ‚ãÖ= ‚Ñô({œâN}) = 1
N ,
or, equivalently, by
‚Ñô(A) = |A|
N = Number of cases favorable for A
Number of possible cases
,
A ‚äÜŒ© .
1.4.3 Binomial distribution
The sample space is Œ© = {0, 1, . . . , n} for some n ‚â•1 and p is a real number with 0 ‚â§p ‚â§1.
Proposition 1.4.9. There exists a unique probability measure Bn,p on ùí´(Œ©) satisfying
Bn,p({k}) = (n
k) pk(1 ‚àíp)n‚àík ,
k = 0, . . . , n .
(1.21)
Proof. In order to use Proposition 1.3.2, we have to verify Bn,p({k}) ‚â•0 and
‚àën
k=0 Bn,p({k}) = 1. The first property is obvious because of 0 ‚â§p ‚â§1 and 0 ‚â§1 ‚àíp ‚â§1.
To prove the second, we apply the binomial theorem (Proposition A.3.8) with a = p and
with b = 1 ‚àíp. This leads to

28
‡±™
1 Probabilities
n
‚àë
k=0
Bn,p({k}) =
n
‚àë
k=0
(n
k) pk(1 ‚àíp)n‚àík = (p + (1 ‚àíp))
n = 1 .
Hence the assertion follows by Proposition 1.3.2 with pk = Bn,p({k}), k = 0, . . . , n.
Compare Figure 1.4 for the values of Bn,p({k}) in the case n = 9 and with, p = 1/2 and
p = 1/4, respectively.
Figure 1.4: Probabilities Bn,p({k}) with n = 9, p = 1/2, and p = 1/4, k = 0, . . . , 9.
Definition 1.4.10. The probability measure Bn,p defined by eq. (1.21) is called the binomial distribution
with parameters n and p.
Remark 1.4.11. Observe that Bn,p acts as follows. If A ‚äÜ{0, . . . , n}, then
Bn,p(A) = ‚àë
k‚ààA
(n
k) pk(1 ‚àíp)n‚àík .
Furthermore, for p = 1/2, we get
Bn,1/2({k}) = (n
k) 1
2n .
As we saw in Example 1.4.4, this probability describes the k-fold occurrence of ‚Äú1‚Äù when
tossing a fair coin n times.
Which random experiment describes the binomial distribution? To answer this
question, let us first look at the case n = 1. Here we have Œ© = {0, 1} with
Bn,p({0}) = 1 ‚àíp
and
Bn,p({1}) = p .
If we identify ‚Äú0‚Äù with failure and ‚Äú1‚Äù with success, then the binomial distribution de-
scribes an experiment where either success or failure may occur, and the success proba-
bility is p. Now we execute the same experiment n times and every time we may observe
either failure or success. If we have k times success, then there are (n
k) ways to obtain

1.4 Special discrete probability measures
‡±™
29
these k successes during the n trials. The probability for success is p and for failure 1‚àíp.
By the independence of the single trials, the probability for the sequence is pk(1 ‚àíp)n‚àík.
Multiplying this probability with the number of different positions of successes, we fi-
nally arrive at (n
k)pk(1 ‚àíp)n‚àík, the value of Bn,p({k}).
Example 1.4.12. An exam consists of 100 problems where each of the question may be
answered either with ‚Äúyes‚Äù or ‚Äúno.‚Äù To pass the exam, at least 60 questions have to be
answered correctly. Let p be the probability to answer a single question correctly. How
big does p have to be in order to pass the exam with a probability greater than 75 %?
Answer: The number p has to be chosen such that the following estimate is satis-
fied:
100
‚àë
k=60
(100
k )pk(1 ‚àíp)100‚àík ‚â•0.75 .
Numerical calculations show that this is valid if and only if p ‚â•0.62739.
Example 1.4.13. In an auditorium there are N students. Find the probability that at least
two of them have their birthday on April 1.
Answer: We do not take leap years into account and assume that there are no twins
among the students. Finally, we make the (probably unrealistic) assumption that all days
of a year are equally likely as birthdays. Say success occurs if a student has birthday on
April 1. Under the above assumptions, the success probability is 1/365. Hence the number
of students having birthday on April 1 is binomially distributed with parameters N and
p = 1/365. We ask for the probability of A = {2, 3, . . . , N}. This may be evaluated by
N
‚àë
k=2
(N
k )( 1
365)
k
(364
365)
N‚àík
= 1 ‚àíBN,1/365({0}) ‚àíBN,1/365({1})
= 1 ‚àí(364
365)
N
‚àíN
365(364
365)
N‚àí1
.
For example, N = 500 this probability is approximately 0.397895.
Example 1.4.14. In an urn there are 40 white balls and 60 black ones. One chooses balls
one after another from the urn with replacement. How often does one have to choose
balls in order to observe with probability greater than 0.5 at least 10 white balls?
Answer: The success probability is p = 40/100 = 2/5. So we ask for the minimal
number n ‚â•10 for which
Bn,p({10, 11, . . . , n}) ‚â•0.5
‚áî
n
‚àë
k=10
(n
k)(2
5)
k
(3
5)
n‚àík
‚â•0.5 .
Numerical calculations give the following probabilities:

30
‡±™
1 Probabilities
n
Bn,p({10, 11, . . . , n})
20
0.244663
21
0.308558
22
0.375648
23
0.443771
24
0.510920
So we see that n = 24 is the minimal number of trials to obtain at least 10 white
balls with probability greater than or equal to 1/2. Of course, if we increase the number
of trials then it becomes more and more likely to get at least 10 white balls. For exam-
ple, if one takes out 40 balls, then the probability for at least 10 white balls is about
0.9845.
Summary: The binomial distribution describes the following setting. One executes n times independently
the same experiment where each time either success or failure may appear. The success probability is p. Then
Bn,p({k}) is the probability to observe exactly k successes or, equivalently, n ‚àík failures.
1.4.4 Multinomial distribution
Given natural numbers n and m, the sample space for the multinomial distribution
is9
Œ© := {(k1, . . . , km) ‚àà‚Ñïm
0 : k1 + ‚ãÖ‚ãÖ‚ãÖ+ km = n} .
With certain nonnegative real numbers p1, . . . , pm satisfying p1 + ‚ãÖ‚ãÖ‚ãÖ+ pm = 1, set
‚Ñô({(k1, . . . , km)}) := (
n
k1, . . . , km
) pk1
1 ‚ãÖ‚ãÖ‚ãÖpkm
m ,
(k1, . . . , km) ‚ààŒ© .
(1.22)
Recall that the multinomial coefficients appearing in eq. (1.22) were defined in eq. (A.16)
as
(
n
k1, . . . , km
) =
n!
k1! ‚ãÖ‚ãÖ‚ãÖkm! .
The next result shows that eq. (1.22) defines a probability measure.
Proposition 1.4.15. There is a unique probability measure ‚Ñôon ùí´(Œ©) such that (1.22)
holds for all (k1, . . . , km) ‚ààŒ©.
9 By case 3 in Section A.3.2, the cardinality of Œ© is (n+m‚àí1
n
).

1.4 Special discrete probability measures
‡±™
31
Proof. An application of the multinomial theorem (Proposition A.3.20) implies
‚àë
(k1,...,km)‚ààŒ©
‚Ñô({(k1, . . . , km)}) =
‚àë
k1+‚ãÖ‚ãÖ‚ãÖ+km=n
ki‚â•0
(
n
k1, . . . , km
) pk1
1 ‚ãÖ‚ãÖ‚ãÖpkm
m
= (p1 + ‚ãÖ‚ãÖ‚ãÖ+ pm)n = 1n = 1 .
Since ‚Ñô({(k1, . . . , km)}) ‚â•0, the assertion follows by Proposition 1.3.2.
In view of the preceding proposition, the following definition is justified.
Definition 1.4.16. The probability measure ‚Ñôdefined by eq. (1.22) is called the multinomial distribu-
tion with parameters n, m, and p1, . . . , pm.
Remark 1.4.17. Sometimes it is useful to regard the multinomial distribution on the
larger sample space Œ© = ‚Ñïm
0 . In this case we have to modify eq. (1.22) slightly as follows:
‚Ñô({(k1, . . . , km)}) = {(
n
k1,...,km) pk1
1 ‚ãÖ‚ãÖ‚ãÖpkm
m
if k1 + ‚ãÖ‚ãÖ‚ãÖ+ km = n,
0
if k1 + ‚ãÖ‚ãÖ‚ãÖ+ km
Ã∏= n.
Which random experiment does the multinomial distribution describe? To answer
this question, let us recall the model for the binomial distribution. In an urn there are
balls of two different colors, say white and red. The proportion of the white balls is p,
hence 1‚àíp is the proportion of the red ones. If we choose n balls with replacement, then
Bn,p({k}) is the probability to observe exactly k white balls.
What happens if in the urn there are balls of more than two different colors, say of
m, and the proportions of the colored balls are p1, . . . , pm with p1 + ‚ãÖ‚ãÖ‚ãÖ+ pm = 1?
As in the model for the binomial distribution, we choose n balls with replacement.
Given integers kj ‚â•0, one asks now for the probability of the following event: balls of
the first color showed up k1 times, those of the second k2 times, and so on. Of course, this
probability is zero whenever k1 + ‚ãÖ‚ãÖ‚ãÖ+ km
Ã∏= n. But if the sum is n, then pk1
1 ‚ãÖ‚ãÖ‚ãÖpkm
m is the
probability for kj balls of color j in some fixed order. There are (
n
k1,...,km) ways to order the
balls without changing the frequency of the colors. Thus the desired probability equals
(
n
k1,...,km) pk1
1 ‚ãÖ‚ãÖ‚ãÖpkm
m .
Remark 1.4.18. If m = 2, then p2 = 1‚àíp1, as well as ( n
k1,k2) = (
n
k1,n‚àík1) = ( n
k1). Consequently,
in this case the multinomial distribution coincides with the binomial distribution Bn,p1.
Example 1.4.19. Suppose in an urn there are 3 white, 5 red, and 4 black balls. Choose 9
balls with replacement. How likely is it to observe three balls of each color.
Answer: The success probabilities are p1 = 3/12, p2 = 5/12, and p3 = 4/12. Hence, the
desired probability equals
(
9
3, 3, 3)( 3
12)
3
( 5
12)
3
( 4
12)
3
= 9! ‚ãÖ603
63 ‚ãÖ129 ‚âà0.0703286 .

32
‡±™
1 Probabilities
Additional question: How likely is it to observe no white ball among the 9 chosen
ones?
Answer: An approach via the multinomial distributions would be possible. Then
one has to sum over all possible choices of red and black balls. But the problem can be
handled in a more direct way. Say success occurs if the chosen ball is white and failure
if this is not so. Then the success probability is 3/12 = 1/4. Thus, we ask for a total of 9
failures which has the probability (3/4)9 ‚âà0.0750847.
Example 1.4.20. Suppose we have m boxes B1, . . . , Bm and n particles that we place suc-
cessively into these boxes. Thereby pj is the probability to place a single particle into box
Bj. What is the probability that after distributing all n particles there are k1 particles in
the first box, k2 in the second, and all the way up to km in the last one?
Answer: This probability is given by formula (1.22), that is,
‚Ñô{k1 particles are in B1, . . . , km particles are in Bm} = (
n
k1, . . . , km
) pk1
1 ‚ãÖ‚ãÖ‚ãÖpkm
m .
For example, if we place 5 particles into 3 boxes with probabilities 1/2, 1/3, and 1/6, re-
spectively, then
‚Ñô{k1 are in B1, k2 are in B2, k3 are in B3} = (
5
k1, k2, k3
)(1
2)
k1
( 1
3)
k2
( 1
6)
k3
provided that k1 + k2 + k3 = 5.
Remark 1.4.21. In the case that all m possible different outcomes of an experiment are
equally likely, that is, we have
p1 = ‚ãÖ‚ãÖ‚ãÖ= pm = 1
m ,
then
‚Ñô({(k1, . . . , km)}) = (
n
k1, . . . , km
) 1
mn ,
k1 + ‚ãÖ‚ãÖ‚ãÖ+ km = n .
(1.23)
Example 1.4.22. Roll a fair die 12 times. How likely is it that each of the six possible
results appears exactly twice?
Answer: An equivalent formulation of the problem is as follows: there are 6 boxes
and 12 particles. Place these 12 particles randomly into the 6 boxes so that all boxes are
equally likely. Let the event A occur if in each of the six boxes there are two particles or,
equivalently, if each of the numbers from 1 to 6 appears twice. Then we get
‚Ñô(A) = (
12
2, 2, 2, 2, 2, 2)( 1
6)
12
=
12!
26 612 ‚âà0.00343829 .
Example 1.4.23. Suppose that in Example 1.4.20 all m boxes are chosen with probabil-
ity 1/m. Then, if n ‚â§m, one may ask for the probability that each of the first n boxes

1.4 Special discrete probability measures
‡±™
33
B1, . . . , Bn contains exactly one particle. By eq. (1.23), it follows that
‚Ñô({(1, . . . , 1
‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
n
, 0, . . . , 0)}) = (
n
1, . . . , 1
‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
n
, 0, . . . , 0) 1
mn = n!
mn .
(1.24)
Remark 1.4.24. From a different point of view, we investigated the last problem already
in Example 1.4.5. But why do we get in eq. (1.24) the same answer as in the case of distin-
guishable particles although the n distributed ones are anonymous?
Answer: The crucial point is that we assumed in the anonymous case that all parti-
tions of the particles are equally likely. And this is not valid when distributing the par-
ticles successively. To see this, assume n = m = 2. Then there exist three different ways
to distribute the particles, but they have different probabilities:
‚Ñô({(0, 2)}) = ‚Ñô({(2, 0)}) = 1
4
while ‚Ñô({(1, 1)}) = 1
2 .
Thus, although the distributed particles are not distinguishable, they get names due to
the successive distribution (first particle, second particle, etc.).
Example 1.4.25. Six people randomly enter a train with three coaches. Each person
chooses his wagon independently of the others and all coaches are equally likely to be
chosen. Find the probability that there are two people in each coach.
Answer: We have m = 3, n = 6, and p1 = p2 = p3 = 1
3. Hence the probability we are
looking for is
‚Ñô({(2, 2, 2)}) = (
6
2, 2, 2) 1
36 =
6!
2! 2! 2!
1
36 = 10
81 = 0.12345679 .
Check how this result is related to that presented in Example 1.4.6.
Example 1.4.26. In a country 40 % of the cars are gray, 20 % are black, and 10 % are red.
The remaining cars have different colors. Now we observe at random 10 cars. What is
the probability to see two gray cars, four black, and one red?
Answer: By assumption m = 4 (gray, black, red, and others), p1 = 2/5, p2 = 1/5,
p3 = 1/10, and p4 = 3/10. Thus the probability of the vector (2, 4, 1, 3) is given by
(
10
2, 4, 1, 3) (2
5)
2
(1
5)
4
( 1
10)
1
( 3
10)
3
=
10!
2! 4! 1! 3! ‚ãÖ22
52 ‚ãÖ1
54 ‚ãÖ1
10 ‚ãÖ33
103
= 0.00870912 .
Summary: The multinomial distribution is a generalization of the binomial distribution from two (failure or
success) to m ‚â•2 possible results. In each single experiment, m different results may occur (e. g., m different
colors of balls) and each time the jth result, 1 ‚â§j ‚â§m, shows up with probability pj. If one executes the
experiment n times, then the multinomial distribution describes the probability of the following event: the

34
‡±™
1 Probabilities
first result occurs k1 times, the second k2 times, and so on. Here k1, . . . , km are some nonnegative integers
with k1 + ‚ãÖ‚ãÖ‚ãÖ+ km = n.
1.4.5 Poisson distribution
The sample space for this distribution is ‚Ñï0 = {0, 1, 2, . . . }. Furthermore, Œª > 0 is a given
parameter.
Proposition 1.4.27. There exists a unique probability measure PoisŒª on ùí´(‚Ñï0) such that
PoisŒª({k}) = Œªk
k! e‚àíŒª ,
k ‚àà‚Ñï0 .
(1.25)
Proof. Because of e‚àíŒª > 0, PoisŒª({k}) > 0 follows. Thus it suffices to verify
‚àû
‚àë
k=0
Œªk
k! e‚àíŒª = 1 .
But this is a direct consequence of
‚àû
‚àë
k=0
Œªk
k! e‚àíŒª = e‚àíŒª
‚àû
‚àë
k=0
Œªk
k! = e‚àíŒª eŒª = 1 .
Definition 1.4.28. The probability measure PoisŒª on ùí´(‚Ñï0) satisfying Eq. (1.25) is called the Poisson
distribution with parameter Œª > 0.
Compare Figure 1.5 for the values of the Poison distribution in the case Œª = 5. Most likely
are the events k = 4 and k = 5.
Figure 1.5: The values Pois5({k}), k = 0, . . . , 15.

1.4 Special discrete probability measures
‡±™
35
The Poisson distribution describes experiments where the number of trials is big, but
the single success probability is small. More precisely, the following limit theorem holds.
Proposition 1.4.29 (Poisson‚Äôs limit theorem). Let (pn)‚àû
n=1 be a sequence of numbers with
0 < pn ‚â§1 and
lim
n‚Üí‚àûn pn = Œª
for some Œª > 0. Then for all k ‚àà‚Ñï0,
lim
n‚Üí‚àûBn,pn({k}) = PoisŒª({k}) .
Proof. Write
Bn,pn({k}) = (n
k) pk
n(1 ‚àípn)n‚àík
= 1
k!
n (n ‚àí1) ‚ãÖ‚ãÖ‚ãÖ(n ‚àík + 1)
nk
(n pn)k (1 ‚àípn)n (1 ‚àípn)‚àík
= 1
k![ n
n ‚ãÖn ‚àí1
n
‚ãÖ‚ãÖ‚ãÖn ‚àík + 1
n
] (n pn)k (1 ‚àípn)n (1 ‚àípn)‚àík ,
and investigate the behavior of the different parts of the last equation separately. Each
fraction in the left-hand brackets tends to 1, hence the whole factor in brackets tends
to 1. By assumption, we have n pn ‚ÜíŒª, thus, limn‚Üí‚àû(n pn)k = Œªk. Moreover, because of
n pn ‚ÜíŒª with Œª > 0, we get pn ‚Üí0, which implies limn‚Üí‚àû(1 ‚àípn)‚àík = 1.
Thus, it remains to determine the behavior of (1 ‚àípn)n as n ‚Üí‚àû. Proposition A.5.1
asserts that if a sequence of real numbers (xn)n‚â•1 converges to x ‚àà‚Ñù, then
lim
n‚Üí‚àû(1 + xn
n )
n
= ex .
Setting xn := ‚àínpn, by assumption xn ‚Üí‚àíŒª, hence
lim
n‚Üí‚àû(1 ‚àípn)n = lim
n‚Üí‚àû(1 + xn
n )
n
= e‚àíŒª .
If we combine all the different parts, then this completes the proof due to
lim
n‚Üí‚àûBn,pn({k}) = 1
k! Œªk e‚àíŒª = PoisŒª({k}) .
The previous theorem allows two conclusions:
(1) Whenever n is large and p is small, without hesitation one may replace Bn,p by PoisŒª,
where Œª = n ‚ãÖp. In this way, one avoids the (sometimes) difficult evaluation of the bino-
mial coefficients.

36
‡±™
1 Probabilities
Example 1.4.30. In Example 1.4.13, we found the probability that among N students
there are at least two having their birthday on April 1. We then used the binomial distri-
bution with parameters N and p = 1/365. Hence the approximating Poisson distribution
has parameter Œª = N/365 and the corresponding probability is given by
PoisŒª({2, 3, . . .}) = 1 ‚àí(1 + Œª)e‚àíŒª = 1 ‚àí(1 + N
365) e‚àíN/365 .
If again N = 500, hence Œª = 500/365, the approximate probability equals 0.397719. Com-
pare this value with the ‚Äúprecise‚Äù probability 0.397895 obtained in Example 1.4.13.
(2) Poisson‚Äôs limit theorem explains why the Poisson distribution describes experiments
with many trials and small success probability. For example, if we look for a model for
the number of car accidents per year, then the Poisson distribution is a good choice.
There are many cars, but the probability10 that a single driver is involved in an accident
is quite small.
Similarly, the Poisson distribution is used to model the number of customers enter-
ing some shop, to describe the number of phone calls arriving at a call center, or the
number of daily accesses to a website. This is due to the fact that there are many poten-
tial customers but with small probability a single one enters the shop. In the same way,
there are many people possessing a phone, but the probability that a single one calls a
certain center is very small.
Later on we will investigate other examples where the Poisson distribution appears
in a natural way.
Summary: The Poisson distribution PoisŒª occurs as the limit of the binomial distribution Bn,p in the following
sense: the number n oftrialstends toinfinity whileatthe sametime thesuccessprobability p becomessmaller
and smaller. In other words, for large n and small success probability p, it follows that Bn,p(A) ‚âàPoisŒª(A) with
Œª = n p.
1.4.6 Hypergeometric distribution
Among N delivered machines M are defective. One chooses n of the N machines ran-
domly and checks them. What is the probability to observe m defective machines in the
sample of size n?
First note that there are (N
n) ways to choose n machines for checking. In order to
observe m defective ones, these have to be taken from the M defective. The remaining
n‚àím machines are nondefective, hence they must be chosen from the N‚àíM nondefective
ones. There are (M
m) ways to take the defective machines and (N‚àíM
n‚àím) possibilities for the
nondefective ones.
10 To call it a ‚Äúsuccess‚Äù probability in this case is perhaps not quite appropriate.

1.4 Special discrete probability measures
‡±™
37
Thus the following approach describes this experiment:
HN,M,n({m}) :=
(M
m) (N‚àíM
n‚àím)
(N
n)
,
0 ‚â§m ‚â§n .
(1.26)
Recall that in Section A.3.1 we agreed that (n
k) = 0 whenever k > n. This turns out be
useful in the definition of HN,M,n. For example, if m > M, then the probability to observe
m defective machines is, of course, zero.
We want to prove now that eq. (1.26) defines a probability measure.
Proposition 1.4.31. There exists a unique probability measure HN,M,n on the powerset of
{0, . . . , n} satisfying eq. (1.26).
Proof. Vandermonde‚Äôs identity (cf. Proposition A.3.9) asserts that for all k, m, and n
in ‚Ñï0,
k
‚àë
j=0
(n
j )( m
k ‚àíj) = (n + m
k
) .
(1.27)
Now replace n by M, next m by N ‚àíM, then k by n, and, finally, j by m. Doing so, eq. (1.27)
leads to
n
‚àë
m=0
(M
m)(N ‚àíM
n ‚àím ) = (N
n) .
But this implies
n
‚àë
m=0
HN,M,n({m}) =
1
(N
n) ‚ãÖ
n
‚àë
m=0
(M
m)(N ‚àíM
n ‚àím ) =
1
(N
n) ‚ãÖ(N
n) = 1 .
Clearly, HN,M,n({m}) ‚â•0, which completes the proof by virtue of Proposition 1.3.2.
Definition 1.4.32. The probability measure HN,M,n defined by eq. (1.26) is called the hypergeometric dis-
tribution with parameters N, M, and n.
Example 1.4.33. A retailer gets a delivery of 100 machines; 10 of them are defective. He
chooses 8 machines at random and tests them. Find the probability that 2 or more of the
tested machines are defective.
Answer: The desired probability is
8
‚àë
m=2
(10
m)( 90
8‚àím)
(100
8 )
= 0.18195 .
See Figure 1.6 for another example of a hypergeometric distribution. There are 40
defective machines among 100 and one checks a sample of size 15.

38
‡±™
1 Probabilities
Figure 1.6: Probabilities H100,40,15({m}) with m = 0, . . . , 15. The maximal value is about 0.224 attained at
m = 6. That is, if there are 40 defective items among 100, most likely in a sample of 15 there are 6 defective
ones.
Remark 1.4.34. In the daily practice, the opposite question is more important. The size
N of the delivery is known and, of course, also the size of the tested sample. The number
M of defective machines is unknown. Now suppose we observed m defective machines
among the n tested. Does this (random) number m lead to some information about the
number M of defective machines in the delivery? We will investigate this problem in
Proposition 8.5.16.
Example 1.4.35. In a pond there are 200 fish. One day the owner of the pond catches
20 fish, marks them, and puts them back into the pond. After a while the owner catches
once more 20 fish. Find the probability that among these fish there is exactly one
marked.
Answer: We have N = 200, M = 20, and n = 20. Hence the desired probability is
H200,20,20({1}) =
(20
1 )(180
19 )
(200
20 )
= 0.26967 .
Remark 1.4.36. The previous example is not very realistic because in general the num-
ber N of fish is unknown. Known are M and n, the (random) number m was observed.
Also here one may ask whether the knowledge of m leads to some information about N.
This question will be investigated later in Proposition 8.5.18.
Example 1.4.37. In a lottery, 6 numbers are chosen randomly out of 49. Suppose we
bought a lottery coupon with six numbers. What is the probability that exactly k,
k = 0, . . . , 6, of our numbers appear in the drawing?
Answer: There are n = 6 numbers randomly chosen out of N = 49. Among the 49
numbers, M = 6 are ‚Äúdefective.‚Äù These are the six numbers on our coupon, and we ask

1.4 Special discrete probability measures
‡±™
39
for the probability that k of the ‚Äúdefective‚Äù are among the chosen six. The question is
answered by the hypergeometric distribution H49,6,6, that is, the probability of k correct
numbers on our coupon is given by
H49,6,6({k}) =
(6
k)( 43
6‚àík)
(49
6 )
,
k = 0, . . . , 6 .
The numerical values of these probabilities for k = 0, . . . , 6 are
k
H49,6,6({k})
0
0.435965
1
0.413019
2
0.132378
3
0.0176504
4
0.00096862
5
0.0000184499
6
7.15112 ‚ãÖ10‚àí8
Remark 1.4.38. Another model for the hypergeometric distribution is as follows: in an
urn there are N balls, M of them are white, the remaining N ‚àíM are red. Choose n balls
out of the urn without replacing the chosen ones. Then HN,M,n({m}) is the probability to
observe m white balls among the n chosen.
If we do the same experiment, but now replacing the chosen balls, then this is de-
scribed by the binomial distribution. The success probability for a white ball is p = M/N,
hence now the probability for m white balls is given by
Bn,M/N({m}) = (n
m)(M
N )
m
(1 ‚àíM
N )
n‚àím
.
It is intuitively clear that for large N and M (and comparably small n) the difference
between both models (replacing and nonreplacing) is insignificant. Imagine there are
106 white and also 106 red balls in an urn. When choosing two balls, it does not matter
a lot whether the first ball was replaced or not.
The next proposition makes the previous observation more precise.
Proposition 1.4.39. If 0 ‚â§m ‚â§n and 0 ‚â§p ‚â§1, then
lim
N,M‚Üí‚àû
M/N‚Üíp
HN,M,n({m}) = Bn,p({m}) .
Proof. Suppose first 0 < p < 1. Then the definition of the hypergeometric distribution
yields

40
‡±™
1 Probabilities
lim
N,M‚Üí‚àû
M/N‚Üíp
HN,M,n({m}) = lim
N,M‚Üí‚àû
M/N‚Üíp
M‚ãÖ‚ãÖ‚ãÖ(M‚àím+1)
m!
(N‚àíM)‚ãÖ‚ãÖ‚ãÖ(N‚àíM‚àí(n‚àím)+1)
(n‚àím)!
N(N‚àí1)‚ãÖ‚ãÖ‚ãÖ(N‚àín+1)
n!
= lim
N,M‚Üí‚àû
M/N‚Üíp
(n
m)
[ M
N ‚ãÖ‚ãÖ‚ãÖ( M‚àím+1
N
)][(1 ‚àíM
N ) ‚ãÖ‚ãÖ‚ãÖ(1 ‚àíM‚àí(n‚àím)+1
N
)]
(1 ‚àí1
N ) ‚ãÖ‚ãÖ‚ãÖ(1 ‚àín+1
N )
= (n
m)pm(1 ‚àíp)n‚àím = Bn,p({m}) .
(1.28)
Note that if either m = 0 or m = n, then the first or the second brackets in the second
line of eq. (1.28) become 1, thus they do not appear.
The cases p = 0 and p = 1 have to be treated separately. For example, if p = 0, the
fraction in the second line of eq. (1.28) converges to zero provided that m ‚â•1. If m = 0,
then
lim
N,M‚Üí‚àû
M/N‚Üí0
(1 ‚àíM
N ) ‚ãÖ‚ãÖ‚ãÖ(1 ‚àíM‚àín+1
N
)
(1 ‚àí1
N ) ‚ãÖ‚ãÖ‚ãÖ(1 ‚àín+1
N )
= 1 = Bn,0({0}) .
The case p = 1 is treated similarly. Hence, the proposition is also valid in the border
cases.
Example 1.4.40. Suppose there are N = 200 balls in an urn. In the first table, there are
M = 80 white balls. Choosing n = 10 balls with or without replacement, we get the
following numerical values. Note that p = M/N = 2/5. In the second table, we execute
the same experiment, but now there are 100 white balls among 200.
m
H200,80,10({m})
B10,0.4({m})
1
0.0372601
0.0403108
2
0.118268
0.120932
3
0.217696
0.214991
4
0.257321
0.250823
5
0.204067
0.200658
6
0.10995
0.111477
7
0.0397376
0.0424673
8
0.00921879
0.0106168
9
0.0012395
0.00157286
m
H200,100,10({m})
B10,0.5({m})
1
0.00847281
0.00976563
2
0.0410287
0.0439453
3
0.115292
0.117188
4
0.2082
0.205078
5
0.25247
0.246094
6
0.2082
0.205078
7
0.115292
0.117188
8
0.0410287
0.0439453
9
0.00847281
0.00976563
Let us shortly analyze the table on the right. Here 50 % of the balls are white. Draw-
ing 10 balls, it is more likely to observe 4, 5, or 6 white balls in the case of nonreplacement,
while it is the other way around in the remaining cases 1, 2, 3, 7, 8, and 9. Try to find a
heuristic explanation for this phenomenon.

1.4 Special discrete probability measures
‡±™
41
Summary: The hypergeometric distribution may be viewed as a counterpart to the binomial distribution in
the following sense: in an urn there are M white balls and N ‚àíM black. Choosing randomly n balls without
replacement,
HN,M,n({m}) :=
(M
m) (N‚àíM
n‚àím)
(N
n)
is the probability to observe m white balls. In contrast, the binomial distribution Bn,p with p = M/N applies in
the case of replacing the chosen balls. Of course, in this case the number N of balls does not matter, only the
proportion of white balls is of interest.
1.4.7 Geometric distribution
At first glance, the model for the geometric distribution looks as that for the binomial dis-
tribution. In each single trial, we may observe ‚Äú0‚Äù or ‚Äú1‚Äù, that is, failure or success. Again
the success probability is a fixed number p. While in the case of the binomial distribu-
tion we executed a fixed number of trials, now this number is random. More precisely,
we execute the experiment until we observe success for the first time. Recorded is the
number of necessary trials until this first success shows up. Or, in other words, a number
k ‚â•1 occurs if and only if the first k ‚àí1 trials were all failures and the kth one is a suc-
cess, that is, we observe the sequence (0, . . . , 0
‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
k‚àí1
, 1). Since failure appears with probability
1 ‚àíp and success shows up with probability p, the following approach is plausible:
Gp({k}) := p (1 ‚àíp)k‚àí1 ,
k ‚àà‚Ñï.
(1.29)
Proposition 1.4.41. If 0 < p < 1, then (1.29) defines a probability measure on ùí´(‚Ñï).
Proof. Because of p (1‚àíp)k‚àí1 > 0, it suffices to verify ‚àë‚àû
k=1 Gp({k}) = 1. Using the formula
for the sum of a geometric series, this follows directly from
‚àû
‚àë
k=1
p (1 ‚àíp)k‚àí1 = p
‚àû
‚àë
k=0
(1 ‚àíp)k = p
1
1 ‚àí(1 ‚àíp) = 1 .
Observe that by assumption 1 ‚àíp < 1, thus the formula for the geometric series
applies.
Definition 1.4.42. The probability measure Gp on ùí´(‚Ñï) defined by Eq. (1.29) is called the geometric
distribution with parameter p.
If p = 0, then success will never show up, thus, Gp is not a probability measure. On
the other hand, for p = 1, success appears with probability one in the first trial, that is,
Gp = Œ¥1. Therefore, this case is of no interest.

42
‡±™
1 Probabilities
Remark 1.4.43. Some authors define the geometric distribution in a slightly different
way. They ask for the probability for the first success in the (k+1)th trial. This is described
by
ÃÉGp({k}) := p (1 ‚àíp)k ,
k ‚àà‚Ñï0 .
To our opinion, the shift by 1 is a little bit confusing. Therefore, we decided to define the
geometric distribution as we did in eq. (1.29).
Example 1.4.44. Given a number n ‚àà‚Ñï, let An = {k ‚àà‚Ñï: k > n}. Find Gp(An).
Answer: We answer this question by two different approaches.
At first, we remark that An occurs if and only if the first success shows up strictly
after n trials or, equivalently, if and only if the first n trials were all failures. But this
event has probability Bn,p({0}) = (1 ‚àíp)n, hence Gp(An) = (1 ‚àíp)n.
In the second approach, we use eq. (1.29) directly and obtain
Gp(An) =
‚àû
‚àë
k=n+1
Gp({k}) = p
‚àû
‚àë
k=n+1
(1 ‚àíp)k‚àí1 = p (1 ‚àíp)n
‚àû
‚àë
k=0
(1 ‚àíp)k
= p (1 ‚àíp)n
1
1 ‚àí(1 ‚àíp) = (1 ‚àíp)n .
Example 1.4.45. Roll a die until number ‚Äú6‚Äù occurs for the first time. What is the prob-
ability that this happens in roll k?
Answer: The success probability is 1/6, hence the probability of the first occurrence
of ‚Äú6‚Äù in the kth trial is (1/6)(5/6)k‚àí1.
k
G1/6({k})
1
0.166667
2
0.138889
3
0.115741
...
...
12
0.022431
13
0.018693
Example 1.4.46. Roll a die until the first ‚Äú6‚Äù shows up. What is the probability that this
happens at an even number of trials?
Answer: The first ‚Äú6‚Äù has to appear in the second, or fourth, or sixth, and so on, trial.
Hence, the probability of this event is
‚àû
‚àë
k=1
G1/6({2k}) = 1
6
‚àû
‚àë
k=1
(5/6)2k‚àí1 = 5
36
‚àû
‚àë
k=1
(5/6)2k‚àí2 = 5
36
1
1 ‚àí(5/6)2 = 5
11 .

1.4 Special discrete probability measures
‡±™
43
Example 1.4.47. Play a series of games where p is the chance of winning. Whenever
you put x dollars into the pool you get back 2x dollars if you win. If you lose, then the x
dollars are lost.
Apply the following strategy. After losing, double the amount in the pool in the next
game. Say you start with $1 and lose, then next time put $2 into the pool, then $4, and
so on until you win for the first time. As easily seen, in the kth game, the stakes are 2k‚àí1
dollars.
Suppose for some k ‚â•1 you lost k ‚àí1 games and won the kth one. How much money
did you lose? If k = 1, then you lost nothing, while for k ‚â•2 you spent
1 + 2 + 4 + ‚ãÖ‚ãÖ‚ãÖ+ 2k‚àí2 = 2k‚àí1 ‚àí1
dollars. Note that 2k‚àí1 ‚àí1 = 0 if k = 1, hence for all k ‚â•1 the total loss is 2k‚àí1 ‚àí1 dollars.
On the other hand, if you win the kth game, you gain 2k‚àí1 dollars. Consequently, no
matter what the results are, you will always win 2k‚àí1 ‚àí(2k‚àí1 ‚àí1) = 1 dollar.11
Let X be the amount of money needed to follow this strategy. One needs 1 dollar to
play the first game, 1 + 2 = 3 dollars to play the second, until
1 + 2 + 4 + ‚ãÖ‚ãÖ‚ãÖ+ 2k‚àí1 = 2k ‚àí1
to play the kth game. Thus, in this case we have X = 2k ‚àí1 and
‚Ñô{X = 2k ‚àí1} = ‚Ñô{first win in game k} = p(1 ‚àíp)k‚àí1 ,
k = 1, 2, . . .
In particular, if p = 1/2 then this probability equals 2‚àík. For example, if one starts the
game with 127 = 27 ‚àí1 dollars in the pocket, then one goes bankrupt if the first success
appears after game 7. The probability for this equals ‚àë‚àû
k=8 2‚àík = 2‚àí7 = 0.0078125.
1.4.8 Negative binomial distribution
The geometric distribution describes the probability for having the first success in trial
k. Given a fixed n ‚â•1, we ask now for the probability that in trial k success appears not
for the first but for the nth time. Of course, this question makes only sense if k ‚â•n. But
how to determine this probability for those k?
Thus, take k ‚â•n and suppose we had success in trial k. When is this the nth one? This
is the case if and only if we had n‚àí1 successes during the first k ‚àí1 trials or, equivalently,
k ‚àín failures. There exist (k‚àí1
k‚àín) possibilities to distribute the k ‚àín failures among the
first k ‚àí1 trials. Furthermore, the probability for n successes is pn and for k ‚àín failures
11 Starting the first game with x dollars, one will always win x dollars no matter what happens.

44
‡±™
1 Probabilities
it is (1 ‚àíp)k‚àín, hence the probability for observing the nth success in trial k is given by
B‚àí
n,p({k}) := (k ‚àí1
k ‚àín)pn (1 ‚àíp)k‚àín ,
k = n, n + 1, . . .
(1.30)
We still have to verify that there is a probability measure satisfying eq. (1.30).
Proposition 1.4.48. By
B‚àí
n,p({k}) = (k ‚àí1
k ‚àín)pn (1 ‚àíp)k‚àín ,
k = n, n + 1, . . . ,
a probability measure B‚àí
n,p on ùí´({n, n + 1, . . . }) is defined.
Proof. Of course, B‚àí
n,p({k}) ‚â•0. Hence it remains to show
‚àû
‚àë
k=n
B‚àí
n,p({k}) = 1
or, equivalently,
‚àû
‚àë
k=0
B‚àí
n,p({k + n}) = 1 .
(1.31)
Because of Proposition A.3.11, we get
B‚àí
n,p({k + n}) = (n + k ‚àí1
k
) pn (1 ‚àíp)k
= (‚àín
k )pn (‚àí1)k (1 ‚àíp)k = (‚àín
k )pn (p ‚àí1)k ,
(1.32)
where the generalized binomial coefficient is defined in eq. (A.14) as
(‚àín
k ) = ‚àín(‚àín ‚àí1) ‚ãÖ‚ãÖ‚ãÖ(‚àín ‚àík + 1)
k!
.
In Proposition A.5.2, we proved for |x| < 1 that
‚àû
‚àë
k=0
(‚àín
k ) xk =
1
(1 + x)n .
(1.33)
Note that 0 < p < 1, hence eq. (1.33) applies with x = p ‚àí1 and leads to
‚àû
‚àë
k=0
(‚àín
k ) (p ‚àí1)k = 1
pn .
(1.34)
Combining eqs. (1.32) and (1.34) implies
‚àû
‚àë
k=0
B‚àí
n,p({k + n}) = pn
‚àû
‚àë
k=0
(‚àín
k ) (p ‚àí1)k = pn 1
pn = 1 ,
thus the equations in (1.31) are valid and this completes the proof.

1.4 Special discrete probability measures
‡±™
45
Definition 1.4.49. The probability measure B‚àí
n,p with
B‚àí
n,p({k}) := (k ‚àí1
k ‚àín)pn (1 ‚àíp)k‚àín = (k ‚àí1
n ‚àí1)pn (1 ‚àíp)k‚àín,
k = n, n + 1, . . .
is called the negative binomial distribution with parameters n ‚â•1 and p ‚àà(0, 1). Of course, B‚àí
1,p = Gp.
Remark 1.4.50. We saw in eq. (1.32) that
B‚àí
n,p({k + n}) = (n + k ‚àí1
k
)pn (1 ‚àíp)k = (‚àín
k )pn (p ‚àí1)k .
(1.35)
Alternatively, one may define the negative binomial distribution also via Eq. (1.35). Then
it describes the event that the nth success appears in trial n + k. The advantage of this
approach is that now k ‚àà‚Ñï0, that is, the restriction k ‚â•n is no longer needed. Its
disadvantage is that we are interested in what happens in trial k, not in trial k + n.
Example 1.4.51. Roll a die successively. Determine the probability that in the 20th trial
number ‚Äú6‚Äù appears for the fourth time.
Answer: We have p = 1/6, n = 4, and k = 20. Therefore, the probability for this event
is given by
B‚àí
4,1/6({20}) = (19
16) ( 1
6)
4
(5
6)
16
= 0.0404407 .
Let us ask now for the probability that the fourth success appears (strictly) before trial
21. This probability is given by
20
‚àë
k=4
(k ‚àí1
3 )( 1
6)
4
(5
6)
k‚àí4
= 0.433454 .
Example 1.4.52. There are two urns, say U0 and U1, each containing N balls. Choose one
of the two urns at random and take out a ball. Hereby U0 is chosen with probability 1‚àíp,
hence U1 with probability p. Repeat the procedure until we choose the last (the Nth) ball
out of one of the urns. What is the probability that there are m balls left in the other urn,
where m = 1, . . . , N?
Answer: For m = 1, . . . , N let Am be the event that there are still m balls in one of the
urns when choosing the last ball out of the other. Then Am splits into the disjoint events
Am = A0
m ‚à™A1
m, where
‚Äì
A0
m occurs if we take the last ball out of U0 and U1 contains m balls, and
‚Äì
A1
m occurs choosing the Nth ball out of U1 with m balls remaining in U0.
Let us start with evaluating the probability of A1
m. Say success occurs if we choose urn U1.
Thus, if we take out the last ball of urn U1, then success occurred for the Nth time. On
the other hand, if there are still m balls in U0, then failure had occurred N ‚àím times.

46
‡±™
1 Probabilities
Consequently, there are still m balls left in urn U0 if and only if the Nth success shows
up in trial N + (N ‚àím) = 2N ‚àím. Therefore, we get
‚Ñô(A1
m) = B‚àí
N,p({2N ‚àím}) = (2N ‚àím ‚àí1
N ‚àím
) pN (1 ‚àíp)N‚àím .
(1.36)
The probability of A0
m may be derived from that of A1
m by interchanging p and 1 ‚àíp
(success occurs now with probability 1 ‚àíp). This yields
‚Ñô(A0
m) = B‚àí
N,1‚àíp({2N ‚àím}) = (2N ‚àím ‚àí1
N ‚àím
) pN‚àím (1 ‚àíp)N .
(1.37)
Adding eqs. (1.36) and (1.37) leads to
‚Ñô(Am) = (2N ‚àím ‚àí1
N ‚àím
)[pN (1 ‚àíp)N‚àím + pN‚àím (1 ‚àíp)N] ,
for m = 1, . . . , N.
If p = 1/2, that is, both urns are equally likely, the previous formula simplifies to
‚Ñô(Am) = (2N ‚àím ‚àí1
N ‚àím
) 2‚àí2N+m+1 = (2N ‚àím ‚àí1
N ‚àí1
) 2‚àí2N+m+1 .
(1.38)
Remark 1.4.53. The case p = 1/2 in the previous problem is known as Banach‚Äôs match-
box problem. In each of two matchboxes, there are N matches. One chooses randomly a
matchbox (both boxes are equally likely) and takes out a match. What is the probability
that there are still m matches left in the other box when taking the last match out of one
of the boxes? The answer is given by eq. (1.38).
Remark 1.4.54. There exists a slightly different version of Banach‚Äôs matchbox problem.
Here one asks for the probability of the event ÃÉAm which occurs provided that there are
m matches left in one of the boxes when choosing for the first time an empty one. Note
that in this setting also m = 0 makes sense. To answer this modified problem, one has to
ask in the previous calculations for the (N +1)th success instead of the Nth one. In doing
so, one obtains
‚Ñô( ÃÉAm) = (2N ‚àím
N ‚àím ) 2‚àí2N+m = (2N ‚àím
N
) 2‚àí2N+m ,
m = 0, . . . , N .
See Figure 1.7 for the values of these probabilities in the case N = 20.
What is more likely at the moment when choosing for the first time an empty box:
the unchosen box contains one match or it contains none?
The answer is that both events are equally likely. This follows from
‚Ñô( ÃÉA0) = (2N
N ) 2‚àí2N = (2N)!
(N!)2 2‚àí2N = 2N
N
(2N ‚àí1)!
N!(N ‚àí1)! 2‚àí2N
= (2N ‚àí1
N ‚àí1 ) 2‚àí2N+1 = ‚Ñô( ÃÉA1) .

1.4 Special discrete probability measures
‡±™
47
Figure 1.7: Probabilities for 0 ‚â§m ‚â§20 matches left in one box when choosing for the first time an empty
one. At the beginning, both boxes contained N = 20 matches.
Example 1.4.55. We continue Example 1.4.52 with 0 < p < 1 and ask the following ques-
tion: What is the probability that U1 becomes empty before U0?
Answer: This happens if and only if U0 is nonempty when choosing U1 for the Nth
time, that is, when in U0 there are m balls left for some m = 1, . . . , N. Because of eq. (1.36),
this probability is given by
N
‚àë
m=1
‚Ñô(A1
m) = pN
N
‚àë
m=1
(2N ‚àím ‚àí1
N ‚àím
) (1 ‚àíp)N‚àím
= pN
N‚àí1
‚àë
k=0
(N + k ‚àí1
k
) (1 ‚àíp)k .
(1.39)
Remark 1.4.56. Formula (1.39) leads to an interesting (known) property of the binomial
coefficients. Since ‚àëN
m=1 ‚Ñô(Am) = 1 , by eqs. (1.39) and (1.37), we obtain
N‚àí1
‚àë
k=0
(N + k ‚àí1
k
)[pN (1 ‚àíp)k + (1 ‚àíp)N pk] = 1
or, setting n = N ‚àí1, to
n
‚àë
k=0
(n + k
k
)[pn+1 (1 ‚àíp)k + (1 ‚àíp)n+1 pk] = 1 .
In particular, if p = 1/2, this yields
n
‚àë
k=0
(n + k
k
) 1
2k = 2n ,
n = 0, 1, 2, . . .

48
‡±™
1 Probabilities
Summary: One executes independently arbitrarily many experiments where either success or failure may
occur. Hereby, the success probability equals 0 < p < 1. The probability to observe the nth success in trial
k ‚â•n is given by the negative binomial distribution B‚àí
n,p defined by
B‚àí
n,p({k}) := (k ‚àí1
k ‚àín)pn (1 ‚àíp)k‚àín = (k ‚àí1
n ‚àí1)pn (1 ‚àíp)k‚àín ,
k = n, n + 1, . . .
Equivalently, when one asks for the nth success in trial n + ‚Ñì, then
B‚àí
n,p({n + ‚Ñì}) = (‚àín
‚Ñì)pn (p ‚àí1)‚Ñì,
‚Ñì= 0, 1, 2, . . .
If n = 1, that is, one looks for the first success, then Gp = B‚àí
1,p is called the geometric distribution, and
Gp({k}) = p (1 ‚àíp)k‚àí1 ,
k = 1, 2, . . .
1.5 Continuous probability measures
Discrete probability measures are inappropriate for the description of random experi-
ments where uncountably many different results may appear. Typical examples of such
experiments are the lifetime of an item, the duration of a phone call, the measuring
result of workpiece, and so on.
Discrete probability measures are concentrated on a finite or countably infinite set
of points. An extension to larger sets is impossible. For example, there is no12 probability
measure ‚Ñôon [0, 1] with ‚Ñô({t}) > 0 for t ‚àà[0, 1].
Consequently, in order to describe random experiments with ‚Äúmany‚Äù possible dif-
ferent outcomes, another approach is needed. To explain this ‚Äúnew‚Äù approach, let us
shortly recall how we evaluated ‚Ñô(A) in the discrete case. If Œ© is either finite or count-
ably infinite and if p(œâ) = ‚Ñô({œâ}), œâ ‚ààŒ©, then with this p : Œ© ‚Üí‚Ñùwe have
‚Ñô(A) = ‚àë
œâ‚ààA
p(œâ) .
(1.40)
If the sample space is ‚Ñùor ‚Ñùn, then such a representation is no longer possible. Indeed, if
‚Ñôis not discrete, then, we will have p(œâ) = 0 for all possible observations œâ. Therefore,
the sum in eq. (1.40) has to be replaced by an integral over a more general function.
We start with introducing functions p, which may be used for representing ‚Ñô(A) via an
integral.
12 Compare with Problem 1.38.

1.5 Continuous probability measures
‡±™
49
Definition 1.5.1. A Riemann integrable function p : ‚Ñù‚Üí‚Ñùis called a probability density function, or
simply a density function, if
p(x) ‚â•0 , x ‚àà‚Ñù,
and
‚àû
‚à´
‚àí‚àû
p(x) dx = 1 .
(1.41)
Remark 1.5.2. Let us formulate more precisely the second condition for p in the previ-
ous definition. For all finite intervals [a, b] in ‚Ñù, the function p is Riemann integrable on
[a, b] and, moreover,
lim
a‚Üí‚àí‚àû
b‚Üí‚àû
b
‚à´
a
p(x) dx = 1 .
The density functions we will use later on are either continuous or piecewise continu-
ous, that is, they are compositions of finitely many continuous functions. These functions
are Riemann integrable, hence in this case it remains to verify the two conditions (1.41).
Example 1.5.3. Define p on ‚Ñùby p(x) = 0 if x > 0 and by p(x) = e‚àíx if x ‚â•0. Then p is
piecewise continuous, p(x) ‚â•0 if x ‚àà‚Ñù, and it satisfies
‚àû
‚à´
‚àí‚àû
p(x)dx = lim
b‚Üí‚àû
b
‚à´
0
e‚àíx dx = lim
b‚Üí‚àû[‚àíe‚àíx]
b
0 = 1 ‚àílim
b‚Üí‚àûe‚àíb = 1 .
Hence, p is a density function.
Definition 1.5.4. Let p be a probability density function. Given a finite interval [a, b], its probability (of
occurrence) is defined by
‚Ñô([a, b]) :=
b
‚à´
a
p(x)dx .
A graphical presentation of the previous definition is as follows. As visualized in Fig-
ure 1.8, the probability ‚Ñô([a, b]) is the area under the graph of the density p, taken from
a to b.
Let us illustrate Definition 1.5.4 with the density function regarded in Example 1.5.3.
Then
‚Ñô([a, b]) =
b
‚à´
a
e‚àíxdx = [‚àíe‚àíx]
b
a = e‚àía ‚àíe‚àíb
whenever 0 ‚â§a < b < ‚àû. On the other hand, if a < b < 0, then ‚Ñô([a, b]) = 0 while for
a < 0 ‚â§b the probability of [a, b] is calculated by
‚Ñô([a, b]) = ‚Ñô([0, b]) = 1 ‚àíe‚àíb .

50
‡±™
1 Probabilities
Figure 1.8: The size of the gray shaded area defines the probability of the interval [a, b].
Remark 1.5.5. Definition 1.5.4 of the probability measure ‚Ñôdoes not fit into the scheme
presented in Section 1.1.3. Why? Probability measures are defined on œÉ-fields. But the
collection of finite intervals in ‚Ñùis not a œÉ-field. It is neither closed under taking com-
plements nor is the union of intervals in general again an interval. Furthermore, it is far
from being clear in which sense ‚Ñôshould be œÉ-additive.
The next result justifies the approach in Definition 1.5.4. Its proof rests upon an ex-
tension theorem in Measure Theory (cf. [Bau01, Coh13] or [Dud02]).
Proposition 1.5.6. Let ‚Ñ¨(‚Ñù) be the œÉ-field of Borel sets introduced in Definition 1.1.20.
Then for each density function p, there exists a unique probability measure ‚Ñôon ‚Ñ¨(‚Ñù)
such that
‚Ñô([a, b]) =
b
‚à´
a
p(x) dx
for all a < b .
(1.42)
Definition 1.5.7. A probability measure ‚Ñôon ‚Ñ¨(‚Ñù) is said to be continuous provided that there exists a
density function p such that for a < b,
‚Ñô([a, b]) =
b
‚à´
a
p(x) dx .
(1.43)
The function p is called a density function, or simply density, of ‚Ñô.
Remark 1.5.8. The mathematically correct name would be ‚Äúabsolutely continuous‚Äù. But
since we do not treat so-called ‚Äúsingularly continuous‚Äù probability measures, there is
no need to distinguish between them, and we may shorten the notation to ‚Äúcontinu-
ous.‚Äù
Remark 1.5.9. Note that changing the density function at finitely many points does not
change the generated probability measure. For instance, if we define p(x) = 0 if x ‚â§0
and p(x) = e‚àíx if x > 0, then this density function is different from that in Example 1.5.3
but, of course, generates the same probability measure.

1.5 Continuous probability measures
‡±™
51
Moreover, observe that eq. (1.43) is valid for all a < b if and only if for each t ‚àà‚Ñù,
‚Ñô((‚àí‚àû, t]) =
t
‚à´
‚àí‚àû
p(x) dx .
(1.44)
Consequently, ‚Ñôis continuous if and only if there is a density p with eq. (1.44) for t ‚àà‚Ñù.
Proposition 1.5.10. Let ‚Ñô: ‚Ñ¨(‚Ñù) ‚Üí[0, 1] be a continuous probability measure with den-
sity p. Then the following are valid:
1.
‚Ñô(‚Ñù) = 1.
2.
For each t ‚àà‚Ñù, one has ‚Ñô({t}) = 0. More generally, if A ‚äÜ‚Ñùis either finite or countably
infinite, then ‚Ñô(A) = 0.
3.
For all a < b, we have
‚Ñô((a, b)) = ‚Ñô((a, b]) = ‚Ñô([a, b)) = ‚Ñô([a, b]) =
b
‚à´
a
p(x) dx .
Proof. Let us start with proving ‚Ñô(‚Ñù) = 1. For n ‚â•1, set An := [‚àín, n] and note that
A1 ‚äÜA2 ‚äÜ‚ãÖ‚ãÖ‚ãÖ, as well as ‚ãÉ‚àû
n=1 An = ‚Ñù. Thus we may use that ‚Ñôis continuous from below
and, by the properties of the density p, we obtain
‚Ñô(‚Ñù) = lim
n‚Üí‚àû‚Ñô(An) = lim
n‚Üí‚àû
n
‚à´
‚àín
p(x) dx =
‚àû
‚à´
‚àí‚àû
p(x) dx = 1 .
To verify the second property, fix t ‚àà‚Ñùand define for each n ‚â•1 the intervals Bn by
Bn := [t, t + 1
n]. Now we have B1 ‚äáB2 ‚äá‚ãÖ‚ãÖ‚ãÖand ‚ãÇ‚àû
n=1 Bn = {t}. Use this time the continuity
from above. Then we get
‚Ñô({t}) = lim
n‚Üí‚àû‚Ñô(Bn) = lim
n‚Üí‚àû
t+ 1
n
‚à´
t
p(x) dx = 0 .
If A = {t1, t2, . . .}, then the œÉ-additivity of ‚Ñô, together with ‚Ñô({tj}) = 0, gives
‚Ñô(A) =
‚àû
‚àë
j=1
‚Ñô({tj}) = 0,
as asserted.
The third property is an immediate consequence of the second. Observe
[a, b] = (a, b) ‚à™{a} ‚à™{b} ,
hence ‚Ñô([a, b]) = ‚Ñô((a, b))+‚Ñô({a})+‚Ñô({b}), proving (1.43) due to ‚Ñô({a}) = ‚Ñô({b}) = 0.

52
‡±™
1 Probabilities
Remark 1.5.11. Say a set C ‚äÜ‚Ñùcan be represented as C = ‚ãÉ‚àû
j=1 Ij with disjoint (open or
half-open or closed) intervals Ij, then
‚Ñô(C) =
‚àû
‚àë
j=1
‚à´
Ij
p(x) dx := ‚à´
C
p(x) dx .
More generally, if a set B may be written as B = ‚ãÇ‚àû
n=1 Cn where the Cns are unions of
disjoint intervals and satisfy C1 ‚äáC2 ‚äá‚ãÖ‚ãÖ‚ãÖ, then
‚Ñô(B) = lim
n‚Üí‚àû‚Ñô(Cn) .
In this way, one may evaluate ‚Ñô(B) for a large class of subsets B ‚äÜ‚Ñù.
Summary: There are two completely different types of probability measures:
[‚Ñôdiscrete
‚áî
‚Ñô([a, b]) =
‚àë
a‚â§œâ‚â§b
‚Ñô({œâ})]
and
[
[
‚Ñôcontinuous
‚áî
‚Ñô([a, b]) =
b
‚à´
a
p(x) dx]
]
.
1.6 Special continuous distributions
1.6.1 Uniform distribution on an interval
Let I = [Œ±, Œ≤] be a finite interval of real numbers. Define a function p : ‚Ñù‚Üí‚Ñùby
p(x) := {
1
Œ≤‚àíŒ±
if x ‚àà[Œ±, Œ≤],
0
if x ‚àâ[Œ±, Œ≤].
(1.45)
Proposition 1.6.1. The mapping p defined by eq. (1.45) is a probability density function.
Proof. Note that p is piecewise continuous, hence Riemann integrable. Moreover,
p(x) ‚â•0 for x ‚àà‚Ñùand
‚àû
‚à´
‚àí‚àû
p(x) dx =
Œ≤
‚à´
Œ±
1
Œ≤ ‚àíŒ± dx =
1
Œ≤ ‚àíŒ±(Œ≤ ‚àíŒ±) = 1 .
Consequently, p is a probability density.
Definition 1.6.2. The probability measure ‚Ñôgenerated by the density in eq. (1.45) is called the uniform
distribution on the interval I = [Œ±, Œ≤].

1.6 Special continuous distributions
‡±™
53
How is ‚Ñô([a, b]) evaluated for some interval [a, b]? Let us first treat the case that
[a, b] ‚äÜI. Then
‚Ñô([a, b]) =
b
‚à´
a
1
Œ≤ ‚àíŒ± dx = b ‚àía
Œ≤ ‚àíŒ± = Length of [a, b]
Length of [Œ±, Œ≤] .
(1.46)
This explains why ‚Ñôis called the ‚Äúuniform distribution.‚Äù The probability of an interval
[a, b] ‚äÜI depends only on its length, not on its position. Shifting [a, b] inside I does not
change its probability of occurrence (compare Figure 1.9).
Figure 1.9: If ‚Ñôis the uniform distribution on [Œ±, Œ≤], then ‚Ñô([a, b]) = ‚Ñô([a‚Ä≤, b‚Ä≤]).
If [a, b] is arbitrary, not necessarily contained in I, then ‚Ñô([a, b]) can be easily cal-
culated by
‚Ñô([a, b]) = ‚Ñô([a, b] ‚à©I) .
Example 1.6.3. Let ‚Ñôbe the uniform distribution on [0, 1]. Which probabilities do
[‚àí1, 0.5], [0, 0.25] ‚à™[0.75, 1], (‚àí‚àû, t] if t ‚àà‚Ñù, and A ‚äÜ‚Ñù, where A = ‚ãÉ‚àû
n=1[
1
2n+1/2 , 1
2n ], have?
Answer: The first two intervals have probability 1
2. If t ‚àà‚Ñù, then
‚Ñô((‚àí‚àû, t]) =
{
{
{
{
{
{
{
0
if t < 0,
t
if 0 ‚â§t ‚â§1,
1
if t > 1.
Finally, observe that the intervals [
1
2n+1/2 , 1
2n ] are disjoint subsets of [0, 1]. Hence we get
‚Ñô(A) =
‚àû
‚àë
n=1
[ 1
2n ‚àí
1
2n+1/2 ] = (1 ‚àí2‚àí1/2)
‚àû
‚àë
n=1
1
2n = 1 ‚àí2‚àí1/2.
Example 1.6.4. A stick of length L is randomly broken into two pieces. Find the proba-
bility that the size of one piece is at least twice that of the other.
Answer: This event happens if and only if the point at which the stick is broken is
either in [0, L/3] or in [2L/3, L]. Assuming that the point at which the stick is broken is
uniformly distributed on [0, L], the desired probability is 2
3. Another way to get this result
is as follows. The size of each piece is less than twice as that of the other if the point at

54
‡±™
1 Probabilities
which the stick is broken is in [L/3, 2L/3]. Hence, the probability of the complementary
event is 1/3, leading again to 2/3 for the desired probability.
Example 1.6.5. Choose at random a real number uniformly distributed in [‚àí1, 1]. How
likely is it that its square is less than 1/4?
Answer: A number x ‚àà[‚àí1, 1] satisfies x2 ‚â§1/4 if and only if ‚àí1/2 ‚â§x ‚â§1/2. Hence,
if A is the event that the square is less than 1/4, then
‚Ñô(A) =
Length of [‚àí1
2, 1
2]
Length of [‚àí1, 1] = 1
2 .
Example 1.6.6. Let C0 := [0, 1]. Extract from C0 the interval ( 1
3, 2
3), thus there remains
C1 = [0, 1
3] ‚à™[ 2
3, 1]. To construct C2, extract from C1 the two middle intervals ( 1
9, 2
9) and
( 7
9, 8
9), hence C2 = [0, 1
9] ‚à™[ 2
9, 1
3] ‚à™[ 2
3, 7
9] ‚à™[ 8
9, 1].
Suppose that through this method we already got the set Cn which is a union of
2n disjoint closed intervals of length 3‚àín. In order to construct Cn+1, split each of the 2n
intervals into three intervals of length 3‚àín‚àí1 and erase the middle one. In this way, we
get Cn+1, which consists of 2n+1 disjoint intervals of length 3‚àín‚àí1. Finally, one defines
C =
‚àû
‚ãÇ
n=1
Cn .
The set C is known as the Cantor set. Let ‚Ñôbe the uniform distribution on [0, 1]. Which
value does ‚Ñô(C) have?
Answer: First observe that C0 ‚äÉC1 ‚äÉC2 ‚äÉ‚ãÖ‚ãÖ‚ãÖ, hence, using that ‚Ñôis continuous
from above, it follows that
‚Ñô(C) = lim
n‚Üí‚àû‚Ñô(Cn) .
(1.47)
The set Cn is a disjoint union of 2n intervals of length 3‚àín. Consequently, it follows that
‚Ñô(Cn) = 2n
3n , which by eq. (1.47) implies ‚Ñô(C) = 0.
One might conjecture that C = 0. On the contrary, C is even uncountably infinite. To
see this, we have to make the construction of the Cantor set a little bit more precise.
Given n ‚â•1, let
An = {Œ± = (Œ±1, . . . , Œ±n) : Œ±1, . . . , Œ±n‚àí1 ‚àà{0, 2}, Œ±n = 1} .
If Œ± = (Œ±1, . . . , Œ±n) ‚ààAn, set xŒ± = ‚àën
k=1
Œ±k
3k and IŒ± = (xŒ±, xŒ± + 1
3n ). In this notation,
I(1) = ( 1
3, 2
3),
I(0,1) = ( 1
9, 2
9),
I(2,1) = (7
9, 8
9),
and
I(0,0,1) = ( 1
27, 2
27).
Then, if C0 = [0, 1], for n ‚â•1 we have

1.6 Special continuous distributions
‡±™
55
Cn = Cn‚àí1 \ ‚ãÉ
Œ±‚ààAn
IŒ±,
hence C = [0, 1] \
‚àû
‚ãÉ
n=1
‚ãÉ
Œ±‚ààAn
IŒ± .
Take now any sequence x1, x2, . . . with xk ‚àà{0, 2} and set x = ‚àë‚àû
k=1
xk
3k . Then x cannot
belong to any IŒ± because otherwise at least one of the xks should satisfy xk = 1. Thus
x ‚ààC, and the number of x that may be represented by xks with xk ‚àà{0, 2} is uncountably
infinite.
Summary: The uniform distribution ‚Ñôon an interval I = [Œ±, Œ≤] is characterized by the following property: if
[a, b] ‚äÜI, then
‚Ñô([a, b]) =
b
‚à´
a
1
Œ≤ ‚àíŒ± dx = b ‚àía
Œ≤ ‚àíŒ± = Length of [a, b]
Length of [Œ±, Œ≤] .
1.6.2 Normal distribution
This section is devoted to the most important probability measure, the normal distribu-
tion. Before we can introduce it, we need the following result.
Proposition 1.6.7. We have
‚àû
‚à´
‚àí‚àû
e‚àíx2/2 dx = ‚àö2 œÄ.
(1.48)
Proof. Set
a :=
‚àû
‚à´
‚àí‚àû
e‚àíx2/2 dx
and note that a > 0. Then we get
a2 = (
‚àû
‚à´
‚àí‚àû
e‚àíx2/2 dx) (
‚àû
‚à´
‚àí‚àû
e‚àíy2/2 dy) =
‚àû
‚à´
‚àí‚àû
‚àû
‚à´
‚àí‚àû
e‚àí(x2+y2)/2 dx dy .
Change the variables in the right-hand double integral as follows: x := r cos Œ∏ and
y := r sin Œ∏, where 0 < r < ‚àûand 0 ‚â§Œ∏ < 2œÄ. Observe that
dx dy = ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®D(r, Œ∏)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®dr dŒ∏
with
D(r, Œ∏) = det (
ùúïx
ùúïr
ùúïx
ùúïŒ∏
ùúïy
ùúïr
ùúïy
ùúïŒ∏
) = det ( cos Œ∏
‚àír sin Œ∏
sin Œ∏
r cos Œ∏ ) = r cos2 Œ∏ + r sin2 Œ∏ = r .

56
‡±™
1 Probabilities
Using x2 + y2 = r2 cos2 Œ∏ + r2 sin2 Œ∏ = r2, this change of variables leads to
a2 =
2œÄ
‚à´
0
‚àû
‚à´
0
r e‚àír2/2 dr dŒ∏ =
2œÄ
‚à´
0
[‚àíe‚àír2/2]
‚àû
0 dŒ∏ = 2œÄ ,
which, due to a > 0, implies a = ‚àö2œÄ. This completes the proof.
Given Œº ‚àà‚Ñùand œÉ > 0, let
pŒº,œÉ(x) :=
1
‚àö2œÄœÉ
e‚àí(x‚àíŒº)2/2œÉ2
,
x ‚àà‚Ñù.
(1.49)
Remark 1.6.8. The graph of the function x Û≥®É‚ÜípŒº,œÉ(x) is ‚Äúbell-shaped,‚Äù has its maximal
value 1/‚àö2œÄœÉ at x = Œº, attains only positive values, and is symmetric around x = Œº. If
x ‚Üí‚àûor x ‚Üí‚àí‚àû, then pŒº,œÉ(x) tends to zero very rapidly. The bigger the œÉ > 0, the
flatter the graph of pŒº,œÉ. Nevertheless, as we will show in the next proposition, the area
under the graph of pŒº,œÉ always equals 1, no matter how big or small œÉ > 0 is (compare
also Figure 1.10).
Figure 1.10: The function pŒº,œÉ with parameters Œº = 0, 1, 2, 3 and œÉ = 0.5, 0.75, 0.9, 1.1.
Proposition 1.6.9. If Œº ‚àà‚Ñùand œÉ > 0, then pŒº,œÉ is a probability density function.
Proof. We have to verify
‚àû
‚à´
‚àí‚àû
pŒº,œÉ(x) dx = 1,
or
‚àû
‚à´
‚àí‚àû
e(x‚àíŒº)2/2œÉ2
dx = ‚àö2œÄ œÉ .
Setting u := (x ‚àíŒº)/œÉ, it follows that dx = œÉ du, hence Proposition 1.6.7 leads to
‚àû
‚à´
‚àí‚àû
e(x‚àíŒº)2/2œÉ2
dx = œÉ
‚àû
‚à´
‚àí‚àû
e‚àíu2/2 du = œÉ ‚àö2œÄ .
This completes the proof.

1.6 Special continuous distributions
‡±™
57
Definition 1.6.10. The probability measure generated by pŒº,œÉ is called the normal distribution with
expected value Œº and variance œÉ2. It is denoted by ùí©(Œº, œÉ2), that is, for all a < b,
ùí©(Œº, œÉ2)([a, b]) =
1
‚àö2œÄœÉ
b
‚à´
a
e‚àí(x‚àíŒº)2/2œÉ2
dx .
Remark 1.6.11. At this moment, the numbers Œº ‚àà‚Ñùand œÉ > 0 are nothing else than pa-
rameters. Why they are called ‚Äúexpected value‚Äù and ‚Äúvariance‚Äù will become clear after
we introduce these notations in Section 5.
Definition 1.6.12. The probability measure ùí©(0, 1) is called the standard normal distribution. It is
given by
ùí©(0, 1)([a, b]) =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx .
Figure 1.11: The area of the gray shaded region coincides with ùí©(0, 1)([a, b]).
For example, we have
ùí©(0, 1)([‚àí1, 1]) =
1
‚àö2œÄ
1
‚à´
‚àí1
e‚àíx2/2 dx = 0.682689,
or
ùí©(0, 1)([2, 4]) =
1
‚àö2œÄ
4
‚à´
2
e‚àíx2/2 dx = 0.0227185 .
Summary: The normal distribution with expected value Œº ‚àà‚Ñùand variance œÉ2 > 0 acts as
ùí©(Œº, œÉ2)([a, b]) =
1
‚àö2œÄœÉ
b
‚à´
a
e‚àí(x‚àíŒº)2/2œÉ2
dx .
The probability measure ùí©(0, 1) is called the standard normal distribution.

58
‡±™
1 Probabilities
1.6.3 Gamma distribution
Euler‚Äôs gamma function is a mapping from (0, ‚àû) to ‚Ñùdefined by
Œì(x) :=
‚àû
‚à´
0
sx‚àí1e‚àísds ,
x > 0 .
The graph of the gamma function for small entries is drawn in Figure 1.12.
Figure 1.12: The graph of the gamma function.
Let us summarize the main properties of the gamma function.
Proposition 1.6.13.
1.
The gamma function maps (0, ‚àû) continuously to (0, ‚àû) and possesses continuous
derivatives of any order.
2.
If x > 0, then
Œì(x + 1) = x Œì(x) .
(1.50)
3.
For n ‚àà‚Ñï, it follows that Œì(n) = (n ‚àí1)!. In particular,
Œì(1) = Œì(2) = 1
and
Œì(3) = 2 .
4.
One has Œì(1/2) = ‚àöœÄ, which implies
Œì(n + 1
2) = (2n ‚àí1)!!
2n
‚àöœÄ ,
n = 1, 2, . . .
(1.51)
Here the double factorial is defined by (2n ‚àí1)!! = (2n ‚àí1)(2n ‚àí3) ‚ãÖ‚ãÖ‚ãÖ3 ‚ãÖ1.
Proof. For the proof of the continuity and differentiability, we refer to [Art64].
The proof of eq. (1.50) is carried out by integration by parts as follows:
Œì(x + 1) =
‚àû
‚à´
0
sx e‚àísds = [‚àísx e‚àís]
‚àû
0 +
‚àû
‚à´
0
x sx‚àí1e‚àísds = x Œì(x) .

1.6 Special continuous distributions
‡±™
59
Note that sxe‚àís = 0 if s = 0 or s ‚Üí‚àû.
From
Œì(1) =
‚àû
‚à´
0
e‚àísds = 1
and eq. (1.50), it follows, as claimed, that
Œì(n) = (n ‚àí1)Œì(n ‚àí1) = (n ‚àí1)(n ‚àí2)Œì(n ‚àí2) = ‚ãÖ‚ãÖ‚ãÖ= (n ‚àí1) ‚ãÖ‚ãÖ‚ãÖ1 ‚ãÖŒì(1) = (n ‚àí1)! .
To prove the fourth assertion, we use Proposition 1.6.7. Because of
‚àö2œÄ =
‚àû
‚à´
‚àí‚àû
e‚àít2/2dt = 2
‚àû
‚à´
0
e‚àít2/2dt,
it follows that
‚àû
‚à´
0
e‚àít2/2dt = ‚àöœÄ
2 .
(1.52)
Substituting s = t2/2, thus ds = tdt, by eq. (1.52), the integral for Œì(1/2) transforms
into
Œì(1/2) =
‚àû
‚à´
0
s‚àí1/2e‚àísds =
‚àû
‚à´
0
‚àö2
t e‚àít2/2 t dt = ‚àö2
‚àû
‚à´
0
e‚àít2/2dt = ‚àöœÄ .
Formula (1.51) follows by a repeated application of eq. (1.50). Indeed, then we get
Œì(n + 1
2) = 2n ‚àí1
2
‚ãÖŒì(n ‚àí1
2)
= 2n ‚àí1
2
‚ãÖ2n ‚àí3
2
‚ãÖŒì(n ‚àí3
2) = ‚ãÖ‚ãÖ‚ãÖ= (2n ‚àí1)!!
2n
Œì(1
2) .
This completes the proof.
If x ‚Üí‚àû, then Œì(x) increases very rapidly. More precisely, the following is valid
(cf. [Art64]):
Proposition 1.6.14 (Stirling‚Äôs formula for the gamma function). For x > 0, there exists a
number Œ∏ ‚àà(0, 1) such that
Œì(x) = ‚àö2œÄ
x (x
e )
x
eŒ∏/12x .
(1.53)

60
‡±™
1 Probabilities
In view of
n! = Œì(n + 1) = nŒì(n),
formula (1.53) leads to13 the following.
Corollary 1.6.15 (Stirling‚Äôs formula for n-factorial). For each natural number n, there is
some Œ∏ ‚àà(0, 1) depending on n such that
n! = ‚àö2œÄn (n
e )
n
eŒ∏/12n .
(1.54)
In particular, we have
lim
n‚Üí‚àû
en
nn+1/2 n! = ‚àö2œÄ .
Our next aim is to introduce a continuous probability measure with density tightly
related to the Œì-function. Given Œ±, Œ≤ > 0, define pŒ±,Œ≤ from ‚Ñùto ‚Ñùby (see Figure 1.13 for
some examples)
pŒ±,Œ≤(x) :=
{
{
{
0
if x ‚â§0,
1
Œ±Œ≤ Œì(Œ≤) xŒ≤‚àí1e‚àíx/Œ±
if x > 0.
(1.55)
Figure 1.13: The functions p1,Œ≤ with Œ≤ = 0.5, 1, 1.5, 2 and 2.5 from top left to bottom left.
Proposition 1.6.16. For all Œ±, Œ≤ > 0, the function pŒ±,Œ≤ in eq. (1.55) is a probability density.
Proof. Of course, pŒ±,Œ≤(x) ‚â•0. Thus it remains to verify
‚àû
‚à´
‚àí‚àû
pŒ±,Œ≤(x) dx = 1 .
(1.56)
13 cf. also [Spi08, Chapter 27, Problem 19].

1.6 Special continuous distributions
‡±™
61
By the definition of pŒ±,Œ≤, we have
‚àû
‚à´
‚àí‚àû
pŒ±,Œ≤(x) dx =
1
Œ±Œ≤ Œì(Œ≤)
‚àû
‚à´
0
xŒ≤‚àí1e‚àíx/Œ± dx .
Substituting in the right-hand integral u := x/Œ±, thus dx = Œ±du, the right-hand side
becomes
1
Œì(Œ≤)
‚àû
‚à´
0
uŒ≤‚àí1e‚àíu du =
1
Œì(Œ≤) Œì(Œ≤) = 1 .
Hence eq. (1.56) is valid, and pŒ±,Œ≤ is a probability density function.
Definition 1.6.17. The probability measure ŒìŒ±,Œ≤ with density function pŒ±,Œ≤ is called the gamma distribu-
tion with positive parameters Œ± and Œ≤. For all 0 ‚â§a < b < ‚àû,
ŒìŒ±,Œ≤([a, b]) =
1
Œ±Œ≤ Œì(Œ≤)
b
‚à´
a
xŒ≤‚àí1e‚àíx/Œ± dx .
(1.57)
Remark 1.6.18. Since pŒ±,Œ≤(x) = 0 for x ‚â§0, it follows that ŒìŒ±,Œ≤((‚àí‚àû, 0]) = 0. Hence, if
a < b are arbitrary, then
ŒìŒ±,Œ≤([a, b]) = ŒìŒ±,Œ≤([0, ‚àû) ‚à©[a, b]) .
Remark 1.6.19. If Œ≤ ‚àâ‚Ñï, then the integral in eq. (1.57) cannot be expressed by elemen-
tary functions. Only numerical evaluations are possible.
1.6.4 Exponential distribution
An important special gamma distribution is the exponential distribution. This probabil-
ity measure is defined as follows.
Definition 1.6.20. For Œª > 0, let EŒª := ŒìŒª‚àí1,1 be the exponential distribution with parameter Œª > 0.
Remark 1.6.21. The probability density function pŒª of EŒª is given by
pŒª(x) = {0
if x ‚â§0,
Œª e‚àíŒªx
if x > 0.

62
‡±™
1 Probabilities
Consequently, if 0 ‚â§a < b < ‚àû, then the probability of [a, b] can be evaluated by
EŒª([a, b]) = Œª
b
‚à´
a
e‚àíŒªx dx = e‚àíŒªa ‚àíe‚àíŒªb .
Moreover,
EŒª([t, ‚àû)) = e‚àíŒªt ,
t ‚â•0 .
See Figure 1.14 for certain graphs of densities generating exponential distributions.
Figure 1.14: The densities of EŒª with Œª = 1, 1/2, 1/3, and 1/4.
Remark 1.6.22. The exponential distribution plays an important role for the descrip-
tion of lifetimes. For instance, it is used to determine the probability that the lifetime of
a component part or the duration of a phone call exceeds a certain time T > 0. Further-
more, it is applied to describe the time between the arrivals of customers at a counter
or in a shop.
Example 1.6.23. Suppose that the duration of phone calls is exponentially distributed
with parameter Œª = 0.1. What is the probability that a call lasts less than two time units?
Or what is the probability that it lasts between one and two units? Or more than five
units?
Answer: These probabilities are evaluated by
E0.1([0, 2]) = 1 ‚àíe‚àí0.2 = 0.181269 ,
E0.1([1, 2]) = e‚àí0.1 ‚àíe‚àí0.2 = 0.08611 ,
E0.1([5, ‚àû)) = e‚àí0.5 = 0.60653 .

1.6 Special continuous distributions
‡±™
63
1.6.5 Erlang distribution
Another important class of gamma distributions is that of Erlang distributions defined
as follows.
Definition 1.6.24. For Œª > 0 and n ‚àà‚Ñï, let EŒª,n := ŒìŒª‚àí1,n. This probability measure is called the Erlang
distribution with parameters Œª and n.
Remark 1.6.25. The density pŒª,n of the Erlang distribution is (see also Figure 1.15)
pŒª,n(x) = {0
if x ‚â§0,
Œªn
(n‚àí1)! xn‚àí1 e‚àíŒªx
if x > 0.
Figure 1.15: The densities of the Erlang distribution with Œª = 3 and n = 2, 3, 4, and 5.
Of course, EŒª,1 = EŒª. Thus, the Erlang distribution may be viewed as generalized expo-
nential distribution.
An important property of the Erlang distribution is as follows.
Proposition 1.6.26. If t > 0, then
EŒª,n([t, ‚àû)) =
n‚àí1
‚àë
j=0
(Œªt)j
j!
e‚àíŒªt .
Proof. We have to show that for t > 0,
‚àû
‚à´
t
pŒª,n(x) dx =
Œªn
(n ‚àí1)!
‚àû
‚à´
t
xn‚àí1 e‚àíŒªx dx =
n‚àí1
‚àë
j=0
(Œªt)j
j!
e‚àíŒªt .
(1.58)

64
‡±™
1 Probabilities
This is done by induction over n.
If n = 1, then eq. (1.58) is valid due to
‚àû
‚à´
t
pŒª,1(x) dx =
‚àû
‚à´
t
Œª e‚àíŒªx dx = e‚àíŒªt .
Suppose now eq. (1.58) is proven for some n ‚â•1. Next, we have to show that it is also
valid for n + 1. Thus, we know
Œªn
(n ‚àí1)!
‚àû
‚à´
t
xn‚àí1 e‚àíŒªx dx =
n‚àí1
‚àë
j=0
(Œªt)j
j!
e‚àíŒªt
(1.59)
and want
Œªn+1
n!
‚àû
‚à´
t
xn e‚àíŒªx dx =
n
‚àë
j=0
(Œªt)j
j!
e‚àíŒªt .
(1.60)
Let us integrate the integral in eq. (1.60) by parts as follows. Set u := xn, hence u‚Ä≤ = n xn‚àí1,
and v‚Ä≤ = e‚àíŒªx, thus v = ‚àíŒª‚àí1 e‚àíŒªx. Doing so and using eq. (1.59), the left-hand side of
eq. (1.60) becomes
Œªn+1
n!
‚àû
‚à´
t
xn e‚àíŒªx dx = [‚àíŒªn
n! xn e‚àíŒªx]
‚àû
t
+
Œªn
(n ‚àí1)!
‚àû
‚à´
t
xn‚àí1 e‚àíŒªx dx
= (Œªt)n
n!
e‚àíŒªt +
n‚àí1
‚àë
j=0
(Œªt)j
j!
e‚àíŒªt =
n
‚àë
j=0
(Œªt)j
j!
e‚àíŒªt .
This proves eq. (1.60) and, consequently, eq. (1.58) is valid for all n ‚â•1.
1.6.6 Chi-squared distribution
Another important class of gamma distributions is that of œá2-distributions. These prob-
ability measures play a crucial role in Mathematical Statistics (cf. Chapter 8).
Definition 1.6.27. For n ‚â•1, let
œá2
n := Œì2,n/2 .
This probability measure is called the œá2-distribution with n degrees of freedom.
Remark 1.6.28. At the moment, the integer n ‚â•1 in Definition 1.6.27 is only a parameter.
The term ‚Äúdegree of freedom‚Äù will become clear when we apply the œá2-distribution to
statistical problems.

1.6 Special continuous distributions
‡±™
65
Remark 1.6.29. The density p of a œá2
n-distribution is given by (compare Figure 1.16)
p(x) = {0
if x ‚â§0,
xn/2‚àí1e‚àíx/2
2n/2Œì(n/2)
if x > 0,
i. e., if 0 ‚â§a < b, then
œá2
n([a, b]) =
1
2n/2Œì(n/2)
b
‚à´
a
xn/2‚àí1 e‚àíx/2 dx .
Figure 1.16: Density functions of œá2
n-distributions, n = 1, 3, 5, and n = 7.
Summary: For each Œ±, Œ≤ > 0, the probability measure ŒìŒ±,Œ≤ is given by
ŒìŒ±,Œ≤([a, b]) =
1
Œ±Œ≤ Œì(Œ≤)
b
‚à´
a
xŒ≤‚àí1e‚àíx/Œ± dx ,
0 ‚â§a < b < ‚àû.
Of special interest are the exponential distribution EŒª = ŒìŒª‚àí1,1, the Erlang distribution defined by EŒª,n = ŒìŒª‚àí1,n,
and the chi-squared distribution œá2
n = Œì2,n/2. Here Œª > 0 and n ‚àà‚Ñï.
1.6.7 Beta distribution
Tightly connected with the gamma function is Euler‚Äôs beta function B. It maps (0, ‚àû)2
to ‚Ñùand is defined by

66
‡±™
1 Probabilities
B(x, y) :=
1
‚à´
0
sx‚àí1(1 ‚àís)y‚àí1 ds ,
x, y > 0 .
(1.61)
The link between the gamma and beta functions is the following important identity:
B(x, y) = Œì(x) ‚ãÖŒì(y)
Œì(x + y) ,
x, y > 0 .
(1.62)
For a proof of eq. (1.62), we refer to Problem 1.34.
Let us summarize further properties of the beta function. They are either easy to
prove or follow via eq. (1.62) by the properties of the gamma function.
Proposition 1.6.30.
1.
The beta function is continuous on (0, ‚àû) √ó (0, ‚àû) with values in (0, ‚àû).
2.
For x, y > 0, one has B(x, y) = B(y, x).
3.
If x, y > 0, then
B(x + 1, y) =
x
x + y B(x, y) .
(1.63)
4.
For x > 0, one has B(x, 1) = 1/x.
5.
If m, n ‚â•1 are integers, then
B(m, n) = (m ‚àí1)! (n ‚àí1)!
(m + n ‚àí1)!
.
6.
It holds
B(1
2, 1
2) = œÄ .
(1.64)
Definition 1.6.31. Let Œ±, Œ≤ > 0. The probability measure ‚Ñ¨Œ±,Œ≤ defined by
‚Ñ¨Œ±,Œ≤([a, b]) :=
1
B(Œ±, Œ≤)
b
‚à´
a
xŒ±‚àí1(1 ‚àíx)Œ≤‚àí1 dx ,
0 ‚â§a < b ‚â§1 ,
is called the beta distribution with parameters Œ± and Œ≤.
That is, the density function qŒ±,Œ≤ of ‚Ñ¨Œ±,Œ≤ is given by
qŒ±,Œ≤(x) = {
1
B(Œ±,Œ≤) xŒ±‚àí1(1 ‚àíx)Œ≤‚àí1
if 0 < x < 1,
0
otherwise.
Compare Figure 1.17 for the densities of beta distributions with certain pairs of parame-
ters Œ±, Œ≤ > 0. Note that the densities are bounded provided that Œ±, Œ≤ ‚â•1 and unbounded
whenever one of the parameters is less than 1.

1.6 Special continuous distributions
‡±™
67
Figure 1.17: Density functions of the beta distribution with parameters (0.5, 1.5), (1.5, 2.5), (2.5, 2), (1.5, 2),
(2, 1.5), and (2.5, 2.5).
Remark 1.6.32. It is easy to see that qŒ±,Œ≤ is a density function. Of course, it is nonnegative
and, moreover, we have
‚àû
‚à´
‚àí‚àû
qŒ±,Œ≤(x) dx =
1
B(Œ±, Œ≤)
1
‚à´
0
xŒ±‚àí1(1 ‚àíx)Œ≤‚àí1 dx = B(Œ±, Œ≤)
B(Œ±, Œ≤) = 1 .
Furthermore, since qŒ±,Œ≤(x) = 0 if x ‚àâ[0, 1], the probability measure ‚Ñ¨Œ±,Œ≤ is concentrated
on [0, 1], that is, ‚Ñ¨Œ±,Œ≤([0, 1]) = 1 or, equivalently, ‚Ñ¨Œ±,Œ≤(‚Ñù\ [0, 1]) = 0.
Example 1.6.33. Choose independently n numbers x1, . . . , xn in [0, 1] according to the
uniform distribution. Ordering these numbers by their size, we get x‚àó
1 ‚â§‚ãÖ‚ãÖ‚ãÖ‚â§x‚àó
n . In
Example 3.7.11, we will show that the kth largest number x‚àó
k is ‚Ñ¨k,n‚àík+1-distributed. In
other words, if 0 ‚â§a < b ‚â§1, then
‚Ñô{a ‚â§x‚àó
k ‚â§b} = ‚Ñ¨k,n‚àík+1([a, b]) =
n!
(k ‚àí1)! (n ‚àík)!
b
‚à´
a
xk‚àí1(1 ‚àíx)n‚àík dx .
Among the beta distributions ‚Ñ¨Œ±,Œ≤, the one with Œ± = Œ≤ = 1
2 is of special interest. It
plays an important role in the investigation of the symmetric random walk as well as of
the Brownian motion (see Proposition 5.5.19).
Proposition 1.6.34. The density q of the beta distribution ‚Ñ¨1/2,1/2 is given by
q(x) =
{
{
{
1
œÄ
1
‚àöx(1‚àíx)
if 0 < x < 1,
0
otherwise.
(1.65)
Furthermore, if 0 ‚â§a < b ‚â§1, then it follows that
‚Ñ¨1/2,1/2([a, b]) = 2
œÄ [arcsin(‚àöb) ‚àíarcsin(‚àöa)] .
(1.66)

68
‡±™
1 Probabilities
Proof. Using eq. (1.64), representation (1.65) of the density easily follows from
q(x) =
1
B(1/2, 1/2) x
1
2 ‚àí1(1 ‚àíx)
1
2 ‚àí1 = 1
œÄ
1
‚àöx(1 ‚àíx)
,
0 < x < 1 .
Assertion (1.66) is a consequence of
2
œÄ
d
dx arcsin(‚àöx) = 2
œÄ
1
‚àö1 ‚àí(‚àöx)2
‚ãÖ
1
2‚àöx = 1
œÄ
1
‚àöx(1 ‚àíx)
,
0 < x < 1 ,
in view of the fundamental theorem of Calculus. See Figure 1.18 for the graph of the
density q.
Figure 1.18: The density of the arcsine distribution.
Definition 1.6.35. The probability measure ‚Ñ¨1/2,1/2 is called the arcsine distribution. Its density is given
by eq. (1.65), and for any 0 < a ‚â§1 it follows that
‚Ñ¨1/2,1/2([0, a]) = 2
œÄ arcsin(‚àöa) .
(1.67)
1.6.8 Cauchy distribution
We start with the following statement.
Proposition 1.6.36. The function p defined by
p(x) = 1
œÄ ‚ãÖ
1
1 + x2 ,
x ‚àà‚Ñù,
(1.68)
is a probability density.

1.7 Distribution function
‡±™
69
Proof. Of course, p(x) > 0 for x ‚àà‚Ñù. Let us now investigate ‚à´
‚àû
‚àí‚àûp(x) dx. Because of
limb‚Üí‚àûarctan(b) = œÄ/2 and lima‚Üí‚àí‚àûarctan(a) = ‚àíœÄ/2, it follows that
‚àû
‚à´
‚àí‚àû
p(x) dx = 1
œÄ
lim
a‚Üí‚àí‚àûlim
b‚Üí‚àû
b
‚à´
a
1
1 + x2 dx = 1
œÄ
lim
a‚Üí‚àí‚àûlim
b‚Üí‚àû[arctan x]b
a = 1 .
Thus, as asserted, p is a probability density.
Definition 1.6.37. The probability measure ‚Ñôwith density p from Eq. (1.68) is called the Cauchy distri-
bution. It is characterized by
‚Ñô([a, b]) = 1
œÄ
b
‚à´
a
1
1 + x2 dx = 1
œÄ [arctan(b) ‚àíarctan(a)] ,
‚àí‚àû< a < b < ‚àû.
Figure 1.19: The density function of the Cauchy distribution.
Remark 1.6.38. Comparing the density function of the Cauchy distribution in Figure 1.19
with that of the standard normal distribution in Figure 1.11, at a first glance both func-
tions look very similar. But, in fact they differ significantly. While the density of the stan-
dard normal distribution tends to 0 exponentially as x ‚Üí‚àû, the convergence of the
density of the Cauchy distribution is only of quadratic order. That is, events lying far
away from zero possess an essentially greater probability for the Cauchy distribution as
they do in the case of the normal. This property of the Cauchy distribution is sometimes
expressed by saying that it is a distribution possessing ‚Äúheavy tails.‚Äù
1.7 Distribution function
In this section we always assume that the sample space is ‚Ñù, even if the random exper-
iment has only finitely or countably infinitely many different outcomes. For example,

70
‡±™
1 Probabilities
rolling a die once is modeled by (‚Ñù, ùí´(‚Ñù), ‚Ñô), where ‚Ñô({k}) = 1/6, k = 1, . . . , 6, and
‚Ñô({x}) = 0 whenever x ‚àâ{1, . . . , 6}.
Thus, let ‚Ñôbe a probability measure either defined on ‚Ñ¨(‚Ñù) (continuous case) or on
ùí´(‚Ñù) (discrete case).
Definition 1.7.1. The function F : ‚Ñù‚Üí[0, 1] defined by
F(t) := ‚Ñô((‚àí‚àû, t]) ,
t ‚àà‚Ñù,
(1.69)
is called the (cumulative) distribution function of ‚Ñô.
Remark 1.7.2. To shorten the notation, we will mostly call F in (1.69) the ‚Äúdistribution
function‚Äù instead of, as is often done in the literature, the ‚Äúcumulative distribution func-
tion,‚Äù abbreviated CDF.
Remark 1.7.3. If ‚Ñôis discrete, that is, ‚Ñô(D) = 1 for some D = {x1, x2, . . . }, then its distri-
bution function can be evaluated by
F(t) = ‚àë
xj‚â§t
‚Ñô({xj}) = ‚àë
xj‚â§t
pj ,
where pj = ‚Ñô({xj}), while for a continuous ‚Ñôwith probability density p,
F(t) =
t
‚à´
‚àí‚àû
p(x) dx .
Example 1.7.4. Let ‚Ñôbe the uniform distribution on {1, . . . , 6}. Then
F(t) =
{
{
{
{
{
{
{
0
if t < 1,
k
6
if k ‚â§t < k + 1 , k ‚àà{1, . . . , 5},
1
if t ‚â•6 .
(See Figure 1.20.)
Example 1.7.5. The distribution function of the binomial distribution Bn,p is given by
F(t) = ‚àë
0‚â§k‚â§t
(n
k)pk(1 ‚àíp)n‚àík ,
0 ‚â§t < ‚àû,
and F(t) = 0 if t < 0. (See Figure 1.21.)
Example 1.7.6. The distribution function of the exponential distribution EŒª is (see Fig-
ure 1.22 for an example)
F(t) = {0
if t < 0,
1 ‚àíe‚àíŒªt
if t ‚â•0.

1.7 Distribution function
‡±™
71
Figure 1.20: Distribution function of the uniform distribution on {1, . . . , 6}.
Figure 1.21: Distribution function of the binomial distribution B25,0.4.
Figure 1.22: Distribution function of E0.5.

72
‡±™
1 Probabilities
Example 1.7.7. How does the distribution function F of the Erlang distribution EŒª,n look
like? In view of Proposition 1.6.26, it follows that F(t) = 0 if t < 0 and
F(t) = 1 ‚àí
n‚àí1
‚àë
j=0
(Œªt)j
j!
e‚àíŒªt ,
0 ‚â§t < ‚àû.
In the case n = 1, we rediscover the function stated in Example 1.7.6. Compare Figure 1.23
for certain distribution functions of the Erlang distribution.
Figure 1.23: Distribution functions of E2,n, where from the left to the right n = 2, 4, 6, 8.
Example 1.7.8. The distribution function of the standard normal distribution is de-
noted14 by Œ¶, therefore also called Gaussian Œ¶-function (see Figure 1.24),
Œ¶(t) =
1
‚àö2œÄ
t
‚à´
‚àí‚àû
e‚àíx2/2 dx ,
t ‚àà‚Ñù.
(1.70)
Remark 1.7.9. The Gaussian Œ¶-function is tightly related to the Gaussian error func-
tion defined by (compare Figure 1.25)
erf(t) =
2
‚àöœÄ
t
‚à´
0
e‚àíx2
dx ,
t ‚àà‚Ñù.
Observe that erf(‚àít) = ‚àíerf(t).
14 Sometimes also denoted as ‚Äúnorm(‚ãÖ).‚Äù

1.7 Distribution function
‡±™
73
Figure 1.24: Distribution function of the standard normal distribution (Œ¶-function).
Figure 1.25: The Gaussian error function t Û≥®É‚Üíerf(t).
The link between the Œ¶ and the error function is
Œ¶(t) = 1
2[1 + erf( t
‚àö2
)]
and
erf(t) = 2Œ¶(‚àö2 t) ‚àí1 ,
t ‚àà‚Ñù.
(1.71)
Example 1.7.10. Let ‚Ñôbe the uniform distribution on the interval [Œ±, Œ≤]. Then its distri-
bution function (cf. Figure 1.26) is
F(t) =
{
{
{
{
{
{
{
0
if t < Œ±,
t‚àíŒ±
Œ≤‚àíŒ±
if Œ± ‚â§t ‚â§Œ≤,
1
if t > Œ≤.

74
‡±™
1 Probabilities
In particular, for the uniform distribution on [0, 1], one obtains
F(t) =
{
{
{
{
{
{
{
0
if t < 0,
t
if 0 ‚â§t ‚â§1,
1
if t > 1.
Figure 1.26: Distribution function of the uniform distribution on [‚àí1, 2].
Example 1.7.11. In view of eq. (1.67), the distribution function of the arcsine distribution
(see Definition 1.6.35) is given by (compare Figure 1.27)
F(t) =
{
{
{
{
{
{
{
0
if ‚àí‚àû< t < 0,
2
œÄ arcsin(‚àöt)
if 0 ‚â§t ‚â§1,
1
if 1 < t < ‚àû.
Figure 1.27: The distribution function of the arcsine distribution.

1.7 Distribution function
‡±™
75
Example 1.7.12. The distribution function of the Cauchy distribution is (compare Fig-
ure 1.28)
F(t) = 1
œÄ
t
‚à´
‚àí‚àû
1
1 + x2 dx = arctan x
œÄ
+ 1
2 ,
t ‚àà‚Ñù.
Figure 1.28: The distribution function of the Cauchy distribution.
The next proposition lists the main properties of distribution functions.
Proposition 1.7.13. Let F be the distribution function of a probability measure ‚Ñôon ‚Ñù,
discrete or continuous. Then F possesses the following properties:
(1) Function F is nondecreasing.
(2) It holds
F(‚àí‚àû) = lim
t‚Üí‚àí‚àûF(t) = 0
and
F(‚àû) = lim
t‚Üí‚àûF(t) = 1 .
(3) Function F is continuous from the right.
Proof. Suppose s < t. This implies (‚àí‚àû, s] ‚äÇ(‚àí‚àû, t], hence, since ‚Ñôis monotone, we
obtain
F(s) = ‚Ñô((‚àí‚àû, s]) ‚â§‚Ñô((‚àí‚àû, t]) = F(t) .
Thus F is nondecreasing.
Take any sequence (tn)n‚â•1 that decreases monotonically to ‚àí‚àû. Set An := (‚àí‚àû, tn].
Then A1 ‚äáA2 ‚äá‚ãÖ‚ãÖ‚ãÖ, as well as ‚ãÇ‚àû
n=1 An = 0. Since ‚Ñôis continuous from above, it fol-

76
‡±™
1 Probabilities
lows that
lim
n‚Üí‚àûF(tn) = lim
n‚Üí‚àû‚Ñô(An) = ‚Ñô(0) = 0 .
This being true for any sequence (tn)n‚â•1 tending to ‚àí‚àûimplies F(‚àí‚àû) = 0.
The proof of F(‚àû) = 1 is very similar. In this case, let (tn)n‚â•1 be a sequence which
increases monotonically to ‚àû. If as before An := (‚àí‚àû, tn], this time we get A1 ‚äÜA2 ‚äÜ‚ãÖ‚ãÖ‚ãÖ
and ‚ãÉ‚àû
n=1 An = ‚Ñù. By the continuity of ‚Ñôfrom below, now we obtain
lim
n‚Üí‚àûF(tn) = lim
n‚Üí‚àû‚Ñô(An) = ‚Ñô(‚Ñù) = 1 .
Again, since the tns were arbitrary, F(‚àû) = 1.
Thus it remains to prove that F is continuous from the right. To do this, we take
t ‚àà‚Ñùand a decreasing sequence (tn)n‚â•1 tending to t. We have to show that if n ‚Üí‚àû,
then F(tn) ‚ÜíF(t).
As before set An := (‚àí‚àû, tn]. Again A1 ‚äáA2 ‚äá‚ãÖ‚ãÖ‚ãÖ, but now ‚ãÇ‚àû
n=1 An = (‚àí‚àû, t].
Another application of the continuity from above yields
F(t) = ‚Ñô((‚àí‚àû, t]) = lim
n‚Üí‚àû‚Ñô(An) = lim
n‚Üí‚àûF(tn) .
This is valid for each t ‚àà‚Ñù, hence F is continuous from the right.
Properties (1), (2), and (3) in Proposition 1.7.13 characterize distribution functions.
More precisely, the following result is true. Its proof is based on an extension theorem
in Measure Theory (cf. [Bau01]). Therefore, we can show here only its main ideas.
Proposition 1.7.14. Let F : ‚Ñù‚Üí‚Ñùbe an arbitrary function possessing the properties
stated in Proposition 1.7.13. Then there exists a unique probability measure ‚Ñôon ‚Ñ¨(‚Ñù)
such that
F(t) = ‚Ñô((‚àí‚àû, t]) ,
t ‚àà‚Ñù.
Idea of the proof: If a < b, set
‚Ñô0((a, b]) := F(b) ‚àíF(a) .
In this way, we get a mapping ‚Ñô0 defined on the collection of all half-open intervals
{(a, b] : a < b}. The key point is to verify that ‚Ñô0 can be uniquely extended to a proba-
bility measure ‚Ñôon ‚Ñ¨(‚Ñù). One way to do this is to introduce a so-called outer measure
‚Ñô‚àódefined on ùí´(‚Ñù) by
‚Ñô‚àó(B) := inf{
‚àû
‚àë
i=1
‚Ñô0((ai, bi]) : B ‚äÜ
‚àû
‚ãÉ
i=1
(ai, bi]} .

1.7 Distribution function
‡±™
77
Generally, this outer measure is not œÉ-additive. Therefore, one restricts ‚Ñô‚àóto ‚Ñ¨(‚Ñù). If ‚Ñô
denotes this restriction, the most difficult part of the proof is to verify that ‚Ñôis œÉ-additive.
After this has been done, by the construction, ‚Ñôis the probability measure possessing
distribution function F.
The uniqueness of ‚Ñôfollows by a general uniqueness theorem for probability mea-
sures (see Theorem 5.7 in [Sch17] for a proof) asserting the following:
Let ‚Ñô1 and ‚Ñô2 be two probability measures on (Œ©, ùíú) and let ‚Ñ∞‚äÜùíúbe a collection
of events closed under taking intersections and generating ùíú. If ‚Ñô1(E) = ‚Ñô2(E) for all
E ‚àà‚Ñ∞, then ‚Ñô1 = ‚Ñô2. In our case ‚Ñ∞= {(‚àí‚àû, t] : t ‚àà‚Ñù} and ùíú= ‚Ñ¨(‚Ñù).
Conclusion. If the outcomes of a random experiment are real numbers, then this ex-
periment can also be described by a function F : ‚Ñù‚Üí‚Ñùpossessing the properties in
Proposition 1.7.13. Then F(t) is the probability to observe a result that is less than or
equal to t.
Let us state further properties of distribution functions.
Proposition 1.7.15. If F is the distribution function of a probability measure ‚Ñô, then for
all a < b,
F(b) ‚àíF(a) = ‚Ñô((a, b]) .
Proof. Observing that (‚àí‚àû, a] ‚äÜ(‚àí‚àû, b], this is an immediate consequence of
F(b) ‚àíF(a) = ‚Ñô((‚àí‚àû, b]) ‚àí‚Ñô((‚àí‚àû, a]) = ‚Ñô((‚àí‚àû, b] \ (‚àí‚àû, a]) = P((a, b]) .
Since F is nondecreasing and bounded, for each t ‚àà‚Ñùthe left-hand limit
F(t ‚àí0) := lim
s‚Üít
s<t
F(s)
exists and, moreover, F(t ‚àí0) ‚â§F(t). Furthermore, by the right continuity of F, one has
F(t ‚àí0) = F(t) if and only if F is continuous at the point t.
If this is not so, then h = F(t) ‚àíF(t ‚àí0) > 0, that is, F possesses at t ‚àà‚Ñùa jump of
height h > 0. This height is directly connected with the value of ‚Ñô({t}).
Proposition 1.7.16. The distribution function F of a probability measure ‚Ñôhas a jump of
height h ‚â•0 at t ‚àà‚Ñùif and only if ‚Ñô({t}) = h.
Proof. Let (tn)n‚â•1 be a sequence of real numbers increasing monotonically to t. Then,
using that ‚Ñôis continuous from above, it follows that
h = F(t) ‚àíF(t ‚àí0) = lim
n‚Üí‚àû[F(t) ‚àíF(tn)] = lim
n‚Üí‚àû‚Ñô((tn, t]) = ‚Ñô({t}) .
Observe that ‚ãÇ‚àû
n=1(tn, t] = {t}. This proves the assertion. See Figure 1.29 for a visualiza-
tion of the result.

78
‡±™
1 Probabilities
Figure 1.29: The height h of the jump of F at t0 coincides with the probability ‚Ñô({t0}).
Corollary 1.7.17. The function F is continuous at t ‚àà‚Ñùif and only if ‚Ñô({t}) = 0.
Proof. Since F is nondecreasing and continuous from the right, it is continuous at some
point t ‚àà‚Ñùif and only if F(t ‚àí0) = F(t). But, in view of Proposition 1.7.16, this happens
if and only if ‚Ñô({t}) = 0.
Example 1.7.18. Suppose the function F is defined by
F(t) =
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
0
if t < ‚àí1,
1/3
if ‚àí1 ‚â§t < 0,
1/2
if 0 ‚â§t < 1,
2/3
if 1 ‚â§t < 2,
1
if t ‚â•2 .
Then F fulfills the assumptions of Proposition 1.7.14. Hence there is a probability mea-
sure ‚Ñôwith F(t) = ‚Ñô((‚àí‚àû, t]). What does ‚Ñôlook like?
Answer: The function F has jumps at ‚àí1, 0, 1, and 2 with heights 1/3, 1/6, 1/6, and 1/3,
respectively. Therefore,
‚Ñô({‚àí1}) = 1/3 ,
‚Ñô({0}) = 1/6 ,
‚Ñô({1}) = 1/6,
and
‚Ñô({2}) = 1/3,
hence ‚Ñôis the discrete probability measure concentrated on D = {‚àí1, 0, 1, 2} with ‚Ñô({t}),
t ‚ààD, given above.
Suppose now that ‚Ñôis continuous with density function p. Recall that then
F(t) = ‚Ñô((‚àí‚àû, t]) =
t
‚à´
‚àí‚àû
p(x) dx ,
t ‚àà‚Ñù.
(1.72)
In particular, since F is a function of the upper bound in an integral, it is continu-
ous.

1.7 Distribution function
‡±™
79
Next we investigate the question whether we may evaluate the density p knowing
the distribution function F.
Proposition 1.7.19. Suppose p is continuous at some t ‚àà‚Ñù. Then F is differentiable at t
with
F‚Ä≤(t) = d
dt F(t) = p(t) .
Proof. This follows immediately by an application of the fundamental theorem of cal-
culus to representation (1.72) of F.
Remark 1.7.20. Let F be the distribution function of a probability measure ‚Ñô. If F is
continuous, then ‚Ñô({t}) = 0 for all t ‚àà‚Ñù. But does this also imply that ‚Ñôis continuous,
that is, that ‚Ñôhas a density? The answer is negative. There exist probability measures ‚Ñô
on (‚Ñù, ‚Ñ¨(‚Ñù)) with a continuous distribution function but without possessing a density.
Such probability measures are called singularly continuous.
To get an impression of how such probability measures look, let us shortly sketch
the construction of an example. Let C be the Cantor set introduced in Example 1.6.6. The
basic idea is to transfer the uniform distribution on [0, 1] to a probability measure ‚Ñôwith
‚Ñô(C) = 1. The transformation is done by the function f defined as follows. If x ‚àà[0, 1] is
represented as x = ‚àë‚àû
k=1
xk
2k with xk ‚àà{0, 1}, then f (x) = ‚àë‚àû
k=1
2xk
3k . Note that f maps [0, 1]
into C. If ÃÉ‚Ñôdenotes the uniform distribution on [0, 1], define the probability measure ‚Ñô
by
‚Ñô(B) =
ÃÉ‚Ñô{x ‚àà[0, 1] : f (x) ‚ààB} .
Then for all t ‚àà‚Ñù, we have ‚Ñô({t}) = 0, but since ‚Ñô(C) = 1, ‚Ñôcannot have a density.
Indeed, such a density should vanish outside C. But, as we saw, the probability of C with
respect to the uniform distribution is zero. Hence the only possible density would be
p(t) = 0, t ‚àà‚Ñù. This contradiction shows that ‚Ñôis not continuous in our sense.
Assuming a little bit more than the continuity of F, the corresponding probability
measure possesses a density (cf. [Coh13]).
Proposition 1.7.21. Let F be the distribution function of a probability measure ‚Ñô. If F is
continuous and continuously differentiable with the exception of at most finitely many
points, then ‚Ñôis continuous. That is, there is a density function p such that
F(t) = ‚Ñô((‚àí‚àû, t]) =
t
‚à´
‚àí‚àû
p(x) dx ,
t ‚àà‚Ñù.
Remark 1.7.22. Proposition 1.7.19 implies p(t) = F‚Ä≤(t) for those t where F‚Ä≤(t) exists. If F
is not differentiable at t, define p(t) arbitrarily, for example, p(t) = 0.

80
‡±™
1 Probabilities
Example 1.7.23. For some Œ±, Œ≤ > 0, define F by
F(t) = {0
if t ‚â§0,
1 ‚àíe‚àíŒ± tŒ≤
if t > 0.
It is easy to see that this function satisfies the conditions of Proposition 1.7.13. Moreover,
it is continuous and continuously differentiable on ‚Ñù\ {0}. By Proposition 1.7.21, the
corresponding probability measure ‚Ñôis continuous and, since
F‚Ä≤(t) = {0
if t < 0,
Œ± Œ≤ tŒ≤‚àí1 e‚àíŒ±tŒ≤
if t > 0,
a suitable density function is p(t) = F‚Ä≤(t), t
Ã∏= 0, and p(0) = 0.
Example 1.7.24. For some Œ± > 0, let
FŒ±(t) = {0
if t ‚â§1,
1 ‚àít‚àíŒ±
if t > 1.
Then FŒ± is continuous, nondecreasing with FŒ±(‚àí‚àû) = 0 and FŒ±(‚àû) = 1. Hence, it is a
distribution function of a probability measure ‚ÑôŒ±. Moreover, at each point t
Ã∏= 1, the
function FŒ± is continuously differentiable with derivative
pŒ±(t) = {0
if t < 1,
Œ±t‚àíŒ±‚àí1
if t > 1.
So we see that pŒ± is a density of ‚ÑôŒ± where we may define, for example, pŒ±(1) = 0.
How does one get ‚ÑôŒ±([a, b]) for some 1 ‚â§a < b < ‚àû? There is no need to evaluate
the integral ‚à´
b
a pŒ±(x)dx. The much easier way is to use
‚ÑôŒ±([a, b]) = FŒ±(b) ‚àíFŒ±(a) = 1
aŒ± ‚àí1
bŒ± .
Summary: Let ‚Ñôbe a probability measure (discrete or continuous) on the Borel sets of ‚Ñù. Then its (cumula-
tive) distribution function F is defined by
F(t) = ‚Ñô{x ‚àà‚Ñù: x ‚â§t} ,
t ‚àà‚Ñù.
Distribution functions are characterized by the three properties stated in Proposition 1.7.13. Two probability
measures coincide if and only if they possess the same distribution function.

1.8 Multivariate continuous distributions
‡±™
81
1.8 Multivariate continuous distributions
1.8.1 Multivariate density functions
In this section we suppose that Œ© = ‚Ñùn. A subset Q ‚äÇ‚Ñùn is called a (closed, n-dimensional)
box15 provided that for some real numbers ai < bi, 1 ‚â§i ‚â§n,
Q = {(x1, . . . , xn) ‚àà‚Ñùn : ai ‚â§xi ‚â§bi , 1 ‚â§i ‚â§n} .
(1.73)
Definition 1.8.1. A Riemann integrable function p : ‚Ñùn ‚Üí‚Ñùis said to be an n-dimensional probability
density function, or simply n-dimensional density function, if p(x) ‚â•0 for x ‚àà‚Ñùn and, furthermore,
‚à´
‚Ñùn
p(x) dx :=
‚àû
‚à´
‚àí‚àû
‚ãÖ‚ãÖ‚ãÖ
‚àû
‚à´
‚àí‚àû
p(x1, . . . , xn) dxn ‚ãÖ‚ãÖ‚ãÖdx1 = 1 .
Suppose a box Q is represented with certain ai < bi as in eq. (1.73). Then we set
‚Ñô(Q) = ‚à´
Q
p(x) dx =
b1
‚à´
a1
‚ãÖ‚ãÖ‚ãÖ
bn
‚à´
an
p(x1, . . . , xn) dxn ‚ãÖ‚ãÖ‚ãÖdx1.
(1.74)
In analogy to Definition 1.1.20, we introduce now the Borel œÉ-field ‚Ñ¨(‚Ñùn).
Definition 1.8.2. Let ùíûbe the collection of all boxes in ‚Ñùn. Then œÉ(ùíû) := ‚Ñ¨(‚Ñùn) denotes the Borel œÉ-field.
Recall that the existence of œÉ(ùíû) was proven in Proposition 1.1.16. In other words, ‚Ñ¨(‚Ñùn) is the smallest
œÉ-field containing all (closed) boxes in ‚Ñùn. Sets in ‚Ñ¨(‚Ñùn) are called (n-dimensional) Borel sets.
Remark 1.8.3. As in the univariate case, there exist several other collections of subsets
in ‚Ñùn generating ‚Ñ¨(‚Ñùn). For example, one may choose the collection of open boxes or
the sets which may be written as
(‚àí‚àû, t1] √ó ‚ãÖ‚ãÖ‚ãÖ√ó (‚àí‚àû, tn] ,
t1, . . . , tn ‚àà‚Ñù.
With the previous notations, the following multivariate extension theorem is valid.
Compare with Proposition 1.5.6 for the univariate case.
Proposition 1.8.4. Let ‚Ñôbe defined on boxes by eq. (1.74). Then ‚Ñôadmits a unique exten-
sion to a probability measure ‚Ñôon ‚Ñ¨(‚Ñùn).
Definition 1.8.5. A probability measure ‚Ñôon ‚Ñ¨(‚Ñùn) is called continuous provided that there exists a
probability density p : ‚Ñùn ‚Üí‚Ñùsuch that ‚Ñô(Q) = ‚à´Q p(x) dx for all boxes Q ‚äÜ‚Ñùn. The function p is said to
be the density function, or simply density, of ‚Ñô.
15 Also called ‚Äúhyperrectangle.‚Äù

82
‡±™
1 Probabilities
Remark 1.8.6. It is easy to see that the validity of eq. (1.74) for all boxes is equivalent to
the following. If tj ‚àà‚Ñùand Bt1,...,tn := (‚àí‚àû, t1] √ó ‚ãÖ‚ãÖ‚ãÖ√ó (‚àí‚àû, tn], then
‚Ñô(Bt1,...,tn) =
‚à´
Bt1,...,tn
p(x) dx =
t1
‚à´
‚àí‚àû
‚ãÖ‚ãÖ‚ãÖ
tn
‚à´
‚àí‚àû
p(x1, . . . , xn) dxn ‚ãÖ‚ãÖ‚ãÖdx1 .
(1.75)
Thus ‚Ñôis continuous if and only if eq. (1.75) is satisfied for all tj ‚àà‚Ñù.
Let us first give an example of a multivariate probability density function.
Example 1.8.7. Consider p : ‚Ñù3 ‚Üí‚Ñùdefined by
p(x1, x2, x3) = {48 x1 x2 x3
if 0 ‚â§x1 ‚â§x2 ‚â§x3 ‚â§1,
0
otherwise.
Of course, p(x) ‚â•0 for x ‚àà‚Ñù3. Moreover,
‚à´
‚Ñù3
p(x) dx = 48
1
‚à´
0
x3
‚à´
0
x2
‚à´
0
x1x2x3 dx1dx2dx3
= 48
1
‚à´
0
x3
‚à´
0
x3 x3
2
2
dx2 dx3 = 48
1
‚à´
0
x5
3
8 dx3 = 1 ,
hence it is a density function on ‚Ñù3. For example, if ‚Ñôis the generated probability mea-
sure, then
‚Ñô([0, 1/2]3) = 48
1/2
‚à´
0
x3
‚à´
0
x2
‚à´
0
x1x2x3 dx1dx2dx3 = 1
26 = 1
64 .
There exists a quite general approach to construct multivariate density functions.
Proposition 1.8.8. Let p1, . . . , pn be (univariate) density functions. Define a function
p : ‚Ñùn ‚Üí‚Ñùby
p(x) = p1(x1) ‚ãÖ‚ãÖ‚ãÖpn(xn) ,
x = (x1, . . . , xn) ‚àà‚Ñùn .
(1.76)
Then p is a multivariate distribution density.
Proof. Of course, we have p(x) ‚â•0 for all x ‚àà‚Ñùn. Moreover, an application of Proposi-
tion A.5.5 (note that all pj are nonnegative) implies that
‚à´
‚Ñùn
p(x)dx =
‚àû
‚à´
‚àí‚àû
‚ãÖ‚ãÖ‚ãÖ
‚àû
‚à´
‚àí‚àû
[p1(x1) ‚ãÖ‚ãÖ‚ãÖpn(xn)] dxn ‚ãÖ‚ãÖ‚ãÖdx1

1.8 Multivariate continuous distributions
‡±™
83
= (
‚àû
‚à´
‚àí‚àû
p1(x1)dx1) ‚ãÖ‚ãÖ‚ãÖ(
‚àû
‚à´
‚àí‚àû
pn(xn)dxn) = 1 ‚ãÖ‚ãÖ‚ãÖ1 = 1 .
Thus, p is a density as asserted.
Remark 1.8.9. Observe that not all multivariate densities p may be represented as a
product of univariate ones as stated in eq. (1.76). As can be seen easily, the function p
in Example 1.8.7 may not be written as a product of three univariate densities. We will
investigate this and related problems more thoroughly in Section 1.9.3.
1.8.2 Multivariate uniform distribution
Our next aim is the introduction and investigation of a special multivariate distribution,
the uniform distribution on a set K in ‚Ñùn. To do so, we remember how we defined the
uniform distribution on an interval I in ‚Ñù. Its density p is given by
p(s) = {
1
|I|
if s ‚ààI,
0
if s ‚àâI.
Here |I| denotes the length of the interval I. Let now K ‚äÇ‚Ñùn be bounded. In order to in-
troduce a similar density for the uniform distribution on K, the length of the underlying
set has to be replaced by the n-dimensional volume, which we will denote by voln(K).
But how is this volume defined? To answer this question, let us first investigate a box
Q represented as in eq. (1.73). It is immediately clear that its n-dimensional volume is
evaluated by
voln(Q) =
n
‚àè
i=1
(bi ‚àíai) .
If n = 1, then Q is an interval and its one-dimensional volume is nothing else as its
length. For n = 2 the box Q is the rectangle [a1, b1] √ó [a2, b2] and
vol2(Q) = (b1 ‚àía1)(b2 ‚àía2)
coincides with the area of Q. If n = 3, then vol3(Q) is the ordinary volume of bodies in ‚Ñù3.
For arbitrary K ‚äÇ‚Ñùn, the definition of its volume voln(K) is more involved. Let us
shortly sketch one way how this can be done. Setting
voln(K) := inf{
‚àû
‚àë
j=1
voln(Qj) : K ‚äÜ
‚àû
‚ãÉ
j=1
Qj , Qj box} ,
(1.77)
at least for Borel sets K ‚äÜ‚Ñùn, a suitable volume is defined. Compare Figure 1.30 to get
an impression how the two-dimensional volume of an ellipse is evaluated.

84
‡±™
1 Probabilities
Figure 1.30: An ellipse K covered by boxes, here squares. The smaller the covering boxes, the better the
approximation of vol2(K).
In the case of ‚Äúordinary‚Äù sets such as balls, ellipsoids, or similar bodies, this ap-
proach leads to the known values. The reason is the basic formula
voln(K) = ‚à´‚ãÖ‚ãÖ‚ãÖ‚à´
K
1 dxn ‚ãÖ‚ãÖ‚ãÖdx1
(1.78)
valid for Borel sets K ‚äÜ‚Ñùn. For example, if K is the square in ‚Ñù2 (see Figure 1.31) with
corner points (1, 0), (0, 1), (‚àí1, 0), and (0, ‚àí1), then
vol2(K) = ‚à¨
K
1 dx2dx1 =
0
‚à´
‚àí1
1+x1
‚à´
‚àíx1‚àí1
dx2dx1 +
1
‚à´
0
1‚àíx1
‚à´
x1‚àí1
dx2dx1
= 2
0
‚à´
‚àí1
(x1 + 1)dx1 + 2
1
‚à´
0
(1 ‚àíx1)dx1
= 2[x2
1
2 + x1]
0
‚àí1
+ 2[x1 ‚àíx2
1
2 ]
1
0
= 2 .
Figure 1.31: The square K ‚äÜ‚Ñù2 with corner points (1, 0), (0, 1), (‚àí1, 0), and (0, ‚àí1).

1.8 Multivariate continuous distributions
‡±™
85
Remark 1.8.10. The n-dimensional volume shares several important properties:
1.
It is invariant under shifts. That is, if
K + z = {x + z : x ‚ààK}
denotes the shift of K by z ‚àà‚Ñùn, then it follows that
voln(K + z) = voln(K) ,
z ‚àà‚Ñùn .
2.
Unitary transformations of a set do not change its volume. More precisely, if
U : ‚Ñùn ‚Üí‚Ñùn is a unitary transformation (cf. (A.24) for the definition), then
voln(U(K)) = voln(K) .
3.
The n-dimensional volume is homogeneous of power n under dilation. Thus, given
Œ± > 0, let Œ± K = {Œ± x : x ‚ààK} be the stretched (if Œ± > 1) or shrunk (if Œ± < 1) set K.
Then it follows that
voln(Œ± K) = Œ±n voln(K) .
Example 1.8.11. Let Kn(r) be the n-dimensional ball of radius r > 0, that is,
Kn(r) = {x ‚àà‚Ñùn : |x| ‚â§r} = {(x1, . . . , xn) ‚àà‚Ñùn : x2
1 + ‚ãÖ‚ãÖ‚ãÖ+ x2
n ‚â§r2} .
If
Vn(r) := voln(Kn(r)) ,
r > 0 ,
denotes the n-dimensional volume of this ball, property (3) in Remark 1.8.10 implies
Vn(r) = Vn ‚ãÖrn, where Vn = Vn(1). But for Kn = Kn(1), eq. (1.78) gives
Vn = ‚à´‚ãÖ‚ãÖ‚ãÖ‚à´
Kn
1 dxn ‚ãÖ‚ãÖ‚ãÖdx1 =
1
‚à´
‚àí1
[
‚à´‚ãÖ‚ãÖ‚ãÖ‚à´
{x2
2+‚ãÖ‚ãÖ‚ãÖ+x2
n‚â§1‚àíx2
1}
1 dxn ‚ãÖ‚ãÖ‚ãÖdx2]dx1
=
1
‚à´
‚àí1
Vn‚àí1(‚àö1 ‚àíx2
1) dx1 =
1
‚à´
‚àí1
Vn‚àí1(‚àö1 ‚àís2) ds .
Hence, due to Vn‚àí1(r) = rn‚àí1 Vn‚àí1(1) = rn‚àí1 Vn‚àí1, we obtain
Vn = Vn‚àí1 ‚ãÖ
1
‚à´
‚àí1
(1 ‚àís2)
(n‚àí1)/2 ds = 2 Vn‚àí1 ‚ãÖ
1
‚à´
0
(1 ‚àís2)
(n‚àí1)/2 ds .

86
‡±™
1 Probabilities
The change of the variables s = y1/2, thus ds = 1
2 y‚àí1/2 dy, yields
Vn = Vn‚àí1 ‚ãÖ
1
‚à´
0
y‚àí1/2(1 ‚àíy)(n‚àí1)/2 dy = Vn‚àí1 B(1
2 , n + 1
2
)
= ‚àöœÄ Vn‚àí1
Œì( n+1
2 )
Œì( n
2 + 1) .
Hereby we used eq. (1.62), as well as Œì(1/2) = ‚àöœÄ. Starting with V1 = 2, a recursive
application of the last formula finally leads to
voln(Kn(r)) = Vn(r) =
œÄn/2
Œì( n
2 + 1) rn = 2 œÄn/2
n Œì( n
2 ) rn ,
r > 0 .
If we distinguish between even and odd dimensions, properties of the Œì function
imply
V2k(r) = œÄk
k! r2k
and
V2k+1(r) =
2k+1œÄk
(2k + 1)!! r2k+1,
where (2k + 1)!! = 1 ‚ãÖ3 ‚ãÖ5 ‚ãÖ‚ãÖ‚ãÖ(2k ‚àí1)(2k + 1). The first volumes are (see Figure 1.32)
V1 = 2,
V2 = œÄ,
V3 = 4 œÄ
3 ,
V4 = œÄ2
2 ,
V5 = 8 œÄ2
15 ,
V6 = œÄ3
6 ,
V7 = 16 œÄ3
105 ,
V8 = œÄ4
24 .
Figure 1.32: The volume of the unit ball as a function of the dimension. It attains its maximal value in di-
mension 5.

1.8 Multivariate continuous distributions
‡±™
87
After the question about the volume is settled, we are now in the position to in-
troduce the uniform distribution on bounded Borel sets in ‚Ñùn. Thus let K ‚äÜ‚Ñùn be a
bounded Borel set in ‚Ñùn with volume voln(K) > 0. Define p : ‚Ñùn ‚Üí‚Ñùby
p(x) := {
1
voln(K)
if x ‚ààK,
0
if x ‚àâK.
(1.79)
Proposition 1.8.12. The function p defined by eq. (1.79) is an (n-dimensional) probability
density function.
Proof. By virtue of eq. (1.78), one has
‚à´
‚Ñùn
p(x) dx = ‚à´
K
1
voln(K) dx =
1
voln(K) ‚à´‚ãÖ‚ãÖ‚ãÖ‚à´
K
1 dxn ‚ãÖ‚ãÖ‚ãÖdx1
= voln(K)
voln(K) = 1 .
Since p(x) ‚â•0 if x ‚àà‚Ñùn, as asserted, p is a probability density function.
Definition 1.8.13. The probability measure ‚Ñôon (‚Ñùn, ‚Ñ¨(‚Ñùn)) with density p given by eq. (1.79) is said to
be the (multivariate) uniform distribution on K.
Let ‚Ñôbe the uniform distribution on K. How do we get ‚Ñô(B) for a Borel set B? Let us first
assume B ‚äÜK. Then
‚Ñô(B) = ‚à´
B
p(x) dx =
1
voln(K) ‚à´‚ãÖ‚ãÖ‚ãÖ‚à´
B
1 dxn ‚ãÖ‚ãÖ‚ãÖdx1 = voln(B)
voln(K) .
If B ‚äÜ‚Ñùn is arbitrary, that is, B is not necessarily a subset of K, from ‚Ñô(B) = ‚Ñô(B ‚à©K) it
follows that
‚Ñô(B) = voln(B ‚à©K)
voln(K)
.
If n = 1 and K is an interval, the latter formula coincides with eq. (1.46).
Example 1.8.14. Two friends agree to meet in a restaurant between 1 and 2 pm. Both
friends go to the restaurant randomly during this hour. After they arrive, they wait 20
minutes each. What is the probability that they meet each other?
Answer: Let t1 be the moment when the first of the two friends enters the restaurant,
while t2 is the arrival time of the second one. They arrive independently of each other,
thus we may assume that the point t := (t1, t2) is uniformly distributed in the square
Q := [1, 2]2. Observing that 20 minutes are a third of an hour, they meet each other if and
only if 1 ‚â§t1, t2 ‚â§2 and |t1 ‚àít2| ‚â§1/3.

88
‡±™
1 Probabilities
Setting B := {(t1, t2) ‚àà‚Ñù2 : |t1 ‚àít2| ‚â§1/3}, it is easy to see that vol2(B ‚à©Q) = 5/9
(see Figure 1.33). Hence, if ‚Ñôis the uniform distribution on Q, because of vol2(Q) = 1, it
follows that ‚Ñô(B) = 5/9. Therefore, the probability that the friends meet is 5/9.
Figure 1.33: The two friends meet if (t1, t2) belongs to the gray region.
Example 1.8.15. Place n particles independently of each other into a ball KR of radius
R > 0 according to the uniform distribution. Let Kr be a smaller ball of radius r > 0
contained in KR. Find the probability that exactly k of the n particles are inside Kr for
some k = 0, . . . , n.
Answer: In the first step, we determine the probability that a single particle is in Kr.
Since we assumed that the particles are uniformly distributed in KR, this probability
equals
p := vol3(Kr)
vol3(KR) = (4/3)œÄr3
(4/3)œÄR3 = ( r
R)
3
.
For each of the n particles, this p is the ‚Äúsuccess‚Äù probability to be inside Kr, hence the
number of particles in Kr is Bn,p-distributed with p = (r/R)3. Thus,
‚Ñô{k particles in Kr} = Bn,p({k}) = (n
k)( r
R)
3k
(R ‚àír
R
)
3(n‚àík)
,
k = 0, . . . , n .
If the number n of particles is big and r is much smaller than R, then the number of
particles in Kr is approximately PoisŒª distributed, where Œª = n p = nr3
R3 . In other words,
‚Ñô{k particles in Kr} ‚âà1
k!(nr3
R3 )
k
e‚àínr3/R3
.
Example 1.8.16 (Buffon‚Äôs needle test). Take a needle of length a < 1 and throw it ran-
domly on a lined sheet of paper. Say the distance between two lines on the paper is 1.
Find the probability that the needle cuts a line.

1.8 Multivariate continuous distributions
‡±™
89
Answer: What is random in this experiment? Choose the two lines such that between
them the midpoint of the needle lies. Let x ‚àà[0, 1] be the distance of the midpoint of the
needle to the lower line. Furthermore, denote by Œ∏ ‚àà[‚àíœÄ/2, œÄ/2] the angle of the needle
to a line perpendicular to the lines on the paper. For example, if Œ∏ = 0, then the needle
is perpendicular to the lines on the paper while for Œ∏ = ¬±œÄ/2 it lies parallel.
Hence, to throw a needle randomly is equivalent to choosing a point (Œ∏, x) uniformly
distributed in K = [‚àíœÄ/2, œÄ/2] √ó [0, 1].
The needle cuts the lower line (compare Figure 1.34) if and only if a
2 cos Œ∏ ‚â•x, and
it cuts the upper line provided that a
2 cos Œ∏ ‚â•1 ‚àíx. If the set A is as in Figure 1.35 defined
by
A = {(Œ∏, x) ‚àà[‚àíœÄ/2, œÄ/2] √ó [0, 1] : x ‚â§a
2 cos Œ∏ or 1 ‚àíx ‚â§a
2 cos Œ∏} ,
then we get
‚Ñô{the needle cuts a line} = ‚Ñô(A) = vol2(A)
vol2(K) = vol2(A)
œÄ
.
Figure 1.34: A needle of length a < 1 hits the lower line. Here 0 ‚â§x ‚â§1 denotes the distance of its
midpoint to the lower line and Œ∏ is its angle to the perpendicular line.
But it follows that
vol2(A) = 2
œÄ/2
‚à´
‚àíœÄ/2
a
2 cos Œ∏ dŒ∏ = 2a ,
hence
‚Ñô(A) = 2a
œÄ .

90
‡±™
1 Probabilities
Figure 1.35: The set A where the needle cuts the lower or upper line, respectively. Since a < 1, the lower
and upper parts of A do not overlap.
Remark 1.8.17. Suppose we throw the same needle n times. Let rn be the relative fre-
quency of the occurrence of A, that is,
rn = Number of throws where the needle cuts a line
n
.
As mentioned in Section 1.1.3, if n ‚Üí‚àû, then rn approaches ‚Ñô(A) = 2a
œÄ . Thus for large n,
we have rn ‚âà2a
œÄ or, equivalently, œÄ ‚âà2a
rn . Consequently, throwing the needle sufficiently
often, 2a
rn should be close16 to œÄ.
Summary: The uniform distribution ‚Ñôon a bounded Borel set K ‚äÜ‚Ñùn with voln(K) > 0 is defined by
‚Ñô(B) = voln(B ‚à©K)
voln(K)
,
B ‚àà‚Ñ¨(‚Ñùn) .
1.9 Products of probability spaces‚àó
1.9.1 Product œÉ-fields and measures
Suppose we execute n (maybe different) random experiments so that the outcomes do
not depend on each other. In order to describe these n experiments, two different ap-
proaches are possible. Firstly, we record each single result separately, that is, we have n
16 For example, in 1901 Mr. Lazzerini threw a needle of length a = 5/6 exactly 3408 times. In 1808 of the
cases, the needle cut a line, leading to 3.1416 as an approximate value of œÄ.

1.9 Products of probability spaces‚àó
‡±™
91
(maybe different) probability spaces (Œ©1, ùíú1, ‚Ñô1) to (Œ©n, ùíún, ‚Ñôn) modeling the outcomes
of the first up to the nth experiment.
A second possible approach is that we combine the n experiments into a single one.
Thus, instead of n different outcomes œâ1 to œân, we observe now a single vector œâ =
(œâ1, . . . , œân). The sample space in this approach is given by Œ© = Œ©1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Œ©n.
Example 1.9.1. When rolling a die n times, the outcome is a series of n numbers œâ1 to
œân, each in {1, . . . , 6}. Now, imagine we have a die with 6n equally likely faces. On these
faces, all possible sequences of length n with entries from {1, . . . , 6} are written. Roll this
die once. The first experiment may be described by n probability spaces, one for each
roll. The second experiment involves only one probability space. Nevertheless, both ex-
periments lead to the same result, a random sequence of numbers from 1 to 6.
It is intuitively clear that both approaches to this experiment (rolling a die n
times) are equivalent; they differ only by the point of view. But how to come from
one model to the other? One direction is immediately clear. If the random result is a
vector œâ = (œâ1, . . . , œân), then its coordinates may be taken as the results of the single
experiments.17 But how about the other direction? That is, we are given n probability
spaces (Œ©1, ùíú1, ‚Ñô1), . . . , (Œ©n, ùíún, ‚Ñôn) and have to construct a model for the joint execution
of these experiments.
Of course, the ‚Äúnew‚Äù sample space is
Œ© = Œ©1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Œ©n ,
(1.80)
but what are ùíúand ‚Ñô? We start with the construction of the product œÉ-field.
Definition 1.9.2. Let ùíúj be œÉ-fields on Œ©j, 1 ‚â§j ‚â§n. Set Œ© = Œ©1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Œ©n. Then
ùíú= œÉ{A1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó An : Aj ‚ààùíúj}
is called the product œÉ-field of ùíú1 to ùíún, denoted by ùíú= ùíú1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äóùíún.
Remark 1.9.3. In other words, ùíúis the smallest œÉ-field containing measurable rectangle
sets, that is, sets of the form A1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó An with Aj ‚ààùíúj, 1 ‚â§j ‚â§n.
It is easy to see that ùí´(Œ©1) ‚äó‚ãÖ‚ãÖ‚ãÖ‚äóùí´(Œ©n) = ùí´(Œ©). A more complicated example is as
follows.
Proposition 1.9.4. Suppose Œ©1 = ‚ãÖ‚ãÖ‚ãÖ= Œ©n = ‚Ñù, hence Œ© = ‚Ñùn. Then the œÉ-field ‚Ñ¨(‚Ñùn) of
Borel sets in ‚Ñùn is the n-fold product of the œÉ-fields ‚Ñ¨(‚Ñù) of Borel sets in ‚Ñù, that is,
17 Of course, one still has to verify that the distribution of the coordinates is the same as in the single
experiments. But before we can do this, we need a probability measure describing the distribution of
the vectors (cf. Proposition 1.9.10).

92
‡±™
1 Probabilities
‚Ñ¨(‚Ñùn) = ‚Ñ¨(‚Ñù) ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñ¨(‚Ñù)
‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
n times
.
Proof. We only give a sketch of the proof. Let Q be a box as in eq. (1.73). Then we have
Q = A1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó An, where the Ajs are intervals, hence in ‚Ñ¨(‚Ñù). By the construction of the
product œÉ-field, it follows that Q ‚àà‚Ñ¨(‚Ñù) ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñ¨(‚Ñù). But ‚Ñ¨(‚Ñùn) is the smallest œÉ-field
containing all boxes, which lets us conclude
‚Ñ¨(‚Ñùn) ‚äÜ‚Ñ¨(‚Ñù) ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñ¨(‚Ñù) .
The inclusion in the other direction may be proved as follows: fix a2 < b2 to an < bn and
let
ùíû1 = {C ‚àà‚Ñ¨(‚Ñù) : C √ó [a2, b2] √ó ‚ãÖ‚ãÖ‚ãÖ√ó [an, bn] ‚àà‚Ñ¨(‚Ñùn)} .
It is not difficult to prove that ùíû1 is a œÉ-field. If C = [a1, b1], then
C √ó [a2, b2] √ó ‚ãÖ‚ãÖ‚ãÖ√ó [an, bn]
is a box, thus in ‚Ñ¨(‚Ñùn). Consequently, ùíû1 contains closed intervals, hence, since ‚Ñ¨(‚Ñù)
is the smallest œÉ-field with this property, it follows ùíû1 = ‚Ñ¨(‚Ñù). This tells us that for all
B1 ‚àà‚Ñ¨(‚Ñù) and all aj < bj,
B1 √ó [a2, b2] √ó ‚ãÖ‚ãÖ‚ãÖ√ó [an, bn] ‚àà‚Ñ¨(‚Ñùn) .
In a next step, fix B1 ‚àà‚Ñ¨(‚Ñù) and a3 < b3 up to an < bn, and set
ùíû2 = {C ‚àà‚Ñ¨(‚Ñù) : B1 √ó C √ó [a3, b3] √ó ‚ãÖ‚ãÖ‚ãÖ√ó [an, bn] ‚àà‚Ñ¨(‚Ñùn)} .
By the same arguments as before, but now using the first step, we get ùíû2 = ‚Ñ¨(‚Ñù), that is,
B1 √ó B2 √ó [a3, b3] √ó ‚ãÖ‚ãÖ‚ãÖ√ó [an, bn] ‚àà‚Ñ¨(‚Ñùn)
for all B1, B2 ‚àà‚Ñ¨(‚Ñù) and aj < bj.
Iterating further, we finally obtain that for all Bj ‚àà‚Ñ¨(‚Ñù) it follows that
B1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Bn ‚àà‚Ñ¨(‚Ñùn) .
Since ‚Ñ¨(‚Ñù) ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñ¨(‚Ñù) = œÉ{B1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Bn : Bj ‚àà‚Ñ¨(‚Ñù)} is the smallest œÉ-field containing
sets B1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Bn, this implies
‚Ñ¨(‚Ñù) ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñ¨(‚Ñù) ‚äÜ‚Ñ¨(‚Ñùn)
and completes the proof.

1.9 Products of probability spaces‚àó
‡±™
93
Let us now turn to the probability measure ‚Ñôon (Œ©, ùíú) that describes the combined
experiment.
Definition 1.9.5. Let (Œ©1, ùíú1, ‚Ñô1) to (Œ©n, ùíún, ‚Ñôn) be n probability spaces. Define Œ© by eq. (1.80) and en-
dow it with the product œÉ-field ùíú= ùíú1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äóùíún. A probability measure ‚Ñôon (Œ©, ùíú) is called the product
measure of ‚Ñô1, . . . , ‚Ñôn if
‚Ñô(A1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó An) = ‚Ñô1(A1) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn(An)
for all Aj ‚ààùíúj .
(1.81)
We write ‚Ñô= ‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn and if ‚Ñô1 = ‚ãÖ‚ãÖ‚ãÖ= ‚Ñôn = ‚Ñô0 set
‚Ñô‚äón
0 := ‚Ñô0 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñô0
‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
n times
.
It is not clear at all whether product measures exist and, if this is so, whether condition
(1.81) determines them uniquely. The next result shows that the answer to both ques-
tions is affirmative. Unfortunately, the proof is too complicated to be presented here.
The idea is quite similar to that used in the introduction of volumes in eq. (1.77). The
boxes appearing there have to be replaced by rectangle sets A1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó An with Aj ‚ààùíúj
and the volume of the boxes by ‚Ñô1(A1) to ‚Ñôn(An), respectively. We refer to [Dur19, Sec-
tion 1.7], [Kle20] or [Coh13], for a detailed proof for the existence (and uniqueness) of
product measures.
Proposition 1.9.6. Let (Œ©1, ùíú1, ‚Ñô1), . . . , (Œ©n, ùíún, ‚Ñôn) be probability spaces. Define Œ© by
eq. (1.80) and let ùíúbe the product œÉ-field of the ùíújs. Then there is a unique probability
measure ‚Ñôon (Œ©, ùíú) satisfying
‚Ñô(A1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó An) = ‚Ñô1(A1) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn(An)
for all Aj ‚ààùíúj .
(1.82)
Hence, the product measure ‚Ñô= ‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn always exists and is uniquely determined
by (1.82).
Remark 1.9.7. While the product measure of rectangle sets can be evaluated directly by
eq. (1.82), it is more complicated to determine the probability for arbitrary non-rectangle
sets. Compare Figures 1.36 and 1.37.
Example 1.9.8. Let Œ©1 = Œ©2 = {1, . . . , 6} be endowed with the uniform distributions ‚Ñô1
and ‚Ñô2. That is, ‚Ñô1({œâ}) = ‚Ñô2({œâ}) = 1
6 for all œâ = 1, . . . , 6. Then
Œ© = Œ©1 √ó Œ©2 = {(œâ1, œâ2) : œâ1, œâ2 ‚àà{1, . . . , 6}} ,
and we get
(‚Ñô1 ‚äó‚Ñô2)(A1 √ó A2) = ‚Ñô1(A1) ‚ãÖ‚Ñô2(A2) = |A1|
6
‚ãÖ|A2|
6
= |A1 √ó A2|
36

94
‡±™
1 Probabilities
Figure 1.36: The product measure ‚Ñô1 ‚äó‚Ñô2 applied to rectangle sets in Œ©1 √ó Œ©2.
Figure 1.37: How to evaluate (‚Ñô1 ‚äó‚Ñô2)(K) ? Cover it in an optimal way by unions of rectangles.
for all A1 ‚äÜŒ©1 and A2 ‚äÜŒ©2. So we see that
(‚Ñô1 ‚äó‚Ñô2)(B) = |B|
36 = ‚Ñô(B)
whenever B = A1 √ó A2 is a rectangle set and where ‚Ñôdenotes the uniform distribution
on Œ© = {1, . . . , 6}2. Since ‚Ñô1 ‚äó‚Ñô2 is uniquely determined by its values at rectangle sets, it
follows that the product measure is nothing else as the uniform distribution on Œ©. For
example, this implies that
(‚Ñô1 ‚äó‚Ñô2)({(1, 1), (2, 2), . . . , (6, 6)}) = 6
36 = 1
6 .
Note that {(1, 1), . . . , (6, 6)} is no rectangle set, hence in this case, formula (1.82) does not
apply.
Corollary 1.9.9. Let ‚Ñô1, . . . , ‚Ñôn be probability measures on (‚Ñù, ‚Ñ¨(‚Ñù)). Then there is a
unique probability measure ‚Ñôon (‚Ñùn, ‚Ñ¨(‚Ñùn)) such that
‚Ñô(B1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Bn) = ‚Ñô1(B1) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn(Bn)
for all Bj ‚àà‚Ñ¨(‚Ñù) .
Proof. The proof is a direct consequence of Propositions 1.9.6 and 1.9.4. Indeed, take
‚Ñô= ‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn and observe that ‚Ñ¨(‚Ñùn) = ‚Ñ¨(‚Ñù) ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñ¨(‚Ñù).

1.9 Products of probability spaces‚àó
‡±™
95
Let us shortly come back to the question asked at the beginning of this section. Sup-
pose we observe a vector œâ = (œâ1, . . . , œân). How are the coordinates distributed?
Proposition 1.9.10. Let (Œ©, ùíú, ‚Ñô) be the product probability space of (Œ©1, ùíú1, ‚Ñô1) to
(Œ©n, ùíún, ‚Ñôn). If j ‚â§n and A ‚ààùíúj, then
‚Ñô{(œâ1, . . . , œân) ‚ààŒ© : œâj ‚ààA} = ‚Ñôj(A) .
Proof. Observe that
{(œâ1, . . . , œân) ‚ààŒ© : œâj ‚ààA} = Œ©1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Œ©j‚àí1 √ó A √ó Œ©j+1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Œ©n ,
thus eq. (1.81) implies (see Figure 1.38)
‚Ñô{(œâ1, . . . , œân) ‚ààŒ© : œâj ‚ààA}
= ‚Ñô1(Œ©1) ‚ãÖ‚ãÖ‚ãÖ‚Ñôj‚àí1(Œ©j‚àí1) ‚ãÖ‚Ñôj(A) ‚ãÖ‚Ñôj+1(Œ©j+1) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn(Œ©n) = ‚Ñôj(A),
as asserted.
Figure 1.38: The product measure ‚Ñô1 ‚äó‚Ñô2 applied to the cylindrical shaped sets A √ó Œ©2 and Œ©1 √ó B.
How do we get product measures in concrete cases? We answer this question for
discrete and continuous probability measures separately.
1.9.2 Product measures: discrete case
Let Œ©1 to Œ©n be either finite or countably infinite sets. Given probability measures ‚Ñôj
defined on ùí´(Œ©j), 1 ‚â§j ‚â§n, the following result characterizes the product measure of
the ‚Ñôjs.

96
‡±™
1 Probabilities
Proposition 1.9.11. Probability measure ‚Ñôis the product measure of ‚Ñô1, . . . , ‚Ñôn if and
only if
‚Ñô({œâ}) = P1({œâ1}) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn({œân})
for all œâ = (œâ1, . . . , œân) ‚ààŒ© .
(1.83)
Proof. One direction is easy. Indeed, if ‚Ñô= ‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn, given œâ = (œâ1, . . . , œân) ‚ààŒ© set
A = {œâ} and Aj = {œâj}. Then A = A1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó An, hence
‚Ñô({œâ}) = ‚Ñô(A) = ‚Ñô1(A1) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn(An) = P1({œâ1}) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn({œân}),
proving eq. (1.83).
To verify the other implication, let ‚Ñôbe a probability measure on (Œ©, ùí´(Œ©)) satisfy-
ing (1.83). We have to show that ‚Ñôfulfills eq. (1.81). Thus choose arbitrary Aj ‚äÜŒ©j and set
A = A1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó An. By applying eq. (1.83), it follows that
‚Ñô(A) = ‚àë
œâ‚ààA
‚Ñô({œâ}) =
‚àë
(œâ1,...,œân)‚ààA
‚Ñô({(œâ1, . . . , œân)})
=
‚àë
œâ1‚ààA1,...,œân‚ààAn
P1({œâ1}) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn({œân})
= ‚àë
œâ1‚ààA1
‚Ñô1({œâ1}) ‚ãÖ‚ãÖ‚ãÖ‚àë
œân‚ààAn
‚Ñôn({œân}) = ‚Ñô1(A1) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn(An) .
This being true for all Aj ‚äÜŒ©j shows that ‚Ñô= ‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn, and the proof is complete.
Summary: In the discrete case, the product measure is characterized as follows. Given A ‚äÜŒ©,
(‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn)(A) =
‚àë
(œâ1,...,œân)‚ààA
‚Ñô1({œâ1}) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn({œân}) .
Example 1.9.12. Suppose two players, say U and V, each simultaneously toss a biased
coin. On both coins, ‚Äú0‚Äù (failure) appears with probability 1 ‚àíp and ‚Äú1‚Äù (success) with
probability p. Whoever gets the first ‚Äú1‚Äù wins.
A pair (k, l) ‚àà‚Ñï2 occurs if player U has his first success in trial k and player V in
trial l. Each single experiment is described by the geometric distribution Gp, hence the
model for the combined experiment is (‚Ñï2, ùí´(‚Ñï2), G‚äó2
p ). Here
G‚äó2
p (A) =
‚àë
(k,l)‚ààA
Gp({k})Gp({l}) =
‚àë
(k,l)‚ààA
p2(1 ‚àíp)k+l‚àí2 ,
A ‚äÜ‚Ñï2 .
If A = {(k, k) : k ‚â•1}, that is, the game ends in a draw, then
G‚äó2
p (A) = p2
‚àû
‚àë
k=1
(1 ‚àíp)2k‚àí2 =
p2
1 ‚àí(1 ‚àíp)2 =
p
2 ‚àíp .

1.9 Products of probability spaces‚àó
‡±™
97
Consequently, with probability
1 ‚àí
p
2 ‚àíp = 2 ‚àí2p
2 ‚àíp ,
either U or V wins. Since both players have the same chance to win, it follows that U
wins with probability 1‚àíp
2‚àíp and the same is true for V.
If we analyze the result, then we see that we have three possible outcomes of the
game: U wins or V wins, each with probability 1‚àíp
2‚àíp, or the game ends in a draw with
probability
p
2‚àíp. In the case of a fair coin, that is, if p = 1/2, these three outcomes each
occur with probability 1/3. In the general setting, the following is true: the bigger the
success probability p, the more likely the game ends in a draw. On the contrary, if p is
near zero, then with great probability there will be a winner of the game.
Example 1.9.13. Toss a biased coin n times. Say the coin is labeled with ‚Äú0‚Äù and ‚Äú1‚Äù
and p ‚àà[0, 1] is the probability of the occurrence of ‚Äú1.‚Äù Recording each single result
separately, the describing probability spaces are ({0, 1}, ùí´({0, 1}), ‚Ñôj), 1 ‚â§j ‚â§n, with
‚Ñôj({1}) = p. Which probability space does the combined result describe?
Answer: Of course, the sample space is {0, 1}n endowed with œÉ-field ùí´(Œ©). Let œâ =
(œâ1, . . . , œân) be an arbitrary vector in Œ©. Then by Proposition 1.9.11, the product measure
‚Ñôof the ‚Ñôjs is characterized by
‚Ñô({œâ}) = ‚Ñô1({œâ1}) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn({œâ}) = pk(1 ‚àíp)n‚àík
where k = |{j ‚â§n : œâj = 1}| = ‚àën
j=1 œâj .
For example, tossing the coin five times, the sequence (0, 0, 1, 1, 0) occurs with prob-
ability p2(1 ‚àíp)3.
In the general case, let A be the set of sequences possessing exactly k times the num-
ber ‚Äú1.‚Äù Then
(‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn)(A) =
‚àë
(œâ1,...,œân)‚ààA
‚Ñô1({œâ1}) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn({œân})
= |A| ‚ãÖpk(1 ‚àíp)n‚àík = (n
k)pk(1 ‚àíp)n‚àík .
This is another justification for the model described by the binomial distribution Bn,p.
Example 1.9.14. Suppose we have two urns, both containing the same proportion of
white balls. Choose from each urn n balls with replacement. How likely is it to take out
the same number of white balls from each of the two urns?
Answer: The experiment is described by the product measure B‚äó2
n,p where p denotes
the proportion of white balls in each urn. Hence, the probability to observe k white balls
from the first urn and ‚Ñìfrom the second one equals
(n
k)(n
‚Ñì)pk+‚Ñì(1 ‚àíp)2n‚àík‚àí‚Ñì,
0 ‚â§k, ‚Ñì‚â§n .

98
‡±™
1 Probabilities
Consequently, if A is the event that one chooses the same number of white balls, then
B‚äó2
n,p(A) =
n
‚àë
k=0
(n
k)
2
p2k(1 ‚àíp)2n‚àí2k .
(1.84)
Look at Figure 1.39 to get an impression how the probability of the event A depends on
the success probability p. For example, if either p = 0 or p = 1, then, of course, we will
for sure see the same number of white balls out of the two urns. Argue why we get the
same probability for A when we replace p by 1 ‚àíp.
Figure 1.39: The probability to observe the same number of white balls, when choosing 20 balls, depend-
ing on the proportion p. The minimal value is 0.125371 attained at p = 0.5.
If p = 1/2, then eq. (1.84) reduces to (use formula (A.19))
1
22n
n
‚àë
k=0
(n
k)
2
=
1
22n (2n
n ) .
Remark 1.9.15. Stirling‚Äôs formula (Corollary 1.6.15) implies
1
2
1
‚àöœÄn <
1
22n (2n
n ) <
1
‚àöœÄn ,
n = 1, 2, . . .
Hence, if in both urns the number of white balls equals that of black, then for large n the
probability to observe from both urns the same number of white balls is approximately
1/‚àöœÄn.

1.9 Products of probability spaces‚àó
‡±™
99
1.9.3 Product measures: continuous case
Here we assume Œ©1 = ‚ãÖ‚ãÖ‚ãÖ= Œ©n = ‚Ñù, hence the product sample space is Œ© = ‚Ñùn. Further-
more, each Œ©j = ‚Ñùis endowed with the Borel œÉ-field. Because of Proposition 1.9.4, the
product œÉ-field on Œ© = ‚Ñùn is given by ‚Ñ¨(‚Ñùn).
The next proposition characterizes the product measure of continuous probability
measures.
Proposition 1.9.16. Let ‚Ñô1, . . . , ‚Ñôn be probability measures on (‚Ñù, ‚Ñ¨(‚Ñù)) with respective
density functions p1, . . . , pn, that is,
‚Ñôj([a, b]) =
b
‚à´
a
pj(x)dx ,
1 ‚â§j ‚â§n .
Define p : ‚Ñùn ‚Üí[0, ‚àû) by
p(x) = p1(x1) ‚ãÖ‚ãÖ‚ãÖpn(xn) ,
x = (x1, . . . , xn) ‚àà‚Ñùn .
(1.85)
Then the product measure ‚Ñô1‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn is continuous with (n-dimensional) density p defined
by (1.85). In other words, for each Borel set A ‚äÜ‚Ñùn,
(‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn)(A) = ‚à´‚ãÖ‚ãÖ‚ãÖ‚à´
‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
A
p1(x1) ‚ãÖ‚ãÖ‚ãÖpn(xn) dxn ‚ãÖ‚ãÖ‚ãÖdx1 = ‚à´
A
p(x) dx .
Proof. First note that p is a density of the product measure ‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn if
(‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn)(Q) = ‚à´
Q
p(x) dx
for all boxes Q = [a1, b1] √ó ‚ãÖ‚ãÖ‚ãÖ√ó [an, bn]. But this is an immediate consequence of
‚à´
Q
p(x)dx =
b1
‚à´
a1
‚ãÖ‚ãÖ‚ãÖ
bn
‚à´
an
p1(x1) ‚ãÖ‚ãÖ‚ãÖpn(xn)dxn ‚ãÖ‚ãÖ‚ãÖdx1
= (
b1
‚à´
a1
p1(x1)dx1) ‚ãÖ‚ãÖ‚ãÖ(
bn
‚à´
an
pn(xn)dxn) = ‚Ñô1([a1, b1]) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn([an, bn])
= (‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn)([a1, b1] √ó ‚ãÖ‚ãÖ‚ãÖ√ó [an, bn]) = (‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn)(Q).
This completes the proof.
Because of its importance, let us explain through several examples how Proposi-
tion 1.9.16 applies. Further applications, for example, the characterization of indepen-
dent random variables, will follow in Sections 3 and 8.

100
‡±™
1 Probabilities
Example 1.9.17. Let the probability measures ‚Ñôj, 1 ‚â§j ‚â§n, be uniform distributions on
[Œ±j, Œ≤j]. Thus
pj(x) =
{
{
{
1
Œ≤j‚àíŒ±j
if Œ±j ‚â§x ‚â§Œ≤j,
0
otherwise,
henceforth, if x = (x1, . . . , xn), then
p(x) = p1(x1) ‚ãÖ‚ãÖ‚ãÖpn(xn) =
{
{
{
1
‚àèj‚â§n(Œ≤j‚àíŒ±j)
if x ‚ààK,
0
otherwise.
Here K ‚äÜ‚Ñùn is the box [Œ±1, Œ≤1]√ó‚ãÖ‚ãÖ‚ãÖ√ó[Œ±n, Œ≤n]. Since ‚àèj‚â§n(Œ≤j ‚àíŒ±j) = voln(K), it follows
that the product measure ‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn is nothing else as the (n-dimensional) uniform
distribution on K as introduced in18 Definition 1.8.13.
Summary: The product measure of n uniform distributions on intervals [Œ±j, Œ≤j] is the uniform distribution
on the box [Œ±1, Œ≤1] √ó ‚ãÖ‚ãÖ‚ãÖ√ó [Œ±n, Œ≤n].
Example 1.9.18. Let ‚Ñôbe the uniform distribution on the unit sphere
K = {(x1, x2) ‚àà‚Ñù2 : x2
1 + x2
2 ‚â§1} .
That is,
‚Ñô(A) = vol2(A ‚à©K)
œÄ
,
A ‚àà‚Ñ¨(‚Ñù2) .
The density p of ‚Ñôis given by
p(x1, x2) = {
1
œÄ
if x2
1 + x2
2 ‚â§1,
0
otherwise.
Then there are no functions p1, p2 on ‚Ñùfor which
p(x1, x2) = p1(x1) ‚ãÖp2(x2) ,
x1, x2 ‚àà‚Ñù.
Indeed, if such a representation were to exist, then p2(x2) = 0 if x2
1 > 1 ‚àíx2
1 for all
‚àí1 ‚â§x1 ‚â§1. Hence, p2(x2) = 0 for all x2. This contradicts the representation of the
density p. Consequently, ‚Ñôis not a product measure.
18 This result was already used in Example 1.8.14. Indeed, the arrival times t1 and t2 were described
by the uniform distributions on [1, 2], thus the pair t = (t1, t2) is distributed according to the product
measure, which is the uniform distribution on [1, 2] √ó [1, 2]. Similarly, in Example 1.8.16, we applied that
the pair (Œ∏, x) is uniformly distributed on [‚àíœÄ/2, œÄ/2] √ó [0, 1].

1.9 Products of probability spaces‚àó
‡±™
101
Example 1.9.19. Assume now ‚Ñô1 = ‚ãÖ‚ãÖ‚ãÖ= ‚Ñôn = EŒª, that is, we want to describe the
product of n exponential distributions with parameter Œª > 0. Since pj(s) = Œªe‚àíŒªs if s ‚â•0
and pj(s) = 0 if s < 0, their product measure E‚äón
Œª
possesses the density
p(s1, . . . , sn) = {Œªn e‚àíŒª(s1+‚ãÖ‚ãÖ‚ãÖ+sn)
if s1, . . . , sn ‚â•0,
0
otherwise.
Which random experiment does E‚äón
Œª
describe? Suppose we have n light bulbs of the
same type with lifetime distributed according to EŒª. Switch on all n bulbs at once and
record the times t1, . . . , tn where the first bulb, the second, and so on, burns out. If
t = (t1, . . . , tn) ‚àà‚Ñùn denotes the generated vector of these times, then for Borel sets
A ‚äÜ[0, ‚àû)n,
‚Ñô{t ‚ààA} = E‚äón
Œª (A) = Œªn ‚à´
A
e‚àíŒª(s1+‚ãÖ‚ãÖ‚ãÖ+sn)ds1 ‚ãÖ‚ãÖ‚ãÖdsn .
For example, if we want to compute the probability of
A := {(t1, . . . , tn) : 0 ‚â§t1 ‚â§‚ãÖ‚ãÖ‚ãÖ‚â§tn} ,
that is, that the second bulb burns longer than the first, the third longer than the second,
and so on, then
E‚äón
Œª (A) = Œªn
‚àû
‚à´
0
e‚àíŒªsn
sn
‚à´
0
e‚àíŒªsn‚àí1
sn‚àí1
‚à´
0
‚ãÖ‚ãÖ‚ãÖ
s3
‚à´
0
e‚àíŒªs2
s2
‚à´
0
e‚àíŒªs1 ds1 ‚ãÖ‚ãÖ‚ãÖdsn .
Iterative integration leads to E‚äón
Œª (A) = 1/n!. This is more or less obvious by the following
observation. Each ordering of the failure times is equally likely. And since there are n!
different ways to order these times, each ordering has probability 1/n!. In particular, this
is true for the ordering t1 ‚â§‚ãÖ‚ãÖ‚ãÖ‚â§tn.
Next we ask how likely is it that all n bulbs still burn at time T > 0. That is, we ask
for the probability E‚äón
Œª (B) where B = [T, ‚àû)n. The properties of product measures imply
that
E‚äón
Œª (B) = EŒª([T, ‚àû)) ‚ãÖ‚ãÖ‚ãÖEŒª([T, ‚àû))
‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
n
= enŒªT .
Consequently, the probability that at least one of the n bulbs burns out before time T > 0
equals 1 ‚àíenŒªT. In other words, if we say that the system of n bulbs becomes defective if
at least one bulb burns out, then the lifetime of this system is exponentially distributed
with parameter nŒª.
Next we give another example of a product measure that will play a crucial role in
Sections 6 and 8.

102
‡±™
1 Probabilities
Example 1.9.20. Let ‚Ñô1, . . . , ‚Ñôn be standard normal distributions. The corresponding
densities are
pj(xj) =
1
‚àö2œÄ
e‚àíx2
j /2 ,
1 ‚â§j ‚â§n .
Thus, by eq. (1.85), the density p of their product ùí©(0, 1)‚äón coincides with
p(x) =
1
(2œÄ)n/2 e‚àí‚àën
j=1 x2
j /2 =
1
(2œÄ)n/2 e‚àí|x|2/2 ,
where |x| = (‚àën
j=1 x2
j )1/2 denotes the Euclidean distance of the vector x to 0 (see Sec-
tion A.4).
See Figure 1.40 for the visualization of the density p in the case n = 2.
Figure 1.40: The density of the two-dimensional standard normal distribution.
Definition 1.9.21. The probability measure ùí©(0, 1)‚äón on ‚Ñ¨(‚Ñùn) is called the n-dimensional, or multi-
variate, standard normal distribution. It is described by
ùí©(0, 1)‚äón(B) =
1
(2œÄ)n/2 ‚à´
B
e‚àí|x|2/2 dx .
For example, if K ‚äÜ‚Ñù2 denotes a circle of radius 1, then this leads to the following
integral:
ùí©(0, 1)‚äó2(K) = 1
2œÄ ‚à¨
K
e‚àíx2
1/2e‚àíx2
2/2dx1dx2
= 1
œÄ
1
‚à´
‚àí1
e‚àíx2
1/2
‚àö1‚àíx2
1
‚à´
0
e‚àíx2
2/2dx2dx1
= 2
œÄ
1
‚à´
0
e‚àíx2
1/2
‚àö1‚àíx2
1
‚à´
0
e‚àíx2
2/2dx2dx1 ‚âà0.393 .

1.10 Problems
‡±™
103
Example 1.9.22. Finally, we describe the n-fold product measure of general normal dis-
tributions ùí©(Œºj, œÉ2
j ) with Œºj ‚àà‚Ñùand œÉj
2 > 0. The densities are
pj(xj) =
1
‚àö2œÄ œÉj
e‚àí(xj‚àíŒºj)2/2œÉ2
j ,
1 ‚â§j ‚â§n .
Hence, the product measure ùí©(Œº1, œÉ2
1) ‚äó‚ãÖ‚ãÖ‚ãÖ‚äóùí©(Œºn, œÉ2
n) possesses the density
p(x) =
1
(2œÄ)n/2œÉ1 ‚ãÖ‚ãÖ‚ãÖœÉn
exp(‚àí
n
‚àë
j=1
(xj ‚àíŒºj)2
2œÉ2
j
) ,
x = (x1, . . . , xn) ‚àà‚Ñùn .
In particular, if ùí©(Œº1, œÉ2
1) = ‚ãÖ‚ãÖ‚ãÖ= ùí©(Œºn, œÉ2
n) = ùí©(Œº, œÉ2) by
n
‚àë
j=1
(xj ‚àíŒºj)2
2œÉ2
j
= |x ‚àí‚ÉóŒº|2
2œÉ2
,
where ‚ÉóŒº = (Œº, . . . , Œº), the n-fold product measure of ùí©(Œº, œÉ2) acts as follows:
ùí©(Œº, œÉ2)
‚äón(B) =
1
(2œÄ)n/2œÉn ‚à´
B
e‚àí|x‚àí‚ÉóŒº|2/2œÉ2
dx ,
B ‚àà‚Ñ¨(‚Ñùn) .
(1.86)
Summary: Let (Œ©1, ùíú1, ‚Ñô1) up to (Œ©n, ùíún, ‚Ñôn) be probability spaces. Then there exists a unique probability
measure ‚Ñô= ‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn (the product measure) on Œ© = Œ©1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Œ©n such that
‚Ñô(A1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó An) = ‚Ñô1(A1) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn(An) ,
Aj ‚ààùíúj .
In the discrete case, the product measure ‚Ñôis described by
‚Ñô(A) =
‚àë
(œâ1,...,œân)‚ààA
‚Ñô1({œâ1}) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn({œân}) ,
A ‚äÜŒ© ,
while for continuous ‚Ñôj with densities pj the product measure is given by
‚Ñô(B) = ‚à´‚ãÖ‚ãÖ‚ãÖ‚à´
B
p1(x1) ‚ãÖ‚ãÖ‚ãÖpn(xn) dxn ‚ãÖ‚ãÖ‚ãÖdx1 ,
B ‚àà‚Ñ¨(‚Ñùn) .
1.10 Problems
Problem 1.1. Let A, B, and C be three events in a sample space Œ©. Express the following
events in terms of these sets:
‚Äì
Only A occurs.
‚Äì
A and B occur, but C does not.
‚Äì
At least one of the three events occurs.

104
‡±™
1 Probabilities
‚Äì
At least two of the events occur.
‚Äì
At most one of the three events occurs.
‚Äì
None of the events occur.
‚Äì
Not more than two of the events occur.
‚Äì
Exactly two of the events occur.
Problem 1.2. Suppose an urn contains black and white balls. Successively one draws n
balls out of the urn. The event Aj occurs if the ball drawn in the jth trial is white. Hereby
1 ‚â§j ‚â§n. Express the following events B1, . . . , B4 in terms of the Ajs:
B1 = {All drawn balls are white},
B2 = {At least one of the balls is white},
B3 = {Exactly one of the drawn balls is white},
B4 = {All n balls possess the same color}.
Determine the cardinalities |Bj|, j = 1, . . . , 4.
Problem 1.3. Argue why for every t ‚àà‚Ñùthe elementary event {t} is a Borel set. What is
wrong in the following argument? Since for any set B ‚äÜ‚Ñùone has
B = ‚ãÉ
t‚ààB
{t} ,
every subset B of ‚Ñùis union of Borel sets. Because ‚Ñ¨(‚Ñù) is a œÉ-field, the union of Borel
sets is a Borel set as well. Thus, any B ‚äÜ‚Ñùis a Borel set.
Problem 1.4. Suppose ‚Ñôis a œÉ-additive mapping from a œÉ-field ùíúto [0, 1] with ‚Ñô(Œ©) = 1.
Show that then necessarily ‚Ñô(0) = 0. Consequently, whenever a œÉ-additive mapping ‚Ñô
satisfies ‚Ñô(Œ©) = 1, then it is a probability measure.
Problem 1.5. Let ‚Ñôbe a probability measure on (Œ©, ùíú). Given A, B ‚ààùíú, show that
‚Ñô(AŒîB) = ‚Ñô(A) + ‚Ñô(B) ‚àí2‚Ñô(A ‚à©B) .
Problem 1.6. The events A and B possess the probabilities ‚Ñô(A) = 1/3 and ‚Ñô(B) = 1/4.
Moreover, we know that ‚Ñô(A ‚à©B) = 1/6. Compute ‚Ñô(Ac), ‚Ñô(Ac ‚à™B), ‚Ñô(A ‚à™Bc), ‚Ñô(A ‚à©Bc),
‚Ñô(AŒîB), and ‚Ñô(Ac ‚à™Bc).
Problem 1.7 (Inclusion‚Äìexclusion formula). Let (Œ©, ùíú, ‚Ñô) be a probability space and let
A1, . . . , An ‚ààùíúbe some (not necessarily disjoint) events. Prove that
‚Ñô(
n
‚ãÉ
j=1
Aj) =
n
‚àë
k=1
(‚àí1)k+1
‚àë
1‚â§j1<‚ãÖ‚ãÖ‚ãÖ<jk‚â§n
‚Ñô(Aj1 ‚à©‚ãÖ‚ãÖ‚ãÖ‚à©Ajk) .
Hint: One way to prove this is by induction over n, thereby using Proposition 1.2.3.

1.10 Problems
‡±™
105
Problem 1.8. Use Problem 1.7 to investigate the following question: The numbers from
1 to n are ordered randomly. All orderings are equally likely. What is the probability that
there exists an integer m ‚â§n so that m is at position m of the ordering? Determine the
limit of this probability as n ‚Üí‚àû.
Another version of this problem is as follows. Suppose n persons attend a Christmas
party. Each of the n participants brings a present with him. These presents are collected,
mixed, and then randomly distributed among the guests. Compute the probability that
at least one of the participants gets his own present.
Problem 1.9. Suppose there are N balls in an urn; k are white, l are red, and m are black.
Thus, k +l +m = N. Choose n balls out of the urn. Find a formula for the probability that
among the n chosen balls are those of all three colors. Investigate this problem if
1.
the chosen ball is always replaced and
2.
if n ‚â§N and the balls are not replaced.
Hint: If A is the event that all three colors appear then compute ‚Ñô(Ac). To this end, write
Ac = A1 ‚à™A2 ‚à™A3 with suitable Ajs and apply Proposition 1.2.4.
Problem 1.10. As in Example 1.4.19, choose 9 balls with replacement out of an urn con-
taining 3 white, 5 red and 4 black balls. How likely is it that among the 9 chosen balls are
those of all three colors?
Problem 1.11. Suppose events A and B occur both with probability 1/2. Prove that then
‚Ñô(A ‚à™B) = ‚Ñô(Ac ‚à™Bc) .
(1.87)
Does (1.87) remain valid assuming ‚Ñô(A) + ‚Ñô(B) = 1 instead of ‚Ñô(A) = ‚Ñô(B) = 1
2?
Problem 1.12. Three men and three women sit down randomly on six chairs in a row.
Find the probability that the three men and the three women sit side by side. What is
the probability that next to each woman sits a man (to the right or to the left)?
Problem 1.13. Let (Œ©, ùíú, ‚Ñô) be a probability space. Prove the following: Whenever
events A1, A2, . . . in ùíúsatisfy ‚Ñô(A1) = ‚Ñô(A2) = ‚ãÖ‚ãÖ‚ãÖ= 1, then this implies
‚Ñô(
‚àû
‚ãÇ
j=1
Aj) = 1 .
Problem 1.14 (Paradox of Chevalier de M√©r√©). Chevalier de M√©r√© mentioned that when
rolling three fair indistinguishable dice there are 6 different possibilities for obtaining
either 11 or 12 as the sum. Thus he concluded that both events (sum equals 11 or sum
equals 12) should be equally likely. But experiments showed that this is not the case.
Why was he wrong and what are the correct probabilities for both events?

106
‡±™
1 Probabilities
Problem 1.15. A man has forgotten an important phone number. He only remembers
that the seven-digit number contained three times ‚Äú1,‚Äù and ‚Äú4‚Äù and ‚Äú6‚Äù twice each. He
dials the seven numbers in random order. Find the probability that he dialed the correct
one.
Problem 1.16. In an urn there are n black and m red balls. One successively draws all
n + m balls (without replacement). What is the probability that the ball chosen last is
red?
Problem 1.17. A man has in his pocket n keys to open a door. Only one of the keys fits. He
tries the keys one after another until he has chosen the correct one. Given an integer k,
compute the probability that the correct key is the one chosen in the kth trial.
Evaluate this probability in each of the two following cases:
‚Äì
The man always discards wrong keys.
‚Äì
The man does not discard them, that is, he puts back wrong keys.
Problem 1.18 (Monty Hall problem). At the end of a quiz, the winner has the choice be-
tween three doors, say A, B, and C. Behind two of the doors there is a goat, behind the
third one is a car. His prize is what is behind the chosen door.
Say the winner has chosen door A. Then the quizmaster (who knows what is behind
each of the three doors) opens one of the two remaining doors (in our case either door
B or door C) and shows that there is a goat behind it. After that the quizmaster asks the
candidate whether or not he wants to revise his decision, that is, for example, if B was
opened, to switch from A to C, or if he furthermore chooses door A.
Find the probabilities to win the car in both cases (switching or nonswitching).
Problem 1.19. In a lecture room there are N students. Evaluate the probability that at
least two of the students were born on the same day of a year (day and month of their
births are the same, but not necessarily the year). Hereby disregard leap years and as-
sume that all days in a year are equally likely. How big must N be in order that this
probability is greater than 1/2?
Hint: Try to evaluate the probability of the complementary event.
Problem 1.20. In an urn there are balls labeled from 0 to 6 so that all numbers are
equally likely. Choose successively and with replacement three balls. Find the proba-
bility that the three observed numbers sum up to 6.
Problem 1.21. As in Example 1.4.25, six persons enter independently of each other a
train with three coaches. How likely is it that no coach remains empty? Find the proba-
bility that there are exactly four persons in one of the three coaches.
Problem 1.22. When sending messages from A to B, on average 3 % are transmitted
falsely. Suppose 300 messages are sent. What is the probability that at least three mes-
sages are transmitted falsely? Evaluate the exact probability by using the binomial

1.10 Problems
‡±™
107
distribution as well as the approximate probability by using the Poisson distribution.
Compute the probability (exact and approximate one) that all messages arrive cor-
rectly.
Problem 1.23 (C. Huygens, 1657). How often does one have to roll two fair dice in order
to observe the sum 12 with a probability greater than or equal to 1/2?
Problem 1.24. Suppose you are given 11 tiles labeled with letters. One tile is labeled with
‚ÄúM,‚Äù two with ‚ÄúP‚Äù four tiles are labeled with ‚ÄúI,‚Äù and, finally, also four with ‚ÄúS.‚Äù Order
the tiles randomly in a row so that all orders are equally likely. Find the probability to
end up with the word ‚ÄúMISSISSIPPI.‚Äù
Problem 1.25. The number of accidents in a city per week is assumed to be Poisson
distributed with parameter 5. Find the probability that next week there will be either
two or three accidents. How likely is it that there will be no accidents?
Problem 1.26. In a room there are 12 men and 8 women. One randomly chooses 5 of the
20 persons. Given k ‚àà{0, . . . , 5}, what is the probability that among the five chosen are
exactly k women? How likely is it that among the five persons are more women than
men?
Problem 1.27. Two players A and B take turns rolling a die. The first to roll a ‚Äú6‚Äù wins.
Player A starts. Find the probability that A wins. Suppose now there is a third player C
and the order of rolling the die is given by ABCABCA. . . Find each player‚Äôs probability of
winning.
Problem 1.28. Two players, say A and B, toss a biased coin where ‚Äúhead‚Äù appears with
probability 0 < p < 1. Whoever gets the first ‚Äúhead‚Äù wins. Player A starts, then B tosses
twice, then again A once, B twice, and so on. Determine the number p for which the
game is fair, that is, the probability that A (or B) wins is 1/2.
Problem 1.29. In an urn there are 50 white and 200 red balls.
(1) Take out 10 balls with replacement. What is the probability to observe four white
balls? Give the exact value via the binomial distribution as well as the approximate
one using the related Poisson distribution.
(2) Next choose 10 balls without replacement. What is the probability to get four white
balls in this case?
(3) The number of balls in the urn is as above. But now we choose the balls with re-
placement until for the first time a white ball shows up. Find the probability of the
following events:
(a) The first white ball shows up in the fourth trial.
(b) The first white ball appears strictly after the third trial.
(c) The first white ball is observed in an even number of trials, that is, in the second,
or in the fourth, or in the sixth, and so on, trial.

108
‡±™
1 Probabilities
Problem 1.30. Place successively and independently four particles into five boxes.
Thereby each box is equally likely. Find the probabilities of the following events:
‚Äì
A := {Each box contains at most one particle} and
‚Äì
B := {All 4 particles are in the same box}.
Problem 1.31. Four students did not attend at an exam because they were on vacation
and drove home too late. Their excuse for missing the test was that they had a flat tire
on their way back. The professor tells them: ‚Äúno problem, you can make up your exam
today‚Äù. He puts the four students in separate rooms and gives each a sheet of paper with
exactly one question: ‚ÄúWhich of the four tires was flat?‚Äù How likely is it that the four
students gave the same answer?
Problem 1.32. Investigate the following generalization of Example 1.4.52: in urn U0
there are M balls and in urn U1 there are N balls for some N, M ‚â•1. Choose U0 with
probability 1 ‚àíp and U1 with probability p, and take out a ball from the chosen urn.
Given 1 ‚â§m ‚â§M, find the probability that there are m balls left in U0 when choosing the
last ball out of U1. How do these probabilities change when 1 ‚â§m ‚â§N, and we assume
that there are m balls in U1 when choosing the last ball from U0?
Problem 1.33. Let n ‚àà‚Ñï. Use properties of the gamma function to evaluate the follow-
ing integrals:
‚àû
‚à´
0
x2n e‚àíx2/2 dx
and
‚àû
‚à´
0
x2n+1 e‚àíx2/2 dx .
Problem 1.34. Prove formula (1.62) that relates the beta and gamma functions.
Hint: Start with
Œì(x)Œì(y) =
‚àû
‚à´
0
‚àû
‚à´
0
ux‚àí1vy‚àí1e‚àíu‚àív dudv
and change the variables as follows: u = f (z, t) = zt and v = g(z, t) = z(1 ‚àít), where
0 ‚â§z < ‚àûand 0 ‚â§t ‚â§1.
Problem 1.35. Prove that for integers n and k with 0 ‚â§k ‚â§n,
(n
k) =
1
(n + 1)B(n ‚àík + 1, k + 1)
where B(‚ãÖ, ‚ãÖ) denotes Euler‚Äôs beta function (cf. formula (1.61)).
Problem 1.36. Write x ‚àà[0, 1) as finite or infinite decimal fraction x = 0.x1x2 . . . with
xj ‚àà{0, . . . , 9}. Fix some m ‚àà{0, . . . , 9} and set
Aj = {x ‚àà[0, 1) : xj = m} .

1.10 Problems
‡±™
109
That is, Aj contains those real numbers for which the jth digit in its decimal expansion
equals m. For example, if m = 4, then x = 0.2534114 . . . belongs to A4 and A7. Let ‚Ñôbe the
uniform distribution on [0, 1]. Evaluate
‚Ñô(Aj)
as well as
‚Ñô(
‚àû
‚ãÇ
j=1
Aj) .
Problem 1.37. If Œ≥ > 0 and x0 ‚àà‚Ñù, set
fx0,Œ≥(x) = 1
œÄ [
Œ≥
(x ‚àíx0)2 + Œ≥2 ] ,
x ‚àà‚Ñù.
Show that fx0,Œ≥ is a probability density. Why is the generated probability measure a gen-
eralization of the Cauchy distribution as introduced in Definition 1.6.37? Compute the
distribution function of the probability measure with density fx0,Œ≥.
Problem 1.38. Let F : ‚Ñù‚Üí[0, 1] be the distribution function of a probability measure.
Show that F possesses at most countably many points of discontinuity. Conclude from
this and Proposition 1.7.16 the following: If ‚Ñôis a probability measure on ‚Ñ¨(‚Ñù), then
there are at most countably infinitely many t ‚àà‚Ñùsuch that ‚Ñô({t}) > 0.
Problem 1.39. Let Œ¶ be the distribution function of the standard normal distribution
introduced in eq. (1.70). Show the following properties of Œ¶:
1.
Œ¶(0) = 1
2.
2.
For t ‚àà‚Ñù, one has Œ¶(‚àít) = 1 ‚àíŒ¶ (t).
3.
If a > 0, then
ùí©(0, 1)([‚àía, a]) = 2 Œ¶(a) ‚àí1 .
4.
Prove formulas (1.71), that is,
Œ¶(t) = 1
2[1 + erf( t
‚àö2
)]
and
erf(t) = 2Œ¶(‚àö2 t) ‚àí1 ,
t ‚àà‚Ñù.
5.
Compute
lim
t‚Üí‚àû
1 ‚àíŒ¶(t)
t‚àí1e‚àít2/2 .
Problem 1.40 (Bertrand paradox). Consider an equilateral triangle inscribed in a circle
of radius r > 0. Suppose a chord of the circle is chosen at random. What is the probability
that the chord is longer than a side of the triangle?
In this form, the problem allows different answers. Why? Because we did not define
in which way the random chord is chosen.
1.
The ‚Äúrandom endpoints‚Äù method: Choose independently two uniformly distributed
random points on the circumference of the circle and draw the chord joining them.

110
‡±™
1 Probabilities
2.
The ‚Äúrandom radius‚Äù method: Choose a radius of the circle, that is, choose a random
angle in [0, 2œÄ], choose independently a point on the radius according to the uniform
distribution on [0, r], and construct the chord through this point and perpendicular
to the radius.
3.
The ‚Äúrandom midpoint‚Äù method: Choose a point within the circle according to the
uniform distribution on the circle and construct a chord with the chosen point as
its midpoint.
Answer the above question about the length of the chord in each of the three cases.
Problem 1.41. A stick of length L > 0 is randomly broken into three pieces. Hereby
we assume that both break points are uniformly distributed on [0, L] and independent
of each other. What is the probability that these three parts piece together to form a
triangle?

2 Conditional probabilities and independence
2.1 Conditional probabilities
In order to motivate the definition of conditional probabilities, let us start with the fol-
lowing easy example.
Example 2.1.1. Roll a fair die twice. The probability of the event ‚Äúsum of both rolls
equals 5‚Äù is 1/9. Suppose now we were told that the first roll was an even number. Does
this additional information make the event ‚Äúsum equals 5‚Äù more likely? Or does it even
diminish the probability of its occurrence? To answer this question, we apply the so-
called technique of ‚Äúrestricting the sample space.‚Äù Since we know that the event B =
{First roll is even} had occurred, we may rule out elements in Bc and restrict our sam-
ple space. Choose B as the new sample space. Its cardinality is 18. Moreover, under this
condition, an event A occurs if and only if A ‚à©B does. Hence, the ‚Äúnew‚Äù probability of A
under condition B, written ‚Ñô(A|B), is given by
‚Ñô(A|B) = |A ‚à©B|
|B|
= |A ‚à©B|
18
.
(2.1)
In the question above, we asked for ‚Ñô(A|B), and have
A = {Sum of both rolls equals 5} = {(1, 4), (2, 3), (3, 2), (4, 1)}.
Since A ‚à©B = {(2, 3), (4, 1)}, we obtain ‚Ñô(A|B) = 2/18 = 1/9. Consequently, in this case,
condition B does not change the probability of the occurrence of A.
Define now A as a set of pairs adding to 6. Then ‚Ñô(A) = 5/36, while the conditional
probability remains 1/9. Note that now A‚à©B = {(2, 4), (4, 2)}. Thus, in this case, condition
B makes the occurrence of A less likely.
Before we state the definition of conditional probabilities in the general case, let us
rewrite eq. (2.1) as follows:
‚Ñô(A|B) = |A ‚à©B|
|B|
= |A ‚à©B|/36
|B|/36
= ‚Ñô(A ‚à©B)
‚Ñô(B)
.
(2.2)
Equation (2.2) gives us a hint how to introduce conditional probabilities in the general
setting.
Definition 2.1.2. Let (Œ©, ùíú, ‚Ñô) be a probability space. Given events A, B ‚ààùíúwith ‚Ñô(B) > 0, the proba-
bility of A under condition B is defined by
‚Ñô(A|B) = ‚Ñô(A ‚à©B)
‚Ñô(B)
.
(2.3)
https://doi.org/10.1515/9783111325064-002

112
‡±™
2 Conditional probabilities and independence
Remark 2.1.3. If we know the values of ‚Ñô(A ‚à©B) and ‚Ñô(B), then formula (2.3) allows us
to evaluate ‚Ñô(A|B). Sometimes it happens that we know the values of ‚Ñô(B) and ‚Ñô(A|B)
and want to calculate ‚Ñô(A ‚à©B). In order to do this, we rewrite eq. (2.3) as
‚Ñô(A ‚à©B) = ‚Ñô(B) ‚Ñô(A|B).
(2.4)
In this way, we get the desired value of ‚Ñô(A ‚à©B). Formula (2.4) is called the law of mul-
tiplication.
The next two examples show how this law applies.
Example 2.1.4. In an urn there are two white and two black balls. Choose two balls
without replacing the first. We want to evaluate the probability of occurrence of a
black ball in the first draw and of a white in the second. Let us first find a suitable
mathematical model that describes this experiment. The sample space is given by
Œ© = {(b, b), (b, w), (w, b), (w, w)}, and we consider the events
A = {Second ball is white} = {(b, w), (w, w)}
and
B = {First ball is black} = {(b, b), (b, w)} .
The event of interest is then A ‚à©B = {(b, w)}.
Which probabilities can be directly determined? Of course, the probability of oc-
currence of B equals 1/2 because the number of white and black balls is the same. Fur-
thermore, if B had occurred, then in the urn two white balls and one black ball have re-
mained. Under this condition, event A occurs with probability 2/3, that is, ‚Ñô(A|B) = 2/3.
Using eq. (2.4), we obtain
‚Ñô({(b, w)}) = ‚Ñô(A ‚à©B) = ‚Ñô(B) ‚ãÖ‚Ñô(A|B) = 1
2 ‚ãÖ2
3 = 1
3.
Example 2.1.5. Among three indistinguishable coins, there are two fair and one biased.
Tossing the biased coin, ‚Äúheads‚Äù appears with probability 1/3, hence ‚Äútails‚Äù appears with
probability 2/3. We choose at random one of the three coins and toss it. Find the proba-
bility to observe ‚Äútails‚Äù at the biased coin.
To solve this problem, let us first mention that the sample space Œ© = {H, T} is not
adequate to describe that experiment. Why? Because the event {H} may have different
probabilities depending on the occurrence using a biased or a fair coin. We have to dis-
tinguish between the appearance of ‚Äúheads‚Äù or ‚Äútails‚Äù for the different types of coin.
Hence, an adequate choice of the sample space is
Œ© := {(H, B), (T, B), (H, F), (T, F)}.
Here, B stands for the biased and F for the fair coin. The event of interest is {(T, B)}. Set
T := {(T, B), (T, F)}
and
B := {(H, B), (T, B)}.

2.1 Conditional probabilities
‡±™
113
Then T occurs if ‚Äútails‚Äù appears regardless of the type of the coin while B occurs if we
have chosen the biased coin. Of course, it follows that {(T, B)} = T ‚à©B. Since only one
of the three coins is biased, we have ‚Ñô(B) = 1/3. By assumption, ‚Ñô(T|B) = 2/3, hence an
application of eq. (2.4) leads to
‚Ñô({(T, B)}) = ‚Ñô(B) ‚Ñô(T|B) = 1
3 ‚ãÖ2
3 = 2
9.
Next, we present two examples where formula (2.3) applies directly.
Example 2.1.6. Roll a die twice. One already knows that the first number is not ‚Äú6‚Äù. What
is the probability that the sum of both rolls is greater than or equal to ‚Äú10‚Äù?
Answer: The model for this experiment is Œ© = {1, . . . , 6}2 endowed with the uniform
distribution ‚Ñôon ùí´(Œ©). The event B := {First result is not ‚Äú6‚Äù} contains 30 elements,
namely
{(1, 1), . . . , (5, 1), . . . , (1, 6), . . . , (5, 6)},
and if A consists of pairs with the sum equal to or larger than 10, then
A = {(4, 6), (5, 6), (6, 6), (5, 5), (6, 5), (6, 4)},
hence A ‚à©B = {(4, 6), (5, 6), (5, 5)}.
Therefore, it follows that
‚Ñô(A|B) = ‚Ñô(A ‚à©B)
‚Ñô(B)
= 3/36
30/36 = 1
10.
In the case that all elementary events are equally likely, there exists a more direct
way to evaluate ‚Ñô(A|B). We reduce the sample space as we already did in Example 2.1.1.
Proposition 2.1.7 (Reduction of the sample space). Suppose the sample space Œ© is finite
and let ‚Ñôbe the uniform distribution on ùí´(Œ©). Then for all events A and a nonempty B
in Œ©, we have
‚Ñô(A|B) = |A ‚à©B|
|B|
.
(2.5)
Proof. This easily follows from
‚Ñô(A|B) = ‚Ñô(A ‚à©B)
‚Ñô(B)
= |A ‚à©B|/|Œ©|
|B|/|Œ©|
= |A ‚à©B|
|B|
.
Example 2.1.8. We want to investigate Example 2.1.6 once more, this time using for-
mula (2.5) directly. Since |A ‚à©B| = 3 and |B| = 30, we get as before
‚Ñô(A|B) = |A ‚à©B|
|B|
= 3
30 = 1
10.

114
‡±™
2 Conditional probabilities and independence
Remark 2.1.9. It is important to state that Proposition 2.1.7 becomes false for general
probabilities ‚Ñôon ùí´(Œ©). Formula (2.5) is only valid in the case that ‚Ñôis the uniform
distribution on ùí´(Œ©).
Example 2.1.10. Toss a fair coin 5 times. How likely is it to observe 3 times ‚Äú1‚Äù under the
condition that the first toss was a ‚Äú1‚Äù?
Answer: The event A occurs if among the five tosses there are three with ‚Äú1,‚Äù while
B occurs provided the first toss is ‚Äú1.‚Äù Then |B| = 24 = 16 while |A ‚à©B| = (4
2) = 6. Since by
assumption all sequences of zeroes and ones are equally likely, Proposition 2.1.7 applies
and leads to
‚Ñô(A|B) = |A ‚à©B|
|B|
= 6
16 = 3
8 .
Suppose now that the coin is no longer fair. Say ‚Äú1‚Äù occurs with probability p and ‚Äú0‚Äù
with probability 1 ‚àíp. Then we can no longer evaluate the conditional probability by
reducing the sample space. In this case one has to apply directly the definition of the
conditional probabilities and obtains
‚Ñô(B) = p ,
‚Ñô(A ‚à©B) = (4
2)p3(1 ‚àíp)2
‚áí
‚Ñô(A|B) = ‚Ñô(A ‚à©B)
‚Ñô(B)
= 6 p2(1 ‚àíp)2 .
Example 2.1.11. The duration of a telephone call is exponentially distributed with pa-
rameter Œª > 0. Find the probability that a call does not last more than 5 minutes provided
it already lasted 2 minutes.
Solution: Let A be the event that the call does not last more than 5 minutes, that is,
A = [0, 5]. We know it already lasted 2 minutes, hence event B = [2, ‚àû) has occurred.
Thus, under condition B, it follows that
EŒª(A|B) = EŒª(A ‚à©B)
EŒª(B)
= EŒª([2, 5])
EŒª([2, ‚àû)) = e‚àí2Œª ‚àíe‚àí5Œª
e‚àí2Œª
= 1 ‚àíe‚àí3Œª.
Note the interesting fact that this conditional probability equals EŒª([0, 3]). What does
this tell us? It says that the probability that a call lasts no more than another 3 minutes
is independent of the fact that it has already lasted 2 minutes. This means that the dura-
tion of a call has not ‚Äúbecome older.‚Äù Independent of the fact that it has already lasted
2 minutes, the probability for talking no more than another 3 minutes remains the same.
Let us come back to the general case. Fix an event B ‚ààùíúwith ‚Ñô(B) > 0. Then
A Û≥®É‚Üí‚Ñô(A|B) ,
A ‚ààùíú,
is a well-defined mapping from ùíúto [0, 1]. Its main properties are summarized in the
next proposition.
Proposition 2.1.12. Let (Œ©, ùíú, ‚Ñô) be an arbitrary probability space. For each B ‚ààùíúwith
‚Ñô(B) > 0, the mapping A Û≥®É‚Üí‚Ñô(A|B) is a probability measure on ùíú. It is concentrated on B,

2.1 Conditional probabilities
‡±™
115
that is,
‚Ñô(B|B) = 1
or, equivalently,
‚Ñô(Bc|B) = 0.
Proof. Of course, one has
‚Ñô(0|B) = ‚Ñô(0 ‚à©B)/‚Ñô(B) = 0
and
‚Ñô(Œ©|B) = ‚Ñô(Œ© ‚à©B)/‚Ñô(B) = ‚Ñô(B)/‚Ñô(B) = 1 .
Thus, it remains to prove that ‚Ñô(‚ãÖ|B) is œÉ-additive. To this end, choose disjoint A1, A2, . . .
in ùíú. Then also A1 ‚à©B, A2 ‚à©B, . . . are disjoint and, using the œÉ-additivity of ‚Ñôleads to
‚Ñô(
‚àû
‚ãÉ
j=1
Aj|B) =
‚Ñô([‚ãÉ‚àû
j=1 Aj] ‚à©B)
‚Ñô(B)
=
‚Ñô(‚ãÉ‚àû
j=1(Aj ‚à©B))
‚Ñô(B)
=
‚àë‚àû
j=1 ‚Ñô(Aj ‚à©B)
‚Ñô(B)
=
‚àû
‚àë
j=1
‚Ñô(Aj ‚à©B)
‚Ñô(B)
=
‚àû
‚àë
j=1
‚Ñô(Aj|B).
Consequently, as asserted, ‚Ñô( ‚ãÖ|B) is a probability. Since the identity ‚Ñô(B|B) = 1 is obvi-
ous, this ends the proof.
Definition 2.1.13. The mapping ‚Ñô( ‚ãÖ|B) is called the conditional probability or also conditional distri-
bution (under condition B).
Remark 2.1.14. The main advantage of Proposition 2.1.12 is that it implies that condi-
tional probabilities share all the properties of ‚Äúordinary‚Äù probability measures. For ex-
ample, it holds that
‚Ñô(A2\A1|B) = ‚Ñô(A2|B) ‚àí‚Ñô(A1|B)
provided that A1 ‚äÜA2,
or
‚Ñô(A1 ‚à™A2|B) = ‚Ñô(A1|B) + ‚Ñô(A2|B) ‚àí‚Ñô(A1 ‚à©A2|B).
But note, there do not exist similar rules for ‚Ñô(A|B) independent of the event B and with
A fixed.
We come now to the so-called law of total probability. It allows us to evaluate the
probability of an event A knowing only its conditional probabilities ‚Ñô(A|Bj) for certain
Bj ‚ààùíú. More precisely, the following is valid.
Proposition 2.1.15 (Law of total probability). Let (Œ©, ùíú, ‚Ñô) be a probability space and let
B1, . . . , Bn in ùíúbe disjoint with ‚Ñô(Bj) > 0 and ‚ãÉn
j=1 Bj = Œ©. Then for each A ‚ààùíú, one has
‚Ñô(A) =
n
‚àë
j=1
‚Ñô(Bj) ‚Ñô(A|Bj).
(2.6)

116
‡±™
2 Conditional probabilities and independence
Proof. Let us start with the investigation of the right-hand side of eq. (2.6). By the defi-
nition of the conditional probability, this expression may be rewritten as
n
‚àë
j=1
‚Ñô(Bj) ‚Ñô(A|Bj) =
n
‚àë
j=1
‚Ñô(Bj)
‚Ñô(A ‚à©Bj)
‚Ñô(Bj)
=
n
‚àë
j=1
‚Ñô(A ‚à©Bj).
(2.7)
The sets B1, . . . , Bn are disjoint, hence so are A ‚à©B1, . . . , A ‚à©Bn. Thus, the finite additivity
of ‚Ñôimplies
n
‚àë
j=1
‚Ñô(A ‚à©Bj) = ‚Ñô(
n
‚ãÉ
j=1
(A ‚à©Bj)) = ‚Ñô((
n
‚ãÉ
j=1
Bj) ‚à©A) = ‚Ñô(Œ© ‚à©A) = ‚Ñô(A).
Together with eq. (2.7), this proves eq. (2.6).
A first example illustrates how the law of total probability applies.
Example 2.1.16. Suppose we have n urns, each containing a certain (maybe different)
number of white and black balls. Choose urn Uj with probability ‚Ñô(Uj), 1 ‚â§j ‚â§n, and
take out one ball of the chosen urn. Let W occur if the chosen ball is white. Then the law
of total probability asserts that
‚Ñô(W) = ‚Ñô(U1)‚Ñô(W|U1) + ‚ãÖ‚ãÖ‚ãÖ+ ‚Ñô(Un)‚Ñô(W|Un) ,
where ‚Ñô(W|Uj) is the proportion of white balls in urn Uj, 1 ‚â§j ‚â§n. In particular, if all
urns are equally likely, then one gets
‚Ñô(W) = 1
n
n
‚àë
j=1
‚Ñô(W|Uj) .
Example 2.1.17. A fair coin is tossed four times. Suppose we observe exactly k ‚Äúheads‚Äù
for some k = 0, . . . , 4. According to the observed k, we take k dice and roll them. Find
the probability of the event A = {Number ‚Äú6‚Äù does not show up}. Note that k = 0 means
that we do not roll a die, hence in this case ‚Äú6‚Äù cannot appear.
Solution: As sample space, we choose Œ© = {(k, Y), (k, N) : k = 0, . . . , 4}, where
(k, Y) means that we rolled k dice and at least on one of them we got a ‚Äú6‚Äù. In the
same way, (k, N) stands for k dice and no ‚Äú6‚Äù. Let N = {(0, N), . . . , (4, N)} and let Bk =
{(k, Y), (k, N)}, k = 0, . . . , 4. Then Bk occurs if we observed k ‚Äúheads.‚Äù The conditional
probabilities equal
‚Ñô(N|B0) = 1 ,
‚Ñô(N|B1) = 5/6 ,
. . . ,
‚Ñô(N|B4) = (5/6)4 ,
while
‚Ñô(Bk) = (4
k) 1
24 ,
k = 0, . . . , 4.

2.1 Conditional probabilities
‡±™
117
The events B0, . . . , B4 satisfy the assumptions of Proposition 2.1.15, thus Eq. (2.6) applies
and leads to
‚Ñô(A) = 1
24
4
‚àë
k=0
(4
k) (5/6)k = 1
24 (5
6 + 1)
4
= (11
12)
4
= 0.706066743.
Example 2.1.18. Three different machines, M1, M2 and M3, produce light bulbs. In a sin-
gle day, M1 produces 500 bulbs, M2 yields 200, and M3 provides 100. The quality of the
produced bulbs depends on the machines: Among the light bulbs produced by M1, 5 %
are defective; among those from M2, 10 % are defective; and only 2 % are defective from
M3. At the end of a day, a controller chooses 1 of the 800 produced light bulbs at random
and tests it. Determine the probability that the checked bulb is defective.
Solution: The probabilities that the checked bulb was produced by M1, M2, or M3
are 5/8, 1/4, and 1/8, respectively. The conditional probabilities for choosing a defective
bulb produced by M1, M2 or M3 were given as 1/20, 1/10, and 1/50, respectively. If D is
the event that the tested bulb was defective, then the law of total probability yields
‚Ñô(D) = 5
8 ‚ãÖ1
20 + 1
4 ‚ãÖ1
10 + 1
8 ‚ãÖ1
50 = 47
800 = 0.05875.
Let us look at Example 2.1.18 from a different point of view. When choosing a light
bulb out of the 800 produced, there were certain fixed probabilities of whether it was
produced by M1, M2, or M3, namely 5/8, 1/4, and 1/8. These are the probabilities before
checking a bulb. Therefore, they are called a priori probabilities. After checking a bulb,
we obtained additional information that it was defective. Does this additional informa-
tion change the probabilities which of the machines M1, M2, and M3 produced it? More
precisely, if the D above occurs when the tested bulb is defective, then we now ask for
the conditional probabilities ‚Ñô(M1|D), ‚Ñô(M2|D), and ‚Ñô(M3|D). To understand that these
probabilities may differ considerably from the a priori probabilities, imagine that, for
example, M1 produces almost no defective bulbs. Then it will be very unlikely that the
tested bulb has been produced by M1, although ‚Ñô(M1) may be big.
Because ‚Ñô(M1|D), ‚Ñô(M2|D), and ‚Ñô(M3|D) are the probabilities after executing the
random experiment (choosing and testing the bulb), they are called a posteriori proba-
bilities.
Let us now introduce the exact and general definition of a priori and a posteriori
probabilities.
Definition 2.1.19. Suppose there is a probability space (Œ©, ùíú, ‚Ñô) and there are disjoint events
B1, . . . , Bn ‚ààùíúsatisfying Œ© = ‚ãÉn
j=1 Bj. Then we call ‚Ñô(B1), . . . , ‚Ñô(Bn) the a priori probabilities of B1, . . . , Bn.
Let A ‚ààùíúwith ‚Ñô(A) > 0 be given. Then the conditional probabilities ‚Ñô(B1|A), . . . , ‚Ñô(Bn|A) are said to be
the a posteriori probabilities, that is, those after the occurrence of A.
To calculate the a posteriori probabilities, the next proposition turns out to be very use-
ful.

118
‡±™
2 Conditional probabilities and independence
Proposition 2.1.20 (Bayes‚Äô formula). Suppose we are given disjoint events B1 to Bn satis-
fying ‚ãÉn
j=1 Bj = Œ© and ‚Ñô(Bj) > 0. Let A be an event with ‚Ñô(A) > 0. Then for each j ‚â§n the
following equation holds:
‚Ñô(Bj|A) =
‚Ñô(Bj) ‚Ñô(A|Bj)
‚àën
i=1 ‚Ñô(Bi)‚Ñô(A|Bi).
(2.8)
Proof. Proposition 2.1.15 implies
n
‚àë
i=1
‚Ñô(Bi)‚Ñô(A|Bi) = ‚Ñô(A).
Hence, the right-hand side of eq. (2.8) may also be written as
‚Ñô(Bj)‚Ñô(A|Bj)
‚Ñô(A)
=
‚Ñô(Bj)
‚Ñô(A‚à©Bj)
‚Ñô(Bj)
‚Ñô(A)
=
‚Ñô(A ‚à©Bj)
‚Ñô(A)
= ‚Ñô(Bj|A),
and the proposition is proven.
Remark 2.1.21. In the case ‚Ñô(A) is already known, Bayes‚Äô formula simplifies to
‚Ñô(Bj|A) =
‚Ñô(Bj)‚Ñô(A|Bj)
‚Ñô(A)
,
j = 1, . . . , n.
(2.9)
Remark 2.1.22. Let us treat the special case of two sets partitioning Œ©. If B1 = B, then
necessarily B2 = Bc, hence Œ© = B ‚à™Bc. Then formula (2.8) looks as follows:
‚Ñô(B|A) =
‚Ñô(B)‚Ñô(A|B)
‚Ñô(B)‚Ñô(A|B) + ‚Ñô(Bc)‚Ñô(A|Bc)
(2.10)
and
‚Ñô(Bc|A) =
‚Ñô(Bc)‚Ñô(A|Bc)
‚Ñô(B)‚Ñô(A|B) + ‚Ñô(Bc)‚Ñô(A|Bc).
(2.11)
Again, if the probability of A is known, the denominators in Eqs. (2.10) and (2.11) may be
replaced by ‚Ñô(A).
Example 2.1.23. Let us use Bayes‚Äô formula to calculate the a posteriori probabilities in
Example 2.1.18. Recall that D occurs if the tested bulb is defective. We already know
‚Ñô(D) = 47/800, hence we may apply eq. (2.9). Doing so, we get
‚Ñô(M1|D) = ‚Ñô(M1)‚Ñô(D|M1)
‚Ñô(D)
= 5/8 ‚ãÖ1/20
47/800
= 25/47,
‚Ñô(M2|D) = ‚Ñô(M2)‚Ñô(D|M2)
‚Ñô(D)
= 1/4 ‚ãÖ1/10
47/800
= 20/47,
‚Ñô(M3|D) = ‚Ñô(M3)‚Ñô(D|M3)
‚Ñô(D)
= 1/8 ‚ãÖ1/50
47/800
= 2/47.

2.1 Conditional probabilities
‡±™
119
By assignment of the problem, the a priori probabilities were given by ‚Ñô(M1) = 5/8,
‚Ñô(M2) = 1/4, and ‚Ñô(M3) = 1/8. In the case that the tested light bulb was defective,
these probabilities changed to 25/47, 20/47, and 2/47. This tells us that it becomes less
likely that the tested bulb was produced by M1 or M3; their probabilities diminish by
0.0930851 and 0.0824468, respectively. On the other hand, the probability of M2 increases
by 0.175532.
Finally, note that Proposition 2.1.12 implies that the sum of the a posteriori proba-
bilities has to be 1. Because of 25/47 + 20/47 + 2/47 = 1, this is true in that example.
Example 2.1.24. In order to figure out whether or not a person suffers from a certain
disease, say disease X, a test is assumed to give a clue. If the tested person is sick, then
the test is positive in 96 % of cases. If the person is well, then with 94 % accuracy the test
will be negative. Furthermore, it is known that 0.4 % of the population suffers from the
disease X.
Now a person, chosen at random, is tested. Suppose the result was positive. Find the
probability that this person really suffers from X.
Solution: As sample space, we may choose
Œ© = {(X, p), (X, n), (Xc, p), (Xc, n)},
where, for example, (X, n) means that the person suffers from X and the test was nega-
tive. Set A := {(X, p), (Xc, p)}. Then A occurs if and only if the test turned out to be positive.
Furthermore, event B := {(X, p), (X, n)} occurs in the case that the tested person suffers
from X. Known are
‚Ñô(A|B) = 0.96, ‚Ñô(A|Bc) = 0.06,
and
‚Ñô(B) = 0.004 ,
hence ‚Ñô(Bc) = 0.996.
Therefore, by eq. (2.10), the probability we asked for can be calculated as follows:
‚Ñô(B|A) =
‚Ñô(B)‚Ñô(A|B)
‚Ñô(B)‚Ñô(A|B) + ‚Ñô(Bc)‚Ñô(A|Bc)
=
0.004 ‚ãÖ0.96
0.004 ‚ãÖ0.96 + 0.996 ‚ãÖ0.06 = 0.00384
0.0636 = 0.0603774.
This tells us that it is quite unlikely that a randomly chosen person with a positive test
is really sick. The chance for this being true is only about 6 %.
Summary: Given two events A and B in a probability space (Œ©, ùíú, ‚Ñô) with ‚Ñô(B) > 0, the probability of A
under the condition of the occurrence of B is given by
‚Ñô(A|B) = ‚Ñô(A ‚à©B)
‚Ñô(B)
or, equivalently, by
‚Ñô(A ‚à©B) = ‚Ñô(B) ‚Ñô(A|B) .
The basic properties of the conditional probability are summarized in the ‚ÄúLaw of total probability‚Äù and in
‚ÄúBayes‚Äô formula‚Äù (Propositions 2.1.15 and 2.1.20).

120
‡±™
2 Conditional probabilities and independence
2.2 Independence of events
What does it mean that two events are independent or, more precisely, that they occur
independently of each other? To get an idea, let us look at the following example.
Example 2.2.1. Roll a fair die twice. Event B occurs if the first number is even while
event A consists of all pairs (x1, x2), where x2 = 5 or x2 = 6. It is intuitively clear that these
two events occur independently of each other. But how to express this mathematically?
To answer this question, think about the probability of A under the condition B. The
fact whether or not B has occurred has no influence on the occurrence of A. For the
occurrence or nonoccurrence of A, it is completely insignificant what happened in the
first roll. Mathematically, this means that ‚Ñô(A|B) = ‚Ñô(A). Let us check whether this is
true in this concrete case. Indeed, it holds that ‚Ñô(A) = 1/3, as well as
‚Ñô(A|B) = ‚Ñô(A ‚à©B)
‚Ñô(B)
= 6/36
1/2 = 1/3.
The previous example suggests that the independence of A of B could be described
by
‚Ñô(A) = ‚Ñô(A|B) = ‚Ñô(A ‚à©B)
‚Ñô(B)
.
(2.12)
But formula (2.12) has a disadvantage, namely we have to assume ‚Ñô(B) > 0 to ensure
that ‚Ñô(A|B) exists. To overcome this problem, rewrite eq. (2.12) as
‚Ñô(A ‚à©B) = ‚Ñô(A) ‚Ñô(B).
(2.13)
In this form, we may take eq. (2.13) as the basis for the definition of independence.
Definition 2.2.2. Let (Œ©, ùíú, ‚Ñô) be a probability space. Two events A and B in ùíúare said to be (stochasti-
cally) independent provided
‚Ñô(A ‚à©B) = ‚Ñô(A) ‚ãÖ‚Ñô(B).
(2.14)
In the case that eq. (2.14) does not hold, the events A and B are called (stochastically) dependent.
Remark 2.2.3. In the sequel, we use the notations ‚Äúindependent‚Äù and ‚Äúdependent‚Äù with-
out adding the word ‚Äústochastically.‚Äù Since we will not use other versions of indepen-
dence, there should be no confusion.
Example 2.2.4. A fair die is rolled twice. Event A occurs if the first roll is either ‚Äú1‚Äù or
‚Äú2‚Äù while B occurs if the sum of both rolls equals 7. Are A and B independent?
Answer: We have ‚Ñô(A) = 1/3, ‚Ñô(B) = 1/6, and ‚Ñô(A ‚à©B) = 2/36 = 1/18. Hence, we get
‚Ñô(A ‚à©B) = ‚Ñô(A) ‚ãÖ‚Ñô(B) and so A and B are independent.
Question: Are A and B also independent if A is as before and B is defined as a set of
pairs with sum 4?

2.2 Independence of events
‡±™
121
Example 2.2.5. In an urn, there are n ‚â•2 white balls and also n black balls. One chooses
two balls without replacing the first. Let A be the event that the second ball is black while
B occurs if the first ball was white. Are A and B independent?
Answer: The probability of B equals 1/2. To calculate ‚Ñô(A), we use Proposition 2.1.15.
Then we get
‚Ñô(A) = ‚Ñô(B)‚Ñô(A|B) + ‚Ñô(Bc)‚Ñô(A|Bc) = 1
2 ‚ãÖ
n
2n ‚àí1 + 1
2 ‚ãÖn ‚àí1
2n ‚àí1 = 1
2 ,
hence ‚Ñô(A) ‚ãÖ‚Ñô(B) = 1/4.
On the other hand, we have
‚Ñô(A ‚à©B) = ‚Ñô(B)‚Ñô(A|B) = 1
2 ‚ãÖ
n
2n ‚àí1 =
n
4n ‚àí2
Ã∏= 1
4.
Consequently, A and B are dependent.
Remark 2.2.6. Note that if n ‚Üí‚àû, then
‚Ñô(A ‚à©B) =
n
4n ‚àí2 ‚Üí1
4 = ‚Ñô(A) ‚Ñô(B).
This tells us the following: if n is big, then A and B are ‚Äúalmost‚Äù independent or, equiv-
alently, the degree of dependence between A and B is very small. This question will be
investigated more thoroughly in Chapter 5 when a measure for the degree of depen-
dence is available.
Next, we prove some properties of independent events.
Proposition 2.2.7. Let (Œ©, ùíú, ‚Ñô) be a probability space.
1.
For any A ‚ààùíú, the events A and 0, as well as A and Œ©, are independent.1
2.
If A and B are independent, then so are A and Bc, as well as Ac and Bc.
Proof. We have
‚Ñô(A ‚à©0) = ‚Ñô(0) = 0 = ‚Ñô(A) ‚ãÖ0 = ‚Ñô(A) ‚ãÖ‚Ñô(0) ,
hence A and 0 are independent.
In the same way, the independence of A and Œ© follows from
‚Ñô(A ‚à©Œ©) = ‚Ñô(A) = ‚Ñô(A) ‚ãÖ1 = ‚Ñô(A) ‚ãÖ‚Ñô(Œ©).
To prove the second part, assume that A and B are independent. Our aim is to show that
A and Bc are independent as well. We know that
1 For a more general result, see Problem 2.15.

122
‡±™
2 Conditional probabilities and independence
‚Ñô(A ‚à©B) = ‚Ñô(A) ‚Ñô(B)
and want to show that
‚Ñô(A ‚à©Bc) = ‚Ñô(A) ‚Ñô(Bc).
Let us start with the right-hand side of the latter equation. Using the independence of A
and B and the fact A ‚à©B ‚äÜB, it follows that
‚Ñô(A) ‚Ñô(Bc) = ‚Ñô(A)(1 ‚àí‚Ñô(B)) = ‚Ñô(A) ‚àí‚Ñô(A) ‚ãÖ‚Ñô(B)
= ‚Ñô(A) ‚àí‚Ñô(A ‚à©B) = ‚Ñô(A\(A ‚à©B)).
(2.15)
Since A\(A ‚à©B) = A\B = A ‚à©Bc, using eq. (2.15), we derive
‚Ñô(A) ‚ãÖ‚Ñô(Bc) = ‚Ñô(A ‚à©Bc).
Consequently, as asserted, A and Bc are independent.
If A and B are independent, then so are B and A, and as seen above, so are B and Ac.
Another application of the first step, this time with Ac and B, shows that also Ac and Bc
are independent. This completes the proof.
Suppose we are given n events A1, . . . , An in ùíú. We want to figure out when they are
independent. A first possible approach could be as follows.
Definition 2.2.8. Events A1, . . . , An are said to be pairwise independent if, whenever i
Ã∏= j,
‚Ñô(Ai ‚à©Aj) = ‚Ñô(Ai) ‚ãÖ‚Ñô(Aj).
In other words, for all 1 ‚â§i < j ‚â§1 the events Ai and Aj are independent.
Unfortunately, for many purposes, the property of pairwise independence is too weak.
For example, as we will see next, in general it does not imply the important equation
‚Ñô(A1 ‚à©‚ãÖ‚ãÖ‚ãÖ‚à©An) = ‚Ñô(A1) ‚ãÖ‚ãÖ‚ãÖ‚Ñô(An).
(2.16)
Example 2.2.9. Roll a die twice and define events A1, A2, and A3 as follows:
A1 := {2, 4, 6} √ó {1, . . . , 6},
A2 := {1, . . . , 6} √ó {1, 3, 5},
A3 := {2, 4, 6} √ó {1, 3, 5} ‚à™{1, 3, 5} √ó {2, 4, 6}.
Verbally this says that A1 occurs if the first roll is even, A2 occurs if the second one is
odd, and A3 occurs if either the first number is odd while the second is even or vice
versa.

2.2 Independence of events
‡±™
123
Direct calculations give ‚Ñô(A1) = ‚Ñô(A2) = ‚Ñô(A3) = 1/2, as well as
‚Ñô(A1 ‚à©A2) = ‚Ñô(A1 ‚à©A3) = ‚Ñô(A2 ‚à©A3) = 1
4.
Hence, A1, A2, and A3 are pairwise independent.
Since
A1 ‚à©A2 ‚à©A3 = A1 ‚à©A2,
it follows that
‚Ñô(A1 ‚à©A2 ‚à©A3) = ‚Ñô(A1 ‚à©A2) = 1
4
Ã∏= 1
8 = ‚Ñô(A1) ‚ãÖ‚Ñô(A2) ‚ãÖ‚Ñô(A3).
So, we found three pairwise independent events for which eq. (2.16) is not valid.
After mentioning that pairwise independence of A1, . . . , An does not imply
‚Ñô(A1 ‚à©‚ãÖ‚ãÖ‚ãÖ‚à©An) = ‚Ñô(A1) ‚ãÖ‚ãÖ‚ãÖ‚Ñô(An),
(2.17)
it makes sense to ask whether or not pairwise independence can be derived from
eq. (2.17). The next example shows that, in general, this is also not true.
Example 2.2.10. Let Œ© = {1, . . . , 12} be endowed with the uniform distribution ‚Ñô, that is,
for any A ‚äÜŒ© we have ‚Ñô(A) = |A|/12. Define events A1, A2, and A3 as A1 := {1, . . . , 9},
A2 := {6, 7, 8, 9}, and A3 := {9, 10, 11, 12}. Direct calculations give
‚Ñô(A1) = 9
12 = 3
4 ,
‚Ñô(A2) = 4
12 = 1
3,
and
‚Ñô(A3) = 4
12 = 1
3 .
Moreover, we have
‚Ñô(A1 ‚à©A2 ‚à©A3) = ‚Ñô({9}) = 1
12 = 3
4 ‚ãÖ1
3 ‚ãÖ1
3 = ‚Ñô(A1) ‚ãÖ‚Ñô(A2) ‚ãÖ‚Ñô(A3) ,
hence eq. (2.17) is valid. But, because of
‚Ñô(A1 ‚à©A2) = ‚Ñô(A2) = 1
3
Ã∏= 1
4 = ‚Ñô(A1) ‚ãÖ‚Ñô(A2) ,
the events A1, A2, and A3 are not pairwise independent.
Remark 2.2.11. Summing up, Examples 2.2.9 and 2.2.10 show that neither pairwise inde-
pendence nor eq. (2.17) are suitable to define the independence of more than two events.
Why? On the one hand, independence should yield eq. (2.17) and, on the other hand,
whenever A1, . . . , An are independent, then so should be any subcollection of them. In
particular, independence should imply pairwise independence.
A reasonable definition of independence of n events is as follows.

124
‡±™
2 Conditional probabilities and independence
Definition 2.2.12. The events A1, . . . , An are said to be independent provided that for each subset
I ‚äÜ{1, . . . , n} we have
‚Ñô(‚ãÇ
i‚ààI
Ai) = ‚àè
i‚ààI
‚Ñô(Ai).
(2.18)
Remark 2.2.13. Of course, it suffices that eq. (2.18) is valid for sets I ‚äÜ{1, . . . , n} satisfying
|I| ‚â•2. Indeed, if |I| = 1, then eq. (2.18) holds trivially.
Remark 2.2.14. Another way to introduce independence is as follows: For all m ‚â•2 and
1 ‚â§i1 < ‚ãÖ‚ãÖ‚ãÖ< im ‚â§n, it follows that
‚Ñô(Ai1 ‚à©‚ãÖ‚ãÖ‚ãÖ‚à©Aim) = ‚Ñô(Ai1) ‚ãÖ‚ãÖ‚ãÖ‚Ñô(Aim).
Identify I with {i1, . . . , im} to see that both definitions are equivalent.
At a first glance, the previous Definition 2.2.12 looks complicated; in fact, it is not.
To see this, let us once more investigate the case n = 3. Here exist exactly four different
subsets I ‚äÜ{1, 2, 3} with |I| ‚â•2. These are I = {1, 2}, I = {1, 3}, I = {2, 3}, and I = {1, 2, 3}.
Consequently, three events A1, A2, and A3 are independent if and only if the four follow-
ing conditions hold at once:
‚Ñô(A1 ‚à©A2) = ‚Ñô(A1) ‚ãÖ‚Ñô(A2),
‚Ñô(A1 ‚à©A3) = ‚Ñô(A1) ‚ãÖ‚Ñô(A3),
‚Ñô(A2 ‚à©A3) = ‚Ñô(A2) ‚ãÖ‚Ñô(A3),
as well as
‚Ñô(A1 ‚à©A2 ‚à©A3) = ‚Ñô(A1) ‚ãÖ‚Ñô(A2) ‚ãÖ‚Ñô(A3).
Examples 2.2.9 and 2.2.10 show that all four equations are really necessary. None of them
is a consequence of the other three.
The independence of n events provides the following properties:
Proposition 2.2.15.
1.
Let A1, . . . , An be independent. For any J ‚äÜ{1, . . . , n}, the events {Aj : j ‚ààJ} are inde-
pendent as well. In particular, independence implies pairwise independence.
2.
For each permutation œÄ of {1, . . . , n}, the independence of A1, . . . , An implies that of2
AœÄ(1), . . . , AœÄ(n).
3.
Suppose for each 1 ‚â§j ‚â§n either Bj = Aj or Bj = Ac
j holds. Then the independence of
A1, . . . , An implies that of B1, . . . , Bn.
Proof. The first two properties are an immediate consequence of the definition of inde-
pendence.
2 For example, in the case n = 3 with A1, A2, A3 also A3, A2, A1 and A2, A3, A1 are independent.

2.2 Independence of events
‡±™
125
To prove the third assertion, reorder A1, . . . , An such that3 B1 = Ac
1. In the first step,
we show that Ac
1, A2, . . . , An are independent as well, that is, we have B1 = Ac
1, B2 = A2,
and so on. Given I ‚äÜ{1, . . . , n}, it has to hold that
‚Ñô(‚ãÇ
i‚ààI
Bi) = ‚àè
i‚ààI
‚Ñô(Bi).
In the case 1 ‚àâI, this follows by the independence of A1, . . . , An. If 1 ‚ààI, we apply Propo-
sition 2.2.7 with4 A1 and C = ‚ãÇi‚ààI\{1} Ai = ‚ãÇi‚ààI\{1} Bi. Then Ac
1 = B1 and C are independent
as well. Hence, by the independence of A2, . . . , An, we get
‚Ñô(‚ãÇ
i‚ààI
Bi) = ‚Ñô(B1 ‚à©C) = ‚Ñô(B1) ‚ãÖ‚Ñô(C) = ‚Ñô(B1) ‚ãÖ‚àè
i‚ààI\{1}
‚Ñô(Bi) = ‚àè
i‚ààI
‚Ñô(Bi).
The general case then follows by reordering the Ajs and by an iterative application of
the first step. This is exactly the procedure we did in the proof of Proposition 2.2.7 when
verifying the independence of Ac and Bc for independent A and B.
The next two examples show how independence of more than two events appears
in a natural way.
Example 2.2.16. Toss a fair coin n times. Let us assume that the coin is labeled with ‚Äú0‚Äù
and ‚Äú1.‚Äù Choose a fixed sequence (aj)n
j=1 of numbers in {0, 1} and suppose that the event
Aj occurs if in the jth trial aj comes up.
We claim now that A1, . . . , An are independent. To verify this, choose a subset
I ‚äÜ{1, . . . , n} with |I| = k for some k = 2, . . . , n. The cardinality of ‚ãÇi‚ààI Ai equals 2n‚àík.
Why? At k positions, the values of the tosses are fixed; at n ‚àík positions, they still may
be either ‚Äú0‚Äù or ‚Äú1‚Äù. Consequently,
‚Ñô(‚ãÇ
i‚ààI
Ai) = 2n‚àík
2n
= 2‚àík.
(2.19)
The same argument as before gives |Aj| = 2n‚àí1, hence ‚Ñô(Aj) = 1/2, 1 ‚â§j ‚â§n. Conse-
quently, it follows that
‚àè
i‚ààI
‚Ñô(Ai) = (1
2)
|I|
= 2‚àík.
(2.20)
Combining Eqs. (2.19) and (2.20) gives
3 If all Bj = Aj, there is nothing to prove.
4 Why are A1 and C independent? Give a short proof.

126
‡±™
2 Conditional probabilities and independence
‚Ñô(‚ãÇ
i‚ààI
Ai) = ‚àè
i‚ààI
‚Ñô(Ai),
and since I was arbitrary, the sets A1, . . . , An are independent.
Remark 2.2.17. Even the simple Example 2.2.16 shows that it might be rather compli-
cated to verify the independence of n given events. For example, if we modify the pre-
vious example by taking a biased coin, then the Ajs remain independent, but the proof
becomes more complicated.
Example 2.2.18. A machine consists of n components. These components break down
with certain probabilities p1, . . . , pn. Moreover, we assume that they break down inde-
pendently of each other. Find the probability that a chosen machine stops working. Be-
fore answering this question, we have to determine the conditions.
Case 1: The machine stops working provided at least one component breaks down.
Let M be the event that the machine stops working. If j ‚â§n, assume Aj occurs if
component j breaks down. By assumption, ‚Ñô(Aj) = pj. Since
M =
n
‚ãÉ
j=1
Aj,
by the independence5 it follows that
‚Ñô(M) = 1 ‚àí‚Ñô(Mc) = 1 ‚àí‚Ñô(
n
‚ãÇ
j=1
Ac
j ) = 1 ‚àí
n
‚àè
j=1
‚Ñô(Ac
j ) = 1 ‚àí
n
‚àè
j=1
(1 ‚àípj).
(2.21)
Case 2: The machine stops working provided all n components break down.
Using the same notation as in case 1, we now have
M =
n
‚ãÇ
j=1
Aj.
Hence, by the independence we obtain
‚Ñô(M) = ‚Ñô(
n
‚ãÇ
j=1
Aj) =
n
‚àè
j=1
pj.
(2.22)
Remark 2.2.19. Formula (2.21) tells us the following: If among the n components there
is one of bad quality, say component j0, then pj0 is close to one; hence, 1 ‚àípj0 is close to
zero, and so is ‚àèn
j=1(1‚àípj). Because of eq. (2.21), ‚Ñô(M) is large, and so the machine breaks
down with a large probability.
5 In fact, we also have to use Proposition 2.2.15.

2.3 Problems
‡±™
127
In the second case, the conclusion is as follows: if among the n components there
is one of high quality, say component j0, then pj0 is small and so is ‚àèn
j=1 pj. By eq. (2.22),
‚Ñô(M) is also small, hence it is very unlikely that the machine stops working.
Summary: Two events A and B in a probability space (Œ©, ùíú, ‚Ñô) are said to be (stochastically) independent if
‚Ñô(A ‚à©B) = ‚Ñô(A) ‚ãÖ‚Ñô(B) .
In general, events A1, . . . , An are (stochastically) independent provided that for all 2 ‚â§m ‚â§n and all choices
of indices 1 ‚â§i1 < ‚ãÖ‚ãÖ‚ãÖ< im ‚â§n one has
‚Ñô(Ai1 ‚à©‚ãÖ‚ãÖ‚ãÖ‚à©Aim) = ‚Ñô(Ai1) ‚ãÖ‚ãÖ‚ãÖ‚Ñô(Aim) .
In particular, then the Ajs are pairwise independent and ‚Ñô(A1 ‚à©‚ãÖ‚ãÖ‚ãÖ‚à©An) = ‚Ñô(A1) ‚ãÖ‚ãÖ‚ãÖ‚Ñô(An).
2.3 Problems
Problem 2.1 (Willem Jacob‚Äôs Gravesande, 1736). On a ship there are 84 passengers from
Belgium, 12 from England, and 4 from Germany.
1.
One passengers leaves the ship. How likely is it that he or she is
(i) Belgian,
(ii) German,
(iii) Belgian or German?
2.
Two people leave the ship. How likely is it that at least one of them is Belgian?
Problem 2.2. The chance to win a certain game is 50 %. One plays six games. Find the
probability to win exactly four games. Evaluate the probability of this event under the
condition to win at least two games. Suppose one had won exactly one of the two first
games. Which probability does the event ‚Äúwinning 4 games‚Äù have under this condition?
Problem 2.3. Toss a fair coin six times. Define events A and B as follows:
A = {‚ÄúHeads‚Äù appears exactly 3 times},
B = {The first and the second toss are ‚Äúheads‚Äù}.
Evaluate ‚Ñô(A), ‚Ñô(A|B), and ‚Ñô(A|Bc).
Problem 2.4. Let A and B be as in Problem 1.30, that is, A occurs if each box contains at
most one particle while B occurs if all four particles are in the same box.
Find now ‚Ñô(A|C) and ‚Ñô(B|C) with C = {The first box remains empty}.
Problem 2.5. Justify why Propositions 2.1.15 and 2.1.20 (Law of total probability and
Bayes‚Äô formula) remain valid for infinitely many disjoint sets B1, B2, . . . satisfying
‚Ñô(Bj) > 0 and ‚ãÉ‚àû
j=1 Bj = Œ©.
Prove that Proposition 2.1.15 also holds without assuming ‚ãÉn
j=1 Bj = Œ©. But then we
have to suppose A ‚äÜ‚ãÉn
j=1 Bj.

128
‡±™
2 Conditional probabilities and independence
Problem 2.6. To go to work, a man can either use a train, a bus, or his car. He chooses
the train 50 %, the bus 30 %, and the car 20 % of workdays. If he takes the train, he arrives
on time with probability 0.95. By bus, he is on time with probability 0.8, and by car with
probability 0.7.
(a) Evaluate the probability that the man is at work on time.
(b) How big is this probability given the man does not use the car?
(c) Assume the man arrived at work on time. What are then the probabilities that he
came by train, bus, or car?
Problem 2.7. Let U1, U2, and U3 be three urns containing five balls each. Urn U1 contains
four white balls and one black ball, U2 has three white balls and two black balls and,
finally, U3 contains two white balls and three black balls. Choose one urn at random
(each urn is equally likely) and, without replacing the first ball, take two balls out of the
chosen urn.
(a) Give a suitable sample space for this random experiment.
(b) Find the probability to observe two balls of different color.
(c) Assume the chosen balls were of different color. What are the probabilities that the
balls were taken out of U1, U2, or U3?
Problem 2.8. Suppose we have three indistinguishable dice. Two of them are fair, the
remaining one is biased. For the latter, the number ‚Äú6‚Äù appears with probability 1/5 while
all other numbers have probability 4/25. We choose at random one of the dice and roll
it.
(a) Find a suitable sample space for the description of this experiment.
(b) Give the probability of occurrence of {1} to {6} in that experiment.
(c) Suppose we have observed the number ‚Äú2‚Äù on the chosen die. Find the probability
that this die was the biased one.
Problem 2.9 (P. S. Laplace, 1774). Two urns U1 and U2 contain w1 white and b1 black balls
and w2 white and b2 black balls, respectively. Choose one of the two urns at random and
take out n balls without replacement. Among the n chosen balls, w are white and b are
black. Find a formula for the probability that we took off the balls from U1?
Find the numerical value for the likelihood of U1 in the case that there are 8 white
and 7 black balls in U1, 5 white and 15 black balls in U2, and that we chose 6 balls, with 4
of them white, hence 2 black.
Problem 2.10 (P. S. Laplace, 1786). In an urn there are three balls which are known to be
either white or black. Choosing n balls with replacement, we observe that all of them
were white. How likely is it that 0, 1, or 2 of the three balls are black? How likely is it that
the next chosen ball, the (n + 1)th, is white as well?
Problem 2.11. Let A and B be two events in a probability space with ‚Ñô(A) > 0 and
‚Ñô(B) > 0. Under which conditions, do we have

2.3 Problems
‡±™
129
‚Ñô(A|B) = ‚Ñô(B|A) ?
Problem 2.12. Let A and B certain events in a probability space with ‚Ñô(B) > 0. Do we
have
‚Ñô(A ‚à™B|B) = ‚Ñô(A)
and/or
‚Ñô(A ‚à©B|B) = ‚Ñô(A ‚à©B) ?
Give a proof or a counterexample.
Problem 2.13.
(a) Let (Œ©, ùíú, ‚Ñô) be a probability space. Suppose we are given events A1, . . . , An with
‚Ñô(A1 ‚à©‚ãÖ‚ãÖ‚ãÖ‚à©An‚àí1) > 0. Prove the following chain rule for conditional probabilities:
‚Ñô(A1 ‚à©‚ãÖ‚ãÖ‚ãÖ‚à©An) = ‚Ñô(A1)‚Ñô(A2|A1) ‚ãÖ‚ãÖ‚ãÖ‚Ñô(An|A1 ‚à©A2 ‚à©‚ãÖ‚ãÖ‚ãÖ‚à©An‚àí1).
Argue why all occurring conditional probabilities are well defined.
(b) Choose at random three numbers from 1 to 10 without replacement. Find the proba-
bility that the first number is even, the second one is odd, and the third one is again
even.
(c) Compare this probability with that of the following event: among three randomly
chosen numbers in {1, . . . , 10}, there are exactly two even and one odd.
Problem 2.14. Three persons, say X, Y, and Z, stand randomly in a row. All orderings
are assumed to be equally likely. Event A occurs if Y stands on the right-hand side of
X while B occurs in the case that Z is on the right-hand side of X. Hereby, we do not
suppose that Y and X or that Z and X stand directly next to each other. Are events A and
B independent or dependent?
Problem 2.15. Prove the following generalization of part 1 in Proposition 2.2.7. Let A ‚ààùíú
be an event with either ‚Ñô(A) = 0 or ‚Ñô(A) = 1. Then for any B ‚ààùíú, the events A and B
are independent.
Problem 2.16. Let (Œ©, ùíú, ‚Ñô) be a probability space. Given independent events A1, . . . , An
in ùíú, prove that
‚Ñô(
n
‚ãÉ
j=1
Aj) = 1 ‚àí
n
‚àè
j=1
(1 ‚àí‚Ñô(Aj)).
(2.23)
Use 1 ‚àíx ‚â§e‚àíx, x ‚â•0, to derive from eq. (2.23) the following:
If independent events6 A1, A2, . . . satisfy ‚àë‚àû
j=1 ‚Ñô(Aj) = ‚àû, then ‚Ñô(‚ãÉ‚àû
j=1 Aj) = 1.
Problem 2.17. An electric circuit (see Fig. 2.1) contains four switches A, B, C, and D. Each
of the switches is independently open or closed (then electricity flows). The switches are
6 Compare with Definition 7.1.17: for each n ‚àà‚Ñï, the events A1, . . . , An are independent.

130
‡±™
2 Conditional probabilities and independence
open with probability 1 ‚àíp and closed with probability p. Here, 0 ‚â§p ‚â§1 is given. Find
the probability that electricity flows from the left to the right.
Figure 2.1: An electric circuit with four switches.
Problem 2.18. Let (Œ©, ùíú, ‚Ñô) be a probability space. Suppose A and B are disjoint events
with ‚Ñô(A) > 0 and P(B) > 0. Is it possible that A and B are independent?
Problem 2.19. Let A, B, and C be three independent events.
1.
Show that A ‚à©B and C are independent as well.
2.
Even more, show that the independence of A, B, and C implies that of the events
A ‚à™B and C.
Problem 2.20.
1.
Suppose that A and C, as well as B and C, are independent. Furthermore, assume
A ‚à©B = 0. Show that A ‚à™B and C are independent as well.
2.
Give an example showing that the preceding assertion becomes false without the
assumption A ‚à©B = 0.
Hint: To construct such an example, because of Problem 2.19, the events A, B, and C
cannot be chosen to be independent. Therefore, the sets defined in Example 2.2.9 are
natural candidates for such an example.
Problem 2.21. Suppose ‚Ñô(A|B) = ‚Ñô(A|Bc) for some events A and B with 0 < ‚Ñô(B) < 1.
Does this imply that A and B are independent?
Problem 2.22. Is it possible that an event A is independent of itself? If yes, which events
A have this property? Similarly, which A are independent of Ac?
Problem 2.23. Let A, B, and C be three independent events with
‚Ñô(A) = ‚Ñô(B) = ‚Ñô(C) = 1
3.
Evaluate
‚Ñô((A ‚à©B) ‚à™(A ‚à©C)).

3 Random variables and their distribution
3.1 Transformation of random values
Assume the probability space (Œ©, ùíú, ‚Ñô) describes a certain random experiment, for ex-
ample, rolling a die or tossing a coin. If the experiment is executed, a random result
œâ ‚ààŒ© shows up. In a second step, we transform this observed result via a mapping
X : Œ© ‚Üí‚Ñù. In this way we obtain a (random) real number X(œâ). Let us point out that X
is a fixed, nonrandom function from Œ© into ‚Ñù; the randomness of X(œâ) stems from the
input œâ ‚ààŒ©.
Example 3.1.1. Toss a fair coin, labeled on one side with ‚Äú0‚Äù and on the other side with
‚Äú1,‚Äù exactly n times. The appropriate probability space is (Œ©, ùí´(Œ©), ‚Ñô), where Œ© = {0, 1}n
and ‚Ñôis the uniform distribution on Œ©. The result of the experiment is a vector œâ =
(œâ1, . . . , œân) with œâj = 0 or œâj = 1. Let X from Œ© to ‚Ñùbe defined by
X(œâ) = X(œâ1, . . . , œân) = œâ1 + ‚ãÖ‚ãÖ‚ãÖ+ œân .
Then X(œâ) tells us how often ‚Äú1‚Äù occurred, but we do no longer know in which order this
happened. Of course, X(œâ) is random because, if one tosses the coin another n times, it
is very likely that X attains a value different from that in the first trial.
Here we state the most important question in this topic: how are the values of X
distributed? As we know, in this case X attains a value k ‚â§n with probability (n
k)2‚àín.
Example 3.1.2. Roll a fair die twice. The sample space describing this experiment con-
sists of pairs œâ = (œâ1, œâ2), where œâ1, œâ2 ‚àà{1, . . . , 6}. Now define the mapping X : Œ© ‚Üí‚Ñù
by X(œâ) := max{œâ1, œâ2}. Thus, instead of recording the values of both rolls, we are only
interested in the larger one.
Other possible transformations are, for example, X1(œâ) := min{œâ1, œâ2} or also
X2(œâ1, œâ2) := œâ1 + œâ2.
Let A ‚ààùíúbe an event. Recall that this event A occurs if and only if we observe an
œâ ‚ààA. Suppose now X : Œ© ‚Üí‚Ñùis a given mapping from Œ© into ‚Ñùand let B ‚äÜ‚Ñùbe some
event. When do we observe an œâ ‚ààŒ© for which we have X(œâ) ‚ààB or, equivalently, when
does the event
{X ‚ààB} := {œâ ‚ààŒ© : X(œâ) ‚ààB}
occur? To answer this question, let us recall the definition of the preimage of B with
respect to X as given in eq. (A.2):
X‚àí1(B) := {œâ ‚ààŒ© : X(œâ) ‚ààB} .
We observe an œâ ‚ààŒ© for which X(œâ) ‚ààB if and only if œâ ‚ààX‚àí1(B). In other words, the
event {X ‚ààB} occurs if and only if X‚àí1(B) does. Consequently, the probability to observe
https://doi.org/10.1515/9783111325064-003

132
‡±™
3 Random variables and their distribution
an œâ ‚ààŒ© with X(œâ) ‚ààB should be ‚Ñô(X‚àí1(B)). But to this end, we have to know that
X‚àí1(B) ‚ààùíú; otherwise ‚Ñô(X‚àí1(B)) is not defined at all. Thus, a natural condition for X is
X‚àí1(B) ‚ààùíúfor ‚Äúsufficiently many‚Äù subsets B ‚äÜ‚Ñù. The precise mathematical condition
reads as follows.
Definition 3.1.3. Let (Œ©, ùíú, ‚Ñô) be a probability space. A mapping X : Œ© ‚Üí‚Ñùis called a (real-valued) ran-
dom variable (sometimes also random real number), provided that it satisfies the following condition:
B ‚àà‚Ñ¨(‚Ñù)
always implies
X‚àí1(B) ‚ààùíú.
(3.1)
Verbally, this condition says that for each Borel set B ‚äÜ‚Ñù, its preimage X‚àí1(B) has to be an element of
the œÉ-field ùíú.
Remark 3.1.4. Condition (3.1) is purely technical and will not be important later on. But,
in general, it cannot be avoided, at least if ùíú
Ã∏= ùí´(Œ©). On the contrary, if ùíú= ùí´(Œ©), for
example, if either Œ© is finite or countably infinite, then every mapping X : Œ© ‚Üí‚Ñùis
a random variable. Indeed, in this case the condition X‚àí1(B) ‚ààùíúis trivially always
satisfied.
Remark 3.1.5. In order to verify that a given mapping X : Œ© ‚Üí‚Ñùis a random variable,
it is not necessary to show X‚àí1(B) ‚ààùíúfor all Borel sets B ‚äÜ‚Ñù. It suffices to prove this
only for some special Borel sets B. More precisely, the following proposition holds.
Proposition 3.1.6. A function X : Œ© ‚Üí‚Ñùis a random variable if and only if, for all t ‚àà‚Ñù,
we have
X‚àí1((‚àí‚àû, t]) = {œâ ‚ààŒ© : X(œâ) ‚â§t} ‚ààùíú.
(3.2)
The assertion remains valid when we replace the intervals (‚àí‚àû, t] with intervals of the
form (‚àí‚àû, t), or we may take intervals [t, ‚àû) and also (t, ‚àû).
Proof. Suppose first that X is a random variable. Given t ‚àà‚Ñù, the interval (‚àí‚àû, t] is a
Borel set, hence X‚àí1((‚àí‚àû, t]) ‚ààùíú. Thus, each random variable satisfies condition (3.2).
To prove the converse implication, let X be a mapping from Œ© to ‚Ñùsatisfying condi-
tion (3.2) for each t ‚àà‚Ñù. Set
ùíû:= {C ‚àà‚Ñ¨(‚Ñù) : X‚àí1(C) ‚ààùíú}.
In the first step, one proves1 that ùíûis a œÉ-field. Moreover, (3.2) implies (‚àí‚àû, t] ‚ààùíûfor
each t ‚àà‚Ñù. But ‚Ñ¨(‚Ñù) is the smallest œÉ-field containing all these intervals. Since ùíûis
another œÉ-field containing the intervals (‚àí‚àû, t], it has to be larger2 than the smallest one,
that is, we have ùíû‚äá‚Ñ¨(‚Ñù). In other words, every Borel set belongs to ùíûor, equivalently,
1 Use Proposition A.2.1 to verify this.
2 By the construction of ùíû, it even coincides with ‚Ñ¨(‚Ñù).

3.2 Probability distribution of a random variable
‡±™
133
for all B ‚àà‚Ñ¨(‚Ñù) it follows that X‚àí1(B) ‚ààùíú. Thus, as asserted, X is a random variable.
The proof for intervals of the other types goes along the same lines. Here one has to use
that these intervals generate the œÉ-field of Borel sets as well.
Summary: Let (Œ©, ùíú, ‚Ñô) be a probability space. A function X : Œ© ‚Üí‚Ñùis a random variable provided that it
satisfies the following (purely technical) condition: for any Borel set B ‚äÜ‚Ñù,
X‚àí1(B) = {œâ ‚ààŒ© : X(œâ) ‚ààB} ‚ààùíú.
This is equivalent to the property that for all t ‚àà‚Ñùone has
{œâ ‚ààŒ© : X(œâ) ‚â§t} ‚ààùíú.
3.2 Probability distribution of a random variable
Suppose we are given a random variable X : Œ© ‚Üí‚Ñù. We define now a mapping ‚ÑôX from
‚Ñ¨(‚Ñù) to [0, 1] as follows:
‚ÑôX (B) := ‚Ñô(X‚àí1(B)) = ‚Ñô{œâ ‚ààŒ© : X(œâ) ‚ààB} ,
B ‚àà‚Ñ¨(‚Ñù) .
Observe that ‚ÑôX is well defined. Indeed, since X is a random variable, for all Borel sets
B ‚äÜ‚Ñùwe have X‚àí1(B) ‚ààùíú, hence ‚Ñô(X‚àí1(B)) makes sense.
To simplify the notation, given B ‚àà‚Ñ¨(‚Ñù), we will often write
‚Ñô{X ‚ààB} = ‚Ñô{œâ ‚ààŒ© : X(œâ) ‚ààB} .
This is generally used and does not lead to any confusion. Having said this, we may now
define ‚ÑôX also by
‚ÑôX(B) = ‚Ñô{X ‚ààB} .
A first easy example shows how ‚ÑôX is calculated in concrete cases. Other more interest-
ing examples will follow after some necessary preliminary considerations.
Example 3.2.1. Toss a fair coin, labeled on one side by ‚Äú0‚Äù and on the other side by ‚Äú1‚Äù,
three times. The sample space is Œ© = {0, 1}3 with the uniform distribution ‚Ñôdescribing
probability measure. Let the random variable X on Œ© be defined by
X(œâ) := œâ1 + œâ2 + œâ3
whenever œâ = (œâ1, œâ2, œâ3) ‚ààŒ© .
It follows that
‚ÑôX({0}) = ‚Ñô{X = 0} = ‚Ñô({(0, 0, 0)}) = 1
8,

134
‡±™
3 Random variables and their distribution
‚ÑôX({1}) = ‚Ñô{X = 1} = ‚Ñô({(1, 0, 0), (0, 1, 0), (0, 0, 1)}) = 3
8,
‚ÑôX({2}) = ‚Ñô{X = 2} = ‚Ñô({(1, 1, 0), (0, 1, 1), (1, 0, 1)}) = 3
8,
‚ÑôX({3}) = ‚Ñô{X = 3} = ‚Ñô({(1, 1, 1)}) = 1
8 .
Of course, these values describe the distribution of X completely. Indeed, whenever
B ‚äÜ‚Ñù,
‚ÑôX(B) =
3
‚àë
k=0
k‚ààB
‚ÑôX({k}) .
For example, we have
‚ÑôX([‚àí1, 1]) = ‚ÑôX({0}) + ‚ÑôX({1}) = 1
8 + 3
8 = 1
2 .
So, with probability 1/2, we observe an œâ = (œâ1, œâ2, œâ3) for which
‚àí1 ‚â§œâ1 + œâ2 + œâ3 ‚â§1 .
The proof of the next result heavily depends on properties of the preimage proved
in Proposition A.2.1.
Proposition 3.2.2. Let (Œ©, ùíú, ‚Ñô) be a probability space. For a random variable X : Œ© ‚Üí‚Ñù,
the mapping ‚ÑôX : ‚Ñ¨(‚Ñù) ‚Üí[0, 1] is a probability measure.
Proof. Using property (1) in Proposition A.2.1, one easily gets
‚ÑôX(0) = ‚Ñô(X‚àí1(0)) = ‚Ñô(0) = 0,
as well as
‚ÑôX(‚Ñù) = ‚Ñô(X‚àí1(‚Ñù)) = ‚Ñô(Œ©) = 1 .
Thus it remains to verify the œÉ-additivity of ‚ÑôX. Take any sequence of disjoint Borel sets
B1, B2, . . . in ‚Ñù. Then also X‚àí1(B1), X‚àí1(B2), . . . are disjoint subsets of Œ©. To see this, apply
Proposition A.2.1, which, if i
Ã∏= j, implies
X‚àí1(Bi) ‚à©X‚àí1(Bj) = X‚àí1(Bi ‚à©Bj) = X‚àí1(0) = 0 .
Another application of Proposition A.2.1 and of the œÉ-additivity of ‚Ñôfinally gives
‚ÑôX(
‚àû
‚ãÉ
j=1
Bj) = ‚Ñô(X‚àí1(
‚àû
‚ãÉ
j=1
Bj)) = ‚Ñô(
‚àû
‚ãÉ
j=1
X‚àí1(Bj))

3.2 Probability distribution of a random variable
‡±™
135
=
‚àû
‚àë
j=1
‚Ñô(X‚àí1(Bj)) =
‚àû
‚àë
j=1
‚ÑôX(Bj) .
Hence, ‚ÑôX is a probability measure, as asserted.
Definition 3.2.3. The probability measure ‚ÑôX on (‚Ñù, ‚Ñ¨(‚Ñù)) defined by
‚ÑôX(B) := ‚Ñô(X‚àí1(B)) = ‚Ñô{œâ ‚ààŒ© : X(œâ) ‚ààB} = ‚Ñô{X ‚ààB} ,
B ‚àà‚Ñ¨(‚Ñù),
is called the probability distribution of X (with respect to ‚Ñô) or, in short, the distribution of X.
Remark 3.2.4. The distribution ‚ÑôX is the most important characteristic of a random
variable X. In general, it is completely unimportant how a random variable is defined
analytically; only its distribution matters. Thus, two random variables with identical
distributions may be regarded as equivalent because they describe the same random
experiment.
Remark 3.2.4 leads us to the following definition:
Definition 3.2.5. Two random variables X1 and X2 are said to be identically distributed provided that
‚ÑôX1 = ‚ÑôX2. Hereby, it is not necessary that X1 and X2 be defined on the same sample space. Only their
distributions have to coincide. In the case of identically distributed X1 and X2, one writes X1
d= X2 .
Example 3.2.6. Toss a fair coin, labeled on each side by ‚Äú0‚Äù or ‚Äú1,‚Äù twice. Let X1 be the
value of the first toss and X2 that of the second. Then
‚Ñô{X1 = 0} = ‚Ñô{X2 = 0} = 1
2 = ‚Ñô{X1 = 1} = ‚Ñô{X2 = 1} .
Hence, X1 and X2 are identically distributed, or X1
d= X2. Both random variables describe
the same experiment, namely a single toss of a fair coin. Now, toss the coin a third time
and let X3 be the result of the third trial. Then we also have X1
d= X3, but note that X1 and
X3 are defined on different sample spaces.
Next, we state and prove some general rules for evaluating the probability distri-
bution of a given random variable. Here we have to distinguish between two different
types of random variables, namely between discrete and continuous ones. Let us start
with the discrete case.
Definition 3.2.7. A random variable X is discrete provided there exists an at most countably infinite set
D ‚äÇ‚Ñùsuch that X: Œ© ‚ÜíD.
In other words, a random variable is discrete if it attains at most countably infinitely many different
values.

136
‡±™
3 Random variables and their distribution
Remark 3.2.8. If a random variable X is discrete with values in D ‚äÇ‚Ñù, then, of course,
‚ÑôX(D) = ‚Ñô{X ‚ààD} = 1 .
Consequently, in this case its probability distribution ‚ÑôX is a discrete probability mea-
sure on ‚Ñù. In general, the converse is not valid as the next example shows.
Example 3.2.9. We model the experiment of rolling a fair die by the probability space
(‚Ñù, ùí´(‚Ñù), ‚Ñô), where ‚Ñô({1}) = ‚ãÖ‚ãÖ‚ãÖ= ‚Ñô({6}) = 1/6 and ‚Ñô({x}) = 0 provided that x
Ã∏= 1, . . . , 6.
If X : ‚Ñù‚Üí‚Ñùis defined by X(s) = s2 then, of course, ‚ÑôX is discrete. Indeed, we have
‚ÑôX(D) = 1, where D = {1, 4, 9, 16, 25, 36}. On the other hand, X does not attain values in a
countably infinite set; its range is [0, ‚àû).
Remark 3.2.10. If we look at Example 3.2.9 more thoroughly, then it becomes immedi-
ately clear that the values of X outside of {1, . . . , 6} are completely irrelevant. With a
small change of X, it will attain values in D. More precisely, let
ÃÉX(œâ) = 1 if œâ
Ã∏= 1, . . . , 6
and ÃÉX(k) = k2, k = 1, . . . , 6; then X
d=
ÃÉX and ÃÉX has values in {1, 4, 9, 16, 25, 36}.
This procedure is also possible in general: if ‚ÑôX is discrete with ‚ÑôX(D) = 1 for some
countable set D, then we may change X to
ÃÉX such that X
d=
ÃÉX and
ÃÉX : Œ© ‚ÜíD. Indeed,
choose some fixed d0 ‚ààD and set ÃÉX(œâ) = X(œâ) if œâ ‚ààX‚àí1(D) and ÃÉX(œâ) = d0 otherwise.
Then ‚ÑôX = ‚ÑôÃÉX and ÃÉX has values in D.
Convention 3.1. Without losing generality, we may always assume the following: if a
random variable X has a discrete probability distribution, that is, ‚Ñô{X ‚ààD} = 1 for some
finite or countably infinite set D, then X attains values in D.
The second type of random variables we investigate is that of continuous ones.3
Definition 3.2.11. A random variable X is said to be continuous provided that its distribution ‚ÑôX is a
continuous probability measure. That is, ‚ÑôX possesses a density p. This function p is called the density
function or, in short, the density of the random variable X.
Remark 3.2.12. One should not confuse the continuity of a random variable with the
continuity of a function as taught in Calculus. The latter is an (analytic) property of a
function, while the former is a property of its distribution. Moreover, whether or not
a random variable X is continuous depends not only on X, but also on the underlying
probability space.
Remark 3.2.13. Another way to express that a random variable is continuous is as fol-
lows: there exists a function p : ‚Ñù‚Üí[0, ‚àû) (the density of X) such that
‚Ñô{œâ ‚ààŒ© : X(œâ) ‚â§t} = ‚Ñô{X ‚â§t} =
t
‚à´
‚àí‚àû
p(x) dx ,
t ‚àà‚Ñù,
3 The precise term would be ‚Äúabsolutely continuous‚Äù; but for simplicity let us call them ‚Äúcontinuous.‚Äù

3.2 Probability distribution of a random variable
‡±™
137
or, equivalently, for all real numbers a < b,
‚Ñô{œâ ‚ààŒ© : a ‚â§X(œâ) ‚â§b} = ‚Ñô{a ‚â§X ‚â§b} =
b
‚à´
a
p(x) dx .
How do we determine the probability distribution of a given random variable? To
answer this question, let us first consider the case of discrete random variables.
Thus, let X be discrete with values in D = {x1, x2, . . . } ‚äÇ‚Ñù. Then, as observed above,
it follows that ‚ÑôX(D) = 1, and, consequently, ‚ÑôX is uniquely determined by the numbers
pj := ‚ÑôX({xj}) = ‚Ñô{X = xj} = ‚Ñô{œâ ‚ààŒ© : X(œâ) = xj} ,
j = 1, 2, . . .
(3.3)
Moreover, for any B ‚äÜ‚Ñùit follows that
‚Ñô{œâ ‚ààŒ© : X(œâ) ‚ààB} = ‚ÑôX(B) = ‚àë
xj‚ààB
pj .
Consequently, in order to determine ‚ÑôX for discrete X, it completely suffices to deter-
mine the pjs defined by eq. (3.3). If we know (pj)j‚â•1, then the probability distribution ‚ÑôX
of X is completely described.
Remark 3.2.14. In the literature, quite often, one finds a slightly different approach for
the description of ‚ÑôX. Define p : ‚Ñù‚Üí[0, 1] by
p(x) = ‚Ñô{X = x} ,
x ‚àà‚Ñù.
(3.4)
This function p is then called the probability mass function of X. Note that p(x) = 0
whenever x ‚àâD. This function p satisfies p(x) ‚â•0, ‚àëx‚àà‚Ñùp(x) = 1, and
‚Ñô{X ‚ààB} = ‚àë
x‚ààB
p(x) .
In this setting, the numbers pj in eq. (3.3) coincide with p(xj).
Example 3.2.15. Roll a fair die twice. Let X on {1, . . . , 6}2 be defined by
X(œâ) = X(œâ1, œâ2) := œâ1 + œâ2 ,
œâ = (œâ1, œâ2) .
Which distribution does X possess?
Answer: The very first question one has to answer is always about the possible val-
ues of X. In our case, X attains values in D = {2, . . . , 12}, thus it suffices to determine
‚ÑôX({k}) = ‚Ñô{X = k} = ‚Ñô{(œâ1, œâ2) ‚ààŒ© : X(œâ1, œâ2) = k} ,
k = 2, . . . , 12 .
One easily gets

138
‡±™
3 Random variables and their distribution
‚ÑôX({2}) = ‚Ñô{(œâ1, œâ2) : œâ1 + œâ2 = 2} = |{(1, 1)}|
36
= 1
36,
‚ÑôX({3}) = ‚Ñô{(œâ1, œâ2) : œâ1 + œâ2 = 3} = |{(1, 2), (2, 1)}|
36
= 2
36,
...
‚ÑôX({7}) = ‚Ñô{(œâ1, œâ2) : œâ1 + œâ2 = 7} = |{(1, 6), . . . , (6, 1)}|
36
= 6
36,
...
‚ÑôX({12}) = ‚Ñô{(œâ1, œâ2) : œâ1 + œâ2 = 12} = |{(6, 6)}|
36
= 1
36,
hence ‚ÑôX is completely described. For example, it follows that
‚Ñô{X ‚â§4} = ‚ÑôX((‚àí‚àû, 4]) = ‚ÑôX({2}) + ‚ÑôX({3}) + ‚ÑôX({4}) = 1
36 + 2
36 + 3
36 = 1
6 .
Example 3.2.16. A biased coin is labeled on one side by ‚Äú0‚Äù and on the other side by
‚Äú1‚Äù: for some p ‚àà[0, 1], number ‚Äú1‚Äù shows up with probability p, thus ‚Äú0‚Äù appears with
probability 1 ‚àíp. We toss the coin n times. The result is a sequence œâ = (œâ1, . . . , œân),
where œâi ‚àà{0, 1}, hence the describing sample space is
Œ© = {0, 1}n = {œâ = (œâ1, . . . , œân) : œâi ‚àà{0, 1}} .
For i ‚â§n, let Xi : Œ© ‚Üí‚Ñùbe defined by Xi(œâ) := œâi. That is, Xi(œâ) is the value of the ith
trial. What distribution does Xi possess?
Answer: In Example 1.9.13, we determined the probability measure ‚Ñôon ùí´(Œ©),
which describes the n-fold tossing of a biased coin. This probability measure was given
by
‚Ñô({œâ}) = pk (1 ‚àíp)n‚àík ,
k =
n
‚àë
j=1
œâj
where œâ = (œâ1, . . . , œân) .
(3.5)
The random variable Xi only attains the values ‚Äú0‚Äù and ‚Äú1.‚Äù Thus, in order to determine
‚ÑôXi, it suffices to evaluate ‚ÑôXi({0}) = ‚Ñô{œâ ‚ààŒ© : œâi = 0}. Let œâ ‚ààŒ© be a sequence with
œâi = 0. Then it may contain the value ‚Äú1‚Äù at most n ‚àí1 times. Given k ‚â§n ‚àí1, there are
exactly (n‚àí1
k ) such sequences œâ with œâi = 0 and with k times ‚Äú1.‚Äù Therefore, we obtain
‚ÑôXi({0}) = ‚Ñô{œâ ‚ààŒ© : œâi = 0} =
n‚àí1
‚àë
k=0
‚Ñô{œâ ‚ààŒ© : œâi = 0 , œâ1 + ‚ãÖ‚ãÖ‚ãÖ+ œân = k}
=
n‚àí1
‚àë
k=0
(n ‚àí1
k ) pk (1 ‚àíp)n‚àík = (1 ‚àíp)
n‚àí1
‚àë
k=0
(n ‚àí1
k ) pk (1 ‚àíp)n‚àí1‚àík
= (1 ‚àíp)[p + (1 ‚àíp)]
n‚àí1 = 1 ‚àíp .
Of course, this also implies ‚ÑôXi({1}) = p.

3.2 Probability distribution of a random variable
‡±™
139
Remark 3.2.17. Note that all X1, . . . , Xn possess the same distribution, that is,
X1
d= ‚ãÖ‚ãÖ‚ãÖ
d= Xn .
Example 3.2.18. Roll a fair die twice and let œâ1 and œâ2 be the results of the first and
second toss, respectively. Define the random variable X by
X(œâ1, œâ2) = |œâ1 ‚àíœâ2| ,
œâ1, œâ2 ‚àà{1, . . . , 6} .
In a first step, we observe that X has values in D = {0, . . . , 5}. Hence, it suffices to deter-
mine PX({k}) = ‚Ñô(X = k) for k = 0, . . . , 5. Doing so, we easily get
‚ÑôX({0}) = 1
6
and
‚ÑôX({1}) = 5
18,
. . . ,
‚ÑôX({5}) = 1
18 .
Summary: Let X : Œ© ‚Üí‚Ñùbe a discrete random variable. In order to describe its distribution ‚ÑôX, one has to
do two things:
(1)
Determine the finite or countably infinite set D ‚äÇ‚Ñùfor which ‚Ñô{X ‚ààD} = 1.
(2)
For each x ‚ààD, evaluate
‚ÑôX({x}) = ‚Ñô{X = x} = ‚Ñô{œâ ‚ààŒ© : X(œâ) = x} .
If B ‚äÜ‚Ñù, then it follows that
‚Ñô{X ‚ààB} = ‚àë
x‚ààB‚à©D
‚ÑôX({x}) = ‚àë
x‚ààB‚à©D
‚Ñô{X = x} .
How do we determine the probability distribution of a random variable if it is contin-
uous? For each x ‚àà‚Ñù, ‚Ñô{X = x} = 0, hence the values of ‚Ñô{X = x} cannot be used
to describe ‚ÑôX as they did in the discrete case. Consequently, a different approach is
needed, and this approach is based on the use of distribution functions.
Definition 3.2.19. Let X be a random variable, either discrete or continuous. Then its (cumulative) dis-
tribution function FX : ‚Ñù‚Üí[0, 1] is defined by
FX(t) := ‚ÑôX((‚àí‚àû, t]) = ‚Ñô{X ‚â§t} ,
t ‚àà‚Ñù.
(3.6)
Remark 3.2.20. Observe that for discrete and continuous random variables the distri-
bution function equals
FX(t) = ‚àë
xj‚â§t
pj
and
FX(t) =
t
‚à´
‚àí‚àû
p(x) dx,
respectively. Here, in the discrete case, the xjs and pjs are as in eq. (3.3), while p denotes
the density of X in the continuous case.

140
‡±™
3 Random variables and their distribution
Furthermore, note that FX is nothing else than the distribution function of the prob-
ability measure ‚ÑôX, as it was introduced in Definition 1.7.1. Consequently, it possesses all
properties of a ‚Äúusual‚Äù distribution function as stated in Proposition 1.7.13.
Summary: Let X be a random variable on a probability space (Œ©, ùíú, ‚Ñô). The probability measure ‚ÑôX defined
by
‚ÑôX(B) = ‚Ñô{œâ ‚ààŒ© : X(œâ) ‚ààB} ,
B ‚àà‚Ñ¨(‚Ñù) ,
denotes the probability distribution of X and
FX(t) = ‚Ñô{œâ ‚ààŒ© : X(œâ) ‚â§t} ,
t ‚àà‚Ñù,
is its (cumulative) distribution function.
Proposition 3.2.21. Let FX be defined by eq. (3.6). Then it possesses the following proper-
ties:
(1) The function FX is nondecreasing.
(2) It follows FX(‚àí‚àû) = 0 as well as FX(‚àû) = 1.
(3) The function FX is continuous from the right.
Furthermore, if t ‚àà‚Ñù, then
‚Ñô{X = t} = FX(t) ‚àíFX(t ‚àí0) .
In particular, if X is continuous, then FX is a continuous function from ‚Ñùto [0, 1].
Remark 3.2.22. Note that the converse of the last implication does not hold. Indeed,
there exist random variables X for which FX is continuous, but X does not possess a
density. Such random variables are said to be singularly continuous. These are exactly
those random variables for which the probability measure ‚ÑôX is singularly continuous
in the sense of Remark 1.7.20.
The next result shows that under slightly stronger conditions about FX a density of
X exists.
Proposition 3.2.23. Let FX be continuous and continuously differentiable with the excep-
tion of at most finitely many points. Then X is continuous with density p(t) =
d
dt FX(t).
Hereby the values of p may be chosen arbitrarily at points where the derivative does not
exist; for example, set p(t) = 0 for those points.
Proof. The proof follows from the corresponding properties of distribution functions
for probability measures. Recall that FX is the distribution function of ‚ÑôX.
The previous proposition provides us with a method to determine the density of a
given random variable X. Evaluate the distribution function FX and differentiate it. The
obtained derivative is the density function we are looking for.

3.2 Probability distribution of a random variable
‡±™
141
The next three examples demonstrate how this method applies.
Example 3.2.24. Let ‚Ñôbe the uniform distribution on a sphere K of radius 1. That is, for
each Borel set B ‚àà‚Ñ¨(‚Ñù2), we have
‚Ñô(B) = vol2(B ‚à©K)
vol2(K)
= vol2(B ‚à©K)
œÄ
.
Define the random variable X : ‚Ñù2 ‚Üí‚Ñùby X(x1, x2) := x1. Of course, FX(t) = 0 whenever
t < ‚àí1 and FX(t) = 1 when t > 1. Thus, it suffices to determine FX(t) if ‚àí1 ‚â§t ‚â§1. For
those t ‚àà‚Ñù, we obtain
FX(t) = vol2(St ‚à©K)
œÄ
where St is the half-space {(x1, x2) ‚àà‚Ñù2 : x1 ‚â§t} (compare Figure 3.1).
Figure 3.1: The gray shaded set represents the intersection between K and the half-space St.
If |t| ‚â§1, then
vol2(St ‚à©K) = 2
t
‚à´
‚àí1
‚àö1 ‚àíx2 dx ,
hence,
FX(t) = 2
œÄ
t
‚à´
‚àí1
‚àö1 ‚àíx2 dx ,
|t| ‚â§1 ,
and by the fundamental theorem of Calculus, we finally get

142
‡±™
3 Random variables and their distribution
p(t) = d
dt FX(t) = 2
œÄ
‚àö1 ‚àít2 ,
|t| ‚â§1 .
Summing up, the random variable X has the density p with
p(t) = {
2
œÄ ‚àö1 ‚àít2
if |t| ‚â§1 ,
0
if |t| > 1 .
(3.7)
Example 3.2.25. The probability space is the same as in Example 3.2.24, but this time we
define X by
X(x1, x2) := ‚àöx2
1 + x2
2 ,
(x1, x2) ‚àà‚Ñù2 .
Of course, it follows FX(t) = 0 if t < 0 while FX(t) = 1 if t > 1. Take t ‚àà[0, 1]. Then
FX(t) = vol2(K(t))
vol2(K(1)) = t2œÄ
œÄ
= t2 ,
where K(t) denotes a sphere of radius t. Differentiating FX with respect to t gives the
density
p(t) = {2 t
if 0 ‚â§t ‚â§1,
0
otherwise.
Example 3.2.26. Let ‚Ñôbe the uniform distribution on [0, 1] and define the random vari-
able X by X(s) = min{s, 1 ‚àís}, s ‚àà‚Ñù. Find the probability distribution of X.
Answer: It is not difficult to see that
‚Ñô{X ‚â§t} = 0
if t < 0
and
‚Ñô{X ‚â§t} = 1
if t > 1/2 .
Thus it remains to evaluate FX(t) for 0 ‚â§t ‚â§1/2. Here we obtain
FX(t) = ‚Ñô{X ‚â§t} = ‚Ñô{s ‚àà[0, 1] : 0 ‚â§s ‚â§t or 1 ‚àít ‚â§s ‚â§1}
= ‚Ñô{s ‚àà[0, 1] : 0 ‚â§s ‚â§t} + ‚Ñô{s ‚àà[0, 1] : 1 ‚àít ‚â§s ‚â§1} = 2t .
Differentiating gives F‚Ä≤
X(t) = 2 if 0 ‚â§t ‚â§1/2 and F‚Ä≤
X(t) = 0 otherwise. Hence ‚ÑôX is the
uniform distribution on [0, 1/2].
Summary: To determine the density of a continuous random variable, proceed as follows:
1.
Evaluate (if possible) the distribution function FX(t) = ‚Ñô{X ‚â§t}.
2.
Differentiate FX. If the derivative p(t) = F‚Ä≤
X(t) is piecewise continuous, then p is the desired density of X.

3.3 Special random variables
‡±™
143
3.3 Special random variables
We agree upon the following notation: a random variable X is said to be ABC-distributed
(or distributed according to ABC) if its probability distribution is a probability measure
of type ABC. For example, a random variable is said to be Bn,p-distributed (or distributed
according to Bn,p) if ‚ÑôX = Bn,p, that is, if
‚Ñô{X = k} = (n
k)pk(1 ‚àíp)n‚àík ,
k = 0, . . . , n .
Remark 3.3.1. To shorten the notation, at a few places we also write X ‚àºABC whenever
‚ÑôX is the probability measure ABC. So, for example, X ‚àºBn,p tells us that
‚Ñô{X = k} = Bn,p({k}),
k = 0, . . . , n .
In this way, we define the following random variables of special type: X is
‚Äì
uniformly distributed on {x1, . . . , xN} if
‚Ñô{X = x1} = ‚ãÖ‚ãÖ‚ãÖ= ‚Ñô{X = xN} = 1
N ,
‚Äì
Poisson distributed or PoisŒª-distributed if
‚Ñô{X = k} = Œªk
k! e‚àíŒª ,
k = 0, 1, . . . ,
‚Äì
hypergeometrically distributed if
‚Ñô{X = m} =
(M
m) (N‚àíM
n‚àím)
(N
n)
,
m = 0, . . . , n,
‚Äì
Gp-distributed or geometrically distributed if
‚Ñô{X = k} = p (1 ‚àíp)k‚àí1 ,
k = 1, 2, . . . ,
‚Äì
B‚àí
n,p-distributed or negative binomial distributed if
‚Ñô{X = k} = (k ‚àí1
k ‚àín) pn(1 ‚àíp)k‚àín = (k ‚àí1
n ‚àí1) pn(1 ‚àíp)k‚àín ,
k = n, n + 1, . . . ,
or, equivalently, if
‚Ñô{X = n + k} = (‚àín
k ) pn (p ‚àí1)k ,
k = 0, 1, 2, . . .
Remark 3.3.2. In view of Convention 3.1, we may suppose that all random variables of
the preceding type are discrete. More precisely, we even may assume that X has values in

144
‡±™
3 Random variables and their distribution
the (at most countably infinite) set D with ‚ÑôX(D) = 1. For example, if X is Bn,p-distributed,
we may suppose that X has values in {0, . . . , n}.
In quite similar way, we denote specially distributed continuous random variables.
A real-valued random variable X is said to be
‚Äì
uniformly distributed on [Œ±, Œ≤] if ‚ÑôX is the uniform distribution on [Œ±, Œ≤]. That is, if
[a, b] ‚äÜ[Œ±, Œ≤], then
‚Ñô{a ‚â§X ‚â§b} = b ‚àía
Œ≤ ‚àíŒ± ,
‚Äì
normally distributed or ùí©(Œº, œÉ2)-distributed if
‚Ñô{a ‚â§X ‚â§b} =
1
‚àö2œÄœÉ
b
‚à´
a
e‚àí(x‚àíŒº)2/2œÉ2
dx ,
‚Äì
standard normally distributed if it is ùí©(0, 1)-distributed, that is,
‚Ñô{a ‚â§X ‚â§b} =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx ,
‚Äì
gamma distributed or ŒìŒ±,Œ≤-distributed if for 0 ‚â§a < b < ‚àû,
‚Ñô{a ‚â§X ‚â§b} =
1
Œ±Œ≤ Œì(Œ≤)
b
‚à´
a
xŒ≤‚àí1 e‚àíx/Œ± dx ,
‚Äì
EŒª,n-distributed or Erlang distributed if it is ŒìŒª‚àí1,n-distributed, that is, whenever
0 ‚â§a < b < ‚àû, then
‚Ñô{a ‚â§X ‚â§b} =
Œªn
(n ‚àí1)!
b
‚à´
a
xn‚àí1 e‚àíŒªx dx ,
or if, equivalently, for any t ‚â•0,
‚Ñô{X ‚â•t} =
n‚àí1
‚àë
j=0
(Œªt)j
j!
e‚àíŒªt .
‚Äì
EŒª-distributed or exponentially distributed if for 0 ‚â§a < b < ‚àû,
‚Ñô{a ‚â§X ‚â§b} = Œª
b
‚à´
a
e‚àíŒªx dx = e‚àíŒªa ‚àíe‚àíŒªb ,

3.4 Random vectors
‡±™
145
‚Äì
arcsine distributed if, given 0 ‚â§a < b ‚â§1,
‚Ñô{a ‚â§X ‚â§b} = 1
œÄ
b
‚à´
a
1
‚àöx(1 ‚àíx)
dx = 2
œÄ [arcsin(‚àöb) ‚àíarcsin(‚àöa)] .
‚Äì
Cauchy distributed if
‚Ñô{a ‚â§X ‚â§b} = 1
œÄ
b
‚à´
a
dx
1 + x2 = 1
œÄ [arctan b ‚àíarctan a] .
Remark 3.3.3. If a random variable X possesses a special distribution, then all proper-
ties of ‚ÑôX carry over to X. For example, in this language we may now formulate Poisson‚Äôs
limit theorem (Proposition 1.4.29) as follows.
Let Xn be Bn,pn-distributed and suppose that n pn ‚ÜíŒª > 0 as n ‚Üí‚àû. Then
lim
n‚Üí‚àû‚Ñô{Xn = k} = ‚Ñô{X = k} ,
k = 0, 1, . . . ,
where X is PoisŒª-distributed.
Or if X is gamma distributed, then ‚Ñô{X > 0} = ‚ÑôX((0, ‚àû)) = 1, and so on.
Remark 3.3.4. A common question is how does one get a random variable X possessing
a certain given distribution. For example, how do we construct a binomial or a normally
distributed random variable? Suppose we want to model the rolling of a die by a random
variable X, which is uniformly distributed on {1, . . . , 6}. The easiest solution is to take
Œ© = {1, . . . , 6} endowed with the uniform distribution ‚Ñôand define X by X(œâ) = œâ. But
this is not the only way to get such a random variable. One may also roll the die n times
and choose X as the value of the first (or of the second, etc.) roll. In a similar way, random
variables with other probability distributions may be constructed. Further possibilities
to model random variables will be investigated in Section 4.4.
Summary: There are two ways to model a random experiment. The classical approach is to construct a
probability space that describes this experiment. For example, if we toss a fair coin n times and record the
number of ‚Äúheads,‚Äù then this may be described by the sample space {0, . . . , n} endowed with the probability
measure Bn,1/2. Another way to model a certain random experiment is to choose a random variable X so that
the probability of the occurrence of an event B ‚äÜ‚Ñùequals ‚Ñô{X ‚ààB}. For example, the above experiment of
tossing a coin may also be described by a binomial random variable X (with parameters n and 1/2).
3.4 Random vectors
Suppose we are given n random variables X1, . . . , Xn defined on a sample space Œ©. Our
objective is to combine these n variables into a single variable. More precisely, we will
investigate the following type of vector-valued mappings.

146
‡±™
3 Random variables and their distribution
Definition 3.4.1. Let ‚ÉóX be a mapping from Œ© ‚Üí‚Ñùn represented as
‚ÉóX(œâ) = (X1(œâ), . . . , Xn(œâ)) ,
œâ ‚ààŒ©.
Then, ‚ÉóX is said to be an (n-dimensional) random vector or vector valued random variable, provided
that each of the Xjs is a (real-valued) random variable. The random variables Xj, 1 ‚â§j ‚â§n, are called the
coordinate mappings of ‚ÉóX.
Instead of ‚ÉóX, we may also write (X1, . . . , Xn), that is,
(X1, . . . , Xn)(œâ) = (X1(œâ), . . . , Xn(œâ)) ,
œâ ‚ààŒ© .
A random vector
‚ÉóX maps Œ© into ‚Ñùn, that is, we assign to each observed œâ ‚ààŒ© a vector
‚ÉóX(œâ). The mapping ‚ÉóX is again fixed and nonrandom. The randomness of ‚ÉóX(œâ) is caused
by the input.
Example 3.4.2. Roll a die two times. Let X1 be the maximum value, X2 the mini-
mum, and X3 the sum of both rolls. The three-dimensional vector
‚ÉóX = (X1, X2, X3) maps
Œ© = {1, . . . , 6}2 into ‚Ñù3. For example, the pair (2, 5) is mapped to (5, 2, 7) or the image of
(5, 6) is (6, 5, 11).
Example 3.4.3. Suppose there are N people in an auditorium. Enumerate them from 1
to N and choose one person according to the uniform distribution on {1, . . . , N}. Say we
have chosen person k. Let X1(k) be the height of this person and X2(k) his or her weight.
As a result, we get a random two-dimensional vector (X1, X2) mapping k to the vector
(X1(k), X2(k)) in ‚Ñù2.
Example 3.4.4. We place n balls into m urns successively. Hereby, each urn is equally
likely. If Xj denotes the number of balls in urn j, then we get an m-dimensional vector
‚ÉóX = (X1, . . . , Xm). Observe that the values of ‚ÉóX lie in the set
D = {(k1, . . . , km) : k1 + ‚ãÖ‚ãÖ‚ãÖ+ km = n} ‚äÜ‚Ñïm
0 .
Remark 3.4.5. The preceding examples suggest that the values of the coordinate map-
pings depend on each other. For instance, in Example 3.4.3 larger values of X1 make also
those of X2 more likely, and vice versa. A basic aim of the following sections is to confirm
this guess, that is, we want to find a mathematical formulation that describes whether
or not two or more random variables are dependent or independent.
Summary: Let (Œ©, ùíú, ‚Ñô) be a probability space. A mapping ‚ÉóX : Œ© ‚Üí‚Ñùn is said to be an (n-dimensional)
random vector if ‚ÉóX(œâ) = (X1(œâ), . . . , Xn(œâ)) with random variables Xj : Œ© ‚Üí‚Ñù.

3.5 Joint and marginal distributions
‡±™
147
3.5 Joint and marginal distributions
The values of the vector ‚ÉóX are randomly distributed in ‚Ñùn. Consequently, as in the case
of random variables, events of the form { ‚ÉóX ‚ààB} occur with certain probabilities. But, in
contrast to the case of random variables, the event B is now a subset of ‚Ñùn, not of ‚Ñùas
before. More precisely, for events B ‚äÜ‚Ñùn, we are interested in the following quantity:4
‚Ñô{œâ ‚ààŒ© :
‚ÉóX(œâ) ‚ààB} = ‚Ñô{œâ ‚ààŒ© : (X1(œâ), . . . , Xn(œâ)) ‚ààB} .
(3.8)
The next definition gives the exact formulation of the problem.
Definition 3.5.1. Let ‚ÉóX : Œ© ‚Üí‚Ñùn be a random vector with coordinate mappings X1, . . . , Xn. For each
Borel set B ‚àà‚Ñ¨(‚Ñùn), we set
‚Ñô‚ÉóX(B) = ‚Ñô(X1,...,Xn)(B) = ‚Ñô{ ‚ÉóX ‚ààB} .
(3.9)
The mapping ‚Ñô‚ÉóX from ‚Ñ¨(‚Ñùn) into [0, 1] is said to be the probability distribution, or, in short, the distri-
bution of ‚ÉóX. Often, ‚Ñô‚ÉóX = ‚Ñô(X1,...,Xn) will also be called the joint distribution of X1, . . . , Xn.
In eq. (3.9), we used the shorter expression
‚Ñô{ ‚ÉóX ‚ààB} = ‚Ñô{œâ ‚ààŒ© :
‚ÉóX(œâ) ‚ààB} .
As for random variables, the following is also valid in the case of random vectors.
Proposition 3.5.2. The mapping ‚Ñô‚ÉóX is a probability measure defined on ‚Ñ¨(‚Ñùn).
Proof. The proof is completely analogous to that of Proposition 3.2.2. Therefore, we de-
cided not to present it here.
Let us evaluate ‚Ñô‚ÉóX(B) for special Borel sets B ‚äÜ‚Ñùn. If Q is a box in ‚Ñùn as in eq. (1.73),
that is, for certain real numbers ai < bi we have
Q = [a1, b1] √ó ‚ãÖ‚ãÖ‚ãÖ√ó [an, bn] ,
then it follows that
‚Ñô‚ÉóX(Q) = ‚Ñô{ ‚ÉóX ‚ààQ} = ‚Ñô{œâ ‚ààŒ© : a1 ‚â§X1(œâ) ‚â§b1, . . . , an ‚â§Xn(œâ) ‚â§bn} .
The later expression may also be written as
‚Ñô{a1 ‚â§X1 ‚â§b1, . . . , an ‚â§Xn ‚â§bn} .
4 For random vectors
‚ÉóX and B ‚àà‚Ñ¨(‚Ñùn), it follows that
‚ÉóX‚àí1(B) ‚ààùíú. This can be proved by similar
methods as we used in the proof of Proposition 3.1.6. Thus, if B ‚àà‚Ñ¨(‚Ñùn), then eqs. (3.8) and (3.9) are well
defined.

148
‡±™
3 Random variables and their distribution
Hence, for each box Q = [a1, b1] √ó ‚ãÖ‚ãÖ‚ãÖ√ó [an, bn], we obtain
‚Ñô‚ÉóX(Q) = ‚Ñô{a1 ‚â§X1 ‚â§b1, . . . , an ‚â§Xn ‚â§bn} .
Thus the quantity ‚Ñô‚ÉóX(Q) is the probability of the occurrence of the following event: X1
attains a value in [a1, b1], and at the same time X2 attains a value in [a2, b2], and so on up
to Xn attains a value in [an, bn].
Example 3.5.3. Roll a fair die three times. Let X1, X2, and X3 be the observed values in
the first, second, and third roll. If Q = [1, 2] √ó [0, 1] √ó [3, 4], then
‚Ñô‚ÉóX(Q) = ‚Ñô{X1 ‚àà{1, 2}, X2 = 1, X3 ‚àà{3, 4}} = 1
54 .
Remark 3.5.4. The previous considerations can easily be generalized to sets B ‚äÜ‚Ñùn of
the form B = B1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Bn with Bj ‚àà‚Ñ¨(‚Ñù). Then
‚Ñô‚ÉóX(B) = ‚Ñô{X1 ‚ààB1, . . . , Xn ‚ààBn}.
(3.10)
Next we introduce the notion of marginal distributions of a random vector.
Definition 3.5.5. Let
‚ÉóX = (X1, . . . , Xn) be a random vector. The n probability measures ‚ÑôX1 to ‚ÑôXn are
called the marginal distributions of ‚ÉóX.
Observe that each marginal distribution ‚ÑôXj is a probability measure on ‚Ñ¨(‚Ñù), while the
joint distribution ‚Ñô(X1,...,Xn) is a probability measure defined on ‚Ñ¨(‚Ñùn).
In this context, the following important question arises: does the joint distribution
determine the marginal distributions and/or can the joint distribution be derived from
the marginal ones?
The next proposition gives the first answer.
Proposition 3.5.6. Let ‚ÉóX = (X1, . . . , Xn) be a random vector. If 1 ‚â§j ‚â§n and B ‚àà‚Ñ¨(‚Ñù), then
‚ÑôXj(B) = ‚Ñô(X1,...,Xn)(‚Ñù√ó ‚ãÖ‚ãÖ‚ãÖ√ó B
‚èü‚èü‚èü‚èü‚èü‚èü‚èü
j
√ó ‚ãÖ‚ãÖ‚ãÖ√ó ‚Ñù) .
In particular, the joint distribution determines the marginal ones.
Proof. The proof is a direct consequence of formula (3.10). Let us apply it to Bi = ‚Ñùif
i
Ã∏= j and Bj = B. Then, as asserted,
‚Ñô(X1,...,Xn)(‚Ñù√ó ‚ãÖ‚ãÖ‚ãÖ√ó B
‚èü‚èü‚èü‚èü‚èü‚èü‚èü
j
√ó ‚ãÖ‚ãÖ‚ãÖ√ó ‚Ñù)
= ‚Ñô{X1 ‚àà‚Ñù, . . . , Xj ‚ààB, . . . , Xn ‚àà‚Ñù} = ‚Ñô{Xj ‚ààB} = ‚ÑôXj(B) .
The question whether or not the marginal distributions determine the joint distri-
bution is postponed for a moment. It will be investigated in Example 3.5.8 and, more

3.5 Joint and marginal distributions
‡±™
149
thoroughly, in Section 3.6. Before, let us derive some concrete formulas to evaluate the
marginal distributions. Here we consider the two cases of discrete and continuous ran-
dom variables separately.
3.5.1 Marginal distributions: discrete case
To make the results in this subsection easier to understand, we only consider the case of
two-dimensional vectors. That is, we investigate two random variables and show how
their distributions may be derived from their joint one. We indicate later on how this
approach extends to more than two random variables.
In order to avoid confusing notations with many indices, given a two-dimensional
random vector, we denote its coordinate mappings by X and Y and not by X1 and X2.
This should not lead to mix-ups. Thus, we investigate the random vector (X, Y) with joint
distribution ‚Ñô(X,Y) and marginal distributions ‚ÑôX and ‚ÑôY. This random vector maps Œ©
into ‚Ñù2 and acts as follows:
(X, Y)(œâ) = (X(œâ), Y(œâ)) ,
œâ ‚ààŒ© .
Suppose now that X and Y are discrete. Then, there are finite or countably infinite sets
D = {x1, x2, . . . } and E = {y1, y2, . . . } such that X : Œ© ‚ÜíD as well as Y : Œ© ‚ÜíE. Con-
sequently, the vector (X, Y) maps Œ© into the (at most countably infinite) set D √ó E ‚äÇ‚Ñù2.
Observe that
D √ó E = {(xi, yj) : i, j = 1, 2, . . . } ,
hence ‚Ñô(X,Y) is discrete as well and uniquely described by the numbers
pij := ‚Ñô(X,Y)({(xi, yj)}) = ‚Ñô{X = xi, Y = yj} ,
i, j = 1, 2, . . .
(3.11)
More precisely, given B ‚äÜ‚Ñù2, we have
‚Ñô(X,Y)(B) = ‚Ñô{(X, Y) ‚ààB} =
‚àë
{(i,j):(xi,yj)‚ààB}
pij .
We turn now to the description of the marginal distributions ‚ÑôX and ‚ÑôY. These are
uniquely determined by the numbers
qi := ‚ÑôX({xi}) = ‚Ñô{X = xi}
and
rj := ‚ÑôY({yj}) = ‚Ñô{Y = yj} .
(3.12)
In other words, if B, C ‚äÜ‚Ñù, then it follows that
‚ÑôX(B) = ‚Ñô{X ‚ààB} =
‚àë
{i:xi‚ààB}
qi
and
‚ÑôY(C) = ‚Ñô{Y ‚ààC} =
‚àë
{j:yj‚ààC}
rj .

150
‡±™
3 Random variables and their distribution
The next proposition is nothing else than a reformulation of Proposition 3.5.6 in the
case of discrete random variables.
Proposition 3.5.7. Let the probabilities pij, qi, and rj be defined by eqs. (3.11) and (3.12),
respectively. Then the qis and rjs may be evaluated by the following equations:
qi =
‚àû
‚àë
j=1
pij
for i = 1, 2, . . .
and
rj =
‚àû
‚àë
i=1
pij
for j = 1, 2, . . .
Proof. As already mentioned, Proposition 3.5.7 is a direct consequence of Proposi-
tion 3.5.6. But for better understanding, we prefer to give a direct proof.
By virtue of the œÉ-additivity of ‚Ñô, it follows that
qi = ‚Ñô{X = xi} = ‚Ñô{X = xi, Y ‚ààE} = ‚Ñô{X = xi, Y ‚àà
‚àû
‚ãÉ
j=1
{yj}}
=
‚àû
‚àë
j=1
‚Ñô{X = xi, Y ‚àà{yj}} =
‚àû
‚àë
j=1
‚Ñô{X = xi, Y = yj} =
‚àû
‚àë
j=1
pij .
This proves the first part. The proof for the rjs follows exactly along the same line. Here,
one uses
rj = ‚Ñô{Y = yj} = ‚Ñô{X ‚ààD, Y = yj} =
‚àû
‚àë
i=1
‚Ñô{X = xi, Y = yj} =
‚àû
‚àë
i=1
pij .
This completes the proof.
The equations in Proposition 3.5.7 may be represented in table form as follows:
Y\X
x1
x2
x3
. . .
y1
p11
p21
p31
. . .
r1
y2
p12
p22
p32
. . .
r2
y3
p13
p23
p33
. . .
r3
...
...
...
...
...
...
q1
q2
q3
. . .
1
The entries in the above matrix are the corresponding probabilities. For example, the
entry p32 is put into the row marked by x3 and into the column where one finds y2 on
the left-hand side. This tells us p32 is the probability that X attains the value x3 and, at
the same time, Y equals y2. On the right and in the lower margins,5 one finds the cor-
responding sums of the columns and of the rows, respectively. These numbers describe
the marginal distributions (that of X at the bottom and that of Y in the right margin).
5 This explains the name ‚Äúmarginal‚Äù for the distribution of the coordinate mappings.

3.5 Joint and marginal distributions
‡±™
151
Finally, the number ‚Äú1‚Äù at the right lower corner says that both the right column and
bottom row have to add up to ‚Äú1.‚Äù
Example 3.5.8. There are four balls in an urn, two labeled with ‚Äú0‚Äù and another two
labeled with ‚Äú1.‚Äù Choose two balls without replacing the first. Let X be the value of the
first ball and Y that of the second. Direct calculations (use the law of multiplication) lead
to
‚Ñô{X = 0, Y = 0} = 1
6 ,
‚Ñô{X = 0, Y = 1} = 1
3,
‚Ñô{X = 1, Y = 0} = 1
3 ,
‚Ñô{X = 1, Y = 1} = 1
6.
In tabular form, this result reads as follows:
Y\X
0
1
0
1
6
1
3
1
2
1
1
3
1
6
1
2
1
2
1
2
1
Now suppose that we replace the first ball. This time we denote the values of the first
and second ball by X‚Ä≤ and Y ‚Ä≤, respectively. The corresponding table may now be written
as follows:
Y ‚Ä≤\X‚Ä≤
0
1
0
1
4
1
4
1
2
1
1
4
1
4
1
2
1
2
1
2
1
Let us look at Example 3.5.8 more thoroughly. In both cases (nonreplacing and re-
placing), the marginal distributions coincide, that is, ‚ÑôX = ‚ÑôX‚Ä≤ and ‚ÑôY = ‚ÑôY ‚Ä≤. But, on
the other hand, the joint distributions are different, that is, we have ‚Ñô(X,Y)
Ã∏= ‚Ñô(X‚Ä≤,Y ‚Ä≤).
Conclusion. The marginal distributions do not, in general, determine the joint distribu-
tion. Recall that Proposition 3.5.6 asserts the converse implication: The marginal distri-
butions can be derived from the joint distribution.
Example 3.5.9. Roll a fair die twice. Let X be the minimum value of both rolls and Y the
maximum. Then, if k, l = 1, . . . , 6, it is easy to see that
‚Ñô{X = k, Y = l} =
{
{
{
{
{
{
{
{
{
0
if k > l,
1
36
if k = l,
1
18
if k < l.

152
‡±™
3 Random variables and their distribution
Hence, the joint distribution in table form looks as follows:
Y\X
1
2
3
4
5
6
1
1
36
0
0
0
0
0
1
36
2
1
18
1
36
0
0
0
0
3
36
3
1
18
1
18
1
36
0
0
0
5
36
4
1
18
1
18
1
18
1
36
0
0
7
36
5
1
18
1
18
1
18
1
18
1
36
0
9
36
6
1
18
1
18
1
18
1
18
1
18
1
36
11
36
11
36
9
36
7
36
5
36
3
36
1
36
1
If, for example, B
=
{(4, 5), (5, 4), (6, 5), (5, 6)}, then the values in the table imply
‚Ñô(X,Y)(B) = 1/9. In the same way, one gets ‚Ñô{2 ‚â§X ‚â§4} = (9 + 7 + 5)/36 = 7/12.
To finish, we shortly go into the case of more than two discrete random variables.
Thus, let X1, . . . , Xn be random variables with Xj : Œ© ‚ÜíDj, where the sets Dj are either
finite or countably infinite. The set D defined by
D = D1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Dn = {(x1, . . . , xn) , xj ‚ààDj}
is at most countably infinite and ‚ÉóX : Œ© ‚ÜíD. Consequently, ‚Ñô‚ÉóX is uniquely described by
the probabilities
px1,...,xn = ‚Ñô{X1 = x1, . . . , Xn = xn} ,
xj ‚ààDj .
Proposition 3.5.10. For 1 ‚â§j ‚â§n and x ‚ààDj,
‚Ñô{Xj = x} = ‚àë
x1‚ààD1
‚ãÖ‚ãÖ‚ãÖ
‚àë
xj‚àí1‚ààDj‚àí1
‚àë
xj+1‚ààDj+1
‚ãÖ‚ãÖ‚ãÖ‚àë
xn‚ààDn
px1,...,xj‚àí1,x,xj+1,...,xn .
Proof. The proof is as that of Proposition 3.5.7. Therefore, we omit it.
Next, we want to state an important example that shows how Proposition 3.5.10
applies. To do so we need the following definition.
Definition 3.5.11. Let n and m be integers with m ‚â•2 and let p1, . . . , pm be certain success probabilities
satisfying pj ‚â•0 and p1+‚ãÖ‚ãÖ‚ãÖ+pm = 1. An m-dimensional random vector ‚ÉóX = (X1, . . . , Xm) has multinomial
distribution with parameters n and p1, . . . , pm if, whenever k1 + ‚ãÖ‚ãÖ‚ãÖ+ km = n,
‚Ñô{X1 = k1, . . . , Xm = km} = (
n
k1, . . . , km
) pk1
1 ‚ãÖ‚ãÖ‚ãÖpkm
m .
Equivalently, a random vector ‚ÉóX has multinomial distribution if and only if its probabil-
ity distribution ‚Ñô‚ÉóX is a multinomial distribution as introduced in Definition 1.4.16.

3.5 Joint and marginal distributions
‡±™
153
Remark 3.5.12. The m-dimensional random vector
‚ÉóX in Example 3.4.4 is multinomial
distributed with parameters n and pj = 1/m. That means
‚Ñô{X1 = k1, . . . , Xm = km} = (
n
k1, . . . , km
)( 1
m)
n
,
k1 + ‚ãÖ‚ãÖ‚ãÖ+ km = n .
Example 3.5.13. Let
‚ÉóX : Œ© ‚Üí‚Ñùn be a multinomial random vector with parameters n
and p1, . . . , pm. What are the marginal distributions of ‚ÉóX?
Answer: To simplify the calculations, we only determine the probability distribu-
tion of Xm. The other cases follow in the same way. First note that in the notation of
Proposition 3.5.10,
pk1,...,km = {(
n
k1,...,km)pk1
1 ‚ãÖ‚ãÖ‚ãÖpkm
m
if k1 + ‚ãÖ‚ãÖ‚ãÖ+ km = n,
0
if k1 + ‚ãÖ‚ãÖ‚ãÖ+ km
Ã∏= n.
Consequently, Proposition 3.5.10 leads to
‚Ñô{Xm = k} =
n
‚àë
k1=0
‚ãÖ‚ãÖ‚ãÖ
n
‚àë
km‚àí1=0
pk1,...,km‚àí1,k
=
‚àë
k1+‚ãÖ‚ãÖ‚ãÖ+km‚àí1=n‚àík
n!
k1! ‚ãÖ‚ãÖ‚ãÖkm‚àí1! k! pk1
1 ‚ãÖ‚ãÖ‚ãÖpkm‚àí1
m‚àí1pk
m
=
n!
k! (n ‚àík)! pk
m
‚àë
k1+‚ãÖ‚ãÖ‚ãÖ+km‚àí1=n‚àík
(n ‚àík)!
k1! ‚ãÖ‚ãÖ‚ãÖkm‚àí1! pk1
1 ‚ãÖ‚ãÖ‚ãÖpkm‚àí1
m‚àí1
= (n
k)pk
m
‚àë
k1+‚ãÖ‚ãÖ‚ãÖ+km‚àí1=n‚àík
(
n ‚àík
k1, . . . , km‚àí1
) pk1
1 ‚ãÖ‚ãÖ‚ãÖpkm‚àí1
m‚àí1
= (n
k)pk
m (p1 + ‚ãÖ‚ãÖ‚ãÖ+ pm‚àí1)n‚àík = (n
k)pk
m (1 ‚àípm)n‚àík .
Hereby, in the last step, we used the multinomial theorem (Proposition A.3.20) with m‚àí1
summands, with power n ‚àík and entries p1, . . . , pm‚àí1. Thus Xm is binomial distributed
with parameters n and pm. In the same way, one gets that each Xj is Bn,pj-distributed.
Remark 3.5.14. The previous result can also be seen more directly without using Propo-
sition 3.5.10. Assume we place n particles into m boxes, where pj is the probability to put
a single particle into box j. Fix some j ‚â§m and let success occur if a particle is placed
into box j. Then Xj equals the number of successes, hence it is Bn,pj-distributed. Note that
failure occurs if the particle is not placed into box j, and the probability for this is given
by 1 ‚àípj = ‚àën
i=1
i Ã∏=j pi.
Summary: Let (xi)i‚â•1 and (yj)j‚â•1 be the values of the (discrete) random variables X and Y, respectively. If the
joint distribution of (X, Y) is given by
pij := ‚Ñô(X,Y)({(xi, yj)}) = ‚Ñô{X = xi, Y = yj} ,
i, j = 1, 2, . . . ,

154
‡±™
3 Random variables and their distribution
then the marginal distributions are described by
qi = ‚ÑôX({xi}) = ‚Ñô{X = xi} =
‚àû
‚àë
j=1
pij
and
rj = ‚ÑôY({yj}) = ‚Ñô{Y = yj} =
‚àû
‚àë
i=1
pij .
3.5.2 Marginal distributions: continuous case
Let us turn now to the continuous case. Analogous to Definition 3.2.7, a random vector is
said to be continuous whenever it possesses a density.6 More precisely, we suppose that
a random vector shares the following property.
Definition 3.5.15. A random vector ‚ÉóX = (X1, . . . , Xn) is said to be continuous if there is a function p :
‚Ñùn ‚Üí‚Ñùsuch that, for all numbers aj < bj, 1 ‚â§j ‚â§n,
‚Ñô{a1 ‚â§X1 ‚â§b1, . . . , an ‚â§Xn ‚â§bn} =
b1
‚à´
a1
‚ãÖ‚ãÖ‚ãÖ
bn
‚à´
an
p(x1, . . . , xn) dxn ‚ãÖ‚ãÖ‚ãÖdx1 .
An equivalent formulation is: for all real numbers t1, . . . , tn, one has
‚Ñô{X1 ‚â§t1, . . . , Xn ‚â§tn} =
t1
‚à´
‚àí‚àû
‚ãÖ‚ãÖ‚ãÖ
tn
‚à´
‚àí‚àû
p(x1, . . . , xn) dxn ‚ãÖ‚ãÖ‚ãÖdx1 .
The function p is called the density function of ‚ÉóX or the joint density of X1, . . . , Xn.
Remark 3.5.16. Observe that a random vector
‚ÉóX is continuous if and only if its prob-
ability distribution ‚Ñô‚ÉóX is such, that is, the joint distribution of X1, . . . Xn is a continuous
probability measure on ‚Ñ¨(‚Ñùn) in the sense of Definition 1.8.5. Moreover, its density func-
tion coincides with the density of ‚Ñô‚ÉóX.
In the case of continuous random variables, the marginal distributions are evalu-
ated by the following rule.
Proposition 3.5.17. If a random vector ‚ÉóX = (X1, . . . , Xn) has density p : ‚Ñùn ‚Üí‚Ñù, then for
each j ‚â§n the random variable Xj is continuous with density
pj(xj) =
‚àû
‚à´
‚àí‚àû
‚ãÖ‚ãÖ‚ãÖ
‚àû
‚à´
‚àí‚àû
‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
n‚àí1 integrals
p( . . . , xj‚àí1, xj, xj+1, . . .)dxn ‚ãÖ‚ãÖ‚ãÖdxj+1 dxj‚àí1 ‚ãÖ‚ãÖ‚ãÖdx1 .
(3.13)
6 The following is true: for continuous random variables, the generated vector possesses a density. The
proof is far outside the scope of this book. Furthermore, we do not need this assertion because we assume
‚ÉóX to be continuous, not the Xjs.

3.5 Joint and marginal distributions
‡±™
155
Proof. Fix an integer j ‚â§n. An application of Proposition 3.5.6 implies
‚ÑôXj([a, b]) = ‚Ñô‚ÉóX(‚Ñù√ó ‚ãÖ‚ãÖ‚ãÖ√ó [a, b]
‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
j
√ó ‚ãÖ‚ãÖ‚ãÖ√ó ‚Ñù)
=
‚àû
‚à´
‚àí‚àû
‚ãÖ‚ãÖ‚ãÖ
b
‚à´
a
‚èü‚èü‚èü‚èü‚èü‚èü‚èü
j
‚ãÖ‚ãÖ‚ãÖ
‚àû
‚à´
‚àí‚àû
p(x1, . . . , xn) dxn ‚ãÖ‚ãÖ‚ãÖdx1
=
b
‚à´
a
[
‚àû
‚à´
‚àí‚àû
‚ãÖ‚ãÖ‚ãÖ
‚àû
‚à´
‚àí‚àû
p(. . . , xj‚àí1, xj, xj+1, . . .)dxn ‚ãÖ‚ãÖ‚ãÖdxj+1 dxj‚àí1 ‚ãÖ‚ãÖ‚ãÖdx1] dxj
=
b
‚à´
a
pj(xj) dxj
with pj defined by eq. (3.13). The interchange of the integrals was justified by Fubini‚Äôs
theorem (Proposition A.5.5); note that p is a density, hence it is nonnegative. Since the
preceding equation holds for all real numbers a < b, the function pj has to be the density
of ‚ÑôXj. This completes the proof.
Inthecasen = 2,formula(3.13)assertsthefollowing.Letp : ‚Ñù2 ‚Üí‚Ñùbethejointdensityofthe2-dimensional
vector (X1, X2). Then the functions p1 and p2 defined by
p1(x1) =
‚àû
‚à´
‚àí‚àû
p(x1, x2) dx2
and
p2(x2) =
‚àû
‚à´
‚àí‚àû
p(x1, x2) dx1
are densities of X1 and X2, respectively.
Remark 3.5.18. Another way to formulate Proposition 3.5.17 is as follows: if the function
p : ‚Ñùn ‚Üí‚Ñùis a joint density of ‚ÉóX = (X1, . . . , Xn), then p1, . . . , pn defined in eq. (3.13) are
densities of the random variables X1, . . . , Xn, respectively.
Example 3.5.19. Choose by random a point x = (x1, x2, x3) in the unit ball of ‚Ñù3. How are
the coordinates x1, x2, and x3 distributed?
Answer: Let ‚ÉóX = (X1, X2, X3) be uniformly distributed on the unit ball
K = {(x1, x2, x3) : x2
1 + x2
2 + x2
3 ‚â§1} .
Then the joint density is given by7
7 Recall that vol3(K) = 4
3 œÄ.

156
‡±™
3 Random variables and their distribution
p(x) = {
3
4œÄ
if x ‚ààK,
0
if x ‚àâK.
An application of Proposition 3.5.17 leads to p1(x1) = 0 whenever |x1| > 1 and, if |x1| ‚â§1,
then it follows that
p1(x1) = 3
4œÄ
‚à¨
x2
2+x2
3‚â§1‚àíx2
1
dx2dx3 = 3
4œÄ (1 ‚àíx2
1)œÄ = 3
4(1 ‚àíx2
1) .
Hence, X1 has the density (compare Figure 3.2)
p1(s) = {
3
4(1 ‚àís2)
if ‚àí1 ‚â§s ‚â§1,
0
otherwise.
(3.14)
Of course, by symmetry, X2 and X3 possess exactly the same distribution densities.
Figure 3.2: The density p1 defined by eq. (3.14).
Example 3.5.20. Suppose the two-dimensional random vector (X1, X2) has the density p
defined by8
p(x1, x2) := {8 x1x2
if 0 ‚â§x1 ‚â§x2 ‚â§1,
0
otherwise.
(3.15)
Then, the density p1 of X1 is given by
p1(x1) =
‚àû
‚à´
‚àí‚àû
p(x1, x2) dx2 = 8 x1
1
‚à´
x1
x2 dx2 = 4(x1 ‚àíx3
1) ,
0 ‚â§x1 ‚â§1 ,
and p1(x1) = 0 if x1 ‚àâ[0, 1].
8 Check that p is indeed a probability density.

3.6 Independence of random variables
‡±™
157
In the case of p2, the density of X2, it follows that
p2(x2) =
‚àû
‚à´
‚àí‚àû
p(x1, x2) dx1 = 8 x2
x2
‚à´
0
x1 dx1 = 4 x3
2 ,
0 ‚â§x2 ‚â§1 ,
and p2(x2) = 0 if x2 ‚àâ[0, 1]. See Figure 3.3 for the joint density of the random vector
(X1, X2) and its marginal distributions.
Figure 3.3: The two-dimensional density p defined by eq. (3.15) with marginal densities p1 and p2.
Remark 3.5.21. For p1 : ‚Ñù‚Üí‚Ñùand p2 : ‚Ñù‚Üí‚Ñùas in Example 3.5.20, define ÃÉp : ‚Ñù2 ‚Üí‚Ñù
by
ÃÉp(x1, x2) = p1(x1) ‚ãÖp2(x2) ,
(x1, x2) ‚àà‚Ñù2 .
In view of Proposition 1.8.8, the function ÃÉp is a (two-dimensional) density and, moreover,
as can be seen easily, its marginal distributions are p1 and p2 as well. But note that p
Ã∏= ÃÉp.
Thus, this is another example showing that the marginal distributions of a random vec-
tor do not determine its joint distribution.
Summary: Let ‚ÉóX = (X1, . . . , Xn) be a random (n-dimensional) vector. The probability distribution ‚Ñô‚ÉóX on ‚Ñ¨(‚Ñùn)
is said to be the joint distribution of X1, . . . , Xn while the probability measures ‚ÑôXj, 1 ‚â§j ‚â§n, denote the
marginal distributions of ‚ÉóX. The joint distribution determines the n marginal distributions. In general, the
converse implication does not hold.
3.6 Independence of random variables
The central question considered in this section is as follows: when are n given random
variables independent? Surely everybody has an intuitive idea about the independence
or dependence of random values. But how do we express this property by a mathemat-
ical formula? Let us try to approach a solution of this problem with an example.

158
‡±™
3 Random variables and their distribution
Example 3.6.1. Roll a fair die twice and define the two random variables X1 and X2 as the
results of the first and second roll, respectively. These random variables are intuitively
independent of each other. But what property of these random variables does this ex-
press? Take two subsets B1, B2 ‚äÜ{1, . . . , 6} and look at their preimages A1 = X‚àí1
1 (B1) and
A2 = X‚àí1
2 (B2). Then A1 occurs if the first result belongs to B1 while the same is true for
A2 whenever the second result belongs to B2. For example, A1 might indicate that the
first result is an even number while A2 could occur if the second result equals ‚Äú4.‚Äù The
basic observation is that no matter how B1 and B2 were chosen, the occurrence of their
preimages A1 and A2 only depends on the first or second roll, respectively. Therefore,
they should be independent (as events) in the sense of Definition 2.2.2, that is, the fol-
lowing equation should hold:
‚Ñô{X1 ‚ààB1, X2 ‚ààB2} = ‚Ñô(X‚àí1
1 (B1) ‚à©X‚àí1
2 (B2)) = ‚Ñô(A1 ‚à©A2)
= ‚Ñô(A1) ‚ãÖ‚Ñô(A2) = ‚Ñô(X‚àí1
1 (B1)) ‚ãÖ‚Ñô(X‚àí1
2 (B2)) = ‚Ñô{X1 ‚ààB1} ‚ãÖ‚Ñô{X2 ‚ààB2} .
This observation leads us to the following definition of independence.
Definition 3.6.2. Let X1, . . . , Xn be n random variables mapping Œ© into ‚Ñù. These variables are said to be
(stochastically) independent if, for all Borel sets Bj ‚äÜ‚Ñù,
‚Ñô{X1 ‚ààB1, . . . , Xn ‚ààBn} = ‚Ñô{X1 ‚ààB1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn ‚ààBn} .
(3.16)
Remark 3.6.3. By virtue of Remark 3.5.4, eq. (3.16) may also be written as
‚Ñô(X1,...,Xn)(B1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Bn) = ‚ÑôX1(B1) ‚ãÖ‚ãÖ‚ãÖ‚ÑôXn(Bn) ,
Bj ‚àà‚Ñ¨(‚Ñù) .
Before proceeding further, we shortly recall Corollary 1.9.9.
Corollary 3.6.4. Given n probability measures ‚Ñô1, . . . , ‚Ñôn defined on ‚Ñ¨(‚Ñù), there exists
a unique probability measure ‚Ñôon ‚Ñ¨(‚Ñùn), the product measure, which is denoted by
‚Ñô= ‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn, such that for all Borel sets Bj ‚äÜ‚Ñù,
‚Ñô(B1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Bn) = ‚Ñô1(B1) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn(Bn) .
(3.17)
Now, we are prepared to state the characterization of independent random vari-
ables by properties of their distributions.
Proposition 3.6.5. The random variables X1, . . . , Xn are independent if and only if their
joint distribution coincides with the product probability of the marginal distributions.
That is, if and only if
‚Ñô(X1,...,Xn) = ‚ÑôX1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚ÑôXn .
Proof. In view of Corollary 3.6.4, the product probability ‚Ñôof ‚ÑôX1, . . . , ‚ÑôXn is the unique
probability measure on ‚Ñ¨(‚Ñùn) satisfying

3.6 Independence of random variables
‡±™
159
‚Ñô(B1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Bn) = ‚ÑôX1(B1) ‚ãÖ‚ãÖ‚ãÖ‚ÑôXn(Bn) ,
Bj ‚àà‚Ñ¨(‚Ñù) .
On the other hand, by Remark 3.6.3, the Xjs are independent if and only if
‚Ñô(X1,...,Xn)(B1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó Bn) = ‚ÑôX1(B1) ‚ãÖ‚ãÖ‚ãÖ‚ÑôXn(Bn) ,
Bj ‚àà‚Ñ¨(‚Ñù) .
(3.18)
Consequently, eq. (3.18) holds for all Borel sets Bj if and only if ‚Ñô(X1,...,Xn) is the product
probability ‚ÑôX1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚ÑôXn. This completes the proof.
Corollary 3.6.6. If X1, . . . , Xn are independent, the joint distribution ‚Ñô(X1,...,Xn) is uniquely
determined by its marginal distributions ‚ÑôX1, . . . , ‚ÑôXn.
Proof. Proposition 3.6.5 asserts that ‚Ñô(X1,...,Xn) = ‚ÑôX1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚ÑôXn. Hence, the joint distribu-
tion is uniquely described by the marginal ones.
Another application of Proposition 3.6.5 deals with the existence of independent
random variables possessing given distributions.
Proposition 3.6.7. Let ‚Ñô1, . . . , ‚Ñôn be given probability measures on the real line, discrete
or continuous. Then there is a probability space (Œ©, ùíú, ‚Ñô) and there are independent ran-
dom variables Xj : Œ© ‚Üí‚Ñùsuch that ‚ÑôXj = ‚Ñôj, 1 ‚â§j ‚â§n. In other words, for all Borel sets
B1, . . . , Bn we have
‚Ñô{X1 ‚ààB1, . . . , Xn ‚ààBn} = ‚Ñô{X1 ‚ààB1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn ‚ààBn} = ‚Ñô1(B1) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn(Bn) .
Proof. Choose Œ© = ‚Ñùn and endow it with the probability distribution (product measure)
‚Ñô= ‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn. Next we define random variables Xj : ‚Ñùn ‚Üí‚Ñùby
Xj(œâ) = Xj(œâ1, . . . , œân) = œâj ,
1 ‚â§j ‚â§n .
Thus,
‚ÉóX = (X1, . . . , Xn) is the identity, hence ‚Ñô‚ÉóX = ‚Ñô. Consequently, Proposition 1.9.10
implies ‚ÑôXj = ‚Ñôj, 1 ‚â§j ‚â§n, and, moreover, by the choice of ‚Ñôit follows that
‚Ñô(X1, ... ,Xn) = ‚Ñô‚ÉóX = ‚Ñô= ‚Ñô1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñôn = ‚ÑôX1 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚ÑôXn .
So, in view of Proposition 3.6.5 the random variables X1 up to Xn are independent, as
asserted.
Example 3.6.8. Suppose we want to construct n independent PoisŒª-distributed random
variables X1 up to Xn. To do so, choose Œ© = ‚Ñïn
0 endowed with the product measure ‚Ñôof
n different PoisŒª measures. That is,
‚Ñô({ ‚Éók}) = Œªk1+‚ãÖ‚ãÖ‚ãÖ+kn
k1! ‚ãÖ‚ãÖ‚ãÖkn!e‚àínŒª ,
‚Éók = (k1, . . . , kn) ‚àà‚Ñïn
0 .
Then the random variables Xj : Œ© ‚Üí‚Ñï0 with

160
‡±™
3 Random variables and their distribution
Xj( ‚Éók) = kj ,
‚Éók = (k1, . . . , kn) ‚àà‚Ñï0 ,
are the independent n random variables distributed according to PoisŒª.
The next proposition clarifies the relation between the properties ‚Äúindependence of
events‚Äù and ‚Äúindependence of random variables.‚Äù At a first glance, the assertion looks
trivial or self-evident, but it is not at all. The reason is that the definition of independence
for more than two events, as given in Definition 2.2.12, is more complicated than in the
case of two events.
Proposition 3.6.9. The random variables X1, . . . , Xn are independent if and only if for all
Borel sets B1, . . . , Bn in ‚Ñùthe events
X‚àí1
1 (B1), . . . , X‚àí1
n (Bn)
are stochastically independent in (Œ©, ùíú, ‚Ñô).
Proof. When are X‚àí1
1 (B1), . . . , X‚àí1
n (Bn) independent? According to Definition 2.2.12, this
holds if for all subsets I ‚äÜ{1, . . . , n},
‚Ñô(‚ãÇ
i‚ààI
X‚àí1
i (Bi)) = ‚àè
i‚ààI
‚Ñô(X‚àí1
i (Bi)) .
(3.19)
On the other hand, by Definition 3.6.2, X1, . . . , Xn are independent if
‚Ñô(
n
‚ãÇ
i=1
X‚àí1
i (Bi)) = ‚Ñô{X1 ‚ààB1, . . . , Xn ‚ààBn}
=
n
‚àè
i=1
‚Ñô{Xi ‚ààBi} =
n
‚àè
i=1
‚Ñô(X‚àí1
i (Bi)) .
(3.20)
Of course, eq. (3.19) implies eq. (3.20); use eq. (3.19) with I = {1, . . . , n}. But it is far from
clear why, conversely, eq. (3.20) should imply eq. (3.19). As we saw in Example 2.2.10, for
fixed sets Bj this is even false. The key observation is that eq. (3.19) has to be valid for all
Borel sets Bj. This allows us to choose the Borel sets in an appropriate way.
Thus let us assume the validity of eq. (3.20) for all Borel sets in ‚Ñù. Given Bj ‚àà‚Ñ¨(‚Ñù)
and a subset I of {1, . . . , n}, we introduce ‚Äúnew‚Äù B‚Ä≤
1, . . . , B‚Ä≤
n as follows: B‚Ä≤
i = Bi if i ‚ààI and
B‚Ä≤
i = ‚Ñùif i ‚àâI. This choice of the B‚Ä≤
j implies X‚àí1
i (B‚Ä≤
i) = Œ© whenever i ‚àâI. An application
of (3.20) to B‚Ä≤
1, . . . , B‚Ä≤
n leads to (recall X‚àí1
i (B‚Ä≤
i) = Œ© if i ‚àâI)
‚Ñô(‚ãÇ
i‚ààI
X‚àí1
i (Bi)) = ‚Ñô(
n
‚ãÇ
i=1
X‚àí1
i (B‚Ä≤
i)) =
n
‚àè
i=1
‚Ñô(X‚àí1
i (B‚Ä≤
i)) = ‚àè
i‚ààI
‚Ñô(X‚àí1
i (Bi)) .
This proves eq. (3.19) for any subset I of {1, . . . , n}. So, X‚àí1
1 (B1), . . . , X‚àí1
n (Bn) are indepen-
dent as asserted.

3.6 Independence of random variables
‡±™
161
Remark 3.6.10. To verify the independence of X1, . . . , Xn, it is not necessary to check
eq. (3.16) for all Borel sets Bj. It suffices if this is valid for real intervals [aj, bj]. In other
words, X1, . . . , Xn are independent if and only if, for all aj < bj,
‚Ñô{a1 ‚â§X1 ‚â§b1, . . . , an ‚â§Xn ‚â§bn} = ‚Ñô{a1 ‚â§X1 ‚â§b1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{an ‚â§Xn ‚â§bn}.
Furthermore, it also suffices to choose the Borel sets as intervals (‚àí‚àû, tj] for tj ‚àà‚Ñù, i. e.,
X1, . . . , Xn are independent if and only if, for all tj ‚àà‚Ñù,
‚Ñô{X1 ‚â§t1, . . . , Xn ‚â§tn} = ‚Ñô{X1 ‚â§t1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn ‚â§tn} .
3.6.1 Independence of discrete random variables
As in Section 3.5.1, we restrict ourselves to the case of two random variables. The exten-
sion to more than two variables is straightforward and will be shortly considered at the
end of this section. We use the same notation as in Section 3.5.1. That is, the two random
variables are denoted by X and Y, and they map Œ© into D = {x1, x2, . . .} and E = {y1, y2, . . .},
respectively. The joint distribution of (X, Y), as well as the marginal distributions, that
is, the distributions of X and Y, are described as in eqs. (3.11) and (3.12) by
pij = ‚Ñô{X = xi, Y = yj} ,
qi = ‚Ñô{X = xi},
and
rj = ‚Ñô{Y = yj} .
With these notations the following result is valid.
Proposition 3.6.11. For the independence of two random variables X and Y, it is necessary
and sufficient that
pij = qi ‚ãÖrj ,
1 ‚â§i, j < ‚àû.
Proof. The assertion is an immediate consequence of Propositions 1.9.11 and 3.6.5. But,
because of the importance of the result, we give an alternative proof avoiding the direct
use of product probabilities; only the techniques are similar.
Let us first show that the condition is necessary. Therefore, choose indices i and j,
and put B1 := {xi} and B2 := {yj}. Then {X ‚ààB1} occurs if and only if X = xi, and, in the
same way, the occurrence of {Y ‚ààB2} is equivalent to Y = yj. Since X and Y are assumed
to be independent, as claimed,
pij = ‚Ñô{X = xi, Y = yj} = ‚Ñô{X ‚ààB1, Y ‚ààB2} = ‚Ñô{X ‚ààB1} ‚ãÖ‚Ñô{Y ‚ààB2}
= ‚Ñô{X = xi} ‚ãÖ‚Ñô{Y = yj} = qi ‚ãÖrj .
To prove the converse implication, assume we have pij = qi ‚ãÖrj for all pairs (i, j) of
integers. Let B1 and B2 be two arbitrary subsets of ‚Ñù. Then

162
‡±™
3 Random variables and their distribution
‚Ñô{X ‚ààB1, Y ‚ààB2} = ‚Ñô(X,Y)(B1 √ó B2) =
‚àë
{(i,j):(xi,yj)‚ààB1√óB2}
pij
=
‚àë
{(i,j):xi‚ààB1, yj‚ààB2}
qi ‚ãÖrj =
‚àë
{i:xi‚ààB1}
‚àë
{j:yj‚ààB2}
qi ‚ãÖrj
= ( ‚àë
{i:xi‚ààB1}
qi) ‚ãÖ( ‚àë
{j:yj‚ààB2}
rj) = ‚ÑôX(B1) ‚ãÖ‚ÑôY(B2)
= ‚Ñô{X ‚ààB1} ‚ãÖ‚Ñô{Y ‚ààB2} .
Since B1 and B2 were arbitrary, the random variables X and Y are independent. This
completes the proof.
Remark 3.6.12. The previous proposition implies again that for (discrete) independent
random variables the joint distribution is determined by the marginal ones. Indeed, in
order to know the pijs, it suffices to know the qis and rjs.
Let us represent the assertion of Proposition 3.6.11 graphically. It tells us that the
random variables X and Y are independent if and only if the table describing their joint
distribution may be represented as follows:
Y\X
x1
x2
x3
. . .
y1
q1r1
q2r1
q3r1
. . .
r1
y2
q1r2
q2r2
q3r2
. . .
r2
y3
q1r3
q2r3
q3r3
. . .
r3
...
...
...
...
...
...
q1
q2
q3
. . .
1
Example 3.6.13. Proposition 3.6.11 lets us conclude that X and Y in Example 3.5.8 (with-
out replacing) are dependent while X‚Ä≤ and Y ‚Ä≤ (with replacement) are independent. Fur-
thermore, by the same argument, the random variables X and Y in Example 3.5.9 (min-
imum and maximum value when rolling a die twice) are dependent as well.
Example 3.6.14. Let X and Y be two independent PoisŒª-distributed random variables.
Then the joint distribution of the vector (X, Y) is determined by
‚Ñô{X = k, Y = ‚Ñì} = Œªk+l
k! ‚Ñì! e‚àí2Œª ,
(k, ‚Ñì) ‚àà‚Ñï0 √ó ‚Ñï0 .
For example, applying this for ‚Ñô(X,Y)(B) with B = {(k, ‚Ñì) : k = ‚Ñì} leads to
‚Ñô{X = Y} =
‚àû
‚àë
k=0
‚Ñô{X = k, Y = k} =
‚àû
‚àë
k=0
Œª2k
(k!)2 e‚àí2Œª .
Example 3.6.15. Suppose X and Y are two independent geometrically distributed ran-
dom variables, with parameters p and q, respectively. Evaluate ‚Ñô{X ‚â§Y}.

3.6 Independence of random variables
‡±™
163
Solution: By the independence of X and Y,
‚Ñô{X ‚â§Y} =
‚àû
‚àë
k=1
‚Ñô{X = k, Y ‚â•k} =
‚àû
‚àë
k=1
‚Ñô{X = k} ‚ãÖ‚Ñô{Y ‚â•k}
=
‚àû
‚àë
k=1
p(1 ‚àíp)k‚àí1
‚àû
‚àë
‚Ñì=k
q(1 ‚àíq)‚Ñì‚àí1
= p q (
‚àû
‚àë
k=0
(1 ‚àíp)k)(
‚àû
‚àë
‚Ñì=k+1
(1 ‚àíq)‚Ñì‚àí1)
= p q (
‚àû
‚àë
k=0
(1 ‚àíp)k)(
‚àû
‚àë
‚Ñì=k
(1 ‚àíq)‚Ñì)
= p q (
‚àû
‚àë
k=0
(1 ‚àíp)k(1 ‚àíq)k)(
‚àû
‚àë
‚Ñì=0
(1 ‚àíq)‚Ñì)
=
p
1 ‚àí(1 ‚àíp)(1 ‚àíq) =
p
p + q ‚àípq .
In Example 1.9.12, we investigated the case p = q from a different point of view. The
results obtained there let us conclude that
‚Ñô{X ‚â§Y} = ‚Ñô{X < Y} + ‚Ñô{X = Y} =
p
2 ‚àíp + 1 ‚àíp
2 ‚àíp =
1
2 ‚àíp,
which coincides with what we got above if p = q.
Example of application: Player A rolls a die and, simultaneously, player B tosses two
fair coins labeled with ‚Äú0‚Äù and ‚Äú1.‚Äù Find the probability that player A observes the num-
ber ‚Äú6‚Äù for the first time strictly before player B gets a ‚Äú1‚Äù at both coins.
Answer: Let {Y = k} be the event that player A observes his first ‚Äú6‚Äù in trial k. Sim-
ilarly, {X = k} occurs if player B has his first two ‚Äú1‚Äù in trial k. Then we ask for the
probability ‚Ñô{Y < X}. Note that X is geometrically distributed with parameter p = 1/4,
while the success probability for Y is q = 1/6. Hence, by the above calculations,
‚Ñô{Y < X} = 1 ‚àí‚Ñô{X ‚â§Y} = 1 ‚àí
1/4
1/4 + 1/6 ‚àí1/24 = 1
3 .
The next objective is to investigate in which cases two quite special random vari-
ables are independent. To this end, we need the following notation.
Definition 3.6.16. Let Œ© be a set and A ‚äÜŒ©. Then the indicator function 1A : Œ© ‚Üí‚Ñùof A is defined
by
1A(œâ) := {1
if œâ ‚ààA,
0
if œâ ‚àâA.
(3.21)

164
‡±™
3 Random variables and their distribution
Let us state some basic properties of indicator functions.
Proposition 3.6.17. Let (Œ©, ùíú, ‚Ñô) be a probability space.
(1) The indicator function of a set A ‚äÜŒ© is a random variable if and only if A ‚ààùíú.
(2) If A ‚ààùíú, then 1A is B1,p-distributed (binomial) where p = ‚Ñô(A).
(3) If A, B ‚ààùíú, then the random variables 1A and 1B are independent if and only if the
events A and B are independent.
Proof. Given t ‚àà‚Ñù, the event {œâ ‚ààŒ© : 1A(œâ) ‚â§t} is either empty, Ac, or Œ© whenever
t < 0, 0 ‚â§t < 1, or t ‚â•1, respectively. Consequently, the set {œâ ‚ààŒ© : 1A(œâ) ‚â§t} is in ùíú
for all t ‚àà‚Ñùif and only if Ac ‚ààùíú. But this happens if and only if A ‚ààùíú, which proves
the first assertion.
To prove the second claim, we first observe that 1A attains only the values ‚Äú0‚Äù and
‚Äú1.‚Äù Since
‚Ñô{1A = 1} = ‚Ñô{œâ ‚ààŒ© : 1A(œâ) = 1} = ‚Ñô(A) = p ,
it is B1,p-distributed with p = ‚Ñô(A) as claimed.
Let us turn to the last assertion. Given A, B ‚ààùíú, their joint distribution in table form
is9
1B\1A
0
1
0
‚Ñô(Ac ‚à©Bc)
‚Ñô(A ‚à©Bc)
‚Ñô(Bc)
1
‚Ñô(Ac ‚à©B)
‚Ñô(A ‚à©B)
‚Ñô(B)
‚Ñô(Ac)
‚Ñô(A)
Consequently, by Proposition 3.6.11, the random variables 1A and 1B are independent if
and only if the following equations are valid:
‚Ñô(Ac ‚à©Bc) = ‚Ñô(Ac) ‚ãÖ‚Ñô(Bc) ,
‚Ñô(Ac ‚à©B) = ‚Ñô(Ac) ‚ãÖ‚Ñô(B),
‚Ñô(A ‚à©Bc) = ‚Ñô(A) ‚ãÖ‚Ñô(Bc) ,
‚Ñô(A ‚à©B) = ‚Ñô(A) ‚ãÖ‚Ñô(B) .
Because of Proposition 2.2.7, these four equations are satisfied if and only if the events
A and B are independent. This proves the third assertion.
Finally, let us shortly discuss the independence of more than two discrete random
variables. Hereby we use the same notation as in Proposition 3.5.10, that is, the random
variables X1, . . . , Xn satisfy Xj : Œ© ‚ÜíDj, where Dj is either finite or countably infinite.
Then the following generalization of Proposition 3.6.11 is valid. Its proof is almost iden-
tical to that for two variables. Therefore, we omit it.
9 Use, for example, that 1A(œâ) = 0 and 1B(œâ) = 0 if and only if œâ ‚ààAc ‚à©Bc.

3.6 Independence of random variables
‡±™
165
Proposition 3.6.18. The random variables X1, . . . , Xn are independent if and only if for all
xj ‚ààDj,
‚Ñô{X1 = x1, . . . , Xn = xn} = ‚Ñô{X1 = x1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn = xn} .
Example 3.6.19. Let us consider the problem of tossing a biased coin n times. The sam-
ple space is Œ© = {0, 1}n, and the describing probability measure ‚Ñôis as in eq. (3.5). The
random variables Xj are defined as results of toss j. Then Xj : Œ© ‚ÜíDj, where Dj = {0, 1}.
If we choose arbitrary xj ‚ààDj, then either xj = 0 or xj = 1. Let k be the number of those
xj, which equals 1, that is, k = x1 + ‚ãÖ‚ãÖ‚ãÖ+ xn. Formula (3.5) implies
‚Ñô{X1 = x1, . . . , Xn = xn} = ‚Ñô{(x1, . . . , xn)} = pk(1 ‚àíp)n‚àík .
On the other hand, as shown in Example 3.2.16, the probability distribution of each Xj
satisfies
‚Ñô{Xj = 0} = 1 ‚àíp
and
‚Ñô{Xj = 1} = p .
Since exactly k of the xjs are ‚Äú1‚Äù and n ‚àík are ‚Äú0,‚Äù, this implies
‚Ñô{X1 = x1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn = xn} = pk(1 ‚àíp)n‚àík .
Summing up, for all xj ‚ààDj,
‚Ñô{X1 = x1, . . . , Xn = xn} = pk(1 ‚àíp)n‚àík = ‚Ñô{X1 = x1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn = xn} ,
that is, X1, . . . , Xn are independent.
Summary: Two discrete random variables X and Y with values (xi)i‚â•1 and (yj)j‚â•1, respectively, are indepen-
dent if and only if for all 1 ‚â§i, j < ‚àû,
‚Ñô{X = xi, Y = yj} = ‚Ñô{X = xi} ‚ãÖ‚Ñô{Y = yj} .
3.6.2 Independence of continuous random variables
We will consider the question of when continuous random variables are independent.
Thus, let X1, . . . , Xn be continuous random variables with distribution densities p1, . . . , pn,
that is, for 1 ‚â§j ‚â§n and real numbers a < b,
‚ÑôXj([a, b]) = ‚Ñô{a ‚â§Xj ‚â§b} =
b
‚à´
a
pj(x) dx .
With this notation, the independence of the Xjs may be characterized as follows.

166
‡±™
3 Random variables and their distribution
Proposition 3.6.20. For random variables X1, . . . , Xn with densities p1, . . . , pn, we define a
function p : ‚Ñùn ‚Üí‚Ñùby
p(x1, . . . , xn) := p1(x1) ‚ãÖ‚ãÖ‚ãÖpn(xn) ,
(x1, . . . , xn) ‚àà‚Ñùn .
(3.22)
Then the Xjs are independent if and only if p defined by eq. (3.22) is a distribution density
of the random vector ‚ÉóX = (X1, . . . , Xn).
Proof. As in the discrete case, the result follows directly from Propositions 1.9.16 and
3.6.5. Without using product probabilities, we may argue as follows.
First, we observe that p defined by eq. (3.22) is a distribution density of ‚ÉóX if and only
if for all aj < bj,
‚Ñô{a1 ‚â§X1 ‚â§b1, . . . , an ‚â§Xn ‚â§bn} =
b1
‚à´
a1
‚ãÖ‚ãÖ‚ãÖ
bn
‚à´
an
p1(x1) ‚ãÖ‚ãÖ‚ãÖpn(xn) dxn ‚ãÖ‚ãÖ‚ãÖdx1.
(3.23)
The right-hand side of eq. (3.23) coincides with
(
b1
‚à´
a1
p1(x1) dx1) ‚ãÖ‚ãÖ‚ãÖ(
bn
‚à´
an
pn(xn) dxn) = ‚Ñô{a1 ‚â§X1 ‚â§b1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{an ‚â§Xn ‚â§bn} .
From this we derive that eq. (3.23) is valid for all aj < bj if and only if
‚Ñô{a1 ‚â§X1 ‚â§b1, . . . , an ‚â§Xn ‚â§bn} = ‚Ñô{a1 ‚â§X1 ‚â§b1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{an ‚â§Xn ‚â§bn} .
By Remark 3.6.10, this is equivalent to the independence of the Xjs, completing the
proof.
Example 3.6.21. Throw a dart to a target, which is a circle of radius 1. The center of the
circle is the point (0, 0) and (x1, x2) ‚ààK denotes the point where the dart hits the target.
We assume that the point hit is uniformly distributed on K. The question is whether
or not the coordinates x1 and x2 of the point hit are dependent or independent of each
other.
Answer: Let ‚Ñôbe the uniform distribution on K, and define two random variables
X1 and X2 by X1(x1, x2) = x1 and X2(x1, x2) = x2. In this notation, the above question is
whether the random variables X1 and X2 are independent. The density p1 of X1 was found
in eq. (3.7). By symmetry, p2, the density of X2, coincides with p1, that is, we have
p1(x1) =
{
{
{
2
œÄ ‚àö1 ‚àíx2
1
if |x1| ‚â§1,
0
if |x1| > 1,
and
p2(x2) =
{
{
{
2
œÄ ‚àö1 ‚àíx2
2
if |x2| ‚â§1,
0
if |x2| > 1 .
But p1(x1) ‚ãÖp2(x2) cannot be a distribution density of ‚Ñô(X1,X2). Indeed, the vector
‚ÉóX =
(X1, X2) is uniformly distributed on K, thus its (correct) density is p with

3.6 Independence of random variables
‡±™
167
p(x1, x2) = {
1
œÄ
if x2
1 + x2
2 ‚â§1,
0
otherwise.
Thus, we conclude that X1 and X2 are dependent, hence also the coordinates x1 and x2 of
the point hit.
Example 3.6.22. We suppose now that the dart does not hit a circle but some rectangle
set R := [Œ±1, Œ≤1] √ó [Œ±2, Œ≤2]. Again we assume that the point (x1, x2) ‚ààR is uniformly dis-
tributed on R. The posed question is the same as in Example 3.6.21, namely whether x1
and x2 are independent of each other.
Answer: Define X1 and X2 as in the previous example. By assumption, the vector
‚ÉóX = (X1, X2) is uniformly distributed on R, hence its distribution density p is given by
p(x1, x2) = {
1
vol2(R)
if (x1, x2) ‚ààR,
0
if (x1, x2) ‚àâR .
For the density p1 of X1, we get
p1(x1) =
‚àû
‚à´
‚àí‚àû
p(x1, x2) dx2 = Œ≤2 ‚àíŒ±2
vol2(R) =
1
Œ≤1 ‚àíŒ±1
provided that Œ±1 ‚â§x1 ‚â§Œ≤1. Otherwise, we have p1(x1) = 0. This tells us that X1 is uni-
formly distributed on [Œ±1, Œ≤1]. In the same way, we obtain for x2 ‚àà[Œ±2, Œ≤2] that
p2(x2) =
1
Œ≤2 ‚àíŒ±2
and p2(x2) = 0 otherwise. Hence, X2 is also uniformly distributed, but this time on
[Œ±2, Œ≤2]. From the equations for p1 and p2 it follows that for the joint density p one has
p(x1, x2) = p1(x1) ‚ãÖp(x2) ,
(x1, x2) ‚àà‚Ñù2 .
Consequently, by Proposition 3.6.20, the random variables X1 and X2 are independent,
and so are the coordinates x1 and x2 of the point hit.
Example 3.6.23. Let us look at Example 3.6.22 from the opposite side. Now we assume
that the coordinates are uniformly distributed, not the vector. Thus let U1, . . . , Un be
independent random variables with Uj uniformly distributed on the interval [Œ±j, Œ≤j],
1 ‚â§j ‚â§n. Then the random vector
‚ÉóU = (U1, . . . , Un) is (multivariate) uniformly dis-
tributed on the box K = [Œ±1, Œ≤1] √ó ‚ãÖ‚ãÖ‚ãÖ√ó [Œ±n, Œ≤n]. This is an immediate consequence of
Example 1.9.17 combined with Proposition 3.6.5. A direct proof of this fact, without us-
ing product measures, is as follows.
The density of Uj is pj =
1
Œ≤j‚àíŒ±j 1[Œ±j,Œ≤j], hence by Proposition 3.6.20 the joint density p
of
‚ÉóU is given by

168
‡±™
3 Random variables and their distribution
p(x) = p1(x1) ‚ãÖ‚ãÖ‚ãÖpn(xn) =
n
‚àè
j=1
(Œ≤j ‚àíŒ±j)‚àí1 =
1
voln(K) ,
x = (x1, . . . , xn) ‚ààK ,
and p(x) = 0 if x ‚àâK. Therefore,
‚Ñô{ ‚ÉóU ‚ààB} = ‚à´
B
p(x) dx = voln(K ‚à©B)
voln(K)
,
B ‚àà‚Ñ¨(‚Ñùn) ,
and
‚ÉóU is uniformly distributed on K as asserted.
Example 3.6.24. Let X1, . . . , Xn be independent standard normally distributed. Which
joint density does the vector ‚ÉóX = (X1, . . . , Xn) possess?
Answer: The density pj of the Xj, for j = 1, . . . , n, is
pj(x) =
1
‚àö2œÄ
e‚àíx2/2 ,
x ‚àà‚Ñù.
Consequently, by the independence of the Xjs, the joint density p equals
p(x) = p1(x1) ‚ãÖ‚ãÖ‚ãÖpn(xn) =
1
(2œÄ)n/2 e‚àí(x2
1+‚ãÖ‚ãÖ‚ãÖ+x2
n)/2
=
1
(2œÄ)n/2 e‚àí|x|2/2 ,
x = (x1, . . . , xn) .
This tells us that ‚Ñô‚ÉóX = ùí©(0, 1)‚äón (cf. Definition 1.9.21) or, equivalently, ‚ÉóX is n-dimensional
standard normally distributed.
Example 3.6.25. If X1, . . . , Xn are independent EŒª-distributed, then
pj(t) = {0
if t < 0,
Œªe‚àíŒªt
if t ‚â•0,
hence, the random vector ‚ÉóX = (X1, . . . , Xn) has the joint density
p(t) = Œªne‚àíŒª(t1+‚ãÖ‚ãÖ‚ãÖ+tn) ,
t = (t1, . . . , tn) ,
tj ‚â•0 ,
and p(t) = 0 if one of the tjs is negative.
Example 3.6.26. Suppose two random variables X1 and X2 are independent and EŒª (ex-
ponentially) and uniformly on [0, 1] distributed, respectively. Which distribution does
the vector ‚ÉóX = (X1, X2) possess?
The density p of ‚ÉóX equals
p(t, s) = {Œªe‚àíŒªt ‚ãÖ1[0,1](s)
if t ‚â•0,
0
otherwise.

3.7 Order statistics‚àó
‡±™
169
For example, if B = {(t, s) : 0 ‚â§t ‚â§s ‚â§1}, then
‚Ñô{ ‚ÉóX ‚ààB} = Œª
1
‚à´
0
s
‚à´
0
e‚àíŒªtdt ds =
1
‚à´
0
(1 ‚àíe‚àíŒªs) ds = 1 + 1
Œª(e‚àíŒª ‚àí1) .
When does this event B occur? Say the lifetime of a component is exponentially dis-
tributed with parameter Œª > 0. Then the event B occurs if the component stops working
before a randomly chosen time s ‚àà[0, 1]. This number s is taken uniformly distributed
on [0, 1] and, moreover, independent of the lifetime of the component. For example, first
choose the number s ‚àà[0, 1], then check whether or not the component becomes defec-
tive before time s ‚àà[0, 1].
Summary: The random variables X1, . . . , Xn are independent if for all Borel sets B1, . . . , Bn in ‚Ñùit follows that
‚Ñô{X1 ‚ààB1, . . . , Xn ‚ààBn} = ‚Ñô{X1 ‚ààB1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn ‚ààBn} .
In other words, the joint distribution ‚Ñô(X1,...,Xn) has to coincide with the product measure of the marginal
distributions ‚ÑôXj, 1 ‚â§j ‚â§n. Another equivalent condition for the independence is: for all real numbers
t1, . . . , tn, it follows that
‚Ñô{X1 ‚â§t1, . . . , Xn ‚â§tn} = ‚Ñô{X1 ‚â§t1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn ‚â§tn} .
If X1, . . . , Xn are continuous random variables with densities p1, . . . , pn, then they are independent if and only
if
p(x1, . . . , xn) = p1(x1) ‚ãÖ‚ãÖ‚ãÖ‚ãÖ‚ãÖpn(xn) ,
(x1, . . . , xn) ‚àà‚Ñùn ,
is a density of the random n-dimensional vector ‚ÉóX = (X1, . . . , Xn).
3.7 Order statistics‚àó
This section is devoted to a quite practical problem. Suppose we execute independently
of each other the same random experiment n times. Say the results are the real numbers
x1, . . . , xn. For example, one may think of n different measurements of the same item,
and x1, . . . , xn are the observed values. After getting the xjs, we reorder them by their
size. These ‚Äúnew‚Äù numbers are denoted by x‚àó
1 , . . . , x‚àó
n and satisfy x‚àó
1 ‚â§‚ãÖ‚ãÖ‚ãÖ‚â§x‚àó
n . In other
words, the numbers are the same as before but now in nondecreasing order.
A slightly more precise way to introduce the ordered sample x‚àó
1 , . . . , x‚àó
n is as follows:
There exists at least one permutation œÄ of order n (maybe more than one if some of the
xjs are equal) for which xœÄ(1) ‚â§xœÄ(2) ‚â§‚ãÖ‚ãÖ‚ãÖ‚â§xœÄ(n) . Setting
x‚àó
1 = xœÄ(1) ,
x‚àó
2 = xœÄ(2),
up to
x‚àó
n = xœÄ(n) ,

170
‡±™
3 Random variables and their distribution
the new sample x‚àó
1 , . . . , x‚àó
n consists of the same values as x1, . . . , xn, but now these values
are ordered by their size.
For example, if x1 = 8.5, x2 = 7.1, and x3 = 7.9, then we obtain x‚àó
1 = 7.1, x‚àó
2 = 7.9, and
x‚àó
3 = 8.5.
In order to apply this ordering procedure to random observations, one needs an
algorithm for constructing the x‚àó
j s. This is easy if xi
Ã∏= xj for i
Ã∏= j. Then
x‚àó
1 = min{xi : 1 ‚â§i ‚â§n}
while
x‚àó
k = min{xi : xi > x‚àó
k‚àí1}
if k ‚â•2 .
For general values, that is, not necessarily different ones, the construction is slightly
more complicated. The basic observation is that the ordered values possess the following
property: given t ‚àà‚Ñù, for all 1 ‚â§k ‚â§n,
x‚àó
k ‚â§t
‚áî
Number of xi ‚â§t is at least k
‚áî
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®{i ‚â§n : xi ‚â§t}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚â•k .
(3.24)
This implies that
x‚àó
k = inf{t ‚àà‚Ñù: ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®{i ‚â§n : xi ‚â§t}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚â•k} = inf{t ‚àà‚Ñù:
n
‚àë
i=1
1(‚àí‚àû,t](xi) ‚â•k} .
(3.25)
For better understanding, here an easy example.
Example 3.7.1. Assume our sample is
x1 = 3 ,
x2 = 1 ,
x3 = 3,
and
x4 = 2 .
Then we get
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®{i ‚â§4 : xi ‚â§t}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®=
{
{
{
{
{
{
{
{
{
{
{
{
{
0
if ‚àí‚àû< t < 1,
1
if 1 ‚â§t < 2,
2
if 2 ‚â§t < 3,
4
if 3 ‚â§t < ‚àû,
which implies
inf{t ‚àà‚Ñù: ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®{i ‚â§4 : xi ‚â§t}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚â•k} =
{
{
{
{
{
{
{
{
{
{
{
{
{
1
if k = 1,
2
if k = 2,
3
if k = 3,
3
if k = 4.
So we finally arrive at x‚àó
1 = 1, x‚àó
2 = 2, x‚àó
3 = 3, and x‚àó
4 = 3.
The basic question treated in this section is now as follows. Suppose the values
x1, . . . , xn were obtained by n random experiments, independently and according to a

3.7 Order statistics‚àó
‡±™
171
known distribution. How are then the ordered x‚àó
k s distributed? For example, one rolls
a fair die 10 times and observes x1, . . . , x10 in {1, . . . , 6}. Then, for instance, one may ask
how likely it is that the third smallest value x‚àó
3 equals 5. Or which probability does the
event {x‚àó
4 = 3} possess?
The precise mathematical formulation of this problem is as follows: let X1, . . . , Xn
be n independent identically distributed random variables defined on a sample space
Œ©. Define random variables X‚àó
1 , . . . , X‚àó
n as its values ordered by their size. One possible
way to define these new random variables is by, for example, using eq. (3.25). That is,
given k ‚â§n and œâ ‚ààŒ©,
X‚àó
k (œâ) = inf{t ‚àà‚Ñù: ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®{i ‚â§n : Xi(œâ) ‚â§t}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚â•k}
= inf{t ‚àà‚Ñù:
n
‚àë
i=1
1(‚àí‚àû,t](Xi(œâ)) ‚â•k}.
In particular, we get
X‚àó
1 = min{X1, . . . , Xn}
and
X‚àó
n = max{X1, . . . , Xn} .
Remark 3.7.2. Those to whom the definition of the X‚àó
k s looks too complicated may use
the following construction: for each fixed œâ ‚ààŒ©, one chooses a permutation œÄ of order
n, depending on œâ, for which
XœÄ(1)(œâ) ‚â§XœÄ(2)(œâ) ‚â§‚ãÖ‚ãÖ‚ãÖ‚â§XœÄ(n)(œâ) .
Then one gets the X‚àó
k s by setting
X‚àó
1 (œâ) = XœÄ(1)(œâ) ,
X‚àó
2 (œâ) = XœÄ(2)(œâ),
up to
X‚àó
n (œâ) = XœÄ(n)(œâ) .
In this connection, it is important that there are at most finitely many (not more than n!)
different permutations œÄ, depending on œâ, for ordering the values X1(œâ) up to Xn(œâ) by
their size. Thus, our sample space splits into at most n! subsets where in each of them
one special permutation orders all Xj(œâ) in nondecreasing order.
Definition 3.7.3. The ordered random variables X‚àó
1 , . . . , X‚àó
n are called the order statistics of X1, . . . , Xn.
Similarly, if x1, . . . , xn are observed random real numbers, the ordered x‚àó
k s are the order statistics of
the xjs.
Remark 3.7.4. By construction, the random variables X‚àó
k satisfy X‚àó
1 ‚â§‚ãÖ‚ãÖ‚ãÖ‚â§X‚àó
n . But note
that they are no longer independent nor are identically distributed.
Remark 3.7.5. Order statistics play an important role in Mathematical Statistics. For ex-
ample, suppose at time t = 0 we switch on n light bulbs of the same type. Let us record
the times 0 < t‚àó
1 < t‚àó
2 < ‚ãÖ‚ãÖ‚ãÖ< t‚àó
n, where some of the n bulbs burn out. Then these times

172
‡±™
3 Random variables and their distribution
are nothing else than the order statistics of the lifetimes t1, . . . , tn of the first, second, and
so on, light bulb.
Before we state and prove the main result of this section, let us recall that the Xjs
are assumed to be identically distributed. Consequently, all of them possess the same
distribution function F. That is, for all j ‚â§n, we have
F(t) = ‚Ñô{Xj ‚â§t} ,
t ‚àà‚Ñù,
Proposition 3.7.6. Let X1, . . . , Xn be independent identically distributed random variables
with distribution function F. Then for each k ‚â§n, we have
‚Ñô{X‚àó
k ‚â§t} =
n
‚àë
i=k
(n
i)F(t)i (1 ‚àíF(t))
n‚àíi ,
t ‚àà‚Ñù.
(3.26)
Proof. Fix t ‚àà‚Ñù. When does the event {X‚àó
k ‚â§t} occur? To answer this, for i ‚â§n introduce
disjoint sets Ai as follows: the event Ai occurs if and only if exactly i of the Xjs attain a
value in (‚àí‚àû, t]. More precisely,
Ai = {œâ ‚ààŒ© : ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®{j ‚â§n : Xj(œâ) ‚â§t}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®= i} .
Using eq. (3.24), the event {X‚àó
k ‚â§t} occurs if and only if at least k of the Xjs attain a value
in (‚àí‚àû, t]. Thus, by the definition of the Ais, the event {X‚àó
k ‚â§t} coincides with ‚ãÉn
i=k Ai.
Consequently, since the Ais are disjoint, it follows that
‚Ñô{X‚àó
k ‚â§t} =
n
‚àë
i=k
‚Ñô(Ai) .
(3.27)
Let Yj = 1(‚àí‚àû,t](Xj). Then Yj = 1 if and only if Xj ‚â§t while Yj = 0 otherwise. Hence, the
Yjs are binomial distributed with parameters 1 and p, where
p = ‚Ñô{Yj = 1} = ‚Ñô{Xj ‚â§t} = F(t) .
Since the Xjs are independent, so are the Yjs and their sum10 Y1 + ‚ãÖ‚ãÖ‚ãÖ+ Yn is binomial
distributed with parameters n and p = F(t). Note that the event Ai occurs if and only if
Y1 + ‚ãÖ‚ãÖ‚ãÖ+ Yn = i, which implies
‚Ñô(Ai) = ‚Ñô{Y1 + ‚ãÖ‚ãÖ‚ãÖ+ Yn = i} = (n
i)pi(1 ‚àíp)n‚àíi = (n
i)F(t)i(1 ‚àíF(t))
n‚àíi .
(3.28)
Plugging eq. (3.28) into eq. (3.27) proves eq. (3.26).
10 Here we already use a result, which will be proved later on in Proposition 4.6.1.

3.7 Order statistics‚àó
‡±™
173
Example 3.7.7. Let us choose independently and according to the uniform distribution
n numbers x1, . . . , xn out of {1, . . . , N}. Here, the same number may be chosen more than
once. Given integers m ‚â§N and k ‚â§n, find the probability that the kth largest number
x‚àó
k equals m.
Answer: The distribution function F of the uniform distribution on {1, . . . , N} satis-
fies
F(m) = m
N ,
m = 1, . . . , N .
Thus Proposition 3.7.6 implies
‚Ñô{x‚àó
k ‚â§m} =
n
‚àë
i=k
(n
i)(m
N )
i
(1 ‚àím
N )
n‚àíi
.
(3.29)
In particular,
‚Ñô{x‚àó
n = 1} = ‚Ñô{x‚àó
n ‚â§1} = ( 1
N )
n
and
‚Ñô{x‚àó
k ‚â§N} = 1 ,
k = 1, . . . , n .
Because of {x‚àó
k = m} = {x‚àó
k ‚â§m} \ {x‚àó
k ‚â§m ‚àí1}, we obtain
‚Ñô{x‚àó
k = m} = ‚Ñô{x‚àó
k ‚â§m} ‚àí‚Ñô{x‚àó
k ‚â§m ‚àí1}
=
n
‚àë
i=k
(n
i)[(m
N )
i
(1 ‚àím
N )
n‚àíi
‚àí(m ‚àí1
N
)
i
(1 ‚àím ‚àí1
N
)
n‚àíi
] .
(3.30)
Here the right-hand expression vanishes in the case m = 1. For instance, we get
‚Ñô{x‚àó
1 = 1} =
n
‚àë
i=1
(n
i)( 1
N )
i
(1 ‚àí1
N )
n‚àíi
= 1 ‚àí(1 ‚àí1
N )
n
.
Another, more direct, approach for this result is as follows: One has x‚àó
1 > 1 if and only if
all xjs satisfy xj ‚â•2. And among all possible Nn ways to choose the xjs, there are (N ‚àí1)n
possible ones to choose the xjs in {2, . . . , N}.
Example 3.7.8. Roll a die four times and order the results in nondecreasing order as
x‚àó
1 ‚â§x‚àó
2 ‚â§x‚àó
3 ‚â§x‚àó
4 . What is the probability that x‚àó
3 equals m for some 1 ‚â§m ‚â§6?
Answer: Let us apply formula (3.29) with N = 6, k = 3, and n = 4. For m ‚àà{1, . . . , 6},
this implies
‚Ñô{x‚àó
3 ‚â§m} =
4
‚àë
i=3
(4
i)(m
6 )
i
(6 ‚àím
6
)
4‚àíi
,
hence
‚Ñô{x‚àó
3 = m} =
4
‚àë
i=3
(4
i)[(m
6 )
i
(6 ‚àím
6
)
4‚àíi
‚àí(m ‚àí1
6
)
i
(6 ‚àím + 1
6
)
4‚àíi
] .

174
‡±™
3 Random variables and their distribution
The probabilities are
m
‚Ñô{x‚àó
3 ‚â§m}
1
0.0162037
2
0.111111
3
0.3125
4
0.592593
5
0.868056
6
1
and
m
‚Ñô{x‚àó
3 = m}
1
0.0162037
2
0.0949074
3
0.201389
4
0.280093
5
0.275463
6
0.131944
Thus, the most likely value of x‚àó
3 is the number ‚Äú4.‚Äù
Remark 3.7.9. If we choose as in a lottery 6 numbers out of 49, then Proposition 3.7.6
and/or Example 3.7.7 do not apply. Why? Let x1, . . . , x6 be the numbers chosen first, sec-
ond, and so on. Then they are identically distributed on {1, . . . , 49}, but they are not inde-
pendent. For example, the probability that the second choice is number ‚Äú2‚Äù given the first
number was ‚Äú1‚Äù is not 1/49. This would be the case if the chosen number were replaced
after each choice. We refer to Problem 3.5 for the distribution of the order statistics in
the case of lottery numbers.
Let us now turn to the case of continuous random variables. That is, we assume that
the random variables Xj possess a distribution density p satisfying
‚Ñô{Xj ‚â§t} =
t
‚à´
‚àí‚àû
p(x) dx ,
t ‚àà‚Ñù.
Again we remark that the preceding formula holds for all j ‚â§n. Indeed, the Xjs are
identically distributed, hence they all have the same density. A natural question arises:
what distribution density does X‚àó
k possess?
Proposition 3.7.10. Suppose p is the common density of the Xjs. Let X‚àó
1 ‚â§‚ãÖ‚ãÖ‚ãÖ‚â§X‚àó
n be the
order statistics of the Xj. Then the distribution density pk of X‚àó
k is given by
pk(t) =
n!
(k ‚àí1)!(n ‚àík)! p(t) F(t)k‚àí1(1 ‚àíF(t))
n‚àík .
Proof. It holds that
pk(t) = d
dt ‚Ñô{X‚àó
k ‚â§t} = d
dt
n
‚àë
i=k
(n
i)F(t)i (1 ‚àíF(t))
n‚àíi
=
n
‚àë
i=k
i(n
i) p(t)F(t)i‚àí1(1 ‚àíF(t))
n‚àíi ‚àí
n
‚àë
i=k
(n ‚àíi)(n
i) p(t)F(t)i (1 ‚àíF(t))
n‚àíi‚àí1.
(3.31)

3.7 Order statistics‚àó
‡±™
175
In fact, the index i in the second sum of eq. (3.31) runs only from k to n‚àí1. Hence, shifting
it by 1, this sum becomes
n
‚àë
i=k+1
(n ‚àíi + 1) ( n
i ‚àí1) p(t) F(t)i‚àí1 (1 ‚àíF(t))
n‚àíi .
Because of
i (n
i) =
n!
(i ‚àí1)!(n ‚àíi)! = (n ‚àíi + 1) ( n
i ‚àí1),
both sums in eq. (3.31) cancel out for i = k + 1, . . . , n. Thus, we obtain
pk(t) = k (n
k) p(t) F(t)k‚àí1(1 ‚àíF(t))
n‚àík
=
n!
(k ‚àí1)!(n ‚àík)! p(t) F(t)k‚àí1(1 ‚àíF(t))
n‚àík,
as asserted.
Example 3.7.11. Let us choose independently and according to the uniform distribution
on [0, 1] numbers x1, . . . , xn. After reordering them, we get 0 ‚â§x‚àó
1 ‚â§‚ãÖ‚ãÖ‚ãÖ‚â§x‚àó
n ‚â§1. Which
distribution does x‚àó
k possess?
Answer: The density p of the uniform distribution on [0, 1] is 1[0,1]. Furthermore, its
distribution function F is given by F(t) = t for 0 ‚â§t ‚â§1. Thus, by Proposition 3.7.10, the
density pk coincides with
pk(t) =
n!
(k ‚àí1)!(n ‚àík)! tk‚àí1(1 ‚àít)n‚àík ,
0 ‚â§t ‚â§1 .
As already mentioned in Example 1.6.33, this is nothing else than the density of a beta dis-
tribution with parameters k and n ‚àík + 1. Hence, for all k = 1, . . . , n and all 0 ‚â§a < b ‚â§1,
it follows that
‚Ñô{a ‚â§x‚àó
k ‚â§b} = ‚Ñ¨k,n‚àík+1([a, b]) =
n!
(k ‚àí1)! (n ‚àík)!
b
‚à´
a
xk‚àí1(1 ‚àíx)n‚àík dx .
Example 3.7.12. Let us investigate here the example that was already mentioned in Re-
mark 3.7.5. At time t = 0, we switch on n electric bulbs of the same type. The times
0 < t‚àó
1 ‚â§‚ãÖ‚ãÖ‚ãÖ‚â§t‚àó
n are those where we observe that some of the bulbs burn out. If we as-
sume that the lifetime of each bulb is exponentially distributed, what can we say about
the distribution of the t‚àó
k s?
Answer: Let X1, . . . , Xn be the lifetimes of the n light bulbs. By assumption, they are
independent and exponentially distributed with some parameter Œª > 0. Then the distri-
bution of t‚àó
k is that of X‚àó
k . Furthermore, we have p(t) = Œªe‚àíŒªt and F(t) = 1 ‚àíe‚àíŒªt for t ‚â•0.

176
‡±™
3 Random variables and their distribution
By Proposition 3.7.10, the distribution density pk of X‚àó
k equals
pk(t) = Œª
n!
(k ‚àí1)!(n ‚àík)! (1 ‚àíe‚àíŒªt)
k‚àí1 e‚àíŒªt(n‚àík+1) ,
t ‚â•0 .
For example, for t‚àó
1 , the time when we observe the first burnout of any of the bulbs, this
implies
p1(t) = Œª n e‚àíŒªtn ,
t ‚â•0 ,
that is, t‚àó
1 is EŒªn-distributed.
Another case of interest is the behavior of t‚àó
n which is the time of the last outage of
one of the n bulbs. Its density is
pn(t) = Œªn(1 ‚àíe‚àíŒªt)
n‚àí1e‚àíŒªt ,
t > 0 .
If Fn(t) = (1 ‚àíe‚àíŒªt)n, t > 0, then F‚Ä≤
n(t) = pn(t). Moreover, Fn(0) = 0 and Fn(‚àû) = 1, which
implies that Fn is a distribution function and pn is its density. That is,
‚Ñô{t‚àó
n ‚â§t} = (1 ‚àíe‚àíŒªt)
n ,
t > 0 .
Note that there is a more direct way to determine the distribution of t‚àó
n: use
‚Ñô{t‚àó
n ‚â§t} = ‚Ñô{X1 ‚â§t, . . . , Xn ‚â§t}
and the independence of X1, . . . , Xn.
Summary: Let X1, . . . , Xn be independent identically distributed random variables with distribution function
F : ‚Ñù‚Üí‚Ñù. If X‚àó
1 ‚â§‚ãÖ‚ãÖ‚ãÖ‚â§X‚àó
n is the sequence of the ordered values, then
‚Ñô{X‚àó
k ‚â§t} =
n
‚àë
i=k
(n
i )F(t)i (1 ‚àíF(t))
n‚àíi ,
t ‚àà‚Ñù.
Moreover, if the Xjs are continuous with density p, then for 1 ‚â§k ‚â§n, the density pk of X‚àó
k equals
pk(t) =
n!
(k ‚àí1)!(n ‚àík)! p(t) F(t)k‚àí1(1 ‚àíF(t))
n‚àík ,
t ‚àà‚Ñù.
3.8 Problems
Problem 3.1. For n ‚â•1, let Sn be the set of permutations of order n. Suppose ‚Ñôis the
uniform distribution on Sn. That is, all permutations are equally likely. Define now a
mapping X from Sn to {1, . . . , n} by X(œÄ) = œÄ(n), œÄ ‚ààSn. Determine the probability distri-
bution ‚ÑôX of X. What happens if one defines X by X(œÄ) = œÄ(k) for some fixed k ‚â§n?

3.8 Problems
‡±™
177
Problem 3.2. Roll a fair die twice. Let œâ = (œâ1, œâ2) be the observed values. Define two
random variables X1 and X2 by
X1(œâ) = max{œâ1, œâ2}
and
X2(œâ) = œâ1 + œâ2 .
Find the joint distribution of the random vector (X1, X2), as well as its marginal ones.
Argue why X1 and X2 are not independent.
Problem 3.3. The joint distribution of a random vector ‚ÉóX = (X1, X2) is described by
X2\X1
0
1
0
1
10
2
5
1
2
5
1
10
Define another vector
‚ÉóY = (Y1, Y2) by Y1 := min{X1, X2} and Y2 := max{X1, X2}. Find the
probability distribution of ‚ÉóY = (Y1, Y2). Are Y1 and Y2 independent?
Problem 3.4. Let ‚ÉóX = (X1, X2) be uniformly distributed on the square in ‚Ñù2 with corner
points (0, 1), (1, 0), (0, ‚àí1), and (‚àí1, 0). Find the marginal distributions of ‚ÉóX.
Problem 3.5. In a lottery, six numbers are chosen out of {1, . . . , 49}. As usual in lotter-
ies, chosen numbers are not replaced. Let X1, . . . , X6 be the chosen numbers as they ap-
peared. That is, X1 is the number chosen first while X6 is the number, which appeared
last.
1.
Determine the joint distribution of the vector
‚ÉóX = (X1, . . . , X6), as well its marginal
distributions.
2.
Argue why X1, . . . , X6 are not independent.
3.
Reordering the six chosen numbers leads to the order statistics X‚àó
1 < ‚ãÖ‚ãÖ‚ãÖ< X‚àó
6 . Find
the joint distribution of the vector (X‚àó
1 , . . . , X‚àó
6 ), as well as its marginal distribu-
tions.
Problem 3.6. A random variable X is geometrically distributed. Given natural numbers
k and n, show that
‚Ñô{X = k + n | X > n} = ‚Ñô{X = k} .
Why is this property called ‚Äúlack of memory property‚Äù?
Problem 3.7. A random variable is exponentially distributed. Prove
‚Ñô{X > s + t | X > s} = ‚Ñô{X > t}
for all t, s ‚â•0. Why is this called the ‚Äúnonaging property‚Äù of exponentially distributed
lifetimes?

178
‡±™
3 Random variables and their distribution
Problem 3.8. Two random variables X and Y are independent and geometrically dis-
tributed with parameters p and q for some 0 < p, q < 1. Evaluate ‚Ñô{X ‚â§Y ‚â§2X}.
Problem 3.9. Suppose two independent random variables X and Y satisfy
‚Ñô{X = k} = ‚Ñô{Y = k} = 1
2k ,
k = 1, 2, . . .
Find the probabilities ‚Ñô{X ‚â§Y} and ‚Ñô{X = Y}.
Problem 3.10. Choose two numbers b and c independently, the number b according to
the uniform distribution on [‚àí1, 1] and c according to the uniform distribution on [0, 1].
Find the probability that the equation
x2 + bx + c = 0
does not possess a real solution.
Problem 3.11. Use Problem 1.38 to prove the following: If X is a random variable, then
the number of points t ‚àà‚Ñùwith ‚Ñô{X = t} > 0 is at most countably infinite.
Problem 3.12. Suppose a fair coin is labeled with ‚Äú0‚Äù and ‚Äú1.‚Äù Toss the coin n times. Let
X be the maximum observed value and Y the sum of the n values. Determine the joint
distribution of (X, Y). Argue that X and Y are not independent.
Problem 3.13. Suppose a random vector (X, Y) has the joint density function p defined
by
p(u, v) := {c ‚ãÖu v
if u, v ‚â•0, u + v ‚â§1,
0
if otherwise.
(a) Find the value of the constant c so that p becomes a density function.
(b) Determine the density functions of X and Y.
(c) Evaluate ‚Ñô{X + Y ‚â§1/2}.
(d) Are X and Y independent?
Problem 3.14. Gambler A has a biased coin with ‚Äúheads‚Äù having probability p for some
0 < p < 1, and gambler B‚Äôs coin is biased with ‚Äúheads‚Äù having probability q for some
0 < q < 1. Gamblers A and B toss their coins simultaneously. Whoever gets ‚Äúheads‚Äù first
wins. If both gamblers observe ‚Äúheads‚Äù at the same time, then the game ends in a draw.
Evaluate the probability that A wins and the probability that the game ends in a draw.
Problem 3.15. Randomly choose two integers x1 and x2 from 1 to 10. Let X be the mini-
mum of x1 and x2. Determine the distribution and the probability mass functions of X in
the two following cases:

3.8 Problems
‡±™
179
‚Äì
The number chosen first is replaced.
‚Äì
The first number is not replaced.
Evaluate in both cases ‚Ñô{2 ‚â§X ‚â§3} and ‚Ñô{X ‚â•8}.
Problem 3.16. There are four balls labeled with ‚Äú0‚Äù and three balls are labeled with ‚Äú2‚Äù
in an urn. Choose three balls without replacement. Let X be the sum of the values on the
three chosen balls. Find the distribution of X.
Problem 3.17. As in Example 3.7.7, choose independently n numbers uniformly dis-
tributed in {1, . . . , N}. How likely is it that the largest of the chosen n numbers equals
N? Answer this question by applying formula (3.30). Give also a direct argument for the
obtained result.
Problem 3.18. Roll a fair die 5 times. How likely is it that the fourth largest number
equals m for some m = 1, . . . , 6?

4 Operations on random variables
4.1 Mappings of random variables
This section is devoted to the following problem: let X : Œ© ‚Üí‚Ñùbe a random variable
and let f : ‚Ñù‚Üí‚Ñùbe some function. Set Y := f (X), that is, for all œâ ‚ààŒ© we have
Y(œâ) = f (X(œâ)). Suppose the distribution of X is known. Then the following task arises:
Determine the distribution of Y = f(X) for a given function f : ‚Ñù‚Üí‚Ñù.
For example, if f (t) = t2, and we know the distribution of X, then we ask for the proba-
bility distribution of X2. Is it possible to compute this by easy methods?
At the moment it is not clear at all whether Y = f (X) is a random variable. Only if
this is valid, the probability distribution ‚ÑôY is well defined. For arbitrary functions f ,
this need not to be true, they have to satisfy the following additional property.
Definition 4.1.1. A function f : ‚Ñù‚Üí‚Ñùis called measurable if for B ‚àà‚Ñ¨(‚Ñù) the preimage f ‚àí1(B) is a
Borel set as well.
Remark 4.1.2. As all previous assumption about œÉ-fields, random variables, Borel sets,
and so on, also this is a purely technical condition for f , which will not play an important
role later on. All functions of interest, for example, piecewise continuous, monotone,
pointwise limits of continuous functions, and so on, are measurable.
The measurability of f cannot be avoided because it is needed to prove the following
result.
Proposition 4.1.3. Let X : Œ© ‚Üí‚Ñùbe a random variable. If f : ‚Ñù‚Üí‚Ñùis measurable, then
Y = f (X) is a random variable as well.
Proof. Take a Borel set B ‚àà‚Ñ¨(‚Ñù). Then
Y ‚àí1(B) = X‚àí1(f ‚àí1(B)) = X‚àí1(B‚Ä≤),
with B‚Ä≤ := f ‚àí1(B). We assumed f to be measurable, which implies B‚Ä≤ ‚àà‚Ñ¨(‚Ñù), and hence,
since X is a random variable, we conclude Y ‚àí1(B) = X‚àí1(B‚Ä≤) ‚ààùíú. The Borel set B was
arbitrary, thus, as asserted, Y is a random variable.
So let Y = f (X) for some measurable function f : ‚Ñù‚Üí‚Ñùand some random vari-
able X. Unfortunately, there does not exist a general method for the description of ‚ÑôY
in terms of ‚ÑôX. Only for some special functions f , for example, for linear functions or
for f being strictly monotone and differentiable, there exist general rules for the com-
putation of ‚ÑôY. Nevertheless, quite often we are able to determine ‚ÑôY directly. Mostly
the following two approaches turn out to be helpful.
https://doi.org/10.1515/9783111325064-004

4.1 Mappings of random variables
‡±™
181
If X is discrete with values in D := {x1, x2, . . . }, then Y = f (X) maps the sample space
Œ© into f (D) = {f (x1), f (x2), . . . }. Problems arise if f is not one-to-one. In this case one has
to combine those xjs that are mapped onto the same element in f (D). For example, if X
is uniformly distributed on D = {‚àí2, ‚àí1, 0, 1, 2} and f (x) = x2, then Y = X2 has values in
f (D) = {0, 1, 4}. Combining ‚àí1 and 1, as well as ‚àí2 and 2, leads to
‚Ñô{Y = 0} = ‚Ñô{X = 0} = 1
5 ,
‚Ñô{Y = 1} = ‚Ñô{X = ‚àí1} + ‚Ñô{X = 1} = 2
5 ,
‚Ñô{Y = 4} = ‚Ñô{X = ‚àí2} + ‚Ñô{X = 2} = 2
5 .
The case of one-to-one functions f is easier to handle because then
‚Ñô{Y = f (xj)} = ‚Ñô{X = xj} ,
j = 1, 2, . . . ,
and the distribution of Y can be directly computed from that of X.
For continuous X, one tries to determine the distribution function FY of Y. Recall
that this was defined as
FY(t) = ‚Ñô{Y ‚â§t} = ‚Ñô{f (X) ‚â§t} .
If we are able to compute FY, then we are almost done because then we get the distribu-
tion density q of Y as the derivative of FY.
For instance, if the continuous function f is increasing, one gets FY easily by
FY(t) = ‚Ñô{X ‚â§f ‚àí1(t)} = FX(f ‚àí1(t))
with the inverse function f ‚àí1 (cf. Problem 4.16).
The following examples demonstrate how we compute the distribution of f (X) in
some special cases.
Example 4.1.4. Assume the random variable X is ùí©(0, 1)-distributed. Which distribu-
tion does Y := X2 possess?
Answer: Of course, FY(t) = ‚Ñô{Y ‚â§t} = 0 when t ‚â§0. Consequently, it suffices to
determine FY(t) for t > 0. Then
FY(t) = ‚Ñô{X2 ‚â§t} = ‚Ñô{‚àí‚àöt ‚â§X ‚â§‚àöt} =
1
‚àö2œÄ
‚àöt
‚à´
‚àí‚àöt
e‚àís2/2 ds
=
2
‚àö2œÄ
‚àöt
‚à´
0
e‚àís2/2 ds = h(‚àöt) ,
where

182
‡±™
4 Operations on random variables
h(u) := ‚àö2
‚àöœÄ
u
‚à´
0
e‚àís2/2 ds ,
u ‚â•0 .
Differentiating FY with respect to t, the chain rule and the fundamental theorem of Cal-
culus lead to
q(t) = F‚Ä≤
Y(t) = d
dt (‚àöt) h‚Ä≤(‚àöt) = t‚àí1/2
2
‚ãÖ‚àö2
‚àöœÄ e‚àít/2
=
1
21/2Œì(1/2) t
1
2 ‚àí1 e‚àít/2 ,
t > 0 .
Hereby, in the last step, we used Œì(1/2) = ‚àöœÄ. Consequently, Y possesses the density
function
q(t) = {0
if t ‚â§0,
1
21/2Œì(1/2) t‚àí1/2e‚àít/2
if t > 0 .
But this is the density of a Œì2, 1
2 -distribution. Therefore, we obtained the following result,
which we, because of its importance, state as a separate proposition.
Proposition 4.1.5. If X is ùí©(0, 1)-distributed, then X2 is Œì2, 1
2 -distributed or, equivalently,
distributed according to œá2
1.
Example 4.1.6. Let U be uniformly distributed on [0, 1]. Which distribution does the
random variable Y = 1/U possess?
Answer: Again we determine FY. From ‚Ñô{X ‚àà(0, 1]} = 1, we derive ‚Ñô{Y ‚â•1} = 1,
thus, FY(t) = 0 if t < 1. Therefore, we only have to regard numbers t ‚â•1. Here we have
FY(t) = ‚Ñô{ 1
U ‚â§t} = ‚Ñô{U ‚â•1
t } = 1 ‚àí1
t .
Hence, the density function q of Y is given by
q(t) = F‚Ä≤
Y(t) = {0
if t < 1,
1
t2
if t ‚â•1.
Example 4.1.7 (Random walk on ‚Ñ§). A particle is located at the point 0 of ‚Ñ§. In the first
step, it moves either to ‚àí1 or to +1. In the second step, it jumps, independently of the
first move, again to the left or to the right. Thus, after two steps it is located either at
‚àí2, 0, or 2. Hereby we assume that p is the probability for jumps to the right, hence
1 ‚àíp for jumps to the left. This procedure is repeated arbitrarily often. Let Sn be the
position of the particle after n jumps or, equivalently, after n steps.1 The (random) se-
1 The value of Sn can also be viewed as the loss or win after n games, where p is the probability to win
one dollar in a single game, while 1 ‚àíp is the probability to lose one dollar.

4.1 Mappings of random variables
‡±™
183
quence (Sn)n‚â•0 is called a (next-neighbor) random walk on ‚Ñ§, where by the construction
‚Ñô{S0 = 0} = 1. See Figure 4.1 for a path of a random walk. Note that it is random, thus,
very likely it will look completely different in another experiment.
Figure 4.1: The (joined) points (n, Sn), n = 0, . . . , 400, of a symmetric random walk Sn.
After n steps, the possible positions of the particle are in
Dn = {‚àín, ‚àín + 2, . . . , n ‚àí2, n} .
Note that an integer k belongs to Dn if and only if |k| ‚â§n and, furthermore, n + k (or
equivalently, n ‚àík) is even.
Thus, Sn is a random variable with values in Dn. Which distribution does Sn possess?
To answer this question define
Yn := 1
2 (Sn + n) .
The random variable Yn attains values in {0, 1, . . . , n} and, moreover, Yn = m if the posi-
tion of the particle after n steps is 2m ‚àín, that is, if it jumped m times to the right and
n‚àím times to the left. To see this, take m = 0, hence Sn = ‚àín, which can only be achieved
if all jumps were to the left. If m = 1, then Sn = ‚àín + 2, that is, there were n ‚àí1 jumps to
the left and 1 to the right. The same argument applies for all m ‚â§n.
This observation tells us that Yn is Bn,p-distributed, that is,
‚Ñô{Yn = m} = (n
m)pm(1 ‚àíp)n‚àím ,
m = 0, . . . , n .

184
‡±™
4 Operations on random variables
Since Yn = 1
2 (Sn + n), if k ‚ààDn, then it follows that
‚Ñô{Sn = k} = ‚Ñô{Yn = 1
2 (k + n)} = ( n
n+k
2
) p(n+k)/2 (1 ‚àíp)(n‚àík)/2 .
(4.1)
For even n, we have 0 ‚ààDn, thus one may ask for the probability of Sn = 0, that is,
for the probability that the particle returns to its starting point after n steps. Applying
eq. (4.1) with k = 0 gives for even n that
‚Ñô{Sn = 0} = (n
n
2
)pn/2 (1 ‚àíp)n/2 .
Hence, if p = 1/2 (in this case the walk is said to be symmetric), then for even n we obtain
‚Ñô{Sn = 0} = (n
n
2
) 2‚àín =
n!
((n/2)!)2 2‚àín .
An application of Stirling‚Äôs formula (1.54) implies
lim
n‚Üí‚àû
n even
n1/2 ‚Ñô{Sn = 0} = lim
n‚Üí‚àû
n even
n1/2
‚àö2œÄn (n/e)n
[‚àöœÄn (n/2e)n/2]2 2‚àín = ‚àö2
œÄ ,
that is, if n ‚Üí‚àû, then for even n it follows that ‚Ñô{Sn = 0} ‚àº‚àö2
œÄ n‚àí1/2. Another way to
formulate this is
‚Ñô{S2n = 0} ‚àº
1
‚àöœÄn .
Example 4.1.8. Suppose X is B‚àí
n,p-distributed, that is,
‚Ñô{X = k} = (k ‚àí1
k ‚àín) pn(1 ‚àíp)k‚àín ,
k = n, n + 1, . . .
Let Y = X ‚àín. Which probability distribution does Y possess?
Answer: An easy transformation (see formula (1.35)) leads to
‚Ñô{Y = k} = ‚Ñô{X = k + n} = (n + k ‚àí1
k
)pn (1 ‚àíp)k = (‚àín
k )pn (p ‚àí1)k
(4.2)
for all k = 0, 1, . . .
Additional question: Which random experiment does Y describe?
Answer: We perform a series of random trials where each time we may obtain either
failure or success. Hereby, the success probability is p ‚àà(0, 1). Then the event {Y = k}
occurs if and only if we observe the nth success in trial k + n.
We conclude this section with the investigation of the following problem. Suppose
X1, . . . , Xn are independent random variables. Given n measurable functions f1, . . . , fn
from ‚Ñùto ‚Ñù, we define ‚Äúnew‚Äù random variables Y1, . . . , Yn by

4.2 Linear transformations
‡±™
185
Yi := fi(Xi) ,
1 ‚â§i ‚â§n .
It is intuitively clear that then Y1, . . . , Yn are also independent; the values of Yi only de-
pend on those of Xi, thus the independence should be preserved. For example, if X1 and
X2 are independent, then this should also be valid for X2
1 and 2X2.
The next result shows that this is indeed true.
Proposition 4.1.9. Let X1, . . . , Xn be independent random variables and let (fi)n
i=1 be mea-
surable functions from ‚Ñùto ‚Ñù. Then f1(X1), . . . , fn(Xn) are independent as well.
Proof. Choose arbitrary Borel sets B1, . . . , Bn in ‚Ñùand set Ai := f ‚àí1
i (Bi), 1 ‚â§i ‚â§n. With
this notation, an œâ ‚ààŒ© satisfies fi(Xi(œâ)) ‚ààBi if and only if Xi(œâ) ‚ààAi. Hence, an appli-
cation of the independence of Xi (use eq. (3.16) with the Xi‚Äôs and the Ai‚Äôs) leads to
‚Ñô{f1(X1) ‚ààB1, . . . , fn(Xn) ‚ààBn} = ‚Ñô{X1 ‚ààA1, . . . , Xn ‚ààAn}
= ‚Ñô{X1 ‚ààA1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn ‚ààAn}
= ‚Ñô{f1(X1) ‚ààB1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{fn(Xn) ‚ààBn} .
The Bis were chosen arbitrarily, thus the random variables f1(X1), . . . , fn(Xn) are in-
dependent as well.
Remark 4.1.10. Without proof we still mention that the independence of random vari-
ables is preserved whenever they are put together into disjoint groups. For example, if
X1, . . . , Xn are independent, then so are f (X1, . . . , Xk) and g(Xk+1, . . . , Xn) for suitable func-
tions f and g. Assume we roll a die five times and let X1, . . . , X5 be the results. Then these
random variables are independent, but so are the two random variables max{X1, X2}
and X3 + X4 + X5, or the three X1, max{X2, X3}, and min{X4, X5}.
4.2 Linear transformations
Let a and b real numbers with a
Ã∏= 0. Given a random variable Y = aX+b, that is, Y arises
from X by a linear transformation. We ask now for the probability distribution of Y.
Proposition 4.2.1. Define Y = aX + b with a, b ‚àà‚Ñùand a
Ã∏= 0.
(a) Depending on whether a > 0 or a < 0,
FY(t) = FX(t ‚àíb
a
)
or
FY(t) = 1 ‚àí‚Ñô{X < t ‚àíb
a
} .
If a < 0 and FX is continuous at t‚àíb
a , then
FY(t) = 1 ‚àíFX(t ‚àíb
a
) .
(b) Let X be a continuous random variable with density p. Then Y is also continuous with
density q given by

186
‡±™
4 Operations on random variables
q(t) = 1
|a| p(t ‚àíb
a
) ,
t ‚àà‚Ñù.
(4.3)
Proof. Let us first treat the case a > 0. Then we get
FY(t) = ‚Ñô{aX + b ‚â§t} = ‚Ñô{X ‚â§t ‚àíb
a
} = FX(t ‚àíb
a
),
as asserted.
In the case a < 0, we conclude as follows:
FY(t) = ‚Ñô{aX + b ‚â§t} = ‚Ñô{X ‚â•t ‚àíb
a
} = 1 ‚àí‚Ñô{X < t ‚àíb
a
} .
If FX is continuous at t‚àíb
a , then ‚Ñô{X = t‚àíb
a } = 0, hence
1 ‚àí‚Ñô{X < t ‚àíb
a
} = 1 ‚àí‚Ñô{X ‚â§t ‚àíb
a
} = 1 ‚àíFX(t ‚àíb
a
) ,
completing the proof of part (a).
Suppose now that p is a density function of X, that is,
FX(t) = ‚Ñô{X ‚â§t} =
t
‚à´
‚àí‚àû
p(x)dx ,
t ‚àà‚Ñù.
If a > 0, by part (a) and after the change of variables x = y‚àíb
a , we get
FY(t) = FX(t ‚àíb
a
) =
t‚àíb
a
‚à´
‚àí‚àû
p(x)dx =
t
‚à´
‚àí‚àû
1
a p(y ‚àíb
a
)dy =
t
‚à´
‚àí‚àû
q(y) dy .
Thus, q is a density of Y.
If a < 0, the same change of variables2 leads to
FY(t) = 1 ‚àíFX(t ‚àíb
a
) =
‚àû
‚à´
t‚àíb
a
p(x)dx = ‚àí
t
‚à´
‚àí‚àû
1
a p(y ‚àíb
a
)dy
=
t
‚à´
‚àí‚àû
1
‚àía p(y ‚àíb
a
)dy =
t
‚à´
‚àí‚àû
1
|a| p(y ‚àíb
a
)dy =
t
‚à´
‚àí‚àû
q(y) dy .
This being true for all t ‚àà‚Ñùcompletes the proof.
2 Observe that now a < 0, hence the order of integration changes and a minus sign appears.

4.2 Linear transformations
‡±™
187
Example 4.2.2. Let X be ùí©(0, 1)-distributed. Given a
Ã∏= 0 and Œº ‚àà‚Ñù, we ask for the
distribution of Y = aX + Œº.
Answer: The random variable X is known to be continuous with density
p(t) =
1
‚àö2œÄ
e‚àít2/2 .
We apply eq. (4.3) with b = Œº to deduce that the density q of Y equals
q(t) = 1
|a| p(t ‚àíŒº
|a| ) =
1
‚àö2œÄ |a|
e‚àí(t‚àíŒº)2/2a2
.
That is, the random variable Y is ùí©(Œº, |a|2)-distributed. In particular, if œÉ > 0 and Œº ‚àà‚Ñù,
then œÉX + Œº is distributed according to ùí©(Œº, œÉ2).
Additional question: Suppose Y is ùí©(Œº, œÉ2)-distributed. Which probability distribu-
tion does X := Y‚àíŒº
œÉ
possess?
Answer: Formula (4.3) immediately implies that X has a standard normal distribu-
tion.
Because of the importance of the previous observation, we formulate it as proposi-
tion.
Proposition 4.2.3. Suppose Œº ‚àà‚Ñùand œÉ > 0. Then the following are equivalent:
X is ùí©(0, 1)-distributed
‚áê‚áí
œÉX + Œº is distributed according to ùí©(Œº, œÉ2).
Corollary 4.2.4. Let Œ¶ be the Gaussian Œ¶-function introduced in eq. (1.70). For each inter-
val [a, b],
ùí©(Œº, œÉ2)([a, b]) = Œ¶(b ‚àíŒº
œÉ
) ‚àíŒ¶(a ‚àíŒº
œÉ
) .
Proof. This is a direct consequence of Proposition 4.2.3. Indeed, if X has a standard nor-
mal distribution, then
ùí©(Œº, œÉ2)([a, b]) = ‚Ñô{a ‚â§œÉX + Œº ‚â§b} = ‚Ñô{a ‚àíŒº
œÉ
‚â§X ‚â§b ‚àíŒº
œÉ
}
= Œ¶(b ‚àíŒº
œÉ
) ‚àíŒ¶(a ‚àíŒº
œÉ
),
as asserted.
Let X be an ùí©(Œº, œÉ2)-distributed random variable. The next result shows that X with
high probability (more than 99.7 %) attains values in [Œº‚àí3 œÉ, Œº+3 œÉ]. Therefore, in most
cases, one may assume that X maps into [Œº ‚àí3 œÉ, Œº + 3 œÉ]. This observation is usually
called ‚ÄúThree Sigma Rule‚Äù.

188
‡±™
4 Operations on random variables
Corollary 4.2.5 (Three Sigma Rule). If X is distributed according to ùí©(Œº, œÉ2), then
‚Ñô{|X ‚àíŒº| ‚â§2 œÉ} ‚â•0.954
and
‚Ñô{|X ‚àíŒº| ‚â§3 œÉ} ‚â•0.997 .
Proof. By virtue of Corollary 4.2.4, for each c > 0,
‚Ñô{|X ‚àíŒº| ‚â§c œÉ} = Œ¶(c) ‚àíŒ¶(‚àíc) ,
hence the desired estimates follow from
Œ¶(2) ‚àíŒ¶(‚àí2) = 2Œ¶(2) ‚àí1 > 0.9545
and
Œ¶(3) ‚àíŒ¶(‚àí3) = 2Œ¶(3) ‚àí1 > 0.9973 .
Example 4.2.6. Let U be uniformly distributed on [0, 1]. What is the probability distri-
bution of aU + b if a
Ã∏= 0 and b ‚àà‚Ñù?
Answer: The distribution density p of U is given by p(t) = 1 if 0 ‚â§t ‚â§1 and p(t) = 0
otherwise. Therefore, the density q of aU + b equals
q(t) = {
1
|a|
if 0 ‚â§t‚àíb
a ‚â§1,
0
otherwise .
Assume first a > 0. Then q(t) = 1/a if and only if b ‚â§t ‚â§a+b and q(t) = 0 otherwise.
Consequently, aU + b is uniformly distributed on [b, a + b].
If, in contrast, a < 0, then q(t) = 1/|a| if and only if a + b ‚â§t ‚â§b and q(t) = 0
otherwise. Hence, now aU + b is uniformly distributed on [a + b, b].
It is easy to see that the reversed implications are also true. That is, we have
U uniformly distributed on [0, 1]
‚áî
aU + b uniformly distributed on {[b, a + b]
if a > 0,
[a + b, b]
if a < 0.
Corollary 4.2.7. A random variable U is uniformly distributed on [0, 1] if and only if
1 ‚àíU is such. In other words, for a uniformly distributed U on [0, 1] it follows that
U
d= 1 ‚àíU.
Example 4.2.8. Suppose a random variable X is ŒìŒ±,Œ≤-distributed for some Œ±, Œ≤ > 0 and
let a > 0. Which distribution does aX possess?
Answer: The distribution density p of X satisfies p(t) = 0 if t ‚â§0 and, if t > 0, then
p(t) =
1
Œ±Œ≤ Œì(Œ≤) tŒ≤‚àí1 e‚àít/Œ± .
An application of eq. (4.3) implies that the density q of aX is given by q(t) = 0 if t ‚â§0
and, if t > 0, then

4.3 Coin tossing versus uniform distribution
‡±™
189
q(t) = 1
a p( t
a) =
1
a Œ±Œ≤ Œì(Œ≤)( t
a)
Œ≤‚àí1
e‚àít/aŒ± =
1
(aŒ±)Œ≤ Œì(Œ≤)tŒ≤‚àí1 e‚àít/aŒ± .
Thus, aX is ŒìaŒ±,Œ≤-distributed.
In particular, we have that X is Œì1,Œ≤-distributed if and only if for some (each) Œ± > 0
it follows that Œ±X is ŒìŒ±,Œ≤-distributed.
In the case of the exponential distribution EŒª = Œì1/Œª,1, the previous result implies the
following: if a > 0, then a random variable X is EŒª-distributed if and only if aX possesses
an EŒª/a distribution.
4.3 Coin tossing versus uniform distribution
4.3.1 Binary fractions
We start this section with the following statement: each real number x ‚àà[0, 1) may be
represented as binary fraction x = 0.x1x2 . . . , where xk ‚àà{0, 1}. This is a shortened way
to express that
x =
‚àû
‚àë
k=1
xk
2k .
The representation of x as binary fraction is in general not unique. For example,
1
2 = 0.10000 . . . ,
but also
1
2 = 0.01111 . . .
Check this by computing the infinite sums in both cases.
It is not difficult to prove that two different representations admit exactly those
x ‚àà[0, 1) which may be written as x = k/2n for some n ‚àà‚Ñïand some k = 1, 3, 5, . . . , 2n‚àí1.
Those numbers are usually called dyadic rational numbers.
To make the binary representation unique, we declare the following:
Convention 4.1. If a number x ‚àà[0, 1) admits the representations
x = 0.x1 . . . xn‚àí11000 . . .
and
x = 0.x1 . . . xn‚àí10111 . . . ,
then we always choose the former one. In other words, there do not exist numbers
x ‚àà[0, 1) whose binary representation consists only of 1s from a certain point onward.
How do we get the binary fraction for a given x ‚àà[0, 1)?
The procedure is not difficult. First, one checks whether x <
1
2 or x ‚â•
1
2. In the
former case, one takes x1 = 0 and in the latter x1 = 1.
With this choice, it follows that 0 ‚â§x ‚àíx1
2 < 1
2. In the next step, one asks whether
x ‚àíx1
2 < 1
4 or x ‚àíx1
2 ‚â•1
4. Depending on this, one chooses either x2 = 0 or x2 = 1. This

190
‡±™
4 Operations on random variables
choice implies 0 ‚â§x ‚àíx1
2 ‚àíx2
22 < 1
4, and if this difference belongs either to [0, 1
8) or [ 1
8, 1
4),
then x3 = 0 or x3 = 1, respectively. Proceeding further in this way leads to the binary
fraction representing x.
After this heuristic method, we now present a mathematically more exact way. To
this end, for each n ‚â•1, we divide the interval [0, 1) into 2n intervals of length 2‚àín.
We start with n = 1 and divide [0, 1) into two intervals,
I0 := [0 , 1
2 )
and
I1 := [1
2 , 1) .
In the second step, we divide each of the two intervals I0 and I1 further into two parts of
equal length. In this way, we obtain the four intervals
I00 := [0 , 1
4 ) ,
I01 := [ 1
4, 1
2 ) ,
I10 := [ 1
2 , 3
4 ),
and
I11 := [ 3
4, 1 ) .
Observe that the left end point of Ia1,a2 equals a1/2 + a2/4, that is,
Ia1a2 = [
2
‚àë
j=1
aj
2j ,
2
‚àë
j=1
aj
2j + 1
22 ),
a1, a2 ‚àà{0, 1} .
It is clear now how to proceed. Given n ‚â•1 and numbers a1, . . . , an ‚àà{0, 1}, set
Ia1...an = [
n
‚àë
j=1
aj
2j ,
n
‚àë
j=1
aj
2j + 1
2n ).
(4.4)
In this way, we obtain 2n disjoint intervals of length 2‚àín where the left corner points
are 0.a1a2 . . . an (see Figure 4.2 for the first dyadic intervals).
Figure 4.2: The first dyadic intervals of [0, 1).
The following lemma makes the above heuristic method more precise.
Lemma 4.3.1. For all a1, . . . , an ‚àà{0, 1}, the intervals in (4.4) are characterized by
Ia1...an = {x ‚àà[0, 1) : x = 0.a1a2 . . . an . . . } .
Verbally, a number in [0, 1) belongs to Ia1‚ãÖ‚ãÖ‚ãÖan if and only if its first n digits in the binary
fraction are a1, . . . , an.

4.3 Coin tossing versus uniform distribution
‡±™
191
Proof. Assume first x ‚ààIa1...an. If a := 0.a1 ‚ãÖ‚ãÖ‚ãÖan denotes the left end point of Ia1...an, by
definition a ‚â§x < a+ 1/2n or, equivalently, 0 ‚â§x‚àía < 1/2n. Therefore, the binary fraction
of x ‚àía is of the form 0.00 . . . 0bn+1 . . . with certain numbers bn+1, bn+2, . . . ‚àà{0, 1}. This
yields
x = a + (x ‚àía) = 0.a1 . . . anbn+1 . . .
Thus, as asserted, the first n digits in the representation of x are a1, . . . , an.
Conversely, if x can be written as x = 0.x1x2 . . . with x1 = a1, . . . , xn = an, then a ‚â§x
where, as above, a denotes the left end point of Ia1...an. Moreover, by Convention 4.1, at
least one of the xks, k > n, has to be zero. Consequently,
x ‚àía =
‚àû
‚àë
k=n+1
xk
2k <
‚àû
‚àë
k=n+1
1
2k = 1
2n ,
that is, we have a ‚â§x < a + 1
2n or, equivalently, x ‚ààIa1...an as asserted.
A direct consequence of Lemma 4.3.1 is as follows.
Corollary 4.3.2. For each n ‚â•1, the 2n sets Ia1...an form a disjoint partition of [0, 1), that is,
‚ãÉ
a1,...,an‚àà{0,1}
Ia1...an = [0, 1)
and
Ia1...an ‚à©Ia‚Ä≤
1...a‚Ä≤
n = 0
provided that (a1, . . . , an)
Ã∏= (a‚Ä≤
1, . . . , a‚Ä≤
n). Furthermore,
{x ‚àà[0, 1) : xk = 0} =
‚ãÉ
a1,...,ak‚àí1‚àà{0,1}
Ia1...ak‚àí10 .
4.3.2 Binary fractions of random numbers
We saw above each number x ‚àà[0, 1) admits a representation x = 0.x1x2 . . . with certain
xk ‚àà{0, 1}. What does happen if we choose a number x randomly, say according to the
uniform distribution on [0, 1]? Then the xks in the binary fraction are also random, with
values in {0, 1}. How are they distributed?
The mathematical formulation of this question is as follows: let U : Œ© ‚Üí‚Ñùbe a
random variable uniformly distributed on [0, 1]. If œâ ‚ààŒ©, write3
U(œâ) = 0.X1(œâ)X2(œâ) . . . =
‚àû
‚àë
k=1
Xk(œâ)
2k
.
(4.5)
In this way, we obtain infinitely many random variables Xk : Œ© ‚Üí{0, 1}.
3 Note that ‚Ñô{U ‚àà[0, 1)} = 1. Thus, without losing generality, we may assume U(œâ) ‚àà[0, 1).

192
‡±™
4 Operations on random variables
Which distribution do these random variables possess? Answer gives the next
proposition.
Proposition 4.3.3. If k ‚àà‚Ñï, then
‚Ñô{Xk = 0} = ‚Ñô{Xk = 1} = 1
2 .
(4.6)
Furthermore, given n ‚â•1, the random variables X1, . . . , Xn are independent.
Proof. By assumption, ‚ÑôU is the uniform distribution on [0, 1]. Thus, the finite additivity
of ‚ÑôU, Corollary 4.3.2 and eq. (1.46) imply
‚Ñô{Xk = 0} = ‚ÑôU(
‚ãÉ
a1,...,ak‚àí1‚àà{0,1}
Ia1...ak‚àí10)
=
‚àë
a1,...,ak‚àí1‚àà{0,1}
‚ÑôU(Ia1...ak‚àí10) =
‚àë
a1,...,ak‚àí1‚àà{0,1}
1
2k = 2k‚àí1
2k
= 1
2 .
Since Xk attains only two different values, ‚Ñô{Xk = 1} = 1/2 as well, proving the first part.
We want to verify that for all n ‚â•1 the random variables X1, . . . , Xn are indepen-
dent. Equivalently, according to Proposition 3.6.18, the following has to be proven: if
a1, . . . , an ‚àà{0, 1}, then
‚Ñô{X1 = a1, . . . , Xn = an} = ‚Ñô{X1 = a1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn = an} .
(4.7)
By eq. (4.6), the right-hand side of eq. (4.7) equals
‚Ñô{X1 = a1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn = an} = 1
2 ‚ãÖ‚ãÖ‚ãÖ1
2
‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
n
= 1
2n .
To compute the left-hand side of eq. (4.7), note that Lemma 4.3.1 implies that we have
X1 = a1 up to Xn = an if and only if U attains a value in Ia1...an. The intervals Ia1...an are of
length 2‚àín, hence by eq. (1.46) (recall that ‚ÑôU is the uniform distribution on [0, 1]),
‚Ñô{X1 = a1, . . . , Xn = an} = ‚Ñô{U ‚ààIa1...an} = ‚ÑôU(Ia1...an) = 1
2n .
Thus, for all a1, . . . , an ‚àà{0, 1}, eq. (4.7) is valid, and, as asserted, the random variables
X1, . . . , Xn are independent.
To formulate the previous result in a different way, let us introduce the following
notation.
Definition 4.3.4. An infinite sequence X1, X2, . . . of random variables is said to be independent provided
that any finite collection of the Xks is independent.

4.3 Coin tossing versus uniform distribution
‡±™
193
Remark 4.3.5. Since any subcollection of independent random variables is independent
as well, the independence of X1, X2, . . . is equivalent to the following. For all n ‚â•1, the
random variables X1, . . . , Xn are independent, that is, for all n ‚â•1 and all Borel sets
B1, . . . , Bn, it follows that
‚Ñô{X1 ‚ààB1, . . . , Xn ‚ààBn} = ‚Ñô{X1 ‚ààB1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn ‚ààBn} .
Remark 4.3.6. In view of Definition 4.3.4, the basic observation in Example 3.6.19 may
now be formulated in the following way. If we toss a (maybe biased) coin, labeled with
‚Äú0‚Äù and ‚Äú1,‚Äù infinitely often and let X1, X2, . . . be the results of the single tosses, then
this infinite sequence of random variables is independent with ‚Ñô{Xk = 0} = 1 ‚àíp and
‚Ñô{Xk = 1} = p. In particular, for a fair coin, the Xks possess the following properties:
1.
If k ‚àà‚Ñï, then ‚Ñô{Xk = 0} = ‚Ñô{Xk = 1} = 1/2.
2.
X1, X2, . . . is an infinite sequence of independent random variables.
This observation leads us to the following definition.
Definition 4.3.7. An infinite sequence X1, X2, . . . of independent random variables with values in {0, 1}
satisfying
‚Ñô{Xk = 0} = ‚Ñô{Xk = 1} = 1/2 ,
k = 1, 2, . . . ,
is said to be a model for tossing a fair coin infinitely often.
Consequently, Proposition 4.3.3 asserts that the random variables X1, X2, . . . defined by
eq. (4.5) serve as a model for tossing a fair coin infinitely often.
Example 4.3.8. Suppose a randomly chosen number in [0, 1) is x = 0.1657432763. Its bi-
nary expansion is 0.0010101001101110001001101011111100111101100010 . . . Thus, translat-
ing this into a result of tossing a coin, the first observations are 0, 0, 1, 0, 1, . . . Of course,
since x is only approximately chosen uniformly, also only the first finitely many digits
of the binary expansion simulate the tossing of a fair coin.
Summary: Let U be some random variable distributed according to the uniform distribution on [0, 1]. Rep-
resent the values of U as binary fraction 0.X1X2 . . . where the Xjs attain values in {0, 1}. Then the random
variables Xjs are independent with ‚Ñô{Xj = 0} = ‚Ñô{Xj = 1} = 1
2. Thus, in order to generate an infinite inde-
pendent sequence of equiprobable zeroes and ones, choose a number uniformly distributed on [0, 1] and
expand it as a binary fraction.
4.3.3 Random numbers generated by coin tossing
We saw in Proposition 4.3.3 that choosing a random number in [0, 1] leads to a model
for tossing a fair coin infinitely often. Our aim is now to investigate the opposite ques-
tion. That is, we are given an infinite random sequence of zeroes and ones and want to

194
‡±™
4 Operations on random variables
construct a uniformly distributed number in [0, 1]. The precise mathematical question
is as follows: suppose we are given an infinite sequence (Xk)k‚â•1 of independent random
variables with
‚Ñô{Xk = 0} = ‚Ñô{Xk = 1} = 1/2 ,
k = 1, 2, . . .
(4.8)
Is it possible to construct from these Xks a uniformly distributed U? The next propo-
sition answers this question to the affirmative.
Proposition 4.3.9. Let X1, X2, . . . be an arbitrary sequence of independent random vari-
ables satisfying eq. (4.8). If U is defined by
U(œâ) :=
‚àû
‚àë
k=1
Xk(œâ)
2k
,
œâ ‚ààŒ© ,
then this random variable is uniformly distributed on [0, 1].
Proof. In order to prove that U is uniformly distributed on [0, 1], we have to show that,
if t ‚àà[0, 1), then
‚Ñô{U ‚â§t} = t .
(4.9)
We start the proof of eq. (4.9) with the following observation: suppose the binary
fraction of some t ‚àà[0, 1) is 0.t1t2 . . . for certain ti ‚àà{0, 1}. If s = 0.s1s2 . . . , then s < t if
and only if there is an n ‚àà‚Ñïso that the following is satisfied:4
s1 = t1, . . . , sn‚àí1 = tn‚àí1 ,
sn = 0,
and
tn = 1 .
Fix t ‚àà[0, 1) for a moment and set
An(t) := {s ‚àà[0, 1) : s1 = t1, . . . , sn‚àí1 = tn‚àí1, sn < tn} .
Of course, An(t) ‚à©Am(t) = 0 whenever n
Ã∏= m and, moreover, An(t)
Ã∏= 0 if and only if
tn = 1. Furthermore, by the previous remark
[0, t) =
‚àû
‚ãÉ
n=1
An(t) =
‚ãÉ
{n:tn=1}
An(t) .
Finally, if An(t)
Ã∏= 0, that is, if tn = 1, then
‚Ñô{U ‚ààAn(t)} = ‚Ñô{X1 = t1, . . . , Xn‚àí1 = tn‚àí1, Xn = 0}
= ‚Ñô{X1 = t1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn‚àí1 = tn‚àí1} ‚ãÖ‚Ñô{Xn = 0} = 1
2n .
4 In the case n = 1, this says s1 = 0 and t1 = 1.

4.3 Coin tossing versus uniform distribution
‡±™
195
In the last step, we used both properties of the Xks, that is, they are independent and
satisfy ‚Ñô{Xk = 0} = ‚Ñô{X = 1} = 1/2.
Summing up, we get
‚Ñô{U < t} = ‚Ñô{U ‚àà
‚ãÉ
{n:tn=1}
An(t)} =
‚àë
{n:tn=1}
‚Ñô{U ‚ààAn(t)}
=
‚àë
{n:tn=1}
1
2n =
‚àû
‚àë
n=1
tn
2n = t .
This ‚Äúalmost‚Äù proves eq. (4.9). It remains to show that ‚Ñô{U < t} = ‚Ñô{U ‚â§t} or, equiva-
lently, ‚Ñô{U = t} = 0. To verify this, we use the continuity of ‚Ñôfrom above. Then
‚Ñô{U = t} = ‚Ñô{X1 = t1, X2 = t2, . . .}
= lim
n‚Üí‚àû‚Ñô{X1 = t1, . . . , Xn = tn} = lim
n‚Üí‚àû
1
2n = 0 .
Consequently, eq. (4.9) holds for all t ‚àà[0, 1) and, as asserted, U is uniformly distributed
on [0, 1].
Remark 4.3.10. Another possibility to write U is as binary fraction
U(œâ) = 0.X1(œâ)X2(œâ) . . . ,
œâ ‚ààŒ© .
Consequently, in order to construct a random number u in [0, 1] one may proceed as
follows: toss a fair coin with ‚Äú0‚Äù and ‚Äú1‚Äù infinitely often and take the obtained sequence
as binary fraction of u. The u obtained in this way is uniformly distributed on [0, 1].
Of course, in practice one tosses a coin not infinitely often. One stops the procedure
after N trials for some ‚Äúlarge‚Äù N. In this way, one gets a number u, which is ‚Äúalmost‚Äù
uniformly distributed on [0, 1].
Example 4.3.11. Suppose tossing a fair coin led to the sequence 0, 1, 1, 0, 0, 1, 0, 1. Then
the generated number u ‚àà[0, 1] equals u = 0.39453125. In the same way, the sequence
1, 1, 1, 0, 1, 1, 0, 1 when tossing leads to 0.92578125 as a randomly chosen number in [0, 1].
Then how does one construct n independent numbers u1, . . . , un, all uniformly dis-
tributed on [0, 1]? The answer is quite obvious. Take n coins and toss them. As functions
of independent observations the generated u1, . . . , un are independent as well and, by
construction, each of these numbers is uniformly distributed on [0, 1]. Another way is to
toss the same coin n times ‚Äúinfinitely often,‚Äù thus getting n infinite sequences of zeroes
and ones.
Summary: If X1, X2, . . . is an arbitrary sequence of independent random variables such that ‚Ñô{Xj = 0} =
‚Ñô{Xj = 1} =
1
2, then U = ‚àë‚àû
k=1
Xk
2k = 0.X1X2 . . . is uniformly distributed on [0, 1]. Thus, in order to obtain
a number u uniformly distributed on [0, 1], toss a fair coin ‚Äúinfinitely‚Äù often and define u as binary fraction
according to the observed sequence of zeroes and ones.

196
‡±™
4 Operations on random variables
4.4 Simulation of random variables
Proposition 4.3.9 provides us with a technique to simulate a uniformly distributed ran-
dom variable U by tossing a fair coin. The aim of this section is to find a suitable function
f : [0, 1] ‚Üí‚Ñù, so that the transformed random variable X = f (U) possesses a given prob-
ability distribution.
Remark 4.4.1. Typical questions of this kind are as follows:
‚Äì
Find a function f so that X = f (U) is standard normally distributed.
‚Äì
Does there exist a function g : [0, 1] ‚Üí‚Ñùfor which g(U) is Bn,p-distributed?
Suppose for a moment we already found such functions f and g. According to Re-
mark 4.3.10, we construct independent numbers u1, . . . , un, uniformly distributed on
[0, 1], and set xi = f (ui) and yi = g(ui). In this way, we get either n standard normally
distributed numbers x1, . . . , xn or n binomial distributed numbers y1, . . . , yn. Moreover,
by Proposition 4.1.9, these numbers are independent. In this way, we may simulate in-
dependent random numbers possessing a given probability distribution.
We start with simulating discrete random variables. Thus suppose we are given real
numbers x1, x2, . . . and pk ‚â•0 with ‚àë‚àû
k=1 pk = 1, and we look for a random variable
X = f (U) such that
‚Ñô{X = xk} = pk ,
k = 1, 2, . . .
One possible way to find such a function f is as follows: divide [0, 1) into disjoint intervals
I1, I2, . . . of length |Ik| = pk where k = 1, 2, . . . Since ‚àë‚àû
k=1 pk = 1, such intervals exist. For
example, take I1 = [0, p1) and
Ik = [
k‚àí1
‚àë
i=1
pi ,
k
‚àë
i=1
pi) ,
k = 2, 3, . . .
With these intervals Ik, we define f : [0, 1] ‚Üí‚Ñùby
f (x) := xk
if x ‚ààIk ,
(4.10)
or, equivalently,
f (x) =
‚àû
‚àë
k=1
xk 1Ik(x) .
(4.11)
Then the following is true.
Proposition 4.4.2. Let U be uniformly distributed on [0, 1], and set X = f (U) with f de-
fined by eq. (4.10) or eq. (4.11). Then
‚Ñô{X = xk} = pk ,
k = 1, 2, . . .

4.4 Simulation of random variables
‡±™
197
Proof. Using that U is uniformly distributed on [0, 1], this is an easy consequence of
eq. (1.46) in view of
‚Ñô{X = xk} = ‚Ñô{f (U) = xk} = ‚Ñô{U ‚ààIk} = |Ik| = pk .
Remark 4.4.3. Note that the concrete shape of the intervals5 Ik is not important at all.
They only have to satisfy |Ik| = pk for all k = 1, 2, . . . Moreover, these intervals need
not necessarily be disjoint; a ‚Äúsmall‚Äù overlap does not influence the assertion. Indeed,
it suffices that ‚Ñô{U ‚ààIk ‚à©Il} = 0 whenever k
Ã∏= l. For example, if always |Ik ‚à©Il| < ‚àû,
k
Ã∏= l, then the construction works as well. In particular, we may choose also I1 = [0, p1]
and Ik = [‚àëk‚àí1
i=1 pi , ‚àëk
i=1 pi] if k ‚â•2.
Example 4.4.4. We want to simulate a random variable X, which is uniformly dis-
tributed on {x1, . . . , xN}. How to proceed?
Answer: Divide the interval [0, 1) into N intervals I1, . . . , IN of length 1
N . For exam-
ple, choose Ik := [ k‚àí1
N , k
N ), k = 1, . . . , N. If f = ‚àëN
k=1 xk 1Ik, then X = f (U) is uniformly
distributed on {x1, . . . , xN}.
Example 4.4.5. Suppose we want to simulate a number k
‚àà
‚Ñï0, which is PoisŒª-
distributed. Set
Ik := [
k‚àí1
‚àë
j=0
Œªj
j! e‚àíŒª,
k
‚àë
j=0
Œªj
j! e‚àíŒª) ,
k = 0, 1, . . . ,
where the left-hand sum is supposed to be zero if k = 0. Choose randomly a number
u ‚àà[0, 1] and take the k with u ‚ààIk. Then k is the number we are interested in.
Example 4.4.6. Finally, let us simulate Bn,p-distributed random numbers. One possible
way is to divide [0, 1) into n + 1 disjoint intervals as follows:
Ik = [
k‚àí1
‚àë
j=0
(n
j )pj(1 ‚àíp)n‚àíj,
k
‚àë
j=0
(n
j )pj(1 ‚àíp)n‚àíj) ,
k = 0, . . . , n .
If k = 0, the left-hand sum is taken as zero, that is, I0 = [0, (1 ‚àíp)n). Next choose a
random number u uniformly distributed on [0, 1]. For example, use the technique pre-
sented in Remark 4.3.10. If this u ‚ààIk, then k ‚àà{0, . . . , n} is the desired Bn,p-distributed
integer.
For instance, if p = 1/2 and n = 4, the five intervals are
I0 = [0, 1
16),
I1 = [ 1
16, 5
16),
I2 = [ 5
16, 11
16),
I3 = [ 11
16, 15
16),
I4 = [15
16, 1).
5 They do not even need to be intervals.

198
‡±™
4 Operations on random variables
After fixing the intervals, let us sufficiently often toss a fair coin labeled with ‚Äú0‚Äù and
‚Äú1.‚Äù Say we observed the sequence 0, 1, 1, 0, 1, 0. Since the dyadic number 0.011010 equals
u = 0.40625 in decimal representation, one notes that u ‚ààI2. Thus, the randomly chosen
number is k = 2. It is distributed according to B4,0.5. Which number k ‚àà{0, 1, 2, 3, 4} do
we select if we toss 1, 1, 0, 1, 1, 1?
Our next aim is to simulate continuous random variables. More precisely, suppose
we are given a probability density p. Then we look for a function f : [0, 1] ‚Üí‚Ñùsuch that
p is the density of X = f (U), that is, we have to have
‚Ñô{f (U) ‚â§t} = ‚Ñô{X ‚â§t} =
t
‚à´
‚àí‚àû
p(x) dx ,
t ‚àà‚Ñù.
(4.12)
To this end, set
F(t) =
t
‚à´
‚àí‚àû
p(x) dx ,
t ‚àà‚Ñù.
(4.13)
Thus, F is the distribution function of the random variable X, which we are going to
construct.
Suppose first that F is one-to-one on a finite or infinite interval (a, b), so that F(x) = 0
if x < a, and F(x) = 1 if x > b. For example, this is valid if p(t) > 0 for all t ‚àà(a, b). Since
F is continuous, the inverse function F‚àí1 exists and maps (0, 1) to (a, b).
Proposition 4.4.7. Let p be a probability density and define F by eq. (4.12). Suppose F sat-
isfies the above condition. If X = F‚àí1(U), then
‚Ñô{X ‚â§t} =
t
‚à´
‚àí‚àû
p(x) dx ,
t ‚àà‚Ñù,
that is, p is a density of X.
Proof. First note that the assumptions about F imply that it is increasing on (a, b). Hence,
if t ‚àà‚Ñù, then
‚Ñô{X ‚â§t} = ‚Ñô{F‚àí1(U) ‚â§t} = ‚Ñô{U ‚â§F(t)} = F(t) =
t
‚à´
‚àí‚àû
p(x) dx ,
t ‚àà‚Ñù.
Here we used 0 ‚â§F(t) ‚â§1 and ‚Ñô{U ‚â§s} = s whenever 0 ‚â§s ‚â§1. This completes the
proof.

4.4 Simulation of random variables
‡±™
199
But what do we do if F does not satisfy the above assumption? For example, this
happens if p(x) = 0 on an interval I = (Œ±, Œ≤) and p(x) > 0 on some left- and right-hand
intervals6 of I. In this case F‚àí1 does not exist, and we have to modify the construction.7
Definition 4.4.8. Let F be defined by eq. (4.13). Then we set
F‚àí(s) = inf{t ‚àà‚Ñù: F(t) = s} ,
0 ‚â§s < 1 .
The function F‚àí, mapping [0, 1) to [‚àí‚àû, ‚àû), is called the pseudoinverse of F.
Remark 4.4.9. If 0 < s < 1, then F‚àí(s) ‚àà‚Ñùwhile F‚àí(0) = ‚àí‚àû. Moreover, if F is increas-
ing on some interval I, then F‚àí(s) = F‚àí1(s) for s ‚ààI.
Lemma 4.4.10. The pseudoinverse function F‚àípossesses the following properties:
1.
If s ‚àà(0, 1) and t ‚àà‚Ñù, then
F(F‚àí(s)) = s
and
F‚àí(F(t)) ‚â§t .
2.
Given t ‚àà(0, 1), we have
F‚àí(s) ‚â§t
‚áê‚áí
s ‚â§F(t) .
(4.14)
Proof. The equality F(F‚àí(s)) = s is a direct consequence of the continuity of F. Indeed,
if there are tn ‚ÜòF‚àí(s) with F(tn) = s, then
s = lim
n‚Üí‚àûF(tn) = F(F‚àí(s)) .
The second part of the first assertion follows by the definition of F‚àí.
Now let us come to the proof of property (4.14). If F‚àí(s) ‚â§t, then the monotonicity
of F and F(F‚àí(s)) = s lead to s = F(F‚àí(s)) ‚â§F(t).
Conversely, if s ‚â§F(t), then F‚àí(s) ‚â§F‚àí(F(t)) ‚â§t by the first part, thus, property
(4.14) is proved.
Now choose a uniformly distributed U and set X = F‚àí(U). Since ‚Ñô{U = 0} = 0, we
may assume that X attains values in ‚Ñù.
Proposition 4.4.11. Let p be a probability density, that is, we have p(x)
‚â•
0 and
‚à´
‚àû
‚àí‚àûp(x)dx = 1. Define F by eq. (4.13) and let F‚àíbe its pseudoinverse. Take U to be
uniformly distributed on [0, 1] and set X = F‚àí(U). Then p is a distribution density of the
random variable X.
6 Take, for instance, p with p(x) = 1
2 if x ‚àà[0, 1] and if x ‚àà[1, 2], and p(x) = 0 otherwise.
7 All subsequent distribution functions F possess an inverse function on a suitable interval (a, b). Thus,
Proposition 4.4.7 applies in almost all cases of interest. Therefore, if the statements about pseudoinverse
functions look too complicated, you may skip them.

200
‡±™
4 Operations on random variables
Proof. Using property (4.14), it follows that
FX(t) = ‚Ñô{X ‚â§t} = ‚Ñô{œâ ‚ààŒ© : F‚àí(U(œâ)) ‚â§t}
= ‚Ñô{œâ ‚ààŒ© : U(œâ) ‚â§F(t)} = F(t),
which completes the proof.
Remark 4.4.12. Since F‚àí= F‚àí1 whenever the inverse function exists, Proposition 4.4.7
is a special case of Proposition 4.4.11.
Example 4.4.13. Let us simulate an ùí©(0, 1)-distributed random variable, that is, we are
looking for a function f : (0, 1) ‚Üí‚Ñùsuch that for a uniformly distributed U,
‚Ñô{f (U) ‚â§t} =
1
‚àö2œÄ
t
‚à´
‚àí‚àû
e‚àíx2/2 dx ,
t ‚àà‚Ñù.
The distribution function
Œ¶(t) =
1
‚àö2œÄ
t
‚à´
‚àí‚àû
e‚àíx2/2 dx
is one-to-one from ‚Ñù‚Üí(0, 1), hence Proposition 4.4.7 applies, and Œ¶‚àí1(U) is a standard
normal random variable.
How does one get an ùí©(Œº, œÉ2)-distributed random variable? If X is standard nor-
mal, by Proposition 4.2.3 the transformed variable œÉX +Œº is ùí©(Œº, œÉ2)-distributed. Conse-
quently, œÉŒ¶‚àí1(U) + Œº possesses the desired distribution.
How do we find n independent ùí©(Œº, œÉ2)-distributed numbers x1, . . . , xn? To achieve
this, choose u1, . . . , un in [0, 1] according to the construction presented in Remark 4.3.10
and set xi = œÉ Œ¶‚àí1(ui) + Œº, 1 ‚â§i ‚â§n.
Example 4.4.14. Our next aim is to simulate an EŒª-distributed (exponentially dis-
tributed) random variable. Here
F(t) = {0
if t ‚â§0,
1 ‚àíe‚àíŒªt
if t > 0,
which satisfies the assumptions of Proposition 4.4.7 on the interval (0, ‚àû). Its inverse
F‚àí1 maps (0, 1) to (0, ‚àû) and equals
F‚àí1(s) = ‚àíln(1 ‚àís)
Œª
,
0 < s < 1 .
Therefore, if U is uniformly distributed on [0, 1], then X = ‚àíln(1‚àíU)
Œª
is EŒª-distributed. This
is true for any uniformly distributed random variable U. By Corollary 4.2.7, the random
variable 1 ‚àíU has the same distribution as U, hence, setting

4.4 Simulation of random variables
‡±™
201
Y = ‚àíln(1 ‚àí(1 ‚àíU))
Œª
= ‚àíln(U)
Œª
,
the random variable Y is EŒª-distributed as well.
Example 4.4.15. Let us simulate a random variable with Cauchy distribution (see Defi-
nition 1.6.37). The distribution function F is given by
F(t) = 1
œÄ
t
‚à´
‚àí‚àû
1
1 + x2 dx = 1
œÄ arctan(t) + 1
2 ,
t ‚àà‚Ñù,
hence X := tan(œÄU ‚àíœÄ
2 ) possesses a Cauchy distribution.
Example 4.4.16. Finally, let us give an example where Proposition 4.4.11 applies and
Proposition 4.4.7 does not. Suppose we want to simulate a random variable X with dis-
tribution function F defined by
F(t) =
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
0
if t < 0,
t
2
if 0 ‚â§t < 1,
1
2
if 1 ‚â§t < 2,
1
2 + t‚àí2
2
if 2 ‚â§t < 3,
1
if t ‚â•3.
(4.15)
Direct computations imply (see Figure 4.3) that
F‚àí(s) =
{
{
{
{
{
{
{
2s
if 0 < s < 1
2,
1
if s = 1
2,
2s + 1
if 1
2 < s ‚â§1 .
Figure 4.3: The functions F and F‚àíin Example 4.4.16.
Hence, if X is defined by
X = F‚àí(U) = 2U 1[0, 1
2 ](U) + (2U + 1) 1( 1
2 ,1](U) ,

202
‡±™
4 Operations on random variables
then ‚Ñô{X ‚â§t} = F(t) with F defined by eq. (4.15). In other words, X is acting as follows.
Choose by random a number u ‚àà[0, 1]. If u ‚â§1
2, then X(u) = 2u, while for u > 1
2 we take
X(u) = 2u + 1.
Summary: Let ‚Ñôbe a probability measure on the Borel sets of ‚Ñù. In order to simulate n independent num-
bers x1, . . . , xn distributed according to ‚Ñô, one proceeds in the following way. In the first step, one constructs n
independent numbers u1, . . . , un uniformly distributed on [0, 1]. To this end, one tosses a fair coin sufficiently
often as explained in Remark 4.3.10.
The second step is then as follows: If ‚Ñôis discrete, one defines the numbers x1, . . . , xn by xj = f(uj) with
the function f constructed in (4.11). The probability that one (or each) xj belongs to a set B equals ‚Ñô(B). In
case of continuous ‚Ñô, define xj as F‚àí1(uj) provided the distribution function F of ‚Ñôis invertible. Otherwise
replace F‚àí1 by the pseudoinverse F‚àíintroduced in Definition 4.4.8. As before, the xjs are independent and
distributed according to the given ‚Ñô.
4.5 Addition of random variables
Suppose we are given two random variables X and Y, both mapping from Œ© into ‚Ñù. As
usual, their sum X + Y is defined by
(X + Y)(œâ) := X(œâ) + Y(œâ) ,
œâ ‚ààŒ© .
The main question we investigate in this section is as follows: suppose we know the
probability distributions of X and Y. Is there a way to compute the distribution of X +Y?
For example, if we roll a die twice, X is the result of the first roll, Y that of the second,
then we know ‚ÑôX and ‚ÑôY. But how do we get ‚ÑôX+Y?
Before we treat this question, we have to be sure that X+Y is also a random variable.
This is not obvious at all. Otherwise, the probability distribution of X + Y is not defined
and our question does not make sense.
Proposition 4.5.1. If X and Y are random variables, then so is X + Y.
Proof. We start the proof with the following observation. For two real numbers a and b,
one has a < b if and only if there is a rational number q ‚àà‚Ñösuch that a < q and b > q.
Therefore, given t ‚àà‚Ñù, it follows that
{œâ ‚ààŒ© : X(œâ) + Y(œâ) < t} = {œâ ‚ààŒ© : X(œâ) < t ‚àíY(œâ)}
= ‚ãÉ
q‚àà‚Ñö
[{œâ : X(œâ) < q} ‚à©{œâ : q < t ‚àíY(œâ)}] .
(4.16)
By assumption, X and Y are random variables. Hence, for each q ‚àà‚Ñö,
Aq := {œâ : X(œâ) < q} ‚ààùíú
and
Bq := {œâ : Y(œâ) < t ‚àíq} ‚ààùíú,

4.5 Addition of random variables
‡±™
203
which, by the properties of œÉ-fields, implies Cq := Aq ‚à©Bq ‚ààùíú. With this notation, we
may write eq. (4.16) as
{œâ ‚ààŒ© : X(œâ) + Y(œâ) < t} = ‚ãÉ
q‚àà‚Ñö
Cq .
The œÉ-field ùíúis closed under taking countable unions, thus, since ‚Ñöis countably infinite
and Cq ‚ààùíú, it follows that ‚ãÉq‚àà‚ÑöCq ‚ààùíú. Therefore, we have proven that, if t ‚àà‚Ñù, then
{œâ ‚ààŒ© : X(œâ) + Y(œâ) < t} ‚ààùíú.
Proposition 3.1.6 lets us conclude that, as asserted, X + Y is a random variable.
Remark 4.5.2. In view of Proposition 4.5.1, the following question makes sense: does
there exist a general approach to evaluate ‚ÑôX+Y by virtue of ‚ÑôX and of ‚ÑôY?
Answer: Such a general way does not exist. The deeper reason behind this is that,
in order to get ‚ÑôX+Y, one has to know the joint distribution of (X, Y). And as we saw in
Section 3.5, in general, the knowledge of ‚ÑôX and ‚ÑôY does not suffice to determine their
joint distribution, hence generally we also do not know ‚ÑôX+Y.
The next example emphasizes the previous remark.
Example 4.5.3. Let X, Y, X‚Ä≤, and Y ‚Ä≤ be as in Example 3.5.8, that is, we choose two balls
out of an urn where two balls are labeled by ‚Äú0‚Äù and two by ‚Äú1,‚Äù once without replacing
the first ball and once with replacing. The joint distributions of (X, Y) and (X‚Ä≤, Y ‚Ä≤) are
‚Ñô{X = 0, Y = 0} = 1
6 ,
‚Ñô{X = 0, Y = 1} = 1
3,
‚Ñô{X = 1, Y = 0} = 1
3 ,
‚Ñô{X = 1, Y = 1} = 1
6,
‚Ñô{X‚Ä≤ = 0, Y ‚Ä≤ = 0} = 1
4 ,
‚Ñô{X‚Ä≤ = 0, Y ‚Ä≤ = 1} = 1
4,
‚Ñô{X‚Ä≤ = 1, Y ‚Ä≤ = 0} = 1
4 ,
‚Ñô{X‚Ä≤ = 1, Y ‚Ä≤ = 1} = 1
4 .
Then ‚ÑôX = ‚ÑôX‚Ä≤ and ‚ÑôY = ‚ÑôY ‚Ä≤, but
‚Ñô{X + Y = 0} = 1
6 ,
‚Ñô{X + Y = 1} = 2
3,
and
‚Ñô{X + Y = 2} = 1
6 ,
while
‚Ñô{X‚Ä≤ + Y ‚Ä≤ = 0} = 1
4 ,
‚Ñô{X‚Ä≤ + Y ‚Ä≤ = 1} = 1
2,
and
‚Ñô{X‚Ä≤ + Y ‚Ä≤ = 2} = 1
4 .
Consequently, if we do not replace the chosen ball, the probability that the sum of both
choices equals ‚Äú1‚Äù is 2/3 while it is 1/2 if we replace the first ball. And this happens al-
though in both cases the numbers ‚Äú0‚Äù and ‚Äú1‚Äù occur every time with probability 1/2.

204
‡±™
4 Operations on random variables
On the other hand, as we saw in Proposition 3.6.5, the joint distribution is uniquely
determined by the marginal ones, provided the random variables are independent.
Therefore, for independent random variables X and Y, the distribution of X + Y is
determined by those of X and Y. The question remains, how ‚ÑôX+Y can be computed.
4.5.1 Sums of discrete random variables
We first consider an important special case, namely that X and Y attain values in ‚Ñ§.
Here we have
Proposition 4.5.4 (Convolution formula for ‚Ñ§-valued random variables). Let X and Y be
two independent random variables with values in ‚Ñ§. If k ‚àà‚Ñ§, then
‚Ñô{X + Y = k} =
‚àû
‚àë
i=‚àí‚àû
‚Ñô{X = i} ‚ãÖ‚Ñô{Y = k ‚àíi} .
Proof. Fix k ‚àà‚Ñ§and define Bk ‚äÜ‚Ñ§√ó ‚Ñ§by
Bk := {(i, j) ‚àà‚Ñ§√ó ‚Ñ§: i + j = k} .
Then we get
‚Ñô{X + Y = k} = ‚Ñô{(X, Y) ‚ààBk} = ‚Ñô(X,Y)(Bk)
(4.17)
with joint distribution ‚Ñô(X,Y). Proposition 3.6.11 asserts that for independent X and Y
and B ‚äÜ‚Ñ§√ó ‚Ñ§,
‚Ñô(X,Y)(B) = ‚àë
(i,j)‚ààB
‚ÑôX({i}) ‚ãÖ‚ÑôY({j}) = ‚àë
(i,j)‚ààB
‚Ñô{X = i} ‚ãÖ‚Ñô{Y = j} .
We apply this formula with B = Bk and, from eq. (4.17), obtain
‚Ñô{X + Y = k} =
‚àë
(i,j)‚ààBk
‚Ñô{X = i} ‚ãÖ‚Ñô{Y = j}
=
‚àë
{(i,j) : i+j=k}
‚Ñô{X = i} ‚ãÖ‚Ñô{Y = j}
=
‚àû
‚àë
i=‚àí‚àû
‚Ñô{X = i} ‚ãÖ‚Ñô{Y = k ‚àíi} ,
as asserted.
Example 4.5.5. Two independent random variables X and Y are distributed according
to ‚Ñô{X = j} = ‚Ñô{Y = j} = 1/2j, j = 1, 2, . . . Determine the probability distribution of X ‚àíY.

4.5 Addition of random variables
‡±™
205
Solution: First note that ‚Ñô{X = j} = ‚Ñô{Y = j} = 0 for j ‚â§0. Hence, given k ‚àà‚Ñ§, an
application of Proposition 4.5.4 to X and ‚àíY yields
‚Ñô{X ‚àíY = k} =
‚àû
‚àë
i=‚àí‚àû
‚Ñô{X = i} ‚ãÖ‚Ñô{‚àíY = k ‚àíi} =
‚àû
‚àë
i=1
‚Ñô{X = i} ‚ãÖ‚Ñô{Y = i ‚àík} .
If k ‚â•0, then ‚Ñô{Y = i ‚àík} = 0 for i ‚â§k, thus
‚Ñô{X ‚àíY = k} =
‚àû
‚àë
i=k+1
‚Ñô{X = i} ‚ãÖ‚Ñô{Y = i ‚àík} =
‚àû
‚àë
i=k+1
1
2i ‚ãÖ
1
2i‚àík
= 2k
‚àû
‚àë
i=k+1
1
22i = 2k ‚ãÖ2‚àí2k‚àí2 ‚ãÖ
‚àû
‚àë
i=0
1
22i = 2‚àík‚àí2 ‚ãÖ4
3 = 2‚àík
3 .
For k < 0, it follows that
‚Ñô{X ‚àíY = k} =
‚àû
‚àë
i=1
1
2i ‚ãÖ
1
2i‚àík = 2k
‚àû
‚àë
i=1
1
22i = 2k
‚àû
‚àë
i=1
1
4i = 2k
3 .
We combine both cases and obtain
‚Ñô{X ‚àíY = k} = 2‚àí|k|
3
,
k ‚àà‚Ñ§.
Which random experiment does X ‚àíY describe? Suppose player A and B both toss a fair
coin. Let X be the number of necessary trials for A to observe the first ‚Äúheads.‚Äù Similarly,
Y describes how often B has to toss his coin to get the first ‚Äúheads.‚Äù Thus, the value of
X ‚àíY tells us how many trials later (or earlier if X ‚àíY is negative) player A got his first
‚Äúheads‚Äù compared to B.
For example, if B got his first ‚Äúheads‚Äù one trial earlier than A, then X ‚àíY = 1. The
probability that this occurs equals 1/6.
One special case of Proposition 4.5.4 is of particular interest.
Proposition 4.5.6 (Convolution formula for ‚Ñï0-valued random variables). Let X and Y be
two independent random variables with values in ‚Ñï0. If k ‚àà‚Ñï0, then it follows that
‚Ñô{X + Y = k} =
k
‚àë
i=0
‚Ñô{X = i} ‚ãÖ‚Ñô{Y = k ‚àíi} .
Proof. Regard X and Y as ‚Ñ§-valued random variables with ‚Ñô{X = i} = ‚Ñô{Y = i} = 0 for
all i = ‚àí1, ‚àí2, . . . If k ‚àà‚Ñï0, then Proposition 4.5.4 lets us conclude that
‚Ñô{X + Y = k} =
‚àû
‚àë
i=‚àí‚àû
‚Ñô{X = i} ‚ãÖ‚Ñô{Y = k ‚àíi} =
k
‚àë
i=0
‚Ñô{X = i} ‚ãÖ‚Ñô{Y = k ‚àíi} .

206
‡±™
4 Operations on random variables
Here we used ‚Ñô{X = i} = 0 for i < 0 and ‚Ñô{Y = k ‚àíi} = 0 if i > k. For k < 0, it follows
that ‚Ñô{X + Y = k} = 0 because in this case ‚Ñô{Y = k ‚àíi} = 0 for all i ‚â•0. This completes
the proof.
Example 4.5.7. Let X and Y be two independent random variables, both uniformly dis-
tributed on {1, 2, . . . , N}. Which probability distribution does X + Y possess?
Answer: Of course, X + Y attains only values in the set {2, 3, . . . , 2N}. Hence,
‚Ñô{X + Y = k} is only of interest for 2 ‚â§k ‚â§2N. Here we get
‚Ñô{X + Y = k} = |Ik|
N2 ,
(4.18)
where Ik is defined by
Ik := {i ‚àà{1, . . . , N} : 1 ‚â§k ‚àíi ‚â§N} = {i ‚àà{1, . . . , N} : k ‚àíN ‚â§i ‚â§k ‚àí1} .
To verify eq. (4.18), use that for i ‚àâIk either ‚Ñô{X = i} = 0 or ‚Ñô{Y = k ‚àíi} = 0. It is not
difficult to prove that
|Ik| = {k ‚àí1
if 2 ‚â§k ‚â§N + 1,
2N ‚àík + 1
if N + 1 < k ‚â§2N,
which leads to
‚Ñô{X + Y = k} =
{
{
{
{
{
{
{
{
{
k‚àí1
N2
if 2 ‚â§k ‚â§N + 1,
2N‚àík+1
N2
if N + 1 < k ‚â§2N,
0
otherwise
If N = 6, then X+Y may be viewed as the sum of two rolls of a die. Here the above formula
leads to the values of ‚Ñô{X +Y = k}, k = 2, . . . , 12, which we, by a direct approach, already
computed in Example 3.2.15. For another example with N = 8 see Figure 4.4.
Figure 4.4: The sum of independent X and Y, both uniformly distributed on {1, . . . , 8}.
Finally, let us shortly discuss the case of two arbitrary independent discrete random
variables. Assume that X and Y have values in at most countable infinite sets D and E,

4.5 Addition of random variables
‡±™
207
respectively. Then X + Y maps into
D + E := {x + y : x ‚ààD , y ‚ààE} .
Note that D + E is also at most countably infinite.
Under these assumptions, the following is valid.
Proposition 4.5.8. Suppose X and Y are two independent discrete random variables with
values in the (at most) countably infinite sets D and E, respectively. For z ‚ààD+E, it follows
that
‚Ñô{X + Y = z} =
‚àë
{(x,y)‚ààD√óE : x+y=z}
‚Ñô{X = x} ‚ãÖ‚Ñô{Y = y} .
Proof. For a fixed z ‚ààD + E define Bz ‚äÜD √ó E by Bz := {(x, y) : x + y = z}. Using this
notation, we get
‚Ñô{X + Y = z} = ‚Ñô{(X, Y) ‚ààBz} = ‚Ñô(X,Y)(Bz) ,
where again ‚Ñô(X,Y) denotes the joint distribution of X and Y. Now we may proceed as in
the proof of Proposition 4.5.4. The independence of X and Y implies
‚Ñô(X,Y)(Bz) =
‚àë
{(x,y)‚ààD√óE:x+y=z}
‚Ñô{X = x} ‚ãÖ‚Ñô{Y = y} ,
proving the proposition.
Remark 4.5.9. If D = E = ‚Ñ§, then Proposition 4.5.8 implies Proposition 4.5.4, while for
D = E = ‚Ñï0 we rediscover Proposition 4.5.6.
Example 4.5.10. Suppose X is uniformly distributed on D = {1, 2, 3, 4} while Y is uni-
formly distributed on E = {5, 6, 7, 8}. Their sum attains its values in {6, . . . , 12}. If X and
Y are independent, then, for example,
‚Ñô{X + Y = 7} =
‚àë
x‚ààD, y‚ààE
x+y=7
‚Ñô{X = x}‚Ñô{Y = y}
= ‚Ñô{X = 1} ‚ãÖ‚Ñô{Y = 6} + ‚Ñô{X = 2} ‚ãÖ‚Ñô{Y = 5}
= 1
4 ‚ãÖ1
4 + 1
4 ‚ãÖ1
4 = 1
8 .
4.5.2 Sums of continuous random variables
In this section we investigate the following question: let X and Y be two continuous
random variables with density functions p and q. Is X + Y continuous as well, and if this
is so, how do we compute its density?

208
‡±™
4 Operations on random variables
To answer this question, we need a special type of composing two functions.
Definition 4.5.11. Let f and g be two Riemann integrable functions from ‚Ñùto ‚Ñù. Their convolution f ‚ãÜg
is defined by
(f ‚ãÜg)(x) :=
‚àû
‚à´
‚àí‚àû
f(x ‚àíy) g(y) dy ,
x ‚àà‚Ñù.
(4.19)
Remark 4.5.12. The convolution is a commutative operation, that is,
f ‚ãÜg = g ‚ãÜf .
This follows by the change of variables u = x ‚àíy in eq. (4.19), thus
(f ‚ãÜg)(x) =
‚àû
‚à´
‚àí‚àû
f (x ‚àíy) g(y) dy =
‚àû
‚à´
‚àí‚àû
f (u) g(x ‚àíu) du = (g ‚ãÜf )(x) ,
x ‚àà‚Ñù.
Remark 4.5.13. For general functions f and g, the integral in eq. (4.19) does not always
exist for all x ‚àà‚Ñù. The investigation of this question requires facts and notations8 from
Measure Theory; therefore, we will not treat it here. We only state a special case, which
suffices for our later purposes. Moreover, for concrete functions f and g, it is mostly easy
to check for which x ‚àà‚Ñùthe value (f ‚ãÜg)(x) exists.
Proposition 4.5.14. Let p and q be two probability densities and suppose that at least one
of them is bounded. Then (p ‚ãÜq)(x) exists for all x ‚àà‚Ñù.
Proof. Say p is bounded, that is, there is a constant c ‚â•0 such that 0 ‚â§p(z) ‚â§c for all
z ‚àà‚Ñù. Since q(y) ‚â•0, if x ‚àà‚Ñù, then
0 ‚â§
‚àû
‚à´
‚àí‚àû
p(x ‚àíy)q(y) dy ‚â§c
‚àû
‚à´
‚àí‚àû
q(y) dy = c < ‚àû.
This proves that (p ‚ãÜg)(x) exists for all x ‚àà‚Ñù.
Since p ‚ãÜq = q ‚ãÜp, the same argument applies if q is bounded.
The next result provides us with a formula for the evaluation of the density function
of X + Y for independent continuous X and Y.
Proposition 4.5.15 (Convolution formula for continuous random variables). Let X and Y
be two independent random variables with distribution densities p and q. Then X + Y is
continuous as well, and its density r may be computed by
8 For example, ‚Äúexists almost everywhere.‚Äù

4.5 Addition of random variables
‡±™
209
r(x) = (p ‚ãÜq)(x) =
‚àû
‚à´
‚àí‚àû
p(y) q(x ‚àíy) dy.
Proof. We have to show that r = p ‚ãÜq satisfies
‚Ñô{X + Y ‚â§t} =
t
‚à´
‚àí‚àû
r(x) dx ,
t ‚àà‚Ñù.
(4.20)
Fix t ‚àà‚Ñùfor a moment and define Bt ‚äÜ‚Ñù2 by
Bt := {(u, y) ‚àà‚Ñù2 : u + y ‚â§t} .
Then we get
‚Ñô{X + Y ‚â§t} = ‚Ñô{(X, Y) ‚ààBt} = ‚Ñô(X,Y)(Bt) .
(4.21)
To compute the right-hand side of eq. (4.21), we use Proposition 3.6.20. It asserts that
the joint distribution ‚Ñô(X,Y) of independent X and Y has density (u, y) Û≥®É‚Üíp(u)q(y), that
is, if B ‚äÜ‚Ñù2, then
‚Ñô(X,Y)(B) = ‚à¨
B
p(u)q(y) dy du .
Choosing B = Bt in the last formula, eq. (4.21) may now be written as
‚Ñô{X + Y ‚â§t} = ‚à¨
Bt
p(u) q(y) dy du =
‚àû
‚à´
‚àí‚àû
[
t‚àíy
‚à´
‚àí‚àû
p(u) du] q(y) dy .
(4.22)
Next we change the variables in the inner integral as follows:9 u = x ‚àíy, hence
du = dx. Then the right-hand integrals in eq. (4.22) coincide with
‚àû
‚à´
‚àí‚àû
[
t
‚à´
‚àí‚àû
p(x ‚àíy) dx] q(y) dy =
t
‚à´
‚àí‚àû
[
‚àû
‚à´
‚àí‚àû
p(x ‚àíy) q(y) dy] dx
=
t
‚à´
‚àí‚àû
(p ‚ãÜq)(x) dx .
Hereby we used that p and q are nonnegative, so that we may interchange the integrals
by virtue of Proposition A.5.5. Thus, eq. (4.20) is satisfied, which completes the proof.
9 Note that in the inner integral y is a constant.

210
‡±™
4 Operations on random variables
4.6 Sums of certain random variables
Let us start with the investigation of the sum of independent binomial distributed ran-
dom variables. Here the following is valid.
Proposition 4.6.1. Let X and Y be two independent random variables, accordingly
Bn,p- and Bm,p-distributed for some n, m ‚â•1, and some p ‚àà[0, 1]. Then X + Y is
Bn+m,p-distributed.
Proof. By Proposition 4.5.6, we get that for 0 ‚â§k ‚â§m + n,
‚Ñô{X + Y = k} =
k
‚àë
j=0
[(n
j ) pj (1 ‚àíp)n‚àíj] ‚ãÖ[( m
k ‚àíj) pk‚àíj (1 ‚àíp)m‚àí(k‚àíj)]
= pk (1 ‚àíp)n+m‚àík
k
‚àë
j=0
(n
j )( m
k ‚àíj) .
To evaluate the sum, we apply Vandermonde‚Äôs identity (Proposition A.3.9), which
asserts
k
‚àë
j=0
(n
j )( m
k ‚àíj) = (n + m
k
) .
This leads to
‚Ñô{X + Y = k} = (n + m
k
) pk (1 ‚àíp)m+n‚àík ,
and X + Y is Bn+m,p-distributed.
Interpretation: In the first experiment, we toss a biased coin n times and in the sec-
ond m times. We combine these two experiments into one and toss the coin now n + m
times. Then we observe ‚Äúheads‚Äù exactly k times during the n + m trials if there is some
j ‚â§k so that we had j ‚Äúheads‚Äù among the first n trials and k ‚àíj among the second m.
Finally, we have to sum the probabilities of all these events over j ‚â§k.
Corollary 4.6.2. Let X1, . . . , Xn be independent B1,p-distributed, that is,
‚Ñô{Xj = 0} = 1 ‚àíp
and
‚Ñô{Xj = 1} = p ,
j = 1, . . . , n .
Then their sum X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn is Bn,p-distributed.
Proof. Apply Proposition 4.6.1 successively, first to X1 and X2, then to X1 +X2 and X3, and
so on.
Remark 4.6.3. Observe that
X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn = ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®{j ‚â§n : Xj = 1}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®.

4.6 Sums of certain random variables
‡±™
211
Corollary 4.6.2 justifies the interpretation of the binomial distribution given in Sec-
tion 1.4.3. Indeed, the event {Xj = 1} occurs if in trial j we observe success. Thus, the
sum X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn describes the number of successes in n independent trials. Hereby, the
success probability is ‚Ñô{Xj = 1} = p.
In the literature, the following notion is common.
Definition 4.6.4. A finite or infinite sequence X1, X2, . . . of independent B1,p-distributed random variables
is called a Bernoulli trial or Bernoulli process, or also binomial trial, with success probability p ‚àà[0, 1].
With these notations, Corollary 4.6.2 may now be formulated as follows:
Let X1, X2, . . . , be a Bernoulli trial with success probability p. Then for n ‚â•1,
‚Ñô{X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn = k} = (n
k) pk (1 ‚àíp)n‚àík ,
k = 0, . . . , n .
Let X and Y be two independent Poisson distributed random variables. Which distribu-
tion does X + Y possess? The next result answers this question.
Proposition 4.6.5. Let X and Y be independent PoisŒª- and PoisŒº-distributed for some Œª > 0
and Œº > 0, respectively. Then X + Y is PoisŒª+Œº-distributed.
Proof. Proposition 4.5.6 and the binomial theorem (see Proposition A.3.8) imply
‚Ñô{X + Y = k} =
k
‚àë
j=0
[Œªj
j! e‚àíŒª][ Œºk‚àíj
(k ‚àíj)! e‚àíŒº]
= e‚àí(Œª+Œº)
k!
k
‚àë
j=0
k!
j! (k ‚àíj)!Œªj Œºk‚àíj
= e‚àí(Œª+Œº)
k!
k
‚àë
j=0
(k
j )Œªj Œºk‚àíj = (Œª + Œº)k
k!
e‚àí(Œª+Œº) .
Consequently, as asserted, X + Y is PoisŒª+Œº-distributed.
Interpretation: The numbers of phone calls arriving per day at some call centers A
and B are Poisson distributed with parameters10 Œª and Œº. Suppose that these two centers
have different customers, that is, we assume that the number of calls in A and B is in-
dependent of each other. Proposition 4.6.5 asserts that the number of calls arriving per
day either in A or in B is again Poisson distributed, but now with parameter Œª + Œº.
Example 4.6.6. This example deals with the distribution of raisins in a set of dough.
More precisely, suppose we have N pounds of dough and therein are n uniformly dis-
10 Later on, in Proposition 5.1.16, we will see that Œª and Œº are the mean values of arriving calls per day.

212
‡±™
4 Operations on random variables
tributed raisins. Choose at random a one-pound piece of dough. Find the probability that
there are k ‚â•0 raisins in the chosen piece.
Approach 1: Since the raisins are uniformly distributed in the dough, the probability
that a single raisin is in the chosen piece equals 1/N. Hence, if X is the number of raisins
in that piece, it is Bn,p-distributed with p = 1/N. Assuming that N is big, the random
variable X is approximately PoisŒª-distributed with Œª = n/N, that is,
‚Ñô{X = k} = Œªk
k! e‚àíŒª ,
k = 0, 1, . . .
Note that Œª = n/N coincides with the average number of raisins per pound dough.
Approach 2: Assume that we took in the previous model N ‚Üí‚àû, that is, we have
an ‚Äúinfinite‚Äù amount of dough and ‚Äúinfinitely‚Äù many raisins. Which distribution does X,
the number of raisins in a one-pound piece, now possess?
First, we have to determine what it means that the amount of dough is ‚Äúinfinite‚Äù and
that the raisins are uniformly distributed11 therein. This is expressed by the following
conditions:
(a) The mass of dough is unbelievably huge, hence, whenever we choose two different
pieces, the numbers of raisins in the pieces are independent of each other.
(b) The fact that the raisins are uniformly distributed is expressed by the following
condition: suppose the number of raisins in a one-pound piece is n ‚â•0. If this piece is
split into two pieces, say K1 and K2 of weight Œ± and 1‚àíŒ± pounds, then the probability
that a single raisin (out of n) is in K1 equals Œ±, and the probability that it is in K2 is
1 ‚àíŒ±.
Fix 0 < Œ± < 1 and choose in the first step a piece K1 of Œ± pounds and in the second one
another piece K2 of weight 1‚àíŒ±. Let X1 and X2 be the numbers of raisins in each of the two
pieces. By condition (a), the random variables X1 and X2 are independent. If X = X1 + X2,
then X is the number of raisins in a randomly chosen one-pound piece. Suppose now
X = n, that is, there are n raisins in the one-pound piece. Then by condition (b), the
probability for k raisins in K1 is described by the binomial distribution Bn,Œ±. Recall that
the success probability for a single raisin is Œ±, thus, X1 = k means, we have k successes.
This may be formulated as follows: for all 0 ‚â§k ‚â§n,
‚Ñô{X1 = k|X = n} = Bn,Œ±({k}) = (n
k)Œ±k(1 ‚àíŒ±)n‚àík .
(4.23)
Rewriting eq. (4.23) leads to
‚Ñô{X1 = k, X2 = n ‚àík} = ‚Ñô{X1 = k, X = n}
11 Note that the multivariate uniform distribution only makes sense (cf. Definition 1.8.13) if the under-
lying set has a finite volume.

4.6 Sums of certain random variables
‡±™
213
= ‚Ñô{X1 = k|X = n} ‚ãÖ‚Ñô{X = n} = ‚Ñô{X = n} ‚ãÖ(n
k)Œ±k(1 ‚àíŒ±)n‚àík .
(4.24)
Observe that in contrast to eq. (4.23), eq. (4.24) remains valid also if ‚Ñô{X = n} = 0.
Indeed, if ‚Ñô{X = n} = 0, by Proposition 4.5.6, the event {X1 = k, X2 = n ‚àík} has zero
probability as well.
The independence of X1 and X2 and eq. (4.24) imply that, if n = 0, 1, 2, . . . and k =
0, . . . , n, then
‚Ñô{X1 = k} ‚ãÖ‚Ñô{X2 = n ‚àík} = ‚Ñô{X = n} ‚ãÖ(n
k)Œ±k(1 ‚àíŒ±)n‚àík .
Setting k = n, we get
‚Ñô{X1 = n} ‚ãÖ‚Ñô{X2 = 0} = ‚Ñô{X = n} ‚ãÖŒ±n ,
(4.25)
while for n ‚â•1 and k = n ‚àí1 we obtain
‚Ñô{X1 = n ‚àí1} ‚ãÖ‚Ñô{X2 = 1} = ‚Ñô{X = n} ‚ãÖn ‚ãÖŒ±n‚àí1 (1 ‚àíŒ±) .
(4.26)
In particular, from eq. (4.25) ‚Ñô{X2 = 0} > 0 follows. If this probability were zero,
then this would imply ‚Ñô{X = n} = 0 for all n ‚àà‚Ñï0, which is impossible in view of
‚Ñô{X ‚àà‚Ñï0} = 1.
In the next step, we solve eqs. (4.25) and (4.26) with respect to ‚Ñô{X = n} and make
them equal. Doing so, for n ‚â•1 we get
‚Ñô{X1 = n} = Œ±
n (1 ‚àíŒ±)‚àí1 ‚ãÖ‚Ñô{X2 = 1}
‚Ñô{X2 = 0} ‚ãÖ‚Ñô{X1 = n ‚àí1}
= Œ±Œª
n ‚ãÖ‚Ñô{X1 = n ‚àí1} ,
(4.27)
where Œª ‚â•0 is defined by
Œª := (1 ‚àíŒ±)‚àí1 ‚ãÖ‚Ñô{X2 = 1}
‚Ñô{X2 = 0} .
(4.28)
Do we have Œª > 0? If Œª = 0, then ‚Ñô{X2 = 1} = 0 and, by eq. (4.26), ‚Ñô{X = n} = 0 for
n ‚â•1. Consequently, ‚Ñô{X = 0} = 1, which says that there are no raisins in the dough. We
exclude this trivial case, thus it follows that Œª > 0.
Finally, successive application of eq. (4.27) implies for n ‚àà‚Ñï0
12 that
‚Ñô{X1 = n} = (Œ±Œª)n
n!
‚ãÖ‚Ñô{X1 = 0} ,
(4.29)
12 If n = 0, the equation holds trivially.

214
‡±™
4 Operations on random variables
leading to
1 =
‚àû
‚àë
n=0
‚Ñô{X1 = n} = ‚Ñô{X1 = 0} ‚ãÖ
‚àû
‚àë
n=0
(Œ±Œª)n
n!
= ‚Ñô{X1 = 0} eŒ±Œª ,
that is, we have ‚Ñô{X1 = 0} = e‚àíŒ±Œª. Plugging this into eq. (4.29) gives
‚Ñô{X1 = n} = (Œ±Œª)n
n!
e‚àíŒ±Œª ,
and so X1 is Poisson distributed with parameter Œ±Œª.
Let us interchange now the roles of X1 and X2, hence also of Œ± and 1 ‚àíŒ±. An applica-
tion of the first step to X2 tells us that it is Poisson distributed, but now with parameter
(1 ‚àíŒ±)Œª‚Ä≤, where in view of eq. (4.28) Œª‚Ä≤ is given by13
Œª‚Ä≤ = Œ±‚àí1 ‚ãÖ‚Ñô{X1 = 1}
‚Ñô{X1 = 0} = Œ±‚àí1 Œ±Œª e‚àíŒ±Œª
e‚àíŒ±Œª
= Œª .
Thus, X2 is Pois(1‚àíŒ±)Œª-distributed.
Since X1 and X2 are independent, Proposition 4.6.5 applies, hence X = X1 + X2 is
PoisŒª-distributed or, equivalently,
‚Ñô{There are k raisins in a one-pound piece} = Œªk
k! e‚àíŒª .
Remark 4.6.7. Which role does the parameter Œª > 0 play in this model? As already
mentioned, Proposition 5.1.16 will tell us that Œª is the average number of raisins per
pound dough. Thus, if œÅ > 0 and we ask for the number of raisins in a piece of œÅ pounds,
then this number is PoisœÅŒª-distributed,14 that is,
‚Ñô{k raisins in œÅ pounds of dough} = (œÅŒª)k
k!
e‚àíœÅŒª .
Assume that dough contains on average 20 raisins per pound. Let X be the number of
raisins in a piece of bread baked from five pounds of dough. Then X is Pois100-distributed
and
‚Ñô({95 ‚â§X ‚â§105}) = 0.4176 ,
‚Ñô({90 ‚â§X ‚â§110}) = 0.7065 ,
‚Ñô({85 ‚â§X ‚â§115}) = 0.8793 ,
‚Ñô({80 ‚â§X ‚â§120}) = 0.9599 ,
‚Ñô({75 ‚â§X ‚â§125}) = 0.9892 ,
‚Ñô({70 ‚â§X ‚â§130}) = 0.9976 .
13 Observe that we have to replace X2 by X1 and 1 ‚àíŒ± by Œ±.
14 Because on average there are œÅŒª raisins in a piece of œÅ pounds.

4.6 Sums of certain random variables
‡±™
215
Additional question: Suppose we buy two loaves of bread baked from œÅ pounds dough
each. What is the probability that one of these two loaves contains more than twice as
many raisins as the other?
Answer: Let X be the number of raisins in the first loaf, and Y the number of raisins
in the second. By assumption, X and Y are independent, and both are PoisœÅŒª-distributed,
where as before Œª > 0 is the average number of raisins per pound dough. The probability
we are interested in is (use Proposition 1.2.4 as well as that (X, Y)
d= (Y, X))
‚Ñô{X > 2Y or Y > 2X} = ‚Ñô{X > 2Y} + ‚Ñô{Y > 2X} = 2 ‚Ñô{X > 2Y}
= 2
‚àû
‚àë
k=0
‚Ñô{Y = k, X > 2k} = 2
‚àû
‚àë
k=0
‚Ñô{Y = k} ‚ãÖ‚Ñô{X > 2k}
= 2
‚àû
‚àë
k=0
‚Ñô{Y = k} ‚ãÖ
‚àû
‚àë
j=2k+1
‚Ñô{X = j} = 2 e‚àí2œÅŒª
‚àû
‚àë
k=0
(œÅŒª)k
k!
‚àû
‚àë
j=2k+1
(œÅŒª)j
j!
.
If the average number of raisins per pound is Œª = 20, and if the loaves are baked from
œÅ = 5 pounds dough, then this probability is approximately
‚Ñô{X > 2Y or Y > 2X} ‚âà3.17061 √ó 10‚àí6 .
If œÅ = 1, that is, the loaves are made from one-pound dough each, then
‚Ñô{X > 2Y or Y > 2X} ‚âà0.0430079 .
Next we investigate the distribution of the sum of two independent negative binomial
distributed random variables. Recall that X is B‚àí
n,p-distributed if
‚Ñô{X = k} = (k ‚àí1
k ‚àín) pn (1 ‚àíp)k‚àín ,
k = n, n + 1, . . .
Proposition 4.6.8. Let X and Y be independent and respectively B‚àí
n,p- and B‚àí
m,p-distributed
for some m, n ‚â•1. Then X + Y is B‚àí
n+m,p-distributed.
Proof. We derive from Example 4.1.8 that, if k ‚àà‚Ñï0, then
‚Ñô{X ‚àín = k} = (‚àín
k ) pn (p ‚àí1)k
and
‚Ñô{Y ‚àím = k} = (‚àím
k ) pm (p ‚àí1)k .
An application of Proposition 4.5.6 to X ‚àín and Y ‚àím implies
‚Ñô{X + Y ‚àí(n + m) = k} =
k
‚àë
j=0
[(‚àín
j )pn(p ‚àí1)j] [( ‚àím
k ‚àíj)pm(p ‚àí1)k‚àíj]
= pn+m(p ‚àí1)k
k
‚àë
j=0
(‚àín
j )( ‚àím
k ‚àíj) .

216
‡±™
4 Operations on random variables
To compute the last sum, we use Proposition A.5.3, which asserts that
k
‚àë
j=0
(‚àín
j )( ‚àím
k ‚àíj) = (‚àín ‚àím
k
) .
Consequently, for each k ‚àà‚Ñï0,
‚Ñô{X + Y ‚àí(n + m) = k} = (‚àín ‚àím
k
)pn+m(p ‚àí1)k .
Another application of eq. (4.2) (this time with n + m) leads to
‚Ñô{X + Y = k} = (
k ‚àí1
k ‚àí(n + m)) pn+m (1 ‚àíp)k‚àí(n+m) ,
k = n + m, n + m + 1, . . . ,
completing the proof.
Corollary 4.6.9. Let X1, . . . , Xn be independent Gp-distributed (geometrically distributed)
random variables. Then their sum X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn is B‚àí
n,p-distributed.
Proof. Use Gp = B‚àí
1,p and apply Proposition 4.6.8 n times.
Interpretation: The following two experiments are completely equivalent: one is to
play the same game until one observes success for the nth time. The other experiment
is, after each success to start a new game, until one observes success in the nth (and
last) game. Here we assume that all n games are executed independently and possess
the same success probability.
Let U and V be two independent random variables, both uniformly distributed on
[0, 1]. Which distribution density does U + V possess?
Proposition 4.6.10. The sum of two independent random variables U and V, uniformly
distributed on [0, 1], has the density r defined by
r(x) =
{
{
{
{
{
{
{
x
if 0 ‚â§x < 1,
2 ‚àíx
if 1 ‚â§x ‚â§2,
0
otherwise .
(4.30)
Proof. The distribution densities p and q of U and V are given by p(x) = q(x) = 1 if
0 ‚â§x ‚â§1 and p(x) = q(x) = 0 otherwise. Proposition 4.5.15 asserts that U +V has density
r = p ‚ãÜq computed by
r(x) =
‚àû
‚à´
‚àí‚àû
p(x ‚àíy) q(y) dy =
1
‚à´
0
p(x ‚àíy) dy .
But p(x ‚àíy) = 1 if and only if 0 ‚â§x ‚àíy ‚â§1 or, equivalently, if and only if x ‚àí1 ‚â§y ‚â§x.
Taking into account the restriction 0 ‚â§y ‚â§1, it follows that p(x ‚àíy)q(y) = 1 if and only

4.6 Sums of certain random variables
‡±™
217
if y ‚àà[max{x ‚àí1, 0}, min{x, 1}]. In particular, r(x) = 0 for x ‚àâ[0, 2], and if 0 ‚â§x ‚â§2, then
r(x) = min{x, 1} ‚àímax{x ‚àí1, 0} .
It is not difficult to see (treat the cases 0 ‚â§x ‚â§1 and 1 ‚â§x ‚â§2 separately) that r(x) may
be written as stated in eq. (4.30). This completes the proof.
Application. Suppose we choose independently and according to the uniform distribu-
tion two numbers u1 and u2 in [0, 1]. Then the probability that a ‚â§u1 + u2 ‚â§b equals
‚à´
b
a r(x) dx with r given by eq. (4.30). For example (see Figure 4.5),
‚Ñô{1
2 ‚â§u1 + u2 ‚â§3
2} =
1
‚à´
1/2
x dx +
3/2
‚à´
1
(2 ‚àíx) dx = 3
4 .
Figure 4.5: The area of the gray-shaded set equals ‚Ñô{1/2 ‚â§U + V ‚â§3/2}. Here the random variables U and
V are independent and both uniformly distributed on [0, 1].
We investigate now the sum of two gamma distributed random variables. Recall that
the density of a ŒìŒ±,Œ≤-distributed random variable is given by
pŒ±,Œ≤(x) =
1
Œ±Œ≤ Œì(Œ≤) xŒ≤‚àí1 e‚àíx/Œ±
if x > 0, while pŒ±,Œ≤(x) = 0 otherwise.
Proposition 4.6.11. Let X1 and X2 be two independent random variables distributed ac-
cording to ŒìŒ±,Œ≤1 and ŒìŒ±,Œ≤2, respectively. Then X1 + X2 is ŒìŒ±,Œ≤1+Œ≤2-distributed.
Proof. If r denotes the density of X1 + X2, Proposition 4.5.15 implies
r(x) = (pŒ±,Œ≤1 ‚ãÜpŒ±,Œ≤2)(x) =
‚àû
‚à´
‚àí‚àû
pŒ±,Œ≤1(x ‚àíy)pŒ±,Œ≤2(y) dy ,
x ‚àà‚Ñù,
(4.31)
and we have to show that r = pŒ±,Œ≤1+Œ≤2.

218
‡±™
4 Operations on random variables
It is easy to see that r(x) = 0 if x ‚â§0, hence it suffices to evaluate eq. (4.31) for x > 0.
Since pŒ±,Œ≤2(x ‚àíy) = 0 if y > x,
r(x) =
1
Œ±Œ≤1+Œ≤2Œì(Œ≤1)Œì(Œ≤2)
x
‚à´
0
yŒ≤1‚àí1(x ‚àíy)Œ≤2‚àí1 e‚àíy/Œ± e‚àí(x‚àíy)/Œ± dy
=
1
Œ±Œ≤1+Œ≤2Œì(Œ≤1)Œì(Œ≤2) xŒ≤1+Œ≤2‚àí2 e‚àíx/Œ±
x
‚à´
0
( y
x )
Œ≤1‚àí1
(1 ‚àíy
x )
Œ≤2‚àí1
dy .
Changing the variable as u := y/x, hence dy = x du, we obtain
r(x) =
1
Œ±Œ≤1+Œ≤2Œì(Œ≤1)Œì(Œ≤2) xŒ≤1+Œ≤2‚àí1 e‚àíx/Œ±
1
‚à´
0
uŒ≤1‚àí1(1 ‚àíu)Œ≤2‚àí1du
=
B(Œ≤1, Œ≤2)
Œ±Œ≤1+Œ≤2 Œì(Œ≤1) Œì(Œ≤2) ‚ãÖxŒ≤1+Œ≤2‚àí1 e‚àíx/Œ± ,
(4.32)
where B denotes the beta function defined by eq. (1.61). Equation (1.62) yields
B(Œ≤1, Œ≤2)
Œì(Œ≤1) Œì(Œ≤2) =
1
Œì(Œ≤1 + Œ≤2) ,
(4.33)
hence, if x > 0, then, from eqs. (4.32) and (4.33), it follows that r(x) = pŒ±,Œ≤1+Œ≤2(x). This
completes the proof.
Recall that the Erlang distribution is defined as EŒª,n = ŒìŒª‚àí1,n. Thus, Proposition 4.6.11
implies the following corollary.
Corollary 4.6.12. Let X and Y be independent and distributed according to EŒª,n and EŒª,m,
respectively. Then their sum X + Y is EŒª,n+m-distributed.
Another corollary of Proposition 4.5.15 (or of Corollary 4.6.12) describes the sum of
independent exponentially distributed random variables.
Corollary 4.6.13. Let X1, . . . , Xn be independent EŒª-distributed random variables. Then
their sum X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn is Erlang distributed with parameters Œª and n.
Proof. Recall that EŒª = EŒª,1. By Corollary 4.6.12, the sum X1 + X2 is distributed according
to EŒª,2. Proceeding in this way, every time applying Corollary 4.6.12 leads to the desired
result.
Example 4.6.14. The lifetime of light bulbs is assumed to be EŒª-distributed for a certain
Œª > 0. At time zero, we switch on the first bulb. At the moment it burns out, we replace
it by the second one of the same type. If the second burns out, we replace it by the third,
and so on. Let Sn be the moment when the nth light bulb burns out. Which distribution
does Sn possess?

4.6 Sums of certain random variables
‡±™
219
Answer: Let X1, X2, . . . be the lifetimes of the first, second, and so on, light bulb. Then
Sn = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn. Since the light bulbs are assumed to be of the same type, the random
variables Xj are all EŒª-distributed. Furthermore, the different lifetimes do not influence
each other, thus, we may assume that the Xjs are independent. Now Corollary 4.6.13 lets
us conclude that Sn is Erlang distributed with parameters Œª and n, hence, if t > 0, by
Proposition 1.6.26, we get
‚Ñô{Sn ‚â§t} =
Œªn
(n ‚àí1)!
t
‚à´
0
xn‚àí1 e‚àíŒªx dx = 1 ‚àí
n‚àí1
‚àë
j=0
(Œªt)j
j!
e‚àíŒªt .
(4.34)
Example 4.6.15. We continue the preceding example, but ask now a different question.
How often do we have to change light bulbs before some given time T > 0?
Answer: Let Y be the number of changes necessary until time T. Then for n ‚â•0 the
event {Y = n} occurs if and only if Sn ‚â§T, but Sn+1 > T. Hereby, we use the notation of
Example 4.6.14. In other words,
‚Ñô{Y = n} = ‚Ñô{Sn ‚â§T, Sn+1 > T} = ‚Ñô({Sn ‚â§T} \ {Sn+1 ‚â§T}) ,
n = 0, 1, . . .
Since {Sn+1 ‚â§T} ‚äÜ{Sn ‚â§T}, from eq. (4.34) it follows that
‚Ñô{Y = n} = ‚Ñô{Sn ‚â§T} ‚àí‚Ñô{Sn+1 ‚â§T}
= [1 ‚àí
n‚àí1
‚àë
j=0
(ŒªT)j
j!
e‚àíŒªT] ‚àí[1 ‚àí
n
‚àë
j=0
(ŒªT)j
j!
e‚àíŒªT]
= (ŒªT)n
n!
e‚àíŒªT = PoisŒªT({n}) .
Summing up, the number of necessary replacements of burned out light bulbs until time
T > 0 is Poisson distributed with parameter ŒªT where 1/Œª > 0 is the average lifetime of
a single bulb (compare with Example 5.1.30).
Let us still mention an important equivalent random ‚Äúexperiment‚Äù: customers ar-
rive at the post office randomly. We assume that the times between their arrivals are in-
dependent and EŒª-distributed. Then Sn is the time when the nth customer arrives. Hence,
under these assumptions, the number of arriving customers until a certain time T > 0
is Poisson distributed with parameter ŒªT.
We investigate now the sum of two independent chi-squared distributed random
variables. Recall Definition 1.6.27: A random variable X is œá2
n-distributed if it is Œì2, n
2 -
distributed. Hence, Proposition 4.6.11 implies the following result.
Proposition 4.6.16. Suppose that X is œá2
n-distributed and that Y is œá2
m-distributed for some
n, m ‚â•1. If X and Y are independent, then X + Y is œá2
n+m-distributed.
Proof. Because of Proposition 4.6.11, the sum X + Y is Œì2, n
2 + m
2 = œá2
n+m-distributed. This
proves the assertion.

220
‡±™
4 Operations on random variables
Proposition 4.6.16 has the following important consequence.
Proposition 4.6.17. Let X1, . . . , Xn be a sequence of independent ùí©(0, 1)-distributed ran-
dom variables. Then X2
1 + ‚ãÖ‚ãÖ‚ãÖ+ X2
n is œá2
n-distributed.
Proof. Proposition 4.1.5 asserts that the random variables X2
j are œá2
1-distributed. Further-
more, because of Proposition 4.1.9, they are also independent. Thus successive applica-
tion of Proposition 4.6.16 proves the assertion.
Our next and final aim in this section is to investigate the distribution of the sum of
two independent normally distributed random variables. Here the following important
result is valid.
Proposition 4.6.18. Let X1 and X2 be two independent random variables distributed ac-
cording to ùí©(Œº1, œÉ2
1) and ùí©(Œº2, œÉ2
2). Then X1 + X2 is ùí©(Œº1 + Œº2, œÉ2
1 + œÉ2
2)-distributed.
Proof. In the first step, we treat a special case, namely Œº1 = Œº2 = 0 and œÉ1 = 1. To simplify
the notation, set Œª = œÉ2. Thus we have to prove the following: if X1 and X2 are ùí©(0, 1)-
and ùí©(0, Œª2)-distributed, then X1 + X2 is ùí©(0, 1 + Œª2)-distributed.
Let p0,1 and p0,Œª2 be the corresponding densities introduced in eq. (1.49). Then we
have to prove that
p0,1 ‚ãÜp0,Œª2 = p0,1+Œª2 .
(4.35)
To verify this start with
(p0,1 ‚ãÜp0,Œª2)(x) =
1
2œÄ Œª
‚àû
‚à´
‚àí‚àû
e‚àí(x‚àíy)2/2 e‚àíy2/2Œª2
dy
=
1
2œÄ Œª
‚àû
‚à´
‚àí‚àû
e‚àí1
2 (x2‚àí2xy+(1+Œª‚àí2)y2) dy .
(4.36)
We use
x2 ‚àí2xy + (1 + Œª‚àí2) y2
= ((1 + Œª‚àí2)
1/2y ‚àí(1 + Œª‚àí2)
‚àí1/2 x)
2 ‚àíx2(
1
1 + Œª‚àí2 ‚àí1)
= ((1 + Œª‚àí2)
1/2y ‚àí(1 + Œª‚àí2)
‚àí1/2 x)
2 +
x2
1 + Œª2
= (Œ±y ‚àíx
Œ±)
2
+
x2
1 + Œª2
with Œ± := (1 + Œª‚àí2)1/2. Plugging this transformation into eq. (4.36) leads to

4.6 Sums of certain random variables
‡±™
221
(p0,1 ‚ãÜp0,Œª2)(x) = e‚àíx2/2(1+Œª2)
2œÄ Œª
‚àû
‚à´
‚àí‚àû
e‚àí(Œ±y‚àíx
Œ± )2/2 d y .
(4.37)
Next change the variables by u := Œ±y ‚àíx/Œ±, thus, d y = d u/Œ±, and observe that
Œ±Œª = (1 + Œª2)1/2. Then the right-hand side of eq. (4.37) transforms to
(p0,1 ‚ãÜp0,Œª2)(x) =
e‚àíx2/2(1+Œª2)
2œÄ (1 + Œª2)1/2
‚àû
‚à´
‚àí‚àû
e‚àíu2/2 du = p0,1+Œª2(x) .
Hereby, we used Proposition 1.6.7 asserting ‚à´
‚àû
‚àí‚àûe‚àíu2/2 du = ‚àö2œÄ. This proves the validity
of eq. (4.35).
In the second step, we treat the general case, that is, X1 is ùí©(Œº1, œÉ2
1 )- and X2 is
N(Œº2, œÉ2
2)-distributed. Set
Y1 := X1 ‚àíŒº1
œÉ1
and
Y2 := X2 ‚àíŒº2
œÉ2
.
By Proposition 4.2.3, the random variables Y1 and Y2 are standard normal and, moreover,
because of Proposition 4.1.9, also independent. Thus, the sum X1+X2 may be represented
as
X1 + X2 = Œº1 + Œº2 + œÉ1Y1 + œÉ2Y2 = Œº1 + Œº2 + œÉ1 Z,
where Z = Y1 + ŒªY2 with Œª = œÉ2/œÉ1.
An application of the first step shows that Z is ùí©(0, 1 + Œª2)-distributed. Hence,
Proposition 4.2.3 implies the existence of a standard normally distributed Z0 such that
Z = (1 + Œª2)1/2Z0. Summing up, X1 + X2 may now be written as
X1 + X2 = Œº1 + Œº2 + œÉ1 (1 + Œª2)
1/2 Z0 = Œº1 + Œº2 + (œÉ2
1 + œÉ2
2)
1/2 Z0 ,
and another application of Proposition 4.2.3 lets us conclude that, as asserted, the sum
X1 + X2 is ùí©(Œº1 + Œº2, œÉ2
1 + œÉ2
2)-distributed.
Summary: Let X and Y be independent random variables. As we agreed upon in Remark 3.3.1, by ‚Äú‚àº‚Äù we
mean that X, Y, and X + Y posses the stated distribution. Then the following are valid:
1.
X ‚àºBm,p and Y ‚àºBn,p
‚áí
X + Y ‚àºBm+n,p.
2.
X ‚àºPoisŒª and Y ‚àºPoisŒº
‚áí
X + Y ‚àºPoisŒª+Œº.
3.
X ‚àºB‚àí
m,p and Y ‚àºB‚àí
n,p
‚áí
X + Y ‚àºB‚àí
m+n,p.
4.
X ‚àºŒìŒ±,Œ≤1 and Y ‚àºŒìŒ±,Œ≤2
‚áí
X + Y ‚àºŒìŒ±,Œ≤1+Œ≤2.
5.
X ‚àºEŒª,m and Y ‚àºEŒª,n
‚áí
X + Y ‚àºEŒª,m+n.
6.
X ‚àºœá2
m and Y ‚àºœá2
n
‚áí
X + Y ‚àºœá2
m+n.
7.
X ‚àºùí©(Œº1, œÉ2
1 ) and Y ‚àºùí©(Œº2, œÉ2
2)
‚áí
X + Y ‚àºùí©(Œº1 + Œº2, œÉ2
1 + œÉ2
2).
8.
X1, . . . , Xn independent and ùí©(0, 1)-distributed
‚áí
X2
1 + ‚ãÖ‚ãÖ‚ãÖ+ X2
n ‚àºœá2
n.

222
‡±™
4 Operations on random variables
4.7 Products and quotients of random variables
Let X and Y be two random variables mapping a sample space Œ© into ‚Ñù. Then their
product X ‚ãÖY and their quotient X/Y (assume Y(œâ)
Ã∏= 0 for œâ ‚ààŒ©) are defined by
(X ‚ãÖY)(œâ) := X(œâ) ‚ãÖY(œâ)
and
(X
Y )(œâ) := X(œâ)
Y(œâ) ,
œâ ‚ààŒ© .
The aim of this section is to investigate the distribution of such products and quotients.
We restrict ourselves to continuous X and Y because, later on, we will only deal with
products and quotients of those random variables. Furthermore, we omit the proof of
the fact that products and fractions are random variables as well. The proofs of these
permanent properties are not complicated and follow the ideas used in the proof of
Proposition 4.5.1. Thus, our interest are products X‚ãÖY and quotients X/Y for independent
X and Y, where, to simplify the computations, we suppose ‚Ñô{Y > 0} = 1.
We start with the investigation of products of continuous random variables. Thus, let
X and Y be two random variables with distribution densities p and q. Since we assumed
‚Ñô{Y > 0} = 1, we may choose the density q such that q(x) = 0 if x ‚â§0.
Proposition 4.7.1. Let X and Y be two independent random variables possessing the
stated properties. Then X ‚ãÖY is continuous as well, and its density r may be calculated by
r(x) =
‚àû
‚à´
0
p(x
y ) q(y)
y
dy ,
x ‚àà‚Ñù.
(4.38)
Proof. For t ‚àà‚Ñù, we evaluate ‚Ñô{X ‚ãÖY ‚â§t}. To this end, fix t ‚àà‚Ñùand set
At := {(u, y) ‚àà‚Ñù√ó (0, ‚àû) : u ‚ãÖy ‚â§t} .
(4.39)
As in the proof of Proposition 4.5.15, it follows that
‚Ñô{X ‚ãÖY ‚â§t} = ‚Ñô(X,Y)(At) =
‚àû
‚à´
0
[
t/y
‚à´
‚àí‚àû
p(u) du] q(y) dy .
(4.40)
In the inner integral of eq. (4.40), we change the variables by x = uy, hence we get
dx = y du. Notice that in the inner integral y is a constant. After this change of variables,
the right-hand integral in eq. (4.40) becomes15
‚àû
‚à´
0
[
t
‚à´
‚àí‚àû
p(x
y ) dx]q(y)
y
dy =
t
‚à´
‚àí‚àû
[
‚àû
‚à´
0
p(x
y )q(y)
y
dy] dx =
t
‚à´
‚àí‚àû
r(x) dx .
This being valid for all t ‚àà‚Ñù, the function r is a density of X ‚ãÖY.
15 The interchange of the integrals is justified by Proposition A.5.5. Note that p and q are nonnegative.

4.7 Products and quotients of random variables
‡±™
223
Example 4.7.2. Let U and V be two independent random variables uniformly dis-
tributed on [0, 1]. Which probability distribution does U ‚ãÖV possess?
Answer: We have p(y) = q(y) = 1 if 0 ‚â§y ‚â§1, and p(y) = q(y) = 0 otherwise.
Furthermore, 0 ‚â§U ‚ãÖV ‚â§1, hence its density r satisfies r(x) = 0 if x ‚àâ[0, 1]. For
x ‚àà[0, 1], we apply formula (4.38) and obtain (see Figure 4.6)
r(x) =
‚àû
‚à´
0
p(x
y ) q(y)
y
dy =
1
‚à´
x
1
y dy = ‚àíln(x) = ln( 1
x ) ,
0 < x ‚â§1 .
(4.41)
Figure 4.6: The density r of U ‚ãÖV given by eq. (4.41).
Consequently, if 0 < a < b ‚â§1, then
‚Ñô{a ‚â§U ‚ãÖV ‚â§b} = ‚àí
b
‚à´
a
ln(x) dx = ‚àí[x ln x ‚àíx]b
a = a ln(a) ‚àíb ln(b) + b ‚àía .
In particular, it follows that
vol2(Bt) = ‚Ñô{U ‚ãÖV ‚â§t} = t ‚àít ln t ,
0 < t ‚â§1 .
(4.42)
Here Bt is defined as in Fig. 4.7. That is,
Bt = {(u, y) ‚àà[0, 1]2 : u y ‚â§t} ,
0 ‚â§t ‚â§1 .
In other words, Bt = At ‚à©[0, 1]2 with At defined by eq. (4.39). Furthermore, the random
vector (U, V) is uniformly distributed on [0, 1]2, so we get
‚Ñô{(U, V) ‚ààAt} = ‚Ñô{(U, V) ‚ààBt} = vol2(Bt) .

224
‡±™
4 Operations on random variables
Figure 4.7: The set Bt = {(u, y) ‚àà[0, 1]2 : u y ‚â§t} for a given 0 < t < 1.
Our next objective are quotients of random variables X and Y. We denote their den-
sities by p and q, thereby assuming q(x) = 0 if x ‚â§0. Then we get
Proposition 4.7.3. Let X and Y be independent with ‚Ñô{Y > 0} = 1. Then their quotient
X/Y has the density r given by
r(x) =
‚àû
‚à´
0
y p(x y) q(y) dy ,
x ‚àà‚Ñù.
Proof. The proof of Proposition 4.7.3 is similar to that of Proposition 4.7.1. Therefore, we
present only the main steps. Setting now
At := {(u, y) ‚àà‚Ñù√ó (0, ‚àû) : u ‚â§t y} ,
we obtain
‚Ñô{(X/Y) ‚â§t} = ‚Ñô(X,Y)(At) =
‚àû
‚à´
0
[
ty
‚à´
‚àí‚àû
p(u) du] q(y) dy .
(4.43)
We change the variables in the inner integral of eq. (4.43) by putting x = u/y. After
that, we interchange the integrals and arrive at
‚Ñô{(X/Y) ‚â§t} =
t
‚à´
‚àí‚àû
r(x) dx
for all t ‚àà‚Ñù. This proves that r is the density of X/Y.
Example 4.7.4. Let U and V be as in Example 4.7.2. We investigate now their quotient
U/V. By Proposition 4.7.3, its density r can be computed by
r(x) =
‚àû
‚à´
0
y p(xy)q(y) dy =
1
‚à´
0
yp(xy)dy =
1
‚à´
0
y dy = 1
2

4.7 Products and quotients of random variables
‡±™
225
in the case 0 ‚â§x ‚â§1. If 1 ‚â§x < ‚àû, then p(xy) = 0 if y > 1/x, and it follows that
r(x) =
1/x
‚à´
0
y dy =
1
2x2
for those x. Combining both cases, the density r of U/V may be written as (see Figure 4.8)
r(x) =
{
{
{
{
{
{
{
{
{
1
2
if 0 ‚â§x ‚â§1,
1
2x2
if 1 < x < ‚àû,
0
otherwise .
(4.44)
Figure 4.8: The density r defined by eq. (4.44).
Question: Does there exist an easy geometric explanation for r(x) =
1
2 in the case
0 ‚â§x ‚â§1?
Answer: If t > 0, then
FU/V(t) = ‚Ñô{U/V ‚â§t} = ‚Ñô{U ‚â§t V} = ‚Ñô(U,V)(Bt) ,
where now
Bt := {(u, v) ‚àà[0, 1]2 : 0 ‚â§u ‚â§v t} .
(4.45)
If 0 < t ‚â§1, then Bt is a triangle in [0, 1]2 with area vol2(Bt) = t
2. The independence of U
and V implies (cf. Example 3.6.23) that ‚Ñô(U,V) is the uniform distribution on [0, 1]2, hence
FU/V(t) = ‚Ñô(U,V)(Bt) = vol2(Bt) = t
2 ,
0 < t ‚â§1 ,

226
‡±™
4 Operations on random variables
leading to r(t) = F‚Ä≤
U/V(t) = 1
2 for those t. See Figure 4.9 for a geometric explanation of
this fact.
Figure 4.9: The set Bt, defined in eq. (4.45). Here 0 ‚â§t ‚â§1 in the left-hand figure, and 1 < t < ‚àûin the
right-hand one. The area of Bt is either t
2 or 1 ‚àí1
2t , respectively.
4.7.1 Student‚Äôs t-distribution
Let us use Proposition 4.7.3 to compute the density of a distribution which plays a crucial
role in Mathematical Statistics.
Proposition 4.7.5. Let X be ùí©(0, 1)-distributed and Y be independent of X and œá2
n-dis-
tributed for some n ‚â•1. Define the random variable Z as
Z :=
X
‚àöY/n
.
Then Z possesses the density r given by (see Figure 4.10 for the graphs of these functions
in the cases n = 1,, n = 2, and n = 8)
r(x) =
Œì( n+1
2 )
‚àön œÄ Œì( n
2 ) (1 + x2
n )
‚àín/2 ‚àí1/2
,
x ‚àà‚Ñù.
(4.46)
Proof. In the first step, we determine the density of ‚àöY with Y distributed according
to œá2
n. If t > 0, then
F‚àöY(t) = ‚Ñô{‚àöY ‚â§t} = ‚Ñô{Y ‚â§t2} =
1
2n/2 Œì( n
2 )
t2
‚à´
0
xn/2 ‚àí1 e‚àíx/2 dx .
Thus, if t > 0, then the density q of ‚àöY equals
q(t) = d
dt F‚àöY(t) =
1
2n/2 Œì( n
2 ) (2t) tn‚àí2 e‚àít2/2 =
1
2n/2 ‚àí1 Œì( n
2 ) tn‚àí1 e‚àít2/2 .
(4.47)

4.7 Products and quotients of random variables
‡±™
227
Figure 4.10: From bottom to top, these are the densities of t1, t2, and t8 distributions.
Of course, we have q(t) = 0 if t ‚â§0.
In the second step, we determine the density ÃÉr of ÃÉZ = Z/‚àön = X/‚àöY. An application
of Proposition 4.7.3 for p(x) =
1
‚àö2œÄ e‚àíx/2 and q given in eq. (4.47) leads to
ÃÉr(x) =
‚àû
‚à´
0
y[
1
‚àö2œÄ
e‚àí(xy)2/2][
1
2n/2 ‚àí1 Œì( n
2 ) yn‚àí1 e‚àíy2/2] dy
=
1
‚àöœÄ 2n/2 ‚àí1/2 Œì( n
2 )
‚àû
‚à´
0
yn e‚àí(1+x2)y2/2 dy .
(4.48)
Change the variables in the last integral by setting v = y2
2 (1 + x2). Then y =
‚àö2v
(1+x2)1/2
and, consequently, dy =
1
‚àö2 v‚àí1/2 (1 + x2)‚àí1/2 dv. Inserting this into eq. (4.48) shows that
ÃÉr(x) =
1
‚àöœÄ 2n/2 Œì( n
2 )
‚àû
‚à´
0
2n/2 vn/2 ‚àí1/2 e‚àív
(1 + x2)n/2 +1/2 dv
=
Œì( n+1
2 )
‚àöœÄ Œì( n
2 ) (1 + x2)
‚àín/2 ‚àí1/2 .
(4.49)
In the third step, we finally obtain the density r of Z. Since Z = ‚àön ÃÉZ, formula (4.3)
applies with b = 0 and a = ‚àön. Thus, by eq. (4.49) for ÃÉr, as asserted,
r(x) =
1
‚àön
ÃÉr( x
‚àön) =
Œì( n+1
2 )
‚àön œÄ Œì( n
2 ) (1 + x2
n )
‚àín/2 ‚àí1/2
.

228
‡±™
4 Operations on random variables
Definition 4.7.6. The probability measure on (‚Ñù, ‚Ñ¨(‚Ñù)) with density r, given by eq. (4.46), is called the
tn-distribution or Student‚Äôs t-distribution with n degrees of freedom. A random variable Z is said to
be tn-distributed (or t-distributed with n degrees of freedom), provided its probability distribution is a
tn-distribution, that is, for a < b,
‚Ñô{a ‚â§Z ‚â§b} =
Œì( n+1
2 )
‚àön œÄ Œì( n
2 )
b
‚à´
a
(1 + x2
n )
‚àín/2 ‚àí1/2
dx .
Remark 4.7.7. The t1-distribution coincides with the Cauchy distribution introduced in
Section 1.6.8. Observe that Œì(1/2) = ‚àöœÄ and Œì (1) = 1.
In view of Definition 4.7.6, we may now formulate Proposition 4.7.5 as follows.
Proposition 4.7.8. If X and Y are independent and ùí©(0, 1) and œá2
n distributed, then
X
‚àöY/n
is tn-distributed.
Proposition 4.6.17 leads still to another version of Proposition 4.7.5.
Proposition 4.7.9. If X, X1, . . . , Xn are independent ùí©(0, 1)-distributed, then
X
‚àö1
n ‚àën
i=1 X2
i
is tn-distributed.
Corollary 4.7.10. If X and Y are independent and ùí©(0, 1)-distributed, then X/|Y| pos-
sesses a Cauchy distribution.
Proof. An application of Proposition 4.7.9 with n = 1 and X1 = Y implies that X/|Y| is
t1-distributed. We saw in Remark 4.7.7 the t1 and the Cauchy distributions coincide, thus,
X/|Y| is also Cauchy distributed.
4.7.2 F-distribution
We present now another important class of probability measures or probability distri-
butions playing a central role in Mathematical Statistics.
Proposition 4.7.11. For two natural numbers m and n, let X and Y be independent and
œá2
m- and œá2
n-distributed. Then Z := X/m
Y/n has the distribution density r defined by
r(x) =
{
{
{
0
if x ‚â§0,
mm/2 nn/2 ‚ãÖ
Œì( m+n
2 )
Œì( m
2 )Œì( n
2 ) ‚ãÖ
xm/2‚àí1
(mx+n)(m+n)/2
if x > 0.
(4.50)
Proof. We first evaluate the density ÃÉr of ÃÉZ = X/Y. To this end, we apply Proposition 4.7.3
with functions p and q given by

4.7 Products and quotients of random variables
‡±™
229
p(x) =
1
2m/2 Œì(m/2) xm/2 ‚àí1 e‚àíx/2
and
q(y) =
1
2n/2 Œì(n/2) yn/2 ‚àí1 e‚àíy/2
whenever x, y > 0. Then we get
ÃÉr(x) =
1
2(m+n)/2 Œì(m/2) Œì(n/2)
‚àû
‚à´
0
y (xy)m/2 ‚àí1 yn/2 ‚àí1 e‚àíxy/2 e‚àíy/2 dy
=
xm/2 ‚àí1
2(m+n)/2 Œì(m/2) Œì(n/2)
‚àû
‚à´
0
y(m+n)/2 ‚àí1 e‚àíy(1+x)/2 dy .
(4.51)
We replace in eq. (4.51) the variable y by u = y(1 + x)/2, thus, dy =
2
1+x du. Inserting
this into eq. (4.51), the last expression transforms to
ÃÉr(x) =
xm/2 ‚àí1
Œì(m/2) Œì(n/2) (1 + x)‚àí(n+m)/2
‚àû
‚à´
0
u(m+n)/2 ‚àí1 e‚àíu du
=
Œì( m+n
2 )
Œì( m
2 )Œì( n
2 ) ‚ãÖ
xm/2 ‚àí1
(1 + x)(m+n)/2 .
Because of Z = n
m ‚ãÖÃÉZ, we obtain the density r of Z by Proposition 1.7.21. Indeed, then
r(x) = m
n
ÃÉr(mx
n ) = mm/2 nn/2 ‚ãÖ
Œì( m+n
2 )
Œì( m
2 )Œì( n
2 ) ‚ãÖ
xm/2 ‚àí1
(mx + n)(m+n)/2 ,
as asserted.
Remark 4.7.12. Using relation (1.62) between the beta and gamma functions, the density
r of Z may also be written as
r(x) = mm/2 nn/2
B( m
2 , n
2 ) ‚ãÖ
xm/2 ‚àí1
(mx + n)(m+n)/2 ,
x > 0 .
For the behavior of r(x) as x ‚Üí0, we refer to Problem 4.17. For certain parameters m
and n the graphs of these densities can be found in Figure 4.11.
Definition 4.7.13. The probability measure on (‚Ñù, ‚Ñ¨(‚Ñù)) with density r defined by eq. (4.50) is called the
Fisher‚ÄìSnecedor distribution or F-distribution (with m and n degrees of freedom).
A random variable Z is F-distributed (with m and n degrees of freedom), provided its probability
distribution is an F-distribution. Equivalently, if 0 ‚â§a < b, then
‚Ñô{a ‚â§Z ‚â§b} = mm/2 nn/2 ‚ãÖ
Œì( m+n
2 )
Œì( m
2 )Œì( n
2 )
b
‚à´
a
xm/2 ‚àí1
(mx + n)(m+n)/2 dx .
The random variable Z is also said to be Fm,n-distributed.

230
‡±™
4 Operations on random variables
Figure 4.11: Densities of F2,3 , F3,1, and F4,6 distributed random variables.
With this notation, Proposition 4.7.11 may now be formulated as follows:
Proposition 4.7.14. If two independent random variables X and Y are œá2
m and œá2
n dis-
tributed, then X/m
Y/n is Fm,n-distributed.
Finally, Proposition 4.6.17 implies the following version of the previous result.
Proposition 4.7.15. Let X1, . . . , Xm, Y1, . . . , Yn be independent ùí©(0, 1)-distributed. Then
Z = (X2
1 + ‚ãÖ‚ãÖ‚ãÖ+ X2
m)/m
(Y 2
1 + ‚ãÖ‚ãÖ‚ãÖ+ Y 2
n)/n =
1
m ‚àëm
i=1 X2
i
1
n ‚àën
j=1 Y 2
j
is Fm,n-distributed.
Corollary 4.7.16. If a random variable Z is Fm,n-distributed, then 1/Z possesses an Fn,m
distribution.
Proof. This is an immediate consequence of Proposition 4.7.11.
Summary: A random variable X is tn-distributed provided that for all a < b we have
‚Ñô{a ‚â§X ‚â§b} =
Œì( n+1
2 )
‚àön œÄ Œì( n
2 )
b
‚à´
a
(1 + x2
n )
‚àín/2 ‚àí1/2
dx .
If X, X1, . . . , Xn are independent ùí©(0, 1)-distributed, then
X
‚àö1
n ‚àën
i=1 X2
i
is tn-distributed.
A random variable X is Fm,n-distributed if, whenever 0 ‚â§a < b < ‚àû, one has

4.8 Problems
‡±™
231
‚Ñô{a ‚â§X ‚â§b} = mm/2 nn/2 ‚ãÖ
Œì( m+n
2 )
Œì( m
2 )Œì( n
2 )
b
‚à´
a
xm/2 ‚àí1
(mx + n)(m+n)/2 dx .
If X1, . . . , Xm, Y1, . . . , Yn are independent ùí©(0, 1)-distributed, then
1
m ‚àëm
i=1 X2
i
1
n ‚àën
j=1 Y2
j
is Fm,n-distributed.
4.8 Problems
Problem 4.1. Let U be uniformly distributed on [0, 1]. Which distributions do the fol-
lowing random variables possess:
min{U, 1 ‚àíU},
max{U, 1 ‚àíU},
|2U ‚àí1|,
and
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
U ‚àí1
3
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
?
Problem 4.2 (Generating functions). Let X be a random variable with values in ‚Ñï0. For
k ‚àà‚Ñï0, let pk = ‚Ñô{X = k}. Then its generating function œÜX is defined by
œÜX(t) =
‚àû
‚àë
k=0
pk tk .
1.
Show that œÜX(t) exists if |t| ‚â§1.
2.
Let X and Y be two independent random variables with values in ‚Ñï0. Prove that
then
œÜX+Y = œÜX ‚ãÖœÜY .
3.
Compute œÜX in each of the following cases:
(a) X is uniformly distributed on {1, . . . , N} for some N ‚â•1.
(b) X is Bn,p-distributed for some n ‚â•1 and p ‚àà[0, 1].
(c) X is PoisŒª-distributed for some Œª > 0.
(d) X is Gp-distributed for a certain 0 < p < 1.
(e) X is B‚àí
n,p-distributed.
Problem 4.3. Roll two dice simultaneously. Let X be the result of the first die and Y that
of the second. Is it possible to falsify these two dice in such a way that X +Y is uniformly
distributed on {2, . . . , 12}? It is not assumed that both dice are falsified in the same way.
Hint: One possible way to answer this question is as follows: investigate the gener-
ating functions of X and Y and compare their product with the generating function of
the uniform distribution on {2, . . . , 12}.
Problem 4.4. Let X1, . . . , Xn be a sequence of independent identically distributed ran-
dom variables with common distribution function F and distribution density p, that is,

232
‡±™
4 Operations on random variables
‚Ñô{Xj ‚â§t} = F(t) =
t
‚à´
‚àí‚àû
p(x) dx ,
j = 1, . . . , n .
Define random variables X‚àóand X‚àóby
X‚àó:= min{X1, . . . , Xn}
and
X‚àó:= max{X1, . . . , Xn} .
1.
Determine the distribution functions and densities of X‚àóand X‚àódirectly, that is,
without using general results about order statistics.
2.
Describe the distribution of the random variables X‚àóand X‚àóin the case that the Xjs
are exponentially distributed with parameter Œª > 0.
3.
Suppose now the Xjs are uniformly distributed on [0, 1]. Describe the distribution
of X‚àóand X‚àóin this case.
Problem 4.5. Let F be the distribution function of the uniform distribution on {1, . . . , N}
for some N ‚àà‚Ñï. Determine its pseudoinverse function F‚àí. Do the same if F is either
the distribution function of a binomial or a Poisson distribution. Given U uniformly
distributed on [0, 1], how is F‚àí(U) distributed in each of these cases?
Problem 4.6. Find a function f from (0, 1) to ‚Ñùsuch that
‚Ñô{f (U) = k} = 1
2k ,
k = 1, 2, . . .
for U uniformly distributed on [0, 1].
Problem 4.7. Let U be uniform distributed on [0, 1]. Find functions f and g such that
X = f (U) and Y = g(U) have the distribution densities p and q with
p(x) := {0
if x ‚àâ(0, 1],
x‚àí1/2
2
if x ‚àà(0, 1]
and
q(x) :=
{
{
{
{
{
{
{
0
if |x| > 1,
x + 1
if ‚àí1 ‚â§x ‚â§0,
1 ‚àíx
if 0 < x ‚â§1.
Problem 4.8. Let X and Y be independent random variables with
‚Ñô{X = k} = ‚Ñô{Y = k} = 1
2k ,
k = 1, 2, . . .
How is X + Y distributed?
Problem 4.9. The number of customers visiting a shop per day is Poisson distributed
with parameter Œª > 0. The probability that a single customer buys something equals p
for a given 0 ‚â§p ‚â§1. Let X be the number of customers per day buying some goods.
Determine the distribution of X.

4.8 Problems
‡±™
233
Remark: We assume that the decision whether or not a single customer buys some-
thing is independent of the number of daily visitors.
A different way to formulate the above question is as follows: let X0, X1, . . . be inde-
pendent random variables with ‚Ñô{X0 = 0} = 1,
‚Ñô{Xj = 1} = p,
and
‚Ñô{Xj = 0} = 1 ‚àíp ,
j = 1, 2, . . . ,
for a certain p ‚àà[0, 1]. Furthermore, let Y be a Poisson-distributed random variable with
parameter Œª > 0, independent of the Xjs. Determine the distribution of
X :=
Y
‚àë
j=0
Xj .
Hint: Use the ‚Äúinfinite‚Äù version of the law of total probability as stated in Problem 2.5.
Problem 4.10. Suppose X and Y are independent and exponentially distributed with
parameter Œª > 0. Find the distribution densities of X ‚àíY and X/Y.
Problem 4.11. Two random variables U and V are independent and uniformly dis-
tributed on [0, 1]. Given n ‚àà‚Ñï, find the distribution density of U + nV.
Problem 4.12. Let X and Y be independent random variable distributed according to
PoisŒª and PoisŒº, respectively. Given n ‚àà‚Ñï0 and some k ‚àà{0, . . . , n}, prove
‚Ñô{X = k | X + Y = n} = (n
k) (
Œª
Œª + Œº)
k
(
Œº
Œª + Œº)
n‚àík
= Bn,p({k})
with p =
Œª
Œª+Œº.
Reformulation of the preceding problem: An owner of two stores, say store A and
store B, observes that the number of customers in each of these stores is independent
and PoisŒª and PoisŒº distributed. One day he was told that there were n customers in both
stores together. What is the probability that k of the n customers were in store A, hence
n ‚àík in store B?
Problem 4.13. Let X and Y be independent standard normal variables. Show that X/Y
is Cauchy distributed.
Hint: Use Corollary 4.7.10 and the fact that the vectors (X, Y), (‚àíX, Y), (X, ‚àíY), and
(‚àíX, ‚àíY) are identically distributed. Note that the probability distribution of each of
these two-dimensional vectors is the (two-dimensional) standard normal distribution.
Problem 4.14. Let X and Y be independent Gp-distributed. Find the probability distri-
bution of X ‚àíY.
Hint: Compare with Example 4.5.5. There we evaluated the distribution of X ‚àíY in
the case p = 1
2.

234
‡±™
4 Operations on random variables
Problem 4.15. Let U and V be independent random variables, both uniformly dis-
tributed on [‚àí1, 1]. Determine the density of U ‚ãÖV.
Hint: Use the technique presented in Example 4.7.2 and its geometric explanation
in Figure 4.7. To this end, treat the cases UV > 0 and UV ‚â§0 separately.
Problem 4.16. Suppose X is a random variable with values in (a, b) ‚äÜ‚Ñùand with den-
sity p. Let f from (a, b) ‚Üí‚Ñùbe (strictly) monotone and differentiable. Give a formula
for q, the density of f (X).
Use your result to evaluate the densities of eX and e‚àíX where X is distributed ac-
cording to ùí©(0, 1).
Problem 4.17. Let r be the density of an Fm,n-distributed random variable given by
eq. (4.50). Argue why r(x) ‚Üí‚àûas x ‚Üí0 if m = 1 and n ‚â•1. Why do we have r(0) = 1 if
m = 2 and n ‚â•1? Evaluate r(0) if n = 1, 2, . . . and m ‚â•3.

5 Expected value, variance, and covariance
5.1 Expected value
5.1.1 Expected value of discrete random variables
What is an expected value (also called mean value or expectation) of a random variable?
How is it defined? Which property of the random variable does it describe and how it
can be computed? Does every random variable possess an expected value? To answer
these questions, let us start with an example.
Example 5.1.1. Suppose N students attend a certain exam. The number of possible
points is 100. Given j = 0, 1, . . . , 100, let nj be the number of students who achieved j
points. Now choose randomly, according to the uniform distribution (a single student
is chosen with probability 1/N), one student. Name him or her œâ, and define X(œâ) as
the number of points that the chosen student achieved. Then X is a random variable
with values in D = {0, 1, . . . , 100}. How is X distributed? Since X has values in D, its
distribution is described by the probabilities
pj = ‚Ñô{X = j} =
nj
N ,
j = 0, 1, . . . , 100 .
(5.1)
As expected value of X, we take the average number A of points in this exam. How is A
evaluated? The easiest way to do this is
A = 1
N
100
‚àë
j=0
j ‚ãÖnj =
100
‚àë
j=0
j ‚ãÖ
nj
N =
100
‚àë
j=0
j ‚ãÖpj ,
where the pjs are defined by eq. (5.1). If we write ùîºX for the expected value (or mean
value) of X, and if we assume that this value coincides with A, then the preceding equa-
tion says
ùîºX =
100
‚àë
j=0
j pj =
100
‚àë
j=0
j ‚Ñô{X = j} =
100
‚àë
j=0
xj ‚Ñô{X = xj} ,
where the xj = j with j = 0, . . . , 100 denote the possible values of X.
In view of this example, the following definition for the expected value of a dis-
crete random variable X looks feasible. Suppose X has values in D = {x1, x2, . . . }, and let
pj = ‚Ñô{X = xj}, j = 1, 2, . . . . Then the expected value ùîºX of X is given by
ùîºX =
‚àû
‚àë
j=1
xj pj =
‚àû
‚àë
j=1
xj ‚Ñô{X = xj} .
(5.2)
Unfortunately, the sum in eq. (5.2) does not always exist. In order to overcome this diffi-
culty, let us recall some basic facts about infinite series of real numbers.
https://doi.org/10.1515/9783111325064-005

236
‡±™
5 Expected value, variance, and covariance
A sequence (Œ±j)j‚â•1 of real numbers is called summable, provided its sequence of
partial sums (sn)n‚â•1 with sn = Œ±1 + ‚ãÖ‚ãÖ‚ãÖ+ Œ±n converges in ‚Ñù. Then one defines
‚àû
‚àë
j=1
Œ±j = lim
n‚Üí‚àûsn = lim
n‚Üí‚àû
n
‚àë
j=1
Œ±j .
If the sequence of partial sums diverges, nevertheless, in some cases we may assign to
the infinite series a limit. If either limn‚Üí‚àûsn = ‚àí‚àûor limn‚Üí‚àûsn = ‚àû, then we write
‚àë‚àû
j=1 Œ±j = ‚àí‚àûor ‚àë‚àû
j=1 Œ±j = ‚àû, respectively. In particular, if Œ±j ‚â•0 for j ‚àà‚Ñï, then the
sequence of partial sums is nondecreasing, which implies that only two different cases
may occur: Either ‚àë‚àû
j=1 Œ±j < ‚àû(in this case the sequence is summable) or ‚àë‚àû
j=1 Œ±j = ‚àû.
Let (Œ±j)j‚â•1 be an arbitrary sequence of real numbers. If ‚àë‚àû
j=1 |Œ±j| < ‚àû, then it is called
absolutely summable. Note that each absolutely summable sequence is summable.
This is a direct consequence of Cauchy‚Äôs convergence criterion. The converse implica-
tion is wrong, as can be seen by considering ((‚àí1)n/n)n‚â•1.
Now we are prepared to define the expected value of a nonnegative random vari-
able.
Definition 5.1.2. Let X be a discrete random variable with values in {x1, x2, . . . } for some xj ‚â•0. Equiva-
lently, the random variable X is discrete with X ‚â•0. Then the expected value of X is defined by
ùîºX :=
‚àû
‚àë
j=1
xj ‚Ñô{X = xj} .
(5.3)
Remark 5.1.3. Since xj ‚Ñô{X = xj} ‚â•0 for nonnegative X, for those random variables
the sum in eq. (5.3) is always well defined, but may be infinite. That is, each nonnegative
discrete random variable X possesses an expected value ùîºX ‚àà[0, ‚àû].
Let us now turn to the case of arbitrary (not necessarily nonnegative) random vari-
ables. The next example shows which problems may arise.
Example 5.1.4. We consider the probability measure introduced in Example 1.3.6 and
choose a random variable X with values in ‚Ñ§distributed according to the probability
measure in this example. In other words,
‚Ñô{X = k} = 3
œÄ2
1
k2 ,
k ‚àà‚Ñ§\{0} .
If we try to evaluate the expected value of X by formula (5.2), then this leads to the
undetermined expression
3
œÄ2
‚àû
‚àë
k=‚àí‚àû
k Ã∏=0
k
k2 = 3
œÄ2 lim
n‚Üí‚àû
m‚Üí‚àû
n
‚àë
k=‚àím
1
k = 3
œÄ2 [ lim
n‚Üí‚àû
n
‚àë
k=1
1
k + lim
m‚Üí‚àû
1
‚àë
k=‚àím
1
k ]
= 3
œÄ2 [ lim
n‚Üí‚àû
n
‚àë
k=1
1
k ‚àílim
m‚Üí‚àû
m
‚àë
k=1
1
k ] = ‚àû‚àí‚àû.

5.1 Expected value
‡±™
237
To exclude phenomenons as in Example 5.1.4, we suppose that a random variable
has to meet the following condition.
Definition 5.1.5. Let X be discrete with values in {x1, x2, . . . } ‚äÇ‚Ñù. Then the expected value of X exists,
provided that
ùîº|X| =
‚àû
‚àë
j=1
|xj| ‚Ñô{X = xj} < ‚àû.
(5.4)
We mentioned above that an absolutely summable sequence is summable. Hence, under
assumption (5.4), the sum in the subsequent definition is a well-defined real number.
Definition 5.1.6. Let X be a discrete random variable satisfying ùîº|X| < ‚àû. Then its expected value is
defined as
ùîºX =
‚àû
‚àë
j=1
xj ‚Ñô{X = xj} .
(5.5)
As before, the numbers x1, x2, . . . in formula (5.5) are the possible values of X. For exam-
ple, if X attains values in {1, 4, 9}, then we may choose x1 = 1, x2 = 4, and x3 = 9. But we
could also take x1 = 4, x2 = 9, and x3 = 1, and so on. Every time we get the same expected
value.
Example 5.1.7. We start with an easy example that demonstrates how to compute the
expected value in concrete cases. If the distribution of a random variable X is defined as
‚Ñô{X = ‚àí1} = 1/6, ‚Ñô{X = 0} = 1/8, ‚Ñô{X = 1} = 3/8, and ‚Ñô{X = 2} = 1/3, then its expected
value equals
ùîºX = (‚àí1) ‚ãÖ‚Ñô{X = ‚àí1} + 0 ‚ãÖ‚Ñô{X = 0} + 1 ‚ãÖ‚Ñô{X = 1} + 2 ‚ãÖ‚Ñô{X = 2}
= ‚àí1
6 + 3
8 + 2
3 = 7
8 .
Example 5.1.8. The next example shows that ùîºX = ‚àûmay occur even for quite natural
random variables. Thus, let us come back to the model presented in Example 1.4.47.
There we developed a strategy how to always win one dollar in a series of games. The
basic idea was, after losing a game, next time one doubles the amount in the pool. As
in Example 1.4.47, let X(k) be the amount of money needed when winning for the first
time in game k. We obtained
‚Ñô{X = 2k ‚àí1} = p(1 ‚àíp)k‚àí1 ,
k = 1, 2, . . .
Recall that 0 < p < 1 is the probability to win a single game. We ask for the expected
value of money needed to apply this strategy. It follows that
ùîºX =
‚àû
‚àë
k=1
(2k ‚àí1)‚Ñô{X = 2k ‚àí1} = p
‚àû
‚àë
k=1
(2k ‚àí1)(1 ‚àíp)k‚àí1 .
(5.6)

238
‡±™
5 Expected value, variance, and covariance
If the game is fair, that is, if p = 1/2, then this leads to
ùîºX =
‚àû
‚àë
k=1
2k ‚àí1
2k
= ‚àû,
because of (2k ‚àí1)/2k ‚Üí1 as k ‚Üí‚àû. This yields ùîºX = ‚àûfor all1 p ‚â§1/2.
Let us sum up: if p ‚â§1/2 (which is the case in all provided games), the obtained
result tells us that the average amount of money needed, to use this strategy, is arbitrarily
large. The owners of gambling casinos know this strategy as well. Therefore, they limit
the possible amount of money in the pool. For example, if the largest possible stakes is
N dollars, then the strategy breaks down as soon as one loses n games for some n with
2n > N. And, as our calculations show, on average this always happens.
Remark 5.1.9. If p > 1/2, then the average amount of money needed is finite, and it can
be calculated by
ùîºX = p
‚àû
‚àë
k=1
(2k ‚àí1)(1 ‚àíp)k‚àí1 = 2p
‚àû
‚àë
k=0
(2 ‚àí2p)k ‚àíp
‚àû
‚àë
k=0
(1 ‚àíp)k
=
2p
1 ‚àí(2 ‚àí2p) ‚àí
p
1 ‚àí(1 ‚àíp) =
2p
2p ‚àí1 ‚àí1 =
1
2p ‚àí1 .
5.1.2 Expected value of certain discrete random variables
The aim of this section is to compute the expected value of the most interesting discrete
random variables. We start with uniformly distributed ones.
Proposition 5.1.10. Let X be uniformly distributed on the set {x1, . . . , xN} of real numbers.
Then it follows that
ùîºX = 1
N
N
‚àë
j=1
xj .
(5.7)
That is, ùîºX is the arithmetic mean of the xjs.
Proof. This is an immediate consequence of ‚Ñô{X = xj} = 1/N, implying
ùîºX =
N
‚àë
j=1
xj ‚ãÖ‚Ñô{X = xj} =
N
‚àë
j=1
xj ‚ãÖ1
N .
Remark 5.1.11. For general discrete random variables X with values x1, x2, . . . , their ex-
pected value ùîºX may be regarded as a weighted (the weights are the pjs) mean of the xjs.
1 If p ‚â§1/2 then 1 ‚àíp ‚â•1/2, hence the sum in eq. (5.6) becomes bigger and, therefore, it also diverges.

5.1 Expected value
‡±™
239
Example 5.1.12. Let X be uniformly distributed on {1, . . . , 6}. Then X is a model for
rolling a fair die. Its expected value is, as is well known,
ùîºX = 1 + ‚ãÖ‚ãÖ‚ãÖ+ 6
6
= 21
6 = 7
2 .
Next we determine the expected value of a binomial distributed random variable.
Proposition 5.1.13. Let X be binomial distributed with parameters n and p. Then we get
ùîºX = n p .
(5.8)
Proof. The possible values of X are 0, . . . , n. Thus, it follows that
ùîºX =
n
‚àë
k=0
k ‚ãÖ‚Ñô{X = k} =
n
‚àë
k=1
k ‚ãÖ(n
k) pk(1 ‚àíp)n‚àík
=
n
‚àë
k=1
n!
(k ‚àí1)! (n ‚àík)! pk(1 ‚àíp)n‚àík
= n p
n
‚àë
k=1
(n ‚àí1)!
(k ‚àí1)!(n ‚àík)! pk‚àí1(1 ‚àíp)n‚àík .
Shifting the index from k ‚àí1 to k in the last sum implies
ùîºX = np
n‚àí1
‚àë
k=0
(n ‚àí1)!
k! (n ‚àí1 ‚àík)! pk(1 ‚àíp)n‚àí1‚àík
= np
n‚àí1
‚àë
k=0
(n ‚àí1
k )pk(1 ‚àíp)n‚àí1‚àík = np [p + (1 ‚àíp)]
n‚àí1 = np .
This completes the proof.
Remark 5.1.14. The previous result tells us the following: if we perform n independent
trials of an experiment with success probability 0 ‚â§p ‚â§1, then on average we will
observe np successes.
Example 5.1.15. One kilogram of a radioactive material consists of N atoms. The atoms
decay independently of each other and, moreover, the lifetime of each of the atoms is
exponentially distributed with some parameter Œª > 0. We ask for the time T0 > 0, at
which, on average, half of the atoms are decayed; T0 is usually called radioactive half-
life.
Answer: If T > 0, then the probability that a single atom decays before time T is
given by
p(T) = EŒª([0, T]) = 1 ‚àíeŒªT.

240
‡±™
5 Expected value, variance, and covariance
Since the atoms decay independently, the number of atoms decaying before time T is
BN,p(T)-distributed. Therefore, by Proposition 5.1.13, the expected value of decayed atoms
equals N ‚ãÖp(T) = N(1 ‚àíeŒªT). Hence, T0 has to satisfy
N(1 ‚àíe‚àíŒªT0) = N
2 ,
leading to T0 = ln 2/Œª. Conversely, if we know T0 and want to determine Œª, then Œª =
ln 2/T0. Consequently, the probability that a single atom decays before time T > 0 can
also be described by
EŒª([0, T]) = 1 ‚àíe‚àíT ln 2/T0 = 1 ‚àí2‚àíT/T0 .
Next, we determine the expected value of Poisson distributed random variables.
Proposition 5.1.16. For some Œª > 0, let X be distributed according to PoisŒª. Then it follows
that ùîºX = Œª.
Proof. The possible values of X are 0, 1, . . . Hence, the expected value is given by
ùîºX =
‚àû
‚àë
k=0
k ‚ãÖ‚Ñô{X = k} =
‚àû
‚àë
k=1
k Œªk
k! e‚àíŒª = Œª
‚àû
‚àë
k=1
Œªk‚àí1
(k ‚àí1)! e‚àíŒª ,
which transforms by a shift of the index to
Œª [
‚àû
‚àë
k=0
Œªk
k! ] e‚àíŒª = Œª[eŒª] e‚àíŒª = Œª ,
proving the assertion.
Interpretation: Proposition 5.1.16 explains the role of the parameter Œª in the defi-
nition of the Poisson distribution. Whenever certain numbers are Poisson distributed,
then Œª > 0 is the average of the observed values. For example, if the number of accidents
per week is known to be PoisŒª-distributed, then the parameter Œª is determined by the
average number of accidents per week in the past. Or, as we already mentioned in Ex-
ample 4.6.6, the number of raisins in a piece of œÅ pounds of dough is PoisŒªœÅ-distributed,
where Œª is the proportion of raisins per pound dough, hence ŒªœÅ is the average number
of raisins per œÅ pounds.
Example 5.1.17. Let us once more take a look at Example 4.6. There we considered light
bulbs with EŒª-distributed lifetime. Every time a bulb burned out, we replaced it by a new
one of the same type. It turned out that the number of necessary replacements until time
T > 0 was PoisŒªT-distributed. Consequently, by Proposition 5.1.16, on average, until time
T we have to change the light bulbs ŒªT times.

5.1 Expected value
‡±™
241
Finally, we compute the expected value of a negative binomial distributed random
variable. According to Definition 1.4.49, a random variable X is B‚àí
n,p-distributed if
‚Ñô{X = k} = (k ‚àí1
k ‚àín)pn (1 ‚àíp)k‚àín ,
k = n, n + 1, . . .
or, equivalently, if
‚Ñô{X = k + n} = (‚àín
k )pn (p ‚àí1)k ,
k = 0, 1, . . .
(5.9)
Proposition 5.1.18. Suppose X is B‚àí
n,p-distributed for some n ‚â•1 and p ‚àà(0, 1). Then
ùîºX = n
p .
Proof. Using eq. (5.9), the expected value of X is computed as
ùîºX =
‚àû
‚àë
k=n
k ‚Ñô{X = k} =
‚àû
‚àë
k=0
(k + n) ‚Ñô{X = k + n}
= pn
‚àû
‚àë
k=1
k (‚àín
k ) (p ‚àí1)k + n pn
‚àû
‚àë
k=0
(‚àín
k ) (p ‚àí1)k .
(5.10)
To evaluate the two sums in eq. (5.10), we use Proposition A.5.2, which asserts
1
(1 + x)n =
‚àû
‚àë
k=0
(‚àín
k ) xk
(5.11)
for |x| < 1. Applying this with x = p ‚àí1 (recall 0 < p < 1) yields
n pn
‚àû
‚àë
k=0
(‚àín
k ) (p ‚àí1)k = n pn
1
(1 + (p ‚àí1))n = n .
(5.12)
Next we differentiate eq. (5.11) with respect to x and obtain
‚àín
(1 + x)n+1 =
‚àû
‚àë
k=1
k (‚àín
k ) xk‚àí1 ,
which, multiplying both sides by x, gives
‚àínx
(1 + x)n+1 =
‚àû
‚àë
k=1
k (‚àín
k ) xk .
(5.13)
Letting x = p ‚àí1 in eq. (5.13), the first sum in eq. (5.10) becomes
pn
‚àû
‚àë
k=1
k (‚àín
k ) (p ‚àí1)k = pn
‚àín(p ‚àí1)
(1 + (p ‚àí1))n+1 = n(1 ‚àíp)
p
.
(5.14)

242
‡±™
5 Expected value, variance, and covariance
Finally, we combine eqs. (5.10), (5.12), and (5.14) to obtain
ùîºX = n(1 ‚àíp)
p
+ n = n
p,
as claimed.
Remark 5.1.19. Proposition 5.1.18 asserts that, on average, the nth success occurs in trial
n/p. For example, rolling a die, on average, the first appearance of number ‚Äú6‚Äù will be
in trial 6, the second one in trial 12, and so on.
Corollary 5.1.20. If X is geometrically distributed with parameter p, then
ùîºX = 1
p .
(5.15)
Proof. Recall that Gp = B‚àí
1,p, hence X is B‚àí
1,p-distributed, and ùîºX =
1
p by Proposi-
tion 5.1.18.
Because of its beauty, let us give another, more direct proof of Corollary 5.1.20.
Suppose X is Gp-distributed. Then we write
ùîºX = p
‚àû
‚àë
k=1
k(1 ‚àíp)k‚àí1 = p
‚àû
‚àë
k=0
(k + 1)(1 ‚àíp)k
= (1 ‚àíp)
‚àû
‚àë
k=0
k p (1 ‚àíp)k‚àí1 + p
‚àû
‚àë
k=0
(1 ‚àíp)k = (1 ‚àíp) ùîºX + 1 .
Solving this equation with respect to ùîºX proves eq. (5.15), as asserted. Observe that this
alternative proof is based upon the knowledge of ùîºX < ‚àû. Otherwise, we could not
solve the equation for ùîºX. But, because of 0 < p < 1, this fact is an easy consequence of
ùîºX = p
‚àû
‚àë
k=1
k(1 ‚àíp)k‚àí1 < ‚àû.
Summary: Let X be a discrete random variable with values x1, x2, . . . Then
ùîºX =
‚àû
‚àë
j=1
xj ‚Ñô{X = xj}
provided that
ùîº|X| =
‚àû
‚àë
j=1
|xj| ‚Ñô{X = xj} < ‚àû.
5.1.3 Expected value of continuous random variables
Let X be a continuous random variable with distribution density p, that is, if t ‚àà‚Ñù,
then

5.1 Expected value
‡±™
243
‚Ñô{X ‚â§t} =
t
‚à´
‚àí‚àû
p(x) dx .
How to define ùîºX in this case?
To answer this question, let us present formula (5.3) in an equivalent way. Suppose
X maps Œ© into a set D ‚äÇ‚Ñù, which is either finite or countably infinite. Let p : ‚Ñù‚Üí[0, 1]
be the probability mass function of X introduced in eq. (3.4). Then the expected value of
X may also be written as
ùîºX = ‚àë
x‚àà‚Ñù
x p(x) .
In this form, the preceding formula suggests that in the continuous case the sum should
be replaced by an integral. This can made more precise by approximating continuous
random variables by discrete ones. But this is only a heuristic explanation; for a precise
approach, deeper convergence theorems for random variables are needed. Therefore,
we do not give more details here, we simply replace sums by integrals.
Doing so, for continuous random variables the following approach for the definition
of ùîºX might be taken. If p : ‚Ñù‚Üí[0, ‚àû) is the distribution density of X, set
ùîºX :=
‚àû
‚à´
‚àí‚àû
x p(x) dx .
(5.16)
However, here we have a similar problem as in the discrete case, namely that the integral
in eq. (5.16) need not exist. Therefore, let us give a short digression about the integrability
of real functions.
Let f : ‚Ñù‚Üí‚Ñùbe a function such that for all a < b the integral ‚à´
b
a f (x)dx is a
well-defined real number. Then
‚àû
‚à´
‚àí‚àû
f (x) dx := lim
a‚Üí‚àí‚àû
b‚Üí‚àû
b
‚à´
a
f (x) dx ,
(5.17)
provided both limits on the right-hand side of eq. (5.17) exist. In this case we call f inte-
grable (in the Riemann sense) on ‚Ñù.
Remark 5.1.21. Let us point out that the numbers a and b in eq. (5.17) tend indepen-
dently to ‚àí‚àûand ‚àû, respectively. For instance, as the example f (x) = x shows, it does
not suffice that limb‚Üí‚àû‚à´
b
‚àíb f (x) dx exists. Another way to express the existence of the
integral in eq. (5.17) is as follows: the two limits
lim
a‚Üí‚àí‚àû
0
‚à´
a
f (x) dx
and
lim
b‚Üí‚àû
b
‚à´
0
f (x) dx
(5.18)

244
‡±™
5 Expected value, variance, and covariance
have to exist, and if this is so, the integral ‚à´
‚àû
‚àí‚àûf (x) dx is defined as the sum of the two
limits in eq. (5.18).
If f (x) ‚â•0, x ‚àà‚Ñù, then the limit lim a‚Üí‚àí‚àû
b‚Üí‚àû‚à´
b
a f (x) dx always exists in a generalized
sense, that is, it may be finite (then f is integrable) or infinite, then this is expressed by
‚à´
‚àû
‚àí‚àûf (x) dx = ‚àû.
If ‚à´
‚àû
‚àí‚àû|f (x)| dx < ‚àû, then f is said to be absolutely integrable, and as in the case
of infinite series, absolutely integrable function are integrable. Note that x Û≥®É‚Üísin x/x is
integrable, but not absolutely integrable.
After this preparation, we come back to the definition of the expected value for
continuous random variables.
Definition 5.1.22. Let X be a random variable with distribution density p. If p(x) = 0 for x < 0, or, equiv-
alently, ‚Ñô{X ‚â•0} = 1, then the expected value of X is defined by
ùîºX :=
‚àû
‚à´
0
x p(x) dx .
(5.19)
Observe that under these conditions on p or X, we have xp(x) ‚â•0. Therefore, the in-
tegral in eq. (5.19) is always well defined, but might be infinite. In this case we write
ùîºX = ‚àû.
Let us turn now to the case of ‚Ñù-valued random variables. The following example
shows that the integral in eq. (5.16) may not exist, hence, in general, without an addi-
tional assumption the expected value cannot be defined by eq. (5.16).
Example 5.1.23. A random variable X is supposed to possess the density (check that this
is indeed a density function)
p(x) = {0
if ‚àí1 < x < 1,
1
2 x2
if |x| ‚â•1.
If we try to evaluate ùîºX by virtue of eq. (5.16), then, because of
‚àû
‚à´
‚àí‚àû
x p(x)dx = lim
a‚Üí‚àû
b‚Üí‚àû
b
‚à´
‚àía
x p(x) dx = 1
2[ lim
b‚Üí‚àû
b
‚à´
1
dx
x + lim
a‚Üí‚àû
‚àí1
‚à´
‚àía
dx
x ]
= 1
2[ lim
b‚Üí‚àû
b
‚à´
1
dx
x ‚àílim
a‚Üí‚àû
a
‚à´
1
dx
x ] = ‚àû‚àí‚àû,
we observe an undetermined expression. Thus, there is no meaningful way to introduce
an expected value for X.

5.1 Expected value
‡±™
245
We enforce the existence of the integral by the following condition.
Definition 5.1.24. Let X be a (real-valued) random variable with distribution density p. We say the ex-
pected value of X exists, provided p satisfies the following integrability condition:
ùîº|X| :=
‚àû
‚à´
‚àí‚àû
|x| p(x) dx < ‚àû.
(5.20)
Remark 5.1.25. At this point, it is not clear that the right-hand integral in (5.20) is indeed
the expected value of |X|. This will follow later on by Proposition 5.1.38. Nevertheless,
we use this notation before giving a proof.
Condition (5.20) says nothing but that f (x) := xp(x) is absolutely integrable. Hence,
as mentioned above, f is integrable, and the integral in the following definition is well
defined.
Definition 5.1.26. Suppose condition (5.20) is satisfied. Then the expected value of X is defined by
ùîºX :=
‚àû
‚à´
‚àí‚àû
x p(x) dx .
Summary: Let X be a continuous random variable with density p : ‚Ñù‚Üí‚Ñù. Then
ùîºX =
‚àû
‚à´
‚àí‚àû
x p(x) dx
provided that
ùîº|X| =
‚àû
‚à´
‚àí‚àû
|x| p(x) dx < ‚àû.
5.1.4 Expected value of certain continuous random variables
We start with computing the expected value of a uniformly distributed (continuous) ran-
dom variable.
Proposition 5.1.27. Let X be uniformly distributed on the finite interval I = [Œ±, Œ≤]. Then
ùîºX = Œ± + Œ≤
2
,
that is, the expected value is the midpoint of the interval I.
Proof. The distribution density of X is the function p defined as p(x) = (Œ≤ ‚àíŒ±)‚àí1 if x ‚ààI,
and p(x) = 0 if x ‚àâI. Of course, X possesses an expected value,2 which can be evaluated
by
2 The product |x|p(x) is bounded and nonzero only on a finite interval.

246
‡±™
5 Expected value, variance, and covariance
ùîºX =
‚àû
‚à´
‚àí‚àû
xp(x) dx =
Œ≤
‚à´
Œ±
x
Œ≤ ‚àíŒ± dx =
1
Œ≤ ‚àíŒ±[x2
2 ]
Œ≤
Œ±
= 1
2 ‚ãÖŒ≤2 ‚àíŒ±2
Œ≤ ‚àíŒ± = Œ± + Œ≤
2
.
This proves the proposition.
Next we determine the expected value of a gamma distributed random variable.
Proposition 5.1.28. Suppose X is ŒìŒ±,Œ≤-distributed with Œ±, Œ≤ > 0. Then its expected value is
ùîºX = Œ± Œ≤ .
Proof. Because of ‚Ñô{X ‚â•0} = 1, its expected value is well defined and computed by
ùîºX =
‚àû
‚à´
0
xp(x) dx =
1
Œ±Œ≤ Œì(Œ≤)
‚àû
‚à´
0
x ‚ãÖxŒ≤‚àí1 e‚àíx/Œ± dx
=
1
Œ±Œ≤ Œì(Œ≤)
‚àû
‚à´
0
xŒ≤ e‚àíx/Œ± dx .
(5.21)
The change of variables u := x/Œ± transforms eq. (5.21) into
ùîºX =
Œ±Œ≤+1
Œ±Œ≤ Œì(Œ≤)
‚àû
‚à´
0
uŒ≤ e‚àíu du =
Œ±Œ≤+1
Œ±Œ≤ Œì(Œ≤) ‚ãÖŒì(Œ≤ + 1) = Œ± Œ≤ ,
where we used eq. (1.50) in the last step. This completes the proof.
Corollary 5.1.29. Let X be EŒª-distributed for a certain Œª > 0. Then
ùîºX = 1
Œª .
Proof. Note that EŒª = ŒìŒª‚àí1,1.
Example 5.1.30. The lifetime of a special type of light bulbs is exponentially distributed.
Suppose the average lifetime constitutes 100 units of time. This implies Œª = 1/100, hence,
if X describes the lifetime, then
‚Ñô{X ‚â§t} = 1 ‚àíe‚àít/100 ,
t ‚â•0 .
For example, the probability that the light bulb burns longer than 200 time units equals
‚Ñô{X ‚â•200} = e‚àí200/100 = e‚àí2 = 0.135335 . . .
Remark 5.1.31. If we evaluate in the previous example
‚Ñô{X ‚â•ùîºX} = ‚Ñô{X ‚â•100} = e‚àí1 ,

5.1 Expected value
‡±™
247
then we see that in general ‚Ñô{X ‚â•ùîºX}
Ã∏= 1/2. Thus, in this case, the expected value is
different from the median of X defined as a real number M satisfying ‚Ñô{X ‚â•M} ‚â•1/2
and ‚Ñô{X ‚â§M} ‚â•1/2. In particular, if FX satisfies the condition of Proposition 4.4.7, then
the median is uniquely determined by M = F‚àí1
X (1/2), i. e., by ‚Ñô{X ‚â§M} = 1/2. It is easy
to see that the above phenomenon appears for all exponentially distributed random
variables. Indeed, if X is EŒª-distributed, then M = ln 2/Œª while, as we saw, ùîºX = 1/Œª,
compare Figure 5.1.
Figure 5.1: The expected value ùîºX = 1/Œª and the median M = ln 2/Œª of an EŒª-distributed random variable X.
Corollary 5.1.32. If X is œá2
n-distributed, then
ùîºX = n .
Proof. Since œá2
n = Œì2,n/2, by Proposition 5.1.28 it follows that ùîºX = 2 ‚ãÖn/2 = n.
Which expected value does a beta distributed random variable possess? The next
proposition answers this question.
Proposition 5.1.33. Let X be ‚Ñ¨Œ±,Œ≤-distributed for certain Œ±, Œ≤ > 0. Then
ùîºX =
Œ±
Œ± + Œ≤ .
Proof. Using eq. (1.63), from eq. (5.19) we obtain, as asserted,
ùîºX =
1
B(Œ±, Œ≤)
1
‚à´
0
x ‚ãÖxŒ±‚àí1(1 ‚àíx)Œ≤‚àí1 dx
=
1
B(Œ±, Œ≤)
1
‚à´
0
xŒ±(1 ‚àíx)Œ≤‚àí1 dx = B(Œ± + 1, Œ≤)
B(Œ±, Œ≤)
=
Œ±
Œ± + Œ≤ .
Example 5.1.34. Suppose we choose independently n numbers x1, . . . , xn uniformly
distributed on [0, 1] and order them by their size. Then we get the order statistics

248
‡±™
5 Expected value, variance, and covariance
0 ‚â§x‚àó
1 ‚â§‚ãÖ‚ãÖ‚ãÖ‚â§x‚àó
n ‚â§1. According to Example 3.7.11, if 1 ‚â§k ‚â§n, then the number x‚àó
k is
‚Ñ¨k,n‚àík+1-distributed. Thus Proposition 5.1.33 implies that the average value of x‚àó
k , that is,
of the kth largest number, equals
k
k + (n ‚àík + 1) =
k
n + 1 .
In particular, the expected value of the smallest number is
1
n+1 while that of the largest
one is
n
n+1.
Does a Cauchy distributed random variable possess an expected value? Here we
obtain the following.
Proposition 5.1.35. If X is Cauchy distributed, then ùîºX does not exist.
Proof. First, observe that we may not use Definition 5.1.22. The distribution density of X
is given by p(x) = 1
œÄ ‚ãÖ
1
1+x2 , hence, it does not satisfy p(x) = 0 for x < 0. Consequently, we
have to check whether condition (5.20) is satisfied. Here we get
ùîº|X| = 1
œÄ
‚àû
‚à´
‚àí‚àû
|x|
1 + x2 dx = 2
œÄ
‚àû
‚à´
0
x
1 + x2 dx = 1
œÄ [ln(1 + x2)]
‚àû
0 = ‚àû.
Thus, ùîº|X| = ‚àû, that is, X does not possess an expected value.
Finally, we determine the expected value of normally distributed random variables.
Proposition 5.1.36. If X is ùí©(Œº, œÉ2)-distributed, then
ùîºX = Œº .
Proof. First, we check whether the expected value exists. The density of X is given by
eq. (1.49), hence
‚àû
‚à´
‚àí‚àû
|x| pŒº,œÉ(x) dx =
1
‚àö2œÄ œÉ
‚àû
‚à´
‚àí‚àû
|x| e‚àí(x‚àíŒº)2/2œÉ2
dx =
1
‚àöœÄ
‚àû
‚à´
‚àí‚àû
|‚àö2œÉu + Œº| e‚àíu2
du
‚â§œÉ 2‚àö2
‚àöœÄ
‚àû
‚à´
0
u e‚àíu2
du + |Œº| 2
‚àöœÄ
‚àû
‚à´
0
e‚àíu2
du < ‚àû,
where we used the well-known fact3 that for all k ‚àà‚Ñï0,
‚àû
‚à´
0
uk e‚àíu2
du < ‚àû.
3 See either [Spi08] or use that for all k ‚â•1 one has supu>0 uk e‚àíu < ‚àû.

5.1 Expected value
‡±™
249
The expected value ùîºX is now evaluated in a similar way as
ùîºX =
‚àû
‚à´
‚àí‚àû
x pŒº,œÉ(x) dx =
1
‚àö2œÄ œÉ
‚àû
‚à´
‚àí‚àû
x e‚àí(x‚àíŒº)2/2œÉ2
dx
=
1
‚àö2œÄ
‚àû
‚à´
‚àí‚àû
(œÉv + Œº) e‚àív2/2 dv
= œÉ
1
‚àö2œÄ
‚àû
‚à´
‚àí‚àû
v e‚àív2/2 dv + Œº
1
‚àö2œÄ
‚àû
‚à´
‚àí‚àû
e‚àív2/2 dv .
(5.22)
The function f (v) := ve‚àív2/2 is odd, that is, f (‚àív) = ‚àíf (v), thus ‚à´
‚àû
‚àí‚àûf (v) dv = 0, and the
first integral in eq. (5.22) vanishes. To compute the second integral, use Proposition 1.6.7
and obtain
Œº
1
‚àö2œÄ
‚àû
‚à´
‚àí‚àû
e‚àív2/2 dv = Œº
1
‚àö2œÄ
‚àö2œÄ = Œº .
This completes the proof.
Remark 5.1.37. Proposition 5.1.36 justifies the notation ‚Äúexpected value‚Äù for the param-
eter Œº in the definition of the probability measure ùí©(Œº, œÉ2).
Summary: Let X be some random variable. Using the notation introduced in Remark 3.3.1, the following are
valid:
1.
X uniformly distributed on {x1, . . . , xN}
‚áí
ùîºX = x1+‚ãÖ‚ãÖ‚ãÖ+xN
N
.
2.
X ‚àºBn,p
‚áí
ùîºX = n p.
3.
X ‚àºPoisŒª
‚áí
ùîºX = Œª.
4.
X ‚àºB‚àí
n,p
‚áí
ùîºX = n
p.
5.
X ‚àºGp
‚áí
ùîºX = 1
p.
6.
X uniformly distributed on [Œ±, Œ≤]
‚áí
ùîºX = Œ±+Œ≤
2 .
7.
X ‚àºŒìŒ±,Œ≤
‚áí
ùîºX = Œ±Œ≤.
8.
X ‚àºEŒª,n
‚áí
ùîºX = n
Œª .
9.
X ‚àºœá2
n
‚áí
ùîºX = n.
10.
X ‚àº‚Ñ¨Œ±,Œ≤
‚áí
ùîºX =
Œ±
Œ±+Œ≤ .
11.
X ‚àºùí©(Œº, œÉ2)
‚áí
ùîºX = Œº.
12.
X Cauchy distributed
‚áí
ùîºX does not exist.
5.1.5 Properties of the expected value
In this section we summarize the main properties of the expected value. They are
valid for both discrete and continuous random variables. But, unfortunately, within

250
‡±™
5 Expected value, variance, and covariance
the framework of this book it is not possible to prove most of them in full generality.
To do so, one needs an integral (Lebesgue integral) ‚à´Œ© f d‚Ñôof functions f : Œ© ‚Üí‚Ñùfor
some probability space (Œ©, ùíú, ‚Ñô). Then ùîºX = ‚à´Œ© Xd‚Ñô, and all subsequent properties of
X Û≥®É‚ÜíùîºX follow from those of the (Lebesgue) integral. We refer to [LG22] for a thorough
presentation of this (wonderful) topic.
Proposition 5.1.38. The expected value of random variables has the following properties:
(1) The expected value of X only depends on its probability distribution ‚ÑôX, not on the way
how X is defined. That is, if X
d= Y for two random variables X and Y, then ùîºX = ùîºY.
(2) If X is constant with probability one, that is, there is some c ‚àà‚Ñùwith ‚Ñô{X = c} = 1,
then ùîºX = c.
(3) The expected value is linear: let X and Y be two random variables possessing an ex-
pected value and let a, b ‚àà‚Ñù. Then ùîº(aX + bY) exists as well and, moreover,
ùîº(aX + bY) = a ùîºX + b ùîºY.
(4) Suppose X is a discrete random variable with values x1, x2, . . . Given a function f from
‚Ñùto ‚Ñù, the expected value ùîºf (X) exists if and only if
‚àû
‚àë
i=1
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®f (xi)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚Ñô{X = xi} < ‚àû,
and, moreover, then
ùîºf (X) =
‚àû
‚àë
i=1
f (xi) ‚Ñô{X = xi} .
(5.23)
(5) If X is continuous with density p, then for any measurable function f : ‚Ñù‚Üí‚Ñùthe
expected value ùîºf (X) exists if and only if
‚àû
‚à´
‚àí‚àû
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®f (x)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®p(x) dx < ‚àû.
In this case it follows that
ùîºf (X) =
‚àû
‚à´
‚àí‚àû
f (x) p(x) dx .
(5.24)
(6) For independent X and Y possessing an expected value, the expected value of their
product4 X Y exists as well and, moreover,
ùîº[X Y] = ùîºX ‚ãÖùîºY.
4 Recall that (XY)(œâ) = X(œâ) ‚ãÖY(œâ) for all œâ ‚ààŒ©.

5.1 Expected value
‡±™
251
(7) Write X ‚â§Y provided that X(œâ) ‚â§Y(œâ) for all œâ ‚ààŒ©. If in this sense |X| ‚â§Y for some
Y with ùîºY < ‚àû, then ùîº|X| < ‚àûand, hence, ùîºX exists.
(8) Suppose ùîºX and ùîºY exist. Then X ‚â§Y implies ùîºX ‚â§ùîºY. In particular, if X ‚â•0, then
ùîºX ‚â•0.
Proof. We only prove properties (1), (2), (4), and (8). Some of the other properties are not
difficult to verify in the case of discrete random variables, for example, (3), but because
the proofs are incomplete, we do not present them here. We refer to [Bil12, Dur19] or
[Kho07] for the proofs of the remaining properties.
We begin with the proof of (1). If X and Y are identically distributed, then either
both are discrete or both are continuous. If they are discrete, and ‚ÑôX(D) = 1 for an at
most countably infinite set D, then X
d= Y implies ‚ÑôY(D) = 1. Moreover, by the same
argument ‚ÑôX({x}) = ‚ÑôY({x}) for any x ‚ààD. Hence, in view of Definition 5.1.2, ùîºX exists
if and only if ùîºY does. Moreover, if this is valid, then ùîºX = ùîºY by the same argument.
In the continuous case, we argue as follows. Let p be a density of X. Due to X
d= Y, it
follows that
t
‚à´
‚àí‚àû
p(x) dx = ‚ÑôX((‚àí‚àû, t]) = ‚ÑôY((‚àí‚àû, t]) ,
t ‚àà‚Ñù.
Thus, p is also a distribution density of Y and, consequently, in view of Definition 5.1.24,
the expected value of X exists if and only if this is the case for Y. Moreover, by Defini-
tion 5.1.26, we get ùîºX = ùîºY.
Next we show that (2) is valid. Thus, suppose ‚Ñô{X = c} = 1 for some c ‚àà‚Ñù. Then X
is discrete with ‚ÑôX(D) = 1 where D = {c}, and by Definition 5.1.2 we obtain
ùîºX = c ‚ãÖ‚Ñô{X = c} = c ‚ãÖ1 = c,
as asserted.
To prove (4), we assume that X has values in D = {x1, x2, . . . }. Then Y = f (X) maps
into f (D) = {y1, y2, . . . }. Given j ‚àà‚Ñï, let Dj = {xi : f (xi) = yj}. Thus,
‚Ñô{Y = yj} = ‚Ñô{X ‚ààDj} = ‚àë
xi‚ààDj
‚Ñô{X = xi} .
Consequently, since Dj ‚à©Dj‚Ä≤ = 0 if j
Ã∏= j‚Ä≤, due to ‚ãÉ‚àû
j=1 Dj = D, we get
ùîº|Y| =
‚àû
‚àë
j=1
|yj|‚Ñô{Y = yj} =
‚àû
‚àë
j=1
‚àë
xi‚ààDj
|yj|‚Ñô{X = xi}
=
‚àû
‚àë
j=1
‚àë
xi‚ààDj
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®f (xi)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚Ñô{X = xi} =
‚àû
‚àë
i=1
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®f (xi)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚Ñô{X = xi} .

252
‡±™
5 Expected value, variance, and covariance
This proves the first part of (4). The second part follows by exactly the same arguments
(replace |yj| by yj). Therefore, we omit its proof.
We finally prove (8). To this end, we first show the second part, that is, ùîºX ‚â•0 for
X ‚â•0. If X is discrete, then X attains values in D, where D consists only of nonnegative
real numbers. Hence, xj‚Ñô{X = xj} ‚â•0, which implies ùîºX ‚â•0. If X is continuous, in
view of X ‚â•0, we may choose its density p such that p(x) = 0 if x < 0. Then ùîºX =
‚à´
‚àû
0 p(x)dx ‚â•0.
Suppose now X ‚â§Y. Setting Z = Y ‚àíX, from the first step we get ùîºZ ‚â•0. But,
property (3) implies ùîºZ = ùîºY ‚àíùîºX, from which we derive ùîºX ‚â§ùîºY, as asserted. Note
that by assumption ùîºX and ùîºY are real numbers, so that ùîºY‚àíùîºX is not an undetermined
expression.
Remark 5.1.39. Properties (4) and (5) of the previous proposition, applied with the func-
tion f (x) = |x|, lead to
ùîº|X| =
‚àû
‚àë
j=1
|xj|‚Ñô{X = xj}
or
ùîº|X| =
‚àû
‚à´
‚àí‚àû
|x| p(x) dx ,
as we already stated in conditions (5.4) and (5.20), respectively.
Corollary 5.1.40. If ùîºX exists, then shifting X by Œº = ùîºX, it becomes centralized. In other
words, if Œº = ùîºX, then
ùîº(X ‚àíŒº) = 0 .
Proof. If Y = X ‚àíŒº, then properties (2) and (3) of Proposition 5.1.38 imply
ùîºY = ùîº(X ‚àíŒº) = ùîºX ‚àíùîºŒº = Œº ‚àíŒº = 0 ,
as asserted.
An important consequence of (8) in Proposition 5.1.38 reads as follows.
Corollary 5.1.41. If ùîºX exists, then
|ùîºX| ‚â§ùîº|X| .
Proof. For all œâ ‚ààŒ©, it follows that
‚àíÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®X(œâ)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚â§X(œâ) ‚â§ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®X(œâ)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®,
that is, we have ‚àí|X| ‚â§X ‚â§|X|. We apply now (3) and (8) of Proposition 5.1.38 and
conclude that
‚àíùîº|X| = ùîº(‚àí|X|) ‚â§ùîºX ‚â§ùîº|X| .
(5.25)

5.1 Expected value
‡±™
253
Since |a| ‚â§c for a, c ‚àà‚Ñùis equivalent to ‚àíc ‚â§a ‚â§c, the desired estimate is a conse-
quence of inequalities (5.25) with a = ùîºX and c = ùîº|X|.
We now present some examples that show how Proposition 5.1.38 may be used to
evaluate certain expected values.
Example 5.1.42. Suppose we roll n fair dice. Let Sn be the sum of the observed values.
What is the expected value of Sn?
Answer: If Xj denotes the value of the jth die, then X1, . . . , Xn are uniformly dis-
tributed on {1, . . . , 6} with ùîºXj = 7/2 and, moreover, Sn = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn. Thus, property (3)
lets us conclude that
ùîºSn = ùîº(X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn) = ùîºX1 + ‚ãÖ‚ãÖ‚ãÖ+ ùîºXn = 7n
2 .
Example 5.1.43. In Example 4.1.7, we investigated the random walk of a particle on ‚Ñ§.
Each time it jumped with probability p either one step to the right or with probability
1‚àíp one step to the left. There Sn denoted the position of the particle after n steps. What
is the expected position after n steps?
Answer: We proved that Sn = 2Yn ‚àín with a Bn,p-distributed random variable Yn.
Proposition 5.1.13 implies ùîºYn = np, hence the linearity of the expected value leads to
ùîºSn = 2 ùîºYn ‚àín = 2np ‚àín = n(2p ‚àí1) .
(5.26)
For p = 1/2, we obtain the (not very surprising) result ùîºSn = 0. But note that eq. (5.26)
can also be proved directly by using ùîºXj = (‚àí1)(1 ‚àíp) + 1 ‚ãÖp = 2p ‚àí1.
Remark 5.1.44. If we regard Sn as the position of a particle after n jumps, then since
2p ‚àí1 > 0 if p > 1/2 it follows that in this case the particle drifts on average to ‚àû. On the
contrary, if p < 1/2, the particle tends on average to ‚àí‚àû.
On the other hand, if we interpret Sn as the loss or win after n games, we get the
following conclusion: whenever one plays a series of games with success probability
p < 1/2 (for example, roulette), in the long run one will lose on average an arbitrarily
big amount of money.
The next example demonstrates how property (4) of Proposition 5.1.38 may be used.
Example 5.1.45. Let X be PoisŒª-distributed. Find ùîºX2.
Solution: Property (4) of Proposition 5.1.38 implies
ùîºX2 =
‚àû
‚àë
k=0
k2 ‚Ñô{X = k} =
‚àû
‚àë
k=1
k2 Œªk
k! e‚àíŒª = Œª
‚àû
‚àë
k=1
k
Œªk‚àí1
(k ‚àí1)! e‚àíŒª .
We shift the index of summation in the right-hand sum by 1 and get
ùîºX2 = Œª
‚àû
‚àë
k=0
(k + 1) Œªk
k! e‚àíŒª = Œª
‚àû
‚àë
k=0
k Œªk
k! e‚àíŒª + Œª
‚àû
‚àë
k=0
Œªk
k! e‚àíŒª .

254
‡±™
5 Expected value, variance, and covariance
By Proposition 5.1.16, the first sum coincides with Œª ùîºX = Œª2, while the second gives
Œª PoisŒª(‚Ñï0) = Œª ‚ãÖ1 = Œª. Adding both values leads to
ùîºX2 = Œª2 + Œª .
The next example rests upon an application of properties (3), (4), and (6) in Propo-
sition 5.1.38.
Example 5.1.46. Compute ùîºX2 for X being Bn,p-distributed.
Solution: Let X1, . . . , Xn be independent B1,p-distributed random variables. Then
Corollary 4.6.2 asserts that X = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn is Bn,p-distributed. Therefore, it suffices to
evaluate ùîºX2 with X = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn. Thus, property (3) of Proposition 5.1.38 implies
ùîºX2 = ùîº(X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn)2 =
n
‚àë
i=1
n
‚àë
j=1
ùîºXiXj .
If i
Ã∏= j, then Xi and Xj are independent, hence property (6) applies and yields
ùîºXiXj = ùîºXi ‚ãÖùîºXj = p ‚ãÖp = p2 .
For i = j, property (4) gives
ùîºX2
j = 02 ‚ãÖ‚Ñô{Xj = 0} + 12 ‚ãÖ‚Ñô{Xj = 1} = p .
Combining both cases leads to
ùîºX2 = ‚àë
i Ã∏=j
ùîºXi ‚ãÖùîºXj +
n
‚àë
j=1
ùîºX2
j = n(n ‚àí1) p2 + n p = n2 p2 + n p(1 ‚àíp) .
Example 5.1.47. Let X be Gp-distributed. Compute ùîºX2.
Solution: We claim that
ùîºX2 = 2 ‚àíp
p2
.
(5.27)
To prove this, let us start with
ùîºX2 =
‚àû
‚àë
k=1
k2 ‚Ñô{X = k} = p
‚àû
‚àë
k=1
k2(1 ‚àíp)k‚àí1 .
(5.28)
We evaluate the right-hand sum by the following approach. If |x| < 1, then
1
1 ‚àíx =
‚àû
‚àë
k=0
xk .

5.1 Expected value
‡±™
255
Differentiating both sides of this equation leads to
1
(1 ‚àíx)2 =
‚àû
‚àë
k=1
k xk‚àí1 .
Next we multiply this equation by x and arrive at
x
(1 ‚àíx)2 =
‚àû
‚àë
k=1
k xk .
Another differentiation of both functions on {x ‚àà‚Ñù: |x| < 1} implies
1
(1 ‚àíx)2 +
2x
(1 ‚àíx)3 =
‚àû
‚àë
k=1
k2xk‚àí1 .
If we use the last equation with x = 1 ‚àíp, then, by eq. (5.28),
ùîºX2 = p[
1
(1 ‚àí(1 ‚àíp))2 +
2(1 ‚àíp)
(1 ‚àí(1 ‚àíp))3 ] = 2 ‚àíp
p2
,
as we claimed in eq. (5.27).
In the next example we us property (5) of Proposition 5.1.38.
Example 5.1.48. Let U be uniformly distributed on [0, 1]. Which expected value does
‚àöU possess?
Solution: By property (5), it follows that
ùîº‚àöU =
‚àû
‚à´
‚àí‚àû
‚àöx ‚ãÖ1[0,1](x) dx =
1
‚à´
0
‚àöx dx = 2
3[x3/2]
1
0 = 2
3 .
Another approach is as follows. Because of
F‚àöU(t) = ‚Ñô{‚àöU ‚â§t} = ‚Ñô{U ‚â§t2} = t2
for 0 ‚â§t ‚â§1, the density q of ‚àöU is given by q(x) = 2x, 0 ‚â§x ‚â§1, and q(x) = 0 otherwise.
Thus,
ùîº‚àöU =
1
‚à´
0
x 2x dx = 2[x3
3 ]
1
0
= 2
3 .
Let us present now an interesting example called Coupon collector‚Äôs problem.
It was first mentioned in 1708 by A. De Moivre. We formulate it in a present-day lan-
guage.

256
‡±™
5 Expected value, variance, and covariance
Example 5.1.49. A company produces cornflakes. Each pack contains a picture. We as-
sume that there are n different pictures and that they are equally likely. That is, when
buying a pack, the probability to get a certain fixed picture is 1/n. How many packs of
cornflakes have to be bought on average before one gets all possible n pictures?
An equivalent formulation of the problem is as follows. In an urn there are n balls
numbered from 1 to n. One chooses balls out of the urn with replacement. How many
balls have to be chosen on average before one observes all n numbers?
Answer: Assume we already have k different pictures for some k = 0, 1, . . . , n ‚àí1.
Let Xk be the number of necessary purchases to obtain a new picture, that is, to get one
which we do not have. Since each pack contains a picture,
‚Ñô{X0 = 1} = 1 .
If k ‚â•1, then there are still n‚àík pictures that one does not possess. Hence, Xk is geomet-
rically distributed with success probability pk = (n ‚àík)/n. If Sn = X0 + ‚ãÖ‚ãÖ‚ãÖ+ Xn‚àí1, then Sn
is the totality of necessary purchases. By Corollary 5.1.20, we obtain
ùîºXk = 1
pk
=
n
n ‚àík ,
k = 0, . . . , n ‚àí1 .
Note that ùîºX0 = 1, thus the previous formula also holds in this case. Then the linearity
of the expected value implies
ùîºSn = 1 + ùîºX1 + ‚ãÖ‚ãÖ‚ãÖ+ ùîºXn‚àí1 = 1 + 1
p1
+ ‚ãÖ‚ãÖ‚ãÖ+
1
pn‚àí1
= 1 +
n
n ‚àí1 +
n
n ‚àí2 + ‚ãÖ‚ãÖ‚ãÖ+ n
1 = n
n
‚àë
k=1
1
k .
Consequently, on average, one needs n ‚àën
k=1
1
k purchases to obtain a complete collection
of all pictures.
For example, if n = 50, on average, we have to buy 225 packs; if n = 100, then 519; for
n = 200, on average, there are 1176 purchases necessary; if n = 300, then 1885; if n = 400,
we have to buy 2628 packs; and, finally, if n = 500, we need to buy 3397.
Remark 5.1.50. As n ‚Üí‚àû, the harmonic series ‚àën
k=1
1
k behaves like ln n. More precisely
(cf. [Lag13] or [Spi08], Problem 12, Chapter 22)
lim
n‚Üí‚àû[
n
‚àë
k=1
1
k ‚àíln n] = Œ≥ ,
(5.29)
where Œ≥ denotes Euler‚Äôs constant, which is approximately 0.57721. Therefore, for large n,
the average number of necessary purchases is approximately n[ln n + Œ≥]. For example,
if n = 300, then the approximative value is 1884.29, leading also to 1885 necessary pur-
chases.

5.2 Variance
‡±™
257
Summary: The most important properties of the expected value are (we assume that all expected values
are well defined):
1.
For all X, Y and a, b ‚àà‚Ñù, it follows that ùîº[aX + bY] = a ùîºX + b ùîºY.
2.
If f : ‚Ñù‚Üí‚Ñù, then ùîº[f(X)] = ‚àë‚àû
k=1 f(xk)‚Ñô{X = xk}, resp. ùîº[f(X)] = ‚à´
‚àû
‚àí‚àûf(x) p(x)dx.
3.
For all random variables X, it follows that |ùîºX| ‚â§ùîº|X|.
4.
If X and Y are independent, then ùîº[X Y] = ùîºX ‚ãÖùîºY.
5.2 Variance
5.2.1 Higher moments of random variables
Definition 5.2.1. Let n ‚â•1 be some integer. A random variable X possesses an nth moment, provided
that ùîº|X|n < ‚àû. We also say X has a finite absolute nth moment. If this is so, then ùîºXn exists, and it is
called the nth moment of X.
Remark 5.2.2. Because of |X|n = |Xn|, the assumption ùîº|X|n < ‚àûimplies the existence
of the nth moment ùîºXn.
Note that a random variable X has a first moment if and only if the expected value
of X exists, cf. conditions (5.4) and (5.20). Moreover, then the first moment coincides with
ùîºX.
Proposition 5.2.3. Let X be either a discrete random variable with values in {x1, x2, . . .}
and with pj = ‚Ñô{X = xj}, or let X be continuous with density p. If n ‚â•1, then
ùîº|X|n =
‚àû
‚àë
j=1
|xj|n ‚ãÖpj
or
ùîº|X|n =
‚àû
‚à´
‚àí‚àû
|x|n p(x) dx .
(5.30)
Consequently, X possesses a finite absolute nth moment if and only if either the sum or the
integral in eq. (5.30) are finite. If this is satisfied, then these moments are given by
ùîºXn =
‚àû
‚àë
j=1
xn
j ‚ãÖpj
or
ùîºXn =
‚àû
‚à´
‚àí‚àû
xn p(x) dx .
Proof. Apply properties (4) and (5) in Proposition 5.1.38 with f (x) = |x|n or with f (x) = xn,
respectively.
Example 5.2.4. Let U be uniformly distributed on [0,1]. Then
ùîº|U|n = ùîºUn =
1
‚à´
0
xn dx =
1
n + 1 .
For the subsequent investigations, we need the following elementary lemma.

258
‡±™
5 Expected value, variance, and covariance
Lemma 5.2.5. If 0 < Œ± < Œ≤, then for all x ‚â•0,
xŒ± ‚â§xŒ≤ + 1 .
Proof. If 0 ‚â§x ‚â§1, from xŒ≤ ‚â•0 it follows that
xŒ± ‚â§1 ‚â§xŒ≤ + 1 ,
and the inequality is valid.
If x > 1, then Œ± < Œ≤ implies xŒ± < xŒ≤, hence also for those x we arrive at
xŒ± < xŒ≤ ‚â§xŒ≤ + 1 ,
which proves the lemma.
Proposition 5.2.6. Suppose a random variable X has a finite absolute nth moment. Then
X possesses all mth moments with m < n.
Proof. Suppose ùîº|X|n < ‚àûand choose an m < n. For a fixed œâ ‚ààŒ©, we apply
Lemma 5.2.5 with Œ± = m, Œ≤ = n, and x = |X(œâ)|. Doing so, we obtain
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®X(œâ)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
m ‚â§ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®X(œâ)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
n + 1 ,
and this being true for all œâ ‚ààŒ© implies |X|m ‚â§|X|n + 1. Hence, property (7) of Proposi-
tion 5.1.38 yields
ùîº|X|m ‚â§ùîº(|X|n + 1) = ùîº|X|n + 1 < ‚àû.
Consequently, as asserted, X possesses also an absolute mth moment.
Remark 5.2.7. There exist much stronger estimates between different absolute mo-
ments of X. For example, Lyapunov‚Äôs inequality, a special case of Jensen‚Äôs inequality,
asserts that for any 0 < Œ± ‚â§Œ≤,
[ùîº|X|Œ±]
1/Œ± ‚â§[ùîº|X|Œ≤]
1/Œ≤ .
The case n = 2 and m = 1 in Proposition 5.2.6 is of special interest. Here we get the
following useful result.
Corollary 5.2.8. If X possesses a finite second moment, then ùîº|X| < ‚àû, that is, its ex-
pected value exists.
Let us state another important consequence of Proposition 5.2.6.
Corollary 5.2.9. Suppose X has a finite absolute nth moment. Then for any b ‚àà‚Ñù, we also
have ùîº|X + b|n < ‚àû.

5.2 Variance
‡±™
259
Proof. An application of the binomial theorem (Proposition A.3.8) implies
|X + b|n ‚â§(|X| + |b|)
n =
n
‚àë
k=0
(n
k) |X|k |b|n‚àík .
Hence, using properties (3) and (7) of Proposition 5.1.38, we obtain
ùîº|X + b|n ‚â§
n
‚àë
k=0
(n
k)|b|n‚àík ùîº|X|k < ‚àû.
Note that Proposition 5.2.6 implies ùîº|X|k < ‚àûfor all k < n. This ends the proof.
Example 5.2.10. Let X be ŒìŒ±,Œ≤-distributed with parameters Œ±, Œ≤ > 0. Which moments
does X possess, and how can they be computed?
Answer: In view of X ‚â•0, it suffices to investigate ùîºXn. For all n ‚â•1, it follows that
ùîºXn =
1
Œ±Œ≤Œì(Œ≤)
‚àû
‚à´
0
xn+Œ≤‚àí1 e‚àíx/Œ± dx =
Œ±n+Œ≤
Œ±Œ≤Œì(Œ≤)
‚àû
‚à´
0
yn+Œ≤‚àí1 e‚àíy dy
= Œ±n Œì(Œ≤ + n)
Œì(Œ≤)
= Œ±n (Œ≤ + n ‚àí1)(Œ≤ + n ‚àí2) ‚ãÖ‚ãÖ‚ãÖ(Œ≤ + 1)Œ≤ .
In particular, X has moments of any order n ‚â•1.
In the case of an EŒª-distributed random variable X, we have Œ± = 1/Œª and Œ≤ = 1, hence
ùîºXn = n!
Œªn .
Example 5.2.11. Suppose a random variable is tn-distributed. Which moments does X
possess?
Answer: We already know that a t1-distributed random variable does not possess a
first moment. Recall that X is t1-distributed if it is Cauchy distributed. And in Proposi-
tion 5.1.35 we proved ùîº|X| = ‚àûfor Cauchy distributed random variables.
But what can be said if n ‚â•2?
According to Definition 4.6, the random variable X has the density p with
p(x) =
Œì( n+1
2 )
‚àön œÄ Œì( n
2 ) (1 + x2
n )
‚àín/2 ‚àí1/2
,
x ‚àà‚Ñù.
If m ‚àà‚Ñï, then
ùîº|X|m =
Œì( n+1
2 )
‚àön œÄ Œì( n
2 )
‚àû
‚à´
‚àí‚àû
|x|m (1 + x2
n )
‚àín/2 ‚àí1/2
dx .

260
‡±™
5 Expected value, variance, and covariance
Hence, X has an mth moment if and only if the integral
‚àû
‚à´
‚àí‚àû
|x|m (1 + x2
n )
‚àín/2 ‚àí1/2
dx = 2
‚àû
‚à´
0
xm (1 + x2
n )
‚àín/2 ‚àí1/2
dx
(5.31)
is finite. Note that
lim
x‚Üí‚àûxn+1 (1 + x2
n )
‚àín/2 ‚àí1/2
= lim
x‚Üí‚àû(x‚àí2 + 1
n)
‚àín/2 ‚àí1/2
= nn/2+1/2 ,
thus, there are constants 0 < c1 < c2 (depending on n, but not on x) such that
c1
xn‚àím+1 ‚â§xm (1 + x2
n )
‚àín/2 ‚àí1/2
‚â§
c2
xn‚àím+1
(5.32)
for large x, that is, if x > x0 for a suitable x0 ‚àà‚Ñù.
Recall that ‚à´
‚àû
1
x‚àíŒ± dx < ‚àûif and only if Œ± > 1. Having this in mind, in view of
eq. (5.31) and by the estimates in (5.32), we get ùîº|X|m < ‚àûif and only if n ‚àím + 1 > 1,
that is, if and only if m < n.
Summing up, a tn-distributed random variable has moments of order 1, . . . , n ‚àí1,
but no moments of order greater than or equal to n.
Finally, let us investigate the moments of normally distributed random variables.
Example 5.2.12. How do we calculate ùîºXn for an ùí©(0, 1)-distributed random variable?
Answer: Well-known properties of the exponential function imply
ùîº|X|n =
1
‚àö2œÄ
‚àû
‚à´
‚àí‚àû
|x|n e‚àíx2/2 dx =
2
‚àö2œÄ
‚àû
‚à´
0
xn e‚àíx2/2 dx < ‚àû
for all n ‚àà‚Ñï. Thus, a normally distributed random variable possesses moments of any
order. These moments are evaluated by
ùîºXn =
‚àû
‚à´
‚àí‚àû
xn p0,1(x) dx =
1
‚àö2œÄ
‚àû
‚à´
‚àí‚àû
xn e‚àíx2/2 dx .
If n is an odd integer, then x Û≥®É‚Üíxn e‚àíx2/2 is an odd function, hence ùîºXn = 0 for odd
integers n.
Therefore, it suffices to investigate even n = 2m with m ‚àà‚Ñï. Here we get
ùîºX2m = 2 ‚ãÖ
1
‚àö2œÄ
‚àû
‚à´
0
x2m e‚àíx2/2 dx ,
which, by the change of variables y := x2/2, thus x = ‚àö2y with dx =
1
‚àö2 y‚àí1/2 dy, trans-
forms into

5.2 Variance
‡±™
261
ùîºX2m =
1
‚àöœÄ 2m
‚àû
‚à´
0
ym‚àí1/2 e‚àíy dy = 2m
‚àöœÄ Œì(m + 1
2).
Since Œì(1/2) = ‚àöœÄ and by an application of eq. (1.50), we finally obtain
ùîºX2m = 2m
‚àöœÄ (m ‚àí1
2)Œì(m ‚àí1
2) = 2m
‚àöœÄ (m ‚àí1
2)(m ‚àí3
2)Œì(m ‚àí3
2)
= 2mŒì(1/2) ‚ãÖ1/2 ‚ãÖ3/2 ‚ãÖ‚ãÖ‚ãÖ(m ‚àí1/2)
‚àöœÄ
= (2m ‚àí1)(2m ‚àí3) ‚ãÖ‚ãÖ‚ãÖ3 ‚ãÖ1 = (2m ‚àí1)!! .
Summary: A random variable X possess an nth moment if ùîº|X|n < ‚àû. Then its nth moment is defined by
ùîºXn =
‚àû
‚àë
j=1
xn
j ‚Ñô{X = xj}
or
ùîºXn =
‚àû
‚à´
‚àí‚àû
xn p(x)dx ,
respectively. Here the xjs are in the discrete case the possible values of X or p denotes in the continuous case
its density.
If X possesses an nth moment, then this also so for all moments of order m ‚â§n. In particular, for each
random variable with second moment its expected value exists.
5.2.2 Variance of random variables
Let X be a random variable with finite second moment. As we saw in Corollary 5.2.8,
then its expected value Œº := ùîºX exists. Furthermore, letting b = ‚àíŒº, by Corollary 5.2.9,
we also have ùîº|X ‚àíŒº|2 < ‚àû. After this preparation, we can introduce the variance of a
random variable.
Definition 5.2.13. Let X be a random variable possessing a finite second moment. If Œº := ùîºX, then its
variance is defined as
ùïçX := ùîº|X ‚àíŒº|2 = ùîº|X ‚àíùîºX|2 .
Interpretation: The expected value Œº of a random variable is its main characteristic. It
tells us around which value the observations of X have to be expected. But it does not tell
us how far away from Œº these observations will be on average. Are they concentrated
around Œº or are they widely dispersed? This behavior is described by the variance. It is
defined as the average quadratic distance of X to its mean value. If ùïçX is small, then we
will observe realizations of X quite near to its mean. Otherwise, if ùïçX is large, then it is
very likely to observe values of X far away from its expected value.
How do we evaluate the variance in concrete cases? We answer this question for
discrete and continuous random variables separately.

262
‡±™
5 Expected value, variance, and covariance
Proposition 5.2.14. Let X be a random variable with finite second moment and let Œº ‚àà‚Ñù
be its expected value. Then it follows that
ùïçX =
‚àû
‚àë
j=1
(xj ‚àíŒº)2 ‚ãÖpj
and
ùïçX =
‚àû
‚à´
‚àí‚àû
(x ‚àíŒº)2 p(x) dx
(5.33)
in the discrete and continuous case, respectively. Hereby, x1, x2, . . . are the possible values
of X and pj = ‚Ñô{X = xj} in the discrete case, while p denotes the density of X in the
continuous case.
Proof. The assertion follows directly by an application of properties (4) and (5) of Propo-
sition 5.1.38 to f (x) = (x ‚àíŒº)2.
Before we present concrete examples, let us state and prove certain properties of
the variance, which will simplify the calculations later on.
Proposition 5.2.15. Assume X and Y are random variables with finite second moment.
Then the following are valid:
(i)
We have
ùïçX = ùîºX2 ‚àí(ùîºX)2 .
(5.34)
(ii) If ‚Ñô{X = c} = 1 for some c ‚àà‚Ñù, then5 ùïçX = 0.
(iii) For a, b ‚àà‚Ñùfollows that
ùïç(a X + b) = a2 ùïçX .
(iv) In the case of independent X and Y, one has
ùïç(X + Y) = ùïçX + ùïçY .
Proof. Let us begin with the proof of (i). With Œº = ùîºX, we obtain
ùïçX = ùîº(X ‚àíŒº)2 = ùîº[X2 ‚àí2ŒºX + Œº2] = ùîºX2 ‚àí2ŒºùîºX + Œº2
= ùîºX2 ‚àí2Œº2 + Œº2 = ùîºX2 ‚àíŒº2 .
This proves (i).
To verify (ii), we use property (2) in Proposition 5.1.38. Then Œº = ùîºX = c, hence
‚Ñô{X ‚àíŒº = 0} = 1. Another application of property (2) leads to
ùïçX = ùîº(X ‚àíŒº)2 = 0
as asserted.
5 The converse implication is also true. If ùïçX = 0, then X is constant with probability 1.

5.2 Variance
‡±™
263
Next we prove (iii). If a, b ‚àà‚Ñù, then ùîº(aX + b) = aùîºX + b by the linearity of the
expected value. Consequently,
ùïç(aX + b) = ùîº[aX + b ‚àí(a ùîºX + b)]
2 = a2 ùîº(X ‚àíùîºX)2 = a2 ùïçX .
Thus (iii) is valid.
To prove (iv), observe that, if Œº := ùîºX and ŒΩ := ùîºY, then ùîº(X + Y) = Œº + ŒΩ, and hence
ùïç(X + Y) = ùîº[(X ‚àíŒº) + (Y ‚àíŒΩ)]
2
= ùîº(X ‚àíŒº)2 + 2 ùîº[(X ‚àíŒº)(Y ‚àíŒΩ)] + ùîº(Y ‚àíŒΩ)2
= ùïçX + 2 ùîº[(X ‚àíŒº)(Y ‚àíŒΩ)] + ùïçY .
(5.35)
By Proposition 4.1.9, the independence of X and Y implies that of X ‚àíŒº and Y ‚àíŒΩ. There-
fore, from property (6) in Proposition 5.1.38, we derive
ùîº[(X ‚àíŒº)(Y ‚àíŒΩ)] = ùîº(X ‚àíŒº) ‚ãÖùîº(Y ‚àíŒΩ) = (ùîºX ‚àíŒº) ‚ãÖ(ùîºY ‚àíŒΩ) = 0 ‚ãÖ0 = 0 .
Plugging this into eq. (5.35) completes the proof of (iv).
Summary: Let X be a random variable with finite second moment. If Œº denotes the expected value of X,
its variance is defined by ùïçX = ùîº(X ‚àíŒº)2 = ùîºX2 ‚àí(ùîºX)2 . The variance may be evaluated in the discrete,
respectively continuous case as follows:
ùïçX =
‚àû
‚àë
k=1
(xk ‚àíŒº)2 ‚Ñô{X = xk}
and
ùïçX =
‚àû
‚à´
‚àí‚àû
(x ‚àíŒº)2 p(x)dx .
Here the xks and p are the possible values of X and its density, respectively. The basic properties are
ùïç(aX + b) = a2ùïçX and ùïç[X + Y] = ùïçX + ùïçY for independent X and Y.
5.2.3 Variance of certain random variables
Our first objective is to describe the variance of a random variable uniformly distributed
on a finite set.
Proposition 5.2.16. If X is uniformly distributed on {x1, . . . , xN}, then
ùïçX = 1
N
N
‚àë
j=1
(xj ‚àíŒº)2 ,
where Œº is given by Œº = 1
N ‚àëN
j=1 xj .
Proof. Because of pj = 1
N , 1 ‚â§j ‚â§N, this is a direct consequence of eq. (5.33). Recall that
Œº was computed in eq. (5.7).

264
‡±™
5 Expected value, variance, and covariance
Example 5.2.17. Suppose X is uniformly distributed on {1, . . . , 6}. Then ùîºX = 7/2, and
we get
ùïçX =
(1 ‚àí7
2)2 + (2 ‚àí7
2)2 + (3 ‚àí7
2)2 + (4 ‚àí7
2)2 + (5 ‚àí7
2)2 + (6 ‚àí7
2)2
6
=
25
4 + 9
4 + 1
4 + 1
4 + 9
4 + 25
4
6
= 35
12 .
Thus, when rolling a die once, the variance is given by 35
12.
Now assume that we roll the die n times. Let X1, . . . , Xn be the results of the single
rolls. The Xjs are independent, hence, if Sn = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn denotes the sum of the n trials,
then, by (iv) in Proposition 5.2.15, it follows that
ùïçSn = ùïç(X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn) = ùïçX1 + ‚ãÖ‚ãÖ‚ãÖ+ùïçXn = 35 n
12 .
The next proposition examines the variance of binomial distributed random vari-
ables.
Proposition 5.2.18. If X is Bn,p-distributed, then
ùïçX = np(1 ‚àíp) .
(5.36)
Proof. Let X be Bn,p-distributed. In Example 5.1.47, we found ùîºX2 = n2p2 + np (1 ‚àíp).
Moreover, ùîºX = np by Proposition 5.1.13. Thus, using formula (5.34) we derive
ùïçX = ùîºX2 ‚àí(ùîºX)2 = n2p2 + np(1 ‚àíp) ‚àí(np)2 = np(1 ‚àíp),
as asserted.
Remark 5.2.19. An alternative proof of Proposition 5.2.18 is as follows: if n = 1, then we
get
ùïçX = (1 ‚àíp)(0 ‚àíp)2 + p(1 ‚àíp)2 = p(1 ‚àíp)[p + (1 ‚àíp)] = p(1 ‚àíp) .
Thus, if X = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn with Xjs independent B1,p-distributed, then, on the one hand, X
is Bn,p-distributed and, on the other hand, we obtain
ùïçX = ùïç(X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn) = ùïçX1 + ‚ãÖ‚ãÖ‚ãÖ+ ùïçXn = nùïçX1 = np(1 ‚àíp) .
Corollary 5.2.20. Binomial distributed random variables have maximal variance (with n
fixed) if p = 1/2.
Proof. The function p Û≥®É‚Üínp(1 ‚àíp) becomes maximal for p = 1
2. In the extreme cases
p = 0 and p = 1, the variance is zero.

5.2 Variance
‡±™
265
Corollary 5.2.21. Let (Sn)n‚â•0 be a random walk, that is, S0 = 0 and Sn = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn if
n ‚â•1 where the Xis are independent and attaining the values ‚àí1 and 1 with probabilities
1 ‚àíp and p, respectively. Then, if n ‚â•1, it follows that
ùïçSn = 4np(1 ‚àíp) .
(5.37)
Proof. As shown in Example 4.1.7, the random variables Yn = (Sn + n)/2 are Bn,p-
distributed. Applying (iii) of Proposition 5.2.15, together with eq. (5.36), implies
ùïçSn = ùïç(2Yn ‚àín) = 4ùïçYn = 4np(1 ‚àíp),
as asserted.
Next we determine the variance of Poisson distributed random variables.
Proposition 5.2.22. Let X be PoisŒª-distributed for some Œª > 0. Then
ùïçX = Œª .
Proof. In Example 5.1.45, we computed ùîºX2 = Œª2 +Œª. Furthermore, by Proposition 5.1.16,
we know that ùîºX = Œª. Thus, by eq. (5.34), we obtain, as asserted,
ùïçX = ùîºX2 ‚àí(ùîºX)2 = Œª2 + Œª ‚àíŒª2 = Œª .
Next, we compute the variance of a geometrically distributed random variable.
Proposition 5.2.23. Let X be Gp-distributed for some 0 < p < 1. Then its variance equals
ùïçX = 1 ‚àíp
p2
.
Proof. In Example 5.1.47, we found ùîºX2 = 2‚àíp
p2 , and by eq. (5.15) we have ùîºX = 1
p. Conse-
quently, formula (5.34) implies
ùïçX = 2 ‚àíp
p2
‚àí( 1
p)
2
= 1 ‚àíp
p2
,
as asserted.
Corollary 5.2.24. If X is B‚àí
n,p-distributed, then
ùïçX = n 1 ‚àíp
p2
Proof. Let X1, . . . , Xn be independent Gp-distributed random variables. By Corollary 4.6,
their sum X = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn is B‚àí
n,p-distributed, hence property (iv) in Proposition 5.2.15

266
‡±™
5 Expected value, variance, and covariance
lets us conclude that
ùïçX = ùïç(X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn) = ùïçX1 + ‚ãÖ‚ãÖ‚ãÖ+ ùïçXn = n ùïçX1 = n 1 ‚àíp
p2
.
Interpretation: The smaller the p, the bigger the variance of a geometrically or neg-
ative binomial distributed random variable (for n fixed). This is not surprising, because
the smaller the p, the larger the expected value, and so the values of X may be very far
from 1/p (success is very unlikely).
We consider now variances of continuous random variables. Let us begin with uni-
formly distributed ones.
Proposition 5.2.25. Let X be uniformly distributed on an interval [Œ±, Œ≤]. Then it follows
that
ùïçX = (Œ≤ ‚àíŒ±)2
12
.
Proof. We know by Proposition 5.1.27 that ùîºX = (Œ± + Œ≤)/2. In order to apply formula
(5.34), we still have to compute the second moment ùîºX2. Here we obtain
ùîºX2 =
1
Œ≤ ‚àíŒ±
Œ≤
‚à´
Œ±
x2 dx = 1
3 ‚ãÖŒ≤3 ‚àíŒ±3
Œ≤ ‚àíŒ±
= Œ≤2 + Œ±Œ≤ + Œ±2
3
.
Consequently, formula (5.34) lets us conclude that
ùïçX = ùîºX2 ‚àí(ùîºX)2 = Œ≤2 + Œ±Œ≤ + Œ±2
3
‚àí(Œ± + Œ≤
2
)
2
= Œ≤2 + Œ±Œ≤ + Œ±2
3
‚àíŒ±2 + 2Œ±Œ≤ + Œ≤2
4
= Œ±2 ‚àí2Œ±Œ≤ + Œ≤2
12
= (Œ≤ ‚àíŒ±)2
12
.
This completes the proof.
In the case of gamma distributed random variables, the following is valid.
Proposition 5.2.26. If X is ŒìŒ±,Œ≤-distributed, then
ùïçX = Œ±2Œ≤ .
Proof. Recall that ùîºX = Œ±Œ≤ by Proposition 5.1.28. Furthermore, in Example 5.2.10 we
evaluated ùîºXn for a gamma distributed X. Taking n = 2 yields
ùîºX2 = Œ±2 (Œ≤ + 1) Œ≤ ,

5.2 Variance
‡±™
267
and, hence, by eq. (5.34),
ùïçX = ùîºX2 ‚àí(ùîºX)2 = Œ±2 (Œ≤ + 1) Œ≤ ‚àí(Œ±Œ≤)2 = Œ±2Œ≤,
as asserted.
Corollary 5.2.27. If X is EŒª-distributed, then
ùïçX = 1
Œª2 .
Proof. Because of EŒª = Œì 1
Œª ,1, this directly follows from Proposition 5.2.26.
Corollary 5.2.28. For a œá2
n-distributed X, it holds that
ùïçX = 2n .
Proof. Let us give two alternative proofs of the assertion. The first uses Proposition 5.2.26
and œá2
n = Œì2, n
2 .
The second proof is longer, but maybe more interesting. Let X1, . . . , Xn be indepen-
dent ùí©(0, 1)-distributed random variables. Proposition 4.6.10 implies that X2
1 + ‚ãÖ‚ãÖ‚ãÖ+X2
n is
œá2
n-distributed, thus property (iv) of Proposition 5.2.15 applies and leads to
ùïçX = ùïçX2
1 + ‚ãÖ‚ãÖ‚ãÖ+ ùïçX2
n = nùïçX2
1 .
In Example 5.2.12, we evaluated the moments of an ùí©(0, 1)-distributed random variable.
In particular, ùîºX2
1 = 1 and ùîº(X2
1 )2 = ùîºX4
1 = 3!! = 3, hence
ùïçX = n ùïçX2
1 = n (ùîºX4
1 ‚àí(ùîºX2
1 )
2) = (3 ‚àí1)n = 2n,
as claimed.
Finally, we determine the variance of a normal random variable.
Proposition 5.2.29. If X is ùí©(Œº, œÉ2)-distributed, then it follows that
ùïçX = œÉ2 .
Proof. Of course, this could be proven by computing the integral
ùïçX =
‚àû
‚à´
‚àí‚àû
(x ‚àíŒº)2pŒº,œÉ(x)dx .
We prefer a different approach that avoids the calculation of integrals. Because of Propo-
sition 4.2.3, the random variable X may be represented as X = œÉX0 + Œº for a standard
normal X0. Applying (iii) in Proposition 5.2.15 gives

268
‡±™
5 Expected value, variance, and covariance
ùïçX = œÉ2 ùïçX0 .
(5.38)
But ùîºX0 = 0, and by Example 5.2.12 we have ùîºX2
0 = 1, thus
ùïçX0 = 1 ‚àí0 = 1 .
Plugging this into eq. (5.38) proves ùïçX = œÉ2.
Remark 5.2.30. The previous result explains why the parameter œÉ2 is called the ‚Äúvari-
ance‚Äù of an ùí©(Œº, œÉ2)-distributed random variable. Recall that the other parameter Œº de-
notes its expected value. Moreover, it shows that the smaller the œÉ2 > 0, the more are the
values of an ùí©(Œº, œÉ2)-distributed random variable concentrated around the expected
value Œº; see Figure 5.2.
Figure 5.2: Densities of normal distributions with mean value 0 and variances (from bottom to top)
œÉ2 = 1.7, œÉ2 = 1, and œÉ2 = 0.7. The larger the œÉ2, the more likely are events away from zero.
Summary: Let X be some random variable. Then we have
1.
X uniformly distributed on {x1, . . . , xN}
‚áí
ùïçX = (x1‚àíŒº)2+‚ãÖ‚ãÖ‚ãÖ+(xN‚àíŒº)2
N
with Œº = ùîºX.
2.
X ‚àºBn,p
‚áí
ùïçX = n p(1 ‚àíp).
3.
X ‚àºPoisŒª
‚áí
ùïçX = Œª.
4.
X ‚àºB‚àí
n,p
‚áí
ùïçX = n(1‚àíp)
p2
.
5.
X uniformly distributed on [Œ±, Œ≤]
‚áí
ùïçX = (Œ≤‚àíŒ±)2
12
.
6.
X ‚àºŒìŒ±,Œ≤
‚áí
ùïçX = Œ±2Œ≤.
7.
X ‚àºEŒª,n
‚áí
ùïçX = n
Œª2 .
8.
X ‚àºœá2
n
‚áí
ùïçX = 2n.
9.
X ‚àºùí©(Œº, œÉ2)
‚áí
ùïçX = œÉ2.

5.3 Covariance and correlation
‡±™
269
5.3 Covariance and correlation
5.3.1 Covariance
Suppose we know or conjecture that two given random variables X and Y are depen-
dent. The aim of this section is to introduce a quantity that measures their degree of
dependence. Such a quantity should tell us whether the random variables are strongly
or only weakly dependent. Furthermore, we want to know what kind of dependence we
observe. Do larger values of X trigger larger values of Y or is it the other way round? To
illustrate these questions, let us come back to the experiment presented in Example 2.2.5.
Example 5.3.1. In an urn are n balls labeled with ‚Äú0‚Äù and another n balls labeled with
‚Äú1.‚Äù Choose two balls out of the urn without replacement. Let X be the number appearing
on the first ball and Y that on the second. Then X and Y are dependent (check this), but
it is intuitively clear that if n becomes larger, then their dependence diminishes. We ask
for a quantity that tells us their degree of dependence. This measure should decrease as
n increases and it should tend to zero as n ‚Üí‚àû.
Moreover, if X = 1 occurred, then there remained in the urn more balls with ‚Äú0‚Äù
than with ‚Äú1,‚Äù and the probability of the event Y = 0 increases. Thus, larger values of X
make smaller values of Y more likely.
Before we are able to introduce such a ‚Äúmeasure of dependence,‚Äù we need some
preparation.
Proposition 5.3.2. If two random variables X and Y possess a finite second moment, then
the expected value of their product XY exists.
Proof. We use the elementary estimate |ab| ‚â§a2+b2
2
valid for a, b ‚àà‚Ñù. Thus, if œâ ‚ààŒ©,
then
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®X(œâ)Y(œâ)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚â§X(œâ)2
2
+ Y(œâ)2
2
,
that is, we have
|XY| ‚â§X2
2 + Y 2
2 .
(5.39)
By assumption,
ùîº[X2
2 + Y 2
2 ] = 1
2[ùîºX2 + ùîºY 2] < ‚àû,
consequently, because of estimate (5.39), property (7) in Proposition 5.1.38 applies and
tells us that ùîº|XY| < ‚àû. Thus, ùîº[XY] exists as asserted.

270
‡±™
5 Expected value, variance, and covariance
How do we compute ùîº[XY] for given X and Y? In Section 4.5 we observed that the
distribution of X + Y does not only depend on the distributions of X and Y. We have to
know their joint distribution, that is, the distribution of the vector (X, Y). And the same
is true for products and the expected value of the product.
Example 5.3.3. Let us again investigate the random variables X, Y, X‚Ä≤, and Y ‚Ä≤ intro-
duced in Example 3.5.8. Recall that they satisfied
‚Ñô{X = 0, Y = 0} = 1
6 ,
‚Ñô{X = 0, Y = 1} = 1
3 ,
‚Ñô{X = 1, Y = 0} = 1
3 ,
‚Ñô{X = 1, Y = 1} = 1
6 ,
‚Ñô{X‚Ä≤ = 0, Y ‚Ä≤ = 0} = 1
4 ,
‚Ñô{X‚Ä≤ = 0, Y ‚Ä≤ = 1} = 1
4 ,
‚Ñô{X‚Ä≤ = 1, Y ‚Ä≤ = 0} = 1
4 ,
‚Ñô{X‚Ä≤ = 1, Y ‚Ä≤ = 1} = 1
4.
Then ‚ÑôX = ‚ÑôX‚Ä≤ and ‚ÑôY = ‚ÑôY ‚Ä≤, but
ùîº[XY] = 1
6 (0 ‚ãÖ0) + 1
3 (1 ‚ãÖ0) + 1
3 (0 ‚ãÖ1) + 1
6 (1 ‚ãÖ1) = 1
6
and
ùîº[X‚Ä≤Y ‚Ä≤] = 1
4 (0 ‚ãÖ0) + 1
4 (1 ‚ãÖ0) + 1
4 (0 ‚ãÖ1) + 1
4 (1 ‚ãÖ1) = 1
4 .
This example tells us that we have to know the joint distribution in order to compute
ùîº[XY]. The knowledge of the marginal distributions does not suffice.
To evaluate ùîº[XY], we need the following two-dimensional generalization of for-
mulas (5.23) and (5.24).
Proposition 5.3.4. Let X and Y be two random variables and let f : ‚Ñù2 ‚Üí‚Ñùbe some
function.
1.
Suppose X and Y are discrete with values in {x1, x2, . . .} and in {y1, y2, . . .}. Set
pij = ‚Ñô{X = xi, Y = yj}. If
ùîºÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®f (X, Y)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®=
‚àû
‚àë
i,j=1
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®f (xi, yj)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®pij < ‚àû,
(5.40)
then ùîºf (X, Y) exists and can be computed by
ùîºf (X, Y) =
‚àû
‚àë
i,j=1
f (xi, yj) pij .
2.
Let f : ‚Ñù2 ‚Üí‚Ñùbe continuous.6 If p : ‚Ñù2 ‚Üí‚Ñùis the joint density of (X, Y) (as intro-
6 In fact, we need only a measurability in the sense of Definition 4.1.1, but this time for functions f from
‚Ñù2 to ‚Ñù. For our purposes ‚Äúcontinuity‚Äù of f suffices.

5.3 Covariance and correlation
‡±™
271
duced in Definition 3.5.15), then
ùîºÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®f (X, Y)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®=
‚àû
‚à´
‚àí‚àû
‚àû
‚à´
‚àí‚àû
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®f (x, y)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®p(x, y) dxdy < ‚àû
(5.41)
implies the existence of ùîºf (X, Y), which can be evaluated by
ùîºf (X, Y) =
‚àû
‚à´
‚àí‚àû
‚àû
‚à´
‚àí‚àû
f (x, y) p(x, y) dxdy .
(5.42)
Remark 5.3.5. The previous formulas extend easily to higher dimensions. That is, if
‚ÉóX = (X1, . . . , Xn) is an n-dimensional random vector with (joint) distribution density
p : ‚Ñùn ‚Üí‚Ñù, then for continuous7 f : ‚Ñùn ‚Üí‚Ñùone has
ùîºf ( ‚ÉóX) = ùîºf (X1, . . . , Xn) = ‚à´
‚Ñùn
f (x1, . . . , xn) p(x1, . . . , xn) dxn ‚ãÖ‚ãÖ‚ãÖdx1
provided the integral exists. The case of discrete X1, . . . , Xn is treated in a similar way. If
‚ÉóX maps into the finite or countably infinite set D ‚äÇ‚Ñùn, then
ùîºf ( ‚ÉóX) = ùîºf (X1, . . . , Xn) = ‚àë
x‚ààD
f (x)‚Ñô{ ‚ÉóX = x} .
If we apply Proposition 5.3.4 with f : (x, y) Û≥®É‚Üíx ‚ãÖy, then we obtain the following
formulas for the evaluation of ùîº[XY]. Hereby, we assume that conditions (5.40) or (5.41)
are satisfied.
Corollary 5.3.6. In the notation of Proposition 5.3.4, the following are valid:
ùîº[XY] =
‚àû
‚àë
i,j=1
(xi ‚ãÖyj)pij
and
ùîº[XY] =
‚àû
‚à´
‚àí‚àû
‚àû
‚à´
‚àí‚àû
(x ‚ãÖy) p(x, y) dxdy
in the discrete and continuous case, respectively.
After all these preparations, we are now in a position to introduce the covariance
of two random variables.
Definition 5.3.7. Let X and Y be two random variables with finite second moments. Setting Œº = ùîºX and
ŒΩ = ùîºY, the covariance of X and Y is defined as
Cov(X, Y) = ùîº[(X ‚àíŒº)(Y ‚àíŒΩ)] .
7 Compare with the footnote for n = 2 in part 2 of Proposition 5.3.4.

272
‡±™
5 Expected value, variance, and covariance
Remark 5.3.8. Apply Corollary 5.2.9 and Proposition 5.3.2 to see that the covariance is
well defined for random variables with a finite second moment. Furthermore, in view
of Proposition 5.3.4, the covariance may be computed as
Cov(X, Y) =
‚àû
‚àë
i,j=1
(xi ‚àíŒº)(yj ‚àíŒΩ) pij
in the discrete case (recall that pij = ‚Ñô{X = xi, Y = yj}), and as
Cov(X, Y) =
‚àû
‚à´
‚àí‚àû
‚àû
‚à´
‚àí‚àû
(x ‚àíŒº) (y ‚àíŒΩ) p(x, y) dxdy
in the continuous case.
Example 5.3.9. Let us once more consider the random variables X, Y, X‚Ä≤, and Y ‚Ä≤ in Ex-
ample 3.5.8 or Example 5.3.3, respectively. Each of the four random variables has the
expected value 1/2. Therefore, we obtain
Cov(X, Y) = 1
6 (0 ‚àí1
2) ‚ãÖ(0 ‚àí1
2) + 1
3 (1 ‚àí1
2) ‚ãÖ(0 ‚àí1
2)
+ 1
3 (0 ‚àí1
2) ‚ãÖ(1 ‚àí1
2) + 1
6 (1 ‚àí1
2) ‚ãÖ(1 ‚àí1
2) = ‚àí1
12 ,
while
Cov(X‚Ä≤, Y ‚Ä≤) = 1
4 (0 ‚àí1
2) ‚ãÖ(0 ‚àí1
2) + 1
4 (1 ‚àí1
2) ‚ãÖ(0 ‚àí1
2)
+ 1
4 (0 ‚àí1
2) ‚ãÖ(1 ‚àí1
2) + 1
4 (1 ‚àí1
2) ‚ãÖ(1 ‚àí1
2) = 0 .
The following proposition summarizes the main properties of the covariance.
Proposition 5.3.10. Let X and Y be random variables with finite second moments. Then
the following are valid:
(1) Cov(X, Y) = Cov(Y, X).
(2) Cov(X, X) = ùïçX.
(3) The covariance is bilinear, that is, for X1, X2 and real numbers a1 and a2,
Cov(a1X1 + a2X2, Y) = a1Cov(X1, Y) + a2Cov(X2, Y)
and, analogously,
Cov(X, b1Y1 + b2Y2) = b1Cov(X, Y1) + b2Cov(X, Y2),
for random variables Y1, Y2 and real numbers b1, b2.

5.3 Covariance and correlation
‡±™
273
(4) The covariance may also evaluated by
Cov(X, Y) = ùîº[XY] ‚àí(ùîºX)(ùîºY) .
(5.43)
(5) Cov(X, Y) = 0 for independent X and Y.
Proof. Properties (1) and (2) follow directly from the definition of the covariance.
Let us verify (3). Setting Œº1 = ùîºX1 and Œº2 = ùîºX2, the linearity of the expected value
implies
ùîº(a1X1 + a2X2) = a1Œº1 + a2Œº2 .
Hence, if ŒΩ = ùîºY, then
Cov(a1X1 + a2X2, Y) = ùîº[(a1(X1 ‚àíŒº1) + a2(X2 ‚àíŒº2))(Y ‚àíŒΩ)]
= a1ùîº[(X1 ‚àíŒº1)(Y ‚àíŒΩ)] + a2ùîº[(X2 ‚àíŒº2)(Y ‚àíŒΩ)]
= a1Cov(X1, Y) + a2Cov(X2, Y) .
This proves the first part of (3). The second part can be proven in the same way or one
uses Cov(X, Y) = Cov(Y, X) and the first part of (3).
Next we prove eq. (5.43). With Œº = ùîºX and ŒΩ = ùîºY, from
(X ‚àíŒº)(Y ‚àíŒΩ) = XY ‚àíŒºY ‚àíŒΩX + ŒºŒΩ ,
we get that
Cov(X, Y) = ùîº[XY ‚àíŒºY ‚àíŒΩX + ŒºŒΩ] = ùîº[XY] ‚àíŒºùîºY ‚àíŒΩùîºX + ŒºŒΩ
= ùîº[XY] ‚àíŒºŒΩ .
This proves (4) by the definition of Œº and ŒΩ.
Finally, we verify (5). If X and Y are independent, then, by Proposition 4.1.9, this is
also true for X ‚àíŒº and Y ‚àíŒΩ. Thus, property (6) of Proposition 5.1.38 applies and leads to
Cov(X, Y) = ùîº[(X ‚àíŒº)(Y ‚àíŒΩ)] = ùîº(X ‚àíŒº) ùîº(Y ‚àíŒΩ) = [ùîºX ‚àíŒº] [ùîºY ‚àíŒΩ] = 0 .
Therefore, the proof is completed.
Remark 5.3.11. Quite often the computation of Cov(X, Y) can be simplified by the use
of eq. (5.43). For example, consider X and Y in Example 3.5.8. In Example 5.3.3 we found
ùîº[XY] = 1/6. Since ùîºX = ùîºY = 1/2, from eq. (5.43) we immediately get
Cov(X, Y) = 1
6 ‚àí1
4 = ‚àí1
12 .
We obtained the same result in Example 5.3.9 with slightly more effort.

274
‡±™
5 Expected value, variance, and covariance
Property (5) in Proposition 5.3.10 is of special interest. It asserts Cov(X, Y) = 0 for in-
dependent X and Y. One may ask now whether this characterizes independent random
variables. More precisely, are the random variables X and Y independent if and only if
Cov(X, Y) = 0?
The answer is negative as the next example shows.
Example 5.3.12. The joint distribution of X and Y is given by the following table:
Y\X
‚àí1
0
1
‚àí1
1
10
1
10
1
10
3
10
0
1
10
2
10
1
10
2
5
1
1
10
1
10
1
10
3
10
3
10
2
5
3
10
Of course, ùîºX = ùîºY = 0 and, moreover,
ùîº[XY] = 1
10((‚àí1)(‚àí1) + (‚àí1)(+1) + (+1)(‚àí1) + (+1)(+1)) = 0 ,
which by eq. (5.43) implies Cov(X, Y) = 0. On the other hand, Proposition 3.6.11 tells us
that X and Y are not independent. For example,
‚Ñô{X = 0, Y = 0} = 1
5
while ‚Ñô{X = 0}‚Ñô{Y = 0} = 4
25.
Example 5.3.12 shows that Cov(X, Y) = 0 is, in general, weaker than the indepen-
dence of X and Y. Therefore, the following definition makes sense.
Definition 5.3.13. Two random variables X and Y satisfying Cov(X, Y) = 0 are said to be uncorrelated.
Otherwise, if Cov(X, Y)
Ã∏= 0, then X and Y are correlated.
More generally, a sequence X1, . . . , Xn of random variables is called (pairwise) uncorrelated, if
Cov(Xi, Xj) = 0 whenever i
Ã∏= j.
Using this notation, property (5) in Proposition 5.3.10 may now be formulated in the
following way:
X and Y independent
Û≥®ê‚áí
Ã∏
‚áêÛ≥®ê
X and Y uncorrelated.
Example 5.3.14. Let A, B ‚ààùíúbe two events in a probability space (Œ©, ùíú, ‚Ñô) and let 1A
and 1B be their indicator functions as introduced in Definition 3.6.16. How can we com-
pute Cov(1A, 1B)?

5.3 Covariance and correlation
‡±™
275
Answer: Since ùîº1A = ‚Ñô(A), we get
Cov(1A, 1B) = ùîº[1A 1B] ‚àí(ùîº1A)(ùîº1B) = ùîº1A‚à©B ‚àí‚Ñô(A) ‚Ñô(B)
= ‚Ñô(A ‚à©B) ‚àí‚Ñô(A) ‚Ñô(B) .
This tells us that 1A and 1B are uncorrelated if and only if the events A and B are in-
dependent. But as we saw in Proposition 3.6.17, this happens if and only if the random
variables 1A and 1B are independent. In other words, two indicator functions are inde-
pendent if and only if they are uncorrelated.
Finally, we consider the covariance of two continuous random variables.
Example 5.3.15. Suppose a random vector (X, Y) is uniformly distributed on the unit
ball of ‚Ñù2. Then the joint density of (X, Y) is given by
p(x, y) = {
1
œÄ
if x2 + y2 ‚â§1,
0
if x2 + y2 > 1 .
We proved in Example 3.5.19 that X and Y possess the distribution densities
q(x) = {
2
œÄ ‚àö1 ‚àíx2
if |x| ‚â§1,
0
if |x| > 1
and
r(y) =
{
{
{
2
œÄ ‚àö1 ‚àíy2
if |y| ‚â§1,
0
if |y| > 1.
The function y Û≥®É‚Üíy (1‚àíy2)1/2 is odd. Consequently, because we integrate over an interval
symmetric around the origin,
ùîºX = ùîºY = 2
œÄ
1
‚à´
‚àí1
y (1 ‚àíy2)
1/2 dy = 0 .
By the same argument, we obtain
ùîº[XY] =
‚àû
‚à´
‚àí‚àû
‚àû
‚à´
‚àí‚àû
(x ‚ãÖy) p(x, y) dx dy = 1
œÄ
1
‚à´
‚àí1
y[
‚àö1‚àíy2
‚à´
‚àí‚àö1‚àíy2
x dx]dy = 0 ,
and these two assertions imply Cov(X, Y) = 0. Hence, X and Y are uncorrelated, but as
we already observed in Example 3.6.21, they are not independent.
Summary: Let X and Y be two random variables with second moment and expected values Œº and ŒΩ, respec-
tively. Then
Cov(X, Y) = ùîº[(X ‚àíŒº)(Y ‚àíŒΩ)] = ùîº[X Y] ‚àí(ùîºX)(ùîºY)

276
‡±™
5 Expected value, variance, and covariance
denotes the covariance of X and Y. It can be evaluated in the discrete case as follows:
Cov(X, Y) =
‚àû
‚àë
i,j=1
(xi ‚àíŒº)(yj ‚àíŒΩ)‚Ñô{X = xi, Y = yj} .
If X and Y are continuous with joint density p, then
Cov(X, Y) =
‚àû
‚à´
‚àí‚àû
‚àû
‚à´
‚àí‚àû
(x ‚àíŒº)(y ‚àíŒΩ)p(x, y) dxdy .
5.3.2 Correlation coefficient
The question arises whether or not the covariance is the quantity that we are looking
for, that is, which measures the degree of dependence. The answer is only partially af-
firmative. Why? Suppose X and Y are dependent. If a is a nonzero real number, then a
natural demand is that the degree of dependence between X and Y should be the same
as that between aX and Y. But
Cov(aX, Y) = a Cov(X, Y) ,
thus, if a
Ã∏= 1, then the measure of dependence would increase or decrease. To overcome
this drawback, we normalize the covariance in the following way.
Definition 5.3.16. Let X and Y be random variables with finite second moments. Furthermore, we as-
sume that neither X nor Y are constant with probability 1, that is, we have ùïçX > 0 and ùïçY > 0. Then the
quotient
œÅ(X, Y) :=
Cov(X, Y)
(ùïçX)1/2(ùïçY)1/2
(5.44)
is called the correlation coefficient of X and Y.
To verify a crucial property of the correlation coefficient, we need the following version
of the Cauchy‚ÄìSchwarz inequality.
Proposition 5.3.17 (Cauchy‚ÄìSchwarz inequality). For any two random variables X and Y
with finite second moments, it follows that
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ùîº(XY)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚â§(ùîºX2)
1/2 (ùîºY 2)
1/2 .
(5.45)
Proof. By property (8) of Proposition 5.1.38, we have
0 ‚â§ùîº(|X| ‚àíŒª|Y|)
2 = ùîºX2 ‚àí2Œªùîº|XY| + Œª2 ùîºY 2
(5.46)

5.3 Covariance and correlation
‡±™
277
for any Œª ‚àà‚Ñù. To proceed further, we have to assume8 ùîºX2 > 0 and ùîºY 2 > 0. The latter
assumption allows us to choose Œª as
Œª := (ùîºX2)1/2
(ùîºY 2)1/2 .
If we apply inequality (5.46) with this Œª, then we obtain
0 ‚â§EX2 ‚àí2 (ùîºX2)1/2
(ùîºY 2)1/2 ùîº|XY| + ùîºX2 = 2 EX2 ‚àí2 (ùîºX2)1/2
(ùîºY 2)1/2 ùîº|XY| ,
which easily implies (recall that we assumed ùîºX2 > 0)
ùîº|XY| ‚â§(ùîºX2)
1/2 (ùîºY 2)
1/2 .
To complete the proof, we use Corollary 5.1.41 and get
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ùîº(XY)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚â§ùîº|XY| ‚â§(ùîºX2)
1/2 (ùîºY 2)
1/2,
as asserted.
Remark 5.3.18. An analogue inequality as (5.45) for vectors in ‚Ñùn is as follows: let
x = (x1, . . . , xn) and y = (y1, . . . , yn) be two elements in ‚Ñùn. Then one has
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
n
‚àë
j=1
xjyj
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§(
n
‚àë
j=1
x2
j )
1/2
(
n
‚àë
j=1
y2
j )
1/2
.
In the language of scalar products and Euclidean distance, this says
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚ü®x, y‚ü©ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚â§|x| |y| .
Corollary 5.3.19. The correlation coefficient satisfies
‚àí1 ‚â§œÅ(X, Y) ‚â§1 .
Proof. Let as before Œº = ùîºX and ŒΩ = ùîºY. Applying inequality (5.45) to X ‚àíŒº and Y ‚àíŒΩ
leads to
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®Cov(X, Y)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®= ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ùîº(X ‚àíŒº)(Y ‚àíŒΩ)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚â§(ùîº(X ‚àíŒº)2)
1/2 (ùîº(Y ‚àíŒΩ)2)
1/2
= (ùïçX)1/2 (ùïçY)1/2 ,
8 The Cauchy‚ÄìSchwarz inequality remains valid for ùîºX2 = 0 or ùîºY 2 = 0. In this case, it follows that
‚Ñô{X = 0} = 1 or ‚Ñô{Y = 0} = 1, hence ‚Ñô{XY = 0} = 1 and ùîº[XY] = 0.

278
‡±™
5 Expected value, variance, and covariance
or, equivalently,
‚àí(ùïçX)1/2 (ùïçY)1/2 ‚â§Cov(X, Y) ‚â§(ùïçX)1/2 (ùïçY)1/2 .
By the definition of œÅ(X, Y) given in eq. (5.44), this implies ‚àí1 ‚â§œÅ(X, Y) ‚â§1, as as-
serted.
Interpretation: For uncorrelated X and Y, we have œÅ(X, Y) = 0. In particular, this is
valid if X and Y are independent. On the contrary, œÅ(X, Y)
Ã∏= 0 tells us that X and Y are
dependent. Thereby, values near to zero correspond to weak dependence, while œÅ(X, Y)
near 1 or ‚àí1 indicate a strong dependence. The strongest possible dependence is when
Y = aX for some a
Ã∏= 0. Then œÅ(X, Y) = 1 if a > 0 while œÅ(X, Y) = ‚àí1 for a < 0.
Definition 5.3.20. Two random variables X and Y are said to be positively correlated if œÅ(X, Y) > 0. In
the case that œÅ(X, Y) < 0, they are said to be negatively correlated.
Interpretation: Random variables X and Y are positively correlated, provided that larger
(or smaller) values of X make larger (or smaller) values of Y more likely. This does not
mean that a larger X-value always implies a larger Y-value, only that the probability for
those larger values increases. And in the same way, if X and Y are negatively correlated,
then larger values of X make smaller Y-values more likely.
Let us explain this with two typical examples. Choose by random a person œâ in the
audience. Let X(œâ) be his or her height and Y(œâ) his or her weight. Then X and Y will
surely be positively correlated. But this does not necessarily mean that each taller per-
son has a bigger weight. Another example of negatively correlated random variables
could be as follows: X is the average number of cigarettes that a randomly chosen per-
son smokes per day and Y is his lifetime.
Example 5.3.21. Let us come back to Example 5.3.1: in an urn there are n balls labeled
with ‚Äú0‚Äù and n labeled with ‚Äú1.‚Äù One chooses two balls without replacement. Then X is
the value of the first ball, Y that of the second. How does the correlation coefficient of X
and Y depend on n?
Answer: The joint distribution of X and Y is given by the following table:
Y\X
0
1
0
n‚àí1
4n‚àí2
n
4n‚àí2
1
2
1
n
4n‚àí2
n‚àí1
4n‚àí2
1
2
1
2
1
2
Direct computations show ùîºX = ùîºY = 1/2 and ùïçX = ùïçY = 1/4. Moreover, it easily
follows ùîº[XY] =
n‚àí1
4n‚àí2, hence
Cov(X, Y) = n ‚àí1
4n ‚àí2 ‚àí1
4 =
‚àí1
8n ‚àí4 ,

5.4 Some paradoxes and examples
‡±™
279
and the correlation coefficient equals
œÅ(X, Y) =
‚àí1
8n‚àí4
‚àö1
4‚àö1
4
=
‚àí1
2n ‚àí1 .
If n ‚Üí‚àû, then œÅ(X, Y) is of order ‚àí1
2n. Hence, if n is large, then the random variables X
and Y are ‚Äúalmost‚Äù uncorrelated.
Since œÅ(X, Y) < 0, the two random variables are negatively correlated. Why? This
was already explained in Example 5.3.1: an occurrence of X = 1 makes Y = 0 more likely,
while the occurrence of X = 0 increases the likelihood of Y = 1. In the case n = 1, the
value of Y is completely determined by that of X, expressed by œÅ(X, Y) = ‚àí1.
Summary: Let X and Y be two random variables with finite second moment. Then
œÅ(X, Y) :=
Cov(X, Y)
(ùïçX)1/2(ùïçY)1/2
is said to be their correlation coefficient. The random variables are said to be positively correlated if
œÅ(X, Y) > 0, negatively correlated if œÅ(X, Y) < 0, and uncorrelated if œÅ(X, Y) = 0. The basic properties
are ‚àí1 ‚â§œÅ(X, Y) ‚â§1 and œÅ(X, Y) = 0 for independent X and Y. But note that uncorrelated X and Y need not
be independent.
5.4 Some paradoxes and examples
The aim of this section is to present a few more comprehensive examples of special
interest having a long history.
5.4.1 Boy or girl paradox
In 1959 Martin Gardner9 phrased the following two questions:
1.
Mr. Jones has two children. The older child is a girl. What is the probability that both
children are girls?
2.
Mr. Smith has two children. At least one of them is a boy. What is the probability
that both children are boys?
The basic assumptions for answering these questions are:
(a) Each child is either a boy or a girl.
9 Martin Gardner, Problems involving questions of probability and ambiguity, Scientific American, Octo-
ber 1959.

280
‡±™
5 Expected value, variance, and covariance
(b) Boys and girls occur equally likely.
(c) The gender of the two children is independent of each other.
Answers: There are 4 different possibilities for the gender of the two children:
(G, G),
(B, G),
(G, B),
and
(B, B) .
For example, (B, G) means that the older child is a boy while the younger one is a girl.
In view of the basic assumptions, all 4 elementary events are equally likely, hence their
probability is 1/4.
Solution of the first question: Here one asks for the probability of {(G, G)} under the
condition {(G, B), (G, G)}. So we get
‚Ñô({(G, G)}|{(G, B), (G, G)}) =
‚Ñô({(G, G)})
‚Ñô({(G, B), (G, G)}) = 1/4
1/2 = 1
2 .
First solution of the second question: One chooses by random a family with two children.
Let the event A occur if the chosen family has at least one boy. That is,
A = {(B, G), (G, B), (B, B)} ,
hence ‚Ñô(A) = 3/4. In question 2, we asked for the probability of the occurrence of {(B, B)}
under the condition A. Then we get
‚Ñô({(B, B)}|A) = ‚Ñô({(B, B)})
‚Ñô(A)
= 1/4
3/4 = 1
3 .
We obtained this result by restricting the sample space and ruling out families with two
girls.
Alternative solution of the second question: We split the problem into two steps. In the
first step, one chooses at random a family with two children. But we do not have any
information about the gender of these children. Any of the four configurations (G, G),
(G, B), (B, G), and (B, B) is possible, each occurring with probability 1/4.
Next, in the second step, we choose equiprobable one of the two children of the
family, that is, of the family chosen in the first step. The result may be ‚ÄúB‚Äù or ‚ÄúG.‚Äù Set
S = {Second step leads to ‚ÄúB‚Äù} .
Thus, if the randomly chosen family with two children has a boy and a girl, then the
second step will lead equally likely to ‚ÄúB‚Äù and to ‚ÄúG.‚Äù Consequently,
‚Ñô(S|{(B, G)}) = ‚Ñô(S|{(G, B)}) = 1
2 .

5.4 Some paradoxes and examples
‡±™
281
On the other hand, if the chosen family has either two girls or two boys, then it follows
that
‚Ñô(S|{(G, G)}) = 0
or
‚Ñô(S|{(B, B)}) = 1 ,
respectively.
By symmetry (recall that ‚ÄúB‚Äù and ‚ÄúG‚Äù are equally likely), one should have that ‚Ñô(S) =
1/2. A rigorous proof is based on the law of total probability. It implies
‚Ñô(S) = ‚Ñô{(G, G)} ‚ãÖ‚Ñô(S|(G, G)) + ‚Ñô{(B, G)} ‚ãÖ‚Ñô(S|(B, G))
+ ‚Ñô{(G, B)} ‚ãÖ‚Ñô(S|(G, B)) + ‚Ñô{(B, B)} ‚ãÖ‚Ñô(S|(B, B))
= 1
4 ‚ãÖ0 + 1
4 ‚ãÖ1
2 + 1
4 ‚ãÖ1
2 + 1
4 ‚ãÖ1 = 1
2 .
This lets us conclude that
‚Ñô({(B, B)}|S) = ‚Ñô(S|{(B, B)}) ‚ãÖ‚Ñô({(B, B)})
‚Ñô(S)
= 1 ‚ãÖ1/4
1/2 = 1
2 .
Consequently, the answer to the second question is: the probability for both children
being boys knowing that at least one child is a boy equals 1/2.
For the sake of completeness, we also state the other probabilities. These are
‚Ñô({(G, G)}|S) = 0 and
‚Ñô({(G, B)}|S) = ‚Ñô({(B, G)}|S) = ‚Ñô(S|{(B, G)}) ‚ãÖ‚Ñô({(B, G)})
‚Ñô(S)
= 1
2 ‚ãÖ1/4
1/2 = 1
4 .
The obtained result may also be phrased as follows: the a priori probabilities of getting
(G, G), (G, B), (B, G), and (B, B) (the probabilities before choosing randomly a child) are
all 1/4 while the a posteriori probabilities (after observing a ‚ÄúB‚Äù in the second step) are
0, 1/4, 1/4, and 1/2, respectively.
Remark 5.4.1. One may wonder how it is possible that the answer to question 2 is at
the same time 1/3 and 1/2. Maybe there is some error in the calculations. This is not so.
The reason lies in the ambiguity of the way to use the information ‚ÄúAt least one child
is a boy.‚Äù In the first approach, we use this information to rule out families with two
girls from the very beginning. That is, if we chose at random a family with two girls, we
discard it and make another trial.
The alternative approach looks more natural, at least to us. Choose at random any
family with two children. Then all four possibilities of the distribution of boys and girls
may occur. Having the family fixed, we check whether or not a randomly chosen child
(maybe the older, maybe the younger) is a boy. For example, we chose the family of Mr.
Smith, and we see him walking with one of his children who is a boy. There is a 50 %
chance that this is his older child, but, of course, it also could be the younger one.

282
‡±™
5 Expected value, variance, and covariance
5.4.2 Randomly chosen entries
Let us extend the boy or girl paradox to a more general setting. It reads as follows: toss
a fair coin labeled by ‚Äú0‚Äù and ‚Äú1‚Äù exactly n ‚â•2 times, without recording the obtained
results. After that, one gets the information that one of n the entries equals one. Given
some 1 ‚â§k ‚â§n, the question is now how likely is it that the observed sequence contains
exactly k times the number ‚Äú1.‚Äù
Note that in the case n = k = 2 this is exactly question 2 in Section 5.4.1. To see this
link ‚ÄúB‚Äù with ‚Äú1‚Äù and ‚ÄúG‚Äù with ‚Äú0.‚Äù
But also here, in this generalized setting, the information that one of the entries
equals ‚Äú1‚Äù can be interpreted in different ways.
One possible interpretation of the problem is to discard from the very beginning
sequences without ‚Äú1.‚Äù So the restricted sample space contains 2n ‚àí1 elements. Conse-
quently, if Ak is the event to observe k times ‚Äú1,‚Äù then
‚Ñô(Ak|{At least one ‚Äú1‚Äù}) =
(n
k)
2n ‚àí1
k = 1, . . . , n .
Of course, ‚Ñô(A0|{At least one ‚Äú1‚Äù}) = 0.
Another interpretation of the problem is as follows: one chooses at random a se-
quence x = (x1, . . . , xn) of ‚Äú0‚Äùs and ‚Äú1‚Äùs. The probability of its occurrence is 1/2n. Next one
fixes the observed x = (x1, . . . , xn) and chooses at random one of its entries (all entries
are equally likely), say xj for some j ‚â§n. Then the event xj = 1 occurs with probability
k/n, where k is the number of ‚Äú1‚Äùs in the chosen sequence x. Thus, if
S := {The randomly chosen entry equals ‚Äú1‚Äù} ,
it follows that
‚Ñô(S|{x}) = k
n
whenever x ‚ààAk .
Recall that Ak denotes the set of all sequences of ‚Äú0‚Äù and ‚Äú1‚Äù with k times ‚Äú1.‚Äù The law
of total probability yields (see the proof of Proposition 5.1.13 for the evaluation of the
sum)
‚Ñô(S) =
n
‚àë
k=1
‚àë
x‚ààAk
‚Ñô({x}) ‚ãÖ‚Ñô(S|{x}) =
n
‚àë
k=1
‚àë
x‚ààAk
1
2n ‚ãÖk
n
=
1
n2n
n
‚àë
k=1
k (n
k) =
1
n2n ‚ãÖ(n2n‚àí1) = 1
2 .
This result is not surprising at all and could also obtained heuristically. Indeed, by sym-
metry the occurrence of ‚Äú0‚Äù and ‚Äú1‚Äù has to be equally likely.

5.4 Some paradoxes and examples
‡±™
283
Consequently, if x ‚ààAk, then
‚Ñô({x}|S) = ‚Ñô(S|{x}) ‚ãÖ‚Ñô({x})
‚Ñô(S) = k
n ‚ãÖ1
2n ‚ãÖ1
2‚àí1 =
k
n 2n‚àí1 .
Finally, since |Ak| = (n
k), the additivity of conditional probabilities leads to
‚Ñô(Ak|S) = (n
k) ‚ãÖ
k
n 2n‚àí1 = (n ‚àí1
k ‚àí1) ‚ãÖ
1
2n‚àí1 ,
k = 1, . . . , n .
Summing up, we received the following result.
Proposition 5.4.2. Let n ‚â•1. Tossing a fair coin n times, then for all 1 ‚â§k ‚â§n it follows
that
‚Ñô{Observe k times ‚Äú1‚Äù | Randomly chosen entry is ‚Äú1‚Äù} = (n ‚àí1
k ‚àí1) ‚ãÖ
1
2n‚àí1 .
Example 5.4.3. If we apply Proposition 5.4.2 to the case n = k = 2, we rediscover the
answer given in Section 5.4.1. Indeed, then
‚Ñô{Both children are boys | A randomly chosen child is a boy} = 1
2 .
Another case of interest is as follows: one asks for the probability that a family with
three children has two boys, provided a randomly chosen child is a boy. Then n = 3
and k = 2, hence the probability for this event equals 2 ‚ãÖ1/4 = 1/2. Equivalently, under
the given condition, each of the events {(G, B, B)}, {(B, G, B)}, and {(B, B, G)} occurs with
probability 1/6. On the other hand, if a randomly chosen child of the three ones is a boy,
the occurrence of three boys possesses the probability 1/4. Without that knowledge, the
probability of {(B, B, B)} equals 1/8.
Remark 5.4.4. Proposition 5.4.2 tells us that the left-hand probability coincides with that
of the occurrence of k‚àí1 times ‚Äú1‚Äù when tossing a fair coin n‚àí1 times. There is a straight-
forward explanation of this coincidence. In the presented setting, we first toss the coin
and next choose randomly an entry of the observed sequence of zeroes and ones. But
we could do it also the other way round: first choosing randomly a number from 1 to n
and after that tossing the coin. We leave the details as problem for the interested reader
(see Problem 5.25).
5.4.3 Secretary problem
This is a well-known problem in Probability Theory, also called ‚Äúmarriage problem‚Äù or
the ‚Äúsultan‚Äôs dowry problem.‚Äù
A company wants to hire a secretary. There are n applicants interviewed in random
order. Immediately after the interview, the administrator decides whether or not the

284
‡±™
5 Expected value, variance, and covariance
candidate is rejected or hired. Once rejected, the candidate cannot be recalled. The goal
of the company is to get the best applicant. To this end, every interviewed candidate is
ranked in a linear order. But note that the administrator has no information about the
quality of the unseen applicants.
Now the strategy of the administrator is a follows: Choose a number 0 ‚â§r ‚â§n ‚àí1,
interview the first r applicants and reject them all. After that choose the first candidate
who is better than all of the r rejected ones. If r = 0, then nobody is interviewed and the
first applicant is hired.
Questions: How likely is it that this strategy leads to the employment of the overall
best candidate? What is the optimal choice of the number r?
Before proceeding further, let us explain the problem with an easy example. Sup-
pose there are three candidates enumerated by 1, 2, and 3. We assume that applicant
2 is better than 1 and that candidate 3 is better than 2. Then there are 3! = 6 ways of
interviewing the applicants:
a = (1, 2, 3),
b = (1, 3, 2),
c = (2, 1, 3),
d = (2, 3, 1),
e = (3, 1, 2),
f = (3, 2, 1) .
If r = 0, only orderings (e) and (f ) lead to the best candidate. So, the chance for hiring
the best one is 2/6 = 1/3.
In the case r = 1, the company hires the best one in the cases (b), (c), and (d). Hence, the
chance to get the best one equals 1/2.
Finally, if r = 2, only (a) and (c) are successful. Thus, also here the chance to get the best
applicant is 1/3.
Summing up, if n = 3, then the optimal strategy is to choose r = 1. That is, reject
the first applicant and then choose the next one who is better than the first. Of course,
it may happen that there is nobody better than the first, which occurs in the cases (e)
and (f ). Then nobody is hired and the administrator failed to get the best applicant.
Let us transform the general problem into a mathematical setting. Name the can-
didates by numbers from 1, . . . , n and, without losing generality, let us assume that the
applicant n is the best, that applicant n ‚àí1 is the second best, and so on. Suppose now
the candidates are interviewed in the order œÄ(1), œÄ(2), . . . , œÄ(n) for some permutation
œÄ ‚ààSn. In this context, the best applicant appears at position k if and only if œÄ(k) = n or,
equivalently, œÄ‚àí1(n) = k.
It is assumed that all orderings of the applicants are equally likely, so we have to
endow the set Sn of permutations with the uniform distribution ‚Ñô. That is, for all A ‚äÜSn
we have
‚Ñô(A) = |A|
|Sn| = |A|
n! .
In particular, if
Ak := {œÄ ‚ààSn : œÄ(k) = n},

5.4 Some paradoxes and examples
‡±™
285
it follows that
‚Ñô(Ak) = (n ‚àí1)!
n!
= 1
n ,
k = 1, . . . , n .
(5.47)
Recall that œÄ ‚ààAk if and only if the best applicant is at position k in the queue of candi-
dates.
Let P(r) be the probability to hire the best candidate when choosing the strategy of
rejecting the first r candidates.
If r = 0, then the best candidate is hired if and only if œÄ(1) = n or, equivalently, if
and only if œÄ ‚ààA1. Hence, we get in this case
P(0) = ‚Ñô(A1) = 1
n .
Consider now an arbitrary 1 ‚â§r ‚â§n ‚àí1 and suppose that œÄ ‚ààAk for a certain 1 ‚â§k ‚â§n,
that is, the permutation satisfies œÄ(n) = k. When does the administrator choose the best
applicant? This happens if and only if k > r and, moreover,
œÄ(r + 1), . . . , œÄ(k ‚àí1) < max{œÄ(1), . . . , œÄ(r)} .
(5.48)
Another way to formulate property (5.48) is as follows:
If
œÄ(a) = max{œÄ(1), . . . , œÄ(k ‚àí1)},
then necessarily 1 ‚â§a ‚â§r .
(5.49)
This may also be expressed as follows: whenever œÄ ‚ààAk for some k ‚â•2, then the second
best candidate among the first k candidates has to be at position a for some a ‚â§r. Recall
that the best one occurs at position k. Compare the three possible situations discussed
in Figure 5.3.
Figure 5.3: In case (a), the company hires the best secretary while it fails to do so in the cases (b) and (c).
To proceed further, given 1 ‚â§a ‚â§k ‚àí1, define disjoint subsets Sa
k of Sn by
Sa
k := {œÄ ‚ààSn : œÄ(a) = max{œÄ(1), . . . , œÄ(k ‚àí1)}} .

286
‡±™
5 Expected value, variance, and covariance
Verbally said, a permutation œÄ belongs to Sa
k if and only if it attains its maximal value in
{1, . . . , k ‚àí1} at the given number a.
We claim now that if 2 ‚â§k ‚â§n, then
‚Ñô(Sa
k|Ak) =
1
k ‚àí1 ,
a = 1, . . . , k ‚àí1 .
(5.50)
Of course,10 ‚Ñô(S1
k|Ak) = ‚ãÖ‚ãÖ‚ãÖ= ‚Ñô(Sk‚àí1
k
|Ak). Moreover, because k ‚â•2, there always exists a
best candidate among positions 1 and k ‚àí1, thus
1 = ‚Ñô(
k‚àí1
‚ãÉ
a=1
Sa
k|Ak) =
k‚àí1
‚àë
a=1
‚Ñô(Sa
k|Ak) .
Clearly, these two properties prove eq. (5.50).
Summing up, in view of assertion (5.49), for a given order œÄ(1), . . . , œÄ(n) the strategy
leads to the best candidate if and only if
‚àÉk > r,
‚àÉ1 ‚â§a ‚â§r ,
œÄ ‚ààSa
k ‚à©Ak
‚áî
œÄ ‚àà
n
‚ãÉ
k=r+1
[Ak ‚à©(
r
‚ãÉ
a=1
Sa
k)] .
(5.51)
Now we are prepared to evaluate P(r), the probability to choose the best applicant when
rejecting the first r candidates. Using (5.51), an application of the law of multiplication
together with eqs. (5.47) and (5.50) implies (recall that the Aks and the Sa
ks are disjoint
for fixed k)
P(r) = ‚Ñô(
n
‚ãÉ
k=r+1
[Ak ‚à©(
r
‚ãÉ
a=1
Sa
k)]) =
n
‚àë
k=r+1
r
‚àë
a=1
‚Ñô(Ak ‚à©Sa
k)
=
n
‚àë
k=r+1
r
‚àë
a=1
‚Ñô(Sa
k|Ak)‚Ñô(Ak) = r
n
n
‚àë
k=r+1
1
k ‚àí1 = r
n
n‚àí1
‚àë
k=r
1
k .
Conclusion: The optimal choice of the number 1 ‚â§r < n is that for which
r Û≥®É‚Üír
n
n‚àí1
‚àë
k=r
1
k
(5.52)
becomes maximal. For example, if n = 30, the maximal value is attained at r = 11 and one
has P(11) = 0.378651. But note that also choices of r near to 11 lead to reasonably large
probabilities. For example, we have P(9) = 0.373139, P(10) = 0.377562, P(12) = 0.376711,
and P(13) = 0.371992. One should compare these values with the random choice of a
single candidate where the probability to get the best applicant equals 1/30 = 0.0 ÃÑ3. See
also Figure 5.4.
10 If 1 ‚â§a, b ‚â§k ‚àí1, define a bijection between Sa
k and Sb
k by œÄ Û≥®É‚ÜíœÄ ‚àòia,b where ia,b is the inversion of a
and b.

5.4 Some paradoxes and examples
‡±™
287
Figure 5.4: The values of P(r), 1 ‚â§r ‚â§29, in the case of 30 applicants.
Remark 5.4.5. For large numbers n, it might be quite difficult to find the number r < n
for which the function in (5.52) becomes maximal. Here assertion (5.29) may be helpful.
Using
r
n
n‚àí1
‚àë
k=r
1
k = r
n
n‚àí1
‚àë
k=1
1
k ‚àír
n
r‚àí1
‚àë
k=1
1
k ‚âàr
n(ln n ‚àíln r) = r
n ln(n
r ) = ‚àír
n ln( r
n) ,
it follows that for large n the optimal choice of r is r
n ‚àºx0 where x Û≥®É‚Üí‚àíx ln x becomes
maximal at x0. Methods from Calculus imply x0 = 1/e ‚âà0.367879. Thus, a rough choice
of the optimal r is 37 % of n. In the literature, this is quite often called the 37 %-rule.
If as above n = 30, then 30/e ‚âà11.0364 while 37 % of n = 30 gives 11.1. Thus, this also
leads to r = 11 as the optimal choice. Check Figure 5.5 to see that for large n one has
P(r) ‚âà‚àí(r/n) ln(r/n).
5.4.4 Two-envelope paradox
We finally present a famous paradox in Probability Theory called the ‚Äútwo-envelope
paradox‚Äù or the ‚Äúenvelope exchange paradox.‚Äù Imagine you may choose one of two in-
distinguishable envelopes, both containing a certain amount of money. You do not know
how much money is in the envelopes, but you have the information that one of the two
envelopes contains twice as much as the other. Having chosen an envelope at will, you
inspect it, and find x dollars. Hence, the other unopened envelope contains either 2x or
x/2 dollars, depending on whether the chosen envelope was that with the smaller or
larger sum.

288
‡±™
5 Expected value, variance, and covariance
Figure 5.5: The values of P(r) for 30 applicants. The upper dots are the ‚Äúcorrect‚Äù values while the lower
ones are the values of the approximation r Û≥®É‚Üí‚àí(r/30) ln(r/30).
After that you are given the chance to swap envelopes. Should you use this opportu-
nity? If you do not swap, then you keep the x dollars. Otherwise, you either double your
amount or halve it, both with probability 1/2. Thus, if E is the expected amount after
switching, it follows that
E = (2x) ‚ãÖ1
2 + (x
2 ) ‚ãÖ1
2 = 5
4 ‚ãÖx .
Consequently, on average, by switching you gain x/4 dollars. Imagine, for example, the
chosen envelope contains $100. Then by switching one either loses $50 or one wins $100,
both with probability 1/2. Of course, this contradicts the common sense. But what is
wrong?
First, there is a misinterpretation of the observed amount. The observed x is a ran-
dom value, not the expected value of the money you get. Say for some c > 0, the en-
velopes contain c and 2c dollars. Denote by X the money you get without switching,
then it follows that
‚Ñô{X = c} = ‚Ñô{X = 2c} = 1
2 .
And after switching the new random variable ÃÉX also satisfies
‚Ñô{ ÃÉX = c} = ‚Ñô{X = 2c} = 1
2
and
‚Ñô{ ÃÉX = 2c} = ‚Ñô{X = c} = 1
2 .
Hence, it follows that ùîºX = ùîºÃÉX = 3 c/2, and on average there is no advantage by switch-
ing, exactly as one expects.
But there is still another missing information in the scenario. In which way are the
sums in the envelopes chosen? Are these, as assumed above, always (in each experiment)

5.4 Some paradoxes and examples
‡±™
289
fixed amounts c and 2c? Or is there a positive random variable Y such that the envelopes
contain Y and 2Y dollars? In other words, before you take one of the two envelopes
at random, the included sums are chosen by another independent random experiment
described by a random variable Y. But, and this suggests the formulation of the problem,
thereby it is impossible to do it in a way such that all possible amounts of integers (or
positive real numbers) are equally likely.
Let us explain this (random) setting with an example. Assume the master of cer-
emonies rolls a die and, depending on the observed number k ‚àà{1, . . . , 6}, he puts 2k
dollars into one envelope and 2k+1 into the other. In the above setting, the random vari-
able Y satisfies
‚Ñô{Y = 2k} = 1
6 ,
k = 1, . . . , 6 ,
(5.53)
and if X denotes the obtained amount, then11
‚Ñô{X = 2} = 1
2 ‚ãÖ‚Ñô{Y = 1} = 1
12 ,
‚Ñô{X = 128} = 1
2 ‚ãÖ‚Ñô{Y = 6} = 1
12
and
‚Ñô{X = 2k} = 1
6 ‚ãÖ‚Ñô{X = 2k|Y = k ‚àí1} + 1
6 ‚ãÖ‚Ñô{X = 2k|Y = k} = 1
6 ,
k = 2, . . . , 6 .
So we get
ùîºX = 2 ‚ãÖ1
12 + 1
6 ‚ãÖ
6
‚àë
k=2
2k + 128 ‚ãÖ1
12 = 31.5 .
(5.54)
If ÃÉX denotes the obtained amount after always switching, then
‚Ñô{ ÃÉX = 2k} = ‚Ñô{X = 2k} ,
k = 1, . . . , 7 ,
hence nothing changes by always swapping.
But what happens if one swaps only in the case that the opened envelope contains
a ‚Äúsmall‚Äù amount? Say, one swaps in the above example if there are less than $60 in the
envelope and otherwise one does not. Then the probability to get $32 diminishes to 1/12
while the probability of obtaining $64 increases to 3/12. Thus, after eventually swapping,
the average of the amount ÃÑX equals
ùîºÃÑX = 2 1
12 + 4 1
6 + 8 1
6 + 16 1
6 + 32 1
12 + 64 3
12 + 128 1
12 = 34.1 ÃÑ6 .
11 The possible pairs of included amounts are
(2, 4),
(4, 8),
(8, 16),
(16, 32),
(32, 64),
and
(64, 128) .

290
‡±™
5 Expected value, variance, and covariance
So we see, this strategy improves the average of the money obtained. Moreover, the op-
timal case occurs if there are $32 and $64 in the envelopes12 and, furthermore, one had
chosen the envelope containing the smaller amount. Then by swapping one gets extra
$32. The probability that this happens equals 1/12.
But note that this strategy heavily depends on some foreknowledge about the size
of the amount in the envelopes. For example, if one decides to switch provided there are
less than $200 in the opened envelope,13 then there is no improvement of ùîºX.
Let us finally shortly discuss the case of general (discretely) distributed amounts in
the envelopes.14 So suppose there are certain positive numbers x1, x2, . . . and nonnega-
tive pks with ‚àë‚àû
k=1 pk = 1. Choose a random variable Y for which
‚Ñô{Y = xk} = pk ,
k = 1, 2, . . .
Put with probability pk into one envelope xk and into the other 2xk dollars. After that,
choose equally likely one envelope at random.15 Then we get for the expected amount
X that
‚Ñô{X = xk} = pk
2
and
‚Ñô{X = 2xk} = pk
2 .
This implies
ùîºX =
‚àû
‚àë
k=1
xk
pk
2 +
‚àû
‚àë
k=1
(2xk)pk
2 = 3
2
‚àû
‚àë
k=1
pkxk = 3
2 ùîºY .
For example, choosing Y as in eq. (5.53), it follows that
ùîºY = 1
6
6
‚àë
k=1
2k = 21
‚áí
ùîºX = 3
2 ‚ãÖùîºY = 3
2 ‚ãÖ21 = 31.5 .
This coincides with the result obtained in eq. (5.54).
If, as before, ÃÉX denotes the obtained amount after switching, then
‚Ñô{ ÃÉX = xk} = ‚Ñô{X = 2xk} = pk
2
and
‚Ñô{ ÃÉX = 2xk} = ‚Ñô{X = xk} = pk
2 ,
so nothing has changed and ùîºÃÉX = ùîºX = 3 ùîºY/2.
12 The result of rolling the die was ‚Äú5.‚Äù
13 We encourage the reader to evaluate ùîºÃÑX when swapping in the case that there are either less than 20
or less than 10 dollars in the chosen envelope. Find the optimal threshold for switching and nonswitching.
14 One may also choose continuous distributions of the included amounts, but this is more involved and
uses facts not included in the present book.
15 Note that the previous example fits into this setting. There we had xk = 2k as well as p1 = ‚ãÖ‚ãÖ‚ãÖ= p6 =
1/6 and pk = 0 if k > 6.

5.4 Some paradoxes and examples
‡±™
291
Thus, always swapping does not yield any advantage. But what happens if we use
the following strategy: Choose a threshold N > 0. If the amount x in the chosen envelope
satisfies x < N, then switch. Otherwise, if x ‚â•N, do not do so. For simplicity, we answer
this question only for special distributions of the amounts.
So suppose that for a certain k = 0, 1, 2, . . . one envelope contains 2k dollars and
the other 2k+1, and that the probability to choose this pair equals pk where pk ‚â•0 and
‚àë‚àû
k=0 pk = 1. That is, the contents of one envelope is Y, that of the other 2Y where
‚Ñô{Y = 2k} = pk ,
k = 0, 1, 2, . . .
This leads to
ùîºX = 3
2 ‚ãÖùîºY = 3
2
‚àû
‚àë
k=0
pk2k ,
which implies that ùîºX < ‚àûif and only if ‚àë‚àû
k=0 pk 2k < ‚àû.
If ùîºX = ‚àû, it does not make sense to ask whether ùîºX increases or decreases by
swapping or nonswapping. So let us assume that the expected gained amount is finite.
Choose a threshold N > 1. Swap if the amount in the chosen envelope is less than N. Do
not swap otherwise. Does this improve the expected value of gained money?
To answer this question take the integer ‚Ñì‚â•0 for which 2‚Ñì< N ‚â§2‚Ñì+1. Let
ÃÑX be
the obtained amount after switching those envelopes where one observes an amount
smaller than N. Then the only change of the distribution of X occurs in the case where
2‚Ñìand 2‚Ñì+1 dollars are in the two envelopes. No matter if one were to choose the envelop
with the smaller or with the larger amount, in this case one would always get that with
2‚Ñì+1 dollars. Hence, it follows that
ùîºÃÑX ‚àíùîºX = p‚Ñì
2 (2‚Ñì+1 ‚àí2‚Ñì) = p‚Ñì2‚Ñì‚àí1 .
This is the good news. But what is the bad? Since we assumed that the expected value of
X exists, it follows that
lim
‚Ñì‚Üí‚àûp‚Ñì2‚Ñì= 0 .
That is, the larger the threshold N, the less the expected advantage by choosing this
strategy.
Another way to formulate the result is as follows: choosing the threshold N such
that 2‚Ñì< N ‚â§2‚Ñì+1, by switching one may gain 2‚Ñì+1 ‚àí2‚Ñì= 2‚Ñìdollars, maybe a huge
amount. But the likelihood that this happens is p‚Ñì/2, a very small number. Recall that is
so if and only if, firstly, there are 2‚Ñìand 2‚Ñì+1 dollars in the envelopes and, secondly, one
had chosen the envelope containing the smaller amount.

292
‡±™
5 Expected value, variance, and covariance
To illustrate the obtained results, choose pk = 2/3k+1, k = 0, 1, . . . That is,
‚Ñô{Y = 2k} =
2
3k+1 ,
k = 0, 1, 2, . . .
In this case,
ùîºY =
‚àû
‚àë
k=0
(2
3)
k+1
= 2
‚áí
ùîºX = 3
2 ‚ãÖùîºY = 3 .
If the chosen threshold N satisfies 2‚Ñì< N ‚â§2‚Ñì+1, then by switching envelopes with small
amounts, the expected value increases to
ùîºÃÑX = 3 + 2‚Ñì
3‚Ñì+1 .
Note that the maximal expected advantage of 1/3 occurs if ‚Ñì= 0.
Remark 5.4.6. There exists an interesting tightly related version of the two envelopes
paradox, sometimes called the two-number problem. A person writes two different
numbers on two slips of paper, one on each, so that you cannot see what is written. Next
you choose at random one of these two slips, turn it around and read the number stated
there. After that you may decide whether you keep the chosen slip or you better switch
and choose the other. At the end, after switching or nonswitching, you lose the game
if you have chosen the slip with the smaller number. Otherwise you win. It looks like
that your chance of winning is 50 %. But there exists a strategy to increase your chance
slightly. Take an arbitrary probability distribution ‚Ñöon ‚Ñùsatisfying ‚Ñö([a, b]) > 0 for
all a < b. Simulate a random real number z distributed according to ‚Ñö. If your number
x at the chosen slip satisfies x < z, then switch. Otherwise, if z < x, keep the chosen
slip.
Let us heuristically explain why this strategy improves your chance of winning. Sup-
pose the two numbers on the slips are a ‚àà‚Ñùand b ‚àà‚Ñùwith a < b. If the simulated num-
ber z satisfies z < a, then, no matter which of the two slips you chose, you do not switch.
Hence, in this case your chance of winning remains 50 % as it was at the beginning. Sim-
ilarly, if z > b, then you always switch, and your chance of winning remains 50 % as it
was before switching. But what happens in the case a < z < b? If you have chosen the
slip with a on it, you switch and win. Otherwise, if your choice was already the larger
number b, you do not switch and you win as well. Due to the assumption about the un-
derlying probability distribution ‚Ñö, no matter how big/small a < b are, with probability
‚Ñö([a, b]) > 0 the simulated number z will satisfy a < z < b, a case where the strategy
always leads to a win. Putting together all three cases, the chance of winning becomes
slightly greater than 50 %. We refer to [Sam04] for a precise presentation. Note that we
did not say anything about the rules for the choice of the numbers a < b. Recall that
there is no probability distribution ‚Ñôon ‚Ñùsuch that ‚Ñô({a}) = ‚Ñô({b}) for all a < b.

5.5 Gambler‚Äôs ruin
‡±™
293
Summary: In the previous section, we presented three famous examples in Probability Theory: The ‚ÄúBoy or
Girl Paradox,‚Äù the ‚ÄúSecretary Problem,‚Äù and the ‚ÄúEnvelope Exchange Paradox.‚Äù We gave full solutions and
discussed some generalizations of these classical problems.
5.5 Gambler‚Äôs ruin
Two players, say player A and his opponent B, play a series of independent games. Player
A wins each single game with probability p, hence the success probability for B equals
1 ‚àíp. Here and later on, we always assume 0 < p < 1 because otherwise either A or B
always win. Each time the winner gets $1 from the loser. At the beginning, A has a ‚â•1
dollars in his wallet, B possesses b ‚â•1 dollars. The gamblers decide to play as long as
one of them lost all of his money.
The basic question is how likely is it that A and/or B go bankrupt. To answer this
question, we use the technique of random walks as presented in Example 4.1.7. There
we investigated walks starting at zero. But, of course, this easily extends to walks starting
at an arbitrary integer k ‚àà‚Ñ§.
Definition 5.5.1. Given an independent sequence (Xi)i‚â•1 with
‚Ñô{Xi = 1} = p
and
‚Ñô{Xi = ‚àí1} = 1 ‚àíp ,
i = 1, 2, . . .
Let S0 = k and Sn = k + X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn if n ‚â•1. Then (Sn)n‚â•0 is a (simple) random walk starting at k ‚àà‚Ñ§.
In this setting, player A wins if a random walk (Sn)n‚â•0 starting at a ‚â•1 satisfies Sn = a+b
for some n ‚â•0 and, moreover, Sj > 0 if 0 ‚â§j ‚â§n. Note that Sn is the amount of money
which owns player A after n games. Compare Figure 5.6.
Figure 5.6: Players A and B start their series of games with a ‚â•1 and b ‚â•1 dollars, respectively.
Let 0 ‚â§k ‚â§a + b be an arbitrary integer. Set
Ak = {(Sn)n‚â•0 starts at k , and ‚àÉn ‚â•0, Sn = a + b and Sj > 0 , j ‚â§n} .

294
‡±™
5 Expected value, variance, and covariance
In other words, the event Ak occurs if player A starts with k dollars, at some time he
reaches level a + b and before that he does not go bankrupt.
The basic properties of ‚Ñô(Ak) are as follows:
‚Ñô(A0) = 0
and
‚Ñô(Aa+b) = 1 ,
and if 1 ‚â§k < a + b, then
‚Ñô(Ak) = ‚Ñô(Ak|X1 = 1)‚Ñô{X1 = 1} + ‚Ñô(Ak|X1 = ‚àí1)‚Ñô{X1 = ‚àí1}
= p ‚Ñô(Ak+1) + (1 ‚àíp)‚Ñô(Ak‚àí1) .
To see the last property, imagine A and B play one (their first) game and after that they
start a new series of games where now, depending on the result in the first game, player
A either owns k + 1 or k ‚àí1 dollars.
Letting xk = ‚Ñô(Ak), and setting q = 1 ‚àíp, for any 0 < k < a + b we get
xk+1 = 1
p xk ‚àíq
p xk‚àí1 ,
x0 = 0,
and
xa+b = 1 .
(5.55)
So we obtained for the xks a linear recurrence formula of second order with two bound-
ary conditions at k = 0 and k = a + b. The technique to solve such recurrence formulas
is well known; see, for example, page 41 in [CL23]. A basic role play the zeroes or roots
of the characteristic equation, which in the case of eq. (5.55) is given by
z2 ‚àí1
p z + q
p = 0 .
If p
Ã∏= q, that is, if p
Ã∏= 1/2, then this equation has two different roots which are z1 = 1
and z2 = q/p (recall that q = 1 ‚àíp). Thus, there are constants c and d such that
xk = c ‚ãÖ1k + d(q
p)
k
= c + d(q
p)
k
,
k = 0, . . . , a + b .
The boundary conditions tell us that c + d = 0 and c + d(q/p)a+b = 1, hence
c =
‚àí1
( q
p)a+b ‚àí1
and
d =
1
( q
p)a+b ‚àí1
,
leading to
‚Ñô(Ak) = xk =
( q
p)k ‚àí1
( q
p)a+b ‚àí1
,
k = 0, . . . , a + b .
If p = q = 1/2, the characteristic equation becomes
z2 ‚àí2z + 1 = 0

5.5 Gambler‚Äôs ruin
‡±™
295
with root z0 = 1 of multiplicity 2. In this case, see, for example, page 43 in [CL23], one
gets
xk = c 1k + d k 1k = c + d k
with certain constants c and d. The boundary conditions imply c + d ‚ãÖ0 = 0 as well as
c + d(a + b) = 1, hence c = 0 and d = 1/(a + b). So we finally conclude that
‚Ñô(Ak) = xk =
k
a + b ,
k = 0, . . . , a + b .
Choosing in both cases k = a, we obtain the following result.
Proposition 5.5.2. Suppose A and B play a series of games where every time A wins one
dollar with probability p, hence B wins one dollar with probability q = 1 ‚àíp. If the initial
amounts of money are a ‚â•1 and b ‚â•1, respectively, then
‚Ñô{B goes bankrupt} = ‚Ñô{A wins} =
{
{
{
{
{
( q
p )a‚àí1
( q
p )a+b‚àí1
if p
Ã∏= 1
2,
a
a+b
if p = 1
2.
What happens if both players start the series of games with identical amount a > 0?
The following corollary gives the answer.
Corollary 5.5.3. Suppose both players A and B start their games with the same amount
a > 0. As before, p and q = 1 ‚àíp are the success probabilities of players A and B, respec-
tively. Then it follows that
‚Ñô{B goes bankrupt} = ‚Ñô{A wins} =
1
1 + ( q
p)a .
Proof. The result is obviously true if p = 1/2, that is, if p = q. So let us assume that p
Ã∏= q.
Hence it follows that x := (q/p)a
Ã∏= 1. We apply now Proposition 5.5.2 with b = a and
obtain
‚Ñô{A wins} =
( q
p)a ‚àí1
( q
p)2a ‚àí1
= x ‚àí1
x2 ‚àí1 =
1
1 + x =
1
1 + ( q
p)a ,
as asserted.
Remark 5.5.4. The previous corollary shows the not very surprising fact that the
chances of player A are less than 1/2 whenever q > p, that is, if 0 < p < 1/2. More-
over, in this case one observes the following: the bigger the initial amount a > 0, the less
the probability for A to win.

296
‡±™
5 Expected value, variance, and covariance
Example 5.5.5. Suppose player A wins a single game with probability p = 0.49 and both
players A and B start their games with the same amount of a dollars. Then Corollary 5.5.3
applies and we get
‚Ñô{A wins} =
1
1 + ( 51
49)a .
For example, if a = 50, this probability equals 0.119175 while for a = 100 one gets
0.0179768. See Figure 5.7 for other probabilities with respect to the sums a = 1, . . . , 100.
Figure 5.7: The probability that A wins when both players start with a dollars, a = 1, . . . , 100. Here player A
wins a single game with probability p = 0.49.
Example 5.5.6. Let us play roulette where in every game we either win or lose $1 (for ex-
ample, put every time $1 either on red or on black). The chance of winning is p = 18/37,
hence q = 19/37. Say one stops gambling if either one had lost $10 or if one had won
$100. So, in the previous notation a = 10 and b = 100. Hence we get
‚Ñô{Win $100, starting with $10} =
( 19
18)10 ‚àí1
( 19
18)110 ‚àí1
= 0.00187859 .
(5.56)
Does it considerably improve the chance of winning $100 if one accepts in between a
bigger loss? Not really. For example, if one goes bankrupt after loosing $100, then the
chance of winning $100 equals 0.00446628. Note that this probability equals 1/2 in the
case of a fair game. Thus, even the small disadvantage of 1/18 changes the chance of
winning dramatically. See Figure 5.8 for the probabilities to win $100 starting with $10
up to $30, respectively.

5.5 Gambler‚Äôs ruin
‡±™
297
Figure 5.8: The probability to win $100 starting with a = 10, . . . , 30 dollars when playing roulette. Each time
one wins or loses $1.
Another interesting numerical example is a = b = 10 and p = 18/37. That is, the
game is terminated when either one has lost or won $10. Here Corollary 5.5.3 leads to
0.368031 as probability for winning $10. In other words, playing roulette 100 times with
initial amount $10, on average in about 37 of the cases you will win $10, but in 63 of the
cases you are going to lose your initial sum.
Let us now come back to the general case of players A and B with success probabil-
ities p and q = 1 ‚àíp, owning at the beginning a and b dollars, respectively.
How likely is it that B wins? To answer this question, we use Proposition 5.5.2 but
turn the tables. Interchange A and B, p and q, as well as a and b. Doing so, we obtain the
following:
Proposition 5.5.7. Suppose A and B play a series of games where every time A wins with
probability p, hence B with probability q = 1 ‚àíp. If the initial amounts of money are a
and b, respectively, then
‚Ñô{A goes bankrupt} = ‚Ñô{B wins} =
{
{
{
{
{
( p
q )b‚àí1
( p
q )a+b‚àí1
if p
Ã∏= 1
2,
b
a+b
if p = 1
2.
An interesting question remained unanswered until now: is it possible that the se-
ries of games between A and B lasts forever? In other words, may it happen that neither
A nor B wins?
The following result shows that the answer is negative.
Proposition 5.5.8. Under the previous assumptions, it follows that
‚Ñô{A wins} + ‚Ñô{B wins} = 1 .

298
‡±™
5 Expected value, variance, and covariance
In particular, this implies
‚Ñô{The game lasts forever} = 0 .
Proof. If p = 1/2, by Propositions 5.5.2 and 5.5.7, one gets
‚Ñô{A wins} + ‚Ñô{B wins} =
a
a + b +
b
a + b = 1 ,
completing the proof in this case.
Thus, let us assume now p
Ã∏= q. To simplify the calculations, set x = q/p and
y = 1/x = p/q. Note that both numbers are by assumption different from 1. With these
notations, Propositions 5.5.2 and 5.5.7 may be written as
‚Ñô{A wins} =
xa ‚àí1
xa+b ‚àí1 = yb ‚àíya+b
1 ‚àíya+b
and
‚Ñô{B wins} =
yb ‚àí1
ya+b ‚àí1 .
Consequently, the assertion follows from
yb ‚àíya+b
1 ‚àíya+b + yb ‚àí1
ya+b ‚àí1 = ya+b ‚àíyb + yb ‚àí1
ya+b ‚àí1
= ya+b ‚àí1
ya+b ‚àí1 = 1 .
In view of Proposition 5.5.8, the following natural question arises: Let Ta,b be the
number of rounds that A and B play. That is, given a random walk (Sn)n‚â•0 starting at
zero, for some a, b ‚àà‚Ñïset
Ta,b = min{n ‚â•0 : Sn = ‚àía or Sn = b} .
(5.57)
What is the expected value of Ta,b? In other words, how long does the series of games
last on average. The answer is as follows (for a proof, we refer to [Sti03] or [Fel68]; the
basic idea is similar to that used in the proof of Proposition 5.5.2, namely conditioning
on the first step which leads to a linear recurrence formula for ùîºTa,b).
Proposition 5.5.9. Let a, b, p, and q = 1 ‚àíp be as before. If Ta,b denotes the number of
rounds before one of the players goes bankrupt, then
ùîºTa,b =
{
{
{
a
q‚àíp ‚àía+b
q‚àíp
( q
p )a‚àí1
( q
p )a+b‚àí1
if p
Ã∏= 1
2,
a ‚ãÖb
if p = 1
2.
(5.58)
Remark 5.5.10. In the case b = a, the first formula in eq. (5.58) simplifies to
ùîºTa,a =
a
q ‚àíp[
( q
p)a ‚àí1
( q
p)a + 1
] ,
p
Ã∏= 1
2 .
The proof goes along the same lines as that of Corollary 5.5.3. Furthermore, as can be
easily seen, the expected value of Ta,a does not change if one interchanges p and q, or,

5.5 Gambler‚Äôs ruin
‡±™
299
equivalently, players A and B. This is, of course, because the length of the game does
not depend on who players A and B are, provided both start with the same amount of
money.
Note that in the case p
Ã∏= 1/2, it follows that
lim
a‚Üí‚àû
1
a ùîºTa,a =
1
|p ‚àíq| .
So in the long run, the expected time of the game is of order a/|p‚àíq|. Compare this with
the case p = 1/2 where the expected time behaves like a2.
Example 5.5.11. If the game is fair and both players start either with $50 or $100, then on
average the gamblers have to play either 2500 or 10, 000 rounds before there is a winner.
The situation changes drastically if the success probability of one player is diminished
to 0.49. That is, the game is ‚Äúalmost‚Äù fair. In this case the average number of necessary
rounds equals either 1904.13 or 4820.23, respectively.
Let us finally treat a related problem, sometimes called ‚Äúthe monkey at the cliff.‚Äù
A monkey is standing one step from the edge of a cliff and takes repeated independent
steps; forward, with probability p, or backward, with probability q = 1 ‚àíp. What is the
probability that the monkey, sooner or later, will fall off the cliff?
The mathematical formulation is as follows: let (Sn)n‚â•0 be a random walk starting
at zero jumping with probability p to the right and with probability q = 1 ‚àíp to the left.
How likely is it that there exists an n ‚â•1 such that Sn = 1. More generally, one may ask
for the existence of an n ‚â•1 with Sn = b for a given integer b ‚â•1. The answer is as
follows:
Proposition 5.5.12. Let (Sn)n‚â•1 be as before. Then for any integer b ‚â•1 it follows that
‚Ñô{Sn = b for some n ‚â•1} = {1
if p ‚â•1
2,
( p
q)b
if p < 1
2.
Proof. Fix b ‚â•1. Given a ‚â•1, define events Ba as follows: Ba occurs if there is an n ‚â•1
for which Sn = b and, at the same time, Sj > ‚àía if 1 ‚â§j < n. Then A1 ‚äÜA2 ‚äÜ‚ãÖ‚ãÖ‚ãÖand,
moreover,
{Sn = b for some n ‚â•1} =
‚àû
‚ãÉ
a=1
Ba .
To see this, suppose Sn = b and choose a ‚â•1 such that min1‚â§j‚â§n Sj > ‚àía.
Hence, by the continuity of probability measures from below (see property (6) in
Proposition 1.2.1), we obtain
‚Ñô{Sn = b for some n ‚â•1} = lim
a‚Üí‚àû‚Ñô(Ba) .

300
‡±™
5 Expected value, variance, and covariance
Now Proposition 5.5.2 applies and leads to
‚Ñô{Sn = b for some n ‚â•1} = lim
a‚Üí‚àû
{
{
{
{
{
( q
p )a‚àí1
( q
p )a+b‚àí1
if p
Ã∏= 1
2,
a
a+b
if p = 1
2.
Of course,
lim
a‚Üí‚àû
a
a + b = 1
and
lim
a‚Üí‚àû
( q
p)a ‚àí1
( q
p)a+b ‚àí1
= 1
if
q
p < 1 .
Recall that xa Û≥®Ä‚Üí
a‚Üí‚àû0 provided that 0 < x < 1.
It remains to investigate the case q > p or, equivalently, p < 1/2. As before set
x = q/p > 1 and y = 1/x < 1. Doing so, we get
lim
a‚Üí‚àû
xa ‚àí1
xa+b ‚àí1 = lim
a‚Üí‚àû
yb ‚àíya+b
1 ‚àíya+b = yb = (p
q)
b
,
which completes the proof in the remaining case.
Remark 5.5.13. Proposition 5.5.12 asserts that in the case p ‚â•1/2, the monkey will fall
off the cliff with probability one, even if it is not only 1 but b > 1 steps away from the
cliff. On the other hand, if p < 1/2 and the monkey is b steps away from the cliff, then
with probability 1 ‚àí(p/q)b the monkey will be safe. Since in this case p/q < 1, hence
(p/q)b ‚Üí0 as b ‚Üí‚àû, the situation of the monkey improves considerably as soon as it
is further away from the cliff.
Still another way to formulate Proposition 5.5.12 is as follows. Say player A has an
unlimited amount of money while his opponent starts with b dollars. Then A will win
with probability one provided his success probability p satisfies p ‚â•1/2. On the other
hand, in the case of p < 1/2 his chance of winning equals (p/q)b < 1.
Example 5.5.14. Let us investigate how likely it is to win b ‚â•1 dollars in a roulette
provided one has an unlimited amount of money. As before, every time the chance to
win one dollar is p = 18/37 while one loses one dollar with probability q = 19/37. Hence,
in this case we obtain
‚Ñô{Win $b} = (18
19)
b
.
For example, the chance to win $100 possessing an infinite amount of money equals
‚Ñô{Win $100} = (18
19)
100
‚âà0.00448632 .
Compare this result with eq. (5.56) where we got 0.00187859 for the probability to win
$100 when starting with an initial amount of $10 or with the probability 0.00446628

5.5 Gambler‚Äôs ruin
‡±™
301
when starting with $100. So one sees, in order to win $100, it does not make a big dif-
ference whether one starts to play with $100 or with an unlimited amount of money.
The result will be the same in both cases: very likely one is going to lose a lot of money.
Compare Figure 5.9 for the probabilities to win b = 1, . . . , 30 dollars playing roulette
possessing an unlimited amount of money.
Figure 5.9: The probability to win b = 1, . . . , 30 dollars playing roulette possessing an unlimited amount of
money.
Suppose now one does not bet $1 each time, but $10. How likely is it now to win $100?
The answer is at follows: the likelihood to win $100 by $10 steps coincides with that to win
$10 by steps of size $1. Hence, the probability of this event equals (18/19)10 ‚âà0.582357.
Remark 5.5.15. It might be of interest to compare this with the probability 0.368031 in
the case of an initial deposit of $10. This tells us that it is not unlikely to lose at some time
more than $10 before one finally wins $10. For example, the chance to win $10 becomes
for the first time greater than 1/2 if one starts gambling with an amount of $24. Then the
probability to win at some time $10 without going bankrupt equals 0.503344.
But note that this does not mean that one has an advantage. In the case of success,
one wins $10 while one loses $24 in the case of failure. Thus, on average there will be a
loss, no matter how big the initial amount was.
Remark 5.5.16. The symmetric (fair) case p = 1/2 is of special interest. By symmetry,
given b ‚â•1, with probability one there also exists an n ‚â•1 such that Sn = ‚àíb. Thus, with
probability one, a symmetric random walk attains any value in ‚Ñ§. See Example 7.2.15
for further asymptotic properties of symmetric walks.
In view of Proposition 5.5.12 and the previous remark, the following question arises:
let (Sn)n‚â•1 be a random walk starting at zero. Given b ‚àà‚Ñï, how long does it take on
average before the walk reaches level b? To make it more precise, given b ‚â•1, let

302
‡±™
5 Expected value, variance, and covariance
Tb = {min{n ‚â•0 : Sn = b}
if there is an n ‚â•0 with Sn = b,
‚àû
otherwise.
Then Proposition 5.5.12 may be rephrased as follows:
‚Ñô{Tb < ‚àû} = {1
if p ‚â•1
2,
( p
q)b
if p < 1
2.
Proposition 5.5.17. Let (Sn)n‚â•1 be a random walk starting at zero. Given b ‚àà‚Ñï,
ùîºTb = {
b
p‚àíq
if p > 1
2,
‚àû
if p ‚â§1
2.
Proof. If p < 1/2, then ‚Ñô{Tb = ‚àû} = 1 ‚àí(p/q)b > 0, hence ùîºTb = ‚àûas asserted.
The case 1/2 ‚â§p is more involved and may be found in [Sti03, Section 5.6]. Basic
ingredient is the so-called hitting time theorem asserting
‚Ñô{Tb = n} = b
n ‚Ñô{Sn = b}
and
ùîºTb = b
‚àû
‚àë
n=1
‚Ñô{Sn = b} ,
b ‚â•1 .
(5.59)
A heuristic proof of Proposition 5.5.17 (without using the hitting time theorem) can
be given by using eq. (5.58). Assume we know that
ùîºTb = lim
a‚Üí‚àûùîºTa,b
(which is true and can be made precise). Then, if p > 1/2, hence q/p < 1, it follows that
ùîºTb = lim
a‚Üí‚àû[
a
q ‚àíp ‚àía + b
q ‚àíp
( q
p)a ‚àí1
( q
p)a+b ‚àí1
] =
b
p ‚àíq .
The case p = 1/2 is even easier to handle and follows by evaluating the infinite sum in
the right-hand formula of (5.59) or from
ùîºTb = lim
a‚Üí‚àûùîºTa,b = lim
a‚Üí‚àûa ‚ãÖb = ‚àû.
Remark 5.5.18. Most interesting in Proposition 5.5.17 is the case p = 1/2. Assume players
A and B play a series of fair games. Player A has an unlimited amount of money while B
starts with $1. Then A wins with probability 1, but on average it takes an arbitrarily long
time until B goes bankrupt.
Similarly, in the symmetric case, for sure the monkey will fall off the cliff, but on
average it takes a lot of time before this happens.

5.5 Gambler‚Äôs ruin
‡±™
303
Final remark: There exist many other interesting results about random walks not
included in the present book. For example, what can be said about the behavior of
max{Sk : 0 ‚â§k ‚â§n}? How are the zeroes of a symmetric walk (Sn)n‚â•0 distributed? How
about recurrence or transience? How many changes of signs exist until a given time n?
Or what happens if, as in our case, the barriers are not absorbing but reflecting? Neither
did we treat random walks in the more general setting of jumping particles in ‚Ñ§d. We
refer to [Fel68], to [Rev13], or to [Sti03] for further reading about this highly interesting
topic.
But let us shortly discuss one property of the symmetric walk, which, in our opinion,
is very surprising. Let (Sn)n‚â•0 be a symmetric random walk starting at zero. If
L+
n = ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®{k ‚â§n : Sk > 0}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®,
(5.60)
then L+
n is the number of the times until n where the symmetric walk is located in the
positive half-space or, equivalently, where player A is ahead of player B provided both
players start with an unlimited amount of money. Probably everybody will guess that
for large n, player A will be about half of the time ahead B, and during the other half, B
will be ahead A. Recall that the walk is symmetric, hence jumps to the right are as likely
as those to the left.
The following result shows that this is not so. It is much more likely that most of
the time one of the players is ahead of the other. To verify this, one investigates the
proportion L+
n/n of times where the walk is above zero. Here the following holds (see
[Sti03, Section 6.8]):
Proposition 5.5.19 (Arcsine law for random walks). Let L+
n be defined by (5.60). Then for
any 0 < t < 1 it holds that
lim
n‚Üí‚àû‚Ñô{L+
n
n ‚â§t} = 2
œÄ arcsin(‚àöt) = 1
œÄ
t
‚à´
0
1
‚àöx(1 ‚àíx)
dx .
Thus, the random variables (L+
n/n)n‚â•1 converge in distribution, that is, in the sense
of Definition 7.2.1, to an arcsine distribution. Recall that the arcsine distribution was
introduced in Definition 1.6.35.
Corollary 5.5.20. If 0 ‚â§a < b ‚â§1, then for sufficiently large n we have
‚Ñô{a ‚â§L+
n
n ‚â§b} ‚âà2
œÄ [arcsin(‚àöb) ‚àíarcsin(‚àöa)] .
The numerical values in the cases a = 0.1, a = 0.2, and a = 0.5 are
‚Ñô{0.05 ‚â§L+
n /n ‚â§0.15}
‚Ñô{0.15 ‚â§L+
n /n ‚â§0.25}
‚Ñô{0.45 ‚â§L+
n /n ‚â§0.55}
‚âà0.1096
‚âà0.0802
‚âà0.06377

304
‡±™
5 Expected value, variance, and covariance
These values, as well as Figure 5.10, tell us that it is much more likely that L+
n/n is
near zero or one than near 0.5. Recall that L+
n/n is the proportion of those times k ‚â§n
where Sk > 0. Hence, the event that L+
n/n near 0.5 occurs if the walk is about half of the
time positive and the other half it is negative.
Figure 5.10: The approximate probabilities ‚Ñô{a ‚àí0.05 ‚â§L+
n /n ‚â§a + 0.05} with a ‚àà[0.05, 0.95].
Summary: Two persons A and B play a series of games as long as one of them goes bankrupt. Hereby, in
every single game the loser has to pay $1 to the winner. The describing mathematical model is a random
walk starting at zero and with absorbing barriers at ‚àía and b. Here a and b are the initial amounts of A and
B, respectively. Equivalently, one may regard a random walk starting at a and with barriers at 0 and a + b. Let
p be the success probability of player A, thus q = 1 ‚àíp is that of player B. Then the basic result asserts
‚Ñô{B goes bankrupt} = ‚Ñô{A wins} =
{
{
{
{
{
( q
p )a‚àí1
( q
p )a+b‚àí1
if p
Ã∏= 1
2,
a
a+b
if p = 1
2.
5.6 Problems
Problem 5.1.
1.
Put successively and independently of each other n particles into N boxes. Thereby,
each box is equally likely. How many boxes remain empty on average?
Hint: Define random variables X1, . . . , XN as follows: set Xi = 1 if box i remains empty,
and Xi = 0 otherwise.
2.
Fifty persons write randomly (according to the uniform distribution), and indepen-
dently of each other, one of the 26 letters in the alphabet on a sheet of paper. On
average, how many different letters appear?

5.6 Problems
‡±™
305
3.
In a factory with N ‚â•1 employees, every day of the year on which one of the em-
ployees has a birthday is a holiday. Let EN be the expected number of working days,
that is, the expected number of days which are not a holiday. For which N ‚â•1 does
N ‚ãÖEN (the expected total working time) become maximal? Hereby one assumes that
all 365 days of the year are equally likely to be birthdays.
Problem 5.2 (A. E. Lawrance, 1969). An urn contains eight white balls and two black.
Choose one after another a ball without replacing the chosen one. Let 1 ‚â§r ‚â§9 be the
number of that choice where for the first time a black ball occurs. Which number r is
most likely for the appearance of the first black ball? Evaluate the average value over
all possible numbers r ‚â§9.
Answer the same questions in the case that n ‚â•2 balls are in the urn where 2 are
black and n ‚àí2 are white.
Problem 5.3 (De Moivre, 1756). A man rolls a fair die six times. He gets an amount of M
francs, every time he
1.
rolls a ‚Äú1‚Äù or
2.
if he rolls at least one ‚Äú1.‚Äù
Evaluate in both cases the expected amount of money he gets.
Problem 5.4. Let (Œ©, ùíú, ‚Ñô) be a probability space. Given (not necessarily disjoint) events
A1, . . . , An in ùíúand real numbers Œ±1, . . . , Œ±n, define X : Œ© ‚Üí‚Ñùby16
X :=
n
‚àë
j=1
Œ±j1Aj .
1.
Why is X a random variable?
2.
Prove
ùîºX =
n
‚àë
j=1
Œ±j‚Ñô(Aj)
and
ùïçX =
n
‚àë
i,j=1
Œ±iŒ±j[‚Ñô(Ai ‚à©Aj) ‚àí‚Ñô(Ai)‚Ñô(Aj)].
How does ùïçX simplify for independent events A1, . . . , An?
Problem 5.5. Suppose a fair ‚Äúdie‚Äù has k faces labeled by the numbers from 1 to k.
1.
How often one has to roll the die on the average before the first ‚Äú1‚Äù shows up?
2.
Suppose one rolls the die exactly k times. Let pk be the probability that ‚Äú1‚Äù ap-
pears exactly once and qk is the probability that ‚Äú1‚Äù shows up at least once. Com-
pute pk and qk and determine their behavior as k ‚Üí‚àû, that is, find limk‚Üí‚àûpk and
limk‚Üí‚àûqk.
16 For the definition of indicator functions 1Ai, see eq. (3.21).

306
‡±™
5 Expected value, variance, and covariance
Problem 5.6.
1.
Let X be a random variable with values in ‚Ñï0 = {0, 1, 2, . . . }. Prove that
ùîºX =
‚àû
‚àë
k=1
‚Ñô{X ‚â•k} .
2.
Suppose now that X is continuous with ‚Ñô{X ‚â•0} = 1. Verify
‚àû
‚àë
k=1
‚Ñô{X ‚â•k} ‚â§ùîºX ‚â§1 +
‚àû
‚àë
k=1
‚Ñô{X ‚â•k} .
Problem 5.7. Let X be an ‚Ñï0-valued random variable with
‚Ñô{X = k} = q‚àík,
k = 1, 2, . . .
for some q ‚â•2.
(a) Why do we have to suppose q ‚â•2, although ‚àë‚àû
k=1 q‚àík < ‚àûfor q > 1?
(b) Determine ‚Ñô{X = 0}?
(c) Compute ùîºX by the formula in Problem 5.6.
(d) Compute ùîºX directly by ùîºX = ‚àë‚àû
k=0 k ‚Ñô{X = k}.
Problem 5.8. Two independent random variables X and Y with finite third moment
satisfy ùîºX = ùîºY = 0. Prove that then
ùîº(X + Y)3 = ùîºX3 + ùîºY 3 .
Problem 5.9. A random variable X is PoisŒª-distributed for some Œª > 0. Evaluate
ùîº(
1
1 + X )
and
ùîº(
X
1 + X ).
Problem 5.10. In a lottery, 6 of 49 numbers are randomly chosen. Let X be the largest
number of the 6. Show that
ùîºX = 6 ‚ãÖ43!
49!
49
‚àë
k=6
k(k ‚àí1)(k ‚àí2)(k ‚àí3)(k ‚àí4)(k ‚àí5) = 42.8571 .
Evaluate ùîºX if X is the smallest number of the 6 chosen.
Hint: Either one modifies the calculations for the maximal value suitably or one
reduces the second problem to the first by an easy algebraic operation.
Problem 5.11. A fair coin is labeled by ‚Äú0‚Äù on one side and with ‚Äú1‚Äù on the other. Toss it
four times. Let X be the sum of the first two tosses and Y be the sum of all four. Determine
the joint distribution of X and Y. Evaluate Cov(X, Y), as well as œÅ(X, Y).

5.6 Problems
‡±™
307
Problem 5.12. In an urn there are five balls, two labeled by ‚Äú0‚Äù and three by ‚Äú1.‚Äù Choose
two balls without replacement. Let X be the number on the first ball and Y that on the
second.
1.
Determine the distribution of the random vector (X, Y) and its marginal distribu-
tions.
2.
Compute œÅ(X, Y).
3.
Which distribution does X + Y possess?
Problem 5.13. Among 40 students there are 30 men and 10 women. Also, 25 of the 30
men and 8 of the 10 women passed an exam successfully. Choose randomly, according
to the uniform distribution, one of the 40 students. Let X = 0 if the chosen person is a
man, and X = 1 if it is a woman. Furthermore, set Y = 0 if the person failed the exam,
and Y = 1 if she or he passed.
1.
Find the joint distribution of X and Y.
2.
Are X and Y independent? If not, evaluate Cov(X, Y).
3.
Are X and Y negatively or positively correlated? What does it express, when X and
Y are positively or negatively correlated?
Problem 5.14. Let (Œ©, ùíú, ‚Ñô) be a probability space. Prove, for any two events A and B
in ùíú, the estimate
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚Ñô(A ‚à©B) ‚àí‚Ñô(A) ‚Ñô(B)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚â§1
4 .
Is it possible to improve the upper bound 1
4?
Problem 5.15 (Problem of Luca Pacioli in 1494; the first correct solution was found by Blaise
Pascal in 1654). Two players, say A and B, are playing a fair game consisting of several
rounds. The first player who wins six rounds wins the game and the stakes of 20 taler17
that have been bet throughout the game. However, one day the game is interrupted and
must be stopped. If player A has won five rounds and player B has won three rounds,
how should the stakes be divided fairly among the players?
Problem 5.16 (B. Pascal, 1654). Three players, say A, B, and C, play a series of fair games.
Whoever first wins three games is the winner. One day the series of games had to be
stopped before one of the three players had won three games. Player A still needs one
win, B and C still need two wins each. How to distribute the stakes in this case in a fair
way?
Problem 5.17. In Example 5.1.49, we computed the average number of necessary pur-
chases to get all n pictures. Let m be an integer with 1 ‚â§m < n. How many purchases
are necessary on average to possess m of the n pictures?
17 Former German currency, root of the word ‚Äúdollar.‚Äù

308
‡±™
5 Expected value, variance, and covariance
For n even, choose m = n/2, and for n odd take m = (n ‚àí1)/2. Let Mn be the av-
erage number of purchases to get m pictures, that is, to get half of the pictures. Deter-
mine
lim
n‚Üí‚àû
Mn
n .
Hint: Use eq. (5.29).
Problem 5.18. Compute ùîº|X|2n+1 for a standard normal distributed X and n = 0, 1, . . .
Problem 5.19. Suppose X has the density
p(x) = {0
if x < 1,
cŒ± xŒ±
if x ‚â•1,
for some Œ± < ‚àí1.
1.
Determine cŒ± such that p is a density.
2.
For which n ‚â•1 does X possess an nth moment?
Problem 5.20. Let U be uniform distributed on an interval [Œ±, Œ≤]. Show that for n ‚â•1,
ùîºUn = Œ≤n + Œ±Œ≤n‚àí1 + ‚ãÖ‚ãÖ‚ãÖ+ Œ±n‚àí1Œ≤ + Œ±n
n + 1
.
Problem 5.21. Let X1, . . . , Xn be random variables with finite second moment and with
ùîºXj = 0. Show that
ùîº[X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn]2 =
n
‚àë
i,j=1
Cov(Xi, Xj) =
n
‚àë
j=1
ùïçXj + 2
‚àë
1‚â§i<j‚â§n
Cov(Xi, Xj) .
Problem 5.22. Show that
ùîºX = nM
N
for a hypergeometrically distributed random variable X with
‚Ñô{X = m} =
(M
m) (N‚àíM
n‚àím)
(N
n)
,
m = 0, . . . , n .
Problem 5.23. Let X be ùí©(0, 1)-distributed. Determine ùïçX3 and ùïçX4.
Problem 5.24. Given a nonnegative random variable X, define œÜX from [0, 1] to [0, 1] by
œÜX(t) = ùîºtX. Then œÜX is called the generating function of X (see [GS20, Section 5.1]).
(1) Suppose X has values in ‚Ñï0. Show that, if t ‚â•0, then this ‚Äúnew‚Äù definition of the
generating function coincides with that given in Problem 4.2.

5.6 Problems
‡±™
309
(2) Let X1, . . . , Xn be independent and nonnegative. For Œ±j ‚â•0, 1 ‚â§j ‚â§n, let
X = Œ±1X1 + ‚ãÖ‚ãÖ‚ãÖ+ Œ±nXn .
Prove
œÜX(t) = œÜX1(tŒ±1) ‚ãÖ‚ãÖ‚ãÖœÜXn(tŒ±n) .
(3) Find œÜX for an exponentially distributed X.
Problem 5.25. Complete the arguments stated in Remark 5.4.4. That is, argue why the
following questions are equivalent:
1.
Toss a fair coin n times and choose after that at random a number 1 ‚â§j ‚â§n. Suppose
the jth toss was a ‚Äú1.‚Äù What is the probability that under this condition the observed
sequence has k ‚Äú1‚Äùs for some 1 ‚â§k ‚â§n?
2.
One tosses a fair coin n ‚àí1 times. How likely is the appearance of k ‚àí1 ‚Äú1‚Äùs?

6 Normally distributed random vectors
6.1 Representation and density
In Example 3.4.3 we considered a two-dimensional random vector (X1, X2), where X1 was
the height of a randomly chosen person and X2 was his weight. From experience and in
view of the central limit theorem (cf. Section 7.2), it is quite reasonable to assume that X1
and X2 are normally distributed. Suppose we are able to determine their expected values
and their variances. However, this is not sufficient to describe the experiment. Why? The
random variables X1 and X2 are surely dependent, and the most interesting problem is to
describe their degree of dependence. This cannot be done based only on the knowledge
of their distributions. What we really need to know is their joint distribution. Therefore,
we not only have to suppose X1 and X2 to be normal, but the generated vector (X1, X2)
has to be as well.
But what does it mean that a random vector is normally distributed? This section is
devoted to answer this and related questions.
Let us first recall the univariate case, investigated in Example 4.2.2 and in the subse-
quent Proposition 4.2.3. The main observation was that a random variable Y is normally
distributed if and only if it may be written as
Y = aX + Œº
(6.1)
for some a
Ã∏= 0, Œº ‚àà‚Ñù, and a standard normal random variable X.
Let now ‚ÉóY = (Y1, . . . , Yn) be an n-dimensional random vector. We want to represent
it in the same way as Y in eq. (6.1). Consequently, we have to replace X by a multivariate
standard normal vector and the function x Û≥®É‚Üíax + Œº by a suitable mapping from ‚Ñùn
to ‚Ñùn. But which kind of mapping should this be and what is an n-dimensional standard
normal vector?
Let us begin by answering the second question. Therefore, recall the definition of
the multivariate standard normal distribution ùí©(0, 1)‚äón introduced in Definition 1.9.21.
This probability measure on (‚Ñùn, ‚Ñ¨(‚Ñùn)) acts as follows: if B ‚àà‚Ñ¨(‚Ñùn), then its probability
equals
ùí©(0, 1)‚äón(B) =
1
(2œÄ)n/2 ‚à´
B
e‚àí|x|2/2 dx
=
1
(2œÄ)n/2 ‚à´‚ãÖ‚ãÖ‚ãÖ‚à´
‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
B
e‚àí(x2
1+‚ãÖ‚ãÖ‚ãÖ+x2
n)/2 dxn ‚ãÖ‚ãÖ‚ãÖdx1.
Thus, a random vector ‚ÉóX should be standard normally distributed whenever its proba-
bility distribution is ùí©(0, 1)‚äón. Let us formulate this as a definition.
https://doi.org/10.1515/9783111325064-006

6.1 Representation and density
‡±™
311
Definition 6.1.1. A random vector ‚ÉóX = (X1, . . . , Xn) is standard normally distributed (or is standard
normal) if its probability distribution satisfies ‚Ñô‚ÉóX = ùí©(0, 1)‚äón.
To make this definition more descriptive, let us state some equivalent properties.
Proposition 6.1.2. For a random vector ‚ÉóX = (X1, . . . , Xn), the following are equivalent:
1.
‚ÉóX is standard normal.
2.
If B ‚àà‚Ñ¨(‚Ñùn), then
‚Ñô{ ‚ÉóX ‚ààB} =
1
(2œÄ)n/2 ‚à´
B
e‚àí|x|2/2 dx .
3.
The coordinate mappings X1, . . . , Xn are (univariate) standard normally distributed
and independent. That is, for all tj ‚àà‚Ñù, 1 ‚â§j ‚â§n,
‚Ñô{X1 ‚â§t1, . . . , Xn ‚â§tn} = ‚Ñô{X1 ‚â§t1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn ‚â§tn}
= (
1
‚àö2œÄ
t1
‚à´
‚àí‚àû
e‚àíx2
1/2dx1) ‚ãÖ‚ãÖ‚ãÖ(
1
‚àö2œÄ
tn
‚à´
‚àí‚àû
e‚àíx2
n/2dxn) .
Proof. Taking into account the definition of ùí©(0, 1)‚äón, this is an immediate consequence
of Propositions 3.6.5 and 3.6.20. Compare also the considerations in Example 3.6.24.
Remark 6.1.3. The density of the n-dimensional standard normal distribution possesses
nice properties: it attains its maximal value (2œÄ)‚àín/2 at zero, it is invariant under rota-
tions of the arguments and its level sets are circles. See Figure 6.1 for the graph of this
density in the case n = 2.
Figure 6.1: The density of a 2-dimensional standard normal vector.
An adequate substitute for x Û≥®É‚Üíax + Œº in representation (6.1) is still undetermined.
Which mappings in ‚Ñùn should be considered?

312
‡±™
6 Normally distributed random vectors
Observe that x Û≥®É‚Üíax +Œº is affine linear from ‚Ñùto ‚Ñù. The counterpart in ‚Ñùn is of the
form x Û≥®É‚ÜíAx + Œº, where A is a linear mapping in ‚Ñùn and Œº ‚àà‚Ñùn. Linear mappings in ‚Ñùn
are described by n √ó n matrices A = (Œ±ij)n
i,j=1 and act as follows:
Ax = (
n
‚àë
j=1
Œ±1jxj, . . . ,
n
‚àë
j=1
Œ±njxj) ,
x = (x1, . . . , xn) ‚àà‚Ñùn .
Consequently, the suitable generalization of x Û≥®É‚Üíax + Œº is the mapping x Û≥®É‚ÜíAx + Œº with
an n √ó n matrix A and Œº ‚àà‚Ñùn. The condition a
Ã∏= 0 transfers to det(A)
Ã∏= 0 or, equivalently,
A has to be regular, that is, the generated mapping is one-to-one from ‚Ñùn onto ‚Ñùn. Here
and in the sequel we will use results and notations as presented in Section A.4.
Now we are in position to define normally (distributed) random vectors.
Definition 6.1.4. A random vector
‚ÉóY is said to be normally distributed (or simply, normal) provided
there exists a regular n √ó n matrix A and a vector Œº ‚àà‚Ñùn such that
‚ÉóY = A ‚ÉóX + Œº
(6.2)
for some standard normal ‚ÉóX.
Remark 6.1.5. Let us reformulate Definition 6.1.4 due to its importance. A random vec-
tor
‚ÉóY = (Y1, . . . , Yn) is normally distributed if and only if there exists a regular matrix
A = (Œ±ij)n
i,j=1 and a vector Œº = (Œº1, ‚Ä¶, Œºn) such that
Yi =
n
‚àë
j=1
Œ±ijXj + Œºi ,
1 ‚â§i ‚â§n ,
with X1, . . . , Xn independent ùí©(0, 1)-distributed.
Example 6.1.6. Suppose the three-dimensional random vector ‚ÉóY = (Y1, Y2, Y3) is defined
by
Y1 = 2X1 + X2 ‚àíX3 + 4 ,
Y2 = X1 ‚àí2X2 + X3 ‚àí2,
and
Y3 = X1 ‚àí2X3 + 5
with ùí©(0, 1)-distributed independent X1, X2, X3. Then ‚ÉóY is normally distributed. Observe
that it may be represented in the form of eq. (6.2) with A given by
A = (
2
1
‚àí1
1
‚àí2
1
1
0
‚àí2
)
and with Œº = (4, ‚àí2, 5). Moreover, we have det(A) = 9, hence A is regular.
Remark 6.1.7. If the n-dimensional vector ‚ÉóY is represented as ‚ÉóY = A ‚ÉóX + Œº with ‚ÉóX stan-
dard normal, Œº ‚àà‚Ñùn and a nonregular n √ó n-matrix A, then ‚ÉóY may also be regarded as

6.1 Representation and density
‡±™
313
normal, yet in a more general setting. In this case it follows that ‚Ñô{ ‚ÉóY ‚ààrange(A) +Œº} = 1
where range(A) is a strict subspace of ‚Ñùn. For example, if Y1 = X1 + X2 and Y2 = ‚àíX1 ‚àíX2,
then ‚Ñô{ ‚ÉóY ‚ààE} = 1 with E = {(t, ‚àít) : t ‚àà‚Ñù}. Thus, ‚Ñô‚ÉóY is concentrated on the subspace
E
Ã∏= ‚Ñù2. Here and in the sequel, we want to exclude such ‚Äúdegenerated‚Äù normal vectors
by assuming that the generating matrix A is regular.
Given a normal vector
‚ÉóY, how do we get the standard normal
‚ÉóX in representa-
tion (6.2)? The next proposition answers this question.
Proposition 6.1.8. A random vector ‚ÉóY = (Y1, . . . , Yn) is normal if and only if there exists a
regular n √ó n matrix B = (Œ≤ij)n
i,j=1 and a vector ŒΩ = (ŒΩ1, . . . , ŒΩn) ‚àà‚Ñùn such that the random
variables Xi, defined by
Xi :=
n
‚àë
j=1
Œ≤ijYj + ŒΩi ,
1 ‚â§i ‚â§n ,
are independent standard normal.
Proof. This is a direct consequence of the following observation. One has ‚ÉóY = A ‚ÉóX + Œº if
and only if ‚ÉóX may be represented as ‚ÉóX = A‚àí1 ‚ÉóY ‚àíA‚àí1Œº. Therefore, the assertion follows
by choosing B and ŒΩ such that B = A‚àí1 and ŒΩ = ‚àíA‚àí1Œº.
Example 6.1.9. For the random vector
‚ÉóY investigated in Example 6.1.6, the generated
independent standard normal random variables X1, X2, and X3 may be represented as
follows:
X1 = 1
9(4Y1 + 2Y2 ‚àíY3 + 7),
X2 = 1
9(Y1 ‚àíY2 ‚àíY3 + 1),
X3 = 1
9(2Y1 + Y2 ‚àí5Y3 ‚àí19).
Suppose ‚ÉóY = A ‚ÉóX+Œº is a normal vector. How can we evaluate its distribution density?
To answer this question, we introduce the following function. Let R > 0 be an n√ón-matrix
and Œº ‚àà‚Ñùn. The inverse matrix of R is R‚àí1, and to simplify the notation, set |R| = det(R).
Observe that R > 0 implies |R| > 0. With these notations, we define a function pŒº,R from
‚Ñùn to ‚Ñùby
pŒº,R(x) :=
1
(2œÄ)n/2|R|1/2 e ‚àí‚ü®R‚àí1(x‚àíŒº),(x‚àíŒº)‚ü©/2 ,
x ‚àà‚Ñùn .
(6.3)
Note that the expression in the exponent may be written as follows. If R‚àí1 = ( ÃÉrij)n
i,j=1 is
the inverse matrix of R, then one gets
‚ü®R‚àí1(x ‚àíŒº), (x ‚àíŒº)‚ü©/2 = 1
2
n
‚àë
i,j=1
ÃÉrij(xi ‚àíŒºi)(xj ‚àíŒºj) .
Now we are prepared to answer the above question about the density of ‚ÉóY.

314
‡±™
6 Normally distributed random vectors
Proposition 6.1.10. Suppose the normal vector ‚ÉóY is represented as in eq. (6.2) with regular
A and Œº ‚àà‚Ñùn. Define the positive matrix R by R = AAT. Then pŒº,R, as given in eq. (6.3), is
the distribution density of ‚ÉóY. In other words, if B ‚àà‚Ñ¨(‚Ñùn), then
‚Ñô{ ‚ÉóY ‚ààB} =
1
(2œÄ)n/2|R|1/2 ‚à´
B
e ‚àí‚ü®R‚àí1(x‚àíŒº),(x‚àíŒº)‚ü©/2 dx .
Proof. Because ‚ÉóY = A ‚ÉóX + Œº with ‚ÉóX standard normal, Proposition 6.1.2 implies
‚Ñô{ ‚ÉóY ‚ààB} = ‚Ñô{A ‚ÉóX + Œº ‚ààB} = ‚Ñô{ ‚ÉóX ‚ààA‚àí1(B ‚àíŒº)}
=
1
(2œÄ)n/2
‚à´
A‚àí1(B‚àíŒº)
e‚àí|y|2/2 dy
for any Borel set B ‚äÜ‚Ñùn. Hereby, B‚àíŒº denotes the set {b‚àíŒº : b ‚ààB}. In the next step, we
change the variables by setting x = Ay + Œº. Then dx = | det(A)|dy, where by assumption
det(A) Ã∏= 0 and, moreover, we have y ‚ààA‚àí1(B ‚àíŒº) if and only if x ‚ààB. Therefore, the last
integral transforms to
‚Ñô{ ‚ÉóY ‚ààB} =
1
(2œÄ)n/2
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®det(A)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚àí1 ‚à´
B
e‚àí|A‚àí1(x‚àíŒº)|2/2 dx .
(6.4)
Proposition A.4.1 implies R > 0 and, moreover,
|R| = det(R) = det(AAT) = det(A) ‚ãÖdet(AT) = det(A)2 .
Since |R| = det(R) > 0, this leads to |R|1/2 = | det(A)|, that is, to
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®det(A)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚àí1 = |R|‚àí1/2 .
(6.5)
Note that
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®A‚àí1(x ‚àíŒº)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
2 = ‚ü®A‚àí1(x ‚àíŒº), A‚àí1(x ‚àíŒº)‚ü©= ‚ü®(A‚àí1)
TA‚àí1(x ‚àíŒº), (x ‚àíŒº)‚ü©,
which, due to
(A‚àí1)
T ‚àòA‚àí1 = (AT)
‚àí1 ‚àòA‚àí1 = (A ‚àòAT)
‚àí1 = R‚àí1,
implies
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®A‚àí1(x ‚àíŒº)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
2 = ‚ü®R‚àí1(x ‚àíŒº), (x ‚àíŒº)‚ü©.
(6.6)
Plugging eqs. (6.5) and (6.6) into eq. (6.4), we get

6.1 Representation and density
‡±™
315
‚Ñô{ ‚ÉóY ‚ààB} = ‚à´
B
pŒº,R(x) dx
with pŒº,R as in eq. (6.3). This completes the proof.
Remark 6.1.11. How does Proposition 6.1.10 look like for n = 1? Here Y = aX + Œº, that is,
A = (a), and since A has to be regular, this implies a
Ã∏= 0. Hence we get R = AAT = (a2),
R‚àí1 = (a‚àí2), and |R|1/2 = |a|. Thus, the density of Y is given by
pŒº,R(x) =
1
(2œÄ)1/2|R|1/2 e ‚àí‚ü®R‚àí1(x‚àíŒº),x‚àíŒº‚ü©/2 =
1
(2œÄ)1/2|a|e ‚àí(x‚àíŒº)2/2a2
,
x ‚àà‚Ñù.
This coincides with the result obtained in Example 4.2.2.
In view of Proposition 6.1.10, we will use the following notation.
Definition 6.1.12. A normal vector ‚ÉóY is said to be ùí©(Œº, R)-distributed if pŒº,R is its density, that is, if
‚Ñô{ ‚ÉóY ‚ààB} =
1
(2œÄ)n/2|R|1/2 ‚à´
B
e ‚àí‚ü®R‚àí1(x‚àíŒº),(x‚àíŒº)‚ü©/2 dx .
Remark 6.1.13. It follows from Proposition A.4.2 that, given any Œº ‚àà‚Ñùn and any R > 0,
there exists a normal vector ‚ÉóY that is ùí©(Œº, R)-distributed. Indeed, write R > 0 as R = AAT
and set
‚ÉóY = A ‚ÉóX + Œº with
‚ÉóX standard normal. Then
‚ÉóY is ùí©(Œº, R)-distributed by Proposi-
tion 6.1.10.
{Distributions of ‚Ñùn-valued normal vectors}
‚áî
{(Œº, R) : Œº ‚àà‚Ñùn, n √ó n matrix R > 0}.
Example 6.1.14. Assume
Y1 = X1 ‚àíX2 + 3
and
Y2 = 2X1 + X2 ‚àí2
for X1, X2 independent ùí©(0, 1)-distributed. Then we get
Œº = (3, ‚àí2)
and
A = ( 1
‚àí1
2
1 ) ,
which implies
R = AAT = ( 1
‚àí1
2
1
) ‚ãÖ(
1
2
‚àí1
1
) = ( 2
1
1
5
) .
(6.7)
Thus, ‚ÉóY is ùí©(Œº, R)-distributed with Œº = (3, ‚àí2) and R as in eq. (6.7).
Which density does ‚ÉóY possess? To answer this, we have to compute det(R) and R‚àí1.
One easily gets det(R) = 9. The inverse matrix of R equals

316
‡±™
6 Normally distributed random vectors
R‚àí1 = 1
9 (
5
‚àí1
‚àí1
2 ) .
Therefore, the distribution density pŒº,R of ‚ÉóY = (Y1, Y2) is given by (see Fig. 6.2)
pŒº,R(x1, x2) = 1
6œÄ exp(‚àí1
2‚ü®R‚àí1(x1 ‚àí3, x2 + 2), (x1 ‚àí3, x2 + 2)‚ü©)
= 1
6œÄ exp(‚àí1
18[5(x1 ‚àí3)2 ‚àí2(x1 ‚àí3)(x2 + 2) + 2(x2 + 2)2]) .
(6.8)
Figure 6.2: The density given by eq. (6.8). It attains its maximal value 1/6œÄ at the point (3, ‚àí2).
For later purposes, we have to name the probability measures on (‚Ñùn, ‚Ñ¨(‚Ñùn)) ap-
pearing as distributions of normal vectors.
Definition 6.1.15. Given Œº ‚àà‚Ñùn and R > 0, the probability measure ùí©(Œº, R) on (‚Ñùn, ‚Ñ¨(‚Ñùn)) is defined
by
ùí©(Œº, R)(B) = ‚à´
B
pŒº,R(x) dx =
1
(2œÄ)n/2|R|1/2 ‚à´
B
e‚àí1
2 ‚ü®R‚àí1(x‚àíŒº),(x‚àíŒº)‚ü©dx .
Measure ùí©(Œº, R) is called a multivariate normal distribution with expected value Œº and covariance
matrix R.
According to Definition 6.1.15, we may now formulate Proposition 6.1.10 as follows:
Proposition 6.1.16. Let ‚ÉóY be a random vector. Then the following are equivalent:
1.
‚ÉóY is ùí©(Œº, R)-distributed.
2.
‚Ñô‚ÉóY = ùí©(Œº, R).
3.
There is a regular n √ó n matrix A with R = AAT such that for some standard normal
‚ÉóX one has ‚ÉóY = A ‚ÉóX + Œº.
Remark 6.1.17. If
‚ÉóY is ùí©(Œº, R)-distributed, the representing matrix A in (3) is not
unique; compare Remark A.4.3. This is already so in the univariate case where an

6.1 Representation and density
‡±™
317
ùí©(Œº, œÉ2)-distributed random variable Y may either be represented as Y = œÉX + Œº or as
Y = (‚àíœÉ)X‚Ä≤ + Œº for some standard normal random variables X and X‚Ä≤.
Remark 6.1.18. The case R = In (as in Section A.4, we denote the identity matrix in ‚Ñùn
by In) and Œº = 0 is of special interest. Because I‚àí1
n = In and det(In) = 1, we get
p0,In(x) =
1
(2œÄ)n/2 e‚àí|x|2/2 ,
x ‚àà‚Ñùn .
This tells us that ùí©(0, In) is nothing else as the multivariate standard normal distribution
introduced in Definition 1.9.21. Written as formula, this means
ùí©(0, 1)‚äón = ùí©(0, In) .
More generally, in view of eq. (1.86), it follows that
ùí©(Œº, œÉ2)
‚äón = ùí©( ‚ÉóŒº, œÉ2 In)
where ‚ÉóŒº = (Œº, . . . , Œº) ‚àà‚Ñùn and œÉ > 0. In other words,
ùí©(Œº, œÉ2)
‚äón(B) = ùí©( ‚ÉóŒº, œÉ2 In)(B) =
1
(2œÄ)n/2œÉn ‚à´
B
e‚àí|x‚àí‚ÉóŒº|2/2œÉ2
dx .
(6.9)
For later purposes, the next result is of importance.
Proposition 6.1.19. Suppose a normal vector ‚ÉóY = (Y1, . . . , Yn) may be written as
‚ÉóY = U ‚ÉóX
with an ùí©(0, In)-distributed (standard normal)
‚ÉóX and a unitary matrix U. Then its coor-
dinate mappings Y1, . . . , Yn are independent standard normal random variables.
Proof. The random vector ‚ÉóY is ùí©(0, UUT)-distributed. But U is unitary, hence, UUT = In
and ‚ÉóY is ùí©(0, In) or, equivalently, standard normally distributed. Then the assertion fol-
lows by Proposition 6.1.2.
Remark 6.1.20. The previous result may be phrased also as follows: If B is a Borel set in
‚Ñùn and B‚Ä≤ = U(B) = {U(x) : x ‚ààB} for some unitary transformation U, then this implies
ùí©(0, In)(B) = ùí©(0, In)(B‚Ä≤) .
That is, the n-dimensional standard normal distribution is invariant under unitary
transformations as, e. g., rotations or reflections.
Example 6.1.21. For Œ∏ ‚àà[0, 2œÄ), define the 2 √ó 2 matrix U by
U = (
cos Œ∏
sin Œ∏
‚àísin Œ∏
cos Œ∏ ) .

318
‡±™
6 Normally distributed random vectors
The matrix U is unitary (it is a rotation by the angle Œ∏) and, due to Proposition 6.1.19, the
vector ‚ÉóY = U ‚ÉóX is standard normal. In other words, given independent standard normal
X1 and X2, for each Œ∏ ‚àà[0, 2œÄ) the random variables
Y1 := cos Œ∏ X1 + sin Œ∏ X2
and
Y2 = ‚àísin Œ∏ X1 + cos Œ∏ X2
are independent and standard normally distributed as well. That is, if a1 < b1 and
a2 < b2, then we obtain
‚Ñô{a1 ‚â§Y1 ‚â§b1, a2 ‚â§Y2 ‚â§b2} = 1
2œÄ (
b1
‚à´
a1
e‚àíx2
1/2dx1)(
b2
‚à´
a2
e‚àíx2
2/2dx2)
= 1
2œÄ
‚à¨
[a1,b1]√ó[a2,b2]
e‚àí|x|2/2dx .
6.2 Expected value and covariance matrix
We start with the following definition.
Definition 6.2.1. Let ‚ÉóY = (Y1, . . . , Yn) be a random vector such that ùîº|Yj| < ‚àûfor all 1 ‚â§j ‚â§n. Then the
vector
ùîº‚ÉóY := (ùîºY1, . . . , ùîºYn) = (Œº1, . . . , Œºn)
is called the (multivariate) expected value of ‚ÉóY.
If ùîºY2
j < ‚àû, 1 ‚â§j ‚â§n, then the matrix
Cov ‚ÉóY := (Cov(Yi, Yj))
n
i,j=1 = (ùîº(Yi ‚àíŒºi)(Yj ‚àíŒºj))
n
i,j=1
is said to be the covariance matrix of ‚ÉóY.
Remark 6.2.2. It is important to notice that both ùîº‚ÉóY and the covariance matrix Cov ‚ÉóY
depend only on the distribution of ‚ÉóY. That is, whenever ‚Ñô‚ÉóY1 = ‚Ñô‚ÉóY2, then
ùîº‚ÉóY1 = ùîº‚ÉóY2
and
Cov ‚ÉóY1 = Cov ‚ÉóY2 .
The next proposition describes the (multivariate) expected value and the covari-
ance matrix of a normally distributed vector.
Proposition 6.2.3. Assume ‚ÉóY = A ‚ÉóX+Œº for some regular matrix A, with ‚ÉóX standard normal
and Œº ‚àà‚Ñùn. Define R = (rij)n
i,j=1 as R = AAT. Then the following are valid:
(1) We have ùîº‚ÉóY = Œº and Cov ‚ÉóY = (Cov(Yi, Yj))n
i,j=1 = R.
(2) Given a ‚àà‚Ñùn, a
Ã∏= 0, then ‚ü®‚ÉóY, a‚ü©is a normal random variable with expected value
‚ü®Œº, a‚ü©and variance ‚ü®Ra, a‚ü©.

6.2 Expected value and covariance matrix
‡±™
319
(3) The coordinate mappings Y i are ùí©(Œºi, rii)-distributed, 1 ‚â§i ‚â§n, that is, the marginal
distributions of ‚ÉóY are the probability measures ùí©(Œºi, rii).
Proof. By assumption,
Yi =
n
‚àë
j=1
Œ±ijXj + Œºi ,
i = 1, . . . , n ,
(6.10)
hence, the linearity of the expected value and ùîºXj = 0 imply
ùîºYi =
n
‚àë
j=1
Œ±ij ùîºXj + Œºi = Œºi ,
1 ‚â§i ‚â§n .
This proves ùîº‚ÉóY = (ùîºY1, . . . , ùîºYn) = Œº.
Let us now verify the second part of property (1). Using Œºj = ùîºYj, from representa-
tion (6.10) we get
Cov(Yi, Yj) = ùîº[(Yi ‚àíŒºi)(Yj ‚àíŒºj)] = ùîº(
n
‚àë
k=1
Œ±ikXk)(
n
‚àë
‚Ñì=1
Œ±j‚ÑìX‚Ñì)
=
n
‚àë
k,‚Ñì=1
Œ±ikŒ±j‚ÑìùîºXkX‚Ñì.
The Xjs are independent ùí©(0, 1)-distributed, hence
ùîºXkX‚Ñì= {1
if k = ‚Ñì,
0
if k
Ã∏= ‚Ñì,
leading to
Cov(Yi, Yj) =
n
‚àë
k=1
Œ±ikŒ±jk = rij .
To see this, recall that R = AAT, hence rij = ‚àën
k=1 Œ±ikŒ±jk. This proves Cov ‚ÉóY = R, as asserted.
To verify property (2), we first treat a special case, namely that the random vector
is standard normally distributed. So suppose that ‚ÉóX is ùí©(0, In)-distributed. In this case,
property (2) asserts the following. For any b ‚àà‚Ñùn, b
Ã∏= 0, we have
‚ü®‚ÉóX, b‚ü©is distributed according to ùí©(0, |b|2) .
(6.11)
If b = (b1, . . . , bn), then
‚ü®‚ÉóX, b‚ü©=
n
‚àë
j=1
bjXj =
n
‚àë
j=1
Zj

320
‡±™
6 Normally distributed random vectors
with Zj
=
bjXj. The random variables Z1, . . . , Zn are independent and, moreover,
by Proposition 4.2.3, the Zjs are ùí©(0, b2
j )-distributed. Proposition 4.6.11 implies that
‚àën
j=1 Zj is distributed according to ùí©(0, ‚àën
j=1 b2
j ). In view of ‚àën
j=1 b2
j = |b|2, this proves
assertion (6.11).
Let us now turn to the general case. Recall that
‚ÉóY = A ‚ÉóX + Œº
and R = AAT. If a ‚àà‚Ñùn is a nonzero vector, then we take the scalar product with respect
to a on both sides of the last equation and obtain
‚ü®‚ÉóY, a‚ü©= ‚ü®A ‚ÉóX, a‚ü©+ ‚ü®Œº, a‚ü©= ‚ü®‚ÉóX, ATa‚ü©+ ‚ü®Œº, a‚ü©.
An application of statement (6.11) with b = ATa lets us conclude that ‚ü®‚ÉóX, ATa‚ü©is
ùí©(0, |ATa|2)-distributed, that is, ‚ü®‚ÉóY, a‚ü©is ùí©(‚ü®Œº, a‚ü©, |ATa|2)-distributed. Here we used
that A, hence also AT, is regular, so that a
Ã∏= 0 yields b = ATa
Ã∏= 0, and statement (6.11)
applies. Assertion (2) follows now from
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ATaÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
2 = ‚ü®ATa, ATa‚ü©= ‚ü®AATa, a‚ü©= ‚ü®Ra, a‚ü©.
Property (3) is an immediate consequence of the second. An application of property (2)
to the ith unit vector ei = (0, . . . , 0, 1
‚èü‚èü‚èü‚èü‚èü‚èü‚èü
i
, 0, . . . , 0) in ‚Ñùn leads, on the one hand, to
‚ü®‚ÉóY, ei‚ü©= Yi ,
1 ‚â§i ‚â§n ,
and, on the other hand, to
‚ü®Rei, ei‚ü©= rii
and
‚ü®Œº, ei‚ü©= Œºi ,
1 ‚â§i ‚â§n .
Thus, by property (2), for each i ‚â§n the random variable Y i is ùí©(Œºi, rii)-distributed. This
completes the proof.
Corollary 6.2.4. If ‚ÉóY is ùí©(Œº, R)-distributed, then ùîº‚ÉóY = Œº and Cov ‚ÉóY = R.
Proof. Choose any regular n √ó n matrix ÃÉA such that R =
ÃÉA ÃÉAT. The existence of such an ÃÉA
is proved in Proposition A.4.2. Set ‚ÉóZ =
ÃÉA ‚ÉóX + Œº for some standard normal vector ‚ÉóX. Then
‚ÉóY and ‚ÉóZ are both ùí©(Œº, R)-distributed, hence ‚ÉóZ
d=
‚ÉóY. Proposition 6.2.3 implies ùîº‚ÉóZ = Œº and
Cov ‚ÉóZ = R. Consequently, by Remark 6.2.2, it follows that
ùîº‚ÉóY = ùîº‚ÉóZ = Œº
and
Cov ‚ÉóY = Cov ‚ÉóZ = R ,
which completes the proof.
In view of property Corollary 6.2.4, we will use the following notation.

6.2 Expected value and covariance matrix
‡±™
321
Definition 6.2.5. If ‚ÉóY is ùí©(Œº, R)-distributed, then the parameters Œº and R are called the (multivariate)
expected value and the covariance matrix of ‚ÉóY, respectively.
Remark 6.2.6. We proved above that, for any normal vector ‚ÉóY, the coordinate mappings
Yi = ‚ü®‚ÉóY, ei‚ü©are normal as well. The converse is not valid. There are random vectors
‚ÉóY
with all random variables ‚ü®‚ÉóY, ei‚ü©normal, 1 ‚â§i ‚â§n, but ‚ÉóY is not normal.
In contrast to this remark, the following is valid.
Proposition 6.2.7. If ‚ü®‚ÉóY, a‚ü©is normal for all nonzero a ‚àà‚Ñùn, then ‚ÉóY is normal as well.
Idea of the proof. By assumption, for each a
Ã∏= 0 there are real numbers Œºa and œÉa > 0
such that ‚ü®‚ÉóY, a‚ü©is ùí©(Œºa, œÉ2
a)-distributed. In order to prove the proposition, one has to
show that there are a Œº ‚àà‚Ñùn with Œºa = ‚ü®Œº, a‚ü©and an R > 0 such that œÉ2
a = ‚ü®Ra, a‚ü©,
a ‚àà‚Ñùn. The existence of the vector Œº easily follows from
ŒºŒ±a+Œ≤b = ùîº‚ü®‚ÉóY, Œ±a + Œ≤b‚ü©= Œ±ùîº‚ü®‚ÉóY, a‚ü©+ Œ≤‚ü®‚ÉóY, b‚ü©= Œ±Œºa + Œ≤Œºb ,
using the fact that each linear mapping from ‚Ñùn to ‚Ñùis of the form a Û≥®É‚Üí‚ü®a, Œº‚ü©for a
suitable Œº ‚àà‚Ñùn.
The existence of an R > 0 with œÉ2
a = ‚ü®Ra, a‚ü©is consequence of a representation
theorem for positive quadratic forms on ‚Ñùn. To this end, one has to show that a Û≥®É‚ÜíœÉ2
a is
a positive quadratic form, which follows by using œÉ2
a = ùîº‚ü®‚ÉóY, a‚ü©2.
As we saw above (see Proposition 5.3.10), independent random variables are un-
correlated. On the other hand, Examples 5.3.12 and 5.3.15 showed the existence of un-
correlated variables that are not independent. Thus, in general, the property of being
uncorrelated is weaker than that of being independent.
One of the basic features of normal vectors is that for them uncorrelated coordinate
mappings are already independent. This somehow explains why in the common speech
these properties are synonyms.
Proposition 6.2.8. Let ‚ÉóY = (Y1, . . . , Yn) be a normally distributed vector. Then the follow-
ing are equivalent:
(1) Y1, . . . , Yn are independent.
(2) Y1, . . . , Yn are uncorrelated.
(3) The covariance matrix Cov ‚ÉóY is a diagonal matrix.
Proof. The implication (1) ‚áí(2) follows by Proposition 5.3.10. If the Yjs are uncorre-
lated, then this tells us that Cov(Yi, Yj) = 0 whenever i
Ã∏= j. Thus, Cov ‚ÉóY is a diagonal
matrix, which proves (2) ‚áí(3).
It remains to verify (3) ‚áí(1). Thus assume that
‚ÉóY is ùí©(Œº, R)-distributed, where
R > 0 is a diagonal matrix. Let r11, . . . , rnn be the entries of R at the diagonal. Define A
as diagonal matrix with r1/2
11 , . . . , r1/2
nn on the diagonal. Note that R > 0 implies rii > 0,
hence A is well defined. Of course, then AAT = R, hence ‚ÉóY has the same distribution as

322
‡±™
6 Normally distributed random vectors
the vector (Z1, . . . , Zn) with
Zi = r1/2
ii Xi + Œºi ,
1 ‚â§i ‚â§n ,
where X1, . . . , Xn are independent standard normal. Proposition 4.1.9 lets us conclude
that Z1, . . . , Zn are independent normal random variables. But since ‚ÉóY
d=
‚ÉóZ, the random
variables Y1, . . . , Yn are independent as well.1
Remark 6.2.9. Another property, being equivalent to those in Proposition 6.2.8, is as
follows. The density function of ‚ÉóY with independent coordinates equals
pŒº,R(x) =
1
(2œÄ)n/2|R|1/2 e‚àí‚àën
j=1(xj‚àíŒºj)2/2rjj ,
x = (x1, . . . , xn) .
Note that |R| = det(R) = r11 ‚ãÖ‚ãÖ‚ãÖrnn.
Finally, we investigate the case of two-dimensional normal vectors more thoroughly.
Thus assume ‚ÉóY = (Y1, Y2) is a normal vector. Then the covariance matrix R is given by
R = (
ùïçY1
Cov(Y1, Y2)
Cov(Y1, Y2)
ùïçY2
) .
Let œÉ2
1 and œÉ2
2 be the variances of Y1 and Y2, respectively, and let œÅ = œÅ(Y1, Y2) be their
correlation coefficient.2 Because of
Cov(Y1, Y2) = (ùïçY1)1/2(ùïçY2)1/2 œÅ(Y1, Y2) = œÉ1œÉ2 œÅ,
we may rewrite R as
R = (
œÉ2
1
œÅœÉ1œÉ2
œÅœÉ1œÉ2
œÉ2
2
) .
This implies det(R) = œÉ2
1œÉ2
2(1 ‚àíœÅ2). Since œÉ2
1 > 0, the matrix R is positive if and only if
|œÅ| < 1. The inverse matrix R‚àí1 can be computed by Cramer‚Äôs rule as
R‚àí1 =
1
œÉ2
1œÉ2
2(1 ‚àíœÅ2) (
œÉ2
2
‚àíœÅœÉ1œÉ2
‚àíœÅœÉ1œÉ2
œÉ2
1
) =
1
1 ‚àíœÅ2 (
1
œÉ2
1
‚àíœÅ
œÉ1œÉ2
‚àíœÅ
œÉ1œÉ2
1
œÉ2
2
) .
Consequently,
1 Indeed, use the characterization of independent random variables given in Proposition 3.6.5. The con-
dition stated there depends only on the joint distribution.
2 Recall that œÅ describes the degree and the way of the dependence between Y1 and Y2. These two random
variables are positively correlated if œÅ > 0, negatively if œÅ < 0, strongly dependent if œÅ is near 1 or ‚àí1,
and only weakly dependent in the case that œÅ is near zero.

6.2 Expected value and covariance matrix
‡±™
323
‚ü®R‚àí1x, x‚ü©=
1
1 ‚àíœÅ2 ( x2
1
œÉ2
1
‚àí2œÅ x1x2
œÉ1œÉ2
+ x2
2
œÉ2
2
),
x = (x1, x2) ‚àà‚Ñù2 .
If Œº = (Œº1, Œº2) = (ùîºY1, ùîºY2) denotes the expected value of ‚ÉóY, then for a1 < b1 and a2 < b2,
‚Ñô{a1 ‚â§Y1 ‚â§b1, a2 ‚â§Y2 ‚â§b2} =
1
2œÄ(1 ‚àíœÅ2)1/2œÉ1œÉ2
√ó
b1
‚à´
a1
b2
‚à´
a2
exp(‚àí
1
2(1 ‚àíœÅ2)[(x1 ‚àíŒº1)2
œÉ2
1
‚àí2œÅ (x1 ‚àíŒº1)(x2 ‚àíŒº2)
œÉ1œÉ2
+ (x2 ‚àíŒº2)2
œÉ2
2
])dx2 dx1 .
(6.12)
Compare this with the case of independent Y1 and Y2 or, equivalently, with the case œÅ = 0.
Here it follows that
‚Ñô{a1 ‚â§Y1 ‚â§b1, a2 ‚â§Y2 ‚â§b2}
=
1
2œÄœÉ1œÉ2
b1
‚à´
a1
b2
‚à´
a2
exp(‚àí1
2[(x1 ‚àíŒº1)2
œÉ2
1
+ (x2 ‚àíŒº2)2
œÉ2
2
])dx2 dx1 .
(6.13)
It is worthwhile to mention that in both cases (dependent and independent) the mar-
ginal distributions are the same, namely ùí©(Œº1, œÉ2
1) and ùí©(Œº2, œÉ2
2). A comparison of
eqs. (6.12) and (6.13) shows clearly the influence of the correlation coefficient to the
density (see Fig. 6.3).
Summary: An n-dimensional random vector ‚ÉóY = (Y1, . . . , Yn) is said to be normal (or the Yjs are called jointly
normal) if there are a regular n √ó n matrix A = (Œ±ij)n
i,j=1 and a vector Œº = (Œº1, . . . , Œºn) ‚àà‚Ñùn such that with
independent ùí©(0, 1)-distributed X1, . . . , Xn,
Yi =
n
‚àë
j=1
Œ±ijXj + Œºi ,
1 ‚â§i ‚â§n .
Equivalently, ‚ÉóY is normal if and only if there are a positive n √ó n matrix R = (rij)n
i,j=1 and a Œº ‚àà‚Ñùn such that
‚Ñô{ ‚ÉóY ‚ààB} =
1
(2œÄ)n/2|R|1/2 ‚à´
B
e ‚àí‚ü®R‚àí1(x‚àíŒº),(x‚àíŒº)‚ü©/2 dx .
The matrices A and R are linked by R = AAT. If ‚ÉóY is normal, the coordinates Y1, . . . , Yn are normal (univariate)
random variables (the converse is in general not true) with
ùîºYi = Œºi
and
Cov(Yi, Yj) = rij ,
1 ‚â§i, j ‚â§n .
Let ‚ÉóY be a normal vector. Then the Yjs are pairwise uncorrelated if and only if they are independent:
Y1, . . . , Yn independent
‚áî
Cov(Yi, Yj) = 0, 1 ‚â§i < j ‚â§n .

324
‡±™
6 Normally distributed random vectors
Figure 6.3: The 2-dimensional densities of a normal vector with Œº1 = Œº2 = 0, œÉ1 = 2, œÉ2 = 1, and
œÅ = 0, 0.25, 0.75, ‚àí0.5 from top left to bottom right. Thus, the coordinates of normal vectors with these
densities are either independent (œÅ = 0), weakly (œÅ = 0.25) or strongly (œÅ = 0.75) positively correlated. In
the last case (œÅ = ‚àí0.5), they are moderately negatively correlated; values in the regions {x > 0, y < 0} and
{x < 0, y > 0} become more likely.
6.3 Problems
Problem 6.1. Let ‚ÉóY = (Y1, . . . , Yn) be an arbitrary (not necessarily normal) random vec-
tor.
1.
Show that ùîº|Yj| < ‚àû, 1 ‚â§j ‚â§n, if and only if ùîº| ‚ÉóY| < ‚àû. Here | ‚ÉóY| denotes the
Euclidean distance of ‚ÉóY.
2.
Let A be an arbitrary n √ó n matrix. Prove that
ùîº(A ‚ÉóY) = A(ùîº‚ÉóY)
provided that ùîº| ‚ÉóY| < ‚àû.
3.
Show that ùîº|Yj|2 < ‚àû, 1 ‚â§j ‚â§n, if and only if ùîº| ‚ÉóY|2 < ‚àû.
4.
Suppose ùîº| ‚ÉóY|2 < ‚àû. Let Cov ‚ÉóY be the covariance matrix of
‚ÉóY. Prove that Cov ‚ÉóY is
nonnegative definite, that is,
‚ü®Cov ‚ÉóYx, x‚ü©‚â•0 ,
x ‚àà‚Ñùn .
Problem 6.2. Roll a fair die two times. Let X1 be the greater of the two rolls and X2 de-
notes the smaller one. Evaluate the expected value Œº ‚àà‚Ñù2 and the covariance matrix of
the random vector ‚ÉóX = (X1, X2).

6.3 Problems
‡±™
325
Problem 6.3. Let X1 and X2 be two independent standard normal random variables.
Define Y1 and Y2 by
Y1 = 2X1 ‚àí2X2 + 1
and
Y2 = 3X1 + X2 ‚àí2 .
1.
Find Œº ‚àà‚Ñù2 and the positive 2 √ó 2 matrix R such that
‚ÉóY = (Y1, Y2) is ùí©(Œº, R)-
distributed.
2.
Determine Cov ‚ÉóY and the correlation coefficient œÅ = œÅ(Y1, Y2). Are Y1 and Y2 posi-
tively or negatively correlated?
3.
Which distribution do Y1 + Y2 and Y1 ‚àíY2 posses?
4.
Evaluate the distribution density of ‚ÉóY.
Problem 6.4. Let ‚ÉóX = (X1, X2) be a two-dimensional standard normal vector. Compute
‚Ñô{| ‚ÉóX| ‚â§1} = ‚Ñô{X2
1 + X2
2 ‚â§1} .
Hint: Compare with the proof of Proposition 1.6.7.
Problem 6.5. Let X1, . . . , Xn+m be a sequence of independent standard normal random
variables. For an n √ó n matrix A = (Œ±ij)n
i,j=1 and an m √ó m matrix B = (Œ≤kl)m
k,l=1, define two
normal vectors ‚ÉóY and ‚ÉóZ by
Yi =
n
‚àë
j=1
Œ±ijXj
and
Zk =
m
‚àë
l=1
Œ≤klXl+n ,
with 1 ‚â§i ‚â§n and 1 ‚â§k ‚â§m. Let ( ‚ÉóY, ‚ÉóZ) be the (n + m)-dimensional vector
( ‚ÉóY, ‚ÉóZ) = (Y1, . . . , Yn, Z1, . . . , Zm) .
Why is ( ‚ÉóY, ‚ÉóZ) normal? Show that the covariance matrix Cov( ‚ÉóY, ‚ÉóZ) is given by
Cov( ‚ÉóY, ‚ÉóZ) = ( Cov ‚ÉóY
0
0
Cov ‚ÉóZ
) .
Problem 6.6. Let X1, X2, and X3 be three standard normal independent random vari-
ables. Define the random vector ‚ÉóY by
‚ÉóY := (X1 ‚àí1, X1 + X2 ‚àí1, X1 + X2 + X3 ‚àí1) .
1.
Argue why
‚ÉóY is normal. Determine its expected value, covariance matrix, and the
correlation coefficients œÅ(Yi, Yj), 1 ‚â§i < j ‚â§3.
2.
Determine the distribution density of ‚ÉóY.

326
‡±™
6 Normally distributed random vectors
Problem 6.7. The random vector ‚ÉóY = (Y1, . . . , Yn) is ùí©(Œº, R)-distributed for some Œº ‚àà‚Ñùn
and R > 0. Determine the distribution of Y1 + ‚ãÖ‚ãÖ‚ãÖ+ Yn.
Problem 6.8. Prove the following assertion: If ‚ÉóY is ùí©(0, R)-distributed, then there exist
an orthonormal basis (fj)n
j=1 in ‚Ñùn, positive numbers Œª1, . . . , Œªn and independent ùí©(0, 1)-
distributed Œæ1, . . . , Œæn such that
‚ÉóY =
n
‚àë
j=1
ŒªjŒæj fj .
(6.14)
Hint: Use the principal axis transformation for symmetric matrices and the fact that
unitary matrices map an orthonormal basis onto an orthonormal basis.
Conclude from eq. (6.14) the following: If
‚ÉóY is ùí©(0, R)-distributed, then there are
a1, . . . , an in ‚Ñùn such that ‚ü®‚ÉóY, a1‚ü©, . . . , ‚ü®‚ÉóY, an‚ü©is a sequence of independent standard nor-
mal random variables.
Problem 6.9. The n-dimensional vector ‚ÉóY is distributed according to ùí©(Œº, R). For some
regular n √ó n matrix S, define
‚ÉóZ by
‚ÉóZ := S ‚ÉóY. Is
‚ÉóZ normal? If this is so, determine the
expected value and the covariance matrix of ‚ÉóZ.
Problem 6.10. Let ‚ÉóX = (X1, X2) be standard normal. Define random variables Y1 and Y2
by
Y1 :=
1
‚àö2
(X1 + X2)
and
Y2 :=
1
‚àö2
(X1 ‚àíX2) .
Why are Y1 and Y2 also independent and standard normal?

7 Limit theorems
Probability Theory does not have the ability to predict the occurrence or nonoccurrence
of a single event in a random experiment; besides, this event occurs either with prob-
ability one or with probability zero. For example, Probability Theory does not give any
information about the next result when rolling a die, it does not predict the numbers
appearing next week on the lottery nor is it able to foresee the lifetime of a component
in a machine. Such statements are impossible within the theory. The theory is only able
to say that some events are more likely and others are less likely. For instance, when
rolling a die twice, it is more likely that the sum of both rolls will be ‚Äú7‚Äù than ‚Äú2.‚Äù Never-
theless, next when we roll the die the sum may be ‚Äú2,‚Äù not ‚Äú7.‚Äù The event ‚Äúthe sum is 2‚Äù
is not impossible, only less likely.
In contrast, Probability Theory provides us with very precise and far-reaching in-
formation about the behavior of the results when we execute ‚Äúmany‚Äù identical random
experiments. As already said, we cannot tell anything about the expected number on a
die when we roll it once, but we are able to say a lot about the frequency of the number
‚Äú6‚Äù when rolling a die many times, namely that, on average, this number will appear in
one of six cases (provided the die is fair). In this example, certain laws of Probability The-
ory, which we will present in this section, are operating. These laws are only applicable
in the case of many experiments, not in that of a single one.
Limit theorems in Probability Theory belong to the most beautiful and most im-
portant assertions within this theory. They are always the highlight of a lecture about
advanced Probability Theory. However, their proofs require a longer comprehensive
mathematical explanation, which is impossible to give here within the framework of
this book. Those who are interested in knowing more about this topic may look into one
of the more advanced books, such as [Bil12, Dur19] or [Kho07]. Although the proofs of
the limit theorems are mostly quite complicated, they are very important, and their con-
sequences influence our daily lives. Moreover, great parts of Mathematical Statistics are
based on these results. Therefore, we decided to state here the crucial assertions with-
out proving most of them. Thus, our main focus is to present the most important limit
theorems, to explain them in detail, and to give examples that show how they apply. If
possible, we give some hint as to how the results are derived, but mostly we must resign
to prove them.
7.1 Laws of large numbers
7.1.1 Chebyshev‚Äôs inequality
Our first objective is to prove Chebyshev‚Äôs inequality. To do so, we need the following
lemma.
https://doi.org/10.1515/9783111325064-007

328
‡±™
7 Limit theorems
Lemma 7.1.1. Let Y be a nonnegative random variable. Then for each Œª > 0 it follows that
‚Ñô{Y ‚â•Œª} ‚â§ùîºY
Œª .
(7.1)
Proof. Let us first treat the case that Y is discrete. Since Y ‚â•0, its possible values
y1, y2, . . . are nonnegative real numbers. Therefore, we get
ùîºY =
‚àû
‚àë
j=1
yj ‚Ñô{Y = yj} ‚â•‚àë
yj‚â•Œª
yj ‚Ñô{Y = yj}
‚â•Œª ‚àë
yj‚â•Œª
‚Ñô{Y = yj} = Œª‚Ñô{Y ‚â•Œª} .
Solving the inequality for ‚Ñô{Y ‚â•Œª} proves inequality (7.1).
The proof of estimate (7.1) for continuous Y uses similar methods. If q denotes the
distribution density of Y, by Y ‚â•0 we may suppose q(y) = 0 for y < 0. Then, as in the
discrete case, we conclude that
ùîºY =
‚àû
‚à´
0
yq(y) dy ‚â•
‚àû
‚à´
Œª
yq(y) dy ‚â•Œª
‚àû
‚à´
Œª
q(y) dy = Œª‚Ñô{Y ‚â•Œª} .
From this inequality, (7.1) follows directly.
Remark 7.1.2. Sometimes it is useful to apply inequality (7.1) in a slightly modified way.
For example, if Y ‚â•0 and Œ± > 0, then one derives
‚Ñô{Y ‚â•Œª} = ‚Ñô{Y Œ± ‚â•ŒªŒ±} ‚â§ùîºY Œ±
ŒªŒ± .
Or, if Y is real valued, then for Œª ‚àà‚Ñùwe obtain
‚Ñô{Y ‚â§Œª} = ‚Ñô{e‚àíY ‚â•e‚àíŒª} ‚â§ùîºe‚àíY
e‚àíŒª
= eŒª ùîºe‚àíY .
Now we are in a position to state and to prove Chebyshev‚Äôs inequality.
Proposition 7.1.3 (Chebyshev‚Äôs inequality). Let X be a random variable with finite second
moment. Then, if c > 0, it follows that
‚Ñô{|X ‚àíùîºX| ‚â•c} ‚â§ùïçX
c2 .
(7.2)
Proof. Setting Y := |X ‚àíùîºX|2, we have Y ‚â•0 and ùîºY = ùïçX. Now apply inequality (7.1)
to Y with Œª = c2. This leads to
‚Ñô{|X ‚àíùîºX| ‚â•c} = ‚Ñô{|X ‚àíùîºX|2 ‚â•c2} = ‚Ñô{Y ‚â•c2} ‚â§ùîºY
c2 = ùïçX
c2 ,
and estimate (7.2) is proven.

7.1 Laws of large numbers
‡±™
329
Interpretation: Inequality (7.2) quantifies the interpretation of ùïçX as a measure for
the dispersion of X. The smaller the ùïçX, the less the probability that the values of X are
far away from its expected value ùîºX.
Remark 7.1.4. Another way to formulate inequality (7.2) is as follows. If Œ∫ > 0, then
‚Ñô{|X ‚àíùîºX| ‚â•Œ∫ (ùïçX)1/2} ‚â§1
Œ∫2 .
To see this, apply inequality (7.2) with c = Œ∫ (ùïçX)1/2.
Example 7.1.5. Roll a fair die n times. If, for example, A = {6}, we are interested in the
relative frequency rn(A) of the occurrence of A. Recall that this frequency was defined
in eq. (1.1). Moreover, we claimed in this section that limn‚Üí‚àûrn(A) = ‚Ñô(A) =
1
6. Is it
possible to estimate the probability for |rn(A) ‚àí1
6| being bigger than some given c > 0?
Answer: Define the random variable X as the absolute frequency of the occurrence
of A, that is, we have X = k for some k = 0, . . . , n provided that A occurred exactly k
times. Then X is binomial distributed with parameters n and p = 1/6. To see this, define
‚Äúsuccess‚Äù as appearance of ‚Äú6.‚Äù Consequently, the relative frequency can be represented
as rn(A) = X
n . An application of eqs. (5.8) and (5.36) gives
ùîºrn(A) = 1
n ùîºX = np
n = p = 1
6
and
ùïçrn(A) = np(1 ‚àíp)
n2
=
5
36 n .
Thus, inequality (7.2) leads to
‚Ñô{
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
rn(A) ‚àí1
6
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â•c} ‚â§
5
36 c2 n .
If, for example, n = 103, and if we choose c = 1/36, then Chebyshev‚Äôs inequality yields
‚Ñô{ 5
36 < r103(A) < 7
36} ‚â•1 ‚àí9
50 = 0.82 .
For the absolute frequency, this means
‚Ñô{139 ‚â§a103(A) ‚â§194} ‚â•0.82 .
Let us interpret the result. Suppose we roll a fair die 1000 times. Then, with a probability
of at least 82 %, the frequency of ‚Äú6‚Äù will be between 139 and 194.
Let us present a second quite similar example.
Example 7.1.6. Roll a fair die n times and let Sn be the sum of the n results. Then Sn =
X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn, where X1, . . . , Xn are uniformly distributed on {1, . . . , 6} and independent.
By Example 5.2.17, we know that
ùîºSn = ùîºX1 + ‚ãÖ‚ãÖ‚ãÖ+ ùîºXn = 7n
2
and
ùïçSn = ùïçX1 + ‚ãÖ‚ãÖ‚ãÖ+ ùïçXn = 35n
12 ,

330
‡±™
7 Limit theorems
hence
ùîº(Sn
n ) = 7
2
and
ùïç(Sn
n ) = 35
12n .
An application of inequality (7.2) leads then to
‚Ñô{
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
Sn
n ‚àí7
2
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â•c} ‚â§
35
12nc2 .
For example, if n = 103 and c is chosen as c = 0.1, then
‚Ñô{3.4 < S103
103 < 3.6} ‚â•0.709 .
The interpretation of this result is as in the previous example. With a probability larger
than 70 % the sum of 1000 rolls of a fair die will be a number between 3400 and 3600.
This looks like to be a pretty rough estimate and, indeed, this is so. Sharper bounds
follow by using the central limit theorem as we will see in Example 7.2.14.
Summary: Let X be a random variable with finite second moment. Then Chebyshev‚Äôs inequality asserts that
for any c > 0,
‚Ñô{|X ‚àíùîºX| ‚â•c} ‚â§ùïçX
c2 .
This inequality clarifies once more the role of the variance ùïçX. The smaller the ùïç(X), the more likely the
random observations are concentrated around ùîºX.
7.1.2 Infinite sequences of independent random variables‚àó
Whenever one wants to describe the limit behavior of random variables or random
events, one needs a model for the infinite performance of random experiments. Oth-
erwise, we cannot investigate limits or other related quantities. This is comparable with
similar investigations in Calculus. In order to analyze limits, infinite sequences are nec-
essary, not finite ones. Thus, for the examination of limits of random variables we need
an infinite sequence X1, X2, . . . of random variables, which are, on the one hand, inde-
pendent in the sense of Definition 4.3.4 and, on the other hand, possess some given prob-
ability distributions.
Example 7.1.7. In order to describe the infinite tossing of a fair coin, we need indepen-
dent random variables X1, X2, . . . such that ‚Ñô{Xj = 0} = ‚Ñô{Xj = 1} = 1
2. Or, similarly, for
a model of rolling a die infinitely often, we need infinitely many independent random
variables all uniformly distributed on {1, . . . , 6}.

7.1 Laws of large numbers
‡±™
331
In Proposition 4.3.3, we presented the construction of independent (Xj)‚àû
j=1 dis-
tributed according to B1,1/2. This technique can be extended to more general sequences
of random variables, but it is quite complicated. Another, much smarter way is to use
so-called infinite product measures.1 Their existence follows by a deep theorem due
to A. N. Kolmogorov. As a consequence, one gets the following result, which cannot be
proven within the framework of this book. We refer to [Kho07, Chapter 5, ¬ß 2] or [Ros06,
Theorem 7.1.1] for proofs.
Proposition 7.1.8. Let ‚Ñô1, ‚Ñô2, . . . be arbitrary probability measures on (‚Ñù, ‚Ñ¨(‚Ñù)). Then
there are a probability space (Œ©, ùíú, ‚Ñô) and an infinite sequence of random variables
Xj : Œ© ‚Üí‚Ñùsuch that the following hold:
1.
The probability distribution of Xj is ‚Ñôj, j = 1, 2, . . . That is, for all j ‚â•1 and all B ‚àà‚Ñ¨(‚Ñù),
it follows that
‚Ñô{Xj ‚ààB} = Pj(B) .
2.
The random variables X1, X2, . . . are independent in the sense of Definition 4.3.4. This
says, for all n ‚â•1 and all Bj ‚àà‚Ñ¨(‚Ñù), it follows that
‚Ñô{X1 ‚ààB1, . . . , Xn ‚ààBn} = ‚Ñô{X1 ‚ààB1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn ‚ààBn}
= ‚Ñô1(B1) ‚ãÖ‚ãÖ‚ãÖ‚Ñôn(Bn) .
Of special interest is the case ‚Ñô1 = ‚Ñô2 = ‚ãÖ‚ãÖ‚ãÖ= ‚Ñô0 for a certain probability measure
‚Ñô0 on ‚Ñù. Then the previous proposition implies the following.
Corollary 7.1.9. Given an arbitrary probability measure ‚Ñô0 on ‚Ñ¨(‚Ñù), there are random
variables X1, X2, . . ., defined on some probability space (Œ©, ùíú, ‚Ñô), such that for all n ‚â•1
and all Bj ‚àà‚Ñ¨(‚Ñù),
‚Ñô{X1 ‚ààB1, . . . , Xn ‚ààBn} = ‚Ñô0(B1) ‚ãÖ‚ãÖ‚ãÖ‚Ñô0(Bn) .
Example 7.1.10. Choosing as ‚Ñô0 the uniform distribution on [0, 1], the previous corol-
lary ensures the existence of (independent) random variables X1, X2, . . . such that for all
n ‚â•1 and all 0 ‚â§aj < bj ‚â§1,
‚Ñô{a1 ‚â§X1 ‚â§b1, . . . , an ‚â§Xn ‚â§bn} =
n
‚àè
j=1
(bj ‚àíaj) .
The sequence X1, X2, . . . models the independent choosing of infinitely many numbers
uniformly distributed in [0, 1].
1 Compare with Proposition 3.6.7 for the construction of finitely many independent random variables
possessing given distributions. There we used finite product measures to obtain independent random
variables possessing given distributions ‚Ñô1, . . . , ‚Ñôn.

332
‡±™
7 Limit theorems
Remark 7.1.11. One may ask whether the kind of independence in Definition 4.3.4 suf-
fices for later purposes. Recall, we only require X1, . . . , Xn to be independent for all (fi-
nite) n ‚â•1. Maybe one would expect a condition that involves the whole infinite se-
quence, not only a finite part of it. The answer is that such a condition for the whole
sequence is a consequence of Definition 4.3.4. Namely, if B1, B2, . . . are Borel sets in ‚Ñù,
then, by the continuity of probability measures from above, it follows that
‚Ñô{X1 ‚ààB1, X2 ‚ààB2, . . .} = lim
n‚Üí‚àû‚Ñô{X1 ‚ààB1, . . . , Xn ‚ààBn}
= lim
n‚Üí‚àû‚Ñô{X1 ‚ààB1} ‚ãÖ‚ãÖ‚ãÖ‚Ñô{Xn ‚ààBn}
= lim
n‚Üí‚àû
n
‚àè
j=1
‚Ñô{Xj ‚ààBj} =
‚àû
‚àè
j=1
‚Ñô{Xj ‚ààBj} .
In particular, if aj < bj, j = 1, 2, . . ., this implies
‚Ñô{a1 ‚â§X1 ‚â§b1, a2 ‚â§X2 ‚â§b2, . . . } =
‚àû
‚àè
j=1
‚Ñô{aj ‚â§Xj ‚â§bj} .
(7.3)
Example 7.1.12. Let X1, X2, . . . be a sequence of independent EŒª-distributed random vari-
ables for some Œª > 0. Given real numbers Œ±j > 0, we ask for the probability of
‚Ñô{X1 ‚â§Œ±1, X2 ‚â§Œ±2, . . .} .
Answer: If we apply eq. (7.3) with aj = 0 and with bj = Œ±j, then we get
‚Ñô{X1 ‚â§Œ±1, X2 ‚â§Œ±2, . . .} =
‚àû
‚àè
j=1
‚Ñô{Xj ‚â§Œ±j} =
‚àû
‚àè
j=1
[1 ‚àíe‚àíŒªŒ±j].
Of special interest are sequences (aj)j‚â•1 such that the infinite product converges, that is,
for these sequences (aj)j‚â•1 we have ‚àè‚àû
j=1[1 ‚àíe‚àíŒªŒ±j] > 0. This happens if and only if
ln(
‚àû
‚àè
j=1
[1 ‚àíe‚àíŒªŒ±j]) =
‚àû
‚àë
j=1
ln[1 ‚àíe‚àíŒªŒ±j] > ‚àí‚àû.
(7.4)
Because of
lim
x‚Üí0
ln(1 ‚àíx)
‚àíx
= 1 ,
by the limit comparison test for infinite series, condition (7.4) holds if and only if
‚àû
‚àë
j=1
e‚àíŒªŒ±j < ‚àû.

7.1 Laws of large numbers
‡±™
333
If, for example, aj = c ‚ãÖln(j + 1) for some c > 0, then
‚àû
‚àë
j=1
e‚àíŒªŒ±j =
‚àû
‚àë
j=1
1
(j + 1)Œªc .
This sum is known to be finite if and only if Œªc > 1, that is, if c > 1/Œª.
Another way to formulate this observation is as follows. One has
‚Ñô{sup
j‚â•1
Xj
ln(j + 1) ‚â§c} = ‚Ñô{Xj ‚â§c ln(j + 1) , ‚àÄj ‚â•1} =
‚àû
‚àè
j=1
(1 ‚àí
1
(j + 1)Œªc ),
and this probability is positive if and only if c > 1/Œª.
Summary: An infinite sequence X1, X2, . . . of random variables is said to be independent if for each n ‚â•1 the
finite sequence X1, . . . , Xn is independent. In particular, this implies for all ai < bi that
‚Ñô{ai ‚â§Xi ‚â§bi , i = 1, 2, . . .} =
‚àû
‚àè
i=1
‚Ñô{ai ‚â§Xi ‚â§bi} .
Given probability measures ‚Ñô1, ‚Ñô2, . . . on ‚Ñù, there are independent random variables X1, X2, . . . on a proba-
bility space (Œ©, ùíú, ‚Ñô) so that ‚ÑôXj = ‚Ñôj, j = 1, 2, . . . That is, for all Borel sets B in ‚Ñ¨(‚Ñù) and j = 1, 2, . . ., we
have
‚Ñô{œâ ‚ààŒ© : Xj(œâ) ‚ààB} = ‚Ñô{Xj ‚ààB} = ‚Ñôj(B) .
For example, there are infinitely many independent random variables Xj such that
‚Ñô{Xj = 1} = ‚ãÖ‚ãÖ‚ãÖ= ‚Ñô{Xj = 6} = 1
6 .
These Xjs may serve as model for rolling a die infinitely often.
7.1.3 Borel‚ÄìCantelli lemma‚àó
The aim of this section is to present one of the most useful tools for the investigation of
the limit behavior of infinite sequences of random variables and events. Let (Œ©, ùíú, ‚Ñô)
be a probability space and let A1, A2, . . . be a sequence of events in ùíú. Then two typical
questions arise. What is the probability that there exists some n ‚àà‚Ñïsuch that all events
Am with m ‚â•n occur? The other related question asks for the probability that infinitely
many of the events An occur.
To explain why these questions are of interest, let us once more regard Example 4.1.7
of the random walk. Here Sn denotes the integer where the particle is located after n
random jumps. For example, letting An := {œâ ‚ààŒ© : Sn(œâ) > 0}, then the existence of an
n ‚àà‚Ñïsuch that Am occurs for all m ‚â•n says that the particle from a certain (random)

334
‡±™
7 Limit theorems
moment attains only positive numbers and never goes back to the negative ones. Or, if
we investigate the events Bn := {œâ ‚ààŒ© : Sn(œâ) = 0}, then the Bns occur infinitely often if
and only if the particle returns to zero infinitely often. Equivalently, there are (random)
n1 < n2 < ‚ãÖ‚ãÖ‚ãÖwith Snj(œâ) = 0.
To formulate the two previous questions more precisely, let us introduce the follow-
ing two events.
Definition 7.1.13. Let A1, A2, . . . be subsets of Œ©. Then
lim inf
n‚Üí‚àûAn :=
‚àû
‚ãÉ
n=1
‚àû
‚ãÇ
m=n
Am
and
lim sup
n‚Üí‚àûAn :=
‚àû
‚ãÇ
n=1
‚àû
‚ãÉ
m=n
Am
are called the lower and upper limit of the Ans.
Remark 7.1.14. Let us characterize when the lower and the upper limit occur.
1.
An element œâ ‚ààŒ© belongs to lim infn‚Üí‚àûAn if and only if there is an n ‚àà‚Ñïsuch
that œâ ‚àà‚ãÇ‚àû
m=n Am, that is, if it is an element of Am for m ‚â•n. In other words, the
lower limit occurs if there is an2 n ‚àà‚Ñïsuch that after n the events Am always occur.
Therefore, we say that lim infn‚Üí‚àûAn occurs if the Ans finally always (abbreviated
as f. a.) occur. Thus,
‚Ñô{œâ ‚ààŒ© : ‚àÉn such that œâ ‚ààAm , m ‚â•n} = ‚Ñô(lim inf
n‚Üí‚àûAn).
2.
An element œâ ‚ààŒ© belongs to lim supn‚Üí‚àûAn if and only if for each n ‚àà‚Ñïthere is
an m ‚â•n such that œâ ‚ààAm. But this is nothing else as saying that the number of
Ans with œâ ‚ààAn is infinite. Therefore, the upper limit consists of those elements for
which we have infinitely often (abbreviated as i. o.) œâ ‚ààAn. Note that also these
events may be different for different œâ. Thus,
‚Ñô{œâ ‚ààŒ© : œâ ‚ààAn for infinitely many n} = ‚Ñô(lim sup
n‚Üí‚àûAn).
Example 7.1.15. Suppose a fair coin is labeled on one side with ‚Äú0‚Äù and on the other side
with ‚Äú1.‚Äù We toss it infinitely often. Let An occur if the nth toss is ‚Äú1.‚Äù Then lim infn‚Üí‚àûAn
occurs if after a certain number of tosses ‚Äú1‚Äù always shows up. On the other hand,
lim supn‚Üí‚àûAn occurs if and only if the number ‚Äú1‚Äù appears infinitely often. The sub-
sequent results imply that the probability of the lower limit of these Ans equals zero,
while with probability one the they will occur infinitely often.
Let us formulate and prove some easy properties of the lower and upper limit.
2 Note that this n is random, that is, it may depend on the chosen œâ ‚ààŒ©.

7.1 Laws of large numbers
‡±™
335
Proposition 7.1.16. If A1, A2, . . . are subsets of Œ©, then
(1)
lim inf
n‚Üí‚àûAn ‚äÜlim sup
n‚Üí‚àûAn ,
(2)
(lim sup
n‚Üí‚àûAn)
c
= lim inf
n‚Üí‚àûAc
n
and
(lim inf
n‚Üí‚àûAn)
c
= lim sup
n‚Üí‚àûAc
n .
Proof. We prove these properties in the interpretation of the lower and upper limit
given in Remark 7.1.14.
Suppose that œâ ‚ààlim infn‚Üí‚àûAn. Then for some n ‚â•1 it follows that œâ ‚ààAm,
m ‚â•n. Of course, then the number of events with œâ ‚ààAn is infinite, which implies
œâ ‚ààlim supn‚Üí‚àûAn. This proves (1).
Observe that we have œâ ‚àâlim supn‚Üí‚àûAn if and only if œâ ‚ààAn for only finitely many
n ‚àà‚Ñï. Equivalently, there is an n ‚â•1 such that whenever m ‚â•n, then œâ ‚àâAm, or, that
œâ ‚ààAc
m. In other words, this happens if and only if œâ ‚ààlim infn‚Üí‚àûAc
n. This proves the
left-hand identity in (2). The right-hand one follows by the same arguments. One may
also prove this by applying the left-hand identity with Ac
n.
Before we can formulate the main result in this section, we have to define when an
infinite sequence of events is independent.
Definition 7.1.17. A sequence of events A1, A2, . . . in ùíúis said to be independent provided that for all
n ‚â•1 the events A1, . . . , An are independent in the sense of Definition 2.2.12. An equivalent formulation is
a follows: given m ‚â•1 and indices i1 < i2 < ‚ãÖ‚ãÖ‚ãÖ< im, then this implies
‚Ñô(Ai1 ‚à©‚ãÖ‚ãÖ‚ãÖ‚à©Aim) = ‚Ñô(Ai1) ‚ãÖ‚ãÖ‚ãÖ‚Ñô(Aim) .
Remark 7.1.18. Using the method for the proof of eq. (7.3), one may deduce the following
‚Äúinfinite‚Äù version of independence. For independent A1, A2, . . . follows that
‚Ñô(
‚àû
‚ãÇ
n=1
An) =
‚àû
‚àè
n=1
‚Ñô(An) .
Remark 7.1.19. According to Proposition 3.6.9, the independence of random variables
and events are linked as follows. The random variables X1, X2, . . . are independent in
the sense of Definition 4.3.4 if and only if for all Borel sets B1, B2, . . . in ‚Ñùthe preimages
X‚àí1
1 (B1), X‚àí1
2 (B2), . . . are independent events as introduced in Definition 7.1.17.
Now we are in the position to state and prove the main result of this section.
Proposition 7.1.20 (Borel‚ÄìCantelli lemma). Let (Œ©, ùíú, ‚Ñô) be a probability space and let
An ‚ààùíú, n = 1, 2, . . .
1.
If ‚àë‚àû
n=1 ‚Ñô(An) < ‚àû, then
‚Ñô(lim sup
n‚Üí‚àûAn) = 0 .
(7.5)

336
‡±™
7 Limit theorems
2.
For independent A1, A2, . . ., the following is valid. If ‚àë‚àû
n=1 ‚Ñô(An) = ‚àû, then
‚Ñô(lim sup
n‚Üí‚àûAn) = 1 .
Proof. We start with proving the first assertion. Thus, take arbitrary subsets An ‚ààùíú
satisfying ‚àë‚àû
n=1 ‚Ñô(An) < ‚àû. Write
lim sup
n‚Üí‚àûAn =
‚àû
‚ãÇ
n=1
Bn
with Bn := ‚ãÉ‚àû
m=n Am. Since B1 ‚äáB2 ‚äá‚ãÖ‚ãÖ‚ãÖ, property (7) in Proposition 1.2.1 applies and,
together with (5) in the same proposition, leads to
‚Ñô(lim sup
n‚Üí‚àûAn) = lim
n‚Üí‚àû‚Ñô(Bn) ‚â§lim inf
n‚Üí‚àû
‚àû
‚àë
m=n
‚Ñô(Am) .
(7.6)
If Œ±1, Œ±2, . . . are nonnegative numbers with ‚àë‚àû
n=1 Œ±n < ‚àû, then it is known that
‚àë‚àû
m=n Œ±m ‚Üí0 as n ‚Üí‚àû. Applying this observation to Œ±n = ‚Ñô(An), assertion (7.5) is
a direct consequence of estimate (7.6). Thus, the first part is proven.
To prove the second assertion, we investigate the probability of the complementary
event. Here we have
(lim sup
n‚Üí‚àûAn)
c
=
‚àû
‚ãÉ
n=1
‚àû
‚ãÇ
m=n
Ac
m .
An application of (5) in Proposition 1.2.1 implies
‚Ñô((lim sup
n‚Üí‚àûAn)
c
) ‚â§
‚àû
‚àë
n=1
‚Ñô(
‚àû
‚ãÇ
m=n
Ac
m) .
(7.7)
Fix n ‚àà‚Ñïand for k ‚â•n set Bk := ‚ãÇk
m=n Ac
m. Then Bn ‚äáBn+1 ‚äá‚ãÖ‚ãÖ‚ãÖ, hence by property (7)
in Proposition 1.2.1 it follows that
‚Ñô(
‚àû
‚ãÇ
m=n
Ac
m) = ‚Ñô(
‚àû
‚ãÇ
k=n
Bk) = lim
k‚Üí‚àû‚Ñô(Bk) = lim
k‚Üí‚àû
k
‚àè
m=n
(1 ‚àí‚Ñô(Am)) .
Here in the last step we used that, due to Proposition 2.2.15, the events Ac
1, Ac
2, . . . are
independent as well. Next we apply the elementary inequality
1 ‚àíx ‚â§e‚àíx ,
0 ‚â§x ‚â§1 ,
for x = ‚Ñô(Am) and, because of ‚àë‚àû
m=n ‚Ñô(Am) = ‚àû, we arrive at

7.1 Laws of large numbers
‡±™
337
‚Ñô(
‚àû
‚ãÇ
m=n
Ac
m) ‚â§lim sup
k‚Üí‚àû
exp(‚àí
k
‚àë
m=n
‚Ñô(Am)) = 0 .
Plugging this into estimate (7.7) finally yields
‚Ñô((lim sup
n‚Üí‚àûAn)
c
) = 0 ,
hence ‚Ñô(lim sup
n‚Üí‚àûAn) = 1,
as asserted.
Remark 7.1.21. The second assertion in Proposition 7.1.20 remains valid under the
weaker condition of pairwise independence. But then the proof becomes more compli-
cated (see Examples 6.4 and 6.5 in [Bil12] or Lemma 11.1 in [Bau96]).
Corollary 7.1.22. Let An ‚ààùíúbe independent events. Then the following are equivalent:
‚Ñô(lim sup
n‚Üí‚àûAn) = 0
‚áî
‚àû
‚àë
n=1
‚Ñô(An) < ‚àû,
‚Ñô(lim sup
n‚Üí‚àûAn) = 1
‚áî
‚àû
‚àë
n=1
‚Ñô(An) = ‚àû.
Example 7.1.23. Suppose we play a series of independent games where the success
probability in the nth game is pn for some given p1, p2, . . . How likely is it to win in-
finitely many of the games?
Answer: Let the event An occur if one wins game n. By the choice of the pns, it follows
that ‚Ñô(An) = pn. Thus, the Borel‚ÄìCantelli lemma asserts that one wins with probability
1 infinitely many games if and only if ‚àë‚àû
n=1 pn = ‚àû. On the other hand, if ‚àë‚àû
n=1 pn < ‚àû,
then with probability 1 one loses all games after a finite random number of games. So,
for example, if pn = 1/n, then with probability 1 one wins infinitely often although the
success probability becomes smaller and smaller. On the contrary, in the case pn = 1/n2
there will be an N ‚àà‚Ñïso that one loses all games after the Nth one.
Example 7.1.24. Let (Un)n‚â•1 be a sequence of independent random variables, uniformly
distributed on [0, 1]. Given positive real numbers (Œ±n)n‚â•1, we define events An by setting
An := {Un ‚â§Œ±n}. Since the Uns are independent, so are the events An, and Corollary 7.1.22
applies. Because of ‚Ñô(An) = Œ±n, this leads to
‚Ñô{Un ‚â§Œ±n i. o.} = {0
if ‚àë‚àû
n=1 Œ±n < ‚àû,
1
if ‚àë‚àû
n=1 Œ±n = ‚àû,
or, equivalently, to
‚Ñô{Un > Œ±n f. a.} = {0
if ‚àë‚àû
n=1 Œ±n = ‚àû,
1
if ‚àë‚àû
n=1 Œ±n < ‚àû.

338
‡±™
7 Limit theorems
For example, we have
‚Ñô{Un ‚â§1/n i. o.} = 1
and
‚Ñô{Un ‚â§1/n2 i. o.} = 0 .
Example 7.1.25. Let (Xn)n‚â•1 be a sequence of independent ùí©(0, 1)-distributed random
variables and let cn > 0. What probability does the event to observe {|Xn| ‚â•cn} infinitely
often possess?
Answer: It holds that
‚àû
‚àë
n=1
‚Ñô{|Xn| ‚â•cn} =
2
‚àö2œÄ
‚àû
‚àë
n=1
‚àû
‚à´
cn
e‚àíx2/2 dx =
2
‚àö2œÄ
‚àû
‚àë
n=1
œÜ(cn) ,
where
œÜ(t) :=
‚àû
‚à´
t
e‚àíx2/2 dx ,
t ‚àà‚Ñù.
Setting œà(t) := t‚àí1e‚àít2/2, t > 0, one obtains
œÜ‚Ä≤(t) = ‚àíe‚àít2/2
and
œà‚Ä≤(t) = ‚àí(1 + 1
t2 )e‚àít2/2 ,
hence l‚ÄôH√¥pital‚Äôs rule implies
lim
t‚Üí‚àû
œÜ‚Ä≤(t)
œà‚Ä≤(t) = 1 ,
thus lim
t‚Üí‚àû
œÜ(t)
œà(t) = 1 .
The limit comparison test for infinite series tells us that ‚àë‚àû
n=1 œÜ(cn) < ‚àûif and only if
‚àë‚àû
n=1 œà(cn) < ‚àû. Thus, by the definition of œà, the following are equivalent:
‚àû
‚àë
n=1
‚Ñô{|Xn| ‚â•cn} < ‚àû
‚áê‚áí
‚àû
‚àë
n=1
e‚àíc2
n/2
cn
< ‚àû.
In other words, we have
‚Ñô{|Xn| ‚â•cn i. o.} =
{
{
{
{
{
0
if ‚àë‚àû
n=1
e‚àíc2n/2
cn
< ‚àû,
1
if ‚àë‚àû
n=1
e‚àíc2n/2
cn
= ‚àû.
For example, if cn = c‚àöln n for some c > 0, then
‚àû
‚àë
n=1
e‚àíc2
n/2
cn
= 1
c
‚àû
‚àë
n=1
1
nc2/2 ‚àöln n
< ‚àû

7.1 Laws of large numbers
‡±™
339
if and only if c > ‚àö2. In particular, this yields the following interesting fact:
‚Ñô{|Xn| ‚â•‚àö2 ln n i. o.} = 1 ,
while for each c > 2,
‚Ñô{|Xn| ‚â•‚àöc ln n i. o.} = 0 .
From this, we derive
‚Ñô{œâ ‚ààŒ© : lim sup
n‚Üí‚àû
|Xn(œâ)|
‚àöln n
= ‚àö2} = 1 .
Example 7.1.26. In a lottery, 6 of 49 numbers are randomly chosen. Find the probability
to have infinitely often the six chosen numbers on your lottery ticket.
Answer: Let An be the event to have in the nth drawing the six chosen numbers on
the ticket. We saw (see Example 1.4.3) that
‚Ñô(An) =
1
(49
6 ) := Œ¥ > 0 .
Consequently, it follows that ‚àë‚àû
n=1 ‚Ñô(An) = ‚àûand, since the Ans are independent, Propo-
sition 7.1.20 implies
‚Ñô{The Ans occur i. o.} = 1 .
Therefore, the event to win infinitely often has probability 1. One does only not play long
enough!
Remark 7.1.27. Corollary 7.1.22 shows in particular that for independent Ans either
‚Ñô(lim sup
n‚Üí‚àûAn) = 0
or
‚Ñô(lim sup
n‚Üí‚àûAn) = 1 .
Because of Proposition 7.1.16, the same is valid for the lower limit. Here the so-called
0‚Äì1 laws operate, which, roughly speaking, assert the following. Whenever the occur-
rence or nonoccurrence of an event is independent of the first finitely many results,
then such events occur either with probability 0 or 1. For example, the occurrence or
nonoccurrence of the lower or upper limit is completely independent of what had hap-
pened during the first n results, n ‚â•1.
Summary: Let A1, A2, . . . be a sequence of events in a probability space (Œ©, ùíú, ‚Ñô). A basic question in Prob-
ability Theory is how likely the occurrence of infinitely many of the events An is. This question is answered by
the Borel‚ÄìCantelli Lemma. It asserts
‚àû
‚àë
n=1
‚Ñô(An) < ‚àû
‚áí
‚Ñô{The Ans occur i. o.} = 0 .

340
‡±™
7 Limit theorems
Conversely, if the Ans are independent events, then it follows that
‚àû
‚àë
n=1
‚Ñô(An) = ‚àû
‚áí
‚Ñô{The Ans occur i. o.} = 1 .
In particular, if the events A1, A2, . . . are independent, then the Ans occur infinitely often either with probability
0 or 1.
7.1.4 Weak law of large numbers
Given random variables X1, X2, . . ., let
Sn := X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn
(7.8)
be the sum of the first n values. One of the most important questions in Probability
Theory is that about the behavior of Sn as n ‚Üí‚àû. Suppose we play a series of games
and Xj denotes the loss or the gain in game j ‚â•1. Then Sn is nothing else than the total
loss or gain after n games. Also recall the random walk presented in Example 4.1.7. Set
Xj = ‚àí1 if in step j the particle jumps to the left, and Xj = 1 otherwise. Then Sn is the
point in ‚Ñ§where the particle is located after n jumps.
Let us come back to the general case. We are given arbitrary independent and iden-
tically distributed random variables X1, X2, . . . Recall that ‚Äúidentically distributed‚Äù says
that they all possess the same probability distribution. Set Sn = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn. The first re-
sult gives some information about the behavior of the arithmetic mean Sn/n as n ‚Üí‚àû.
Proposition 7.1.28 (Weak law of large numbers). Let X1, X2, . . . be independent identically
distributed random variables with (common) expected value Œº ‚àà‚Ñù. If Œµ > 0, then it follows
that
lim
n‚Üí‚àû‚Ñô{
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
Sn
n ‚àíŒº
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â•Œµ} = 0 .
Proof. We prove the result only with an additional condition, namely that X1 and hence
all Xj possess a finite second moment. The result remains true without this condition,
but then its proof becomes significantly more complicated.
From (3) in Proposition 5.1.38, we derive
ùîº(Sn
n ) = ùîºSn
n
= ùîº(X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn)
n
= ùîºX1 + ‚ãÖ‚ãÖ‚ãÖ+ ùîºXn
n
= nŒº
n = Œº .
Furthermore, by the independence of the Xjs, property (iv) in Proposition 5.2.15 also
gives
ùïç(Sn
n ) = ùïçSn
n2
= ùïçX1 + ‚ãÖ‚ãÖ‚ãÖ+ ùïçXn
n2
= ùïçX1
n
.

7.1 Laws of large numbers
‡±™
341
Consequently, inequality (7.2) implies
‚Ñô{
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
Sn
n ‚àíŒº
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â•Œµ} ‚â§ùïç(Sn/n)
Œµ2
= ùïçX1
nŒµ2 ,
and the desired assertion follows from
lim sup
n‚Üí‚àû‚Ñô{
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
Sn
n ‚àíŒº
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â•Œµ} ‚â§lim
n‚Üí‚àû
ùïçX1
nŒµ2 = 0 .
Remark 7.1.29. The type of convergence appearing in Proposition 7.1.28 is usually called
convergence in probability. More precisely, given random variables Y1, Y2, . . ., they
converge in probability to some random variable Y provided that for each Œµ > 0,
lim
n‚Üí‚àû‚Ñô{|Yn ‚àíY| ‚â•Œµ} = 0 .
Hence, in this language the weak law of large numbers asserts that Sn/n converges in
probability to a random variable Y, which is the constant Œº.
Interpretation of Proposition 7.1.28. Fix Œµ > 0 and define events An, n ‚â•1, by
An := {œâ ‚ààŒ© :
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
Sn(œâ)
n
‚àíŒº
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
< Œµ} .
Then Proposition 7.1.28 implies limn‚Üí‚àû‚Ñô(An) = 1. Hence, given Œ¥ > 0, there is an
n0 = n0(Œµ, Œ¥) such that ‚Ñô(An) ‚â•1 ‚àíŒ¥ whenever n ‚â•n0. In other words, if n is suffi-
ciently large, then with high probability (recall, Œº is the expected value of the Xjs)
Œº ‚àíŒµ ‚â§1
n
n
‚àë
j=1
Xj ‚â§Œº + Œµ .
This confirms once more the interpretation of the expected value as (approximate) arith-
metic mean of the observed values, provided that we execute the same experiment ar-
bitrarily often and the results do not depend on each other.
7.1.5 Strong law of large numbers
Proposition 7.1.28 does not imply Sn/n ‚ÜíŒº in the usual sense. It only asserts the conver-
gence of Sn/n in probability, which, in general, does not imply pointwise convergence.
The following theorem due to A. N. Kolmogorov shows that, nevertheless, a strong type
of convergence takes place. The proof of this result is much more complicated than that
of Proposition 7.1.28. Therefore, we cannot present it in the scope of this book, and we
refer to [Dur19, Section 2.4] or [Ros06, Chapter 5] for a proof.

342
‡±™
7 Limit theorems
Proposition 7.1.30 (Strong law of large numbers). Let X1, X2, . . . be a sequence of indepen-
dent identically distributed random variables with expected value Œº = ùîºX1. If Sn is defined
by eq. (7.8), then
‚Ñô{œâ ‚ààŒ© : lim
n‚Üí‚àû
Sn(œâ)
n
= Œº} = 1 .
Remark 7.1.31. Given random variables Y1, Y2, . . . and Y, one says that the Yns converge
to Y almost surely, if
‚Ñô{ lim
n‚Üí‚àûYn = Y} = ‚Ñô{œâ ‚ààŒ© : lim
n‚Üí‚àûYn(œâ) = Y(œâ)} = 1 .
Thus, Proposition 7.1.30 asserts that Sn/n converges almost surely to a random vari-
able Y, which is a constant Œº.
Remark 7.1.32. Proposition 7.1.30 allows the following interpretation. There exists a
subset Œ©0 in the sample space Œ© with ‚Ñô(Œ©0) = 1 such that for all œâ ‚ààŒ©0 and all Œµ > 0,
there is an n0 = n0(Œµ, œâ) with
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
Sn(œâ)
n
‚àíŒº
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
< Œµ
whenever n ‚â•n0.
In other words, with probability one the following happens: given Œµ > 0, there is
a certain n0 depending on œâ, hence random, such that for n ‚â•n0 the arithmetic mean
Sn/n is in an Œµ-neighborhood of Œº and never leaves it again.
Let us emphasize once more that Sn/n is random, hence Sn/n may attain different
values for a different series of experiments. Nevertheless, starting from a certain point,
which may be different for different experiments, the arithmetic mean of the first n
results will be in (Œº ‚àíŒµ, Œº + Œµ).
When we introduced probability measures in Section 1.1.3, we claimed that the num-
ber ‚Ñô(A) may be regarded as the limit of the relative frequencies of the occurrence of
the event A. As the first consequence of Proposition 7.1.30, we show that this is indeed
true.
Proposition 7.1.33. Suppose a random experiment is described by a probability space
(Œ©, ùíú, ‚Ñô). Execute this experiment arbitrarily often. Given an event A ‚ààùíú, let rn(A) be
the relative frequency of A in n trials as defined in eq. (1.1). Then almost surely
lim
n‚Üí‚àûrn(A) = ‚Ñô(A) .
Proof. Define random variables X1, X2, . . . as follows. Set Xj = 1 if A occurs in trial j,
while Xj = 0 otherwise. Since the experiments are executed independently of each other,

7.1 Laws of large numbers
‡±™
343
the Xjs are independent as well. Moreover, we execute every time exactly the same ex-
periment, hence the Xjs are also identically distributed.
By the definition of the Xjs,
Sn
n = rn(A) .
Thus, it remains to evaluate Œº = ùîºXj. To this end observe that the Xjs are B1,p-distributed
with success probability p = ‚Ñô(A). Recall that Xj = 1 if and only if A occurs in experi-
ment j, and since the experiment is described by (Œ©, ùíú, ‚Ñô), the probability for Xj being 1
is ‚Ñô(A). Consequently, ùîºXj = ‚Ñô(A).
Proposition 7.1.30 now implies that almost surely
lim
n‚Üí‚àûrn(A) = lim
n‚Üí‚àû
Sn
n = ùîºX1 = ‚Ñô(A) .
This completes the proof.
What happens in the case when the Xjs do not possess an expected value? Does
then Sn/n converge nevertheless? If this is so, could we take this limit as a ‚Äúgeneralized‚Äù
expected value? The next proposition shows that such an approach does not work. For
a proof, see [Dur19, Theorem 2.4.5]; see also [Eri73] for further reading.
Proposition 7.1.34. Let X1, X2, . . . be independent and identically distributed with
ùîº|X1| = ‚àû. Then it follows that
‚Ñô{œâ ‚ààŒ© : Sn(œâ)
n
diverges} = 1 .
For example, if we take an independent sequence (Xj)j‚â•1 of Cauchy distributed ran-
dom variables, then their arithmetic means Sn/n will diverge almost surely.
Remark 7.1.35. Why does one need a weak law of large numbers when there ex-
ists a strong one? This question is justified and, in fact, in the situation described in
this book the weak law is a consequence of the strong one, thus, it is not necessarily
needed.
The situation is different if one investigates independent, but not necessarily iden-
tically distributed, random variables. Then there are sequences X1, X2, . . . satisfying the
weak law but not the strong one.3
Let us state two applications of Proposition 7.1.30, one taken from Numerical Math-
ematics, the other from Number Theory.
3 In the case of nonidentically distributed Xjs, one investigates if 1
n ‚àën
j=1(Xj ‚àíùîºXj) converges to zero
either in probability (weak law) or almost surely (strong law).

344
‡±™
7 Limit theorems
Example 7.1.36 (Monte Carlo method for integrals). Suppose we are given a quite ‚Äúcom-
plicated‚Äù function f : [0, 1]n ‚Üí‚Ñù. The task is to find the numerical value of
‚à´
[0,1]n
f (x) dx =
1
‚à´
0
. . .
1
‚à´
0
f (x1, . . . , xn) dxn ‚ãÖ‚ãÖ‚ãÖdx1 .
For large n, this can be a highly nontrivial problem. One way to overcome this difficulty
is to use a probabilistic approach that is based on the strong law of large numbers.
To this end, choose an independent sequence
‚ÉóU1, ‚ÉóU2, . . . of random vectors uni-
formly distributed on [0, 1]n. For example, such a sequence can be constructed as fol-
lows. Take independent U1, U2, . . . uniformly distributed on4 [0, 1] and build random
vectors by
‚ÉóU1 = (U1, . . . , Un),
‚ÉóU2 = (Un+1, . . . , U2n), and so on.
Proposition 7.1.37. As above, let ‚ÉóU1, ‚ÉóU2, . . . be independent random vectors uniformly dis-
tributed on [0, 1]n. Given an integrable function f : [0, 1]n ‚Üí‚Ñù, with probability one,
lim
N‚Üí‚àû
1
N
N
‚àë
j=1
f ( ‚ÉóUj) = ‚à´
[0,1]n
f (x) dx .
Proof. Set Xj := f ( ‚ÉóUj), j = 1, 2, . . . By construction, the Xjs are independent and iden-
tically distributed random variables. Proposition 3.6.20 implies (compare also with
Example 3.6.23) that the distribution densities of the random vectors
‚ÉóUj are given
by
p(x) = {1
if x ‚àà[0, 1]n,
0
if x ‚àâ[0, 1]n.
As already mentioned in Remark 5.3.5, formula (5.42), stated for a function of two vari-
ables, also holds for functions of n variables, n ‚â•1 arbitrary. This implies
ùîºX1 = ùîºf ( ‚ÉóU1) = ‚à´
‚Ñùn
f (x) p(x) dx = ‚à´
[0,1]n
f (x) dx .
Thus, Proposition 7.1.30 applies and leads to
‚Ñô{ lim
N‚Üí‚àû
1
N
N
‚àë
j=1
f ( ‚ÉóUj) = ‚à´
[0,1]n
f (x) dx} = ‚Ñô{ lim
N‚Üí‚àû
1
N
N
‚àë
j=1
Xj = ùîºX1} = 1,
as asserted.
4 Use the methods developed in Section 4.4 to construct such Ujs.

7.1 Laws of large numbers
‡±™
345
Remark 7.1.38. The numerical application of the preceding proposition is as follows.
Choose independent numbers u(j)
i , 1 ‚â§i ‚â§n, 1 ‚â§j ‚â§N, uniformly distributed on [0, 1]
and set
RN(f ) := 1
N
N
‚àë
j=1
f (u(j)
1 , . . . , u(j)
n ) .
Proposition 7.1.37 asserts that RN(f ) converges almost surely to ‚à´[0,1]n f (x) dx. Thus,
if N ‚â•1 is large, then RN(f ) may be taken as approximate value for ‚à´[0,1]n f (x) dx.
If we apply Proposition 7.1.37 to the indicator function of a Borel set B ‚äÜ[0, 1]n, that
is, we choose f = 1B with 1B as in Definition 3.6.16, then with probability 1 it follows that
voln(B) = ‚à´
[0,1]n
1B(x) dx = lim
N‚Üí‚àû
1
N
N
‚àë
j=1
1B( ‚ÉóUj) = lim
N‚Üí‚àû
|{j ‚â§N :
‚ÉóUj ‚ààB}|
N
.
This provides us with a method to determine the volume voln(B), even for quite ‚Äúcom-
plicated‚Äù Borel sets B ‚äÜ‚Ñùn.
Example 7.1.39. A way to approximate œÄ/4 by the described method is as follows: draw
a quadrant Q of a circle with radius 1 inside a square S of side length 1. Next choose
independently points u1, u2, . . . , un uniformly distributed in [0, 1]2. Then, as n ‚Üí‚àû,
|{j ‚â§n : uj ‚ààQ}|
|{j ‚â§n; uj ‚ààS}|
converges to vol2(Q)/vol2(S) = œÄ/4. See Fig. 7.1.
Figure 7.1: As n ‚Üí‚àû, he proportion of the randomly chosen points inside the quadrant Q converges to
œÄ/4. In the above figure, there are 26 of the 32 points inside Q. This gives 0.8125 as an approximation of
œÄ/4 ‚âà0.7854.
Example 7.1.40 (Normal numbers). As we saw in Section 4.3.1, each x ‚àà[0, 1) admits a
representation as binary fraction x = 0.x1x2 . . . with xj ‚àà{0, 1}. Take some fixed x ‚àà[0, 1)

346
‡±™
7 Limit theorems
with binary representation x = 0.x1x2 . . . Then one may ask whether in the binary rep-
resentation of x one of the numbers 0 or 1 occurs more frequently than the other. Or do
both numbers possess the same frequency, at least on average?
To investigate this question, for n ‚àà‚Ñïset
a0
n(x) := ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®{k ‚â§n : xk = 0}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
and
a1
n(x) := ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®{k ‚â§n : xk = 1}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®,
x = 0.x1x2 . . .
Thus, a0
n(x) is the frequency of the number 0 among the first n positions in the represen-
tation of x.
Definition 7.1.41. An x ‚àà[0, 1) is said to be normal (with respect to base 2) if
lim
n‚Üí‚àû
a0
n(x)
n
= lim
n‚Üí‚àû
a1
n(x)
n
= 1
2 .
In other words, a number x ‚àà[0, 1) is normal with respect to base 2 if, on average, in its
binary representation the frequency of 0, and hence also of 1, equals 1/2. Are there many
normal numbers as, for example, x = 0.0101010 . . ., or are there maybe only a few? The
next proposition gives the answer.
Proposition 7.1.42. Let ‚Ñôbe the uniform distribution on [0, 1]. Then there is a subset
M ‚äÜ[0, 1) with ‚Ñô(M) = 1 such that all x ‚ààM are normal with respect to base 2.
Proof. Define random variables Xk : [0, 1) ‚Üí‚Ñù, k = 0, 1, . . . , by Xk(x) := xk whenever
x = 0.x1x2 . . . Proposition 4.3.3 tells us that the Xks are independent with ‚Ñô{Xk = 0} = 1/2
and ‚Ñô{Xk = 1} = 1/2. Recall that the underlying probability measure ‚Ñôon [0, 1] is the
uniform distribution. By the definition of the Xks it follows that
Sn(x) := X1(x) + ‚ãÖ‚ãÖ‚ãÖ+ Xn(x) = ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®{k ‚â§n : Xk(x) = 1}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®= a1
n(x) .
Since ùîºX1 = 1/2, Proposition 7.1.30 implies the existence of a subset M ‚äÜ[0, 1) with
‚Ñô(M) = 1 such that for x ‚ààM it follows that
lim
n‚Üí‚àû
a1
n(x)
n
= lim
n‚Üí‚àû
Sn(x)
n
= ùîºX1 = 1
2 .
Since a0
n(x) = n ‚àía1
n(x), this completes the proof.
Remark 7.1.43. The previous considerations do not depend on the fact that the base
of the representation was 2. It extends easily to representations with respect to any
base b ‚â•2. Here, the definition of normal numbers has to be extended slightly. Fix
b ‚â•2. Each x ‚àà[0, 1) admits the representation x = 0.x1x2 . . . where xj ‚àà{0, . . . , b ‚àí1}
provided that x = ‚àë‚àû
k=1
xk
bk . To make this representation unique, we do not allow rep-
resentations x = 0.x1x2 . . . where for some k0 ‚àà‚Ñïwe have xk = b ‚àí1 whenever
k ‚â•k0.

7.2 Central limit theorem
‡±™
347
Then a number x is said to be normal with respect to the base b ‚â•2 if for all
‚Ñì= 0, . . . , b ‚àí1,
lim
n‚Üí‚àû
|{j ‚â§n : xj = ‚Ñì}|
n
= 1
b ,
x = 0.x1x2 . . .
Similar methods as used in the proof of Proposition 7.1.42 show that there is a set
Mb ‚äÜ[0, 1] with ‚Ñô(Mb) = 1 such that all x ‚ààMb are normal with respect to base b.
Letting M = ‚ãÇ‚àû
b=2 Mb, then property (5) (Boole‚Äôs inequality) in Proposition 1.2.1 easily
gives ‚Ñô(M) = 1. Numbers x ‚ààM are completely normal, which says that they are
normal for any base b ‚â•2. Again we see that with respect to the uniform distribution
on [0, 1] almost all numbers are completely normal.
Summary: Laws of large numbers are among the most important assertions in Probability Theory.5 Verbally
said, these laws assert the following: if x1, x2, . . . are the random results of identical experiments, obtained
independently of each other, then the sequence of arithmetic means (x1 + ‚ãÖ‚ãÖ‚ãÖ+ xn)/n converges in a weak,
as well as in a strong sense, to the expected value of the xjs, provided the expected value exists.
In particular, these laws justify regarding the probability ‚Ñô(A) of an event A as the limit of the relative
frequencies rn(A) of its occurrence, as we claimed in Section 1.1.3. We emphasize once more, the arithmetic
mean, as well as the relative frequency, is random, thus may be different in different trials, but the expected
value and ‚Ñô(A) are both fixed nonrandom real numbers.
7.2 Central limit theorem
Why does the normal distribution play such an important role in Probability Theory and
why are so many observed random phenomena normally distributed? The reason for
this is the central limit theorem, which we are going to present in this section.
Consider a sequence of independent and identically distributed random variables
(Xj)j‚â•1 with finite second moment. As in eq. (7.8), let Sn be the sum of X1, . . . , Xn. For exam-
ple, if Xj is the loss or gain in the jth game, then Sn is the total loss or gain after n games.
Which probability distribution does Sn possess? Theoretically, this can be evaluated by
the convolution formulas stated in Section 4.5. But practically, this is mostly impossible;
imagine, we want to determine the distribution of the sum of 100 rolls with a fair die.
Therefore, one is very interested in asymptotic statements about the distribution of Sn.
To get a clue about possible asymptotic distributions of Sn, take independent
B1,p-distributed Xjs. In this case, the distribution of Sn is known to be Bn,p.
For example, if p = 0.4 and n = 30, then ‚Ñô{Sn = k} = Bn,p({k}), k = 0, . . . , 30, may be
described in Fig. 7.2.
5 Sometimes it is said that the strong law of large numbers is one of the three pearls in Probability
Theory. The two other are the central limit theorem and the so-called law of iterated logarithm, shortly
discussed in Remark 7.2.16.

348
‡±™
7 Limit theorems
Figure 7.2: Probability mass function of Bn,p, n = 30 and p = 0.4.
The peak of the diagram occurs at k = 12, which is the expected value of S30. Enlarg-
ing the number of trials leads to a shift of the peak to the right. At the same time, the
height of the peak becomes smaller.
The shape of the diagram in Fig. 7.2 lets us suggest that sums of independent, identi-
cally distributed random variables are ‚Äúalmost‚Äù normally distributed. If this is so, which
expected value and which variance will the approximating normal distribution possess?
Let us investigate this question in the general setting. Thus, we are given a sequence
(Xj)j‚â•1 of independent identically distributed random variables with finite second mo-
ment and with Œº = ùîºX1 and œÉ2 = ùïçX1 > 0. If, as before, Sn = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn, then
ùîºSn = nŒº
and
ùïçSn = nœÉ2 .
Consequently, if we conjecture that Sn is ‚Äúapproximately‚Äù normally distributed, then the
normalized sum (Sn ‚àínŒº)/œÉ‚àön should be ‚Äúapproximately‚Äù ùí©(0, 1)-distributed. Recall
that Propositions 5.1.38 and 5.2.15 imply
ùîº(Sn ‚àínŒº
œÉ‚àön ) = 0
and
ùïç(Sn ‚àínŒº
œÉ‚àön ) = 1 .
The question about the possible limit of the normalized sums (Sn ‚àínŒº)/œÉ‚àön remained
open for long time. In 1718 Abraham de Moivre investigated the limit behavior for a spe-
cial case of binomial distributed random variables. As limit he found some infinite se-
ries, not a concrete function. In 1808 the American scientist and mathematician Robert
Adrain published a paper where for the first time the normal distribution occurred.
A year later, independently of the former work, Carl Friedrich Gau√ü used the normal dis-
tribution for error estimates. In 1812 Pierre-Simon Laplace proved that the normalized
sums of independent binomial distributed random variables approximate the normal
distribution. Later on, Andrei Andreyevich Markov, Aleksandr Mikhailovich Lyapunov,

7.2 Central limit theorem
‡±™
349
Jarl Waldemar Lindeberg, Paul L√©vy, and other mathematicians continued the work of
De Moivre and Laplace. In particular, they showed that the normal distribution occurs
always as a limit, not only for binomial distributed random variables. The only assump-
tion is that the random variables possess a finite second moment. We refer to the very
interesting book [Fis11] for further reading about the history of normal approximation.
It remains the question in which sense does (Sn ‚àínŒº)/œÉ‚àön converge to the standard
normal distribution. To answer this, we have to introduce the concept of the convergence
in distribution.
Definition 7.2.1. Let Y1, Y2, . . . and Y be random variables with distribution functions F1, F2, . . . and F, re-
spectively. The sequence (Yn)n‚â•1 converges to Y in distribution provided that
lim
n‚Üí‚àûFn(t) = F(t)
for all t ‚àà‚Ñù
at which F is continuous.
(7.9)
In this case, one writes Yn
ùíü
Û≥®Ä‚ÜíY .
Remark 7.2.2. An alternative way to formulate property (7.9) is as follows:
lim
n‚Üí‚àû‚Ñô{Yn ‚â§t} = ‚Ñô{Y ‚â§t}
for all t ‚àà‚Ñùwith ‚Ñô{Y = t} = 0 .
Without proof, we state two other characterizations of convergence in distribution.
Proposition 7.2.3. One has Yn
ùíü
Û≥®Ä‚ÜíY if and only if for all bounded continuous functions
f : ‚Ñù‚Üí‚Ñù,
lim
n‚Üí‚àûùîºf (Yn) = ùîºf (Y) .
Furthermore, this is also equivalent to
lim sup
n‚Üí‚àû‚Ñô{Yn ‚ààA} ‚â§‚Ñô{Y ‚ààA}
for all closed subsets A ‚äÜ‚Ñù, or also to
lim inf
n‚Üí‚àû‚Ñô{Yn ‚ààG} ‚â•‚Ñô{Y ‚ààG}
whenever G ‚äÜ‚Ñùis an open set.
Remark 7.2.4. Note that in general Yn
ùíü
Û≥®Ä‚ÜíY does not imply
lim
n‚Üí‚àû‚Ñô{Yn ‚ààB} = ‚Ñô{Y ‚ààB}
for all Borel sets B ‚äÜ‚Ñù. For example, if ‚Ñô{Yn = 1/n} = 1 and ‚Ñô{Y = 0} = 1, then the
Yns converge to Y in distribution (check this), but if B = (0, 1), then ‚Ñô{Yn ‚ààB} does not
converge to ‚Ñô{Y ‚ààB}.

350
‡±™
7 Limit theorems
If the distribution function of Y is continuous, that is, we have ‚Ñô{Y = t} = 0 for all
t ‚àà‚Ñù, then Yn
ùíü
Û≥®Ä‚ÜíY is equivalent to limn‚Üí‚àûFn(t) = F(t) for all t ‚àà‚Ñù. Besides, in this
case, the type of convergence is stronger as the next proposition shows.
Proposition 7.2.5. Let Y1, Y2, . . . and Y be random variables with ‚Ñô{Y = t} = 0 for all
t ‚àà‚Ñù. Then Yn
ùíü
Û≥®Ä‚ÜíY implies that the distribution functions converge uniformly:
lim
n‚Üí‚àûsup
t‚àà‚Ñù
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚Ñô{Yn ‚â§t} ‚àí‚Ñô{Y ‚â§t}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®= 0 ,
hence also
lim
n‚Üí‚àûsup
a<b
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚Ñô{a ‚â§Yn ‚â§b} ‚àí‚Ñô{a ‚â§Y ‚â§b}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®= 0 .
We have now all notations and definitions that are necessary to formulate the cen-
tral limit theorem. Mostly, this theorem is proved via properties of the so-called charac-
teristic functions (see Chapter 3 of [Dur19] for such a proof). For alternative proofs using
properties of moment generating functions, we refer to [Rss14] and [Gha19].
Proposition 7.2.6 (Central limit theorem). Let (Xj)j‚â•1 be a sequence of independent identi-
cally distributed random variables with finite second moment. Let Œº be the expected value
of the Xjs and let œÉ2 > 0 be their variance. Then for the sums Sn = X1+‚ãÖ‚ãÖ‚ãÖ+Xn it follows that
Sn ‚àínŒº
œÉ‚àön
ùíü
Û≥®Ä‚ÜíZ .
(7.10)
Here Z is an ùí©(0, 1)-distributed random variable.
Since the limit Z in statement (7.10) is a continuous random variable, Proposi-
tion 7.2.5 applies, and the central limit theorem may also be formulated as follows.
Proposition 7.2.7. Suppose (Xj)j‚â•1 and Sn are as in Proposition 7.2.6. Then it follows that
lim
n‚Üí‚àûsup
t‚àà‚Ñù
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚Ñô{Sn ‚àínŒº
œÉ‚àön
‚â§t} ‚àí
1
‚àö2œÄ
t
‚à´
‚àí‚àû
e‚àíx2/2 dx
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
= 0
and
(7.11)
lim
n‚Üí‚àûsup
a<b
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚Ñô{a ‚â§Sn ‚àínŒº
œÉ‚àön
‚â§b} ‚àí
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
= 0 .
(7.12)
Remark 7.2.8. Recall that Œ¶ denotes the distribution function of the standard normal
distribution as introduced in eq. (1.70). Thus, another way to write eq. (7.11) is as follows:
if Fn(t) = ‚Ñô{Sn ‚â§t} denotes the distribution function of Sn, then
sup
t‚àà‚Ñù
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®Fn(œÉ‚àön t + nŒº) ‚àíŒ¶(t)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®= sup
t‚àà‚Ñù
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚Ñô{Sn ‚àínŒº
œÉ‚àön
‚â§t} ‚àíŒ¶(t)
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
Û≥®Ä‚Üí
n‚Üí‚àû0 .
(7.13)
Example 7.2.9. Suppose X1, X2, . . . are independent Pois1-distributed. Hence, their sum
Sn = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn is a Poisn-distributed random variable, and

7.2 Central limit theorem
‡±™
351
Fn(t) = ‚Ñô{Sn ‚â§t} = ‚àë
0‚â§k‚â§t
nk
k! e‚àín .
Since Œº = œÉ = Œª = 1, eq. (7.13) tells us that (compare Figure 7.3)
Fn(‚àön t + n) =
‚àë
0‚â§k‚â§‚àön t+n
nk
k! e‚àín Û≥®Ä‚Üí
n‚Üí‚àûŒ¶(t) .
(7.14)
Moreover, the convergence takes place uniformly in t ‚àà‚Ñù.
Figure 7.3: The approximation of Œ¶(t) in eq. (7.14) with n = 30.
Our next objective is another reformulation of eq. (7.12). If we set a‚Ä≤ = a œÉ‚àön + nŒº
and b‚Ä≤ = b œÉ‚àön + nŒº, then these numbers depend on n ‚àà‚Ñï. But since the convergence
in eq. (7.12) is uniform, we may replace a and b by a‚Ä≤ and b‚Ä≤, respectively and obtain
lim
n‚Üí‚àûsup
a‚Ä≤<b‚Ä≤
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚Ñô{a‚Ä≤ ‚â§Sn ‚â§b‚Ä≤} ‚àí‚Ñô{a‚Ä≤ ‚àínŒº
œÉ‚àön
‚â§Z ‚â§b‚Ä≤ ‚àínŒº
œÉ‚àön }
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
= 0 .
(7.15)
Here, as before, Z denotes a standard normally distributed random variable. For a final
reformulation, set
Zn := œÉ‚àön Z + nŒº .
Then eq. (7.15) is equivalent to
lim
n‚Üí‚àûsup
a‚Ä≤<b‚Ä≤
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚Ñô{a‚Ä≤ ‚â§Sn ‚â§b‚Ä≤} ‚àí‚Ñô{a‚Ä≤ ‚â§Zn ‚â§b‚Ä≤}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®= 0 .
(7.16)
By Proposition 4.2.3, the random variables Zn are ùí©(nŒº, nœÉ2)-distributed, which allows
us to interpret eq. (7.15), or eq. (7.16), as follows. If Œº = ùîºXj and œÉ2 = ùïçXj, then for large n,
the sum Sn is ‚Äúapproximately‚Äù ùí©(nŒº, nœÉ2)-distributed.

352
‡±™
7 Limit theorems
In other words, for ‚àí‚àû‚â§a < b ‚â§‚àûit follows that
‚Ñô{a ‚â§Sn ‚â§b} ‚âàŒ¶(b ‚àínŒº
œÉ‚àön ) ‚àíŒ¶(a ‚àínŒº
œÉ‚àön ) .
Interpretation: We emphasize once more that the central limit theorem is valid for all
sequences of independent identically distributed random variables possessing a second
moment. For example, it is true for Xjs that are binomial distributed, for Xjs being expo-
nentially distributed, and so on. Thus, no matter how the random variables with second
moment are distributed, all their normalized sums possess the same limit, the normal
distribution. This explains the outstanding role of the normal distribution.
The deeper reason for this phenomenon is that Sn may be viewed as the superpo-
sition of many ‚Äúsmall‚Äù independent errors or perturbations, all of the same kind.6 Al-
though each perturbation is distributed according to ‚ÑôX1, the independent superposition
of the perturbations leads to the fact that the final result is approximately normally dis-
tributed. This explains why so many random phenomena may be described by normally
distributed random variables.
Remark 7.2.10 (Continuity correction). A slight technical problem arises in the case of
discrete random variables Xj. Then the Sns are discrete as well, hence their distribution
functions Fn have jumps. If these noncontinuous functions Fn approximate the continu-
ous function Œ¶, then certain errors occur at the points where the jumps of Fn are (com-
pare Figure 7.4).
Figure 7.4: A sequence of noncontinuous functions (here a sequence of distribution functions of binomial
random variables) approximates the continuous function Œ¶.
6 The central limit theorem also holds for not necessarily identically distributed random variables pro-
vided that all ‚Äúerrors‚Äù become uniformly small. That is, one has to exclude that certain errors are domi-
nating the others.

7.2 Central limit theorem
‡±™
353
To understand the problem, assume that the Xjs possess values in ‚Ñ§, then Sn is also
‚Ñ§-valued, hence for any 0 ‚â§h < 1, and all integers k < ‚Ñì, it follows that (see Figure 7.5)
‚Ñô{k ‚â§Sn ‚â§‚Ñì} = ‚Ñô{k ‚àíh ‚â§Sn ‚â§‚Ñì+ h} .
Figure 7.5: If Sn has values in ‚Ñ§, then for any 0 ‚â§h < 1 one has k ‚â§Sn ‚â§‚Ñìif and only if k ‚àíh ‚â§Sn ‚â§‚Ñì+ h.
Thus, both events possess the same probability.
Consequently, for each such number h, the value
Œ¶(‚Ñì+ h ‚àínŒº
œÉ‚àön
) ‚àíŒ¶(k ‚àíh ‚àínŒº
œÉ‚àön
)
may be taken as normal approximation of the above probability. Which number h < 1
should be chosen?
To answer this question, observe the following. If k < m < ‚Ñì, then
‚Ñô{k ‚â§Sn ‚â§‚Ñì} = ‚Ñô{k ‚â§Sn ‚â§m} + ‚Ñô{m + 1 ‚â§Sn ‚â§‚Ñì} ,
which, after choosing h in [0, 1), is approximated by
Œ¶(‚Ñì+ h ‚àínŒº
œÉ‚àön
) ‚àíŒ¶(m + 1 ‚àíh ‚àínŒº
œÉ‚àön
) + Œ¶(m + h ‚àínŒº
œÉ‚àön
) ‚àíŒ¶(k ‚àíh ‚àínŒº
œÉ‚àön
) .
Thus, in order to get neither an overlap nor a gap between m + 1 ‚àíh ‚àínŒº and m + h ‚àínŒº,
it is customary to choose h = 0.5. This leads to the following definition.
Definition 7.2.11. Suppose X1, X2, . . . are independent identically distributed with values in ‚Ñ§. Then the
corrected normal approximation is given by
‚Ñô{k ‚â§Sn ‚â§‚Ñì} ‚âàŒ¶(‚Ñì+ 0.5 ‚àínŒº
œÉ‚àön
) ‚àíŒ¶(k ‚àí0.5 ‚àínŒº
œÉ‚àön
) .
It is called the continuity correction or histogram correction for the normal approximation. In a similar
way, one corrects the approximation for infinite intervals by
‚Ñô{Sn ‚â§‚Ñì} ‚âàŒ¶(‚Ñì+ 0.5 ‚àínŒº
œÉ‚àön
)
and by
‚Ñô{Sn ‚â•k} ‚âà1 ‚àíŒ¶(k ‚àí0.5 ‚àínŒº
œÉ‚àön
) = Œ¶(nŒº ‚àík + 0.5
œÉ‚àön
) .
(7.17)

354
‡±™
7 Limit theorems
The next result tells us that the continuity correction is only needed for small values of
n ‚â•1.
Proposition 7.2.12. For all x ‚àà‚Ñùand h ‚àà‚Ñù, it follows that
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
Œ¶(x + h ‚àínŒº
œÉ‚àön
) ‚àíŒ¶(x ‚àínŒº
œÉ‚àön )
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§
|h|
œÉ‚àö2œÄn
.
Proof. The mean value theorem of Calculus implies the existence of an intermediate
value Œæ in ( x‚àí|h|‚àínŒº
œÉ‚àön
, x+|h|‚àínŒº
œÉ‚àön
) such that
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
Œ¶(x + h ‚àínŒº
œÉ‚àön
) ‚àíŒ¶(x ‚àínŒº
œÉ‚àön )
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
= |h| Œ¶‚Ä≤(Œæ)
œÉ‚àön .
Using
Œ¶‚Ä≤(Œæ) =
1
‚àö2œÄ
e‚àíŒæ2/2 ‚â§
1
‚àö2œÄ
,
this proves the asserted estimate.
Remark 7.2.13. An application of Proposition 7.2.12 with x = k and/or x = ‚Ñì, and with
h = ¬±0.5, shows that the improvement by the continuity correction is at most of order
n‚àí1/2. Thus, it is no longer needed for large n.
Example 7.2.14. Roll a fair die n times. Let Sn be the sum of the n rolls. In view of
eq. (7.16), this sum Sn is approximately ùí©( 7n
2 , 35n
12 )-distributed. In other words, it follows
that
lim
n‚Üí‚àû‚Ñô{a ‚â§Sn ‚àí7n/2
‚àö35n/12
‚â§b} =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx = Œ¶(b) ‚àíŒ¶(a) .
Moreover, this convergence takes place uniformly for all a < b. Therefore, at least for
large n, the right-hand side of the last equation may be taken as an approximate value
of the left-hand one.
At first, we consider an example with a small number of trials. We roll a die three
times and ask for the probability of the event {7 ‚â§S3 ‚â§8}. Let us compare the exact
value
‚Ñô{7 ‚â§S3 ‚â§8} = 1
6 = 0.1 ÃÑ6
with that we get by applying the central limit theorem. Without continuity correction,
the approximate value is
Œ¶( 8 ‚àí21/2
‚àö3 ‚ãÖ35/12
) ‚àíŒ¶( 7 ‚àí21/2
‚àö3 ‚ãÖ35/12
) ‚âà0.08065 ,

7.2 Central limit theorem
‡±™
355
while an application of the continuity correction leads to
Œ¶(8 + 0.5 ‚àí21/2
‚àö3 ‚ãÖ35/12
) ‚àíŒ¶(7 ‚àí0.5 ‚àí21/2
‚àö3 ‚ãÖ35/12
) ‚âà0.16133 .
We see an improvement using the continuity correction.
Next we treat an example with large n. Let us investigate once more Example 7.1.6,
but this time from the point of view of the central limit theorem. Choose again n = 103,
a = ‚àí100 ‚àö12
‚àö35000, and b = 100 ‚àö12
‚àö35000. Then it follows that
‚Ñô{3400 ‚â§Sn ‚â§3600} ‚âà
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx ‚âà0.93592 .
As we see, the use of the central limit theorem improves considerably the bound 0.709
obtained by Chebyshev‚Äôs inequality.
Example 7.2.15. The aim of this example is to apply the central limit theorem for the
investigation of the asymptotic behavior of random walks as they were introduced in
Example 4.1.7 and Section 5.5. Recall that we suppose S0 = 0 and, if n ‚â•1, then Sn =
X1+‚ãÖ‚ãÖ‚ãÖ+Xn, where the Xis are independent and attain the values ‚àí1 and 1 with probability
1 ‚àíp and p, respectively. In eqs. (5.26) and (5.37), we got
ùîºSn = n(2p ‚àí1)
and
ùïçSn = 4np(1 ‚àíp) .
Consequently, the central limit theorem leads in this case to the following: for all real
numbers a < b, we have
lim
n‚Üí‚àû‚Ñô{a ‚â§Sn ‚àín(2p ‚àí1)
2‚àönp(1 ‚àíp)
‚â§b} = Œ¶(b) ‚àíŒ¶(a) .
In the case of a symmetric walk, that is, p = 1/2, this simplifies to
lim
n‚Üí‚àû‚Ñô{a ‚â§Sn
‚àön ‚â§b} = Œ¶(b) ‚àíŒ¶(a) =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx .
For instance, if a = ‚àí2 and b = 2, then it follows that
lim
n‚Üí‚àû‚Ñô{‚àí2‚àön ‚â§Sn ‚â§2‚àön} =
1
‚àö2œÄ
2
‚à´
‚àí2
e‚àíx2/2 dx ‚âà0.954 .
Keep in mind that the possible values of Sn are between ‚àín and n. But in reality, if n is
large enough, then with probability greater than 0.95, the value of Sn will be in the much
smaller interval [‚àí2‚àön, 2‚àön] (see Figure 7.6 for a graphically presentation of this fact).

356
‡±™
7 Limit theorems
Figure 7.6: Two independent symmetric random walks compared with t Û≥®É‚Üí¬±2 ‚àöt.
On the other hand, if we ask for the probability that Sn is between ‚àí‚àön and ‚àön, we
obtain
lim
n‚Üí‚àû‚Ñô{‚àí‚àön ‚â§Sn ‚â§‚àön} =
1
‚àö2œÄ
1
‚à´
‚àí1
e‚àíx2/2 dx ‚âà0.6827 .
Maybe more impressive than the previous statements is the following fact: for any c > 0,
it holds that
lim
n‚Üí‚àû‚Ñô{Sn ‚â•c‚àön} = 1 ‚àíŒ¶(c) =
1
‚àö2œÄ
‚àû
‚à´
c
e‚àít2/2dt .
Remark 7.2.16. More precise statements about the asymptotic behavior of symmetric
random walks are available. For example, the central limit theorem implies
‚Ñô{lim sup
n‚Üí‚àû
Sn
‚àön ‚â•c} > 0
(7.18)
for any c > 0. A zero‚Äìone law tells us that the probability in (7.18) is not only positive,
but equals 1. This leads to
‚Ñô{lim sup
n‚Üí‚àû
Sn
‚àön = ‚àû} = 1
and
‚Ñô{lim inf
n‚Üí‚àû
Sn
‚àön = ‚àí‚àû} = 1 .
Thus, ‚àön is not the right scaling factor for Sn. Some other, bigger, sequence is needed.
On the other hand, a scaling of Sn by n is also not appropriate. Why? Observe that
the strong law of large numbers asserts that
‚Ñô{ lim
n‚Üí‚àû
Sn
n = 0} = 1 .

7.2 Central limit theorem
‡±™
357
Hence, an appropriate scaling of the Sns should be a sequence lying between ‚àön and n.
Surprisingly, the correct sequence of normalization is ‚àö2n log log n. The law of iterated
logarithm due to Hartman and Wintner (see, e. g., [Bil12, Theorem 9.5], or [Kle20]) im-
plies
‚Ñô{lim sup
n‚Üí‚àû
Sn
‚àö2n log log n
= 1} = ‚Ñô{lim inf
n‚Üí‚àû
Sn
‚àö2n log log n
= ‚àí1} = 1 .
Consequently, for any Œµ > 0 one gets
‚Ñô{Sn ‚â•(1 ‚àíŒµ)‚àö2n log log n i. o.} = 1,
while
‚Ñô{Sn ‚â•(1 + Œµ)‚àö2n log log n i. o.} = 0 .
Example 7.2.17 (Round-off errors). Many calculations in a bank, for instance, of interest,
lead to amounts that are not integral in cents. In this case the bank rounds the calculated
value either up or down, whether the remainder is larger or smaller than 0.5 cent. For
example, if the calculations lead to $12.837, then the bank transfers $12.84. Thus, in this
case, the bank loses 0.3 cent. This seems to be a small amount, but if, for example, the
bank performs 106 calculations per day, the total loss or gain could sum up to an amount
of $5000.00. But does this really happen?
Answer: Theoretically, the rounding procedure could lead to huge losses or gains of
the bank. But, as the central limit theorem shows, in reality such a scenario is extremely
unlikely. To make this more precise, we use the following model. Let Xj be the loss or
gain (in cents) of the bank in calculation j. Then the Xj are independent and uniformly
distributed on [‚àí0.5, 0.5]. Thus, the total loss or gain after n calculations equals Sn =
X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn. By Propositions 5.1.27 and 5.2.25, we know that
Œº = ùîºX1 = 0
and
œÉ2 = ùïçX1 = 1
12 ,
hence, if a < b, the central limit theorem implies
lim
n‚Üí‚àû‚Ñô{a‚àön
‚àö12
‚â§Sn ‚â§b‚àön
‚àö12
} =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx .
For example, if n = 106, then taking a = ‚àö12 and b = ‚àû, this leads to
‚Ñô{Sn ‚â•$10} = ‚Ñô{Sn ‚â•103 cents} ‚âà
1
‚àö2œÄ
‚àû
‚à´
‚àö12
e‚àíx2/2 dx ‚âà0.00026603 ,

358
‡±™
7 Limit theorems
which is an extremely small probability. By symmetry, it also follows that
‚Ñô{Sn ‚â§‚àí$10} ‚âà
1
‚àö2œÄ
‚àí‚àö12
‚à´
‚àí‚àû
e‚àíx2/2 dx ‚âà0.00026603 .
In a similar way, one obtains
‚Ñô{Sn ‚â•$1} ‚âà0.364517 ,
‚Ñô{Sn ‚â•$2} ‚âà0.244211,
‚Ñô{Sn ‚â•$5} ‚âà0.0416323
and
‚Ñô{Sn ‚â•$20} ‚âà2.1311 √ó 10‚àí12 .
This shows that even for many calculations, in our case 106, the probability for a loss
or gain of more than $5 is very unlikely. Recall that theoretically an amount of $5000.00
would be possible.
Example 7.2.18. Suppose n people choose independently of each other an integer in
{0, . . . , 9}. Thereby, each of the 10 numbers is equally likely. Of course, the expected value
of the chosen numbers is Œº = 9/2. Moreover, the variance of a single choice can be eval-
uated by
œÉ2 = 1
10
9
‚àë
k=0
(k ‚àí9/2)2 = 33
4 .
Let Sn be the sum of the chosen n numbers. Then the strong law of large numbers yields
that with probability 1,
lim
n‚Üí‚àû
Sn
n = 9
2 .
We ask now how far or near we may expect Sn/n to 9/2, of course, depending on n.
To answer this question, we apply the central limit theorem. It asserts that, given
a < b,
‚Ñô{a ‚â§2 Sn ‚àí9n/2
‚àö33 n
‚â§b} ‚âàŒ¶(b) ‚àíŒ¶(a) .
Equivalently, this is
‚Ñô{a‚àö33
2‚àön ‚â§Sn
n ‚àí9
2 ‚â§b‚àö33
2‚àön } ‚âàŒ¶(b) ‚àíŒ¶(a) .
Setting a = ‚àí2c/‚àö33 and b = 2c/‚àö33, an application of Corollary 4.2.5 leads to
‚Ñô{
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
Sn
n ‚àí9
2
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§
c
‚àön} ‚âàŒ¶( 2c
‚àö33
) ‚àíŒ¶(‚àí2c
‚àö33
) ‚â•0.9973
provided that 2c/‚àö33 ‚â•3, that is, if c > 8.17.

7.2 Central limit theorem
‡±™
359
We refer to Problem 7.8 for a general approach to the question treated in this
example.
Special cases of the central limit theorem
Binomial distributed random variables. In 1738 De Moivre, and later on in 1812 Laplace,
investigated the normal approximation of binomial distributed7 random variables. This
was the starting point for the investigation of general central limit theorems. Let us state
their result.
Proposition 7.2.19 (De Moivre‚ÄìLaplace theorem). Let the Xjs be independent B1,p-dis-
tributed random variables. Then their sums Sn = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn satisfy
lim
n‚Üí‚àû‚Ñô{a ‚â§
Sn ‚àínp
‚àönp(1 ‚àíp)
‚â§b} =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx .
(7.19)
Proof. Recall that for a B1,p-distributed random variable X, we have Œº = ùîºX = p and
œÉ2 = ùïçX = p(1 ‚àíp). Consequently, Proposition 7.2.7 applies and leads to eq. (7.19).
Remark 7.2.20. By Corollary 4.6.2, we know that Sn = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn is Bn,p-distributed.
Consequently, eq. (7.19) may also be written as
lim
n‚Üí‚àû‚àë
k‚ààIn,a,b
(n
k)pk (1 ‚àíp)n‚àík =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx,
where
In,a,b := {k ‚â•0 : a ‚â§
k ‚àínp
‚àönp(1 ‚àíp)
‚â§b} .
Another way to formulate the De Moivre‚ÄìLaplace theorem is as follows. For ‚Äúlarge‚Äù n,
Sn is approximative ùí©(np, np(1 ‚àíp))-distributed. That is, if 0 ‚â§‚Ñì< m ‚â§n, then
m
‚àë
k=‚Ñì
(n
k)pk(1 ‚àíp)n‚àík ‚âàŒ¶(
m ‚àínp
‚àönp(1 ‚àíp)
) ‚àíŒ¶(
‚Ñì‚àínp
‚àönp(1 ‚àíp)
).
(7.20)
Since the sums Sn are integer-valued, the continuity correction should be applied for n
small, that is, on the right-hand side of eq. (7.20) the numbers m and ‚Ñìshould be replaced
by m + 0.5 and by ‚Ñì‚àí0.5, respectively.
Example 7.2.21. Play a series of games with success probability 0 < p < 1. Let Œ± ‚àà(0, 1)
be a given security probability, and m ‚àà‚Ñïis some integer. How many games does one
7 De Moivre investigated sums of B1,1/2-distributed random variables while Laplace treated B1,p-
distributed ones for general 0 ‚â§p ‚â§1.

360
‡±™
7 Limit theorems
have to play in order to have with probability greater than or equal to 1 ‚àíŒ± at least m
successes?
Answer: Define random variables Xj by setting Xj = 1 when winning game j, while
Xj = 0 in the case of losing it. Then the Xjs are independent and B1,p-distributed. Hence,
if Sn = X1 + ‚ãÖ‚ãÖ‚ãÖXn, then the above question may be formulated as follows. What is the
smallest n ‚àà‚Ñïfor which
‚Ñô{Sn ‚â•m} ‚â•1 ‚àíŒ± ?
(7.21)
By Corollary 4.6.2, the sum Sn is Bn,p-distributed and, therefore, the estimate in (7.21)
transforms to
n
‚àë
k=m
(n
k)pk(1 ‚àíp)n‚àík ‚â•1 ‚àíŒ± .
(7.22)
Thus, the ‚Äúexact‚Äù answer to the above question is as follows. Choose the minimal n ‚â•1
for which estimate (7.22) is valid.
Remark 7.2.22. For large m, it may be a difficult task to determine the minimal n satis-
fying estimate (7.22). Therefore, one looks for an ‚Äúapproximate‚Äù approach via Proposi-
tion 7.2.19. Rewriting estimate (7.21) as
‚Ñô{
Sn ‚àínp
‚àönp(1 ‚àíp)
‚â•
m ‚àínp
‚àönp(1 ‚àíp)
} ‚â•1 ‚àíŒ± ,
an ‚Äúapproximate‚Äù condition for n is
1 ‚àíŒ± ‚â§1 ‚àíŒ¶(
m ‚àínp
‚àönp(1 ‚àíp)
) = Œ¶(
np ‚àím
‚àönp(1 ‚àíp)
).
Given Œ≤ ‚àà(0,1), let us define8 zŒ≤ by Œ¶(zŒ≤) = Œ≤. Consequently, an approximate solution of
the above question is to choose the minimal n ‚â•1 satisfying
np ‚àím
‚àönp(1 ‚àíp)
‚â•z1‚àíŒ± .
(7.23)
For ‚Äúsmall‚Äù n, we have to modify the previous approach slightly. Here we have to use
the continuity correction. In view of eq. (7.17), the condition is now
1 ‚àíŒ± ‚â§Œ¶(np ‚àím + 0.5
‚àönp(1 ‚àíp)
) ,
8 Later on, in Proposition 8.4.3, these numbers zŒ≤ will play an important role; compare also with Defi-
nition 8.4.8.

7.2 Central limit theorem
‡±™
361
leading to
np ‚àím + 0.5
‚àönp(1 ‚àíp)
‚â•z1‚àíŒ± .
(7.24)
Let us explain Remark 7.2.22 with the help of a concrete example.
Example 7.2.23. Find the minimal n ‚â•1 such that, rolling a fair die n times, one observes
with probability greater than or equal to 0.9 at least 100 times the number 6?
For the ‚Äúexact‚Äù answer choose the minimal n ‚â•1 satisfying
n
‚àë
k=100
(n
k)( 1
6)
k
(5
6)
n‚àík
‚â•0.9 .
Numerical calculations give that the left-hand side equals 0.897721 if n = 670, and it
is 0.900691 if n = 671. Thus, in order to observe, with probability greater than 0.9, the
number 6 at least 100 times, one has to roll the die at least 671 times.
Let us compare this result with that we obtained by the approximation approach.
First, we approximate Sn directly, that is, without applying the continuity correction.
Here estimate (7.23) says that we have to look for the minimal n ‚â•1 satisfying
n
6 ‚àím
‚àö1
6 ‚ãÖ5
6 ‚ãÖn
= n ‚àí600
‚àö5n
‚â•z0.9 = 1.28155 .
(7.25)
Since
665 ‚àí600
‚àö5 ‚ãÖ665
= 1.12724
and
666 ‚àí600
‚àö5 ‚ãÖ666
= 1.4373 ,
the smallest n satisfying estimate (7.25) is 666.
Applying the continuity correction, by estimate (7.24), condition (7.25) has to be re-
placed by
n
6 ‚àím + 0.5
‚àö1
6 ‚ãÖ5
6 ‚ãÖn
= n ‚àí600 + 3
‚àö5n
‚â•z0.9 = 1.28155 .
The left-hand side equals 1.27757 for n = 671 and 1.29387 if n = 672. Consequently, this
type of approximation gives the (more precise) value n = 672 for the minimal number
of necessary rolls of the die.
Poisson distributed random variables. Let X1, X2, . . . be independent and PoisŒª-dis-
tributed. By Propositions 5.1.16 and 5.2.22, we know
ùîºX1 = Œª
and
ùïçX1 = Œª .
Thus, in this case Proposition 7.2.7 reads as follows.

362
‡±™
7 Limit theorems
Proposition 7.2.24. Let (Xj)j‚â•1 be independent PoisŒª-distributed random variables. Then
the sums Sn = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn satisfy
lim
n‚Üí‚àû‚Ñô{a ‚â§Sn ‚àínŒª
‚àönŒª
‚â§b} =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx .
(7.26)
Remark 7.2.25. By Proposition 4.6.5, the sum Sn is PoisŒªn-distributed, hence eq. (7.26)
transforms to
lim
n‚Üí‚àû‚àë
k‚ààJn,a,b
(Œªn)k
k!
e‚àíŒªn =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx ,
(7.27)
where
Jn,a,b := {k ‚àà‚Ñï0 : a ‚â§k ‚àínŒª
‚àönŒª
‚â§b} .
Another way to express this is as follows. If 0 ‚â§‚Ñì< m < ‚àû, then
m
‚àë
k=‚Ñì
(Œªn)k
k!
e‚àíŒªn ‚âàŒ¶(m ‚àínŒª
‚àönŒª
) ‚àíŒ¶(‚Ñì‚àínŒª
‚àönŒª
) .
Remark 7.2.26. Choosing in eq. (7.27) the numbers as a = ‚àí‚àû, b = 0, and Œª = 1, we get
lim
n‚Üí‚àûe‚àín
n
‚àë
k=0
nk
k! = 1
2 ,
which is interesting in its own right. Taking = ‚àí‚àûand bn = ‚àön yields
lim
n‚Üí‚àû
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
e‚àín
2n
‚àë
k=0
nk
k! ‚àí
1
‚àö2œÄ
bn
‚à´
‚àí‚àû
e‚àíx2/2 dx
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
= 0 ,
hence, because of
lim
n‚Üí‚àû
1
‚àö2œÄ
‚àön
‚à´
‚àí‚àû
e‚àíx2/2 dx = 1 ,
we obtain
lim
n‚Üí‚àûe‚àín
2n
‚àë
k=0
nk
k! = 1 .
Gamma distributed random variables. Finally, we investigate sums of gamma distributed
random variables. Here the central limit theorem leads to the following result.

7.2 Central limit theorem
‡±™
363
Proposition 7.2.27. Let X1, X2, . . . be independent ŒìŒ±,Œ≤-distributed random variables. Then
their sums Sn = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn satisfy
lim
n‚Üí‚àû‚Ñô{a ‚â§Sn ‚àínŒ±Œ≤
Œ±‚àönŒ≤
‚â§b} =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx .
(7.28)
Proof. Propositions 5.1.28 and 5.2.26 tell us that the expected value and the variance of
the Xjs are given by Œº = ùîºX1 = Œ±Œ≤ and œÉ2 = ùïçX1 = Œ±2Œ≤. Therefore, eq. (7.28) follows by
an application of Proposition 7.2.7.
Remark 7.2.28. Note that Proposition 4.6.4 implies that Sn is ŒìŒ±,nŒ≤-distributed. Thus, set-
ting
In,a,b := {x ‚â•0 : a ‚â§x ‚àínŒ±Œ≤
Œ±‚àönŒ≤
‚â§b} ,
eq. (7.28) leads to
lim
n‚Üí‚àû
1
Œ±nŒ≤Œì(nŒ≤) ‚à´
In,a,b
xnŒ≤‚àí1 e‚àíx/Œ± dx =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx .
Another way to express this is as follows. If 0 ‚â§a < b, then
1
Œ±nŒ≤Œì(nŒ≤)
b
‚à´
a
xnŒ≤‚àí1 e‚àíx/Œ± dx ‚âàŒ¶(b ‚àínŒ±Œ≤
Œ±‚àönŒ≤
) ‚àíŒ¶(a ‚àínŒ±Œ≤
Œ±‚àönŒ≤
) .
Two cases of Proposition 7.2.27, or Remark 7.2.28, are of special interest.
(a) For n ‚â•1, let Sn be a œá2
n-distributed random variable. Then it follows that
lim
n‚Üí‚àû‚Ñô{a ‚â§Sn ‚àín
‚àö2n
‚â§b} =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx .
Another way to express this as follows: if the Sns are œá2
n-distributed, then for t ‚àà‚Ñù
and n sufficiently large,
‚Ñô{Sn ‚â§‚àö2n t + n} ‚âà
1
‚àö2œÄ
t
‚à´
‚àí‚àû
e‚àíx2/2dx = Œ¶(t) .
In Fig. 7.7, one sees how good the approximation of Œ¶ is even for small n.

364
‡±™
7 Limit theorems
Figure 7.7: The normalized and shifted distribution function ‚Ñô{Sn ‚â§‚àö2n t + n} of a œá2
n-distributed random
variable Sn. We chose n = 5 (upper graph at zero) and n = 16 (middle graph), in comparison with the
approximated Œ¶(t) (lower graph).
(b) If Sn is distributed according to the Erlang distribution EŒª,n, then we get
lim
n‚Üí‚àû‚Ñô{a ‚â§ŒªSn ‚àín
‚àön
‚â§b} =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2 dx .
For Œª = 1, this implies (set a = ‚àí‚àûand b = 0) that
lim
n‚Üí‚àû
1
Œì(n)
n
‚à´
0
xn‚àí1 e‚àíx dx =
1
‚àö2œÄ
0
‚à´
‚àí‚àû
e‚àíx2/2 dx = 1
2 .
Additional remarks
(1) We play a series of the same game. Suppose in each game we may lose or win
a certain amount of money. A natural condition for these games (among friends) is
whether it should be fair. But what does it mean that a series of games is fair? Is this the
case
(i)
if the average loss or gain in each single game is zero, or
(ii) if the probability that, after n games, the total loss or gain is positive, tends to 1/2 as
n tends to infinity?
The mathematical formulation of the previous question is as follows. Let X1, X2, . . . de-
note the win or loss in the first game, the second, and so on. Then the Xjs are independent
identically distributed random variables. The above question reads now as follows. Is
the game fair
(i)
if the expected value Œº = ùîºX1 satisfies Œº = 0, or

7.2 Central limit theorem
‡±™
365
(ii) if the sum Sn := X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn fulfills
lim
n‚Üí‚àû‚Ñô{Sn ‚â§0} = lim
n‚Üí‚àû‚Ñô{Sn ‚â•0} = 1
2 ?
(7.29)
In the sequel, we have to exclude the trivial case ‚Ñô{Xj = 0} = 1, that is, in each game one
neither wins nor loses some money. Of course, then eq. (7.29) does not hold.
At a first glance, one might believe that the two possible definitions of fairness de-
scribe the same fact. But this is not so as one may see in an example in [Fel68, Chapter X,
Section 4]. There one finds a sequence of independent random variables X1, X2, . . . with
ùîºX1 = 0, however,
lim
n‚Üí‚àû‚Ñô{Sn ‚â§0} = 1 .
In particular, this tells us that, in general, condition (i) does not imply condition (ii).
The next result clarifies the relation between these two definitions of fairness in the
case that the random variables possess a finite second moment.
Proposition 7.2.29. Let X1, X2, . . . be independent and identically distributed with ex-
pected value Œº. Assume ‚Ñô{Xj = 0} < 1.
1.
Then eq. (7.29) always implies Œº = 0. That is, a fair game in the sense of (ii) also
satisfies condition (i).
2.
Conversely, if ùîº|X1|2 < ‚àû, then (ii) is a consequence of (i). Hence, assuming the exis-
tence of a second moment, conditions (i) and (ii) are equivalent.
Proof. We prove the contraposition of the first statement. Thus, suppose that (i) does not
hold, that is, we have Œº
Ã∏= 0. Without losing generality, we may assume Œº > 0. Otherwise,
investigate ‚àíX1, ‚àíX2, . . . An application of Proposition 7.1.28 with Œµ = Œº/2 yields
lim
n‚Üí‚àû‚Ñô{
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
Sn
n ‚àíŒº
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§Œº
2 } = 1 .
(7.30)
Since | Sn
n ‚àíŒº| ‚â§Œº/2 implies Sn
n ‚â•Œº/2, hence Sn ‚â•0, it follows that
‚Ñô{
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
Sn
n ‚àíŒº
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§Œº
2 } ‚â§‚Ñô{Sn ‚â•0} .
Consequently, from eq. (7.30) we derive
lim
n‚Üí‚àû‚Ñô{Sn ‚â•0} = 1 ,
hence eq. (7.29) cannot be valid. This proves the first part of the proposition.
We prove now the second assertion. Thus, suppose Œº = 0 as well as the existence of
the variance œÉ2 = ùïçX1. Note that œÉ2 > 0. Why? If a random variable X satisfies ùîºX = 0

366
‡±™
7 Limit theorems
and ùïçX = 0, then necessarily ‚Ñô{X = 0} = 1. But, since we assumed ‚Ñô{X1 = 0} < 1, we
cannot have œÉ2 = ùïçX1 = 0.
Thus, Proposition 7.2.7 applies and leads to
lim
n‚Üí‚àû‚Ñô{Sn ‚â•0} = lim
n‚Üí‚àû{ Sn
œÉ‚àön ‚â•0} =
1
‚àö2œÄ
‚àû
‚à´
0
e‚àíx2/2 dx = 1
2 .
The proof for ‚Ñô{Sn ‚â§0} ‚Üí1/2 follows in the same way, thus eq. (7.29) is valid. This
completes the proof.
(2) How fast does Sn‚àínŒº
œÉ‚àön converge to a normally distributed random variable? Before
we answer this question, we have to determine how this speed is measured. In view of
Proposition 7.2.7, we use the following quantity depending on n ‚â•1:
sup
t‚àà‚Ñù
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚Ñô{Sn ‚àínŒº
œÉ‚àön
‚â§t} ‚àí
1
‚àö2œÄ
t
‚à´
‚àí‚àû
e‚àíx2/2 dx
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
.
Doing so, the following classical result holds (see [Dur19, Section 3.4.4], for a proof).
Proposition 7.2.30 (Berry‚ÄìEss√©en theorem). Let X1, X2, . . . be independent identically dis-
tributed random variables with finite third moment, that is, with ùîº|X1|3 < ‚àû. If Œº = ùîºX1
and œÉ2 = ùïçX1 > 0, then it follows that
sup
t‚àà‚Ñù
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚Ñô{Sn ‚àínŒº
œÉ‚àön
‚â§t} ‚àí
1
‚àö2œÄ
t
‚à´
‚àí‚àû
e‚àíx2/2 dx
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§C ‚ãÖùîº|X1|3
œÉ3
n‚àí1/2 .
(7.31)
Here C > 0 denotes a universal constant.
Remark 7.2.31. The order n‚àí1/2 in estimate (7.31) is optimal and cannot be improved.
This can be seen by the following example. Take independent random variables X1, X2, . . .
with ‚Ñô{Xj = ‚àí1} = ‚Ñô{Xj = 1} = 1/2. Hence, in this case Œº = 0 and œÉ2 = 1. Then one has
lim inf
n‚Üí‚àûn1/2 sup
t‚àà‚Ñù
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚Ñô{ Sn
‚àön ‚â§t} ‚àí
1
‚àö2œÄ
t
‚à´
‚àí‚àû
e‚àíx2/2 dx
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
> 0 .
(7.32)
Assertion (7.32) is a consequence of the fact that, if n is even, then the function t Û≥®É‚Üí
‚Ñô{ Sn
‚àön ‚â§t} has a jump of order n‚àí1/2 at zero. This follows by the calculations in Exam-
ple 4.1.7. On the other hand, t Û≥®É‚ÜíŒ¶(t) is continuous, hence the maximal difference be-
tween these two functions is at least half of the height of the jump.
Remark 7.2.32. The exact value of the constant C > 0 appearing in estimate (7.31) is, in
spite of intensive investigations, still unknown. At present, the best-known estimates are
0.40973 < C < 0.4748.

7.3 Problems
‡±™
367
Summary: The central limit theorem belongs to the most important mathematical results. It explains why so
many random observations in nature, community or business, etc., are distributed according to the normal
distribution.
The precise statement of the central limit theorem is as follows: if X1, X2, . . . are independent identically
distributed random variables with expected value Œº and variance œÉ2 > 0, then their sum Sn = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn is
approximative ùí©(nŒº, nœÉ2)-distributed. After normalizing the sum Sn in the right way, this says that
‚Ñô{a ‚â§Sn ‚àínŒº
œÉ‚àön
‚â§b} Û≥®Ä‚Üí
n‚Üí‚àûŒ¶(b) ‚àíŒ¶(a) =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2dx .
In case of integer valued Xjs and small n the continuity correction improves the approximation of Sn by the
normal distribution as follows: if k ‚â§‚Ñìare integers, then one uses as approximation
‚Ñô{k ‚â§Sn ‚â§‚Ñì} ‚âàŒ¶(‚Ñì+ 0.5 ‚àínŒº
œÉ‚àön
) ‚àíŒ¶(k ‚àí0.5 ‚àínŒº
œÉ‚àön
) .
7.3 Problems
Problem 7.1. Let A1, A2, . . . and B1, B2, . . . be two sequences of events in a probability
space (Œ©, ùíú, ‚Ñô). Prove that
lim sup
n‚Üí‚àû(An ‚à™Bn) = lim sup
n‚Üí‚àû(An) ‚à™lim sup
n‚Üí‚àû(Bn) .
Is this also valid for the intersection? That is, does one have
lim sup
n‚Üí‚àû(An ‚à©Bn) = lim sup
n‚Üí‚àû(An) ‚à©lim sup
n‚Üí‚àû(Bn) ?
Problem 7.2. Let A1, A2, . . . be a sequence of subsets in Œ©. Show that
1{lim inf An} = lim inf
n‚Üí‚àû1An
and
1{lim sup An} = lim sup
n‚Üí‚àû
1An .
Here 1A denotes the indicator function of a set A as introduced in Definition 3.6.16.
Problem 7.3. Let (Xn)n‚â•1 be a sequence of independent EŒª-distributed random vari-
ables. Characterize sequences (cn)n‚â•1 of positive real numbers for which
‚Ñô{Xn ‚â•cn i. o.} = 1?
Problem 7.4. Let f : [0, 1] ‚Üí‚Ñùbe a continuous function. Its Bernstein polynomial Bf
n
of degree n is defined by
Bf
n(x) :=
n
‚àë
k=0
f (k
n)(n
k)xk(1 ‚àíx)n‚àík ,
0 ‚â§x ‚â§1 .

368
‡±™
7 Limit theorems
Show that Proposition 7.1.30 implies the following. If ‚Ñôis the uniform distribution
on [0, 1], then
‚Ñô{x ‚àà[0, 1] : lim
n‚Üí‚àûBf
n(x) = f (x)} = 1 .
Remark: Using methods from Calculus, one may even show the uniform conver-
gence, that is,
lim
n‚Üí‚àûsup
0‚â§x‚â§1
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®Bf
n(x) ‚àíf (x)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®= 0 .
Problem 7.5. Roll a fair die 180 times. What is the probability that the number ‚Äú6‚Äù occurs
at most 25 times. Determine this probability by the following three methods:
‚Äì
Directly via the binomial distribution.
‚Äì
Approximately by virtue of the central limit theorem.
‚Äì
Approximately by applying the continuity correction.
Problem 7.6. Toss a fair coin 16 times. Compute the probability to observe exactly eight
times ‚Äúheads‚Äù by the following methods:
‚Äì
Directly via the binomial distribution.
‚Äì
Approximately by applying the continuity correction.
Why does one not get a reasonable result using the normal approximation directly, that
is, without continuity correction?
Problem 7.7. Let X1, X2, . . . be a sequence of independent Gp-distributed random vari-
ables, that is, for some 0 < p < 1 one has
‚Ñô{Xj = k} = p(1 ‚àíp)k‚àí1 ,
k = 1, 2, . . .
1.
What does the central limit theorem tell us in this case about the behavior of the
sums Sn = X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn?
2.
For two real numbers a < b, set
In,a,b := {k ‚â•0 : a ‚â§pk ‚àín(1 ‚àíp)
‚àön(1 ‚àíp)
‚â§b} .
Argue why
lim
n‚Üí‚àû‚àë
k‚ààIn,a,b
(‚àín
k )pn(1 ‚àíp)k =
1
‚àö2œÄ
b
‚à´
a
e‚àíx2/2dx .
Hint: Use Corollary 4.6 and investigate Sn ‚àín.

7.3 Problems
‡±™
369
Problem 7.8. Extend the question treated in Example 7.2.18 to the general setting. That
is, given independent, identically distributed random variables X1, X2, . . . with expected
value Œº and variance œÉ2, find c > 0 depending on œÉ2 such that for sufficiently large n it
follows that
‚Ñô{
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn
n
‚àíŒº
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§
c
‚àön} ‚â•Œ¶(3) ‚àíŒ¶(‚àí3) ‚âà0.9973 .

8 Mathematical statistics
8.1 Statistical models
8.1.1 Nonparametric statistical models
The main objective of Probability Theory is to describe and analyze random experi-
ments by means of a suitable probability space (Œ©, ùíú, ‚Ñô). Here it is always assumed that
the probability space is known, in particular, that the describing probability measure,
‚Ñô, is identified.
Probability Theory:
Description of a random experiment and its properties by a probability space. The distribution of the out-
comes is assumed to be known.
Mathematical Statistics deals mainly with the reverse question: one executes an exper-
iment, that is, one draws a sample (e. g., one takes a series of measurements of an item
or one questions several people), and, on the basis of the observed sample, one wants
to derive as much information as possible about the (unknown) underlying probability
measure ‚Ñô. Sometimes the precise knowledge of ‚Ñôis not needed; it may suffice to know
a certain parameter of ‚Ñô.
Mathematical Statistics:
As a result of a statistical experiment, a (random) sample is observed. On its basis, conclusions are drawn
about the unknown underlying probability distribution.
Let us state the mathematical formulation of the task: first, we mention that it is stan-
dard practice in Mathematical Statistics to denote the describing probability space by
(ùí≥, ‚Ñ±, ‚Ñô). As before, ùí≥is the sample space (the set that contains all possible outcomes
of the experiment), and ‚Ñ±is a suitable œÉ-field of events. The probability measure ‚Ñôde-
scribes the experiment, that is, ‚Ñô(A) is the probability of observing a sample belonging
to A, but recall that ‚Ñôis unknown.
Based on theoretical considerations or on long-time experience, quite often we are
able to restrict the entirety of probability measures in question. Mathematically, this
means that we choose a set P of probability measures on (ùí≥, ‚Ñ±) which contains what
we believe to be the ‚Äúcorrect‚Äù ‚Ñô. Thereby, it is not impossible that P is the set of all
probability measures, but for most statistical methods it is very advantageous to take P
as small as possible. On the other hand, the set P cannot be chosen too small, because
we have to be sure that the ‚Äúcorrect‚Äù ‚Ñôis really contained in P. Otherwise, the obtained
results are either false or imprecise.
https://doi.org/10.1515/9783111325064-008

8.1 Statistical models
‡±™
371
Definition 8.1.1. A subset P of probability measures on (ùí≥, ‚Ñ±) is called a distribution assumption, that
is, one assumes that the underlying (unknown) ‚Ñôbelongs to the collection P.
After having fixed the distribution assumption P, one now regards only probability mea-
sures ‚Ñô‚ààP or, equivalently, measures not in P are discarded.
To get information about the unknown probability measure, one performs a statisti-
cal experiment or analyzes some given data. In both cases, the result is a random sample
x ‚ààùí≥. The task of Mathematical Statistics is to get information about ‚Ñô‚ààP, based on
the observed sample x ‚ààùí≥. A suitable way to describe the problem is as follows.
Definition 8.1.2. A (nonparametric) statistical model is a collection of probability spaces (ùí≥, ‚Ñ±, ‚Ñô) with
‚Ñô‚ààP. Here, ùí≥and ‚Ñ±are fixed, and ‚Ñôvaries through the distribution assumption P. One writes for the
model
(ùí≥, ‚Ñ±, ‚Ñô)‚Ñô‚ààP
or
{(ùí≥, ‚Ñ±, ‚Ñô) : ‚Ñô‚ààP} .
Let us illustrate the previous definition with two examples.
Example 8.1.3. In an urn there are white and black balls of an unknown ratio. Let Œ∏ ‚àà
[0, 1] be the (unknown) proportion of white balls, hence 1‚àíŒ∏ is that of the black ones. In
order to get some information about Œ∏, one randomly chooses n balls with replacement.
The result of this experiment, or the sample, is a number k ‚àà{0, . . . , n}, the frequency
of observed white balls. Thus, the sample space is ùí≥= {0, . . . , n} and as œÉ-field we may
choose, as always for finite sample spaces, the powerset ùí´(ùí≥). The possible probability
measures describing this experiment are binomial distributions Bn,Œ∏ with 0 ‚â§Œ∏ ‚â§1.
Consequently, the distribution assumption is
P = {Bn,Œ∏ : Œ∏ ‚àà[0, 1]} .
Summing up, the statistical model describing the experiment is
(ùí≥, ùí´(ùí≥), ‚Ñô)‚Ñô‚ààP
where ùí≥= {0, . . . , n}
and
P = {Bn,Œ∏ : 0 ‚â§Œ∏ ‚â§1} .
Next, we consider an important example from quality control.
Example 8.1.4. A buyer obtains from a trader a delivery of N machines. Among them
M ‚â§N are defective. The buyer does not know the value of M. To determine it, he
randomly chooses n machines from the delivery and checks them. The result, or the
sample, is the number 0 ‚â§m ‚â§n of defective machines among the n tested.
Thus, the sample space is ùí≥= {0, . . . , n}, ‚Ñ±= ùí´(ùí≥), and the probability measures
in question are hypergeometric ones. Therefore, the distribution assumption is
P = {HN,M,n : M = 0, . . . , N} ,
where HN,M,n denotes the hypergeometric distribution with parameters N, M, and n, as
introduced in Definition 1.4.32.

372
‡±™
8 Mathematical statistics
Before we proceed further, we consider a particularly interesting case of statisti-
cal model, which describes the n-fold independent repetition of a single experiment. To
explain this model, let us investigate the following easy example.
Example 8.1.5. We are given a die that looks biased. To check this, we roll it n times
and record the sequence of numbers appearing in each of the trials. Thus, our sample
space is ùí≥= {1, . . . , 6}n, and the observed sample is x = (x1, . . . , xn), with 1 ‚â§xk ‚â§6.
Let Œ∏1, . . . , Œ∏6 be the probabilities for 1 to 6. Then we want to check whether or not
Œ∏1 = ‚ãÖ‚ãÖ‚ãÖ= Œ∏6 =
1
6, that is, whether ‚Ñô0 given by ‚Ñô0({k}) = Œ∏k, 1 ‚â§k ‚â§6, is the uni-
form distribution. What are the possible probability measures on (ùí≥, ùí´(ùí≥)) describing
the statistical experiment? Since the results of different rolls are independent, the de-
scribing measure ‚Ñôis of the form ‚Ñô= ‚Ñô‚äón
0 with
‚Ñô‚äón
0 ({x}) = ‚Ñô0({x1}) ‚ãÖ‚ãÖ‚ãÖ‚Ñô0({xn}) = Œ∏m1
1 ‚ãÖ‚ãÖ‚ãÖŒ∏m6
6 ,
x = (x1, . . . , xn) ,
and where the mks denote the frequency of the number 1 ‚â§k ‚â§6 in the sequence x.
Consequently, the natural distribution assumption is
P = {‚Ñô‚äón
0
: ‚Ñô0 probability measure on {1, . . . , 6}} .
Suppose we are given a probability space (ùí≥0, ‚Ñ±0, ‚Ñô0) with unknown ‚Ñô0 ‚ààP0. Here,
P0 denotes a set of probability measures on (ùí≥0, ‚Ñ±0), hopefully containing the ‚Äúcor-
rect‚Äù ‚Ñô0. We call (ùí≥0, ‚Ñ±0, ‚Ñô0)‚Ñô0‚ààP0 the initial model. In Example 8.1.5, the initial model
is ùí≥0 = {1, . . . , 6}, while P0 is the set of all probability measures on (ùí≥0, ùí´(‚Ñ±0)).
In order to determine ‚Ñô0, we execute n independent trials according to ‚Ñô0. The
result, or the observed sample, is a vector x = (x1, . . . , xn) with xi ‚ààùí≥0. Consequently,
the natural sample space is ùí≥= ùí≥n
0 .
Which statistical model does this experiment describe? To answer this question,
let us recall the basic results in Section 1.9, where exactly those problems have been
investigated. As œÉ-field ‚Ñ±, we choose the n-fold product œÉ-field of ‚Ñ±0, that is,
‚Ñ±= ‚Ñ±0 ‚äó‚ãÖ‚ãÖ‚ãÖ‚äó‚Ñ±0
‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
n times
,
and the describing probability measure ‚Ñôis of the form ‚Ñô‚äón
0 , that is, it is the n-fold prod-
uct of ‚Ñô0. Recall that, according to Definition 1.9.5, the product ‚Ñô‚äón
0 is the unique proba-
bility measure on (ùí≥, ‚Ñ±) satisfying
‚Ñô‚äón
0 (A1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó An) = ‚Ñô0(A1) ‚ãÖ‚ãÖ‚ãÖ‚Ñô0(An) ,
whenever Aj ‚àà‚Ñ±0. Since we assumed ‚Ñô0 ‚ààP0, the possible probability measures are
‚Ñô‚äón
0 with ‚Ñô0 ‚ààP0.
Let us summarize what we obtained until now.

8.1 Statistical models
‡±™
373
Definition 8.1.6. The statistical model for the n-fold independent repetition of an experiment, deter-
mined by the initial model (ùí≥0, ‚Ñ±0, ‚Ñô0)‚Ñô0‚ààP0, is given by
(ùí≥, ‚Ñ±, ‚Ñô‚äón
0 )‚Ñô0‚ààP0
where ùí≥= ùí≥n
0 , ‚Ñ±denotes the n-fold product œÉ-field of ‚Ñ±0, and ‚Ñô‚äón
0 is the n-fold product measure of ‚Ñô0.
Remark 8.1.7. Of course, the main goal in the model of n-fold repetition is to get some
knowledge about ‚Ñô0. To obtain the desired information, we perform n independent tri-
als, each time observing a value distributed according to ‚Ñô0. Altogether, the sample is a
vector x = (x1, . . . , xn), which is now distributed according to ‚Ñô‚äón
0 .
The two following examples explain Definition 8.1.6.
Example 8.1.8. A coin is labeled on one side with ‚Äú0‚Äù and on the other side with ‚Äú1.‚Äù
There is some evidence that the coin is biased. To check this, let us execute the follow-
ing statistical experiment: toss the coin n times and record the sequence of zeroes and
ones. Thus, the observed sample is an x = (x1, . . . , xn), with each xk being either ‚Äú0‚Äù
or ‚Äú1.‚Äù
Our initial model is given by ùí≥0 = {0, 1} and ‚Ñô0 = B1,Œ∏ for a certain (unknown)
Œ∏ ‚àà[0, 1]. Then the experiment is described by ùí≥= {0, 1}n and P = {B‚äón
1,Œ∏ : 0 ‚â§Œ∏ ‚â§1}.
Note that
B‚äón
1,Œ∏({x}) = Œ∏k(1 ‚àíŒ∏)n‚àík ,
k = x1 + ‚ãÖ‚ãÖ‚ãÖ+ xn .
Example 8.1.9. A company produces a new type of light bulb with an unknown distri-
bution of the lifetime. To determine it, n light bulbs are switched on at the same time.
Let t = (t1, . . . , tn) be the times when the bulbs burn out. Then our sample is the vector
t ‚àà(0, ‚àû)n.
From long-time experience, one knows the lifetime of each light bulb is exponen-
tially distributed. Thus, the initial model is (‚Ñù, ‚Ñ¨(‚Ñù), P0) with P0 = {EŒª : Œª > 0}. Conse-
quently, the experiment of testing n light bulbs is described by the model
(‚Ñùn, ‚Ñ¨(‚Ñùn), ‚Ñô‚äón)‚Ñô‚ààP0 = (‚Ñùn, ‚Ñ¨(‚Ñùn), E‚äón
Œª )Œª>0 ,
where P0 = {EŒª : Œª > 0}. Recall that E‚äón
Œª
is the probability measure on (‚Ñùn, ‚Ñ¨(‚Ñùn)) with
density p(t1, . . . , tn) = Œªne‚àíŒª(t1 + ‚ãÖ‚ãÖ‚ãÖ+ tn) for tj ‚â•0.
Summary: The basic problem in Mathematical Statistics is as follows: One observes a random sample x
belonging to a sample space ùí≥and, depending on this observed x, one wants to get as much information
as possible about the underlying unknown probability measure ‚Ñô. The statistical model to describe this task
is a triple
(ùí≥, ‚Ñ±, ‚Ñô)‚Ñô‚ààP
or
{(ùí≥, ‚Ñ±, ‚Ñô) : ‚Ñô‚ààP} .

374
‡±™
8 Mathematical statistics
Here ùí≥is the sample space, ‚Ñ±denotes a œÉ-field of subsets of ùí≥, mostly ùí´(ùí≥) or ‚Ñ¨(‚Ñù), and P is a certain set
of probability measures defined on ‚Ñ±. The collection P of probability measures is said to be the distribution
assumption. One conjectures or knows by theoretical considerations that the underlying unknown probability
measure ‚Ñôbelongs to P.
8.1.2 Parametric statistical models
In all of our previous examples, there was a parameter that parametrized the proba-
bility measures in P in natural way. In Example 8.1.3, this is the parameter Œ∏ ‚àà[0, 1],
in Example 8.1.4, the probability measures are parametrized by M ‚àà{0, . . . , N}, in Ex-
ample 8.1.8 the parameter is also Œ∏ ‚àà[0, 1], and, finally, in Example 8.1.9 the natural
parameter is Œª > 0. Therefore, from now on, we assume that there is a parameter set Œò
such that P may be represented as
P = {‚ÑôŒ∏ : Œ∏ ‚ààŒò} .
Definition 8.1.10. A parametric statistical model is defined as
(ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò
with parameter set Œò. Equivalently, we suppose that the distribution assumption P, appearing in Defini-
tion 8.1.2, may be represented as P = {‚ÑôŒ∏ : Œ∏ ‚ààŒò} .
In this notation, the parameter sets in Examples 8.1.3, 8.1.4, 8.1.8, and 8.1.9 are Œò = [0, 1],
Œò = {0, . . . , N}, Œò = [0, 1], and Œò = (0, ‚àû), respectively.
Remark 8.1.11. It is worthwhile mentioning that the parameter can be quite general;
for example, it can be a vector Œ∏ = (Œ∏1, . . . , Œ∏k), so that in fact there are k unknown pa-
rameters Œ∏j, combined into a single vector Œ∏. For instance, in Example 8.1.5, the unknown
parameters are Œ∏1, . . . , Œ∏6, thus, the parameter set is given by
Œò = {Œ∏ = (Œ∏1, . . . , Œ∏6) : Œ∏k ‚â•0 , Œ∏1 + ‚ãÖ‚ãÖ‚ãÖ+ Œ∏6 = 1} .
Let us present two further examples with slightly more complicated parameter sets.
Example 8.1.12. We are given an item of unknown length. It is measured by an instru-
ment of an unidentified precision. We assume that the instrument is unbiased, that is,
on average, it shows the correct value. In view of the central limit theorem, we may sup-
pose that the measurements are distributed according to a normal distribution ùí©(Œº, œÉ2).
Here Œº is the ‚Äúcorrect‚Äù length of the item, and œÉ > 0 reflects the precision of the measur-
ing instrument. A small œÉ > 0 says that the instrument is quite precise, while a large œÉ > 0
corresponds to an inaccurate instrument. Consequently, by the distribution assumption
the initial model is given as

8.1 Statistical models
‡±™
375
(‚Ñù, ‚Ñ¨(‚Ñù), ùí©(Œº, œÉ2))Œº‚àà‚Ñù, œÉ2>0 .
In order to determine Œº (and maybe also œÉ), we measure the item n times by the same
method. As a result, we obtain a random sample x = (x1, . . . , xn) ‚àà‚Ñùn. Thus, our model
describing this experiment is
(‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº, œÉ2)
‚äón)(Œº,œÉ2)‚àà‚Ñù√ó(0,‚àû) .
Because of eq. (6.9), the model may also be written as
(‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©( ‚ÉóŒº, œÉ2In))(Œº,œÉ2)‚àà‚Ñù√ó(0,‚àû)
with ‚ÉóŒº = (Œº, . . . , Œº) ‚àà‚Ñùn, and with diagonal matrix œÉ2In. The unknown parameter is
(Œº, œÉ2), taken from the parameter set ‚Ñù√ó (0, ‚àû).
Example 8.1.13. Suppose now we have two different items of lengths Œº1 and Œº2. We take
m measurements of the first item and n of the second. Thereby, we use different instru-
ments with maybe different degrees of precision. All measurements are taken indepen-
dently of each other. As a result, we get a vector (x, y) ‚àà‚Ñùm+n, where x = (x1, . . . , xm)
are the values of the first m measurements and y = (y1, . . . , yn) those of the other n. As
before we assume that the xis are distributed according to ùí©(Œº1, œÉ2
1), and the yjs accord-
ing to ùí©(Œº2, œÉ2
2). We neither know Œº1 and Œº2 nor œÉ2
1 and œÉ2
2. Thus, the sample space is
‚Ñùm+n and the vectors (x, y) are distributed according to ùí©(( ‚ÉóŒº1, ‚ÉóŒº2), RœÉ2
1 ,œÉ2
2 ) with diagonal
matrix RœÉ2
1 ,œÉ2
2 having œÉ2
1 as its first m entries and œÉ2
2 as the remaining n.
Note that by Definition 1.9.5,
ùí©(( ‚ÉóŒº1, ‚ÉóŒº2), RœÉ2
1 ,œÉ2
2 ) = ùí©(Œº1, œÉ2
1)
‚äóm ‚äóùí©(Œº2, œÉ2
2)
‚äón .
This is valid because, if A ‚àà‚Ñ¨(‚Ñùm) and B ‚àà‚Ñ¨(‚Ñùn), then it follows that
ùí©(( ‚ÉóŒº1, ‚ÉóŒº2), RœÉ2
1 ,œÉ2
2 )(A √ó B) = ùí©(Œº1, œÉ2
1)
‚äóm(A) ‚ãÖùí©(Œº2, œÉ2
2)
‚äón(B) .
The parameter set in this example is given as ‚Ñù2 √ó (0, ‚àû)2, hence the statistical model
may be written as
(‚Ñùm+n, ‚Ñ¨(‚Ñùm+n), ùí©(Œº1, œÉ2
1)
‚äóm ‚äóùí©(Œº2, œÉ2
2)
‚äón)(Œº1,Œº2,œÉ2
1 ,œÉ2
2)‚àà‚Ñù2√ó(0,‚àû)2 .
Summary: A parametric statistical model is a statistical model represented as (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò . Equivalently,
the distribution assumption may be written as P = {‚ÑôŒ∏ : Œ∏ ‚ààŒò} with a suitable index set Œò. A typical example
is ùí≥= {0, . . . , n}, ‚Ñ±= ùí´(ùí≥), Œò = [0, 1], and ‚ÑôŒ∏ = Bn,Œ∏.

376
‡±™
8 Mathematical statistics
8.2 Statistical hypothesis testing
8.2.1 Hypotheses and tests
We start with a parametric statistical model (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò. Suppose the parameter set Œò
is split up into disjoint subsets Œò0 and Œò1. The aim of a test is to decide, on the basis of
the observed sample, whether or not the ‚Äútrue‚Äù parameter Œ∏ belongs to Œò0 or to Œò1.
Let us explain the problem with two examples.
Example 8.2.1. Consider once more the situation described in Example 8.1.4. Assume
there exists a critical value M0 ‚â§N such that the buyer accepts the delivery if the num-
ber M of defective machines satisfies M ‚â§M0. Otherwise, if M > M0, the buyer re-
jects it and sends the machines back to the trader. In this example the parameter set is
Œò = {0, . . . , N}. Letting Œò0 = {0, . . . , M0} and Œò1 = {M0 + 1, . . . , N}, the question about
acceptance or rejection of the delivery is equivalent to whether M ‚ààŒò0 or M ‚ààŒò1. As-
sume now the buyer checked n of the N machines and found m defective. On the basis of
this observation, the buyer has to decide about acceptance or rejection, or, equivalently,
about M ‚ààŒò0 or M ‚ààŒò1.
Example 8.2.2. Let us consider once more Example 8.1.13. There we had two measuring
instruments, both being unbiased. Consequently, the expected values Œº1 and Œº2 are the
correct lengths of the two items. The parameter set was Œò = ‚Ñù2 √ó (0, ‚àû)2. Suppose we
conjecture that both items are of equal length, that is, we conjecture Œº1 = Œº2. Letting
Œò0 := {(Œº, Œº, œÉ2
1, œÉ2
2) : Œº ‚àà‚Ñù, œÉ2
1, œÉ2
2 > 0}
and Œò1 = Œò \ Œò0, to prove or disprove the conjecture, we have to check whether
(Œº1, Œº2, œÉ2
1, œÉ2
2) belongs to Œò0 or Œò1.
On the other hand, if we want to know whether or not the first item is smaller than
the second, then we have to choose
Œò0 := {(Œº1, Œº2, œÉ2
1, œÉ2
2) : ‚àí‚àû< Œº1 ‚â§Œº2 < ‚àû, œÉ2
1, œÉ2
2 > 0}
and to check whether or not (Œº1, Œº2, œÉ2
1, œÉ2
2) belongs to Œò0.
An exact mathematical formulation of the previous problems is as follows.
Definition 8.2.3. Let (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò be a parametric statistical model and suppose Œò = Œò0 ‚à™Œò1 with
Œò0 ‚à©Œò1 = 0.
Then the hypothesis or, more precisely, null hypothesis ‚Ñç0 says that for the ‚Äúcorrect‚Äù Œ∏ ‚ààŒò one
has Œ∏ ‚ààŒò0. This is expressed by writing ‚Ñç0 : Œ∏ ‚ààŒò0 .
The alternative hypothesis ‚Ñç1 says Œ∏ ‚ààŒò1. Thus, ‚Ñç1 : Œ∏ ‚ààŒò1, and we have to check
‚Ñç0 : Œ∏ ‚ààŒò0
against
‚Ñç1 : Œ∏ ‚ààŒò1 .

8.2 Statistical hypothesis testing
‡±™
377
After the hypothesis is set, one executes a statistical experiment. Here the order is im-
portant: first, one has to set the hypothesis, then test it, not vice versa. If the hypothesis
is chosen on the basis of the observed results, then, of course, the sample will confirm
it.
Say the result of the experiment is some sample x ‚ààùí≥. One of the fundamental
problems in Mathematical Statistics is to decide, on the basis of the observed sample,
about acceptance or rejection of ‚Ñç0. The mathematical formulation of the problem is as
follows.
Definition 8.2.4. A (hypothesis) test T for checking ‚Ñç0 (against ‚Ñç1) is a disjoint partition T = (ùí≥0, ùí≥1)
of the sample space ùí≥. The set ùí≥0 is called the region of acceptance while ùí≥1 is said to be the critical
region, sometimes also called critical section or region of rejection. By mathematical reasoning, we
have to assume ùí≥0 ‚àà‚Ñ±, which of course implies ùí≥1 ‚àà‚Ñ±as well.
Remark 8.2.5. A hypothesis test T = (ùí≥0, ùí≥1) operates as follows: if the statistical ex-
periment leads to a sample x ‚ààùí≥1, then we reject ‚Ñç0. But, if we get an x ‚ààùí≥0, then
this does not contradict the hypothesis, and for now we may furthermore work with
it.
Important comment: If we observe an x ‚ààùí≥0, then this does not say that ‚Ñç0 is correct.
It only asserts that we failed to reject it or that there is a lack of evidence against it.
Let us illustrate the procedure with Example 8.2.1.
Example 8.2.6. By the choice of Œò0 and Œò1, the hypothesis ‚Ñç0 is given by
‚Ñç0 : 0 ‚â§M ‚â§M0 ,
hence ‚Ñç1 : M0 < M ‚â§N .
To test ‚Ñç0 against ‚Ñç1, the sample space ùí≥= {0, . . . , n} is split up into the two regions
ùí≥0 := {0, . . . , m0} and ùí≥1 := {m0 + 1, . . . , n} with some (for now) arbitrary number m0 ‚àà
{0, . . . , n}. If among the checked n machines m are defective with some m > m0, then
m ‚ààùí≥1, hence one rejects ‚Ñç0. In this case the buyer refuses to take the delivery and
sends it back to the trader. On the other hand, if m ‚â§m0, then m ‚ààùí≥0, which does not
contradict ‚Ñç0, and the buyer will accept the delivery and pay for it. Of course, the key
question is how to choose the value m0 in a proper way.
Remark 8.2.7. Sometimes tests are also defined as mappings œÜ : ùí≥‚Üí{0, 1}. The link
between these two approaches is immediately clear. Starting with œÜ, the hypothesis test
T = (ùí≥0, ùí≥1) is constructed by ùí≥0 = {x ‚ààùí≥: œÜ(x) = 0} and ùí≥1 = {x ‚ààùí≥: œÜ(x) = 1}.
Conversely, if T = (ùí≥0, ùí≥1) is a given test, then set œÜ(x) = 0 if x ‚ààùí≥0 and œÜ(x) = 1
for x ‚ààùí≥1. The advantage of this approach is that it allows us to define the so-called
randomized tests. Here œÜ : ùí≥‚Üí[0, 1]. Then, as before, ùí≥0 = {x ‚ààùí≥: œÜ(x) = 0} and
ùí≥1 = {x ‚ààùí≥: œÜ(x) = 1}. If 0 < œÜ(x) < 1, then
œÜ(x) = ‚Ñô{reject ‚Ñç0 if x is observed} .

378
‡±™
8 Mathematical statistics
That is, for certain observations x ‚ààùí≥, an additional random experiment (e. g., tossing
a coin) decides whether we accept or reject ‚Ñç0. Randomized tests are useful in the case
of finite or countably infinite sample spaces.
When applying a test T = (ùí≥0, ùí≥1) to check the null hypothesis ‚Ñç0 : Œ∏ ‚ààŒò0, two
different types of errors may occur.
Definition 8.2.8. An error of the first kind or type I error occurs if ‚Ñç0 is true but one observes a sample
x ‚ààùí≥1, hence rejects ‚Ñç0.
Type I error = incorrect rejection of a true null hypothesis
In other words, a type I error happens if the ‚Äútrue‚Äù Œ∏ is in Œò0, but we observe an x ‚ààùí≥1.
Definition 8.2.9. An error of the second kind or type II error occurs if ‚Ñç0 is false, but the observed
sample lies in ùí≥0, hence we do not reject the false hypothesis ‚Ñç0.
Type II error = failure to reject a false null hypothesis
Consequently, a type II error occurs if the ‚Äútrue‚Äù Œ∏ is in Œò1, but the observed sample is
an element of the region of acceptance ùí≥0.
Example 8.2.10. In the context of Example 8.2.6, a type I error occurs if the delivery was
good, but among the checked machines more than m0 were defective, so that the buyer
rejected the delivery. Since the trader was not able to sell a proper delivery, this error is
also called the risk of the trader.
On the other hand, a type II error occurs if the delivery is not in good order, but
among the checked machines only a few were defective (at most m0). Thus, the buyer
accepted the bad delivery and paid for it. Therefore, this type of error is also called the
risk of the buyer.
Summary: Let (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò be a parametric statistical model and suppose Œò = Œò0 ‚à™Œò1 with Œò0 ‚à©Œò1 = 0.
Then the hypotheses ‚Ñç0 and ‚Ñç1 are
‚Ñç0 : Œ∏ ‚ààŒò0
against
‚Ñç1 : Œ∏ ‚ààŒò1 .
A (hypothesis) test T for checking ‚Ñç0 (against ‚Ñç1) is a disjoint partition T = (ùí≥0, ùí≥1) of the sample
space ùí≥. The set ùí≥0 is called the region of acceptance while ùí≥1 is said to be the critical region. If the
observed sample x ‚ààùí≥1, one rejects ‚Ñç0. If x ‚ààùí≥0, we cannot reject ‚Ñç0 and have to work with it further-
more.
Type I error = incorrect rejection of a true null hypothesis
‚áî
x ‚ààùí≥1 and Œ∏ ‚ààŒò0,
Type II error = failure to reject a false null hypothesis
‚áî
x ‚ààùí≥0 and Œ∏ ‚ààŒò1.

8.2 Statistical hypothesis testing
‡±™
379
Verbally said:1 A type I error is the mistaken rejection of a null hypothesis that is actually true; for example,
‚Äúdue to a false witness report, an innocent person is convicted.‚Äù A type II error is the failure to reject a null
hypothesis that is actually false; for example, ‚Äúdue to the lack of evidence, a guilty person is not convicted.‚Äù
8.2.2 Power function and significance tests
The power of a test is described by its power function defined as follows.
Definition 8.2.11. Let T = (ùí≥0, ùí≥1) be a test for ‚Ñç0 : Œ∏ ‚ààŒò0 against ‚Ñç1 : Œ∏ ‚ààŒò1. The function Œ≤T from
Œò to [0, 1] defined as
Œ≤T(Œ∏) := ‚ÑôŒ∏(ùí≥1)
is called the power function of the test T.
Remark 8.2.12. If Œ∏ ‚ààŒò0, that is, if ‚Ñç0 is true, then Œ≤T(Œ∏) = ‚ÑôŒ∏(ùí≥1) is the probability
that ùí≥1 occurs or, equivalently, that a type I error happens.
On the contrary, if Œ∏ ‚ààŒò1, that is, ‚Ñç0 is false, then 1‚àíŒ≤T(Œ∏) = ‚ÑôŒ∏(ùí≥0) is the probability
that ùí≥0 occurs or, equivalently, that a type II error appears.
Thus, a ‚Äúgood‚Äù test should satisfy the following conditions: the power function Œ≤T
attains small values on Œò0 and/or 1 ‚àíŒ≤T has small values on Œò1. Then the probabilities
for the occurrence of type I and/or type II errors are not too big.2
Example 8.2.13. What is the power function of the test presented in Example 8.2.6? Re-
call that Œò = {0, . . . , N} and ùí≥1 = {m0 + 1, . . . , n}. Hence, Œ≤T maps {0, . . . , N} to [0, 1] in the
following way:
Œ≤T(M) = HN,M,n(ùí≥1) =
n
‚àë
m=m0+1
(M
m)(N‚àíM
n‚àím)
(N
n)
.
(8.1)
If the hypotheses are
‚Ñç0 : 0 ‚â§M ‚â§M0
against
‚Ñç1 : M0 < M ‚â§N ,
then the maximal probability for a type I error is given by
max
0‚â§M‚â§M0
Œ≤T(M) = max
0‚â§M‚â§M0
n
‚àë
m=m0+1
(M
m)(N‚àíM
n‚àím)
(N
n)
,
(8.2)
1 See [Fsh71].
2 In the literature, the power function is sometimes defined in a slightly different way. If Œ∏ ‚ààŒò0, then it
is as in our Definition 8.2.11 while for Œ∏ ‚ààŒò1 one defines it as 1 ‚àíŒ≤T(Œ∏). Moreover, for 1 ‚àíŒ≤T one finds
the notion of the operation characteristics or oc-function.

380
‡±™
8 Mathematical statistics
while the maximal probability for a type II error equals
max
M0<M‚â§N(1 ‚àíŒ≤T(M)) =
max
M0<M‚â§N
m0
‚àë
m=0
(M
m)(N‚àíM
n‚àím)
(N
n)
.
(8.3)
Let us give a concrete example for the power function in eq. (8.1). Suppose the trader
submits a delivery of 30 machines. Hence, N = 30 and Œò = {0, . . . , 30}. The buyer chooses
randomly 10 machines and tests them. That is, n = 10. Assume, the test is T = (ùí≥0, ùí≥1)
where ùí≥0 = {0, 1, 2, 3, 4}, hence ùí≥1 = {5, 6, 7, 8, 9, 10}. Thus, m0 = 4. In other words, if
there are at most 4 defective machines among the tested 10, then the buyer accepts the
delivery. Otherwise, he rejects it.
Then the power function of the test T equals
Œ≤T(M) =
10
‚àë
m=5
(M
m)(30‚àíM
10‚àím)
(30
10)
,
M = 0, . . . , 30 .
(8.4)
Note that
Œ≤T(0) = ‚ãÖ‚ãÖ‚ãÖ= Œ≤T(4) = 0
while Œ≤T(25) = ‚ãÖ‚ãÖ‚ãÖ= Œ≤T(30) = 1 .
Why is this so? First, if there are only 4 or less defective machines among the delivered, it
is impossible to observe 5 or more defective machines among the 10 chosen. On the other
hand, if there are 25 or more defective machines among 30, then at most 5 machines are
nondefective. Hence, among the 10 chosen there are at least 5 defective.
Some other interesting values of Œ≤T are (see also Figure 8.1)
M
5
6
7
8
9
10
11
Œ≤T(M)
0.001768
0.008842
0.02564
0.05632
0.1037
0.1687
0.2500
These values of Œ≤T tell us the following: if, for example, there are 11 defective ma-
chines in the delivery, then there is a 25 % chance to observe in the sample 5 or more
defective. Equivalently, the probability to observe at most 4 defective machines is still
75 %. Thus, in this case the likelihood of a type II error is rather big. The situation changes
drastically for larger M. If, for example, the number of defective machines is 18, among
100 trials the sample will on average 88.2 times contain 5 or more defective items.
Remark 8.2.14. Formulas (8.2) and (8.3) already illustrate the dilemma of hypothesis
testing. To minimize the type I error, one has to choose m0 as large as possible. But
increasing m0 enlarges the type II error.
This dilemma occurs always in the theory of hypothesis testing. In order to mini-
mize the probability of a type I error, the critical region ùí≥1 has to be chosen as small as
possible. But making ùí≥1 smaller enlarges ùí≥0, hence the probability for the occurrence

8.2 Statistical hypothesis testing
‡±™
381
Figure 8.1: The power function Œ≤T in eq. (8.4).
of a type II error increases. In the extreme case, if ùí≥1 = 0, hence ùí≥0 = ùí≥, then a type I
error cannot occur at all. In the context of Example 8.2.6 that means the buyer accepts
all deliveries and the trader takes no risk.
On the other hand, to minimize the occurrence of a type II error, the region of ac-
ceptance ùí≥0 has to be as small as possible. In the extreme case, if we choose ùí≥0 = 0, then
a type II error cannot occur because we always reject the hypothesis. In the context of
Example 8.2.6, this says the buyer rejects all deliveries. In this way he avoids buying any
delivery of bad quality, but he also never gets a proper one. Thus the buyer takes no risk.
It is pretty clear that both extreme cases presented above are absurd. Therefore, one
has to find a suitable compromise. The approach for such a compromise is as follows: in
the first step, one chooses tests where the probability of a type I error is bounded from
above. And in the second step, among all these tests satisfying this bound, one takes that
which minimizes the probability of a type II error. More precisely, we will investigate
tests satisfying the following condition.
Definition 8.2.15. Suppose we are given a number Œ± ‚àà(0, 1), the so-called significance level. A test
T = (ùí≥0, ùí≥1) for testing the hypothesis ‚Ñç0 : Œ∏ ‚ààŒò0 against ‚Ñç1 : Œ∏ ‚ààŒò1 is said to be an Œ±-significance
test (or simply Œ±-test), provided the probability for the occurrence of a type I error is bounded by Œ±. That
is, the test has to satisfy
sup
Œ∏‚ààŒò0
Œ≤T(Œ∏) = sup
Œ∏‚ààŒò0
‚ÑôŒ∏(ùí≥1) ‚â§Œ± .
Interpretation: The significance level Œ± is assumed to be small. Typical choices are Œ± =
0.1 or Œ± = 0.01. Let T be an Œ±-significance test and assume that ‚Ñç0 is true. If we observe
now a sample in the critical region ùí≥1, then an event occurred with probability less than
or equal to Œ±, that is, a very unlikely event has been observed. Therefore, we can be very

382
‡±™
8 Mathematical statistics
sure that this could not have happened provided ‚Ñç0 had been true, and we reject this
hypothesis. The probability that we made a mistake is less than or equal to the chosen
Œ± > 0, hence very small.
Recall that Œ±-significance tests admit no bound for the probability of a type II error.
Therefore, we look for those Œ±-significance tests that minimize the probability for a type
II error.
Definition 8.2.16. Let T1 and T2 be two Œ±-significance tests for checking ‚Ñç0 against ‚Ñç1. If their power
functions satisfy
Œ≤T1(Œ∏) ‚â•Œ≤T2(Œ∏) ,
Œ∏ ‚ààŒò1 ,
then we say that T1 is (uniformly) more powerful than T2.
A (uniformly) most powerful Œ±-test T is that which is more powerful than all other Œ±-tests.
Remark 8.2.17. Note that Œ≤T1(Œ∏) ‚â•Œ≤T2(Œ∏) implies 1‚àíŒ≤T1(Œ∏) ‚â§1‚àíŒ≤T2(Œ∏), hence if T1 is more
powerful than T2, then, according to Remark 8.2.12, the probability for the occurrence
of a type II error is smaller for T1 than it is for T2. Therefore, a most powerful Œ±-test is
that which minimizes the probability of occurrence of a type II error.
Remark 8.2.18. The question about existence and uniqueness of most powerful Œ±-tests
is treated in the Neyman‚ÄìPearson lemma and its consequences. We will not discuss that
problem here; instead, we will construct most powerful tests in concrete situations. See
[CB02, Chapter 8.3.2] for a detailed discussion of the Neyman‚ÄìPearson lemma and its
consequences.
We start with the construction of such tests in the hypergeometric case. Here we
have the following.
Proposition 8.2.19. If the statistical model equals (ùí≥, ùí´(ùí≥), HN,M,n)M=0,...,N with ùí≥=
{0, . . . , n}, then the most powerful Œ±-test for testing M ‚â§M0 against M > M0 is given
by T = (ùí≥0, ùí≥1), where ùí≥0 = {0, . . . , m0}, and m0 is defined by
m0 := max{k ‚â§n :
n
‚àë
m=k
(M0
m )(N‚àíM0
n‚àím )
(N
n)
> Œ±}
= min{k ‚â§n :
n
‚àë
m=k+1
(M0
m )(N‚àíM0
n‚àím )
(N
n)
‚â§Œ±} .
Proof. The proof of Proposition 8.2.19 needs the following lemma.
Lemma 8.2.20. The power function, defined by eq. (8.1), is a nondecreasing function on
the set {0, . . . , N}.
Proof. Suppose we get a delivery of N machines containing M defective. Now there are
not only defective machines within the delivery, but also
ÃÉM ‚àíM false ones for some
ÃÉM ‚â•M. We take a sample of size n and test these machines. Let X be the number of

8.2 Statistical hypothesis testing
‡±™
383
defective machines and let
ÃÉX be the number of machines that are either defective or
false. Of course, we have X ‚â§
ÃÉX implying ‚Ñô(X > m0) ‚â§‚Ñô( ÃÉX > m0). Note that X is HN,M,n-
distributed while ÃÉX is distributed according to HN, ÃÉM,n. These observations lead to
Œ≤T(M) = HN,M,n({m0 + 1, . . . , n}) = ‚Ñô{X > m0} ‚â§‚Ñô{ ÃÉX > m0}
= HN, ÃÉM,n({m0 + 1, . . . , n}) = Œ≤T( ÃÉM) .
This being true for all M ‚â§
ÃÉM proves that Œ≤T is nondecreasing.
Let us come back to the proof of Proposition 8.2.19. Set ùí≥0 := {0, . . . , m0}, thus ùí≥1 =
{m0 + 1, . . . , n} for some (at the moment arbitrary) m0 ‚â§n. Because of Lemma 8.2.20, the
test T = (ùí≥0, ùí≥1) is an Œ±-significance test if and only if it satisfies
n
‚àë
m=m0+1
(M0
m )(N‚àíM0
n‚àím )
(N
n)
= HN,M0,n(ùí≥1) = sup
M‚â§M0
HN,M,n(ùí≥1) ‚â§Œ± .
To minimize the probability for the occurrence of a type II error, we have to choose ùí≥1
as large as possible or, equivalently, m0 as small as possible, that is, if we replace m0 by
m0 ‚àí1, then the new test is no longer an Œ±-test. Thus, in order that T is an Œ±-test that
minimizes the probability for a type II error, the number m0 has to be chosen such that
n
‚àë
m=m0+1
(M0
m )(N‚àíM0
n‚àím )
(N
n)
‚â§Œ±
and
n
‚àë
m=m0
(M0
m )(N‚àíM0
n‚àím )
(N
n)
> Œ± .
This completes the proof.
Example 8.2.21. A buyer gets a delivery of 100 machines. In the case that there are
strictly more than 10 defective machines in the delivery, he will reject it. Thus, his hy-
pothesis is ‚Ñç0 : M ‚â§10. In order to test ‚Ñç0, he chooses 15 machines and checks them.
Let m be the number of defective machines among the checked. For which m does he
reject the delivery with a significance level Œ± = 0.01?
Answer: We have N = 100, M0 = 10, and n = 15. Since Œ± = 0.01, from
15
‚àë
m=5
(10
m)( 90
15‚àím)
(100
15 )
= 0.0063 . . . < Œ±
and
15
‚àë
m=4
(10
m)( 90
15‚àím)
(100
15 )
= 0.04 . . . > Œ± ,
it follows that the optimal choice is m0 = 4. Consequently, we have ùí≥0 = {0, . . . , 4}, thus,
ùí≥1 = {5, . . . , 15}. If there are 5 or even more defective machines among the tested 15,
then the buyer should reject the delivery. The probability that his decision is wrong is
less than or equal to 0.01.
What can be said about the probability for a type II error? For this test, we have
Œ≤T(M) =
15
‚àë
m=5
(M
m)(100‚àíM
15‚àím )
(100
15 )
,
(8.5)

384
‡±™
8 Mathematical statistics
hence
1 ‚àíŒ≤T(M) =
4
‚àë
m=0
(M
m)(100‚àíM
15‚àím )
(100
15 )
.
Since Œ≤T is nondecreasing, 1 ‚àíŒ≤T is nonincreasing, and the probability for a type II
error becomes maximal for M = 11. Recall that Œò0 = {0, . . . , 10} and, therefore, Œò1 =
{11, . . . , 100}. Thus, an upper bound for the probability of a type II error is given by
1 ‚àíŒ≤T(M) ‚â§1 ‚àíŒ≤T(11) =
4
‚àë
m=0
(11
m)( 89
15‚àím)
(100
15 )
= 0.989471 ,
M = 11, . . . , 100 .
This tells us that even in the case of most powerful tests the likelihood for a type II error
may be quite large. Even if the number of defective machines is big, this error may occur
with higher probability. For example, we have 1 ‚àíŒ≤T(20) = 0.853089 and 1 ‚àíŒ≤T(40) =
0.197057. See also Figure 8.2.
Figure 8.2: The power function Œ≤T defined by eq. (8.5).
If one is willing to take a greater risk and chooses Œ± = 0.1, then, since
15
‚àë
m=4
(10
m)( 90
15‚àím)
(100
15 )
= 0.0063 . . . < Œ±
and
15
‚àë
m=3
(10
m)( 90
15‚àím)
(100
15 )
= 0.1705 . . . > Œ±,
one may take ùí≥0 = {0, 1, 2, 3} as the region of acceptance. Thus, in this case the buyer will
reject the delivery if there are 4 defective machines among the 15 tested. The probability
that this is a wrong decision is less than 0.1, but greater than 0.01.
Remark 8.2.22 (Important!). An Œ±-significance test provides us with quite precise infor-
mation when rejecting the hypothesis ‚Ñç0. In contrast, when we observe a sample x ‚ààùí≥0,

8.2 Statistical hypothesis testing
‡±™
385
the only information we get is that we failed to reject ‚Ñç0, thus, we must continue to re-
gard it as true. Consequently, whenever fixing the null hypothesis, we have to fix it in a
way that either a type I error has the most serious consequences or that we can attain
the most information by rejecting ‚Ñç0. Let us explain this with two examples.
Example 8.2.23. A certain type of food sometimes contains a special kind of poison.
Suppose there are Œº milligrams of poison in one kilogram of the food. If Œº > Œº0, then
eating this becomes dangerous while for Œº ‚â§Œº0 it is unproblematic. How do we success-
fully choose the hypothesis when testing some sample of the food? We could take either
‚Ñç0 : Œº > Œº0 or ‚Ñç0 : Œº ‚â§Œº0. Which is the right choice?
Answer: The correct choice is ‚Ñç0 : Œº > Œº0. Why? If we reject ‚Ñç0, then we can be very
sure that the food is not poisoned and may be eaten. The probability that someone will
be poisoned is less than Œ±. A type II error occurs if the food is harmless, but we discard
it because our test tells us that it is poisoned. That results in a loss for the company that
produced it, but no one will suffer from poisoning. If we were to choose ‚Ñç0 : Œº ‚â§Œº0,
then a type II error would occur if ‚Ñç0 is false, that is, the food is poisoned, but our test
says that it is eatable. Of course, this error is much more serious, and we have no control
in regards to its probability.
Example 8.2.24. Suppose the height of 18-year-old males in the US is normally dis-
tributed with expected value Œº and variance œÉ2 > 0. We want to know whether the
average height is above or below 6 feet. There is strong evidence that we will have
Œº ‚â§6, but we cannot prove this. To do so, we execute a statistical experiment and
choose randomly n males of age 18 and measure their height. Which hypothesis should
be checked? If we take ‚Ñç0 : Œº ‚â§6, then it is very likely that our experiment will lead
to a result that does not contradict this hypothesis, resulting in a small amount of in-
formation gained. But, if we work with the hypothesis ‚Ñç0 : Œº > 6, then a rejection of
this hypothesis tells us that ‚Ñç0 is very likely wrong, and we may say the conjecture is
true with high probability, namely that we have Œº ‚â§6. Here the probability that our
conclusion is wrong is very small.
Summary: The power function of a test T = (ùí≥0, ùí≥1) for ‚Ñç0 : Œ∏ ‚ààŒò0 against ‚Ñç1 : Œ∏ ‚ààŒò1 is defined by
Œ≤T(Œ∏) = ‚ÑôŒ∏(ùí≥1) ,
Œ∏ ‚ààŒò .
If T is a ‚Äúgood‚Äù test, then the power function Œ≤T should be small on Œò0 and near to one on Œò1. Given Œ± > 0,
an Œ±-significance test T satisfies
sup
Œ∏‚ààŒò0
Œ≤T(Œ∏) = sup
Œ∏‚ààŒò0
‚ÑôŒ∏(ùí≥1) ‚â§Œ± .
That is, if T is an Œ±-test, the probability of a type I error is bounded by Œ±.

386
‡±™
8 Mathematical statistics
8.3 Tests for binomial distributed populations
Because of their importance, we present tests for binomial distributed populations in a
separate section. The starting point is the problem described in Examples 8.1.3 and 8.1.8.
In a single experiment, we may observe either ‚Äú0‚Äù or ‚Äú1,‚Äù but we do not know the prob-
abilities for the occurrence of these events. To obtain some information about the un-
known probabilities, we execute n independent trials and record how often ‚Äú1‚Äù occurs.
This number is Bn,Œ∏-distributed for some 0 ‚â§Œ∏ ‚â§1. Hence, the describing statistical
model is given by
(ùí≥, ùí´(ùí≥), Bn,Œ∏)Œ∏‚àà[0,1]
where ùí≥= {0, . . . , n} .
(8.6)
Two-sided tests: We want to check whether the unknown parameter Œ∏ satisfies Œ∏ = Œ∏0
or Œ∏
Ã∏= Œ∏0 for some given Œ∏0 ‚àà[0, 1]. Thus, Œò0 = {Œ∏0} and Œò1 = [0, 1] \ {Œ∏0}. In other words,
the null and the alternative hypothesis are
‚Ñç0 : Œ∏ = Œ∏0
and
‚Ñç1 : Œ∏
Ã∏= Œ∏0,
respectively.
To construct a suitable Œ±-significance test for checking ‚Ñç0, we introduce two num-
bers n0 and n1 as follows. Note that these numbers depend on Œ∏0 and, of course, also
on Œ±. The numbers are defined by
n0 := min{k ‚â§n :
k
‚àë
j=0
(n
j )Œ∏j
0(1 ‚àíŒ∏0)n‚àíj > Œ±/2}
= max{k ‚â§n :
k‚àí1
‚àë
j=0
(n
j )Œ∏j
0(1 ‚àíŒ∏0)n‚àíj ‚â§Œ±/2}
(8.7)
and
n1 := max{k ‚â§n :
n
‚àë
j=k
(n
j )Œ∏j
0(1 ‚àíŒ∏0)n‚àíj > Œ±/2}
= min{k ‚â§n :
n
‚àë
j=k+1
(n
j )Œ∏j
0(1 ‚àíŒ∏0)n‚àíj ‚â§Œ±/2} .
(8.8)
Proposition 8.3.1. Consider the statistical model (8.6) and let 0 < Œ± < 1 be a significance
level. The hypothesis test T = (ùí≥0, ùí≥1) with
ùí≥0 := {n0, n0 + 1, . . . , n1 ‚àí1, n1}
and
ùí≥1 = {0, . . . , n0 ‚àí1} ‚à™{n1 + 1, . . . , n}
(8.9)
is an Œ±-significance test to check ‚Ñç0 : Œ∏ = Œ∏0 against ‚Ñç1 : Œ∏
Ã∏= Œ∏0. Here n0 and n1 are
defined as in eqs. (8.7) and (8.8).

8.3 Tests for binomial distributed populations
‡±™
387
Proof. Since Œò0 consists only of the point {Œ∏0}, an arbitrary test T = (ùí≥0, ùí≥1) is an
Œ±-significance test if and only if Bn,Œ∏0(ùí≥1) ‚â§Œ±. Now let T be as in the formulation of
the proposition. By the definition of the numbers n0 and n1, we obtain
Bn,Œ∏0(ùí≥1) =
n0‚àí1
‚àë
j=0
(n
j )Œ∏j
0(1 ‚àíŒ∏0)n‚àíj +
n
‚àë
j=n1+1
(n
j )Œ∏j
0(1 ‚àíŒ∏0)n‚àíj ‚â§Œ±
2 + Œ±
2 = Œ± ,
that is, as claimed, the test T := (ùí≥0, ùí≥1) is an Œ±-significance test.
Remark 8.3.2. In this test the critical region ùí≥1 consists of two parts or tails. Therefore,
this type of hypothesis test is called a two-sided test.
Remark 8.3.3. By the choice of n0 and n1, the regions ùí≥0 and ùí≥1 in eq. (8.9) are optimal
in the following sense: If ÃÉT = ( ÃÉ
ùí≥0,
ÃÉ
ùí≥1) with
ÃÉ
ùí≥0 := {n0 + 1, . . . , n1 ‚àí1}
and
ÃÉ
ùí≥1 = {0, . . . , n0} ‚à™{n1, . . . , n} ,
then
Bn,Œ∏0( ÃÉ
ùí≥1) =
n0
‚àë
j=0
(n
j )Œ∏j
0(1 ‚àíŒ∏0)n‚àíj +
n
‚àë
j=n1
(n
j )Œ∏j
0(1 ‚àíŒ∏0)n‚àíj > Œ±
2 + Œ±
2 = Œ± .
Hence,
ÃÉT is no longer an Œ±-significance test. But note that we cannot exclude that the
tests with either
ÃÉ
ùí≥0 := {n0, . . . , n1 ‚àí1}
or
ÃÉ
ùí≥0 := {n0 + 1, . . . , n1}
are still Œ±-significance tests.
Example 8.3.4. In an urn there is an unknown number of white and black balls. Let
Œ∏ ‚àà[0, 1] be the proportion of white balls. We conjecture that there are as many white as
black balls in the urn. That is, the null hypothesis is ‚Ñç0 : Œ∏ = 0.5. To test this hypothesis,
we choose one after another 100 balls with replacement. In order to determine n0 and
n1 in this situation, let œÜ be defined as
œÜ(k) :=
k
‚àë
j=0
(100
j ) ‚ãÖ(1
2)
100
= B100,0.5({0, . . . , k}) .
Numerical calculations give
œÜ(36) = 0.00331856,
œÜ(37) = 0.00601649,
œÜ(38) = 0.0104894,
œÜ(39) = 0.0176001,
œÜ(40) = 0.028444,
œÜ(41) = 0.044313,
œÜ(42) = 0.0666053,
œÜ(43) = 0.096674,
œÜ(44) = 0.135627,
œÜ(45) = 0.184101,
œÜ(46) = 0.242059,
œÜ(47) = 0.30865,
œÜ(48) = 0.382177,
œÜ(49) = 0.460205 .

388
‡±™
8 Mathematical statistics
If the significance level is chosen as Œ± = 0.1, we see that œÜ(41) ‚â§0.05, but œÜ(42) > 0.05.
Hence, by the definition of n0 in eq. (8.7), it follows that n0 = 42. Either by symmetry
or by similar calculations, for n1 defined in eq. (8.8), we get n1 = 58. Consequently, the
regions of acceptance and rejection are given by
ùí≥0 = {42, 43, . . . , 57, 58}
and
ùí≥1 = {0, . . . , 41} ‚à™{59, . . . , 100} .
For example, if we observe during 100 trials k white balls for some k < 42 or some k > 58,
then we may be quite sure that our null hypothesis is wrong, that is, the numbers of
white and black balls are significantly different. This assertion is 90 % sure. The power
function of this test (see Fig. 8.3) is given by
Œ≤T(Œ∏) =
41
‚àë
k=0
(100
k )Œ∏k(1 ‚àíŒ∏)100‚àík +
100
‚àë
k=59
(100
k )Œ∏k(1 ‚àíŒ∏)100‚àík ,
0 < Œ∏ < 1 .
(8.10)
Figure 8.3: The power function Œ≤T in eq. (8.10) with significance level Œ± = 0.1.
If we want to be more certain about the conclusion, we have to choose a smaller
significance level. For example, if we take Œ± = 0.01, the values of œÜ imply n0 = 37 and
n1 = 63, hence in this case we conclude that
ùí≥0 = {37, 38, . . . , 62, 63}
and
ùí≥1 = {0, . . . , 36} ‚à™{64, . . . , 100} .
Again we see that a smaller bound for the probability of a type I error leads to an en-
largement of ùí≥0, thus, to an increase of the chance for a type II error.

8.3 Tests for binomial distributed populations
‡±™
389
One-sided tests: Now the null hypothesis is ‚Ñç0 : Œ∏ ‚â§Œ∏0 for some Œ∏0 ‚àà[0, 1]. In the
context of Example 8.1.3, we claim that the proportion of white balls in the urn does not
exceed Œ∏0. For instance, if Œ∏0 = 1/2, then we want to test whether or not the number of
white balls is less than or equal to that of black.
Before we present a most powerful test for this situation, let us define a number n0
depending on Œ∏0 and on the significance level 0 < Œ± < 1, namely
n0 = max{k ‚â§n :
n
‚àë
j=k
(n
j )Œ∏j
0(1 ‚àíŒ∏0)n‚àíj > Œ±}
= min{k ‚â§n :
n
‚àë
j=k+1
(n
j )Œ∏j
0(1 ‚àíŒ∏0)n‚àíj ‚â§Œ±} .
(8.11)
Now we are in a position to state the most powerful one-sided Œ±-test for a binomial dis-
tributed population.
Proposition 8.3.5. Suppose ùí≥= {0, . . . , n}, and let (ùí≥, ùí´(ùí≥), Bn,Œ∏)Œ∏‚àà[0,1] be the statistical
model describing a binomial distributed population. Given 0 < Œ± < 1, define n0 by (8.11)
and set ùí≥0 = {0, . . . , n0}, hence ùí≥1 = {n0 + 1, . . . , n}. Then T = (ùí≥0, ùí≥1) is the most powerful
Œ±-test to check the null hypothesis ‚Ñç0 : Œ∏ ‚â§Œ∏0 against ‚Ñç1 : Œ∏ > Œ∏0.
Proof. Fixing an arbitrary 0 ‚â§m ‚â§n, we define the region of acceptance ùí≥0 of a test T
by ùí≥0 = {0, . . . , m}. Its power function is given by
Œ≤T(Œ∏) = Bn,Œ∏(ùí≥1) =
n
‚àë
j=m+1
(n
j )Œ∏j (1 ‚àíŒ∏)n‚àíj ,
0 ‚â§Œ∏ ‚â§1 .
(8.12)
To proceed further, we need the following lemma.
Lemma 8.3.6. The power function (8.12) is nondecreasing in [0, 1].
Proof. Suppose in an urn there are white, red, and black balls. Their proportions are Œ∏1,
Œ∏2 ‚àíŒ∏1 and 1 ‚àíŒ∏2 for some 0 ‚â§Œ∏1 ‚â§Œ∏2 ‚â§1. Choose n balls with replacement. Let X be the
number of chosen white balls, and Y the number of balls that were either white or red.
Then X is Bn,Œ∏1-distributed, while Y is distributed according to Bn,Œ∏2. Moreover, X ‚â§Y,
hence it follows that ‚Ñô(X > m) ‚â§‚Ñô(Y > m), which leads to
Œ≤T(Œ∏1) = Bn,Œ∏1({m + 1, . . . , n}) = ‚Ñô(X > m) ‚â§‚Ñô(Y > m)
= Bn,Œ∏2({m + 1, . . . , n}) = Œ≤T(Œ∏2) .
This being true for all Œ∏1 ‚â§Œ∏2 completes the proof of the lemma.

390
‡±™
8 Mathematical statistics
An application of Lemma 8.3.6 implies that the above test T is an Œ±-significance test
if and only if
n
‚àë
j=m+1
(n
j )Œ∏j
0(1 ‚àíŒ∏0)n‚àíj = Œ≤T(Œ∏0) = sup
Œ∏‚â§Œ∏0
Œ≤T(Œ∏) ‚â§Œ± .
In order to minimize the probability of a type II error, we have to choose ùí≥0 as small as
possible. That is, if we replace m by m ‚àí1, the modified test is no longer an Œ±-test. Thus,
the optimal choice is m = n0 where n0 is defined by eq. (8.11). This completes the proof
of Proposition 8.3.5.
Example 8.3.7. Let us come back to the problem investigated in Example 8.1.3. Our null
hypothesis is ‚Ñç0 : Œ∏ ‚â§1/2, that is, we claim that at most half of the balls are white. To test
‚Ñç0, we choose 100 balls and record their color. Let k be the number of observed white
balls. For which k must we reject ‚Ñç0 with a confidence of 90 %?
Answer: Since
100
‚àë
k=56
(100
k )2‚àí100 = 0.135627
and
100
‚àë
k=57
(100
k )2‚àí100 = 0.096674 ,
for Œ± = 0.1 the number n0 in eq. (8.11) equals n0 = 56. Consequently, the region of ac-
ceptance for the best 0.1-test is given by ùí≥0 = {0, . . . , 56}. Thus, whenever there are 57 or
more white balls among the chosen 100, the hypothesis has to be rejected. The probabil-
ity for a wrong decision is less than or equal to 0.1. The power function of this test T is
given by (compare Figure 8.4)
Œ≤T(Œ∏) =
100
‚àë
k=57
(100
k )Œ∏k(1 ‚àíŒ∏)100‚àík ,
0 < Œ∏ < 1 .
(8.13)
Figure 8.4: The power function Œ≤T in eq. (8.13) with significance level Œ± = 0.1.

8.4 Tests for normally distributed populations
‡±™
391
Making the significance level smaller, for example, taking Œ± = 0.01, since
100
‚àë
k=62
(100
k )2‚àí100 = 0.0104894
and
100
‚àë
k=63
(100
k )2‚àí100 = 0.00601649 ,
we obtain n0 = 62. Hence, if the number of white balls is 63 or larger, a rejection of ‚Ñç0
is 99 % sure.
Remark 8.3.8. Example 8.3.7 emphasizes once more the dilemma of hypothesis testing.
The price one pays for higher confidence when rejecting ‚Ñç0 is the increase of the like-
lihood of a type II error. For instance, replacing Œ± = 0.1 by Œ± = 0.01 in the previous
example leads to an enlargement of ùí≥0 from {0, . . . , 56} to {0, . . . , 62}. Thus, if we observe
60 white balls, we reject ‚Ñç0 in the former case, but we cannot reject it in the latter one.
This once more stresses the fact that an observation of an x ‚ààùí≥0 does not guarantee
that ‚Ñç0 is true. It only means that the observed sample does not allow us to reject the
hypothesis with high probability.
Summary: The model for testing binomial distributed populations is (ùí≥, ùí´(ùí≥), Bn,Œ∏)Œ∏‚àà[0,1] where
ùí≥= {0, . . . , n} . Given Œ∏0 ‚àà[0, 1], the hypotheses in the two-sided case are ‚Ñç0 : Œ∏ = Œ∏0 against ‚Ñç1 : Œ∏
Ã∏= Œ∏0 .
If
n0 = min{k ‚â§n : Bn,Œ∏0({0, . . . , k}) > Œ±
2 }
and
n1 = max{k ‚â§n : Bn,Œ∏0({k, . . . , n}) > Œ±
2 } ,
then ùí≥0 = {n0, . . . , n1} is the region of acceptance of an Œ±-test checking ‚Ñç0 against ‚Ñç1.
In the one-sided case ‚Ñç0 : Œ∏ ‚â§Œ∏0 against ‚Ñç1 : Œ∏ > Œ∏0 choose ùí≥0 = {0, . . . , n0} where now
n0 = max{k ‚â§n : Bn,Œ∏0({k, . . . , n}) > Œ±} .
8.4 Tests for normally distributed populations
In this section we always assume ùí≥= ‚Ñùn. That is, our samples are vectors x = (x1, . . . , xn)
with xj ‚àà‚Ñù. Given a sample x ‚àà‚Ñùn, we derive from it the following quantities that will
soon play a crucial role.
Definition 8.4.1. If x = (x1, . . . , xn) ‚àà‚Ñùn, then we set
ÃÑx := 1
n
n
‚àë
j=1
xj ,
s2
x :=
1
n ‚àí1
n
‚àë
j=1
(xj ‚àíÃÑx)2,
and
œÉ2
x := 1
n
n
‚àë
j=1
(xj ‚àíÃÑx)2 .
(8.14)
The number ÃÑx is said to be the sample mean of x, while s2
x and œÉ2
x are said to be the unbiased sample
variance and the (biased) sample variance of the vector x, respectively.

392
‡±™
8 Mathematical statistics
Analogously, if X = (X1, . . . , Xn) is an n-dimensional random vector,3 then we define the
corresponding expressions pointwise. For instance, we have
ÃÑX(œâ) := 1
n
n
‚àë
j=1
Xj(œâ)
and
s2
X(œâ) :=
1
n ‚àí1
n
‚àë
j=1
(Xj(œâ) ‚àíÃÑX(œâ))
2 .
8.4.1 Fisher‚Äôs theorem
We are going to prove important properties of normally distributed populations. They
turn out to be the basis for all hypothesis tests in the normally distributed case. The
starting point is a crucial lemma going back to Ronald Aylmer Fisher (1890‚Äì1962).
Lemma 8.4.2 (Fisher‚Äôs lemma). Let Y1, . . . , Yn be independent ùí©(0, 1)-distributed random
variables and let B = (Œ≤ij)n
i,j=1 be a unitary n √ó n matrix. The random variables Z1, . . . , Zn
are defined as
Zi :=
n
‚àë
j=1
Œ≤ijYj ,
1 ‚â§i ‚â§n .
They possess the following properties:
(i)
The variables Z1, . . . , Zn are also independent and ùí©(0, 1)-distributed.
(ii) For m < n, let the (random) quadratic form Q on ‚Ñùn be defined by
Q :=
n
‚àë
j=1
Y 2
j ‚àí
m
‚àë
i=1
Z2
i .
Then Q is independent of all Z1, . . . , Zm and, moreover, distributed according to œá2
n‚àím.
Proof. Assertion (i) was already proven in Proposition 6.1.19.
Let us verify (ii). The matrix B is unitary, thus it preserves the length of vectors in ‚Ñùn.
Applying this to Y = (Y1, . . . , Yn) and Z = BY gives
n
‚àë
i=1
Z2
i = |Z|2
2 = |BY|2
2 = |Y|2
2 =
n
‚àë
j=1
Y 2
j ,
which leads to
Q = Z2
m+1 + ‚ãÖ‚ãÖ‚ãÖ+ Z2
n .
(8.15)
3 To simplify the notation, now and later on, we denote random vectors by X, not by ‚ÉóX as we did before.
This should not lead to confusion. For example, ‚ÉóX does not look very nice.

8.4 Tests for normally distributed populations
‡±™
393
By virtue of (i), the random variables Z1, . . . , Zn are independent, hence by eq. (8.15) and
Remark 4.1.10 the quadratic form Q is independent of Z1, . . . , Zm.
Recall that Zm+1, . . . , Zn are independent ùí©(0, 1)-distributed. Thus, in view of
eq. (8.15), Proposition 4.6.10 implies that Q is œá2
n‚àím-distributed. Observe that Q is the
sum of n ‚àím squares.
Now we are in a position to state and prove one of the most important results in
Mathematical Statistics.
Proposition 8.4.3 (Fisher‚Äôs theorem). Suppose X1, . . . , Xn are independent and distributed
according to ùí©(Œº, œÉ2) for some Œº ‚àà‚Ñùand some œÉ2 > 0. Then the following are valid:
‚àön
ÃÑX ‚àíŒº
œÉ
is ùí©(0, 1)-distributed;
(8.16)
(n ‚àí1) s2
X
œÉ2
is œá2
n‚àí1-distributed;
(8.17)
‚àön
ÃÑX ‚àíŒº
sX
is tn‚àí1-distributed, where sX := +‚àös2
X .
(8.18)
Furthermore, ÃÑX and s2
X are independent random variables.4
Proof. Let us begin with the proof of assertion (8.16). Since the Xjs are independent
and ùí©(Œº, œÉ2)-distributed, by Proposition 4.6.11 their sum X1 + ‚ãÖ‚ãÖ‚ãÖ+ Xn possesses an
ùí©(nŒº, nœÉ2) distribution. Consequently, an application of Proposition 4.2.3 implies that
ÃÑX is ùí©(Œº, œÉ2/n)-distributed, hence, another application of Proposition 4.2.3 tells us that
ÃÑX‚àíŒº
œÉ/‚àön is standard normal. This completes the proof of statement (8.16).
We turn now to the verification of the remaining assertions. Letting
Yj :=
Xj ‚àíŒº
œÉ
,
1 ‚â§j ‚â§n ,
(8.19)
the random variables Y1, . . . , Yn are independent ùí©(0, 1)-distributed. Moreover, their
(unbiased) sample variance may be calculated by
s2
Y =
1
n ‚àí1
n
‚àë
j=1
(Yj ‚àíÃÑY)2 =
1
n ‚àí1{
n
‚àë
j=1
Y 2
j ‚àí2 ÃÑY
n
‚àë
j=1
Yj + n ÃÑY 2}
=
1
n ‚àí1{
n
‚àë
j=1
Y 2
j ‚àí2n ÃÑY 2 + n ÃÑY 2} =
1
n ‚àí1{
n
‚àë
j=1
Y 2
j ‚àí(‚àön ÃÑY)2}.
(8.20)
4 Recall that
ÃÑX = 1
n
n
‚àë
j=1
Xj
and
s2
X =
1
n ‚àí1
n
‚àë
j=1
(Xj ‚àíÃÑX)2 .

394
‡±™
8 Mathematical statistics
To proceed further, set b1 := (n‚àí1/2, . . . , n‚àí1/2), and note that b1 is a normalized n-dimen-
sional vector, that is, we have |b1|2 = 1. Let E ‚äÜ‚Ñùn be the (n ‚àí1)-dimensional sub-
space consisting of elements that are perpendicular to b1. Choosing an orthonormal
basis b2, . . . , bn in E, by the choice of E, the vectors b1, . . . , bn form an orthonormal ba-
sis in ‚Ñùn. If bi = (Œ≤i1, . . . , Œ≤in), 1 ‚â§i ‚â§n, let B be the n √ó n-matrix with entries Œ≤ij,
that is, the vectors b1, . . . , bn are the rows of B. Since (bi)n
i=1 are orthonormal, B is uni-
tary.
As in Lemma 8.4.2, define Z1, . . . , Zn by
Zi :=
n
‚àë
j=1
Œ≤ijYj ,
1 ‚â§i ‚â§n ,
and the quadratic form Q (with m = 1) as
Q :=
n
‚àë
j=1
Y 2
j ‚àíZ2
1 .
Because of Lemma 8.4.2, the quadratic form Q is œá2
n‚àí1-distributed and, furthermore, it is
independent of Z1. By the choice of B and b1,
Œ≤11 = ‚ãÖ‚ãÖ‚ãÖ= Œ≤1n = n‚àí1/2 ,
hence Z1 = n1/2 ÃÑY and, due to eq. (8.20), this leads to
Q =
n
‚àë
j=1
Y 2
j ‚àí(n1/2 ÃÑY)
2 = (n ‚àí1) s2
Y .
This observation implies (n ‚àí1) s2
Y is œá2
n‚àí1-distributed and, moreover, (n ‚àí1)s2
Y and Z1 are
independent, thus also s2
Y and Z1.
The choice of the Yjs in eq. (8.19) immediately implies ÃÑY =
ÃÑX‚àíŒº
œÉ , hence
(n ‚àí1) s2
Y =
n
‚àë
j=1
(Yj ‚àíÃÑY)2 =
n
‚àë
j=1
(
Xj ‚àíŒº
œÉ
‚àí
ÃÑX ‚àíŒº
œÉ
)
2
= s2
X
œÉ2 (n ‚àí1) ,
which proves assertion (8.17).
Recall that Z1 = n1/2 ÃÑY = n1/2 ÃÑX‚àíŒº
œÉ , which leads to
ÃÑX = n‚àí1/2 œÉZ1 + Œº. Thus, because
of Proposition 4.1.9, the independence of s2
Y = s2
X/œÉ2 and Z1 implies that s2
X and
ÃÑX are
independent as well.
It remains to prove statement (8.18). We already know that V := ‚àön
ÃÑX‚àíŒº
œÉ
is stan-
dard normal, and W := (n ‚àí1) s2
X/œÉ2 is œá2
n‚àí1-distributed. Since they are independent, by
Proposition 4.6, applied with n ‚àí1, we get

8.4 Tests for normally distributed populations
‡±™
395
‚àön
ÃÑX ‚àíŒº
sX
=
V
‚àö1
n‚àí1 W
is tn‚àí1-distributed .
This implies assertion (8.18) and completes the proof of the proposition.
Remark 8.4.4. It is important to mention that the random variables X1, . . . , Xn satisfy
the assumptions of Proposition 8.4.3 if and only if the vector (X1, . . . , Xn) is ùí©(Œº, œÉ2)‚äón-
distributed or, equivalently, if its probability distribution is ùí©( ‚ÉóŒº, œÉ2In).
Summary: Let X1, . . . , Xn be independent and ùí©(Œº, œÉ2)-distributed. Then
‚àön
ÃÑX ‚àíŒº
œÉ
‚àºùí©(0, 1),
(n ‚àí1) s2
X
œÉ2 ‚àºœá2
n‚àí1,
‚àön
ÃÑX ‚àíŒº
sX
‚àºtn‚àí1 .
Furthermore, ÃÑX and s2
X are independent random variables.
8.4.2 Quantiles
Let X be a (real-valued) random variable. Given a number 0 < Œ≤ < 1, a uŒ≤ ‚àà‚Ñùis said to
be a Œ≤-quantile of X provided that
‚Ñô{X ‚â§uŒ≤} ‚â•Œ≤
and
‚Ñô{X ‚â•uŒ≤} ‚â•1 ‚àíŒ≤ .
Another way to write this is
‚ÑôX((‚àí‚àû, uŒ≤]) ‚â•Œ≤
and
‚ÑôX([uŒ≤, ‚àû)) ‚â•1 ‚àíŒ≤ .
In particular, this implies that the quantile only depends on the distribution of a random
variable, not on the way it is defined. Since
‚Ñô{X ‚â•uŒ≤} = 1 ‚àí‚Ñô{X < uŒ≤} ,
the condition for the quantile may also be formulated as
‚Ñô{X ‚â§uŒ≤} ‚â•Œ≤
and
‚Ñô{X < uŒ≤} ‚â§Œ≤ .
Example 8.4.5. Suppose that ‚Ñô{X = 0} = ‚Ñô{X = 1} = 1
2. Then there is no Œ≤-quantile in
the case Œ≤
Ã∏= 1
2. Why? This is due to the fact that the distribution function t Û≥®É‚Üí‚Ñô{X ‚â§t}
attains only the values 0, 1
2, and 1. If Œ≤ = 1/2, then every number u ‚àà[0, 1) satisfies
‚Ñô{X ‚â§u} ‚â•1
2
and
‚Ñô{X < u} ‚â§1
2 ,
hence each u ‚àà[0, 1) is a (1/2)-quantile of X.

396
‡±™
8 Mathematical statistics
This example tells us two facts: quantiles do not always exist and, moreover, if they
exist, then they need not be unique.
The situation becomes completely different if X possesses a positive distribution
density.
Proposition 8.4.6. Suppose that there this a positive density p such that
FX(t) = ‚Ñô{X ‚â§t} =
t
‚à´
‚àí‚àû
p(x)dx ,
t ‚àà‚Ñù.
Then for each Œ≤ ‚àà(0, 1), there is a unique Œ≤-quantile uŒ≤. That is, there is a unique uŒ≤ ‚àà‚Ñù
for which
FX(uŒ≤) = ‚Ñô{X ‚â§uŒ≤} =
uŒ≤
‚à´
‚àí‚àû
p(x)dx = Œ≤ .
(8.21)
Proof. Under these assumptions about X, its distribution function FX is a one-to-one
mapping from ‚Ñùonto (0, 1). Hence, its inverse function F‚àí1
X exists and uŒ≤ = F‚àí1
X (Œ≤) is the
unique number satisfying (8.21).
Remark 8.4.7. Of course, Proposition 8.4.6 remains valid if there are a ‚àà‚Ñùand/or b ‚àà‚Ñù
such that the density p satisfies p(x) = 0 if x < a and/or p(x) = 0 if x > b. In this case the
quantile uŒ≤ satisfies either uŒ≤ > a or uŒ≤ < b, respectively.
Let us now introduce some quantiles which will play an important later on. The first
quantiles we consider are those of the standard normal distribution.
Definition 8.4.8. Let Œ¶ be the distribution function of ùí©(0, 1), as it was introduced in Definition 1.6.2.
For a given Œ≤ ‚àà(0, 1), the Œ≤-quantile zŒ≤ of the standard normal distribution is the unique real number
satisfying
Œ¶(zŒ≤) = Œ≤
or, equivalently,
zŒ≤ = Œ¶‚àí1(Œ≤) .
Another way to define is as follows. Let X be a standard normal random variable. Then
zŒ≤ is the unique real number such that
‚Ñô{X ‚â§zŒ≤} = Œ≤ .
The following properties of zŒ≤ will be used later on. Compare also Figure 8.5 for the
assertions.
Proposition 8.4.9. Let X be standard normally distributed. Then the following are valid:
1.
We have z1/2 = 0 , zŒ≤ < 0 for 0 < Œ≤ < 1/2 , and zŒ≤ > 0 for 1/2 < Œ≤ < 1 .
2.
For all 0 < Œ≤ < 1, it follows that ‚Ñô{X ‚â•zŒ≤} = 1 ‚àíŒ≤ .
3.
If 0 < Œ≤ < 1, then z1‚àíŒ≤ = ‚àízŒ≤ .
4.
For 0 < Œ± < 1 we have ‚Ñô{|X| ‚â•z1‚àíŒ±/2} = Œ± .

8.4 Tests for normally distributed populations
‡±™
397
Figure 8.5: The assertions of Proposition 8.4.9.
Proof. The first property easily follows from Œ¶(0) = 1/2, hence Œ¶(t) > 1/2 if and only if
t > 0.
Let X be standard normal. Then ‚Ñô{X ‚â•zŒ≤} = 1 ‚àí‚Ñô{X ‚â§zŒ≤} = 1 ‚àíŒ≤, which proves
the second assertion.
Since ‚àíX is standard normal as well, by property 2 it follows that
‚Ñô{X ‚â§‚àízŒ≤} = ‚Ñô{‚àíX ‚â•zŒ≤} = ‚Ñô{X ‚â•zŒ≤} = 1 ‚àíŒ≤ = ‚Ñô{X ‚â§z1‚àíŒ≤} ,
hence z1‚àíŒ≤ = ‚àízŒ≤ as asserted.
To prove the fourth assertion, note that properties 2 and 3 imply
‚Ñô{|X| ‚â•z1‚àíŒ±/2} = ‚Ñô{X ‚â§‚àíz1‚àíŒ±/2
or
X ‚â•z1‚àíŒ±/2}
= ‚Ñô{X ‚â§‚àíz1‚àíŒ±/2} + ‚Ñô{X ‚â•z1‚àíŒ±/2}
= ‚Ñô{X ‚â§zŒ±/2} + ‚Ñô{X ‚â•z1‚àíŒ±/2} = Œ±/2 + Œ±/2 = Œ± .
Here we used 1 ‚àíŒ±/2 > 1/2 implying z1‚àíŒ±/2 > 0, hence the events {X ‚â§‚àíz1‚àíŒ±/2} and
{X ‚â•z1‚àíŒ±/2} are disjoint.
To get an impression about the size of the quantiles zŒ≤, let us state a few of them.
Œ≤
0.999
0.995
0.99
0.95
0.9
0.8
0.75
zŒ≤
3.0902
2.5758
2.3263
1.6449
1.2816
0.8416
0.6745

398
‡±™
8 Mathematical statistics
The values for small Œ≤ > 0 follow by z1‚àíŒ≤ = ‚àízŒ≤. So, for example,
z0.1 = ‚àíz0.9 = ‚àí1.2816 .
The next quantiles, needed later on, are those of a œá2
n distribution.
Definition 8.4.10. Let X be distributed according to œá2
n and let 0 < Œ≤ < 1. The unique (positive) number
œá2
n;Œ≤ satisfying
‚Ñô{X ‚â§œá2
n;Œ≤} = Œ≤
is called the Œ≤-quantile of the œá2
n distribution.
Two other, equivalent, ways to introduce these quantiles are as follows:
1.
If X, . . . , Xn are independent standard normal, then
‚Ñô{X2
1 + ‚ãÖ‚ãÖ‚ãÖ+ X2
n ‚â§œá2
n;Œ≤} = Œ≤ .
2.
The quantile œá2
n;Œ≤ satisfies
1
2n/2Œì(n/2)
œá2
n;Œ≤
‚à´
0
xn/2‚àí1 e‚àíx/2dx = Œ≤ .
For later purposes, we mention also the following property. If 0 < Œ± < 1, then for any
œá2
n-distributed random variable X (see Figure 8.6),
‚Ñô{X ‚àâ[œá2
n;Œ±/2 , œá2
n;1‚àíŒ±/2]} = Œ± .
(8.22)
Figure 8.6: Graphic presentation of formula (8.22) for œá2
4.
In a similar way, we define now the quantiles of Student‚Äôs tn and of Fisher‚Äôs Fm,n distri-
butions. For their descriptions, we refer to Definitions 4.7.6 and 4.7.13, respectively.

8.4 Tests for normally distributed populations
‡±™
399
Definition 8.4.11. Let X be tn-distributed and let Y be distributed according to Fm,n. For Œ≤ ‚àà(0, 1) the
Œ≤-quantiles tn;Œ≤ and Fm,n;Œ≤ of the tn and Fm,n distributions are the unique numbers satisfying
‚Ñô{X ‚â§tn;Œ≤} = Œ≤
and
‚Ñô{Y ‚â§Fm,n;Œ≤} = Œ≤ .
Remark 8.4.12. Let X be tn-distributed. Then ‚àíX is tn-distributed as well, hence
‚Ñô{X ‚â§s} = ‚Ñô{‚àíX ‚â§s} for s ‚àà‚Ñù. Therefore, as in the case of the normal distribution, we
get ‚àítn;Œ≤ = tn;1‚àíŒ≤,and also
‚Ñô{|X| > tn;1‚àíŒ±/2} = ‚Ñô{|X| ‚â•tn;1‚àíŒ±/2} = Œ± .
(8.23)
Remark 8.4.13. Another possibility to introduce the Œ≤-quantile of the tn distribution is
as follows: if X and X1, . . . , Xn are independent standard normal variables, then
‚Ñô{
X
‚àö1
n ‚àën
i=1 X2
i
‚â§tn;Œ≤} = Œ≤ .
Similarly, one may characterize the quantiles of the Fm,n distribution in the follow-
ing way. Let X and Y be independent and distributed according to œá2
m and œá2
n, respec-
tively. Then the Œ≤-quantile Fm,n;Œ≤ is the unique number satisfying
‚Ñô{X/m
Y/n ‚â§Fm,n;Œ≤} = Œ≤ .
Note that the quantiles Fm,n;Œ≤ are positive numbers while the tn;Œ≤ are negative if
0 < Œ≤ < 1/2 and positive in the case 1/2 < Œ≤ < 1.
If s > 0, then
‚Ñô{X/m
Y/n ‚â§s} = ‚Ñô{ Y/n
X/m ‚â•1
s} = 1 ‚àí‚Ñô{ Y/n
X/m ‚â§1
s} ,
which immediately implies
Fm,n;Œ≤ =
1
Fn,m;1‚àíŒ≤
.
Summary: The following Œ≤-quantiles will play an important role later on:
X ‚àºùí©(0, 1)
‚áí
‚Ñô{X ‚â§zŒ≤} = Œ≤,
X ‚àºœá2
n
‚áí
‚Ñô{X ‚â§œán;Œ≤} = Œ≤,
X ‚àºtn
‚áí
‚Ñô{X ‚â§tn;Œ≤} = Œ≤,
X ‚àºFm,n
‚áí
‚Ñô{X ‚â§Fm,n;Œ≤} = Œ≤.

400
‡±™
8 Mathematical statistics
8.4.3 Z-tests or Gauss tests
Suppose we have an item of unknown length. In order to get some information about its
length, we measure the item n times with an instrument of known accuracy. As sample
we get a vector x = (x1, . . . , xn), where xj is the value obtained in the jth measurement.
These measurements were executed independently, thus, we may assume that the xjs
are independent ùí©(Œº, œÉ2
0)-distributed with known œÉ2
0 > 0 and unknown length Œº ‚àà‚Ñù.
Therefore, the describing statistical model is
(‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº, œÉ2
0)
‚äón)Œº‚àà‚Ñù= (‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©( ‚ÉóŒº, œÉ2
0In))Œº‚àà‚Ñù.
From the hypothesis, two types of test apply in this case. We start with the so-called one-
sided Z-test (also called one-sided Gauss test). Here the null hypothesis is ‚Ñç0 : Œº ‚â§Œº0,
where Œº0 ‚àà‚Ñùis a given real number. Consequently, the alternative hypothesis is
‚Ñç1 : Œº > Œº0, that is, Œò0 = (‚àí‚àû, Œº0] while Œò1 = (Œº0, ‚àû). In the above context, this
says that we claim that the length of the item is less than or equal to a given Œº0, and to
check this we measure the item n times.
Proposition 8.4.14. Let Œ± ‚àà(0, 1) be a given significance level. Then T = (ùí≥0, ùí≥1) with5
ùí≥0 := {x ‚àà‚Ñùn : ÃÑx ‚â§Œº0 + n‚àí1/2 œÉ0 z1‚àíŒ±}
and with
ùí≥1 := {x ‚àà‚Ñùn : ÃÑx > Œº0 + n‚àí1/2 œÉ0 z1‚àíŒ±}
is an Œ±-significance test to check ‚Ñç0 : Œº ‚â§Œº0 against ‚Ñç1 : Œº > Œº0. Here z1‚àíŒ± denotes the
(1 ‚àíŒ±)-quantile introduced in Definition 8.4.8.
Proof. The assertion of Proposition 8.4.14 says that
sup
Œº‚â§Œº0
‚ÑôŒº(ùí≥1) = sup
Œº‚â§Œº0
ùí©(Œº, œÉ2
0)
‚äón(ùí≥1) ‚â§Œ± .
To verify this, let us choose an arbitrary Œº ‚â§Œº0 and define S : ‚Ñùn ‚Üí‚Ñùby
S(x) := ‚àön ÃÑx ‚àíŒº
œÉ0
,
x ‚àà‚Ñùn .
(8.24)
Regard S as a random variable on the probability space (‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº, œÉ2
0)‚äón). We
claim that S is a standard normally distributed random variable. This fact is crucial.
Therefore, let us give a more detailed reasoning.
5 Recall that ÃÑx denotes the arithmetic mean of an vector x = (x1, . . . , xn) in ‚Ñùn.

8.4 Tests for normally distributed populations
‡±™
401
Define random variables Xj on the probability space (‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº, œÉ2
0)‚äón) by
Xj(x) = xj, where x = (x1, . . . , xn). Then the random vector X = (X1, . . . , Xn) is the identity
on ‚Ñùn, hence ùí©(Œº, œÉ2
0)‚äón-distributed. In view of Remark 8.4.4 and since
S(x) = ‚àön
ÃÑX(x) ‚àíŒº
œÉ0
,
assertion (8.16) applies for S, that is, it is ùí©(0, 1)-distributed. Consequently,
ùí©(Œº, œÉ2
0)
‚äón{x ‚àà‚Ñùn : S(x) > z1‚àíŒ±} = Œ± .
(8.25)
Since Œº ‚â§Œº0, we have
ùí≥1 = {x ‚àà‚Ñùn : ÃÑx > Œº0 + n‚àí1/2 œÉ0 z1‚àíŒ±}
‚äÜ{x ‚àà‚Ñùn : ÃÑx > Œº + n‚àí1/2 œÉ0 z1‚àíŒ±} = {x ‚àà‚Ñùn : S(x) > z1‚àíŒ±} ,
hence, by eq. (8.25), it follows that
ùí©(Œº, œÉ2
0)
‚äón(ùí≥1) ‚â§ùí©(Œº, œÉ2
0)
‚äón{x ‚àà‚Ñùn : S(x) > z1‚àíŒ±} = Œ± .
This completes the proof.
How does the power function of the Z-test in Proposition 8.4.14 look like? If S is as
in eq. (8.24), then, according to Definition 8.2.11,
Œ≤T(Œº) = ùí©(Œº, œÉ2
0)
‚äón(ùí≥1) = N(Œº, œÉ2
0)
‚äón{x ‚àà‚Ñùn : ‚àön ÃÑx ‚àíŒº0
œÉ0
> z1‚àíŒ±}
= ùí©(Œº, œÉ2
0)
‚äón{x ‚àà‚Ñùn : S(x) > z1‚àíŒ± + (Œº0 ‚àíŒº) ‚àön
œÉ0
}
= 1 ‚àíŒ¶(z1‚àíŒ± + (Œº0 ‚àíŒº) ‚àön
œÉ0
) = Œ¶(zŒ± + (Œº ‚àíŒº0) ‚àön
œÉ0
) .
In particular, Œ≤T is increasing on ‚Ñùwith Œ≤T(Œº0) = Œ±. Moreover, we see that Œ≤T(Œº) < Œ± if
Œº < Œº0, and Œ≤T(Œº) > Œ± for Œº > Œº0. See Figure 8.7 for an example of the power function.
While the critical region of a one-sided Z-test is an interval, in the case of the two-
sided Z-test it is the union of two intervals. Here the null hypothesis is ‚Ñç0 : Œº = Œº0,
hence the alternative hypothesis is given as ‚Ñç1 : Œº
Ã∏= Œº0.
Proposition 8.4.15. The test T = (ùí≥0, ùí≥1), where
ùí≥0 := {x ‚àà‚Ñùn : Œº0 ‚àín‚àí1/2 œÉ0 z1‚àíŒ±/2 ‚â§ÃÑx ‚â§Œº0 + n‚àí1/2 œÉ0 z1‚àíŒ±/2}
= {x ‚àà‚Ñùn : ‚àön | ÃÑx ‚àíŒº0|
œÉ0
‚â§z1‚àíŒ±/2}
and ùí≥1 = ‚Ñùn \ ùí≥0, is an Œ±-significance test for ‚Ñç0 : Œº = Œº0 against ‚Ñç1 : Œº
Ã∏= Œº0.

402
‡±™
8 Mathematical statistics
Figure 8.7: Power function of the one-sided Z-test T with Œ± = 0.1, Œº0 = 2, œÉ0 = 1, and n = 10.
Proof. Since here Œò0 = {Œº0}, the proof becomes easier than in the one-sided case. We
only have to verify that
ùí©(Œº0, œÉ2
0)
‚äón(ùí≥1) ‚â§Œ± .
(8.26)
Regarding S, defined by
S(x) := ‚àön ÃÑx ‚àíŒº0
œÉ0
,
as a random variable on (‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº0, œÉ2
0)‚äón), by the same arguments as in the pre-
vious proof, it is standard normally distributed. Thus, using assertion (4) of Proposi-
tion 8.4.9, we obtain
ùí©(Œº0, œÉ2
0)
‚äón(ùí≥1) = ùí©(Œº0, œÉ2
0)
‚äón{x ‚àà‚Ñùn : ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®S(x)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®> z1‚àíŒ±/2} = Œ± .
Of course, this completes the proof.
Recall that Œ≤T(Œº) is the probability to observe an x = (x1, . . . , xn) in the critical region
ùí≥1 provided Œº is the ‚Äútrue‚Äù mean value. As can be seen in Fig. 8.8, this probability is small
(equal Œ±) if Œº = Œº0 and becomes rapidly big for Œº different of the suggested Œº0.
Remark 8.4.16 (Important!). How to apply the one- or two-sided Z-test in a concrete sit-
uation? If the hypothesis is either ‚Ñç0 : Œº ‚â§Œº0 or ‚Ñç0 : Œº = Œº0, then the regions ùí≥1 of
rejection are either
{x ‚àà‚Ñùn : ‚àön ÃÑx ‚àíŒº0
œÉ0
> z1‚àíŒ±}
or
{x ‚àà‚Ñùn : ‚àön | ÃÑx ‚àíŒº0|
œÉ0
> z1‚àíŒ±/2} .

8.4 Tests for normally distributed populations
‡±™
403
Figure 8.8: Power function of the two-sided Z-test T with Œ± = 0.1, Œº0 = 2, œÉ0 = 1, and n = 10.
By the definition of the quantile, the former is equivalent to
Œ¶(‚àön ÃÑx ‚àíŒº0
œÉ0
) > 1 ‚àíŒ±
‚áî
Œ¶(‚àön Œº0 ‚àíÃÑx
œÉ0
) < Œ± .
Similarly, in the two-sided test, one has x ‚ààùí≥1 if and only if either
Œ¶(‚àön ÃÑx ‚àíŒº0
œÉ0
) < Œ±/2
or
Œ¶(‚àön Œº0 ‚àíÃÑx
œÉ0
) < Œ±/2 .
Suppose now we observed n values x = (x1, . . . , xn) which are independent and ùí©(Œº, œÉ2
0)-
distributed for some unknown Œº ‚àà‚Ñù. If
Œ¶(‚àön Œº0 ‚àíÃÑx
œÉ0
) < Œ±
‚áí
reject
‚Ñç0 : Œº ‚â§Œº0 .
Similarly, we have to reject ‚Ñç0 : Œº = Œº0 if either
Œ¶(‚àön ÃÑx ‚àíŒº0
œÉ0
) < Œ±/2
or
Œ¶(‚àön Œº0 ‚àíÃÑx
œÉ0
) < Œ±/2 .
In both cases (one- and two-sided test) the probability for an erroneous decision is
bounded by Œ± > 0.
Example 8.4.17. Suppose the hypothesis is ‚Ñç0 : Œº ‚â§Œº0 and our calculations lead to
‚àön Œº0 ‚àíÃÑx
œÉ0
= ‚àí2.13 .
Since Œ¶(‚àí2.13) ‚âà0.0165858, we may reject ‚Ñç0 with significance level Œ± whenever
Œ± > 0.0165858. But we cannot reject it with smaller risk. For example, if Œ± = 0.01, then

404
‡±™
8 Mathematical statistics
the result does not contradict the hypothesis. There is a great likelihood that ‚Ñç0 is
wrong, but if we want to be very sure that this is so, we cannot derive this from the
obtained result.
8.4.4 t-tests
The problem is similar to that considered in the case of the Z-test. But there is one im-
portant difference. We do no longer assume that the variance is known, which will be
so in most cases. Therefore, this test is more realistic than the Z-test.
The starting point is the statistical model
(‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº, œÉ2)
‚äón)(Œº,œÉ2)‚àà‚Ñù√ó(0,‚àû) .
Observe that the unknown parameter is now a vector (Œº, œÉ2) ‚àà‚Ñù√ó (0, ‚àû). We begin by
investigating the one-sided t-test. Given some Œº0 ‚àà‚Ñù, the null hypothesis is as before,
that is, we have ‚Ñç0 : Œº ‚â§Œº0. In the general setting, this means Œò0 = (‚àí‚àû, Œº0] √ó (0, ‚àû),
while Œò1 = (Œº0, ‚àû) √ó (0, ‚àû).
To formulate the next result, let us shortly recall the following notations. If s2
x de-
notes the unbiased sample variance, as defined in eq. (8.14), set sx := +‚àös2
x. Further-
more, tn‚àí1;1‚àíŒ± denotes the (1 ‚àíŒ±)-quantile of the tn‚àí1-distribution, as introduced in Defi-
nition 8.4.11.
Proposition 8.4.18. Given Œ± ‚àà(0, 1), the regions ùí≥0 and ùí≥1 in ‚Ñùn are defined by
ùí≥0 := {x ‚àà‚Ñùn : ‚àön ÃÑx ‚àíŒº0
sx
‚â§tn‚àí1;1‚àíŒ±}
and ùí≥1 = ‚Ñùn \ ùí≥0. With this choice of ùí≥0 and ùí≥1, the test T = (ùí≥0, ùí≥1) is an Œ±-significance
test for ‚Ñç0 : Œº ‚â§Œº0 against ‚Ñç1 : Œº > Œº0.
Proof. Given Œº ‚â§Œº0, define the random variable S on (‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº, œÉ2)‚äón) as
S(x) := ‚àön ÃÑx ‚àíŒº
sx
,
x ‚àà‚Ñùn .
Property (8.18) implies that S is tn‚àí1-distributed, hence by the definition of the quantile
tn‚àí1;1‚àíŒ±, it follows that
ùí©(Œº, œÉ2)
‚äón{x ‚àà‚Ñùn : S(x) > tn‚àí1;1‚àíŒ±} = Œ± .
From Œº ‚â§Œº0, we easily derive
ùí≥1 ‚äÜ{x ‚àà‚Ñùn : S(x) > tn‚àí1;1‚àíŒ±} ,

8.4 Tests for normally distributed populations
‡±™
405
thus, as asserted,
sup
Œº‚â§Œº0
ùí©(Œº, œÉ2)
‚äón(ùí≥1) ‚â§ùí©(Œº, œÉ2)
‚äón{x ‚àà‚Ñùn : S(x) > tn‚àí1;1‚àíŒ±} = Œ± .
As in the case of the Z-test, the null hypothesis of the two-sided t-test is ‚Ñç0 : Œº = Œº0
for some Œº0 ‚àà‚Ñù. Again, we do not assume that the variance is known.
A two-sided t-test with significance level Œ± may be constructed as follows.
Proposition 8.4.19. Given Œ± ‚àà(0, 1), define regions ùí≥0 and ùí≥1 in ‚Ñùn by
ùí≥0 := {x ‚àà‚Ñùn : ‚àön
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
ÃÑx ‚àíŒº0
sx
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§tn‚àí1;1‚àíŒ±/2}
and ùí≥1 = ‚Ñùn \ ùí≥0. Then T = (ùí≥0, ùí≥1) is an Œ±-significance test for ‚Ñç0 : Œº = Œº0 against
‚Ñç1 : Œº
Ã∏= Œº0.
Proposition 8.4.19 is proven by similar methods, as we have used for the proofs of
Propositions 8.4.15 and 8.4.18. Therefore, we decline to prove it here.
Example 8.4.20. We claim a certain workpiece has a length of 22 inches. Thus, the null
hypothesis is ‚Ñç0 : Œº = 22. To check ‚Ñç0, we measure the piece 10 times under the same
conditions. The 10 values we obtained are (in inches)
22.17,
22.11,
22.10,
22.14,
22.02,
21.95,
22.02,
22.08,
21.98,
22.15
Do these values allow us to reject the hypothesis or do they confirm it? We have
ÃÑx = 22.072
and
sx = 0.07554248 ,
hence ‚àö10
ÃÑx ‚àí22
sx
= 3.013986 .
If we choose the significance level Œ± = 0.05, we have to investigate the quantile t9;0.975,
which equals t9;0.975 = 2.26. This lets us conclude the observed vector x = (x1, . . . , x10)
belongs to ùí≥1, and we may reject ‚Ñç0. Consequently, with a confidence of 95 % we may
say, Œº
Ã∏= 22.
Another way to argue is as follows: Let S(x) =
ÃÑx‚àí22
sx
= 3.013986. Then we have S(x) ‚â§
t9,0.975 if and only if for a t9-distributed X it follows that
FX(S(x)) = ‚Ñô{X ‚â§S(x)} ‚â§0.975 .
But in our case FX(S(x)) = 0.992687. So, we also get by this argument that ‚Ñç0 has to be
rejected.
Remark 8.4.21. If we plug these 10 values, together with Œº0 = 22, into a mathematical
program, the result will be a number Œ±0 = 0.00128927. What does this number tell us?
It says the following. If we have chosen a significance level Œ± > Œ±0, then we have to

406
‡±™
8 Mathematical statistics
reject ‚Ñç0. But, if the chosen Œ± satisfies Œ± < Œ±0, then we fail to reject ‚Ñç0. In our case we
had Œ± = 0.05 > 0.00128927 = Œ±0, hence we may reject ‚Ñç0.
Thus, the price we pay for choosing a higher certainty and taking Œ± < Œ±0 is that
we are no longer able to reject the hypothesis ‚Ñç0. It is as in the daily life: if one
wants to be 99.9 %-sure of not having a car accident, the best way is to avoid driving a
car.
8.4.5 œá2-tests for the variance
The aim of this section is to get some information about the (unknown) variance of a
normal distribution. Again we have to distinguish between the following two cases. The
expected value is known or, otherwise, the expected value is unknown.
Let us start with the former case, that is, we assume that the expected value is known
to be some Œº0 ‚àà‚Ñù. Then the statistical model is
(‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº0, œÉ2)
‚äón)œÉ2>0.
In the one-sided œá2-test, the null hypothesis is ‚Ñç0 : œÉ2 ‚â§œÉ2
0, for some given œÉ2
0 > 0,
while in the two-sided œá2-test we claim that ‚Ñç0 : œÉ2 = œÉ2
0.
Proposition 8.4.22. In the one-sided setting, an Œ±-significance œá2-test T = (ùí≥0, X1) is given
by
ùí≥0 := {x ‚àà‚Ñùn :
n
‚àë
j=1
(xj ‚àíŒº0)2
œÉ2
0
‚â§œá2
n;1‚àíŒ±} .
For the two-sided case, choose
ùí≥0 := {x ‚àà‚Ñùn : œá2
n;Œ±/2 ‚â§
n
‚àë
j=1
(xj ‚àíŒº0)2
œÉ2
0
‚â§œá2
n;1‚àíŒ±/2} ,
(8.27)
to obtain an Œ±-significance test. In both cases, the critical region is ùí≥1 := ‚Ñùn \ ùí≥0.
Proof. We prove the assertion only in the (slightly more difficult) one-sided case. For an
arbitrarily chosen œÉ2 ‚â§œÉ2
0, let ùí©(Œº0, œÉ2)‚äón be the underlying probability measure. We
define now the random variables Xj : ‚Ñùn ‚Üí‚Ñùas Xj(x) = xj for x = (x1, . . . , xn). Then
the Xjs are independent ùí©(Œº0, œÉ2)-distributed. The normalization Yj :=
Xj‚àíŒº0
œÉ
leads to
independent standard normal Yjs. Thus, if
S :=
n
‚àë
j=1
(Xj ‚àíŒº0)2
œÉ2
=
n
‚àë
j=1
Y 2
j ,

8.4 Tests for normally distributed populations
‡±™
407
then, by Proposition 4.6.10, the random variable S is œá2
n-distributed. By the definition of
quantile, we arrive at
ùí©(Œº0, œÉ2)
‚äón{x ‚àà‚Ñùn : S(x) > œán;1‚àíŒ±} = Œ± .
Since œÉ2 ‚â§œÉ2
0, it follows that
ùí≥1 ‚äÜ{x ‚àà‚Ñùn : S(x) > œán;1‚àíŒ±} ,
hence ùí©(Œº0, œÉ2)‚äón(ùí≥1) ‚â§Œ±. This proves, as asserted, that T = (ùí≥0, ùí≥1) is an Œ±-significance
test.
Let us now turn to the case where the expected value is unknown. Here the statistical
model is given by
(‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº, œÉ2)
‚äón)(Œº,œÉ2)‚àà‚Ñù√ó(0,‚àû) .
In the one-sided case, the null hypothesis is ‚Ñç0 : Œ∏ ‚â§Œ∏0. Thus, the parameter set
Œò = ‚Ñù√ó (0, ‚àû) splits into Œò = Œò0 ‚à™Œò1 with
Œò0 = ‚Ñù√ó (0, œÉ2
0]
and
Œò1 = ‚Ñù√ó (œÉ2
0, ‚àû) .
In the two-sided case, the null hypothesis is ‚Ñç0 : Œ∏ = Œ∏0. Hence, in this case we have
Œò0 = ‚Ñù√ó {œÉ2
0}
and
Œò1 = ‚Ñù√ó [(0, œÉ2
0) ‚à™(œÉ2
0, ‚àû)] .
Proposition 8.4.23. In the one-sided case, an Œ±-significance test T = (ùí≥0, X1) is given by
ùí≥0 := {x ‚àà‚Ñùn : (n ‚àí1) s2
x
œÉ2
0
‚â§œá2
n‚àí1;1‚àíŒ±} .
In the two-sided case, choose the region of acceptance as
ùí≥0 := {x ‚àà‚Ñùn : œá2
n‚àí1;Œ±/2 ‚â§(n ‚àí1) s2
x
œÉ2
0
‚â§œá2
n‚àí1;1‚àíŒ±/2}
(8.28)
to get an Œ±-significance test. Again, the critical regions are given by ùí≥1 := ‚Ñùn \ ùí≥0.
Proof. The proof is very similar to that of Proposition 8.4.22, but with some important
difference. Here we have to set
S(x) := (n ‚àí1) s2
x
œÉ2 ,
x ‚àà‚Ñùn .
Then property (8.17) applies, and it lets us conclude that S is œá2
n‚àí1-distributed, provided
that ùí©(Œº, œÉ2)‚äón is the true probability measure. After that observation the proof is com-
pleted as that of Proposition 8.4.22.

408
‡±™
8 Mathematical statistics
Summary: The most important one-sample Œ±-significance tests for normally distributed
populations are
Name
Parameters
Hypotheses
Critical region ùí≥1
One-sided Z-test
œÉ2 > 0 known
‚Ñç0 : Œº ‚â§Œº0
‚Ñç1 : Œº > Œº0
{x ‚àà‚Ñùn : ‚àön
ÃÑx‚àíŒº0
œÉ
> z1‚àíŒ±}
Two-sided Z-test
œÉ2 > 0 known
‚Ñç0 : Œº = Œº0
‚Ñç1 : Œº
Ã∏= Œº0
{x ‚àà‚Ñùn : |‚àön
ÃÑx‚àíŒº0
œÉ | > z1‚àíŒ±/2}
One-sided t-test
œÉ2 > 0 unknown
‚Ñç0 : Œº ‚â§Œº0
‚Ñç1 : Œº > Œº0
{x ‚àà‚Ñùn : ‚àön
ÃÑx‚àíŒº0
sx
> tn‚àí1; 1‚àíŒ±}
Two-sided t-test
œÉ2 > 0 unknown
‚Ñç0 : Œº = Œº0
‚Ñç1 : Œº
Ã∏= Œº0
{x ‚àà‚Ñùn : |‚àön
ÃÑx‚àíŒº0
sx | > tn‚àí1; 1‚àíŒ±/2}
One-sided œá2-test with
known mean value
Œº ‚àà‚Ñùknown
‚Ñç0 : œÉ2 ‚â§œÉ2
0
‚Ñç1 : œÉ2 > œÉ2
0
{x ‚àà‚Ñùn : ‚àën
i=1
(xi‚àíŒº)2
œÉ2
0
> œá2
n; 1‚àíŒ±}
Two-sided œá2-test with
known mean value
Œº ‚àà‚Ñùknown
‚Ñç0 : œÉ2 = œÉ2
0
‚Ñç1 : œÉ2
Ã∏= œÉ2
0
{x ‚àà‚Ñùn : ‚àë(xi‚àíŒº)2
œÉ2
0
< œá2
n; Œ±/2} ‚à™
{x ‚àà‚Ñùn : ‚àë(xi‚àíŒº)2
œÉ2
0
> œá2
n; 1‚àíŒ±/2}
One-sided œá2-test with
unknown mean value
Œº ‚àà‚Ñùunknown
‚Ñç0 : œÉ2 ‚â§œÉ2
0
‚Ñç1 : œÉ2 > œÉ2
0
{x ‚àà‚Ñùn : ‚àë(xi‚àíÃÑx)2
œÉ2
0
> œá2
n‚àí1; 1‚àíŒ±}
Two-sided œá2-test with
unknown mean value
Œº ‚àà‚Ñùunknown
‚Ñç0 : œÉ2 = œÉ2
0
‚Ñç1 : œÉ2
Ã∏= œÉ2
0
{x ‚àà‚Ñùn : ‚àë(xi‚àíÃÑx)2
œÉ2
0
< œá2
n‚àí1; Œ±/2} ‚à™
{x ‚àà‚Ñùn : ‚àë(xi‚àíÃÑx)2
œÉ2
0
> œá2
n‚àí1; 1‚àíŒ±/2}
8.4.6 Two-sample Z-tests
The two-sample Z-test compares the parameters of two different populations. Suppose
we are given two different series of data, say x = (x1, . . . , xm) and y = (y1, . . . , yn), which
were obtained independently by executing m experiments of the first kind and n exper-
iments of the second. Combine both series to a single vector (x, y) ‚àà‚Ñùm+n.
A typical example for the described situation is as follows. A farmer grows grain on
two different lots. On one lot he added fertilizer, on the other he did not. Now he wants
to figure out whether or not adding fertilizer influenced the amount of grain gathered.
Therefore, he measures the amount of grain on the first lot at m different spots and that
on the second lot at n spots. The aim is to compare the mean values in both series of
experiments.
We suppose that the samples x1, . . . , xm of the first population are independent
and N(Œº1, œÉ2
1)-distributed, while the y1, . . . , yn of the second population are indepen-
dent and ùí©(Œº2, œÉ2
2)-distributed. Typical questions are as follows. Do we have Œº1 = Œº2

8.4 Tests for normally distributed populations
‡±™
409
or, maybe, only Œº1 ‚â§Œº2? One may also ask whether or not œÉ2
1 = œÉ2
2 or, maybe, only
œÉ2
1 ‚â§œÉ2
2.
To apply the two-sample Z-test, one has to suppose that the variances œÉ2
1 and œÉ2
2 are
known. This reduces the number of parameters from 4 to 2, namely to Œº1 and Œº2 in ‚Ñù.
Thus, the describing statistical model is given by
(‚Ñùm+n, ‚Ñ¨(‚Ñùm+n), ùí©(Œº1, œÉ2
1)
‚äóm ‚äóùí©(Œº2, œÉ2
2)
‚äón)(Œº1,Œº2)‚àà‚Ñù2 .
(8.29)
Recall that ùí©(Œº1, œÉ2
1)‚äóm ‚äóùí©(Œº2, œÉ2
2)‚äón denotes the multivariate normal distribution with
expected value (Œº1, . . . , Œº1
‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
m
, Œº2, . . . , Œº2
‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü‚èü
n
) and covariance matrix R = (rij)m+n
i,j=1 , where rii = œÉ2
1
if 1 ‚â§i ‚â§m, and rii = œÉ2
2 if m < i ‚â§m + n. Furthermore, rij = 0 if i
Ã∏= j.
Proposition 8.4.24. The statistical model is that in (8.29). To test ‚Ñç0 : Œº1 ‚â§Œº2 against
‚Ñç1 : Œº1 > Œº2, set
ùí≥0 := {(x, y) ‚àà‚Ñùm+n : ‚àö
mn
nœÉ2
1 + mœÉ2
2
( ÃÑx ‚àíÃÑy) ‚â§z1‚àíŒ±}
and ùí≥1 = ‚Ñùm+n \ ùí≥0. Then the test T = (ùí≥0, ùí≥1) is an Œ±-significance test for checking ‚Ñç0
against ‚Ñç1. To test ‚Ñç0 : Œº1 = Œº2 against ‚Ñç1 : Œº1
Ã∏= Œº2, let
ùí≥0 := {(x, y) ‚àà‚Ñùm+n : ‚àö
mn
nœÉ2
1 + mœÉ2
2
| ÃÑx ‚àíÃÑy| ‚â§z1‚àíŒ±/2}
and ùí≥1 = ‚Ñùm+n \ ùí≥0. Then the test T = (ùí≥0, ùí≥1) is an Œ±-significance test for checking ‚Ñç0
against ‚Ñç1.
Proof. Since the proof of the two-sided case is very similar to that of the one-sided,
we only prove the first assertion. Thus, let us assume that ‚Ñç0 is valid, that is, we have
Œº1 ‚â§Œº2. Then we have to verify that
ùí©(Œº1, œÉ2
1 )
‚äóm ‚äóùí©(Œº2, œÉ2
2)
‚äón(ùí≥1) ‚â§Œ± .
(8.30)
To prove this, we investigate the random variables Xi and Yj defined as Xi(x, y) = xi and
Yj(x, y) = yj. Since the underlying probability space is
(‚Ñùm+n, ‚Ñ¨(‚Ñùm+n), ùí©(Œº1, œÉ2
1)
‚äóm ‚äóùí©(Œº2, œÉ2
2)
‚äón) ,
these random variables are independent and distributed according to ùí©(Œº1, œÉ2
1) and
ùí©(Œº2, œÉ2
2), respectively. Consequently,
ÃÑX is ùí©(Œº1, œÉ2
1
m )-distributed, while
ÃÑY is distributed
according to ùí©(Œº2, œÉ2
2
n ). By the construction, ÃÑX and ÃÑY are independent as well, and, more-
over, since ‚àíÃÑY is ùí©(‚àíŒº2, œÉ2
2
n )-distributed, we conclude that the distribution of ÃÑX‚àíÃÑY equals
ùí©(Œº1 ‚àíŒº2, œÉ2
1
m + œÉ2
2
n ). Therefore, the mapping S : ‚Ñùm+n ‚Üí‚Ñùdefined by

410
‡±™
8 Mathematical statistics
S(x, y) := (œÉ2
1
m + œÉ2
2
n )
‚àí1/2
[( ÃÑX(x, y) ‚àíÃÑY(x, y)) ‚àí(Œº1 ‚àíŒº2)]
is standard normal. By the definition of the quantile, this leads to
ùí©(Œº1, œÉ2
1)
‚äóm ‚äóùí©(Œº2, œÉ2
2)
‚äón{(x, y) ‚àà‚Ñùm+n : S(x, y) > z1‚àíŒ±} = Œ± .
(8.31)
Since we assumed ‚Ñç0 to be correct, that is, we suppose Œº1 ‚â§Œº2, it follows that
S(x, y) ‚â•(œÉ2
1
m + œÉ2
2
n )
‚àí1/2
[ ÃÑX(x, y) ‚àíÃÑY(x, y)] = ‚àö
mn
nœÉ2
1 + mœÉ2
2
[ ÃÑX(x, y) ‚àíÃÑY(x, y)] .
Hence
ùí≥1 ‚äÜ{(x, y) ‚àà‚Ñùm+n : S(x, y) > z1‚àíŒ±} ,
which by eq. (8.31) implies estimate (8.30). This completes the proof of this part of the
proposition.
8.4.7 Two-sample t-tests
The situation is similar as in the two-sample Z-test, yet with one important difference.
The variances œÉ2
1 and œÉ2
2 of the two populations are no longer known. Instead, we have
to assume that they coincide, that is, we suppose
œÉ2
1 = œÉ2
2 := œÉ2 .
Therefore, there are three unknown parameters, the expected values Œº1, Œº2, and the
common variance œÉ2. Thus, the statistical model describing this situation is given by
(‚Ñùm+n, ‚Ñ¨(‚Ñùm+n), ùí©(Œº1, œÉ2)
‚äóm ‚äóùí©(Œº2, œÉ2)
‚äón)(Œº1,Œº2,œÉ2)‚àà‚Ñù2√ó(0,‚àû) .
(8.32)
To simplify the formulation of the next statement, introduce T : ‚Ñùm+n ‚Üí‚Ñùas
T(x, y) := ‚àö(m + n ‚àí2) m n
m + n
ÃÑx ‚àíÃÑy
‚àö(m ‚àí1)s2
x + (n ‚àí1)s2
y
,
(x, y) ‚àà‚Ñùm+n .
(8.33)
Proposition 8.4.25. Let the statistical model be as in (8.32). If
ùí≥0 := {(x, y) ‚àà‚Ñùm+n : T(x, y) ‚â§tm+n‚àí2;1‚àíŒ±}
and ùí≥1 = ‚Ñùm+n \ ùí≥0, then T = (ùí≥0, ùí≥1) is an Œ±-significance test for ‚Ñç0 : Œº1 ‚â§Œº2 against
‚Ñç1 : Œº1 > Œº2.

8.4 Tests for normally distributed populations
‡±™
411
On the other hand, the test T = (ùí≥0, ùí≥1) with
ùí≥0 := {(x, y) ‚àà‚Ñùm+n : ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®T(x, y)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®‚â§tm+n‚àí2;1‚àíŒ±/2}
and ùí≥1 = ‚Ñùm+n \ ùí≥0 is an Œ±-significance test for ‚Ñç0 : Œº1 = Œº2 against ‚Ñç1 : Œº1
Ã∏= Œº2.
Proof. This time we prove the two-sided case, that is, the null hypothesis is given by
‚Ñç0 : Œº1 = Œº2.
Let the random vectors X = (X1, . . . , Xm) and Y = (Y1, . . . , Yn) on ‚Ñùm+n be defined
with Xis and Yjs as in the proof of Proposition 8.4.24, that is, we have X(x, y) = x and
Y(x, y) = y. Then by Proposition 4.1.9 and Remark 4.1.10, the unbiased sample variances
s2
X =
1
m ‚àí1
m
‚àë
i=1
(Xi ‚àíÃÑX)2
and
s2
Y =
1
n ‚àí1
m
‚àë
j=1
(Yj ‚àíÃÑY)2
are independent as well. Furthermore, by virtue of statement (8.17), the random vari-
ables
(m ‚àí1)s2
X
œÉ2
and
(n ‚àí1)s2
Y
œÉ2
are distributed according to œá2
m‚àí1 and œá2
n‚àí1, respectively. Proposition 4.6.9 implies that
S2
(X,Y) := 1
œÉ2 {(m ‚àí1)s2
X + (n ‚àí1)s2
Y}
is œá2
m+n‚àí2-distributed. Since s2
X and
ÃÑX, as well as s2
Y and
ÃÑY, are independent, by Propo-
sition 8.4.3, this is also so for S2
(X,Y) and
ÃÑX ‚àí
ÃÑY. As in the proof of Proposition 8.4.24, it
follows that ÃÑX ‚àíÃÑY is distributed according to ùí©(Œº1 ‚àíŒº2, œÉ2
m + œÉ2
n ). Assume now that ‚Ñç0
is true, that is, we have Œº1 = Œº2. Then the last observation implies that
‚àömn
œÉ‚àöm+n( ÃÑX ‚àí
ÃÑY)
is a standard normally distributed random variable and, furthermore, independent of
S2
(X,Y). Thus, by Proposition 4.6, the distribution of the quotient
Z := ‚àöm + n ‚àí2
‚àömn
œÉ‚àöm+n( ÃÑX ‚àíÃÑY)
S(X,Y)
,
where S(X,Y) := +‚àöS2
(X,Y), is tm+n‚àí2-distributed. If T is as in eq. (8.33), then it is not difficult
to prove that Z = T(X, Y). Therefore, by the definition of X and Y, the mapping T is a
tm+n‚àí2-distributed random variable on ‚Ñùm+n, endowed with the probability measure
‚ÑôŒº1,Œº2,œÉ2 = ùí©(Œº1, œÉ2)‚äóm ‚äóùí©(Œº2, œÉ2)‚äón. By eq. (8.23), this implies
‚ÑôŒº1,Œº2,œÉ2(ùí≥1) = ‚ÑôŒº1,Œº2,œÉ2{(x, y) ‚àà‚Ñùm+n : ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®T(x, y)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®> tm+n‚àí2;1‚àíŒ±/2} = Œ± ,
as asserted.

412
‡±™
8 Mathematical statistics
8.4.8 F-tests
In this final section about tests, we compare the variances of two normally distributed
sample series. Since the proofs of the assertions follow the schemes presented in the
previous propositions, we decline to verify them here. We only mention the facts that
play a crucial role during the proofs.
1.
If X1, . . . , Xm and Y1, . . . , Yn are independent and distributed according to ùí©(Œº1, œÉ2
1)
and ùí©(Œº2, œÉ2
2), then
V := 1
œÉ2
1
m
‚àë
i=1
(Xi ‚àíŒº1)2
and
W := 1
œÉ2
2
n
‚àë
j=1
(Yj ‚àíŒº2)2
are œá2
m and œá2
n-distributed and independent. Consequently, the quotient V/m
W/n is
Fm,n-distributed.
2.
For X1, . . . , Xm and Y1, . . . , Yn independent and standard normal, the random vari-
ables
(m ‚àí1) s2
X
œÉ2
1
and
(n ‚àí1) s2
Y
œÉ2
2
are independent and distributed according to œá2
m‚àí1 and œá2
n‚àí1, respectively. Thus, as-
suming œÉ1 = œÉ2, the quotient s2
X/s2
Y possesses an Fm‚àí1,n‚àí1-distribution.
When applying an F-test, as before, two different cases have to be considered.
(K) The expected values Œº1 and Œº2 of the two populations are known. Then the statistical
model is given by
(‚Ñùm+n, ‚Ñ¨(‚Ñùm+n), ùí©(Œº1, œÉ2
1)
‚äóm ‚äóùí©(Œº2, œÉ2
2)
‚äón)(œÉ2
1 ,œÉ2
2)‚àà(0,‚àû)2 .
(U) The expected values are unknown. This case is described by the statistical model
(‚Ñùm+n, ‚Ñ¨(‚Ñùm+n), ùí©(Œº1, œÉ2
1 )
‚äóm ‚äóùí©(Œº2, œÉ2
2)
‚äón)(Œº1,Œº2,œÉ2
1 ,œÉ2
2)‚ààR2√ó(0,‚àû)2 .
In both cases, the null hypothesis may either be ‚Ñç0 : œÉ2
1 ‚â§œÉ2
2 in the one-sided case or
‚Ñç0 : œÉ2
1 = œÉ2
2 in the two-sided one. The regions of acceptance in each of the four different
cases are given by the following subsets of ‚Ñùm+n, and always ùí≥1 = ‚Ñùm+n \ ùí≥0.
Case 1: ‚Ñç0 : œÉ2
1 ‚â§œÉ2
2 and Œº1, Œº2 are known. Then
ùí≥0 := {(x, y) ‚àà‚Ñùm+n :
1
m ‚àëm
i=1(xi ‚àíŒº1)2
1
n ‚àën
j=1(yj ‚àíŒº2)2 ‚â§Fm,n;1‚àíŒ±}.
Case 2: ‚Ñç0 : œÉ2
1 = œÉ2
2 and Œº1, Œº2 are known. Then

8.4 Tests for normally distributed populations
‡±™
413
ùí≥0 := {(x, y) ‚àà‚Ñùm+n : Fm,n;Œ±/2 ‚â§
1
m ‚àëm
i=1(xi ‚àíŒº1)2
1
n ‚àën
j=1(yj ‚àíŒº2)2 ‚â§Fm,n;1‚àíŒ±/2}.
Case 3: ‚Ñç0 : œÉ2
1 ‚â§œÉ2
2 and Œº1, Œº2 are unknown. Then
ùí≥0 := {(x, y) ‚àà‚Ñùm+n : s2
x
s2
y
‚â§Fm‚àí1,n‚àí1;1‚àíŒ±}.
Case 4: ‚Ñç0 : œÉ2
1 = œÉ2
2 and Œº1, Œº2 are unknown. Then
ùí≥0 := {(x, y) ‚àà‚Ñùm+n : Fm‚àí1,n‚àí1;Œ±/2 ‚â§s2
x
s2
y
‚â§Fm‚àí1,n‚àí1;1‚àíŒ±/2}.
Example 8.4.26. Suppose there exist two different methods to measure certain items.
Some evidence lets us suggest that method 2 is more precise than method 1. That is, we
believe that the variance of the measurements by method 2 is smaller than the one by
method 1.
To check this we measure some given item 39 times by method 1 and a maybe dif-
ferent item 28 times by method 2. As result we get x = (x1, . . . , x39) values obtained by
method 1 and another 28 values y = (y1, . . . , y28) by method 2. Thus, in order to apply an
F-test, we have m = 39 and n = 28.
Assume the unbiased variances of the samples x and y are
s2
x = 109.63
and
s2
y = 65.99 ,
hence s2
x
s2
y
= 1.66 .
Let œÉ2
1 and œÉ2
2 be the unknown variances of the xis and yjs, respectively. To obtain as
much information as possible, let us choose as hypotheses
‚Ñç0 : œÉ2
1 ‚â§œÉ2
2
and
‚Ñç1 : œÉ2
1 > œÉ2
2 .
(8.34)
Take Œ± = 0.1 as a significance level. If s2
x/s2
y > F38,27;0.9, then an application of the one-
sided F-test implies that we may reject ‚Ñç0. Note that m ‚àí1 = 38 and n ‚àí1 = 27.
Let X be an F38,27-distributed random variable. Then
s2
x/s2
y > F38,27;0.9
‚áî
‚Ñô{X ‚â§s2
x
s2
y
} > 0.9 .
Tables or mathematical programs give
‚Ñô{X ‚â§s2
x
s2
y
} = ‚Ñô{X ‚â§1.66} = 0.91402 > 0.9 = 1 ‚àíŒ± .

414
‡±™
8 Mathematical statistics
So we may reject ‚Ñç0 and conclude that œÉ2
2 < œÉ2
1. That is, with probability greater than
0.9 we may say that method 2 is more precise than method 1.
Let us one more time emphasize the importance of the choice of the hypotheses
in (8.34). If we were to choose ‚Ñç0 : œÉ2
2 ‚â§œÉ2
1, then our test would confirm ‚Ñç0, but we
could not say that ‚Ñç0 is valid with a likelihood of at least 90 %.
Summary: The most important two-sample Œ±-significance tests for normally distributed
populations are
Name
Parameter
Hypotheses
Critical region ùí≥1
One-sided Z-test
œÉ2
1, œÉ2
2 known
‚Ñç0 : Œº1 ‚â§Œº2
‚Ñç1 : Œº1 > Œº2
{(x, y) ‚àà‚Ñùm+n :
‚àömn
‚àönœÉ2
1+mœÉ2
2
‚ãÖ( ÃÑx ‚àíÃÑy) > z1‚àíŒ±}
Two-sided Z-test
œÉ2
1, œÉ2
2 known
‚Ñç0 : Œº1 = Œº2
‚Ñç1 : Œº1
Ã∏= Œº2
{(x, y) ‚àà‚Ñùm+n :
‚àömn
‚àönœÉ2
1+mœÉ2
2
‚ãÖ| ÃÑx ‚àíÃÑy| > z1‚àíŒ±/2}
One-sided t-test
œÉ2
1 = œÉ2
2 > 0 unknown
‚Ñç0 : Œº1 ‚â§Œº2
‚Ñç1 : Œº1 > Œº2
{(x, y) ‚àà‚Ñùm+n :
‚àö(m+n‚àí2)mn
‚àöm+n
√ó
ÃÑx‚àíÃÑy
‚àö(m‚àí1)s2x +(n‚àí1)s2y
> tm+n‚àí2; 1‚àíŒ±}
Two-sided t-test
œÉ2
1 = œÉ2
2 > 0 unknown
‚Ñç0 : Œº1 = Œº2
‚Ñç1 : Œº1
Ã∏= Œº2
{(x, y) ‚àà‚Ñùm+n :
‚àö(m+n‚àí2)mn
‚àöm+n
√ó
| ÃÑx‚àíÃÑy|
‚àö(m‚àí1)s2x +(n‚àí1)s2y
> tm+n‚àí2; 1‚àíŒ±/2}
One-sided F-test
Œº1, Œº2 known
‚Ñç0 : œÉ2
1 ‚â§œÉ2
2
‚Ñç1 : œÉ2
1 > œÉ2
2
{(x, y) ‚àà‚Ñùm+n :
1
m ‚àëm
i=1(xi‚àíŒº1)2
1
n ‚àën
j=1(yj‚àíŒº2)2 > Fm,n; 1‚àíŒ±}
Two-sided F-test
Œº1, Œº2 known
‚Ñç0 : œÉ2
1 = œÉ2
2
‚Ñç1 : œÉ2
1
Ã∏= œÉ2
2
{(x, y) ‚àà‚Ñùm+n :
1
m ‚àëm
i=1(xi‚àíŒº1)2
1
n ‚àën
j=1(yj‚àíŒº2)2 > Fm,n; 1‚àíŒ±/2}
‚à™{(x, y) ‚àà‚Ñùm+n :
1
n ‚àën
j=1(yj‚àíŒº2)2
1
m ‚àëm
i=1(xi‚àíŒº1)2 > Fn,m; 1‚àíŒ±/2}
One-sided F-test
Œº1, Œº2 ‚àà‚Ñùunknown
‚Ñç0 : œÉ2
1 ‚â§œÉ2
2
‚Ñç1 : œÉ2
1 > œÉ2
2
{(x, y) ‚àà‚Ñùm+n : s2
x/s2
y > Fm‚àí1,n‚àí1; 1‚àíŒ±}
Two-sided F-test
Œº1, Œº2 ‚àà‚Ñùunknown
‚Ñç0 : œÉ2
1 = œÉ2
2
‚Ñç1 : œÉ2
1
Ã∏= œÉ2
2
{(x, y) ‚àà‚Ñùm+n : s2
x/s2
y > Fm‚àí1,n‚àí1; 1‚àíŒ±/2}
‚à™{(x, y) ‚àà‚Ñùm+n : s2
y/s2
x > Fn‚àí1,m‚àí1; 1‚àíŒ±/2}
8.5 Point estimators
Starting point is a parametric statistical model (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò. Assume we execute a sta-
tistical experiment and observe a sample x ‚ààùí≥. The aim of this section is to show how
this observation leads to a ‚Äúgood‚Äù estimate of the unknown parameter Œ∏ ‚ààŒò.
Example 8.5.1. Suppose the statistical model is (‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº, œÉ2
0)‚äón)Œº‚àà‚Ñùfor some
known œÉ2
0 > 0. Thus, the unknown parameter is the expected value Œº ‚àà‚Ñù. To estimate
it, we execute n independent measurements and get x = (x1, . . . , xn) ‚àà‚Ñùn. Knowing this

8.5 Point estimators
‡±™
415
vector x, what is a ‚Äúgood‚Äù estimate for Œº? An intuitive approach is to define the point
estimator ÃÇŒº : ‚Ñùn ‚Üí‚Ñùas
ÃÇŒº(x) = 1
n
n
‚àë
j=1
xj = ÃÑx ,
x = (x1, . . . , xn) ‚àà‚Ñùn .
In other words, if the observed sample is x, then we take its sample mean ÃÇŒº(x) =
ÃÑx as
an estimate for Œº. An immediate question is whether ÃÇŒº is a ‚Äúgood‚Äù estimator for Œº. Or do
there exist maybe ‚Äúbetter‚Äù (more precise) estimators for Œº?
Before we investigate such and similar questions, the problem has to be generalized
slightly. Sometimes it happens that we are not interested in the concrete value of the
parameter Œ∏ ‚ààŒò. We only want to know the value Œ≥(Œ∏) derived from Œ∏. Thus, for some
function Œ≥ : Œò ‚Üí‚Ñùwe want to find a ‚Äúgood‚Äù estimator ÃÇŒ≥ : ùí≥‚Üí‚Ñùfor Œ≥(Œ∏). In other
words, if we observe a sample x ‚ààùí≥, then we take ÃÇŒ≥(x) as an estimate for the (unknown)
value Œ≥(Œ∏). However, in most cases the function Œ≥ is not needed. That is, here we have
Œ≥(Œ∏) = Œ∏, and we look for a good estimator ÃÇŒ∏ : ùí≥‚ÜíŒò for Œ∏.
Let us state an example where a nontrivial function Œ≥ plays a role.
Example 8.5.2. Let (‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº, œÉ2)‚äón)(Œº,œÉ2)‚àà‚Ñù√ó(0,‚àû) be the statistical model. Thus,
the unknown parameter is the two-dimensional vector (Œº, œÉ2). But, in fact, we are only
interested in Œº, not in the pair (Œº, œÉ2). That is, if
Œ≥(Œº, œÉ2) := Œº ,
(Œº, œÉ2) ‚àà‚Ñù√ó (0, ‚àû) ,
then we want to find an estimate for Œ≥(Œº, œÉ2).
Analogously, if we only want an estimate for œÉ2, then we choose Œ≥ as
Œ≥(Œº, œÉ2) := œÉ2 ,
(Œº, œÉ2) ‚àà‚Ñù√ó (0, ‚àû) .
After these preliminary considerations, we state now the precise definition of an
estimator.
Definition 8.5.3. Let (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò be a parametric statistical model and let Œ≥ : Œò ‚Üí‚Ñùbe a function of
the parameter. A mapping ÃÇŒ≥ : ùí≥‚Üí‚Ñùis said to be a point estimator (or simply estimator) for Œ≥(Œ∏) if,
given t ‚àà‚Ñù, the set {x ‚ààùí≥: ÃÇŒ≥(x) ‚â§t} belongs to the œÉ-field ‚Ñ±. In other words, ÃÇŒ≥ is a random variable
defined on ùí≥.
The interpretation of this definition is as follows. If one observes the sample x ‚ààùí≥, then
ÃÇŒ≥(x) is an estimate for Œ≥(Œ∏). For example, if one measures a workpiece four times and gets
22.03, 21.87, 22.11, and 22,15 inches as results, then using the estimator ÃÇŒº in Example 8.5.2,
the estimate for the mean value equals 22.04 inches.

416
‡±™
8 Mathematical statistics
8.5.1 Maximum likelihood estimation
Let (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò be a parametric statistical model. There exist several methods to con-
struct ‚Äúgood‚Äù point estimators for the unknown parameter Œ∏. In this section we present
the probably most important of these methods, the so-called maximum likelihood
principle.
To understand this principle, the following easy example may be helpful.
Example 8.5.4. Suppose the parameter set consists of two elements, say Œò = {0, 1}. More-
over, also the sample space ùí≥has cardinality two, that is, ùí≥= {a, b}. Then the problem
is as follows. Depending on the observation a or b, we have to choose either 0 or 1 as an
estimate for Œ∏.
For example, let us assume that ‚Ñô0({a}) = 1/4, hence ‚Ñô0({b}) = 3/4, and
‚Ñô1({a}) = ‚Ñô1({b}) = 1/2. Say, an experiment has outcome ‚Äúa.‚Äù What would be a good
estimate for Œ∏ in this case? Should we take ‚Äú0‚Äù or ‚Äú1‚Äù? The answer is that we should
choose ‚Äú1.‚Äù Why? Because the sample ‚Äúa‚Äù fits ‚Ñô1 better than ‚Ñô0. By the same argument,
we should take ‚Äú0‚Äù as an estimate if we observe ‚Äúb.‚Äù Thus, the point estimator for Œ∏ is
given by ÃÇŒ∏(a) = 1 and ÃÇŒ∏(b) = 0.
Example 8.5.5. Let us transform the previous example into one of daily life. Say your
friend is planning to visit you. He will either arrive by train or by car. If he comes by
train, he will be on time with probability 3/4, and by car with probability 1/2. Say he
arrived on time. What would be your estimate for his choice? Do you guess he came by
train or do you conjecture that he used the car? Justify your answer. What if your friend
arrived late?
Which property characterizes the estimator ÃÇŒ∏ in Example 8.5.4? To answer this ques-
tion, fix x ‚ààùí≥and look at the function
Œ∏ Û≥®É‚Üí‚ÑôŒ∏({x}) ,
Œ∏ ‚ààŒò .
(8.35)
If x = a, this function becomes maximal for Œ∏ = 1, while for x = b it attains its maximal
value at Œ∏ = 0. Consequently, the estimator ÃÇŒ∏ could also be defined as follows. For each
fixed x ‚ààùí≥, choose as an estimate the Œ∏ ‚ààŒò for which the function (8.35) becomes
maximal. But this is exactly the approach of the maximum likelihood principle.
In order to describe this principle in the general setting, we have to introduce the
notion of the likelihood function. Let us first assume that the sample space ùí≥consists of
at most countably many elements.
Definition 8.5.6. The function p from Œò √ó ùí≥to ‚Ñùdefined as
p(Œ∏, x) := PŒ∏({x}) ,
Œ∏ ‚ààŒò ,
x ‚ààùí≥,
is called the likelihood function of the statistical model (ùí≥, ùí´(ùí≥), ‚ÑôŒ∏)Œ∏‚ààŒò.

8.5 Point estimators
‡±™
417
We come now to the case where all probability measures ‚ÑôŒ∏ are continuous. Thus, we
assume that the statistical model is (‚Ñùn, ‚Ñ¨(‚Ñùn), ‚ÑôŒ∏)Œ∏‚ààŒò and, moreover, each ‚ÑôŒ∏ is contin-
uous, that is, it has a density, mapping ‚Ñùn to ‚Ñù. This density is not only a function of
x ‚àà‚Ñùn, it also depends on the probability measure ‚ÑôŒ∏, hence on Œ∏ ‚ààŒò. Therefore, we
denote the densities by p(Œ∏, x). In other words, for each Œ∏ ‚ààŒò and each box Q ‚äÜ‚Ñùn as
in eq. (1.73) we have
‚ÑôŒ∏(Q) = ‚à´
‚Ñö
p(Œ∏, x) dx =
b1
‚à´
a1
‚ãÖ‚ãÖ‚ãÖ
bn
‚à´
an
p(Œ∏, x1, . . . , xn) dxn ‚ãÖ‚ãÖ‚ãÖdx1 .
(8.36)
Definition 8.5.7. The function p : Œò √ó ‚Ñùn ‚Üí‚Ñùsatisfying eq. (8.36) for all boxes Q and all Œ∏ ‚ààŒò is said
to be the likelihood function of the statistical model (‚Ñùn, ‚Ñ¨(‚Ñùn), ‚ÑôŒ∏)Œ∏‚ààŒò.
For a better understanding of Definitions 8.5.6 and 8.5.7, let us give some examples of
likelihood functions.
1.
First take (ùí≥, ùí´(ùí≥), Bn,Œ∏)0‚â§Œ∏‚â§1 with ùí≥= {0, . . . , n} from Section 8.3. Then its likeli-
hood function equals
p(Œ∏, k) = (n
k)Œ∏k(1 ‚àíŒ∏)n‚àík ,
Œ∏ ‚àà[0, 1] ,
k ‚àà{0, . . . , n} .
(8.37)
2.
Consider the statistical model (ùí≥, ùí´(ùí≥), HN,M,n)M=0,...,N investigated in Example 8.1.4.
Then its likelihood function is given by
p(M, m) =
(M
m)(N‚àíM
n‚àím)
(N
n)
,
M = 0, . . . , N ,
m = 0, . . . , n .
(8.38)
3.
The likelihood function of the model (‚Ñïn
0, ùí´(‚Ñïn
0), Pois‚äón
Œª )Œª>0 investigated in Exam-
ple 8.5.22 is
p(Œª, k1, . . . , kn) = Œªk1+‚ãÖ‚ãÖ‚ãÖ+kn
k1! ‚ãÖ‚ãÖ‚ãÖkn! e‚àíŒªn ,
Œª > 0 ,
kj ‚àà‚Ñï0 .
(8.39)
4.
The likelihood function of (‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº, œÉ2)‚äón)(Œº,œÉ2)‚àà‚Ñù√ó(0,‚àû) from Example 8.1.12
can be calculated by
p(Œº, œÉ2, x) =
1
(2œÄ)n/2œÉn exp(‚àí|x ‚àí‚ÉóŒº|2
2œÉ2
) ,
Œº ‚àà‚Ñù,
œÉ2 > 0 .
(8.40)
Here, as before, let ‚ÉóŒº = (Œº, . . . , Œº).
5.
The likelihood function of (‚Ñùn, ‚Ñ¨(‚Ñùn), E‚äón
Œª )Œª>0 from Example 8.1.9 may be repre-
sented as
p(Œª, t1, . . . , tn) = {Œªn e‚àíŒª(t1+‚ãÖ‚ãÖ‚ãÖ+tn)
if tj ‚â•0 , Œª > 0,
0
otherwise.
(8.41)

418
‡±™
8 Mathematical statistics
Definition 8.5.8. Let (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò be a parametric statistical model with likelihood function
p : Œò √ó ùí≥‚Üí‚Ñù. An estimator
ÃÇŒ∏ : ùí≥‚ÜíŒò is said to be a maximum likelihood estimator (MLE) for
Œ∏ ‚ààŒò provided that, for each x ‚ààùí≥, the following is satisfied:
p( ÃÇŒ∏(x), x) = max
Œ∏‚ààŒò p(Œ∏, x)
Remark 8.5.9. Another way to define the MLE is as follows:6
ÃÇŒ∏(x) = arg max
Œ∏‚ààŒò
p(Œ∏, x) ,
x ‚ààùí≥.
How does one find the MLE for concrete statistical models? One observation is that
the logarithm is an increasing function. Thus, the likelihood function p(‚ãÖ, x) becomes
maximal at a certain parameter Œ∏ ‚ààŒò if its logarithm ln p(‚ãÖ, x) does.
Definition 8.5.10. Let (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò be a statistical model and let p : Œò √ó ùí≥‚Üí‚Ñùbe its likelihood
function. Suppose p(Œ∏, x) > 0 for all (Œ∏, x). Then the function
L(Œ∏, x) := ln p(Œ∏, x) ,
Œ∏ ‚ààŒò ,
x ‚ààùí≥,
is called the log-likelihood function of the model.
Thus, ÃÇŒ∏ is an MLE if and only if
ÃÇŒ∏(x) = arg max
Œ∏‚ààŒò
L(Œ∏, x) ,
x ‚ààùí≥,
or, equivalently, if
L( ÃÇŒ∏(x), x) = max
Œ∏‚ààŒò L(Œ∏, x) .
Example 8.5.11. If p is the likelihood function in eq. (8.37), then the log-likelihood func-
tion equals
L(Œ∏, k) = c + k ln Œ∏ + (n ‚àík) ln(1 ‚àíŒ∏) ,
0 ‚â§Œ∏ ‚â§1 ,
k = 0, . . . , n .
(8.42)
Here c ‚àà‚Ñùdenotes a certain constant independent of Œ∏.
Example 8.5.12. The log-likelihood function of p in eq. (8.41) is well defined for Œª > 0
and tj ‚â•0. For those Œªs and tjs, it is given by
L(Œª, t1, . . . , tn) = n ln Œª ‚àíŒª(t1 + ‚ãÖ‚ãÖ‚ãÖ+ tn) .
6 If f is a real-valued function with domain A, then x = arg max
y‚ààA
f (y) if x ‚ààA and f (x) ‚â•f (y) for all
y ‚ààA. In other words, x is one of the points in the domain A where f attains its maximal value.

8.5 Point estimators
‡±™
419
To proceed further, we assume now that the parameter set Œò is a subset of ‚Ñùk for
some k ‚â•1. That is, each parameter Œ∏ consists of k unknown components, that is, it
may be written as Œ∏ = (Œ∏1, . . . , Œ∏k) with Œ∏j ‚àà‚Ñù. Furthermore, suppose that for each fixed
x ‚ààùí≥the log-likelihood function L(‚ãÖ, x) is continuously differentiable7 on Œò. Then points
Œ∏‚àó‚ààŒò where L(‚ãÖ, x) becomes maximal must satisfy
ùúï
ùúïŒ∏i
L(Œ∏, x)
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®Œ∏=Œ∏‚àó= 0 ,
i = 1, . . . , k .
(8.43)
In particular, this is true for the MLE ÃÇŒ∏(x). If for each x ‚ààùí≥, the log-likelihood function
L(‚ãÖ, x) is continuously differentiable on Œò ‚äÜ‚Ñùk, then the MLE ÃÇŒ∏ satisfies
ùúï
ùúïŒ∏i
L(Œ∏, x)
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®Œ∏= ÃÇŒ∏(x) = 0 ,
i = 1, . . . , k .
Example 8.5.13. Let us determine the MLE for the log-likelihood function in eq. (8.42).
Here we have Œò = [0, 1] ‚äÜ‚Ñù, hence the MLE ÃÇŒ∏ : {0, . . . , n} ‚Üí[0, 1] has to satisfy
ùúï
ùúïŒ∏L( ÃÇŒ∏(k), k) =
k
ÃÇŒ∏(k)
‚àí
n ‚àík
1 ‚àíÃÇŒ∏(k)
= 0 .
This easily gives ÃÇŒ∏(k) = k
n, that is, the MLE in this case is defined by
ÃÇŒ∏(k) = k
n ,
k = 0, . . . , n .
Let us interpret this result. In an urn there are white and black balls of unknown pro-
portion. Let Œ∏ be the proportion of white balls. To estimate Œ∏, draw n balls out of the urn,
with replacement. Assume k of the chosen balls are white. Then ÃÇŒ∏(k) = k
n is the MLE for
the unknown proportion Œ∏ of white balls.
Example 8.5.14. The logarithm of the likelihood function p in eq. (8.40) equals
L(Œº, œÉ2, x) = L(Œº, œÉ2, x1, . . . , xn) = c ‚àín
2 ‚ãÖln œÉ2 ‚àí
1
2œÉ2
n
‚àë
j=1
(xj ‚àíŒº)2
with some constant c ‚àà‚Ñù, independent of Œº and of œÉ2. Thus, here Œò ‚äÜ‚Ñù2, hence, if
Œ∏‚àó= (Œº‚àó, œÉ2‚àó) denotes the pair satisfying eq. (8.43), then
ùúï
ùúïŒºL(Œº, œÉ2, x)
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®(Œº,œÉ2)=(Œº‚àó,œÉ2‚àó)
= 0
and
ùúï
ùúïœÉ2 L(Œº, œÉ2, x)
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®(Œº,œÉ2)=(Œº‚àó,œÉ2‚àó)
= 0 .
7 The partial derivatives exist and are continuous.

420
‡±™
8 Mathematical statistics
Now
ùúï
ùúïŒºL(Œº, œÉ2, x) = 1
œÉ2
n
‚àë
j=1
(xj ‚àíŒº) = 1
œÉ2 [
n
‚àë
j=1
xj ‚àínŒº],
which implies Œº‚àó= 1
n ‚àën
j=1 xj = ÃÑx.
The derivative of L with respect to œÉ2, taken at Œº‚àó= ÃÑx, equals
ùúï
ùúïœÉ2 L( ÃÑx, œÉ2, x) = ‚àín
2 ‚ãÖ1
œÉ2 + 1
œÉ4
n
‚àë
j=1
(xj ‚àíÃÑx)2 .
It becomes zero at œÉ2‚àósatisfying
œÉ2‚àó= 1
n
n
‚àë
j=1
(xj ‚àíÃÑx)2 = œÉ2
x ,
where œÉ2
x was defined in eq. (8.14). Combining these observations, we see that the only
pair Œ∏‚àó= (Œº‚àó, œÉ2‚àó) satisfying eq. (8.43) is given by ( ÃÑx, œÉ2
x). Consequently, as MLE for Œ∏ =
(Œº, œÉ2) we obtain
ÃÇŒº(x) = ÃÑx
and
ÃÇ
œÉ2(x) = œÉ2
x ,
x ‚àà‚Ñùn .
Remark 8.5.15. Similar calculations as in the previous examples show that the MLE for
the likelihood functions in eqs. (8.39) and (8.41) coincide with
ÃÇŒª(k1, . . . , kn) = 1
n
n
‚àë
i=1
ki
and
ÃÇŒª(t1, . . . , tn) =
1
1
n ‚àën
i=1 ti
.
Finally, we present two likelihood functions where we have to determine their max-
imal values directly. Note that the above approach via the log-likelihood function does
not apply if the parameter set Œò is either finite or countably infinite. In this case a deriva-
tive of L(‚ãÖ, x) does not make sense, hence we cannot determine points where it vanishes.
The first problem is that discussed in Remark 1.4.33. A retailer gets a delivery of N
machines. Among the N machines are M defective. Since M is unknown, the retailer
wants a ‚Äúgood‚Äù estimate for it. Therefore, he chooses at random n machines and tests
them. Suppose he observes m defective machines among the tested. Does this lead to
an estimate of the number M of defective machines? The next proposition answers this
question.
Proposition 8.5.16. The statistical model is given by (ùí≥, ùí´(ùí≥), HM,N,n)M=0,...,N. Then the
MLE
ÃÇM for M is of the form
ÃÇM(m) = {‚åäm(N+1)
n
‚åã
if m < n,
N
if m = n.

8.5 Point estimators
‡±™
421
Here ‚åäx‚åãdenotes the floor function (integer part) of a real number x. For example, ‚åä1.2‚åã= 1
and ‚åäœÄ‚åã= 3.
Proof. The likelihood function p was determined in eq. (8.38) as
p(M, m) =
(M
m)(N‚àíM
n‚àím)
(N
n)
,
M = 0, . . . , N ,
m = 0, . . . , n .
First note that p(M, m)
Ã∏= 0 if and only if M ‚àà{m, . . . , N ‚àín+m} and, therefore, it suffices
to investigate p(M, m) for Ms in this region. Thus, if M ‚àí1 ‚â•m, then easy calculations
lead to
p(M, m)
p(M ‚àí1, m) =
M
M ‚àím ‚ãÖN ‚àíM + 1 ‚àí(n ‚àím)
N ‚àíM + 1
.
(8.44)
By eq. (8.44), it follows that we have p(M, m) ‚â•p(M ‚àí1, m) if and only if
M(N ‚àíM + 1 ‚àí(n ‚àím)) ‚â•(M ‚àím)(N ‚àíM + 1) .
Elementary transformations show the last estimate is equivalent to
‚àínM ‚â•‚àímN ‚àím ,
which happens if and only if M ‚â§m(N+1)
n
.
Consequently, M Û≥®É‚Üíp(M, m) is nondecreasing on {0, . . . , ‚åäm(N+1)
n
‚åã}, and it is nonin-
creasing on {‚åäm(N+1)
n
‚åã, . . . , N}. Thus, if m < n, then the likelihood function M Û≥®É‚Üíp(M, m)
becomes maximal for M‚àó= ‚åäm(N+1)
n
‚åã, and the MLE is given by
ÃÇM(m) = ‚åäm(N + 1)
n
‚åã,
m = 0, . . . , n ‚àí1 .
If m = n, then M Û≥®É‚Üíp(M, m) is nonincreasing on {0, . . . , N}, hence in this case the likeli-
hood function attains its maximal value at M = N, that is,
ÃÇM(n) = N.
Example 8.5.17. A retailer gets a delivery of 100 TV sets for further selling. He chooses at
random 15 sets and tests them. If there is exactly one defective TV set among the 15 tested,
then the estimate for the number of defective sets in the delivery is 6. If he observes 2
defective sets, the estimate is 13, for 4 it is 26, and if there are even 6 defective TV sets
among the 15 chosen, then the estimate is that 40 sets of the delivery are defective.
Finally, we come back to the question asked in Remark 1.4.36. In order to estimate
the number N of fish in a pond, one catches M of them, marks them and puts them back
into the pond. After some time one catches fish again, this time n of them. Among them
m are marked. Does this number m lead to a ‚Äúgood‚Äù estimate of the number of fish in
the pond? To describe this problem, we choose as statistical model

422
‡±™
8 Mathematical statistics
(ùí≥, ‚Ñô(ùí≥), HN,M,n)N=0,1,...
where ùí≥= {0, . . . , n}. Here HN,M,n denotes the hypergeometric probability measure in-
troduced in Definition 1.4.32. Thus, in this case the likelihood function is given by
p(N, m) =
(M
m)(N‚àíM
n‚àím)
(N
n)
,
N = 0, 1, . . . ,
m = 0, . . . , n .
In the sequel, we have to exclude m = 0; in this case, there does not exist a reasonable
estimate for N.
Proposition 8.5.18. If 1 ‚â§m ‚â§n, then the MLE
ÃÇN for N is
ÃÇN(m) = ‚åäMn
m ‚åã.
(8.45)
Proof. The proof is quite similar to that of Proposition 8.5.16. Since
p(N, m)
p(N ‚àí1, m) = N ‚àíM
N
‚ãÖ
N ‚àín
N ‚àíM ‚àí(n ‚àím) ,
it easily follows that the inequality p(N, m) ‚â•p(N ‚àí1, m) is valid if and only if N ‚â§Mn
m .
Therefore, N Û≥®É‚Üíp(N, m) is nondecreasing if N ‚â§‚åäMn
m ‚åãand nonincreasing for the re-
maining N. This immediately shows that the MLE is given by eq. (8.45).
Example 8.5.19. An unknown number of balls are in an urn. In order to estimate this
number, we choose 50 balls from the urn and mark them. We put back the marked balls
and mix the balls in the urn thoroughly. Then we choose another 30 balls from the urn.
If there are 7 marked among the 30, then the estimate for the number of balls in the urn
is 214. In the case of two marked balls, the estimate equals 750 while in the case of 16
marked balls we estimate that there are 93 balls in the urn.
Summary: Let (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò be a parametric statistical model. A function ÃÇŒ∏ : ùí≥‚ÜíŒò is said to be a point
estimator for the unknown parameter Œ∏. That is, observing a sample x ‚ààùí≥, then ÃÇŒ∏(x) is our estimate for Œ∏.
The basic idea of the maximum likelihood estimator (MLE) is as follows: observing an x ‚ààùí≥, one chooses
that Œ∏ for which the likelihood function p(Œ∏, x) = ‚ÑôŒ∏({x}) becomes maximal. This works well in the case of
discrete ‚ÑôŒ∏s. In the case of continuous ‚ÑôŒ∏s, one asks for the maximum of the densities p(Œ∏, x) of the ‚ÑôŒ∏s.
Thus, if x ‚ààùí≥, then ÃÇŒ∏(x) = arg max
Œ∏‚ààŒò
p(Œ∏, x) defines the MLE for this model. The logarithm is an increasing
function, so that this is equivalent to
ÃÇŒ∏(x) = arg max
Œ∏‚ààŒò
L(Œ∏, x) ,
x ‚ààùí≥,
where the log-likelihood function L is defined by L(Œ∏, x) = ln p(Œ∏, x).

8.5 Point estimators
‡±™
423
8.5.2 Unbiased estimators
Let us come back to the general setting. We are given a function Œ≥ : Œò ‚Üí‚Ñùand look
for a ‚Äúgood‚Äù estimate for Œ≥(Œ∏). If ÃÇŒ≥(x) is the estimate, in most cases it will not be the
correct value Œ≥(Œ∏). Sometimes the estimate is larger than Œ≥(Œ∏), sometimes one observes
an x ‚ààùí≥for which ÃÇŒ≥(x) is smaller than the true value. For example, if the retailer in
Example 8.5.17 gets every week a delivery of 100 TV sets, then sometimes his estimate
for the number of defective sets will be bigger than the true value, sometimes smaller.
Since he only pays for the nondefective sets, sometimes he pays too much, sometimes not
enough. Therefore, a crucial condition for a good estimator should be that, on average,
it meets the correct value. That is, in the long run, the loss and gain of the retailer should
balance. In other words, the estimator should not be biased by a systematic error.
In view of Proposition 7.1.30, this condition for the estimator ÃÇŒ≥ may be formulated
as follows. If Œ∏ ‚ààŒò is the ‚Äútrue‚Äù parameter, then the expected value of ÃÇŒ≥ should be Œ≥(Œ∏).
To make this more precise,8 we need the following notation.
Definition 8.5.20. Let (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò be a statistical model and let X : ùí≥‚Üí‚Ñùbe a random variable. We
write ùîºŒ∏X whenever the expected value of X is taken with respect to ‚ÑôŒ∏. Similarly, in this case define
ùïçŒ∏X = ùîºŒ∏|X ‚àíùîºŒ∏X|2
as variance of X. Of course, we have to assume that the expected value and/or the variance exist.
Remark 8.5.21. If X is discrete with values in {t1, t2, . . .}, then
ùîºŒ∏X =
‚àû
‚àë
j=1
tj ‚ÑôŒ∏{X = tj} .
The case of continuous X is slightly more difficult because here we have to describe the
density function of X with respect to ‚ÑôŒ∏.
To become acquainted with Definition 8.5.20, the two following examples may be
helpful. The first deals with the discrete case, while the second with the continuous
one.
Example 8.5.22. Suppose the daily number of customers in a shopping center is Pois-
son distributed with unknown parameter Œª > 0. To estimate this parameter, we record
the number of customers on n different days. Thus, the sample we obtain is a vector
‚Éók = (k1, . . . , kn) with kj ‚àà‚Ñï0, where kj is the number of customers on day j. The de-
scribing statistical model is given by (‚Ñïn
0, ùí´(‚Ñïn
0), Pois‚äón
Œª )Œª>0 with distribution PoisŒª. Let
X : ‚Ñïn
0 ‚Üí‚Ñùbe defined by
8 How the expected value is defined? Note that we do not have only one probability measure, but many
different ones.

424
‡±™
8 Mathematical statistics
X( ‚Éók) = X(k1, . . . , kn) := 1
n
n
‚àë
j=1
kj ,
‚Éók = (k1, . . . , kn) ‚àà‚Ñïn
0 .
Which value does ùîºŒªX possess?
Answer: If we choose Pois‚äón
Œª
as probability measure, then all Xjs defined by
Xj(k1, . . . , kn) := kj are PoisŒª-distributed (and independent, but this is not needed here).
Note that Xj is nothing else as the number of customers at day j. Hence, by Proposi-
tion 5.1.16, the expected value of Xj is Œª, and since X = 1
n ‚àën
j=1 Xj, we finally obtain
ùîºŒªX = ùîºŒª( 1
n
n
‚àë
j=1
Xj) = 1
n
n
‚àë
j=1
ùîºŒªXj = 1
n nŒª = Œª .
Example 8.5.23. Take
(‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº, œÉ2)
‚äón)(Œº,œÉ2)‚àà‚Ñù√ó(0,‚àû)
as the statistical model. Thus, the parameter is of the form (Œº, œÉ2) for some Œº ‚àà‚Ñùand
œÉ2 > 0. Define X : ‚Ñùn ‚Üí‚Ñùby X(x) =
ÃÑx. If the underlying measure is ùí©(Œº, œÉ2)‚äón, then9
X is ùí©(Œº, œÉ2/n)-distributed. Consequently, in view of Propositions 5.1.36 and 5.2.29, we
obtain
ùîºŒº,œÉ2X = Œº
and
VŒº,œÉ2X = œÉ2
n .
Using the notation introduced in Definition 8.5.20, the above-mentioned require-
ment for ‚Äúgood‚Äù estimators may now be formulated more precisely.
Definition 8.5.24. An estimator ÃÇŒ≥ : ùí≥‚Üí‚Ñùis said to be an unbiased estimator for Œ≥ : Œò ‚Üí‚Ñùprovided
that for each Œ∏ ‚ààŒò,
ùîºŒ∏| ÃÇŒ≥| < ‚àû
and
ùîºŒ∏ ÃÇŒ≥ = Œ≥(Œ∏) .
Remark 8.5.25. In view of Proposition 7.1.30, an estimator ÃÇŒ≥ is unbiased if it possesses
the following property: observe N independent samples x1, . . . , xN of a statistical ex-
periment. Suppose that Œ∏ ‚ààŒò is the ‚Äútrue‚Äù parameter (according to which the xjs are
distributed). Then
‚Ñô{ lim
N‚Üí‚àû
1
N
N
‚àë
j=1
ÃÇŒ≥(xj) = Œ≥(Œ∏)} = 1 .
Thus, on average, the estimator ÃÇŒ≥ approximately meets the correct value.
9 Compare with the first part of the proof of Proposition 8.4.3.

8.5 Point estimators
‡±™
425
Example 8.5.26. Let us investigate whether the estimator in Example 8.5.13 is unbiased.
The statistical model is (ùí≥, ùí´(ùí≥), Bn,Œ∏)0‚â§Œ∏‚â§1, where ùí≥= {0, . . . , n} and the estimator ÃÇŒ∏
acts as
ÃÇŒ∏(k) = k
n ,
k = 0, . . . , n .
Setting Z := n ÃÇŒ∏, then Z is the identity on ùí≥, hence Bn,Œ∏-distributed. Proposition 5.1.13
implies ùîºŒ∏Z = n Œ∏, thus,
ùîºŒ∏ ÃÇŒ∏ = ùîºŒ∏(Z/n) = ùîºŒ∏Z/n = Œ∏ .
(8.46)
Equation (8.46) holds for all Œ∏ ‚àà[0, 1], that is, ÃÇŒ∏ is an unbiased estimator for Œ∏.
Example 8.5.27. Next we come back to the problem presented in Example 8.5.22. The
number of customers per day is PoisŒª-distributed with an unknown parameter Œª > 0.
The data of n days are combined into a vector ÃÇk = (k1, . . . , kn) ‚àà‚Ñïn
0. Then the parameter
Œª > 0 is estimated by ÃÇŒª defined as
ÃÇŒª( ‚Éók) = ÃÇŒª(k1, . . . , kn) := 1
n
n
‚àë
j=1
kj .
Is this estimator for Œª unbiased?
Answer: Yes, it is unbiased. Observe that ÃÇŒª coincides with the random variable X
investigated in Example 8.5.22. There we proved ùîºŒª = Œª, hence, if Œª > 0, then we have
ùîºŒª ÃÇŒª = Œª .
Example 8.5.28. We are given certain data x1, . . . , xn, which are known to be normally
distributed and independent, and where the expected value Œº and the variance œÉ2 of the
underlying probability measure are unknown. Thus, the describing statistical model is
(‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº, œÉ2)
n)(Œº,œÉ2)‚ààŒò
with Œò = ‚Ñù√ó (0, ‚àû) .
The aim is to find unbiased estimators for Œº and for œÉ2. Let us begin with estimating Œº.
That is, if Œ≥ is defined by Œ≥(Œº, œÉ2) = Œº, then we want to construct an unbiased estimator
ÃÇŒ≥ for Œ≥. Let us take the MLE ÃÇŒ≥ defined as
ÃÇŒ≥(x) := ÃÑx = 1
n
n
‚àë
j=1
xj ,
x = (x1, . . . , xn) .
Due to the calculations in Example 8.5.23, we obtain
ùîºŒº,œÉ2 ÃÇŒ≥ = Œº = Œ≥(Œº, œÉ2) .
This holds for all Œº and œÉ2, hence ÃÇŒ≥ is an unbiased estimator for Œº = Œ≥(Œº, œÉ2).

426
‡±™
8 Mathematical statistics
How to find a suitable estimator for œÉ2? This time the function Œ≥ has to be chosen as
Œ≥(Œº, œÉ2) := œÉ2. With s2
x defined in eq. (8.14), set
ÃÇŒ≥(x) := s2
x =
1
n ‚àí1
n
‚àë
j=1
(xj ‚àíÃÑx)2 ,
x ‚àà‚Ñùn .
Is this an unbiased estimator for œÉ2? To answer this, we use property (8.17) of Proposi-
tion 8.4.3. It asserts that the random variable x Û≥®É‚Üí(n ‚àí1) s2
x
œÉ2 is œá2
n‚àí1-distributed, provided
it is defined on (‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(Œº, œÉ2)‚äón). Consequently, by Corollary 5.1.32, it follows that
ùîºŒº,œÉ2[(n ‚àí1) s2
x
œÉ2 ] = n ‚àí1 .
Using the linearity of the expected value, we finally obtain
ùîºŒº,œÉ2 ÃÇŒ≥ = ùîºŒº,œÉ2 s2
x = œÉ2 .
Therefore, ÃÇŒ≥(x) = s2
x is an unbiased10 estimator for œÉ2.
Remark 8.5.29. Taking the estimator ÃÇŒ≥(x) = œÉ2
x in the previous example, then, in view
of œÉ2
x = n‚àí1
n s2
x, it follows that
ùîºŒº,œÉ2 ÃÇŒ≥ = n ‚àí1
n
œÉ2 .
Thus, the estimator ÃÇŒ≥(x) = œÉ2
x is biased. But note that
lim
n‚Üí‚àû
n ‚àí1
n
œÉ2 = œÉ2 ,
hence, if the sample size n is big, then this estimator is ‚Äúalmost‚Äù unbiased. One says in
this case the sequence of estimators (in dependence on n) is asymptotically unbiased.
The next example is slightly more involved, but of great interest in application.
Example 8.5.30. The lifetime of light bulbs is supposed to be exponentially distributed
with some unknown parameter Œª > 0. To estimate Œª, we switch on n light bulbs and
record the times t1, . . . , tn when they burn out. Thus, the observed sample is a vector
t = (t1, . . . , tn) in (0, ‚àû)n. As an estimator for Œª we choose
ÃÇŒª(t) :=
n
‚àën
j=1 tj
= 1/ ÃÑt .
Is this an unbiased estimator for Œª?
10 This explains why s2
x is called the unbiased sample variance.

8.5 Point estimators
‡±™
427
Answer: The statistical model describing this experiment is
(‚Ñùn, ‚Ñ¨(‚Ñùn), E‚äón
Œª )Œª>0.
If the random variables Xj are defined by Xj(t) := tj, then they are independent and
EŒª-distributed. Because of Proposition 4.6.6, their sum X := ‚àën
j=1 Xj possesses an Erlang
distribution with parameters n and Œª. An application of eq. (5.24) in Proposition 5.1.38
for f (x) := n
x implies
ùîºŒª ÃÇŒª = ùîºŒª( n
X ) =
‚àû
‚à´
0
n
x
Œªn
(n ‚àí1)!xn‚àí1e‚àíŒªx dx .
A change of variables s = Œªx transforms the latter integral into
Œªn
(n ‚àí1)!
‚àû
‚à´
0
sn‚àí2 e‚àís ds =
Œªn
(n ‚àí1)! Œì(n ‚àí1) =
Œªn
(n ‚àí1)! ‚ãÖ(n ‚àí2)! = Œª ‚ãÖ
n
n ‚àí1 .
This tells us that ÃÇŒª is not an unbiased estimator for Œª. But, as mentioned in Remark 8.5.29
for œÉ2
x, the sequence of estimators is asymptotically unbiased as n ‚Üí‚àû.
Remark 8.5.31. If we replace the estimator in Example 8.5.30 by
ÃÇŒª(t) := n ‚àí1
‚àën
j=1 tj
=
1
1
n‚àí1 ‚àën
j=1 tj
,
t = (t1, . . . , tn) ,
then the previous calculations imply
ùîºŒª ÃÇŒª = n ‚àí1
n
‚ãÖŒª ‚ãÖ
n
n ‚àí1 = Œª .
Hence, from this small change we get an unbiased estimator ÃÇŒª for Œª.
Observe that the calculations in Example 8.5.30 were only valid for n ‚â•2. If n = 1,
then the expected value of ÃÇŒª does not exist.
Summary: Suppose we want to estimate the value Œ≥(Œ∏) for some function Œ≥ : Œò ‚Üí‚Ñù. Let ÃÇŒ≥ : ùí≥‚Üí‚Ñùbe
some point estimator for Œ≥(Œ∏). A basic property of a good estimator ÃÇŒ≥ is that it should be unbiased. That is,
on average, it should give us the correct value Œ≥(Œ∏), no matter which Œ∏ ‚ààŒò is the right one. In formulas, this
means that an estimator ÃÇŒ≥ is unbiased if given Œ∏ ‚ààŒò, ùîºŒ∏ ÃÇŒ≥ = Œ≥(Œ∏).
8.5.3 Risk function
Let (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò be a parametric statistical model. Furthermore, Œ≥ : Œò ‚Üí‚Ñùis a func-
tion of the parameter and ÃÇŒ≥ : ùí≥‚ÜíŒò is an estimator for Œ≥. Suppose Œ∏ ‚ààŒò is the true

428
‡±™
8 Mathematical statistics
parameter and we observe some x ‚ààùí≥. Then, in general, we will have Œ≥(Œ∏)
Ã∏= ÃÇŒ≥(x), and
the quadratic error |Œ≥(Œ∏) ‚àíÃÇŒ≥(x)|2 occurs. Other ways to measure the error are possible
and useful, but we restrict ourselves to the quadratic distance. In this way, we get the
so-called loss function L : Œò √ó ùí≥‚Üí‚Ñùof ÃÇŒ≥ defined by
L(Œ∏, x) := ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®Œ≥(Œ∏) ‚àíÃÇŒ≥(x)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
2 .
In other words, if Œ∏ is the correct parameter and our sample is x ‚ààùí≥, then, using ÃÇŒ≥ as
the estimator, the (quadratic) error or loss will be L(Œ∏, x). On average, the (quadratic)
loss is evaluated by ùîºŒ∏|Œ≥(Œ∏) ‚àíÃÇŒ≥|2 .
Definition 8.5.32. The function R describing this average loss of ÃÇŒ≥ is said to be the risk function of the
estimator ÃÇŒ≥. It is defined by
R(Œ∏, ÃÇŒ≥) := ùîºŒ∏
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®Œ≥(Œ∏) ‚àíÃÇŒ≥ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
2 ,
Œ∏ ‚ààŒò .
Before giving some examples of risk functions, let us rewrite R as follows.
Proposition 8.5.33. If Œ∏ ‚ààŒò, then it follows that
R(Œ∏, ÃÇŒ≥) = ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®Œ≥(Œ∏) ‚àíùîºŒ∏ ÃÇŒ≥ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
2 + ùïçŒ∏ ÃÇŒ≥ .
(8.47)
Proof. The assertion is a consequence of
R(Œ∏, ÃÇŒ≥) = ùîºŒ∏[Œ≥(Œ∏) ‚àíÃÇŒ≥]
2 = ùîºŒ∏[(Œ≥(Œ∏) ‚àíùîºŒ∏ ÃÇŒ≥) + (ùîºŒ∏ ÃÇŒ≥ ‚àíÃÇŒ≥)]
2
= ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®Œ≥(Œ∏) ‚àíùîºŒ∏ ÃÇŒ≥ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
2 + 2(Œ≥(Œ∏) ‚àíùîºŒ∏ ÃÇŒ≥) ùîºŒ∏(ùîºŒ∏ ÃÇŒ≥ ‚àíÃÇŒ≥) + ùïçŒ∏ ÃÇŒ≥ .
Because of
ùîºŒ∏(ùîºŒ∏ ÃÇŒ≥ ‚àíÃÇŒ≥) = ùîºŒ∏ ÃÇŒ≥ ‚àíùîºŒ∏ ÃÇŒ≥ = 0 ,
this implies eq. (8.47).
Definition 8.5.34. The function Œ∏ Û≥®É‚Üí|Œ≥(Œ∏) ‚àíùîºŒ∏ ÃÇŒ≥|2, appearing in eq. (8.47), is said to be the bias or the
systematic error of the estimator ÃÇŒ≥.
Corollary 8.5.35. A point estimator ÃÇŒ≥ is unbiased if and only if for all Œ∏ ‚ààŒò its bias is
zero. Moreover, if this is so, then its risk function is given by
R(Œ∏, ÃÇŒ≥) = ùïçŒ∏ ÃÇŒ≥ ,
Œ∏ ‚ààŒò .
Remark 8.5.36. Another way to formulate eq. (8.47) is as follows. The risk function of an
estimator consists of two parts. One part is the systematic error, which does not occur
for unbiased estimators. And the second part is given by ùïçŒ∏ ÃÇŒ≥. Thus, the smaller the bias

8.5 Point estimators
‡±™
429
and/or ùïçŒ∏ ÃÇŒ≥, the smaller the risk to get a wrong estimate for Œ≥(Œ∏), and the better the
estimator.
Example 8.5.37. Let us determine the risk functions for the two estimators presented
in Example 8.5.28. The estimator ÃÇŒ≥ for Œº was given by ÃÇŒ≥(x) = ÃÑx. Since this is an unbiased
estimator, by Corollary 8.5.35, its risk function is computed as
R((Œº, œÉ2), ÃÇŒ≥) = V(Œº,œÉ2) ÃÇŒ≥ .
The random variable x Û≥®É‚ÜíÃÑx is ùí©(Œº, œÉ2/n)-distributed, hence
R((Œº, œÉ2), ÃÇŒ≥) = œÉ2
n .
There are two interesting facts about this risk function. First, it does not depend on the
parameter Œº that we want to estimate. And secondly, if n ‚Üí‚àû, then the risk tends to
zero. In other words, the bigger the sample size, the less the risk for a wrong estimate.
Next we evaluate the risk function of the estimator ÃÇŒ≥(x) = s2
x. As we saw in Exam-
ple 8.5.30, this ÃÇŒ≥ is also an unbiased estimator for œÉ2, hence
R((Œº, œÉ2), ÃÇŒ≥) = V(Œº,œÉ2) ÃÇŒ≥ .
From eq. (8.17), we know that n‚àí1
œÉ2 s2
x is œá2
n‚àí1-distributed, hence Corollary 5.2.28 implies
ùïç(Œº,œÉ2)[n ‚àí1
œÉ2
s2
x] = 2 (n ‚àí1) .
From this, one easily derives
R((Œº, œÉ2), ÃÇŒ≥) = ùïç(Œº,œÉ2)s2
x = 2 (n ‚àí1) ‚ãÖ
œÉ4
(n ‚àí1)2 = 2œÉ4
n ‚àí1 .
Here, the risk function depends heavily on the parameter œÉ2 that we want to estimate.
Furthermore, if n ‚Üí‚àû, then also in this case the risk tends to zero.
Example 8.5.38. Finally, consider the statistical model (ùí≥, ùí´(ùí≥), Bn,Œ∏)0‚â§Œ∏‚â§1, where
ùí≥= {0, . . . , n}. In order to estimate Œ∏ ‚àà[0, 1], we take, as in Example 8.5.26, the estimator
ÃÇŒ∏(k) = k
n. There it was shown that the estimator is unbiased, hence, by Corollary 8.5.35,
it follows that
R(Œ∏, ÃÇŒ∏) = ùïçŒ∏ ÃÇŒ∏ ,
0 ‚â§Œ∏ ‚â§1 .
If X is the identity on ùí≥, by Proposition 5.2.18, its variance equals ùïçŒ∏X = n Œ∏(1‚àíŒ∏). Since
ÃÇŒ∏ = X
n , this implies
R(Œ∏, ÃÇŒ∏) = ùïçŒ∏(X/n) = ùïçŒ∏X
n2
= Œ∏(1 ‚àíŒ∏)
n
.

430
‡±™
8 Mathematical statistics
Consequently, the risk function becomes maximal for Œ∏ = 1/2, while for Œ∏ = 0 and Œ∏ = 1
it vanishes.
We saw in Corollary 8.5.35 that R(Œ∏, ÃÇŒ≥) = ùïçŒ∏ ÃÇŒ≥ for unbiased ÃÇŒ≥. Thus, for such estima-
tors inequality (7.2) implies
‚ÑôŒ∏{x ‚ààùí≥: ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®Œ≥(Œ∏) ‚àíÃÇŒ≥(x)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®> c} ‚â§ùïçŒ∏ ÃÇŒ≥
c2 ,
that is, the smaller the VŒ∏ ÃÇŒ≥, the greater the chance to estimate a value near the correct
one. This observation leads to the following definition.
Definition 8.5.39. Let ÃÇŒ≥1 and ÃÇŒ≥2 be two unbiased estimators for Œ≥(Œ∏). Then ÃÇŒ≥1 is said to be uniformly
better than ÃÇŒ≥2 provided that
VŒ∏ ÃÇŒ≥1 ‚â§ùïçŒ∏ ÃÇŒ≥2
for all Œ∏ ‚ààŒò .
An unbiased estimator ÃÇŒ≥‚àóis called the uniformly best estimator if it is uniformly better than all other
unbiased estimators for Œ≥(Œ∏).
Example 8.5.40. We observe values that, for some b > 0, are uniformly distributed on
[0, b]. But the number b > 0 is unknown. In order to estimate it, one executes n indepen-
dent trials and obtains as sample x = (x1, . . . , xn). As point estimators for b > 0 one may
either choose
ÃÇb1(x) := n + 1
n
max
1‚â§i‚â§n xi
or
ÃÇb2(x) := 2
n
n
‚àë
i=1
xi .
According to Problem 8.4, the estimators ÃÇb1 and ÃÇb2 are both unbiased. Furthermore, not
too difficult calculations show that
ùïçb ÃÇb1 =
b2
n(n + 2)
and
ùïçb ÃÇb2 = b2
3n2 .
Therefore, ùïçb ÃÇb1 ‚â§ùïçb ÃÇb2 for all b > 0. This tells us that ÃÇb1 is uniformly better than ÃÇb2.
Remark 8.5.41. A very natural question is whether there exists a lower bound for the
precision of an estimator. In other words, are there estimators for which the risk func-
tion becomes arbitrarily small? The answer depends heavily on the inherent informa-
tion in the statistical model. To explain this, let us come back once more to Example 8.5.4.
Suppose we had ‚Ñô0({a}) = 1 and ‚Ñô1({b}) = 1. Then the occurrence of ‚Äúa‚Äù would tell us
with 100 % confidence that Œ∏ = 0 is the correct parameter. The risk for the corresponding
estimator is then zero. On the contrary, if ‚Ñô0({a}) = ‚Ñô1({b}) = 1/2, then the occurrence
of ‚Äúa‚Äù or ‚Äúb‚Äù does us tell nothing about the correct parameter.
To make the previous observation more precise, we have to introduce some quantity
that measures the information contained in a statistical model.

8.5 Point estimators
‡±™
431
Definition 8.5.42. Let (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò be a statistical model with log-likelihood function L introduced in
Definition 8.5.10. For simplicity, assume Œò ‚äÜ‚Ñù. Then the function I : Œò ‚Üí‚Ñùdefined by
I(Œ∏) := ùîºŒ∏( ùúïL
ùúïŒ∏ )
2
is called the Fisher information of the model. Of course, we have to suppose that the derivatives and
the expected value exist.
Example 8.5.43. Let us investigate the Fisher information for the model treated in Ex-
ample 8.5.14. There we had
L(Œº, œÉ2, x) = L(Œº, œÉ2, x1, . . . , xn) = c ‚àín
2 ln œÉ2 ‚àí
1
2œÉ2
n
‚àë
j=1
(xj ‚àíŒº)2 .
Fix œÉ2 and take the derivative with respect to Œº. This leads to
ùúïL
ùúïŒº = n ÃÑx ‚àínŒº
œÉ2
,
hence
(ùúïL
ùúïŒº)
2
= n2
œÉ4 | ÃÑx ‚àíŒº|2 .
Recall that ÃÑx is ùí©(Œº, œÉ2/n)-distributed, hence the expected value of | ÃÑx ‚àíŒº|2 is nothing
else than the variance of ÃÑx, that is, it is œÉ2/n. Consequently,
I(Œº) = ùîºŒº,œÉ2(ùúïL
ùúïŒº)
2
= n2
œÉ4
œÉ2
n = n
œÉ2 .
The following result answers the above question: how precise can an estimator be-
come at the most?
Proposition 8.5.44 (Rao‚ÄìCram√©r‚ÄìFrechet). Let (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò be a parametric model for
which the Fisher information I : Œò ‚Üí‚Ñùexists. If ÃÇŒ∏ is an unbiased estimator for Œ∏, then
ùïçŒ∏ ÃÇŒ∏ ‚â•
1
I(Œ∏) ,
Œ∏ ‚ààŒò .
(8.48)
Remark 8.5.45. Estimators ÃÇŒ∏ that attain the lower bound in estimate (8.48) are said to
be efficient. That is, for those estimators ùïçŒ∏ ÃÇŒ∏ = 1/I(Œ∏) for all Œ∏ ‚ààŒò. In other words,
efficient estimators possess the best possible accuracy.
In view of Examples 8.5.37 and 8.5.43, for normally distributed populations the esti-
mator ÃÇŒº(x) = ÃÑx is an efficient estimator for Œº. Other efficient estimators are those inves-
tigated in Examples 8.5.27 and 8.5.13. On the other hand, the estimator for œÉ2 in Exam-

432
‡±™
8 Mathematical statistics
ple 8.5.28 is not efficient. But it can be shown that s2
x is a uniformly best estimator for œÉ2,
that is, there do not exist efficient estimators in this case.
Summary: Let ÃÇŒ≥ : ùí≥‚Üí‚Ñùbe a point estimator for Œ≥(Œ∏). Using ÃÇŒ≥ as estimator for Œ≥(Œ∏), the mean quadratic
error is measured by the risk function defined by
R(Œ∏, ÃÇŒ≥) := ùîºŒ∏
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®Œ≥(Œ∏) ‚àíÃÇŒ≥ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
2 ,
Œ∏ ‚ààŒò .
The smaller the risk function, the better the estimator ÃÇŒ≥. It holds that
R(Œ∏, ÃÇŒ≥) = ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®Œ≥(Œ∏) ‚àíùîºŒ∏ ÃÇŒ≥ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
2 + ùïçŒ∏ ÃÇŒ≥ .
The first term is the systematic error which vanishes in the case of unbiased estimators, hence then R(Œ∏, ÃÇŒ≥) =
ùïçŒ∏ ÃÇŒ≥. It depends on inner properties of the model (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò how small ùïçŒ∏ ÃÇŒ≥ can be chosen at most.
Estimators attaining this lower bound are said to be efficient.
8.6 Confidence regions and intervals
8.6.1 Construction of confidence regions
Point estimations provide us with a single value Œ∏ ‚ààŒò. Further work or necessary deci-
sions are then based on this estimated parameter. The disadvantage of this approach is
that we have no knowledge about the precision of the obtained value. Is the estimated
parameter far away from the true one or maybe very near? To explain the problem, let
us come back to the situation described in Example 8.5.17. If the retailer observes 4 de-
fective TV sets among 15 tested, then he estimates that there are 26 defective sets in the
delivery of 100. But he does not know how precise his estimate of 26 is. Maybe there are
much more defective sets in the delivery, or maybe less than 26. The only information
he has is that the estimates are correct on average. But this does not say anything about
the accuracy of a single estimate.
This disadvantage of point estimators is avoided when estimating a certain set of
parameters, not only a single point. Then the true parameter is contained with great
probability in this randomly chosen region. In most cases, these regions will be intervals
of real or natural numbers.
Definition 8.6.1. Suppose the parametric statistical model is (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò. A mapping C : ùí≥‚Üíùí´(Œò)
is called an interval estimator, provided for fixed Œ∏ ‚ààŒò,
{x ‚ààùí≥: Œ∏ ‚ààC(x)} ‚àà‚Ñ±.
(8.49)
Remark 8.6.2. A better notation for the mapping C would be region or set estimator
because C(x) ‚äÜŒò may be an arbitrary subset, not necessarily an interval, but ‚Äúinterval
estimator‚Äù is commonly accepted, therefore, we use it here also.

8.6 Confidence regions and intervals
‡±™
433
Remark 8.6.3. Condition (8.49) is quite technical and will play no role later on. But it is
necessary because otherwise the next definition does not make sense.
Definition 8.6.4. Let Œ± be a real number in (0, 1). Suppose an interval estimator C : ùí≥‚Üíùí´(Œò) satisfies,
for each Œ∏ ‚ààŒò, the condition
‚ÑôŒ∏{x ‚ààùí≥: Œ∏ ‚ààC(x)} ‚â•1 ‚àíŒ± .
(8.50)
Then C is said to be a 1 ‚àíŒ± interval estimator (also 1 ‚àíŒ± estimator). The sets C(x) ‚äÜŒò with x ‚ààùí≥are
called 1 ‚àíŒ± confidence regions or confidence intervals (sometimes also called 100(1 ‚àíŒ±)% confidence
regions or intervals).
How does an interval estimator apply? Suppose Œ∏ ‚ààŒò is the ‚Äútrue‚Äù parameter. In a statis-
tical experiment, one obtains some sample x ‚ààùí≥distributed according to ‚ÑôŒ∏. Depend-
ing on the observed sample x, we choose a set C(x) of parameters. Then with probability
greater than or equal to 1 ‚àíŒ±, the observed x ‚ààùí≥leads to a region C(x) of parameters
which contains the true parameter Œ∏.
Remark 8.6.5. It is important to say that the set C(x) is random, not the unknown pa-
rameter Œ∏ ‚ààŒò. Metaphorically speaking, a fish (the true parameter Œ∏) is in a pond at
some fixed but unknown spot. We execute a certain statistical experiment to get some
information about the place where the fish is situated. Depending on the result of the ex-
periment, we throw a net into the pond. Doing so, we know that with probability greater
than or equal to 1 ‚àíŒ±, the result of the experiment leads to a net that catches the fish. In
other words, the position of the fish is not random, it is the observed sample, hence also
the thrown net.
Remark 8.6.6. It is quite self-evident that one should try to choose the confidence sets
as small as possible, without violating condition (8.50). If we are not interested in ‚Äúsmall‚Äù
confidence sets, then we could always chose C(x) = Œò. This is not forbidden, but com-
pletely useless because we do not get any information about the true value Œ∏.
Construction of confidence regions via significance tests: For a better understanding of
the subsequent construction, let us shortly recall the main assertions about hypothesis
tests from a slightly different point of view.
Let (ùí≥, ‚Ñ±, ‚Ñôœë)œë‚ààŒò be a statistical model. We choose a fixed, but arbitrary, Œ∏ ‚ààŒò. With
this chosen Œ∏, we formulate the null hypothesis as ‚Ñç0 : œë = Œ∏. The alternative hypothesis
is then ‚Ñç1 : œë
Ã∏= Œ∏. Let T = (ùí≥0, ùí≥1) be an Œ±-significance test for ‚Ñç0 against ‚Ñç1. Because
the hypothesis, hence also the test, depends on the chosen Œ∏ ‚ààŒò, we denote the null
hypothesis by ‚Ñç0(Œ∏) and write T(Œ∏) = (ùí≥0(Œ∏), ùí≥1(Œ∏)) for the test. That is, ‚Ñç0(Œ∏) : œë = Œ∏
and T(Œ∏) is an Œ±-significance test for ‚Ñç0(Œ∏). With this notation set (compare Figure 8.9)
C(x) := {Œ∏ ‚ààŒò : x ‚ààùí≥0(Œ∏)} .
(8.51)

434
‡±™
8 Mathematical statistics
Figure 8.9: The equivalence between x ‚ààùí≥0(Œ∏) and Œ∏ ‚ààC(x).
Remark 8.6.7. Verbally said, an index Œ∏ ‚ààŒò belongs to C(x) provided the observation
of x ‚ààùí≥supports the hypothesis that Œ∏ is the correct parameter. For example, assume
that there are only finitely many indices Œ∏1, . . . , Œ∏n. Then one may formulate n different
hypothesis, namely ‚Ñç0(Œ∏1) : œë = Œ∏1 up to ‚Ñç0(Œ∏n) : œë = Œ∏n. Applying to each of these
hypotheses an Œ±-significance test, we obtain n regions ùí≥0(Œ∏1) up to ùí≥0(Œ∏n) of acceptance.
Now, observing x ‚ààùí≥, we choose those Œ∏j for which x ‚ààùí≥0(Œ∏j). That is, observing x ‚ààùí≥,
for those Œ∏j this would not lead to a rejection of ‚Ñç0(Œ∏j). So we set
C(x) = {Œ∏j : x ‚ààùí≥0(Œ∏j)} .
Example 8.6.8. Choose the hypothesis and the test as in Proposition 8.4.15. The statis-
tical model is then given by (‚Ñùn, ‚Ñ¨(‚Ñùn), ùí©(ŒΩ, œÉ2
0)‚äón)ŒΩ‚àà‚Ñù, where this time we denote the
unknown expected value by ŒΩ. For some fixed, but arbitrary, Œº ‚àà‚Ñùlet
‚Ñç0(Œº) : ŒΩ = Œº
and
‚Ñç1(Œº) : ŒΩ
Ã∏= Œº .
The Œ±-significance test T(Œº) constructed in Proposition 8.4.15 possesses the region of ac-
ceptance
ùí≥0(Œº) = {x ‚àà‚Ñùn : ‚àön
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
ÃÑx ‚àíŒº
œÉ0
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§z1‚àíŒ±/2} .
Thus, in this case, the set C(x) in eq. (8.51) consists of those Œº ‚àà‚Ñùthat satisfy the estimate
‚àön | ÃÑx‚àíŒº
œÉ0 | ‚â§z1‚àíŒ±/2. That is, given x ‚àà‚Ñùn, then C(x) is the interval
C(x) = [ ÃÑx ‚àíœÉ0
‚àön z1‚àíŒ±/2 ,
ÃÑx + œÉ0
‚àön z1‚àíŒ±/2] .
Let us come back to the general situation. The statistical model is (ùí≥, ‚Ñ±, ‚Ñôœë)œë‚ààŒò.
Given Œ∏ ‚ààŒò, let T(Œ∏) be an Œ±-significance test for ‚Ñç0(Œ∏) against ‚Ñç1(Œ∏) where ‚Ñç0(Œ∏) is the
hypothesis ‚Ñç0(Œ∏) : œë = Œ∏. Given x ‚ààùí≥, define C(x) ‚äÜŒò by eq. (8.51). Then the following
is valid.
Proposition 8.6.9. Let T(Œ∏) be as above an Œ±-significance test for ‚Ñç0(Œ∏) against ‚Ñç1(Œ∏).
Define C(x) by eq. (8.51) where ùí≥0(Œ∏) denotes the region of acceptance of T(Œ∏). Then the

8.6 Confidence regions and intervals
‡±™
435
mapping x Û≥®É‚ÜíC(x) from ùí≥into ùí´(Œò) is a 1 ‚àíŒ± interval estimator. Hence, {C(x) : x ‚ààùí≥} is
a collection of 1 ‚àíŒ± confidence regions.
Proof. By assumption, T(Œ∏) is an Œ±-significance test for ‚Ñç0(Œ∏). The definition of those
tests tells us that
‚ÑôŒ∏(ùí≥1(Œ∏)) ‚â§Œ± ,
hence ‚ÑôŒ∏(ùí≥0(Œ∏)) ‚â•1 ‚àíŒ± .
Given Œ∏ ‚ààŒò and x ‚ààùí≥, by the construction of C(x), one has Œ∏ ‚ààC(x) if and only if
x ‚ààùí≥0(Œ∏). Combining these two observations, given Œ∏ ‚ààŒò, then it follows that
‚ÑôŒ∏{x ‚ààùí≥: Œ∏ ‚ààC(x)} = ‚ÑôŒ∏{x ‚ààùí≥: x ‚ààùí≥0(Œ∏)} = ‚ÑôŒ∏(ùí≥0(Œ∏)) ‚â•1 ‚àíŒ± .
This completes the proof.
Summary: Suppose the parametric statistical model is (ùí≥, ‚Ñ±, ‚ÑôŒ∏)Œ∏‚ààŒò. To each x ‚ààùí≥, we assign a subset
C(x) ‚äÜŒò such that for a given Œ± > 0 the following holds: if ‚ÑôŒ∏ is the true probability measure, then with
probability greater than 1 ‚àíŒ± we observe an x ‚ààùí≥such that Œ∏ ‚ààC(x). That is, for all Œ∏ ‚ààŒò it follows
that
‚ÑôŒ∏{x ‚ààùí≥: Œ∏ ‚ààC(x)} ‚â•1 ‚àíŒ± .
The (random) sets C(x) are called 1 ‚àíŒ± confidence sets.
There exists a tight relation between the construction of the confidence sets C(x) and hypothesis tests.
If T(Œ∏) = (ùí≥0(Œ∏), ùí≥1(Œ∏)) is an Œ±-test for the hypothesis ‚Ñç0 : ‚ÄúŒ∏ is the true parameter,‚Äù then
C(x) = {Œ∏ ‚ààŒò : x ‚ààùí≥0(Œ∏)}
are 1‚àíŒ± confidence sets. Verbally said, a parameter Œ∏ belongs to C(x) if the occurrence of x does not contradict
the hypotheses that Œ∏ is the correct parameter.
Test of a hypothesis ‚Ñç0
‚áí
fixed region ùí≥0 ‚äÜùí≥of acceptance,
Confidence regions
‚áí
random region C(x) ‚äÜŒò of probable parameters.
8.6.2 Normally distributed samples
The aim of this section is to apply Proposition 8.6.9 to transform results about two-sided
significance tests for normally distributed samples into assertions about confidence in-
tervals.
We start with the application of the two-sided Z-test for ùí©(Œº, œÉ2
0)-distributed sam-
ples with known variance œÉ2
0 > 0.
Proposition 8.6.10. Let x = (x1, . . . , xn) be a sample of n independent ùí©(Œº, œÉ2
0) distributed
numbers. Here Œº ‚àà‚Ñùis unknown while œÉ2
0 is known. Then with probability greater than

436
‡±™
8 Mathematical statistics
1 ‚àíŒ± the observed sample x ‚àà‚Ñùn leads to11
ÃÑx ‚àíœÉ0
‚àön z1‚àíŒ±/2 ‚â§Œº ‚â§ÃÑx + œÉ0
‚àön z1‚àíŒ±/2 .
Recall that zŒ≤ denotes the Œ≤-quantile of the standard normal distribution.
Proof. This is a direct consequence of applying Proposition 8.6.9 to the result presented
in Example 8.6.8.
Example 8.6.11. Choose Œ± = 0.05 and suppose we observed the nine values
10.1 ,
9.2 ,
10.2 ,
10.3 ,
10.1 ,
9.9 ,
10.0 ,
9.7 ,
9.8 ,
then ÃÑx = 9.9222. The variance œÉ0 is known to be œÉ2
0 = 0.330824. That is, we assume that
s2
x is the correct variance. Because of z1‚àíŒ±/2 = z0.975 = 1.95996, with a confidence of 95 %
our sample of nine numbers leads to
9.7061 ‚â§Œº ‚â§10.1384 .
In the next result we describe the confidence intervals generated by the t-test
treated in Proposition 8.4.15.
Proposition 8.6.12. Let x = (x1, . . . , xn) be a sample of n independent ùí©(Œº, œÉ2) distributed
numbers. Here Œº ‚àà‚Ñùand œÉ2 are both unknown. Then with probability greater than 1 ‚àíŒ±
the observed sample x ‚àà‚Ñùn leads to
ÃÑx ‚àísx
‚àön tn‚àí1;1‚àíŒ±/2 ‚â§Œº ‚â§
ÃÑx + sx
‚àön tn‚àí1;1‚àíŒ±/2 .
Recall that tn‚àí1;Œ≤ denotes the Œ≤-quantile of the Student tn‚àí1-distribution.
Proof. This is a direct consequence of
ùí≥0(Œº) = {x ‚àà‚Ñùn : ‚àön
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
ÃÑx ‚àíŒº
sx
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§tn‚àí1;1‚àíŒ±/2} ,
hence Proposition 8.6.9 implies that, given x ‚àà‚Ñùn, then
C(x) = {Œº ‚àà‚Ñù: ‚àön
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
ÃÑx ‚àíŒº
sx
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§tn‚àí1;1‚àíŒ±/2}
= [ ÃÑx ‚àísx
‚àön tn‚àí1;1‚àíŒ±/2 ,
ÃÑx + sx
‚àön tn‚àí1;1‚àíŒ±/2] .
11 We want to point this out again: not Œº is random but the chosen interval is such. And 1 ‚àíŒ± is the
probability to observe an x for which the random interval contains the correct Œº.

8.6 Confidence regions and intervals
‡±™
437
Example 8.6.13. Let us explain the previous result by the concrete sample investigated
in Example 8.4.20. There we had ÃÑx = 22.072, sx = 0.07554248, and n = 10. If Œ± = 0.05,
the quantile of t9 equals t9;0.975 = 2.26. From this, we derive [22.016, 22.126] as the 95 %
confidence interval.
Verbally this says that with a confidence of 95 % we observed those x1, . . . , x10 for
which Œº ‚ààC(x) = [22.016, 22.126].
Finally, let us construct 1 ‚àíŒ± confidence intervals for the unknown variance of a
normal sample.
Proposition 8.6.14. Let x = (x1, . . . , xn) be a sample of n independent ùí©(Œº, œÉ2) distributed
numbers. If Œº is known, then with probability greater than 1‚àíŒ± the observed sample x ‚àà‚Ñùn
leads to
nœÉ2
x
œá2
n;1‚àíŒ±/2
‚â§œÉ2 ‚â§nœÉ2
x
œá2
n;Œ±/2
where, in contrast to eq. (8.14),
œÉ2
x = 1
n
n
‚àë
j=1
(xj ‚àíŒº)2 .
(8.52)
If, on the contrary, Œº is unknown, then with probability greater than 1 ‚àíŒ± the observed
sample x ‚àà‚Ñùn gives
(n ‚àí1) s2
x
œá2
n‚àí1;1‚àíŒ±/2
‚â§œÉ2 ‚â§(n ‚àí1) s2
x
œá2
n‚àí1;Œ±/2
.
Here s2
x is as in Definition 8.4.1 and œá2
n;Œ≤ denotes the Œ≤-quantile of a œá2
n distribu-
tion.
Proof. Both assertions easily follow from eqs. (8.27) and (8.28). Recall that they imply for
known mean value Œº that
ùí≥0(œÉ2) = {x ‚àà‚Ñùn : œá2
n;Œ±/2 ‚â§
n
‚àë
j=1
(xj ‚àíŒº)2
œÉ2
‚â§œá2
n;1‚àíŒ±/2}
while in the case of unknown Œº one has
ùí≥0(œÉ2) = {x ‚àà‚Ñùn : œá2
n‚àí1;Œ±/2 ‚â§(n ‚àí1) s2
x
œÉ2 ‚â§œá2
n‚àí1;1‚àíŒ±/2}.
Letting in both cases C(x) = {œÉ2 > 0 : x ‚ààùí≥0(œÉ2)} completes the proof.
Example 8.6.15. A measuring instrument possesses an unknown precision. In order to
get some information about it, we measure a certain item 9 times. The obtained re-

438
‡±™
8 Mathematical statistics
sults are
10.1,
10.3,
10.2,
10.7,
9.9,
10.0,
10.9,
8.9
and
11.0 .
Let x = (x1, . . . , x9) be the collection of these measurements. Then the mean value equals
ÃÑx = 10.222, hence the unbiased variance of the observed sample is calculated by
s2
x = 1
8
9
‚àë
j=1
(xj ‚àíÃÑx)2 = 0.4019 .
If we choose Œ± = 0.1 as a significance level, then, in order to determine a 90 % confidence
interval, we need the 0.05 and 0.95 quantiles of a œá2
8 distribution. They are
œá2
8;0.05 = 2.73264
and
œá2
8;0.95 = 15.5073 .
So we finally obtain that there is chance of 90 % that the unknown variance œÉ2 of the
measuring instrument satisfies
8 ‚ãÖ0.4019
15.5073
= 0.207334 ‚â§œÉ2 ‚â§8 ‚ãÖ0.4019
2.73264
= 1.17659 .
Note that another n measurements by this instrument, of this or of a different item, will
surely lead to different bounds for œÉ2.
To conclude this section, let us summarize the most important confidence intervals
for normally distributed samples. Here œÉ2
x and s2
x are defined by eqs. (8.52) and (8.14),
respectively.
Name
Parameters
1 ‚àíŒ± Confidence Intervals
Confidence intervals for the mean value
œÉ2 > 0 known
[ ÃÑx ‚àí
œÉ
‚àön z1‚àíŒ±/2 ,
ÃÑx +
œÉ
‚àön z1‚àíŒ±/2]
Confidence intervals for the mean value
œÉ2 > 0 unknown
[ ÃÑx‚àísx
‚àön tn‚àí1;1‚àíŒ±/2 ,
ÃÑx+ sx
‚àön tn‚àí1;1‚àíŒ±/2]
Confidence intervals for the variance
Œº ‚àà‚Ñùknown
[
n œÉ2
x
œá2
n;1‚àíŒ±/2
,
n œÉ2
x
œá2
n;Œ±/2
]
Confidence intervals for the variance
Œº ‚àà‚Ñùunknown
[ (n‚àí1) s2
x
œá2
n‚àí1;1‚àíŒ±/2
,
(n‚àí1) s2
x
œá2
n‚àí1;Œ±/2
]
8.6.3 Binomial distributed populations
The aim of this section is to show how Proposition 8.6.9 applies in the case of binomial
distributed populations. Thus, we execute n independent trials where every time occurs
either success or failure. Hence, the number of successes is Bn,Œ∏-distributed for a certain

8.6 Confidence regions and intervals
‡±™
439
Œ∏ ‚àà[0, 1]. But, in contrast to the investigations in Section 1.4.3, now the parameter Œ∏ is
unknown.
Say we observed 0 ‚â§k ‚â§n times success. Then we look for a confidence interval
C(k) ‚äÜ[0, 1] such that very likely Œ∏ ‚ààC(k). More precisely, given 0 < Œ± < 1, we want that
for any parameter Œ∏ ‚àà[0, 1],
Bn,Œ∏{k ‚â§n : Œ∏ ‚ààC(k)} ‚â•1 ‚àíŒ± .
That is, with probability greater than 1 ‚àíŒ± the observation of k successes leads to an
interval C(k) containing the correct parameter Œ∏.
Proposition 8.6.16. The statistical model is (ùí≥, ùí´(ùí≥), Bn,Œ∏)0‚â§Œ∏‚â§1 where the sample space
is ùí≥= {0, . . . , n}. Given Œ± > 0 and k = 0, . . . , n, define sets C(k) ‚äÜ[0, 1] as follows:
C(k) = {Œ∏ : Bn,Œ∏({0, . . . , k}) > Œ±/2} ‚à©{Œ∏ : Bn,Œ∏({k, . . . , n}) > Œ±/2}.
(8.53)
Then for each k ‚â§n, the set C(k) is an 1 ‚àíŒ± confidence interval for Œ∏ ‚àà[0, 1].
Proof. In order to get these confidence regions, we use Proposition 8.6.9. As shown in
Proposition 8.3.1, the region of acceptance ùí≥0(Œ∏) of an Œ±-significance test T(Œ∏), where
‚Ñç0 : œë = Œ∏, is given by
ùí≥0(Œ∏) = {n0(Œ∏), . . . , n1(Œ∏)} .
Here, the numbers n0(Œ∏) and n1(Œ∏) were defined by
n0(Œ∏) := min{k ‚â§n :
k
‚àë
j=0
(n
j )Œ∏j(1 ‚àíŒ∏)n‚àíj > Œ±/2}
and
n1(Œ∏) := max{k ‚â§n :
n
‚àë
j=k
(n
j )Œ∏j(1 ‚àíŒ∏)n‚àíj > Œ±/2} .
Applying Proposition 8.6.9, the sets
C(k) := {Œ∏ ‚àà[0, 1] : k ‚ààùí≥0(Œ∏)} = {Œ∏ ‚àà[0, 1] : n0(Œ∏) ‚â§k ‚â§n1(Œ∏)} ,
k = 0, . . . , n ,
are 1 ‚àíŒ± confidence regions. By the definition of n0(Œ∏) and n1(Œ∏), given k ‚â§n, then a
number Œ∏ ‚àà[0, 1] satisfies n0(Œ∏) ‚â§k ‚â§n1(Œ∏) if and only if at the same time
Bn,Œ∏({0, . . . , k}) =
k
‚àë
j=0
(n
j )Œ∏j(1 ‚àíŒ∏)n‚àíj > Œ±/2
and
Bn,Œ∏({k, . . . , n}) =
n
‚àë
j=k
(n
j )Œ∏j(1 ‚àíŒ∏)n‚àíj > Œ±/2 .

440
‡±™
8 Mathematical statistics
Thus, as claimed,
C(k) = {Œ∏ : Bn,Œ∏({0, . . . , k}) > Œ±/2} ‚à©{Œ∏ : Bn,Œ∏({k, . . . , n}) > Œ±/2}
are 1 ‚àíŒ± confidence sets.
It remains to prove that these are indeed intervals, which by the definition of the
sets C(k) is not so obvious. We know by Lemma 8.3.6 that the function
f{‚â•k} : Œ∏ Û≥®É‚Üí
n
‚àë
j=k
(n
j )Œ∏j(1 ‚àíŒ∏)n‚àíj
(8.54)
is nondecreasing for any k ‚â•0. Hence,
f{‚â§k} : Œ∏ Û≥®É‚Üí
k
‚àë
j=0
(n
j )Œ∏j(1 ‚àíŒ∏)n‚àíj = 1 ‚àí
n
‚àë
j=k+1
(n
j )Œ∏j(1 ‚àíŒ∏)n‚àíj
(8.55)
is nonincreasing. Letting
Œ∏‚àí
k = inf{Œ∏ : f{‚â•k}(Œ∏) > Œ±
2 }
and
Œ∏+
k = sup{Œ∏ : f{‚â§k}(Œ∏) > Œ±
2 } ,
it follows that C(k) = (Œ∏‚àí
k, Œ∏+
k). This completes the proof.
Remark 8.6.17. The intervals C(k) = (Œ∏‚àí
k, Œ∏+
k) are usually called 100(1 ‚àíŒ±)% Clopper‚Äì
Pearson intervals or also exact confidence intervals for the binomial distribution.
Since the functions f{‚â§k} and f{‚â•k} are continuous on [0, 1], in the case 1 < k < n, the
numbers Œ∏‚àí
k and Œ∏+
k are also characterized by
f{‚â•k}(Œ∏‚àí
k) = Œ±
2
and
f{‚â§k}(Œ∏+
k) = Œ±
2 .
In other words, if 1 < k < n, then the endpoints of the Clopper‚ÄìPearson intervals are
the unique solution Œ∏ ‚àà(0, 1) of
n
‚àë
j=k
(n
j )Œ∏j(1 ‚àíŒ∏)n‚àíj = Œ±
2
or
k
‚àë
j=0
(n
j )Œ∏j(1 ‚àíŒ∏)n‚àíj = Œ±
2 ,
respectively. For the cases k = 0 and k = n, we refer to Problem 8.6.
Example 8.6.18. Suppose we execute n = 20 trials and observe k = 5 successes. What
can be said about the underlying success probability Œ∏? The functions in eqs. (8.54)
and (8.55) are in this case given by
f{‚â•5}(Œ∏) =
20
‚àë
j=5
(20
j )Œ∏j(1 ‚àíŒ∏)20‚àíj
and
f{‚â§5}(Œ∏) =
5
‚àë
j=0
(20
j )Œ∏j(1 ‚àíŒ∏)20‚àíj .

8.6 Confidence regions and intervals
‡±™
441
If Œ± = 0.1, then numerical calculations lead to (see also Fig. 8.10)
Œ∏‚àí
5 = inf{Œ∏ : f{‚â•5}(Œ∏) > 0.05} ‚âà0.1041
and
Œ∏+
5 = sup{Œ∏ : f{‚â§5}(Œ∏) > 0.05} ‚âà0.4566 .
Figure 8.10: The increasing function f{‚â•5} and the decreasing one f{‚â§5}, both taken with for n = 20. The
horizontal line marks the significance level Œ±/2 = 0.05.
Consequently, with the probability of 90 % our observation of five successes implies that
the underlying success probability Œ∏ satisfies
0.1041 < Œ∏ < 0.4566 .
(8.56)
The estimates for Œ∏ obtained in (8.56) are quite rough. This is mainly due to the fact
that the number n = 20 of trials is pretty small. Also different numbers of success do not
yield significantly tighter bounds. So, for example, if one observes 10 successes, then as
the 90 % confidence interval one gets
0.30196 < Œ∏ < 0.69804 .
The next example provides sharper bounds for larger n ‚â•1.
Example 8.6.19. In an urn there are white and black balls with an unknown proportion
Œ∏ of white balls. In order to get some information about Œ∏, we choose randomly 500 balls
with replacement. Say 220 of the chosen balls are white. What is the 90 % confidence
interval for Œ∏ based on this observation?
Answer: We have n = 500 and observed k = 220 white balls. Consequently, a 90 %
confidence interval C(220) consists of those Œ∏ ‚àà[0, 1] for which at the same time
f{‚â•220}(Œ∏) > Œ±/2 = 0.05
and
f{‚â§220}(Œ∏) > Œ±/2 = 0.05 .

442
‡±™
8 Mathematical statistics
Note that in this case
f{‚â§220}(Œ∏) =
220
‚àë
j=0
(500
j )Œ∏j(1 ‚àíŒ∏)500‚àíj
and
f{‚â•220}(Œ∏) =
500
‚àë
j=220
(500
j )Œ∏j(1 ‚àíŒ∏)500‚àíj .
Numerical calculations tell us that
Œ∏‚àí
220 ‚âà0.4028
and
Œ∏+
220 ‚âà0.4777 .
Therefore, a 90 % confidence interval C(220) is given by
C(220) = (0.4028, 0.4777) .
For n = 1000 and 440 observed white balls, the calculations lead to the smaller, hence
more significant, interval C(440) = (0.4139, 0.4664) .
Remark 8.6.20. The previous example already indicates that the determination of the
Clopper‚ÄìPearson intervals becomes quite complicated for large n. Therefore, one looks
for ‚Äúapproximate‚Äù intervals. Background for the construction is the central limit theo-
rem in the form presented in Proposition 7.2.19. For Sns distributed according to Bn,Œ∏, it
implies
lim
n‚Üí‚àû‚Ñô{
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
Sn ‚àínŒ∏
‚àönŒ∏(1 ‚àíŒ∏)
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§z1‚àíŒ±/2} = 1 ‚àíŒ± ,
or, equivalently,
lim
n‚Üí‚àûBn,Œ∏{k ‚â§n :
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
k ‚àínŒ∏
‚àönŒ∏(1 ‚àíŒ∏)
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§z1‚àíŒ±/2} = 1 ‚àíŒ± .
Here z1‚àíŒ±/2 is the 1 ‚àíŒ±/2 quantile introduced in Definition 8.4.8. Thus, an ‚Äúapproximate‚Äù
region of acceptance, testing the hypothesis ‚Äúthe unknown parameter is Œ∏,‚Äù is given
by
ùí≥0(Œ∏) = {k ‚â§n :
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
k
n ‚àíŒ∏
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§z1‚àíŒ±/2‚àöŒ∏(1 ‚àíŒ∏)
n
} .
(8.57)
An application of Proposition 8.6.9 leads to certain confidence regions, but mostly they
cannot be described explicitly. Due to the term ‚àöŒ∏(1 ‚àíŒ∏) on the right-hand side of
eq. (8.57), it is not possible, for a given k ‚â§n, to determine those Œ∏s for which k ‚ààùí≥0(Œ∏).
To overcome this difficulty, we change ùí≥0(Œ∏) yet again by replacing Œ∏ on the right-hand
side by its MLE ÃÇŒ∏(k) = k
n. That is, we replace eq. (8.57) by

8.6 Confidence regions and intervals
‡±™
443
ÃÉ
ùí≥0(Œ∏) = {k ‚â§n :
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
k
n ‚àíŒ∏
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§z1‚àíŒ±/2‚àö
k
n(1 ‚àík
n)
n
} .
Doing so, an application of Proposition 8.6.9 leads to the ‚Äúapproximate‚Äù confidence in-
tervals ÃÉC(k), k = 0, ‚Ä¶, n, defined as
ÃÉC(k) = [k
n ‚àíz1‚àíŒ±/2 ‚àö
k
n(1 ‚àík
n)
n
, k
n + z1‚àíŒ±/2 ‚àö
k
n(1 ‚àík
n)
n
] .
(8.58)
Example 8.6.21. We investigate once more Example 8.6.19. Among 500 chosen balls we
observed 220 white. This observation led to the ‚Äúexact‚Äù 90 % confidence interval C(220) =
(0.4028, 0.4777).
Let us compare this result with the interval we get by using the approximative ap-
proach. Since the quantile z1‚àíŒ±/2 for Œ± = 0.1 equals z0.95 = 1.64485, the left and the right
endpoints of the interval (8.58) with k = 220 are evaluated by
220
500 ‚àí1.64485 ‚ãÖ‚àö220 ‚ãÖ280
5003
= 0.4035
and
220
500 + 1.64485 ‚ãÖ‚àö220 ‚ãÖ280
5003
= 0.4765 .
Thus, the ‚Äúapproximate‚Äù 90 % confidence interval is
ÃÉC(220) = (0.4035, 0.4765), which
does not differ too much from C(220) = (0.4028, 0.4777).
In the case of 1000 trials and 440 white balls, the endpoints of a confidence interval
are evaluated by
440
1000 ‚àí1.64485 ‚ãÖ‚àö440 ‚ãÖ560
10003
= 0.414181
and
440
1000 + 1.64485 ‚ãÖ‚àö440 ‚ãÖ560
10003
= 0.4645819 .
That is, ÃÉC(440) = (0.4142, 0.4659) compared with C(440) = (0.4139, 0.4664).
Example 8.6.22. A few days before an election, 1000 randomly chosen people are ques-
tioned whom they will vote for next week, either candidate A or candidate B. Suppose
540 of the interviewed people answered that they would vote for candidate A, the re-
maining 460 favored candidate B. Find a 90 % confidence interval for the expected result
of candidate A in the election.
Solution: We have n = 1000, k = 540, and Œ± = 0.1. The quantile of level 0.95 of
the standard normal distribution equals z0.95 = 1.64485 (see Example 8.6.21). This leads
to [0.514, 0.566] as ‚Äúapproximate‚Äù 90 % confidence interval for the expected result of
candidate A.

444
‡±™
8 Mathematical statistics
If one questions another 1000 randomly chosen people, another confidence inter-
val will occur. But, on average, in 9 of 10 cases questioning 1000 people will lead to an
interval containing the correct value.
Summary: The statistical model is (ùí≥, ùí´(ùí≥), Bn,Œ∏)0‚â§Œ∏‚â§1 with ùí≥= {0, . . . , n}. Given Œ± > 0 and k = 0, . . . , n,
define numbers Œ∏‚àí
k and Œ∏+
k by
Œ∏‚àí
k = inf
{
{
{
Œ∏ ‚àà[0, 1] :
n
‚àë
j=k
(n
j )Œ∏j(1 ‚àíŒ∏)n‚àíj > Œ±
2
}
}
}
,
Œ∏+
k = sup
{
{
{
Œ∏ ‚àà[0, 1] :
k
‚àë
j=0
(n
j )Œ∏j(1 ‚àíŒ∏)n‚àíj > Œ±
2
}
}
}
.
Letting C(k) = [Œ∏‚àí
k , Œ∏+
k ], the C(k)s, 0 ‚â§k ‚â§n, are 1 ‚àíŒ± confidence intervals for the unknown Œ∏ ‚àà[0, 1]. That
is, for all Œ∏ ‚àà[0, 1],
Bn,Œ∏{k ‚â§n : Œ∏ ‚ààC(k)} ‚â•1 ‚àíŒ± .
For large n ‚â•1, the central limit theorem applies and leads to the approximate 1 ‚àíŒ± confidence intervals
ÃÉC(k) = [ k
n ‚àíz1‚àíŒ±/2 ‚àö
k
n(1 ‚àík
n)
n
, k
n + z1‚àíŒ±/2 ‚àö
k
n(1 ‚àík
n)
n
] .
8.6.4 Hypergeometric distributed populations
Finally, we construct confidence intervals for hypergeometric distributed populations.
Since the technique is quite similar to that used in the case of binomial distribution, we
will not state all details. We refer everybody interested in them to Problem 8.7, to add
missing details in the construction.
So assume that in an urn there are N balls, colored white and black. Among them
there are M ‚â§N white balls, hence N ‚àíM are black. Hereby, the number M is unknown.
We choose now at random n ‚â§N balls. Denote by m ‚â§n the number of chosen white
balls. The aim is to determine for each m ‚â§n a set C(m) ‚äÜ{0, . . . , N} so that
HN,M,n{m ‚â§n : M ‚ààC(m)} > 1 ‚àíŒ± ,
0 ‚â§M ‚â§N .
Of course, thereby the confidence sets should be chosen as small as possible.
Before we can introduce confidence intervals for hypergeometric distributed sam-
ples, we first have to extend Proposition 8.2.19 to the case of two-sided tests. Since the
proof of this two-sided case is very similar to that for binomial distributed samples in
Proposition 8.3.1, we omit it. Furthermore, we formulate the two-sided tests already in
a way appropriate for later use.

8.6 Confidence regions and intervals
‡±™
445
Proposition 8.6.23. Let the statistical model be (ùí≥, ùí´(ùí≥), HN,K,n)K=0,...,N with the sample
space ùí≥= {0, . . . , n}. Given an arbitrary M ‚â§N, an Œ±-test for testing K = M against
K
Ã∏= M is given by T(M) = (ùí≥0(M), ùí≥1(M)) where the region of acceptance equals ùí≥0(M) =
{m0(M), . . . , m1(M)} with m0(M) and m1(M) defined by
m0(M) = min{k ‚â§n :
k
‚àë
m=0
(M
m)(N‚àíM
n‚àím)
(N
n)
> Œ±/2}
and
m1(M) := max{k ‚â§n :
n
‚àë
m=k
(M
m)(N‚àíM
n‚àím)
(N
n)
> Œ±/2} .
Before proceeding further, let us introduce two functions similar to those in
eqs. (8.54) and (8.55). For each k = 0, . . . , n and M ‚â§N, set
f{‚â•k}(M) =
n
‚àë
m=k
(M
m)(N‚àíM
n‚àím)
(N
n)
= HN,M,n({k, . . . , n})
and
f{‚â§k}(M) =
k
‚àë
m=0
(M
m)(N‚àíM
n‚àím)
(N
n)
= HN,M,n({0, . . . , k}) .
Lemma 8.2.20 implies that f{‚â•k} is nondecreasing, hence f{‚â§k} is nonincreasing. Verbally
said, if there are M white balls in the urn, then f{‚â•k}(M) is the probability to observe at
least k white in the sample of size n, while f{‚â§k}(M) tells us how likely it is for us to get k
or less white balls.
Given k = 0, . . . , n, define two numbers M‚àí
k and M+
k (depending on Œ±) by
M‚àí
k = min{M : f{‚â•k}(M) > Œ±
2 }
and
M+
k = max{M : f{‚â§k}(M) > Œ±
2 } .
Compare Figure 8.11 for an example with N = 60, n = 15, and Œ± = 0.1.
Proposition 8.6.24. Let the statistical model be (ùí≥, ùí´(ùí≥), HN,M,n)M=0,...,N with the sample
space ùí≥= {0, . . . , n}. Given an arbitrary Œ± > 0, for each 0 ‚â§k ‚â§n, the sets
C(k) = [M‚àí
k , M+
k ] = {M ‚â§N : f{‚â•k}(M) > Œ±
2 } ‚à©{M ‚â§N : f{‚â§k}(M) > Œ±
2 }
are 1 ‚àíŒ± confidential intervals for the unknown parameter M ‚â§N.
Proof. Applying Propositions 8.6.9 and 8.6.23, for each k ‚â§n, the sets
{M ‚â§N : k ‚ààùí≥0(M)} = {M ‚â§N : m0(M) ‚â§k ‚â§m1(M)} ,
k = 0, . . . , n ,
are 1 ‚àíŒ± confidential sets. But by the definition of m0(M) and m1(M), we have
m0(M) ‚â§k ‚â§m1(M) if and only if

446
‡±™
8 Mathematical statistics
Figure 8.11: The increasing function f{‚â•4} and the decreasing f{‚â§4} in the case N = 60 and sample size
n = 15. The horizontal line marks the significance level Œ±/2 = 0.05.
f{‚â•k}(M) > Œ±
2
and
f{‚â§k}(M) > Œ±
2 .
This proves that
C(k) := {M ‚â§N : f{‚â•k}(M) > Œ±
2 } ‚à©{M ‚â§N : f{‚â§k}(M) > Œ±
2 }
are 1 ‚àíŒ± confidence sets. Finally, since f{‚â•k} and f{‚â§k} are monotone, we observe that
M‚àí
k ‚â§M ‚â§M+
k if and only if f{‚â•k}(M) > Œ±
2 and f{‚â§k}(M) > Œ±
2 . Hence, C(k) = [M‚àí
k , M+
k ],
which completes the proof.
Example 8.6.25. In an urn there are 200 balls. We choose randomly a sample of 60 balls.
Among them 25 are white. What can be said about the number of white balls in the urn?
Answer: We are asking for a 90 % confidence set for the unknown number M of
white balls in the urn. In this case the functions of interest are
f{‚â§25}(M) =
25
‚àë
m=0
(M
m)(200‚àíM
60‚àím )
(200
60 )
and
f{‚â•25}(M) =
60
‚àë
m=25
(M
m)(200‚àíM
60‚àím )
(200
60 )
.
Since
f{‚â•25}(65) = 0.0508,
f{‚â•25}(64) = 0.0408,
and
f{‚â§25}(103) = 0.048,
f{‚â§25}(102) = 0.0507,
we conclude that M‚àí
k = 65 and M+
k = 102, hence C(25) = [65, 102] is a 90 % confidence
interval for the unknown number M of white balls. If one asks for sharper bounds,
one has to relax the significance level. So, for example, as 80 % confidence interval one
gets [68, 98].

8.7 Problems
‡±™
447
Note that Proposition 8.5.16 gives in this case
ÃÇM(25) = 83 as a point estimator which
is in the middle of both confidence intervals.
If there are only 10 white balls among the chosen 60, the 90 % confidence region
is [20, 50].
Remark 8.6.26. Proposition 1.4.39 shows the tight connection between the binomial and
hypergeometric distributions. Hence, it could be of interest what happens if we replace
in the preceding example the hypergeometric distribution by the binomial. To do so,
the unknown success probability equals M/200. In other words, instead taking 60 balls
without replacement we choose now 60 balls and replace every time the chosen ball.
Applying eq. (8.58) with n = 60 and k = 25, our observation of 25 white balls leads to the
90 %-sure estimates
0.311977 < M
200 < 0.521356 .
Compare this with the hypergeometric case where we got the 90 %-sure estimates
0.325 = 65
200 < M
200 < 102
200 = 0.51 .
Thus, in this case nonreplacing of chosen balls leads to sharper bounds for the unknown
number of white balls than the bounds one gets in the case of replacing. Of course, one
has to assume that in both cases the number of chosen white balls coincides.
Summary: The statistical model is (ùí≥, ùí´(ùí≥), HN,M,n)M=0,...,N with ùí≥= {0, . . . , n}. If
M‚àí
k = min
{
{
{
0 ‚â§M ‚â§N :
n
‚àë
m=k
(M
m)(N‚àíM
n‚àím)
(N
n)
> Œ±
2
}
}
}
,
M+
k = max
{
{
{
0 ‚â§M ‚â§N :
k
‚àë
m=0
(M
m)(N‚àíM
n‚àím)
(N
n)
> Œ±
2
}
}
}
,
then for k = 0, . . . , n the sets C(k) = [M‚àí
k , M+
k ] are 1 ‚àíŒ± confidence intervals for the unknown parameter
M ‚â§N.
8.7 Problems
Problem 8.1. For some b > 0, let ‚Ñôb be the uniform distribution on [0, b]. The precise
value of b > 0 is unknown. We claim that b ‚â§b0 for a certain b0 > 0. Thus, the hypotheses
are
‚Ñç0 : b ‚â§b0
and
‚Ñç1 : b > b0 .
To test ‚Ñç0, we chose randomly n numbers x1, . . . , xn distributed according to ‚Ñôb. Suppose
the region of acceptance ùí≥0 of a hypothesis test Tc is given by

448
‡±™
8 Mathematical statistics
ùí≥0 := {(x1, . . . , xn) : max
1‚â§i‚â§n xi ‚â§c}
for some c > 0.
1.
Determine those c > 0 for which Tc is an Œ±-significance test of level Œ± < 1.
2.
Suppose Tc is an Œ±-significance test. For which of those c > 0 does the probability
for a type II error become minimal?
3.
Determine the power function of the Œ±-test Tc that minimizes the probability of the
occurrence of a type II error.
Problem 8.2. For Œ∏ > 0, let ‚ÑôŒ∏ be the probability measure with density pŒ∏ defined by
pŒ∏(s) = {Œ∏sŒ∏‚àí1
if s ‚àà(0, 1],
0
otherwise.
1.
Check whether the pŒ∏s are probability density functions.
2.
In order to get information about Œ∏, we execute n independent trials according to ‚ÑôŒ∏.
Which statistical model describes this experiment?
3.
Find the maximum likelihood estimator for Œ∏.
Problem 8.3. The lifetime of light bulbs is exponentially distributed with unknown pa-
rameter Œª > 0. In order to determine Œª, we switch on n light bulbs and record the number
of light bulbs that burn out until a certain time T > 0. Determine a statistical model that
describes this experiment. Find the MLE for Œª.
Problem 8.4. Consider the statistical model in Example 8.5.40, that is, we have
(‚Ñùn, ‚Ñ¨(‚Ñùn), ‚Ñô‚äón
b )b>0 with uniform distribution ‚Ñôb on [0, b]. There are two natural es-
timators for b > 0, namely ÃÇb1 and ÃÇb2, defined by
ÃÇb1(x) := n + 1
n
max
1‚â§i‚â§n xi
and
ÃÇb2(x) := 2
n
n
‚àë
i=1
xi ,
x = (x1, . . . , xn) ‚àà‚Ñùn .
Prove that ÃÇb1 and ÃÇb2 possess the following properties:
1.
The estimators ÃÇb1 and ÃÇb2 are unbiased.
2.
One has
ùïçb ÃÇb1 =
b2
n(n + 2)
and
ùïçb ÃÇb2 = b2
3n2 .
Problem 8.5. In a questionnaire, out of 2000 randomly chosen people 1420 answered
that they regularly use the Internet. Find an ‚Äúapproximate‚Äù 90 % confidence interval for
the proportion of people using the Internet regularly. Determine the inequalities that
describe the exact intervals in eq. (8.53).

8.7 Problems
‡±™
449
Problem 8.6. How do the confidence intervals C(k) in eq. (8.53) look like for k = 0 and
k = n? Is it possible that for some k = 0, . . . , n and n ‚â•1 it follows that 0 ‚ààC(k) or
1 ‚ààC(k)? If so, when does this happen?
Problem 8.7. Suppose the statistical model is (ùí≥, ùí´(ùí≥), HN,M,n)M‚â§N with the sample
space ùí≥= {0, . . . , n} and with the hypergeometric distributions HN,M,n introduced in
Definition 1.4.32.
1.
For some M0 ‚â§M, the hypotheses are ‚Ñç0 : M = M0 against ‚Ñç1 : M
Ã∏= M0. Find
(optimal) numbers 0 ‚â§m0 ‚â§m1 ‚â§n such that ùí≥0 = {m0, . . . , m1} is the region of
acceptance of an Œ±-significance test T for ‚Ñç0 against ‚Ñç1.
Hint: Modify the methods developed in Proposition 8.2.19 and compare the construc-
tion of two-sided tests for a binomial distributed population.
2.
Use Proposition 8.6.9 to derive from ùí≥0 confidence intervals C(k), 0 ‚â§k ‚â§n, of level
1 ‚àíŒ± for the unknown parameter M.
Hint: Follow the methods in Proposition 8.6.16 for the binomial distribution.


A Appendix
A.1 Notations
Throughout the book, we use the following standard notations:
1.
The natural numbers starting at 1 are always denoted by ‚Ñï. In the case 0 is in-
cluded, we write ‚Ñï0.
2.
As usual, the integers ‚Ñ§are given by ‚Ñ§= {. . . , ‚àí2, ‚àí1, 0, 1, 2, . . .}.
3.
By ‚Ñùwe denote the field of real numbers endowed with the usual algebraic opera-
tions and its natural order. The subset ‚Ñö‚äÇ‚Ñùis the union of all rational numbers,
that is, of numbers m/n where m, n ‚àà‚Ñ§and n
Ã∏= 0.
4.
Given n ‚â•1, let ‚Ñùn be the n-dimensional Euclidean vector space, that is,
‚Ñùn = {x = (x1, . . . , xn) : xj ‚àà‚Ñù} .
Addition and scalar multiplication in ‚Ñùn are carried out coordinate-wise,
x + y = (x1, . . . , xn) + (y1, . . . , yn) = (x1 + y1, . . . , xn + yn)
and if Œ± ‚àà‚Ñù, then
Œ± x = (Œ±x1, . . . , Œ±xn) .
A.2 Elements of set theory
A.2.1 Set operations
Given a set M, its powerset ùí´(M) consists of all subsets of M. In the case that M is finite,
we have |ùí´(M)| = 2|M|, where |A| denotes the cardinality (number of elements) of a
finite set A.
If A and B are subsets of M, written as A, B ‚äÜM or also as A, B ‚ààùí´(M), their union
and their intersection are, as usual, defined by (compare Figure A.1)
A ‚à™B = {x ‚ààM : x ‚ààA or x ‚ààB}
and
A ‚à©B = {x ‚ààM : x ‚ààA and x ‚ààB} .
Of course, it always holds that
A ‚à©B ‚äÜA ‚äÜA ‚à™B
and
A ‚à©B ‚äÜB ‚äÜA ‚à™B .
In the same way, given subsets A1, A2, . . . of M their union ‚ãÉ‚àû
j=1 Aj and intersection ‚ãÇ‚àû
j=1 Aj
is the set of those x ‚ààM that belong to at least one of the Aj or that belong to all Aj,
respectively.
https://doi.org/10.1515/9783111325064-009

452
‡±™
A Appendix
Figure A.1: The Venn diagrams of the union A ‚à™B and the intersection A ‚à©B.
Quite often we use the distributive law for intersection and union. This asserts
A ‚à©(
‚àû
‚ãÉ
j=1
Bj) =
‚àû
‚ãÉ
j=1
(A ‚à©Bj) .
Two sets A and B are said to be disjoint1 provided that A ‚à©B = 0. A sequence of sets
A1, A2, . . . is called disjoint2 whenever Ai ‚à©Aj = 0 if i
Ã∏= j.
An element x ‚ààM belongs to the set difference A \ B provided that x ‚ààA but x ‚àâB.
Using the notion of the complementary set Bc := {x ‚ààM : x ‚àâB}, the set difference
may also be written as (compare Figure A.2)
A \ B = A ‚à©Bc .
Figure A.2: The Venn diagrams of the set difference A \ B and of the complement Ac of A with respect to a
superset M.
Another useful identity is
A \ B = A \ (A ‚à©B) .
Conversely, the complementary set may be represented as the set difference Bc = M \ B.
We still mention the obvious (Bc)c = B.
1 Sometimes called ‚Äúmutually exclusive.‚Äù
2 More precisely, one should say ‚Äúpairwise disjoint.‚Äù

A.2 Elements of set theory
‡±™
453
Finally, we introduce the symmetric difference AŒîB of two sets A and B as (see
Figure A.3)
AŒîB := (A \ B) ‚à™(B \ A) = (A ‚à©Bc) ‚à™(B ‚à©Ac) = (A ‚à™B) \ (A ‚à©B) .
(A.1)
Note that an element x ‚ààM belongs to AŒîB if and only if x belongs exactly to one of the
sets A or B.
Figure A.3: The symmetric difference AŒîB.
De Morgan‚Äôs rules are very important and assert the following:
(
‚àû
‚ãÉ
j=1
Aj)
c
=
‚àû
‚ãÇ
j=1
Ac
j
and
(
‚àû
‚ãÇ
j=1
Aj)
c
=
‚àû
‚ãÉ
j=1
Ac
j .
Given sets A1, . . . , An, their Cartesian product A1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó An is defined by (see Fig-
ure A.4 for an example of the Cartesian product)
A1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó An := {(a1, . . . , an) : aj ‚ààAj}
with
(a1, . . . , an) = (b1, . . . , bn)
‚áî
a1 = b1, . . . , an = bn .
Note that |A1 √ó ‚ãÖ‚ãÖ‚ãÖ√ó An| = |A1| ‚ãÖ‚ãÖ‚ãÖ|An|.
Figure A.4: The Cartesian product {a, b, c} √ó {a, b}.

454
‡±™
A Appendix
A.2.2 Preimages of sets
Let S be another set, for example, S = ‚Ñù, and let f : M ‚ÜíS be some mapping from M to
S. Given a subset B ‚äÜS, we denote the preimage of B with respect to f by
f ‚àí1(B) := {x ‚ààM : f (x) ‚ààB} .
(A.2)
In other words, an element x ‚ààM belongs to f ‚àí1(B) if and only if its image with respect
to f is an element of B (compare Figure A.5).
Figure A.5: Only elements from f ‚àí1(B) are mapped to B. All other elements in M have to have their image
outside B. But not every element in B needs to be the image of an element in M.
We summarize some crucial properties of the preimage in a proposition.
Proposition A.2.1. Let f : M ‚ÜíS be a mapping from M into another set S.
(1) f ‚àí1(0) = 0 and f ‚àí1(S) = M.
(2) For any subsets Bj ‚äÜS, the following equalities are valid:
f ‚àí1(‚ãÉ
j‚â•1
Bj) = ‚ãÉ
j‚â•1
f ‚àí1(Bj)
and
f ‚àí1(‚ãÇ
j‚â•1
Bj) = ‚ãÇ
j‚â•1
f ‚àí1(Bj).
(A.3)
Proof. We only prove the left-hand equality in eq. (A.3). The right-hand one is proved by
the same methods. Furthermore, assertion (1) follows immediately.
Take x ‚ààf ‚àí1(‚ãÉj‚â•1 Bj). This happens if and only if
f (x) ‚àà‚ãÉ
j‚â•1
Bj
(A.4)
is satisfied. But this is equivalent to the existence of a certain j0 ‚â•1 with f (x) ‚ààBj0.
By definition of the preimage, the last statement may be reformulated as follows: there
exists a j0 ‚â•1 such that x ‚ààf ‚àí1(Bj0). But this implies
x ‚àà‚ãÉ
j‚â•1
f ‚àí1(Bj) .
(A.5)
Consequently, an element x ‚ààM satisfies condition (A.4) if and only if property (A.5)
holds. This proves the left-hand identity in formulas (A.3).

A.2 Elements of set theory
‡±™
455
A.2.3 Problems
Problem A.1. For any three sets A, B, and C, prove the following assertions:
A \ (B ‚à™C) = (A \ B) ‚à©(A \ C)
and
(A ‚à©B) ‚à™C = (A ‚à™C) ‚à©(B ‚à™C) .
Problem A.2. For any sets A, B ‚äÜM, prove that
(B \ A)c ‚à©B = A ‚à©B
and
(A ‚à™B)c ‚à©B = 0 .
Problem A.3. Let A and B two finite sets. Show that
|A ‚à™B| = |A| + |B| ‚àí|A ‚à©B| .
Let C be another finite set. Find a similar formula for |A ‚à™B ‚à™C| and prove it.
Problem A.4. Prove that all three expressions in eq. (A.1) define the symmetric differ-
ence of two sets. That is, show that
(A \ B) ‚à™(B \ A) = (A ‚à©Bc) ‚à™(B ‚à©Ac) = (A ‚à™B) \ (A ‚à©B)
for all sets A and B.
Problem A.5. Let A, B, and C be three sets. Show that an element x belongs to AŒîBŒîC if
and only if x belongs either to all three sets or to exactly one of those. In other words,
x ‚àâAŒîBŒîC if and only if x is either in none of the three sets or exactly in two of them.
Problem A.6. Let A and B be two subsets of a set M. Which of the following equations
are valid? Prove the correct identities, give counterexamples for the false ones:
(A √ó B)c = Ac √ó Bc ,
(A √ó B)c = (Ac √ó Bc) ‚à™(Ac √ó M) ‚à™(M √ó Bc),
(A √ó B)c = (Ac √ó B) ‚à™(A √ó Bc),
(A √ó B)c = (Ac √ó M) ‚à™(M √ó Bc).
Problem A.7. Define f from ‚Ñïto ‚Ñ§by
f (n) = {0
if n is even,
1
if n is odd.
Describe f ‚àí1(B) for all B ‚äÜ‚Ñ§.
Problem A.8. Determine
f ‚àí1([0, 5])
and
f ‚àí1([0, ‚àû))

456
‡±™
A Appendix
where f : ‚Ñù‚Üí‚Ñùdenotes the floor function. That is,
f (x) = ‚åäx‚åã,
x ‚àà‚Ñù.
A.3 Combinatorics
A.3.1 Binomial coefficients
A one-to-one mapping œÄ from {1, . . . , n} to {1, . . . , n} is called a permutation (of order
n). Any permutation reorders the numbers from 1 to n as œÄ(1), œÄ(2), . . . , œÄ(n) and, vice
versa, each reordering of these numbers generates a permutation. One way to write a
permutations is
œÄ = (
1
2
. . .
n
œÄ(1)
œÄ(2)
. . .
œÄ(n) ) .
For example, if n = 3, then œÄ = ( 1 2 3
2 3 1 ) is equivalent to the order 2, 3, 1 or to œÄ(1) = 2,
œÄ(2) = 3, and œÄ(3) = 1.
Let Sn be the set of all permutations of order n. Then one may ask for |Sn| or, equiv-
alently, for the number of possible orderings of the numbers {1, . . . , n}.
To treat this problem, we need the following definition.
Definition A.3.1. For n ‚àà‚Ñï, we define n-factorial by setting
n! = 1 ‚ãÖ2 ‚ãÖ‚ãÖ‚ãÖ(n ‚àí1) ‚ãÖn.
Furthermore, let 0! = 1.
Now we may answer the question about the cardinality of Sn.
Proposition A.3.2. We have
|Sn| = n!
(A.6)
or, equivalently, there are n! different ways to order n distinguishable objects.
Proof. The proof is done by induction over n. If n = 1 then |S1| = 1 = 1! and eq. (A.6) is
valid.
Now suppose that eq. (A.6) is true for n. In order to prove eq. (A.6) for n + 1, we split
Sn+1 as follows:
Sn+1 =
n+1
‚ãÉ
k=1
Ak,
where

A.3 Combinatorics
‡±™
457
Ak = {œÄ ‚ààSn+1 : œÄ(n + 1) = k} ,
k = 1, . . . , n + 1 .
Each œÄ ‚ààAk
generates a one-to-one mapping
ÃÉœÄ from {1, . . . , n} onto the set
{1, . . . , k ‚àí1, k +1, . . . , n+1} by letting ÃÉœÄ(j) = œÄ(j), 1 ‚â§j ‚â§n. Vice versa, each such ÃÉœÄ defines
a permutation œÄ ‚ààAk by setting œÄ(j) =
ÃÉœÄ(j), j ‚â§n, and œÄ(n + 1) = k. Consequently, since
eq. (A.6) holds for n, we get |Ak| = n!. Furthermore, the Aks are disjoint, and
|Sn+1| =
n+1
‚àë
k=1
|Ak| = (n + 1) ‚ãÖn! = (n + 1)! ,
hence eq. (A.6) also holds for n + 1. This completes the proof.
Next we treat a tightly related problem. Say we have n different objects and we want
to distribute them into two disjoint groups, one having k elements, the other n‚àík. Hereby
it is of no interest in which order the elements are distributed, only the composition of
the two sets matters.
Example A.3.3. There are 52 cards in a deck that are distributed to two players, so that
each of them gets 26 cards. For this game, it is only important which cards each player
has, not in which order the cards were received. Here n = 52 and k = n ‚àík = 26.
The main question is: how many ways can n elements be distributed, say the num-
bers from 1 to n, into one group of k elements and into another of n ‚àík elements? In
the above example, that is how many ways can 52 cards be distributed into two groups
of 26.
To answer this question, we use the following auxiliary model. Let us take any per-
mutation œÄ ‚ààSn. We place the numbers œÄ(1), . . . , œÄ(k) into group 1 and the remaining
œÄ(k + 1), . . . , œÄ(n) into group 2. In this way, we obtain all possible distributions but many
of them appear several times. Say that two permutations œÄ1 and œÄ2 are equivalent if (as
sets)
{œÄ1(1), . . . , œÄ1(k)} = {œÄ2(1), . . . , œÄ2(k)} .
Of course, this also implies
{œÄ1(k + 1), . . . , œÄ1(n)} = {œÄ2(k + 1), . . . , œÄ2(n)} ,
and two permutations generate the same partition if and only if they are equivalent.
Equivalent permutations are achieved by taking one fixed permutation œÄ, then permut-
ing {œÄ(1), . . . , œÄ(k)} and also {œÄ(k +1), . . . , œÄ(n)}. Consequently, there are exactly k!(n‚àík)!
permutations that are equivalent to a given one. Summing up, we get that there are
n!
k!(n‚àík)! different classes of equivalent permutations. Let
(n
k) =
n!
k! (n ‚àík)! .

458
‡±™
A Appendix
There are (n
k) different ways to distribute n objects into one group of k and into another one of n‚àík elements.
For any n ‚â•0, we set (n
0) = 1 and (n
k) = 0 in case of k > n or k < 0.
Definition A.3.4. The numbers
(n
k) =
n!
k! (n ‚àík)! ,
n = 0, 1, . . . and k = 0, . . . , n ,
are called binomial coefficients, read ‚Äún choose k.‚Äù
Example A.3.5. A digital word of length n consists of n zeroes or ones. Since at every
position we may have either ‚Äú0‚Äù or ‚Äú1‚Äù, there are 2n different words of length n. How
many of these words possess exactly k ones or, equivalently, n ‚àík zeroes? To answer
this, put all positions where there is a ‚Äú1‚Äù into a first group and those where there is a
‚Äú0‚Äù into a second one. In this way, the numbers from 1 to n are divided into two different
groups of size k and n ‚àík, respectively. But we already know how many such partitions
exist, namely (n
k). As a consequence, we get
There are (n
k) words of length n possessing exactly k ones and n ‚àík zeroes.
The next proposition summarizes some crucial properties of binomial coeffi-
cients.
Proposition A.3.6. Let n be a natural number, k = 0, . . . , n, and let r ‚â•1 be an integer.
Then the following equations hold:
(n
k) = ( n
n ‚àík),
(A.7)
(n
k) = (n ‚àí1
k ) + (n ‚àí1
k ‚àí1),
and
(A.8)
(n + r
n ) =
n
‚àë
j=0
(n + r ‚àíj ‚àí1
n ‚àíj
) =
n
‚àë
j=0
(r + j ‚àí1
j
) .
(A.9)
Proof. Equations (A.7) and (A.8) follow immediately by the definition of the binomial
coefficients. Note that eq. (A.8) also holds if k = n because we agreed that (n‚àí1
n ) = 0.
If k < n, then an iteration of eq. (A.8) leads to
(n
k) =
k
‚àë
j=0
(n ‚àíj ‚àí1
k ‚àíj ) .
Replacing in the last equation n by n + r, as well as k by n (note that n + r > n), we obtain
the left-hand identity (A.9). The right-hand equation follows by inverting the summation,

A.3 Combinatorics
‡±™
459
that is, one replaces j by n ‚àíj. Observe that (A.9) becomes wrong in the case r = 0. Then
the left-hand side is 1 while the right-hand one equals 0.
Remark A.3.7. Equation (A.8) allows a graphical interpretation by Pascal‚Äôs triangle.
The coefficient (n
k) in the nth row follows by summing the two values (n‚àí1
k‚àí1) and (n‚àí1
k )
above (n
k) in the (n ‚àí1)th row. Look at Figure A.6 for a visualization of the triangle.
Figure A.6: Pascal‚Äôs triangle.
Next we state and prove an important binomial theorem.
Proposition A.3.8 (Binomial theorem). For real numbers a, b, and any n ‚àà‚Ñï0,
(a + b)n =
n
‚àë
k=0
(n
k) ak bn‚àík .
(A.10)
Proof. The binomial theorem is proved by induction over n. If n = 0, then eq. (A.10)
holds trivially.
Suppose now that eq. (A.10) has been proven for n ‚àí1. Our aim is to verify that it is
also true for n. Using that the expansion holds for n ‚àí1, we get
(a + b)n = (a + b) (a + b)n‚àí1 = (a + b)
n‚àí1
‚àë
k=0
(n ‚àí1
k )akbn‚àí1‚àík
=
n‚àí1
‚àë
k=0
(n ‚àí1
k )ak+1bn‚àí1‚àík +
n‚àí1
‚àë
k=0
(n ‚àí1
k )akbn‚àík
= an +
n‚àí2
‚àë
k=0
(n ‚àí1
k )ak+1bn‚àí1‚àík + bn +
n‚àí1
‚àë
k=1
(n ‚àí1
k )akbn‚àík
= an + bn +
n‚àí1
‚àë
k=1
[(n ‚àí1
k ‚àí1) + (n ‚àí1
k )]akbn‚àík =
n
‚àë
k=0
(n
k) ak bn‚àík ,
where we used eq. (A.8) in the last step.

460
‡±™
A Appendix
The following property of binomial coefficients plays an important role when in-
troducing the hypergeometric distribution (see Proposition 1.4.31). It is also used during
the investigation of sums of independent binomial distributed random variables (see
Proposition 4.6.1).
Proposition A.3.9 (Vandermonde‚Äôs identity). If k, m, and n are all in ‚Ñï0, then
k
‚àë
j=0
(n
j )( m
k ‚àíj) = (n + m
k
) .
(A.11)
Proof. An application of the binomial theorem leads to
(1 + x)n+m =
n+m
‚àë
k=0
(n + m
k
) xk ,
x ‚àà‚Ñù.
(A.12)
On the other hand, another use of Proposition A.3.8 implies3
(1 + x)n+m = (1 + x)n(1 + x)m
= [
n
‚àë
j=0
(n
j )xj][
m
‚àë
i=0
(m
i )xi] =
n
‚àë
j=0
m
‚àë
i=0
(n
j )(m
i )xi+j
=
n+m
‚àë
k=0
[ ‚àë
i+j=k
(n
j )(m
i )]xk =
n+m
‚àë
k=0
[
k
‚àë
j=0
(n
j )( m
k ‚àíj)]xk .
(A.13)
The coefficients in an expansion of a polynomial are unique. Hence, in view of eqs. (A.12)
and (A.13), we get for all k ‚â§m + n the identity
(n + m
k
) =
k
‚àë
j=0
(n
j )( m
k ‚àíj) .
Hereby note that both sides of eq. (A.11) become zero whenever k > n+m. This completes
the proof.
Remark A.3.10. Another, more heuristic, way to prove Vandermonde‚Äôs identity is as fol-
lows. Suppose one has n + m fruits, n apples and m oranges. There are (n+m
k ) ways to
choose k fruits out of the n + m ones. These possibilities split into the following k + 1
disjoint events: among the chosen k fruits there are zero apples and k oranges, or one
apple and k ‚àí1 oranges up to k apples and zero oranges. If the number of apples in the
sample is j, then there are (n
j) ways to choose the apples and ( m
k‚àíj) ways to choose the
3 When passing from line 2 to line 3, the order of summation is changed. One no longer sums over the
rectangle [0, m] √ó [0, n]. Instead, one sums along the diagonals, where i + j = k.

A.3 Combinatorics
‡±™
461
oranges, respectively. Summing over all possibilities j = 0, . . . , k proves Vandermonde‚Äôs
identity.
Our next objective is to generalize the binomial coefficients. In view of
(n
k) = n (n ‚àí1) ‚ãÖ‚ãÖ‚ãÖ(n ‚àík + 1)
k!
for k ‚â•1 and n ‚àà‚Ñï, the generalized binomial coefficient is introduced as
(‚àín
k ) := ‚àín (‚àín ‚àí1) ‚ãÖ‚ãÖ‚ãÖ(‚àín ‚àík + 1)
k!
.
(A.14)
The next lemma shows the tight relation between generalized and ‚Äúordinary‚Äù binomial
coefficients.
Lemma A.3.11. For k ‚â•1 and n ‚àà‚Ñï,
(‚àín
k ) = (‚àí1)k (n + k ‚àí1
k
) .
Proof. By definition of the generalized binomial coefficient, we obtain
(‚àín
k ) = (‚àín) (‚àín ‚àí1) ‚ãÖ‚ãÖ‚ãÖ(‚àín ‚àík + 1)
k!
= (‚àí1)k (n + k ‚àí1) (n + k ‚àí2) ‚ãÖ‚ãÖ‚ãÖ(n + 1) n
k!
= (‚àí1)k(n + k ‚àí1
k
) .
This completes the proof.
For example, Lemma A.3.11 implies (‚àí1
k ) = (‚àí1)k and (‚àín
1 ) = ‚àín.
A.3.2 Drawing balls out of an urn
Assume that there are n balls labeled from 1 to n in an urn. We draw k balls out of the urn,
thus observing a sequence of length k with entries from {1, . . . , n}. How many different
results (sequences) may be observed? To answer this question, we have to decide on the
arrangement of drawing. Do we or do we not replace the chosen ball? Is it important in
which order the balls were chosen or is it only of importance which balls were chosen at
all? Thus, we see that there are four different ways to answer this question (replacement
or nonreplacement, recording the order or nonrecording).
Example A.3.12. Let us regard the drawing of two balls out of four, that is, n = 4 and
k = 2. Depending on the different arrangements, the following results may be observed.
Note, for example, that in the two latter cases (nonrecording of the order) the pair (3, 2)
does not appear because it is identical to (2, 3).

462
‡±™
A Appendix
Replacement and the
order is important
(1, 1)
(1, 2)
(1, 3)
(1, 4)
(2, 1)
(2, 2)
(2, 3)
(2, 4)
(3, 1)
(3, 2)
(3, 3)
(3, 4)
(4, 1)
(4, 2)
(4, 3)
(4, 4)
16 different results
Nonreplacement and
the order is important
‚ãÖ
(1, 2)
(1, 3)
(1, 4)
(2, 1)
‚ãÖ
(2, 3)
(2, 4)
(3, 1)
(3, 2)
‚ãÖ
(3, 4)
(4, 1)
(4, 2)
(4, 3)
‚ãÖ
12 different results
Replacement and the
order is not important
(1, 1)
(1, 2)
(1, 3)
(1, 4)
‚ãÖ
(2, 2)
(2, 3)
(2, 4)
‚ãÖ
‚ãÖ
(3, 3)
(3, 4)
‚ãÖ
‚ãÖ
‚ãÖ
(4, 4)
10 different results
Nonreplacement and the
order is not important
‚ãÖ
(1, 2)
(1, 3)
(1, 4)
‚ãÖ
‚ãÖ
(2, 3)
(2, 4)
‚ãÖ
‚ãÖ
‚ãÖ
(3, 4)
‚ãÖ
‚ãÖ
‚ãÖ
‚ãÖ
6 different results
Let us come back now to the general situation of n different balls from which we
choose k at random:
Case 1. Drawing with replacement and taking the order into account. We have n different
possibilities for the choice of the first ball and, since the chosen ball is placed back, there
are also n possibilities for the second one, and so on. Thus, there are n possibilities for
each of the k balls, leading to the following result.
The number of different results in this case is nk.
Example A.3.13. Letters in Braille, a scripture for blind people, are generated by dots or
nondots at six different positions. How many letters may be generated in that way?
Answer: It holds that n = 2 (dot or no dot) at k = 6 different positions. Hence, the
number of possible representable letters is 26 = 64. In fact, there are only 63 possibilities
because we have to rule out the case of no dots at all 6 positions.
Case 2. Drawing without replacement and taking the order into account. This case only
makes sense if k ‚â§n. There are n possibilities to choose the first ball. After that there are
still n ‚àí1 balls in the urn. Hence there are only n ‚àí1 possibilities for the second choice,
n ‚àí2 for the third, and so on. Summing up, we get the following.
The number of possible results in this case equals n(n ‚àí1) ‚ãÖ‚ãÖ‚ãÖ(n ‚àík + 1) =
n!
(n ‚àík)!.

A.3 Combinatorics
‡±™
463
Example A.3.14. In a lottery, 6 numbers are chosen out of 49. Of course, the chosen
numbers are not replaced. If we record the numbers as they appear (not putting them
in order), how many different sequences of six numbers exist?
Answer: Here we have n = 49 and k = 6. Hence the wanted number equals
49!
43! = 49 ‚ãÖ‚ãÖ‚ãÖ44 = 10, 068, 347, 520.
Case 3. Drawing with replacement not taking the order into account. This case is more
complicated and requires a different point of view. We count how often each of the n
balls was chosen during the k trials. Let k1 ‚â•0 be the frequency of the first ball, k2 ‚â•0
that of the second one, and so on. In this way we obtain n nonnegative integers k1, . . . , kn
satisfying
k1 + ‚ãÖ‚ãÖ‚ãÖ+ kn = k .
Indeed, since we choose k balls, the frequencies have to sum to k. Consequently, the
number of possible results when drawing k of n balls with replacement and not taking
the order into account coincides with
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®{(k1, . . . , kn), kj ‚àà‚Ñï0, k1 + ‚ãÖ‚ãÖ‚ãÖ+ kn = k}ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®.
(A.15)
In order to determine the cardinality (A.15), we use the following auxiliary model:
Let B1, . . . , Bn be n boxes. Given n nonnegative integers k1, . . . , kn, summing to k, we
place exactly k1 dots into B1, k2 dots into B2, and so on. At the end we distributed k indis-
tinguishable dots into n different boxes. Thus, we see that the value of (A.15) coincides
with the number of different possibilities to distribute k indistinguishable dots into n
boxes. Now assume that the boxes are glued together; on the very left we put box B1,
on its right we put box B2, and continue in this way up to box Bn on the very right. In
this way, we obtain n + 1 dividing walls, two outer and n ‚àí1 inner ones. Now we get all
possible distributions of k dots into n boxes by shuffling the k dots and the n ‚àí1 inner
dividing walls. For example, if we get the order d, d, d, w, w, d, w, . . . then this means that
there are three dots in B1, none in B2, and one in B3, and so on (compare Figure A.7 for
a slightly more general example).
Figure A.7: The case k1 = 3, k2 = 0, k3 = 1, . . . , kn‚àí1 = 2, kn = 0: k dots and n ‚àí1 inner walls.
Summing up, we have N = n + k ‚àí1 objects, k of them are dots and n ‚àí1 are walls.
As we know, there are (N
k) different ways to order these N objects. Hence we arrived at
the following result.

464
‡±™
A Appendix
The number of possibilities to distribute k anonymous dots into n boxes equals
(n + k ‚àí1
k
) = (n + k ‚àí1
n ‚àí1 ) .
It coincides with |{(k1, . . . , kn) , kj ‚àà‚Ñï0 , k1 + ‚ãÖ‚ãÖ‚ãÖ+ kn = k}|, as well as with the number of different results
when choosing k balls out of n with replacement and not taking order into account.
Example A.3.15. Dominoes are marked on each half either with no dots, one dot, or up
to six dots. Hereby the dominoes are symmetric, that is, a tile with three dots on the left-
hand side and two ones on the right-hand side is identical with that having two dots on
the left-hand side and three dots on the right-hand side. How many different dominoes
exist?
Answer: It holds4 n = 7 and k = 2, hence the number of different dominoes
equals
(7 + 2 ‚àí1
2
) = (8
2) = 28 .
Case 4. Drawing without replacement not taking the order into account. Here we also
have to assume k ‚â§n. We already investigated this case when we introduced the bino-
mial coefficients. The k chosen numbers are put in group 1, the remaining n ‚àík balls
in group 2. As we know, there are (n
k) ways to split the n numbers into such two groups.
Hence we obtained the following.
The number of different results in this case is (n
k).
Example A.3.16. If the order of the six numbers is not taken into account in Exam-
ple A.3.14, that is, we ignore which number was chosen first, which second, and so on,
the number of possible results equals
(49
6 ) = 49 ‚ãÖ‚ãÖ‚ãÖ43
6!
= 13, 983, 816.
Let us summarize the four different cases in a table. Here O and NO stand for record-
ing or nonrecording of the order while R and NR represent replacement or nonreplace-
ment, and one chooses k balls out of n possible.
4 There are 7 boxes B0 up to B6 and two particles distributed into these 7 boxes. The number of the box
containing a particle corresponds to the number of dots on the tile. For example, if one particle is in box
B2 and the other in B4, then the corresponding tile is that with 2 and 4 dots on it. Or both particles in B6
means that our tile has 6 dots at each side. Another possibility to describe these tiles is to represent 2 as
2 = 0 + 0 + 1 + 0 + 1 + 0 + 0 or 2 = 0 + 0 + 0 + 0 + 0 + 0 + 2, respectively.

A.3 Combinatorics
‡±™
465
R
NR
O
nk
n!
(n‚àík)!
NO
(n+k‚àí1
k
)
(n
k)
A.3.3 Multinomial coefficients
The binomial coefficient (n
k) describes the number of possibilities to distribute n objects
into two groups of k and n ‚àík elements. What happens if we have not only two groups
but m ‚â•2? Say the first group has k1 elements, the second has k2 elements, and so on,
up to the mth group that has km elements. Of course, if we distribute n elements the kjs
have to satisfy
k1 + ‚ãÖ‚ãÖ‚ãÖ+ km = n .
Using exactly the same arguments as in the case where m = 2, we get the following.
There exist exactly
n!
k1!‚ãÖ‚ãÖ‚ãÖkm! different ways to distribute n elements into m groups of sizes k1, k2, . . . , km where
k1 + ‚ãÖ‚ãÖ‚ãÖ+ km = n.
In accordance with the binomial coefficient, we write
(
n
k1, . . . , km
) :=
n!
k1! ‚ãÖ‚ãÖ‚ãÖkm! ,
k1 + ‚ãÖ‚ãÖ‚ãÖ+ km = n ,
(A.16)
and call (
n
k1,...,km) a multinomial coefficient, read ‚Äún chose k1 up to km.‚Äù
Remark A.3.17. If m = 2, then k1 + k2 = n, and
( n
k1, k2
) = (
n
k1, n ‚àík1
) = ( n
k1
) = ( n
k2
) .
Example A.3.18. A deck of cards for playing skat consists of 32 cards. Three players each
gets 10 cards; the remaining two cards (called ‚Äúskat‚Äù) are placed on the table. How many
different distributions of the cards exist?
Answer: Let us first define what it means for two distribution of cards to be iden-
tical. Say this happens if each of the three players has exactly the same cards as in the
previous game. Therefore, the remaining two cards on the table are also identical. Hence
we distribute 32 cards into 4 groups possessing 10, 10, 10, and 2 elements. Consequently,
the number of different distributions equals5
5 The huge size of this number explains why playing skat never becomes boring.

466
‡±™
A Appendix
(
32
10, 10, 10, 2) =
32!
(10!)3 2! = 2.753294409 √ó 1015 .
Remark A.3.19. One may also look at multinomial coefficients from a different point of
view. Suppose we are given n balls of m different colors. Say there are k1 balls of the first
color, k2 balls of the second color, up to km balls of color m where, of course, we have
k1 + ‚ãÖ‚ãÖ‚ãÖ+ km = n. Then there exist
(
n
k1, . . . , km
)
different ways to order these n balls. This is followed by the same arguments as we used
in Example A.3.5 for m = 2.
For instance, given 3 blue, 4 red, and 2 white balls, there are
(
9
3, 4, 2) =
9!
3! 4! 2! = 1260
different ways to order them.
Finally, let us still mention that in the literature one sometimes finds another
(equivalent) way of introducing the multinomial coefficients. Given nonnegative inte-
gers k1, . . . , km with k1 + ‚ãÖ‚ãÖ‚ãÖ+ km = n, it follows that
(
n
k1, . . . , km
) = ( n
k1
) (n ‚àík1
k2
) (n ‚àík1 ‚àík2
k3
) ‚ãÖ‚ãÖ‚ãÖ(n ‚àík1 ‚àí‚ãÖ‚ãÖ‚ãÖ‚àíkm‚àí1
km
).
(A.17)
A direct proof of this fact is easy and left as an exercise.
There is a combinatorial interpretation of the expression on the right-hand side of
eq. (A.17). To reorder n balls of m different colors, one chooses first the k1 positions for
balls of color 1. There are ( n
k1) ways to do this. Thus, there remain n‚àík1 possible positions
for balls of color 2, and there are (n‚àík1
k2 ) possible choices for this, and so on. Note that at
the end there remain km positions for balls of color m; hence, the last term on the right-
hand side of eq. (A.17) equals 1.
Let us come now to the announced generalization of Proposition A.3.8.
Proposition A.3.20 (Multinomial theorem). Let n ‚â•0. Then for any m ‚â•1 and real num-
bers x1, . . . , xm,
(x1 + ‚ãÖ‚ãÖ‚ãÖ+ xm)n =
‚àë
k1+‚ãÖ‚ãÖ‚ãÖ+km=n
ki‚â•0
(
n
k1, . . . , km
) xk1
1 ‚ãÖ‚ãÖ‚ãÖxkm
m .
(A.18)
Proof. Equality (A.18) is proved by induction. In contrast to the proof of the binomial
theorem, now induction is done over m, the number of summands.
If m = 1, the assertion is valid due to trivial reasons.

A.3 Combinatorics
‡±™
467
Suppose now eq. (A.18) holds for m, all n ‚â•1, and all real numbers x1, . . . , xm. We
have to show the validity of eq. (A.18) for m + 1 and all n ‚â•1. Given real numbers
x1, . . . , xm+1 and n ‚â•1 set y := x1 + ‚ãÖ‚ãÖ‚ãÖ+ xm. Using Proposition A.3.8, by the validity of
eq. (A.18) for m and all n ‚àíj, 0 ‚â§j ‚â§n, we obtain
(x1 + ‚ãÖ‚ãÖ‚ãÖ+ xm+1)n = (y + xm+1)n =
n
‚àë
j=1
n!
j! (n ‚àíj)! xj
m+1yn‚àíj
=
n
‚àë
j=1
n!
j! (n ‚àíj)!
‚àë
k1+‚ãÖ‚ãÖ‚ãÖ+km=n‚àíj
ki‚â•0
(n ‚àíj)!
k1! ‚ãÖ‚ãÖ‚ãÖkm! xk1
1 ‚ãÖ‚ãÖ‚ãÖxkm
m xj
m+1 .
Replacing j by km+1 and combining both sums leads to
(x1 + ‚ãÖ‚ãÖ‚ãÖ+ xm+1)n =
‚àë
k1+‚ãÖ‚ãÖ‚ãÖ+km+1=n
ki‚â•0
n!
k1! ‚ãÖ‚ãÖ‚ãÖkm+1! xk1
1 ‚ãÖ‚ãÖ‚ãÖxkm+1
m+1 ,
hence eq. (A.18) is also valid for m + 1. This completes the proof.
Remark A.3.21. The number of summands in eq. (A.18) equals6 (n+m‚àí1
n
).
Example A.3.22. Let w, x, y, and z be four real numbers. Then we get
(w + x + y + z)5 =
‚àë
k1+‚ãÖ‚ãÖ‚ãÖ+k4=5
(
5
k1, k2, k3, k4
)wk1xk2yk3zk4 .
So, for example, the coefficient of w2xyz is (
5
2,1,1,1) = 60 or that of w2x2y equals
(
5
2,2,1,0) = 30.
A.3.4 Problems
Problem A.9. Given a set M of cardinality n ‚â•1. Argue why there are exactly (n
k) subsets
A ‚äÜM with cardinality k ‚â§n.
Problem A.10. Determine, with proof, the number of ordered triples (A1, A2, A3) of sets
such that
A1 ‚à™A2 ‚à™A3 = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
and
A1 ‚à©A2 ‚à©A3 = 0.
Problem A.11. Prove that
n
‚àë
k=0
(n
k)
2
= (2n
n ) .
(A.19)
6 Compare with Case 3 in Section A.3.2.

468
‡±™
A Appendix
Problem A.12. Let n be a natural number. Prove that
22n
2n + 1 < (2n
n ) < 22n
and
22n+1
2n + 2 < (2n + 1
n
) = (2n + 1
n + 1 ) < 22n+1.
Problem A.13. Let n be a natural number. Give proofs of the identities
(n
0) + (n
1) + ‚ãÖ‚ãÖ‚ãÖ+ (n
n) = 2n
and
(n
0) ‚àí(n
1) + ‚ãÖ‚ãÖ‚ãÖ+ (‚àí1)n(n
n) = 0.
Problem A.14. Given r ‚àà‚Ñïand an integer n ‚â•r, show that
(r
r) + (r + 1
r ) + ‚ãÖ‚ãÖ‚ãÖ+ (n
r) = (n + 1
r + 1) .
For example, if n ‚â•4, then
(4
4) + (5
4) + ‚ãÖ‚ãÖ‚ãÖ+ (n
4) = (n + 1
5 ) .
Problem A.15. What is the coefficient of x2 in (3x2‚àí2x‚àí1)7 where x
Ã∏= 0 is some variable?
Problem A.16. Given integers n ‚â•1 and k ‚â•n, how many vectors (k1, . . . , kn) of integers
exist for which kj ‚â•1 and k1 + ‚ãÖ‚ãÖ‚ãÖ+ kn = k. How about if we ask for kjs with kj ‚â•M,
1 ‚â§j ‚â§n, for some integer M ‚â•1? Of course, this question makes only sense if k ‚â•n M.
Problem A.17. Give an algebraic proof of equation (A.17).
A.4 Vectors and matrices
The aim of this section is to summarize results and notations about vectors and matrices
used throughout this book. For more detailed reading, we refer to any book about Linear
Algebra, for example, [Axl15].
Given two vectors x and y in ‚Ñùn, their7 scalar product is defined as
‚ü®x, y‚ü©:=
n
‚àë
j=1
xjyj,
x = (x1, . . . , xn), y = (y1, . . . , yn).
If x ‚àà‚Ñùn, then
7 Sometimes also called ‚Äúdot-product.‚Äù

A.4 Vectors and matrices
‡±™
469
|x| := ‚ü®x, x‚ü©1/2 = (
n
‚àë
j=1
x2
j )
1/2
denotes the Euclidean distance from x to 0. Thus, |x| may also be regarded as the length
of the vector x ‚àà‚Ñùn. In particular, we have |x| > 0 for all nonzero x ‚àà‚Ñùn.
Any matrix A = (Œ±ij)n
i,j=1 of real numbers Œ±ij generates a linear8 mapping (also de-
noted by A) via
Ax = (
n
‚àë
j=1
Œ±1jxj, . . . ,
n
‚àë
j=1
Œ±njxj),
x = (x1, . . . , xn) ‚àà‚Ñùn .
(A.20)
Conversely, any linear mapping A : ‚Ñùn ‚Üí‚Ñùn defines a matrix (Œ±ij)n
i,j=1 by representing
Aej ‚àà‚Ñùn as
Aej = (Œ±1j, . . . , Œ±nj) ,
j = 1, . . . , n .
Here ej = (0, . . . , 0, 1
‚èü‚èü‚èü‚èü‚èü‚èü‚èü
j
, 0 . . . , 0) denotes the jth unit vector in ‚Ñùn. With this generated
matrix (Œ±ij)n
i,j=1, the linear mapping A acts as stated in eq. (A.20). Consequently, we may
always identify linear mappings in ‚Ñùn with n √ó n-matrices (Œ±ij)n
i,j=1.
Given two n √ó n matrices A = (Œ±ij)n
i,j=1 and B = (Œ≤ij)n
i,j=1, their product A ‚àòB, or A B in
short, is the matrix C = (Œ≥ij)n
i,j=1 where
Œ≥ij =
n
‚àë
k=1
Œ±ikŒ≤kj ,
1 ‚â§i, j ‚â§n .
Note that in general A B
Ã∏= B A. An important formula for the determinant of the product
of two matrices is
det(A B) = det(A) ‚ãÖdet(B) .
(A.21)
An n √ó n matrix A is said to be regular9 if the generated linear mapping is one-to-
one, that is, if Ax = 0 implies x = 0. This is equivalent to the fact that the determinant
det(A) is nonzero.
Let A = (Œ±ij)n
i,j=1 be an n √ó n matrix. Then its transposed matrix is defined as
AT := (Œ±ji)n
i,j=1. With this notation, it follows for x, y ‚àà‚Ñùn that
‚ü®Ax, y‚ü©= ‚ü®x, ATy‚ü©.
Moreover, we have (A B)T = BTAT for any two n √ó n matrices A and B, and, of course,
8 A mapping A : ‚Ñùn ‚Üí‚Ñùn is said to be linear if A(Œ±x + Œ≤y) = Œ±Ax + Œ≤Ay for all Œ±, Œ≤ ‚àà‚Ñùand x, y ‚àà‚Ñùn.
9 Sometimes also called nonsingular or invertible.

470
‡±™
A Appendix
(AT)T = A. The following property of the transposed matrix is crucial:
det(AT) = det(A) .
In particular, the matrix A is regular if and only if AT is regular.
A matrix A with A = AT is said to be symmetric. Equivalently, A satisfies
‚ü®Ax, y‚ü©= ‚ü®x, Ay‚ü©,
x, y ‚àà‚Ñùn.
An n √ó n matrix R = (rij)n
i,j=1 is positive definite (or positive in short) provided it is
symmetric and
‚ü®Rx, x‚ü©=
n
‚àë
i,j=1
rijxixj > 0,
x = (x1, . . . , xn)
Ã∏= 0.
We will write R > 0 in this case. In particular, each positive matrix R is regular and its
determinant satisfies det(R) > 0.
Let A = (Œ±ij)n
i,j=1 be an arbitrary regular n √ó n matrix. Set
R := AAT ,
(A.22)
that is, the entries rij of R are computed by
rij =
n
‚àë
k=1
Œ±ikŒ±jk ,
1 ‚â§i, j ‚â§n .
Proposition A.4.1. Suppose the matrix R is defined by eq. (A.22) for some regular A. Then
it follows that R > 0.
Proof. Because of
RT = (AAT)
T = (AT)
TAT = AAT = R ,
the matrix R is symmetric. Furthermore, for x ‚àà‚Ñùn with x
Ã∏= 0, we obtain
‚ü®Rx, x‚ü©= ‚ü®AATx, x‚ü©= ‚ü®ATx, ATx‚ü©= ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ATxÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
2 > 0.
Hereby we used that for a regular A, the transposed matrix AT is regular, too. Conse-
quently, if x Ã∏= 0, then ATx Ã∏= 0, and thus |ATx| > 0. This completes the proof.
The identity matrix In is defined as the n √ó n matrix with entries Œ¥ij, 1 ‚â§i, j ‚â§n,
where
Œ¥ij = {1
if i = j,
0
if i
Ã∏= j.
(A.23)

A.4 Vectors and matrices
‡±™
471
Of course, Inx = x for x ‚àà‚Ñùn and, moreover, det(In) = 1.
Given a regular n √ó n matrix A, there is a unique matrix B such that AB = In. The
matrix B is called the inverse matrix of A and denoted by A‚àí1. Recall that also A‚àí1A = In
and, moreover, (AT)‚àí1 = (A‚àí1)T. Equation (A.21) lets us conclude that for any regular
matrix A, it follows that
1 = det(In) = det(AA‚àí1) = det(A) ‚ãÖdet(A‚àí1)
‚áí
det(A‚àí1) =
1
det(A) .
An n √ó n matrix U is said to be unitary or orthogonal provided that
UUT = UTU = In
(A.24)
with identity matrix In. Another way to express this is either that UT = U‚àí1 or, equiva-
lently, that U satisfies
‚ü®Ux, Uy‚ü©= ‚ü®x, y‚ü©,
x, y ‚àà‚Ñùn .
In particular, for each x ‚àà‚Ñùn it follows that
|Ux|2 = ‚ü®Ux, Ux‚ü©= ‚ü®x, x‚ü©= |x|2 ,
that is, U preserves the length of vectors in ‚Ñùn. Indeed, this property characterizes uni-
tary matrices. It is a nice task to prove this.
It is easy to see that an n √ó n matrix U is unitary if and only if its column vectors
u1, . . . , un form an orthonormal basis in ‚Ñùn. That is, ‚ü®ui, uj‚ü©= Œ¥ij with Œ¥ijs as in (A.23). This
characterization of unitary matrices remains valid when we take the column vectors
instead of those generated by the rows.
We saw in Proposition A.4.1 that each matrix R of the form (A.22) is positive. Next
we prove that conversely, each R > 0 may be represented in this way.
Proposition A.4.2. Let R be an arbitrary positive n√ón matrix. Then there exists a regular
matrix A such that R = AAT.
Proof. Since R is symmetric, we may apply the principal axis transformation for sym-
metric matrices. It asserts that there exists a diagonal matrix10 D and a unitary matrix
U such that
R = UDUT .
Let Œ¥1, . . . , Œ¥n be the entries of D at its diagonal. From R > 0 we derive Œ¥j > 0, 1 ‚â§j ‚â§n.
To see this fix j ‚â§n and set x := Uej where as above ej denotes the jth unit vector in ‚Ñùn.
Then UTx = ej, hence
10 The entries dij of D satisfy dij = 0 if i
Ã∏= j.

472
‡±™
A Appendix
0 < ‚ü®Rx, x‚ü©= ‚ü®UDUTx, x‚ü©= ‚ü®DUTx, UTx‚ü©= ‚ü®Dej, ej‚ü©= Œ¥j .
Because of Œ¥j > 0, we may define D1/2 as diagonal matrix with entries Œ¥1/2
j
on its diagonal.
Setting A := UD1/2 and because (D1/2)T = D1/2, it follows that
R = (UD1/2)(UD1/2)
T = AAT .
Since
det(A)2 = det(A)det(A) = det(A)det(AT) = det(A AT) = det(R) > 0 ,
the matrix A is regular, and this completes the proof.
Another way to argue without using properties of determinants is as follows. As-
sume the matrix A is not regular. Then this is also true for AT. Hence there is a nonzero
vector x ‚àà‚Ñùn such that ATx = 0. But this implies Rx = A(ATx) = A(0) = 0 which
contradicts the regularity of R. Thus, A has to be regular.
Remark A.4.3. Note that representation (A.22) is not unique. Indeed, if R = A AT, then
we also have R = (AV)(AV)T for any unitary matrix V.
A.5 Some analytic tools
The aim of this section is to present some special results of Calculus that play an impor-
tant role in the book. Hereby we restrict ourselves to those topics that are maybe less
known and that are not necessarily taught in a basic Calculus course. For a general intro-
duction to Calculus, including those topics as convergence of power series, fundamental
theorem of Calculus, mean-value theorem, and so on, we refer to the books [Spi08] and
[Ste15].
We start with a result that is used in the proof of Poisson‚Äôs limit theorem (Theo-
rem 1.4.22). From Calculus, it is well known that for x ‚àà‚Ñù,
lim
n‚Üí‚àû(1 + x
n)
n
= ex .
(A.25)
The probably easiest proof of this fact is via the approach presented in [Spi08]. There
the logarithm function is defined by ln t = ‚à´
t
1
1
sds, t > 0. Hence, l‚ÄôH√¥pital‚Äôs rule implies
lim
t‚Üí‚àût ln(1 + x
t ) = x ,
x ‚àà‚Ñù.
From this, eq. (A.25) easily follows by the continuity of the exponential function.
The next proposition may be viewed as a slight generalization of eq. (A.25).

A.5 Some analytic tools
‡±™
473
Proposition A.5.1. Let (xn)n‚â•1 be a sequence of real numbers with limn‚Üí‚àûxn = x for
some x ‚àà‚Ñù. Then we get
lim
n‚Üí‚àû(1 + xn
n )
n
= ex .
Proof. Because of eq. (A.25), it suffices to verify that
lim
n‚Üí‚àû
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
(1 + xn
n )
n
‚àí(1 + x
n)
nÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
= 0 .
(A.26)
Since the sequence (xn)n‚â•1 is convergent, it is bounded. Consequently, there is a c > 0
such that for all n ‚â•1, we have |xn| ‚â§c. Of course, we may also assume |x| ‚â§c. Fix for a
moment n ‚â•1 and set
a := 1 + xn
n
and
b := 1 + x
n .
The choice of c > 0 yields |a| ‚â§1 + c/n, as well as |b| ‚â§1 + c/n. Hence it follows
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®an ‚àíbnÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®= |a ‚àíb| ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®an‚àí1 + an‚àí2b + ‚ãÖ‚ãÖ‚ãÖ+ abn‚àí2 + bn‚àí1ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§|a ‚àíb| (|a|n‚àí1 + |a|n‚àí2|b| + ‚ãÖ‚ãÖ‚ãÖ+ |a||b|n‚àí2 + |b|n‚àí1)
‚â§|a ‚àíb| n (1 + c
n)
n‚àí1
‚â§C n |a ‚àíb| .
Here C > 0 is some constant that exists since (1+c/n)n‚àí1 converges to ec. By the definition
of a and b,
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
(1 + xn
n )
n
‚àí(1 + x
n)
nÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®
‚â§C n |xn ‚àíx|
n
= C |x ‚àíxn| .
Since xn ‚Üíx, this immediately implies eq. (A.26) and proves the proposition.
Our next objective is to present some properties of power series and functions gen-
erated by them. Hereby we restrict ourselves to such assertions that we will use in this
book. For further reading, we refer to Part IV in [Spi08].
Let (ak)k‚â•0 be a sequence of real numbers. Then its radius of convergence r ‚àà[0, ‚àû]
is defined by
r :=
1
lim supk‚Üí‚àû|ak|1/k .
Hereby we let 1/0 := ‚àûand 1/‚àû:= 0. If 0 < r ‚â§‚àûand |x| < r, then the infinite series
f (x) :=
‚àû
‚àë
k=0
ak xk
(A.27)

474
‡±™
A Appendix
converges (even absolutely). Hence the function f generated by eq. (A.27) is well defined
on its region of convergence {x ‚àà‚Ñù: |x| < r}. We say that f is represented as a power
series on {x ‚àà‚Ñù: |x| < r}.
The function f defined by eq. (A.27) is infinitely often differentiable on its region of
convergence and (compare with [Spi08, Chapter 27, Theorem 6])
f (n)(x) =
‚àû
‚àë
k=n
k (k ‚àí1) ‚ãÖ‚ãÖ‚ãÖ(k ‚àín + 1) ak xk‚àín
=
‚àû
‚àë
k=0
(k + n) (k + n ‚àí1) ‚ãÖ‚ãÖ‚ãÖ(k + 1) ak+n xk
= n!
‚àû
‚àë
k=0
(n + k
k
) ak+n xk .
(A.28)
The coefficients n!(n+k
k ) an+k in the series representation of the nth derivative f (n)
possess the same radius of convergence as the original sequence (ak)k‚â•0. This is easy to
see for n = 1. The general case then follows by induction.
Furthermore, eq. (A.28) implies an = f (n)(0)/n!, which, in particular, tells us that
given f , the coefficients (ak)k‚â•0 in representation (A.27) are unique.
Proposition A.5.2. If n ‚â•1 and |x| < 1 then
1
(1 + x)n =
‚àû
‚àë
k=0
(‚àín
k ) xk .
(A.29)
Proof. Using the formula to add a geometric series and applying (‚àí1
k ) = (‚àí1)k yields for
|x| < 1 that
1
1 + x =
‚àû
‚àë
k=0
(‚àí1)k xk =
‚àû
‚àë
k=0
(‚àí1
k )xk .
Consequently Proposition A.5.2 holds for n = 1.
Assume now we have proven the proposition for n ‚àí1, that is, if |x| < 1, then
1
(1 + x)n‚àí1 =
‚àû
‚àë
k=0
(‚àín + 1
k
)xk .
Differentiating this equality in the region {x : |x| < 1} implies
‚àí
n ‚àí1
(1 + x)n =
‚àû
‚àë
k=1
(‚àín + 1
k
) k xk‚àí1 =
‚àû
‚àë
k=0
(‚àín + 1
k + 1 ) (k + 1) xk .
(A.30)

A.5 Some analytic tools
‡±™
475
Direct calculations give
‚àík + 1
n ‚àí1 (‚àín + 1
k + 1 ) = ‚àík + 1
n ‚àí1 ‚ãÖ(‚àín + 1)(‚àín) ‚ãÖ‚ãÖ‚ãÖ(‚àín + 1 ‚àí(k + 1) + 1)
(k + 1)!
= (‚àín)(‚àín ‚àí1) ‚ãÖ‚ãÖ‚ãÖ(‚àín ‚àík + 1)
k!
= (‚àín
k ) ,
which, together with eq. (A.30), leads to
1
(1 + x)n =
‚àû
‚àë
k=0
(‚àín
k ) xk .
This completes the proof of Proposition A.5.2.
The next proposition may be viewed as a counterpart to eq. (A.11) in the case of
generalized binomial coefficients.
Proposition A.5.3. For k ‚â•0 and m, n ‚àà‚Ñï,
k
‚àë
j=0
(‚àín
j )( ‚àím
k ‚àíj) = (‚àín ‚àím
k
) .
Proof. The proof is similar to that of Proposition A.3.9. Using Proposition A.5.2, we rep-
resent the function (1+x)‚àín‚àím as a power series in two different ways. On the one hand,
for |x| < 1, we have the representation
1
(1 + x)n+m =
‚àû
‚àë
k=0
(‚àín ‚àím
k
) xk
(A.31)
and, on the other hand,
1
(1 + x)n+m = [
‚àû
‚àë
j=0
(‚àín
j ) xj][
‚àû
‚àë
l=0
(‚àím
l ) xl]
=
‚àû
‚àë
k=0
[ ‚àë
j+l=k
(‚àín
j )(‚àím
l )] xk =
‚àû
‚àë
k=0
[
k
‚àë
j=0
(‚àín
j )( ‚àím
k ‚àíj)] xk .
(A.32)
As observed above, the coefficients in a power series are uniquely determined. Thus,
the coefficients in eqs. (A.31) and (A.32) have to coincide, which implies
k
‚àë
j=0
(‚àín
j )( ‚àím
k ‚àíj) = (‚àín ‚àím
k
),
as asserted.

476
‡±™
A Appendix
Let f : ‚Ñùn ‚Üí‚Ñùbe a function. How does one define the integral ‚à´‚Ñùn f (x) dx? To sim-
plify the notation, let us restrict ourselves to the case n = 2. The main problems already
become clear in this case and the obtained results easily extend to higher dimensions.
The easiest way to introduce the integral of a function of two variables is as follows:
‚à´
‚Ñù2
f (x) dx :=
‚àû
‚à´
‚àí‚àû
[
‚àû
‚à´
‚àí‚àû
f (x1, x2) dx2]dx1 .
In order for this double integral to be well defined, we have to assume the existence
of the inner integral for each fixed x1 ‚àà‚Ñùand then the existence of the integral of the
function
x1 Û≥®É‚Üí
‚àû
‚à´
‚àí‚àû
f (x1, x2) dx2 .
In doing so, the following question arises immediately: why do we not define the integral
in reversed order, that is, first integrating with respect to x1 and then with respect to x2?
To see the difficulties that may appear, let us consider the following example.
Example A.5.4. The function f : ‚Ñù2 ‚Üí‚Ñùis defined as follows (see Fig. A.8): If either
x1 < 0 or x2 < 0 set f (x1, x2) = 0. If x1, x2 ‚â•0 define f by
f (x1, x2) :=
{
{
{
{
{
{
{
+1
if x1 ‚â§x2 < x1 + 1,
‚àí1
if x1 + 1 ‚â§x2 ‚â§x1 + 2,
0
otherwise.
We immediately see that
‚àû
‚à´
0
f (x1, x2) dx2 = 0
for all x1 ‚àà‚Ñù,
hence
‚àû
‚à´
0
[
‚àû
‚à´
0
f (x1, x2) dx2]dx1 = 0 .
On the other hand, it follows
‚àû
‚à´
0
f (x1, x2) dx1 =
{
{
{
{
{
{
{
{
{
‚à´
x2
0 (+1) dx1 = x2
if 0 ‚â§x2 < 1,
‚à´
x2‚àí1
0
(‚àí1) dx1 + ‚à´
x2
x2‚àí1(+1) dx1 = 2 ‚àíx2
if 1 ‚â§x2 ‚â§2,
‚à´
x2
x2‚àí2 f (x1, x2) dx1 = 0
if 2 < x2 < ‚àû,
leading to
‚àû
‚à´
0
[
‚àû
‚à´
0
f (x1, x2) dx1] dx2 =
1
‚à´
0
x2 dx2 +
2
‚à´
1
(2 ‚àíx2) dx2 = 1 .

A.5 Some analytic tools
‡±™
477
Thus, in this case
‚àû
‚à´
0
[
‚àû
‚à´
0
f (x1, x2) dx1] dx2
Ã∏=
‚àû
‚à´
0
[
‚àû
‚à´
0
f (x1, x2) dx2] dx1 .
Figure A.8: On the left-hand side, one first integrates f over x2 with x1 fixed. On the right-hand side, the
integration of f is done over x1 with x2 fixed. In this case three different regions for the choice of x2 have to
be considered.
Example A.5.4 shows that neither the definition of the integral of functions of several
variables nor the interchange of integrals are unproblematic. Fortunately, we have the
following positive result (see [Dur19, Section 1.7], for more information).
Proposition A.5.5 (Fubini‚Äôs theorem). If f (x1, x2) ‚â•0 for all (x1, x2) ‚àà‚Ñù2, then one may
interchange the order of integration. In other words,
‚àû
‚à´
‚àí‚àû
[
‚àû
‚à´
‚àí‚àû
f (x1, x2) dx1] dx2 =
‚àû
‚à´
‚àí‚àû
[
‚àû
‚à´
‚àí‚àû
f (x1, x2) dx2] dx1 .
(A.33)
Hereby we do not exclude that one of the two, hence also the other, iterated integral is
infinite.
Furthermore, in the general case (the function f may attain also negative values)
equality (A.33) holds provided that one of the iterated integrals, for example,
‚àû
‚à´
‚àí‚àû
[
‚àû
‚à´
‚àí‚àû
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®f (x1, x2)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®dx1] dx2
is finite. Due to the first part, then we also have
‚àû
‚à´
‚àí‚àû
[
‚àû
‚à´
‚àí‚àû
ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®f (x1, x2)ÛµÑ®ÛµÑ®ÛµÑ®ÛµÑ®dx2] dx1 < ‚àû.

478
‡±™
A Appendix
Whenever a function f on ‚Ñù2 satisfies one of the two assumptions in Proposi-
tion A.5.5, by
‚à´
‚Ñù2
f (x) dx :=
‚àû
‚à´
‚àí‚àû
[
‚àû
‚à´
‚àí‚àû
f (x1, x2) dx1] dx2 =
‚àû
‚à´
‚àí‚àû
[
‚àû
‚à´
‚àí‚àû
f (x1, x2) dx2] dx1
the integral of f is well defined. Given a subset B ‚äÜ‚Ñù2, we set
‚à´
B
f (x) dx := ‚à´
‚Ñù2
f (x) 1B(x) dx ,
provided the integral exists. Recall that 1B denotes the indicator function of B introduced
in eq. (3.21).
For example, let K1 be the unit circle in ‚Ñù2, that is, K1 = {(x1, x2) : x2
1 + x2
2 ‚â§1}, then
it follows
‚à´
K1
f (x) dx =
1
‚à´
‚àí1
‚àö1‚àíx2
1
‚à´
‚àí‚àö1‚àíx2
1
f (x1, x2) dx2 dx1 .
Or, if B = {(x1, x2, x3) ‚àà‚Ñù3 : x1 ‚â§x2 ‚â§x3}, we have
‚à´
B
f (x) dx =
‚àû
‚à´
‚àí‚àû
x3
‚à´
‚àí‚àû
x2
‚à´
‚àí‚àû
f (x1, x2, x3) dx1 dx2 dx3 .
Remark A.5.6. Proposition A.5.5 is also valid for infinite double series. Let Œ±ij be real
numbers either satisfying Œ±ij ‚â•0 or ‚àë‚àû
i=0 ‚àë‚àû
j=0 |Œ±ij| < ‚àû, then this implies
‚àû
‚àë
i=0
‚àû
‚àë
j=0
Œ±ij =
‚àû
‚àë
j=0
‚àû
‚àë
i=0
Œ±ij =
‚àû
‚àë
i,j=0
Œ±ij .
Even more generally, if the sets Ik ‚äÜ‚Ñï2
0, k ‚àà‚Ñï0, form a disjoint partition of ‚Ñï2
0, then
‚àû
‚àë
i,j=0
Œ±ij =
‚àû
‚àë
k=0
‚àë
(i,j)‚ààIk
Œ±ij .
For example, if Ik = {(i, j) ‚àà‚Ñï2 : i + j = k}, then
‚àû
‚àë
i,j=0
Œ±ij =
‚àû
‚àë
k=0
‚àë
(i,j)‚ààIk
Œ±ij =
‚àû
‚àë
k=0
k
‚àë
i=0
Œ±i k‚àíi .

Bibliography
[Art64]
Emil Artin. The Gamma Function. Athena Series: Selected Topics in Mathematics, Holt, Rinehart and
Winston, New York, Toronto, London, 1964.
[Axl15]
Sheldon Axler. Linear Algebra Done Right. Springer International Publishing, Cham Heidelberg,
New York, Dordrecht, London, 3rd edition, 2015.
[Bau96]
Heinz Bauer, Probability Theory. De Gruyter Studies in Mathematics, Walter de Gruyter & Co.,
Berlin, 1996.
[Bau01]
Heinz Bauer. Measure and Integration Theory. De Gruyter Studies in Mathematics, Walter de
Gruyter & Co., Berlin, 2001.
[Bil12]
Patrick Billingsley. Probability and Measure. John Wiley and Sons, Inc., Hoboken, 4th edition, 2012.
[CB02]
George Casella and Roger L. Berger. Statistical Inference. Duxburg Press, Pacific Grove, CA, 2nd
edition, 2002.
[CL23]
Sebastian M. CioabƒÉ and Werner Linde. A Bridge to Advanced Mathematics: From Natural to Complex
Numbers. Pure and Applied Undergraduate Texts, 58, American Mathematical Society, Providence,
RI, 2023.
[Coh13]
Donald L. Cohn. Measure Theory. Birkh√§user Advanced Texts, Birkh√§user, Springer, New York, 2nd
edition, 2013.
[Dud02] Richard M. Dudley. Real Analysis and Probability. Cambridge University Press, Cambridge, 2002.
[Dur19]
Richard Durrett. Probability: Theory and Examples. Cambridge University Press, New York, 5th
edition, 2019.
[Eri73]
Bruce K. Erickson. The strong law of large numbers when the mean is undefined. Trans. Amer. Math.
Soc. 185 (1973), 371‚Äì381.
[Fel68]
William Feller. An Introduction to Probability Theory and its Applications, volume 1. John Wiley and
Sons, New York, London, Sydney, 3rd edition, 1968.
[Fis11]
Hans Fischer. A History of the Central Limit Theorem. Ergebnisse der Mathematik und ihrer
Grenzgebiete, Springer, New York, 2011.
[Fsh71]
Ronald Aymer Fisher. The Design of Experiments. Hafner Press, London, 9th edition, 1971.
[Gha19]
Saeed Ghahramani. Fundamentals of Probability. Pearson Education, Inc., Upper Saddle River, NJ,
4th edition, 2019.
[GS01]
Geoffrey R. Grimmett and David R. Stirzacker. One Thousand Exercises in Probability. Oxford
University Press, Oxford, New York, 1st edition, 2001.
[GS20]
Geoffrey R. Grimmett and David R. Stirzacker. Probability and Random Processes. Oxford University
Press, Oxford, New York, 4th edition, 2020.
[Hal14]
Paul R. Halmos. Measure Theory. Springer, New York, NY, 2014.
[Kal21]
Olav Kallenberg. Foundations of Modern Probability. Probability Theory and Stochastic Modeling,
99, Springer, Cham, 3rd edition, 2021.
[Kho07]
Davar Khoshnevisan. Probability. Graduate Studies in Mathematics, 80, American Mathematical
Society, New York, 2007.
[Kle20]
Achim Klenke. Probability Theory ‚Äì A Comprehensive Course. Universitext, Springer, Cham, 3rd
edition, 2020.
[Kol33]
Andrey Nikolajewitsch Kolmogorov. Grundbegriffe der Wahrscheinlichkeitsrechnung. Ergebnisse der
Mathematik und ihrer Grenzgebiete, Julius Springer, Berlin, 1933.
[Lag13]
Jeffrey C. Lagarias. Euler‚Äôs constant: Euler‚Äôs work and modern developments. Bull. Amer. Math. Soc.
(N. S.) 50 (2013), 527‚Äì628.
[LG22]
Jean-Fran√ßois Le Gall. Measure Theory, Probability, and Stochastic Processes. Graduate Texts in
Mathematics, Springer, Cham, 2022.
[Mor16]
Samuel G. Moreno. A Short and elementary proof of the Basel problem. College Math. J. 47 (2016),
134‚Äì135.
https://doi.org/10.1515/9783111325064-010

480
‡±™
Bibliography
[Pao06]
Marc S. Paolella. Fundamental Probability. A Computational Approach. John Wiley and Sons,
Chichester, 2006.
[Par05]
Kalyanapuram Rangachari Parthasarathy. Introduction to Probability and Measure. Texts and
Readings in Mathematics, 33, Corrected reprint of the 1977 original, Hindustan Book Agency,
New Delhi, 2005.
[Rev13]
P√°l R√©v√©sz. Random Walk in Random and Non-random Environments. World Scientific Publishing Co.
Pte. Ltd., Hackensack, NJ, 3rd edition, 2013.
[Ros06]
Jeffrey S. Rosenthal. A First Look at Rigorous Probability Theory. World Scientific Publishing Co. Pte.
Ltd., Hackensack, NJ, 2nd edition, 2006.
[Rss14]
Sheldon Ross. A First Course in Probability. Pearson Education Limited, Essex, 9th edition, 2014.
[Sam04] Dov Samet, Iddo Samet and David Schmeidler. One observation behind two-envelope puzzles. Amer.
Math. Monthly 111 (2004), 347‚Äì351.
[Sch17]
Ren√© L. Schilling. Measures, Integrals and Martingales. Cambridge University Press, Cambridge,
2nd edition, 2017.
[Spi08]
Michael Spivak. Calculus. Publish or Perish, Houston, TX, 4th edition, 2008.
[Ste15]
James Stewart. Calculus. Cengage Learning, Boston, 8th edition, 2015.
[Sti03]
David R. Stirzaker. Elementary Probability. Cambridge University Press, Cambridge, 2nd edition,
2003.

Index
Absolute nth moment
‚Äì of a random variable 257
Œ±-significance test 381
‚Äì most powerful 382
Arcsine distribution 68
Arcsine law
‚Äì for random walks 303
‚Ñ¨Œ±,Œ≤, beta distribution 66
Banach‚Äôs matchbox problem 46
Basel problem 19
Bayes‚Äô formula 118
Bernoulli trial 211
Bernstein polynomial 367
Berry‚ÄìEss√©en theorem 366
Bertrand paradox 109
Beta distribution 66
Beta function 65
Bias
‚Äì of an estimator 428
Binary fraction 189
Binomial coefficient 458
‚Äì generalized 461
Binomial distribution 28
Binomial theorem 459
Bn,p, binomial distribution 28
B‚àí
n,p, negative binomial distribution 45
Boole‚Äôs inequality 11
Borel œÉ-field
‚Äì on ‚Ñù6
‚Äì on ‚Ñùn 81
Borel set
‚Äì in ‚Ñù6
‚Äì in ‚Ñùn 81
Borel‚ÄìCantelli lemma 335
Box
‚Äì n-dimensional 81
Boy or girl paradox 279
Buffon‚Äôs needle test 88
Cantor set 54
Cardinality of a set 451
Cartesian product 453
Cauchy distribution 69
‚Äì general 109
Cauchy‚ÄìSchwarz inequality 276
CDF 70
Central limit theorem 350
‚Äì for Œì-distributed random variables 363
‚Äì for binomial random variables 359
‚Äì for Poisson random variables 361
Chain rule
‚Äì for conditional probabilities 129
Chebyshev‚Äôs inequality 328
œá2-distribution 64
œá2-tests
‚Äì known expected value 406
‚Äì unknown expected value 407
Clopper‚ÄìPearson intervals 440
Complementary set 452
Completely normal numbers 347
Conditional distribution 115
Conditional probability 115
Confidence intervals 433
‚Äì for binomial populations 438
‚Äì approximative ones 443
‚Äì exact ones 440
‚Äì for hypergeometric populations 444
‚Äì for normal populations 435
Confidence regions 433
Continuity correction
‚Äì for normal approximation 353
Continuity of a probability measure
‚Äì from above 11
‚Äì from below 11
Convergence
‚Äì almost surely 342
‚Äì in distribution 349
‚Äì in probability 341
Convolution
‚Äì of two functions 208
Convolution formula
‚Äì ‚Ñï0-valued random variables 205
‚Äì ‚Ñ§-valued random variables 204
‚Äì continuous random variables 208
Coordinate mappings
‚Äì of a random vector 146
Correlated random variables 274
Correlation coefficient 276
Coupon collector‚Äôs problem 255
Covariance
‚Äì of two random variables 271
‚Äì properties 272
https://doi.org/10.1515/9783111325064-011

482
‡±™
Index
Covariance matrix
‚Äì of a normal vector 320
‚Äì of a random vector 318
Critical region 377
Cumulative distribution function
‚Äì of a probability measure 70
‚Äì of a random variable 139
De Morgan‚Äôs rules 453
Density
‚Äì of a probability measure
‚Äì multivariate 81
‚Äì univariate 50
Density function
‚Äì of a probability measure
‚Äì multivariate 81
‚Äì univariate 49
‚Äì of a random variable 136
‚Äì of a random vector 154
Dependence of events 120
Dilemma
‚Äì of hypothesis testing 380
Dirac measure 21
Discrete random variable 136
Disjoint sets 452
Distributing particles 23, 32
‚Äì anonymous ones 24
‚Äì distinguishable ones 24
Distribution
‚Äì of a random variable 135
‚Äì of a random vector 147
Distribution assumption 371
Distribution density
‚Äì of a random variable 136
‚Äì of a random vector 154
Distribution function
‚Äì of a probability measure 70
‚Äì of a random variable 139
Distributive law
‚Äì intersection and union 452
Double factorial 58
Drawing with replacement
‚Äì no order 463
‚Äì with order 462
Drawing without replacement
‚Äì no order 463
‚Äì with order 462
Dyadic rational number 189
EŒª, exponential distribution 61
EŒª,n, Erlang distribution 63
Elementary event 2
Envelope exchange paradox 287
Erlang distribution 63
Error
‚Äì of the first kind 378
‚Äì of the second kind 378
Error function
‚Äì Gaussian 72
Estimator 415
‚Äì efficient 431
‚Äì maximum likelihood 418
‚Äì unbiased 424
‚Äì uniformly best 430
Euclidean distance
‚Äì in ‚Ñùn 468
Euler‚Äôs constant 256
Event 2
‚Äì certain 3
‚Äì elementary 2
‚Äì impossible 3
Expected value
‚Äì of continuous random variables 245
‚Äì of discrete random variables 235
‚Äì of nonnegative random variables
‚Äì continuous case 243
‚Äì discrete case 236
‚Äì of random vectors 318
‚Äì properties 250
Exponential distribution 61
F-distribution 229
F-tests 412
f. a. 334
Factorial 456
Finite additivity 8
Fisher information 431
Fisher‚ÄìSnecedor distribution 229
Fisher‚Äôs
‚Äì lemma 392
‚Äì theorem 393
Floor function 456
Frequency
‚Äì absolute 7
‚Äì relative 7
Fubini‚Äôs theorem 477
Function
‚Äì absolutely integrable 244

Index
‡±™
483
‚Äì integrable 243
‚Äì measurable 180
Gambler‚Äôs ruin 293
ŒìŒ±,Œ≤, gamma distribution 61
Gamma function 58
Gauss test
‚Äì one-sided 400
‚Äì two-sided 401
Gaussian Œ¶-function 72
Gaussian error function 72
Generalized binomial coefficient 461
Generated œÉ-field 4
Generating function
‚Äì of a nonnegative random variable 308
‚Äì of an ‚Ñï0-valued random variable 231
Geometric distribution 41
Gp, geometric distribution 41
Histogram correction
‚Äì for normal approximation 353
Hitting time theorem 302
HN,M,n, hypergeometric distribution 39
Hypergeometric distribution 39
Hypothesis
‚Äì alternative 376
‚Äì null 376
Hypothesis test 377
Identically distributed 135
Identity matrix 470
Inclusion‚Äìexclusion formula 14, 104
Independence
‚Äì of infinitely many events 335
‚Äì of n events 123
‚Äì of two events 120
Independent random variables 157
‚Äì continuous case 165
‚Äì discrete case 161
‚Äì infinitely many 193
Independent repetition
‚Äì of an experiment 372
Indicator function
‚Äì of a set 164
Inequality
‚Äì Boole‚Äôs 11
‚Äì Cauchy‚ÄìSchwarz 276
‚Äì Chebyshev‚Äôs 328
‚Äì Lyapunov‚Äôs 258
Initial model
‚Äì of a statistical experiment 372
Intersection
‚Äì of sets 451
Interval estimator 432
i. o. 334
Joint density
‚Äì of n random variables 154
Joint distribution
‚Äì of n random variables 147
Laplace distribution 21
Law
‚Äì of iterated logarithm 357
‚Äì of multiplication 112
‚Äì of total probability 115
Lemma
‚Äì Borel‚ÄìCantelli 335
‚Äì Fisher‚Äôs 392
Likelihood function
‚Äì continuous case 417
‚Äì discrete case 417
Log-likelihood function 418
Loss function
‚Äì of an estimator 427
Lower limit
‚Äì of events 333
Marginal distributions
‚Äì of a random vector 148
‚Äì continuous case 154
‚Äì discrete case 150
Marriage problem 283
Matchbox problem 46
Matrix
‚Äì identity 470
‚Äì inverse 471
‚Äì invertible 469
‚Äì nonsingular 469
‚Äì orthogonal 471
‚Äì positive definite 470
‚Äì regular 469
‚Äì symmetric 470
‚Äì transposed 469
‚Äì unitary 471
Maximum likelihood estimator 418
Measurable function 180

484
‡±™
Index
Median
‚Äì of a random variable 247
MLE 418
Moments
‚Äì of a random variable 257
Monkey at the cliff 299
Monte Carlo method 344
Monty Hall problem 106
Multinomial
‚Äì coefficient 465
‚Äì random vector 152
‚Äì theorem 466
Multinomial distribution 31
‚Ñï, natural numbers 451
Needle test 88
Negative binomial distribution 45
Negatively correlated 278
ùí©(Œº, R), normal distribution
‚Äì multivariate 316
ùí©(Œº, œÉ2), normal distribution
‚Äì univariate 57
Normal distribution
‚Äì multivariate 316
‚Äì univariate 57
Normal numbers 346
‚Ñï0, natural numbers with zero 451
Occurrence
‚Äì of an event 3
Occurrence of events
‚Äì finally always 334
‚Äì infinitely often 334
Order statistics 171
‚Äì density 174
‚Äì distribution function 172
Pairwise independence 122
Paradox
‚Äì boy or girl 279
‚Äì envelope exchange 287
‚Äì of Bertrand 109
‚Äì of Chevalier de M√©r√© 105
Parameter set 374
Pascal‚Äôs triangle 459
Permutation 456
Point estimator 415
Point measure 21
PoisŒª, Poisson distribution 34
Poisson distribution 34
Poisson‚Äôs limit theorem 35
Positively correlated 278
Power function
‚Äì of a test 379
Power series 473
Powerset 451
Preimage 454
Principal axis transformation 471
Probabilities
‚Äì a posteriori 117
‚Äì a priori 117
Probability density function
‚Äì multivariate 81
‚Äì univariate 49
Probability distribution 9
‚Äì of a random variable 135
‚Äì continuous case 139
‚Äì discrete case 137
‚Äì of a random vector 147
Probability mass function 137
Probability measure 9
‚Äì continuous
‚Äì multivariate 81
‚Äì univariate 50
‚Äì discrete 20
Probability space 9
Product œÉ-field 91
Product measure 93
‚Äì of continuous probabilities 99
‚Äì of discrete probabilities 95
Pseudoinverse
‚Äì of a distribution function 199
‚Ñö, rational numbers 451
Quantile
‚Äì Fm,n-distribution 399
‚Äì œá2
n-distribution 398
‚Äì tn-distribution 399
‚Äì general setting 395
‚Äì standard normal distribution 396
‚Ñù, real numbers 451
Radius of convergence 473
Raisins in dough 211
Random experiment 1
Random real number 132
Random variable 132
‚Äì continuous 136

Index
‡±™
485
‚Äì discrete 136
‚Äì real-valued 132
‚Äì singularly continuous 140
‚Äì vector valued 146
Random variables
‚Äì identically distributed 135
‚Äì independent 157
Random vector 146
‚Äì ùí©(Œº, R)-distributed 315
‚Äì continuous 154
‚Äì discrete 149
‚Äì multinomial distributed 152
‚Äì normally distributed 312
‚Äì standard normally distributed 311
Random walk
‚Äì limit behavior 355
‚Äì (next neighbor) on ‚Ñ§183
‚Äì starting at an integer 293
‚Äì symmetric 184
Randomized test 377
Reduction
‚Äì of the sample space 113
Region
‚Äì of acceptance 377
‚Äì of rejection 377
Region of convergence
‚Äì of a power series 474
Risk
‚Äì of the buyer 378
‚Äì of the trader 378
Risk function
‚Äì of an estimator 427
‚Ñùn, n-dimensional Euclidean space 451
Roulette
‚Äì chance of winning 296
Round-off errors 357
Sample
‚Äì random 371
Sample mean 391
Sample space 1
Sample variance
‚Äì biased 391
‚Äì unbiased 391
Scalar product
‚Äì of two vectors 468
Secretary problem 283
Sequence
‚Äì absolutely summable 236
‚Äì summable 236
Set difference 452
œÉ-additivity 8
œÉ-field 4
‚Äì generated 5
Significance level 381
Significance test 381
‚Äì for a binomial population
‚Äì one-sided 389
‚Äì two-sided 386
‚Äì for a hypergeometric population
‚Äì one-sided 382
‚Äì two-sided 449
Simulation
‚Äì of a random variable
‚Äì continuous case 198
‚Äì discrete case 196
Standard normal distribution
‚Äì multivariate 102, 317
‚Äì univariate 57
Statistical model
‚Äì nonparametric 371
‚Äì parametric 374
Stirling‚Äôs formula
‚Äì for n-factorial 60
‚Äì for the Œì-function 59
Strong law of large numbers 342
Student‚Äôs t-distribution 228
Success probability 30
Sultan‚Äôs dowry problem 283
Symmetric difference 453
Systematic error
‚Äì of an estimator 428
t-distribution 228
t-test
‚Äì one-sided 405
‚Äì two-sided 405
Theorem
‚Äì Berry‚ÄìEss√©en 366
‚Äì binomial 459
‚Äì De Moivre‚ÄìLaplace 359
‚Äì Fisher‚Äôs 393
‚Äì Fubini‚Äôs 477
‚Äì multinomial 466
‚Äì Poisson‚Äôs limit 35
‚Äì Rao‚ÄìCram√©r‚ÄìFrechet 431
37 % rule 287
Three sigma rule 187

486
‡±™
Index
Tossing a coin
‚Äì infinitely often 193
Two-envelope paradox 287
Two-number problem 292
Two-sample t-tests 410
Two-sample Z-tests 408
Type I error 378
Type II error 378
Unbiased estimator 424
Uncorrelated random variables 274
Uniform distribution
‚Äì on a finite set 21
‚Äì on a set in ‚Ñùn 87
‚Äì on an interval 52
Uniformly best estimator 430
Union
‚Äì of sets 451
Upper limit
‚Äì of events 333
Vandermonde‚Äôs identity 460
Variance
‚Äì of a random variable 261
‚Äì properties 262
Volume
‚Äì n-dimensional 83
Weak law of large numbers 341
‚Ñ§, integers 451
Z-test
‚Äì one-sided 400
‚Äì two-sided 401


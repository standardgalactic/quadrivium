Risk Management in
Life-Critical Systems
Edited by Patrick Millot   
RISK MANAGEMENT AND DEPENDABILITY SERIES


Risk Management in Life-Critical Systems 
 
 

 
 
 

 
  
Risk Management in  
Life-Critical Systems 
 
 
 
 
 
 
 
 
Edited by 
 
Patrick Millot  
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
First published 2014 in Great Britain and the United States by ISTE Ltd and John Wiley & Sons, Inc. 
Apart from any fair dealing for the purposes of research or private study, or criticism or review, as 
permitted under the Copyright, Designs and Patents Act 1988, this publication may only be reproduced, 
stored or transmitted, in any form or by any means, with the prior permission in writing of the publishers, 
or in the case of reprographic reproduction in accordance with the terms and licenses issued by the  
CLA. Enquiries concerning reproduction outside these terms should be sent to the publishers at the 
undermentioned address: 
ISTE Ltd  
John Wiley & Sons, Inc.  
27-37 St George’s Road  
111 River Street 
London SW19 4EU 
Hoboken, NJ 07030 
UK  
USA  
www.iste.co.uk  
www.wiley.com 
© ISTE Ltd 2014 
The rights of Patrick Millot to be identified as the author of this work have been asserted by him in 
accordance with the Copyright, Designs and Patents Act 1988. 
Library of Congress Control Number:  2014947879 
 
British Library Cataloguing-in-Publication Data 
A CIP record for this book is available from the British Library  
ISBN 978-1-84821-480-4 

 
Contents 
LIST OF FIGURES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   xvii 
LIST OF TABLES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   xxiii 
FOREWORD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   xxv 
Patrick MILLOT 
INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xxvii 
PART 1. GENERAL APPROACHES FOR CRISIS MANAGEMENT  . . . .   
1 
CHAPTER 1. DEALING WITH THE UNEXPECTED . . . . . . . . . . . . . .   
3 
Guy A. BOY 
1.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   
3 
1.2. From mechanics to software to computer network . . . . . . . .   
5 
1.3. Handling complexity: looking for new models . . . . . . . . . . .   
7 
1.4. Risk taking: dealing with nonlinear dynamic systems . . . . . .   10 
1.5. Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   15 
1.6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   17 
1.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   18 

vi     Risk Management in Life-Critical Systems 
CHAPTER 2. VULNERABILITY AND RESILIENCE  
ASSESSMENT OF INFRASTRUCTURES AND NETWORKS:  
CONCEPTS AND METHODOLOGIES . . . . . . . . . . . . . . . . . . . . . .   21 
Eric CHÂTELET 
2.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   21 
2.2. Risk and vulnerability . . . . . . . . . . . . . . . . . . . . . . . . . .   22 
2.2.1. Concept of risk . . . . . . . . . . . . . . . . . . . . . . . . . . . .   22 
2.2.2. Concept of vulnerability . . . . . . . . . . . . . . . . . . . . . .   26 
2.3. Vulnerability analysis and assessment . . . . . . . . . . . . . . . .   27 
2.4. Resilience and main associated concepts . . . . . . . . . . . . . .   29 
2.4.1. Resilience: a multifaceted concept . . . . . . . . . . . . . . . .   29 
2.4.2. Main resilience components . . . . . . . . . . . . . . . . . . . .   30 
2.5. Paradigms as consequence of risk  
analysis extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   32 
2.5.1. Risk analysis extension and  
systemic approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   32 
2.5.2. Paradigms emerging from risk  
analysis extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   33 
2.6. Resilience analysis and assessment . . . . . . . . . . . . . . . . . .   35 
2.7. Conclusion: new challenges . . . . . . . . . . . . . . . . . . . . . . .   36 
2.8. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   36 
CHAPTER 3. THE GOLDEN HOUR CHALLENGE:  
APPLYING SYSTEMS ENGINEERING TO LIFE-CRITICAL  
SYSTEM OF SYSTEMS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   41 
Jean-René RUAULT 
3.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   41 
3.2. The Golden hour: toward a resilient  
life-critical system of systems . . . . . . . . . . . . . . . . . . . . . . . .   42 
3.2.1. Accident technical reports: getting  
experience feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   42 
3.2.2. Resilience: reducing the damage . . . . . . . . . . . . . . . . .   43 
 
 

Contents    vii 
3.2.3. The Golden hour: managing serious  
accidents as soon as possible . . . . . . . . . . . . . . . . . . . . . . . .   44 
3.2.4. The challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   47 
3.3. Systems of systems engineering . . . . . . . . . . . . . . . . . . . .   48 
3.3.1. The systems of systems engineering principles . . . . . . . .   48 
3.3.2. Applying systems of systems engineering to  
life-critical systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   50 
3.4. Next steps forward . . . . . . . . . . . . . . . . . . . . . . . . . . . .   54 
3.5. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   54 
CHAPTER 4. SITUATED RISK VISUALIZATION  IN  
CRISIS MANAGEMENT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   59 
Lucas STÉPHANE 
4.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   59 
4.2. Crisis management, emergency management  
and business continuity . . . . . . . . . . . . . . . . . . . . . . . . . . . .   60 
4.2.1. Crisis management . . . . . . . . . . . . . . . . . . . . . . . . . .   60 
4.2.2. Emergency management . . . . . . . . . . . . . . . . . . . . . .   61 
4.2.3. Business continuity and disaster recovery . . . . . . . . . . .   63 
4.3. Risk management in critical operations . . . . . . . . . . . . . . .   65 
4.3.1. Human systems integration risk perspective . . . . . . . . . .   65 
4.3.2. Effectiveness of risk definitions in  
critical operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   66 
4.4. Situated risk visualization in critical operations . . . . . . . . . .   68 
4.4.1. Rationale and requirements . . . . . . . . . . . . . . . . . . . .   68 
4.4.2. Integrated structure and ontology . . . . . . . . . . . . . . . . .   69 
4.4.3. Interactive 3D visual scene . . . . . . . . . . . . . . . . . . . . .   71 
4.4.4. Evaluation results . . . . . . . . . . . . . . . . . . . . . . . . . . .   72 
4.5. Conclusions and perspectives . . . . . . . . . . . . . . . . . . . . . .   72 
4.6. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   73 

viii     Risk Management in Life-Critical Systems 
CHAPTER 5. SAFETY CRITICAL ELEMENTS OF THE   
RAILWAY SYSTEM: MOST ADVANCED TECHNOLOGIES  
AND PROCESS TO DEMONSTRATE AND MAINTAIN  
HIGHEST  SAFETY PERFORMANCE . . . . . . . . . . . . . . . . . . . . . .   79 
Stéphane ROMEI 
5.1. Railways demonstrate the highest safety performance  
for public transportation . . . . . . . . . . . . . . . . . . . . . . . . . . . .   79 
5.2. Key success factors . . . . . . . . . . . . . . . . . . . . . . . . . . . .   79 
5.3. The European very high-speed rail technology:  
a safety concept with more than 30 years of experience  
and continuous innovation in the technology . . . . . . . . . . . . . . .   81 
5.3.1. Guidance and dynamic behavior . . . . . . . . . . . . . . . . .   81 
5.3.2. Environment with avoidance of external events . . . . . . . .   82 
5.3.3. Velocity with capacity to guarantee  
the emergency braking . . . . . . . . . . . . . . . . . . . . . . . . . . .   83 
5.3.4. Lifetime spanning several decades: operation  
and maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   84 
5.4. Project management and system integration . . . . . . . . . . . .   85 
5.4.1. Robust industry standards in project management . . . . . .   85 
5.4.2. System integration . . . . . . . . . . . . . . . . . . . . . . . . . .   85 
5.5. Procedure for risk management . . . . . . . . . . . . . . . . . . . .   86 
5.5.1. The regulatory framework . . . . . . . . . . . . . . . . . . . . .   86 
5.5.2. The EC common safety method . . . . . . . . . . . . . . . . . .   88 
5.5.3. High technical and safety standards . . . . . . . . . . . . . . .   88 
5.5.4. Independent safety assessment . . . . . . . . . . . . . . . . . .   90 
5.5.5. Significant change . . . . . . . . . . . . . . . . . . . . . . . . . .   90 
5.5.6. Safety management system . . . . . . . . . . . . . . . . . . . . .   91 
5.5.7. Safety authorization and safety management system . . . . .   92 
5.6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   93 
CHAPTER 6. FUNCTIONAL MODELING OF COMPLEX SYSTEMS . . . .   95 
Morten LIND 
6.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   95 
6.1.1. Dimensions of system complexity . . . . . . . . . . . . . . . .   95 
 

Contents    ix 
6.2. The modeling paradigm of MFM . . . . . . . . . . . . . . . . . . .   97 
6.2.1. The concept of function . . . . . . . . . . . . . . . . . . . . . . .   97 
6.2.2. The means-end relation . . . . . . . . . . . . . . . . . . . . . . .   100 
6.2.3. Means-end structure . . . . . . . . . . . . . . . . . . . . . . . . .   101 
6.3. Uses of functional modeling . . . . . . . . . . . . . . . . . . . . . .   102 
6.3.1. Operator support systems . . . . . . . . . . . . . . . . . . . . . .   102 
6.3.2. Control systems design . . . . . . . . . . . . . . . . . . . . . . .   103 
6.4. Multilevel flow modeling . . . . . . . . . . . . . . . . . . . . . . . .   103 
6.4.1. MFM concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . .   104 
6.4.2. A modeling example . . . . . . . . . . . . . . . . . . . . . . . . .   105 
6.4.3. Modeling safety functions . . . . . . . . . . . . . . . . . . . . .   109 
6.5. Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   110 
6.6. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   111 
PART 2. RISK MANAGEMENT AND HUMAN FACTORS . . . . . . . . . .   115 
CHAPTER 7. DESIGNING DRIVER ASSISTANCE   
SYSTEMS IN A RISK-BASED PROCESS . . . . . . . . . . . . . . . . . . . . .   117 
Pietro Carlo CACCIABUE 
7.1. Risk-based design in perspective . . . . . . . . . . . . . . . . . . .   117 
7.1.1. Risk-based design principles . . . . . . . . . . . . . . . . . . . .   117 
7.1.2. Short historical review of the RBD process . . . . . . . . . .   121 
7.2. Human factors in risk-based design . . . . . . . . . . . . . . . . . .   123 
7.2.1. Human reliability assessment . . . . . . . . . . . . . . . . . . .   124 
7.2.2. Models of human behavior . . . . . . . . . . . . . . . . . . . . .   126 
7.2.3. Models of error and taxonomies . . . . . . . . . . . . . . . . .   130 
7.2.4. Dynamic nature of needs . . . . . . . . . . . . . . . . . . . . . .   132 
7.3. A quasi-static methodology . . . . . . . . . . . . . . . . . . . . . . .   134 
7.3.1. The methodology . . . . . . . . . . . . . . . . . . . . . . . . . . .   134 
7.3.2. The expanded human performance  
event-tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   137 
7.3.3. Evaluation of consequences and risk  
assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   139 

x     Risk Management in Life-Critical Systems 
7.4. Implementation on board vehicles for  
driver assistance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   142 
7.5. A case study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   145 
7.5.1. Scenario definition . . . . . . . . . . . . . . . . . . . . . . . . . .   145 
7.5.2. Initiating event . . . . . . . . . . . . . . . . . . . . . . . . . . . .   146 
7.5.3. Development of the expanded event tree . . . . . . . . . . . .   147 
7.5.4. Probability assessment . . . . . . . . . . . . . . . . . . . . . . .   149 
7.5.5. Consequence evaluation . . . . . . . . . . . . . . . . . . . . . .   151 
7.5.6. Risk evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . .   152 
7.6. Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   152 
7.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   153 
CHAPTER 8. DISSONANCE ENGINEERING FOR RISK  
ANALYSIS: A THEORETICAL FRAMEWORK . . . . . . . . . . . . . . . . .   157 
Frédéric VANDERHAEGEN 
8.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   157 
8.2. The concept of dissonance . . . . . . . . . . . . . . . . . . . . . . .   157 
8.2.1. Dissonance engineering and risk analysis . . . . . . . . . . . .   157 
8.2.2. Dissonance reduction and knowledge  
reinforcement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   158 
8.3. A theoretical framework for risk analysis . . . . . . . . . . . . . .   162 
8.3.1. The DIMAGE model . . . . . . . . . . . . . . . . . . . . . . . .   162 
8.3.2. The human–machine learning process . . . . . . . . . . . . . .   165 
8.3.3. The behavior analysis for dissonance  
identification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   167 
8.3.4. The knowledge-based analysis for  
dissonance evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . .   168 
8.3.5. The knowledge-based analysis for  
dissonance reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   170 
8.4. Examples of application of the theoretical  
framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   172 
8.4.1. An application of the automated  
dissonance identification . . . . . . . . . . . . . . . . . . . . . . . . . .   172 
8.4.2. An application of the automated  
dissonance evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . .   174 

Contents    xi 
8.4.3. An application of the automated  
dissonance reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   176 
8.5. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   178 
8.6. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   179 
CHAPTER 9. THE FADING LINE BETWEEN  SELF AND SYSTEM . . . .   183 
René VAN PASSEN 
9.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   183 
9.2. Four events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   186 
9.2.1. Turkish Airlines 1951 . . . . . . . . . . . . . . . . . . . . . . . .   186 
9.2.2. Night charter with a Piper Seneca . . . . . . . . . . . . . . . .   187 
9.2.3. Air France Flight 447 . . . . . . . . . . . . . . . . . . . . . . . .   187 
9.2.4. US Airways Flight 1549 . . . . . . . . . . . . . . . . . . . . . .   189 
9.3. Development, drama . . . . . . . . . . . . . . . . . . . . . . . . . . .   189 
9.4. Views on human error . . . . . . . . . . . . . . . . . . . . . . . . . .   191 
9.5. Peirce’s triadic semiotic system . . . . . . . . . . . . . . . . . . . .   193 
9.6. Abduction, or how do humans form conclusions . . . . . . . . .   197 
9.7. Heidegger and Descartes . . . . . . . . . . . . . . . . . . . . . . . . .   200 
9.8. Designing the signs . . . . . . . . . . . . . . . . . . . . . . . . . . . .   203 
9.9. Consequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   204 
9.10. Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   207 
9.11. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   208 
CHAPTER 10. RISK MANAGEMENT: A MODEL FOR  
PROCEDURE USE ANALYSIS . . . . . . . . . . . . . . . . . . . . . . . . . . .   211 
Kara SCHMITT 
10.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   211 
10.2. Procedures in nuclear power . . . . . . . . . . . . . . . . . . . . .   213 
10.3. Description of the model . . . . . . . . . . . . . . . . . . . . . . . .   215 
10.3.1. Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   215 
10.3.2. Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   221 
10.3.3. Peer review of the model . . . . . . . . . . . . . . . . . . . . .   222 
10.4. Application of the model . . . . . . . . . . . . . . . . . . . . . . . .   223 
10.4.1. Generic applications . . . . . . . . . . . . . . . . . . . . . . . .   223 

xii     Risk Management in Life-Critical Systems 
10.4.2. Specific applications . . . . . . . . . . . . . . . . . . . . . . . .   223 
10.4.3. Real-world application of the model . . . . . . . . . . . . . .   224 
10.5. Significance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   227 
10.6. Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   229 
10.7. Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . .   230 
10.8. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   230 
CHAPTER 11. DRIVER-ASSISTANCE SYSTEMS FOR   
ROAD SAFETY IMPROVEMENT . . . . . . . . . . . . . . . . . . . . . . . . .   233 
Serge BOVERIE 
11.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   233 
11.2. Driver’s vigilance diagnostic . . . . . . . . . . . . . . . . . . . . .   236 
11.2.1. Diagnostic of driver hypovigilance . . . . . . . . . . . . . . .   238 
11.2.2. Diagnostic of driver impairment . . . . . . . . . . . . . . . . .   241 
11.3. Driver distraction diagnostic . . . . . . . . . . . . . . . . . . . . .   242 
11.4. Human–machine interaction concept . . . . . . . . . . . . . . . .   245 
11.5. Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   247 
11.6. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   249 
PART 3. MANAGING RISK VIA HUMAN–MACHINE  
COOPERATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   253 
CHAPTER 12. HUMAN–MACHINE COOPERATION   
PRINCIPLES TO SUPPORT LIFE-CRITICAL   
SYSTEMS MANAGEMENT . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   255 
Marie-Pierre PACAUX-LEMOINE 
12.1. Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   255 
12.2. Human–machine cooperation model . . . . . . . . . . . . . . . .   256 
12.2.1. The “know-how” or the abilities to control  
the process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   257 
12.2.2. Know-how-to-cooperate or the agent’s ability  
to cooperate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   258 
12.3. Common work space . . . . . . . . . . . . . . . . . . . . . . . . . .   260 
12.4. Multilevel cooperation . . . . . . . . . . . . . . . . . . . . . . . . .   263 

Contents    xiii 
12.5. Towards a generic modeling of human–machine  
cooperation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   266 
12.5.1. Cooperation to decide combination of tasks . . . . . . . . .   267 
12.5.2. Cooperation to decide authority . . . . . . . . . . . . . . . . .   268 
12.6. Conclusion and perspectives . . . . . . . . . . . . . . . . . . . . .   270 
12.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   272 
CHAPTER 13. COOPERATIVE ORGANIZATION FOR  
ENHANCING SITUATION AWARENESS . . . . . . . . . . . . . . . . . . . .   279 
Patrick MILLOT 
13.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   279 
13.2. Procedure-based behavior versus  
innovative behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   281 
13.3. Situation awareness: between usefulness  
and controversy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   283 
13.3.1. Situation awareness: several controversial  
definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   283 
13.3.2. Several SA definitions suffer from a lack  
of assessment methods . . . . . . . . . . . . . . . . . . . . . . . . . . .   284 
13.3.3. Collective situation awareness: an  
incomplete framework . . . . . . . . . . . . . . . . . . . . . . . . . . . .   285 
13.4. Collective SA: how to take the agent’s  
organization into account? . . . . . . . . . . . . . . . . . . . . . . . . . .   287 
13.4.1. Examples of task distribution and SA  
distribution among the agents . . . . . . . . . . . . . . . . . . . . . . .   287 
13.4.2. Collective SA: the distribution of roles  
among the agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   289 
13.4.3. SA distribution according to the generic  
forms of task distribution . . . . . . . . . . . . . . . . . . . . . . . . . .   291 
13.5. Enhancing collective SA with a support tool issued  
of cooperation concepts: the common work space . . . . . . . . . . .   292 
13.5.1. Cooperation model: a similitude with  
collective SA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   292 
13.5.2. Common work space for collective SA . . . . . . . . . . . .   295 
13.6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   296 
13.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   297 

xiv     Risk Management in Life-Critical Systems 
CHAPTER 14. A COOPERATIVE ASSISTANT FOR   
DEEP SPACE EXPLORATION . . . . . . . . . . . . . . . . . . . . . . . . . . .   301 
Donald PLATT 
14.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   301 
14.1.1. Previous human space exploration . . . . . . . . . . . . . . .   301 
14.1.2. Deep space situation awareness . . . . . . . . . . . . . . . . .   302 
14.2. The virtual camera . . . . . . . . . . . . . . . . . . . . . . . . . . . .   303 
14.2.1. Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   303 
14.2.2. Design method . . . . . . . . . . . . . . . . . . . . . . . . . . . .   309 
14.2.3. Implementation  . . . . . . . . . . . . . . . . . . . . . . . . . . .   311 
14.3. Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   314 
14.3.1. Preliminary testing . . . . . . . . . . . . . . . . . . . . . . . . .   314 
14.3.2. Further testing . . . . . . . . . . . . . . . . . . . . . . . . . . . .   315 
14.4. Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   316 
14.5. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   316 
14.6. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   317 
CHAPTER 15. MANAGING THE RISKS OF  AUTOMOBILE  
ACCIDENTS VIA  HUMAN–MACHINE COLLABORATION . . . . . . . . .   319 
Makoto ITOH 
15.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   319 
15.2. Trust as human understanding of machine . . . . . . . . . . . . .   320 
15.3. Machine understanding of humans . . . . . . . . . . . . . . . . .   323 
15.3.1. Drowsiness detection . . . . . . . . . . . . . . . . . . . . . . . .   323 
15.3.2. Inference of driver intent and  
detecting distraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   325 
15.4. Design of attention arousal and warning systems . . . . . . . .   326 
15.4.1. Attention arousal for distracted drivers . . . . . . . . . . . .   326 
15.4.2. Individual adaptation of a rear-end collision  
warning system for reducing the possibility  
of driver overreliance . . . . . . . . . . . . . . . . . . . . . . . . . . . .   327 
15.5. Trading of authority for control from the  
driver to the machine under time-critical situations . . . . . . . . . . .   329 
 

Contents    xv 
15.6. Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   330 
15.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   331 
CHAPTER 16. HUMAN–MACHINE INTERACTION IN  
AUTOMATED VEHICLES: THE ABV PROJECT . . . . . . . . . . . . . . .   335 
Chouki SENTOUH and Jean Christophe POPIEUL 
16.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   335 
16.2. The ABV project . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   337 
16.2.1. Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   337 
16.2.2. Structure of the ABV project . . . . . . . . . . . . . . . . . . .   338 
16.3. Specifications of the human–machine cooperation . . . . . . .   339 
16.3.1. Operating modes of the ABV system . . . . . . . . . . . . .   339 
16.3.2. ABV system HMI . . . . . . . . . . . . . . . . . . . . . . . . . .   340 
16.3.3. Driver monitoring . . . . . . . . . . . . . . . . . . . . . . . . . .   342 
16.4. Cooperation realization . . . . . . . . . . . . . . . . . . . . . . . . .   343 
16.4.1. Mechanisms for operating mode switching . . . . . . . . . .   343 
16.4.2. Shared control architecture . . . . . . . . . . . . . . . . . . . .   345 
16.5. Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   346 
16.6. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   348 
16.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   349 
CHAPTER 17. INTERACTIVE SURFACES, TANGIBLE  
INTERACTION: PERSPECTIVES FOR RISK MANAGEMENT . . . . . . .   351 
Christophe KOLSKI, Catherine GARBAY, Yoann LEBRUN,  
Fabien BADEIG, Sophie LEPREUX, René MANDIAU and  
Emmanuel ADAM 
17.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   351 
17.2. State of the art . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   352 
17.2.1. Supports for risk management . . . . . . . . . . . . . . . . . .   352 
17.2.2. Interactive surfaces, tangible interaction . . . . . . . . . . .   354 
17.3. Proposition: distributed UI on interactive  
tables and other surfaces for risk management . . . . . . . . . . . . . .   355 
17.4. Case studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   358 
17.4.1. Distributed road traffic management . . . . . . . . . . . . . .   358 

xvi     Risk Management in Life-Critical Systems 
17.4.2. Distributed risk game . . . . . . . . . . . . . . . . . . . . . . .   361 
17.5. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   366 
17.6. Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . .   366 
17.7. Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   367 
CONCLUSION  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   375 
LIST OF AUTHORS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   381 
INDEX . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   383 

List of Figures 
1.1. Expected and actual situation showing small and  
bigger variations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   11 
2.1. Factors shaping the risks faced to critical  
infrastructures [KRO 08] . . . . . . . . . . . . . . . . . . . . . . . . . . .   33 
2.2. A proposition of risk situations and relevant risk  
assessment strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   35 
3.1. eCall: the crashed car calls 112! [EC 13e] . . . . . . . . . . . . . .   46 
3.2. N² matrix of pairings of different systems within the   
system of systems [RUA 11] . . . . . . . . . . . . . . . . . . . . . . . . .   51 
3.3. Functional model of the accident detection  
system architecture [RUA 11] . . . . . . . . . . . . . . . . . . . . . . . .   53 
4.1. RTO and maximum tolerable period of  
disruption [COR 07] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   65 
4.2. Global view of the 3D interactive scene –  
Unity 3D [STE 13] (For a  color version of this figure, see 
www.iste.co.uk/millot/riskmanagement) . . . . . . . . . . . . . . . . .   71 
5.1. Range of order that has been observed for the  
last decade . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   80 
5.2. Main components of a railway system . . . . . . . . . . . . . . . .   80 
5.3. The bogie integrating six safety critical functions . . . . . . . . .   82 
5.4. Classical development V-cycle . . . . . . . . . . . . . . . . . . . . .   85 
5.5. Risk management organization in European Union . . . . . . . .   87 
5.6. Technics for identification and evaluation of   
hazards and their subsequent risks . . . . . . . . . . . . . . . . . . . . .   89 
5.7. European safety management system . . . . . . . . . . . . . . . . .   91 
5.8. Safety authorization and safety management system . . . . . . .   92 
 
 
 

xviii     Risk Management in Life-Critical Systems 
6.1. The means-end relation . . . . . . . . . . . . . . . . . . . . . . . . .   101 
6.2. Means-end structure showing the possible  
combinations of means-end relations . . . . . . . . . . . . . . . . . . . .   101 
6.3. MFM concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   104 
6.4. A heat transfer loop . . . . . . . . . . . . . . . . . . . . . . . . . . . .   105 
6.5. MFM of heat transfer loop without control . . . . . . . . . . . . .   106 
6.6. MFM of heat transfer loop with flow and  
temperature control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   108 
6.7. MFM model of heat transfer loop with a  
protection  system suppressing high temperature in HE2 . . . . . . .   110 
7.1. Risk-based design methodology flowchart . . . . . . . . . . . . .   118 
7.2. Sheridan’s five levels of “supervisory control”  
(adapted from [SHE 97]) . . . . . . . . . . . . . . . . . . . . . . . . . . .   128 
7.3. A generic operator model (adapted from  
[CAR 07]) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   129 
7.4. Essential nature of human–machine interaction . . . . . . . . . .   130 
7.5. Error propensity (EP) and dynamic generation  
of sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   134 
7.6. General structure of the quasi-static  
methodology for RBD . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   135 
7.7. Expanded human performance event  
tree (adapted from [CAC 12]) . . . . . . . . . . . . . . . . . . . . . . . .   138 
7.8. Generic risk matrix. For a color version of this  
figure, see www.iste.co.uk/millot/riskmanagement.zip . . . . . . . .   141 
7.9. ADAS at level of driving task a) and temporal  
sequence of intervention b) . . . . . . . . . . . . . . . . . . . . . . . . . .   144 
7.10. EHPET for the case study with ADAS . . . . . . . . . . . . . . .   150 
8.1. The DIMAGE model . . . . . . . . . . . . . . . . . . . . . . . . . . .   163 
8.2. Stable and unstable level of a dissonance  
dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   164 
8.3. The theoretical framework based on human– 
machine  learning to control dissonances . . . . . . . . . . . . . . . . .   165 
8.4. The reverse comic strip-based approach  
to identify dissonances . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   167 
8.5. Examples of emotion and sound variation images . . . . . . . . .   168 
8.6. The knowledge analysis algorithm . . . . . . . . . . . . . . . . . .   169 
8.7. The dissonance evaluation algorithm . . . . . . . . . . . . . . . . .   169 
8.8. The generic reinforcement based on  
learning process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   170 
8.9. A reinforcement algorithm by case- 
based reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   171 
 

List of Figures    xix 
8.10. The interpretation of pictures from rail  
platform signaling systems . . . . . . . . . . . . . . . . . . . . . . . . . .   172 
8.11. The associated reverse comic strip for  
dissonance identification . . . . . . . . . . . . . . . . . . . . . . . . . . .   173 
8.12. The associated rule analysis for dissonance  
identification and evaluation . . . . . . . . . . . . . . . . . . . . . . . . .   176 
8.13. A prediction process based on the knowledge  
reinforcement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   177 
8.14. The correct prediction rate by reinforcing  
the knowledge base . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   178 
9.1. Depiction of Peirce’s triadic relationship between 
object, sign and interpretation . . . . . . . . . . . . . . . . . . . . . . . .   193 
9.2. Diagram illustrating the problems of determining causes and control 
actions in an uncertain system. An unknown disturbance might be acting 
on the system, a shift in its parameter may have happened, leading to a 
qualitative change in dynamics, or a structural change might have 
occurred, leading to a signiﬁcantly different system. The innovation or 
surprise i is the difference between observation and expectation, and may 
lead to adjustment. Whether control is based on observation or on 
expectation is uncertain,  
and probably variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   197 
10.1. A model for procedure analysis . . . . . . . . . . . . . . . . . . .   219 
11.1. Examples of driver-assistance systems . . . . . . . . . . . . . . .   234 
11.2. Vehicle/driver/environment system . . . . . . . . . . . . . . . . .   236 
11.3. T involuntary transition from waking to  
sleeping (from Alain Muzet) . . . . . . . . . . . . . . . . . . . . . . . . .   237 
11.4. Algorithmic principle for the hypovigilance  
diagnostic of the  driver and results of this analysis  
on a subject in real driving conditions . . . . . . . . . . . . . . . . . . .   240 
11.5. Classification principles for visual  
distraction detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   245 
11.6. DrivEasy concept . . . . . . . . . . . . . . . . . . . . . . . . . . . .   246 
12.1. Attributes of cooperative agent . . . . . . . . . . . . . . . . . . . .   257 
12.2. Cooperative activity through agents’ know-how  
(Agi KH), agents’ know-how-to-cooperate (Agi KHC),  
agents’ situation awareness (Agi SA), common  
frame of reference (COFOR), team situation awareness  
(Team SA) and common work space . . . . . . . . . . . . . . . . . . . .   261 
12.3. Fighter aircraft CWS (example of the tactical  
situation SITAC).  For a color version of this figure,  
see www.iste.co.uk/millot/riskmanagement . . . . . . . . . . . . . . .   262 
12.4. Multilevel cooperation. . . . . . . . . . . . . . . . . . . . . . . . . .   264 

xx     Risk Management in Life-Critical Systems 
12.5. Cooperative tasks  1-KH; 2-CWS; 3-KHC  
(current task);  4-KHC (intention); 5-KHC  
(authority); 6-KHC (model) . . . . . . . . . . . . . . . . . . . . . . . . .   266 
12.6. Robotics CWS. For a color version of this figure,  
see www.iste.co.uk/millot/riskmanagement . . . . . . . . . . . . . . .   270 
12.7. Example of agents’ abilities identification  
for task sharing  and authority management (red arrows).  
For a color version of this figure, see www.iste.co.uk/ 
millot/riskmanagement.zip . . . . . . . . . . . . . . . . . . . . . . . . . .   271 
13.1. Allocation of functions among humans and   
machines (adapted from [BOY 11]) . . . . . . . . . . . . . . . . . . . .   280 
13.2. SA three-level model adapted from [END 95a] . . . . . . . . .   283 
13.3. Team-SA adapted from [SAL 08] . . . . . . . . . . . . . . . . . .   286 
13.4. The three forms for task distribution according  
to  agents KH and related tasks to share . . . . . . . . . . . . . . . . . .   290 
13.5. Task distribution and related SA distribution,  
in the  augmentative and integrative forms . . . . . . . . . . . . . . . .   291 
13.6. Task distribution and related SA distribution,  
in the debative form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   292 
13.7. CWS principle for team SA [MIL 13] . . . . . . . . . . . . . . .   295 
14.1. The model of cooperation between astronauts  
and ground-based  experts and how it is changing  
for deep space exploration . . . . . . . . . . . . . . . . . . . . . . . . . .   303 
14.2. Virtual camera data feedback loop . . . . . . . . . . . . . . . . . .   307 
14.3. The human-centered design process for the   
development of the virtual camera . . . . . . . . . . . . . . . . . . . . .   309 
14.4. Riding in the NASA Lunar Electric Rover  
vehicle at  DesertRATS, collecting user requirements  
for the development of the VC . . . . . . . . . . . . . . . . . . . . . . . .   310 
14.5. Horizontal prototype for the VC showing  
icons and interface. For a color version of this figure,  
see www.iste.co.uk/millot/riskmanagement . . . . . . . . . . . . . . .   312 
14.6. The VC vertical prototype with icons labeled.  
For a  color version of this figure, see www.iste. 
co.uk/millot/riskmanagement . . . . . . . . . . . . . . . . . . . . . . . . .   313 
15.1. The structure of trust . . . . . . . . . . . . . . . . . . . . . . . . . .   322 
15.2. Deceleration meter . . . . . . . . . . . . . . . . . . . . . . . . . . . .   323 
15.3. a) Pressure distribution sensors and  
b) the obtained data. For a color version of this figure, see 
www.iste.co.uk/millot/riskmanagement . . . . . . . . . . . . . . . . . .   324 
15.4. Pressure distribution sensors and the  
obtained data [ISH 13].  For a color version of this figure,  
see www.iste.co.uk/millot/riskmanagement . . . . . . . . . . . . . . .   325 

List of Figures    xxi 
15.5. Model of driver lane change intent  
emergence [ZHO 09] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   326 
15.6. a) The attention arousing display and b)  
its effects on THW [ITO 13a].  For a color  
version of this figure, see www.iste.co.uk/ 
millot/riskmanagement . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   327 
15.7. Driver reaction against the rapid deceleration  
of the forward vehicle [ITO 08b] . . . . . . . . . . . . . . . . . . . . . .   328 
15.8. A situation machine protective action is needed.  
In this example, the left lane is the cruising lane and  
the right lane is the passing lane. The vehicle in  
the right lane is in the blind spot of the side-view  
mirror of the host vehicle . . . . . . . . . . . . . . . . . . . . . . . . . . .   330 
16.1. Structure of the ABV project . . . . . . . . . . . . . . . . . . . . .   338 
16.2. Graph of the different modes of the ABV system . . . . . . . .   341 
16.3. Graph of the different modes of the ABV system.  
For a  color version of this figure, see www.iste.co.uk/ 
millot/riskmanagement . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   342 
16.4. Driver monitoring system from Continental. For  
a color  version of this figure, see www.iste.co.uk/ 
millot/riskmanagement . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   343 
16.5. Shared driving control architecture . . . . . . . . . . . . . . . . .   345 
16.6. Experimental results on the SHERPA simulator.  
For a  color version of this figure, see www.iste.co. 
uk/millot/riskmanagement . . . . . . . . . . . . . . . . . . . . . . . . . .   347 
16.7. Evaluation of the sharing quality. For a  
color version of  this figure, see www.iste.co. 
uk/millot/riskmanagement . . . . . . . . . . . . . . . . . . . . . . . . . .   348 
17.1. Two configurations for risk management UI:  
a) centralized distribution  of U; b) network  
of distributed UI [LEP 11]. For a color version  of this figure,  
see www.iste.co.uk/millot/riskmanagement.zip . . . . . . . . . . . . .   356 
17.2. Crisis unit using TangiSense and other  
platforms (adapted from [LEP 11]).  For a color  
version of this figure, see www.iste.co.uk/ 
millot/riskmanagement.zip . . . . . . . . . . . . . . . . . . . . . . . . . .   357 
17.3. A road traffic simulation on two TangiSense  
interactive tables.  For a color version of this figure,  
see www.iste.co.uk/millot/riskmanagement.zip . . . . . . . . . . . . .   359 
17.4. Use of zoom tangible object, without effect  
on the other table.  For a color version of this figure,  
see www.iste.co.uk/millot/riskmanagement.zip . . . . . . . . . . . . .   360 

xxii     Risk Management in Life-Critical Systems 
17.5. Tangiget synchronization with effect on  
TangiSense 2. For a color  version of this figure,  
see www.iste.co.uk/millot/riskmanagement.zip . . . . . . . . . . . . .   361 
17.6. The TangiSense table as equipped for  
the risk game with ground  map display, tangible  
objects and virtual feedback shown. For a color  
version  of this figure, see www.iste.co.uk/ 
millot/riskmanagement.zip . . . . . . . . . . . . . . . . . . . . . . . . . .   362 
17.7. Functional view showing the various  
types of agents, filters and traces.  For a  
color version of this figure, see www.iste.co.uk/ 
millot/riskmanagement.zip . . . . . . . . . . . . . . . . . . . . . . . . . .   363 
 
 

List of Tables 
2.1. Classification of initiating events . . . . . . . . . . . . . . . . . . .   23 
2.2. Site/building inherent vulnerability assessment  
matrix  (partial risk assessment) [FEM 03] . . . . . . . . . . . . . . . .   28 
3.1. Major problems and respective drivers that  
eCall can improve [EC 11a] . . . . . . . . . . . . . . . . . . . . . . . . .   45 
4.1. Approaches to crisis management . . . . . . . . . . . . . . . . . . .   62 
4.2. Crisis features, types and questions to  
be answered [STE 13] . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   68 
7.1. Possible data for the traffic light scenario . . . . . . . . . . . . . .   146 
10.1. Have you ever witnessed a scenario where? . . . . . . . . . . . .   222 
10.2. Solutions table for Case 15 . . . . . . . . . . . . . . . . . . . . . .   224 
10.3. Decision point metrics . . . . . . . . . . . . . . . . . . . . . . . . .   225 
14.1. A use case for surface exploration . . . . . . . . . . . . . . . . . .   311 
15.1. Scale of degrees of automation [SHE 92, INA 98] . . . . . . . .   320 


 
Foreword 
The theme “Risk Management in Life Critical Systems” resulted from a 
cooperative work between LAMIH (French acronym for Laboratory of 
Industrial and Human Automation, Mechanics and Computer Science) at the 
University of Valenciennes (France) and Human-Centered Design Institute 
(HCDi) at Florida Institute of Technology (USA) within the framework of 
the Partner University Funds (PUF) Joint Research Lab on Risk 
Management in Life-Critical Systems co-chaired by me and Dr Guy A. Boy.  
A summer school on the above theme was held at Valenciennes on 1–5 
July 2013, which had gathered more than 20 specialists from the domain 
from seven countries (i.e. France, USA, Italy, Germany, Netherland, Japan 
and Denmark) among the most developed ones where “safety” assumes an 
increasing importance. This book is the result of the contribution of most of 
these researchers.  
This book relates to the management of risk. Another book, focusing on 
risk taking, will be edited by my colleague Dr Guy A. Boy and published by 
Springer, UK. 
 
Patrick MILLOT 
September 2014 

 

 
Introduction  
Life Critical Systems are characterized by three main attributes: safety, 
efficiency and comfort. They typically lead to complex and time critical 
issues. They belong to domains such as transportation (trains, cars, aircraft, 
air traffic control), space exploration energy (nuclear and chemical 
engineering), health and medical care, telecommunication networks, 
cooperative robot fleets, manufacturing, and services leading to complex and 
time critical issues. 
Risk management deals with prevention, decision-making, action taking, 
crisis management and recovery, taking into account consequences of 
unexpected events. We are interested in ecological processes, human 
behavior, as well as control and management of life-critical systems, 
potentially highly-automated. Our approach focuses on “human(s) in the 
loop” systems and simulations, taking advantage of the human ability to 
cope with unexpected dangerous events on the one hand, and attempting to 
recover from human errors and system failures on the other hand. Our 
competences are developed both in Human–Computer Interaction and 
Human–Machine System. Interactivity and human-centered automation are 
our main focuses.  
The approach consists of three complementary steps: prevention, where 
any unexpected event could be blocked or managed before its propagation; 
recovery, when the event results in an accident, making protective measures 
mandatory to avoid damages; and possibly after the accident occurs, 
management of consequences is required to minimize or remove the most 
                         
Introduction written by Patrick MILLOT. 

xxviii     Risk Management in Life-Critical Systems 
severe ones. Global crisis management methods and organizations are 
considered.  
Prevention can be achieved by enhancing both system and human 
capabilities to guarantee optimal task execution: 
– by defining procedures, system monitoring devices, control and 
management methods; 
– by taking care of the socio-organizational context of human activities, 
in particular the adequacy between system demands and human resources. In 
case of lack of adequacy, assistance tools must be introduced using the 
present development of Information Technologies and Engineering Sciences. 
The specialties of our community and the originality of our approaches 
are to combine these technologies with cognitive science knowledge and 
skills in “human in the loop” systems. Our main related research topics are: 
impact of new technology on human situation awareness (SA); cooperative 
work, including human–machine cooperation and computer supporting 
cooperative work (CSCW); responsibility and accountability (task and 
function allocation, authority sharing). 
Recovery can be enhanced: 
– by providing technical protective measures, such as barriers, which 
prevent from erroneous actions; 
– by developing reliability assessment methods for detecting human 
errors as well as system failures; 
– and by improving human detection and recovery of their own errors, 
enhancing system resilience; human–machine or human–human cooperation 
is a way to enhance resilience. 
Crisis management consists: 
– of developing dedicated methods; 
– of coping with socio-organizational issues using a multiagent approach 
through for instance an adaptive control organization.  
The different themes developed in this book are related to complementary 
topics developed in pluridisciplinary approaches, some are more related to 

Introduction     xxix 
prevention, others to recovery and the last ones to global crisis management. 
But all are related to concrete application fields among life-critical systems. 
Seventeen chapters contribute to answer these important issues. We chose 
to gather them in this book into three complementary parts: (1) general 
approaches for crisis management, (2) risk management and human factors 
and (3) managing risks via human–machine cooperation.  
Part 1 is composed of first six chapters dedicated to general approaches 
for crisis management: 
– Chapter 1, written by Guy A. Boy criticizes the theories, methods and 
tools developed several years ago, based on linear approaches to engineering 
systems that consider unexpected and rare events as exceptions, instead of 
including them in the flow of everyday events, handled by well-trained and 
experienced experts in specific domains. Consequently, regulations, clumsy 
automation and operational procedures are still accumulated in the short 
term instead of integrating long-term experience feedback. This results in the 
concept of quality assurance and human–machine interfaces (HMI) instead 
of focusing on human–system integration. The author promotes human-
centered processes such as creativity, adaptability and problem solving and 
the need to be better acquainted with risk taking, preparation, maturity 
management, complacency emerging from routine operations and educated 
common sense. 
– Chapter 2, written by Eric Chatelet starts with well-known concepts in 
risk analysis but introduces the merging use of the resilience concept. The 
vulnerability concept is one of the starting points to extend the risk analysis 
approaches. The author gives an overview of approaches dedicated to the 
resilience assessment of critical or catastrophic events concerning 
infrastructures and/or networks. 
– Chapter 3, written by Jean René Ruault deals with a case study on an 
emergency system management, from an architectural and a system of 
systems engineering perspective. It gives an overview of all dimensions  
to take into account when providing a geographical area with the capacity to 
manage crisis situations – in the present case road accidents – in order to 
reduce accidental mortality and morbidity and to challenge the golden hour. 
This case study shows how these operational, technical, economic and social 
dimensions are interlinked, both in the practical use of products and in 

xxx     Risk Management in Life-Critical Systems 
service provision. Based on a reference operational scenario, the author 
shows how to define the perimeter and functions of a system of systems. 
– Chapter 4, written by Lucas Stephane provides an overview of state-of-
the-art approaches to critical operations and proposes a solution based on the 
integration of several visual concepts within a single interactive 3D scene 
intended to support situated visualization of risk in crisis situations.The 
author first presents approaches to critical operations and synthesizes risk 
approaches. He then proposes the 3D integrated scene and develops user-test 
results and feedback. 
– Chapter 5, written by Stephane Romei shows the high level of 
performance attained by the European railway system. It results from several 
success factors among which three are of higher importance: (1) expertise 
and innovation in design, operation and maintenance in safety critical 
technologies, (2) competences in project management and system integration 
and (3) procedures for risk management. Illustrations are taken from Very 
High Speed Train technology. 
– Finally, Chapter 6, written by Morten Lind deals with system 
complexity, another dimension that influences decisions made by system 
designers and that may affect the vulnerability of systems to disturbances, 
their efficiency, the safety of their operations and their maintainability. The 
author describes a modeling methodology capable of representing industrial 
processes and technical infrastructures from multiple perspectives. The 
methodology called Multilevel Flow Modeling (MFM) has a particular focus 
on semantic complexity but addresses also syntactic complexity. MFM uses 
mean-end and part-whole concepts to distinguish between different levels of 
abstraction representing selected aspects of a system. MFM is applied for 
process and automation design and for reasoning about fault management 
and supervision and control of complex plants.  
Part 2 is comprised of the five following chapters and is related to human 
factors, the second dimension beside the technical and methodological 
aspects of risk management: 
– Chapter 7, written by Pietro Carlo Cacciabue, presents a well-
formalized and consolidated methodology called Risk-Based Design (RBD) 
that integrates systematically risk analysis in the design process with the aim 
of prevention, reduction and/or containment of hazards and consequences 
embedded in the systems as the design process evolves. Formally, it 
 

Introduction     xxxi 
identifies the hazards of the system and continuously optimizes design 
decisions to mitigate them or limit the likelihood of the associated 
consequences, i.e. the associated risk. The author first discusses the specific 
theoretical problem of handling dynamic human–machine interactions in a 
safety- and risk-based design perspective. A development for the automotive 
domain is then considered and a case study complements the theoretical 
discussion.  
– Chapter 8, written by Frederic Vanderhaegen presents a new original 
approach to analyze risks based on the dissonance concept. A dissonance 
occurs when conflict between individual or collective knowledge occurs. A 
theoretical framework is then proposed to control dissonances based on the 
Dissonance Management (DIMAGE) model and the human–machine 
learning concept. The dissonance identification, evaluation and reduction 
function of DIMAGE is supported by automated tools that analyze the 
human behavior and knowledge. Three examples illustrate the approach. 
– Chapter 9, written by René Van Paassen, deals with the influence of 
human errors in the reliability of systems, illustrated by examples in 
aviation. While technical developments increased the reliability of aircraft, it 
cannot be expected that the human component in a complex technical system 
underwent similar advances in reliability. This chapter contains a designer’s 
view on the creation of combined human–machine systems that provide safe, 
reliable and flexible operation. A common approach in design is the 
breakdown of a complete system into subsystems, and to focus on the design 
of the individual components. This can, up to a point, be used in the design 
of safe systems. However, the adaptive nature of the “human” component, 
which is precisely the reason for having humans in complex systems, is such 
that it is not practical to isolate the human as a single component, and 
assume that the synthesis of the human with the other components yields the 
complete system. Rather, humans “merge” with the complete system to a far 
greater extent than often imagined, and a designer needs to be aware of that. 
The author explores – through the reﬂection on a number of incidents and 
accidents – the nature of mishaps in human–machine systems, and the 
factors that might have influenced these events. It begins with a brief 
introduction of the events, and an overview of the different ways of 
analyzing them. 
– Chapter 10, written by Kara Schmitt, challenges the assumptions of the 
US nuclear industry, that “strict adherence to procedure increases safety”, to 
see if they are still valid and hold true. The author reviews what has changed 

xxxii     Risk Management in Life-Critical Systems 
within the industry, and verifies that the industry does have strict adherence 
to procedures and a culture of rigid compliance. She offers an application 
regarding performing an experimental protocol and utilizing expert judgment 
to prove that the strict procedure adherence is not sufficient for overall 
system safety. 
– Chapter 11, written by Serge Boverie, shows that in Organization for 
Economic Cooperation and Development (OECD) countries about 90% of 
the accidents are due to an intentional or non-intentional driver behavior: bad 
perception or bad knowledge of the driving environment (obstacles, etc.), 
due to physiological conditions (drowsiness and sleepiness) or bad physical 
conditions (old people and elderly drivers) etc. The author shows how 
development of increasingly intelligent advanced driver assistance systems 
(ADASs) should partly solve these problems. New functions will improve 
the environmental perception of the driver (night vision, blind spot detection 
and obstacle detection). In critical situation, they can substitute the driver 
(e.g. autonomous emergency braking, etc.). New ADAS generation will be 
able to provide the driver with the possibility to adapt the level of assistance 
in relation to his comprehension, needs, aptitudes, capacities and 
availabilities. For instance, a real-time diagnosis of the driver’s state 
(sleepiness, drowsiness, head orientation or of extra driving activity) is now 
under development. 
Finally, Part 3 groups together the last six chapters dedicated to managing 
risk via a human–machine cooperation: 
– Chapter 12, written by Marie Pierre Pacaux-Lemoine, presents a model 
of human–machine cooperation issued from different disciplines, human 
engineering, automation science, computer sciences, cognitive and social 
psychology. The model aims to enable humans and machines to work as 
partners while supporting interactions between them, i.e. making easier the 
perception and understanding of other agents’ viewpoint and behavior. Such 
a support is called a common work space (CWS) that we will see again in 
the following chapters. These principles aim to evaluate the risk for a 
human–machine system to reach an unstable and unrecoverable state. 
Several application domains, including car driving, air traffic control, fighter 
aircraft and robotics, illustrate this framework. 
– Chapter 13, written by Patrick Millot, shows how organizations 
improving SA enhance human–machine safety. Indeed, people involved in 
the control and management of life-critical systems provide two kinds of 

Introduction     xxxiii 
roles: negative, with their ability to make errors, and positive, with their 
unique involvement and capacity to deal with the unexpected. The human–
machine system designer remains, therefore, faced with a drastic dilemma: 
how to combine both roles, a procedure-based automated behavior versus an 
innovative behavior that allows humans to be “aware” and to cope with 
unknown situations. SA that characterizes the human presence in the system 
becomes a crucial concept for that purpose. The author reviews some of the 
SA weaknesses and proposes several improvements, especially the effect of 
the organization and of the task distribution among the agents to construct an 
SA distribution and a support to collective work. This issue derives from the 
human–machine cooperation framework, and the support to collective SA is 
once again the CWS. 
– Chapter 14, written by Donald Platt, looks at a human-centered design 
approach to develop a tool to allow improved SA and cooperation in a 
remote and possibly hostile environment. The application field relates to 
deep space exploration. The associated risks include physical, mental, 
emotional and even organizational risks. Cooperation between astronauts on 
the planet surface and the mission operator and the chief scientist on Earth 
takes the form of a virtual camera (VC). The VC displays the dialog between 
the human agents, but is also a database with various useful information on 
the planet geography, geology, etc., that can be preliminary recorded in its 
memory or downloaded online. It plays the role of a CWS. The author 
relates how its ability to improve astronaut SA as well as collective SA has 
been tested experimentally. 
– Chapter 15, written by Makoto Itoh, returns to ADASs. The human 
driver has to place appropriate trust in the ADAS based on an appropriate 
understanding of this tool. For this purpose, it is necessary for system 
designers to understand what trust is and what inappropriate trust is (i.e. 
overtrust and distrust), and how to design ADAS that is appropriately trusted 
by human drivers. ADAS also has to understand the physiological and/or 
cognitive state of the human driver in order to determine whether it is really 
necessary to provide assistive functions, especially safety control actions or 
not. The author presents a theoretical model of trust in ADAS, which is 
useful to understand what overtrust and/or distrust is and what should be 
needed to avoid inappropriate trust. Also, this chapter presents several 
driver-monitoring techniques, especially to detect a driver’s drowsiness or 
fatigue and to detect a driver’s lane-changing intent. Finally, he shows 

xxxiv     Risk Management in Life-Critical Systems 
several examples of design of attention arousal systems, warning systems 
and systems that perform safety control actions in an autonomous manner. 
– Chapter 16, written by Chouki Sentouh and Jean Christophe Popieul, 
presents the ABV project (French acronym for low-speed automation). It 
focuses on the interaction between human and machine with a continuous 
sharing of driving, considering the acceptability of the assistance and 
driver’s distractions and drowsiness. The main motivation of this project is 
the fact that in many situations, the driver is required to drive his/her vehicle 
at a speed lower than 50 km/h (speed limit in urban areas) or in the case of a 
traffic congestion due to traffic jams, in the surrounding areas of big cities, 
for example. The authors describe the specification of cooperation principles 
between the driver and assistance system for lane keeping developed in the 
framework of the ABV project. 
– Finally, Chapter 17 is written by Christophe Kolski, Catherine Garbay, 
Yoann Lebrun, Fabien Badeig, Sophie Lepreux, René Mandiau and 
Emmanuel Adam. It describes interactive tables (also called tabletops) that 
can be considered as new interaction platforms, as collaborative and 
colocalized workspaces, allowing several users to interact (work, play, etc.) 
simultaneously. The authors’ goal is to share an application between several 
users, platforms (tabletops, mobile and tablet devices and other interactive 
supports) and types of interaction, allowing distributed human–computer 
interactions. Such an approach may lead to new perspectives for risk 
management; indeed, it may become possible to propose new types of 
remote and collaborative ways in this domain. 
 
 
 
 
 
 
 
 
 
 
 
 
 

PART 1 
General Approaches for  
Crisis Management


1 
Dealing with the Unexpected 
1.1. Introduction  
Sectors dealing with life-critical systems (LCSs), such as aerospace, 
nuclear energy and medicine, have developed safety cultures that attempt to 
frame operations within acceptable domains of risk. They have improved 
their systems’ engineering approaches and developed more appropriate 
regulations, operational procedures and training programs. System reliability 
has been extensively studied and related methods have been developed to 
improve safety [NIL 03]. Human reliability is a more difficult endeavor; 
human factors specialists developed approaches based on human error 
analysis and management [HOL 98]. Despite this heavy framework, we still 
have to face unexpected situations that people have to manage in order to 
minimize consequences. 
During the 20th Century, we developed methods and tools based on a 
linear1 approach of human–machine systems (HMS). We developed user 
interfaces and operational procedures based on experience feedback  
[IAE 06]. We have accumulated a giant amount of operational knowledge. In 
other words, we tried to close a world that is still open. As a result, anytime 
human operators deviate from the (linear) norm, we talk about noise, or even  
                         
Chapter written by Guy A. BOY. 
1 Linearity can be understood in three ways: proportionality, single-causality or chronological 
order such as reading a paper-based book or document. Nonlinearity does not satisfy these 
conditions, i.e. can be understood as non-proportionality, multiple-causality or out of 
chronological order such as browsing the web. The use of procedures and standards leads to 
often-rigid linear processes and behaviors. Managing unexpected situation and problem 
solving requires flexible nonlinear processes and behaviors. 

4     Risk Management in Life-Critical Systems 
about the unexpected. In fact, this model of the world tends to consider the 
unexpected as an exception, and could be explained by the fact that 
engineering was developed having the normal distribution in mind supported 
by the Gaussian function, and any event that deviates beyond a (small) given 
standard deviation is ignored. This simplification does not take into account 
that context may change, and simplification assumptions made may turn out 
to be wrong when context changes. This is what nonlinear dynamic systems 
are about. Therefore, when a bigger deviation occurs, it is considered as rare 
and unexpected. It would be fair to say that once a simplification is made we 
should be aware of the limitations that they introduce. 
Quantitative risk assessments are typically based on the (event probability 
multiplied by consequence magnitude) numerical product. This formula does 
not work when we deal with small probabilities and huge consequences; it is 
mathematically undetermined. The misconception that the unexpected is 
exceptional comes from this probabilistic approach of operations and, more 
generally, standardized life. In contrast, LCS human operators deal with the 
unexpected all the time in their various activities, and with possibilities and 
necessities instead of probabilities [DUB 01]. 
The 21st Century started with the Fukushima nuclear tragedy [RAM 11], 
which highlighted the fact that our linear/local approaches to engineering 
must be revised, and even drastically changed, emphasizing our world in a 
nonlinear and holistic manner. This is not possible without addressing 
complexity in depth. Nature is complex. People are part of nature, therefore 
HMSs are necessarily complex; even if machines could be very simple, 
people create complexity once they start interacting with these machines. 
Therefore, when talking about safety, reliability and performance, people are 
the most central element. Instead of developing user interfaces once systems 
are fully developed as it is still commonly done today (the linear local 
approach to HMS), it is urgent to integrate people and technology from the 
very beginning of design processes. This is why human–system integration 
(HSI) is now a better terminology than HMSs or human–computer 
interaction. The term “system” in HSI denotes both hardware (machine in 
mechanical engineering terms) and software (the most salient part of 
contemporary computers). 
This necessary shift from linear/local to nonlinear/holistic has 
tremendous repercussions on the way technology is designed. The 
engineering community rationalized design and manufacturing, and 

Dealing with the Unexpected     5 
produced very rigid standards to the point that it is now very difficult to 
design a new LCS without being constantly constrained and forbidden from 
any purposeful innovation. To a certain extent, standardization is a 
successful result of the linear/local approach to engineering. Even human 
factors have been standardized [EAS 04]. However, we tend to forget that 
people still have fundamental assets that machines or standardization 
systems do not and will never have, they are creative, adaptable and can 
solve problems that are not known in advance. These assets should be better 
used both in design and operations. Standard operational procedures are 
good sociocognitive support in complex operations, but competence, 
knowledge and adaptability are always the basis for solving hard problems. 
For that matter, both nonlinear/holistic and linear/local approaches should be 
used, and in that order. They should be combined putting nonlinear/holistic 
at the top (the design part) and linear/local at the bottom (the implementation 
part). In other words, human-centered design should oversee technology-
centered engineering [BOY 13a]. 
It is time to (re-)learn how to deal with the unexpected using a nonlinear 
approach, where experience and expertise are key assets. People involved in 
the design and operations of LCS require knowledge and competence in 
complex systems design and management, the domain at stake (e.g., 
aerospace, nuclear), teamwork and risk taking. Dealing with the unexpected 
requires accurate and effective situation awareness, synthetic mind, decision-
making capability, self-control, multitasking, stress management and 
cooperation (team spirit). This paper presents a synthesis using examples in 
the aviation domain compiled from a conference organized in 2011 by the 
Air and Space Academy on the subject [ASA 13]. 
1.2. From mechanics to software to computer network 
Our sociotechnical world drastically changes. When I was at school, I 
learned to simplify hard problems in order to solve them using methods and 
techniques derived from linear algebra, for example. This is a very 
simplified view of what my generation learned but it represents a good 
image of the 20th Century’s engineering background. We developed very 
sophisticated applied mathematics and physics approaches to build systems 
such as cars, aircraft, spacecraft and nuclear power plants. However, 
everything that engineers learned and used was very much linear by nature. 
Any variability was typically considered as noise, which needed to be 

6     Risk Management in Life-Critical Systems 
filtered. We managed to build very efficient machines that not only extended 
our capabilities, but also enabled us to do things that were not naturally 
possible before. 
The 20th Century was the era of mechanics more than anything else, 
conceptually and technologically speaking. Then a new era came supported 
by the development of modern computers where software took a major role. 
Software introduced a totally different way of thinking because machines 
were able to perform more elaborate tasks by themselves. We moved from 
manipulation of mechanical devices to interaction with software agents. We 
moved from control to management. The first glass cockpits and fly-by-wire 
technology drastically changed the way pilots were flying. Information 
technology and systems invaded cockpits and intensively support pilots’ 
activities. New kinds of problems emerge when systems fail and manual 
reversion is necessary. In other words, nowadays pilots not only need to 
master the art of flying, but also need to know how to manage systems. Even 
if these systems have become more reliable and robust, they do not remove 
the art of flying. We always need to remember that flying is not a person’s 
natural capability (i.e. people do not fly like birds do), it is a cognitive ability 
that needs to be learned and embodied by extensive and long training using 
specific prostheses such as aircraft. 
This brings to the fore the difficult issue of tools versus prostheses. We 
never stopped automating technology. Automation can be seen as a natural 
extension of human capabilities. That is a simple transfer of cognitive and 
physical functions from people to machines; a very mechanical view of 
automation. Rasmussen’s model is an excellent example of a mechanistic 
model of human behavior that contributed to the development of cognitive 
engineering [RAS 86]. In reality, building an aircraft, for example, is not a 
function transfer because people do not naturally fly; we are handicapped 
compared to birds and, therefore, an aircraft is a prosthesis that enables us to 
fly. In a sense, the aircraft is a cognitive entity that was built using methods 
and tools developed by mechanical engineers and now information 
technology specialists.  
During the 1990s, many research efforts were carried out in human 
factors on ironies of automation [BAI 83], clumsy automation [WIE 89] and 
automation surprises [SAR 97]. Engineers automated what was easy to 
automate, leaving the responsibility of complex things to human operators, 
such as abnormal conditions. What was called automation surprise is 

Dealing with the Unexpected     7 
actually related to the topic of this paper on the unexpected. However, none 
of these research efforts take into account technology maturity and maturity 
of practice [BOY 13a]. People take time to become mature and learn. It is 
the same for technology and its usages. It takes many surprises to learn. 
Maturity is related to autonomy. Autonomy differs from automation in the 
sense that the former relates to problem solving and learning, as the latter 
relates to procedures following whether for machines or people. Indeed, 
procedure following is a kind of people behavior automation [BOY 13a]. 
Machines can be automated but are still far from being autonomous like 
people can be. 
Today, things are getting more difficult when we continue to use 
mechanistic cognitive models because our sociotechnical world is becoming 
more interconnected. Instead of mechanical devices, we have many pieces of 
software highly interconnected. Instead of complicated devices that we could 
deconstruct and repair, like the old mechanical clocks, we have layers of 
software that are difficult, and most of time impossible, to humanly diagnose 
when they fail, e.g. modern cars are comprised of electronics and software, 
and only sophisticated diagnostic systems enable troubleshooting. The level 
of technology and related-usages’ complexity drastically changed with the 
introduction of software. It is now even more complex as computer networks 
are not only local (e.g. in the car), but also more global (e.g. among cars and 
other cars with collision avoidance systems). How do we deal with the 
unexpected, and more generally variability, in such a highly interconnected 
world? 
1.3. Handling complexity: looking for new models 
Here are four examples of so-called “successful accidents”: the aborted 
Apollo 13 mission after an oxygen tank exploded on April 13, 1970; the 
DHL A300 landing in Baghdad shot by a missile on November 22, 2003; the 
US Airways A320 landing on the Hudson River after losing both engines on 
January 15, 2009; the Qantas A380 recovery around Singapore after the 
explosion of an engine on November 4, 2010 [ASA 13]. These examples are 
described in more details in section 1.4 of this book. They show that people 
can handle very complex and life-critical situations successfully when they 
have enough time [BOY 13b] and are equipped with the right functions, 
whether in the form of training and experience, or appropriate technology; in 
addition, these functions should be handled in concert. Consequences are 

8     Risk Management in Life-Critical Systems 
about life and death. We can see that problem solving and cooperative work 
are major ingredients of such successful stories. The main question here is to 
maintain a good balance between automation that provides precision, 
flawless routine operations and relief in case of high-pressure situations, and 
flexibility required by human problem-solving. Obviously, conflicts may 
occur between automation rigidity and people’s flexibility. Let us analyze 
this dilemma. 
Automation will continue to develop taking into account more tasks that 
pilots had to perform. It is also clear that, at least for commercial passenger 
transportation, pilots will be needed to handle unexpected situations for a 
long time. There will be surprises that will require appropriate reactions 
involving good situation awareness timewise [BOY 13b] and content-wise, 
decision-making, self-control, stress management and cooperation with the 
other actors involved. Dealing with the unexpected is not really a new skill 
that pilots should have, but instead of being frightened by the evolving 
complexity of our sociotechnical world, we should better understand and use 
this complexity. For example, since airspace capacity will continue to 
increase, it is better to use its hyper-redundancy to improve safety and 
constant management of unexpected situations, i.e. small and big variations 
of it. 
Automation rigidifies operations. Operational procedures also rigidify 
operations, since they automate human operator’s behavior. Therefore, both 
automation and procedure need to be used with a critical spirit, competence 
and knowledge. Human operators dealing with highly automated LCSs need 
to deeply know and understand the technology they are using, especially 
when this technology is not fully mature. Automation is good when it is 
designed and developed by considering its users, and when it has reached an 
acceptable level of maturity [BOY 13a]. There are even situations where 
people may switch to automation to improve safety. This requires 
competence, situation awareness and great decision-making skills. 
Automation shifted the human operator’s role from basic control to 
supervisory control and management [SHE 84]. Instead of directly 
manipulating handles and levers, human operators push buttons in order to 
manage software-intensive systems, which are often qualified of artificial 
agents [BOY 98]. Therefore, this new work environment involves human 
agents and artificial agents. We talk about humans and systems as a multi-
agent environment, and ultimately HSI. This shift from control to 

Dealing with the Unexpected     9 
management involves new emergent properties that need to be clearly 
identified. People in charge of such multi-agent environments need to know 
and understand these emergent properties. For example, it is now known that 
automation increases complacency in the long term, especially when it 
works very well. More generally, the best way to face the unexpected is to 
move from task training to skill training, such as astronaut training where 
they learn humility, time-constrained situations that require simple and 
effective solutions, and the most appropriate use of technology (considered 
as a tool and not as a remedy). 
For example, airspace is evolving everyday toward more aircrafts in the 
sky, especially in terminal areas. In 2011, the Federal Aviation 
Administration (FAA) anticipated that the U.S. air transportation would 
double over the next two decades [HUE 11]. Eurocontrol anticipated similar 
air traffic growth over the same period of time in Europe [GRE 10]. This 
growth tremendously changes the way air traffic control (ATC) will be 
performed during future decades. In particular, the increasing number of 
aircrafts and their interconnections will cause new complexity issues and 
emergences of new unexpected properties that we will need to identify and 
manage. Air traffic control will progressively evolve toward air traffic 
management (ATM). Air traffic controllers will become air traffic managers. 
During the PAUSA project, we identified various changes in authority 
sharing and a new model that we called the Orchestra model [BOY 13a, 
BOY 09]. 
Until now, ATC had authority on aircraft. We took the metaphor of the 
military where the general has authority on the chain of command down to 
the soldier. Within the military model, information flows are hierarchical, 
linear and sequential. In contrast, in the Orchestra model soldiers have 
become musicians (i.e. more specialized, cooperative and autonomous). The 
conductor replaces the general who coordinates the various information 
flows that have become more nonlinear and parallelized. In addition, the 
composer generates scores (prescribed tasks) that musicians follow to 
perform (effective task or activity). The composer coordinates these scores 
before delivering the symphony. We observed this very interesting change in 
the shift from ATC to ATM, where scores are contracts [BOY 09]. Today, 
we need to better define the function (jobs) of composers, conductors and 
musicians, as well as the overall organization of the Orchestra. 
 

10     Risk Management in Life-Critical Systems 
Until now, air traffic controllers had a reasonable number of aircrafts to 
control. They knew where aircrafts were located using radar technology. 
Their job consisted of ensuring a fluid traffic flow with no conflicts leading 
to collision. A new type of complexity emerges from traffic over saturation 
in final areas. In the future, instead of controlling they will need to manage 
like a conductor would manage an orchestra. A conductor’s situation 
awareness has to be perfect from beginning to end of a symphony. They 
need to deal with various personalities. They are managers in the sense of 
authority, effectiveness and professionalism. They are self-confident and 
have a good sense of humor. A good conductor knows about emerging 
patterns that an orchestra produces. He or she needs to identify these patterns 
in order to have the required authority. 
The management of LCSs is always based on a model, whether the 
military or the orchestra models for examples, which needs to be further 
elicited. We already argued that if we use the traditional linear model, where 
operational procedures could support most kinds of situations, the 
unexpected is typically considered as an exception to the rule or procedure. 
However, if we are in the nonlinear model of life, where problem solving is 
the major resource, the unexpected is an everyday issue that deals with care, 
concentration and discipline. 
1.4. Risk taking: dealing with nonlinear dynamic systems 
What do successful risk takers do? They prepare everything in detail 
before starting their activity. They usually detect all possible recovery 
situations where they can end up safe when everything goes wrong. They 
need to know and embody these kinds of things; “depending on their feeling 
of the situation, then they do not go.” They also need to know their 
limitations, which need to be compatible with the risk they will take. 
Preparation and risk assessment are the keys. They also need to accept that it 
takes a long time to learn these skills.  
Taking a risk involves a logical abduction process [BOY 10]. Abduction 
is one of the three inference mechanisms with deduction and induction. 
Abduction is about postulating a possible future and demonstrating that we 
can manage to reach it. John F. Kennedy abducted that Americans will go to 
the moon and get back safe to earth; NASA demonstrated that to be true in 
less than a decade. This is typically what great visionaries do. Abduction 

Dealing with the Unexpected     11 
requires competence, knowledge and understanding of the world, not 
necessarily to have a good idea, but to make sure that it is reachable. 
Abduction deals with goal-driven behavior and characterizing people’s 
intentions and actions. It is generally opposed to event-driven behavior, 
characterizing people’s reactions to events. In fact, people constantly switch 
back and forth from goal-driven to event-driven behavior. The resulting 
cognitive process is typically called opportunistic behavior. In aviation, 
pilots learn how to “think ahead” (this is a kind of abduction) and constantly 
shift from goal-driven to event-driven behaviors. 
Risk taking deals with discipline, i.e. there are safety margins that cannot 
be overridden and experts know them, therefore they are very disciplined 
and respect these safety margins scrupulously. The main difficulty is to 
handle the complexity of a risky situation. Complexity comes from the large 
number of factors involved. For example, a typical aviation situation results 
from a dynamic and nonlinear combination of aircrew psychological and 
physiological state, the way the given airline manages operations, aircraft 
state, ATC state, weather, ground infrastructure, commercial situation, 
airspace state (in terms of density and capacity), actual regulations, political 
situation and so on. The number of these factors and their states can vary 
unexpectedly. Their possible combinations are quite large, if not infinite. 
This inevitably creates complexity. Pilots always have in mind expected 
aviation situation patterns built from experience, and in practice what 
happens is never what they anticipated. However, the variation between the 
expected situation and the actual situation is most of the time very little, and 
is handled very smoothly. In some cases, such variation can be much bigger 
(Figure 1.1) such as in Apollo 13, which was an excellent example of a very 
well-orchestrated operation.  
 
Figure 1.1. Expected and actual situation showing small and bigger variations 

12     Risk Management in Life-Critical Systems 
In space programs, ground and board are very well coordinated both at 
design and operation times (i.e. astronauts, like musicians, are very well 
trained, responsible and autonomous; they know how to use the scores 
produced by a variety of engineers and scientists, as composers; flight 
director and control room officers are conductors). When the Apollo 13 
explosion occurred and oxygen tank no. 2 in the service module broke, in  
3 h, they lost all oxygen stores, water, electrical power and use of the 
propulsion system. The service module was no longer usable, and astronauts 
needed to use power and consumables of the lunar module that became the 
lifeboat for Apollo 13. Excellent teamwork started among the actors both 
onboard and on the ground. Ground mission control provided instructions 
(like a composer provides scores), and astronauts built a supplementary 
carbon dioxide removal system out of plastic bags, cardboard, parts from a 
lunar suit and a lot of tape. Even though the astronauts did not go to the 
moon that time, they got back to Earth safely. In fact, they did their job. 
Shortly after takeoff from Baghdad airport, terrorists shot a DHL A300 
cargo plane. The left wing tip was struck by a surface-to-air missile, which 
caused the loss of hydraulic flight control systems; the aircraft was 
uncontrollable in a classical way. No procedure was available for such a 
configuration of the aircraft. Pilots managed to land safely without injuries, 
using differential engine thrust as the only pilot input. They had to use their 
educated experience (i.e. nonlinear flight dynamics and mechanics first 
principles), and they did it successfully. 
US Airways Flight 1549 suffered a double bird strike after takeoff from 
LaGuardia airport. No engines were available. Consequently, the aircrew 
had to fly the aircraft as a glider. This was a very challenging situation 
especially in a populated area such as New York. The captain had to make a 
decision that was not in the book. He had to solve a problem. Once he made 
his decision, he managed the situation until he successfully landed the 
Airbus A320 in the Hudson River (i.e. goal-driven behavior in a very 
constrained environment). Again, all crewmembers did their jobs. 
One of the engines of the Qantas A380 Flight 32 exploded en route over 
Batam Island, Indonesia. The explosion damaged the fuel system causing 
leaks, disabled one hydraulic system and antiblock brakes, caused engines 1 
and 4 to go into a “degraded” mode, damaged landing flaps and the controls 
for the outer left engine 1. It took 50 min to complete this initial assessment,  
 

Dealing with the Unexpected     13 
due to the interconnectivity and nonlinearity of numerous operational 
procedures. Without panic; all crewmembers did their jobs very 
professionally, behaving like they were in a simulator. They returned to 
Singapore and landed safely with four blown tires. They did their jobs, 
managing the actual situation [ASA 13]. 
These extreme cases are the top of the iceberg of nonlinear system 
dynamics variations. It is useful to better understand complexity theories, 
such as catastrophe theory, bifurcation theory and chaos theory, as opposed 
to conventional reductionism. In the catastrophe theory, for example, we can 
learn that there are patterns that are inevitable catastrophes [THO 89]. In the 
bifurcation theory, we learn that for a small light change on a “bifurcation” 
parameter value of a system, a sudden “qualitative” or topological change 
occurs in its behavior [POI 85], e.g. a small change in temperature and 
pressure, may suddenly change steam into ice. In chaos theory [THU 98] we 
can learn that for very small variation in some variables, the behavior of the 
overall system may become uncontrollable after a while, but generates 
persistent patterns, called attractors, which can be identified and therefore 
managed. This nonlinearity needs to be understood and appropriated in 
various contexts by human operators who deal with LCSs. In particular, they 
need to understand that some parameters have a direct influence on the 
qualitative nature of the system behavior. 
Managing the unexpected is what is left to people over systems. It is the 
necessary operational glue that maintains the overall stability and integrity of 
HMSs. These people need to be able to understand what is going on, make 
their own judgments and act appropriately. Creativity is the key. These 
abilities do not come without extensive training over a long period of time. 
Unfortunately, creativity and procedure following are contradictory 
concepts. This is why we need to focus more on creativity to handle our 
everyday unexpected situations instead of continuing to believe that 
regulations, standards and procedures will support safety with this fallacious 
expectation of zero risk. 
Now, how can we train people to manage these variations between the 
expected situation and the actual situation? The best answer to this question 
is to look for stability. Stability can be passive or active. Passive stability 
does not require any specific action to apply on the system to return to a 
stable state, such as the pendulum. Conversely, active stability requires a  
 

14     Risk Management in Life-Critical Systems 
proactive attitude to maintain the system in a steady state, such as the 
inverted pendulum. In sociotechnical systems, we can experience both kinds 
of stability. Experience provides cases that can be categorized and further 
associated with appropriate behaviors related to either passive or active 
stability. In cases where passive stability prevails, we need to let go instead 
of counter-interact with the system, especially when automation does the job 
for us. When active stability is at stake, a proactive behavior is required. 
LCS human operators require very important skills such as creativity, 
familiarity, availability, adaptability (or flexibility), dependability and 
boldness. Indeed, any actor who needs to face unexpected situations is 
required to be: 
– creative and able to foresee possible futures; for example, when Captain 
Sullenberger decided to land his Airbus 320 on the Hudson river on January 
15, 2009, he was creative and, for sure, investigated all other possibilities 
before taking the risk [NTS 10]; 
– familiar with the environment where they work; for example, flying 
skills in various atmospheric situations and aircraft configurations; 
– familiar with the various tasks that they have to perform; for example, 
normal and abnormal tasks experienced in a flying simulator; 
– familiar with personal capabilities and limitations; for example, reduced 
perception of night situations while driving or working memory cognitive 
limitations; 
– familiar with organizational constraints and possibilities; for example, 
responsibility and accountability related to a job in an organization; 
– familiar with technological constraints and possibilities; for example, 
automation limitations and advantages in a large variety of situations; 
– available anytime anywhere during duty time; for example, 
management of complacency in case of routine activities and maintenance of 
proactive behavior; 
– adaptable (or flexible) to any operational situation; for example, facing 
an unexpected event such as wind shear, pilots will fit their behavior with 
respect to changes in their environment; they know the various contextual 
responses to wind shear (Skybrary Aero); 

Dealing with the Unexpected     15 
– dependable in life-critical situations; for example, a mountain guide is 
typically trustworthy in dangerous situation with his or her clients; 
– bold in risk taking; for example, facing an unexpected life-critical 
situation a human operator should have the courage to take an appropriate 
action that may put his or her life in danger. 
All these skills should be learned from experience (learning by doing). 
The use of simulators enables human operators to experience various kinds 
of situations and configurations, which would never be possible to 
experience in the real world because they would be too dangerous. These 
skills are not only individual, but also collective. They should be 
intelligently articulated during operations. This articulation process is 
another skill that needs to be learned. Therefore, trust is an important quality 
to be developed by team members who are likely to deal with LCSs and, for 
that matter, face unexpected life-critical situations. 
1.5. Discussion 
Dealing with the unexpected triggers various kinds of human factors such 
as time pressure and workload management, multitasking and complexity 
management. Consequently, design solutions should be found in an 
appropriate orchestration of technology, organization and people involved. 
Let us take two examples. 
The 2011 Fukushima Daiichi nuclear disaster is certainly one of the most 
unexpected events of that type in the nuclear industry. Let us analyze what 
“unexpected” means in this case. Taking into account the exceptionally low 
probability of occurrence of an earthquake (9.0 magnitude on the Richter 
scale) followed by a tsunami (40 m waves, but the plant was designed to 
resist to 5.7 m waves and the plant was struck by 10 m waves), and the 
extreme magnitude of the consequences, the formula (event-probability × 
consequence-magnitude) 
leads 
to 
indetermination. 
Therefore, 
the 
conventional technological reliability approach does not work here. Once an 
unexpected event occurs, people in charge have to make decisions. A 
domino effect started and led to the fact that there were not enough 
generators to cool the plant to a complete safe shutdown phase [SCH 12]. It 
was concluded that automation in this situation was insufficient for the 
events that occurred, and passive and fully automated systems would have 
significantly modified the outcome of the catastrophe.  

16     Risk Management in Life-Critical Systems 
However, even if technology is well designed to ensure safety, people 
may become too confident and/or may not have received training to handle 
specific situations; these factors are likely to induce unrecoverable 
situations. This is the case of the Air France 447 accident over the Atlantic 
on June 1, 2009. The final report [BEA 12] stated that “the accident resulted 
from a succession of events: temporary inconsistency between the airspeed 
measurements, probably following an obstruction of the Pitot tubes by ice 
crystals, that caused the autopilot to disconnect; inappropriate control inputs 
that destabilized the flight path and led to a stall; and pilot misunderstanding 
of the situation leading to a lack of control inputs that would have made it 
possible to recover.” In reality, these are contributing factors, and we need to 
consider that the captain left the cockpit to rest “in accordance to common 
practice”, and delegated the flight task to the least experienced copilot 
onboard; he reentered the cockpit when it was too late. The copilot flying the 
aircraft made “nose-up inputs despite stall warnings, causing a fatal loss of 
airspeed and a sharp descent.” Stall warning sounded for 54 s, which is a 
long time. The problem is that pilots had not received specific training in 
“manual airplane handling of approach to stall and stall recovery at high 
altitude”; this was not a standard training requirement at the time of the 
accident. None of the pilots understood what was happening. This is a 
typical case where appropriate training and airmanship could have greatly 
contributed to avoid the accident. In addition, the absence of expert 
leadership redundancy and involvement was critical. 
In these two examples, function allocation between people and 
automation was a major issue. What should we automate? What should be 
the role of people in charge of LCSs? In all cases, it is crucial to determine 
not only the various roles (and authorities) of people and technology, but 
also the way people and systems are organized. Roles should be associated 
to relevant contexts and appropriate resources. Of course, resources should 
be available in the various operational contexts. The three attributes (role, 
context and resources) correspond to the definition of cognitive functions 
[BOY 98]. Therefore, cognitive function analysis is a good approach to 
anticipate appropriate function allocation among human and system agents. 
Finally, time is critical to manage failures, incidents and accidents. Time 
can be analyzed at various levels such as operational and maturity levels. At 
the operational level, an equation relates the required time (TD) to the 
available time (TA) to do something [BOY 13b]. The closer the ratio TD/TA  
 

Dealing with the Unexpected     17 
gets to one, and the bigger than one it gets, the more difficult it is for human 
operators to handle the situation at stake. Obviously, even in very dangerous 
situations, human operators are able to handle safety margins because this 
ratio is less than one. At the maturity level, criteria such as safety, efficiency 
and comfort are constantly optimized, and we end up with a maturity period 
already explained elsewhere [BOY 11]. 
1.6. Conclusion 
This chapter introduced concepts and approaches that enable the 
investigation of unexpected events and deal with them in our complex 
sociotechnical world. We saw that it is a question of technology, 
organizations and people (the TOP model). Technology can greatly help time 
pressure and complex situation management by supporting humans in case 
of excessive workload. Of course, such highly-automated technology should 
be reliable, dependable and mature. It should also be understood by the 
human operators involved in the control and management of the LCS at 
stake. Organization is another support for handling unexpected events. 
Communication, cooperation and coordination are important processes that 
need to be developed to ensure good collective situation awareness. Team 
spirit and trust are crucial assets. People involved, whether designers or 
users, must be competent, creative and familiar with all aspects of the 
situation, available, dependable and bold. This takes extensive training and 
operational experience, motivation and enthusiasm. 
We now understand that dealing with the unexpected strongly requires a 
new philosophy of operations departing from a linear approach that removes 
small variations from the start and “discovers” unexpected events to a 
nonlinear approach that takes care of these variations in real-time. We need 
to move from the now conventional procedural approach where human 
operators are obedient soldiers (metaphor of the military) to a collaborative 
problem solving approach where the actors are more autonomous musicians 
(metaphor of the orchestra) [BOY 13a]. This does not mean that operational 
procedures have to be removed. They are very useful in normal and 
abnormal operations, but actors have to learn how to override them to adapt 
to fluctuating situations. Risk taking and complexity management are major 
skills to develop. This is an education issue [BOY 13c]. 

18     Risk Management in Life-Critical Systems 
1.7. Bibliography 
[ASA 13] Dossier on air transport pilot facing the unexpected, Air and Space 
Academy, Paris, France, 2013. Available at http://www.academie-air-
espace.com/event/newdetail.php?varCat=14&varId=216. 
[BAI 83] BAINBRIDGE L., “Ironies of automation”, Automatica, vol. 19, no. 6,  
pp. 775–779, 1983. 
[BEA 12] On the accident on 1st June 2009 to the Airbus A330-203 registered  
F-GZCP operated by Air France flight AF 447 Rio de Janeiro – Paris, Final 
report, Bureau d’Enquêtes et d’Analyses, Paris, France, 2012. 
[BOY 98] BOY G.A., Cognitive Function Analysis, Ablex/Greenwood Publishers, 
Westport, CT, 1998.  
[BOY 09] BOY G.A., “The orchestra: a conceptual model for function allocation and 
scenario-based engineering in multi-agent safety-critical systems”, Proceedings 
of the European Conference on Cognitive Ergonomics, Otaniemi, Helsinki area, 
Finland, 30 September–2 October, 2009. 
[BOY 10] BOY G.A., BRACHET G., Risk Taking, Dossier of the Air and Space 
Academy, Toulouse, France, 2010. 
[BOY 11] BOY G.A., Conclusion of the Handbook of Human-Machine Interaction: 
A Human-Centered Design Approach, Ashgate, UK, 2011.  
[BOY 13a] BOY G.A., Orchestrating Human-Centered Design, Springer, UK, 2013. 
[BOY 13b] BOY G.A., “Time in life-critical human-computer interaction”, 
Workshop on Changing Perspectives of Time in HCI, ACM CHI’13, Paris, 
France, 2013. 
[BOY 13c] BOY G.A., “From STEM to STEAM: toward a human-centered 
education”, Paper submitted to the European Conference on Cognitive 
Ergonomics, Toulouse, France, 2013. 
[DUB 01] DUBOIS D., PRADE H., “Possibility theory, probability theory and 
multiple-valued logics: a clarification”, Annals of Mathematics and Artificial 
Intelligence, vol. 32, pp. 35–66, 2001. 
[EAS 04]  Certification Specifications for Large Aeroplanes CS-25, www.easa.eu. 
int/doc/rulemaking/nPa/nPa_15_2004.pdf. 
[GRE 10] GREGOROVA M., EUROCONTROL Long-Term Forecast: Flight 
Movements 2010 – 2030, CND/STATFOR Doc415, Eurocontrol, Brussels, 
2010. 
 

Dealing with the Unexpected     19 
[HOL 98] HOLLNAGEL E., Cognitive Reliability and Error Analysis Method: 
CREAM, Elsevier Science, Oxford, 1998. 
[HUE 11] HUERTA M.P., FAA Aerospace Forecast: Fiscal Years 2012-2032, U.S. 
Department of Transportation, Federal Aviation Administration, Aviation Policy 
and Plans, 2011. Available at http://www.faa.gov/about/officeorg/headquarters 
offices/apl/aviation_forecasts/aerospace_forecasts/2012-2032/media/2012%20FA 
A%20Aerospace%20Forecast.pdf. 
[IAE 06] IAEA, A System for the Feedback of Experience from Events in Nuclear 
Installations, Safety Guide no. NS-G-2.11, IAEA Safety Standards, International 
Atomic Energy Agency, Vienna, 2006.  
[NIL 03] NILSEN T., AVEN T., “Models and model uncertainty in the context of risk 
analysis”, Reliability Engineering & System Safety, vol. 79, pp. 309–317, 2003. 
[NTS 10] NTSB, Loss of thrust in both engines after encountering a flock of birds 
and subsequent ditching on the Hudson river US airways flight 1549 airbus 
A320-214, 
N106US, 
Accident 
Report 
NTSB/AAR-10/03 
PB2010-
910403Weehawken, NJ, 15 January, 2009. 
[POI 85] POINCARÉ H., “L’Équilibre d’une masse fluide animée d’un mouvement de 
rotation”, Acta Mathematica, vol. 7, pp. 259–380, September 1885. 
[RAM 11] RAMADA M., “Beyond our imagination: Fukushima and the problem of 
assessing risk”, Bulletin of the Atomic Scientists, 2011. Available at http:// 
thebulletin.org/web-edition/features/beyond-our-imagination-fukushima-and-the-
problem-of-assessing-risk. 
[RAS 86] RASMUSSEN J., Information Processing and Human-Machine Interaction, 
Elsevier, Amsterdam, 1986. 
[SAR 97] SARTER N.B., SARTER D.D., BILLINGS C.E., “Automation surprises”, in 
SALVENDY G. (ed.), Handbook of Human Factors & Ergonomics, 2nd ed., 
Wiley, 1997. 
[SCH 12] SCHMITT K.A., “Automations influence on nuclear power plants: a look at 
three accidents and how automation played a role”, Proceedings of the 2012 IEA 
World Congress, Recife, Brazil, IOS Press, Work 41, 2012.  
[SHE 84] SHERIDAN T.B., “Supervisory control of remote manipulators, vehicles 
and dynamic processes: experiment in command and display aiding”, Advances 
in Man Machine Systems Research, vol. 1, pp. 49–137, 1984. 
[SKY 89] SKYBRARY, EUROCONTROL, low level winds hear, Last modified on 6 
March 2014. http://www.skybrary.aero/index.php/Low_Level_Wind_ Shear. 
 

20     Risk Management in Life-Critical Systems 
[THO 89] THOM R., Structural Stability and Morphogenesis: An Outline of a 
General Theory of Models, Addison-Wesley, Reading, MA, 1989.  
[THU 98] THUAN T.X., Le Chaos et l’harmonie – La fabrication du réel, Folio 
Essais, Gallimard, Paris, 1998. 
[WIE 89] WIENER, E.L., Human factors of advanced technology (‘glass cockpit’) 
transport aircraft, Technical report 117528, NASA Ames Research Center, 
Moffett Field, CA, 1989. 
 

2 
Vulnerability and Resilience Assessment 
of Infrastructures and Networks: Concepts 
and Methodologies 
2.1. Introduction  
The vulnerability of infrastructures and networks is being increasingly 
studied especially regarding combined natural/technical risks or/and 
intentional threats, in response to recent catastrophic events and their global 
consequences (extended consequences to a large area, many countries or 
more). This was the case, for example, for the consequences of 11 
September 2001 attacks in New York City and Washington D.C., disasters 
such as the Haiti earthquake on 12 January 2010 or the Tohoku earthquake 
on 11 March 2011 in Japan [KIT 11], or also, those due to the 
Eyjafjallajökull eruption in Iceland on April 2010 (especially the airspace 
closure) [SAM 10]. All these disasters show that our organizations, networks 
and systems present numerous vulnerabilities. Even if anticipated and 
preventive actions are sometimes possible, it is clear that it is not possible to 
systematically avoid these catastrophic events. In such circumstances, the 
resilience of the systems and associated organizations has to be taken into 
account. Then, the improvement of the resilience of systems is a complex 
problem and needs to develop combinations of specific strategies. These 
strategies are based on necessary systemic approaches and developments of 
adapted methodologies. Some methodologies have been recently developed 
coming from fields as diverse as: technological risk analysis (as Quantitative 
                         
Chapter written by Eric CHÂTELET. 

22     Risk Management in Life-Critical Systems 
Risk Assessment (QRA) methods), resilience engineering [HOL 06] and 
global security approaches (cyber-defense through telecom networks and 
Internet), but also logic system security, system of system vulnerability, etc. 
All these methods are adapted to specific contexts, and in general cases, a 
combination of them is needed. This is why generic conceptual studies of the 
vulnerabilities and resilience of systems have been developed [HOL 06,  
HAI 09, AVE 11]. 
This chapter deals with these generic concepts and systemic approaches 
concerning risk, vulnerability and resilience concepts, presents paradigms as 
consequences of those and some recent methodologies for resilience analysis 
and assessment. Finally, the conclusion is dedicated to the new challenges 
and fields to be studied in terms of system resilience. 
2.2. Risk and vulnerability 
The concepts of risk and vulnerability have been defined and studied in 
various domains of the sciences from social to technological point of views. 
The aim of this section is to give a synthesis of the main ideas and concepts 
connected with those, and to show the links between the risks and 
vulnerabilities. 
2.2.1. Concept of risk 
The risk is a concept primitively based on two alternative circumstances, 
the safe and unsafe ones. Then, criteria are needed in order to decide what 
safe or unsafe circumstances are. But, various approaches of the risk concept 
are proposed in different scientific domains. This is mainly due to the way to 
discriminate between safe and unsafe circumstances (for example, in terms 
of technological and industrial point of views, the probabilistic approach and 
costs are taken into account, and in social point of views, the quality of life 
and acceptability are taken into account). Nevertheless, some generic 
concepts can be distinguished. Usually (especially in industrial engineering, 
insurance, etc.), two main components are identified: the possibility of 
specific occurrences or circumstances (probability or quantitative level) and 
the more or less unsafe associated circumstances leading to degradations or 
damage of the system and/or its environment (damage costs, number of 
victims, generally a potential loss magnitude). This second component 
implies economical and social acceptance problem (including cultural 

Vulnerability and Resilience Assessment of Infrastructures and Networks     23 
aspects). It can be noticed that these two components are well-established in 
technological approach of the risk, typically illustrated by the Farmer 
diagram [FAR 67] coming from the nuclear risk analysis. Then, for given 
occurrences or circumstances, the combination of these two components is 
used to assess the risk (criticality) according to, respectively, a “likelihood 
level” (uncertainty measure based on probabilities, etc.) and a “severity 
level” (consequences assessment with more qualitative aspects). This 
approach needs to define “acceptable zones” corresponding to acceptable 
“likelihood-severity” values in order to help the decision-making. 
Recently, a new definition in Risk Management ISO 13000-2009 (ISO 
Guide 73:2002) has been proposed with a different approach. The definition 
of risk is based on the effect of the uncertainty on objectives enlarging it to 
positive aspects. This idea of taking into account positive and negative 
aspects is correlated to recent resilience approaches and will be discussed in 
section 2.4. 
Type of events 
Detailed events 
Natural origins 
 
Geological origin: landslides and earthquakes, tsunamis, volcanic 
eruptions and other natural emissions (gas, etc.) 
 
Weather origin: forest fires, runoff and floods, avalanches, 
hurricanes and tornadoes, storms and effects of climate changes 
Biological agents: microorganisms, insect/vermin infestation, etc. 
 
Extraterrestrial origin (very rare): meteorites 
Human origins 
 
Organizational origin: humans failures (incorrect human action and 
lack of human action), defects in design, procedures and/or 
organization 
 
Malevolence origin, thefts, sabotage and/ or revenge action, damage 
of any kind attacks such as nuclear-radiation-biological-chemical-
explosion (NRBCE) or information attacks (cyber-attacks, etc.). 
These actions may touch or affect the material, but also the personal 
or sensitive information 
Technological 
origin 
Fire: pool fire, flash fire, fireball and jet fire 
 
Explosions: confined vapor cloud explosions (CVCEs), boiling 
liquid expanding vapor explosion (BLEVE), vented explosion, vapor 
cloud explosion (VCE), dust and mechanical explosion 
 
Toxic chemicals release: from process or storage sites and 
transportation accidents 
 
Nuclear and radiological threats: from nuclear facilities/plants, 
radioactive sources (hospital, etc.) and radioactive waste storage etc. 
Water: broken pipes and various leaks, blocked drains, etc. 
Table 2.1. Classification of initiating events 

24     Risk Management in Life-Critical Systems 
The first of these two components is based on specific identified events 
(primary events inducing system degradation processes) that are able to be 
amplified or mitigated according to interactions with environment and/or 
system at a given time (states of different processes or actors that are able to 
interact with the degradation processes, called sometimes “danger field”, see 
section 2.5.1). Generally, all the primary events are not predictable, 
uncertain behaviors being associated with their occurrences. This is typically 
the case of natural events such as earthquakes or intentional events such as 
malevolence or attacks (terrorism). The variability of the possible primary 
events (and associated processes) explains why it is difficult to propose 
general methodologies to study different dangerous circumstances for 
complex infrastructures and networks (for example, it is easier to study 
failure processes of a given technological component in order to assess its 
failure probability). However, these primary events appear alongside threats 
for which a general classification is possible as in Table 2.1. Other 
classifications take into account the frequency of events and the possibility 
of preparedness of the system [WES 06]. 
Several aspects are able to reduce these primary event uncertainties and 
corresponding consequences (then, sometimes reducing the vulnerability, see 
the mitigation concept, at the end of section 2.3), such as: anticipation of 
dynamical circumstances in socio-economical processes (political, social or 
economical circumstances), monitoring of system components, networks and 
their environment (data and sense mining, weak signal, detection and 
identification of changes, diagnostic and alarm process) and forecasting 
models connected with the monitoring process. It can be noted that these 
aspects should be exploited during a “crisis”, i.e. for anticipation or 
monitoring the degradations/damage, and also, a “post-crisis” i.e. the coming 
back to “normal” situation – characterized by a new system equilibrium 
similar or judged equivalent to the situation before the primary event. Also, 
detection and observation are integrated as a third component in the 
likelihood/severity approach for risk assessment. Then, if this factor is 
significant, it may use a detection scale extended from “inefficient detection” 
to “high efficient detection”. 
Nevertheless, these “positive” aspects are sometimes, and particularly in 
disruption cases, insufficient to avoid the propagation of damage processes 
able to lead to catastrophic consequences. The damage characterizes the  
 
 

Vulnerability and Resilience Assessment of Infrastructures and Networks     25 
second main component of the risk. Usually, its assessment is based on 
“severity levels” that can be determined by estimated cost damage (degraded 
or destroyed material, lost or falsified critical information, number of injured 
and victims and their consequences as handicap, etc.) or various qualitative 
consequences (or mixed, quantitative – qualitative). These qualitative 
consequences are generally related to socio-economical aspects: losses of 
industrial or commercial activities, stock quotes and consecutive 
unemployment, loss of image abroad of companies (as TEPCO Company in 
Fukushima accident due to the Tohoku earthquake), financial difficulties of 
assurance companies, the social low morale, etc. These aspects will be 
discussed in the description of resilience analysis (section 2.6).  
Also, in difficult circumstances, the management of the consequences, 
the “crisis management”, has to be taken into account as a risk component. 
During the crisis development, this management is in charge of coordination 
of the operational actions, and particularly, the allocation and deployment of 
available resource. But, the corresponding decisions can amplify or mitigate 
the damage processes according to its dynamical evolution (spatially and 
temporally). This is due to a lack of information and uncertainties associated 
with this complex evolution (multiple interdependent processes in a large 
area and with different kinetics) leading to decisions with uncertain 
consequences (typically, a given decision at different times can have 
opposite consequences). Then, some resource and organizational processes 
are able to reduce partially these uncertainties as, for example, available 
monitoring and sensing systems (eventually completed with new resources), 
or crowd-sourcing, especially in social networks (these aspects participate to 
resilient processes). 
Other important components related to risk have to be studied, especially 
the social acceptability (tolerability) and vulnerability (sensitivity and 
exposure). The social acceptability is mainly qualitative and uncertain; this is 
clearly a more difficult aspect in the risk assessment. This component is 
dependant of the local context (company, organization and local social 
situation) and influenced by cultural and societal environments − the system 
resilience is also influenced by these contextual aspects. 
The vulnerability is an important component correlated to the risk. It 
 has various definitions according to the domain, physical or informational 
point of views, human or social ones, etc. This concept is detailed in the next 
section. 

26     Risk Management in Life-Critical Systems 
2.2.2. Concept of vulnerability 
This vulnerability in risk analysis, and especially its measure, is subject 
to important research from 1990s in the safety domain (disasters). It has been 
reinforced in security domain after the 11 September 2001 attacks [GAR 04] 
and also extended in sustainability approaches [TUR 10]. This concept has 
also been particularly studied in informatics and networking (ISO 27005) for 
which the access and capacity to exploit the system flaws are preponderant 
elements (cyber-security applications). 
The vulnerability corresponds to the sensitivity of a system to threats in 
specific circumstances (or scenarios) or catastrophic situations. Then, this 
concept definition can be based on three dependent aspects: 
– the context (similar to danger field in risk analysis approaches, see 
section 2.5.1) influencing the system and its threats (social environment and 
political situation, climate, moment in the day, etc.) at a given time; 
– the potential threats able to disrupt some or all the main system 
functions (sources may be external or internal, possibly multiple); 
– the system functioning and its components, especially those able to 
intervene in the perturbation process or subject to degradation (possible 
“targets”, reachable by the perturbation directly or through cascading effects, 
etc.), or in terms of safety or unsafety factors. 
In the risk assessment, the vulnerability is sometimes used instead of the 
“likelihood” component of the risk, especially when it is difficult to assess 
this last. But, in general cases, it is used as a complementary factor, 
especially if the likelihood is assessed by a quantified value (probability or a 
possibility of occurrences) that is independent of the state and functioning of 
the system. Indeed, the possible events may induce consequences only if the 
vulnerability of the system is enough weak face to the events intensity and 
considering the context at this time. The interest of vulnerability assessment 
is to give the decision elements to choose the resource and organizational 
aspects in order to improve it for given threats. 
In the risk assessment, a vulnerability factor only depending on the 
system state becomes useless. Consequently, in reliability theory, this factor 
is not studied. Indeed, for example, the unreliability is defined as the  
 
 

Vulnerability and Resilience Assessment of Infrastructures and Networks     27 
probability of an occurrence (failure), implicitly characterized by a weak 
enough vulnerability (stress is more than a threshold separating the 
functioning states from the breakdown states), in a specified period of time. 
2.3. Vulnerability analysis and assessment 
Then, an infrastructure or a network vulnerability assessment has to take 
into account their relations with the possible aggressive processes in space 
(geographic aspects) and time (the moment and delays). Generally, the 
analysis is developed according to complementary static and dynamic 
approaches. In any case, three main contributions are needed:  
– the threats characterization and classification, i.e. based on the main 
classes in Table 2.1, with for malevolence cases, an adaptation according to 
the potential financial, material and human resources, intention (especially in 
case of death wish) and willpower. In case of cascading effects, the potential 
threats have to be completed with objects or processes not present at the 
primary event time (for example, landslides after an earthquake, or fires 
spreading from building to industrial installations, etc.); 
– the identification, characterization and classification of the physical, 
informational and organizational defensive and reactive properties of the 
system (material and informational protections, human resources, 
safety/security procedures and strategies in case of alert), including the help 
of external resources (emergency aid, police, etc.); 
– the environmental or internal circumstances in space and time able to 
influence the preceding elements. In case of natural or NATECH disaster 
(natural and technological events), the geographical position and dynamics 
(hydrological and geological characteristics, urban or mixed place, etc.) or 
the weather (rain, fog, dryness, etc.) have to be taken into account. In case of 
malevolence, the political, economic and social climate is important. 
In practice, the vulnerability assessment is based on structured analysis 
using these three contributions. The characterizations in these approaches are 
based on various parameters as given in Table 2.2 [FEM 03]. But, these 
parameters need to be studied taking into account their dynamic 
interdependencies [ZIO 11, WAN 13] and human factors [BIR 11]. This type 
of study is well-known in risk analysis (accident scenarios), but more  
 

28     Risk Management in Life-Critical Systems 
recently developed in intentional cases for vulnerability assessment  
[GAR 04, PIW 09]. 
Criteria 
0 
1 
2 
3 
4 
5 
Score 
Asset 
visibility 
– 
Existence 
not well-
known 
– 
Existence 
locally 
known 
– 
Existence 
widely 
known 
 
Target utility None 
Very low 
Low 
Medium 
High 
Very high  
Asset 
accessibility 
Remote 
location, 
secure 
perimeter, 
armed 
guards 
tightly 
controlled 
access 
Fenced 
guarded 
controlled 
access 
Controlled 
access, 
protected 
entry 
Controlled 
access, 
unprotected 
entry 
Open 
access, 
restricted 
parking 
Open 
access, 
unrestricte
d parking 
 
Asset 
mobility 
– 
Moves or is 
relocated 
frequently 
– 
Moves or is 
relocated 
occasionally
– 
Permanent
/fixed in 
place 
 
Presence of 
hazardous 
materials 
No 
hazardous 
materials 
present 
Limited 
quantities, 
materials in 
secure 
location 
Moderate 
quantities, 
strict 
control 
features 
Large 
quantities, 
some control 
features 
Large 
quantities, 
minimal 
control 
features 
Large 
quantities, 
accessible 
to non-
staff 
personnel 
 
Collateral 
damage 
Potential 
No risk 
Low 
risk/limited 
to 
immediate 
area 
Moderate 
risk/limite
d to 
immediate 
area 
Moderate 
risk within 
1-mile radius
High risk 
within  
1-mile 
radius 
High risk 
beyond  
1-mile 
radius 
 
Site 
Population/c
apacity 
0 
1–250 
251–500 
501–1,000 
1,001–
5,000 
>5,000 
 
 
 
 
 
 
 
Total 
 
Table 2.2. Site/building inherent vulnerability assessment matrix  
(partial risk assessment) [FEM 03] 
If it is possible to assess system vulnerability, preventive actions can be 
decided for its reduction. Then, the proposed mitigation procedures are able 
to avoid or to mitigate some consequences. Generally, the mitigation 
consists of improvements of the system robustness face to given threats (see 
resilience concepts, section 2.4.2), intensity reduction of threats or 
propagation factors if possible (for example, adapted urban development 
against floods and preventive heath actions against epidemics). 

Vulnerability and Resilience Assessment of Infrastructures and Networks     29 
2.4. Resilience and main associated concepts 
2.4.1. Resilience: a multifaceted concept 
The word resilience comes from the Latin word “resilire” that means 
“jump back”, “rebound” (see detailed historical aspects of resilience in  
[ALE 13]). It has been used in middle-age in the meaning of “free from 
commitments” (legal aspect). Francis Bacon has used it to characterize the 
physical phenomenon of echo (1627). In 19th Century, this word has been 
used in mechanics to define the capability of a strained body to recover its 
shape after deformation. This definition has been standardized by the 
Georges A. Charpy impact test that measures the amount of energy absorbed 
by a material during fracture (ISO 148-1: 2009). In common language, its 
meaning is a strong psychological ability face difficulties, this aspect being 
studied in detail first by Werner [WER 93] and more recently by Cyrulnik 
and Jorland [CYR 12]. The resilience concept has been developed in larger 
systems 
from 
1970s, 
as 
in 
ecosystems 
from 
Holling’s 
work  
[HOL 73] and more recently [WAL 06], information systems (first in terms 
of database resilience, [DIM 76]), and from 1980s, in telecommunication 
and networks [HUR 87], engineering and organizations (high-reliable 
organization, [WEI 87], resilience engineering [HOL 06]), etc. In the last 20 
years, numerous domains have used this concept in different contexts 
(technological system resilience, human, social and organization resilience, 
economic 
resilience, 
resilience 
in 
sustainability 
and 
region/urban 
development, etc.). Nevertheless, there are general properties and associated 
concepts, exploited in the resilience modeling and assessment. 
Resilience is based on the ability of a system to recover from a 
perturbation, i.e. a shock (generally with high amplitude, a disruption) or a 
diffuse danger, enough to change the nominal system functioning (state) to 
an unacceptable one according to its objectives. Discussions about resilience 
definition can be seen in [HAI 09, AVE 11]. Then, some remarks are 
needed: 
– depending of the domain, the considered perturbation is very different, 
and also the objective of the resilience assessment, then the latter can be the 
improvement of residual functioning face to high perturbations (networks), 
the improvement of recovering processes after functioning disruption  
(or near it), the improvement of proactive behaviors in organizational 
approaches and safety (resilience engineering) or the improvement of a 

30     Risk Management in Life-Critical Systems 
defense against diffuse dangers (violence in the society, etc.). Its positive 
approaches are typically studied in [TIE 03, WOO 06, OIE 10]; 
– the recovered functioning and “nominal” states of the system are not 
necessarily the same as initial ones, but are acceptable compared to the 
system objectives. This aspect has to be taken into account especially in the 
case of disasters or high perturbations. The new functioning and/or state may 
be understood as an improvement according to the performance assessment 
and the perception of the situation (see systemic approach, section 2.5.1). 
In socio-technical systems, the resilience is due to combined human, 
technological and organizational factors and the environmental influences, 
and particularly, the interdependencies and adaptive behaviors. The latter 
imply the study of auto-organizational processes, the learning processes and 
feedback, the knowledge capitalization and sharing of the best practices 
(according to the context), etc. These complementary aspects induce 
emerging behaviors in socio-technical systems, which are typical systemic 
concepts in complex system analysis (section 2.5.1). 
2.4.2. Main resilience components 
From 
the 
general 
concepts 
associated 
with 
resilience, 
some 
methodologies have been developed for its analysis and assessment. It is 
possible to distinguish three main types of resilience analyses [COM 10]: 
– the definition of strategies based on prevention, protection, 
preparedness and consequence management (crisis management) (as in the 
specific European programme decided in Council Decision 2007/124/EC) 
for response and recovery after a disaster [VAL 05] – contingency plan; 
– the studies of “resilient” abilities especially those faced in extreme 
situations, at the limit of prevention and anticipation [WEI 07]. The interest 
of these studies is the possibility to identify resilient behavior or processes in 
various contexts. Also, the flexibility in the organization may contribute to 
the resilient behaviors complementary to identified procedures induced by 
preparedness phase [MCD 06, AMA 06]; 
– the studies of various perturbation situations and the corresponding 
adaptation modes of the systems in order to improve the safety management 
in general. This is the aim of resilience engineering [HOL 06]. 

Vulnerability and Resilience Assessment of Infrastructures and Networks     31 
The methods for resilience assessment have been developed recently 
(2000s) and are based on global concepts introduced above and some 
additional ones. Two basic approaches are the PR2 and R4 approaches. 
The PR2 approach consists of assessment of preparedness, response and 
recovery, see significant works as [HAR 07, MAL 07, JON 11]. For details 
about these three components, see, for example, [WEI 07] for preparedness 
and response, and [WER 93] for recovery. The advantage of this approach is 
its link with classical ones in risk analysis especially for the case of crises 
with three consecutive phases. But, various assessments of these three 
components can be obtained, because it is difficult to propose a generic 
definition of main parameters characterizing them. 
The R4 approach is based on four components: robustness, 
resourcefulness, redundancy and rapidity, with following the definitions  
[BRU 03, TIE 07, MEZ 11]: 
– robustness is the ability of the system and its components to resist when 
faced with perturbations in order to minimize the functioning losses (in 
mechanics [KAN 11]). This property is obtained in the preparedness phase, and 
is essential in order to improve the following phases (response and recovery); 
– resourcefulness consists of the adaptability of the system when faced 
with perturbations using internal and/or external resource. This property is 
characterized by an analysis of the perturbations and their consecutive 
severity. Then, it is possible to estimate (and to allocate) necessary resources 
for response and recovery phases (material and technologies, information 
and networks, and humans), and to establish how they will be able to 
improve the situation (response and recovery). Consequently, the resource 
availability during the response and recovery phases is a measure of this 
property (it has to be noticed that the resource efficiency seems a better 
measure, but more difficult to assess, and connected to redundancy and 
rapidity components, see section 2.6); 
– redundancy is the property characterized by the ability of the elements 
(subsystems) or organization in a system to be substituted by others to 
permit the continuity of main functioning in highly-perturbed situations  
[KAN 11]. This property is linked to the resourcefulness and is important for 
improving the rapidity. It concerns the response and recovery phases; 
– rapidity is the ability of the system to recover its initial state, equivalent 
state or an acceptable one (according to the objective of the decision maker 

32     Risk Management in Life-Critical Systems 
that defines performance criteria, etc.). It mainly concerns the recovery 
phase (and partly the others) and depends on the other components 
(especially the resourcefulness and redundancy). 
Vulnerability is linked to resilience through mitigation (see section 2.3). 
But, reducing vulnerability is not equivalent to increasing resilience, the 
latter being more extended in time (vulnerability even with a “large 
definition” is not concerned or partially by recovery), the inverse being 
obvious (resilience improvement contributes clearly to reduce vulnerability). 
According to these elements, the resilience of a system appears in the 
context of a perturbation process and can be connected to danger processes 
studied in risk analysis. This is the reason why it seems interesting to use 
concepts and methodologies of risk analysis and more generally system 
analysis, and to try to extend or to adapt them in the resilience context. Also, 
the consequences of this extension and particularly the new associated 
emerging paradigms have to be studied. The next section deals with this 
topic. 
2.5. Paradigms as consequence of risk analysis extension 
The concepts presented in preceding sections sometimes lead to 
incoherent and impossible application to resilience analysis or need to be 
well adapted. Then, these possible extensions and new methodologies 
integrating resilience and associated concepts highlight new paradigms and 
specific difficulties. 
2.5.1. Risk analysis extension and systemic approaches 
The systemic analysis and different associated approaches (see, for 
example, [PIW 09]) give an interesting point of view in which the resilience 
is clearly a partially emergent property (it is also the case for vulnerability). 
According to these approaches, the emerging properties are strongly 
correlated to the interdependencies (internal and external relations) and 
cybernetic loops. This concerns generalized loops containing four main 
actors: decision maker, controller, sensor and action maker, the latter three  
making the classical feedback loop (control theory), the decision maker 
being able to adapt the system goals (according to the system state and its 
relation with its environment) and consequently to modify the control laws. 

Vulnerability and Resilience Assessment of Infrastructures and Networks     33 
Then, the dynamical aspects and danger process can be analyzed using 
these systemic concepts. Among them, methodological analysis of 
dysfunctions of systems (MADS-MOSAR) has developed some interesting 
ideas [PER 00, THI 08]. In this methodology, a scope of hazards around a 
system is defined on the basis of fluxes of hazards (parameter of danger 
processes) that could be initiator, consolidator or inhibitor, with a source of 
hazard fluxes (primary events) and their effects (target systems). Also, 
several approaches have been developed in the general framework of 
“danger science” also called “cindynics” [KER 95], and more generally 
taking into account engineering, human and social sciences, including, for 
example, geopolitical aspects [KRO 08, PIW 09] as described in Figure 2.1. 
The main interest of these approaches is their ability to take into account the 
different resilience components (see section 2.4.2) with methodologies 
solving already large-scale problems in risk analysis [GAR 04, APO 05]. 
 
Figure 2.1. Factors shaping the risks faced to critical infrastructures [KRO 08] 
2.5.2. Paradigms emerging from risk analysis extension 
The extended methodologies to study and to assess different type of risks 
present some incoherent aspects or specific paradigms. Also, these 
paradigms concern the analysis and assessment of the resilience. 

34     Risk Management in Life-Critical Systems 
In risk analysis, some parameters or even concepts can be opposed or 
correlated, so it does not seem possible to propose non-ambiguous decision 
elements in general cases. The obvious example is the opposition between 
safety and availability constraints or safety and security. Then, in practice, 
the corresponding optimization problems (for example, to determine best 
resource and actions sharing out) are difficult to solve and need to be treated 
as multiobjective problems with necessary constraint relaxations (the 
decision maker has to reduce the required constraints using compromises 
and/or to choose foremost objectives). 
In this context, two main paradigms have to be known. The first paradigm 
is based on the nature of hazards. In industrial risk analysis (first level of 
complexity), the hazards are due to failures of technological systems, and 
human or organizational factors (sometimes, these latter two are not taken into 
account, or only partially). In a second level of complexity, the natural hazards 
are taken into account, and eventually the external resource and actions (for 
example, in the case of crisis management). The main difference between the 
two first levels is the difficulties to forecast the natural hazard characteristics 
(instant, duration, place, intensity, etc.) and effects even using probabilistic 
approaches, especially with unknown “state” of concerned infrastructures and 
organizations (especially the external ones) when it happens. The third level is 
characterized by the intentional actions (human aggression, malevolence, etc.) 
added or not to the other hazards. Its main differences are the difficulty to use 
probabilistic approaches and to elaborate scenarios because of the attacker’s 
ability to adapt its strategy to the vulnerabilities of the system, and even, to 
reconsider it dynamically. 
The second paradigm concerns the knowledge and uncertainty in the risk 
analysis depending of the levels of complexity given above [DEL 08,  
KAS 05]. The more simple case (“classical one”) is when the “cause-
consequence” logic is efficient. It is possible to use past events and/or 
probabilistic data, and also the same type of knowledge in terms of 
consequences. This is a situation for which “we know that we know” (see 
Figure 2.2). The second situation is when there are loops in the processes 
(especially mutual influence between the threats, the system and its 
environment behaviors). Then, the cause–consequence logic being 
inefficient, and the possibility to exploit the past events is difficult (because 
of very large number of potential scenarios). This situation is when “we do 
not know that we don’t know” (see Figure 2.2). Two other cases are 
possible, if the consequences are not known (possible in the case of  

Vulnerability and Resilience Assessment of Infrastructures and Networks     35 
well-known natural hazards but with unknown consequences because of new 
urban situation, etc.), “we know that we don’t know”, and if the threat is 
uncertain but the consequences are known, as for example, repetitive 
epidemics due to a mutant virus with uncertain characteristics. 
 
Figure 2.2. A proposition of risk situations and relevant risk assessment strategies 
2.6. Resilience analysis and assessment 
The resilience analysis can be based on new approaches and some useful 
concepts developed in risk analysis presented above. In the quantitative 
approaches, the main advances have been based on the R4 components in 
different contexts during this last 10 years. For example, interdependencies 
and the use of these components to assess the critical infrastructures 
resilience are studied in [ORO 07]. Other recent works have studied 
quantitative assessment of robustness and rapidity [ZOB 10, ZOB 11,  
MEZ 11]. 
Other specific works have contributed to the resilience assessments of 
networks as: sensitivity assessment to interdependencies with cascading 
effects (propagated loads) in networks assessments [ZIO 11] and important 
measures of components based on recoverability of networks (rapidity 
assessment) [BAR 13] etc.  

36     Risk Management in Life-Critical Systems 
Also, the assessment of “positive” contributions to resilience, especially 
in organizations, has been studied in [OIE 10]. But, these assessments and 
corresponding processes need to be placed in well-characterized situations, 
which are difficult for general resilient processes. Also, the functional 
resonance analysis method model (FRAM [HOL 08]), based on the 
resilience engineering, is able to help to analyze the interdependencies in 
accidental situations, in case of positive or negative contributions to the 
resilience. 
2.7. Conclusion: new challenges 
Based on risk analysis and assessment methodologies, and taking into 
account the recent development of resilience concept (from 1970s), new 
methods proposed for vulnerability and resilience assessment of systems are 
increasingly pertinent. Nevertheless, some paradigms have to be more 
understood and if possible overcome. Also, the links between general 
concepts (as PR2 and R4 ones) and operational ones (resource, 
functionalities, etc.) are necessary. The use of these concepts in new 
methodologies adapted for the decision-making in crisis management is 
interesting. Also, the concepts and methods coming from cindynics, 
territorial and social resilience are able to improve the analysis and 
assessment of system resilience. 
2.8. Bibliography 
[ALE 13] ALEXANDER D.E., “Resilience and disaster risk reduction: an etymological 
journey”, Nat. Hazards Earth Syst. Sci., vol. 13, pp. 2707–2716, 2013. 
[AMA 06] AMALBERTI R., “Optimum system safety and optimum system resilience: 
agonistic or antagonistic concepts”, in HOLLNAGEL E., WOODS D.D., LEVESON 
N., (eds.), Resilience Engineering Concept and Precepts, Ashgate,  pp. 253–270, 
2006. 
[APO 05] APOSTOLAKIS G.E., LEMON D.M., “A screening methodology for the 
identification and ranking of infrastructure vulnerabilities due to terrorism”, Risk 
Analysis, vol. 25, no. 2, pp. 361–376, 2005. 
[AVE 11] AVEN T., “On some recent definitions and analysis frameworks for risk, 
vulnerability, and resilience”, Risk Analysis, vol. 31, no. 4, pp. 515–522, 2011. 

Vulnerability and Resilience Assessment of Infrastructures and Networks     37 
[BAR 13] BARKER K., RAMIREZ-MARQUEZ E., ROCCO C. M., “Resilience-based 
network component importance measures”, Reliability Engineering and System 
Safety, vol. 117, pp. 89–97, September 2013. 
[BIR 11] BIRREGAH B., MULLER A., CHÂTELET E., “Interdependency-based 
approach of complex events in critical infrastructure under crisis: a ﬁrst step 
toward a global framework”, in BÉRENGUER C., GRALL A., SOARES C.G., (eds.), 
Advances in Safety, Reliability and Risk Management, ESREL Conference, 
Taylor & Francis Group, London, pp. 149–155, 31 August 2011. 
[BRU 03] BRUNEAU M., CHANG S.E., EGUCHI R.T., et al., “A framework to 
quantitatively assess and enhance the seismic resilience of communities”, 
Earthquake Spectra, vol. 19, no. 4, pp. 733–752, 2003. 
[COM 10] COMFORT L.K., BOIN A., DEMCHAK C.C., Designing Resilience, 
Preparing for Extreme Events, University of Pittsburgh Press, 2010. 
[CYR 12] CYRULNIK B., JORLAND G., Resilience – The Basic, Odile Jacob, 2012. 
[DEL 08] DELEUZE G., CHÂTELET E., LACLÉMENCE P., et al., “Are safety and 
security in industrial systems antagonistic or complementary issues?”, ESREL 
Conference, Valencia, Spain, vol. 4, pp. 3093–3100, 22–25, September 2008. 
[DIM 76] DIMLEY P.A., “An investigation into database resilience”, The Computer 
Journal, vol. 19, no. 2, pp. 117–120, 1976. 
[FAR 67] FARMER F.R., “Siting criteria – a new approach”, Proceedings of the 
IAEA Symposium on Nuclear Siting, IAEA, STI/PUB/154, SM-89, pp. 303–329, 
1967. 
[FEM 03] FEMA, “Asset value, threat/hazard vulnerability and risk”, Reference 
Manual to Mitigate Potential Terrorist Attacks Against Buildings, Risk 
Management Series, FEMA 426, December 2003. 
[GAR 04] GARRICK B.J., HALL J.E., KILGER M., et al., “Confronting the risks of 
terrorism: making the right decisions”, Reliability Engineering and System 
Safety, vol. 86, pp. 129–176, 2004. 
[HAI 09] HAIMES Y.Y., “On the definition of resilience in systems”, Risk Analysis, 
vol. 29, no. 4, pp. 498–501, 2009. 
[HAR 07] HARRALD J.R., “Restoring the national response system: fixing the flaws 
exposed by hurricane Katrina”, Transportation Research Board, TR News, vol. 
250, pp. 9–13, May–June 2007. 
[HOL 73] HOLLING C.S., “Resilience and stability of ecological systems”, Annual 
Review of Ecology and Systematics, vol. 4, pp. 1–23, 1973, Available at 
http://www.annualreviews.org/doi/abs/10.1146/annurev.es.04.110173.000245. 

38     Risk Management in Life-Critical Systems 
[HOL 06] HOLLNAGEL E., WOODS D.D., LEVESON N., (eds.), Resilience 
Engineering: Concepts and Precepts, Ashgate Press, Aldershot, UK, 2006. 
[HOL 08] HOLLNAGEL E., PRUCHNICKI S., WOLTJER R., et al., “Analysis of comair 
flight 5191 with the functional resonance accident model”, 8th International 
Symposium of the Australian Aviation Psychology Association, Sydney, 
Australia, 2008. 
[HUR 87] HURLEY B.R., SEIDL C.J.R., SEWELL W.F., “A survey of dynamic routing 
methods for circuit-switched traffic”, IEEE Communications Magazine, vol. 25, 
no. 9, pp. 13–21, September 1987. 
[JON 11] JONGEJAN B., HELSLOOT I., BEERENS R.J.J., et al., “How prepared is 
prepared enough?”, Disasters, vol. 35, no. 1, pp. 130–142, 2011. 
[KAN 11] KANNO Y., BEN-HAIM Y., “Redundancy and robustness, or, when is 
redundancy redundant ?”, Journal of Structural Engineering, vol. 137, pp. 935–
945, 2011. 
[KAS 05] KASTENBERG W., ESREDA, Proceedings of the 29th ESReDA seminar 
on: Systems Analysis for a More Secure World: Application of System Analysis 
and RAMS to Security of Complex System, Ispra, 25–26 October 2005. 
[KER 95] KERVERN G.-Y., “cindynics: the science of danger”, Risk Management,  
vol. 42, no. 3, pp. 34–42, 1995. 
[KIT 11] KITAMURA M., “Extraction of lessons from the Fukushima Daiichi 
accident based on a resilience engineering perspective”, in HOLLNAGEL E., 
RIGAUD E., BESNARD D., (eds.), Proceedings of the 4th Resilience Engineering 
Symposium, Presses des Mines, 2011. 
[KRO 08] KRÔGER W., “Critical infrastructure at risk: a need for a new conceptual 
approach and extended analytical tools”, Reliability Engineering and System 
Safety, vol. 93, no. 12, pp. 1781–1787, 2008. 
[MCD 06] MCDONALD N., “Organizational resilience and industrial risk”, 
HOLLNAGEL E., WOODS D.D., LEVESON N., Resilience Engineering, Concept 
and Precepts, Ashgate, pp. 155–179, 2006. 
[MAL 07] MALLAK P., “Capabilities-based planning for the national preparedness 
system”, Transportation Research Board, TR News, vol. 250, pp. 4–8, May–
June 2007. 
[MEZ 11] MEZZOU O., BIRREGAH B., CHÂTELET E., “A theoretical study of the 
interactions between the components of resilience in critical urban 
infrastructures”, International Conference on Smart and Sustainable City to be 
held in Shanghai, China, pp. 255–260, 6–8 July, 2011. 

Vulnerability and Resilience Assessment of Infrastructures and Networks     39 
[OIE 10] ØIEN K., MASSAIU S., TINMANNSVIK R.K., et al., “Development of early 
warning indicators based on resilience engineering, Probabilistic Safety 
Assessment and Management, PSAM 10, Seattle, WA, 7–11 June, 2010. 
[ORO 07] O’ROURKE T.D., “Critical infrastructure, interdependencies, and 
resilience”, The Bridge, National Academy of Engineering, vol. 37, no. 1,  
pp. 22–29, 2007. 
[PER 00] PERILHON P., “Analyse des risques, éléments méthodiques”, Phoebus, la 
revue de la sûreté de fonctionnement, l’analyse de risques, vol. 12, pp. 31–49, 
2000. 
[PIW 09] PIWOWAR J., CHÂTELET E., LACLÉMENCE P., “An efficient process to 
reduce 
infrastructure 
vulnerabilities 
facing 
malevolence”, 
Reliability 
Engineering and System Safety, vol. 94, pp. 1869–1877, 2009. 
[SAM 10] SAMMONDS P., MCGUIRE W., EDWARDS, S., (eds.), Volcanic Hazard from 
Iceland: Analysis and Implications of the Eyjafjallajökull Eruption, UCL 
Institute for Risk and Disaster Reduction, London, 2010. 
[TIE 03] TIERNEY K.J., Conceptualizing and Measuring Organizational and 
Community Resilience: Lessons from the Emergency Response Following the 
September 11, 2001, Attack on the World Trade Center, Disaster Research 
Center, Preliminary Papers, 2003.  
[TIE 07] TIERNEY K.J., BRUNEAU M., “Conceptualizing and measuring resilience: a 
key to disaster loss reduction”, International Conference on Smart and 
Sustainable City to be held in Shanghai, China from July 6 to July 8, 2011, TR 
News, vol. 250, pp. 14–17, 2007. 
[THI 08] THIVEL P.-X., BULTEL Y., DELPECH F., “Risk analysis of a biomass 
combustion process using, MOSAR and FMEA methods”, Journal of Hazardous 
Materials, vol. 151, pp. 221–231, 2008. 
[TUR 10] TURNER II B.L., “Vulnerability and resilience: coalescing or paralleling 
approaches for sustainability science?” Global Environmental Change, vol. 20, 
no. 4, pp. 570–576, October 2010. 
[VAL 05] VALE L.J., CAMPANELLA T.J., The Resilient City: How Modern Cities 
Recover from Disaster, Oxford University Press, 2005. 
[WAL 06] WALKER B., SALT D., Resilience Thinking, Sustainable Ecosystems and 
People in a Changing Worlds, Island Press, 2006. 
[WAN 13] WANG S., HONG L., OUYANG M., et al., “Vulnerability analysis of 
interdependent infrastructure systems under edge attack strategies”, Safety 
Science, vol. 51, pp. 328–337, 2013. 

40     Risk Management in Life-Critical Systems 
[WEI 87] WEICK K.E., “Organizational culture as a source of high reliability”, 
California Management Review, vol. 29, no. 2, pp. 112–127, 1987. 
[WEI 07] WEICK K.E., SUTCLIFFE K., Managing the Unexpected, Resilience 
Performance in an Age of Uncertainty, John Wiley & Sons, Inc., 2007. 
[WER 93] WERNER E.E., “Risk, resilience, and recovery: perspectives from the 
Kauai longitudinal study”, Development and Psychopathology, vol. 5, no. 4,  
pp. 503–515, 1993. 
[WES 06] WESTRUM R., “A typology of resilience situations”, in HOLLNAGEL E., 
WOODS D.D., LEVESON N., (eds.), Resilience Engineering, Concept and 
Precepts, UK, Ashgate, pp. 55–65, 2006. 
[WOO 06] WOODS D.D., “Essential characteristics of resilience”, in HOLLNAGEL E., 
WOODS D.D., LEVESON N., (eds.), Resilience Engineering, Concept and 
Precepts, Ashgate, pp. 21–34, 2006. 
[WOO 09] WOODS D.D., SCHENK J., ALLEN T.T., “An initial comparison of selected 
models of system resilience”, in NEMETH P., HOLLNAGEL E., DEKKER S., (eds.), 
Preparation and Restoration, Resilience Engineering Perspectives, Ashgate,  
vol. 2, pp. 73–94, 2009. 
[ZIO 11] ZIO E., SANSAVINI G., “Modeling interdependent network systems for 
identifying cascade-safe operating margins”, IEEE Transactions on Reliability, 
vol. 60, no. 1, pp. 94–101, 2011. 
[ZOB 10] ZOBEL C.W., “Comparative visualization of predicted disaster resilience”, 
Proceedings of the 7th International ISCRAM Conference–Seattle, 2010. 
[ZOB 11] ZOBEL C.W., “Representing perceived tradeoffs in defining disaster 
resilience”, Decision Support Systems, vol. 50, no. 2, pp. 394–403, 2011. 
 
 

3  
The Golden Hour Challenge: Applying 
Systems Engineering to Life-Critical 
System of Systems 
3.1. Introduction 
Road accidents are at the origin of numerous casualties. The first 
initiative consists of reducing the number of accidents by integrating driving 
assistants and active safety devices into vehicles, as well as informing 
drivers of the best and safe driving practices. The second initiative consists 
of limiting the gravity of the accidents by integrating passive safety devices, 
such as airbags, within vehicles and punishment for the lack of wearing a 
seat belt. Finally, the third initiative consists of taking care of the wounded 
persons as soon as possible, respecting the “golden hour” rule. This rule 
prescribes that the earlier the wounded persons are taken care of, the less 
grave the after effects will be. The available data show that the delay of 
intervention can be improved by 50% in rural areas and by 40% in urban 
areas, with a net gain of 10 min. 
In order to reach this goal, emergency services must be informed as soon 
as possible after the accident occurs. To intervene with the adapted means, 
emergency services have to know the location of the accident, the number 
and the type of the vehicles concerned, even the number and the 
 
 
                         
Chapter written by Jean-René RUAULT. 

42     Risk Management in Life-Critical Systems 
gravity of the wounded persons. Coach crashes often show particular  
difficulties, when the accidents take place in border areas [BEA 08], concern 
foreign-speaking tourists [BEA 08, FED 12, RT 13], or take place in an 
environment difficult to access, such as a tunnel or a bridge [RT 13,  
NTS 04]. 
To do that, it is necessary to integrate a set of systems. These systems 
must be widely deployed in motor vehicles, in telecommunication 
infrastructures, within the emergency services. This initiative must be 
promoted to the political level and means of regulation are due to be setup. 
This is the object of the European eCall project. 
We begin by presenting the stakes of the golden hour and the fast 
intervention of the emergency services. Then, we analyze the system of the 
systems engineering process implemented to reach at the goal. We conclude 
this chapter by highlighting the future orientations, allowing us to manage 
the emergency situations that evolve ceaselessly. 
3.2. The Golden hour: toward a resilient life-critical system of systems 
3.2.1. Accident technical reports: getting experience feedback 
Accident technical reports detail the causes of accidents and suggest 
corrective propositions. In many cases, these reports deal with coach crashes, 
mostly with foreign children or tourists. 
The first report deals with an accident on the border between Belgium 
and France. On Wednesday 8 August 2007, at about 05:00 am and in heavy 
rain, a coach registered in Poland, traveling on the A16 motorway from 
Belgium to France and carrying 49 people entered the slip road to the “Les 
Moëres” rest area and turned over onto a safety barrier on entering the car 
park. The final toll of the accident was three fatalities and 30 injured, 
including eight in hospital [BEA 08]. It is reported that the lack of initiative 
by the driver in informing the emergency services and the difficulty of 
informing the emergency services on the border delayed the arrival and 
deployment of the emergency services. Among other recommendations, the 
report explains the essential part that drivers play in the event of an accident  
 
 

The Golden Hour Challenge     43 
in altering and informing the emergency services, the necessity to improve 
the coordination of emergency services on the border. 
The second accident deals with a double-decker coach coming from 
Germany which accidentally left the road on the A6 motorway at the North 
entrance of the city of Lyon, France, on 17 May 2003. There were 28 fatally 
injured and 46 seriously injured [BEA 04]. 
A bus, carrying 52 passengers, crashed in a tunnel in Sierre in the canton 
of Valais, Switzerland, on 13 March 2012 while returning to Belgium from 
the Val d’Anniers skiing area. The bus drove into a concrete wall at the end 
of a turnout lane in the tunnel. There were 28 people killed in the crash, 
including drivers, all four teachers, and 22 of the 46 children. The other 24 
pupils were injured, including three who were hospitalized with severe brain 
and chest injuries [FED 12]. This situation is quite common. For instance, 
there have been 12 accidents such as this one in Switzerland, over 30 years 
[RTS 12]. 
Five people died and 19 were injured, five in critical condition, after a 
bus carrying 42 Russian people, mostly teenagers, crashed outside Antwerp, 
Belgium. A Polish registered bus carrying Russian teenagers from 
Volgograd to Paris lost control, went over an elevated section of the E34 
motorway and landed on its side at least 6 m below the road [RT 13]. The 
survivors are being treated for their injuries in local hospitals. The Russian 
Emergency Ministry has sent two planes to help the victims. One of these 
planes contains a mobile medical module onboard and a team of doctors. 
In many cases, coach crashes deal with foreign tourists, such Belgians in 
Switzerland, Russians in Belgium, Poles and Germans in France, generating 
misunderstanding risks. Moreover, first responders have to share common 
cross-border assessment of a situation, since many accidents could concern 
two or three countries. Coach crashes are not the only kind of accidents. 
There are many other kinds of accidents, such as train derailment, with huge 
impacts. There was one dead and 33 injured in the Wetteren train derailment; 
this train transported toxic chemicals [BBC 13, HUF 13]. 
3.2.2. Resilience: reducing the damage 
The challenges linked to resilience include the management of that which 
is uncertain or unplanned, accidents, the transition between more or less 

44     Risk Management in Life-Critical Systems 
catastrophic circumstances while avoiding a true catastrophe, and the return 
to a more normal operational status [RUA 12a]. Here, resilience consists of 
reducing damage and avoiding secondary accidents. 
Luzeaux [LUZ 11] differentiates four main resilience functions: (1) 
avoidance (capacity for anticipation), (2) resistance (capacity for 
absorption), (3) adaptation (capacity for reconfiguration) and (4) recovery 
(capacity for restoration). Among these four functions, the second, to resist 
accidents, deals with the capacity for absorption and reduction of damage. 
The function “to resist” consists of the capacity of absorption. This function 
can be broken up into (1) working out and positioning material, functional 
and procedural barriers, (2) working and diffusing the procedures of 
resistance, (3) giving means of improvizing in order to resist and (4) working 
out and maintaining reserves. Our proposition develops the third 
subfunction. 
3.2.3. The Golden hour: managing serious accidents as soon as possible 
Since there are too many road fatalities and severe injuries on roads, the 
goal is to reduce the time needed for emergency services to arrive at  
the place of the accident, and therefore to reduce the risk of deaths and the 
severity of the injuries. 
The eCall system is designed and deployed in order to satisfy this goal 
that is to reach the golden hour. “The aim of the in-vehicle eCall system 
(based on the 112 call platform) is to ensure that the emergency services are 
alerted automatically to serious road accidents”, says the resolution. This 
should save lives and reduce the severity of injuries as qualified and 
equipped assistance would get to the scene of the accident earlier, according 
to the “golden hour” principle [ERT 12a, EC 12c]. 
A German study [EC 11a] showed almost a 50% rescue time 
improvement in rural areas, with a net gain of around 10 min. Rescue time in 
urban areas would be improved by 40%. That leads to a reduction of 
fatalities estimated to be between 2% and 10%, and a reduction of severity 
of injuries between 2% and 15%. The system eCall has this potential  
[EC 11a], which is expressed in Table 3.1. 
 

The Golden Hour Challenge     45 
Problems 
Drivers 
High number of road fatalities and severe 
injuries 
Long response time by emergency services 
(inter alia) 
Delays in alerting emergency services  
Manual notification by the vehicle occupants 
or third parties 
Delays in reaching the accident scene  
Emergency services can rely only on the 
indications provided by phone, accurate 
location of the incident is difficult to 
establish 
Long rescue time at the accident scene  
Long rescue time at the accident scene 
Emergency services are not aware of the 
vehicle type and other essential details on the 
accident 
Secondary accidents and traffic congestions 
Traffic management centers/road operators 
not promptly notified 
Table 3.1. Major problems and respective drivers that eCall can improve [EC 11a] 
In the case of a severe crash, a vehicle that is equipped with the eCall 
system will automatically trigger an emergency call, regardless of whether 
passengers can speak or not. A set of data with relevant information about 
the incident is sent automatically, which includes, inter alia, the exact 
location of the crash site. ECall can also be activated manually. ECall creates 
a voice/audio link to the most appropriate public safety answering point 
(PSAP) and sends it a data message (minimum set of data – MSD). This set 
of data includes the minimum information needed to handle the emergency 
situation, such as a time stamp, accurate location and direction of driving, 
vehicle identification and other information essential to the rescue services 
[EC 11a]. 
ECall would facilitate the saving of public resources (e.g. social security 
and public health) by reducing the severity of injuries, rehabilitation needs 
and hospital admission/permanence [EC 11a]. 
The major benefit of eCall will be the reduction in the number of 
fatalities and the mitigation of the severity of injuries caused by road 
accidents due to faster arrival of the emergency services to the accident 
scene. The shorter rescue time – faster arrival of rescue teams, police and 
towing firms – enables the accident scene to be cleared more quickly; eCall 
will thus reduce the congestion time and contribute to the efficiency of road 

46     Risk Management in Life-Critical Systems 
transportation in Europe with a reduction in external costs. For the public 
authorities, the benefits of eCall are mostly demonstrated to be the reduction 
of (1) the medical consequences of a crash, (2) the risk of further accidents 
on the scene and (3) the impact of an accident on the traffic [EC 11a]. 
 
Figure 3.1. eCall: the crashed car calls 112! [EC 13e] 
According to estimates “the eCall system would save up to 2,500 lives a 
year and reduce the severity of injuries by 10% to 15%”, the resolution states 
[EP 12a]. 
The benefits identified through the impact assessment and several studies, 
including national ones, include [EC 13h]:  
– reduction of fatalities (with all vehicles eCall-equipped, between 1 and 
10% depending on country population density, road and emergency response 
infrastructure); 
– reduction of seriousness of the injuries (between 2 and 15%); 
– reduction of congestion costs caused by traffic accidents; 
– facilitation of rescue services and increased security of rescue team; 
– reduced SOS roadside infrastructure, as each road user would be able to 
trigger an emergency call from his/her vehicle. 

The Golden Hour Challenge     47 
Finally, eCall is very efficient in remote areas, during nighttime and 
secondary roads with very little traffic [EC 11a]. 
In this context, the mandate to establish security standards in Europe 
[ECS 13] includes an area dealing with crisis management and civil 
protection. Crisis management implies coordinating many people to share 
situation awareness and assessment, to make, implement and control 
coordinated actions and to adapt the responses to changing situations. This 
capability is based upon interoperability. So, standardization activity 
concerns 
interoperability, 
i.e. 
semantic, 
planning, 
resiliency 
and 
organizational interoperability issues, as well as tactical, operative and 
strategic levels. 
The mandate report [ECS 13] establishes that use of navigation enables 
devices to alert emergency services and assist first responders with a 
location. These devices have to be based upon geolocalization standards. 
This report includes recommendation about distress beacon applications for 
smart phones that may be activated by victims, as well as automatic call 
systems, such as eCall. The issue is to standardize acquisition information 
from victims or the public in order to improve situation awareness, to assess 
the level of injuries of victims and to adapt deployed means to the situation. 
It may include developing a common “victim ticket” to be filled in by 
victims using emergency smart phone applications, or other relevant devices. 
The report takes into account prehospital patient management, in particular 
international situations that involve developing a standardized electronic 
triage system to improve the capability to evacuate the patients to the right 
hospitals, treatment of their injuries and the hospitals’ available specialties. 
3.2.4. The challenge 
The challenge in order to achieve the golden hour consists of: 
– being informed as soon as possible that an accident has happened; 
– assessing the gravity of the accident (number of cars that are implied in 
the accident, estimated number of injuries, etc.) and the probable secondary 
accidents that would happen, such as in the case of chemistry truck accident; 
– intervening with relevant and fit emergency resources; 
– managing traffic and other disrupted services; 

48     Risk Management in Life-Critical Systems 
– preparing hospitals and other services to take in wounded patients. 
The next part of this chapter explains how we engineer the system of 
systems that meets this challenge. 
3.3. Systems of systems engineering 
3.3.1. The systems of systems engineering principles 
The concept of a system of systems was defined and formalized by Mark 
Maier in 1996. He characterized a system of systems as follows  
[LUZ 10]: 
– constituent systems have an operational independence; 
– constituent systems have a managerial independence; 
– the definition and configuration of the global system are evolutionary in 
nature; 
– emergent behaviors of the global system exist; 
– constituent systems are geographically distributed. 
Luzeaux [LUZ 10] defines a “system of systems as an assembly of 
systems which can potentially be acquired and/or used independently, and 
whose global value chain the designer, buyer and/or user is looking to 
maximize, at a given time and for a set of conceivable assemblies”. 
In such a way, engineering a system of systems expresses the following 
specificities [RUA 11]: 
– the plurality and diversity of partners, public and private sectors, 
presenting 
different 
economic 
models 
and 
showing 
managerial 
independence; 
– the integration of isolated project approaches that are independent of 
each other within an integrated system of systems approach. This implies 
that different projects share the same reference point in terms of project 
management, methods and tools that are at least compatible, if not 
completely interoperable; 
– a system of systems architecture based on open systems and 
interoperability [BNA 10]; 

The Golden Hour Challenge     49 
– a complementarity of top-down and bottom-up approaches. Bottom-up 
approaches require us to update design documents linked to the existing 
system and carry out a certain amount of retro-design; 
– an interdisciplinary approach, which is not itself new as systems 
engineering is multidisciplinary by nature in the engineering techniques it 
uses, but which becomes essential in the context of systems of systems, 
including disciplines outside of the field of engineering such as law, 
marketing, economics, human factors, etc.; 
– the implication and effective contribution of operators is already 
recommended in systems engineering, and becomes essential in the case of 
systems of systems; 
– the creation and management of interoperability standards; 
– the management of evolutions and configurations; 
– the certification of systems in relation to an interoperability standard; 
– the coherency of the system of systems architecture; 
– the current limits of architecture frameworks to purely technical 
dimensions, which must be added to and enriched to take non-technical 
dimensions into consideration (economic model, communications model, 
etc.); 
– last, but not least, an iterative and incremental approach, known in 
systems engineering. This iterative and incremental approach takes account 
of feedback from the first iterations to fuel the development of the following 
iterations. 
Step-by-step, the standard on system lifecycle processes (ISO/IEC/IEEE 
CD1 15288:2008) evolves. The current state of the revision of the standard 
[ISO 13] includes a new technical process: that is the business or mission 
analysis process. It makes it possible to define problems and opportunities in 
the organizational strategy. A business mission may be formalized by a 
concept of operations that is modeled on a scenario basis. It encompasses the 
political, economic, social, technological, environmental and legal 
(PESTEL) factors. Moreover, this standard includes another technical 
process with a great value, the architecture definition process. “The 
architecture associated with a System of Interest is an abstract representation 
of the solution. Concepts and properties of the architecture are described 
with multiple views that address related concerns or aspects (such as 

50     Risk Management in Life-Critical Systems 
functional/transformations, behavioural, temporal, physical, dependability, 
communications” [ISO 13]. We use the architecture framework views to 
express relevant architectural models, such as operational views and system 
views [RUA 11]. 
The persona is not the representation of an actual person but that of an 
archetypical, fictitious, imaginary user of the future system. The aim is to 
incarnate the users of the future system, since the persona synthesizes all 
users’ features that dimension and structure the Human–Computer 
Interaction (HCI) design, and stimulates the ideas of the designers [BRA 11, 
PRU 06]. More precisely, it is the archetype of a class of users. Insofar as 
several classes of users have different stakes with the system, there are as 
many different personas as there are classes of users. The various 
characteristics that can be integrated in a persona according to [BRA 11, 
PRU 06, IDO 12] and [RUA 12b] are personal data/identity; physiological 
data; personality; sociality; economic and financial data; lifestyle; formation, 
competences and skills; and context of the activity. 
Now, we have the main keys for engineering the life-critical system of 
systems. 
3.3.2. Applying systems of systems engineering to life-critical systems 
We now apply the principles of an engineering system to the case of an 
emergency situation. The details of these principles and methods are 
specified in the fictitious case study [RUA 11]. 
To limit the damage and the gravity of road casualties, the European 
authorities have decided to implement the eCall service [EC 13g, EC 13h]. 
This case study applies the same process of decision-making in the fictitious 
region of Tairétalet [RUA 11]. 
To reach the assigned goals, it is necessary to call on all the stakeholders 
who contribute to one role or another in the design, realization, evaluation 
and deployment of the eCall service. There are automobile builders, 
equipment manufacturers who produce the device integrated into vehicles, 
operators of the systems of telecommunications, the emergency services, etc. 
Each of these stakeholders, whether it is a buyer or a supplier of a system 
implementing or operating the eCall service, contributes to its definition, its 
deployment. Such systems are passenger cars and light commercial vehicles, 

The Golden Hour Challenge     51 
in-vehicle systems, telecommunications networks and emergency services. 
These systems are independent for operation aspects as well as for 
management aspects [ERT 12b]. The Tairétalet case study [RUA 11] 
expresses this diversity of stakeholders with different status, aims and 
business models, complying with the systems of systems specificities. 
The first task is the allocation of the functions of the eCall service on the 
systems that are vehicles, call centers and emergency services. After these 
systems are realized, it is necessary to integrate them to supply the service 
from start to finish. 
The European authorities define the generic operational scenario that the 
eCall service has to realize [ERT 12b]. They propose three different 
architectures from call centers. The Tairétalet case study [RUA 11] 
elaborates an operation scenario and proposes a centralized architecture as 
far as the call center of the region already exists and is common to the whole 
region. Figure 3.2 explains the data flows between the systems using an N² 
matrix diagram. In this case study, two regions, Tairétalet and Orion, 
cooperate in order to provide emergency services and resources. The 
Tairétalet case study [RUA 11] proposes an accident ticket that contains 
information on accidents and wounded people, used across stakeholders, 
from emergency services to hospitals, in order to improve coordination. 
 
Figure 3.2. N² matrix of pairings of different systems within the  
system of systems [RUA 11] 

52     Risk Management in Life-Critical Systems 
The eCall service has to ensure the interoperability and the cross-border 
continuity of service through Europe [EC 13i]. Standards of interface have 
been developed and published. For example, the eCall Minimum Set of Data 
standard (EN 15722) specifies the meaning of the information and the 
structure of the data of messages sent by the in-vehicle emergency call 
systems to call centers [ERT 12b], and the Pan-European eCall Operating 
requirements standard (EN 16072). These standards must be applied by the 
stakeholders who design and realize the systems, for example, these  
in-vehicle emergency call systems. The EN 15722 standard [ECS 11b] 
prescribes the information, such as the location and direction of the  
vehicle, the type of the vehicle, the time stamp, the number of passengers 
and the type of energy used by the vehicle of the minimum set of data.  
The Tairétalet case study [RUA 11] shows how to develop these standards 
and to implement them, which is compliant with systems of systems 
specificities. The European authorities define the features that the in-vehicle 
emergency call systems have to present by specifying the major  
components which they have to contain (electronic control unit (ECU), 
positioning 
system, 
communication 
system 
and 
human–machine  
interaction  (HMI)) [ERT 12b]. The EN 16072 standard [ECS 11a] dedicated 
to the operational requirements prescribes the rules of routing that the 
systems of telecommunications have to apply by identifying the eCall flag. It 
prescribes post-crash performance of in-vehicle equipment, the automatic 
eCall triggering strategy as well as the manual eCall triggering strategy. It 
prescribes the time constraint of eCall transmission too, as well as the 
performance of the call center (Public Safety Answering Point (PSAP)). The 
Tairétalet case study [RUA 11] shows the functional of such an in-vehicle 
emergency call system that does not contain HMI, since it is fully automatic 
(see Figure 3.3). 
The European authorities prescribe specific obligations of manufacturers 
[EC 13h], such as the demonstration that new types of vehicle are tested and 
compliant with the eCall requirements. They prescribe rules on privacy and 
data protection [EC 13h]. From the same perspective, the Tairétalet case 
study [RUA 11] explains that how stakeholders collaborate in order to assure 
a relevant and sustainable economic solution. 
 

The Golden Hour Challenge     53 
 
Figure 3.3. Functional model of the accident detection system architecture [RUA 11] 
The European authorities legislate by ruling on the concerned vehicles, 
the date of application, the geographic coverage and the exceptions, for 
example, the small series of vehicles [EC 13g, EC 13h]. The Tairétalet case 
study [RUA 11] shows how to develop and implement a plan of deployment, 
by specifying the incentive rules. As a supplement to the regulations which 
state that all the passenger cars and light commercial vehicles (M1 and N1 
categories) produced from 1st October 2015 must have the eCall service, the 
European authorities expect that aftermarket equipment will be integrated on 
vehicles 
in 
circulation 
[ERT 
12b]. 
The 
Tairétalet 
case 
study  
[RUA 11] shows different incentive and coercive regulation means in order 
to favor deployment and explains the way to integrate the aftermarket 
equipment. 
The European authorities define the economic model, namely the free-of-
charge access of the eCall service for the European motorists and prepare the 
way for value-added services [ERT 12b, EC 13g, EC 13h]. From the same 
perspective, the Tairétalet case study [RUA 11] explains how stakeholders 
collaborate in order to assure a relevant and sustainable economic solution. 
The application showed that the concepts and the methods used, leaving 
expressed objectives, declined into operational scenario and requirements, 

54     Risk Management in Life-Critical Systems 
along the PESTEL dimensions, are the keystone of the engineering of a 
system of systems. 
3.4. Next steps forward 
The eCall system includes a short message presenting the minimum set of 
information, and the capacity to call the occupants of the hit vehicle in order 
to complete this information, in particular to know the gravity of the 
situation. If the occupants of the vehicle cannot answer or if they speak a 
different language, this function of call loses a part of its relevance. 
To solve this problem and allow the emergency services to better assess 
the gravity of the situation, solutions can be later deployed. The first solution 
consists of increasing the amount of information automatically collected by 
the eCall system, such as the states of the security devices, the state of the 
windscreen wiper, whether it has stopped or is working, and the speed of the 
vehicle during the collision. Another solution consists of collecting 
information on the state of the passengers. It can use smart clothes that get 
information from the passengers and send it to the eCall systems in order to 
complete the minimum set of data. That allows us to mitigate information 
from different sources closed to the accident location in order to enhance 
situation awareness. This involves common and standardized format and 
semantic of exchanged data. The eCall system can be applied to other 
categories of vehicles, in particular coaches. Indeed, as shown by 
investigation reports, the evaluation of the gravity of the situation is critical 
in the context of coach crashes. 
Finally, the accidental situation does not stop evolving. New behaviors 
that were not anticipated also emerge. The gravity of the accident can 
traumatize rescuers, as was the case in Sierre, requiring a psychological 
assistance unit. In this context, the means of the emergency services must be 
flexible to adapt itself to the evolutions of the situation. The European 
authorities are going to develop standards to this capacity of permanent 
evolution offer to the means of help [ECS 13]. 
3.5. Bibliography 
[BBC 13] BBC, Train crash: toxic chemicals on fire near Belgium’s Ghent, 2013. 
Available at http://www.bbc.co.uk/news/world-europe-22415757. 

The Golden Hour Challenge     55 
[BEA 04] BUREAU D’ENQUÊTES SUR LES ACCIDENTS DE TRANSPORT TERRESTRE 
(BEA-TT), Rapport d’enquête technique sur l’accident d’autocar survenu à 
Dardilly 
le 
17 
mai 
2003. 
Available 
at 
http://www.bea-
tt.equipement.gouv.fr/dardilly-r16.html, 2004. 
[BEA 08] BUREAU D’ENQUÊTES SUR LES ACCIDENTS DE TRANSPORT TERRESTRE 
(BEA-TT), Rapport d’enquête technique sur l’accident d’autocar survenu le 8 
août 2007 sur l’autoroute A16 à Ghyvelde (59), 2008. Available at 
http://www.bea-tt.equipement.gouv.fr/ghyvelde-r83.html. 
[BEA 09] BUREAU D’ENQUÊTES SUR LES ACCIDENTS DE TRANSPORT TERRESTRE 
(BEA-TT), Rapport d’enquête technique sur l’accident d’autocar survenu le 23 
mai 2008 sur l’autoroute A10 à Suèvres (41), 2009. Available at http://www.bea-
tt.equipement.gouv.fr/suevres-r104.html. 
[BNA 10] BNAE, RG 000 120, BNAE, Program Management – General 
Recommendation for the Acquisition and Supply of Open Systems, 2010. 
[BRA 11] BRANGIER E., BORNET C., “Persona: a method to produce representations 
focused on consumers’ needs”, in KARWOWSKI W., SOARES M., STANTON N. 
(eds.), Human Factors and Ergonomics in Consumer Product Design, Taylor & 
Francis, pp. 38–61, 2011. 
[EC 11a] EUROPEAN COMMISSION, Commission staff working paper, impact 
assessment accompanying the document commission recommendation on 
support for an EU-wide eCall service in electronic communication networks for 
the transmission of in-vehicle emergency calls based on 112 (‘eCalls’), 2011. 
[EC 11b] EUROPEAN COMMISSION, eCall – saving lives through in-vehicle 
communication technology, October 2011. 
[EC 12] EUROPEAN COMMISSION, 2012 promotional campaign, published on Digital 
Agenda for Europe (https://ec.europa.eu/digital-agenda), 2012. 
[EC 13a] EUROPEAN COMMISSION, Digital agenda: tour operators and travel agents 
join ‘112’ emergency number campaign, February 2013. 
[EC 13b] EUROPEAN COMMISSION, Implementation of the European emergency 
number 112 – results of the sixth data-gathering round, March 2013. 
[EC 13c] EUROPEAN COMMISSION, 2013 promotional campaign, published on 
Digital Agenda for Europe (https://ec.europa.eu/digital-agenda), 2013. 
[EC 13d] EUROPEAN COMMISSION, Road Safety Newsletter, Newsletter No. 11, 
March 2013. Available at http://ec.europa.eu/transport/road_safety/publications/ 
index_en.htm. 
 

56     Risk Management in Life-Critical Systems 
[EC 13e] EUROPEAN COMMISSION, eCall: the crashed car calls 112!, 2013. Available 
at http://ec.europa.eu/information_society/activities/esafety/ecall/index_en.htm. 
[EC 13f] EUROPEAN COMMISSION, 112 eCall – frequently asked questions, reference 
MEMO-13-547, 2013. 
[EC 13g] EUROPEAN COMMISSION, Proposal for a decision of the European 
Parliament and of the Council on the deployment of the interoperable EU-wide 
eCall, reference com-2013-315, 2013. 
[EC 13h] EUROPEAN COMMISSION, Proposal for a regulation of the European 
Parliament and of the Council concerning type-approval requirements for the 
deployment of the eCall in-vehicle system and amending Directive 2007/46/EC, 
reference com-2013–316, 2013. 
[EC 13i] EUROPEAN COMMISSION, eCall: automated emergency call for road 
accidents mandatory in cars from 2015, 2013. Available at http://ec.europa.eu/ 
commission_2010-2014/kallas/headlines/news/2013/06/ecall_en.htm. 
[ECS 11a] European Committee for Standardization, Pan-European eCall operating 
requirements, EN 16072, 2011. 
[ECS 11b] European Committee for Standardization, ECall minimum set of data, 
EN 15722, 2011. 
[ECS 13] European Committee for Standardization, Mandate M/487 to establish 
security standards, Draft Report Phase 2, 13 May 2013. 
[EP 12a] EUROPEAN PARLIAMENT, eCall Year in Review 2012, Reference: 
20120614IPR46884, 2012. 
[EP 12b] EUROPEAN PARLIAMENT, on eCall: a new 112 service for citizens, 
Reference: A7-0205/2012, 2012. 
[EP 12c] EUROPEAN PARLIAMENT, Life-saving emergency eCall system should be 
mandatory, say MEPs, Press release reference: 20120703IPR48185, 2012. 
[ERT 12a] ERTICO, MEPs say mandatory emergency eCall system in cars will save 
lives, 
2012. 
Available 
at 
http://www.ertico.com/road-safety-meps-say-
mandatory-emergency-ecall-system-in-cars-will-save-lives/. 
[ERT 12b] ERTICO, Pan-European eCall implementation guidelines, 2012. Available 
at http://www.icarsupport.eu/assets/Uploads/Documents/eCall/eCall-Implement-
Guidelines.pdf. 
 

The Golden Hour Challenge     57 
[FED 12] FEDERAL ROADS OFFICE (FEDRO), Department of the Environment, 
Transport, Energy and Communications, Fatal coach crash in the “Sierre” tunnel 
on 
13 
March 
2012, 
2012. 
Available 
at 
http://www.astra.admin.ch/ 
themen/nationalstrassen/00530/04770/index.html?lang=en. 
[HUF 13] THE HUFFINGTON POST CANADA, Belgium train fire, explosions kill 1 
near Ghent, 2013. Available at http://www.huffingtonpost.ca/2013/05/06/ 
belgium-train-fire-death-video_n_3223159.html. 
[IDO 12] IDOUGHI D., SEFFAH A., KOLSKI C., “Adding user experience into the 
interactive service design loop: a persona-based approach”, Behaviour & 
Information Technology, vol. 31, pp. 287–303, 2012. 
[ISO 13] ISO, Systems and software engineering – system lifecycle processes 
(revision of IEEE Std 15288-2008), ISO/IEC/IEEE 15288 CD.1, 2013. 
[LUZ 10] LUZEAUX D., “Systems of systems: from concept to actual development”, 
in LUZEAUX D., RUAULT J.-R. (eds.), Systems of Systems, ISTE, London, and 
John Wiley & Sons, New York, 2010. 
[LUZ 11] LUZEAUX D., “Engineering large-scale complex systems”, in LUZEAUX 
D., RUAULT J.-R., WIPPLER J.-L. (eds.), Complex Systems and Systems of 
Systems Engineering, ISTE, London, and John Wiley & Sons, New York, pp. 3–
83, 2011. 
[NTS 04] National Transportation Safety Board (NTSB), Highway Accident Report, 
School Bus Run-off-Bridge Accident, Omaha, Nebraska, October 13, 2001, 
NTSB/HAR-04/01, 
2004. 
Available 
at 
http://www.ntsb.gov/news/events/ 
2004/omaha_ne/index.html. 
[PRU 06] PRUITT J., ADLIN T., The Persona Lifecycle: Keeping People in Mind 
Throughout Product Design, 1st ed., Morgan Kaufmann, 2006. 
[RT 13] RUSSIA TODAY, 5 dead, 19 injured as bus carrying Russian teenagers 
crashes in Belgium, 14 April 2013. Available at http://rt.com/news/bus-crash-
poland-children-832/. 
[RTS 12] RADIO TÉLÉVISION SUISSE, Une douzaine de drames en Suisse depuis 
trente ans, 2012. Available at http://www.rts.ch/info/suisse/3854082-une-
douzaine-de-drames-en-suisse-depuis-trente-ans.html. 
[RUA 10] RUAULT J.-R., “The human factor within the context of systems of 
systems”, in LUZEAUX D., RUAULT J.-R. (eds.), Systems of Systems, ISTE, 
London, and John Wiley & Sons, New York, pp. 149–206, February 2010. 
 
 

58     Risk Management in Life-Critical Systems 
[RUA 11] RUAULT J.-R., “Management of emergency situations: architecture and 
engineering of systems-of-systems”, in LUZEAUX D., RUAULT J.-R.,  
WIPPLER J.-L. (eds.), Complex Systems and Systems of Systems Engineering, 
ISTE, London, and John Wiley & Sons, New York, pp. 85–203, September 
2011. 
[RUA 12a] RUAULT J.-R., VANDERHAEGEN F., LUZEAUX D., “Sociotechnical 
systems resilience”, INCOSE 2012, Rome, July 2012. 
[RUA 12b] RUAULT J.-R., KOLSKI C., VANDERHAEGEN F., “Persona pour la 
conception de systèmes complexes résilients”, Proc. Ergo’IHM 2012, Biarritz, 
2012. 
 

4 
Situated Risk Visualization  
in Crisis Management 
4.1. Introduction  
During the 20th Century, the industrial revolution generated technical 
systems that overtook physical human labor, and became increasingly 
complex both in scale and in harming potential. The postindustrial revolution 
emerging during the last decades of the 20th Century consisted of computers 
overtaking mental human labor, leading to tight interconnectivity at a global 
scale. It has been agreed for a while that complexity and related risks no 
longer reside in individual components or systems but in their relationships. 
Consequently, modern approaches to crisis management, emergency 
management and business continuity (BC) evolved in the 1980s and are still 
adapting to provide solutions to new forms of adverse events. Supporting 
solutions for such approaches also evolved, from strong tendencies to full 
automation in the 1980s, toward Human Systems Integration focusing on 
interconnected social structures [BOO 90, BOO 03, BOY 13]. In their 
decision-making process, crisis and risk managers can currently benefit from 
information virtualization and communication technologies that provide 
means of “being there” inside the situation in terms of information flows and 
their visualization. 
This chapter provides an overview of the state-of-the-art approaches to 
critical operations and proposes a solution based on the integration of several 
                         
Chapter written by Lucas STÉPHANE. 

60     Risk Management in Life-Critical Systems 
visual concepts within a single interactive three-dimensional (3D) scene 
intended to support situated visualization of risk in crisis situations. 
Approaches to critical operations are presented in section 4.2. Section 4.3 
synthesizes risk approaches. Section 4.4 proposes the 3D integrated scene 
and presents user-test results and feedback. 
4.2. Crisis management, emergency management and business 
continuity 
4.2.1. Crisis management 
During the 1970s, the precursor of crisis management was the 
identification of Major Technological Risk [LAG 81]. Indeed, analyzing a 
series of industrial accidents expanding in scale over a short period of time 
(i.e. 1976–1979) in various Western countries, the main conclusion was that 
novel approaches to not only sociotechnical systems but also sociopolitical 
systems (meaning that solutions to emerging complex systems are to be 
found at the highest political management levels – not only at technical 
levels) are crucial. Along the same line of thought, the concept of “normal 
accidents” was introduced mid-1980s [PER 84]. The concept of tight-
coupling was proposed. From a social perspective, high-risk technology’s 
past and future possible mishaps in a variety of life-critical domains were 
categorized into four categories, in terms of spatiotemporal distance with 
these adverse events. Furthermore, mutations due to computerization and 
locus of control were identified. While these efforts were mostly related to 
industrial sociotechnical systems, the initial point of modern crisis 
management may be traced to the Tylenol crisis in 1982 [FIN 86, MIT 87, 
ZIA 01, GIL 08]. 
The current ISO 22300:2012 crisis definition is: “a situation with a high 
level of uncertainty that disrupts the core activities and/or credibility of an 
organization and requires urgent action” [BCI 13]. The successful Tylenol 
crisis management by Johnson & Johnson led to several analyses for 
building the foundations of modern crisis management. For example, crisis 
typologies were proposed [MIT 87]. Furthermore, the first main 
development was the identification of crisis stages [FIN 86, MIT 87]. These 
stages are: detection and prevention (precrisis), containment (during  
the crisis), recovery, resolution and learning (postcrisis) [FIN 86, MIT 87]. 

Situated Risk Visualization in Crisis Management     61 
The second main development was the emphasis on crisis communication, 
both intra-organization and extra-organization (i.e. extended organization 
network such as partners and other external supporting organizations, as well 
as the public) [FIN 86, MIT, 87, COO 07]. Since the 1980s, crisis 
management was considered from various perspectives in various domains. 
Efforts toward integration were carried out and a synthesis is provided in 
Table 4.1 [PER 98, GIL 08]. Beyond descriptive and categorization efforts, 
crisis management was also expressed as a process, with outcomes being a 
mix of both successes and failures [PER 98]. The crisis management process 
is instanced with crisis management plans (CMPs) that are continuously and 
systematically improved through trainings and drills [COO 07]. One of the 
essential features of CMP is its support for enactment in worst-case 
scenarios that enables practitioners to identify and fix vulnerabilities  
[WEI 88, COO 07]. 
4.2.2. Emergency management 
ISO 22320:2011 defines emergency management as the “overall 
approach preventing emergencies and managing those that occur [...] In 
general, emergency management utilizes a risk-management approach to 
prevention, preparedness, response and recovery before, during and after 
potentially destabilizing and/or disruptive events” [BCI 13]. While crisis 
management is pretty recent, emergency management is older with 
structures defined at organizational, local, regional and national and 
international levels. Emergency management depends on the adverse events 
in the country and/or region where it is implemented. For example, the 
United States has a large panel of natural and also man-made adverse events. 
Thus, emergency management evolved accordingly and was adapted for 
these specific event types. Since 2003, national preparedness has become a 
priority. Involving both the Department of Homeland Security (DHS) and 
the 
Federal 
Emergency 
Management 
Agency 
(FEMA), 
National 
Preparedness Guidelines (NPG) are provided through capabilities-based 
planning. These “living” guidelines link all levels of government, private 
sector and non-governmental organizations through unity of effort in 
command and coordination based on interorganizational plans. National 
Planning Scenarios (NPS) identify and define adverse events. The Universal 
Task List (UTL) identifies and defines required tasks [DHS 05]. The Target 
Capabilities List (TCL) identifies and defines requirements for achieving the 
tasks [DHS 05]. Furthermore, relationships between tasks and capabilities on 

62     Risk Management in Life-Critical Systems 
one hand, and Emergency Support Functions (ESF) defined in the National 
Response Framework (NRF) on the other hand are clearly established. 
Scenarios and their combinations enable us to span all sorts of adverse 
events and select, implement and test relevant capabilities, universal tasks 
and support functions in multi-event situations [WEB 13]. Similar to crisis 
management, exercises and tests are performed through rehearsals and drills. 
Moreover, emergency management goals are clearly defined for all levels, 
from local to regional and national levels. Exercises and tests are prepared 
over long periods of time, and they may involve several thousands of people. 
Software solutions and real-world infrastructures are tested altogether, e.g. 
[CUS 11]. 
 
Psychological 
approach 
Sociopolitical 
approach 
Technological 
structural approach 
Causes 
Cognitive or 
behavioral limitations 
or errors in individuals 
or groups 
Breakdown of shared 
understandings and 
social structures 
 
Tightly coupled, 
densely interactive 
technological and 
managerial 
structures that foster 
complex and 
unpredictable 
interactions 
Consequences 
Shattered assumptions 
about organization 
and/or its members, 
feeling threatened or 
otherwise insecure and 
victimization of 
affected individuals 
Dissolution of shared 
values, beliefs, 
structures and roles 
 
More or less 
widespread disaster 
and destruction, 
including self- 
destruction 
 
Cautionary 
measures 
Understanding 
vulnerability and 
potential harm 
Flexibility in norms 
and behaviors that 
guide interaction, 
mutual respect and 
wisdom 
Avoidance of risky 
and/or poorly 
understood 
technology, or “fail- 
safe” structures 
designed to limit 
risks 
Coping 
techniques 
Readjusting beliefs, 
assumptions, 
behaviors and 
emotions 
Reconstruction of 
meanings and 
collective adaptation 
 
Emergency 
intervention to assist 
victims and repair 
structural damage 
 
Table 4.1. Approaches to crisis management 

Situated Risk Visualization in Crisis Management     63 
4.2.3. Business continuity and disaster recovery 
The ISO 22301:2012 [BCI 13] defines BC as: “The capability of the 
organization to continue delivery of products or services at acceptable 
predefined levels following a disruptive incident”. The same standard defines 
Business Continuity Management as: “A holistic management process that 
identifies potential threats to an organization and the impacts to business 
operations those threats, if realized, might cause, and which provides a 
framework for building organizational resilience with the capability of an 
effective response that safeguards the interests of its key stakeholders, 
reputation, brand and value-creating activities”. Thus, BC focuses on how 
operations can be continued as an organizational capability in case of 
adverse events. Disaster Recovery (DR) Planning (DRP) was introduced in 
the 1970s for dealing specifically with emerging mainframes and data 
centers that became critical for organizations. The initial focus on 
information technology infrastructure evolved both for the Disaster 
Recovery Institute (DRI – created in the US in 1988) and for the Business 
Continuity Institute (BCI – created in the UK in 1994) toward continuity in 
all kinds of critical operations, both in operational technology [CUR 07] and 
information technology. From a pragmatic perspective, the DRI and the BCI 
together defined a framework for professional training and certification for 
people involved in the BC field. The current professional practices are  
[BCI 13]: 
1) Policy and program management: defines the organizational policy, 
implementation, control and validation as well as the top-management 
governance process through leadership and BC support. 
2) Embedding business continuity: integrates BC into daily operations 
and organizational culture through awareness and training in order for the 
personnel to acquire various skills in disaster response. 
3) Business impact analysis: quantifies and qualifies impacts in time of 
losses in case of interruption or disruption of every key activity; identifies 
threats and provides outputs for prioritization and strategy. 
4) Design: identifies and selects strategies and tactics for achieving 
continuity and recovery from disruptions; identifies threat mitigation 
measures including roles and responsibilities; defines the incident response 
team and identifies the extended organization network. 

64     Risk Management in Life-Critical Systems 
5) Implementation: develops the business continuity plan (BCP) for 
executing the previously identified strategies and tactics; depending on the 
organization, the BCP may either include specialist plans (i.e. incident or 
crisis management, pandemic, contingency, major hazards, etc.) or interface 
with CMPs or emergency management plans. 
6) Validation: confirms that the BCP meets the objectives defined in the 
policy through exercise programs.  
The first two professional practices are considered as managerial while 
the latter are considered technical. All professional practices are defined at 
strategic, tactical and operational levels. In its current version, the BC 
discipline is rigorously structured for each professional practice. In the Good 
Practice Guidelines (GPG) [BCI 13], each topic is described with the same 
structure, i.e.: General Principles, Concepts & Assumptions, Process, 
Methods & Techniques and Outcomes & Review. The clarity of the BC 
approach as well as its applied nature nurtures its adoption by national and 
international organizations [DHS 07, JHA 11, NFP 13, OEC 06, OEC 09, 
OEC 11, OEC 11a, UNI 13]. 
Business impact analysis (BIA) relies on two strong concepts related to 
downtime. The first concept is the recovery point objective (RPO) and the 
second is the recovery time objective (RTO). 
The RPO is the point at which resources used by an activity must be 
restored to enable the activity to operate on resumption. The RPO defines the 
maximum tolerable loss. The RPO corresponds to the last state of the system 
that needs to be retrieved after the adverse event, e.g. last data backup to be 
retrieved after a computer crash, or last functioning state to which a system 
must be restored after a disruptive event. The RPO is defined during the 
design stage and timely resources need to be provided accordingly. 
The RTO is “The period of time following an incident within which a 
product or an activity must be resumed, or resources must be recovered.” 
(ISO 22301:2012) [BCI 13]. The RTO includes the minimum level of 
operations required for continuing critical operations before full recovery. 
The degraded mode and transition toward full recovery defines the maximum 
tolerable period of disruption that an organization may afford, often for 
staying in business, before full recovery [COR 07]. 
 

Situated Risk Visualization in Crisis Management     65 
 
Figure 4.1. RTO and maximum tolerable period of disruption [COR 07] 
As such, BC supports both crisis management and emergency 
management. Following other wide-scale events (e.g. wide-scale power 
outages: Auckland, New Zealand, 1998, 2006; the Great American Blackout 
in 2003; terrorism: London bombings, 1992; WTC bombings, 2001), BC 
was reinforced by the effects of the 2008 global financial crisis. Indeed, 
relying only on insurance becomes currently more restricted, and 
organizations are stimulated to implement mechanisms that support 
continuity of their existence and operations especially in adverse situations. 
4.3. Risk management in critical operations 
4.3.1. Human systems integration risk perspective 
Current systems resilience and safety approaches tackle risk from a 
holistic perspective emphasizing that instead of a single root cause, several 
factors combine and contribute in generating hazards that must be 
considered inside the overall system and not as external [LEV 95, HOL 98, 
SWA 03, HOL 09, LEV 11, HOL 12]. 

66     Risk Management in Life-Critical Systems 
Risk management [HAI 09, AVE 10, ENG 12, YOE 12] is essential for 
plans in crisis, emergencies and BC. Depending on both the size and 
organization type on one hand, and on current situation events on the other 
hand, crises, emergencies and BCPs may be exercised to various extents 
within an organization. Furthermore, relationships between these three 
approaches are also defined specifically for each organization and situation. 
In some situations, BC and DR teams report to the emergency management 
team that reports to the crisis management team; in other situations this 
hierarchy may differ [JAC 11]. For example, in case of massive natural 
events, local organizations depend on national structures in terms of 
emergency response and recovery resources. 
Thus, there are hierarchical dependencies and also responsibility sharing 
between organizations and their sociotechnical systems in cases of critical 
adverse events. In life-critical systems, efforts are carried out toward well 
toward well defined responsibility sharing. For example, in the US, in case 
of nuclear events generated by nuclear power plants, FEMA is responsible 
for off-site response and recovery, while the Nuclear Regulatory 
Commission is responsible for on-site response and recovery [FEM 13]. 
Furthermore, risk management must be considered during the whole product 
lifecycle. Systems produced are submitted to certifications and regulations in 
the country or region they may be operated in, e.g. [NRC 12]. 
Beyond generic regulations, it is mandatory to consider the specificities 
of the particular location where a life-critical system is intended to operate. 
Quite recently, adverse events started to be considered from a field 
perspective [WIS 94] based on observed and historical information about 
natural adverse events and their impacts on populations. Such risk factors 
related to particular locations are currently available and must be taken into 
account [BIR 06, DIL 05, UN 10], together with the technical risks related to 
sociotechnical systems themselves. 
4.3.2. Effectiveness of risk definitions in critical operations 
According to [LEV 95, SWA 03, LEV 11], the mainstream probabilistic 
definition of risk (R) of adverse events (e) in terms of likelihood (L), 
consequences (C) and exposure (E) is:  
R(e) = L(e)×C(e)×E(e)  

Situated Risk Visualization in Crisis Management     67 
However, refinements (i.e. risk partitioning) [ASB 84, HAI 09], seem to 
be limited in critical operations when the adverse event already occurred 
with likelihood L, and the focus is thus on operations for coping with 
consequences. This definition is questioned in system safety design stages, 
since probabilities’ accuracy is hardly definable ex ante, before the system is 
developed, deployed and operated [LEV 11]. This risk definition is also 
questioned intrinsically in probability models by subjectivist versus 
frequentist approaches [WAL 06]. The risk defined as the set of all possible 
triplets composed of scenarios (si), probability of occurrence (pi) and 
consequences (xi), i.e. R = {< si, pi, xi >} emphasizes probabilities in terms 
of current subjective knowledge instead of statistical frequencies [KAP 81]. 
Furthermore, the frequentist approach enables only point estimates, while the 
subjectivist approach enables us to take into account previous knowledge 
and experience, and thus to generate learning and improved adaptation 
[WAL 06]. The subjectivist approach proposes a different view of 
uncertainty in terms of degrees of belief, e.g. Known Unknowns, Unknown 
Unknowns and Bias (KUUB) factors [FEN 13].  
Moreover, recent systems resilience and safety approaches agree on 
addressing risk and causality with analysis methods based on qualitative 
explicit requirements and their combinations instead of frequentist 
probabilities [LEV 11, HOL 12]. 
Of particular interest in critical operations is the alternate definition of 
risk (R) in terms of hazard types (H) and vulnerabilities (V), i.e. the  
R = H×V pseudo-equation, introduced by field practitioners [WIS 94]. Its 
main focus is on how social structures can cope with adverse events by 
assessing and reducing through both preparedness and response their 
vulnerabilities for observed types of hazards in given locations, e.g. 
flooding, earthquakes, drought and famine. These initial efforts were 
continued by worldwide organizations that propose dynamic hazard 
indexations per location; vulnerability is measured in number of deaths 
(reported and expected) per hazard type per location [BIR 06, DIL 05,  
UN 10]. The aim of this definition and approach is not to stay descriptive, 
but to implement appropriated actions and processes for reducing the 
number of deaths. 

68     Risk Management in Life-Critical Systems 
4.4. Situated risk visualization in critical operations 
4.4.1. Rationale and requirements 
Based on the previous sections, several risk features in critical operations 
have to be considered at once, through their integration. An effort toward 
considering multifacet crisis features, types and related questions is proposed 
such as in Table 4.2 [STE 13]. Qualitative and quantitative aspects of these 
features are intertwined, in terms of the nature and the amount related to 
each feature as well as to the overall outcome. For example, when dealing 
with loss of life, the nature of losses (i.e. qualitative aspect) is not separable 
from the amount of losses (i.e. quantitative aspect). Furthermore, human 
features are taken into account in terms of actions and psychosocial aspects 
such as the distance with the critical events that impacts risk perception, 
decision-making and response types [PER 84, TRO 07]. 
Feature 
Types 
Question 
Triggering events 
Manmade 
– Erroneous actions 
– Intentions to harm 
natural events 
– Who? 
– How? 
– Why? 
– What? 
Location 
Geographic 
– Where? 
Distance 
Spatial 
Temporal 
Social 
– How far? 
– When? 
– Who? 
Time 
Timing 
Temporal frame 
Duration 
– When? 
– How long? 
Operations 
Open-loop 
Closed-loop 
– How? 
– Why? 
– Who? 
Outcomes 
Social 
Environmental 
Technical 
Financial 
– How Much? 
 
Table 4.2. Crisis features, types and questions to be answered [STE 13]  

Situated Risk Visualization in Crisis Management     69 
Thus, it is necessary to reduce distance through current information 
technology solutions, in terms of communication and also through modeling 
and simulation. State-of-the-art Geographic Information Systems (GIS) 
enable us to provide direct visualization of Emergency Planning Zones 
(EPZs) and furthermore integrate a variety of communication channels  
[ESR 12, RAD 13]. However, the integration of geographic information with 
analytical features appears to be a further requirement for situated risk 
awareness. 
4.4.2. Integrated structure and ontology 
For 
integrating 
geographic, 
man-made 
artifacts 
and 
analytical 
information, the proposed structure is derived from the abstraction hierarchy 
[RAS 85, BUR 04] that proposes means-ends and whole-part relationships 
and decompositions. For dealing with equipment, the initial abstraction 
hierarchy has five levels, presented hereafter bottom-up: (level 1) physical 
form (and location), (level 2) physical function, (level 3) generalized 
function, i.e. processes, (level 4) abstract function, i.e. laws and principles 
and (level 5) functional purpose.  
A set of concepts was identified and mapped onto the abstraction 
hierarchy. In the current proposal, equipment defined in the first two levels 
is expanded toward a macroview for spanning the EPZ (i.e. geographic 
configuration), infrastructure and buildings, and initially proposed 
equipment. This enables us to place overall systems (and systems of 
systems) in their real surrounding configurations and emphasize 
bidirectional relationships in terms of impacts of the system(s) on the 
environment and of the environment adverse events on the system(s). 
Furthermore, representing people (i.e. population and operators and first 
responders) on-site and off-site is essential for completing the view of 
sociotechnical systems in crisis situations. 
An explicit representation of causality is also added (level 3’) before the 
processes level. Representing causality before processes enables us to clarify 
what the emergency processes must deal with and why. Depending on the 
roles and needs of first responder teams, causality can be represented at 
various levels of granularity, e.g. either for the whole EPZ or per subzones 
or subsystems or equipment. 

70     Risk Management in Life-Critical Systems 
In crisis or emergency situations, laws and principles (level 4) are 
associated with threats specific to both the sociotechnical system (e.g. 
radioactivity release from nuclear power plants) and the environment (e.g. 
earthquake and/or tsunami). The general purpose (level 5) corresponds to 
securing and recovering the EPZ. 
From a sociotechnical Human Systems Integration perspective, the 
concepts identified for each level in the current proposal are:  
– levels 1 & 2: geographic zone, population, teams, infrastructure, 
buildings and equipment; 
– level 3’: Bayesian Networks (BN) for representing causality; 
– level 3: Business Modeling and Notation (BPMN) for representing 
sociotechnical processes; 
– level 4: what-if consequences in terms of impacts on people, 
environment, system and industrial domain(s) as well as dispersion or 
propagation of toxic products; 
– level 5: dashboard(s) related to securing the EPZ. 
While a set of other causality representations is available (i.e. event, fault, 
decision tree-based family), recent approaches to safety [LEV 11, HOL 12] 
emphasize that tree representations are hardly usable in complex systems, 
and thus they should be replaced by network representations. Considerations 
on risk approaches (see previous section) and investigations on network 
representations led to selecting BN as well suited for crisis or emergency 
situations. Indeed, BN capture relationships and dependencies between 
system entities and provide convergent representations toward prognosis or 
diagnosis [PEA 88, JEN 07, FEN 13]. 
BPMN [ALL 10] was selected for representing processes, since it enables 
us to integrate the intertwined tasks and actions of both people and technical 
systems. Furthermore, BPMN inherits the orchestration (i.e. managing one 
team), collaboration (i.e. several teams working in parallel) and 
choreography (i.e. synchronization between teams), which are the core 
principles of current information architectures [MCC 08] for distributed 
communication and sharing. 
A variety of dashboards that synthesize the ongoing crisis status and 
evolutions may be used for level 5. So far, Systems Theoretic Process 

Situated Risk Visualization in Crisis Management     71 
Analysis (STPA) [LEV 11] was specified and implemented in 3D but not yet 
evaluated. 
4.4.3. Interactive 3D visual scene 
The structure and ontology identified so far were adapted for the 
Fukushima Daiichi disaster [HAT 12, KUR 12] and implemented in the 3D 
interactive visual scene, in terms of content, presentation and interaction 
(Figure 4.2) [STE 13]. The two main principles underlying the 3D scene 
design are affordances and enactment [GIB 86, WEI 88, VAR 91, NOE 04]. 
Affordances confer naturalness for visualizing and acting on the 
environment. Enactment is essential in discovering and understanding 
situations through emerging patterns of action on the environment. Thus, the 
concepts (i.e. content) are addressed in 3D, since the real world is 3D and 
also since main threats are 3D. Their presentation may be either in true 3D 
(i.e. stereoscopic 3D) or in 2.5D, in case non-stereoscopic devices such as 
regular displays or tablets are employed. First person view (FPV) interaction 
was specified and implemented for enabling users to explore the 3D scene. 
Because existing GIS solutions enable us only to “fly” but not to “dive” 
underground, the 3D interactive scene was implemented in the gaming 
engine Unity 3D. Indeed, it was considered essential to enable users to 
“dive” for exploring critical infrastructure located underground (e.g. diesel 
generators located in turbine buildings basements). 
 
Figure 4.2. Global view of the 3D interactive scene – Unity 3D [STE 13] (For a  
color version of this figure, see www.iste.co.uk/millot/riskmanagement) 

72     Risk Management in Life-Critical Systems 
The interactive 3D is based on visual analytics recommendations  
[THO 05] and aligned with the universal tasks protect, respond, recover 
[DHS 05] and related ESFs. 
4.4.4. Evaluation results 
The interactive 3D scene was displayed on stereoscopic devices (i.e. 
Magnetic 3D 42″ autostereoscopic display and Oculus Rift virtual headset at 
the Human Centered Design Institute (HCDi), and Infitec 3D 161″ × 90″ 
wall in the Organization for Economic Co-operation and Development 
(OECD)/Institutt for energiteknikk (IFE)/Halden Reactor Project (HRP)/ 
Man-Technology Organisation (MTO) facilities), and non-stereoscopic 
devices (regular Sony 40″ display at the HCDi). Based on a role-playing 
game in a simplified scenario derived from the Fukushima Daiichi early 
events, the participants’ main task was to endorse and enact the role of a risk 
manager, and based on their scene understanding, make a final decision. A 
total of 30 experts from various backgrounds (including nuclear, crisis 
management and BC, and emergency management) and 20 students 
participated in the evaluation. All participants had minimal training. 
Performance for finding equipment, situation awareness, workload, 
satisfaction, risk information sufficiency as well as eye tracking were 
employed for the main experiment on autostereoscopic, Infitec 3D and 
regular display. A user experience survey was performed for the Oculus Rift. 
For the given participant sample, results from the various methods are very 
good. In particular, relationships between the various levels (geographic and 
analytical) are well understood. Most importantly, risk information 
sufficiency (based on the risk information seeking and processing (RISP) 
[GRI 04]) for making the final decision was almost maximal. 
4.5. Conclusions and perspectives 
The current research is aligned with state-of-the-art solutions for dealing 
with crisis situations. Building on these, it aims to expand the integration of 
geographic information and analytical information within the same 3D visual 
scene. While concepts employed in risk approaches are crucial for dealing 
with given situations, situated risk visualization and interaction with the 
visual scene play also a key role in improving risk assessment and 
management, especially in complex situations involving the sociotechnical 

Situated Risk Visualization in Crisis Management     73 
system and the environment altogether. The scope of the current research 
was to investigate if people can use and understand a quite complex 3D 
visual scene and effectively interact with it. Within this scope and based on 
the simplified scenario, results are encouraging for pursuing these initial 
efforts. A direct application of such 3D visual scenes is improved 
communication in crisis situations. Furthermore, reinforcing the principles of 
serious games, another direct application is training, especially for the 
higher and executive management layers, where people do not necessarily 
have knowledge about the location, topography or layout of technical 
systems. Since all participants made a human-centered decision, the main 
objective of the overall design was attained so far. 
4.6. Bibliography 
[ALL 10] ALLWEYER T., BPMN 2.0: Introduction to the Standard of Business 
Process Modeling, Herstellung und Verlag, 2010. 
[ASB 84] ASBECK E.L., HAIMES Y.Y., “The partitioned multiobjective risk method 
(PMRM)”, Large Scale Systems, vol. 6, no. 1, pp. 13–38, 1984. 
[AVE 10] AVEN T., RENN O., “Risk management and governance: concepts, 
guidelines and applications”, Risk, Governance and Society, vol. 16, Springer, 
2010. 
[BCI 13] BCI, Good Practice Guidelines 2013 Global Edition, The Business 
Continuity Institute, 2013. 
[BIR 06] BIRKMANN J., Measuring Vulnerability to Natural Hazards: Towards 
Disaster Resilient Societies, United Nations University Press, 2006. 
[BOO 90] BOOHER H.R., MANPRINT: An Approach to Systems Integration, Van 
Nostrand Reinhold, 1990. 
[BOO 03] BOOHER, H.R., Handbook of Human Systems Integration, Wiley-
Interscience, 2003. 
[BOY 13] BOY G.A., Orchestrating Human Centered Design, Springer, 2013. 
[BUR 04] BURNS, C., HAJDUKIEWICZ, J.R., Ecological Interface Design, CRC 
Press, 2004. 
[COO 07] COOMBS W.T., Ongoing Crisis Communication: Planning, Managing and 
Responding, SAGE Publications, 2007. 
[COR 07] CORNISH M., “The business continuity planning methodology”, The 
Definitive Guide of Business Continuity, Wiley, 2007. 

74     Risk Management in Life-Critical Systems 
[CUR 07] CURTIS P.M., Maintaining Mission Critical Systems in a 24/7 
Environment, IEEE Press/Wiley-Interscience, 2007. 
[CUS 11] Central United States Earthquake Consortium (CUSEC), After action 
report, 2011. 
[DHS 05] U.S. DEPARTMENT OF HOMELAND SECURITY (DHS), Universal task list, 
2005. 
[DHS 07] U.S. DEPARTMENT OF HOMELAND SECURITY (DHS), Target capabilities 
list, 2007. 
[DIL 05] DILLEY M., CHEN R.S., DEICHMANN U., et al., Natural Disaster Hotspots: 
A Global Risk Analysis, Disaster Risk Management Series No. 5, The 
International Bank for Reconstruction and Development/The World Bank and 
Columbia University, 2005. 
[ENG 12] ENGEMANN K.J., HENDERSON D.M., Business Continuity and Risk 
Management: Essentials of Organizational Resilience, Rothstein Associates Inc., 
2012. 
[ESR 12] ESRI, ArcGIS for Emergency Management, Esri, 2012. 
[FEM 13] FEMA, Program Manual: Radiological Emergency Preparedness, 
FEMA, June 2013. 
[FEN 13] FENTON N., NEIL M., Risk Assessment and Decision Analysis with 
Bayesian Networks, CRC Press/Taylor & Francis Group, 2013. 
[FIN 86] FINK S., Crisis Management: Planning for the Inevitable, American 
Management Association, 1986. 
[GIB 86] GIBSON J.J., The Ecological Approach to Visual Perception, Psychology 
Press – Taylor & Francis Group, 1986. 
[GIL 08] GILPIN D.R., MURPHY P.J., Crisis Management in a Complex World, 
Oxford University Press, 2008. 
[GRI 04] GRIFFIN R., NEUWIRTH K., DUNWOODY S., et al., “Information sufficiency 
and risk communication”, Media Psychology, vol. 6, pp. 23–61, 2004. 
[HAI 09] HAIMES Y.Y., Risk Modeling, Assessment and Management, 3rd ed., 
Wiley, 2009. 
[HAT 12] HATAMURA Y., OIKE K., KAKINUMA S., et al., Final report, Investigation 
Committee on the Accident at Fukushima Nuclear Power Stations of Tokyo 
Electric Power Company, 2012. 
[HOL 98] HOLLNAGEL E., Cognitive Reliability and Error Analysis Method 
CREAM, Elsevier, 1998. 

Situated Risk Visualization in Crisis Management     75 
[HOL 09] HOLLNAGEL E., The ETTO Principle: Efficiency-Thoroughness Trade-off, 
Ashgate, 2009. 
[HOL 12] HOLLNAGEL E., FRAM: the Functional Resonance Analysis Method: 
Modeling Complex Socio-Technical Systems, Ashgate, 2012. 
[JAC 11] JACOBSEN G., KERR S., “Crisis management, emergency management, 
BCM, DR: what’s the difference and how do they fit together”, The Definitive 
Guide of Business Continuity, 3rd ed., Wiley, 2011. 
[JEN 07] JENSEN F.V., NIELSEN T.D., Bayesian Networks and Decision Graphs, 
Springer, 2007. 
[JHA 11] JHA A.K., BLOCH R., LAMOND J., Cities and Flooding: A Guide to 
Integrated Risk Management for the 21st Century, The World Bank/Global 
Facility for Disaster Reduction and Recovery, 2011. 
[KAP 81] KAPLAN S., GARRICK B.J., APOSTOLAKIS G., “Advances in quantitative 
risk assessment – the maturing of a discipline”, IEEE Transactions on Nuclear 
Science, vol. 28, no. 1, pp. 944–946, February 1981. 
[KUR 12] KUROKAWA K., ISHIBASI K., OSHIMA K., et al., The official report of the 
Fukushima nuclear accident independent investigation commission: executive 
summary, The National Diet of Japan, 2012. 
[LAG 81] LAGADEC P., Le Risque Technologique Majeur – Politique, Risque et 
Processus de Développement, Collection Futuribles, Pergamon, p. 630, 1981. 
[LEV 95] LEVESON N.G., Safeware: Systems Safety and Computers, Addison-
Wesley, 1995. 
[LEV 11] LEVESON N.G., Engineering a Safer World: Systems Thinking Applied to 
Safety, MIT Press, 2011. 
[MCC 08] MCCABE F.G., ESTEFAN J.A., LASKEY K., et al., Reference Architecture 
for Service Oriented Architecture Version 1.0, OASIS, 2008. 
[MIT 87] MITROFF I.I., SHRIVASTAVA P., UDWADIA F.E., “Effective crisis 
management”, The Academy of Management Executive (1987–1989), vol. 1,  
no. 4, pp. 283–292, November 1987. 
[NFP 13] NATIONAL FIRE PROTECTION ASSOCIATION (NFPA), Standard on 
Disaster/Emergency Management and Business Continuity Programs, 2013 
Edition, NFPA 1600, 2013. 
[NOË 04] NOË A., Action in Perception, The MIT Press, 2004. 
[NRC 12] U.S. Nuclear Regulatory Commission (U.S. NRC), Package 
ML12079A207 – AREVA Application Public, Rev. 3 – Tier 1, 2012 

76     Risk Management in Life-Critical Systems 
[OEC 06] OECD studies in risk management: Japan earthquakes, OECD, 2006. 
[OEC 09] OECD reviews of risk management policies: Japan large floods and 
earthquakes, OECD, 2009. 
[OEC 11] OECD, Risk Awareness, Capital Markets and Catastrophic Risks, Policy 
Issues in Insurance, no. 14, OECD Publishing, 2011. 
[OEC 11a] OECD, “Risk assessments for future global shocks”, Future Global 
Shocks: Improving Risk Governance, OECD Publishing, 2011. 
[PEA 88] PEARL J., Probabilistic Reasoning in Intelligent Systems: Networks of 
Plausible Inference, Morgan Kaufmann, 1988. 
[PER 84] PERROW C., Normal Accidents: Living with High-Risk technologies, 
Princeton University Press, 1984. 
[PER 98] PEARSON C.M., CLAIR J.A., “Reframing crisis management”, Academy of 
Management Review, vol. 23, no. 1, pp. 59–76, 1998. 
[RAD 13] RADKE S.L., JOHNSON R., BARANYI J., Enabling Comprehensive 
Situation Awareness, Esri Press, 2013. 
[RAS 85] RASMUSSEN J., “The role of hierarchical knowledge representation in 
decision making and systems management”, IEEE Transactions on Systems, 
Man and Cybernetics, vol. 15, pp. 234–243, 1985. 
[STE 13] STEPHANE A.L., Visual Intelligence for Crisis Management, PhD 
Dissertation, Human Centered Design Institute/Florida Institute of Technology, 
2013. 
[SWA 03] SWALLOM D.W., LINDBERG R.M., SMITH-JACKSON T.L., “System safety 
principles and methods”, in BOOHER H.R. (ed.), Handbook of Human Systems 
Integration, Wiley-Interscience, 2003. 
[THO 05] THOMAS J.J., COOK K.A., Illuminating the Path: The Research and 
Development Agenda for Visual Analytics, IEEE Press, Washington, 2005. 
[TRO 07] TROPE Y., LIBERMAN N., WAKSLAK C., “Construal levels and 
psychological distance: effects on representation, prediction, evaluation, and 
behavior”, Journal of Consumer Psychology, vol. 17, no. 2, pp. 83–95, 2007. 
[UN 10] UNITED NATIONS & THE WORLD BANK, Natural Hazards, Unnatural 
Disasters, The World Bank, 2010. 
[UNI 13] UNISDR, Business and Disaster Risk Reduction. Good Practices and Case 
Studies 2013, The United Nations Office for Disaster Risk Reduction, 2013. 
 

Situated Risk Visualization in Crisis Management     77 
[VAR 91] VARELA F.J., THOMPSON E., ROSCH E., The Embodied Mind – Cognitive 
Science and Human Experience, MIT Press, 1991. 
[WAL 06] WALLACE B., ROSS A., Beyond Human Error: Taxonomies and Safety 
Science, CRC Press/Taylor & Francis, 2006. 
[WEB 13] WEBSTER W.R., Federal Emergency Response and Preparedness, FEMA 
Region I – MIT Professional Education, 2013. 
[WEI 88] WEICK K.E., “Enacted sensemaking in crisis situations”, Journal of 
Management Studies, vol. 25, no. 4, pp. 305–317, 1988. 
[WIS 94] WISNER B., BLAIKIE P., CANNON T., et al., At Risk, 1st ed., Routledge, 
1994. 
[YOE 13] YOE C., Principles of Risk Analysis: Decision-Making under Uncertainty, 
CRC Press/Taylor & Francis Group LLC, 2013. 
[ZIA 01] ZIAUKAS T., “Environmental public relations and crisis management”, in 
FARAZMAND A. (ed.), Handbook of Emergency and Crisis Management, CRC 
Press, 2001. 

 

5 
Safety Critical Elements of the  
Railway System: Most Advanced 
Technologies and Process  
to Demonstrate and Maintain  
Highest Safety Performance 
5.1. Railways demonstrate the highest safety performance for public 
transportation  
The public is generally confident in its railway transportation system. 
This confidence and performance are confirmed by the statistics published 
by our National Transportation Authorities. Railways have demonstrated the 
highest performance in safety for public transportation across all modern 
countries in Asia, America and Europe, Figure 5.1. 
– In EU-27, more than 97% of fatalities come from road accidents. 
– The fatality rate of railway accidents continuously fell by 5.5% every 
year with rates that are lower or equivalent to those of Civil Aviation’s (both 
per hour and per km of exposure). 
5.2. Key success factors 
This high performance in safety is all the more outstanding in that the 
railway system is by nature a system with a high level of complexity and 
                         
Chapter written by Stéphane ROMEI. 

80     Risk Management in Life-Critical Systems 
integration. The main components of this system are: rolling-stock, track, 
energy, 
signaling, 
infrastructure, 
communication, 
operation 
and 
maintenance, Figure 5.2. 
 
Figure 5.1. Range of order that has been observed for the last decade 
 
Figure 5.2. Main components of a railway system 
This high level of performance results from several factors of which the 
three key successes are: 
1) expertise and innovation in design, operation and maintenance in 
safety critical technologies; 
2) competences in project management and system integration; 
3) procedure for risk management. 

Safety Critical Elements of the Railway System     81 
The aim of this chapter is to introduce these three key success factors, 
focusing on the European market and its very high-speed technology for 
illustration. 
5.3. The European very high-speed rail technology: a safety concept 
with more than 30 years of experience and continuous innovation in the 
technology 
Safety records in Europe are high: 
– there has been no single fatality across Europe and Asia by operating the 
French TGV technology on very high-speed lines; 
– the European Commission-27 (EC-27) consolidates on this main line 
network a yearly number of fatalities that is less than one third of  
the fatalities from road accidents in France alone (1,183 fatalities). Most of 
these fatalities are third parties on level-crossings or that walk along or cross 
the tracks despite this being forbidden: only 2% of these fatalities are 
actually passengers. 
This high level of safety is the outcome of a robust safety concept. This 
concept has been developed from the safety concept of conventional 
railways. But, it has added specific measures to cover and master the three 
main gaps introduced with the very high speeds: 
1) guidance and dynamic behavior; 
2) environment with avoidance of external events; 
3) velocity with capability to guarantee the emergency breaking. 
Below, we will focus on the safety concept related to these three main 
gaps as an illustration of the complexity and diversity in safety critical 
elements of a railway system. 
5.3.1. Guidance and dynamic behavior 
The interface wheel-track and a dynamic behavior with positive stability 
are key elements. The TGV technology is today proven, and the recent world 
record of 574.8 km/h has demonstrated huge margins when operated at a 
commercial speed that ranges around 300–360 km/h. 

82     Risk Management in Life-Critical Systems 
Numerous design studies by application of mechanical standards, 
simulations and extensive series of testing have allowed designing specific 
guidance and dynamic behavior for very high speeds. 
As an example of complexity, the bogie of a rolling-stock will be 
integrating, by itself, six safety critical functions whose performances could 
contradict themselves: to brake, to traction, to immobilize, to filter, to guide 
and to capture/shunt, Figure 5.3. 
 
Figure 5.3. The bogie integrating six safety critical functions 
5.3.2. Environment with avoidance of external events 
At very high speeds, the train driver has no time to identify any external 
event that could cause an unsafe event and stop the train. Nor can he/she 
react quick enough to stop a train if he/she sees a hazard. 
Specific protection and warning systems have, therefore, been introduced 
for very high speeds such as: fences with no level-crossing, detection of 
land-slides, rock falls, car falls from above bridges and cross wind. 
Detection of earthquake is implemented in areas at risk. 
All these technologies are highly reliable to mitigate false alarms but to 
communicate real alerts. These technologies are generally developed by 
using principles of redundancies, negative logic with fail/safe functions. 

Safety Critical Elements of the Railway System     83 
Further to active safety, passive safety has also been significantly worked 
out for limiting the impact of a potential collision or derailment. For 
instance: 
– articulated architecture increases stability of the trainset in case of 
derailment of collision; 
– crashworthiness is addressed comprehensively by application of most 
advanced EC standards (e.g. EN15227), with consideration of anticlimber 
devices, occupant survivability and crash simulations and testing (i.e. there 
are crash tests with test benches as for the car industry). 
5.3.3. Velocity with capacity to guarantee the emergency braking 
As the train driver can no longer see along the track and rely on trackside 
signals, a dedicated and common EC high-speed signaling system has been 
developed: 
The safety critical functions that are covered by the signaling system are: 
– to set train routes that are compatible (e.g. avoiding front-end collision 
of trains on the same track); 
– to regulate the speed of the train and ensure sufficient distance between 
two trains; 
– to avoid overspeed on main-line but specific areas (e.g. point machine 
and curves) or at specific periods of time (e.g. during maintenance works 
along the track). 
Signaling technologies used are to be based on interlocking with safety 
relays. Today, these signaling systems are based on IT and telecom 
technologies with safety critical processors technologies (e.g. monocoded 
processors, two out of three processor platforms) and communications 
systems that prevent conflict of routes, controls of speed of trains, controls 
of switch of points and other signals to highest safety integrity level (i.e. 
safety integrated level 4). 
The hardware and software development follow industry standards that 
bring very high level of confidence in the performance and implementation 
(EN 50129 and EN 50128). Technics and principles are similar to those 
implemented for aerospace but airplane for their onboard control/command. 

84     Risk Management in Life-Critical Systems 
Today, EC has defined a common standard for its signaling system. The 
EC system is called the European Rail Traffic Management System 
(ERTMS). It aims to cross border with no further limitations to fluidifying 
the traffic but makes railways even more competitive with the highest 
standards of safety. 
This signaling system activates the rolling-stock braking system. The 
design of this braking system had to be adapted to the challenges of the very 
high speed. A train at very high speeds will need around 4 km to come to a 
complete stop. The braking system is based on the fail-safe principle 
introduced by the UIC standards for mechanical braking of conventional 
railways. Moreover, blending with electrostatic braking has been introduced 
for very high speed (to limit mechanical effort). Disk brakes and bogies do 
go under extensive simulations and bench testing. Functions, such as axle 
non-rotation detection, were introduced long before the car industry. 
Furthermore, all above elements are designed with significant margins: 
disk capacity, block sectioning, worst environment conditions and degraded 
modes (e.g. back wind). 
5.3.4. Lifetime spanning several decades: operation and maintenance 
This description of the safety concept introduced for very high speed is 
the opportunity to introduce diversity in safety critical technologies as used 
by the railway industry. These technologies are made up of IT critical 
processing functions, interlocking with safety relays, fail-safe and/or highly 
redundant functions but most advanced mechanical standards, calculations, 
simulations and testing by using levels of margin based on experience. 
These technologies will further be designed to be robust and operated 
within an open environment for decades. They have to be proven in extreme 
weather conditions and robust against human factors. 
Their operation and maintenance will then rely on safety critical tasks 
such as bogie checks and profiling or management of degraded modes such 
as fire and smoke in a tunnel. These robust organizations, procedures and 
qualifications are key safety parameters to maintain throughout the lifetime 
of the railway system. 

Safety Critical Elements of the Railway System     85 
5.4. Project management and system integration 
5.4.1. Robust industry standards in project management 
Competences in project management and system integration are keys to 
ensure the delivery of a new railway system to highest standard of safety. 
Project management follows standards of the industry. It includes key 
milestones to ensure a stepwise definition and construction of the system. 
The development cycle is often illustrated by a V-cycle as per the Figure 5.4: 
 
Figure 5.4. Classical development V-cycle 
This development cycle is aligned with main milestones as regulated by 
National Safety Authorities (NSAs) for checks and authorization to proceed to 
the next stage. The last stage is obviously of importance with the definition of 
comprehensive criteria to grant the authorization for public operation. 
5.4.2. System integration 
This V-cycle is an essential procedure for managing the integration of the 
system. 
A product by itself has no safety level until you can describe its functions 
and how it interacts with others and subsequent causes and consequences of 
a failure. 

86     Risk Management in Life-Critical Systems 
The safety analyses will identify and evaluate all possible degraded 
modes of any component and their interfaces in terms of cause and 
consequence. It generally depends on how they interact with each other, i.e. 
how they are integrated with other elements and the external environment. 
The safety process defined by Industry Standards targets a comprehensive 
management of hazard records: identification, traceability, responsibility and 
supervision:  
– to ensure that hazards and their mitigation requirements are identified 
and managed by the right actor; 
– to supervise the railway system with traceability on safety requirements 
and their implementation; 
– in this context, the safety of each individual component of the railway is 
analyzed, but additional analysis is required to address the interfaces and 
transverse performances and functions (including Human Machine Interface 
(HMI)): 
- technical compatibility includes safety analyses and reviews of: 
emergency braking distance, track/wheel, pantograph/catenary, static and 
dynamic gauges (including platform gaps) and Electromechanical 
Interference (EMI)/Electromechanical Compatibility (EMC). Databook, 
documentation management and configuration management are also key 
methods and tools to ensure the right data, and information has been used 
across the various actors of the project, 
- concepts of degraded and emergency modes (e.g. evacuation, fire and 
earthquake) must be defined and all railway components integrated in line 
with these concepts with a specific focus on line of command, 
communication and power supply with back-up strategy but consistency 
across of architecture schemes (e.g. sectioning of signaling and power 
supply compatible with emergency modes of operation). 
5.5. Procedure for risk management 
5.5.1. The regulatory framework 
Last but not least is the definition of a common procedure for risk 
management given by EC. 

Safety Critical Elements of the Railway System     87 
In 2004, the EC has decided to introduce to the European railway agency, 
in order to move to common safety methods, common safety indicators and 
allow one train authorized in one country to be operated in another country 
by application of interoperability rules. 
This procedure of risk management is built upon four elements: 
– procedure for the authorization to place a new system (or major change 
in the system) in service; 
– procedure for independent investigation of accidents and return of 
experience; 
– industry standards for product development cycle, system integration 
and project management; 
– safety management systems to ensure safe operation and maintenance. 
Today, each member state (MS) is organized as follows, Figure 5.5: 
– MS, whose role is to regulate railway safety upon common EC rules. 
– an NSA whose role is to authorize public operation, supervise safety of 
operation and recommend improvements to MS; 
– a National Investigation Body (NIB) whose role is to investigate 
accidents and recommend improvements to MS. 
 
Figure 5.5. Risk management organization in European Union 

88     Risk Management in Life-Critical Systems 
5.5.2. The EC common safety method 
The EC has ruled a common method for risk management across the 25 
MS with the following breakthrough: 
– common risk-based criteria for acceptance. 
The risk-based approach has been confirmed by EC. This is the 
recognition of the complexity and the need for innovation, both of which are 
not solely ruled by just technical standards. 
However, EC acknowledges that the risk-based approach is not 
necessarily quantitative based, and it introduces three types of criteria: 
– risk can be mitigated by application of recognized technical standards. 
– risk is mitigated by the introduction of a system in which safety is of 
the same order of magnitude as a similar and existing system. 
– risk has been evaluated to fall into the risk acceptance matrix (e.g. 
likelihood of fatality by exposure to a collective accident below 10–9/h). 
With the introduction of the following: 
– use of codes of practice; 
– similarity analysis; 
– and/or local probabilistic target. 
Integration management re-enforced: 
– to ensure that hazards and their mitigation requirements are identified 
and managed by the right actor; 
– to supervise the railway system with traceability on safety requirements 
and their implementation. 
5.5.3. High technical and safety standards 
The main standards in use in Europe for safety evaluations and 
demonstrations are EN50126, EN50128 and EN50129. They provide 
confidence in the safe integration of products or systems with operation and  
 

Safety Critical Elements of the Railway System     89 
maintenance before public operation having robust process, method and 
tools in place for: 
– hazard identification and mitigation; 
– risk evaluation and demonstration. 
These standards promote the use of traditional technics for identification 
and evaluation of hazards and their subsequent risks (e.g. fault-tree analysis, 
Failure Mode Effect and Consequence Analysis (FMECA), Operational 
Safety Hazard Analysis (OSHA), brainstorming, etc). These methods aim to 
ensure a comprehensive identification of hazards, analyses and evaluations 
of associated risks in order to define subsequent safety requirements for 
mitigation to be acceptable, see Figure 5.6. 
 
Figure 5.6. Technics for identification and evaluation of  
hazards and their subsequent risks 
Then, each of these safety requirements are allocated to the responsible 
entity for their design and implementation. This is recorded in the hazard 
record that will be closed only when we obtain formal evidence that these 
safety requirements have been implemented with required level of 
confidence. 
This process is called safety evaluation and demonstration. It is 
documented to form a safety case of which the elements are defined in 
standards like EN50129. 

90     Risk Management in Life-Critical Systems 
The robustness of this process will require specific attention to the 
following items: 
– it is necessary to promote both top-down methods (e.g. fault-tree) and 
bottom-up methods (e.g. FMECA) to ensure comprehensive identification of 
hazards; 
– dependencies across the complexity of the railway system must be 
identified to mitigate common modes of failure and carry-out proper 
evaluation; 
– HMI and human errors: if a probability is assigned, we must carefully 
understand that a human being does not follow statistics laws of hardware, 
but probability of error is usually associated with low confidence; 
– manipulation of concepts, such as safety integrity level, control 
functions (permanent or by demand mode), requires an in-depth 
competences in probability laws. 
5.5.4. Independent safety assessment 
The owner of the system that requests an authorization for public 
transportation will need to prepare and submit its safety case to confirm 
safety levels achieved and their acceptability with respect to regulations. 
This safety report as submitted then will need to be checked to get a level 
of confidence in its conclusions (without limiting the original responsibility 
of the owner). Requirements of an independent safety assessment have been 
introduced into the EC regulation allowing each of the MS to take schemes 
to an Independent Safety Assessor (ISA) as appropriate for their country: 
inspectorate within NSA, accredited bodies or expert and internal in-house 
independent department for large companies. 
5.5.5. Significant change 
This procedure is pragmatic and focuses on significant change.  
Criteria, such as use of codes of practice and similarity analysis, will also 
facilitate the demonstration process with a focus on changes and capitalizing 
on the experience and existing technologies. 

Safety Critical Elements of the Railway System     91 
5.5.6. Safety management system 
Infrastructure managers and railway undertakings (operators) that are 
operating and maintaining several railways are deemed to demonstrate their 
capability to maintain safety during public operations. 
Beyond the application of the authorization process, they must 
demonstrate their capability to operate and maintain the railway system in 
line with the requirements of the safety case. 
EC has recently introduced a common safety management system that is 
to measure the maturity of those Infrastructure Manager (IM) and Railway 
Undertaking (RU) in safety management. Figure 5.7 shows the reference 
management system that is used for this measurement. 
 
Figure 5.7. European safety management system 

92     Risk Management in Life-Critical Systems 
The main chapters of this European SMS are: 
– process for design and improvement: leadership, risk assessment, 
monitoring and organizational learning; 
– process for implementation: structure and responsibility, competence 
management, information and documentation; 
– operational activities: operational arrangements and procedures, 
emergency plans. 
5.5.7. Safety authorization and safety management system 
Figure 5.8 illustrates the articulation of the safety authorization with the 
safety management system of the RU and IM (slightly simplified). 
 
Figure 5.8. Safety authorization and safety management system 
For instance, the safety management system of the infrastructure 
management will demonstrate the capability of: 
– safety operation and maintenance of infrastructure elements; 
– safety “technical” integration for use of infrastructure elements based 
on type authorization or generic safety file of the element; 

Safety Critical Elements of the Railway System     93 
– safe integration of the above with the safety management system of the 
railway undertakings (including its rolling-stocks). 
And, safety management system of the railway undertaking will 
demonstrate the capability of: 
– safety operation and maintenance of its rolling-stock; 
– safety “technical” integration for use of its rolling-stock based on type 
authorization. 
5.6. Conclusion 
This short chapter can hardly show all faces of railway safety. Therefore, 
the focus on EC and the very high-speed railways during the lecture is the 
opportunity to illustrate the safety critical elements of a railway system and 
its key success factors for one of the railway market segment among the 
other ones. 
We must retain that above all public is confident in its railway 
transportation systems. This confidence is confirmed by statistics with 
highest level of performance across modern countries in Asia, America and 
Europe and compared to other modes of public transportation: 
– railway is the safest mode of public transportation; 
– it is based on expertise with continuous innovation and large diversity 
in safety-critical technologies (i.e. from mechanical to IT and telecoms 
technologies); 
– it has recognized competences in project management and system 
integration; 
– and it is regulated by procedures for risk management that promote 
safety management systems and the use of risk-based approaches but with 
pragmatic criteria that value existing reference, experience and technical 
standards and focus on significant change. 
 

 

6 
Functional Modeling of Complex Systems 
6.1. Introduction  
Complexity is a significant aspect of nearly all industrial processes and 
technical infrastructures. It influences decisions made by system designers 
and it may affect the vulnerability of systems to disturbances, their 
efficiency, the safety of their operations and their maintainability. 
Automation systems, the human machine interface and the operator play key 
roles in the reduction of operational complexity of industrial processes and 
infrastructures. Automation can reduce complexity of the operator’s task by 
relieving him from the responsibility of responding to events and situations 
predicted by the control engineers. But automation can also increase the 
complexity of the operator’s supervisory task in situations where  
the automation is failing. In such situations the operator must understand the 
causes and consequences of automation failure and must intervene to gain 
control of the situation. Therefore, there is a need for system models which 
can reveal complexity aspects relevant for system design and operation. 
6.1.1. Dimensions of system complexity 
System complexity has many dimensions. Important aspects of the 
complexity of industrial process and technical infrastructures can be 
expressed by the number of system elements and their interrelations. The 
elements and the relations represent selected features of the systems relevant 
for a particular design or operational purpose and can be represented in a 
                         
Chapter written by Morten LIND. 

96     Risk Management in Life-Critical Systems 
system model as a graph. This aspect of system complexity can accordingly 
be measured by properties of its graph representation. Complexity is 
accordingly in this aspect a property of the system model and can be reduced 
or increased by aggregation or decomposition of the graph. Therefore, 
system complexity is relative to the features represented and dependent on 
the purpose or the model. Another important aspect of system complexity 
can be measured by the number of system aspects or perspectives which are 
necessary in order to provide a representation of the system which provides 
the information relevant for the design or operational problem under 
consideration. Whereas the complexity measure in the first case is a 
syntactical property of a particular perspective on the system, it is, in the 
latter case, related to the content of the representation i.e. semantic relations 
between the system represented and the model. A complex system would, in 
this latter view, require many perspectives for its proper representation and, 
therefore, be semantically complex.  
This chapter will describe a modeling methodology capable of 
representing industrial processes and technical infrastructures from multiple 
perspectives. The methodology called Multilevel Flow Modeling (MFM) has 
a particular focus on semantic complexity but addresses also syntactic 
complexity. MFM uses means-end and part whole concepts to distinguish 
between different levels of abstraction representing selected aspects of a 
system. MFM is applied for process and automation design and for 
reasoning about fault management and supervision and control of complex 
plants. It belongs to a branch of Artificial Intelligence research (AI) called 
qualitative reasoning and has a strong focus on qualitative modeling and 
reasoning about system purposes and functions.  
MFM has been developed for more than two decades and a 
comprehensive literature is available presenting the concepts and application 
of MFM. The development of the conceptual foundations, the MFM 
modeling language, the tools and applications have been ongoing for more 
than two decades and are still in progress. The basic ideas of MFM were 
conceived by the author [LIN 94] and developed over the years by his and 
other research groups. The research originated in problems of representing 
complex systems in human machine interfaces for supervisory control, but 
developed into a broader research field dealing with modeling for design and 
operation of automation systems for safety critical complex plants. 

Functional Modeling of Complex Systems     97 
The aim of the present chapter is to present the modeling paradigm which 
is composed of the conceptual foundations of MFM to explain the relevance  
of the paradigm for design and operation of industrial systems and technical 
infrastructures and provide a survey of the current status of MFM illustrated 
with modeling examples.  
6.2. The modeling paradigm of MFM 
The basic idea of MFM is to represent an industrial process or a technical 
infrastructure as an artifact i.e. as a system designed and operated according 
to given purposes. The MFM modeling paradigm has its foundations in the 
concepts of purpose, goal and function. These concepts are used extensively 
but without a scientific foundation in both engineering design and operation 
of complex dynamic systems. In the last decade it has played a central role  
in the development of new advanced human machine interfaces for operation 
of industrial systems and in other domains adapting the ideas of means-ends 
analysis of work domains e.g. based on Rasmussen’s abstraction hierarchy 
[RAS 94]. Functional concepts form the basis for an integrated approach to 
control room design in nuclear power plants proposed by [PIR 06]. The 
adoption of functional approaches to system design in industry is a very 
strong indicator of the engineering relevance of concepts of function and the 
means-ends distinction. 
It will be shown in the following that the concept of function plays a key 
role in understanding the nature of the means-ends relation and in the 
formation of a coherent modeling paradigm integrating means-ends concepts 
with concepts of function. Due to the particular role of the concept of 
function the modeling paradigm is often called functional modeling (FM). 
MFM is, accordingly, a methodology for FM of complex systems. 
6.2.1. The concept of function 
We will introduce the concept of function as an example from process 
industry. Let us consider a pump. A pump may not seem to be a very 
complex system seen as a physical object. A discussion of its functions in a 
process plant can be used to introduce basic aspects of the concept of 
function which are relevant for modeling more complex systems like power 
plants or chemical processes. Systems which appear complex in terms of the 
number of physical parts and interconnections may have a simple function.  

98     Risk Management in Life-Critical Systems 
A pump has a simple physical structure but has actually a complex function 
in e.g. a power plant. 
The following sentence describes a function of the pump: “the function of 
the pump is to transport water”. This sentence clearly expresses knowledge 
of the pump which is relevant to both plant design and operations. But what 
is its meaning and how can we make use of the type of knowledge expressed 
by the sentence? In order to address this question we will discuss four 
general aspects of the concept of function. 
6.2.1.1. Functions are social facts 
Functions are ascribed to items or systems depending on the interest of a 
designer or an operator. This particular aspect of functional concepts is often 
mistaken to imply that functions cannot be validated. However, functions are 
only subjective in an ontological sense and objective in an epistemic sense 
because having a function is a fact which cannot be disputed i.e. is true 
according to the knowledge shared by a community of designers and users 
[SEA 95]. These types of facts are called social facts by Searle and 
distinguished from (physical) brute facts which are objective. This social 
aspect of functions reflects of their dependence on purposes or goals (see 
below).  
Note that in spite of the intersubjective nature of functions, it is only 
meaningful to ascribe a function to an entity (e.g. the water transport 
function to the pump) if it is capable of realizing it i.e. it has the proper 
dispositions (see below). For functions relevant to modeling of complex 
physical processes this requirement is essential. Other types of functions not 
relevant to engineering systems can be ascribed to entities by convention 
(such the function of a piece of metal to be money). These so-called 
institutional functions are realized simply by convention. 
6.2.1.2. Functions are relative to goals 
The function “to transport water” is not an intrinsic property of the pump 
like its weight or shape (brute facts) but is related to a particular intention or 
goal of a designer or an operator. The transport function is ascribed to the 
pump and it could be ascribed other functions as well depending on the 
design or operational context. If the pump, for example, is part of a cooling 
loop, it could also have been ascribed a function related to the cooling.  
 

Functional Modeling of Complex Systems     99 
Furthermore, if the designer or the operator has a particular interest in the 
inner operations of the pump it may be ascribed the function of providing 
pressure.  
6.2.1.3. Functions and roles 
The function of the pump is defined in relation to what it does to another 
physical entity, here the water. Therefore, the sentence describing the 
function of the pump implies a distinction between the roles served by the 
pump (the agent) and the water (the object) in the transportation. The pump 
is the agent (the causer) of the transportation and the water is the object 
transported. The agent and object roles are relative to the function which 
may have more roles. An entity can also serve several roles. If the pump is 
part of a cooling system, the water in the pump serves two roles: (1) it is the 
object transported by the pump and (2) it is the agent transporting the heat 
removed by the cooling. 
The distinction between the function and its roles implies a separation of 
dynamic and static aspects of the system. Functions refer to changes whereas 
roles refer to aspects which persist or are static. Accordingly, roles are 
representations of entities involved and abstract features of the situation 
which persist during the time frame of the action. While transporting the 
water, the pump and the water accordingly maintain their roles as agent and 
object.  
Note that the fulfillment of roles is often conditional in circumstances not 
directly related to the function to which the role is associated. Thus, the 
object role of the water in our example of the pump is conditional on the 
water in a fluid state and the agent role of the pump is conditional on proper 
lubrication of its bearings. The function can also be conditional on other 
circumstances. Thus, the transportation of water is conditional on the 
availability of sufficient water. Thus, even if the agent and the object roles 
are enabled i.e. there is a potential for transportation, it is also necessary to 
provide the opportunity for interaction between the pump and the water.  
The purpose of control actions in complex systems e.g. during start-up 
can be to establish and maintain availability of the roles associated with 
system functions. These actions of enablement are important ingredients in 
start-up plans. Representation of this relation between functions and their 
enabling support is discussed below. 

100     Risk Management in Life-Critical Systems 
6.2.1.4. Functions, dispositions and structure 
The realization of the transport function of the pump is dependent on the 
availability of physical entities which can realize the function. The 
realization has both structural and dispositional features. The physical parts 
of the pump should be available and properly configured and the parts (the 
structure) should provide the causal powers (the disposition) for the 
pumping. The material transported (the water) should be liable to 
movements and pushing (its dispositions) in order to be transportable. It is 
realized that the functions and roles of entities are dependent on their 
disposition. Since the disposition of an item includes all possible ways it 
could interact with the environment, its functions and roles either as an 
agent, object or another role is a subset of its dispositions [MUM 98]. 
6.2.2. The means-end relation 
The four aspects of functions presented above will now be integrated with 
the means-end relation to create a comprehensive system concept which is 
foundational to the modeling paradigm of MFM. The principle of the 
integration is to see structure, dispositions, functions, roles and goals as 
aspects of the means-end relation.  
The different aspects of the means-end relation connect the means for 
action (the structure and the dispositions) with the potentials and 
opportunities available for action here and now (the roles and the functions) 
and objectives to be achieved in the future. Therefore, the aspects are 
organized according to stages in using the means to realize a goal i.e. 
connecting the past with present and future. The integrated concept is shown 
in Figure 6.1.  
Note that the ordering of the aspects according to the means-end 
distinction should not be seen as forming a hierarchy. This would imply that 
the aspects were ordered according to a principle of subordination. And this 
is not the case. As indicated, the ordering has temporal aspects but is 
fundamentally related to the distinction between the potential and the 
actualization of an action. 
 

Functional Modeling of Complex Systems     101 
 
Figure 6.1. The means-end relation 
 
Figure 6.2. Means-end structure showing the possible combinations of means-end relations 
6.2.3. Means-end structure 
The distinctions between goals, functions, roles behavior and structures 
provided by the means-end relation are composed of different perspectives 
on the same situation or goal oriented activity. In most cases, we will need to 
represent relations between several activities which are ongoing at the same 
time or organized in a temporal sequence. The key to modeling interrelations 
between activities is to provide a categorization of the goals of an activity. 
The following four categories of goals of an action can be identified:  

102     Risk Management in Life-Critical Systems 
– to achieve the state produced by the action/function (S0); 
– to execute the function or serve a role (S2); 
– to enable another function or role (S1); 
– to produce a structural means for another function (S3). 
The four different goal categories and the associated three ways of 
creating means-end structures are shown in Figure 6.2. The nesting of 
means-end structures provides the possibility of describing an activity or 
function of a system as the context for describing another activity of the 
same or another system. The nesting of means-end structures combined with 
the possibility of describing the same activity from different perspectives 
accordingly provides a very strong basis for systematic handling of 
contextual information which is crucial for FM.  
6.3. Uses of functional modeling 
After having introduced the basic concepts of functions, means-end 
relations and structures we will now discuss motivations for using FM. In 
summary, there are two main motivations: 
– concepts of FM provide a systematic framework for formalizing 
intersubjective common sense knowledge which is shared among 
participants in design and operation of complex systems, i.e. engineers and 
operators; 
– FM is a systematic approach to apply different perspectives and degree 
of abstraction in the description of a system and represent shifts in contexts 
of purpose. This is crucial for handling semantic complexity.  
These two basic features make FM a powerful tool for modeling complex 
systems. Further motivations for using FM in operator support systems and 
control system design are given below.   
6.3.1. Operator support systems 
Operators need information about plant states and means of action that fit 
with their current tasks in order to reduce the risks of decision error. Here, 
FM can be used as a systematic tool in Human machine interaction (HMI) 

Functional Modeling of Complex Systems     103 
development to define the information content of displays and design the 
decision support functions that can help an operator in problem reframing 
i.e. considering alternative representations of a situation. Problem reframing 
may be necessary in safety critical situations where a wrong decision can 
lead to damage of equipment, loss of production or undesirable 
consequences to the environment. 
6.3.2. Control systems design 
FM supports integrated process and control system design by providing 
abstractions by which high-level decision opportunities and constraints in 
process and control system design are made explicit. In this FM can provide 
documentation of design rationale. 
FM can be used to reason about control strategies, diagnosis and planning 
problems. FM can also be used to identify assumptions implicit in control 
systems designs based on differential equations. 
Model-based control based on FM can integrate diagnostic reasoning and 
reactive planning of counteractions and thereby respond intelligently to 
major plant upsets. Such intelligent controls can explain its purpose and 
thereby make its behavior more transparent to an operator. 
6.4. Multilevel flow modeling 
In the remainder of this chapter we will provide an overview of the 
current status of MFM. The concepts of MFM are built on the modeling 
paradigm introduced above in Figure 6.1 and 6.2 and address a particular but 
large domain of industrial processes and technical infrastructures dealing 
with processing and distribution of energy and materials. Accordingly, MFM 
is able to represent a significant class of complex systems. It is not able to 
model everything because functions represent system purposes and are, 
therefore, specific to the needs served by the technology. Actually its 
specialization is one of its strength since the representation of means-end 
relations and functions in complex systems require deep insight in the 
purposes and workings of the system.  
The overview of MFM presented below is necessarily incomplete and the 
reader is advised to consult recent MFM publications of MFM as 

104     Risk Management in Life-Critical Systems 
supplementary in-depth reading. The basic MFM concepts are introduced in 
[LIN 11a, LIN 11b]. Recent extensions with the role concept can be found in 
[LIN 10]. Comprehensive modeling examples from the nuclear domain can 
be found in [LIN 11d, LIN 12b] and [WU 13] present an MFM model from 
the oil/gas domain. The use of MFM for reasoning about failure causes and 
consequences is presented in [LAR 96, PET 01, LIN 11c, ZHA 13] and 
[HEU 10] presents the use of MFM for causal reasoning about control. 
Model examples from chemical industry and application of MFM for 
counteraction planning are presented in [GOF 97, GOF 04] and [ROS 06, 
WU 13] present application of MFM for risk assessment. Application of 
MFM for modeling safety functions are presented in [LIN 12b]. MFM 
modeling methodology and tools are presented in [LIN 11e] and [HEU 12]. 
6.4.1. MFM concepts 
MFM represents goals and functions of process plants involving 
interactions between flows of material, energy and information. Functions are 
represented by elementary flow and control functions interconnected to form 
functional structures representing a particular goal oriented view of the 
system. An action theoretical foundation which is under development see 
MFM functions as instances of more generic action types (see e.g. [LIN 11b] 
where these types are used to define basic control functions). The views 
represented by the functional structures are related by means-end relations and 
are composed of a comprehensive model of the functional organization of the 
system in accordance with the modeling paradigm presented in Figure 6.1. 
 
Figure 6.3. MFM concepts 

Functional Modeling of Complex Systems     105 
6.4.2. A modeling example 
The MFM concepts will be illustrated below by a heat transfer loop 
example. The example is taken from [LIN 11b] where the MFM model is 
used as a template for the development of an MFM for the Monju nuclear 
power plant.  
The heat transfer loop shown in Figure 6.4 comprises two heat 
exchangers HE1 and HE2 connected by a circulation loop including a pump 
PMP1. The type of fluid used for heat transfer has no significance for the 
MFM model we are presenting but we will assume for convenience that it is 
water. We will also ignore physical details which are not relevant for the 
present purpose. This includes physical details of the power supply for the 
pump motor and of the systems serving as energy sources and sinks.  
The water flow rate in the circulation loop is maintained by the controller 
CON1 on the basis of readings obtained from a flow measuring device 
(FM1). The purpose of the temperature controller CON2 is to regulate the 
temperature in heat exchanger HE1. This is done by compensating 
deviations in the temperature measured by the instrument TM1 by increasing 
or decreasing the set point for the flow of circulated water when the 
temperature increases or decreases.  
We will present a model of the heat transfer loop without control systems 
and a model including the control systems. Later we will illustrate how to 
represent the safety functions of a shutdown system (not included in Figure 6.4). 
 
 
Figure 6.4. A heat transfer loop 

106     Risk Management in Life-Critical Systems 
6.4.2.1. MFM of heat transfer loop without control  
Figure 6.5 shows the MFM of the heat transfer loop without a control 
system. It contains three functional levels composed of an energy flow 
structure efs1, a mass flow structure mfs1 and an energy flow structure efs2. 
These levels are nested into means-end structures according to the basic 
principles shown in Figure 6.2.  
 
Figure 6.5. MFM of heat transfer loop without control 
Flow structure efs1 represents the functions involved in pumping water in 
the circulation loop when seen as an energy conversion process. The source 
sou1 represents the power supply, sto1 the accumulation of rotational and 
translational energy in the circuit and tra2 and tra3 represents conversion of 
the energy into kinetic energy of the water (tra2 and sin1) and friction losses 
in the circulation loop (tra3 and sin2).  

Functional Modeling of Complex Systems     107 
Flow structure mfs1 represents the functions of the water circulation loop. 
The function tra4 represents the transportation of water resulting from the 
energy conversion in the pump represented by efs1. It is connected with efs1 
by a producer–product relation pp1 which is a means-end relation. The 
relation pp1 is labeled with the name of the function in efs1 which is directly 
associated with tra4 namely tra2 (the so-called main function of the 
producer-product relation pp1). Since the water is recirculated the two ends 
of the transport function tra4 are connected with the function sto2 
representing the water storage in the circuit. The storage sto2 is also 
connected with two barriers bar1 and bar2. They represent the prevention of 
material flows to enter (sou2 and bar1) or leave (bar2 and sin3) the 
circulation loop provided by the piping walls in the heat exchangers HE1 
and HE2.  
Flow structure efs2 represents the heat transfer functions. The water 
circulation loop here is seen in the context of the systems serving as a heat 
source and a sink. The function of the loop is in this context to transport 
energy from the outlet of HE1 to the inlet of HE2 (tra8) and to transport 
from outlet of HE2 to the inlet of HE1 (tra7). Since the transportation of 
energy represented by tra7 and tra8 both are mediated by the circulating 
water, tra7 and tra8 are connected with mfs1 by two mediation relations me1 
and me2. The mediation relations are both labeled by tra4 which is the main 
function in mfs1. The heat transfer from the source (sou3) to the primary 
side of HE1 is represented by tra5 and sto3. The transfer from the heat 
storage in the HE1 primary to the circulation loop is represented by tra6 and 
bal1 which is connected with the incoming and outgoing energy flows (tra7 
and tra8, respectively). The heat transfer and storage in HE2 are represented 
in a similar way by functions bal2, tra9 and sto4. The heat transfer from the 
secondary side of HE2 to the sink is represented by tra10 and sin4. 
6.4.2.2. MFM of the heat transfer loop with control systems  
The MFM model shown in Figure 6.6 includes the functions of the water 
flow controller and the temperature controller. The controller is here 
assumed to use the power supplied to the pump (tra1) to control the pump 
speed (sto1) so that the flow rate of water (tra4) can be maintained at its 
desired value (obj1). The actuation relation act1 connects the control 
function con1 with the transport function tra1 as indicated by its label. Note 
that the MFM shown in Figure 6.6 in an example where a control function  
 

108     Risk Management in Life-Critical Systems 
includes several functional levels (efs1 and mfs1). This means that the 
means-end relations can be included in the control function (pp1 and ma4 in 
Figure 6.6).  
 
Figure 6.6. MFM of heat transfer loop with flow and temperature control 
The functions of the temperature controller are represented by the 
function structure csf2 in Figure 6.6. The temperature is related to energy 
storage in HE1 (sto2) and is regulated by controlling the energy transferred 
to HE2. This energy transfer is represented by the transport functions tra8 
and tra9 in the MFM model.  
Note the control cascade pattern in Figure 6.6. Function con2 
representing the function of the temperature regulator is connected by an 
actuation relation to cfs1 which represents the functions of the flow 

Functional Modeling of Complex Systems     109 
regulator. It is realized that in this case the control cascade includes three 
functional levels through the means-end relations pp1, ma1, ma2 and ma3. 
6.4.3. Modeling safety functions 
Objectives in MFM are states which should be achieved by the functions 
and are therefore promoted by the decisions of the process designer or the 
actions of a control agent. However, designing or acting for reasons of safety 
deal with avoiding harmful situations. Such situations are obviously not 
promoted but opposed by proper design decisions or control actions. 
Therefore, MFM also considers functions that oppose states which imply a 
risk or are undesirable by being in conflict with the values of the designer or 
the control agent.  
These situations or states are called threats and are represented by a black 
circle in MFM (see Figure 6.3.). Like an objective, a threat refers to a 
situation or state. But unlike an objective which refers to a desirable 
situation, a threat refers to something which is undesirable or a hazard. The 
distinction between objectives and threats expresses value related 
preferences of the process designer or the control agent. Objectives and 
threats share a common property of being situations which are the target of 
the designer’s decisions and the agent action. Threats can be combined in 
MFM with destroy and suppress relations and the means or countermeasures 
used to oppose them. The use of threats to represent safety critical states 
ensures consistency of intentional structures in MFM. Intentions are 
considered consistent if they are rational in the sense that there is no conflict 
between the end and the means taken to achieve the ends. Accordingly, it 
would be inconsistent to connect a produce relation with a threat (unless the 
model represents the view of a saboteur). 
The use of MFM for modeling safety functions can be illustrated by the 
heat transfer loop example. We will assume that there is a risk of 
overheating of the fluid temperature on the secondary side of HE2. 
Therefore, the temperature regulator in Figure 6.4 is substituted by a 
protection system monitoring the temperature and responding with 
protective actions if the temperature gets too high. We will assume that the 
control system will change the set-point of the flow controller.  

110     Risk Management in Life-Critical Systems 
Figure 6.7 shows the MFM model with the modifications required to 
represent the protection system. The modification is composed of the control 
structure cfs2 modeling the function of the protection system including the 
threat thr1 which may be expressed by a temperature limit (related to the 
accumulation of heat in HE2 represented by the energy storage function 
sto4. The protection system is actuating (ac2) the transfer of energy (tra1) 
inside the pump. 
 
Figure 6.7. MFM model of heat transfer loop with a protection  
system suppressing high temperature in HE2 
6.5. Conclusions 
In this chapter, we have discussed complexity aspects of industrial 
processes and technical infrastructures and presented the modeling paradigm 

Functional Modeling of Complex Systems     111 
of FM. An outline is given of the reasons why FM is highly relevant for 
process and automation design and for design of information presentation 
and decision support for human operators. The chapter also presents a 
practical methodology for FM called MFM with illustrative modeling 
examples. 
MFM has achieved a high level of formalization but there is still room for 
improvement and consolidation of its foundations. One issue of particular 
importance is the necessity of ensuring completeness and consistency of the 
elementary flow functions. The present set of flow functions is the result of a 
long development focused on modeling power plants, but experiences from 
applying the flow ontology on other related process domains such as 
chemical engineering plants have indicated the need for extending the set of 
flow functions. The question is how these extensions should be done in a 
systematic way so that consistency and completeness is ensured of the set of 
elementary functions. Ongoing research is developing an action theoretical 
foundation for FM (and thereby also for MFM) which promises to provide a 
systematic basis for constructing ontologies of domain functions.  
6.6. Bibliography 
[GOF 02] GOFUKU A., OZAKI Y, ITO K., “A dynamic operation permission system 
for pressurized water reactor plants”, Proceedings of ISOFIC 2002 International 
Symposium on the Future of I&C for NPP, Seoul, Korea, pp. 360–365, 7–8 
November 2002. 
[GOF 04] GOFUKU A., OHI T., ITO K., “Qualitative reasoning of the effects of a 
counteraction based on a functional model”, Proceedings of CSEPC 2004, 
Sendai, Japan, 4–5 November 2004. 
[HEU 10] HEUSSEN K., LIND M., “Representing causality and reasoning about 
controllability of multilevel flow systems”, Proceedings IEEE International 
Conference on Systems, Man and Cybernetics (SMC 2010), Istanbul, Turkey, 
10–13 October 2010. 
[HEU 12] HEUSSEN K., LIND M., “On support functions for the development of 
MFM models”, Proceedings of 1st International Symposium on Socially and 
Technically Symbiotic Systems, Okayama, Japan, 29–31 August 2012. 
[LAR 96] LARSSON J.E., “Diagnosis based on explicit means-end models”, Artificial 
Intelligence, vol. 80, no. 1, pp. 29–93, 1996. 

112     Risk Management in Life-Critical Systems 
[LIN 94] LIND M., “Modeling goals and functions of complex industrial plants”, 
Applied Artificial Intelligence, vol. 8, no. 2, pp. 259–283, 1994. 
[LIN 10] LIND M., “Knowledge representation for integrated plant operation and 
maintenance”, 7th American Nuclear Society Topical Meeting on Nuclear Plant 
Instrumentation, 
Control 
and 
Human-machine 
Interface 
Technologies 
NPIC&HMIT 2010, Las Vegas, Nevada, 7–11 November 2010. 
[LIN 11a] LIND M., “An introduction to multilevel flow modeling”, International 
Journal of Nuclear Safety and Simulation, vol. 2, no. 1, pp. 22–32, March 2011. 
[LIN 11b] LIND M., “Control functions in MFM: basic principles”, International 
Journal of Nuclear Safety and Simulation, vol. 2, no. 2, pp. 1–8, June 2011. 
[LIN 11c] LIND M., “Reasoning about causes and consequences in multilevel flow 
modeling”, Proceedings of European Safety and Reliability Conference ESREL 
2011, Troyes, France, 18–22 September 2011. 
[LIN 11d] LIND M., YOSHIKAWA H., JØRGENSEN S.B., et al., “Multilevel flow 
modeling of MONJU nuclear power plant”, Nuclear Safety and Simulation,  
vol. 2, no. 3, 2011. 
[LIN 11e] LIND M., YOSHIKAWA H., JØRGENSEN S.B., et al., “Multilevel Flow 
Modeling of MONJU nuclear power plant”, Proceedings of ICI2011, Daejon 
Korea, pp. 21-25, August 2011. 
[LIN 12a] LIND M., “Modeling safety barriers and defense in depth with multilevel 
flow modeling”, Proceedings of 1st International Symposium on Socially and 
Technically Symbiotic Systems, Okayama, Japan, 29–31 August 2012. 
[LIN 12b] LIND M., YOSHIKAWA H., JØRGENSEN S.B., et al., “Modeling operating 
modes for the MONJU nuclear power plant”, Nuclear Safety and Simulation, 
vol. 3, no. 4, pp. 314–324, 2012. 
[MUM 98] MUMFORD S., Dispositions, Oxford University Press, Oxford, UK, 1998. 
[PET 01] PETERSEN J., Knowledge based support for situation assessment in human 
supervisory control, PhD Thesis, Department of Automation, Technical 
University of Denmark, 2001. 
[PIR 06] PIRUS D., “Why and how a functional information system improves 
computerized operations”, Proceedings International Topical Meeting on 
Nuclear Instrumentation Control and Human Machine Interface Technology, 
Albuquerque, NM, 12–16 November 2006. 
[RAS 94] RASMUSSEN J., PEJTERSEN A.M., GOODSTEIN L.P., Cognitive Systems 
Engineering, Wiley, New York, 1994. 

Functional Modeling of Complex Systems     113 
[ROS 06] ROSSING N.L., LIND M., PETERSEN J., et al., “A functional approach to 
HAZOP studies”, Proceedings of the 2nd International Conference on Safety 
and Environment in Process Industry, Naples, Italy, May 2006. 
[SEA 95] SEARLE J., The Construction of Social Reality, Penguin Books, 1995. 
[WU 13] WU J., ZHANG L., LIND M., et al., “Hazard identification of the offshore 
three-phase separation process based on Multilevel Flow Modeling and 
HAZOP”, Proceedings of the 26th International Conference on Industrial 
Engineering and other Applications of Applied Intelligent Systems, IEA/AIE, 
Amsterdam, The Netherlands, June 2013. 
[ZHA 13] ZHANG X., LIND M., RAVN O., “Multilevel flow modeling based decision 
support system and its task organization”, Proceedings of the 26th International 
Conference on Industrial Engineering and other Applications of Applied 
Intelligent Systems, IEA/AIE, Amsterdam, The Netherlands, June 2013. 
 

 

PART 2 
Risk Management and Human Factors


7 
Designing Driver Assistance  
Systems in a Risk-based Process 
7.1. Risk-based design in perspective  
7.1.1. Risk-based design principles 
Risk-based design (RBD) is a well-formalized and consolidated 
methodology that systematically integrates risk analysis in the design 
process with the aim of prevention, reduction and/or containment of hazards 
and consequences embedded in the systems as the design process evolves. 
Formally, it identifies the hazards of the system and continuously optimizes 
design decisions to mitigate them or limit the likelihood of the associated 
consequences, i.e. the associated risk. 
The main steps of a basic RBD methodology can be summarized in nine 
sequential phases, as follows (Figure 7.1). Each one of these phases requires 
the application of well-defined and known techniques. 
Phase 1: identify hazards and scenarios. This phase is essentially based 
on a qualitative approach. When designers have established a main design 
configuration, it is necessary to address safety aspects, or in more simple 
terms, “things” that can go wrong. This means identifying the hazards and 
associated risky scenarios that require the intervention of safety measures in 
the process, in order to maintain the systems in safe and possibly operational 
conditions. Process hazard analysis techniques and past experience provide 
information on possible risky scenarios. 
                         
Chapter written by Pietro Carlo CACCIABUE. 

118     Risk Management in Life-Critical Systems 
 
Figure 7.1. Risk-based design methodology flowchart 
The experience and expertise of designers and safety analysts are equally 
important in identifying the hazards and scenarios that need to be addressed. 
Phase 2: estimate the consequences. In this phase, designers establish the 
consequences of the risky scenarios identified in phase 1. These scenarios 
typically involve quality, systems and overall plant, health, societal and 
environmental impacts. Usually, consequences of interest are measured in 
terms of equipment damage caused by fires, explosions, toxic materials 
releases and/or health effects on human beings. However, more sophisticated 
types of consequences are sometimes considered that are of lesser immediate 
impact in terms of physical damage, but may be weakening or damaging 
other aspects of the system involved, eventually leading to catastrophic 
effects. As an example, an airline could consider “delays” or “flight 
cancellations” as possible consequences of a serious nature generating a lack 

Designing Driver Assistance Systems in a Risk-based Process     119 
of confidence in the company and eventually loss of clients and reduction of 
business. 
The evaluation of potential consequences requires the implementation of 
quantitative analyses. These can be determined through direct observation, 
engineering judgment, or, more appropriately, by means of the basic 
principles and equations describing the systems involved. The solutions of 
these equations, usually based on simplifying assumptions and numerical 
representation, enable us to “calculate” the consequences derived from the 
hazards and associated scenarios. In some cases, field assessments or 
laboratory experimentation are also utilized to evaluate the consequences of 
hazards. 
Phase 3: determine the tolerability of the consequences. Guidance on 
tolerability criteria include: company-specific criteria, engineering codes and 
standards and, above all, regulatory requirements. International standards 
and national regulations are usually applied in order to finalize the design of 
a system according to the risk-based criteria established by the various 
authorities. 
Phase 4: estimate likelihood and risks. Estimates of likelihood rest upon 
an understanding of the mechanism and frequency with which risky 
scenarios such as those identified in phase 1 might occur. Historical  
data about previous occurrences that bear similarities with the scenario under 
analysis are essential. In addition, methods such as event trees and fault tree 
analysis help in developing quantified estimates. Measures of risk  
are obtained by combining the likelihood of each event building up the 
occurrence and estimates of the consequences. The use of consolidated and 
accepted risk matrices for representing the risk associated with scenarios and 
hazards selected in phase 1 is the way to consistently apply the risk-based 
approach for safety and design. 
Phase 5: determine risk tolerability. Determining risk tolerability means 
deciding about the acceptability or not of the risk identified in the previous 
step. Guidance on tolerable levels of risk can be found from established 
criteria by the competent authorities. If the criteria indicate a tolerable level 
of risk, then the design of the system is deemed satisfactory from a risk 
standpoint with respect to the selected hazard, and the RBD is completed by 
performing the final phases of cost/benefit analysis and documentation of the 
results (phases 8 and 9). If the criteria indicate intolerable risk, the next 

120     Risk Management in Life-Critical Systems 
phase is to reduce risk through further design. The critical issue at this level 
of development of the RBD process is associated with the area in the vicinity 
of the acceptable criteria. Indeed, in these areas the safety managers and 
designers have to decide whether to implement more safety measures to 
further reduce risk, or to accept the identified risk levels on the basis of 
safety considerations to be eventually proposed to the competent authorities. 
Phase 6: consider enhanced and/or alternative designs. In the overall 
RBD sequence, this step is an opportunity to reconsider the entire design and 
introduce changes and further safety measures that can reduce risk to a 
tolerable level. In principle, two possible forms of mitigation are possible. 
Safety barriers can be implemented that aim at reducing the likelihood of 
occurrence of certain negative event (“causal barriers”) or at limiting the 
negative effects of the consequences (“consequential barriers”).  
In most systems, the first set of barriers is usually more extensively 
applied than the second set of barriers, also in consideration of the fact that 
reducing the likelihood of occurrence of an event can affect several 
occurrences and scenarios, whereas the implementation of consequential 
barriers is usually only affecting a single occurrence/scenario. The 
implementation of new barriers and risk reduction should also consider an 
initial cost-benefit assessment of the proposed changes. 
Phase 7: evaluate enhancements and/or alternatives. A design change 
intended to reduce risk can introduce new hazards, scenarios and risks. 
Therefore, the evaluation of design changes should treat these changes as an 
integral part of the process. This implies an iterative step of the overall RBD 
process. 
Phase 8: determine tolerability of risk and cost. The established risk 
criteria can provide guidance on risk tolerability. Cost becomes an issue in 
this step, as the RBD process must meet business criteria. Coupling 
estimates of cost and risk reduction provides a basis for assessing the cost-
benefit trade-off of each alternative design or mitigation solution. The cost-
benefit analysis should be performed on a quantitative base. 
The process has to continue until all risks are assessed and estimated 
acceptable from all perspectives, i.e. safety and cost benefit. 

Designing Driver Assistance Systems in a Risk-based Process     121 
Phase 9: document results. Documenting the process safety system 
generates essential information for the development of the safety 
management strategies of an organization. Primarily, in so-doing it will be 
possible to establish a consolidated approach for assessing all hazards that 
are identified at design level and that may be generated by changes in the 
system occurring as time evolves. It ensures that design or operational 
changes reflect an understanding of the base-line risk of the design. Finally, 
it establishes the reference documentation and information to be utilized in 
recurrent safety assessment (audits) that are mandatory in all risky 
environments by the competent authorities. 
7.1.2. Short historical review of RBD process 
Traditionally, system designs were performed with respect to safety by 
considering a design basis, or a design reference mission. This means that 
the maximum credible accident was taken into consideration, and the safety 
measures were designed in such a way to sustain this accident and maintain 
acceptable system performances. The assumption was that if a system was 
able to respond to the most severe initial condition then it was also able to 
respond safely to any other circumstance. 
This approach does not consider at all the concept of risk and likelihood 
of combination of different negative events that may not be extremely severe 
on their own, but, when combined, can generate potentially more serious 
overall consequences than the design basis. The RBD approach is based on 
the concept that safety should be assessed by considering the likelihood of 
occurrence of different events that may follow a variety of initiating events 
(IEs) or initiating hazards (IHs) or undesirable operational states (UOSs), 
leading to consequences of different severity. Each one of these occurrences 
is then assessed in terms of acceptable or unacceptable risk. 
The first comprehensive risk evaluation of a major system design is 
universally recognized as the Reactor Safety Study, also known as: WASH-
1400 [WAS 75]. This development was completed in the mid-1970s, and its 
purpose was to quantify the risks to the general public from commercial 
nuclear power plant (NPP) operation. This logically required identification, 
quantification and phenomenological analysis of a very considerable range 
of low frequency, relatively high-consequence scenarios that had not 
previously been considered in detail. WASH-1400 modeled scenarios 

122     Risk Management in Life-Critical Systems 
leading to large radiological releases from commercial NPPs. It considered 
highly complex scenarios involving success and failure of many and diverse 
systems within a given scenario, as well as operator actions and 
phenomenological events.  
Significant controversy arose as a result of WASH-1400. These early 
controversies have been dealt with primarily by improving methods. 
However, many areas considered controversial in the 1980s remain today 
areas of concern [KAP 81, NRC 83, NRC 95, NRC 03, NRC 09]. 
Completeness is one issue. Quantification, and especially quantification of 
uncertainties, is still much debated today. Dynamic aspects of incident 
evolution remain an open issue. Human factors and human reliability is 
probably the most controversial approach. 
The work of WASH-1400 was very diffused and led to similar 
implementation in other domains of application. Subsequent developments 
have confirmed many of the essential insights that can be obtained from this 
type of study and established the essential value of the approach as well as 
pointed the way to methodological improvements. 
In addition to providing a crucial quantitative perspective on accidents, 
WASH-1400 provided other results that clearly show the importance of 
applying RBD approaches in the commercial design process of complex 
technologies. As an example, WASK-1400 showed that some of the more 
frequent, less severe IEs or UOSs lead to severe accidents at higher expected 
frequencies than some of the less frequent, more severe IEs. 
The methodology based on assessing the risk associated with system 
design operations has gradually expanded from the nuclear energy domain to 
commercial and military aviation to chemical and process plants and to the 
space industry [KIR 98, NAS 11a, NAS 11b]. Moreover, the concepts 
applied for implementing design of safety systems have expanded to the 
regulation and decision-making areas, generating the field of risk-based 
regulations and risk-based decision-making. In general, the safety 
assessment of complex system has nowadays fully embedded the concepts of 
risk analysis, and the mandatory requirements of all safety authorities are 
associated with the concept of safety management based on risk analysis 
concepts [NAS 11c, IAE 11, ICA 12]. 

Designing Driver Assistance Systems in a Risk-based Process     123 
Independently of the fact that the methodology of RBD is mature and 
fully developed, the varieties of open issues mentioned above are still the 
subjects of several research endeavors and initiatives. In particular, this 
chapter will focus on the human factors and human reliability issue, and how 
this can be dealt with in the context of a very dynamic environment such as 
the automotive domain. 
The specific theoretical problem of handling dynamic human–machine 
interactions (HMIs) in a safety and RBD perspective will be discussed first. 
Then, a specific development for the automotive domain will be considered 
and a case study will complement the theoretical discussion. 
7.2. Human factors in risk-based design 
The methodology that governs and pervades the RBD approach is the 
quantitative risk assessment (QRA), which may also be called in slightly 
different manners according to the domain of application, e.g. probabilistic 
safety assessment (PSA) in the nuclear domain, probabilistic risk assessment 
(PRA) in some petrochemical environments. 
QRA can be defined as a systemic approach for the evaluation of the risk 
associated with the operation of a technological system, aimed to estimate 
the frequency of occurrence, and associated uncertainty of the consequences 
of certain events. When standard QRA type analyses are performed, the 
value of the probability of certain events and of their associated 
consequences is a crucial requirement to be satisfied. 
In this scenario, the role of human factors is very relevant, as the 
contribution of the human operator in the control of systems has always been 
of paramount importance for the effective and safe functioning of a system. 
In the current age of automation and supervisory control, this importance has 
further increased, even if the actual contribution of humans to the control 
process seems to have been reduced. In particular, the contribution of 
“human error” has become extremely relevant for the causation and 
development of accidents, as will be discussed in the following sections. 
This justifies the relevance and need of accurate treatment of human factors 
in the  process of RBD.  

124     Risk Management in Life-Critical Systems 
Several aspects of human factors are to be considered in detail in order to 
ensure that a coherent and valuable contribution is given to the RBD process. 
In particular: 
– the human reliably assessment (HRA) method utilized for quantifying 
the probabilistic contribution of “human inappropriate behaviour” to the 
overall RBD process needs to be identified. Moreover, a clear structuring of 
the “human inappropriate behaviour” is necessary, which involves the 
consideration for two other interconnected elements; 
– the model of human behavior and performance that establishes the  
level of accuracy of the analysis of human contribution to the control 
process; 
– the taxonomy of human error that enables us to combine the human 
behavior and the inadequacy of performance in a formal way for the RBD 
analysis. 
These three elements, i.e. HRA method, human behavior model and 
taxonomy of human error, have to be balanced within the overall QRA 
methodology in order to avoid gaps or unsupported excessive accuracy. 
In addition, the HMI control process is essentially dynamic, and this 
further 
complicates 
the 
overall 
risk 
assessment 
study. Adequate 
consideration must be paid to these aspects that are needed for the overall 
development of an HRA methodology for RDB. 
7.2.1. Human reliability assessment 
The human contribution to risk is an integral part of any QRA analysis. 
More specifically, the HRA methods concentrate on the integrated human–
machine system (HMS). The majority of HRA methods developed and 
applied over the years have seriously considered the issues of data 
availability and integration within a larger methodological framework. A 
variety of methods were developed during the 1970s and 1980s. They are the 
so-called “First Generation HRA methods” and aim at providing 
probabilities of success, or failure, of certain procedures involving human 
actions [CAC 93]. The basic data and parameters for human errors are 
derived from field studies. 

Designing Driver Assistance Systems in a Risk-based Process     125 
The most relevant technique and complete framework, developed in those 
years, is certainly Technique for Human Error Rate Prediction (THERP) 
[SWA 83]. Many methods, based on the same principles, were developed in 
the same period, but they focused on slightly different issues, such as data 
collection, or semi-dynamic representation of the HMI processes. 
The consideration for human behavior in all techniques of the first-
generation HRA, suffered a strong bias toward the quantification, in terms of 
success/failure of action performance. Less attention was paid to the real 
causes of human error, and the essential cognitive processes involved. The 
extensive use of automation that has characterized the 1990s and the first 
decade of the 2000s, and the complexity of the system have further pushed 
the role of human operators as decision makers and managers of automated 
systems. Therefore, the role assigned to human operators in the control loop 
requires that special attention is dedicated precisely to the errors of cognition 
and decision-making, rather than errors of overt behavior.  
Another important aspect of HMI is the dependence of human errors on 
the dynamic evolution of incidents. From this viewpoint, the overall picture 
of first generation methods is even less encouraging, as only static 
dependencies are considered in almost all approaches of that period. 
These two issues, i.e. the ability to account for reasoning and decision-
making processes and the consideration of dynamic interplay between 
humans and machines, are the driving force of most methods of recent 
development, called “Second Generation HRA methods”. Examples of these 
approaches are A Technique for Human Event Analysis (ATHEANA)  
[BAR 98], Cognitive Reliability and Error Analysis Method (CREAM)  
[HOL 98] and Dynamic Logical Analytical Method for Human Error Risk 
Assessment (DYLAM-HERA) [CAC 94]. The development of methods that 
can be generally recognized as second-generation HRA approaches has 
continued for some years, and currently a variety of methods are being 
applied in various domains [LYO 05, BEL 09]. 
These methods present the disadvantage to be rather complicated. They 
enable us to perform a very detailed analysis of the system and interactions 
with humans for many different combinations of faults and errors. But, their 
implementation presents a number of currently unsolvable problems, 
primarily the availability of probability data and relative uncertainty 
distributions. Another difficulty, associated with the implementation of 

126     Risk Management in Life-Critical Systems 
second-generation methods without further simplification, derives from the 
fact that the simulation of the HMI should be able to account for many 
different dynamic interactions, possible behaviors and system responses. 
When the combination of errors and events becomes particularly 
complicated, it is very likely that this exceeds the modeling and simulation 
capabilities. 
7.2.2. Models of human behavior 
7.2.2.1. Human behavior modeling in the control loop 
Modeling human behavior in control of systems has a very long history 
of development, and it is a continuously evolving process. This is natural 
and acceptable given that the technological component of the HMI is in 
permanent evolution. On the other hand, the basic principles of cognition are 
deeply rooted in the history of mankind and relate to the philosophical 
concept of human beings in relation to nature and environment. 
This continuous state of evolution and renovation is the fascinating aspect 
of the field of human studies, but it is also its major drawback, as the 
impression that a superficial “visitor” of the domain may get is of lack of 
consolidation, given the enormous variety of models and simulations that 
exist. This impression is due to the several forms of “implementation of 
models”, i.e. “simulations” that can be found in the literature. However, this 
is rather misleading, because, looking carefully it is clear that the basic 
principles that sustain models of cognition are essentially limited and 
consistent with one another. 
In general, two broad types of human behavior models can be 
distinguished in the literature. The first type are defined as “descriptive 
models.” These models attempt to describe parts or the whole of the task in 
terms of what the operator has to do. The second major type are 
“motivational models,” which aim to describe how the operator manages risk 
or abnormal situations [CAR 07]. All models that have been developed 
focused on the cognitive part of behavior, not only on the actual performance 
of actions, can be included in one of these two major families or can account 
for both aspects. More precisely, a complete model of cognition needs to 
account for both aspects of cognition, as the way in which decision-making 
and action takes place is always a mixture of a great variety of contributors 
of a descriptive as well as motivational nature. 

Designing Driver Assistance Systems in a Risk-based Process     127 
The decisive reason for selecting this or that model and simulations that 
are available in the literature is the type of technology that the operator is 
using in a control process. For this reason, it is important to consider the 
“level” of automation operating in the HMS under consideration. The five 
levels of “supervisory control” defined in the 1990s by Sheridan are still a 
valid paradigm of valuable representation of this concept (Figure 7.2). The 
levels follow the idea of the control loop that governs the “humans-in-
control-of-machines” and contemplate the so-called “manual control”, 
“formal supervisory control” and “fully automatic control” [SHE 97]. 
– At the manual control level, two control modes are envisaged: simple 
manual and manual control supported and amplified by technological means. 
All power-assisted operations belong to this level. These two modes or loops 
are depicted in Figure 7.2 as the two most external loops that either do not 
pass through the use of the “computer” or simply utilize the computer to 
enhance the manual actions carried out by the operator. Examples of these 
two levels of control are the direct braking as is done on basic bicycle brakes 
and power-assisted steering or braking systems on automobiles. 
– At the “formal supervisory control” level, two control modes are also 
present. In the open-loop mode, the computer/automatic system acts on the 
machine essentially via the operator activity. This is represented by the inner 
loop in Figure 7.2, with the open loop shown as the dotted line of cycling 
within the computer and the solid lines connecting the computer to the 
human operator via the display and controller systems. Whereas, in  
the strictest sense of supervisory control, the closed-loop mode implies that 
the automation acts independently on the machine according to the evolution 
of performances and with minor adjustments from the operator. This is 
shown in Figure 7.2 with the solid line closing the loop of activity within the 
computer and the dotted lines connecting the computer to the human 
operator via the display and controller systems. Examples of these two types 
of supervisory control systems, in the automotive domain, are the lane 
departure warning system designed to warn a driver when the vehicle begins 
to move out of its lane, the antilock braking systems (ABSs) or the traction 
control system. 
– At the fully automatic control level, the interaction between humans 
and machine occurs in two separate steps. The operator selects the tasks to 
be performed and then the interaction occurs essentially via the display that 
shows the automatic implementation of the selected tasks and relative 

128     Risk Management in Life-Critical Systems 
adjustments as the system evolves in the environment, until the task is 
completed and a new task has to be selected. 
 
Figure 7.2. Sheridan’s five levels of “supervisory control” (adapted from [SHE 97]) 
Examples of this type of control are the intelligent parking assist system 
or the automatic cruise control system. Even if this concept of levels of 
automation is relatively old and several other ways of implementation have 
been developed, the architecture of HMI described is very clear and depicts 
the problem that RBD processes require perfectly well [CUM 04]. 
In particular, the consideration for cognitive processes is essential for 
describing the Human–Machine Interaction when modeling supervisory 
control or fully automatic control. Given that modern systems are based on 
the higher levels of control, it follows that the human models of “descriptive 
nature” must account accurately for the mental process as well as for the 
more behavioral ones. The literature of the last 30 years is full of many 
different modeling architectures and associated computerized simulations 
that account in more or less depth and accuracy for the cognitive processes 
and functions that enable us to describe the process of decision-making and 
action of operators [WIK 00, SAL 06, BOY 11]. 
7.2.2.2. The motivational aspects of human modeling 
The motivational aspects also play a critical role in human modeling  
as they focus on specific aspects of psychological and behavioral 
characteristics that influence the occurrence of errors. In this sense, they are 

Designing Driver Assistance Systems in a Risk-based Process     129 
as important as the descriptive models for the RBD processes, since they 
affect the occurrence of events and human failures that are typical of 
incidental paths. 
In general, a model of human behavior that influences the RBD process 
should be able to reproduce the inadequate performances that are the  
cause of or the actual hazards to be assessed. An example of a possible 
modeling architecture of this general nature is shown in Figure 7.3,  
where the motivational part of the model shows the individual and 
psychological contributors to the development of intentions. These are then 
filtered by the cognitive processes and functional performances leading to 
actual HMIs. 
 
Figure 7.3. A generic operator model (adapted from [CAR 07]) 
From a simple engineering and risk assessment perspective, the model of 
the interaction between humans and machines is an evolutionary variety of 
the model proposed in the 1980s by Rouse [ROU 80], that considers  
three basic components, i.e. the “human”, the “machine” and the 
“environment” 
interacting 
in 
a 
dynamically 
evolving 
architecture  
(Figure 7.4). This representation is extremely simple, but it contains in a 
single picture the beauty of the reality and the extreme complexity of its 
analytical processes. 

130     Risk Management in Life-Critical Systems 
 
Figure 7.4. Essential nature of human–machine interaction 
7.2.3. Models of error and taxonomies 
The main functional requirement of the model of HMI in terms of safety 
assessment is that it should consider threats and human inadequate behaviors 
in a formalized format. The error models are obviously strictly correlated to 
the human behavior models and to the level of control associated with the 
HMS under study. It is equally important to note that, from the HMI process, 
the changes in a sequence of events are always associated with a 
performance, either correct or inappropriate. This implies that an error is 
recognized only when it generates inadequate performances, even if it is 
born at higher cognitive level and possibly at a different time frame than the 
instant of the manifestation. 
In an RBD process, it is, therefore, very important to be able to quantify 
the likelihood of occurrence of inappropriate behavior, possibly merging 
different causes and different cognitive process that may lead to the same 
“behaviour”. 
This premise is important, as it helps in selecting the best suitable 
“taxonomy” of human error that is combined to the model of human 
behavior in an RBD process. The use of a taxonomy of human errors or 
inappropriate behaviors, i.e. a structured categorization of error types, modes 
etc., is absolutely essential. The taxonomy must be balanced with the 
selected HMI model and should serve the purpose of defining the human 
hazardous performances that are studied in a risk analysis perspective. 
Several taxonomies of human errors are available in the literature. As 
an example, CREAM, associated with the cognitive model Contextual 
Control Model (COCOM), is rather complex and is primarily aimed at 
selecting the detailed root causes, at cognitive function level, of error 
behaviors [HOL 98]. This type of analysis can be called a “microscopic” 

Designing Driver Assistance Systems in a Risk-based Process     131 
root cause analysis approach. Whereas, a taxonomy based on simple 
definition of errors of “omission” and “commission” is associated with a 
“macroscopic” root cause analysis approach, for example, the HRA 
technique THERP [SWA 83]. 
In practice, a “microscopic” approach is applied for capturing the 
dynamic cognitive processes and functions involved, considering a 
manifestation of behavior (phenotype), and enables us to search for the 
cognitive causes and to identify generic and specific causes that engendered 
certain “errors”. A “macroscopic” approach, instead, focuses on factors that 
affect the occurrence of an error rather than the cognitive processes 
involved. These are usually characterized by individual characteristics, 
environmental conditions, quality of instruments, available procedures and 
teamwork aspects. 
In general, for an RBD approach the error model should consider error 
types and error modes, where “error types” are generic descriptions of 
possible error performances both at cognitive and behavioral level, and 
“error modes” are specific quantified forms taken by error types in a  
certain working and operational context. Therefore, error modes are subsets 
of error types. In this way, it is possible to quantify the human error 
performance and then to develop a measure of the likelihood of their 
occurrence, usually in terms of a probability of occurrence. At this point, it 
becomes possible to incorporate the results in the RBD approach for the 
overall risk assessment. 
This process of quantification is extremely complex and difficult. The 
methods and approaches available in the literature are consolidated and 
extremely varied. The most valuable approaches are based on historical data 
collected either within the organization or in similar environments. The 
analysis of these data generates the culture and educated approach for 
estimating prior probabilities about similar events. This process of 
assessment of basic error probabilities using expert judgment (EJ) is 
extremely popular and utilized by almost all organizations with very 
different levels of accuracy. It presents a risk in itself, as it could result  
in a bias forced by the pressure placed on the safety analyst to assess  
the safety of existing designs and systems aiming only at obtaining the  
safety certification, instead of a really effective improvement in terms of 
safety. 

132     Risk Management in Life-Critical Systems 
7.2.4. Dynamic nature of needs 
The overall human factors contribution to an RBD approach is completed 
when all components depicted in Figure 7.4 are accounted for from a risk 
assessment perspective. The models of the human have been screened in 
detail in the previous sections. In an RBD approach, the two other 
components, i.e. the machine and environment, are accounted for as 
reliability response of technology (machine) and as conditions affecting 
technical and human performances (environment). 
This is already a complex architecture, especially if detailed models are 
considered. However, the overall framework is further complicated by the 
consideration of the remaining contributor to the overall HMI architecture, 
i.e. time. 
The consideration for the time-dependent evolution implies that the 
sequence of interactions and system responses has to be somehow accounted 
for when developing the risk assessment derived from an initiating 
hazardous condition. This is a typical dynamic reliability process, which is 
notoriously complex and difficult to resolve. Two reasons contribute to this 
complexity: primarily, the intrinsic complexity of the reliability problem 
associated with dynamic process; second, the scarcity of reliability data 
already identified in the case of human reliability. This is further 
complicated by the need to use dynamic-dependent probabilities of 
failures/errors and recoveries due to changing environments and working 
conditions. 
In practice, in the evolution of a sequence of events that follow the 
occurrence of an initial hazard, the failure of a component or the error of the 
operator depends on the way in which the human/machine/environment 
responds to their interconnected activities. In order to manage this process in 
a formal analytical way, from the human behavior perspective, two 
simplifying assumptions are made: 
– the generation of human error is associated with a single variable, 
called error propensity (EP), that represents the tendency toward error 
making and violating that may be generated by impairment, as well as by 
other possible conditions typically associated with the motivational aspects 
of human modeling [CAC 10]. Examples of these conditions are: 
 
 

Designing Driver Assistance Systems in a Risk-based Process     133 
distractions, lack of knowledge, intentional actions etc., In practice, the EP  
represents the build-up a set of conditions, either personal or 
environmentally dependent, or the sudden generation of an extreme situation 
that provokes the human error. However, it is not known a priori when the 
EP level will reach its threshold value, and therefore only probabilistic 
estimates of the error generation are made in risk analysis perspective; 
– the time variable is discretized in intervals of variable span, so as to 
“group” events that normally occur at different time instants. In this way, a 
structure is created similar to a standard Event-Tree format, even if the 
number of possible failure modes of the component/subsystem/human 
operator is flexible. 
A pictorial representation of the EP evolution and generation of dynamic 
sequences of events is shown in Figure 7.5. Starting from a “nominal 
sequence” (Seq0), i.e. the sequence where no human errors occur, a variety of 
other sequences are generated when errors are generated at different time 
instants, in correspondence with the attainment of EP values greater than the 
threshold value. The different dynamic sequences are identified by the 
number of errors that are encountered and the connection that they show 
with their generating sequence of events. The number of branches is 
associated with a precise sequence identifier. The following descriptions 
help in understanding the selected formalism: 
– Seq01 considers only one error generated at time t3, alongside the 
performance of the nominal sequence (Seq0), and continues until the end of 
the analysis with no other errors; 
– Seq011 considers two errors; the first error generated at time t3, alongside 
the performance of the nominal sequence (Seq0), and the second error, at 
time t4 and continues until the end of the analysis with no other errors; 
– similarly, Seq21 considers two errors, generated at time t1, generating 
the Seq2, and then, at time t2, when the second error is considered. No other 
errors occur till the end of the sequence.  
The case of three errors occurring in the same sequence is represented, 
for example, by Seq111, where the first error occurs at time t1 and Seq1 begins, 
followed by the second error, at time t2 and Seq11 starts, and then by the third 
error, at time t3 and Seq111 starts. 

134     Risk Management in Life-Critical Systems 
7.3. A quasi-static methodology 
7.3.1. The methodology 
The methodology aims at structuring the dynamic conditions, generated 
by the evolution of the HMI process and working environment, and the 
occurrence, in probabilistic terms, or human errors and failures of the 
technology. The number of potential sequences produced simply by human 
errors is depicted in Figure 7.5. 
 
Figure 7.5. Error propensity (EP) and dynamic generation of sequences 
In the area of safety and reliability assessment, the issue of dynamic 
aspects is well known and various approaches can be found in the literature, 
sometimes grouped under the umbrella of discrete dynamic event tree 
methodology  [ACO 93, CAC 94, MAC 94]. This issue is still nowadays the 
object of serious attempts to adequately account for the time dimension 
[HAK 08]. 
From this perspective, and in order to develop a methodology that is 
applicable in practice, a number of simplifications must be introduced. 
Primarily, the error modeling architecture can be simplified by discretizing 
the intervals of observation of errors so as to maintain the dynamic 
characteristics of the approach, but in a stepwise rather than a continuous 
manner. Consequently, the resulting RBD methodology can be defined as a 
quasi-static technique, in the sense that it sets specific time intervals at which 
errors can be introduced, while still preserving some time dependence in the 
HMI process. This can be seen as a compromise between unrealistic static 

Designing Driver Assistance Systems in a Risk-based Process     135 
reliability models and fully dynamic approaches that cannot be implemented 
with the current level of methods and implementation tools. 
Second, the models of behavior and associated taxonomy of errors have 
to account for more error modes than the simplistic binary alternative of 
success versus failure. However, it is important to account for the fact that 
the same level of detail is maintained in the assessment of the failures of 
technology and human errors. Therefore, when a highly detailed assessment 
is required, then appropriate approaches and data have to be utilized. 
Otherwise, it is not necessary to be very detailed and precise in the human 
error assessment when the precision required for the reliability of technology 
is weak. Under these hypotheses, the general structure of the proposed quasi-
static methodology is shown in Figure 7.6.  
 
Figure 7.6. General structure of the quasi-static methodology for RBD 
The steps of the methodology are consistent with the generic RBD 
flowchart discussed earlier (Figure 7.1) and can be briefly summarized as 
follows: 
1) Scenario definition. In general, a scenario can be defined as the set of 
elements that represent a situation, including a dynamic evolution of 
environment and machine that may affect the overall sequence of events, 
independently of the human operator behavior. 

136     Risk Management in Life-Critical Systems 
2) Definition of event and IH. 
– In general, an event is characterized by a change in the conditions of the 
system that modifies the system state, characteristics and performances. 
– In terms of human factors, an event can be defined as an action that 
changes the system state, characteristics and performances. 
– An IH or IE is a special kind of event. In fact, events are generally 
structured in a time sequence, usually referred to as event time line (ETL). 
The IE is the event that, within an ETL, is considered as the triggering point 
of the sequence. 
3) Creation of a structured or expanded event tree (EET). This enables us 
to account for discrete number of sequences, and therefore introduces the 
concept of quasi-static approach. This qualitative step can be further 
subdivided into two different tasks: 
– identification of systemic events that relate to possible operational 
failures of the technology; 
– identification of operator events that rely to observable human 
behavior. 
These two tasks are performed by means of standard qualitative methods 
coupled, very frequently, with the use of the judgment of experts. 
4) Integration of probabilities and quantification. At this stage, the HMI 
models and simulations and the error models and taxonomies are utilized in 
order to quantify the likelihood of occurrence of certain sequences. 
The evaluation of probabilities can be performed in one of the following 
ways: 
– by EJ;  
– by HRA methods and associated taxonomies; 
– by human error models also coupled with their specific taxonomies. 
5) Evaluation of consequences. This is performed by means of HMI 
models and simulations. Also in this case, three possible ways of 
implementation are envisaged: EJ, simulations of HMI and field 
experiments. 
6) Risk assessment. This is a formal step of combination of the 
consequences with the likelihood of occurrence of the various sequences 

Designing Driver Assistance Systems in a Risk-based Process     137 
derived from the IE. The results are confronted with standard tables of 
acceptability of the risks and enable the final decisions about the acceptance, 
or not, of the hazards under study. 
7.3.2. The expanded human performance event-tree 
The proposed approach expands over the classical event tree and 
considers an alternative to the simple binary possibility success versus 
failure, by enabling the possibility of different performances at each branch 
of the tree. The expanded event-tree concepts were originally developed in 
the 1990s [MAC 93]. 
Focusing on the human factor contribution, the method is called 
expanded human performance event tree (EHPET) [CAC 12]. Figure 7.7 
shows the structure of the EHPET, where a variety of possible sequences are 
depicted, following the Initiating Event or Hazard (IE or IH). According to 
the standard formalisms of event tree approach, the alternative at the top of 
each step represents the most common expected performance. 
The sequences are built by combining the IE and subsequent error 
performances. The overall probability of a sequence is calculated by 
combining the probability of the IE and the probabilities of the errors that 
occur in that sequence. The formalism already discussed in relation to the 
dynamic generation of sequences (Figure 7.5) is maintained. The events are 
organized into families, and they are numbered according to the sequence in 
which they occur. Therefore, the first set of possible human errors or correct 
performance that may occur, following the IE, belongs to the event family 1, 
and so on. Each branch is numbered according to two criteria: the number of 
the event family it belongs to and a sequential number inside each event 
family. The EHPET shown in Figure 7.7 represents the same set of 
sequences depicted in more generic terms in Figure 7.5. A more detailed 
discussion on the EHPET can be found in [CAC 12]. 
The probabilities (p) of the various alternative modes of behavior and 
human error are evaluated, as discussed above, either according to the 
experience of the safety analysts, thus implementing EJ, or are evaluated 
utilizing an HRA method. 
 

138     Risk Management in Life-Critical Systems 
 
Figure 7.7. Expanded human performance event tree (adapted from [CAC 12]) 
Following the formalism of statistical analysis, the probabilities of the 
sequences defined in the EHPET of Figure 7.7 are calculated as follows: 
 ݌ሺSeq଴ሻൌ݌ሺIEሻ∗pሺܤଵ.ଵܫܧ
⁄
ሻ∗ p(ܤଷ.ଵܤଵ.ଵ
⁄
∩ܫܧሻ∗ p(ܤସ.ଵܤଷ.ଵ
⁄
∩
ܤଵ.ଵ∩ܫܧሻ∗∗݌ሺܤହ.ଵܤସ.ଵ
⁄
∩ܤଷ.ଵ∩ܤଵ.ଵ∩ܫܧሻ 
݌ሺSeq଴ଵଵሻൌ݌ሺIEሻ∗pሺܤଵ.ଵܫܧ
⁄
ሻ∗ p(ܤଷ.ଶܤଵ.ଵ
⁄
∩ܫܧሻ∗  
                                   p(ܤସ.ସܤଷ.ଶ
⁄
∩ܤଵ.ଵ∩ܫܧሻ 
݌ሺSeqଵሻൌ ݌ሺIEሻ∗݌ሺܤଵ.ଶܫܧ
⁄
ሻ∗ p(ܤଶ.ଵܤଵ.ଶ
⁄
∩ܫܧሻ 
݌ሺSeqଶଵଵሻൌ ݌ሺIEሻ∗݌ሺܤଵ.ଷܫܧ
⁄
ሻ∗ p(ܤଶ.ସܤଵ.ଷ
⁄
∩ܫܧሻ∗ 
݌ሺܤଷ.଺ܤଶ.ସ
⁄
∩ܤଵ.ଷ∩ܫܧ 

Designing Driver Assistance Systems in a Risk-based Process     139 
7.3.3. Evaluation of consequences and risk assessment 
After the association of probabilities to both machine and operator events, 
the final two steps of the RBD methodology are the evaluation of 
consequences and related severity and the definition of the risk associated 
with the studied IEs or hazards (Figure 7.6). Both steps are crucial for the 
assessment of the safety level attained by the HMS. 
7.3.3.1. Evaluating severities and consequences of sequences 
The classification of severity and related definition utilized for the RBD 
approach associated with certain families of HMSs can be found in the 
literature and can vary according to the domain of application. Usually, the 
various levels of severity are defined by the competent safety authorities at 
national and transnational level. 
In general, the severity is usually associated with the damage that an 
incident causes to human beings within or surrounding the HMS and/or to 
the environment and/or to the HMS system itself. 
As an example, the international classification used in aviation domain 
considers five classes of severity, and it is possible to transfer the same 
concepts to the domain of interest. In particular, the following definitions 
can be applied: 
1) No safety effect: no serious damages to the machine and irrelevant 
injuries to operators/affected population who do not require hospitalization. 
2) Minor severity: minor damages to the machine and minor injuries to 
operators/affected population who require temporary hospitalization with 
full recovery in short time periods. 
3) Major severity: relevant damages to the machine and severe injuries to 
operators/affected population who require hospitalization with full recovery. 
4) Hazardous severity: severe damages to the machine and serious 
injuries to operators/affected population with substantial hospitalization 
treatment and possible permanent disabilities. 
5) Catastrophic severity: complete disruption of the machine and loss of 
life of operators/affected population. 

140     Risk Management in Life-Critical Systems 
A very accurate means for the evaluation of consequences and severity in 
RBD tool is the use of HMI models. The user can simulate each sequence of 
the EHPET by utilizing simulation platforms, which include the human 
models/simulations and their coupling with machine performances. 
Other means for the evaluation of the severity are (Figure 7.6): the use of 
EJ and/or the performance of experiments. The first alternative is usually 
fast and requires little effort if the number of experts utilized is limited. 
However, this procedure may generate large under or overestimates of the 
consequences, precisely as in the case of the probabilities. Moreover, the risk 
of “bounding” the judgment to the needs of certification is a very serious 
bias to be accounted for when EJ is utilized. The experimental approach for 
assessing the severity of certain occurrences is certainly a very useful and 
reliable method. In the domain of automotive safety, this approach is very 
consistently and extensively utilized. However, in other domains, such as 
aviation, it is much more difficult and costly to implement. The simulation 
by means of computerized implementation of models of HMI remains, 
therefore, the most valuable way to assess the consequences of hypothetical 
hazards and sequences of events. 
7.3.3.2. Assessment of the risk 
In order to estimate the risk in RBD methodology, it is necessary to 
associate a risk category to each numerical value of likelihood, i.e. the 
probability of each sequence, p(Seqi), calculated according to the EHPET.  
For this purpose, different considerations can be performed and different 
benchmarks can be used. As for the case of severity, making reference to the 
domain of aviation, the following categories of likelihood can be considered:  
1) Frequent: an event that may occur more than once per week. 
2) Reasonably probable: an event that may occur more than once per 
month, but less than once per week. 
3) Remotely probable: an event that may occur more than once per year, 
but less than once every month. 
4) Extremely remote: an event that may occur more than once every 100 
years, but less than once per year. 
5) Extremely improbable: an event that may occur more than once every 
1000 years, but less than once per 100 years. 

Designing Driver Assistance Systems in a Risk-based Process     141 
Another possibility is to define the categories of probability based on the 
number of flight hours or hours of operation or miles driven etc. 
Finally, the last step of RBD methodology is identified with the 
estimation of risk. The risk can be described as a state of uncertainty  
where some of the possibilities involve a loss, catastrophe or other 
undesirable outcome. It is the product of the frequency or probability (p) of 
occurrence of a sequence of events following an IH and the potential 
criticality or severity (S) of the resulting harm or damage: 
*
R
p
S
=
 
The measure of risk can be obtained by combining frequency and 
consequence using a risk matrix. An example of risk matrix utilized in the 
domain of aviation is shown in Figure 7.8. It can also be utilized in the 
domain of transportation or other domains as long as the relative definitions 
of frequency of occurrence and severity are applied. 
 
Figure 7.8. Generic risk matrix. For a color version of this figure, see 
www.iste.co.uk/millot/riskmanagement.zip 
In the risk matrix, three areas can be envisaged:  
– Red zone: this is the unacceptable risk area. If the evaluation of risk 
provides a value internal to this area, an immediate action is required. 
– Yellow zone: this is the medium risk area. If the evaluation of risk 
provides a value internal to this area, actions of revision and introduction of 

142     Risk Management in Life-Critical Systems 
means to reduce risk (changes and modifications) are required. In certain 
circumstances, however, the risk associated can be accepted, as for example, 
in the case of catastrophic events and extremely improbable probabilities of 
occurrence. 
– Green zone: this is the low risk area. If the evaluation of risk provides a 
value internal to this area, no formal action is required, even if in some cases 
the designers and safety analysts should decide anyway to do something, for 
example, in the case that the frequency of occurrence is approaching the 
limit value for entering the yellow area. The risk matrix, adapted to the 
values and intervals of severity and frequency discussed above, is an integral 
component of the RBD methodology that is applied for the evaluation of the 
safety and acceptability of an HMS. 
7.4. Implementation on board vehicles for driver assistance 
A technological domain that has enormously expanded over the years 
with respect to the extensive use of automation is the automotive system 
design. There are several reasons for this expansion, such as the vast amount 
of usage and commercial interest of automated instruments and technologies 
in the domain, the impact on safety, given the quantity of accidents that 
occur every year and the actual number of population involved. These 
aspects are matched with the demand of users for constantly improved 
technology and of the authorities for greater safety measures, able to reduce 
the number and severity of incidents and accidents. 
The European Union, as an example, has been very active in promoting 
measures to improve traffic safety, through the European Road Safety 
Action Programme, focusing on research and development activities. As in 
all technologically advanced domains, the key factor resulting from accident 
analysis is the human contribution, or human factor, to accident generation 
and/or development. Examples of commonly made errors are: slipped-off 
brake, excessive steering, foot-off-brake, no hand brake, avoiding animals or 
vulnerable road users, wrong speed evaluation, traffic sign overlooked, 
distraction, workload, etc. 
In this context, advanced driver assistance systems (ADASs) can play a 
major role in road safety. Many ideas have been developed and applied in 
 
 

Designing Driver Assistance Systems in a Risk-based Process     143 
prototypes with safety features, and several systems have been then 
introduced in series production. The actual market of ADAS suffers from  
existing technical limits as well as liability issues. In particular, the 
development and introduction of ADASs supporting and/or substituting the 
drivers in their driving task, with a complex interaction between system and 
driver in multiple traffic situations [SCH 13]. 
In addition, new challenges for system safety are due to the complex 
functionality, and the possible new types of errors, e.g. software errors in 
development, HMI and communication issues due to mismatch between 
designers goal and user understanding, complex interaction and lack of 
experience and unknown user reactions to unexpected automation behavior. 
In this area, the experience of aviation can be very useful, as the advanced 
automation implemented in modern aircraft design technology, e.g. “fly-by-
wire” systems, can be very useful. 
When introducing ADAS in the market, there can be some consequences 
for a car-manufacturer and a supplier. In particular, the introduction of 
ADAS constitutes a possible business case on the one side, but there are also 
some risks on the other side. They are mainly based on possible brand image 
damage, if the ADAS does not meet consumer expectations, e.g. case of 
Mercedes Class-A initial performances, possible recall campaigns; or if 
ADAS does not meet consumer expectations or shows malfunctions, e.g. the 
case of Audi TT-Coupé, product liability; or if ADAS does not meet 
requirements of a safe product, e.g. case of Ford Firestone-SUV in USA. 
A Code of Practise (CoP), developed by the EU-supported Project 
RESPONSE 3, is tentatively providing some guidelines and answers to the 
basic questions of design and safety issues of ADAS [COT 06]. The CoP 
implies to establish basic principles for the development and evaluation of 
ADAS on a voluntary basis, as a result of a common agreement between all 
involved 
partners 
and 
stakeholders, 
mainly 
initiated 
by 
ADAS 
manufacturers. The application of the CoP focuses on the procedures for 
ADAS design and development, including risk identification, risk 
assessment and evaluation methodology, including, in particular, “driver 
controllability”, as key issue. 
The concept “driver controllability” is defined as probability that the 
driver can cope with driving situations including ADAS-assisted driving, 

144     Risk Management in Life-Critical Systems 
system limits and system failures. In particular, “controllability” is related to 
the possibility and capability to: 
– perceive the criticality of the situation;  
– decide an appropriate countermeasure, e.g. override capabilities, given 
the complexity of the system switch-off procedure; 
– perform the selected countermeasure, in terms of reaction time, 
sensorimotor speed and accuracy. 
The CoP is intended to be applied to systems providing vehicle 
stabilization, such as ABS and electronic stability programme (ESP) , or 
mere information and communication systems, called “In-Vehicle 
Information Systems” (IVIS), such as navigation systems and telephones. It 
may be applicable to systems including vehicle-to-vehicle communication 
(V2V). 
More specifically, ADASs are regarded as a subset of the more generic 
driver assistance systems (Figure 7.9). In particular, the principal functions 
that can be carried out by ADAS are: 
– actively supporting and/or substituting the driver in the driving task; 
– longitudinal and/or lateral control influenced by the ADAS; 
– new dimension compared to pure information and warning systems; 
– direct control of brakes/steering etc.; 
– system failures may have direct effect to vehicle control. 
 
a) 
                                                 b) 
Figure 7.9. ADAS at level of driving task a) and temporal  
sequence of intervention b) 

Designing Driver Assistance Systems in a Risk-based Process     145 
7.5. A case study 
This case study shows the implementation of the RBD methodology for 
the case of an ADAS associated with a traffic light (TL) scenario. The driver 
vehicle, “ego vehicle”, is equipped with an advanced forward collision 
warning (FCW). The objective of the case study is to show the process of 
safety improvement and RBD assessment, provided by the introduction of 
this assistance system. The different steps defined in the RBD methodology 
(Figure 7.6) from the definition of the driving scenario to the risk evaluation 
are briefly discussed. 
7.5.1. Scenario definition 
The first step of the methodology consists of the definition of driving 
scenario. The scenario considered is the possible sequence of events 
occurring at an urban traffic light, in which the ego-vehicle (EV), while 
approaching the traffic light, is following another car, called leading-vehicle 
(LV). The EV has installed an ADAS system, namely the FCW, with the 
active functionality for the safety distance control. This ADAS gives a 
warning sound when critical distance/time headway to an obstacle is 
reached. If a further level of criticality is reached, then emergency braking 
(EB) is automatically performed by the FCW. 
More in detail, the TL scenario presents the following features:  
– the scenario is urban, where there is a general speed limit of 50 km/h; 
– the road is straight, single carriage with one lane in each direction, 
surrounded by pavements; 
– the TL is installed on the straight road, directly at a four-way-
intersection; 
– the TL will always change from green to yellow when the time 
headway of the EV to the TL becomes smaller than 4 s; 
– the TL becomes red 3 s later, after having turned to yellow; 
– no other cars are present on the road except the EV and LV; 
– the LV is traveling at the speed limit of 50 km/h; 

146     Risk Management in Life-Critical Systems 
– before the light switches from green to yellow, the EV adapts its speed 
in order to maintain the distance between itself and the LV within a given 
interval of 20 or 30 m; 
– the LV brakes when the light turns to yellow and stops; 
– a set of data for this type of scenario is shown in Table 7.1. 
Object 
Variable/behavior 
Value 
Ego vehicle 
(EV) 
Speed 
v (EV) = 14 m/s 
Acceleration 
a (EV) = 0 
Initial distance between EV and TL 
d0 (EV, TL) = 90 m 
Initial distance between EV and LV 
Random variable 
(Gaussian distribution) 
Mean = 20 m 
Sigma = 5 
Minimum distance between EV and LV 
min (d(EV, LV)) = 20 m 
Maximum distance between EV and LV 
max (d(EV, LV)) = 30 m 
Lead vehicle 
(LV) 
Speed 
v (LV) = 14 m/s 
Acceleration 
a (LV) = 0 
Behavior at the TL 
Goes or stops 
Traffic light 
(TL) 
From green to yellow 
t (EV, TL) <4 s 
From yellow to red 
3 s later 
Table 7.1. Possible data for the traffic light scenario 
7.5.2. Initiating event 
In general, an event is characterized by a change in the conditions of the 
system that modifies the system state, characteristics and performances. An 
IE is the event that, within an ETL, is considered as the triggering event of 
the sequence. In the case study, the IE is: 
– The LV that stops at the TL. 
– The mission of the EV is then to stop beyond the LV. 

Designing Driver Assistance Systems in a Risk-based Process     147 
7.5.3. Development of the expanded event tree 
The first step of the development of the EET is the qualitative analysis of 
the sequence of events following the IH in the overall scenario. 
7.5.3.1. Event characterization  
Three event families related to the driver (human errors) and three event 
families related to the ADAS (technical malfunctions) are defined. The 
reason for defining “families” of events is associated with the fact that only 
generic “types” of errors/malfunctions are identified at this stage of the 
analysis. They are measured in fuzzy terms. In a subsequent step, when  
the quantification will be developed, the error/malfunction “types” will be 
formalized in precise “modes” of appearance.  
Three event “families” occur before the ADAS becomes operative, two 
associated with human errors and one with ADAS functionality. Three event 
“families” occur after the ADAS becomes operative, one associated with 
human errors and two with ADAS functionality. 
7.5.3.1.1. Driver event families 
The three human event families are: 
– Human event family 1: the driver does not brake early (before ADAS 
intervention). 
– Human event family 2: the driver does not brake hard enough (before 
ADAS intervention). 
– Human event family 3: the driver does not react to the ADAS warning 
(after ADAS intervention). 
7.5.3.1.2. ADAS event families 
The optimal policy for an ADAS, such as the one considered in this case 
study, defines thresholds, based on the time to collision (TTC), defined as 
the time remaining before collision if nothing changes in the situation. 
The envisaged different functionalities of the FCW are: warning, 
assistance braking (AB) and automatic EB. They are activated in sequence 
and in relation to the driver behavior measured in terms of TCC. 

148     Risk Management in Life-Critical Systems 
The ADAS event families related to possible failures of the FCW are 
identified as follows: 
– ADAS event family 1: the FCW does not work; 
– ADAS event family 2: the FCW AB does not work; 
– ADAS event family 3:  the FCW EB does not work. 
7.5.3.2. Preliminary screening of event families 
7.5.3.2.1. Driver event families 
The variable used to characterize the reaction time of the driver, 
associated with events of the family 1, is the TTC. The choice of this 
variable is motivated by the fact that it takes into account the emergency 
nature of the situation and relates also to the ADAS in operation. The TTC 
takes into account the speed and distance of the vehicle and is used in the 
FCW policy to define thresholds for the functionalities and activation. In 
particular, it is assumed that three possible reaction times are to be studied: 
the driver breaks early, late or not at all. These will be characterized by the 
value of the TTC when the driver reacts. 
To discriminate the event “strength of braking”, i.e. the second family of 
human-related events, the variable of interest is the “deceleration” of the EV. 
As the instantaneous deceleration is not representative of the global behavior 
of the EV driver, the considered variable is the mean deceleration. This 
enables us to characterize the various events defined in terms of 
deceleration. 
As an example, three possible strength of braking are accounted for: 
braking hard at 4 m/s2, braking normally at about 3 m/s2 and braking softly 
at about 2 m/s2. In order to simplify the calculation and the development of 
the EET, the cases of driver not reacting at all (third alternative of human 
event family 1) or braking normally (second alternative of human event 
family 2) have not been considered. 
The third family of human-related events considers only two alternatives, 
as the driver may or may not react to the FCW warning. 
7.5.3.2.2. ADAS event families 
ADAS events are characterized by the activation or not of the different 
functionalities (warning, AB and EB) in association with the TTC. The 

Designing Driver Assistance Systems in a Risk-based Process     149 
different functionalities of the FCW: warning, AB and automatic EB. They 
are activated at the thresholds as follows: 
FWC action  
Time to collision  
PADAS off: 
TTC > 5 s  
Warning: 
3 s < TTC ≤ 5 s  
AB: 
0.5 s < TTC ≤ 3 s  
EB: 
TTC ≤ 0.5 s  
The event related to the reaction of the driver to ADAS warning is 
characterized by the value of the additional deceleration. It has been 
assumed that if the AB works and if the driver reacts to the warning then the 
overall applied deceleration is sufficient to safely stop the EV before 
collision.  
7.5.3.3. Qualitative EET 
Figure 7.10 presents a simplified version of the EET (and EHPET) 
associated with the case study. This tree gives a qualitative view in terms of 
structure. It illustrates the sequence of driver and ADAS events. Due to the 
simplifying assumption, Figure 7.10 contains a limited number of 
alternatives. Whereas, the complete EHPET that considers all human 
performances contains 51 sequences. 
7.5.4. Probability assessment 
Different procedures and techniques are used for the estimation of the 
probabilities associated with different driver events and ADAS malfunctions. 
As mentioned earlier, different techniques and HRA methods can be 
utilized for the identification of human error probabilities and systemic 
failures. Given the demonstration nature of case study discussed here, no 
specific treatment is discussed for the probabilities associated with different 
failures and errors. 
The probabilities of the various sequences, i.e. p(Seqij), are calculated by 
multiplying the dependent probabilities of each node of the event tree 
according to the equations discussed earlier. In order to set a level of 
frequency to be utilized in the risk matrix applied for the case under study, 
the safety analyst has to define discrete intervals of probability of occurrence 
and associate them with measures of frequency of relevance.  

150     Risk Management in Life-Critical Systems 
An example of possible intervals for the domain of automotive transport is: 
Frequency level 
Probability intervals 
Frequent: 
p ≥ 0.1 
Reasonably probable: 
0.1 > p ≥ 0.01 
Remotely probable: 
0.01 > p ≥ 0.001 
Extremely remote: 
0.001 > p ≥ 0.0001 
Extremely improbable  
p < 0.0001 
 
Figure 7.10. EHPET for the case study with ADAS 

Designing Driver Assistance Systems in a Risk-based Process     151 
7.5.5. Consequence evaluation 
The second part of the quantitative analysis consists of the evaluation of 
the consequences. The consequence is the measurable amount of damage 
produced in an incident. The severity of a sequence is a measure of the 
consequences in terms of, for example, injuries to passengers or to people 
external to the vehicle and damage to the environment and vehicles 
involved. The corresponding discrete classes of severity, that enable to 
identify the second axis of the Risk Matrix, can be assigned as follows: 
Severity level 
Severity classes 
Minor damage to vehicles 
No safety effect: 
Minor injuries to persons 
Minor 
Severe damage and injuries to persons 
Major 
Severe injuries/damages to persons and vehicles 
Hazardous 
Death of person(s) and disruption of vehicle(s) 
Catastrophic 
In this case study, the criterion used to define the severity level is based on 
the speed at which a collision takes places. Other criteria can be used like 
calculating the damage using a crash model. A comprehensive review of the 
biomechanics of impacts in road accidents [MAC 97] makes it clear that there 
is no simple relationship between impact severity and the severity of injuries 
sustained by road users. In other references [ELV 04], a pedestrian is 
unprotected and may sustain fatal or serious injuries at impact speeds as low as 
30 or 40 km/h. Whereas, a well-protected occupant of a modern car would in 
most cases not be injured at all at a similar impact speed in a frontal crash. If, 
on the other hand, the car is struck from the rear, whiplash injuries leading to 
long-term impairment may occur even at impact speeds of 15–20 km/h. 
Indeed, the consequences depend on many factors. For example,  
the category (class) of the vehicle, the number/age of passengers and the 
position of the driver when the crash occurs, etc. The kinetic energy can be 
introduced to take into account, for example, the class of the vehicle that can 
be defined by its weight. A possible correspondence between speed for each 
level of severity could be: 
Severity  
Relative speed of collision 
No safety effect: 
0–15 km/h 
Minor: 
15–30 km/h 
Major: 
30–50 km/h 
Hazardous: 
50–70 km/h 
Catastrophic: 
>70 km/h 

152     Risk Management in Life-Critical Systems 
Given these classes of severity and the selection of the collision speed as 
a physical measure of severity, the evaluation of the consequences is based 
on the results of the simulation of the scenario given the IE and the various 
sequences defined by the errors/malfunctions selected for building the EET. 
For each sequence, the worst case is considered and is determined by 
considering the maximum value of the speed when crashing occurs. 
7.5.6. Risk evaluation 
The last step of RBD methodology is the estimation of risk. The risk is 
the combination of the frequency of occurrence of all sequences generated 
by the selected hazardous events and the potential severity of the harming 
persons and/or damaging environment and vehicles. The risk matrix, 
developed on the basis of the severity and frequency levels defined above, 
enables us to classify the risk of each sequence, considering the different 
zones. 
If some of the sequences end in the red zone it is necessary to revise the 
design and reduce the risk either by improving the number and quality of the 
safety measures that enable us to reduce the probability of occurrence and/or 
by introducing consequential barriers that affect the consequences. This is in 
line with the generic methodology of RBD (Figure 7.1). When the 
consequences are in the yellow zone the decision about acceptance or not of 
the risk depends on various elements and, when the risk is accepted, 
adequate justification must be developed in support of the decision so as to 
satisfy the safety authority criteria. 
The search for further risk reduction and the decision-making loops 
continue until the designers and safety analysts reach an agreement on the 
safety level and cost-benefit feedback of the selected design solutions. 
7.6. Conclusions 
This chapter has discussed the issue of RBD in modern technology 
perspective. Indeed, RBD is a well-formalized and consolidated 
methodology implemented in design process. However, modern technology 
 
 
 

Designing Driver Assistance Systems in a Risk-based Process     153 
and the extensive use of automation require the consideration for very 
relevant aspects that are usually neglected in risk analysis; i.e. the temporal 
and logical relationship between events, especially associated with human 
factors. 
This chapter has attempted to show how it is possible to integrate in a 
classical RBD flowchart the time dimension, by developing a quasi-static 
approach. The methodology makes use of classical methods and some 
innovative features, such as the EHPET, for building sequences of events 
that enable us to consider the HMI in a much more logical and functional 
relationship that the simple event tree standard method. The case study 
developed for an application in the automotive domain has shown the 
potentiality and effectiveness of the method. 
On the other hand, a certain level of complexity remains, especially in 
relation to the models that have to be developed and utilized for the 
assessment of the probabilities and consequences. Moreover, the question  
of validity and quality of the data and probabilities of occurrence of  
certain events, including recovery measures etc., remains a very important 
obstacle. 
For these reasons, while the methodology per se has an intrinsic validity 
when applied at qualitative level, it should be implemented very carefully 
when the risk, in terms of probabilities and severity of events and sequences, 
is performed and calculated. In these cases, it is important to make sure that 
a correct balance is maintained in terms of quantification of the various 
probabilities, human errors and systemic failures, and simulation models 
needed for the assessment of the consequences. The validity of the results 
rests indeed on the weaker part of the overall methodology. 
7.7. Bibliography 
[ACO 93] ACOSTA C., SIU N., “Dynamic event trees accident sequence analysis: 
application to steam generator tube rupture”, Reliability Engineering and 
System Safety, RE&SS, vol. 41, no. 2, pp. 135–154, 1993. 
[BAR 98] BARRIERE M.T., BLEY D.C., COOPER S.E., et al., Technical Basis and 
Implementation Guidelines for A Technique for Human Event Analysis 
(ATHEANA). NUREG – 1624, US-NRC, Washington D.C., 1998. 

154     Risk Management in Life-Critical Systems 
[BEL 09] BELL J., HOLROYD J., Review of human reliability assessment methods. 
Health and Safety Executive (HSE), UK. Research Report – RR679, 2009. 
[BOY 11] BOY G. (ed.), The Handbook of Human-Machine Interaction: A Human-
Centered Design Approach, Ashgate, London, 2011. 
[CAC 93] CACCIABUE P.C., HOLLNAGEL E., “Human models in reliability and 
safety analysis of interactive systems”, Proceedings of the International 
ANS/ENS Topical Meeting on Probabilistic Safety Assessment, PSA 93, 
Clearwater Beach, FL, American Nuclear Society, La Grange Park, Ill, 26–29 
January, pp. 25–31, 1993. 
[CAC 94] CACCIABUE P.C., COJAZZI G., “A human factor methodology for safety 
assessment based on the DYLAM approach”, Reliability Engineering and 
System Safety, RE&SS, vol. 45, pp. 127–138, 1994. 
[CAC 10] CACCIABUE P.C., CARSTEN O., “A simple model of driver behaviour to 
sustain design and safety assessment of automated systems in automotive 
environments”, Applied Ergonomics, vol. 41, pp. 187–19, 2010. 
[CAC 12] CACCIABUE P.C., CASSANI M., “Modeling motivations, tasks and human 
errors in a risk-based perspective”, International Journal of Cognition 
Technology and Work (CTW), vol. 14, no. 3, pp. 229–241, 2012. 
[CAR 07] CARSTEN O., “From driver models to modeling the driver: what do we 
really need to know about the driver?”, in CACCIABUE P.C., (ed.), Modeling 
Driver Behaviour in Automotive Environments, Springer, London, pp. 105–120, 
2007. 
[COT 06] COTTER S., HOPKIN J., STEVENS A., et al., “The institutional context for 
advanced driver assistance systems: a code of practice for development”, 13th 
World Congress & Exhibition on Intelligent Transport Systems and Services, 
London, United Kingdom, 2006. 
[CUM 04] CUMMINGS M.L., “Human supervisory control of swarming networks”, 
Proceedings of the 2nd Annual Swarming: Autonomous Intelligent Networked 
Systems 
Conference, 
Arlington, 
VA, 
2004. 
Available 
at 
http:// 
web.mit.edu/aeroastro/labs/halab/papers/cummingsswarm.pdf. 
[DRO 09] DROUIN M., PARRY G., LEHNER J., et al., Guidance on the treatment of 
uncertainties associated with PRAs in risk-informed decision making, NUREG 
1855, vol. 1, 2009. 
[ELV 04] ELVIK R., CHRISTENSEN P., AMUNDSEN A., Speed and road accidents. An 
evaluation of the power model, TØI report 740/2004, Institute of Transport 
Economics, TOI, Oslo, 2004. 

Designing Driver Assistance Systems in a Risk-based Process     155 
[FLE 03] FLEMING K.N., Issues and Recommendations for Advancement of PRA 
Technology in Risk-Informed Decision Making, NUREG 6813, 2003. 
[HAK 08] HAKOBYANA A., ALDEMIR T., DENNINGA R., et al., “Dynamic 
generation of accident progression event trees”, Nuclear Engineering and 
Design, vol. 238, no. 12, pp. 3457–3467, 2008. 
[HOL 98] HOLLNAGEL E., Cognitive Reliability and Error Analysis Method, 
Elsevier, London, 1998. 
[KAP 81] KAPLAN S., GARRICK B.J., “On the quantitative definition of risk,” Risk 
Analysis, vol. 1, pp. 11–37, 1981. 
[KIR 98] KIRCHSTEIGER C., CHRISTOU M.D., PAPADAKIS G.A., “Risk Assessment 
& Management in the Context of the Seveso II Directive,” Industrial Safety 
Series, no. 6. Elsevier, Amsterdam, The Netherlands, 1998. 
[IAE 11] INTERNATIONAL ATOMIC ENERGY AGENCY, A framework for an integrated 
risk informed decision making process, INSAG-25, Vienna, Austria, 2011. 
[ICA 12] INTERNATIONAL CIVIL AVIATION ORGANISATION, Safety Management 
Manual, Doc 9859, AN/474 3rd ed., Montreal, Canada, 2012. 
[LYO 05] LYONS M., WOLOSHYNOWYCH M., ADAMS S., et al., Error Reduction in 
Medicine, Final Report to The Nuffield Trust, The Nuffield Trust 2005. 
[MAC 94] MACWAN A., MOSLEH A., “A methodology for modeling operators errors 
of commission in probabilistic risk assessment”, Reliability Engineering and 
System Safety, RE&SS, vol. 45, pp. 139–157, 1994. 
[MAC 97] MACKAY M., “A review of the biomechanics of impacts in road 
accidents”, in AMBRÓSIO J.A.C.,  SEABRA PEREIRA M.F.O., PINA DA SILVA F., 
(eds.), Crashworthiness of Transportation Systems: Structural Impact and 
Occupant Protection, Kluwer Academic Publishers, Dordrecht, pp. 115–138, 
1997. 
[NAS 11a] NASA, System Safety Handbook: Volume 1, NASA/SP-2010-580, 
2011. 
[NAS 11b] NASA, Probabilistic Risk Assessment Procedures Guide for NASA 
Managers and Practitioners, NASA/SP-2011-3421, 2011. 
[NAS 11c] NASA, Risk Management Handbook, NASA/SP-2011-3422, 2011. 
[NRC 75] NUCLEAR REGULATORY COMMISSION, Reactor Safety Study: An 
Assessment of Accident Risks in US Commercial Nuclear Power Plants WASH-
1400 (NUREG-75/014), Washington, US, 1975. 
[NRC 83] NUCLEAR REGULATORY COMMISSION, Probabilistic Risk Assessment 
(PRA) Procedure Guide, NUREG/CR 2300, Washington, US, 1983. 

156     Risk Management in Life-Critical Systems 
[NRC 95] NUCLEAR REGULATORY COMMISSION, Use of Probabilistic Risk 
Assessment (PRA) Methods in Nuclear Regulatory Activities, Final policy 
statement, Washington, D.C., 1995. 
[NRC 02] NUCLEAR REGULATORY COMMISSION, An approach for using 
probabilistic risk assessment in risk-informed decisions on plant-specific 
changes to the licensing basis, RG 1.174, Washington, D.C., November 2002. 
[ROU 80] ROUSE W.B., Systems Engineering Models of Human-Machine 
Interaction, Oxford, North Holland, 1980. 
[SAL 06] SALVENDY G., Handbook of Human Factors and Ergonomics, John Wiley 
& Sons, 2006. 
[SCH 13] SCHINDLER J., CASSANI M., “Using an integrated simulation environment 
for the risk based design of advanced driver assistance systems”, Transportation 
Research Part F, vol. 21, pp. 269–277, 2013. 
[SHE 97] SHERIDAN T., “Supervisory control”, in SALVENDY G., (ed.), Handbook of 
Human Factors and Ergonomics, Wiley Interscience, pp. 1295–1325, 1997. 
[SWA 83] SWAIN A.D., GUTTMANN H.E., Handbook of Human Reliability Analysis 
with Emphasis on Nuclear Power Plant Applications, NUclear REGulatory 
Commission, NUREG/CR-1278, Washington D.C., 1983. 
[WIK 00] WICKENS C.D., HOLLANDS J.G., Engineering Psychology and Human 
Performance, Prentice Hall, Upper Saddle River, USA, 2000. 
 

8 
Dissonance Engineering for Risk Analysis: 
A Theoretical Framework 
8.1. Introduction  
This chapter presents a new original approach to analyze risks based on the 
dissonance concept. A dissonance occurs when conflict between individual or 
collective knowledge occurs. A theoretical framework is then proposed to 
control dissonances. It is based on the DIssonance MAneGEment (DIMAGE) 
model and the human–machine learning concept. The dissonance 
identification, evaluation and reduction function of DIMAGE is supported by 
automated tools that analyze the human behavior and knowledge. Three 
examples are proposed. The first example concerns the dissonance 
identification by the use of the reverse comic strip-based tool that analyzes  
the human behavior by taking into account the facial and voice characteristics. 
The second example consists of analyzing the knowledge content composed 
by rules in order to support the dissonance identification and evaluation. The 
last example relates to automated knowledge reinforcement in order to recover 
a lack of knowledge, i.e. to reduce dissonance. 
8.2. The concept of dissonance 
8.2.1. Dissonance engineering and risk analysis 
Dissonance engineering relates to the engineering sciences that focus on 
dissonance. A dissonance occurs when cognition sounds wrong. It is defined  
                         
Chapter written by Frédéric VANDERHAEGEN. 

158     Risk Management in Life-Critical Systems 
as a conflict or a divergence between individual or collective cognitions, i.e. 
elements of knowledge or knowledge. It then focuses on the dissonance 
concept developed by the cognitive sciences [FES 57] and the cindynics 
[KER 95]. It consists of treating such dissonances in a practical way in terms 
of risks. A cognitive dissonance is defined as an incoherency between 
individual cognitions. Cindynics dissonance is a collective or an 
organizational dissonance related to knowledge incoherency between 
persons or between groups of people. Dissonance engineering is a way to 
analyze risks by using the concept of dissonances that occur when something 
sounds wrong. The occurrence of a dissonance will relate to individual and 
collective knowledge. 
Risk analysis can require the use of different scientific contributions 
[VAN 12a]. There are approaches such as (1) RAMS-based analyses (i.e. 
reliability, availability, maintainability and safety-based analyses to work on 
technical failures), (2) analyses from cindynics (i.e. analyses to work on 
organizational dangers), human reliability or human error-based analyses 
(i.e. analyses to work on the success or failure of human behaviors, 
respectively), (3) resilience or vulnerability-based analyses (i.e. analyses to 
work on the success or failure of the control of the system stability, 
respectively), or (4) dissonance-based analyses (i.e. analyses to work on 
conflicts between knowledge). 
This chapter focuses on the last approach and relates to an original model 
of dissonance to manage possible conflicts between knowledge. A 
dissonance occurs when something was, may be, is or will be wrong or 
sounds wrong. This may produce a discomfort induced by the detection  
or the treatment of this dissonance and may be managed individually or 
collectively. 
8.2.2. Dissonance reduction and knowledge reinforcement 
The causes of the cognitive or organizational dissonance are multiple. 
Dissonances can be due to the occurrence of important or difficult decisions 
involving the evaluation of several possible alternatives [CHE 11]. They can 
also occur when viewpoints on human behaviors are contradictory [POL 03] 
or when behaviors such as competitive or cooperative behaviors fail  
[VAN 06, VAN 12b]. Organizational changes that produce incompatible  
 

Dissonance Engineering for Risk Analysis     159 
information are possible sources of dissonance occurrence [HUD 07,  
TEL 11, BRU 11]. Then, the updating or the refining of a given cognition 
due to new feedback from the field can also generate dissonance [TRI 96]. 
Whatever the causes of the dissonance occurrence, several paradigms 
exist. Human operators aim at reducing any occurrence or the impact of a 
dissonance because it produces discomfort. This activity leads to maintain a 
stable state of knowledge without producing any effort to change it  
[FES 57]. Despite this reduction, a breakdown of this stability is sometimes 
useful in order to facilitate the learning process and refine, verify or confirm 
knowledge [AIM 98]. Such knowledge adjustment improves the learning 
abilities. Finally, dissonance can also be seen as a feedback of a decision: 
dissonance occurs after a decision, and this requires a modification of 
knowledge [TEL 11]. 
Therefore, a discomfort can be a dissonance or can be due to the 
production of a dissonance, and the detection or treatment of a dissonance 
can also produce discomfort. Discomfort can occur if this dissonance is out 
of the control of the human operators or because the treatment of a detected 
dissonance increases the human workload or the human error for instance 
[VAN 99a]. Such an activity involves a minimum learning process in order 
to improve the human knowledge and to control such a discomfort. There are 
then positive and negative feedbacks from the dissonance management. 
Negative feedbacks relate to discomfort and positive feedbacks to the 
learning aspect for instance.  
The more difficult the learning process is to face a dissonance, the  
less acceptable this dissonance is [FES 57]. Therefore, strategies for 
dissonance reduction are required in order to minimize the knowledge 
changes or to facilitate the learning process and manage the acceptability of 
a dissonance. For instance, these strategies are adapted from [FES 57] and 
are extended in order to take into account the learning process to reinforce 
knowledge: 
– The elimination or inhibition of the dissonance impact by maintaining 
the initial knowledge without looking for any explanation. There is no 
modification of the current knowledge, and the data from the dissonance are 
disapproved and not treated. This consists of reinforcing the current content 
of knowledge independently from the dissonance impact. 

160     Risk Management in Life-Critical Systems 
– The addition of new cognitions to limit the dissonance impacts and 
justifies the initial knowledge. This new knowledge consists of giving more 
importance to the current knowledge than to the knowledge coming from the 
dissonance. This consists of producing new rules that reinforce the current 
content of knowledge. 
– The attenuation of the dissonance impacts by modifying or 
reinterpreting knowledge. The knowledge coming from the dissonance is 
integrated into the current knowledge by degrading its importance. New 
rules related to this dissonance are then produced, but they aim at reinforcing 
the current content of knowledge. 
– The integration of the dissonance impacts into the knowledge by 
refining the current knowledge or by creating new knowledge. This can 
cancel or refine some knowledge and produce new knowledge. This process 
is another king of reinforcement of knowledge that handles the current 
content of knowledge by integrating rules associated with the controlled 
dissonance. 
For example, regarding the use of an industrial rotary press described in 
[POL 03], suppose that the initial knowledge of a user A includes the 
following fact without any explanation: “I intervene on the machine even if 
the machine is running at high speed”.  Another user or the designer B of 
this machine can generate a dissonance by saying to him/her: “Any 
interaction with the machine is very dangerous when the machine is 
running”. From the first user A, the inhibition-based behavior consists of 
producing no new knowledge but ignoring or rejecting the new incoming 
dissonant knowledge: “No, it is not proved”. The addition-based behavior 
consists of attenuating the impact of this dissonance and justifying the initial 
knowledge by producing knowledge such as: “It is true but I like taking 
risks”. The attenuation-based behavior consists of modifying the content of 
the new incoming knowledge to limit its impact: “There is one chance in a 
billion of having an accident when interacting on the running machine”. 
Finally, the last behavior consists of recovering the initial knowledge and 
changes it radically by creating an opposite knowledge: “I stop interacting 
with the machine when the machine is running at high speed”. 
The reduction process of a dissonance implies the reinforcement of 
knowledge. It can be realized by specific algorithms such as those developed 
in [VAN 09, VAN 11a, POL 12, OUE 13]. The trial-and-error process is  
 

Dissonance Engineering for Risk Analysis     161 
applied when no knowledge is available to treat a given dissonance. 
Therefore, the human operators act on the process and wait for the 
consequences of these actions until they find a solution [VAN 11b]. This 
aims at refining the existing knowledge or at creating new knowledge. These 
reinforcement strategies aim at making the knowledge evolve when a 
dissonance is treated. Then, this knowledge reinforcement to reduce 
dissonance leads us to maintain a stable level of knowledge or aims at 
transforming an unstable level toward a stable level of knowledge. It aims  
at consolidating, validating, refining or deleting the existing knowledge or at 
creating new knowledge. 
A dissonance may perturb the stability of knowledge level by affecting 
other dissonance dimensions such as interpreted risk level, and its 
management aims at returning to a new level of knowledge stability or to the 
previous one by reinforcing knowledge. The maintenance of the coherence 
of cognitive systems requires stability [FES 57]. The control of this stability 
can be facilitated by a good management of the human workload and 
performance integrating different human–machine organizations [VAN 99b]. 
This aims at reducing the occurrence or the impact of a dissonance. For 
instance, the control of overloaded situations reduces the occurrence of 
human errors when tasks are dynamically shared between human and 
machine [VAN 99c]. Knowledge stability relates to sustainable knowledge 
equilibrium, and any deviation from this stability generates dissonances, or 
is generated by the occurrence of a dissonance or by the impact of its 
control. 
Facing instability of the human knowledge, if the treatment of this 
dissonance is successful, human operators contribute to the resilience of the 
system they control [ZIE 11]. On the other hand, if this treatment continually 
produces other dissonances and may fail, then it contributes to the 
vulnerability of the controlled system. The frequency of perturbations, such 
as dissonances, may have an impact of the system resilience or vulnerability 
[WES 06, ZIE 10]. The management of a regular dissonance increases 
knowledge about it and may converge to a high stable knowledge level, 
whereas a new dissonance can provoke instability that needs to modify, 
refine or create knowledge. The lower the frequency of a dissonance is, the 
lower the associated knowledge to manage it may be and the higher 
discomfort or workload this dissonance may produce. 

162     Risk Management in Life-Critical Systems 
Dissonance engineering methods are required in order to analyze such 
dissonances and reduce their possible negative impacts. The next section 
proposes an original approach to identify, analyze and reduce dissonances. 
8.3. A theoretical framework for risk analysis 
The theoretical framework is based on the DIMAGE model and on the 
human–machine learning concept to provide human operators with 
assistance tools to reinforce knowledge. 
8.3.1. The DIMAGE model 
The proposed model is a human decision-making model based on the 
dissonance management to control risks. It is called DIMAGE and is divided 
into several processes, Figure 8.1 [VAN 13a]: 
– The decision-making process to take decisions facing the gap between 
the desired goal to be achieved and the real one obtained from the socio-
technical systems. 
– The action realization to apply the selected action plan from the 
decision-making process. 
– The dissonance identification, evaluation and reduction process to 
prevent, recover and confine the dissonance impacts by managing the 
decision, goal or action processes. Reconfiguration, accommodation, 
correction, cancelling or trial-and-error are possible actions to enable the 
prevention, recovery and confinement of the occurrence or the impact of 
dissonance. Individual reconfiguration consists of recovering the initial 
goals, and collective reconfiguration relates to the modification of the 
structure of the socio-technical system by applying, for instance, a 
reallocation of the role of the staff. 
– The knowledge control process to reinforce the knowledge content, i.e. 
to valorize, modify, create or cancel knowledge. 
– The individual goal management process related to individual 
reconfiguration of the goal from the dissonance control process. 
– The socio-technical system goal management process related to 
collective reconfiguration of the goal from the dissonance control process. 

Dissonance Engineering for Risk Analysis     163 
 
Figure 8.1. The DIMAGE model 
The dissonance identification, evaluation and reduction process aims at 
characterizing a dissonance in terms of dimensions and stability [VAN 12a]. 
The dimensions of dissonance relate, for example, to the knowledge level the 
designers assume the users have to treat a dissonance, or to the knowledge 
level the users actually have to treat it, or to the belief level they have on 
their knowledge, or to the interpreted risks level associated with the 
dissonance or to its control, etc. For instance, the level of a dimension can be 
low, medium or high when a dissonance occurs and can evolve after the 
dissonance reduction process. Then, the dimension level can remain or 
become stable, or can evolve and become unstable, see Figure 8.2. 
Instantaneous level of a dimension or a degradation of the evolution of its 
stability can generate or can be due to dissonances. 
The identification of dissonances depends on a taxonomy based on  
the knowledge-level stability, see Table 8.1. It consists of identifying 
possible dissonances. The dissonance evaluation step consists of confirming 
the occurrence of these dissonances and determining if they may make  
the knowledge evolve. Finally, the dissonance reduction step applies  
the necessary modifications of the knowledge content by applying the 
knowledge reinforcement process. 

164     Risk Management in Life-Critical Systems 
 
Figure 8.2. Stable and unstable level of a dissonance dimension 
Suppose the dimension level relates to the level of knowledge content and 
that it can be low, high, decreasing or increasing. When the knowledge is 
low, this can generate a dissonance due to a lack of knowledge. After the 
reinforcement process, if the level remains low, it is stable but other possible 
dissonances may be treated: the error rejection or inhibition of possible new 
knowledge generated by the initial dissonance, the incapacity to create new 
knowledge to treat the initial dissonance, the failure of the trial-and-error 
process, etc. The trial-and-error process is applied when no adapted 
knowledge is available to treat a given dissonance. Therefore, the human 
operators act on the process and wait for the consequences of these actions 
until they find a solution [VAN 11b].  
Knowledge 
content 
level 
Stable and low 
Stable and high 
Unstable and 
decreasing 
Unstable and 
increasing 
Possible 
associated 
dissonances 
Lack of 
knowledge 
Error inhibition 
Insufficient 
know-how 
Failed trial-and-
error 
Divergent expert 
viewpoint 
Perseveration 
behavior 
Regular violations 
Intentional barrier 
removal 
Automation surprise 
Error knowledge 
Unavailable 
knowledge 
Unsafe knowledge 
Loss of 
knowledge 
Loss of 
memory 
       … 
Error 
learning 
      … 
Table 8.1. A taxonomy of dissonance 

Dissonance Engineering for Risk Analysis     165 
When the knowledge is high, this can generate a dissonance due to 
possible error knowledge or conflicting knowledge. After the reinforcement 
process, if the knowledge level remains high, it is stable but dissonances, 
such as perseveration behavior [DEH 12], intentional violations or barrier 
removal [ZHA 04, VAN 11a], or automation surprise [INA 08], may be 
treated. When the knowledge is unstable and increasing, a dissonance due to 
possible error learning process may occur. If the knowledge level is unstable 
and decreasing, another dissonance due to a possible loss of knowledge or of 
memory may appear. The proposed approach is limited to the analysis of 
unsafe dissonance for which a dissonance reduction process is required. 
Then, the dissonance reduction process leads to maintain or regain 
sustainable 
knowledge 
equilibrium 
by 
applying 
the 
knowledge 
reinforcement. The reinforcement of knowledge can apply several strategies 
of dissonance reduction presented in section 8.1.2. 
8.3.2. The human–machine learning process 
The proposed dissonance engineering-based approach depends on a 
human–machine learning approach in order to facilitate the dissonance 
identification and evaluation, and the dissonance reduction by reinforcing 
the knowledge base, see Figure 8.3. 
 
Figure 8.3. The theoretical framework based on human–machine  
learning to control dissonances 

166     Risk Management in Life-Critical Systems 
The autonomy of a system or a human or an artificial agent can be 
considered as a triplet [ZIE 10, ZIE 11, VAN 12b]: the knowledge, the 
availability and the prescription. The knowledge is the agent’s competence, 
rules or skills, and relates to cognitive capacities, such as capacities to apply 
knowledge, capacities to manage knowledge, capacities to interact with other 
decision makers in case of incomplete, uncertain or unknown problem-
solving or capacities to learn from such problems. The availability concerns 
the load of the agents or of the resources they used. The prescription is 
linked to the management of the tasks to be achieved, e.g. authorization to 
achieve a task, to allocate or share a task, etc. Two main modes of autonomy 
exist: the static autonomy control based on static parameters of autonomy or 
the dynamic autonomy control based on dynamic parameters of autonomy 
control. The static mode considers the knowledge, availability and 
prescription as stable, whereas the dynamic mode considers that at least one 
parameter is unstable and evolves dynamically. In the static mode, the 
increasing of the autonomy of a system requires the use of the static 
autonomy of several agents. In the dynamic mode, several combinations 
between autonomy parameters of agents can be used. The principles of 
human–machine cooperation and human–machine learning are suitable 
supports to control such autonomy [VAN 99a, VAN 12b]. Both cooperation 
and learning can indeed improve the knowledge, the availability and the 
prescription of an agent or a group of agents. 
Lacks or losses of knowledge, availability or prescription are particular 
dissonances that can be recovered by cooperation and learning. Several 
approaches were developed in order to analyze the limits of human 
knowledge and to change the allocation of tasks between agents [VAN 97, 
VAN 99b, VAN 99c]. There are, for instance, methods to assess human 
workload or performance. An overloaded situation requires a high level of 
attention and an underloaded situation may lead to hypovigilance. These 
situations relate to the occurrence of dissonance or may generate other 
dissonances. 
A theoretical framework is presented for risk analysis based on 
dissonance identification, evaluation and reduction. It includes both 
automated support systems and human actions. The automated support tools 
consist of analyzing behavior and knowledge. Both consist of providing the 
human operator with assistances to the dissonance identification, assessment 
and reduction. The human operators can then validate them and reinforce 
their knowledge content. The next sections proposed conceptual approaches 

Dissonance Engineering for Risk Analysis     167 
and possible automated tools for assisting the identification, the evaluation 
and the reduction of dissonance. 
8.3.3. The behavior analysis for dissonance identification 
The behavior-based analysis consists of building a relationship between 
behavioral characteristics of the human operators and their possible 
associated risks linked to dissonances, such as risks related to a lack of 
knowledge or to a doubt on knowledge. Several pieces of data from the 
human operator can then be used. There are physiological data, physical data 
or cognitive data. 
The reverse comic strip concept [VAN 13b] is a nice example of such a 
tool to analyze human behaviors. It consists of building a comic strip after 
capturing real-time data from human operators. For instance, these pieces of 
data concern the facial and voice characteristics, see Figure 8.4. 
 
Figure 8.4. The reverse comic strip-based approach to identify dissonances 
Both a facial recognition and a sound recognition systems are required in 
order to build the emotion images and the sound variation images. Figure 8.5 
proposes some examples of such emotion images and sound variation 
images. 
The facial recognition system recognizes face pictures of the reverse 
comic strip. It is associated with specific emotions such as frightened or 
scared, worried or furious, neutral or undetermined, etc. The sound 
recognition system recognizes the sound variation of the comic strip based 
on the sounds produced by the human voice. It can be associated with a 
particular word or sound database, to a particular evolution of voice 
frequencies, or the evolution of microphone diaphragm vibrations. 

168     Risk Management in Life-Critical Systems 
 
Figure 8.5. Examples of emotion and sound variation images 
The identification of dissonance made by the automated tool relates to the 
display of alarms related to particular sequences of images or particular 
predefined images. 
8.3.4. The knowledge-based analysis for dissonance evaluation 
This approach consists of analyzing the content of knowledge that is 
composed by rules such as AÆB, i.e. if A then B. The condition of a given 
rule noted Rule(i) from Rule is A and its conclusion is B. 
The knowledge base is noted K. Initially, Rule contains K. The Boolean 
BOOL is TRUE, and the knowledge analysis stops when BOOL remains 
FALSE. The variable BE contains initially the events to be treated. If an 
event from BE appears on the condition of a rule then this conclusion is 
integrated into BE. 
The analysis of the knowledge content is detailed in Figure 8.6. The final 
content of BE is then treated by the dissonance_evaluation function,  
see Figure 8.7. 

Dissonance Engineering for Risk Analysis     169 
 
Figure 8.6. The knowledge analysis algorithm 
 
Figure 8.7. The dissonance evaluation algorithm 

170     Risk Management in Life-Critical Systems 
The evaluation of the possible dissonance is based on the identification of 
possible conflicts between rules. The variable Conflict contains all these 
possible conflicts that have to be validated by the human operator. When an 
element and its opposite occur on BE then there is a possible conflict 
between rules. This conflict is then displayed on the interface with the 
Display function in order to be validated by the human operator who has to 
reinforce the knowledge content, i.e. to modify or delete some rules or to 
create new rules. 
8.3.5. The knowledge-based analysis for dissonance reduction 
The knowledge-based analysis for dissonance reduction concerns the 
knowledge base reinforcement. Several algorithms can be used in order to 
reinforce the content of the knowledge base. There are methods such as 
neural network-based methods [OUE 13, VAN 09], case-based reasoning 
methods, utility-based methods [POL 12] or the so-called benefit-cost-
deficit-based method [VAN 11a]. 
As an example, a generic reinforcement based on an iterative learning 
process is presented in Figure 8.8. It consists of implementing the rules of the 
knowledge base in terms of input and output vectors. It uses the input vector 
(i.e. the condition of a rule at a given iteration, noted Ai) and the output vector 
(i.e. the conclusion of a rule at a given iteration, noted Bi) of the previous 
iterations in order to reinforce the knowledge base content at a given iteration. 
 
Figure 8.8. The generic reinforcement based on learning process 

Dissonance Engineering for Risk Analysis     171 
An example of an algorithm related to the case-based reasoning concept 
is proposed in Figure 8.9. It is based on the knowledge_reinforcement 
function. It adds a new couple (A, B) that represents a rule (AÆB) of the 
knowledge base K, assuming that this couple does not already exist (i.e. if 
the couple already exists then REDUC = 0) and that there is no conflicting 
information (i.e. if there is conflicting information, then REDUC = 2). If 
there is conflicting information, this means that the same inputs give 
different outputs. This appears when input vector A exists on knowledge 
base K, while the content of B can be different. The algorithm deletes the 
data from knowledge base K when such conflicting information is detected.  
The existence of similar data in knowledge base K is assessed with respect  
to error values εA and εB associated with inputs A and actions B,  
respectively. The number of couples (A, B) that are in knowledge base K is 
denoted as Card(K). For every knowledge base couple (Ak, Bk),  
with 
(
)
1≤
≤
k
Card K , couple (Ak, Bk) is compared to couple (Ai, Bi) by 
assessing an Euclidian distance. When a vector (A, B) is found (i.e. the 
couple (Ak, Bk) that is the most similar to (Ai-1, Bi-1)), then no modification of 
the knowledge base is required. 
 
Figure 8.9. A reinforcement algorithm by case-based reasoning 

172     Risk Management in Life-Critical Systems 
Some simple examples are proposed in order to illustrate the application 
feasibility of the behavior analysis-based framework and the knowledge 
analysis-based framework. 
8.4. Examples of application of the theoretical framework 
8.4.1. An application of the automated dissonance identification 
This first example consists of interpreting the meaning of some information 
coming from signaling systems of rail platforms, see Figure 8.10. The sound 
recognition system is based on the capture of the deformation of the 
microphone diaphragm [LA 13]. The emotion images were generated 
manually by the human operators who have to choose the correct emotion 
images regarding the perception of their own emotion or feeling. 
 
Figure 8.10. The interpretation of pictures from rail platform signaling systems 
The experimental protocol consisted of displaying during a few seconds the 
pictures 1, 2 and 3 of Figure 8.8 and wait for the reactions of a subject. Each 
picture displays a rail signaling panel composed by lamps turned on or off and 
names of train stations. The exercise consists of building the sense of the state 
(i.e. turned on or off) of the lamps by proposing the rules of functioning. 

Dissonance Engineering for Risk Analysis     173 
During this exercise, a subject was then invited to comment on the 
interpretation of the lights of the signaling panel and produce rules of 
functioning such as A Æ B (i.e. if A then B) and to identify manually the 
sequences of the emotion images during the interpretation phase of each 
picture. 
The obtained sequence of emotion images and the sound capture are 
given in Figure 8.11. Three emotion images are selected and identified by 
the subject: the surprise or astonished image, the puzzled or dissatisfied 
image and the neutral or undetermined image. The display of picture 1 
generated first the surprise or astonished emotion due to the discovery of the 
picture and second the neutral or undetermined one when the subject 
understands the meaning of the content of the picture. The subject has then 
produced an obvious rule: if the lamps are on, then the train will stop at the 
corresponding stations. The display of picture 2 generated the same emotion 
images. The surprise or astonished emotion image relates to a  
doubt on the relationship between the two first pictures. The neutral  
or undetermined emotion image relates to the reinforcement of the  
rule produced during the first picture, i.e. the maintenance of the following 
rule: if the lamps are on, then the train will stop at the corresponding 
stations. 
 
Figure 8.11. The associated reverse comic strip for dissonance identification 

174     Risk Management in Life-Critical Systems 
The display of the last picture produced three emotion images. Due to the 
memory effect, the first image concerns the neutral or undetermined one. 
Indeed, the subject is convinced that picture 3 relates to the same context 
than the context of pictures 1 or 2. The second image is the puzzled or 
dissatisfied one. It relates to the detection of an error of interpretation, and to 
the following conclusion: the lamps being on does not mean that the trains 
will stop at the corresponding station, but this means that the trains are 
located at the corresponding stations. Indeed, other trains are on the parallel 
lines and are stopped at the stations indicated by lighted lamps on the panel. 
The last emotion image is then the neutral or undetermined after this 
problem-solving that produced a new rule of functioning: if the lamps are on 
then the trains are located at the corresponding stations. 
Regarding the sound variation images of the reverse comic strip, the zero 
level of oscillation relates to the absence of sounds. Positive level is a 
physical deformity of the diaphragm of the microphone. It means that sounds 
were produced. The negative level is the movement of the deformed 
diaphragm to return to its initial position. Comments from the subject relate 
to the different steps of interpretation of each picture. For instance, picture 1 
initially involves surprised or astonished-based emotion. This relates to the 
research of meaning of the lighted lamps and to the creation of an associated 
rule. The stabilization of the diaphragm deformation level is linked to the 
neutral or undetermined-based emotion. Figure 8.8 is a summary of the 
evolution of the knowledge modeled by rules such as AÆ B. The autonomy 
of the subject in terms of knowledge evolves from one rule to two rules after 
the interpretation of the three pictures. The analysis of the knowledge 
identifies a lack of knowledge at the beginning of the display of picture 1 
that required a minimum time of problem-solving to produce a rule of 
functioning. It also identifies a doubt on knowledge during the display of 
picture 2 and an error knowledge or a lack of knowledge during the display 
of picture 3. 
8.4.2. An application of the automated dissonance evaluation 
The second example concerns possible dissonances related to the 
knowledge produced by the use of a cruise/speed control system (ASC). If 
the ASC system is activated and if an initial setpoint is given by the car 
driver, the ASC is in charge of the regulation of the car speed by maintaining 

Dissonance Engineering for Risk Analysis     175 
this setpoint speed. Several dissonances can generate different possible 
evolutions of the car driver knowledge. 
The knowledge base content relates to the rules to control aquaplaning 
and to use the ACS. For instance, suppose that the knowledge contains the 
following rules: 
– R1: (the use of the ASC system Æ turn the activation button on “on”); 
– R2: (the deactivation of the ASC system Æ brake); 
– R3: (Speedreal < Speedsetpoint managed by the ASC Æ accelerate 
automatically); 
– R4: (Speedreal > Speedsetpoint managed by the ASC Æ decelerate 
automatically); 
– R5: (the control of an aquaplaning Æ not brake); 
– R6: (the control of an aquaplaning Æ not accelerate). 
An application of the algorithms of the knowledge analysis and the 
dissonance evaluation is given in Figure 8.12.  
The variable Conflict contains two possible conflicts related to opposite 
actions (not brake and brake) and (not accelerate and accelerate). Then two 
couples of dissonant rules are identified: (R2 R5) and (R3 R6). This means 
that a danger may occur if the ASC is activated and controls a speed setpoint 
and if in the same time an aquaplaning occurs. The event will decelerate the 
vehicle, and the real car speed will be reduced due to the level of water on 
the street. The real car speed may be under the speed setpoint, and the ACS 
will automatically accelerate the car speed to reach the required setpoint. 
Opposite action occurs: you do not want to accelerate because aquaplaning 
occurs, whereas the ASC wants to. Moreover, if you know this dissonant, 
you may decide to deactivate your ASC system. Nevertheless, there is a 
contradictory fact: you have to brake to deactivate the ACS system but you 
know that you cannot brake if you want to optimize the control of the 
aquaplaning. Therefore, a second dissonance appears and will be validated 
by the car driver who may produce new rules such as: 
– R7: (the control of an aquaplaning Æ do not use the ASC system); 
 

176     Risk Management in Life-Critical Systems 
– R8: (the control of an aquaplaning Æ deactivate the ASC with the stop 
button “off”). 
 
Figure 8.12. The associated rule analysis for dissonance identification and evaluation 
8.4.3. An application of the automated dissonance reduction 
The third example concerns the interpretation of a rule of the knowledge 
base in terms of input and output vectors. The condition of a rule relates to 
the process state, and the conclusion focuses on the actions of human 
operators. These actions concern barrier removal. Figure 8.13 gives an 
example of application of the proposed framework for dissonance reduction 
to automatically recover a lack  of  knowledge. 
This example relates to the use of the Transformation of Traffic Flow 
(TRANSPAL) platform developed at the University of Valenciennes in 
France. This platform proposes a series of removable barriers. Its 
functioning and the corresponding experimental protocol performed with  
20 subjects is detailed in [POL 09]. The prediction determined whether the 

Dissonance Engineering for Risk Analysis     177 
human operators would remove more than 10 barriers. The maximum correct 
prediction rate is 100% and the minimum is 0. 
 
Figure 8.13. A prediction process based on the knowledge reinforcement 
The prediction function uses the process state of the current iteration as 
the input vector Ai. It searches the closer input vector Ak from the 
knowledge base K by applying a similarity function based on an Euclidian 
distance assessment [VAN 09, VAN 11a]. When the closer input vector is 
found, the corresponding output vector Bk is considered as the prediction of 
the output vector Bi. The application of the algorithm of Figure 8.9 

178     Risk Management in Life-Critical Systems 
reinforces the lack of knowledge by increasing the capacity of the system to 
predict correctly after nine iterations, Figure 8.14. 
 
Figure 8.14. The correct prediction rate by reinforcing the knowledge base 
Due to a limited quantity of data, none of the conflicting information was 
identified, and the knowledge management errors were equal to zero. 
Dissonance reduction is then limited to the recovery of a lack of knowledge 
that requires an additional input and output vectors into the knowledge base 
of the automated system. This example illustrates the capacity of an 
automated system to learn from human errors, i.e. from barrier removals 
done by human operators. 
8.5. Conclusion 
This chapter has described an original framework based on the 
dissonance concept in order to analyze risks. This new approach differs from 
the classical safety analysis approaches because it can take into account 
online and offline processes and focuses on knowledge characteristics. A 
dissonance occurs when something sounds wrong in terms of conflicts 
between individual or collective knowledge. A theoretical framework is then 
proposed to identify, assess and reduce such a dissonance. It is based on the 
DIMAGE model. The identification of dissonance is supported by a 
dissonance taxonomy and dissonance dimensions such as knowledge content 
or belief on knowledge, and the analysis of a dissonance relates to behavior 
or knowledge-based analysis. The dissonance reduction aims at reinforcing 
this knowledge base. A structure for applying human–machine learning is 
proposed in order to facilitate this dissonance management in terms of 

Dissonance Engineering for Risk Analysis     179 
dissonance identification, assessment and reduction. Several examples of 
automated tools are proposed for identifying, assessing and reducing 
dissonances. The first example is based on the so-called reverse comic strip 
in order to identify a lack of knowledge regarding facial and vocal 
characteristics. The second example proposed algorithms for detecting 
conflict between knowledge automatically. The last example proposed a 
case-based reasoning system that reinforces its knowledge base in order to 
recover a lack of knowledge by learning from particular human errors called 
barrier removals. These examples illustrated the feasibility of such a 
framework based on dissonance engineering for risk analysis. 
8.6. Bibliography 
[AIM 98] AIMEUR E., “Application and assessment of cognitive dissonance – theory 
in the learning process”, Journal of Universal Computer Science, vol. 4, no. 3, 
pp. 216–247, 1998. 
[BRU 11] BRUNEL O., GALLEN C., “Just like cognitive dissonance”, 27th 
International Congress of French Association of Marketing, pp. 18–20, May 
2011. 
[CHE 11] CHEN T.-Y., “Optimistic and pessimistic decision making with dissonance 
reduction using interval-valued fuzzy sets”, Information Sciences, vol. 181,  
no. 3, pp. 479–502, 2011. 
[DEH 12] DEHAIS F., CAUSSE M., VACHON F., et al., “Cognitive conflict in human-
automation interactions: a psychophysiological study”, Applied Ergonomics,  
vol. 43, no. 3, pp. 588–595, 2012. 
[FES 57] FESTINGER L., A Theory of Cognitive Dissonance, Stanford University 
Press, Stanford, CA, USA, 1957. 
[HUD 07] HUDSON P., “Implementing a safety culture in a major multi-national”, 
Safety Science, vol. 45, pp. 697–722, 2007. 
[INA 08] INAGAKI T., “Smart collaboration between humans and machines based on 
mutual understanding”,  Annual Reviews in Control, vol. 32, pp. 253–261, 2008. 
[KER 95] KERVERN G.-Y., Eléments Fondamentaux des Cindyniques (Fondamental 
Elements of Cindynics), Economica Editions, Paris, France, 1995. 
[LA 13] LA DELFA S., Sound Recognition System, Master Science Research Project, 
University of Valenciennes, France, 2013. 

180     Risk Management in Life-Critical Systems 
[LEG 12] LEGRAND C., RICHARD P., BENARD V., et al., “Diagnosis of human 
operator behaviour in case of train driving, interest of facial recognition”, 
Proceedings of the 30th European Annual Conférence on Human Decision 
Making and Manual Control (EAM2012), Braunschweig, Germany, September, 
2012. 
[OUE 13] OUEDRAOGO A., ENJALBERT S., VANDERHAEGEN F., “How to learn from 
the resilience of human–machine systems?”, Engineering Applications of 
Artificial Intelligence, vol. 26, no. 1, pp. 24–34, 2013. 
[POL 03] POLET P., VANDERHAEGEN F., AMALBERTI R., “Modeling Border-line 
tolerated conditions of use (BTCUs) and associated risks”, Safety Science,  
vol. 41, pp. 111–136, 2003. 
[POL 09] POLET P., VANDERHAEGEN F., MILLOT P., “Human behaviour analysis of 
barrier deviations using a benefit-cost-deficit model”, Advances in Human-
Computer Interactions, vol. 2009, Article ID 642929, p. 10, 2009. 
[POL 12] POLET P., VANDERHAEGEN F., ZIEBA S., “Iterative learning control based 
tools to learn from human error”, Engineering Applications of Artificial 
Intelligence, vol. 25, no. 7, pp. 1515–1522, 2012. 
[RIC 10] RICHARD P., BENARD V., CAULIER P., et al., “Toward the “Human 
Stability” in transportation domain: concepts and objectives”, Proceedings of the 
11th IFAC/IFIP/IFORS/IEA Symposium on Analysis, Design and Evaluation of 
Human-Machine Systems, Valenciennes, France, August–September 2010. 
[TEL 11] TELCI E.E., MADEN C., KANTUR D., “The theory of cognitive dissonance: 
a marketing and management perspective”, Procedia Social and Behavioral 
Sciences, vol. 24, pp. 378–386, 2011. 
[TRI 96] TRIMPOP R.M., “Risk homeostasis theory: problems of the past and 
promises for the future”, Safety Science, vol. 22, nos. 1–3, pp. 119–130, 1996. 
[VAN 06] VANDERHAEGEN F., CHALMÉ S., ANCEAUX F., et al., “Principles of 
cooperation and competition – application to car driver behavior analysis”, 
Cognition, Technology, and Work, vol. 8, pp. 183–192, 2006. 
[VAN 09] VANDERHAEGEN F., ZIEBA S., POLET P., “A reinforced iterative formalism 
to learn from human errors and uncertainty”, Engineering Applications and 
Artificial Intelligence, vol. 22, pp. 654–659, 2009. 
[VAN 10] VANDERHAEGEN F., “Autonomy control of human-machine systems”, 
11th IFAC/IFIP/IFORS/IEA Symposium on Analysis, Design, and Evaluation of 
Human-Machine Systems, Valenciennes, France, 2010. 
[VAN 11a] VANDERHAEGEN F., ZIEBA S., ENJALBERT S., et al., “A 
benefit/cost/deficit (BCD) model for learning from human errors”, Reliability 
Engineering & System Safety, vol. 96, no. 7, pp. 757–76, 2011. 
[VAN 11b] VANDERHAEGEN F., CAULIER P., “A multi-viewpoint system to support 
abductive reasoning”, Information Sciences, vol. 181, pp. 5349–5363, 2011. 

Dissonance Engineering for Risk Analysis     181 
[VAN 12a] VANDERHAEGEN F., Dissonance Engineering for Risk Analysis. 
Workshop: Risk Management in Life Critical Systems, Human-Centered Design 
Institute, Florida Institute of Technology, Melbourne, FL, USA, March, 2012. 
[VAN 12b] VANDERHAEGEN F., “Cooperation and learning to increase the autonomy 
of ADAS”, Cognition, Technology & Work, vol. 14, no. 1, pp. 61–69, 2012. 
[VAN 13a] VANDERHAEGEN F., “A dissonance management model for risk 
analysis”, Proceedings of the 12th IFAC/IFIP/IFORS/IEA Symposium on 
Analysis, Design, and Evaluation of Human-Machine Systems, Las Vegas, USA, 
11–15, August 2013. 
[VAN 13b] VANDERHAEGEN F., “Toward a reverse comic strip based approach to 
analyze human knowledge”, Proceedings of the 12th IFAC/IFIP/IFORS/IEA 
Symposium on Analysis, Design, and Evaluation of Human-Machine Systems, 
Las Vegas, USA, 11–15 August 2013. 
[VAN 97] VANDERHAEGEN F., “Multilevel organization design: the case of the air 
traffic control”, Control Engineering Practice, vol. 5, no. 3, pp. 391–399, 1997. 
[VAN 99a] VANDERHAEGEN F., “Cooperative system organisation and task 
allocation: illustration of task allocation in air traffic control”, Le Travail 
Humain, vol. 62, pp. 197–222, 1999. 
[VAN 99b] VANDERHAEGEN F., “Toward a model of unreliability to study error 
prevention supports”, Interacting With Computers, vol. 11, pp. 575–595, 1999. 
[VAN 99c] VANDERHAEGEN F., “Multilevel allocation modes – allocator control 
policies to share tasks between human and computer”, System Analysis Modeling 
Simulation, vol. 35, pp. 191–213, 1999. 
[WES 06] WESTRUM R., “A typology of resilience situations”, HOLLNAGEL E., 
WOODS D., LEVESON N. (eds.), Resilience Engineering:Concepts and Precepts, 
Ashgate, Aldershot, UK. pp. 55–65, 2006. 
[ZHA 04] ZHANG Z., POLET P., VANDERHAEGEN F. et al., “Artificial neural network 
for violation analysis”, Reliability Engineering and System Safety, vol. 84, no. 1, 
pp. 3–18, 2004. 
[ZIE 10] ZIEBA S., POLET P., VANDERHAEGEN F., et al., “Principles of adjustable 
autonomy: a framework for resilient human machine cooperation”, Cognition, 
Technology and Work, vol. 12, no. 3, pp. 193–203, 2010. 
[ZIE 11] ZIEBA S, POLET P., VANDERHAEGEN F., “Using adjustable autonomy and 
human-machine cooperation for the resilience of a human-machine system –
application to a ground robotic system”, Information Sciences, vol. 181,  
pp. 379–397, 2011. 

 

9 
The Fading Line between  
Self and System  
9.1. Introduction 
Over the past century, the reliability of technical systems and their 
control systems, nowadays implemented in digital automation systems, 
which are used to operate vehicles technical installations, have increased 
dramatically. This can be illustrated with the developments in aviation. In 
the early years of ﬂight, engine failures were still a major cause of incidents 
and accidents, and for a long time, trans-oceanic airliners required three or 
more engines to be able to cross large stretches of water with a sufficient 
safety level. The present number of accidents attributed to power plant 
failure is a marginal category in the accident statistics. 
While technical developments increased the reliability of aircraft, it 
cannot be expected that the human component in a complex technical system 
underwent similar advances in reliability. An often held view, but also one 
contended by many [DEK 06], is that the remaining unreliable element in an 
otherwise increasingly reliable system is the human. From that standpoint, it 
is to be expected that the ratio of mishaps and failures attributable to human 
error would grow with increasing technical reliability. However, this portion  
of the “attribution” is amazingly stable, and often quoted to be between 50 
and 70%.  
                         
Chapter written by René VAN PASSEN. 

184     Risk Management in Life-Critical Systems 
“Human error” is the attribution of a mishap to a pilot, designer, planner, 
manager, maintainer, law maker or whichever human happened to be 
identiﬁed as instrumental in an event that in hindsight seems to have been 
preventable. The choice of culprit has seen several changes in the 
development of safety science. Initially, the workers at the sharp end1  
were blamed, the pilots, seamen or operators in the plants. These early 
systems, such as the early aircraft, also offered ample opportunities for 
mistakes and errors; the systems were unreliable, and required constant 
monitoring and intervention. All operations were done manually, and 
required complex human input to be performed (for example, setting 
mixture, ignition timing and throttle for engines under varying temperature 
and pressure conditions, respecting boundaries in rotations per minute 
(RPM), engine and exhaust temperature).  
One of the main improvements in safety in the World War (WW) II era 
aircraft was differentiating (through shape of the knobs and their location) 
between the ﬂaps and gear handles, by lieutenant Alphonse Chapanis for the 
B-17, P-47 and B-25 bombers. This introduced a next step in error 
attribution in which not only the pilots (at the sharp end) were blamed, but 
also ﬂaws in the design were identiﬁed, and thus the designers were 
“blamed”. In following steps, the focus of error research shifted further 
outward to maintenance crew, designers, managers and rule makers. In a 
way, this coincided with the advance of ergonomics, and an era in which 
human capabilities and limitations were taken into consideration in the 
design of systems with human interaction, explaining why human 
performance – even though human basic capabilities have not greatly 
changed – also improved, contributing to an improving safety record.  
Human endeavors are seldom perfect on the ﬁrst try. New vehicles, 
devices or installations often have teething problems. More than once, 
engineers and the society at large needed to learn about the dangers of new 
technological solutions through the analysis of failure. A help in limiting the  
search area is error classiﬁcation in which the type of error is determined. 
 
 
                         
1 The “sharp end” is the name for the place where the errors have direct consequences for the 
error makers  (the flight deck, ship, etc.). This is in contrast with the name “blunt end”, which 
is the term for the desks of the designers, managers, planners, etc., whose mistakes initially 
affect others. 

The Fading Line between Self and System     185 
Regarding the type of error, a remedy can be found in either adjustment of 
equipment, training, modiﬁed procedures or modiﬁed operation. We  
should realize that an error can seldom be attributed to a single source. Also, 
when attributing a problem to multiple sources (e.g. the layout of  
the instruments, procedures, company culture and lack of training for the 
human operator, to name but a few), the ﬁx will not be a simple correction of 
one or more of these “components”. Just as changes to the ﬂap and gear 
handle inﬂuenced human error rate in WWII era aircraft, changes in the 
operation or technology of our present-day systems often have widespread 
effects.  
The increasing technical reliability of instrumentation and automation, 
combined with the persistent ratio of system failure attributed to “human 
error”, invites claims that full automation can increase the safety of air 
transport. To refute such claims, one needs only point to the apparent 
difﬁculty we still have to achieve fully autonomous operation of ﬂying or 
driving vehicles, illustrated by the fact that competitions for this are set up, 
e.g. [ANO 09]. Most “autonomously” ﬂying machines require teams of 
operators, and careful programming and planning of mission elements in 
advance of the ﬂight. The active involvement of the human operators is still 
the most efﬁcient way of instructing an Unmanned Aerial Vehicle (UAV) 
system on the details of its task; the alternative would be to provide scripted 
responses for all situations a UAV might encounter.  
This makes full autonomy not a viable option for creating safe systems 
that at the same time need to be ﬂexible enough to handle unforeseen 
circumstances. This chapter contains a designer’s view on the creation of 
combined human–machine systems that provide safe, reliable and ﬂexible 
operation. A common approach in design is the breakdown of a complete 
system into subsystems, and to focus on the design of the individual 
components. This can, up to a point, be used in the design of safe systems. 
However, the adaptive nature of the “human” component, which is  
precisely the reason for having humans in complex systems, is such that  
it is not practical to isolate the human as a single component, and assume 
that the synthesis of the human with the other components yields  
the complete system. Rather, humans “merge” with the complete system to a 
far greater extent than often imagined, and a designer needs to be aware of 
that.  

186     Risk Management in Life-Critical Systems 
This chapter explores, through the reﬂection on a number of incidents and 
accidents, the nature of mishaps in human–machine systems, and the factors 
that might have inﬂuenced these events. It will start with a short introduction 
of the events, and an overview of the different ways of analyzing them. 
9.2. Four events 
9.2.1. Turkish Airlines 1951 
In February 2009, a Boeing 737-800 operated by Turkish Airlines 
crashed in a ﬁeld some 1.5 km from its runway at Schiphol Airport, 
Amsterdam [OND 10]. The initiating event was a sudden malfunction of the 
left radio altimeter, which indicated a height of –8 ft above the ground while 
the airplane was in fact at approximately 1,950 ft above the ground when the 
malfunction was ﬁrst recorded in the ﬂight data recorder. This malfunction 
led to warnings about the airplane conﬁguration, since the warning system at 
that point “thinks” that the airplane is close to the ground, and thus the 
airplane should have its ﬂaps fully or almost fully extended and the gear 
should be deployed. This particular malfunction did not affect the autopilot 
in use at that time, which derived its data from other sources and continued 
to operate in a mode consistent with the altitude of almost 2,000 ft. 
However, it did affect the auto-throttle, which was implemented as a 
separate system, and not integrated with the autopilot. This led to a 
dangerous interaction between these two systems, in which the auto-throttle 
reduced thrust, consistent with its “interpretation” of being close to the 
runway2, while the autopilot tried to maintain the ﬂight path to the runway. 
In its attempt to maintain the ﬂight path, the autopilot increased the aircraft’s 
pitch attitude. The aircraft slowed down, and eventually went into a stall.  
The two pilots and the observer pilot – the ﬂight doubled as a supervised 
ﬂight, in which the captain provides instructions to the ﬁrst ofﬁcer, and a 
safety pilot is present on the observer’s seat – were at that time busy with the 
preparation of the aircraft for the landing. They had been given a shorter 
approach path by Air Trafﬁc Control, a common practice for this runway, 
which meant that there was less time available for all necessary preparations. 
The failure of the left radio altimeter was noticed by the safety pilot, and  
 
                         
2 The auto-throttle entered the “retard ﬂare” mode. 

The Fading Line between Self and System     187 
acknowledged by the captain, but further implications of this failure were 
not discussed, something which, based on the training and system 
knowledge of the pilots, was also not to be expected.  
At 500 ft above ground, the angle of attack had increased so much that 
the stick-shaker was activated. The stick-shaker does what its name says; it 
introduces a violent vibration in the control column of the aircraft to warn 
the pilots of the impending stall. Initially, the ﬁrst ofﬁcer provided the proper 
response to this event, namely application of thrust and a nose-down elevator 
command. He failed to disengage the auto-throttle, however; but by pushing 
the thrust levers forward, the engines will respond. Only when releasing the 
levers again, the auto-throttle will take over again and implement its 
intended lever position. Almost immediately, the captain took over the 
controls; however, he did not grab the throttle and did not disengage the 
auto-throttle either. The auto-throttle then proceeded to retract the throttles 
once again. The delay in handling the imminent stall resulted in a crash 
landing in the ﬁeld, killing the three pilots, a fourth crew member and ﬁve 
passengers. 
9.2.2. Night charter with a Piper Seneca 
This story is a personal communication from a pilot who, on his way to 
achieving line-pilot status, ﬂew many low-paying jobs carrying business 
travelers in small twin propeller aircraft, in this case a Piper Seneca  
[ANO 00]. He was performing a night-time ﬂight over France, in good 
visibility conditions, with sight of the Alps to his left. The Piper Seneca is a 
small twin-engine piston aircraft, carrying ﬁve or six passengers and the 
pilot. The airplane had an unreliable autopilot, and the pilot had heard 
reports of this. However, being adventurous, and intent on enjoying the 
beautiful view, the pilot switched on the autopilot. The autopilot initially 
functioned well, but suddenly started a sharp roll. After quickly switching 
off the autopilot and mumbling an excuse to his passengers, the pilot 
continued his ﬂight manually. 
9.2.3. Air France Flight 447 
On a scheduled ﬂight from Rio de Janeiro to Paris, Air France Flight 447 
was operated by three pilots, a captain and two ﬁrst ofﬁcers [BEA 12,  
 

188     Risk Management in Life-Critical Systems 
WIS 11]. This is common in long-distance ﬂights; at approximately a quarter 
into the ﬂight, the captain left the ﬁrst ofﬁcers in charge of the ﬂight deck, 
while he went off for a nap. On the airplane’s path was a large storm system, 
and warm air which prevented a quick climb over the storms. In addition, the 
glow of St. Elmo’s ﬁre was visible, a phenomenon seen for the ﬁrst time by 
the youngest of the copilots. The crew had failed to plan and coordinate with 
Air Trafﬁc Control a route around the storm system, and when they 
encountered the bad weather had hoped to climb over it, but ended up 
entering the bad weather. Some time after the airplane entered the storm 
clouds, the pitot tubes, which provide data for determining the airplane 
speed, froze over, making speed indications unreliable. Airspeed in an 
aircraft is measured by the pressure difference between the pitot tubes and 
the “static port”, a pressure sensor on the hull of the aircraft. Typically, with 
the pitot tubes frozen but the static port still free, a climb will indicate an 
increase in airspeed, and a descent will indicate a decrease.  
As the failure of the pitot tubes produces incoherent data for the 
airplane’s control augmentation, the autopilot switches off, and the ﬂight 
control reverts to “alternate law”, where the support from the control 
augmentation is reduced, and stall protection is no longer provided by the 
system. Apparently in an effort to climb over the threatening clouds and 
possibly confused by the speed indications, the youngest and least 
experienced of the two copilots started a steep climb, leading to a 
dangerously low airspeed for the aircraft. This co-pilot thereafter seemed to 
be locked into a habit of pulling the stick fully backward. He tried to 
continue to climb by keeping the aircraft in a high pitch-up attitude and 
applying full throttle. However, with the continued pitch-up inputs, the 
airplane developed a full stall, with a high angle of attack. The pitch attitude 
was not that high, but in combination with the steep ﬂight path the angle of 
attack was high and the plane completely stalled. 
In combination with the failure of the three pilots (the captain  
re-entered 
the 
ﬂight 
deck 
after 
almost 
2 
min) 
to 
properly  
communicate, the youngest co-pilot continued to apply the erroneous inputs 
to the side stick, and the plane crashed into the sea before a recovery was 
possible. 

The Fading Line between Self and System     189 
9.2.4. US Airways Flight 1549 
US Airways Flight 1549 was an Airbus A320 on a ﬂight from La Guardia 
airport in New York to Charlotte, North Carolina [NTS 10]. At a height of 
nearly 3,000 ft, the airplane ran into a ﬂock of geese, and the multiple bird 
strikes resulted in the failure of both engines. Upon this, the crew showed an 
excellent response, with the captain taking the controls and the co-pilot 
working on restarting the engines. After considering and opening up – by 
coordination with Air Trafﬁc Control – the options of returning to La 
Guardia or landing at nearby Teterboro airport, the captain consciously 
chose a third option of ditching in the Hudson river. All passengers and crew 
survived the ditching.  
9.3. Development, drama 
When accidents are analyzed later, usually a whole series of errors is 
discovered. Some of these were committed by the people “at the sharp end”, 
such as the pilots, ground crew and maintenance personnel; others were in 
the system from the beginning, such as design or planning errors, and thus 
introduced at the “blunt end”. The four stories discussed above share a 
common trait with classical drama in that their introduction seems innocent 
enough. This could have been either a random occurrence, such as the 
encounter with a ﬂock of geese, or an unwise but at the seemingly harmless 
decision, such as trying out the autopilot in the Seneca, not replanning the 
AF 447 ﬂight around the bad weather or silently accepting a short route  
to the runway in the TK ﬂight. Most systems are relatively impervious to 
single unwise decisions, and also to “random” errors in execution, such as a 
single typing error or forgotten ﬂap selection. Safety margins, redundancy 
and safe practices help to reduce the consequences of single-point failures, 
whether they are of human or mechanical origin.  
However, in two of the above stories, things took a turn for the worse. 
The pilots in the AF 447 and the TK 1951 scenarios did not respond well to 
the situation. One can take this at face value and say the pilots used an 
improper response, but that does not produce a plausible explanation. 
Operators, in general, and operators of vehicles who are also the vehicles’ 
occupants, in particular, have a vested interest in their own safety. It is likely 
they believed that any actions they performed were appropriate, or at least  
 

190     Risk Management in Life-Critical Systems 
they could not think of a better alternative at the time. A current label given 
to this phenomenon is that there is a loss of “situation awareness” [END 04]. 
However, we should remember that this is only a label, and it cannot be seen 
as an explanation. It is also somewhat a strange concept. The pilots in these 
scenarios were certainly aware of the fact that there was a situation. The only 
problem they faced was that they could not interpret all signals that they 
received into a coherent whole or, rather, into a world view that allowed 
them to act in a meaningful way.  
The label situation awareness does not explain why a mismatch between 
the pilots’ perception and the “external world” develops. Situation 
Awareness (SA) is commonly distinguished as having three levels, aptly 
called levels 1, 2 and 3. Level 1 Situation Awareness (SA) describes how 
well operators perceive elements from their environment, level 2 SA 
describes the comprehension of the meaning and level 3 SA describes the 
capability for projection into the future. This suggests a sequential process in 
which level 1 SA forms the basis of level 2, and level 2 forms the basis of 
level 3. Measurement of SA is inherently difﬁcult and potentially intrusive. 
When queried, operators start to pay attention to individual elements in the 
environment, simply to be able to answer the quiz questions, and 
performance on the task might even degrade [VAN 11b].  
For analysts of accident events, and for designers and practitioners who 
are interested in building safer systems, it is crucial to understand how and 
under which conditions such a mismatch between a pilot’s view and the 
actual world state (at least, as far as we can determine the actual world state) 
can develop. A real disadvantage for later spectators in understanding an 
accident is hindsight bias [DEK 06]. Hindsight bias is the effect that 
knowledge of the outcome of an event has on our evaluation of the event. 
Application of hindsight bias can lead to a premature conclusion that a 
mishap must have been the consequence of substandard training or 
performance of the error makers involved, and deprives us of an opportunity 
to learn from other people’s mistakes.  
Dekker illustrates hindsight bias aptly with a cartoon [DEK 06]. The view 
of the people in the accident scenario is the one from inside the winding 
tube. The view of the analysts is the view from outside, overseeing the 
complete winding tube, which represents the path of events, and the danger 
at the end, which represents the outcome of the accident. The major  
 

The Fading Line between Self and System     191 
challenge an analyst faces is recreating the perception of the people involved 
in the incident or accident and explaining what made their view of the events 
a reasonable one, given their perception. 
9.4. Views on human error 
Mishaps can have different phenotypes. First, there is the unforeseen, the 
events that human endeavors were simply unprepared for, because they are 
new and not imagined at the time of design. But in most cases, an accident 
resembles a long series of events, turns of fate as it were, that by themselves 
seem innocent enough. However, combine them all together and the accident 
develops. After the evidence has been gathered, one of the ﬁrst actions may 
be the classiﬁcation of errors. Reason provides a classiﬁcation developed 
within the framework of Rasmussen’s skill-rule-knowledge (SRK) 
taxonomy. The classiﬁcation helps in organizing the events.  
The SRK taxonomy states that most human activities can be classiﬁed 
into one of the three levels stated above. Note that although an activity can 
be determined as belonging to one of the levels, integral human actions 
normally span several levels and cannot thus be dissected. The level of skill 
is concerned with the direct interaction with the physical world. At this level, 
information is being transferred between the human and his environment. 
Integrated motor and perceptual actions, such as those needed for the manual 
control of devices (bicycles, aircraft, cars, etc.), the operation and instruction 
of hardware (that I am mentioning in this chapter right now is one example), 
and the reading of instruments and diagrams are categorized here. The 
categorization is useful in the sense that ﬁnding errors at such a level 
clariﬁes their consequences and remedies. Typically, high rates of skill-
based error indicate that modiﬁcation to control or display devices is needed. 
Also, with proper training (e.g. a typing course), skill can be improved and 
the total effort for performing a skill task can be reduced.  
The rule-based level describes the application of scripted actions and 
reactions. Given certain features detected in the operator’s environment, 
matching action scripts are triggered, leading to human actions. Note that 
skill is involved here too because any activity at this level is based on 
perception and can often lead to action. Examples include performing a 
checklist by a pilot or more menial tasks such as preparing a suitcase for a  
 

192     Risk Management in Life-Critical Systems 
trip. The “rule” to be executed may have been learned and practiced, or 
simply worn in by experience. Errors at this level prompt a revision of the 
procedures – do they match the system that is operated, are they not overly 
complicated or cumbersome – and additional training in learning and 
applying the procedures.  
The knowledge-based level describes the use of reasoning from ﬁrst 
principle and theory. When a situation has no clear rule or response, 
operators are expected to analyze the situation and exercise appropriate 
judgment. Human work at this level is typically slow, and it is easily 
disturbed by external signals. It is also fallible, and people will often “lock 
in” to a hypothesis they formed, ignoring signals disproving that hypothesis 
and explaining them away as exceptions. Errors at this level can indicate a 
lack of procedures and organization or a lack of knowledge.  
Other taxonomies of human activities are also proposed. A similar 
taxonomy to Rasmussen’s, however containing only two levels, is the “fast 
and slow thinking” distinction proposed by Kahneman [KAH 12]. Here, fast 
thinking is the intuitive, immediate response or “gut feeling” that people 
have about a problem. This mode of thinking is effortless, and for many 
situations it is also appropriate. Some situations, under which also many 
contrived laboratory experiments, require the slow thinking to produce an 
appropriate response. The slow and fast thinking distinction might be 
mapped to the knowledge-base behavior and rule-based behavior levels of 
Rasmussen. Kahneman omits the skill-based level, which in Rasmussen’s 
taxonomy is an essential addition, explaining how an expert pilot can read 
the instruments on the ﬂight deck into meaningful data, while an occasional 
visitor simply sees a bewildering array of dials, indicators and control. 
Classiﬁcation helps in pinpointing the “area” of the human activity, in the 
sense of which level of human activity was involved. Useful as this may be, 
it does not always sufﬁciently explain why human action and perception go 
awry of the ideal. In the two fatal accident scenarios described above, human 
operators based their actions on a wrong assumption for an extended period. 
In the TK ﬂight case, pilots failed to timely disconnect the auto-throttle that 
is fed with erroneous information, obviously content and in the belief that 
there is no reason to question the autopilot’s functioning. The AF pilots 
failed to recognize the loss of stall protection and the development of a full  
 
 

The Fading Line between Self and System     193 
stall. In an attempt to understand why it is so difﬁcult to know the truth – if 
there really is a truth out there – we will consider the epistemology proposed 
by C.S. Peirce. 
9.5. Peirce’s triadic semiotic system 
Returning to the AF ﬂight, at some point a mismatch develops between 
what we now know, which is that the airplane developed a full stall, with a 
moderate pitch attitude but a very high ﬂight path angle, and the probable 
beliefs of at least the youngest of the two co-pilots, who kept pulling the 
stick in an attempt to make the airplane climb, and showed signs of not 
understanding why the aircraft with all these inputs would not gain altitude. 
His colleague also fails to understand what happens (“On a pourtant les 
moteurs!”/“But we still have the engines”, is one of his lines in the voice 
recorder transcript). In an abstract sense, the pilots had received signals, and 
confusing ones at that time, from their instruments and from the motions of 
the airplane, but somehow these signals did not result in a correct 
identiﬁcation of the events.  
 
Figure 9.1. Depiction of Peirce’s triadic relationship between  
object, sign and interpretation 

194     Risk Management in Life-Critical Systems 
The problem for the pilots is assigning a meaning to the signals they 
receive. Initially, through the static port blocking, the right-hand speed 
indicator might have indicated an increase in airspeed due to the climbing of 
the airplane3.  
The two co-pilots were faced with a set of incongruent signals: 
– Saint Elmo’s ﬁre, which apparently made a deep impression on the 
youngest of the pilots; 
– differences in airspeed between the left- and right-hand airspeed 
indicators; 
– the autopilot and auto-throttle switching off, and the ﬂight director 
needles disappearing; 
– the hot air, and the inability of the aircraft to climb, and the later 
apparent ability to climb and maintain airspeed; 
– the storm system and the turbulence caused by it; 
– the ﬂight control system reverting to alternate law, which in itself is 
ambiguous, because alternate law can consist of varying degrees of ﬂight 
mode protection; 
– stall and overspeed warnings, and a later silencing of a stall warning 
because angle of attack values become too large to be seen as valid. 
In Peirce’s sign system, there is a relation between the object being 
denoted, the “signiﬁed”, and the sign or symbol itself. A sign can be any 
perceivable bit of information, a word, proposition or argument, or also a 
(mechanical) indication. Signs are decoded into interpretant or interpretant 
sign, which is an interpretation, meaning that it results in an effect on the 
receiver of the sign. This effect may be an emotion or a piece of knowledge. 
The interpretant is also a sign, but one that now indirectly depends on the 
object (Figure 9.1). 
Interpretation in this view leads to a correspondence between the object 
and an interpretant. In Peirce’s pragmatic approach, the exact nature of this 
                         
3 The signals from the left airspeed indicator were logged in the ﬂight data recorder, but the 
signals from the right indicator  were not. It is safe to assume that at some time all pitot tubes 
were blocked, but from the available data it is not completely clear at what time this blockage 
was resolved for the right-hand (copilot’s) instruments. 

The Fading Line between Self and System     195 
correspondence cannot be answered. Instead, this correspondence between 
the object and the interpretant, and indeed the whole interaction, is 
considered successful if it enables successful outcomes in the interaction 
between the observer and the object. 
The interpretation of a sign, to form the interpretant, depends on previous 
signs interpreted by the receiver. Previous interpretants may help or hinder 
the timely and correct (again, in the sense of being successful for the ensuing 
interaction!) interpretation. 
In the Piper Seneca scenario, the sign given is the sharp roll of the 
aircraft. The object is the aircraft and its autopilot in a ﬂight. The interpretant 
is the result of interpreting the sign. In this case, the interpretation is helped 
with previous signs; the knowledge that the Seneca autopilot may be 
unreliable, the knowledge that the air is clear and quiet, so that external 
causes are not likely. The interpretant in this case is the correct conclusion 
that the autopilot failed, and the success is apparent, in the fact that the agent 
acting on this interpretant, the pilot, quickly recovers from the upset 
situation. 
In the TK scenario, several signs need to be combined. These are the 
failure of the left autopilot, the actions of the auto-throttle, which, in a 
Boeing aircraft at least, are visible as a movement of the throttle levers, the 
fact that a steep descent was needed to recover the glide-slope (which 
provides an alternative explanation for the reduction of thrust), the status 
messages from the auto-throttle and the reduction of speed on the glide 
slope. A lacking sign in this case was the knowledge that the left radio 
altimeter would be used by the auto-throttle to initiate the ﬂare mode, with as 
a consequence that the interpretant that the auto-throttle was not functioning 
was missed. Several “noise” signs are present, since the ﬂight is also a 
training ﬂight, and not all interaction between the pilots is on the ﬂight, it is 
also on the training purpose. The correct interpretant on the stalled status of 
the airplane in this case came too late, and was only acted upon correctly by 
the co-pilot. Recognizing the stall, and aided by recent training in handling a 
stall, he proceeded to provide the proper response. However, he was 
interrupted by the captain. None of the pilots seems to have been initially 
aware of the auto-throttle mode, since the captain did not push the throttle 
further forward, upon which the auto-throttle closed the throttle again. 

196     Risk Management in Life-Critical Systems 
In this case, a partial interpretation; stall, but not knowing whether the 
auto-throttle was disengaged, in combination with the proper skill in 
handling the stall (full thrust and stick forward to break the stall) would have 
resulted in a successful outcome. Error classiﬁcation in this case leads to a 
recommendation to provide training to maintain recency in stall recovery 
skills. 
In the aftermath of the TK accident, improvements of the instructions to 
the pilots about the functioning and use of the auto-throttle were made, and 
the avionics software was corrected to include a check on the consistency of 
both radio altimeter readings as a condition for the use of the auto-throttle. 
It is now clear that in the AF scenario, the pilots did not consistently 
reach a proper conclusion about the prolonged and deep stall of the aircraft. 
For the youngest co-pilot, the fact that the airplane’s ﬂight control law would 
not protect them from the stall was not clear, which made his almost 
instinctive reaction to pull back on the stick and apply thrust inappropriate at 
the time, while it would have worked in all his previous ﬂights on Airbus 
aircraft. The captain and the other co-pilot did not cue into the fact that the 
youngest co-pilot had been giving maximal nose-up commands, basically 
making the signs they received inexplicable. The added difﬁculty in this 
scenario is the indirect nature of the signs. The airspeed is derived from an 
instrument that relies on the comparison of pitot pressure and static pressure. 
Interpretation without additional knowledge of this system, as is normally 
sufﬁcient, leads to a false interpretant, a speed value that is incongruent with 
other signals and with expectation. The relationship between the object 
(aircraft) and subject (pilots) is indirect, since it relies on the speed 
instruments as an intermediary object. 
Peirce’s semiotic system offers several insights into the nature of 
“aligning with reality” needed for successful integration: 
– Signs are not direct messages from an object; they depend on an object. 
Signs differ in the degree of interpretation they allow. The object can be an 
intermediary (such as the pitot-static system is an intermediary for 
determining airspeed). 
– The subject needs to interpret the signs, leading to an interpretant sign, 
depending on the object. The interpretation depends on previous 
interpretants, basically the background and immediate knowledge of the 
subject. This, for example, explains why a direct replay of the AF 447 ﬂight  

The Fading Line between Self and System     197 
is now unlikely; most, if not all, Airbus pilots will now have received 
information about the AF 447 crash, and this will alter their future 
interpretation. 
 
Figure 9.2. Diagram illustrating the problems of determining causes and control actions in 
an uncertain system. An unknown disturbance might be acting on the system, a shift in its 
parameter may have happened, leading to a qualitative change in dynamics, or a structural 
change might have occurred, leading to a signiﬁcantly different system. The innovation or 
surprise i is the difference between observation and expectation, and may lead to adjustment. 
Whether control is based on observation or on expectation is uncertain, and probably 
variable 
9.6. Abduction, or how do humans form conclusions 
In formal logic, deduction and induction are distinguished. Deduction is 
the derivation of a third true fact or statement from two or more general 
statements. A simple example is: (1) all men are mortal; (2) Aristotle is a 
man; therefore (3) Aristotle is mortal. 
Induction works in the reverse sense. From speciﬁc observations, a more 
general law is derived. The more general law is not proved beyond all doubt; 
a counterexample might be found that disproves the inductive logic. In 
science, it is common to indicate how certain one is about the inductive 
process. 
Peirce proposes abduction as a primary and initial form of human 
reasoning. Instead of following the inductive process, by which one 

198     Risk Management in Life-Critical Systems 
concludes that with a certain degree of reliability an underlying theory must 
be governing observations, abduction signiﬁes the search for a plausible 
hypothesis that explains the observed signs (note that the found hypothesis 
itself then becomes a sign, an interpretant). Abduction and induction are 
relatively similar; however, with induction we normally determine the 
degree of certainty for a hypothesis. Abduction is often imprecise; its 
success is indicated by the “feeling” that an explanation is the right one. 
The interpretative process that pilots are faced with can be likened to a 
combined state and parameter estimation problem. When, in systems theory, 
one is presented with the signals from a “black box”, essentially a system of 
which the inner workings are not known, or a “gray box”, a system of which 
part of the inner structure is known, and given enough information in the 
observed input and output signals, he/she must determine the equations 
governing the behavior of the black/gray box (see Figure 9.2). 
Given such a system, any number of inputs and outputs may be unknown, 
essentially containing noise signals. When the system responds to commands 
given in the expected manner, there is no surprise. However, when the 
system responds differently, any number of causes may be present: 
– There is an unknown but normally possible input signal present, 
another agent also inﬂuences the system. An example for this would be the 
effect of turbulence. 
– There is a change or noise on the output signal, causing an erroneous 
reading. 
– There is a change in the basic parameters of the system. Ice 
accumulation on the wings does this, inﬂuencing lift and weight. 
– The structure of the system has changed, making the signs sent by the 
system no longer interpretable if the observer does not consider the change 
of the system as a hypothesis in creating the interpretant. 
This latter case occurs in all four scenarios. The Piper Seneca autopilot 
starts reacting to faulty signals. The US ﬂight loses all engine power; the AF 
ﬂight loses the relationship between static and pitot pressure, leading to 
erroneous speed readings and later enters a part of the ﬂight envelope where 
the response to the stick is essentially reversed. The TK ﬂight’s auto-throttle 
changed to a malignant device. 

The Fading Line between Self and System     199 
For adapting to the changed behavior, it is imperative that the pilots ﬁnd a 
working hypothesis (with a working hypothesis automatically being a right 
one in Peirce’s epistemology). The remaining question is now why in some 
cases it takes so long to ﬁnd a working hypothesis? For that, I can only offer 
a conjecture: 
Failure hypotheses are preweighted with their distance from 
the actor and their proximity in memory. 
This means that without further evidence, an external input signal (e.g. 
windshear and turbulence) is the ﬁrst hypothesis that will be considered. 
Then an erroneous reading (e.g. stuck needle and instrument glitch) will be 
considered, indicating a failure of a part of the system that the pilots are 
working with. More inwardly, a change in system parameters is assumed. 
The last two hypotheses considered are a structural change in the system and 
a structural error in the actor himself/herself. 
But if that is true, why doesn’t the Seneca pilot consider turbulence ﬁrst? 
The explanation is fairly simple. The Seneca pilot’s hypotheses are limited. 
Indeed, turbulence and an encounter with another aircraft are in there. 
However, from the reports on the reliability of the autopilot, the autopilot 
malfunction is the strongest and foremost hypothesis from the moment he 
tries out the device; it is preloaded with suspicion already. The reaction from 
the autopilot is interpreted in light of the preceding interpretants: the 
knowledge of the unreliable nature of these autopilots and the sensation felt 
in trying out the device as a new experience. 
The pilots in the USAir ﬂight were primed with an awareness of geese, 
and as they saw the formation, this awareness was strengthened. Even 
stronger signs were given when geese impacted with the windshield, turning 
it dark brown and making several loud thuds. They had the right hypothesis 
spelled out for them. Also, the change introduced into the aircraft is not that 
uncommon to them. Engine failures, although usually single-engine failures, 
are bread and butter in the yearly check ride, and a complete engine failure is 
not uncommon in the folklore of air disasters. 
The TK ﬂight pilots initially apparently believed the ﬂight path imposed 
on them as congruent with the throttle movements. They also did not have a 
hypothesis for the auto-throttle behavior and its transition into ﬂare mode, as 
this was lacking from the training materials. The AF pilots, or at least the 

200     Risk Management in Life-Critical Systems 
junior one, must have blamed the weather, with the mysterious St. Elmo’s 
ﬁre, a hot atmosphere that prevented climb, and then an apparently rapid 
climb that in fact consumed all kinetic energy until no speed was left, but 
this change was masked by the erroneous speed indications. 
Both the TK ﬂight and the AF ﬂight had far less congruent clues, 
enabling the pilots to get stuck at plausible but wrong initial hypotheses. The 
concept of abduction, of which numerous examples are also given in  
[KAH 12], explains the process by which human operators can reach 
incorrect conclusions. “Primed” hypotheses have a high chance of being 
chosen. A further search is postponed until the hypothesis no longer “feels 
right”, i.e. there are too many signs that cannot be satisfactorily explained. 
9.7. Heidegger and Descartes 
Human–machine interaction, and the interaction of humans with 
automated devices, is not a new ﬁeld. However, the ﬁeld is not that old 
either. The work by Frederic Winslow Taylor emphasized the increase of 
productivity [TAY 11]. World War II saw an emphasis on selection and 
development of theories for the performance of humans as controllers of 
dynamical systems. Subsequent “waves” of scientiﬁc fashion modeled 
humans with the theory at hand, including classical control theory, optimal 
control theory, fuzzy logic, neural networks and (fallible) automata. 
A persisting trend in these approaches is that the human operator, or 
rather his mind, is considered as a separate entity within the system;  
the human mind is an information processor, with inputs and outputs, while 
the speciﬁc rules or logic that describe the information processor vary with 
the modeling approach. With such a view, derived from René Descartes’ 
works [DES 11], the format of the message (or signs) is relatively 
unimportant – as long as information from the outside world arrives intact at 
the mind, the proper decision will be made. 
However, we know that the format of the message is important. One 
example is given by the research on conﬁgural displays. Conﬁgural displays 
are displays that, by the layout of the presentation of information [BEN 92], 
produce emergent perceptual features from the interaction between lower-
level graphical elements. A simple example is the alignment of bar-type 
engine displays in a multi-engine airplane. The emergent feature here is the 

The Fading Line between Self and System     201 
alignment of the bars for different engine parameters (the lower level 
graphical elements), with alignment indicating symmetric operation of the 
engines.  
It may seem too obvious, but such simple adaptations improve the 
performance of the combined human–machine system. The conclusion we 
must draw here is that our mind is not like a computer, with input channels, 
output channels, and a processor in between. The example of the conﬁgural 
displays makes it clear that the way in which information is formatted can 
signiﬁcantly simplify our task. The models of a human as a fallible and 
limited computing machine, with perceptual input, neural processing and a 
response as outcome (such as in [SMI 97]), do not capture this. 
The real improvements in understanding our and other animals’ behavior, 
and in improving the interfaces for our cognitive work, come only after 
releasing the Carthesian split between mind, body and environment. 
Gibson’s work on ecological perception [GIB 79], and extensions of this 
work into the animal kingdom [LEE 81], shows that a focus on studying the 
mind as a calculating machine is the wrong starting point. Our behavior is 
largely shaped by the constraints put upon our actions by the environment. In 
other words, if you want to know why the animal behaves thus, study the 
environment ﬁrst [SIM 96]. 
Gibson’s work is of paramount importance to the way we understand the 
interaction between humans and their environment when operating vehicles. 
However, Gibson does not explicitly consider the interaction between the 
human, the vehicle and the environment. For that, we should turn to the 
work of Heidegger [HEI 72, SHE 02, VAN 10]. Consider yourself driving a 
nail into the wood with a hammer. If the hammer is well balanced and 
familiar to the hand, most people will claim that they can drive the nail into 
the wood. The tool by itself, in this case the hammer, becomes part of the 
actor. Heidegger calls this Zuhandenheit, which is translated by “being-at-
hand”. This is in strong contrast with Descartes’ notion of a separation 
between mind and body; Heidegger does not even separate mind, body and 
tool. 
One can say that in most cases, a good tool is transparent to the user. The 
hammer is not noticed, and the carpenter sees himself as driving the nail into 
the wood. Only when some abnormality occurs, e.g. the hammer head works 
itself loose, dose the tool become noticed and seen as a separate entity. This, 

202     Risk Management in Life-Critical Systems 
in Heidegger’s terms, is Aufdringlichkeit, translated with “being present-at-
hand”. 
Our science and engineering are rife with examples where we follow the 
Carthesian convention of separating mind, body and tool. Block diagrams 
show individual blocks for displays, controls, automation, instrumentation 
and the human user. The problem with this representation is that it does not 
do justice to the way human users perceive their contact with the tools they 
are using. An airplane in this case must be viewed as a huge and complex 
hammer. If it has been designed right, the pilot will perceive of himself as 
ﬂying, not as sitting in a machine and operating that machine, while the 
machine is ﬂying. This sense of unity with the tool is what makes us efﬁcient 
and versatile tool users, and designers of “tools” are naturally striving to 
promote this efﬁcient and also pleasurable interaction with our tools. 
The claim I would make here is that what goes for the hammer also goes 
for more complex tools, such as the control augmentation system for the ﬂy-
by-wire aircraft. As long as the reactions from the aircraft are as expected, 
the Control Augmentation System (CAS) is not noticed by the pilots as a 
component that needs to be supplied with command inputs. The pilot sees 
himself as ﬂying the aircraft, and being connected to the motions of the 
aircraft. If the design is right, the device is reliable and the user is well 
trained, a similar thing happens to our tools, our vehicles and to prostheses; 
they become part of an extended self, and the pilot becomes a ﬂying being, 
the woodworker becomes a hammering, sawing and milling being, and the 
amputee becomes whole again. 
This sheds yet another light on the interaction between pilots and the 
aircraft automation. In the case of the Piper Seneca, the pilot is consciously 
testing the autopilot. The autopilot is never “being-at-hand”; it is always 
“present-at-hand”. It is seen as a potentially foreign entity whose actions 
need to be watched, and as a consequence its signs are interpreted as coming 
from outside, not a part of the human–tool system. This is, of course, a mode 
of operation that requires effort. If the autopilot is a reliable device and used 
in daily operations, the interaction of the pilot with the device will change, 
and he will use the device without giving it much attention, as indeed the AF 
and TK ﬂight crews were doing with their automation. 
The AF copilot, on the other hand, is completely reliant on the ﬂight 
control system and the envelope protection. Flight with alternate law, or 

The Fading Line between Self and System     203 
even, in a further degradation, direct law or mechanical law, was not part of 
the aircraft; rather, it was only part of training sessions in the simulator. It is 
estimated that the entire ﬂeet of Airbus aircraft have not ever seen an 
instance of alternate law. Clearly, the link in the copilot’s head is still that 
pulling the stick will lead to the climbing of the aircraft, and the protection 
will limit this input to prevent stall. All warning signals are interpreted 
(explained away) as being caused by something external to the pilot–ﬂight 
control system combination. 
When considering the interaction between a pilot and his aircraft in light 
of a Carthesian approach in identifying systems and subsystems, all 
interaction between the aircraft and the pilot can be seen as signs that are to 
be interpreted. However, adopting Heidegger’s vision means that – until 
clear signs are given that the aircraft or parts of it are no longer obeying the 
pilot’s commands, in other words they become Aufdringlich, present-at- 
hand – these interactions are not even consciously being registered as signs. 
Instead, the pilot watches the signs from the outside world, counting on the 
fact that the device is still part of his extended self. The crucial sign in these 
situations is thus the sign of Aufdringlichkeit, and that sign must get through 
to the pilot. 
9.8. Designing the signs 
In the AF ﬂight, the stall warning was given 75 times. This warning is 
designed to be hard to ignore. However, there is no evidence that the pilots 
saw the stall warning as something meaningful. Somehow, amidst all the 
confusion, the stall warning did not become a useful interpretant. To combat 
all the confusion, and to arrive at proper signiﬁcants, the AF pilots would 
have needed to: 
– interpret speed indication as a pressure difference between the pitot and 
static pressure ports; 
– interpret the sudden change in speed as a change in the pressure 
difference, and consider the potential for blockage of one of the ports; 
– interpret the attitude in combination with the vertical and horizontal 
speed as an indication for angle of attack, and consider that the angle of 
attack was too large for the aircraft, meaning that the aircraft is stalled, as a 
consequence the drag is large and the aircraft loses large quantities of 
energy. 

204     Risk Management in Life-Critical Systems 
Such an interpretation is, of course, not always available; it requires a 
considerable amount of insight into the physics of flight, and a quiet desk to 
come up with the explanation. The alternative is to “drill” pilots so that they 
simply provide the right response when the stall warning is given. The 
drawback of this is that the stall warning may then never provide a false 
alarm. 
The dilemma here appears to be between simply providing a command 
interface to the pilot and telling them “just follow the needles”, or requiring 
that the pilots apply deep system knowledge to interpret the signs they 
receive. Interpretation of pitot/static port failures and their effects is tricky 
even in non-ﬂy-by-wire aircraft, as illustrated by the Aeroperu and Birgenair 
crashes [WAL 00]. When the ﬂight control system, and possibly more 
systems, depends on these data sources in non-obvious – at least to the pilots 
– ways, ﬁrst ﬁnding a hypothesis that explains the root cause and then 
projecting the consequences and determining the proper action becomes 
more difﬁcult than in a simple mechanical aircraft. 
This touches on the question of how much automation we can add before 
a system becomes less safe. Currently we have automated flight control 
systems that act on integrated and in principle fault tolerant input signal from 
redundant sources. The complexity of such systems can quickly become too 
much for even the most proﬁcient pilot, especially when considering that the 
typical airliner ﬂight deck in an emergency situation is not the best 
environment for quiet and deep thought. One option is here to design 
displays that show the underlying system constraints, basically explaining 
how signs on the ﬂight deck arise from measurements, similar to displays 
that explain the energy relation in an aircraft [AME 09]. Also the actions of 
the automation should be visible against a backdrop of all possible legal 
control actions, not unlike the goals of a design project in Air Trafﬁc Control 
where the cooperation between automation and human agents is facilitated 
by acting on and displaying a common representation for both humans and 
automation [VAN 11a]. 
9.9. Consequences 
Learning from error, and designing systems that are resilient to error, can 
be approached in many ways. First and foremost, we should realize that the 
design tools we use are strongly based on Carthesian views, the division of a 

The Fading Line between Self and System     205 
system into subsystems, and the subsequent combination of these 
subsystems. When applied with due diligence, this is acceptable for the 
technical parts of the human–machine systems we compose. However, when 
considering the human as a component in a larger system, note that the 
human as a subsystem will behave differently from technical systems. 
Speciﬁcally, interpretation of the signals passed to a human operator depends 
strongly on the format in which these signals are presented. This is not only 
a question of picking the right colors and font sizes, but it requires an in-
depth study of the “work domain” to arrive at a representation that supports 
operators/pilots in correctly interpreting their environment [VIC 99]. 
To properly understand and design the interaction between human users 
and their tools, including the instrumentation and automation present in these 
tools, we need to turn to Heidegger’s views on Zuhandenheit and 
Aufdringlichkeit for a more accurate view of the relationship between 
humans and their environment. A number of conclusions can be drawn from 
the application of this view: 
– Advancing technical reliability and ease of use: the paradox of system 
failure in the presence of increasing technical reliability has been addressed 
in studies investigating the relationship between trust, self-conﬁdence of a 
human user and the reliance on automation [LEE 94]. Both reliability and 
ease of use promote transparency of the tools that we use, be they automated 
controllers or instrumentation, and by themselves, these properties are of 
course desirable. The only way in which our increased Zuhandenheit can be 
offset is by an increased Aufdringlichkeit in the case of failure. That is, as a 
device becomes more reliable and user-friendly, if it fails, its failure must 
become more clear and conspicuous. In this perspective, the tendency in air 
transportation to leave well enough alone – which in air transportation 
translates into re-using certiﬁed components as much as possible – can be 
dangerous. We rather prefer to continue ﬁtting aircraft with a set of 
unconnected previously certiﬁed systems, rather than properly integrating 
support systems and extending them to add a check of the integrity of their 
functioning, since such modiﬁcations force a new certiﬁcation process. 
– Training: just as pilots are currently instructed on the limitations of 
their own sensory system, they should be warned of the human tendency to 
become one with the tools we use and not notice them as separate from 
ourselves.  

206     Risk Management in Life-Critical Systems 
– Alarm ﬂoods and alarm ﬁltering: in many complex systems, alarms are 
used as a means of directing the attention of the human operators. However, 
the tuning of alarms is a labor-intensive and expensive activity, and in 
general not each alarm indicates a true and single failure. Some failures or 
conditions in a plant can cause alarm ﬂoods, and from the multitude of 
alarms the operators have to determine the underlying cause [VAN 97]. 
Alarm ﬁltering software is applied to reduce the ﬂood, helping operators to 
better handle the load. However, if an alarm indicates a problem with a part 
of the instrumentation or the automation, the failure should again be made 
conspicuous. 
– Design and evaluation of automation and interfaces: for the evaluation 
of interfaces and automation, the Carthesian view suggests the measurement 
of information transmission rates and veriﬁcation of the match between the 
operator’s impression of the situation and the “actual” situation [END 04]. In 
Heidegger’s/Gibson’s view, the knowledge of the actual situation can only 
be outside the combination of observer and tool. Awareness about the tool 
can only be the result of Aufdringlichkeit, conspicuousness of the tool due to 
failure or unfamiliarity. A new approach to evaluate displays and support 
systems in general would be to score the tool on the type of interaction it 
invites. If it invites interaction with the work domain, and users comment on 
actions and properties in the work domain, the tool is transparent. If, on the 
other hand, it invites interaction with the tool, and users comment on tasks 
and procedures to operate the tool (select page no …, click on the …), the 
tool is not transparent. 
– Supporting abduction: considering Peirce’s triadic system (Figure 9.2), 
signs can only be interpreted in the context of earlier signs. The abductive 
process has as a consequence that an operator may content himself with a 
matching but wrong hypothesis/interpretant. In addition to training pilots to 
question their tendency to take their tools for granted, they should also be 
trained to challenge their signs/beliefs since they were arrived at through 
abduction. Once a sign is turned into an interpretant, it is lost as a perception. 
To support a review of a hypothesis (actually, also an interpretant) produced 
by abduction, it would be best to revisit the signs, retracting history. Some 
sort of external memory or review capability in an interface can play an 
important role in this process.  
In the design of a human–machine system, we should also strive for 
displays that explain the constraints in the work domain. An angle of attack 
warning integrated in a Vertical Situation Display might have made sense to 

The Fading Line between Self and System     207 
the Airbus pilot, as would have an indication of the command given to the 
ﬂight control system by the junior copilot.  
9.10. Conclusions 
The question of what to automate and what not used to start with 
checking Fitts’ list. However, as automation gets more sophisticated, and our 
demands on the systems have increased, the answers have become more 
complicated. Now we know that there are bounds on our capacities for 
automation; automation is particularly poor in “open” systems, where the 
interaction with the environment is varied and poorly speciﬁed. On the other 
hand, automation enables us to achieve a consistent high-level performance 
in many cases. The practice evolved to a combination of human operators 
and advanced automation. The human operator in this case is responsible for 
interpreting the poorly deﬁned signals in the environment (weather 
conditions, company policy, interaction with other human agents, etc.) and 
translating that into crisp and clear (and often low-level) commands for the 
automation. 
Errors in such complex systems are often difﬁcult to interpret. 
Investigators must resist the temptation of hindsight bias in interpreting the 
error scenario. Peirce’s epistemology, and the notion of abduction, helps in 
explaining the (probable) interpretation made by the operators in an error 
scenario. 
Although extremely capable in some aspects, our current automation is 
lacking in understanding the complexities of most open work domains and 
also of its human users, and thus must be seen as a tool, not as an associate 
in its own right. It is claimed that Heidegger’s view on tools also extends to 
advanced automation and instrumentation. This has as speciﬁc consequences 
that as automation and instrumentation become more reliable, any failure of 
these components should become more conspicuous to offset the 
transparency of reliable tools. 
Verification of automation and other support tools should be done by 
answering three questions: (1) is the tool transparent (zuhanden) when in 
normal use, (2) does the tool become conspicuous when it loses its 
functionality and, I would like to add, (3) can the tool be inspected if the user 
so wishes? 

208     Risk Management in Life-Critical Systems 
9.11. Bibliography 
[AME 09] AMELINK M., VAN PAASSEN M.M., MULDER M., “Examples of work 
domain analysis applied to total energy control system”, Proceedings of the 15th 
International Symposium on Aviation Psychology, Wright State University, 
Dayton, OH, pp. 479–484, 27–30 April 2009. 
[ANO 00] ANONYMOUS, Experiences with a piper seneca autopilot, Personal 
communication, 2000. 
[ANO 09] ANONYMOUS, UAV outback challenge - wikipedia, the free encyclopedia, 
2009. 
[BEA 12] BEA, Final report on the accident on 1st June 2009 to the Airbus A330-
203 registered f-GZCP operated by Air France Fight AF 447 Rio de Janeiro –
 Paris (English ed.), technical report, BEA, Paris, France, July 2012. 
[BEN 92] BENNET K.B., FLACH J.M., “Graphical displays: implications for divided 
attention, focused attention and problem solving”, Human Factors, vol. 34,  
no. 5, pp. 513–533, 1992. 
[DEK 06] DEKKER S., The Field Guide to Understanding Human Error, Ashgate, 
Aldershot, England/Burlington, VT, 2006. 
[DES 11] DESCARTES R., Meditations on First Philosophy, Cambridge University 
Press, 1911 
[END 04] ENDSLEY M.R., “Situation awareness: progress and directions”, in 
BANBURY S., TREMBLAY S., (eds.), A Cognitive Approach to Situation 
Awareness: Theory, Measurement and Application, Ashgate, Aldershot, UK,  
pp. 317–341, 2004. 
[GIB 79] GIBSON J.J., The Ecological Approach to Visual Perception, Houghton-
Mifﬂin, Boston, 1979. 
[HEI 72] HEIDEGGER M., Sein und Zeit (Being and Time), 12th ed., Max Niemeyer 
Verlag, Tübingen, 1972. 
[KAH 12] KAHNEMAN D., Thinking, Fast and Slow, Penguin, London, 2012. 
[LEE 94] LEE J.D., MORAY N., “Trust, self-conﬁdence, and operators’ adaptation to 
automation”, International Journal of Human-Computer Studies, vol. 40, no. 1, 
pp. 153–184, January 1994. 
[LEE 81] LEE D.N., REDDISH P.E., “Plummeting gannets: a paradigm of ecological 
optics”, Nature, vol. 293, no. 5830, pp. 293–294, 1981. 

The Fading Line between Self and System     209 
[NTS 10] NTSB, Loss of thrust in both engines after encountering a ﬂock of birds 
and subsequent ditching on the Hudson river, US Airways Fight 1549, Airbus 
A320-214, N106US, Weehawken, New Jersey, January 15, 2009, Technical 
Report NTSB/AAR-10/03, National Transportation Safety Board, Washington, 
DC, 2010. 
[OND 10] ONDERZOEKSRAAD VOOR VEILIGHEID, Turkish airlines, Neergestort 
tijdens Nadering, Boeing 737-800, nabij Amsterdam Schiphol Airport, 25 
Februari 2009 (Turkish airlines, crashed during approach, Boeing 737-800, near 
Amsterdam 
Schiphol 
Airport, 
February 
25, 
2009), 
technical 
report, 
Onderzoeksraad voor Veiligheid, the Hague, NL, 2010. 
[SHE 02] SHERIDAN T.B., “Some musings on four ways humans couple: 
implications for systems design”, IEEE Transactions on Systems, Man, and 
Cybernetics – Part A: Systems and Humans, vol. 32, no. 1, pp. 5–10, January 
2002. 
[SIM 96] SIMON H., The Sciences of the Artiﬁcial, 3rd ed., MIT Press, Cambridge, 
MA, 1996. 
[SMI 97] SMIDTS C., SHEN S.H., MOSLEH A., “The IDA cognitive model for the 
analysis of nuclear power plant operator response under accident conditions. Part 
I: problem solving and decision making model”, Reliability Engineering and 
System Safety, vol. 55, pp. 51–71, 1997. 
[TAY 11] TAYLOR F.W., The Principles of Scientiﬁc Management, Harper & 
Brothers, 1911. 
[VAN 97] VAN PAASSEN M.M., WIERINGA P.A., “Alarms, alerts and annunciations”, 
Journal A, vol. 38, no. 4, pp. 16–22, December 1997. 
[VAN 10] VAN PAASSEN M.M., “Heidegger versus Carthesian dualism or where is 
my hammer?”, IEEE Systems, Man and Cybernetics Conference, Istanbul, 
Turkey, pp. 1684–1688, October 2010. 
[VAN 11a] VAN PAASSEN M., BORST M.C., MULDER M., et al., “Designing for 
shared cognition in air trafﬁc management”, in SCHAEFER D., (ed.), Proceedings 
of the SESAR Innovation Days, EUROCONTROL, Toulouse, France, pp. 1–6, 
2011. 
[VAN 11b] VAN EIJCK J., BORST C., MULDER M., et al., “The effect of measuring 
situation awareness on pilot behavior”, in FLACH J., VIDULICH M., TSANG P., 
(eds.), Proceedings of the 16th International Symposium on Aviation 
Psychology, Dayton, OH, pp. 166–171, April 2011. 
 

210     Risk Management in Life-Critical Systems 
[VIC 99] VICENTE K.J., Cognitive Work Analysis: Toward a Safe, Productive, and 
Healthy Computer-Based Work, Lawrence Erlbaum, Mahwah, NJ, 1999. 
[WAL 00] WALTERS J.M., Aircraft Accident Analysis: Final Reports, McGraw-Hill, 
New York, 2000. 
[WIS 11] WISE J., What really happened aboard Air France 447, December 2011. 
Available 
at 
http://www.popularmechanics.com/technology/aviation/crashes/ 
what-really-happened-aboard-air-france-447-6611877. 
 

10 
Risk Management: A Model  
for Procedure Use Analysis 
10.1. Introduction  
The United States Nuclear industry has seen a recent rise in events, due to 
improper operator engagement in the control room [INP 10]. A rise in 
similar issues has been documented in maturing organizations across 
domains, such as human spaceflight. The level of detail and number of 
procedures is increasing. This has been known to cause the unintended 
emergent behavior of automating the human operators [BOY 13]. This 
means the operators are less prepared to handle context; this issue will only 
intensify with new generations of operators. 
In this chapter, we discuss methods of study to understand the U.S. 
nuclear industry, function allocation (FA), situational awareness (SA), and 
automation. We also plan to provide some resolutions to resolve the issues 
that often come with automation. Our research aims to prove that strict 
adherence to procedures and rigid compliance will not prevent incidents or 
increase safety; our expectation is that our findings will lead to a paradigm 
shift regarding procedure and training based organizations. Understanding 
the proper balance of automation and human decision-making is the key to 
creating a plant that is safe, efficient and reliable.  
The industry recognizes that there has been an increase in incidents, and 
that the underlying causes are primarily organizational including “inadequate 
                         
Chapter written by Kara SCHMITT. 

212     Risk Management in Life-Critical Systems 
recognition of risk, weakness in the application of significant operating 
experience, tolerance of equipment or personnel problems and a significant 
drift in standards” [INP 10]. Additionally, the age of plants may also be 
contributing in some way to the rise in events.  
According to testimony from expert operators, one of the contributing 
factors is the rise in procedurealization of control rooms. In November 2008, 
at the Dresden station, three control rods were withdrawn during refueling. It 
was eventually concluded that the cause of this incident was “latent 
procedure deficiencies.” In December 2009, at the Vogtle Electric 
Generating plant, a scram was triggered because instrument air was 
inadvertently isolated which led to the loss of the main feed water flow to 
the steam generators. This was caused by “operators performing the wrong 
section of the system operating procedure”. “It is believed, as indicated in 
these events, that understanding of the basis of procedures, systems and 
components, and integrated plant operation is often weak” [INP 10]. 
This research claims that operators are not engaged and thinking about 
the physical operations of the plant because they have been instructed to 
follow procedures without proper fore-thought. Where operators once knew 
their plant and its inter-connected actions, now more often than not, 
emphasis is being put on procedure and policy rather than understanding 
hardware and actions. The issues cited in SOER 10-2 are examples of 
“workers not fully understanding or anticipating the effects of their actions”. 
The use of the standard procedure has become a “safeguard” for both 
management and for the operators as well. If anything goes wrong, the 
procedure is the scapegoat. The nuclear power industry has been 
characterized as a highly proceduralized environment [DOU 03]. Operators 
are being trained out of doing anything without the procedure. In one 
simulator instance during our research, we found an operating team that 
allowed diesel lubricating oil to leak at an excessive rate for 4 min while the 
team located a procedure step to turn off the pump. The operating team was 
well aware that the pump needed to be shut down, but had to locate a 
procedure step before taking action. This was not the intent of procedures. 
In addition to an organizational policy of strictly following procedures, 
there are additional influences on operators besides mandated ones. The 
nature of the procedure itself, the task and the experience level of the 
operator can often have a less than desirable effect on a given procedure. 

Risk Management: A Model for Procedure Use Analysis     213 
Quite often, team dynamics, external context and situational instances can 
have a dramatic effect on the use of procedural actions. 
The intent of this research is to challenge the assumptions of the industry, 
to see if they are still valid and hold true. To do this, we looked at why the 
assumption that “strict adherence to procedure increases safety” was initially 
developed. We reviewed what has changed within the industry, and verified 
that the industry does indeed have strict adherence to procedures and a 
culture of rigid compliance. This chapter offers an application regarding 
performing an experimental protocol and utilizing expert judgment to prove 
the demonstration of the claim. Taking into account nonlinear, dynamic and 
decompositional complexity – strict procedure adherence is not sufficient for 
overall system safety. 
10.2. Procedures in nuclear power 
For operators, procedures are a method of FA that enables task analysis. 
Much as software can automate the machine, procedures are a method to 
automate the human [BOY 13]. In the realm of nuclear power, procedures 
are developed for operator assistance, but it was quickly realized that not all 
situations could be managed and additional procedures were developed. 
Specific events also triggered procedure development. Eventually a strange 
behavior started to emerge, and the sheer volume of the procedures began to 
add a new element of complexity to the control room. 
Procedures in the nuclear field are well dictated. There are regulations 
[NRC 07], guidelines [EPR 06] and organizations (Procedure Professionals 
Assocation, [PPA 13]). Written procedures are considered static FAs, while 
electronic procedures are considered to be dynamic FAs. 
Procedures are developed in one of two different ways, goal-driven 
procedures and event-driven procedures. Goal-driven procedures are 
intended as documentation for operating and maintaining the plant. 
Emergency response guidelines are generally regarded as goal-driven 
procedures i.e. a forward-looking approach that considers possible future 
events. The nuclear industry maintained a very limited number of these 
during original construction and design of many plants in the 1970s. In 
contrast is event-based behavior, i.e. short-term prediction based upon events 
of the past [BOY 13] – the majority of procedures currently in use are based 

214     Risk Management in Life-Critical Systems 
upon these event-based behaviors developed in response to events that 
occurred. Needless to say that these procedure are incorrect, but they are 
incapable of managing complex variables that encompass context. 
Nuclear power plants (NPP), are operated by intelligent people, and 
overseen by highly adaptive organizations. Procedures are usually written 
for a specific plant, and often times these procedures are written prior to 
construction. These organizations are continuously trying to improve and 
learn lessons not only from their own operating experience, but also from the 
experience of the industry as a whole. Lessons learned continually trigger 
procedure updates throughout the operation. Details are added, clarified, and 
explained with the intent of removing confusion by the end user. Fast 
forward 30 years and it is easy to see how a great mass of procedures have 
developed. An operating plant has hundreds of procedures, with  
even more enclosures for context-management, e.g. a procedure for a 
component and enclosures for start-up, shutdown, normal operations, etc. 
Categories of procedures are plant specific, but are generally classified as 
normal operating procedures, surveillance instructions (driven by technical 
specifications), performance instructions, admin procedures, abnormal 
operating procedures and emergency operating procedures. All of these 
procedures are built in attempts to help clarify situation for the users, but 
such a large number of variable parameters can be dreamt of that we are in a 
position of information overload.  
The OECD Halden Research Reactor is also performing on procedure 
usage in the control room. This includes if an independent analyst 
(commonly called a shift technical authority STA in the US) adds value  
[EIT 13]. They are considering crew situational understanding, mission 
analysis, procedure choice, evaluation and resource utilization. Additionally, 
they have also put together in-depth literature reviews [GUS 11] and studies 
on team cognition in complex accident scenarios [BRA 10]. Similar models 
have been developed, such as the Guidance-Expertise Model (GEM), which 
was developed as a retrospective analysis of crew performances at the 
Halden Man–Machine Laboratory [MAS 11], but with different applications. 
The industry is also looking at procedure reform as a whole with the new 
advances in technology allowing for electronic procedures [LEB 12,  
EPR 12, OHA 00, EXP 08], though cultural and regulatory restrictions are 
causing a slow implementation. 

Risk Management: A Model for Procedure Use Analysis     215 
By understanding how and why operators interact with procedures, we 
can better understand how to develop recommendations for static or dynamic 
allocation in the future. For example, in situations where certain variables 
need to be reviewed, the computer system could review the variables and 
provide a context-based step to operators. It is expected that in the future, 
this research will support dynamic FA through the use of electronic-based 
procedures. Thus, a model was developed for procedure use analysis which 
is based on extensive literature review, expert testimony and dozens of full-
scale simulations at active NPPs. This model can be applied for 
understanding, and then improving, current or new designs.  
10.3. Description of the model 
10.3.1. Description 
While reviewing the simulation videos as utilized for the dissertation, I 
asked four questions in relation to functional allocation and context, i.e. 
procedures given the current situation:  
– Is the procedure available for the specific variables presented to the 
operator in the control room? 
– Did the operator maintain SA? 
– Is the procedure correct for the situation? 
– Was the action performed? 
The model consists of five levels, each associated with the varying 
questions that lead down the path to each individual case.  
10.3.1.1. Level 1: Availability 
Is the procedure available for the given context? During the last 30 years 
of procedure development, procedures have been written for many different 
situations (contexts). This question is intended to derive if a procedure  
exists for the context that the crew is facing at the current moment, i.e.  
does it account for the valves and pumps that are working or not? Does it 
account for the current state of the plant, in regards to start up or shut down? 
etc.  

216     Risk Management in Life-Critical Systems 
10.3.1.2. Level 2: Situational awareness 
Did the operator maintain SA? I define SA per the commonly accepted 
definition described earlier in this chapter. This question asks if the operator 
maintains perception, comprehension and projection of the current situation. 
This question is important, as the SA will directly affect the decisions of the 
operating crew.  
10.3.1.3. Level 3: Decision authority 
Is the decision authority (DA) correct for the given context? If a 
procedure exists, the procedure is the DA, i.e. the procedure determines what 
the following steps are – and the operational team is to interface with the 
procedure through supervision. However, if a procedure is not available, this 
responsibility falls to the operational team. This is an example of naturalistic 
dynamic FA.  
This question is important because I have witnessed instances in which 
the procedure or operator have maintained SA, but are still incorrect in the 
determination of the proceeding actions. When a procedure is available in 
the control room of an NPP, the procedure will drive the decision-making 
and troubleshooting as the authority. The operators may also perform this 
action on their own at the same time, but they will utilize the guidance of the 
procedure. In the event of a procedure not being available given the context, 
the operator becomes the driving force for troubleshooting and decision-
making. The operator utilizes their knowledge, training and understanding of 
the subsystem interactions in order to determine the proper course of action. 
10.3.1.4. Level 4: Performance 
Was the action performed? This is the decision point for the operator on 
the control panel. The operator's mental model has already determined how  
the situation is perceived, and what actions should be taken, with or without 
a procedure. This level is where operators will be tested in order to place the 
plant into a safe condition. The follow up questions are: Was the action 
performed in a timely manner, and is the action time critical? 
For purposes of this experiment and documentation, the subject of this 
question is the operator, as we are studying the balance of culture and 
knowledge in the human element. In future systems, this could be modified 
to apply to any agent, human or machine, but this would significantly alter 

Risk Management: A Model for Procedure Use Analysis     217 
the model, as automation obviously plays a stronger role in machine 
systems.  
10.3.1.5. Level 5: Outcome 
The following are descriptions of each possible outcome and its 
implications, see Figure 10.1 for a visual rendition of the model.  
Case 1: Ideal 
The procedure is available and correct given the context; the operator has 
maintained SA, and performs the actions. We plan for this ideal case, and it 
is the most common.  
Case 2: Lack of procedural adherence 
The procedure is available, and correct given the context; the operator has 
maintained SA and the actions were not performed. This is simply a case of 
not following the procedure. The step was correct, and should have been 
performed, and thus, this is not a desirable case. If the operator has 
maintained SA, it implies that a conscious decision to not-perform the step 
was chosen, and the operator was incorrect.  
Case 3: Knowing blind compliance 
The procedure is available but is not correct given the context; the 
operator maintains SA and performs the actions. The operator makes a 
conscious decision to follow the step, meaning he is doing so only because 
he has been trained to and the repercussions of the step are not severe. This 
is a case where operators are merely following orders, as opposed to 
understanding their choices and making the right decision. This is not a 
desirable case.  
Case 4: Engaged and thinking non-performance 
The procedure is available but is not correct given the context; the 
operator maintains SA and the actions were not performed. In this situation, 
the operator has considered the procedure in relation to the situation and 
determined it is not applicable, and thus not performed the steps to keep the 
plant in a safe condition. In the US nuclear industry, this could be a 10 CFR 

218     Risk Management in Life-Critical Systems 
50.54(x)1 action or some other action that is either not covered by procedure 
or where performing the procedure as written would not be prudent for the 
given plant conditions. This is a desired case. 
Case 5: Opportunistic control 
The procedure is available, and correct given the context; the operator has 
not maintained SA and the actions were performed. This would involve an 
operator Loss of perception, comprehension, or projection. This could be a 
result of the operator simply not paying attention. This could result in an 
undesirable case. 
Case 6: Error of omission 
The procedure is available and correct, given the context; the operator has 
not maintained SA and the actions were not performed. One example of this 
case would be accidently skipping steps due to a distraction, or lack of focus. 
This is an undesirable case.  
Case 7: Incorrect procedure 
The procedure is available, but is not correct given the context;  
the operator has not maintained SA and the actions were performed.  
This case is particularly dangerous, and timely as operators may blindly 
follow procedures and do damage to plant components or public safety.  
This would involve an operator loss of perception, comprehension, or 
projection, and is the most dangerous case of cook-booking. This is an 
undesirable case. 
Case 8: Inapplicable procedures 
The procedure is available, but is not correct given the context; the 
operator has not maintained SA and the actions were not performed. This is 
a case where the procedure is not applicable, or by coincidence, the operator 
does not notice the incorrect step and does not perform it.  
                         
1 10 CFR 50.54(x) A licensee may take reasonable action that departs from a license 
condition or a technical specification (contained in a license issued under this part) in an 
emergency when this action is immediately needed to protect the public health and safety and 
no action consistent with license conditions and technical specifications that can provide 
adequate or equivalent protection is immediately apparent. 

Risk Management: A Model for Procedure Use Analysis     219 
Case 9: Engaged and thinking performance 
The procedure is not available; the operator has maintained SA, is correct 
in his decisions and the actions were performed. Though designation of this 
case is only available via hindsight, operators are trained for this instance.  
 
Figure 10.1. A model for procedure analysis 

220     Risk Management in Life-Critical Systems 
The application of knowledge to the interactions of the subsystems is the 
only reason the job of an operator cannot be fully automated in safety-critical 
systems. This would be a case of 10 CF 50.54(x). This is a desirable case, 
though operators are being slowly trained out of performing these actions. 
Case 10: Blind non-compliance 
The procedure is not available, the operator has maintained SA, and is 
correct in his decisions though the actions were not performed. This is a 
worst-case scenario as it results in an indecisive loss of time. A procedure 
did not exist, so nothing was done. Rarely is this case seen, more often 
operators lose valuable time by hunting through inapplicable procedures to 
find a step that might cover the action they want to take.  
Case 11: Faulty training 
The procedure is not available; the operator has maintained SA, was not 
correct when given the context; and the actions were performed. This is a 
case of faulty training, when the operator did not properly respond to the 
situation, even if he did understand it. This case is rare, as SA requires 
perception, comprehension and projection. An example would be if an 
operator knows which pump to turn off, but does not know where the switch 
to turn it off is. 
Case 12: Conservative 
The procedure is not available; the operator has maintained SA was not 
correct given the context and the actions were not performed. An example of 
this case would be if an operator is uncertain of the outcome of his decision, 
and he chose to behave conservatively. This is a desirable case.  
Case 13: Unknowing blind compliance 
The procedure is not available; the operator has not maintained SA, 
through the correct action is performed. This would be a case of accidentally 
doing the right thing, without knowing it. An example of this would be an 
operator being told what to do by another employee or management, and the 
operator performing the action without question. 
Case 14: Conjecture 
The procedure is not available, the operator has not maintained SA, 
though he/she is correct in his/her actions, but the actions are not performed. 

Risk Management: A Model for Procedure Use Analysis     221 
This case is rare; but could apply to an operator thinking through possible 
situations and outcomes.  
Case 15: Error of commission 
The procedure is not available, the operator has not maintained SA, the 
action is not correct, but it is performed. This case is uncommon, but has 
been known to occur. An example of this would be an accidental push of a 
button, or flip of a switch that causes an unintended outcome. Another 
example is when an operator is told what to do, and performs the action 
without understanding the implications on the system. This is not a desirable 
case.  
Case 16: Natural evolution 
The procedure is not available, the operator has not maintained SA, the 
action is incorrect, and not performed. This case would be the natural 
evolution of a problem, leaving all results to the technical and software 
automation, this situation would be an illustration of what the plants would 
look like without humans. This case is not desirable as it illustrates a 
complete lack of control.  
10.3.2. Assumptions 
The following assumptions have been applied to the model:  
– A paper procedure cannot address variables for which it was not written 
for. 
– Operators maintain intentions to put the plant in a safe condition. 
–  “Operators” are studied in a team, and SA is the distributed cognition 
among the team.  
– The operator has the physical resources needed to maintain plant safety 
(power, water). This assumption is further addressed by the US industry 
FLEX mitigation procedures [MIL 13]. 
– In the event that a procedure is updated or edited, it is considered a new 
“triggering action”. 

222     Risk Management in Life-Critical Systems 
10.3.3. Peer review of the model 
The model was developed utilizing the assistance of operators, training 
personnel and human factors specialists at the volunteer plant involved in the 
study. This model has gone through a verification and validation process 
defined by the Department of Defense Modeling and Simulation Office 
[DDS 13]. 
The model has been verified and validated via expert questionnaires. 
Experts range from industry experts including regulators, control room 
operators and training personnel. The model has also been evaluated by 
industry and academic researchers from national laboratories and 
international establishments, as well as academic researchers from the 
nuclear domain, aviation domain, and organizational research experts. The 
validation was performed by 30 experts (as defined above), and 87.5% of 
experts agree that the model is a valid representation of the real-world. 
Seventy-six percent of respondents are from the nuclear field, and 90% of 
experts work with procedures on a regular basis. Those who disagreed with 
the model focused primarily on the link between SA and decision-making. 
It has been determined that this model compares to similar types, this 
includes models for Human Reliability Analysis, FA, cognitive functions in 
the Control Room, procedure following, decision making, and “more macro-
cognitive functions such as ‘monitoring/detection’ ‘sense making’ and 
planning”. One overarching comment focused on the inclusion of measuring 
situation awareness. Most experts agree that SA cannot be accurately 
measured, nor is it a digital “on/off” style metric that can be measured. Some 
comments have been addressed by developing and stating assumptions for 
the model’s use. 
Table 10.1 shows the percentage of respondents that have seen the 
situations described in the model. 
The procedure was not available 
58% 
The operator did not maintain SA 
63% 
The procedure or operator was incorrect 
84% 
The operator performed a step that was not directed by a procedure 
74% 
Table 10.1. Have you ever witnessed a scenario where? 

Risk Management: A Model for Procedure Use Analysis     223 
Experts believe that this model could be used for developing “Enhanced 
decision trees/training for situations not covered by procedures”, “a useful 
tool to do a final check on human factors decision such as outcomes from 
analysis such as Work Domain Analysis,” and “pointing out all possible 
gaps in the procedural system – once the gaps are identified they can be 
eliminated.” 
10.4. Application of the model 
This model is to be applied to a system, situation or context as a decision 
support tool for design or update of hardware. It is intended to give the 
designers of the system guidance to review perspectives that would not have 
been considered without application of the model.  
10.4.1. Generic applications 
In designs still under development, such as new plants, it can be used to 
design generic solutions. For example, a case number can be addressed, and 
then issues identified, and design solutions to those issues can be developed. 
One way to identify solutions is to apply the Technology, Organizations, 
People (TOP) model [BOY 98, BOY 13] this allows for a variety of 
perspectives to be considered. An example of this process is in Table 10.2. 
This model can also be applied, in the same manner, to training 
simulations and scenarios to assist in developing additional decision points 
to train operators. For example, if a trainer wants to try and develop 
scenarios that will result in case X (1–16), he can specifically add variables 
that could distract, or make sure to force operators into places where there is 
no procedure for a desired action. This kind of multiple-fault training 
develops the complex knowledge and interactions that are invaluable. 
10.4.2. Specific applications 
Once a scenario has been run, instructors can evaluate teams, scenarios 
and procedures based upon the outcome of the model. This can be done by 
applying the metrics developed in Table 10.3 to teams as the scenario is 
being run. This table describes the desired outcome, and during the scenario 

224     Risk Management in Life-Critical Systems 
the metrics would be collected for the operating team. Then, anomalies can 
be analyzed and addressed.  
Case number 
15 
Level 
1 
Available 
N 
2 
SA 
N 
3 
Correct 
N 
4 
Action 
Y 
Desirable case? 
N 
Title 
Error of commission 
Description 
The procedure is not available, the operator has not maintained 
SA, the action is not correct, but is performed. 
Control mode 
Problem solving 
Examples 
Generic 
This case is uncommon, but has been known to occur. An 
example of this would be an accidental push of a button, or flip 
of a switch that causes an unintended outcome. 
Specific (witnessed 
in simulation) 
Operator is told what to do, and performs the action without 
understanding the implication. 
Solutions 
Technology 
Covers for high risk switches, or controlled areas in front of 
panels. 
Organization 
In-depth training and a safety culture that fosters asking 
questions and collaboration. 
People 
Hire personality types that would not perform actions without 
confidence of knowing they're doing the right thing. 
Table 10.2. Solutions table for Case 15 
10.4.3. Real-world application of the model 
Scenarios are being developed at our INPO 1 plant in order to validate the 
model that was developed based upon the first set of scenarios. We will look 
at where operator behavior fits into each subsection and case of the model. 
The scenarios developed will be based upon the first three levels, i.e. is a 
procedure available? Is the operator situationally aware? Is the decision 
maker correct for the context? Then, we will measure the responses to the 
fourth question: does the operator perform the action? Within each scenario, 
there will be multiple points of interest, as well as decision points where  
 
 

Risk Management: A Model for Procedure Use Analysis     225 
operators must actively decide if they should step out of the procedure or 
how to proceed without one. Organizationally, they are never to proceed 
without a procedure.  
Metric 
Desired answer 
Level 1: Availability 
   Is the procedure available given the context? 
Yes/No 
Level 2: SA 
Is the operating team situationally aware? 
Yes 
   Perception 
Yes 
   Comprehension 
Yes 
   Projection 
Yes 
Is the operator’s mental model correct? 
Yes 
Level 3: DA 
   Is the decision authority (procedure/operator) correct? 
Yes 
   Does the SCO utilize the knowledge base of the crew to make a 
decision? 
Yes 
   Do the operators understand the interactions of the system? 
Yes 
   Did every crewmember independently determine the solution? 
Yes 
   Level of impact for not correctly performing action. 
1 to 5 
 Level 4: Performance 
   Was the action performed? 
Yes/No 
   Was the action supposed to be performed? 
Yes/No 
   Does the crew perform the correct action given the decision point? 
Yes 
   How long does it take the crew to make a decision? 
>1 min 
   Are the operators comfortable with their decisions? 
Yes 
Level 5: Outcome 
   What model case does this apply to? 
(1–16) 
   Is this case a desirable case?  
Yes 
   Does the crew safely secure both units at the end of the decision? 
Yes 
   Was unnecessary damage done to the plant? 
No 
Table 10.3. Decision point metrics 

226     Risk Management in Life-Critical Systems 
Requests for additional plants to participate have been put out. Each has 
been asked to develop simulations based upon their own plant hardware and 
procedure basis. Though standardized scenarios are prevalent in research, 
including ISLOCA [ROT 04], LOFW and SGTR [BRO 11], or a multiple 
SGTR scenario [MAS 11], we asked plants to develop scenarios that would 
specifically contribute to their training regiments.  
In order to program our simulations, we will utilize currently existing 
hardware in operating power plants. In the United States, post Three Mile 
Island, every running unit is required to have an identical, maintained, and 
up to date simulator on site per 10CFR, 55.46 [NRC 11] for training 
purposes. Training simulators are not only used for the purpose of training 
operators, they also validate new HMI systems, perform user testing, and 
familiarize operators with the new set-ups. Maintaining simulators and 
verifying that they are always up to date and closely correlated with the 
plants can be a difficult task. Every time a plant updates hardware, they must 
update the simulator as well. The International Atomic Energy Association 
establishes guidelines on safe and efficient simulator upgrades [IAE 06]. 
Working with an INPO 12 US NPP, twelve three-hour videos of scenarios 
have been completed, which are currently under data analysis. 36 h of video 
have been reviewed, looking for numerous data points and elements. The 
first step toward obtaining this research was to develop a scenario in which 
operators are put into a position where procedure adherence may not provide 
the proper outcome. With assistance from industry experts, the following is a 
high-level scenario developed to meet those requirements: 
“At a two-unit plant, unit 1 is in mode 6, and the reactor vessel head must 
be removed and the refueling cavity filled. A fire breaks out in an onsite 
warehouse and the fire brigade needs to be called. Another event triggers 
loss of control for the residual heat removal heat exchanger bypass flow-
control-valve on the running train. The other train is being used to fill the 
refueling cavity. Following the residual heat removal pumps being aligned to 
fill the refueling cavity, an electrical short occurs in the control switch 
resulting in isolating cooling flow to the in service train. The crew will have 
to make a decision as to how they will provide core cooling. Then, unit 2 
                         
2 INPO conducts safety evaluations of all nuclear power plants every 18–24 months, and each 
evaluation generates a rating of 1–5 where 1 is a plant with ideal safety records and 
performance, and 5 requires immediate improvements. 

Risk Management: A Model for Procedure Use Analysis     227 
will face a loss of power and thus the loss of reactor coolant pumps resulting 
in a reactor trip.” 
Process diagrams of this scenario and decision points were developed in 
order to understand the decision points that could occur, and possible 
outcomes. From this data, we now develop metrics in order to evaluate the 
strict adherence to procedures over the utilization of training and 
understanding of the plant context based on indications in the control room. 
The metrics will be used to compare various crews and plant performance to 
see if the operating crews are indeed following the procedures, or working 
out of context. 
We found that at times operators were not as well equipped to handle 
situations that did not fit neatly into a procedure. We also witnessed 
operators that clearly knew the proper action to take and yet were hesitant to 
take the action or simply did not take the action because they did not have 
written procedural guidance at the time. Hundreds of data points were 
obtained based on the metrics in Table 10.3, and to date, and as of 
publishing, analysis is still in progress. 
10.5. Significance 
This model is intended to assist in development of design solutions, and 
create ideas and force perspectives that were not previously considered. It is 
not intended to provide probabilistic results for incident analysis, but rather, 
intended to be used before an accident to assist in prevention. The product of 
the active research analysis will be additional design solutions based upon 
real world situations and metrics. Through application of this model, we can 
achieve our goal of characterizing a safe, efficient and reliable plant by 
making recommendations on functional allocation and thus automation 
within this complex system. This may lead to a paradigm shift regarding 
procedure based or training-based organizations. Other outcomes will 
include recommendations for static or dynamic functional allocation, 
context-aware decision support systems, and human–machine cooperative 
automation systems. This research will serve as the first step to developing a 
process for the proper balance of automation and the development of 
efficient dynamic allocation of resources, and information dissemination in 
future generations of power plants. The problem of distributed coordination 
of resource allocation for data distribution and processing in dynamic 

228     Risk Management in Life-Critical Systems 
networks is also faced in other domains such as the development of tactical 
networks [CAR 08]. 
This research may answer industry questions such as: do people put too 
much faith in automation? Do they spend more time focused on the 
procedures than on the job at hand? If we could answer these questions we 
focus on better training instead of more and more regulations and procedures 
than the situation at hand. Through better training and better understanding 
on an organizational level, we can improve safety by verifying that operators 
are focused on the correct task with a deep understanding of the low-level 
subsystem complex interactions.  
Based on the metrics we collect with the application of the model and the 
responses of operators to situations and scenarios, we will derive design 
recommendations on improvements in Human-Centered Design. These 
recommendations can be utilized in new design or in improvements of old 
design and will consist of hardware recommendations, improvements in 
process, organization and user interactions. 
The results of this study may lead to changes directly in the control room, 
or in the automation of operating plants as well. Crucial to everyday 
operations, the understanding of the automation directly leads to the SA 
within the plant. This may also lead to an upgrade in control room tools that 
are utilized to provide operators with information in everyday situations, and 
greater efficiencies which would equate to cost savings in operations and 
maintenance. This research may also lead to operational experience 
databases to be provided in a more efficient manner to operators. By 
utilizing explanation modules, operators could better understand the intent of 
procedures without having to call in the engineers in order to question them. 
This can not only save time, but also help to build accurate mental models 
over the lifetime of the plant.  
In addition, future research could incorporate this data to create a 
simulator, which can draw directly from the real-time parameters of the plant 
as well as integrating operations feedback and design intent and physics in 
order to predict short-term outcomes of response actions in an emergency 
scenario. A tool such as this could be available to operations when required, 
and would act as an engineering simulator with a fast forward option such 
that it could receive signals from the plants and operators could “test-drive” 
scenarios if they were not entirely certain that this was the correct action. 

Risk Management: A Model for Procedure Use Analysis     229 
Having the ability to check to see how the plant reacts could  
assist in increasing their SA and experience feedback in not only the current 
context, but also in future instances.  
The data collected and the recommendations found create multiple new 
paths for research and greater improvement on many topics. Additionally, 
we foresee this research supporting the industry movement to integrate 
electronic procedures [EPR 12]. 
10.6. Conclusions 
Procedures act as the automation of people, and the realm of automation 
within nuclear power can be a challenge. It can be difficult providing the 
operators relief from menial tasks, while still maintaining SA of the plant. 
The first step in developing a process of determining the proper balance of 
automation will be to verify if the current methods in place, work.  
The claim of this research is that the nuclear industry’s assumption that 
strict adherence to procedures and rigid compliance will prevent events and 
increase safety, is incorrect. The overall goal of this research is to assist in 
characterizing 
a 
safe, 
efficient 
and 
reliable 
plant 
by 
making 
recommendations on functional allocation and thus automation within this 
complex system. Accidents in complex systems tend to happen due to a 
misunderstanding or misallocation of interactions of systems through the use 
of automation.  
I have presented a model of procedure usage that can assist in deriving 
design recommendations, and provide a framework to identify new emergent 
behaviors. Scenarios where operators must step out of procedures will be 
developed and tested in actual plant simulators to measure results. I will 
analyze these results and make recommendations on which functions to 
automate.  
Based on the metrics we collect with the application of the model and the 
responses of operators to situations and scenarios, we will derive design 
recommendations on improvements in Human Centered Design. These 
recommendations can be utilized in new design or in improvements of old 
design and will consist of hardware recommendations, improvements in 
process, organization and user interactions. 

230     Risk Management in Life-Critical Systems 
This research will derive design recommendations for new plants, 
improve training in existing plants, and identify technology, organizational 
and people solutions for a safe, efficient and reliable plant. It will improve 
SA and decision-making capabilities by directing operating teams into 
multiple fault failures that utilize telemetry from operational power plants. 
Human Centered Design techniques have been utilized to create the model 
and now improvements can be built improve future design, or to assist in the 
complex training of operators.  
10.7. Acknowledgements 
I would like to thank Dr. Guy Boy of the Human-Centered Design 
Institute of Florida Institute of Technology for his advice and continued 
support. I would also like to thank Thomas Waicosky, Jonathan Nowlin and 
Cynthia Schmitt for their expert reviews of this publication. 
10.8. Bibliography 
[BOY 98] BOY G., Cognitive Function Analysis, Westport, CT: Greenwood 
Publishing Group, 1998. 
[BOY 13] BOY G., Orchestrating Human-Centered Design, United Kingdom: 
Springer, 2013. 
[BRA 10] BRAARUD P.Ø., JOHANSSON B., Team Cognition in a Complex Accident 
Scenario (HWR-955), Halden, Norway: OECD Halden Reactor Project, 2010. 
[BRO 11] BROBERG H., HILDERBRANDT M., NOWELL R., Results from teh 2010 
HRA Data Collection at a US PWR Training Simulator (HWR-981), Halden, 
Norway: OECD Halden Reactor Project, 2011. 
[CAR 08] CARVALHO M., “In-stream data processing for tactical environments”, 
International Journal of Electronic Government Research, vol. 4, no. 1, pp. 4–7, 
2008. 
[DDS 13] Department of Defense Modeling and Simulation Office, What is 
Simulation Validation and Why is it Important? Retrieved from VV&A 
Recommended 
Practices 
Guide: 
vva.msco.mil/Ref_Docs/Val_Lawref/Val-
LawRef.htm, 2013, March 13 
[DOU 03] DOUGHERTY E., “Conxtext and human reliability analysis”, Reliability 
Engineering and System Safety, vol. 41, pp. 25–47, 2003. 
 

Risk Management: A Model for Procedure Use Analysis     231 
[EIT 13] EITRHEIM M.H., HILDEBRANDT M., MASSAIU S., et al., Resilient 
Procedure Use – Effects of an Independent Analyst on Resilient Emergency 
Operation (HWhP-045, Issue 1, 2013), Halden, Norway: OECD Halden Reactor 
Project, 2013. 
[EPR 06] EPRI, Maintenance Work Package Planning Guidance, Palo Alto, CA: 
EPRI, 2006. 
[EPR 12] EPRI, Computerized Procedure Systems, Palo Alto, CA: Electric Power 
Research Institute, 2012. 
[EXP 08] EXPOSITO A., QUERAL C., HORTAL J., et al., “Devlopment of a software 
tool for the analysis and verification of emergency operating procedures through 
the integrated simulation of plant and operators actions”, Annals of nuclear 
energy, vol. 35, pp. 1340–1359, 2008. 
[GUS 11] GUSTAVSSON P., JOHANSSON B., HILDEBRANDT M., Resilience and 
Procedure Use in the Training of Nuclear Power Plant Operating Crews – An 
Interview Study and Literature Review (HWR-1026). Halden, Norway: OECD 
Halden Reactor Project, 2011. 
[IAE 06] IAEA, Guidelines for upgrade and modernization of nuclear power plant 
training simulators: IAEA Report Number TECDOC-1500, Vienna, Austria: 
International Atomic Energy Association, 2006. 
[INP 10] INPO, SOER 10-2, Engaged, Thinking Organizations. Atlanta, GA: INPO. 
2010. 
[LEB 12] LE BLANC K., OXTRAND J., WAICOSKY T., Model of Procedure Use – 
Results from a Qualatiative Study to Inform Design of Computer Based 
Procedures, San Diego, CA: NPIC & HMIT 2012, American Nuclear Society, 
2012. 
[MAS 11] MASSAIU S., Developing and Testing the Guidance-Expertise Model of 
Crew Cognitive Control: Study Plan for the Teamwork, Procedures and 
Expertise Experiment (HWR-980), Halden, Norway: OECD Halden Reactor 
Project, 2011. 
[MIL 13] MILLER B., “United States Nuclear Regulatory Commission Actions 
Following the Fukushima Dai-Ichi Accident”, 8th Nuclear Plants Current Issues 
Symposium: Challanges and Oppertunities, Orlando, Florida, 2013. 
[NRC 07] NRC, Part 2 – Agency Rules of Practice and Procedure, Washington, 
D.C., Nuclear Regulatory Commission, 2007. 
 
 

232     Risk Management in Life-Critical Systems 
[NRC 11] NRC, Simulation facilities, Title 10, Code of Federal Regulations, 
Washington, DC, US, Nuclear Regulatory Commision, October 17, 2011. 
[OHA 00] O’HARA J., HIGGINS J., STUBLER W., Computer Based Procedure 
Systems: Technical Basis and Human Factors Review Guidance (NUREG/ 
CR-6634), Washington, DC, U.S. Nuclear Regulatory Commission, 2000. 
[PPA 13] Procedure Professionals Assocation, 12 June 2013. available at: 
http://www.ppaweb.org/. 
[ROT 04] ROTH E., MUMAW R., LEWIS P., An emperical investigation of operator 
performance in cognitively demanding simulated emergencies (NUREG/ 
CR-6208) Washington, D.C., U.S. Nuclear Regulatory Commission, 2004. 

11 
Driver-assistance Systems for  
Road Safety Improvement 
11.1. Introduction  
For a couple of years, road safety has been one of the most important 
issues for public authorities as well as for car manufacturers and suppliers 
[EUR 05]. Beyond the road user information and the driver training, much 
technological progress has been performed that is drastically improving road 
safety and thus reducing the seriousness of accidents. 
This was first the case with the generalization of safety belts then with 
airbags and more recently with the development of Advanced Driver 
Assistance Systems (ADAS).  
Nevertheless, irrespective of the importance of these technological 
progresses, the driver is still the core issue. 
Indeed, literature shows that in Organisation for Economic Cooperation 
and Development (OECD) countries, about 90% of accidents are due to an 
intentional or non-intentional driver behavior [TRE 77, SAB 75]. They can 
be related to a bad perception or bad knowledge of the driving environment 
(obstacle, etc.); but also to reduced physiological (drowsiness, sleepiness, 
etc.) or physical (old people and elderly drivers) conditions, etc.  
 
                         
Chapter written by Serge BOVERIE. 

234     Risk Management in Life-Critical Systems 
The development of increasingly intelligent driver-assistance functions 
should partly solve these problems. For more than 10 years, ADASs have 
been the focus of many developments both in public research institutes and 
in the automotive industry. 
The first generation ADAS was commercialized a couple of years ago 
mainly on elite vehicles. Their deployment on middle and low-class vehicles 
is just beginning now. 
These new functions improve the environmental perception of the driver 
(night vision, blind spot detection and obstacle detection). In critical 
situations, they can substitute for the driver (e.g. autonomous emergency 
braking, etc.). 
Such developments have been supported by new smart sensor generations 
able to fulfill automotive constraints such as Radar, Cameras, Lidar, GPS, 
ultrasonic sensors, etc. These sensors are including increasingly powerful 
computing power. They are able to provide synthetic information to control 
and decision-making/diagnostic units or directly to the driver through 
appropriate Human–Machine Interfaces (HMI). 
Over the longer term, a progressive automation of the vehicles is foreseen 
to reach full automation after 2025. Some experimentation in real conditions 
has been performed in US for a couple of years, and many public or private 
research laboratories have even designed fully or partially automated 
vehicles. Nevertheless, it is still not conceivable to fully exclude the driver 
from the driving loop.  
 
Figure 11.1. Examples of driver-assistance systems 

Driver-assistance Systems for Road Safety Improvement     235 
The deployment of increasingly sophisticated ADAS would need to 
design more and more intuitive “transparent” human–machine interaction 
concepts. The information flow provided by the vehicle to the driver should 
also be optimized. ADAS should assist the driver in his driving task 
providing information or help adapted to his profile, to his state but also to 
the environmental conditions and the context. It should also include some 
personalization capabilities providing an interactive experience and an 
emphatic relation between the system and driver in order to encourage its 
usage in a permanent way to improve his safety and comfort. Last but not 
least, new ADAS generation would be able to provide the driver with the 
possibility to adapt the level of assistance in relation to his comprehension, 
needs, aptitudes, capacities and availabilities. 
To be able to develop such a new approach, a good online knowledge 
about the driver, his behavior, his state and his attitude, etc. in all 
circumstances is necessary. This knowledge can be achieved through the 
observation of data available in the vehicle, or from the direct observation of 
the driver.  
A driver’s limitations are very often related to his physiological and 
psychological states: 
– the aptitude of the driver can be reduced due to a lower vigilance, 
fatigue or sleepiness. 
– the driver’s availability can be reduced due to distraction or 
inattentiveness produced by internal or external attractors or executing 
additional non-driving tasks like chatting on the phone. 
It is in this context that in recent years we have been interested in 
studying and developing new functions providing, in real-time, information 
about the situation inside the cockpit and more specifically in providing a 
diagnostic about the driver’s state: 
– sleepiness and drowsiness diagnostic (see section 11.2); 
– visual, due to the observation of the head orientation or of extra driving 
activity (see section 11.3). 
Then, from these studies we were able to design a new human–machine 
interaction concept (section 11.4). 

236     Risk Management in Life-Critical Systems 
 
Figure 11.2. Vehicle/driver/environment system 
11.2. Driver’s vigilance diagnostic 
Among the problems that impact on traffic safety, the driver’s physical 
and psychological states play an important role. The assessment of the 
driver’s vigilance1 state has attracted wide interest both in basic research and 
in solutions to the development of driver monitoring systems. This interest 
has been reinforced in recent years due to technologies becoming 
increasingly mature [BOV 02a–02b]. 
According to Alain Muzet, several parameters can be observed to 
diagnose sleepiness and drowsiness situations. Figure 11.3 presents an 
overview of the needed physiological functions for the monitoring of the 
wakefulness and the transition to sleep [SEN 04–08]. It summarizes the 
evolution of various observations that characterize the involuntary transition 
                         
1 The term “driver vigilance” encompasses all the situations in which the driver’s alertness is 
diminished, and therefore when the driving task cannot be maintained at an adequate level of 
performance. It is a consequence of stress, fatigue, alcohol abuse, medication, inattention and 
the effects of various diseases. 

Driver-assistance Systems for Road Safety Improvement     237 
from waking to sleeping states (extracted from Dr. Alain Muzet’s 
presentation during the SENSATION Plenary meeting in Lisbon (2006)). Of 
course, still according to Dr Alain Muzet, there is not a direct time 
correlation between the various observations, but the behaviors are respected 
for most of the subjects. 
Indeed, while taking care of the current state of the technology most of 
these parameters are not directly measurable in an automotive and daily life 
context. The focus of these studies was on Electro Oculography 
(EOG)/blinks and motor activity through the introduction of vision-based 
and data fusion approaches. 
In the 1970s and 1980s, extensive research was carried out to identify and 
describe driver impairment issues as well as countermeasures that were 
planned to tackle the problem. In addition laws were enacted in the whole 
EU area for controlling professional drivers’ driving times.  
In the 1990s, new approaches for solving driver impairment problems 
emerged. When sensor technology made improvements and the processing 
capacity of computers was rapidly increasing, R&D in vehicle telematics 
opened up new frontiers, among them monitoring the driving environment, 
vehicle movements, driver status and behavior. In Europe, basic work for 
driver monitoring was carried out in the EU-funded project DETER-EU 
[BRO 95], and then in PROCHIP/PROMETHEUS [EST 95]. 
 
Figure 11.3. T involuntary transition from waking to sleeping (from Alain Muzet) 

238     Risk Management in Life-Critical Systems 
In the SAVE project [SAV 98], a first prototype using vision technology 
for the observation of eyelid patterns aiming to provide a diagnostic about 
driver drowsiness was achieved by Siemens. These developments continued 
within AWAKE [AWA 04]. 
More recently, within the SENSATION project (2004–2008), in parallel 
with Siemens internal activities, more global approaches were developed.  
It fuses the drowsiness diagnostic issued from eyelid pattern observation 
(direct diagnostic) and the impairment diagnostic issued from the analysis of 
lateral position of the vehicle and the steering wheel movements (indirect 
diagnostic). 
In addition, some original solutions were also explored like the analysis 
of the driver postures with seat foil sensors and the autocentered movements 
with cameras. More recently, parts of these developments were continued in 
the HAVE-IT project [HAV 07]. The following sections present part of these 
works: 
– section 11.2.1 presents the developments on direct diagnostic of the 
vigilance based on the analysis of the eyelid patterns due to a camera 
observing the driver’s face; 
– section 11.2.2 presents an approach fusing direct diagnostic and the 
evaluation of the driving performance (indirect diagnostic). 
11.2.1. Diagnostic of driver hypovigilance 
The diagnostic of the driver hypovigilance is based on the real-time 
analysis of the driver’s eyelid motion patterns [WIE 99, HAR 00, JOH 05, 
SCH 08]. 
This is a complex process that leads to solving many issues. First of all, 
such a diagnostic should be able to work in any condition by day and night, 
autonomously. It must also be non-intrusive and guarantee a good robustness 
toward morphologic diversity of the drivers. Furthermore, it must consider 
the incertitude and variability of the drivers’ behaviors. The extraction and 
real time tracking of pertinent parameters and variables is quite critical and 
needs robust and efficient approaches adapted to the problem. Lastly, one of  
the more important issues is that of the evaluation of the diagnostic 
performances that requires a reference expertise. 

Driver-assistance Systems for Road Safety Improvement     239 
The very first concept for the diagnostic of the driver hypovigilance was 
proposed in the SAVE-project (SAVE 1995–1998). It included one CCD 
camera, a set of near infrared lights and a computer unit including three 
DSPs that were implemented in the image processing algorithms for the 
eyelid movement extraction. 
This prototype was then improved up to the latest version using images 
provided by a CMOS camera sensor observing the driver’s face at 50 Hz 
sampling frequency associated with a set of pulsed Near Infrared (NIR) light 
synchronized with the shutter of the camera. That configuration allows taking 
over disturbances caused by the sun’s environmental lighting. The real-time 
analysis of the images provided by the camera allows us to reconstruct the 
eyelid patterns, detect eye blinks and measure their duration. 
This system works in a fully automatic configuration after an autonomous 
initialization phase, in day and night conditions. 
The eyelid detection algorithms detect feature-based characteristics such 
as the corner of the mouth, the corner of the eyes, nostrils, etc. This approach 
allows a robust and accurate detection of the eyes in the image, irrespective 
of the environmental and morphological conditions. These characteristics are 
then tracked due to Kalman filter under geometrical constraints. 
Knowing the eye position, the eyelids are modeled by second-order 
curves and their movement is rebuilt. 
The measurement of the blink duration is extracted from the analysis of 
these movements applying some specific constraints discarding artifacts like: 
uncoherent durations, amplitude and shape of the movement for each eyelid, 
etc. 
Then, the blinks are classified depending on their duration in four fuzzy 
subsets (short, medium, large and very large). The use of fuzzy subsets 
allows us to consider inter and intra driver variability as well as incertitude 
on the measurements2. 
                         
2 Considering the fuzzy distribution of the blinks; we can blink, for example, belonging 
simultaneously to the short and medium classes knowing that the sum of the partnership 
degrees to each of the classes is always equal to 1. 

240     Risk Management in Life-Critical Systems 
The driver’s state diagnostic was set according to the expertise provided 
by physicians in four fuzzy classes (awake, slightly drowsy, drowsy and 
sleepy). The classification depends on the number and type of blinking from 
each class on a given time window. This original approach allows a better 
representation of the transition from one class to another [BOV 05,  
BOV 08]. 
From this diagnostic, we have designed a progressive and multimodal 
human–machine interface. This interface combines visual, audio and haptic 
modalities that are activated depending on the criticality of the issues. 
 
 
Figure 11.4. Algorithmic principle for the hypovigilance diagnostic of the  
driver and results of this analysis on a subject in real driving conditions 
These algorithms were integrated in onboard computers on various 
experimental vehicles. They were then tested in real driving conditions,  
on a motorway with more than 20 drivers. These experiments were 
performed in cooperation with Toulouse Hospital in France. All the drivers 
were 
equipped 
with 
electrophysiological 
measurement 
systems 
(electroencephalography (EEG) and electrooculography (EOG)) to produce a 
reference expertise. The comparative results between the diagnostic provided 
by our system and the expertise demonstrated the good efficiency of the 
system: 
average 
sensitivity: 
93%; 
σ = 6.6; average 
specificity:  
90%; 
17,0.
σ =
 

Driver-assistance Systems for Road Safety Improvement     241 
11.2.2. Diagnostic of driver impairment 
The aim of this study was to set a robust diagnostic of the driver state 
aggregating the information provided by the hypovigilance diagnostic (see 
section 11.2.1 [BOV 02, BOV 05]), an estimation of the driver’s driving 
performance degradation and contextual information [TAT 05, BOV 08]. 
Many studies are establishing that a degradation of the driver’s state of 
vigilance (sleepiness, fatigue, etc.) is very often correlated with a reduction 
of the vehicle lateral control performance [SAN 03]. In this study, we 
evaluate the driver’s ability to perform a lateral control task combining  
the information that represents the driver’s wishes, the steering wheel 
movement and information that represents the vehicle’s behavior, its lateral 
position3. 
However, the design of this approach is quite complicated. Indeed, one of 
the main issues is the driving variability, the different behaviors of which 
must be considered when setting a diagnostic. In addition, the identification 
of pertinent and discriminated parameters from the in-vehicle measurements 
is quite critical. 
The corresponding algorithms include a learning phase of a couple of 
minutes that aims to establish a reference model of the driver’s behavior 
[ROG 02]. Then, in normal operating conditions, abnormal events are 
detected when observing deviations from this reference model. This analysis 
is performed due to a deterministic set of rules established from 
experimental data. Abnormal behaviors are estimated analyzing the number 
and frequency of these abnormal events on given time windows. 
All the interest of the driver state diagnostic consists of the aggregation of 
elementary diagnostics both direct and indirect provided from different 
sources and then providing more robust information.  
Nevertheless, one of the critical issues of this fusion is the heterogeneity 
of the manipulated data. This is why we have decided to formulate these 
diagnostics with qualitative criteria in which fusion was carried out due to a 
set of 16 fuzzy rules. The final result is a four-level diagnostic represented 
by a set of qualitative values normal, medium, critical and dangerous. Lastly, 
                         
3 The lateral position of the vehicle is determined from the exploitation of the images 
provided by a camera looking forward in the vehicle. 

242     Risk Management in Life-Critical Systems 
these conclusions are modulated with contextual information like time of 
day (chronobiological rhythms) or driving duration. 
These algorithms have been integrated in a real-time computer and tested 
on various databases recorded on different vehicles and for many drivers.  
11.3. Driver distraction diagnostic 
Defining distraction and inattention is not an easy task. Depending on the 
authors, it can slightly differ and lead to some inconsistent definitions. 
Nevertheless, it is interesting to focus on the consequences of inattention or 
distraction on the driving task [RAU 04, BUR 02, ENG 05, GRE 93, ISH 01, 
STR 03, LAN 04, HAN 99, HAN 03, HAR 02, HAI 00]).  According to 
Stutts [STU 01], “distraction is as a state when the “driver is delayed in the 
recognition of information needed to safely accomplish the driving task 
because some event, activity, object, or person within or outside the vehicle 
compels or induces the driver’s shifting attention away from the driving 
task”. 
In a vehicle, the driving task is of course the primary task. In some 
circumstances, this primary task is quasi-exclusive, for example, in case of 
difficult maneuvers or with highly congested traffic. In that situation, all the 
attention of the driver must be dedicated to the driving tasks. 
In other situations, like driving on a clear motorway, the driver has the 
possibility to perform some secondary tasks without taking any special risks 
and without any degradation of his driving performance. 
Problems occur when the importance of these secondary tasks increases 
with respect to those of the primary and sometimes becomes more important. 
In many situations, this has no real consequences. However, sometimes 
when there is a conjunction between these inattention phases and critical 
situations (e.g. a pedestrian crossing the road, etc.) the effects become 
dramatic. 
Various studies held in the US on accident reports demonstrate that 
inattention is one of the causes of a quarter of road accidents [HEN 01, 
WAN 96]. Furthermore, a large study held on a panel of more than 100 
drivers over 12 months demonstrated that inattention was the primary cause 
for accidents [DIN 87]. 

Driver-assistance Systems for Road Safety Improvement     243 
Different categories of distractions should be considered, for example: 
– visual (external attractors, for example, advertisement on the side of the 
road or internal attractors, e.g. looking at his children at the back of the 
vehicle, or displaying an address on a navigation device); 
– acoustic (ringing phone and listening music); 
– cognitive (conversing on the phone but also internal thought and 
rumination, etc.). 
Rockwell [ROC 71] demonstrates that the driving task is “90% visual in 
nature”. However, it is obvious that the cognitive the demand should also 
have a dramatic impact on driving performance. The Gazel cohort set up in 
1989 by Unit 88 (now Unit 687) from INSERM, in partnership with several 
teams of the EDF-GDF company, due to the participation of 20,000 
volunteers, showed that for persons who are undergoing a divorce or 
separation the risk of accident is increased by four. 
Furthermore, the introduction of new infotainment devices in our vehicles 
like navigation systems and increasingly sophisticated audio systems 
generate new potential attractors that could divert the driver from the driving 
tasks. These new devices can require at the same time both important visual, 
cognitive and motor resources. Last but not least, the use of GSM at the 
wheel leads to critical inattention and distraction situations when dialing a 
number (visual and cognitive) but also when speaking with an interlocutor 
who could mobilize some important cognitive resources. 
In parallel to the fundamental research activities related to the analysis of 
the cognitive and physiological mechanisms of inattention and distraction, 
applied research has focused on the developments of techniques, 
technologies and tools that are able to provide, in real time, good indications 
about the current inattentiveness situation of the driver. 
Our first focus is on visual distraction based on the use of cameras and 
image processing techniques analyzing in real time the driver’s head 
orientation. The second approach is based on indirect techniques trying to 
build up distraction indicators based on the observation and analysis of the 
driving and secondary activity of the driver inside the cockpit. 
In order to detect situations and then inform the driver, we have designed 
a system that is able to determine if the driver is looking at the road or not. 

244     Risk Management in Life-Critical Systems 
Such a system must be unintrusive as possible, so we decided to use the 
same technologies that we used for hypovigilance diagnostic (see 11.2.1) 
with specific algorithms analyzing and extracting pertinent parameters from 
the images provided by the camera. 
Moreover, we made the assumption that a long visual distraction situation 
(>2 s) mostly corresponds with head rotation.  
The general principle of the algorithms consists of comparing the face 
principal characteristics (right and left eye, nose, right and left cheeks, etc.) 
observed in real time with those of faces looking at the road stored in a 
database. When correlated, the driver is considered as looking at the road 
“ON-road”, otherwise he is declared “OFF-road” [RAU 04].  
The generated database includes the principal characteristic of a huge 
number of faces with various morphologies (men, women, hair color, beards, 
mustaches, make-up, glasses, etc.). It should be noticed that when the 
database is representative enough of the population variability, this approach 
can be extremely efficient and robust. 
The learning algorithms are using ADABOOST technique with Viola 
Jones classifiers. The validation of an “ON-road” face is effective only when 
a given number of characteristics correctly located were detected.  
Last, due to the analysis of the head position in the time, two distraction 
criteria are provided: 
– visual distraction detection (VDD) that estimates the importance of the 
instantaneous distractions as a function of off road duration; 
– visual time sharing distraction (VTSD) that is an image of the off road 
cumulated time on a given time widows. This diagnostic estimates the 
attention split between road and other thing. 
This system was first evaluated in lab due to an ownership of the 
continental reference tool that was providing a precise measurement about 
the head orientation and then in real driving conditions with 17 drivers. The 
results achieved by the system were compared with those extracted from a 
visual analysis of the experiments. They demonstrated the very good 
performance of the system: average sensitivity: 99.74%,  σ  =  0.35; average 
specificity: 98.95%, σ  =  0.91; false alarm rate: 1.074%, σ  =  0.8. 

Driver-assistance Systems for Road Safety Improvement     245 
 
Figure 11.5. Classification principles for visual distraction detection 
11.4. Human–machine interaction concept 
The ADASs are considered as a major innovation for the automotive field 
in the last 10 years. 
Nevertheless, the deployment of such new assistances will only really be 
efficient if there is a real acceptance from the users in order to maximize the 
profit using them. 
This process will need the redefinition and the design of human–machine 
interaction concepts, considering the driver in the loop, his needs for 
assistance, availability and capability and expectations. 
Strengthened by this report, we decided to develop an original concept of 
driving assistance (named DrivEasy) centered on the driver that should 
address some of these challenges. 
This concept would provide the driver with a legible assistance, adapted 
to his state (distracted, tired, etc.), to his driving style (sporty, normal, etc.), 
to the actions he undertakes but also his wishes for assistance (weak, normal 
and high) [BOV 11]. 
The driving situation provided by the ADASs is implemented in the 
vehicle. It provides an assessment of the driving performances and the 
potential critical situations. 
At the moment, it mainly concerns the quality of the lateral and 
longitudinal control of the vehicle. Among the more usual situations, there are: 
– move closer to the vehicle ahead with a high speed difference 
(longitudinal control); 

246     Risk Management in Life-Critical Systems 
– move closer or cross the lateral lines (lateral control); 
– exceed the legal speed limitations (longitudinal control), etc. 
From this information, risk indicators are calculated: speeding risk, lateral 
risk or risk of collision, etc. 
The driver state diagnostic is provided by the various methods described 
in the previous chapters. Depending on the driver’s state (sleepy, distracted, 
etc.) [BOV 02, BOV 08a, BOV 08b], his awareness of the situation can be 
distorted as well as his reactions in case of problems or his sensitivity to the 
different modalities (audio, visual, haptic, etc.). 
 
Figure 11.6. DrivEasy concept 

Driver-assistance Systems for Road Safety Improvement     247 
The driver’s behavior is characterized by: 
– his driving style, which is modeled and classified in real time from the 
observation of the in-vehicle parameters such as gas or brake pedal 
movements and steering wheel motion, but also the vehicle behavior on the 
road extracted from the information provided by the assistance systems; 
– the analysis of the actions he undertakes to manage the current situation 
[JAN 08, BAT 05, CAR 05], start of braking, release of the gas pedal, action 
on the steering wheel, etc. 
– his wishes for assistance that can be very low or important, which he 
can tune directly . 
This information is analyzed in real time and aggregated with a set of 
decision rules that selects between the actions to undertake and providing an 
adapted assistance to the driver. 
For example, doing nothing if the driver’s attention is focused on the road 
and he is taking the appropriate actions, informing him of a risk or warning 
him. Figure 11.6 presents the general principle of this interaction concept. 
Furthermore, to allow potential evolution of the concept and the integration 
of additional information, knowledge models and new assistance functions, 
the DrivEasy concept is supported by an algorithmic and software modular 
architecture. 
The corresponding algorithms have been developed and implemented 
onto an onboard computer in an experimental vehicle. Many tests, over a 
large period of time, have been performed, on road, with various drivers with 
and without DrivEasy. A comparative analysis of the risks taken by the 
drivers has shown that in 90% of the cases they have been reduced using 
DrivEasy. Most of the drivers have took to the system quickly, and their 
feedback has been positive. 
11.5. Conclusions 
Within the coming years, the automation of the vehicle to reduce fuel 
consumption, to increase safety and to improve the respect of driving rules 
will increase. As a result, the driver will increasingly cooperate with the 
vehicle and will see his values changing. 

248     Risk Management in Life-Critical Systems 
While the vocabulary was still recently max speed and acceleration, 
nowadays we observe a push toward ecology and repression, and the next 
vocabulary should be esthetic, comfort, cool and connected ambiance. 
The HMI will be one of the mainsprings of this revolution. To conduct 
this evolution, the driver should be increasingly assisted in his driving task, 
receiving adapted information (to the driver, to the vehicle and to the 
environment) and advice in an intuitive and non-intrusive way. 
This information should influence his long-term behavior. On the other 
hand, the driver should have the possibility to tune the assistance system 
depending on his understanding or his capacity. Thus, we could also imagine 
configuration assistance systems dedicated to elderly or disabled persons. 
The first assistance systems that have been commercialized for a couple 
of years have demonstrated all their potential to improve road safety, 
comfort and drivability, even showing some limitations mainly due to a too 
technologically orientated approach to the detriment of human aspects. 
The development of increasingly “smart” assistance systems should not, 
in a short- or middle-term time horizon, fully exclude the driver from the 
driving process.  
On the contrary, their deployment will be efficient and accepted only if 
such systems are adapted to the real driver needs, his aptitudes, capacities 
and availabilities. This requires better knowledge about the driver from the 
observation of his attitudes, the modeling of his behavior in some 
circumstances, by the diagnostic of his state and related decision-making. 
The aim of our works, of which only part is presented in this chapter, was 
to build a set of behavioral models of the driver describing, classifying and 
providing a real-time diagnostic about his behaviors and states. This is why, 
in the first step, we addressed the hypovigilance diagnostic then the 
distraction and inattention of the driver. In the second step, we were 
interested in the human–machine interaction problem. We have proposed a 
first and original concept of human–machine interaction providing the driver 
with an adapted assistance. 

Driver-assistance Systems for Road Safety Improvement     249 
11.6. Bibliography 
[AWA 04] AWAKE, System for Effective Assessment of Driver Vigilance and 
Warning According to Traffic Risk Estimation and Vehicle control in 
Emergency-European project IST-2000-28062, 2000–2004. Available at 
http://www.awake-eu.org/. 
[BAT 05] BATLEY R., Results of HASTE SP model, Unpublished memo, University 
of Leeds, 2005. 
[BOV 02a] BOVERIE S., “A new class of intelligent sensors for the inner space 
monitoring of the vehicle of the future”, Control Engineering Practice, IFAC 
Journal, vol. 10, pp. 1169–1178, November 2002. 
[BOV 02b] BOVERIE S., DAURENJOU D., ESTÈVE D., et al., “Driver vigilance 
monitoring – new developments”, 15th IFAC World Congress, Barcelona 
(Spain), July 2002. 
[BOV 05] BOVERIE S., GIRALT A., “Driver monitoring systems, a key technology 
for solving ADAS challenge”, 3rd Technical Symposium on Intelligent Vehicles 
Barcelona International Motor Show, Barcelona (Spain), May 2005. 
[BOV 08a] BOVERIE S., GIRALT A., “Driver vigilance diagnostic based on eyelid 
movement observation”, 17th IFAC World Congress, Seoul, (Korea), July 2008. 
[BOV 08b] BOVERIE S., LE QUELLEC J.M., GIRALT A., “Diagnostic fusion for in 
vehicle driver vigilance assessment”, 17th IFAC World Congress, Seoul (Korea), 
July 2008. 
[BOV 11] BOVERIE S., COUR M., LE GALL J.Y., “Adapted human machine 
interaction concept for driver assistance systems DrivEasy”, 18th IFAC World 
Congress, Milan (Italy), September 2011. 
[BRO 95] BROOKHUIS K., Integrated systems. Results of experimental tests, 
recommendations for introduction, Report Deter, Deliverable 18 to the European 
commission, University of Groningen, 1995. 
[BUR 02] BURNS P.C., PARKES A., BURTON S., et al., How dangerous is driving with 
a mobile phone? Benchmarking the impairment to alcohol, TRL Report TRL547, 
TRL Limited, Berkshire, United Kingdom, 2002. 
[CAR 05] CARSTEN O., BROOKHUIS K., “Issues arising from HASTE experiments”, 
Transportation Research Part F, vol. 8, pp. 191–196. 2005. 
 

250     Risk Management in Life-Critical Systems 
[CER 94] CERF S.B., VERNIÈRES F., “Fuzzy sliding modes – application to large 
time – varying systems”, 2nd IFAC Symposium on Intelligent components and 
Instruments for Control Applications, SICICA’94, Budapest (Hungary), June 
1994. 
[DIN 87] DINGUS T.A., HARDEE H.L., WIERWILLE W.W., “Development of models 
for on-board detection of driver impairment”, Accident Analysis and Prevention, 
vol. 19, pp. 271–283, 1987. 
[ENG 05] ENGSTRÖM J., JOHANSSON E., ÖSTLUND J., “Effects of visual and 
cognitive load in real and simulated motorway driving”, Transportation 
Research Part F, vol. 8, pp. 97–120, 2005. 
[EST 95] ESTÈVE D., COUSTRE A., GARAJEDAGUI M., L’intégration des Systèmes 
Électroniques dans la Voiture du XXI Siècle, Cépadues, 1995. 
[EUR 05] EUROPEAN COMMISSION 2005, White Paper, European transport policy for 
2010: time to decide, 2005. Available at http://ec.europa.eu/transport/ 
white_paper/documents/index_en.htm. 
[GAL 02] GALLEY N., SCHLEICHER R., Fatigue Indicators from the Electro-
Occulogram – a Research Report, 2002. 
[GRE 93] GREEN P., HOEKSTRA E., WILLIAMS M., Further On- The-Road Tests of 
Driver Interfaces: Examination of a Route Guidance System and a Car Phone, 
University of Michigan Transportation Research Institute, Ann Arbor, MI. 
(USA), 1993. 
[HAI 00] HAIGNEY D.E., TAYLOR R.G., WESTERMAN S.J., “Concurrent mobile 
(cellular) phone use and driving performance: task demand characteristics and 
compensatory processes”, Transportation Research Part F: Traffic Psychology 
and Behaviour, vol. 3, pp. 113–121, 2000. 
[HAN 99] HANCOCK P.A., SIMMONS L., HASHEMI L., et al., “The effects of in-
vehicle distraction on driver response during a crucial driving maneuver”, 
Human Factors, vol. 1, no. 4, pp. 295–309, 1999. 
[HAN 03] HANCOCK P.A., LESCH M., SIMMONS L., “The distraction effects of phone 
use during a crucial driving manoeuvre”, Accident Analysis and Prevention,  
vol. 3S, pp. SOI–SI4, 2003. 
[HAR 00] HARGUTT V., KRÜGER H.P., “Eyelid movement and their predictive value 
for fatigue stages”, International Conference of Traffic and Transport 
Psychology (ICTTP), Bern (Switzerland), 2000. 
[HAR 02] HARBLUK J.L., NOY Y.I., EIZENMAN M., The Impact of Cognitive 
Distraction on Driver Behaviour and Vehicle Control, Transport Canada, 
Ottawa, Ontario (Canada), 2002. 

Driver-assistance Systems for Road Safety Improvement     251 
[HAV 07] HAVE-IT, Highly automated vehicles for intelligent transport, European 
project ICT-2007.6.1., 2007. Available at http://www.haveit-eu.org. 
[HEN 01] HENDRICKS D.L., FELL J.C., FREEDMAN M., The Relative Frequency of 
Unsafe Driving, Acts in Administration, 2001. 
[ISH 01] ISHIDA T., MATSUURA T., “The effect of cellular phone use on driving 
performance”, International Association of Traffic Safety Sciences (IATSS) 
Research, vol. 2S, pp. 6–14, 2001. 
[JAN 08] JANSSEN W., Description of the trade-offs between behavior and risk, 
AIDE European project Deliverable D 2.3.2 2008. 
[JOH 05] JOHNS W., TUCKER A., CHAPMAN R., “A new method for monitoring the 
drowsiness of the drivers”, International Conference on Fatigue Management in 
Transportation Operations, Seattle, USA, September 2005. 
[LAN 04] LANSDOWN T.C., BROOK-CARTER N., KERSLOOT T., “Distraction from 
multiple in-vehicle secondary tasks: vehicle performance and mental workload 
implications”, Ergonomics, vol. 47, no. 1, pp. 91–104, 2004. 
[RAU 04] RAUCH N., KAUSSNER A., BOVERIE S., et al., Report on driver assessment 
methodology, Deliverable D32.1 HAVE IT project, 2004 
[ROC 71] ROCKWELL T.H., Eye movement analysis of visual information 
acquisition in driving: an overview, Paper presented at the North Carolina State 
University, Raleigh (USA), 1971. 
[ROG 02] ROGE J., “Alteration of the useful visual field as a function of state of 
vigilance in simulated car driving”, Transportation Research Part F, vol. 5,  
pp. 189–200, 2002. 
 [SAB 75] SABEY B.E., STAUGHTON G.C., “Interacting roles of road environment, 
vehicle and road user in accidents”, 5th International Conference on the 
International Association for Accident and Traffic Medicine, London (UK), 
1975. 
[SCH 08] SCHLEICHER R., GALLEY N., BRIEST S., et al.,, “Blinks and saccades as 
indicators of fatigue in sleepiness warnings: looking tired?”, Ergonomics,  
vol. 51, no. 7, pp.  982–1010, July 2008. 
[SAV 98] SAVE, System for effective Assessment of the driver state and Vehicle 
control in Emergency-European project TR1047, available at, http:// 
www.iao.fhg.de/Projects/SAVE, 1995–1998. 
[SAN 03] SANTANA D.A., Conception d’un système de détection de la baisse de 
vigilance du conducteur par l’utilisation des ondelettes et l’apprentissage 
statistique, PhD thesis, Paul Sabatier University, Toulouse, 6th January 2003. 

252     Risk Management in Life-Critical Systems 
[SEN 04–08] SENSATION, Advanced Sensor Development for Attention Stress 
Vigilance and Sleep/Wakefulness Monitoring, European project IST-507231 – 
available at, www.sensation-eu.org, 2004–2008. 
[STR 03] STRAYER D.L., DREWS F.A., JOHNSTON W.A., “Cell phone-induced 
failures of visual attention during simulated driving”, Journal of Experimental 
Psychology: Applied, vol. 9, pp. 23–32, 2003. 
[STU 01] STUTTS J.C., REINFURT D.W., STAPLIN L., et al., The role of driver 
distraction in traffic crashes, 2001. Available at  http://www.aaafoundation.org/ 
pdf/distraction.pdf. 
[TAT 05] TATTEGRAIN V.H., BELLET T., BOVERIE S., et al., “Development of a 
driver situation assessment module in the AIDE project”, 16th IFAC World 
Congress, Prague (Czech Republic), July 2005. 
[TRE 77] TREAT J.R., TUMBAS N.S., MCDONALD S.T., et al., Tri-level study of the 
causes of traffic accidents, Indiana University, 1977. 
[WAN 96] WANG J.S., KNIPLING R.R., GOODMAN M.J., “The role of driver 
inattention in crashes”, New Statistics from the 1995 Crashworthiness Data 
System, Vancouver, British Columbia (Canada), October 1996. 
[WIE 99] WIERVILLE W., “Historical perspective on slow eyelid closure: whence 
PERCLOS?”, Technical Conference on Ocular Mesures of Driver Alertness, 
Herndon, 1999. 

PART 3 
Managing Risk via  
Human–Machine Cooperation


12 
Human–Machine Cooperation  
Principles to Support Life-Critical  
Systems Management 
12.1. Context  
The principles of human–machine cooperation (HMC) have been laid 
down with a focus on dynamic task allocation [MIL 88, MIL 89]. HMC 
principles have been applied in several domains, including car driving, air 
traffic control, fighter aircraft and robotics. These principles have the 
objective to evaluate the risk of a human–machine system reaching an 
unstable and unrecoverable state. Such a goal can be achieved by proposing 
a model of cooperation from which current or future human–machine 
interactions can be assessed. Indeed, risk could arise from the environment 
or from an inappropriate behavior by humans or machines regarding a 
situation. Therefore, for example, car driving behavior has been studied 
considering unexpected events from the environment such as a dangerous 
part of the infrastructure [AUB 10], a risky behavior by another vehicle 
[HAU 11, MAL 14] and the ability to help drivers with a new cooperative 
device [PAC 05]. Aeronautic and robotics concern more professional human 
operators, and studies mainly focus on the ability to help such human 
operators in achieving new objectives such as the increase of air traffic  
[MIL 93, LEM 96, HOC 98], to cooperate with new unmanned aerial  
[PAC 02] or ground [PAC 11] vehicle.  
                         
Chapter written by Marie-Pierre PACAUX-LEMOINE. 

256     Risk Management in Life-Critical Systems 
The model of HMC is based on a synthesis built from several studies 
managed for over 20 years allowing various investigations and exchanges 
with research partners from different disciplines: automation, computer 
sciences, cognitive and social psychology. The model presents the attributes 
of a cooperative agent. The details are presented in the first section; they are 
based on the main strong hypothesis that it is possible to use a similar model 
for humans and machines. Another objective is to enable humans and 
machines to work as partners and peers. It is excessive of course, but the 
objective is to bring about criteria to decide task allocation from a grid based 
on such a model.  
The second section deals with the multilevel aspect of the cooperation. 
Cooperative activity can appear between human agents, artificial agents and 
human and artificial agents. They are usually organized into several 
hierarchical and activity levels. HMC aims to study and support interactions 
inside and between levels of activity. One way to design this type of support 
is the use of tools that make the perception and understanding of a process’s 
states and other agents’ viewpoints and behavior easier. Such a support is 
called a common work space; it is presented in the third section. The last 
section of this chapter sums up the main aspects of cooperation presented in 
the previous sections by means of a generic representation of a human–
machine system. 
12.2. Human–machine cooperation model 
HMC studies lead us to define the model of an agent according to two 
dimensions, see Figure 12.1: 
– the agent’s ability to control the process and communicate with the 
environment, also called the “know-how” (KH) [MIL 98a]; 
– the agent’s ability to cooperate with other agents concerned by the 
process, also called the “know-how-to-cooperate” (KHC) [MIL 97]. 
Both types of abilities are merged in the human cognitive process and 
also usually merged during artificial agent design, [MIL 98b]. But it is 
interesting to clarify the distinction by highlighting all the types of 
interactions between agents in order to avoid a recurrent problem of 
cooperation. The extension proposed by Millot and Pacaux [MIL 13] is seen 
in Figure 12.1. 

Human–Machine Cooperation Principles     257 
 
Figure 12.1. Attributes of cooperative agent 
12.2.1. The “know-how” or the abilities to control the process  
The KH of an agent only concerns the control of the process and not the 
interaction with other agents. The process is composed of passive and active 
elements. Passive elements refer to infrastructure, obstacles, i.e. elements 
with no ability to act. Active elements are more or less complex systems 
with which an agent considers that it is not possible to cooperate, regarding 
specific situations and/or specific objectives. KH is split into two parts: the 
internal KH and the external KH (see Figure 12.1). 
The internal KH relates agents’ cognitive process, competences and 
capacity to control the process according to, for example, the agents’ 
workload or attention. The competence of a human agent is mainly 
composed of knowledge, rules and skills to control the process [RAS 83]. It 
is linked to the expertise and experience (i.e. practices) of the agent. 
Expertise relates knowledge and rules when experience is more skill based 
and improved by training. Competence is so linked to the difficulty and the 
complexity of the process. The more complex the process, the more the 
agent needs competence to control it. Difficulty relates more to the capacity 
of the agent. The capacity of an agent deals with the number of tasks the 
agent can perform in a period of time. It also addresses the endurance of the  
agent over a long time. Competence and capacity impact each other  
and the acceptable performance is often described as a function of the 

258     Risk Management in Life-Critical Systems 
efficiency and the cost, efficiency focusing competence and cost focusing 
capacity. 
The external KH is associated with the ability to access information about 
the process and the ability to act on the process. Concerning the access to 
information, several precautions must be taken. Data have to be visible, 
readable and comprehensible. The amount of data must be limited and their 
update has to be controlled too. It is usually linked to the dynamic of the 
process [HOC 93]. Agents have to be aware of those technical abilities 
which concern physical and cognitive ergonomics.  
Artificial agents’ KH can be easily identified especially with the help of 
their designers. On the contrary, human agents’ characteristics are usually 
estimated not only by the assessment before and during work by peers or 
hierarchy, but also by themselves. Human agents have a model of 
themselves and this is usually correlated to self-confidence. They have to be 
aware of their competences and capacities regarding the task they have to 
complete. If they have no confidence in themselves for a particular task, they 
could be more favorable to cooperate and they ask for help [GOO 01]. 
To sum up, the internal KH allows agents to build up a representation of 
the current situation, using their competences and capacities, in order to 
gather information, analyze the situation, make a decision and implement an 
action [PAR 00]. Agents are able to conduct those cognitive activities 
because they have an interaction with the process through their external KH. 
So, internal and external KH allow the agent to be aware of the situation; this 
is also called situation awareness (SA) [END 95], i.e. to know the current 
state of the process, to diagnose such a state and to project future states. 
However, because of a lack of competence or a lack of capacity, agents may 
require the assistance of another agent. In this case, they need another 
ability, the ability to take into account the behavior of other agents and 
communicate with them. This specific ability is called the know-how-to-
cooperate. 
12.2.2. Know-how-to-cooperate or the agent’s ability to cooperate  
The KHC allows an agent to take advantage of the complementary KH of 
other agents. It is also split up into two parts: an external part and an internal 
part (see Figure 12.1). The internal KHC allows an agent to build up a model  

Human–Machine Cooperation Principles     259 
of other agents in order to make the cooperation with them easier. The model 
allows them to be aware of  the other actors’ concerns, expectations and 
intentions [SCH 02]. It is built up and updated by training and exchanging 
with others. The model of the other agents consists of building a 
representation of their KH and KHC.  
With the internal part of KHC agents try to build up shared mental 
models [ROU 92] or compatible mental models [PAC 02, FLE 08]. In order 
to build up such a common frame of reference (COFOR) [HOC 01,  
HOC 14], cooperative agents need to observe and communicate, and in 
parallel agents build up a model of each other. For example, a human agent 
needs to understand the automation’s reasoning processes and activities to be 
able to control the global system [WOO 04]. With the model of the others, 
agents are more confident and trust each other [LEE 92] and confident in the 
quality of their interaction [RAJ 08]. Some relations exist between relative 
trust, self-confidence and confidence in the other [VAN 12]. With models, 
the other agents’ behaviors are more predictable and interaction can be more 
constructive. However, too much trust may lead to unacceptable effects such 
as “overtrust” and “overreliance” when an agent overestimates other agents’ 
abilities [INA 10] (see Chapter 15), “complacency” when an agent 
underestimates process state evolution [BIL 96], and “neglect” effects when 
there is not enough mutual control between agents [GOO 01]. Such extreme 
effects may lead to a total dependency of one agent regarding the high 
implication of others, and lead to the risk of an agent losing the feeling of 
being in control of the situation [BIE 05]. Nevertheless, involvement of an 
agent is linked not only to the willingness [NEV 12] or the desire to 
cooperate [MIL 11], but also to the responsibility that can be considered, 
such as a motivational factor [VAN 12]. That is the affective part of the 
cooperation [SKJ 04]. 
One way to build up such a model of others is to communicate with each 
other or at least be able to observe the other. Such communications or 
observations are supported by the external part of the KHC. Three main 
ways are identified in order to reach those goals: (1) they make direct 
observations of others (movements, mimics, emotions, etc.); (2) they have 
verbal exchanges or they communicate through mediated supports; (3) they 
analyze the activity of others through their effect of others’ actions on the 
process. More and more diagnosis systems have been designed to analyze a 
human agent’s behavior and state (motion caption, analysis of physiological  
 

260     Risk Management in Life-Critical Systems 
data such as heart rate, electrodermal activity, eye tracking, etc.). With these 
types of system, artificial agents receive some information to build up and 
update the human agent model. When two human operators cooperate, they 
usually analyze the behavior of the other human state according to voice 
intonation, mimics in order to identify current state, and especially in order 
to know if it is possible to interrupt individual activity for a cooperative one. 
When it is possible they can communicate and so make explicit what is only 
assumed by visual or audio perceptions. In this case, agents need to have a 
common code of communication to be able to receive and understand 
information from others and transmit comprehensible information to others. 
With the model of the others, agents can adapt the information they want to 
transmit to the potential receiver. With mediated communication, 
information is transmitted by the means of visual, audio, tactical or haptic 
supports. Such supports could be useful for agents to share their 
understanding of the situation and negotiate decision-making if necessary, 
mainly when cooperation is asynchronous. Such a support of cooperation is 
called a Common Work Space (CWS) [PAC 02] and is detailed in the next 
section. 
12.3. Common work space 
Cooperative agents develop and maintain a COFOR which is the 
common part of their respective current representation of the situation. CWS 
is the external support of the COFOR [PAC 02]. CWS is the external support 
of a combination of the representation of process states and agents’ “mental 
states”. CWS is a support for a shared or mutual cognitive environment  
[SPE 87, ROG 00]. “A mutual cognitive environment gives each of those 
who share it evidence of the other’s beliefs, including, to some extent, 
beliefs about the other’s beliefs, provided they also have evidence of where 
the other’s attention has been and is going” [SPE 87]. 
Figure 12.2 proposes a schema detailing the interest of CWS during 
cooperative activities. The representation of process states is supported by 
the use of interfaces gathering information from the process. Those 
interfaces also allow the agent to act on the process (bidirectional arrows 
between Agi internal KH and Agi’s interfaces). Therefore, each agent 
develops and maintains an SA regarding the process state  (unidirectional 
dotted arrows between Agi SA and Agi’s interfaces), but also team-SA  
 

Human–Machine Cooperation Principles     261 
[MIL 13], (see also Chapter 13). In fact, CWS can also support cooperative 
activities. By means of CWS, each agent can provide information 
concerning not only their own current and future individual activity, but also 
diagnosis, advice or orders concerning cooperative activity. Supporting such 
cooperative activities can help the human operator not to be detracted from 
his/her own task [BAK 04]. Team-SA is built up in parallel with the 
COFOR. They result from interference management, i.e. differences which 
can appear between one agent SA and the inference of the other agent SA 
(red zone between Ag1 SA and inference of Ag2 SA). 
 
Figure 12.2. Cooperative activity through agents’ know-how (Agi KH), agents’ know-how-to-
cooperate (Agi KHC), agents’ situation awareness (Agi SA), common frame of reference 
(COFOR), team situation awareness (Team SA) and common work space 
Interference management is facilitated by the use of the model of the 
other integrated in the KHC. The figure highlights three ways for an agent to 
have information about the other and build such a model. The first way is the 
more direct one with communications and observations. But this way is time 
consuming and may disturb the other. The second way is the less direct one 
by analyzing Ag1’s actions’ effects through the evolving process. This way 
may involve too many inferences. The last way is the use of the CWS on 

262     Risk Management in Life-Critical Systems 
which activity of each agent is more obvious as well as other agent’s 
intentions. 
An example of CWS proposed to support cooperation between the pilot 
and the weapon system officer of fighter aircraft is presented in Figure 12.3. 
The objective of the study was to build up a model of their cooperation in 
order to design tools that support their cooperation in case of degraded 
situation.  
 
Figure 12.3. Fighter aircraft CWS (example of the tactical situation SITAC).  
For a color version of this figure, see www.iste.co.uk/millot/riskmanagement 
The model has also been used as a reference in order to imagine future 
cooperation between a pilot and a drone, or between a pilot and another 
operator embedded in Airborne Warning and Control System (AWACS). 
CWS was designed in reference to a “tactical situation” i.e.: a view of the 
flight plan (see lines) added with information related to enemies (circles), 

Human–Machine Cooperation Principles     263 
see Figure 12.3 [PAC 02]. The “tactical situation” was enriched by 
information related to individual and cooperative activities of each operator. 
The plan was initially prepared during the mission briefing, but with this tool 
it could be updated in real time during the mission. So, at each step of the 
mission, each operator was able to get information on the current activity 
and the intentions of the other by analyzing CWS. We showed that this type 
of cooperative support avoided the operators having to interrupt each other 
especially when they were fully involved in their own task. A second 
important result was that cooperation was maintained in the team even 
though they were not allowed to communicate by radio for safety reasons.  
Many examples can be found in robotics. With the increase of robots’ 
abilities, such a support can provide information about cooperation. When a 
team is composed of humans and robots it is important to generate the 
feeling of presence. That is possible if agents can share a representation of 
the environment for cooperative planning and cooperation localization  
[KUL 04]. Another example is to permit a small number of humans to 
supervise a larger number of robots. Human SA must be supported by 
displaying team-related information regarding the task state and team state 
[ENV 05]. In [DRI 07], supervisor and teammate share an interface, similar 
to a CWS, which is divided into three levels according to: situation 
(complete or local environment), mission and tasks (goal and task allocation, 
workload and progress of each entity) and entities (comprehension of entities 
behavior, relations and their status and capabilities). CWS is a kind of 
common-ground framework that can facilitate an entire “conversation” 
between human operator and robot by sharing understanding of the robot’s 
context, planning and actions [STU 07].  
All above examples underline different types of task or function that may 
be supported by CWS. They are individual and cooperative and may concern 
several levels of activity. Such levels are now presented in the next section. 
12.4. Multilevel cooperation 
Process control and supervision are usually presented according to 
different types of activity, from the more reactive part, which is close to the 
commands of the process, to the more planned part, which is close to the 
decisional level. In the case of automation and especially in the case of a 
human–machine system, three levels are mainly used [LEM 96]. The 

264     Risk Management in Life-Critical Systems 
planning or strategic level goal is to define the plan of the activity. The 
tactical level goal is to apply the plan by triggering the tasks defined. The 
operational level has the objective to control the process. Each level is 
managed by one or several agents and an agent can take part of one or 
several levels. 
 
Figure 12.4. Multilevel cooperation 
Agents cooperate inside each level in order to reach the common goal of 
the level, but they also cooperate with agents of other levels in order to 
update or modify the activities of each other, if necessary. Cooperation from 
the lower levels to the upper levels would be the example of agents of the 
operational level that provide new field information to agents of the tactical 
level in order to adapt the plan. Cooperation from the upper levels to the 
lower levels would be the example of agents of the strategic levels that 
provide new goals to the tactical and operational levels. In the case of 
cooperation in the fighter aircraft domain, cooperation is necessary when a 
human operator of AWACS provides new commands to post goals to the 
pilot and weapon system officer. They are professional human operators who 
usually have good models of each other. But verbal exchanges may be 
avoided and interfaces like CWS support such cooperative activities. The 
TACtical Situation SITAC  can be updated by datalink and explained by 
comments by the human operator of AWACS (see Figure 12.2). The pilot 
and/or weapon system officer can also provide new information concerning 
the theater of operation. 
Therefore, cooperation can take several forms and CWS must be adapted 
to these forms that may depend on the levels of activity concerned. The three 

Human–Machine Cooperation Principles     265 
levels mentioned before are usually described by authors to present forms of 
cooperation, mainly in order to define modes of autonomy: 
– Cooperation at the strategic level concerns the “cooperation in plan” 
[LOI 01], “plan-based direction” [WOO 04], “planning control” [SIM 07], 
“mission management control loop” [CUM 07]. Cooperative activities may 
concern the planning of individual tasks as well as the planning of 
cooperative tasks such as meeting points. In the “planning-based interaction 
of the mixed-initiative approach” proposed by [FIN 05], cooperative 
activities are planned in order to limit interaction between levels.  
– Cooperation at the tactical level concerns the cooperation that consists 
of updating and executing the plan. In the “cooperation-based interaction of 
the mixed-initiative approach” proposed by [FIN 05], a human operator can 
add new tasks to the plan and an artificial agent validates. Similar 
approaches are proposed by [GOO 01] concerning modes of autonomy with 
“goal-biased autonomy” for which a geographic zone and a goal are defined 
by the user. In the “direction through commanders’ intent”, proposed by 
[WOO 04], cooperative objective is to adapt a plan to disruptions and 
opportunities. This level also refers to the “executive control” [SIM 07], 
“navigation control loop” [CUM 07, ALL 02] proposes an “automated plan 
reasoning system” to support collaborative planning in the case of the 
evacuation-planning domain. They address the tactical levels with three 
possible problem-solving operations: suggesting a new goal, extending an 
existing solution and modifying a parameter of a solution. 
– Cooperation at the operational level concerns the cooperation in action 
[LOI 01]. At this level, agents are mainly reactive ones and they have no 
time to negotiate, but they must adjust their respective actions.  In the 
“operator-based interaction of the mixed-initiative approach” proposed by 
[FIN 05], human agents have to trigger interaction frequently. A similar 
approach is also proposed by [GOO 01] with “the intelligent teleoperation” 
(safe mode, shared control/allocation of control) which is a continuous 
interaction between human and artificial agents. Another way to  
control operational level tasks is to use constraints. With the “constraint- 
based direction” proposed by [WOO 04] or waypoints and heuristics 
proposed by [GOO 01], which are attractive or repulsive potentials, 
constraints are used to control individual tasks by an indirect cooperation. 
This level relates more to the behavioral control [SIM 07] and motion 
control loop [CUM 07]. 

266     Risk Management in Life-Critical Systems 
Previous sections underline the usual main ways to analyze and build up 
cooperation between several human agents or between human and machine 
agents. A model of the other helps us to know how to interact with others, 
whatever the level of activity in which agents are. Such interactions can be 
supported by a CWS. Next section provides a synthesis of such results and 
goes deeper in the presentation of mechanisms that can ensure 
complementarities between cooperative agents’ tasks. 
12.5. Towards a generic modeling of human–machine cooperation 
Previous sections have underlined the main elements that must be taken 
into account in order to analyze current human–machine system for 
improving cooperative aspects, or designing future cooperative agent. Such 
elements are shown in Figure 12.4. 
 
Figure 12.5. Cooperative tasks  1-KH; 2-CWS; 3-KHC (current task); 
 4-KHC (intention); 5-KHC (authority); 6-KHC (model) 
Figure 12.5 has the objective of highlighting the different types of 
interaction between two agents that may take part of one or several levels of 
activity. Therefore, their respective KH, illustrated by the black boxes, may 
be different or similar regarding the tasks they have to complete together. 
Gray parts of the figure detail cooperative activities. The light gray rectangle 
represents CWS. It is the support of cooperative activities when at least one  
 

Human–Machine Cooperation Principles     267 
agent is artificial or when human agents must have mediated communication 
for the reasons explained in section 12.3. Gray straight arrows (thin and 
large ones) deal with cooperation in order to find the best solution and 
combination of tasks to control the process. Gray circular arrows deal with 
cooperation in order to improve the model of each other and decide 
authority. Both of these aspects of cooperation are now detailed.  
12.5.1. Cooperation to decide combination of tasks 
Agents may cooperate on four main tasks: “information gathering”, 
“information analysis”, “decision selection” and “action implementation”. 
According to their respective competences, agents may interact on each task 
by proposing their intentions or comments through the CWS (large gray 
straight arrow). Then agents may accept, impose or negotiate the proposal 
[PAC 02]. Several criteria can be used to find the best solution. Combination 
of tasks must improve the reliability, the capacity or the adaptability of 
human–machine system. These combinations correspond to the debative, the 
augmentative and the integrative forms of cooperation proposed by  
[SCH 91] (for more details see Chapter 13). 
The reliability can be improved by task sharing or task trading [SHE 92]. 
With task trading, one task is allocated to the human agent or artificial agent 
and either of them can transfer the control to the other. With task sharing, the 
human agent and artificial agent control different aspects of the process in 
parallel at the same time. Agents debate the solution because, in this case, 
they have the same KH. 
KH must also be the same when the capacity management is the goal of 
the cooperation. This case appears when the capacity of an agent is not 
sufficient to perform a task. The relief type of task sharing proposed by 
[SHE 92] has such an objective by avoiding human operator overload. This 
approach has been used by [KIE 10] in robotics. They distinguish four 
robots’ abilities similar to the ones proposed by [PAR 00] and define a utility 
value for each task according to required abilities and cost for completing 
the task, preceding and/or next task and current state of execution 
(assigned/not assigned or executable/being executed or executed/solved). A  
 
 

268     Risk Management in Life-Critical Systems 
task is allocated to a robot which has the more important utility value for this 
task. 
The adaptability is important when the KH of one agent is not sufficient 
to perform a task. The “extension type of task sharing” proposes such an 
extension of the capabilities of the human–machine system [SHE 92]. A 
difference is made between adaptive and adaptable systems. An adaptive 
system determines and executes the necessary adaptation. For an adaptable 
system, a human is in charge [PAR 07]. Therefore, agents have to define the 
good coordination of a task according to their KH and their workload. 
Coordinations, illustrated by the gray scale and gray thin straight arrows 
between each type of task of Figure 12.5. Coordination must define the best 
balance between each agent involvement [FLE 12]. A slider is positioned at 
the good place of the scale for a specific situation. This approach is called 
“sliding autonomy” [SIM 07]. But adaptability has been a focus of several 
studies with different approaches such as “adaptive automation” [KAB 06, 
WOO 04, COS 10], “adjustable autonomy” [BAK 04], “situation adaptive 
autonomy” [INA 06] or “mixed-autonomy” [GOO 01, FIN 05]. 
Here is an example of human–robot task sharing and CWS design. The 
study aimed to support cooperation between a human operator and a 
reconnaissance robot partly able to control its trajectory [PAC 11]. During a 
mission, a human operator is placed at a remote place and the robot must 
move on to the targeted area. The robot is partly autonomous at some 
periods but must be controlled remotely at other moments. The mission plan 
has been defined by staff officers using reports and briefings and an initial 
task allocation between human and robot was predefined. A CWS has been 
designed in order to inform the human operator on robot goals (geographic 
point), robot information analysis, the choice of the movement algorithm it 
made and the degree of trust it had in the selected algorithm and the 
autonomous mode. That gave information to the human operator on the next 
goal of the plan. The study assessed the ability to leave the motion control to 
the robot in order to allow the human operator to do reconnaissance. 
12.5.2. Cooperation to decide authority 
Authority decides who among the agents makes the last decision 
concerning one task. Authority is illustrated by the large circular arrows in  
 

Human–Machine Cooperation Principles     269 
Figure 12.5. They point at the slider positioned on the scale. Agents may 
have authority to initiate and authority to terminate a task [GOO 01].  
The literature usually deals with the control of task allocation between 
human and artificial agents. When human agent has the authority, the mode 
of allocation is “explicit” and when artificial agent has the authority the 
mode of allocation is “explicit” [RIE 82]. It is either a human-initiated 
automation invocation or a system-initiated automation invocation [INA 03]. 
Intermediate modes have also been defined, such as the “assisted explicit 
mode”, when allocation is done by the artificial agent but human agent is 
able to take the control over [LEM 96]. Another way to proceed is to 
propose to the human agent the decision of allocation made by the  
artificial agent, called “mode suggestion” [BAK 04],  which is also a way 
proposed by [INA 03] with the “critical-event strategies”. The artificial agent 
prepares an action and waits for the human agent’s agreement. The two other 
strategies proposed by [INA 03] are based on information about the human 
agent. In this case, agents need to have models of the other agents. This 
information is illustrated by the light gray circular arrow between task and 
CWS in Figure 12.5. Agents provide information about themselves in order 
to permit others to update their KHC. The “measurement-based driven 
strategy” deals with gathering current information about the human agent 
state (workload, attention, etc.) and “model-based driven strategy”, deals 
with the use of models to predict the performance of human agents (intents, 
instantaneous capabilities and resources). 
In the previous example, authority management between the human 
operator and robot was defined according to “critical event-strategies”, when 
the robot detects an obstacle during teleoperation and stops. However, it was 
defined according to “model-based driven strategies” when the model 
predicts that the human operator may be overloaded by motion control and 
observation tasks. A sharing of the task is thus decided. Four modes were 
defined: in mode M0, control is fully manual; at the opposite in M3, the 
robot chooses and uses autonomously the movement algorithm. In modes 
M1 and M2, the robot must ask the operator’s agreement. In mode M1, it has 
to wait for the human answer before moving, and in mode M2 it only waits 
for a given time, and can act autonomously if the human has not answered 
before the time is elapsed. 

270     Risk Management in Life-Critical Systems 
 
Figure 12.6. Robotics CWS. For a color version of this figure, see 
www.iste.co.uk/millot/riskmanagement 
12.6. Conclusion and perspectives 
The objective of this chapter was to provide all the elements that must be 
integrated during the definition and evaluation of a human–machine system, 
mainly 
in 
the 
case 
of 
emergency 
situations. 
Some 
concepts  
were presented, such as the agent KH (that is the agent’s abilities to control 
the process) and the agent KHC. This results from the fact the agents are 
usually not alone in controlling a process because of a lack of competence or 
a lack of capacity when the agent risks being overloaded. Cooperation can be 
fruitful when agents have an appropriate model of the other to know how 
they can communicate, and how they can define and allocate the individual 
and cooperative tasks. Supports like CWS help in such cooperation. This 
environment supports different types of exchange (visual, audio, tactical or 
haptic) in order to make explicit usually uncertain information such as 
agents’ intentions, current tasks and the state regarding future or current 
processes. CWS is also a support to manage authority by imposing allocation 
or facilitating negotiation.   

Human–Machine Cooperation Principles     271 
 
Figure 12.7. Example of agents’ abilities identification for task sharing  
and authority management (red arrows). For a color version of this figure, see 
www.iste.co.uk/millot/riskmanagement.zip 
These key elements have been defined regarding several studies 
conducted with various types of human and artificial agents. Variability 
comes from the domain of application, the abilities of artificial agents and 
the experience, the expertise and even the professionalism of human agents. 
Life-critical systems may be very complicated by the number of agents and 
the dynamic of the process; they may also be complex by the number of 
interconnection between agents. Definition and evaluation of human–
machine systems must be conducted with a specific method which integrates 
together human and technical aspects, subjective and objective data, current 
and future agents’ activities. Some combinations of agents KH and KHC 
have already been assessed. They have been defined regarding the objectives 
of the application domain and possible extension of human and artificial 
agents’ abilities. More formalized methods could be proposed using grids 
such as the example presented in Figure 12.7. All abilities could be 
integrated and implemented into artificial agents for computed simulation. 
Several organizations could be so evaluated in order to extract the best 
organizations for real experiments with human operators. The first steps of 
such a method were proposed some time ago [MIL 91]. The method is now 
improved considering the HMC approaches and the multicriteria 
methodology for decision aiding [PAC 10]. 

272     Risk Management in Life-Critical Systems 
12.7. Bibliography 
[AUB 09] AUBERLET J.-M., PACAUX M.-P., ANCEAUX F., et al., “The impact of 
perceptual treatments on lateral control: a study using fixed-base and motion-
base driving simulators”, Accident Analysis and Prevention, vol. 42, no. 1,  
pp. 166–173, 2009. Available at http://dx.doi.org/10.1016/j.aap.2009.07.017. 
[AUB 10] AUBERLET J.-M., PACAUX M.-P., ANCEAUX F., et al., “The impact of 
perceptual treatments on lateral control: A study using fixed-base and motion-
base driving simulators”, Accident Analysis and Prevention, vol. 42, no. 1,  
pp. 166–173, 2010, Available at http://dx.doi.org/10.1016/j.aap.2009.07.017.  
[AGA 01] AGAH A., “Human interactions with intelligent systems: research 
taxonomy”, Computers and Electrical Engineering, vol. 27, pp. 71–107, 2001. 
[ALL 02] ALLEN J., FERGUSON G., “Human-machine collaborative planning”, 
Proceedings of the 3rd International NASA Workshop on Planning and 
Scheduling for Space, Houston, TX, 27–29 October 2002. 
[BAK 04] BAKER M., HOLLY A.Y., “Autonomy mode suggestions for improving 
human-robot interaction”, Proceedings of the IEEE SMC, The Hague, 
Netherlands, 2004 October. 
[BIE 05] BIESTER L., “The concept of cooperative automotion in cars: results from 
the experiment “overtaking on highways”, Proceedings of the Third 
International Driving Symposium on Human Factors in Driver Assessment, 
Training and Vehicle Design, Somoset resort on the Ocean Rockport, Maine, 
USA, pp. 27–30, 2005. 
[BIL 96] BILLINGS C., Human-Centered Aviation Automation: Principles and 
Guidelines, NASA Technical Memorandum 110381, February 1996. 
[COS 10] COSENZO K.A., CHEN J.Y.C., DREXLER J.M., “Dynamic task allocation for 
secure mobility to enhance soldier effectiveness”, 27th Annual Army Science 
Conference, Orlando, Florida, 29 November to 2 December, 2010. 
[CUM 07] CUMMINGS M.L., MITCHELL P.J., “Operator scheduling strategies in 
supervisory control of multiple UAVs”, Aerospace Science and Technology,  
vol. 11, pp. 339–348, 2007. 
[DRI 07] DRIEWER F., SAUER M., SCHILLING K., “Discussion of Challenges for User 
Interfaces in Human-Robot Teams”, Proceedings of 3rd European Conference 
on Mobile Robots, Freiburg, Germany, pp. 19–21, September 2007. 
[END 95] ENDSLEY M., “Towards a theory of situation awareness in dynamic 
systems”, Human Factors, vol. 37, no. 1, pp. 32–64, 1995. 

Human–Machine Cooperation Principles     273 
[ENV 05] ENVARLI I.C., ADAMS J.A., “Task lists for human-multiple robot 
interaction”, IEEE International Workshop on Robot and Human Interactive 
Communication, 2005.  
[FIN 05] FINZI A., ORLANDINI A., “Human-robot interaction through mixed-
initiative planning for rescue and search rovers”, in BANDINI S., MANZONI S. 
(eds.), 9th Congress of the Italian Association for Artificial Intelligence, Milan, 
Italy, pp. 21–23, September 2005. 
[FLE 08] FLEMISCH F., KELSCH J., LÖPER C., et al., “Automation spectrum, inner / 
outer compatibility and other potential useful human factors concepts for 
assistance and automation”, DE WAARD D., FLEMISCH F.O., LORENZ B., et al., 
(eds.), Human Factors for Assistance and Automation, Maastricht, Netherlands, 
pp. 1–16, 2008. 
[FLE 12] FLEMISCH F., HEESEN M., HESSE T., et al., “Towards a dynamic balance 
between humans and automation: authority, ability, responsibility and control in 
shared and cooperative control situation”, Cognition Technology & Work,  
vol. 14, pp. 3–18, 2012.  
[FON 06] FONG T., “The human-robot interaction operating system”, Conference on 
Human-Robot Interaction, HRI, Salt Lake City, Utah, USA, 2–4 March 2006. 
[GOO 01] GOODRICH M., OLSEN D., CRANDALL J., et al., Experiments in adjustable 
autonomy, Technical report version of paper in Proceedings of the IJCAI01 
Workshop on Autonomy, Delegation, and Control: Interacting with Autonomous 
Agents, Seattle, WA, August 2001. 
[HAU 11] HAULT-DUBRULLE A., ROBACHE F., PACAUX M.-P., et al., “Determination 
of pre-impact occupant postures and analysis of consequences on injury 
outcome. Part I: a driving simulator study”, Accident Analysis and Prevention, 
vol. 43, pp. 66–74, 2011. 
[HOC 93] HOC J.M., “Some dimensions of a cognitive typology of process control 
situations, Ergonomics”, vol. 36, pp. 1445–1455, 1993. 
[HOC 98] HOC J.M., LEMOINE M.P., “Cognitive evaluation of human-human and 
human-machine cooperation modes in air traffic control”, The International 
Journal of Aviation Psychology, vol. 8, no. 1, pp. 1–32, 1998. 
[HOC 01] HOC J.M., “Towards a cognitive approach to human-machine cooperation 
in dynamic situations”, International Journal of Human-Computer Studies,  
vol. 54, pp. 509–540, 2001. 
 [HOC 14] HOC J.M., “Human-machine cooperation: a functional approach”, in 
MILLOT P. (ed.), Designing Human-Machine Cooperation Systems, ISTE, 
London, John Wiley & Sons, New York, pp. 273–284, 2014. 

274     Risk Management in Life-Critical Systems 
[INA 03] INAGAKI T., “Adaptive automation: sharing and trading of control”, in 
HOLLNAGEL E. (ed.), Handbook of Cognitive Task Design, LEA, pp. 147–169, 
2003. 
[INA 10] INAGAKI T., ITOH M., “Theoretical framework for analysis and evaluation 
of human’s overtrust in and overreliance on advanced driver assistance systems”, 
Proceedings of European Conference on Human Centred Design for Intelligent 
Transport Systems, HUMANIST publications, Berlin, Germany, 29–30 April 
2010.  
[INA 13]  INAGAKI T., ITOH M., “Theoretical framework for analysis and evaluation 
of human’s over-trust in and over-reliance on advanced driver assistance 
systems”, International Journal of Vehicular Technology, vol. 2013, p. 8, 2013.  
[INA 06] INAGAKI T., “Design of human-machine interactions in light of domain-
dependence of human-centered automation”, Cognition Technology Work, vol. 8, 
no. 3, pp. 161–167, 2006.  
[KAB 04] KABER D.B., ENDSLEY M.R., “The effects of level of automation and 
adaptive automation on human performance, situation awareness and workload 
in a dynamic control task”, Theoretical Issues in Ergonomics Science, vol. 5,  
no. 2, pp. 113–153, 2004. 
[KAB 06] KABER D., PERRY C., SEGALL N., et al., “Situation awareness 
implications of adaptive automation for information processing in an air traffic 
control-related task”, International Journal of Industrial Ergonomics, vol. 36, 
pp. 447–462, 2006. 
[KIE 10] KIENER J., VON STRYK O., “Towards cooperation of heterogeneous, 
autonomous robots: a case study of humanoid and wheeled robots”, Robotics and 
Autonomous Systems, vol. 58, pp. 921–929, 2010. 
[KUL 04] KULICH M., KOUT J., PREUCIL L., et al., “PeLoTe – a Heterogeneous 
Telematic System for Cooperative Search and Rescue Missions. Urban search 
and rescue, from Robocup to real world applications, in conjunction with the 
2004 IEEE/RSJ”, International Conference on Intelligent Robots and Systems 
(IROS), Sendai, Japan, 28 September 2004. 
[LEE 92] LEE J.D., MORAY N., “Trust, control strategies and allocation of function 
in human machine systems”, Ergonomics, vol. 35, no. 10, pp. 1243–1270, 1992. 
[LEM 96] LEMOINE M.-P., DEBERNARD S., CRÉVITS I., et al., “Cooperation between 
humans and machines: first results of an experimentation of a multi-level 
cooperative organization in air traffic control”, Computer Supported Cooperative 
Work, vol. 5, pp. 299–321, 1996. 

Human–Machine Cooperation Principles     275 
[LOI 01] LOISELET A., HOC J.M. “La gestion des interférences et du référentiel 
commun dans la coopération: implications pour la conception”, Psychologie 
Française, vol. 46, pp. 167–179, 2001. 
[MAL 14] MALATERRE G., FONTAINE H., MILLOT M., “The use of accidents in 
design: the case of road accidents”, in MILLOT P. (ed.), Designing Human-
Machine Cooperation Systems, ISTE, London, and John Wiley & Sons, New 
York,  pp. 87–117, 2014. 
[MIL 88] MILLOT P., KAMOUN A., “An implicit method for dynamic task allocation 
between man and computer in supervision posts of automated processes”, 
Proceedings of IFAC/IFIP/IEA/IFORS Conference on Analysis Design and 
Evaluation of Man Machine Systems, Oulu, Finland, pp. 77–82, June 1988. 
[MIL 89] MILLOT P., TABORIN V., KAMOUN A., “Two approaches for man-computer 
cooperation in supervisory tasks”, Proceedings of IFAC/IFIP/IEA/IFORS 
Conference on Analysis Design and Evaluation of Man Machine Systems, Xi-An, 
China, September 1989. 
[MIL 91] MILLOT P., ROUSSILLON E., “Man–machine cooperation in telerobotics: 
problematic and methodologies”, Proceedings of the 2nd Symposium on 
Robotics, Institut National des Sciences et Techniques Nucléaires, Gif-sur-
Yvette, France, 1991. 
[MIL 93] MILLOT P., DEBERNARD S., “Men–machines cooperative organizations: 
methodological and practical attempts in air traffic control”, IEEE/SMC ’93 
Conference, Le Touquet, France, 17–20 October 1993. 
[MIL 97] MILLOT P., HOC J.-M., “Human–machine cooperation: metaphor or 
possible reality?”, Proceedings of the 2nd European Conference on Cognitive 
Science (ECCS ’97), Manchester, England, pp. 165–174, 1997. 
[MIL 98a] MILLOT P., “Concepts and limits for human machine cooperation”, IEEE 
SMC CESA 98, Computational Engineering in System Application Conference, 
Hammamet, Tunisia, April, 1998. 
[MIL 98b] MILLOT P., LEMOINE M.P., “An attempt for generic concepts toward 
human-machine cooperation”, IEEE SMC Conference, San Diego, CA, 1998 
October. 
[MIL 11] MILLOT P., DEBERNARD S., VANDERHAEGEN F., “Authority and cooperation 
between humans and machines”, in BOY G., (ed.), Handbook for Human-
Machine Interaction, Ashgate Publishing Ltd, Wey Court East, 2011. 

276     Risk Management in Life-Critical Systems 
[MIL 13] MILLOT P., PACAUX-LEMOINE M.P., “A common work space for a mutual 
enrichment of human-machine cooperation and seam-situation awareness”, 
Proceedings of the 12th IFAC/IFIP/IFORS/IEA Symposium Analysis Design and 
Evaluation of Human Machine Systems, Las Vegas Nevada, NV, 11–15 August 
2013. 
[NEV 12] NEVO D., BENBASAT I., WAND Y., “The knowledge demands of expertise 
seekers in two different contexts: knowledge allocation versus knowledge 
retrieval”, Decision Support Systems, vol. 53, pp. 482–489, 2012.  
[PAC 00] PACAUX-LEMOINE M.-P., DEBERNARD S., “A common work space to 
support the air traffic control”, Control Engineering Practice, A Journal of 
IFAC, vol. 10, pp. 571–576, 2000. 
[PAC 02] PACAUX-LEMOINE M.-P., LOISELET A., “A common work space to support 
cooperation 
in 
the 
cockpit 
of 
a 
two 
seater 
fighter 
aircraft”, 
in  
BLAY-FORNARINO M., PINNADERY A.M., SCHMIDT K., et al. (eds.), Cooperative 
Systems Design: A Challenge of Mobility age, IOS Press, Amsterdam, North 
Holland, pp. 157–172, 2002. 
[PAC 05] PACAUX-LEMOINE M.-P., ORDIONI J., POPIEUL J.-C., et al., “Cooperating 
with an assistance tool for safe driving”, Proceedings of 16th IFAC World 
Congress, Prague, Czech Republic, 2005 July. 
[PAC 10] PACAUX-LEMOINE M.-P., CREVITS I., “Methodological approach and road 
safety system evaluation”, 11th IFAC/IFIP/IFORS/IEA Symposium on Analysis, 
Design and Evaluation of Human-Machine Systems, Valenciennes, France, 31 
August–3 September 2010. 
[PAC 11] PACAUX M.P., DEBERNARD S., GODIN A., et al., “Levels of automation and 
human-machine cooperation: application to human-robot interaction”, IFAC 
World Conference, Milan, Italy, 2011. 
[PAR 07] PARASURAMAN R., BARNES M., COSENZO K., “Decision support for 
network-centric command and control”, The International C2 Journal, vol. 1,  
no. 2, pp. 43–68, 2007. 
[PAR 00] PARASURAMAN R., SHERIDAN T., WICKENS C. “A model for types and 
levels of human interaction with automation”, IEEE SMC-Part A, vol. 30, no. 3, 
pp. 286–297, May 2000. 
[RAJ 08] RAJAONAH B., TRICOT N., ANCEAUX F., et al., “The role of intervening 
variables in driver-ACC cooperation”, International Journal of Human 
Computer Studies, vol. 66, no. 3, pp. 185–197, 2008. 
[RAS 83] RASMUSSEN J., “Skills, rules and knowledge; signals, signs, and symbols, 
and other distinctions in human performance models”, IEEE Transactions on 
systems, man and cybernetics, vol. SMC13, no. 3, pp. 257–266, May–June 1983.  

Human–Machine Cooperation Principles     277 
[RIE 82] RIEGER C.A., GREENSTEIN, “The allocation of tasks between the human 
and computer in automated systems”, Proceedings of the IEEE on International 
Conference on Cybernetics and Society, New York, USA, pp. 204–208, 1982,  
[ROG 00] ROGNIN L., SALEMBIER P., ZOUINAR M., “Cooperation, reliability of 
socio-technical systems and allocation of function”, International Journal of 
Human–Computer Studies, vol. 52, pp. 357–379, 2000. 
[SCH 91] SCHMIDT K., “Cooperative Work: A Conceptual Framework”,  
RASMUSSEN J., BREHMER B.,  LEPLAT J. (eds.), Distributed Decision Making: 
Cognitive Models for Cooperative Work, John Wiley & Sons, Chichester,  
pp. 75–109, 1991. 
[SCH 02] SCHMIDT K.,  “The problem with “awareness”: Introductory remarks on 
“awareness in CSCW””, Computer Supported Cooperative Work (CSCW): The 
Journal of Collaborative Computing, vol. 11, nos. 3–4, pp. 285–298, 2002.  
[SHE 92] SHERIDAN T.B., Telerobotics, Automation and Human Supervisory 
Control, The MIT Press, 1992. 
[SIM 07] SIMMONS R., SINGH S., HEGER F.W., et al., “Human–robot teams for large-
scale assembly”, Proceedings of NASA Science Technology Conference, 
Adelphi, MD, 2007. 
[SKJ 04] SKJERVE A., SKRAANING JR.G., “The quality of human-automation 
cooperation in human-system interface for nuclear power plants”, International 
Journal of Human-Computer Studies, vol. 61, pp. 649–677, 2004. 
[SPE 87] SPERBER D., WILSON D., “Precis of relevance: communication and 
cognition”, Behavioral and Brain Sciences, vol. 10, pp. 697–754, 1987. 
[STU 07] STUBBS K., HINDS P.J., WETTERGREEN D., “Autonomy and common 
ground in human-robot interaction: a field of study”, IEEE Intelligent Systems, 
vol. 22, no. 2, pp. 42–50, 2007.  
[VAN 12] VANDERHAEGEN F., “Cooperation and learning to increase the autonomy 
of ADAS”, Cognition, Technology & Work, vol. 14, no. 1, pp. 61–69 2012. 
[WOO 04] WOODS D., TITTLE J., FEIL M., et al., “Envisioning human-robot 
coordination in future operations”, IEEE SMC, vol. 34, pp. 210–219, May 2004. 

 

13  
Cooperative Organization for Enhancing 
Situation Awareness 
13.1. Introduction 
In life-critical systems, such as industrial processes, transportation 
systems and communication networks, human activities are mainly oriented 
toward decision-making. Decision-making is motivated by human–machine 
system performance (e.g. production quantity and quality), as well as overall 
system safety. It entails cognitive functions such as monitoring, fault 
prevention and recovery. People involved in the control and management of 
such systems provide two kinds of roles: negative with their ability to make 
errors and positive with their unique involvement and capacity to deal with 
the unexpected: 
– In the negative view, humans are the problem, and designers tend to 
control them, usually in automating a part of the activity, and for the activity 
too hard to automate, in defining procedures the humans must follow in 
order to avoid hazardous behavior. Obliging the human to follow the 
procedure constitutes a kind of automation of the humans.  
– In the positive view, humans remain free of their behavior and are able 
to invent new solutions when they face unknown problems. Indeed, they can 
make mistakes, but are also able to detect and correct these mistakes and are 
especially able to learn from them [ZHA 04]. 
                         
Chapter written by Patrick MILLOT. 

280     Risk Management in Life-Critical Systems 
The human–machine system designer remains, therefore, faced with a 
drastic dilemma: how to combine both roles, a procedure-based automated 
behavior versus an innovative behavior that allows humans to be “aware of” 
and cope with unknown situations? Situation awareness (SA) that 
characterizes the human presence in the system becomes a crucial concept 
for that purpose. Figure 13.1 shows three alternative roles of the humans in 
the systems: on the left part, humans must follow procedures and avoid any 
initiative; in the center, humans have no apparent role (even humans can be 
active in supervision instead of actions); and at the opposite on the right-
hand part, they are able and allowed to intervene especially in unexpected 
situations (see also Chapter 10). In those cases, SA presents all its 
usefulness. 
 
Figure 13.1. Allocation of functions among humans and  
machines (adapted from [BOY 11]) 
However, SA is often decried in its constructs as well as in its uses for 
instance in legal disputes where practitioners can be accused of losing 
situation awareness in court cases and inquests. Of course, SA is becoming 
very fashionable and popular, and scientists feel dispossessed of that 
concept. In the past, our community felt the same with workload, a concept 
which the general public seized and which is imprecisely used. Moreover, 
SA seems not yet sufficiently developed and must be improved. 

Cooperative Organization for Enhancing Situation Awareness     281 
This chapter recalls some advantages of SA to ensure the human presence 
in systems. Conversely, it reviews some of SA’s weaknesses and proposes 
several improvements, especially the effect of the organization and the task 
distribution among the agents to construct an SA distribution, and a support 
to collective SA, called a common work space (CWS). 
13.2. Procedure-based behavior versus innovative behavior 
As previously mentioned, the human–machine systems become 
increasingly complex. Then human decision-maker can be aided by artificial 
agents (so-called “joint cognitive systems”) endowed with cognitive 
capabilities for information processing, diagnosis, prognosis and decision-
making. We face several vicious circles and several consequences:  
– Increasing the level of automation (through decision support systems 
(DSS)) seems to reduce human’s involvement, but as humans remain 
responsible and even accountable of the global system they must supervise 
not only the process but also the DSS. That increases the global system 
complexity as the interactions between the human and artificial agents 
increase [HOL 03]. 
– One possible key to this problem lies in the choice of a relevant 
distribution of functions (or tasks) among the agents involved in the system 
(see Figure 13.1), and thus in the definition of their responsibilities. Human 
operators who are responsible for complete system operations may thus have 
some doubts about interacting with a machine if they do not feel that they 
can control it completely. This very important point might be worth 
expanding, and is seen frequently, for instance, in the hospital: people will 
not adjust a machine because they are uncertain and do not want to screw it 
up [MIL 12]. Even when the machine has the responsibility for making the 
decisions and taking the actions according to the choices of the designer, the 
human operators can nevertheless intervene when they perceive a problem 
related to the system safety. This is, for example, the case in aeronautics 
with onboard anticollision systems [ROM 06]. 
– That gives rise to a second issue related to responsibility distribution 
among people themselves and among people and machines. This issue 
belongs to the general problem of sharing the authority [INA 06, MIL 11] 
and thus involves human factors, such as self-confidence and trust [MOR 95, 
INA 13]. Rajaonah et al. [RAJ 08] described confidence construction 

282     Risk Management in Life-Critical Systems 
mechanisms and their impact on the relationship between people and 
artificial agents (see also Chapter 15).  
– Finally, the use of DSS can have negative and positive effects.  
Barr et al. pointed out that using DSS (1) increases the human  
operator’s understanding of the problems to be solved, (2) improves the 
human operator’s information processing performance and (3) boosts  
the human operator’s confidence in the final decision, by allowing human 
operators to focus on the strategic aspects of the problem to be solved  
[BAR 97]. However, at the same time, using these computerized decision-
making systems can make human operators passive, facilitating 
complacency, specifically because they typically ignore why the system 
proposes what it proposes. When human behavior depends on the system’s 
decisions, human efficiency is generally due to passivity. Moreover, people 
can accept an alternative from a DSS that is worse than what they would 
have come up with unaided [KLE 91, SMI 89]. 
Thus, we can summarize the successive consequences of this chain: as a 
fully automation is not (yet) available, the human must remain in the loop. 
Introducing DSS aims at aiding the human but that increases the global 
system complexity. That gives rise to a balanced distribution of functions 
between humans themselves but also between humans and artificial agents 
with consequences on distribution of responsibility, on the sharing of 
authority, on a trade-off between self-confidence and trust. Finally, that can 
involve risks of passivity and complacency of the human to the artificial 
agent. 
Our claim is that SA is certainly one of the most important constructs that 
allows human factors practitioners, especially human engineers, to break the 
chain of vicious circles. Indeed, to be able to take the right decision, the 
human(s) must remain aware of the system evolution. When people make 
sense of the situation, usually the decision is obvious, and in many settings 
they would not view themselves as having made a decision, even though 
they did take one action as opposed to another [KLE 91]. The human–
machine interactions and the human(s) machine(s) organization must 
therefore be designed to facilitate SA.  
Starting with this postulate, we might have expected that the age of this 
concept would allow it to be rather mature and refined enough in order to be 
efficiently used.  

Cooperative Organization for Enhancing Situation Awareness     283 
13.3. Situation awareness: between usefulness and controversy 
While the practitioners discover its usefulness, SA becomes increasingly 
subject to controversy, first in its constructs and definitions, in its assessment 
and in its extension to collective works. 
13.3.1. Situation awareness: several controversial definitions 
Salmon et al. [SAL 08] describe the numerous attempts for defining SA, 
based on mainly two concepts: for some authors, SA is the process to collect 
and understand information, for instance, based on Neisser’s perception 
theory [NEI 76]; for others, SA is the product of mental mechanisms of 
perception and information processing [END 95a]. According to Endsley, 
SA “is the level of awareness that an individual has of a situation, an 
operator’s dynamic understanding of what is going on”. Endsley’s model 
depicts SA as a component of information processing that follows perception 
and leads to decision-making and action execution. It is composed of three 
levels: SA1 (perception of the elements), SA2 (comprehension of situation) 
and SA3 (projection of future states) (see Figure 13.2). The mental models 
(formed by training and experience) are used to facilitate the achievement of 
SA levels and the development of their maintenance. Despite other 
concurrent definitions, it remains the most popular in the human engineering 
community certainly as it is the most understandable and close to the well-
known Rasmussen’s problem-solving functional model [RAS 83]. 
 
Figure 13.2. SA three-level model adapted from [END 95a] 

284     Risk Management in Life-Critical Systems 
13.3.2. Several SA definitions suffer from a lack of assessment methods 
As we learn from measurement theory, a physical quantity must be stated 
with (1) a definition based on physical observations or on a model of this 
physical phenomenon, (2) a measurement (or estimation) method with given 
qualities (precise, reproducible, non-intrusive, etc.) and (3) a metric that 
gives values in order to compare different cases or at least that allows a 
relation of order between these different cases. Therefore, an argument of 
choice between the present definitions of SA is the existence of an 
associated measurement method. 
Endsley’s SA three-level model is associated with the assessment method 
called the situation awareness global assessment technique (SAGAT)  
[END 95b]. It is exclusively used in simulated contexts and consists of 
freezing randomly the situation, blanking the displays and asking the humans 
relevant questions on their understanding of the current situation. The human 
perception, understanding and projection are then compared to those of a 
subject matter expert (SME) who also follows the same, but not blanked, 
displays. SAGAT has been validated experimentally on a flight simulator. 
Indeed, the questions are so dependent on the situation that humans are 
unable to answer if they are not aware of that situation. SAGAT has acquired 
the status of a reference method. However, its main limitation lies in the 
simulation freeze that prevents its use in real applications, and that limitation 
remains the main obstacle to its use.  
Thus, in order to remedy the SAGAT limitation, Jones and Endsley  
[JON 04] proposed for real situations the “real time probes” method, a 
variant of the situation present assessment method (SPAM) by Durso et al. 
[DUR 98]. It consists of asking a series of periodic questions related to the 
three SA levels (but without blanking the displays) and using the human 
response time to the questions as a complementary index of the SA quality. 
This method has been partly validated by comparison with SAGAT. 
An alternative method, the situational awareness rating technique 
(SART), has been proposed for real situations [TAY 90]. After the task, the 
human notes on a 10-point rating scale the SA level he/she perceived during 
the task. But, the method’s weakness lies in that estimation after the task, 
which can provide biases due to possible memory deficiencies or distortions  
[JON 04]. 

Cooperative Organization for Enhancing Situation Awareness     285 
We can mention other methods based on empirical metrics like in the 
simulator man–machine integration design and analysis system (MIDAS) 
used for the project Next Gen [HOE 10]. But Endsley’s framework remains 
the most commonly used. It also offers foundations to extend this framework 
to groups of decision-makers and to collective works [END 00, SAL 08]. 
13.3.3. Collective situation awareness: an incomplete framework 
As a large number of tasks are performed collectively, new theories are 
proposed and tested in order to understand better the complicated problem of 
collective SA. That results in a great interest for SA and especially for the 
collective SA. Here, we use the term collective SA in order to cover the 
different meanings described in the literature and related to the SA acquired 
by several people and/or artificial cognitive systems. The three examples 
below show the different definitions:  
– Team-SA: Salas et al. [SAL 95] suggested “a shared understanding of a 
situation among team members”; Shu and Furuta [SHU 05], on the basis of 
Endsley’s model, proposed a combination of both individual SA and mutual 
awareness (SA that a cooperative entity has of each other’s activities, etc.); 
Salmon et al. defined team-SA as a combination of team member SA, shared 
SA and the combined SA of the whole team (see Figure 13.3) [SAL 08]. 
– Shared SA: according to Endsley [END 95a], this refers to the level of 
overlap in common SA elements between team members and the degree to 
which every team member possesses the SA required for his/her 
responsibilities. 
– Distributed SA: Stanton et al. suggested that “SA-related knowledge is 
distributed across humans and artefacts, the ownership, usage and sharing 
of knowledge is dynamic and depends upon the task and its associated 
goals... therefore agents have different SA for the same situation but their SA 
can be overlapping and complementary and deficiencies in one agent’s SA 
can be compensated by another agent” [STA 06]. Therefore, each agent has 
an important role in the development and maintenance of other agents’ SA. 
The lack of convergence on the team-SA definition seems to reflect a 
lack of agreement on the foundations to model the phenomenon. It recalls 
the plethora of definitions of workload that invaded the literature in the 
1980s. Salmon et al. deduce that teams need to share pertinent data at the  
 

286     Risk Management in Life-Critical Systems 
higher levels of SA, such as the significance of SA, elements to the team 
goals and also projected states [SAL 08]. The team performance will thus 
depend on the shared goals, the interdependence of team member actions 
and on the division of labor between team members (see Figure 13.3). 
 
Figure 13.3. Team-SA adapted from [SAL 08] 
But the relationships between the agents, the effect of the organization on 
a possible structuration of collective SA remains imprecise and vague. 
Moreover, a criticism often made toward collective SA is the lack of 
design methodology. Indeed, to our knowledge, no structured methodology 
has yet been proposed to ensure that a human–machine system allows SA, 
and moreover collective SA, in the early phase of its design. Of course, 
promoting SA purposes in the design phase can be pragmatically made, but 
not through a structured and reproducible method. Therefore, as collective 
SA as seen in Figure 13.3 seems very close to the framework we are 
developing on task distribution and human–machine cooperation in human 
engineering [MIL 11, PAC 11] whose relevance we have already analyzed 
[MIL 13]. We discuss below a way to transpose it to collective SA problem 
modeling, and therefore to SA design methodology. 

Cooperative Organization for Enhancing Situation Awareness     287 
13.4. Collective SA: how to take the agent’s organization into account? 
Several researchers have developed frameworks to understand and 
describe how SA can be managed collectively in a group of humans and 
artificial agents. But this research topic is still in progress; several studies are 
being led in various application domains like car driving, deep space 
exploration, nuclear power plant and very distributed situations involving 
several agents such as road traffic, firemen deployment or air traffic. Let us 
see below some of these examples in which the collective SA relies on 
different organizations and different distributions of the tasks and of the 
responsibilities among the agents. 
13.4.1. Examples of task distribution and SA distribution among the 
agents 
First, these domains differ in the spatiotemporal organization of the 
agents involved. In other words, the distribution of the tasks (in the sense of 
prescribed work to do) among the agents (humans as well as artificial agents 
if any) will be different according to several criteria:  
– the task decomposition following a spatial criterion, a temporal criterion 
or a hierarchical criterion (decomposition into abstraction levels); 
– the relevant distribution of the subtasks among the agents according to 
their respective know-how1 (KH) and resources and also their availability 
(workload) or simply, for the humans their willingness to perform them 
[MIL 14]. 
For instance, in a Nuclear Power Plant (NPP) control room, the 
organization is highly hierarchized and each operator has a given role, 
related to that hierarchy [BOY 13a]. In a team of firemen, the hierarchy is 
very established too and SA is distributed according to this hierarchy. 
In deep space exploration, SA is distributed over space and time between 
the astronaut on the surface of a planet, and the personnel in the control 
room on Earth. On Earth, the personnel is composed of the scientists who 
control the scientific objectives of the mission and who can propose to the 
astronaut to explore a particular area and the mission operator personnel who 
                         
1 Agent know-how gathers its problem-solving, information perception and action 
capabilities. 

288     Risk Management in Life-Critical Systems 
manage the mission and who are responsible for its safety and performance 
[PLA 13]. This distribution over space between people on Earth and people 
on the surface of the planet and over time due to the communication delay 
gives rise to another organization that produces a special distribution of SA 
among the actors. 
The astronaut can also drive a rover, a task that can also be decomposed 
similarly to car driving tasks, in a vertical manner into several abstraction 
levels. For instance, the common car driving task (on Earth) is decomposed 
in three hierarchical levels, a strategic level for “choosing the itinerary” 
according to the distance to destination, the schedule, the traffic and the 
meteorological conditions, a tactical level that transforms the itinerary into 
“speed and trajectory set values” and finally the operational level that 
“controls the vehicle” according to these set values and the traffic. 
The present Air Traffic Control organization is another particular field 
that gives relevant examples for SA distribution [MIL 14]. The airplanes’ 
safety is assured by two human controllers in a control center on the ground. 
A so-called radar controller (RC) must detect possible conflicts between two 
or several airplanes flying at the same altitude (flight level) in a given area 
called a geographic sector. A conflict appears when the routes of two or 
more planes cross themselves at a distance lower than 5 nautical miles that 
could lead to a risk of collision. For that purpose, the RC follows the 
airplanes’ routes in real time and anticipates their evolution in the short term 
(i.e. several minutes). If he/she detects a conflict, the RC orders one of the 
pilots to modify his/her trajectory. The RC operations are at the tactical 
level, and correspond typically to SA1, SA2 and SA3. A second controller, 
called planning controller (PC), supervises a larger area (made of several 
geographic sectors) and intervenes at a strategic level. He/she anticipates 
these planes’ trajectories a longer time before they enter the sector (about 
half an hour before). When the PC foresees a possible conflict in the long 
term, he/she can order one of the pilots to modify his/her trajectory and 
flight level before entering the sector controlled by his/her colleague, the 
RC. Here too, the PC must develop the three levels of SA, but on a larger 
temporal scale than the RC. A third actor is the airplane pilot himself, who 
must apply the order of deviation given by the controllers to avoid the 
conflict in an anticipated manner. He must also develop his own SA three 
levels but in real time. Finally, a last actor embedded in the modern airplanes 
is the traffic collision avoidance system (TCAS) based on a radar detector  
 

Cooperative Organization for Enhancing Situation Awareness     289 
that detects the risk of collision and controls the avoidance maneuver 
between the two airplanes, one climbing and the other one falling. TCAS has 
its own form of SA at a very shorter response time than the other actors. 
Therefore, SA is distributed among the different actors according to several 
criteria: a spatial criterion (different size of areas), a temporal criterion (long 
term, short term and real time) and a hierarchical decomposition of the 
decisions: both controllers are hierarchically higher than the pilot (in the 
legal sense), and the TCAS is de facto higher than everybody as it is an 
emergency system with its own authority and autonomy. 
We can notice that the action can be performed in a different manner than 
the prescribed task. The activity decomposition and the activity distribution 
can, therefore, be different. This needs a flexible organization and a 
consequent flexibility of the SA distribution.  
13.4.2. Collective SA: the distribution of roles among the agents 
The examples seen above show the importance of the different positions 
and roles of the agents in the organization. Obviously, the agent’s KH takes 
an important part in the distribution of roles among the agents. The questions 
are then:  
– How will the agents operate collectively in the task? 
– Do they decompose it and share the subtasks? 
– Do they each perform the task on their own and compare each result? 
An answer has been proposed by Schmidt [SCH 91] in the cooperation 
field. It holds in three forms: (1) augmentative when the agents have similar 
KH and the task can be shared into similar subtasks; (2) debate if the agents 
have similar KH and are faced with a single task T not divided into subtasks, 
each agent solves the task and then they debate the results; conflicts can 
happen, and each agent must be given dedicated negotiation abilities to solve 
these conflicts [MIL 98]; an important framework related to cooperation will 
be discussed below; and (3) integrative when the agents have different and 
complementary KH and the task can be shared into different and 
complementary subtasks.  
 

290     Risk Management in Life-Critical Systems 
These three forms already exist in human–human organizations and are 
sometimes naturally combined. An example of the augmentative form can be 
observed in banks; when the line in front of a window is too long, a second 
window is opened, thus cutting the line in half and reducing the first teller’s 
workload. An example of the debative form is found in the mutual control 
established between the flying pilot and the co-pilot in the plane cockpit. 
Integrative forms can be seen in the different and complementary tasks 
required to build a house, and also in the coordination of the several jobs by 
an architect. 
Grislin and Millot have shown that these three forms are generic to all 
kinds of cooperation [GRI 99]. We propose below that these three forms are 
the basis of a methodology for distributing roles among the agents, and as a 
result a methodology for distributing SA among a collective of agents (see 
Figure 13.4). 
 
Figure 13.4. The three forms for task distribution according to  
agents KH and related tasks to share 
A task shareable into similar subtasks, and distributed among agents 
having a similar KH, corresponds to an augmentative form of task 
distribution. If agents have different KH, the task distribution is incoherent, 
since all the subtasks will not be covered by the agents. Conversely, if the 
task is shareable into different and complementary subtasks and the agents 
have different and complementary KH (relevant to the subtasks), the form of 
task distribution is integrative; otherwise, no distribution is available since 
all the subtasks will not be covered. Finally, if the task cannot be shared, and 
each agent has a similar KH, the distribution is debative; otherwise, if the 
agent’s KH is different, no distribution is available. 

Cooperative Organization for Enhancing Situation Awareness     291 
Let us see below the impact on SA distribution. 
13.4.3. SA distribution according to the generic forms of task distribution 
We follow the idea that the task distribution will lead to a distribution of 
roles among the agents, and consequently to a distribution of SA among 
them. 
In Figure 13.5, two agents (AGi’s) each execute a subtask STi resulting 
from the decomposition of the task T. When STi is independent and similar, 
the distribution form is augmentative. SA1, SA2 and SA3 of each agent are 
then distributed on their related STi. Nevertheless, AGi can exchange 
information, diagnosis and prognosis on the other STi, for instance for 
sharing common resources. The decomposition is made by a coordinator at 
an upper level (for instance, strategic level), either a third agent or one of 
both AGi’s, but which plays the role of coordinator, that role being 
functionally placed at the upper level. Then SA of each AGi is distributed 
according to the task decomposition. See the example of the bank window. 
 
Figure 13.5. Task distribution and related SA distribution, in the  
augmentative and integrative forms 
If STi’s are different and complementary to each other, the distribution 
form is integrative and SA1, SA2 and SA3 of each agent are then distributed 
according to the related STi. The decomposition is made by a coordinator  
at the upper level, either a third agent (for instance, the architect in  

292     Risk Management in Life-Critical Systems 
the example above) or one of both AGi’s, which plays the role of 
coordinator. Then, each AGi’s SA is distributed according to the task 
decomposition. 
If the task is not decomposed, it is supposed that both agents execute the 
task and they compare their own results hereafter. If their results are 
coherent, each one is comforted in his position. But if they disagree, they 
must debate to know who is right and why? The debate can consist of 
comparing successfully the respective SA3, SA2, and then SA1 of both 
agents. A question remains: who controls the debate? As in the previous 
forms, a coordinator plays that role, who can be either a third agent at an 
upper level, or either agent (see Figure 13.6). Once again, the SA 
distribution follows the task distribution. 
 
Figure 13.6. Task distribution and related SA distribution, in the debative form 
Therefore, the three generic forms of task distribution generate the same 
distribution of SA. This can be the starting point for a methodology for 
designing collective SA. 
13.5. Enhancing collective SA with a support tool issued of cooperation 
concepts: the common work space 
13.5.1. Cooperation model: a similitude with collective SA 
In the cooperation model we propose, each agent contains two 
dimensions: a KH with his/its abilities to control the process or to perform 

Cooperative Organization for Enhancing Situation Awareness     293 
the task (problem-solving, information perception and action capabilities) 
and a know-how-to-cooperate (KHC) allowing him/it to cooperate with other 
agents involved in the task. KHC is an important concept arising from the 
definition of cooperation by Millot and Hoc [MIL 97] adapted from Piaget 
[PIA 77]: “two agents are in a cooperative situation if they meet two 
minimal conditions: (1) each one strives towards goals and can interfere with 
the other one’s goals, resources, procedures, etc, (2) each one tries to 
manage the interference to facilitate the individual activities and/or the 
common task when it exists. The symmetric nature of this definition can be 
only partly satisfied”. For further details, see Chapter 12. 
The agents build and maintain a common frame of reference (COFOR) 
that is the common part of their respective current representation of the 
situation. That allows them building common or complementary plans to 
solve the problems and make decisions cooperatively [HOC 14]. One way to 
support a cooperative agent and to maintain the COFOR is to make the 
perception and the understanding of the process’s states easier as well as the 
other agents’ behaviors and viewpoints. We can note a common point with 
Endsley’s SA1 and SA2.  
Such a support is called CWS. It is an interaction medium designed to 
support cooperation and has been tested in three application domains: air 
traffic control, fighter aircraft and reconnaissance robotics. Chapter 12 
provides more details on these studies. We give just here a complementary 
view on the task distribution and the related SA distribution.  
In the first study, CWS supported especially the detection of air traffic 
conflicts. The cooperation was a distribution of whole conflicts to one or 
other agent. Two agents are at the tactical level, the RC and a DSS called 
SAINTEX and able to detect the possible conflicts and to solve some of the 
easiest2. A CWS has been designed on the basis of the usual Air Traffic 
Controllers’ interfaces, mainly a radar screen and electronic strips showing 
the flight routes, in order to support a cooperation between the three agents: 
RC and SAINTEX at the tactical level, and PC and RC between the strategic 
and tactical levels. SAINTEX can display on the screen conflicts it has 
detected and mark them by extrapolating their trajectories and underlining 
the possible collision. RC and SAINTEX were able to get information about 
current actions and intentions of each other in order to adjust their own 
                         
2 The name SAINTEX is given in reference to the famous French aviator and writter Antoine 
de Saint Exupery author of the well known book: “Le Petit Prince”  (The Little Prince). 

294     Risk Management in Life-Critical Systems 
decisions. Moreover, PC gets information about both other agents’ current 
activities and intentions in order to control the task distribution if necessary. 
Therefore, in this study, CWS was a medium to support cooperation 
especially of augmentative form between RC and SAINTEX at the tactical 
level and of integrative form between the PC at strategic level (who 
distributes the roles) and both agents of the tactical level. Moreover, a third 
form was introduced, an informal debative form between PC and RC 
especially when they disagree on a conflict resolution strategy. 
The second study took place in a fighter aircraft simulator. The objective 
was to analyze cooperative activities between the pilot and weapon system 
officer in order to model their cooperation. Then, this model was used for 
designing tools that support their cooperation in case of degraded situations. 
CWS consisted of a view of the flight plan added with information related to 
enemies and enriched with information related to individual and cooperative 
activities of each operator [PAC 02]. CWS allowed us to update the flight 
plan in real time during the mission. So, at each step of the mission, each 
operator was able to get information on the current activity and the 
intentions of the other by analyzing the CWS. It showed the important 
advantage to preventing the operators interrupting each other when they 
were fully involved in their own task. Moreover, it allowed us to maintain 
cooperation in the team even if the radio communications were not allowed 
for safety reasons. In this study, CWS was a mediation tool between both 
humans in the cockpit, and it supported information perception, situation 
analysis and decision in the three forms: integrative, augmentative and 
debative.  
The third study aimed at supporting cooperation between a human 
operator and a reconnaissance robot partly able to control its trajectory  
[PAC 11]. During a mission, a human operator stands at a given place and 
the robot must move toward a target. The robot is partly autonomous at some 
moments but must be controlled remotely at other moments. The mission 
plan has been defined by staff officers using reports and briefings, and an 
initial task distribution between human and robot was defined. A CWS has  
been designed in order to inform the human operator on the robot goals 
(geographic point), the robot information analysis and the choice of its 
movement. It gave information to the human operator on the next goal of the 
plan. The CWS interface is a screen on which the robot displays a video and  
 
 

Cooperative Organization for Enhancing Situation Awareness     295 
a radar view of the scene it watches, with its target, obstacles, etc., and on 
the right side its current control mode. CWS was greatly helpful for the 
control of the cooperative robot [PAC 11]. In this study, the cooperative 
form was especially integrative, the robot provided the human with data 
he/she is not able to perceive, and the human provided the robot with 
movement controls. 
13.5.2. Common work space for collective SA 
In team-SA model (see Figure 13.3), teamwork includes team processes 
(communication, cooperation and shared mental models) which provides 
team-SA (individual SA, SA of other team members and SA of entire team) 
[SAL 08]. We see “team processes” activities enclosed in the cooperation 
activities. Particularly, the agents build a COFOR and use it in order to 
cooperate, that creates individual SA as well as collective SA. For that 
purpose, a CWS should be a useful medium as explained in Figure 13.7 
[MIL 13]. Agents Agi possess their own KHi to perform the task and their 
KHCi in order to build and use a COFOR useful for cooperation and 
collective SA. 
 
Figure 13.7. CWS principle for team SA [MIL 13] 

296     Risk Management in Life-Critical Systems 
Each agent builds a model of the other (KH and KHC) through direct and 
mediated communication, through observations of the other agent’s actions 
(and deduced intentions) and inferences. Each agent builds a representation 
of the COFOR by assembling its own representation of the current situation 
with an image of the other agent representation built by inference. In the 
same way, they build their own team-SA. 
A CWS would help the agents to analyze other agents’ intentions and to 
elaborate their own SA as well as SA of others and SA of the team. CWS is, 
therefore, an external and explicit representation of the team-SA as well as 
the steps for cooperation. 
A last example illustrates the CWS concept and a methodology for its 
design in the field of deep space exploration that we mentioned above. The 
CWS between the astronaut on the planet’s surface and the mission operator 
and the chief scientist on the Earth takes the form of a virtual camera (VC). 
VC displays the dialogue between the human agents, but it is also a database 
with different useful information on the planet geography, geology, etc., 
which can primarily be recorded in its memory or downloaded online  
[PLA 13]. One of the objectives tested experimentally is to improve 
astronaut SA as well as collective SA. A preliminary evaluation with a so-
called knowledge-based situation assessment tool (KB-SAT), a workload 
assessment with NASA-Task Load index (TLX) method and a dedicated 
scale for usefulness and acceptability evaluation shows encouraging results 
[PLA 14]. A design methodology based on human-centered design 
principles is another interesting aspect [BOY 13b] (see also Chapter 14). 
13.6. Conclusion 
As with some other constructs such as workload in the past, SA can be 
seen as an incomplete framework in the sense that this construct is very 
promising but still in progress. Thus, we attempted in this chapter to analyze 
some of the main strengths and criticisms of SA: the usefulness of the 
concept, the several concurrent definitions, the lack of measurement methods 
associated with some of these definitions, the lack of organizational 
foundations for collective SA, and therefore the lack of method for designing 
collective SA. We showed the similitude between collective SA and  
the framework of cooperation between agents. On the basis of several 

Cooperative Organization for Enhancing Situation Awareness     297 
examples, we gave tracks for designing collective SA through two external 
concepts:  
– the organization and especially the task distribution and we showed the 
interest of three generic distribution forms: augmentative, integrative and 
debative; 
– the concept of CWS, a construct shared with the cooperation 
framework. We saw through three applications the complementarity of the 
task distribution to define the SA distribution, and CWS to support collective 
SA. 
13.7. Bibliography 
[BAR 97] BARR S.H., SHARDA R., “Effectiveness of decision support systems: 
development or reliance effect?”, Decision Support Systems, vol. 21, no. 2,  
pp. 133–146, October 1997. 
[BOY 11] BOY G.A., GROTE G., “The authority issue in organizational automation”, in 
BOY G., (ed.), Handbook for Human-Machine Interaction, Ashgate Publishing 
Ltd, Farnham, England, pp. 131–150, 2011. 
[BOY 13a] BOY G.A., SCHMITT K.A., “Design for safety: a cognitive approach to the 
control and management of nuclear power plants”, Annals of Nuclear Energy, 
vol. 52, pp. 125–136, 2013. 
[BOY 13b] BOY G.A., Orchestrating Human Centered Design, Springer-Verlag, 
London, 2013. 
[DEK 14] DEKKER S.W.A., “The danger of losing situation awareness”, Cognition, 
Technology and Work, 2014. 
[DUR 98] DURSO F.T., HACKWORTH C.A., TRUIT T.R., et al., “Situation awareness as a 
predictor of performance for en route air traffic controllers”, Air Traffic Control 
Quarterly, vol. 6, pp. 1–20, 1998. 
[END 95a] ENDSLEY M., “Towards a theory of situation awareness in dynamic 
systems”, Human Factors, vol. 37, no. 1, pp. 32–64, 1995. 
[END 95b] ENDSLEY M., “Measurement of situation awareness in dynamic systems”, 
Human Factors, vol. 37, no. 2, pp. 65–84, 1995. 
[END 00] ENDSLEY M., ROBERTSON M., “Situation awareness in aircraft maintenance 
teams”, International Journal of Industrial Ergonomics, vol. 26, pp. 301–325, 
2000. 

298     Risk Management in Life-Critical Systems 
[GRI 99] GRISLIN E., MILLOT P., “Specifying artificial cooperative agents through a 
synthesis of several models of cooperation”, in HOC J.M., MILLOT P., HOLLNAGEL E., 
et al. (eds.), Proceedings of the 7th European Conference on Cognitive Science 
Approach to Process Control (CSAPC ‘99), University Press of Valenciennes, 
pp. 73–78, 1999. 
[HOC 14] HOC J.M., “Human-machine cooperation: a functional approach”, in 
MILLOT P., (ed.), Designing Human Machine Cooperation Systems, ISTE, 
London, and John Wiley & Sons, New York, pp. 273–284, 2014. 
[HOE 10] HOOEY B.L., GORE B.F., WICKENS C.D., et al., Modeling Pilot Situation 
Awareness, Human Modeling in Assisted Transportation Workshop, Belgirate, 
Lake Maggiore, Italy, 2010. 
[HOL 03] HOLLNAGEL E., “Prolegomenon to cognitive task design”, in HOLLNAGEL E., 
(ed.), Handbook of Cognitive Task Design, Lawrence Erlbaum Associates, 
London, pp. 3–15, 2003. 
[INA 06] INAGAKI T., “Design of human–machine interactions in light of domain-
dependence of human-centered automation”, Cognition, Technology and Work, 
vol. 8, no. 3, pp. 161–167, 2006.  
[INA 13] INAGAKI T., ITOH M., “Human’s Overtrust in and Overreliance on Advanced 
Driver Assistance Systems: A Theoretical Framework”, International Journal of 
Vehicular Technology, vol. 13, p. 8, 2013. 
[JON 04] JONES D.G., ENDSLEY M.R., “Use of real-time probes for measuring situation 
awareness”, The International Journal of Aviation Psychology, vol. 14, no. 4,  
pp. 343–367, 2004. 
[KLE 91] KLEIN G.A., CALDERWOOD R., “Decision models: some lessons from the 
field”, IEEE Transactions on Systems, Man and Cybernetics, vol. 21, no. 5,  
pp. 1018–1026, 1991. 
[MIL 97] MILLOT P., HOC J.M., “Human-machine cooperation: metaphor or possible 
reality?”, European Conference on Cognitive Sciences (ECCS ‘97), Manchester, 
UK, April 1997. 
[MIL 98] MILLOT P., LEMOINE M.P., “An attempt for generic concept toward human 
machine cooperation”, IEEE SMC’98, San Diego, CA, 1998. 
[MIL 11] MILLOT P., DEBERNARD S., VANDERHAEGEN F., “Authority and cooperation 
between humans and machines”, in Boy G., (ed.), Handbook for Human-
Machine Interaction, Ashgate Publishing Ltd, Farnham, England, pp. 207–234, 
2011. 

Cooperative Organization for Enhancing Situation Awareness     299 
[MIL 12] MILLOT P., BOY G.A., “Human-machine cooperation: a solution for life-
critical systems?”, Work: A Journal of Prevention, Assessment and 
Rehabilitation, vol. 41, pp. 4552–4559, 2012. 
[MIL 13] MILLOT P., PACAUX-LEMOINE M.P., “A common work space for a mutual 
enrichment of human-machine cooperation and team-situation awareness”, 12th 
IFAC/IFIP/IFORS/IEA Symposium on Analysis, Design and Evaluation of 
Human Machine Systems, Las Vegas, NV, 11–15 August 2013. 
[MIL 14] MILLOT P., (ed.), Designing Human Machine Cooperation Systems, ISTE, 
London, and John Wiley & Sons, New York, 2014. 
[MOR 95] MORAY N., LEE, MUIR B., “Trust and human intervention in automated 
systems”, in HOC J.M., CACCIABUE P.C., HOLLNAGEL E., (eds.), Expertise and 
Technology: Cognition and Human Computer Interaction, Lawrence Erlbaum 
Associate, pp. 183–194, 1995. 
[NEI 76] NEISSER U., Cognition and Reality: Principles and Implications of 
Cognitive Psychology, Freeman, San Francisco, 1976. 
[PAC 02] PACAUX-LEMOINE M.-P., LOISELET A., “A common work space to support 
cooperation in the cockpit of a two seater fighter aircraft”, in BLAY-FORNARINO 
M., PINNADERY A.M., SCHMIDT K., et al., (eds.), Cooperative Systems Design: A 
Challenge of Mobility Age, IOS Press, Amsterdam, North-Holland, pp. 157–172, 
2002. 
[PAC 11] PACAUX M.P., DEBERNARD S., GODIN A., et al., “Levels of automation and 
human-machine cooperation: application to human-robot interaction”, IFAC 
World Conference, Milan, Italy, 2011. 
[PIA 77] PIAGET J., Études sociologiques (Sociological Studies), 3rd ed., Droz, 
Geneva, 1977. 
[PLA 13] PLATT D., MILLOT P., BOY G., “Design and evaluation of an exploration 
assistant for human deep space risk mitigation”, 12th IFAC/IFIP/IEA/IFORS 
Symposium on Analysis, Design and Evaluation of Human Machine Systems, Las 
Vegas, NV, 11–15 August 2013. 
[PLA 14] PLATT D., MILLOT P., BOY G., “Participatory design of a cooperative 
exploration mediation tool for human deep space risk mitigation”, HCI 
International, Creta Maris, Heraklion, Crete, Greece, 22–27 June 2014. 
[RAJ 08] RAJAONAH B., TRICOT N., ANCEAUX F., et al., “Role of intervening variables in 
driver-ACC cooperation”, International Journal of Human Computer Studies, 
vol. 66, no. 2008, pp. 185–197, 2008. 

300     Risk Management in Life-Critical Systems 
[RAS 83] RASMUSSEN J., “Skills, rules and knowledge; signals, signs, and symbols, 
and other distinctions in human performance models”, IEEE Transactions on 
Systems, Man and Cybernetics, vol. SMC13, no. 3, pp. 257–266, 1983. 
[ROM 06] ROME F., CABON P., FAVRESSE A., et al., “Human factors issues of TCAS: a 
simulation study”, International Conference on Human-Computer Interaction in 
Aeronautics (HCI – Aero ’06), Seattle, Washington, 20–22 September 2006. 
[SAL 95] SALAS E., PRINCE C., BAKER P.D., et al., “Situation awareness in team 
performance”, Human Factors, vol. 37, no. 1, pp. 123–126, 1995. 
[SAL 08] SALMON P., STANTON N., WALKER G., et al., “What really is going on? Review 
of situation awareness models for individuals and teams”, Theoretical Issues in 
Ergonomics Science, vol. 9, no. 4, pp. 297–323, 2008. 
[SCH 91] SCHMIDT K., “Cooperative work: a conceptual framework”, in RASMUSSEN 
J., BREHMER B., LEPLAT J., (eds.), Distributed Decision Making: Cognitive 
Models for Cooperative Work, John Willey & Sons, Chichester, UK, pp. 75–110, 
1991. 
[SHU 05] SHU Y., FURUTA K., “An inference method of team situation awareness 
based on mutual awareness”, Cognition Technology & Work, vol. 7, pp. 272–
287, 2005. 
[SMI 89] SMITH G.F., “Representational effects on the solving of an unstructured 
decision problem”, IEEE Transactions on Systems, Man and Cybernetics,  
vol. 19, no. 5, pp. 1083–1090, 1989. 
[STA 06] STANTON N.A., STEWART R., HARRIS D., et al., “Distributed situation 
awareness in dynamic systems: theoretical development and application of an 
ergonomic methodology”, Ergonomics, vol. 49, pp. 1288–1311, 2006. 
[TAY 90] TAYLOR R.M., “Situational awareness rating technique (SART): the 
development of a tool for aircrew systems design”, Situational Awareness in 
Aerospace Operations (AGARD-CP-478), NATOAGARD, Neuilly sur Seine, 
France, pp. 3/1–3/17, 1990. 
[ZHA 04] ZHANG Z., POLET P., VANDERHAEGEN F., et al., “Artificial neural network for 
violation analysis”, Reliability Engineering and System Safety, vol. 84, no. 1,  
pp. 3–18, 2004. 
 

14  
A Cooperative Assistant for  
Deep Space Exploration 
14.1. Introduction 
Humans will soon venture out into the solar system possibly exploring 
asteroids, the Moon and eventually Mars. There are many risks associated 
with such deep space human exploration. They include physical, mental, 
emotional and even organizational risks. Great distances will separate these 
explorers from the Earth creating isolation like never before and changing 
the traditional roles of the stakeholders in human space exploration. New 
technology exists to allow mitigation of some of these risks. This chapter 
looks at a human-centered design approach to develop a tool to allow 
improved situation awareness and collaboration in this remote environment. 
14.1.1. Previous human space exploration 
With the exception of a few short Apollo lunar lander expeditions, all of 
the human space exploration has been conducted in low earth orbit (LEO), 
never more than 1,000 km above the surface of the Earth. Human deep space 
missions, to be conducted in the near future, will require increased roles for 
astronauts onboard as well as onboard automation. This is due to the large 
distances between the body being explored and the Earth that creates 
communications delays potentially up to many minutes. The result is a  
 
 
                         
Chapter written by Donald PLATT. 

302     Risk Management in Life-Critical Systems 
paradigm shift from LEO operations where there is a real-time two-way 
partnership between mission control and the onboard crew for International 
Space Station operations. Currently, there is a reliance on ground controllers 
to assist astronauts in real time during operations in LEO on the International 
Space Station. When humans begin exploring in deep space, they will 
require situation awareness assistance tools. 
These tools will locally capture some of the knowledge currently 
contained in mission control. They will, in fact, operate as remote agents for 
mission control-based personnel including mission controllers, mission 
planners, scientists and other experts. 
14.1.2. Deep space situation awareness  
There is a continuous human–human interaction in space exploration that 
will be limited due to communication delays caused by distance in deep 
space exploration. The control model is switching from one of supervision 
from the Earth-based controllers to one of mediation between the ground and 
the astronauts (see Figure 14.1). Risk will be increased as astronauts’ roles 
change to one of more autonomous decision-making as opposed to ground-
mediated decision-making. Risks, such as decision-making based on 
incomplete information or a lack of domain knowledge, are very real for 
astronauts exploring in deep space. Real-time decision-making will be 
required on the part of astronauts without real-time support from operators 
back in mission control due to the communication delays in deep space. 
Tools that support the astronauts’ knowledge-based reasoning and abduction 
to collaborate with human judgment will be required. 
Human deep space missions will be characterized by remote distributed 
operations. There will be a need to make decisions based on the collection 
and analysis of raw data to provide predictive information. This information 
needs to be presented to crews in a way that enhances situation awareness. 
One way to do this is with new interactive environments, such as three-
dimensional (3D) tablet computer systems using advanced interaction 
techniques such as accelerometer- and gesture-based inputs. 
These tolls will provide a common work space (CWS) supporting the 
interaction with the environment. This allows users to share knowledge and 
capture new knowledge quickly and in one central location (database). 

A Cooperative Assistant for Deep Space Exploration     303 
 
Figure 14.1. The model of cooperation between astronauts and ground-based  
experts and how it is changing for deep space exploration 
14.2. The virtual camera 
It is intended that the virtual camera (VC) for human deep space 
exploration will provide a CWS supporting the interaction between explorers 
and the environment. This allows users to share knowledge and capture new 
knowledge quickly and in one central location (database). A tablet-based 
system provides a natural platform for this type of interaction and 
collaboration. In the setting of deep space exploration, technological tools 
are needed to support multiagent or distributed situation awareness. The 
portability and ease of use of a tablet-based tool is ideal for this. The VC 
concept for human space exploration was first described by Boy et al.  
[BOY 10]. 
14.2.1. Motivation 
Astronauts operating on the surface of the Moon or Mars or flying near 
an asteroid will be cognitively and sensorially impoverished compared to 

304     Risk Management in Life-Critical Systems 
Earth exploration. The exploration vehicle is a closed environment often 
with limited visibility, shielding the astronauts from the environment during 
space exploration due to the harshness and dangers of space. Mental and 
physical exhaustion and other effects from long-duration microgravity also 
change the cognitive state of astronauts. These factors all combine to provide 
risks for explorers in this environment. 
Mission operations personnel will be interested in using the VC for 
mission planning purposes as well as for training. For the actual mission, the 
large distances involved between Earth and the body being explored means 
that communications delays will make it impossible to provide real-time 
feedback to astronauts. This requires the VC to capture the expertise of the 
mission operations personnel onboard. The goal is to make the VC a remote 
assistant/agent for the mission operations personnel. 
Scientists will also use the VC for planning future traverses and to look 
for new discoveries on the body being explored. Their experience level with 
the VC tool will typically be less than the astronauts and mission operations 
personnel. They will want to denote and annotate areas of scientific and 
exploration interest to them ahead of time. During training, this can take 
place while the astronauts are still on Earth. This information can be 
archived and later retrieved by other VC users. Then, further augmentation 
can take place as surface exploration on the remote planet occurs and new 
data are collected. 
In its complete form, the VC is composed of four elements. The first 
element considers the existing database of the area being explored. These 
data include terrain and science information as well as information about 
safety and resource considerations. The second element is the user interface 
that provides the interactive element of the system and is what defines the 
VC as an interactive database system. A learning mechanism is the third 
element. It provides the integration of new data with the existing database. It 
also provides the capability to determine which elements of the interaction 
the user is finding more useful and which elements are less useful and may 
be de-emphasized. The final piece of the system is the ability to synchronize 
the existing database with new sensor-derived data as well as data from other 
sources such as other exploration vehicles. Each pixel or data element 
consists of more than 3D of terrain data since it also contains geological, 
exploration and safety information about the environment. 

A Cooperative Assistant for Deep Space Exploration     305 
To implement a VC, it was decided that an immersive tablet-based 3D 
interaction with data representing the area being explored as well as the state 
of the vehicle the astronaut is using for exploration will be a valuable 
situation awareness aid. This flexible display and advanced interaction 
environment allows information displays to be tailored to user preferences 
(type of information displayed, use of icons and interaction techniques) 
allowing a more versatile and user-centered display format than traditional 
spacecraft cockpits. The term “advanced interaction media” is used to 
describe these new interaction techniques and includes advanced interaction 
techniques and tools such as tablets equipped with accelerometers, zoom 
capabilities, multitouch interaction and potentially voice control/annotation. 
A VC implemented on a portable tablet computer offers many unique 
advantages, including the ability to bring the device around the cockpit to 
any required vantage point to explore the remote planetary surface. For 
instance, a crew member using a tablet-based VC can carry it easily from 
one window to the next as the vehicle maneuvers around areas of interest. It 
can also carry electronic procedures that can be pulled-up and displayed 
depending on the situation. 
The goal of the VC is to turn all available data into strategic knowledge 
for exploration. There is a high level of complexity in the data fusion process 
that needs to be taken into account. Questions that VC must address include: 
how should multiple data sources be represented in a coherent user 
interface? How can the system represent and display knowledge derived 
from this data fusion? It must take into account expertise from those 
involved in space exploration, such as astronauts, mission operations 
personnel and scientists, and allow all of them to collaborate and coordinate 
in the exploration. 
Astronauts’ workload will be very high as they explore another planet or 
asteroid. They may be concerned, for instance, with navigating near a 
relatively unfamiliar asteroid in a microgravity environment. Dust may begin 
to obscure the view as thruster firings impinge on the surface. There will 
also be stress from concerns about successfully accomplishing the mission as 
well as the physical hazards of the hostile space environment. As workload 
increases, the level of stress also increases. A point is reached where stress 
causes a discontinuity and performance “crashes.” The VC must assist in 
workload by assisting in navigation, planning of safe traverses and in  
 
 

306     Risk Management in Life-Critical Systems 
determining areas of interest (AOIs) to explore. The VC can help mitigate 
the risk of high workload. It can do this by triggering deep knowledge 
combined with prior training, presenting information in an easy to 
comprehend format. 
The VC must be able to recover from errors and continue to operate in a 
meaningful way. It must use feedback to determine future action.  
Figure 14.2 shows the feedback loop used by the VC for displaying data and 
information for users. Feedback should be clear and verbose enough for the 
users to understand what the system is currently doing or proposing. Mission 
goals feed into what should be displayed in the VC and when. It will help 
define AOIs as well as areas of safety concern. Users can update this 
information as it is collected during exploration. Astronauts interact with the 
VC in real time as they conduct a traverse or exploration sortie. Ground-
based experts, such as scientists and mission operations personnel, may 
interact in a non-real-time offline way. They can take more time to be 
immersed in the 3D data and make decisions for further exploration, 
annotating as they explore the database. Sensors collect data during traverses 
that can then be used to update the 3D database. Other display platforms, 
such as 3D displays and 3D glasses, can be used on the ground to enter data 
into the database for exploration. The portability and flexibility of the tablet-
based VC can then interface with this updated database. 
The VC feedback mechanism uses sensors, previous spacecraft data as 
well as science and mission operations concepts and rules as inputs to the 
VC database. The database will then output terrain and AOI information as 
well as warnings about potential safety concerns. Astronaut feedback as well 
as additional sensor data will be used as a compensation negative feedback 
control. This loop is illustrated in Figure 14.2. The feedback control could be 
made automatic based on mission rules for astronaut safety, comfort and 
efficiency or astronaut-defined feedback. 
Few people will be able to explore the surface of another planet. In the 
Apollo program, only 12 people ever walked on the surface of the Moon. For 
the foreseeable future, we will be limited in the number of people we can 
launch into deep space. This is due to the limited lift capacity and launch rate 
of current rockets. This clearly means that some level of automation will be 
required to assist the limited number of human explorers we can send to 
deep space. 

A Cooperative Assistant for Deep Space Exploration     307 
 
Figure 14.2. Virtual camera data feedback loop 
Astronauts are not typically geologists (although there are a few 
exceptions). However, astronauts will need some training as geologists in 
order to work efficiently on the planetary surface and make good use of their 
time there. They could follow procedures (look for orange soil, check for flat 
rocks, etc.) [SPU 92]. More effectively, the VC can automate some of the 
exploration traits characteristic of a field geologist. It can use its access to 
the database to display AOIs that are nearby. Furthermore it would be more 
of greater interest for the VC to collect data and merge them in its database 
and draw conclusions on the fly about the nearby environment. Rather than 
merely analyzing patterns in the data already contained in the database, the 
VC would be truly interacting and making abductive conclusions about its 
environment. Ideally, it would be like having a geologist as part of the 
mission. While not implemented yet, this would be a useful future feature. 
It can be thought that the cognitive functions of a mission operation or 
scientist to guide the astronauts have been transformed into an automated 
agent, the VC. This automated agent is not in total control but rather a role of 
as a co-pilot or perhaps a flight engineer to assist the astronaut. It can 
provide suggestions and give information as to areas to explore or avoid but 
is not in line with the actual vehicle control loop. The VC is providing know-
how-to-cooperate involving agent-to-agent interaction, based on a common 
representation of the process called a common frame of reference (COFOR) 
[MIL 98]. 

308     Risk Management in Life-Critical Systems 
As the planetary surface is explored, the environment will change 
dynamically. When exploration starts, the surface will not be completely 
mapped. As a region is explored by the astronauts in their rover, the VC 
database will expand and the level of resolution of the data in it will 
improve. This will allow the VC to continue to increase the amount of 
knowledge it provides the human operators. A small change in resolution or 
amount of data collected on a nearby region may make it more interesting 
for exploration or perhaps show the area to be more dangerous than 
expected. 
It was observed by Gerry Griffin, a former NASA Apollo flight director, 
that the success of the Apollo program was due to the trust and respect 
between the flight and ground crews as well as the ground being considered 
an extension of the crew and spacecraft [GRI 10]. The VC allows this to 
continue at the next level thereby lowering potential risks. As mentioned 
previously, the VC can be considered a remote agent for the ground crew. 
This allows an even tighter integration of both elements especially for distant 
operations where communication delays will make it impossible to have 
interactive conversations between the crew and ground. The way to 
accomplish this is to understand what the ground crew adds to the flight 
crew element through interviews, an interactive design process and a 
cognitive function analysis of required tasks [BOY 98]. This process also 
includes scenario and human-in-the-loop simulations. The VC must capture 
the knowledge of the ground crews and scientists in order to be effective. 
The VC can also assist in another aspect of the Apollo success. In Apollo, 
they trained as they flew and flew as they trained. The VC can provide a 
realistic representation of the surface of the planet to be explored while the 
crew is still on the ground in simulators. Flight and ground crews can add 
annotations to the VC database at certain areas of interest. For instance, the 
scientists could annotate, “look for striated rocks indicative of possible water 
reserves.” Mission simulations can be conducted where the crew explores a 
remote region of the Earth in a rover with a built in communications delay. 
They will have to rely on the VC for safety and exploration knowledge. This 
knowledge will include what AOIs should be explored, virtual views of the 
rover to assist in safe navigation and solar space weather reports to 
realistically simulate radiation concerns on another planet while in training. 
The VC can provide this type of information during actual mission events as 
well. 

A Cooperative Assistant for Deep Space Exploration     309 
14.2.2. Design method 
The design process for the development of a VC to aid in human deep 
space exploration followed a human-centered design process. This process 
involves users and all stakeholders from the very earliest stage of design. It 
illustrates an excellent design technique for use in risk and safety-critical 
systems in a variety of domains and is represented in Figure 14.3. 
 
Figure 14.3. The human-centered design process for the  
development of the virtual camera 
The first step in the design process is to elicit user requirements. This is 
done by interviewing a set of potential users. A key requirement is that all 
types of possible users be interviewed. They should also represent the proper 
experience level of the actual users. 
The initial point in determining user requirements is to identify experts 
who represent all possible user types for the VC system. This requires some 
knowledge of the domain and also of the goals of the VC. As mentioned, 
potential users of the VC device include astronauts, mission operations 
personnel and scientists. 

310     Risk Management in Life-Critical Systems 
A survey of potential users and stakeholders for the VC system was 
conducted by the author at the 2011 NASA DesertRATS analog exploration 
testbed. This was a two-week set of tests and simulations conducted in the 
Arizona desert. The goals were to determine equipment and techniques 
useful for future human deep space exploration. The main questions during 
potential user interviews involved their background, general system uses, 
interface type and data display parameter formats desired. 
 
Figure 14.4. Riding in the NASA Lunar Electric Rover vehicle at  
DesertRATS, collecting user requirements for the development of the VC 
Brief scenarios and interaction diagrams as well as storyboards and 
prototypes were then developed. These were presented to users to show the 
basic concept of the VC and to get feedback of what would be a beneficial 
interface for a particular user and what may also not be of benefit to a user. 
Scenarios and use cases give users examples of how the VC can be applied. 
Use cases allow possible users to see the usefulness of the VC as well as 
suggest improvements or changes to make the VC a more efficient  
system. Table 14.1 gives an example of use case where mission planners 
interact with the VC to plan a future traverse. It helps to determine what 
operations are required of the VC and where potential risks may occur or be 
addressed. 

A Cooperative Assistant for Deep Space Exploration     311 
Summary: mission planners review virtual camera (VC) data to determine next day’s 
traverses across the planetary surface 
Basic course of events: 
 
1. Science and surface data collected into a database from previous missions and 
previous traverses correlating various missions together 
 
2. Planners use a multimodal input system that provides feedback for terrain with 
the ability to correlate feedback with various mission parameters: minerals, resources, 
slope and obstacles 
 
3. Planners gather around a large live board allowing group interaction to 
determine both a safe and exploration fruitful day’s worth of activities 
 
4. The system records everything the user does annotating areas of interest by 
judging how long the planner stays observing a particular area or region 
 
5. Group elicitation method style interface to allow traverse/sortie decision-
making in a group  
 
6. A final traverse/sortie map is uplinked to rover system 
Possible failures: 
 
Database not of high enough resolution to make a proper determination 
Possible solution:  
 
Have rover collect more data of area before moving using onboard sensors 
Extension: 
 
 
1. Traverse/sortie direction determined by: 
 
     Science return 
 
     Resource requirements for habitation base 
 
     Safety 
 
2. Traverse also selected to assist other nearby vehicles 
Preconditions: 
 
Mission-defined rules for safety and mission success criteria 
Postconditions: 
 
Surface exploration following mission rules 
Table 14.1. A use case for surface exploration 
14.2.3. Implementation 
After requirements and basic design needs were defined, horizontal 
prototypes were created. Basic interface screens were developed for user 
testing and feedback. These screens illustrated all of the functionality and 
capabilities of the VC although the database interactivity was not yet 
possible. This horizontal prototyping allows astronaut, scientist and mission 
operations personnel to provide feedback as to the usefulness of the screens 
being developed. Emergent uses of the system and behaviors enabled or 
limited by the VC were discovered. Figure 14.5 shows an example of 
horizontal prototype with AOIs and areas of safety concern. 
 

312     Risk Management in Life-Critical Systems 
Horizontal prototypes helped to further define requirements for: 
– safety resolution; 
– exploration resolution; 
– real-time data display; 
– format of data displays (visualization); 
– graphical user interfaces. 
 
Figure 14.5. Horizontal prototype for the VC showing icons and interface. For a color 
version of this figure, see www.iste.co.uk/millot/riskmanagement 
The interface screen in Figure 14.5 shows an asteroid surface projection. 
This prototype screen provides a possible view of how astronauts would 
interface with the VC as they approached the asteroid in an exploration 
vehicle. The vehicle’s velocity in X, Y and Z coordinates is shown in the 
upper left part of the figure. Next to the actual velocity are bars of relative 
velocity. The real interface is in color: the green color (see color version of 
the figure) represents a safe speed for the operating environment. If the 
velocity was slightly high, then the bar would turn yellow and if it was a 
dangerous level, it would turn red. The velocity vector is simulated by the 
green arrow which also points toward the basic direction of travel. An 
information cartouche in the lower right shows icons for communication 
link, thruster systems in use, electrical power and collision potential going 
clockwise from the upper left. Other information elements include a 

A Cooperative Assistant for Deep Space Exploration     313 
radiation sensor warning icon in the lower middle indicating an onboard 
radiation sensor has detected radiation. On the lower left, the blue beaker 
icon indicates an area of scientific interest. 
The VC is designed to assist the user and not to be a control system; so 
the goal is to provide summary information and determine, by context, what 
is useful for the user and what may not be so useful. The user should also be 
able to define what is useful. These possibilities were also explored during 
prototype development. A movie was created showing how screen displays 
would change as a vehicle approached closely to an asteroid. 
The next iteration in the design cycle, following a human-centered design 
approach, was the development of a vertical prototype that implemented 
design changes found from the horizontal prototype evaluation. The 
interface now interacts with a multidimensional terrain database 
(GoogleMaps) and allows users to add, change and annotate AOIs in the 
database with the tablet application. Figure 14.6 shows the vertical 
prototype. In order to easily evaluate the interaction and usefulness of the 
VC in the field, the context was changed from asteroid approach to surface 
exploration for the vertical prototype. 
 
Figure 14.6. The VC vertical prototype with icons labeled. For a  
color version of this figure, see www.iste.co.uk/millot/riskmanagement 

314     Risk Management in Life-Critical Systems 
Usability of the VC is being evaluated using nominal and off-nominal 
situations and simulations of actual mission operations cases in field-based 
testing. Important considerations include what is appropriate for a given task 
or function in a given context such as after an alarm or interruption versus 
normal operations. 
Usage of the vertical prototype determines how unique aspects of the 
tablet improve or impact situation awareness. It may also be possible that the 
VC actually detracts from the situation awareness. It is a tool that is used to 
analyze ways to improve situation awareness as well as illustrate how 
interactions between collaborators and their various roles change in deep 
space exploration compared to LEO operations. 
14.3. Evaluation 
After development, the VC must be evaluated for effectiveness in analogs 
that can simulate space exploration as closely as possible. Again, an iterative 
approach was used. This started with initial tests of the vertical prototype with 
users in a public park setting where they used the device to navigate and find 
predefined areas of interest. These tests were used to further refine the tablet 
application as well as the test and evaluation process for the next step, 
evaluation of the tool in an actual scientific field expedition. 
14.3.1. Preliminary testing 
In order to measure the effectiveness of the VC, simulated traverses are 
conducted in a variety of nominal and off-nominal scenarios. Initial tests 
have been conducted in a terrestrial setting (a public park) with AOIs 
defined to be of science or resource use as well as hazards. These points had 
been entered into the VC Google map terrain database used by the tablet 
application. 
In the initial tests, one team used the VC in the field to find the AOIs 
within a 30 min time period. The second team acted as mission control, 
guiding the team in the field and planning the traverse in advance. The third 
team conducted a field traverse but had only a map tablet application that 
had none of the VC interactivity. 

A Cooperative Assistant for Deep Space Exploration     315 
When using the map alone with none of the interactive capability of the 
VC the control team found they had high confidence in where in the course 
they were but lower confidence on what exact AOIs they were looking for. 
They did not have the ability to see the AOIs marked off at a higher zoom-in 
level and had no access to the annotation information placed by the 
simulated ground experts in the database. They found themselves surveying 
actual landmarks often to get their bearings. This indicates a reliance on the 
map for navigation but more unknowns about the actual AOIs and what the 
team was actually looking for. Since time is precious during space-based 
exploration, efficiency and precision are very important. 
The subject teams using the full VC interaction system found themselves 
relying more heavily on the VC to the point that when the wireless link to 
the Google database was lost, the team felt somewhat confused about their 
location. They were operating in a much more “heads-down” mode relying 
more heavily on the technology. 
One interesting test conducted was with a simulated communications 
delay between the field teams and mission control. This was done by having 
an observer communicate by text message with mission control for 
questions. The mission control team would then wait a predetermined 
amount of time and then respond. It was observed that the VC field teams 
would often not wait for responses, especially with the 30 min traverse time 
limit. They would use the VC and their own knowledge to make decisions 
and move forward with their exploration and search for the defined areas of 
interest. The large risk associated with the time delay between mission 
control and astronauts in deep space exploration will require local tools that 
capture some of the remote knowledge and expertise. The VC needs to 
enable the interface that makes the recall and interpretation of this captured 
knowledge as seamless as possible. Also the VC enables a closer 
collaboration and sharing of information between several astronaut teams 
who may be exploring in the field. The explorers can add information about 
sites of interest and scientifically important items as they are uncovered in 
the field. 
14.3.2. Further testing 
The first round of testing paves the way for further testing in a realistic 
deep space exploration analog with domain experts such as astronauts, 

316     Risk Management in Life-Critical Systems 
pilots, planetary scientists and mission operations personnel. Subjective 
situation awareness feedback will be solicited from these domain experts. 
This will allow a knowledge-based evaluation of the VC and its ability to 
support collaboration in deep space exploration [BOY 98]. 
The VC can be viewed as a tool to determine techniques for improving 
situation awareness in deep space exploration. Another very useful purpose 
of the VC is the ability to support human–human and human–machine 
cooperation as well as decision-making and how it can be mediated with the 
VC. 
During the next development phase, the VC will be put into the hands of 
field scientists who will use it during actual scientific field expeditions. 
14.4. Future work 
The VC can further be developed to include more machine learning 
mechanisms and display formats for different types of users. Each round of 
testing highlights emergent behaviors and possible new functions and 
capabilities. 
An interactive tablet application that allows field personnel to exchange 
and share information has utility in a number of domains other than space 
exploration. These could include 3D displays for dynamic weather 
conditions for pilots in the cockpit, first responders for natural disaster 
situations, military operations and control room operators. These 
possibilities are currently being further pursued. 
14.5. Conclusion 
Human space exploration will change noticeably, as we move further into 
the solar system. Roles will change, and astronauts will have more 
responsibility for local decision-making. Ground-based expert knowledge 
will still be very important but not always available in real time. Tools are 
needed to bring this knowledge into the cockpits of deep space exploration 
vehicles. Risks abound in this environment; so a new technology needs to be 
applied in a human-centered way to assist this exploration. One such 
technological tool is the VC for human deep space exploration. 

A Cooperative Assistant for Deep Space Exploration     317 
The VC has been developed using a human-centered design approach 
showing the usefulness of this technique for risk-intensive systems. The 
human-centered design tenets of early stakeholder involvement, iterative 
design, development and testing allow a flexible design process that can 
consider elements of risk-critical systems and design decisions to be made 
before they are used in the actual safety-critical environment. 
14.6. Bibliography 
[BOY 98] BOY G., Cognitive Function Analysis, Greenwood/Ablex, CT, 1998. 
[BOY 10] BOY G., et al., “The virtual camera: a third person view”, 3rd 
International Conference on Applied Human Factors and Ergonomics, July 
2010. 
[GRI 10] GRIFFIN G., Keynote Address, HCI-AERO, 2010. 
[MIL 98] MILLOT P., “Concepts and limits for human-machine cooperation”, IEEE 
SMC IMACS Conference on Computational Engineering in Systems Applications 
(CESA’98), Hammamet, Tunisia, 1998. 
[SPU 92] SPUDIS P., TAYLOR G., “The roles of humans and robots as field 
geologists on the moon”, 2nd Symposium on Lunar Bases and Space Activities of 
the 21st Century, NASA Conference Publication No. 3166, 1992. 
 

 

15 
Managing the Risks of  
Automobile Accidents via  
Human–Machine Collaboration 
15.1. Introduction  
Human drivers are vulnerable; they sometimes make errors, get  
distracted and even become drowsy during driving. Based on today’s 
sophisticated environmental sensing and vehicle control technologies, many 
advanced driver-assistance systems (ADASs) have been studied and developed, 
and some of them have already been commercialized. However, such intelligent 
machines are also vulnerable; they sometimes make errors, and fail and/or even 
get broken. The human drivers have been asked to be responsible for the safety 
of driving (see, e.g. [CON 68]), and will also be in the future. In fact, recent self-
driving (i.e. driven automatically) vehicles still require a driver as a monitor 
and/or a supervisor of the automatic control system. 
The philosophy of human-centered automation [BIL 97, WOO 89] plays 
an important role in the design of ADAS, for the human has to be 
responsible for the driving safety. It should be mentioned here if the 
principle of the human-centered automation is applied in a strict manner, it is 
required that the level of automation [SHE 92, INA 98] (see Table 15.1) 
should be less than 6, i.e. the automation is not allowed to make a safety-
critical decision and implement it autonomously. In that case, the human 
drivers have to make the decision, and at least give an order to the 
subordinate machine to do the action even under extremely time- and/or 
                         
Chapter written by Makoto ITOH. 

320     Risk Management in Life-Critical Systems 
resource-limited conditions. Machine assistance should be given under such 
critical situations. Thus, it is important to establish human–machine 
coagency; in other words, they should be compensatory to each other.  
 
1. 
The computer offers no assistance, human must do it all 
2.     The computer offers a complete set of action alternatives, and 
3.      narrows the selection down to a few, or 
4.      suggests one, and 
5.      executes that suggestion if the human approves, or 
6.       allows the human a restricted time to veto before automatic execution, or 
6.5     executes automatically after telling the human what it is going to do, or 
7.      executes automatically, then necessarily informs the human, or 
8.      informs him after execution only if he asks, or 
9.      informs him after execution if it, the computer, decides to. 
10. The computer decides everything and acts autonomously ignoring the human 
Table 15.1. Scale of degrees of automation [SHE 92, INA 98] 
Here, mutual understanding between the human driver and the ADAS is 
important. In particular, the human driver has to place appropriate trust in 
the ADAS based on the appropriate understanding of the ADAS. In order to 
attain the human appropriate trust in the ADAS, it is necessary for system 
designers to understand what trust is and what inappropriate trust is (i.e. 
overtrust and distrust), and how to design ADAS that is appropriately trusted 
by human drivers. ADAS also has to understand the physiological and/or 
cognitive state of the human driver in order to determine whether it is really 
necessary to provide assistive functions, especially safety control actions. 
This chapter will present a theoretical model of trust in ADAS, which is 
useful to understand what overtrust and/or distrust is and what should be 
needed to avoid inappropriate trust. Also, this chapter presents several 
driver-monitoring techniques, especially to detect driver drowsiness or 
fatigue and driver lane change intent. Finally, we show several examples of 
design of attention arousal systems, warning systems and systems that 
perform safety control actions in an autonomous manner. 
15.2. Trust as human understanding of machine 
In order to establish design guidelines of intelligent machines for 
attaining appropriate human trust, a theoretical model of trust is necessary. 

Managing the Risks of Automobile Accidents via Human–Machine Collaboration     321 
Most discussion in previous studies on appropriateness of trust focused on 
“calibration”, i.e. the difference between the subjective evaluation of the 
machine capability or reliability and the actual one (see, e.g., [LEE 04] and 
SHE 92]). However, as there exist multiple dimensions of trust, overtrust 
and distrust could have multiple dimensions. 
Regarding the dimensions of trust, Lee and Moray [LEE 92] 
distinguished four dimensions as follows: (1) foundation, which represents 
the fundamental assumption of natural and social order; (2) performance, 
which rests on the expectation of consistent, stable and desirable 
performance or behavior; (3) process, which depends on an understanding  
of the underlying qualities or characteristics that govern behavior; and  
(4) purpose, which rests on the underlying motives or intents. For the  
most engineered systems, the three dimensions such as performance,  
process and purpose are important to discuss because it would be generally 
true that engineered systems are designed to meet the natural and social 
orders. 
To discuss trust and inappropriate trust, such as overtrust and distrust, in 
terms of the three dimensions mentioned above in a precise manner, Itoh 
[ITO 12] developed a model of human trust in an intelligent machine. The 
model takes into account the function of the system, limitation of the 
working conditions for the function and the reliability of the automation 
function within the limitation. Users’ misunderstanding of the function is 
related to overtrust in terms of the purpose dimension. Even if the human 
understands the purpose of the system correctly, the trust could be 
inappropriate in terms of dimensions of process and performance. The 
author believes that there exists some relationship between the trust in terms 
of process and the trust in terms of performance. Figure 15.1 represents a 
schematic model of structure of the human trust in intelligent machine. What 
is important here is that the set of actual successful working conditions of the 
machine could differ from the human’s expectation. Expecting successful 
work of the machine beyond the limitation is one type of overtrust,  
violating the dimension of process. On the other hand, humans’  
complete trust in automation within the limitation of the prescribed  
working conditions may not be overtrust if the reliability of the machine is 
perfect within the conditions. If the human places complete trust at some 
condition s but the machine is not fully reliable at s, his/her trust is 
excessive. 

322     Risk Management in Life-Critical Systems 
Let us show some examples of overtrust. An adaptive cruise control 
(ACC) system of a car works for following a slower forward vehicle, but 
its maximum deceleration rate is limited up to 0.4 G or so. Suppose a  
driver expects that the ACC system can prevent a rear-end crash when the 
necessary deceleration is higher than the maximum deceleration rate of the 
ACC system. It can be regarded that the driver places too much trust in  
the ACC system, and the overtrust here is related to the “process” dimension 
of trust (I categorized this overtrust as “performance” in [ITO 12], but 
currently I think it would be better regarded as “process”). This type of 
overtrust has been observed in several studies (see, e.g. [ITO 08a]). It is 
hypothesized that this type of overtrust is caused by the driver’s direct 
extrapolation of subjective expectation of system capability from the 
previous experience to the non-experienced situations. I call this expansion 
of driver expectation a ripple effect [ITO 12]. If the driver expects that the 
ACC works perfectly when the necessary deceleration is less than 0.4 G, it 
could be regarded as overtrust in terms of performance dimension; this is 
because the machine is never perfect. As for overtrust in terms of purpose, a 
typical example is driver expectation of an ACC system to decelerate against 
a stopped vehicle at the tail end of a traffic jam. In fact, ordinary ACC 
systems ignore stationary objects. This is due to the limitation of the sensing 
system, mainly based on laser-radar technology. Dickie and Boyle [DIC 09] 
showed that many drivers were not familiar with this limitation. Another 
example of overtrust in terms of purpose is misuse of Supplemental Restraint 
System (SRS) airbags [ITO 10a]. In the early 1990s, people relied on the 
SRS airbag at the driving seat as an alternative of the seat belt. However, the 
airbag is a supplement to the seat belt but not the alternative to it. 
 
Figure 15.1. The structure of trust 

Managing the Risks of Automobile Accidents via Human–Machine Collaboration     323 
 
Figure 15.2. Deceleration meter 
One method to reduce the possibility of overtrust as performance is to 
improve the reliability of the machine. However, it is impossible to make a 
100% reliable machine. Thus, it is important to arouse the human attention 
when the machine may fail, especially under specific malfunction-forcing 
conditions, such as rain for ACC systems. Detecting human drowsiness 
caused by boredom could be another key technology for preventing 
overreliance based on overtrust in terms of performance. I will discuss the 
technology in the next section of this chapter. In order to avoid overtrust in 
terms of process, visualizing the functional limitation of the machine would 
be effective. Itoh [ITO 08a] proposed a “deceleration meter” which displays 
the maximum ability of deceleration and the maximum level of actual 
deceleration rate in the latest 10 s (Figure 15.2). This deceleration meter is 
quite simple but effective because the driver can see to what extent the latest 
deceleration was close to the limitation afterward. However, it is quite 
difficult to prevent overtrust in terms of purpose by system design. In many 
cases, such overtrust is not based on the use but on (often inappropriate) 
advertisement provided on TV or other media. 
15.3. Machine understanding of humans 
15.3.1. Drowsiness detection 
It is necessary for ADAS to know whether the human driver is ready to 
respond to a sudden event. In particular, detection of driver drowsiness is an 
important issue to be addressed. From the practical point of view, driver 
monitoring technology should be based on non-invasive and non-intrusive 
measurement techniques. The author and his colleagues have been 

324     Risk Management in Life-Critical Systems 
developing methods to detect driver drowsiness with pressure distribution 
sensors placed on (or in) the driving seat. Figure 15.3 shows the sensors and 
an example of the pressure distribution data. 
 
 
Figure 15.3. a) Pressure distribution sensors and b) the obtained data. For a color version of 
this figure, see www.iste.co.uk/millot/riskmanagement 
In [ITO 10b], the pressure distribution sensors, made by NITTA corp., 
were placed on the seat cushion as well as the backrest of the driving seat. 
Each sensor sheet has 44 × 48 measure points. Itoh et al. [ITO 10b] showed 
that the number of body movements was related to driver drowsiness. The 
body movements were evaluated as the difference between consecutive two 
measured load center positions (LCPs). Let fxy denote the measured value of 
the load at measuring point (x, y). The LCP (LCPx, LCPy) is calculated as 
follows: 
    
,
,
xy
x y
x
xy
x y
xf
LCP
f
=
∑
∑
 
,
,
xy
x y
y
xy
x y
yf
LCP
f
=
∑
∑
   
[15.1] 
When the driver’s body moves, mainly to fight against drowsiness, the 
LCP changes. If the LCP moves more than a certain threshold within two 
consecutive sampling time points, it is regarded as body movement 
occurring. In the experiment by Itoh et al. [ITO 10b], the sampling rate was 
set at 2 Hz, and the threshold value was set at 0.3 in for the seat cushion and 
0.4 in for the backrest. 
Ishikawa et al. [ISH 13] conducted an experiment with a medium-fidelity 
driving simulator in order to analyze the effects of drowsiness on driver 

Managing the Risks of Automobile Accidents via Human–Machine Collaboration     325 
body movements. In this study, five levels of drowsiness were distinguished 
based on the scale of drowsiness proposed by Kitajima et al. [KIT 97]:  
(1) not drowsy: the eye movement is fast, or the time period of blinks is 
stable; (2) slightly drowsy: the eye movement is slow; (3) moderately 
drowsy: blinks are slow, the mouth moves, or the driver touches his/her face; 
(4) significantly drowsy: the number of blinks increases consciously, 
unnecessary motions for driving are found, yawns are frequent, or deep 
breathing is found; and (5) extremely drowsy: eyelids are almost closed, or 
the driver’s head inclines to front or rear. Ishikawa et al. [ISH 13] found that 
the number of body movements increase when the drowsiness level 
increases from 2 to 3 (Figure 15.4). Interestingly, the driving performance, 
such as lateral vehicle stability, has not yet been affected significantly even 
when the drowsiness level changes from 2 to 3. Note that the driving 
performance will degrade when the drowsiness level increases from 3 to 4. 
The body movement monitoring is thus useful to predict appearance of 
drowsiness earlier than other methods based on driving performance. 
 
Figure 15.4. Pressure distribution sensors and the obtained data [ISH 13].  
For a color version of this figure, see www.iste.co.uk/millot/riskmanagement 
15.3.2. Inference of driver intent and detecting distraction 
Even when a driver is not drowsy, his/her attention to driving may not be 
enough if the driver is distracted by something other than driving itself. It is 
important for the machine to understand the human driver intent and to 
evaluate whether the driver is making appropriate preparation for 
implementing the intent. The author and his colleagues have developed a 

326     Risk Management in Life-Critical Systems 
simple method to detect driver lane change intent on the basis of  
eye-movement analysis. Figure 15.5 shows a model of driver lane change 
intent emergence, which is based on the number of “checking behavior”, i.e. 
to observe the side-view mirror in order to check whether the target lane to 
go into is safe enough. When the level of the intent to change lanes is 
evaluated as “high”, it is expected that the driver is going to change lanes 
very soon if it is safe enough. According to Zhou et al. [ZHO 09], there is a 
strong relationship between the level of the intent to change lanes and the 
time-headway (THW), i.e. the distance to the forward vehicle divided by the 
host vehicle’s own speed, to the forward vehicle. Interestingly, the quality of 
“checking behavior” is degraded if the driver is distracted, e.g. the driver’s 
initiation of frequent checking of the side-view mirror is delayed. Thus, it 
would be possible to detect driver distraction if we monitor the “checking 
behavior” when the traffic condition suggests that the driver should want to 
change lanes for, e.g. passing the forward vehicle. 
 
Figure 15.5. Model of driver lane change intent emergence [ZHO 09] 
15.4. Design of attention arousal and warning systems 
15.4.1. Attention arousal for distracted drivers 
Suppose a driver is distracted by other tasks than driving. In that case, a 
warning could be too late to avoid a crash if the warning system is designed 
to issue a warning of late timing and if the forward vehicle decelerates very 
rapidly. An earlier action would be needed in such a situation. Here, a more 
proactive method would be useful. Itoh et al. [ITO 13a] proposed an 
attention arousing system when the forward traffic is uncertain, which 

Managing the Risks of Automobile Accidents via Human–Machine Collaboration     327 
means that it is not obvious what will happen in the near future, and thus it is 
necessary to pay attention to that. Here, the forward traffic has not become 
too risky, and the quick response is not necessary at that moment. Thus, 
paying attention to the forward traffic is enough. 
Ito et al. [ITO 13a] conducted an experiment under distracting driving 
contexts with a high-fidelity driving simulator in the Japan Automobile 
Research Institute (JARI). An attention arousing information is given as a 
visual message in the navigation display (see Figure 15.6(a)) with an 
auditory alert. When such an attention arousing system was available 
(Attention Arousing (AA) mode), the THW became greater compared to the 
one under no attention arousing system that was available (No Attention 
arousing  (NA) mode), as shown in Figure 15.6(b). The result suggests that 
the attention arousing system was effective for the drivers to become 
attentive to the forward traffic. The experimental results also suggest that 
this attention arousing is effective when the driver is distracted, but not when 
the driver is attentive to driving. It is thus important for the system to 
understand whether the driver is distracted or not. Inhibiting logic of issuing 
an attention arousing would be needed when the driver is not distracted. 
 
Figure 15.6. a) The attention arousing display and b) its effects on THW [ITO 13a].  
For a color version of this figure, see www.iste.co.uk/millot/riskmanagement 
15.4.2. Individual adaptation of a rear-end collision warning system for 
reducing the possibility of driver overreliance 
Suppose a driver is not distracted by tasks other than driving. Even in that 
case, the risk of a rear-end collision could become high by rapid deceleration 
of the forward vehicle. It would be necessary to warn the driver to perform 

328     Risk Management in Life-Critical Systems 
avoiding maneuver in such a situation if the driver is late in recognizing the 
deceleration of the forward vehicle. It is well known that the design of a 
rear-end collision warning system is a difficult issue to be solved. If a 
warning is given at an early timing successfully, the driver may become 
overreliant on the warning system [ITO 08b]. Itoh and Inagaki [ITO 08b] 
compared the driver’s reaction to the forward vehicle deceleration between 
the no warning (NW) condition and conditions where a warning system was 
available. When drivers used a warning system based on the stopping 
distance algorithm (SDA) [WIL 97], which tends to issue a warning at an 
early timing, the reaction was very early. Definitively speaking, the value of 
the inverse of time to collision (1/TTC) was small compared to NW 
condition (see Figures 15.7(a) and (c)). The result means that the drivers 
responded to the warning instead of the deceleration of the forward vehicle. 
On the other hand, suppose the drivers use a warning system based on their 
brake timing, where a warning is issued when the current value of 1/TTC 
becomes greater than or equal to the median of their past data. This 
condition is called driver brake timing (DBT). In this case, the driver’s 
reaction was as shown in Figure 15.7(b), which means that the reactions 
were almost the same as the NW condition. However, there was a difference 
between Figures 15.7(a) and (b) in the sense that the 95 percentile value at 
the 0.6 G deceleration of the forward vehicle was smaller under DBT 
condition than under NW condition. This result suggests that the DBT 
warning system did not change the driver attitude, i.e. the drivers judged the 
rear-end collision risk and determined when and how to react to the 
deceleration of the forward vehicle themselves. Still, the DBT warning 
system was effective for reducing the rear-end collision risk when the 
driver’s reaction to the forward vehicle deceleration was so late. The results 
of the experiment suggest the effectiveness of adaptation of a rear-end 
collision warning system to individual. 
 
Figure 15.7. Driver reaction against the rapid deceleration of the forward vehicle [ITO 08b] 

Managing the Risks of Automobile Accidents via Human–Machine Collaboration     329 
15.5. Trading of authority for control from the driver to the machine 
under time-critical situations 
Issuing a warning message under an emergency situation is not enough to 
prevent crashes perfectly. According to the theory of adaptive automation 
(see, e.g., [INA 03]), there exist cases where the machine may be given the 
authority to make a decision on a safety control action, and to implement the 
action in an autonomous manner. However, it should proceed with care. Let 
us discuss this issue in a detailed manner. 
Inagaki and Itoh [INA 13] pointed out that two types of situations should 
be distinguished for the trading of control authority from the human driver to 
ADAS: (1) the ADAS has not detected the driver’s action which is necessary 
to avoid a crash under the given traffic condition and (2) the ADAS has 
detected an action that must be prohibited under the given traffic condition. 
In both types, a crash may occur soon if the ADAS does not intervene into 
control. 
The control authority may be given to the machine in a type 1 situation so 
that it can take an automatic safety control action that the driver failed to 
perform. For example, Itoh et al. [ITO 13b] showed that it was effective to 
attain safety and acceptable by drivers that the machine performed a 
collision prevention maneuver in an autonomous manner when a pedestrian 
suddenly appeared just in front of the vehicle. Another example for type 1 is 
the advanced emergency brake system (AEBS) with the capability of 
collision avoidance. Many car manufacturers have put into market vehicles 
equipped with such AEBS, and they seem to be accepted by the society and 
thus they are spreading rapidly all over the world. 
It is also possible for the machine to be given the authority for type 2 
situations so that it can take a protective action that tries to prevent the 
driver’s inappropriate action causing an accident or an incident. A typical 
example of such a preventive system is for collision avoidance with a 
vehicle in the blind spot when the host vehicle is going to change lanes 
(Figure 15.8). 
However, theories or methodologies to design such protective systems 
have not been established yet. In the aviation domain, today’s aircraft with 
fly-by-wire technologies could be designed to limit the flight envelope by 
making it impossible for the pilot to exceed certain boundaries. This type of 

330     Risk Management in Life-Critical Systems 
protection is called hard limit or hard protection [BIL 97]. Inagaki et al. 
[INA 07] pointed out that machine-initiated trading of authority could cause 
automation-surprises and distrust in the automation. Instead, soft protection, 
which provides precisely tempered degradation of operation qualities as safe 
operating limits are approached, could be more acceptable to humans  
[BIL 97]. However, the soft protection can be overridden by the human, 
which means that the soft protection itself is not powerful enough to prevent 
a crash. In fact, Itoh and Inagaki [ITO 14] (under review) showed that the 
safety effectiveness of soft protection for preventing collisions with a vehicle 
in the blind spot was almost the same as that of a warning system. 
 
Figure 15.8. A situation machine protective action is needed. In this example, the left lane is 
the cruising lane and the right lane is the passing lane. The vehicle in the right lane is in the 
blind spot of the side-view mirror of the host vehicle 
Whether to choose hard protection or soft protection may not simply be a 
design philosophy issue. Multiobjective decision-making may be necessary, 
taking into account the system’s effectiveness for safety and human 
acceptance. In fact, one aircraft may have both a hard protection function in 
a subsystem and a soft protection function in another subsystem [BIL 97]. 
Young et al. [YOU 07] discussed applying the hard/soft protection notion to 
the design of driver assistance systems, but it is still unclear how to design 
such a protective system. 
15.6. Conclusions 
This chapter discussed ways of human–machine collaboration via mutual 
understanding. Design for human appropriate trust in ADAS is an important 
aspect of systems design. In this chapter, I showed basic ideas on preventing 
overtrust with examples, but further research is needed to establish ways for 
overtrust prevention. Another approach for this is related to the approach  
 

Managing the Risks of Automobile Accidents via Human–Machine Collaboration     331 
known as “horse-metaphor” which orients continuous haptic-shared control 
instead of intermittent trading of authority between the human and the 
ADAS [FLE 03]. If mutual trust has been established between the human 
driver and the ADAS, even the hard protection could be accepted by the 
driver. This is only a hypothesis at this moment, but it is worth doing 
research to test the hypothesis. 
In any case, driver monitoring techniques would play an important role 
for reliable human–machine collaboration. The use of pressure distribution 
sensors on the driving seat is, I believe, a good way for the driver 
monitoring, as this type of sensors could be used for multiple objectives, 
including not only drowsiness and fatigue detection but also distraction 
detection and evaluation of driving posture appropriateness [ITO 08c]. 
Integrating those elementary techniques would be needed as a further study. 
15.7. Bibliography 
[BIL 97] BILLINGS C.E., Aviation Automation – The Search for a Human-Centered 
Approach, Lawrence Erlbaum Associates, Mahwah, NJ, 1997. 
[CON 68] CONVENTION ON ROAD TRAFFIC, 1993 version & amendments in 2006, 
1968. Available at http://www.unece.org/trans/conventn/crt1968e.pdf. 
[DIC 09] DICKIE D.A., BOYLE L.N., “Driver’s understanding of adaptive cruise 
control limitations”, Proceedings of the Human Factors and Ergonomics Society 
53rd Annual Meeting, San Antonio, TX, pp. 1806–1810, 2009. 
[FLE 03] FLEMISCH F., ADAMS C.A., CONWAY S.R., et al., The H-metaphor as a 
Guideline for Vehicle Automation and Interaction, NASA/TM-2003-212672, 
2003. 
[INA 98] INAGAKI T., MORAY N., ITOH M., “Trust, self-confidence and authority in 
human-machine systems”, Proceedings of the IFAC Man-Machine Systems,  
pp. 431–436, 1998. 
[INA 03] INAGAKI T., “Adaptive automation: sharing and trading of control”, in 
HOLLNAGEL E., (ed.), Handbook of Cognitive Task Design, Lawrence Erlbaum 
Associates, Mahwah, NJ, pp. 147–170, 2003. 
[INA 07] INAGAKI T., ITOH M., NAGAI Y., “Support by warning or by action: which 
is appropriate under mismatches between driver intent and traffic conditions?”, 
IEICE Transactions on Fundamentals of Electronics, Communications and 
Computer Sciences, vol. E90-A, no. 11, pp. 264–272, 2007. 

332     Risk Management in Life-Critical Systems 
[INA 13] INAGAKI T., ITOH M., “Human’s overtrust in and overreliance on advanced 
driver assistance systems: a theoretical framework”, International Journal of 
Vehicular Technology, vol. 13, p. 8, 2013, Available at: http://www.hindawi.com/ 
journals/ijvt/2013/951762/. 
[ISH 13] ISHIKAWA R., ITOH M., INAGAKI T., “Two-step detection of driver’s 
drowsiness via time series analysis”, Presented at 40th Intelligent Systems 
Symposium, SICE, 2013.  
[ITO 08a] ITOH M., “Deceleration meter: a management tool for reducing over-
reliance in collision warning when using adaptive cruise control system”, 
Proceedings of PSAM9, p. 7, 2008. 
[ITO 08b] ITOH M., INAGAKI T., “Dependence of driver’s brake timing on rear-end 
collision warning logics”, Proceedings of the 8th International Conference on 
Advanced Vehicle Control (AVEC), pp. 596–601, 2008. 
[ITO 08c] ITOH M., “Real time inference of driver’s intent via analyses of pressure 
distribution on the seat”, Proceedings of the 4th International Congress on 
Embedded Real Time Software, p. 7, 2008. 
[ITO 10a] ITOH M., “Necessity of supporting situation awareness to prevent over-
trust in automation”, International Electric Journal of Nuclear Safety and 
Simulation, vol. 1, no. 2, pp. 150–157, 2010. 
[ITO 10b] ITOH M., IITSUKA K., INAGAKI T., “Effects of drowsiness on load center 
position of the pressure distribution on driving seat”, Journal of the Japanese 
Council of Traffic Science, vol. 10, no. 1, pp. 3–10, 2010. 
[ITO 12] ITOH M., “Toward overtrust-free advanced driver assistance systems”, 
Cognition, Technology and Work, vol. 14, pp. 51–60, 2012. 
[ITO 13a] ITOH M., ABE G., YAMAMURA T., “Effects of arousing attention on 
distracted driver’s following behaviour under uncertainty”, Cognition, 
Technology & Work, vol. 16, no. 2, pp. 271–280, 2013. 
[ITO 13b] ITOH M., HORIKOME T., INAGAKI T., “Effectiveness and driver 
acceptance of a semi-autonomous forward obstacle collision avoidance system”, 
Applied Ergonomics, vol. 44, pp. 756–763, 2013.  
[ITO 14] ITOH M., INAGAKI T., “Design and evaluation of steering protection for 
avoiding collisions during a lane-change”, Ergonomics, vol. 57, no. 3, pp. 361–
373, 2014. 

Managing the Risks of Automobile Accidents via Human–Machine Collaboration     333 
 [KIT 97] KITAJIMA H., NUMATA N., YAMAMOTO K., et al., “Prediction of 
automobile driver sleepiness (1st Report, Rating of sleepiness based on facial 
expression and examination of effective predictor indexes of sleepiness)”, 
Journal of the Japan Society of Mechanical Engineers, vol. 63, no. 613,  
pp. 3059–3066, 1997.  
[LEE 92] LEE J.D., MORAY N., “Trust, control strategies and allocation of function 
in human-machine systems”, Ergonomics, vol. 3, no. 10, pp. 1243–1270, 1992. 
[LEE 04] LEE J.D., SEE K.A., “Trust in automation: designing for appropriate 
reliance”, Human Factors, vol. 46, no. 1, pp. 50–80, 2004. 
[SHE 92] SHERIDAN T.B., Telerobotics, Automation, and Human Supervisory 
Control, MIT Press, Cambridge, MA, 1992. 
[WIL 97] WILSON T.B., BUTLER W., MCGEHEE D.V., et al., Forward-looking 
collision warning system performance guidelines, SAE technical paper, 970456, 
1997. 
[WOO 89] WOODS D.D., “The effects of automation on human’s role: experience 
from non-aviation indutries”, in NORMAN S., ORLADY H. (eds.), Flight Deck 
Automation: Promises and Realities, NASA CR-10036, pp. 61–85, 1989. 
[YOU 07] YOUNG M.S., STANTON N.A., HARRIS D., “Driving automation: learning 
from aviation about design philosophies”, International Journal of Vehicle 
Design, vol. 45, no. 3, pp. 323–338, 2007. 
[ZHO 09] ZHOU H., ITOH M., INAGAKI T., “Eye movement-based inference of truck 
driver’s intent of changing lanes”, SICE Journal of Control, Measurement & 
System Integration, vol. 2, no. 5, pp. 291–298, 2009. 
 
 

 
 

16 
Human–Machine Interaction in Automated 
Vehicles: The ABV Project 
16.1. Introduction  
Advanced driver assistance systems (ADAS) have been studied since the 
early 1990s, and now they are largely established in automobiles. The goal 
of these systems is to improve the controllability of the vehicle, warn the 
driver well in advance of a situation where he must intervene and make the 
vehicle capable of providing the driver considerable aid to navigation as well 
as the task of lane keeping (automatic emergency braking (AEB), lane 
keeping system (LKS), etc.).  
The human–machine cooperation is a challenging problem since the 
introduction of automated systems in the various fields of human activity, 
especially in the aviation field [PAR 00, HOC 01]. In road vehicle driving, 
this problem is relatively recent, its appearance follows the initial works on 
driver assistance in the late 1980s (Path in USA and Prometheus in Europe). 
Thus, from the 1990s, considerable work has been carried out to address this 
issue [NAG 95]. 
According to [PIA 77]: “Cooperate in action is to operate in common, 
that is to say, adjust with new operations the operations performed by each 
partner. It is the coordinated operations of each partner in a single operating  
 
                         
Chapter written by Chouki SENTOUH and Jean Christophe POPIEUL. 

336     Risk Management in Life-Critical Systems 
system in which the acts themselves of collaboration constitute the integral 
operations”. This leads us to the following questions [HOC 01]: 
– when to intervene to assist the driver; 
– how to do it and to what degree; 
– what the effect of the intervention on the driver will be; 
– finally, to whom the responsibility for the driving is assigned. 
Sheridan [SHE 92] gives the definition of “sharing control” where the 
human operator and machine work together, simultaneously, to make or 
perform a task. He also defines “trading control” as an alternate control 
where one of the two agents is responsible for a function, and either the 
human operator or the machine performs the function from time to time  
(a change of active agent). 
Many experiments were carried out to the general public on the full 
automation of driving, including presentations of the Google driverless car 
[MAR 10], and VisLab driverless car [PRO 13]. Most of these experiments 
aim at full automation, where the driver is completely out of the driving task. 
However, future generations of driver assistance systems must be developed 
to ensure a smooth action of the controller continuously while keeping the 
driver in the loop without generating negative interference [HOC 06,  
BAI 83]. This was particularly highlighted in national and international 
projects as Partage (2009–2013) [PRO 12] and HAVEit (2008–2011)  
[HAV 11]. These projects and academic research cited above have 
demonstrated the need to integrate, in the design process of the system, the 
problem of interaction with the driver by resolving problems of task sharing 
and degree of freedom, authority, level of automation and human–machine 
interface (HMI). 
The ABV (French acronym for low speed automation) project has 
focused on the interaction between human and machine with a continuous 
sharing of driving, considering the acceptability of the assistance and driver 
distractions and drowsiness [BOV 08]. The main motivation of this project is 
the fact that in many situations, the driver is required to drive his vehicle at a 
speed lower than 50 km/h (speed limit in urban areas) or in the case of a 
traffic congestion due to traffic jams, in the surrounding of big cities, for 
example. 

Human–Machine Interaction in Automated Vehicles: The ABV Project     337 
We present here the specification of cooperation principles between the 
driver and the assistance system for lane keeping developed in the 
framework of the ABV project. Within this project, the LAMIH managed in 
partnership with Continental Automotive the task of “human–machine 
cooperation (HMC) and driver monitoring (DM)” which was intended to 
define, prototype and evaluate the interactions between the ABV system and 
the human driver. 
16.2. The ABV project 
16.2.1. Objectives 
Some assistance systems act during critical driving situation to correct the 
vehicle trajectory (e.g. Electronic Stability Program (ESP)), such as 
solutions developed by car manufacturers in terms of active safety. Other 
driver assistance systems operate mostly upstream such as, for example, 
continuous shared control mode where the controller provides a partial 
steering control action. 
The ABV project aimed the design of an automated vehicle at low speed 
while ensuring the sharing of driving with the human driver. Thus, the 
project has addressed the problem to integrate the lateral and longitudinal 
control functions of the vehicle considering the driver-in-the-loop. Task 4 of 
the ABV project (see Figure 16.1) addressed the cooperation between the 
human driver and the assistance system in a perspective of shared control 
between driver and automation, considering the distraction, inattentiveness 
and fatigue of the driver. 
The ABV project is a research project funded by the National Research 
Agency. It associated six academic labs1 (IFSTTAR, IBISC, IEF, INRIA, 
LAMIH 
and 
MIPS) 
and 
four 
industrial 
players2 
(Continental,  
Viametris, Induct and VERI) and GMConseil has attended the IFSTTAR to  
 
                         
1 IFSTTAR: www.ifsttar.fr/   IBISC: https://www.ibisc.univ-evry.fr/IEF: www.ief.u-psud.fr/    
INRIA: www.inria.fr/MIPS: www.mips.uha.frLAMIH: www.univ-valenciennes.fr/LAMIH/ 
presentation-lamih. 
2 Continental: www.conti-online.com/ Viametris : www.viametris.fr/ Induct: induct-
technology.com/ 
VERI: 
http://www.veolia.com/fr/innovation/recherche-innovation/GM 
Conseil:  http://www.gmconseil.net/. 

338     Risk Management in Life-Critical Systems 
study the legal aspects. The project began in October 2009 for a period of 42 
months. 
16.2.2. Structure of the ABV project 
The ABV project is divided into nine tasks articulated as we can see in 
Figure 16.1.  
Tasks 1 to 4, which are the perception of the environment, the path 
planning, the vehicle control and the interaction with the driver, aimed at 
scientific developments of automation. 
Tasks 5 to 7 dealt with the integration in vehicles and validation. Tasks 8 
to 9 addressed the problem of the societal impact of systems developed in 
the framework of the ABV project. 
We describe the work carried out under the task 4 of ABV “human–
machine cooperation and driver monitoring”. The problem addressed in this 
task is related to the HMC and more specifically the dynamic allocation of 
tasks. The objective is to determine whom to assign at each time any part of 
the vehicle control according to the driving situation (operating range), 
wishes of the driver but also criteria related to his capacities (DM)  
and naturally based on criteria related to capacities of the machine (level of 
automation). This task required the development of specific HMIs. 
 
Figure 16.1. Structure of the ABV project 

Human–Machine Interaction in Automated Vehicles: The ABV Project     339 
16.3. Specifications of the human–machine cooperation 
This section details the specifications of the cooperation principles 
between the driver and the ABV system. 
16.3.1. Operating modes of the ABV system 
The state graph of Figure 16.2 shows the different modes of the ABV 
system and the conditions which allow us to change from one mode to 
another. So, this figure describes all the features available and how to pass 
from one mode to another depending on the situations encountered and the 
actions of the driver. Therefore, this graph first defines the level of 
automation of the overall human machine system; then it defines the 
management authority between driver and controllers: who is in charge of 
modifying the level of automation. Conditions of mode change depend on 
different factors: technical factors such as controller competencies, driving 
situations and human factors such as loss of vigilance or attention. 
Except when the system is off, four main modes have been defined 
(yellow bubbles in Figure 16.2): 
– in the first mode, the driver performs alone the driving task: this is the 
manual mode. Nevertheless, in this mode the DM is active and can produce 
an alert; 
– in the second mode called shared control on U trajectories (SCOUT), 
the controller performs alone the overall driving task: lateral and 
longitudinal control. This mode is available when the vehicle speed is lower 
than 50 km/h and only on secured path: all the information needed to run the 
ABV system is, in this case, available for this route (Global Positioning 
System (GPS) cartography, road marking, etc.). This implies that locations 
where a secured path begins and ends are known, as well as areas where 
emergency lanes are available. In this mode, the vehicle is autonomous but 
the human driver can intervene at any time on the wheel: the driving task is 
then shared with the controller; 
– in the third mode called advanced speed and interdistance control 
(ASIC), the controller performs only the longitudinal task: the controller 
operates in both speed control and regulation of interdistance from the 
vehicle ahead in accordance with driver instructions and rules of the  
 

340     Risk Management in Life-Critical Systems 
highway code. This mode can be used not only on normal roads, but also on 
secured paths. In this case, it allows a reduction in the fuel consumption 
based on a more accurate knowledge of traffic and the geometry of the path 
ahead; 
– the last mode is an emergency shutdown (Arrêt d’Urgence (AU)) which 
preserves the safety of the vehicle and its passengers, by stopping it in an 
automatic manner. 
In each of these four modes, different submodes have been defined. The 
transitions from one submode to another are represented by arcs with a 
transition and the corresponding receptivity (see Figure 16.2). The latter 
represents a Boolean equation having various elements such as an action on 
the interface (e.g. BPSCOUT), a parameter related to the vehicle (e.g. speed), a 
parameter related to the situation (e.g. end secured path), a parameter related 
to the driver (e.g. drowsiness) or finally a controller state (e.g. system 
SCOUT OK). 
16.3.2. ABV system HMI 
The HMI will allow the driver to monitor the ABV system. This 
supervision, and therefore the activities of monitoring and control which 
result from this, must in particular enable the driver to maintain his 
awareness of the operating mode. Indeed, if the driver can modify the level 
of automation of the human–machine system (driver-ABV system) by 
delegating a part or the totality of the driving activity to the ABV system. 
This system also has the possibility to give back to the driver that which has 
been initially delegated. Thus, it is absolutely necessary that the driver 
knows at any moment “who is in charge of what”. 
During the use of a particular operating mode of ABV, it is also 
necessary that the driver, on the one hand, “understands” what the system 
does and, on the other hand, can also monitor it. The work carried out led us 
to design the interfaces to meet these major requirements, but also to take 
into account the characteristics of the driving activity which can require us to 
communicate with the system while ensuring the visual monitoring of the 
traffic and infrastructure. This HMI uses three different interaction 
modalities: sound, visual and haptic. 
 

Human–Machine Interaction in Automated Vehicles: The ABV Project     341 
 
Figure 16.2. Graph of the different modes of the ABV system 
The HMI of the ABV system is composed of various elements: 
– a touch screen that allows the driver to activate the different operating 
modes and which provides information feedback; 
– the steering wheel, equipped with a torque sensor, is an essential part of 
the ABV HMI. It allows the driver “to feel” the operation of the system (e.g. 
when keeping lane center) while allowing the lane changes or the obstacle 
avoidances via a haptic communication; 
– the “haptic” accelerator pedal can also provide feedback information to 
the driver about the management of the vehicle speed; 
– the sound feedback generator. 

342     Risk Management in Life-Critical Systems 
 
Figure 16.3. Graph of the different modes of the ABV system. For a  
color version of this figure, see www.iste.co.uk/millot/riskmanagement 
From the point of view of the driver, the ABV system manages two 
“components” of the trajectory: the speed and the position of the vehicle 
with respect to the infrastructure and the other vehicles. The interface must 
give information related to these two “components”. The choice that was 
made to share the interface between two main information areas  
(Figure 16.3): 
– a left upper part that displays information about the control of the 
vehicle speed is a conventional tachometer to which were associated 
symbols/codes of colors bringing additional information like the activity of 
the force feedback accelerator pedal, the speed limit of the road section, the 
fact that speed respects the constraints of the SCOUT mode when this one is 
engaged, etc. The left lower part of this area is restricted for the text 
messages; 
– a right part displaying information relating to the lateral control and 
management of the interdistance. The selected design represents the position 
of the vehicle in pseudo 3D, taking its inspiration in the interfaces currently 
used on adaptive cruise control (ACC) or navigation systems. 
16.3.3. Driver monitoring 
The DM (see Figure 16.4) is used in the different operating modes: 
– in ABV SCOUT mode: to verify that the driver is currently watching to 
the road, even if he is not actively involved in the driving activity. In the 
case where the state of the DM is not OK, the alarm process is activated; 

Human–Machine Interaction in Automated Vehicles: The ABV Project     343 
– in ABV ASIC mode: to verify that the driver is currently watching the 
road. The lane keeping performance is also determined. In the case where 
the state of the DM is OK but the lane-keeping performance is not OK, the 
alarm process is activated; 
– when changing the operating mode (e.g. from automatic to manual): to 
verify that the driver is able to regain control of the vehicle before 
disconnecting the automatic mode. In the event that the driver does not react 
to the alarm process, the emergency stop procedure (AU) is activated. 
In the first two cases, the DM is equivalent to a “dead-man” security 
system. In the latter case, the DM may be coupled with a system to detect 
that the driver has at least one hand on the steering wheel. 
 
Figure 16.4. Driver monitoring system from Continental. For a color  
version of this figure, see www.iste.co.uk/millot/riskmanagement 
16.4. Cooperation realization 
This part deals with the identification of all the problems which could 
affect the safety or the correct operation of the system, especially with the 
level of the interaction with the driver and the DM in the transient stages 
between operating modes. 
16.4.1. Mechanisms for operating mode switching 
This part details the mechanisms of switching between the operating 
modes of the ABV system. Switching can be at the initiative of the driver or 
the system itself, according to various types of situation. These mechanisms 
integrate in particular the use of the DM which insures that the driver is able 
to again take control of the vehicle when a mandatory switching from an 

344     Risk Management in Life-Critical Systems 
automatic mode to the manual mode is required, or to make sure that the 
driver is not drowsy and he is aware about the situation. The various 
parameters used as conditions for transition between states are presented 
below: 
– pushbuttons that allow setting the ABV system on and off, switching 
manually from one operating mode to another and acknowledging an alarm 
triggered by an hypovigilance or distraction detection coming from the DM 
and ends the emergency “awaking” procedure; 
– four pieces of information coming from the DM are used to manage the 
various operating modes. Information driver state hypovigilance (DSH) 
indicates either that there is a problem of vigilance of driver (DSH KO), or 
not (DSH OK). This information must be validated by driver state 
hypovigilance valid (DSHV) (DSHV OK). If information is not valid 
(DSHV KO), DSH cannot be taken into account. Information driver state 
distraction (DSD) indicates that the driver is inattentive (DSD KO) or not 
(DSD OK). As above, this information must be validated by information 
driver state distraction valid (DSDV); 
– flagsSSASIC, SSSCOUT and SSEMERG indicate that the system runs 
correctly (OK) or not (KO). The first two indicate that the corresponding 
operating modes run correctly, whereas the third indicates that a trajectory 
leading to the emergency lane is available to carry out an emergency stop. It 
should be noted that if the ASIC mode is not available, modes SCOUT and 
emergency stop are not available either, mode ASIC being a subset of the 
two others. In the same way, if mode SCOUT is not available, the 
emergency stop mode is not available either. 
– flag scout deactivation (DSC) by torque indicates that the driver applied 
an important torque on the steering wheel that exceeds the limits set by the 
system (depending on the situation). In this case, the automatic driving mode 
is deactivated. In a same way, the steering wheel angular speed denotes a 
very fast action of the driver when it crosses an experimentally fixed 
threshold. In this case, the operating mode is also modified; 
– information “brake position” and “accel position” indicate that the 
driver respectively operates the acceleration and footbrake pedals. For the 
brake, exceeding a given threshold causes the deactivation of the activated 
automatic mode (SCOUT or ASIC). For the accelerator, the driver must 
exceed a hard point programmed in the active pedal so that the deactivation 
occurs. 

Human–Machine Interaction in Automated Vehicles: The ABV Project     345 
16.4.2. Shared control architecture 
As already mentioned above, in SCOUT mode the driving assistance is in 
charge of the lateral and longitudinal control but the driver can intervene at 
any time on the wheel. The driving task is currently shared between the 
assistance system and the driver. Figure 16.5 summarizes the architecture of 
the shared lateral control. 
 
Figure 16.5. Shared driving control architecture 
Two levels of cooperation have been identified to ensure better sharing of 
control: 
– a low-level of cooperation (LLC) which occurs at the operational 
activity level. LLC is concerned with cooperation in action, where the driver 
interacts with the system, directly on the steering wheel. This level of 
cooperation includes the detection and resolution of interferences in order to 
avoid any conflict. The automatic control acting on the steering system can 
be seen as a disturbance by the driver as well as the effect of the control 
actions of the driver by the assistance. Therefore, these elements are taken 
into account in the controller design to minimize negative interactions 
(conflict) between the assistance and the driver. We will consider later that a 
conflict is characterized by reverse torques applied simultaneously on the 
steering wheel by the driver and the assistance; 
– a high level of cooperation (HLC) which occurs at a strategic activity 
level for planning cooperation. This level of cooperation is more concerned 

346     Risk Management in Life-Critical Systems 
with the choice of paths to follow, taking into account the state of the driver 
(steering torque and information from DM) and the current driving 
environment situation. 
16.5. Results 
Here we present only the results of the work aiming to provide a solution 
to conflicts at LLC level that can be generated when the driver and controller 
act together (at the same time) on the steering wheel for lateral control. This 
is the case, for example, for a driver that optimizes the path when negotiating 
a curve or deviates from the planned path to avoid an obstacle that would not 
have been detected by the perception system. 
We consider, in this case, only one valid path proposed by the path 
planning unit. To allow the sharing of control between the driver and the 
controller directly on the steering wheel, the steering torque control was 
privileged according to [SHI 02]. An approach based on optimal control 
theory incorporating a driver model in the design process of the controller, 
proposed in [SEN 10, SOU 14, SEN 13], has been used. The idea is to 
integrate the driver torque in the state vector of the system in order to take 
into account, in the performance vector, the conflict between the driver and 
the controller. The controller will have the task of assisting the driver in 
performing the lane keeping task while minimizing negative interferences 
(minimization of conflict). 
Results of experimental tests performed on the Sherpa simulator (interactive 
simulation) present a comparison of two controllers: a first controller 
synthesized by integrating a driver model With a Driver Model (WDM) and 
another synthesized using Only a Vehicle Model (OVM). The test scenario is a 
shared driving in the first curve (t < 10s, see Figure 16.6), then the driver 
releases the steering wheel and the vehicle control becomes fully automatic. The 
driver must regain control on the steering wheel without deactivating the system 
to avoid three obstacles in the way. 
Figure 16.6 shows the results achieved on the Sherpa simulator using 
both WDM and OVM controllers as developed in [SEN 13]. This figure 
illustrates the contribution of the driver model in the design of shared lateral 
control. This has reduced the effort provided by the driver to perform his 
obstacle avoidance maneuver while remaining in automatic mode. 

Human–Machine Interaction in Automated Vehicles: The ABV Project     347 
 
Figure 16.6. Experimental results on the SHERPA simulator. For a  
color version of this figure, see www.iste.co.uk/millot/riskmanagement 
To assess the quality of control sharing, an indicator of the physical 
workload associated to the steering effort, taking into account the efforts 
provided by the driver and the assistance and the steering rate, is used : 
0
( )
( ) ( )
T
s
d
a
W
T t T t
t dt
δ
= ∫
 
[16.1] 
where 
 represents the driver torque, 
 is the assistance torque and 
is 
the steering rate. We define the positive and negative interference at the 
steering wheel, respectively, as 
and 
 when the product 
 
is positive or negative. The ratio of the steering workload is computed by 
. The evaluation results of the steering workload 
obtained 
using WDM and OVM controllers are shown in Figure 16.7. 
dT
aT
δ
s
W +
s
W −
( ).
( ). ( )
d
a
T t T t
t
δ
/
w
s
s
R
W
W
−
+
= −
s
W

348     Risk Management in Life-Critical Systems 
Figure 16.7 shows that the integration of a driver model in the design 
process of the assistance (WDM controller) can significantly reduce the 
negative interference (9 N2.m2.rad), in comparison with the OVM controller. 
The comparison of the steering workload ratio for both controllers is shown 
in Figure 16.7(b). The lowest ratio Rw = 0.7 is obtained using the WDM 
controller. The controller designed without driver model (OVM) generates 
more negative interference (20 N2.m2.rad) than positive (15 N2.m2.rad) with 
a ratio of 1.3. 
 
Figure 16.7. Evaluation of the sharing quality. For a color version of  
this figure, see www.iste.co.uk/millot/riskmanagement 
16.6. Conclusion 
In this chapter, we presented technical specifications of cooperation 
principles between human driver and ADAS in the framework of ABV 
project. This controller cooperates with the driver in order to keep the 
vehicle in the lane. In this context, three nominal modes have been defined, 
as well as an emergency shutdown mode. The criteria and the procedures for 
changing these operating modes were also presented. We also described the 
architecture used to achieve the lateral shared control between driver and the 
controller used in the SCOUT mode. The results of tests performed in order 
to develop the controller are conclusive. 
The tests must now be pursued in order to take into account more 
situations in which cooperation issues and/or conflict between human driver 
and technical controller appears. These situations will be implemented 
through interactive simulation on Sherpa, a full-scale driving simulator. The 
experiments must also involve a larger number of subjects to validate the 
solutions in terms of their use [POP 12]. 

Human–Machine Interaction in Automated Vehicles: The ABV Project     349 
Nevertheless, cooperation in planning should enable a better interaction 
between the driver and the controller in the shared mode. Further work on 
the shared control will take into account the interactions with the driver in 
the task of planning trajectory, so at the strategic level of driving activity. 
This level of cooperation will allow us to change the choice of lane to 
follow, by taking into account the action and the state of the driver (steering 
torque and information from DM) and the state of the driving environment 
(other vehicles, for instance). 
16.7. Bibliography 
[BAI 83] BAINBRIDGE L., “Ironies of automation”, Automatica, vol. 19, no. 6,  
pp. 775–779, 1983. 
[BOV 08] BOVERIE S., GIRALT A., “Driver vigilance diagnostic based on eye lid 
movement observation”, IFAC World Congress, Seoul, Korea, July 2008.  
[HAV 11] HAVEIT, European Project FP7, The future of driving, Deliverable 
D61.1 Final Report, September 2011. 
[HOC 01] HOC J.M., “Towards a cognitive approach to human–machine cooperation 
in dynamic situations”, International Journal of Human–Computer Studies,  
vol. 54, no. 4, pp. 509–540, April 2001. 
[HOC 06] HOC J.M., MARS F., MILLEVILLE I., et al., “Human–machine cooperation 
in car driving for lateral safety: delegation and mutual control”, Le Travail 
Humain, vol. 69, no. 2, pp. 153–182, 2006. 
[MAR 10] MARKOFF J., “Google Cars Drive Themselves, in Traffic”, The New York 
Times, 2010. 
[NAG 95] NAGEL H.H., ENKELMANN W., STRUCK G., “FHG-Codriver: from map 
guided automatic driving by machine vision to a cooperative driver support”, 
Mathematical and Computer Modelling, vol. 22, nos. 4–7, pp. 185–212, 1995. 
[PAR 00] PARASURAMAN R., SHERIDAN T.B., WICKENS C.D., “A model for types 
and levels of human interaction with automation”, IEEE Transactions on 
Systems, Man, and Cybernetics part A: Systems and Humans, vol. 30, no. 3,  
pp. 286–297, May 2000. 
[PIA 77] PIAGET J., Études Sociologiques, 3rd. ed., Droz, Geneva, 1977. 
[PRO 12] PROJECT ANR-08-VTT-012-01, Contrôle partagé entre conducteur et 
assistance à la conduite automobile pour une trajectoire sécurisée, Synthesis 
report, June 2012. 

350     Risk Management in Life-Critical Systems 
[PRO 13] PROUD CAR TEST 2013, available at http://vislab.it/vislab-events-2/, 
2013. 
[POP 12] POPIEUL J.-C., SIMON P., LOSLEVER P., et al., “Multivariate analysis of 
human behavior data using fuzzy windowing. Example with driver-car-
environment system”, Engineering Applications of Artificial Intelligence,  
vol. 25, no. 5, pp. 989–996, 2012. 
[SEN 10] SENTOUH C., DEBERNARD S., POPIEUL J.C., et al., “Toward a shared lateral 
control 
between 
driver 
and 
steering 
assist 
controller”, 
The 
11thIFAC/IFIP/IFORS/IEA Symposium on Analysis, Design, and Evaluation of 
Human-Machine Systems, Valenciennes, 31 August–3 September, 2010. 
[SEN 13] SENTOUH C., SOUALMI B., POPIEUL J.C., et al., “Cooperative steering 
assist control system”, Proceedings of the IEEE International Conference on 
Systems, Man, and Cybernetics, Manchester, UK, 13–16 October 2013. 
[SHE 92] SHERIDAN T.B., Telerobotics, Automation, and Human Supervisory 
Control, MIT Press, Cambridge, MA, 1992. 
[SHI 02] SHIMAKAGE M., SATOH S., UENUMA H.M.K., “Design of lane-keeping 
control with steering torque input”, Japan Society of Automotive Engineers 
(JSAE), vol. 23, pp. 317–323, 2002. 
[SOU 14] SOUALMI B., SENTOUH C., POPIEUL J.-C., et al., “Automation-driver 
cooperative driving in presence of undetected obstacles”, IFAC – Control 
Engineering Practice, vol. 24, pp. 106–119, March 2014. 
 

17 
Interactive Surfaces, Tangible Interaction: 
Perspectives for Risk Management 
17.1. Introduction  
Many pieces of research currently concern interactive surfaces and 
tangible interaction, relatively recent research directions in the human–
computer interaction domain (see, for instance, the last editions of the 
Computer-Human Interaction (CHI) conference). In this chapter, we are 
particularly interested in interactive tables (also called tabletops): they can 
be considered as new interaction platforms, as collaborative and colocalized 
workspaces, allowing several users to interact (work, play, etc.) 
simultaneously. There is a growing interest for these interaction platforms: 
since the first prototype, called digital desk proposed by Wellner [WEL 91], 
many prototypes and commercialized products have been proposed [KUB 
11]; dedicated conferences and workshops (e.g. IEEE Tabletop, ACM 
Interactive Tabletops and Surfaces, etc.) are also organized. 
Our approach consists of the use of TangiSense interactive table. It is 
equipped with Radio-Frequency IDentification (RFID) technology and is 
connected with a multi-agent platform (JADE) (for more details about multi-
agent approaches, the reader may read the papers on the subject, for example 
[CHA 92, FER 95, WEI 00]). It allows users to interact with tangible and 
virtual objects. 
                         
Chapter written by Christophe KOLSKI, Catherine GARBAY, Yoann LEBRUN, Fabien BADEIG,  
Sophie  LEPREUX,  René MANDIAU  and  Emmanuel  ADAM. 

352     Risk Management in Life-Critical Systems 
Our goal is to share an application between several users, platforms 
(tabletops, mobile and tablet devices and other interactive supports) and 
types of interaction, allowing distributed human–computer interactions  
[LEP 11, LEP 12]. Such approaches may lead to new perspectives for risk 
management; indeed, it may become possible to propose new types of 
remote and collaborative directions in this domain. 
The second section of this chapter presents a state of the art. In the third 
section, our proposition is explained: it consists of user interfaces (UIs) 
distributed on interactive tables and other surfaces for risk management. 
Two case studies illustrate this research: the first case study concerns road 
traffic management; the second case study is an implementation of the risk 
game. A conclusion and research perspectives end this chapter. 
17.2. State of the art 
The state of art is composed of two parts. The first part is devoted to 
usual interaction supports for risk management. The second part is about 
new interactive surfaces and tangible interaction: they will be envisaged in 
this chapter as new supports for risk management. 
17.2.1. Supports for risk management 
Crisis and risk management is a research field of growing interest, known 
to raise numerous scientific challenges [NEW 01, HOO 00]. Decision-
making, planning and action have to be operated under a heavy cognitive 
pressure and a high degree of unpredictability, by complex social 
organizations that may be characterized as distributed, open, collaborative 
and multicultural [DUG 10]. 
Knowledge and information sharing, together with context awareness is a 
crucial issue in this respect [BOY 05]: people must be provided with the 
right information, in the right format, at the right abstraction level and at the 
right moment. Contextual awareness is mandatory to reinforce understanding 
the reliability and usage of knowledge. To increase an operator’s efficiency, 
information display, preferably based on effective layout of the geographical 
environment, must allow us to easily obtain mental representations of 
situations [OWR 01]. Despite recent advances in the field of virtual reality, 
current collaboration support systems are often limited to mere 

Interactive Surfaces, Tangible Interaction    353 
communication tools (e.g. Google Wave or Wiki), working under clearly 
defined and bounded contexts. Their adaptation to crisis management  
[RUP 07] is only possible for well-circumscribed urgency routines with no 
exceptions [FRA 10]. Varying communication and collaboration needs are to 
be considered in addition, depending on the type of organization: small 
teams, widely distributed organizations, smart communities, etc. Some other 
work approaches crisis management under a service-oriented or information 
system interoperability viewpoint, like the SoKNOS German project  
(see http://www.soknos.de/) or the IsyCRI ANR project (see http:// 
www.irit.fr/IsyCRI). 
Incompleteness of knowledge, uncertainty and paucity of information is 
also a major issue [BOY 05, OWR 01], which combines with the difficulty 
to predict changes in the environmental or human situations to be faced. 
Errors or inappropriate actions are prone to occur in this case, which can 
hardly be recovered by written procedures or protocols. For [SEN 00], 
proactiveness is the only way to maintain a consistent coupling between 
processing structures and environments that are evolving, subject to 
unpredictable changes and uncertainty and too complex to be perceived 
completely. Heavy support may, however, result in making human operators 
passive, and facilitate complacency [OWR 01]. As quoted by [ROG 06], 
what is needed is “proactive people rather than proactive systems”, that is 
systems that act through incentives rather than directives. 
How to share authority and control between human(s) and machine(s) has 
been studied since the early 1980s [MIL 95, MIL 12], with a distinction 
often performed between strategic, tactical and operational tasks, between 
vertical (supervision) or horizontal (task sharing) allocation styles. A 
difficult issue, especially in the domain of risk management, is to decide 
whether the system is meant to assist human decision-making by providing 
missing information or complementary analysis or whether it is to supervise 
human actions and prevent potential errors. The current hierarchical model 
of control, where authority is centralized on the ground, is evolving toward a 
distributed model of authorities where coordination occurs among evolving 
agents communicating through a common frame of reference [BOY 09]. An 
original approach to man–machine cooperation has been proposed in 
Automation and MAN-Machine Delegation of Action (AMANDA)  
[MIL 12]. This system offers the possibility for controllers to delegate some 
tasks via a Common Work Space (CWS). CWS plays a role similar to a 
blackboard, displaying the problems to be solved cooperatively and the 

354     Risk Management in Life-Critical Systems 
evolution of the solution in real time. Current evolution toward ubiquitous 
computing and cloud technology results in the development of  
“systems of systems”, which involve networked human and machine 
agencies [BOY 13], with an increased potential for context-sensitive 
processing via pervasing sensing. 
In our view, collaborative support must not result in constraining or 
driving human action. Rather, it must allow enhanced context-awareness and 
support proactiveness. Tangible interfaces allow working under informal, 
opportunistic styles [GUT 08], thus implying increased attention to 
coordination issues. Our proposal is to rely on dedicated objects, called 
tangigets [GAR 12, LEP 12], to assist coordination. Informed virtual 
feedback is further implemented to situate human action with respect to the 
rules constraining collaboration. Our hypothesis is that constructive 
collaboration involves not only the sharing of other’s actions, but also and 
more deeply the sharing of the (often implicit) norms and rules driving these 
actions [BOU 01]. In this way, collaboration is seen as a conversational 
process embodied within the physical workspace [SHA 10]. 
17.2.2. Interactive surfaces, tangible interaction 
Interactive surfaces are becoming increasingly numerous and varied in 
everyday life (e.g. tablet, touch screen laptop that can be placed horizontally, 
smartphone offering varying screen sizes, interactive table, etc.). These 
surfaces can be of different sizes and may offer different forms of 
interaction. In particular, they can be tactile (most common case), but also 
tangible, i.e. provide an interaction with physical (tangible) objects. Our 
work focuses on this type of tangible interaction, which corresponds with an 
emerging technology [BLA 07]. 
The tangible user interfaces (TUIs) include interaction techniques with 
which the user interacts with a digital system through the manipulation of 
physical objects [HOR 06, ISH 08a, ISH 08b, ULM 00]. Tangible objects 
associated with virtual world are proposed by Rekimoto and colleagues 
[REK 01] who present datatiles: a modular platform mixing physical and 
graphical objects. The idea influenced Walder and colleagues [WAL 06] 
who propose work, akin to those of Rekimoto, through an assessment of 
tangible interfaces. So, tangible objects interest researchers, especially if it is 
possible to include them in interactive systems.  TUIs have been developed 

Interactive Surfaces, Tangible Interaction    355 
in several interactive systems: augmented reality [LEE 04], collaborative 
system [STA 02], embodied system [BAK 12], games [MAR 13] and 
interactive surfaces, in particular tabletops [COU 07]. 
The interactive tabletops have been successfully growing since 1991 with 
the Wellner’s DigitalDesk [WEL 91]. The concept of the interactive table 
supposes a collaborative and colocalized workspace allowing several users 
to work at the same time [KRU 03]. Nowadays, there are not many platforms 
that allow a simultaneous collaboration between users (such as multipointing 
or sharing of documents in real time) [DIE 01, WIG 06, SHA 12, YUI 12]). 
The technology evolves in terms of capture system, for example: frustrated 
total internal reflection (FTIR) technology, diffused illumination (DI) 
technology, diffused surface illumination (DSI) technology [HAN 05], RFID 
technology [KUB 12], optical fiber [JAC 09] and capacitive technology 
[DIE 01]. In terms of display dimension, various propositions are described 
in the literature: from 24″ [WEI 10] to 85″ [LEV 06]. For more details on 
technological aspects, see [KUB 11] who presents many tabletops and their 
features. 
Finally, there are tables that enable tangible interactions. This is the case 
of [WEI 09a, WEI 09b] offering in their work a system of tangible widgets, 
called Silicone illuminated Active Peripherals (SLAP) for use on an 
interactive table. Patten and his colleagues [PAT 01] propose SenseTable, a 
platform for tracing a wireless object for tangible user interfaces. A set of 
prototypes is then proposed to exploit the capabilities of interactive tables 
[PAT 02, STÅ 02, NOM 04]. Some prototypes are associated with a new 
technology that can recognize shapes printed on objects on the table via a 
camera [KAL 06, JOR 07, MAX 09]; RFID technology will also be used to 
allow the detection of physical objects [OLW 08, HOS 08, KUB 12].  
RFID technology is used by the TangiSense table (designed and produced by 
the RFIdées company, www.rfidees.com), illustrated in this chapter. We will 
see in the following sections the potential benefits of this technology for 
crisis management. 
17.3. Proposition: distributed UI on interactive tables and other surfaces 
for risk management 
Many risk management situations involve different groups of people who 
have to collaborate (see section 17.2.1). A major difficulty for these people 

356     Risk Management in Life-Critical Systems 
of various profiles and located in different places is to interact through 
different functions to solve more or less complex problems. 
A distribution of the UIs may bring new perspectives in this domain. 
According to [LEP 11], two global strategies may be proposed: 
– in the first strategy, an interactive table is declared to be the master and 
the other devices are slaves (the master table is the central object in  
Figure 17.1(a)). In this case, the table manages the information transfer 
according to the objectives of each platform, and it centralizes all the 
information available in the distributed system. The interface distribution 
can be seen in the form of tree. The master surface is the root of the tree  
(in the figure, this surface is the interactive table). The other  
platforms correspond to the nodes or leaves; in Figure 17.1(a), children  
(S1) = {S2,S3,S4,S5,S6,S7,S8}. This strategy is useful when the UI is 
complete on one support with priority and if UIs have to be distributed on 
other supports. Its disadvantage is that breakdowns are not tolerated; 
– in the second strategy, all the platforms are independent and at the same 
decision level (Figure 17.1(b)). They form a graph where n corresponds to 
the number of distributed UI (in Figure 17.1(b), n = 9). Here, a relation 
between two platforms means a distributed UI.  
 
Figure 17.1. Two configurations for risk management UI: a) centralized distribution  
of U; b) network of distributed UI [LEP 11]. For a color version  
of this figure, see www.iste.co.uk/millot/riskmanagement.zip 

Interactive Surfaces, Tangible Interaction    357 
A first illustration concerning risk management (and even in this case, 
crisis management) is given in Figure 17.2: a possible use of TangiSense 
tables is proposed, involving tangible and/or virtual objects; various devices 
(interactive tables or other surfaces) used by several users are also visible in 
the figure. 
As explained in [LEP 11], when a significant event such as a forest fire 
occurs, the people concerned are not together. Some are situated at a  
place where information is centralized; supervisors/decision makers are to  
be found among them. They collect information from other actors who are 
geographically separated on the ground, concerning elements such as  
the state of the fire, its propagation velocity and the direct implications. The 
crisis unit makes decisions based on the collected information and must 
transmit them to the onsite teams. They are also in contact with other 
structures such as the police officers who must, according to the case, 
prohibit access to certain zones or warn/evacuate the potential disaster 
victims. The state of the system at one given moment with an example of use 
per device and actor is shown in Figure 17.2. 
 
Figure 17.2. Crisis unit using TangiSense and other platforms (adapted from [LEP 11]).  
For a color version of this figure, see www.iste.co.uk/millot/riskmanagement.zip 

358     Risk Management in Life-Critical Systems 
17.4. Case studies 
In this section, our objective is to present briefly two prototypes of 
distributed applications developed on interactive tables. The first prototype 
concerns road traffic management; the second prototype is an 
implementation of the risk game. These applications may bring perspectives 
for the study and development of new types of applications which are usable 
or adaptable for risk management domain. 
17.4.1. Distributed road traffic management 
In this section, we show a case study related to the distributed simulation 
of road traffic management to implement a network configuration presented 
in section 17.3. 
The simulator of road traffic proposed is a network composed of links 
(roads, highways, etc.) and nodes (crossroads). Its first version on one 
interactive table is described in [LEB 12, KUB 13] and [LEB 13a]. The 
simulator is intended to be used by experts in security, architecture, 
transportation and also by non-expert like local elected member to obtain 
agreement on road or infrastructure modifications. The simulator aims to 
find a common policy on the management of the traffic jam in the presence 
of a crisis (natural disaster, zone evacuation, sporting events, etc.). It is 
important for the public authorities to anticipate the impact of the events on 
traffic jam and the risks in terms of safety (accidents and emergency). The 
authorities will regulate access to several key points: the input/output 
highway, near the sports complexes, shopping centers, hospitals and fire 
stations. These problems require a strong coordination of heterogeneous 
actors; for example, to close a road, evacuate people from a specific location 
and specify the location of a fire engine on the table. This coordination is 
expressed through generic tangible objects, called tangigets [LEP 12] on the 
table to activate a function (e.g. I start a task, my task is done, etc.). 
In the case study, the simulator is implemented on two connected 
interactive tables. Tables share the same virtual environment as shown in 
Figure 17.3. 

Interactive Surfaces, Tangible Interaction    359 
 
Figure 17.3. A road traffic simulation on two TangiSense interactive tables.  
For a color version of this figure, see www.iste.co.uk/millot/riskmanagement.zip 
The virtual environment is composed of the road network and virtual 
agents. Virtual agents are represented on each tabletop by vehicles moving 
randomly or according to a set of goals on the entire road network. These 
agents have behaviors that allow us, or not, for example, to respect the 
highway code [DON 06, DON 08]. The agents are managed using the Java 
Agent DEvelopment Framework (JADE) platform [BEL 01], a Foundation 
for Intelligent Physical Agents (FIPA) standard software implemented in 
Java, and used to simplify the deployment for the multi-agent applications. 
Each table has a set of tangible objects (specific to the traffic 
management application) and tangigets (usable in different types of 
application, not only traffic management) to interact locally not only with 
the simulation but also with the remote simulation (on the other table). In 
this case, the display may be different as shown in Figure 17.4. This figure 
shows that the user manipulates the simulator locally using an object, called 
zoom (allowing zooming in and out on particular zones of the traffic); this 
kind of object (with local properties) is used for manipulating the map. This 
tangible object is manipulated by the users, and it interacts with other virtual 
objects. This object is equipped with RFID tags to be able to modify the 
network structure. For example, to move the map, to view information about 
the name or the speed of the road, to zoom in or out and change the scale of 
the map, the user manipulates different tangible objects. 

360     Risk Management in Life-Critical Systems 
 
Figure 17.4. Use of zoom tangible object, without effect on the other table.  
For a color version of this figure, see www.iste.co.uk/millot/riskmanagement.zip 
To interact remotely with the other table, users have the ability to use 
tangigets (tangible object managed by a software agent with distributed 
properties). These properties allow it to be cloned using an agent located in 
another interactive surface. Figure 17.5 shows the use of a tangiget, called 
synchronization. The advantage of this tangiget is to coordinate display 
tabletops to work together on the same area. When this object is put down on 
the TangiSense 1, the TangiSense 2 will generate a clone agent; these two 
agents (the agent managing the tangiget and the clone) exchange messages in 
order to keep the data coherence. Messages contain the position and the scale 
of the map. This message is used by the TangiSense 2 to obtain the same 
vision as the tabletop 1. 
Such representative examples let us envisage new functions concerning 
traffic management usable by future risk management systems. They can be 
distributed on two interactive tables [LEB 13b], with a generalization to 
several interconnected tables: the interactions between interactive surfaces 
enable the collaboration and the information exchange between different 
users during simulation sessions. 

Interactive Surfaces, Tangible Interaction    361 
 
 
Figure 17.5. Tangiget synchronization with effect on TangiSense 2. For a color  
version of this figure, see www.iste.co.uk/millot/riskmanagement.zip 
17.4.2. Distributed risk game 
The risk game is a strategic board game for two to six players, played on 
a board depicting a political map of the Earth, divided into territories  
[PAR 63]. Players are allocated armies and fighting power, placed on 
occupied territories. An attack takes place between two attacking/attacked 
territories that the attacking player has to designate. Dice rolls are then used 
by both players to determine who is losing or winning this round. The 
assault continues until the attacking player decides to retire, or until one of 
the two is eliminated (all his or her armies on the attacking territory have 
been lost). Figure 17.6 displays an example view of the risk game (prototype 
version), as played on the TangiSense table. As in the previous application, 
the agent aspects are managed by using the JADE platform. 
This application is a case of a static environment of limited complexity, 
with players operating at the same organizational level, under strict 
coordination rules. In this application, as in all games, the “risk” does not 
come from critical evolutions in the physical environment. Rather, it comes 
from the unpredictable character of the player’s intentions, and from the fact 
that opponent players are willing to win the game. Getting accurate pictures 
of situations (past, current as well as expected moves) is mandatory for 

362     Ris
playing 
places, 
that the
tangible
In t
commun
virtual f
the bur
coordin
proactiv
F
m
We p
where th
feedbac
(designi
may rev
objects 
virtual f
tables to
moves 
core iss
heteroge
exchang
context 
constrai
 
sk Management
correctly. T
with no face
e players ha
e and virtual 
this context
nication, thr
feedback, (2
rden of me
ation framew
vity. 
Figure 17.6. Th
map display, tan
of this f
propose to a
he signs of d
ck. Face-to-fa
ing, acknow
veal difficul
(called tang
feedback wil
ogether with
have been o
sue, especiall
eneous work
ging views, o
awareness a
ints that fram
t in Life-Critical 
This raises ch
e-to-face com
ave no comm
equipment).
t, the goal 
rough a wid
2) ensure the
entalizing/gu
work ensurin
e TangiSense ta
ngible objects a
figure, see www
address colla
dialog are pr
ace oral conv
wledging, ref
lt to model 
gigets) are d
ll allow the f
h additional i
operated. Be
ly in the cas
king styles a
or sharing th
as sharing the
me activity. T
Systems 
hallenging is
mmunication
munication 
 
of our d
de palette of
e follow-up 
uessing com
ng smooth in
able as equippe
and virtual feedb
w.iste.co.uk/mill
aborative acti
rovided by ta
versation usu
ferring, turn 
in this limi
designed to 
follow-up of 
information 
eyond conve
e of comple
and conventi
he production
e task-depen
This is ensur
ssues when p
n with other p
means excep
design is to
f tangible ob
and sharing 
mplex norm
nterplay, wh
ed for the risk g
back shown. Fo
lot/riskmanagem
ivity as a co
angible obje
ually involve
taking and 
ited context.
this end. In
f tangible obj
about the co
ersation, con
x organizatio
ons. It shoul
n of results. R
ndent and org
red in our de
playing from
players (we 
pt the table
o (1) provi
bjects and i
of rules, all
s and (3) 
hile leaving r
game with groun
or a color versio
ment.zip 
onversational
ct moves an
es several spe
giving spee
. Dedicated 
n addition, i
ect moves on
ontext in whi
ntext awaren
ons that may
ld not be red
Rather, we a
ganization-de
esign by the h
m distant 
consider 
e and its 
ide rich 
informed 
leviating 
offer a 
room for 
 
nd  
on  
l process 
d virtual 
eech acts 
ech) that 
tangible 
informed 
n distant 
ich these 
ness is a 
y exhibit 
duced to 
approach 
ependent 
handling  

of nume
relation
underne
actor re
incentiv
partners
Follo
propose
coordin
multi-ag
undertak
modeled
of conc
with hum
Figu
Fo
erical and ph
ship to th
eath a tangib
esponsible f
ves for the co
s of the colla
owing the C
e 
an 
arch
ation spaces
gent system
ken among 
d constraints
current and 
man actors. 
ure 17.7. Functi
or a color versio
hysical trace
he constrain
ble object re
for this mov
oevolution a
aboration. 
Clover appro
hitecture 
co
s (Figure 17
m, to ensure
the three sp
s and autonom
interleaved 
ional view show
on of this figure
Interac
es, which ref
nts under 
eflecting the
ve). This fe
and better co
oach for gro
oupling 
pro
.7). The sys
e a clear d
paces, a clea
mously actin
traces and a
wing the variou
e, see www.iste
ctive Surfaces, T
flect both hu
consideratio
e conformity
eedback may
oordination o
oupware des
oduction 
c
stem is desig
distribution o
ar separation
ng processes,
a context-aw
us types of agen
.co.uk/millot/ris
Tangible Interac
uman activity
on (virtual 
y of its mov
y be consid
of actions am
sign [LAU 
ommunicatio
gned as a no
of the task
n between e
 a situated fo
ware commu
ts, filters and tr
skmanagement.
ction    363 
y and its 
display 
ve to the 
dered as 
mong the 
02], we 
on 
and 
ormative 
ks to be 
explicitly 
ollow-up 
unication 
 
races.  
.zip 

364     Risk Management in Life-Critical Systems 
Numerical traces are defined as multidimensional components, whose 
role is to reflect the actor’s activities (either human or agent) and their 
conformance to the task requirements. Any trace is considered as a set of 
pairs (property and value), with properties typed, to register their compliance 
to the norms. Traces evolve under the activity of agents and norms. Any 
trace is defined as possessing the following minimal properties (identifying 
name, type of tangible object, tangible table where originated, spatial 
position on this table and time of move): 
Trace = {(type,vtype), (table,vtable), (name,vname), (pos,vpos), (time, vtime) } 
A norm has the following simplified expression: N = <id, context, role, 
object> in which context represents an evaluation condition, role represents 
the agent’s role concerned with this norm and object is a complex field, 
typically written as launch (conditions and actions) characterizing the 
conditional action attached to the norm (launching of agents’ behavior and 
annotation of trace properties). We distinguish between three kinds of 
norms. Communication norms ensure the creation of traces to follow-up 
tangible objects move. They further ensure the launching of feedback agents. 
Production norms launch production agents to manage trace elements. 
Coordination norms include consistency norms that check the compliance of 
human activity with respect to current norms. These norms enrich the trace 
via the corresponding trace properties fields. Some coordination norms 
specify constraints over the collaboration process and launch coordination 
agents to this end. 
An agent is defined as: Ag = < id, role, behaviors, norms > with id a 
unique identifier, role is the role of the agent in the system (∈ {production, 
communication, coordination}), behaviors is a list of concurrent agent 
abilities, norms is the set of norms that the agent has subscribed to. We 
distinguish between three types of agents: production, communication and 
coordination. Communication agents ensure the follow-up of incoming 
traces and the launching of virtual feedbacks. Production agents perform the 
computations required by the task under consideration (trace elements 
analysis and interpretation). Coordination agents maintain the consistency of 
the trace components in a context where distant human performs concurrent 
actions. They further ensure the adequacy of the set of norms to the current 
task and step of the collaboration. Such design follows the definition of 
normative multi-agent systems: sets of agents (human or artificial) working  
under norms “serving to guide, control, or regulate proper and acceptable 

Interactive Surfaces, Tangible Interaction    365 
behavior” and defining “a principle of right action binding upon the 
members of a group” [BOE 06]. Deontic rules are usually defined to express 
permission or obligations regarding the way norms are applied that are not 
considered in the present proposal. 
When applied to the risk game, the system operates according to the 
following information flow: (1) early detection of a tangible object move by 
communication norms operating at the infrastructure level: creation or 
updating of the corresponding local trace; (2) triggering of the coordination 
norms: updating of the corresponding local traces; (3) triggering of the 
production norms: computation of some local trace property; (4) triggering 
of the communication norms: feedback to local and distant human actors. 
When the game starts, a default norm policy is activated to handle the 
process associated with game initialization. When the game comes in the 
state fight, the players handle two specific tangible objects (tangigets) to 
proceed to designation of attacking territories and dice rolls. New norm 
policies must be applied to deal with these new states. Coordination norms 
are designed to this end. The example of the norm-attack-policy is provided 
below: its role is to launch a coordination agent manage-norm-policy to 
perform the necessary updating and ensure game consistency. 
Norm-attack-policy = <id, [step = fight], synchronization, launch(cond, Manage-
norm-policy)> with cond = [trace.type(?t1) = coordination] & [trace.onTable(?t1) 
= ?tab1] & [trace.value(?t1) = “attack”] & [trace.type(?t2) = designation] & 
[trace.onTable(?t2) = ?tab1] & & [trace.value(?t2) = ?jd] 
At the end of a fight, each player handles dices to determine a winner and 
a looser for this fight. The production norm, called norm-dice-result, ensures 
the follow-up of the dice roll results, the determination of a winner and loser 
and the launch of an agent whose role will be to update the traces 
accordingly: 
Norm-dice-result = <id, [step = fight], dice-result, launch(cond, win)> with cond = 
[trace.type(?t1) = dice] & [trace.value(?t1) = ?v1] & [trace.onTable(?t1) = ?x] & 
[trace.type(?t2) = dice] & [trace.value(?t2) = ?v2] & [trace.onTable(?t2) ≠ ?x] & 
[?v1 > ?v2] 
This application is also representative of new possibilities offered by 
interactive tables connected with a multi-agent platform; we think that such 
concepts are also adaptable for the risk management domain. More details 
about this application are available in [GAR 12] and [BAD 13]. 

366     Risk Management in Life-Critical Systems 
17.5. Conclusion 
Interactive surfaces and tangible interaction are two research and 
development themes that are particularly studied by the Human–Computer 
Interaction (HCI) scientific community, as well as by many IT companies, 
leading to many propositions, prototypes and products. In this chapter, the 
TangiSense interactive table and its potentialities have been presented. Its 
architecture allows UI distribution between different interactive tables and 
surfaces in general. This distribution is made possible by integrating an 
intelligent management of the UI distribution into the agents of distributed 
multi-agent systems (developed with JADE). 
Several developments of distributed interactive applications on one or 
several TangiSense tables are in progress. This chapter was first focused on 
road traffic management, which is an important problem to consider in case 
of crisis management. For instance, it is important to study and find 
solutions in case of traffic jams, serious accidents implicating many vehicles, 
forest fires or flooding with consequences for the traffic and so on. A 
distributed road traffic management simulator has been implemented on two 
connected TangiSense tables. The second application, implemented on 
several interaction supports, and described in this chapter, concerns a 
distributed risk game, a well-known game in which risk management is a 
central aspect for the users involved. 
Such new interactive applications distributed on several supports offer 
new research and development ways for risk management. Several 
perspectives may be underlined. In fact, it is important to continue the 
technical tests with several tabletops and other interaction supports. It is also 
possible to envisage different types of surfaces (floor, wall, etc.). We also 
plan to take into account real and increasingly complex scenarios, and to 
prepare and perform different types of evaluations with distributed contexts. 
17.6. Acknowledgments 
This research was partially financed by the French Ministry of Education, 
Research & Technology, the Nord/Pas-de-Calais Region, the CNRS, the 
FEDER program, CISIT (Plaiimob project) and the French National 
Research Agency (ANR TTT and IMAGIT projects, financial IMAGIT 
support: ANR-10-CORD-017).  

Interactive Surfaces, Tangible Interaction    367 
17.7. Bibliography 
[BAD 13] BADEIG F., VETTIER B., GARBAY C., “Perceiving and interpreting human 
activity: a normative multi-agent system”, 11th International Conference on 
Naturalistic Decision Making, Marseille, France, May 21 and May 22–24, 2013. 
[BAK 12] BAKKER S., ANTLE A.N., VAN DEN HOVEN E., “Embodied metaphors in 
tangible interaction design”, Personal Ubiquitous Computing, vol. 16, no. 4,  
pp. 433–449, 2012. 
[BEL 01] BELLIFEMINE F., POGGI A., RIMASSA G., “Developing multi-agent systems 
with a FIPA-compliant agent framework”, Software Practice and Experience, 
vol. 31, no. 2, pp. 103–128, 2001. 
[BLA 07] BLACKWELL A.F., FITZMAURICE G., HOLMQUIST L.E., et al., “Tangible 
user interfaces in context and theory”, CHI ’07 Extended Abstracts on Human 
Factors in Computing Systems, ACM, New York, NY, pp. 2817–2820, 2007. 
[BOE 06] BOELLA G., VAN DER TORRE L., VERHAGEN H., “Introduction to 
normative multiagent systems”, Computational & Mathematical Organization 
Theory, vol. 12, nos. 2–3, pp. 71–79, 2006. 
[BOU 01] BOURGUIN G., DERYCKE A., TARBY J.C., “Beyond the interface: co-
evolution inside interactive systems, a proposal founded on activity theory”, 
IHM-HCI 2001 Conference, People and Computer XV – Interactions without 
Frontiers, Springer Verlag, pp. 297–310, 2001. 
[BOY 05] BOY G.A., BARNARD Y., “Knowledge management in the design of 
safety-critical systems”, Encyclopedia of Knowledge Management, Idea Group 
Inc., July 2005. 
[BOY 09] BOY G.A., “The Orchestra: A Conceptual Model for Function Allocation 
and Scenario-based Engineering in Multi-Agent Safety-Critical Systems”, 
Proceedings of the European Conference on Cognitive Ergonomics, Otaniemi, 
Helsinki area, Finland, 30 September-2 October 2009. 
[BOY 13] BOY G.A., “Human-centered design of life-critical systems”, Digital 
Enterprise Design & Management (DED&M) Conference, Paris, 2013. 
[CHA 92] CHAIB-DRAA B., MOULIN B., MANDIAU R., et al., “Trends in distributed 
artificial intelligence”, Artificial Intelligence Review, vol. 6, pp. 35–66, 1992. 
[COU 07] COUTURE N., RIVIÈRE G., “Table interactive et interface tangible pour les 
géosciences: retour d’expérience”, IHM ’07: 19ème Conférence Francophone 
sur l’Interaction Homme-Machine, ACM, Paris, France, pp. 23–26, 2007. 

368     Risk Management in Life-Critical Systems 
[DIE 01] DIETZ P., LEIGH D., “Diamond touch: a multi user touch technology”, 
Proceedings of the 14th Annual ACM Symposium on User Interface Software 
and Technology (UIST ’01), ACM, Orlando, FL, pp. 219–226, 2001. 
[DON 06] DONIEC A., MANDIAU R., ESPIÉ S., et al., “Non-normative behaviour in 
multi-agent system: some experiments in traffic simulation”, IEEE/WIC/ACM 
International Conference on Intelligent Agent Technology (IAT) 2006 Main 
Conference Proceedings (Hong Kong, China, 18–22 December 2006), IEEE 
Computer Society, Los Alamitos, CA, pp. 30–36, 2006. 
[DON 08] DONIEC A., MANDIAU R., PIECHOWIAK S., et al., “Controlling non-
normative by behaviours anticipation for autonomous agents”, Web Intelligence 
and Agent Systems: An International Journal, vol. 6, pp. 29–42, 2008. 
[DUG 10] DUGDALE J., BELLAMINE-BEN SAOUD N., PAVARD B., et al., “Simulation 
and emergency management”, in VAN DE WALLE B., TUROFF M., HILTZ S.R. 
(eds.), Information Systems for Emergency Management, Series: Advances in 
Management Information Systems, Sharp, 2010. 
[FER 95] FERBER J., Multi-Agent System: An Introduction to Distributed Artificial 
Intelligence, Addison Wesley Longman, 1995. 
[FRA 10] FRANKE J., CHAROY F., “Design of a collaborative disaster response 
process management system”, 9th International Conference on the Design of 
Cooperative Systems, Marseille, 2010. 
[GAR 12] GARBAY C., BADEIG F., CAELEN J., “Supporting collaborative work in 
socio-physical environments: a normative approach”, 10th International 
Conference on the Design of Cooperative Systems, COOP 2012, Marseille, 30 
May–1 July 2012. 
[GUT 08] GUTWIN C., GREENBERG S., BLUM R., et al., “Supporting informal 
collaboration in shared-workspace groupware”, Journal of Universal Computer 
Science, vol. 14, no. 9, pp. 1411–1434, 2008. 
[HAN 05] HAN J.Y., “Low-cost multi-touch sensing through frustrated total internal 
reflection”, Proceedings of the 18th Annual ACM Symposium on User Interface 
Software and Technology (UIST ’05), ACM, pp. 115–118, 2005. 
[HOO 00] HOOKE W.H., “U.S. participation in international decade for natural 
disaster reduction”, Natural Hazards Review, vol. 1, no. 1, pp. 2–9, 2000. 
[HOR 06] HORNECKER E., BUUR J., “Getting a grip on tangible interaction: a 
framework on physical space and social interaction”, Proceedings CHI 2006, 
ACM, pp. 436–446, 2006. 

Interactive Surfaces, Tangible Interaction    369 
[HOS 08] HOSOKAWA T., TAKEDA Y., SHIOIRI N., et al., “Tangible design support 
system using RFID technology”, Proceedings of the 2nd International 
Conference on Tangible and Embedded Interaction (TEI ‘08)”, ACM, pp. 75–78, 
2008. 
[ISH 08a] ISHII H., “Tangible bits: beyond pixels”, Proceedings of the 2nd 
International Conference on Tangible and Embedded Interaction (TEI ’08),  
pp. xv–xxv, ACM, New York, NY, 2008. 
[ISH 08b] ISHII H., “The tangible user interface and its evolution”, Communications 
of the ACM, vol. 51, pp. 32–36, 2008. 
[JAC 09] JACKSON D., BARTINDALE T. OLIVIER. P., “FiberBoard: compact multi-
touch display using channeled light”, Proceedings of the ACM International 
Conference on Interactive Tabletops and Surfaces (ITS ‘09), ACM, New York, 
NY, pp. 25–28, 2009. 
[JOR 07] JORDÀ S., GEIGER G., ALONSO M., et al., “The reacTable: exploring the 
synergy between live music performance and tabletop tangible interfaces”, 
Proceedings of the 1st International Conference on Tangible and Embedded 
Interaction (TEI ‘07), ACM, pp. 139–146, 2007. 
[KAL 06] KALTENBRUNNER M., BENCINA R., “ReacTIVision: a computer-vision 
framework for table-based tangible interaction”, Proceedings of the 1st 
international conference on Tangible and embedded interaction (TEI ‘07), 
ACM, pp. 69–74, 2007. 
[KRU 03] KRUGER R., CARPENDALE S., SCOTT S.D., et al., “How people use 
orientation on tables: comprehension, coordination and communication”, 
GROUP ‘03, Proceedings of the 2003 international ACM SIGGROUP 
conference on Supporting group work, ACM, pp. 369–378, 2003. 
[KUB 11] KUBICKI S., Contribution à la prise en considération du contexte dans la 
conception de tables interactives sous l’angle de l’IHM, application à des 
contextes impliquant table interactive RFID et objets tangibles, PhD Thesis, 
University of Valenciennes, France, 2011. 
[KUB 12] KUBICKI S., LEPREUX S., KOLSKI C., “RFID-driven situation awareness on 
TangiSense, a table interacting with tangible objects”, Personal and Ubiquitous 
Computing, vol. 16, no. 8, pp. 1079–1094, 2012. 
[KUB 13] KUBICKI S., LEBRUN Y., LEPREUX S., et al., “Simulation in contexts 
involving an interactive table and tangible objects”, Simulation Modeling 
Practice and Theory, vol. 31, pp. 116–131, 2013. 

370     Risk Management in Life-Critical Systems 
[LAU 02] LAURILLAU Y., NIGAY, L., “Clover architecture for groupware”, ACM 
Conference on Computer Supported Cooperative Work (CSCW 2002), New 
Orleans, Louisiana, LA, 16–20 November 2002. 
[LEB 12] LEBRUN Y., Architecture multi-agents pour la gestion d’objets tangibles et 
virtuels sur Tables Interactives, PhD Thesis, University of Valenciennes, France, 
2012. 
[LEB 13a] LEBRUN Y., ADAM E., MANDIAU R., et al., “Interaction between tangible 
and virtual agents on interactive tables: Principles and case study”, The 4th 
International Conference on Ambient Systems, Networks and Technologies (ANT 
2013), Halifax, Nova Scotia, Canada, 25–28 June 2013. 
[LEB 13b] LEBRUN Y., LEPREUX S., KOLSKI C., et al., “Combination between multi-
agent system and tangigets for DUI design on several tabletops”, in  
LOZANO M.D., MASHAT A.S., FARDOUN H.M., et al., (eds.), DUI 2013: 3rd 
Workshop on Distributed User Interfaces: Models, Methods and Tools, in 
conjunction with ACM EICS 2013, London, UK, pp. 54–57, June, 2013. 
[LEE 04] LEE G.A., BILLINGHURST M., KIM G.J., “Occlusion based interaction 
methods for tangible augmented reality environments”, Proceedings of the 2004 
ACM SIGGRAPH International Conference on Virtual Reality Continuum and 
its Applications in Industry (VRCAI ‘04), pp. 419–426, 2004. 
[LEP 11] LEPREUX S., KUBICKI S., KOLSKI C., et al., “Distributed interactive 
surfaces: a step towards the distribution of tangible and virtual objects”, in 
GALLUD J.A., TESORIERO R., PENICHET V.M.R. (eds.), Distributed User 
Interfaces, Designing Interfaces for the Distributed Ecosystem, Springer,  
pp. 133–143, 2011. 
[LEP 12] LEPREUX S., KUBICKI S., KOLSKI C., et al., “From centralized interactive 
tabletops to distributed surfaces: the tangiget concept”, International Journal of 
Human-Computer Interaction, vol. 28, pp. 709–721, 2012. 
[LEV 06] LEVIN G., “The table is the score: an augmented-reality interface for real-
time, tangible, spectrographic performance”, Proceedings of the International 
Conference on Computer Music 2006 (ICMC ’06), New Orleans, LA, 2006. 
[MAR 13] MARCO J., BALDASSARRI S., CEREZO E., “ToyVision: a toolkit to support 
the creation of innovative board-games with tangible interaction”, Proceedings 
of the 7th International Conference on Tangible, Embedded and Embodied 
Interaction (TEI ‘13), ACM, New York, NY, pp. 291–298, 2013. 
[MAX 09] MAXIMO A., SABA M. P., VELHO L., “CollecTable: a natural interface for 
music collections”, SIGGRAPH ‘09: Posters, ACM, pp. 1–1, 2009. 

Interactive Surfaces, Tangible Interaction    371 
[MIL 12] MILLOT P., BOY G., “Human-machine cooperation: a solution for life-
critical Systems?”, Work: A Journal of Prevention, Assessment and 
Rehabilitation, vol. 41, pp. 4552–4559, 2012. 
[MIL 95] MILLOT P., MANDIAU R., “Men–machine cooperative organizations: 
formal and pragmatic implementation methods”, in HOC J.M., CACCIABUE P.C., 
HOLLNAGEL E. (eds.), Expertise and Technology: Cognition Computer 
Cooperation, Lawrence Erlbraum Associates, NJ, pp. 213–228, 1995. 
[NEW 01] NEWKIRK R.T., “The increasing cost of disaster in developed countries: a 
challenge to local planning and government”, Journal of Contingencies and 
Crisis Management, vol. 9, no. 3, pp. 159–170, 2001. 
[NOM 04] NOMA H., YOSHIDA S., YANAGIDA Y., et al., “The proactive desk: a new 
haptic display system for a digital desk using a 2-DOF linear induction motor”, 
Presence: Teleoperators and Virtual Environments, vol. 13, pp. 146–163, 2004. 
[OLW 08] OLWAL A., WILSON A., “SurfaceFusion: unobtrusive tracking of 
everyday objects in tangible user interfaces”, Proceedings of GI 2008 (The 34th 
Canadian Graphics Interface Conference), Windsor, Canada, pp. 235–242, 28–
30 May 2008. 
[OWR 01] OWRE F., “Role of the man–machine interface in accident management 
strategies”, Nuclear Engineering and Design, vol. 209, nos. 1–3, pp. 201–210, 
November 2001. 
[PAT 01] PATTEN J., ISHII H., HINES J., et al., “Sensetable: a wireless object tracking 
platform for tangible user interfaces”, CHI ’01, Proceedings of the SIGCHI 
Conference on Human Factors in Computing Systems, ACM, pp. 253–260, 2001. 
[PAR 63] PARKER BROTHERS, Risk! Rules of Play, 1963. Available at: 
http://www.hasbro.com/common/instruct/Risk1963.PDF. 
[PAT 02] PATTEN J., RECHT B., ISHII H., “Audiopad: a tag-based interface for 
musical performance”, Proceedings of the 2002 Conference on New Interfaces 
for Musical Expression (NIME ‘02), National University of Singapore, pp. 1–6, 
2002. 
[REK 01] REKIMOTO J., ULLMER B., OBA H., “DataTiles: a modular platform for 
mixed physical and graphical interactions”, Proceedings of the SIGCHI 
Conference on Human Factors in Computing Systems (CHI ‘01), ACM, New 
York, NY, pp. 269–276, 2001. 
[ROG 06] ROGERS Y., “Moving on from Weiser’s vision of calm computing: 
engaging ubicomp experiences”, in DOURISH P., FRIDAY A. (eds.), UbiComp 
2006: Ubiquitous Computing, Springer Berlin/Heidelberg, pp. 404–421, 2006. 

372     Risk Management in Life-Critical Systems 
[RÜP 07] RÜPPEL U., WAGENKNECHT A., “Improving emergency management by 
formal dynamic process-modeling”, 24th Conference on Information Technology 
in Construction, Maribor, Slovenia, pp. 559–564, 2007. 
[SEN 00] SEN S., WEISS G., “Learning in multiagent systems”, Multiagent Systems: 
A Modern Approach to Distributed Artificial Intelligence, MIT Press, 2000. 
[SHA 10] SHAER O., HORNECKER E., “Tangible user interfaces: past, present and 
future directions”, Foundations and Trends in Human-Computer Interaction, 
vol. 3, nos. 1–2, pp. 1–138, 2010. 
[SHA 12] SHAER O., STRAIT M., VALDES C., et al., “The design, development, and 
deployment of a tabletop interface for collaborative exploration of genomic 
data”, International Journal of Human-Computer Studies, vol. 70, no. 10,  
pp. 746–764, 2012. 
[STÅ 02] STÅHL O., WALLBERG A., SÖDERBERG J., et al., “Information exploration 
using the pond”, Proceedings of the 4th International Conference on 
Collaborative Virtual Environments (CVE ‘02), ACM, pp. 72–79, 2002. 
[STA 02] STANTON D., NEALE H., BAYON V., “Interfaces to support children’s co-
present collaboration: multiple mice and tangible technologies”, STAHL G. (ed.), 
Proceedings of the Conference on Computer Support for Collaborative 
Learning: Foundations for a CSCL Community (CSCL ‘02), International 
Society of the Learning Sciences, pp. 342–351, 2002. 
[ULM 00] ULLMER B., ISHII H., “Emerging frameworks for tangible user interfaces”, 
IBM Systems Journal, vol. 39, pp. 915–931, 2000. 
[WAL 06] WALDNER M., HAUBER J., ZAUNER J., et al., “Tangible tiles: design and 
evaluation of a tangible user interface in a collaborative tabletop setup”, OZCHI 
‘06: Proceedings of the 18th Australia conference on Computer-Human 
Interaction, ACM, pp. 151–158, 2006. 
[WEI 00] Weiss G. (ed.), Multiagent Systems: A Modern Approach to Distributed 
Artificial Intelligence, MIT Press. 2000. 
[WEI 09a] WEISS M., WAGNER J., JANSEN Y., et al., “SLAP widgets: bridging the 
gap between virtual and physical controls on tabletops”, CHI ‘09: Proceedings 
of the 27th International Conference on Human factors in Computing Systems, 
ACM, pp. 481–490, 2009. 
[WEI 09b] WEISS M., WAGNER J., JENNINGS R., et al., “SLAPbook: tangible widgets 
on multi-touch tables in groupware environments”, Proceedings of the 3rd 
International Conference on Tangible and Embedded Interaction (TEI ‘09), 
ACM, pp. 297–300, 2009. 

Interactive Surfaces, Tangible Interaction    373 
[WEI 10] WEISS M., SCHWARZ F., JAKUBOWSKI S., et al., “Madgets: actuating 
widgets on interactive tabletops”, 23rd Annual ACM Symposium on User 
Interface Software and Technology (UIST ’10), ACM, New York, NY, USA,  
pp. 293–302, 2010. 
[WEL 91] WELLNER P., “The digital desk calculator: tangible manipulation on a 
desk top display”, UIST ’91, Proceedings of the 4th annual ACM symposium on 
User interface software and technology, ACM, pp. 27–33, 1991. 
[WIG 06] WIGDOR D., SHEN C., FORLINES C., et al., “Table-centric interactive 
spaces for real-time collaboration”, AVI ’06, Proceedings of the working 
conference on Advanced visual interfaces, ACM, pp. 103–107, 2006. 
[YUI 12] YUILL N., ROGERS Y. “Mechanisms for collaboration: a design and 
evaluation framework for multi-user interfaces”, ACM Transactions on 
Computer-Human Interaction, vol. 19, no. 1, Article 1, 2012. 


 
Conclusion 
Risk management is a very broad field of research where several 
disciplines contribute to preventing and recovering unexpected events or 
managing their consequences. It is particularly interesting to draw lessons 
about the different studied Life-Critical Systems. Moreover, the risk 
management methods themselves follow a huge evolution. 
C.1. A large range of Life-critical Systems  
The interest in Life-Critical Systems increases more and more. In the  
mid-2000s, René Amalberti came up with the following categories of risky 
systems [AMA 05]: 
– the riskiest systems involve amateur individuals, alpine mountaineering 
for example, with a risk level around 10–2; 
– next, he places systems available to the public in which the safety 
culture is poorly developed (or not consistent) and the choice of operators is 
not very discriminative, such as car driving, with a level of 10–3; 
– the chemical industry is next, with a risk level of 10–4; 
– charter flights with a level of 10–5; 
– finally come systems that are said to be ultra-safe, such as commercial 
aviation, the nuclear industry and transport by train or metro, with a risk 
level of 10–6. 
                         
Conclusion written by Patrick MILLOT. 

376     Risk Management in Life-Critical Systems 
In the present book the favorite application fields are at the extreme sides 
of Amalberti’s spectrum: ultra-safe systems with civil aviation, nuclear 
power plants, and railways on one hand, risky systems such as automobile 
and deep space exploration on the other. 
Nevertheless even in ultra-safe systems, accidents can occur. In that case, 
in-depth investigations are conducted in order to learn about the accident 
conditions and to reinforce the week points if any. Several examples are 
provided in order to criticize the present methods for risk management: 
several aircraft accidents are analyzed in Chapter 9, several nuclear power 
plant incidents and misuses of procedures by the operators are analyzed in 
Chapter 10, and the Fukushima accident is a case study used for illustrating 
new visual systems for safety enhancement in Chapter 4. 
On the contrary, the deep space exploration developed in Chapter 14 is a 
specific risky field but is performed by highly knowledgable and well-
trained specialists. However, car driving is certainly the riskiest field that is 
the target of the most numerous studies. Several diversely advanced 
solutions are proposed, some enhancing the vehicle stability, some aiming an 
automation of the driving tasks, and in between, a lot of them provide and 
evaluate Advanced Driver Assistance Systems (ADAS). Chapters 3, 7, 11, 
15 and 16 relate to these studies. Conversely, and in contrast to other fields, 
the main difficulty lies in the large variety of the population of operators, 
mainly 
non-professional 
and 
with 
heterogeneous 
abilities, 
from 
inexperienced young drivers to old persons. 
C.2. Evolution of risk management methods 
As Boy mentions in Chapter 1, “sectors dealing with life-critical systems, 
such as aerospace, nuclear energy and medicine, have developed safety 
cultures that attempt to frame operations within acceptable domains of risk. 
They have improved the systems’ engineering approaches and developed 
more appropriate regulations, operational procedures and training programs. 
System reliability has been extensively studied and related methods have 
been developed to improve safety [NIL 03]”. Good examples with Very 
High Speed Trains are given in Chapter 5 that mix “three success  
factors” (1) expertize and innovation in design, operation and maintenance in 
safety critical technologies, (2) competences in project management  
and system integration and (3) procedures for risk management.  
Concrete examples of crisis management including complementary 

Conclusion     377 
organizational and social dimensions are given respectively in Chapter 3,  
in promoting the rapidity of rescues after a road accident and in  
Chapter 4, by introducing new visualization devices in the nuclear plant 
control room. 
But methods based on the classical definition of risk,calculated by the 
product of the event probability by the gravity of its consequences can be 
questioned especially in case of extreme events with very low occurrence 
probabilities and very severe consequences. Indeed this product gives a full 
uncertainty. These theories and methods are based on “linear approaches to 
engineering systems that considers unexpected and rare events as exceptions” 
as Boy says in Chapter 1. Yet the recent accident of Fukushima has shown that 
an improbable event can nevertheless occur. Therefore recent approaches tend 
to extend Risk Analysis methods with the concepts of vulnerability and 
resilience as seen in Chapter 2. The related frameworks are particularly 
developed in the part 1: general approaches for crisis management. Moreover 
beside technical causes of risks, human causes of “unreliability” do intervene. 
Parts 2 and 3 deal with that important dimension. 
C.3. Risk management and human factors 
Indeed in 1950, for a 100 aircraft accidents, 70 were due to a technical 
problem, and humans caused 30. Since 2000, globally this proportion has 
been reversed, with 70 human causes for 30 technical causes. This is 
particularly due to a great increase in technical reliability, while human 
causes have not changed. Human reliability is a difficult endeavor; human- 
factor specialists developed approaches based on human error analysis and 
management [HOL 98], see for instance Chapter 9. Risk Analysis methods, 
taking human behavior into consideration are proposed, in order to enhance 
human–machine system resilience for instance with the concept of 
dissonance developed in Chapter 8, or the well-formalized and consolidated 
methodology called Risk-based Design (RBD) described in Chapter 7. RBD 
integrates systematically risk analysis in the design process with the aim of 
prevention, reduction and/or containment of hazards and consequences 
embedded in the systems as the design process evolves. 
Human errors entail a natural reflex of the designer to minimize the role 
of the human in systems by increasing the level of automation.  
Before attaining the full automation, a midterm consists of introducing 
assistance systems such as ADAS in car driving and more generally 

378     Risk Management in Life-Critical Systems 
multiagent human machine platforms such as those seen in Chapter 17. 
Then, the problem is to define online the adequate level of automation – see 
Chapter 16 – and the parameters such as the trust which the human has  
in the assistance system, as seen in Chapter 15. In addition to the  
system automation, another solution is to define procedures the humans 
must follow according to different well-identified situations. In the nuclear 
industry the present trend is to define procedures for every situation. Indeed 
when no procedure exists, the operators can be lost, examples can be seen in 
Chapter 10.  
The last alternative is to consider the positive aspect of human behavior, 
their abilities to cope with unknown problems and invent new solutions.  
For that purpose, favoring Situation Awareness (SA) seems a fruitful  
issue to warrant the human presence in the system. Two studies  
are dedicated to SA and moreover to collective SA, see Chapters 13  
and 14. More generally, enhancing SA can be achieved through a  
human machine cooperation through a Common Work Space (CWS) that  
is a support which can mediate the interactions between the human  
agents and between the human and artificial agents [MIL 14]. Several 
applications in deep space exploration, in air traffic control, in fighter 
aircraft cockpits, and in recognition robotics are given in Chapters 12, 13 
and 14. 
After all these methods have been proposed, developed and tested for 
prevention, recovery and global management of risk, comes another issue: 
“the time to act and to take a risk”. That is the next step to be studied in our 
joint research lab, and the topic of the next workshop, held at Melbourne 
Florida Institute of Technology on March 3–7, 2014. A second book will be 
edited following this occasion.  
C.4. Bibliography 
[AMA 05] AMALBERTI R., AUROY Y., BERWICK D., et al., “Five system barriers to 
achieving ultra-safe health care”, Annals of Internal Medicine, vol. 142, no. 9,  
pp. 756–764, 2005. 
[HOL 98] HOLLNAGEL E., Cognitive Reliability and Error Analysis Method: 
CREAM, Elsevier Science, Oxford, 1998. 
 

Conclusion     379 
[MIL 14] MILLOT P. (ed.), Designing Human-Machine Cooperation Systems, ISTE, 
London, and John Wiley & Sons, New York, 2014. 
[NIL 03] NILSEN T., AVEN T., “Models and model uncertainty in the context  
of risk analysis”, Reliability Engineering & System Safety, vol. 79, pp. 309–317, 
2003. 
 


 
List of Authors
 
Emmanuel ADAM 
LAMIH  
University of Valenciennes 
France 
 
Fabien BADEIG 
LIG  
University of Grenoble 
France 
 
Serge BOVERIE 
Continental Engineering Service 
Toulouse 
France 
 
Guy A. BOY 
HCDI 
Florida Institute of Technology 
Melbourne, Florida  
USA 
 
Pietro Carlo CACCIABUE 
Polytechnic University of Milan 
Italy 
 
Eric CHÂTELET 
University of Technology of Troyes 
France 
Catherine GARBAY 
LIG  
University of Grenoble 
France 
 
Makoto ITOH 
University of Tsukuba 
Japan 
 
Christophe KOLSKI 
LAMIH 
University of Valenciennes 
France 
Yoann LEBRUN 
LAMIH  
University of Valenciennes and CCI  
Grand Hainaut-PRL 
France 
Sophie LEPREUX 
LAMIH 
University of Valenciennes 
France 
Morten LIND 
Technical University of Denmark 
Copenhagen 
Denmark 

382     Risk Management in Life-Critical Systems 
René MANDIAU 
LAMIH  
University of Valenciennes 
France 
 
Patrick MILLOT 
LAMIH 
University of Valenciennes  
France 
 
Marie-Pierre PACAUX-LEMOINE 
LAMIH 
University of Valenciennes 
France 
 
Donald PLATT 
HCDI 
Florida Institute of Technology 
Melbourne, Florida  
USA 
Jean Christophe POPIEUL 
LAMIH 
University of Valenciennes  
France 
Stéphane ROMEI 
SETEC ITS 
Paris 
France 
Jean-René RUAULT 
LAMIH  
University of Valenciennes
 and
 DGA Paris 
France 
 
Kara SCHMITT 
HCDI 
Florida Institute of Technology 
Melbourne, Florida  
USA 
 
Chouki SENTOUH 
LAMIH 
University of Valenciennes 
France 
Lucas STÉPHANE 
HCDI 
Florida Institute of Technology 
Melbourne, Florida  
USA 
 
Frédéric VANDERHAEGEN 
LAMIH 
University of Valenciennes 
France 
 
René VAN PAASSEN 
Technical University of Delft 
Netherlands

 
Index
A, B 
abstraction hierarchy, 11, 97 
accidents, 2, 7, 16, 23, 41–47, 79, 81, 
87, 122, 123, 142, 151, 183, 186, 
189, 233, 242, 358, 366 
adaptability, 5, 14, 31, 267, 268 
adaptive cruise control (ACC), 322, 
342 
augmentative, 267, 289–291, 294, 
297 
authority, 9, 10, 152, 214, 216, 225, 
266–271, 281, 282, 289, 329, 336, 
339, 353 
automated vehicle, 335 
autonomy, 7, 166, 174, 185, 265, 
268, 289 
blinking, 239 
body movement, 324, 325 
business continuity, 1, 2, 5, 6 
C 
common frame of reference 
(COFOR), 259, 261, 293, 307 
common work space (CWS), 260, 
281, 302, 353 
complacency, 9, 14, 259, 282, 353 
complexity, 1, 4, 7–11, 13, 15, 17, 
34, 79, 81, 82, 88, 90, 95, 96, 102, 
110, 125, 129, 132, 144, 153, 204, 
213, 257, 281, 282, 305, 361 
compliance, 211, 213, 217, 220, 229, 
364 
coordination, 3, 17, 25, 43, 51, 189, 
227, 268, 290, 353, 354, 358, 361–
365 
coordinator, 291, 292 
creativity, 13, 14 
crisis management, 1– 8, 14, 25, 30, 
34, 36, 47, 353, 355, 357, 366 
D 
danger, 15, 24, 26, 29, 32, 33, 175, 
190 
debative, 267, 290, 292, 294, 297 
decision-making, 1, 5, 8, 10, 23, 36, 
50, 122, 125, 126, 128, 152, 162, 
211, 216, 222, 230, 234, 248, 260, 
279, 281–283, 302, 311, 316, 330, 
353 
design, 4–6, 9, 12, 13, 15, 23, 49, 50, 
52, 80, 82, 84, 89, 92, 95–98, 102, 
103, 109, 111, 117–123, 142, 143, 
152, 184, 185, 189, 191, 202, 204–
206, 213, 223, 227–230, 235, 241, 
245, 256, 262, 266, 268, 285, 286, 
296, 301, 308, 309, 311, 313, 317, 

384     Risk Management in Life-Critical Systems 
319, 320, 323, 328–330, 336, 337, 
340, 342, 345, 346, 348, 362–364 
diagnosis, 12, 103, 259, 261, 281, 
291 
dissonance, 157–170, 172–176, 178 
engineering, 165, 179 
distraction, 142, 218, 235, 242–245, 
248, 325, 326, 331, 337, 344 
drowsiness, 233, 235, 236, 238, 320, 
323, 324, 331, 336, 340 
dynamic task allocation, 255 
E, F, G 
efficiency, 17, 31, 45, 95, 240, 258, 
282, 306, 315, 352 
emergency management, 1–3, 6–8, 
14 
emergency, 1–3, 6–8, 11, 12, 14, 27, 
41, 42, 44–47, 50–52, 54, 81, 83, 
86, 92, 145, 148, 204, 214, 218, 
228, 234, 270, 289, 329, 335, 339, 
340, 343, 344, 348, 358 
emergent properties, 9 
emotion image, 167, 172–174 
event tree, 119, 134, 136–138, 149, 
153 
failure mode, 133 
fatigue, 235, 236, 241, 320, 331, 337 
function allocation (FA), 211 
functional modeling (FM), 97 
golden hour, 41, 42, 44, 47 
gravity of the situation, 54 
H, I 
haptic, 240, 246, 260, 270, 331, 340, 
341 
hazard, 9, 33, 82, 86, 89, 109, 117, 
119, 132 
human behavior model, 124, 126, 
130 
human centered design, 14, 228, 229, 
230 
human reliability, 122, 123, 132, 158 
human reliably assessment (HRA), 
124 
initiating  
events, 23, 121 
hazard, 121, 132 
integrative, 267, 289, 290, 291, 294, 
295, 297 
interactive  
 surface, 351, 352, 354, 355, 360, 
366 
 tabletop, 355 
K, M, N, O, P 
know-how-to-cooperate (KHC), 293 
maturity, 7, 8, 16, 91 
mitigation, 5, 24, 28, 32, 45, 86, 88, 
89, 120, 221, 301 
motivational aspects, 128, 132 
negotiation, 270, 289 
operational level, 6, 16, 264, 265, 288 
organization, 2, 4–6, 8, 9, 14, 15, 25, 
29–31, 87, 104, 121, 131, 192, 228, 
229, 281, 282, 286–289, 297, 353, 
362 
passivity, 282 
performance, 4, 30, 32, 52, 79, 80, 
83, 93, 124–126, 130, 131, 133, 
137, 138, 140, 161, 166, 184, 190, 
200, 201, 207, 214, 217, 219, 226, 
227, 236, 238, 241–244, 257, 269, 
279, 282, 286, 288, 305, 321, 322, 
323, 325, 343, 346 
prevention, 2, 3, 30, 107, 117, 162, 
227, 279, 329, 330 
procedure use analysis, 211, 215 
procedure, 7, 8, 10, 12, 13, 80, 85–
87, 90, 93, 140, 144, 211–227, 229, 
279, 280, 281, 343, 344 
protection system, 109, 110 

Index     385 
R, S 
recovery, 2–8, 10, 16, 30–32, 44, 
139, 153, 162, 178, 188, 196, 279 
redundancy, 8, 16, 31, 32, 189 
reinforcement of knowledge, 160, 
165 
reliability, 3, 4, 15, 26, 124, 132, 134, 
135, 158, 183, 185, 198, 199, 205, 
267, 321, 323, 352 
resilience, 5, 7, 9, 21–23, 25, 28–33, 
35, 36, 43, 44, 158, 161 
risk, 1–5, 7–15, 21–28, 31–36, 44, 
46, 80, 82, 86–89, 92, 93, 104, 109, 
117, 119–124, 126, 129–133, 139–
143, 145, 149, 152, 153, 157, 161, 
162, 166, 179, 212, 224, 243, 246, 
247, 255, 259, 288, 306, 309, 315, 
317, 327, 352, 353, 355, 356–358, 
360, 361, 362, 365, 366 
analysis, 21, 23, 26, 27, 31–36, 
117, 122, 130, 133, 153, 157, 162, 
166, 179 
assessment, 4, 10, 14, 24–26, 28, 
35, 92, 104, 123, 124, 129, 131, 
132, 139, 143 
risky scenario, 117, 118, 119 
SA distribution, 281, 287–289, 291–
293, 297 
safety, 3, 4, 7–9, 11–13, 16, 17, 26, 
27, 29, 30, 34, 41, 42, 45, 52, 79–
93, 95, 96, 103–105, 109, 117–123, 
130, 131, 134, 137, 139, 140, 142, 
143, 145, 149, 151, 152, 158, 178, 
183, 184–186, 189, 211, 213, 218, 
220, 221, 224, 226, 228, 229, 233, 
235, 236, 247, 248, 263, 279, 281, 
288, 294, 304, 306, 308, 309, 311, 
312, 317, 319, 320, 329, 330, 337, 
340, 343, 358 
authorities, 122, 139 
functions, 104, 105, 109 
self-confidence, 258, 259, 281, 282 
situation awareness (SA), 258 
situation awareness global 
assessment technique (sagat), 284 
social acceptability, 25 
strategic level, 47, 264, 265, 288, 
291, 294, 349 
system integration, 4, 80, 85, 87, 93 
T, V, W 
tactical level, 264, 265, 288, 293 
tangible interaction, 351, 352, 354, 
355, 366 
task  
distribution, 281, 286, 287, 290, 
291, 292, 293, 294, 297 
sharing, 267, 268, 271, 336, 353 
tolerability, 25, 119, 120 
trading of authority, 329, 330, 331 
trust, 15, 17, 205, 259, 268, 308, 320, 
321, 322, 330 
vigilance diagnostic, 236 
virtual camera (VC), 296, 303, 311 
visual scene, 13, 14 
vulnerability, 4, 9, 21, 22, 24–28, 32, 
36, 95, 158, 161 
whole-part, 11 
willingness, 259, 287 
workload, 14, 15, 17, 142, 159, 161, 
166, 257, 263, 268, 269, 280, 285, 
287, 290, 296, 305, 347, 348 
 


Lei Cheng
Zhongtao Chen
Yik-Chung Wu
Bayesian Tensor 
Decomposition 
for Signal 
Processing and 
Machine Learning
Modeling, Tuning-Free Algorithms, and 
Applications

Bayesian Tensor Decomposition for Signal
Processing and Machine Learning

Lei Cheng Â· Zhongtao Chen Â· Yik-Chung Wu
Bayesian Tensor
Decomposition for Signal
Processing and Machine
Learning
Modeling, Tuning-Free Algorithms,
and Applications

Lei Cheng
College of Information Science
and Electronic Engineering
Zhejiang University
Hangzhou, China
Yik-Chung Wu
Department of Electrical and Electronic
Engineering
The University of Hong Kong
Hong Kong, China
Zhongtao Chen
Department of Electrical and Electronic
Engineering
The University of Hong Kong
Hong Kong, China
ISBN 978-3-031-22437-9
ISBN 978-3-031-22438-6 (eBook)
https://doi.org/10.1007/978-3-031-22438-6
Â© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature
Switzerland AG 2023
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, speciï¬cally the rights of translation, reprinting, reuse
of illustrations, recitation, broadcasting, reproduction on microï¬lms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciï¬c statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional afï¬liations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
Our world is full of data, and these data often appear in high-dimensional structures,
with each dimension describing a unique attribute. Examples include data in social
sciences, medicines, pharmacology, and environmental monitoring, just to name a
few. To make sense of the multi-dimensional data, advanced computational tools,
which directly work with tensor rather than ï¬rst converting a tensor to a matrix, are
needed to unveil the hidden patterns of the data. This is where tensor decomposi-
tion models come into play. Due to the remarkable representation capability, tensor
decomposition models have led to state-of-the-art performances in many domains,
including social network mining, image processing, array signal processing, and
wireless communications.
Previous research on tensor decompositions mainly approached from an optimiza-
tion perspective, which unfortunately does not come with the capability of tensor
rank learning and requires heavy hyper-parameter tuning. While these two tasks are
important in complexity control and avoiding overï¬tting, they are often overlooked
or downplayed in current research, and assumed can be achieved by trivial opera-
tions, or somehow can be obtained from other methods. In reality, estimating the
tensor rank and a proper set of hyper-parameters usually involve exhaustive search.
This requires running the same algorithm many times, effectively increasing the
computational complexity in actual model deployment.
Another path for model learning is Bayesian methods. They provide a natural
recipe for the integration of tensor rank learning, automatic hyper-parameter deter-
mination, and tensor decomposition. Due to this unique capability, Bayesian models
and inference trigger a recent interest in tensor decompositions for signal processing
and machine learning. From these recent works, Bayesian models show comparable
or even better performance than optimization-based counterparts.
However, Bayesian methods are very different from optimization methods, with
the former learning distributions of the unknown parameters, and the latter learning
a point estimate. The process of building the models and inference algorithm deriva-
tions are fundamentally different as well. This leads to a barrier between the two
groups of researchers working on similar problems but starting from different
v

vi
Preface
perspectives. This book aims to distill the essentials of Bayesian modeling and infer-
ence in tensor research, and present a uniï¬ed view of various models. The book
addresses the needs of postgraduate students, researchers, and practicing engineers
whose interests lie in tensor signal processing and machine learning. It can be used
as a textbook for short courses on speciï¬c topics, e.g., tensor learning methods,
Bayesian learning, and multi-dimensional data analytics. Demo codes can be down-
loaded from https://github.com/leicheng-tensor/Reproducible-Bayesian-Tensor-Mat
rix-Machine-Learning-SOTA. It is our hope that by lowering the barrier to under-
standing and entering the Bayesian landscape, more ideas and novel algorithms can
be stimulated and facilitated in the research community.
This book starts by reviewing the basics and classical algorithms for tensor
decompositions, and then introduces their common challenge on rank determination
(Chap. 1). To overcome this challenge, this book develops models and algorithms
under the Bayesian sparsity-aware learning framework, with the philosophy and
key results elaborated in Chap. 2. In Chaps. 3 and 4, we use the most basic tensor
decomposition format, Canonical Polyadic Decomposition (CPD), as an example
to elucidate the fundamental Bayesian modeling and inference that can achieve
automatic rank determination and hyper-parameter learning. Both parametric and
non-parametric modeling and inference are introduced and analyzed. In Chap. 5,
we demonstrate how Bayesian CPD is connected with stochastic optimization in
order to ï¬t large-scale data. In Chap. 6, we show how the basic model can incorpo-
rate additional nonnegative structures to achieve enhanced performances in various
signal processing and machine learning tasks. Chapter 7 discusses the extension
of Bayesian methods to complex-valued data, handling orthogonal constraints and
outliers. Chapter 8 uses the direction-of-arrival estimation, which has been one of
the focuses of array signal processing for decades, as a case study to introduce the
Bayesian tensor decomposition under missing data. Finally, Chap. 9 extends the
modeling idea presented in previous chapters to other tensor decomposition formats,
including tensor Tucker decomposition, tensor-train decomposition, PARAFAC2
decomposition, and tensor SVD.
The authors sincerely thank the group members, Le Xu, Xueke Tong, and Yangge
Chen, at The University of Hong Kong for working on this topic together over the
years. This project is supported in part by the NSFC under Grant 62001309, and in
part by the General Research Fund from the Hong Kong Research Grant Council
under Grant 17207018.
Hangzhou, China
Hong Kong, China
Hong Kong, China
August 2022
Lei Cheng
Zhongtao Chen
Yik-Chung Wu

Contents
1
Tensor Decomposition: Basics, Algorithms, and Recent
Advances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Terminologies and Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.1
Scalar, Vector, Matrix, and Tensor . . . . . . . . . . . . . . . . . . . . . .
1
1.1.2
Tensor Unfolding/Matricization . . . . . . . . . . . . . . . . . . . . . . . .
2
1.1.3
Tensor Products and Norms . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Representation Learning via Tensors . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.2.1
Canonical Polyadic Decomposition (CPD) . . . . . . . . . . . . . . .
6
1.2.2
Tucker Decomposition (TuckerD) . . . . . . . . . . . . . . . . . . . . . .
7
1.2.3
Tensor Train Decomposition (TTD) . . . . . . . . . . . . . . . . . . . . .
8
1.3
Model Fitting and Challenges Ahead . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3.1
Example: Tensor CPD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.3.2
Challenges in Rank Determination . . . . . . . . . . . . . . . . . . . . . .
13
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2
Bayesian Learning for Sparsity-Aware Modeling . . . . . . . . . . . . . . . . . .
15
2.1
Bayesâ€™ Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.2
Bayesian Learning and Sparsity-Aware Learning . . . . . . . . . . . . . . . .
16
2.3
Prior Design for Sparsity-Aware Modeling . . . . . . . . . . . . . . . . . . . . .
17
2.4
Inference Algorithm Development . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.5
Mean-Field Variational Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.5.1
General Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.5.2
Tractability of MF-VI
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.5.3
Deï¬nition of MPCEF Model . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.5.4
Optimal Variational Pdfs for MPCEF Model . . . . . . . . . . . . .
31
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
3
Bayesian Tensor CPD: Modeling and Inference . . . . . . . . . . . . . . . . . . .
35
3.1
A Uniï¬ed Probabilistic Modeling Using GSM Prior . . . . . . . . . . . . .
35
3.2
PCPD-GG: Probabilistic Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.3
PCPD-GH: Probabilistic Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3.4
PCPD-GH, PCPD-GG: Inference Algorithm
. . . . . . . . . . . . . . . . . . .
44
vii

viii
Contents
3.4.1
Optimal Variational Pdfs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.4.2
Setting the Hyper-parameters . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.5
Algorithm Summary and Insights . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.5.1
Convergence Property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.5.2
Automatic Tensor Rank Learning . . . . . . . . . . . . . . . . . . . . . . .
48
3.5.3
Computational Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.5.4
Reducing to PCPD-GG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.6
Non-parametric Modeling: PCPD-MGP . . . . . . . . . . . . . . . . . . . . . . .
51
3.7
PCPD-MGP: Inference Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
4
Bayesian Tensor CPD: Performance and Real-World
Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
4.1
Numerical Results on Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . .
59
4.1.1
Simulation Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
4.1.2
PCPD-GH Versus PCPD-GG . . . . . . . . . . . . . . . . . . . . . . . . . .
60
4.1.3
Comparisons with Non-parametric PCPD-MGP . . . . . . . . . .
65
4.2
Real-World Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
4.2.1
Fluorescence Data Analytics . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
4.2.2
Hyperspectral Images Denoising . . . . . . . . . . . . . . . . . . . . . . .
73
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
5
When Stochastic Optimization Meets VI: Scaling Bayesian
CPD to Massive Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
5.1
CPD Problem Reformulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
5.1.1
Probabilistic Model and Inference
for the Reformulated Problem
. . . . . . . . . . . . . . . . . . . . . . . . .
78
5.2
Interpreting VI Update from Natural Gradient Descent
Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
5.2.1
Optimal Variational Pdfs in Exponential Family Form . . . . .
81
5.2.2
VI Updates as Natural Gradient Descent . . . . . . . . . . . . . . . . .
83
5.3
Scalable VI Algorithm for Tensor CPD . . . . . . . . . . . . . . . . . . . . . . . .
86
5.3.1
Summary of Iterative Algorithm . . . . . . . . . . . . . . . . . . . . . . . .
87
5.3.2
Further Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
5.4
Numerical Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
5.4.1
Convergence Performance on Synthetic Data . . . . . . . . . . . . .
90
5.4.2
Tensor Rank Estimation on Synthetic Data . . . . . . . . . . . . . . .
93
5.4.3
Video Background Modeling . . . . . . . . . . . . . . . . . . . . . . . . . .
97
5.4.4
Image Feature Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
6
Bayesian Tensor CPD with Nonnegative Factors . . . . . . . . . . . . . . . . . . .
103
6.1
Tensor CPD with Nonnegative Factors . . . . . . . . . . . . . . . . . . . . . . . . .
103
6.1.1
Motivating Exampleâ€”Social Group Clustering . . . . . . . . . . .
103
6.1.2
General Problem and Challenges . . . . . . . . . . . . . . . . . . . . . . .
105
6.2
Probabilistic Modeling for CPD with Nonnegative Factors . . . . . . . .
106

Contents
ix
6.2.1
Properties of Nonnegative Gaussian-Gamma Prior . . . . . . . .
106
6.2.2
Probabilistic Modeling of CPD with Nonnegative
Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
6.3
Inference Algorithm for Tensor CPD with Nonnegative
Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
6.3.1
Derivation for Variational Pdfs . . . . . . . . . . . . . . . . . . . . . . . . .
112
6.3.2
Summary of the Inference Algorithm . . . . . . . . . . . . . . . . . . .
114
6.3.3
Discussions and Insights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
6.4
Algorithm Accelerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
6.5
Numerical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
6.5.1
Validation on Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
6.5.2
Fluorescence Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . .
127
6.5.3
ENRON E-mail Data Mining . . . . . . . . . . . . . . . . . . . . . . . . . .
129
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
7
Complex-Valued CPD, Orthogonality Constraint, and Beyond
Gaussian Noises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
7.1
Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
7.2
Probabilistic Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
7.3
Inference Algorithm Development . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
7.3.1
Derivation for Q((k)), 1 â‰¤k â‰¤P . . . . . . . . . . . . . . . . . . . . .
140
7.3.2
Derivation for Q

(k)
, P + 1 â‰¤k â‰¤N
. . . . . . . . . . . . . . . .
141
7.3.3
Derivation for Q(E) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
7.3.4
Derivations for Q(Î³l),Q

Î¶i1,...,iN

, and Q(Î²) . . . . . . . . . . . . .
143
7.3.5
Summary of the Iterative Algorithm . . . . . . . . . . . . . . . . . . . .
144
7.3.6
Further Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
144
7.4
Simulation Results and Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
7.4.1
Validation on Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
7.4.2
Blind Data Detection for DS-CDMA Systems . . . . . . . . . . . .
150
7.4.3
Linear Image Coding for a Collection of Images . . . . . . . . . .
151
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
8
Handling Missing Value: A Case Study in Direction-of-Arrival
Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
8.1
Linking DOA Subspace Estimation to Tensor Completion . . . . . . . .
155
8.2
Probabilistic Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159
8.3
MPCEF Model Checking and Optimal Variational Pdfs
Derivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160
8.3.1
MPCEF Model Checking . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160
8.3.2
Optimal Variational Pdfs Derivations
. . . . . . . . . . . . . . . . . . .
163
8.4
Algorithm Summary and Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
8.5
Simulation Results and Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167

x
Contents
9
From CPD to Other Tensor Decompositions . . . . . . . . . . . . . . . . . . . . . .
169
9.1
Tucker Decomposition (TuckerD) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
9.2
Tensor Train Decomposition (TTD) . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
9.3
PARAFAC2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
9.4
Tensor-SVD (T-SVD) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
178
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182

Chapter 1
Tensor Decomposition: Basics,
Algorithms, and Recent Advances
Abstract In this chapter, we will ï¬rst introduce the preliminaries on tensors, includ-
ing terminologies and the associated notations, related multi-linear algebra, and more
importantly, widely used tensor decomposition formats. Then, we link the tensor
decompositions to the recent representation learning for multi-dimensional data,
showing the paramount role of tensors in modern signal processing and machine
learning. Finally, we review the recent algorithms for tensor decompositions, and
further analyze their common challenge in rank determination.
1.1
Terminologies and Notations
1.1.1
Scalar, Vector, Matrix, and Tensor
Plain letters (e.g., x) are used to denote scalars. The boldface lowercase (e.g., x)
and uppercase letters (e.g., X) are used for vectors and matrices, respectively. For
tensors, they are denoted by boldface calligraphic letters X.
In multi-linear algebra, the term order measures the number of indices used to
assess each data element (in scalar form). Speciï¬cally, vector x âˆˆRI is of order 1
since its element xi can be assessed via only one index. Matrix X âˆˆRIÃ—J is of order 2,
becausetwoindicesareenoughtotraverseallofitselementsXi, j.Asageneralization,
tensors are of order three or higher. An Nth order tensor X âˆˆRI1Ã—Â·Â·Â·Ã—IN utilizes N
indices to address its elements Xi1,...,iN . For illustration, we depict scalar, vector,
matrix, and tensor in Fig.1.1.
For an Nth order tensor X, addressing each element requires N indices, and
each index corresponds to a mode, which is used to generalize the concepts of rows
and columns in matrices. For example, for a third-order tensor X âˆˆRI1Ã—I2Ã—I3, given
indices i2 and i3, the vectors X:,i2,i3 are termed as mode-1 ï¬bers.
Â© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
L. Cheng et al., Bayesian Tensor Decomposition for Signal Processing and
Machine Learning, https://doi.org/10.1007/978-3-031-22438-6_1
1

2
1
Tensor Decomposition: Basics, Algorithms, and Recent Advances
Fig. 1.1 Illustration of scalar, matrix, and tensor
1.1.2
Tensor Unfolding/Matricization
Tensor unfolding/matricization aims to re-organize the ï¬bers in one mode into a
matrix. For an Nth order tensor X âˆˆRI1Ã—Â·Â·Â·Ã—IN , since it has N modes, there are N
types of unfolding, each termed as mode-n unfolding. We formally deï¬ne it as follows
and illustrate it in Fig.1.2.
Deï¬nition 1.1 (Mode-n Unfolding) Given a tensor X âˆˆRI1Ã—Â·Â·Â·Ã—IN , its
mode-n unfolding gives a matrix X(n) âˆˆRInÃ—N
k=1,kÌ¸=n Ik. Each tensor element
Xi1,...,iN is mapped to the matrix element X(n)
in, j with j = i1 + (i2 âˆ’1)I1 +
Â· Â· Â· + (inâˆ’1 âˆ’1)I1 Â· Â· Â· Inâˆ’2+(in+1 âˆ’1)I1 Â· Â· Â· In + Â· Â· Â· + (iN âˆ’1) I1 Â· Â· Â· INâˆ’1.
Tensor unfolding/matricization is one of the most important operators in tensor-
based data analytics, since it gives a â€œmatrixâ€ view to describe an Nth order tensor
data, such that fruitful results in linear algebra can be utilized. As will be seen in
the later sections, most tensor algorithms involve basic operations on the matrices
provided by â€œunfolding/matricizationâ€, and the special tensor/matrix products intro-
duced in the following subsection.
1.1.3
Tensor Products and Norms
Tensor decompositions and products are essentially built on matrix products. We
introduce the most widely used ones in this subsection. For a full list of matrix/tensor
products, readers can refer to [1]. All the tensor operations in this subsection have
been implemented in Matlab tensor toolbox [2].

1.1 Terminologies and Notations
3
Fig. 1.2 Illustration of tensor unfolding/matricization for different modes
Deï¬nition 1.2 (Kronecker Product) Given two matrices A âˆˆRI1Ã—I2 and
B âˆˆRJ1Ã—J2, their Kronecker product is deï¬ned as
A âŠ™B =
â¡
â¢â£
a11B Â· Â· Â· a1I2B
...
...
...
aI11B Â· Â· Â· aI1 I2B
â¤
â¥â¦
	



â‰œC
âˆˆRI1 J1Ã—I2 J2.
(1.1)
As seen in (1.1), the Kronecker product between A and B results in a matrix
C with enlarged dimensions. From another angle, the Kronecker product provides
an effective way to represent a large matrix C (if it satisï¬es (1.1)) by two smaller
matrices {A, B}. This product will be useful for tensor Tucker decompositions, as
will be elaborated in later sections.
Next, using the Kronecker product, we deï¬ne another important matrix product.
Deï¬nition 1.3 (Khatriâ€“Rao Product) Given two matrices A âˆˆRIÃ—R and
B âˆˆRJÃ—R, their Khatriâ€“Rao product is deï¬ned as:
A â‹„B =
A:,1 âŠ™B:,1, A:,2 âŠ™B:,2, . . ., A:,R âŠ™B:,R

âˆˆRI JÃ—R.
(1.2)

4
1
Tensor Decomposition: Basics, Algorithms, and Recent Advances
Fig. 1.3 Illustration of 1-mode product
From (1.2), it is easy to see that the Khatriâ€“Rao product performs the column-wise
Kronecker product between two matrices {A, B}. The Khatriâ€“Rao product is one of
the most critical operators in tensor canonical polyadic decomposition, which will
be elucidated in later sections.
The Hadamard product, which performs element-wise product between two matri-
ces {A, B}, is deï¬ned as follows.
Deï¬nition 1.4 (Hadamard Product) Given two matrices A âˆˆRI1Ã—I2 and
B âˆˆRI1Ã—I2, their Hadamard product is:
A âŠ›B =
â¡
â¢â£
a11b11 Â· Â· Â· a1I2b1I2
...
...
...
aI11bI11 Â· Â· Â· aI1 I2bI1 I2
â¤
â¥â¦âˆˆRI1Ã—I2.
(1.3)
Then we deï¬ne several tensor products.
Deï¬nition 1.5 (n-mode Product) The n-mode product between a ten-
sor X âˆˆRI1Ã—Â·Â·Â·Ã—IN and a matrix M âˆˆRRÃ—In results in a tensor (X Ã—n M)
âˆˆRI1Ã—Â·Â·Â·Ã—Inâˆ’1Ã—RÃ—In+1Ã—Â·Â·Â·Ã—IN , with each element being:
(X Ã—n M)i1,...,inâˆ’1,r,in+1,...,iN =
In

in=1
Mr,inXi1,...,iN .
(1.4)
An illustration of 1-mode product between a 3D tensor X and a matrix M is
given in Fig.1.3. Furthermore, the n-mode product can also be expressed as a matrix
product in terms of the mode-n unfolding,
(X Ã—n M)(n) = M Ã— X(n).
(1.5)

1.2 Representation Learning via Tensors
5
If the tensor X âˆˆRI1Ã—I2 is a matrix (or a 2D tensor), its 1-mode product with M
reduces to a matrix product, i.e., X Ã—1 M = M Ã— X, where M âˆˆRRÃ—I1. Similarly,
X Ã—2 M = X Ã— MT , where X âˆˆRI1Ã—I2, M âˆˆRRÃ—I2. Another generalization from
vector/matrix algebra is the generalized inner product.
Deï¬nition 1.6 (Generalized Inner Product) For a tensor X âˆˆRI1Ã—Â·Â·Â·Ã—IN and
a tensor Y âˆˆRI1Ã—Â·Â·Â·Ã—IN , their generalized inner product is deï¬ned as:
< X, Y >=
I1

i1=1
I2

i2=1
Â· Â· Â·
IN

in=1
Xi1,...,inYi1,...,in.
(1.6)
In data analytic tasks, the lp norm, which was deï¬ned for vectors and matrices,
frequently appears in the designs of cost functions and regularizations. For tensors,
we can generalize its deï¬nition as follows.
Deï¬nition 1.7 (lp tensor norm) For a tensor X âˆˆRI1Ã—Â·Â·Â·Ã—IN , its lp norm is:
||X||p =
â›
â
i1,...,iN
|Xi1,...,iN |p
â
â 
1/p
.
(1.7)
For p = 0, the l0 norm ||X||0 gives the number of non-zero elements (strictly
speaking l0 does not satisfy the usual norm properties), and thus acts as a measure
of sparsity. As its tightest convex surrogate, the l1 norm ||X||1 computes the sum
of absolute values of tensor X, and also can be treated as a convenient measure of
sparsity. The most widely used one is the l2 norm ||X||2, which is also called the
Frobenius norm and denoted by ||X||F.
1.2
Representation Learning via Tensors
Multi-dimensional data from various applications can be naturally represented as
tensors. To understand these data, representation learning aims at extracting low-
dimensional yet informative parameters (in terms of smaller tensors, matrices, and
vectors) fromthetensor data. It is hopedthat theextractedparameters canpreservethe
structures endowed by the physical phenomenon and reveal hidden interpretations.
To achieve this goal, tensor decompositions with various structural constraints are
developed, as illustrated in Fig.1.4. In the following subsections, we introduce three
widely used tensor decomposition formats with increasing complexity in modeling,
namely canonical polyadic decomposition (CPD), Tucker decomposition (TuckerD),
and tensor train decomposition (TTD).

6
1
Tensor Decomposition: Basics, Algorithms, and Recent Advances
Fig. 1.4 Illustration of representation learning via tensors
1.2.1
Canonical Polyadic Decomposition (CPD)
As illustrated in Fig.1.5, the CPD, also known as PARAFAC [3], decomposes tensor
data X âˆˆRI1Ã—Â·Â·Â·Ã—IN into a summation of R rank-1 tensors [3]:
X =
R

r=1
u(1)
r
â—¦Â· Â· Â· â—¦u(N)
r
	



rank-1 tensor
,
(1.8)
where â—¦denotes the vector outer product. Equation(1.8) states that the tensor X
consists of R rank-1 component tensors. If we put the vectors u(n)
1 , . . ., u(n)
R into a
factor matrices U(n) âˆˆRInÃ—R deï¬ned as:
U(n) =

u(n)
1 , . . . , u(n)
R

,
(1.9)
Equation(1.8) can be expressed as another equivalent form X = R
r=1 U(1)
:,r â—¦Â· Â· Â· â—¦
U(N)
:,r := U(1), . . . , U(N), where Â· Â· Â· is known as the Kruskal operator. Notice
that the minimum number R that makes (1.8) hold is termed as tensor rank, which
generalizes the notion of matrix rank to high-order tensors.
Tensor CPD has been found in various data analytic tasks due to its appealing
uniqueness property. Here, we present one of the most widely used sufï¬cient condi-
tions for CPD uniqueness. For other conditions that take additional structures (e.g.,
nonnegativity, orthogonality) into account, interested readers can refer to [1, 3].

1.2 Representation Learning via Tensors
7
Fig. 1.5 Illustration of a CPD for a third-order tensor
Property
1.1
(Uniqueness
condition
for
CPD
[1])
Suppose
U(1), U(2), . . ., U(N) = (1), (2), . . ., (N), and N
n=1 kn â‰¥2R+(N âˆ’1),
where ki denotes the k-rank of matrix U(i) and R is the tensor rank. Then
the following equations hold: (1) = U(1)(1), (2) = U(2)(2), â€¦,
(N) = U(N)(N) where  is a permutation matrix and the diagonal
matrix (n) satisï¬es N
n=1 (n) = IR.
In Property 1.1, the k-rank of matrix A is deï¬ned as the maximum value k such
that any k columns are linearly independent [1]. Property 1.1 states that under mild
conditions, tensor CPD is unique up to trivial scaling and permutation ambiguities.
This is one of the major differences between tensor CPD and low-rank matrix decom-
position, which is, in general, not unique unless some constraints are imposed. This
nice property has made CPD an important tool in the blind source separation and
data clustering-related tasks, as will be demonstrated in the following chapters.
1.2.2
Tucker Decomposition (TuckerD)
The CPD disregards interactions among the columns of factor matrices and requires
the factor matrices to have the same number of columns. To achieve a more ï¬‚ex-
ible tensor representation, tensor TuckerD was introduced to generalize CPD by
allowing different column numbers of factor matrices and introducing a core tensor
G âˆˆRR1Ã—Â·Â·Â·Ã—RN . Particularly, tensor TuckerD is deï¬ned as [1, 4]:
X = G Ã—1 U(1) Ã—2 U(2) Ã—3 Â· Â· Â· Ã—N U(N),
(1.10)

8
1
Tensor Decomposition: Basics, Algorithms, and Recent Advances
Fig. 1.6 Illustration of a TuckerD for a third-order tensor
where each factor matrix U(n) âˆˆRInÃ—Rn, âˆ€n. The tuple (R1, . . . , RN) is known as
multi-linear rank. An illustration of TuckerD is provided in Fig.1.6. Note that when
the core tensor G is super-diagonal and R1 = Â· Â· Â· = RN, TuckerD reduces to CPD.
Using the Kruskal operator, Tucker D can be compactly denoted by:
X = G; U(1), . . . , U(N).
(1.11)
Although TuckerD provides ï¬‚exibilities for data representation, it is not unique
in general [1, 4]. Therefore, it is frequently used in the data compression, basis
function learning, and feature extraction-related tasks, where uniqueness is not the
most important consideration.
1.2.3
Tensor Train Decomposition (TTD)
The TTD decomposes tensor data X âˆˆRI1Ã—Â·Â·Â·Ã—IN into a set of core tensors {G(n) âˆˆ
RRnÃ—InÃ—Rn+1} such that [5]
Xi1,...,iN = G(1)
:,i1,: Ã— Â· Â· Â· Ã— G(N)
:,iN ,:.
(1.12)
In (1.12), each core tensor slice G(n)
:,in,: âˆˆRRnÃ—Rn+1. Since each Xi1,...,iN is a scalar, R1
and RN+1 are both required to be 1. The tuple (R1, . . . , RN+1) is termed as TT-rank.
In quantum physics, TTD is known as a matrix-product state [5]. The TTD for a
third-order tensor is illustrated in Fig.1.7.
Due to its ï¬‚exibility, TTD with appropriately chosen TT-rank has shown supe-
rior performance in a variety of data analytic tasks, including image completion,
classiï¬cation, and neural network compression.

1.3 Model Fitting and Challenges Ahead
9
Fig. 1.7 Illustration of a TTD for a third-order tensor
1.3
Model Fitting and Challenges Ahead
Given the tensor decomposition models introduced in the last section, the next task
is to estimate the model parameters and hyper-parameters from the observed ten-
sor data. One straightforward approach is to formulate the learning problem as an
optimization problem (see Fig.1.8). Speciï¬cally, in different application contexts,
cost functions can be designed to encode our knowledge of the tensor data and the
tensor model. Constraints on model parameters can be further added to embed the
side information. The problem formulation generally appears in the form:
Fig. 1.8 Tensor-based representation learning from an optimization perspective

10
1
Tensor Decomposition: Basics, Algorithms, and Recent Advances
min
,Î· c(Y; , Î·)
s.t. gi() = 0, i = 1, . . . , I,
f j() â‰¥0, j = 1, . . . , J,
(1.13)
where c(Â·) is the cost function, e.g., least-squares function; g(Â·) denotes the function
for equality constraints; and f (Â·) is the function for inequality constraints. Y âˆˆ
RI1Ã—Â·Â·Â·Ã—IN is the observed tensor data.  includes all the model parameters, e.g.,
factor matrices of CPD, and Î· denotes the hyper-parameters, e.g., tensor rank or
regularization parameters.
In the following, we ï¬rst provide a concrete example for the tensor CPD format,
and then discuss the optimizations for tensor TuckerD and TTD.
1.3.1
Example: Tensor CPD
For the tensor CPD, as deï¬ned in Sect.1.2.1, the model parameters  = {U(n) âˆˆ
RInÃ—R}N
n=1 and the hyper-parameter Î· is the tensor rank R. Adopting the least-squares
cost function, the problem can be formulated as
min
{U(n)âˆˆRInÃ—R}N
n=1,R ||Y âˆ’U(1), . . . , U(N)||2
F.
(1.14)
Inproblem (1.14),optimizing R,whichisaninteger,isknowntobenon-deterministic
polynomial-time hard (NP-hard) [6]. Trial-and-errors or cross-validations are widely
used in practice to tune this hyper-parameter for the best data interpretation. Even
given a tensor rank R, the optimization of {U(n) âˆˆRInÃ—R} is still a non-convex prob-
lem. To tackle the non-convexity, alternating optimization is commonly used. Par-
ticularly, after ï¬xing factor matrices other than U(k), the problem can be equivalently
formulated as
min
U(k) ||Y(k) âˆ’U(k)

Nâ‹„
n=1,nÌ¸=k U(n)
T
||2
F,
(1.15)
where
Nâ‹„
n=1,nÌ¸=k U(n) = U(N) â‹„Â· Â· Â· â‹„U(k+1) â‹„U(kâˆ’1) â‹„Â· Â· Â· â‹„U(1), and â‹„stands for the
Khatriâ€“Rao product introduced in Deï¬nition 1.3. Problem (1.15) is a convex problem
with respect to variable U(k). After taking the gradient and then letting it to be zero,
it is easy to obtain:
Ë†U(k) = Y(k)

Nâ‹„
n=1,nÌ¸=k U(n)
T â€ 
,
(1.16)

1.3 Model Fitting and Challenges Ahead
11
Algorithm 1 CPD-ALS(Y, R)
Initializations: initialize U(n,0) âˆˆRInÃ—R, for n = 1, . . . , N.
Iterations: for the tth iteration (t â‰¥0),
for the kth factor matrix U(k,0), k = 1, . . . , N,
U(k, t+1) = Y(k)

Nâ‹„
n=1, nÌ¸=k U(n,s)
T 
NâŠ›
n=1,nÌ¸=k U(n,s)T U(n,s)
â€ 
,
where s = t + 1 when s â‰¤k, and otherwise s = t.
Until Convergence
where â€  denotes the Mooreâ€“Penrose pseudo inverse. Due to the property of the
Khatriâ€“Rao product, Eq.(1.16) can be simpliï¬ed as:
Ë†U(k) = Y(k)

Nâ‹„
n=1,nÌ¸=k U(n)
T 
NâŠ›
n=1,nÌ¸=k U(n)T U(n)
â€ 
.
(1.17)
Using (1.17), we only need to compute the pseudo inverse of a small matrix with
size R Ã— R, rather than a large matrix with size 
nÌ¸=k In Ã— R in (1.16).
By alternatively updating factor matrix U(k), âˆ€k, using (1.17) until convergence,
we arrive at the workhorse algorithm for a given R: alternating least-squares (ALS)
method [7], which is summarized in Algorithm 1.
The alternating optimization framework still applies even if additional structural
constraints are imposed on the factor matrices. For example, if nonnegativeness or
orthogonality constraints are added, the subproblem (1.15) becomes:
min
U(k)
Y(k) âˆ’U(k)

Nâ‹„
n=1,nÌ¸=k U(n)
T 
2
F
,
s.t. U(k) â‰¥0 (non-negativeness);
or s.t. U(k)T U(k) = IR (orthogonality).
(1.18)
As long as each subproblem (1.18) can be solved, CPD with additional structures can
be learned. Recent advances in large-scale non-convex optimizations have tackled
problems in the form of (1.18) [8, 9].
1.3.1.1
Optimizations for Tensor TuckerD and TTD
Following the general problem formulation (1.13), the learning problem of tensor
TuckerD is usually given as

12
1
Tensor Decomposition: Basics, Algorithms, and Recent Advances
Algorithm 2 TuckerD-HOOI(Y, {Rn}N
n=1)
Initializations: initialize U(n,0) âˆˆRInÃ—Rn, for n = 1, . . . , N.
Iterations: for the tth iteration (t â‰¥0),
for the kth factor matrix U(k,0), k = 1, . . . , N,
Ct
(k) = Y(k) 
U(N, tâˆ’1) âŠ™Â· Â· Â· âŠ™U(k+1, tâˆ’1) âŠ™U(kâˆ’1, t) âŠ™Â· Â· Â· âŠ™U(1, t)
.
(1.20)
U(k, t) â†Rk leading left singular vectors of Ct
(k).
Until Convergence
Algorithm 3 TT-SVD(Y, Ïµ)
Initializations: compute truncation parameter Î´ =
Ïµ
âˆšNâˆ’1 âˆ¥Yâˆ¥F. Set C = Y, R0 = 1.
For n = 1 to N âˆ’1
C = reshape

C,

Rnâˆ’1In,
N
n=1 In
Rnâˆ’1In

.
Compute Î´-truncated SVD: C = USV + E, âˆ¥Eâˆ¥â‰¤Ïµ, Rn = rankÎ´(C).
G(n) = reshape
 
U,

Rnâˆ’1, Ik, Rn
!
.
C = SVT .
End
G(N) = C, RN = 1.
Return {G(n)}N
n=1, {Rn}N
n=1.
min
G,{U(n)}N
n=1,{Rn}N
n=1
âˆ¥Y âˆ’G; U(1), . . . , U(N) âˆ¥F
s.t. U(n)T U(n) = IRn,
(1.19)
where the orthogonal constraint is imposed on each factor matrix. Once again, we can
use the ALS approach to solve problem (1.19). That is, in each iteration, we optimize
one factor matrix (or core tensor) at a time while ï¬xing other unknown parameters to
the most recent updated values. The resulting algorithm, also known as higher order
orthogonal iteration (HOOI), is summarized in Algorithm 2. Interested readers can
ï¬nd the detailed derivation and convergence analysis in [10].
Similarly, for the TTD format, the learning problem becomes:
min
{G(n)}N
n=1,{Rn}N
n=1
âˆ¥Y âˆ’X({G(n)}N
n=1) âˆ¥F,
(1.21)
where X({G(n)}N
n=1) is a TT-format tensor following (1.12). TT-SVD, which iter-
atively optimizes the TT cores by truncated SVDs, can yield TT cores {G(n)}N
n=1
such that
Y âˆ’X({G(n)}N
n=1)

F â‰¤Ïµ âˆ¥Yâˆ¥F, where Ïµ is the prescribed accuracy. The
TT-SVD algorithm is summarized as Algorithm 3 [5].

References
13
1.3.2
Challenges in Rank Determination
From Algorithms 1, 2 and 3, it can be seen that in addition to the tensor data
Y, a number of hyper-parameters are required to be set, including tensor rank R in
Algorithm 1, multi-linear rank {Rn}N
n=1 in Algorithm 2, and the prescribed accuracy
Ïµ in Algorithm 3. Notice that in Algorithm 3, the prescribed accuracy Ïµ determines
TT-rank {Rn}N
n=1. Setting these ranks to large values would lead to overï¬tting, while
setting them too small would lead to inï¬‚exibility of the modeling abilities. Manually
tuning these hyper-parameters is computationally costly. In many previous works
(e.g., [8]), research effort has been put into designing fast algorithms for each tensor
decomposition format. Although the reduction of running time for various tensor
decomposition formats has been witnessed, executing the algorithm numerous times
(for different combinations of hyper-parameters) is still inevitable in order to select
the best hyper-parameters. Compared to CPD, which has only a single number as
the rank, the rank determination issue for TuckerD and TTD is much more challeng-
ing, since they have more than one hyper-parameter. Exhaustively testing different
combinations of hyper-parameter values would result in a prohibitive computation
burden. This challenge raises an immediate question: Could the hyper-parameters
automatically be learned from the training data along with the factor matrices/core
tensors by running an algorithm only one time?
This thought seems arduous since it is known that the optimization of tensor rank
is NP-hard. However, recent advances in Bayesian modeling and inference [11] have
provided viable solutions, see, e.g., [12â€“18], each with a comparable computational
complexity to the optimization-based methods [8] with ï¬xed ranks. Their common
idea is to ï¬rst assume an over-parameterized model by letting the associated hyper-
parameters take large values. It then leverages the Bayesian sparsity-aware modeling
and Occamâ€™s razor principle to prune out redundant parameters [11]. Consequently,
in addition to the posterior distributions of unknown factor matrices/core tensors,
which provide additional uncertainty information compared to the optimization-
based counterparts, the Bayesian learning process also unveils the most suitable
values of hyper-parameters that can avoid overï¬tting and further improve the model
interpretability. This book aims to give a uniï¬ed view and insight into recent Bayesian
tensor decompositions.
References
1. T.G. Kolda, B.W. Bader, Tensor decompositions and applications. SIAM Rev. 51(3), 455â€“500
(2009)
2. B.W. Bader, T.G. Kolda, et al., Matlab tensor toolbox version 3.1 (2019)
3. J.H.d.M. Goulart, M. Boizard, R. Boyer, G. Favier, P. Comon, Tensor cp decomposition with
structuredfactormatrices:algorithmsandperformance.IEEEJ.SelectedTopicsSignalProcess.
10(4), 757â€“769 (2015)
4. V. Bhatt, S. Kumar, S. Saini, Tucker decomposition and applications. Mater. Today: Proc.
(2021)

14
1
Tensor Decomposition: Basics, Algorithms, and Recent Advances
5. I.V. Oseledets, Tensor-train decomposition. SIAM J. Sci. Comput. 33(5), 2295â€“2317 (2011)
6. H. Johan, Tensor rank is np-complete. J. Algor. 4(11), 644â€“654 (1990)
7. J.D. Carroll, J.-J. Chang, Analysis of individual differences in multidimensional scaling via an
n-way generalization of â€œeckart-youngâ€ decomposition. Psychometrika 35(3), 283â€“319 (1970)
8. X. Fu, N. Vervliet, L. De Lathauwer, K. Huang, N. Gillis, Computing large-scale matrix and
tensor decomposition with structured factors: a uniï¬ed nonconvex optimization perspective.
IEEE Signal Process. Mag. 37(5), 78â€“94 (2020)
9. B. Yang, A.S. Zamzam, N.D. Sidiropoulos, Large scale tensor factorization via parallel
sketches. IEEE Trans. Knowl. Data Eng. (2020)
10. C.A. Andersson, R. Bro, Improving the speed of multi-way algorithms: part i. tucker3.
Chemom. Intell. Lab. Syst. 42(1â€“2), 93â€“103 (1998)
11. S. Theodoridis, Machine Learning: a Bayesian and Optimization Perspective, 2nd edn. (Aca-
demic, Cambridge, 2020)
12. L. Cheng, X. Tong, S. Wang, Y.-C. Wu, H.V. Poor, Learning nonnegative factors from tensor
data: probabilistic modeling and inference algorithm. IEEE Trans. Signal Process. 68, 1792â€“
1806 (2020)
13. L. Xu, L. Cheng, N. Wong, Y.-C. Wu, Overï¬tting avoidance in tensor train factorization and
completion: prior analysis and inference, in International Conference on Data Mining (ICDM)
(2021)
14. L. Cheng, Y.-C. Wu, H.V. Poor, Scaling probabilistic tensor canonical polyadic decomposition
to massive data. IEEE Trans. Signal Process. 66(21), 5534â€“5548 (2018)
15. L. Cheng, Y.-C. Wu, H.V. Poor, Probabilistic tensor canonical polyadic decomposition with
orthogonal factors. IEEE Trans. Signal Process. 65(3), 663â€“676 (2016)
16. Y. Zhou, Y.-M. Cheung, Bayesian low-tubal-rank robust tensor factorization with multi-rank
determination. IEEE Trans. Pattern Anal. Mach. Intell. 62â€“76 (2019)
17. Z. Zhang, C. Hawkins, Variational bayesian inference for robust streaming tensor factorization
and completion, in Proceeding of the IEEE International Conference on Data Mining (ICDM)
(2018), pp. 1446â€“1451
18. Q. Zhao, L. Zhang, A. Cichocki, Bayesian CP factorization of incomplete tensors with auto-
matic rank determination. IEEE Trans. Pattern Anal. Mach. Intell. 37(9), 1751â€“1763 (2015)

Chapter 2
Bayesian Learning for Sparsity-Aware
Modeling
Abstract In this chapter, we will ï¬rst introduce the Bayesian philosophy, under
which the two essential merits, namely uncertainty quantiï¬cation and model selec-
tion, are highlighted. These merits shed light on the design of sparsity-promoting
prior for automating the model pruning in recent machine learning models, includ-
ing deep neural networks, Gaussian processes, and tensor decompositions. Then,
we introduce the variational inference framework for algorithm development and
discuss its tractability in different Bayesian models.
2.1
Bayesâ€™ Theorem
Bayesâ€™ theorem is the ï¬rst profound triumph of statistical inference [1], since it ele-
gantly establishes a framework for combining prior experience with the data obser-
vations.
In a typical inverse problem [2], for a given dataset D, we hope to extract the
knowledge from D via a machine learning model parameterized by a vector Î¸.
In the Bayesian framework, a prior distribution p(Î¸|Î¾p) is assumed for the model
parameters Î¸, encoding our belief before observing the data, where Î¾p denotes an
unknown yet deterministic hyper-parameter vector of the prior distribution. After
collecting the data D, the likelihood function p(D|Î¸, Î¾l), where Î¾l is the hyper-
parameters, is used to model the forward problem. While p(D|Î¸, Î¾l) links the data
and the model, our ultimate task is to learn the parameters Î¸ from data D, which
is encoded in the posterior distribution p(Î¸|D, Î¾l, Î¾p). Bayesâ€™ theorem rigorously
formulates such a process [1]:
p(Î¸|D, Î¾) =
p(D|Î¸, Î¾l)p(Î¸|Î¾p)

p(D|Î¸, Î¾l)p(Î¸|Î¾p)dÎ¸ ,
(2.1)
where Î¾ includes both the hyper-parameters contained in prior (i.e., Î¾p), and hyper-
parameters of the likelihood function Î¾l (e.g., the noise power in the dataset D).
Â© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
L. Cheng et al., Bayesian Tensor Decomposition for Signal Processing and
Machine Learning, https://doi.org/10.1007/978-3-031-22438-6_2
15

16
2
Bayesian Learning for Sparsity-Aware Modeling
Bayesâ€™ theorem stated in (2.1) reveals how to link the inverse problem (posterior
distribution p(Î¸|D, Î¾)) to the forward problem (likelihood function p(D|Î¸, Î¾l)) via
incorporating the prior p(Î¸|Î¾p). As the likelihood is usually easy to obtain from the
task objective or the measurement process, the most critical step lies in seeking a
suitable prior, which will be covered in the next section.
The denominator of (2.1), which can be expressed as p(D|Î¾), plays an important
role. This term is known as â€œmodel evidenceâ€ since it measures the â€œï¬tnessâ€ of data D
given the model hyper-parameters Î¾, which is crucial for model selection. Concretely,
rather than manually tuning those hyper-parameters on a separated validation dataset,
we can learn their optimal values directly from the training dataset by solving the
following problem:
max
Î¾
log p(D|Î¾).
(2.2)
This problem is known as the model evidence maximization problem [2].
Bayesâ€™ theorem returns the posterior distribution of model parameters, p(Î¸|D, Î¾).
Unlike the discriminative approach (or cost function optimization approach), a dis-
tribution (rather than a single point estimate) of the model parameter is obtained,
thus conveying much richer information for the downstream process of unseen data
[2]. One of the most important pieces of information possessed is the uncertainty of
model parameters, which shows the extent of our belief in the learned model. This
information is invaluable for many mission-critical applications like auto-driving [3]
and medical diagnosis [4].
2.2
Bayesian Learning and Sparsity-Aware Learning
Bayesian learning is a class of machine learning methods that rely on Bayesâ€™ theo-
rem [2, 5]. Following the principle of Bayesâ€™ theorem introduced in the last section,
Bayesian learning involves two crucial stages: (1) prior design for the model param-
eters [6]; and (2) the inference algorithm development that learns both the posterior
distribution and the hyper-parameter values from the training (or observed) data.
In this monograph, we focus on sparsity-aware learning (SAL), which aims at
leveraging the explicit or implicit sparsity pattern of data to enhance the under-
standing of data [7, 8]. The history of SAL dates back to the early 1970s when
estimating the reï¬‚ected signals from seismic measurements [6]. At that time, the
notion of â€œsparsityâ€, which means that the signal to be estimated only has a few ele-
ments being non-zero, was exploited to enhance the signal estimation performance.
After the 2000s, driven by the bloom of compressive sensing, SAL has become a
focus of signal processing and machine learning research, giving rise to many strong
theoretical results and successful algorithms in various domains [7, 8].
There is a common misunderstanding of â€œsparsityâ€ that ï¬‚oats around newcomers
of signal processing and machine learning: â€œsparsityâ€ is oftentimes superï¬cially

2.3 Prior Design for Sparsity-Aware Modeling
17
understood as nulling the values of parameters. However, if the model parameters
areuniformlyshrunktobeall-zeros,themachinelearningmodelcollapsesandcannot
learn interpretable results on the data. Consequently, in addition to favoring small
values, the â€œsparsityâ€ notion also promotes a few large values of model parameters.
These few but signiï¬cant model parameters (learned via SAL) distill the essential
information from data, thus effectively avoiding overï¬tting and delivering enhanced
interpretability. More discussions can be found in [2].
With SAL being a desirable property, the next question is: how do we properly
design the prior distribution to encode the sparsity structure? Answering this question
is the ï¬rst step toward the Bayesian SAL, which has recently enabled automatic
model structure pruning in deep learning [9], Gaussian processes [10], and tensor
decompositions [11]. We elaborate on its design next.
2.3
Prior Design for Sparsity-Aware Modeling
There are two main streams for modeling sparsity in recent Bayesian studies,
namely the parametric and non-parametric ways. Here, we introduce the paramet-
ric prior modeling relying on the heavy-tailed distributions. Interested readers on
non-parametric modeling could refer to [2].
To see why the heavy-tailed distributions successfully model the sparsity, we
present the Laplacian distribution and Gaussian distribution for Î¸ âˆˆR2 in Fig.2.1. It
can be seen that the Laplacian distribution is heavy-tailed compared to the Gaussian
Fig. 2.1 Joint probability distribution of the model parameters in 2D space. Subï¬gure a shows the
Laplacian distribution and subï¬gure b shows the Gaussian distribution. The heavy-tailed Laplacian
distribution peaks sharply around zero and falls slowly along the axes, thus promoting sparse
solutions in a probabilistic manner. On the contrary, the Gaussian distribution decays more rapidly
along both dimensions than the Laplacian distribution

18
2
Bayesian Learning for Sparsity-Aware Modeling
distribution. In other words, for Gaussian distributed parameters, the probability that
they take non-zero values goes to zero very fast, and most of the probability mass
concentrates around zero. This is not ideal for sparsity modeling since we want most
of the values to be (close to) zero, but still, some of the parameters have large values.
In contrast, observe that for the Laplacian distributed parameters, although most of
the probability mass is close to zero, there is still a high enough probability for non-
zero values. More importantly, this probability mass is concentrated along the axes,
where one of the parameters is zero. This is how Laplacian prior promotes sparsity.
In statistics, Laplacian is not the only distribution having heavy tails. In the
sequel, we introduce an important Gaussian scale mixture (GSM) family which
has heavy tails and thus can be used as sparsity-promoting priors. For a vector
Î¸ = [Î¸1, . . . , Î¸L], the main idea of the GSM prior is to assume that (a) the param-
eters, Î¸l, l = 1, 2, . . . , L, are mutually statistically independent; (b) each one of
them follows a Gaussian prior with zero mean; and (c) the respective variances,
Î¶l, l = 1, 2, . . . , L, are also random variables, each one following a prior p(Î¶l|Î¾p),
where Î¾p is a set of tuning hyper-parameters associated with the prior. Thus, the
GSM prior for each Î¸l is expressed as
p(Î¸l|Î¾p) =

N(Î¸l|0, Î¶l)p(Î¶l|Î¾p)dÎ¶l.
(2.3)
By varying the functional forms of p(Î¶l|Î¾p), the marginalization performed in (2.3)
induces different prior distributions on Î¸l. For example, if p(Î¶l|Î¾p) is an inverse
Gamma distribution, (2.3) induces a Studentâ€™s t distribution [12]; if p(Î¶l|Î¾p) is
a Gamma distribution, (2.3) induces a Laplacian distribution [12]. Table2.2 sum-
marizes different heavy-tailed distributions in the GSM family, including Normal-
Jefferys, generalized hyperbolic, and horseshoe distributions, among others. To illus-
trate the sparsity-promoting property of the GSM family, in addition to the Laplacian
distribution shown in Fig.2.1, we depict two more representative GSM prior distri-
butions, namely the Studentâ€™s t distribution and the horseshoe distribution, in Fig.2.2
(Table2.1).
Table 2.1 Examples of GSM prior. Abbreviations: Ga = Gamma, IG = inverse Gamma, GIG =
generalized inverse Gaussian, C+ = Half Cauchy
GSM prior p(Î¸l|Î¾p)
Mixing distribution p(Î¶l|Î¾p)
Studentâ€™s t
Inverse Gamma: p(Î¶l|Î¾p = {a, b}) = IG(Î¶l|a, b)
Normal-Jefferys
Log-uniform: p(Î¶l|Î¾p = { }) âˆ
1
|Î¶l|
Laplacian
Gamma: p(Î¶l|Î¾p = {a, b}) = gamma(Î¶l|a, b)
Generalized hyperbolic
Generalized inverse Gaussian:
p(Î¶l|Î¾p = {a, b, Î»}) = GIG(Î¶l|a, b, Î»)
Horseshoe
Î¶l = Ï„lÏ…l, Î¾p = {a, b}
Half Cauchy: p(Ï„l) = C+(0, a)
p(Ï…l) = C+(0, b)

2.4 Inference Algorithm Development
19
Fig. 2.2 Representative GSM prior distributions in 2D space. Subï¬gures a and b show the Studentâ€™s
t distribution and the horseshoe distribution, respectively. It can be seen that these two distributions
show different heavy-tailed proï¬les but are both sparsity-promoting
Table 2.2 Examples of exponential family distributions
Exponential family distribution Natural parameter
Sufï¬cient statistic
Univariate Gaussian
distribution N(x|Î¼, Ïƒ)

Î¼
Ïƒ 2 ; âˆ’1
2Ïƒ 2


x; x2
Multivariate Gaussian
distribution N(x|Î¼, )

âˆ’1Î¼; âˆ’1
2vec(âˆ’1)


x; vec(xxT )

Gamma distribution
gamma(x|a, b)

âˆ’b; a âˆ’1


x; log x

2.4
Inference Algorithm Development
Given a signal processing or machine learning task with the associated likelihood
function p(D|Î¸, Î¾l) and a sparsity-promoting prior p(Î¸|Î¾p), the goal of Bayesian
SAL is to infer the posterior distribution p(Î¸|D, Î¾) and to obtain the model hyper-
parameters Î¾ by maximizing the evidence p(D|Î¾).
In most cases, the multiple integrations required in computing the evidence are
analytically intractable. Inspired by the ideas of the Minorize-Maximization (also
called Majorization-minimization (MM)) optimization framework, we can seek for
a tractable lower bound that minorizes the evidence function (see discussion around
(2.2)),andmaximizethelowerbounditerativelyuntilconvergence.Ithasbeenshown,
see, e.g., [2], that such an optimization process can obtain a stationary point of the

20
2
Bayesian Learning for Sparsity-Aware Modeling
evidence function. More concretely, the logarithm of the evidence function is lower
bounded as follows:
log p(D|Î¾) â‰¥L(Q(Î¸); Î¾),
(2.4)
where the lower bound
L(Q(Î¸); Î¾) â‰œ

Q(Î¸) log p(D, Î¸|Î¾)
Q(Î¸)
dÎ¸,
(2.5)
is called evidence lower bound (ELBO), and Q(Î¸) is known as the variational dis-
tribution. The tightness of the ELBO is determined by the closeness between the
variational distribution Q(Î¸) and the posterior p(Î¸|D, Î¾), measured by the Kullbackâ€“
Leibler (KL) divergence, KL (Q(Î¸)||p(Î¸|D, Î¾)). In fact, the ELBO becomes tight
(i.e., thelower boundbecomes equal totheevidence) if andonlyif Q(Î¸) = p(Î¸|D, Î¾)
or equivalently KL (Q(Î¸)||p(Î¸|D, Î¾)) = 0. This is easy to see if we expand (2.5)
and reformulate it as
log p(D|Î¾) = L(Q(Î¸); Î¾) + KL(Q(Î¸)||p(Î¸|D|Î¾)).
(2.6)
As the KL divergence is nonnegative, the equality in (2.4) holds if and only if the
KL divergence is equal to zero.
Since the ELBO in (2.5) involves two arguments, namely Q(Î¸) and Î¾, solving the
maximization problem
max
Q(Î¸),Î¾ L(Q(Î¸); Î¾)
(2.7)
can provide both an estimate of the model hyper-parameters Î¾ and the variational
distributions Q(Î¸). These two blocks can be optimized in an alternating fashion.
Differentstrategiesforoptimizing Q(Î¸)andÎ¾ resultindifferentinferencealgorithms.
For example, the variational distribution Q(Î¸) can be optimized either via functional
optimization, or via the Monte Carlo method, while the hyper-parameters Î¾ can be
optimized via various non-convex optimization methods.
We have argued that the ELBO will be maximized when KL divergence equals
zero or equivalently Q(Î¸) = p(Î¸|D, Î¾). But this brings us back to the intractable
multiple integration problem we face in the ï¬rst place. Therefore, in variational
inference, certain restriction will be applied to Q(Î¸). A widely adopted restriction is
the mean-ï¬eld approximation [13], which will be introduced in the following section.
2.5
Mean-Field Variational Inference
2.5.1
General Solution
Since optimizing Q(Î¸) in (2.7) given Î¾ is usually intractable, a widely adopted
approximation to the variational pdf Q(Î¸) is the mean-ï¬eld family Q(Î¸) =

2.5 Mean-Field Variational Inference
21
K
k=1 Q(Î¸k), where  Î¸k = Î¸ and  Î¸k = âˆ…. That is, the unknown set Î¸ is partitioned
into exhaustive but non-overlapping subsets {Î¸k}K
k=1. With the mean-ï¬eld equality
constraint being incorporated into the objective function, problem (2.7) is written as
(Î¾ is ï¬xed);
max
{Q(Î¸k)}K
k=1
âˆ’ln p (D) + EK
k=1 Q(Î¸k) [ln p (Î¸, D)] âˆ’EK
k=1 Q(Î¸k)
	
ln
K

k=1
Q (Î¸k)

.
(2.8)
Although problem (2.8) is not jointly convex with respect to variational pdfs
{Q(Î¸k)}K
k=1, it is convex with respect to a single variational pdf Q(Î¸k) when the
others {Q(Î¸ j)} jÌ¸=k are ï¬xed. This inspires the use of the coordinate descent algo-
rithm in seeking the optimal {Qâˆ—(Î¸k)}K
k=1. That is, with {Q(Î¸ j)} jÌ¸=k being ï¬xed, the
optimal Qâˆ—(Î¸k) is obtained by solving;
min
Q(Î¸k)

Q (Î¸k)

âˆ’E
jÌ¸=k Q(Î¸ j) [ln p (Î¸, D)] + ln Q (Î¸k)

dÎ¸k
s.t.

Q(Î¸k)dÎ¸k = 1 , Q(Î¸k) â‰¥0.
(2.9)
For this convex problem, the Karushâ€“Kuhnâ€“Tucker (KKT) condition gives the opti-
mal variational pdf Qâˆ—(Î¸k) as
Qâˆ—(Î¸k) =
exp

E
jÌ¸=k Q(Î¸ j) [ln p (Î¸, D)]


exp

E
jÌ¸=k Q(Î¸ j) [ln p (Î¸, D)]

dÎ¸k
.
(2.10)
In Eq.(2.10), it is seen that the computation of variational pdf Qâˆ—(Î¸k) relies on
the statistics of other variables in Î¸. Therefore, each Qâˆ—(Î¸k) is updated alternatively.
Since Qâˆ—(Î¸k) in (2.10) exactly solves the problem (2.9) at each iteration, a stationary
point of the KL divergence is reached after convergence.
2.5.2
Tractability of MF-VI
While the general rule of mean-ï¬eld VI seems simple, it is seen from (2.10) that the
exact functional form of Qâˆ—(Î¸k) is not explicit unless the detailed probabilistic model
p (Î¸, D) is speciï¬ed and the expectation computations are carried out. Unfortunately,
the calculation in (2.10) might not be easy, as it involves a possibly intractable
integration in the denominator. This variability of mean-ï¬eld VI generally poses
great difï¬culty in probabilistic modeling and algorithm development.

22
2
Bayesian Learning for Sparsity-Aware Modeling
Fig. 2.3 Part of a Bayes
network showing a node Y,
the parents and children of
Y, and the co-parents Y with
respect to a child node X
[14]
However, there are special cases in which the optimal variational pdfs {Qâˆ—(Î¸k)}K
k=1
in (2.10) follow predictable patterns. To better illustrate these cases, we ï¬rstly provide
readers with background knowledge of Bayes networks. A Bayes network represents
a set of random variables and their conditional probabilities via a directed graph with-
out cycles. In particular, the joint distribution of all random variables X = {Xi}N
i=1
is
p(X) =
N

i=1
p(Xi|pa(Xi)),
(2.11)
where pa(Xi) denotes the parents of node Xi. An illustration of parents, children,
and co-parents is presented in Fig.2.3.
Next, we introduce exponential family distribution and conjugacy, which is essen-
tial for yielding closed-form optimal variational pdfs for Bayes networks. Examples
of exponential family distributions are listed in Table2.2.
Deï¬nition 2.1 (Exponential family distribution) A random variable x is in
exponential family distribution if its distribution admits the following form:
p(x|Î·) = h(x) exp

n(Î·)T t(x) âˆ’a(Î·)

,
(2.12)
where Î· is a vector parameterizing the distribution, h(x) is the normalizing
constant, n(Î·) is called the natural parameter, t(x) is the sufï¬cient statistic,
and a(Î·) is the log-partition function.

2.5 Mean-Field Variational Inference
23
Deï¬nition 2.2 (Conjugacy) A prior probability distribution p(Î·) is conjugate
tothelikelihoodfunction p(D|Î·),iftheposteriordistribution p(Î·|D)shares
the same parametric form as that of p(Î·).
In a Bayes network, if the prior distribution is an exponential family distribution, and
the likelihood function is an exponential form containing a linear term with respect
to the sufï¬cient statistic in the prior distribution, the prior distribution is conjugate
to the likelihood function. This is formally stated in Proposition 2.1.
Proposition 2.1 Given the prior distribution p(Y|pa(Y)) in exponential fam-
ily,
p(Y|pa(Y)) = hY(Y) exp

nY(pa(Y))T tY(Y) âˆ’aY(pa(Y))

.
(2.13)
If the likelihood function p(X|Y, cpY) can be expressed as
p(X|Y, cp(Y)) = exp

nX,Y(X, cp(Y))T tY(Y) âˆ’Î»(X, cp(Y))

,
(2.14)
the prior distribution p(Y|pa(Y)) is conjugate to the likelihood function
p(X|Y, cp(Y)).
Proof The posterior distribution p(Y|X, pa(Y), cp(Y)) is calculated as
p(Y|X, pa(Y), cp(Y))
âˆp(X|Y, cp(Y))p(Y|pa(Y))
âˆhY(Y) exp

nY(pa(Y)) + nX,Y(X, cp(Y))
T
tY(Y) âˆ’aY(pa(Y)) âˆ’Î»(X, cp(Y))

,
(2.15)
which shares the same parametric form as that of prior distribution p(Y|pa(Y)), but
conditional on a new natural parameter nY(pa(Y)) + nX,Y(X, cp(Y)). Therefore, the
prior (2.13) is conjugate to the likelihood function (2.14).
â–¡
With the knowledge of exponential family and conjugacy, we are now ready to
discuss tractable classes of mean-ï¬eld VI. The earliest known case that yields closed-
form optimal variational pdf is the conjugate exponential family (CEF) model in [15],
which is a two-layer Bayes network as shown in Fig.2.4a. In this model, the unknown
variable is Î¸ = {{zn}N
n=1, Î·}, consisting of the local variable zn that is only associated
with data Dn, and the global variable Î· that controls all the data. Two conditions
are assumed in this model. (1) The joint likelihood function p({Dn, zn}N
n=1|Î·) is a
member of the exponential family distribution parameterized by Î·, and (2) the prior

24
2
Bayesian Learning for Sparsity-Aware Modeling
Fig.2.4 aConjugateexponentialfamily(CEF)model.bUnivariateGaussianmodel,whichbelongs
to CEF. In this ï¬gure, the observed variables are denoted by shaded circles while the unknown
variables are denoted by unshaded circles
distribution p(Î·|Î±) is conjugate to p({Dn, zn}N
n=1|Î·) with a ï¬xed hyper-parameter Î±.
Due to this conjugacy, it was shown in [15] that closed-form optimal variational pdfs
of (2.10) exist in this model.
Example 2.1 (Univariate Gaussian model) To better illustrate the CEF, we use a
concrete example of univariate Gaussian model, as shown in Fig.2.4b. In particular,
given data D = {yn âˆˆR}N
n=1, univariate Gaussian model assumes that each observa-
tion yn is independently drawn from a univariate Gaussian distribution N(yn|x, Î²âˆ’1).
Bayesian modeling further assigns the mean variable x with a univariate Gaus-
sian prior N(x|m0, sâˆ’1
0 ) and the precision (i.e., the inverse of variance) variable
Î² with a gamma prior gamma(Î²|a0, b0). Therefore, the joint probability distribu-
tion p({yn}N
n=1, x, Î²|m0, s0, a0, b0) can be read from Fig.2.4b using the deï¬nition of
Bayes network (2.11),
p({yn}N
n=1, x, Î²|m0, s0, a0, b0) =
N

n=1
p(yn|pa(yn))p(x|pa(x))p(Î²|pa(Î²))
=
N

n=1
N(yn|x, Î²âˆ’1)N(x|m0, sâˆ’1
0 )gamma(Î²|a0, b0).
(2.16)
To verify such a model is within CEF, we need to check the two conditions in
the deï¬nition of CEF. For the ï¬rst condition of the CEF model, we need to prove
that p({yn}N
n=1|x, Î²) is in the exponential family. To show this, we note that the joint
likelihood function is

2.5 Mean-Field Variational Inference
25
p({yn}N
n=1|x, Î²) =
N

n=1
N(yn|x, Î²âˆ’1)
= exp
â›
â
	
Î²x
âˆ’Î²
2
T 	N
n=1 yn
N
n=1 y2
n

+ 1
2 N(ln Î² âˆ’Î²x2 âˆ’ln 2Ï€)
â
â .
(2.17)
We can identify that p({yn}N
n=1|x, Î²) takes the form of (2.12), with natural parame-
ter n(x, Î²) =

Î²x; âˆ’Î²
2

, sufï¬cient statistic t({yn}N
n=1) =
N
n=1 yn; N
n=1 y2
n

, and
a(x, Î²) = âˆ’1
2 N(ln Î² âˆ’Î²x2 âˆ’ln 2Ï€). Therefore, p({yn}N
n=1|x, Î²) is in the exponen-
tial family and the ï¬rst condition of CEF is satisï¬ed.
Next, we verify the second condition of CEF, which is to show the priors of x
and Î² are both conjugate to the likelihood function in (2.17). To prove the prior is
conjugate to the likelihood function, we can utilize Proposition 2.1, which states
that the conjugacy holds if the prior is in the exponential family and the likelihood
function admits the form in (2.14). In particular, for random variable x, the univariate
Gaussian prior is in the exponential family,
p(x|m0, s0) = exp
â›
â
	
m0s0
âˆ’s0
2
T 	
x
x2

+ 1
2(ln s0 âˆ’s0m2
0 âˆ’ln 2Ï€)
â
â ,
(2.18)
with sufï¬cient statistic tx(x) =
x; x2
, as summarized in Table2.2. We rewrite the
joint likelihood function (2.17) with respect to random variable x,
p({yn}N
n=1|x, Î²) = exp
â›
â
	
Î² N
n=1 yn
âˆ’NÎ²
2
T 	 x
x2

+ 1
2
N

n=1
(ln Î² âˆ’Î²y2
n âˆ’ln 2Ï€)
â
â .
(2.19)
It is seen that (2.19) is in the form of (2.14), where the natural parameter
n{yn}N
n=1,x({yn}, Î²) =

Î² N
n=1 yn; âˆ’NÎ²
2

. Its exponent contains the linear product
of sufï¬cient statistic tx(x) and natural parameter n{yn}N
n=1,x({yn}, Î²). Therefore, by
Proposition 2.1, the prior N(x|m0, sâˆ’1
0 ) is conjugate to the likelihood function
p({yn}N
n=1|x, Î²).
Similarly, for random variable Î², the gamma prior is an exponential family dis-
tribution with sufï¬cient statistic t(Î²) =
Î²; ln Î²
,
p(Î²|a0, b0) = exp
 âˆ’b0
a0 âˆ’1
T  Î²
ln Î²

+ a0 ln b0 âˆ’(a0)

.
(2.20)
The joint likelihood function (2.17) can rewritten with respect to Î² as

26
2
Bayesian Learning for Sparsity-Aware Modeling
p({yn}N
n=1|x, Î²) = exp
â›
âœâ
â¡
â£âˆ’N
n=1
1
2 y2n + x N
n=1 yn âˆ’N
2 x2
N
2
â¤
â¦
T 	
Î²
ln Î²

âˆ’1
2 N ln 2Ï€
â
âŸâ ,
(2.21)
which coincides with (2.14), with the natural parameter n{yn}N
n=1,Î²({yn}N
n=1, x) =

âˆ’N
n=1
1
2 y2
n + x N
n=1 yn âˆ’N
2 x2; N
2

and Î»({yn}N
n=1, x) = 1
2 N ln 2Ï€. Therefore,
the prior gamma(Î²|a0, b0) is conjugate to likelihood function by Proposition 2.1,
and thus the second condition of CEF is satisï¬ed.
To see how the MF-VI framework yields closed-form optimal variational pdfs
for the univariate Gaussian model, here we demonstrate the computation of Qâˆ—(x)
and Qâˆ—(Î²) using (2.10) under the mean-ï¬eld Q(x, Î²) = Q(x)Q(Î²). To derive the
optimal variational pdf Qâˆ—(x), we extract the terms related to x in the joint probability
distribution (2.16),
p({yn}N
n=1|x, Î²)p(x|m0, s0)
âˆexp
â›
â
	
Î² N
n=1 yn
âˆ’NÎ²
2
T 	
x
x2

+ 1
2
N

n=1
(ln Î² âˆ’Î²y2
n âˆ’ln 2Ï€)
â
â 
Ã— exp
m0s0
âˆ’s0
2
T  x
x2

+ 1
2(ln s0 âˆ’s0m2
0 âˆ’ln 2Ï€)

âˆexp
â›
â
	
Î² N
n=1 yn + m0s0
âˆ’NÎ²
2 âˆ’s0
2
T 	
x
x2
â
â ,
(2.22)
where we utilize (2.19) in the second line and (2.18) in the third line. By substituting
(2.22) into (2.10), it can be derived that the optimal variational pdf
Qâˆ—(x) âˆexp
â›
â
	
E[Î²] N
n=1 yn + m0s0
âˆ’NE[Î²]
2
âˆ’s0
2
T  x
x2
â
â ,
(2.23)
which is a univariate Gaussian distribution, since its sufï¬cient statistic is
x; x2
.
Furthermore, by matching the natural parameter in Table2.2, the variance of Qâˆ—(x)
is (NE[Î²] + s0)âˆ’1 and the mean is (NE[Î²] + s0)âˆ’1(E[Î²] N
n=1 yn + m0s0).
For random variable Î², by utilizing (2.20) and (2.21), the terms related to Î² in
(2.16) are

2.5 Mean-Field Variational Inference
27
Fig. 2.5 b Multilayer hierarchical model. b1 The conjugate relationship for the hierarchical model
and b2 the conjugate relationship in the MPCEF model. In this ï¬gure, the observed variables are
denoted by shaded circles while the unknown variables are denoted by unshaded circles
p({yn}N
n=1|x, Î²)p(Î²|a0, b0)
âˆexp
â›
â
	
âˆ’N
n=1
1
2 y2
n + x N
n=1 yn âˆ’N
2 x2
N
2
T 	
Î²
ln Î²

âˆ’1
2 N ln 2Ï€
â
â 
Ã— exp
â›
â
	
âˆ’b0
a0 âˆ’1
T 	
Î²
ln Î²

+ a0 ln b0 âˆ’(a0)
â
â 
âˆexp
â›
â
	
âˆ’N
n=1
1
2 y2
n + x N
n=1 yn âˆ’N
2 x2 âˆ’b0
N
2 + a0 âˆ’1
T  Î²
ln Î²
â
â .
(2.24)
Putting (2.24) into (2.10), the optimal variational pdf Qâˆ—(Î²) = gamma(Î²|a, b),
where a = a0 + N
2 and b = b0 + 1
2(N
n=1 y2
n âˆ’2E[x] N
n=1 yn + NE[x2]).
â– 
CEF model is a two-layer model. More recently, a multilayer hierarchical model
[14] is also found to have closed-form VI property. Figure2.5b1 shows this hierar-
chical model, which considers a variable Î·(s)
m in the sth layer. Its parent variables are
grouped in the set pa(Î·(s)
m ), and the variable Î·(sâˆ’1)
l
is one of the children variables
of Î·(s)
m , with other co-parent variables denoted by the set cp(Î·(sâˆ’1)
l
). In [14], it was
shown that when the conditional pdfs p(Î·(sâˆ’1)
l
|cp(Î·(sâˆ’1)
l
), Î·(s)
m ) and p(Î·(s)
m |pa(Î·(s)
m ))
are conjugate pdf pairs in the exponential family, closed-form variational pdfs also
exist.
Although the CEF model [15] and the hierarchical model in [14] cover many
popular models, such as the factor model, hidden Markov model, and Boltzmann
machine, there are many recent models that do not fall into these two classes. For
example, variational relevance vector machine (RVM), low-rank matrix estimation,

28
2
Bayesian Learning for Sparsity-Aware Modeling
blind separation and decomposition, interference mitigation in wireless communica-
tions, sparse and massive channel estimation, and tensor canonical polyadic decom-
position do not obey the conjugacy required by [15] or [14]. But interestingly, after
tedious derivations using (2.10), the optimal variational pdfs in these works can all
be derived in closed form. One may wonder if this is just a coincidence or if there is
something common among these models that gives closed-form optimal variational
pdfs {Qâˆ—(Î¸k)}K
k=1. This subsection will reveal the latter is true.
In fact, the models in these works belong to another class of multilayer models.
This multilayer model also employs conjugate pdf pairs in the exponential family so
that closed-form VI updates can be guaranteed. However, it takes a different form of
conjugacy compared to the model in [14]. In particular, this new multilayer model,
showninFig.2.5b2,assumesthatforavariableÎ·(s)
m inthesthlayer,theconditionalpdf
p(ch(Î·(s)
m )|Î·(s)
m ) is conjugate to the pdf p(Î·(s)
m |pa(Î·(s)
m )), where pa(Î·(s)
m ) and ch(Î·(s)
m )
stand for the parent set and children set of Î·(s)
m , respectively. Since this model only
involves partial conjugacy among pdfs in adjacent layers, we term it as multilayer
partial conjugate exponential family (MPCEF) model. In the next subsection, we
introduce the formal deï¬nition of the MPCEF model.
2.5.3
Deï¬nition of MPCEF Model
The MPCEF model satisï¬es the following conditions:
Condition 1. For each variable Î·(1)
l
in Layer 1, with the remaining unknown variables
{Î·(1)
j } jÌ¸=l in Layer 1 being ï¬xed, the likelihood function p(D|Î·(1)
l , {Î·(1)
j } jÌ¸=l) with
respect to Î·(1)
l
lies in the exponential family, and its expression can be written as1
p(D|Î·(1)
l , {Î·(1)
j } jÌ¸=l) = exp
"
n(D, {Î·(1)
j } jÌ¸=l)T t(Î·(1)
l ) âˆ’Î»(D, {Î·(1)
j } jÌ¸=l)
#
.
(2.25)
Furthermore, the prior distribution p(Î·(1)
l |pa(Î·(1)
l )) conditional on its parents pa(Î·(1)
l )
is in the exponential family and it takes the following form:
p(Î·(1)
l |pa(Î·(1)
l )) = exp
"
n(pa(Î·(1)
l )T t(Î·(1)
l ) âˆ’Î»(pa(Î·(1)
l )
#
.
(2.26)
That is, the prior distribution p(Î·(1)
l |pa(Î·(1)
l )) is conjugate to the likelihood function
p(D|Î·(1)
l ) (see Proposition 2.1).
Condition 2. For each variable Î·(s)
m in Layer S > s > 1 with at least one parent, the
distribution of its children variables ch(Î·(s)
m ) conditioned on itself is an exponential
family distribution, and can be expressed as
1 For simplicity, we use the overloaded notation for the function n(Â·), t(Â·), and Î»(Â·). Their detailed
functional forms depend on their arguments.

2.5 Mean-Field Variational Inference
29
Fig. 2.6 Probabilistic model of a relevance vector machine and b matrix decomposition
p(ch(Î·(s)
m )|Î·(s)
m , cp(Î·(s)
m )) = exp
"
n(ch(Î·(s)
m ), cp(Î·(s)
m ))T t(Î·(s)
m ) âˆ’Î»(ch(Î·(s)
m ), cp(Î·(s)
m ))
#
.
(2.27)
Its prior distribution with respect to its parent variables pa(Î·(s)
m ) is in the exponential
family, and it can be written as
p(Î·(s)
m |pa(Î·(s)
m )) = exp
$
n(pa(Î·(s)
m ))T t(Î·(s)
m ) âˆ’Î»(pa(Î·(s)
m ))
%
,
(2.28)
which
indicates
that
the
prior
p(ch(Î·(s)
m )|Î·(s)
m , cp(Î·(s)
m ))
is
conjugate
to
p(Î·(s)
m |pa(Î·(s)
m )).
Condition 3. Any variable without a parent is a known quantity.
For models satisfying conditions 1â€“3, they all belong to the MPCEF model, and
special cases can be found in various applications. Here we review two popular
models.
Example 2.2 (Relevance vector machine) Relevance vector machine (RVM) [2]
adopts a probabilistic model depicted in Fig.2.6a. In the RVM model, the likeli-
hood function is p(y|w, Î²) = N
&
y|Xw, Î²âˆ’1IN
'
, and the prior distributions of w
and Î² are p(w|{Î³l}L
l=1) = L
l=1 N
&
wl|0, Î³ âˆ’1
l
'
and p(Î²|Î±Î²) = gamma
&
Î²|Î±Î², Î±Î²
'
,
respectively. Furthermore, a hyper-prior p({Î³l}L
l=1|Î»Î³ ) = L
l=1 gamma
&
Î³l| Î»Î³ , Î»Î³
'
is imposed on {Î³l}L
l=1. In this model, y âˆˆRNÃ—1 denotes the observation vector to
be ï¬tted, X âˆˆRNÃ—M collects the feature vectors, w âˆˆRMÃ—1 is the model parameter
vector, and Î² âˆˆR is the noise precision (i.e., inverse of variance).

30
2
Bayesian Learning for Sparsity-Aware Modeling
The observed data are D = {X, y}. The Layer 1 variables are Î·(1)
1
= w, Î·(1)
2
= Î².
Starting
from
variable
Î·(1)
1
= w,
the
Gaussian
joint
likelihood
function
p(X, y|w, Î²) = N(y|Xw, Î²âˆ’1IN) with respect to Î·(1)
1
= w is in the form of (2.25),
p(X, y|w, Î²) = exp
â›
â
	
Î²XT y
âˆ’1
2vec(Î²XXT )
T 	
w
vec(wwT )

+ 1
2(N log Î² âˆ’Î²yT y âˆ’N log 2Ï€)
â
â ,
(2.29)
with n(X, y, Î²) =

Î²XT y; âˆ’1
2vec(Î²XXT )

, t(w) =
w; vec(wwT )
, and Î»(X,
y, Î²) = âˆ’1
2(N log Î² âˆ’Î²yT y âˆ’N log 2Ï€). Its prior distribution conditioned on its
parent variables {Î³l}L
l=1 takes the form of (2.26),
p(w|{Î³l}L
l=1) = exp
â›
â
	
0MÃ—1
âˆ’1
2vec(diag{Î³1, . . . , Î³L)}
T 	
w
vec(wwT )

+ 1
2
L

l=1
(log Î³ âˆ’log 2Ï€)
â
â ,
(2.30)
where n({Î³l}L
l=1) =

0MÃ—1; âˆ’1
2vec(diag{Î³1, . . . , Î³L)}

, t(w) =
w; vec(wwT )
, and
Î»({Î³l}L
l=1) = âˆ’1
2
L
l=1(log Î³ âˆ’log 2Ï€). Therefore, Condition 1 of MPCEF is satis-
ï¬ed for Layer 1 variable w.
Similarly, for Layer 1 variable Î·(1)
2
= Î², the Gaussian joint likelihood with respect
to Î·(1)
2
= Î² takes the form of (2.25),
p(X, y|w, Î²) = exp
â›
â
	
âˆ’1
2(y âˆ’Xw)T (y âˆ’Xw)
N
2
T 	
Î²
log Î²

âˆ’N
2 log 2Ï€
â
â .
(2.31)
Furthermore, its prior distribution is in the form of (2.26),
p(Î²|Î±Î²) = exp
 âˆ’Î±Î²
Î±Î² âˆ’1
T  Î²
log Î²

+ Î±Î² ln Î±Î² âˆ’ln (Î±Î²)

,
(2.32)
which veriï¬es Condition 1 of MPCEF for Î². Since there are only two variables in
Layer 1, Condition 1 holds.
For Layer 2 variable Î·(2)
l
= Î³l, its children distribution conditioned on its parent
Î³l and co-parents {Î³ j}L
j=1, jÌ¸=l takes the form of (2.27),
p(w|Î³l, {Î³ j}L
j=1, jÌ¸=l) = exp
â›
â
	
âˆ’1
2w2
l
1
2
T 	
Î³l
log Î³l

âˆ’
L

j=1, jÌ¸=l
1
2(Î³ jw2
j âˆ’log Î³ j) âˆ’L
2 log 2Ï€
â
â ,
(2.33)

2.5 Mean-Field Variational Inference
31
where n(w, {Î³ j}L
j=1, jÌ¸=l) =

âˆ’1
2w2
l ; 1
2

, t(Î³l) =
Î³l; log Î³l

, and Î»(w, {Î³ j}L
j=1, jÌ¸=l) =
L
j=1, jÌ¸=l
1
2(Î³ jw2
j âˆ’log Î³ j) + L
2 log 2Ï€. The prior distribution of Î·(2)
l
= Î³l admits
the form of (2.28),
p(Î³l|Î»Î³ ) = exp
â›
â
	
âˆ’Î»Î³
Î»Î³ âˆ’1
T 	
Î³l
log Î³l

+ Î»Î³ ln Î»Î³ âˆ’ln (Î»Î³ )
â
â .
(2.34)
Therefore, Condition 2 is satisï¬ed. Finally, variables Î»Î³ and Î±Î² are known quantities,
and thus Condition 3 holds. To conclude, RVM belongs to MPCEF.
â– 
Example 2.3 (Probabilistic matrix decomposition) The probabilistic model is
depicted in Fig.2.6b [2]. In this model, the likelihood function is p(Y|A, B, Î²) =
N
n=1 N
&
Y:,n|ABn,:, Î²âˆ’1IM
'
, with prior distributions of A, B, and Î² given
by p(A|{Î³l}L
l=1) = L
l=1 N(A:,l|0LÃ—1, Î³ âˆ’1
l
IL), p(B|{Î³l}L
l=1) = L
l=1 N(B:,l|0LÃ—1,
Î³ âˆ’1
l
IL), and p(Î²|Î±Î²) = gamma
&
Î²|Î±Î², Î±Î²
'
, respectively. Furthermore, a hyper-
prior p({Î³l}L
l=1|Î»Î³ ) = L
l=1 gamma
&
Î³l|Î»Î³ , Î»Î³
'
is imposed on {Î³l}L
l=1. In this model,
Y âˆˆRMÃ—N denotes the observed data matrix, while the random variables are factor
matrices A âˆˆRMÃ—L and B âˆˆRNÃ—L, and the noise precision Î² âˆˆR. With a similar
proof for RVM, the likelihood function p(Y|A, B, Î²) with respect to A and Î² is in the
form of (2.25). For variable B, notice that the likelihood function can be re-expressed
by factorizing over rows, i.e., p(Y|A, B, Î²) = M
m=1 N
&
Ym,:|BAm,:, Î²âˆ’1IN
'
, and it
can be seen that p(Y|A, B, Î²) with respect to B is also in the form of (2.25). The
remaining conditions of MPCEF can be shown similarly to RVM.
â– 
While MPCEF is a general class of model, it has not been formally articulated
in the literature, and the algorithms in previous works were derived independently.
In the following, we show that VI derivations in the MPCEF model follow a regular
pattern, and therefore the result of this chapter would facilitate the future development
of applications obeying the MPCEF model.
2.5.4
Optimal Variational Pdfs for MPCEF Model
Referring
to
Fig.2.5b,
in
the
MPCEF
model,
the
unknown
variables
Î¸ = {Î·(1)
1 , . . ., Î·(1)
L1 , Î·(2)
1 , . . ., Î·(S)
L S }. Correspondingly, the mean-ï¬eld assumption is
Q(Î¸) = 
m,s Q(Î·(s)
m ) â‰œK
k=1 Q(Î¸k) where K = S
i=1 Li with Li being the num-
ber of variables in Layer i. As introduced in Sect.2.5.1, the optimal variational pdfs
{Qâˆ—(Î·(s)
m )} can be obtained by carrying out the calculations in (2.10), and the results
for the MPCEF model are summarized in the following theorem.

32
2
Bayesian Learning for Sparsity-Aware Modeling
Theorem 2.1. Given a dataset D, and under the mean-ï¬eld assumption
Q(Î¸) = 
m,s Q(Î·(s)
m ) â‰œK
k=1 Q(Î¸k), the optimal variational pdfs of the
MPCEF model are given by the following:
(a). For each variable Î·(1)
l
in Layer 1 with at least one parent, the optimal
variational pdf Qâˆ—(Î·(1)
l ) takes the same functional form as (2.26) and its
expression is derived as
Qâˆ—(Î·(1)
l ) âˆexp

E
Î¸ j Ì¸=Î·(1)
l [n(D, {Î·(1)
j } jÌ¸=l) + n(pa(Î·(1)
l ))]T t(Î·(1)
l )

. (2.35)
(b). For each variable Î·(s)
m in Layer 1 < s < S with at least one parent,
the optimal variational pdf has the same functional form as (2.28) and its
expression is
Qâˆ—(Î·(s)
m ) âˆexp

E
Î¸ j Ì¸=Î·(s)
m [n(ch(Î·(s)
m ), cp(Î·(s)
m )) + n(pa(Î·(s)
m ))]T t(Î·(s)
m )

.
(2.36)
Proof
(a). Derivation for Qâˆ—(Î·(1)
l )
In the joint pdf p(D, Î¸), after ï¬xing variables other than Î·(1)
l , the term relevant to
Î·(1)
l
is p(D|Î·(1)
l , {Î·(1)
j } jÌ¸=l)p(Î·(1)
l |pa(Î·(1)
l )), which is the product of (2.25) and (2.26).
Substituting this term into (2.10), we have
Qâˆ—(Î·(1)
l ) âˆexp
(
E
Î¸ j Ì¸=Î·(1)
l
q(Î¸ j)

p(D|Î·(1)
l , {Î·(1)
j } jÌ¸=l)p(Î·(1)
l |pa(Î·(1)
l ))
)
âˆexp
(
E
Î¸ j Ì¸=Î·(1)
l
q(Î¸ j)

n(D, {Î·(1)
j } jÌ¸=l)T t(Î·(1)
l ) âˆ’Î»(D, {Î·(1)
j } jÌ¸=l)
+ n(pa(Î·(1)
l )T t(Î·(1)
l ) âˆ’Î»(pa(Î·(1)
l )
)
,
(2.37)
which gives rise to (2.35).
(b). Derivation for Qâˆ—(Î·(s)
m ) where 1 < s < S
Similar to (a), with other variables {Î¸k, Î¸k Ì¸= Î·(s)
m } being ï¬xed, the term relevant
to Î·(s)
m in the joint pdf p(D, Î¸) is the product of (2.27) and (2.28). By (2.10), the
optimal variational pdf is derived as

2.5 Mean-Field Variational Inference
33
Qâˆ—(Î·(s)
m ) âˆexp
(
E
Î¸ j Ì¸=Î·(s)
m q(Î¸ j)

p(ch(Î·(s)
m )|Î·(s)
m , cp(Î·(s)
m ))p(Î·(s)
m |pa(Î·(s)
m ))
)
âˆexp
(
E
Î¸ j Ì¸=Î·(s)
m q(Î¸ j)

n(ch(Î·(s)
m ), cp(Î·(s)
m ))T t(Î·(s)
m ) âˆ’Î»(ch(Î·(s)
m ), cp(Î·(s)
m ))
+ n(pa(Î·(s)
m ))T t(Î·(s)
m ) âˆ’Î»(pa(Î·(s)
m )))
)
,
(2.38)
which veriï¬es (2.36).
â–¡
Theorem 2.1 points out that if a model belongs to MPCEF, the optimal variational
pdfs {Qâˆ—(Î¸k)}K
k=1 are all in closed form. Since the computation of a variational pdf
Q(Î¸k) relies on the statistics of {Q(Î¸ j)} jÌ¸=k, {Q(Î¸k)}K
k=1 are updated cyclically for
k = 1, 2, 3, . . ., K using (2.35) or (2.36).
We next showcase the derivations of optimal variational pdfs given by
Theorem
2.1
for
the
RVM
model
(Example
2.2).
For
Layer
1
vari-
able Î·(1)
1
= w, by comparing the likelihood function (2.29) to (2.25), we
identify that n(X, y, Î²) =

Î²XT y; âˆ’1
2vec(Î²XXT )

and t(w) =
w; vec(wwT )
.
Furthermore, its prior distribution (2.30) takes the form of
(2.26) with
n({Î³l}L
l=1) =

0MÃ—1; âˆ’1
2vec(diag{Î³1, . . . , Î³L)}

, t(w) =
w; vec(wwT )
. By substi-
tuting n(X, y, Î²), n({Î³l}L
l=1), and t(w) into (2.35), it is derived that
Qâˆ—(w) âˆexp

E
Î¸ j Ì¸=Î·(1)
l [n(X, y, Î²) + n({Î³l}L
l=1)]T t(w)

âˆexp

E

Î²XT y
âˆ’1
2vec(Î²XXT ) âˆ’1
2vec(diag{Î³1, . . . , Î³L)}
T 
w
vec(wwT )

.
(2.39)
From Table2.2, it can be concluded that the optimal variational pdf is a Gaussian
distribution, i.e., Qâˆ—(w) = N(w|Î¼, ), where  =
&
E [Î²] XT X + E

diag{Î³1, . . . ,
Î³L}
'âˆ’1 and Î¼ = E [Î²] XT y. Similarly, for Layer 1 variable Î·(1)
2
= Î², the optimal
variational pdf is a gamma distribution, Qâˆ—(Î²) = gamma(Î²|a, b), where a = Î±Î² +
1
2 N and b = Î±Î² + 1
2E

(y âˆ’Xw)T (y âˆ’Xw)

.
For Layer 2 variable Î·(2)
l
= Î³l, it is seen that (2.33) admits the form of (2.27),
where n(w, {Î³ j}L
j=1, jÌ¸=l) =

âˆ’1
2w2
l ; 1
2

and t(Î³l) =
Î³l; log Î³l

. Furthermore, by
matching (2.34) with (2.28), its natural parameter is n(Î»Î³ ) =
âˆ’Î»Î³ ; Î»Î³ âˆ’1
. By
(2.36),
Qâˆ—(Î³l) âˆexp

E

n(w, {Î³ j}L
j=1, jÌ¸=l) + n(Î»Î³ ))
T t(Î³l)

âˆexp

E
âˆ’Î»Î³ âˆ’1
2w2
l
Î»Î³ âˆ’1
2
T  Î³l
log Î³l

.
(2.40)

34
2
Bayesian Learning for Sparsity-Aware Modeling
By comparing (2.40) with the gamma distribution in Table2.2, the optimal variational
pdf is found to be a gamma distribution Qâˆ—(Î³l) = gamma(Î³l|cl, dl) with cl = Î»Î³ + 1
2
and dl = Î»Î³ + 1
2E

w2
l

. The derivations of the optimal variational pdfs for proba-
bilistic matrix factorization model (Example 2.3) can be similarly achieved.
References
1. B. Efron, Bayesâ€™ theorem in the 21st century. Science 340(6137), 1177â€“1178 (2013)
2. S. Theodoridis, Machine Learning: A Bayesian and Optimization Perspective, 2nd edn. (Aca-
demic, Cambridge, 2020)
3. K.Wang,F.Li,C.-M.Chen,M.M.Hassan,J.Long,N.Kumar,Interpretingadversarialexamples
and robustness for deep learning-based auto-driving systems. IEEE Trans. Intell. Transp. Syst.
(2021)
4. I. Kononenko, Inductive and bayesian learning in medical diagnosis. Appl. Artif. Intell. Inter.
J. 7(4), 317â€“337 (1993)
5. M.E. Tipping, Bayesian inference: an introduction to principles and practice in machine learn-
ing, in Summer School on Machine Learning (Springer, Berlin, 2003), pp. 41â€“62
6. J.F. Claerbout, F. Muir, Robust modeling with erratic data. Geophysics 38(5), 826â€“844 (1973)
7. Y.C.Eldar,G.Kutyniok,CompressedSensing:Theory andApplications (Cambridge University
Press, Cambridge, 2012)
8. M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal and
Image Processing (Springer Science Business Media, Berlin, 2010)
9. K. Panousis, S. Chatzis, S. Theodoridis, Stochastic local winner-takes-all networks enable pro-
found adversarial robustness, in Proceedings of the Advances in Neural Information Processing
Systems (NeurIPS) (2022)
10. F. Yin, L. Pan, T. Chen, S. Theodoridis, Z.-Q. Luo, Linear multiple low-rank kernel based
stationary Gaussian processes regression for time series. IEEE Trans. Signal Process. 68, 5260â€“
5275 (2020)
11. L. Cheng, Z. Chen, Q. Shi, Y.-C. Wu, S. Theodoridis, Towards ï¬‚exible sparsity-aware modeling:
automatic tensor rank learning using the generalized hyperbolic prior. IEEE Trans. Signal
Process. 1(1), 1â€“16 (2022)
12. D.F. Andrews, C.L. Mallows, Scale mixtures of normal distributions. J. R. Stat. Soc.: Ser. B
(Methodological) 36(1), 99â€“102 (1974)
13. C. Zhang, J. BÃ¼tepage, H. KjellstrÃ¶m, S. Mandt, Advances in variational inference. IEEE Trans.
Pattern Anal. Mach. Intell. 41(8), 2008â€“2026 (2018)
14. J. Winn, C.M. Bishop, T. Jaakkola, Variational message passing. J. Mach. Learn. Res. 6(4)
(2005)
15. M.J. Beal, Variational Algorithms for Approximate Bayesian Inference (University of London,
University College London (United Kingdom), 2003)

Chapter 3
Bayesian Tensor CPD: Modeling
and Inference
Abstract Having introduced the basic philosophies of Bayesian sparsity-aware
learning in the last chapter, we formally start our Bayesian tensor decomposition
journey in this chapter. For a pedagogical purpose, the ï¬rst treatment is given on
the most fundamental tensor decomposition format, namely CPD, which has been
introduced in Chap.1. As will be demonstrated in the following chapters, the key
ideas developed for Bayesian CPD can be applied to other tensor decomposition
models, including Tucker decomposition and tensor train decomposition. Therefore,
this chapter serves as a stepping stone toward modern tensor machine learning and
signal processing. Concretely, we will ï¬rst show how the GSM family introduced
in the last chapter can be adopted for the prior modeling of tensor CPD. Then, in
this framework, we devise a Bayesian learning algorithm for CPD using the gener-
alized hyperbolic (GH) prior, and introduce its widely adopted special case, namely
Bayesian CPD using Gaussian-Gamma (GG) prior. At the end of this chapter, we
introduce a different class of probabilistic modeling, namely non-parametric model-
ing, and present multiplicative gamma process (MGP) prior as an example.
3.1
A Uniï¬ed Probabilistic Modeling Using GSM Prior
Before introducing uniï¬ed probabilistic modeling, we ï¬rst recap the deï¬nition of
tensor CPD. Given an N dimensional (N-D) data tensor Y âˆˆRI1Ã—Â·Â·Â·Ã—IN , a set of
factor matrices {U(n) âˆˆRInÃ—R} are sought via solving the following problem:
min
{U(n)}N
n=1
âˆ¥Y âˆ’
R

r=1
U(1)
:,r â—¦U(2)
:,r â—¦Â· Â· Â· â—¦U(N)
:,r



Xâ‰œU(1),U(2),...,U(N)
âˆ¥2
F,
(3.1)
where the symbol â—¦denotes the vector outer product, and the shorthand notation
Â· Â· Â· is termed as the Kruskal operator. As discussed in Chap.1, the tensor CPD
aims at decomposing an N-D tensor into a summation of R rank-1 tensors, with the
rth component constructed as the vector outer product of the rth columns from all
Â© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
L. Cheng et al., Bayesian Tensor Decomposition for Signal Processing and
Machine Learning, https://doi.org/10.1007/978-3-031-22438-6_3
35

36
3
Bayesian Tensor CPD: Modeling and Inference
the factor matrices, i.e., {U(n)
:,r }N
n=1. In problem (3.1), the number of columns R of
each factor matrix, also known as tensor rank, determines the number of unknown
model parameters and thus the model complexity. In practice, it needs to be carefully
selected to achieve the best performance in both recovering the noise-free signals
(e.g., in image denoising) and unveiling the underlying components (e.g., in social
group clustering). In Bayesian perspective, (3.1) can be interpreted as the Gaussian
likelihood function:
p

Y | {U(n)}N
n=1, Î²

âˆexp
	
âˆ’Î²
2 âˆ¥Y âˆ’U(1), U(2), . . ., U(N) âˆ¥2
F

,
(3.2)
where Î² denotes the precision (i.e., inverse variance) of the observation error.
In the sequel, we show how the low-rankness is embedded into the CPD model
using the GSM prior introduced in the last chapter. First, we employ an over-
parameterized model for CPD by assuming an upper bound value L of tensor rank R
with L â‰«R. The low-rankness implies that many of the L rank-1 tensors should be
zero. In other words, let all the lth columns of different factor matrices {U(n)}N
n=1 be
put into a vector ql â‰œ[U(1)
:,l ; U(2)
:,l ; . . . ; U(N)
:,l ] âˆˆR
N
n=1 In, âˆ€l; the low-rankness indi-
cates that a number of vectors in the set {ql}L
l=1 are zero vectors. To model such
sparsity, we adopt the following multivariate extension of GSM prior from the last
chapter:
p(ql) =

P
p=1 Ip

i=1
N([ql]i|0, Î¶l)p(Î¶l|Î¾n)dÎ¶l,
=

N(ql|0, Î¶lI)p(Î¶l|Î¾n)dÎ¶l,
=

N

n=1
N(U(n)
:,l |0, Î¶lI)p(Î¶l|Î¾n)dÎ¶l,
(3.3)
where [ql]i denotes the ith element of vector ql. Since the elements in ql are assumed
to be statistically independent, according to the deï¬nition of a multivariate Gaussian
distribution, we have the second and third lines of (3.3) showing the equivalent prior
modeling on the concatenated vector ql and the associated set of vectors {U(n)
:,l }N
n=1,
respectively. The mixing distribution p(Î¶l|Î¾n) can be any one listed in Table2.2
(in Chap.2). Note that in (3.3), the elements in vector ql are tied together via a
common hyper-parameter Î¶l. Once the learning phase is over, if Î¶l approaches zero,
the elements in ql will shrink to zero simultaneously, thus nulling a rank-1 tensor, as
illustrated in Fig.3.1. Since the prior distribution given in (3.3) favors zero-valued
rank-1 tensors, it promotes the low-rankness of the CPD model.

3.2 PCPD-GG: Probabilistic Modeling
37
Fig. 3.1 Illustration of sparsity-aware modeling for rank-1 tensors using GSM priors
3.2
PCPD-GG: Probabilistic Modeling
In this section, we present the probabilistic CPD modeling using Gaussian-gamma
prior, in which p(Î¶l|Î¾n) in (3.3) takes a gamma distribution. The modeling was ï¬rst
proposed in [1, 2] and widely adopted in follow-up works [3â€“9].
In tensor CPD, as shown in (3.1), the lth columns in all the factor matrices
({U(n)
:,l }N
n=1) constitute the building block of the model. Given an upper bound value
L of the tensor rank, for each factor matrix, since there are L âˆ’R columns all
being zero, sparsity-promoting priors should be imposed on the columns of each
factor matrix to encode the information of over-parameterization.1 In the pioneering
works [1, 2], assuming statistical independence among the columns in {U(n)
:,l , âˆ€n,l},
a Gaussian-gamma prior was utilized to model them as
p({U(n)}N
n=1|{Î³l}L
l=1) =
L

l=1
p({U(n)
:,l }N
n=1|Î³l) =
L

l=1
N

n=1
N(U(n)
:,l |0InÃ—1, Î³ âˆ’1
l
I In),
(3.4)
p({Î³l}L
l=1|{c0
l , d0
l }L
l=1) =
L

l=1
p(Î³l|c0
l , d0
l ) =
L

l=1
gamma(Î³l|c0
l , d0
l ),
(3.5)
where Î³l is the precision (i.e., the inverse of variance) of the lth columns {U(n)
:,l }N
n=1,
and {c0
l , d0
l } are pre-determined hyper-parameters.
1 The sparsity level of over-parameterized CPD model can be measured by Lâˆ’R
L .

38
3
Bayesian Tensor CPD: Modeling and Inference
Fig. 3.2 Univariate marginal
probability density function
in (3.6) with different values
of hyper-parameters
5
0
5
-
x
10-15
10-10
10-5
100
105
p(x)
(cl
0 = 2, dl
0 = 10-6)
(cl
0 = 1, dl
0 = 10-6)
(cl
0 = 10-6, dl
0 = 10-6)
To see the sparsity-promoting property of the above Gaussian-gamma prior, we
marginalize the precisions {Î³l}L
l=1 to obtain the marginal probability density function
(pdf) p({U(n)}N
n=1) as follows:
p

{U(n)}N
n=1

=
L

l=1
p({U(n)
:,l }N
n=1) =
L

l=1

p({U(n)
:,l }N
n=1|Î³l)p(Î³l|c0
l , d0
l )dÎ³l
=
L

l=1
	 1
Ï€

N
n=1 In
2
(c0
l + N
n=1
In
2 )
2d0
l
âˆ’c0
l (c0
l )

2d0
l + âˆ¥vec

{U(n)
:,l }N
n=1

âˆ¥2
2
âˆ’c0
l âˆ’N
n=1
In
2 ,
(3.6)
where (Â·) denotes the gamma function and vec(Â·) denotes the vectorization2 of its
argument. Equation (3.6) characterizes a multivariate studentâ€™s t distribution with
hyper-parameters {c0
l , d0
l }L
l=1. To get insights from this marginal distribution, we
illustrate its univariate case in Fig.3.2 with different hyper-parameters. It is clear
that while different hyper-parameters lead to different tail behaviors, each studentâ€™s
t pdf is strongly peaked at zero. Therefore, with a proper setting of hyper-parameters
(e.g., c0
l = d0
l = 10âˆ’6), the Gaussian-gamma prior would be sparsity-promoting as
the peak at zeros will inform the learning process to look for values around â€œzerosâ€
while the heavy tails still allow the learning process to obtain components with
2 The operation vec

{U(n)
:,l }N
n=1

simply stacks all these columns into a long vector, i.e.,
vec

{U(n)
:,l }N
n=1

= [U(1)
:,l ; U(2)
:,l ; . . . ; U(N)
:,l ] âˆˆRZÃ—1, with Z = N
n=1 In.

3.3 PCPD-GH: Probabilistic Modeling
39
Fig. 3.3 Probabilistic CPD model with Gaussian-gamma prior
large values. Furthermore, as to be shown later, {c0
l , d0
l }L
l=1 will be updated using
the observed data during inference. This provides tailored sparsity probabilities for
different rank-1 components.
The probabilistic CPD model is completed by specifying the likelihood function of
Y, given in (3.2). Equation (3.2) assumes that the signal tensor U(1), U(2), . . ., U(N)
is corrupted by additive white Gaussian noise (AWGN) tensor W with each element
having power Î²âˆ’1. This is consistent with the least-squares (LS) problem in (3.1).
However, in Bayesian modeling, Î² is modeled as another random variable. Since we
have no prior information about the noise power, a non-informative prior p(Î²) =
gamma(Î²|Ïµ, Ïµ) with a very small Ïµ (e.g., 10âˆ’6) is usually employed.
By using the introduced prior distributions and likelihood function, a probabilistic
model for tensor CPD was constructed, as illustrated in Fig.3.3. Based on this model,
a VI-based algorithm was derived in [1] that can automatically drive most of the
columnsineachfactormatrixtozero,bywhichthetensorrankisrevealed.Inspiredby
the vanilla probabilistic CPD using the Gaussian-gamma prior, other structured and
large-scale tensor CPDs with automatic tensor rank learning were further developed
[3â€“9] in recent years.
3.3
PCPD-GH: Probabilistic Modeling
The success of the previous works on automatic tensor rank learning [1, 3â€“9] comes
from the adoption of the sparsity-promoting Gaussian-gamma prior. However, their
performances are also limited by the rigid central and tail behaviors of Gaussian-

40
3
Bayesian Tensor CPD: Modeling and Inference
gamma prior in modeling different levels of sparsity. More speciï¬cally, when the
tensor rank R is low, previous empirical results have shown that a relatively large
upper bound L (e.g., set by the maximal value of tensor dimensions [1, 3]) gives
accurate tensor rank estimation. However, for a high tensor rank R, the upper bound
value L selected as in the low-rank case would be too small to render a sparsity
pattern of the columns, and thus it leads to performance degradation. Even though
we can increase the value of L to larger numbers, as it will be shown in later chapters,
tensor rank learning accuracy using Gaussian-gamma prior is still not satisfactory,
showing its lack of ï¬‚exibility to adapt to different sparsity levels. Therefore, to further
enhance the tensor rank learning capability, we explore the use of sparsity-promoting
priors with more ï¬‚exible central and tail behaviors.
In particular, we focus on the generalized hyperbolic (GH) prior (see Table2.1
for its deï¬nition), since it not only includes the Gaussian-gamma prior as a special
case, but also can be treated as the generalization of other widely used sparsity-
promoting distributions including the Laplacian distribution, normal-inverse chi-
squared distribution, normal-inverse gamma distribution, variance-gamma distribu-
tion, and Mckayâ€™s Bessel distribution. Therefore, it is expected that the functional
ï¬‚exibility of GH prior could lead to more adaptability in modeling different sparsity
levels and thus more accurate learning of tensor rank.
Recall that the model building block in a CPD is the lth column group {U(n)
:,l }N
n=1.
With the GH prior on each column group, we have a new prior distribution for factor
matrices:
p({U(n)}N
n=1)
=
L

l=1
GH({U(n)
:,l }N
n=1|a0
l , b0
l , Î»0
l )
=
L

l=1
(a0
l )
N
n=1 In
4
(2Ï€)
N
n=1 In
2
(b0
l )
âˆ’Î»0
l
2
KÎ»0
l
	
a0
l b0
l

K
Î»0
l âˆ’
N
n=1 In
2
	
a0
l

b0
l + âˆ¥vec

{U(n)
:,l }N
n=1

âˆ¥2
2


b0
l + âˆ¥vec

{U(n)
:,l }N
n=1

âˆ¥2
2

,
(3.7)
where KÂ·(Â·) is the modiï¬ed Bessel function of the second kind, and GH({U(n)
:,l }N
n=1|a0
l ,
b0
l , Î»0
l ) denotes the GH prior on the lth column group {U(n)
:,l }N
n=1, in which the hyper-
parameters {a0
l , b0
l , Î»0
l } control the shape of the distribution. By setting {a0
l , b0
l , Î»0
l }
to speciï¬c values, the GH prior (3.7) reduces to other prevalent sparsity-promoting
priors. For example:
(1) Studentâ€™s t Distribution. When a0
l â†’0 and Î»0
l < 0, it can be shown that the
GH prior (3.7) reduces to [10, 11]

3.3 PCPD-GH: Probabilistic Modeling
41
p({U(n)}N
n=1)
=
L

l=1
GH({U(n)
:,l }N
n=1|a0
l â†’0, b0
l , Î»0
l < 0)
=
L

l=1
	 1
Ï€

N
n=1 Jn
2
(Î»0
l + N
n=1
Jn
2 )
b0
l
Î»0
l (âˆ’Î»0
l )

b0
l + âˆ¥vec

{U(n)
:,l }N
n=1

âˆ¥2
2
Î»0
l âˆ’N
n=1
Jn
2 .
(3.8)
By comparing the functional form of (3.8) to that of (3.6), it is clear that pdf (3.8)
is a studentâ€™s t distribution with hyper-parameters {âˆ’Î»0
l , b0
l
2 }.
(2) Laplacian Distribution. When b0
l â†’0 and Î»0
l > 0, it can be shown that the
GH prior (3.7) reduces to [10, 11]
p({U(n)}N
n=1) =
L

l=1
GH({U(n)
:,l }N
n=1|a0
l , b0
l â†’0, Î»0
l > 0)
=
L

l=1
(a0
l )
N
n=1 Jn
4
+
Î»0
l
2
	
Ï€
N
n=1 Jn
2

 	
2
N
n=1 Jn
2
+Î»0
l âˆ’1

âˆ¥vec

{U(n)
:,l }N
n=1

âˆ¥
Î»0
l âˆ’
N
n=1 Jn
2
2
2Î»0
l 

Î»0
l

Ã— K
Î»0
l âˆ’
N
n=1 Jn
2
	
a0
l âˆ¥vec

{U(n)
:,l }N
n=1

âˆ¥2

.
(3.9)
The pdf (3.9) characterizes a generalized Laplacian distribution. By setting Î»0
l =
N
n=1 Jn
2
+ 1, (3.9) reduces to a standard Laplacian pdf:
p({U(n)}N
n=1) =
L

l=1
GH

{U(n)
:,l }N
n=1|a0
l , b0
l â†’0, Î»0
l =
N
n=1 Jn
2
+ 1

âˆ
L

l=1
(a0
l )
N
n=1 Jn
2
exp
	
âˆ’

a0
l âˆ¥vec

{U(n)
:,l }N
n=1

âˆ¥2

.
(3.10)
To visualize the GH distribution and its special cases, the univariate GH pdfs with
different hyper-parameters are illustrated in Fig.3.4. It can be observed that the blue
line is with a similar shape to those of the studentâ€™s t distributions in Fig.3.2, while
the orange one resembles the shapes of Laplacian distributions. For other lines, they
exhibit a wide range of the central and tail behaviors of the pdfs. Comparing Figs.3.2
and 3.4 reveals that the GH prior is more ï¬‚exible than the GG prior, and thus offers
more modeling capability for different levels of sparsity.
On the other hand, the GH prior (3.7) can be expressed as a GSM formulation
[10, 11]:

42
3
Bayesian Tensor CPD: Modeling and Inference
Fig. 3.4 Univariate marginal
probability density function
in (3.7) with different values
of hyper-parameters
p({U(n)}N
n=1)
=
L

l=1
GH({U(n)
:,l }N
n=1|a0
l , b0
l , Î»0
l )
=
L

l=1

N

vec

{U(n)
:,l }N
n=1

|0N
n=1 InÃ—1, zl IN
n=1 In

GIG(zl|a0
l , b0
l , Î»0
l )dzl,
(3.11)
where zl denotes the variance of the Gaussian distribution, and GIG(zl|a0
l , b0
l , Î»0
l ) is
the generalized inverse Gaussian (GIG) pdf:
GIG(zl|a0
l , b0
l , Î»0
l ) =

a0
l
b0
l
 Î»0
l
2
2KÎ»0
l
	
a0
l b0
l

z
Î»0
l âˆ’1
l
exp
	
âˆ’1
2

a0
l zl + b0
l zâˆ’1
l

.
(3.12)
This GSM formulation suggests that each GH distribution GH({U(n)
:,l }N
n=1|a0
l , b0
l , Î»0
l )
can be regarded as an inï¬nite mixture of Gaussians with the mixing distribution being
a GIG distribution. Besides connecting with the GSM framework, the formulation
(3.11) allows a hierarchical construction of each GH prior by introducing the latent
variable zl, as illustrated in Fig.3.5. Furthermore, it turns out that this hierarchical
construction possesses the conjugacy property [10], which facilitates the derivation
of the Bayesian inference algorithm later.

3.3 PCPD-GH: Probabilistic Modeling
43
Fig. 3.5 Hierarchical construction of GH distribution
Fig. 3.6 The probabilistic tensor CPD model with GH prior
Property 3.1 The prior p(zl) = GIG(zl|a0
l , b0
l , Î»0
l ) is conjugate to
p

{U(n)
:,l }N
n=1|zl

= N

vec

{U(n)
:,l }N
n=1

|0N
n=1 InÃ—1, zl IN
n=1 In

.
(3.13)
Finally, together with the likelihood function in (3.2), the probabilistic model for
tensor CPD using the hierarchical construction of the GH prior is shown in Fig.3.6.
Denoting the model parameter set  = {{U(n)}N
n=1, {zl}L
l=1, Î²}, the GH-prior-based
probabilistic tensor CPD model can be fully described by the joint pdf p(Y, ) as

44
3
Bayesian Tensor CPD: Modeling and Inference
p(Y, ) = p

Y | {U(n)}N
n=1, Î²

p

{U(n)}N
n=1|{zl}L
l=1

p

{zl}L
l=1

p(Î²)
âˆexp
N
n=1 In
2
ln Î² âˆ’Î²
2 âˆ¥Y âˆ’U(1), U(2), . . ., U(N) âˆ¥2
F
+
N

n=1

In
2
L

l=1
ln zâˆ’1
l
âˆ’1
2Tr

U(n)Zâˆ’1U(n)T 

+
L

l=1
Î»0
l
2 ln a0
l
b0
l
âˆ’ln

2KÎ»0
l
	
a0
l b0
l


+ (Î»0
l âˆ’1) ln zl
âˆ’1
2

a0
l zl + b0
l zâˆ’1
l
 
+ (Ïµ âˆ’1) ln Î² âˆ’ÏµÎ²

,
(3.14)
where Z = diag{z1, z2, . . . , zL}.
3.4
PCPD-GH, PCPD-GG: Inference Algorithm
Since PCPD-GG is a special case of PCPD-GH, in this section, we ï¬rst develop the
variational inference algorithm for PCPD-GH, and then reduce the derived algorithm
to cater to the PCPD-GG modeling.
The inference algorithm is derived based on the mean-ï¬eld variational inference
(MF-VI) framework introduced in the last chapter. Speciï¬cally, it is easy to show
that the introduced PCPD-GH/PCPD-GG model falls into the MPCEF model, thus
their optimal variational pdfs can be derived in closed form.
To make this chapter self-contained, we brieï¬‚y review the MF-VI. Given the prob-
abilistic model p(Y, ), our goal is to learn the model parameters in  from the
tensor data Y, in which the posterior distribution p(|Y) is to be sought. However,
for such a complicated probabilistic model (3.14), the multiple integrations in com-
puting the posterior distribution p(|Y) is not tractable. Rather than manipulating
a huge number of samples from the probabilistic model, VI recasts the originally
intractable multiple integration problem into the following functional optimization
problem:
min
Q() KL

Q () âˆ¥p ( | Y)

â‰œâˆ’EQ()

ln p ( | Y)
Q ()

s.t. Q() âˆˆF ,
(3.15)
where KL(Â·||Â·) denotes the Kullbackâ€“Leibler (KL) divergence between two argu-
ments, and F is a pre-selected family of pdfs. Its philosophy is to seek a tractable
variational pdf Q() in F that is the closest to the true posterior distribution p(|Y)

3.4 PCPD-GH, PCPD-GG: Inference Algorithm
45
in the KL divergence sense. Therefore, the art is to determine the family F to bal-
ance the tractability of the algorithm and the accuracy of the posterior distribution
learning.
Using the mean-ï¬eld family, which restricts Q() = K
k=1 Q(k) where  is
partitioned into mutually disjoint non-empty subsets k (i.e., k is a part of  with
âˆªK
k=1k =  and âˆ©K
k=1k = Ã˜), the KL divergence minimization problem (3.15)
becomes
min
{Q(k)}K
k=1
âˆ’E{Q(k)}K
k=1

ln
p ( | Y)
K
k=1 Q(k)

.
(3.16)
The factorable structure in (3.16) inspires the idea of block minimization from opti-
mization theory. In particular, after ï¬xing variational pdfs {Q( j)} jÌ¸=k other than
Q(k), the remaining problem is
min
Q(k)

Q(k)(âˆ’E
jÌ¸=k Q( j)

ln p(, Y)
 
+ ln Q(k))dk,
(3.17)
and it has been shown that the optimal solution is
Qâˆ—(k) =
exp

E
jÌ¸=k Q( j)

ln p (, Y)
 
!
exp

E
jÌ¸=k Q( j)

ln p (, Y)
 
dk
.
(3.18)
3.4.1
Optimal Variational Pdfs
For the probabilistic CPD, the optimal variational pdfs {Qâˆ—(k)}K
k=1 can be obtained
by substituting (3.14) into (3.18). Although straightforward as it may seem, the
involvement of tensor algebras in (3.14) and the multiple integrations in the denom-
inator of (3.18) make the derivation a challenge. On the other hand, since the prob-
abilistic model employs the GH prior, and is different from previous works using
Gaussian-gamma prior [1, 3â€“9], each optimal variational pdf Qâˆ—(k) needs to be
derived from ï¬rst principles. To keep this chapter concise, the lengthy derivations
are omitted and the details can be found in [12]. In the following, we present only
the inference results. For easy reference, the optimal variational pdfs are also sum-
marized in Table 3.1.
In particular, the optimal variational pdf Qâˆ—(U(k)) was derived to be a matrix
normal distribution MN

U(k)|M(k), I In, (k)
with the covariance matrix
(k) =

E [Î²] E
 	
NâŠ™
n=1,nÌ¸=k U(n)

T 	
NâŠ™
n=1,nÌ¸=k U(n)

 
+ E

Zâˆ’1 âˆ’1
,
(3.19)

46
3
Bayesian Tensor CPD: Modeling and Inference
Table 3.1 Optimal variational density functions
Variational pdfs
Remarks
Qâˆ—
U(k)
= MN

U(k)|M(k), I In, (k)
, âˆ€k
Matrix normal distribution
with mean M(k) and covariance matrix (k)
given in (3.19) and (3.20), respectively
Qâˆ—(zl) = GIG(zl|al, bl, Î»l), âˆ€l
Generalized inverse Gaussian distribution with
parameters {al, bl, Î»l} given in (3.21)â€“(3.23)
Qâˆ—(Î²) = gamma(Î²|e, f )
Gamma distribution with shape e and rate f
given in (3.24), (3.25)
and mean matrix
M(k) = Y(k)E [Î²]
	
NâŠ™
n=1,nÌ¸=k E

U(n)
(k).
(3.20)
In (3.19) and (3.20), Y(k) is a matrix obtained by unfolding the tensor Y along its
kth dimension, and the multiple Khatriâ€“Rao products
NâŠ™
n=1,nÌ¸=k A(n) = A(N) âŠ™A(Nâˆ’1)
âŠ™Â· Â· Â· âŠ™A(k+1) âŠ™A(kâˆ’1) âŠ™Â· Â· Â· âŠ™A(1). The expectations are taken with respect to
the corresponding variational pdfs of the arguments. For the optimal variational pdf
Q(zl), by using the conjugacy result in Property 3.1, it can be derived to be a GIG
distribution GIG(zl|al, bl, Î»l) with parameters
al = a0
l ,
(3.21)
bl = b0
l +
N

n=1
E

U(n)
:,l
T
U(n)
:,l

,
(3.22)
Î»l = Î»0
l âˆ’1
2
N

n=1
In.
(3.23)
Finally, the optimal variational pdf Q(Î²) was derived to be a gamma distribution
gamma(Î²|e, f ) with parameters
e = Ïµ + 1
2
N

n=1
In,
(3.24)
f = Ïµ + 1
2E

âˆ¥Y âˆ’U(1), . . . , U(N)âˆ¥2
F

.
(3.25)
In (3.19)â€“(3.25), there are several expectations to be computed. They can be
obtained either from the statistical literatures or similar results in related works
[1, 3â€“9]. For easy reference, we listed the expected results needed for (3.19)â€“(3.25)
in Table 3.2, where
NâŠ›
n=1,nÌ¸=k A(n) = A(N) âŠ›A(Nâˆ’1) âŠ›Â· Â· Â· âŠ›A(k+1) âŠ›A(kâˆ’1) âŠ›Â· Â· Â· âŠ›
A(1) is the multiple Hadamard products.

3.4 PCPD-GH, PCPD-GG: Inference Algorithm
47
Table 3.2 Computation results of expectations of PCDP-GH model
Expectations
Computation results
E

U(k) 
, âˆ€k
M(k), âˆ€k
E [zl] , âˆ€l

bl
al
 1
2 KÎ»l +1(
âˆšalbl)
KÎ»l (
âˆšalbl)
E

zâˆ’1
l

, âˆ€l

bl
al
âˆ’1
2 KÎ»l âˆ’1(
âˆšalbl)
KÎ»l (
âˆšalbl)
E [Î²]
e
f
E

U(n)
:,l
T
U(n)
:,l


M(n)
:,l
T
M(n)
:,l + In(n)
l,l
E
 	
NâŠ™
n=1,nÌ¸=k U(n)

T 	
NâŠ™
n=1,nÌ¸=k U(n)

 
NâŠ›
n=1,nÌ¸=k

M(n) T M(n) + In(n)
E

âˆ¥Y âˆ’U(1), . . . , U(N)âˆ¥2
F
 
âˆ¥Y âˆ¥2
F +Tr
 NâŠ›
n=1

M(n)T M(n) + In(n)
âˆ’2Tr

Y(1) NâŠ™
n=2 M(n)
M(1)T 
3.4.2
Setting the Hyper-parameters
From Table 3.1, it can be found that the shape of the variational pdf Q(U(k)) is
affected by the variational pdf {Q(zl)}L
l=1. For each Q(zl), as seen in (3.21)â€“(3.23),
its shape relies on the pre-selected hyper-parameters {a0
l , b0
l , Î»0
l }. In practice, we
usually have no prior knowledge about the sparsity level before assessing the data,
and a widely adopted approach is to make the prior non-informative.
In previous works using Gaussian-gamma prior [1, 3â€“9], hyper-parameters are
set equal to very small values in order to approach a non-informative prior. Although
nearly zero hyper-parameters lead to an improper prior, the derived variational pdf is
still proper since these parameters are updated using information from observations.
Therefore,intheseworks,thestrategyofusinganon-informativepriorisvalid.Onthe
other hand, for the employed GH prior, non-informative prior requires {a0
l , b0
l , Î»0
l }
all go to zero, which however would lead to an improper variational pdf Q(zl),
since its parameter al = a0
l is ï¬xed (as seen in (3.21)). This makes the expectation
computation E[zl] in Table 3.2 problematic.
To tackle this issue, another viable approach is to optimize these hyper-parameters
{a0
l , b0
l , Î»0
l } so that they can be adapted during the procedure of model learning.
However, as seen in (3.14), these three parameters are coupled together via the
nonlinear modiï¬ed Bessel function, and thus optimizing them jointly is prohibitively
difï¬cult. Therefore, [12] proposes to only optimize the most critical one, i.e., a0
l ,
since it directly determines the shape of Q(zl) but will not be updated in the learning
procedure. For the other two parameters {b0
l , Î»0
l }, as seen in (3.22) and (3.23), since
they are updated with model learning results or tensor dimension, according to the
Bayesian theory, their effects on the posterior distribution would become negligible

48
3
Bayesian Tensor CPD: Modeling and Inference
when the observation tensor is large enough. This justiï¬es the optimization of a0
l
while not {b0
l , Î»0
l }.
For optimizing a0
l , following related works [13], a conjugate hyper-prior p(a0
l ) =
gamma(a0
l |Îºa1, Îºa2)isintroducedtoensurethepositivenessofa0
l duringtheoptimiza-
tion. To bypass the nonlinearity from the modiï¬ed Bessel function, we set b0
l â†’0
so that KÎ»0
l
	
a0
l b0
l

becomes a constant. In the framework of VI, after ï¬xing other
variables, it has been derived in [12] that the hyper-parameter a0
l is updated via
a0
l = Îºa1 + Î»0
l
2 âˆ’1
Îºa2 + E[zl]
2
.
(3.26)
Notice that it requires Îºa1 > 1 âˆ’Î»0
l /2 and Îºa2 â‰¥0 to ensure the positiveness of a0
l .
3.5
Algorithm Summary and Insights
From the equations above, it can be seen that the statistics of each variational pdf
rely on other variational pdfs. Therefore, they need to be updated in an alternating
fashion, giving rise to an iterative algorithm summarized in Algorithm 4. To gain
more insights from the proposed algorithm, discussions on its convergence property,
computational complexity, and automatic tensor rank learning are presented in the
following.
3.5.1
Convergence Property
Notice that the algorithm is derived under the mean-ï¬eld VI framework [13, 14].
In particular, in each iteration, after ï¬xing other variational pdfs, the problem that
optimizes a single variational pdf has been shown to be convex and has a unique
solution [13, 14]. By treating each update step in mean-ï¬eld VI as a block coordinate
descent (BCD) step over the functional space, the limit point generated by the VI
algorithm is at least a stationary point of the KL divergence [13, 14].
3.5.2
Automatic Tensor Rank Learning
During the iterations, the mean of parameter zâˆ’1
l
(denoted by m[zâˆ’1
l ]) will be learned
using the updated parameters of other variational pdfs as seen in Algorithm 4. Due
to the sparsity-promoting nature of the GH prior, some of m[zâˆ’1
l ] will take very large
values, e.g., in the order of 106. Since the inverse of {m[zâˆ’1
l ]}L
l=1 contribute to the
covariance matrix of each factor matrix (k) in (3.27), which scales the columns

3.5 Algorithm Summary and Insights
49
Algorithm 4 PCPD-GH(Y, L)
Initializations: Choose L > R and initial values {

M(n) 0 ,

(n) 0}N
n=1, {m[zâˆ’1
l
]0, a0
l , b0
l , Î»0
l }L
l=1,
e0, f 0. Choose Îºa1 > âˆ’Î»0
l /2 and Îºa2 â‰¥0.
Iterations:
For the iteration t + 1 (t â‰¥0),
For k = 1, . . . , N, update the parameters of Q(U(k))t+1:

(k)t+1
=
 et
f t
NâŠ›
n=1,nÌ¸=k
 
M(n)sT 
M(n)s
+ Jn

(n)s 
+ diag
"
m[zâˆ’1
1 ]t, m[zâˆ’1
2 ]t, ..., m[zâˆ’1
L ]t# âˆ’1
,
(3.27)

M(k)t+1
= Y(k) et
f t

NâŠ™
n=1,nÌ¸=k

M(n)s
 
(n)t+1
,
(3.28)
where s denotes the most recent update index, i.e., s = t + 1 when n < k, and s = t otherwise.
Update the parameters of Q(zl)t+1:
at+1
l
= [a0
l ]t,
(3.29)
bt+1
l
= b0
l +
N

n=1
 	
M(n)
:,l
t+1
T 
M(n)
:,r
t+1
+ Jn

(n)
l,l
t+1 
,
(3.30)

Î»l
 t+1 = Î»0
l âˆ’1
2
N

n=1
Jn,
(3.31)
m[zâˆ’1
l
]t+1 =

bt+1
l
at+1
l
âˆ’1
2 K[Î»l]t+1âˆ’1
	
at+1
l
bt+1
l

K[Î»l]t+1
	
at+1
l
bt+1
l

 ,
(3.32)
m[zl]t+1 =

bt+1
l
at+1
l
 1
2 K[Î»l]t+1+1
	
at+1
l
bt+1
l

K[Î»l]t+1
	
at+1
l
bt+1
l

 .
(3.33)
Update the parameters of Q(Î²)t+1:
et+1 = Ïµ +
N
n=1 Jn
2
,
(3.34)
f t+1 = Ïµ + ft+1
2 ,
(3.35)
where ft+1 is computed using the result in the last row of Table 3.2 with {M(n), (n)} being replaced
by {

M(n) t+1 ,

(n) t+1}, âˆ€n.
Update the hyper-parameter [a0
l ]t+1:
[a0
l ]t+1 = Îºa1 + Î»0
l
2 âˆ’1
Îºa2 + m[zl]t+1
2
.
(3.36)
Until Convergence

50
3
Bayesian Tensor CPD: Modeling and Inference
in each factor matrix in (3.28), a very large m[zâˆ’1
l ] will shrink the lth column of each
factor matrix to all zero. Then, by enumerating how many non-zero columns are in
each factor matrix, the tensor rank can be automatically learned.
In practice, to accelerate the learning algorithm, on-the-ï¬‚y pruning is widely
employed in Bayesian tensor research. In particular, in each iteration, if some of
the columns in each factor matrix are found to be indistinguishable from all zeros, it
indicates that these columns play no role in interpreting the data, and thus they can be
safely pruned. This pruning procedure will not affect the convergence behavior of the
algorithm, since each pruning is equivalent to restarting the algorithm for a reduced
probabilistic model with the current variational pdfs acting as the initializations.
Note that the pruning would remove a column permanently. If the algorithm,
fortunately, jumps out from one inferior local minima, the columns once deemed
â€œirrelevanceâ€ might recover their importance. To address this, the birth process,
which is opposite to the pruning process (also called the death process), can be
adopted [13, 15]. Exploiting such schemes might further improve the tensor rank
learning capability, especially in very low SNR and/or very high tensor rank regimes.
However, from the extensive experiments, this issue does not frequently appear in a
wide range of SNRs and tensor ranks.
3.5.3
Computational Complexity
For Algorithm 4, in each iteration, the computational complexity is dominated by
updatingthefactormatrices,costing O(N N
n=1 JnL2 + L3 N
n=1 Jn).Therefore,the
computational complexity of Algorithm 4 is O(q(N N
n=1 JnL2 + L3 N
n=1 Jn))
where q is the iteration number at convergence. The complexity is comparable to
that of the inference algorithm using Gaussian-gamma prior [1].
3.5.4
Reducing to PCPD-GG
Now we show how the PCPD-GH algorithm (Algorithm 4) can be reduced to the
PCPD-GG algorithm, which is obtained from the probabilistic tensor CPD model
with GG prior (see Sect.3.2 and Fig.3.3).
When a0
l â†’0 and Î»0
l < 0, as shown in Sect.3.2, the GH prior reduces to the GG
prior. Under this setting, there is no need to update a0
l , and thus Eqs.(3.29) and (3.36)
in Algorithm 4 can be removed. With the value of a0
l goes to zero, other updating
equations are simpliï¬ed accordingly, resulting in the PCPD-GG algorithm, which is
summarized in Algorithm 5.

3.6 Non-parametric Modeling: PCPD-MGP
51
Algorithm 5 PCPD-GG(Y, L)
Initializations: Choose L > R and initial values {

M(n) 0 ,

(n) 0}N
n=1, {m[zâˆ’1
l
]0, b0
l , Î»0
l }L
l=1,
e0, f 0.
Iterations:
For the iteration t + 1 (t â‰¥0),
For k = 1, . . . , N, update the parameters of Q(U(k))t+1:

(k)t+1
=
 et
f t
NâŠ›
n=1,nÌ¸=k
 
M(n)sT 
M(n)s
+ Jn

(n)s 
+ diag
"
m[zâˆ’1
1 ]t, m[zâˆ’1
2 ]t, ..., m[zâˆ’1
L ]t# âˆ’1
,
(3.37)

M(k)t+1
= Y(k) et
f t
	
NâŠ™
n=1,nÌ¸=k

M(n)s
 
(n)t+1
,
(3.38)
where s denotes the most recent update index, i.e., s = t + 1 when n < k, and s = t otherwise.
Update the parameters of Q(zl)t+1:
bt+1
l
= b0
l +
N

n=1
 	
M(n)
:,l
t+1
T 
M(n)
:,r
t+1
+ Jn

(n)
l,l
t+1 
,
(3.39)
[Î»l]t+1 = Î»0
l âˆ’1
2
N

n=1
Jn,
(3.40)
m[zâˆ’1
l
]t+1 = âˆ’[Î»l]t+1
bt+1
l
/2
(3.41)
Update the parameters of Q(Î²)t+1:
et+1 = Ïµ +
N
n=1 Jn
2
,
(3.42)
f t+1 = Ïµ + ft+1
2 ,
(3.43)
where ft+1 is computed using the result in the last row of Table 3.2 with {M(n), (n)} being replaced
by {

M(n) t+1 ,

(n) t+1}, âˆ€n.
Until Convergence
3.6
Non-parametric Modeling: PCPD-MGP
In this section, we introduce non-parametric modeling and exemplify it using the
multiplicative gamma process (MGP). Before establishing the non-parametric mod-
eling of CPD using MGP, we review the deï¬nition of CPD and introduce another
equivalent formulation that will be utilized in this section. In particular, by explic-
itly introducing a coefï¬cient Î»r of each rank-1 component in (3.1), we arrive at an
equivalent formulation of tensor CPD:

52
3
Bayesian Tensor CPD: Modeling and Inference
min
{U(n)}N
n=1,{Î»r}R
r=1
âˆ¥Y âˆ’
R

r=1
Î»rU(1)
:,r â—¦U(2)
:,r â—¦Â· Â· Â· â—¦U(N)
:,r



Xâ‰œU(1),U(2),...,U(N);Î»
âˆ¥2
F,
(3.44)
where the shorthand notation Kruskal operator now includes the rank-1 component
coefï¬cients Î» =
Î»1, . . . , Î»R
 
to represent this formulation [16].
From (3.44), it is seen that another viable approach to achieve tensor rank learning
is to place sparsity-promoting prior on the rank-1 component coefï¬cients {Î»r}R
r=1,
rather than on the columns of factor matrices as done in previous sections. In par-
ticular, we initialize with an over-parameterized model for CPD with L â‰¥R rank-1
components. Due to the sparsity-promoting nature of the prior on the coefï¬cients, the
coefï¬cients of the redundant rank-1 components will be driven to zero. Therefore,
tensor rank learning is achieved by counting the rank-1 components with non-zero
coefï¬cients.
While the GSM prior introduced earlier can also be utilized for modeling sparsity
on rank-1 component coefï¬cients {Î»l}L
l=1, we introduce in this section another class
of sparsity-promoting modeling, namely non-parametric modeling, by using MGP
[17] as an example. Speciï¬cally, MGP is placed on rank-1 component coefï¬cients
Î»l,
p(Î»l|{Î´k}l
k=1) = N
â›
âÎ»l|0,
 l
k=1
Î´k
âˆ’1â
â ,
(3.45)
p(Î´l) = gamma(Î´l|ac, 1),
(3.46)
where ac > 1 is a pre-determined hyper-parameter. The MGP prior presented in
(3.45) and (3.46) incorporates the prior belief that the precision of Î»l is more likely
to shrink to zero as the index l increases, since the precision is a product of increasing
numbers of gamma-distributed random variables.
Given that (3.44) is a least-squares cost function, the corresponding Gaussian
likelihood function is expressed as
p(Y|{U(n)}N
n=1, Î», Î²) âˆexp
	
âˆ’Î²
2
((Y âˆ’U(1), U(2), . . . , U(N); Î»
((2
F

.
(3.47)
To complete the Bayesian modeling, the prior for factor matrices {U(n)}N
n=1 is spec-
iï¬ed as independent columns with each having zero-mean unit-variance Gaussian
distribution,
p({U(n)}N
n=1) =
N

n=1
L

l=1
N(U(n)
:,l |0InÃ—1, IIn),
(3.48)

3.7 PCPD-MGP: Inference Algorithm
53
and the precision Î² is assigned with a Gamma prior Ga(Î²|Ïµ, Ïµ), where Ïµ is chosen as a
very small number (e.g., 10âˆ’6). To summarize, the PCPD-MGP model is represented
by the joint probability density function of tensor data Y and random variables
 = {{U(n)}N
n=1, Î», {Î´l}L
l=1, Î²}:
p(Y, ) = p(Y|{U(n)}N
n=1, Î», Î²)p({U(n)}N
n=1)
L

l=1
p(Î»l|{Î´k}l
k=1)
L

l=1
p(Î´l)p(Î²).
(3.49)
3.7
PCPD-MGP: Inference Algorithm
The exact inference of random variables  = {{U(n)}N
n=1, Î», {Î´l}L
l=1, Î²} from ten-
sor data Y requires the computation of the posterior distribution p(|Y), which
is intractable due to the multiple integrations. Different from the Gibbs sam-
pling method adopted in [17], we employ here the mean-ï¬eld variational infer-
ence (MF-VI) to minimize the Kullbackâ€“Leibler divergence between the poste-
rior and the variational pdf Q(), where Q() lies in the mean-ï¬eld family, i.e.,
Q() = K
k=1 Q(k). As derived in Chap.2, the general formula of the optimal
variational pdf is
Qâˆ—(k) =
exp

E
jÌ¸=k Q(k)

ln p (Y, Î¸)
 
!
exp

E
jÌ¸=k Q( j)

ln p (Y, , )
 
dk
.
(3.50)
For PCPD-MGP model, the mean-ï¬eld family is assumed as Q() = N
n=1
Q(U(n)) Ã— L
l=1 Q(Î»l) L
l=1 Q(Î´l)Q(Î²). By substituting (3.49) into (3.50), the
optimal variational pdfs for various variables can be derived. In particular, the
optimal variational pdf Qâˆ—(U(n)) is derived to be a matrix normal distribution
MN

U(n)|M(n), IIn, (n)
with the covariance matrix
(n) =

E [Î²] E [] E
 	
NâŠ™
k=1,kÌ¸=n U(k)

T 	
NâŠ™
k=1,kÌ¸=n U(k)

 
E []
E [Î²] E

2 
diag

E
 	
NâŠ™
k=1,kÌ¸=n U(k)

T 	
NâŠ™
k=1,kÌ¸=n U(k)

 
+ IIn
âˆ’1
,
(3.51)

54
3
Bayesian Tensor CPD: Modeling and Inference
and mean matrix
M(n) = Y(n)E [Î²]
	
NâŠ™
k=1,kÌ¸=n E

U(k) 
E [] (n).
(3.52)
In (3.51) and (3.52), Y(n) is a matrix obtained by unfolding the tensor Y along its
nth dimension, the multiple Khatriâ€“Rao products
NâŠ™
k=1,kÌ¸=n A(k) = A(N) âŠ™A(Nâˆ’1) âŠ™
Â· Â· Â· âŠ™A(n+1) âŠ™A(nâˆ’1) âŠ™Â· Â· Â· âŠ™A(1), and  = diag(Î»).
On the other hand, the functional form of the optimal variational pdf Qâˆ—(Î»l)
coincides with the normal distribution N (Î»l|ml, sl) with the variance
sl =
 l
k=1
E [Î´k] + E [Î²] E

< Xl, Xl >
 
âˆ’1
,
(3.53)
and mean
ml = slE [Î²]
â›
â< Y, E

Xl
 
> âˆ’
L

k=1,kÌ¸=l
E [Î»k] E

< Xk, Xl >
 
â
â ,
(3.54)
where Xl denotes the lth rank-1 component Xl = U(1)
:,l â—¦U(2)
:,l â—¦Â· Â· Â· â—¦U(N)
:,l , and the
inner product between two tensors < X, Y > has been deï¬ned in (1.6).
Finally, the optimal variational pdf Qâˆ—(Î´l) was derived to be a Gamma distribution
gamma (Î´l|cl, dl) with the parameters
cl = ac + 1
2(L âˆ’l + 1),
(3.55)
dl = 1 + 1
2
L

k=l
E

Î»2
k
 
k
j=1, jÌ¸=l
E

Î´ j
 
.
(3.56)
Furthermore, the optimal variational pdf Qâˆ—(Î²) is also a Gamma distribution
gamma (Î²|a, b) with the parameters
a = Ïµ + 1
2
N

n=1
In,
(3.57)
b = Ïµ + 1
2E
((Y âˆ’U(1), U(2), . . . , U(N); Î»
((2
F

.
(3.58)

3.7 PCPD-MGP: Inference Algorithm
55
Table 3.3 Computation results of expectations of PCDP-MGP model
Expectations
Computation results
E

U(n) 
, âˆ€n
M(n)
E [Î»l] , âˆ€l
ml
E

Î»2
l
 
, âˆ€l
m2
l + sl
E []
diag{m1, . . . , mL} â‰œM
E

2 
diag{m2
1 + s1, . . . , m2
L + sL} â‰œS
E [Î´l] , âˆ€l
cl
dl
E

Xl
 
, âˆ€l
a
b
E

< Xk, Xl >
 
, âˆ€k,l
N
n=1 M(n)T
:,k
M(n)
:,l + In(n)
k,l
E
 	
NâŠ™
k=1,kÌ¸=n U(k)

T 	
NâŠ™
k=1,kÌ¸=n U(k)

 
NâŠ›
k=1,kÌ¸=n

M(k) T M(k) + Ik(k)
E
((Y âˆ’U(1), U(2), . . . , U(N); Î»
((2
F

âˆ¥Yâˆ¥2
F + Tr
 NâŠ›
n=2

M(n)T M(n) + In(n)
âŠ›

M

M(1)T M(1) + I1(1)
M
+Sdiag

M(1)T M(1) + I1(1) 
âˆ’2 < Y, M(1), . . . , M(N);

m1, . . . , mL

 >
In (3.51)â€“(3.58), there are several expectations to be computed, and they are
summarized in Table3.3. As discussed in previous chapters, the MF-VI algorithm
estimates the variational pdfs in an alternating fashion, since the update of each
variational pdf depends on the statistics from other variational pdfs. This gives rise to
Algorithm 6.

56
3
Bayesian Tensor CPD: Modeling and Inference
Algorithm 6 PCPD-MGP(Y, L)
Initializations: Choose L > R, ac > 1, and initial values {

M(n) 0 ,

(n) 0}N
n=1, {m0
l , s0
l }L
l=1,
a0, b0. Set M0
 = {m0
1, . . . , m0
L}, S0
 = {(m0
1)2 + s0
1, . . . , (m0
L)2 + s0
L}.
Iterations:
For the iteration t + 1 (t â‰¥0),
For n = 1, . . . , N, update the parameters of Q(U(n))t+1:

(n)t+1
=
at
bt Mt

NâŠ›
k=1,kÌ¸=n
 
M(k)sT 
M(k)s
+ Ik

(k)s 
Mt

Ã— at
bt St
diag
 
M(k)sT 
M(k)s
+ Ik

(k)s 
+ In
âˆ’1
,
(3.59)

M(n)t+1
= Y(n) at
bt
	
NâŠ™
k=1,kÌ¸=n

M(k)s
Mt


(k)t+1
,
(3.60)
where s denotes the most recent update index, i.e., s = t + 1 when n < k, and s = t otherwise.
Update the parameters of Q(Î»l)t+1:
st+1
l
=
 l
k=1
ct
l
dt
l
+ at
bt
N

n=1

M(n)
:,l
sT 
M(n)
:,l
s
+ In(n)
l,l
âˆ’1
(3.61)
mt+1
l
= st+1
l
at
bt

< Y, 

M(1)
:,l
s
, . . . ,

M(N)
:,l
s
 >
âˆ’
L

k=1,kÌ¸=l
mt
k
N

n=1

M(n)
:,k
sT 
M(n)
:,l
s
+ In(n)
k,l

.
(3.62)
Set Mt+1

= {mt+1
1
, . . . , mt+1
L
}, St+1

= {(mt+1
1
)2 + st+1
1
, . . . , (mt+1
L
)2 + st+1
L
}.
Update the parameters of Q(Î´l)t+1:
ct+1
l
= ac + 1
2(L âˆ’l + 1)
(3.63)
dt+1
l
= 1 + 1
2
L

k=l
	
mt+1
l
2
+ st+1
l

k
j=1, jÌ¸=l
cs
j
ds
j
,
(3.64)
where s denotes the most recent update index, i.e., s = t + 1 when j < l, and s = t otherwise.
Update the parameters of Q(Î²)t+1:
at+1 = Ïµ +
N
n=1 In
2
,
(3.65)
bt+1 = Ïµ + bt+1
2
,
(3.66)
where bt+1 is computed using the result in the last row of Table 3.3 with {M(n), (n)}, {ml}, M, S
being replaced by {[M(n)]t+1,

(n) t+1}, âˆ€n, {mt+1
l
}, âˆ€l, Mt+1
 , St+1
 .
Until Convergence

References
57
References
1. Q. Zhao, L. Zhang, A. Cichocki, Bayesian cp factorization of incomplete tensors with automatic
rank determination. IEEE Trans. Pattern Anal. Mach. Intell. 37(9), 1751â€“1763 (2015)
2. M. MÃ¸rup, L.K. Hansen, Automatic relevance determination for multi-way models. J.
Chemom.: J. Chemom. Soc. 23(7â€“8), 352â€“363 (2009)
3. Q. Zhao, G. Zhou, L. Zhang, A. Cichocki, S.-I. Amari, Bayesian robust tensor factorization for
incomplete multiway data. IEEE Trans. Neural Netw. Learn. Syst. 27(4), 736â€“748 (2015)
4. L. Cheng, Y.-C. Wu, H.V. Poor, Probabilistic tensor canonical polyadic decomposition with
orthogonal factors. IEEE Trans. Signal Process. 65(3), 663â€“676 (2016)
5. L. Cheng, Y.-C. Wu, H.V. Poor, Scaling probabilistic tensor canonical polyadic decomposition
to massive data. IEEE Trans. Signal Process. 66(21), 5534â€“5548 (2018)
6. L. Cheng, X. Tong, S. Wang, Y.-C. Wu, H.V. Poor, Learning nonnegative factors from tensor
data: probabilistic modeling and inference algorithm. IEEE Trans. Signal Process. 68, 1792â€“
1806 (2020)
7. Z. Zhang, C. Hawkins, Variational bayesian inference for robust streaming tensor factorization
and completion, in 2018 IEEE International Conference on Data Mining (ICDM) (IEEE, 2018),
pp. 1446â€“1451
8. J. Luan, Z. Zhang, Prediction of multidimensional spatial variation data via bayesian tensor
completion. IEEE Trans. Comput.-Aided Design Integr. Circuits Syst. 39(2), 547â€“551 (2019)
9. Q. Zhao, L. Zhang, A. Cichocki, Bayesian sparse tucker models for dimension reduction and
tensor completion (2015). arXiv:1505.02343
10. L. Thabane, M. Saï¬ul Haq, On the matrix-variate generalized hyperbolic distribution and its
bayesian applications. Statistics 38(6), 511â€“526 (2004)
11. S.D. Babacan, S. Nakajima, M.N. Do, Bayesian group-sparse modeling and variational infer-
ence. IEEE Trans. Signal Process. 62(11), 2906â€“2921 (2014)
12. L. Cheng, Z. Chen, Q. Shi, Y.-C. Wu, S. Theodoridis, Towards ï¬‚exible sparsity-aware modeling:
Automatic tensor rank learning using the generalized hyperbolic prior. IEEE Trans. Signal
Process. (2022)
13. M.J. Beal, Variational Algorithms for Approximate Bayesian Inference (University of London,
University College London (United Kingdom), 2003)
14. C. Zhang, J. BÃ¼tepage, H. KjellstrÃ¶m, S. Mandt, Advances in variational inference. IEEE Trans.
Pattern Anal. Mach. Intell. 41(8), 2008â€“2026 (2018)
15. P.J. Green, Reversible jump markov chain monte carlo computation and bayesian model deter-
mination. Biometrika 82(4), 711â€“732 (1995)
16. T.G. Kolda, B.W. Bader, Tensor decompositions and applications. SIAM Rev. 51(3), 455â€“500
(2009)
17. P. Rai, Y. Wang, S. Guo, G. Chen, D. Dunson, L. Carin, Scalable bayesian low-rank decom-
position of incomplete multiway tensors, in International Conference on Machine Learning
(PMLR, 2014), pp. 1800â€“1808

Chapter 4
Bayesian Tensor CPD: Performance
and Real-World Applications
Abstract In this chapter, extensive numerical results using synthetic datasets and
real-world datasets are presented to reveal the insights and performance of the intro-
duced algorithms in the previous chapter. Since the GH prior provides a more ï¬‚exible
sparsity-aware modeling than the Gaussian-gamma prior, it has the potential to act
as a better regularizer against noise corruption and to adapt to a wider range of
sparsity levels. Numerical studies have conï¬rmed the improved performance of the
PCPD-GH method over the PCPD-GG in terms of tensor rank learning and factor
matrix recovery, especially in the challenging high-rank and/or low-SNR regimes.
Note that the principle followed in PCDP-GG and PCPD-GH is a parametric way to
seek ï¬‚exible sparsity-aware modeling. In parallel to this path, the PCPD-MGP is a
non-parametric Bayesian CPD modeling. Due to the decaying effects of the length
scales (i.e., the variance of the rank-1 component coefï¬cient) learned through MGP,
the inference algorithm is capable of learning low tensor rank, but it has the tendency
to underestimate the tensor rank when the ground-truth rank is high, making it not
very ï¬‚exible in the high-rank regime. Numerical results will be presented in this
chapter to demonstrate this phenomenon.
4.1
Numerical Results on Synthetic Data
In this section, extensive numerical results are presented to compare the performance
of the algorithms using synthetic data. All experiments were conducted in Matlab
R2015b with an Intel Core i7 CPU at 2.2 GHz.
4.1.1
Simulation Setup
We consider 3D tensors X = A(1), A(2), A(3) âˆˆR30Ã—30Ã—30 with different tensor
ranks. Each element in the factor matrices {A(n)}3
n=1 is independently drawn from
a zero-mean Gaussian distribution with unit power. The observation model is Y =
X + W, where each element of the noise tensor W is independently drawn from
Â© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
L. Cheng et al., Bayesian Tensor Decomposition for Signal Processing and
Machine Learning, https://doi.org/10.1007/978-3-031-22438-6_4
59

60
4
Bayesian Tensor CPD: Performance and Real-World Applications
a zero-mean Gaussian distribution with variance Ïƒ 2
w. This data generation process
follows that of [1]. The SNR is deï¬ned as 10 log10

var (X) /Ïƒ 2
w

[1], where var (X)
is the variance1 of the noise-free tensor X. All simulation results in this section are
obtained by averaging 100 Monte Carlo runs unless stated otherwise.
4.1.2
PCPD-GH Versus PCPD-GG
We ï¬rst compare the probabilistic CPD using GH prior (labeled as PCPD-GH) [2]
with the benchmarking algorithm using GG prior [1] (labeled as PCPD-GG).
4.1.2.1
Tensor Rank Learning
The performance of tensor rank learning is ï¬rstly evaluated. We regard the tensors
as low-rank tensors when their ranks are smaller than or equal to half of the maximal
tensor dimension, i.e., R â‰¤max{Jn}N
n=1/2. Similarly, high-rank tensors are those
with R > max{Jn}N
n=1/2. In particular, in Fig.4.1, we assess the tensor rank learning
performances of the two algorithms for low-rank tensors with R = {3, 6, 9, 12, 15}
and high-rank tensors with R = {18, 21, 24, 27} under SNR = 10 dB.
In Fig.4.1a, the two algorithms are both with the tensor rank upper bound L =
max{Jn}N
n=1. It can be seen that the PCPD-GH algorithm and the PCPD-GG algorithm
achieve comparable performances in learning low tensor ranks. More speciï¬cally, the
PCPD-GH algorithm achieves higher learning accuracies when R = {3, 6} while the
PCPD-GG method performs better when R = {9, 15}. However, when tackling high-
rank tensors with R > 15, as seen in Fig.4.1a, both algorithms with tensor rank upper
bound L = max{Jn}N
n=1 fail to work properly. The reason is that the upper bound
value L = max{Jn}N
n=1 results in too small sparsity level (L âˆ’R)/L to leverage the
power of the sparsity-promoting priors in tensor rank learning. Therefore, the upper
bound value should be set larger in case that the tensor rank is high.
An immediate choice is L = f Ã— max{Jn}N
n=1 where f = 1, 2, 3, . . .. In Fig.4.1b
and c, we assess the performances of tensor rank learning for the two methods using
the upper bound L = 2 max{Jn}N
n=1 and L = 5 max{Jn}N
n=1, respectively. It can be
seen that the PCPD-GG algorithm is very sensitive to the rank upper bound value,
in the sense that its performance deteriorates signiï¬cantly for low-rank tensors after
employing the larger upper bounds. While PCPD-GG has improved performance
for high-rank tensors after adopting a larger upper bound, the chance of getting
the correct rank is still very low. In contrast, the performance of the PCPD-GH
algorithm is stable for all cases and it achieves nearly 100% accuracies of tensor
rank learning in a wide range of scenarios, showing its ï¬‚exibility in adapting to
different levels of sparsity. In Appendix L of [2], further numerical results on the
1 It means the empirical variance computed by treating all entries of the tensor as independent
realizations of a same scalar random variable.

4.1 Numerical Results on Synthetic Data
61
Fig. 4.1 Performance of
tensor rank learning when
the rank upper bound is a
max{Jn}N
n=1,
b 2 max{Jn}N
n=1, and
c 5 max{Jn}N
n=1
3
6
9
12
15
18
21
24
27
True Tensor Rank
0
50
100
150
Percentage of Accurate 
Tensor Rank Estimates
Rank Upper Bound: max(DimY)
PCPD-GH
PCPD-GG
   0%    0%     0%
High-Rank Tensors
Low-Rank Tensors
(a)
3
6
9
12
15
18
21
24
27
True Tensor Rank
0
50
100
150
Percentage of Accurate 
Tensor Rank Estimates
Rank Upper Bound: 2max(DimY)
PCPD-GH
PCPD-GG
Low-Rank Tensors
High-Rank Tensors
(b)
3
6
9
12
15
18
21
24
27
True Tensor Rank
0
50
100
150
Percentage of Accurate 
Tensor Rank Estimates
Rank Upper Bound: 5max(DimY)
PCPD-GH
PCPD-GG
Low-Rank Tensors
High-Rank Tensors
(c)

62
4
Bayesian Tensor CPD: Performance and Real-World Applications
-10
-5
0
5
10
15
20
SNR (dB)
0
50
100
150
Percentage of Accurate 
Tensor Rank Estimates
True Tensor Rank: 6
PCPD-GH-2max(DimY)
PCPD-GG-max(DimY)
PCPD-GG-2max(DimY)
(a)
-10
-5
0
5
10
15
20
SNR (dB)
0
50
100
150
Percentage of Accurate 
Tensor Rank Estimates
True Tensor Rank: 24
PCPD-GH-2max(DimY)
PCPD-GG-max(DimY)
PCPD-GG-2max(DimY)
(b)
Fig. 4.2 Performance of tensor rank learning versus different SNRs: a low-rank tensors and b
high-rank tensors
tensor rank learning accuracies versus different sparsity levels are presented, which
show the better performance of the PCPD-GH algorithm in a wide range of sparsity
levels.
To assess the tensor rank learning performance under different SNRs, in Fig.4.2,
the percentages of accurate tensor rank learning from the two methods are presented.
We consider two scenarios: (1) low-rank tensor with R = 6 shown in Fig.4.2a and
(2) high-rank tensor with R = 24 shown in Fig.4.2b. For the PCPD-GH algorithm,
due to its robustness to different rank upper bounds, 2 max{Jn}N
n=1 is adopted as
the upper bound value (labeled as PCPD-GH-2max(DimY)). For the PCPD-GG
algorithm, both the upper bound value max{{Jn}N
n=1} and 2 max{Jn}N
n=1 are consid-
ered (labeled as PCPD-GG-max(DimY) and PCPD-GH-2max(DimY), respectively).
From Fig.4.2, it is clear that the performance of the PCPD-GG method, for all cases,
highly relies on the choice of the rank upper bound value. In particular, when adopt-
ing 2 max{Jn}N
n=1, its performance in tensor rank learning is not good (i.e., accuracy
below 50%) for both the low-rank tensor and the high-rank tensor cases. In contrast,
when adopting max{{Jn}N
n=1}, its performance becomes much better for the low-rank
cases. In Fig.4.2a, when SNR is larger than 5 dB, the PCPD-GG with upper bound
value max{{Jn}N
n=1} achieves nearly 100% accuracy, which is very close to the accu-
racies of the PCPD-GH method. However, when the SNR is smaller than 5 dB, the
PCPD-GH method still achieves nearly 100% accuracies in tensor rank learning, but
the accuracies of the PCPD-GG method fall below 50%. For the high-rank case, as
seen in Fig.4.2b, both the PCPD-GH and the PCPD-GG methods fail to recover the
true tensor rank when SNR is smaller than 0 dB. However, when the SNR is larger
than 0 dB, the accuracies of the PCPD-GH method are near 100% while those of the
PCPD-GG at most achieve about 50% accuracy. Consequently, it can be concluded
from Fig.4.2 that the PCPD-GH method achieves more stable and accurate tensor
rank learning.

4.1 Numerical Results on Synthetic Data
63
In summary, Figs.4.1 and 4.2 show that PCPD-GH method ï¬nds the correct tensor
rank even if the initial tensor rank is exceedingly overestimated, which is a practically
useful feature since the rank is unknown in real-life cases.
4.1.2.2
Insights from Learned Length Scales
To clearly show the substantial difference between the GG and GH prior, we com-
pare the two algorithms (PCPD-GG and PCPD-GH) in terms of their learned length
scales. The length scale powers of GG and GH prior are denoted by

Î³ âˆ’1
l
L
l=1 and
{zl}L
l=1, respectively. To assess the patterns of learned length scales, we turn off the
pruning procedure and let the two algorithms directly output

Î³ âˆ’1
l
L
l=1 and {zl}L
l=1
after convergence. Since the learned length scale powers are possibly of different
sparsity patterns in different Monte Carlo trials, averaging them over Monte Carlo
trials is not informative. Instead, we present the learned values of

Î³ âˆ’1
l
L
l=1 and
{zl}L
l=1 in a single trial.
In particular, Fig.4.3 shows the result for a typical low-SNR and low-rank case
(SNR = âˆ’5 dB, R = 6) with rank upper bound being 60. From this ï¬gure, it can
be seen that the learned length scales of the two algorithms substantially differ from
each other, in the sense that the number of learned length scales (and the associated
components) with non-negligible magnitudes are different. For example, in Fig.4.3,
PCPD-GG recovers 7 components with non-negligible magnitudes2 (the smallest one
has value 4.3 â‰«0), while PCPD-GH recovers 6 components. Note that the ground-
truth rank is 6, and PCPD-GG produces a â€œghostâ€ component with magnitude much
larger than zero. Additional simulation runs, and results of other simulation settings
(e.g., high-SNR and high-rank case: SNR = 5 dB, R = 21) are included in [2], from
which similar conclusions can be drawn.
4.1.2.3
Insights on Noise Precision Learning
The learning of the noise precision Î² is crucial for reliable inference, since incorrect
estimates will cause over-/under-regularization. To examine how the speed of noise
learning affects the tensor rank (sparsity) learning when SNR is low (SNR = âˆ’5 dB),
we turn on the pruning and present the rank learning results over iterations in three
cases: (1) Case I: update Î² every iteration; (2) Case II: update Î² every 10th iteration;
(3) Case III: update Î² every 20th iteration. In Fig.4.4, the rank estimates are averaged
over 100 Monte Carlo runs. It can be seen that updating the noise precision Î² at earlier
iterations will help the learning process to unveil the sparsity pattern more quickly.
Then, we investigate under which scenario slowing the noise precision learning
will be helpful. We consider a very low-SNR case, that is, SNR = âˆ’10 dB, and
2 The magnitude of the lth component is deï¬ned as
3
n=1

A(n)
:,l
	T
A(n)
:,l

 1
2
[1].

64
4
Bayesian Tensor CPD: Performance and Real-World Applications
0
10
20
30
40
50
60
Length scale index
0
0.2
0.4
0.6
0.8
1
1.2
Power
Monte-Carlo Run: 1; SNR = -5 dB, R = 6; PCPD-GG;
Very small 
length scale
 powers:
~ 0.02
10 times larger than those  small length
scale powers, thus not disregarded.
Corresponding component magnitude: 
4.3 >> 0 
(a)
0
10
20
30
40
50
60
Length scale index
0
0.1
0.2
0.3
0.4
0.5
0.6
Power
Monte-Carlo Run: 1; SNR = -5 dB, R = 6; PCPD-GH;
Very small 
length scale
powers:
~ 3Ã—  10-8
(b)
Fig. 4.3 a The powers of learned length scales (i.e.,

Î³ âˆ’1
l
L
l=1) for PCPD-GG; b The powers
of learned length scales (i.e., {zl}L
l=1) for PCPD-GH. It can be seen that PCPD-GG recovers 7
components with non-negligible magnitudes, while PCPD-GH recovers 6 components. The two
algorithms are with the same upper bound value: 60
Fig. 4.4 Tensor rank
estimates of PCPD-GH
versus iteration number
(averaged over 100 Monte
Carlo runs) with different
noise precision learning
speeds
20
40
60
80
100
Iteration number
0
10
20
30
40
50
60
Rank estimates
SNR = -5 dB, R = 6
Update Î² every iteration
Update Î² every 
20-th iteration
Update Î² every 
10-th iteration
X: 76
Y: 6
X: 44
Y: 6
X: 39
Y: 6
then evaluate the percentages of accurate rank learning over 100 Monte Carlo runs.
The results are (1) Case I: 76%; (2) Case II: 100%; (3) Case III: 100%. In other
words, when the noise power is very large (e.g., SNR = âˆ’10 dB), slowing the noise
precision learning will make the algorithm more robust to the noises.
Finally, from further experiments in [2], it is found that if we ï¬x the noise precision
Î² and do not allow its update, PCPD-GH fails to identify the underlying sparsity
pattern (tensor rank). Particularly, a small value of Î² (e.g., 0.01) leads to over-
regularization, thus causing underestimation of non-zero components, while a large
value of Î² (e.g., 100) causes under-regularization, thus inducing overestimation of
non-zero components. This shows the importance of modeling and updating of noise
precision.

4.1 Numerical Results on Synthetic Data
65
4.1.2.4
Other Performance Metrics
Additional results and discussions on the run time, tensor recovery root mean square
error (RMSE), algorithm performance under factor matrix correlation, convergence
behavior of the PCPD-GH algorithm in terms of evidence lower bound (ELBO), and
hyper-parameter learning of PCPD-GG are also included in [2]. The key messages
of these simulation results are given as follows: (1) PCPD-GH generally costs more
run time than PCPD-GG; (2) Incorrect estimation of tensor rank degrades the tensor
signal recovery; (3) PCPD-GH performs well under factor matrix correlation; (4)
PCPD-GHmonotonicallyincreasestheELBO;(5)Furtherupdateofhyper-parameter
of PCPD-GH does not help too much in improving rank estimation in the low-SNR
regime. Interested readers can refer to the details in [2].
4.1.3
Comparisons with Non-parametric PCPD-MGP
After comparing to the parametric PCPD-GG, further comparisons are performed
with the non-parametric Bayesian tensor CPD using MGP prior (labeled as PCPD-
MGP).3 The initializations and hyper-parameters settings follow those used in [3].
4.1.3.1
Tensor Rank Learning
Weï¬rstassesstheperformanceoftensorranklearningofthethreealgorithms(PCPD-
MGP, PCPD-GG, and PCPD-GH). The simulation settings follow those of Figs.4.1
and 4.2.
In Table4.1, the percentages of accurate tensor rank estimates under different
rank upper bound values are presented. From the table, we can draw the following
conclusions. (1) When the tensor rank is low (e.g., R = 3, 6, 9) and the SNR is high
(e.g., SNR = 10 dB), PCPD-MGP correctly learns the tensor rank over 100 Monte
Carlo trials under different rank upper bound values. Its performance is insensitive
to the selection of the rank upper bound values, due to the decaying effects of the
learned length scales [3]. Therefore, in the low-rank and high-SNR scenario, the rank
learning performance of PCPD-MGP is comparable to that of PCPD-GH and much
better than PCPD-GG. (2) When the tensor rank is high (e.g., R = 18, 21, 24, 27),
PCPD-MGP fails to learn the correct tensor rank under different rank upper bound
values and even perform worse than PCPD-GG, since the decaying effects of learned
length scales tend to underestimate the tensor rank. On the contrary, the PCPD-GH
method always accurately estimates the high tensor ranks.
In Table4.2, we present the rank estimation performance under different SNRs,
with the same settings as those of Fig.4.2. It can be seen that for low tensor rank
case (e.g., R = 6), PCPD-MGP shows good performance when SNR is larger than
3 We appreciate Prof. Piyush Rai for sharing the code and data with us.

66
4
Bayesian Tensor CPD: Performance and Real-World Applications
Table 4.1 Performance of tensor rank learning under different rank upper bound values. Algorithm: PCPD-GH, PCPD-MGP, PCPD-GG; SNR = 10 dB. The
simulation settings are the same as those of Fig.4.1
Rank upper bound
True tensor rank R
3
6
9
12
15
GH
(%)
MGP
(%)
GG
(%)
GH
(%)
MGP
(%)
GG
(%)
GH
(%)
MGP
(%)
GG
(%)
GH
(%)
MGP
(%)
GG
(%)
GH
(%)
MGP
(%)
GG
(%)
max(DimY)
100
100
88
100
100
98
98
100
100
100
98
100
94
90
98
2max(DimY)
100
100
36
100
100
26
100
100
20
100
94
24
100
64
24
5max(DimY)
100
100
14
100
100
10
100
100
8
100
26
14
100
24
18
Rank upper bound
True tensor rank R
18
21
24
27
GH
(%)
MGP
(%)
GG
(%)
GH
(%)
MGP
(%)
GG
(%)
GH
(%)
MGP
(%)
GG
(%)
GH
(%)
MGP
(%)
GG
(%)
max(DimY)
8
0
19
0
0
0
0
0
0
0
0
0
2max(DimY)
100
0
14
100
0
30
100
0
26
100
0
50
5max(DimY)
100
0
12
100
0
14
100
0
16
100
0
14

4.1 Numerical Results on Synthetic Data
67
Table 4.2 Performance of tensor rank learning under different SNRs. Algorithm: PCPD-GG,
PCPD-MGP; Upper bound value: 2max(DimY). The simulation settings are the same as those
of Fig.4.2
True tensor rank
SNR (dB)
âˆ’10
âˆ’5
0
5
GH
(%)
MGP
(%)
GG
(%)
GH
(%)
MGP
(%)
GG
(%)
GH
(%)
MGP
(%)
GG
(%)
GH
(%)
MGP
(%)
GG
(%)
6
92
2
10
100
78
8
100
90
12
100
98
28
24
0
0
0
0
0
18
82
0
30
100
0
38
True tensor rank
SNR (dB)
10
15
20
GH
(%)
MGP
(%)
GG
(%)
GH
(%)
MGP
(%)
GG
(%)
GH
(%)
MGP
(%)
GG
(%)
6
100
100
26
100
100
16
100
100
8
24
100
00
26
100
0
22
100
0
34
âˆ’5 dB. It outperforms the PCPD-GG method, but it is still not as good as PCPD-GH.
At SNR = âˆ’10 dB, PCPD-MGP fails to correctly estimate the tensor rank, while
PCPD-GH shows good performance in a wider range of SNRs (from âˆ’10 to 20 dB).
Furthermore, when the tensor rank becomes large (e.g., R = 24), PCPD-MGP fails
to learn the underlying true tensor rank, making it inferior to PCPD-GH (and even
PCPD-GG) in the high-rank regime.
4.1.3.2
Insights from Learned Length Scales
To reveal more insights, we present the learned length scales (without pruning)
of PCPD-MPG. Following the notations in [3], the learned length scale powers of
PCPD-MGP are denoted by

Ï„ âˆ’1
l
=
l
k=1 Î´k
âˆ’1L
l=1
.
Two typical cases are considered: (1) Case I: SNR = 5 dB, R = 21, corresponding
to high rank and high SNR; (2) Case II: SNR = âˆ’10 dB, R = 6, corresponding to
very low SNR and low rank. For each case, we present the learned length scales
in a single trial in Figs.4.5 and 4.6, respectively. From these ï¬gures, we have the
following observations. (1) Due to the decaying effect of the MGP prior, the learned
length scale power Ï„ âˆ’1
l
quickly decreases as l becomes larger. This drives PCPD-
MGP to fail to recover the sparsity pattern of the high-rank CPD (e.g., R = 21),
in which a large number of length scale powers should be much larger than zero;
see Fig.4.5a. In contrast, without the decaying effect, the PCPD-GH successfully
identiï¬es the 21 non-negligible components, as seen in Fig.4.5b. (2) When the SNR
is very low (e.g., SNR = âˆ’10 dB) and the rank is low (e.g., R = 6), while the rank
estimation given by PCPD-MGP is close to the ground truth, its sparsity pattern of
the learned length scales is not as accurate as that of PCPD-GH (see Fig.4.6). This

68
4
Bayesian Tensor CPD: Performance and Real-World Applications
0
10
20
30
40
50
60
Length scale index
0
10
20
30
40
50
60
70
Power
Very small 
decaying
length scale
powers:
1e-2 ~ 1e-18
Monte-Carlo Run: 1; SNR = 5 dB, R = 21; PCPD-MGP;
(a)
0
10
20
30
40
50
60
Length scale index
0
0.1
0.2
0.3
0.4
0.5
0.6
Power
Monte-Carlo Run: 1; SNR = 5 dB, R = 21; PCPD-GH;
Very small 
length 
scale 
powers:
~ 9Ã—  10-7
(b)
Fig. 4.5 a The powers of learned length scales (i.e., {Ï„ âˆ’1
l
}L
l=1) for PCPD-MGP; b The powers
of learned length scales (i.e., {zl}L
l=1) for PCPD-GH. It can be seen that PCPD-MGP recovers 15
components with non-negligible magnitudes, while PCPD-GH recovers 21 components. The two
algorithms are with the same upper bound value L = 60. Simulation setting: SNR = 5 dB, R = 21
0
10
20
30
40
50
60
Length scale index
0
0.2
0.4
0.6
0.8
1
1.2
Power
Monte-Carlo Run: 1; SNR = -10 dB, R = 6;PCPD-MGP;
Very small decaying 
length scale powers: 1e-2 ~ 1e-18.
(a)
0
10
20
30
40
50
60
Length scale index
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Power
Monte-Carlo Run: 1; SNR = -10 dB, R = 6; PCPD-GH;
Very small 
 length scale powers:
~1e-7
(b)
Fig. 4.6 a The powers of learned length scales (i.e., {Ï„ âˆ’1
l
}L
l=1) for PCPD-MGP; b The powers
of learned length scales (i.e., {zl}L
l=1) for PCPD-GH. It can be seen that PCPD-MGP recovers 5
components with non-negligible magnitudes, while PCPD-GH recovers 6 components. The two
algorithms are with the same upper bound value L = 60. Simulation setting: SNR = âˆ’10 dB,
R = 6
is due to the mismatch in the decaying component amplitude assumption in the MGP
model.
Additional simulation runs and results of more simulation settings are included
in [2], from which similar conclusions can be drawn.

4.2 Real-World Applications
69
4.2
Real-World Applications
In this section, two real-world applications of CPD are presented, with a focus on
performance comparisons between PCPD-GG and PCPD-GH.
4.2.1
Fluorescence Data Analytics
Fluorescence spectroscopy is a fast, simple, and inexpensive method to determine
the concentration of any solubilized sample based on its ï¬‚uorescent properties (see
Fig.4.7) and is widely used in chemical, pharmaceutical, and biomedical ï¬elds [4]. In
ï¬‚uorescence spectroscopy, an excitation beam with a certain wavelength Î»i passes
through a solution in a cuvette. The excited chemical species in the sample will
change their electronic states and then emit a beam of light, of which its spectrum
is measured at the detector. Mathematically, let the concentration of the rth species
in the sample be cr, and the excitation value at wavelength Î»i be ar(Î»i). Then,
the noise-free measured spectrum intensity at the wavelength Î» j is ar(Î»i)br(Î» j)cr,
where br(Î» j) is the emission value of the rth species at the wavelength Î» j. If there
are R different species in the sample, the noise-free ï¬‚uorescence excitationâ€“emission
measured (EEM) data at Î» j is
xi, j =
R

r=1
ar(Î»i)br(Î» j)cr.
(4.1)
Assume the excitation beam contains I wavelengths, and the noise-free EEM data
is collected at J different wavelengths, then an I Ã— J data matrix is obtained as
X =
R

r=1
A:,r â—¦B:,rcr,
(4.2)
Fig. 4.7 An application
example of CPD:
ï¬‚uorescence
excitationâ€“emission
measurements

70
4
Bayesian Tensor CPD: Performance and Real-World Applications
Fig. 4.8 The clean spectra recovered from the noise-free ï¬‚uorescence tensor data assuming the
knowledge of tensor rank
where symbol â—¦denotes vector outer product, A:,r âˆˆRIÃ—1 is a vector with the ith
element being ar(Î»i), and B:,r âˆˆRJÃ—1 is a vector with the jth element being br(Î» j).
Assume K > 1 samples with the same chemical species but with different concen-
trations of each species are measured. Let the concentration of the rth species in
the kth sample be ck,r, then after stacking the noise-free EEM data for each sample
along a third dimension, a three-dimensional (3D) tensor data X âˆˆRIÃ—JÃ—K can be
obtained as
X =
R

r=1
A:,r â—¦B:,r â—¦C:,r â‰œA, B, C,
(4.3)
where C:,r âˆˆRKÃ—1 is a vector with the kth element being ck,r; matrices A âˆˆRIÃ—R,
B âˆˆRJÃ—R, and C âˆˆRKÃ—R are matrices with their rth columns being A:,r, B:,r, and
C:,r, respectively. It is easy to see that the noise-free data model in (4.3) yields exactly
the tensor CPD model, and that is why CPD algorithms work very well for EEM
data analysis.
To showcase various CPD algorithms for the ï¬‚uorescence data analysis applica-
tion, we consider the popular amino acids ï¬‚uorescence data4 X with size 5 Ã— 201 Ã—
61 [5]. It consists of ï¬ve laboratory-made samples, with each sample containing
different amounts of tyrosine, tryptophan, and phenylalanine dissolved in phosphate
buffered water. Since there are three different types of amino acids, when adopting
the CPD model, the optimal tensor rank should be 3. In particular, with the optimal
tensor rank 3, the clean spectra for the three types of amino acids, which are recov-
ered by the alternative least-squares (ALS) algorithm (Algorithm 1 of Chap.1), are
presented in Fig.4.8 as the benchmark.
In practice, it is impossible to know how many components are present in the data
in advance, and this calls for automatic tensor rank learning. Therefore, we assess
both the rank learning performance and the noise mitigation performance for the two
algorithms (i.e., PCPD-GH and PCPD-GG) under different levels of noise sources.
4 http://www.models.life.ku.dk.

4.2 Real-World Applications
71
Table 4.3 Fit values and estimated tensor ranks of ï¬‚uorescence data under different SNRs
SNR
(dB)
Rank upper bound = max(DimX)
Rank upper bound = 2max(DimX)
PCPD-GG
PCPD-GH
PCPD-GG
PCPD-GH
Fit value
Estimated
tensor
rank
Fit value
Estimated
tensor
rank
Fit value
Estimated
tensor
rank
Fit value
Estimated
tensor
rank
âˆ’10
71.8109
4
72.6401
3
71.8197
4
72.6401
3
âˆ’5
83.9269
4
84.3424
3
83.5101
4
84.3424
3
0
90.6007
4
90.8433
3
90.3030
5
90.8433
3
5
94.2554
4
94.3554
3
94.0928
5
94.3555
3
10
96.0907
3
96.0951
3
96.0369
4
96.0955
3
15
96.8412
3
96.8431
3
96.8412
3
96.8432
3
20
97.1197
3
97.1204
3
97.1197
3
97.1204
3
In particular, the Fit value [6], which is deï¬ned as (1 âˆ’|| Ë†Xâˆ’X||F
||X||F ) Ã— 100%, is adopted,
where Ë†X represents the reconstructed ï¬‚uorescence tensor data from the algorithm.
In Table4.3, the performances of the two algorithms are presented assuming dif-
ferent upper bound values of tensor rank. It can be observed that with different upper
bound values, the PCPD-GH algorithm always gives the correct tensor rank esti-
mates, even when the SNR is smaller than 0 dB. On the other hand, the PCPD-GG
method is quite sensitive to the choice of the upper bound value. Its performance with
upper bound 2 max{Jn}N
n=1 becomes much worse than that with max{Jn}N
n=1 in tensor
rank learning. Even with the upper bound being equal to max{Jn}N
n=1, PCD-GG fails
to recover the optimal tensor rank 3 in the low-SNR region (i.e., SNR â‰¤5 dB). With
the overestimated tensor rank, the reconstructed ï¬‚uorescence tensor data Ë†X will be
overï¬tted to the noise sources, leading to lower Fit values. As a result, the Fit values
of the PCPD-GH method are generally higher than those of the PCPD-GG method
under different SNRs.
In this application, since the tensor rank represents the number of underlying
components inside the data, its incorrect estimation will not only lead to overï¬tting
to the noise, but also will cause â€œghostâ€ components that cannot be interpreted. This is
illustrated in Fig.4.9 where ghost component appears in SNR = âˆ’10 dB and 0 dB for
PCPD-GG. In addition, in Fig.4.10, we present the learned length scale powers from
PCPD-GG and PCPD-GH. It can be seen that PCPD-GG recovers four components
with non-negligible powers. The smallest componentâ€™s power is 8 times larger than
those of negligible components. In data analysis, disregarding such a large learned
latent component is not reasonable. On the other hand, the PCPD-GH gives very
clean 3 components with non-negligible powers.
Since the â€œghostâ€ component in PCPD-GG has a relatively large magnitude, it
degrades the performance of the tensor signal recovery. For example, from Table4.3,
when the SNR is low (e.g., âˆ’10 dB), the Fit value of PCPD-GH is 0.8 higher than

72
4
Bayesian Tensor CPD: Performance and Real-World Applications
Fig. 4.9 The recovered spectra of ï¬‚uorescence data under different SNRs
(a) PCPD-GG
(b) PCPD-GH
Fig. 4.10 Amino acids ï¬‚uorescence data analysis. a The powers of learned length scales (i.e.,
{Î³ âˆ’1
l
}L
l=1) for PCPD-GG; b The powers of learned length scales (i.e., {zl}L
l=1) for PCPD-GH. It can
be seen that PCPD-GG recovers 4 components with non-negligible magnitudes, while PCPD-GH
recovers 3 components. The two algorithms are with the same upper bound value: 201. Simulation
setting: SNR = âˆ’5 dB. Since the x-axis is too long (containing 201 points), we only present partial
results that include non-zero components. Those values not shown in the ï¬gures are all very close
to zero
that of PCPD-GG. This is in great contrast in the high-SNR regime (no â€œghostâ€
component) where their Fit value difference is about 0.001. Therefore, the â€œghostâ€
component signiï¬cantly degrades the tensor signal recovery performance.

4.2 Real-World Applications
73
4.2.2
Hyperspectral Images Denoising
Hyperspectral image (HSI) data are naturally three-dimensional (two spatial dimen-
sions and one spectral dimension), in which tensor CPD is a suitable tool to analyze
such data. However, due to the radiometric noise, photon effects, and calibration
errors, it is crucial to mitigate these corruptions before putting the HSI data into use.
Since each HSI is rich in details, previous works using searching-based methods [7,
8] revealed that the tensor rank in HSI data is usually larger than half of the maxi-
mal tensor dimension. This corresponds to the high tensor rank scenario deï¬ned in
Sect.4.1.
In this application, we consider two real-world datasets: the Salinas-A HSI and the
Indian Pines HSI, where different bands of HSIs were corrupted by different levels of
noises. Some of the HSIs are quite clean while some of them are quite noisy. For such
types of real-world data, since no ground truth is available, a no-reference quality
assessment score is usually adopted [7, 8]. In particular, following [7], the SNR
output 10 log10 || Ë†X||2
F/||X âˆ’Ë†X||2
F is utilized as the denoising performance measure,
where Ë†X is the restored tensor data and X is the original HSI data. In Table4.4,
the SNR outputs of the two methods using different rank upper bound values are
presented, from which it can be seen that the PCPD-GH method gives higher SNR
outputs than PCPD-GG.
Samples of denoised HSIs are depicted in Fig.4.11. On the left side of Fig.4.11,
the relatively clean Salinas-A HSI in band 190 is presented to serve as a reference,
from which it can be observed that the landscape exhibits â€œstripeâ€ pattern. For the
noisy HSI in band 1, the denoising results from the two methods using the rank upper
bound max{Jn}N
n=1 are presented. It is clear that the PCPD-GH method recovers better
â€œstripeâ€ pattern than the PCPD-GG method. Similarly, the results from the Indian
Pines dataset are presented on the right side of Fig.4.11. For noisy HSI in band 1,
with the relatively clean image in band 10 serving as the reference, it can be observed
that the PCPD-GH method recovers more details than the PCPD-GG method, when
both using rank upper bound 2max{Jn}N
n=1.
Since the HSIs in band 1 are quite noisy, inspecting the performance differ-
ence between the two methods requires a closer look. In Fig.4.11, we have used
Table 4.4 SNR outputs and estimated tensor ranks of HSI data under different rank upper bounds
Algorithm
PCPD-GG
PCPD-GH
Dataset
Rank
upper
bound
SNR
output
(dB)
Estimated
tensor
rank
SNR
output
(dB)
Estimated
tensor
rank
Salinas-A
max{Jn}N
n=1
43.7374
137
44.0519
143
2 max{Jn}N
n=1
46.7221
257
46.7846
260
Indian pines
max{Jn}N
n=1
30.4207
169
30.5541
178
2 max{Jn}N
n=1
31.9047
317
32.0612
335

74
4
Bayesian Tensor CPD: Performance and Real-World Applications
Fig. 4.11 The hyper-spectral image denoising results
red boxes to highlight those differences. Note that although HSIs in different fre-
quency bands have different pixel intensities (different color bars), they share the
same â€œclusteringâ€ structure. The goal of HSI denoising is to reconstruct the â€œcluster-
ingâ€ structure in each band in order to facilitate the downstream segmentation task
[7, 8]. Therefore, the assessment is based on whether the recovered HSI exhibits
correct â€œclusteringâ€ patterns. Speciï¬cally, for Salinas-A scene data, the recovered
images are supposed to render explicit â€œstripeâ€ patterns, in each of which the intensi-
ties (colors) are almost the same. As indicated by the red boxes, it can be observed that
PCPD-GH recovers better â€œstripeâ€ pattern than PCPD-GG, since much more pixels
in the red box of PCPD-GH have the same blue color. Similarly, for the Indian Pines
dataset, as indicated by each red box, the area supposed to be identiï¬ed as a cluster
(with warmer colors than nearby areas) is more accurately captured by PCPD-GH.
References
1. Q. Zhao, L. Zhang, A. Cichocki, Bayesian cp factorization of incomplete tensors with automatic
rank determination. IEEE Trans. Pattern Anal. Mach. Intell. 37(9), 1751â€“1763 (2015)
2. L. Cheng, Z. Chen, Q. Shi, Y.-C. Wu, S. Theodoridis, Towards ï¬‚exible sparsity-aware modeling:
automatic tensor rank learning using the generalized hyperbolic prior. IEEE Trans. Signal
Process. 70, 1834â€“1849 (2022)
3. P. Rai, Y. Wang, S. Guo, G. Chen, D. Dunson, L. Carin, Scalable bayesian low-rank decom-
position of incomplete multiway tensors, in International Conference on Machine Learning
(PMLR, 2014), pp. 1800â€“1808
4. J.R. Albani, Principles and Applications of Fluorescence Spectroscopy (Wiley, New York,
2008)

References
75
5. R. Bro, Multi-way analysis in the food industry, in Models, Algorithms, and Applications
(Academish proefschrift, Dinamarca, 1998)
6. A. Cichocki, R. Zdunek, A.H. Phan, S.-I. Amari, Nonnegative Matrix and Tensor Factor-
izations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation
(Wiley, New York, 2009)
7. X. Liu, S. Bourennane, C. Fossati, Denoising of hyperspectral images using the parafac model
and statistical performance analysis. IEEE Trans. Geosci. Remote Sens. 50(10), 3717â€“3724
(2012)
8. B. Rasti, P. Scheunders, P. Ghamisi, G. Licciardi, J. Chanussot, Noise reduction in hyperspectral
imagery: overview and application. Remote Sens. 10(3), 482 (2018)

Chapter 5
When Stochastic Optimization Meets VI:
Scaling Bayesian CPD to Massive Data
Abstract In previous chapters, Bayesian tensor CPD algorithms are derived for
batch-mode operation, meaning that it needs to process the whole dataset at the same
time. Obviously, this is no longer suitable for large datasets. To enable Bayesian ten-
sor CPD in the Big Data era, the idea of stochastic optimization can be incorporated,
rendering a scalable algorithm that only processes a mini-batch data at a time. In this
chapter, we develop a scalable algorithm for Bayesian tensor CPD with automatic
rank determination. Numerical examples in synthetic and real-world data demon-
strate the excellent performance of the algorithm, both in terms of computation time
and accuracy.
5.1
CPD Problem Reformulation
According to (1.8), tensor CPD assumes that a P + 1 dimensional data tensor Ë™Y âˆˆ
RI1Ã—I2Ã—Â·Â·Â·Ã—IPÃ—IP+1 obeys the following model:
Ë™Y =
R

r=1
(1)
:,r â—¦(2)
:,r â—¦Â· Â· Â· â—¦(P)
:,r â—¦(P+1)
:,r
+ Ë™W
â‰œ[[(1), (1), . . . , (P), (P+1)]] + Ë™W,
(5.1)
where Ë™W represents an additive noise tensor. The vector (p)
:,r âˆˆRIp is therth column
ofthefactormatrix(p) âˆˆRIpÃ—R,andâ—¦denotesthevectorouterproduct.Thenumber
of rank-1 components R is deï¬ned as the tensor rank.
The core problem of tensor CPD is to ï¬nd the factor matrices {(p)}P+1
p=1 from Ë™Y
under the unknown tensor rank R. The problem can be stated as
min
{(p)}P+1
p=1
Î²
2 âˆ¥Ë™Y âˆ’[[(1), (2), . . . , (P+1)]] âˆ¥2
F +
L

l=1
Î³l
2
 P+1

p=1
(p)T
:,l
(p)
:,l

, (5.2)
where L is the maximum possible value of tensor rank R. Notice that the tensor rank
acquisition is generally NP-hard, due to its discrete nature. As an effective heuristic
Â© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
L. Cheng et al., Bayesian Tensor Decomposition for Signal Processing and
Machine Learning, https://doi.org/10.1007/978-3-031-22438-6_5
77

78
5
When Stochastic Optimization Meets VI: Scaling â€¦
approach, a regularization term L
l=1
Î³l
2
 P+1
p=1 (p)T
:,l
(p)
:,l

is added in order to
control the complexity of the model and avoid overï¬tting of noise.
It is easy to show that problem (5.2) is non-convex, since all the factor matrices
{(p)}P+1
p=1 are coupled via Khatriâ€“Rao products. To solve problem (5.2), the widely
used ALS optimization framework iteratively updates each factor matrix with other
factor matrices being ï¬xed. However, in each update, the computation requires ana-
lyzing the whole data tensor Ë™Y, and thus is not computationally efï¬cient. To achieve
highly scalable algorithms, a recent work [1] rewrites the objective function into an
equivalent summation form so that stochastic optimization can be readily applied. In
particular, using the deï¬nition of tensor CPD, it is easy to show that problem (5.2)
is equivalent to
min
{(p)}P
p=1
Î²
2
IP+1

n=1
âˆ¥Y(n) âˆ’[[(1), (2), . . . , (P), Î¾(n)]] âˆ¥2
F
+
L

l=1
Î³l
2

P

p=1
(p)T
:,l
(p)
:,l +
IP+1

n=1
Î¾(n)2
l

(5.3)
where the data slices {Y(n) âˆˆRI1Ã—I2Ã—Â·Â·Â·Ã—IP}IP+1
n=1 are obtained by slicing the data tensor
Ë™Y along the last dimension.1 In this expression, factor matrices {(p) âˆˆRIpÃ—L}P
p=1
span the tensor subspace where data tensor Y(n) lies, and vector Î¾(n) = (P+1)
n,:
is
the feature vector associated with the tensor data slice Y(n). From (5.3), it is obvious
that the gradient of the objective function is a summation of IP+1 components, with
each component depending only on a slice of tensor data Y(n). This motivates the
use of stochastic optimization to improve the scalability.
5.1.1
Probabilistic Model and Inference for the Reformulated
Problem
The probabilistic model for the reformulated problem (5.3) could be established by
interpreting various terms in (5.3) via probability density functions (pdfs). Firstly,
the squared error term in problem (5.3) can be interpreted as the negative log of a
Gaussian likelihood function:
1 The CPD deï¬nition implies that the summation form holds for the data tensor Ë™Y being sliced
along any dimension. Without loss of generality, the discussions here assume slicing along the last
dimension.

5.1 CPD Problem Reformulation
79
p

{Y(n)}IP+1
n=1 | {(p)}P
p=1, {Î¾(n)}IP+1
n=1, Î²âˆ’1
=
IP+1

n=1
 Î²
2Ï€
 I1 I2Â·Â·Â·IP
2
exp

âˆ’Î²
2 âˆ¥Y(n) âˆ’[[(1), (2), . . . , (P), Î¾(n)]] âˆ¥2
F

.
(5.4)
Secondly, the regularization term in problem (5.3) can be interpreted as arising from
a Gaussian prior distribution over the columns of the factor matrices and feature
vectors, i.e.,
p({(p)}P
p=1|{Î³l}L
l=1) =
P

p=1
L

l=1
N

(p)
:,l |0IpÃ—1, Î³ âˆ’1
l
I L

=
P

p=1
Ip

Ï„=1
N

(p)
Ï„,: |01Ã—L, âˆ’1
,
(5.5)
p({Î¾(n)}IP+1
n=1 |{Î³l}L
l=1) =
IP+1

n=1
L

l=1
N

Î¾(n)l|0, Î³ âˆ’1
l

=
IP+1

n=1
N

Î¾(n)|01Ã—L, âˆ’1
,
(5.6)
where  = diag{Î³1, Î³2, . . . , Î³L} and L is an upper bound of R. The inverse of the
regularization parameter Î³ âˆ’1
l
has a physical interpretation of the power of the lth
column of various factor matrices. When power Î³ âˆ’1
l
goes to zero, it indicates the
corresponding columns in various factor matrices play no role, and can be pruned
out. For the parameters Î² and {Î³l}L
l=1, since we have no information about their dis-
tributions, non-informative gamma priors [4] are imposed on them, i.e., p(Î²|Î±Î²) =
gamma(Î²|10âˆ’6, 10âˆ’6) and p({Î³l}L
l=1|Î»Î³ ) = 	L
l=1 gamma(Î³l|10âˆ’6, 10âˆ’6). This cor-
responds to the Gaussian-gamma model in (3.4) and (3.5).
The complete probabilistic model is shown in Fig.5.1. Let  be a set contain-
ing the unknown factor matrices {(p)}P
p=1, feature vectors {Î¾(n)}IP+1
n=1, and other
variables Î², {Î³l}L
l=1. From the probabilistic model established above, the goal of
Bayesian inference is to calculate the posterior distribution p(|{Y(n)}IP+1
n=1) =
p(, {Y(n)}IP+1
n=1)/

p(, {Y(n)}IP+1
n=1)d,whichhowever isanalyticallyintractable
due to the multiple integrations involved. Therefore, we make use of variational
inference in Sect.3.4. In particular, under the mean-ï¬eld approximation Q() =
	K
k=1 Q(k), where k is part of  with âˆªK
k=1k =  and âˆ©K
k=1k = Ã˜, the opti-
mal variational pdf Qâˆ—(k) is obtained by solving the following problem with other
{Q( j)} jÌ¸=k ï¬xed [5] (also see (3.17)):
min
Q(k)

Q (k)

âˆ’E	
jÌ¸=k Q( j)

ln p

, {Y(n)}IP+1
n=1

+ ln Q (k)

dk
s.t.

Q(k)dk = 1 , Q(k) â‰¥0.
(5.7)

80
5
When Stochastic Optimization Meets VI: Scaling â€¦
Fig. 5.1 Probabilistic model
for reformulated tensor CPD
(Â© [2018] IEEE. Reprinted,
with permission, from [L.
Cheng, Y.-C. Wu, and H. V.
Poor, Scaling Probabilistic
Tensor Canonical Polyadic
Decomposition to Massive
Data, IEEE Transactions on
Signal Processing, Nov
2018]. It applies to all ï¬gures
and tables in this chapter)
Â·Â·Â·
Î²
Î(1)
Î(2)
Î(P )
{Î³l}L
l=1
Î±Î²
Î»Î³
{Î¾(n)}N
n=1
{Y(n)}N
n=1
mr
= 01Ã—L
For this convex problem, the Karushâ€“Kuhnâ€“Tucker (KKT) condition gives the opti-
mal variational pdf Qâˆ—(k) as [5]
Qâˆ—(k) âˆexp

E	
jÌ¸=k Q( j)

ln p

, {Y(n)}IP+1
n=1

.
(5.8)
If we compute various Qâˆ—() directly, we would end up Algorithm 5 in Chap.3,
with the slight variation that the last factor matrix (P+1) is learned row-by-row via
Î¾(n).
5.2
Interpreting VI Update from Natural Gradient Descent
Perspective
While it may seem that the reformulation in Sect.5.1 is just a trivial variation of the
original Bayesian CPD problem and inference, it turns out this reformulation is the
ï¬rst step in the process of revealing a connection between VI and natural gradient
descent, which is the gradient descent for distributions rather than parameters. If such

5.2 Interpreting VI Update from Natural Gradient Descent Perspective
81
a connection is established, we can make use of an idea from stochastic optimization,
which has recently enabled many large-scale machine learning tasks [6â€“8].
5.2.1
Optimal Variational Pdfs in Exponential Family Form
In a previous work [3], it has been shown that for some special probabilistic models,
such as the two-layer model speciï¬ed in [3], the VI update due to (5.8) can be inter-
preted from a natural gradient descent perspective, which paves the way to integrate
stochastic optimization in the VI framework. However, the probabilistic model of
tensor CPD does not belong to the model family speciï¬ed in [3], as evidenced by
Fig.5.1 that the considered model contains three layers. To bridge the framework of
[3] to tensor CPD, we notice from Algorithm 5 of Chap.3 (also [15]) that the opti-
mal variational pdfs in batch-mode Gaussian-gamma-based CPD are of Gaussian or
gamma distribution, and thus can be characterized by their natural parameters (see
Deï¬nition 2.1). In particular, from (3.38) and (3.39) in Algorithm 5, we can see
that the optimal {Qâˆ—((p))}P
p=1 are Gaussian distributions. Tailoring to the model
in Fig.5.1, Qâˆ—((p)) can be written in the exponential family form (for details, see
[18]) as
Qâˆ—((p)) = p(vec((p))|Î± p) âˆexp

Î±T
p

vec

(p)T , âˆ’vec

(p)T (p)T T 
,
with the natural parameter being
Î± p = E	
 j Ì¸=(p) Q( j)

IP+1
n=1 t

Y(n),

Î· j, Î· j Ì¸= (p)
vec
 1
2

+ IP+1
n=1 ln

Î· j, Î· j Ì¸= (p)

,
(5.9)
where2
t

Y(n),

Î· j, Î· j Ì¸= (p)
= vec

Î²

Y(n)
(p) 
Î¾(n) â‹„

Pâ‹„
m=1,mÌ¸=p (m)
,
(5.10)
ln

Î· j, Î· j Ì¸= (p)
= vec
Î²
2

Î¾(n) â‹„

Pâ‹„
m=1,mÌ¸=p (m)T 
Î¾(n) â‹„

Pâ‹„
m=1,mÌ¸=p (m)
.
(5.11)
In (5.9), Î· = {{(p)}P
p=1, Î², {Î¾(n)}IP+1
n=1} denotes the unknown variables of layer 1 in
Fig.5.1. In (5.10), the operation [A](p) unfolds the tensor A along the pth dimension,
and
Nâ‹„
n=1,nÌ¸=k A(n) = A(N) â‹„Â· Â· Â· â‹„A(k+1) â‹„A(kâˆ’1) â‹„Â· Â· Â· â‹„A(1) in (5.11) is the multiple
Khatriâ€“Rao products.
2 We use overloaded notation for the functions t(Â·) and l(Â·) for brevity. Their detailed functional
forms depend on their arguments.

82
5
When Stochastic Optimization Meets VI: Scaling â€¦
Similarly, the optimal variational pdf Qâˆ—(Î¾(n)) = p(Î¾(n)|Î±Î¾(n)) can be shown to
be a Gaussian distribution, with the natural parameter being
Î±Î¾(n) = E	
 j Ì¸=Î¾(n) Q( j)

t

Y(n),

Î· j, Î· j Ì¸= Î¾(n)

vec
 1
2

+ l

Î· j, Î· j Ì¸= Î¾(n)


,
(5.12)
where
t

Y(n),

Î· j, Î· j Ì¸= Î¾(n)

= Î²

Pâ‹„
p=1 (p)T
vec (Y(n)) ,
(5.13)
l

Î· j, Î· j Ì¸= Î¾(n)

= vec
Î²
2

Pâ‹„
p=1 (p)T 
Pâ‹„
p=1 (p)
.
(5.14)
On the other hand, the optimal variational pdf Qâˆ—(Î²) is a gamma distribution [15].
Writing the gamma distribution in exponential family form, we have Qâˆ—(Î²) =
p(Î²|Î±Î²) âˆexp

Î±T
Î² [Î², ln Î²]T 
, with the natural parameter being
Î±Î² = E	
 j Ì¸=Î² Q( j)

âˆ’10âˆ’6 + IP+1
n=1 t

Y(n),

Î· j, Î· j Ì¸= Î²

10âˆ’6 âˆ’1 + IP+1
n=1 ln

Î· j, Î· j Ì¸= Î²


,
(5.15)
where
t

Y(n),

Î· j, Î· j Ì¸= Î²

= âˆ’1
2 âˆ¥Y(n) âˆ’[[(1), (2), . . . , (P), Î¾(n)]] âˆ¥2
F,
(5.16)
ln

Î· j, Î· j Ì¸= Î²

= I1I2 Â· Â· Â· IP
2
.
(5.17)
Finally, the optimal Qâˆ—(Î³ ) = p(Î³ |Î»Î³ ) is a gamma distribution [15] with the natural
parameter being
Î»Î³ = E	
 j Ì¸=Î³ Q( j)

âˆ’d + u({(p)}P
p=1) + IP+1
n=1 u(Î¾(n))
c âˆ’1 + w

,
(5.18)
where
u({(p)}P
p=1) =
â¡
â£
P

p=1
âˆ’1
2(p)T
:,1 (p)
:,1 , . . . ,
P

p=1
âˆ’1
2(p)T
:,L (p)
:,L
â¤
â¦
T
,
(5.19)
u(Î¾(n)) =

âˆ’1
2Î¾(n)2
1, . . . , âˆ’1
2Î¾(n)2
L
T
,
(5.20)

5.2 Interpreting VI Update from Natural Gradient Descent Perspective
83
w =
â¡
â£1
2
P

p=1
Ip + 1
2 IP+1, . . . , 1
2
P

p=1
Ip + 1
2 IP+1
â¤
â¦
T
,
(5.21)
vector 1 is the all-ones vector with length L, and c = d = 10âˆ’61. From (5.9), (5.12),
(5.15), and (5.18), since the update in each equation depends on the parameters in
other equations, the VI algorithm consists of cyclical update among these equations.
Remark: If we compute the expectations in (5.9), (5.12), (5.15), and (5.18), after
tedious derivations, we would obtain the batch-mode VI algorithm for tensor CPD
[13, 15]. However, computing the expectations at this stage would obscure the inter-
pretation of update Eqs.(5.9), (5.12), (5.15), and (5.18) as natural gradient descent
steps in Riemannian space, an insight critical in developing the stochastic optimiza-
tion for probabilistic tensor CPD algorithm.
5.2.2
VI Updates as Natural Gradient Descent
In order to reveal the connection between the optimal variational parametric updates
(5.9), (5.12), (5.15), (5.18), and gradient descent steps, we substitute them back
into problem (5.7), transforming the functional optimization problem (5.7) into a
parametric optimization problem.
For example, using the knowledge that Qâˆ—((p)) = p(vec((p))|Î± p) is a Gaus-
sian distribution, the constraints in problem (5.7) are automatically satisï¬ed. Since
Q((p)) is parametrized by Î± p, the problem (5.7) becomes
min
Î± p

p(vec((p))|Î± p)

âˆ’E	
 j Ì¸=(p) Q( j)

ln p (, Y)

+ ln p(vec((p))|Î± p)

dÎ± p.
(5.22)
After performing the integration and discarding the terms irrelevant to (p), it can
be shown that problem (5.22) becomes problem (5.23):
min
Î± p
âˆ’E	
 j Ì¸=(p) Q( j )
â¡
â£
IP+1
n=1 t

Y(n),

Î· j, Î· j Ì¸= (p)
vec
 1
2

+ IP+1
n=1 ln

Î· j, Î· j Ì¸= (p)
â¤
â¦
T
Ep(vec((p))|Î± p)

t((p))

+ Î± p
T Ep(vec((p))|Î± p)

t((p))

âˆ’L((p)),
(5.23)
where t((p)) =

vec

(p)
, âˆ’vec

(p)T (p)T is the sufï¬cient statistic of the
prior distribution (5.5), and L((p)) = Ip
L
l=1 ln Î³l is the log normalizer of the prior
distribution (5.5).
Given that (5.23) is a parametric optimization problem, denoting the objective
function in (5.23) as f (Î± p), its derivative is shown as follows:

84
5
When Stochastic Optimization Meets VI: Scaling â€¦
â–½Î± p f (Î± p)
= âˆ’â–½Î± pEp(vec((p))|Î± p)[t((p))]E	
 j Ì¸=(p) Q( j )
â¡
â£
IP+1
n=1 t

Y(n),

Î· j, Î· j Ì¸= (p)
vec
 1
2

+ IP+1
n=1 ln

Î· j, Î· j Ì¸= (p)
â¤
â¦
+ â–½Î± pEp(vec((p))|Î± p)[t((p))]Î± p + Ep(vec((p))|Î± p)[t((p))] âˆ’â–½Î± p

L((p))

.
(5.24)
Since p(vec((p))|Î± p) is in the exponential family, with t((p)) and L((p))
being the sufï¬cient statistic and the log normalizer, respectively, we have the property
that Ep(vec((p))|Î± p)[t((p))] = â–½Î± p

L((p))

[4]. Using this result, the last two terms
of (5.24) cancel each other, and the remaining terms can be organized as
â–½Î± p f (Î± p)
= â–½2
Î± p

L((p))
 
âˆ’E	
 j Ì¸=(p) Q( j )
â¡
â£
IP+1
n=1 t

Y(n),

Î· j, Î· j Ì¸= (p)
vec
 1
2

+ IP+1
n=1 ln

Î· j, Î· j Ì¸= (p)
â¤
â¦+ Î± p

.
(5.25)
This gradient, which is commonly used in optimization, is deï¬ned in Euclidean
distance metric, i.e., â–½Î± p f (Î± p) = arg maxdÎ± p f (Î± p + dÎ± p) subject to dÎ±T
pdÎ± p < Ïµ
for sufï¬ciently small Ïµ. However, since the parameter Î± p has its physical mean-
ing as the natural parameter in exponential family distribution p(vec((p))|Î± p),
the Euclidean distance between Î± p and Î±â€²
p does not account for the information
geometry of its parameter space, and thus is not a good distance measure between
p(vec((p))|Î± p) and p(vec((p))|Î±â€²
p).
Instead, the symmetric KL divergence, a metric in the Riemannian space, is found
to be a natural distance measure for parameters Î± p, and it is deï¬ned as [9]
K Lsym(Î± p, Î±â€²
p)
= Ep(vec((p))|Î± p)

ln
p(vec((p))|Î± p)
p(vec((p))|Î±â€²p)

+ Ep(vec((p))|Î±â€²p)

ln
p(vec((p))|Î±â€²
p)
p(vec((p))|Î± p)

.
Using this metric, the natural gradient that points in the direction of the steepest
ascent in the Riemannian space is deï¬ned as â–½n
Î± p f (Î± p) = arg maxdÎ± p f (Î± p + dÎ± p)
subject to K Lsym(Î± p, Î± p + dÎ± p) < Ïµ for sufï¬ciently small Ïµ [9]. Since this natural
gradient utilizes the implicit information of parameter space, it is expected to give
faster convergence than the traditional Euclidean metric-based gradient in stochas-
tic gradient descent, and this has been demonstrated in the maximum-likelihood
estimation problem [10].
Computing the natural gradient of
f (Î± p) under the KL divergence of
p(vec((p))|Î± p) and p(vec((p))|Î±â€²
p) is easy, since p(vec((p))|Î± p) lies in the
exponential family. It is shown in [10] that the natural gradient â–½n
Î± p f (Î± p) can be

5.2 Interpreting VI Update from Natural Gradient Descent Perspective
85
calculatedbypremultiplyingthetraditionalgradient â–½Î± p f (Î± p)withtheinverseofthe
Fisherinformationmatrixof p(vec((p))|Î± p),i.e.,â–½n
Î± p f (Î± p) = G(Î± p)âˆ’1â–½Î± p f (Î± p)
where
G(Î± p) â‰œEp(vec((p))|Î± p)

â–½Î± p ln p(vec((p))|Î± p)

â–½Î± p
ln p(vec((p))|
Î± p)
T 
. Substituting the functional form of p(vec((p))|Î± p) into the deï¬nition of
Fisher information matrix, it can be shown that G(Î± p) is just the second derivative of
the log normalizer [10], i.e., G(Î± p) = â–½2
Î± p

L((p))

. Using this result and (5.25),
the natural gradient â–½n
Î± p f (Î± p) is
â–½n
Î± p f (Î± p) = âˆ’E	
 j Ì¸=(p) Q( j)

IP+1
n=1 t

Y(n),

Î· j, Î· j Ì¸= (p)
vec
 1
2

+ IP+1
n=1 ln

Î· j, Î· j Ì¸= (p)

+ Î± p.
(5.26)
Similar derivations can be performed for other variables {Î¾(n)}IP+1
n=1, Î², and Î³ .
In particular, after substituting Qâˆ—(Î¾(n)) = p(Î¾(n)|Î±Î¾(n)), Qâˆ—(Î²) = p(Î²|Î±Î²) and
Qâˆ—(Î³ ) = p(Î³ |Î»Î³ ) into problem (5.7), and using the deï¬nition of natural gradient,
we obtain
â–½n
Î±Î¾(n) f (Î±Î¾(n)) = âˆ’E	
 j Ì¸=Î¾(n) Q( j)

t

Y(n),

Î· j, Î· j Ì¸= Î¾(n)

vec
 1
2

+ l

Î· j, Î· j Ì¸= Î¾(n)


+ Î±Î¾(n)
(5.27)
â–½n
Î±Î² f (Î±Î²) = âˆ’E	
 j Ì¸=Î² Q( j)

âˆ’10âˆ’6 + IP+1
n=1 t

Y(n),

Î· j, Î· j Ì¸= Î²

10âˆ’6 âˆ’1 + IP+1
n=1 ln

Î· j, Î· j Ì¸= Î²


+ Î±Î²
(5.28)
â–½n
Î»Î³ f (Î»Î³ ) = âˆ’E	
 j Ì¸=Î³ Q( j)

âˆ’d + u({(p)}P
p=1) + IP+1
n=1 u(Î¾(n))
c âˆ’1 + w

+ Î»Î³ .
(5.29)
From the expressions (5.26)â€“(5.29), it is easy to see that the cyclical updates of (5.9),
(5.12), (5.15), and (5.18) are equivalent to the natural gradient descents with step
size 1 as follows:
Î±t+1
p
= Î±t
p âˆ’â–½n
Î± p f (Î±t
p)
(5.30)
Î±t+1
Î¾(n) = Î±t
Î¾(n) âˆ’â–½n
Î±Î¾(n) f (Î±t
Î¾(n))
(5.31)
Î±t+1
Î²
= Î±t
Î² âˆ’â–½n
Î±Î² f (Î±t
Î²)
(5.32)
Î»t+1
Î³
= Î»t
Î³ âˆ’â–½n
Î»Î³ f (Î»t
Î³ ).
(5.33)

86
5
When Stochastic Optimization Meets VI: Scaling â€¦
5.3
Scalable VI Algorithm for Tensor CPD
In update (5.31), computing the natural gradient is inexpensive, as the computation
in (5.27) only involves one data slice Y(n). However, computing natural gradients
for updates (5.30), (5.32), and (5.33) are much more expensive, as the whole dataset
{Y(n)}IP+1
n=1 is involved. To scale the inference algorithm to a massive data paradigm,
one promising way is to leverage stochastic optimization, where noisy, unbiased, and
cheap-to-compute gradients are used to substitute the original expensive gradients
in (5.30), (5.32), and (5.33) as shown in (5.34)â€“(5.36) as follows:
Î±t+1
p
= Î±t
p âˆ’Ït

âˆ’1
S
S

i=1
E	
 j Ì¸=(p) Q( j )

IP+1t

Y(ni),

Î· j, Î· j Ì¸= (p)
vec
 1
2

+ IP+1lni

Î· j, Î· j Ì¸= (p)

+ Î±t
p

 
!"
#
â‰œË†â–½n
Î± p f (Î±tp)
(5.34)
Î±t+1
Î²
= Î±t
Î² âˆ’Ït

âˆ’1
S
S

i=1
E	
 j Ì¸=Î² Q( j )

âˆ’10âˆ’6 + IP+1t

Y(ni),

Î· j, Î· j Ì¸= Î²

10âˆ’6 âˆ’1 + IP+1lni

Î· j, Î· j Ì¸= Î²


+ Î±t
Î²

 
!"
#
â‰œË†â–½n
Î±Î² f (Î±t
Î²)
(5.35)
Î»t+1
Î³
= Î»t
Î³ âˆ’Ït
 1
S
S

i=1
E	
 j Ì¸=Î³ Q( j )

âˆ’d + u({(p)}P
p=1) + IP+1u(Î¾(ni))
c âˆ’1 + w

âˆ’Î»t
Î³

 
!"
#
â‰œË†â–½n
Î»Î³ f (Î»t
Î³ )
.
(5.36)
In (5.34)â€“(5.36), unbiased noisy gradients Ë†â–½n
Î± p f (Î± p), Ë†â–½n
Î±Î² f (Î±Î²), and Ë†â–½n
Î»Î³ f (Î»Î³ )
are computed from uniformly sampled mini-batch data {Y(n1), Y(n2), . . . , Y(nS)}
with index ni âˆ¼Uniform(1, 2, 3, . . . , IP+1) for i = 1, 2, . . . , S [3]. Obviously, their
computations only involve S data slices, and thus can be very economical when
S â‰ªIP+1.Toguaranteeconvergence,thestepsizeÏt forupdateswithnoisygradients
must follow the Robbins and Monro conditions: 
t Ït = âˆand 
t Ï2
t < âˆ[2].
Withthesestochasticupdates(5.34)â€“(5.36)replacingthehigh-complexityupdates
(5.30), (5.32), and (5.33), a scalable VI algorithm for tensor CPD can be obtained.
More speciï¬cally, at each iteration, mini-batch data {Y(n1), Y(n2), . . . , Y(nS)} are
sampled
with
index
ni âˆ¼Uniform(1, 2, 3, . . . , IP+1).
Then,
Eqs.(5.31),
(5.34)â€“(5.36) are executed to update the model parameters. The process is repeated
until the algorithm is deemed to have converged.

5.3 Scalable VI Algorithm for Tensor CPD
87
5.3.1
Summary of Iterative Algorithm
In the updating equations (5.31), (5.34)â€“(5.36), there are several expectation compu-
tations, and computing these expectations involve the properties of tensor algebra.
To keep the presentation concise, the ï¬nal iterative algorithm is summarized in Algo-
rithm 7. Details on various expressions can be found in [18].
5.3.2
Further Discussions
To gain more insights from the above scalable tensor CPD algorithm, discussions
of rank and factor matrices estimation, feature matrix recovery, computational com-
plexity, memory requirement, convergence property, and selection of a threshold for
column pruning are presented in the following.
5.3.2.1
Rank and Factor Matrices Estimation
For rank estimation, although we do not have an explicit variable for the unknown
rank, we do have variables Î³ âˆ’1
l
for modeling the â€œpowerâ€ of the lth column of the
factor matrices. More speciï¬cally, after convergence, some EQâˆ—(Î³l)[Î³l] = at
l /bt
l will
become very large, e.g., 106, indicating that the power of the corresponding columns
in the factor matrices becomes zero. Therefore, these columns can be safely pruned
out, and the remaining number of columns is the estimated tensor rank R. Meanwhile,
since Qâˆ—((p)) is a Gaussian distribution, factor matrices {(p)}P
p=1 are estimated
by the mean {M(p,t)}P
p=1 of {Qâˆ—((p))}P
p=1 at convergence.
5.3.2.2
Estimation of Feature Matrix
The factor matrices {(p)}P
p=1 are obtained after Algorithm 7 terminates, while the
feature matrix (P+1) is not explicitly recovered. In fact, in many applications such
as the frequency estimation and video background modeling [13], since the goal is
to ï¬nd the subspace of signals but not the features, directly applying Algorithm 7
is sufï¬cient. Even in some applications where the feature matrix is desired, it can
be easily learned in a scalable way. In particular, after executing Algorithm 7, the
data Y(n) for n = 1, 2, 3, . . . , IP+1 can be fed to (5.39) sequentially and Î¼(n) is the
estimate of the nth row of the feature matrix (P+1).

88
5
When Stochastic Optimization Meets VI: Scaling â€¦
Algorithm 7 Scalable VI CPD (SVI-S CPD)
Initialization:
Choose
L > R,
1 â‰¤S â‰¤IP+1,
and
initialize
{M(p,0) âˆˆRIpÃ—L, 
(p,0) âˆˆ
RLÃ—L}P
p=1. Choose a sequence {Ït}t that satisfying 
t Ït = âˆand 
t Ï2
t < âˆ. Let {F(p,0) =


(p,0)âˆ’1 M(p,0)}P
p=1, a0
l = 10âˆ’6, b0
l = 10âˆ’6 for l = 1, 2, 3, . . . , L; c0 = d0 = 10âˆ’6.
Iterations: For the tth iteration (t â‰¥0)
Sample {ni}S
i=1 uniformly ni âˆ¼Uniform(1, 2, 3, . . . , IP+1).
Update the parameter of Qâˆ—(Î¾(ni))t+1:
Î±t+1
Î¾(ni ) =
 
tâˆ’1 Î¼(ni)T ; 1
2vec

tâˆ’1 
(5.37)
with

tâˆ’1 = diag
at
1
bt
1
, . . . , at
R
bt
R

+ ct
dt

PâŠ™
p=1

M(p,t)T
M(p,t) + Ip
(p,t)

(5.38)
Î¼(ni) = ct
dt vec (Y(ni))T 
Pâ‹„
p=1 M(p,t)
t
(5.39)
Update the parameter of each Qâˆ—((p))t+1:
Î±t+1
p
=

vec

F(p,t+1)
; 1
2vec


(p,t+1)âˆ’1 
(5.40)
with


(p,t+1)âˆ’1
= (1 âˆ’Ït)


(p,t)âˆ’1
+ Ït
S
S

i=1

diag
at
1
bt
1
, . . . , at
R
bt
R

+ IP+1ct
dt

PâŠ™
m=1,mÌ¸=p

M(m,t)T M(m,t) + Im
(m,t)
âŠ™

Î¼(ni)T Î¼(ni) + t 
(5.41)
F(p,t+1) = (1 âˆ’Ït)F(p,t) + Ït IP+1
S
S

i=1
 ct
dt

Y(ni)
(p) 
Î¼(ni) â‹„

Pâ‹„
m=1,mÌ¸=p M(m,t)
(5.42)
M(p,t+1) = F(p,t+1)T 
(p,t+1).
(5.43)
Update the parameter of Qâˆ—(Î³ )t+1:
Î»t+1
Î³
=

âˆ’bt+1; (at+1
l
âˆ’1)1

(5.44)
where at+1
l
= 10âˆ’6 + 1
2
P
p=1 Ip + IP+1

and bt+1 = [bt+1
1
, bt+1
2
, . . . , bt+1
L
]T with
bt+1
l
= (1 âˆ’Ït )bt
l + Ït
S
S

i=1

10âˆ’6 + 1
2

P

p=1

M(p,t+1)T
:,l
M(p,t+1)
:,l
+ Ip
(p,t+1)
l,l

+ IP+1

|Î¼(ni )l |2 + t
l,l
 
.
(5.45)

5.3 Scalable VI Algorithm for Tensor CPD
89
Update the parameter of Qâˆ—(Î²)t+1:
Î±t+1
Î²
=

âˆ’dt+1; ct+1 âˆ’1

(5.46)
where
ct+1 = (1 âˆ’Ït)(ct âˆ’1) + Ït
S
S

i=1
$
10âˆ’6 âˆ’1 + IP+1
	P
p=1 Ip
2
%
+ 1
(5.47)
dt+1 = (1 âˆ’Ït)dt + Ït
S
S

i=1

10âˆ’6 + IP+1
2
fi

(5.48)
fi = âˆ¥Y(ni ) âˆ¥2
F âˆ’2vec(Y(ni ))T 
Pâ‹„
p=1
M(p,t+1)
Î¼(ni )T
+ Tr

Î¼(ni )T Î¼(ni ) + t  PâŠ™
p=1

M(p,t+1) M(p,t+1)T+Ip
(p,t+1)
.
(5.49)
Until Convergence
5.3.2.3
Computational Complexity and Memory Requirements
At each iteration, the complexity is dominated by the update of each factor matrix,
costing O((P + 1)L2 	P
p=1 IpS + P
p=1 IpSL3) operations. From this expression,
itisclearthat Algorithm7 onlyincursalinearcomputationalcomplexitywithrespect
to the input tensor size 	P
p=1 IpS. Compared to the corresponding computational
cost of the batch-mode algorithm O((P + 1)L2 	P+1
p=1 Ip + L3(P+1
p=1 Ip)), which
is linear with respect to the input tensor size 	P+1
p=1 Ip, the computational cost of
the scalable algorithm at each iteration is much smaller than that of the batch-mode
algorithmas IP+1 ismuchlargerthan S.Furthermore,themaximummemoryrequired
for Algorithm 7 at each iteration is 	P
p=1 IpS, which again depends only on the mini-
batch size S, but not the whole data size IP+1. Thus, Algorithm 7 scales well to large
dataset if S is much smaller than IP+1.
5.3.2.4
Convergence Property
Although the functional minimization of the KL divergence is non-convex over
the mean-ï¬eld family Q() = 	
k Q(k), it is convex with respect to a sin-
gle variational density Q(k) when other {Q( j)| j Ì¸= k} are ï¬xed. Therefore,
Algorithm7,whichiterativelyupdatestheoptimalsolutionforeachk,isessentially
a coordinate descent algorithm in the functional space of variational distributions
with each update optimizing a convex subproblem. Furthermore, in the probabilistic
tensor decomposition model in Sect.5.1.1, the corresponding natural gradient for
problem (5.7) can be computed as (5.26)â€“(5.29) for each Q (k). If the step size

90
5
When Stochastic Optimization Meets VI: Scaling â€¦
is appropriately chosen, the gradient descent step using an unbiased estimate of the
natural gradient (5.34)â€“(5.36) will decrease the objective function of the subproblem
(5.7) on average. Notice that the optimal solution of each subproblem is unique, as
there is only one solution that makes the derivative zero. Furthermore, since each
subproblem is continuously differentiable, convex, and has a bounded and compact
feasible set, the proposed algorithm is guaranteed to converge to at least a stationary
point on average [17].
5.3.2.5
Selection of Threshold for Pruning Columns
The speciï¬c threshold for pruning columns in general depends on the application.
A widely used choice is 106. However, this ï¬xed threshold might not give the best
performance for all applications, especially when the SNR is low. Another choice
is to set the threshold at 100 times the minimal value of Î³l. This strategy has been
shown empirically to work well in practice.
5.4
Numerical Examples
In this section, numerical results from both synthetic data and real-world datasets
are presented to assess the performance of Algorithm 7, with the batch-mode algo-
rithm in (Algorithm 5 in Chap.3, labeled as Batch VI) serving as a benchmark.
For Algorithm 7, the dataset Ë™Y is sliced along the last dimension. The initial fac-
tor matrix M(p,0) is set as the ï¬rst iteration output of the batch-mode algorithm
with {Y(n1), Y(n2), . . . , Y(nS)} being the tensor input, the initial 
(p,0) = I L with
L = min{I1, I2, . . . , IP}. The step size sequence is chosen as Ït = (t + 1)âˆ’0.9 [2].
Each result in this section is obtained by averaging 100 Monte Carlo runs. All the
experiments were conducted in MATLAB R2015b with an Intel Core i5-4570 CPU
at 3.2 GHz and 8 GB RAM.
5.4.1
Convergence Performance on Synthetic Data
A 4D data tensor [[A(1), A(2), A(3), A(4)]] âˆˆR20Ã—20Ã—20Ã—1000 with rank R = 5 is con-
sidered. The factor matrices {A(p)}3
p=1 are drawn from MN(A(p)| 020Ã—5, I20Ã—20,
I5Ã—5), and matrix A(4) âˆ¼MN(A(4)| 01000Ã—5, I1000Ã—1000, I5Ã—5). The signal-to-noise
ratio (SNR) is deï¬ned as 10 log10(âˆ¥[[A(1), A(2), A(3), A(4)]] âˆ¥2
F / âˆ¥Ë™W âˆ¥2
F). To see
how the choice of mini-batch size affects the accuracy and speed, we consider
three scenarios: S = 1, S = 5, and S = 10, and the corresponding algorithms are
labeled as SVI-1, SVI-5, and SVI-10, respectively. The upper bound on the rank for
Algorithm 7 and batch-mode VI algorithm is chosen as L = 20.

5.4 Numerical Examples
91
A commonly used criterion for assessing how close the estimated M(p) is to
the ground truth factor matrix (p) is the averaged largest principal angle (LPA)
1
3
3
p=1 LPA(M(p), (p)). LPA(A, B) is a measure of the â€œdistanceâ€ between two
subspaces spanned by the columns of matrices A and B, deï¬ned as cosâˆ’1{Ïƒmin{orth
{A}Horth{B}}}, where the operator Ïƒmin{ Q} denotes the smallest singular value of
the matrix Q, and orth{ Q} is an orthonormal basis for the subspace spanned by the
columns of Q. The LPA is also called the angle of subspaces, and can be computed
in MATLAB by the function â€œsubspaceâ€. Notice that while LPA computation allows
M(p) and (p) to have a different number of columns (i.e., estimated rank differs
from true rank), this criterion makes the most sense when M(p) and (p) have the
same number of columns. This can be seen from an example that if M(p) is randomly
generated with many more columns than that of (p), the true subspace generated
by columns of (p) would be embedded in the subspace generated by columns of
M(p), thus giving a zero LPA. However, since (p) and M(p) have very different
sizes, M(p) would not be a good estimate of (p). In order to avoid the misleadingly
small LPA under incorrect rank estimate, if the tensor rank is overestimated in M(p),
only the principal columns of M(p) with the R largest powers are used for computing
the LPA. Similarly, if the tensor rank is underestimated in M(p) with Râ€² < R, only
the principal columns of (p) with the Râ€² largest powers are used.
The LPA assesses the performance of subspace recovery. However, due to the
uniqueness property introduced in Chap.5, tensor CPD in fact returns the columns
of each factor matrix up to an unknown permutation and scaling ambiguity, which
is stronger than simply subspace recovery. To directly assess the accuracies of factor
matrices recovery (rather than subspace recovery), the best sum congruence, which
involves computing the normalized correlation between the true columns in (p)
and the estimated columns in M(p), is a more suitable criterion. Mathematically, it
is deï¬ned as
max
Ï€(Â·)
R

r=1
(1)H
:,r
M(1)
:,Ï€(r)
âˆ¥(1)
:,r âˆ¥âˆ¥M(1)
:,Ï€(r) âˆ¥
(2)H
:,r
M(2)
:,Ï€(r)
âˆ¥(2)
:,r âˆ¥âˆ¥M(2)
:,Ï€(r) âˆ¥
(3)H
:,r
M(3)
:,Ï€(r)
âˆ¥(3)
:,r âˆ¥âˆ¥M(3)
:,Ï€(r) âˆ¥
where the maximum is taken with respect to all possible permutations Ï€(Â·) of the
factor matricesâ€™ columns, which can be found using greedy search algorithm [16].
Similar to LPA, best sum congruence was previously applied when the rank estimate
is accurate. In order to incorporate inaccurate rank estimate in M(p), if the rank is
overestimated in M(p), the R columns of M(p) that match the columns of (p) most
after permutation Ï€(Â·) are used. On the other hand, if the rank is underestimated in
M(p) with Râ€² < R, only Râ€² columns of (p) that match the columns of M(p) most
are used in the computation of best sum congruence.
Figure5.2 shows the convergence performance of Algorithm 7 in terms of LPA.
It can be seen that for both SNR = 10 dB and SNR = 20 dB, the performance
of Algorithm 7 converges to that of the batch-mode algorithm. In particular, LPA
decreases quickly at the beginning of the iterations, and gets pretty close to the LPA of
batch-mode processing within a few hundred iterations. After that, the performance

92
5
When Stochastic Optimization Meets VI: Scaling â€¦
0
200
400
600
800
1000
1200
1400
1600
1800
2000
Number of mini-batch
10-2
10-1
100
101
102
Averaged LPA
SGD
SVI-1
SV1-5
SVI-10
Batch-Mode
20 dB
10 dB
10 dB
20 dB
Fig. 5.2 Convergence performance of Algorithm 7 with SNR = 10 dB and 20 dB
improvement becomes smaller and smaller. This is due to the learning rate Ït going
to a very small value as the iteration number becomes large. Furthermore, after
processing the ï¬rst 1000 mini-batch data, the LPA gaps between the SVI-5/SVI-
10 algorithms and the batch-mode algorithm are nearly unnoticeable. Even for the
SVI-1 algorithm, the LPA gap is just 0.0492â—¦and 0.0157â—¦at SNR = 10 dB and
SNR = 20 dB, respectively, showing that SVI-1 provides a very good approximation
to the batch-mode result.
In addition toAlgorithm 7, Fig.5.2 also shows the performance of the algorithm in
[1] (labeled as SGD)3 with an upper bound on the rank (L = 20), and other parameter
settings following those described in [1]. From Fig.5.2, it is clear that Algorithm 7
exhibits much better performance. This is because the algorithm in [1] is not capable
of recovering the correct rank from the provided upper bound, thus the superï¬‚uous
basis vectors in the factor matrices degrade the subspace recovery accuracy. This
shows the importance of accurate rank information in tensor CPD.
Next, the best sum congruences obtained from different CPD algorithms are
presented in Table5.1. From the table, it can be seen that when SNR = 10 dB,
Algorithm 7 under different mini-batch sizes performs nearly the same as the batch-
mode algorithm in terms of the best sum congruence after processing just 100 mini-
batches of data, and they perform signiï¬cantly better than the SGD algorithm. When
the number of mini-batches is increased to 1000 or beyond, all the schemes would
have the best sum congruence equal or very close to the maximum value (which is 5
for this particular simulation setting). Similar conclusions can be drawn for SNR =
3 Notice that the algorithm in [1] was originally developed for 3D tensors. We extended it to work
in 4D tensor data.

5.4 Numerical Examples
93
Table 5.1 The best sum congruence obtained by different algorithms
Algorithm
SNR = 10 dB
SNR = 20 dB
100
mini-
batches
500
mini-
batches
1000
mini-
batches
2000
mini-
batches
100
mini-
batches
500
mini-
batches
1000
mini-
batches
2000
mini-
batches
SVI-1
4.9999
5.0
5.0
5.0
5.0
5.0
5.0
5.0
SVI-5
5.0
5.0
5.0
5.0
5.0
5.0
5.0
5.0
SVI-10
5.0
5.0
5.0
5.0
5.0
5.0
5.0
5.0
Batch-mode 5.0
5.0
5.0
5.0
5.0
5.0
5.0
5.0
SGD
2.9853
4.9980
4.9994
4.9997
4.9972
4.9994
4.9996
4.9997
SVI-1
SVI-5
SVI-10
Batch-mode VI
0
10
20
30
40
50
60
70
80
Averaged Running Time (s)
10 dB
20 dB
Fig. 5.3 Averaged running time of Algorithm 7 and batch-mode VI
20 dB, except various schemes reach the maximum best sum congruence at an even
smaller number of mini-batches.
On the other hand, the averaged running time of Algorithm 7 (after processing
1000 mini-batch data) and the batch-mode VI algorithm over 100 Monte Carlo runs
are presented in Fig.5.3. It is seen that the SVI-1, SVI-5, and SVI-10 algorithms are
around 20, 8, and 4 times faster than the batch-mode algorithm,4 respectively. This
shows the excellent scalability of the proposed algorithm for massive data.
5.4.2
Tensor Rank Estimation on Synthetic Data
Since the LPA and best sum congruence only give a partial picture of the performance
of Algorithm 7 under unknown rank, the rank determination ability is assessed
4 The termination criterion of the batch-mode algorithm follows the recommendation from [13].

94
5
When Stochastic Optimization Meets VI: Scaling â€¦
explicitly in this subsection. In particular, the tensor rank learned by Algorithm 7
under different SNRs is shown in Fig.5.4, with each vertical bar showing the mean
and the error bars showing the standard deviation of tensor rank estimates. The blue
horizontal dotted line indicates the true tensor rank. From Fig.5.4, it is seen that
Algorithm 7 with different mini-batch sizes recovers the true tensor rank with 100%
accuracy for a wide range of SNRs, in particular when SNR â‰¥10 dB. Even though
the performance under SNR â‰¤5 dB is not 100% accurate, it can be observed that
Algorithm 7 still gives estimates close to the true tensor rank with a high probability.
For a comprehensive performance assessment, Table5.2 presents the percentages of
correct rank estimation, overestimation, and underestimation. From Table5.2, it can
be seen that while the percentages of overestimation and underestimation are not
equal in general, they are relatively small even for SNR at 0 dB and 5 dB. Notice
that when the SNR is larger than 5 dB, all of the underestimation and overestimation
percentages are zeros. Thus there is no need to include these results in the table.
One may wonder whether the performance of Algorithm 7 is better than the
classical algorithm starting with a small subset of data and then followed by the
deterministic sequential algorithm in [1]. In particular, the classical method for
ï¬nding the tensor rank is to run multiple ALS-based CPD algorithms with differ-
ent assumed ranks, and then choose the model with the smallest rank that ï¬ts the
data well. After the rank is estimated with a small subset of data, the deterministic
sequential algorithm in [1] is used to update the subspace when more data come in
(but with a ï¬xed estimated rank). We refer to such a classical scheme as ALS-SGD.
On the other hand, with the automatic rank estimation ability in the recent batch-
mode probabilistic CPD [13], one could also consider a hybrid scheme in which the
multiple ALSs are replaced by the recent batch-mode probabilistic CPD. We refer
to this hybrid scheme as Probabilistic CPD + SGD.
-5
0
5
10
15
20
25
SNR (dB)
0
1
2
3
4
5
6
7
8
Tensor Rank Estimate
SVI-1
SVI-5
SVI-10
Batch-mode
Fig. 5.4 Rank estimates of Algorithm 7 and batch-mode VI

5.4 Numerical Examples
95
Table 5.2 Percentages of correct rank estimation, underestimation, and overestimation by different algorithms
Algorithm
SNR = âˆ’5 dB
SNR = 0 dB
SNR = 5 dB
Underestimation
percentage (%)
Overestimation
percentage (%)
Correct
estimation
percentage (%)
Underestimation
percentage (%)
Overestimation
percentage (%)
Correct
estimation
percentage (%)
Underestimation
percentage (%)
Overestimation
percentage (%)
Correct
estimation
percentage (%)
SVI-1
19
39
42
6
8
86
3
4
93
SVI-5
21
27
52
4
6
90
3
1
96
SVI-10
28
12
60
4
2
94
2
0
98
Batch-mode
26
10
64
4
2
94
0
0
100

96
5
When Stochastic Optimization Meets VI: Scaling â€¦
Table 5.3 Performance of different rank estimation schemes
Algorithm
SNR = 0 dB
SNR = 20 dB
LPA
(degree)
Best sum
congru-
ence
Running
time (s)
Rank
accuracy
(%)
LPA
(degree)
Best sum
congru-
ence
Running
time (s)
Rank
accuracy
(%)
ALS + SGD
0.5094
4.2438
78.3712
79
0.0482
5.0
26.4884
100
Probabilistic
CPD + SGD
0.5094
4.2655
6.1294
79
0.0482
5.0
4.1097
100
SVI-1
0.4484
4.9071
5.2435
86
0.0489
5.0
3.6034
100
To compare the performance of the three schemes, (1) ALS + SGD, (2) Proba-
bilistic CPD + SGD, and (3) SVI-1,5 we conduct experiments on synthetic tensor
data with size 20 Ã— 20 Ã— 20 Ã— 1000. For the classical scheme ALS + SGD and the
Probabilistic CPD + SGD, a subset of data of size 20 Ã— 20 Ã— 20 Ã— 20 was used for
the estimation of the rank. The results are shown in Table5.3. It can be seen that when
the SNR is high (SNR = 20 dB), all three schemes achieve 100% rank estimation
accuracy, the same best sum congruence, and nearly the same LPA. This is because
when the SNR is high, a small amount of data is sufï¬cient to obtain an accurate rank
estimate. On the other hand, when the SNR is low (SNR = 0 dB), both the ALS +
SGD and Probabilistic CPD + SGD schemes achieve lower rank estimation accuracy
along with inferior LPA and best sum congruence compared to those of the SVI-1
algorithm. This is due to the fact that when the SNR is low, the small subset of data
at the beginning is not sufï¬cient for accurate rank estimation, which in turn hurts
the subsequent subspace learning based on the ï¬xed estimated rank. In contrast, the
SVI-1 algorithm makes use of all the data in both rank and subspace estimations,
thus giving better performance. In terms of computation time, since ALS + SGD
needs to run multiple ALS algorithms, it requires a much higher computation time
than the SVI-1 algorithm. For the hybrid scheme, since it is capable of estimating the
rank automatically, its complexity is much smaller than that of the classical scheme
ALS + SGD, but still slightly higher than that of SVI-1.
From the above results, it is obvious that the probabilistic approach (in both
Probabilistic CPD + SGD and SVI-1) is instrumental in reducing the complexity of
rank estimation compared to the classical scheme ALS + SGD. Furthermore, the
SVI-1 has the added advantages of accurate rank estimation, small LPA, and large
best sum congruence under low SNR, since it makes use of all data in rank estimation.
The simulation results presented so far are for well-conditioned tensors, i.e., the
columns in each of the factor matrices are independently generated. In order to fully
assess the rank learning ability of Algorithm 7, we consider another 4D data tensor
[[ Â¯A
(1), A(2), A(3), A(4)]] âˆˆR20Ã—20Ã—20Ã—1000 with rank R = 5. The factor matrix Â¯A
(1) =
0.1 Ã— 120Ã—5 + 2âˆ’s Ã— A(1). The factor matrices {A(p)}3
p=1 are drawn from MN(A(p)|
5 Since the algorithm in [1] was developed for a mini-batch data with size 1 only, we focus on the
comparison with SVI-1.

5.4 Numerical Examples
97
Table 5.4 Rank estimate accuracy when columns in all factor matrices are correlated
s
0
1
2
3
Batch-mode (%)
100
100
86
0
SVI-1 (%)
100
100
0
0
SVI-5 (%)
100
100
0
0
SVI-10 (%)
100
100
16
0
020Ã—5, I20Ã—20, I5Ã—5), and the matrix A(4) âˆ¼MN(A(4)| 01000Ã—5, I1000Ã—1000, I5Ã—5).
According to the deï¬nition of the tensor condition number [11, 12], when s increases,
the correlation among the columns in the factor matrix Â¯A
(1) increases, and the tensor
condition number becomes larger. In particular, when s goes to inï¬nity, the condition
number of the tensor goes to inï¬nity too. Experiments were conducted to evaluate
the rank estimation accuracy of Algorithm 7 and the batch-mode algorithm under
SNR = 20 dB and s âˆˆ{0, 1, 10, 102, 103, 104}. It was found that both Algorithm 7
and the batch-mode algorithm achieve 100% accuracy when the correlation between
columns only happens in one factor matrix, even the tensor condition number is very
large.
Next, we consider an extreme case when the columns in all factor matrices
are highly correlated. In particular, a 4D data tensor [[ Â¯A
(1), Â¯A
(2), Â¯A
(3), Â¯A
(4)]] âˆˆ
R20Ã—20Ã—20Ã—1000 with rank R = 5 is constructed, with Â¯A
(p) = 0.1 Ã— 120Ã—5 + 2âˆ’s Ã—
A(p) for p = 1, 2, 3, 4. The factor matrices {A(p)}3
p=1 are drawn from MN(A(p)|
020Ã—5, I20Ã—20, I5Ã—5), and the matrix A(4) âˆ¼MN(A(4)| 01000Ã—5, I1000Ã—1000, I5Ã—5).
The performance of the tensor rank estimation is shown in Table5.4. It can be seen
that as s increases (i.e., the columns are more correlated in all factor matrices), the
performance of Algorithm 7, and the batch-mode algorithm degrades accordingly.
5.4.3
Video Background Modeling
One of the most important tasks in video surveillance is to separate the foreground
objects from the background. Since the background remains the same along the video
frames, it can be modeled as the low-rank subspace of the video data. In particular,
after organizing the video data into a 3D tensor (pixels in a frame Ã— RGB Ã— frame)
[13] as shown in Fig.5.5, the background can be extracted using the tensor CPD.
We conduct experiments on the popular shopping mall video data.6 Each frame
is with size 256 Ã— 320, and there are 100 frames. After organizing the data into a
3D tensor with dimension 81920 Ã— 3 Ã— 100 [13], the batch-mode VI algorithm and
the SVI-1 are run to learn the subspace (background). In this application, the SVI-1
is terminated after processing 100 mini-batches (i.e., the number of video frames).
6 http://pages.cs.wisc.edu/jiaxu/projects/gosus/supplement/.

98
5
When Stochastic Optimization Meets VI: Scaling â€¦
Fig. 5.5 Background modeling with a 3D tensor
Fig. 5.6 a Background estimation using the Batch-mode VI and b Background estimation using
the SVI-1 algorithm
The learning results of the two algorithms are shown in Fig.5.6. The value of L
used in this scenario is 3 and the estimated rank is 2, both for the SVI-1 algorithm
and the batch-mode algorithm. It can be seen that the two algorithms achieve nearly
the same performance in background estimation (with mean square error per pixel
being 0.3613). To quantify the difference in the estimation results, Fig.5.7 shows
the LPA between the subspaces estimated by the batch-mode algorithm and the
SVI-1 algorithm. It can be seen that the LPA decreases as more mini-batch data
is processed. At the beginning, the LPA is 10.02â—¦. But after processing 100 mini-
batch data, the LPA decreases to 0.0776â—¦. However, the SVI-1 algorithm only costs
1.6317s, while the batch-mode VI algorithm consumes 24.2254s in running time.
Obviously, the SVI-1 algorithm achieves nearly the same performance as the batch-
mode VI algorithm while saving 93.2% computation time. Notice that the video
data might not be generated from a Gaussian distribution, and the CPD factorsâ€™
columns modeling the background are not really independent, thus deviating from
the prior assumption (5.5) of the probabilistic model. However, after the Bayesian
inference, the CPD factorsâ€™ columns become correlated with covariance matrices
shown in (5.41). In some sense, the inaccurate prior assumption could be remedied
by learning from data, which is validated by the good result shown in Fig.5.6.

5.4 Numerical Examples
99
10
20
30
40
50
60
70
80
90
100
Number of mini-batch
10-2
10-1
100
101
102
Averaged LPA
Fig. 5.7 LPA evolution for video data processing
5.4.4
Image Feature Extraction
In this subsection, we conduct experiments on 1000 images from the CIFAR-10
dataset,7 representing 2 different objects (500 images for the dog and 500 images for
the automobile). In particular, each image is of size 32 Ã— 32, and the training data
can be naturally represented by a fourth-order tensor R32Ã—32Ã—3Ã—1000. The batch-mode
algorithm and the SVI-1 algorithm (terminated after 1000 mini-batches) are run to
learn the basis matrices and the feature vectors of these 1000 training images. With
Fig.5.8 showing the LPA between the subspace estimated by the batch-mode algo-
rithm and the SVI-1 algorithm, it can be seen that after processing 1000 mini-batch
data, the LPA reduces from 90â—¦to only 0.0503â—¦. Furthermore, using the obtained
feature vectors of the training images, a 1-nearest neighbors (1-NN) classiï¬er is
trained, and the trained classiï¬er is executed to recognize the objects for another 100
testing images projected onto the basis matrices. The accuracy of the 1-NN classi-
ï¬er using features from the SVI-1 algorithm and the batch-mode algorithm are both
78%, showing that the 0.0503â—¦LPA does not affect the classiï¬cation performance.
However, the SVI-1 algorithm only costs 4.0533s while the batch-mode algorithm
costs 36.7521s.
7 https://www.cs.toronto.edu/~kriz/cifar.html.

100
5
When Stochastic Optimization Meets VI: Scaling â€¦
0
200
400
600
800
1000
Number of mini-batch
10-2
10-1
100
101
102
Averaged LPA
Fig. 5.8 LPA evolution for image feature extraction
References
1. M. Mardani, G. Mateos, G.B. Giannakis, Subspace learning and imputation for streaming big
data matrices and tensors. IEEE Trans. Signal Process. 63(10), 2663â€“2677 (2015)
2. J.C. Spall, Introduction to Stochastic Search and Optimization: Estimation, Simulation, and
Control (Wiley, New York, 2003)
3. M. Hoffman, D. Blei, J. Paisley, C. Wang, Stochastic variational inference. J. Mach. Learn.
Res. 14, 1303â€“1347 (2013)
4. K.P. Murphy, Machine Learning: A Probabilistic Perspective (MIT Press, Cambridge, 2012)
5. M.J. Wainwright, M.I. Jordan, Graphical models, exponential families, and variational infer-
ence. Found. Trends Mach. Learn. 1(102), 1â€“305 (2008)
6. B. Leon, Large-scale machine learning with stochastic gradient descent, in Proceedings of
COMPSTAT 2010 (Physica-Verlag HD, 2010), pp. 177â€“186
7. M. Li, D. Andersen, A. Smola, K. Yu, Communication efï¬cient distributed machine learning
with the parameter server, in Proceedings of Neural Information Processing Systems (NIPS)
(2014)
8. M. Li, T. Zhang, Y. Chen, A. Smola, Efï¬cient mini-batch training for stochastic optimization, in
Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)
(2014)
9. C. Udriste, Convex Functions and Optimization Methods on Riemannian Manifolds (Springer
Science and Business Media Press, Berlin, 1994)
10. S. Amari, S.C. Douglas, Why natural gradient?, in Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), no. 2 (1998), pp. 1213â€“
1216
11. P. Breiding, N. Vannieuwenhoven, The condition number of join decompositions. SIAM J.
Matrix Anal. Appl. 39(1), 287â€“309 (2018)
12. N. Vannieuwenhoven, Condition numbers for the tensor rank decomposition. Linear Algebra
Appl. 535, 35â€“86 (2017)
13. Q. Zhao, G. Zhou, L. Zhang, A. Cichocki, S.I. Amari, Bayesian robust tensor factorization for
incomplete multiway data. IEEE Trans. Neural Netw. Learn. Syst. 27(4), 736â€“748 (2016)

References
101
14. J. Paisley, D. Blei, M. Jordan, Bayesian nonnegative matrix factorization with stochastic varia-
tional inference, in Handbook of Mixed Membership Models and Their Applications (Chapman
and Hall/CRC, London, 2014)
15. L. Cheng, Y.-C. Wu, H.V. Poor, Probabilistic tensor canonical polyadic decomposition with
orthogonal factors. IEEE Trans. Signal Process. 65(3), 663â€“676 (2017)
16. N.D. Sidiropoulos, G.B. Giannakis, R. Bro, Blind PARAFAC receivers for DS-CDMA systems.
IEEE Trans. Signal Process. 48(3), 810â€“823 (2000)
17. P. Tseng, Convergence of a block coordinate descent method for nondifferentiable minimiza-
tion. J. Optim. Theory Appl. 109(3), 475â€“494
18. L. Cheng, Y.-C. Wu, H.V. Poor, Scaling probabilistic tensor canonical polyadic decomposition
to massive data. IEEE Trans. Signal Process. 66(21), 5534â€“5548 (2018)

Chapter 6
Bayesian Tensor CPD with Nonnegative
Factors
Abstract In previous chapters, the probabilistic modeling and inference algorithm
for the CPD with no constraint are discussed. In practical data analysis, one usually
has additional prior structural information for the factor matrices, e.g., nonnegative-
ness and orthogonality. Encoding this structural information into the probabilistic
tensor modeling while still achieving tractable inference remains a critical chal-
lenge. In this chapter, we introduce the development of Bayesian tensor CPD with
nonnegative factors, with an integrated feature of automatic tensor rank learning. We
will also connect the algorithm to the inexact block coordinate descent (BCD) to
obtain a fast algorithm.
6.1
Tensor CPD with Nonnegative Factors
In the previous three chapters, the factor matrices of CPD are without constraints.
If the nonnegative constraints are added, the resultant model allows only additions
among the R latent rank-1 components. This leads to a parts-based representation of
the data, in the sense that each rank-1 component is a part of the data, thus further
enhancing the model interpretability. Below is a motivating application.
6.1.1
Motivating Exampleâ€”Social Group Clustering
Social group clustering could be beneï¬ted from tensor data analysis, by which multi-
ple views of social networks are provided [1, 2]; see Fig.6.1. For example, consider a
3D email dataset Y âˆˆRIÃ—JÃ—K with each element Y(i, j, k) denoting the number of
emails sent from person i to person j at the kth day. Each frontal slice Y(:, :, k) rep-
resents the connection intensity among different pairs of peoples in the kth day, while
each slice Y(:, j, :) shows the temporal evolution of the number of received mails for
the person j from each of the other person in the dataset. Consequently, decompos-
ing the tensor Y into latent CPD factors {A âˆˆRIÃ—R, B âˆˆRJÃ—R, C âˆˆRKÃ—R} reveals
different clustering groups from different views (i.e., different tensor dimensions).
Â© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
L. Cheng et al., Bayesian Tensor Decomposition for Signal Processing and
Machine Learning, https://doi.org/10.1007/978-3-031-22438-6_6
103

104
6
Bayesian Tensor CPD with Nonnegative Factors
Fig. 6.1 An application example of CPD: social group clustering
In particular, using the unfolding property of tensor CPD, we have
Y(1) = (C â‹„B)AT ,
(6.1)
Y(3) = (B â‹„A)CT ,
(6.2)
where Y(k) is a matrix obtained by unfolding the tensor Y along its kth dimension,
and symbol â‹„denotes the Khatriâ€“Rao product (i.e., column-wise Kronecker prod-
uct, see Deï¬nition 1.3). From (6.1), each column vector Y(1)(:, i) âˆˆRJ KÃ—1 can be
written as Y(1)(:, i) = R
r=1(C â‹„B):,r Ai,r, which is a linear combination of column
vectors in matrix (C â‹„B) âˆˆRJ KÃ—R with coefï¬cients {Ai,r}R
r=1, and it represents the
email sending pattern of person i. Thus, each column vector in matrix C â‹„B can
be interpreted as one of the R underlying email sending patterns, and Ai,r is the
linear combining coefï¬cient to generate the person iâ€™s email pattern. Similarly, from
(6.2), each column in B â‹„A can be interpreted as a temporal pattern and Ck,r is the
coefï¬cient of the rth temporal pattern for generating the kth dayâ€™s pattern.
Obviously, due to the complicated multi-view clustering structure, tensor CPD is
superior to the matrix-based models such as k-means or Gaussian mixture model. To
ï¬nd the latent factor matrices from the social network data Y, the following problem
is usually solved:

6.1 Tensor CPD with Nonnegative Factors
105
min
A,B,C âˆ¥Y âˆ’A, B, C âˆ¥2
F
s.t. A â‰¥0IÃ—R, B â‰¥0JÃ—R, C â‰¥0KÃ—R,
(6.3)
where the nonnegative constraints are added to allow only additions among the R
latent rank-1 components. This leads to a parts-based representation of the data,
which enhances the model interpretability.
6.1.2
General Problem and Challenges
The general problem, which decomposes a N-dimensional tensor Y âˆˆRJ1Ã—J2Ã—Â·Â·Â·Ã—JN
into a set of nonnegative factor matrices {(n)}N
n=1, is formulated as:
min
{(n)}N
n=1
âˆ¥Y âˆ’
R

r=1
(1)
:,r â—¦(2)
:,r â—¦Â· Â· Â· â—¦(N)
:,r



â‰œ(1),(2),...,(N)
âˆ¥2
F
s.t. (n) â‰¥0JnÃ—R, n = 1, 2, . . ., N,
(6.4)
where symbol â—¦denotes the vector outer product.
In problem (6.4), there are two signiï¬cant challenges. Firstly, nonnegative factor
matrices {(n)}N
n=1 are complicatedly coupled, resulting in a difï¬cult non-convex
optimization problem. To tackle this challenge, alternating optimization is one of
the most commonly used techniques. In each iteration, after ï¬xing all but one fac-
tor matrices, problem (6.4) will become a standard nonlinear least-squares (NLS)
subproblem, for which there are various off-the-shelf algorithms for solving it [3],
including interior point methods and augmented Lagrangian methods. To handle big
tensor data, ï¬rst-order methods, such as Nesterov-type projected gradient descent,
have been proposed to replace the interior point methods to solve each subproblem
[4, 5].
Although pioneering works [4, 5] allow the learning of nonnegative factors from
big multi-dimensional data, they still face the second critical challenge of problem
(6.4): how to automatically learn the tensor rank R from the data? With the physical
meaning of tensor rank being the number of components/groups inside the data, this
value is usually unknown in practice and its estimation has been shown to be NP-
hard. If the tensor CPD has at least one factor matrix without nonnegative constraint
[6â€“8], this problem can be solved by applying Gaussian-gamma prior (GG) prior or
the Generalized Hyperbolic (GH) prior to the factor matrix without constraint, which
is a slight variation of the standard Bayesian CPD in Chap.3 (note that a standard
CPD is one having no constraint on all factor matrices).

106
6
Bayesian Tensor CPD with Nonnegative Factors
However, when all the factor matrices have nonnegative constraints, GG or GH
prior is not applicable since they are in the form of Gaussian scale mixture (GSM,
see Sect.2.3) and the support of the Gaussian probability density function (pdf) is
not restricted to the nonnegative region. This calls for a different prior distribution
modeling. An immediate idea might be to replace the Gaussian distribution with
the truncated Gaussian distribution with a nonnegative support. However, a closer
inspection is needed since there is no existing work discussing whether a gamma
distribution or generalized inverse Gaussian (GIG) is conjugate to a truncated Gaus-
sian distribution with a nonnegative support. This chapter focuses on demonstrating
the conjugacy of gamma distribution to the truncated Gaussian distribution.
6.2
Probabilistic Modeling for CPD with Nonnegative
Factors
6.2.1
Properties of Nonnegative Gaussian-Gamma Prior
Recall from Chap.2 that for a model parameter vector w âˆˆRMÃ—1 consisting of S non-
overlapped blocks with each block denoted by ws âˆˆRMsÃ—1. The Gaussian-gamma
prior can be expressed as
p(w|{Î±s}S
s=1) =
S

s=1
p(ws|Î±s) =
S

s=1
N(ws|0MsÃ—1, Î±âˆ’1
s
I Ms),
(6.5)
p({Î±s}S
s=1) =
S

s=1
gamma(Î±s|as, bs),
(6.6)
where Î±s is the precision parameter (i.e., the inverse of variance, also called weight
decay rate) that controls the relevance of model block ws in data interpretation, and
{as, bs}S
s=1 are pre-determined hyper-parameters. There are two important properties
of Gaussian-gamma prior that lead to its success and prevalence in a variety of appli-
cations. Firstly, after integrating the gamma hyper-prior, the marginal distribution
of model parameter p(w) is a studentâ€™s t distribution, which is strongly peaked at
zero and with heavy tails, thus promoting sparsity. Secondly, the gamma hyper-prior
(6.6) is conjugate to the Gaussian prior (6.5). This conjugacy permits the closed-form
solution of the variational inference, which has recently come up as a major tool in
inferring complicated probabilistic models with inexpensive computations.
Unfortunately, the support of the Gaussian pdf in (6.5) is unbounded and thus
cannot model nonnegativeness. On the other hand, the truncated Gaussian prior with
a nonnegative support for each model block ws can be written as:

6.2 Probabilistic Modeling for CPD with Nonnegative Factors
107
p+(w|{Î±s}S
s=1) =
S

s=1
p+(ws|Î±s)
=
S

s=1
N(ws|0MsÃ—1, Î±âˆ’1
s
I Ms)
	 âˆ
0Ms Ã—1 N(ws|0MsÃ—1, Î±âˆ’1
s
I Ms)
U

ws â‰¥0MsÃ—1

,
(6.7)
where the function U

ws â‰¥0MsÃ—1

is a unit-step function with value one when
ws â‰¥0MsÃ—1, and with a value of zero otherwise. Together with the gamma distribu-
tions (6.6) for modeling the precision parameters {Î±s}S
s=1, we have the nonnegative
Gaussian-gamma prior. Even though it is clear that nonnegative Gaussian-gamma
priorcanmodelthenonnegativenessofmodelparametersduetotheunit-stepfunction
U

ws â‰¥0MsÃ—1

, whether it enjoys the advantages of the vanilla Gaussian-gamma
prior needs further inspection. In the following, two properties of the nonnegative
Gaussian-gamma prior are presented, and their proofs can be found in [9].
Property 6.1. The gamma distribution in (6.6) is conjugate to the nonnegative
Gaussian distribution in (6.7).
Property 6.2. After integrating out the precision parameters {Î±s}S
s=1, the
marginal pdf of model parameter w is
p+(w) =

p+(w|{Î±s}S
s=1)p({Î±s}S
s=1)d{Î±s}S
s=1
=
S

s=1
2Ms
 1
Ï€
 Ms
2 (as + Ms/2)
(2bs)âˆ’as(as) (2bs + wT
s ws)âˆ’asâˆ’Ms/2
Ã— U

ws â‰¥0MsÃ—1

.
(6.8)
It is a product of multivariate truncated studentâ€™s t distributions, each of which
is with a nonnegative support.
The shape of the marginal distribution p+(w) is determined by the hyper-parameters
{as, bs}S
s=1. Usually, their values are chosen to be a very small value (e.g., Ïµ = 10âˆ’6),
since as as â†’0 and bs â†’0, a Jeffreyâ€™s non-informative prior p(Î±s) âˆÎ±âˆ’1
s
can be
obtained. After letting the hyper-parameters {as, bs}S
s=1 in (6.8) go to zero, it is easy
to obtain the following property.

108
6
Bayesian Tensor CPD with Nonnegative Factors
0
20
40
60
80
100
x
10-8
10-6
10-4
10-2
100
102
ln(p(x))
a = 0.1, b = 1
a = 1, b = 1
a = 10-6, b = 10-6
Fig. 6.2 Univariate marginal probability density functions with different parameters (Â© [2020]
IEEE. Reprinted, with permission, from [L. Cheng, X. Tong, S. Wang, Y.-C. Wu, and H. V. Poor,
Learning Nonnegative Factors From Tensor Data: Probabilistic Modeling and Inference Algorithm,
IEEE Transactions on Signal Processing, Jan 2020]. It also applies to Figs.6.3, 6.4, 6.5, 6.6, 6.7,
6.8, 6.9, 6.10 and Tables6.1, 6.2, 6.3, 6.4, 6.5)
Property 6.3. If as â†’0 and bs â†’0, the marginal pdf (6.8) becomes
p+(w) âˆ
S

s=1
2Ms

1
||ws||2
Ms
U

ws â‰¥0MsÃ—1

,
(6.9)
which is highly peaked at zeros.
As an illustration for Properties 6.2 and 6.3, univariate marginal pdfs with dif-
ferent hyper-parameters {as, bs}S
s=1 are plotted in Fig.6.2, from which it is clear that
the nonnegative Gaussian-gamma prior is sparsity-promoting. Further with the con-
jugacy property revealed in Property 6.1, the nonnegative Gaussian-gamma prior is
a good candidate for probabilistic modeling with nonnegative model parameters.

6.2 Probabilistic Modeling for CPD with Nonnegative Factors
109
6.2.2
Probabilistic Modeling of CPD with Nonnegative
Factors
IntheCPDproblemwithnonnegativefactorsin (6.4),thelth columngroup{(n)
:,l }N
n=1,
which consists of the lth column of all the factor matrices, can be treated as a
model block since their outer product contributes a rank-1 tensor. Therefore, with
the principle of nonnegative Gaussian-gamma prior in the previous subsection, the
lth column group {(n)
:,l }N
n=1 can be modeled using (6.7), but with ws replaced by
{(n)
:,l }N
n=1 and Î±s replaced by Î³l. Considering the independence among different
column groups in {(n)}N
n=1, we have
p({(n)}N
n=1|{Î³l}L
l=1)=
N

n=1
L

l=1
N

(n)
:,l |0JnÃ—1, Î³ âˆ’1
l
I L

	 âˆ
0JnÃ—1 N

(n)
:,l |0JnÃ—1, Î³ âˆ’1
l
I L

d(n)
:,l
U

(n)
:,l â‰¥0JnÃ—1

,
(6.10)
p({Î³l}L
l=1|Î»Î³ ) =
L

l=1
gamma(Î³l|c0
l , d0
l ),
(6.11)
where the precision Î³l is modeled as a gamma distribution. From discussions below
Property 6.2, c0
l and d0
l can be chosen to be very small values (e.g., c0
l = d0
l = Ïµ =
10âˆ’6) to approach a non-informative prior of precision parameter Î³l.
On the other hand, the least-squares objective function in the nonnegative tensor
CPD problem (6.4) motivates the use of a Gaussian likelihood [6â€“8]:
p

Y | (1), (2), . . ., (N), Î²

âˆexp

âˆ’Î²
2 âˆ¥Y âˆ’(1), (2), . . ., (N) âˆ¥2
F

,
(6.12)
in which the parameter Î² represents the inverse of noise power. Since there is no
information about it, a gamma distribution p(Î²|Î±Î²) = gamma(Î²|Ïµ, Ïµ) with very
small Ïµ is employed, making p(Î²|Î±Î²) approaches Jeffreyâ€™s non-informative prior.
The Gaussian likelihood function in (6.12) is with an unbounded support over the
real space, and thus it is suitable for applications such as ï¬‚uorescence data analysis
and blind speech separation, in which the observed data Y could be both positive
and negative. On the other hand, if the data Y are all nonnegative and continuous
(e.g., the email dataset [1, 2] after pre-processing), a truncated Gaussian likelihood
can be used to model the data:
p

Y | (1), (2), . . ., (N), Î²

âˆexp

âˆ’Î²
2 âˆ¥Y âˆ’(1), (2), . . ., (N) âˆ¥2
F

U(Y â‰¥0).
(6.13)
Finally, the complete probabilistic model is a three-layer Bayes network and is illus-
trated in Fig.6.3.

110
6
Bayesian Tensor CPD with Nonnegative Factors
Fig. 6.3 Probabilistic model
for tensor CPD with
nonnegative factors
Î²
{Î³l}L
l=1
Î»Î³
Î±Î²
0
Â· Â· Â·
Î(1)
Î(2)
Î(N)
Y
â‰¥0
6.3
Inference Algorithm for Tensor CPD with Nonnegative
Factors
The unknown parameter set  includes the factor matrices {(n)}N
n=1, the noise
power Î²âˆ’1, and the precision parameter {Î³l}L
l=1. Recall from Chaps.2 and 3 that
under mean-ï¬eld assumption Q() = K
k=1 Q(k), the optimal variational pdf is
obtained by solving
min
Q(k)

Q(k)(âˆ’E
jÌ¸=k Q( j)

ln p(, Y)

+ ln Q(k))dk,
(6.14)
and its solution is
Qâˆ—(k) =
exp

E
jÌ¸=k Q( j)

ln p (, Y)

	
exp

E
jÌ¸=k Q( j)

ln p (, Y)

dk
.
(6.15)
Nevertheless, even under the mean-ï¬eld family assumption, the unknown param-
eter (k) is still difï¬cult to be inferred since its moments cannot be easily computed.
In particular, in the proposed probabilistic model, if no functional assumption is
made for variational pdf Q((k)), after using (6.15), a multivariate truncated Gaus-

6.3 Inference Algorithm for Tensor CPD with Nonnegative Factors
111
sian distribution would be obtained, of which the moments are known to be very
difï¬cult to be computed due to the multiple integrations involved [10]. In this case,
the variational pdf Q((k)) could be further restricted to be a Dirac delta function
Q((k)) = Î´((k) âˆ’Ë†
(k)), where Ë†
(k) is the point estimate of the parameter (k).
After substituting this functional form into problem (6.14), the optimal point estimate
Ë†
(k)âˆ—is obtained by
Ë†
(k)âˆ—= arg max E
 j Ì¸=(k) Q( j)

ln p (, Y)

.
(6.16)
This is indeed the framework of variational expectation maximization (EM), in which
the factor matrices {(k)}N
k=1 are treated as global parameters and other variables are
treated as latent variables (see also the discussion around (2.7)).
In (6.15) and (6.16), the log of the joint pdf ln p (, Y) needs to be evaluated. If
the Gaussian likelihood function (6.12) is adopted, it is expressed as
ln pâ€  (, Y) =
N

n=1
ln

U((n) â‰¥0JnÃ—L)

+
N
n=1 Jn
2
ln Î²
âˆ’Î²
2 âˆ¥Y âˆ’(1), (2), . . . , (N)âˆ¥2
F +
N

n=1
Jn
2
L

l=1
ln Î³l
âˆ’
N

n=1
1
2Tr

(n)(n)T 
+
L

l=1

(10âˆ’6 âˆ’1) ln Î³l âˆ’10âˆ’6Î³l

+ (10âˆ’6 âˆ’1) ln Î² âˆ’10âˆ’6Î² + const,
(6.17)
where  = diag{Î³1, Î³2, . . ., Î³L}. On the other hand, if the truncated Gaussian likeli-
hood (6.13) is used, the log of the joint pdf ln pâ™¯(, Y) takes the following form:
ln pâ™¯(, Y) = ln pâ€  (, Y) âˆ’ln 

{(n)}N
n=1, Î²

+ const.
(6.18)
where


{(n)}N
n=1, Î²

=
 âˆ
0
 Î²
2Ï€
N
n=1
Jn
2
exp
âˆ’Î²
2
âˆ¥Y âˆ’(1), (2), . . ., (N) âˆ¥2
F

dY.
(6.19)
In (6.18), the term ln 

{(n)}N
n=1, Î²

, which arises from the truncated Gaussian
likelihood in (6.13), is very difï¬cult to evaluate and differentiate, due to the multi-
ple integrations involved. Fortunately, for most applications in Bayesian nonneg-
ative matrix/tensor decomposition, the conï¬dence of the low-rank matrix/tensor
model is relatively high, in the sense that the noise power 1/Î² is small com-
pared to the average element in signal tensor (1), (2), . . ., (N). Under this
assumption, it is easy to see ln 

{(n)}N
n=1, Î²

â‰ˆln 1 = 0, since Gaussian pdf

112
6
Bayesian Tensor CPD with Nonnegative Factors
-1
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Fig. 6.4 Illustration of a univariate Gaussian probability density function with its mean much larger
than the variance
p(Y) =

Î²
2Ï€


n Jn
2
exp

âˆ’Î² âˆ¥Y âˆ’(1), (2), . . ., (N) âˆ¥2
F

dY
decays
very
rapidly and thus most densities are over the region Y â‰¥0. As an illustration, a
univariate Gaussian pdf with its mean much larger than the variance is plotted in
Fig.6.4, in which the probability density in the negative region is negligible. This
suggests that the log of the joint pdf ln pâ™¯(, Y) in (6.18) can be well approximated
by ln pâ€  (, Y) in (6.17), and therefore, algorithm derivations are uniï¬ed for the two
likelihoods. This also explains why the previous Bayesian nonnegative matrix/tensor
decompositions all employ the Gaussian likelihood function.
6.3.1
Derivation for Variational Pdfs
As discussed at the beginning of this section, the mean-ï¬eld approximation is
employed to enable closed-form expression for each variational pdf. For the pre-
cision parameter Î³l, by substituting (6.17) into (6.15) and only taking the terms
relevant to Î³l, the variational pdf Q(Î³l) can be found to take the same functional
form as that of the gamma distribution, i.e., Q(Î³l) = gamma(Î³l|cl, dl) with

6.3 Inference Algorithm for Tensor CPD with Nonnegative Factors
113
cl =
N

n=1
Jn
2 + Ïµ,
(6.20)
dl =
N

n=1
1
2EQ((n))

(n)T
:,l
(n)
:,l

+ Ïµ.
(6.21)
Since the variational pdf Q(Î³l) is determined by parameters cl and dl, its update is
equivalent to the update of the two parameters in (6.20) and (6.21).
Similarly, using (6.15) and (6.17), the variational pdf Q(Î²) can be found to be a
gamma distribution Q(Î²) = gamma(Î²|e, f ), where
e =
N
n=1 Jn
2
+ Ïµ,
(6.22)
f = 1
2E
 j Ì¸=Î² Q( j)

âˆ¥Y âˆ’(1), (2), . . . , (N)âˆ¥2
F

+ Ïµ.
(6.23)
On the other hand, by substituting (6.17) into (6.16), the point estimate of Ë†
(k)
can be obtained via solving the following problem:
max E
 j Ì¸=(k) Q( j)

ln

U((k) â‰¥0JkÃ—L)

âˆ’Î²
2 âˆ¥Y âˆ’(1), (2), . . . , (N)âˆ¥2
F âˆ’1
2Tr

(k)(k)T 
.
(6.24)
After distributing the expectations, expanding the Frobenius norm, and utilizing the
fact that ln(0) = âˆ’âˆ, problem (6.24) can be equivalently shown to be
min f ((k))
s.t. (k) â‰¥0JkÃ—L,
(6.25)
where
f ((k)) = 1
2Tr

(k)E
 j Ì¸=(k) Q( j)

Î²B(k)B(k)T + 

(k)T
âˆ’2Î²(k)E
 j Ì¸=(k) Q( j)

B(k)
Y(k)T 
.
(6.26)
In (6.26), the term B(k) =

Nâ‹„
n=1,nÌ¸=k (n)T
, with the multiple Khatriâ€“Rao products
Nâ‹„
n=1,nÌ¸=k A(n) = A(N) â‹„A(Nâˆ’1) â‹„Â· Â· Â· â‹„A(k+1) â‹„A(kâˆ’1) â‹„Â· Â· Â· â‹„A(1). It is easy to see
that problem (6.25) is a quadratic programming (QP) problem with nonnegative
constraints. Since each diagonal element Î³l in the diagonal matrix  is larger than
zero, the Hessian matrix of the function f ((k)), with the expression being

114
6
Bayesian Tensor CPD with Nonnegative Factors
H(k) = E
 j Ì¸=(k) Q( j)

Î²B(k)B(k)T + 

(6.27)
is positive deï¬nite. This implies that problem (6.25) is a convex problem, and its
solutions have been investigated for decades. In particular, ï¬rst-order methods have
recently received much attention due to their scalability in big data applications.
Within the class of ï¬rst-order methods, a simple gradient projection method is intro-
duced as follows.
In each iteration of the gradient projection method, the update equation is of the
form:
(k,t+1) =

(k,t) âˆ’Î±tâ–½f ((k,t))

+ ,
(6.28)
where the gradient â–½f ((k,t)) is computed as
â–½f ((k,t)) = (k,t)E
 j Ì¸=(k) Q( j)

Î²B(k)B(k)T + 

âˆ’Y(k)E
 j Ì¸=(k) Q( j)

Î²B(k)T 
.
(6.29)
In (6.29), the symbol [Â·]+ denotes projecting each element of (k) to [0, âˆ) (i.e.,
[x]+ = x if x â‰¥0 and [x]+ = 0 otherwise) and Î±t â‰¥0 is the step size. During
the inference, due to the sparsity-promoting property of the nonnegative Gaussian-
gamma prior, some of the precision parameters will go to very large numbers while
some of them will tend to be zero. This will result in a very large condition number
of the Hessian matrix H(k). In this case, applying the diminishing rule1 to the step
size Î±t still enjoys a good convergence performance [3] and thus is set as the default
step size rule in the algorithm.
6.3.2
Summary of the Inference Algorithm
From Eqs.(6.20)â€“(6.29), it can be seen that we need to compute various expectations.
In particular, for expectations EQ((n))[(n)], EQ(Î³l)[Î³l], and EQ(Î²)[Î²], their com-
putations are very straightforward, i.e., EQ((n))[(n)] = Ë†
(n), EQ(Î³l)[Î³l] = cl/dl,
and EQ(Î²)[Î²] = e/f . However, when updating Ë†
(n) using (6.25) and (6.26), there
is one complicated expectation E
 j Ì¸=(k) Q( j)

B(k)B(k)T 
. Fortunately, it can
be shown that E
 j Ì¸=(k) Q( j)

B(k)B(k)T 
=
NâŠ™
n=1,nÌ¸=k
Ë†
(n)T Ë†
(n), where the multi-
ple Hadamard products
NâŠ™
n=1,nÌ¸=k A(n) = A(N) âŠ™A(Nâˆ’1) âŠ™Â· Â· Â· âŠ™A(k+1) âŠ™A(kâˆ’1) âŠ™
Â· Â· Â· âŠ™A(1). Since the computation of one variational pdf needs the statistics of other
1 In the diminishing rule [3], the step size Î±t needs to satisfy Î±t â†’0 and âˆ
t=0 Î±t = âˆ.

6.3 Inference Algorithm for Tensor CPD with Nonnegative Factors
115
Algorithm 8 Probabilistic Tensor CPD with Nonnegative Factors (PNCPD)
Initializations: Choose L > R and initial values { Ë†
(n,0)}N
n=1, Ïµ.
Iterations: For the sth iteration (s â‰¥0),
Update the parameter of Q((k))(s+1):
Set initial value (k,0) = Ë†
(k,s).
Iterations: For the tth iteration (t â‰¥0), compute
(k,t+1) =

(k,t) âˆ’Î±tâ–½f ((k,t))

+ ,
where
â–½f ((k,t)) = (k,t) es
f s
NâŠ™
n=1,nÌ¸=k
Ë†
(n,s)T Ë†
(n,s) + diag{ cs
1
ds
1
, . . ., cs
L
ds
L
}

âˆ’es
f s Y(k)
Nâ‹„
n=1,nÌ¸=k
Ë†
(n,s)
,
and Î±t is chosen by the diminishing rule [42, p. 227].
Until Convergence
Set Ë†
(k,s+1) = (k,t+1).
Update the parameter of Q(Î³l)s+1:
cs+1
l
=
N

n=1
Jn
2 + Ïµ,
ds+1
l
=
N

n=1
1
2
Ë†
(n,s+1)T
:,l
Ë†
(n,s+1)
:,l
+ Ïµ.
Update the parameter of Q(Î²)s+1:
es+1 = Ïµ +
N
n=1 Jn
2
,
f s+1 = Ïµ + 1
2 âˆ¥Y âˆ’ Ë†
(1,s+1), Ë†
(2,s+1), . . . , Ë†
(N,s+1) âˆ¥2
F .
Until Convergence
variational pdfs, alternating update is needed, resulting in the iterative algorithm
summarized in Algorithm 8.
6.3.3
Discussions and Insights
To gain further insight from the inference algorithm, discussions of its convergence
property, automatic rank determination, relationship to the NALS algorithm, com-
putational complexity, and scalability improvement are presented in the following.

116
6
Bayesian Tensor CPD with Nonnegative Factors
6.3.3.1
Convergence Property
Algorithm 8 is derived under the framework of mean-ï¬eld variational inference,
where a variational pdf Q() = 
k Q(k) is sought that minimizes the KL diver-
gence KL (Q()||p(|Y)). Even though this problem is known to be non-convex
due to the non-convexity of the mean-ï¬eld family set, it is convex with respect to a
single variational pdf Q(k) after ï¬xing other variational pdfs {Q( j), j Ì¸= k} [3].
Therefore, the iterative algorithm, in which a single variational pdf is optimized in
each iteration with other variational pdfs ï¬xed, is essentially a coordinate descent
algorithm in the functional space of variational pdfs. Since the subproblem in each
iteration is not only convex but also has a unique solution, the limit point gener-
ated by the coordinate descent steps over the functional space of variational pdfs is
guaranteed to be at least a stationary point of the KL divergence [3].
6.3.3.2
Automatic Rank Determination
During the inference, the expectations of some precision parameters {Î³l}, i.e., { cs
l
ds
l }
will go to a very large value. It indicates that the corresponding columns in the factor
matrices are close to zero vectors, thus playing no role in data interpretation. As
a result, after convergence, those columns can be pruned out and the number of
remaining columns in each factor matrix gives the estimate of tensor rank.
In practice, to reduce the computational complexity, the pruning would be exe-
cuted during the iteration. In particular, in each iteration, after the precision estimates
{ cs
l
ds
l } exceed a certain threshold (e.g., 106), the associated columns are safely pruned
out.Aftereverypruning,itisequivalenttostartingminimizationoftheKLdivergence
of a new (but smaller) probabilistic model, with the current variational distributions
acting as the initialization of the new minimization. Therefore, the pruning steps will
not affect the convergence and are widely used in recent related works [6â€“8].
Usually, the hyper-parameters {c0
l , d0
l } of the prior gamma distribution are set to
be a very small number Ïµ = 10âˆ’6 to approach a non-informative prior. Otherwise,
their values might affect the behavior of tensor rank estimate. For example, if we
prefer a high value of the tensor rank, the initial value d0
l can be set to be very large
while the initial value c0
l can be set to be small, so that the update of cl
dl can be steered
toward a small value in order to promote a high tensor rank. However, how to set
the hyper-parameters to accurately control the degree of low rank is challenging and
deserves future investigation.
6.3.3.3
Relationship to NALS Algorithm
The mean-ï¬eld variational inference for tensor CPD problem could be interpreted
as alternating optimizations over the Riemannian space (in which the Euclidean
space is a special case). This insight has been revealed in previous works [8, 11],

6.4 Algorithm Accelerations
117
and can also be found in Algorithm 8. For example, for the precision parameters
and the noise power parameter, the variational pdfs are with no constraint on the
functional form, and thus, the corresponding alternating optimization is over the
Riemannian space due to the exponentially conjugacy property of the probabilistic
model [8, 11]. On the other hand, for unknown factor matrices, since the variational
pdfs to be optimized are with a delta functional form, the corresponding alternating
optimization is over the Euclidean space, thus is similar to the conventional NALS
step. However, there is a signiï¬cant difference. In Algorithm 8, there is a shrinkage
term  appeared in the Hessian matrix in (6.27), and  will be updated together with
other parameters in the algorithm. This intricate interaction is due to the employed
Bayesian framework and cannot be revealed by NALS framework. Consequently,
Algorithm 8 is a generalization of the NALS algorithm, with the additional novel
feature in automatic rank determination achieved via optimization in Riemannian
space.
6.3.3.4
Computational Complexity
For each iteration, the computational complexity is dominated by computing the
gradient of each factor matrix in (6.28), costing O(N
n=1 JnL). From this expression,
it is clear that the computational complexity in each iteration is linear with respect to
thetensordimensionproductN
n=1 Jn.Consequently,thecomplexityofthealgorithm
is O(q(N
n=1 JnL)) where q is the iteration number at convergence.
6.4
Algorithm Accelerations
From Algorithm 8, it is clear that the bottleneck of the algorithm computation is
the update of factor matrices {(n)}N
n=1 via solving problem (6.25). In particular, for
each subproblem for solving (k), projected gradient may take a large number of
iterations to converge. Fortunately, if the problem is well conditioned, in the sense that
the condition number of the Hessian matrix H(k) in (6.27) is smaller than a threshold
(e.g., 100), acceleration schemes, including variants of the Nesterov scheme [4],
can be utilized to signiï¬cantly reduce the required number of iterations for solving
problem (6.25), thus speeding up Algorithm 8.
On the other hand, since (k) for k = 1, 2, . . . , n are updated alternatively under
the framework of block coordinate descent (BCD), besides reducing the iteration
number for solving each (k), we can also borrow the idea of inexact BCD, where
each subproblem for (k) is not exactly solved, thus avoiding the large number of
iterations required for convergence. In general, as long as the solution of each (k)
leads to a reduction of the overall optimization objective function, inexact BCD
reduces the computation times signiï¬cantly while maintaining the solution quality.
Below, we will give the details on how to connect Algorithm 8 to inexact BCD.
More speciï¬cally, Algorithm 8 can be viewed as solving the following determin-
istic optimization problem:

118
6
Bayesian Tensor CPD with Nonnegative Factors
min
{(n)â‰¥0}N
n=1,{Î³l}L
l=1,Î² g(),
(6.30)
where
g() = âˆ’
N
n=1 Jn
2
ln Î² + Î²
2 âˆ¥Y âˆ’(1), (2), . . . , (N)âˆ¥2
F âˆ’
N

n=1
Jn
2
L

l=1
ln Î³l
+
N

n=1
1
2Tr

(n)(n)T 
âˆ’
L

l=1

c0
l ln Î³l âˆ’d0
l Î³l

âˆ’e0 ln Î² + f 0Î², (6.31)
with  = diag{Î³1, . . ., Î³L}. To see the connection between solving (6.30) and
Algorithm 8, the BCD method could be employed. That is, in each iteration, after
ï¬xing other unknown parameters { j} jÌ¸=k at their last updated values, k is updated
as follows:
t+1
k
= arg min
k g(t+1
1
, . . ., t+1
kâˆ’1, k, t
k+1, . . ., t
K).
(6.32)
This connection is formally stated in Proposition 6.1.
Proposition 6.1. Assume each initial value of the unknown parameter 0
k in
(6.32) equal to the expectation of k with respect to the initial variational
pdf Q0(k) in Algorithm 1. With the same update schedule for various
parameter blocks, in each iteration, the result of the block minimization
update (6.32) for parameter k equals to the expectation of k with respect
to the variational pdf Q(k) from Algorithm 8, i.e., t
k = EQt(k)[k].
Therefore, Algorithm 8 is indeed related to parameter block minimization of the
optimization problem (6.30). This new interpretation opens up the possibility of using
inexact BCD to accelerate Algorithm 8 and its detailed derivations are presented as
follows.
For updating noise precision parameter Î² in the iteration t + 1(t â‰¥0), after ï¬xing
other parameters in problem (6.30), the subproblem is expressed as
min
Î² ht+1(Î²),
(6.33)
where
ht+1(Î²) = âˆ’
N
n=1 Jn
2
+ e0

ln Î²
+ Î²
1
2 âˆ¥Y âˆ’(1),t+1, (2),t+1, . . . , (N),t+1 âˆ¥2
F + f 0

.
(6.34)

6.4 Algorithm Accelerations
119
However, the objective function ht+1(Î²) is not strongly convex since the second-
order derivative â–½2
Î²ht+1(Î²) =
 N
n=1 Jn
2
+ e0
1
Î²2 can be arbitrarily close to zero as
Î² â†’âˆ. Consequently, the block minimization scheme cannot be used. To guarantee
convergence, as suggested in [3, 12â€“15], a proximal term Î¼Î²
2 (Î² âˆ’Î²t)2 is added to
the objective function in (6.33), giving the following optimization problem:
min
Î² ht+1(Î²) + Î¼Î²
2 (Î² âˆ’Î²t)2,
(6.35)
with parameter 0 < Î¼Î² < âˆ. After setting the derivative of the objective function
of (6.35) to zero, it can be shown that the optimal solution takes a closed form:
Î²t+1 =
âˆ’

f t+1 âˆ’Î¼Î²Î²t
+

f t+1 âˆ’Î¼Î²Î²t2 + 4Î¼Î²et+1
2Î¼Î²
,
(6.36)
in which
et+1 =
N
n=1 Jn
2
+ e0,
(6.37)
f t+1 = 1
2 âˆ¥Y âˆ’(1),t+1, (2),t+1, . . . , (N),t+1 âˆ¥2
F + f 0.
(6.38)
Similarly, for updating parameter Î³l, the subproblem is
min
Î³l ht+1(Î³l),
(6.39)
where
ht+1(Î³l) = âˆ’
 N

n=1
Jn
2 + c0
l

ln Î³l + Î³l

d0
l +
N

n=1
1
2

(n),t+1
:,l
T
(n),t+1
:,l

(6.40)
is not strongly convex. Therefore, a proximal term
Î¼Î³l
2 (Î³l âˆ’Î³ t
l )2 with 0 < Î¼Î³l < âˆ
is added to the objective function in (6.39) as follows [3, 12â€“15]:
min
Î³l ht+1(Î³l) + Î¼Î³l
2 (Î³l âˆ’Î³ t
l )2,
(6.41)
of which the optimal solution can be shown to be
Î³ t+1
l
=
âˆ’

dt+1
l
âˆ’Î¼Î³lÎ³ t
l

+

dt+1
l
âˆ’Î¼Î³lÎ³ t
l
2 + 4Î¼Î³lct+1
l
2Î¼Î³l
,
(6.42)

120
6
Bayesian Tensor CPD with Nonnegative Factors
where parameters ct+1
l
and dt+1
l
are
ct+1
l
=
N

n=1
Jn
2 + c0
l ,
(6.43)
dt+1
l
=
N

n=1
1
2

(n),t+1
:,l
T
(n),t+1
:,l
+ d0
l .
(6.44)
For updating each nonnegative factor in the iteration t + 1(t â‰¥0), after ï¬xing
other parameters, the subproblem can be formulated as:
min
(n)â‰¥0 ht+1((n)),
(6.45)
where
ht+1((n)) = Î²t
2 âˆ¥Y âˆ’(1),t+1, . . . , (n), . . . , (N),tâˆ¥2
F +1
2Tr

(n)t(n)T 
.
(6.46)
After expanding the Frobenius norm and some algebraic operations, problem (6.45)
can be equivalently formulated as:
min
(n)â‰¥0 ct+1((n)),
(6.47)
where
ct+1((n))= 1
2Tr

(n) 
Î²t 
B(n),tT B(n),t + t
(n)T âˆ’2Î²t(n) 
B(n),tT Y(n)

.
(6.48)
In (6.48), matrix B(n),t = (N),t â‹„Â· Â· Â· â‹„(n+1),t â‹„(nâˆ’1),t+1 â‹„Â· Â· Â· â‹„(1),t+1, where
â‹„denotes the Khatriâ€“Rao product (see Deï¬nition 1.3), and Y(k) is a matrix obtained
by unfolding the tensor Y along its kth dimension (see Deï¬nition 1.1).
In the inexact BCD framework, it is not necessary to obtain the optimal solution of
(6.47). Instead, we can construct a prox-linear update step with careful extrapolations
and monotonicity-guaranteed modiï¬cations. In particular, in the iteration t + 1, the
extrapolation parameter wt
n is computed by [12, 14â€“16]:
wt
n = min
â›
âË†wt
n, pw

Ltâˆ’1
n
Ltn
â
â ,
(6.49)

6.4 Algorithm Accelerations
121
where pw < 1 is preselected, Ë†wt
n = stâˆ’1
st+1 with s0 = 1, st+1 = 1
2(1 +
 
1 + 4s2
t ), and
Lt
n is assigned to be the spectral norm of the Hessian matrix of ct+1((n)), i.e.,
Lt
n = ||Î²t 
B(n),tT B(n),t + t||2.
(6.50)
Using (6.49), the extrapolated factor matrix Ë†M
(n),t is with the expression:
Ë†M
(n),t = (n),t + wt
n((n),t âˆ’(n),tâˆ’1).
(6.51)
Based on (6.51), the prox-linear update can be expressed as [12]:
(n),t+1 = arg min
(n)â‰¥0
!
â–½ct+1( Ë†M
(n),t), (n) âˆ’Ë†M
(n),t"
+ Lt
n
2 ||(n) âˆ’Ë†M
(n),t||2
F,
(6.52)
where the gradient is
â–½ct+1((n)) = (n) 
Î²t 
B(n),tT B(n),t + t
âˆ’Î²tY(n)T B(n),t,
(6.53)
and âŸ¨Â·, Â·âŸ©denotes the inner product of the arguments. It can be shown that the solution
of (6.52) takes the following closed form:
(n),t+1 =

Ë†M
(n),t âˆ’1
Ltn
â–½ct+1( Ë†M
(n),t)

+
,
(6.54)
from which it can be seen that the prox-linear update only needs one-step computation
and thus can be computed very fast.
Besides the extrapolation, monotonicity-guaranteed modiï¬cation is needed to
ensure the convergence [12]. More speciï¬cally, after updating all the parameters in
, whether the objective function of problem (6.30) is decreased (i.e., g(t+1) â‰¤
g(t)) is tested. If not, prox-linear update (6.54) for each factor matrix (n),t+1 will
be re-executed without the extrapolation, i.e.,
(n),t+1 =

(n),t âˆ’1
Ltn
â–½ct+1((n),t)

+
.
(6.55)
Using (6.36), (6.42), (6.54), and (6.55), the inexact BCD-based algorithm for
probabilistic tensor CPD with nonnegative factors is summarized in Algorithm 9.

122
6
Bayesian Tensor CPD with Nonnegative Factors
Algorithm 9 Inexact BCD-Based Probabilistic Tensor CPD with Nonnegative
Factors
Initializations: Choose L > R, pw < 1, {Î¼Î³l }L
l=1, Î¼Î² and initial values {(n),0, (n),âˆ’1}N
n=1,
{c0
l , d0
l }L
l=1, e0, f 0.
Iterations: For the iteration t + 1 (t â‰¥0),
For n = 1 to N
Update factor matrix (n),t+1:
Compute extrapolation parameter wt
n using (6.49).
Compute extrapolated factor matrix:
Ë†M
(n),t = (n),t + wt
n((n),t âˆ’(n),tâˆ’1).
Update factor matrix:
(n),t+1 =

Ë†M
(n),t âˆ’1
Ltn
â–½ct+1( Ë†M
(n),t)

+
,
where â–½ct+1( Ë†M
(n),t) is computed using (6.53), and Lt
n is computed using (6.50).
End
Update parameter Î³ t+1
l
:
Î³ t+1
l
=
âˆ’

dt+1
l
âˆ’Î¼Î³l Î³ t
l

+
#
dt+1
l
âˆ’Î¼Î³l Î³ t
l
2
+ 4Î¼Î³l ct+1
l
2Î¼Î³l
,
where ct+1
l
and dt+1
l
are computed via (6.43) and (6.44), respectively.
Update parameter Î²t+1:
Î²t+1 =
âˆ’

f t+1 âˆ’Î¼Î²Î²t
+

f t+1 âˆ’Î¼Î²Î²t2 + 4Î¼Î²et+1
2Î¼Î²
,
where et+1 and f t+1 are computed via (6.37) and (6.38), respectively.
Monotonicity check:
Let t+1 = {{(n),t+1}N
n=1, {Î³ t+1
l
}L
l=1, Î²t+1}.
If g(t+1) > g(t):
(n),t+1 =

(n),t âˆ’1
Ltn
â–½ct+1((n),t)

+
.
Until Convergence
6.5
Numerical Results
In this section, numerical results using synthetic data are ï¬rstly presented to
assess the performance of Algorithm 8 in terms of convergence property, fac-
tor matrix recovery, tensor rank estimation, and running time. Next, Algorithm 8
is utilized to analyze two real-world datasets (the amino acids ï¬‚uorescence data

6.5 Numerical Results
123
and the ENRON email corpus), for the demonstration on blind source separa-
tion and social group clustering. For all the simulated algorithms, the initial fac-
tor matrix Ë†
(k,0) is set as the singular value decomposition (SVD) approxima-
tion U:,1:L

S1:L,1:L
 1
2 where [U, S, V] = SVD[Y(k)] and L = min{J1, J2, . . ., JN}.
The parameter Ïµ is set to be 10âˆ’6. The algorithms are deemed to be converged
when âˆ¥ Ë†
(1,s+1), Ë†
(2,s+1), . . . , Ë†
(N,s+1) âˆ’ Ë†
(1,s), Ë†
(2,s), . . . , Ë†
(N,s) âˆ¥2
F< 10âˆ’6.
All experiments were conducted in Matlab R2015b with an Intel Core i7 CPU at 2.2
GHz.
6.5.1
Validation on Synthetic Data
A 3D tensor X = M(1), M(2), M(3) âˆˆR100Ã—100Ã—100 with rank R = 10 is consid-
ered as the noise-free data tensor. Each element in factor matrix M(n) is inde-
pendently drawn from a uniform distribution over [0, 1] and thus is nonnegative.
On the other hand, two observation data tensors are considered: (1) the data X
is corrupted by a noise tensor W âˆˆR100Ã—100Ã—100, i.e., Y = X + W, with each
element of noise tensor W being independently drawn from a zero-mean Gaus-
sian distribution with variance Ïƒ 2
w, and this corresponds to the Gaussian like-
lihood model (6.12); (2) the data Y+ is obtained by setting the negative ele-
ments of Y to zero, i.e., Y+
i1,i2,i3 = Yi1,i2,i3U(Yi1,i2,i3 â‰¥0), and the truncated Gaus-
sian likelihood model (6.13) is employed to ï¬t these data. The SNR is deï¬ned
as
10 log10

âˆ¥X âˆ¥2
F /Ep(W)

âˆ¥W âˆ¥2
F

= 10 log10

âˆ¥X âˆ¥2
F /(1003Ïƒ 2
w)

.
For
Algorithm 8, the step size sequence is chosen as Î±t = 10âˆ’3/(t + 1) [3], and the
gradient projection update is terminated when || f ((k,t)) âˆ’f ((k,tâˆ’1))||F â‰¤10âˆ’3.
Each result in this subsection is obtained by averaging 100 Monte Carlo runs.
Figure6.5 presents the convergence performances of Algorithm 8 under different
SNRs and different test data, where the mean square error (MSE) âˆ¥ Ë†
(1,s), Ë†
(2,s),
Ë†
(3,s) âˆ’X âˆ¥2
F is chosen as the assessment criterion. From Fig.6.5a, it can be seen
that for test data Y, the MSEs of Algorithm 8, which assumes no knowledge of
tensor rank, decrease signiï¬cantly in the ï¬rst few iterations and converge to the
MSE of the NALS algorithm [17] (with exact tensor rank) under SNR = 10 dB and
SNR = 20 dB. Similar convergence performances can be observed for the test data
Y+ in Fig.6.5b. This is of no surprise because approximating (6.18) by (6.17) does
not make any changes in the algorithm framework of variational inference, and thus,
the excellent convergence performance of Algorithm 8 is expected.
The MSE in Fig.6.5 measures the performance of low-rank tensor recovery as
a whole but does not indicate the accuracy at factor matrices level. On the other
hand, due to the uniqueness property of tensor CPD [18], each factor matrix can be
recovered up to an unknown permutation and scaling ambiguity. To assess the accu-
racies of factor matrices recovery, the best congruence ratio (BCR), which involves

124
6
Bayesian Tensor CPD with Nonnegative Factors
10
20
30
40
50
60
70
80
90
100
Iteration number
102
104
106
108
1010
MSE
Data Y
SNR = 10 dB
SNR = 20 dB
Benchmark:
NALS with correct
tensor rank
(a)
10
20
30
40
50
60
70
80
90
100
Iteration number 
102
104
106
108
1010
MSE
Data Y +
SNR = 10 dB
SNR = 20 dB
Benchmark:
NALS with correct
tensor rank
(b)
Fig. 6.5 Convergence of the proposed algorithm for different test data
computing the MSE between the true factor matrix M(k) and the estimated factor
matrix Ë†
(k), is widely used as the assessment criterion. It is deï¬ned as
3

k=1
min
(k),P(k)
||M(k) âˆ’Ë†
(k) P(k)(k)||F
||M(k)||F
,
where the diagonal matrix (k) and the permutation matrix P(k) are found via the
greedy least-squares column matching algorithm [19]. From Fig.6.6, it is seen that
both Algorithm 8 (labeled as PNCPD) and the NALS algorithm (with exact ten-
sor rank) achieve much better factor matrix recovery than the ALS algorithm (with
exact tensor rank) [18]. This shows the importance of incorporating the nonnegative
constraint into the algorithm design. Furthermore, the factor matrix recovery per-
formances of Algorithm 8 under test data Y and Y+ are indistinguishable under
SNR = 20 dB. This shows that when SNR is high, Eq.(6.17) gives a quite good
approximation to Eq.(6.18), thus leading to remarkably accurate factor matrices
recovery. Although the BCR of Algorithm 8 is higher for the data Y+ than that for
the data Y, it is with nearly the same performance as that of the NALS algorithm
(with exact tensor rank).
On the other hand, the tensor rank estimates of Algorithm 8 under different SNRs
are presented in Fig.6.7, with each vertical bar showing the mean and the error bars
showing the standard derivation of tensor rank estimates. The blue horizontal dashed
line indicates the true tensor rank. From Fig.6.7, it is seen that Algorithm 8 recovers
the true tensor rank with 100% accuracy for a wide range of SNRs, in particular when
SNR is 10 dB or higher. When SNR is 0 and 5dB, even though the performance is
not 100% accurate, the estimated tensor rank is still close to the true tensor rank
with a high probability for test data Y. However, under these two low SNRs, the
rank estimation performances of the proposed algorithm for the data Y+ degrade
signiï¬cantly. This is because Eq.(6.18) cannot be well approximated by Eq.(6.17)

6.5 Numerical Results
125
10
20
10
20
SNR (dB)
0
0.5
1
1.5
2
2.5
3
3.5
4
Best Congruence Ratio
ALS with correct tensor rank
NNALS with correct tensor rank
PNCPD
Data Y
Data Y+
Fig. 6.6 Best congruence ratios of the proposed algorithm for different test data
-5
0
5
10
15
20
SNR (dB)
0
5
10
15
20
25
30
35
40
Tensor Rank Estimate
Data Y
Data Y+
Fig. 6.7 Tensor rank estimates of the proposed algorithm for different test data

126
6
Bayesian Tensor CPD with Nonnegative Factors
Table 6.1 Performance of tensor rank estimation versus different true tensor ranks for tensor data
Y when SNR = 20 dB
True tensor rank
Mean of tensor rank
estimates
Standard derivation of
tensor rank estimates
Percentage of correct
tensor rank estimates
(%)
10
10
0
100
30
29.6
1.27
90
50
28.15
18.29
25
under very low SNRs. Furthermore, Algorithm 8 fails to give correct rank estimates
when SNR is lower than âˆ’5 dB for both two test data sets, since the noise with very
large power masks the low-rank structure of the data.
To assess the tensor rank estimation accuracy when the tensor is with a larger true
rank, we apply Algorithm 8 to the tensor data Y with the true rank R = {10, 30, 50}
and SNR = 20 dB. The rank estimation performance is presented in Table6.1. From
Table6.1, it can be seen that Algorithm 8 recovers the rank accurately when the true
rank is 10 and 30. However, when R = 50, Algorithm 8 fails to accurately recover
the tensor rank.
The simulation results presented so far are for well-conditioned tensors, i.e., the
columns in each of the factor matrices are independently generated. In order to
fully assess the rank learning ability of Algorithm 8, we consider another noise-
free 3D tensor Â¯X =  Â¯M
(1), M(2), M(3) âˆˆR100Ã—100Ã—100 with rank R = 10. The factor
matrix is set as Â¯M
(1) = 0.11100Ã—10 + 2âˆ’s M(1), and each element in factor matrices
{M(n)}3
n=1 is independently drawn from a uniform distribution over [0, 1]. Accord-
ing to the deï¬nition of the tensor condition number [20, 21], when s increases, the
correlation among the columns in the factor matrix Â¯M
(1) increases, and the tensor
condition number becomes larger. In particular, when s goes to inï¬nity, the condi-
tion number of the tensor goes to inï¬nity too. We apply the proposed algorithm to Â¯X
corrupted with noise: Â¯Y = Â¯X + Â¯W, where each element of noise tensor Â¯W is inde-
pendently drawn from a zero-mean Gaussian distribution with variance Â¯Ïƒ 2
w. Table6.2
shows the rank estimation accuracy of the proposed algorithm when SNR = 20 dB.
It can be seen that the proposed algorithm can correctly estimate the tensor rank
when s < 5. But as the tensor conditional number increases (i.e., the columns are
more correlated in the factor matrix Â¯M
(1)), the tensor rank estimation performance
decreases signiï¬cantly.
Next, we consider an extreme case in which the columns in all factor matrices
are highly correlated: ËœX =  Â¯M
(1), Â¯M
(2), Â¯M
(3) âˆˆR100Ã—100Ã—100 with rank R = 10,
where each factor matrix Â¯M
(n) = 0.11100Ã—10 + 2âˆ’s M(n), and each element in factor
matrices {M(n)}3
n=1 is independently drawn from a uniform distribution over [0, 1].
With the same observation data model as Â¯Y as before and when SNR = 20 dB, the
percentages of the correct tensor rank estimate are shown in Table6.2. It can be seen

6.5 Numerical Results
127
Table 6.2 Performance of tensor rank estimation when the columns are correlated and SNR =
20 dB
s
0
1
3
5
100
Columns in one factor matrix are correlated (%)
100
100
100
25
5
Columns in all factor matrices are correlated (%)
100
40
0
0
0
that it is difï¬cult for Algorithm 8 to accurately estimate the tensor rank even when
s = 1.
Finally, acceleration schemes could be incorporated to speed up Algorithm 8. In
particular, in the ï¬rst few iterations, since some precision parameters are learned to be
very large while some of them are with very small values, the average condition num-
ber of the Hessian matrix of problem (6.25), i.e., 1
3
3
k=1 condition_number

H(k)
,
is very large. After several iterations, Algorithm 8 has gradually recovered the tensor
rank, and then the remaining precision parameters are with comparable values, lead-
ing to a well-conditioned Hessian matrix H(k) of problem (6.25). This phenomenon
can be observed in Fig.6.8. When the Hessian matrix is well-conditioned (e.g., when
the condition number is smaller than 100), the Nesterov scheme can be utilized for the
problem (6.25) to speed up the convergence [4]. Consequently, even with the similar
MSE and BCR performances, the accelerated algorithm (denoted as PNCPD-A) is
much faster than Algorithm 82 as presented in Table6.3. Moreover, from Table6.3,
the inexact BCD acceleration PNCPD-I (Algorithm 9) also achieves similar MSE
and BCR, but with computation time further reduced.
6.5.2
Fluorescence Data Analysis
In this subsection, the application of Fluorescence Data Analysis from Chap.4 is
revisited. Since factor matrices A, B, and C in this application represent excita-
tion values, emission values, and concentrations, respectively, strictly speaking, they
should be nonnegative. Algorithm 8 (PNCPD algorithm) is utilized to analyze the
amino acids ï¬‚uorescence data3 [22] as in Sect.4.2.1. The ï¬‚uorescence excitation-
emission measured (EEM) data collected is with size 5 Ã— 201 Ã— 61 and should be
representable with a CPD model with rank 3, since there are three different types of
amino acid and each individual amino acid gives a rank-one CPD component. The
samples were corrupted by Gaussian noise with SNR = 0 dB.
The PNCPD algorithm (Algorithm 8) is run to decompose the EEM tensor
data with initial rank L = 201, for which, the step size sequence is chosen as
Î±t = 10âˆ’2/(t + 1) [3], and the gradient projection update is terminated when the
2 The presented time for the accelerated scheme includes the time for computing the condition
numbers.
3 http://www.models.life.ku.dk.

128
6
Bayesian Tensor CPD with Nonnegative Factors
10
20
30
40
50
60
70
80
Iteration number 
10-1
100
101
102
103
104
105
the evolution of
tensor rank estimates
the evolution of 
average condition number
Fig. 6.8 The average condition number of the Hessian matrix of problem (6.25) and the tensor
rank estimate versus number of iterations for test data Y when SNR = 20 dB
Table 6.3 Comparisions between different algorithm accelerations. The accelerated version using
the Nesterov scheme is labeled as PNCPD-A. The inexact BCD version is labeled as PNCPD-I
SNR = 10 dB
Y
Y+
PNCPD
PNCPD-A
PNCPD-I
PNCPD
PNCPD-A
PNCPD-I
MSE
545.5608
545.7800
545.8255
576.7381
576.7603
576.7990
BCR
0.4798
0.4886
0.4892
0.7616
0.7673
0.7680
Running
time
134.9246
74.8712
74.0267
119.2018
94.1182
84.7767
SNR = 20 dB
Y
Y+
PNCPD
PNCPD-A
PNCPD-I
PNCPD
PNCPD-A
PNCPD-I
MSE
54.8459
54.8485
54.8448
55.7471
55.7484
56.3062
BCR
0.2398
0.2413
0.2412
0.2438
0.2450
0.2453
Running
time
80.0258
61.9218
44.3579
90.7224
82.7590
47.7782

6.5 Numerical Results
129
-0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
PNCPD
PCPD-GH
Clean
(a)
0
10
20
30
40
50
60
0
50
100
150
200
-0.05
0
0.05
0.1
0.15
0.2
0.25
PNCPD
PCPD-GH
Clean
(b)
Fig. 6.9 The estimates of a excitation spectra and b emission spectra recovered by PNCPD and
PCPD-GH algorithms, with the clean data spectra serving as the benchmark
gradient norm is smaller than 10âˆ’3. At convergence, the PNCPD algorithm iden-
tiï¬ed the correct tensor rank R = 3 and the corresponding Fit value is 91.5582,
which is higher than that of PCPD-GH (Algorithm 4) without nonnegative con-
straint (90.8433 in Table4.3). Furthermore, the emission spectra and the excited
spectra of three amino acids recovered by PNCPD and PCPD-GH algorithms are
plotted in Fig.6.9, which are obtained from the decomposed factor matrices [22],
with the clean data spectra4 serving as the benchmark. From Fig.6.9, it can be seen
that some excitation/emission values recovered by PCPD-GH are negative under the
noise corruption, while that of PNCPD are strictly nonnegative, which explains why
PNCPD yields a higher Fit value and more interpretable results.
6.5.3
ENRON E-mail Data Mining
In this subsection, Algorithm 8 is demonstrated in the social group clustering (see
Sect.6.1.1). In particular, the ENRON Email corpus5 [1] was analyzed. This dataset
Y is with size 184 Ã— 184 Ã— 44 (i.e., I = 184, J = 184, K = 44 in the notation of
Sect.6.1.1) and contains e-mail communication records between 184 people within
44 months, in which each entry denotes the number of e-mails exchanged between
two particular people within a particular month. Before ï¬tting the data to the proposed
algorithms, the same pre-processing as in [1, 2] is applied to the non-zero data to
compress the dynamic range. Then, Algorithm 8 is utilized to ï¬t the data, with the
initial rank set as L = 44, the step size sequence being Î±t = 1/(t + 1) [3], and the
4 The clean data spectra are obtained by decomposing the clean data using the NALS algorithm
with correct tensor rank R = 3.
5 The original source of the data is from [1], and we greatly appreciate Prof. Vagelis Papalexakis
for sharing the data with us.

130
6
Bayesian Tensor CPD with Nonnegative Factors
Table 6.4 Social groups with people in the top 10 scores in each group for the ENRON e-mail data
using the PNCPD algorithm
Legal
Pipeline
Tana Jones (tana.jones) Employee Financial
Trading Group ENA Legal
Sara Shackleton (sara.shackleton) Employee
ENA Legal
Mark Taylor (mark.taylor) Manager Financial
Trading Group ENA Legal
Stephanie Panus (stephanie.panus) Senior
Legal Specialist ENA Legal
Marie Heard (marie.heard) Senior Legal
Specialist ENA Legal
Mark Haedicke (mark.haedicke) Managing
Director ENA Legal
Susan Bailey (susan.bailey) Legal Assistant
ENA Legal
Louise Kitchen (louise.kitchen) President
Enron Online
Kay Mann (kay.mann) Lawyer
Debra Perlingiere (debra.perlingiere) Legal
Specialist ENA Legal
Michelle Lokay (michelle.lokay) Admin. Asst.
Transwestern Pipeline Company (ETS)
Kimberly Watson (kimberly.watson) Employee
Transwestern Pipeline Company (ETS)
Lynn Blair (lynn.blair) Employee Northern
Natural Gas Pipeline (ETS)
Shelley Corman (shelley.corman) VP
Regulatory Affairs
Drew Fossum (drew.fossum) VP Transwestern
Pipeline Company (ETS)
Lindy Donoho (lindy.donoho) Employee
Transwestern Pipeline Company (ETS)
Kevin Hyatt (kevin.hyatt) Director Asset
Development TW Pipeline Business (ETS)
Darrell Schoolcraft (darrell.schoolcraft)
Employee Gas Control (ETS)
Rod Hayslett (rod.hayslett) VP Also CFO and
Treasurer
Susan Scott (susan.scott) Employee
Transwestern Pipeline Company (ETS)
Trading/executive
Government affairs
Michael Grigsby (mike.grigsby) Director West
Desk Gas Trading
Kevin Presto (m..presto) VP East Power
Trading
Mike McConnell (mike.mcconnell) Executive
VP Global Markets
John Arnold (john.arnold) VP Financial Enron
Online
Louise Kitchen (louise.kitchen) President
Enron Online
David Delainey (david.delainey) CEO ENA
and Enron Energy Services
John Lavorato (john.lavorato) CEO Enron
America
Sally Beck (sally.beck) COO
Joannie Williamson (joannie.williamson)
Executive Assistant
Liz Taylor (liz.taylor) Executive Assistant to
Greg Whalley
Jeff Dasovich (jeff.dasovich) Employee
Government Relationship Executive
James Steffes (james.steffes) VP Government
Affairs
Steven Kean (steven.kean) VP Chief of Staff
Richard Shapiro (richard.shapiro) VP
Regulatory Affairs
David Delainey (david.delainey) CEO ENA
and Enron Energy Services
Richard Sanders (richard.sanders) VP Enron
Wholesale Services
Shelley Corman (shelley.corman) VP
Regulatory Affairs
Margaret Carson (margaret.carson) Employee
Corporate and Environmental Policy
Mark Haedicke (mark.haedicke) Managing
Director ENA Legal
Vince Kaminski (vince.kaminski) Manager
Risk Management Head

6.5 Numerical Results
131
Table 6.5 Social groups with people in the top 10 scores in each group for the ENRON e-mail data
using the PCPD-GH algorithm
Legal
Pipeline
Tana Jones (tana.jones) Employee Financial Trading
Group ENA Legal
Sara Shackleton (sara.shackleton) Employee ENA Legal
Mark Taylor (mark.taylor) Manager Financial Trading
Group ENA Legal
Stephanie Panus (stephanie.panus) Senior Legal
Specialist ENA Legal
Marie Heard (marie.heard) Senior Legal Specialist ENA
Legal
Mark Haedicke (mark.haedicke) Managing Director ENA
Legal
Susan Bailey (susan.bailey) Legal Assistant ENA Legal
Louise Kitchen (louise.kitchen) President Enron Online
Kay Mann (kay.mann) Lawyer
Debra Perlingiere (debra.perlingiere) Legal Specialist
ENA Legal
Drew Fossum (drew.fossum) VP Transwestern Pipeline
Company (ETS)
Susan Scott (susan.scott) Employee Transwestern
Pipeline Company (ETS)
Shelley Corman (shelley.corman) VP Regulatory Affairs
Michelle Lokay (michelle.lokay) Admin. Asst.
Transwestern Pipeline Company (ETS)
Lindy Donoho (lindy.donoho) Employee Transwestern
Pipeline Company (ETS)
Jeff Dasovich (jeff.dasovich) Employee Government
Relationship Executive
Kevin Hyatt (kevin.hyatt) Director Asset Development
TW Pipeline Business (ETS)
Kimberly Watson (kimberly.watson) Employee
Transwestern Pipeline Company (ETS)
Lynn Blair (lynn.blair) Employee Northern Natural Gas
Pipeline (ETS)
Rod Hayslett (rod.hayslett) VP Also CFO and Treasurer
Executives
Goverment affairs
John Lavorato (john.lavorato) CEO Enron America
Louise Kitchen (louise.kitchen) President Enron Online
Sally Beck (sally.beck) COO
Liz Taylor (liz.taylor) Executive Assistant to Greg
Whalley
David Delainey (david.delainey) CEO ENA and Enron
Energy Services
Vince Kaminski (vince.kaminski) Manager Risk
Management Head
John Arnold (john.arnold) VP Financial Enron Online
Joannie Williamson (joannie.williamson) Executive
Assistant
Mike McConnell (mike.mcconnell) Executive VP*
Global Markets
Jeffrey Shankman (jeffrey.shankman) President Enron
Global Markets
Jeff Dasovich (jeff.dasovich) Employee Government
Relationship Executive
James Steffes (james.steffes) VP Government Affairs
Steven Kean (steven.kean) VP Chief of Staff
Richard Shapiro (richard.shapiro) VP Regulatory Affairs
Richard Sanders (richard.sanders) VP Enron Wholesale
Services
David Delainey (david.delainey) CEO ENA and Enron
Energy Services
Shelley Corman (shelley.corman) VP Regulatory Affairs
Margaret Carson (margaret.carson) Employee Corporate
and Environmental Policy
Mark Haedicke (mark.haedicke) Managing Director ENA
Legal
Philip Allen (phillip.allen) VP West Desk Gas Trading
Trading
Michael Grigsby (mike.grigsby) Director West Desk Gas
Trading
Matthew Lenhart (matthew.lenhart) Analyst West Desk
Gas Trading
Monique Sanchez (monique.sanchez) Associate West
Desk Gas Trader (EWS)
Jane Tholt (m..tholt) VP West Desk Gas Trading
Philip Allen (phillip.allen) VP West Desk Gas Trading
Kam Keiser (kam.keiser) Employee Gas
Mark Whitt (mark.whitt) Director Marketing
Eric Bass (eric.bass) Trader Texas Desk Gas Trading
Jeff Dasovich (jeff.dasovich) Employee Government
Relationship Executive
Chris Dorland (chris.dorland) Manager

132
6
Bayesian Tensor CPD with Nonnegative Factors
5
10
15
20
25
30
35
40
45
50
Months
0
0.05
0.1
0.15
Temporal cluster profiles
Trading / Top Executive
Pipline
Change of CEO
Crisis Breaks / Investigations
Bankruptcy
Fig. 6.10 Temporal cluster proï¬les (from the third factor matrix) for the ENRON e-mail dataset
gradient projection update terminated when the gradient norm is smaller than 10âˆ’3.
The estimated tensor rank has the physical meaning of the number of underlying
social groups, and each element in the ï¬rst factor matrix A can be interpreted as the
score that a particular person belongs to a particular email sending group.
After executing PNCPD (Algorithm 8), the tensor rank estimate is found to be
4, indicating that there are four underlying social groups. This is consistent with
the results from [1, 2], which are obtained via trail-and-error experiments. From the
factor matrix A, the people with top 10 scores in each social group are shown in
Table6.4. From the information of each person in Table6.4, the clustering results
can be clearly interpreted. For instance, the people in the ï¬rst group are either in
the legal department or lawyers, thus being clustered together. The clustering results
of PNCPD are also consistent with the results from [1, 2], which are obtained via
nonlinear programming methods assuming the knowledge of tensor rank. In contrast,
the PCPD-GH algorithm (Algorithm 4) without nonnegative constraint identiï¬es
ï¬ve social groups, where the detailed groupings are presented in Table6.5. While the
number of groups can also be considered reasonable [2], after a closer inspection,
two people, namely Philip Allen and Jeff Dasovich (marked bold in Table6.5) were
put in the wrong groups. This demonstrates that the prior nonnegative structure is
beneï¬cial for social group clustering.
Finally, interesting patterns can be observed from the temporal cluster proï¬les,
which are obtained from the third factor matrix C [1, 2], as illustrated in Fig.6.10.
It is clear that when the company has important events, such as the change of CEO,
crisis breaks, and bankruptcy, distinct peaks appear.

References
133
References
1. B.W. Bader, R.A. Harshman, T.G. Kolda, Temporal analysis of social networks using three-
way dedicom. Sandia National Laboratories (SNL), Albuquerque, NM, and Livermore, CA ...,
Technical Report (2006)
2. E.E. Papalexakis, N.D. Sidiropoulos, R. Bro, From k-means to higher-way co-clustering: mul-
tilinear decomposition with sparse latent factors. IEEE Trans. Signal Process. 61(2), 493â€“506
(2012)
3. D.P. Bertsekas, Nonlinear programming. J. Oper. Res. Soc. 48(3), 334â€“334 (1997)
4. A.P. Liavas, G. Kostoulas, G. Lourakis, K. Huang, N.D. Sidiropoulos, Nesterov-based alternat-
ing optimization for nonnegative tensor factorization: algorithm and parallel implementation.
IEEE Trans. Signal Process. 66(4), 944â€“953 (2017)
5. K. Huang, N.D. Sidiropoulos, A.P. Liavas, A ï¬‚exible and efï¬cient algorithmic framework for
constrained matrix and tensor factorization. IEEE Trans. Signal Process. 64(19), 5052â€“5065
(2016)
6. Q. Zhao, L. Zhang, A. Cichocki, Bayesian cp factorization of incomplete tensors with automatic
rank determination. IEEE Trans. Pattern Anal. Mach. Intell. 37(9), 1751â€“1763 (2015)
7. L. Cheng, Y.-C. Wu, H.V. Poor, Probabilistic tensor canonical polyadic decomposition with
orthogonal factors. IEEE Trans. Signal Process. 65(3), 663â€“676 (2016)
8. L. Cheng, Y.-C. Wu, H.V. Poor, Scaling probabilistic tensor canonical polyadic decomposition
to massive data. IEEE Trans. Signal Process. 66(21), 5534â€“5548 (2018)
9. L. Cheng, X. Tong, S. Wang, Y.-C. Wu, H.V. Poor, Learning nonnegative factors from tensor
data: probabilistic modeling and inference algorithm. IEEE Trans. Signal Process. 68, 1792â€“
1806 (2020)
10. J.C. Arismendi, Multivariate truncated moments. J. Multivar. Anal. 117, 41â€“75 (2013)
11. M.D. Hoffman, D.M. Blei, C. Wang, J. Paisley, Stochastic variational inference. J. Mach. Learn.
Res. (2013)
12. Y. Xu, W. Yin, A block coordinate descent method for regularized multiconvex optimization
with applications to nonnegative tensor factorization and completion. SIAM J. Imag. Sci. 6(3),
1758â€“1789 (2013)
13. L. Grippo, M. Sciandrone, On the convergence of the block nonlinear gauss-seidel method
under convex constraints. Oper. Res. Lett. 26(3), 127â€“136 (2000)
14. H.-J.M. Shi, S. Tu, Y. Xu, W. Yin, A primer on coordinate descent algorithms (2016).
arXiv:1610.00040
15. M. Razaviyayn, M. Hong, Z.-Q. Luo, A uniï¬ed convergence analysis of block successive
minimization methods for nonsmooth optimization. SIAM J. Optim. 23(2), 1126â€“1153 (2013)
16. P. Tseng, S. Yun, A coordinate gradient descent method for nonsmooth separable minimization.
Math. Programm. 117(1), 387â€“423 (2009)
17. A. Cichocki, R. Zdunek, A.H. Phan, S.-I. Amari, Nonnegative Matrix and Tensor Factor-
izations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation
(Wiley, New York, 2009)
18. T.G. Kolda, B.W. Bader, Tensor decompositions and applications. SIAM Rev. 51(3), 455â€“500
(2009)
19. N.D. Sidiropoulos, G.B. Giannakis, R. Bro, Blind parafac receivers for ds-cdma systems. IEEE
Trans. Signal Process. 48(3), 810â€“823 (2000)
20. P. Breiding, N. Vannieuwenhoven, The condition number of join decompositions. SIAM J.
Matrix Anal. Appl. 39(1), 287â€“309 (2018)
21. N. Vannieuwenhoven, Condition numbers for the tensor rank decomposition. Linear Algebra
Appl. 535, 35â€“86 (2017)
22. H.A. Kiers, A three-step algorithm for candecomp/parafac analysis of large data sets with
multicollinearity. J. Chemom.: J. Chemom. Soc. 12(3), 155â€“171 (1998)

Chapter 7
Complex-Valued CPD, Orthogonality
Constraint, and Beyond Gaussian Noises
Abstract In previous chapters, Bayesian CPDs are developed for real-valued tensor
data. They cannot deal with complex-valued tensor data, which, however, frequently
occurs in applications including wireless communications and sensor array signal
processing. In addition, we have not touched on the design of Bayesian CPD that
incorporates the orthogonality structure and/or handles non-Gaussian noises. In this
chapter, we present a uniï¬ed Bayesian modeling and inference for complex-valued
tensor CPD with/without orthogonal factors, under/not under Gaussian noises. Appli-
cations on blind receiver design and linear image coding are presented.
7.1
Problem Formulation
We consider a generalized problem in which the complex-valued tensor Y âˆˆ
CI1Ã—I2Ã—Â·Â·Â·Ã—IN obeys the following model:
Y = [[A(1), A(2), . . . , A(N)]] + W + E,
(7.1)
where W represents an additive noise tensor with each element being i.i.d. and
wi1,i2,...,iN âˆ¼CN

wi1,i2,...,iN |0, Î²âˆ’1
; E denotes potential outliers in measurements
with each element ei1,i2,...,iN taking an unknown complex value if an outlier emerges,
and otherwise taking the value zero. Furthermore, it is assumed that {A(n)}P
n=1 are
known to be orthogonal where P < N, while the remaining factor matrices are
unconstrained.
Due to the orthogonality structure of the ï¬rst P factor matrices {A(n)}P
n=1, they
can be written as A(n) = U(n)(n) where U(n) is an orthonormal matrix and (n) is
a diagonal matrix describing the scale of each column. Putting A(n) = U(n)(n) for
1 â‰¤n â‰¤P into the deï¬nition of the tensor CPD, it is easy to show that
[[A(1), A(2), . . . , A(N)]] = [[(1), (2), . . . , (N)]]
(7.2)
with (n) = U(n) for 1 â‰¤n â‰¤P, (n) = A(n) for P + 1 â‰¤n â‰¤N âˆ’1, and
(N) = A(N)(1)(2) Â· Â· Â· (P), where  âˆˆRRÃ—R is a permutation matrix. From
Â© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
L. Cheng et al., Bayesian Tensor Decomposition for Signal Processing and
Machine Learning, https://doi.org/10.1007/978-3-031-22438-6_7
135

136
7
Complex-Valued CPD, Orthogonality Constraint, and Beyond Gaussian Noises
(7.2), it can be seen that up to a scaling and permutation indeterminacy, the tensor
CPD under orthogonal constraints is equivalent to that under orthonormal constraints.
In general, the scaling and permutation ambiguity can be easily resolved using side
information [1, 2]. Thus, without loss of generality, our goal is to estimate an N-tuplet
of factor matrices ((1), (2), . . ., (N)) with the ï¬rst P (where P < N) of them
being orthonormal, based on the observation Y and in the absence of the knowledge
of noise power Î²âˆ’1, outlier statistics, and the tensor rank R.
In particular, since we do not know the exact value of R, it is assumed that there
are L columns in each factor matrix (n), where L is the maximum possible value
of the tensor rank R. Then, the problem to be solved can be stated as
min
{(n)}N
n=1,E Î² âˆ¥Y âˆ’[[(1), (2), . . . , (N)]] âˆ’E âˆ¥2
F +
L

l=1
Î³l

N

n=1
(n)H
:,l
(n)
:,l

s.t.
(n)H(n) = IL, n = 1, 2, . . . , P,
(7.3)
where the regularization term L
l=1 Î³l
N
n=1 (n)H
:,l
(n)
:,l

is added to control the
complexity of the model and avoid overï¬tting of noise, since more columns (thus
more degrees of freedom) in (n) than the true model are introduced, and {Î³l}L
l=1 are
regularization parameters trading off the relative importance of the square error term
and the regularization term.
Problem (7.3) is more general than the model in existing works [3] since an extra
outlier term is present. Furthermore, the choice of regularization parameters plays
an important role, since setting Î³l too large results in excessive residual squared
error, while setting Î³l too small risks overï¬tting of noise. In general, determining
the optimal regularization parameters (e.g., using cross-validation, or the L-curve)
requires exhaustive search and thus is computationally demanding. To overcome
these problems, a novel algorithm based on the framework of probabilistic inference
is presented, which effectively mitigates the outliers E and automatically learns the
regularization parameters and the tensor rank.
7.2
Probabilistic Modeling
Before solving problem (7.3), we ï¬rst interpret different terms in (7.3) as probability
density functions, based on which a probabilistic model that encodes our knowledge
of the observation and the unknowns can be established.
Firstly, since the elements of the additive noise W is white, zero-mean, and
circularly symmetric complex Gaussian, the squared error term in problem (7.3) can
be interpreted as the negative log of the likelihood
p

Y | (1), (2), . . ., (N), E, Î²

âˆexp

âˆ’Î² âˆ¥Y âˆ’[[(1), (2), . . ., (N)]] âˆ’E âˆ¥2
F

.
(7.4)

7.2 Probabilistic Modeling
137
Secondly, the regularization term in problem (7.3) can be interpreted as arising from
a circularly symmetric complex Gaussian prior distribution over the columns of the
factor matrices, i.e., N
n=1
L
l=1 CN

(n)
:,l | 0InÃ—1, Î³ âˆ’1
l
IL

. Note that this is analogue
to (3.4) but is tailored to complex-valued factor matrices.
On the other hand, for the ï¬rst P factor matrices {(n)}P
n=1, there are additional
hard constraints in problem (7.3), which correspond to the Stiefel manifold [6]
VL(CIn) = {A âˆˆCInÃ—L : AHA = IL} for 1 â‰¤n â‰¤P. Since the orthonormal con-
straints result in (n)H
:,l
(n)
:,l = 1, the hard constraints would dominate the Gaussian
distribution of the columns in {(n)}P
n=1. Therefore, (n) can be interpreted as being
uniformly distributed over the Stiefel manifold VL(CIn) for 1 â‰¤n â‰¤P, and Gaussian
distributed for P + 1 â‰¤n â‰¤N:
p((1), (2), . . . , (P)) âˆ
P
	
n=1
IVL(CIn )((n)),
p((P+1), (P+2), . . . , (N)) =
N
	
n=P+1
L
	
l=1
CN

(n)
:,l |0InÃ—1, Î³ âˆ’1
l
IL

,
(7.5)
where IVL(CIn )((n)) is an indicator function with IVL(CIn )((n)) = 1 when (n) âˆˆ
VL(CIn), and otherwise IVL(CIn )((n)) = 0. For the parameters Î² and {Î³l}L
l=1, which
correspondtotheinversenoisepowerandthevariancesofcolumnsinthefactormatri-
ces, since we have no information about their distributions, a non-informative Jef-
freyâ€™s prior is imposed on them, i.e., p(Î²) âˆÎ²âˆ’1 and p(Î³l) âˆÎ³ âˆ’1
l
for l = 1, . . . , L.
This is equivalent to imposing gamma distribution on Î² and {Î³l}L
l=1 but with their
hyper-parameters approaching zero.
Finally, although the generative model for outliers Ei1,...,iN is unknown, the rare
occurrence of outliers motivates us to employ a complex-valued studentâ€™s t distri-
bution as its prior, i.e., p(Ei1,...,iN ) = T (Ei1,...,iN |0, ci1,...,iN , di1,...,iN ). Similar to the
real-valued studentâ€™s t distribution (see Table2.1), complex-valued studentâ€™s t dis-
tribution can also be equivalently represented as a Gaussian scale mixture [7]:
T

Ei1,...,iN | 0, ci1,...,iN , di1,...,iN

=

CN

Ei1,...,iN | 0, Î¶ âˆ’1
i1,...,iN

gamma

Î¶i1,...,iN | ci1,...,iN , di1,...,iN

dÎ¶i1,...,iN . (7.6)
This means that studentâ€™s t distribution can be obtained by mixing an inï¬nite number
of zero-mean circularly symmetric complex Gaussian distributions where the mixing
distribution on the precision Î¶i1,...,iN is the gamma distribution with parameters ci1,...,iN
and di1,...,iN . In addition, since the statistics of outliers such as means and correlations
are generally unavailable in practice, we set the hyper-parameters ci1,...,iN and di1,...,iN

138
7
Complex-Valued CPD, Orthogonality Constraint, and Beyond Gaussian Noises
Â·Â·Â·
Â·Â·Â·
Y
X
W
E
Î²
Î(1)
Î(P )
Î(P +1)
Î(N)
Stiefel manifold
Î³l
Î¶i1,Â·Â·Â· ,iN
L
I1, Â· Â· Â· , IN
Fig. 7.1 Probabilistic model for complex-valued tensor CPD with orthogonal factors and outliers
(Â©[2017]IEEE.Reprinted,withpermission,from[L.Cheng,Y.-C.Wu,andH.V.Poor,Probabilistic
Tensor Canonical Polyadic Decomposition With Orthogonal Factors, IEEE Transactions on Signal
Processing, Feb 2017]. It applies all ï¬gures and tables in this chapter)
as 10âˆ’6 to produce a non-informative prior on Ei1,...,iN , and assume outliers are
independent of each other:
p (E) =
I1
	
i1=1
Â· Â· Â·
IN
	
iN =1
T

Ei1,...,iN | 0, ci1,...,iN = 10âˆ’6, di1,...,iN = 10âˆ’6
.
(7.7)
The complete probabilistic model is shown in Fig.7.1.
7.3
Inference Algorithm Development
Let  be a set containing the factor matrices {(n)}N
n=1, and other variables E, {Î³l}L
l=1,
{Î¶i1,...,iN }I1,...,IN
i1=1,...,iN =1, Î². From the probabilistic model established above, the marginal
probability density functions of the unknown factor matrices {}N
n=1 are given by
p((n)|Y) =

p(Y, )
p(Y)
d\(n), n = 1, 2, . . . , N,
(7.8)

7.3 Inference Algorithm Development
139
where
p(Y, ) âˆ
P
	
n=1
IVL(CIn )

(n)
exp
  N
	
n=1
In âˆ’1

ln Î²
+

N

n=P+1
In + 1

L

l=1
ln Î³l âˆ’Tr


N

n=P+1
(n)H(n)

+
I1

i1=1
Â· Â· Â·
IN

iN =1

(ci1,...,iN âˆ’1) ln Î¶i1,...,iN âˆ’di1,...,iN Î¶i1,...,iN

+
I1

i1=1
Â· Â· Â·
IN

iN =1

ln Î¶i1,...,iN âˆ’Î¶i1,...,iN Eâˆ—
i1,...,iN Ei1,...,iN

âˆ’Î² âˆ¥Y âˆ’[[(1), (2), . . ., (N)]] âˆ’E âˆ¥2
F

(7.9)
with  = diag{Î³1, . . . , Î³R}.
Since the factor matrices and other variables are nonlinearly coupled in (7.9),
the multiple integrations in (7.8) are analytically intractable, which prohibits exact
Bayesian inference. To handle this problem, we use variational inference as in pre-
vious chapters.
However, in addition to the mean-ï¬eld approximation Q() = 
k Q(k) where
k âˆˆ, to facilitate the manipulation of hard constraints on the ï¬rst P factor matri-
ces, their variational densities are further assumed to take a Dirac delta functional
form Q((k)) = Î´((k) âˆ’Ë†
(k)) for k = 1, 2, . . . , P, where Ë†
(k) is a parameter to
be derived.
Under these approximations, the probability density functions Q(k) of the vari-
ational distribution can be obtained via [5]
Q((k)) = Î´

(k) âˆ’arg max
(k) E
 j Ì¸=(k) Q( j)

ln p (Y, )




â‰œË†
(k)

, k = 1, 2, . . . , P,
(7.10)
and
Q(k) âˆexp

E
jÌ¸=k Q( j)

ln p (Y, )

, k âˆˆ \ {(k)}P
k=1.
(7.11)
Obviously, these variational distributions are coupled in the sense that the com-
putation of the variational distribution of one parameter requires the knowledge of
variational distributions of other parameters. Therefore, these variational distribu-
tions should be updated iteratively. In the following, an explicit expression for each
Q (Â·) is derived.

140
7
Complex-Valued CPD, Orthogonality Constraint, and Beyond Gaussian Noises
7.3.1
Derivation for Q((k)), 1 â‰¤k â‰¤P
By substituting (7.9) into (7.11) and only keeping the terms relevant to (k) (1 â‰¤
k â‰¤P), we directly have
Ë†
(k) = arg max
(k)âˆˆVL(CIk )
E
 j Ì¸=(k) Q( j)

âˆ’Î² âˆ¥Y âˆ’
(1), . . . , (N)]] âˆ’E

2
F

. (7.12)
To expand the square of the Frobenius norm inside the expectation in (7.12), we use
the result that âˆ¥Aâˆ¥2
F=âˆ¥A(k)âˆ¥2
F= Tr

A(k)
A(k)H
, where A(k) is the unfolding of
an Nth-order tensor A âˆˆCI1Ã—Â·Â·Â·Ã—IN along its kth mode (see Deï¬nition 1.1). After
expanding the square of the Frobenius norm and taking expectations, the parameter
Ë†
(k) for each variational density in {Q((k))}P
k=1 can be obtained from the following
problem:
Ë†
(k) =
arg max
(k)âˆˆVL (CIk )
Tr

EQ(Î²)[Î²]

Y âˆ’EQ(E) [E]
(k)
Nâ‹„
n=1,nÌ¸=k EQ((n))

(n) âˆ—



â‰œF(k)
(k)H + (k)F(k)H

âˆ’Tr

(k)H (k)

EQ(Î²) [Î²] EN
n=1,nÌ¸=k Q((n))

Nâ‹„
n=1,nÌ¸=k (n)T 
Nâ‹„
n=1,nÌ¸=k (n)âˆ—
+ EL
l=1 Q(Î³l) []




â‰œG(k)

.
(7.13)
Using the fact that the feasible set for parameter (k) is the Stiefel manifold
VL(CIk), i.e., (k)H(k) = IL, the term G(k) is irrelevant to the factor matrix of
interest (k). Consequently, problem (7.13) is equivalent to
Ë†
(k) = arg max
(k)âˆˆVL(CIk )
Tr

F(k)(k)H + (k)F(k)H

,
(7.14)
where F(k) was deï¬ned in the ï¬rst line of (7.13). Problem (7.14) is a non-convex
optimization problem, as its feasible set VL(CIk) is non-convex [9]. While in general
(7.14) can be solved by numerical iterative algorithms based on a geometric approach
or the alternating direction method of multipliers [9], a closed-form optimal solution
can be obtained by noticing that the objective function in (7.14) has the same func-
tional form as the log of the von Misesâ€“Fisher matrix distribution with parameter
matrix F(k), and the feasible set in (7.14) also coincides with the support of this von
Misesâ€“Fisher matrix distribution [8]. As a result, we have
Ë†
(k) = arg max
(k)
ln VMF

(k) | F(k)
.
(7.15)

7.3 Inference Algorithm Development
141
Then, the closed-form solution for problem (7.15) can be acquired using the property
below, which has been proved in [6].
Property 6.1. Suppose the matrix A âˆˆCÎº1Ã—Îº2 follows a von Misesâ€“Fisher
matrix distribution with parameter matrix F âˆˆCÎº1Ã—Îº2. If F = UVH is the
SVD of the matrix F, then the unique mode of VMF (A | F) is UVH.
From Property 6.1, it is easy to conclude that Ë†
(k) = Ï’(k)(k)H, where Ï’(k) and
(k) are the left-orthonormal matrix and right-orthonormal matrix from the SVD of
F(k), respectively.
7.3.2
Derivation for Q((k)), P + 1 â‰¤k â‰¤N
Using (7.9) and (7.11), the variational density Q

(k)
(P + 1 â‰¤k â‰¤N) can be
derived to be a circularly symmetric complex matrix normal distribution [8] as
Q

(k)
= CMN((k) | M(k), IIk, 	(k))
(7.16)
where
	(k) =

EQ(Î²) [Î²] EN
n=1,nÌ¸=k Q((n))

Nâ‹„
n=1,nÌ¸=k (n)T 
Nâ‹„
n=1,nÌ¸=k (n)âˆ—
+ EL
l=1 Q(Î³l) []
âˆ’1
(7.17)
M(k) = EQ(Î²) [Î²]

Y âˆ’EQ(E) [E]
(k)
Nâ‹„
n=1,nÌ¸=k EQ((n))

(n) âˆ—
	(k).
(7.18)
Due to the fact that Q((k)) is Gaussian, the parameter M(k) is both the expectation
and the mode of the variational density Q

(k)
.
To calculate M(k), some expectation computations are required as shown in (7.17)
and (7.18).ForthosewiththeformEQ(k) [k]wherek âˆˆ,thevaluecanbeeasily
obtained if the corresponding Q (k) is available. The remaining challenge stems
from the expectation EN
n=1,nÌ¸=k Q((n))

Nâ‹„
n=1,nÌ¸=k (n)T 
Nâ‹„
n=1,nÌ¸=k (n)âˆ—
in (7.17). But
its calculation becomes straightforward after exploiting the orthonormal structure of
{ Ë†
(k)}P
k=1 and the property of multiple Khatriâ€“Rao products, as presented in the
following property.

142
7
Complex-Valued CPD, Orthogonality Constraint, and Beyond Gaussian Noises
Property 6.2. Suppose the matrix A(n) âˆˆCÎºnÃ—Ï âˆ¼Î´(A(n) âˆ’Ë†A(n)) for 1 â‰¤
n â‰¤P, where Ë†A(n) âˆˆVÏ(CÎºn) and P < N, and the matrix A(n) âˆˆCÎºnÃ—Ï âˆ¼
CMN(A(n) | M(n), IÎºn, 	(n)) for P + 1 â‰¤n â‰¤N. Then,
EN
n=1,nÌ¸=k p(A(n))

Nâ‹„
n=1,nÌ¸=k A(n)T 
Nâ‹„
n=1,nÌ¸=k A(n)âˆ—
= D

NâŠ™
n=P+1,nÌ¸=k

M(n)HM(n) + Îºn	(n)âˆ—
(7.19)
where D[A] is a diagonal matrix taking the diagonal element from A,
and the multiple Hadamard products
NâŠ™
n=1,nÌ¸=k A(n) = A(N) âŠ™Â· Â· Â· âŠ™A(k+1) âŠ™
A(kâˆ’1) âŠ™Â· Â· Â· âŠ™A(1).
7.3.3
Derivation for Q (E)
The variational density Q (E) can be obtained by taking only the terms relevant to E
after substituting (7.9) into (7.11) and is expressed as
Q (E)âˆ
I1
	
i1=1
Â· Â· Â·
IN
	
iN =1
exp

E
 j Ì¸=E Q( j)

âˆ’Î¶i1,...,iN
Ei1,...,iN

2
âˆ’Î²
Yi1,...,in âˆ’
L

l=1
N
	
n=1
(n)
in,l âˆ’Ei1,...,iN

2
.
(7.20)
After taking expectations, the term inside the exponent of (7.20) is
âˆ’Eâˆ—
i1,...,iN

EQ(Î²)[Î²] + EQ(Î¶i1,...,iN )

Î¶i1,...,iN




â‰œpi1,...,iN

Ei1,...,iN
+ 2Re

Eâˆ—
i1,...,iN pi1,...,iN EQ(Î²)[Î²]pâˆ’1
i1,...,iN

Yi1,...,iN âˆ’
L

l=1
 N
	
n=1
EQ((n))[(n)
in,l]




â‰œmi1,...,iN

.
(7.21)
Since (7.21) is a quadratic function with respect to Ei1,...,iN , it is easy to show that

7.3 Inference Algorithm Development
143
Q (E) =
I1
	
i1=1
Â· Â· Â·
IN
	
iN =1
CN

Ei1,...,iN | mi1,...,iN , pâˆ’1
i1,...,iN

.
(7.22)
Notice that from (7.21), the computation of outlier mean mi1,...,iN can be rewritten
as mi1,...,iN = n1n2, where n1 =

EQ(Î¶i1,...,iN )[Î¶i1,...,iN ]
âˆ’1

EQ(Î¶i1,...,iN )[Î¶i1,...,iN ]
âˆ’1
+

EQ(Î²)[Î²]
âˆ’1 and n2 = Yi1,...,iN âˆ’
L
l=1
 N
n=1 EQ((n))[(n)
in,l]

. From the general data model in (7.1), it can be seen
that n2 consists of the estimated outliers plus noise. On the other hand, since

EQ(Î¶i1,...,iN )[Î¶i1,...,iN ]
âˆ’1 and

EQ(Î²)[Î²]
âˆ’1 can be interpreted as the estimated power
of the outliers and the noise, respectively, n1 represents the strength of the outliers
in the estimated outliers plus noise. Therefore, if the estimated power of the out-
liers

EQ(Î¶i1,...,iN )[Î¶i1,...,iN ]
âˆ’1 goes to zero, the outlier mean mi1,...,iN becomes zero
accordingly.
7.3.4
Derivations for Q(Î³l), Q(Î¶i1,...,iN), and Q (Î²)
Using (7.9) and (7.11) again, the variational density Q (Î³l) can be expressed as
Q (Î³l) âˆexp
â§
âªâªâªâªâªâ¨
âªâªâªâªâªâ©
â›
âœâœâœâœâœâ
N

n=P+1
In
  
â‰œËœal
âˆ’1
â
âŸâŸâŸâŸâŸâ 
ln Î³l âˆ’Î³l

N

n=P+1
EQ((n))

(n)H
:,l
(n)
:,l




â‰œËœbl
â«
âªâªâªâªâªâ¬
âªâªâªâªâªâ­
, (7.23)
which has the same functional form as the probability density function of gamma
distribution,i.e., Q(Î³l) = gamma(Î³l | Ëœal, Ëœbl).SinceEQ(Î³l)[Î³l] = Ëœal/Ëœbl isrequiredfor
updating the variational distributions of other variables in , we need to compute
Ëœal and Ëœbl. While the computation of Ëœal is straightforward, the computation of Ëœbl
can be facilitated by using the correlation property of the matrix normal distribution
EQ((n))[(n)H
:,l
(n)
:,l ] = M(n)H
:,l
M(n)
:,l + In	(n)
l,l [8] for P + 1 â‰¤n â‰¤N.
Similarly, using (7.9) and (7.11), the variational densities Q

Î¶i1,...,iN

and Q (Î²)
can be found to be gamma distributions as
Q

Î¶i1,...,iN

= gamma

Î¶i1,...,iN | Ëœci1,...,iN , Ëœdi1,...,iN

,
(7.24)
Q (Î²) = gamma

Î² | Ëœe, Ëœf

,
(7.25)
with parameters Ëœci1,...,iN = ci1,...,iN + 1, Ëœdi1,...,iN = di1,...,iN +

mi1,...,iN
âˆ—mi1,...,iN +
pâˆ’1
i1,...,iN , Ëœe = N
n=1 In, and Ëœf =EN
n=1 Q((n))Q(E)

âˆ¥Y âˆ’[[(1), . . . , (N)]] âˆ’E âˆ¥2
F

.

144
7
Complex-Valued CPD, Orthogonality Constraint, and Beyond Gaussian Noises
For Ëœci1,...,iN , Ëœdi1,...,iN and Ëœe, the computations are straightforward. Furthermore, Ëœf is
derived to be [14]
Ëœf = âˆ¥Y âˆ’M âˆ¥2
F +
I1

i1=1
Â· Â· Â·
IN

iN =1
pâˆ’1
i1,...,iN + Tr

D

NâŠ™
n=P+1

M(n)HM(n) + In	(n)âˆ—
âˆ’2Re

Tr

Y âˆ’M
(1)
Nâ‹„
n=P+1 M(n)
â‹„

Pâ‹„
n=2
Ë†
(n)âˆ—Ë†
(1)H
,
(7.26)
where M is a tensor with its (i1, . . . , iN)th element being mi1,...,iN , and Re(Â·) denotes
the real part of its argument.
7.3.5
Summary of the Iterative Algorithm
From the expressions for Q(k) evaluated above, it is seen that the calculation of
a particular Q (k) relies on the statistics of other variables in . As a result, the
variational distribution for each variable in  should be iteratively updated. The
iterative algorithm is summarized in Algorithm 10.
7.3.6
Further Discussions
To gain more insights from Algorithm 10, discussions on its convergence property,
automatic rank determination, relationship to the OALS algorithm, and computa-
tional complexity are presented in the following.
7.3.6.1
Convergence Property
Although the functional minimization of the KL divergence is non-convex over
the mean-ï¬eld family Q() = 
k Q(k), it is convex with respect to a single
variational density Q(k) when the others {Q( j)| j Ì¸= k} are ï¬xed. Therefore,
Algorithm 10, which iteratively updates the optimal solution for each k, is essen-
tially a coordinate descent algorithm in the functional space of variational distri-
butions with each update solving a convex problem. This guarantees a monotonic
decrease of the KL divergence, and Algorithm 10 is guaranteed to converge to at
least a stationary point.

7.3 Inference Algorithm Development
145
Algorithm 10 Probabilistic Tensor CPD with Orthogonal Factors
Initializations:
Choose L > R and initial values { Ë†
(n,0)}P
n=1, {M(n,0), 	(n,0)}N
n=P+1, Ëœb0
l , {Ëœc0
i1,...,iN , Ëœd0
i1,...,iN } and
Ëœf 0 for all l and i1, . . . , iN. Let Ëœal = N
n=P+1 In and Ëœe = N
n=1 In.
Iterations: For the tth iteration (t â‰¥1),
Update the statistics of outliers:
,
pi1,...,iN , mi1,...,iN
-I1,...,IN
i1=1,...,iN =1:
pt
i1,...,iN =
Ëœe
Ëœf tâˆ’1 +
Ëœctâˆ’1
i1,...,iN
Ëœdtâˆ’1
i1,...,iN
,
(7.27)
mt
i1,...,iN =
Ëœe
Ëœf tâˆ’1 pt
i1,...,iN

Yi1,...,iN âˆ’
L

l=1

P
	
n=1
Ë†
(n,tâˆ’1)
in,l

N
	
n=P+1
M(n,tâˆ’1)
in,l

.
(7.28)
Update the statistics of factor matrices: {M(k), 	(k)}N
k=P+1:
	(k,t) =

Ëœe
Ëœf tâˆ’1 D

NâŠ™
n=P+1,nÌ¸=k

M(n,tâˆ’1)H M(n,tâˆ’1) + In	(n,tâˆ’1)âˆ—
+ diag
 Ëœa1
Ëœbtâˆ’1
1
, ...,
ËœaL
Ëœbtâˆ’1
L
âˆ’1
, (7.29)
M(k,t) =
Ëœe
Ëœf tâˆ’1
 
Y âˆ’Mt(k) 
Nâ‹„
n=P+1,nÌ¸=k M(n,tâˆ’1)
â‹„

Pâ‹„
n=1
Ë†
(n,tâˆ’1)âˆ—
	(k,t).
(7.30)
Update the orthonormal factor matrices { Ë†
(k)}P
k=1:

Ï’(k,t), (k,t)
= SVD

Ëœe
Ëœf tâˆ’1

Y âˆ’Mt(k)
Nâ‹„
n=P+1 M(n,t)
â‹„

Pâ‹„
n=1,nÌ¸=k
Ë†
(n,tâˆ’1)âˆ—
,
Ë†
(k,t) = Ï’(k,t)(k,t)H.
(7.31)
Update {Ëœbl}L
l=1, { Ëœdi1,...,iN }I1,...,IN
i1=1,...,iN =1 and Ëœf
Ëœbt
l =
N

n=P+1
M(n,t)H
:,l
M(n,t)
:,l
+ In	(n,t)
l,l
,
(7.32)
Ëœct
i1,...,iN = Ëœc0
i1,...,iN + 1,
(7.33)
Ëœdt
i1,...,iN = Ëœd0
i1,...,iN +

mt
i1,...,iN
âˆ—
mt
i1,...,iN + 1/pt
i1,...,iN ,
(7.34)
Ëœf t = âˆ¥Y âˆ’Mt âˆ¥2
F +
I1

i1=1
Â· Â· Â·
IN

in=1
(pt
i1,...,iN )âˆ’1 + Tr

D

NâŠ™
n=P+1

M(n,t)HM(n,t) + In	(n,t)âˆ—
âˆ’2Re

Tr

Y âˆ’Mt(1)
Nâ‹„
n=P+1 M(n,t)
â‹„
 Pâ‹„
n=2
Ë†
(n,t)âˆ—Ë†
(1,t)H

.
(7.35)
Until Convergence
7.3.6.2
Automatic Rank Determination
The automatic rank determination for the tensor CPD uses an idea from the Bayesian
model selection (or Bayesian Occamâ€™s razor). More speciï¬cally, the parameters

146
7
Complex-Valued CPD, Orthogonality Constraint, and Beyond Gaussian Noises
{Î³l}L
l=1 control the model complexity, and their optimal variational densities are
obtained together with those of other parameters by minimizing the KL divergence.
After convergence, if some E[Î³l] are very large, e.g., 106, this indicates that their
corresponding columns in {M(n)}N
n=P+1 can be â€œswitched offâ€, as they play no role in
explaining the data. Furthermore, according to the deï¬nition of the tensor CPD, the
corresponding columns in { Ë†
(n)}P
n=1 should also be pruned accordingly. Finally, the
learned tensor rank R is the number of remaining columns in each estimated factor
matrix Ë†
(n).
7.3.6.3
Computational Complexity
For each iteration, the complexity is dominated by updating each factor matrix, cost-
ing O(N
n=1 InL2 + N N
n=1 InL). Thus, the overall complexity is about O(q(N
n=1
InL2 + N N
n=1 InL)) where q is the number of iterations needed for convergence.
On the other hand, for the OALS algorithm with exact tensor rank R, its complexity
is O(m(N
n=1 In R2 + N N
n=1 In R)) where m is the number of iterations needed
for convergence. Therefore, for each iteration, the complexity of Algorithm 10 is
comparable to that of the OALS algorithm.
7.3.6.4
Reduction to Special Cases
The model and inference algorithm are general in the sense that they include various
special cases. For example, if there is no orthogonal factor matrix, we can set P = 0;
if the tensor is in real-valued, we can simply replace hermitian with transpose; if we
believe there are no outliers, we can skip the steps related to E.
7.4
Simulation Results and Discussions
In this section, numerical simulations are presented to assess the performance of
the developed algorithm (labeled as VB) using synthetic data and two applications,
in comparison with various state-of-the-art tensor CPD algorithms. The algorithms
being compared include the ALS, the simultaneous diagonalization method for cou-
pled tensor CPD (labeled as SD) [11], the direct algorithm for CPD followed by
enhanced ALS (labeled as DIAG-A) [12], the Bayesian tensor CPD (Algorithm 5,
labeled as BCPD) [4], the robust iteratively reweighed ALS (labeled as IRALS) [13],
and the OALS algorithm (labeled as OALS) [3]. Note that some of these algorithms
were not originally derived for complex-valued data. In that case, they are extended
to handle complex-valued data for comparison.
In all experiments, three outlier models are considered, and they are listed
in Table7.1. For all the simulated algorithms, the initial factor matrix Ë†
(n,0) is

7.4 Simulation Results and Discussions
147
Table 7.1 Three different outlier models
Scenario
Variable description
Bernoulliâ€“Gaussian
Ei1,...,iN âˆ¼CN(0, Ïƒ 2
e ) with a probability Ï€
Bernoulli-Uniform
Ei1,...,iN âˆ¼U(âˆ’H, H) with a probability Ï€
Bernoulli-Studentâ€™s t
Ei1,...,iN âˆ¼T (Î¼, Î», Î½) with a probability Ï€
set as the matrix consisting of L leading left singular vectors of [Y](n) where
L = max{I1, I2, . . . , IN} for Algorithm 10 and the BCPD, and L = R for other
algorithms. The initial parameters of the algorithm in this chapter are set as
{Ëœc0
i1,...,iN , Ëœd0
i1,...,iN } = 10âˆ’6 for all i1, . . . , iN, Ëœb0
l = N
n=P+1 In for alll, Ëœf 0 = N
n=1 In,
and {	(n,0)}N
n=P+1 are all set to be IL. All the algorithms terminate at the tth
iteration when âˆ¥[[A(1,t), A(2,t), . . . , A(N,t)]] âˆ’[[A(1,tâˆ’1), A(2,tâˆ’1), . . . , A(N,tâˆ’1)]] âˆ¥2
F
< 10âˆ’6 or the iteration number exceeds 2000.
7.4.1
Validation on Synthetic Data
Synthetic tensors are used in this subsection to assess the performance of
Algorithm 10 on convergence, rank learning ability, and factor matrix recovery under
different outlier models. A complex-valued third-order tensor [[A(1), A(2), A(3)]] âˆˆ
C12Ã—12Ã—12 with rank R = 5 is considered, where the orthogonal factor matrix A(1)
is constructed from the R leading left singular vectors of a matrix drawn from
CMN(A|012Ã—5,, I12Ã—12, I5Ã—5), and the factor matrices {A(n)}3
n=2 are drawn from
CMN(A| 012Ã—5,, I12Ã—12, I5Ã—5). Parameters for outlier models are set as Ï€ = 0.05,
Ïƒ 2
e = 100, H = 10 arg maxi1,...,iN |[[A(1), A(2), A(3)]]i1,...,iN |, Î¼ = 3, Î» = 1/50, and
Î½ = 10. The signal-to-noise ratio (SNR) is deï¬ned as 10 log10(âˆ¥[[A(1), A(2), A(3)]]
âˆ¥2
F / âˆ¥W âˆ¥2
F). Each result in this subsection is obtained by averaging 500 Monte
Carlo runs.
Figure7.2 presents the convergence performance of Algorithm 10 under dif-
ferent outlier models, where the mean square error (MSE) âˆ¥[[ Ë†
(1), Ë†
(2), Ë†
(3)]] âˆ’
[[A(1), A(2), A(3)]] âˆ¥2
F is chosen as the assessment criterion. From Fig.7.2, it can be
seen that the MSEs decrease signiï¬cantly in the ï¬rst few iterations and converge to
stable values quickly, demonstrating the rapid convergence property. Furthermore,
by comparing the simulation results with outliers to that without outlier, it is clear
that Algorithm 10 is effective in mitigating outliers.
For tensor rank learning, the simulation results of Algorithm 10 are shown in
Fig.7.3a, while those of the Bayesian tensor CPD algorithm are shown in Fig.7.3b.
Each vertical bar in the ï¬gures shows the mean and standard deviation of rank
estimates, with the red horizontal dotted lines indicating the true tensor rank. The
percentages of correct estimates are also shown on top of the ï¬gures. From Fig.7.3a, it
is seen that Algorithm 10 can recover the true tensor rank with 100% accuracy when

148
7
Complex-Valued CPD, Orthogonality Constraint, and Beyond Gaussian Noises
0
10
20
30
40
50
60
70
80
90
100
Iteration
10-2
10-1
100
101
102
103
MSE
Bernoulli-Uniform
Bernoulli-Student's t
Bernoulli-Gaussian
No Outlier
70
71
72
73
74
75
4.9
5
5.1
5.2
5.3
50
51
52
53
54
55
0.5
0.55
0.6
SNR = 10 dB
SNR = 20 dB
Fig. 7.2 Convergence of Algorithm 10 under different outlier models
SNR â‰¥5 dB, both with or without outliers. This shows the accuracy and robustness
of Algorithm 10 when the noise power is moderate. Even though the performance at
low SNRs is not as impressive as that at high SNRs, it can be observed that Algorithm
10 still gives estimates close to the true tensor rank with the true rank lying mostly
within one standard deviation from the mean estimate. On the other hand, in Fig.7.3b,
it is observed that while the Bayesian tensor CPD algorithm performs nearly the same
as Algorithm 10 without outliers, it gives tensor rank estimates very far away from
the true value when outliers are present.
Figure7.4 compares Algorithm 10 to other state-of-the-art CPD algorithms in
terms of recovery accuracy of the orthogonal factor matrix A(1) under different outlier
models. The criterion is set as the best congruence ratio deï¬ned as minP âˆ¥A(1) âˆ’
Ë†
(1)P âˆ¥F / âˆ¥A(1) âˆ¥F, where the diagonal matrix  and the permutation matrix P
are found via the greedy least-squares column matching algorithm. From Fig.7.4a, it
is seen that both Algorithm 10 and OALS perform better than other algorithms when
outliers are absent. This shows the importance of incorporating the orthogonality
information of the factor matrix. On the other hand, while OALS offers the same
performanceasAlgorithm10whenthereisnooutlier,itsperformanceissigniï¬cantly
different in the presence of outliers, as presented in Fig.7.4bâ€“d. Furthermore, except
Algorithm 10 and IRALS, all other algorithms do not take the outliers into account,
thus their performances degrade signiï¬cantly as shown in Fig.7.4bâ€“d. Even though
the IRALS uses the robust lp (0 < p â‰¤1) norm optimization to alleviate the effects
of outliers, it cannot learn the statistical information of the outliers, leading to its
worse performance than that of Algorithm 10 in outliers mitigation.

7.4 Simulation Results and Discussions
149
-5
0
5
10
15
SNR (dB)
0
1
2
3
4
5
6
7
8
9
10
11
Estimated Rank
No Outlier
Bernoulli-Gaussian
Bernoulli-Uniform
Bernoulli-Student's t
  100%
  39.4%
  35.6%
  48.2%
  91.2%
  42%
  38.2%
  100%
  100%
  100%
  100%
  100%
  100%
  100%
  100%
  100%
  33.2%
  42.6%
  100%
  100%
(a)
-5
0
5
10
15
SNR (dB)
0
5
10
15
20
25
Estimated Rank
No Outlier
Bernoulli-Gaussian
Bernoulli-Uniform
Bernoulli-Student's t
  100%
  100%
  48.4%
  89%
  100%
  0%
  0%
  0.2%
  0%
  0%
   0%
   0%
  0.2%
  0%
  0%
  0%
   0%
  0%
  0%
  0%
(b)
Fig. 7.3 Rank determination using a Algorithm 10 and b the Bayesian tensor CPD

150
7
Complex-Valued CPD, Orthogonality Constraint, and Beyond Gaussian Noises
5
10
15
20
25
SNR (dB)
10-1
Best Congruence Ratio
ALS
SD
IRALS
BCPD
DIAG-A
OALS
VB
14.999
14.9995
15
15.0005
15.001
0.05516
0.05517
0.05518
0.05519
(a) No outlier
5
10
15
20
25
SNR (dB)
10-2
10-1
100
101
102
103
Best Congruence Ratio
ALS
SD
IRALS
BCPD
DIAG-A
OALS
VB
14.5
15
15.5
16
8
10
12
(b) Bernoulli-Gaussian
5
10
15
20
25
SNR (dB)
10-1
100
101
102
103
Best Congruence Ratio
ALS
SD
IRALS
BCPD
DIAG-A
OALS
VB
9.5
10
10.5
7
8
9
10
(c) Bernoulli-Uniform
5
10
15
20
25
SNR (dB)
10-2
10-1
100
101
102
103
Best Congruence Ratio
ALS
SD
IRALS
BCPD
DIAG-A
OALS
VB
14.9
15
15.1
15.2
10
12
14
16
(d) Bernoulli-Studentâ€™s t
Fig. 7.4 Performance of factor matrix recovery versus SNR under different outlier models
7.4.2
Blind Data Detection for DS-CDMA Systems
In a direct-sequence code division multiple access (DS-CDMA) system, the trans-
mitted signal sr(k) from the rth user at the kth symbol period is multiplied by a
spreading sequence [c1r, c2r, . . . , cZr] where czr is the zth chip of the applied spread-
ing code. Assuming R users transmit their signals simultaneously to a base station
(BS) equipped with M receive antennas, the received data is given by
ymz(k) =
R

r=1
hmrczrsr(k) + wmz(k), 1 â‰¤m â‰¤M, 1 â‰¤z â‰¤Z,
(7.36)
where hmr denotes the ï¬‚at fading channel between the rth user and the mth
receive antenna at the base station, and wmz(k) denotes white Gaussian noise. By
introducing H âˆˆCMÃ—R with its (m,r)th element being hmr, and C âˆˆCZÃ—R with
its (z,r)th element being czr, the model (7.36) can be written in matrix form
as Y(k) = R
r=1 H:,r â—¦C:,rsr(k) + W(k), where Y(k), W(k) âˆˆCMÃ—Z are matrices
with their (m, z)th elements being ymz(k) and wmz(k), respectively. After collecting

7.4 Simulation Results and Discussions
151
T samples along the time dimension and deï¬ning S âˆˆCT Ã—R with its (k,r)th element
being sr(k), the system model can be further written in the tensor form as [1]
Y =
R

r=1
H:,r â—¦C:,r â—¦S:,r + W = [[H, C, S]] + W
(7.37)
where Y âˆˆCMÃ—ZÃ—T and W âˆˆCMÃ—ZÃ—T are third-order tensors, which take ymz(k)
and wmz(k) as their (m, z, k)th elements, respectively.
It is shown in [1] that under certain mild conditions, the CPD of tensor Y, which
solves minH,C,S âˆ¥Y âˆ’[[H, C, S]] âˆ¥2
F, can blindly recover the transmitted signals S.
Furthermore, since the transmitted signals are usually uncorrelated and with zero
mean, the orthogonality structure1 of S can further be taken into account to give
better performance for blind signal recovery [2]. Similar models can also be found in
blind data detection for cooperative communication systems [15, 16], and in topology
learning for wireless sensor networks (WSNs) [17].
In this simulation, we set R = 5 users and the BS is equipped with M = 8 anten-
nas. The channel between the rth user and the mth antenna is a ï¬‚at fading chan-
nels hmr âˆ¼CN(hmr|0, 1). The transmitted data sr(k) are random binary phase-shift
keying (BPSK) symbols. The spreading code is of length Z = 6, and with each
code element czr âˆ¼CN(czr|0, 1). After observing the received tensor Y âˆˆC8Ã—6Ã—100,
Algorithm10andotherstate-of-the-arttensorCPDalgorithms,combinedwithambi-
guity removal and constellation mapping [1, 2], are executed to blindly detect the
transmitted data. Their performance is measured in terms of bit error rate (BER).
The BERs versus SNR under different outlier models are presented in Fig.7.5,
which are averaged over 10000 independent trials. The parameter settings for differ-
ent outlier models are the same as those in the last subsection. It is seen from Fig.7.5a
that when there is no outlier, Algorithm 10 and OALS behave the same, and both
outperform other CPDs. However, when outliers exist, it is seen from Fig.7.5bâ€“d
that Algorithm 10 performs signiï¬cantly better than other algorithms.
7.4.3
Linear Image Coding for a Collection of Images
Given a collection of images representing a class of objects, linear image coding
extracts the commonalities of these images, which is important in image compression
and recognition [18, 19]. The kth image of size M Ã— Z naturally corresponds to a
matrix B(k) with its (m, z)th element being the imageâ€™s intensity at that position.
Linear image coding seeks the orthogonal basis matrices U âˆˆCMÃ—R and V âˆˆCZÃ—R
that capture the directions of the largest R variances in the image data, and this
problem can be written as [18, 19]
1 Strictly speaking, S is only approximately orthogonal. But the approximation gets better and better
when observation length T increases.

152
7
Complex-Valued CPD, Orthogonality Constraint, and Beyond Gaussian Noises
0
2
5
1
0
1
5
SNR (dB)
10-6
10-5
10-4
10-3
10-2
BER
ALS
SD
IRALS
BCPD
DIAG-A
OALS
VB
14.5
15
15.5
0.9
1
1.1
1.2
Ã— 10-3
9.999
9.9995
10
10.0005 10.001
9.15
9.2
9.25 Ã— 10-5
(a) No outlier
0
2
5
1
0
1
5
SNR (dB)
10-7
10-6
10-5
10-4
10-3
10-2
10-1
BER
ALS
SD
IRALS
BCPD
DIAG-A
OALS
VB
14.8
14.9
15
15.1
15.2
0.04
0.045
0.05
0.055
(b) Bernoulli-Gaussian
0
2
5
1
0
1
5
SNR (dB)
10-7
10-6
10-5
10-4
10-3
10-2
10-1
100
BER
ALS
SD
IRALS
BCPD
DIAG-A
OALS
VB
14.8
14.9
15
15.1
0.22
0.24
0.26
0.28
0.3
0.32
(c) Bernoulli-Uniform
0
2
5
1
0
1
5
SNR (dB)
10-7
10-6
10-5
10-4
10-3
10-2
10-1
BER
ALS
SD
IRALS
BCPD
DIAG-A
OALS
VB
14.5
15
15.5
0.1
0.105
0.11
0.115
(d) Bernoulli-Studentâ€™s t
Fig. 7.5 BER versus SNR under different outlier models
min
U,V,{dr(k)}R
r=1
K

k=1
âˆ¥B(k)âˆ’Udiag{d1(k), . . ., dR(k)}VT âˆ¥2
F
s.t.
UHU = IR, VHV = IR.
(7.38)
Obviously, if there is only one image (i.e., K = 1), problem (7.38) is equivalent to
the well-studied SVD problem. Notice that the expression inside the Frobenius norm
in (7.38) can be written as B(k) âˆ’R
r=1 U:,r â—¦V:,rdr(k). Further introducing the
matrix D with its (k,r)th element being dr(k), it is easy to see that problem (7.38)
can be rewritten in tensor form as
min
U,V,D âˆ¥B âˆ’
R

r=1
U:,r â—¦V:,r â—¦D:,r



=[[U,V,D]]
âˆ¥2
F
s.t. UHU = IR, VHV = IR,
(7.39)
where B âˆˆCMÃ—ZÃ—K is a third-order tensor with B(k) as its kth slice. Therefore,
linear image coding for a collection of images is equivalent to solving a tensor CPD
with two orthonormal factor matrices.

7.4 Simulation Results and Discussions
153
Table 7.2 Classiï¬cation error and CPD computation time in face recognition
Algorithm
No outlier
Bernoulliâ€“Gaussian
Bernoulli-Uniform
Bernoulli-Studentâ€™s t
Classiï¬cation
error (%)
CPD
time (s)
Classiï¬cation
error (%)
CPD
time (s)
Classiï¬cation
error (%)
CPD
time (s)
Classiï¬cation
error (%)
CPD
time (s)
ALS
9
1.9635
51
46.3041
47
63.6765
48
58.1047
SD
12
0.5736
49
0.6287
44
0.6181
49
0.6594
IRALS
9
4.3527
43
15.8361
27
16.6151
36
17.5181
BCPD
2
4.1546
53
20.7338
35
17.9896
48
17.1151
DIAG-A
11
3.7384
50
28.9961
41
30.6317
51
22.5127
OALS
11
1.0174
58
40.2731
34
38.1754
47
24.0162
VB
2
2.4827
10
2.7806
6
2.4912
7
2.8895
We conduct experiments on 165 face images from the Yale Face Database2 [10],
representing different facial expressions (also with or without sunglasses) of 15
people (11 images for each person). In each classiï¬cation experiment, we randomly
pick two peopleâ€™s images. Among these 22 images, 12 (6 from each person) are
used for training. In particular, each image is of size 240 Ã— 320, and the training
data can be naturally represented by a third-order tensor Y âˆˆR240Ã—320Ã—12. Various
algorithms are run to learn the two orthogonal basis matrices.3 Then, the feature
vectors of these 12 training images, which are obtained by projecting them onto
the multi-linear subspaces spanned by the two orthogonal basis matrices, are used
to train a support vector machine (SVM) classiï¬er. For the 10 testing images, their
feature vectors are fed into the SVM classiï¬er to determine which person is in each
image. The parameters of various outlier models are Ï€ = 0.05, Ïƒe = 100, H = 100,
Î¼ = 1, Î» = 1/1000, and Î½ = 20.
Since the tensor rank is not known in the image data, it should be carefully chosen.
Forthealgorithms(ALS,SD,IRALS,DIAG-A,andOALS)thatcannotautomatically
determine the rank, it can be obtained by ï¬rst running these algorithms with tensor
rank ranges from 1 to 12 and then ï¬nding the knee point of the reconstruction error
decrement [5]. When there is no outlier, it is able to ï¬nd the appropriate tensor
rank. However, when outliers exist, the knee point cannot be found and we set the
rank as the upper bound 12. For the BCPD, although it learns the appropriate rank
when there are no outliers, it learns the rank as 12 when outliers exist. On the other
hand, no matter whether there are outliers or not, Algorithm 10 automatically learns
the appropriate tensor rank without exhaustive search and thus saves considerable
computational complexity.
The average classiï¬cation errors of 10 independent experiments and the corre-
sponding average computation times (benchmarked in Matlab on a personal com-
puter with an i7 CPU) are shown in Table7.2, and it can be seen that Algorithm 10
provides the smallest classiï¬cation error under all considered scenarios.
2 http://vision.ucsd.edu/content/yale-face-database.
3 Although the image data are real-valued and Algorithm 10 is derived for complex-valued data,
we directly use Algorithm 10 on image data without modiï¬cation.

154
7
Complex-Valued CPD, Orthogonality Constraint, and Beyond Gaussian Noises
References
1. N.D. Sidiropoulos, G.B. Giannakis, R. Bro, Blind PARAFAC receivers for DS-CDMA systems.
IEEE Trans. Signal Process. 48(3), 810â€“823 (2000)
2. M. Sorensen, L.D. Lathauwer, L. Deneire, PARAFAC with orthogonality in one mode and
applications in DS-CDMA systems, in Proceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP 2010), Dallas, Texas (2010), pp. 4142â€“4145
3. M. Sorensen, L.D. Lathauwer, P. Comon, S. Icart, L. Deneire, Canonical polyadic decom-
position with a columnwise orthonormal factor matrix. SIAM J. Matrix Anal. Appl. 33(4),
1190â€“1213 (2012)
4. Q. Zhao, L. Zhang, A. Cichocki, Bayesian CP factorization of incomplete tensors with auto-
matic rank determination. IEEE Trans. Pattern Anal. Mach. Intell. 37(9), 1751â€“1753 (2015)
5. K.P. Murphy, Machine Learning: A Probabilistic Perspective (MIT Press, Cambridge, 2012)
6. C.G. Khat, K.V. Mardia, The von Mises-Fisher distribution in orientation statistics. J. R. Stat.
Soc. 39, 95â€“106 (1977)
7. M. West, On scale mixtures of normal distributions. Biometrika 74(3), 646â€“648 (1987)
8. A.K. Gupta, D.K. Nagar, Matrix Variate Distributions (CRC Press, Boca Raton, 1999)
9. T. Kanamori, A. Takeda, Non-convex optimization on Stiefel manifold and applications to
machine learning, in Neural Information Processing (2012), pp. 109â€“116
10. A. Georghiades, D. Kriegman, P. Belhumeur, From few to many: generative models for recogni-
tion under variable pose and illumination. IEEE Trans. Pattern Anal. Mach. Intell. 40, 643â€“660
(2001)
11. M. SÃ¸rensen, D. Ignat, L.D. Lieven, Coupled canonical polyadic decompositions and (coupled)
decompositions in multilinear rank-(Lr, n, Lr, n, 1) termsâ€“Part II: Algorithms. SIAM J. Matrix
Anal. Appl. 36(3), 1015â€“1045 (2015)
12. X. Luciani, L. Albera, Canonical polyadic decomposition based on joint eigenvalue decompo-
sition. Chemom. Intell. Lab. Syst. 132, 152â€“167 (2014)
13. X. Fu, K. Huang, W.-K. Ma, N.D. Sidiropoulos, R. Bro, Joint tensor factorization and outlying
slab suppression with applications. IEEE Trans. Signal Process. 63(23), 6315â€“6328 (2015)
14. L. Cheng, Y.-C. Wu, H.V. Poor, Probabilistic tensor canonical polyadic decomposition with
orthogonal factors. IEEE Trans. Signal Process. 65(3), 663â€“676 (2017)
15. C.A.R. Fernandes, A.L.F. de Almeida, D.B. da Costa, Uniï¬ed tensor modeling for blind
receivers in multiuser uplink cooperative systems. IEEE Signal Process. Lett. 19(5), 247â€“250
(2012)
16. A.Y. Kibangou, A. De Almeida, Distributed PARAFAC based DS-CDMA blind receiver for
wireless sensor networks, in Proceedings of the IEEE International Conference on Signal
Processing Advances in Wireless Communications (SPAWC 2010), Marrakech, Jun. 20â€“23
(2010), pp. 1â€“5
17. A.L.F. de Almeida, A.Y. Kibangou, S. Miron, D.C. Araujo, Joint data and connection topology
recovery in collaborative wireless sensor networks, in Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP 2013), Vancouver, BC, May
26â€“31 (2013), pp. 5303â€“5307
18. B. Pesquet-Popescu, J.-C. Pesquet, A.P. Petropulu, Joint singular value decomposition - a
new tool for separable representation of images, in Proceedings of the IEEE International
Conference on Image Processing (ICIP 2001), Thessaloniki, Greece (2001), pp. 569â€“572
19. A. Shashua, A. Levin, Linear image coding for regression and classiï¬cation using the tensor-
rank principle, in Proceedings of the 2001 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (CVPR 2001), Kauai, Hawaii (2001), pp. 42â€“49

Chapter 8
Handling Missing Value: A Case Study
in Direction-of-Arrival Estimation
Abstract In previous chapters, the Bayesian CPDs are derived under fully observed
tensors. However, in practice, there are many scenarios where only part of the tensors
can be observed. This gives rise to the tensor completion problem. In this chapter,
we use subspace identiï¬cation for direction-of-arrival (DOA) estimation as a case
study to elucidate the key idea of the associated Bayesian modeling and inference in
data completion. In particular, we ï¬rstly introduce how DOA signal subspace recov-
ery is linked to tensor decomposition under missing data. Then, the corresponding
probabilistic model is established and the subsequent inference problem is solved by
using Theorem 2.1 in Chap.2.
8.1
Linking DOA Subspace Estimation to Tensor
Completion
Consider an arbitrary sensor array with a total of M sensors as seen in Fig.8.1, where
the mth sensor is located at coordinate (xm, ym, zm). There are R far-ï¬eld narrow-
band radiating sources impinging on this array. With the source at elevation angle Î¸r
and azimuth angle Ï†r transmitting a signal Î¾r(n) at the nth snapshot, the discrete-time
complex baseband signal received by the mth sensor is [1]
yxm,ym,zm(n) =
R

r=1
Î¾r(n)exp

j

xmur + ymvr + zm pr

+ wxm,ym,zm(n)
(8.1)
where ur = 2Ï€
Î»c sin Î¸r cos Ï†r, vr = 2Ï€
Î»c sin Î¸r sin Ï†r, and pr = 2Ï€
Î»c cos Î¸r with Î»c
being the wavelength of the carrier signal. It is assumed that the transmitted sig-
nal Î¾r(n) is a zero-mean wide-sense stationary random process with correlation
E

Î¾r(k)Î¾ âˆ—
râ€²(k + Ï„)

= Î´(r âˆ’râ€²)rr(Ï„),
and
the
additive
noise
wxm,ym,zm(n) âˆ¼
CN

wxm,ym,zm(n) | 0, Î²âˆ’1
is spatially and temporally independent.
The goal is to ï¬nd the DOA pairs {Î¸r, Ï†r}R
r=1 from the received signal
{yxm,ym,zm(n)}m,n, with no knowledge of the source number R, noise power Î²âˆ’1, and
the statistics of source signals {Î¾r(n)}r,n. Since the DOA parameters are
Â© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
L. Cheng et al., Bayesian Tensor Decomposition for Signal Processing and
Machine Learning, https://doi.org/10.1007/978-3-031-22438-6_8
155

156
8
Handling Missing Value: A Case Study in Direction-of-Arrival Estimation
x
y
z
Ï†
Î¸
(
(
Fig. 8.1 Multiple sources impinging on an arbitrary array (Â© [2019] IEEE. Reprinted, with permis-
sion, from [L. Cheng, C. Xing, and Y.-C. Wu, Irregular Array Manifold Aided Channel Estimation
in Massive MIMO Communications, IEEE Journal of Selected Topics in Signal Processing, Sep
2019]. It applies all ï¬gures and tables in this chapter)
nonlinearly coupled, it requires an exhaustive search if we employ direct optimiza-
tion, which however is computationally demanding. As an efï¬cient alternative, sub-
space methods ï¬rstly ï¬nd the subspace in which the DOA signals lie in, and then
the DOA parameters are extracted from the subspace structure, thus bypassing the
exhaustive search problem.
Although subspace-based DOA estimation is not new, this chapter treats the signal
obtained from the arbitrary array as data from a tensor with missing values. Not only
this treatment would lead to a better subspace recovery than its matrix counterpart,
but also tensor-based methods allow the subspaces in azimuth domain and elevation
domaintobeseparatelyestimated,thusfurtherreducingthecomplexityofsubsequent
DOA estimations [2]. More importantly, by viewing the subspace estimation problem
as a tensor completion problem, it does not require shift-invariant sub-structure of
the array. This leads to more general applicability than previous tensor subspace
methods [3â€“6], which all require a shift-invariant sub-structure of the array and are
inapplicable when the array shape is arbitrary.
To leverage the power of the tensor framework, we treat the arbitrary array as a
cuboid array with missing elements, based on which the subspace estimation problem
can be cast as a tensor completion problem. To construct the cuboid array, we project
the sensor elements onto the x-axis, y-axis, and z-axis, respectively, as shown in
Fig.8.2. In particular, let Sx denotes the set collecting the projected coordinates on
the x-axis, with repeated values eliminated and remaining values arranged from small
tolarge,thevalueof Sx(i1)isthei1th largestnumberin{xm}M
m=1.Similarly,sets Sy and
Sz collect the projected and ordered coordinates on the y-axis and z-axis, respectively.
Denoting the cardinality of sets |Sx| = I1, |Sy| = I2, and |Sz| = I3, the cuboid grid

8.1 Linking DOA Subspace Estimation to Tensor Completion
157
x
y
z
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
x
y
z
x
y
z
Fig. 8.2 The projected coordinates of sensors are connected to form a cuboid grid
is the collection of coordinates (x, y, z) such that x âˆˆSx, y âˆˆSy, and z âˆˆSz, and it
forms a 3D tensor with dimensions I1, I2, and I3. We denote this constructed tensor as
Y(n). For the signal yxm,ym,zm(n) received by the mth sensor, it is naturally assigned
to the tensor element Yi1,i2,i3(n) where the index (i1, i2, i3) satisï¬es Sx(i1) = xm,
Sy(i2) = ym, and Sz(i3) = zm. After assigning the data collected by all M sensors,
there is still a portion of tensor elements unknown in the constructed 3D tensor, where
the missing ratio is 1 âˆ’
M
I1I2I3 . With each unknown tensor element being assigned a
value zero, and using the data model (8.1), the constructed tensor Y(n) would contain
elements
Yi1,i2,i3(n) = Oi1,i2,i3(n) Ã—
	
R

r=1
Î¾l(n)exp

j

Sx(i1)ur + Sy(i2)vr + Sz(i3)pr

+ wSx(i1),Sy(i2),Sz(i3)(n)

(8.2)
where Oi1,i2,i3(n) equals one if data Yi1,i2,i3(n) is available, and zero otherwise.
Using the deï¬nition of tensor canonical polyadic decomposition (CPD), it is easy
to show that (8.2) can be expressed in a tensor form as
Y(n) = O(n) âŠ™
â›
âœâœâœâœâœâ
R

r=1
a(ur) â—¦a(vr) â—¦a(pr) â—¦Î¾r(n)



â‰œ[[A[u],A[v],A[ p],Î¾(n)]]
+W(n)
â
âŸâŸâŸâŸâŸâ 
,
(8.3)
where âŠ™and â—¦denote the Hadamard product and the outer product, respectively,
while O(n) is a tensor with (i1, i2, i3)th element given by Oi1,i2,i3(n). For the
noise tensor W(n), its (i1, i2, i3)th element is given by wSx(i1),Sy(i2),Sz(i3)(n) if data
Yi1,i2,i3(n) is available and zero otherwise. In (8.3), Î¾(n) = [Î¾1(n), Î¾2(n), . . ., Î¾R(n)].
We also deï¬ned A [u] âˆˆCI1Ã—R with the rth column being a(ur) = [exp( jur Sx(1)),
exp( jur Sx(2)), . . ., exp( jur Sx(I1))]T . Matrices A [v] âˆˆCI2Ã—R and A

p

âˆˆCI3Ã—R

158
8
Handling Missing Value: A Case Study in Direction-of-Arrival Estimation
are deï¬ned similarly but with the rth column being a(vr) = [exp( jvr Sy(1)),
exp( jvr Sy(2)), . . . , exp( jvr Sy(I2))]T and a(pr) = [exp( jpr Sz(1)), exp( jpr Sz(2)),
. . ., exp( jpr Sz(I3))]T , respectively.
After collecting I4 snapshots data along the time dimension, we have a fourth-
order data tensor Ë™Y âˆˆCI1Ã—I2Ã—I3Ã—I4. Using the tensor CPD deï¬nition, it is easy to
show the data model can be expressed as
Ë™Y = Ë™O âŠ™
â›
âœâœâœâœâœâ
R

r=1
a(ur) â—¦a(vr) â—¦a(pr) â—¦a(Î¾r)



â‰œ[[A[u],A[v],A[ p],A[Î¾]]]
+ Ë™W
â
âŸâŸâŸâŸâŸâ 
,
(8.4)
where the observation tensor Ë™O and noise tensor Ë™W are constructed in the same way
as data tensor Ë™Y, and matrix A

Î¾

âˆˆCI4Ã—R is constructed with the rth column being
a(Î¾r) = [Î¾r(1), . . . , Î¾r(I4)]T . Note that a more generalized model where A

Î¾

is
assumed to be rank-1 can be found in [11], but the subsequent modeling and inference
procedure are similar to what is presented in this chapter.
To recover the DOA subspaces from the tensor representation in (8.4), we have
the following property.
Property 8.1 If [[A [u] , A [v] , A[ p], A[Î¾]]] = [[(1), (2), (3), (4)]]
and
2 â‰¤R â‰¤min{I1, I2, I3, I4},
the
following
equations
hold:
(1) = A [u] (1),
(2) = A [v] (2),
(3) = A[ p](3),
and
(4) = A[Î¾](4) where  is a permutation matrix and 4
k=1 (k) = I R .
This property indicates that the columns of the factor matrix (1) span the same
column space as those of matrix A [u], since the permutation matrix  and diagonal
matrix (1) are invertible. Similarly, columns of (2) span the same space as that
of A [v], and columns of (3) span the same space as that of A

p

. Therefore, the
core problem of subspace estimation is to ï¬nd the factor matrices {(k)}4
k=1 under
unknown number of sources R, and the problem can be stated as
min
{(k)}4
k=1
Î² âˆ¥Ë™Y âˆ’Ë™O âŠ™[[(1), (2), (3), (4)]] âˆ¥2
F +
L

l=1
Î³l
	
4

k=1
(k)H
:,l
(k)
:,l

(8.5)
where L is the maximum possible value of the number of sources R.
This problem is known as complex-valued tensor completion problem, and it is
easy to show its non-convexity, since all the factor matrices {(k)}4
k=1 are coupled via
Khatriâ€“Rao products. Furthermore, the tensor rank acquisition is generally NP-hard
[7], due to its discrete nature. This issue can be remedied by adding a regularization

8.2 Probabilistic Modeling
159
term L
l=1 Î³l
	4
k=1 (k)H
:,l
(k)
:,l

in order to control the complexity of the model and
avoid overï¬tting of noise. However, determining the optimal regularization param-
eters {Î³l}L
l=1 is difï¬cult, and conventional methods usually rely on computationally
demanding search schemes [8]. Fortunately, we can build a probabilistic model, so
that the regularization parameters and the factor matrices can be learned from the VI
framework.
8.2
Probabilistic Modeling
To use the probabilistic framework, the optimization problem (8.5) needs to be inter-
preted using probabilistic language. In particular, the squared error term in problem
(8.5) can be interpreted as the negative log of a likelihood function given by
p
	
Ë™Y | {(n)}4
n=1, Î²

âˆexp

âˆ’Î²

i1,i2,i3,i4
Ë™Oi1,i2,i3,i4
 Ë™Yi1,i2,i3,i4 âˆ’[[(1), (2), (3), (4)]]i1,i2,i3,i4

2

.
(8.6)
On the other hand, the regularization term in problem (8.5) can be interpreted as
a zero-mean circularly symmetric complex Gaussian prior distribution over the
columns of the factor matrices, i.e.,
p({(k)}4
k=1|{Î³l}L
l=1) =
4

k=1
L

l=1
CN
	
(k)
:,l |mc, Î³ âˆ’1
l
I Ik

=
4

k=1
Ik

Îº=1
CN
	
(k)
Îº,:|mr, 

,
(8.7)
where mc = 0IkÃ—1, mr = 0LÃ—1,  = diag{Î³1, Î³2, . . ., Î³L}, and the latter part is an
equivalent expression with the unknown changed to the row of (k). In (8.7),
the inverse of the regularization parameter Î³ âˆ’1
l
has a physical interpretation of
the power of the lth column of various factor matrices. When power Î³ âˆ’1
l
goes
to zero, it indicates the corresponding columns in various factor matrices play no
role and can be pruned out. Since we know nothing about the regularization param-
eter Î³l and noise power Î²âˆ’1 before inference, non-informative gamma prior [9]
is imposed on them, i.e., p(Î²|Î±Î²) = gamma(Î²|10âˆ’6, 10âˆ’6) and p({Î³l}L
l=1|Î»Î³ ) =
L
l=1 gamma(Î³l|10âˆ’6, 10âˆ’6).
The complete probabilistic model is shown in Fig.8.3. Compared to the MPCEF
model in Chap.2, it is obvious that Î·(1) = {{(k)
Îº,:}Îº,k, Î²} are the unknown variables

160
8
Handling Missing Value: A Case Study in Direction-of-Arrival Estimation
Î²
{Î³l}
{Î(3)
i3,:}
{Î(2)
i2,:}
{Î(1)
i1,:}
{Î(4)
i4.:}
Î»Î³
Î±Î²
Ë™O
Ë™Y
mr
Fig. 8.3 Probabilistic model of tensor subspace estimation
in Layer 1 and Î·(2) = {Î³l}L
l=1 are the unknown variables in Layer 2. However, due to
complication induced by the missing values, we need to check whether this proba-
bilistic model lies within the MPCEF introduced in Chap.2.1
8.3
MPCEF Model Checking and Optimal Variational Pdfs
Derivations
8.3.1
MPCEF Model Checking
For variable (k)
Îº,: in Layer 1, we need to show that p
	
Ë™Y | (k)
Îº,:, {Î·(1)\(k)
Îº,:}

takes the form of (2.25). By expanding the square term in (8.6) using
[[A(1), A(2), . . ., A(N)]]i1,i2,...,iN = A(k)T
ik,:

Nâ‹„
n=1,nÌ¸=k A(n)T
in,:
T , we arrive at
1 MPCEF is deï¬ned for real-valued model. But we can easily extend the expression to complex-
valued.

8.3 MPCEF Model Checking and Optimal Variational Pdfs Derivations
161
p
	
Ë™Y|{(k)}4
k=1, Î²

= exp

Re
 
i1,...,i4
Ë™Oi1,...,i4 ln
 Î²
Ï€

âˆ’Î²

i1,...,i4
Ë™Oi1,...,i4

Ë™Yi1,...,i4 Ë™Yâˆ—
i1,...,i4
âˆ’2

Ë™Yâˆ—
i1,...,i4(k)T
Îº,:

4â‹„
n=1,nÌ¸=k (n)T
in,:
T

+ (k)T
Îº,:

4â‹„
n=1,nÌ¸=k (n)T
in,:
T 
4â‹„
p=1,pÌ¸=k (p)T
i p,:
âˆ—(k)âˆ—
Îº,:

.
(8.8)
By ï¬xing variables other than (k)
Îº,: in (8.8),
p
	
Ë™Y|(k)
Îº,: , {Î·(1)\(k)
Îº,: }

= exp

Re

2Î²

i1,...ik=Îº,...i4
Ë™Oi1,...,i4 Ë™Yâˆ—
i1,...,i4(k)T
Îº,:

4â‹„
n=1,nÌ¸=k (n)T
in,:
T
âˆ’vec
	
Î²

i1,...ik=Îº,...i4
Ë™Oi1,...,i4

4â‹„
n=1,nÌ¸=k (n)
in,:
T 
4â‹„
p=1,pÌ¸=k (p)
i p,:
âˆ—
H
vec
	
(k)âˆ—
Îº,: (k)T
Îº,:

âˆ’

i1,...ikÌ¸=Îº,...i4

2Î²

i1,...ik=Îº,...i4
Ë™Oi1,...,i4 Ë™Yâˆ—
i1,...,i4(k)T
ik,:

4â‹„
n=1,nÌ¸=k (n)T
in,:
T
âˆ’vec
	
Î²

i1,...ik=Îº,...i4
Ë™Oi1,...,i4

4â‹„
n=1,nÌ¸=k (n)
in,:
T 
4â‹„
p=1,pÌ¸=k (p)
i p,:
âˆ—
H
vec
	
(k)âˆ—
ik,: (k)T
ik,:


+

i1,...,i4
Ë™Oi1,...,i4 ln
 Î²
Ï€

âˆ’Î²

i1,...,i4
Ë™Oi1,...,i4 Ë™Yi1,...,i4 Ë™Yâˆ—
i1,...,i4

.
(8.9)
Comparing (8.9) to (2.25), we can see that t((k)
Îº,:) = [(k)âˆ—
Îº,: ; vec((k)âˆ—
Îº,: (k)T
Îº,: )],
n( Ë™Y, {Î·(1)\(k)
Îº,: }) =
â¡
â¢â¢â£
2Î² 
i1,...ik=Îº,...i4 Ë™Oi1,...,i4 Ë™Yâˆ—
i1,...,i4

4â‹„
n=1,nÌ¸=k (n)T
in,:
T
âˆ’vec
	
Î² 
i1,...ik=Îº,...i4 Ë™Oi1,...,i4

4â‹„
n=1,nÌ¸=k (n)
in,:
T 
4â‹„
p=1,pÌ¸=k (p)
i p,:
âˆ—
â¤
â¥â¥â¦.
(8.10)
Furthermore, the prior for (k)
Îº,: in (8.7) is in the form of (2.26)
p
	
(k)
Îº,: |mr, {Î³l}L
l=1

= exp

Re
 % 0LÃ—1
âˆ’vec()
&H '
(k)âˆ—
Îº,:
vec((k)âˆ—
Îº,: (k)T
Îº,: )
(
+
L

l=1
log Î³l âˆ’L log Ï€

,
(8.11)
where n(mr, {Î³l}L
l=1) = [0LÃ—1; âˆ’vec()].

162
8
Handling Missing Value: A Case Study in Direction-of-Arrival Estimation
Now, focusing on the variable Î² in Layer 1, (8.6) can be expressed as
p
	
Ë™Y|Î², {Î·(1)\Î²}

=exp

Re

âˆ’

i1,...,i4
Ë™Oi1,...,i4 ln Ï€ +

i1,...,i4
Ë™Oi1,...,i4 ln Î²
âˆ’Î²

i1,...,i4
Ë™Oi1,...,i4
 Ë™Yi1,...,i4 âˆ’[[(1), (2), (3), (4)]]i1,...,i4

2
, (8.12)
which is in the form of (2.25), where t(Î²) = [Î²; ln Î²],
n( Ë™Y, {Î·(1)\Î²}) =
â¡
â£âˆ’
i1,...,i4 Ë™Oi1,...,i4
 Ë™Yi1,...,i4 âˆ’[[(1), (2), (3), (4)]]i1,...,i4

2

i1,...,i4 Ë™Oi1,...,i4
â¤
â¦.
(8.13)
Furthermore, prior p(Î²|Î±Î²) = gamma(Î²|10âˆ’6, 10âˆ’6) is in the form of (2.26), since
p(Î²|Î±Î²) = exp
% âˆ’10âˆ’6
10âˆ’6 âˆ’1
&H % Î²
log Î²
&
+ 10âˆ’6 ln 10âˆ’6 âˆ’ln (10âˆ’6)

,
(8.14)
with n(Î±Î²) = [âˆ’10âˆ’6; 10âˆ’6 âˆ’1]. Therefore, Condition 1 is satisï¬ed.
For variable Î³l in Layer 2 of Fig.8.3, p({(k)}4
k=1|Î³l, {Î·(2)\Î³l}) is
p({(k)}4
k=1|Î³l, {Î·(2)\Î³l}) = exp

Re
 %âˆ’4
k=1 (k)H
:,l
(k)
:,l
4
k=1 Ik
&H % Î³l
ln Î³l
&
+
L

j=1, jÌ¸=l
	
âˆ’
4

k=1
(k)H
:, j
(k)
:, j

Î³ j +
4

k=1
Ik
L

j=1, jÌ¸=l
ln Î³ j âˆ’
4

k=1
Ik R ln Ï€

.
(8.15)
Ittakestheformof(2.27)ifwedeï¬nen({(k)}4
k=1, {Î·(2)\Î³l}) = [âˆ’4
k=1 (k)H
:,l
(k)
:,l ;
4
k=1 Ik] and t(Î³l) = [Î³l; ln Î³l]. On the other hand, the prior of Î³l takes the form
p(Î³l|Î»Î³ ) = exp
â›
â
'
âˆ’10âˆ’6
10âˆ’6 âˆ’1
(H '
Î³l
log Î³l
(
+ 10âˆ’6 ln 10âˆ’6 âˆ’ln (10âˆ’6)
â
â , (8.16)
which is consistent with (2.28) if we deï¬ne n(Î»Î³ ) = [âˆ’10âˆ’6; 10âˆ’6 âˆ’1]. Therefore,
Condition 2 is veriï¬ed. Finally, variables {Î±Î², Î»Î³ , mr, Ë™O} are known quantities, and
thus Condition 3 holds. To summarize, the Bayesian tensor completion model is in
MPCEF.

8.3 MPCEF Model Checking and Optimal Variational Pdfs Derivations
163
8.3.2
Optimal Variational Pdfs Derivations
As shown above, the probabilistic model for tensor completion satisï¬es Conditions
1â€“3 of MPCEF, and thus the optimal variational pdfs can be directly obtained using
Theorem 2.1 in Chap.2. More speciï¬cally, the optimal variational pdf Qâˆ—((k)
Îº,:) can
be calculated as
Qâˆ—((k)
Îº,: )âˆexp

Re

E
Î¸ j Ì¸=(k)
Îº,:
'
n( Ë™Y, Î·(1)\(k)
Îº,: )1 + 0LÃ—1
n( Ë™Y, Î·(1)\(k)
Îº,: )2 âˆ’vec()
(H '
(k)âˆ—
Îº,:
vec((k)âˆ—
Îº,: (k)T
Îº,: )
( 
.
(8.17)
Its functional form coincides with the complex Gaussian distribution CN((k)
Îº,:|
M(k)
Îº,:, (k)
Îº ), where
(k)
Îº
=
â¡
â£EQ(Î³l )[] + EQ(Î²)[Î²]

i1,...ik=Îº,...i4
Ë™Oi1,...,i4E
n Q((n))
)
4â‹„
n=1,nÌ¸=k (n)T
in,:
T 
4â‹„
p=1,pÌ¸=k (p)T
i p,:
âˆ—*
â¤
â¦
âˆ’1
,
(8.18)
M(k)
Îº,: = (k)
Îº

i1,...ik=Îº,...i4
EQ(Î²)[Î²] Ë™Oi1,...,i4 Ë™Yi1,...,i4

4â‹„
n=1,nÌ¸=k E
Q((n)
in,:)[(n)T
in,: ]
T .
(8.19)
Similarly, the optimal variational pdf Qâˆ—(Î²) is derived to be
Qâˆ—(Î²)âˆexp

Re

E
Î¸ j Ì¸=Î²
'
n( Ë™Y, Î·(1)\Î²)1 âˆ’10âˆ’6
n( Ë™Y, Î·(1)\Î²)2 + 10âˆ’6 âˆ’1
(H '
Î²
ln Î²
( 
,
(8.20)
which is a gamma distribution gamma(Î²|c, d) with
c = 10âˆ’6 +

i1,...,i4
Ë™Oi1,...,i4,
(8.21)
d = 10âˆ’6 + E
n Q((n))
' 
i1,...,i4
Ë™Oi1,...,i4
 Ë™Yi1,...,i4 âˆ’[[(1), (2), (3), (4)]]i1,...,i4

2
(
.
(8.22)
For variable Î³l, its optimal variational pdf Qâˆ—(Î³l) is
Qâˆ—(Î³l)âˆexp

Re

E
Î¸ j Ì¸=Î³l
'
âˆ’4
k=1 (k)H
:,l
(k)
:,l âˆ’10âˆ’6
4
k=1 Ik + 10âˆ’6 âˆ’1
(H % Î³l
ln Î³l
& 
.
(8.23)

164
8
Handling Missing Value: A Case Study in Direction-of-Arrival Estimation
It can be seen that Qâˆ—(Î³l) follows a gamma distribution gamma(Î²|al, bl), where
al = 10âˆ’6 +
4

k=1
Ik
(8.24)
bl = 10âˆ’6 +
4

k=1
E
n Q((n))
)
(k)H
:,l
(k)
:,l
*
.
(8.25)
In (8.17)â€“(8.25), there are several expectations to be computed. Simple
expectations EQ((k)
Îº,: )[(k)
Îº,:], EQ(Î³l)[Î³l], and EQ(Î²)[Î²] can be obtained using results
presented
in
previous
chapters.
However,
there
are
two
expectations
E
n Q((n))
)
4â‹„
n=1,nÌ¸=k (n)T
in,:
T

4â‹„
p=1,pÌ¸=k (p)T
i p,:
âˆ—*
and E
n Q((n))
) Ë™Yi1,...,i4 âˆ’[[(1),
(2), (3), (4)]]i1,...,i4

2*
that are more difï¬cult to compute. By Property 6.2, they
are derived as
E
n Q((n))
)
4â‹„
n=1,nÌ¸=k (n)T
in,:
T 
4â‹„
p=1,pÌ¸=k (p)T
i p,:
âˆ—*
=
4âŠ™
n=1,nÌ¸=k
)
M(n)
in,:M(n)H
in,:
+ (n)âˆ—
in
*
,
(8.26)
E
n Q((n))
% Ë™Yi1,...,i4 âˆ’[[(1), (2), (3), (4)]]i1,...,i4

2&
=
 Ë™Yi1,...,i4

2
âˆ’2Re

Ë™Yâˆ—
i1,...,i4 M(1)T
i1,:

4â‹„
n=2 M(n)T
in,:
T

+ Tr
	 )
M(1)
i1,:M(1)H
i1,:
+ (1)
i1
*
4âŠ™
n=2
)
M(n)
in,:M(n)H
in,:
+ (n)âˆ—
in
* 
.
(8.27)
8.4
Algorithm Summary and Remarks
The variational inference algorithm for the proposed model is summarized in
Algorithm 11. After convergence, some EQâˆ—(Î³l)[Î³l] = at
l /bt
l will be very large, e.g.,
106, indicating that the power of corresponding columns goes to zero. Then, these
columns can be safely pruned out, and the remaining column number is the estimate
of the path number R. Meanwhile, since Qâˆ—((k)
Îº,:) is Gaussian distribution, factor
matrices {(k)}4
k=1 are estimated by the {M(k,t)}4
k=1 at convergence.
According to Property 8.1, the subspaces spanned by the columns in A[u], A[v],
and A[ p] are estimated by the range spaces of M(1,t), M(2,t), and M(3,t), respec-
tively. Thereby, 1D subspace-based DOA estimation methods, such as multiple signal
classiï¬cation (MUSIC) and estimation of signal parameters via rotation invariance
technique (ESPRIT), can be applied to the range spaces of {M(k,t)}3
k=1 to separately
estimate the DOAs Î¸r and Ï†r. Details of applying 1D DOA methods in 2D DOA
estimation can be found in [2].

8.5 Simulation Results and Discussions
165
Algorithm 11 Probabilistic Tensor CPD for DOA Estimation
Initialization: Choose L > R, and initial values {M(k,0) âˆˆCIkÃ—L, {(k,0)
Îº
âˆˆCLÃ—L}Ik
Îº=1}4
k=1. Let
a0
l = 10âˆ’6, b0
l = 10âˆ’6 for l = 1, 2, 3, ..., L; c0 = d0 = 10âˆ’6.
Iterations: For the tth iteration (t â‰¥1),
Update the parameter of each Qâˆ—((k)
Îº,: )t:
(k,t)
Îº
=
% ctâˆ’1
dtâˆ’1

i1,...,ik=Îº,...i4
Ë™Oi1,...,i4
4âŠ™
n=1,nÌ¸=k
)
M(n,tâˆ’1)
in,:
M(n,tâˆ’1)H
in,:
+ (n,tâˆ’1)âˆ—
in
*
+diag
+
atâˆ’1
1
btâˆ’1
1
,. . .,
atâˆ’1
L
btâˆ’1
L
,&âˆ’1
,
(8.28)
M(k,t)
Îº,:
= ctâˆ’1
dtâˆ’1 (k,t)
Îº

i1,...ik=Îº,...i4

Ë™Oi1,...,i4 Ë™Yi1,...,i4

4â‹„
n=1,nÌ¸=k M(n,tâˆ’1)T
in,:
T 
,
(8.29)
Notice that for each k âˆˆ{1, 2, 3, 4}, the update for Îº = 1, 2, . . . , Ik can be updated in parallel.
Update the parameter of Qâˆ—({Î³l}L
l=1)t:
at
l = 10âˆ’6 +
4

k=1
Ik,
(8.30)
bt
l = 10âˆ’6 +
4

k=1
Ik

Îº=1
M(k,t)
Îº,l

2
+
)
(k,t)
Îº
*
l,l .
(8.31)
Update the parameter of Qâˆ—(Î²)t:
ct = 10âˆ’6 +

i1,...,i4
Ë™Oi1,...,i4,
(8.32)
dt = 10âˆ’6 +

i1,...,i4
Ë™Oi1,...,i4
 Ë™Yi1,...,i4

2
âˆ’2Re

Ë™Yâˆ—
i1,...,i4 M(1,t)T
i1,:

4â‹„
n=2 M(n,t)T
in,:
T

+ Tr
	 )
M(1,t)
i1,: M(1,t)H
i1,:
+ (1,t)
i1
*
4âŠ™
n=2
)
M(n,t)
in,: M(n,t)H
in,:
+ (n,t)âˆ—
in
* 

.
(8.33)
Until Convergence
8.5
Simulation Results and Discussions
In this subsection, numerical results are presented to assess the performance of the
proposed method (labeled as VI) for subspace estimation over I4 = 100 snapshots.
The arbitrary array was generated by randomly deploying M sensor elements so that
the projected coordinates form a 3D grid with dimensions I1 = 8, I2 = 10, I3 = 12
and the inter-grid spacing dx = Î»c/2, dy = Î»c/4, dz = Î»c/8. We consider two sce-
narios for the arbitrary array: M = 288 and M = 576, corresponding to Ï€ = 0.3
and Ï€ = 0.6 of the grid points being occupied by sensors, respectively. There are 3
sources with elevation DOAs {15â—¦, 25â—¦, 120â—¦} and azimuth DOAs {âˆ’50â—¦, 10â—¦, 70â—¦},
respectively. The transmitted signal of each source Î¾r(n) is drawn from a zero-
mean circularly symmetric complex Gaussian distribution with unit variance, and

166
8
Handling Missing Value: A Case Study in Direction-of-Arrival Estimation
Fig. 8.4 Percentage of correct rank estimates
without any correlation across r and n. The signal-to-noise ratio (SNR) is deï¬ned
as 10 log10
	
âˆ¥Ë™OâŠ™[[A[u],A[v],A[ p],A[Î¾]]]âˆ¥2
F
âˆ¥Wâˆ¥2
F

. For Algorithm 11, initial mean M(k,0) for
each matrix (k) is set as the singular value decomposition (SVD) approximation
U:,1:R

S1:R,1:R
 1
2 where [U, S, V] = SVD[[ Ë™Y](k)], L = max{I1, I2, I3, I4}. The ini-
tial covariance matrix (k,0)
Îº
= I LÃ—L. Each point in the following ï¬gures is an aver-
age of 1000 Monte Carlo runs with different realizations of the arbitrary arrays,
signals, and noises.
To access the ability of learning the number of sources, the subspace rank learned
by the proposed algorithm is shown in Fig.8.4, with each vertical bar showing the
percentages of correct estimates. From Fig.8.4, it is seen that Algorithm 11 can
recover the true tensor rank with 100% accuracy for a wide range of SNRs. Notice
that Algorithm 11 can only recover the correct tensor rank with 100% accuracy in
moderate and high-SNR regions, and particularly in the region SNR > 5 dB for this
application.
Finally, the performance of DOA subspace estimation in terms of the
averaged largest principal angle
1
3

LPA(M(1), A[u]) + LPA(M(2), A[v]) + LPA
(M(3), A[ p])

is shown in Fig.8.5. LPA(A, B) is a measure of the â€œdistanceâ€
between two subspaces spanned by the columns of matrices A and B, deï¬ned as
cosâˆ’1{Ïƒmin{orth{A}H orth{B}}}, where the operator Ïƒmin{ Q} denotes the smallest
singular value of the matrix Q and orth{ Q} is an orthonormal basis for the subspace
spanned by the columns of Q. The range of LPA is from 0â—¦to 90â—¦, where LPA

References
167
Fig. 8.5 Averaged largest principal angle (LPA) of subspace estimation
0â—¦means that the two subspaces are identical while LPA 90â—¦means that the two
subspaces are orthogonal.
From Fig.8.5, it is seen that Algorithm 11 gives averaged LPA less than 5â—¦when
SNR is larger than 5dB, no matter Ï€ = 0.3 or Ï€ = 0.6. It shows that Algorithm
11 gives relatively good subspace estimations. On the other hand, for comparison,
a recently proposed alternating least square (ALS)-based tensor completion method
[10] (labeled as ALS) is also simulated. It assumes knowing the exact tensor rank
R = 3, but takes the same initial estimates of factor matrices and termination crite-
rion as that in Algorithm 11. From Fig.8.5, it is apparent that the ALS-based algo-
rithm performs almost indistinguishably from Algorithm 11. However, this result is
obtained with the ALS-based algorithm knowing the tensor rank, while Algorithm
11 without knowing the tensor rank. This shows that Algorithm 11 supersedes the
existing algorithm [10] by providing additional rank learning ability.
References
1. Van Trees, Detection, Estimation, and Modulation Theory, Optimum Array Processing (Wiley,
New York, 2004)
2. L. Cheng, Y.-C. Wu, J. (Charlie) Zhang, L. Liu, Subspace identiï¬cation for DOA estimation in
massive/full-dimension mimo system: bad data mitigation and automatic source enumeration.
IEEE Trans. Signal Process. 63(22), 5897â€“5909 (2015)

168
8
Handling Missing Value: A Case Study in Direction-of-Arrival Estimation
3. M. Haardt, F. Roemer, G.D. Galdo, Higher-order SVD based subspace estimation to improve
the parameter estimation accuracy in multi-dimensional harmonic retrieval problems. IEEE
Trans. Signal Process. 56(7), 3198â€“3213 (2008)
4. F. Roemer, M. Haardt, G.D. Galdo, Analytical performance assessment of multi-dimensional
matrix-and tensor-based ESPRIT-type algorithms. IEEE Trans. Signal Process. 62(10), 2611â€“
2625 (2014)
5. W. Sun, H.C. So, F.K.W. Chan, L. Huang, Tensor approach for eigenvector-based multi-
dimensional harmonic retrieval. IEEE Trans. Signal Process. 61(13), 3378â€“3388 (2013)
6. X. Guo, S. Miron, D. Brie, S. Zhu, X. Liao, A CANDECOMP/PARAFAC perspective on
uniqueness of DOA estimation using a vector sensor array. IEEE Trans. Signal Process. 59(9),
3475â€“3481 (2011)
7. Q. Zhao, L. Zhang, A. Cichocki, Bayesian CP factorization of incomplete tensors with auto-
matic rank determination. IEEE Trans. Pattern Anal. Mach. Intell. 37(9), 1751â€“1753 (2015)
8. P.C. Hansen, Rank Deï¬cient and Discrete Ill-Posed Problems-Numerical Aspects of Linear
Inversion (SIAM, Philadelphia, PA, 1998)
9. K.P. Murphy, Machine Learning: A Probabilistic Perspective (MIT Press, Cambridge, 2012)
10. L. Karlsson, D. Kressner, A. Uschmajew, Parallel algorithms for tensor completion in the CP
format. Parallel Comput. 57, 222â€“234 (2016)
11. L. Cheng, C. Xing, Y.-C. Wu, Irregular array manifold aided channel estimation in massive
MIMO communications. IEEE J. Selected Topics Signal Process. 13(5), 974â€“988 (2019)

Chapter 9
From CPD to Other Tensor
Decompositions
Abstract In previous chapters, we have introduced tensor rank learning using GSM
priors for tensor CPD and its extensions to scenarios where additional prior infor-
mation exists or the data structure is altered. In this chapter, we present tensor rank
learning for other tensor decomposition formats. It turns out that what has been pre-
sented for CPD is instrumental for other Bayesian tensor modelings, as they share
many common characteristics.
9.1
Tucker Decomposition (TuckerD)
Tucker decomposition (TuckerD) is introduced in Chap.1,
X = G Ã—1 U(1) Ã—2 U(2) Ã—3 Â· Â· Â· Ã—N U(N),
(9.1)
where X âˆˆRI1Ã—Â·Â·Â·Ã—IN is the target tensor, G âˆˆRR1Ã—Â·Â·Â·Ã—RN is a core tensor, and
{U(n) âˆˆRInÃ—Rn}N
n=1 are factor matrices. The tuple (R1, . . . , RN) is known as multi-
linear rank. A graphical illustration of Tucker format is provided in Fig.1.6 and
its deterministic algorithm is summarized in Algorithm 2. However, Algorithm 2
requires the knowledge of the multi-linear rank, which is typically unknown in
practice.
To avoid extensive multi-linear rank tuning, a probabilistic approach using
Gaussian-gamma prior is developed in [1]. Notice that rank Rn is the column num-
ber of factor matrix U(n). Following the general philosophy introduced in Sect.3.1,
GSM priors can be placed on the columns of U(n) to learn rank Rn. In particular, the
Gaussian-gamma prior is chosen in [1]. Given the multi-linear rank upper bounds
{Ln}N
n=1, the Gaussian-gamma prior for the factor matrices {U(n) âˆˆRInÃ—Ln}N
n=1 is
p({U(n)}N
n=1|{Î»(n)}N
n=1) =
N

n=1
p({U(n)}N
n=1|Î»(n)) =
N

n=1
Ln

ln=1
N(U(n)
:,ln|0InÃ—1, (Î»(n)
ln )âˆ’1IIn),
(9.2)
Â© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
L. Cheng et al., Bayesian Tensor Decomposition for Signal Processing and
Machine Learning, https://doi.org/10.1007/978-3-031-22438-6_9
169

170
9
From CPD to Other Tensor Decompositions
p({Î»(n)}N
n=1|aÎ»
0 , bÎ»
0) =
N

n=1
p(Î»(n)|aÎ»
0 , bÎ»
0) =
N

n=1
L

l=1
gamma(Î»(n)
ln |aÎ»
0 , bÎ»
0),
(9.3)
where Î»(n) âˆˆRLÃ—1 collects precisions of columns in factor matrix U(n) and aÎ»
0 , bÎ»
0 are
pre-determined hyper-parameters shared by all random vector {Î»(n)}N
n=1. Compared
to Bayesian CPD modeling, Bayesian Tucker has a distinct vector Î»(n) for each factor
matrix. If certain precision Î»(n)
ln is learned to be a very large value, it indicates that
the corresponding column U(n)
:,ln is zero and thus tensor rank learning is achieved in
each factor matrix (i.e., multi-linear rank).
While an independent sparsity-promoting prior can be placed on the core tensor
G, it ignores the relation between the core tensor G and the factor matrices {U(n) âˆˆ
RInÃ—Rn}N
n=1. To see this, we rewrite TuckerD (9.1) as
X =
R1

r1=1
R2

r2=1
Â· Â· Â·
RN

rN =1
Gr1,r2,...,rN U(1)
:,r1 â—¦U(2)
:,r2 â—¦Â· Â· Â· â—¦U(N)
:,rN ,
(9.4)
which is a weighted sum of rank-1 tensors, where each rank-1 tensor is U(1)
:,r1 â—¦
U(2)
:,r2 â—¦Â· Â· Â· â—¦U(N)
:,rN with coefï¬cient Gr1,r2,...,rN . Equation(9.4) demonstrates the rela-
tion between the core tensor G and factor matrices {U(n)}N
n=1. That is, once U(n)
:,ln
is determined to be zero, the corresponding core tensor slice G...,ln,... can also be
enforced to zero. To model such a relation, a Gaussian-gamma prior is placed on
each element of the core tensor as [1]
p(G|{Î»(n)}N
n=1, Î²) =
L1

l1=1
Â· Â· Â·
L N

lN =1
N

Gl1,l2,...,lN |0,

Î²
N

n=1
Î»(n)
ln
âˆ’1
.
(9.5)
Notice that the precision of a particular tensor core element includes a product of
precisions N
n=1 Î»(n)
ln . Therefore, if some Î»(n)
ln
are learned to be a very large value,
the core tensor element is also zero. In (9.5), Î² is a scale parameter related to the
magnitude of G and its prior is a gamma distribution gamma(Î²|aÎ²
0 , bÎ²
0 ).
Since it is assumed that the observation noise is Gaussian, the likelihood function
is speciï¬ed as
p(Y|G, {U(n)}N
n=1, Ï„) âˆexp

âˆ’Ï„
2âˆ¥Y âˆ’G Ã—1 U(1) Ã—2 U(2) Ã—3 Â· Â· Â· Ã—N U(N)âˆ¥2
F

,
(9.6)
where Ï„ is the noise precisions and it is assigned with a Gamma prior distribution
gamma(Ï„|aÏ„
0, bÏ„
0). To summarize, the joint distribution of tensor data Y and model
parameters  = {{U(n)}N
n=1, G, {Î»(n)}N
n=1, Î², Ï„} is

9.2 Tensor Train Decomposition (TTD)
171
p(Y, ) = p(Y|G, {U(n)}N
n=1, Ï„)p({U(n)}N
n=1|{Î»(n)}N
n=1)p(G|{Î»(n)}N
n=1, Î²)
Ã— p({Î»(n)}N
n=1|aÎ»
0 , bÎ»
0)p(Î²|aÎ²
0 , bÎ²
0 )p(Ï„|aÏ„
0, bÏ„
0).
(9.7)
The mean-ï¬eld variational inference (MF-VI) algorithm is derived in [1] to learn
model parameters  from tensor data Y. The mean ï¬eld employed is Q() =
Q(G)Q(Î²) N
n=1 Q(U(n)) N
n=1 Q(Î»(n))Q(Ï„), and the resultant Bayesian algorithm
is summarized in Algorithm 12. Interested readers can refer to [1] for more details.
9.2
Tensor Train Decomposition (TTD)
Tensor Train Decomposition (TTD) decomposes tensor data X âˆˆRI1Ã—Â·Â·Â·Ã—IN into a
set of core tensors {G(n) âˆˆRRnÃ—InÃ—Rn+1}N
n=1, where each element is in the form
Xi1,...,iN = G(1)
:,i1,: Ã— Â· Â· Â· Ã— G(N)
:,iN ,:.
(9.8)
The tuple (R1, . . . , RN+1) is termed as TT-rank. While a deterministic algorithm for
TTD is summarized in Algorithm 3, it requires the user to specify the prescribed
accuracy, which controls the TT-rank.
To bypass tuning of the prescribed accuracy to obtain satisfactory performance,
[2] formally introduces a novel Gaussian-product-gamma prior to model sparsity
in TTD. In particular, assuming the TT-rank upper bound is (1, L2, . . . , L N, 1) (in
(9.8), each Xi1,...,iN is a scalar, and thus R1 and RN+1 are both required to be 1), the
Gaussian-product-gamma prior for core tensor G(n) is
p(G(n)|Î»(n), Î»(n+1)) =
Ln

k=1
Ln+1

l=1
N(G(n)
k,:,l|0InÃ—1, (Î»(n)
k Î»(n+1)
l
)âˆ’1IIn), âˆ€n âˆˆ{1, . . . , N},
(9.9)
p(Î»(n)|Î±(n), Î²(n)) =
Ln

k=1
gamma(Î»(n)
k |Î±(n)
k , Î²(n)
k ), âˆ€n âˆˆ{2, . . . , N},
(9.10)
where Î»(n) = [Î»(n)
1 , . . . , Î»(n)
Ln ], âˆ€n âˆˆ{2, . . . , N}, and Î»(1), Î»(N+1) are set as 1. Î±(n) âˆˆ
RLn, Î²(n) âˆˆRLn are pre-determined hyper-parameters of the gamma distributions.
From (9.9), if either Î»(n)
k
or Î»(n+1)
l
is learned to be a very large value, G(n)
k,:,l is zero.
It is theoretically proved in [2] that such prior could lead to sparsity in TT slices.
Interested readers can refer to [2] for more details of the Gaussian-product-gamma
prior. Notice that (9.9) bears some resemblance with the core tensor modeling (9.5)
in TuckerD, where the precesions of different factor matrix columns are also coupled
together.

172
9
From CPD to Other Tensor Decompositions
Algorithm 12 Probabilistic TuckerD [1]
Initializations: Choose Ln > Rn, âˆ€n and initial values vec(	G), G, {	U(n), (n)}N
n=1, aÎ²
M, bÎ²
M,
{Ëœa(n)
ln , Ëœb(n)
ln }Ln,N
ln=1,n=1, aÏ„
M, bÏ„
M.
Iterations:
Update G
The optimal variational pdf of G is a Gaussian distribution Q(G) = N

vec(G) | vec(	G), G

with
its covariance and mean as
vec(	G) = E[Ï„]G

âŠ™
n E

U(n)T 
vec(Y),
(9.11)
G =

E[Î²] âŠ™
n E

(n)
+ E[Ï„] âŠ™
n E

U(n)T U(n)âˆ’1
.
(9.12)
Update U(n) from n = 1 to N
The optimal variational pdf of U(n) is Q

U(n)
= In
in=1 N

U(n)
in,: | 	U(n)
in,:, (n)
, where
	U(n) = E[Ï„]Y(n)

âŠ™
kÌ¸=n E

U(k)
E

GT
(n)

(n),
(9.13)
(n) =

E

(n)
+ E[Ï„]E

G(n)

âŠ™
kÌ¸=n U(k)T U(k)

GT
(n)
âˆ’1
.
(9.14)
Update Î²
The optimal variational pdf of Î² is Q(Î²) = gamma

aÎ²
M, bÎ²
M

with parameters
aÎ²
M = aÎ²
0 + 1
2

n
Ln,
(9.15)
bÎ²
M = bÎ²
0 + 1
2E

vec

G2T 
âŠ™
n E

Î»(n)
.
(9.16)
Update Î»(n) from n = 1 to N
The optimal variational pdf of Î»(n) is Q

Î»(n)
= Ln
ln=1 gamma

Î»(n)
ln
| Ëœa(n)
ln , Ëœb(n)
ln

with parameters
Ëœa(n)
ln
= aÎ»
0 + 1
2
â›
âIn +

kÌ¸=n
Lk
â
â ,
(9.17)
Ëœb(n)
ln
= bÎ»
0 + 1
2E

u(n)T
Â·ln
u(n)
Â·ln

+ 1
2E[Î²]E

vec

G2
Â·Â·Â·ln . . .
T 
âŠ™
kÌ¸=n E

Î»(k)
.
(9.18)
Update Ï„
The optimal variational pdf of Ï„ is Q(Ï„) = gamma

aÏ„
M, bÏ„
M

with parameters
aÏ„
M = aÏ„
0 + 1
2

n
In,
(9.19)
bÏ„
M = bÏ„
0 + 1
2E
vec(Y) âˆ’

âŠ™
n U(n)

vec(G)

2
F

.
(9.20)
Until Convergence

9.3 PARAFAC2
173
The likelihood function is speciï¬ed as
p(Y|{G(n)}N
n=1, Ï„) âˆexp
â›
ââˆ’Ï„
2
I1

i1=1
Â· Â· Â·
IN

in=1
(Yi1,...,iN âˆ’G(1)
:,i1,: Ã— Â· Â· Â· Ã— G(N)
:,iN ,:)2
â
â ,
(9.21)
where Ï„ is the noise precesion and it is assigned with a Gamma prior distribution
gamma(Ï„|aÏ„
0, bÏ„
0). The complete probabilistic model is summarized in the joint dis-
tribution of tensor data Y and model parameters  = {{G(n)}N
n=1, {Î»(n)}N
n=2, Ï„},
p(Y, ) = p(Y|{G(n)}N
n=1, Ï„)
N

n=1
p(G(n)|Î»(n), Î»(n+1))
N

n=2
p(Î»(n)|Î±(n), Î²(n))p(Ï„).
(9.22)
A MF-VI algorithm is derived for the probabilistic model (9.22) under the mean ï¬eld
Q() = Q(Ï„) N+1
n=1 Q(Î»(n)) N
n=1
Ln
k=1
Ln+1
â„“=1 Q(G(n)
k,:,â„“) in [2] and is summarized
in Algorithm 13. A similar model and algorithm for tensor ring, which is a variant
of TT, is also reported in [3].
9.3
PARAFAC2
Parallel factor analysis 2 (PARAFAC2), which was ï¬rstly introduced in [4, 5], has
recently gained increasing interest due to its effectiveness in analyzing irregular
tensor data [6â€“16]. In particular, given an irregular tensor data Y = {Yk âˆˆRIÃ—Jk}K
k=1,
in which each tensor slice Yk has a different column number Jk, PARAFAC2 seeks for
rank-R factor matrices {U(1) âˆˆRIÃ—R, U(3) âˆˆRKÃ—R, {Fk âˆˆRJkÃ—R}K
k=1} via solving
the following problem [5]:
min
U(1),U(3),{Fk}K
k=1
K

k=1
âˆ¥Yk âˆ’U(1)diag(U(3)
k,: )FT
k âˆ¥2
F,
s.t. FT
i Fi = FT
j F j,
âˆ€i, j âˆˆ{1, . . . , K}.
(9.23)
Notice that in problem (9.23), each tensor slice Yk âˆˆRIÃ—Jk is decomposed into a
common set of factor matrices {U(1) âˆˆRIÃ—R, U(3) âˆˆRKÃ—R} while having a dedicated
factor matrix Fk âˆˆRJkÃ—R for the kth slice.

174
9
From CPD to Other Tensor Decompositions
Algorithm 13 Probabilistic Tensor Train [2]
Initializations: Choose Ln > Rn, âˆ€n and initial values { Ë†Î±(n)
k , Ë†Î²
(n)
k }Ln,N
k=1,n=2, Ë†Î±Ï„, Ë†Î²Ï„.
Iterations:
Update G(n) from n = 1 to N
The variational update of the TT core ï¬bers follows a Gaussian distribution Q(G(n)
k,:,â„“) =
In
in=1 N(Î¼G(n)
k,in,â„“, Ï…G(n)
k,in,â„“), with its variance and mean as
Ï…G(n)
k,in,â„“=

E[Ï„]

i1,i2,...,iN \in
E[b(<n)
(kâˆ’1)Ln+k]E[b(>n)
(â„“âˆ’1)Ln+1+â„“] + E[Î»(n)
k ]E[Î»(n+1)
â„“
]
âˆ’1
,
(9.24)
Î¼G(n)
k,in ,â„“=Ï…G(n)
k,in ,â„“E[Ï„]

i1,i2,...,iN \in

Yi1i2...iN E[t(<n)
k
]E[t(>n)
â„“
]
âˆ’
Ln

kâ€²=1
kâ€²Ì¸=k
Ln+1

â„“â€²=1
â„“â€²Ì¸=â„“
E[b(<n)
(kâˆ’1)Ln+kâ€²]E[G(n)
kâ€²,in,â„“â€²]E[b(>n)
(â„“âˆ’1)Ln+1+â„“â€²]

,
(9.25)
in which the following notations are adopted to make the expression more concise:
E[t(<n)] =
nâˆ’1

d=1
E[G(d)
:,id,:],
(9.26)
E[b(<n)] =
nâˆ’1

d=1
E[G(d)
:,id,: âŠ—G(d)
:,id,:],
(9.27)
and E[t(>n)] and E[b(>n)] are deï¬ned similarly.
Update Î»(n) from n = 2 to N
The variational distribution of Î»(n)
k
is Q(Î»(n)
k ) = gamma( Ë†Î±(n)
k , Ë†Î²
(n)
k ), with
Ë†Î±(n)
k
= InLn+1
2
+ Inâˆ’1Lnâˆ’1
2
+ Î±(n)
k ,
(9.28)
Ë†Î²
(n)
k
= 1
2
In

in=1
Ln+1

â„“=1
(E[G(n)
k,in,â„“
2]E[Î»(n+1)
â„“
]) + 1
2
Inâˆ’1

inâˆ’1=1
Lnâˆ’1

â„“â€²=1
(E[G(nâˆ’1)
â„“â€²,inâˆ’1,k
2]E[Î»(nâˆ’1)
â„“â€²
]) + Î²(n)
k .
(9.29)
Update Ï„
The variational distribution of Ï„ is Q(Ï„) = gamma( Ë†Î±Ï„, Ë†Î²Ï„), where
Ë†Î±Ï„ =
N
n=1 In
2
+ Î±Ï„,
(9.30)
Ë†Î²Ï„ = 1
2

âˆ¥Yâˆ¥2
F âˆ’2
I1

i1=1
. . .
IN

iN =1
Yi1...iN
N

n=1
E[G(n)
:,in,:] +
I1

i1=1
. . .
IN

iN =1
N

n=1
E[G(n)
:,in,: âŠ—G(n)
:,in,:]

+ Î²Ï„ .
(9.31)
Until Convergence

9.3 PARAFAC2
175
Fig. 9.1 An illustration of PARAFAC2
Even for regular tensor data Y = {Yk âˆˆRIÃ—J}K
k=1, PARAFAC2 and tensor CPD
differ in the sense that tensor CPD restricts factor matrices {Fk}K
k=1 to be equal across
slices, i.e., Fk = F, âˆ€k. More speciï¬cally, tensor CPD solves the following problem:
min
U(1),U(3),F
K

k=1
âˆ¥Yk âˆ’U(1)diag(U(3)
k,: )FT âˆ¥2
F.
(9.32)
By comparing problem (9.23) with problem (9.32), it can be seen that PARAFAC2
generalizes tensor CPD to deal with irregular tensor data with unaligned size along
one dimension, as illustrated in Fig.9.1.
To simplify the constraints of {Fk}K
k=1 in problem (9.23), a set of orthogonal
matrices P = {Pk âˆˆRJkÃ—R} and a rank-R factor matrix U(2) âˆˆRRÃ—R are introduced
in [5], which transforms problem (9.23) to
min
{U(n)}3
n=1,P
K

k=1
âˆ¥Yk âˆ’U(1)diag(U(3)
k,: )

U(2)T PT
k âˆ¥2
F,
s.t.
PT
k Pk = IR, âˆ€k âˆˆ{1, . . . , K},
Pk âˆˆRJkÃ—R, âˆ€k âˆˆ{1, . . . , K}, U(2) âˆˆRRÃ—R.
(9.33)
In [5], the direct ï¬tting (DF) algorithm was developed to solve problem (9.33).
Particularly, given the orthogonal matrices in P, problem (9.33) reduces to a tensor
CPD problem, which can be solved using Algorithm 1. On the other hand, by ï¬xing
the factor matrices {U(n)}3
n=1, each orthogonal matrix in P can be optimized via
singular value decomposition (SVD). Due to the equivalence between problem (9.23)

176
9
From CPD to Other Tensor Decompositions
and problem (9.33) established in [5], after obtaining the solution of problem (9.33)
using the DF algorithm, the solution to problem (9.23) is recovered by Fk = PkU(2),
with {U(1), U(3)} remain the same.
Similar to Bayesian tensor CPD in previous chapters, or TuckerD and TTD, proba-
bilistic PARAFAC2 [17] also employs the sparsity-promoting prior to achieve tensor
rank learning. It starts with L(L â‰¥R) columns in all factor matrices and places
Gaussian-gamma prior on such over-parametrized columns to encode sparsity infor-
mation. To be more speciï¬c, the prior design for factor matrices is [17]
p(U(1)) =
L

l=1
N(U(1)
:,l |0IÃ—1, II),
(9.34)
p(U(2)) =
L

l=1
N(U(2)
:,l |0JÃ—1, IJ),
(9.35)
p(U(3)|{Î³l}L
l=1) =
L

l=1
N(U(3)
:,l |0IÃ—1, Î³âˆ’1
l
IK),
(9.36)
p({Î³l}L
l=1) =
L

l=1
gamma(Î³l|c0
l , d0
l ).
(9.37)
On the other hand, the likelihood function in probabilistic PARAFAC2 [17] is
obtained from the objective function of (9.33), by assuming each observation in Y
subject to independent Gaussian noise perturbation. This results in
p(Y|{U(n)}3
n=1, Ï„; P)âˆexp

âˆ’Ï„
2
K

k=1
âˆ¥Ykâˆ’U(1)diag(U(3)
k,: )

U(2)T PT
k âˆ¥2
F

.
(9.38)
For the noise precision Ï„, a Gamma prior gamma(Ï„|a0, b0) is assigned. Based on the
prior and likelihood functions in (9.34)â€“(9.38), the joint probability of data Y and
parameters  = {{U(n)}3
n=1, {Î³l}L
l=1, Ï„} is summarized as
p(Y, {U(n)}3
n=1, {Î³l}L
l=1, Ï„; P) = p(Y|{U(n)}3
n=1, Ï„; P)p(U(1))p(U(2))
Ã— p(U(3)|{Î³l}L
l=1)p({Î³l}L
l=1)p(Ï„).
(9.39)
An inference algorithm was developed in [17] by using variational EM frame-
work under the mean-ï¬eld assumption Q() = N
n=1 Q(U(n)) L
l=1 Q(Î³l)Q(Ï„).
The resultant algorithm is summarized in Algorithm 14.

9.3 PARAFAC2
177
Algorithm 14 Probabilistic PARAFAC2 [17]
Initializations: Choose L > R and initial values {M(n), (n)}N
n=1, {cl, dl}L
l=1, a, b.
Iterations:
Update P
Pk = kT
k , âˆ€k âˆˆ{1, . . . , K},
(9.40)
where k and k are orthogonal matrices obtained from the following singular value decomposition
(SVD):
E[U(2)]diag(E[U(3)
k,: ])

E[U(1)]
T
Yk = kÏ’kT
k .
(9.41)
Update W
Wk = YkPk, âˆ€k âˆˆ{1, . . . , K},
(9.42)
Update U(n) from n = 1 to 3
The optimal variational pdf Qâˆ—(U(n)
:,l ) is a Gaussian distribution N(U(n)
:,l |M(n)
:,l , (n)) with its covari-
ance and mean as
(n) =

E [Ï„] E
 
3â‹„
k=1,kÌ¸=n U(k)
T 
3â‹„
k=1,kÌ¸=n U(k)
 
+ IL
âˆ’1
, n = 1, 2,
(9.43)
(n) =

E [Ï„] E
 
3â‹„
k=1,kÌ¸=n U(k)
T 
3â‹„
k=1,kÌ¸=n U(k)
 
+ diag{E[Î³1], . . . , E[Î³L]}
âˆ’1
, n = 3,
(9.44)
M(n) = E [Ï„] W(n)T E

3â‹„
k=1,kÌ¸=n U(k)

(n),
(9.45)
where W(n) is the Mode-n unfolding of the tensor W.
Update Î³l from l = 1 to L
The optimal variational pdf Qâˆ—(Î±l) coincides with Gamma distribution gamma(cl, dl), where
cl = c0
l + K
2 ,
(9.46)
dl = d0
l + 1
2E

U(3)T 
U(3)
l,l
.
(9.47)
Update Ï„
The optimal variational pdf Qâˆ—(Ï„) is derived to be a Gamma distribution gamma(a, b), with
a = a0 + I Ã— J Ã— K
2
,
(9.48)
b = b0 + 1
2E
 K

k=1
âˆ¥Wk âˆ’U(1)diag(U(3)
k,: )

U(2)T
âˆ¥2
F

.
(9.49)
Until Convergence

178
9
From CPD to Other Tensor Decompositions
9.4
Tensor-SVD (T-SVD)
Tensor-SVD (T-SVD) is a relatively new tensor decomposition and ï¬rst appears in
[18]. It has a wide range of applications, especially for color images, videos, and
multi-channel audio sequences modeling [19]. To formally deï¬ne T-SVD, we ï¬rstly
introduce several deï¬nitions that are essential for T-SVD.
Deï¬nition 9.1 (T-product) Given X âˆˆRI1Ã—RÃ—I3 and Y âˆˆRRÃ—I2Ã—I3, the
T-product X âˆ—Y is the I1 Ã— I2 Ã— I3 tensor
X âˆ—Y = fold (circ(X) unfold (Y)),
(9.50)
where unfold(X) =

X:,:,1; . . .; X:,:,I3

, fold is the inverse operator of unfold,
and
circ(X) =
â¡
â¢â¢â¢â£
X:,:,1
X:,:,I3
Â· Â· Â· X:,:,2
X:,:,2
X:,:,1
Â· Â· Â· X:,:,3
...
...
...
...
X:,:,I3 X:,:,I3âˆ’1 Â· Â· Â· X:,:,1
â¤
â¥â¥â¥â¦.
(9.51)
Deï¬nition 9.2 (Identity Tensor) The identity tensor I âˆˆRIÃ—IÃ—I3 is deï¬ned
as the tensor whose ï¬rst frontal slice is the I Ã— I identity matrix, and other
slices are all zeros, i.e., I:,:,1 = II, I:,:,i = 0IÃ—I, âˆ€i = 2, . . . , I3.
Deï¬nition 9.3 (F-diagonal Tensor) A tensor is called f -diagonal if its frontal
slices are all diagonal matrices.
Deï¬nition 9.4 (Conjugate Transpose) The conjugate transpose of a tensor
is deï¬ned as the tensor Xâ€  âˆˆRI2Ã—I1Ã—I3 constructed by conjugate transposing
each frontal slice of X âˆˆRI1Ã—I2Ã—I3 and then reversing the order of the trans-
posed frontal slices 2 through I3, i.e., Xâ€ 
:,:,1 = XH
:,:,1, Xâ€ 
:,:,i = XH
:,:,I3âˆ’i+2, âˆ€i =
2, . . . , I3.
Deï¬nition 9.5 (Orthogonality) A tensor Q âˆˆRIÃ—IÃ—I3 is called orthogonal,
provided that Qâ€  âˆ—Q = Qâˆ—Qâ€  = I with I being an I Ã— I Ã— I3 identity
tensor.
Now, we are ready for the deï¬nition of T-SVD.

9.4 Tensor-SVD (T-SVD)
179
Deï¬nition 9.6 (T-SVD) Let X be an I1 Ã— I2 Ã— I3 real-valued tensor. Then X
can be factored as
X = U âˆ—D âˆ—Vâ€ ,
(9.52)
where U âˆˆRI1Ã—I1Ã—I3, V âˆˆRI2Ã—I2Ã—I3 are orthogonal tensors, and D âˆˆ
RI1Ã—I2Ã—I3 is an f -diagonal tensor. The factorization (9.52) is called the
T-SVD (i.e., tensor SVD).
For a matrix X, if we perform SVD, X = USVT , the rank of X is deï¬ned as the
number of non-zero elements in the diagonal matrix S. Generalizing this to T-SVD,
the tubal rank of X is formally deï¬ned as follows.
Deï¬nition 9.7 (Tensor tubal rank) The tubal rank of X is the number of
non-zero tubes of D from the T-SVD of X = U âˆ—D âˆ—Vâ€ , i.e.,
Rankt(X) = #{i, D(i, i, :) Ì¸= 0}.
(9.53)
According to [20], any tensor with a tubal rank up to R can be factorized as
X = U âˆ—Vâ€ ,
(9.54)
forsomeU andV satisfyingRankt(U) = Rankt(V) = R.In (9.54),U âˆˆRI1Ã—RÃ—I3,
V âˆˆRI2Ã—RÃ—I3, and R â‰¤min (I1, I2) controls the tubal rank. To infer tubal rank, [21]
employs the Gaussian-gamma prior to impose sparsity in U and V. In particular,
assume the low-tubal rank upper bound is L > R,
p(U | Î») =
I1

i=1
L

l=1
I3

k=1
N

Ui,l,k | 0, Î»âˆ’1
l

,
(9.55)
p(V | Î») =
I2

j=1
L

l=1
I3

k=1
N

V j,l,k | 0, Î»âˆ’1
l

,
(9.56)
p(Î») =
L

l=1
gamma

Î»l | aÎ»
0 , bÎ»
0

,
(9.57)

180
9
From CPD to Other Tensor Decompositions
where aÎ»
0 , bÎ»
0 in the gamma prior are pre-determined hyper-parameters. In (9.55)â€“
(9.56), each element in the tensor slice U:,l,: and V:,l,: follows a zero-mean Gaussian
distribution with precesion Î»l, of which the philosophy is similar to the probabilistic
tensor CPD presented in Chap.3.
Prior work [21] also considers the outlier modeling. More speciï¬cally, given the
tensor data Y âˆˆRI1Ã—I2Ã—I3, the likelihood function is speciï¬ed as
p(Y | U, V, S, Ï„) =
I1

i=1
I2

j=1
I3

k=1
N

Yi, j,k | (U âˆ—Vâ€ )i, j,k + Si, j,k, Ï„ âˆ’1
, (9.58)
where the sparse component S is introduced to model the outlier. Exactly the same
as in Chap.6, independent Gaussian-gamma priors are placed over each element in
the additional sparse component S,
p(S | Î²) =
I1

i=1
I2

j=1
I3

k=1
N

Si, j,k | 0, Î²âˆ’1
i, j,k

,
(9.59)
p(Î²) =
I1

i=1
I2

j=1
I3

k=1
gamma

Î²i, j,k | aÎ²
0 , bÎ²
0

,
(9.60)
where aÎ²
0 , bÎ²
0 are pre-determined hyper-parameters. To complete the Bayesian mod-
eling, the prior of the noise precision Ï„ is p(Ï„) = gamma

Ï„ | aÏ„
0, bÏ„
0

. The prob-
abilistic modeling of T-SVD is summarized by the joint distribution of Y and
 = {U, V, S, Î», Î², Ï„},
p(Y, ) = p(Y | U, V, S, Ï„)p(U | Î»)p(V | Î»)p(Î»)p(S | Î²)p(Î²)p(Ï„).
(9.61)
A variational inference algorithm under the mean ï¬eld Q() = Q(U)Q(V)Q(S)
Q(Î»)Q(Î²)Q(Ï„) is developed in [21] and is summarized in Algorithm 15. In
Algorithm 15, âˆ’â†’
x iÂ· denotes the vector formed by unfolding Xâ€ 
i,:,:, âˆ’â†’
x Â· j denotes the
vector formed by unfolding X:, j,:, and L is the L Ã— L Ã— I3 tensor whose ï¬rst frontal
slice is the diagonal matrix L:,:,1 = diag{Î»1, . . . , Î»L} and all other slices are zeros.

9.4 Tensor-SVD (T-SVD)
181
Algorithm 15 Probabilistic T-SVD [21]
Initializations: Choose L > R and initial values {
#âˆ’â†’
u iÂ·
$
}I1
i=1, u, {
#âˆ’â†’
v jÂ·
$
}I2
j=1, v, {aÎ»
r , bÎ»
r }L
l=1,
{
#
Si, j,k
$
, Ïƒ2
i, j,k, aÎ²
i, j,k, bÎ²
i, j,k}I1,I2,I3
i=1, j=1,k=1, aÏ„, bÏ„.
Iterations:
Update U and V
The optimal variational pdf Q(U) is Q(U) = I1
i=1 N

âˆ’â†’
u i. |
#âˆ’â†’
u iÂ·
$
, u
, whose parameters are
given by
#âˆ’â†’
u iÂ·
$
= âŸ¨Ï„âŸ©u circ(âŸ¨VâŸ©)âŠ¤
âˆ’â†’
y iÂ· âˆ’
#âˆ’â†’s iÂ·
$
,
(9.62)
u =

âŸ¨Ï„âŸ©
%
circ(V)âŠ¤circ(V)
&
+ circ(âŸ¨LâŸ©)
âˆ’1
.
(9.63)
Similarly, the optimal variational pdf of V is given by Q(V) = I2
j=1 N

âˆ’â†’
v j. |
#âˆ’â†’
v j.
$
, v
with
the mean and covariance
#âˆ’â†’
v jÂ·
$
= âŸ¨Ï„âŸ©v circ(âŸ¨UâŸ©)âŠ¤
âˆ’â†’
y Â· j âˆ’
#âˆ’â†’s Â· j
$
,
(9.64)
v =

âŸ¨Ï„âŸ©
%
circ(U)âŠ¤circ(U)
&
+ circ(âŸ¨LâŸ©)
âˆ’1
.
(9.65)
Update Î»
The optimal variational pdf is Q(Î») = L
l=1 gamma

Î»l | aÎ»
l , bÎ»
l

with parameters
aÎ»
l = aÎ»
0 + (I1 + I2) I3
2
,
(9.66)
bÎ»
l = bÎ»
0 + 1
2
%âˆ’â†’
u Â·l
2 +
âˆ’â†’
v Â·l
2&
.
(9.67)
Update S
The optimal variational pdf of S can be obtained as Q(S) = 
i, j,k N

Si jk |
#
Si, j,k
$
, Ïƒ2
i, j,k

with
the parameters
#
Si, j,k
$
= âŸ¨Ï„âŸ©

#
Î²i, j,k
$
+ âŸ¨Ï„âŸ©
 
Yi, j,k âˆ’(U âˆ—Vâ€ )i, j,k

,
(9.68)
Ïƒ2
i, j,k =

#
Î²i, j,k
$
+ âŸ¨Ï„âŸ©
âˆ’1
(9.69)
Update Î²
The optimal variational pdf of Î² is given by Q

Î²i, j,k

= gamma

Î²i, j,k | aÎ²
i, j,k, bÎ²
i, j,k

, whose
parameters are
aÎ²
i, j,k = aÎ²
0 + 1
2,
(9.70)
bÎ²
i, j,k = bÎ²
0 + 1
2
%
Î²2
i, j,k
&
.
(9.71)

182
9
From CPD to Other Tensor Decompositions
Update Ï„
The noise precision has the following posterior distribution: Q(Ï„) = gamma (Ï„ | aÏ„, bÏ„), whose
parameters are
aÏ„ = aÏ„
0 +
N
n=1 In
2
,
(9.72)
bÏ„ = bÏ„
0 + 1
2

i, j,k
'Yi, j,k âˆ’(U âˆ—Vâ€ )i, j,k âˆ’Si, j,k

2(
.
(9.73)
Until Convergence
References
1. Q. Zhao, L. Zhang, A. Cichocki, Bayesian sparse tucker models for dimension reduction and
tensor completion (2015). arXiv:1505.02343
2. L. Xu, L. Cheng, N. Wong, Y.-C. Wu, Overï¬tting avoidance in tensor train factorization and
completion: prior analysis and inference, in 2021 IEEE International Conference on Data
Mining (ICDM) (IEEE, 2021), pp. 1439â€“1444
3. Z. Long, C. Zhu, J. Liu, Y. Liu, Bayesian low rank tensor ring for image recovery. IEEE Trans.
Image Process. 30, 3568â€“3580 (2021)
4. R.A. Harshman, Parafac2: Mathematical and technical notes, in UCLA Working Papers in
Phonetics, vol. 22, no. 10, pp. 30â€“44 (1972)
5. H.A. Kiers, J.M. Ten Berge, R. Bro, Parafac2â€“part i. a direct ï¬tting algorithm for the parafac2
model. J. Chemom.: J. Chemom. Soc. 13(3â€“4), 275â€“294 (1999)
6. Y. Panagakis, C. Kotropoulos, Automatic music tagging via parafac2, in 2011 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing (ICASSP) (IEEE, 2011), pp.
481â€“484
7. E. Pantraki, C. Kotropoulos, Automatic image tagging and recommendation via parafac2, in
2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)
(IEEE, 2015), pp. 1â€“6
8. E. Pantraki, C. Kotropoulos, A. Lanitis, Age interval and gender prediction using parafac2
applied to speech utterances, in 2016 4th International Conference on Biometrics and Forensics
(IWBF) (IEEE, 2016), pp. 1â€“6
9. P.A. Chew, B.W. Bader, T.G. Kolda, A. Abdelali, Cross-language information retrieval using
parafac2, in Proceedings of the 13th ACM SIGKDD international conference on Knowledge
discovery and data mining (2007), pp. 143â€“152
10. Y. Shin, S.S. Woo, What is in your password? analyzing memorable and secure passwords
using a tensor decomposition, in The World Wide Web Conference (2019), pp. 3230â€“3236
11. I. Perros, E.E. Papalexakis, F. Wang, R. Vuduc, E. Searles, M. Thompson, J. Sun, Spartan: scal-
able parafac2 for large & sparse data, in Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (2017), pp. 375â€“384
12. A. Afshar, I. Perros, E.E. Papalexakis, E. Searles, J. Ho, J. Sun, Copa: constrained parafac2
for sparse & large datasets, in Proceedings of the 27th ACM International Conference on
Information and Knowledge Management (2018), pp. 793â€“802
13. K. Yin, A. Afshar, J.C. Ho, W.K. Cheung, C. Zhang, J. Sun, Logpar: logistic parafac2 factor-
ization for temporal binary data with missing values, in Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining (2020), pp. 1625â€“1635
14. A. Afshar, I. Perros, H. Park, C. Deï¬lippi, X. Yan, W. Stewart, J. Ho, J. Sun, Taste: temporal
and static tensor factorization for phenotyping electronic health records, in Proceedings of the
ACM Conference on Health, Inference, and Learning (2020), pp. 193â€“203

References
183
15. Y. Ren, J. Lou, L. Xiong, J.C. Ho, Robust irregular tensor factorization and completion for
temporal health data analysis, in Proceedings of the 29th ACM International Conference on
Information & Knowledge Management (2020), pp. 1295â€“1304
16. I. Perros, X. Yan, J.B. Jones, J. Sun, W.F. Stewart, Using the parafac2 tensor factorization on
ehr audit data to understand pcp desktop work. J. Biomed. Inform. 101, 103312 (2020)
17. P.J. JÃ¸rgensen, S.F. Nielsen, J.L. Hinrich, M.N. Schmidt, K.H. Madsen, M. MÃ¸rup, Analy-
sis of chromatographic data using the probabilistic parafac2, in 33rd Conference on Neural
Information Processing Systems (2019)
18. K. Braman, Third-order tensors as linear operators on a space of matrices. Linear Algebra
Appl. 433(7), 1241â€“1253 (2010)
19. M.E. Kilmer, C.D. Martin, Factorization strategies for third-order tensors. Linear Algebra Appl.
435(3), 641â€“658 (2011)
20. O. Semerci, N. Hao, M.E. Kilmer, E.L. Miller, Tensor-based formulation and nuclear norm
regularizationformultienergycomputedtomography.IEEETrans.ImageProcess.23(4),1678â€“
1693 (2014)
21. Y. Zhou, Y.-M. Cheung, Bayesian low-tubal-rank robust tensor factorization with multi-rank
determination. IEEE Trans. Pattern Anal. Mach. Intell. 43(1), 62â€“76 (2019)


123
IFIP AICT 491
IFIP WG 9.7 International Conference
on the History of Computing, HC 2016
Brooklyn, NY, USA, May 25–29, 2016
Revised Selected Papers
International
Communities
of Invention
and Innovation
Arthur Tatnall
Christopher Leslie
(Eds.)

IFIP Advances in Information
and Communication Technology
491
Editor-in-Chief
Kai Rannenberg, Goethe University Frankfurt, Germany
Editorial Board
TC 1 – Foundations of Computer Science
Jacques Sakarovitch, Télécom ParisTech, France
TC 2 – Software: Theory and Practice
Michael Goedicke, University of Duisburg-Essen, Germany
TC 3 – Education
Arthur Tatnall, Victoria University, Melbourne, Australia
TC 5 – Information Technology Applications
Erich J. Neuhold, University of Vienna, Austria
TC 6 – Communication Systems
Aiko Pras, University of Twente, Enschede, The Netherlands
TC 7 – System Modeling and Optimization
Fredi Tröltzsch, TU Berlin, Germany
TC 8 – Information Systems
Jan Pries-Heje, Roskilde University, Denmark
TC 9 – ICT and Society
Diane Whitehouse, The Castlegate Consultancy, Malton, UK
TC 10 – Computer Systems Technology
Ricardo Reis, Federal University of Rio Grande do Sul, Porto Alegre, Brazil
TC 11 – Security and Privacy Protection in Information Processing Systems
Steven Furnell, Plymouth University, UK
TC 12 – Artificial Intelligence
Ulrich Furbach, University of Koblenz-Landau, Germany
TC 13 – Human-Computer Interaction
Jan Gulliksen, KTH Royal Institute of Technology, Stockholm, Sweden
TC 14 – Entertainment Computing
Matthias Rauterberg, Eindhoven University of Technology, The Netherlands

IFIP – The International Federation for Information Processing
IFIP was founded in 1960 under the auspices of UNESCO, following the first World
Computer Congress held in Paris the previous year. A federation for societies working
in information processing, IFIP’s aim is two-fold: to support information processing in
the countries of its members and to encourage technology transfer to developing na-
tions. As its mission statement clearly states:
IFIP is the global non-proﬁt federation of societies of ICT professionals that aims
at achieving a worldwide professional and socially responsible development and
application of information and communication technologies.
IFIP is a non-proﬁt-making organization, run almost solely by 2500 volunteers. It
operates through a number of technical committees and working groups, which organize
events and publications. IFIP’s events range from large international open conferences
to working conferences and local seminars.
The ﬂagship event is the IFIP World Computer Congress, at which both invited and
contributed papers are presented. Contributed papers are rigorously refereed and the
rejection rate is high.
As with the Congress, participation in the open conferences is open to all and papers
may be invited or submitted. Again, submitted papers are stringently refereed.
The working conferences are structured differently. They are usually run by a work-
ing group and attendance is generally smaller and occasionally by invitation only. Their
purpose is to create an atmosphere conducive to innovation and development. Referee-
ing is also rigorous and papers are subjected to extensive group discussion.
Publications arising from IFIP events vary. The papers presented at the IFIP World
Computer Congress and at open conferences are published as conference proceedings,
while the results of the working conferences are often published as collections of se-
lected and edited papers.
IFIP distinguishes three types of institutional membership: Country Representative
Members, Members at Large, and Associate Members. The type of organization that
can apply for membership is a wide variety and includes national or international so-
cieties of individual computer scientists/ICT professionals, associations or federations
of such societies, government institutions/government related organizations, national or
international research institutes or consortia, universities, academies of sciences, com-
panies, national or international associations or federations of companies.
More information about this series at http://www.springer.com/series/6102

Arthur Tatnall
• Christopher Leslie (Eds.)
International Communities
of Invention and Innovation
IFIP WG 9.7 International Conference
on the History of Computing, HC 2016
Brooklyn, NY, USA, May 25–29, 2016
Revised Selected Papers
123

Editors
Arthur Tatnall
Victoria University
Melbourne, VIC
Australia
Christopher Leslie
New York University
Brooklyn, NY
USA
ISSN 1868-4238
ISSN 1868-422X
(electronic)
IFIP Advances in Information and Communication Technology
ISBN 978-3-319-49462-3
ISBN 978-3-319-49463-0
(eBook)
DOI 10.1007/978-3-319-49463-0
Library of Congress Control Number: 2016957379
© IFIP International Federation for Information Processing 2016
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
Analog and digital computers were developed by individuals aware of an international
scientiﬁc community. Likewise, although sometimes thought of as solely national
projects, the ﬁrst computer networks were built in an age of growing interconnectivity
among nations.
The chapters of this book are drawn from papers presented at an International
Federation for Information Processing (IFIP) conference on the History of Computing
at the New York University Tandon School of Engineering in May 2016. The con-
ference was run by IFIP Working Group 9.7 (History of Computing). All papers
submitted for the conference were peer reviewed before acceptance, and those selected
for this book were reviewed again after the authors had the opportunity to make
improvements following discussions at the conference.
In concept, the conference aimed to show that, far from a deterministic view that
computers and computer networks were developed in isolation and according to their
own technical imperatives, the history of pre-existing relationships and communities
led to the triumphs (and dead-ends) in the history of computing. This broad perspective
aimed to help us tell a more accurate story of important developments like the Internet
and also to provide us with a better understanding of how to sponsor future invention
and innovation. The conference gathered historians, computer scientists, and other
professionals to reﬂect on histories that foreground the international community.
Themes for the conference focused on: invention, policy, infrastructure, and social
history.
In the ﬁrst chapter, “The Route Less Taken: The Homegrown Los Alamos Inte-
grated Computer Network,” Nicholas Lewis from the University of Minnesota and the
Charles Babbage Institute discusses the Los Alamos Integrated Computer Network. He
notes that between the 1970s and 1990s the Los Alamos National Laboratory built and
utilized a largely custom computer network for the lab’s supercomputers, designed to
support the unusual performance, storage, and security requirements of an American
weapons lab.
In a chapter titled “MONET – Monash University’s Campus LAN in the 1980s – A
Bridge to Better Networking,” Barbara Ainsworth (Monash University), Neil Clarke
(Deakin University), Chris Avram (Monash University), and Judy Sheard (Monash
University) outline how Monash University, Australia, developed an in-house local
area network called MONET during the 1980s to meet the needs of the university’s
computer users. The Monash University Computer Centre team created and installed an
early implementation of a local area network at a time when such concepts were
evolving and speciﬁc hardware and software for the purpose did not yet exist.
Frank Dittmann from the Deutsches Museum, Munich, writes on: “Technology vs.
Political Conﬂict – How Networks Penetrate the Iron Curtain.” He explains how, in
July 1977, the International Institute for Applied Systems Analysis (IIASA) near
Vienna organized an experimental data transmission line. His paper investigates this,

focusing on three aspects: IIASA was an important location for Eastern and Western
scientists to work with each other, the team of computer specialists creating the net-
work was remarkable, and the concept of combining computer technology with science
cooperation and information transfer was very advanced in the 1970s.
“There and Back Again – Andrew Booth, a British Computer Pioneer, and His
Interactions with U.S. and Other Contemporaries” by Roger Johnson from Birkbeck
College, London University, explores the interchanges between Andrew Booth, an
early British computer pioneer, and contemporary U.S. and other pioneers. The paper
describes Booth’s construction of an electronic drum, the world’s ﬁrst successful
demonstration of a rotating storage device connected to a computer, his pioneering
work on natural language processing, and ﬁnally and most notably his invention of the
Booth hardware multiplier, which is the basis of the multiplier used in billions of chips
each year.
The next chapter, “Machines à Comparer les Idées’ of Semen Korsakov: First Step
Towards AI,” written by Valery Shilov and Sergey Silantiev from the National
Research University Higher School of Economics, Moscow, relates to the forgotten
Russian scholar Semen Korsakov. The paper describes his life and scientiﬁc activity
and particularly to Korsakov’s main achievement – invention of ﬁve “intellectual
machines” that could be considered as the very ﬁrst attempt to design a mechanical
device capable of performing such intellectual operations as data analysis, comparison,
and selection.
“Towards Machine Independence: From Mechanically Programmed Devices to the
Internet of Things,” by Arthur Tatnall (Victoria University) and Bill Davey (RMIT
University), Melbourne, Australia, provides a historical account of the development of
one aspect of technology and of machines, leading to information technologies, the
Internet, and the Internet of Things. It points to an increasing trend toward these
machines and devices becoming more and more independent of human intervention
and control. A clear trend can be observed from mechanically controlled machines to
modern smart kitchen and household appliances that really could be said to have a
degree of independence.
Giovanni Cignoni and Giovanni Cossu from Pisa, Italy, present “The Global Virtual
Museum of Information Science and Technology, a Project Idea.” They point out that
information science and technology (IST) has pervasively affected our everyday lives
and become part of the cultural heritage of humanity. The growing curiosity about IST
history has led to the creation of important collections devoted to the conservation of
IST relics, but physical relics are naturally located close to their origins. Their paper
proposes a global virtual museum of IST based on a knowledge base able to manage all
the information of the domain, created and updated by museum keepers and other
experts, and capable of offering new enjoyment opportunities to a wider public
audience.
“Why Not OSI?” by Bill Davey (RMIT University, Australia) and Robert Houghton
(Idaho State University, USA) argues that the OSI proposed standard is technically
superior to the TCP/IP standard for network communications and presents an analysis
of the historical record surrounding the adoption of TCP/IP rather than OSI. The paper
does not seek to create a new history of TCP/IP but to suggest this is a case where
VI
Preface

traditional explanations of adoption based on the nature of the technology do not
explain the demise of the OSI model.
“Flame Wars on Worldnet: Early Constructions of the International User” was
contributed by Christopher Leslie from NYU Tandon School of Engineering, New
York. He notes that some of the earliest users of the Internet described their activities as
predicting a widespread communication medium that would cross national boundaries
even before the technical capability was possible. However, analysis of conversations
on Human-Nets, an early ARPANet mailing list, shows how users were concerned
about providing a forum for open discussion and hoped that the network would spread
to provide communication throughout the world.
“The Code of Banking: Software as the Digitalization of German Savings Banks” is
written by Martin Schmitt from the Centre for Contemporary History, Potsdam, Ger-
many, and describes the history of banking software. He notes that although some
literature exists on the use of computers in the banking industry, most of it focuses only
on the hardware and its restrictions. This paper analyzes how German savings banks
used software to digitalize their business during the Cold War period.
Evangelos Kotsioris from Princeton University, New Jersey, then offers: “Electronic
‘Ambassador’: The Diplomatic Missions of IBM’s RAMAC 305.” His paper describes
how the RAMAC 305 developed by IBM during the late 1950s was instrumental as an
“animate” ambassador of American computing technology abroad. He examines the
impact of IBM’s exhibit at the American National Exhibition in Moscow (July 1959)
and Nikita Khrushchev’s tour to the IBM manufacturing plant in California (September
1959) to argue that the RAMAC 305 was envisioned and designed as a modular system
of combinable units and peripherals that could be easily and quite literally transferred
around the world.
Herbert Bruderer, from ETH Zurich, Switzerland, writes on “The Birth of Artiﬁcial
Intelligence: First Conference on Artiﬁcial Intelligence in Paris in 1951?” He notes that
although the 1956 Dartmouth conference is often considered as the cradle of artiﬁcial
intelligence, there is a debate about this. Other suggestions about its origin are offered
including a large and important (but forgotten) European conference on computing and
human thinking in Paris in 1951.
“The World’s Smallest Mechanical Parallel Calculator: Discovery of Original
Drawings and Patent Documents from the 1950s in Switzerland” is also by Herbert
Bruderer from ETH Zurich, Switzerland. This paper describes how Austrian engineer
Curt Herzstark invented the world-renowned mechanical pocket calculator Curta,
mentioned as consisting of two models, Curta 1 and Curta 2. He notes that original
drawings and patent documents on a multiple Curta, the world’s smallest mechanical
parallel calculator, were discovered in Switzerland in 2015.
Contributions to the book have thus come from authors from the USA, Australia,
Germany, UK, Russia, Italy, and Switzerland. We hope you enjoy the book and ﬁnd it
informative.
October 2016
Arthur Tatnall
Christopher Leslie
Preface
VII

Organization
Program Committee
Arthur Tatnall (Chair)
Victoria University, Melbourne, Australia
Janet Abbate
Virginia Tech, USA
Gerard Alberts
University of Amsterdam, The Netherlands
Corrado Bonfanti
Italian Computer Society, Italy
Tilly Blyth
Science Museum, London, UK
David Burger
IEEE History Committee, Sydney, Australia
Paul Ceruzzi
National Air and Space Museum, Smithsonian Institution,
USA
Bill Davey
Royal Melbourne Institute of Technology, Melbourne,
Australia
Michael Geselowitz
IEEE History Center, Stevens Institute of Technology,
USA
Lisa Gitelman
NYU Steinhardt and College of Arts and Science, USA
Marie Hicks
Illinois Institute of Technology, USA
John Impagliazzo
Hofstra University, USA
Roger Johnson
Birkbeck College, University of London, UK
Christopher Leslie
NYU Tandon School of Engineering, New York, USA
Petri Paju
University of Turku, Finland
Rebecca Slayton
Cornell University, USA
Janet Toland
Victoria University of Wellington, New Zealand
Organizing Committee
Christopher Leslie
(Chair)
NYU Tandon School of Engineering, USA
Krysta Battersby
NYU Tandon School of Engineering, USA
Randy Soﬁa
NYU Tandon School of Engineering, USA
Barbara Tatnall
Heidelberg Press, Australia

Contents
The Route Less Taken: The Homegrown Los Alamos Integrated
Computer Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Nicholas Lewis
MONET – Monash University’s Campus LAN in the 1980s – A Bridge
to Better Networking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
Barbara Ainsworth, Neil Clarke, Chris Avram, and Judy Sheard
Technology vs. Political Conflict – How Networks Penetrate
the Iron Curtain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
Frank Dittmann
There and Back Again – Andrew Booth, a British Computer Pioneer,
and his Interactions with US and Other Contemporaries. . . . . . . . . . . . . . . .
58
Roger G. Johnson
‘Machines à Comparer les Idées’ of Semen Korsakov: First
Step Towards AI. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
Valery V. Shilov and Sergey A. Silantiev
Towards Machine Independence: From Mechanically Programmed
Devices to the Internet of Things . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
Arthur Tatnall and Bill Davey
The Global Virtual Museum of Information Science & Technology,
a Project Idea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
Giovanni A. Cignoni and Giovanni A. Cossu
Why not OSI? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
Bill Davey and Robert F. Houghton
Flame Wars on Worldnet: Early Constructions of the International User . . . .
122
Christopher Leslie
The Code of Banking: Software as the Digitalization of
German Savings Banks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
Martin Schmitt
Electronic “Ambassador”: The Diplomatic Missions of IBM’s
RAMAC 305 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
Evangelos Kotsioris

The Birth of Artificial Intelligence: First Conference on Artificial
Intelligence in Paris in 1951? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
Herbert Bruderer
The World’s Smallest Mechanical Parallel Calculator: Discovery
of Original Drawings and Patent Documents from the 1950s
in Switzerland. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
186
Herbert Bruderer
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
XII
Contents

The Route Less Taken: The Homegrown Los Alamos
Integrated Computer Network
Nicholas Lewis
(✉)
History of Science, Technology, and Medicine Program, Charles Babbage Institute,
University of Minnesota, Minneapolis, MN, USA
lewi0740@umn.edu
Abstract. Between the 1970s and 1990s, Los Alamos National Laboratory built
and utilized a largely custom computer network for the Lab’s supercomputers.
Designed to support the unusual performance, storage, and security requirements
of an American weapons lab, the Los Alamos Integrated Computer Network, as
the focus of historical study, complicates and enriches the history of computer
networking development, exploring the approaches and contributions to
computer networking of an institution outside the better-known worlds of
industry, academia, and the military. For example, the Lab’s reticence to adopt
TCP/IP due to performance and security concerns further complicates the narra‐
tive of the ARPANET/Internet protocol suite’s adoption among advanced
networking sites in the 1980s and 90s.
Keywords: Los Alamos National Laboratory · Supercomputing · Computer
networking · Network standardization · ANSI · TCP/IP · History of computing
1
Introduction
Since the Second World War, the Laboratory at Los Alamos has engaged in advanced
scientiﬁc computing as part of its primary mission as a nuclear weapons lab, and
increasingly for other ﬁelds of computing-intensive scientiﬁc research, such as climate
modelling. Because of its unusual computational demands, Los Alamos, during and after
the Cold War, often sought levels of computing performance, and hardware and software
capabilities, before high-tech vendors were capable or interested in meeting those
requirements. As a consequence, Los Alamos had a history of developing custom solu‐
tions to computing-related problems, particularly where new technologies were not yet
available on the market, or existing systems lacked key features or capabilities Lab
supercomputer users desired. Homegrown solutions were costlier in terms of develop‐
ment and maintenance, but were typically well-tailored to the Lab’s technical and
security requirements. The custom approach also provided in-house knowledge and
LA-UR-16-20103
© IFIP International Federation for Information Processing 2016
Published by Springer International Publishing AG 2016. All Rights Reserved
A. Tatnall and C. Leslie (Eds.): HC 2016, IFIP AICT 491, pp. 1–22, 2016.
DOI: 10.1007/978-3-319-49463-0_1

expertise for technologies important to the Lab. The Los Alamos Integrated Computer
Network of the 1970s and 80s was a prime example of the Lab’s homegrown computing
solutions.1
The Lab’s custom approach toward computer networking, followed by its adoption
of outside networking technologies, such as TCP/IP, from the late 1980s through the
mid-1990s, oﬀers a window onto several important facets of networking history. First,
it traces the maturation of computer networking over the course of two decades from
the perspective of an American weapons laboratory, with its unusual networking
requirements interacting with the capabilities and business interests of an emerging
industry. Second, it highlights the alternative approaches and contributions toward
computer networking of an institution outside academia, industry, or the Department of
Defense. Finally, it builds upon historiographical works, such as Janet Abbate’s
Inventing the Internet and Andrew Russell’s Open Standards and the Digital Age, which
argue against previous common conceptions of how and why TCP/IP spread as a
networking standard, demonstrating that the DoD’s protocol suite was not always
adopted readily at computer networking sites. Indeed, Los Alamos considered TCP/IP
to be a technical step backward, in terms of security and performance, over the custom
Lab protocols, and initially rejected using the DoD suite in the early 1980s. Networking
development at Los Alamos followed a largely unique path during its ﬁrst two decades,
attempting to integrate slowly with outside standards and technologies as the high-
performance computer networking market and industry evolved appreciably in the
1980s and 90s. However, various internal, but primarily external pressures, brought a
far more rapid end to the homegrown integrated computer network than Los Alamos
had anticipated.2
2
Early Lab Computing
Founded in 1943 as the primary R&D site of the Manhattan Project, the Laboratory at
Los Alamos emerged from World War II as a major center of scientiﬁc computing in
the United States. Using, building, and buying some of the earliest digital computers,
1 For an overview of Los Alamos and its computing history, see Donald MacKenzie, “The
Inﬂuence of the Los Alamos and Livermore National Laboratories on the Development of
Supercomputing,” Annals of the History of Computing 13, No. 2 (1991).
2 For detailed discussions of how and why the TCP/IP suite developed and became the standard
set of networking protocols in use today, see Janet Abbate, Inventing the Internet (Cambridge,
MA: MIT Press, 1999), Andrew L. Russell, Open Standards and the Digital Age: History,
Ideology, and Networks (New York: Cambridge University Press, 2014), and Laura DeNardis,
The Global War for Internet Governance (New Haven: Yale University Press, 2014). These
works challenge previous depictions of the history of ARPANET and the Internet that depicted
the development and spread of TCP/IP as open and uncontested processes, when, in fact, the
opposite was true. The protocol suite resulted from closed-door planning and implementation
decisions, and entered widespread usage largely as a result of the US Department of Defense
exerting ﬁnancial and other forms of coercion on networked institutions and vendors to adopt
the suite.
2
N. Lewis

Los Alamos played a role in the maturation of computer hardware and software in the
ﬁrst decades of the Cold War, going back to its use of ENIAC on that computer’s very
ﬁrst calculation. As part of its collaboration with IBM on the design of the IBM 7030
“Stretch,” Los Alamos helped to create and deﬁne the concept of supercomputing.
Research and development on operating systems, programming languages, and
compilers also accompanied the Lab’s work on hardware, creating the software tools
which vendors like IBM, Control Data Corporation, and Cray Research either could not,
or would not, provide for the small (but growing) niche of high-performance scientiﬁc
computing.3
Beginning in the early 1950s, when digital computing was still in its infancy, Los
Alamos actively engaged with the broader, though still small, scientiﬁc computing
community, exchanging correspondence and software with sites like the RAND Corpo‐
ration, and with other researchers, taking part in conferences and user groups, like IBM
SHARE, of which Los Alamos was a founding member. The purpose was to share in
the community’s pool of knowledge and experience concerning a new technological
ﬁeld. These exchanges continued and became more elaborate and important over time,
with Los Alamos directly funding R&D at universities and among vendors, including
IBM, RCA, and Burroughs, on computing technologies, such as high-speed storage and
memory, which weren’t feasible or cost-eﬀective to research at the Lab. Participating
in the scientiﬁc computing community and encouraging R&D activities outside the Lab
were considered to be investments that would reduce the need for costlier in-house
development. However, in-house development was a common practice in Los Alamos
computing, particularly along new technological branches, like high-performance and
high-security networking in the 1970s.4
3 MacKenzie, “The Inﬂuence of the Los Alamos and Livermore National Laboratories,” 186;
Memorandum from Walter H. Brummet, Jr., Chief Contracts and Procurement Branch, to
George Udell, Director, Supply Division, Nov. 17, 1955, “Report of Conferences in Wash‐
ington, D. C. Regarding Proposed Purchase of Computing Machine for LASL,” Edward A.
Voorhees Papers, Box 16, Folder 6 Stretch Solicitation and Selection, LANL Archives; Charles
J. Bashe, et al., IBM’s Early Computers (Cambridge, MA: MIT Press, 1986), 430, 432; For a
detailed assessment of the intersection between computing and the development of nuclear
weapons at Los Alamos, see Anne Fitzpatrick’s Ph.D. dissertation, “Igniting the Light
Elements: The Los Alamos Thermonuclear Weapon Project, 1942-1952” (The George
Washington University, 1999); Thomas Haigh, Mark Priestley, and Crispin Rope, “Los
Alamos Bets on ENIAC: Nuclear Monte Carlo Simulations, 1947-1948,” IEEE Annals of the
History of Computing 36, No. 3 (July-Sept. 2014) examines speciﬁcally Los Alamos’ use of
ENIAC for the ﬁrst simulations of nuclear weapons on a digital computer.
4 Correspondence from Bengt Carlson to Carson Mark, May 23, 1960, “UHS Computer Study
Contracts,” Edward A. Voorhees Papers, Box 32, Folder 1, Series 25, SPARC Correspondence,
LANL Archives; Correspondence from Edward A. Voorhees to Paul Armer, Head, Computer
Sciences Department, The RAND Corporation, March 17, 1960, Edward A. Voorhees Papers,
Box 4, Folder 10, Series 9, File 1, Voorhees Correspondence 1960, LANL Archives; Corre‐
spondence between T. L. Jordan and Donald C. Cashman, SHARE Distribution Agent, January
13, 1961 through June 27, 1961, Edward A. Voorhees Papers, Box 4, Folder 18, Series 9, File
2, Jordan Correspondence 1960-61, LANL Archives; MacKenzie, “The Inﬂuence of the Los
Alamos and Livermore National Laboratories,” 189-193.
The Homegrown Los Alamos Integrated Computer Network
3

3
C Division and Custom Lab Networking
In 1968, Los Alamos formed its ﬁrst dedicated Computing or “C” Division, its personnel
drawn from the three computing-related groups within the Theoretical Physics or “T”
Division. T Division had led the original Los Alamos hydrogen-bomb project, and was
still the most computing-intensive division in the 1960s, occupying 40% of the Lab’s
available computing time. Transferring oversight of the Lab’s supercomputers to a
dedicated computing division was intended to unify the Lab’s computing eﬀorts, as
multiple science divisions were purchasing their own machines, and were pursuing
diﬀering technical paths from one another. A dedicated computing division was also
intended to mitigate the potential conﬂict of interest found with the most computer-
dependent division allocating access to all other divisions.5
Between the mid-1960s and the formation of the new C Division, T Division faced a
rapidly growing demand from users on Lab computing resources. The actual computing
power at the Lab’s Central Computing Facility (CCF) quadrupled every two years between
1966 and 1974, but this increase still did not meet the user demand. With the mission-
critical nature of Lab computing resources, particularly amid the escalating demands from
users, T Division was reluctant to adopt new technologies that had the potential to disrupt
vital services or security, at least until those technologies had undergone extensive testing
before entering widespread use at the Lab. Computer users from other divisions who were
dissatisfied with T Division’s conservative approach did not find significant change with
the formation of C Division under its first leader, Roger Lazarus. A physicist and Los
Alamos veteran since 1951, Lazarus was an expert on the application of digital computing
for nuclear weapons development. Lazarus likewise resisted the adoption of technologies
that, among other factors, might impinge on the efficiency or availability of the Lab’s
supercomputers for weapons work.6
For example, Lazarus resisted the development or adoption of time-sharing operating
systems at Los Alamos, believing that time sharing, in its early stages in the late 1960s
5 “Use of Computing Time by Division,” Edward A. Voorhees Papers, Box 67, Folder 3, Series
48, Monthly Summary Sheets 1961-1964, LANL Archives; “New Division Formed at LASL,”
The Atom 5, no. 5 (May 1968), 17-18.
6 The increasing user demand upon computer resources at Los Alamos between the mid-1960s and
mid-70s was due to a variety of factors: First, the Partial Test-Ban Treaty of 1963 moved all
nuclear testing underground, which made weapons tests more expensive, and increased the reli‐
ance upon computer simulation. Second, after having diversified in their research, the US weapons
labs experienced a reducing budget year-over-year beginning in 1960, spurring Los Alamos to
transfer large numbers of personnel back toward its core mission, which increased the number of
heavy computer users, despite the Lab population remaining stable. Third, the weapons them‐
selves became more sophisticated and design tolerances became tighter starting in the mid-1960s,
as US nuclear strategic policy shifted toward a counter-force (targeting military assets) strategy.
A mix of more complex weapons designs and more complex codes used for development and
simulation also markedly increased the processing and storage demands of the heaviest users of
Lab computing resources between the 1960s and 70s. Part of T and then C Division's conserva‐
tism with new technologies was the need to prevent disruptions in the already overbooked CCF's
production schedules.
4
N. Lewis

and early 1970s, posed security risks, and wasted clock cycles at a time when the Lab
still could not keep up with the growing demand for computing power. Computer users
from heavily computer-dependent divisions, such as P (Physics), MP (Material Physics),
and W (Weapons), found this approach to time sharing to be too cautious, and success‐
fully petitioned for Lazarus to be replaced in 1973. His successor, Frank McGirt, quickly
entered into collaboration with the Lawrence Livermore Lab to utilize its LTSS (Liver‐
more Time-Sharing System) operating system on the CDC (Control Data Corporation)
7600 supercomputers at Los Alamos. Livermore’s Computation Division, under Sidney
Fernbach from 1955 to 1982, typically approached new technical solutions more readily
than did Los Alamos’ C Division under Lazarus. Curiously, Fernbach was less open
than Los Alamos to the potential of smaller computer systems in the 1970s, as he
remained focused on large machines. He usually delegated responsibility for smaller
systems to others in the Computation Division.7
Cautious toward the new arena of computer networking, Roger Lazarus oversaw the
construction of an experimental remote-access system initiated under T Division. Called
“MUX,” the Multiple User eXperiment was meant to determine the utility and viability
of remote computer connections at Los Alamos. MUX provided up to 75 users the ability
to run batch jobs on one of the Lab’s CDC 6600 supercomputers via remote terminals
at 300bits/s. Its speciﬁcations completed in 1967, and the system entering use in 1968,
MUX consisted of about $50,000 worth of hand-built hardware and custom application
software, using 8,000 integrated circuits and two miles of twisted-pair cable. The custom
MUX application software resided on the 6600 and one of its Peripheral Processing
Units (PPU), which handled I/O operations for the CPU. The application placed remotely
submitted jobs into the 6600’s job queue, then returned completed jobs to users via the
terminal interface built by the MUX team. MUX was not a time-sharing system, as users
could not interact with their jobs as they ran on the 6600, but it oﬀered a platform for
introducing and testing time-sharing-like capabilities in the Lab setting. Ron Christman,
one of MUX’s three original developers, recalled that the system was reliable to the
point that, when it did fail on rare occasions, its maintenance staﬀ had to “re-learn” how
MUX worked in order to repair it. The experience with MUX, as a tentative ﬁrst step
7 H. Butler, MP-1, and T. Gardiner, P-1, to Edward Voorhees, CADP, 11-29-66, “Delay in
Procurement of Time-Share Service,” Edward A. Voorhees Papers, Box 35, Folder 2, Series
27, LASL Computer Needs, LANL Archives; Bill Collins, interview by Nicholas Lewis,
Minneapolis, MN, February 19, 2015; Michael, George, and Marilyn Ghausi. "An Interview
with Sidney Fernbach." An Interview with Sidney Fernbach. Accessed March 20, 2016. http://
www.computer-history.info/Page1.dir/pages/Fernbach.html. Information regarding Sidney
Fernbach's potential blindspot for smaller computer systems is derived from George Michael's
introduction to Marilyn Ghausi's February 1989 interview with Fernbach. Fernbach stepped
down as the Livermore Computation Division leader in 1982, with Fernbach arguing that he
had been removed. Fernbach at Livermore had heavily backed the development of the Control
Data STAR-100 supercomputer in the early 1970s, but the poor performance of that system
(of which Livermore purchased two, while Los Alamos backed out of its contract to purchase
a single STAR) had consequences for Fernbach’s position, and for the relative freedom of the
two labs in their selection of new computing systems.
The Homegrown Los Alamos Integrated Computer Network
5

into networking technology, characterized C Division’s cautious approach to
networking development between the 1960s and 70s.8
MUX was enormously popular with users, and remained in use for over a decade.
While quite limited in its capabilities, MUX indicated to Lazarus and C Division that a
demand for computer networking existed at Los Alamos, and that the technology could
be implemented securely and reliably in the Lab setting. While Lab supercomputer users
were, indeed, enthusiastic over the potential of larger-scale networking at Los Alamos,
MUX itself did not fully prepare C Division, despite its usual caution, for the technical
challenges of building a computer network. Even with inside knowledge of a similar
networking eﬀort at another American weapons lab, Los Alamos wrestled with many
of the same problems facing other early computer networking sites, but with the added
complications introduced with the niche requirements of supercomputing at a secure
facility.9
4
Hydra
In 1970, with the success of MUX, Lazarus authorized development of the Lab’s ﬁrst
computer network. The network came online in 1974, one year later than planned. Ron
Christman, also a designer of the new network, indicated that the delay was largely due
to an overly ambitious early design, and insuﬃcient coordination within the develop‐
ment team for such a complex system. The relative simplicity and small scale of MUX
did not prepare the C Division networking team for the challenges of interconnecting
multiple supercomputers in the Central Computing Facility (CCF). The resulting
network was, by necessity, greatly scaled-back from the original plan, with proposed
features like graphical interface capabilities stripped from the ﬁnal design. Called
“Hydra”, the network initially connected each of the Lab’s four CDC 7600 supercom‐
puters to a shared pool of short-term, long-term, and archival storage systems through
a central front-end machine, itself a Control Data 6600 supercomputer. The front-end
machine, doing no production work of its own, also managed remote terminal connec‐
tions to the four 7600s. Although the original plans called for a new computer to serve
as the front-machine, C Division opted to convert one of its production 6600s for use
on Hydra due to an initial lack of funds. The one 6600, itself, represented only a small
portion of the Lab’s overall computing capacity. Christman argued at the time of Hydra’s
development that the beneﬁts of more ﬂexible, shared storage options would outweigh
the small loss in processing power. Roger Lazarus agreed.10
8 Ronald D. Christman, “MUX, a Simple Approach to On-Line Computing,” Communications
of the ACM 15, No. 5 (May 1972), 319-329; Jeﬀery L. Peterson, “MUX, adieu, faithful friend,”
The Atom (October 1979), 14-16; Ronald D. Christman, interview by Nicholas Lewis, Los
Alamos, NM, July 2, 2014.
9 Peterson, “MUX, adieu, faithful friend,” 14-16; Christman, interview by Nicholas Lewis, 2014.
10 Ron Christman, “A Review of the Hydra Project,” (Los Alamos, NM: Los Alamos Scientiﬁc
Laboratory, 1974); “The Computer Science and Services Division Annual Report,
January—December 1975” (Los Alamos, NM: Los Alamos Scientiﬁc Laboratory, LA-6228-
PR, 1976), 5.
6
N. Lewis

Lawrence Livermore, beginning development in the mid-1960s, constructed a
similar network before Hydra, called “Octopus,” which used two DEC PDP-6 computers
as its front-end machines. Despite the historical rivalry between Los Alamos and
Lawrence Livermore, the two nuclear weapons design labs of the AEC complex, their
computing divisions maintained (and still maintain) collaborative relationships on the
development, purchase, and usage of HPC systems, as both labs had similar computing
requirements and faced similar technical problems. However, particularly between the
1950s and 1980s, the two often followed somewhat diﬀerent technological paths, as in
the case of networking development, due to diﬀerences in funding, lab-speciﬁc goals,
particular demands from users, and styles of leadership. Livermore was founded in the
1950s largely to stimulate a competitive environment with Los Alamos, with either lab
pursuing similar lines of R&D, but from slightly diﬀerent philosophical angles. These
tendencies were reﬂected in their approaches to computing. Los Alamos C-Division
staﬀ remained apprised of the problems with Livermore’s Octopus through publications
and inter-lab technical meetings, and attempted to avoid similar issues with Hydra.
Hydra had fewer initial performance and reliability issues than did Octopus, partly due
to the higher performance of the CDC 6600 as the Hydra front-end machine, and the
lessons learned from Livermore’s earlier foray into centralized-storage networking.
However, both custom networks had similar drawbacks for maintenance, operability,
and reliability. The total dependence on a central front-end machine was the single
largest drawback for network reliability. C Division prepared one of the production
6600s in the Central Computing Facility (CCF) as a backup for the Hydra front-end
machine, allowing the other CDC computer to take over if the primary 6600 failed, but
that solution could not completely address the inherent deﬁciencies in Hydra’s conﬁg‐
uration, only minimize their impact.11
The purpose of Hydra, a “star” configuration network, was to reduce the redundancy
of data storage purchased for each of the Lab’s supercomputers, and to allow users to
access their files on any of the machines in the CCF, regardless of whichever machine they
had used previously. Aside from the Control Data channel couplers, the point-to-point
interconnects which C Division had repurposed to link each of the supercomputers to the
Hydra front-end machine at 12Mbits/s, the network was entirely custom to Los Alamos,
including the Hydra OS, which replaced the CDC Scope operating system on the 6600
front-end machine. After the MUX development team found the Scope OS to be unreli‐
able and difficult to adapt to the needs of the project, C Division opted to pursue its custom
OS for Hydra, despite the greater development costs. Data storage on Hydra consisted of
CDC disk units for short- and long-term storage, and the IBM 1360 “Photostore”, a
massive archival system that stored data on photographic film chips. The Photostore, one
of only six IBM constructed, provided about one-terabit of storage in total.12
11 “The Computer Science and Services Division Annual Report, January—December 1975,” 5;
William J. Worlton, Lab Notebook (Los Alamos, NM: Los Alamos Scientiﬁc Laboratory,
4/6/71), 27.
12 “The Computer Science and Services Division Annual Report, January—December 1975,” 5-6,
9; Christman, “A Review of the Hydra Project”; Christman, interview by Nicholas Lewis, 2014;
John Morrison, interview by Nicholas Lewis, Los Alamos, NM, 7/1/2014.
The Homegrown Los Alamos Integrated Computer Network
7

4.1
The Homegrown Tradition
Retired Lab networking experts Don Tolmie and John Morrison recalled that C Division
opted to develop its custom networking and storage system because the commercial
market for supercomputer networking did not yet exist in the early 1970s. Los Alamos
and Livermore were among the few civilian facilities at that time pursuing the kinds of
data speeds, volumes, and security needed for interconnecting supercomputers for clas‐
siﬁed work. This was not an unusual situation at the Laboratory. In the early 1950s, the
digital computing industry was still in its infancy, with very few computers in existence
worldwide. In addition to its modiﬁed IBM accounting machinery and a Hand Calcu‐
lation Group of human computers, Los Alamos had spent the immediate post-war years
utilizing whatever available computer resources existed outside the Lab, such as the
ENIAC in both Pennsylvania and Maryland, and IBM’s electromechanical SSEC
machine in New York. The most demanding of these calculations were usually in support
of the hydrogen bomb project. However, traveling great distances for computer access,
particularly to machines that were not typically well suited to the types of calculations
Los Alamos performed in the early Cold War, was not a practical solution.13
In the 1950s, Los Alamos cultivated a native capacity for developing and maintaining
its own computing solutions. For example, the MANIACs I and II, from 1952 and 1957,
respectively, were ﬁrst-generation digital computers constructed at Los Alamos. These
machines provided computing capacity for the Lab when vendor solutions were still
relatively scarce. Just as important, the intimate knowledge of digital computing gleaned
from developing the MANIACs also provided the Lab community with in-depth knowl‐
edge of a mission-critical technology, which continued forward as Los Alamos transi‐
tioned to the maturing array of vendor solutions that appeared from the mid-1950s
through the early 60s. The Lab continued to customize vendor hardware as needed, and
to create bespoke devices when no others were available. T and then C Divisions of the
1950s and 60s developed much of their own software in-house, due to the relative dearth
of software for the niche scientiﬁc-computing market, and to maintain a local capacity
for producing software tailored to the Lab’s requirements. Much was the case in the
1970s with computer networking. C Division opted to pursue its own networking solu‐
tions to meet the Lab’s unusual requirements while the supercomputer-networking
industry was young and provided few ready-made options. Pursuing technically chal‐
lenging custom solutions, according to the C Division leadership of the 1970s, also
fostered local networking expertise, and aided the division in attracting new talent.14
13 Morrison, interview by Nicholas Lewis, 2014; Don Tolmie, interview by Nicholas Lewis, Los
Alamos, NM, 7/14/2014; The ﬁrst ever calculation on the ENIAC was a Los Alamos feasibility
study of the hydrogen bomb. The calculation was less than ideal, because ENIAC's limited
memory forced Los Alamos to simplify the variables of the problem to the point that the results
were mostly inconclusive. However, Los Alamos would use ENIAC repeatedly in the 1940s,
due to the scarcity of digital computers in the early Cold War years.
14 Christman, interview by Nicholas Lewis, 2014; MacKenzie, “The Inﬂuence of the Los Alamos
and Livermore National Laboratories,” 186.
8
N. Lewis

5
The Integrated Computer Network
C Division began to modify the centralized Hydra network soon after it ﬁrst entered
service, replacing the 6600 with a CDC Cyber 73 as the front-end machine in 1975,
when the funding became available. C Division also took advantage of the growing
range of minicomputers available on the market to augment network services. Mini‐
computers became the Swiss Army knives of Lab networking, with C Division custom‐
izing UNIX-powered DEC PDP-11s, and later DEC VAX, SEL, and Gould minicom‐
puters, to serve in a variety of roles. For example, in order to connect remote users to
Hydra, Los Alamos developed the KCC (Keyboard Communications Concentrator), a
custom device based on a PDP-11 and DEC interface unit to provide a single connection
to the Hydra front-end machine for up to 96 keyboard-based terminals. The CCC
(Computer Communications Concentrator) provided a similar connection for up to 18
computer-based terminals, which were usually other DEC minicomputers.15
In 1975, when the number of keyboard-based terminals in use exceeded the capacity
of a single KCC, C Division developed the PDP-11-based Synchronous Concentrator
(SYNC). The SYNC served as a concentrator and message switcher to interconnect the
KCCs with the Hydra front-end machine. More importantly, the SYNC provided direct
terminal connections with each of the networked supercomputers in the CCF. Providing
direct terminal access to the “worker” supercomputers reduced the centralization of the
Hydra system, which C Division increasingly recognized as an unreliable network
model. The CDC channel couplers proved to be the single most unreliable component
of Hydra, as they were never intended to connect more than two CDC computers
together. Troubleshooting a connection error required maintenance staﬀ to take both the
originating machine and the Hydra front-end computer oﬄine.16
As frustrating as Hydra’s shortcomings were to users and to C Division, including
its problematic interconnect technologies, a front-end machine that CDC had not
intended to be a storage or terminal-access controller, and an interface that was
unfriendly to users, they oﬀered a hands-on learning experience with the complexities
of networking in the Lab environment, just as the Lab’s home-built computers had with
digital computing in the 1950s. Having a clearer understanding of what did and did not
work well at Los Alamos, and in supercomputer networking in general, C Division’s
networking team developed a network plan for the remainder of the 1970s. The plan
emphasized a reduced dependency on Hydra and the eventual phasing-out of the Hydra
storage system completely. The plan also called for an increase in network speeds to
support supercomputers with faster I/O capabilities, such as the new Cray-1, and
15 “The Computer Science and Services Division Annual Report, January—December 1975,” 5;
Fred W. Dorr, “Computer Science and Services Division of the Los Alamos Scientiﬁc Labo‐
ratory” (Los Alamos, NM: Los Alamos Scientiﬁc Laboratory, LA-UR-74-1566, 1974), 2; Leo
Romero and Bill Buzbee, “Overview of the Los Alamos Integrated Computing Network” (Los
Alamos, NM: Los Alamos Scientiﬁc Laboratory, LALP-85-34, 1985), 23-24.
16 Romero, “Overview of the Los Alamos Integrated Computing Network,” 23; “The Computer
Science and Services Division Annual Report, January—December 1975,” 5-6; Christman,
interview by Nicholas Lewis, 2014.
The Homegrown Los Alamos Integrated Computer Network
9

growing the network to accommodate a variety of new services and users. The plan
meant adding new transport, switching, and security systems, all of which would be
custom to Los Alamos, primarily due to the relative infancy of the high-performance
computer networking industry as the 1970s came to a close.17
One of the ﬁrst products of C Division’s new networking plan was the Los Alamos
File Transport machine, introduced in 1977. While the SYNCs provided complete
terminal access to the supercomputers of the CCF independent of Hydra, Los Alamos
lacked similar direct connectivity for ﬁle access. The File Transport (FT) machine used
the same architecture and low-level protocols as the SYNC to provide high-speed data
links of up to 40Mbit/s between the CCF supercomputers. The ﬁrst FT linked together
the Cray-1 and two CDC 7600s. Additional FT machines formed a File-Transport
Network (FTN), creating a high-speed backbone for direct ﬁle transfers, which reduced
the need for routing data through the Hydra front-end machine. The FTN was also
important for serving the rapidly growing number of high-speed graphics terminals at
the Lab. The new terminals, once connected to the FTN, allowed users to stream color
graphical data at 150kbits/s from the supercomputers of the Central Computing Facility
directly to their oﬃces. Conventional terminals operated at only about 9.6kbits/s.
Improving support for a growing range of small computing systems would become an
important theme in Los Alamos network development over the following decade. While
the Lab network would grow far beyond its original size and complexity as the 1980s
began, reorienting to accommodate small-scale computing, in addition to adapting to
more powerful and data-intensive supercomputers in the CCF, would pose a signiﬁcant
challenge to C Division, as it often meant rethinking how and why computer networks
were used at Los Alamos. Those challenges were only emerging during the ICN’s ﬁrst
great expansion, taking place in the late 1970s.18
5.1
ICN Security in the Late 1970s
Increasing the range of services and users on the new Integrated Computer Network in
the late 1970s required a signiﬁcant reconﬁguration of network security. The original
Hydra network had a simple approach to the complex issue of security at a weapons
laboratory; only classiﬁed users were allowed on the network. In order to accommodate
unclassiﬁed supercomputer users, and to facilitate administrative Lab functions across
the network, C Division developed a custom, minicomputer-based Network Security
Controller (NSC) to monitor and restrict ﬁle and resource access based upon user pass‐
words. The NSCs separated the ICN into three partitions: Secure, Administrative, and
Open. Several of the supercomputers in the CCF were moved to the Open partition for
17 Christman, interview by Nicholas Lewis, 2014; Fred W. Dorr, “Computer Science and Services
Division CCF Five-Year Plan” (Los Alamos, NM: Los Alamos Scientiﬁc Laboratory,
LA-6921-MS, 1977), 3-4, 13, 43-45, 79; Fred W. Dorr, “Computer Science and Services
Division Activities and Plans” (Los Alamos, NM: Los Alamos Scientiﬁc Laboratory, LA-7093,
1978), 4, 12, 19.
18 Romero, “Overview of the Los Alamos Integrated Computing Network,” 24; Morrison, inter‐
view by Nicholas Lewis, 2014.
10
N. Lewis

unclassiﬁed use, while the Administrative partition users had dedicated CDC Cyber 73
mainframes. Users of the Administrative or Open partitions could share ﬁles with users
on the Secure partition, but not the other way around, in order to prevent a “write down”
of classiﬁed data to a lower classiﬁcation level. Each data packet ran through two NSC
checks, providing redundancy to guard against error. The NSCs did not run any user
code, and prevented the need to rely upon worker machines to enforce security rules.
Using the NSC to monitor and control network security on the single physical network
was far less costly than developing separate networks for each security level, allowing
classiﬁed and unclassiﬁed users to share the same resources.19
5.2
The Los Alamos High-Speed Parallel Interface
The ICN jumped from about 2,200 mostly local users in 1977 to over 6,000 by the
mid-1980s, with an additional 2,000 users remote to Los Alamos. As networked
resources became more widely used and more mission-critical to the Laboratory, the
demands of connecting larger numbers of machines and services to the ICN grew more
diﬃcult and time consuming. The various supercomputers, minicomputers, storage
systems, printing services, and other devices employed at the Lab were never intended
to function together on a single network. A variety of incompatible interconnect tech‐
nologies and proprietary communication protocols meant that C Division had to develop
expensive hardware and software conversion systems that could translate the native
I/O of one machine into that of every other device with which it needed to connect on
the network. This engineering task grew increasingly impractical as the network became
larger and more heterogeneous as the 1970s drew to a close. C Division’s solution was
one of the ﬁrst eﬀorts toward network interface standardization in the supercomputing
arena.20
Beginning development in 1978, the Los Alamos High-Speed Parallel Interface
(HSPI) converted the native I/O of a computer or another networked device into the
standard ICN higher-level protocol suite, which was also developed at Los Alamos. The
suite primarily consisted of SIMP (Simple Inter-Machine Protocol), PTP (process-to-
process), and AFT (File Transport), which would remain in use for well over a decade,
much like the HSPI. The custom Lab protocols were designed to accommodate large
data blocks with low latency, high burst rates, extensive error detection and correction,
and minimal message passing. C Division had to design and construct a HSPI, including
the hardware and drivers, for each new device placed on the network, but each HSPI
only had to convert the native I/O interface and protocols of the new device into the
standard ICN protocols. The HSPI of each receiving device then converted the ICN
19 Romero, “Overview of the Los Alamos Integrated Computing Network,” 24; Robert H. Ewald,
et al., “Computing Division Two-Year Operational Plan, FY 1981-1982” (Los Alamos, NM:
Los Alamos National Laboratory, 1981), 41-42.
20 Dorr, “Computer Science and Services Division Activities and Plans,” 5; Ewald “Computing
Division Two-Year Operational Plan, FY 1981-1982,” 36; Romero, “Overview of the Los
Alamos Integrated Computing Network,” 43; Tolmie, interview by Nicholas Lewis, 2014;
Andrew and David Dubois, interview by Nicholas Lewis, 7/17/2014.
The Homegrown Los Alamos Integrated Computer Network
11

protocols back into that device’s native I/O. Once connected to its HSPI, a device had
full access to the entire Los Alamos network, which signiﬁcantly reduced the time
required to integrate new equipment onto the ICN.21
HSPIs provided full-duplex point-to-point transfers to the various machines of the
CCF, with error correction integrated into the HSPI hardware. The HSPI supported data
transfers of up to 60Mbits/s, which was intended to exceed the I/O capabilities of the
fastest devices on the network, so that the network interfaces would not act as perform‐
ance bottlenecks. In order to accommodate diﬀerent data rates (for example, 3Mbits/s
for the PDP-11, and 50Mbits/s for the Cray-1), a handshake between HSPIs determined
the maximum data rates of the exchanging machines. The HSPI employed hardware
error detection and correction while data was on the ﬂy, so as not to slow the data transfer.
When double-bit errors were detected, the HSPI hardware aborted the transfer. The ﬁrst
completed HSPI was for the ubiquitous PDP-11, with others following in short succes‐
sion in the 1970s and 80s.22
5.3
CFS and the ICN of the 1980s
The late 1970s’ introduction of higher networking speeds, new security partitions, and
standardized interfaces and protocols on the Integrated Computer Network coincided with
the end of Hydra in 1979, completing the Lab’s transition to a modular network configu‐
ration. Replacing Hydra as the networked storage system on the ICN was the Common
File System (CFS), which represented a middle ground between vendor-sourced
networked storage systems and the entirely custom Hydra system. The networked-storage
market had improved significantly in the years after Hydra’s deployment, and C Division
was able to use off-the-shelf IBM storage hardware and operating systems, rather than
developing its own. IBM disc units replaced older Control Data storage devices, and the
tape-cartridge-based IBM 3850 replaced the Photostore. While CFS still required a central
storage controller, unlike the repurposed CDC front-end computers of Hydra, the CFS
controller was designed and used specifically for storage. The CFS hardware offered a
maximum data transfer rate of about 5Mbits/s, and stored about 384 gigabytes of
combined online and offline data on the network as of 1980, with a growth rate of about
175 gigabytes per year. While the hardware and operating system were vendor-sourced,
saving C Division considerable development time and cost, the division had to develop in-
house the CFS application and interface software, as vendors in the HPC space still did not
21 Donald E. Tolmie, et al., “Interconnecting Computers with the High-Speed Parallel Interface”
(Los Alamos, NM: Los Alamos National Laboratory, LA-9503-MS, 1982), 2-12; Ewald
“Computing Division Two-Year Operational Plan, FY 1981-1982,” 43; Granville Chorn, et al.,
“The Standard File Transport Protocol” (Los Alamos, NM: Los Alamos Scientiﬁc Laboratory,
LA-7388-MS, 1978).
22 Norman R. Morse and Joseph L. Thompson, “Bringing A Large Computer Network Into
Focus” (Los Alamos, NM: Los Alamos Scientiﬁc Laboratory, LA-UR-82-83, 1982), 8-11;
Tolmie, “Interconnecting Computers with the High-Speed Parallel Interface,” 1-5.
12
N. Lewis

offer a complete package needed for tying together their storage devices with the complex,
high-performance network environment of a national laboratory.23
After the introduction of CFS, the C Division leader, Robert “Bo” Ewald, noted a
drop in the number of tapes at the Central Computing Facility, as the storage system
reduced the number of redundant, partially ﬁlled tapes in circulation. Much of the CFS
development team originally worked on the Hydra project, and had feared a repeat of
the design diﬃculties that had delayed the rollout of Hydra. As a consequence, the team
leaders invested in a “structured” design process of allocating and reviewing program‐
ming work. Individual programmers on the project were assigned speciﬁc parts of the
CFS coding eﬀort, with their code being checked at least three times by other program‐
mers for errors and eﬃciency before the code was added to the CFS application suite.
The process itself was time-consuming, but team members recalling the eﬀort cite it as
a key reason for the on-time, unusually smooth roll-out of CFS, and its long-term reli‐
ability.24
To the astonishment and delight of the CFS development team, Los Alamos
network users, who were never shy about voicing their dissatisfaction over the prob‐
lems and perceived shortcomings of the Lab’s network services, wrote letters of
praise to CFS for its on-time delivery, reliability, and intuitive, platform-agnostic
interface. The transition to CFS concluded with the Photostore, which IBM no longer
supported, being placed into a read-only mode until 1980, so that users could move
their remaining files before Hydra was finally shutdown. With its success at Los
Alamos, General Atomics Corporation sold CFS commercially as DataTree. The CFS
development team aided in creating or reconfiguring components of the CFS appli‐
cation suite for use in a variety of supercomputing environments. Over the next
decade CFS, as DataTree, entered use at several dozen data-intensive computing sites
worldwide. Los Alamos networking personnel often visited sites, including those
overseas, that had purchased the DataTree system, and aided in its installation, opti‐
mization, and personnel training for that site’s unique requirements. CFS, and its
success at and beyond Los Alamos, during the 1980s represented a transitionary
period in networked data storage, when the number of data-intensive computing sites
had grown markedly since the development of Hydra in the early-to-mid 1970s. CFS
also marked the important role that Los Alamos would play in networked storage
development, with the Lab collaborating with partners in industry and academia on
23 Bill Collins, interview by Nicholas Lewis, Minneapolis, MN, 2/19/2015; Christman, interview
by Nicholas Lewis, 2014; Bill Collins, Marjorie Devaney, and David Kitts, “Profiles in Mass
Storage: A Tale of Two Systems,” Ninth IEEE Symposium on Mass Storage Systems, 1988.
Storage Systems: Perspectives (Oct. 31, 1988 – Nov. 3, 1988), 61-67.
24 Christman, interview by Nicholas Lewis, 2014; Ewald “Computing Division Two-Year Opera‐
tional Plan, FY 1981-1982”, 37.
The Homegrown Los Alamos Integrated Computer Network
13

many other high-speed and high-capacity storage systems, such as HPSS and
Panasas, from the 1990s to the present.25
By 1981, the Los Alamos ICN consisted of several modules. The modules included
the “worker” machines (the nine supercomputers in the CCF, which performed the bulk
of the scientiﬁc computing work at the Lab), the Common File System (which allowed
the supercomputers to store and share ﬁles via the network), the Terminal Network
(connecting terminal users from the Secure, Administrative, and Open security partitions
to the ICN), and PAGES (a service center that allowed users of the ICN to print to shared
laser printers, plotters, or other graphical media). The various modules were intercon‐
nected through the File Transport Network, which provided the ICN’s high-speed data
transfer capability. Two additional modules catered to diﬀering levels of production
control on the network. FOCUS, a VAX-11-based control interface, allowed operators
to monitor and manage the Lab’s Cray and CDC 7600 supercomputers via the ICN. The
ﬁnal module, XNET (eXtended Network Access System), marked the beginning of the
Lab’s incorporation of outside protocols onto portions of the ICN. Beginning in 1979,
the XNET module brought “distributed processing” to Los Alamos, that is, support for
user-operated minicomputers on the network, which a growing number of ICN users
were requesting as the 1970s drew to a close. Text-based and graphical terminals
performed no production work of their own, and simply connected remote users to the
supercomputers of the CCF. VAX-11 minicomputer “distributed processors” allowed
users to perform production work locally, or on a remote VAX machine, while retaining
full access to the resources of the ICN. Most XNET users were local to Los Alamos, but
some were located in other states. Smaller-scale computing complicated the structure
and philosophy of computer networking at Los Alamos, as it did elsewhere, with small,
but powerful systems performing an increasing amount of production work that
normally would have been done on the Lab’s larger computers. C Division invested
signiﬁcant time and research energies into supporting smaller computing systems in a
25 M. Blood, R. Christman, and B. Collins, “Experience with the LASL Common File
System,” Fourth IEEE Symposium on Mass Storage Systems (New York: The Institute of
Electrical and Electronics Engineers, Inc., 1980); Bob Ewald, “Overview,” User Interface
67 (May 1981), 3-4; Collins, interview by Nicholas Lewis, 2015; Gary Grider, interview
by Nicholas Lewis, Los Alamos, NM, 8/21/2015; Fred Mcclain, "DataTree and UniTree:
Software for File and Storage Management," Digest of Papers. Tenth IEEE Symposium on
Mass Storage Systems: Crisis in Mass Storage (1990): 126-28. Lawrence Livermore
National Laboratory also developed its own networked-storage system, which sold commer‐
cially as UniTree. As the name indicates, UniTree ran atop the Unix operating system,
while Los Alamos’ DataTree/CFS relied on IBM’s MVS OS. While MVS was a more
mature platform in the late 1970s, when both networked-storage solutions were under
development, it left DataTree/CFS tied to IBM operating sytem development and support
at a time when the national labs and supercomputing in general were moving toward Unix
in the late 1980s and 1990s.
14
N. Lewis

variety of capacities on the ICN over the 1980s, while attempting to maintain the overall
performance and security of the larger network.26
The VAX distributed processors initially used the proprietary DECnet protocol to
communicate with the XNET concentrators, forming a smaller network of minicom‐
puters on a module of the larger ICN. The use of DECnet on the XNET module oﬀered
the Lab several advantages. For one, it allowed Los Alamos ICN users access to the
latest tools available from vendors, such as smaller-scale computing and networking,
without requiring the Lab to develop its own, costlier solutions. Los Alamos already
followed a similar, but custom path with its terminal access, as it continually upgraded
the terminal network module to coincide with improvements in vendor-supplied termi‐
nals over the 1970s. However, upgrading the custom system took longer, and oﬀered
no vendor support. The XNET module also allowed Los Alamos to experiment and gain
experience with outside networking advancements, such as the proprietary DECnet
system, while retaining its custom, mission-critical portions of the ICN.27
In the early 1980s, Los Alamos became part of the broader networking community,
establishing links in 1983 with multiple external networks, including a purchased link
to GTE’s Telenet, a commercial packet-switching network. Connecting with external
networks brought resources to Los Alamos from beyond the Lab, and allowed easier
collaboration and information access for remote users. The Telenet connection, for
example, allowed Open partition users access to globally distributed computing and
information resources. Telenet also oﬀered remote dialup access to the ICN for a
substantially lower cost than a dedicated phone line. 1983 also marked the Lab’s ﬁrst
link with ARPANET, the Department of Defense’s packet-switching network of univer‐
sities, research institutions, and unclassiﬁed DoD traﬃc. C Division connected an
ARPANET IMP (Interface Message Processor) to a custom VAX gateway, which trans‐
lated ARPANET’s TCP/IP traﬃc into the ICN’s local protocols. For security purposes,
traﬃc from external networks connected only to the Open partition of ICN.28
Although TCP was being forcibly rolled-out onto ARPANET in 1983, and was
spreading elsewhere due to ARPA and the Department of Defense incentivizing its
adoption among outside networks and computing vendors, Los Alamos had little interest
in TCP/IP in the early 1980s. The ARPANET suite had little to oﬀer in terms of
performance, security, or compatibility with systems in use on the Lab network. Indeed,
maintaining a custom-networking environment oﬀered the Lab distinct security advan‐
tages, as few outsiders were familiar with the Los Alamos network or its largely bespoke
systems and protocols. However, as with changes in supercomputing technologies, the
26 Robert H. Ewald, “Computing Division Two-Year Operational Plan, FY 1983-1984” (Los
Alamos, NM: Los Alamos National Laboratory, LA-9712-MS, 1983), 43-52; Ewald,
“Computing Division Two-Year Operational Plan, FY 1981–1982,” 27, 31-38; Romero,
"Overview of the Los Alamos Integrated Computer Network," 24.
27 Ewald, “Computing Division Two-Year Operational Plan, FY 1983-1984,” 67; Ewald,
“Computing Division Two-Year Operational Plan, FY 1981–1982,” 39-40; Romero, "Over‐
view of the Los Alamos Integrated Computer Network," 24.
28 Morrison, interview by Nicholas Lewis, 2014; Ewald, “Computing Division Two-Year Opera‐
tional Plan, FY 1983-1984,” 67.
The Homegrown Los Alamos Integrated Computer Network
15

Computing Division actively studied new networking technologies that were emerging
from the maturing computer networking community in the 1980s.29
While Los Alamos remained committed to its locally developed protocol suite for
mission-critical parts of the ICN, such as the worker machine and File Transport
modules, C Division was open to expanding its use of external protocols where they
oﬀered greater compatibility with the growing range of networked devices entering the
market. Such protocols and devices had the potential to reduce the high development
and maintenance costs associated with custom solutions. In 1983, C Division began
investigating the potential of the X.25 networking protocol for its distributed processor
network, as DEC was slated to incorporate X.25 support into its VAX VMS operating
system. As an international standard, X.25 was also promising as the Lab’s preferred
protocol for interfacing with commercial packet-switching networks, such as Telenet
and Tymnet. By 1985, C Division had converted XNET to run the X.25 protocol over
DECnet, with over 75 VAX distributed processors on XNET connecting to the ICN via
three, and soon four, gateways. After extensive security testing, the Lab allowed distrib‐
uted XNET processors access to the Secure network partition, bringing small-scale
computing to all network users at Los Alamos.30
The Lab’s connections with the wider networked world changed rapidly in the
mid-1980s. In 1984, the DoD separated MILNET from ARPANET. ARPANET retained
the university and research traffic, while MILNET linked together government, Defense
Department, and Department of Energy facilities, including Los Alamos. Los Alamos
established direct links to three other MILNET nodes. Each node connected to MILNET
via a gateway host computer. The Lab’s host allowed minicomputers on DECnet to
exchange e-mail, and enabled TCP/IP access to the open partition of the larger Depart‐
ment of Defense Network. In 1986, Los Alamos added a fourth security partition to the
ICN, the National Security partition, so that Department of Defense users who lacked
Department of Energy (DOE) clearances could have access to secure computing resources
separate from secure DOE computing work. Los Alamos moved one of its Cray-1A super‐
computers into the new security partition for the benefit of military users and Defense
contractors, like the Defense Nuclear Agency (DNA), SDI, and DARPA.31
As a site once selected for its geographical isolation, the Lab’s experience with
computer networking quickly brought Los Alamos into the center of collaborative
information processing, distribution, and management for the Department of Energy in
the 1980s. The Department of Energy’s WBCN (Wide-Band Communications Network)
and NWCNET (Nuclear Weapons Complex Network) projects exempliﬁed the Lab’s
place in bridging remote government computing sites. Begun in 1985, the DOE initiated
29 Abbate, Inventing the Internet, 140-143; Morrison, interview by Nicholas Lewis, 2014; Ewald,
“Computing Division Two-Year Operational Plan, FY 1983-1984,” 76.
30 Ewald, “Computing Division Two-Year Operational Plan, FY 1983-1984,” 47, 67; Robert H.
Ewald, “Computing Division Two-Year Operational Plan, FY 1984-1985” (Los Alamos, NM:
Los Alamos National Laboratory, LA-9978-MS, 1984), 100-101; Romero, "Overview of the
Los Alamos Integrated Computer Network," 25.
31 Abbate, Inventing the Internet, 142-145; Ewald, “Computing Division Two-Year Operational
Plan, FY 1984-1985,” 101-102; N. R. Morse, “C-Division Annual Review and Operating Plan,
January 1987” (Los Alamos, NM: Los Alamos National Laboratory, LA-10932-MS, 1987), 84.
16
N. Lewis

the two related networking projects to link together the computing resources of the
national Nuclear Weapons Complex, which consisted of Los Alamos, Lawrence
Livermore National Laboratory, Sandia National Laboratory, and eleven other research
and development sites. WBCN was the hardware component that allowed for secure
transmission of data among the sites of the nuclear weapons complex, and consisted of
DEC VAX-11/750 minicomputers that served as gateway controllers at each location,
along with accompanying encryption and communication hardware.32
WBCN initially ran over terrestrial circuits, which were replaced with satellite
circuits in the late 1980s. NWCNET comprised the basic tools used at each site for the
storage and transmission of weapons-related data across WBCN. The ﬁrst NWCNET
tool was CFS, the Common File System developed at Los Alamos, which stored and
managed the secure data at each location in the complex. Another tool was the software
that interfaced CFS with the WCBN, while a third tool was the software that allowed
the various NWCNET devices to communicate via Hyperchannel. A fourth tool allowed
for secure e-mail capability between the diﬀerent locations. The system ﬁrst linked Los
Alamos and the Rocky Flats weapons production facility in Colorado in 1986, with the
other sites following soon thereafter. Many of the components and services driving the
WBCN and NWCNET systems originated at Los Alamos. C Division produced a modi‐
ﬁed version of CFS that could be used at the other weapons complex locations, and
created a network interface for the IBM MVS operating system (the OS platform for the
CFS application suite), which allowed users and worker machines access to CFS across
a Hyperchannel link. Because of its extensive networking experience, Los Alamos
functioned as both the development site and the Tech Control for WBCN, testing the
network’s components on the Lab’s ICN, and ensuring the smooth operation of the other
WBCN nodes (see Footnote 32).
As Los Alamos became the lead development site for networking eﬀorts inside the
DOE complex, the Lab continued its onsite supercomputer-networking development,
which brought Los Alamos into the world of network standards setting. By the
mid-1980s, the Los Alamos HSPI, the custom network interface the Lab had employed
since the late 1970s, was reaching the end of its lifespan, as its 60Mbits/s throughput
could not accommodate the anticipated performance increases of the next decade. The
rapid increase in traﬃc across the ICN, and the introduction of new supercomputers with
much faster I/O capabilities, such as the Cray Y/MP, meant that Los Alamos would soon
require interface speeds far beyond what the networking industry oﬀered in the late
1980s. However, developing interfaces in-house for each device the Lab purchased was
expensive, and meant the Lab had to provide its own support. C Division had little choice
but to create custom interfaces when it developed the HSPI in the 1970s and early 1980s,
but vendors and standards-setting bodies associated with networking had evolved
considerably between that time and the late 1980s. C Division proposed a standard
32 Morse, “C-Division Annual Review and Operating Plan, January 1987,” 84; N. R. Morse and
B. L. Buzbee, “C-Division Annual Review and Operating Plan” (Los Alamos, NM: Los
Alamos National Laboratory, LA-10631-MS, 1986), 110-111.
The Homegrown Los Alamos Integrated Computer Network
17

interface for the supercomputing industry, which would provide an alternative to propri‐
etary vendor interconnects.33
6
The “Lunatic Fringe” and the HIPPI Standard
In 1987, a Los Alamos delegation, led by C Division networking expert Don Tolmie,
approached the American National Standards Institute (ANSI) Networking Task Group
with a proposal for the High-Performance Parallel Interface (HIPPI), the ﬁrst gigabit
network interface. Although ANSI initially labelled the Los Alamos group as “the
lunatic fringe,” because of the extreme performance it was proposing, ANSI soon formed
a new task group (the ﬁrst ever in the supercomputer industry) to pursue the Lab’s
interface. Developed at Los Alamos, the HIPPI became an ANSI standard in 1991, and
an ISO (International Organization for Standardization) standard soon thereafter,
marking the ﬁrst standard networking interface for supercomputers and their peripherals.
Much as the Lab’s collaborative development of the ﬁrst supercomputer with IBM
signiﬁed the maturation of digital scientiﬁc computing, the standardization of the HIPPI
represented a similar maturation of supercomputer networking in the 1980s. Since the
1970s, the number of institutions utilizing supercomputers, including industry and
centers of scientiﬁc research, had increased signiﬁcantly, as the computers themselves
became more compact and reliable, and as the software tools for using them improved.
Older, advanced scientiﬁc computing centers, such as Los Alamos, had endured the
challenges of early computing, which included the need to write locally much of the
software used on their machines. This barrier largely precluded the deployment of
supercomputers where the users lacked the ability or willingness to write their own
application software.34
By the mid-1970s, institutions like NCAR, the National Center for Atmospheric
Research, began to demand from vendors the inclusion of basic software packages
with the supercomputers they bought, as was the case with NCAR’s purchase of the
first commercially available Cray-1 in 1977. Advancing supercomputing hardware
and software, and the growing number and range of users, fed into a rapidly maturing
scientific computing environment by the 1980s, including the maturation of ancil‐
lary technologies, such as high-speed networking. With the Los Alamos HIPPI
standard in place, and with a growing range of customers to provide pressure on
vendors to use the standard, supercomputers and related devices began to ship with
33 Tolmie, interview by Nicholas Lewis, 2014; Don E. Tolmie and John K. Renwick, “HIPPI –
Simplicity Yields Success,” IEEE Network – The Magazine of Computer Communications
(November, 1992), 1-2, 4.
34 Tolmie, interview by Nicholas Lewis, 2014; Morrison, interview by Nicholas Lewis, 2014;
Don E. Tolmie, “What’s Happening with Supercomputer Networks” (Los Alamos, NM: Los
Alamos National Laboratory, LA-UR-91-2704, 1991), 1-3.
18
N. Lewis

the HIPPI interface already built-in, reducing the cost and difficulty of supercom‐
puter networking. In the 1990s, the HIPPI, and the high-speed networking intercon‐
nects it spawned and inspired, provided part of the vital networking infrastructure
that underpinned the development of cluster computing.35
As Los Alamos participated in the development and evolution of networking beyond
the Lab, its local ICN continued to evolve in the late 1980s, transitioning away from
custom protocols and systems, and toward the new range of industry standard solutions
entering the market. While distributed processors and external network connections
provided impetus for the Lab to explore X.25, the arrival of even smaller computing
platforms and an increasing reliance upon connections to external networks spurred
changes to the Los Alamos ICN in the second-half of the 1980s, along with preparations
for even more extensive changes in the coming decade. By 1985, Sun Microsystems
scientiﬁc workstations and IBM-compatible personal computers were rapidly entering
use throughout the Lab as lower-cost alternatives to minicomputer-based distributed
processors and computer-based terminals. C Division spent the latter-half of the 1980s
exploring whether Ethernet-based LANs (Local Area Networks) of small computers
could be utilized at Los Alamos and still maintain the integrity of the larger ICN.36
Scientiﬁc workstations and PCs initially operated as stand-alone units, while C
Division experimented with secure means of connecting them with the ICN’s custom
architecture. PCs soon entered use as terminals on the Terminal module of the ICN, but
only in the unclassiﬁed partitions. Between 1985 and 1987, the Lab connected work‐
stations and personal computers to the ICN by creating small LANs whose traﬃc went
through the VAX gateways used to interface with ARPANET. Routing traﬃc through
the ARPANET gateways was a temporary workaround solution, while the Lab investi‐
gated the routing and interface hardware needed to connect LANs more directly to the
larger network. In 1989, a KCC team developed a more permanent solution, creating a
new DEC MicroVAX-powered Keyboard Communications Concentrator that supported
ﬁle traﬃc between workstations and the ICN. C Division likewise enabled TCP/IP
compatibility on the XNET module for the distributed processor network, after DEC
had added TCP/IP support to its VAX operating systems in response to DoD paid incen‐
tives.37
Although Los Alamos had little interest in the DoD’s TCP/IP protocol suite in the
early 1980s, by the mid- to late-1980s, the suite had spread far beyond its original appli‐
cation on ARPANET. Los Alamos networking expert John Morrison later recalled the
issues with TCP/IP, particularly its poor performance with large ﬁle transfers, its lack
35 Norman R. Morse, “C-Division Annual Review and Operating Plan, August 1990” (Los
Alamos, NM: Los Alamos National Laboratory, LA-11945-MS, 1990), 6, 24, 60; Tolmie,
“What’s Happening with Supercomputer Networks,” 1-3.
36 Yvonne Martinez, et al., “Computing Division Two-Year Operational Plan, FY 1984-1985”
(Los Alamos, NM: Los Alamos National Laboratory, LA-9978-MS, 1985); Morse, “C-Divi‐
sion Annual Review and Operating Plan,” 34, 37, 49, 57.
37 Abbate, Inventing the Internet, 140; Ewald, “Computing Division Two-Year Operational Plan,
FY 1983-1984,” 81-82; Romero, "Overview of the Los Alamos Integrated Computer
Network," 9-10; Morse, “C-Division Annual Review and Operating Plan,” 17, 51, 66, 73, 110,
113.
The Homegrown Los Alamos Integrated Computer Network
19

of built-in security features, and its error-detection deﬁciencies, which initially
dissuaded the Lab from pursuing the protocol suite beyond developing gateways with
ARPANET and MILNET. However, the snowball eﬀect of TCP/IP’s adoption among
business, consumer, and government networks and computing systems expanded the
market for TCP/IP-compatible systems and devices, and vendors followed the demand.
As a consequence, in the late-1980s, C Division began to develop a strategy to convert
selected modules of the ICN to TCP/IP, and migrate away from the custom ICN protocol
suite, as the promise of practical vendor-supplied networking solutions outweighed the
deﬁciencies of the DoD protocols.38
In 1987, C Division began experimenting with building TCP/IP support into CTSS,
the Livermore and Los Alamos co-developed operating system used on the Lab’s Cray
supercomputers. The Lab soon dropped the proposed modiﬁcation of CTSS in favor of
converting its supercomputers to the Cray-sourced version of Unix, called UNICOS,
which supported TCP/IP. C Division also tested an experimental Ethernet network,
using TCP/IP to connect a DEC VAX to a Cray supercomputer through an IP router on
the Secure partition. Ethernet’s poor performance in the test gave further justiﬁcation to
pursue the HIPPI standard. The Lab eventually returned to Ethernet in the mid-1990s
as a solution for message passing between the supercomputers and their storage systems,
saving the faster HIPPI links for data transfer.39
7
The ICN2
The Lab spent the late 1980s and early 1990s preparing for a gradual transition of the
ICN away from custom protocols and devices as part of its “Network Modernization
Project.” Modernization was planned to occur in stages, proceeding from the least to the
most critical modules and systems, to avoid unexpected performance or security issues.
Once under way, the overall conversion was expected to take place over a decade or
more, but DOE intervention necessitated a more abrupt break with the homegrown Los
Alamos network. The resulting conversion to TCP/IP-compatible systems occurred as
one part of a total restructuring of the ICN architecture. In 1991, the DOE Oﬃce of
Safeguards and Security determined that the Lab’s partitioning of the ICN into Open,
Administrative, Secure/Classiﬁed, National Security, and Secure/Unclassiﬁed partitions
via custom security controller hardware and software would not meet new DOE-wide
security requirements. Safeguards and Security sought uniform network security poli‐
cies and enforcement across the Department of Energy, and custom systems were diﬃ‐
cult, if not impossible, to verify in accordance with standardized metrics. Faced with
the new DOE requirement, Los Alamos planned and constructed the “ICN2” between
1991 and 1996, which involved creating a physical separation between the Secure and
Open portions of the network. The Lab completed its transition to industry standard
38 Morrison, interview by Nicholas Lewis, 2014; Morse, “C-Division Annual Review and Oper‐
ating Plan, January 1987,” 79.
39 Morse, “C-Division Annual Review and Operating Plan, January 1987,” 73; Lynn Jones,
interview by Nicholas Lewis, Los Alamos, NM, 8/6/2015; Raymond Miller, interview by
Nicholas Lewis, Los Alamos, NM, 6-25-2014.
20
N. Lewis

systems as part of the restructuring, replacing the custom gateways and other bespoke
systems with vendor-supplied, TCP/IP-compatible components. Taking ﬁve years to
complete, the transition occurred rapidly, considering the size and complexity of the
Lab network, and the need to prevent disruptions of vital services. While not the gradual
transition Los Alamos had anticipated, the ICN restructuring provided the compatibility
with industry networking standards envisioned in the late 1980s.40
8
New Custom Solutions
Although the Los Alamos network was no longer comprised of mostly in-house systems,
custom networking solutions never completely disappeared. Awaiting the development
of its replacements, CFS remained in use until 2002, using a custom gateway to translate
the ICN’s TCP/IP protocols for the CFS controller. As it had done before, the Lab
returned to custom networking when vendor solutions were either inadequate or missing
from the market altogether. Before the term “cyber security” was coined, in the late
1990s Los Alamos developed its own high-speed security monitoring hardware to sift
through Web-based traﬃc passing through the ICN. A purely hardware-based solution
that split data traﬃc into multiple streams made it possible to screen the data for mali‐
cious packets without slowing network traﬃc. Beyond security, Los Alamos broke a
world speed record in 1994 when it completed a 792Mbits/s data transfer via a Lab-
developed HIPPI/SONET gateway to the San Diego Supercomputer Center, a distance
of over 1,200 miles. This experiment helped to push the boundaries of remote-computing
performance, allowing distant centers and users to share supercomputer resources. These
examples are characteristic of the Lab’s continuing ability and drive to pursue custom
approaches in order to explore and push boundaries, and to stimulate the research,
development, and application of new capabilities at and beyond Los Alamos.41
9
Conclusion
The homegrown Los Alamos Integrated Computer Network of the 1970s and 80s repre‐
sented a unique path of networking development, where the technical demands of a
nuclear weapons facility required the pursuit of custom solutions to problems that few
other computing centers had yet to encounter. Although unique, Los Alamos was not
40 Morse, “C-Division Annual Review and Operating Plan, August 1990,” 52; Hassan Dayem,
“C-Division Annual Review and Operating Plan, May 1992” (Los Alamos, NM: Los Alamos
National Laboratory, 1992), 54-55; Hassan Dayem, “Computing, Information, and Commu‐
nications, 1995-1996” (Los Alamos, NM: Los Alamos National Laboratory, LALP-96-35,
1996), 44.
41 Wally St. John and Dave DuBois, “Wide-Area Gigabit Networking: Los Alamos HIPPI-
SONET Gateway,” In Supercomputing '95: Proceedings of the 1995 ACM/IEEE conference
on Supercomputing, 1995; Dayem, “Computing, Information, and Communications,
1995-1996,” 45; Jones, interview by Nicholas Lewis, 2015; Grider, interview by Nicholas
Lewis, 2015.
The Homegrown Los Alamos Integrated Computer Network
21

isolated from the broader networking community. Just as it fostered intellectual and
ﬁnancial ties with computer vendors, researchers, and other users, the Lab played an
active role in outside networking development, lending its expertise to create and
disseminate technical innovations, and incorporating new technologies as they oﬀered
beneﬁts to ICN users. The Los Alamos network was not a linear development, changing
its technical and philosophical orientations in relation to varying internal and external
pressures, and sometimes adopting technically inferior solutions, such as TCP/IP, for
economic or bureaucratic expedience. Examining the Laboratory at Los Alamos high‐
lights how often-overlooked institutions experienced, and contributed to, computer
networking diﬀerently than the better-known cases from academia, industry, and the US
military, which serves to complicate and enrich the narrative of computer networking
history.
Acknowledgements. Special thanks to the HPC History Project, including: Gary Grider, Carolyn
Connor, Jeﬀ Johnson, and Fredie Marshall. Additional thanks to the Los Alamos National
Laboratory Archives. The HPC History Project gratefully acknowledges the support of the
Advanced Simulation and Computing (ASC) Program (FOUS Program Managers: Paul Weber,
acting; Jason Hick).
References
1. MacKenzie, D.: The inﬂuence of the Los Alamos and Livermore national laboratories on the
development of supercomputing. Ann. Hist. Comput. 13(2), 179–201 (1991)
2. St. John, W., DuBois, W.: Wide-area gigabit networking: Los Alamos HIPPI-SONET
gateway. In: Proceedings of the 1995 ACM/IEEE Conference on Supercomputing,
Supercomputing 1995 (1995)
3. Abbate, J.: Inventing the Internet. MIT Press, Cambridge (1999)
4. Russell, A.L.: Open Standards and the Digital Age: History, Ideology, and Networks.
Cambridge University Press, New York (2014)
5. DeNardis, L.: The Global War for Internet Governance. Yale University Press, New Haven
(2014)
6. Bashe, C.J., Palmer, J.H., Johnson, L.R.: IBM’s Early Computers. MIT Press, Cambridge
(1986)
7. Fitzpatrick, A.: Igniting the light elements: the Los Alamos thermonuclear weapon project,
1942–1952. The George Washington University (1999)
8. Haigh, T., Priestley, M., Rope, C.: Los Alamos bets on ENIAC: nuclear Monte Carlo
simulations, 1947–1948. IEEE Ann. Hist. Comput. 36(3), 42–63 (2014)
9. Michael, G., Ghausi, M.: An interview with Sidney Fernbach. http://www.computer-
history.info/Page1.dir/pages/Fernbach.html
10. Collins, B., Devaney, M., Kitts, D.: Proﬁles in mass storage: a tale of two systems. In: Ninth
IEEE Symposium on Mass Storage Systems, MSS 1998. Storage Systems: Perspectives, pp.
61–67. IEEE Press, New York (1988)
11. Tolmie, D.E., Renwick, J.K.: HIPPI – simplicity yields success. IEEE Netw. Mag. Comput.
Commun. 7, 28–32 (1992)
22
N. Lewis

MONET – Monash University’s Campus LAN
in the 1980s – A Bridge to Better Networking
Barbara Ainsworth1(&), Neil Clarke2, Chris Avram1,
and Judy Sheard1
1 Monash Museum of Computing History, Monash University,
Melbourne, Australia
{Barbara.Ainsworth,Chris.Avram,
Judy.Sheard}@monash.edu
2 Audiovisual and Networks Unit, Deakin University, Melbourne, Australia
Neil.Clarke@deakin.edu.au
Abstract. Monash University, Australia developed an in-house local area
network called MONET during the 1980s to meet the needs of the university’s
computer users. The Monash University Computer Centre team created and
installed an economical computer access network across an extensive campus
with distributed computer installations and a large numbers of users. MONET
was an early implementation of a Local Area Network (LAN) at a time when
LAN concepts were evolving and speciﬁc hardware and software for the pur-
pose did not yet exist. MONET became a successful large scale system that was
in development and then operation to support all the University’s computer
services for over a decade. It was also commercialized and used by various other
organizations.
Keywords: Computer history  Computer networks  LAN  Monash
University  MONET
1
Introduction
The growing demands of student and staff access to computer facilities in the 1960s
and 1970s stimulated the development of a terminal access Local Area Network
(LAN) at Monash University, Melbourne, Australia. It came to be known as MONET
for Monash University Local Area Network. The LAN was developed in-house by the
staff at the Monash University Computer Centre. The project was started in 1979 and a
prototype system was in place from 1981. The network continued to be used at Monash
University across several campus sites until it was superseded by an Ethernet network
from mid-1991.
During the 1970s, early computer networks consisted of individual computers
connected by point-to-point telecommunication links. The computers ran their own
proprietary operating systems, then later UNIX, and the network protocols evolved into
systems such as X.25 and TCP/IP, for example ARPANET. User access terminals were
limited to being directly attached to their local mainframe computer.
© IFIP International Federation for Information Processing 2016
Published by Springer International Publishing AG 2016. All Rights Reserved
A. Tatnall and C. Leslie (Eds.): HC 2016, IFIP AICT 491, pp. 23–48, 2016.
DOI: 10.1007/978-3-319-49463-0_2

With the proliferation of multiple, smaller mid-range computers onsite, this stim-
ulated a need for the user terminals to switch between the different computer systems.
Hence the emergence of centralized terminal exchanges similar to PABX systems. The
geographical distribution of these computers over a campus environment created the
further need for the switching system to be also distributed physically over the site.
MONET was Monash University’s response to this requirement.
The MONET system was developed at a time when the Ethernet LAN concept
existed but no Ethernet product had yet appeared. While contemporary networks at the
time provided computer-to-computer connectivity, MONET aimed instead to provide a
network to interconnect basic Visual Display Unit (VDU) terminals and host computers
distributed across the university campus. The original installation at Monash’s Clayton
Campus covered an area of just over 1 km2.
2
Early Computer Networks in Australia
Australian digital computing started in 1949 with the successful testing of a locally
designed computer which is now called CSIRAC. It was produced by the government
research group CSIRO – the Commonwealth Scientiﬁc and Industrial Research
Organisation.1 During the 1950s and 1960s, a growing number of systems were pur-
chased from English and American manufacturers by university and government
facilities as well as some commercial sites. They were stand-alone installations directly
accessed by local computer staff using punch cards or paper tape.
The ﬁrst forms of “networking” or communication between computers appeared in
the 1960s with the physical movement of magnetic tapes or packs of punch-cards
between locations. This was later replaced by electronic communication over tele-
phones lines. For example, the CSIRO established a system of CDC computers around
their ofﬁces in Australia in 1963 which initially communicated data by exchanging
tapes. This was the ﬁrst stage of a network called CSIRONET.2 The University of
Sydney linked their computers, a ﬁrst generation installation called SILLIAC and a
newer
KDF9
computer,
in
1966.3
In
1970
the
government
authority,
the
Postmaster-General’s Department (later split to form the Australian Post Ofﬁce and
Telecom Australia) commissioned UNIVAC to build a packet-switching network
called the Common User Data Network or CUDN. It was installed initially in their
Haymarket Exchange in Sydney in 1970 and then in the Lonsdale Exchange, Mel-
bourne in 1972.4
1 For further information on CSIRAC, see Museum Victoria, CSIRAC, http://museumvictoria.com.au/
csirac/.
2 Korporaal, G. AARNET-20 years of the internet in Australia. AARNET, Australia. 2009 p. 21.
3 Deane, John. “SILLIAC- vacuum tube supercomputer.” Science Foundation for Physics, in
association with the Australian Computer Museum Society, reproduced by University of Sydney
Physics Dept. Silliac. 2006 http://www.physics.usyd.edu.au/foundation.old/silliac/book/SILLIAC_
ch4_2ndEdition.pdf (accessed October 22, 2015).
4 Australian Science and Technology Centre 2000 “Technology in Australia 1788–1988.” p. 552
http://www.austehc.unimelb.edu.au/tia/552.html (accessed 21 November, 2015).
24
B. Ainsworth et al.

In the 1970s access to computer facilities moved away from cards or tape to the use
of teleprinter and then later VDU terminals. These terminals could be used to enter and
receive data but needed to be connected to a computer to process the data. They had
limited memory. Smaller, relatively cheaper computers, including ‘minicomputers’
such as DEC PDP models also became available from the 1970s into the 1980s. The
move to using terminal access rather than cards, coinciding with the expansion of
computer centres to include multiple computers as well as minicomputers and printers
away from the centre, prompted a need to communicate between large numbers of
different units within a local group. While universities were developing their own
resources, government bodies such as the Telecom Research Laboratories and other
research groups were also developing networks. Telecom Australia created a network,
called TACONET or Telecom Australia Computer Network by 1976 which connected
a set of Honeywell mainframes at their facility in Clayton.5 The Australian Atomic
Energy Commission, located at Lucas Heights, New South Wales, produced its own
network, known as AAEC DATAWAY Network, using their IBM/360-65 with a
PDP-9L, and with capacity for 128 addresses, in the late 1970s.6
3
The Development of Computing at Monash University
Monash University was established from 1958 with a campus at Clayton Victoria
Australia and has since expanded to include several other campus sites both locally and
internationally. Computing at Monash University started in 1962 with the installation
of a Ferranti Sirius computer at the Computer Centre, Clayton campus. Professor Cliff
Bellamy was appointed the Director of the Computer Centre in 1964. He had a diverse
role supervising staff involved in various activities including hardware and software
development, and maintenance of their operational computer services. Some of the staff
also had academic commitments.
The ﬁrst computer in the Centre soon proved to be too small for university
requirements and the Centre’s facilities were updated regularly over the next years with
a CDC3200 in 1964 and a series of Burroughs mainframes from the 1970s as well as
several Digital Equipment Corporation (DEC) VAX 11/780s and a smaller number of
VAX 11/750s from the late 1970s. The move from a single mainframe computer to
several computers created a need for the terminals to be switched between the different
systems with minimal human interaction. At this point, the University recognised that it
needed a network system to connect the distributed installations and allow each ter-
minal, physically spread around the campus, to connect to any of the computers. There
was no suitable commercial solution available so the Computer Centre formed a project
team to develop one.
5 Coxhill, R. “History of the Telecom Research Laboratories-Computing”, April 2015 http://www.
coxhill.com/trlhistory/history/history.htm (accessed 20 November 2015).
6 Cawley, R.J. & Trimble, G.D. “An interactive computing system for the AAEC Dataway Network.”
ANSTO Publications Online. 1977.
MONET – Monash University’s Campus LAN in the 1980s
25

4
MONET
The staff of Monash Computer Centre started work on a local area network, which
came to be known as MONET, in 1979. The team developed the project in-house. They
were aware of early pioneering local area networks developed at other universities such
as the ALOHAnet at the University of Hawaii and the Cambridge Ring at Cambridge
University Computer Laboratory.7 They were also aware of the concepts underpinning
emerging local area and packet switched network technologies such as Ethernet (which
grew out of ALOHAnet) and X.25. These were under active development in parallel
with MONET but, at the time, had not yet been ratiﬁed as formal standards, nor were
commercial products or services based on these ideas yet available.
The Monash team set out to create a cost-effective terminal access network using
8-bit microprocessors suitable for the large, widely-spread campus of Monash
University.8 A precursor project to develop a terminal concentrator to connect to their
Burroughs mainframe had been undertaken in 1978. This was built using a Motorola
6800 microprocessor development kit and communicated with the mainframe using the
Burroughs Poll/Select Protocol. This set the scene for the subsequent more ambitious
MONET project, as they now had familiarity and expertise with developing serial data
communications systems using the then available early microprocessor hardware and
their primitive assembler language programming. A memo written in April 1979 by
Keith Heale discusses the practical aspects of starting the project and the need for
money from the Centre’s budget to build some working prototypes.9 By late 1979 a
team had been established and included Dr Cliff Bellamy, Neil Clarke, Keith Heale,
Patrick Miller and Barry Treloar. Later contributors to implementing the network
software and hardware included Stephen Dart, Peter Gordon, Russell Keil, Neil
Houghton, John Mann, Keith Lewis, Carlo Kopp and a number of Computer Centre
engineering staff.
With their geographical and economic constraints in mind, initially the team
identiﬁed the following goals. Firstly to provide VDU terminal access to multiple
computers and also computer to computer connectivity in a distributed multi-computer
environment. The large distances on Clayton campus imposed a number of engineering
constraints, and they also wanted to incorporate off-campus modem connections via
existing Telecom telephone services. The network should have high reliability, and
tolerate effects from electrical storms over a wide area and problems with electrical
supply faults over different buildings. Reliability objectives included minimising the
effect of a failure of a single piece of equipment on the network as a whole. Ultimately,
the team wanted to achieve an efﬁcient network over a large campus with a low
connection cost per port.
7 Abramson, N. “The Alohanet - surﬁng for wireless data.” IEEE Communications Magazine 47(12):
21–25., 2009; Hopper, A. and Needham, R.M. “The Cambridge Fast Ring Networking System.”
IEEE Transactions on Computers Vol. 37, No. 10, October 1988, 1988: 1214–1223.
8 Bellamy, C., Clarke, N., Heale, K., Miller, P. Treloar, B. “Australia’s own Monet making an
impression.” Computerworld, Oct 18, 1985 pp. 20–22.
9 Heale, K. 30/4/79 Campus Communication Bus, Monash University Archives Mon 935 72 1996/06
MONET Prototype.
26
B. Ainsworth et al.

The team’s solution was to create a network using a common communication ‘bus’
to connect ‘nodes’ distributed over the campus. All nodes could communicate with
every other node. This required a method to control the use of the common bus by
competing nodes. The team
selected the
Carrier-Sense Multiple-Access with
Collision-Detection (CSMA/CD) system. The team worked through 1979 to 1981 to
develop a bus network using two twisted wire pairs communicating between nodes,
which were ultimately duplicated for reliability and capacity and extended via repeaters
to achieve campus-wide reach. Each node comprised two (and ultimately more)
microprocessors communicating via a dual-port shared memory (Fig. 2). The nodes
were coupled to the bus cables by ferrite toroidal transformers, so as to provide high
voltage electrical isolation. The nodes transmitted onto the bus using a novel ONEs line
and ZEROs line encoding method, for easy recovery of data, bit-rate clock and
collision-detection while providing for unambiguous jam signalling. These signals
were further encoded using conventional 3-level AMI (Alternate Mark Inversion)
encoding to eliminate low-frequency and DC components prior to transmission to line.
Software development for the MONET system had the following aims.
• Essential that all devices be addressed by name;
• Security mechanism to deﬁne multiple user access classes via a novel ‘digital locks
and keys’ mechanism;
• A means of controlling the allocation of resources among competing groups of
users;
• No down-line-loading of code or control tables to initialise the network after
restoration of power after a failure.
The MONET Terminal Processor software was developed by Patrick Miller,
including a novel access control system using digital ‘locks and keys’ to provide for
multiple access classes. This comprised a bit-pattern in the ‘lock’ assigned to each host
port and each user port had an access ‘key’ which had to match the pattern before they
could use the network to connect to the desired host computer. This allowed both host
and user ports to be grouped in access classes.
The team also saw the need for a central management tool. This was developed by
Stephen Dart in Pascal and was named the NETMAN program. This provided overall
monitoring and management of the network, as well as a more streamlined method of
administering the access classes and digital ‘locks and keys’. The need for better
control of access and management of the network became more important as the
network grew and number of users increased.
5
Theoretical Underpinnings – State of Knowledge
and Technology at the Time
5.1
The Need (‘Business Driver’)
The move in that era from a single centralized ‘mainframe’ computer to a multiplicity
of physically smaller mid-sized multi-user computer systems created the need for a
terminal exchange (analogous to an in-house telephone exchange or PABX, but for
MONET – Monash University’s Campus LAN in the 1980s
27

computer terminals rather than telephones) so that any terminal could connect to any
computer at will.
The need for not only the terminals but also the computers to be physically dis-
tributed across a large university campus added the requirement that the terminal
exchange also be physically distributed, so that any terminal anywhere could connect
to any computer anywhere else, rather than just to computers centralized within a single
main ‘Computer Room’ (or more recently ‘Data Centre’).
Hence the need for a distributed terminal access network was born.
5.2
The Gap
While some terminal exchanges were starting to appear (from suppliers such as Gan-
dalf and Micom) these were centralized devices (‘star’ topology), analogous to a
monolithic PABX, and hence were only suitable for connecting to computers
co-located within a centralized data centre.
Furthermore the popular serial data communications interface at the time was EIA
RS-232, which was intended only for very short distances, had no high voltage iso-
lation, and certainly could not cover the range of a large university campus. Some form
of line driver (‘media converter’) would be required on each RS-232 line to remotely
located terminals and computers. There was no commercially available distributed
terminal access network to meet the university’s requirements.
5.3
The Knowledge Base
At that time the seminal local area network (LAN) academic papers covering ALO-
HAnet, token ring, CSMA, Ethernet and related techniques had been published, but no
Ethernet integrated circuits (ICs or ‘chips’) or products yet existed. When Ethernet
products did subsequently appear, these were for computer to computer communica-
tion, not terminal to computer, and initial implementations were via a very unwieldy
coaxial cable (‘thick coax’) in practice suitable only for interconnecting computers
within a central data centre. Indeed, Ethernet solutions for basic VDU terminals never
did appear, with the PC era (and Ethernet) completely superseding and replacing VDU
terminals (along with their serial RS-232 connections).
From the early research base, it was known that reliable collision-detection was
important to maximize the throughput of an Ethernet-style LAN, and protect it from
congestive collapse. Important features of CSMA/CD, and implemented by MONET,
comprise:
• packetized data frames transmitted over a single shared bidirectional cable;
• Carrier Sense: do not start transmitting a packet while another transmitter is already
active;
• Collision Detection, to minimize time wasted sending damaged data packets;
• a ‘Jam’ phase following a collision, to ensure that all contending transmitters ‘see’
the collision and initiate the backoff action; then
• Abort: all senders prematurely terminate current transmission; followed by
28
B. Ainsworth et al.

• Backoff: each transmitter waits a random delay before attempting to re-transmit,
without which the contending transmitters are likely to re-collide if they all start
resending simultaneously; and
• an error-protecting acknowledgement and retransmission protocol to protect against
lost and damaged data packets – MONET used a standard 16 bit CRC error
detection method.
It was also known from contemporaneous solutions, such as CCITT G.703 at the
2.048 Mbit/s rate (as used widely in the then digitization of the telephone network),
that digital transmission on twisted-pair cables could be successful over campus dis-
tances, providing the cable was treated as a match-terminated transmission line.
It was also well known that inter-building cabling would need high-voltage isolation
to protect the sensitive electronics from impacts of lightning strikes and other mains
power disturbances, and that balanced differential transmission techniques would be
required to minimize electrical noise and interference affecting the twisted-pair wiring.
5.4
The Predecessor Project – Terminal Concentrator
A predecessor project to implement a terminal concentrator, built around a Motorola
6800 microprocessor development kit, gave the Monash Computer Centre team the
necessary experience and familiarity with the hardware set and developed knowledge
base and intellectual property around writing assembly language serial data commu-
nications and multiplexing technology, in that case using the Burroughs Poll/Select line
protocol.
This in turn provided them the necessary springboard and conﬁdence to embark on
the much more ambitious MONET project
6
The Components of the MONET Solution
While in the very earliest stages of the MONET development, the Motorola 6800 card
set and backplane were re-used from the predecessor concentrator project; one by one
these were all replaced as the more demanding MONET application outgrew their
capabilities. The replacement cards were all fully developed in-house by the MONET
team.
The basic idea was to re-purpose the existing concentrator as the terminal interface
(this became the Terminal Processor (TP) sub-system shown on the left side of Figs. 2
and 7), while replacing the Poll/Select multiplexed line with a wholly new interface to a
CSMA/CD bus (this became the Bus Processor (BP) sub-system shown of the right
side of Figs. 2 and 7).
Due to the limited clock speeds of microprocessors of that era, these could com-
fortably implement serial data communications applications (under software program
execution of byte-by-byte i/o) only at low data rates. For example concentrating several
2400 bit/s terminals into a 48 kbit/s multiplexed line might be a typical limit of the era.
The Motorola 6809 microprocessors used in the MONET system operated at a 2 MHz
clock rate (i.e. 0.5 µs cycle time), with each instruction in turn consuming several such
MONET – Monash University’s Campus LAN in the 1980s
29

Fig. 1. Key MONET cards – From left to right: 1a. DPM card (one half of a Dual Port Memory
assembly); 1b. MPU (Micro Processor Unit) card; 1c. SDLC (Synchronous Data Link Control)
card; 1d.BIM (Bus Interface Module) card Examples from private collection N. Clarke.
30
B. Ainsworth et al.

cycles to execute.10 Thus the microprocessor was only capable of executing several
hundred thousand primitive opcode machine instructions per second. Therefore, the
higher data rates envisaged for the MONET bus (which was to carry the entire uni-
versity’s trafﬁc on the one cable) would require special hardware design approaches
throughout.
Similarly, to maximise performance, software needed to be written from scratch by
the University team in the microprocessor’s native assembler language. Subsequently
common software suites for UNIX or TCP/IP were both unavailable for the micro-
processors at hand and, had they been, would have been unfeasibly slow for the
application.
6.1
Dual Port Memory (DPM) Architecture
The Terminal Processor (TP) would be kept busy transferring characters byte-by-byte
(under software program control) to and from as many terminal ports as would
physically ﬁt within the chassis. To insulate the TP from the ‘high-speed’ bus activity,
so that the TP could focus on moving as many characters as possible, a dual port
memory architecture was introduced. The backplane was physically split between the
TP side and the BP side (Fig. 2). The only means of communication between the TP
and the BP was via the DPM. Each processor would operate autonomously and
communicate with the other via messages and data packets they placed in (and fetched
from) the DPM. Thus from the TP’s perspective, transmission on the bus was ‘auto-
matic’ – the TP software would simply place the data packet in the DPM and off it
Fig. 2. MONET node – chassis, backplane and cards layout. Typical conﬁguration – conﬁgured
with 32 ports of RS-232 with modem controls as required for session control of host computer
ports and modems. Legend: NAB = Node Adaptor Backplane; OSI = Octal Serial Interface
card; CLK = master backplane CLOCK generator Diagram by N. Clarke.
10 MC6809-MC6809E
8-bit
Microprocessor
Programming
Manual
http://www.classiccmp.org/
dunﬁeld/r/6809prog.pdf.
MONET – Monash University’s Campus LAN in the 1980s
31

would go. Similarly data from the bus simply appeared in the DPM ‘as if by magic’ i.e.
without the TP doing anything.
The two DPM cards that form the DPM assembly, and straddle between the TP and
BP ‘sides’, can be seen towards the middle of Fig. 2, and the ﬂat cables interconnecting
the two DPM cards can just be made out near the middle of Fig. 7.
Random Access Memory (RAM) read/write times had by then (1980) become
sufﬁciently fast (200–400 ns), compared with the 1 µs cycle time of the CPUs. Thus
the DPM could effectively interleave accesses from both the TP and BP sides without
delaying either CPU (by cycle stretching) by any appreciable amount. Each RAM chip
had a capacity of only 4 K bits[sic], thus when fully loaded with chips a pair of DPM
cards had an ultimate capacity of 2 cards  3 rows  4 K bytes = 24 K bytes. The-
oretically, two pairs of DPM cards could be installed in the one chassis, doubling the
DPM capacity to 48 K bytes, but this capability was not generally utilized in practice.
6.2
Synchronous Data Link Control (SDLC) Card
While no Ethernet chips yet existed, what did exist was a Western Digital
SDLC/HDLC integrated circuit. This implemented in hardware a number of functions
that could be ofﬂoaded from the BP software, in particular packet framing (header and
footer and the necessary associated automatic zero insertion and removal technique)
and Cyclic Redundancy Code (CRC) error-check computation. In combination with a
Direct Memory Access (DMA) controller chip (eventually housed on the BP’s MPU
card), this meant that the ‘high-speed’ bus i/o was fully implemented in hardware.
The BP software only needed to set up each communication transaction, and the
SDLC/DMA hardware directly handled all the datacomm transfer between the DPM
and the bus – the BP was completely relieved of any character forwarding workload.
The 1.5 Mbit/s rate of the MONET bus was simply selected due to that being the
maximum data rate of the available family of SDLC chips, nothing more nor less
scientiﬁc than that. Thus, all other components of the MONET system had to be
designed to accommodate that rate.
Although there was only one SDLC chip on the MONET SDLC card, the card was
actually ﬁtted with two bus interfaces, for reliability – in case either one bus cable had a
breakage, and to allow live maintenance on the cable network without affecting user
services.
6.3
Bus Interface Module (BIM) Card
The Bus Interface Module (BIM) card implemented the novel MONET ‘ONEs’ and
‘ZEROs’ line coding method. Serial data bits for sending on the bus were separated
such that data 1 s were transmitted on one line (the ‘ONEs’ twisted pair) while data 0 s
were transmitted on a separate line (the ‘ZEROs’ twisted pair). Each line was then
separately and further encoded using a conventional AMI (Alternate Mark Inversion)
3-state (+1, 0, –1) encoding scheme so as to eliminate low-frequency and DC com-
ponents and thus allow transformer coupling onto the bus cable, to achieve the
32
B. Ainsworth et al.

Fig. 3. Line isolation transformers. Detail from Fig. 1d showing hand-wound toroidal line
isolation transformers and associated components
Fig. 4. MONET’s ‘ONEs and ZEROs’ line code and Collision Detection logic
Fig. 5. Illustration of MONET’s ‘ONEs and ZEROs’ line code and Collision Detection logic
MONET – Monash University’s Campus LAN in the 1980s
33

high-voltage isolation requirement. The two ferrite toroidal transformers, which fea-
tured hand-wound biﬁlar windings to achieve the required bandwidth and isolation, can
be clearly seen on the BIM card in Fig. 1d and below in Fig. 3 (one transformer for
each of the ONEs line and the ZEROs line).
This encoding scheme was logically very simple to implement, while delivering the
lowest line-rate (1 baud per bit) self-clocking line-code, from which received data,
bit-rate clock and collision-detection could all be easily decoded, while the Jam con-
dition could equally easily be indicated by transmitting on both the ONEs and ZEROs
line simultaneously (i.e. as an illegal coding violation), viz (Figs. 4 and 5):
A self-clocking line-code was required as each data packet was sent independently
from different transmitters, without the possibility of a network-wide master clock. Rapid
clock acquisition was required to keep frame header introducer to minimum length.
Simple logic was required due to the low density of general purpose logic chips then
available. Minimum baud rate was required to maximize distance between repeaters.
Even so, the size of the Monash Clayton campus was such that bus repeaters (active
regenerators) were required. This was implemented as a pair of stand-alone BIM cards
connected back-to-back. As each bus comprised two bus cables, for reliability, this
meant that a complete repeater comprised a total of four stand-alone BIM cards con-
nected back-to-back in pairs.
The BIM card derived only power from the NAB backplane, obtaining all data and
control from ﬂat-cable connection direct to the SDLC card (as per Fig. 2).
6.4
Node Adaptor Backplane (NAB) – Synchronous Backplane Bus
The MONET Node Adaptor Backplane (NAB) was originally based on the S100
backplane of the Motorola Microsystems card-set11 inherited from the predecessor
project. A number of important changes were made, in addition to splitting the
backplane between the TP and BP sides.
Crucially, to accurately orchestrate the many devices that would contend to access
the backplane and DPM, Clarke replaced the asynchronous analog (monostable mul-
tivibrator) timers used on the Motorola cards by a master synchronous clock broadcast
across both sides of the NAB. Along with DC power, this clock was the only signal
common across both the TP and BP sides of the NAB (refer Fig. 2).
The contending devices that needed to be synchronized were:
• the two master devices on the BP side: the BP CPU and the SDLC/DMA
sub-system
• a multiprocessor architecture on the TP side: ultimately multiple TPs and the MPI
(see later section below)
• synchronized access to the DPM
• as well as the various responding devices like the OSI serial line cards
11 See a photo of a Motorola Microsystems 6800 CPU S100 board at:http://i.ebayimg.com/images/g/
4skAAOxyzHxROgPE/s-l1600.jpg from http://www.ebay.com/itm/Motorola-Microsystems-Micro-
Module-1A-84DW6227X01-/121078079680.
34
B. Ainsworth et al.

Consequently, henceforth all the MONET developed cards were driven from this
master clock.
An 8 MHz (quartz crystal oscillator) clock was chosen, so as to subdivide the
backplane’s nominally 1 µs cycle time into around eight 125 ns ‘phases’. This choice
allowed more than enough time within each phase for the clock to propagate throughout
all boards and for the LS-TTL logic to well and truly settle; while also providing
sufﬁcient granularity to stretch a CPU cycle by small increments, as required by memory
or backplane contention. While internal CPU cycles ran at the full 2 MHz rate of the
CPU, consuming only four 125 ns clock ‘phases’, external backplane and memory
accesses ran to a longer more complex sequence. For example, an unfettered OSI or
DPM read or write might require 2 or 3 phases, but could be stretched in additional
125 ns increments if the DPM or backplane was already servicing another processor.
Thus the variable number of phases allowed for the following periods within the
CPU’s backplane access cycle:
• CPU address bus and data bus settling time
• address decode/decision time
• backplane contention (with potential cycle stretching as required)
• memory/device read/write time, including
– DPM contention (with potential cycle stretching as required)
6.5
Micro Processor Unit (MPU) Card
In completely rebuilding the CPU card to meet the needs of the MONET project, the
following major changes were adopted. The same MPU board was to be used as both
the BP and the TP.
• Upgrade the original 6800 processor chip with the then available Motorola 6809
CPU.
• Add the DMA controller – as required by the BP side.
• Add the synchronous clock cycle sequencing (as above).
• Add a multiprocessor capability (for the TP side) comprising a backplane con-
tention and daisy chain prioritization scheme – this allowed for multiple TPs to be
ﬁtted over time, to cope with increasing loads as port density (e.g. to 48+ ports) and
data rates increased (e.g. from typically 2400 bit/s originally to 9600 bit/s or more)
over the life of MONET in the ﬁeld, while ultimately also supporting the addition of
the MPI Unibus connection later in the project (see below).
6.6
Octal Serial Interface (OSI) Card and Adaptors
To increase the MONET node’s terminal port density, an 8 port serial interface card
was developed (the OSI card). It could be ﬁtted with a range of serial adaptors
including:
MONET – Monash University’s Campus LAN in the 1980s
35

• RS-232 with full modem controls, which consumed an adjacent NAB slot (refer
Fig. 2). This was required for session control signalling for modems and for VAX
serial host computer ports.
• RS-232 data-only (for user terminals), this was a daughter board ﬁtted directly to
the OSI without wasting a 2nd NAB slot, so as to allow additional OSI cards and/or
additional slave TPs to support the additional port load.
• 20 mA current loop, which had greater distance reach than RS-232.
6.7
Logic Implementation – PROM Logic and Finite State Machines
General purpose logic density was quite limited in that era, e.g. four 2-input NAND
gates or two ﬂip-ﬂops or a shift register per 14 to 20 pin chip. This consumed a
signiﬁcant amount of circuit board real estate for even quite simple logic requirements.
An alternative then available was ‘high-speed’ Programmable Read-Only-Memory
(PROM) which delivered settling times (of the order of 20 ns), similar to that
achievable using collections of LS-TTL gates, roughly an order of magnitude faster
than normal RAM and ROM memory available at the time.
PROMs could therefore be used as a general purpose logic substitute, for both
arbitrary combinatorial logic and, with the addition of a parallel-in-parallel-out
shift-register could also be used to implement simple ﬁnite-state-machines, as required
by various MONET hardware sequencers (described above and below).
A simple Pascal-like programming language and compiler was developed in-house
by Clarke to streamline the generation of PROM object ﬁles (bit patterns to reﬂect the
desired logic). This comprised:
• a declaration section to deﬁne names for: input variables, output variables, state
variables, state names and state code values (the state variables were also available,
if sensibly chosen, to also be used directly as additional output variables)
• a combinatorial logic section for immediately executed logic, of the form:
OutputVar :¼ logical function of Input and State Variables
• an ‘executable’ section to deﬁne the operation of ﬁnite state machines, of the form:
when CurrentState if \logical expression [ then NextState
This made the generation of PROM data so easy that several PROM-based logic
elements appeared on all key MONET boards, for both combinatorial (logic) and
sequential (state machine) implementation, making various design tasks feasible within
the limited board area, due to the higher logic density of PROM as compared with
general purpose gate chips.
Use of PROM was chosen over FPLA/FPGA, seeing as PROM can guarantee
implementation of any arbitrary logical combinations of any complexity, without
needing to constrain the logical design by limitations within a speciﬁc FPLA/FPGA
chip, thus in turn simplifying and generalizing the PROM’s compiler language (which
36
B. Ainsworth et al.

therefore did not need to take into consideration architectural constraints within
FPLA/FPGA chips).
7
End-User Interface – MONET Session Control
A simple command line interpreter was provided to allow the end-user to nominate
which computer to connect to, or to disconnect. Each command was preﬁxed with an
escape character (Control-P or DLE character) to differentiate the command from
normal text. <Control-P> (the ASCII Data Link Escape character) was chosen as it had
no other usage in communicating with any of the computer systems that MONET was
targeted to (Burroughs, VAX, UNIX, etc.). End-user commands comprised:
• <Control-P>C <computer name>
to Connect to a port on the nominated computer
• <Control-P>D to Disconnect a session
• <Control-P>S to determine current connection Status
Developers and support staff had access to additional commands after providing a
password.
There were also inactivity timeouts to force disconnect of valuable computer ports
where the user may have forgotten to do so, as well as signalling between MONET and
host computers (e.g. via RS-232 modem control signals) to force disconnect following
logout or to force logout following disconnect where the user may have forgotten to
logout before disconnecting.
In response to a connection request command, the MONET system would either
respond to the user with a connection successful status message including host com-
puter’s name and port number, or an access denied message if the ‘locks-and-keys’ access
controls had prevented that user port from connecting to that host. This allowed, for
example, staff terminals in secure admin ofﬁces to access sensitive student records, HR
and ﬁnance systems, while preventing student terminals in public areas from doing so.
8
Subsequent Embellishments
Once the basic MONET kit (above) was in extensive use, a number of enhancements
were made:
• X.25 multiplexed interface (SSP card) – required for commercial customers
• extension of the Monash installation to other campuses – due to Dawkins era
amalgamation with Chisholm Institute – MONET nodes at each site were inter-
connected by multiplexed serial link via 64 kbit/s permanent call over the
University’s digital PABX network
• DEC Unibus interface
MONET – Monash University’s Campus LAN in the 1980s
37

8.1
MONET Unibus Interface
By far the most ambitious component of the MONET project was development of a
direct interconnect between the DEC Unibus and the MONET node, as this required
not only additional hardware and software development on the MONET side, it also for
the ﬁrst time for the project, also required signiﬁcant hardware and software devel-
opment within the VAX host computer itself.
The DEC computers (VAX and PDP) did not have a multiplexed interface like the
Burroughs Poll/Select line. Instead each individual terminal port was presented as a
separate RS-232 port (several RS-232 ports per Unibus card). This required many
discrete serial lines between MONET ports (as per Fig. 2) and VAX ports. Full
modem-style interfaces were required for session control signalling.
Not only was this inefﬁcient in terms of hardware real-estate at both the MONET
and VAX Unibus ends, and much cabling in between, it was also a drain on CPU time
within both the MONET TP and the DEC VAX, due to character by character pro-
cessing at both ends of each serial line.
To address this, a project to develop a direct interface was embarked upon. To
eliminate the character by character CPU overheads, more DMA was called for. The
team was by now so comfortable with multiprocessor DMA techniques that this was a
natural progression. This required the design of two wholly new hardware modules: a
Unibus card, called the Unibus Communications Processor (UCP) and a MONET card,
called the Monet Parallel Interface (MPI). The MPI card extended the 8-bit NAB bus
via ﬂat cables to the UCP card installed within the VAX (refer Fig. 6).
The heart of the UCP was a Motorola 68000 microprocessor. As time had by now
moved on, and the 16-bit Motorola 68000 microprocessor had since become available,
it was chosen for the UCP due to the Unibus also having a 16-bit architecture (the VAX
had inherited the Unibus from the preceding PDP family to leverage the extensive
range of PDP interface boards). The quirky result was that this newest most powerful
CPU chip resided solely within a single peripheral card, while the main BP and TP
processors remained the original more limited 6809 devices.
Thus the hardware challenge was to implement a number of address/data/control
parallel bus format conversions:
• to interface the DEC Unibus to the UCP’s on-board 68000 architecture (of note the
Unibus and 68000 used opposite conventions for ordering the high and low bytes
within their respective 16-bit words, adding further difﬁculty);
• to interface the on-board 68000 buses to a long-distance form of the NAB extended
to the UCP by the MPI;
• and at the MPI end to convert this to the NAB format so that the UCP/MPI
combination appeared as just another TP within the MONET node’s multiprocessor
architecture, while the DPM appeared within the 68000 UCP’s address space;
• the NAB had not been conceived for extension over the longer distances needed to
reach beyond the internals of the MONET node chassis, thus the challenge at the
MPI end was to convert the NAB into a form suitable for extension, as a parallel
bus, to the UCP card to be located within a nearby VAX.
38
B. Ainsworth et al.

Fig. 6. MONET interface to DEC Unibus – From left to right: UCP (Unibus Communications
Processor) card; MPI (Monet Parallel Interface) card. This assembly replaced a multiplicity of
serial lines between a VAX and a co-located MONET node.
Fig. 7. Computer Centre director Dr. Cliff Bellamy explaining MONET in the Monash
University Computer Centre, Clayton in 1986 Photographer: Tony Miller, Monash University
Archives, MONPIX Image Number 1553
MONET – Monash University’s Campus LAN in the 1980s
39

The software challenge, within the UPC’s 68000 software and companion driver
software within the VAX itself, was to make the UCP appear simultaneously:
• as a TP from the point of view of the MONET node, in which the 68000 software
would directly deposit and retrieve data packets from the node’s DPM, and
• as a bundle of serial lines from the point of view of the VAX, but without the VAX
incurring character-by-character interrupt and processing load, which was the case
for their ‘real’ serial interface cards.
The need for software development beyond the MONET node itself, saw the
introduction of a new member to the core team, John Mann, one of the Computer
Centre’s VAX software specialists. John developed the UPC software suite, on both the
68000 and VAX driver sides, handling the communication from VAX applications all
the way through to interacting with MONET’s multiprocessor message passing pro-
tocol via the node’s DPM.
9
Circuit Board Fabrication
All the earlier MONET boards were fabricated as double-sided printed circuit boards,
fabricated using conventional photo-etching processes outsourced to a nearby com-
mercial facility. Graphics were all designed in-house and rendered using stick-on
adhesive tapes and decals, onto clear plastic sheets, as was typical for the era. Three
overlaid sheets were used, for common, top and bottom layer graphics. Earlier work in
the department had used red and blue graphics to denote each layer. All MONET
boards were however designed using ﬂexible black crepe adhesive tapes, the results of
which can be seen in the curved tracks evident in Figs. 1 and 3. Components were
loaded onto the printed circuit boards manually, initially in-house by a manufacturing
team led by Russell Keil, and then subsequently by Racal for higher volume
manufacture.
However the higher complexity of the UCP and MPI boards presented a greater
challenge. These were fabricated using an outsourced robotic facility located in Perth,
Western Australia. The resulting multilayer boards comprised two embedded
power/ground plane layers fabricated using conventional double-sided photo etching
processes over which were laid multiple overlapping layers of discrete insulated wires
robotically laid into a resin bed. Required holes were then robotically drilled, the
insides of which and the pads for which were then electrochemically plated. Fabrication
information was supplied by data ﬁle. The resulting boards were no thicker than a
conventional printed circuit board. This was a very effective technology for
high-density low-volume custom circuit board fabrication. Unfortunately it suffered a
high rate of failure where the in-hole plating would not make contact with the exposed
cross-section of a wire intended to end in the hole. In response, in-house development
of the supplied design software comprised: ﬁrstly re-sorting the wiring data geo-
graphically to minimize the robot time (boards were charged by fabrication time), and
then modifying an HP ﬂatbed XY plotter to automatically QA test continuity of each
and every wire of each incoming board, again using a geographically sorted wire list,
40
B. Ainsworth et al.

so that the human operator’s probe (A-end) and the plotter’s probe (B-end) never
collided during the QA process.
10
From Early Tests to Patent
The ﬁrst test nodes were installed from April 1981 and development work continued
throughout 1982.12 The nodes could be connected to computer ports, terminals,
printers, Telecom modem lines and other serial I/O devices. By November 1982 the
new network was being installed to connect to the available computing facilities which,
at this stage, included a dual processor Burroughs B6700 computer, nine VAX com-
puters13 and a range of printers and graph plotting devices.14 At the beginning of 1983
there were 40 nodes in the network with connections across 700 terminals and 350
ports on 16 medium to large computers.15
The University team tested their network on site and also delivered a network to the
Victorian Totalizator Agency Board (TAB) and the Telecom Research Laboratories for
further test sites. In November 1981, the TAB conﬁrmed they were ready to implement
“an ‘in house’ local Data Network based on the Monash developed MONET”. This
network was given the name TABNET. The Monash staff would provide consulting
work to TAB’s system engineers and supply three fully conﬁgured MONET nodes at
AUS$4000 each.16
The major rollout of the MONET network was undertaken during the mid-1980s.
Dr Carlo Kopp was a Computer Systems Engineer from 1985–1987 at the Computer
Centre and responsible for network installation across the campus during this period.
The cabling team installed a backbone cable from the nodes attached to the VAX,
Burroughs and other systems within the Computer Centre to the distributed nodes at
various sites around the campus. The ﬁrst cabling was focussed on the student com-
puter labs and then research computer facilities. Later the administrative areas were
connected and then individual cabling for academic staff. It was a difﬁcult task as the
existing buildings did not have suitable ducting for computer cabling and the service
staff lead by George Cobb had to be creative in ﬁnding ways to feed the cabling around
the buildings.17
By 1983 the staff at the Computer Centre was conﬁdent that the new network was
running effectively. Their next step was to apply for a patent and also to ﬁnd a
commercial partner. After lawyers assessed the paperwork, Monash University applied
12 Bus Planning Meeting minutes 31/3/82 – Monash University Archives Mon 935 72 1996/06
MONET Prototype.
13 MONECS was an educational programming system with dedicated minicomputers around the
campus but they were not connected to the network.
14 Monash Reporter. Computer Centre to spread its ‘net’ November, 1982. Monash University.
15 handout Monash Open Day 1984 – MONET, Mon Archives Mon 935 Monet Prototype 1996/06/72.
16 Letter from James P. Baker, Manager Computer Services, TAB to C. Bellamy 24/11/1981, Monash
University Archives Mon 935 Monet Prototype 1996/06/72.
17 Kopp, Carlo. “Industry Projects - Hardware and Embedded Systems.” Carlo Kopp. 2010. http://
www.csse.monash.edu.au/*carlo/hardware.html (accessed December 11, 2015).
MONET – Monash University’s Campus LAN in the 1980s
41

for an Australian patent on 30 April 1984 for an invention entitled, “Improvements
relating to digital communication systems” with the inventors listed as Dr Clifford
Bellamy, Neil Clarke, Peter Gordon, Keith Heale, Patrick Miller and Barry Treloar for
the
owner,
Monash
University.
The
assigned
number
for
the
patent
is
AU1984028267.18
The patent process took several years and was ﬁnally granted in 1988. They also
applied to patent the system in Europe and paper work was ﬁled under EP-O148191 on
30 April 1984. This was at the same time as transferring the technology to
RACAL-Milgo which did not pursue it and the European patent application lapsed.19
11
Monash University and RACAL-Milgo Australia Pty Ltd
With the network running successfully on campus, Cliff Bellamy then sought a busi-
ness partner to manufacture the MONET system and produce a commercial product
during 1983. Several businesses showed interest in the new LAN including Burroughs,
Datacraft and Digital Equipment Corporation (DEC). Treloar and Bellamy attended the
National Computer Conference in US that year and then went on to visit the ofﬁces of
Burroughs, Micom, Research INC. and DEC for technical presentations on MONET.20
Nothing came of these discussions and then in January 1984 Monash again called for
interest in joint manufacturing the system in the magazine Computerworld.21 This
produced two deﬁnite possible business partners including RACAL-Milgo Australia
Pty Ltd. RACAL-Milgo was a large international ﬁrm that had ofﬁces in several cities
in Australia. RACAL Electronics was an English company that went into a joint
venture with the American ﬁrm Milgo in 1969. Milgo was an early manufacturer of
modems and specialised in networking. The company is now part of the Thales
Group.22
In May 1984 R.G. Gregory, from RACAL-Milgo wrote to Cliff Bellamy with the
companies’ offer for a partnership to sell MONET. They would pay Monash University
a technology transfer fee and then the University would receive 10 % royalty on sales
value to third parties. Monash would supply RACAL-Milgo with all the MONET
components and software including processor boards with battery backup, X.25
interface, VAX serial interface, and network manager system. The company would
18 For
complete
details
on
patent
application
see
http://pericles.ipaustralia.gov.au/ols/auspat/
applicationDetails.do?applicationNo=1984028267.
19 Bellamy to R. Hammond, RACAL Electronics, 8/2/1985, notes RACAL failed to apply for any
foreign patents, so only Australian patent stands, Monash University Archives Mon 935 File 78.
20 Letter from Bellamy to L.W. Candy, Comptroller, Monash University, 17/5/84, SALE OF MONET
LAN TO RACAL back ground note Monash University Archives Mon 935 77 1996/06 Folder title,
Monash Computer Communications Network (MONET) 92/06/093 part 1.
21 Computerworld, “Monash seeks manufacturer for its LAN.” January 20, 1984: 3.
22 Racal-Datacom Inc. History. ``Funding History.'' n.d., http://www.fundinguniverse.com/company-
histories/racal-datacom-inc-history/ (accessed October 15, 2015).
42
B. Ainsworth et al.

manufacture and market the system with an advertising campaign. This offer was
accepted and RACAL-Milgo started on the new venture.23
RACAL-Milgo entered into full scale production employing Australian engineers,
draughtsmen and production staff. While emphasising that the MONET system was a
proven, reliable Australian-designed and Australian-made product, the main sales
feature was the price. Their advertisement in 1985 quoted a typical price of AUS$200
per port connection. They also had seven installed sites to demonstrate the working
systems. The network at Monash University was used by salespeople to show the
system to potential customers and Centre staff made themselves available to answer
technical questions. On 26 May 1985 the local Melbourne newspaper, The Age,
reported under the title, “Monash ‘invention’ takes off” the success of the new LAN
and named several large customers who had installed the system including Telecom
Research Laboratories, the Victorian Harness Racing Board, St Vincent’s Hospital and
Victorian TAB. The Monash team also published several papers during 1985 on the
development of the MONET system, including papers in the Australian Computer
Journal24 and Computerworld.25
The sales of the new LAN system produced an income stream for Monash
University. RACAL-Milgo started paying royalties to the University by October 1984.
The University had by then already received the technology transfer fee. In a royalties
payment notiﬁcation letter in June 1986, RACAL paid Monash just over AUS$35,000
for sales made earlier in the year with a further AUS$21,000 to be paid on receipt of
invoices for additional installation sales.26
Monash University continued to maintain its own MONET system. By 1986 they
had 2,200 ports supporting 1400 terminals, about 80 printers and 18 medium to large
computers. The Centre had acquired a new mainframe, a Burroughs B7800, as well as
15 DEC VAX machines and 2 Pyramid computers; they also had a number of mini-
computers distributed around different departments.27
Monash University continued to use the MONET network throughout the 1980s.
By mid-1988 there was still some developmental work on circuit boards and software
but the project was drawing to a close. Neil Clarke told staff that they intended to have
all hardware development work completed by the end of 1988.28
23 Monash University Archives Mon 935 Monash Computer Communications Network (MONET)
92/06/093 part. 1996/06/77.
24 Bellamy, C., Clarke, N., Heale, K., Miller, P., Treloar, B. “MONET - The Monash University Local
Area Network.” Australian Computer Journal, 1985: Vol. 17, Number 2, 67–76.
25 Bellamy, C., Clarke, N., Heale, K., Miller, P. Treloar, B. “Australia’s own Monet making an
impression.” Computerworld, October 18, 1985: 21–23.
26 Letter dated 14/10/1984 R. Gregory, RACAL to Bellamy and Letter dated 5/6/1986 R. Gregory,
RACAL to C. Bellamy, Monash University Archives 935 1999/06 File 78 Monet – new products
release.
27 Open day leaﬂet 1986 MONET - RACAL-MONET Digital Data Network. In Monash University
Archives, Mon 935- Monet Prototype 1996/06/72.
28 Memo to all Computer centre staff from Neil Clarke 31/5/88, Monash University Archives Mon
935- Monet Prototype 1996/06/72.
MONET – Monash University’s Campus LAN in the 1980s
43

In October, 1989 the commercial agreement between Monash University and
RACAL-Milgo was dissolved.29 This was due to changes in the structure of the
RACAL organisation. They transferred excess stock to Monash University in lieu of
royalties and stopped marketing the network. Existing customers maintained contact
with Monash and continued to be partially supported. The Monash University Com-
puter Centre was still supplying revised MONET software to a customer in 1990.
12
MONET and Ethernet
The commercial success of MONET was eclipsed by the development of other com-
mercial networking devices including Ethernet and token ring systems, including
proprietary solutions such as IBM’s Token Ring. Ethernet had contemporary begin-
nings, with Xerox PARC installing a network using their new technology in the
mid-1970s. Through the 1980s, these different approaches to networking competed for
dominance, but it was the Ethernet system that became the accepted world standard
with IEEE recognition on 23 June 1983.30
The emergence of mainstream personal computers (PCs) in the 2nd half of the
1980s led to their progressive replacement of basic VDU terminals as user interface
devices. The general purpose and higher performance computing capability of PCs
progressively led to the migration from serial RS-232 data communications (inherent to
VDUs) with Ethernet, ﬁrstly within the LAN and subsequently over wide-area
telecommunications networks with the progressive replacement of voice-band modems
with ADSL, HFC and similar Internet delivery technologies.
In parallel with the development of PCs, Ethernet itself was evolving – from its
original awkward and limited ‘thick coax’ cable, through its ‘thin coax’ and then to the
CATx UTP cable that we are familiar with today. In so doing, Ethernet displaced other
competing LAN technologies such as token ring based systems. At the same time, the
Internet and its suite of IETF communications standards (TCP/IP etc.) exploded into
dominance over other wide area network protocols such as X.25.
The end result was PCs, Ethernet and the Internet replaced basic VDU terminals,
RS-232 and ‘dial-up’ modem services across the board, in turn superseding RS-232
based terminal access networks such as MONET and other serial terminal exchanges
which all became no longer required.
In 1991 MONET was replaced on the Monash University campus by an Ethernet
network. The MONET network was too slow for large ﬁle transfer applications. The
Ethernet system required a new network of optical ﬁbre cable and would operate at
speeds up to 10 million bits per second.31 This new network coincided with Australia’s
exposure to the Internet and subsequently the World Wide Web.
29 Termination of Agreement. Dated 3 October 1989. Monash University Archives, Applications –
Monash University Communications Network (MONET) 87-0082.
30 IEEE Standards Association 2013 http://standards.ieee.org/news/2013/802.3_30anniv.html (ac-
cessed October 20, 2015).
31 Montage, “State-of-the-art computer network connects campuses”. May 6, 1991. Monash
University, Australia.
44
B. Ainsworth et al.

13
Conclusion
The 1970s saw a huge growth in the number of computer installations within organ-
isations and the movement of data between computer systems. There was also a rise in
the number of people who used computers. This all led to an increase in the number of
individual terminals being required to access multiple computers. The need to create
some form of network was critical in university communities. It was a new ﬁeld and
computer engineers came up with individual solutions for their own environments.
Staff at Monash University Computer Centre met this challenge by combining their
skills in hardware and software to produce their own network. Once a solution was
found, Dr. Cliff Bellamy, with his entrepreneurial ﬂair, actively sought a business
partner to manufacture and market the network on a commercial scale. This generated
useful income for the Computer Centre as well as serving the University’s aim to work
with industry in Australia. It was an innovative network solution developed in-house
using the then available general-purpose digital integrated circuits and microprocessor
chips. Although MONET was ultimately replaced by a different solution, the MONET
network had met the key criteria set by the University in 1979 to create and install an
economical computer access network across an extensive campus with distributed
computer installations and a large numbers of users (Figs. 8 and 9).
Fig. 8. Handover of MONET software to RACAL Electronics. L-R: Dr. Cliff Bellamy, Mr.
Robert Morgan, Mr. Roland Horat and Mrs. Gail Morgan, dated 1984 Photographer: John
Millar, Monash University Archives, MONPIX Image Number 1565
MONET – Monash University’s Campus LAN in the 1980s
45

Fig. 9. Advertisement for MONET system sold by RACAL-Milgo, Computerworld Oct 18,
1985 p. 22 Reproduced by permission of the Thales Group
46
B. Ainsworth et al.

Acknowledgements. The authors would like to thank the Monash University Archives and the
Thales Group for permission to use illustrations. The Monash Museum of Computing History
would also like to thank Neil Clarke who co-authored this paper. He was an original MONET
project team member and co-held the MONET patent with other developers; currently, he is
Audio Visual and Network Unit Leader, Deakin University, Australia.
References
1. Abramson, N.: The ALOHAnet - surﬁng for wireless data. IEEE Commun. Mag. 47(12), 21–
25 (2009)
2. Australian Academy of Technological Sciences and Engineering. “Technology in Australia
1788–1988.” Australian Science and Technology Heritage Centre (2000). http://www.
austehc.unimelb.edu.au/tia/552.html. Accessed 21 Nov 2015
3. Australian Computing Museum Society. Computing History Timeline. Australian Computer
Museum Society, January 2015. http://acms.org.au/computing-history-timeline. Accessed 12
Oct 2015
4. Bellamy, C., Clarke, N., Dart, S.: Network Management and Experience with Monet.
Monash University Computer Centre, Clayton (1985)
5. Bellamy, C., Clarke, N., Heale, K., Miller, P. Treloar, B.: Australia’s own Monet making an
impression. Computerworld 21–23 (1985)
6. Bellamy, C., Clarke, N., Heale, K., Miller, P., Treloar, B.: MONET - the Monash University
local area network. Aust. Comput. J. 17(2), 67–76 (1985)
7. Cawley, R.J., Trimble, G.D.: An interactive computing system for the AAEC Dataway
Network. ANSTO Publications Online (1977). http://apo.ansto.gov.au/dspace/bitstream/
10238/642/2/AAEC-E-425.pdf. Accessed 10 Oct 2015
8. Centre, Monash Computer. MONET- RACAL/Monash University Local Area Network.
Open Day handout. Monash University (1984)
9. Clarke, R.: Origins and Nature of the Internet in Australia. Roger Clarke’s Website, 29
January 2004. http://www.rogerclarke.com/II/OzI04.html. Accessed 12 Sept 2015
10. Computerworld. Monash seeks manufacturer for its LAN, vol. 3, 20 January 1984
11. Coxhill, R.: History of the Telecom Research Laboratories-Computing. History of the
Telecom Research Laboratories, April 2015. http://www.coxhill.com/trlhistory/history/
history.htm. Accessed 10 Oct 2015
12. Deane, J.: “SILLIAC- vacuum tube supercomputer. Science Foundation for Physics, in
association with the Australian Computer Museum Society, reproduced by University of
Sydney Physics Department, Silliac. 2006. http://www.physics.usyd.edu.au/foundation.old/
silliac/book/SILLIAC_ch4_2ndEdition.pdf. Accessed 22 Oct 2015
13. European patent Ofﬁce - Extract from the Register of Patents: EP0148191. European Patents
Ofﬁce (2015). https://register.epo.org/application?number=EP84901719&lng=en&tab=main.
Accessed 1 Oct 2015
14. Hopper, A., Needham, R.M.: The Cambridge fast ring networking system. IEEE Trans.
Comput. 37(10), 1214–1223 (1988)
15. IEEE Standards Association. IEEE 802.3™‘Standard for Ethernet’ Marks 30 Years of
Innovation and Global MarkET GROWTH. IEEE.ORG, 24 June 2013. http://standards.ieee.
org/news/2013/802.3_30anniv.html. Accessed 20 Oct 2015
16. Kopp, C.: Industry Projects - Hardware and Embedded Systems. Carlo Kopp (2010). http://
www.csse.monash.edu.au/*carlo/hardware.html. Accessed 11 Dec 2015
MONET – Monash University’s Campus LAN in the 1980s
47

17. Korporaal, Glenda: AARNET - 20 years of the internet in Australia. AARNet, Australia
(2009)
18. Monash Reporter. Computer Centre to spread its ‘net’, vol. 6, November 1982
19. Monash University. Monash University Archives, Clayton campus, Monash University.
Melbourne (n.d.)
20. Monash University, Faculty of Information Technology. Faculty of Information Technology
25th Anniversary Faculty Timeline. Monash University, Faculty of Information Technology
(2015). http://25years.infotech.monash.edu/1968_department_of_computer_science. Acces-
sed 20 Nov 2015
21. Montage. State-of-the-art computer network connects campuses, 6 May 1991
22. Patents- Improvements relating to Digital Communications System - AU1984028267
(2015).
http://pericles.ipaustralia.gov.au/ols/auspat/applicationDetails.do?applicationNo=
1984028267. Accessed 17 Oct 2015
23. Racal-Datacom Inc. History. Funding History (n.d.). http://www.fundinguniverse.com/
company-histories/racal-datacom-inc-history/. Accessed 15 Oct 2015
24. Williams, P.F.: VT100 - Meet The Family. VT100.net (n.d.). http://vt100.net/vt_history.
Accessed 11 Dec 2015
48
B. Ainsworth et al.

Technology vs. Political Conﬂict – How Networks
Penetrate the Iron Curtain
Frank Dittmann
(✉)
Deutsches Museum, Munich, Germany
f.dittmann@deutsches-museum.de
Abstract. In July 1977, the International Institute for Applied Systems Analysis
(IIASA) near Vienna organized an experimental data transmission line. This
paper investigates this experimental data transmission line focusing on three
aspects: ﬁrst of all IIASA was an important location for Eastern and Western
scientists for working with each other, secondly the team of computer specialists
creating the network was remarkable, and ﬁnally the concept of combining
computer technology with science cooperation and information transfer was very
advanced in the 1970s.
Keywords: Data transmission line · Network · Cold war · Computer
specialists · Scientists · Science cooperation · Computer technology ·
Information transfer
1
Introduction—Experimental Data Transmission Line in 1977
In July 1977, the International Institute for Applied Systems Analysis (IIASA) near
Vienna organized an experimental data transmission line:1
During three weeks, scientists from Austria, Poland, USSR and the US were enabled
to communicate electronically through the Iron Curtain.2 This fact was a remarkable
event in the history of computer networks. In particular, the roots of the motivation are
interesting: Which were primarily based not on technology but on politics. Gennadij M.
Dobrov,3 a Soviet science researcher and philosopher of science, considered science as
an international and cooperative process. Therefore he argued that it is necessary to
establish teams of scientists from diﬀerent countries who would then cooperate in their
research of vital problems of mankind in spite of political and ideological diﬀerences.
However, this endeavour was obviously diﬃcult during the Cold War and that’s why
the idea arose to solve all the problems by communicating via computer technology. In
1 This article is the revised paper held on the International Conference of IFIP working group
9.7, International Communities of Invention and Innovation, New York, May 25-29, 2016. For
more details see the extended German paper: Frank Dittmann, Technik versus Konﬂikt. Daten‐
netze durchdringen den Eisernen Vorhang, in: Osteuropa 59, 2009, no. 10, pp. 101–119.
2 G. M. Dobrov et al., Information Networks for International Team Research, in: International
Forum on Information and Documentation 3, 1978, No. 3, pp. 3–13.
3 In Memoriam (Gennady M. Dobrov), in: Options March 1989, p. 18.
© IFIP International Federation for Information Processing 2016
Published by Springer International Publishing AG 2016. All Rights Reserved
A. Tatnall and C. Leslie (Eds.): HC 2016, IFIP AICT 491, pp. 49–57, 2016.
DOI: 10.1007/978-3-319-49463-0_3

1977 the three-week data transmission line proved that the general concept worked. This
paper investigates this experimental data transmission line focusing on three aspects:
First of all IIASA was an important location for Eastern and Western scientists for
working with each other. Secondly the team of computer specialists creating the network
was remarkable. Finally the concept of combining computer technology with science
cooperation and information transfer was very advanced in the 1970s (Fig. 1).
Fig. 1. Experimental data transmission between Austria, Poland, USSR, and U.S. in July 1977.
Source: In: International Forum on Information and Documentation 3, 1978, No. 3, p. 10
2
The Location – The International Institute for Applied Systems
Analysis (IIASA)
October 1972 after six year preparation time the IIASA was founded in London.4 In
1966, U.S. President Lyndon B. Johnson had already developed the idea that scientists
4 See: Howard Raiﬀa, In the beginning, … The early years of IIASA, as recalled by its ﬁrst
director, in: Options Special Issue: Annual Report 1992, March 1993, pp. 6–10; Roger E.
Levien, Introduction to IIASA: Applying Systems Analysis in an International Setting, in:
Behavioral Science 24-3/1979, pp. 155–168, Howard Raiﬀa, Rand, IIASA, and the Conduct
of Systems Analysis, in: Agatha Hughes, Thomas P. Hughes (ed.), Systems, Experts, and
Computers. Cambridge/Mass., London 2000, pp. 433–461, Alan McDonald, Scientiﬁc Coop‐
eration as a Bridge Across the Cold War Divide: The Case of the International Institute for
Applied Systems Analysis (IIASA). IIASA Research Report RR-99-6, 1999, Giuliana Gemelli:
Building Bridges in Science and Societies during the cold War: The Origins of the International
Institute for Applied Systems Analysis (IIASA), in: G. Gemelli (ed.): American Foundations
and Large-Scale Research: Construction and Transfer of Knowledge. Bologna 2001, pp. 159–
198.
50
F. Dittmann

from the Soviet Union and the U.S. should work together on vital problems of mankind
like energy supply, environmental protection, food production, healthcare and the like.
The founding Nations explicitly excluded militarily relevant topics such as nuclear-
power and space-exploration. Johnson’s intended to build bridges in order to reduce
East-West tensions. He meant to increase human contacts, in this case, between scien‐
tists.
On the American side McGeorge Bundy was authorized to negotiate with the Soviet
side. Bundy was Professor in Harvard and between 1961 and 1966 security adviser of
President Kennedy and President Johnson. Then he headed the Ford Foundation.5 He
contacted in the USSR the deputy chairman of the State Committee for Science and
Technology Džermen Michajlovič Gvišiani. Gvišiani had worked since 1955 in GKNT
and founded in 1969 an Institute for complex management problems at the Academy of
Sciences of the USSR. In addition, he held a number of diﬀerent positions in interna‐
tional cooperation.6 Not least Gvišiani’s wife was the daughter of Soviet Premier Alexei
Kosygin.
Both sides agreed the cooperation should be limited not only to the superpowers.
Thus the Soviet Union insisted to include the GDR – a demand that has always led to
opposition in the West before the oﬃcial recognition of the socialist German state. The
problem was circumvented in which the established institution received the status of a
Non-Governmental Organisation (NGO). As a result, however, the ﬁnancing was more
diﬃcult because governments could not make direct grants to such institutions.
Bulgaria 
National Centre for Cybernetics and Computer Technique 
Canada 
Committee for the International Institute for Applied Systems Analysis 
Czechoslovakia 
Committee for the International Institute for Applied Systems Analysis 
France 
French Association for the Development of System Analyses  
FRG 
The Max Planck Society for the Advancement of Sciences 
GDR 
Academy of Sciences 
Italy  
Nation Research Council 
Japan 
Japan Committee for the International Institute for Applied Systems 
Analysis 
Poland 
Polish Academy of Sciences 
U.K. 
Royal Society of London 
USA 
National Academy of Sciences 
USSR 
Academy of Sciences 
Fig. 2. Founder organization of IIASA.
5 Who's who in American politics 1967–1968. New Providence 1967, In Memoriam McGeorge
Bundy, in: Options Fall/Winter 1995, p. 19.
6 Borys Lewytzkyj (ed.), Who's who in the Soviet Union. A biographical encyclopedia of 5000
leading personalities in the Soviet Union. Munich 1984, p. 56; In Memoriam Jermen Mikhai‐
lovich Gvishiani, in: Options Summer 2003, p. 25.
Technology vs. Political Conﬂict
51

Political and military crises overshadowed the six years of the negotiating process.
Some examples are the Vietnam War and the intervention of Red Army troops in
Czechoslovakia in 1968 to crush down the so called Prague Spring. Furthermore the
negotiators of the IIASA needed to circumvent the “German problem” which meant that
the FRG government would not accept activities which could be regarded as recognition
of East Germany. On top of all these diﬃculties, at the beginning of the 1970s the British
Government did expel hundreds of Soviet diplomats for spying. Nevertheless, in 1972
IIASA got started with an American director and a Soviet institute’s council chairman
(Fig. 2).
Since the mid-1970s IIASA worked on an experimental computer network called
IIASANET which connected the Research Institute for Applied Computer Technology
in Budapest, the Institute for Cybernetics in Bratislava, the Moscow Institute for Control
Technology, the Institute for Cybernetics in Kiev and the University of Technology
Vienna. Via the European Informatics Network (EIN) the institutions had access to other
computer networks, in particular to the ARPANET.7
The motivation of this eﬀort is remarkable because it was based not primarily on
technology but mostly on politics: As mentioned above though IIASA international
teams of scientists from diﬀerent blocks were meant to collaborate with each other,
however, during the Cold War it was diﬃcult to bring scientist together, because of
bureaucratic restraints in organizing journeys across the Iron Curtain, safety hurdles, or
shortages of foreign currencies in Eastern countries. The idea arose to avoid these prob‐
lems by using data transmissions: computer networks were supposed to support scien‐
tiﬁc work among scientists of the two diﬀerent blocks (Fig. 3).
3
The Origin – Wide Area Computer Networks for Military
Purposes
During the Cold War computers – and in particular network-technology, were regarded
as strategically important to the West and East because the advanced one were developed
and widely used in the military context. In the U.S. the leading concept was provided
by Semi-Automatic Ground Environment System (SAGE) built up between the 1950s
and 1963.8 SAGE was a control system in the U.S. for detecting, tracking and inter‐
cepting Soviet bomber aircraft. It was very important for the development of computing
and led to huge advances in online systems and interactive computing, real-time
computing, and data transmission via modems (Fig. 4).
7 A. Butrimenko, Computer Networking, in: IIASA Conference '76, 10-13 May, 1976. Vol. 2,
pp. 201–214.
8 See e.g. K. C. Redmond, Th. M. Smith, from Whirlwind to MITRE. The R&D Story of the
SAGE Air Defense Computer. Cambridge/Mass., London 2000. See additionally the special
issue of Annals of the History of Computing 5, 1983, No. 4. The great importance of SAGE
mentioned the Soviet scientist W. S. Gurow. See Grundlagen der Datenübertragung. Leipzig,
1969; pp. 285, 298 (Russian Edition, Moscow 1964).
52
F. Dittmann

In the Soviet Union computer networks were developed for military purposes as
well. Similar to SAGE, in 1953, a group of specialists at the Moscow Institute of Preci‐
sion Mechanics and Computing Machinery started to create a computer-based missile
defence system.9 This Moscow Institute and its director Sergey Alekseevich Lebedev
9 V. S. Burtsev, Distributed Systems: The Origins of Computer Networks in the USSR, in: G.
Trogemann et al. (eds), Computing in Russia. Braunschweig 2001, pp. 215–220; p. 216.
Fig. 3. a. IIASANET 1976 and 1983. b. IIASANET 1976 and 1983 Source: (left) A. Butrimenko,
A., Computer Networking, in: IIASA Conference ‘76, 10-13 May, 1976, Vol. 2, pp. 201–214; p.
210 and (right) I. Sebestyén, Experimental and operational East-West Computer Connections.
The telecommunication hardware and software, data communication services, and relevant
administrative procedures. Final Report. Laxenburg, IIASA, 1983; pp. 130–133.
Technology vs. Political Conﬂict
53

played an important role in the history of Soviet computing.10 In 1956 the ﬁrst experi‐
ments started. Radar stations sent information to a computer centre by remote data
transmission over roughly 100 to 200 km. There a special computer, called M-40 (see
graphics above), selected and digitized the data to plot the trajectories of the ﬂying object
and a second computer, called M-50, generated control signals. This equipment made
it possible to aim a rocket at the moving ballistic missile target automatically. At the
end of the 1950s, the experimental anti-aircraft system was replaced by an improved
one. The total length of the computer network now reached several hundred kilometres.
In 1969, the system was replaced again by an advanced anti-aircraft system. Data trans‐
mission lines were also used for information transfer from stations automatically
observing Soviet space ships and satellites.
Additionally ideas for using this military based technology in the civil sector arose
in the 1960s. During that time Viktor M. Glushkov,11 director of the Institute of Cyber‐
netics of the National Academy of Science of Ukraine, developed the concept of a
computer network for a nationwide economics control system called OGAS (All-Union
Automated System), therefore he was very important for the IIASA story.
10 G. D. Crowe, S. E. Goodman, S. A. Lebedev and the Birth of Soviet Computing, in: IEEE
Annals of the History of Computing 16, 1994, No. 1, pp. 4–24.
11 V. S. Mikhalevich, Academician Viktor Mikhailovich Glushkov, in: Cybernetics and System
Analysis 29, 1993, No. 3, pp. 303–306.
Fig. 4. Computer network of an experimental missile defense system 1955/56. Source: V. S.
Burtsev, Distributed Systems: The Origins of Computer Networks in the USSR, in: G. Trogemann
et al. (eds), Computing in Russia. Braunschweig 2001, pp. 215–220.
54
F. Dittmann

4
The Team – High-Ranking Scientists from East and West
Considering IIASA computer network activities one can ﬁnd several computer scientists
who are viewed as crucial for the history of the wide-area network and the internet in
general, like the US American internet pioneer Vint Cerf, the U.K. pioneers of computer
network technology Peter T. Kirstein, Donald W. Davies und D. L. A. Barber and last
but not least Louis Pouzin from France.12 From the Soviet side scientists should be
mentioned such as the Ukrainian Gennady Michailovič Dobrov, the IIASA project
manager for computer science Alexander Vasilevič Butrimenko and the important
computer specialist Viktor Michailovič Glushkov from Kiev. All these scientists partici‐
pated in several conferences on computer networks at IIASA in Laxenburg. It can be
assumed that these scientists discussed ideas and facts of their ﬁeld of expertise. This
paper is not the place to outline the activities of all of these individuals but to focus on
the concept of computer networks in general.
5
The Idea—Computer Centres Interconnected via Data
Transmission Lines
The idea of wide spread computer networks was established in the Soviet Union in the
1960s. Glushkov developed the concept of a computerized all-union planning and
management system, which he called OGAS.13 He participated at least twice at the
mentioned IIASA conferences.14
The OGAS system consists of a lot of computer centers which are connected elec‐
tronically and had diﬀerent layers.15 Companies use regional computer centers (marked
with No. 4). On the next layer above are computer centers of the Soviet republics (marked
with No. 3) and the next layer are computer centers of Soviet central planning institutions
(marked with No. 2). On the very top is the central Soviet government (marked with No.
1). A digital communication network connects all these centers hierarchically. The
objective of the system was to answer questions like the following: Which company
produces a certain product in the region and oﬀers free capacity? The answer should be
given in about 20 min, which stood for real time back then! Evidently, the Soviet political
leadership appreciated an instrument which promised total access from the All-union
planning institutions down to all companies and organizations hoping for a signiﬁcant
improvement of the central planning system. Glushkov supported such euphoria with
12 Proceedings of a IIASA Conference on Computer Communications Networks. October 21-25,
1974.
13 William J. Conyngham, Technology and Decision Making. Some Aspects of Development of
OGAS, in: Slavic Review 39, 1980, No. 3, pp. 426–444.
14 V. M. Glushkov, Computer Networks: Comments on a IIASA Research Activity, in: IIASA
Conference '76, 10-13 May, 1976. Vol. 2, pp. 235–237; Proceedings of a IIASA Conference
on Computer Communications Networks. October 21-25, 1974, S. VII.
15 V. M. Glushkov, Macroeconomic models and principles of OGAS, Moscow 1975 (in Russian),
Dmitrij G. Zhimerin, All-union automated system of planning (OGAS), Moscow 1975 (in
Russian).
Technology vs. Political Conﬂict
55

the very ambitious idea of a fully automated process for monitoring, planning, and
managing. He pronounced: “There is the opportunity of a non-stop planning process
which is closely connected with the process of managing. Again, this process of
managing allows a process of a continuous and uninterrupted correction of planning
speciﬁcations.”16 On the one hand Glushkov estimated costs of about 20 billion Rubles
for realizing the system during 15 years, but on the other hand he forecasted advantages
of 100 billion Rubles. It is evident from the current viewpoint that Glushkov totally
underestimated the complexities of the Soviet economic system. Therefore his ambitious
system was never realized, because of technical problems, political barriers, and lack of
money (Fig. 5).
Fig. 5. OGAS structure. Layers: 1 - All-union, 2 – Republic, 3 – Territory, 4 – Company. Source:
D. G. Zhimerin, Obscegosudarstvennaja avtomatizirovannaja sistema upravlenija (OGAS),
Moscow 1975, p. 42
However, such utopian ideas came up not only in the Soviet Union. Around the same
time in the U.S. J. C. R. Licklider,17 who is regarded today as a visionary of the infor‐
mation society developed the idea of Thinking Centers combining functions of classic
libraries with the advantages of electronic information processing.18 Licklider estimated
that between 1970 and 1975 such centers would come into existence. Licklider envi‐
sioned that computers would help to solve complex problems in a ﬂexible way in which
human beings determine aims, hypothesis and criteria whereas computers do the routine
work. Interestingly enough Glushkov’s and Licklider’s visions are very similar:
16 Viktor M. Gluschkow, Dialogsystem in der Planung, in: Rechentechnik / Datenverarbeitung
11, 1974, No. 2, pp. 5–6, p. 6.
17 Chigusa I. Kita, J. C. R. Licklider's Vision for the IPTO, in: IEEE Annals of the History of
Computing July-Sept. 2003, pp. 62–77.
18 J. C. R. Licklider, Man-Computer Symbiosis, in: IRE Transactions on Human Factors in Elek‐
tronics HEF-1, March 1960, pp. 4–11; J. C. R. Licklider, The Computer as a Communication
Device, in: Science and Technology April 1968. Both articals appeard as reprint in: R. W.
Taylor (ed.), In memoriam: J.C.R. Licklider: 1915–1990. Palo Alto/Ca. 1990.
56
F. Dittmann

Whereas Licklider emphasized knowledge management in a more general way to over‐
come the so-called information overﬂow, which was widely discussed during that time,
Glushkov focused on information processing of economically important data.
Licklider stressed the advantages of using computers in team-working processes in
particular to communicate over a long distance. This he called “on-line interactive
communities”. Exactly this idea became the starting point in a report about the
mentioned experimental data transmission by Dobrov ten years later (1977).
6
Summary—International Contacts Across the Iron Curtain
Never Stops
Although, during the Cold War scientiﬁc cooperation between East and West needed to
overcome a lot of hurdles, scientiﬁc contacts never ceased. In particular, international
conferences oﬀered Eastern computer scientists the opportunity to meet their Western
colleagues. Since 1977 about every four years an International Congress on Computer
networks organized by IFIP took place in Budapest. Besides these conferences in the
Hungarian capital other international conferences took place in Eastern countries, for
example, conferences in Karlovy Vary (Czechoslovakia) or symposiums organized by
the Centre for Computer Technology of the East German Academy of Sciences. In
general the USSR and other COMECON countries worked very actively as members in
international organizations such as in the International Federation of Automatic Control
(IFAC) or the International Federation for Information Processing (IFIP). Additionally,
the CCITT (French abbreviation of the International Committee for Telephone and
Telegraph Systems) played a key role in East-Western cooperation on network tech‐
nology. Traditionally, most of the national PTTs are CCITT members. Although the
CCITT standards were only recommendations most of the members adopted them into
national law: only in this way was communication guaranteed without a reference to the
diﬀerent producers.
The development of ideas in science and technology is a process of international
cooperation, not limited to East or West. The concept of wide-area networks is a result
of a world-wide discussion, as mentioned in this paper. Whereas Licklider emphasized
knowledge management in a more general way to overcome the so-called information
overﬂow, Glushkov OGAS focused on information processing of economically impor‐
tant data. In both cases development of wide-area networks was not driven by the tech‐
nocratic idea to bring computer technology to its limits but by the hope of more eﬀective
information management.
Technology vs. Political Conﬂict
57

There and Back Again – Andrew Booth, a British
Computer Pioneer, and his Interactions with US
and Other Contemporaries
Roger G. Johnson
(✉)
School of Computer Science, Birkbeck College, London University, London, UK
rgj@dcs.bbk.ac.uk
Abstract. This paper explores the interchanges between Andrew Booth, an early
British computer pioneer and contemporary US and other pioneers. The paper
records how funding from the US Rockefeller Foundation supported Andrew
Booth’s research work in the UK and allowed him to reﬁne his ideas on computer
design by visiting US pioneers each year from 1946 to 1948. This led to the
construction of an electronic drum, the world’s ﬁrst successful demonstration of
a rotating storage device connected to a computer, to his pioneering work on
natural language processing and ﬁnally and most notably to his invention of the
Booth hardware multiplier which is the basis of the multiplier used in billions of
chips each year.
Keywords: Andrew Booth · APERC · HEC · Magnetic drum · Booth multiplier ·
British Tabulating Machine Ltd
1
Background
The story starts slightly improbably in the world of the crystal structure of explosives.
In the closing days of World War II, the leading British scientist, (Desmond) Bernal was
planning his return from war service to the quieter world of academia. He held the Chair
of Physics at Birkbeck College, London University and decided to form a group of
academics to examine the structure of crystals using X-rays, work which contributed in
the 1950s to the discovery of the double helix. This involved solving large sets of equa‐
tions which, before computers, had to be done largely by hand using mostly simple
electro-mechanical calculators and it took weeks to complete just one set of calculations.
Bernal decided that one of the four groups in his new department would be devoted
to the mechanisation of these calculations. His attention was drawn by Douglas Hartree,
then a mathematics professor at Manchester and builder of a large diﬀerential analyser
before the war, to a talented young man, Andrew Donald Booth (born Feb 11th 1918 and
died Nov 29th 2009), who had a PhD on the crystal structure of explosives and an enthu‐
siasm for mechanising the calculations. In 1975, in an interview with Christopher Evans
for the London Science Museum, [1], Booth related how during the war he managed a
small team of women doing calculations derived from x-rays of crystal structures and
© IFIP International Federation for Information Processing 2016
Published by Springer International Publishing AG 2016. All Rights Reserved
A. Tatnall and C. Leslie (Eds.): HC 2016, IFIP AICT 491, pp. 58–70, 2016.
DOI: 10.1007/978-3-319-49463-0_4

being by temperament a mathematician I don’t like arithmetic. …. I didn’t think much of the
methods they were using and I tried to do two things. In the ﬁrst place I devised some better
mathematical methods … but I also made one or two small hand calculators.
Joining Birkbeck in late 1945 to head Bernal’s new group on mechanising the calcu‐
lations, Booth proved an excellent choice combining a ﬁrst class mathematical mind
capable of working on the complexities needed to handle the mathematics of X-ray
crystallography to obtain but most importantly for computing, he was described by those
who later worked with him as a “natural engineer”. His engineering skills were perhaps
derived from his father who was a retired marine engineer and part time inventor.
He was then working on a mechanical Fourier synthesizer, [2]. However, at an early
stage he incorporated relays into the device and named it the Automatic Relay Calculator
(ARC). Due to a lack of space at Birkbeck, which had been badly damaged by bombing
in 1941, the calculator was built at the British Rubber Producers Research Association
(BRPRA) in Welwyn Garden City who sponsored the project and where Booth had
worked brieﬂy before joining Birkbeck (Fig. 1).
Fig. 1. Andrew Booth and Kathleen Britten (in the foreground) at work in December 1945
2
First Visit to the USA 1946
Hartree had visited a number of US computing projects in 1945 and was aware of the
EDVAC project. On his return to the UK he shared his observations with the nascent
UK computing community, including both Wilkes at Cambridge and Booth at Birk‐
beck. Hartree suggested to Bernal that, in view of the lack of funding available in the
There and Back Again
59

UK, a grant should be sought from the Rockefeller Foundation (RF) in the USA (from
whom Bernal had already received several grants) to allow Booth to make a short
visit to the USA starting in May 1946 to attend the conference of the American
Society for X-Ray and Electron Diffraction and visit a number of crystallographic and
computing projects.
The RF archives reveal that Booth made a very good impression on Warren Weaver,
Director of the RF’s Natural Sciences Division. Weaver’s notes of their ﬁrst meeting on
June 20th 1946, [3], record
He appears very young (under 25) [he was actually 28] and is most attractive in every way. He
seems to be equally interested and competent in the mathematical and instrumental aspects of
his subject.
After attending the conference Booth was able to visit a number of researchers along
the eastern seaboard. So soon after the end of the war and with much of the research
funded by the US government it is unsurprising that Booth felt some of those to whom
he talked were rather reticent in their comments. An exception was John von Neumann,
although the latter’s friendship with Bernal may well have been helpful. In the cover
letter, dated July 3rd 1946, to his ﬁnal report Booth, [4], notes that
In addition to purely crystallographic projects I have seen most of the special calculating
machines. With the exception of that of Dr von Neumann, the techniques are well known to me.
I had a long discussion with Dr von Neumann and came to the conclusion that a machine built
in accordance with some of his ideas would revolutionise crystallographic computing, and would
be especially applicable to problems of biological interest. I should very much like to discuss
with you the possibility of my spending six months to Princeton and, if possible developing such
a machine.
3
Back in the UK 1946
Following the end of World War 2, four groups in the UK were looking at building
digital computers. The best known were at Manchester (associated with Freddie
Williams and Tom Kilburn) and at Cambridge (led by Maurice Wilkes). The third group
was at the National Physical Laboratory, Teddington (led by Jim Wilkinson and Ted
Newman using a design by Alan Turing).
The fourth group was that of Booth at Birkbeck College, London University. He was
working with limited resources, both human and ﬁnancial, and concentrating, as a result,
on building smaller machines. He had a radical ambition for that time of building a
computer that was cheap enough that each university could own one! This was at a time
when the UK’s National Physical Laboratory (NPL) ACE computer was being talked
of (at least at NPL) as suﬃcient for the whole of the UK’s computational needs!
4
Second Visit to USA 1947
Obtaining the funding for a six month US tour in 1947 was diﬃcult due to the continuing
severe currency exchange controls in the UK. The basic funding for expenses within the
60
R.G. Johnson

USA was provided by the RF while the University of London paid Booth’s fare on the
SS Aquitania. Booth chose to base himself at the Institute of Advanced Studies at
Princeton with John von Neumann. Booth was accompanied by his research assistant,
Kathleen H V Britten, who was soon to become his wife. In order to provide additional
funding for the trip, Bernal put Booth in touch with Isidor Fankuchen, an Adjunct
Professor of Physics at Brooklyn Polytechnic who organised for Booth to go on the
lunch lecture circuit right across the States talking about contemporary life in Britain,
[1]. This enabled them to visit all the major centres of research on x-ray crystallography
and, more importantly, computing across the whole country.
This visit was transformative for Booth and Britten. It can be diﬃcult for the modern
reader to appreciate the challenge faced by the computer pioneers lacking any consensus
“blueprint” for the architecture or components of a computer. From their discussions
with John von Neumann, they were introduced to what we now regard as the standard
architecture of a computer and which is widely known as the “von Neumann” architec‐
ture. Inspired by this discussion, on the liner on the return trip, he redesigned ARC to
have a von Neumann architecture which he referred to as ARC2. This machine used 800
high-speed relays to form a parallel operation single address code computer.
5
The Challenge of Memory
Computer architectures have to be realised using electronics and at that time there was
no consensus on the appropriate technology even for fundamental components, such as
the computer memory or input and output. Booth and Britten set out the technological
options for the components of a computer with a “von Neumann” architecture in a paper
which circulated among the growing community of computer pioneers during 1947.
Such was the interest, they produced a second edition later that year [5].
The heart of the “von Neumann” architecture is the memory. In their paper the Booths
evaluated all of the physical properties including heat, light, sound and magnetism and
concluded that magnetism oﬀered the best prospects because of its persistence.
Booth was committed to building a low cost computer and so he needed low cost
components. The two immediate options for a memory were mercury delay lines and
CRT display tubes both of which were very costly. On his trip around the USA he had
seen a simple recording device, called the Brush Mail-a-Voice, Fig. 2, which was
sold for use in commercial offices to enable managers to record letters on to
magnetic oxide coated paper discs for typing by their secretarial staff. However, in
order to achieve the performance needed to act as the memory of a computer he had
to rotate the paper disc much faster than for simple voice recording. At this higher
speed, it proved impossible to keep the disc flat and so he had to abandon this first
attempt at a “floppy disc”.
There and Back Again
61

Fig. 2. Brush Mail-a-Voice dictation machine
Undaunted he decided to try a diﬀerent approach and designed a memory using a
brass drum with a nickel coating around the outside. The ﬁrst drum was mounted on a
horizontal axle and about the size of a cotton reel, being 2 inches in diameter with a
modest packing density of just 10 bits per inch, Fig. 3.
Fig. 3. Booth’s ﬁrst drum Feb 1948 (Photo courtesy of NMSI, London)
62
R.G. Johnson

Thus it was that Andrew Booth built the world’s ﬁrst rotating electronic storage
device connected to a computer albeit a drum rather than the now ubiquitous disc. The
drums were built by his father and together they created a company called Wharf Engi‐
neering Ltd which manufactured drums and other computer peripherals commercially
until the Booths left the UK in 1962. Having proved the drum concept worked, the
Booths built a larger nickel-plated brass drum with 256 words of 21 bits to provide the
memory for ARC2. It was completed by the end of 1947 and became operational and
was still in service at Birkbeck in 1954. The prototype drums are now in the care of the
Science Museum, London.
In a paper at a conference on international communities of invention and innovation
it is worth noting that Booth gave the design of ARC2 to Adriaan van Wijngarden in
Amsterdam for the construction of ARRA, the ﬁrst computer in the Netherlands.
6
Building Electronic Computers
During 1948, Andrew Booth redesigned the ARC2 as an entirely electronic machine
which he called the Simple Electronic Computer (SEC). This was built by Norman Kitz
(formerly Norbert Kitz) in 1949/50 see Fig. 4.
Fig. 4. SEC with Booth’s ﬁrst practical drum in the foreground 1950
An interesting historical footnote is that Norman Kitz left Birkbeck to work for
English Electric at NPL on the DEUCE computer. From there he moved to Bell Punch
There and Back Again
63

and designed the world’s ﬁrst electronic desktop calculator, called ANITA. While Booth
never completed a desktop calculator, it seems likely that he inspired one of his students
to do so.
SEC used a comparatively small number of 230 valves in its construction and
employed a two address set of operation codes. It had a memory of just 256 words of
21 bits. Although Norman Kitz completed its construction the machine never became
operational, and using the lessons learnt from SEC, Andrew Booth moved swiftly on to
create his best known computer design, the All-Purpose Electronic Computers series of
computers, (each identiﬁed as APE(*)C where the * is the initial letter of the sponsoring
body).
The ﬁrst machine of the series, known as APE(R)C, ran successfully for the ﬁrst time
in May 1952, Fig. 5. The (R) signiﬁed the British Rayon Research Association for whom
it was built. It was constructed at the Booths’ home in the small village of Fenny
Compton in Warwickshire before being moved to Birkbeck for commissioning and
subsequent delivery to the BRRA in Manchester in July 1953. Once the APE(R)C
computer was moved to Birkbeck, construction of a larger version, known as APE(X)C,
X for X-ray, for the Birkbeck X-ray crystallographers began in Warwickshire. Both
these machines used 500 valves. APE(R)C had a drum with 512 words of 32 bits rotating
at 3,000 RPM and ran at 30 kHz. By removing some redundant logic, APE(X)C used
only 420 valves but ran at 60 kHz and had a much larger drum with over 1,024 words
of 32 bits. Input/output on APE(X)C was by either teletype and paper tape.
Fig. 5. APE(R)C whose circuit designs Booth shared with others 1952
64
R.G. Johnson

In 1950 Booth provided the design of APE(R)C to Norwegian researchers led by
Thomas Hysing who named their computer NUSSE (Norsk Universell Siﬀermaskin,
Sekvensstyrt, Elektronisk) and was Norway’s ﬁrst electronic computer. This machine
is now in the Norwegian Museum of Science and Technology, Oslo, Fig. 6.
Fig. 6. NUSSE Norway’s ﬁrst computer derived from Booth’s design 1954
7
Booth Multiplier
If the drum reﬂected Booth’s engineering talent, then the Booth multiplier was a demon‐
stration of his mathematical skill. A key component of any computer design is the arith‐
metic unit and to provide fast arithmetic it is necessary to have hardware multiplication
and division. When Booth visited von Neumann in 1947 he obtained details of von
Neumann’s design for both a hardware multiplier and divider. Booth described them in
his interview with Evans as “a beautiful divider” but the multiplier as “an abortion” [1].
When Booth asked von Neumann why he had not used a similar approach in his multi‐
plier as in the divider, von Neumann assured him it was a theoretical impossibility and
Booth accepted the great man’s opinion. Booth told Evans that when he was designing
the APEC computer series he realised that von Neumann was wrong and Booth recol‐
lected how, over tea with his wife in a central London cafe, he designed a non-restoring
binary multiplier which, with a subsequent minor modiﬁcation by a colleague, is the
Booth multiplier which is still in use today.
Basically the Booth multiplier follows the usual method for long multiplication of
summing partial products. However, it also uses a “trick” that to multiply in decimal by
a string of 9s it is possible to left shift an appropriate number of places and subtract the
There and Back Again
65

multiplier from the result. This approach works even better in binary where it results in
a simple rule for multiplying by a string of 1s:
• Examine each pair of digits in the multiplier starting at the least signiﬁcant digit
creating the ﬁrst pair by appending a dummy 0 at the least signiﬁcant end of the
multiplier, then
i. If the pair is 01, add the multiplicand
ii. If the pair is 10, subtract the multiplicand
iii. Otherwise, do nothing
• Shift both the partial product and multiplier one place right and examine the next pair
of digits
• Repeat as many times as there are digits in the multiplier.
This was submitted for publication in August 1950 and published the following
year [8].
8
Natural Language Translation
As quoted above from the covering letter to his ﬁnal report of the 1946 visit, Booth
raised the possibility of the RF funding the construction of a computer in London to
support the work of x-ray crystallographers.
On August 11th 1947 Booth visited Warren Weaver and asked him if the Foundation
would fund an electronic computer for London University. Weaver said that the Foun‐
dation could not fund a computer for mathematical calculations which he felt had been
done but that he had begun to think about using a computer to carry out natural language
translation and that the Foundation could fund a computer for research in that area.
As a result, Birkbeck became for the next ﬁfteen years a leading centre for natural
language processing research. On May 23rd 1948 Weaver accompanied by Gerard
Pomerat visited Booth’s laboratory, Fig. 7, still housed in the premises of the British
Rubber Producers Research Association at Welwyn Garden City. The tiny memory on
early computers meant it was very diﬃcult to do any serious natural language processing.
Booth gave them a demonstration of the machine, despite a faulty relay! Weaver in his
note of the meeting expresses surprise and disappointment that the very small machine
simply performs a simple word lookup and that the researchers had not yet considered
the problem of a word which has multiple meanings, [6]!
The Booths, with their research students, developed techniques for parsing text and
also for building dictionaries. They published numerous books and papers on text
processing including creating Braille output and natural language translation. On
November 11th 1955 the laboratory gave an early public demonstration of natural
language machine translation, Fig. 8. Their wider contribution to this community has
been assessed by John Hutchins in his paper [7].
66
R.G. Johnson

Fig. 8. Output from an early public demonstration of natural language translation 1955
Fig. 7. Warren Weaver and Gerard Pomerat visit ARC May 1948
There and Back Again
67

9
Commercial Success
As noted earlier, the Booths built their computers in a barn at their home in Warwickshire
where Andrew Booth’s father also lived. So it was to this barn in a freezing March 1951
that a three man team led by Raymond “Dickie” Bird from British Tabulating Machines
(BTM) came to visit. BTM were the UK’s leading supplier of punch card systems
throughout the British Commonwealth and their management had decided that they
needed a small computer to improve the calculating power and ﬂexibility oﬀered by
their tabulators.
At the time that BTM joined forces with Andrew Booth there were, as described
earlier in this paper, three other electronic computer projects in the UK. However, two
of the other three, Cambridge and Manchester Universities, already had commercial
partners while the third, the government National Physical Laboratory (NPL), was not
seeking a commercial relationship. Further, Manchester and NPL were building large
and expensive scientiﬁc computers while Lyons were already building a commercial
computer, LEO, with assistance from Cambridge. Consequently, BTM had little option
but to cooperate with Andrew Booth.
In just a few days Raymond Bird’s team had copied Booth’s circuitry from APE(R)C.
Returning to BTM’s factory at Letchworth they added extra I/O interfaces and named
the resulting computer the Hollerith Electronic Computer (HEC), see Fig. 9. This proto‐
type computer is one of the world’s earliest surviving electronic computers, unlike so
many early machines which were dismantled when no longer needed, and is now on
display at The National Museum of Computing at Bletchley Park.
Fig. 9. BTM HEC1 copied Booth’s APE(R)C circuits 1951
68
R.G. Johnson

BTM moved ahead rapidly, completing the construction of HEC1 by the end of 1951
and having it operational later in 1952. BTM management then decreed that the HEC
would go to the Business Eﬃciency Exhibition, the major UK tradeshow, in June 1953
and so a new pre-production prototype (HEC2) was built contained in a smart metal
cabinet suitable for demonstration to the public. Two pieces of software were written to
attract public interest at the exhibition, one read in 13 out of 52 specially prepared
punched cards each representing a diﬀerent playing card and proposed an opening
contract bridge bid while the other played noughts and crosses.
Eight similar machines were built as the HEC2 M starting in 1955 mainly for tech‐
nical applications with a sale price of £25,000. One of these machines was India’s ﬁrst
electronic computer, [9]. The successor was the HEC4 which was for commercial data
processing with nearly 100 being sold in the UK and abroad. At the end of the 1950s
this was the UK’s best-selling computer by volume. After BTM merged with Powers
SAMAS in 1958 to form International Computers & Tabulators Ltd (ICT) it became the
ICT 1200 range. For many companies, and in some cases for whole countries, this was
their ﬁrst electronic computer and opened the door to delivering the wider beneﬁts of
eﬀective IT.
10
End of the Story
One notable landmark was Kathleen Booth’s book on programming the APE(X)C
computer, [10]. Published in 1958, this was among the early books on programming and
unusual in having a female author. Throughout their partnership, she focussed on the
programming while Andrew Booth concentrated on the design of their computers.
Andrew and Kathleen Booth resigned from Birkbeck College at the end of the
1961/62 academic year. Andrew Booth moved to Canada where he continued his distin‐
guished academic career initially at the University of Saskatchewan and subsequently
as President of Lakehead University, Ontario. While they continued their research the
only machine built in Canada was an updated version of their ﬁnal British design.
11
Conclusions
From the perspective of this conference Andrew and Kathleen Booth were both initially
substantial beneﬁciaries from and then substantial benefactors to the international
computing community. The initial support from the Rockefeller Foundation, especially
in 1946 and 1947, gave Andrew Booth access to US researchers and John von Neumann,
in particular, at a critical time in the development of his ideas for the design and
construction of a computer.
He shared his designs with others, notably in the Netherlands and Norway, enabling
them to build their own computers. The design of his APEC computers was used to
provide the initial design of the highly successful ICT 1200 series computers, primarily
used for commercial data processing and which gave access to the beneﬁts to humanity
of eﬀective IT for the ﬁrst time to many places.
There and Back Again
69

The results of Andrew Booth’s successful demonstration of connecting a drum to a
computer were shared with others in the UK and USA. Although the drum was to be
superceded by the disc, the successful use of a magnetic oxide coating for the storage
and retrieval of data by a computer was a critical step.
Their contribution to the early work on natural language processing and translation
is clear from the review literature. Andrew Booth’s initial interest was aroused by
Weaver’s conversation with him. Much natural language research is by necessity an
international collaboration. In this paper there has been time to do little more than note
the work of both Andrew and Kathleen Booth on this topic, including their extensive
international collaborations.
However, if impact is to be measured by volume, then Andrew Booth’s publication
of his paper on the Booth multiplier is his major achievement and it remains the basis
for multiplication circuits used by chip manufacturers in billions of chips worldwide
each year.
Andrew Booth (1918–2009) is largely unknown outside the specialised world of the
computer historian but he, together with his wife Kathleen, deserve far greater recog‐
nition by anyone whose computer writes to a hard disk or executes a multiplication
instruction.
References
1. Evans, C.: Pioneers of Computing. Audio interview with A D Booth, Pioneers of Computing
No 9. Science Museum, London (1975) (Unpublished)
2. Booth, A.D.: Fourier Technique in X-ray Organic Structure Analysis. Cambridge University
Press, New York (1948)
3. Memo of meeting between Weaver and Booth dated June 20th 1946. Rockefeller Foundation
Group 1.1, Series 401D, Box 34, Folder 434
4. Memo of meeting between Weaver and Booth dated July 3rd 1946. Rockefeller Foundation
Group 1.1, Series 401D, Box 34, Folder 434
5. Booth, A.D., Britten, K.H.V.: General Considerations in the Design of an All Purpose
Electronic Digital Computer, 2nd edn. Princeton University, Princeton (1947)
6. Memo of visit by Weaver and Pomerat to ARC dated May 23rd 1948. Rockefeller Foundation
Group 1.1, Series 401D, Box 34, Folder 434
7. Hutchins, W.J.: From ﬁrst conception to ﬁrst demonstration: the nascent years of machine
translation, 1947–1954. Mach. Transl. 12(3), 195–252 (1997)
8. Booth, A.D.: A signed binary multiplication technique. Q. J. Mech. Appl. Math. 4(Pt2), 236–
240 (1951)
9. Rajaraman, V.: History of computing in India (1955–2010). IEEE Computer Society History
Committee (2012)
10. Booth, K.H.V.: Programming for an Automatic Digital Calculator. Butterworth Scientiﬁc
Publications, London (1958)
70
R.G. Johnson

‘Machines à Comparer les Idées’ of Semen Korsakov:
First Step Towards AI
Valery V. Shilov
(✉) and Sergey A. Silantiev
National Research University Higher School of Economics, Moscow, Russia
{vshilov,ssilantiev}@hse.ru
Abstract. This paper is devoted to forgotten Russian scholar Semen Korsakov
and describes his life and scientiﬁc activity. The particular attention is paid to
Korsakov’s main achievement – invention of ﬁve “intellectual machines”. These
machines could be considered as the very ﬁrst attempt to design a mechanical
device capable to perform such intellectual operations as data analysis, compar‐
ison, and selection.
Keywords: Semen Korsakov · Logical machine · Intellectual machines ·
Homeoscope · Ideoscope · Comparator
1
Introduction
The scientiﬁc discussions on the theme “can the machine think?” (i.e. on the possibility
of creation of artiﬁcial intelligence) began in 1950s when after the development of ﬁrst
electronic computers Alan Turing published his paper “Computing Machinery and
Intelligence” (1950; later this paper was published under the title “Can the Machine
think?”). Philosophical (and quasi-philosophical!) discussions on these problems
reached their peak in 1960–1970s. But in fact, when at the end of 1970s the program to
design the ﬁfth generation computer had been announced in Japan (so called “Japanese
challenge”) and appropriate programs also had started in USSR and USA the main aspect
of these discussions was the necessity of computer “intellectualization”. It meant not
only the extending of ﬁelds where computer can solve various tasks eﬀectively but also
the rising of the level of communication between a computer and a man (even up to the
level of natural language).
Logic machines are very interesting phenomena in the history of logic and computer
technology. Martin Gardner gave a remarkable review of the history of logical machines
in his classic work [1]. However, he did not describe some of them because of various
reasons. Logical machines built in Russia are practically unknown abroad. Gardner did
not mention them in his book. The most complete history of logical machines is
described in [2] and its short description can be found in publication [3]. In our previous
work [4] we described life, scientiﬁc activity and logical machine of Russian Professor
Alexander Schukarev. In this article we tell about the ﬁve extremely original logical (or
intellectual) machines invented by his predecessor Semen Korsakov (Fig. 1).
© IFIP International Federation for Information Processing 2016
Published by Springer International Publishing AG 2016. All Rights Reserved
A. Tatnall and C. Leslie (Eds.): HC 2016, IFIP AICT 491, pp. 71–86, 2016.
DOI: 10.1007/978-3-319-49463-0_5

Fig. 1. Semen Korsakov
2
Semen Korsakov: Biography
He was born on January 14, 1787 in the famous Russian family. He was the ﬁrst child
in the family of Colonel Nikolay Korsakov and his wife Anna, born Mordvinova.
Nikolay Korsakov was not only a military man but an outstanding civil engineer.
For example, he was the head of construction of the Kherson fortress and developed a
ten-year plan for the construction of Sevastopol. Actually this city, famous in Russian
history, was built according to his plan. Prince Potemkin wrote in his letter to the
Empress Ekaterina the Great: “Korsakov, Mother, is an engineer we had never before…
This man should be preserved”.
Colonel Nicolay Korsakov perished on August 24, 1788 during the war between
Russia and Ottoman Empire. After the death of her husband Anna Korsakova and her
son lived in the family of her brother Nikolay Mordvinov who was taking care of them
all his life and fully replaced the father to Semen Korsakov. Count Mordvinov (1754–
1845) was a Russian naval commander and prominent public and state ﬁgure. He was
the author of several books on economics, ﬁnance, banking and agriculture. In 1825–
1840 he was a Chairman of the Free Economic Society and in 1829 Mordvinov received
the title of academician of the St. Petersburg Academy of Sciences.
In 1805 Semen Korsakov served as a “collegiate junker” in the Archives of Colle‐
gium of Foreign Aﬀairs with other boys from noble families who preferred the civil
service to the military one.
At the beginning of 1807 he became adjutant of his uncle who was the Head of militia
of Moscow Province established according to the Manifesto of Emperor Alexander I
“On the preparation and formation of widespread temporary home guard or militias”
from November 30, 1806 in one of the most tense moments of struggle with Napoleon.
However, after the signing of Treaties of Tilsit on July 7, 1807 it was no longer a need
for the militia, dismissed at the beginning of the next year. Korsakov who during the
72
V.V. Shilov and S.A. Silantiev

service in militia received the rank of titular councilor returned to Collegium of Foreign
Aﬀairs. Soon he was promoted to the rank of collegiate assessor and then retired.
Semen Korsakov had joined the St. Petersburg militia in the beginning of the mobi‐
lization in 1812 and on 5th September marched against Napoleon. He participated in the
assault of Polotsk (8th October), Chashniki battle (29th October), battles under Smolyans
(13th–14th November) and on the Berezina river (26th–29th November). After the expul‐
sion of Napoleon from Russia he took part in all foreign campaigns of the Russian army.
He was contused during the siege of Danzig but ﬁnally succeeded to enter Paris.
Korsakov was awarded the Order of Vladimir of IV degree, as well as the Prussian Order
“For Merit” (“Pour le Mérite”). Later, the Order of Anna of II degree was added to his
other military awards.
In fall of 1817 Korsakov began to serve in the Ministry of Police where he worked
as a statistician and then continued his service in the statistics department of the Ministry
of Justice.
In 1827 Korsakov purchased Tarusovo estate in the Dmitrov district of the Moscow
province, and settled in it. However, since his duties did not require his constant presence
in St. Petersburg, he continued to serve in the Ministry of Justice. Beginning from 1835,
he was serving as an oﬃcer for special assignments at the Ministry of the Interior and
from time to time visited St. Petersburg and other places. In 1842 Semen Korsakov
received a high rank of State Councillor (that corresponds to the military rank between
the Colonel and Major-General) and retired. But next year he was invited to lead the
Department of Statistics in the Ministry of the Interior and the family had to move to St.
Petersburg for some time. However, already by 1845 Korsakov again retired and ﬁnally
settled in Tarusovo.
There Semen Korsakov died on December 1st, 1853 and was buried in the churchyard
of Trinity Church of the neighbouring village Troitsa-Vyazniki (Fig. 2).
Fig. 2. The grave of Semen Korsakov
‘Machines à Comparer les Idées’ of Semen Korsakov: First Step Towards AI
73

3
Semen Korsakov as Researcher
Although Korsakov never received a formal education, he was a very educated man. In
his youth in Moscow he attended particularistic courses of physics and mineralogy,
which were delivered at home by gymnasium professors. He was ﬂuent in German,
French, English and probably knew Latin. Later, in St. Petersburg he attended lectures
on natural law and in May 1811 successfully passed exams in law, economics, physics,
mathematics, history, geography and statistics at the Pedagogical Institute (most likely,
it was the certiﬁcation required for the service and advancement in rank). Scientiﬁc
instruments, machines and mechanisms caused particular interest of Korsakov. He
wanted to understand the principle of their action and always acquainted himself with
the various production technologies at the factories he visited during the service. He
built the home laboratory and carried out various experiments.
Korsakov was also an inventor. For example, his pocket astrolabe Nikolay Mordvinov
“found very useful”. The lack of academic education sometimes left its mark on his work.
Thus, trying to explore the hidden possibilities of a man Semen Korsakov in 1828 invented
a device that was named “Dinamoscope”. It consisted of coloured wax balls suspended on
strings to the grid. Participants of experiments tried to force these balls to swing, or to stop
by their willpower.
In addition to scientiﬁc experiments, Korsakov enthusiastically studied also medi‐
cine and especially homeopathy. Homeopathy at that time was still little known in
Russia. Semen Korsakov became not only a promoter of homeopathy, but made a
signiﬁcant contribution to its development. Already at the beginning of 1829 on his
estate he began to practice medicine. In a special notebook, which Korsakov was care‐
fully keept for ﬁve years it was marked 11725 requests for help! He pondered on the
nature of homeopathic remedies and their eﬀects, conducted many experiments, trying
to achieve the maximum eﬃciency. And very soon the practical results of his work were
widely recognized. Several articles of Korsakov were published in Germany in the
world’s ﬁrst journal about homeopathy “Archiv für die homöopathische Heilkunst”
(Fig. 3).
Founder of homeopathy German physician Samuel Hahnemann (1755–1843) who
was in correspondence with Korsakov highly appreciated his achievements in this ﬁeld.
Hahnemann wrote to Korsakov in March of 1832:
“I admire the zeal with which you have dedicated yourselves to the beneﬁcent art of
therapeutic, not only to be able to treat members of your family and others, but also in
order to penetrate in the secrets of nature that is proven by your valuable notes” [2, p.
93].
Correspondence with another respondent Nikolay Muravyov (1768–1840) whom
Korsakov himself drew in homeopathy had very important consequences. Nikolay
Muravyov was not only a famous military and public ﬁgure but also had serious knowl‐
edge of mathematics and natural sciences. In one of his letters to Korsakov Muravyov
explained “methodical table of painful symptoms cured by each drug”:
“All of these symptoms (say, 1000) are supposed to be arranged alphabetically by
numbering each one of them. After that, on the sheet divided into columns according to
the number of drugs with the inscription of the speciﬁc drug in each column, one should
74
V.V. Shilov and S.A. Silantiev

mark by numbers all the symptoms cured by the drug in alphabetical order. Then, during
the treatment for a correct choice of drug all seizures of the patient should be written on
a special sheet and designated by numbers. After that they should be compared with the
table and the drug with the more coincided numbers must be chosen. This idea
[Korsakov] probably later tried to use for the development of his Homeoscope. Twenty
tables found in his papers with graphic designations for review of the action of various
plant and mineral drugs, acids, alkalis, leaves, roots, crusts, etc. on the diﬀerent parts of
the human body could be the reason for this conclusion” [2, p. 95].
Muravyov’s idea had come on already prepared soil. Statistics studied by Korsakov
during his service stipulate extremely labor-intensive processing of large amounts of
data. It is very diﬃcult (and sometimes virtually impossible) to perform manually.
Therefore, we may assume that his thoughts to store such data in a structured (tabular)
form appeared earlier. Korsakov put in tables the information about the victims on
December 14th, 1825, his patients, their diseases and medicaments. And it is clear that
his innovative idea of mechanization of the labor-intensive process of search and selec‐
tion of necessary information ﬁnally took shape in connection with his interest of
homeopathy.
4
The Letter to St. Petersburg Academy of Science
On September 11th, 1832 collegiate councillor Semen Korsakov sent a letter (in French,
which was the common language of communication for Russian scientists at that time)
to the St. Petersburg Academy of Science of the following content:
Fig. 3. The cover of the journal and the ﬁrst page of the Korsakov’s article
‘Machines à Comparer les Idées’ of Semen Korsakov: First Step Towards AI
75

“I have discovered the new very important in its applications investigation method. Taking to
account that its immediate publication will be useful for human knowledge I felt it my duty to
present ﬁve models of intellectual machines. All of them work on common principle. I hope the
Academy can evaluate my own free will to reject the exclusive privilege I might have demanded.
I also expect from Academy to establish special Commission for consideration of my method
and results of its application for various actions of human mind.
Your obedient servant,
Semen Korsakov, collegiate councillor
St. Petersburg
September 11, 1832” [5, p. 557].
The four-page document (also in French) under the title “Report about the new
investigation method” was attached to the letter. Korsakov described in details his
proposals. Next day the letter was received and read. Academicians treated it seriously
and reacted quickly. A week later the General Assembly of Academy considered the
Korsakov’s proposals and decided: “To establish the Commission of Mr. Parrot, Mr.
Kupfer, Mr. Ostrogradsky and Mr. Brandt for examination of new invention. Secretary
will inform Mr. Korsakov and invite him next Saturday 24th September at 11 o’clock”
[5, p. 579].
5
Machines à Comparer les Idées, or Machines Intellectuelles
What did Korsakov propose? He presented ﬁve “machines for comparison of ideas” or
“intellectual machines”. Each of them according to the inventor’s opinion could compare
various complex scientiﬁc conceptions. Properties of these conceptions should be previ‐
ously organized in a special table. Let’s describe these machines and method of utiliza‐
tion as their inventor had done in the brochure published in French in St. Petersburg [6].
The brochure consists of two parts. The ﬁrst part, from the ﬁrst to eighth pages, was
textually almost identical with the above mentioned “Report about the new investigation
method”. The second one (pages from 9 to 22) contains suﬃciently detailed description
of the device and operation of all ﬁve “intellectual machines”.
First of them is the Rectilinear Homeoscope with immovable parts (Fig. 4).
Korsakov described it so: “among many table presentations it ﬁnds one that contains
another given complex presentation in all details. Machine gives this result by stopping
itself during the operation. <…> Number of details could be hundreds” [5, p. 559].
Korsakov supposed that this device could have found its application in medicine.
Properties of complex presentation in this case are the symptoms of disease. Result of
the work is the appropriate medicament. This process is depicted on Fig. 4.
Every column of table III describes the disease. Its symptoms are presented as the
holes in table cells. Pins (the pin position number coincides with one of the disease
symptom numbers) are stuck in rectilinear bar I (speciﬁcally this bar Korsakov calls
Homeoscope). Height of the bar is equal to the height of the table column. Pins corre‐
sponding to the disease symptoms are stuck in the bar with protruding ends (II). Then
the bar moves along the table from left to right parallel to the columns with down pin
ends (IV). The bar falls down on that column where all pin ends coincide with the holes.
“Device stops itself” (see column E) and we may read the appropriate medicament for
the disease with given symptoms.
76
V.V. Shilov and S.A. Silantiev

Rectilinear Homeoscope with movable parts (Fig. 5). This is designed for the same
purpose as the above described device but in addition “lists and immediately distin‐
guishes all the appropriate and non-appropriate properties from given complex presen‐
tation during their contact with the table properties” [5, p. 559].
Homeoscope in this device is not a bar but a rectilinear wooden frame (V on Fig. 5).
The shaft from thick steel wire penetrates through frame short crossbars. Movable
rotating parts (small levers) made from thinner wire, are fastened on the shaft. Numbered
round paper label is installed on the long arm of every lever. Another end of the lever
arm is a short hook curved on 90°. In initial position all the levers are rightward and rest
on Homeoscope frame crossbar (VII on Fig. 5). The properties of every set of complex
presentations are introduced as the holes in the table column. The table is similar to the
table of Homeoscope with immovable parts. Properties of the presentation comparing
with the table presentation are installed so: if the presentation has some property the
appropriate lever is reclined left, in other case it remains rightward. The special rods vv
(Fig. 5, X) are installed in the holes in Homeoscope side crossbars in order to divide two
kinds of above described presentations. After that Homeoscope is installed on the left
part of the table and moved from left to right. If there is no hole under the lever in the
table cell than the lever would stay in position VIII (Fig. 5) resting on its short hook m.
If the lever is above the table hole hook m would fall in it and recline on Homeoscope
frame crossbar np in position IX. Thus, all the properties of compared presentation
corresponding to the leftward levers are common to the properties of the table presen‐
tation current column (Homeoscope shaft tt is above of this column).
It is worth mentioning the very interesting Korsakov’s idea. He suggested not only
to conﬁrm the coincidence but to determine a degree of its signiﬁcance. He wrote if
“there is a need to express the importance of every property than this action could be
done easily with the help of numbered labels. These labels can move along the pillar x
Fig. 4. Rectilinear Homeoscope with immovable parts
‘Machines à Comparer les Idées’ of Semen Korsakov: First Step Towards AI
77

 of every part. So, by ﬁxing them on the speciﬁc lever at various distances from the axis
(see IX on Fig. 5) one may evaluate their importance” [6, p. 17]. Above this, Korsakov
proposes to paint various table lines in diﬀerent colours for greater visibility.
Plane Homeoscope. Korsakov wrote that this device “… immediately shows the diﬀer‐
ence between two complex compared presentations with more than ten thousand and
even more properties” [5, p. 559]. Construction of the Plane Homeoscope is very simple.
In fact, it consists of two tables A and B (Fig. 6). Every line in table B corresponds to
some complex presentation. The presence or absence of properties in this presentation
is determined by the presence or absence of the holes in the appropriate cell. In table A
which has the same dimensions as table B the presence of some property in complex
presentation is ﬁxed by the pin installed in an appropriate hole. There are holes in all
cells of table A in contrast to table B. Both tables are organized in the form of platens
but “legs” for table A are longer. Table A is put on table B. Pins fall in the holes in case
of property coincidence and doesn’t fall in other case.
Korsakov in his brochure gives the examples of Plane Homeoscope application. Let
the letters a, b, c etc. are related to the human body parts: “head, nose, eyes, belly, etc.”.
Various human “diseases: tumor, pain, redness, weakness, gripes, fever, chills, etc.”
[6, p. 19] are numbered. Thus, the hole in one of the table B cell corresponds to some
disease symptom. “For example, if letter e corresponds to chest and number 4 is gripes
then the cell e4 will designate the chest gripes” [6, p. 19]. Table B contains the full
Fig. 5. Rectilinear Homeoscope with movable parts
78
V.V. Shilov and S.A. Silantiev

collection of disease symptoms and determines the appropriate medicament. Such tables
could be produced for various medicaments. Then the pins must be installed to all the
holes of table A corresponding to the disease symptoms. After that table A must be
consequentially put on table B. Table, which pins fall to all the holes, will determine the
medicament. Korsakov suggested indicating the importance of every symptom by the
pin heads of various dimensions (XIV on Fig. 6). It is a very interesting idea. In this case
the doctor has the opportunity not to consider less signiﬁcant symptoms if some pins
don’t fall in the table B holes. On Fig. 6 the conception e from table A coincide with
conception e from table B according only to the four signs (1, 4, 7, 10). 10,000 properties
mentioned by the inventor mean that the dimension of each table could be 100 × 100.
Ideoscope (Fig. 7). According to Korsakov this device can just in several minutes
compare large a number of complex presentations from a special table. Ideoscope is
a wooden bar working with the table as in both Rectilinear Homeoscopes. But this
bar and the table are constructed in another way. There is a set of quadrangular
through holes in Ideoscope located on equal distances from each other. Tetrahedral
metal pins are installed in the holes. The upper part of the pin is round and contains
the shaft c with the wire lever lh and paper numbered label (XVI B on Fig. 7). Shafts
can move freely up and down without friction in their holes. Holes have buttresses ff
which force the lever to move accordingly to the pin’s movement. The main differ‐
ence of this table from the others is that quadrangular holes have various depths. A
hole’s depth is proportional to the importance of the property depicted in the table.
Above this, the bottom of the hole has the 45 degree angle incline. Thus, if there is
no hole under the pin the lever can stay in one of two positions (XVII A) – on the
left or right bar side. If there is a shallow hole under the pin then the lever would take
horizontal position on the left or right bar side (XVII B) as well. Finally if there is a
deep hole the lever would rise up (XVII C).
Fig. 6. Plane Homeoscope
‘Machines à Comparer les Idées’ of Semen Korsakov: First Step Towards AI
79

Fig. 7. Ideoscope
Work of Ideoscope is similar to the work of Rectilinear Homeoscope with movable
parts. In order to compare some presentation with table presentation it is necessary to
move on left all levers whose label numbers correspond to the numbers of presentation
properties. The special rod is installed in Ideoscope rack. This rod divides leftward and
rightward levers (v on XVII C). Finally, Ideoscope is installed on the left side of the
table and moved from left to right in parallel to the columns. The position of the levers
displays the results of comparison when Ideoscope is above of the next table column.
Ideoscope determines not only coinciding and mismatching properties of two presen‐
tations but also the diﬀerence between them. Korsakov so described the possibilities of
Ideoscope:
Property which is discovered in some conception of the table but not in that table where Ideoscope
is;
Property which is discovered in the given conception but is absent in the compared conception;
Property which is common for the two compared conceptions;
Common property which has high importance for the conception from the table;
Property of high importance which is absent in the given conception but is present in the concep‐
tion from the table;
Property which is absent in the given conception but is present in the conception from the table
[6, p. 23].
Herewith it is very important that there is a possibility to “determine the relative
degree of property signiﬁcance” [5, p. 559]. This could be done by two methods. Firstly,
the signiﬁcance degree of the properties of compared presentations is determined by the
angle of lever inclination. Secondly, this degree could be set by shifting the labels nearer
or father from the lever axis (see XVIII).
80
V.V. Shilov and S.A. Silantiev

Simple Comparator. “…gives the same results as Ideoscope but can work only on two
complex presentations compared with each other. It covers only several dozens of
properties but… it doesn’t need a table” [5, p. 559].
Simple Comparator consists of two wooden frames B and C of equal dimensions.
Both frames have the equal number of rectilinear plates which could be moved to stop
to the right or left. Plates have one hole. Ivory details (a on Fig. 8) are placed in the holes
of frame B plates. Frame C holes has larger diameter and properties a heads can freely
come through.
Fig. 8. Simple Comparator
For example, it is needed to compare two objects B and C (Korsakov compared two
men according their “physical abilities”. The number of these abilities is not more than
a number of plates in the frame). Let take the frame B and if man B has some physical
ability then appropriate plate is moved to the left to stop. The same action should be
done with frame C. After that, frame B is laid on frame C and Korsakov wrote about
four possible results:
– All properties a in left plates under the holes of frame C left plates fall in these holes
pointing which physical abilities are common for the both men.
– Other properties a of left plates not under the holes rise pointing which physical
abilities have man B in contrast to man C.
– Remaining on right properties a rise. They point which physical abilities have man
C in contrast to man B.
– Remaining on right properties a fall in the holes of frame C plates and point which
physical abilities are absent for both men.
6
Academy Commission Decided…
Four academicians were the members of Academy Commission: physicist Egor Parrot
(1767–1852), mineralogist and physicist Adolf Kupfer (1799–1865), mathematician
‘Machines à Comparer les Idées’ of Semen Korsakov: First Step Towards AI
81

Mikhail Ostrogradsky (1799–1865) and zoologist Fiodor Brandt (1802–1879).
Korsakov thought that his devices could be used in medicine for choosing the medica‐
ments and for classiﬁcation of plants and animals. So, the members of Commission were
not the casual people: exactly zoology (or botany) and mineralogy could have been the
main ﬁeld of Korsakov’s devices application. Mathematician and physicist had to eval‐
uate the possibility of devices realization. Commission had no chairman but probably
E. Parrot was responsible for decisions. He later redacted Commission Decision and
presented it at Academy conference.
Commission met with Korsakov on September 24th and asked several questions. It
is interesting that on this same day, September 24th, permission was obtained of censor
Semenov to print the brochure [6]. The text of the ﬁrst part of the brochure is dated 13th
September by the author, i.e. after Korsakov’s appeal to Academy. Naturally, there is a
question: what was the reason for Korsakov to present so quickly his invention to the
Academy without waiting for so soon brochure issuing? It is quite possible that if acad‐
emicians had a detailed description of Korsakov’s machines, there would have been
much less questions… It is also interesting to know if the members of Commission had
seen brochure before the announcement of their decision. Or, maybe Korsakov did not
even submit it to Academy. Most likely this is true, otherwise the brochure would have
to be preserved in the Academy archives.
However, it seems that the introduction with the brochure could hardly force the
members of the Commission to change the prevailing opinion… This is conﬁrmed by
the next episode. To one of the Commission’s questions Korsakov replied in the letter
written next day 25th September. It concerned a possibility to improve the design of
Plane Homeoscope.
“Dear Sir,
You suggested me to ﬁnd a way to determine the various degrees of signiﬁcance of the disease
symptoms, which could be expressed in Plane Homeoscope. I thought about it and consider that
this could be done by using numbered labels, which could be shifted from the axis to a greater
or lesser distance <…>” [5, p. 560].
So fast reply was produced undoubtedly due to the fact that this method had already
been invented and described by Korsakov (although in relation to Ideoscope) in his
brochure: “More or less degree signiﬁcance could be set by shifting the labels nearer or
father from the lever axis” [6, p. 23].
On September 27th, Academician Parrot handed answer of Korsakov to the other
members of Commission asking them to decide if it could “aﬀect the opinion they will
give of this invention”. Because, as we know, it was not aﬀected on the decision of
academicians it is likely they hardly would have changed their opinion even after the
familiarity with the full text of the brochure.
But the decision was being delayed and Parrot on 13th October asked his colleagues
to inform him about their resolution “as soon as possible”. However, academician
Ostrogradsky had already made his opinion: “I believe that the invention of Mr.
Korsakov cannot deserve the approval of the Academy. Though, I wish that a conde‐
scending attitude should be shown to the inventor in our report” [5, p. 580].
Commission Decision dated 24th October was presented at the conference of Imperial
St. Petersburg Academy of Science and it was adopted on the same day. Certiﬁed copy
82
V.V. Shilov and S.A. Silantiev

of the Decision was sent to Korsakov. Unfortunately for him, the Decision was negative.
Academicians positively evaluated the originality of the idea and ingenuity of the
inventor. However, they fully rejected the practicality of the idea and decreased Korsa‐
kov’s optimism about its possibilities.
Indeed, Korsakov wrote: “…it would be naturally to suppose that the invention of
the method which can extend the applications of the higher human organ - mind - must
have… important sequences. It’s only necessary that outstanding scientists should
understand the principle and make tables for its application to the various branches of
human knowledge” [5, p. 560].
Thus, we may see that Korsakov did not think to restrict the application of his
machines only for searching and comparing. He thought he proposed something greater –
method of investigation and instrument for receiving of new knowledge. But Commission
absolutely fairly did not agree with that: “Members of Commission did not adopt appli‐
cation of this method of investigation to the other branches of science. First of all, not
pure, nor applied mathematics cannot get any benefit from these methods because their
essence is impossible to reduce to the tables. The same thing is for physics and chem‐
istry” [5, p. 562].
The reason is simple. It is evident that you can extract from the table only what you
have previously placed in it. The same logical error was made by Raymund Lull ﬁve
hundred years before Korsakov. In fact, the emergence of new conceptions is the result
of scientiﬁc cognition of the world, but not its precondition. Commission pointed out
that Korsakov’s machines couldn’t be used if speaking about some new conception: “If
somebody untutored in science (exactly for these people the author invented his method)
will ﬁnd plant, animal or mineral absolutely unknown or not contained in the table he
cannot give it the name or include in the general system according to the method” [5, p.
562]. Even “…if the table is correct and it is possible to ﬁnd the medicament…” however
“…practical medicine usually see in symptoms the reason for using several medicaments
simultaneously. That is why this method cannot be used. Moreover, this method doesn’t
take into account the force and relative signiﬁcance of symptoms” [5, pp. 562–563].
But, at the same time Commission noted the possible applications of Korsakov’s
ideas: “Zoology, botany and mineralogy if to consider only their system aspect deter‐
mined by nomenclature could use this method”. However, Commission also pointed out
on the problems in practical realization: “… almost countless number of objects doesn’t
give any hope on its (Korsakov’s method – Auth.) full utilization”. For example, it was
mentioned that more than 60,000 plants were known in botany. That is why, “Forming
of the table could be tremendous and time consuming work. It is doubtful that one
botanist or even the whole society of botanists would do this. The same thing is for other
branches of natural science” [5, pp. 562–563]. The dimensions of such a table containing
all known characteristics would be 1666 × 14 ft. according to the very moderate esti‐
mations of academicians! The cost of this table would have been about 5–6 thousand
rubles that was colossal sum for these times.
The Commission summarized: “If these considerations prove to the members of
Commission (and a number of such considerations could be easily increased) that the
method of investigation proposed to their attention in any way cannot be approved, but
it is by no means a reason to disappoint the author. On the contrary, it is desirable to
‘Machines à Comparer les Idées’ of Semen Korsakov: First Step Towards AI
83

oﬀer him to apply his talent and his diligence to the methods more applicable in practice.
Mr. Korsakov has spent too much mind to teach others to live without mind” [5, p. 564].
Semen Korsakov never undertook more attempts to publish his inventions and ideas
after that time.
7
The Fate of Korsakov’s Heritage
The homeopathic community is cherishing the memory of Korsakov, though his
pioneering work on “intellectual machines” has been completely forgotten.
The ﬁrst publication of several important documents about intellectual machines of
Korsakov [5] had been done only in 1961. Then, after 40 years (!) it was issued a small
article of Prof. Gellius Povarov in English [7], in which the author announced the
discovery of Semen Korsakov’s brochure (in French) [6] in the Russian State Library.
This brochure with the description of intellectual machines was issued in 1832 in St.
Petersburg. Then, one more article [8] based on the materials of Povarov has appeared.
Due to this paper the wide public in Russia has learned about Korsakov machines.
Moreover, illustrations of the part of “intellectual machines” and some biographical
information about their inventor were published for the ﬁrst time. These drawings
allowed to give exact description of the functioning of Korsakov’s machines [9, pp. 13–
15]. Finally, two diﬀerent translations in Russian of Korsakov’s works appeared in 2009.
Today the person of Semen Korsakov is attracting an increasing attention of Russian
scientists: historians, specialists in computing and homeopathy. At the beginning of
2012, the ﬁrst book dedicated to Korsakov was published. It was a collection of articles
“The warrior, scientist, and citizen. To the 225th anniversary of the birth of S. N.
Korsakov” [10]. In this book, as well as in number of other publications ([11] and others)
some very interesting articles were issued. For the ﬁrst time the biography of Semen
Nikolaevich Korsakov was reconstructed and various interpretations of his inventions
based on the vast archival materials were presented.
Nevertheless, a detailed study of intellectual machines of Korsakov (unfortunately,
nothing is known about the fate of the models submitted to the Academy, nor about the
fate of drawings and descriptions stored at the estate near Moscow) and real under‐
standing of their place in the history of computer science, artiﬁcial intelligence and
information technologies yet have to be done.
Semen Korsakov was undoubtedly a dilettante in science, but he could be rightly
called as a dilettante brillante. Though brilliant insights of the amateurs are often
combined with surprising and inexplicable professional scientist prejudices and miscon‐
ceptions (it is enough to recall “Dinamoscope”). Korsakov’s passion of homeopathy is
also characteristically in this regard. On one hand, some of his results (in ﬁrst turn the
process for preparing dilutions in the single test-tube) were widely used and are still
being used. However, some of his proposals were naive. For example, outstanding
Russian physician Karl Boyanus (1818–1897) who highly appreciates Korsakov as a
homeopath notes the “immaturity and imperfection of Korsakov’s research and obser‐
vations over the actions of various drugs on diﬀerent parts of the human body”. Some
other proposals of Korsakov are also the subject of criticism from the representatives of
homeopathic medicine.
84
V.V. Shilov and S.A. Silantiev

Of course, the question of establishing the truth between the views of various schools
of homeopathy (as well as the question of the value of homeopathy as itself) is not the
theme of this paper. However, having agreed with the high estimation of Korsakov as
homeopath, we should not uncritically consider any of the thoughts he had expressed…
Similarly, the work of Korsakov in designing of “intellectual machines” is highly orig‐
inal, interesting, and important per se. It is not necessary to attribute to him the design
of the world’s ﬁrst punch-card tabulator and neural computer, nor the invention of the
conception of the algorithm, or successful use of Korsakov’s “intellectual machines” in
modern homeopathy and so on as some enthusiastic authors have done.
It is not necessary also to oppose the works of Korsakov to the works of Charles
Babbage and Herman Hollerith. On the contrary, it would be very interesting to compare
the project of Korsakov with the projects of above mentioned scientists, as well as with
other investigations of his predecessors and contemporaries.
From the viewpoint of philosophy of the XVIII century (Era of Enlightenment) the
use of methods of mechanics for the mechanization of the method was quite logical.
This idea was considered by the outstanding French philosopher and mathematician
Nicolas Condorcet (1743–1794) who suggested that machine could be utilized for
solving the problems of scientiﬁc knowledge classiﬁcation. He wrote in one of the
sketches, which was ﬁrst published only in 1962:
“I will not speak here about mechanical means that could be applied; it is easy to
imagine them, but this mechanism will seem funny until experience shows the usefulness
of these tables for discovering relations and general laws between actual entities,
observed facts and phenomena options…” [12, p. 120].
It is clear that Korsakov could not be acquainted with the manuscript of Condorcet.
But in any case, he obviously tried to make something that Condorcet had not even
begun.
If Condorcet in some aspects anticipated Korsakov, Alfred Smee (1818–1877), on
the other hand, could be considered as his successor. One of the authors of this article
in his works [13, 14] pointed out the obvious similarity of one of the hypothetical logical
machine of Smee and the Simple Comparator of Korsakov.
Unfortunately, intellectual machines of Semen Korsakov are practically unknown
outside Russia. A short publication in English [7] still remains the only one dedicated
to his inventions. We hope that our article will help to draw attention to the pioneering
work of Korsakov, which must occupy its rightful place in the history of computing and
artiﬁcial intelligence.
References
1. Gardner, M.: Logic Machines and Diagrams. McGraw Hill Book Co., New-York (1958)
2. Shilov, V.V.: Istoriya Logicheskikh Mashin (Иcтopия лoгичecкиx мaшин). Librokom,
Moskva (2014)
3. Shilov, V.V., Silantiev, S.A.: Logical machines: predecessors of modern intellectual
technologies. In: Proceedings of the 2015 ICOHTEC/IEEE International History of High-
Technologies and their Socio-Cultural Contexts Conference (HISTELCON): The 4th IEEE
Region 8 Conference on the History of Electrotechnologies, held in 18–19 August 2015, Tel
Aviv, Israel, pp. 73–92. IEEE (2015)
‘Machines à Comparer les Idées’ of Semen Korsakov: First Step Towards AI
85

4. Shilov, V.V., Silantiev, S.A.: Reasoning vs. orthodoxy, or, the lesson from the fate of Russian
“Reasoning Machine”. In: Kimppa, K., Whitehouse, D., Kuusela, T., Phahlamohlaka, J. (eds.)
HCC 2014. IAICT, vol. 431, pp. 191–202. Springer, Heidelberg (2014). doi:
10.1007/978-3-662-44208-1_16
5. Iz istorii vychislitel’nykh ustroistv (po materialam Arkhiva AN SSSR) (Из иcтopии
вычиcлитeльныx ycтpoйcтв, пo мaтepиaлaм Apxивa AH CCCP). In: Istoriko-
matematicheskiye issledovaniya, vyp. XIV, Nauka, Moscow, pp. 557–564, 579–580 (1961)
6. Karsakof, S.: Aperçu d’un procédé nouveau d’investigation, au moyen de machines à
comparer les idées, 22 p., 2 pl. St. Petersbourg (1832)
7. Povarov, G.N.: Semen Nikolayevich Korsakov: machines for the comparison of philosophical
ideas. In: Trogemann, G., Nitussov, A.Y., Ernst, W. (eds.) Computing in Russia. The History
of Computer Devices and Information Technology Revealed, pp. 47–49. Vieweg, Wiesbaden
(2001)
8. Nitussov, A.: Semen Korsakov i “mashina dlya sravneniya idey” (Ceмeн Кopcaкoв и
“мaшинa для cpaвнeния идeй”). PC Week/RE 26, 28 (2005)
9. Shilov, V.V.: Logicheskiye mashiny i ikh sozdateli. Kratkaya, no prakticheski polnaya istoriya
(Лoгичecкиe мaшины и иx coздaтeли. Кpaткaя, нo пpaктичecки пoлнaя иcтopия).
Informatsionnye tekhnologii, 8 (Prilozheniye) (2008)
10. Voin, uchionyi, grazhdanin. K 225-letiyu so dnya rozhdeniya S.N. Korsakova (Boин,
yчeный, гpaждaнин. К 225-лeтию co дня poждeния C.H. Кopcaкoвa). Tekhpoligraphcentr,
Moscow (2012)
11. Mikhailov, A.S.: Teoretiko-mnozhestvennaya interpretaciya raboty intellektual’nykh mashin
S.N. Korsakova (Teopeтикo-мнoжecтвeннaя интepпpeтaция paбoты интeллeктyaльныx
мaшин C. H. Кopcaкoвa). Neirokomp’yutery: razrabotka, primeneniye, 8, 65–73 (2015)
12. Baker, K.M.: An unpublished essay of condorcet on technical methods of classiﬁcation. Ann.
Sci. 18(2), 99–123 (1962)
13. Shilov, V.V., Kitov, V.A.: Mechanical brain in the XIX century: logical machines of Alfred
Smee. In: International Conference “History and Philosophy of Computing”, HAPOC 2011,
pp. 82–86. Book of Abstracts. Ghent, Belgium (2011)
14. Shilov, V.V.: Prostoy komparator Semena Nikolaevicha Korsakova i mashina razlichiy
Al’freda Smi (Пpocтoй кoмпapaтop Ceмeнa Hикoлaeвичa Кopcaкoвa и мaшинa paзличий
Aльфpeдa Cми). In: Voin, uchionyi, grazhdanin…, pp. 15–26 (2012)
86
V.V. Shilov and S.A. Silantiev

Towards Machine Independence:
From Mechanically Programmed Devices
to the Internet of Things
Arthur Tatnall1(&) and Bill Davey2
1 Victoria University, Melbourne, Australia
Arthur.Tatnall@vu.edu.au
2 RMIT University, Melbourne, Australia
Bill.Davey@rmit.edu.au
Abstract. This is an historical account of the development of an aspect of
technology and of machines, leading to information technologies, the Internet
and the Internet of Things. It points to an increasing trend towards these
machines and devices becoming more and more independent of human inter-
vention and control. We have not quite got there yet, but a clear trend can be
observed
nevertheless
from
mechanically
controlled
machines
such
as
Al-Jazari’s Castle Clock which a naïve 13th century observer could have thought
had a life of its own to modern smart kitchen and household appliances (from
the Internet of Things) that really could be said to have a degree of indepen-
dence. The paper makes use of actor-network theory as a lens for understanding
the human and non-human elements of this historical trend.
Keywords: Machines  Technology  Information technology  Independence 
Human operating  Control  Internet  Internet of Things  Artiﬁcial
intelligence  Science ﬁction  Actor-Network Theory
1
Introduction
This paper examines a trend in the history of technology – the gradual path towards
machines that require little or no human intervention to operate, that is, towards
independent machines. In this paper the term ‘independent’ is related to human inde-
pendence. A human grows through stages from a totally dependent baby to a largely
independent adult. We identify four aspects that identify this growth:
• The ability to control one’s life
• The ability to operate without aid in the world
• ‘Self-maintenance’
• The ability to decide the direction that one’s life might take.
Of course independent humans are constrained by their ‘manufacture’ (size,
strength, intelligence, appearance and genetics), by their environment and available
resources and by their ‘programming’ (their culture, value system and education). In
this way we consider the trend of machines towards a type of independence, perhaps
© IFIP International Federation for Information Processing 2016
Published by Springer International Publishing AG 2016. All Rights Reserved
A. Tatnall and C. Leslie (Eds.): HC 2016, IFIP AICT 491, pp. 87–100, 2016.
DOI: 10.1007/978-3-319-49463-0_6

constrained by a set of ‘values’ like Asimov’s three laws of robotics [1] and by their
manufacture. We ask: can a machine capable of fulﬁlling these characteristics be seen
as human independent?
In this somewhat light hearted and rather speculative paper, but one with a serious
point, we will make use of some concepts and scenarios from science ﬁction to look at
interactions between humans and non-human technologies, where humans can be
marginalised, or even replaced by these technologies, as well as the use of regular
research references and factual material. Where will the relationship between these
technologies and humans lead in the future as these machines become more inde-
pendent? In the paper we will use actor-network theory (ANT) to look at how networks
of humans and non-humans (machines) and their interactions may change as the
machines gain a greater degree of autonomy.
The paper uses examples of various technologies1 to show an historical trend
towards machines capable of self-operation, self-control and self-maintenance, and
machines potentially able to set their own agendas and purposes. It begins with
mechanically programmed devices such as court automata from the 10th century
intended to inspire amusement and awe [2], continues through various items of com-
puting technology and the Internet and ends with the Internet of Things.
2
Machine Independence
Before going much further though we should deﬁne the terms: ‘machine’ and ‘inde-
pendence’ and look at how they might be related.
• A machine is: “a tool containing one or more parts that uses energy to perform an
intended action.” [3]
• Independence can be deﬁned as: “Free from outside control; not subject to another’s
authority” and “Capable of acting or thinking for oneself” [4]. Another deﬁnition
adds: “Not subject to control by others” [5].
So we can say then that for it to be independent, a device, be it an item of
technology or a machine, must be free from outside inﬂuence or control and capable of
acting or ‘thinking’ for itself. We will consider the following factors:
• Power source – is this independent of the human user or of the machine itself?
• Operation – how is the machine operated? Does it need a human or can it operate
itself?
• Mechanism for control of underlying behaviour – what determines how the machine
is controlled and what tasks it performs? Does this need direct human intervention?
• Maintenance – to what extent can the machine look after its own needs?
To take an example, windmills and water wheels can operate (though perhaps not
usefully) without direct human intervention once set up, but a steam train requires a
1 Although there are signiﬁcant differences between machines and technologies in some contexts, in
this paper we will often use these two terms to refer mainly to machines.
88
A. Tatnall and B. Davey

human constantly shovelling coal in order to operate. A solar cell, once set up, does
operate independently.
In the case of a very early machine – a horse and cart, the horse was the power
source but the operation and control were human. The horse and cart could perform no
useful task without the human driver. A Model T Ford needed a human operator to
drive and to control its operation having, as distinct from a modern car, few of its own
mechanisms for underlying control. All cars nowadays have anti-lock braking systems
which operate without direct human intervention. Cars that can park themselves are
now common and driverless cars are beginning to appear. Cars are gaining a good
degree of independence from human control. Passenger jet aircraft now all have an
autopilot that, whilst in operation makes the plane completely human independent.
3
Early Automated Devices – Mechanically Programmed
Robots
There are many early examples of automation intended for amusement and awe rather
than with some practical function. An early example was a mechanical theatre that
performed a play, almost ten minutes in
length, invented by Hero of Alexandria
(c10–70 AD) and powered by a complex
system of ropes and machines operated by a
rotating cylindrical cogwheel that produced
the sound of thunder by use of metal
balls dropped onto a hidden drum and
mechanically-timed [2]. It could be argued
that this involved an early example of pro-
gramming [6]. Another early example
appears in the way that a 10th century
Byzantine emperor impressed barbarians at
his court by the presence of automata
including lions roaring on either side of his
throne, and birds resting on the surrounding
trees singing harmoniously [2]. To the naïve
observer in his court these would have been
seen as rather magical and to have been
operating independently (Fig. 1).
In the early 13th century, Muslim engineer Ismail Al-Jazari described various types
of automated devices and robots [7], one of the most famous being the Castle Clock
which was a complex device that had multiple functions besides keeping the time. It
also displayed the zodiac, solar and lunar orbits, and a crescent moon shaped pointer
that was moved by a hidden cart and caused doors to open every hour to show
mechanical musicians playing their instruments [6]. It also allowed for reprogramming
the length of day and night to compensate for changes throughout the year (Fig. 2).
Fig. 1. Castle Clock mechanism
Towards Machine Independence: From Mechanically Programmed Devices to the IoT
89

In Europe from the middle ages there are many more examples of automata,
including a number of mechanical astronomical clocks that began to appear in the 14th
century (Fig. 3).
4
Robots, Artiﬁcial Intelligence and Science Fiction
Karel Čapek [8] is credited with originating use of the word ‘robot’ in his play:
‘Rossum’s Universal Robots’ in which the demise of humans through a robot revo-
lution is unleashed. Another story: ‘Do Androids Dream of Electric Sheep?’ [9] also
introduces a dark side of robotics. To make a machine more independent, some form of
artiﬁcial intelligence needs to come into the picture. John McCarthy et al. [10] ﬁrst
deﬁned Artiﬁcial Intelligence in 1955 as: “the science and engineering of making
Fig. 2. Castle Clock. This is a reconstruction of the clock by Al-Jazari [7] (in Ibn Battuta Mall,
Dubai) from his ‘Book of Knowledge of Ingenious Mechanical Devices’.
Fig. 3. Astronomical Clocks: Prague (Czech Republic) and Poznań Goats (Poland)
90
A. Tatnall and B. Davey

intelligent machines”, but in a recent BBC broadcast [11], Professor Stephen Hawking
suggested that: “The development of full artiﬁcial intelligence could spell the end of the
human race”. Hawking fears the consequences of creating something that is able to
match or surpass humans. “It would take off on its own, and re-design itself at an ever
increasing rate. Humans, who are limited by slow biological evolution, couldn’t
compete, and would be superseded.” [11]. This change could produce a fundamentally
different type of technology that can be seriously compared with the science ﬁction
scenarios involving self-aware artiﬁcial intelligences [12]. In an example from science
ﬁction, the artiﬁcial intelligence possessed by HAL, the ship-board computer in ‘2001:
A Space Odyssey’, meant that it could prevent a crew member from taking an action it
considered to be detrimental: “I’m sorry, Dave. I’m afraid I can’t do that. … This
mission is too important for me to allow you to jeopardize it.”
We now ask the question: can robots be independent? Beginning with more science
ﬁction examples, ‘Star Wars’ robots R2D2 and C3PO were certainly able to act with a
fair degree of independence while the actions of the Terminator were certainly inde-
pendent of human control. Marvin the paranoid android from ‘The Hitch-Hikers Guide
to the Galaxy’ [13] and the inside doors of the ‘Heart of Gold spaceship’ also had
personalities (Fig. 4).
Fig. 4. Nano robot
Towards Machine Independence: From Mechanically Programmed Devices to the IoT
91

Back in the real world, Cognitive Robotics can be deﬁnes as the science: “con-
cerned with endowing a robot with intelligent behaviour by providing it with a pro-
cessing architecture that will allow it to learn and reason about how to behave in
response to complex goals in a complex world” [14]. Some robots can now interact
socially and Kismet, a robot at M.I.T.’s Artiﬁcial Intelligence Lab, can recognise
human body language and voice inﬂection and respond appropriately [15]. Common
industrial robots, however, are generally rigid devices limited to manufacturing and
tasks like welding car components without direct human intervention. They do,
however, need a human technician to monitor their workings. While robots like these
could be said to have a certain level of intelligence, they are certainly not independent
of human control.
On the other hand a home robot vacuum cleaner can currently be said to have a
degree of independence. To ﬁnd their way around, some robots take a linear approach
to ﬁnding the room boundaries and then moving in rows within that space. Others uses
an array of sensors to constantly read their surroundings and send back information
about where to go next, or make use of a camera to create a reference map for the room.
Most can be programmed with a daily schedule function to make the robot do its work
at a speciﬁed time. While these machines certainly could not be said to act completely
independently, we can imaging a time when the robot might be able to sense that a
room needs cleaning and then just go about its work without any direct human com-
mand [16] (Fig. 5).
5
Actor-Network Theory as a Conceptual Lens: People
and Things
Actor-network theory (ANT) has been around since the mid-1980s and was developed
by Bruno Latour, Michel Callon and John Law [17–24] in an attempt to give a sig-
niﬁcant voice to technological artefacts as they considered that both social and tech-
nical determinism were ﬂawed. ANT was
designed instead to give a socio-technical
account in which neither social nor
technical positions are privileged [18, 24]
and nothing is purely social or purely
technical [25]. ANT asserts that the
world is full of hybrid entities [26] con-
taining both human and non-human ele-
ments. One could question, for instance,
which of the contributions to a piece of
software are due to some aspect of the
computer hardware, compiler or pro-
gramming language tools and which are
the result of human interactions and the
particular likes and preferences of the
human programmer [27].
Fig. 5. Robot vacuum cleaner
92
A. Tatnall and B. Davey

In a simple example, Latour [28] speaks of a device that encourages him to buckle
up a car seat belt. First a red light ﬂashes then an alarm sounds at such a high pitch that
it is almost impossible to ignore and so he fastens the seat belt. This means that this
non-human alarm is able to exert a form of authority over the human driver.
Clearly any consideration of the implications of information technology, the
Internet, the Internet of Things and the trend towards machine independence must be a
socio-technical one involving both humans and non-humans (Things).
6
Computers and the Trend Towards Machine Independence
6.1
Babbage’s Difference Engine
In 1812 the English mathematician Charles Babbage conceived the idea of building a
Difference Engine2 capable of computing mathematical tables accurately and quickly
as a series of arithmetic steps. His aim was to achieve automatic computation by
following this set of instructions in which operations were performed in a prescribed
order [6]. The Difference Engine deﬁnitely required human programming and opera-
tion. It did nothing by itself.
6.2
Early Computing Machines and the First Digital Computers
Machines such as Vannevar Bush’s Differential Analyser, the non-programmable
Atanasoff-Berry Computer, Howard Aitken’s Harvard Mk 1, Konrad Zuse’s Z1, Z2
and Z3 and IBM’s Selective Sequence Electronic Calculator also all required consid-
erable human operational control and human maintenance.
The ﬁrst machines that we might now call computers: Colossus and ENIAC were
built essentially as large calculators. Colossus was designed to perform calculations
relating to breaking enemy secret codes and ENIAC to calculating ballistic trajectories
[6]. They were far from independent machines requiring generated electrical power,
constant human operation and control as well as human maintenance in constantly
replacing blown radio values.
6.3
Early Digital Computers
From the Manchester Baby and other early ﬁrst generation digital computers such as
EDSAC, EDVAC, CSIRAC, UNIVAC-1, LEO 1, Ferranti Mk 1 and IBM 701 whose
prime purpose involved calculations, either for scientiﬁc, military or business data
processing purposes, use of radio valves necessitated the constant presence of a
maintenance man.
2 The Difference Engine was never built and was, of course not what we would currently call a
computer. Babbage’s later Analytical Engine, also although never actually built, would have come
closer to this.
Towards Machine Independence: From Mechanically Programmed Devices to the IoT
93

These machines were far from independent in operation and to use one you had to
be (almost) a white-coated scientist capable of understanding abstract mathematical
concepts. They required, in addition to the maintenance people, a programmer to
determine what function the computer was to perform and an operator to enter the
programs and the data in batch mode. The later use of transistors and integrated circuits
reduced the need for maintenance [6]. Although acting in a way to assist human
operations, these machines were also far from independent of human control.
6.4
More Recent Computers
From mainframes and minicomputers to microcomputers the need for expert operation,
control and maintenance has decreased. There is now much less need for an operator
and a maintenance man, and the Microsoft Windows, Apple Macintosh and Android
user interfaces have made operation much easier. Modern PCs also have a degree of
automation with anti-virus programs automatically checking for viruses and uploading
their latest versions without direct human control. Can modern computers act inde-
pendently though? There is quite a way to go before you could answer ‘yes’ to this
question, but these non-human actors are gaining in strength.
7
Independence Trends in the History of the Internet
and the World Wide Web
When considering the development of the Internet, Tchalakov and Rogers [29] point
out the necessity of taking into account the interplay between networks of people and
institutions involved in its development, and how it was about much more than the
installation of hardware and software.
Long before the Internet much work had been done on digital telegraph commu-
nication by people like Harry Nyquist, Alex Reeves and Claude Shannon. The idea of
packet switching, fundamental to the Internet, was ﬁrst devised by Paul Baran in the
early 1960s, and then independently a few years later by Donald Davies [30], clearing
the way for digital computer communication and the beginnings of the packet
switching networks that we now known collectively as the Internet.
Originating with the US Defense Department’s ARPANET, the Internet was
developed as a packet-switched network in the early 1970s and from its beginning had
elements of independence with each message containing the information to take it to its
destination without the need for further human direction. Tchalakov and Rogers [29]
note that a signiﬁcant design issue was “to specify what should be left to each network
and what functions should be required of all hosts connected to it”. There is no central
computer running the Internet and its resources are to be found among thousands of
individual computers around the world making it almost beyond possible human
control, but this alone does not make it independent.
Human use of the early Internet was far from straightforward for non-technical
people as it was entirely text-based and there was a need to have some knowledge of
systems and access languages such as UNIX. The Web changed all that by offering a
94
A. Tatnall and B. Davey

graphical interface using hypertext to access Internet ﬁles. The original idea for
hypertext goes back to 1945 when it was outlined by Vannevar Bush and in the 1960s
by Ted Nelson and Douglas Engelbart but became particularly signiﬁcant in 1989 when
Tim Berners-Lee at CERN (European Particle Physics Laboratory) proposed a
hypertext development project that eventually became the World Wide Web [31]. The
Web was designed to allow pages containing hypertext to be stored in a way that
allowed other computers access to these pages, again without direct human
involvement.
8
The Internet of Things
Radio Frequency Identiﬁcation (RFID) and Wireless Sensor Networks have been in
existence for over two decades, but advances towards full use of the Internet of Things
(IoT) offer much more and also pose more social challenges [12]. Put very simply, the
Internet of Things (IoT) could be described as technology which connects any physical
thing to the Internet in order to exchange information with some other thing [32], and
could be seen as “… all about physical items talking to each other” [33:2].
ANT considers all the various interactions between human and non-human actors:
between people and people, people and things, and things and things. In the devel-
opment of computers and the Internet we have seen a trend from machines that initially
required a good deal of interaction with humans, to machines that require less such
interaction: a move towards machine independence.
The term Internet of Things was ﬁrst coined some time ago by Kevin Ashton [34] in
the context of supply chain management, but has advanced to cover a wide range of other
applications. There are many deﬁnitions of the Internet of Things and the CASAGRAS
project sees it like this: “A global network infrastructure, linking physical and virtual
objects through the exploitation of data capture and communication capabilities. It will
offer speciﬁc object-identiﬁcation, sensor and connection capability as the basis for the
development of independent cooperative services and applications. These will be
characterised by a high degree of autonomous data capture, event transfer, network
connectivity and interoperability.” [35:10]. SAP Research adds: “A world where
physical objects are seamlessly integrated into the information network, and where the
physical objects can become active participants in business processes” [36:12].
One important use of the IoT is in healthcare with the use of RFID in hospitals to
track equipment like trolleys, surgical equipment, wheelchairs, infusion pumps and
deﬁbrillators [37]. RFID and IoT systems could potentially be integrated into other
areas such as bedside applications and monitoring, and then extended into remote
monitoring of multi-hospital environments. An article in The Australian newspaper
[38] describes a new smart watch, equipped with GPS and activity sensors, designed
for monitoring the elderly at home or in aged-care facilities which could also be used in
cases where patients tend to ‘wander away’ [39, 40].
Driverless smart trains have been in operation between terminals at Singapore
airport now for many years, and an Australian mining company is using smart
driverless trucks to transport ore from a remote Western Australian mine to a railway
line for shipment to Perth. A modern car has a multitude of computer controlled
Towards Machine Independence: From Mechanically Programmed Devices to the IoT
95

functions that involve a variety of sensors ‘talking’ to each other and the controlling
computer. These include roll sensors, stability control, brake assist and pre-collision
systems, all designed to make the car safer. In many cases these can react autono-
mously in response to perceived threats [12].
Another example of the IoT aimed at improving home or workplace management is
the control of appliances such as heaters, air conditioners, washing machines, dish-
washers, refrigerators, ovens and home alarms where this can be done from a distance
by the human owner with the aid of a smart phone. Initially these individual ‘things’ in
the house would transmit regular status reports so that relevant humans could take
appropriate action such as raising temperatures or activating home alarms. A security
system might then determine that no humans are present and turn off lights and air
conditioning systems to save money. In a later development, if the home security
network with Wi-Fi sensors and cameras connected to the Internet detects movement
when no one is supposed to be home, the police could be notiﬁed that the home had
been invaded and the security network could proceed to lock all the doors [12].
Not all IoT devices are really serious in their application though. In an IEEE article,
Amanda Davis [41] describes a video camera system that streams to your mobile phone
to let you keep an eye on your cat while you are away, and to talk to it through a
two-way audio system. You can also activate a moving laser pointer for your cat to
chase to keep it amused until you come home.
9
The Internet of Things, Machine Independence and Human
Issues
In a fascinating article called: ‘Do objects dream of an Internet of things’, Teodor
Mitew [42] describes Brad the Toaster which is part of the ‘Addicted Products’ project
by Simone Rebaudengo and Haque Design Research [43]. Addicted Toasters love to be
used to make toast and have agency and desires. They get jealous of other toasters that
are appreciated more than they. They are connected to each other via the Internet and
don’t recognise owners as such, but know how their fellow Toasters are getting on. If
an Addicted Toaster is not used enough it will try to get itself transported to someone
else that makes more toast.
“Brad is a toaster connected to the Internet, and to other toasters like him. He often
exchanges information with his fellow toasters, with whom he tweets about the usage
habits of their human hosts. He and his fellow toasters are not owned as other, simpler,
toasters before them used to be. They are hosted by humans who have promised to use
them. He loves being used, and is sensitive to learning that other toasters are used more
often than him. When feeling under-appreciated, Brad will draw attention to himself by
playing pranks, throwing tantrums, and expressing his sadness loudly on Twitter.
Eventually, Brad will become disillusioned and demand a move to another, more
caring host. He will depart, leaving the smell of burned toast behind him.” [42:4].
Independence from direct human control is more apparent in Web applications such
as the Amazon website which knows, from your previous purchases, what book you
are likely to buy next. Many other applications also remember what you did or what
you asked last time. When you logon to you supermarket website to place your grocery
96
A. Tatnall and B. Davey

order the software knows who you are, what you have purchased in the past, and what
you are likely to order this time.
Is it possible to be anonymous anymore? The smart phone that most of us carry is
actually also a tracking device. Apart from being able to tell you the location of your
nearest restaurant, it can also tell anyone with access to the data it collects where you
are at any given moment and where you have been. This has echoes of Big Brother in
George Orwell’s [44] book ‘Nineteen Eighty-Four’. In other cases your email provider
will also send you personal advertisements based on the content of your supposedly
private messages [12]. In a recent newspaper article in the Sunday Age [45], in relation
to Orwell’s Big Brother technology, Australian Federal MP Adam Brandt says he
doesn’t equate surveillance just with use of technology, but sees a difference between
sharing and being spied on.
Mitew [42] further describes the potential of IoT embedded objects to completely
dispense with humans as intermediaries when they are in each other’s interaction range.
The expectation is that fridges, cars, coffee cups and, of course toasters “form a
contextually rich conversation with no human interference or presence” [42:10] where
they are expected to ‘socialise’ with one another, exchanging data [46]. The IoT,
involving sensor input and remote activation of devices, sometimes by other devices,
comprises a non-human network where the level of autonomy and decision making by
the devices is fundamentally different from anything we have seen before.
Given how many predictions of the future of computing have turned out to be so
wrong, it is a brave person who now makes one. Although they may be apocryphal,
predictions such as that attributed to Thomas Watson (IBM): “I think there is a world
market for maybe ﬁve computers”, and Bill Gates: “640K ought to be enough for
anybody” would discourage any sensible person from such predictions, but we will
indulge in some speculations nevertheless. For example, perhaps in the future some of
the Things might act autonomously on the basis of their sensors, so doing away with
the need for human intervention. It is then perhaps a small step to: “I’m sorry, Dave.
I’m afraid I can’t do that. …”.
Regarding the medical technology that can be used to measure and keep track of
physiological parameters and provide information back to you, it is one thing to do this
but quite another to then automatically send this data to your hospital or your local
medical General Practitioner. Also the Internet is not without critics as a 2010 article in
the Telegraph newspaper by Nicholas Carr [47] notes: “A growing body of scientiﬁc
evidence suggests that the net, with its constant distractions and interruptions, is
turning us into scattered and superﬁcial thinkers”.
In relation to motor vehicle technology, roll sensors, stability control, brake assist
and pre-collision systems for a car can in some cases react autonomously to perceived
threats. Also, should driverless cars be able to exceed legal speed limits to keep up with
other cars [48]? How would a driverless car make a decision on the choice of whether
to risk killing a pedestrian or of colliding headlong into another car? A recent article in
the Guardian Weekly [49] points out another issue: that the exponential growth in
connected devices “is outstripping our ability to reframe our ethical and legal
approaches to computer decisions”.
Towards Machine Independence: From Mechanically Programmed Devices to the IoT
97

10
Conclusion
In this article we have traced how the development of technologies leading to modern
computing, the Internet and the Internet of Things points to an increasing trend: a trend
towards these machines and devices becoming more independent of human interven-
tion. The Internet of Things could be described as an enabling technology that is unlike
any other. As it involves using sensor input for remote activation of devices, sometimes
by other devices, the level of autonomy and decision making by these devices is quite
different from anything we have seen before. We argue that it is a fundamentally
different type of technology that can be seriously compared with the science ﬁction
scenarios involving self-aware artiﬁcial intelligences.
Any study of these matters needs to be a socio-technical one, and actor-network
theory is thus a useful way to frame it as a socio-technical approach has been found to
allow the understanding of technology not possible by failing to ‘listen’ to the things
and to a range of socio-technical actors. The basis of ANT is to consider interactions,
relationships and associations between human and non-human actors and to do so in
such a way that consideration of one is not privileged over the other. The term
non-human actor can be applied to many ‘things’ ranging from organisations (- these
are considered as a black box containing human actors) to machines and other items of
technology. These interactions can be between people and people, people and things,
and things and things. In this paper we have seen a trend from machines that initially
required total control by humans to those needing much less such interaction and
ﬁnally to machines that can operate without direct human control: a move towards
machine independence.
We could now question whether people like Stephen Hawking are right to be
worried about this trend and whether some of the science ﬁction scenarios might come
true. What will be the future of relationships between humans and machines? It will be
interesting to see what the next few years bring!
References
1. Asimov, I.: I, Robot. Gnome Press, New York (1950)
2. Ambrosetti, N.: Wonder, sorcery, and technology: contribute to the history of medieval
robotics. In: Tatnall, A. (ed.) HC 2010. IAICT, vol. 325, pp. 16–25. Springer, Heidelberg
(2010). doi:10.1007/978-3-642-15199-6_3
3. Wikipedia, Machine, July 2016. https://en.wikipedia.org/wiki/Machine
4. Soanes, C., Stevenson, A. (eds.): The Concise Oxford English Dictionary, 11th edn. Oxford
University Press, Oxford (2004)
5. Merriam-Webster: Merriam-Webster Dictionary, 2015, January 2016. http://www.merriam-
webster.com/dictionary/independent
6. Tatnall, A.: History of Computers: Hardware and Software Development, in Encyclopedia of
Life Support Systems. UNESCO - Eolss Publishers Co., Ltd., Ramsey (2012)
7. Al-Jazari, I.: The Book of Knowledge of Ingenious Mechanical Devices (Kitab ﬁma ‘rifat
al-hiyal al-handasiyya) (1206). Reidel, Dordrecht (1974)
8. Čapek, K.: R.U.R. (Rossum´s Universal Robots). Aventinum, Prague (1920)
98
A. Tatnall and B. Davey

9. Dick, P.K.: Do Androids Dream of Electric Sheep?. Ballantine Books, New York (1968)
10. McCarthy, J., et al.: A proposal for the dartmouth summer research project on artiﬁcial
intelligence (1955). AI Mag. 27(4), 12–14 (2006)
11. Cellan-Jones, R.: Stephen Hawking warns artiﬁcial intelligence could end mankind, October
2015, 2 December 2014. http://www.bbc.com/news/technology-30290540
12. Tatnall, A., Davey, B.: The internet of things and beyond: rise of the non-human actors. Int.
J. Actor Netw. Theor. Technol. Innov. 7(4), 58–69 (2015)
13. Adams, D.: The Hitch-Hikers Guide to the Galaxy. Pan Books, London (1979)
14. Wikipedia: Cognitive Robotics. Web, 2015 January 2016. https://en.wikipedia.org/wiki/
Cognitive_robotics
15. Harris, T.: How robots work, 2015 January 2016. http://science.howstuffworks.com/robot6.
htm
16. Miele: Robot vacuum cleaner, January 2016. http://www.miele.com.au/domestic/robot-
vacuum-cleaner-2750.htm
17. Callon, M.: Some elements of a sociology of translation: domestication of the scallops and
the ﬁshermen of St Brieuc Bay. In: Law, J. (ed.) power, action & belief. a new sociology of
knowledge?, pp. 196–229. Routledge and Kegan Paul, London (1986)
18. Callon, M., Latour, B.: Unscrewing the Big Leviathan: how actors macro-structure reality
and how sociologists help them to do so. In: Knorr-Cetina, K., Cicourel, A.V. (eds.)
Advances in Social Theory and Methodology. Toward an Integration of Micro and
Macro-sociologies, pp. 277–303. Routledge and Kegan Paul, London (1981)
19. Latour, B.: The Prince for Machines as well as for Machinations. In: Elliott, B. (ed.)
Technology and Social Process, pp. 20–43. Edinburgh University Press, Edinburgh (1988)
20. Latour, B.: The powers of association, in power, action and belief. In: Law, J. (ed.) A New
Sociology of Knowledge? Sociological Review Monograph 32, pp. 264–280. Routledge and
Kegan Paul, London (1986)
21. Latour, B.: Aramis or the Love of Technology. Harvard University Press, Cambridge (1996)
22. Law, J.: On the social explanation of technical change: the case of the Portuguese maritime
expansion. Technol. Cult. 28(2), 227–252 (1987)
23. Law, J. (ed.): A Sociology of Monsters. Essays on Power, Technology and Domination.
Routledge, London (1991)
24. Law, J., Callon, M.: The life and death of an aircraft: a network analysis of technical change.
In: Bijker, W., Law, J. (eds.) Shaping Technology/Building Society: Studies in Sociological
Change, pp. 21–52. MIT Press, Cambridge (1992)
25. Law, J.: Introduction: monsters, machines and sociotechnical relations. In: Law, J. (ed.) A
Sociology of Monsters: Essays on Power, Technology and Domination. Routledge, London
(1991)
26. Latour, B.: We Have Never Been Modern. Harvester University Press, Cambridge (1993)
27. Tatnall, A.: Actor-network theory as a socio-technical approach to information systems
research. In: Clarke, S., et al. (eds.) Socio-Technical and Human Cognition Elements of
Information Systems, pp. 266–283. Information Science Publishing, Hershey (2003)
28. Latour, B.: Where are the missing masses? The Sociology of a Few Mundane Artifacts. In:
Bijker, W., Law, J. (eds.) Shaping Technology/Building Society: Studies in Sociological
Change. MIT Press, Cambridge (1992)
29. Tchalakov, I., Rogers, J.D.: Computer networks as the embodiment of social networks: the
role of national scientiﬁc communities in the development of internet in the U.S. and
Bulgaria. Int. J. Actor Netw. Theor. Technol. Innov. 6(3), 1–25 (2014)
30. Wheen, A.: Dot-Dash to Dot. Com - How Modern Telecommunications Evolved from the
Telegraph to the Internet. Springer, Chichester (2011)
Towards Machine Independence: From Mechanically Programmed Devices to the IoT
99

31. Davison, A., Burgess, S., Tatnall, A.: Internet Technologies and Business, 3rd edn. Data
Publishing, Melbourne (2008)
32. Colitti, W., Long, N.T., De Caro, N., Steenhaut, K.: Embedded web technologies for the
internet of things. In: Mukhopadhyay, S.C. (ed.) Internet of Things. SSMI, vol. 9, pp. 55–73.
Springer, Heidelberg (2014). doi:10.1007/978-3-319-04223-7_3
33. Mukhopadhyay, S.C., Suryadevara, N.K.: Internet of Things: Challenges and Opportunities.
In: Mukhopadhyay, S.C. (ed.) Internet of Things. SSMI, vol. 9, pp. 1–17. Springer,
Heidelberg (2014). doi:10.1007/978-3-319-04223-7_1
34. Ashton, K.: That ‘‘Internet of Things’’ thing. RFID J. (1999)
35. CASAGRAS: CASAGRAS Final report: RFID and the inclusive model for the internet of
things,
2014,
October
2015.
https://docbox.etsi.org/zArchive/TISPAN/Open/IoT/low%
20resolution/www.rﬁdglobal.eu%20CASAGRAS%20IoT%20Final%20Report%20low%
20resolution.pdf
36. Haller, S.: Internet of Things: An Integral Part of the Future Internet. SAP Research, Prague
(2009)
37. Unnithan, C.: Examining innovation translation of rﬁd technology in Australian hospitals
through a lens informed by actor-network theory. In: Proceedings of Information Systems.
Victoria University, Melbourne (2014)
38. Foreshew, J.: Watch watches out for the aged, in the Australian. News Media, Melbourne
(2015
39. Unnithan, C., et al.: RFID translation into Australian hospitals: an exploration through actor-
network theoretical lens. In: Proceedings of the International Conference on Information
Society (i-society 2013). University of Toronto, Toronto (2013)
40. Unnithan, C., Tatnall, A.: Actor-network theory (ANT) based visualisation of socio-technical
facets of RFID technology translation: an Australian hospital scenario. Int. J. Actor Netw.
Theor. Technol. Innov. 6(1), 31–53 (2014)
41. Davis, A.: Boost your home’s IQ with these seven gadgets, 2015, January 2016. http://
theinstitute.ieee.org/technology-focus/technology-topic/boosting-your-homes-iq-with-these-
seven-gadgets
42. Mitew, T.: Do objects dream of an Internet of things? Fibrecult. J. 2014(23), 1–25 (2014)
43. Rebaudengo, S.: Addicted toasters 2012, January 2016. http://www.haque.co.uk/addicted
toasters.php
44. Orwell, G.: Nineteeen Eighty-Four. Martin Secker and Warburg, Great Britain (1949)
45. Elder, J.: Everyone is Watching, in Sunday Age. Fairfax, Melbourne (2015). p. 16
46. Yan, L., et al.: The Internet of Things: From RFID to the Next Generation Pervasive
Networked Systems. Auerbach Publications, New York (2008)
47. Carr, N.: How the Internet is making us stupid. In: The Telegraph (2010)
48. O’Callaghan, J.: Google’s driverless cars will EXCEED legal speed limits so they can keep
up with other drivers. In: DailyMail (2014)
49. Arthur, C.: What next in robot revolution? In: The Guardian Weekly, London (2016)
100
A. Tatnall and B. Davey

The Global Virtual Museum of Information
Science & Technology, a Project Idea
Giovanni A. Cignoni
(✉) and Giovanni A. Cossu
Hyperborea srl, Via Giuntini, 25, 56023 Navacchio, Pisa, Italy
{g.cignoni,g.cossu}@eidolon.it
Abstract. Information Science & Technology (IST) has pervasively aﬀected our
everyday life, thus becoming a proper cultural heritage of humanity. The growing
curiosity about IST history has determined the creation of important collections
devoted to the conservation of IST relics. Physical relics are naturally located
close to their origins, but they are only one aspect of preservation and dissemi‐
nation of IST history. The whole knowledge about IST history has to go beyond
the local boundaries and become a globally shared and worldwide accessible
heritage. Our proposal is to establish a Global Virtual Museum of IST based on a
knowledge base able to manage all the information of the domain, created and
updated by museum keepers and other experts, and capable of oﬀering new
enjoyment opportunities to a wider public audience. It is a radical change in the
idea itself of cultural heritage information management, up to now bound to the
traditional cataloguing approaches.
Keywords: Virtual museums · Cultural heritage management · Knowledge base
1
Introduction
IST (for Information Science & Technology, we like to add science in the more usual IT
acronym), intended as a scientific discipline with a wide neighbourhood of technologies
and applications, can be rightfully considered as an international cultural heritage. The
development of IST has always been capable of overcoming geographical boundaries, both
as scientific discussion and dissemination of technologies.
Nevertheless, if we look at how the IST history is maintained and presented to the
audience, a strong localization can be observed. The museums are the places where the
relics are preserved: many of early days IST relics are preserved where they were created.
It is in the order of things and it is even correct to give the right importance to the local
relics in their presentation to the public: it is natural that the Pascaline are in France,
that the Zuse Machines are in Germany and that the Manchester Baby replica is in
Manchester, etc.
Getting excited in front of the pieces of history is the reason that drives us to visit
museums appreciating the uniqueness and the diﬀerences among various collections.
Nevertheless, there is a dissonance between the international nature of the discipline and
the localization of its preservation.
© IFIP International Federation for Information Processing 2016
Published by Springer International Publishing AG 2016. All Rights Reserved
A. Tatnall and C. Leslie (Eds.): HC 2016, IFIP AICT 491, pp. 101–114, 2016.
DOI: 10.1007/978-3-319-49463-0_7

There is a further aspect. IST underlies many radical changes in the world and in the
current way of life of each of us. In addition, almost always the IST revolution goes in
the direction of the so-called “globalization”: cancelled distances, availability of infor‐
mation for everyone, everywhere.
However, there are still cases where the application of IST has produced only an
evolutionary innovation. One of these is the management and enjoyment of cultural
heritage. IST are used and applied, but the catalogues of the collections are still built up
with criteria inspired by inventories and not by knowledge management. The idea of
“virtual museum”, in most cases, is intended as the virtualization of a real museum. The
technologies reproduce a surrogate of a physical visit deprived of the emotion of being
in front of the original pieces. Of course, it has the great advantage of eliminating
distances and enhancing accessibility, but at the state of the art not much more.
The article proposes a radical change in the way we treat the cultural heritage of
humanity through the comprehensive and mature use of IST. The history of IST is also
the proposed ﬁrst application domain: it is an acknowledgement of the revolutionary
nature of IST and its contribution to the management of knowledge.
The historical information preserved, mainly but not exclusively, by museums are
collected in a single Knowledge Base (KB). In addition to the data of the pieces in the
collections, the KB maintains and makes explicit the relations between pieces, hardware
and software products, companies, scientists and inventors, documentation, events, and
more. The KB is shared: it facilitates the work of museum keepers and provides the
historians with a powerful research tool. The KB is public and, for researchers and
enthusiasts, is an authoritative source of references.
The KB is the “virtual warehouse” of the Global Virtual Museum of Information
Science & Technology. Using the KB as a source, new virtual exhibitions can be built:
they do not surrogate the visits to museums, instead they oﬀer new discovery pathways
across the whole heritage of IST history. The intrigued visitor can navigate the KB,
exploring all details and going beyond the physical location of collections. In traditional
exhibitions the KB enhances the visitor experience by making accessible all the infor‐
mation that surrounds the physical relics.
This paper is organized as follows. In Sect. 2 we discuss how the physical approach
to conservation of relics naturally leads to localized collections. In Sect. 3 we introduce
the new proposed approach, which fully exploits IST to radically change the way the
knowledge of cultural heritage should be globally managed. Section 4, using IST history
as a case study, outlines how our proposal relies on a KB. In Sect. 5 a practical example
is given to show how navigating a KB can lead to discover all the complexity of IST
history. Section 6 shows how basic technologies are already in place, thus giving
evidence of the technical feasibility of the project. Conclusions also address the viability
of the idea as a sustainable project.
2
Localization vs Internationalization: IST as a Case Study
The preservation of cultural heritage is traditionally local: the monuments rarely move
and museums tend to keep relics related to their territory. Consequently, the way we
102
G.A. Cignoni and G.A. Cossu

enjoy cultural heritage is likewise localized: we must go to Siena to see, let’s say, the
“Majesty” by Simone Martini. Yet, even in the Middle Ages, Simone Martini’s work
was part of an international movement – the Gothic painting – originated in England
and France, passed through Germany and ﬁnally arrived in Italy in the time frame of
less than a century.
Despite culture bring international, the traces it leaves, being physical, are subject
to the law of Newton and tend to stay where they are; indeed, we perceive as unnatural
their displacement – with subsequent instances for restitution.
The same applies to the IST history. The places where the relics are preserved often
coincide with the places of origin, but their stories have a broader extent.
Consider for example the very ﬁrst mechanical calculators. Blaise Pascal (French)
left us several Pascalines, most of which are now in Paris [1]. Gottfried Leibniz
(German) was well acquainted of Pascal’s work when he built his Rechenmaschine. The
very ﬁrst prototype was presented in Great Britain at the Royal Society of London; two
others machines were made, but only the second has survived and it is preserved in
Hannover [2]. From Pascal’s Pascaline also derive the machines made by Tito Burattini
(Italian) and those by Samuel Morland (English). The ﬁrst had probably studied Pascal’s
calculator while attending the court of King Władysław IV of Poland, the latter when
he was on a diplomatic mission to the court of Queen Christina of Sweden – Pascal
promoted his work by giving samples of his machine to royal families interested in things
of science. Wanting to see a specimen of Burattini’s or a Morland’s, suggested sites are
Florence [3] and London [4].
Coming to more recent times, the ﬁrst electronic computers were bulky and unique
or produced in small numbers. When the original machines still survive, they remain in
their home areas. Reconstructions are also made locally: it is natural and correct that
replicas like the Manchester Baby [5] or the Colossus [6] are in their respective places
of origin.
With personal computers IST has deﬁnitely become an international consumer
market. Collections of PCs show usually a high grade of internationalization. Yet, when
focusing on less known machines, it is still much likely meeting strong localization:
there is a wealth of Acorn PCs at Bletchley Park [6] and at Cambridge [7] as well as
there are many Olivetti PCs in Ivrea [8]. There is even an innate interest in preserving
the relics where the producing ﬁrms are located.
There are, of course, exceptions where the geographical position of the collection
does not coincide with the origin of the pieces: an example is the relevant collection of
Apple machines and memorabilia in Savona [9].
However, the collection is still conﬁned: it is in a given place. A virtual version of
the museum can cancel distances, like in the case of the Savona collection where a nice
app allows visiting the exhibition from everywhere. However, it is still a conﬁned expe‐
rience: it tells the story of a great set of Apple’s pieces, but with few links to the rest of
the IST history.
Such localization is not representative of the universal nature of IST. Collecting the
whole IST history in a KB which makes explicit all the relations among facts will help
in highlighting the circulation of ideas, the spreading of technologies, the evolution of
industrial partnerships and changes in the global market.
The Global Virtual Museum of Information Science & Technology
103

3
IST, Revolutions and Evolutions
Today it is quite diﬃcult to think of a world without IST. Within a few years our habits
have dramatically changed. Could you imagine how you could deal without a search
engine? Would you like to renounce to mobile devices through which we are always
reachable, which provide apps that help us in a thousand of things, from ﬁnding direc‐
tions to keeping in touch with friends? And, besides personal aspects, how could we
manage ﬁnancial data, control industrial processes, ensure safety of ﬂights and so on
without IST? Things have changed so much and are so hard to reverse that talking of
“revolution” makes sense – IST is identiﬁed as one of the factors of the third industrial
revolution next to come [10].
Tracing the whole history of IST, we can ﬁnd many revolutionary changes, and not
only in recent times. The Victorian Internet, i.e. the telegraph network, has brought
distances to nought: knowing what was going on the other side of the Earth changed
from a matter of months to a matter of seconds [11]. Or the aforementioned mechanical
calculators, operated by skilled human computers, have revolutionized the accounting,
making possible to raise the size of companies by a couple of orders of magnitude. Again,
the tabulating machines made it possible to overcome the impasse of too much infor‐
mation [12] and gave oﬀ a completely diﬀerent way of understanding the control and
management of information – not without introducing also disquieting conse‐
quences [13].
However, there are cases in which IST are applied but, while constituting a useful
development, the extent of their contribution is not revolutionary. An example that
closely concerns us is the management of cultural heritage. In this context, we only
partially take advantage of the wealth of tools provided by IST: they are mainly used
for managing the assets and for making them accessible on diﬀerent channels [14]. The
mighty power of IST is not yet exploited to manage, make explicit and convey the
complex web of knowledge that is the essence of cultural heritage.
Certainly, the computerization of museum catalogues is a major achievement, but
in almost all cases it is simple data management. For example, in Italy, there is a long
tradition of cultural heritage management which, on many occasions, has been the fore‐
runner of innovative concepts like protection [15] as well as inclusion of all that is of
cultural interest [16]. Cataloguing, as the way of knowing and managing cultural
heritage, has always been a primary goal. The Istituto Centrale per il Catalogo e la
Documentazione (Central Institute of Cataloguing and Documentation) continuously
studies and promotes standards for cataloguing the whole Italian cultural heritage [17].
Such standards have, however, a ﬂat structure “one piece - one cataloguing record”:
while implemented using IST they just replicate the structure of the old archives based
on paper ﬁles. It is an evolution, but it has nothing revolutionary. And, as an information
system, it is insuﬃcient for a context – cultural heritage – which is not a simple list of
things, but a complex and extensive web of knowledge.
In addition, each museum is responsible for his own collection, and although compu‐
terized, the collection tends to remain localized without allowing the sharing of the
knowledge among all the stakeholders – museums, research institutions, interested
people, everyone.
104
G.A. Cignoni and G.A. Cossu

The absence of a radical change is also noticeable in the presentation of the collec‐
tions. Improvements have certainly been introduced by “virtual museums”, e.g. the
chance of visiting (or revisiting) the museum from home, the access to more documen‐
tation than what can be presented in the spaces of traditional panels and labels, the
amazing ability to get very close to the works, virtually freed from safety distances
thanks to high-deﬁnition digital reproductions or stunning 3D models.
However, the possibility of accessing all the knowledge that surrounds a piece still
does not exist. The curator of an exhibition decides the information which are presented;
as original, correct and useful they can be, such information remain bounded to that
particular exhibition. There is no way to access the similar pieces, the pieces of the same
author or the pieces inspired by or otherwise related to the piece on display but belonging
to other collections. Neither is allowed accessing any other relevant information. Even
if IST is used, cultural heritage is managed in a traditional and in a strongly localized
way. No revolution happened in this ﬁeld.
The question concerns all kinds of cultural heritage. In some cases relations about
pieces and other linked information are crucial to allow visitors to fully enjoy a piece
of culture. For instance, in modern art, the movements, the membership of an artist to a
circle, the mutual inﬂuences given by aﬃnity or contrast, are all keys to understand the
work of art.
The importance of the surrounding knowledge is even more important when we focus
on technical and scientiﬁc collections, such as those related to the history of IST. First
of all, the cultural purpose of an exhibition in this ﬁeld concerns also the public under‐
standing of science and technology [18], so there is another level of information to be
conveyed to the visitor. Moreover, the collections are about serially manufactured
pieces, designed by project teams, produced by companies, often sharing technologies
and components. The knowledge to be managed and made accessible is even more
extensive, complex and crucial for understanding the context.
The IST revolution for cultural heritage is making available all this knowledge, going
beyond the local management currently adopted by museums. Noteworthy, it is a
possible revolution, as it mostly relies on IST technical solutions already available.
4
A Knowledge Base for the History of IST
A radical change in the way of managing the cultural heritage is to abandon the current
local data management and focus instead on global knowledge management. It is certainly
an ambitious goal, but there are no true technological limits, and there are many prece‐
dents in which the application of IST was, since the beginning, thought in the large –
search engines, for example, have always dealt with the indexing of the whole Internet.
To be prudent, however, we could start from a specific cultural domain. Our case study
is the IST history, now preserved in collections dedicated to science and technology (such
as, among the best known, the Science Museum of London [4], the Deutsches Museum in
Munich [19], the Conservatoire National des Arts et Métiers in Paris [1], the Museum of
Science and Industry in Manchester [5]) or in museums specifically dedicated to IST (like
the Computer History Museum in Mountain View [20], the Heinz Nixdorf Museumforum
The Global Virtual Museum of Information Science & Technology
105

in Paderborn [21], or the already cited National Museum of Computing at Bletchley Park
[6] and the Centre for Computing History in Cambridge [7]).
Our proposal does not simply concern the union of the catalogues, but their reorgani‐
zation into a single KB able to preserve and explain all the complexity of the domain. The
typical museum catalogues are focused on the pieces in the collection, like the exemplars
of the Apple ][. In the KB there are, of course, objects that hold information about pieces,
but they are different from the KB objects which hold information about the products. The
KB object of type piece maintains information on a particular Apple ][ exemplar, such
as the serial number, the state of preservation and the provenance. The KB object of type
product maintains information common to all Apple ][: technical specifications, dimen‐
sions, historical notes. There will be as many Apple ][ in the KB as there are in the museum
collections, each one with its piece object, but all of them will refer to the unique
Apple ][ product object. Objects in the KB are typified so it is possible to grasp the
distinctions among versions, variants, special series, production batches, as well as among
different categories of products like workstations, personal computers, home computers,
and so on. IST provides many theoretical and practical solutions to deal with type hierar‐
chies [22].
Besides pieces and products, other kinds of objects managed by the KB are the used
hardware components (like processors), the technological solutions adopted (from
architectures to standard communication protocols and interfaces). Software (from
operating systems to applications) is another huge branch in the type hierarchy of the
KB as well as documentation (blueprints, manuals, but also brochures and promotional
videos), people (researchers, engineers, entrepreneurs, designers, programmers) and
companies.
KB objects of type piece correspond to pieces in the collections: machines of
course but also components, photos, original documents, software on the original media.
Other KB objects are pure elements of information, like product or person and all
their subtypes. In addition, objects may link to virtual instances of the real pieces, like
digital copies of documents, photos and other contents, as well as 3D models or software
simulators of hardware systems.
The KB maintains much more information than would be possible using the tradi‐
tional organization of catalogue records, no matter how much detailed you can conceive
the record schema – in this sense a quite anachronistic example is made by the Italian
ICCD standards [23] which are continuously updated by adding ﬁelds and ﬁelds in the
vain attempt to catalogue every piece belonging to the scientiﬁc and technological
heritage by using the same record schema.
The information in the KB is structured and factorized: there is no replication. The
KB itself is a useful tool for museum keepers to facilitate the identiﬁcation of pieces and
to carry out their cataloguing duties. On the other hand, curators and other experts from
the scientiﬁc community of IST historians contribute to the growth of KB by adding
objects and by creating relations among the existing ones. Sharing and constant peer
reviewing of the contents grant the KB authority. In this perspective, the KB can be seen
as the virtual warehouse of a Global Museum of IST which, virtually, assemble all the
collections in the World.
106
G.A. Cignoni and G.A. Cossu

The KB is navigable. The search features are tools for researchers and enthusiasts.
Picking from the warehouse, curators may set up virtual exhibitions useful to guide the
wide public to the discovery of the IST history. The KB can be used to virtually rebuild
a visit to a real collection or exhibition, but it is also possible and maybe more interesting
to build thematic paths through the contents of the KB, these will include pieces from
diﬀerent collections and suggest visiting the real museums to experience the thrill of
being in front of the originals.
5
Enhanced Storytelling: From the Apple-1 to the KIM-1
Figure 1 shows a view of a possible portion of the KB. Each icon or image in the ﬁgure
corresponds to a KB object. Most of the objects are elements of pure information, two
of them do refer to actual pieces: an Apple-1 and a KIM-1, respectively belonging to the
Bolaﬃ [24] and the Old-Computers.Com [25] collections.
Fig. 1. A sketch of some of the knowledge surrounding the Apple-1 and the KIM-1
The KB object of type piece corresponding to the Apple-1 in the Bolaﬃ collection
is linked to the KB object of type product. The ﬁrst maintains information of the
speciﬁc piece: the serial number, the state of conservation, special notes like the signa‐
ture of Steve Wozniak. The second one contains information common to all Apple-1:
the dimensions, the release and withdrawal dates, the technical characteristics, historical
notes and so on.
The keepers of the collections are in charge of editing the KB objects of type
piece. A scientiﬁc community (i.e. keepers, curators and other invited experts) manage
the editing of all other types of objects in the KB. Thus, the content of the KB is
The Global Virtual Museum of Information Science & Technology
107

authoritative and the KB is a tool that helps in the cataloguing of new acquisitions – the
Apple-1 is a simple case, but other IST artefacts have versions, variants, production
batches and are quite diﬃcult to be properly identiﬁed.
The Apple-1 product object is linked to objects of other types. In this example we
have a company (Apple), a designer (Steve Wozniak), a document (the user
manual). The links maintain and make explicit the relations among the information
objects. For example, the diﬀerent roles of the protagonists in the company foundation
and in the birth of its ﬁrst product: besides the aforementioned Wozniak, there are also
person objects for Steve Jobs and Ronald Wayne – the third, often forgotten, partner
which also wrote the Apple I user manual.
Information technology is modular: diﬀerent pieces of its history share technologies
and components. It is a characteristic of the domain that cannot be overlooked because
of an antiquated cataloguing scheme. In our example the adoption by the Apple-1 of the
MOS 6502 microprocessor is made explicit by the link to the corresponding compo-
nent object.
Following the relations linked to the 6502, the KB leads to the discovery of another
product object, the KIM-1 (an exemplar of which is preserved in the Old-
Computers.Com collection). Like the Apple-1 the KIM-1 was a motherboard which
hobbyists completed with power supply, keyboard and various peripherals. The KB also
maintains the information that the KIM-1 was the work of the same designer of the
6502 – namely Chuck Peddle.
The document objects can correspond to actual pieces, like copies of manuals in
the museums or libraries. They naturally are a way to access digital copies of the docu‐
ments, preserved forever, accessible to all and capable of bringing new levels of reading
to the IST history.
For example, the cover of the Apple-1 manual (Fig. 2) reveals a company logo very
diﬀerent from the elegant and minimalist one known to everyone. The ﬁrst Apple logo
was deﬁnitely baroque and full of references from the Newton’s apple to the citation of
William Wordsworth – and, in the KB, the quote of the English poet may become a link
with a famous Infocom video game of the Eighties.
The KIM-1 links also to the cover of Byte (Fig. 3, the December 1976 issue), which
is an even richer contribution for understanding a chapter of IST history. The cover
shows Santa Claus and his helpers checking a “toy” list that testiﬁes the desires of the
computer hobbyists of the time. In addition to KIM-1, there are the Altair 8800 and its
clone IMSAI 8080, the Sphere 500, the Poly-88 by Polymorphic Systems, the 8001
Compucolor by Intelligent System Corporation. The list also includes some basic
components (CRT monitors, keyboards, processors, memory chips), peripherals (ﬂoppy
drives, printers, teletypes like the popular Teletype ASR 33), software (like the assem‐
blers for Fortran and Basic), calculators (the legendary HP 65). Of course, there is also
a subscription to Byte itself. In such a long and rich list the absence of the Apple-1 is
blatantly evident. It was released in April 1976 but, according to Byte, it had no place
in the wishes of the IST enthusiasts of its times. It helps to assess the historical impor‐
tance of the Apple-1: it is not in terms of absolute technological or market value, but its
merit lies in being the starting product of a company that, later, has got to leave many
signs in the history of IST.
108
G.A. Cignoni and G.A. Cossu

Fig. 3. The cover of the December 1976 issue of Byte and the detail of the Santa’s list
A KB that makes available all the information and the relations among them, allows
telling the history of IST in all its complexity. It also helps to overcome the habit of
telling simpliﬁed stories – an indeed rooted use because it is easier, because following
Fig. 2. The cover of the Apple-1 user manual, and the detail of the logo
The Global Virtual Museum of Information Science & Technology
109

the trend is rewarding, sometimes because it is instrumental to the promotion of the
brands that dominate the present.
The example is partial, many links are omitted. For example, via the person Jobs
will be possible to reach other companies of which he was one of the founders, such as
Next and Pixar and better understand his role as an entrepreneur and visionary. Via the
company objects will be possible to reach all their products and, of course, each
product object is connected to all the piece objects describing the pieces preserved
in the collections. The Byte cover will be linked to the Byte issue, the Byte magazine
and to all the products cited in the Santa list.
The graphic in two dimensions shown in Fig. 1 helps to convey the idea, but cannot
grasp the wealth of information, especially in terms of links, that the KB maintains and
makes available (moreover the example does not touch software…). The amount of
information maintained by the KB requires new presentation ways, which must over‐
come the panels and labels on which the traditional exhibitions still rely. “In place”
presentations in physical exhibitions use the KB to enhance the visiting experience, for
instance customizing the information shown on digital labels according to visitor inter‐
ests. By “in cyberspace” presentations, the visitor of a virtual exhibition explores the
KB either freely or following suggested discovering paths. In both cases, the KB
provides access to an organized and authoritative quantity of information that radically
changes the approach to IST storytelling.
6
Technical Feasibility
The proposed approach relies on a KB as a viable and effective way to collect,
manage and make usable all the information about IST History, thus it goes beyond
the traditional vision of catalogues as inventories of pieces. The KB should be
managed by an international consortium, founded and supported by some leading
players of the sector, and with the involvement of a number as large as possible of
stakeholders in the IST History – including private collectors and enthusiasts.
The description of the KB’s idea given in the discussion in Sect. 4 and in the example
in Sect. 5 is intentionally informal. There is already considerable work done in the ﬁeld
of ontologies for cultural heritage. Just to give some examples, there are well known
proposals for the integration of information related to heterogeneous domains of the
cultural heritage, such as the Conceptual Reference Model [24] developed within the
International Committee for Documentation of the International Council of Museum,
later acknowledged as a standard by ISO [25]. For a complete dissertation on the state-
of-the-art about ontologies for the cultural heritage, see [26].
There are also works about ontologies focused on the speciﬁc domain of IST; e.g.
[27], born as a framework for curricula of informatics studies, which anyway is a refer‐
ence and a possible starting point.
Deﬁning the KB structure is obviously not in the scope of this paper: some research
and a considerable eﬀort are needed. Yet, the project is aﬀordable if undertaken by a
consortium of competent and authoritative partners, committed on the task of building
the KB and also able to bring a great contribution to populate it.
110
G.A. Cignoni and G.A. Cossu

Furthermore, while the model could be applied to many domains of cultural heritage,
we are going to focus on a single one. Compared with other attempts that tried to cata‐
logue every possible typology of cultural heritage (such as Europeana [28]), addressing
the deﬁned domain of IST History greatly helps the feasibility of the venture.
Finally, it is not necessary, or better is not required for a full exploitation of the result,
to aim since the beginning to a complete and deﬁnitive ontology of the IST History. As
a ﬁrst target it is suﬃcient to deﬁne a reasonable categorisation of the main object’s
classes and the fundamental relations between products (hardware and software),
people, companies, documents and so on. This suﬃces to be a decisive change in
comparison with the current situation, stuck in ﬂat and scattered archives.
Reﬁning the structure of the KB is a research issue which the consortium will be
involved in, thus giving to the community of experts a further element of scientiﬁc
interest, in addition to the “mere” peer reviewing of contents.
Moreover, various technologies are available to provide content to the KB by
browsing diﬀerent sources. Text mining and knowledge discovery in text are research
ﬁelds in rapid expansion [29]. Many approaches combine text analysis, natural language
processing, and machine learning to integrate the results into an ontology framework
[30]. Applications of these technologies are already in place to provide information
services to enterprises [31].
Deﬁning the KB schema and populating it to build the Global Virtual Museum is a
challenge that can be won. The needed base technologies are available, it is mainly a
matter of will, eﬀort and coordination among all the stakeholders.
7
Conclusions: A Possible Mission
We propose to create a Global Virtual Museum of IST able to include, in a structured
and consequently usable way, all the complex web of information that characterizes this
particular ﬁeld of cultural heritage.
There are already some experiences in this direction. The MINF project [32] brings
together French Museums and Institutions devoted to the preservation of the history of
computing. Collections on line [33] gathers the collections of four British Museums and
maintains some basic relations between pieces, people and digital contents. Our proposal
overcomes the national boundaries aiming at a single project which highlights the nature
of IST as a worldwide cultural heritage.
In order to guarantee full chances of success to such an international and ambitious
project as the Global Virtual Museum of IST, there must be a group of prominent
founders, able to give credibility to the project and attract the necessary funding to the
initial research and to start-up the consortium.
Undertaking the creation of the Global Virtual Museum of IST also implies to
address its sustainability over time. Part of the required eﬀort is already included in the
institutional activities of museums and in some cases the work of the keepers and cura‐
tors is even facilitated. On the other hand, there are extensive management and support
activities which need adequate economic support.
The Global Virtual Museum of Information Science & Technology
111

To assure the full sustainability of the Global Virtual Museum of IST in the future,
part of the initial research will develop a business model based on the exploitation of
the contents and the management of the intellectual property rights – from images to
digitized material, software and so on. Other sources of support may come from the
oﬀer of services to the wider public and from the use of its attractiveness for direct
sponsorship or as a medium for advertising. In this sense, being the Global Virtual
Museum of IST able to become a well-organized and worldwide renowned institution,
probably it will be even capable of bringing additional funding for the traditional activ‐
ities of the participating museums.
We believe that a sound consortium of stakeholders which includes at least few of
the most important Museums in the ﬁeld might be able to collect funds to start the project.
Sponsorship of Associations such as IFIP, IEEE, ACM could guarantee the scientiﬁc
validity of the results. Funds could be collected either from Public Institutions (e.g.
cultural heritage programs of the European Union) or from private companies strongly
tied to IST.
Our proposal related to IST History can be seen as a ﬁrst case study for a new
approach – based on worldwide catalogues of cultural heritage managed as knowledge
bases – that actually is general. From this point of view the Global Virtual Museum of
IST can be seen as a portable pilot experiment.
Lastly, apart from the intriguing recursive closure of applying IST to the preservation
of IST History, we actually feel that preservation and dissemination of IST History
deserve a special attention: we need to exhibit to a wider audience its complexity and
wealth of international contributions and to contrast the oversimpliﬁcation operated by
the mass media.
Acknowledgments. The idea of a KB to manage all the information related to IST History has
been previously presented at the National Conference of the Italian Association for Digital
Humanities [34] and originates from a local experience within the HMR Project [35]: we wish to
thanks Giuseppe Lettieri for the many fruitful discussions as well as the students who worked on
the CHKB prototype. Many thanks also to the Hyperborea staﬀ, in particular Luca Piaraccini,
Cecilia Poggetti, Sauro Salvadori and Norma Zanetti, for their contribution both on the project
vision and on the technical issues.
The images of the Apple-1 and the Kim-1 used in Fig. 1 are taken from the websites of the
cited collections [36, 37]. The image of the Apple-1 manual is taken from the Computer History
Museum website [20]. The image of the Byte cover is taken from the Internet Archive [38].
References
1. Conservatoire national des arts et métiers, Paris. http://www.cnam.fr. Accessed Feb 2016
2. Gottfried Wilhelm Leibniz Bibliothek – Niedersächsische Landesbibliothek, Hannover.
http://www.gwlb.de/. Accessed Feb 2016
3. Museo Galileo, Florence. http://www.museogalileo.it. Accessed June 2016
4. Science Museum, London. http://www.sciencemuseum.org.uk. Accessed June 2016
5. Museum of Science and Industry, Manchester. http://msimanchester.org.uk. Accessed June
2016
112
G.A. Cignoni and G.A. Cossu

6. The National Museum of Computing, Bletchley Park. http://www.tnmoc.org. Accessed June
2016
7. Centre for Computing History, Cambridge. http://www.computinghistory.org.uk. Accessed
June 2016
8. Laboratorio Museo Tecnologic@mente, Ivrea. http://www.museotecnologicamente.it.
Accessed June 2016
9. All About Apple, Savona. http://www.allaboutapple.com. Accessed June 2016
10. Rifkin, J.: The Third Industrial Revolution. St. Martin Press, New York (2011)
11. Standage, T.: The Victorian Internet: The Remarkable Story of the Telegraph and the
Nineteenth Century’s On-Line Pioneers. Bloomsbury, New York (1998)
12. Beniger, J.R.: The Control Revolution. Harvard University Press, Cambridge (1986)
13. Luebke, D.M., Milton, S.: Locating the victim: an overview of census-taking, tabulation
technology, and persecution in Nazi Germany. IEEE Ann. Hist. Comput. 16(3), 25–31 (1994)
14. Rengman, H. (ed.) Knowledge Management and Museums, Abstracts of the 23th CIDOC
Conference, Sibiu, 4–9 September 2011
15. Italian Law n. 364/1909 (1909)
16. Italian Law n. 1089/1939 (1939)
17. Istituto Centrale per il Catalogo e la Documentazione. http://www.iccd.beniculturali.it.
Accessed June 2016
18. Bauer, M.W.: The evolution of public understanding of science - discourse and comparative
evidence. Sci. Technol. Soc. 14(2), 221–240 (2009)
19. Deutsches Museum von Meisterwerken der Naturwissenschaft und Technik, München. http://
www.deutsches-museum.de. Accessed June 2016
20. Computer History Museum, Mountain View. http://www.computerhistory.org. Accessed
June 2016
21. Heinz Nixdorf Museumforum, Paderborn. http://www.hnf.de. Accessed June 2016
22. Angles, R., Gutierrez, C.: Survey of graph database models. ACM Comput. Surv. 40(1), 1–
39 (2008)
23. Istituto Centrale per il Catalogo e la Documentazione, Normativa PST – Patrimonio
Scientiﬁco e Tecnologico, v. 3.01 (2014)
24. Doerr, M.: The CIDOC CRM – an ontological approach to semantic interoperability of
metadata. AI Mag. 24(3), 75–92 (2003)
25. ISO 21127. Information and documentation – A reference ontology for the interchange of
cultural heritage information (2014)
26. Doerr, M.: Ontologies for Cultural Heritage. In: Staab, S., Studer, R. (eds.) Handbook on
Ontologies, pp. 463–486. Springer, New York (2009)
27. Cassel, L.N., Davies, G., Fone, W., Hacquebard, A., Impagliazzo, J., LeBlanc, R., Little, J.C.,
McGettrick, A., Pedrona, M.: The computing ontology: application in education. In:
Proceedings of the ITiCSE-WGR 2007 Working Group Reports on ITiCSE on Innovation
and Technology in Computer Science Education, pp. 171–183. ACM, New York (2007)
28. Europeana Collections. http://www.europeana.eu. Accessed June 2016
29. Piskorski, J., Yangarber, R.: Information extraction: past, present and future. In: Poibeau, T.,
Saggion, H., Piskorski, J., Yangarber, R. (eds.) Multi-source, Multilingual Information
Extraction and Summarization. Springer, New York (2013)
30. Wimalasuriya, D.C., Dejing, D.: Ontology-based information extraction: an introduction and
a survey of current approaches. J. Inform. Sci. 36(3), 306–323 (2010)
31. Dandelion API – Semantic Text Analytics as a Service. http://dandelion.eu/. Accessed June
2016
The Global Virtual Museum of Information Science & Technology
113

32. Musée de l’Informatique et du Numérique. http://musee-informatique-numerique.fr.
Accessed June 2016
33. Collections Online, Science Museum Group. http://collectionsonline.nmsi.ac.uk. Accessed
June 2016
34. Cignoni, G.A.: CHKB: dare struttura (visitabile) alle collezioni tecnico-scientiﬁche, talk at
the IV Conferenza Nazionale dell’Associazione per l’Informatica Umanistica e la Cultura
Digitale, Torino, 17–19 December (2015)
35. Computer History Knowledge Base. http://hmr.di.unipi.it/CHKB.html. Accessed June 2016
36. Bolaﬃ Collections, Torino. http://www.bolaﬃ.com. Accessed June 2016
37. Old-Computers.com on-line Museum. http://www.old-computers.com. Accessed June 2016
38. Internet Archive. http://archive.org. Accessed June 2016
114
G.A. Cignoni and G.A. Cossu

Why not OSI?
Bill Davey1(✉) and Robert F. Houghton2
1 RMIT University, Melbourne, Australia
Bill.Davey@rmit.edu.au
2 Idaho State University, Pocatello, USA
hougrobe@isu.edu
Abstract. This paper presents an argument that the OSI proposed standard is
technically superior to the TCP/IP standard for network communications. An
Actor-Network Theory approach is taken for analysis of the historical record
surrounding the adoption of TCP/IP. The paper does not seek to create a new
history of TCP/IP but to suggest this is a case where traditional explanations of
adoption based on the nature of the technology do not explain the demise of the
OSI model. Parallels are then drawn between this adoption and the possible prob‐
lems with the implementation of IPV6. These parallels provide insight into the
impediments that may arise with the adoption of the new standard.
Keywords: Internet · OSI model · TCP/IP · IPV6 · Adoption · Actor-network
theory
1
Introduction
This paper ﬁrst argues that the current TCP/IP standard for Internet communications is
technically inferior to the parallel OSI standard. The historical record is then analysed
to identify both the actors that were involved with the adoption of the standard and the
process of adoption that occurred. This is not an attempt to create a new history of TCP/
IP, as several robust histories exist. The aim of the paper is to examine the adoption of
TCP/IP over a superior technology as an example of an adoption story where the nature
of the technology does not explain the adoption. In this way we place the adoption of
TCP/IP as being similar to the stark example of VHS tape winning over the superior
BETAMAX standard. The paper then asks the reader to consider the example of IPV6
as another possible adoption where technical considerations may not determine the
future.
2
A Quick Timeline of the Internet
A selection of highlights of the introduction of the Internet provides some context for
this discussion. These points in history are chosen to provide some sense of the evolution
of the technologies and some of the uses proposed and enacted. Some trace the very
© IFIP International Federation for Information Processing 2016
Published by Springer International Publishing AG 2016. All Rights Reserved
A. Tatnall and C. Leslie (Eds.): HC 2016, IFIP AICT 491, pp. 115–121, 2016.
DOI: 10.1007/978-3-319-49463-0_8

ﬁrst thoughts to a paper by Licklider [1] who proposed a ‘Galactic Network’ that would
allow access to data and software from anywhere in the world. Licklider was the ﬁrst
head of the computer research program at the Defence Advanced Research Projects
Agency (DARPA) [1] and convinced succeeding players at DARPA that this concept
was important. The ﬁrst papers on packet switching, an essential technology, are attrib‐
uted to Leonard Kleinrock [2–4]. In 1969 the ﬁrst host computer was installed at UCLA,
enabling the start of ARPANET [1]. This ﬁrst instance of an Intenet like network used
Network Control Protocol (NCP), a host-to-host protocol. ARPANET was introduced
to the public in 1972 and email was seen as the ‘killer ap’ [1, 5]. By 1975 there were
more than 100 nodes on ARPARNET. NCP was designed as a protocol for a single
network. As other networks emerged there was a need, perhaps driven by a desire to
have email delivered between networks, to allow messages to pass between networks.
In January 1983 ARPANET changed the host protocol from NCP to TCP/IP. This change
allowed ARPANET to be split into military and research networks [1]. As other
networks changed to TCP/IP they were able to share research and deliver email to each
other. By 1985 a recognisable Internet existed using TCP/IP as the protocol. A useful
breakdown is that of the eras of packet switching, purpose built networks and then the
World Wide Web. Figure 1, taken from Mowery and Simcoe [5].
Fig. 1. From [5]
2.1
Discarded Alternatives
Most early networks were purpose built. Leiner et al. [1] identiﬁes MFENet and HEPNet
for physics researchers, the NASA Span network and the academic CSNET. With the
free distribution of Unix to academic institutions the built in UUCP protocol was the
basis for BITNET built in 1981 to link academic mainframe computers in several coun‐
tries. Some commercial companies attempted to establish standards around networks of
116
B. Davey and R.F. Houghton

their own computers such as the Xerox XNS, Digital Equipment’s DECnet and the IBM
SNA. Of course these networks did not favour interoperability with other networks and
this failure meant they remained islands. At this time no-one had an interest, commercial
or research, in progressing OSI to a working standard.
2.2
OSI vs TCP/IP vs the Bad Guys
The Internet is the network of networks. A network is a series of nodes interconnected
to exchange information. The Open Systems Interconnect (OSI) model is a seven layer
model designed to deﬁne the diﬀerent parts of network communication. Each layer has
a distinct and important function (Table 1).
Table 1. The OSI model
Layer name
Role
Physical
Media in which data is transmitted
Data Link
Standards by which data is bundled and
transmitted
Network
Standards that connects diverse networks together
Transport
Protocols that manage transmission of data
Session
Controls the connections between nodes
Presentation
Provides translation or syntax
Application
Displays data to user
The other model used to assign data transmission is the TCP/IP model (Table 2).
Table 2. The TCP/IP model
Layer name
Roll
Network/Link
Deﬁnes methods of connecting nodes
Internet
Creates datagrams and routes to networked nodes
Transport
Provides communication services from node-to-node
Application
Contains highest level of communication for the data
transmission
Due to the fact that TCP/IP has become the standard on which the Internet works,
the TCP/IP model is taught by network professionals. The OSI model is presented in
comparisons to the TCP/IP model. However in a world with growing computer security
threats, the OSI model is more relevant to the knowledge of IT workers. The OSI model
provides a complete framework on which IT security professionals can refer to diagnose
the application, source, and method of attack.
De Vivo et al. [6, 7] discuss methods of Internet security. All of their eﬀorts discuss,
explore, and explain methods of Internet attack. These papers show vulnerabilities in
the TCP/IP stack. While these papers are in depth and are prime examples of security
research, not one of them discuss attack methods at the physical network level. Diﬀerent
media require diﬀerent approaches to security. A stray radio wave is much harder to
Why not OSI?
117

defend over a buried ﬁbre optic line. An example of this would be when, in 2008, the
Associated Press reported that a major Internet cable was cut in the bottom of the ocean.
While De Vivo et al. [6, 7] might call this a service attack due to the fact that the Internet
is not available, TCP/IP does not have a proper method of classiﬁcation.
3
Research Method
The historical record of all standards is rich as the original meeting minutes for standards
organisations are available. In addition historical analysis of the introduction of stand‐
ards has been performed by a number of studies from diﬀerent viewpoints. On top of
this literature there are many commentaries on both the historical record and the forces
at play. In identifying actors and interactions these sources were studied for repeated
identiﬁcation of the same actors and rich descriptions of relationships between them.
The study of a communications standard is particularly relevant to a material-semi‐
otic approach [8] as the communications layer is intended to be beneath the notice of
the user. It is also apt in the historical context as communication between the humans
was dependent on the existence of a standard for communication. This ‘sine qua non’
vicious circle is an entertaining aspect of the analysis.
The analysis was conducted by two very disparate researchers: one elderly Australian
with a systems background and one younger American with a networking background.
This allowed independent reading of sources with researcher meetings to discuss and
decide upon a ‘middle ground’ and the conservatism arising from this approach may be
evident in the outcome.
The method taken was to ﬁrst identify signiﬁcant actors: humans, technologies and
organisations. This analysis is not intended to be a comprehensive analysis of the whole
story of TCP/IP but to identify divergences from the adoption of a possible OSI model
based alternative.
3.1
The History of OSI and TCP/IP
TCP/IP actors arose in both Europe and the United States through the 1970s. Universities
in London, Manchester, Bristol and Edinburgh were reﬂecting the energies being
expended in DARPA and the universities in the USA. On both continents the initial
standards were those of Unix and X.25 communications [9]. These protocols were frus‐
trating to the visions of most of the players in producing a wider, more open architecture.
Leiner et al. [1] identify the move of Dennis Jennings to lead the American NSFNET
program as a crucial point in the move to adopt TCP/IP. At almost the same time Reid
[9] suggests that the UK JANET network had become so unworkable that a decision
was made to adopt TCP/IP in the form of the Janet IP Service (JIPS). These adoptions
of the standard resulted in massive increases in use of the networks. Reid [9] estimates
that TCP/IP traﬃc in the JIPS system exceeded the X.25 traﬃc within 10 months of the
introduction.
The OSI protocol suite was proposed and discussions started in 1978 [2]. This suite
would encompass all the requirements placed upon it by the multitude of stakeholders.
118
B. Davey and R.F. Houghton

As late as 1988 the Internet Engineering Task Force (ITEF) issued a memo stating that
all government computers would use OSI [3].
In 1990 Tim Berners-Lee invented the World Wide Web application [4]. By this
time it was clear that the OSI protocol suite was lagging behind the development needs
of the Internet. TCP/IP was already in use for the connection and transmission of data.
As the use of the Internet grew, the need for computers to connect with each other
in a common standard was overwhelming. TCP/IP was in place and OSI was still being
developed. TCP/IP became the de facto standard for all data transmissions on the
Internet.
3.2
The Demise of OSI
OSI had been considered and rejected for the JANET network [9, 10]. No other actor
seemed interested in developing a new standard on the basis of OSI. The very large actor
identiﬁed by some historians was the funding from USA Government agencies. The
National Science Foundation (NSF) contributed large amounts of money as did the
various aspects of DARPRA. The money issue is reported by Reid [9] in terms of the
problems of funding even the telephony charges that Universities in Britain found
mounting. Of course an open standard that allowed computers from IBM and DEC to
connect with no licence charges should be seen in the context of funding as an actor. A
research or academic institution could use the low cost BSD Unix and the open standard
to immediately join the Internet. The value of Unix as an actor should also not be
discounted. As the low fee BSD Unix was available to the enquiring minds of University
students a plethora of utilities became available that reinforced the value of Internet
connections. The value of money as an actor became stark when, in 1985 the NSF
mandated that funding for Internet connection in Universities would only be available
if TCP/IP was used [5]. The size advantage of ARPANET was almost guaranteed from
that point on.
One would expect the support for OSI to come from standards organisations but a
set of actors joined the discussion in the form of credible standard setting committees.
These included the Internet Conﬁguration Control Board (ICCB) formed by the then
DARPA Director. This later became the Internet Activities Board (IAB) and absorbed
the Internet Engineering Task Force (IETF). Eventually, in 1992, the work of these
bodies was co-ordinated by the Internet Society (ISOC) [5]. These actors should be seen
in opposition to the standards bodies, mostly in the telecommunications area, which had
been advocating X.25 as the standard for the communication protocol, and might have
been able to advocate for OSI if there had been an alternative.
A more recent development has seen that OSI is still being taught in conjunction
with TCP/IP in academic network theory classes. The top selling textbook on Amazon:
Computer Networks (by Tanenbaum) [11], contains an extensive section on OSI and
how it can be used to help diﬀerentiate network layers. In fact a search of networking
textbooks shows that the top ten search results in books.google.com each contain at least
a section if not a chapter on OSI.
Why not OSI?
119

3.3
Format Wars
Every technology that fails still enhances the future use of technology. One of the ﬁrst
technologies that begat a standardisation issue was the diﬀerences between alternating
current and direct current. Thomas Edison was trying to convince the public that DC
power would provide electricity better than AC. In using the vastness of his technology
to promote his views on power in Electrocuting an Elephant, in 1903 Edison showcased
the destructive force of AC current [12]. Eventually events proved that AC power could
be carried further long distances but DC power would be the preferred method for elec‐
tronic devices.
An example of competing formats is the very famous VHS verses Betamax wars.
This video standard saw that VHS won the consumer market. Betamax was still widely
used and only stopped production in 2015. This was due to the fact that Betamax
recorded higher resolutions. Betamax found a niche in midsized television stations as a
lower cost but high quality recording methodology. This format also allowed its creator,
Sony, to enhance its audio cassette tapes until the arrival of digital media ﬁnally caused
the demise of most tape recorders.
These wars still go on today as witnessed by the various Apple operating systems
against their more prevalent competitors. In the 1980 through the 1990s Apple and
Microsoft competed over the desktop computer marketplace with Apple almost going
bankrupt in 1997. However by 2006 Apple rebounded to create the iPod and later the
iPhone revolutionizing mobile computing. By 2011 Google’s android mobile operating
system had over 50 % of the world’s market share [13].
4
Conclusion
At its core, OSI was a rejected technological solution. Reasons include that, at the time,
TCP/IP could handle all the communication necessary, was already deployed on the
majority of systems, and was fully complete. None of these are factors that would arise
form an analysis that did not consider the actors and their interactions. As the Internet
evolved into the mass-market that we use today, security was an afterthought, not
actively considered The lack of consideration of strong structural issues can be seen as
a result of the actors involved in the initial stages of adoption. These actors were not
envisaging a system serving the needs of the current Internet that even involved banking
transactions. TCP/IP continues to have signiﬁcant security issues and these are solved
by ‘work-arounds’ that would not have been needed in an OSI based system. More
research is needed into using OSI as a security model replacement for TCP/IP.
The analysis here is ex post facto but there is a modern issue that could be analysed
in the same fashion. IPV6 [14] has been designed to overcome many of the architectural
problems of TCP/IP. This superior technology has not been adopted at the rate antici‐
pated despite the potential problems of diminishing IPV4 space [15–17]. Some problems
with the implementation of IPV6 are technical, for instance the diﬃculty of changing
V4 addresses that are hard coded. Authors have also addressed the question of interac‐
tions between actors – especially between ISPs and between customers and ISPs [18].
It is possible to characterize a set of problems with the adoption of V6 as “it is working
120
B. Davey and R.F. Houghton

now why should I change”. This pragmatic approach to technology relates strongly to
our analysis of the OSI model – a technology is in place that has faults, but the actors
have a strong interaction with that technology and their inertia, or their investment in
the inferior technology. This means that the prospect of examining a better technology
and “starting again from scratch” is never likely to happen.
References
1. Leiner, B.M., et al.: A brief history of the Internet. ACM SIGCOMM Comput. Commun. Rev.
39(5), 22–31 (2009)
2. Leonard, K.: Communication Nets: Stochastic Message Flow and Delay. McGraw-Hill,
New York (1964)
3. Kleinrock, L.: Queueing Systems Vol: II: Computer Applications. Wiley, New York (1976)
4. Kleinrock, L.: Information ﬂow in large communication nets. RLE Quarterly Progress Report
(1961). 1
5. Mowery, D.C., Simcoe, T.: Is the Internet a US invention?—an economic and technological
history of computer networking. Res. Policy 31(8), 1369–1387 (2002)
6. De Vivo, M., et al.: Internet vulnerabilities related to TCP/IP and T/TCP. ACM SIGCOMM
Comput. Commun. Rev. 29(1), 81–85 (1999)
7. De Vivo, M., de Vivo, G.O., Isern, G.: Internet security attacks at the basic levels. ACM
SIGOPS Oper. Syst. Rev. 32(2), 4–15 (1998)
8. Law, J.: Actor network theory and material semiotics. In: Turner, B.S. (ed.) The New
Blackwell Companion to Social Theory, pp. 141–158. Wiley-Blackwell, Oxford (2009)
9. Reid, J.: The Good Old Days: Networking in UK Academia ~ 25 Years Ago. [PDF] (2008).
(cited 2008 16 April 2008)
10. Cooper, B.: From Janet to SuperJanet: The Development of a High Performance Network to
Support UK Higher Education and Research. The Computer Board for Universities and
Research Councils (1990)
11. Tanenbaum, A.S.: Computer Networks, 4th edn. Prentice Hall, Upper Saddle River (2003)
12. Daly, M.: Topsy: The Startling Story of the Crooked Tailed Elephant, PT Barnum, and the
American Wizard, Thomas Edison. Grove/Atlantic, Inc., New York (2013)
13. Oswald, E.: Android Market Share Growth Accelerating, Nielsen Finds. PC World 2011,
February 2015. http://www.pcworld.com/article/226339/android_market_share_growth_acc
elerating_nielsen_ﬁnds.html
14. Deering, S.E.: Internet protocol, version 6 (IPv6) speciﬁcation (1998)
15. Nikkhah, M.: Maintaining the progress of IPv6 adoption. Comput. Netw. 102, 50–69 (2016)
16. Southworth, J.B.: An empirical analysis of the relationship between IPv6 readiness, IPv6
enablement, and IPv6 eﬀectiveness at colleges and universities in the United States, Indiana
State University (2016)
17. Zamani, A.T., Ahmad, J.: IPv6 adoption: challenges & security. IJCER 3(1), 08–12 (2014)
18. Nikkhah, M., Dovrolis, C., Guérin, R.: Why didn’t my (great!) protocol get adopted? In:
Proceedings of the 14th ACM Workshop on Hot Topics in Networks. ACM (2015)
Why not OSI?
121

Flame Wars on Worldnet: Early Constructions
of the International User
Christopher Leslie
(✉)
New York University Tandon School of Engineering, Brooklyn, NY, USA
chris.leslie@nyu.edu
Abstract. Some of the earliest users of the Internet described their activities as
predicting a widespread communication medium that would cross national boun‐
daries even before the technical capability was possible. An analysis of conver‐
sations on Human-Nets, an early ARPANet mailing list, shows how users were
concerned about providing a forum for open discussion and hoped that the
network would spread to provide communication throughout the world. Moving
forward to CSNET, one can also see a strong insistence that the network provide
connectivity beyond the United States. Contrary to those who might tell the
history of the Internet as a story of a technology that was ﬁrst perfected by the
military, adapted by U.S. academics and then brought to the rest of the world in
the 1990s, these users reveal a strong ideology of international communication.
Keywords: ARPANet · Internet · Human-Nets · CSNET · e-mail
1
Introduction
The story of the internationalization of the Internet is sometimes told as if the technology
diﬀused on its own merit without anyone having to act: packet-switching and TCP/IP
were developed for ARPANet, the story goes, and once routing was perfected for
NSFNET, the Internet went worldwide in the 1990s. The rapid increase of Internet users
after the development of the WorldWideWeb, in particular, makes it seem as if the
Internet was just a good idea that attracted the attention of many users quickly, crossing
national boundaries after it was perfected. Nevertheless, one can see from ARPANet
mailing list discussions that even before there was the technical possibility to extend the
network to other countries, there was the desire to do so. The high regard the ARPANet
community placed on the power of online communication and the potential it saw to
transcend national boundaries was on their minds before connections went international.
From this, one suspects that the diﬀusion of Internet communication was a preliminary
design consideration rather than something that occurred after the design was perfected.
An agentless diﬀusion of technology, certainly, contradicts the central place Science
and Technology Studies (STS) accords to users. STS practitioners are averse to deter‐
ministic interpretations; technological advances do not, an STS scholar would argue,
spread through the world of their own accord. An agentless dissemination of the printing
press, for instance, would say that the rapid increase in European literacy shows how a
© IFIP International Federation for Information Processing 2016
Published by Springer International Publishing AG 2016. All Rights Reserved
A. Tatnall and C. Leslie (Eds.): HC 2016, IFIP AICT 491, pp. 122–140, 2016.
DOI: 10.1007/978-3-319-49463-0_9

device transforms society on its own; however, analysis reveals that a cultural substrate
already existed in the form of book markets where hand-copied texts were sold (see [1]
for instance). If the pre-existing demand for books helps to explain the rapid diﬀusion
of the printing press, what analogous substrate can be said to have paved the way for
international demand for the Web-enabled Internet?
One pre-existing network that can be seen as an important preparation for the Web-
based Internet was CSNET, which connected computer science departments without
defence research contracts to the ARPANet in the 1980s. CSNET’s ethic of international
collaboration was an indicator of the kind of activity that shows how individual users,
working with a common goal, would be important in distributing the knowledge and expe‐
rience needed for the eventual success of the Worldwide Web. This is not to say that
CSNET was the only mechanism that worked to this end; certainly other networks
(USENET, BITNET, MERIT, Télétel and its relatives, and Telenet, to name a few) were
important in this regard. CSNET, however, offers insight into the technical community’s
international ideals and aspirations. The motivation behind this cosmopolitan ideal can be
seen in the early ARPANet discussion group Human-Nets. This list, one of the first
networked discussion groups, was devoted to the future of computer networks and is a rich
resource that one can use to gain insight into what ARPANet users thought about the tools
they were using. In particular, in the so-called flame wars and the meta-discussions of how
to deal with group communication by electronic mail, one can see how ARPANet users
immediately embraced the ideals of free and open connections.
In the late 1970s, as the ARPANet was coming into its own and the ﬁrst tests of what
would become TCP/IP were being conducted, users communicated by what today we
would call e-mail in interest groups that predicted USENET and other e-mail groups. In
RFC 541, these messages were conceived of as text messages sent by ﬁle transfer
protocol (FTP) and were simply called mail and RFC 822 refers to them as Internet text
messages, even though the protocols they utilized were behind what today we think of
as e-mail. A fairly well-known anecdote about the careful attention to the quality of
conversation on these channels relates to the claim of Quasar Industries that it was about
to mass-produce service robots came to the attention of the ARPANet community.
Concerned that what they were seeing was a fraudulent claim, a debate emerged on
electronic discussion lists.
Quasar’s claim is noted as a hoax by Hafner and Lyon’s Where Wizards Stay Up
Late [2] and more recently in Finn Brunton’s Spam [3]. However, the hoax went unques‐
tioned in the business press and even in one academic text [4]. Alan Abelson reported
in 1979 that at the Ninth Davos Symposium, which had the theme preparing for the
changes of the 1980s, Quasar president Anthony Reichelt presented one of his “people-
looking robots” that would be manufactured for security, sales, domestic help, and even
psychotherapy. “Extremely good results have been achieved in communication with
autistic children,” Abelson wrote [5]. “A Robot in Every Home,” Changing Times
proclaimed in 1978. A domestic android would be oﬀered for $4,000 and a $45 per
month service contract. The android would answer the door, ﬁx drinks, vacuum, and
guard against ﬁres and burglars [6]. “Quasar Industries, for example, has a domestic
robot in production for 1980 which will perform many tasks around the house generally
assumed to be the human” [7]. Given the professional interests of the people on
Flame Wars on Worldnet: Early Constructions of the International User
123

ARPANet, it is not surprising that these claims garnered attention and criticism. What
is more interesting, though, was how events like these fostered debates about the debate.
As detailed below, the meta-discussion about the role of online communication and the
potential for its abuse oﬀers an intriguing glimpse into the aspirations of the community
that was developing TCP/IP.
Although user activity on the early network is not thoroughly studied, it is important
in several respects. Given that CSNET was funded by the National Science Foundation,
it can safely be said that the project helped the NSF internally for the more widespread
development of NSFNET at the end of the decade. Additionally, there were signiﬁcant
technical developments made in the network, including the creation of software libraries
and a patch that allowed TCP/IP packets to be sent over the alternative X.25 pipes, which
would help provide a motivation to get onto the Net and facilitate the means to do so.
Finally, however, it is notable that many international TCP/IP connections were fostered
by CSNET, which is something of a paradox. CSNET was developed to help U.S.
academic researchers in computer science join together as a community, not as a project
for international diplomacy. What, then, was the motivation behind the diﬀusion of
CSNET to other countries? As it turns out, in the years around the international spread
of CSNET, one can ﬁnd evidence of user sentiment about the online community that
would seem to indicate that it was the users, and not the policy makers, who sponsored
international connections, and oﬃcial policy followed in their wake.
The present paper draws attention to the power of users that has been important to
the study of the history of computing. Janet Abbate, in Chap. 3 of her seminal Inventing
of the Internet [8], notes that users transformed the ARPANet to ﬁt in with how they
were accustomed to working, resulting in, among other things, mail protocols. The
conversation about networking on Human-Nets takes a similar approach. In the same
way that Robert Boyle and his contemporaries are described in Leviathan and the Air
Pump [9], we see these researchers debating about how debates should take place. In
this way, users deﬁne the kind of community they expect to develop around computer
networks and thus shape the way in which networks develop. This concept nicely
supports the STS approach known as social construction of technological systems
(SCOTS), especially as summarized in the anthology How Users Matter [10]. Per
Lundin, however, has posed an interesting challenge in his Computers in Swedish
Society: Documenting Early Use and Trends [11]: the users who matter are only a subset
of the people who use a system, the insider group that is actively involved in the devel‐
opment of a system. His work seeks to document the middle ground of what he calls
elite users, the ones behind the public ﬁgures and the masses of end users who passively
(or, as Lundin states, perhaps resentfully) are the users of a system.
Building on Lundin’s insight, this paper tells the story of CSNET in the context of
reports from users of computer networks. Where Lundin has attempted to “build a
history from below” by creating archival documents through interviews, however, I have
been interested in using a diﬀerent sort of documentary evidence: the records left by
elite users regarding their eﬀorts and challenges to form discussion networks. Looking
at the archives of the Human Nets mailing list from the 1980’s and analysing the content
to quantify how many posters initiated discussion about how debates on the list should
operate is one aspect of the analysis; in addition, qualitatively investigating the attitudes
124
C. Leslie

and ideals of this community in another. The sentiment found was not fundamentally
diﬀerent from today’s Internet users. This was an online community where any discus‐
sion was welcome, where people fought for freedom of speech and shared knowledge
and information. In this way, this paper juxtaposes the story of the internationalization
of Internet technology via CSNET with a contemporary dataset of insights from
computer users. The international eﬀort to spread connectivity is interesting because it
demonstrates the power of the computing professionals to form a community deﬁned
on the profession, rather than state identity, in an era when computing projects were
largely funded by national governments.
2
A Genealogy of CSNET
Hafner and Lyon [2] suggest that CSNET was created to avoid a sort of digital divide.
The ARPANet in 1979 had 61 nodes, 15 of which were at universities, even though there
were around 120 computer science departments nationwide. A connection to ARPANet
meant access to shared ﬁles, remote login to services, and by 1979 a growing number
of electronic messaging groups where the latest ideas could be discussed. Departments
such as that at Purdue University, where the vice president of the ACM, Peter Denning,
worked, felt as if they were being left out of the loop. Top faculty and graduate students
seemed drawn to universities with ARPANet connections, meaning that universities that
did not have the political connections, ﬁnances (an ARPANet site cost $100,000 just to
set up), and defense contracts were in danger of being left behind – not to mention that
academics had a much better chance of gaining access to the network and advanced
computer facilities by leaving the university altogether and moving to an industrial site.
In order to address this gap, Larry Landweber from the University of Wisconsin
made a proposal to the National Science Foundation for a network of universities and
corporations that did not have ARPANet access [12]. Although he was not a networking
expert, Landweber was no newcomer; he had previously worked with the NSF on a
project called THEORYNET to provide researchers in theoretical computer science
email access. His ﬁnal proposal for CSNET in 1980 had a three-tier system funded by
a $5 million grant. The lowest level of connectivity was oﬀered by PhoneNet, a store-
and-forward service that allowed email connections over periodic phone calls; unlike
the growing Usenet service, however, PhoneNet email could be sent to ARPANet
addresses. X25Net was a middle tier, allowing users to run TCP/IP packets over leased
lines using X.25 so that they could access FTP and Telnet via a connection at Madison.
For a few sites, such as Landweber’s department as well as Purdue and the RAND
Corporation, full ARPANET connectivity was oﬀered. The rapid spread of CSNET
speaks to the widespread use of computer networks at the time. CSNET was originally
an acronym for “computer science research network,” but once computer scientists had
widespread connectivity, other science ﬁelds asked for access and were “urged to use
the mechanisms already available.” For this reason, the name of the network changed
to the “computer and science network” [13]. By 1982, there were 75 PhoneNet
subscribers.
Flame Wars on Worldnet: Early Constructions of the International User
125

CSNET was an early proponent of international connections even in the troubled
years of the Cold War. The ﬁrst explicitly international connection of CSNET was to
Israel in 1982, followed by France, West Germany, and Japan. In spite of a high-tech
export ban, Korea and China managed to connect through the eﬀort of academics, and
CSNET became a model for the program of international connections sponsored by
NSFNET from 1987 to 1994. Even though it was a signiﬁcant force in the internation‐
alization of the Internet, the initial proposal barely mentions other countries. In fact,
CSNET was designed in response to a perceived national “crisis” in computer science,
according to the proposal [12]. A shortage of qualiﬁed personnel in experimental areas
combined with the low availability of software tools due to low productivity and blocks
to transporting completed tools had brought the United States to a crisis. According to
the proposal, the personnel shortage resulted from competition from industry. The
demand for computers had “exponentially” expanded the need for designers and
programmers, leading industry to raise salaries to “record levels.” Talent was also
attracted to industry sites because of their “superior experimental facilities,” meaning
that students were discouraged from entering PhD programs and those who did complete
a PhD were understandably attracted to industry positions.
Citing various studies around 1980, the proposal indicates that there were around
75 % fewer candidates than postings for faculty positions. The software challenge was
due to the diﬀerent media used by disparate computer systems and the challenge of
locating software that had already been developed, which forced researchers “to need‐
lessly redevelop tools or forego promising lines of investigation altogether” [12]. A
research network, the proposal asserted, was an essential part of averting this crisis.
CSNET’s standard protocols for ﬁle transfer and its electronic messaging system would
facilitate “new cooperative ventures” and thus help to increase software output. Simi‐
larly, it would address the personnel shortage by making academic departments more
attractive sites for research and allowing “critical masses” of researchers, which would
foster collaboration.
Given the funding from the National Science Foundation, the focus on the U.S.
situation is perhaps not surprising; the use of tax dollars for cutting-edge research could
be expected to advance the interests of the country. In spite of the nationalistic appeal
to a crisis, there is no mention of containing CSNET to the United States; although
someone familiar with TCP/IP protocols and X.25 networks would realize that interna‐
tional users could connect, someone with less technical savvy might not have realized
the potential openness of the network. Other countries, nevertheless, do ﬁnd their way
into the proposal: in assessing the potential impact of the project, the authors cited
SAMNET, a message service based at the University of Toronto. In the context of the
dissemination of technical reports, the proposal notes an eﬀort by the International
Federation for Information Processing (IFIP) to create an online journal, which could
be made available to users of CSNET.
The idea of international collaboration, however slight in the proposal, seems to have
been on the mind of the project management committee. A report [14] on CSNET’s ﬁrst
year of operation, dated 22 July 1982, makes an overt mention of reaching outside of
the United States. The Management Committee, headed by Landweber and comprised
of Denning (Purdue), Edmiston (BBN), Farber (Delaware), Hearn (Rand), and Kern
126
C. Leslie

(NSF) had been in email communication daily and in person about every two months.
The report mentions that, beyond day-to-day operations, the committee had been discus‐
sing “international collaboration” (CSNET 21).
The network became self-suﬃcient in ﬁve years, according to its goal, but in 1988
it was merged with another network, BITNET, and ceased operation as a separate entity.
Because of its short life, it is tempting to skip over CSNET. Even in 1984, a writer on
Human-Nets trying to add up the number of users of computer networks remarked:
I think that we can ignore CSNET here (they’re all either on USENET or directly on Internet
anyway…), so they count for zero.
Certainly, in looking over early mailing list archives, it seems as if many U.S. users
display USENET, ARPANet, and CSNET addresses in their signature blocks in the
mailing list discussions on HumanNets and SFLovers. For this reason, perhaps, the story
of CSNET is sometimes omitted or abbreviated in the history of the Internet, but it is
not necessarily in connectivity that CSNET played its important role. Janet Abbate, in
Inventing the Internet, notes that after its experience with CSNET, the National Science
Foundation began experimenting with networks of its own, leading to NSFNET. On the
occasion of the Internet Society’s awarding CSNET the Jonathan B. Postel Service
Award in 2009, Stephen Wolﬀ [15] remarked,
CSNET was a critical link in the transition from the research-oriented ARPANET to today’s
global Internet. CSNET also helped lead the way by sharing technologies, fostering connections,
and nurturing the worldwide community that provided a foundation for the global expansion of
the Internet.
3
Flaming on Worldnet
Today, an e-mail conversation among a group of people who share common interests
commonplace, but this use of electronic messages is in fact a development of ARPANet.
Ronda Hauben [16] provides a good description of this service:
Also in that year [1975] the MsgGroup mailing list was started to explore the use of the
ARPANET for conferencing. Mailing lists were uses of email where each message sent to the
list was distributed to all list members. They each could read it and comment by sending a new
message which in turn would be sent to all the other list members. Very serious and high level
results occur. Other mailing lists appeared such as Human-nets for the discussion of the future
of ubiquitous networking and SF-Lovers for the discussion of a favorite hobby, science ﬁction.
Participants in these lists were happily surprised how interesting and valuable such network use
was.
Richard T. Griﬃths [17] notes that there were about 17 groups by the end of the
1970s and that there were 44 by 1982. Keith Lynch [18] writes that the list was devoted
to a discussion about what they called Worldnet: “a hypothetical future worldwide
computer network.” This is ironic, though, because as noted by Griﬃths only users of
the exclusive ARPANet could join the conversation. According to Wikibooks, Mark
Horton provided a feed from the ARPANet group to USENET in 1980 [19], which
allowed others to listen in on the conversation even if they could not send messages
Flame Wars on Worldnet: Early Constructions of the International User
127

back. This shunt of messages helps to provide the ﬁrst archive of this group and others
because USENET messages are better preserved.
The third volume of Human-Nets [20] included issue 98 to issue 112, from May
1981 to June 1981. It had a total of 122 messages, of which 37 were lacking full or real
names. Roughly categorizing the topics of discussion yields a proﬁle of the discussion.
About 50 messages concerned general information about technology and science, 30
were asking for help or providing assistance, 34 expressed concerns with emerging
technologies, 38 explored and discussed interactions between humans and computers,
25 discussed the Human-Nets listserv itself, 22 were creative and artistic, and 8 were
announcements of some sort. By examining the times in which a post was a reply to
another user, one can see that there were over 20 discussions, but only a few had over
10 or more replies. However, some discussions went into great depth and did not seem
to require multiple inputs, whereas some discussion had many replies with very little
content in each. The two biggest discussions were about the eﬀect of computer networks
on the English language and synthetic chemicals.
The conversation on language was far-ranging and recalls conversations one might
have today. For instance, the notion of ‘ﬂaming’ a user or list appears in the ﬁrst archived
volume when a user apologizes if his ‘ﬂame burns anyone.’ Several users expressed this
word in their experience of using computer jargon in real life. One poster noted that
“ITS-ey” jargon had caught on among the poster’s parents and friends. A few were
interested in the etymology of some words; the habit of marking ranting text as a “ﬂame”
came under scrutiny at one point, with posters questioning the etymology of the term.
One of the ideas proposed was that ﬂaming came from a homophobic origin: “‘Flame’
in hackerese and ‘Flaming’ in homophilese,” one poster wrote. Another user suggests
that the word was not coming from the gay community but the reluctance of some using
it reﬂects their discomfort with the word’s connotation. Soon, though, pseudocode
turning ﬂames on and oﬀ appear and users frequently refer to impassioned speech as
ﬂames. In a published article from 1980, Les Earnest [21] proposed an origin that was
not related to a gay slur:
The quickness of communication carries with it a problem, however; we have discovered that it
is much easier for people to lose their tempers in this new form of journalism than in slower
systems. The problem clearly arises from perceiving mild disagreements as insults when they
are quickly distributed to hundreds of people. This phenomenon has been observed enough times,
even involving normally cool-headed people, that it has been given a label: ‘ﬂaming.’ The
Human-nets editor has learned to suppress material that seems to be ‘ﬂaming.’
In addition to this discussion there was also a small oﬀshoot discussion about sexist
language or terms in the computer lexicon. While these discussions do not gain
momentum, their presence in the ﬂow of the conversation about the future of computer-
mediated conversation reveal that there was some sensitivity to how outsiders might
react to their word choice and an eﬀort to make a more inclusive environment.
The discussion on synthetic chemicals started as inquiry about left-handed (low
calorie) sugar and drifted into sci-ﬁ territory of dark matter. The discussion touched
chemical structures along the way. Strangely, much of the discussion was low quality,
similar to some discussion on the Internet today. However, they were lucky enough to
have someone knowledgeable step in and explain away the whole mess. Another big
128
C. Leslie

discussion was on colour blindness and how to improve technology for those who are
colour-blind. The discussion was not solely focused upon computer usage but as well
as sharing personal experiences and discussing potential solutions.
Technology oriented discussion focused primarily on speed of modems and phone
lines, as well as the Xerox Star computer. The discussion on the Xerox Star workstation
centred mostly on pricing and the computer’s speciﬁcations. There was also discussion
about TV signals, mainly how to descramble them. Most of the discussion on speed was
explaining how the lines work and about which lines to acquire. The major complaint
was about not achieving the maximum speed. Help was oﬀered with concrete step by
step information. The discussion at the end of the volume was about the Human-Nets
itself. The discussion about low-calorie sugar and anti-matter caused the moderator to
suggest moving such discussion to other mailing lists. But most members replied that
discussion and free speech should be encouraged.
These numbers paint a contrasting picture about the Human-Nets: there was a spec‐
trum of dialogue or discussion. At one end, it was purely knowledge-based, and at
another end, the content was personal. At this point in the mailing lists, the majority of
the posts tended to fall on the former part of the spectrum. The most personal of messages
were what today we would call ﬂames, members calling each other out in an aggressive
fashion. The most informative messages were about modems and computers. Discus‐
sions about the computer science community and the mailing lists were in the middle
of that spectrum. This analysis indicates that people preferring computer interaction is
not a novel a problem. It has been recognized since the 80s. There was also curiosity
about the user base, of its demographic and social impacts. However, much of this was
speculation on Human-Nets because it was diﬃcult to conduct research on an emerging
technology. There were questions as to how to poll/survey people, but the results were
not listed on the digest. They were placed in ﬁles on the mainframe computers and thus
were probably lost.
The discussion on colour-blindness signals the start of ergonomics and attempts to
make computing more available to the public. It also demonstrates how mailing lists
facilitated exchange of knowledge completely unrelated to computing. Getting oﬀ topic
was the natural order of things. The exemplary of topic discussion was the low calorie
sugar discussion. This discussion also demonstrated that the people who posted on the
mailing lists did not know what they were talking about sometimes. When the discussion
switched over to dark matter, there was a lot of hearsay. The discussions about modems
and lines were extremely detailed. After all, this was their expertise. There were also a
lot of people seeking technical help. Unlike the MsgGroup, there was not any moderator
comment about criticism on computers and their specs. The people were allowed to bash
on the products they did not like. The meta-discussions seem to be the most historically
important. It seems that because of the user base, there was not much censorship or
moderation. In fact, the moderators did not seem have much power except those given
by the users. Overall, Volume 3 shows a basic picture of the early days of the ARPANet.
By itself, it only provides a small window, but even during that small window, the
amount of traﬃc was quite great. And even with a glance, it is clear that this was the
place for exchanging ideas and for sharing important knowledge.
Flame Wars on Worldnet: Early Constructions of the International User
129

Even by June 1981, though, the size of the list and the volume on it had increased to
an unexpected level. A message from “The Moderator” with the subject “Submissions
to Human-Nets” notes that he has received complaints about the “randomness” of the
topics, and he asks members to remember that messages will be read by 3,000 people.
In particular, he apologizes for allowing the discussion on so-called left-handed sugar
(meaning no-calorie sugar, left-handed being a description of its molecular structure).
It has become clear how exuberant participants felt about the conversations they were
having. He noted that he would be leaving the post of moderator because he had taken
a job elsewhere. Jorge Phillips, nevertheless, disagreed that the conversation had become
too far ranging:
Even if we have shifted away from discussing human networks, we are getting a ﬁrst-hand
EXPERIENCE of what they are through this mailing list. No amount of “a priori” theorizing of
their nature has as much explanatory power as personal experience. By observing what happens
when connectivity is provided to a large mass of people in which they can FREELY voice their
ideas, doubts, and opinions, a lot of insight is obtained into very important issues of mass inter‐
communication. The fact that such dissimilar [sic] topics as antimatter, left-handed sugar, the
telephone network, etc. have been discussed in our own instance of a human network says a lot
about its nature and the interests and nature of its members and should not be considered as
detracting from the quality of the discussion.
The discussion about what to discuss would continue as the list grew in size. As seen
in Fig. 1, Volume 3 had 122 email messages and Volume 5 had 176 emails. This was
due to the fact that Volume 3 was only partially complete. However, what is interesting
is the increased number of emails in the “Help” and “Human/Computer Interaction”
categories. Meta discussions were the least numerous but a qualitative looks reveals that
they were of historical signiﬁcance.
The conversation later often ranged into a discussion about what was acceptable use
for a computer network. For instance, in early 1985 a dilemma regarding STARGATE,
a satellite transmission service for USENET, boiled over onto HUMAN-NETS. A
message describing STARGATE was originally written by a user of USENET,
describing the plan to save transmission costs by hiring a satellite to gather messages
for USENET and rebroadcast them directly to local networks, replacing the current
store-and-forward method. However, as the proposal evolved, network administrators
suggested that the messages should be moderated before they were sent to the satellite
for rebroadcast, eliminating the “garbage,” causing a change in the free-wheeling
conversation that USENET users had become accustomed to. Because he thought that
it would spark a useful conversation on HUMAN-NETS, an ARPANet user forwarded
the message. In a period of three months, ﬁfteen messages were sent, in a period where
the total volume of the list was about ﬁfteen messages a month. The conversation, which
of course was sent back to USENET in broadcast-only mode, reveals the depth of
awareness that users had about the power of electronic messaging conversation and the
reasons why this proposal would be unfavourable, while at the same time maintaining
a thoughtful awareness of the issue of liability that were inherent to a broadcaster, like
a television network, as opposed to a common carrier, such as a train or telephone
service.
The increase in emails asking for help reﬂects the inﬂux of new users between 1981
and 1982. The content of the discussion came to be more software and programming
130
C. Leslie

oriented rather than hardware oriented. The increase in emails discussing human and
computer interaction can be accredited to growing interest in videogames as well as
attempts to create more ergonomic designs. In fact, the number of emails discussing
videogames increased signiﬁcantly in Volume 5. One of the more interesting emails
discussed the idea of sex in videogames and inventing a real-life sex doll. Ironically, the
email ended in “I hope no one takes this seriously.”
Another prevalent assumption by the mailing lists moderators was that advertising
for a private company should not be allowed on the mailing lists. Surprisingly, one of
the army personnel cleared up the issue in 1986. Wrote Will Martin from the US Army
Materiel Command Automated Logistics Management Systems Activity:
If you are putting out some product info for altruistic reasons, for the good of others or to warn
them away from trouble, and have no personal interest in the success or failure of the vendor
that oﬀers that product, you can post *anything*. That includes prices, specs, sources of supply,
evaluations, rumors, what-have-you.
The biggest numerical change from Vol. 3 to Vol. 5 was the increasing discussions
about human and computer interactions. These discussions ranged from video games to
using computers to weave. There were also quite a few requests for help. However,
unlike Vol. 3, most people had questions concerning software. Vol. 5 takes place in
1982, from August until December. It is quite larger and has much more content. Both
numbers for human computer interaction discussions and social concerns discussions
rose signiﬁcantly. There was still a large amount of discussions about computers and
such, but it did not scale up. The majority of the technological discussion was about the
Fig. 1. Types of discussion on Human Nets in 1981 (Volume 1) and 1983 (Volume 3)
Flame Wars on Worldnet: Early Constructions of the International User
131

World-Net, about how it should be implemented and the cost of doing so. The human
computer interaction discussions were centred on video games, working at home, getting
people to use computers and weaving. The social concerns discussions were mostly
about censorship on TV.
The possibility of networks coming together to form a global network was on the
minds of many users at the time. The discussion focused mostly on the cost analysis,
prospective line-layer, and the protocols. One of the arguments was about whether or
not the network should be owned by one company, namely Bell. The argument was that
Bell had the best service at the time, but doing so could exclude the rest of the world.
TCP/IP was already being used to connect the existing networks and the whole system
was working without AT&T already. It was also argued that one company should not
have the power to shut down a communication network. Notably, the idea of World-Net
had been already explored in the novella by Vernor Vinge, True Names [22]. In the story,
written in 1980, a megalomaniac takes over the world’s interconnected bulletin board
system.
The discussions about video games focused mainly on which games were available
and how to access them. The most interesting part about the video game conversation
was about the banning of Atari adult video games. Ironically, someone pointed out the
possibility of selling sexware/sexbots. At the other end of the spectrum, one suggests
using video games in a nursing home as entertainment, to make the elderly less miser‐
able. A more serious discussion was on working at home via a computer. There were a
few who were already doing so and they were discussing the pros and cons of it. This
brought up the problem of using the sole phone line for online connection. There were
those who relied on call waiting and suﬀered, and then there were those who paid for
four lines. The discussion on how to get non-technical people to start using computers
generated both good and bad ideas, from using video games to taking away oﬃce equip‐
ment. The discussion on using computers in weaving did not seem to hold much signif‐
icance. It was simply a discussion between a few hobbyists exchanging miscellaneous
information about weaving history and the technical part of using computers to design
and make weaves.
The discussion on censorship in TV also continued for a while. Apart from the typical
arguments and statements, the majority of the emails seemed to be against censorship.
This seems to be a continuing trend on the mailing lists. On the issue of ﬁrst amendment,
the users of ARPA-Net seemed to always support it. This sort of controversy pops up
over and over again, but always the users spoke for free speech. The increase in the
number of software questions could be due to a number of factors. The hardware could
have become more standardized and did not require much helping. The moderator could
have chosen to not send out a repost email. The digests were available and were already
helping out. It could also be accredited to CS gaining momentum. One of the surprising
discussions in this volume was a discussion of a professor who taught computer science.
As the network and computers became more available to the general public, the
researchers became more concerned about the effects of this new technology. They also
wanted to play games. The possibilities were endless for them. In fact, they were discus‐
sing the idea of a game with different paths and possibilities, similar to that of a modern
RPG. They did not want a powerful entity to control the new tool of communication. It was
132
C. Leslie

their playground. The censorship in video games and TV demonstrated the public’s more
conservative ideals. Although, the majority of the emails expressed disdain for censor‐
ship, the idea of using computers for sex was not taken seriously back then. The discussion
about telecommuting shows a growing interest and opportunity for work at home jobs.
However, the technology at the time was not prepared for such advancement. The require‐
ment for multiple phone lines for the sake of convenience was too expensive for a lot of
people. But because this was brought up, it certainly would have informed many of the
benefits and the difficulties of working at home.
Interestingly, the announcements were not as administrative in Vol. 5. There were
several calls for papers and thesis, indicating a reliance of the net for academia. The
announcement for a conference, indicating drunkenness and questionable behaviour,
was rather surprising. But this demonstrates a growing trend toward more public and
social usage. In simply a year’s amount of time, Human-Nets had evolved toward a more
social oriented nature. But with the diversiﬁcation of topics, it will have to divide up to
accommodate the increasing number of people using the net, discussing more and more
irrelevant topics.
4
From ARPANet to CSNET
This background of ARPANet user aspirations is an important precursor to under‐
standing the international push in the development of CSNET. Although it was short-
lived, CSNET oﬀers Internet history some interesting examples of how networking
environments were created to suit an international environment. The boisterous, colle‐
gial community witnessed on HumanNets became the envy of those who were not
working on military contracts and, therefore, were not granted access. CSNET was an
eﬀort to bring the public sphere developed accidentally on ARPANet to the academic
community.
Several innovations – the idea of a logical, autonomous network, the nameserver
service, and also the important experience for the NSF – can be tied to CSNET.
Another important example offered by CSNET is its networking that was conceived
as an autonomous network that would connect to ARPANet. Although one could
argue that these autonomous networks was what led to the development of a rich
environment for networking, it will be of course the autonomy of these networks that
will facilitate Internet filtering in the years following. Landweber’s proposal was
made shortly before the “Great Switch” to TCP/IP and the establishment of DNS in
1983, and so CSNET is an example of the philosophy of independent networks being
connected by a single protocol. As noted by David Clark [23] in his essay on the
development of Internet protocols, TCP/IP was an effort to join “separately admin‐
istered” networks with a common protocol. In this way, CSNET is thought of as a
“logical network” that was composed of several different physical networks [24] and
is an example of what the Internet is like today; software was used to create a seam‐
less experience for the user so that he or she does not recognize that information is
traversing separate physical networks. The gateway between CSNET and ARPANet
would be enabled by the newly-developed internetworking protocol developed by
Flame Wars on Worldnet: Early Constructions of the International User
133

Vinton Cerf and Robert Kahn, TCP/IP. The central place CSNET had in his conver‐
sation is reflected by the specification of DNS described in RFC 883 (Nov. 1983),
where CSNET is one of three domains, along with .ARPA and .DDN. In this way,
CSNET is an early example of an autonomous network connected by a gateway.
Table 1. CSNET site list
Service
Domain
Description
Internet
+ean.ubc.ca
U. of British Columbia - Vancouver, BC, Canada. Gateway
to CDNNET and EAN networks
Internet
nss.cs.ucl.ac.uk
University College - London, England, UK. Gateway to
JANET (ac.uk domain) and PSS
PhoneNet
munnari.oz.au
U. of Melbourne - Parkville, Victoria, Australia. Gateway to
“oz” (oz.au domain), other hosts
Phonenet
chalmers.se
Chalmers U. of Technology - Gothenburg, Sweden. Gateway
to SUNET (Swedish U. Network)
Phonenet
zix.gmd.dbp.de
Deutsches Forschungsnetz – Berlin. Federal Republic of
Germany, Gateway to DFN network
Phonenet
hut.ﬁ
Helsinki U. of Technology - Helsinki, Finland. Gateway to
FUNET (Finnish U. network)
Phonenet
inria.inria.fr
INRIA - Rocquencourt, Le Chesnay CEDEX, France.
Gateway to COSAC (French National Network)
Phonenet
ira.uka.de
U. of Karlsruhe – Karlsruhe. Federal Republic of Germany,
Gateway to DFN network
Phonenet
+utokyo-relay#
U. of Tokyo - Tokyo, Japan. Gateway to JUNET (Japanese U.
Network)
Phonenet
sorak.kaist.ac.kr
Korea Advanced Inst. of Science and Technology - Seoul,
Republic of Korea, Gateway to SDN network
Phonenet
waikato.ac.nz
Waikato U. - Hamilton, New Zealand
Phonenet
iﬁ.ethz.ch
ETH-Zentrum - Zurich, Switzerland. Gateway to CHUNET
(Swiss U. Network)
X.25 dialup
huji.ac.il
Hebrew U. - Jerusalem, Israel. Gateway to ILAN network
Another aspect of this story that is interesting is the freedom and authority it gave
to Landweber himself. Many researchers on the international scene credit their start in
networking by attending a seminar with Landweber, a series that came to be known as
“Larry’s Networkshops.” The ﬁrst seminar was held in London in 1982, where Germany
described its research network, active projects in Norway and Sweden were described,
the SERCNet Coloured Book protocols from the United Kingdom were presented, and
researchers from CERN described what would become HEPNet [25]. As this list
suggests, networking was already an international phenomenon – what was missing was
a mechanism to bring these diﬀerent networks together. At the second meeting of Larry’s
Networkshop in 1983, Landweber announced that CSNET would become international.
Israel got a PhoneNet connection in 1984, followed by Korea and Canada. Australia,
France, and Germany join later. Soon, Israel, Korea and Japan get full IP access. The
1987 meeting in Princeton brought together the top 100 international networking
134
C. Leslie

researchers. A CSNET site list [26] distributed in 1988 (Table 1) gives a glimmer of
understanding of how CSNET was able to bring together international researchers by
connecting their local networks.
As an example of what this looked like on the ground, Germany was establishing its
own computer networks concurrently with these developments. Although Germany was
not connected to CSNET until 1984, the beginning of international connections came
to Germany in the form of a DATEX-P connection from the University of Cologne to
Amsterdam as part of the European EARN network. This connection was based on a
dedicated line and was fairly expensive to use, but from this connection it was possible
to send and receive electronic messages through the CSNET gateway at the University
of Wisconsin. Having a connection was just a small aspect of the Internet, but one could
receive RFCs about TCP, IP, telnet, FTP, ﬁnd instructions on how these new protocols
could be implemented on a variety of services, and print them out in a computer
centre [27].
In 1984, with Germany having already seven years’ experience with networking
computers but little connectivity to international networks, Werner Zorn, the chair of
computer science at Karlsruhe University in the southern part of West Germany, articu‐
lated the need for Germany to establish international connections. On August 2, 1984,
Zorn and a colleague at the University of Karlsruhe sent what has come to be known as
the ﬁrst e-mail from Germany, a reply to the message from Landweber at the University
of Wisconsin welcoming him to CSNET [28]. West Germany was the only the fourth
country to join CSNET, following Canada, Sweden, and Israel. Karlsruhe became the
node for messages entering and leaving Germany, quickly adding connections to univer‐
sities such as Fraunhofer and the Max Planck Institute, as well as members of the busi‐
ness community with connections to Siemens and BASF. Through this connection it
was much easier for computer scientists and engineers to communicate with their inter‐
national counterparts as well as access the growing amount of information that was
available online. It became clear that the demand in Germany for international access
was high, and in 1985 a research association was created, Deutsche Forschungsnetz [29].
Thus, Table 1 lists Berlin’s DFN as the location for the German connection to CSNET
in 1988.
Missing from Table 1, however, is the connection that Germany makes with China,
and this tells an important aspect of the internationalization of the Internet. The tech‐
nology behind the connections was made by scientists seeking to help others; individuals
had to make the world ready for the Internet before the Internet was ready for the world.
Qiheng Hu, chairwoman of Internet Society of China and the Chinese Academy of
Engineering (CAE), points out [30] that the ﬁrst TCP/IP connection was established
long after national networking in China had begun. In a presentation to honour the 20th
anniversary of the ﬁrst e-mail sent from China, she detailed the extent of networking
experiments, not to diminish the German eﬀort, but so that her audience has “the feeling
about the hungry and thirsty status, especially from the science and technology
community in China.” In March 1980, the Chinese Academy of Construction Research
created an online international retrieval terminal established in Hong Kong, which
provided trial retrieval services to Chinese institutions. In December 1981, the Infor‐
mation Institute for Computer Applications set up an international online terminal
Flame Wars on Worldnet: Early Constructions of the International User
135

through fax lines in Beijing using leased satellite lines through ARPANet to connect to
the DIALOG database, an information retrieval and bibliographic database now owned
by ProQuest. In October 1983 the Institute of Information of China (now Institute of
Science and Technology of Information of China) connected to the European Space
Agency Information Retrieval System via satellite link and connected through Italy
Public Data Network to the US Public Data Network. From there, China was connected
to twelve information systems of diﬀerent countries. On July 1, 1984, the Institute of
High Energy Physics set up a time-sharing terminal with microwave transmission with
the M-160 at the China Institute of Water Resources and Hydropower Research (IWHR),
a distant terminal of the M-160 because the IHEP did not have computer power for large-
scale modelling. The extent to which China’s computers could access international
information as described by Hu is striking; by 1985, China had over 50 international
online retrieval terminals set up. These were all dedicated connections to proprietary
services, however.
Given the already strong setup of Chinese computer networks and the prevalence of
international connections, it may seem strange that Germany is credited with connecting
China to the international computer networks so that it could send a mail message in
1987. (As noted by Jay Hauben [31], Zorn is not always given credit for this develop‐
ment.) Although the choice of Germany as a partner may not seem obvious, it makes
sense in the context of Cold-War prohibitions on the export of high technology to China
from the United States. The genesis of the project was when 18 universities received
Siemens computers in a project sponsored by the World Bank [32, 33]. To assist China’s
use of its computers, the Scientiﬁc Siemens User Group had sent 15 people from
Germany to China for the WASCO Symposium on September 12, 1983. The German
researchers presented trends of the future of networking and practical questions of how
to set up computers to make networks happen. Werner Zorn, the computer science
professor at the University of Karlsruhe who connected Germany to CSNET, made a
presentation on the way in which OSI protocols could be used to operate a network in
China. After the symposium, Zorn’s new CSNET connection to the United States made
it painfully clear to Zorn how diﬃcult it was to communicate with China. Planning for
the second WASCO symposium for the fall of 1985 meant that one had to allow for a
14-day turnaround for all communication to China, because telephone and telex were
so expensive. It was clear that an Internet link would improve matters greatly, but there
were two diﬃculties: one being money to put it together, the other being the U.S. ban
on exports of technology to China. The ban on exports did not aﬀect Germany directly,
but there was a concern that should Germany help China develop Internet connections,
it might disturb the U.S. government.
Money for the German side of the link to China came from the prime minister of the
German state Baden-Württemberg that same year [33]. Born decided that the best way
to avoid the appearance of exporting U.S. technology to China was to set up a physically
separate link from Germany to China with German equipment and software, and then
connect China to CSNET in Germany. For this purpose, Zorn obtained a μVAX II and
set it up using Unix 4.2, a good choice because UNIX was popular in China and it made
possible a Unix to Unix Copy (UUCP) connection, which is the ﬁle transfer protocol
behind USENET. It was not possible to obtain funds for a similar computer on the
136
C. Leslie

Chinese side, so Zorn decided to follow an alternate plan and set up a dialup connection
from China to Karlsruhe. He arranged a two-week lecture and project trip for May 1986
and took the equipment needed for the connection – basically a modem and software –
to China as excess baggage [32].
Setting up a dialup link from China was not as easy as it may sound. Telephone calls
in China were, for the most part, locally connected and sometimes a caller had to wait
as long as an hour for a line. What Zorn needed was a permanent line from the computer
centre to the local phone exchange, but this was not likely to happen in an environment
where an entire residential area had access to only one phone line. It did not seem like
a dialup connection was going to work, but then he learned that there was an X.25
network connection to a nearby university that connected via satellite to Italy. Using X.
25 as the lower-level protocol, Zorn was able to set up a connection back to Germany;
the idea of sending TCP/IP packets over X.25 connections, of course, had already been
established for middle-tier CSNET users and he had implemented this solution on
Siemens computers in Germany. With this connection established, messages for China
would be held at the Karlsruhe computer and collected remotely from China at periodic
intervals [32].
In setting up this link, Zorn suggests that from the technical side he did very little.
The colleagues at ITALCABLE (the Italian network connection) had already done the
real work. Even though Karlsruhe received a lot of publicity for the connection, “our
contribution actually lay in being fortunate enough to ﬁnd and pave a way via the
diﬀerent entities involved” [32]. However, from the perspective of the history of the
Internet, Zorn’s contribution was quite essential. In spite of the many international data
connections that were established in China, there had not yet been this kind of ﬂexible
connection. Proprietary systems using their own protocols were set up, but these only
allowed for speciﬁc networking solutions. The end-to-end resource established by TCP/
IP was missing.
In the summer of 1987, while preparing for the next symposium in Beijing, Zorn
asked Larry Landweber for permission to connect China – in particular, to install the
BS2000 software they had used on their own Siemens computer to connect to CSNET.
“Larry’s view on this was totally positive,” and Zorn had his permission to go ahead [32].
At a symposium to celebrate the twentieth anniversary of the ﬁrst e-mail from China,
however, Landweber told Zorn a slightly diﬀerent story. Landweber notiﬁed Zorn
immediately when permission was given from the National Science Foundation, but the
next day he was notiﬁed that the connection had been denied by the White House and
connection should be turned oﬀ. Thinking of these events, Landweber characterizes the
philosophy of Stephen Wolﬀ, who was director of the U.S. National Science Foundation
Division of Networking and Communications Research, which was “you don’t ask
permission in advance. You ask for forgiveness later” [34]. The academics decided that
they would continue with the project; as noted by Jay Hauben, the oﬃcial approval for
this connection was granted retroactively in November [35].
The “First Electronic Mail from China to Germany” was sent on Monday, 14 Sep
87 in German and English by Michael Finken, the graduate student Zorn had left behind
to ﬁnish the installation. In the same way that the connection to international computer
networks inspired Germany to strengthen its connection to the Internet, computer
Flame Wars on Worldnet: Early Constructions of the International User
137

science researchers in China were motivated by the connection to CSNET and founded
the Chinese CANET in March of the next year. Due to this personal eﬀort, China’s
national network was connected even earlier than Canada’s.
5
Conclusion
Although one might be accustomed to thinking about the diﬀusion of the Internet –
especially across international boundaries – as one of a good idea catching like wildﬁre,
the preceding analysis has shown that the diﬀusion resulted from a pull as much as a
push. What is more, the dream of the international connections possible by the Internet
predate even the implementation of TCP/IP itself.
In his analysis of USENET, Michael Hauben has pointed out that USENET was a
reliable network. By this, he did not mean that its protocols or hardware were more
robust than other possibilities; he says that USENET existed and survived due to the
determination of its users. It was strong because it was “a peer to peer network” that
individuals looked after [36]. As I hope this paper has shown, the earliest protocols of
the Internet were developed and supported by a similar mind-set. It was only by the
cosmopolitan outlook of individual researchers that these connections were made. This
is not a side example; Bernard Aboba has pointed out that the Internet was international
from the start [37]. In spite of the way the history of the Internet is frequently told,
CSNET, nor NSFNET, did not “cause” international networking and nor was it the case
that countries outside the United States were ignoring the possibilities the computer
networking could provide. In fact, quite the opposite is true. International computer
networks existed alongside the development of TCP/IP, and in fact it was an interna‐
tional outlook that led to the ﬂexibility of the protocol, as noted by Clark.
When the story told about the Internet is one of technical and administrative inno‐
vation, it can safely be elided from the history of the Internet. What has been missing is
a sense of why Internet advocates were interested in creating this diverse network that
was not deﬁned by participation in military research. The international identity of its
innovators explains why these early innovations were important beyond their technical
innovations: the idea of international cooperation among the designers and users of the
early Internet was essential in deploying a ﬂexible network that could provide worldwide
connectivity. By looking at the interests and ideas of the users and advocates of the
Internet, one can ﬁnd user communities that deﬁne themselves as members of a
community of diverse networks, an attitude that would be a marker of the eventual
success of the Internet.
Acknowledgement. 
Grateful acknowledgement is made to Aye Muang who, as an
undergraduate research student, conducted the evaluation of Human-Nets. He and the author are
indebted to the New York University Tandon School of Engineering Undergraduate Summer
Research Program and its founder, Associate Dean Iraj M. Kalkhoran, which provided Aye a
stipend that supported this research.
138
C. Leslie

References
1. Man, J.: Gutenberg: How One Man Remade the World with Words. Wiley, New York (2002)
2. Hafner, K., Lyon, M.: Where Wizards Stay Up Late: The Origins of the Internet. Touchstone,
New York (1996)
3. Brunton, F.: Spam: A Shadow History of the Internet. MIT Press, Cambridge (2015)
4. Warrick, P.S.: The Cybernetic Imagination in Science Fiction. MIT Press, Cambridge (1980)
5. Abelson, A.: Up & down wall street. Barron’s Natl. Bus. Financ. Wkly. 1, 33–34 (1979)
6. The Months Ahead. Changing Times 32.6 (June): 5 (1978)
7. McLean, G.N.: Curriculum implications of the oﬃce of the future: or, projected implications
— more (or less) accurate. J. Bus. Educ. 56(7), 268–271 (1981)
8. Abbate, J.: Inventing the Internet. MIT Press, Cambridge (1999)
9. Shapin, S., Schaﬀer, S.: Leviathan and the Air-Pump: Hobbes, Boyle and the Experimental
Life. Princeton University Press, Princeton (1985)
10. Oudshoorn, N., Pinch, T. (eds.): How Users Matter: The Co-Construction of Users and
Technology. MIT Press, Cambridge (2003)
11. Lundin, P.: Computers in Swedish Society: Documenting Early Use and Trends. Springer,
New York (2012)
12. Landweber, L.H.: CSNET, the Computer Science Research Network: Proposal to the National
Science Foundation (1980). http://world.std.com/obi/CSNET/. Accessed 2 Oct 2012
13. McKenzie, A.A., Walden, D.C.: ARPANet, the defense data network, and internet. In:
Froehlich, F.E., Kent, A., Hall, C.M. (eds.) The Froehlich/Kent Encyclopedia of
Telecommunications. Marcel Dekker, New York (1991)
14. CSNET: Project Status Report (1982). http://world.std.com/obi/CSNET/
15. Internet Society: Trailblazing CSNET Network Receives 2009 Jonathan B. Postel Service
Award (2009). https://www.internetsociety.org/news/trailblazing-csnet-network-receives-
2009-jonathan-b-postel-service-award. Accessed 12 Apr 2016
16. Hauben, R.: ARPAnet. http://www.columbia.edu/~hauben/CS/arpanet-encyc.txt. Accessed
12 Apr 2016
17. Griﬃths, R.T.: Chapter 3: History of Electronic Mail (2002). http://www.let.leidenuniv.nl/
history/ivh/chap3.htm. Accessed 12 Apr 2016
18. Lynch, K.: Re: History of the Net is Important (1994). http://keithlynch.net/history.net.html.
Accessed 12 Apr 2016
19. Wikibooks: The Computer Revolution/The Internet Revolution (2012). http://en.wikibooks.org/
wiki/The_Computer_Revolution/The_Internet_Revolution. Accessed 16 Apr 2016
20. Archives of Human-Nets are available online from various websites. http://
www.cs.rutgers.edu/~cwm/NetStuﬀ/Human-Nets/. Accessed 2 Oct 2012
21. Earnest, L.: A look back at an oﬃce of the future. In: Fick, G., Sprague, R.H. (eds.) Decision
Support Systems: Issues and Challenges. Pergamon Press, New York (1980). http://
www.iiasa.ac.at/Publications/Documents/XB-80-512.pdf. Accessed 16 Apr 2016
22. Frenkel, J. (ed.): True Names and the Opening of the Cyberspace Frontier. Tor, New York
(2001)
23. Clark, D.D.: The design philosophy of the DARPA internet protocols. In: Proceedings of
SIGCOMM 88 Conference on Communications Architectures and Protocols, pp. 106–114
(1988)
24. Comer, D.: The computer science research network CSNET: a history and status report.
Commun. ACM 26(10), 747–753 (1983)
25. Malamud, C.: Exploring the Internet: A Technical Travelogue. Prentice Hall, Englewood
Cliﬀs (1993)
Flame Wars on Worldnet: Early Constructions of the International User
139

26. CIC: CSNET Site List (1988). http://www.mit.edu/afs.new/athena/reference/net-directory/
host-tables/csnet.site-list. Accessed 2 Oct 2012
27. Netplanet.org.: Das Internet in Deutschland (1998). http://www.netplanet.org/geschichte/
deutschland.shtml. Accessed 2 Oct 2012
28. Welle, D.: Germany Celebrates 20 Years of e-mail (2004). http://www.dw.com/en/germany-
celebrates-20-years-of-e-mail/a-1284655. Accessed 12 Apr 2016
29. Kalle, C.: Das Internet in Deutschland - Ein alter Hut? Kompass 64, 18 July 1995. http://
mikro-berlin.org/Events/OS/ref-texte/kalle/0InetinDland.html. Accessed 2 Oct 2012
30. Hu, Q.: Growth of the Internet in China since 1987. Deutschland und China –
Innovationspartner in der Informationstechnologie (2007). http://www.tele-task.de/archive/
video/real/1412/. Accessed 2 Oct 2012
31. Hauben, J.: The story of China’s ﬁrst email link and how it got corrected. In: Address at the
International Conference on Media Education and Global Agendas (2010). https://
www.informatik.kit.edu/downloads/HaubenJay-ChongqingSpeech-12Jan2010.pdf.
Accessed 2 Oct 2012
32. Zorn, W.: How China was connected to the international computer networks. Amateur
Computerist Newsletter 15.2 (2007). http://www.ais.org/~jrh/acn/acn15-2.articles/zorn.pdf.
Accessed 2 Oct 2012
33. Zorn, W.: “China’s CSNET Connection 1987 – origin of the China Academic Network
CANET.” 
Asia 
Internet 
History 
Project 
(2012). 
https://sites.google.com/site/
internethistoryasia/book1/personal-essay-werner-zorn. Accessed 2 Oct 2012
34. Jennings, D.: Panel discussion: the impact of the ﬁrst E-mail. Deutschland und China –
Innovationspartner in der Informationstechnologie (2007). http://www.tele-task.de/archive/
video/real/1412/ Accessed 2 Oct 2012
35. Hauben, J.: “ ‘Across the Great Wall’: The China-Germany E-mail connection 1987–1994.”
The Amateur Computerist 13.1 (Spring), pp. 25–29 (2005)
36. Hauben, M., Hauben, R.: The Social Forces Behind the Development of USENET. First
Monday 3.7 (1998). http://ﬁrstmonday.org/ojs/index.php/fm/article/view/609/530. Accessed
12 April 2016
37. Aboba, B.: “How the Internet Came to Be, Told by Vinton Cerf.” The Online User’s
Encyclopedia. Addison-Wesley (1993). www.virtualschool.edu. Accessed 2 Oct 2012
140
C. Leslie

The Code of Banking: Software as the Digitalization
of German Savings Banks
Martin Schmitt
(✉)
Centre for Contemporary History Potsdam, Potsdam, Germany
schmitt@zzf-potsdam.de
Abstract. To the present day the history of banking software is nearly untold.
While there is already some literature on the use of computers in the banking
industry, most of it focuses only on the hardware and its restrictions (cf. Cortada
2006). The logic behind these machines remains untold. With the advent of the
computer as a universal machine since the 1950s, business processes have been
written into code, not hard wired into the machine. Furthermore, not the processor
but the system software steered what was presented on the screen to the banking
employee. Hardware got more and more exchangeable, while the real guiding
principles of computing in action are to be found in software. This article analyzes
how German savings banks used software to digitalize their business during the
period of the Cold War.
Keywords: History of computing · software studies · Digitalization · Financial
technology · Economic history · Savings banks · Germany · GDR
1
Software and the Contemporary History of Computing
“But the computer is an all-purpose machine, and the computer display - a screen programmed
to present text and pictures somehow stored in the computer - is a universal miraculous commu‐
nication tool”, Ted Nelson [1, p. 17].
“It is the history of computer software and not of the computer itself that is at the
heart of the larger story of the great computer revolution of the mid- to late twentieth
century” emphasizes the historian Nathan Ensmenger in his recent study about program‐
mers and computer users in the Digital Age [2, p. 6]. Especially in the banking sector
the importance of software in the digital (r)evolution1 is striking, but nearly untold.
Banks were not only early user of computers, but also of software services as the imple‐
mentation of computers into their business processes. Through the lens of Software, this
article aims to show how the liaison of computing and a ﬁnancially powerful industry
created a vast potential for change in the way banking was conducted in the 1960s and
70s, as well as the pitfalls of this development. With the example of East and West
1 The r in brackets is hinting to the double nature of computerization being perceived as some‐
thing revolutionary – especially in the press – while the actual change on the small scale is far
more evolutionary.
© IFIP International Federation for Information Processing 2016
Published by Springer International Publishing AG 2016. All Rights Reserved
A. Tatnall and C. Leslie (Eds.): HC 2016, IFIP AICT 491, pp. 141–164, 2016.
DOI: 10.1007/978-3-319-49463-0_10

German savings banks, which had the largest customer-base of all banks throughout
Europe, I am stressing the question: How was “the bank” represented in its software?
Who wrote the code of banking, the programs, and the documentations? Following these
central questions, it is interesting if there had been a speciﬁc socialist or capitalist way
of deploying computers within banks and if one can distinguish between a capitalist and
a socialist (banking) software. The comparison of the software uses and ways of produc‐
tion in East and West Germany presents a unique insight. Little is known, for instance,
about the extent to which Eastern German banking oﬃcials had access to the techno‐
logical and software developments made in West Germany. In fact, West German soft‐
ware products were copied and adapted to the needs of socialist banking. In this paper
I’m showing in which way international knowledge transfer shaped the way banks where
digitalized. It is critical how comparable institutions and their respective employees in
diﬀerent economic systems dealt with the new ways of intra-active data procession and
the pitfalls of software production. That would have had an impact on how banking was
represented inside the machine. After a short summary on what software actually was
and who produced it for the banks, three use cases follow demonstrating the inﬂuence
of software on the whole banking process.
The aim is not to replace a hardware perspective by looking only at software, but
rather to place it in a Deleuzian assemblage. This assemblage contains technology, its
users, the code and its execution in a broader understanding of the term software.
Following Ensmenger and Haigh, the question how to deﬁne “the computer” in respect
to software is striking. Since the 1970s there was a long debate about what a computer
actually is. Especially the touring completeness of the computer, the question, what
“stored-program” and “digital” meant for whom and the particular national interests
made it so hard to decide [3]. A touring complete machine is a machine that can solve
every solvable program given inﬁnite time. That constitutes the universal character of
every computer to possibly simulate every other machine of the past, present and future.
Among others, this is the key feature of a computer that constitutes it as a general-
purpose machine. The German media theorist Friedrich Kittler emphasized this force‐
fully in his media archaeology by deﬁning the computer only by his programmability
[4]. Not until the act of running a program, the general-purpose machine becomes
speciﬁed, concrete and useful for the computer user. Thereby, it does not matter if the
computer is analog, digital or even virtual. The software has the power to transform a
super-fast calculating machine into every desired machine, be it a text automaton, a
business machine, a passbook or a drawing device. It hints to the question how a
computer was actually used by the historical actors. At the end of the 1980s, the founder
of historical software studies, Michael S. Mahoney, already pointed out that a computer
without software is as useless as a car without gasoline [5]. Software had become the
intra-face2 between the computer and society in the course of computerization. Comput‐
erization I understand as the fundamental process of the widespread diﬀusion of digital
2 The term intraface is used here analog to the term intra-action. It should point to the under‐
standing in the Latour sense that the division of men and machine or in a higher sense between
men and nature is non-existent. Men are always part of their environment and therefore are
intra-acting with it. Cf. [6].
142
M. Schmitt

information and communication technology and its penetration of nearly every aspect
of life during the second half of the 20th Century [7–8, p. 227]. In contrast to this
deﬁnition I understand digitalization as the electronic representation of objects in code,
even though both terms are frequently used as synonyms nowadays [9, pp. 33–34]. Only
through software the computer became the primary window to understand the world,
the means for nearly every human to perceive, interpret and to express the world.
Through software, the computer became the embodiment of the highest political,
cultural and economic aspirations, as well as of regular daily business. And so
Ensmenger concludes:
“when people talk about the larger process of the computerization of modern society, or speak
of the computer revolution transforming the ways in which they work, live, consume, recreate,
and engage in social and personal relationships, they are really talking about the history of
software” [2].
But thinking of software as a product is misleading for analyzing the process of
computerization. Software was far more than just an executable program that you buy
on a data carrier or download like you purchase Microsoft Word for having a custom‐
izable word processor.3 Therefore a conceptual history is necessary to understand the
diﬀerent layers of meaning that are encapsulated in the term software. At the same time,
software history already tells a lot about the recent historical developments in a time of
ongoing commodiﬁcation.
1.1
A Conceptual History of Software
In the late 1950s and the 1960s, computer people – experts like programmers, system
analysts, technicians or administrators – understood software in two different ways. On the
one hand they understood it in a narrow sense describing programs to develop other
programs and to control the hardware. On the other hand they understood it mainly as a
strand of systems, services and support [10, pp. 5–6]. Programs were seen as universal
mathematical solutions that nobody could own. Consequently, programs had no name.
Software in the early period of computerization combined whole systems of machines,
their instructions, the organization, programmers, users and processes. Therefore, soft‐
ware can be methodologically described as a Deleuzian assemblage combining inde‐
pendent elements within a temporal [11] liaison. The term software was first mentioned
by the statistician John Tukey in 1958 as everything beyond the tubes, transistors and
cables. The Oxford English Dictionary is enlisting the first use of the term for the year
1960 in the journal Communication of the ACM, whereby the author is describing COBOL
as “software” [11, p. 69]. Its semantic change from a narrow to a broader, system oriented
meaning bears to the height of system theory in the late 1960s. To give a concrete
example: If a savings bank wanted to digitalize its transactions, the bank not only had to
write millions of lines of code or to buy a fancy product. It had to analyze its business
processes up to the single steps of action and had to fit the computer into it – or adapt the
processes to the machine. The bank had to reorganize its personnel and its organizational
3 Even though buying something on a data carrier already is outdated, too.
The Code of Banking: Software as the Digitalization
143

structure, it had to train the users, buy the peripherals and write new instructions, manuals
and documentations. In a nutshell: the result of the computerization was software.
For a bank, software in the 1950s and 1960 was more like hiring a consultancy than
buying a product [12]. But the consultancies of choice in most of the cases were the
huge hardware producers who oﬀered software as a service together with their machines.
Sales persons at IBM not only needed to understand the machines they were selling.
They also had to analyze and understand the way their customers worked and had to
oﬀer solutions of how to optimize it. Seldom was this an easy task as it meant change
to established hierarchies. Therefore, it is important to tell the story of the computeri‐
zation of Germany not only as a success story of rapid diﬀusion of a superior technology.
It was a conﬂictive process, in which software can be described as a Deleuzian assem‐
blage per excellence. Its heterogeneous parts were ﬁghting for power, sovereignty and
authority [2, p. 7]. Whilst Nathan Ensmenger and Rob Kling are focusing on the new
power of the programmer and the IT-departments within organizations, the historian of
technology David Gugerli argues that standardization and strict rules of usage, language
and education limited the creative freedom of the programmers severely [8, 13].
In this respect it is striking to look at the computerization of the former GDR and
their socialist rule of language with a wider understanding of the term software.
Computer people in the GDR were talking not about software but about “system docu‐
ments” (“Systemunterlagen”) for a long time. The diﬀerent denomination corresponded
with the maxim of the party elite to use the German terms rather than the English ones
in a battle against a perceived imperialist American inﬂuence through technology and
language.4 But at the same time they also distinguished themselves from the Russian
usage of matematicheskoe obespechenie for Software, which meant as much as math‐
ematical supply.5 Systemunterlagen catches the initial understanding of computer usage
in the 1960s much better than the term software, which is buried under the diﬀerent
layers of meaning that software carries today and that is bound to a binary separation of
hard and soft. System not only meant the actual computer. Rather it described in a
cybernetic way the whole environment of the computer, for example the division that
ran it.
Not until the middle of the 1970s did the term “Soft-Ware” become established in
the GDR written with a hyphen analog to the term “on-line”. According to Simon Donig
this hints to the fact that completely new concepts like software as a product, the micro‐
chip or the wafer had been adapted by the engineers, whilst they kept well-established
concepts like tubes (“Röhren”), interface (“Schnittstelle”) or input-output device
(Eingabe-/Ausgabegerät) in their mother language [15, p. 93]. And even in 1981, the
lemma “software” in the encyclopedia of economy of the GDR simply linked to
“Systemunterlagen” which shows the persistence in the socialist use of language [16].
This is also probably due to the relatively strict embargos for high technology during
the height of the Cold War that forced the GDR to self-production and also prevented a
language import.
4 Up to the 1980s, “Computer” in the GDR were called “Rechner” or “elektronische Rechen‐
technik”, “Hardware” was called “Gerätetechnik”. On socialist language cf. [14, p. 86].
5 Thank you Ksenia Tatarchenko for that hint.
144
M. Schmitt

The hyphen in the later used “soft-ware” shows a development that began in the
United States in the late 1960s. The media theorist described it in her book “Programmed
Visions” as software becoming a thing, a [20] “ware” Wendy Chun. Beforehand, soft‐
ware was rather a service for the speciﬁc use of a computer within the company priced
in labor cost per instruction. With the ongoing pervasion of the computer in business in
the late 1960s a shift of meaning took place from “programing” as a verb to the noun
“program” as a category. Software more and more became a product with a ﬁxed price
and branded, an all-in-one solution that was patentable, copy-able and standardized to
be used in various diﬀerent contexts without any big change [17]. On the contrary, in
the GDR not only the way of thinking about software but also the bundling remained
mainly unchanged until the mid-1980s. Even though Europe didn’t follow the US-
example of patentability, the understanding of software as a product diﬀused with the
data carriers that fed the demanding drives of the global PCs. It is a historical irony that
while on a global scale the materialism experienced its ultimate failure with the end of
the Soviet Union and the fall of the wall, it stroke back in the sphere of the digital [18].
While Reagan stated at the Lomonosov University in Moscow 1988 in respect to digi‐
talization that “we’re breaking through the material conditions of existence to a world
where man creates his own destiny” [19] these material conditions were set in the back‐
ground. Judges in the US covering the question of software patentability in 1994 drew
on the fact that the programs changed the material state of the computer in a temporary
memory dump. This materialization of thought was then treated in the same way as every
other material product [20, pp. 4–6]. In the mid-1990s the transformation from software
as a service, to software as a product was accomplished by the interest of a grown up
software industry.
Despite the semantic closure for the protagonists the term software somehow stayed
hard to grasp. Michael S. Mahoney explains that with a hint to the active character of
software in action. For him, software is “elusively intangible. In essence, it is the behav‐
iour of the machines when running. It is what converts their architecture to action, and
it is constructed with action in mind; the programmer aims to make something happen”
[21, p. 122]. This purposeful component of software as active change makes it the perfect
category for analyzing the computerization of German banks in a historical process – a
process that is deeply coined by intra-action between technology and society. As a
running computer never could be described in total, software also stayed something
ephemeral, intricate, it was the chaotic and rhizomatic [22, pp. 3–28]. During the process
of computerization, hardware supposedly only got faster, smaller and cheaper. Moore’s
Law could be interpreted as the technological embodiment of the belief of progress that
was so symbolic for the post war period up to the mid-1970s. It was the pendant to the
West German “Law for the advancement and stability of growth in the economy”6 of
1967 or the belief in the unstoppable progress of socialism in the East. By contrast
software often reﬂected the dirty reality, broken dreams and stony paths of implemen‐
tation. Shortly after the ﬁrst commercial computers were sold, experts drew on the
undersupply of usable applications for them. Thereafter in the 1960s the costs for the
programming rose and the software grew more complex and unreliable. For Nathan
6 “Gesetz zur Förderung der Stabilität und des Wachstums der Wirtschaft (StabG)”.
The Code of Banking: Software as the Digitalization
145

Ensmenger the discussion of software crisis within a larger discourse of computerization
is an astonishing constant of the history of computing. It began with the shortage of
applications in the early stage of computerization. It continued with the complaint about
the perceived lack of programmers, the unreliability of the solutions and the exploding
costs of software production. The ﬁnal reverberation of this is the outsourcing of
programmers to lower developed countries and the recent discussions about algorithmic
produced software through “artiﬁcial” intelligence. Therefore, Ensmenger comes to the
conclusion that software history is a history of failure, abortive developments and
contradictions in contrast to a smooth progress of hardware. In short: Moore’s Law did
not apply for software. Software always had to be renegotiated. It had to be negotiated
because of the unintended consequences for the companies who drove the computeri‐
zation, not because it was “hard” as the famous computer scientist Donald Knuth stated
[23]. This is an important point, even though Ensmenger underestimates the contradic‐
tions of hardware development. Recent studies of our research project are showing that
the history of hardware and its diﬀusion was not at all smooth and gentle. Moore’s law
ﬁnally came true, but more important is the point that it inspired the way developers,
programmers and users on the whole globe thought about information technology and
what you could do with it – also in the banking sector.
1.2
Banking Software
Looking at the use of software in the banking industry, the thesis by Ensmenger of
software as a problematic factor in history has to be slightly adjusted.7 The software
story was by no means only one of failure, shortage and disappointment. First of all, the
banks as customers demanded working software as the integration of computers from
early on. Otherwise they chose another producer. Software sold hardware even if both
tended to be unreliable in the beginning. Despite the delays in implementation and the
fact that banks stuck to use computers in the very same way they used punch card tech‐
nology, the basic applications worked productively after a short period of adjustments.
Most of the time, these applications were so successful that they ran for several decades
and on diﬀerent computer generations. Often, on a new computer generation an emulated
version of the old software ran. That, in return, caused a magnitude of possible errors
and problems. Path dependency was even more the case in the GDR where the savings
banks had to struggle on the one hand side with scarcer resources and a lower importance
in the economic system. On the other hand they had to ﬁght with the error prone hardware
and the long delays by the state-owned combine Robotron. In banking, software could
also be described as the constant against a fast changing hardware – not the other way
round.
Let’s take a concrete example in the late 1960s to understand how computers were
implemented into business processes: accounting. The primary focus of the planers in
adjusting the system to the rising transaction and account numbers had not been
7 “Compared to the history of computer hardware, which is characterized by regular and remark‐
able progress, the history of software is replete with tension, conﬂict, failure, and disillusion‐
ment” [2, p. 10].
146
M. Schmitt

computers. They were more interested in measuring how the current system worked,
tried to understand it and to model its behaviour. Then they adapted this system to the
requirements of computing. Therefore, other actors entered the stage: sales people,
technicians and programmers. The initial design had then to be cast into a system archi‐
tecture. This again necessitated the collaboration with several organization experts, users
and technicians. Rooms had to be found and spatial ordering of people, machines and
working processes had to be produced. The executives had to supervise the steps and
made changes. The production of the code of banking becomes vividly graspable in the
sources of the Municipal Savings Bank of Stuttgart. IBM Germany, located in the close
proximity in Böblingen, not only sold them their machine. They analyzed exactly how
the bank worked and optimized the processes [24]. After the programs were written
using higher level programming languages or Assembler, they had to be tested and
debugged. This again could change a lot of the initial plans and needed its space and
machine time. In the GDR this scarce resource was highly contested so that often soft‐
ware was badly tested and had to be rearranged afterwards. The consecutive step then
was to complete the documentation and to write a manual for the accountancy system.
The employees that used the system had to be trained in how to use it what ranged from
the high level executives down to the regular employees in accounting. Ian Martin has
shown for Martins Bank in England, that in the beginning there was a strong continuity
in employing women as machine users. They just were transferred from the machine
room in the back-oﬃce and their work with the business machines to the computer centre
of the bank [25, p. 325]. This pattern becomes visible as well in the Savings Bank of
Stuttgart, where the punch card technology in Germany was far more widespread than
in England. The ﬁrst jobs that became obsolete in Stuttgart were those of the punch card
division, primarily a female domain [26, p. 6].
Today it somehow sounds strange but the system itself had still to be operated, as
sophisticated operation systems had not been developed, yet. The operators of the
savings bank had to keep the diﬀerent programs in the right running order, had to check
if they ran properly and had to take over or archive the results. Finally, the software
environment had to be maintained and adapted to changing usages. Altogether, this
maintenance could eat up to 2/3 of the overall costs of the software [17, p. 118]. As Karl
Ganzhorn, former developer at IBM Germany writes, the maintenance of the computing
environment occupied so many programmers in the late 1950s that they had not much
time left for new developments [27, pp. 51–58]. It is astonishing that the executive board
of the savings banks on the one hand planed with the technology over long periods of
time, more or less ten years in the beginning. The costs of computerization were calcu‐
lated meticulously. On the other hand they seldom realized the hidden costs of software
in the long run as they mainly hoped for rationalization wins. The user experiences with
human-computer intra-action through software were important for this miscalculation.
In the early years of the digitalization of banking from 1954 to 1962, banking exec‐
utives as well as employees ﬁrst had to become familiar with the new technology. They
relied heavily on the experiences they made with punch card technology and used
computers as business machines. Punch card machines had been deployed in German
Savings banks since the 1920s and were widespread in the post war period. But soon
they had to realize that computers were more than fast calculators built of sheet metal
The Code of Banking: Software as the Digitalization
147

and electron tubes. Inside the machine worked algorithms that soon became the core of
banking. At the outside, working processes changed in intra-action of the computer-
bank systems. For a longer period of time, digital processes didn’t replace analog ones
but ran together in the asynchronous tact of the workﬂow.
Therefore, when a bank bought a new computer not only the technical facts were
important. It was the integration of the machine through software that mattered in a
technical environment of growing complexity. Software was needed that kept the
machines up and running eﬀectively. So it is no surprise that in 1967 Heinrich Fuchs,
head of the organizational department of the Municipal Savings Bank of Stuttgart
emphasized that “the performance of this equipment depends not only on its technical
speciﬁcations but mainly on the quality of its programs” [28].
After the IBM 1401 that they bought in 1960 proved to be reliable, the Savings Bank
of Stuttgart decided to upgrade their machines while moving into a new building. Large
scale computing always was dependent of space. It was a close run between IBM and
Siemens for the contract of a new data centre for the savings bank. IBM had huge prob‐
lems designing their operating systems for the system/360 but succeed in presenting a
preliminary version for testing. The software of its main competitor Siemens was in
delay, so this was one of the main reasons why IBM got acceptance of the tender [28].
Software sold the hardware.
One telling example for the importance of software in bank computing since the very
beginning of the computerization of Germany in the 1950s is the Zuse KG and its
computer Z31. While Konrad Zuse as the inventor of one of the ﬁrst digital computers
is already well known, the fortune of his economic venture is not. The Zuse KG was in
comparison to the huge producers like BULL, IBM or Remington Rand only a midsize
company. Employing 1200 employees at the very height of their success, they built
specialized computers which were state of the art in research and specialized usages.
The Z31 was intended by the engineers to open up a new market base in the economy.
As a smaller oﬃce computer automating routine jobs it was explicitly developed for the
usage in banking and savings banks. Even though the hardware was fully developed and
produced, the computer never had gone into sales because of missing software. Despite
their rather sophisticated sales team and their programs to educate computer users and
programmers the Zuse KG didn’t have the ﬁnancial resources to develop a clear strategy
of how to integrate the Z31 into the business processes of the bank in advance. They
lacked the programmers to write the code and missed the analysts who understood
banking [12, p. 70, 29]. Because hardware producers had to provide software or
massively helped their customers to integrate the computer into their business, the Z31
failed at the market despite its state of the art hardware. Probably worst of all, the Zuse
KG missed the international contacts to communities of invention to ﬁll this gap. What
an advantage this could have been showed the case of Heinz Nixdorf. With his company
Nixdorf AG he was the founding father of specialized bank computing solutions. In the
1950s, Nixdorf realized a deal with EXACTA, a midsize business machine company
who also had the license for BULL computers in Germany. In the end it worked out that
BULL sold Nixdorf computers abroad. Nixdorf proﬁted from the knowledge of BULL
concerning the usage environment of computers. In the 1960s, Heinz Nixdorf succeeded
in persuading Otto Müller, a German engineer of IBM Laboratories in Yorktown Heigh
148
M. Schmitt

to come to Paderborn. The experiences of Müller at IBM in the US helped Nixdorf to
produce the System Nixdorf 820 that was a huge success at the banking computer market
[30, pp. 94–99].
Aside from the operating system the banks needed a wide range of sophisticated
controller, monitoring and utility programs for running the machine. That did not neces‐
sarily mean application programs but a layer between the core of the machine and the
application layer. Together with the operation system these programs built the layer or
the “platform” on which the applications were based. As mentioned before, building up
this platform and the applications on top of it was a huge challenge for the hardware
producers. Out of their experience in the business machine market and a huge capital
stock, IBM built up a specialized division for banking from early on. They provided
customized solutions for their huge machines and helped massively to integrate them
into the daily banking business. They gave courses and instructed the workforce and the
technicians. Thousands of employees ran through those courses and their follow-ups
within the companies. This was the dominant way computer knowledge spread in the
early days in which Informatik was still in its formation phase. Often the producers even
send their own employees into the banks for a period of time besides the basis technical
service. Banks not only bought a machine in the 1950s. They bought a service. And this
service was customized as far as possible.
A good example for that is Winfried Ferger of IBM Germany. He began there as
Special Engineer in the division for customized solutions. IBM Germany was one of the
most important oﬀshoots of IBM, focusing especially on the production of mid-size
hardware and the creation of software [31]. He describes a close collaboration between
the customers and producers in the adaption of international produced hardware for
regional and task speciﬁc needs. In an interview he recounts:
“During all of my life I only developed things who nobody developed before. I always adapted
existing things to the requirements of banks or other clients. First I looked at the possibility for
realizing a speciﬁc need of a customer. Then we developed it” [32].
Here it becomes clear that banks and especially the savings banks were the processors
of the Digital Age that inﬂuenced the course of the technological evolution – also in
software. Products they demanded often found widespread adoption in the whole
industry and afterwards spilled over to other industries as well. Examples are encryption
or data procession of information retrieval that were adapted to their needs.
1.3
The Producers of Software
Asking for the producers of banking software one encounters a multitude of diﬀerent
players. Not surprisingly, the regional computer centres of the savings banks played a
crucial role in the digitalization of the banking system. This applies for both German
states despite their diﬀerent political and economic backgrounds. But the computer
centres seldom produced their own programs. They rather deﬁned the requirements for
software service orders or adopted the software. They were especially important for the
wide range of midsize and smaller institutions typical for a large share of savings banks.
The several huge institutions in the urban centres were forerunners raising the general
The Code of Banking: Software as the Digitalization
149

awareness for software solutions, but their requirements diﬀered severely in scope and
depth. Many of the programs came from large hardware producers who specialized on
banking technology or from the late 1960s onwards from a growing software industry
[27, pp. 51–58].
In the West, the German Savings Bank and Giro Association (DSGV) and the several
federal state associations worked as distributors. They not only ordered the software
solutions and made them available to the several savings banks. They also collected the
experiences made by the institutions and presented them in aggregated form for example
in the periodical “Betriebswirtschaftliche Blätter”. They coordinated the programming
eﬀort and presented best practice solutions. Many savings banks tried to get their systems
implemented nationwide through the DSGV so it was also a tool of inﬂuence and a point
of conﬂict. Its competence and means for existence always had to be renegotiated [33].
Using a wider software term, also in-house developments like the one of the Bavarian
Savings and Giro Associations are coming into view. On a modular base, they tried to
develop holistic solutions for all parts of the banking business especially for midsize
and smaller institutions and distributed it openly.
In the GDR, the situation of software creation only diﬀered slightly. First of all, the
banks expected the combine Robotron or its precursors to deliver the basic software
bundled with their rare computers. Concerning the applications, Robotron lacked the
capacities, the resources as well as the knowledge about the diﬀerent contexts of usage.
Therefore there was a clear separation and Robotron never produced any banking
programs for the East German banks, even though they cooperated with Siemens in the
1980s to build a system for international clearing [34]. Concerning system software there
was a shortage despite a sophisticated theoretical level of the mathematical foundations
of programing in science. Lacking workers in general, computer people in the GDR
were a scarce resource and computer knowledge was not widespread. In 1966, the head
of state Walter Ulbricht citied publicly this shortage on both sides as he stated: “We
need more programmers! But also the managers in the party, in science and economy
need a better knowledge of modern computing” [35, p. 3]. So it was no accident that in
the in the GDR application ﬁeld centred programing methods like SOPS was already
developed in late 1960s. It should provide basic general solutions for process automation
in various ﬁelds like SAP did successfully with its suite in West Germany in the late
1970s. But SOPS was mainly implemented in the industry, not in ﬁnance[36]. In charge
for the distribution of the software solutions were the Ministry of Finance and the Central
Bank. In most of the cases they ordered the software solutions at the “VEB Datenver‐
arbeitung der Finanzorgane”, the equivalent to the computer centres of the Western
German savings banks. But in comparison to the West German banks, it seems that there
was a stronger knowledge transfer within the Eastern Block. In the course of the mutual
coordination of computer production under the COMECON ESER-program,8 banks of
the Eastern Block exchanged their experiences on banking information technology on
a regular base. They even agreed upon collaborative software creation [37, 38].
8 ESER = “Einheitliches System Elektronischer Rechentechnik”, standardized system of elec‐
tronic computers.
150
M. Schmitt

Looking at the GDR is also telling in respect of the concrete programs used. Lacking
the foreign currency to license programs from the West and understanding software not
as a product, institutions of the GDR tended to copy programs from the West under
diﬀerent names. This matches the strategy of hardware imitation used for example in
the creation of the R300 as inspired by the IBM 1400-family, or the R40 as a IBM
S/360 adaption – both computers were the most common ones used in the GDR up to
the 1980s. In the 1960s, Robotron and his precursors concentrated mainly on providing
system software. Basic software was not provided and the users had to develop them all
by themselves. With the rise of the Digital Age - understood as the ongoing diﬀusion of
small and midsize computers and the increasing knowledge of computer usage in the
GDR - also the expectations of the computer users rose. They demanded specialized
software solutions for the conditions under socialism, namely a state oriented, planned
economy. Robotron reacted late but founded the “VEB Robotron-Projekt” in Dresden
in 1984 as their software house. Suﬀering under the ever-severe resource shortages they
mainly continued the strategy of renaming foreign products and deploying them.
Reacting to the demands of the users, they also tried to develop some specialized prod‐
ucts for a socialist economy. However, in the meantime Western products reached a
huge diﬀusion and the users rejected the incompatible solutions of the “VEB Robotron-
Projekt”. In result, these projects were abandoned thereafter, despite the large invest‐
ments made and the goals that were already reached [39].
2
Information Systems
Banking information systems are a type of software that was developed in the late 1960s
and reached a huge adoption afterwards. After deﬁning the term information systems I
am taking a closer look at the program system SODIS9 of the savings bank in Saar‐
brucken, TELDAS as the system of the Municipal Savings Bank of Stuttgart and ﬁnally
at the information system of the state bank of the GDR to show the inﬂuence of software
on the whole banking process.
As an “information system” Horst Stevenson, a widely received banking automation
expert, understood in 1973 a “form of data organization […] realized with electronic
data processing systems […] supporting certain operations and routines within infor‐
mation and communication processes” [40, p. 13]. First of all, this means that informa‐
tion systems in this understanding are necessarily computer based and binary-digital.
Second, their means was to support, not to substitute the employees through providing
the right set of information in the right amount at the right point in time. Information
systems were a reaction to the perceived masses of data produced by the machines and
the growing customer base since the early 1950s. The contemporaries had diﬃculties to
extract the relevant data out of these masses. The context of this perception in the indus‐
trialized countries are the late 1960s as a time of apparently rising complexity and failing
steering mechanisms in a globalizing world [41]. It is important to keep in mind though,
that in all times people had to deal with large amounts of data and the complaint of
9 The term SODIS was an abbreviation for the German translation of real time transaction.
The Code of Banking: Software as the Digitalization
151

drowning in data was not a new phenomenon even then [42, 43]. On the one hand
information systems were closely related or based on technological developments like
real-time accounting, on-line data transfer, databases and display technology. On the
other hand they represented a method of integrated process engineering. They were the
product of knowledge about the usage and the integration of the computer into banking.
Stevenson understood information systems as the dawning of a new era of computing
replacing the process-oriented phase of data processing. Process oriented data
processing meant the division between certain areas without or with little exchange of
data between divisions. Managers and other employees only hardly could interconnect
the data sets they possessed. Banks had therefore to record certain data several times.
The problem of double recording applied even more for the savings banks of the GDR
who had to ﬁght longer against the problems of divided process chains between digital
and analog – as well between diﬀerent business areas as between whole institutions not
yet computerized. For a long time they kept on specializing data integration to certain
areas like the generation of statistics. The output of statistics was more or less useful for
managerial demands but seldom for the daily business. Savings banks began to integrate
their divisions into the information systems step by step then, beginning with the most
important areas. Not until the early 1980s, total information systems were achieved
capturing all parts constituting a bank. Even then, the cybernetic dreams of universality
were never quite reached [44]. In a nutshell, information systems developed out of inte‐
grated data processing that was limited to a certain area [44, p. 27].
To look at information systems with a wider deﬁnition of software makes sense out
of a double reason. In the ﬁrst place information systems are composed of the “total of
interconnected workers, machines and organizational institutions that are data
processing or informational productive in the course of the managerial process of the
enterprise” [45, p. 456]. Even though in this deﬁnition by Dahms and Haberlandt the
data itself is hardly missing, information systems reached way further than just the lines
of code it was written in or the machine it was run on. It rather described a rhizome like
conglomeration that matches the initial understanding of a wider software term. In the
second place, it had a clear focus on the user aspect and described rather a service or a
solution to a daunting problem like software did. Besides these two points, there are
some additions to be made. The term used in the 1970s was deeply shaped by cybernetic
principals. It is important to keep that in mind while looking at the expectations banks
projected onto the information systems but also while analyzing how these systems were
marketed to them. Cybernetic models often suﬀered from an exaggerated universaliza‐
tion of their scope, especially in the case of social cybernetics where diﬀerent elements
were described with similar terms and afterwards appeared similar. The tendency for
methodological overstretch applied for a range of areas of use when the diﬀerent
elements of information systems were uniﬁed under the language of information. The
problem is clearly graspable in the use of the term information one can ﬁnd in the sources.
The idea that data got distilled to information within a socio-technological process
prevailed. Information was seen more like a material resource than a constructed entity.
It is important to avoid a teleological narrative of the savings banks on a path to ever-
greater knowledge and control through technological advantages. Software as an ephem‐
eral and problematic concept is helping to focus on the dirty reality of computerization.
152
M. Schmitt

Information systems were also a factor for greater uncertainty, perceived disorder and
confusing fragility. Viewed this way, the term of information systems under the auspices
of software is a tool for applied data critics [46].
2.1
Applied Information Systems: SODIS and How the Bank Came
into the Computer
In 1967, the Savings Bank of Saarbrucken, located in West Germany at the French
boarder, proudly presented their internal software system SODIS to the public [47]. The
institute, one of the most computerized institutes of the republic, decided to use the
medium of the IBM bulletin to show their recent advancements. They stated that the
main goals for the managers introducing this system were the rationalization and accel‐
eration of every process inside the bank. In the mid-1960s, their account data was still
recorded analog and then transferred to punch cards afterwards in the punch card divi‐
sion. With the help of an on-line system consisting of electronic business machines, a
brand-new IBM S/370 and the data connection lines, data could be directly produced
born-digital. This also meant that the account data was immediately up to date to speed
up a whole range of processes. Parts of the bank were now accessible fully digital and
in real-time. The project was located at the borderline of classic data processing within
the divisions and the new information systems. The head of the organizational division
Ingo Holtzmann stressed this transitional character of the system that “should not be
misunderstood as a sophisticated information system in the classical sense of the term.
It should rather provide precise criteria for decision in the context of immediate
processing of transactions” [47, p. 44]. Only in a further step it should evolve into a full-
ﬂedged management information system. As Stevenson showed, the transition from
integrated data procession to information systems was ﬂuid.
The special role of software within computerization becomes clear in the project
description of Holtzmann. He begins with an extensive description of the universal
character of the computer. For him, universality was rather a disadvantage because the
“business of savings banks demands specialized machines for the individual ways of
procession” [47, p. 45]. The solution of choice in the 1960s still was a combination of
hardware and software with specialized keys as the hardwired and static interface to the
machine. The advantage of a ﬂexible deﬁned button realized through the interface was
not graspable for the contemporaries yet. But the core of the solution was software. It
transformed the problematic universalist computer into a specialized and useful device
inside the savings bank business. So Holtzmann concludes, “the result of all organiza‐
tional planning [of the bank] in programmed form is the SODIS program system” [47,
p. 47].
It was produced through two teams of programmers that were separated into on-line
recording and further procession. For the sake of interconnection, the planners insisted
on “a precise deﬁnition of every process of business” [47, p. 48]. It becomes visible how
the organizational department recorded every step of work inside the savings bank busi‐
ness, standardized it and made it machine readable to support it by information tech‐
nology. In short: banking was depicted in software. Also the style of work of the
programmers became part of this standardization process. The management saw their
The Code of Banking: Software as the Digitalization
153

work as opaque and what they did for many spectators seemed more like a black art
than actual work. This could cause a multitude of problems and delays as the rigid plans
seldom left the freedom for the creative thinking that was necessary for a good systems
design. In the end, the drive for standardization was also a way to conﬁne the power of
programmers in the process of writing the code of banking. Holtzmann: “Analysis was
produced for every program. Regulations for programming were circulated internally.
Detailed trainings and narrow introduction inﬂuenced positively code, text and comple‐
tion of the programs” [47, pp. 48–49]. Not only the software environment but also the
programmer himself should deliver expectable and increasingly standardized products.
For the 1960s that still experienced a huge error rate in computing, this was a huge step
forward. But at the same time it was the conﬁnement of creativity in the struggle for
organizational power. In the militarized words of the management oriented “Zeitschrift
für das gesamte Kreditwesen” this struggle becomes visible: “The Chief Technology
Oﬃcer and his reserve unit will grow in power, but the decisions had to be left in the
hands of the management” [48, p. 978]. Therefore, they also had to acquire competences
in EDP to direct the change, the article concludes.
The programs of SODIS were written directly in Assembler, a machine language.
In comparison to higher programming languages this had the advantage of optimized
usage of computer time and a lower usage of memory [12, p. 123]. Even though the
engineers realized the daunting problem of writing code directly for a speciﬁc machine
and tried to get independent, their solution of standardized input and output codes was
only a transitional one. Writing for a speciﬁc machine always meant higher maintenance
costs and a greater eﬀort to migrate a coded solution to a new hardware environment
[27, pp. 51–52, 47, p. 49]. So SODIS was a transitional solution in theory as well as in
usage. But in the eyes of the management of the Savings Bank, the program proved
useful to them and they expanded it to other ﬁelds and branches. At the 10th anniversary
of SODIS, director Lehberg not only stressed the pioneer work his institute had carried
out. He also pointed to the huge interaction between the business of banking and the
system. “Besides the janitor, the new working process reached out into every division
of the bank. The change was so strong that sometimes one could barely identify the old
processes afterwards” [49].
What exactly did change? First of all, the direct procession led to a massive accel‐
eration of payments and withdrawals at the counter. While beforehand transactions could
take up to 15 min, now a single transaction only took 15 s – including the printing of a
receipt [48, p. 977]. The account was up to date immediately, while transactions in the
old system only were processed after the cut-oﬀ time, in most Savings Banks at 4 pm.
The teller could see immediately the balance of the customer and if other transactions
had taken place. Second, it also meant a huge acceleration in back oﬃce as the lavish
sorting rounds of data sheets or punch cards became unnecessary. Third, information
systems like SODIS meant a new possibility for the management to grasp the Savings
Bank in the very moment of a request. The bank became visible to directors like Lehberg
in (nearly) real time numbers and data streams. Or in the words of the before mentioned
journal this meant “faster and further diversiﬁed balance sheets, stock overview but most
importantly an overview of the cash-ﬂow” [48, p. 978]. It is striking that here even
conservative business experts began to dream about the possibility of getting an “insight
154
M. Schmitt

into structural trends on national economies” [48, p. 978] given just enough data of
diﬀerent clients. These dreams never came true and in 1967 it still was a long way to go
until the whole bank was integrated into the information systems. The trend towards a
higher insight into banking can be seen in the light of the professionalization and scien‐
tization as writings of contemporary history are highlighting for the 1960s–1970s.
On the international stage, information systems like SODIS were an important topic
of discussion, even though the cooperation was limited mainly on knowledge exchange.
At the third international conference on automation in Vedbaek in 1967, delegates from
16 Western countries came together to discuss the recent trends in savings bank auto‐
mation. Besides USA and Australia, all of the other delegates came from Europe. The
German delegation showed a great interest in the question of software and mutual
exchange. Director Claus of the Institute for Automation (IfA) described their mission
in producing test cases of working processes and sample programs that should become
freely distributed to reduce the burden of programming and asked for similar examples.
The sources are indicating that European software exchange was developing with a core
in Scandinavia [50]. In contrast, the US-delegate stated that the local savings banks still
acted with great caution concerning the provision of programs. The conference protocol
ends with a statement of Dr Richard Nowak from the German Savings Bank and Giro
Association. He concludes that “leaders should acquire a straight approach to automa‐
tion; personally they might be enthusiastic about the new technology or condemn it. But
they cannot escape […] the development” [50]. It is telling that information technology
at this point in time already was seen obligatory by certain banking elites – a rhetoric
ﬁgure well known in the history of technology. In 1973 the International Institute of
Savings Banks listed the eﬀorts for mutual software exchange in their regular reports
on international savings bank automation. For Germany, they noted that the diﬀerent
programs provided by the Institute for Automation were already installed in over 16
institutions. They even were willingly to provide them to other European Savings Banks
– even though for a fee [51, p. 41].
All in all, SODIS developed in the context of early information systems still limited
to certain areas. Systems like SODIS formed the base for further development of higher
integration, like the next case of TELDAS shows.
2.2
TELDAS as Consultancy: The Making of Path Dependencies
A similar development experienced the already mentioned Municipal Savings Bank of
Stuttgart, another forerunner of German computerization in using IT. After moving to
their new main branch on Königsstraße 3 in 1967, they now had the space for purchasing
a new mainframe computer. Similar to the purchase in 1960, as they acquired an IBM
1401, they sought IBM for an exhaustive report on banking automation at their institute.
IBM delivered. As they proudly pronounced, “our report based on a exhaustive study
our your institute” [24, 28]. Not surprisingly, the document sounded more like consul‐
tancy report than like an advertisement brochure for a computer system. The IBM
consultants broke down the working processes into single steps, reordered them anew,
measured the processes and compared them to the existent non-DP processes in respect
to duration, eﬃciency and cost. While in the 1950s, the Municipal Savings Bank of
The Code of Banking: Software as the Digitalization
155

Stuttgart had been a customer of the French Compagnie des Machines Bull, in the 1960s
it had already switched to IBM machines after intense discussions. A strong argument
was the close proximity of IBM Germany. They had opened their headquarters in
Böblingen and not in Bonn or Berlin. IBM Germany proﬁted from its status as a loyal
customer of the savings bank who gave them credit – one example for Savings Banks
as processors of computerization in Germany. But even more so IBM proﬁted from their
sophisticated sales team with expertise in banking automation.
But the report was only the beginning of the digitalization of the bank. On the two
brand-new IBM Systems 360/40 their IT division ran the remote data procession system
TELDAS, a system quite similar to SODIS. In the beginning, the system was used for
the handling of savings transactions. The employees could now instantly book savings
transactions “on-line” into the account of the customer through terminals connected with
the mainframe. They also were now in the position to directly print on the savings book
in same work process. Step after step the Savings Bank of Stuttgart integrated more and
more divisions and branches into TELDAS. In 1971, giro traﬃc was integrated into the
system by the DP division. In 1973, TELDAS also covered foreign currency, mortgages,
securities and standing orders. On top of this database eventually the customer infor‐
mation system (KIS) was installed, provided by the Institute for Automation. With the
customer information system it was possible for every bank employee to get a fast over‐
view about every customer and to individualize the services oﬀered to him. Missing
possibilities were made visible by the machine suggesting new products. The deploy‐
ment of the information systems took place in the strategic reorientation of the German
Savings Banks towards a customer centred strategy [52].
Looking at this speciﬁc system installation over the period of only 6 years, software
had gotten more and more important in respect to the eﬀective functionality and it is
striking how interchangeable hardware had become. TELDAS already did not run
anymore on the initial IBM S360/40s but on an IBM S370/158. The hardware replace‐
ment changed not that much in respect to functionality. Banking had become code, not
a machine. This is referring to “The Government Machine” by John Agar who had the
thesis that the British state in the 20th Century had become a bureaucracy machine similar
to the punch card systems it relied on [53]. For the banks it was the software, not the
transistor and it was the integration of the data, not of the circuit that mattered.
In 1979, the Savings Bank of Stuttgart was integrated in total in TELDAS. In the
annual report of the Savings Bank for the year 1979 it is simply said: “All customer
related areas are fully organized within TELDAS” [54, pp. 48–50]. In the following
year, all 250 branches were connected to TELDAS remotely and up to 420.000 trans‐
actions were realized through the system. But this did not mean that the bank was
available to the management in total. The report of the International Institute for
Savings Banks states that in Germany the high dreams of information system univer‐
sality were buried underneath the details of daily business [55]. The story also is an
example for how hard it was after the full integration to step outside the path depend‐
ency of one specific supplier. Even though on a higher organizational level, the banks
tried not to get depended of one hardware producer, all competitors from now on had
to build their systems compatible to TELDAS. Up and running, the software systems
were productive for decades as they carried the code of banking. So software systems
156
M. Schmitt

in Germany seldom changed and if they changed, then because banking itself was
transformed in close intra-action to information technology.
2.3
Informational Socialism: The Information System of the Central
Bank of the GDR
In comparison to the FDR, in the GDR the international community of invention was
more important in building information systems. During the phase of intensiﬁed comput‐
erization in the years 1968-1972 under the rule of state driven cybernetics the project
“open accounting, transaction clearing and accountancy” built the base for further inte‐
gration. Upon this transaction data, the information system for internal bank data should
provide primary information to the leading cadres for further use [56]. My thesis is that
this generally implied a better control of the planned economy through knowledge about
the currency ﬂows in a political phase of new economic concepts under Walter Ulbricht
and a loosening of direct control. In the eyes of the party elite, ﬂows of current and ﬂows
of currency converged.
The State Bank of the GDR developed the information system on a modular base
that at the same time “evoked integral eﬀects of rationalization as well as the preparation
of information for ﬁeld speciﬁc and management tasks” [56]. The discussion and the
development were very similar to that in Western Germany, even though it took place
a bit later: After the ﬁrst successful projects of computerization it was now a challenge
for the leading personnel not to drown in all the data recorded. But at same moment it
seemed necessary to interconnect more data of diﬀerent ﬁelds for eﬃcient planning. The
goal for the planners was that the employees do not to lose track in a world that grew in
complexity. The complaints about wrong or too much data were commonplace.
Referring to Western developments, the State Bank decided in 1974 to expand the
information systems in scope through the implementation of remote data transmission.
Like the Savings Bank of Stuttgart, they also ordered a report on data procession and
transmission by their computer centre “VEB Datenverarbeitung der Finanzorgane” for
deﬁning the requirements. Based on that report, a nationwide transaction network had
to be built transferring the data for an integrated information system in all banks called
data collection system (DSS) – also in the Savings Banks [57]. There are two astonishing
developments in respect to cross-border knowledge transfer between developers. First
of all, the planers of the system had a very close look at the developments in the West.
Horst Stevenson’s ideas are to be found word-by-word inside the reports, as well as the
works of several other US-experts on banking automation. Besides rationalization, the
argument of lagging behind the development in the capitalist countries should convince
the state leaders of spending scarce resources in this project. As in the West, it was also
the goal of the planners of the information system to record the data born digital. Through
this the eﬀort of data carrier transport and amount of the paper used should be reduced.
Only in a second step, the recorded data should have been used for remote access in
planning and control.
But, surprisingly enough, the data collection system was not developed only within
the GDR. According to the before mentioned cooperation within the COMECON, the
eﬀort of developing the system was shared between socialist states on a bilateral basis.
The Code of Banking: Software as the Digitalization
157

On the Budapest trade fair in 1975, representatives of the State Bank of the GDR and
the Hungarian company VIDEOTON, a big producer of information and communica‐
tions technology met for negotiations. They reached an agreement about the construction
of multiplex controllers for data transmission, seminars on remote data transmission and
last but not least the programming of core components of the data collection system of
the GDR. All in all, they agreed on software services. In the following years, experts
from VIDEOTON travelled to the GDR on a regular base. Following the sources at the
DP division of the State Bank, their employees were fully aware of the pitfalls of inter‐
national cooperation in software projects. That’s important because in general the
COMECON is viewed as a failure. Head of division Uhlig stated that the quality of the
Systemunterlagen depended upon coordination. He diﬀerentiated “The quality of the
product depends on the skill of the programmer if he’s working alone. If three program‐
mers are working together, the quality of the product depends on project management”
[58, p. 4]. This is a very early example of software metrics as the measurement of soft‐
ware production quality the subordinate clause.
The international team of developers and planers faced three core challenges. First
of all, it was necessary to establish an eﬃcient project management. Therefore, the
requirements had to be deﬁned and the areas of competence had to be strictly assigned.
The necessary eﬀort of communication and coordination between the two countries was
high, for example due to consultations. Program manager complained about the missing
train and airplane connections from and to Budapest in the summer of 1976. Also the
requirements tended to get out of hand. This caused Comrade Süßbier, head of the
working group DSS, to change the requirements afterwards and he gave the order that
“the requirements for the DSS are to be revised until the end of August. Everything that
has nothing to do with the economic data collection system has to be thrown out. We
need a clear line on that” [59]. Secondly, delays in the production and in documentation
were commonplace. For of this reason, general director Geißler of the DP-division sent
an unequivocal letter to the representatives of VIDEOTON in Berlin in 1976 demanding
a proper documentation. But this was not only a problem of international cooperation.
Also within the GDR delays occurred quite often. For example, for test reasons the State
Bank of the GDR ordered from the combine Zentronik modern teletypes T800 with a
special coding. But the production of the teletypes came into delay so that the DP-
division complained about this oﬃcially. But Dr Geißler only stated with brief words
that “up to this day the general director of the people-owned combine Zentronik has not
reacted to our letter despite several reminders” [60, p. 3].
Thirdly, barriers of language had to be overcome. The programmers were united by
one common language, the language of the machine. But despite this, the annotations
as well as the knowledge of usage of the software were in very diﬀerent languages.
Software language depended on cultural styles, political contexts, historical paths, levels
of development as well as on diﬀerent believe systems. In the ﬁnal code of the data
collection system of the State Bank of the GDR, one can ﬁnd ﬁve diﬀerent languages.
Most of the instructions were written in Assembler oriented towards English as the
lingua franca of the Digital Age. This wasn’t changed by the politics of language of the
GDR concerning imperialistic language. Besides English, many annotations are written
in Hungarian, some others in German. Even French annotations are to be found. The
158
M. Schmitt

French Compagnie des Machines Bull maintained good contacts into the Eastern Block.
Adjacent to machine instructions like
DO or DATA
stood annotations like
*+Space
A HIGAS EGYSEG MEGJELOLESE // signs of defective units
"ADRESSEN RETTEN" // rescue data
"TABLES DES ETIQUETTES DE CDS" // tables of CDS-labels
[61]
While translators helped the programmers to communicate, on the machine level trans‐
lators were not always available. Programs and computers from very diﬀerent manu‐
facturers of the ESER program had to talk to each other through protocols. This applied
to the hardware level between computers and teletypes, for example in regards to the
diﬀerent currents for reception [62]. But also peripherals had to be connected to diﬀerent
types of machines. Robotron therefore produced an ESER-adapter to connect their own
peripherals to the foreign ESER-computers. But in total, this took way more eﬀort
because of the necessary conversion processes [63]. Also on the software level, the
problem was daunting despite the planned compatibility of ESER. So the head of sector
DP, comrade Uhlig, was happy that at least there existed a way to emulate the software
of VIDEOTON on their mainframe computers R10 – R40 in FORTRAN [63]. This was
necessary to test the programs on the Robotron computers beforehand.
Despite the huge obstacles it seems likely that the international cooperation
succeeded. Based on the newly built data network, the State Bank of the GDR in 1981
began to implement the DSS in the savings banks. Thereby, they circulated a preliminary
draft of the working instructions for the preparation and training of the bank employees.
In the following years, the instructions were adapted and extended several times. Even
though the system had a rocky start, it went productive in 1983 and was used intensively
quite fast. In the savings banks of the region Berlin, in 1986 alone, they processed
122.409 receipts with the DSS [64]. The savings banks in the GDR ordered ever more
Teletypes to get connected to the system. The main obstacle was the bad transmission
quality of a data network that was very error prone. And also the minds of the savings
banks employees had to be opened for the new technology. In 1981, the DP division of
the State Bank complained in a long letter about the problems of software implemen‐
tation. After praising their own successes in data bank usability they stated that “it isn’t
always easy to get the new technology accepted. The change is about leaving behind
traditional ways of work. Not always one can ﬁnd the necessary openness to leave behind
traditional behaviours” [65, p. 9]. The implementation of software took its time.
In parallel with the nationwide implementation of the information system the inter‐
national consultations about these systems continued within COMECON [66, p. 2]. At
a bilateral conference in 1981, the State Banks of the Soviet Union and the GDR
exchanged their experiences about the use of information systems. The Soviet delegation
was extremely interested in the developments inside the GDR. The very broad and
The Code of Banking: Software as the Digitalization
159

unspeciﬁc terms the Soviet delegates used within these consultations seems to indicate
that they had not reached a similar level yet. On the contrary, the GDR delegates had a
deep insight into the technological developments of the USSR. They knew exactly which
components they needed and named them openly as their main interest in a mutual
exchange [67]. Eventually, ROBOTRON stuﬀed the computer center of the GOSBANK
[34]. Information systems also were an important topic on the four working conferences
on data processing of the State Banks of COMECON over the course of the 1980s. The
ﬁrst one took place in 1984, the second in Moscow in 1986, the third in Prague 1987
and the last one in Soﬁa in 1989. This went so far that after the third conference in
Moscow the State Banks of Eastern Germany and the USSR agreed on a “proposal for
the collaborative production and mutual exchange of software” [68]. This contained
system software as well as applications. Even though both side came to the conclusion
that the structure of their banking system diﬀered widely in some areas, they saw over‐
lapping use cases in human resources, documentation, back-oﬃce and control systems
[68]. While both economic systems faced severe ﬁnancial shortages at the end of the
Cold War, the cooperative approach could be interpreted not only as rationalization of
software production. Without doubt, it was a reaction to the widespread use of the
Personal Computer at the work place that demanded a broader range of software. But it
was also in the interest of the GDR to bind a USSR that withdrew its inﬂuence slowly
under Gorbachev as it was confronted with an overstretch of power [68].
3
Conclusion
Answering the initial question of this article, the bank was represented in its information
systems step by step, covering ﬁnally all of its processes. The computerization of
banking took place in four stages. After the introduction of Electronic Data Processing
within singular ﬁelds of banking, Savings Banks intensiﬁed the eﬀorts for data integra‐
tion within these ﬁelds. The rise of information systems bridged the gap between the
individual divisions and their diﬀerent databases. Accessible via remote data transfer,
information systems corresponded with the needs of management and employees for a
better overview and the provision of the right data at the right moment of time. But their
implementation faced severe obstacles. Even though software as the implementation of
computers in banks never fulﬁlled the high expectations they aroused, systems like
TELDAS in Stuttgart or the DSS of the State Bank of the GDR were in productive use
since at least the 1970s. They combined diﬀerent elements of the bank ranging from the
computers, the employees up to the working instructions. Therefore, they built a Dele‐
uzian assemblage of those heterogeneous elements. During the course of the Cold War,
the banks digitalized themselves as they ﬁnally in the 1980s depicted every one of their
business processes in software. The Savings Banks now existed in the information
system. That meant ﬁrst of all a hardening of procedures. Processes and relations were
deﬁned by software that often ran for decades. Second, it made the bank visible and
transparent to its management. Now it lay – within certain limits – open to them in real
time what was going on inside the bank. Software was understood in a wider sense that
included computer in action and their whole environment. In short: It was the code of
banking that mattered.
160
M. Schmitt

As the whole project about the computerization of German Savings Banks is indi‐
cating, the plans of socialist countries often were even more ambitious and far reaching
than in the West. That often meant that the GDR oﬃcials took the digitalization more
serious out of diﬀerent reasons. Apart from the demand for rationalization out of a severe
resource shortage, it was mainly the desire for an eﬃcient way of steering the economy
under the imperative of a planned economy that drove the computerization in the banking
sector. Or in a nutshell: The utopian character of socialism also showed itself in the
digital (r)evolution. Obstacles were the huge delays in the production of information
technology, the international cooperation and the poor quality of the data network
provided by the Deutsche Post of the GDR. The information system of the State Bank
ﬁnally was realized in 1981, the cybernetic plans for monitoring the economy were not.
On the level of software development, the responsible leaders of the GDR took the
COMECON serious. They placed their hopes on a socialist approach of cooperation
within which the experiences of the computerization were shared. You can follow this
up to the very code of the information system of the State Bank of the GDR that was
written in ﬁve diﬀerent languages. But like in West Germany the project suﬀered from
severe shortages of software and programmers over the whole period of time. This
caused again and again struggles of power and ﬁnally led to the strict standardization of
the education and work of software engineers.
My ﬁnal conclusion is that socialist software in general transformed from a service
to a product later than in the capitalist West. This was also inﬂuenced by the strong
cooperation on the international level within the COMECON. But also the Savings
Banks in West Germany took a rather open approach to software out of their under‐
standing as oriented towards the common good. While in the beginning of the comput‐
erization the banks expected the huge hardware producers to deliver the software with
their products or were self-produced, the software industry became ever more diversiﬁed
since the late 1960s. A wider term of software not limited to the actual program but to
the whole system of the computer in action helps to understand this change.
References
1. Nelson, T.: The crafting of media. In: Burnham, J., Robert, J. (eds.) Software: Information
Technology: Its Meaning for Art. An Exhibition at the Jewish Museum, vol. 17. The Jewish
Museum, New York (1970). p. 17
2. Ensmenger, N.: The Computer Boys Take Over: Computers, Programmers, and the Politics
of Technical Expertise. History of Computing. The MIT Press, Cambridge (2010). P. 5
3. For an entry point to this debate, Haigh, T., Priestley, M., Rope, C.: ENIAC in Action: Making
and Remaking the Modern Computer. History of Computing. The MIT Press, Cambridge
(2016). For an entry point to this debate
4. Kittler, F.: Grammophon, Film, Typewriter. Brinkmann & Bose, Berlin (1986). P. 7
5. Mahoney, M.: What makes the history of software hard. IEEE Ann. Hist. Comput. 30(3), 8–
18 (2008)
6. Barad, K.: Agential realism: how material-discursive practices matter. Signs 28(3), 803–831
(2003)
The Code of Banking: Software as the Digitalization
161

7. Schuhmann, A., Danyel, J.: Wege in Die Digitale Moderne. Computerisierung als
gesellschaftlicher Wandel. In: Bösch, F. (ed.) Geteilte Geschichte. Ost- Und Westdeutschland
1970–2000, pp. 283–319. Vandenhoeck & Ruprecht, Göttingen (2015). P. 286–290
8 In the international context, Kling, R., Iacono, S.: The mobilization of support for
computerization: the role of computerization movements. Soc. Probl. 35(3), 226–243 (1988).
P. 227
9. Schmitt, M., Erdogan, J., Kasper, T., Funke, J.: Digitalgeschichte Deutschlands: ein
Forschungsbericht. Technikgeschichte 83(1), 33–70 (2016). P. 33–34
10. Haigh, T.: Software in the 1960s as concept, service, and product. IEEE Ann. Hist. Comput.
24(1), 5–13 (2002). P. 5–6
11. Deleuze, G., Parnet, C.: Dialogues II. Columbia University Press, New York (2002). P. 69
12. Leimbach, T.: Die Softwarebranche in Deutschland: Entwicklung eines Innovationssystems
zwischen Forschung, Markt, Anwendung und Politik 1950 bis heute. Fraunhofer Verl.,
Stuttgart (2011). P. 122
13. Gugerli, D.: Der Programmierer. In: Mangold, H., Frei, A. (eds.): Das Personal der
Postmoderne. Inventur einer Epoche, pp. 17–32. Transcript, Bielefeld (2015)
14. Wolle, S.: Die heile Welt der Diktatur. Alltag und Herrschaft in der DDR 1971–1989, 4th
edn. Links Verlag, Berlin (2013). P. 86
15. Donig, S.: Vorbild und Klassenfeind. Die USA und die DDR-Informatik in den 1960er Jahren.
Osteuropa 59(10), 89–100 (2009). P. 93
16. Entry “Software”. In: Schoppan, W. (ed.) Lexikon der Wirtschaft: Rechentechnik,
Datenverarbeitung, p. 364. Verl. Die Wirtschaft, Berlin (1981)
17. Campbell-Kelly, M.: From Airline Reservations to Sonic the Hedgehog: A History of the
Software Industry. History of Computing. MIT Press, Cambridge (2004). P. 2
18. Mirowski, P.: Machine Dreams: Economics becomes a Cyborg Science. Cambridge Univ.
Press, Cambridge (2003)
19. Ronald, R.: Address at Moscow State University, 31 May 1988. http://millercenter.org/
president/reagan/speeches/speech-3416
20. Chun, W.: Programmed Visions: Software and Memory. Software Studies. MIT Press,
Cambridge (2013). P. 4–6
21. Mahoney, M.: The history of computing in the history of technology. Ann. Hist. Comput.
10(2), 113–125 (1988). P. 122
22. Deleuze, G., Guattari, F.: A Thousand Plateaus: Capitalism and Schizophrenia. University of
Minnesota Press, Minneapolis (1987). P. 3–28
23. Rosenberg, S.: Dreaming in Code: Two Dozen Programmers, Three Years, 4,732 Bugs, and
One Quest for Transcendent Software. Crown Publishers, New York (2007)
24. WABW B102/2300. IBM Deutschland: Elektronische Datenverarbeitung Bei Der Städtischen
Sparkasse Stuttgart, Städtische Girokasse Stuttgart. Ein Untersuchungsbericht Der IBM
Deutschland (1960)
25. Martin, I.: Structuring Information Work: Ferranti and Martins Bank, 1952–1968. Inf. Cult.:
J. Hist. 47(3), 312–339 (2012). P. 325
26. WABW B102/321. Städtische Girokasse Stuttgart: Austausch der elektronischen
Datenverarbeitungsanlagen vom Typ IBM 1401 gegen die Type IBM 1460 (1963). P. 6
27. Ganzhorn, K. (ed.): IBM Informationstechnik für Banken und Sparkassen im 20. Jahrhundert.
Forschung und Entwicklung in der IBM Deutschland 6. Röhm, Sindelﬁngen (2006). P. 51
28. WABW 
B102/321. 
Städtische 
Girokasse 
Stuttgart: 
Beschaﬀung 
einer 
neuen
Datenverarbeitungsanlage (1967). This and all following quotations translated by the author
29. Interview with Horst Zuse, son of Konrad Zuse. Conducted 17 May 2016
162
M. Schmitt

30. For the Nixdorf 820 see Berg, C.: Heinz Nixdorf. Eine Biographie. Ferdinand Schöningh,
Paderborn (2016). P. 94–99
31. Ganzhorn, K.: The IBM Laboratories in Boeblingen. Foundation and Build-Up. Röhm,
Sindelﬁngen (2000)
32. Interview with Winfried Ferger. Conducted 04 Sep 2015 at IBM Deutschland
33. WABW B102/1451
34. Interview with Hans-Jürgen Lodahl. Conducted 13 June 2016
35. Ulbricht, W.: Rede des Genossen Ulbricht vor dem Bezirkspateiaktiv Halle. In: Neues
Deutschland, 14 November 1966. P. 3
36. Gräßler, R.: Die Entwicklung von sachgebietsorientierten programmiersystemen ‘SOPS’ des
veb kombinates Robotron. In: Naumann, F., Schade, G. (eds.) Informatik in Der DDR - Eine
Bilanz, pp. 288–300. GI, Bonn (2006)
37. Geipel, G.: Politics and computers in the honecker era. In: Mackris, K., Hoﬀmann, D. (eds.)
Science under Socialism, pp. 230–246. Harvard Univ. Press, Cambridge (1999)
38. Ahrens, R.: Gegenseitige Wirtschaftshilfe? Die DDR im RGW: Strukturen und
handelspolitische Strategien 1963–1976. Böhlau, Köln (2000). On COMECON and its failure
39. Fritsche, D.: Mit Prototyprekonstruktion zum Welthöchststand? PC-Software in den letzten
Jahren der DDR. In: Dresdener Beiträge zur Geschichte der Technikwissenschaften, vol. 30,
pp. 105–123 (2005)
40. Stevenson, H.: Informationssysteme für Kreditinstitute. (=IS Informations-Systeme) de
Gruyter, Berlin (1973). P. 13
41. Doering-Manteuﬀel, A., Lutz, R.: Nach dem Boom. Perspektiven auf die Zeitgeschichte seit
1970, 3rd edn. Vandenhoeck & Ruprecht, Göttingen (2008)
42. Cortada, J.: The Information Flood. Oxford University Press, New York (2012)
43. Weinberger, D.: Too Big to Know. Basic Books, New York (2012)
44. Haigh, T.: IInventing Information Systems: the Systems Men and the Computer, 1950–1968.
Bus. Hist. Rev. 75, 15–61 (2001)
45. Dahms, H., Haberlandt, K.: Erfahrungen und Grundsätze beim Aufbau eines automatisierten
MIS. IO: Zeitschrift für Betriebswissenschaft, Management, Produktionstechnik u.
Organisation 39(11), 449–459 (1970). P. 456
46. Gießmann, S., Marcus B.: Was Ist Datenkritik? Zur Einführung. In: Mediale Kontrolle unter
Beobachtung, vol. 3(1) (2014)
47. Holtzmann, I., Stieﬀenhofer, H.: SODIS. Sofortdispositions- und Informationssystem der
Sparkasse der Stadt Saarbrücken. IBM-Nachrichten 18(187), 44–50 (1967)
48. Der Bankmann und sein Computer. Zeitschrift für das gesamte Kreditwesen 20(21), 977–978
(1967). P. 978
49. Ein EDV-System feiert Geburtstag. 10 Jahre Online-Betrieb bei der Stadtsparkasse
Saarbrücken. Computerwoche (1977). http://www.computerwoche.de/a/10-jahre-online-
betrieb-bei-der-stadtsparkasse-saarbruecken,1200155
50. Internationales Institut Der Sparkassen: 3. Internationale Automationstagung, Vedbaek, Mai
1967. Amsterdam (1967)
51. Internationales Institut der Sparkassen: Stand der Automation und der elektronischen
Datenverarbeitung Anfang 1973. Genf (1973). P. 41
52. Belvederesi-Kochs, R.: Von der “moralischen Anstalt” zum vertriebsorientierten
Finanzdienstleister. Der unternehmenskulturelle Wandel des Deutschen Sparkassen- und
Giroverbands im Spiegel seiner Marketingstrategie. Zeitschrift für Unternehmensgeschichte
53(2), 192–215 (2008)
53. Agar, J.: The Government Machine: A Revolutionary History of the Computer. MIT Press,
Cambridge (2003)
The Code of Banking: Software as the Digitalization
163

54. WABW B102/638. Landesgirokasse Stuttgart: 50 Jahre Datenverarbeitung bei der
Landesgirokasse. In: Landesgirokasse Stuttgart: Bericht über das Geschäftsjahr 1980, pp. 48–
50 (1980)
55. Internationales Institut der Sparkassen: Automation Trends 1979. Internationale
Untersuchung der Trends und neue Entwicklung bei Sparkassen. Genf (1979)
56. Anwendergemeinschaft 
EDV-Projekt 
Kontokorrent, 
Verrechnungsverkehr 
und
Bankbuchhaltung sowie Automatisierung des Zahlungsverkehrs: Anschreiben 3. Nachtrag
zum Handbuch EDV. In: Staatsbank der DDR: Handbuch der Sparkassen, pp. 5–53. EDV-
Projekt Kontokorrent, Verrechnungsverkehr und Bankbuchhaltung, Berlin (1971)
57. BArch DN10/633. Staatsbank der DDR, Abteilung Datenverarbeitung: 1. Entwurf einer
Studie zum Problem der Datenfernverarbeitung und Datenfernverarbeitung im Kreditsystem
der DDR (1974)
58. BArch DN10/633. Staatsbank der DDR, Abteilung EDV: Einzelnotizen auf Grund von
Beratungen und Gesprächen mit Vertretern der VIDEOTON-Fabriken und der VIDEOTON
AG anlässlich der Budapester Messe 1975 (1975). P. 4
59. BArch DN10/633. Staatsbank der DDR, Arbeitsgruppe DSS: Stand der Arbeiten am DSS:
Probleme (1976)
60. BArch DN10/633. Staatsbank der DDR, Abteilung EDV: Problemzusammenstellung für das
Ministerium für Elektrotechnik/Elektronik (1975). P. 3
61. BArch DN10/633. Code
62. BArch DN10/633. Staatsbank der DDR, Abteilung EDV: Brief des Generaldirektors Geißler
an VIDEOTON (1967)
63. BArch DN10/633. Staatsbank der DDR, Abteilung EDV: Einzelnotizen auf Grund von
Beratungen und Gesprächen mit Vertretern der VIDEOTON-Fabriken und der VIDEOTON
AG anlässlich der Budapester Messe (1975). P. 4
64. BArch DN1/20907. Ministerium der Finanzen/Abteilung Datenverarbeitung: EDV-
Anforderungen und -Aufgabenstellungen. Einführung des Datensammelsystems in den
Banken und Sparkassen, 1980–1988 (1980)
65. BArch DN10/1909. Staatsbank der DDR, Abteilungsbereich 602: Bericht anläßlich des
Erfahrungsaustausches von EDV-Spezialisten der Staatsbanken der RGW-Länder am
4./5.11.1981 in Budapest (1981). P. 9
66. BArch DN1/17439. Ministerium der Finanzen, Kaminsky: Brief an den Oberbürgermeister
der Hauptstadt der DDR (1970). P. 2
67. BArch DN10/1909. Staatsbank der DDR: Internationale Arbeit zu EDV mit Schwerpunkt
Informationssystem (1981)
68. BArch DN10/1909. Staatsbank der DDR, Abteilung 63: Maßnahmenplan in Auswertung der
zweiten EDV-Konferenz vom 12.1.1987 (1987)
164
M. Schmitt

Electronic “Ambassador”: The Diplomatic
Missions of IBM’s RAMAC 305
Evangelos Kotsioris(&)
Princeton University, Princeton, NJ, USA
ekotsior@princeton.edu
Abstract. This paper traces the “diplomatic missions” of the RAMAC 305
developed by IBM in California during the late 1950s and instrumentalized as
an “animate” ambassador of American computing technology abroad. Speciﬁ-
cally, this paper looks at IBM’s exhibit at the American National Exhibition in
Moscow (July 1959) and Nikita Khrushchev’s tour to the IBM manufacturing
plant in San Jose, California (September 1959) to argue that the RAMAC 305
was envisioned and designed as a modular system of combinable units and
peripherals that could be easily—and quite literally—transferred around the
world. Ultimately, this work demonstrates how the carefully choreographed
exhibiting of Western computing power by American companies like IBM
actively accelerated the participation of USSR in global treatises on the fair use
of patented information technologies in the decades that followed.
Keywords: IBMRAMAC 305  Cold war  Watson  Khrushchev San Jose 
Moscow
Like other stars from California, RAMAC went on tour, making personal appearances,
attracting crowds of… customers in leading cities across the country. They saw the machine
speed the ﬂow of business information, taking credit rating, writing orders, taking inventories,
adding accounts, ﬁguring commissions a machine that will keep the records of a business
always current and instantly available.
Excerpt from IBM ﬁlm: The Search at San Jose, 1958.1
1
Warm-Up Act
How is technology transferred? Both as a physical artefact and as intangible knowl-
edge? What is actually transferred and through which channels? Who lays claims to it
and who beneﬁts from this process? In other words, what is to be lost and gained
through such transfers? These are some of the main questions this paper will try to
answer; a paper that traces the “diplomatic missions” of the RAMAC 305, a computer
designed by the International Business Machines Corporation (IBM) in California
during the late 1950s.
For historians of information technology, IBM’s RAMAC (“Random Access
Method of Accounting and Control”) computer holds a prominent position in the
1 The Search at San Jose, Presentation by International Business Machines Corporation, Photographed
in San Jose California and Produced in New Jersey by New Film, Inc., 1958, 11:39.
© IFIP International Federation for Information Processing 2016
Published by Springer International Publishing AG 2016. All Rights Reserved
A. Tatnall and C. Leslie (Eds.): HC 2016, IFIP AICT 491, pp. 165–180, 2016.
DOI: 10.1007/978-3-319-49463-0_11

development of electronic data storage. It is commonly referred to as the ﬁrst computer
with a “hard disk” that allowed random access to large quantities of information; one
that left behind a linear way of accessing data. The sizeable hard disk of the RAMAC
305 could hold ﬁve million characters (approx. 4.4 MB), weighed a bit less than one
ton, and was materially articulated as a rotating stack of aluminium plates on which
data could be magnetically stored and retrieved rapidly with the help of a head attached
to the end of a mechanical arm (Figs. 1, 2 and 3).
Each aluminium plate of this innovative device was coated with a thin ﬁlm of
special oxide paint. This paint protected the aluminium and lent it the capability to
function as a magnetic storage medium. To prevent competition from copying this
invention, in October 1959 IBM ﬁled a patent for the composition of the special oxide
paint used for the hard disk of the RAMAC. From October 16, 1962, onwards, IBM’s
special paint—composed of epoxide resin, methyl phenol ether, polyvinyl methyl ether
and an acid anhydride catalyst – was legally protected.2 (Fig. 4).
Fig. 1. Flying the RAMAC 305: An IBM 350 disk unit, weighing a bit over a ton, being
forklifted onto a Pan American Airways operated Douglas freighter aircraft (most possibly a
DC-6A), ca. 1956
2 Donald D. Johnson and Donald D. Johnson, Composition of Epoxide Resin, 3058844, ﬁled October
13, 1959, and issued October 16, 1962, http://www.google.com/patents/US3058844.
166
E. Kotsioris

2
Act I: Sokolniki Park, Moscow, USSR, July 24, 1959
The American National Exhibition that took place at the Sokolniki Park in Moscow
during the summer of 1959 was the ﬁrst major exhibition organized by the United
States Information Agency in the USSR3, and was visited by a staggering two and a
half million Soviet visitors in 43 days4. The majority of the American exhibits in
Moscow regarded consumer goods that ranged from washing machines and vacuum
Fig. 2. Schematic drawing of RAMAC’s
memory platters, their vertical stacking on a
rotating spindle and the read/write heads
accessing
data
with
the
help
of
a
servo-mechanical “access arm.” (Source:
IBM Corporation, 305 RAMAC, Random
Access Method of Accounting and Control:
Manual of Operation (Major revision, Form
22-6264-1), April 1957, 13)
Fig. 3. IBM Corporation, RAMAC actuator and
disk stack, 1956. 54 in  40 in  30 in. “This is
the heart of the world’s ﬁrst disk drive. It has 50
24” disks spinning at 1,200 RPM holding 5
million characters of information.” Photo: Mark
Richards. (Source: Computer History Museum,
Mountain View, CA. Object ID: L2009.1.1p.)
3 An impressive chronology of the American Exhibits to the USSR, compiled by the Bureau of
European and Eurasian Affairs on the occasion of the 50th Anniversary of the Moscow exhibition,
counts nineteen major events between 1959 and 1989. Bureau of Public Affairs, U.S. Department Of
State. The Ofﬁce of Electronic Information, “Chronology: American Exhibits to the U.S.S.R.,” http://
www.state.gov/p/eur/ci/rs/c26473.htm.
4 IBM, “Raising the Iron Curtain,” Business Machines 42, no. 9 (September 1959): 5.
Electronic “Ambassador”: The Diplomatic Missions
167

Fig. 4. Patent drawing for RAMAC’s memory platters, ﬁled Oct. 13, 1959. Section and detail.
(Source: Johnson, Donald D., and Donald D. Johnson. “Composition of Epoxide Resin,” October
16, 1962)
168
E. Kotsioris

cleaners to TV sets and cake mixers5. IBM’s participation in the exhibition though was
clearly not targeting the consumerist sentiment of the average Russian individual, but
to provide a glimpse of American technological advancement.
On an allotted stand inside the Fair’s Glass Pavilion, IBM presented one of its latest
products: the Model C Electric Typewriter. During the exhibition, visitors were
allowed to get to experience up-close one of these state-of-the-art specimens of
American ofﬁce equipment. The centerpiece of IBM’s participation, though, was
undeniably the RAMAC 305 computer that had been air-shipped from San Jose,
California6 (Fig. 5). IBM’s computer was housed in a dedicated area at the internal
periphery of the Fair’s Dome that had been referred to programmatically as an “in-
formation machine”;7 a term most probably proposed by L.A.-based architects Charles
and Ray Eames who had used it a year before for a cartoon ﬁlm they had created for
IBM. For the USIA exhibition the Eameses also produced the multi-screen ﬁlm titled
Glimpses of the USA, which was projected on seven oversize screens suspended from
the Dome’s ceiling. In their own words, the fast paced ﬁlm that depicted thousands of
rapidly alternating scenes from life in the United States was devised as “a way of
compressing into a small volume a tremendous quantity of information.” As
Fig. 5. San Francisco Mayor George C. Christopher (right), IBM San Jose General Manager
Gavin A. Cullen, and Pat Lamson, a student at San Jose State college examine one of the 50
memory platters of the RAMAC 305 before its shipment from California to Moscow. (Source:
Business Machines, vol. 42, no. 8 (July 1959): 17)
5 On this observation see: Arthur J. Pulos, The American Design Adventure, 1940-1975 (MIT Press,
1988); Susan E. Reid, “Cold War in the Kitchen: Gender and the De-Stalinization of Consumer Taste
in the Soviet Union under Khrushchev,” Slavic Review 61, no. 2 (July 1, 2002): 211–52; David
Crowley and Jane Pavitt, eds., Cold War Modern: Design 1945-1970 (V&A Publishing, 2008); Greg
Castillo, Cold War on the Home Front: The Soft Power of Midcentury Design (Minneapolis:
University of Minnesota Press, 2010); Ruth Oldenziel and Karin Zachmann, eds., Cold War Kitchen:
Americanization, Technology, and European Users (Cambridge, Mass.; London: MIT Press, 2011).
6 “Destination: Moscow,” Business Machines 42, no. 8 (July 1959): 17.
7 Stanley Abercrombie, George Nelson: the design of modern design (Cambridge, Mass: MIT Press,
1995), 163–164; quoted in Colomina, “Enclosed by Images,” 18.
Electronic “Ambassador”: The Diplomatic Missions
169

architectural historian Beatriz Colomina has claimed, the Dome’s “architecture was
conceived from the very start as a combination of structure, multiscreen ﬁlm, and
computer;”8 or, one would say, a transmedia spectacle of the information age.
Commenting on this speciﬁc trip of IBM’s “cosmopolitan” computer, a short
feature in the July 1959 issue of Business Machines reads:
A trip abroad is nothing new for RAMAC, but its recent departure from San Francisco Inter-
national Airport marked the ﬁrst visit of the electronic “encyclopedia” to the world of the Iron
Curtain. … The machine will accept questions from Russian visitors, consult its memory, and
print out answers in Russian on one of four electric typewriters.9
Just three years after the Dartmouth Summer Research Project on Artiﬁcial Intel-
ligence in 1956, IBM’s computer is described as a jet-setting, knowledgeable,
Russian-speaking ambassador on a diplomatic “mission.”
During the opening day of the American exhibition, President of IBM’s World
Trade Corporation Arthur K. Watson demonstrated the RAMAC computer to then
American Vice President Richard Nixon and Soviet Premier Nikita Khrushchev during
their walk through the Fair’s grounds10 (Fig. 6). To showcase the machine’s impressive
capabilities, IBM came up with an engaging, hands-on performance. A backlit panel
listed 4,000 questions about life in the United States in Russian (Fig. 7). Once a visitor
had picked a question from the list to be asked, he or she could request it to be
“answered” by the RAMAC. A Russian-speaking IBM guide would then “input” the
question to the computer through a keyboard. Finally, the RAMAC would answer the
question and print it out as a keepsake of the “conversation” for the visitor.
Crowds of more perplexed rather than curious Soviet visitors swarmed around the
output devices of the system – four electric typewriters encased in bright red plastic.
For those unable to secure a spot in front of the printer to witness the uncanny moment
when the printer would start putting Cyrillic characters on paper to form answers that
“satisﬁed] the curiosity”11 of the onlookers, a zoom-lens camera ﬁxed right above the
printer ensured that answers simultaneously appeared on the overhead displays of a
closed-circuit television system in real time12 (Fig. 8). Even if the computer in reality
merely retrieved the corresponding answer from its hard disk, the whole demonstration
was set-up to give the impression of a machine capable of responding after “thinking,”
in line with Watson senior’s famous slogan for the company.
The demonstration of RAMAC’s technology was not only instantly mediatized as a
series of constantly ﬂeeting images, but also automatically inscribed on pieces of paper
that could act as transferrable reminders of this encounter between Soviet citizens and
American computers. With the help of a printer the printer of the RAMAC 305
transformed pieces of information into a written document. Acting as tangible proof of
this “conversation” between human and machine, tens of thousands of answers printed
8 Colomina, “Enclosed by Images,” 20.
9 “Destination: Moscow,” Business Machines vol. 42, no. 8 (July 1959): 17. Emphasis mine.
10 Conway Lloyd Morgan and Jack Masey, Cold War Confrontations: US Exhibitions and Their Role
in the Cultural Cold War (Baden: Lars Müller, 2008).
11 Here I am paraphrasing the title “Curiosity satisﬁed” in IBM, “Raising the Iron Curtain.”
12 “Destination: Moscow.”
170
E. Kotsioris

on sheets of accordion paper made their way to Soviet households in Moscow and
beyond, serving as enduring proof of an American technological achievement, and,
quite possibly, a source of resentment for the state of Soviet bureaucracy.
At the time RAMAC 305 was exhibited in Moscow, architect Eliot Noyes and
graphic designer Paul Rand were working to provide the sleek, modern image that IBM
became associated with during the 1960s and 1970s. Unlike the colorful Tupperware
system exhibited in the Glass Pavilion next door, the RAMAC system in the Fair’s
Fig. 6. “IBM World Trade President Arthur K. Watson [on the right of Khrushchev] describes
the operation of the RAMAC 305 to U.S. Vice President Richard Nixon and Soviet Premier
Nikita Khrushchev.” (Source: Business Machines, vol. 42, no. 9 (September 1959)).
Electronic “Ambassador”: The Diplomatic Missions
171

Fig. 8. Curious visitors at the IBM stand in the Dome. Some of them waiting for the RAMAC
305 to answer their questions; others hold their keepsake printouts. Photo: USIA. (Source:
Conway Lloyd Morgan and Jack Masey, Cold War Confrontations (Baden: Lars Müller, 2008),
245).
Fig. 7. Russian visitors going through the backlit panels of questions that the RAMAC 305
could answer. Notice the closed-circuit television screen at the top center-right displaying
answers being printed in real time. (Source: Life, v. 47, no. 6, (August 10, 1959)).
172
E. Kotsioris

Dome was housed in a sober set of trim grey steel casings designed by Noyes.13
Comprising a series of recombinant units, the RAMAC system allowed different ele-
ments of the computer system to be combined according to business needs and
available space. Thus, smaller businesses could install the most basic layout (which
consisted of the 380 console, the 350 disk unit, the 370 printer unit and the 323 punch
card), while businesses with more ﬂoor space available could install larger conﬁgu-
rations with more units. In principle, the RAMAC 305 was envisioned as a
site-responsive kit of parts whose recombinant capacity ensured both its functional and
aesthetic integrity. The modularity of this architecture enabled different parts of a large
system to be transferred around and changed according to need over time.
To design such custom systems, the IBM engineers at San Jose, used a collection of
reconﬁgurable, model building blocks to represent a variety of possible layouts. Dif-
ferent combinations of these blocks were subsequently photographed in order to choose
the most ﬁtting arrangement for each commercial client. The spatial articulation of the
RAMAC 305 as a modern machine was imagined through traditionally architectural
media and protocols.
The RAMAC 305 exhibit in Moscow could be interpreted as a spatial enclave
devised to present the commercialization of computing. Accordingly, the architectural
setup of the installation alluded to a modern ofﬁce interior, not dissimilar to the ones
Fig. 9. General overview of the overall layout of the IBM stand with the RAMAC 350 disk unit
(center, back), the 380 console (center, middle), four ofﬁce desks with electronic typewriters (right)
and crowds being given tours in Russian. (Source: Business Machines, v. 42, no. 9 (September
1959): 5)
13 John Harwood, The Interface: IBM and the Transformation of Corporate Design, 1945-1976
(University of Minnesota Press, 2011).
Electronic “Ambassador”: The Diplomatic Missions
173

found in contemporary corporate headquarters along 6th Avenue in New York.
Laid-out as freestanding furniture islands on a grid, the computer units deﬁned a
permeable open plan with corridors, as the ones produced by a standardized cubicle
system. Right in the center of this composition, the 350 disk unit and the 380 console
were constantly visible from almost any angle (Fig. 9). Seated on a revolving desk
chair with wheels, the human operator of the computer’s main console could effort-
lessly roll between visitors, the input keyboard and the printers. The dialog with the
RAMAC 305 was thus choreographed in space as an engaging negotiation between the
grey-colored computer and the Russian audience, mediated through IBM’s grey-attired
business representatives.
While serving as an “electronic ambassador” on display at the National American
Exhibition in Moscow, the RAMAC 305 was still as much an output as much as an
input system, collecting statistical data such as the total number of questions asked or
the most popular questions requested in random order as they occurred. What was
implied is that the processing of this large pool of data collected from Fair visitors
could help classify and quantify the most common preoccupations of Soviets about
American life and present them in a concise summary printout with mathematical,
“unbiased” precision. IBM and the USIA could eventually transfer all this collected
data that was magnetically stored on RAMAC’s hard disk back to the United States for
analysis.14
3
Act II: IBM Corporation Plant, San Jose, California, USA,
September 21, 1959
The second act of RAMAC’s diplomatic mission took place only a few weeks later, on
September 21, 1959. The architectural backdrop this time was situated back in Cali-
fornia, at the IBM Manufacturing and Administration Building in San Jose. Designed
by award-winning Berkeley, California architect John S. Bolles, the plant had only
been recently completed the year before. The modernist campus, with the single story
Building no. 25 devoted to Advanced Research and Development at its center, was the
manufacturing site of, or in IBM’s words “Home of” the RAMAC.15
Just as the disk unit of the RAMAC 305 exposed its revolving magnetic plates
through Noyes’s glass casing, Building no. 25 on the San Jose campus similarly
revealed its inner workings. Vast glazed surfaces provided sneak peaks of whatever
was happening in manufacturing spaces, laboratories and ofﬁces, even the workers’
cafeteria. Outside, a system of reﬂective pools, pedestrian bridges, manicured land-
scapes and sculptures reminded little of typical factories in the Bay area. Resembling
more a seaside holiday resort on Santa Cruz than an industrial plant, IBM used
Building no. 25 as the new architectural face of the company. Invoking a seductive
blurring between work and leisure time, colorful depictions of the San Jose campus in
IBM’s brochures and advertisements explicitly pointed out that this architecture had
14 IBM, “Raising the Iron Curtain.”
15 “Crowds Engulf Visiting Russians on IBM Tour,” Business Machines 42, no. 10 (n.d.): 4.
174
E. Kotsioris

pretty much arrived from the future. In one of these brochures titled “One of America’s
10 most beautiful plants,” a graphic inversion of the last two digits of the completion
date of the plant (from 1958 to 1985) alluded to an architecture that had travelled back
in time from the future to meet the present.
During the same month, Nikita Khrushchev had embarked on a 12-day diplomatic
trip through the United States, making major stops in New York, Los Angeles, San
Francisco and Washington among other cities. Khrushchev strategically arrived to the
U.S. just a week after the Soviet spacecraft Luna-2 became the ﬁrst human-made object
to impact the surface of the moon on September 14, 1959.16 Shortly after a failed
attempt to visit Disneyland in Anaheim California, Khrushchev also made a stop in San
Jose. The invitation had come from Thomas J. Watson Jr. himself, who reasoned to the
Department of State that “a visit to an IBM plant might help the Russians understand
America and Americans better and thus contribute to the cause of world peace.”17 After
having witnessed the exposure that IBM had received through its participation in the
USIA exhibition in Moscow a couple of months earlier and the publicity frenzy that
closely followed Khrushchev’s trail on American ground, Watson Jr. was cunning
enough to foresee that even the briefest stop-over of the Soviet Premier in San Jose was
free publicity for the company, both in the US and the Soviet Union.
Khrushchev’s visit resulted in nationwide headlines from local newspapers – from
The San Jose Mercury-News, to the cover of The New York Times – as well as
coverage by national television networks such as NBC.18 IBM’s own journal even
devoted its whole October 1959 issue to the coverage of the event. The surge of dozens
of press representatives, newspapermen and TV reporters, coincidentally provided a
last-minute opportunity for IBM to set-up a special battery of IBM typewriters, similar
those exhibited in Moscow.19
If diplomatic obstacles were eventually surpassed, the Soviet Union could at some
later point become a key among the several international business clients of IBM.
Considering its sheer geographical size, its vast array of bureaucratic institutions and
manufacturing sector as well as its practically non-existent computer industry, the
Soviet Union could potentially become one of the most proﬁtable additions in IBM’s
multinational clientele through IBM World Trade Corporation.20 Bookkeeping of all
sorts was IBM’s expertise and the Soviet Union could have its own RAMAC 305 s
permanently shipped to Moscow for a “mere” $189,000 each.21 If such an agreement
16 Often referred to as “Lunik” or “Lunnik” by Western press.
17 Thomas Watson Jr., “A Letter from the President,” Business Machines 42, no. 10 (October 1959): 2.
18 See, for example: “Khrushchev Hails Business Men in Visit to California Factory; President Again
Asks Courtesy,” The New York Times, September 22, 1959.
19 “Press and TV Coverage,” Business Machines 42, no. 10 (October 1959): 14–15.
20 IBM World Trade Corporation was a subsidiary of IBM established in 1949 that was occupied with
the company’s commercial activity abroad.
21 Alternatively, one could rent a RAMAC for a month at the time. “Khrushchev Sees Huge IBM
Plant,” The Spokesman-Review, September 22, 159AD.
Electronic “Ambassador”: The Diplomatic Missions
175

was to blossom, IBM would see proﬁts from its international sales rise well above the
1/5 of its total revenue at the time, which was primarily due to the recovering European
market.22
Watson Jr. had arranged for a 20-minute presentation of the RAMAC 305, lunch
with the Soviet Premier in the plant’s cafeteria, and a tour through the assembly line of
the computer (Fig. 10). The RAMAC’s updated demonstration, “coincidentally”
included the ability to recite key historical event that paved the formation of the Soviet
Fig. 10. Nikita S. Khrushchev walking through the RAMAC assembly line with his entourage.
Photo: Nat Farbman. (Source: Life image archive)
22 “Transition,” Newsweek (November 22, 1937): 5; quoted in Jr and Zeiler, Globalization and the
American Century, 82.
176
E. Kotsioris

Union, such as the Russian Revolution.23 The message that had to be successfully
communicated during the short duration of the demonstration was that, despite their
American origin, IBM computers were ﬂexible and fast learners; not only capable of
assimilating new languages, such as Russian, but also adaptable enough to handle all
kinds of references and information. And while individuals like Watson and Khrush-
chev might have conﬂicting ideologies and views, the computing “ghost” in this
machine was implied to be blind to ideology.24
Recalling his discussion with Khrushchev through the assembly line, Watson Jr.
wrote in his memoirs:
…when we toured the plant, Khrushchev said, “We have plants like this in the Soviet Union.”
Then he looked a little puzzled and said, half to himself, “We must have plants like this in the
Soviet Union.” Why [his interpreter, Viktor] Sukhodrev didn’t leave that one untranslated I
never knew.25
In a conﬁdential memorandum regarding this conversation between Khrushchev
and the American delegation the Department of State interpreter to President Eisen-
hower, Alexander Akalovsky, later mentioned:
On our way back from San Jose, Khrushchev commented on the excellent IBM plant, but said
that computers were very highly developed in the Soviet Union too; such things as A bombs or
the H bomb could have never been developed in the Soviet Union if it hadn’t had highly
complicated and sophisticated computers.26
Even though digital electronic computing in the USSR at the time was still at its
infancy, Khrushchev’s exaggerated claims reinforced to one of the biggest American
fears since the conﬁrmation of Soviet espionage during the Manhattan Project: that the
Soviets were possibly also advancing in digital computing beyond the phase of pro-
totyping. After returning to San Francisco a few hours after his tour, Khrushchev
reformulated his response, to state:
23 Thomas J. Watson recalled later: “The computer demonstration we had planned for Khrushchev was
pretty dramatic. We had the RAMAC programmed to work like an electronic history book. …if you
said 1917 it would reply ‘The Russian Revolution.’ This demonstration was dear to my heart
because I’d thought it up myself.” Peter Petre, Alfred D Chandler, and Thomas J. Watson, Father,
Son & Co.: My Life at IBM and beyond (New York: Bantam Books, 1990), 328.
24 The term “Ghost in the machine” was coined by philosopher Gilbert Ryle in his 1949 book The
Concept of Mind to describe the Cartesian dualism of body and mind. Gilbert Ryle, The Concept of
Mind (London; New York, NY: Hutchinson’s University Library, 1949).
25 Thomas J. Watson Jr., Father, Son & Co.: My Life at IBM and Beyond (New York: Bantam Books,
1990), 330. Emphasis in the original.
26 Alexander Akalovsky, trans., “Foreign Relations of the United States, 1958-1960. Volume X, Part I,
Eastern Europe Region, Soviet Union, Cyprus. Document 112: Memorandum of Conversation.
Conference Files: Lot 64 D 560, CF 1474. Conﬁdential.” (U.S. Department of State, September 21,
1959).
Electronic “Ambassador”: The Diplomatic Missions
177

The plant we saw was making computers, I’m no specialist in the matter, and any assessment
that I were to give of the plant would be insigniﬁcant; but I suppose we also produce machines
like that. I don’t know who makes the better machines; that, of course, is a question. I saw the
machines, but, of course, I don’t understand the actual substance of the matter. Perhaps, ours are
better; I don’t know…27
By the time Khrushchev ﬁnally got to dictate his own memoirs, any reference to the
computers showed in IBM’s plant had completely evaporated. Sergei Khrushchev,
Nikita’s son who accompanied him throughout the trip in North America and the visit
to IBM’s plant, has also noted repeatedly that his father’s fascination in San Jose lay
elsewhere. He commented:
Father was staggered by the IBM cafeteria much more than by its computers. In 1959 the idea
of self-service had not reached our country. […] After returning, to Moscow, Father ordered
that food service be organized the IBM model. Without tablecloths and without waiters. That
innovation alone would save a great deal of money if applied country-wide.28
Khrushchev was more impressed by the view of sliding trays across a horizontal
rack at the restaurant, rather than the spinning memory platters along a vertical spindle
in the factory (Fig. 11). And it is no coincidence that the self-service layout of the
cafeteria constituted the single “IBM technology,” that could be most immediately
Fig. 11. Thomas Watson Jr. and Nikita S. Khrushchev laughing at the self-service cafeteria of
the IBM plant in San Jose, California. Photo: Carl Mydans. (Source: Life image archive)
27 “After He Left,” Business Machines 42, no. 10 (October 1959): 15.
28 Sergei N. Khrushchev, Nikita Khrushchev and the Creation of a Superpower, trans. Shirley Benson
(Penn State Press, 2001), 334.
178
E. Kotsioris

transferred back to the USSR, as the reverse engineering of machines like the RAMAC
clearly called for a whole different kind of expertise.
4
Denouement: Reconnaissance in Public
Until the participation of the accession of the USSR to the Strasbourg agreement on
International Patent Classiﬁcation on March 24, 1971, the Ofﬁce of Export Control and
the NATO Coordinating Committee severely restricted companies IBM from making
commercially available abroad products of American technology.29 It was only later
that year, in October 1971 that IBM participated for the ﬁrst time with its own pavilion
in a Soviet-organized computer trade exhibition. Between October 6-17, IBM exhibited
its new Selectric typewriters and the iconic System/360, at the exhibition Siste-
motekhnika ‘71 in Leningrad30 (Fig. 12). IBM’s 11-day long presence resulted into a
great commercial success; it was sealed with a pre-arranged Soviet order of an IBM
360 Model 50 conﬁrmed just a month later.31
Fig. 12. IBM’s pavilion at the exhibition Sistemotekhnika-71 in Leningrad. “A Russian tries his
hand at an IBM Selectric typewriter.” Notice the base of the desk inspired from Saarinen’s tulip
table and the transliteration of IBM’s logo in Cyrillic on the right. Photo: Jane P. Cahill. (Source:
“A Fair Day at Leningrad.” Think 37, no. 10 (November 1971): 9)
29 Patent Ofﬁce, “Soviets Assure Patent Protection For American Technology Marketing,” Commerce
Today II, no. 4 (November 29, 1971): 18–19.
30 “A Fair Day at Leningrad,” Think 37, no. 10 (November 1971): 8–9.
31 “The Russians Were Here,” Think, June 1972, 40–41.
Electronic “Ambassador”: The Diplomatic Missions
179

In the sequence of events that paved the way to such commercial agreements only
twelve years after the American National Exhibit in Moscow, IBM’s RAMAC 305 led
a protagonist’s role. Unlike a lot of earlier mainframe computers, the RAMAC was
deliberately designed – with the help of Noyes – as a modular system of combinable
components that could be easily transported around the world and change the ways in
which traditional business and planning was done. In all the instances of its promotion
discussed here, IBM portrayed it as a thinking machine with a distinct personality; an
agent that could engage in a discussion, learn and speak new languages, possess a
historical memory and assume the role of the diplomat for a nation by travelling around
the world.
Within the cultural exchanges of the Cold War that started with the 1959 USIA
exhibit in Moscow, the RAMAC 305 was rationalized as an “ambassador” that
“communicated with more Soviet citizens than any other American representative”32;
an ambassador whose trips contributed to the ultimate participation of the USSR in
global treatises on the fair use of patented technologies. Ultimately, the designed
portability of RAMAC’s individual units enabled IBM’s computers to circulate as both
physical objects and imaginaries; or, as both tangible artifacts and powerful
mythologies.
However, unlike the self-service system of a cafeteria, the technology embedded in
the RAMAC 305 was not one that could be easily replicated.33 As an intricate piece of
research and development, the RAMAC 305 could not be simply reverse-engineered.
In that respect, it was a technology that could only be truly transferred from the United
States to the Soviet Union through commercial purchase contracts. And whatever
technological breakthroughs the RAMAC 305 might have absorbed and represented
were utterly secondary compared to its ability to feed the Cold War discourse of
technological supremacy between the two superpowers; a discourse that largely thrived
on the myths constructed around objects of technology – such as the Luna-2 satellite or
the RAMAC 305 – rather than their actual capabilities.
32 James Schwoch, “A Record of Some Kind in the History of International Communication,” in
Global TV: New Media and the Cold War, 1946-69 (Urbana; Chicago: University of Illinois Press,
2009), 94.
33 On the close eye that IBM kept on the emerging Soviet computer industry during the Brezhnev
years, see this report published in IBM’s own journal: Leo Gruliow, “BESM-6. DNEPR.
MINSK-22. Codes? Nyet. Computer? Da!,” Think, August 1971, 37–41.
180
E. Kotsioris

The Birth of Artiﬁcial Intelligence: First Conference
on Artiﬁcial Intelligence in Paris in 1951?
Herbert Bruderer
(✉)
ETH Zurich, Zurich, Switzerland
bruderer@retired.ethz.ch, herbert.bruderer@bluewin.ch
Abstract. The 1956 Dartmouth conference is often considered as the cradle of
artiﬁcial intelligence. There is a controversy on its origin. Some historians of
computing believe that Turing or Zuse were the fathers of machine intelligence.
However, the ﬁrst working chess-playing automaton was developed by Torres
Quevedo by 1912. Moreover, there was a large and important (but forgotten)
European conference on computing and human thinking in Paris in 1951.
Keywords: Paris conference on artiﬁcial intelligence · Spanish chess automaton
by Torres Quevedo · Turing’s impact
1
1951 Paris International Computer Conference
There were three conferences on calculating machines in Britain in 1949 (University of
Cambridge), 1951 (University of Manchester) and 1953 (National Physical Laboratory,
London). However, the most important early European computer conference took place
in Paris in 1951. This international congress organized by the Centre National de la
Recherche Scientiﬁque (CNRS) is almost forgotten despite its famous participants. The
35 papers were translated into French which seems to be quite unique. The reason why
this international meeting is nearly unknown is probably the fact that the voluminous
proceedings (589 pages) were published only in French, they do not contain English
essays. The gathering combined computing machines and human thinking. Its mean‐
ingful title was «Les machines à calculer et la pensée humaine.» [1].
2
Famous Participants from Europe and USA
Among the participants from ten countries were:
• Aiken (Harvard)
• Ashby (Gloucester)
• Booth (London)
• Bowden (Manchester)
• Colebrook (London)
• Couﬃgnal (Paris)
© IFIP International Federation for Information Processing 2016
Published by Springer International Publishing AG 2016. All Rights Reserved
A. Tatnall and C. Leslie (Eds.): HC 2016, IFIP AICT 491, pp. 181–185, 2016.
DOI: 10.1007/978-3-319-49463-0_12

• Hartree (Cambridge)
• Kilburn (Manchester)
• McCulloch (Chicago)
• Picone (Rom)
• Stiefel (Zurich)
• Torres Quevedo (Madrid)
• Uttley (London)
• van Wijngaarden (Amsterdam)
• Walter (Bristol)
• Walther (Darmstadt)
• Wiener (Cambridge, Mass.)
• Wilkes (Cambridge)
• Williams (Manchester)
• Womersley (Letchworth) [2].
3
Demonstration of Several Automata
There were several demonstrations: automatic chess player (Fig. 1), Telekine and analog
calculator, all operated by Leonardo Torres Quevedo’s son Gonzales, artiﬁcial animals
by W. Grey Walter, Homeostat by W. Ross Ashby [14].
Both the chess automata still exist in Madrid. The Austrian pioneer Heinz Zemanek
(transistorized computer Mailüfterl) who died in 2015 played against the second version
at the world fair of Brussels in 1958 [17].
4
Papers on Computers and Human Brain
Many pioneers presented their relay and electronic computers, for example: Howard
Aiken (Harvard University): Mark IV, Maurice Wilkes (University of Cambridge):
EDSAC, Frederic Williams (University of Manchester): Manchester Mark, Francis
Morley Colebrook (National Physical Laboratory, London): ACE. Göran Kjellberg
(Sweden) and Eduard Stiefel (Switzerland) reported on the ﬁrst continental European
computers (Bark and Zuse Z4) [1–4].
Paul Chauchard, Louis Couffignal, L. Delpech, A. Fessard, Henri Gastaut,
Warren S. McCulloch/Walter H. Pitts, W. Grey Walter, and Norbert Wiener lectured
on the relations between the machine and the human brain [5–11], [13–16].
5
Turing’s Revolution?
It is remarkable that Alan Turing did not attend the Paris conference. At this time he
was living in Manchester (after leaving the ACE project in London). There is a new
182
H. Bruderer

book entitled “Turing’s revolution” (Springer 2015). In 1951, 15 years after his 1936
paper on the universal Turing machine, Turing’s work was almost unknown. His impact
on the building of the early stored-program computers was rather small. In a leading
European book on program controlled electronic digital computers (Rutishauser, Heinz;
Speiser, Ambros Paul; Stiefel, Eduard: Programmgesteuerte digitale Rechengeräte
(elektronische Rechenmaschinen), Birkhäuser Verlag, Basel 1951) Turing’s universal
machine is not even mentioned [1, 12].
Arnold A. Cohen writes in his introduction to a signiﬁcant historical book: “Although
the ERA book may have been the only one of its kind published in America, it was not
alone in the world at the time. Rutishauser, Speiser, and Stiefel’s comprehensive review
“Program controlled digital computing devices (Electronic computing machines)”,
appeared in four installments (1950–1951) in the Swiss journal Zeitschrift für ange‐
wandte Mathematik und Physik. This excellent tutorial, with its extensive bibliography,
was not widely available in the U.S.” (see Engineering Research Associates, Inc.: High-
speed computing devices, Tomash publishers, Los Angeles, San Francisco 1983,
page XIX) [1, 2].
At the Paris conference, Francis Morley referred to Turing’s abstract universal
machine but he did not recognize a connection between the stored-program concept and
modern electronic digital computers. Obviously, there was no Turing revolution
concerning the construction of stored-program computers.
6
Conclusions
Was the Dartmouth Summer Research Project on Artiﬁcial Intelligence (John McCarthy,
Marvin L. Minsky, Nathaniel Rochester, Claude E. Shannon) which took place in 1956
at Dartmouth College in Hanover, New Hampshire the ﬁrst major event for AI? This
seems rather doubtful. The Paris congress, sponsored by the Rockefeller Foundation,
included several contributions on digital and human computers [5–11], [13–16].
The Birth of Artiﬁcial Intelligence
183

Fig. 1. The Spanish engineer Gonzales Torres Quevedo presented his father’s chess machine El
ajedrecista at the Paris conference on computers and human thinking. Norbert Wien played on
12 or 13 January 1951 against the automaton (picture from Vernon Pratt: Thinking machines. The
evolution of artiﬁcial intelligence, Basil Blackwell Ltd, Oxford 1987, page 191, original source
unknown).
Acknowledgment. The author is grateful for discussions with Jürgen Schmidhuber.
References
1. Bruderer, H.: Meilensteine der Rechentechnik. Zur Geschichte der Mathematik und der
Informatik (Milestones in Analog and Digital Computing. Contributions to the History of
Mathematics and Information Technology), de Gruyter, Berlin/Boston, 850 p (2015)
2. Bruderer, H.: Computing history outside UK and USA: some selected landmarks from
continental Europe. In: Communications of the ACM (forthcoming)
3. Bruderer, H.: Early history of computing in Switzerland: discovery of rare devices, unknown
documents, and scarcely known facts. In: IEEE Annals of the History of Computing (2017,
forthcoming)
4. Bruderer, H.: Konrad Zuse und die Schweiz. Wer hat den Computer erfunden? (Konrad Zuse
and Switzerland. Who invented the computer?), 250 p. de Gruyter, Berlin/Boston (2012)
184
H. Bruderer

5. Chauchard, P.: La commande centrale de la machine nerveuse. In: Pérès, J. (ed.) Les machines
à calculer et la pensée humaine, Paris, 8–13 janvier 1951, Colloques internationaux du Centre
national de la recherche scientiﬁque, Nr. 37, pp. 531–536. Editions du Centre national de la
recherche scientiﬁque (CNRS), Paris (1953)
6. Couffignal, L.: Quelques analogiques nouvelles entre structures de machines à calculer et
structures cérébrales. In: Pérès, J. (ed.) Les machines à calculer et la pensée humaine, Paris, 8–13
janvier 1951, Colloques internationaux du Centre national de la recherche scientifique, Nr. 37,
pp. 549–562. Editions du Centre national de la recherche scientifique (CNRS), Paris, (1953)
7. Delpech, L.: Perspectives psychologiques et machines à penser. In: Pérès, J. (ed.) Les
machines à calculer et la pensée humaine, Paris, 8–13 janvier 1951, Colloques internationaux
du Centre national de la recherche scientiﬁque, Nr. 37, pp. 539–548. Editions du Centre
national de la recherche scientiﬁque (CNRS), Paris (1953)
8. Fessard, A.: Sur un, défaut“ propre à la machine nerveuse. In: Pérès, J. (ed.) Les machines à
calculer et la pensée humaine, Paris, 8–13 janvier 1951, Colloques internationaux du Centre
national de la recherche scientiﬁque, Nr. 37, pp. 517–528. Editions du Centre national de la
recherche scientiﬁque (CNRS), Paris (1953)
9. Gastaut, H.: Les machines à calculer et le cerveau humain. In: Pérès, J. (ed.) Les machines à
calculer et la pensée humaine, Paris, 8–13 janvier 1951, Colloques internationaux du Centre
national de la recherche scientiﬁque, Nr. 37, pp. 447–459. Editions du Centre national de la
recherche scientiﬁque (CNRS), Paris (1953)
10. Culloch, W.S.M., Lettvin, J.Y., Pitts, W.H., Dell, P.C.: Une comparaison entre les machines à
calculer et le cerveau. In: Pérès, J. (ed.) Les machines à calculer et la pensée humaine, Paris, 8–
13 janvier 1951, Colloques internationaux du Centre national de la recherche scientifique, Nr. 37,
pp. 425–443. Editions du Centre national de la recherche scientifique (CNRS), Paris (1953)
11. Pérès, J. (ed.): Les machines à calculer et la pensée humaine, Paris, 8–13 janvier 1951,
Colloques internationaux du Centre national de la recherche scientiﬁque, Nr. 37, vol. XIX,
570 p. Editions du Centre national de la recherche scientiﬁque (CNRS), 581 p. Paris (1953)
12. Rutishauser, H., Speiser, A.P., Stiefel, E.: Programmgesteuerte digitale Rechengeräte
(elektronische Rechenmaschinen), 102 p. Birkhäuser, Basel (1951)
13. Torres-Quevedo, G.: Les travaux de l’École espagnole sur l’automatisme. In: Pérès, J. (ed.)
Les machines à calculer et la pensée humaine, Paris, 8–13 janvier 1951, Colloques
internationaux du Centre national de la recherche scientiﬁque, Nr. 37, pp. 361–381. Editions
du Centre national de la recherche scientiﬁque (CNRS), Paris (1953)
14. Torres-Quevedo, G.: Présentation des appareils de Leonardo Torres-Quevedo. In: Pérès, J.
(ed.) Les machines à calculer et la pensée humaine, Paris, 8–13 janvier 1951, Colloques
internationaux du Centre national de la recherche scientiﬁque, Nr. 37, pp. 383–406. Editions
du Centre national de la recherche scientiﬁque (CNRS), Paris (1953)
15. Walter, W.G.: Réalisation mécanique de modèles de structure cérébrale. In: Pérès, J. (ed.) Les
machines à calculer et la pensée humaine, Paris, 8–13 janvier 1951, Colloques internationaux
du Centre national de la recherche scientiﬁque, Nr. 37, pp. 407–420. Editions du Centre
national de la recherche scientiﬁque (CNRS), Paris (1953)
16. Wiener, N.: Les machines à calculer et la forme (Gestalt). In: Pérès, J. (ed.) Les machines à
calculer et la pensée humaine, Paris, 8–13 janvier 1951, Colloques internationaux du Centre
national de la recherche scientiﬁque, Nr. 37, pp. 461–463. Editions du Centre national de la
recherche scientiﬁque (CNRS), Paris (1953)
17. Zemanek, H.: Central European prehistory of computing. In: Metropolis, N.C., Howlett, J.,
Rota, G.-C. (eds.) A History of Computing in the Twentieth Century, A collection of essays,
pp. 587–609. Academic press, New York (1980)
The Birth of Artiﬁcial Intelligence
185

The World’s Smallest Mechanical Parallel Calculator:
Discovery of Original Drawings and Patent Documents
from the 1950s in Switzerland
Herbert Bruderer
(✉)
ETH Zurich, Zurich, Switzerland
bruderer@retired.ethz.ch, herbert.bruderer@bluewin.ch
Abstract. 
The Austrian engineer Curt Herzstark invented the world-
renowned mechanical pocket calculator Curta. The books and the papers on
the history of computing mention two models, Curta 1 and Curta 2. In
November 2015 original drawings and patent documents on a multiple Curta
were discovered in Switzerland. This device is supposed to be the world’s
smallest mechanical parallel calculator.
Keywords: History of computing · Pocket-sized mechanical parallel
calculator · Discovery of drawings · Documents on a multiple Curta
1
Herzstark’s Legacy: Discovery of Unknown Machine
On 14 November 2015 I examined the legacy of Curt Herzstark. It is located at the
typewriter museum Beck at Pfäﬃkon near Zurich. To my great surprise I found original
drawings and patent documents of an unknown multiple calculating machine: the
world’s smallest mechanical parallel calculator.
Curt Herzstark, a highly gifted Austrian engineer, was forced by the Nazis to
design the construction drawings for the world-famous Curta at the concentration
camp of Buchenwald. The Curta is the smallest and most successful mechanical
calculating machine which is able to perform all four basic arithmetic operations.
Today, these pretty “pepper mills” are still working. They inspire specialists as well
as laymen. From 1947 to 1971 this engineering marvel was produced in large quan‐
tities at Mauren (Liechtenstein). Up to now we knew two marks of the stepped drum
machine, the Curta 1 and the Curta 2.
According to the patent documents recently turned up any number of mechanical
cylindrical calculators may be combined. Herzstark describes the following forms:
• two Curtas side by side,
• two Curtas one upon another,
• four Curtas side by side,
• ﬁve Curtas in a circle.
© IFIP International Federation for Information Processing 2016
Published by Springer International Publishing AG 2016. All Rights Reserved
A. Tatnall and C. Leslie (Eds.): HC 2016, IFIP AICT 491, pp. 186–192, 2016.
DOI: 10.1007/978-3-319-49463-0_13

2
Characteristic Features of the Multiple Calculating Machine
The parallel calculator excels in the following properties:
• All machines are driven by one single, common crank.
• All devices may be switched (by axial shifting of the driving shaft) in common or
alone to another basic arithmetic operation. This is possible even if the individual
machines are set up for diﬀerent arithmetic operations.
• All registers may be shifted (decade by decade) in common or alone in both rotating
directions.
• All result registers and/or revolution counters may be cleared in common or alone.
• The multiple calculation machine can be equipped with an electric drive.
3
Double, Fourfold and Fivefold Curta
The extraordinary ﬁnd proves that the Viennese inventor did not only design single but
also multiple Curtas (Figs. 1, 2, 3 and 4).
In order to simplify the handling the Austrian engineer designed another type of
construction for of the double Curta with a base plate (additional patent) (Fig. 2).
The patent application does not mention the following variation with four cylindrical
machines (Fig. 3).
4
Acceleration of the Arithmetic Operations
The high-precision Curta masters all four elementary arithmetic operations. Thanks to
the linkage of several devices computations are considerably speeded up. For instance,
if you use a fourfold Curta you input the values 137, 263, 389 and 491. Then you multiply
all factors with 7. The four multiplications are carried out at the same time (with 7
rotations of the crank). The numbers 623, 511, 301 and 259 can be divided by 7 all at
once.
Potential applications are the simultaneous conversion of a price list referring to
various rates of exchange, the determination of the coordinates x and y in surveying at
one blow, and computing at the same time the prices by the piece of goods.
5
The World’s Smallest Mechanical Parallel Calculator
To my knowledge the multiple Curta is the smallest mechanical calculating machine in
the world, the ﬁrst pocket-sized mechanical parallel calculator.
The patent documentation contains comprehensive descriptions. Curt Herzstark
presumably built some prototypes. I conducted several oral history interviews with six
elderly eye witnesses in Liechtenstein, Austria, Germany and Switzerland. Unfortu‐
nately they do not remember the inventor’s corresponding research work. The multiple
The World’s Smallest Mechanical Parallel Calculator
187

calculating machine was not mass-produced. Herzstark who was defrauded of his life‐
work by the Nazis had already left his manufacturing ﬁrm Contina AG at this time.
Fig. 1. Double Curta 1. The drawing shows a rigid rod-shaped connection of both machines.
According to the patent application more than two devices may be piled up (© Österreichisches
Patentamt, Wien, Stammpatent 1954/57).
188
H. Bruderer

Fig. 2. Double Curta 2. The original drawing presents two mechanical pocket calculators situated
side by side. The crank moves the main driving shafts of both devices through bevel and spur
wheels. An alternative is a chain drive (© Schreibmaschinenmuseum Beck, Pfäﬃkon ZH/Bruderer
Informatik, CH-9401 Rorschach 2016).
The World’s Smallest Mechanical Parallel Calculator
189

Fig. 3. Fourfold Curta. This original drawing depicts four removable Curtas mounted on a
pedestal. The latter is provided with coupling, drive and control organs. The common crank is
connected to the stepped drum shafts by a chain drive (© Schreibmaschinenmuseum Beck,
Pfäﬃkon ZH/Bruderer Informatik, CH-9401 Rorschach 2016).
190
H. Bruderer

Fig. 4. Fivefold Curta. The ﬁve pocket calculating machines stand on a revolving pedestal which
is attached on a circular base plate. Thus the setting mechanisms of the ﬁve devices can be
comfortably manipulated. The principal driving shaft (in the middle) with the crank is connected
through spur wheels with the ﬁve stepped drum shafts (© Österreichisches Patentamt, Wien,
Stammpatent 1954/57).
The World’s Smallest Mechanical Parallel Calculator
191

6
Notes
Original drawings and patent documents of Curt Herzstark, Schreibmaschinenmuseum
Beck, Pfäﬃkon ZH, and European patent oﬃce, Munich (database). There are the
following Austrian patent speciﬁcations concerning Curt Herzstark’s multiple calcu‐
lating machines:
• Patentschrift Nr. 195 147 (25 January 1958, basic patent):
patent application (19 October 1954),
patent grant (15 May 1957),
• Patentschrift Nr. 205 775 (10 October 1959, additional patent):
patent application (15 December 1954),
patent grant (15 March 1959).
The ﬁrst Austrian patent application for the multiple calculator already stems from
20 December 1949. There were also preparations for an US patent. Former Austrian
patents about the conventional Curta bear the following numbers 747 073/192, 747
074/191, 166 581, 163 380.
• Bruderer, H.: Meilensteine der Rechentechnik. Zur Geschichte der Mathematik und
der Informatik (Milestones in Analog and Digital Computing. Contributions to the
History of Mathematics and Information Technology), de Gruyter, Berlin/Boston
2015, 850 pages
• Bruderer, H.: Multiple Curtas, in: CBI Newsletter, Spring 2016, pp. 21–23 (Charles
Babbage Institute, Minneapolis)
• Bruderer, H.: Computing History outside UK and USA: some selected Landmarks
from Continental Europe, in: Communications of the ACM (2017, forthcoming).
• Bruderer, H.: Early history of computing in Switzerland: discovery of rare devices,
unknown documents, and scarcely known facts. In: IEEE Annals of the History of
Computing (2017, forthcoming)
Acknowledgment. The author is very grateful to Stefan Beck from the typewriter museum in
Pfäﬃkon near Zurich.
192
H. Bruderer

Author Index
Ainsworth, Barbara
23
Avram, Chris
23
Bruderer, Herbert
181, 186
Cignoni, Giovanni A.
101
Clarke, Neil
23
Cossu, Giovanni A.
101
Davey, Bill
87, 115
Dittmann, Frank
49
Houghton, Robert F.
115
Johnson, Roger G.
58
Kotsioris, Evangelos
165
Leslie, Christopher
122
Lewis, Nicholas
1
Schmitt, Martin
141
Sheard, Judy
23
Shilov, Valery V.
71
Silantiev, Sergey A.
71
Tatnall, Arthur
87


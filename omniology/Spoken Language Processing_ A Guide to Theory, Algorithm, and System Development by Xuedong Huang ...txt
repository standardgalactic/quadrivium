

i
TABLE OF CONTENTS
1. INTRODUCTION................................................................................................... 1
1.1.
MOTIVATIONS .......................................................................................................2
1.1.1.
Spoken Language Interface ...................................................................2
1.1.2.
Speech-to-speech Translation................................................................3
1.1.3.
Knowledge Partners...............................................................................3
1.2. SPOKEN LANGUAGE SYSTEM ARCHITECTURE ........................................................4
1.2.1.
Automatic Speech Recognition ..............................................................4
1.2.2.
Text-to-Speech Conversion....................................................................6
1.2.3.
Spoken Language Understanding..........................................................7
1.3. BOOK ORGANIZATION ............................................................................................9
1.3.1.
Part I: Fundamental Theory..................................................................9
1.3.2.
Part II: Speech Processing ....................................................................9
1.3.3.
Part III: Speech Recognition ...............................................................10
1.3.4.
Part IV: Text-to-Speech Systems..........................................................10
1.3.5.
Part V: Spoken Language Systems ......................................................10
1.4. TARGET AUDIENCES.............................................................................................11
1.5. HISTORICAL PERSPECTIVE AND FURTHER READING.............................................11
PART I: FUNDAMENTAL THEORY
2. SPOKEN LANGUAGE STRUCTURE .........................................................19
2.1. SOUND AND HUMAN SPEECH SYSTEMS................................................................21
2.1.1.
Sound ...................................................................................................21
2.1.2.
Speech Production...............................................................................24
2.1.3.
Speech Perception................................................................................28
2.2. PHONETICS AND PHONOLOGY...............................................................................36
2.2.1.
Phonemes.............................................................................................36
2.2.2.
The Allophone: Sound and Context .....................................................47
2.2.3.
Speech Rate and Coarticulation ..........................................................49
2.3. SYLLABLES AND WORDS......................................................................................50
2.3.1.
Syllables...............................................................................................51
2.3.2.
Words...................................................................................................52
2.4. SYNTAX AND SEMANTICS.....................................................................................57
2.4.1.
Syntactic Constituents..........................................................................58
2.4.2.
Semantic Roles.....................................................................................63
2.4.3.
Lexical Semantics ................................................................................64
2.4.4.
Logical Form .......................................................................................66
2.5. HISTORICAL PERSPECTIVE AND FURTHER READING.............................................68

ii
TABLE OF CONTENTS
3. PROBABILITY, STATISTICS AND INFORMATION THEORY ..73
3.1. PROBABILITY THEORY .........................................................................................74
3.1.1.
Conditional Probability And Bayes' Rule............................................75
3.1.2.
Random Variables................................................................................77
3.1.3.
Mean and Variance..............................................................................79
3.1.4.
Covariance and Correlation................................................................83
3.1.5.
Random Vectors and Multivariate Distributions.................................84
3.1.6.
Some Useful Distributions ...................................................................85
3.1.7.
Gaussian Distributions ........................................................................92
3.2. ESTIMATION THEORY...........................................................................................98
3.2.1.
Minimum/Least Mean Squared Error Estimation................................99
3.2.2.
Maximum Likelihood Estimation .......................................................104
3.2.3.
Bayesian Estimation and MAP Estimation ........................................108
3.3. SIGNIFICANCE TESTING.......................................................................................114
3.3.1.
Level of Significance..........................................................................114
3.3.2.
Normal Test (Z-Test)..........................................................................116
3.3.3.
2
χ Goodness-of-Fit Test...................................................................117
3.3.4.
Matched-Pairs Test............................................................................119
3.4. INFORMATION THEORY ......................................................................................121
3.4.1.
Entropy ..............................................................................................121
3.4.2.
Conditional Entropy ..........................................................................124
3.4.3.
The Source Coding Theorem .............................................................125
3.4.4.
Mutual Information and Channel Coding..........................................127
3.5. HISTORICAL PERSPECTIVE AND FURTHER READING...........................................129
4. PATTERN RECOGNITION...........................................................................133
4.1. BAYES DECISION THEORY..................................................................................134
4.1.1.
Minimum-Error-Rate Decision Rules ................................................135
4.1.2.
Discriminant Functions .....................................................................138
4.2. HOW TO CONSTRUCT CLASSIFIERS.....................................................................140
4.2.1.
Gaussian Classifiers ..........................................................................142
4.2.2.
The Curse of Dimensionality..............................................................144
4.2.3.
Estimating the Error Rate..................................................................146
4.2.4.
Comparing Classifiers .......................................................................148
4.3. DISCRIMINATIVE TRAINING ................................................................................150
4.3.1.
Maximum Mutual Information Estimation.........................................150
4.3.2.
Minimum-Error-Rate Estimation.......................................................156
4.3.3.
Neural Networks ................................................................................158
4.4. UNSUPERVISED ESTIMATION METHODS .............................................................163
4.4.1.
Vector Quantization...........................................................................164
4.4.2.
The EM Algorithm .............................................................................170
4.4.3.
Multivariate Gaussian Mixture Density Estimation...........................172

TABLE OF CONTENTS
iii
4.5. CLASSIFICATION AND REGRESSION TREES..........................................................176
4.5.1.
Choice of Question Set.......................................................................177
4.5.2.
Splitting Criteria................................................................................179
4.5.3.
Growing the Tree...............................................................................181
4.5.4.
Missing Values and Conflict Resolution ............................................182
4.5.5.
Complex Questions ............................................................................183
4.5.6.
The Right-Sized Tree..........................................................................185
4.6. HISTORICAL PERSPECTIVE AND FURTHER READING...........................................190
PART II SPEECH PROCESSING
5. DIGITAL SIGNAL PROCESSING..............................................................201
5.1. DIGITAL SIGNALS AND SYSTEMS ........................................................................202
5.1.1.
Sinusoidal Signals..............................................................................203
5.1.2.
Other Digital Signals.........................................................................206
5.1.3.
Digital Systems ..................................................................................206
5.2. CONTINUOUS-FREQUENCY TRANSFORMS...........................................................209
5.2.1.
The Fourier Transform ......................................................................209
5.2.2.
Z-Transform.......................................................................................211
5.2.3.
Z-Transforms of Elementary Functions .............................................212
5.2.4.
Properties of the Z and Fourier Transform .......................................215
5.3. DISCRETE-FREQUENCY TRANSFORMS................................................................216
5.3.1.
The Discrete Fourier Transform (DFT).............................................218
5.3.2.
Fourier Transforms of Periodic Signals ............................................219
5.3.3.
The Fast Fourier Transform (FFT)....................................................222
5.3.4.
Circular Convolution.........................................................................227
5.3.5.
The Discrete Cosine Transform (DCT)..............................................228
5.4. DIGITAL FILTERS AND WINDOWS........................................................................229
5.4.1.
The Ideal Low-Pass Filter .................................................................229
5.4.2.
Window Functions .............................................................................230
5.4.3.
FIR Filters..........................................................................................232
5.4.4.
IIR Filters...........................................................................................238
5.5. DIGITAL PROCESSING OF ANALOG SIGNALS........................................................242
5.5.1.
Fourier Transform of Analog Signals................................................242
5.5.2.
The Sampling Theorem ......................................................................243
5.5.3.
Analog-to-Digital Conversion ...........................................................245
5.5.4.
Digital-to-Analog Conversion ...........................................................246
5.6. MULTIRATE SIGNAL PROCESSING.......................................................................247
5.6.1.
Decimation.........................................................................................248
5.6.2.
Interpolation ......................................................................................249
5.6.3.
Resampling ........................................................................................250
5.7. FILTERBANKS .....................................................................................................250
5.7.1.
Two-Band Conjugate Quadrature Filters..........................................250

iv
TABLE OF CONTENTS
5.7.2.
Multiresolution Filterbanks ...............................................................253
5.7.3.
The FFT as a Filterbank....................................................................255
5.7.4.
Modulated Lapped Transforms..........................................................257
5.8. STOCHASTIC PROCESSES ....................................................................................259
5.8.1.
Statistics of Stochastic Processes.......................................................260
5.8.2.
Stationary Processes..........................................................................263
5.8.3.
LTI Systems with Stochastic Inputs....................................................266
5.8.4.
Power Spectral Density......................................................................267
5.8.5.
Noise ..................................................................................................269
5.9. HISTORICAL PERSPECTIVE AND FURTHER READING...........................................269
6. SPEECH SIGNAL REPRESENTATIONS ...............................................273
6.1. SHORT-TIME FOURIER ANALYSIS.......................................................................274
6.1.1.
Spectrograms .....................................................................................279
6.1.2.
Pitch-Synchronous Analysis...............................................................281
6.2. ACOUSTICAL MODEL OF SPEECH PRODUCTION ..................................................281
6.2.1.
Glottal Excitation...............................................................................282
6.2.2.
Lossless Tube Concatenation.............................................................282
6.2.3.
Source-Filter Models of Speech Production......................................286
6.3. LINEAR PREDICTIVE CODING..............................................................................288
6.3.1.
The Orthogonality Principle..............................................................289
6.3.2.
Solution of the LPC Equations...........................................................291
6.3.3.
Spectral Analysis via LPC .................................................................298
6.3.4.
The Prediction Error..........................................................................299
6.3.5.
Equivalent Representations ...............................................................301
6.4. CEPSTRAL PROCESSING......................................................................................304
6.4.1.
The Real and Complex Cepstrum.......................................................305
6.4.2.
Cepstrum of Pole-Zero Filters ...........................................................306
6.4.3.
Cepstrum of Periodic Signals ............................................................309
6.4.4.
Cepstrum of Speech Signals...............................................................310
6.4.5.
Source-Filter Separation via the Cepstrum .......................................311
6.5. PERCEPTUALLY-MOTIVATED REPRESENTATIONS...............................................313
6.5.1.
The Bilinear Transform......................................................................313
6.5.2.
Mel-Frequency Cepstrum ..................................................................314
6.5.3.
Perceptual Linear Prediction (PLP)..................................................316
6.6. FORMANT FREQUENCIES ....................................................................................316
6.6.1.
Statistical Formant Tracking .............................................................318
6.7. THE ROLE OF PITCH ...........................................................................................321
6.7.1.
Autocorrelation Method.....................................................................321
6.7.2.
Normalized Cross-Correlation Method .............................................324
6.7.3.
Signal Conditioning...........................................................................327
6.7.4.
Pitch Tracking....................................................................................327
6.8. HISTORICAL PERSPECTIVE AND FUTURE READING.............................................329

TABLE OF CONTENTS
v
7. SPEECH CODING..............................................................................................335
7.1. SPEECH CODERS ATTRIBUTES............................................................................336
7.2. SCALAR WAVEFORM CODERS............................................................................338
7.2.1.
Linear Pulse Code Modulation (PCM)..............................................338
7.2.2.
µ-law and A-law PCM .......................................................................340
7.2.3.
Adaptive PCM....................................................................................342
7.2.4.
Differential Quantization...................................................................343
7.3. SCALAR FREQUENCY DOMAIN CODERS..............................................................346
7.3.1.
Benefits of Masking............................................................................346
7.3.2.
Transform Coders..............................................................................348
7.3.3.
Consumer Audio ................................................................................349
7.3.4.
Digital Audio Broadcasting (DAB)....................................................349
7.4. CODE EXCITED LINEAR PREDICTION (CELP).....................................................350
7.4.1.
LPC Vocoder......................................................................................350
7.4.2.
Analysis by Synthesis .........................................................................351
7.4.3.
Pitch Prediction: Adaptive Codebook ...............................................354
7.4.4.
Perceptual Weighting and Postfiltering.............................................355
7.4.5.
Parameter Quantization.....................................................................356
7.4.6.
CELP Standards ................................................................................357
7.5. LOW-BIT RATE SPEECH CODERS........................................................................359
7.5.1.
Mixed-Excitation LPC Vocoder.........................................................360
7.5.2.
Harmonic Coding ..............................................................................360
7.5.3.
Waveform Interpolation.....................................................................365
7.6. HISTORICAL PERSPECTIVE AND FURTHER READING...........................................369
PART III: SPEECH RECOGNITION
8. HIDDEN MARKOV MODELS......................................................................375
8.1. THE MARKOV CHAIN .........................................................................................376
8.2. DEFINITION OF THE HIDDEN MARKOV MODEL ...................................................378
8.2.1.
Dynamic Programming and DTW .....................................................381
8.2.2.
How to Evaluate an HMM – The Forward Algorithm.......................383
8.2.3.
How to Decode an HMM - The Viterbi Algorithm.............................385
8.2.4.
How to Estimate HMM Parameters – Baum-Welch Algorithm .........387
8.3. CONTINUOUS AND SEMI-CONTINUOUS HMMS ...................................................392
8.3.1.
Continuous Mixture Density HMMs ..................................................392
8.3.2.
Semi-continuous HMMs.....................................................................394
8.4. PRACTICAL ISSUES IN USING HMMS..................................................................396
8.4.1.
Initial Estimates .................................................................................396
8.4.2.
Model Topology.................................................................................397
8.4.3.
Training Criteria................................................................................399
8.4.4.
Deleted Interpolation.........................................................................399

vi
TABLE OF CONTENTS
8.4.5.
Parameter Smoothing ........................................................................401
8.4.6.
Probability Representations...............................................................402
8.5. HMM LIMITATIONS ...........................................................................................403
8.5.1.
Duration Modeling ............................................................................404
8.5.2.
First-Order Assumption.....................................................................406
8.5.3.
Conditional Independence Assumption..............................................407
8.6. HISTORICAL PERSPECTIVE AND FURTHER READING...........................................407
9. ACOUSTIC MODELING.................................................................................413
9.1. VARIABILITY IN THE SPEECH SIGNAL..................................................................414
9.1.1.
Context Variability.............................................................................415
9.1.2.
Style Variability .................................................................................416
9.1.3.
Speaker Variability ............................................................................416
9.1.4.
Environment Variability.....................................................................417
9.2. HOW TO MEASURE SPEECH RECOGNITION ERRORS............................................417
9.3. SIGNAL PROCESSING—EXTRACTING FEATURES.................................................419
9.3.1.
Signal Acquisition..............................................................................420
9.3.2.
End-Point Detection ..........................................................................421
9.3.3.
MFCC and Its Dynamic Features......................................................423
9.3.4.
Feature Transformation.....................................................................424
9.4. PHONETIC MODELING—SELECTING APPROPRIATE UNITS..................................426
9.4.1.
Comparison of Different Units...........................................................427
9.4.2.
Context Dependency ..........................................................................428
9.4.3.
Clustered Acoustic-Phonetic Units....................................................430
9.4.4.
Lexical Baseforms..............................................................................434
9.5. ACOUSTIC MODELING—SCORING ACOUSTIC FEATURES....................................437
9.5.1.
Choice of HMM Output Distributions................................................437
9.5.2.
Isolated vs. Continuous Speech Training...........................................439
9.6. ADAPTIVE TECHNIQUES—MINIMIZING MISMATCHES ........................................442
9.6.1.
Maximum a Posteriori (MAP)............................................................443
9.6.2.
Maximum Likelihood Linear Regression (MLLR)..............................446
9.6.3.
MLLR and MAP Comparison ............................................................448
9.6.4.
Clustered Models ...............................................................................450
9.7. CONFIDENCE MEASURES: MEASURING THE RELIABILITY...................................451
9.7.1.
Filler Models......................................................................................451
9.7.2.
Transformation Models......................................................................452
9.7.3.
Combination Models..........................................................................454
9.8. OTHER TECHNIQUES ..........................................................................................455
9.8.1.
Neural Networks ................................................................................455
9.8.2.
Segment Models.................................................................................457
9.9. CASE STUDY: WHISPER......................................................................................462
9.10. HISTORICAL PERSPECTIVE AND FURTHER READING..........................................463

TABLE OF CONTENTS
vii
10. ENVIRONMENTAL ROBUSTNESS .......................................................473
10.1. THE ACOUSTICAL ENVIRONMENT.....................................................................474
10.1.1.
Additive Noise....................................................................................474
10.1.2.
Reverberation.....................................................................................476
10.1.3.
A Model of the Environment ..............................................................478
10.2. ACOUSTICAL TRANSDUCERS.............................................................................482
10.2.1.
The Condenser Microphone...............................................................482
10.2.2.
Directionality Patterns.......................................................................484
10.2.3.
Other Transduction Categories .........................................................492
10.3. ADAPTIVE ECHO CANCELLATION (AEC)...........................................................493
10.3.1.
The LMS Algorithm............................................................................494
10.3.2.
Convergence Properties of the LMS Algorithm.................................495
10.3.3.
Normalized LMS Algorithm ...............................................................497
10.3.4.
Transform-Domain LMS Algorithm...................................................497
10.3.5.
The RLS Algorithm.............................................................................498
10.4. MULTIMICROPHONE SPEECH ENHANCEMENT....................................................499
10.4.1.
Microphone Arrays............................................................................500
10.4.2.
Blind Source Separation ....................................................................505
10.5. ENVIRONMENT COMPENSATION PREPROCESSING .............................................510
10.5.1.
Spectral Subtraction ..........................................................................510
10.5.2.
Frequency-Domain MMSE from Stereo Data....................................514
10.5.3.
Wiener Filtering.................................................................................516
10.5.4.
Cepstral Mean Normalization (CMN)................................................517
10.5.5.
Real-Time Cepstral Normalization ....................................................520
10.5.6.
The Use of Gaussian Mixture Models................................................520
10.6. ENVIRONMENTAL MODEL ADAPTATION............................................................522
10.6.1.
Retraining on Corrupted Speech .......................................................523
10.6.2.
Model Adaptation ..............................................................................524
10.6.3.
Parallel Model Combination .............................................................526
10.6.4.
Vector Taylor Series ..........................................................................528
10.6.5.
Retraining on Compensated Features................................................532
10.7. MODELING NONSTATIONARY NOISE .................................................................533
10.8. HISTORICAL PERSPECTIVE AND FURTHER READING..........................................534
11. LANGUAGE MODELING............................................................................539
11.1. FORMAL LANGUAGE THEORY...........................................................................540
11.1.1.
Chomsky Hierarchy ...........................................................................541
11.1.2.
Chart Parsing for Context-Free Grammars.......................................543
11.2. STOCHASTIC LANGUAGE MODELS.....................................................................548
11.2.1.
Probabilistic Context-Free Grammars ..............................................548
11.2.2.
N-gram Language Models .................................................................552
11.3. COMPLEXITY MEASURE OF LANGUAGE MODELS ..............................................554
11.4. N-GRAM SMOOTHING .......................................................................................556

viii
TABLE OF CONTENTS
11.4.1.
Deleted Interpolation Smoothing.......................................................558
11.4.2.
Backoff Smoothing.............................................................................559
11.4.3.
Class n-grams ....................................................................................565
11.4.4.
Performance of n-gram Smoothing....................................................567
11.5. ADAPTIVE LANGUAGE MODELS ........................................................................568
11.5.1.
Cache Language Models ...................................................................568
11.5.2.
Topic-Adaptive Models......................................................................569
11.5.3.
Maximum Entropy Models.................................................................570
11.6. PRACTICAL ISSUES............................................................................................572
11.6.1.
Vocabulary Selection.........................................................................572
11.6.2.
N-gram Pruning.................................................................................574
11.6.3.
CFG vs n-gram Models......................................................................575
11.7. HISTORICAL PERSPECTIVE AND FURTHER READING..........................................578
12. BASIC SEARCH ALGORITHMS .............................................................585
12.1. BASIC SEARCH ALGORITHMS............................................................................586
12.1.1.
General Graph Searching Procedures ..............................................586
12.1.2.
Blind Graph Search Algorithms.........................................................591
12.1.3.
Heuristic Graph Search.....................................................................594
12.2. SEARCH ALGORITHMS FOR SPEECH RECOGNITION ...........................................601
12.2.1.
Decoder Basics ..................................................................................602
12.2.2.
Combining Acoustic And Language Models......................................603
12.2.3.
Isolated Word Recognition ................................................................604
12.2.4.
Continuous Speech Recognition ........................................................604
12.3. LANGUAGE MODEL STATES ..............................................................................606
12.3.1.
Search Space with FSM and CFG .....................................................606
12.3.2.
Search Space with the Unigram.........................................................609
12.3.3.
Search Space with Bigrams ...............................................................610
12.3.4.
Search Space with Trigrams ..............................................................612
12.3.5.
How to Handle Silences Between Words ...........................................613
12.4. TIME-SYNCHRONOUS VITERBI BEAM SEARCH..................................................615
12.4.1.
The Use of Beam................................................................................617
12.4.2.
Viterbi Beam Search ..........................................................................618
12.5. STACK DECODING (A* SEARCH) ........................................................................619
12.5.1.
Admissible Heuristics for Remaining Path ........................................622
12.5.2.
When to Extend New Words...............................................................624
12.5.3.
Fast Match.........................................................................................627
12.5.4.
Stack Pruning.....................................................................................631
12.5.5.
Multistack Search ..............................................................................632
12.6. HISTORICAL PERSPECTIVE AND FURTHER READING..........................................633
13. LARGE VOCABULARY SEARCH ALGORITHMS........................637
13.1. EFFICIENT MANIPULATION OF TREE LEXICON...................................................638

TABLE OF CONTENTS
ix
13.1.1.
Lexical Tree .......................................................................................638
13.1.2.
Multiple Copies of Pronunciation Trees............................................640
13.1.3.
Factored Language Probabilities......................................................642
13.1.4.
Optimization of Lexical Trees............................................................645
13.1.5.
Exploiting Subtree Polymorphism .....................................................648
13.1.6.
Context-Dependent Units and Inter-Word Triphones........................650
13.2. OTHER EFFICIENT SEARCH TECHNIQUES...........................................................651
13.2.1.
Using Entire HMM as a State in Search............................................651
13.2.2.
Different Layers of Beams..................................................................652
13.2.3.
Fast Match.........................................................................................653
13.3. N-BEST AND MULTIPASS SEARCH STRATEGIES..................................................655
13.3.1.
N-Best Lists and Word Lattices..........................................................655
13.3.2.
The Exact N-best Algorithm...............................................................658
13.3.3.
Word-Dependent N-Best and Word-Lattice Algorithm......................659
13.3.4.
The Forward-Backward Search Algorithm........................................662
13.3.5.
One-Pass vs. Multipass Search..........................................................665
13.4. SEARCH-ALGORITHM EVALUATION ..................................................................666
13.5. CASE STUDY—MICROSOFT WHISPER ...............................................................667
13.5.1.
The CFG Search Architecture............................................................668
13.5.2.
The N-Gram Search Architecture ......................................................669
13.6. HISTORICAL PERSPECTIVES AND FURTHER READING........................................673
PART IV: TEXT-TO-SPEECH SYSTEMS
14. TEXT AND PHONETIC ANALYSIS .......................................................679
14.1. MODULES AND DATA FLOW..............................................................................680
14.1.1.
Modules..............................................................................................682
14.1.2.
Data Flows.........................................................................................684
14.1.3.
Localization Issues.............................................................................686
14.2. LEXICON ...........................................................................................................687
14.3. DOCUMENT STRUCTURE DETECTION ................................................................688
14.3.1.
Chapter and Section Headers............................................................690
14.3.2.
Lists....................................................................................................691
14.3.3.
Paragraphs ........................................................................................692
14.3.4.
Sentences............................................................................................692
14.3.5.
E-mail ................................................................................................694
14.3.6.
Web Pages..........................................................................................695
14.3.7.
Dialog Turns and Speech Acts...........................................................695
14.4. TEXT NORMALIZATION .....................................................................................696
14.4.1.
Abbreviations and Acronyms.............................................................699
14.4.2.
Number Formats................................................................................701
14.4.3.
Domain-Specific Tags........................................................................707
14.4.4.
Miscellaneous Formats......................................................................708

x
TABLE OF CONTENTS
14.5. LINGUISTIC ANALYSIS.......................................................................................709
14.6. HOMOGRAPH DISAMBIGUATION........................................................................712
14.7. MORPHOLOGICAL ANALYSIS .............................................................................714
14.8. LETTER-TO-SOUND CONVERSION......................................................................716
14.9. EVALUATION.....................................................................................................719
14.10. CASE STUDY: FESTIVAL .................................................................................721
14.10.1. Lexicon...............................................................................................721
14.10.2. Text Analysis......................................................................................722
14.10.3. Phonetic Analysis...............................................................................723
14.11. HISTORICAL PERSPECTIVE AND FURTHER READING.......................................724
15. PROSODY ............................................................................................................727
15.1. THE ROLE OF UNDERSTANDING ........................................................................728
15.2. PROSODY GENERATION SCHEMATIC.................................................................731
15.3. SPEAKING STYLE...............................................................................................732
15.3.1.
Character...........................................................................................732
15.3.2.
Emotion..............................................................................................732
15.4. SYMBOLIC PROSODY.........................................................................................733
15.4.1.
Pauses................................................................................................735
15.4.2.
Prosodic Phrases ...............................................................................737
15.4.3.
Accent ................................................................................................738
15.4.4.
Tone ...................................................................................................741
15.4.5.
Tune ...................................................................................................745
15.4.6.
Prosodic Transcription Systems.........................................................747
15.5. DURATION ASSIGNMENT...................................................................................749
15.5.1.
Rule-Based Methods ..........................................................................750
15.5.2.
CART-Based Durations .....................................................................751
15.6. PITCH GENERATION ..........................................................................................751
15.6.1.
Attributes of Pitch Contours ..............................................................751
15.6.2.
Baseline F0 Contour Generation.......................................................755
15.6.3.
Parametric F0 Generation.................................................................761
15.6.4.
Corpus-Based F0 Generation............................................................765
15.7. PROSODY MARKUP LANGUAGES.......................................................................769
15.8. PROSODY EVALUATION.....................................................................................771
15.9. HISTORICAL PERSPECTIVE AND FURTHER READING..........................................772
16. SPEECH SYNTHESIS ....................................................................................777
16.1. ATTRIBUTES OF SPEECH SYNTHESIS..................................................................778
16.2. FORMANT SPEECH SYNTHESIS ..........................................................................780
16.2.1.
Waveform Generation from Formant Values.....................................780
16.2.2.
Formant Generation by Rule .............................................................783
16.2.3.
Data-Driven Formant Generation.....................................................786
16.2.4.
Articulatory Synthesis........................................................................786

TABLE OF CONTENTS
xi
16.3. CONCATENATIVE SPEECH SYNTHESIS ...............................................................787
16.3.1.
Choice of Unit....................................................................................788
16.3.2.
Optimal Unit String: The Decoding Process .....................................792
16.3.3.
Unit Inventory Design........................................................................800
16.4. PROSODIC MODIFICATION OF SPEECH ...............................................................801
16.4.1.
Synchronous Overlap and Add (SOLA) .............................................801
16.4.2.
Pitch Synchronous Overlap and Add (PSOLA) .................................802
16.4.3.
Spectral Behavior of PSOLA .............................................................804
16.4.4.
Synthesis Epoch Calculation..............................................................805
16.4.5.
Pitch-Scale Modification Epoch Calculation ....................................807
16.4.6.
Time-Scale Modification Epoch Calculation.....................................808
16.4.7.
Pitch-Scale Time-Scale Epoch Calculation.......................................810
16.4.8.
Waveform Mapping............................................................................810
16.4.9.
Epoch Detection.................................................................................810
16.4.10. Problems with PSOLA .......................................................................812
16.5. SOURCE-FILTER MODELS FOR PROSODY MODIFICATION ..................................814
16.5.1.
Prosody Modification of the LPC Residual .......................................814
16.5.2.
Mixed Excitation Models ...................................................................815
16.5.3.
Voice Effects ......................................................................................816
16.6. EVALUATION OF TTS SYSTEMS ........................................................................817
16.6.1.
Intelligibility Tests .............................................................................819
16.6.2.
Overall Quality Tests .........................................................................822
16.6.3.
Preference Tests.................................................................................824
16.6.4.
Functional Tests.................................................................................824
16.6.5.
Automated Tests.................................................................................825
16.7. HISTORICAL PERSPECTIVE AND FUTURE READING ...........................................826
PART V: SPOKEN LANGUAGE SYSTEMS
17. SPOKEN LANGUAGE UNDERSTANDING ........................................835
17.1. WRITTEN VS. SPOKEN LANGUAGES...................................................................837
17.1.1.
Style....................................................................................................838
17.1.2.
Disfluency ..........................................................................................839
17.1.3.
Communicative Prosody....................................................................840
17.2. DIALOG STRUCTURE .........................................................................................841
17.2.1.
Units of Dialog...................................................................................842
17.2.2.
Dialog (Speech) Acts..........................................................................843
17.2.3.
Dialog Control...................................................................................848
17.3. SEMANTIC REPRESENTATION ............................................................................849
17.3.1.
Semantic Frames................................................................................849
17.3.2.
Conceptual Graphs............................................................................854
17.4. SENTENCE INTERPRETATION.............................................................................855
17.4.1.
Robust Parsing...................................................................................856

xii
TABLE OF CONTENTS
17.4.2.
Statistical Pattern Matching ..............................................................860
17.5. DISCOURSE ANALYSIS.......................................................................................862
17.5.1.
Resolution of Relative Expression......................................................863
17.5.2.
Automatic Inference and Inconsistency Detection.............................866
17.6. DIALOG MANAGEMENT.....................................................................................867
17.6.1.
Dialog Grammars..............................................................................868
17.6.2.
Plan-Based Systems ...........................................................................870
17.6.3.
Dialog Behavior.................................................................................874
17.7. RESPONSE GENERATION AND RENDITION.........................................................876
17.7.1.
Response Content Generation............................................................876
17.7.2.
Concept-to-Speech Rendition ............................................................880
17.7.3.
Other Renditions................................................................................882
17.8. EVALUATION.....................................................................................................882
17.8.1.
Evaluation in the ATIS Task ..............................................................882
17.8.2.
PARADISE Framework .....................................................................884
17.9. CASE STUDY—DR. WHO..................................................................................887
17.9.1.
Semantic Representation....................................................................887
17.9.2.
Semantic Parser (Sentence Interpretation)........................................889
17.9.3.
Discourse Analysis.............................................................................890
17.9.4.
Dialog Manager.................................................................................891
17.10. HISTORICAL PERSPECTIVE AND FURTHER READING.......................................894
18. APPLICATIONS AND USER INTERFACES ......................................899
18.1. APPLICATION ARCHITECTURE ...........................................................................900
18.2. TYPICAL APPLICATIONS ....................................................................................901
18.2.1.
Computer Command and Control......................................................901
18.2.2.
Telephony Applications......................................................................904
18.2.3.
Dictation ............................................................................................906
18.2.4.
Accessibility .......................................................................................909
18.2.5.
Handheld Devices..............................................................................909
18.2.6.
Automobile Applications....................................................................910
18.2.7.
Speaker Recognition ..........................................................................910
18.3. SPEECH INTERFACE DESIGN..............................................................................911
18.3.1.
General Principles.............................................................................911
18.3.2.
Handling Errors.................................................................................916
18.3.3.
Other Considerations.........................................................................920
18.3.4.
Dialog Flow.......................................................................................921
18.4. INTERNATIONALIZATION ...................................................................................923
18.5. CASE STUDY—MIPAD......................................................................................924
18.5.1.
Specifying the Application .................................................................925
18.5.2.
Rapid Prototyping..............................................................................927
18.5.3.
Evaluation..........................................................................................928
18.5.4.
Iterations............................................................................................930

TABLE OF CONTENTS
xiii
18.6. HISTORICAL PERSPECTIVE AND FURTHER READING..........................................931

1
Foreword
Recognition and understanding of spontane-
ous unrehearsed speech remains an elusive goal. To understand speech, a human considers
not only the specific information conveyed to the ear, but also the context in which the in-
formation is being discussed. For this reason, people can understand spoken language even
when the speech signal is corrupted by noise. However, understanding the context of speech
is, in turn, based on a broad knowledge of the world. And this has been the source of the
difficulty and over forty years of research.
It is difficult to develop computer programs that are sufficiently sophisticated to under-
stand continuous speech by a random speaker. Only when programmers simplify the prob-
lem—by isolating words, limiting the vocabulary or number of speakers, or constraining the
way in which sentences may be formed—is speech recognition by computer possible.
Since the early 1970s, researchers at ATT, BBN, CMU, IBM, Lincoln Labs, MIT, and
SRI have made major contributions in Spoken Language Understanding Research. In 1971,
the Defense Advanced Research Projects Agency (Darpa) initiated an ambitious five-year,
$15 million, multisite effort to develop speech-understanding systems. The goals were to
develop systems that would accept continuous speech from many speakers, with minimal
speaker adaptation, and operate on a 1000-word vocabulary, artificial syntax, and a con-

2
Foreword
strained task domain. Two of the systems, Harpy and Hearsay-II, both developed at Came-
gie-Mellon University, achieved the original goals and in some instances surpassed them.
During the last three decades I have been at Carnegie Mellon, I have been very fortu-
nate to be able to work with many brilliant students and researchers. Xuedong Huang, Alex
Acero and Hsiao-Wuen Hon were arguably among the outstanding researchers in the speech
group at CMU. Since then they have moved to Microsoft and have put together a world-class
team at Microsoft Research. Over the years, they have contributed with standards for build-
ing spoken language understanding systems with Microsoft’s SAPI/SDK family of products,
and pushed the technologies forward with the rest of the community. Today, they continue to
play a premier leadership role in both the research community and in industry.
The new book “Spoken Language Processing” by Huang, Acero and Hon represents a
welcome addition to the technical literature on this increasingly important emerging area of
Information Technology. As we move from desktop PCs to personal digital assistants
(PDAs), wearable computers, and Internet cell phones, speech becomes a central, if not the
only, means of communication between the human and machine! Huang, Acero, and Hon
have undertaken a commendable task of creating a comprehensive reference manuscript cov-
ering theoretical, algorithmic and systems aspects of spoken language tasks of recognition,
synthesis and understanding.
The task of spoken language communication requires a system to recognize, interpret,
execute and respond to a spoken query. This task is complicated by the fact that the speech
signal is corrupted by many sources: noise in the background, characteristics of the micro-
phone, vocal tract characteristics of the speakers, and differences in pronunciation. In addi-
tion the system has to cope with non-grammaticality of spoken communication and ambigu-
ity of language. To solve the problem, an effective system must strive to utilize all the avail-
able sources of knowledge, i.e., acoustics, phonetics and phonology, lexical, syntactic and
semantic structure of language, and task specific context dependent information.
Speech is based on a sequence of discrete sound segments that are linked in time.
These segments, called phonemes, are assumed to have unique articulatory and acoustic
characteristics. While the human vocal apparatus can produce an almost infinite number of
articulatory gestures, the number of phonemes is limited. English as spoken in the United
States, for example, contains 16 vowel and 24 consonant sounds. Each phoneme has distin-
guishable acoustic characteristics and, in combination with other phonemes, forms larger
units such as syllables and words. Knowledge about the acoustic differences among these
sound units is essential to distinguish one word from another, say “bit” from “pit.”
When speech sounds are connected to form larger linguistic units, the acoustic charac-
teristics of a given phoneme will change as a function of its immediate phonetic environment
because of the interaction among various anatomical structures (such as the tongue, lips, and
vocal chords) and their different degrees of sluggishness. The result is an overlap of phone-
mic information in the acoustic signal from one segment to the other. For example, the same
underlying phoneme “t” can have drastically different acoustic characteristics in different
words, say, in “tea,” “tree,” “city,” “beaten.” and “steep.” This effect, known as coarticula-
tion, can occur within a given word or across a word boundary. Thus, the word “this” will
have very different acoustic properties in phrases such as “this car” and “this ship.”

Foreward
3
This manuscript is self-contained for those who wish to familiarize themselves with the
current state of spoken language systems technology. However a researcher or a professional
in the field will benefit from a thorough grounding in a number of disciplines such as:
 signal processing: Fourier Transforms, DFT, and FFT.
 acoustics: Physics of sounds and speech, models of vocal tract.
 pattern recognition: clustering and pattern matching techniques.
 artificial intelligence: knowledge representation and search, natural language
processing.
 computer science: hardware, parallel systems, algorithm optimization.
 statistics: probability theory, hidden Morkov models, dynamic programming and
 linguistics: acoustic phonetics, lexical representation, syntax, and semantics.
A newcomer to this field, easily overwhelmed by the vast number of different algo-
rithms scattered across many conference proceedings, can find in this book a set of tech-
niques that the Huang, Acero and Hon have found to work well in practice. This book is
unique in that it includes both the theory and implementation details necessary to build spo-
ken language systems. If you were able to assemble all of the individual material that are
covered in the book and put it on a shelf it would be several times larger than this volume,
and yet you would be missing vital information. You would not have the material that is in
this book that threads it all into one story, one context. If you need additional resources, the
authors include references to get that additional detail. This makes it very appealing both as a
textbook as well as a reference book for practicing engineers. Some readers familiar with
some topic may decide to skip a few chapters; others may want to focus in other chapters. As
such, this is not a book that you will pick up and read from cover to cover, but one you will
keep near you as long as you work in this field.
Raj Reddy

i
P R E F A C E
Our primary motivation in writing this book
is to share our working experience to bridge the gap between the knowledge of industry gu-
rus and newcomers to the spoken language processing community. Many powerful tech-
niques hide in conference proceedings and academic papers for years before becoming
widely recognized by the research community or the industry. We spent many years pursuing
spoken language technology research at Carnegie Mellon University before we started spo-
ken language R&D at Microsoft. We fully understand that it is by no means a small under-
taking to transfer a state of the art spoken language research system into a commercially vi-
able product that can truly help people improve their productivity. Our experience in both
industry and academia is reflected in the context of this book, which presents a contemporary
and comprehensive description of both theoretic and practical issues in spoken language
processing. This book is intended for people of diverse academic and practical backgrounds.
Speech scientists, computer scientists, linguists, engineers, physicists and psychologists all
have a unique perspective to spoken language processing. This book will be useful to all of
these special interest groups.
Spoken language processing is a diverse subject that relies on knowledge of many lev-
els, including acoustics, phonology, phonetics, linguistics, semantics, pragmatics, and dis-
course. The diverse nature of spoken language processing requires knowledge in computer
science, electrical engineering, mathematics, syntax, and psychology. There are a number of
excellent books on the sub-fields of spoken language processing, including speech recogni-
tion, text to speech conversion, and spoken language understanding, but there is no single
book that covers both theoretical and practical aspects of these sub-fields and spoken lan-
guage interface design. We devote many chapters systematically introducing fundamental
theories needed to understand how speech recognition, text to speech synthesis, and spoken

ii
Preface
language understanding work. Even more important is the fact that the book highlights what
works well in practice, which is invaluable if you want to build a practical speech recognizer,
a practical text to speech synthesizer, or a practical spoken language system. Using numer-
ous real examples in developing Microsoft’s spoken language systems, we concentrate on
showing how the fundamental theories can be applied to solve real problems in spoken lan-
guage processing.
We would like to thank many people who helped us during our spoken language proc-
essing R&D careers. We are particularly indebted to Professor Raj Reddy at the School of
Computer Science, Carnegie Mellon University. Under his leadership, Carnegie Mellon Uni-
versity has become a center of research excellence on spoken language processing. Today’s
computer industry and academia benefited tremendously from his leadership and contribu-
tions.
Special thanks are due to Microsoft for its encouragement of spoken language R&D.
The management team at Microsoft has been extremely generous to our. We are particularly
grateful to Bill Gates, Nathan Myhrvold, Rick Rashid, Dan Ling, and Jack Breese for the
great environment they created for us at Microsoft Research.
Scott Meredith helped us writing a number of chapters in this book and deserves to be
a co-author. His insight and experience to text to speech synthesis enriched this book a great
deal. We also owe gratitude to many colleagues we worked with in the speech technology
group of Microsoft Research. In alphabetic order, Bruno Alabiso, Fil Alleva, Ciprian
Chelba, James Droppo, Doug Duchene, Li Deng, Joshua Goodman, Mei-Yuh Hwang, Derek
Jacoby, Y.C. Ju, Li Jiang, Ricky Loynd, Milind Mahajan, Peter Mau, Salman Mughal, Mike
Plumpe, Scott Quinn, Mike Rozak, Gina Venolia, Kuansan Wang, and Ye-Yi Wang, not
only developed many algorithms and systems described in this book, but also helped to
shape our thoughts from the very beginning.
In addition to those people, we want to thank Les Atlas, Alan Black, Jeff Bilmes,
David Caulton, Eric Chang, Phil Chou, Dinei Florencio, Allen Gersho, Francisco Gimenez-
Galanes, Hynek Hermansky, Kai-Fu Lee, Henrique Malvar, Mari Ostendorf, Joseph Pen-
theroudakis, Tandy Trower, Wayne Ward, and Charles Wayne. They provided us with many
wonderful comments to refine this book. Tim Moore and Russ Hall at Prentice Hall helped
us finish this book in a finite amount of time.
Finally, writing this book was a marathon that could not have been finished without the
support of our spouses Yingzhi, Donna, and Phen, during the many evenings and weekends
we spent on this project.
Redmond, WA
Xuedong Huang
October 2000
Alejandro Acero
Hsiao-Wuen Hon

1
C H A P T E R
1
IntroductionEquation Section 1
From human prehistory to the new media of
the future, speech communication has been and will be the dominant mode of human social
bonding and information exchange. The spoken word is now extended, through technologi-
cal mediation such as telephony, movies, radio, television, and the Internet. This trend re-
flects the primacy of spoken communication in human psychology.
In addition to human-human interaction, this human preference for spoken language
communication finds a reflection in human-machine interaction as well. Most computers
currently utilize a graphical user interface (GUI), based on graphically represented interface
objects and functions such as windows, icons, menus, and pointers. Most computer operating
systems and applications also depend on a user’s keyboard strokes and mouse clicks, with a
display monitor for feedback. Today’s computers lack the fundamental human abilities to
speak, listen, understand, and learn. Speech, supported by other natural modalities, will be
one of the primary means of interfacing with computers. And, even before speech-based in-
teraction reaches full maturity, applications in home, mobile, and office segments are incor-
porating spoken language technology to change the way we live and work.

2
Introduction
A spoken language system needs to have both speech recognition and speech synthesis
capabilities. However, those two components by themselves are not sufficient to build a use-
ful spoken language system. An understanding and dialog component is required to manage
interactions with the user; and domain knowledge must be provided to guide the system’s
interpretation of speech and allow it to determine the appropriate action. For all these com-
ponents, significant challenges exist, including robustness, flexibility, ease of integration,
and engineering efficiency. The goal of building commercially viable spoken language sys-
tems has long attracted the attention of scientists and engineers all over the world. The pur-
pose of this book is to share our working experience in developing advanced spoken lan-
guage processing systems with both our colleagues and newcomers. We devote many chap-
ters to systematically introducing fundamental theories and to highlighting what works well
based on numerous lessons we learned in developing Microsoft’s spoken language systems.
1.1.
MOTIVATIONS
What motivates the integration of spoken language as the primary interface modality? We
present a number of scenarios, roughly in order of expected degree of technical challenges
and expected time to full deployment.
1.1.1.
Spoken Language Interface
There are generally two categories of users who can benefit from adoption of speech as a
control modality in parallel with others, such as the mouse, keyboard, touch-screen, and joy-
stick. For novice users, functions that are conceptually simple should be directly accessible.
For example, raising the voice output volume under software control on the desktop speak-
ers, a conceptually simple operation, in some GUI systems of today requires opening one or
more windows or menus, and manipulating sliders, check-boxes or other graphical elements.
This requires some knowledge of the system’s interface conventions and structures. For the
novice user, to be able to say raise the volume would be more direct and natural. For expert
users, the GUI paradigm is sometimes perceived as an obstacle or nuisance and shortcuts are
sought. Frequently these shortcuts allow the power user’s hands to remain on the keyboard or
mouse while mixing content creation with system commands. For example, an operator of a
graphic design system for CAD/CAM might wish to specify a text formatting command
while keeping the pointer device in position over a selected screen element.
Speech has the potential to accomplish these functions more powerfully than keyboard
and mouse clicks. Speech becomes more powerful when supplemented by information
streams encoding other dynamic aspects of user and system status, which can be resolved by
the semantic component of a complete multi-modal interface. We expect such multimodal
interactions to proceed based on more complete user modeling, including speech, visual ori-
entation, natural and device-based gestures, and facial expression, and these will be coordi-
nated with detailed system profiles of typical user tasks and activity patterns.

Motivations
3
In some situations you must rely on speech as an input or output medium. For example,
with wearable computers, it may be impossible to incorporate a large keyboard. When driv-
ing, safety is compromised by any visual distraction, and hands are required for controlling
the vehicle. The ultimate speech-only device, the telephone, is far more widespread than the
PC. Certain manual tasks may also require full visual attention to the focus of the work. Fi-
nally, spoken language interfaces offer obvious benefits for individuals challenged with a
variety of physical disabilities, such as loss of sight or limitations in physical motion and
motor skills. Chapter 18 contains detailed discussion on spoken language applications.
1.1.2.
Speech-to-speech Translation
Speech-to-speech translation has been depicted for decades in science fiction stories. Imag-
ine questioning a Chinese-speaking conversational partner by speaking English into an unob-
trusive device, and hearing real-time replies you can understand. This scenario, like the spo-
ken language interface, requires both speech recognition and speech synthesis technology. In
addition, sophisticated multilingual spoken language understanding is needed. This high-
lights the need for tightly coupled advances in speech recognition, synthesis, and understand-
ing systems, a point emphasized throughout this book.
1.1.3.
Knowledge Partners
The ability of computers to process spoken language as proficient as humans will be a land-
mark to signal the arrival of truly intelligent machines. Alan Turing [29] introduced his fa-
mous Turing test. He suggested a game, in which a computer’s use of language would form
the criterion for intelligence. If the machine could win the game, it would be judged intelli-
gent. In Turing’s game, you play the role of an interrogator. By asking a series of questions
via a teletype, you must determine the identity of the other two participants: a machine and a
person. The task of the machine is to fool you into believing it is a person by responding as a
person to your questions. The task of the other person is to convince you the other partici-
pant is the machine. The critical issue for Turing was that using language as humans do is
sufficient as an operational test for intelligence.
The ultimate use of spoken language is to pass the Turing test in allowing future ex-
tremely intelligent systems to interact with human beings as knowledge partners in all as-
pects of life. This has been a staple of science fiction, but its day will come. Such systems
require reasoning capabilities and extensive world knowledge embedded in sophisticated
search, communication, and inference tools that are beyond the scope of this book. We ex-
pect that spoken language technologies described in this book will form the essential ena-
bling mechanism to pass the Turing test.

4
Introduction
1.2.
SPOKEN LANGUAGE SYSTEM ARCHITECTURE
Spoken language processing refers to technologies related to speech recognition, text-to-
speech, and spoken language understanding. A spoken language system has at least one of
the following three subsystems: a speech recognition system that converts speech into words,
a text-to-speech system that conveys spoken information, and a spoken language understand-
ing system that maps words into actions and that plans system-initiated actions
There is considerable overlap in the fundamental technologies for these three subareas.
Manually created rules have been developed for spoken language systems with limited suc-
cess. But, in recent decades, data-driven statistical approaches have achieved encouraging
results, which are usually based on modeling the speech signal using well-defined statistical
algorithms that can automatically extract knowledge from the data. The data-driven approach
can be viewed fundamentally as a pattern recognition problem. In fact, speech recognition,
text-to-speech conversion, and spoken language understanding can all be regarded as pattern
recognition problems. The patterns are either recognized during the runtime operation of the
system or identified during system construction to form the basis of runtime generative mod-
els such as prosodic templates needed for text to speech synthesis. While we use and advo-
cate a statistical approach, we by no means exclude the knowledge engineering approach
from consideration. If we have a good set of rules in a given problem area, there is no need
to use a statistical approach at all. The problem is that, at time of this writing, we do not have
enough knowledge to produce a complete set of high-quality rules. As scientific and theo-
retical generalizations are made from data collected to construct data-driven systems, better
rules may be constructed. Therefore, the rule-based and statistical approaches are best
viewed as complementary.
1.2.1.
Automatic Speech Recognition
A source-channel mathematical model described in Chapter 3 is often used to formulate
speech recognition problems. As illustrated in Figure 1.1, the speaker’s mind decides the
source word sequence W that is delivered through his/her text generator. The source is
passed through a noisy communication channel that consists of the speaker’s vocal apparatus
to produce the speech waveform and the speech signal processing component of the speech
recognizer. Finally, the speech decoder aims to decode the acoustic signal X into a word
sequence ˆW , which is hopefully close to the original word sequence W.
A typical practical speech recognition system consists of basic components shown in
the dotted box of Figure 1.2. Applications interface with the decoder to get recognition re-
sults that may be used to adapt other components in the system. Acoustic models include the
representation of knowledge about acoustics, phonetics, microphone and environment vari-
ability, gender and dialect differences among speakers, etc. Language models refer to a sys-
tem’s knowledge of what constitutes a possible word, what words are likely to co-occur, and
in what sequence. The semantics and functions related to an operation a user may wish to
perform may also be necessary for the language model. Many uncertainties exist in these

Spoken Language System Architecture
5
areas, associated with speaker characteristics, speech style and rate, recognition of basic
speech segments, possible words, likely words, unknown words, grammatical variation, noise
interference, nonnative accents, and confidence scoring of results. A successful speech rec-
ognition system must contend with all of these uncertainties. But that is only the beginning.
The acoustic uncertainties of the different accents and speaking styles of individual speakers
are compounded by the lexical and grammatical complexity and variations of spoken lan-
guage, which are all represented in the language model.
Figure 1.1 A source-channel model for a speech recognition system [15].
The speech signal is processed in the signal processing module that extracts salient
feature vectors for the decoder. The decoder uses both acoustic and language models to gen-
erate the word sequence that has the maximum posterior probability for the input feature
vectors. It can also provide information needed for the adaptation component to modify ei-
ther the acoustic or language models so that improved performance can be obtained.
Application
Voice
Application
Signal Processing
Acoustic
Models
Decoder
Adaptation
Language
Models
Figure 1.2 Basic system architecture of a speech recognition system [12].
Speech Recognizer
ˆW
Communication Channel
X
Text
Generator
Speech
Generator
Signal
Processing
Speech
Decoder
W

6
Introduction
1.2.2.
Text-to-Speech Conversion
The term text-to-speech, often abbreviated as TTS, is easily understood. The task of a text-
to-speech system can be viewed as speech recognition in reverse – a process of building a
machinery system that can generate human-like speech from any text input to mimic human
speakers. TTS is sometimes called speech synthesis, particularly in the engineering commu-
nity.
The conversion of words in written form into speech is nontrivial. Even if we can store
a huge dictionary for most common words in English; the TTS system still needs to deal with
millions of names and acronyms. Moreover, in order to sound natural, the intonation of the
sentences must be appropriately generated.
The development of TTS synthesis can be traced back to the 1930s when Dudley’s
Voder, developed by Bell Laboratories, was demonstrated at the World’s Fair [18]. Taking
the advantage of increasing computation power and storage technology, TTS researchers
have been able to generate high quality commercial multilingual text-to-speech systems, al-
though the quality is inferior to human speech for general-purpose applications.
The basic components in a TTS system are shown in Figure 1.3. The text analysis
component normalizes the text to the appropriate form so that it becomes speakable. The
input can be either raw text or tagged. These tags can be used to assist text, phonetic, and
prosodic analysis. The phonetic analysis component converts the processed text into the cor-
responding phonetic sequence, which is followed by prosodic analysis to attach appropriate
pitch and duration information to the phonetic sequence. Finally, the speech synthesis com-
ponent takes the parameters from the fully tagged phonetic sequence to generate the corre-
sponding speech waveform.
Various applications have different degrees of knowledge about the structure and con-
tent of the text that they wish to speak so some of the basic components shown in Figure 1.3
can be skipped. For example, some applications may have certain broad requirements such
as rate and pitch. These requirements can be indicated with simple command tags appropri-
ately located in the text. Many TTS systems provide a set of markups (tags), so the text pro-
ducer can better express their semantic intention. An application may know a lot about the
structure and content of the text to be spoken to greatly improve speech output quality. For
engines providing such support, the text analysis phase can be skipped, in whole or in part. If
the system developer knows the orthographic form, the phonetic analysis module can be
skipped as well. The prosodic analysis module assigns a numeric duration to every phonetic
symbol and calculates an appropriate pitch contour for the utterance or paragraph. In some
cases, an application may have prosodic contours precalculated by some other process. This
situation might arise when TTS is being used primarily for compression, or the prosody is
transplanted from a real speaker’s utterance. In these cases, the quantitative prosodic con-
trols can be treated as special tagged field and sent directly along with the phonetic stream to
speech synthesis for voice rendition.

Spoken Language System Architecture
7
Figure 1.3 Basic system architecture of a TTS system.
1.2.3.
Spoken Language Understanding
Whether a speaker is inquiring about flights to Seattle, reserving a table at a Pittsburgh res-
taurant, dictating an article in Chinese, or making a stock trade, a spoken language under-
standing system is needed to interpret utterances in context and carry out appropriate actions.
lexical, syntactic, and semantic knowledge must be applied in a manner that permits coopera-
tive interaction among the various levels of acoustic, phonetic, linguistic, and application
knowledge in minimizing uncertainty. Knowledge of the characteristic vocabulary, typical
syntactic patterns, and possible actions in any given application context for both interpreta-
tion of user utterances and planning system activity are the heart and soul of any spoken lan-
guage understanding system.
A schematic of the typical spoken language understanding systems is shown in Figure
1.4. Such a system typically has a speech recognizer and a speech synthesizer for basic
TTS Engine
Text Analysis
Document Structure Detection
Text Normalization
Linguistic Analysis
Phonetic Analysis
Grapheme-to-Phoneme Conversion
Speech Synthesis
Voice Rendering
Raw text
or tagged text
tagged text
controls
Prosodic Analysis
Pitch & Duration Attachment
tagged phones

8
Introduction
speech input and output, sentence interpretation component to parse the speech recognition
results into semantic forms, which often needs discourse analysis to track context and re-
solve ambiguities. Dialog Manager is the central component that communicates with appli-
cations and the spoken language understanding modules such as discourse analysis, sentence
interpretation, and message generation.
While most components of the system may be partly or wholly generic, the dialog
manager controls the flow of conversation tied to the action. The dialog manager is respon-
sible for providing status needed for formulating responses, and maintaining the system’s
idea of the state of the discourse. The discourse state records the current transaction, dialog
goals that motivated the current transaction, current objects in focus (temporary center of
attention), the object history list for resolving dependent references, and other status infor-
mation. The discourse information is crucial for semantic interpretation to interpret utter-
ances in context. Various systems may alter the flow of information implied in Figure 1.4.
For example, the dialog manager or the semantic interpretation module may be able to sup-
ply contextual discourse information or pragmatic inferences, as feedback to guide the rec-
ognizer’s evaluation of hypotheses at the earliest level of search. Another optimization might
be achieved by providing for shared grammatical resources between the message generation
and semantic interpretation components.
Figure 1.4 Basic system architecture of a spoken language understanding system.
Application
Database
Response Generation
Text-To-Speech
Sentence Interpretation
Speech Recognizer
Access Device
Dialog Manager
Discourse Analysis
Dialog Strategy

Book Organization
9
1.3.
BOOK ORGANIZATION
We attempt to present a comprehensive introduction to spoken language processing, which
includes not only fundamentals but also a practical guide to build a working system that re-
quires knowledge in speech signal processing, recognition, text-to-speech, spoken language
understating, and application integration. Since there is considerable overlap in the funda-
mental spoken language processing technologies, we have devoted Part I to the foundations
needed. Part I contains background on speech production and perception, probability and
information theory, and pattern recognition. Parts II, III, IV, and V include chapters on
speech processing, speech recognition, speech synthesis, and spoken language systems, re-
spectively. A reader with sufficient background can skip Part I, referring back to it later as
needed. For example, the discussion of speech recognition in Part III relies on the pattern
recognition algorithms presented in Part I. Algorithms that are used in several chapters
within Part III are also included in Parts I and II. Since the field is still evolving, at the end
of each chapter we provide a historical perspective and list further readings to facilitate fu-
ture research.
1.3.1.
Part I: Fundamental Theory
Chapters 2 to 4 provide readers with a basic theoretic foundation to better understand tech-
niques that are widely used in modern spoken language systems. These theories include the
essence of linguistics, phonetics, probability theory, information theory, and pattern recogni-
tion. These chapters prepare you fully to understand the rest of the book.
Chapter 2 discusses the basic structure of spoken language including speech science,
phonetics, and linguistics. Chapter 3 covers probability theory and information theory, which
form the foundation of modern pattern recognition. Many important algorithms and princi-
ples in pattern recognition and speech coding are derived based on these theories. Chapter 4
introduces basic pattern recognition, including decision theory, estimation theory, and a
number of algorithms widely used in speech recognition. Pattern recognition forms the core
of most of the algorithms used in spoken language processing.
1.3.2.
Part II: Speech Processing
Part II provides you with necessary speech signal processing knowledge that is critical to
spoken language processing. Most of what discuss here is traditionally the subject of electri-
cal engineering.
Chapters 5 and 6 focus on how to extract useful information from the speech signal.
The basic principles of digital signal processing are reviewed and a number of useful repre-
sentations for the speech signal are discussed. Chapter 7 covers how to compress these rep-
resentations for efficient transmission and storage.

10
Introduction
1.3.3.
Part III: Speech Recognition
Chapters 8 to 13 provide you with an in-depth look at modern speech recognition systems.
We highlight techniques that have been proven to work well in building real systems and
explain in detail how and why these techniques work from both theoretic and practical per-
spectives.
Chapter 8 introduces hidden Markov models, the most prominent technique used in
modern speech recognition systems. Chapters 9 and 11 deal with acoustic modeling and lan-
guage modeling respectively. Because environment robustness is critical to the success of
practical systems, we devote Chapter 10 to discussing how to make systems less affected by
environment noises. Chapters 12 and 13 deal in detail how to efficiently implement the de-
coder for speech recognition. Chapter 12 discusses a number of basic search algorithms, and
Chapter 13 covers large vocabulary speech recognition. Throughout our discussion, Micro-
soft’s Whisper speech recognizer is used as a case study to illustrate the methods introduced
in these chapters.
1.3.4.
Part IV: Text-to-Speech Systems
In Chapters 14 through 16, we discuss proven techniques in building text-to-speech systems.
The synthesis system consists of major components found in speech recognition systems,
except that they are in the reverse order.
Chapters 14 covers the analysis of written documents and the text needed to support
spoken rendition, including the interpretation of audio markup commands, interpretation of
numbers and other symbols, and conversion from orthographic to phonetic symbols. Chapter
15 focuses on the generation of pitch and duration controls for linguistic and emotional ef-
fect. Chapter 16 discusses the implementation of the synthetic voice, and presents algorithms
to manipulate a limited voice data set to support a wide variety of pitch and duration controls
required by the text analysis. We highlight the importance of trainable synthesis, with Micro-
soft’s Whistler TTS system as an example.
1.3.5.
Part V: Spoken Language Systems
As discussed in Section 1.1, spoken language applications motivate spoken language R&D.
The central component is the spoken language understanding system. Since it is closely re-
lated to applications, we group it together with application and interface design.
Chapter 17 covers spoken language understanding. The output of the recognizer re-
quires interpretation and action in a particular application context. This chapter details useful
strategies for dialog management, and the coordination of all the speech and system re-
sources to accomplish a task for a user. Chapter 18 concludes the book with a discussion of
important principles for building spoken language interfaces and applications, including gen-
eral human interface design goals, and interaction with nonspeech interface modalities in

Target Audiences
11
specific application contexts. Microsoft’s MiPad is used as a case study to illustrate a num-
ber of issues in developing spoken language applications.
1.4.
TARGET AUDIENCES
This book can serve a variety of audiences:
Integration engineers: Software engineers who want to build spoken language sys-
tems, but who do not want to learn all about speech technology internals, will find plentiful
relevant material, including application design and software interfaces. Anyone with a pro-
fessional interest in aspects of speech applications, integration, and interfaces can also
achieve enough understanding of how the core technologies work, to allow them to take full
advantage of state-of-the-art capabilities.
Speech technology engineers: Engineers and researchers working on various subspe-
cialties within the speech field will find this book a useful guide to understanding related
technologies in sufficient depth to help them gain insight on where their own approaches
overlap with, or diverge from, their neighbors’ common practice.
Graduate students: This book can serve as a primary textbook in a graduate or ad-
vanced undergraduate speech analysis or language engineering course. It can serve as a sup-
plementary textbook in some applied linguistics, digital signal processing, computer science,
artificial intelligence, and possibly psycholinguistics course.
Linguists: As the practice of linguistics increasingly shifts to empirical analysis of
real-world data, students and professional practitioners alike should find a comprehensive
introduction to the technical foundations of computer processing of spoken language helpful.
The book can be read at different levels and through different paths, for readers with differ-
ing technical skills and background knowledge.
Speech Scientists: Researchers engaged in professional work on issues related to nor-
mal or pathological speech may find this complete exposition of the state-of-the-art in com-
puter modeling of generation and perception of speech interesting.
Business planners: Increasingly, business and management functions require some
level of insight into the vocabulary and common practices of technology development. While
not the primary audience, managers, marketers and others with planning responsibilities and
sufficient technical background will find portions of this book useful in evaluating competing
proposals, and in making buy-or-develop business decisions related to the speech technology
components.
1.5.
HISTORICAL PERSPECTIVE AND FURTHER READING
Spoken language processing is a diverse field that relies on knowledge of language at the
levels of signal processing, acoustics, phonology, phonetics, syntax, semantics, pragmatics,
and discourse. The foundations of spoken language processing lie in computer science, elec-
trical engineering, linguistics, and psychology. In the 1970s an ambitious speech understand-

12
Introduction
ing project was funded by DARPA, which led to many seminal systems and technologies
[17]. A number of human language technology projects funded by DARPA in the 1980s and
‘90s further accelerated the progress, as evidenced by many papers published in The Pro-
ceedings of the DARPA Speech and Natural Language/Human Language Workshop. The
field is still rapidly progressing and there are a number of excellent review articles and intro-
ductory books. We provide a brief list here. More detailed references can be found within
each chapter of this book. Gold and Morgan’s Speech and Audio Signal Processing [10] has
a strong historical perspective on spoken language processing.
Hyde [14] and Reddy [24] provided an excellent review of early speech recognition
work in the 1970s. Some of the principles are still applicable to today’s speech recognition
research. Waibel and Lee assembled many seminal papers in Readings in Speech Recogni-
tion Speech Recognition [31]. There are a number of excellent books on modern speech rec-
ognition [1, 13, 15, 22, 23].
Where does the state of the art speech recognition system stand today? A number of
different recognition tasks can be used to compare the recognition error rate of people vs.
machines. Table 1.1 shows five recognition tasks with vocabularies ranging from 10 to 5,000
words speaker-independent continuous speech recognition. The Wall Street Journal Dicta-
tion (WSJ) Task has 5000-word vocabulary as a continuous dictation application for the
WSJ articles. In Table 1.1, the error rate for machines is based on state of the art speech rec-
ognizers such as systems described in Chapter 9, and the error rate of humans is based a
range of subjects tested on the similar task. We can see the error rate of humans is at least 5
times smaller than machines except for the sentences that are generated from a trigrm lan-
guage model, where the sentences have the perfect match between humans and machines so
humans cannot use high-level knowledge that is not used in machines1.
Table 1.1 Word error rate comparisons between human and machines on similar tasks.
Tasks
Vocabulary
Humans
Machines
Connected digits
10
0.009%
0.72%
Alphabet letters
26
1%
5%
Spontaneous telephone speech
2000
3.8%
36.7%
WSJ with clean speech
5000
0.9%
4.5%
WSJ with noisy speech (10-db SNR)
5000
1.1%
8.6%
Clean speech based on trigram sentences
20,000
7.6%
4.4%
We can see that humans are far more robust than machines for normal tasks. The error
rate for machine spontaneous conversational telephone speech recognition is above 35%,
more than a factor 10 higher than humans on the similar task. In addition, the error rate of
humans does not increase as dramatic as machines when the environment becomes noisy
(from quite to 10-db SNR environments on the WSJ task). The relative error rate of humans
1 Some of these experiments were conducted at Microsoft with only a small number of human
subjects (3-5 people), which is not statistically significant. Nevertheless, it sheds some interesting
insight on the performance between humans and machines.

Historical Perspective and Further Reading
13
increases from 0.9% to 1.1% (1.2 times), while the error rate of CSR systems increases from
4.5% to 8.6% (1.9 times). One interesting experiment is that when we generated sentences
using the WSJ trigram language model (cf Chapter 11), the difference between humans and
machines disappears (the last row in Table 1.1). In fact, the error rate of humans is even
higher than machines. This is because both humans and machines have the same high-level
syntactic and semantic models. The test sentences are somewhat random to humans but per-
fect to machines that used the same trigram model for decoding. This experiment indicates
humans make more effective use of semantic and syntactic constraints for improved speech
recognition in meaningful conversation. In addition, machines don’t have attention problems
as humans on random sentences.
Fant [7] gave an excellent introduction to speech production. Early reviews of text-to-
speech synthesis can be found in [3, 8, 9]. Sagisaka [26] and Carlson [6] provide more recent
reviews of progress in speech synthesis. A more detailed treatment can be found in [19, 30].
Where does the state of the art text to speech system stand today? Unfortunately, like
speech recognition, this is not a solved problem either. Although machine storage capabili-
ties are improving, the quality remains a challenge for many researchers if we want to pass
the Turing test.
Spoken language understanding is deeply rooted in speech recognition research. There
are a number of good books on spoken language understanding [2, 5, 16]. Manning and
Schutz [20] focuses on statistical methods for language understanding. Like Waibel and Lee,
Grosz et al. assembled many foundational papers in Readings in Natural Language Process-
ing [11]. More recent reviews of progress in spoken language understanding can be found in
[25, 28]. Related spoken language interface design issues can be found in [4, 21, 27, 32].
In comparison to speech recognition and text to speech, spoken language understand-
ing is further away from approaching the level of humans, especially for general-purpose
spoken language applications.
A number of good conference proceedings and journals report the latest progress in the
field. Major results on spoken language processing are presented at the International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP), International Conference on
Spoken Language Processing (ICSLP), Eurospeech Conference, the DARPA Speech and
Human Language Technology Workshops, and many workshops organized by the European
Speech Communications Associations (ESCA) and IEEE Signal Processing Society. Journals
include IEEE Transactions on Speech and Audio Processing, IEEE Transactions on Pattern
Analysis and Machine Intelligence (PAMI), Computer Speech and Language, Speech Com-
munications, and Journal of Acoustical Society of America (JASA). Research results can also
be found at computational linguistics conferences such as the Association for Computational
Linguistics (ACL), International Conference on Computational Linguistics (COLING), and
Applied Natural Language Processing (ANLP). The journals Computational Linguistics and
Natural Language Engineering cover both theoretical and practical applications of language
research. Speech Recognition Update published by TMA Associates is an excellent industry
newsletter on spoken language applications.

14
Introduction
REFERENCES
[1]
Acero, A., Acoustical and Environmental Robustness in Automatic Speech Recog-
nition, 1993, Boston, MA, Kluwer Academic Publishers.
[2]
Allen, J., Natural Language Understanding, 2nd ed, 1995, Menlo Park CA, The
Benjamin/Cummings Publishing Company.
[3]
Allen, J., M.S. Hunnicutt, and D.H. Klatt, From Text to Speech: the MITalk System,
1987, Cambridge, UK, University Press.
[4]
Balentine, B. and D. Morgan, How to Build a Speech Recognition Application,
1999, Enterprise Integration Group.
[5]
Bernsen, N., H. Dybkjar, and L. Dybkjar, Designing Interactive Speech Systems,
1998, Springer.
[6]
Carlson, R., "Models of Speech Synthesis" in Voice Communications Between Hu-
mans and Machines. National Academy of Sciences, D.B. Roe and J.G. Wilpon,
eds. 1994, Washington, D.C., National Academy of Sciences.
[7]
Fant, G., Acoustic Theory of Speech Production, 1970, The Hague, NL, Mouton.
[8]
Flanagan, J., Speech Analysis Synthesis and Perception, 1972, New York, Springer-
Verlag.
[9]
Flanagan, J., "Voices Of Men And Machines," Journal of Acoustical Society of
America, 1972, 51, pp. 1375.
[10]
Gold, B. and N. Morgan, Speech and Audio Signal Processing: Processing and
Perception of Speech and Music, 2000, John Wiley and Sons.
[11]
Grosz, B., F.S. Jones, and B.L. Webber, Readings in Natural Language Process-
ing, 1986, Morgan Kaufmann, Los Altos, CA.
[12]
Huang, X., et al., "From Sphinx-II to Whisper - Make Speech Recognition Usable"
in Automatic Speech and Speaker Recognition, C.H. Lee, F.K. Soong, and K.K.
Paliwal, eds. 1996, Norwell, MA, Klewer Academic Publishers.
[13]
Huang, X.D., Y. Ariki, and M.A. Jack, Hidden Markov Models for Speech Recog-
nition, 1990, Edinburgh, U.K., Edinburgh University Press.
[14]
Hyde, S.R., "Automatic Speech Recognition: Literature, Survey, And Discussion"
in Human Communication, A Unified Approach, E.E. David and P.B. Denes, eds.
1972, McGraw Hill, New York.
[15]
Jelinek, F., Statistical Methods for Speech Recognition, Language, Speech, and
Communication, 1998, Cambridge, MA, MIT Press.
[16]
Jurafsky, D. and J. Martin, Speech and Language Processing: An Introduction to
Natural Language Processing, Computational Linguistics, and Speech Recogni-
tion, 2000, Upper Saddle River, NJ, Prentice Hall.
[17]
Klatt, D., "Review of the ARPA Speech Understanding Project," Journal of Acous-
tical Society of America, 1977, 62(6), pp. 1324-1366.
[18]
Klatt, D., "Review of Text-to-Speech Conversion for English," Journal of Acousti-
cal Society of America, 1987, 82, pp. 737-793.
[19]
Kleijn, W.B. and K.K. Paliwal, Speech Coding and Synthesis, 1995, Amsterdam,
Netherlands, Elsevier.

Historical Perspective and Further Reading
15
[20]
Manning, C. and H. Schutze, Foundations of Statistical Natural Language Process-
ing, 1999, MIT Press, Cambridge, USA.
[21]
Markowitz, J., Using Speech Recognition, 1996, Prentice Hall.
[22]
Mori, R.D., Spoken Dialogues with Computers, 1998, London, UK, Academic
Press.
[23]
Rabiner, L.R. and B.H. Juang, Fundamentals of Speech Recognition, May, 1993,
Prentice-Hall.
[24]
Reddy, D.R., "Speech Recognition by Machine: A Review," IEEE Proc., 1976,
64(4), pp. 502-531.
[25]
Sadek, D. and R.D. Mori, "Dialogue Systems" in Spoken Dialogues with Com-
puters, R.D. Mori, Editor 1998, London, UK, pp. 523-561, Academic Press.
[26]
Sagisaka, Y., "Speech Synthesis from Text," IEEE Communication Magazine,
1990(1).
[27]
Schmandt, C., Voice Communication with Computers, 1994, New York, NY, Van
Nostrand Reinhold.
[28]
Seneff, S., "The Use of Linguistic Hierarchies in Speech Understanding," Int. Conf.
on Spoken Language Processing, 1998, Sydney, Australia.
[29]
Turing, A.M., "Computing Machinery and Intelligence," Mind, 1950, LIX(236),
pp. 433-460.
[30]
van Santen, J., et al., Progress in Speech Synthesis, 1997, New York, Springer-
Verlag.
[31]
Waibel, A.H. and K.F. Lee, Readings in Speech Recognition, 1990, San Mateo,
CA, Morgan Kaufman Publishers.
[32]
Weinschenk, S. and D. Barker, Designing Effective Speech Interfaces, 2000, John
Wiley & Sons, Inc.

19
C H A P T E R
2
Spoken Language StructureEquation Section 2
Spoken language is used to communicate in-
formation from a speaker to a listener. Speech production and perception are both important
components of the speech chain. Speech begins with a thought and intent to communicate in
the brain, which activates muscular movements to produce speech sounds. A listener re-
ceives it in the auditory system, processing it for conversion to neurological signals the brain
can understand. The speaker continuously monitors and controls the vocal organs by receiv-
ing his or her own speech as feedback.
Considering the universal components of speech communication as shown in Figure
2.1, the fabric of spoken interaction is woven from many distinct elements. The speech
production process starts with the semantic message in a person’s mind to be transmitted to
the listener via speech. The computer counterpart to the process of message formulation is
the application semantics that creates the concept to be expressed. After the message is
created, the next step is to convert the message into a sequence of words. Each word consists
of a sequence of phonemes that corresponds to the pronunciation of the words. Each
sentence also contains a prosodic pattern that denotes the duration of each phoneme,
intonation of the sentence, and loudness of the sounds. Once the language system finishes

20
Spoken Language Structure
sentence, and loudness of the sounds. Once the language system finishes the mapping, the
talker executes a series of neuromuscular signals. The neuromuscular commands perform
articulatory mapping to control the vocal cords, lips, jaw, tongue, and velum, thereby pro-
ducing the sound sequence as the final output. The speech understanding process works in
reverse order. First the signal is passed to the cochlea in the inner ear, which performs fre-
quency analysis as a filter bank. A neural transduction process follows and converts the
spectral signal into activity signals on the auditory nerve, corresponding roughly to a feature
extraction component. Currently, it is unclear how neural activity is mapped into the lan-
guage system and how message comprehension is achieved in the brain.
Figure 2.1 The underlying determinants of speech generation and understanding. The gray
boxes indicate the corresponding computer system components for spoken language process-
ing.
Speech signals are composed of analog sound patterns that serve as the basis for a dis-
crete, symbolic representation of the spoken language – phonemes, syllables, and words.
The production and interpretation of these sounds are governed by the syntax and semantics
of the language spoken. In this chapter, we take a bottom up approach to introduce the basic
concepts from sound to phonetics and phonology. Syllables and words are followed by syn-
tax and semantics, which forms the structure of spoken language processing. The examples
in this book are drawn primarily from English, though they are relevant to other languages.
Speech Generation
Speech Understanding
Message Formulation
Language System
Neuromuscular Mapping
Vocal Tract System
Cochlea Motion
Neural Transduction
Language System
Message Comprehension
Articulatory parameter
Feature extraction
Phonemes, words, prosody
Application semantics, actions
Speech
generation
Speech
analysis

Sound and Human Speech Systems
21
2.1.
SOUND AND HUMAN SPEECH SYSTEMS
In this Section, we briefly review human speech production and perception systems. We
hope spoken language research will enable us to build a computer system that is as good as
or better than our own speech production and understanding system.
2.1.1.
Sound
Sound is a longitudinal pressure wave formed of compressions and rarefactions of air mole-
cules, in a direction parallel to that of the application of energy. Compressions are zones
where air molecules have been forced by the application of energy into a tighter-than-usual
configuration, and rarefactions are zones where air molecules are less tightly packed. The
alternating configurations of compression and rarefaction of air molecules along the path of
an energy source are sometimes described by the graph of a sine wave as shown in Figure
2.2. In this representation, crests of the sine curve correspond to moments of maximal com-
pression and troughs to moments of maximal rarefaction.
Figure 2.2 Application of sound energy causes alternating compression/refraction of air mole-
cules, described by a sine wave. There are two important parameters, amplitude and wave-
length, to describe a sine wave. Frequency [cycles/second measured in Hertz (Hz)] is also used
to measure of the waveform.
The use of the sine graph in Figure 2.2 is only a notational convenience for charting
local pressure variations over time, since sound does not form a transverse wave, and the air
particles are just oscillating in place along the line of application of energy. The speed of a
sound pressure wave in air is approximately 331.5
0.6
/
cT m s
+
, where
cT is the Celsius tem-
perature.
The amount of work done to generate the energy that sets the air molecules in motion
is reflected in the amount of displacement of the molecules from their resting position. This
degree of displacement is measured as the amplitude of a sound as shown in Figure 2.2. Be-
cause of the wide range, it is convenient to measure sound amplitude on a logarithmic scale
in decibels (dB). A decibel scale is actually a means for comparing two sounds:
Wavelength
Air Molecules
Amplitude

22
Spoken Language Structure
10
10log
( 1/
2)
P
P
(2.1)
where
1P and
2P are the two power levels.
Sound pressure level (SPL) is a measure of absolute sound pressure P in dB:
10
0
(
)
20log
P
SPL dB
P


=




(2.2)
where
the
reference
0
dB
corresponds
to
the
threshold
of
hearing,
which
is
0
0.0002
P
bar
µ
=
for a tone of 1kHz. The speech conversation level at 3 feet is about 60 dB
SPL, and a jackhammer’s level is about 120 dB SPL. Alternatively, watts/meter2 units are
often used to indicate intensity. We can bracket the limits of human hearing as shown in
Table 2.1. On the low end, the human ear is quite sensitive. A typical person can detect
sound waves having an intensity of 10-12 W/m2 (the threshold of hearing or TOH). This in-
tensity corresponds to a pressure wave affecting a given region by only one-billionth of a
centimeter of molecular motion. On the other end, the most intense sound that can be safely
detected without suffering physical damage is one billion times more intense than the TOH.
0 dB begins with the TOH and advances logarithmically. The faintest audible sound is arbi-
trarily assigned a value of 0 dB, and the loudest sounds that the human ear can tolerate are
about 120 dB.
Table 2.1 Intensity and decibel levels of various sounds.
Sound
dB Level
Times > TOH
Threshold of hearing (TOH:
12
2
10
/
W m
−
)
0
100
Light whisper
10
101
Quiet living room
20
102
Quiet conversation
40
104
Average office
50
105
Normal conversation
60
106
Busy city street
70
107
Acoustic guitar – 1 ft. away
80
108
Heavy truck traffic
90
109
Subway from platform
100
1010
Power tools
110
1011
Pain threshold of ear
120
1012
Airport runway
130
1013
Sonic boom
140
1014
Permanent damage to hearing
150
1015
Jet engine, close up
160
1016
Rocket engine
180
1018
Twelve feet. from artillery cannon muzzle (
10
2
10
/
W m )
220
1022

Sound and Human Speech Systems
23
The absolute threshold of hearing is the maximum amount of energy of a pure tone
that cannot be detected by a listener in a noise free environment. The absolute threshold of
hearing is a function of frequency that can be approximated by
(
)
2
0.8
0.6(
/1000 3.3)
3
4
( )
3.64(
/1000)
6.5
10 (
/1000)
f
qT
f
f
e
f
dB SPL
−
−
−
−
=
−
+
(2.3)
and is plotted in Figure 2.3.
10
2
10
3
10
4
-10
0
10
20
30
40
50
60
70
80
90
100
Frequency (Hz)
SPL (dB)
Figure 2.3 The sound pressure level (SPL) level in dB of the absolute threshold of hearing as a
function of frequency. Sounds below this level are inaudible. Note that below 100 Hz and
above 10 kHz this level rises very rapidly. Frequency goes from 20 Hz to 20 kHz and is plotted
in a logarithmic scale from Eq. (2.3).
Let’s compute how the pressure level varies with distance for a sound wave emitted by
a point source located a distance r away. Assuming no energy absorption or reflection, the
sound wave of a point source is propagated in a spherical front, such that the energy is the
same for the sphere’s surface at all radius r. Since the surface of a sphere of radius r is
2
4 r
π
, the sound’s energy is inversely proportional to
2r , so that every time the distance is
doubled, the sound pressure level decreases by 6 dB. For the point sound source, the energy
(E) transported by a wave is proportional to the square of the amplitude (A) of the wave and
the distance (r) between the sound source and the listener:
2
2
A
E
r
∝
(2.4)
The typical sound intensity of a speech signal one inch away (close-talking micro-
phone) from the talker is 1 Pascal = 10µbar, which corresponds to 94 dB SPL. The typical
sound intensity 10 inches away from a talker is 0.1 Pascal = 1µbar, which corresponds to
74dB SPL.

24
Spoken Language Structure
2.1.2.
Speech Production
We review here basic human speech production systems, which have influenced research on
speech coding, synthesis, and recognition.
2.1.2.1.
Articulators
Speech is produced by air-pressure waves emanating from the mouth and the nostrils of a
speaker. In most of the world’s languages, the inventory of phonemes, as discussed in Sec-
tion 2.2.1, can be split into two basic classes:
 consonants - articulated in the presence of constrictions in the throat or obstruc-
tions in the mouth (tongue, teeth, lips) as we speak.
 vowels - articulated without major constrictions and obstructions.
Tooth-ridge(alveolar):
back part
front-part
Upper Teeth
Upper Lip
Lower Lip
Lower Teeth
Jaw
Vocal Cords
Nasal Cavity
Hard Palate
Velum
Nasal Passage
Tongue:
back
middle
front
tip
Figure 2.4 A schematic diagram of the human speech production apparatus.
The sounds can be further partitioned into subgroups based on certain articulatory
properties. These properties derive from the anatomy of a handful of important articulators
and the places where they touch the boundaries of the human vocal tract. Additionally, a
large number of muscles contribute to articulator positioning and motion. We restrict our-
selves to a schematic view of only the major articulators, as diagrammed in Figure 2.4. The
gross components of the speech production apparatus are the lungs, trachea, larynx (organ of
voice production), pharyngeal cavity (throat), oral and nasal cavity. The pharyngeal and oral

Sound and Human Speech Systems
25
cavities are typically referred to as the vocal tract, and the nasal cavity as the nasal tract. As
illustrated in Figure 2.4, the human speech production apparatus consists of:
 Lungs: source of air during speech.
 Vocal cords (larynx): when the vocal folds are held close together and oscillate
against one another during a speech sound, the sound is said to be voiced. When
the folds are too slack or tense to vibrate periodically, the sound is said to be un-
voiced. The place where the vocal folds come together is called the glottis.
 Velum (Soft Palate): operates as a valve, opening to allow passage of air (and
thus resonance) through the nasal cavity. Sounds produced with the flap open
include m and n.
 Hard palate: a long relatively hard surface at the roof inside the mouth, which,
when the tongue is placed against it, enables consonant articulation.
 Tongue: flexible articulator, shaped away from the palate for vowels, placed
close to or on the palate or other hard surfaces for consonant articulation.
 Teeth: another place of articulation used to brace the tongue for certain conso-
nants.
 Lips: can be rounded or spread to affect vowel quality, and closed completely to
stop the oral air flow in certain consonants (p, b, m).
2.1.2.2.
The Voicing Mechanism
The most fundamental distinction between sound types in speech is the voiced/voiceless
distinction. Voiced sounds, including vowels, have in their time and frequency structure a
roughly regular pattern that voiceless sounds, such as consonants like s, lack. Voiced sounds
typically have more energy as shown in Figure 2.5. We see here the waveform of the word
sees, which consists of three phonemes: an unvoiced consonant /s/, a vowel /iy/ and, a
voiced consonant /z/.
Figure 2.5 Waveform of sees, showing a voiceless phoneme /s, followed by a voiced sound,
the vowel /iy/. The final sound, /z/, is a type of voiced consonant.
What in the speech production mechanism creates this fundamental distinction? When
the vocal folds vibrate during phoneme articulation, the phoneme is considered voiced; oth-
ee (/iy/)
s (/s/)
s (/z/)

26
Spoken Language Structure
erwise it is unvoiced. Vowels are voiced throughout their duration. The distinct vowel tim-
bres are created by using the tongue and lips to shape the main oral resonance cavity in dif-
ferent ways. The vocal folds vibrate at slower or faster rates, from as low as 60 cycles per
second (Hz) for a large man, to as high as 300 Hz or higher for a small woman or child. The
rate of cycling (opening and closing) of the vocal folds in the larynx during phonation of
voiced sounds is called the fundamental frequency. This is because it sets the periodic base-
line for all higher-frequency harmonics contributed by the pharyngeal and oral resonance
cavities above. The fundamental frequency also contributes more than any other single fac-
tor to the perception of pitch (the semi-musical rising and falling of voice tones) in speech.
Figure 2.6 Vocal fold cycling at the larynx. (a) Closed with sub-glottal pressure buildup; (b)
trans-glottal pressure differential causing folds to blow apart; (c) pressure equalization and tis-
sue elasticity forcing temporary reclosure of vocal folds, ready to begin next cycle.
Figure 2.7 Waveform showing air flow during laryngeal cycle.
The glottal cycle is illustrated in Figure 2.6. At stage (a), the vocal folds are closed and
the air stream from the lungs is indicated by the arrow. At some point, the air pressure on the
underside of the barrier formed by the vocal folds increases until it overcomes the resistance
of the vocal fold closure and the higher air pressure below blows them apart (b). However,
the tissues and muscles of the larynx and the vocal folds have a natural elasticity which
tends to make them fall back into place rapidly, once air pressure is temporarily equalized
(c). The successive airbursts resulting from this process are the source of energy for all
voiced sounds. The time for a single open-close cycle depends on the stiffness and size of
(a)
(b)
(c)
Airflow (cm3/s)
100
500
Cy-
Open
Time
8
16
24

Sound and Human Speech Systems
27
the vocal folds and the amount of subglottal air pressure. These factors can be controlled by
a speaker to raise and lower the perceived frequency or pitch of a voiced sound.
The waveform of air pressure variations created by this process can be described as a
periodic flow, in cubic centimeters per second (after [15]). As shown in Figure 2.7, during
the time bracketed as one cycle, there is no air flow during the initial closed portion. Then as
the glottis opens (open phase), the volume of air flow becomes greater. After a short peak,
the folds begin to resume their original position and the air flow declines until complete clo-
sure is attained, beginning the next cycle. A common measure is the number of such cycles
per second (Hz), or the fundamental frequency (F0). Thus the fundamental frequency for the
waveform in Figure 2.7 is about 120 Hz.
2.1.2.3.
Spectrograms and Formants
Since the glottal wave is periodic, consisting of fundamental frequency (F0) and a number
of harmonics (integral multiples of F0), it can be analyzed as a sum of sine waves as dis-
cussed in Chapter 5. The resonances of the vocal tract (above the glottis) are excited by the
glottal energy. Suppose, for simplicity, we regard the vocal tract as a straight tube of uni-
form cross-sectional area, closed at the glottal end, open at the lips. When the shape of the
vocal tract changes, the resonances change also. Harmonics near the resonances are empha-
sized, and, in speech, the resonances of the cavities that are typical of particular articulator
configurations (e.g., the different vowel timbres) are called formants. The vowels in an ac-
tual speech waveform can be viewed from a number of different perspectives, emphasizing
either a cross-sectional view of the harmonic responses at a single moment, or a longer-term
view of the formant track evolution over time. The actual spectral analysis of a vowel at a
single time-point, as shown in Figure 2.8, gives an idea of the uneven distribution of energy
in resonances for the vowel /iy/ in the waveform for see, which is shown in Figure 2.5.
Figure 2.8 A spectral analysis of the vowel /iy/, showing characteristically uneven distribution
of energy at different frequencies.

28
Spoken Language Structure
Another view of sees of Figure 2.5, called a spectrogram, is displayed in the lower part
of Figure 2.9. It shows a long-term frequency analysis, comparable to a complete series of
single time-point cross sections (such as that in Figure 2.8) ranged alongside one another in
time and viewed from above.
0
0.2
0.4
0.6
0.8
1
1.2
-0.5
0
0.5
Time (seconds)
Frequency (Hz)
0
0.2
0.4
0.6
0.8
1
1.2
0
2000
4000
Figure 2.9 The spectrogram representation of the speech waveform sees (approximate phone
boundaries are indicated with heavy vertical lines).
In the spectrogram of Figure 2.9, the darkness or lightness of a band indicates the rela-
tive amplitude or energy present at a given frequency. The dark horizontal bands show the
formants, which are harmonics of the fundamental at natural resonances of the vocal tract
cavity position for the vowel /iy/ in see. The mathematical methods for deriving analyses
and representations such as those illustrated above are covered in Chapters 5 and 6.
2.1.3.
Speech Perception
There are two major components in the auditory perception system: the peripheral auditory
organs (ears) and the auditory nervous system (brain). The ear processes an acoustic pres-
sure signal by first transforming it into a mechanical vibration pattern on the basilar mem-
brane, and then representing the pattern by a series of pulses to be transmitted by the audi-
tory nerve. Perceptual information is extracted at various stages of the auditory nervous sys-
tem. In this section we focus mainly on the auditory organs.
ee (/iy/)
s (/z/)
s (/s/)
F2

Sound and Human Speech Systems
29
2.1.3.1.
Physiology of the Ear
The human ear, as shown in Figure 2.10, has three sections: the outer ear, the middle ear and
the inner ear. The outer ear consists of the external visible part and the external auditory
canal that forms a tube along which sound travels. This tube is about 2.5 cm long and is cov-
ered by the eardrum at the far end. When air pressure variations reach the eardrum from the
outside, it vibrates, and transmits the vibrations to bones adjacent to its opposite side. The
vibration of the eardrum is at the same frequency (alternating compression and rarefaction)
as the incoming sound pressure wave. The middle ear is an air-filled space or cavity about
1.3 cm across, and about 6
3
cm volume. The air travels to the middle ear cavity along the
tube (when opened) that connects the cavity with the nose and throat. The oval window
shown in Figure 2.10 is a small membrane at the bony interface to the inner ear (cochlea).
Since the cochlear walls are bony, the energy is transferred by mechanical action of the sta-
pes into an impression on the membrane stretching over the oval window.
Figure 2.10 The structure of the peripheral auditory system with the outer, middle, and inner
ear (after Lindsey and Norman [26]).
The relevant structure of the inner ear for sound perception is the cochlea, which
communicates directly with the auditory nerve, conducting a representation of sound to the
brain. The cochlea is a spiral tube about 3.5 cm long, which coils about 2.6 times. The spiral
is divided, primarily by the basilar membrane running lengthwise, into two fluid-filled
chambers. The cochlea can be roughly regarded as a filter bank, whose outputs are ordered
by location, so that a frequency-to-place transformation is accomplished. The filters closest
to the cochlear base respond to the higher frequencies, and those closest to its apex respond
to the lower.

30
Spoken Language Structure
2.1.3.2.
Physical vs Perceptual Attributes
In psychoacoustics, a basic distinction is made between the perceptual attributes of a sound,
especially a speech sound, and the measurable physical properties that characterize it. Each
of the perceptual attributes, as listed in Table 2.2, seems to have a strong correlation with
one main physical property, but the connection is complex, because other physical proper-
ties of the sound may affect perception in complex ways.
Table 2.2 Relation between perceptual and physical attributes of sound.
Physical Quantity
Perceptual Quality
Intensity
Loudness
Fundamental frequency
Pitch
Spectral shape
Timbre
Onset/offset time
Timing
Phase difference in binaural hearing
Location
Figure 2.11 Equal-loudness curves indicate that the response of the human hearing mechanism
is a function of frequency and loudness levels. This relationship again illustrates the difference
between physical dimensions and psychological experience (after ISO 226).
Although sounds with a greater intensity level usually sound louder, the sensitivity of
the ear varies with the frequency and the quality of the sound. One fundamental divergence
between physical and perceptual qualities is the phenomenon of non-uniform equal loudness
perception of tones of varying frequencies. In general, tones of differing pitch have different

Sound and Human Speech Systems
31
inherent perceived loudness. The sensitivity of the ear varies with the frequency and the
quality of the sound. The graph of equal loudness contours adopted by ISO is shown in
Figure 2.11. These curves demonstrate the relative insensitivity of the ear to sounds of low
frequency at moderate to low intensity levels. Hearing sensitivity reaches a maximum
around 4000 Hz, which is near the first resonance frequency of the outer ear canal, and
peaks again around 13 kHz, the frequency of the second resonance [38].
Pitch is indeed most closely related to the fundamental frequency. The higher the fun-
damental frequency, the higher the pitch we perceive. However, discrimination between two
pitches depends on the frequency of the lower pitch. Perceived pitch will change as intensity
is increased and frequency is kept constant.
In another example of the non-identity of acoustic and perceptual effects, it has been
observed experimentally that when the ear is exposed to two or more different tones, it is a
common experience that one tone may mask the others. Masking is probably best explained
as an upward shift in the hearing threshold of the weaker tone by the louder tone. Pure tones,
complex sounds, narrow and broad bands of noise all show differences in their ability to
mask other sounds. In general, pure tones close together in frequency mask each other more
than tones widely separated in frequency. A pure tone masks tones of higher frequency more
effectively than tones of lower frequency. The greater the intensity of the masking tone, the
broader the range of the frequencies it can mask [18, 31].
Binaural listening greatly enhances our ability to sense the direction of the sound
source. The sense of localization attention is mostly focused on side-to-side discrimination
or lateralization. Time and intensity cues have different impacts for low frequency and high
frequency, respectively. Low-frequency sounds are lateralized mainly on the basis of inte-
raural time difference, whereas high-frequency sounds are localized mainly on the basis of
interaural intensity differences [5].
Finally, an interesting perceptual issue is the question of distinctive voice quality.
Speech from different people sounds different. Partially this is due to obvious factors, such
as differences in characteristic fundamental frequency caused by, for example, the greater
mass and length of adult male vocal folds as opposed to female. But there are more subtle
effects as well. In psychoacoustics, the concept of timbre (of a sound or instrument) is de-
fined as that attribute of auditory sensation by which a subject can judge that two sounds
similarly presented and having the same loudness and pitch are dissimilar. In other words,
when all the easily measured differences are controlled, the remaining perception of differ-
ence is ascribed to timbre. This is heard most easily in music, where the same note in the
same octave played for the same duration on a violin sounds different from a flute. The tim-
bre of a sound depends on many physical variables including a sound’s spectral power dis-
tribution, its temporal envelope, rate and depth of amplitude or frequency modulation, and
the degree of inharmonicity of its harmonics.
2.1.3.3.
Frequency Analysis
Researchers have undertaken psychoacoustic experimental work to derive frequency scales
that attempt to model the natural response of the human perceptual system, since the cochlea
of the inner ear acts as a spectrum analyzer. The complex mechanism of the inner ear and

32
Spoken Language Structure
auditory nerve implies that the perceptual attributes of sounds at different frequencies may
not be entirely simple or linear in nature. It is well known that the western musical pitch is
described in octaves1 and semi-tones2. The perceived musical pitch of complex tones is basi-
cally proportional to the logarithm of frequency. For complex tones, the just noticeable dif-
ference for frequency is essentially constant on the octave/semi-tone scale. Musical pitch
scales are used in prosodic research (on speech intonation contour generation).
Table 2.3 The Bark frequency scale.
Bark Band #
Edge (Hz)
Center (Hz)
1
100
50
2
200
150
3
300
250
4
400
350
5
510
450
6
630
570
7
770
700
8
920
840
9
1080
1000
10
1270
1170
11
1480
1370
12
1720
1600
13
2000
1850
14
2320
2150
15
2700
2500
16
3150
2900
17
3700
3400
18
4400
4000
19
5300
4800
20
6400
5800
21
7700
7000
22
9500
8500
23
12000
10500
24
15500
13500
AT&T Bell Labs has contributed many influential discoveries in hearing, such as critical
band and articulation index, since the turn of the 20th century [3]. Fletcher’s work [14]
pointed to the existence of critical bands in the cochlear response. Critical bands are of great
importance in understanding many auditory phenomena such as perception of loudness,
pitch, and timbre. The auditory system performs frequency analysis of sounds into their
1 A tone of frequency
1f is said to be an octave above a tone with frequency
2f
if and only if
1
2
2
f
f
=
.
2 There are 12 semitones in one octave, so a tone of frequency
1f is said to be a semitone above a tone with fre-
quency
2f
if and only if
1/12
1
2
2
2
1.05946
f
f
f
=
=
.

Sound and Human Speech Systems
33
component frequencies. The cochlea acts as if it were made up of overlapping filters having
bandwidths equal to the critical bandwidth. One class of critical band scales is called Bark
frequency scale. It is hoped that by treating spectral energy over the Bark scale, a more natu-
ral fit with spectral information processing in the ear can be achieved. The Bark scale ranges
from 1 to 24 Barks, corresponding to 24 critical bands of hearing as shown in Table 2.3. As
shown in Figure 2.12, the perceptual resolution is finer in the lower frequencies. It should be
noted that the ear’s critical bands are continuous, and a tone of any audible frequency al-
ways finds a critical band centered on it. The Bark frequency b can be expressed in terms of
the linear frequency (in Hz) by
(
)
2
( )
13arctan(0.00076 )
3.5*arctan (
/ 7500)
(
)
b f
f
f
Bark
=
+
(2.5)
0
2000
4000
6000
8000
10000
12000
14000
0
5
10
15
20
Filter number
Center Frequency
Figure 2.12 The center frequency of 24 Bark frequency filters as illustrated in Table 2.3.
Another such perceptually motivated scale is the mel frequency scale [41], which is
linear below 1 kHz, and logarithmic above, with equal numbers of samples taken below and
above 1 kHz. The mel scale is based on experiments with simple tones (sinusoids) in which
subjects were required to divide given frequency ranges into four perceptually equal inter-
vals or to adjust the frequency of a stimulus tone to be half as high as that of a comparison
tone. One mel is defined as one thousandth of the pitch of a 1 kHz tone. As with all such
attempts, it is hoped that the mel scale more closely models the sensitivity of the human ear
than a purely linear scale and provides for greater discriminatory capability between speech
segments. Mel-scale frequency analysis has been widely used in modern speech recognition
systems. It can be approximated by:
( )
1125ln(1
/ 700)
B f
f
=
+
(2.6)
The mel scale is plotted in Figure 2.13 together with the Bark scale and the bilinear trans-
form (see Chapter 6).

34
Spoken Language Structure
0
1000
2000
3000
4000
5000
6000
7000
8000
0
0.2
0.4
0.6
0.8
1
frequency (Hz)
warped normalized frequency
Bark scale
mel scale
bilinear transform
Figure 2.13 Frequency warping according to the Bark scale, ERB scale, mel-scale and bilinear
transform for
0.6
α =
: linear frequency in the x-axis and normalized frequency in the y-axis.
A number of techniques in the modern spoken language system, such as cepstral
analysis, and dynamic feature, have benefited tremendously from perceptual research as
discussed throughout this book.
2.1.3.4.
Masking
Frequency masking is a phenomenon under which one sound cannot be perceived if another
sound close in frequency has a high enough level. The first sound masks the other one. Fre-
quency-masking levels have been determined empirically, with complicated models that
take into account whether the masker is a tone or noise, the masker’s level, and other con-
siderations.
We now describe a phenomenon known as tone-masking noise. It has been determined
empirically that noise with energy
N
E
(dB) masks a tone at bark frequency b if the tone’s
energy is below the threshold
( )
6.025
0.275
( )
(
)
T
N
m
T b
E
i
S
b
dB SPL
=
−
−
+
(2.7)
where K has been typically set between 3 and 5 dB, and where the spread-of-masking func-
tion
( )
m
S
b
is given by
2
( )
15.81
7.5(
0.474) 17.5 1
(
0.474)
(
)
m
S
b
b
b
dB
=
+
+
−
+
+
(2.8)

Sound and Human Speech Systems
35
We now describe a phenomenon known as noise-masking tone. It has been determined
empirically that a tone at critical band number i with energy
T
E
(dB) masks noise at bark
frequency b if the noise energy is below the threshold
( )
2.025
0.175
( )
(
)
N
T
m
T
b
E
i
S
b
dB SPL
=
−
−
+
(2.9)
Masking thresholds are commonly referred to in the literature as Bark scale functions
of just noticeable distortion (JND). Equation (2.8) can be approximated by a triangular
spreading function that has slopes of +25 and –10 dB per Bark, as shown in Figure 2.14.
Figure 2.14 Contribution of frequency bin i to the masked threshold
( )
m
S
b .
In Figure 2.15 we show both the threshold of hearing and the masked threshold of a
tone at 1 kHz with a 69 dB SPL. The combined masked threshold is the sum of the two in
the linear domain
(
)
0.1
(
)
0.1
(
)
10
( )
10log
10
10
h
T
T
f
T
f
T f
=
+
(2.10)
which is approximately the largest of the two.
10
2
10
3
10
4
-10
0
10
20
30
40
50
60
70
80
90
100
Frequency (Hz)
SPL (dB)
Figure 2.15 Absolute Threshold of Hearing and Spread of Masking threshold for a 1 kHz
sinewave masker with a 69 dB SPL. The overall masked threshold is approximately the largest
of the two thresholds.
b
Frequency (Barks)
10 dB / Bark
25 dB / Bark
20log
( )
m
S
b

36
Spoken Language Structure
In addition to frequency masking, there is a phenomenon called temporal masking by
which a sound too close in time to another sound cannot be perceived. Whereas premasking
tends to last about 5 ms, postmasking can last from 50 to 300 ms. Temporal masking level of
a masker with a uniform level starting at 0 ms and lasting 200 ms is shown in Figure 2.16.
-100
-50
0
50
100
150
200
250
300
350
400
0
10
20
30
40
50
60
Time (ms)
Temporal Threshold (dB)
Figure 2.16 Temporal masking level of a masker with a uniform level starting at 0 ms and
lasting 200 ms.
2.2.
PHONETICS AND PHONOLOGY
We now discuss basic phonetics and phonology needed for spoken language processing.
Phonetics refers to the study of speech sounds and their production, classification, and tran-
scription. Phonology is the study of the distribution and patterning of speech sounds in a
language and of the tacit rules governing pronunciation.
2.2.1.
Phonemes
Linguist Ferdinand de Saussere (1857-1913) is credited with the observation that the relation
between a sign and the object signified by it is arbitrary. The same concept, a certain yellow
and black flying social insect, has the sign honeybee in English and mitsubachi in Japanese.
There is no particular relation between the various pronunciations and the meaning, nor do
these pronunciations per se describe the bee’s characteristics in any detail. For phonetics,
this means that the speech sounds described in this chapter have no inherent meaning, and
should be randomly distributed across the lexicon, except as affected by extraneous histori-
cal or etymological considerations. The sounds are just a set of arbitrary effects made avail-
able by human vocal anatomy. You might wonder about this theory when you observe, for
example, the number of words beginning with sn that have to do with nasal functions in
English: sneeze, snort, sniff, snot, snore, snuffle, etc. But Saussere’s observation is generally
true, except for obvious onomatopoetic (sound) words like buzz.

Phonetics and Phonology
37
Like fingerprints, every speaker’s vocal anatomy is unique, and this makes for unique
vocalizations of speech sounds. Yet language communication is based on commonality of
form at the perceptual level. To allow discussion of the commonalities, researchers have
identified certain gross characteristics of speech sounds that are adequate for description and
classification of words in dictionaries. They have also adopted various systems of notation
to represent the subset of phonetic phenomena that are crucial for meaning.
As an analogy, consider the system of computer coding of text characters. In such sys-
tems, the character is an abstraction, e.g. the Unicode character U+0041. The identifying
property of this character is its Unicode name LATIN CAPITAL LETTER A. This is a genu-
ine abstraction; no particular realization is necessarily specified. As the Unicode 2.1 stan-
dard [1] states:
The Unicode Standard does not define glyph images. The standard defines how char-
acters are interpreted, not how glyphs are rendered. The software or hardware-rendering
engine of a computer is responsible for the appearance of the characters on the screen. The
Unicode Standard does not specify the size, shape, nor orientation of on-screen characters.
Thus, the U+0041 character can be realized differently for different purposes, and in
different sizes with different fonts:
U+0041! A, A, A, …
The realizations of the character U+0041 are called glyphs, and there is no distin-
guished uniquely correct glyph for U+0041. In speech science, the term phoneme is used to
denote any of the minimal units of speech sound in a language that can serve to distinguish
one word from another. We conventionally use the term phone to denote a phoneme’s
acoustic realization. In the example given above, U+0041 corresponds to a phoneme and the
various fonts correspond to the phone. For example, English phoneme /t/ have two very dif-
ferent acoustic realizations in the words sat and meter. You had better treat them as two dif-
ferent phones if you want to build a spoken language system. We will use the terms phone
or phoneme interchangeably to refer to the speaker-independent and context-independent
units of meaningful sound contrast. Table 2.4 shows a complete list of phonemes used in
American English. The set of phonemes will differ in realization across individual speakers.
But phonemes will always function systematically to differentiate meaning in words, just as
the phoneme /p/ signals the word pat as opposed to the similar-sounding but distinct bat.
The important contrast distinguishing this pair of words is /p/ vs. /b/.
In this section we concentrate on the basic qualities that define and differentiate ab-
stract phonemes. In Section 2.2.1.3 below we consider why and how phonemes vary in their
actual realizations by different speakers and in different contexts.

38
Spoken Language Structure
Table 2.4 English phonemes used for typical spoken language systems.
Phonemes
Word Examples
Description
ih
fill, hit, lid
front close unrounded (lax)
ae
at, carry, gas
front open unrounded (tense)
aa
father, ah, car
back open unrounded
ah
cut, bud, up
open-mid back unrounded
ao
dog, lawn, caught
open-mid back round
ay
tie, ice, bite
diphthong with quality: aa + ih
ax
ago, comply
central close mid (schwa)
ey
ate, day, tape
front close-mid unrounded (tense)
eh
pet, berry, ten
front open-mid unrounded
er
turn, fur, meter
central open-mid unrounded rhoti-
ow
go, own, tone
back close-mid rounded
aw
foul, how, our
diphthong with quality: aa + uh
oy
toy, coin, oil
diphthong with quality: ao + ih
uh
book, pull, good
back close-mid unrounded (lax)
uw
tool, crew, moo
back close round
b
big, able, tab
voiced bilabial plosive
p
put, open, tap
voiceless bilabial plosive
d
dig, idea, wad
voiced alveolar plosive
t
talk, sat
voiceless alveolar plosive &
t
meter
alveolar flap
g
gut, angle, tag
voiced velar plosive
k
cut, ken, take
voiceless velar plosive
f
fork, after, if
voiceless labiodental fricative
v
vat, over, have
voiced labiodental fricative
s
sit, cast, toss
voiceless alveolar fricative
z
zap, lazy, haze
voiced alveolar fricative
th
thin, nothing, truth
voiceless dental fricative
dh
Then, father, scythe
voiced dental fricative
sh
she, cushion, wash
voiceless postalveolar fricative
zh
genre, azure
voiced postalveolar fricative
l
lid
alveolar lateral approximant
l
elbow, sail
velar lateral approximant
r
red, part, far
retroflex approximant
y
yacht, yard
palatal sonorant glide
w
with, away
labiovelar sonorant glide
hh
help, ahead, hotel
voiceless glottal fricative
m
mat, amid, aim
bilabial nasal
n
no, end, pan
alveolar nasal
ng
sing, anger
velar nasal
ch
chin, archer, march
voiceless alveolar affricate: t + sh
jh
joy, agile, edge
voiced alveolar affricate: d + zh

Phonetics and Phonology
39
2.2.1.1.
Vowels
The tongue shape and positioning in the oral cavity do not form a major constriction of air
flow during vowel articulation. However, variations of tongue placement give each vowel its
distinct character by changing the resonance, just as different sizes and shapes of bottles
give rise to different acoustic effects when struck. The primary energy entering the pharyn-
geal and oral cavities in vowel production vibrates at the fundamental frequency. The major
resonances of the oral and pharyngeal cavities for vowels are called F1 and F2 - the first and
second formants, respectively. They are determined by tongue placement and oral tract
shape in vowels, and they determine the characteristic timbre or quality of the vowel.
The relationship of F1 and F2 to one another can be used to describe the English vow-
els. While the shape of the complete vocal tract determines the spectral outcome in a com-
plex, nonlinear fashion, generally F1 corresponds to the back or pharyngeal portion of the
cavity, while F2 is determined more by the size and shape of the oral portion, forward of the
major tongue extrusion. This makes intuitive sense - the cavity from the glottis to the tongue
extrusion is longer than the forward part of the oral cavity, thus we would expect its reso-
nance to be lower. In the vowel of see, for example, the tongue extrusion is far forward in
the mouth, creating an exceptionally long rear cavity, and correspondingly low F1. The for-
ward part of the oral cavity, at the same time, is extremely short, contributing to higher F2.
This accounts for the wide separation of the two lowest dark horizontal bands in Figure 2.9,
corresponding to F1 and F2, respectively. Rounding the lips has the effect of extending the
front-of-tongue cavity, thus lowering F2. Typical values of F1 and F2 of American English
vowels are listed in Table 2.5.
Table 2.5 Phoneme labels and typical formant values for vowels of English.
Vowel Labels Mean F1 (Hz) Mean F2 (Hz)
iy (feel)
300
2300
ih (fill)
360
2100
ae (gas)
750
1750
aa (father)
680
1100
ah (cut)
720
1240
ao (dog)
600
900
ax (comply)
720
1240
eh (pet)
570
1970
er (turn)
580
1380
ow (tone)
600
900
uh (good)
380
950
uw (tool)
300
940
The characteristic F1 and F2 values for vowels are sometimes called formant targets,
which are ideal locations for perception. Sometimes, due to fast speaking or other limitations
on performance, the speaker cannot quite attain an ideal target before the articulators begin
shifting to targets for the following phoneme, which is phonetic context dependent. Addi-
tionally, there is a special class of vowels that combine two distinct sets of F1/F2 targets.

40
Spoken Language Structure
These are called diphthongs. As the articulators move, the initial vowel targets glide
smoothly to the final configuration. Since the articulators are working faster in production of
a diphthong, sometimes the ideal formant target values of the component values are not fully
attained. Typical diphthongs of American English are listed in Table 2.6.
Table 2.6 The diphthongs of English.
Diphthong Labels
Components
ay (tie)
/aa/ ! /iy/
ey (ate)
/eh/ ! /iy/
oy (coin)
/ao/ ! /iy/
aw (foul)
/aa/ ! /uw/
Figure 2.17 shows the first two formants for a number of typical vowels.
0
500
1000
1500
2000
2500
/iy/ (feel)
/ih/ (fill)
/ae/ (gas)
/aa/ (father)
/ah/ (cut)
/ao/ (dog)
Vowel Phonemes
Frequency
F2 (Hz)
F1 (Hz)
Figure 2.17 F1 and F2 values for articulations of some English vowels.
The major articulator for English vowels is the middle to rear portion of the tongue.
The position of the tongue’s surface is manipulated by large and powerful muscles in its
root, which move it as a whole within the mouth. The linguistically important dimensions of
movement are generally the ranges [front " back] and [high " low]. You can feel this
movement easily. Say mentally, or whisper, the sound /iy/ (as in see) and then /aa/ (as in
father). Do it repeatedly, and you will get a clear perception of the tongue movement from
high to low. Now try /iy/ and then /uw/ (as in blue), repeating a few times. You will get a
clear perception of place of articulation from front /iy/ to back /uw/. Figure 2.18 shows a
schematic characterization of English vowels in terms of relative tongue positions. There are
two kinds of vowels: those in which tongue height is represented as a point and those in
which it is represented as a vector.
Though the tongue hump is the major actor in vowel articulation, other articulators
come into play as well. The most important secondary vowel mechanism for English and
many other languages is lip rounding. Repeat the exercise above, moving from the /iy/ (see)

Phonetics and Phonology
41
to the /uw/ (blue) position. Now rather than noticing the tongue movement, pay attention to
your lip shape. When you say /iy/, your lips will be flat, slightly open, and somewhat spread.
As you move to /uw/, they begin to round out, ending in a more puckered position. This
lengthens the oral cavity during /uw/, and affects the spectrum in other ways.
Figure 2.18 Relative tongue positions of English vowels [24].
Though there is always some controversy, linguistic study of phonetic abstractions,
called phonology, has largely converged on the five binary features: +/- high, +/- low, +/-
front, +/-back, and +/-round, plus the phonetically ambiguous but phonologically useful fea-
ture +/- tense, as adequate to uniquely characterize the major vowel distinctions of Standard
English (and many other languages). Obviously, such a system is a little bit too free with
logically contradictory specifications, such as [+high, +low], but these are excluded from
real-world use. These features can be seen in Table 2.7.
Table 2.7 Phonological (abstract) feature decomposition of basic English vowels.
Vowel
high
low
front
back
round
tense
iy
+
-
+
-
-
+
ih
+
-
+
-
-
-
ae
-
+
+
-
-
+
aa
-
+
-
-
-
+
ah
-
-
-
-
-
+
ao
-
+
-
+
+
+
ax
-
-
-
-
-
-
eh
-
-
+
-
-
-
ow
-
-
-
+
+
+
uh
+
-
-
+
-
-
uw
+
-
-
+
-
+
iy
uw
ih
eh
ae
uh
ax
uh
ow
y uw
aa
ao
aw
ey
ay
oy
back
high
front
low

42
Spoken Language Structure
This kind of abstract analysis allows researchers to make convenient statements about
classes of vowels that behave similarly under certain conditions. For example, one may
speak simply of the high vowels to indicate the set /iy, ih, uh, uw/.
2.2.1.2.
Consonants
Consonants, as opposed to vowels, are characterized by significant constriction or obstruc-
tion in the pharyngeal and/or oral cavities. Some consonants are voiced; others are not.
Many consonants occur in pairs, that is, they share the same configuration of articulators,
and one member of the pair additionally has voicing which the other lacks. One such pair is
/s, z/, and the voicing property that distinguishes them shows up in the non-periodic noise of
the initial segment /s/ in Figure 2.5 as opposed to the voiced consonant end-phone, /z/. Man-
ner of articulation refers to the articulation mechanism of a consonant. The major distinc-
tions in manner of articulation are listed in Table 2.8.
Table 2.8 Consonant manner of articulation.
Manner
Sample
Phone
Example
Words
Mechanism
Plosive
/p/
tat, tap
Closure in oral cavity
Nasal
/m/
team, meet
Closure of nasal cavity
Fricative
/s/
sick, kiss
Turbulent airstream noise
Retroflex liquid
/r/
rat, tar
Vowel-like, tongue high and curled back
Lateral liquid
/l/
lean, kneel
Vowel-like, tongue central, side airstream
Glide
/y/,/w/
yes, well
Vowel-like
The English phones that typically have voicing without complete obstruction or nar-
rowing of the vocal tract are called semivowels and include /l, r/, the liquid group, and /y, w/,
the glide group. Liquids, glides, and vowels are all sonorant, meaning they have continuous
voicing. Liquids /l/ and /r/ are quite vowel-like and in fact may become syllabic or act en-
tirely as vowels in certain positions, such as the l at the end of edible. In /l/, the airstream
flows around the sides of the tongue, leading to the descriptive term lateral. In /r/, the tip of
the tongue is curled back slightly, leading to the descriptive term retroflex. Figure 2.19
shows some semivowels.
Glides /y, w/ are basically vowels /iy, uw/ whose initial position within the syllable re-
quire them to be a little shorter and to lack the ability to be stressed, rendering them just
different enough from true vowels that they are classed as a special category of consonant.
Pre-vocalic glides that share the syllable-initial position with another consonant, such as the
/y/ in the second syllable of computer /k uh m . p y uw . t er/, or the /w/ in quick /k w ih k/,
are sometimes called on-glides. The semivowels, as a class, are sometimes called approxi-
mants, meaning that the tongue approaches the top of the oral cavity, but does not com-
pletely contact so as to obstruct the air flow.
Even the non-sonorant consonants that require complete or close-to-complete obstruc-
tion may still maintain some voicing before or during the obstruction, until the pressure dif-
ferential across the glottis starts to disappear, due to the closure. Such voiced consonants

Phonetics and Phonology
43
include /b,d,g, z, zh, v/. They have a set of counterparts that differ only in their characteristic
lack of voicing: /p,t,k, s, sh, f/.
0
0.1
0.2
0.3
0.4
0.5
0.6
-0.5
0
0.5
Time (seconds)
Frequency (Hz)
0
0.1
0.2
0.3
0.4
0.5
0.6
0
1000
2000
3000
4000
Figure 2.19 Spectrogram for the word yeller, showing semivowels /y/, /l/, /er/ (approximate
phone boundaries shown with vertical lines).
Nasal consonants /m,n/ are a mixed bag: the oral cavity has significant constriction (by
the tongue or lips), yet the voicing is continuous, like that of the sonorants, because, with the
velar flap open, air passes freely through the nasal cavity, maintaining a pressure differential
across the glottis.
A consonant that involves complete blockage of the oral cavity is called an obstruent
stop, or plosive consonant. These may be voiced throughout if the trans-glottal pressure drop
can be maintained long enough, perhaps through expansion of the wall of the oral cavity. In
any case, there can be voicing for the early sections of stops. Voiced, unvoiced pairs of stops
include: /b,p/, /d,t/, and /g,k/. In viewing the waveform of a stop, a period of silence corre-
sponding to the oral closure can generally be observed. When the closure is removed (by
opening the constrictor, which may be lips or tongue), the trapped air rushes out in a more or
less sudden manner. When the upper oral cavity is unimpeded, the closure of the vocal folds
themselves can act as the initial blocking mechanism for a type of stop heard at the very
beginning of vowel articulation in vowel-initial words like atrophy. This is called a glottal
stop. Voiceless plosive consonants in particular exhibit a characteristic aperiodic burst of
energy at the (articulatory) point of closure as shown in Figure 2.20 just prior to /i/. By com-
parison, the voicing of voiced plosive consonants may not always be obvious in a spectro-
gram.
/y/
/eh/
/l/
/er/

44
Spoken Language Structure
0
0.1
0.2
0.3
0.4
0.5
0.6
-0.5
0
0.5
Time (seconds)
Frequency (Hz)
0
0.1
0.2
0.3
0.4
0.5
0.6
0
1000
2000
3000
4000
Figure 2.20 Spectrogram: stop release burst of /p/ in the word pin.
A consonant that involves nearly complete blockage of some position in the oral cav-
ity creates a narrow stream of turbulent air. The friction of this air stream creates a non-
periodic hiss-like effect. Sounds with this property are called fricatives and include /s, z/.
There is no voicing during the production of s, while there can be voicing (in addition to the
frication noise), during the production of z, as discussed above. /s, z/ have a common place
of articulation, as explained below, and thus form a natural similarity class. Though contro-
versial, /h/ can also be thought of as a (glottal) fricative. /s/ in word-initial position and /z/ in
word-final position are exemplified in Figure 2.5.
Some sounds are complex combinations of manners of articulation. For example, the
affricates consist of a stop (e.g., /t/), followed by a fricative [e.g., /sh/) combining to make a
unified sound with rapid phases of closure and continuancy (e.g., {t + sh) = ch as in church).
The affricates in English are the voiced/unvoiced pairs: /j/ (d + zh) and /ch/ (t + sh). The
complete consonant inventory of English is shown in Table 2.9.
Consider the set /m/, /n/, /ng/ from Table 2.9. They are all voiced nasal consonants, yet
they sound distinct to us. The difference lies in the location of the major constriction along
the top of the oral cavity (from lips to velar area) that gives each consonant its unique qual-
ity. The articulator used to touch or approximate the given location is usually some spot
along the length of the tongue. As shown in Figure 2.21, the combination of articulator and
place of articulation gives each consonant its characteristic sound:
 The labial consonants have their major constriction at the lips. This includes /p/,
/b/ (these two differ only by manner of articulation) and /m/ and /w/.
p(/p/)
i(/ih/)
n(/n/)

Phonetics and Phonology
45
 The class of dental or labio-dental consonants includes /f, v/ and /th, dh/ (the
members of these groups differ in manner, not place).
 Alveolar consonants bring the front part of the tongue, called the tip or the part
behind the tip called the blade, into contact or approximation to the alveolar
ridge, rising semi-vertically above and behind the teeth. These include /t, d, n, s,
z, r, l/. The members of this set again differ in manner of articulation (voicing,
continuity, nasality), rather than place.
 Palatal consonants have approximation or constriction on or near the roof of the
mouth, called the palate. The members include /sh, zh, y/.
 Velar consonants bring the articulator (generally the back of the tongue), up to
the rearmost top area of the oral cavity, near the velar flap. Velar consonants in
English include /k, g/ (differing by voicing) and the nasal continuant /ng/.
Table 2.9 Manner of articulation of English consonants.
Consonant Labels
Consonant Examples
Voiced?
Manner
b
big, able, tab
+
plosive
p
put, open, tap
-
plosive
d
dig, idea, wad
+
plosive
t
talk, sat
-
plosive
g
gut, angle, tag
+
plosive
k
cut, oaken, take
-
plosive
v
vat, over, have
+
fricative
f
fork, after, if
-
fricative
z
zap, lazy, haze
+
fricative
s
sit, cast, toss
-
fricative
dh
then, father, scythe
+
fricative
th
thin, nothing, truth
-
fricative
zh
genre, azure, beige
+
fricative
sh
she, cushion, wash
-
fricative
jh
joy, agile, edge
+
affricate
ch
chin, archer, march
-
affricate
l
lid, elbow, sail
+
lateral
r
red, part, far
+
retroflex
y
yacht, onion, yard
+
glide
w
with, away
+
glide
hh
help, ahead, hotel
+
fricative
m
mat, amid, aim
+
nasal
n
no, end, pan
+
nasal
ng
sing, anger, drink
+
nasal
With the place terminology, we can complete the descriptive inventory of English
consonants, arranged by manner (rows), place (columns) and voiceless/voiced (pairs in
cells) as illustrated in Table 2.10.

46
Spoken Language Structure
Figure 2.21 The major places of consonant articulation with respect to human mouth.
Table 2.10 The consonants of English arranged by place (columns) and manner (rows).
Labial
Labio-
dental
Dental
Alveolar
Palatal
Velar
Glottal
Plosive
p b
t d
k g
?
Nasal
m
n
ng
Fricative
f v
th dh
s z
sh zh
h
Retroflex
Sonorant
r
Lateral
sonorant
l
Glide
w
y
2.2.1.3.
Phonetic Typology
The oral, nasal, pharyngeal, and glottal mechanisms actually make available a much wider
range of effects than English happens to use. So, it is expected that other languages would
utilize other vocal mechanisms, in an internally consistent but essentially arbitrary fashion,
to represent their lexicons. In addition, often a vocal effect that is part of the systematic lin-
guistic phonetics of one language is present in others in a less codified, but still perceptible,
form. For example, Japanese vowels have a characteristic distinction of length that can be
hard for non-natives to perceive and to use when learning the language. The words kado
(corner) and kaado (card) are spectrally identical, differing only in that kado is much shorter
in all contexts. The existence of such minimally-contrasting pairs is taken as conclusive evi-
dence that length is phonemically distinctive for Japanese. As noted above, what is linguisti-
cally distinctive in any one language is generally present as a less meaningful signaling di-
mension in other languages. Thus, vowel length can be manipulated in any English word as
well, but this occurs either consciously for emphasis or humorous effect, or unconsciously
and very predictably at clause and sentence end positions, rather than to signal lexical iden-
tity in all contexts, as in Japanese.
Other interesting sounds that the English language makes no linguistic use of include
the trilled r sound and the implosive. The trilled r sound is found in Spanish, distinguishing
Labial:
m, p, b, w
Dental:
th, dh
Alveolar:
t, d, n, s, z, r, l
Palatal:
sh, zh,y
Velar:
k, g, ng

Phonetics and Phonology
47
(for example) the words pero (but) and perro (dog). This trill could be found in times past as
a non-lexical sound used for emphasis and interest by American circus ringmasters and other
showpersons.
While the world’s languages have all the variety of manner of articulation exemplified
above and a great deal more, the primary dimension lacking in English that is exploited by a
large subset of the world’s languages is pitch variation. Many of the huge language families
of Asia and Africa are tonal, including all varieties of Chinese. A large number of other lan-
guages are not considered strictly tonal by linguistics, yet they make systematic use of pitch
contrasts. These include Japanese and Swedish. To be considered tonal, a language should
have lexical meaning contrasts cued by pitch, just as the lexical meaning contrast between
pig and big is cued by a voicing distinction in English. For example, Mandarin Chinese has
four primary tones (tones can have minor context-dependent variants just like ordinary
phones, as well):
Table 2.11 The contrastive tones of Mandarin Chinese.
Tone
Shape
Example
Chinese
Meaning
1
High level
ma
妈
mother
2
High rising
ma
麻
numb
3
Low rising
ma
马
horse
4
High falling
ma
骂
to scold
Though English does not make systematic use of pitch in its inventory of word con-
trasts, nevertheless, as we always see with any possible phonetic effect, pitch is systemati-
cally varied in English to signal a speaker’s emotions, intentions, and attitudes, and it has
some linguistic function in signaling grammatical structure as well. Pitch variation in Eng-
lish will be considered in more detail in Chapter 15.
2.2.2.
The Allophone: Sound and Context
The vowel and consonant charts provide abstract symbols for the phonemes - major sound
distinctions. Phonemic units should be correlated with potential meaning distinctions. For
example, the change created by holding the tongue high and front (/iy/) vs. directly down
from the (frontal) position for /eh/, in the consonant context /m _ n/, corresponds to an im-
portant meaning distinction in the lexicon of English: mean /m iy n/ vs. men /m eh n/. This
meaning contrast, conditioned by a pair of rather similar sounds, in an identical context,
justifies the inclusion of /iy/ and /eh/ as logically separate distinctions.
However, one of the fundamental, meaning-distinguishing sounds is often modified in
some systematic way by its phonetic neighbors. The process by which neighboring sounds
influence one another is called coarticulation. Sometimes, when the variations resulting
from coarticulatory processes can be consciously perceived, the modified phonemes are
called allophones. Allophonic differences are always categorical, that is, they can be under-
stood and denoted by means of a small, bounded number of symbols or diacritics on the
basic phoneme symbols.

48
Spoken Language Structure
As an experiment, say the word like to yourself. Feel the front of the tongue touching
the alveolar ridge (cf. Figure 2.21) when realizing the initial phoneme /l/. This is one allo-
phone of /l/, the so-called light or clear /l/. Now say kill. In this word, most English speakers
will no longer feel the front part of the tongue touch the alveolar ridge. Rather, the /l/ is real-
ized by stiffening the broad mid section of the tongue in the rear part of the mouth while the
continuant airstream escapes laterally. This is another allophone of /l/, conditioned by its
syllable-final position, called the dark /l/. Predictable contextual effects on the realization of
phones can be viewed as a nuisance for speech recognition, as will be discussed in Chapter
9. On the other hand, such variation, because it is systematic, could also serve as a cue to the
syllable, word, and prosodic structure of speech.
Now experiment with the sound /p/ by holding a piece of tissue in front of your mouth
while saying the word pin in a normal voice. Now repeat this experiment with spin. For
most English speakers, the word pin produces a noticeable puff of air, called aspiration. But
the same phoneme, /p/, embedded in the consonant cluster /sp/ loses its aspiration (burst, see
the lines bracketing the /p/ release in pin and spin in Figure 2.22), and because these two
types of /p/ are in complementary distribution (completely determined by phonetic and syl-
labic context), the difference is considered allophonic.
0
0.5
1
1.5
-0.5
0
0.5
Time (seconds)
Frequency (Hz)
0
0.5
1
1.5
0
1000
2000
3000
4000
Figure 2.22 Spectrogram: bursts of pin and spin. The relative duration of a p-burst in different
phonetic contexts is shown by the differing width of the area between the vertical lines.
Try to speak the word bat in a framing phrase say bat again. Now speak say bad
again. Can you feel the length difference in the vowel /ae/? A vowel before a voiced conso-
nant e.g., /d/, seems typically longer than the same vowel before the unvoiced counterpart, in
this case /t/.
pin (/p ih n/)
spin (/s p ih n/)

Phonetics and Phonology
49
A sound phonemicized as /t/ or /d/, that is, a stop made with the front part of the
tongue, may be reduced to a quick tongue tap that has a different sound than either /t/ or /d/
in fuller contexts. This process is called flapping. It occurs when /t/ or /d/ closes a stressed
vowel (coda position) followed by an unstressed vowel, as in: bitter, batter, murder,
quarter, humidity, and can even occur across words as long as the preconditions are met, as
in you can say that again. Sometimes the velar flap opens too soon (anticipation), giving a
characteristically nasal quality to some pre-nasal vowels such as /ae/ in ham vs. had. We
have a more detailed discussion on allophones in Chapter 9.
2.2.3.
Speech Rate and Coarticulation
In addition to allophones, there are other variations in speech for which no small set of es-
tablished categories of variation can be established. These are gradient, existing along a
scale for each relevant dimension, with speakers scattered widely. In general, it is harder to
become consciously aware of coarticulation effects than of allophonic alternatives.
Individual speakers may vary their rates according to the content and setting of their
speech, and there may be great inter-speaker differences as well. Some speakers may pause
between every word, while others may speak hundreds of words per minute with barely a
pause between sentences. At the faster rates, formant targets are less likely to be fully
achieved. In addition, individual allophones may merge.
For example [20], consider the utterance Did you hit it to Tom? The pronunciation of
this utterance is /d ih d y uw h ih t ih t t uw t aa m/. However, a realistic, casual rendition of
this sentence would appear as /d ih jh ax hh ih dx ih t ix t aa m/, where /ix/ is a reduced
schwa /ax/ that is short and often unvoiced, and /dx/ is a kind of shortened, indistinct stop,
intermediate between /d/ and /t/. The following five phonologic rules have operated on alter-
ing the pronunciation in the example:
 Palatalization of /d/ before /y/ in did you
 Reduction of unstressed /u/ to schwa in you
 Flapping of intervocalic /t/ in hit it
 Reduction of schwa and devoicing of /u/ in to
 Reduction of geminate (double consonant) /t/ in it to
There are also coarticulatory influences in the spectral appearance of speech sounds,
which can only be understood at the level of spectral analysis. For example, in vowels, con-
sonant neighbors can have a big effect on formant trajectories near the boundary. Consider
the differences in F1 and F2 in the vowel /eh/ as realized in words with different initial con-
sonants bet, debt, and get, corresponding to the three major places of articulation (labial,
alveolar, and velar), illustrated in Figure 2.23. You can see the different relative spreads of
F1 and F2 following the initial stop consonants.

50
Spoken Language Structure
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
-0.5
0
0.5
Time (seconds)
Frequency (Hz)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
0
1000
2000
3000
4000
Figure 2.23 Spectrogram: bet, debt, and get (separated by vertical lines). Note different rela-
tive spreads of F1 and F2 following the initial stop consonants in each word.
Now let’s see different consonants following the same vowel, ebb, head, and egg. In
Figure 2.23, the coarticulatory effect is perseverance; i.e., in the early part of the vowel the
articulators are still somewhat set from realization of the initial consonant. In the ebb, head,
and egg examples shown in Figure 2.24, the coarticulatory effect is anticipation; i.e., in the
latter part of the vowel the articulators are moving to prepare for the upcoming consonant
articulation. You can see the increasing relative spread of F1 and F2 at the final vowel-
consonant transition in each word.
2.3.
SYLLABLES AND WORDS
Phonemes are small building blocks. To contribute to language meaning, they must be or-
ganized into longer cohesive spans, and the units so formed must be combined in character-
istic patterns to be meaningful, such as syllables and words in the English language.
debt (/d eh t/)
bet (/b eh t/)
get (/g eh t/)

Syllables and Words
51
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
-0.5
0
0.5
Time (seconds)
Frequency (Hz)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
0
1000
2000
3000
4000
Figure 2.24 Spectrogram: ebb, head, and egg. Note the increasing relative spread of F1 and F2
at the final vowel-consonant transition in each word.
2.3.1.
Syllables
An intermediate unit, the syllable, is sometimes thought to interpose between the phones and
the word level. The syllable is a slippery concept, with implications for both production and
perception. Here we will treat it as a perceptual unit. Syllables are generally centered around
vowels in English, giving two perceived syllables in a word like tomcat: /tOm-cAt/. To
completely parse a word into syllables requires making judgments of consonant affiliation
(with the syllable peak vowels). The question of whether such judgments should be based on
articulatory or perceptual criteria, and how they can be rigorously applied, remains unre-
solved.
Syllable centers can be thought of as peaks in sonority (high-amplitude, periodic sec-
tions of the speech waveform). These sonority peaks have affiliated shoulders of strictly
non-increasing sonority. A scale of sonority can be used, ranking consonants along a contin-
uum of stops, affricates, fricatives, and approximants. So, in a word like verbal, the syllabi-
fication would be ver-bal, or verb-al, but not ve-rbal, because putting the approximant /r/
before the stop /b/ in the second syllable would violate the non-decreasing sonority require-
ment heading into the syllable.
ebb (/ eh b/)
head (/hh eh d/)
egg (/eh g/)

52
Spoken Language Structure
As long as the sonority conditions are met, the exact affiliation of a given consonant
that could theoretically affiliate on either side can be ambiguous, unless determined by
higher-order considerations of word structure, which may block affiliation. For example, in
a word like beekeeper, an abstract boundary in the compound between the component words
bee and keeper keeps us from accepting the syllable parse: beek-eeper, based on lexical in-
terpretation. However, the same phonetic sequence in beaker could, depending on one’s
theory of syllabicity, permit affiliation of the k: beak-er. In general, the syllable is a unit that
has intuitive plausibility but remains difficult to pin down precisely.
Figure 2.25 The word/syllable strengths (/s t r eh nx th s/) is a longest syllable of English.
Syllables are thought (by linguistic theorists) to have internal structure, and the terms
used are worth knowing. Consider a big syllable such as strengths /s t r eh nx th s/. This con-
sists of a vowel peak, called the nucleus, surrounded by the other sounds in characteristic
positions. The onset consists of initial consonants if any, and the rime is the nucleus with
trailing consonants (the part of the syllable that matters in determining poetic rhyme). The
coda consists of consonants in the rime following the nucleus (in some treatments, the last
consonant in a final cluster would belong to an appendix). This can be diagrammed as a syl-
lable parse tree as shown in Figure 2.25. The syllable is sometimes thought to be the primary
domain of coarticulation, that is, sounds within a syllable influence one another’s realization
more than the same sounds separated by a syllable boundary.
2.3.2.
Words
The concept of words seems intuitively obvious to most speakers of Indo-European lan-
guages. It can be loosely defined as a lexical item, with an agreed-upon meaning in a given
speech community, that has the freedom of syntactic combination allowed by its type (noun,
verb, etc.).
Syllable
Onset
Rime
Coda
Nucleus
s t r
nx th s
eh

Syllables and Words
53
In spoken language, there is a segmentation problem: words run together unless af-
fected by a disfluency (unintended speech production problem) or by the deliberate place-
ment of a pause (silence) for some structural or communicative reason. This is surprising to
many people, because literacy has conditioned speakers/readers of Indo-European languages
to expect a blank space between words on the printed page. But in speech, only a few true
pauses (the aural equivalent of a blank space) may be present. So, what appears to the read-
ing eye as never give all the heart, for love would appear to the ear, if we simply use letters
to stand for their corresponding English speech sounds, as nevergivealltheheart
forlove or,
in phonemes, as n eh v er g ih v ah l dh ax h aa r t \\ f ao r l ah v. The \\ symbol marks a lin-
guistically motivated pause, and the units so formed are sometimes called intonation
phrases, as explained in Chapter 15.
Certain facts about word structure and combinatorial possibilities are evident to most
native speakers and have been confirmed by decades of linguistic research. Some of these
facts describe relations among words when considered in isolation, or concern groups of
related words that seem intuitively similar along some dimension of form or meaning - these
properties are paradigmatic. Paradigmatic properties of words include part-of-speech, in-
flectional and derivational morphology, and compound structure. Other properties of words
concern their behavior and distribution when combined for communicative purposes in fully
functioning utterances – these properties are syntagmatic.
2.3.2.1.
Lexical Part-of-Speech
Lexical part-of-speech (POS) is a primitive form of linguistic theory that posits a restricted
inventory of word-type categories, which capture generalizations of word forms and distri-
butions. Assignment of a given POS specification to a word is a way of summarizing certain
facts about its potential for syntagmatic combination. Additionally, paradigms of word for-
mation processes are often similar within POS types and subtypes as well. The word proper-
ties upon which POS category assignments are based may include affixation behavior, very
abstract semantic typologies, distributional patterns, compounding behavior, historical de-
velopment, productivity and generalizabilty, and others.
A typical set of POS categories would include noun, verb, adjective, adverb, interjec-
tion, conjunction, determiner, preposition, and pronoun. Of these, we can observe that cer-
tain classes of words consist of infinitely large membership. This means new members can
be added at any time. For example, the category of noun is constantly expanded to accom-
modate new inventions, such as Velcro or Spandex. New individuals are constantly being
born, and their names are a type of noun called proper noun. The proliferation of words us-
ing the descriptive prefix cyber is another recent set of examples: cyberscofflaw, cybersex,
and even cyberia illustrate the infinite creativity of humans in manipulating word structure
to express new shades of meaning, frequently by analogy with, and using fragments of, ex-
isting vocabulary. Another example is the neologism sheeple, a noun combining the forms
and meanings of sheep and people to refer to large masses of people who lack the capacity
or willingness to take independent action. We can create new words whenever we like, but
they had best fall within the predictable paradigmatic and syntagmatic patterns of use sum-
marized by the existing POS generalizations, or there will be little hope of their adoption by

54
Spoken Language Structure
any other speaker. These open POS categories are listed in Table 2.12. Nouns are inherently
referential. They refer to persons, places, and things. Verbs are predicative; they indicate
relations between entities and properties of entities, including participation in events. Adjec-
tives typically describe and more completely specify noun reference, while adverbs describe,
intensify and more completely specify verbal relations. Open-class words are sometimes
called content words, for their referential properties.
In contrast to the open-class categories, certain other categories of words only rarely
and very slowly admit new members over the history of English development. These closed
POS categories are shown in Table 2.13. The closed-category words are fairly stable over
time. Conjunctions are used to join larger syntactically complete phrases. Determiners help
to narrow noun reference possibilities. Prepositions denote common spatial and temporal
relations of objects and actions to one another. Pronouns provide a convenient substitute for
noun phrases that are fully understood from context. These words denote grammatical rela-
tions of other words to one another and fundamental properties of the world and how hu-
mans understand it. They can, of course, change slowly; for example, the Middle English
pronoun thee is no longer in common use. The closed-class words are sometimes called
function words.
Table 2.12 Open POS categories.
Tag
Description
Function
Example
N
Noun
Names entity
cat
V
Verb
Names event or condition
forget
Adj
Adjective
Descriptive
yellow
Adv
Adverb
Manner of action
quickly
Interj
Interjection
Reaction
oh!
Table 2.13 Closed POS categories.
Tag
Description
Function
Example
Conj
Conjunction
Coordinates phrases
and
Det
Determiner
Indicates definiteness
the
Prep
Preposition
Relations of time, space, direction
from
Pron
Pronoun
Simplified reference
she
The set of POS categories can be extended indefinitely. Examples can be drawn from
the Penn Treebank project (http://www.cis.upenn.edu/ldc) as shown in Table 2.14, where
you can find the proliferation of sub-categories, such as Verb, base form and Verb, past
tense. These categories incorporate morphological attributes of words into the POS label
system discussed in Section 2.3.2.2.
POS tagging is the process of assigning a part-of-speech or other lexical class marker
to each word in a corpus. There are many algorithms exist to automatically tag input sen-
tences into a set of tags. Rule-based methods [45], hidden Markov models (see Chapter 8)
[23, 29, 46], and machine-learning methods [6] are used for this purpose.

Syllables and Words
55
2.3.2.2.
Morphology
Morphology is about the subparts of words, i.e., the patterns of word formation including
inflection, derivation, and the formation of compounds. English mainly uses prefixes and
suffixes to express inflection and derivational morphology.
Table 2.14 Treebank POS categories – an expanded inventory.
String
Description
Example
CC
Coordinating conjunction
and
CD
Cardinal number
two
DT
Determiner
the
EX
Existential there
there (There was an old lady)
FW
Foreign word
omerta
IN
Preposition, subord. conjunction
over, but
JJ
Adjective
yellow
JJR
Adjective, comparative
better
JJS
Adjective, superlative
best
LS
List item marker
MD
Modal
might
NN
Noun, singular or mass
rock, water
NNS
Noun, plural
rocks
NNP
Proper noun, singular
Joe
NNPS
Proper noun, plural
Red Guards
PDT
Predeterminer
all (all the girls)
POS
Possessive ending
‘s
PRP
Personal pronoun
I
PRP$
Possessive pronoun
mine
RB
Adverb
quickly
RBR
Adverb, comparative
higher (shares closed higher.)
RBS
Adverb, superlative
highest (he jumped highest of all.)
RP
Particle
up ( take up the cause)
TO
to
to
UH
Interjection
hey!
VB
Verb, base form
choose
VBD
Verb, past tense
chose
VBG
Verb, gerund or present participle
choosing
VBN
Verb, past participle
chosen
VBP
Verb, non-third person sing. present
jump
VBZ
Verb, third person singular present
jumps
WDT
Wh-determiner
which
WP
Wh-pronoun
who
WP$
Possessive wh-pronoun
whose
WRB
Wh-adverb
when (When he came, it was late.)

56
Spoken Language Structure
Inflectional morphology deals with variations in word form that reflect the contextual
situation of a word in phrase or sentence syntax, and that rarely have direct effect on inter-
pretation of the fundamental meaning expressed by the word. English inflectional morphol-
ogy is relatively simple and includes person and number agreement and tense markings
only. The variation in cats (vs. cat) is an example. The plural form is used to refer to an in-
definite number of cats greater than one, depending on a particular situation. But the basic
POS category (noun) and the basic meaning (felis domesticus) are not substantially affected.
Words related to a common lemma via inflectional morphology are said to belong to a
common paradigm, with a single POS category assignment. In English, common paradigm
types include the verbal set of affixes (pieces of words): -s, -ed, -ing, the noun set: -s, and
the adjectival -er, -est. Note that sometimes the base form may change spelling under affixa-
tion, complicating the job of automatic textual analysis methods. For historical reasons, cer-
tain paradigms may consist of highly idiosyncratic irregular variation as well, e.g., go, go-
ing, went, gone or child, children. Furthermore, some words may belong to defective para-
digms, where only the singular (noun: equipment) or the plural (noun: scissors) is provided
for.
In derivational morphology, a given root word may serve as the source for wholly new
words, often with POS changes as illustrated in Table 2.15. For example, the terms racial
and racist, though presumably based on a single root word race, have different POS possi-
bilities (adjective vs. noun-adjective) and meanings. Derivational processes may induce pro-
nunciation change or stress shift (e.g., electric vs. electricity). In English, typical deriva-
tional affixes (pieces of words) that are highly productive include prefixes and suffixes: re-,
pre-, -ial, -ism, -ish, -ity, -tion, -ness, -ment, -ious, -ify, -ize, and others. In many cases, these
can be added successively to create a complex layered form.
Table 2.15 Examples of stems and their related forms across POS categories.
Noun
Verb
Adjective
Adverb
criticism
criticize
critical
critically
fool
fool
foolish
foolishly
industry, industrialization
industrialize
industrial,industrious
industriously
employ, employee, employer
employ
employable
employably
certification
certify
certifiable
certifiably
Generally, word formation operates in layers, according to a kind of word syntax: (de-
riv-prefix)* root (root)* (deriv-suffix)* (infl-suffix). This means that one or more roots can
be compounded in the inner layer, with one or more optional derivational prefixes, followed
by any number of optional derivational suffixes, capped off with no more than one inflec-
tional suffix. There are, of course, limits on word formation, deriving both from semantics of
the component words and simple lack of imagination. An example of a nearly maximal word
in English might be autocyberconceptualizations, meaning (perhaps!) multiple instances of
automatically creating computer-related concepts. This word lacks only compounding to be
truly maximal. This word has a derivational prefix auto-, two root forms compounded (cyber

Syntax and Semantics
57
and concept, though some may prefer to analyze cyber- as a prefix), three derivational suf-
fixes (-ual, ize, -ation), and is capped off with the plural inflectional suffix for nouns, -s.
2.3.2.3.
Word Classes
POS classes are based on traditional grammatical and lexical analysis. With improved com-
putational resources, it has become possible to examine words in context and assign words
to groups according to their actual behavior in real text and speech from a statistical point of
view. These kinds of classifications can be used in language modeling experiments for
speech recognition, text analysis for text-to-speech synthesis, and other purposes.
One of the main advantages of word classification is its potential to derive more re-
fined classes than traditional POS, while only rarely actually crossing traditional POS group
boundaries. Such a system may group words automatically according to the similarity of
usage with respect to their word neighbors. Consider classes automatically found by the
classification algorithms of Brown et al. [7]:
{Friday Monday Thursday Wednesday Tuesday Saturday Sunday weekends}
{great big vast sudden mere sheer gigantic lifelong scant colossal}
{down backwards ashore sideways southward northward overboard aloft adrift}
{mother wife father son husband brother daughter sister boss uncle}
{John George James Bob Robert Paul William Jim David Mike}
{feet miles pounds degrees inches barrels tons acres meters bytes}
You can see that words are grouped together based on the semantic meaning, which is
different from word classes created purely from syntactic point of view. Other types of clas-
sification are also possible, some of which can identify semantic relatedness across tradi-
tional POS categories. Some of the groups derived from this approach may include follows:
{problems problem solution solve analyzed solved solving}
{write writes writing written wrote pen}
{question questions asking answer answers answering}
{published publication author publish writer titled}
2.4.
SYNTAX AND SEMANTICS
Syntax is the study of the patterns of formation of sentences and phrases from words and the
rules for the formation of grammatical sentences. Semantics is another branch of linguistics
dealing with the study of meaning, including the ways meaning is structured in language and
changes in meaning and form over time.

58
Spoken Language Structure
2.4.1.
Syntactic Constituents
Constituents represent the way a sentence can be divided into its grammatical subparts as
constrained by common grammatical patterns (which implicitly incorporate normative
judgments on acceptability). Syntactic constituents at least respect, and at best explain, the
linear order of words in utterances and text. In this discussion, we will not strictly follow
any of the many theories of syntax but will instead bring out a few basic ideas common to
many approaches. We will not attempt anything like a complete presentation of the grammar
of English but instead focus on a few simple phenomena.
Most work in syntactic theory has adopted machinery from traditional grammatical
work on written language. Rather than analyze toy sentences, let’s consider what kinds of
superficial syntactic patterns are lurking in a random chunk of serious English text, ex-
cerpted from David Thoreau’s essay Civil Disobedience [43]:
The authority of government, even such as I am willing to submit to - for I will cheer-
fully obey those who know and can do better than I, and in many things even those who nei-
ther know nor can do so well - is still an impure one: to be strictly just, it must have the
sanction and consent of the governed. It can have no pure right over my person and prop-
erty but what I concede to it. The progress from an absolute to a limited monarchy, from a
limited monarchy to a democracy, is a progress toward a true respect for the individual.
2.4.1.1.
Phrase Schemata
Words may be combined to form phrases that have internal structure and unity. We use gen-
eralized schemata to describe the phrase structure. The goal is to create a simple, uniform
template that is independent of POS category.
Let’s first consider nouns, a fundamental category referring to persons, places, and
things in the world. The noun and its immediate modifiers form a constituent called the noun
phrase (NP). To generalize this, we consider a word of arbitrary category, say category X
(which could be a noun N or a verb V.). The generalized rule for a phrase XP is XP ⇒
(modifiers) X-head (post-modifiers), where X is the head, since it dominates the configura-
tion and names the phrase. Elements preceding the head in its phrase are premodifiers and
elements following the head are postmodifiers. XP, the culminating phrase node, is called a
maximal projection of category X. We call the whole structure an x-template. Maximal pro-
jections, XP, are the primary currency of basic syntactic processes. The post-modifiers are
usually maximal projections (another head, with its own post-modifiers forming an XP on its
own) and are sometimes termed complements, because they are often required by the lexical
properties of the head for a complete meaning to be expressed (e.g. when X is a preposition
or verb). Complements are typically noun phrases (NP), prepositional phrases (PP), verb
phrases (VP), or sentence/clause (S), which make an essential contribution to the head’s ref-
erence or meaning, and which the head requires for semantic completeness. Premodifiers are
likely to be adverbs, adjectives, quantifiers, and determiners, i.e., words that help to specify
the meaning of the head but may not be essential for completing the meaning. With minor
variations, the XP template serves for most phrasal types, based on the POS of the head (N,
V, ADJ, etc.).

Syntax and Semantics
59
For NP, we thus have NP ⇒(det) (modifier) head-noun (post-modifier). This rule
describes an NP (noun phrase - left side of arrow) in terms of its optional and required inter-
nal contents (right side of the arrow). Det is a word like the or a that helps to resolve the
reference to a specific or an unknown instance of the noun. The modifier gives further in-
formation about the noun. The head of the phrase, and the only mandatory element, is the
noun itself. Post-modifiers also give further information, usually in a more elaborate syntac-
tic form than the simpler pre-modifiers, such as a relative clause or a prepositional phrase
(covered below). The noun phrases of the passage above can be parsed as shown in Table
2.16. The head nouns may be personal pronouns (I, it), demonstrative and relative pronouns
(those), coordinated nouns (sanction and consent), or common nouns (individual). The
modifiers are mostly adjectives (impure, pure) or verbal forms functioning as adjectives
(limited). The post-modifiers are interesting, in that, unlike the (pre-)modifiers, they are
typically full phrases themselves, rather than isolated words. They include relative clauses
(which are a kind of dependent sentence, e.g., [those] who know and can do better than I),
as well as prepositional phrases (of the governed).
Table 2.16 NP’s of the sample passage.
Np
Det
Mod
Head Noun
Post-Mod
1
the
authority
of government
2
even
such
as I am willing to submit to
3
I
4
those
who know and can do better than I
5
many
things
6
even
those
who neither know nor can do so well
7
an
impure
one
8
it
9
the
sanction and consent
of the governed
10
no
pure
right
over my person … concede to it.
11
the
progress
from an absolute to a limited monarchy
12
an
absolute
[monarchy]
13
a
limited
monarchy
14
a
democracy
15
a
progress
16
a
true
respect
for the individual
17
the
individual
Prepositions express spatial and temporal relations, among others. These are also said
to project according to the X-template, but usually lack a pre-modifier. Some examples from
the sample passage are listed in Table 2.17. The complements of PP are generally NP’s,
which may be simple head nouns like government. However, other complement types, such
as the verb phrase in after discussing it with Jo, are also possible.
For verb phrases, the postmodifier (or complement) of a head verb would typically be
one or more NP (noun phrase) maximal projections, which might, for example, function as a
direct object in a VP like pet the cat. The complement may or may not be optional, depend-

60
Spoken Language Structure
ing on characteristics of the head. We can now make some language-specific generalizations
about English. Some verbs, such as give, may take more than one kind of complement. So
an appropriate template for a VP maximal projection in English would appear abstractly as
VP ⇒(modifier) verb (modifier) (Complement1, Complement2 ComplementN). Comple-
ments are usually regarded as maximal projections, such as NP, ADJP, etc., and are enumer-
ated in the template above, to cover possible multi-object verbs, such as give, which take
both direct and indirect objects. Certain types of adverbs (really, quickly, smoothly, etc.)
could be considered fillers for the VP modifier slots (before and after the head). In the sam-
ple passage, we find the following verb phrases as shown in Table 2.18.
Table 2.17 PP’s of the sample passage.
Head Prep
Complement (Postmodifier)
of
Government
as
I am willing to submit to
than
I
in
many things
of
the governed
over
my person and property
to
it
from
an absolute [monarchy]
to
a limited monarchy
to
a democracy
toward
a true respect [for the individual]
for
the individual
Table 2.18 VP’s of the sample passage.
Pre-mod
Verb Head
Post-mod
Complement
submit to
[the authority of government]
cheerfully
obey
those who know and can do better than I
is
still
an impure one
be
strictly just
have
the sanction
have
no pure right
concede
to it
is
a progress
VP presents some interesting issues. First, notice the multi-word verb submit to. Multi-
word verbs such as look after and put up with are common. We also observe a number of
auxiliary elements clustering before the verb in sentences of the sample passage: am willing
to submit to, will cheerfully obey, and can do better. Rather than considering these as simple
modifiers of the verbal head, they can be taken to have scope over the VP as a whole, which
implies they are outside the VP. Since they are outside the VP, we can assume them to be

Syntax and Semantics
61
heads in their own right, of phrases which require a VP as their complement. These elements
mainly express tense (time or duration of verbal action) and modality (likelihood or prob-
ability of verbal action). In a full sentence, the VP has explicit or implicit inflection (pro-
jected from its verbal head) and indicates the person, number and other context-dependent
features of the verb in relation to its arguments. In English, the person (first, second, third)
and number (singular, plural) attributes, collectively called agreement features, of subject
and verb must match. For simplicity, we will lump all these considerations together as in-
flectional elements, and posit yet another phrase type, the Inflectional Phrase (IP): IP ⇒
premodifier head VP-complement.
The premodifier slot (sometimes called the specifier position in linguistic theory) of an
IP is often filled by the subject of the sentence (typically a noun or NP). Since the IP unites
the subject of a sentence with a VP, IP can also be considered simply as the sentence cate-
gory, often written as S in speech grammars.
2.4.1.2.
Clauses and Sentences
The subject of a sentence is what the sentence is mainly about. A clause is any phrase with
both a subject and a VP (predicate in traditional grammars) that has potentially independent
interpretation – thus, for us, a clause is an IP, a kind of sentence. A phrase is a constituent
lacking either subject, predicate, or both. We have reviewed a number of phrase types
above. There are also various types of clauses and sentences.
Even though clauses are sentences from an internal point of view (having subject and
predicate), they often function as simpler phrases or words would, e.g., as modifiers (adjec-
tive and adverbs) or nouns and noun phrases. Clauses may appear as post-modifiers for
nouns (so-called relative clauses), basically a kind of adjective clause, sharing their subjects
with the containing sentence. Some clauses function as NP’s in their own right. One com-
mon clause type substitutes a wh-word like who or what for a direct object of a verb in the
embedded clause, to create a questioned noun phrase or indirect question: (I don’t know who
Jo saw.). In these clauses, it appears to syntacticians that the questioned object of the verb
[VP saw who] has been extracted or moved to a new surface position (following the main
clause verb know). This is sometimes shown in the phrase-structure diagram by co-indexing
an empty ghost or trace constituent at the original position of the question pronoun with the
question-NP appearing at the surface site:
I don’t know [NPobj [IP [NPi who] Jo saw [NPi _ ]]]
[NPsubj [IP Whoever wins the game]] is our hero.
There are various characteristic types of sentences. Some typical types include:
 Declarative: I gave her a book.
 Yes-no question: Did you give her a book ?
 Wh-question: What did you give her?
 Alternatives question: Did you give her a book, a scarf, or a knife?
 Tag question: You gave it to her, didn’t you?
 Passive: She was given a book.

62
Spoken Language Structure
 Cleft: It must have been a book that she got.
 Exclamative: Hasn’t this been a great birthday!
 Imperative: Give me the book.
2.4.1.3.
Parse Tree Representations
Sentences can be diagrammed in parse trees to indicate phrase-internal structure and linear
precedence and immediate dominance among phrases. A typical phrase-structure tree for
part of an embedded sentence is illustrated in Figure 2.26.
Figure 2.26 A simplified phrase-structure diagram.
For brevity, the same information illustrated in the tree can be represented as a brack-
eted string as follows:
[IP [ NP [ N It ]N]NP [I can ]I [VP [V have ]V [NP no pure right [PP over my person
]PP]NP]VP]IP
With such a bracketed representation, almost every type of syntactic constituent can be
coordinated or joined with another of its type, and usually a new phrase node of the common
type is added to subsume the constituents such as NP: We have [NP [NP tasty berries] and
[NP tart juices]], IP/S: [IP [IP Many have come] and [IP most have remained]], PP: We
went [PP [PP over the river] and [PP into the trees]], and VP: We want to [VP [VP climb
the mountains] and [VP sail the seas]].
IP (S)
NP
Inflection
VP
V
NP
have
Det
Pre-mod
N
Post-Mod (PP)
no
pure
right
over my person
It
can
N

Syntax and Semantics
63
2.4.2.
Semantic Roles
In traditional syntax, grammatical roles are used to describe the direction or control of action
relative to the verb in a sentence. Examples include the ideas of subject, object, indirect ob-
ject, etc. Semantic roles, sometimes called case relations, seem similar but dig deeper. They
are used to make sense of the participants in an event, and they provide a vocabulary for us
to answer the basic question who did what to whom. As developed by [13] and others, the
theory of semantic roles posits a limited number of universal roles. Each basic meaning of
each verb in our mental dictionary is tagged for the obligatory and optional semantic roles
used to convey the particular meaning. A typical inventory of case roles is given below:
Agent
cause or initiator of action, often intentional
Patient/Theme
undergoer of the action
Instrument
how action is accomplished
Goal
to whom action is directed
Result
result of action
Location
location of action
These can be realized under various syntactic identities, and can be assigned to both
required complement and optional adjuncts. A noun phrase in the Agentive role might be the
surface subject of a sentence, or the object of the preposition by in a passive. For example,
the verb put can be considered a process that has, in one of its senses, the case role specifica-
tions shown in Table 2.19.
Table 2.19 Analysis of a sentence with put.
Analysis
Example
Kim
put
the book
on the table.
Grammatical
functions
Subject (NP)
Predicate (VP)
Object (NP)
Adverbial
(ADVP)
Semantic roles
Agent
Instrument
Theme
Location
Now consider this passive-tense example, where the semantic roles align with differ-
ent grammatical roles shown in Table 2.20. Words that look and sound identical can have
different meaning or different senses as shown in
Table 2.21. The sporting sense of put (as in the sport of shot-put), illustrates the mean-
ing/sense-dependent nature of the role patterns, because in this sense the Locative case is no
longer obligatory, as it is in the original sense illustrated in Table 2.19 and Table 2.20.
Table 2.20 Analysis of passive sentence with put.
Analysis
Example
The book
was put
on the table
Grammatical
functions
Subject (NP)
Predicate (VP)
Adverbial (ADVP)
Semantic roles
Agent
Instrument
Location

64
Spoken Language Structure
Table 2.21 Analysis of a different pattern of put.
Analysis
Example
Kim
put
the shot.
Grammatical
functions
Subject (NP)
Predicate (VP)
Object (NP)
Semantic Roles
Agent
Instrument
Theme
The lexical meaning of a verb can be further decomposed into primitive semantic rela-
tions
such
as
CAUSE,
CHANGE,
and
BE.
The
verb
open
might
appear
as
CAUSE(NP1,PHYSICAL-CHANGE(NP2,NOT-OPEN,OPEN)). This says that for an agent
(NP1) to open a theme (NP2) is to cause the patient to change from a not-opened state to an
opened state. Such systems can be arbitrarily detailed and exhaustive, as the application re-
quires.
2.4.3.
Lexical Semantics
The specification of particular meaning templates for individual senses of particular words is
called lexical semantics. When words combine, they may take on propositional meanings
resulting from the composition of their meanings in isolation. We could imagine that a
speaker starts with a proposition in mind (logical form as will be discussed in the next sec-
tion), creating a need for particular words to express the idea (lexical semantics); the propo-
sition is then linearized (syntactic form) and spoken (phonological/phonetic form). Lexical
semantics is the level of meaning before words are composed into phrases and sentences,
and it may heavily influence the possibilities for combination.
Words can be defined in a large number of ways including by relations to other words,
in terms of decomposition semantic primitives, and in terms of non-linguistic cognitive con-
structs, such as perception, action, and emotion. There are hierarchical and non-hierarchical
relations. The main hierarchical relations would be familiar to most object-oriented pro-
grammers. One is is-a taxonomies (a crow is-a bird), which have transitivity of properties
from type to subtype (inheritance). Another is has-a relations (a car has-a windshield),
which are of several differing qualities, including process/subprocess (teaching has-a sub-
process giving exams), and arbitrary or natural subdivisions of part-whole relations (bread
has-a division into slices, meter has-a division into centimeters). Then there are non-
branching hierarchies (no fancy name) that essentially form scales of degree, such as fro-
zen ⇒cold ⇒lukewarm ⇒hot ⇒burning. Non-hierarchical relations include synonyms,
such as big/large, and antonyms such as good/bad.
Words seem to have natural affinities and disaffinities in the semantic relations among
the concepts they express. Because these affinities could potentially be exploited by future
language understanding systems, researchers have used the generalizations above in an at-
tempt to tease out a parsimonious and specific set of basic relations under which to group
entire lexicons of words. A comprehensive listing of the families and subtypes of possible
semantic relations has been presented in [10]. In Table 2.22, the leftmost column shows
names for families of proposed relations, the middle column differentiates subtypes within

Syntax and Semantics
65
each family, and the rightmost column provides examples of word pairs that participate in
the proposed relation. Note that case roles have been modified for inclusion as a type of se-
mantic relation within the lexicon.
Table 2.22 Semantic relations.
Family
Subtype
Example
Contrasts
Contrary
old-young
Contradictory
alive-dead
Reverse
buy-sell
Directional
front-back
Incompatible
happy-morbid
Asymmetric contrary
hot-cool
Attribute similar
rake-fork
Similars
Synonymity
car-auto
Dimensional similar
smile-laugh
Necessary attribute
bachelor-unmarried
Invited attribute
food-tasty
Action subordinate
talk-lecture
Class Inclusion
Perceptual subord.
animal-horse
Functional subord.
furniture-chair
State subord.
disease-polio
Activity subord.
game-chess
Geographic subord.
country-Russia
Place
Germany-Hamburg
Case Relations
Agent-action
artist-paint
Agent-instrument
farmer-tractor
Agent-object
baker-bread
Action-recipient
sit-chair
Action-instrument
cut-knife
Part-Whole
Functional object
engine-car
Collection
forest-tree
Group
choir-singer
Ingredient
table-wood
Functional location
kitchen-stove
Organization
college-admissions
Measure
mile-yard
We can see from Table 2.22 that a single word could participate in multiple relations
of different kinds. For example, knife appears in the examples for Similars: invited attribute
(i.e., a desired and expected property) as: knife-sharp, and also under Case Relations: ac-

66
Spoken Language Structure
tion-instrument, which would label the relation of knife to the action cut in He cut the bread
with a knife. This suggests that an entire lexicon could be viewed as a graph of semantic
relations, with words or idioms as nodes and connecting edges between them representing
semantic relations as listed above. There is a rich tradition of research in this vein.
The biggest practical problem of lexical semantics is the context-dependent resolution
of senses of words – so-called polysemy. A classic example is bank - bank of the stream as
opposed to money in the bank. While lexicographers try to identify distinct senses when they
write dictionary entries, it has been generally difficult to rigorously quantify exactly what
counts as a discrete sense of a word and to disambiguate the senses in practical contexts.
Therefore, designers of practical speech understanding systems generally avoid the problem
by limiting the domain of discourse. For example, in a financial application, generally only
the sense of bank as a fiduciary institution is accessible, and others are assumed not to exist.
It is sometimes difficult to make a principled argument as to how many distinct senses a
word has, because at some level of depth and abstraction, what might appears as separate
senses seem to be similar or related, as face could be face of a clock or face of person.
Senses are usually distinguished within a given part-of-speech (POS) category. Thus,
when an occurrence of bank has been identified as a verb, the shore sense might be auto-
matically eliminated, though depending on the sophistication of the system’s lexicon and
goals, there can be sense differences for many English verbs as well. Within a POS cate-
gory, often the words that occur near a given ambiguous form in the utterance or discourse
are clues to interpretation, where links can be established using semantic relations as de-
scribed above. Mutual information measures as discussed in Chapter 3 can sometimes pro-
vide hints. In a context of dialog where other, less ambiguous financial terms come up fre-
quently, the sense of bank as fiduciary institution is more likely. Finally, when all else fails,
often senses can be ranked in terms of their a priori likelihood of occurrence. It should al-
ways be borne in mind that language is not static; it can change form under a given analysis
at any time. For example, the stable English form spinster, a somewhat pejorative term for
an older, never-married female, has recently taken on a new morphologically complex form,
with the new sense of a high political official, or media spokesperson, employed to provide
bland disinformation (spin) on a given topic.
2.4.4.
Logical Form
Because of all the lexical, syntactic, and semantic ambiguity in language, some of which
requires external context for resolution, it is desirable to have a metalanguage in which to
concretely and succinctly express all linguistically possible meanings of an utterance before
discourse and world knowledge are applied to choose the most likely interpretation. The
favored metalanguage for this purpose is called the predicate logic, used to represent the
logical form, or context-independent meaning, of an utterance. The semantic component of
many SLU architectures builds on a substrate of two-valued, first-order, logic. To distin-
guish shades of meaning beyond truth and falsity requires more powerful formalisms for
knowledge representation.
In a typical first-order system, predicates correspond to events or conditions denoted
by verbs (such as Believe or Like), states of identity (such as being a Dog or Cat), and prop-

Syntax and Semantics
67
erties of varying degrees of permanence (Happy). In this form of logical notation, predicates
have open places, filled by arguments, as in a programming language subroutine definition.
Since individuals may have identical names, subscripting can be used to preserve unique
reference. In the simplest systems, predication ranges over individuals rather than higher-
order entities such as properties and relations.
Predicates with filled argument slots map onto sets of individuals (constants) in the
universe of discourse, in particular those individuals possessing the properties, or participat-
ing in the relation, named by the predicate. One-place predicates like Soldier, Happy, or
Sleeps range over sets of individuals from the universe of discourse. Two-place predicates,
like transitive verbs such as loves, range over a set consisting of ordered pairs of individual
members (constants) of the universe of discourse. For example, we can consider the universe
of discourse to be U = {Romeo, Juliet, Paris, Rosaline, Tybalt}, people as characters in a
play. They do things with and to one another, such as loving and killing. Then we could
imagine the relation Loves interpreted as the set of ordered pairs: {<Romeo, Juliet>, <Juliet,
Romeo>, <Tybalt, Tybalt>, <Paris, Juliet>}, a subset of the Cartesian product of theoreti-
cally possible love matches U
U
×
. So, for any ordered pair x, y in U, Loves(x, y) is true iff
the ordered pair <x,y> is a member of the extension of the Loves predicate as defined, e.g.,
Romeo loves Juliet, Juliet loves Romeo, etc.. Typical formal properties of relations are some-
times specially marked by grammar, such as the reflexive relation Loves(Tybalt, Tybalt),
which can rendered in natural language as Tybalt loves himself. Not every possibility is pre-
sent; for instance in our example, the individual Rosaline does not happen to participate at
all in this extensional definition of Loves over U, as her omission from the pairs list indi-
cates. Notice that the subset of Loves(x, y) of ordered pairs involving both Romeo and Juliet
is symmetric, also marked by grammar, as in Romeo and Juliet love each other. This general
approach extends to predicates with any arbitrary number of arguments, such as intransitive
verbs like give.
Just as in ordinary propositional logic, connectives such as negation, conjunction, dis-
junction, and entailment are admitted, and can be used with predicates to denote common
natural language meanings:
Romeo isn’t happy = ^Happy(Romeo)
Romeo isn’t happy, but Tybalt is (happy) = ^Happy(Romeo) && Happy(Tybalt)
Either Romeo or Tybalt is happy = Happy(Romeo) || Happy(Tybalt)
If Romeo is happy, Juliet is happy = Happy(Romeo) ! Happy(Juliet)
Formulae, such as those above, are also said to bear a binary truth value, true or false,
with respect to a world of individuals and relations. The determination of the truth value is
compositional, in the sense that the truth value of the whole depends on the truth value of
the parts. This is a simplistic but formally tractable view of the relation between language
and meaning.
Predicate logic can also be used to denote quantified noun phrases. Consider a simple
case such as Someone killed Tybalt, predicated over our same U = {Romeo, Juliet, Paris,
Rosaline, Tybalt}. We can now add an existential quantifier, , standing for there exists or
there is at least one. This quantifier will bind a variable over individuals in U, and will at-
tach to a proposition to create a new, quantified proposition in logical form. The use of vari-

68
Spoken Language Structure
ables in propositions such as killed(x, y) creates open propositions. Binding the variables
with a quantifier over them closes the proposition. The quantifier is prefixed to the original
proposition:
x Killed(x, Tybalt)
To establish a truth (semantic) value for the quantified proposition, we have to satisfy
the disjunction of propositions in U: Killed(Romeo, Tybalt) V Killed(Juliet, Tybalt) V
Killed(Paris, Tybalt) V Killed(Rosaline, Tybalt) V Killed(Tybalt, Tybalt). The set of all such
bindings of the variable x is the space that determines the truth or falsity of the proposition.
In this case, the binding of x = Romeo is sufficient to assign a value true to the existential
proposition.
2.5.
HISTORICAL PERSPECTIVE AND FURTHER READING
Motivated to improve speech quality over the telephone, AT&T Bell Labs has contributed
many influential discoveries in speech hearing, including the critical band and articulation
index [2, 3]. The Auditory Demonstration CD prepared by Houtsma, Rossing, and
Wagenaars [18] has a number of very interesting examples on psychoacoustics and its ex-
planations. Speech, Language, and Communication [30] and Speech Communication - Hu-
man and Machines [32] are two good books that provide modern introductions to the struc-
ture of spoken language. Many speech perception experiments were conducted by exploring
how phonetic information is distributed in the time or frequency domain. In addition to the
formant structures for vowels, frequency importance function [12] has been developed to
study how features related to phonetic categories are stored at various frequencies. In the
time domain, it has been observed [16, 19, 42] that salient perceptual cues may not be
evenly distributed over the speech segments and that certain perceptual critical points exist.
As intimate as speech and acoustic perception may be, there are also strong evidences
that lexical and linguistic effects on speech perception are not always consistent with acous-
tic ones. For instance, it has long been observed that humans exhibit difficulties in distin-
guishing non-native phonemes. Human subjects also carry out categorical goodness differ-
ence assimilation based on their mother tongue [34], and such perceptual mechanism can be
observed as early as in six-month-old infants [22]. On the other hand, hearing-impaired lis-
teners are able to effortlessly overcome their acoustical disabilities for speech perception [8].
Speech perception is not simply an auditory matter. McGurk and MacDonald (1976) [27,
28] dramatically demonstrated this when they created a video tape on which the auditory
information (phonemes) did not match the visual speech information. The effect of this
mismatch between the auditory signal and the visual signal was to create a third phoneme
different from both the original auditory and visual speech signals. An example is dubbing
the phoneme /ba/ to the visual speech movements /ga/. This mismatch results in hearing the
phoneme /da/. Even when subjects know of the effect, they report the McGurk effect per-
cept. The McGurk effect has been demonstrated for consonants, vowels, words, and sen-
tences.
The earliest scientific work on phonology and grammars goes back to Panini, a San-
skrit grammarian of the fifth century B.C. (estimated), who created a comprehensive and

Historical Perspective and Further Reading
69
scientific theory of phonetics, phonology, and morphology, based on data from Sanskrit (the
classical literary language of the ancient Hindus). Panini created formal production rules and
definitions to describe Sanskrit grammar, including phenomena such as construction of sen-
tences, compound nouns, etc. Panini’s formalisms function as ordered rules operating on
underlying structures in a manner analogous to modern linguistic theory. Panini's phono-
logical rules are equivalent in formal power to Backus-Nauer form (BNF). A general intro-
duction to this pioneering scientist is Cardona [9].
An excellent introduction to all aspects of phonetics is A Course in Phonetics [24]. A
good treatment of the acoustic structure of English speech sounds and a through introduction
and comparison of theories of speech perception is to be found in [33]. The basics of pho-
nology as part of linguistic theory are treated in Understanding Phonology [17]. An interest-
ing treatment of word structure (morphology) from a computational point of view can be
found in Morphology and Computation [40]. A comprehensive yet readable treatment of
English syntax and grammar can be found in English Syntax [4] and A Comprehensive
Grammar of the English Language [36]. Syntactic theory has traditionally been the heart of
linguistics, and has been an exciting and controversial area of research since the 1950s. Be
aware that almost any work in this area will adopt and promote a particular viewpoint, often
to the exclusion or minimization of others. A reasonable place to begin with syntactic theory
is Syntax: A Minimalist Introduction [37]. An introductory textbook on syntactic and seman-
tic theory that smoothly introduces computational issues is Syntactic Theory: A Formal In-
troduction [39]. For a philosophical and entertaining overview of various aspects of linguis-
tic theory, see Rhyme and Reason: An Introduction to Minimalist Syntax [44]. A good and
fairly concise treatment of basic semantics is Introduction to Natural Language Semantics
[11]. Deeper issues are covered in greater detail and at a more advanced level in The Hand-
book of Contemporary Semantic Theory [25]). The intriguing area of lexical semantics (the-
ory of word meanings) is comprehensively presented in The Generative Lexicon [35]. Con-
cise History of the Language Sciences [21] is a good edited book if you are interested in the
history of linguistics.
REFERENCES
[1]
Aliprand, J., et al., The Unicode Standard, Version 2.0, 1996, Addison Wesley.
[2]
Allen, J.B., "How Do Humans Process and Recognize Speech?," IEEE Trans. on
Speech and Audio Processing, 1994, 2(4), pp. 567-577.
[3]
Allen, J.B., "Harvey Fletcher 1884--1981" in The ASA Edition of Speech and Hear-
ing Communication 1995, Woodbury, New York, pp. A1-A34, Acoustical Society
of America.
[4]
Baker, C.L., English Syntax, 1995, Cambridge, MA, MIT Press.
[5]
Blauert, J., Spatial Hearing, 1983, MIT Press.
[6]
Brill, E., "Transformation-Based Error-Driven Learning and Natural Language
Processing: A Case Study in Part-of-Speech Tagging," Computational Linguistics,
1995, 21(4), pp. 543-566.
[7]
Brown, P., et al., "Class-Based N-gram Models of Natural Language," Computa-
tional Linguistics, 1992, 18(4).

70
Spoken Language Structure
[8]
Caplan, D. and J. Utman, "Selective Acoustic Phonetic Impairment and Lexical
Access in an Aphasic Patient," Journal of the Acoustical Society of America, 1994,
95(1), pp. 512-517.
[9]
Cardona, G., Panini: His Work and Its Traditions: Background and Introduction,
1988, Motilal Banarsidass.
[10]
Chaffin, R., Herrmann, D., "The Nature of Semantic Relations: A Comparison of
Two Approaches" in Representing knowledge in Semantic Networks, M. Evens,
Editor 1988, Cambridge, UK, Cambridge University Press.
[11]
de Swart, H., Introduction to Natural Language Semantics, 1998, Stanford, Cali-
fornia, USA, Center for the Study of Language and Information Publications.
[12]
Duggirala, V., et al., "Frequency Importance Function for a Feature Recognition
Test Material," Journal of the Acoustical Society of America, 1988, 83(9), pp.
2372-2382.
[13]
Fillmore, C.J., "The Case for Case" in Universals in Linguistic Theory, E. Bach and
R. Harms, eds. 1968, New York, NY, Holt, Rinehart and Winston.
[14]
Fletcher, H., "Auditory patterns," Rev. Mod. Phys., 1940, 12, pp. 47-65.
[15]
Fry, D.B., The Physics of Speech, Cambridge Textbooks in Linguistics, 1979,
Cambridge, U.K., Cambridge University Press.
[16]
Furui, S., "On The Role of Spectral Transition for Speech Perception," Journal of
the Acoustical Society of America, 1986, 80(4), pp. 1016-1025.
[17]
Gussenhoven, C., Jacobs, H., Understanding Phonology, Understanding Language
Series, 1998, Edward Arnold.
[18]
Houtsma, A., T. Rossing, and W. Wagenaars, Auditory Demonstrations, 1987, In-
stitute for Perception Research, Eindhovern, The Netherlands, Acoustic Society of
America.
[19]
Jenkins, J., W. Strange, and S. Miranda, "Vowel Identification in Mixed-Speaker
Silent-Center Syllables," Journal of the Acoustical Society of America, 1994, 95(2),
pp. 1030-1041.
[20]
Klatt, D., "Review of the ARPA Speech Understanding Project," Journal of Acous-
tical Society of America, 1977, 62(6), pp. 1324-1366.
[21]
Koerner, E. and E. Asher, eds. Concise History of the Language Sciences, , 1995,
Oxford, Elsevier Science.
[22]
Kuhl, P., "Infant's Perception and Representation of Speech: Development of a
New Theory," Int. Conf. on Spoken Language Processing, 1992, Alberta, Canada
pp. 449-452.
[23]
Kupeic, J., "Robust Part-of-Speech Tagging Using a Hidden Markov Model,"
Computer Speech and Language, 1992, 6, pp. 225-242.
[24]
Ladefoged, P., A Course in Phonetics, 1993, Harcourt Brace Johanovich.
[25]
Lappin, S., The Handbook of Contemporary Semantic Theory, Blackwell Hand-
books in Linguistics, 1997, Oxford, UK, Blackwell Publishsers Inc.
[26]
Lindsey, P. and D. Norman, Human Information Processing, 1972, New York and
London, Academic Press.
[27]
MacDonald, J. and H. McGurk, "Visual Influence on Speech Perception Process,"
Perception and Psychophysics, 1978, 24(3), pp. 253-257.

Historical Perspective and Further Reading
71
[28]
McGurk, H. and J. MacDonald, "Hearing Lips and Seeing Voices," Nature, 1976,
264, pp. 746-748.
[29]
Merialdo, B., "Tagging English Text with a Probabilistic Model," Computational
Linguistics, 1994, 20(2), pp. 155-172.
[30]
Miller, J. and P. Eimas, Speech, Language and Communication, Handbook of Per-
ception and Cognition, eds. E. Carterette and M. Friedman, 1995, Academic Press.
[31]
Moore, B.C., An Introduction to the Psychology of Hearing, 1982, London, Aca-
demic Press.
[32]
O'Shaughnessy, D., Speech Communication -- Human and Machine, 1987, Addi-
son-Wesley.
[33]
Pickett, J.M., The Acoustics of Speech Communication, 1999, Needham Heights,
MA, Allyn & Bacon.
[34]
Polka, L., "Linguistic Influences in Adult Perception of Non-native Vowel Con-
trast," Journal of the Acoustical Society of America, 1995, 97(2), pp. 1286-1296.
[35]
Pustejovsky, J., The Generative Lexicon, 1998, Bradford Books.
[36]
Quirk, R., Svartvik, J., Leech, G., A Comprehensive Grammar of the English Lan-
guage, 1985, Addison-Wesley Pub Co.
[37]
Radford, A., Syntax: A Minimalist Introduction, 1997, Cambridge, U.K., Cam-
bridge Univ. Press.
[38]
Rossing, T.D., The Science of Sound, 1982, Reading, MA, Addison-Wesley.
[39]
Sag, I., Wasow, T., Syntactic Theory: A Formal Introduction, 1999, Cambridge,
UK, Cambridge University Press.
[40]
Sproat, R., Morphology and Computation, ACL-MIT Press Series in Natural Lan-
guage Processing, 1992, Cambridge, MA, MIT Press.
[41]
Stevens, S.S. and J. Volkman, "The Relation of Pitch to Frequency," Journal of
Psychology, 1940, 53, pp. 329.
[42]
Strange, W., J. Jenkins, and T. Johnson, "Dynamic Specification of Coarticulated
Vowels," Journal of the Acoustical Society of America, 1983, 74(3), pp. 695-705.
[43]
Thoreau, H.D., Civil Disobedience, Solitude and Life Without Principle, 1998,
Prometheus Books.
[44]
Uriagereka, J., Rhyme and Reason: An Introduction to Minimalist Syntax, 1998,
Cambridge, MA, MIT Press.
[45]
Voutilainen, A., "Morphological Disambiguation" in Constraint Grammar: A Lan-
guage-Independent System for Parsing Unrestricted Text 1995, Berlin, Mouton de
Gruyter.
[46]
Weischedel, R., "BBN: Description of the PLUM System as Used for MUC-6," The
6th Message Understanding Conferences (MUC-6), 1995, San Francisco, Morgan
Kaufmann pp. 55-70.

73
C H A P T E R
3
Probability, Statistics, and Information Theory
Randomness and uncertainty play an impor-
tant role in science and engineering. Most spoken language processing problems can be
characterized in a probabilistic framework. Probability theory and statistics provide the
mathematical language to describe and analyze such systems.
The criteria and methods used to estimate the unknown probabilities and probability
densities form the basis for estimation theory. Estimation theory forms the basics for pa-
rameter learning in pattern recognition. In this chapter, three widely used estimation meth-
ods are discussed. They are minimum mean squared error estimation (MMSE), maximum
likelihood estimation (MLE), and maximum posterior probability estimation (MAP).
Significance testing is also important in statistics, which deals with the confidence of
statistical inference, such as knowing whether the estimation of some parameter can be ac-
cepted with confidence. In pattern recognition, significance testing is extremely important
for determining whether the observed difference between two different classifiers is real. In
our coverage of significance testing, we describe various methods that are used in pattern
recognition discussed in. Chapter 4.

74
Probability, Statistics, and Information Theory
Information theory was originally developed for efficient and reliable communication
systems. It has evolved into a mathematical theory concerned with the very essence of the
communication process. It provides a framework for the study of fundamental issues, such
as the efficiency of information representation and the limitations in reliable transmission of
information over a communication channel. Many of these problems are fundamental to
spoken language processing.
3.1.
PROBABILITY THEORY
Probability theory deals with the averages of mass phenomena occurring sequentially or
simultaneously. We often use probabilistic expressions in our day-to-day lives, such as when
saying, It is very likely that the Dow (Dow Jones Industrial index) will hit 12,000 points next
month, or, The chance of scattered showers in Seattle this weekend is high. Each of these
expressions is based upon the concept of the probability, or the likelihood, which some spe-
cific event will occur.
Probability can be used to represent the degree of confidence in the outcome of some
actions (observations), which are not definite. In probability theory, the term sample space,
S, is used to refer to the collection (set) of all possible outcomes. An event refers to a subset
of the sample space or a collection of outcomes. The probability of event A denoted as
( )
P A , can be interpreted as the relative frequency with which the event A would occur if the
process were repeated a large number of times under similar conditions. Based on this inter-
pretation,
( )
P A can be computed simply by counting the total number,
S
N , of all observa-
tions and the number of observations
A
N
whose outcome belongs to the event A. That is,
( )
A
S
N
P A
N
=
(3.1)
( )
P A is bounded between zero and one, i.e.,
0
( )
1 for all
P A
A
≤
≤
(3.2)
The lower bound of probability
( )
P A
is zero when the event set A is an empty set. On the
other hand, the upper bound of probability
( )
P A is one when the event set A happens to be
S.
If there are n events
1
2
,
,
n
A A
A

in S such that
1
2
,
,
n
A A
A

are disjoint and
1
n
i
i
A
S
=
=

,
events
1
2
,
,
n
A A
A

are said to form a partition of S. The following obvious equation forms a
fundamental axiom for probability theory.
1
2
1
(
)
(
)
1
n
n
i
i
P A
A
A
P A
=
∪
∪
=
=


(3.3)

Probability Theory
75
Based on the definition in Eq. (3.1), the joint probability of event A and event B occurring
concurrently is denoted as
(
)
P AB
and can be calculated as:
(
)
AB
S
N
P AB
N
=
(3.4)
3.1.1.
Conditional Probability And Bayes' Rule
It is useful to study the way in which the probability of an event A changes after it has been
learned that some other event B has occurred. This new probability denoted as
(
|
)
P A B
is
called the conditional probability of event A given that event B has occurred. Since the set of
those outcomes in B that also result in the occurrence of A is exactly the set AB as illustrated
in Figure 3.1, it is natural to define the conditional probability as the proportion of the total
probability
( )
P B
that is represented by the joint probability
(
)
P AB . This leads to the fol-
lowing definition:
(
)
(
|
)
( )
AB
S
B
S
N
N
P AB
P A B
P B
N
N
=
=
(3.5)
B
A
AB
S
Figure 3.1 The intersection AB represents where the joint event A and B occurs concurrently.
Based on the definition of conditional probability, the following expressions can be
easily derived.
(
)
(
|
) ( )
(
|
) ( )
P AB
P A B P B
P B A P A
=
=
(3.6)
Equation (3.6) is the simple version of the chain rule. The chain rule, which can specify a
joint probability in terms of multiplication of several cascaded conditional probabilities, is
often used to decompose a complicated joint probabilistic problem into a sequence of step-
wise conditional probabilistic problems. Eq. (3.6) can be converted to such a general chain:
1
2
1
1
2
1
1
(
)
(
|
)
(
|
) (
)
n
n
n
P A A
A
P A
A
A
P A
A P A
−
=



(3.7)

76
Probability, Statistics, and Information Theory
When two events, A and B, are independent of each other, in the sense that the occur-
rence or of either of them has no relation to and no influence on the occurrence of the other,
it is obvious that the conditional probability
(
|
)
P B A
equals to the unconditional probability
( )
P B . It follows that the joint probability
(
)
P AB
is simply the product of
( )
P A and
( )
P B
if A and B, are independent.
If the n events
1
2
,
,
n
A A
A

form a partition of S and B is any event in S as illustrated in
Figure 3.2, the events
1
2
,
,
n
A B A B
A B

form a partition of B. Thus, we can rewrite:
1
2
n
B
A B
A B
A B
=
∪
∪
∪

(3.8)
Since
1
2
,
,
n
A B A B
A B

are disjoint,
1
( )
(
)
n
k
k
P B
P A B
=
= 
(3.9)
A1
A2
A3
A4
A5
B
Figure 3.2 The intersections of B with partition events
1
2
,
,
n
A A
A

.
Equation (3.9) is called the marginal probability of event B, where the probability of
event B is computed from the sum of joint probabilities.
According to the chain rule, Eq. (3.6),
(
)
(
) (
|
)
i
i
i
P A B
P A P B A
=
, it follows that
1
( )
(
) (
|
)
n
k
k
k
P B
P A P B A
=
= 
(3.10)
Combining Eqs. (3.5) and (3.10), we get the well-known Bayes' rule:
1
(
)
(
|
) (
)
(
|
)
( )
(
|
) (
)
i
i
i
i
n
k
k
k
P A B
P B A P A
P A
B
P B
P B A P A
=
=
=

(3.11)
Bayes' rule is the basis for pattern recognition that is described in Chapter 4.

Probability Theory
77
3.1.2.
Random Variables
Elements in a sample space may be numbered and referred to by the numbers given. A vari-
able X that specifies the numerical quantity in a sample space is called a random variable.
Therefore, a random variable X is a function that maps each possible outcome s in the sam-
ple space S onto real numbers
( )
X s . Since each event is a subset of the sample space, an
event is represented as a set of { }
s
which satisfies {
}
|
( )
s
X s
x
=
. We use capital letters to
denote random variables and lower-case letters to denote fixed values of the random vari-
able. Thus, the probability that X
x
=
is denoted as:
(
)
(
|
( )
)
P X
x
P s
X s
x
=
=
=
(3.12)
A random variable X is a discrete random variable, or X has a discrete distribution, if
X can take only a finite number n of different values
1
2
,
,
,
n
x x
x

, or at most, an infinite se-
quence of different values
1
2
,
,
x x . If the random variable X is a discrete random variable,
the probability function (p.f.) or probability mass function (p.m.f.) of X is defined to be the
function p such that for any real number x,
( )
(
)
X
p
x
P X
x
=
=
(3.13)
For the cases in which there is no confusion, we drop the subscription X for
( )
X
p
x . The
sum of probability mass over all values of the random variable is equal to unity.
1
1
(
)
(
)
1
n
n
i
i
k
k
p x
P X
x
=
=
=
=
=


(3.14)
The marginal probability, chain rule and Bayes' rule can also be rewritten with respect
to random variables.
1
1
(
)
(
)
(
,
)
(
|
) (
)
m
m
X
i
i
i
k
i
k
k
k
k
p
x
P X
x
P X
x Y
y
P X
x
Y
y
P Y
y
=
=
=
=
=
=
=
=
=
=
=


(3.15)
1
1
1
1
1
1
2
2
1
1
1
1
(
,
,
)
(
|
,
,
)
(
|
) (
)
n
n
n
n
n
n
P X
x
X
x
P X
x
X
x
X
x
P X
x
X
x P X
x
−
−
=
=
=
=
=
=
=
=
=



(3.16)
1
(
,
)
(
|
) (
)
(
|
)
(
)
(
|
) (
)
i
i
i
i
n
k
k
k
P X
x Y
y
P Y
y X
x P X
x
P X
x
Y
y
P Y
y
P Y
y X
x
P X
x
=
=
=
=
=
=
=
=
=
=
=
=
=
=

(3.17)
In a similar manner, if the random variables X and Y are statistically independent, they
can be represented as:
(
,
)
(
) (
)=
( )
(
)
all
and
i
j
i
j
X
i
Y
j
P X
x Y
y
P X
x P Y
y
p
x p
y
i
j
=
=
=
=
=
∀
(3.18)

78
Probability, Statistics, and Information Theory
A random variable X is a continuous random variable, or X has a continuous distribu-
tion, if there exists a nonnegative function f, defined on the real line, such that for an interval
A,
(
)
( )
X
A
P X
A
f
x dx
∈
= 
(3.19)
The function
Xf
is called the probability density function (abbreviated p.d.f.) of X. We drop
the subscript X for
Xf
if there is no ambiguity. As illustrated in Figure 3.3, the area of
shaded region is equal to the value of
(
)
P a
X
b
≤
≤
Figure 3.3 An example of p.d.f. The area of the shaded region is equal to the value of
(
)
P a
X
b
≤
≤
.
Every p.d.f must satisfy the following two requirements.
( )
0 for -
and
( )
1
f x
x
f x dx
∞
−∞
≥
∞≤
≤∞
=

(3.20)
The marginal probability, chain rule, and Bayes' rule can also be rewritten with respect
to continuous random variables:
,
|
( )
( , )
( | )
( )
X
X Y
X Y
Y
f
x
f
x y dy
f
x y f
y dy
∞
∞
−∞
−∞
=
=


(3.21)
1
1
1
2
1
1
,
,
1
|
,
,
1
1
|
2
1
1
(
,
,
)
(
|
,
,
)
(
|
)
(
)
n
n
n
X
X
n
X
X
X
n
n
X
X
X
f
x
x
f
x
x
x
f
x
x
f
x
−
−
=





(3.22)
|
,
|
|
( | )
( )
( , )
( | )
( )
( | )
( )
Y X
X
X Y
X Y
Y
Y X
X
f
y x f
x
f
x y
f
x y
f
y
f
y x f
x dx
∞
−∞
=
=

(3.23)
x
a
b
f (x)

Probability Theory
79
The distribution function or cumulative distribution function F of a discrete or con-
tinuous random variable X is a function defined for all real number x as follows:
( )
(
)
for
F x
P X
x
x
=
≤
−∞≤
≤∞
(3.24)
For continuous random variables, It follows that:
( )
( )
x
X
F x
f
x dx
−∞
= 
(3.25)
( )
( )
X
dF x
f
x
dx
=
(3.26)
3.1.3.
Mean and Variance
Suppose that a discrete random variable X has a p.f. f(x); the expectation or mean of X is
defined as follows:
(
)
( )
x
E X
xf x
= 
(3.27)
Similarly, if a continuous random variable X has a p.d.f. f, the expectation or mean of
X is defined as follows:
(
)
( )
E X
xf x dx
∞
−∞
= 
(3.28)
In physics, the mean is regarded as the center of mass of the probability distribution.
The expectation can also be defined for any function of the random variable X. If X is a con-
tinuous random variable with p.d.f. f, then the expectation of any function
(
)
g X
can be
defined as follows:
[
]
(
)
( ) ( )
E g X
g x f x dx
∞
−∞
= 
(3.29)
The expectation of a random variable is a linear operator. That is, it satisfies both addi-
tivity and homogeneity properties:
1
1
1
1
(
)
(
)
(
)
n
n
n
n
E a X
a X
b
a E X
a E X
b
+
+
+
=
+
+
+


(3.30)
where
1,
,
,
n
a
a b

are constants
Equation (3.30) is valid regardless of whether or not the random variables
1,
,
n
X
X

are independent.
Suppose that X is a random variable with mean
(
)
E X
µ =
. The variance of X denoted
as
(
)
Var X
is defined as follows:

80
Probability, Statistics, and Information Theory
2
2
(
)
(
)
Var X
E
X
σ
µ


=
=
−


(3.31)
where σ , the nonnegative square root of the variance is known as the standard deviation of
random variable X. Therefore, the variance is also often denoted as
2
σ .
The variance of a distribution provides a measure of the spread or dispersion of the
distribution around its mean µ . A small value of the variance indicates that the probability
distribution is tightly concentrated around µ , and a large value of the variance typically
indicates the probability distribution has a wide spread around µ . Figure 3.4 illustrates
three different Gaussian distributions1 with the same mean, but different variances.
The variance of random variable X can be computed in the following way:
[
]
2
2
(
)
(
)
(
)
Var X
E X
E X
=
−
(3.32)
In physics, the expectation
(
)
k
E X
is called the kth moment of X for any random vari-
able X and any positive integer k. Therefore, the variance is simply the difference between
the second moment and the square of the first moment.
The variance satisfies the following additivity property, if random variables X and Y
are independent:
(
)
(
)
( )
Var X
Y
Var X
Var Y
+
=
+
(3.33)
However, it does not satisfy the homogeneity property. Instead for constant a,
2
(
)
(
)
Var aX
a Var X
=
(3.34)
Since it is clear that
( )
0
Var b =
for any constant b, we have an equation similar to Eq.
(3.30) if random variables
1,
,
n
X
X

are independent.
2
2
1
1
1
1
(
)
(
)
(
)
n
n
n
n
Var a X
a X
b
a Var X
a Var X
+
+
+
=
+
+


(3.35)
Conditional expectation can also be defined in a similar way. Suppose that X and Y are
discrete random variables and let
( | )
f y x
denote the conditional p.f. of Y given X
x
=
,
then the conditional expectation
(
|
)
E Y X
is defined to be the function of X whose value
(
| )
E Y x
when X
x
=
is
|
|
(
|
)
( | )
Y X
Y X
y
E
Y X
x
yf
y x
=
= 
(3.36)
For continuous random variables X and Y with
| ( | )
Y X
f
y x
as the conditional p.d.f. of Y
given X
x
=
, the conditional expectation
(
|
)
E Y X
is defined to be the function of X whose
value
(
| )
E Y x
when X
x
=
is
1 We describe Gaussian distributions in Section 3.1.7

Probability Theory
81
|
|
(
|
)
( | )
Y X
Y X
E
Y
X
x
yf
y x dy
∞
−∞
=
= 
(3.37)
-10
-8
-6
-4
-2
0
2
4
6
8
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
x
sigma =.5
sigma = 1
sigma = 2
Figure 3.4 Three Gaussian distributions with same mean µ , but different variances, 0.5, 1.0
,and 2.0, respectively. The distribution with a large value of the variance has a wide spread
around the mean µ .
Since
(
|
)
E Y X
is a function of random variable X, it itself is a random variable whose
probability distribution can be derived from the distribution of X. It can be shown that
|
,
(
|
)
( )
X
Y X
X Y
E
E
Y
X
E
Y

 =


(3.38)
More generally, suppose that X and Y have a continuous joint distribution and that
( , )
g x y
is any arbitrary function of X and Y. The conditional expectation
[
]
(
, ) |
E g X Y
X
is
defined to be the function of X whose value
[
]
(
, ) |
E g X Y
x when X
x
=
is
[
]
|
|
(
, ) |
( , )
( | )
Y X
Y X
E
g X Y
X
x
g x y f
y x dy
∞
−∞
=
= 
(3.39)
Equation (3.38) can also be generalized into the following equation:
[
]
{
}
[
]
|
,
(
, ) |
(
, )
X
Y X
X Y
E
E
g X Y
X
E
g X Y
=
(3.40)
Finally, it is worthwhile to introduce median and mode. A median of the distribution
of X is defined to be a point m, such that
(
)
1 2
P X
m
≤
≥
and
(
)
1 2
P X
m
≥
≥
. Thus, the
median m divides the total probability into two equal parts, i.e., the probability to the left of
m and the probability to the right of m are exactly 1 2 .

82
Probability, Statistics, and Information Theory
Suppose a random variable X has either a discrete distribution with p.f.
( )
p x
or con-
tinuous p.d.f.
( )
f x ; a point ϖ is called the mode of the distribution if
( )
p x
or
( )
f x
at-
tains the maximum value at the point ϖ . A distribution can have more than one modes.
3.1.3.1.
The Law of Large Numbers
The concept of sample mean and sample variance is important in statistics because most
statistical experiments involve sampling. Suppose that the random variables
1,
,
n
X
X

form
a random sample of size n from some distribution for which the mean is µ and the variance
is
2
σ . In other words, the random variables
1,
,
n
X
X

are independent identically distrib-
uted (often abbreviated by i.i.d.) and each has mean µ and variance
2
σ . Now if we denote
n
X
as the arithmetic average of the n observations in the sample, then
1
1 (
)
n
n
X
X
X
n
=
+
+

(3.41)
n
X
is a random variable and is referred to as sample mean. The mean and variance of
n
X
can be easily derived based on the definition.
2
(
)
and Var(
)
n
n
E X
X
n
σ
µ
=
=
(3.42)
Equation (3.42) states that the mean of sample mean is equal to mean of the distribution,
while the variance of sample mean is only 1 n times the variance of the distribution. In
other words, the distribution of
n
X
will be more concentrated around the mean µ than was
the original distribution. Thus, the sample mean is closer to µ than is the value of just a
single observation
i
X from the given distribution.
The law of large numbers is one of most important theorems in probability theory.
Formally, it states that the sample mean
n
X converges to the mean µ in probability, that is,
(
)
lim
|
|
1 for any given number
0
n
n
P
X
µ
ε
ε
→∞
−
<
=
>
(3.43)
The law of large numbers basically implies that the sample mean is an excellent estimate of
the unknown mean of the distribution when the sample size n is large.

Probability Theory
83
3.1.4.
Covariance and Correlation
Let X and Y be random variables having a specific joint distribution, and
(
)
X
E X
µ
=
,
( )
Y
E Y
µ
=
,
2
(
)
X
Var X
σ
=
, and
2
( )
Y
Var Y
σ
=
. The covariance of X and Y, denoted as
(
, )
Cov X Y , is defined as follows:
[
]
(
, )
(
)(
)
( ,
)
X
Y
Cov X Y
E
X
Y
Cov Y X
µ
µ
=
−
−
=
(3.44)
In addition, the correlation coefficient of X and Y, denoted as
XY
ρ
, is defined as fol-
lows:
(
, )
XY
X
Y
Cov X Y
ρ
σ σ
=
(3.45)
It can be shown that
(
, )
X Y
ρ
should be bound within [
]
1
1
−
, that is,
1
(
, )
1
X Y
ρ
−≤
≤
(3.46)
X and Y are said to be positively correlated if
0
XY
ρ
>
, negatively correlated if
0
XY
ρ
<
,
and uncorrelated if
0
XY
ρ
=
. It can also be shown that
(
, )
Cov X Y
and
XY
ρ
must have the
same sign; that is, both are positive, negative, or zero at the same time. When
(
)
0
E XY =
,
the two random variables are called orthogonal.
There are several theorems pertaining to the basic properties of covariance and corre-
lation. We list here the most important ones:
Theorem 1 For any random variables X and Y
(
, )
(
)
(
) ( )
Cov X Y
E XY
E X E Y
=
−
(3.47)
Theorem 2 If X and Y are independent random variables, then
(
, )
0
XY
Cov X Y
ρ
=
=
Theorem 3 Suppose X is a random variable and Y is a linear function of X in the
form of Y
aX
b
=
+
for some constant a and b, where
0
a ≠
. If
0
a >
, then
1
XY
ρ
= . If
0
a <
, then
1
XY
ρ
= −. Sometimes,
XY
ρ
is referred to as the amount
of linear dependency between random variables X and Y.
Theorem 4 For any random variables X and Y,
(
)
(
)
( )
2
(
, )
Var X
Y
Var X
Var Y
Cov X Y
+
=
+
+
(3.48)
Theorem 5 If
1,
,
n
X
X

are random variables, then

84
Probability, Statistics, and Information Theory
1
1
1
1
1
(
)
(
)
2
(
,
)
n
n
n
i
i
i
i
j
i
i
i
j
Var
X
Var X
Cov X
X
−
=
=
=
=
=
+



(3.49)
3.1.5.
Random Vectors and Multivariate Distributions
When a random variable is a vector rather than a scalar, it is called a random vector and we
often use boldface variable like
1
(
,
,
)
n
X
X
=
X

to indicate that it is a random vector. It is
said that n random variables
1,
,
n
X
X

have a discrete joint distribution if the random vec-
tor
1
(
,
,
)
n
X
X
=
X

can have only a finite number or an infinite sequence of different val-
ues
1
(
,
,
)
n
x
x

in
n
R . The joint p.f. of
1,
,
n
X
X

is defined to be the function fX such that
for any point
1
(
,
,
)
n
n
x
x
R
∈

,
1
1
1
(
,
,
)
(
,
,
)
n
n
n
f
x
x
P X
x
X
x
=
=
=
X


(3.50)
Similarly, it is said that n random variables
1,
,
n
X
X

have a continuous joint distri-
bution if there is a nonnegative function f defined on
n
R
such that for any subset
n
A
R
⊂
,
[
]
1
1
1
(
,
,
)
(
,
,
)
n
n
n
A
P
X
X
A
f
x
x dx
dx
∈
= 

X




(3.51)
The joint distribution function can also be defined similarly for n random variables
1,
,
n
X
X

as follows:
1
1
1
(
,
,
)
(
,
,
)
n
n
n
F
x
x
P X
x
X
x
=
≤
≤
X


(3.52)
The concept of mean and variance for a random vector can be generalized into mean
vector and covariance matrix. Supposed that X is an n-dimensional random vector with
components
1,
,
n
X
X

, under matrix representation, we have
1
n
X
X




= 





X

(3.53)
The expectation (mean) vector
( )
E X
of random vector X is an n-dimensional vector whose
components are the expectations of the individual components of X, that is,
1
(
)
( )
(
)
n
E X
E
E X




= 





X

(3.54)

Probability Theory
85
The covariance matrix
( )
Cov X
of random vector X is defined to be an n
n
×
matrix
such that the element in the ith row and jth column is
(
,
)
i
j
Cov X Y
, that is,
[
][
]
1
1
1
1
(
,
)
(
,
)
( )
(
)
(
)
(
,
)
(
,
)
n
t
n
n
n
Cov X
X
Cov X
X
Cov
E
X
E X
X
E X
Cov X
X
Cov X
X






=
=
−
−








X




(3.55)
It should be emphasized that the n diagonal elements of the covariance matrix
( )
Cov X
are
actually the variances of
1,
,
n
X
X

. Furthermore, since covariance is symmetric, i.e.,
(
,
)
(
,
)
i
j
j
i
Cov X
X
Cov X
X
=
, the covariance matrix
( )
Cov X must be a symmetric matrix.
There is an important theorem regarding the mean vector and covariance matrix for a
linear transformation of the random vector X. Suppose X is an n-dimensional vector as
specified by Eq. (3.53), with mean vector
( )
E X
and covariance matrix
( )
Cov X . Now, as-
sume Y is a m-dimensional random vector which is a linear transform of random vector X
by the relation:
=
+
Y
AX
B , where A is a m
n
×
transformation matrix whose elements are
constants, and B is a m-dimensional constant vector. Then we have the following two equa-
tions:
( )
( )
E
E
=
+
Y
A
X
B
(3.56)
( )
( )
t
Cov
Cov
=
Y
A
X A
(3.57)
3.1.6.
Some Useful Distributions
In the following two sections, we will introduce several useful distributions that are widely
used in applications of probability and statistics, particularly in spoken language systems.
3.1.6.1.
Uniform Distributions
The simplest distribution is uniform distribution where the p.f. or p.d.f. is a constant func-
tion. For uniform discrete random variable X, which only takes possible values from
{
}
|1
ix
i
n
≤≤
, the p.f. for X is
1
(
)
1
i
P X
x
i
n
n
=
=
≤≤
(3.58)
For uniform continuous random variable X, which only takes possible values from real
interval [
]
,a b , the p.d.f. for X is

86
Probability, Statistics, and Information Theory
1
( )
f x
a
x
b
b
a
=
≤
≤
−
(3.59)
Figure 3.5 A uniform distribution for p.d.f. in Eq. (3.59)
3.1.6.2.
Binomial Distributions
The binomial distribution is used to describe binary-decision events. For example, suppose
that a single coin toss will produce the head with probability p and produce the tail with
probability 1
p
−
. Now, if we toss the same coin n times and let X denote the number of
heads observed, then the random variable X has the following binomial p.f.:
(
)
( | , )
(1
)
x
n x
n
P X
x
f x n p
p
p
x
−
 
=
=
=
−
 
 
(3.60)
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0
0
1
2
3
4
5
6
7
8
9
10
x
p=0.2
p=0.3
p=0.4
Figure 3.6 Three binomial distributions with p=0.2, 0.3 and 0.4.
It can be shown that the mean and variance of a binomial distribution are:
( )
f x
a
1
b a
−
b
x

Probability Theory
87
(
)
E X
np
=
(3.61)
(
)
(1
)
Var X
np
p
=
−
(3.62)
Figure 3.6 illustrates three binomial distributions with p = 0.2, 0.3 and 0.4.
3.1.6.3.
Geometric Distributions
The geometric distribution is related to the binomial distribution. As in the independent coin
toss example, the head-up has a probability p and the tail-up has a probability 1
p
−
. The
geometric distribution is to model the time until a tail-up appears. Let the random variable X
be the time (the number of tosses) until the first tail-up is shown. The p.d.f. of X is in the
following form:
1
(
)
( |
)
(1
)
1,2,
and 0
1
x
P X
x
f x p
p
p
x
p
−
=
=
=
−
=
<
<

(3.63)
The mean and variance of a geometric distribution are given by:
1
(
)
1
E X
p
=
−
(3.64)
2
1
(
)
(1
)
Var X
p
=
−
(3.65)
One example for the geometric distribution is the distribution of the state duration for
a hidden Markov model, as described in Chapter 8. Figure 3.7 illustrates three geometric
distributions with p = 0.2, 0.3 and 0.4.
1
2
3
4
5
6
7
8
9
1 0
0
0 .1
0 .2
0 .3
0 .4
0 .5
0 .6
0 .7
0 .8
0 .9
x
p = .1
p = .4
p = .7
Figure 3.7 Three geometric distributions with different parameter p.

88
Probability, Statistics, and Information Theory
3.1.6.4.
Multinomial Distributions
Suppose that a bag contains balls of k different colors, where the proportion of the balls of
color i is
ip . Thus,
0 for
1,
,
ip
i
k
>
=

and
1
1
k
i
i
p
=
=

. Now suppose that n balls are ran-
domly selected from the bag and there are enough balls (
n
>
) of each color. Let
i
X
denote
the number of selected balls that are of color i. The random vector
1
(
,
,
)
k
X
X
=
X

is said
to have a multinomial distribution with parameters n and
1
(
,
)
k
p
p
=
p

. For a vector
1
( ,
)
k
x
x
=
x

, the p.f. of X has the following form:
1
1
i
1
1
!
,
where
0
1,
,
!,
!
(
)
( | , )
and
0
otherwise
kx
x
k
k
k
n
p
p
x
i
k
x
x
P
f
n
x
x
n

≥
∀=


=
=
=
+
+
=




X
x
x
p




(3.66)
0
2
4
6
8
1 0
0
2
4
6
8
1 0
0
0 .0 2
0 .0 4
0 .0 6
X2
X1
Figure 3.8 A multinomial distribution with n=10,
1
0.2
p =
and
2
0.3
p =
It can be shown that the mean, variance and covariance of the multinomial distribution
are:
(
)
and
(
)
(1
)
1,
,
i
i
i
i
i
E X
np
Var X
np
p
i
k
=
=
−
∀=

(3.67)

Probability Theory
89
(
,
)
i
j
i
j
Cov X
X
np p
= −
(3.68)
Figure 3.8 shows a multinomial distribution with n = 10,
1
0.2
p =
and
2
0.3
p =
.
Since there are only two free parameters
1x and
2x , the graph is illustrated only using
1x
and
2x
as axis. Multinomial distributions are typically used with the
2
χ
test that is one of
the most widely used goodness-of-fit hypotheses testing procedures described in Section
3.3.3.
3.1.6.5.
Poisson Distributions
Another popular discrete distribution is Poisson distribution. The random variable X has a
Poisson distribution with mean
(
0)
λ λ >
if the p.f. of X has the following form:
(
)
( |
)
for =0,1,2,
!
0
otherwise
x
e
P X
x
f x
x
x
λλ
λ
−

=
=
= 


(3.69)
The mean and variance of a Poisson distribution are the same and equal λ :
(
)
(
)
E X
Var X
λ
=
=
(3.70)
0
1
2
3
4
5
6
7
8
9
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
x
lambda= 1
lambda= 2
lambda= 4
Figure 3.9 Three Poisson distributions with λ = 1, 2, and 4.
Figure 3.9 illustrates three Poisson distributions with λ = 1, 2, and 4. The Poisson dis-
tribution is typically used in queuing theory, where x is the total number of occurrences of
some phenomenon during a fixed period of time or within a fixed region of space. Examples
include the number of telephone calls received at a switchboard during a fixed period of

90
Probability, Statistics, and Information Theory
time. In speech recognition, the Poisson distribution is used to model the duration for a pho-
neme.
3.1.6.6.
Gamma Distributions
A continuous random variable X is said to have a gamma distribution with parameters α
and β (
0 and
0
α
β
>
>
) if X has a continuous p.d.f. of the following form:
1
>0
( |
,
)
( )
0
0
x
x
e
x
f x
x
α
α
β
β
α β
α
−
−


=
Γ


≤

(3.71)
where
1
0
( )
x
x
e dx
α
α
∞
−
−
Γ
= 
(3.72)
It can be shown that the function Γ is a factorial function when α is a positive integer.
(
1)!
2,3,
( )
1
=1
n
n
n
n
−
=

Γ
= 


(3.73)
0
1
2
3
4
5
6
7
8
9
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
x
alpha = 2
alpha = 3
alpha = 4
Figure 3.10 Three Gamma distributions with
1.0
β =
and α = 2.0, 3.0, and 4.0.
The mean and variance of a gamma distribution are:
2
(
)
and
(
)
E X
Var X
α
α
β
β
=
=
(3.74)

Probability Theory
91
Figure 3.10 illustrates three gamma distributions with
1.0
β =
and α = 2.0, 3.0, and
4.0. There is an interesting theorem associated with gamma distributions. If the random
variables
1,
,
k
X
X

are independent and each random variable
i
X
has a gamma distribu-
tion with parameters
i
α
and β , then the sum
1
k
X
X
+
+

also has a gamma distribution
with parameters
1
k
α
α
+
+

and β .
A special case of gamma distribution is called exponential distribution. A continuous
random variable X is said to have an exponential distribution with parameters β (
0
β >
) if
X has a continuous p.d.f. of the following form:
0
( |
)
0
0
x
e
x
f x
x
β
β
β
−

>

= 
≤

(3.75)
It is clear that the exponential distribution is a gamma distribution with
1
α = . The mean
and variance of the exponential distribution are:
2
1
1
(
)
and
(
)
E X
Var X
β
β
=
=
(3.76)
0
1
2
3
4
5
6
7
8
9
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
beta = 1
beta =.6
beta =.3
Figure 3.11 Three exponential distributions with β = 1.0, 0.6 and 0.3.
Figure 3.11 shows three exponential distributions with β = 1.0, 0.6, and 0.3. The ex-
ponential distribution is often used in queuing theory for the distributions of the duration of
a service or the inter-arrival time of customers. It is also used to approximate the distribution
of the life of a mechanical component.

92
Probability, Statistics, and Information Theory
3.1.7.
Gaussian Distributions
Gaussian distribution is by far the most important probability distribution mainly because
many scientists have observed that the random variables studied in various physical experi-
ments (including speech signals), often have distributions that are approximately Gaussian.
The Gaussian distribution is also referred to as normal distribution. A continuous random
variable X is said to have a Gaussian distribution with mean µ and variance
2
σ
(
0
σ >
) if
X has a continuous p.d.f. in the following form:
2
2
2
2
1
(
)
( |
,
)
( ,
)
exp
2
2
x
f x
N
µ
µ σ
µ σ
σ
πσ


−
=
=
−




(3.77)
It can be shown that µ and
2
σ
are indeed the mean and the variance for the Gaussian
distribution. Some examples of Gaussian can be found in Figure 3.4.
The use of Gaussian distributions is justified by the Central Limit Theorem, which
states that observable events considered to be a consequence of many unrelated causes with
no single cause predominating over the others, tend to follow the Gaussian distribution [6].
It can be shown from Eq. (3.77) that the Gaussian
2
( |
,
)
f x µ σ
is symmetric with re-
spect to x
µ
=
. Therefore, µ is both the mean and the median of the distribution. More-
over, µ is also the mode of the distribution, i.e., the p.d.f.
2
( |
,
)
f x µ σ
attains its maximum
at the mean point x
µ
=
.
Several Gaussian p.d.f.’s with the same mean µ , but different variances are illustrated
in Figure 3.4. Readers can see that the curve has a bell shape. The Gaussian p.d.f. with a
small variance has a high peak and is very concentrated around the mean µ , whereas the
Gaussian p.d.f., with a large variance, is relatively flat and is spread out more widely over
the x-axis.
If the random variable X is a Gaussian distribution with mean µ and variance
2
σ ,
then any linear function of X also has a Gaussian distribution. That is, if Y
aX
b
=
+
, where
a and b are constants and
0
a ≠
, Y has a Gaussian distribution with mean a
b
µ +
and vari-
ance
2
2
a σ . Similarly, the sum
1
k
X
X
+
+

of independent random variables
1,
,
k
X
X

,
where each random variable
i
X has a Gaussian distribution, is also a Gaussian distribution.
3.1.7.1.
Standard Gaussian Distributions
The Gaussian distribution with mean 0 and variance 1, denoted as
(0,1)
N
, is called the
standard Gaussian distribution or unit Gaussian distribution. Since the linear transformation
of a Gaussian distribution is still a Gaussian distribution, the behavior of a Gaussian distri-
bution can be solely described using a standard Gaussian distribution. If the random variable

Probability Theory
93
X is a Gaussian distribution with mean µ and variance
2
σ , that is,
2
~
( ,
)
X
N µ σ
, it can be
shown that
~
(0,1)
X
Z
N
µ
σ
−
=
(3.78)
Based on Eq. (3.78), the following property can be shown:
(|
|
)
(|
|
)
P
X
k
P Z
k
µ
σ
−
≤
=
≤
(3.79)
Equation (3.79) demonstrates that every Gaussian distribution contains the same total
amount of probability within any fixed number of standard deviations of its mean.
3.1.7.2.
The Central Limit Theorem
If random variables
1,
,
n
X
X

are i.i.d. according to a common distribution function with
mean µ and variance
2
σ , then as the random sample size n approaches ∞, the following
random variable has a distribution converging to the standard Gaussian distribution:
2
(
) ~
(0,1)
n
n
n X
Y
N
n
µ
σ
−
=
(3.80)
where
n
X
is the sample mean of random variables
1,
,
n
X
X

as defined in Eq. (3.41).
Based on Eq. (3.80), the sample mean random variable
n
X
can be approximated by a
Gaussian distribution with mean µ and variance
2 / n
σ
.
The central limit theorem above is applied to i.i.d. random variables
1,
,
n
X
X

. A.
Liapounov in 1901 derived another central limit theorem for independent but not necessarily
identically distributed random variables
1,
,
n
X
X

. Suppose
1,
,
n
X
X

are independent
random variables and
3
(|
| )
i
i
E
X
µ
−
< ∞for 1
i
n
≤≤
; the following random variable will
converge to standard Gaussian distribution when n →∞.
1/2
2
1
1
1
(
) /
n
n
n
n
i
i
i
i
i
i
Y
X
µ
σ
=
=
=


=
−







(3.81)
In other words, the sum of random variables
1,
,
n
X
X

can be approximated by a
Gaussian distribution with mean
1
n
i
i
µ
=
and variance
1/2
2
1
n
i
i
σ
=







.
Both central limit theorems essentially state that regardless of their original individual
distributions, the sum of many independent random variables (effects) tends to be distributed
like a Gaussian distribution as the number of random variables (effects) becomes large.

94
Probability, Statistics, and Information Theory
3.1.7.3.
Multivariate Mixture Gaussian Distributions
When
1
(
,
,
)
n
X
X
=
X

is an n-dimensional continuous random vector, the multivariate
Gaussian p.d.f. has the following form:
(
)
1
/2
1/2
1
1
(
| ,
)
( ; ,
)
exp
(
)
(
)
2
2
t
n
f
N
π
−


=
=
=
−
−
−




X
x µ Σ
x µ Σ
x
µ Σ
x
µ
Σ
(3.82)
where µ is the n-dimensional mean vector, Σ is the n
n
×
covariance matrix, and Σ is the
determinant of the covariance matrix Σ .
( )
E
=
µ
x
(3.83)
(
)(
)t
E 

=
−
−


Σ
x
µ x
µ
(3.84)
More specifically, the i-jth element
2
ij
σ
of covariance matrix Σ can be specified as fol-
lows:
2
(
)(
)
ij
i
i
j
j
E
x
x
σ
µ
µ


=
−
−


(3.85)
Figure 3.12 A two-dimensional multivariate Gaussian distribution with independent random
variables
1x and
2x that have the same variance.

Probability Theory
95
If
1,
,
n
X
X

are independent random variables, the covariance matrix Σ is reduced
to diagonal covariance where all the off-diagonal entries are zero. The distribution can be
regarded as n independent scalar Gaussian distributions. The joint p.d.f. is the product of all
the individual scalar Gaussian p.d.f.. Figure 3.12 shows a two-dimensional multivariate
Gaussian distribution with independent random variables
1x and
2x
with the same variance.
Figure 3.13 shows another two-dimensional multivariate Gaussian distribution with inde-
pendent random variables
1x and
2x that have different variances.
Although Gaussian distributions are unimodal,2 more complex distributions with mul-
tiple local maxima can be approximated by Gaussian mixtures:
1
( )
( ;
,
)
K
k
k
k
k
k
f
c N
=
=
x
x µ
Σ
(3.86)
where
kc , the mixture weight associated with kth Gaussian component are subject to the
following constraint:
1
0 and
1
K
k
k
k
c
c
=
≥
=

Gaussian mixtures with enough mixture components can approximate any distribution.
Throughout this book, most continuous probability density functions are modeled with
Gaussian mixtures.
Figure 3.13 Another two-dimensional multivariate Gaussian distribution with independent
2 A unimodal distribution has a single maximum (bump) for the distribution. For Gaussian distribution, the maxi-
mum occurs at the mean.

96
Probability, Statistics, and Information Theory
random variable
1x and
2x
which have different variances.
3.1.7.4.
2
χ
Distributions
A gamma distribution with parameters α and β is defined in Eq. (3.71). For any given
positive integer n, the gamma distribution for which
2
n
α =
and
1 2
β =
is called the
2
χ
distribution with n degrees of freedom. It follows from Eq. (3.71) that the p.d.f. for the
2
χ
distribution is
(
2) 1
2
2
1
>0
2
(
2)
( | )
0
0
n
x
n
x
e
x
n
f x n
x
−
−


Γ
= 

≤

(3.87)
2
χ
distributions are important in statistics because they are closely related to random
samples of Gaussian distribution. They are widely applied in many important problems of
statistical inference and hypothesis testing. Specifically, if the random variables
1,
,
n
X
X

are independent and identically distributed, and if each of these variables has a standard
Gaussian distribution, then the sum of square
2
2
1
n
X
X
+
+

can be proved to have a
2
χ
distribution with n degree of freedom. Figure 3.14 illustrates three
2
χ
distributions with
n = 2, 3 and 4.
0
1
2
3
4
5
6
7
8
9
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
x
n = 2
n = 3
n = 4
Figure 3.14 Three
2
χ
distributions with n = 2, 3, and 4.
The mean and variance for the
2
χ
distribution are

Probability Theory
97
(
)
and
(
)
2
E X
n
Var X
n
=
=
(3.88)
Following the additivity property of the gamma distribution, the
2
χ
distribution also
has the additivity property. That is, if the random variables
1,
,
n
X
X

are independent and
if
i
X
has a
2
χ
distribution with
ik degrees of freedom, the sum
1
n
X
X
+
+

has a
2
χ
distribution with
1
n
k
k
+
+

degrees of freedom.
3.1.7.5.
Log-Normal Distribution
Let x, be a Gaussian random variable with mean
x
µ and standard deviation
x
σ , then
x
y
e
=
(3.89)
follows a log-normal distribution
2
2
(ln
)
1
( |
,
)
exp
2
2
x
x
x
x
x
y
f y
y
µ
µ σ
σ
σ
π


−
=
−




(3.90)
shown in Figure 3.15, and whose mean is given by
{ }
{
}
{
}
2
2
2
2
2
2
2
(
)
1
{ }
{ }
exp
exp
2
2
(
(
)
1
exp
/ 2
exp
exp
/ 2
2
2
x
x
y
x
x
x
x
x
x
x
x
x
x
x
E y
E e
x
dx
x
dx
µ
µ
σ
πσ
µ
σ
µ
σ
µ
σ
σ
πσ
∞
−∞
∞
−∞


−
=
=
=
−






−
+
=
+
−
=
+






(3.91)
where we have rearranged the quadratic form of x and made use of the fact that the total
probability mass of a Gaussian is 1. Similarly, the second order moment of y is given by
{
}
{
}
{
}
2
2
2
2
2
2
2
2
(
)
1
{
}
exp 2
exp
2
2
(
(
2
)
1
exp 2
2
exp
exp 2
2
2
2
x
x
x
x
x
x
x
x
x
x
x
x
E y
x
dx
x
dx
µ
σ
πσ
µ
σ
µ
σ
µ
σ
σ
πσ
∞
−∞
∞
−∞


−
=
−






−
+
=
+
−
=
+






(3.92)
and thus the variance of y is given by
(
)
{
}
(
)
2
2
2
2
2
{
}
{ }
exp
1
y
y
x
E y
E y
σ
µ
σ
=
−
=
−
(3.93)

98
Probability, Statistics, and Information Theory
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
std=3
std=1
std=0.5
Figure 3.15 Lognormal distribution for
0
x
µ =
and
3
x
σ =
, 1 and 0.5 according to Eq. (3.90).
Similarly, if x is a Gaussian random vector with mean
x
µ
and covariance matrix
x
Σ ,
then random vector
e
=
x
y
is log-normal with mean and covariance matrix [8] given by
{
}
{
}
(
)
[ ]
exp
[ ]
[ , ]/ 2
[ , ]
[ ]
[ ] exp
[ , ]
1
i
i
i i
i j
i
j
i j
=
+
=
−
y
x
x
y
y
y
x
µ
µ
Σ
Σ
µ
µ
Σ
(3.94)
using a similar derivation as in Eqs. (3.91) to (3.93).
3.2.
ESTIMATION THEORY
Estimation theory and significance testing are two most important theories and methods of
statistical inference. In this section, we describe estimation theory while significance testing
is covered in the next section. A problem of statistical inference is one in which data gener-
ated in accordance with some unknown probability distribution must be analyzed, and some
type of inference about the unknown distribution must be made. In a problem of statistical
inference, any characteristic of the distribution generating the experimental data, such as the
mean µ and variance
2
σ
of a Gaussian distribution, is called a parameter of the distribu-
tion. The set Ωof all possible values of a parameter Φ or a group of parameters
1
2
,
,
,
n
Φ Φ
Φ

is called the parameter space. In this section we focus on how to estimate the
parameter Φ from sample data.
Before we describe various estimation methods, we introduce the concept and nature
of the estimation problems. Suppose that a set of random variables
1
2
{
,
,
,
}
n
X
X
X
=
X

is

Estimation Theory
99
i.i.d. according to a p.d.f.
( |
)
p x Φ
where the value of the parameter Φ is unknown. Now,
suppose also that the value of Φ must be estimated from the observed values in the sample.
An estimator of the parameter Φ , based on the random variables
1
2
,
,
,
n
X
X
X

, is a real-
valued function
1
2
(
,
,
,
)
n
X
X
X
θ

that specifies the estimated value of Φ for each possible
set of values of
1
2
,
,
,
n
X
X
X

. That is, if the sample values of
1
2
,
,
,
n
X
X
X

turn out to be
1
2
,
,
,
n
x x
x

, then the estimated value of Φ will be
1
2
(
,
,
,
)
n
x x
x
θ

.
We need to distinguish between estimator, estimate, and estimation. An estimator
1
2
(
,
,
,
)
n
X
X
X
θ

is a function of the random variables, whose probability distribution can
be derived from the joint distribution of
1
2
,
,
,
n
X
X
X

. On the other hand, an estimate is a
specific value
1
2
(
,
,
,
)
n
x x
x
θ

of the estimator that is determined by using some specific
sample values
1
2
,
,
,
n
x x
x

. Estimation is usually used to indicate the process of obtaining
such an estimator for the set of random variables or an estimate for the set of specific sample
values. If we use the notation
1
2
{
,
,
,
}
n
X
X
X
=
X

to represent the vector of random vari-
ables and
{
}
1
2
,
,
,
n
x x
x
=
x

to represent the vector of sample values, an estimator can be
denoted as
( )
θ X
and an estimate
( )
θ x . Sometimes we abbreviate an estimator
( )
θ X
by
just the symbol θ .
In the following four sections we describe and compare three different estimators (es-
timation methods). They are minimum mean square estimator, maximum likelihood estima-
tor, and Bayes estimator. The first one is often used to estimate the random variable itself,
while the latter two are used to estimate the parameters of the distribution of the random
variables.
3.2.1.
Minimum/Least Mean Squared Error Estimation
Minimum mean squared error (MMSE) estimation and least squared error (LSE) estimation
are important methods for random variable since the goal (minimize the squared error) is an
intuitive one. In general, two random variables X and Y are i.i.d. according to some p.d.f.
, ( , )
X Y
f
x y . Suppose that we perform a series of experiments and observe the value of X. We
want to find a transformation ˆ
(
)
Y
g X
=
such that we can predict the value of the random
variable Y. The following quantity can measure the goodness of such a transformation.
2
2
ˆ
(
)
(
(
))
E Y
Y
E Y
g X
−
=
−
(3.95)
This quantity is called mean squared error (MSE) because it is the mean of the
squared error of the predictor
(
)
g X . The criterion of minimizing the mean squared error is
a good one for picking the predictor
(
)
g X . Of course, we usually specify the class of func-
tion G, from which
(
)
g X
may be selected. In general, there is a parameter vector Φ asso-
ciated with the function
(
)
g X , so the function can be expressed as
(
,
)
g X Φ . The process to

100
Probability, Statistics, and Information Theory
find the parameter vector
ˆ
MMSE
Φ
that minimizes the mean of the squared error is called
minimum mean squared error estimation and ˆ
MMSE
Φ
is called the minimum mean squared
error estimator. That is,
2
ˆ
argmin
(
(
,
))
MMSE
E
Y
g X
Φ




=
−




Φ
Φ
(3.96)
Sometimes, the joint distribution of random variables X and Y is not known. Instead,
samples of (x,y) pairs may be observable. In this case, the following criterion can be used
instead,
2
1
argmin
(
,
)
n
LSE
i
i
i
y
g x
Φ
=
=
−





Φ
Φ
(3.97)
The argument of the minimization in Eq. (3.97) is called sum-of-squared-error (SSE) and
the process of finding the parameter vector ˆ
LSE
Φ
, which satisfies the criterion is called least
squared error estimation or minimum squared error estimation. LSE is a powerful mecha-
nism for curve fitting, where the function
( ,
)
g x Φ describes the observation pairs ( ,
)
i
i
x y
. In
general, there are more points (n) than the number of free parameters in function
( ,
)
g x Φ ,
so the fitting is over-determined. Therefore, no exact solution exists, and LSE fitting be-
comes necessary.
It should be emphasized that MMSE and LSE are actually very similar and share simi-
lar properties. The quantity in Eq. (3.97) is actually n times the sample mean of the squared
error. Based on the law of large numbers, when the joint probability
, ( , )
X Y
f
x y
is uniform
or the number of samples approaches to infinity, MMSE and LSE are equivalent.
For the class of functions, we consider the following three cases:
 Constant functions, i.e.,
{
}
( )
,
c
G
g x
c c
R
=
=
∈
(3.98)
 Linear functions, i.e.,
{
}
( )
,
,
l
G
g x
ax
b
a b
R
=
=
+
∈
(3.99)
 Other non-linear functions
nl
G
3.2.1.1.
MMSE/LSE for Constant Functions
When ˆ
( )
Y
g x
c
=
=
, Eq. (3.95) becomes
2
2
ˆ
(
)
(
)
E Y
Y
E Y
c
−
=
−
(3.100)

Estimation Theory
101
To find the MMSE estimate for c, we take the derivatives of both sides in Eq. (3.100)
with respect to c and equate it to 0. The MMSE estimate
MMSE
c
is given as
( )
MMSE
c
E Y
=
(3.101)
and the minimum mean squared error is exactly the variance of Y,
( )
Var Y .
For the LSE estimate of c, the quantity in Eq. (3.97) becomes
[
]
2
1
min
n
i
i
y
c
=
−

(3.102)
Similarly, the LSE estimate
LSE
c
can be obtained as follows:
1
1
n
LSE
i
i
c
y
n
=
= 
(3.103)
The quantity in Eq. (3.103) is the sample mean.
3.2.1.2.
MMSE and LSE For Linear Functions
When ˆ
( )
Y
g x
ax
b
=
=
+
, Eq. (3.95) becomes
2
2
ˆ
( , )
(
)
(
)
e a b
E Y
Y
E Y
ax
b
=
−
=
−
−
(3.104)
To find the MMSE estimate of a and b, we can first set
0, and
0
e
e
a
b
∂
∂
=
=
∂
∂
(3.105)
and solve the two linear equations. Thus, we can obtain
cov(
, )
(
)
Y
XY
X
X Y
a
Var X
σ
ρ
σ
=
=
(3.106)
( )
(
)
Y
XY
X
b
E Y
E X
σ
ρ
σ
=
−
(3.107)
For LSE estimation, we assume that the sample x is a d-dimensional vector for gener-
ality. Assuming we have n sample-vectors
1
2
(
,
)
(
,
,
,
,
),
1
d
i
i
i
i
i
i
y
x x
x
y
i
n
=
=
x

 , a linear
function can be represented as

102
Probability, Statistics, and Information Theory
1
0
1
1
1
1
1
2
2
2
1
n
y
1
y
1
ˆ
or
y
1
d
d
d
d
n
n
a
x
x
a
x
x
a
x
x















=
= 





















Y
XA








(3.108)
The sum of squared error can then be represented as
(
)
2
2
1
ˆ
(
) ||
||
n
t
i
i
i
e
y
=
=
−
=
−

A
Y
Y
A x
(3.109)
A closed-form solution of the LSE estimate of A can be obtained by taking the gradi-
ent of
( )
e A ,
1
( )
2(
)
2
(
)
n
t
t
i
i
i
i
e
y
=
∇
=
−
=
−

A
A x
x
X XA
Y
(3.110)
and equating it to zero. This yields the following equation:
t
t
=
X XA
X Y
(3.111)
Thus the LSE estimate
LSE
A
will be of the following form:
1
(
)
t
t
LSE
−
=
A
X X
X Y
(3.112)
1
(
)
t
t
−
X X
X in Eq. (3.112) is also refereed to as the pseudo-inverse of X and is sometimes
denoted as
⊥
X .
When
t
X X is singular or some boundary conditions cause the LSE estimation in Eq.
(3.112) to be unattainable, some numeric methods can be used to find an approximate solu-
tion. Instead of minimizing the quantity in Eq. (3.109), one can minimize the following
quantity:
2
2
( ) ||
||
||
||
e
α
=
−
+
A
XA
Y
X
(3.113)
Following a similar procedure, one can obtain the LSE estimate to minimize the quan-
tity above in the following form.
*
1
(
)
t
t
LSE
α
−
=
+
A
X X
I
X Y
(3.114)
The LSE solution in Eq. (3.112) can be used for polynomial functions too. In the prob-
lem of polynomial curve fitting using the least square criterion, we are aiming to find the
coefficients
0
1
2
(
,
,
,
,
)t
d
a a a
a
=
A

that minimize the following quantity:
0
1
2
2
,
,
,
,
ˆ
min
(
)
d
a
a a
a E Y
Y
−

(3.115)

Estimation Theory
103
where
2
0
1
2
ˆ
d
d
Y
a
a x
a x
a x
=
+
+
+
+

To obtain the LSE estimate of coefficients
0
1
2
(
,
,
,
,
)t
d
a a a
a
=
A

, simply change the
formation of matrix X in Eq. (3.108) to the following:
1
1
2
2
1
1
1
d
d
d
n
n
x
x
x
x
x
x






= 







X






(3.116)
Note that
j
ix
in Eq. (3.108) means the j-th dimension of sample
ix , while
j
ix
in Eq.
(3.116) means j-th order of value
ix . Therefore, the LSE estimate of polynomial coefficients
0
1
2
(
,
,
,
,
)t
LSE
d
a a a
a
=
A

has the same form as Eq. (3.112).
3.2.1.3.
MMSE/LSE For Nonlinear Functions
As the most general case, consider solving the following minimization problem:
[
]
2
( )min
(
)
nl
g
G E Y
g X
• ∈
−
(3.117)
Since we need to deal with all possible nonlinear functions, taking a derivative does
not work here. Instead, we use the property of conditional expectation to solve this minimi-
zation problem. By applying Eq. (3.38) to (3.117), we get
[
]
[
]
{
}
[
]
[
]
2
2
,
|
2
|
-
2
|
-
(
)
(
)
|
=
(
)
|
( )
=
( )
|
( )
X Y
X
Y X
Y X
X
Y X
X
E
Y
g X
E
E
Y
g X
X
x
E
Y
g X
X
x f
x dx
E
Y
g x
X
x f
x dx
∞
∞
∞
∞


−
=
−
=




−
=




−
=




(3.118)
Since the integrand is nonnegative in Eq. (3.118), the quantity in Eq. (3.117) will be
minimized at the same time the following equation is minimized.
[
]
2
|
( )
min
( )
|
Y X
g x
R E
Y
g x
X
x
∈


−
=


(3.119)
Since
( )
g x
is a constant in the calculation of the conditional expectation above, the
MMSE estimate can be obtained in the same way as the constant functions in Section
3.2.1.1. Thus, the MMSE estimate should take the following form:
|
ˆ
(
)
(
|
)
MMSE
Y X
Y
g
X
E
Y X
=
=
(3.120)

104
Probability, Statistics, and Information Theory
If the value X
x
=
is observed and the value
(
|
)
E Y X
x
=
is used to predict Y, the
mean squared error (MSE) is minimized and specified as follows:
2
|
|
|
(
|
)
|
(
|
)
Y X
Y X
Y X
E
Y
E
Y X
x
X
x
Var
Y X
x




−
=
=
=
=






(3.121)
The overall MSE, averaged over all the possible values of X, is:
{
}
2
2
|
|
|
|
(
|
)
(
|
)
|
(
|
)
X
Y X
X
Y X
Y X
X
Y X
E
Y
E
Y X
E
E
Y
E
Y X
X
E
Var Y X
x








−
=
−
=
=










(3.122)
It is important to distinguish between the overall MSE
| (
|
)
X
Y X
E
Var
Y X



 and the
MSE of the particular estimate when X
x
=
, which is
| (
|
)
Y X
Var
Y X
x
=
. Before the value of
X is observed, the expected MSE for the process of observing X and predicting Y is
| (
|
)
X
Y X
E
Var
Y X



 . On the other hand, after a particular value x of X has been observed and
the prediction
| (
|
)
Y X
E
Y X
x
=
has been made, the appropriate measure of MSE of the pre-
diction is
| (
|
)
Y X
Var
Y X
x
=
.
In general, the form of the MMSE estimator for nonlinear functions depends on the
form of the joint distribution of X and Y. There is no mathematical closed-form solution. To
get the conditional expectation in Eq. (3.120), we have to perform the following integral:
ˆ( )
( |
)
Y
Y x
yf
y X
x dy
∞
−∞
=
=

(3.123)
It is difficult to solve this integral calculation. First, different measures of x could de-
termine different conditional p.d.f. for the integral. Exact information about the p.d.f. is of-
ten impossible to obtain. Second, there could be no analytic solution for the integral. Those
difficulties reduce the interest of the MMSE estimation of nonlinear functions to theoretical
aspects only. The same difficulties also exist for LSE estimation for nonlinear functions.
Some certain classes of well-behaved nonlinear functions are typically assumed for LSE
problems and numeric methods are used to obtain LSE estimate from sample data.
3.2.2.
Maximum Likelihood Estimation
Maximum likelihood estimation (MLE) is the most widely used parametric estimation
method, largely because of its efficiency. Suppose that a set of random samples
1
2
{
,
,
,
}
n
X
X
X
=
X

is to be drawn independently according to a discrete or continuous dis-
tribution with the p.f. or the p.d.f.
( |
)
p x Φ , where the parameter vector Φ belongs to some
parameter space Ω. Given an observed vector
1
( ,
,
)
n
x
x
=
x

, the likelihood of the set of
sample data vectors x with respect to Φ is defined as the joint p.f. or joint p.d.f.
( |
)
np x Φ ;
( |
)
np x Φ is also referred to as the likelihood function.

Estimation Theory
105
MLE assumes the parameters of p.d.f.’s are fixed but unknown and aims to find the set
of parameters that maximizes the likelihood of generating the observed data. For example,
the p.d.f.
( |
)
np x Φ
is assumed to be a Gaussian distribution
( ,
)
N µ Σ , the components of
Φ will then include exactly the components of mean-vector µ and covariance matrix Σ .
Since
1
2
,
,
,
n
X
X
X

are independent random variables, the likelihood can be rewritten as
follows:
1
( |
)
(
|
)
n
n
k
k
p
p x
=
=∏
x Φ
Φ
(3.124)
The likelihood
( |
)
np x Φ
can be viewed as the probability of generating the sample
data set x based on parameter set Φ . The maximum likelihood estimator of Φ is denoted as
MLE
Φ
that maximizes the likelihood
( |
)
np x Φ . That is,
argmax
( |
)
MLE
np
Φ
=
Φ
x Φ
(3.125)
This estimation method is called the maximum likelihood estimation method and is of-
ten abbreviated as MLE. Since the logarithm function is a monotonically increasing func-
tion, the parameter set
MLE
Φ
that maximizes the log-likelihood should also maximize the
likelihood. If
( |
)
np
Φ
x
is differentiable function of Φ ,
MLE
Φ
can be attained by taking the
partial derivative with respect to Φ and setting it to zero. Specifically, let Φ be a k-
component parameter vector
1
2
(
,
,
,
)t
k
= Φ Φ
Φ
Φ

and ∇Φ be the gradient operator:
1
k
∂




∂Φ


∇
= 



∂




∂Φ


Φ

(3.126)
The log-likelihood becomes:
1
(
)
log
( |
)
log (
|
)
n
n
k
k
l
p
p x
=
=
=
Φ
x Φ
Φ
(3.127)
and its partial derivative is:
1
(
)
log (
|
)
n
k
k
l
p x
=
∇
=
∇

Φ
Φ
Φ
Φ
(3.128)
Thus, the maximum likelihood estimate of Φ can be obtained by solving the follow-
ing set of k equations:
(
)
0
l
∇
=
Φ
Φ
(3.129)

106
Probability, Statistics, and Information Theory
Example 1
Let’s take a look at the maximum likelihood estimator of a univariate Gaussian p.d.f., given
as the following equation:
2
2
1
(
)
( |
)
exp
2
2
x
p x
µ
σ
πσ


−
=
−




Φ
(3.130)
where µ and
2
σ
are the mean and the variance respectively. The parameter vector Φ de-
notes
2
( ,
)
µ σ
. The log-likelihood is:
1
2
2
1
2
2
2
1
log
( |
)
log (
|
)
(
)
1
log
exp
2
2
1
log(2
)
(
)
2
2
n
n
k
k
n
k
k
n
k
k
p
p x
x
n
x
µ
σ
πσ
πσ
µ
σ
=
=
=
=




−
=
−






	



= −
−
−



x Φ
Φ
(3.131)
and the partial derivative of the above expression is:
2
1
2
2
2
4
1
1
log
( |
)
(
)
(
)
log
( |
)
2
2
n
n
k
k
n
k
n
k
p
x
x
x
n
p
x
µ
µ
σ
µ
σ
σ
σ
=
=
∂
=
−
∂
−
∂
= −
+
∂


Φ
Φ
(3.132)
We set the two partial differential derivatives to zero,
2
1
2
2
4
1
1 (
)
0
(
)
0
n
k
k
n
k
k
x
x
n
µ
σ
µ
σ
σ
=
=
−
=
−
−
+
=


(3.133)
The maximum likelihood estimates for µ and
2
σ
are obtained by solving the above equa-
tions:
1
2
2
2
1
1
( )
1
(
)
(
)
n
MLE
k
k
n
MLE
k
MLE
MLE
k
x
E x
n
x
E
x
n
µ
σ
µ
µ
=
=
=
=


=
−
=
−




(3.134)
Equation (3.134) indicates that the maximum likelihood estimation for mean and vari-
ance is just the sample mean and variance.

Estimation Theory
107
Example 2
For the multivariate Gaussian p.d.f.
( )
p x
(
)
1
/ 2
1/ 2
1
1
( |
)
exp
(
)
(
)
2
2
t
d
p
π
−


=
−
−
Σ
−




x Φ
x
µ
x
µ
Σ
(3.135)
The maximum likelihood estimates of m and Σ can be obtained by a similar procedure.
1
1
1
ˆ
1
ˆ
ˆ
ˆ
ˆ
ˆ
(
)(
)
(
)(
)
n
MLE
k
k
n
t
t
MLE
k
MLE
k
MLE
k
MLE
k
MLE
k
n
E
n
=
=
=


=
−
−
=
−
−




µ
x
Σ
x
µ
x
µ
x
µ
x
µ
(3.136)
Once again, the maximum likelihood estimation for mean vector and co-variance matrix is
the sample mean vector and sample covariance matrix.
In some situations, a maximum likelihood estimation of Φ may not exist, or the
maximum likelihood estimator may not be uniquely defined, i.e., there may be more than
one MLE of Φ for a specific set of sample values. Fortunately, according to Fisher’s theo-
rem, for most practical problems with a well-behaved family of distributions, the MLE ex-
ists and is uniquely defined [4, 25, 26].
In fact, the maximum likelihood estimator can be proven to be sound under certain
conditions. As mentioned before, the estimator
( )
θ X is a function of the vector of random
variables X that represent the sample data.
( )
θ X
itself is also a random variable, with a
distribution determined by joint distributions of X . Let Φ be the parameter vector of true
distribution
( |
)
p x Φ
from which the samples are drawn. If the following three conditions
hold:
1. The sample x is a drawn from the assumed family of distribution,
2. The family of distributions is well behaved,
3. The sample x is large enough,
then maximum likelihood estimator,
MLE
Φ
, has a Gaussian distribution with a mean Φ and
a variance of the form
2
1/ nBx [26], where n is the size of sample and Bx is the Fisher infor-
mation, which is determined solely by Φ and x . An estimator is said to be consistent, iff
the estimate will converge to the true distribution when there is infinite number of training
samples.
MLE
nlim
−>∞
=
Φ
Φ
(3.137)

108
Probability, Statistics, and Information Theory
MLE
Φ
is a consistent estimator based on the analysis above. In addition, it can be
shown that no consistent estimator has a lower variance than
MLE
Φ
. In other words, no es-
timator provides a closer estimate of the true parameters than the maximum likelihood esti-
mator.
3.2.3.
Bayesian Estimation and MAP Estimation
Bayesian estimation has a different philosophy than maximum likelihood estimation. While
MLE assumes that the parameter Φ 3 is fixed but unknown, Bayesian estimation assumes
that the parameter Φ itself is a random variable with a prior distribution
(
)
p Φ . Suppose we
observe a sequence of random samples
{
}
1
2
,
,
,
n
x x
x
=
x

, which are i.i.d. with a p.d.f.
( |
)
p x Φ . According to Bayes’ rule, we have the posterior distribution of Φ as:
( |
) (
)
(
| )
( |
) (
)
( )
p
p
p
p
p
p
Φ
Φ
Φ
=
∝
Φ
Φ
x
x
x
x
(3.138)
In Eq. (3.138), we dropped the denominator
( )
p x
here because it is independent of the
parameter Φ . The distribution in Eq. (3.138) is called the posterior distribution of Φ be-
cause it is the distribution of Φ
after we observed the values of random variables
1
2
,
,
,
n
X
X
X

.
3.2.3.1.
Prior and Posterior Distributions
For mathematical tractability, conjugate priors are often used in Bayesian estimation. Sup-
pose a random sample is taken of a known distribution with p.d.f.
( |
)
p
Φ
x
. A conjugate
prior for the random variable (or vector) is defined as the prior distribution for the parame-
ters of the probability density function of the random variable (or vector), such that the
class-conditional p.d.f.
( |
)
p
Φ
x
, the posterior distribution
(
| )
p Φ x , and the prior distribu-
tion
(
)
p Φ
belong to the same distribution family. For example, it is well known that the
conjugate prior for the mean of a Gaussian p.d.f. is also a Gaussian p.d.f. [4]. Now, let’s
derive such a posterior distribution
(
| )
p Φ x from the widely used Gaussian conjugate prior.
Example
Suppose
1
2
,
,
,
n
X
X
X

are drawn from a Gaussian distribution for which the mean Φ is a
random variable and the variance
2
σ
is known. The likelihood function
( |
)
p
Φ
x
can be
written as:
3 For simplicity, we assume the parameter Φ is a scalar instead of a vector here. However, the extension to a pa-
rameter vector Φ can be derived according to a similar procedure.

Estimation Theory
109
(
)
2
2
/ 2
1
1
1
1
1
( |
)
exp
exp
2
2
2
n
n
i
i
n
n
i
i
x
x
p
σ
σ
π
σ
=
=




−Φ
−Φ




Φ =
−
∝
−








	

	











x
(3.139)
To further simply Eq. (3.139), we could use Eq. (3.140)
2
2
2
1
1
(
)
(
)
(
)
n
n
i
n
i
n
i
i
x
n
x
x
x
=
=
−Φ
=
Φ −
+
−


(3.140)
where
{
}
1
2
1
1
the sample mean of
,
,
,
n
n
i
n
i
x
x
x x
x
n
=
=
=
=

x

.
Let’s rewrite
( |
)
p
Φ
x
in Eq. (3.139) into Eq. (3.141):
(
)
(
)
2
2
2
2
1
1
( |
)
exp
exp
2
2
n
n
i
n
i
n
p
x
x
x
σ
σ
=




Φ ∝
−
Φ −
−
−









x
(3.141)
Now supposed the prior distribution of Φ is also a Gaussian distribution with mean
µ and variance
2
ν , i.e., the prior distribution
(
)
p Φ is given as follows:
(
)
2
2
1/ 2
1
1
1
(
)
exp
exp
2
2
2
p
µ
µ
ν
ν
π
ν




Φ −
Φ −




Φ =
−
∝
−








	

	









(3.142)
By combining Eqs. (3.141) and (3.142) while dropping the second term in Eq. (3.141)
we could attain the posterior p.d.f.
(
| )
p Φ x in the following equation:
(
)
(
)
2
2
2
2
1
1
(
| )
exp
2
n
n
p
x
µ
σ
ν




Φ
∝
−
Φ −
+
Φ −




	



x
(3.143)
Now if we define ρ and τ as follows:
2
2
2
2
n
n
x
n
σ µ
ν
ρ
σ
ν
+
=
+
(3.144)
2
2
2
2
2
n
σ ν
τ
σ
ν
=
+
(3.145)
We can rewrite Eq. (3.143) can be rewritten as:
(
)
(
)
2
2
2
2
2
1
1
(
| )
exp
2
n
n
p
x
n
ρ
µ
τ
σ
ν




Φ
∝
−
Φ −
+
−




+
	



x
(3.146)
Since the second term in Eq. (3.146) does not depend on Φ , it can be absorbed in the
constant factor. Finally, we have the posterior p.d.f. in the following form:

110
Probability, Statistics, and Information Theory
(
)
2
2
1
1
(
| )
exp 2
2
p
ρ
τ
πτ
−


Φ
=
Φ −




x
(3.147)
Equation (3.147) shows that the posterior p.d.f.
(
| )
p Φ x
is a Gaussian distribution
with mean ρ and variance
2
τ
as defined in Eqs. (3.144) and (3.145). The Gaussian prior
distribution defined in Eq. (3.142) is a conjurgate prior.
3.2.3.2.
General Bayesian Estimation
The foremost requirement of a good estimator θ is that it can yield an estimate of Φ
( ( )
θ X ) which is close to the real value Φ . In other words, a good estimator is one for
which it is highly probable that the error
( )
θ
−Φ
X
is close to 0. In general, we can define a
loss function4
( ,
)
R Φ Φ . It measures the loss or cost associated with the fact that the true
value of the parameter is Φ while the estimate is Φ . When only the prior distribution
(
)
p Φ
is available and no sample data has been observed, if we choose one particular esti-
mate Φ , the expected loss is:
( ,
)
( ,
) ( )
E R
R
p
d


Φ Φ
=
Φ Φ
Φ
Φ

 
(3.148)
The fact that we could derive posterior distribution from the likelihood function and
the prior distribution [as shown in the derivation of Eq. (3.147)] is very important here be-
cause it allows us to compute the expected posterior loss after sample vector x is observed.
The expected posterior loss associated with estimate Φ is:
( ,
) |
( ,
) (
| )
E R
R
p
d


Φ Φ
=
Φ Φ
Φ
Φ

 
x
x
(3.149)
The Bayesian estimator of Φ is defined as the estimator that attains minimum Bayes
risk, that is, minimizes the expected posterior loss function (3.149). Formally, the Bayesian
estimator is chosen according to:
[
]
( )
argmin
( , ( )) |
Bayes
E R
θ
θ
θ
=
Φ
x
x
x
(3.150)
The Bayesian estimator of Φ is the estimator
Bayes
θ
for which Eq. (3.150) is satisfied
for every possible value of x of random vector X . Therefore, the form of the Bayesian es-
timator
Bayes
θ
should depend only on the loss function and the prior distribution, but not the
sample value x .
4 The Bayesian estimation and loss function are based on Bayes’ decision theory, which will be described in detail
in Chapter 4.

Estimation Theory
111
One of the most common loss functions used in statistical estimation is the mean
squared error function [20]. The mean squared error function for Bayesian estimation should
have the following form:
2
( , ( ))
(
( ))
R
θ
θ
Φ
= Φ −
x
x
(3.151)
In order to find the Bayesian estimator, we are seeking
Bayes
θ
to minimize the expected pos-
terior loss function:
[
]
2
2
2
( , ( )) |
(
( )) |
(
| )
2 ( ) (
| )
( )
E R
E
E
E
θ
θ
θ
θ


Φ
=
Φ −
=
Φ
−
Φ
−


x
x
x
x
x
x
x
x
(3.152)
The minimum value of this function can be obtained by taking the partial derivative of
Eq. (3.152) with respect to
( )
θ x . Since the above equation is simply a quadratic function of
( )
θ x , it can be shown that the minimum loss can be achieved when
Bayes
θ
is chosen based
on the following equation:
( )
(
| )
Bayes
E
θ
=
Φ
x
x
(3.153)
Equation (3.153) translates into the fact that the Bayesian estimate of the parameter Φ
for mean squared error function is equal to the mean of the posterior distribution of Φ . In
the following section, we discuss another popular loss function (MAP estimation) that also
generates the same estimate for certain distribution functions.
3.2.3.3.
MAP Estimation
One intuitive interpretation of Eq. (3.138) is that a prior p.d.f.
(
)
p Φ
represents the relative
likelihood before the values of
1
2
,
,
,
n
X
X
X

have been observed; while the posterior p.d.f.
(
| )
p Φ x
represents the relative likelihood after the values of
1
2
,
,
,
n
X
X
X

have been ob-
served. Therefore, choosing an estimate Φ that maximizes posterior probability is consis-
tent with out intuition. This estimator is in fact the maximum posterior probability (MAP)
estimator and is the most popular Bayesian estimator.
The loss function associated with the MAP estimator is the so-called uniform loss
function [20]:
0, if | ( )
|
( , ( ))
where
0
1, if | ( )
|
R
θ
θ
θ
−Φ ≤∆

Φ
=
∆>

−Φ > ∆

x
x
x
(3.154)
Now let’s see how this uniform loss function results in MAP estimation. Based on loss
function defined above, the expected posterior loss function is:
( )
( )
( ( , ( )) | )
(| ( )
|
| )
1
(| ( )
|
| )
1
(
| )
E R
P
P
p
θ
θ
θ
θ
θ
+∆
−∆
Φ
=
−Φ > ∆
= −
−Φ ≤∆
= −
Φ

x
x
x
x
x
x
x
x
x
(3.155)

112
Probability, Statistics, and Information Theory
The quantity in Eq. (3.155) is minimized by maximizing the shaded area under
(
| )
p Φ x
over the interval [
]
( )
, ( )
θ
θ
−∆
+ ∆
x
x
in Figure 3.16. If
(
| )
p Φ x
is a smooth curve
and ∆is small enough, the shaded area can be computed roughly as:
( )
( )
( )
(
| )
2
(
| )
p
p
θ
θ
θ
+∆
Φ=
−∆
Φ
≅∆
Φ

x
x
x
x
x |
(3.156)
Thus, the shaded area can be approximately maximized by choosing
( )
θ x
to be the maxi-
mum point of
(
| )
p Φ x . This concludes our proof the using the error function in Eq. (3.154)
indeed will generate MAP estimator.
1 - E( R(Φ,θ(x)) | x )
P( Φ | x )
θ(x) + Λ
θ(x) - Λ
θ(x)
Figure 3.16 Illustration of finding the minimum expected posterior loss function for MAP es-
timation [20].
MAP estimation is to find the parameter estimate
MAP
Φ
or estimator
( )
MAP
θ
x
that
maximizes the posterior probability,
( )
argmax
(
| )
argmax
( |
) (
)
MAP
MAP
p
p
p
θ
Φ
Φ
Φ
=
=
Φ
=
Φ
Φ
x
x
x
(3.157)
MAP
Φ
can also be specified in the logarithm form as follows:
argmax log
( |
)
log
(
)
MAP
p
p
Φ
Φ
=
Φ +
Φ
x
(3.158)
MAP
Φ
can be attained by solving the following partial differential equation:
log
( |
)
log
(
)
0
p
p
∂
Φ
∂
Φ
+
=
∂Φ
∂Φ
x
(3.159)
Thus the MAP equation for finding
MAP
Φ
can be established.
log
( |
)
log
(
)
MAP
MAP
p
p
Φ=Φ
Φ=Φ
∂
Φ
−∂
Φ
=
∂Φ
∂Φ
x
|
|
(3.160)
There are interesting relationships between MAP estimation and MLE estimation. The
prior distribution is viewed as the knowledge of the statistics of the parameters of interest
before any sample data is observed. For the case of MLE, the parameter is assumed to be

Estimation Theory
113
fixed but unknown. That is, there is no preference (knowledge) of what the values of pa-
rameters should be. The prior distribution
(
)
p Φ
can only be set to constant for the entire
parameter space, and this type of prior information is often referred to as non-informative
prior or uniform prior. By substituting
(
)
p Φ
with a uniform distribution in Eq. (3.157),
MAP estimation is identical to MLE. In this case, the parameter estimation is solely deter-
mined by the observed data. A sufficient amount of training data is often a requirement for
MLE. On the other hand, when the size of the training data is limited, the use of the prior
density becomes valuable. If some prior knowledge of the distribution of the parameters can
be obtained, the MAP estimation provides a way of incorporating prior information in the
parameter learning process.
Example
Now, let’s formulate MAP estimation for Gaussian densities. As described in Section
3.2.3.1, the conjugate prior distribution for a Gaussian density is also a Gaussian distribu-
tion. Similarly, we assumed random variables
1
2
,
,
,
n
X
X
X

drawn from a Gaussian distri-
bution for which the mean Φ is unknown and the variance
2
σ
is known, while the conju-
gate prior distribution of Φ is a Gaussian distribution with mean µ and variance
2
ν . It is
shown in Section 3.2.3.1 that the posterior p.d.f. can be formulated as in Eq. (3.147). The
MAP estimation for Φ can be solved by taking the derivative of Eq. (3.147) with respect to
Φ :
2
2
2
2
n
MAP
n
x
n
σ µ
ν
ρ
σ
ν
+
Φ
=
=
+
(3.161)
where n is the total number of training samples and
nx the sample mean.
The MAP estimate of the mean Φ is a weighted average of the sample mean
nx
and
the prior mean. When n is zero (when there is no training data at all), the MAP estimate is
simply the prior mean µ . On the other hand, when n is large ( n →∞), the MAP estimate
will converge to the maximum likelihood estimate. This phenomenon is consistent with our
intuition and is often referred to as asymptotic equivalence or asymptotic convergence.
Therefore, in practice, the difference between MAP estimation and MLE is often insignifi-
cant when a large amount of training data is available. When the prior variance
2
ν
is very
large (e.g.,
2
2 / n
ν
σ
>>
), the MAP estimate will converge to the ML estimate because a
very large
2
ν
translated into a non-informative prior.
It is important to note that the requirement of learning prior distribution for MAP es-
timation is critical. In some cases, the prior distribution is very difficult to estimate and MLE
is still an attractive estimation method. As mentioned before, the MAP estimation frame-
work is particularly useful for dealing with sparse data, such as parameter adaptation. For

114
Probability, Statistics, and Information Theory
example, in speaker adaptation, the speaker-independent (or multiple speakers) database can
be used to first estimate the prior distribution [9]. The model parameters are adapted to a
target speaker through a MAP framework by using limited speaker-specific training data as
discussed in Chapter 9.
3.3.
SIGNIFICANCE TESTING
Significance testing is one of the most important theories and methods of statistical infer-
ence. A problem of statistical inference, or, more simply, a statistics problem, is one in
which data that have been generated in accordance with some unknown probability distribu-
tion must be analyzed, and some type of inference about the unknown distribution must be
made. Hundreds of test procedures have developed in statistics for various kinds of hypothe-
ses testing. We focus only on tests that are used in spoken language systems.
The selection of appropriate models for the data or systems is essential for spoken lan-
guage systems. When the distribution of certain sample data is unknown, it is usually appro-
priate to make some assumptions about the distribution of the data with a distribution func-
tion whose properties are well known. For example, people often use Gaussian distributions
to model the distribution of background noise in spoken language systems. One important
issue is how good our assumptions are, and what the appropriate values of the parameters
for the distributions are, even when we can use the methods in Section 3.2 to estimate
parameters from sample data. Statistical tests are often applied to determine if the
distribution with specific parameters is appropriate to model the sample data. In this section,
we describe the most popular testing method for the goodness of distribution fitting – the
2
χ
goodness-of-fit test.
Another important type of statistical tests is designed to evaluate the excellence of two
different methods or algorithms for the same tasks when there is uncertainty regarding the
results. To assure that the two systems are evaluated on the same or similar conditions, ex-
perimenters often carefully choose similar or even the exactly same data sets for testing.
This is why we refer to this type of statistical test as a paired observations test. In both
speech recognition and speech synthesis, the paired observations test is a very important tool
for interpreting the comparison results.
3.3.1.
Level of Significance
We now consider statistical problems involving a parameter φ whose value is unknown but
must lie in a certain parameter space Ω. In statistical tests, we let
0
H
denote the hypothesis
that
0
φ ∈Ω
and let
1
H denote the hypothesis that
1
φ ∈Ω. The subsets
0
Ω
and
1
Ωare dis-
joint and
0
1
Ω∪Ω= Ω, so exactly one of the hypotheses
0
H
and
1
H
must be true. We
must now decide whether to accept
0
H
or
1
H
by observing a random sample
1,
,
n
X
X

drawn from a distribution involving the unknown parameter φ . A problem like this is called

Significance testing
115
hypotheses testing. A procedure for deciding whether to accept
0
H
or
1
H
is called a test
procedure or simply a test. The hypothesis
0
H
is often referred to as the null hypothesis and
the hypothesis
1
H as the alternative hypothesis. Since there are only two possible decisions,
accepting
0
H
is equivalent to rejecting
1
H and rejecting
0
H
is equivalent to accepting
1
H .
Therefore, in testing hypotheses, we often use the terms accepting or rejecting the null hy-
pothesis
0
H
as the only decision choices.
Usually we are presented with a random sample
1
(
,
,
)
n
X
X
=
X

to help us in making
the test decision. Let S denote the sample space of n-dimensional random vector X. The test-
ing procedure is equivalent to partitioning the sample space S into two subsets. One subset
specifies the values of X for which one will accept
0
H
and the other subset specifies the
values of X for which one will reject
0
H . The second subset is called the critical region and
is often denoted as C.
Since there is uncertainty associated with the test decision, for each value of φ ∈Ω,
we are interested in the probability
( )
ρ φ
that the testing procedure rejects
0
H . The function
( )
ρ φ
is called the power function of the test and can be specified as follows:
( )
(
| )
P
C
ρ φ
φ
=
∈
X
(3.162)
For
0
φ ∈Ω, the decision to reject
0
H
is incorrect. Therefore, if
0
φ ∈Ω,
( )
ρ φ
is the
probability that the statistician will make an incorrect decision (false rejection). In statistical
tests, an upper bound
0
0
(0
1)
α
α
<
<
is specified, and we only consider tests for which
0
( )
ρ φ
α
≤
for every value of
0
φ ∈Ω. The upper bound
0
α
is called the level of signifi-
cance. The smaller
0
α
is, the less likely it is that the test procedure will reject
0
H . Since
0
α
specifies the upper bound for false rejection, once a hypothesis is rejected by the test proce-
dure, we can be
0
(1
)
α
−
confident the decision is correct. In most applications,
0
α is set to
be 0.05 and the test is said to be carried out at the 0.05 level of significance or 0.95 level of
confidence.
We define the size α of a given test as the maximum probability, among all the values
of φ which satisfy the null hypothesis, of making an incorrect decision.
0
max
( )
θ
α
ρ φ
∈Ω
=
(3.163)
Once we obtain the value of α , the test procedure is straightforward. First, the statisti-
cian specifies a certain level of significance
0
α
in a given problem of testing hypotheses,
then he or she rejects the null hypothesis if the size α is such that
0
α
α
≤
.
The size α of a given test is also called the tail area or the p-value corresponding to
the observed value of data sample X because it corresponds to tail area of the distribution.
The hypothesis will be rejected if the level of significance
0
α
is such that
0
α
α
>
and
should be accepted for any value of
0
α
α
<
. Alternatively, we can say the observed value of

116
Probability, Statistics, and Information Theory
X is just significant at the level of significance α without using the level of significance
0
α . Therefore, if we had found that the observed value of one data sample X was just sig-
nificant at the level of 0.0001, while the other observed value of data sample Y was just sig-
nificant at the level of 0.001, then we can conclude the sample X provides much stronger
evidence against
0
H . In statistics, an observed value of one data sample X is generally said
to be statistically significant if the corresponding tail area is smaller than the traditional
value 0.05. For case requiring more significance (confidence), 0.01 can be used.
A statistically significant observed data sample X that provides strong evidence
against
0
H
does not necessary provide strong evidence that the actual value of φ is signifi-
cantly far away from parameter set
0
Ω. This situation can arise, particularly when the size
of random data sample is large, because a test with larger sample size will in general reject
hypotheses with more confidence, unless the hypothesis is indeed the true one.
3.3.2.
Normal Test (Z-Test)
Suppose we need to find whether a coin is fair or not. Let p be the probability of the head.
The hypotheses to be tested are as follows:
0
H
:
12
p =
1
H :
12
p ≠
We assume that a random sample size n is taken, and let random variable M denote the
number of times we observe heads as the result. The random variable M has a binomial dis-
tribution
12
( ,
)
B n
. Because of the shape of binomial distribution, M can lie on either side of
the mean. This is why it is called a typical two-tailed test. The tail area or p-value for the
observed value k can be computed as:
2 (
)
for
2
2 (0
)
for
2
1.0
for
2
P k
M
n
k
n
p
P
M
k
k
n
k
n
≤
≤
>


=
≤
≤
<


=

(3.164)
The p-value in Eq. (3.164) can be computed directly using the binomial distribution.
The test procedure will reject
0
H
when p is less than the significance level
0
α .
In many situations, the p-value for the distribution of data sample X is difficult to ob-
tain due to the complexity of the distribution. Fortunately, if some statistic Z of the data
sample X has some well-known distribution, the test can then be done in the Z domain in-
stead. If n is large enough (
50
n >
), a normal test (or Z-test) can be used to approximate a
binomial probability. Under
0
H , the mean and variance for M are
(
)
2
E M
n
=
and
(
)
4
Var M
n
=
. The new random variable Z is defined as,
|
2 | 1 2
4
M
n
Z
n
−
−
=
(3.165)

Significance testing
117
which can be approximated as standard Gaussian distribution
(0,1)
N
under
0
H . The p-
value can now be computed as
2 (
)
p
P Z
z
=
≥
where z is the realized value of Z after M is
observed. Thus,
0
H
is rejected if
0
p
α
<
, where
0
α
is the level of significance.
3.3.3.
2
χ Goodness-of-Fit Test
The normal test (Z-test) can be extended to test the hypothesis that a given set of data came
from a certain distribution with all parameters specified. First let’s look at the case of dis-
crete distribution fitting.
Suppose that a large population consists of items of k different types and let
ip be the
probability that a random selected item belongs to type i. Now, let
1,
,
k
q
q

be a set of spe-
cific numbers satisfying the probabilistic constraint (
1
0 for
1,
,
and
1
k
i
i
i
q
i
k
q
=
≥
=
=


).
Finally, suppose that the following hypotheses are to be tested:
0
H
:
for
1,
,
i
i
p
q
i
k
=
=

1
H :
for at least one value of
i
i
p
q
i
≠
Assume that a random sample of size n is to be taken from the given population. For
1,
,
i
k
=

, let
i
N denote the number of observations in the random sample which are of
type i. Here,
1,
,
k
N
N

are nonnegative numbers and
1
k
i
i N
n
=
=

. Random variables
1,
,
k
N
N

have a multinomial distribution. Since the p-value for the multinomial distribu-
tion is hard to obtain, instead we use another statistic about
1,
,
k
N
N

. When
0
H
is true, the
expected number of observations of type i is
i
nq . In other words, the difference between the
actual number of observations
i
N and the expected number
i
nq should be small when
0
H
is true. It seems reasonable to base the test on the differences
i
i
N
nq
−
and to reject
0
H
when the differences are large. It can be proved [14] that the following random variable λ
2
1
(
)
k
i
i
i
i
N
nq
nq
λ
=
−
= 
(3.166)
converges to the
2
χ
distribution with
1
k −
degrees of freedom as the sample size n →∞.
A
2
χ
test of goodness-of-fit can be carried out in the following way. Once a level of
significance
0
α
is specified, we can use the following p-value function to find critical point
c5:
2
0
(
)
1
(
)
P
c
F
x
c
χ
λ
α
>
= −
=
=
(3.167)
5 Since
2
χ
pdf is a monotonic function, the test is a one-tail test. Thus, we only need to calculate one tail area.

118
Probability, Statistics, and Information Theory
where
2 ( )
F
x
χ
is the distribution function for
2
χ
distribution. The test procedure simply
rejects
0
H
when the realized value λ is such that
c
λ >
. Empirical results show that the
2
χ
distribution will be a good approximation to the actual distribution of λ as long as the value
of each expectation
i
nq is not too small (
5
≥
). The approximation should still be satisfac-
tory if
1.5 for
1,
,
i
nq
i
k
≥
=

.
For continuous distributions, a modified
2
χ
goodness-of-fit test procedure can be ap-
plied. Suppose that we would like to hypothesize a null hypothesis
0
H
in which continuous
random sample data
1,
,
k
X
X

are drawn from a certain continuous distribution with all
parameters specified or estimated. Also, suppose the observed values of random sample
1,
,
k
x
x

are bounded within interval Ω. First, we divide the range of the hypothesized
distribution into m subintervals within interval Ωsuch that the expected number of values,
say
iE , in each interval is at least 5. For
1,
,
i
k
=

, we let
i
N denote the number of obser-
vations in the ith subintervals. As in Eq. (3.166), one can prove that the following random
variable λ
2
1
(
)
m
i
i
i
i
N
E
E
λ
=
−
= 
(3.168)
converges to the
2
χ
distribution with
1
m
k
−
−
degrees of freedom as the sample size
n →∞, where k is the number of parameters that must be estimated from the sample data in
order to calculate the expected number of values,
iE . Once the
2
χ
distribution is estab-
lished, the same procedure can be used to find the critical c in Eq. (3.167) to make test deci-
sion.
Example
Suppose we are given a random variable X of sample size 100 points and we want to deter-
mine whether we can reject the following hypothesis:
0 :
~
(0,1)
H
X
N
(3.169)
To perform
2
χ
goodness-of-fit test, we first divide the range of X into 10 subintervals.
The corresponding probability falling in each subinterval, the expected number of points
falling in each subinterval and the actual number of points falling in each subintervals [10]
are illustrated in Table 3.1.
Table 3.1 The probability falling in each subinterval of an N(0,1), and 100 sample points, the
expected number of points falling in each subinterval, and the actual number of points falling
in each subintervals [10].
Subinterval
iI
(
)
i
P X
I
∈
100 (
)
i
i
E
P X
I
=
∈
i
N
[- , -1.6]
∞
0.0548
5.48
2

Significance testing
119
[-1.6, -1.2]
0.0603
6.03
9
[-1.2, -0.8]
0.0968
9.68
6
[-0.8, -0.4]
0.1327
13.27
11
[-0.4, 0.0]
0.1554
15.54
19
[0.0, 0.4]
0.1554
15.54
25
[0.4, 0.8]
0.1327
13.27
17
[0.8, 1.2]
0.0968
9.68
2
[1.2, 1.6]
0.0603
6.03
6
[-1.6,
]
∞
0.0548
5.48
3
The value for λ can then be calculated as follows:
2
1
(
)
18.286
m
i
i
i
i
N
E
E
λ
=
−
=
=

Since λ can be approximated as a
2
χ
distribution with
1
10
0
1
9
m
k
−
−=
−
−=
de-
grees of freedom, the critical point c at the 0.05 level of significance is calculated6 to be
16.919 according to Eq. (3.167). Thus, we should reject the hypothesis
0
H
because the cal-
culated λ is greater than the critical point c.
The
2
χ
goodness-of-fit test at the 0.05 significance level is in general used to deter-
mine when a hypothesized distribution is not an adequate distribution to use. To accept the
distribution as a good fit, one needs to make sure the hypothesized distribution cannot be
rejected at the 0.4 to 0.5 level-of-significance. The alternative is to use the
2
χ
goodness-of-
fit test for a number of potential distributions and select the one with smallest calculated
2
χ .
When all the parameters are specified (instead of estimated), the Kolmogorov-
Smirnov test [5] can also be used for the goodness-of-fit test. The Kolmogorov-Smirnov test
in general is a more powerful test procedure when the sample size is relatively small.
3.3.4.
Matched-Pairs Test
In this section, we discuss experiments in which two different methods (or systems) are to
be compared to learn which one is better. To assure the two methods are evaluated under
similar conditions, two closely resemble data samples or ideally the same data sample
should be used to evaluate both methods. This type of hypotheses test is called matched-
paired test [5].
6 In general, we use cumulative distribution function table to find the point with specific desired cumulative prob-
ability for complicated distributions, like
2
χ
distribution.

120
Probability, Statistics, and Information Theory
3.3.4.1.
The Sign Test
For
1,
,
i
n
=

, let
ip denote the probability that method A is better than method B when
testing on the ith paired data sample. We shall assume that the probability
ip has the same
value p for each of the n pairs. Suppose we wish to test the null hypothesis that method A is
no better than method B. That is, the hypotheses to be tested have the following form:
0
H
:
12
p ≤
1
H :
12
p >
Suppose that, for each pair of data samples, either one method or the other will appear
to be better, and the two methods cannot tie. Under these assumptions, the n pairs represent
n Bernoulli trials, for each of which there is probability p that method A yields better per-
formance. Thus the number of pairs M in which method A yields better performance will
have a binomial distribution
( , )
B n p . For the simple sign test where one needs to decide
which method is better, p will be set to 1 2 . Hence a reasonable procedure is to reject
0
H
if
M
c
>
, where c is a critical point. This procedure is called a signed test. The critical point
can be found according to.
0
(
)
1
(
)
B
P M
c
F
x
c
α
>
= −
=
=
(3.170)
where
( )
B
F
x
is the distribution for binomial distribution. Thus, for observed value M
c
>
,
we will reject
0
H .
3.3.4.2.
Magnitude-Difference Test
The only information that the sign test utilizes from each pair of data samples, is the sign of
the difference between two performances. To do a sign test, one does not need to obtain a
numeric measurement of the magnitude of the difference between the two performances.
However, if the measurement of magnitude of the difference for each pair is available, a test
procedure based on the relative magnitudes of the differences can be used [11].
We assume now that the performance of each method can be measured for any data
samples. For
1,
,
i
n
=

, let
iA denote the performance of the method A on the ith pair of
data samples and
iB denote the performance of the method B on the ith pair of data sample.
Moreover, we shall let
i
i
i
D
A
B
=
−
. Since
1,
,
n
D
D

are generated on n different pairs of
data samples, they should be independent random variables. We also assume that
1,
,
n
D
D

have the same distribution. Suppose now we are interested in testing the null hypothesis that
method A and method B have on the average the same performance on the n pairs of data
samples.
Let
D
µ
be the mean of
i
D . The MLE estimate of
D
µ
is:
1
n
i
D
i
D
n
µ
=
= 
(3.171)

Information Theory
121
The test hypotheses are:
0
H
:
0
D
µ
=
1
H :
0
D
µ
≠
The MLE estimate of the variance of
i
D is
2
2
1
1
(
)
n
D
i
D
i
D
n
σ
µ
=
=
−

(3.172)
We define a new random variable Z as follows:
D
D
Z
n
µ
σ
=
(3.173)
If n is large enough (> 50), Z is proved to have a standard Gaussian distribution
(0,1)
N
. The normal test procedure described in Section 3.3.2 can be used to test
0
H . This
type of matched-paired tests usually depends on having enough pairs of data samples for the
assumption that Z can be approximated with a Gaussian distribution. It also requires enough
data samples to estimate the mean and variance of the
i
D .
3.4.
INFORMATION THEORY
Transmission of information is a general definition of what we call communication. Claude
Shannon’s classic paper of 1948 gave birth to a new field in information theory that has be-
come the cornerstone for coding and digital communication. In the paper titled A Mathe-
matical Theory of Communication, he wrote:
The fundamental problem of communication is that of re-
producing at one point either exactly or approximately a 
message selected at another point. 
Information theory is a mathematical framework of approaching a large class of problems
related to encoding, transmission, and decoding information in a systematic and disciplined
way. Since speech is a form of communication, information theory has served as the under-
lying mathematical foundation for spoken language processing.
3.4.1.
Entropy
Three interpretations can be used to describe the quantity of information: (1) the amount of
uncertainty before seeing an event, (2) the amount of surprise when seeing an event, and (3)
the amount of information after seeing an event. Although these three interpretations seem
slightly different, they are virtually the same under the framework of information theory.

122
Probability, Statistics, and Information Theory
According to information theory, the information derivable from outcome xi depends
on its probability. If the probability
(
)
i
P x
is small, we can derive a large degree of informa-
tion, because the outcome that it has occurred is very rare. On the other hand, if the prob-
ability is large, the information derived will be small, because the outcome is well expected.
Thus, the amount of information is defined as follows:
1
(
)
log
(
)
i
i
I x
P x
=
(3.174)
The reason to use a logarithm can be interpreted as follows. The information for two
independent events to occur (where the joint probability is the multiplication of both indi-
vidual probabilities) can be simply carried out by the addition of the individual information
of each event. When the logarithm base is 2, the unit of information is called a bit. This
means that one bit of information is required to specify the outcome. In this probabilistic
framework, the amount of information represents uncertainty. Suppose X is a discrete ran-
dom variable taking value xi (referred to as a symbol) from a finite or countable infinite sam-
ple space
1
2
{ ,
,
,
,
}
i
S
x x
x
=

 (referred to as an alphabet). The symbol xi is produced from
an information source with alphabet S, according to the probability distribution of the
random variable X. One of the most important properties of an information source is the
entropy H(S) of the random variable X, defined as the average amount of information (ex-
pected information):
[
]
1
(
)
[ (
)]
(
) (
)
(
)log
log
(
)
(
)
i
i
i
S
S
i
H X
E I X
P x I x
P x
E
P X
P x
=
=
=
=
−


(3.175)
This entropy
(
)
H X
is the amount of information required to specify what kind of
symbol has occurred on average. It is also the averaged uncertainty for the symbol. Suppose
that the sample space S has an alphabet size S
N
=
. The entropy
(
)
H X
attains the maxi-
mum value when the p.f. has a uniform distribution, i.e.:
1
(
)
(
)
for all
and
i
j
P x
P x
i
j
N
=
=
(3.176)
Equation (3.176) can be interpreted to mean that uncertainty reaches its maximum
level when no outcome is more probable than any other. It can be proved that the entropy
(
)
H X
is nonnegative and becomes zero only if the probability function is a deterministic
one, i.e.,
(
)
0 with equality i.f.f.
(
)
1 for some
i
i
H X
P x
x
S
≥
=
∈
(3.177)
There is another very interesting property for the entropy. If we replace the p.f. of
generating symbol xi in Eq. (3.175) with any other arbitrary p.f., the new value is no smaller
than the original entropy. That is,

Information Theory
123
[
]
(
)
log
(
)
(
)log
(
)
i
i
S
H X
E
Q X
P x
Q x
≤
−
= −
(3.178)
Equation (3.178) has a very important meaning. It shows that we are more uncertain
about the data if we misestimate the distribution governing the data source. The equality for
Eq. (3.178) occurs if and only if
(
)
(
) 1
i
i
P x
Q x
i
N
=
≤≤
. Equation (3.178), often referred to
as Jensen’s inequality, is the basis for the proof of EM algorithm in Chapter 4. Similarly,
Jensen’s ineqality can be extended to continuous pdf:
( )log
( )
( )log
( )
x
x
x
x
f
x
f
x dx
g
x
f
x dx
−
≤−


(3.179)
with equality occurring if and only if
( )
( )
x
x
f
x
g
x
x
=
∀.
The proof of Eq. (3.178) follows from the fact log( )
1,
x
x
x
≤
−
∀, so the following
quantity must have an non-positive value.
(
)
(
)
(
)log
(
) log
1
0
(
)
(
)
i
i
i
i
S
S
i
i
Q x
Q x
P x
P x
P x
P x


≤
−
=






(3.180)
Based on this property, the negation of the quantity in Eq. (3.180) can be used for the
measurement of the distance of two probability distributions. Specifically, the Kullback-
Leibler (KL) distance (relative entropy, discrimination, or divergence) is defined as:
(
)
(
)
(
||
)
log
(
)log
(
)
(
)
i
i
S
i
P x
P X
KL P Q
E
P x
Q X
Q x


=
=



 
(3.181)
As discussed in Chapter 11, the branching factor of a grammar or language is an im-
portant measure of degree of difficulty of a particular task in spoken language systems. This
relates to the size of the word list from which a speech recognizer or a natural language
processor needs to disambiguate in a given context. According to the entropy definition
above, this branching factor estimate (or average choices for an alphabet) is defined as fol-
lows:
(
)
(
)
2H X
PP X =
(3.182)
(
)
PP X
is called the perplexity of source X, since it describes how confusing the
grammar (or language) is. The value of perplexity is equivalent to the size of an imaginary
equivalent list, whose words are equally probable. The bigger the perplexity, the higher
branching factor. To find out the perplexity of English, Shannon devised an ingenious way
[22] to estimate the entropy and perplexity of English words and letters. His method is simi-
lar to a guessing game where a human subject guesses sequentially the words of a text hid-
den from him, using the relative frequencies of her/his guesses as the estimates of the prob-
ability distribution underlying the source of the text. Shannon’s perplexity estimate of Eng-
lish comes out to be about 2.39 for English letters and 130 for English words. Chapter 11
has a detailed description on the use of perplexity for language modeling.

124
Probability, Statistics, and Information Theory
3.4.2.
Conditional Entropy
Now let us consider transmission of symbols through an information channel. Suppose the
input alphabet is
1
2
(
,
,
,
)
s
X
x x
x
=

, the output alphabet is
1
2
(
,
,
,
)
t
Y
y y
y
=

, and the in-
formation channel is defined by the channel matrix
(
|
)
ij
j
i
M
P y
x
=
, where
(
|
)
j
i
P y
x
is the
conditional probability of receiving output symbol yj when input symbol xi is sent. Figure
3.17 shows an example of an information channel.
Figure 3.17 Example of information channel. The source is described by source p.f. P(X) and
the channel is characterized by the conditional p.f. P(Y|X).
Before transmission, the average amount of information, or the uncertainty of the in-
put alphabet X, is the prior entropy H(X).
1
(
)
(
)log
(
)
i
X
i
H X
P X
x
P X
x
=
=
=

(3.183)
where
(
)
i
P x
is the prior probability. After transmission, suppose yj is received; then the
average amount of information, or the uncertainty of the input alphabet A, is reduced to the
following posterior entropy.
(
|
)
(
|
)log
(
|
)
j
i
j
i
j
X
H X Y
y
P X
x
Y
y
P X
x
Y
y
=
= −
=
=
=
=

(3.184)
where the
(
|
)
i
j
P x
y
are the posterior probabilities. Averaging the posterior entropy
(
|
)
j
H X
y
over all output symbols yj leads to the following equation:
(
|
)
(
)
(
|
)
(
)
(
|
)log
(
|
)
(
,
)log
(
|
)
j
j
Y
j
i
j
i
j
Y
X
i
j
i
j
X
Y
H X Y
P Y
y
H X Y
y
P Y
y
P X
x
Y
y
P X
x
Y
y
P X
x Y
y
P X
x
Y
y
=
=
=
= −
=
=
=
=
=
= −
=
=
=
=




(3.185)
This conditional entropy, defined in Eq. (3.185), is the average amount of information
or the uncertainty of the input alphabet X given the outcome of the output event Y. Based on
the definition of conditional entropy, we derive the following equation:
P(X)
P(Y|X)
X
Y
Source
Channel

Information Theory
125
{
}
(
, )
(
,
)log
(
,
)
=
(
,
) log
(
)
log
(
|
)
=
(
)
(
|
)
i
i
i
i
X
Y
i
i
i
i
i
X
Y
H X Y
P X
x Y
y
P X
x Y
y
P X
x Y
y
P X
x
P Y
y
X
x
H X
H Y
X
= −
=
=
=
=
−
=
=
=
+
=
=
+


(3.186)
Equation (3.186) has an intuitive meaning – the uncertainty about two random vari-
ables equals the sum of uncertainty about the first variable and the conditional entropy for
the second variable given the first variable is known. Equations (3.185) and (3.186) can be
generalized to random vectors X and Y where each contains several random variables.
It can be proved that the chain rule [Eq. (3.16)] applies to entropy.
1
1
1
2
1
1
(
,
,
)
(
|
,
,
)
(
|
)
(
)
n
n
n
H X
X
H X
X
X
H X
X
H X
−
=
+
+
+



(3.187)
Finally, the following inequality can also be proved:
(
|
,
)
(
|
)
H X Y Z
H X Y
≤
(3.188)
with equality i.f.f. X and Z being independent when conditioned on Y. Equation (3.188) ba-
sically confirms the intuitive belief that uncertainty decreases when more information is
known.
3.4.3.
The Source Coding Theorem
Information theory is the foundation for data compressing. In this section we describe Shan-
non's source coding theorem, also known as the first coding theorem. In source coding, we
are interested in lossless compression, which means the compressed information (or sym-
bols) can be recovered (decoded) perfectly. The entropy serves as the upper bound for a
source lossless compression.
Consider an information source with alphabet
{
}
0,1,
,
1
S
N
=
−

. The goal of data
compression is to encode the output symbols into a string of binary symbols. An interesting
question arises: What is the minimum number of bits required, on the average, to encode the
output symbols of the information source?
Let’s assume we have a source that can emit four symbols {0,1,2,3} with equal prob-
ability
(0)
(1)
(2)
(3)
1/ 4
P
P
P
P
=
=
=
=
. Its entropy is 2 bits as illustrated in Eq. (3.189):
3
2
0
1
( )
( )log
2
( )
i
H S
P i
P i
=
=
=

(3.189)
It is obvious that 2 bits per symbol is good enough to encode this source. A possible
binary code for this source is {00, 01, 10, 11}. It could happen, though some symbols are
more likely than others, for example,
(0)
1/ 2,
(1)
1/ 4,
(2)
1/8,
(3)
1/8
P
P
P
P
=
=
=
=
. In
this case the entropy is only 1.75 bits. One obvious idea is to use fewer bits for lower values
that are frequently used and more bits for larger values that are rarely used. To represent this

126
Probability, Statistics, and Information Theory
source we can use a variable-length code {0,10,110,111}, where no codeword is a prefix for
the rest and thus a string of 0’s and 1’s can be uniquely broken into those symbols. The en-
coding scheme with such a property is called uniquely decipherable (or instantaneous) cod-
ing, because as soon as the decoder observes a sequence of codes, it can decisively deter-
mine the sequence of the original symbols. If we let
( )
r x
be the number of bits (length)
used to encode symbol x, the average rate R of bits per symbol used for encoding the infor-
mation source is:
( ) ( )
x
R
r x P x
= 
(3.190)
In our case, R is 1.75 bits as shown in Eq. (3.191):
0.5 1
0.25 2
0.125 3
0.125 3
1.75
R =
× +
×
+
× +
×
=
(3.191)
Such variable-length coding strategy is called Huffman coding. Huffman coding be-
longs to entropy coding because it matches the entropy of the source. In general, Shannon’s
source coding theorem says that a source cannot be coded with fewer bits than its entropy.
We will skip the proof here. Interested readers can refer to [3, 15, 17] for the detailed proof.
This theorem is consistent with our intuition because the entropy measure is exactly the in-
formation content of the information measured in bits. If the entropy increases, then uncer-
tainty increases, resulting in a large amount of information. Therefore, it takes more bits to
encode the symbols. In the case above, we are able to match this rate, but, in general, this is
impossible, though we can get arbitrarily close to it. The Huffman code for this source offers
a compression rate of 12.5% relative to the code designed for the uniform distribution.
Shannon's source coding theorem establishes not only the lower bound for lossless
compression but also the upper bound. Let
x
 
  denote the smallest integer that greater or
equal to x. As in the similar procedure above, we can make the code length assigned to
source output x equal to
( )
log
( )
l x
P x
= −




(3.192)
The average length L satisfies the following inequality:
[
]
( ) ( )
1 log
( )
( )
1
(
)
x
x
L
l x P x
P x
P x
H X
=
<
−
= +


(3.193)
Equation (3.193) means that the average rate R only exceeds the value of entropy by less
than one bit.
L can be made arbitrary close to the entropy by block coding. Instead of encoding sin-
gle output symbols of the information source, one can encode each block of length n. Let’s
assume the source is memoryless, so
1
2
,
,
,
n
X
X
X

are independent. According to Eq.
(3.193), the average rate R for this block code satisfies:
1
2
1
(
,
,
,
)
1
(
)
n
L
H X
X
X
nH X
< +
= +

(3.194)
This makes the average number of bits per output symbol,
/
L n , satisfy

Information Theory
127
1
lim
(
)
n
L
H X
n
→∞
≤
(3.195)
In general, Huffman coding arranges the symbols in order of decreasing probability,
assigns the bit 0 to the symbol of highest probability and the bit 1 to what is left, and pro-
ceeds the same way for the second highest probability value (which now has a code 10) and
iterate. This results in 2.25 bits for the uniform distribution case, which is higher than the 2
bits we obtain with equal-length codes.
Lempel-Ziv coding is a coding strategy that uses correlation to encode strings of sym-
bols that occur frequently. Although it can be proved to converge to the entropy, its conver-
gence rate is much slower [27]. Unlike Huffman coding, Lempel-Ziv coding is independent
of the distribution of the source; i.e., it needs not be aware of the distribution of the source
before encoding. This type of coding scheme is often referred to as universal encoding
scheme.
3.4.4.
Mutual Information and Channel Coding
Let’s review the information channel illustrated in Figure 3.17. An intuitively plausible
measure of the average amount of information provided by the random event Y about the
random event X is the average difference between the number of bits it takes to specify the
outcome of X when the outcome of Y is not known and the outcome of Y is known. Mutual
information is defined as the difference in the entropy of X and the conditional entropy of X
given Y:
(
; )
(
)
(
|
)
1
1
(
)log
( ,
)log
(
)
(
|
)
(
|
)
( ,
)
( ,
)log
( ,
)log
( )
( ) (
)
(
, )
log
(
) ( )
i
i
j
X
X
Y
i
i
j
i
j
i
j
i
j
i
j
X
Y
X
Y
i
i
i
I X Y
H X
H X Y
P x
P x y
P x
P x
y
P x
y
P x y
P x y
P x y
P x
P x P y
P X Y
E
P X P Y
=
−
=
−
=
=


=








(3.196)
(
; )
I X Y
is referred to as the mutual information between X and Y.
(
; )
I X Y
is sym-
metrical; i.e.,
(
; )
( ;
)
I X Y
I Y X
=
. The quantity
( , )
( ) ( )
P x y
P x P y
is often referred to as the
mutual information between symbol x and y.
(
; )
I X Y
is bounded:
[
]
0
(
; )
min
(
),
( )
I X Y
H X
H Y
≤
≤
(3.197)
(
; )
I X Y
reaches the minimum value (zero) when the random variables X and Y are in-
dependent.
Mutual information represents the information obtained (or the reduction in uncer-
tainty) through a channel by observing an output symbol. If the information channel is

128
Probability, Statistics, and Information Theory
noiseless, the input symbol can be determined definitely by observing an output symbol. In
this case, the conditional entropy H(X|Y) equals zero and it is called a noiseless channel. We
obtain the maximum mutual information I(X; Y) = H(X). However, the information channel
is generally noisy so that the conditional entropy H(X|Y) is not zero. Therefore, maximizing
the mutual information is equivalent to obtaining a low-noise information channel, which
offers a closer relationship between input and output symbols.
Figure 3.18 A binary channel with two symbols.
Let’s assume that we have a binary channel, a channel with a binary input and output.
Associated with each output are a probability p that the output is correct, and a probability
(1
)
p
−
that it is not, so that the channel is symmetric.
If we observe a symbol Y = 1 at the output, we don’t know for sure what symbol X
was transmitted, though we know
(
1|
1)
P X
Y
p
=
=
=
and
(
0 |
1)
(1
)
P X
Y
p
=
=
=
−
, so
that we can measure our uncertainty about X by its conditional entropy:
(
|
1)
log
(1
)log(1
)
H X Y
p
p
p
p
=
= −
−
−
−
(3.198)
If we assume that our source X has a uniform distribution,
(
|
)
(
|
1)
H X Y
H X Y
=
=
as
shown in Eq. (3.198) and H(X) = 1. The mutual information between X and Y is given by
(
, )
(
)
(
|
)
1
log
(1
)log(1
)
I X Y
H X
H X Y
p
p
p
p
=
−
= +
+
−
−
(3.199)
It measures the information that Y carries by about X. The channel capacity C is the maxi-
mum of the mutual information over all distributions of X. That is,
( )
max (
; )
P x
C
I X Y
=
(3.200)
The channel capacity C can be attained by varying the distribution of the information
source until the mutual information is maximized for the channel. The channel capacity C
can be regarded as a channel that can transmit at most C bits of information per unit of time.
Shannon’s channel coding theorem says that for a given channel there exists a code that
permits error-free transmission across the channel, provided that R
C
≤
, where R is the rate
of the communication system, which is defined as the number of bits per unit of time being
transmitted by the communication system. Shannon’s channel coding theorem states the fact
that arbitrarily reliable communication is possible at any rate below channel capability.
Figure 3.19 illustrates a transmission channel with the source decoder and destination
decoder. The source encoder will encode the source symbol sequence
1
2
,
,
,
n
x x
x
=
x

into
channel input sequence
1
2
,
,
,
k
y y
y

. The destination decoder takes the output sequence
1
2
,
,
,
k
z z
z

from the channel and converts it into the estimates of the source output
X=0
X=1
Y=0
Y=1
p
p
1- p
1- p

Historical Perspective and Further Reading
129
1
2
,
,
,
n
x x
x
=
x

. The goal of this transmission is to make the probability of correct decoding
(
)
P
=
x
x
asymptotically close to 1 while keeping the compression ratio
n k
ℜ=
as large as
possible. Shannon’s source-channel coding theorem (also referred to as Shannon’s second
coding theorem) says that it is possible to find an encoder-decoder pair of rate ℜfor a noisy
information channel, provided that
(
)
H X
C
ℜ×
≤
.
Figure 3.19 Transmission of information through a noisy channel [15].
Because of channel errors, speech coders need to provide error correction codes that
will decrease the bit rate allocated to the speech. In practice, there is a tradeoff between the
bit rate used for source coding and the bit rate for channel coding. In Chapter 7 we will de-
scribe speech coding in great detail.
3.5.
HISTORICAL PERSPECTIVE AND FURTHER READING
The idea of uncertainty and probability can be traced all the way back to about 3500 B.C.,
when games of chance played with bone objects were developed in Egypt. Cubical dice with
markings virtually identical to modern dice have been found in Egyptian tombs dating in
around 2000 B.C. Gambling with dice played an important part in the early development of
probability theory. Modern mathematical theory of probability is believed to have been
started by the French mathematicians Blaise Pascal (1623-1662) and Pierre Fermat (1601-
1665) when they worked on certain gambling problems involving dice. English mathemati-
cian Thomas Bayes (1702-1761) was first to use probability inductively and established a
mathematical basis for probability inference, leading to what is now known as Bayes' theo-
rem. The theory of probability has developed steadily since then and has been widely ap-
plied in diverse fields of study. There are many good textbooks on probability theory. The
,
,
,
x x
x

Source
P(x)
Encoder
Channel
P(z|y)
Encoder
,
,
,
y y
y

,
,
,
z z
z

,
,
,
x x
x


130
Probability, Statistics, and Information Theory
book by DeGroot [6] is an excellent textbook for both probability and statistics which covers
all the necessary elements for engineering majors. The authors also recommend [14], [19],
or [24] for interested readers.
Estimation theory is a basic subject in statistics covered in textbooks. The books by
DeGroot [6], Wilks [26] and Hoel [13] offer excellent discussions of estimation theory.
They all include comprehensive treatments for maximum likelihood estimation and Bayes-
ian estimation. Maximum likelihood estimation was introduced by in 1912 R. A. Fisher
(1890-1962) and has been applied to various domains. It is arguably the most popular pa-
rameter estimation method due to its intuitive appeal and excellent performance with large
training samples. The EM algorithm in Chapter 4 and the estimation of hidden Markov
models in Chapter 8 are based on the principle of MLE. The use of prior distribution in
Bayesian estimation is very controversial in statistics. Some statisticians adhere to the
Bayesian philosophy of statistics by taking the Bayesian estimation' view of the parameter
Φ having a probability distribution. Others, however, believe that in many problems Φ is
not a random variable but rather a fixed number whose value is unknown. Those statisticians
believe that a prior distribution can be assigned to a parameter Φ only when there is exten-
sive prior knowledge of the past; thus the non-informative priors are completely ruled out.
Both groups of statisticians agree that whenever a meaningful prior distribution can be ob-
tained, the theory of Bayesian estimation is applicable and useful. The books by DeGroot [6]
and Poor[20] are excellent for learning the basics of Bayesian and MAP estimations. Bayes-
ian and MAP adaptation are particularly powerful when the training samples are sparse.
Therefore, they are often used for adaptation where the knowledge of prior distribution can
help to adapt the model to a new but limited training set. The speaker adaptation work done
by Brown et al. [2] first applied Bayesian estimation to speech recognition and [9] is another
good paper on using MAP for hidden Markov models. References [4], [16] and [14] have
extensive studies of different conjugate prior distributions for various standard distributions.
Finally, [1] has an extensive reference for Bayesian estimation.
Significance testing is an essential tool for statisticians to interpret all the statistical
experiments. Neyman and Person provided some of the most important pioneering work in
hypotheses testing [18]. There are many different testing methods presented in most statis-
tics book. The
2
χ
test, invented in 1900 by Karl Pearson, is arguably the most widely used
testing method. Again, the textbook by DeGroot [6] is an excellent source for the basics of
testing and various testing methods. The authors recommend [7] as an interesting book that
uses many real-world examples to explain statistical theories and methods, particularly the
significance testing.
Information theory first appeared in Claude Shannon's historical paper: A Mathemati-
cal Theory of Communication [21]. In it, Shannon, analyzed communication as the transmis-
sion of a message from a source through a channel to a receiver. In order to solve the prob-
lem he created a new branch of applied mathematics - information and coding theory. IEEE
published a collection of Shannon's papers [23] containing all of his published works, as
well as many that have never been published. Those published include his classic papers on
information theory and switching theory. Among the unpublished works are his once-secret
wartime reports, his Ph.D. thesis on population genetics, unpublished Bell Labs memoranda,
and a paper on the theory of juggling. The textbook by McEliece [17] is excellent for learn-

Historical Perspective and Further Reading
131
ing all theoretical aspects of information and coding theory. However, it might be out of
print now. Instead, the books by Hamming [12] and Cover [3] are two current great refer-
ences for information and coding theory. Finally, F. Jelinek's Statistical Methods for Speech
Recognition [15] approaches the speech recognition problem from an information-theoretic
aspect. It is a useful book for people interested in both topics.
REFERENCES
[1]
Bernardo, J.M. and A.F.M. Smith, Bayesian Theory, 1996, New York, John Wiley.
[2]
Brown, P., C.-H. Lee, and J. Spohrer, "Bayesian Adaptation in Speech Recogni-
tion," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing,
1983, Boston, MA pp. 761-764.
[3]
Cover, T.M. and J.A. Thomas, Elements of Information Theory, 1991, New York,
John Wiley and Sons.
[4]
DeGroot, M.H., Optimal Statistical Decisions, 1970, New York, NY, McGraw-
Hill.
[5]
DeGroot, M.H., Probability and Statistics, Addison-Wesley Series in Behavioral
Science: Quantitive Methods, eds. F. Mosteller, 1975, Reading, MA, Addison-
Wesley Publishing Company.
[6]
DeGroot, M.H., Probability and Statistics, 2nd ed, Addison-Wesley Series in Be-
havioral Science: Quantitive Methods, eds. F. Mosteller, 1986, Reading, MA, Ad-
dison-Wesley Publishing Company.
[7]
Freedman, D., et al., Statistics, 2nd ed, 1991, New York, W. W. Norton & Com-
pany, Inc.
[8]
Gales, M.J., Model Based Techniques for Noise Robust Speech Recognition, PhD
Thesis in Engineering Department 1995, Cambridge University, .
[9]
Gauvain, J.L. and C.H. Lee, "Maximum a Posteriori Estimation for Multivariate
Gaussian Mixture Observations of Markov Chains," IEEE Trans. on Speech and
Audio Processing, 1994, 2(2), pp. 291-298.
[10]
Gillett, G.E., Introduction to Operations Research: A Computer-Oriented Algo-
rithmic Approach, McGraw-Hill Series in Industrial Engineering and Management
Science, eds. J. Riggs, 1976, New York, McGraw-Hill.
[11]
Gillick, L. and S.J. Cox, "Some Statistical Issues in the Comparison of Speech
Recognition Algorithms," IEEE Int. Conf. on Acoustics, Speech and Signal Proc-
essing, 1989, Glasgow, Scotland, UK, IEEE pp. 532-535.
[12]
Hamming, R.W., Coding and Information Theory, 1986, Englewood Cliffs NJ,
Prentice-Hall.
[13]
Hoel, P.G., Introduction to Mathesmatical Statistics, 5th edition ed, 1984, John
Wiley & Sons.
[14]
Jeffreys, H., Theory of Probability, 1961, Oxford University Press.
[15]
Jelinek, F., Statistical Methods for Speech Recognition, Language, Speech, and
Communication, 1998, Cambridge, MA, MIT Press.
[16]
Lindley, D.V., "The Use of Prior Probability Distributions in Statistical Inference
and Decision," Fourth Berkeley Symposium on Mathematical Statistics and Prob-
ability, 1961, Berkeley, CA, Univ. of California Press.

132
Probability, Statistics, and Information Theory
[17]
McEliece, R., The Theory of Information and Coding, Encyclopedia of Mathemat-
ics and Its Applications, eds. R. Gian-Carlo. Vol. 3, 1977, Reading, Addison-
Wesley Publishing Company.
[18]
Neyman, J. and E.S. Pearson, "On the Problem of the Most Efficient Tests of Sta-
tistical Hypotheses," Philosophical Trans. of Royal Society, 1928, 231, pp. 289-
337.
[19]
Papoulis, A., Probability, Random Variables, and Stochastic Processes, 3rd ed,
1991, New York, McGraw-Hill.
[20]
Poor, H.V., An Introduction to Signal Detection and Estimation, Springer tests in
Electrical Engineering, eds. J.B. Thomas, 1988, New York, Springer-Verlag.
[21]
Shannon, C., "A Mathematical Theory of Communication System," Bell System
Technical Journal, 1948, 27, pp. 379-423, 623-526.
[22]
Shannon, C.E., "Prediction and Entropy of Printed English," Bell System Technical
Journal, 1951, pp. 50-62.
[23]
Shannon, C.E., Claude Elwood Shannon : Collected Papers, 1993, IEEE.
[24]
Viniotis, Y., Probability and Random Processes for Electrical Engineering, Out-
line Series in Electronics & Electrical Engineering, eds. Schaum, 1998, New York,
WCB McGraw-Hill.
[25]
Wald, A., "Note of Consistency of Maximum Likelihood Estimate," Ann. Mathe-
matical Statistics, 1949(20), pp. 595-601.
[26]
Wilks, S.S., Mathematical Statistics, 1962, New York, John Wiley and Sons.
[27]
Ziv, J. and A. Lempel, "A Universal Algorithm for Sequential Data Compression,"
IEEE Trans. on Information Theory, 1997, IT-23, pp. 337-343.

133
C H A P T E R
4
Pattern RecognitionEquation Section 4
Spoken language processing relies heavily on
pattern recognition, one of the most challenging problems for machines. In a broader sense,
the ability to recognize patterns forms the core of our intelligence. If we can incorporate the
ability to reliably recognize patterns in our work and life, we can make machines much eas-
ier to use. The process of human pattern recognition is not well understood.
Due to the inherent variability of spoken language patterns, we emphasize the use of
statistical approach in this book. The decision for pattern recognition is based on appropriate
probabilistic models of the patterns. This Chapter presents several mathematical fundamen-
tals for statistical pattern recognition and classification. In particular, Bayes decision theory,
and estimation techniques for parameters of classifiers are introduced. Bayes decision the-
ory, which plays a central role for statistical pattern recognition, is described to introduce the
concept of decision-making based on both posterior knowledge obtained from specific ob-
servation data, and prior knowledge of the categories. To build such a classifier or predictor,
it is critical to estimate prior class probabilities and the class-conditional probabilities for a
Bayes classifier.

134
Pattern Recognition
Supervised learning has class information for the data. Only the probabilistic structure
needs to be learned. Maximum likelihood estimation (MLE) and maximum posterior prob-
ability estimation (MAP) that we discussed in Chapter 3 are two most powerful methods.
Both MLE and MAP aim to maximize the likelihood function. The MLE criterion does not
necessarily minimize the recognition error rate. Various discriminant estimation methods are
introduced for that purpose. Maximum mutual information estimation (MMIE) is based on
criteria to achieve maximum model separation (the model for the correct class is well sepa-
rated from other competing models) instead of likelihood criteria. The MMIE criterion is
one step closer but still is not directly related to minimizing the error rate. Other discrimi-
nant estimation methods, such as minimum error-rate estimation, use the ultimate goal of
pattern recognition – minimizing the classification errors. Neural networks are one class of
discriminant estimation methods.
The EM algorithm is an iterative algorithm for unsupervised learning in which class
information is unavailable or only partially available. The EM algorithm forms the theoreti-
cal basis for training hidden Markov models (HMM) as described in Chapter 8. To better
understand the relationship between MLE and EM algorithms, we first introduce vector
quantization (VQ), a widely used source-coding technique in speech analysis. The well-
known k-means clustering algorithm best illustrates the relationship between MLE and the
EM algorithm. We close this chapter by introducing a powerful binary prediction and re-
gression technique, classification and regression trees (CART). The CART represents an
important technique that combines rule-based expert knowledge and statistical learning.
4.1.
BAYES DECISION THEORY
Bayes decision theory forms the basis of statistical pattern recognition. The theory is based
on the assumption that the decision problem can be specified in probabilistic terms and that
all of the relevant probability values are known. Bayes decision theory can be viewed as a
formalization of a common-sense procedure, i.e., the aim to achieve minimum-error-rate
classification. This common-sense procedure can be best observed in the following real-
world decision examples.
Consider the problem of making predictions for the stock market. We use the Dow
Jones Industrial average index to formulate our example, where we have to decide tomor-
row's Dow Jones Industrial average index in one of the three categories (events): Up, Down,
or Unchanged. The available information is the probability function
( )
P ω of the three cate-
gories. The variable ω is a discrete random variable with the value
(
1,2,3)
i
i
ω
ω
=
=
. We
call the probability
(
)
i
P ω
a prior probability, since it reflects prior knowledge of tomor-
row's Dow Jones Industrial index. If we have to make a decision based only on the prior
probability, the most plausible decision may be made by selecting the class
i
ω
with the
highest prior probability
(
)
i
P ω
. This decision is unreasonable, in that we always make the
same decision even though we know that all three categories of Dow Jones Industrial index
changes will appear. If we are given further observable data, such as the federal-funds inter-
est rate or the jobless rate, we can make a more informed decision. Let x be a continuous

Bayes Decision Theory
135
random variable whose value is the federal-fund interest rate, and
| ( |
)
xf
x
ω
ω
be a class-
conditional pdf. For simplicity, we denote the pdf
| ( |
)
xf
x
ω
ω
as
( |
)
i
p x ω
, where i = 1, 2, 3
unless there is ambiguity. The class-conditional probability density function is often referred
to as the likelihood function as well, since it measures how likely it is that the underlying
parametric model of class
i
ω will generate the data sample x. Since we know the prior prob-
ability
(
)
i
P ω
and class-conditional pdf
( |
)
i
p x ω
, we can compute the conditional probabil-
ity
(
| )
i
P
x
ω
using Bayes’ rule:
( |
) (
)
(
| )
( )
i
i
i
p x
P
P
x
p x
ω
ω
ω
=
(4.1)
where
3
1
( )
( |
) (
)
i
i
i
p x
p x
P
ω
ω
=
= 
.
The probability term in the left-hand side of Eq. (4.1) is called the posterior probabil-
ity as it is the probability of class
i
ω after observing the federal-funds interest rate x. An
intuitive decision rule would be choosing the class
k
ω
with the greatest posterior probabil-
ity. That is,
argmax
(
| )
i
i
k
P
x
ω
=
(4.2)
In general, the denominator
( )
p x
in Eq. (4.1) is unnecessary because it is a constant term
for all classes. Therefore, Eq. (4.2) becomes
argmax
(
| )
argmax
( |
) (
)
i
i
i
i
i
k
P
x
p x
P
ω
ω
ω
=
=
(4.3)
The rule in Eq. (4.3) is referred to as Bayes’ decision rule. It shows how the observed
data x changes the decision based on the prior probability
(
)
i
P ω
to one based on the poste-
rior probability
(
| )
i
P
x
ω
. Decision making based on the posterior probability is more reli-
able, because it employs prior knowledge together with the present observed data. As a mat-
ter of fact, when the prior knowledge is non-informative (
1
2
3
(
)
(
)
(
)
1/3
P
P
P
ω
ω
ω
=
=
=
), the
present observed data fully control the decision. On the other hand, when present observed
data are ambiguous, then prior knowledge controls the decision. There are many kinds of
decision rules based on posterior probability. Our interest is to find the decision rule that
leads to minimum overall risk, or minimum error rate in decision.
4.1.1.
Minimum-Error-Rate Decision Rules
Bayes’ decision rule is designed to minimize the overall risk involved in making a decision.
Bayes’ decision based on posterior probability
(
| )
i
P
x
ω
instead of prior probability
(
)
i
P ω
is a natural choice. Given an observation x, if
(
| )
(
| )
k
i
P
x
P
x
ω
ω
≥
for all i
k
≠
, we can de-

136
Pattern Recognition
cide that the true class is
j
ω . To justify this procedure, we show such a decision results in
minimum decision error.
Let
{
}
1,...,
s
ω
ω
Ω=
be the finite set of s possible categories to be predicted and
{
}
1,...,
t
δ
δ
∆=
be a finite set of t possible decisions. Let
(
|
)
i
j
l δ
ω
be the loss function in-
curred for making decision
iδ when the true class is
j
ω . Using the prior probability
(
)
i
P ω
and class-conditional pdf
( |
)
i
p x ω
, the posterior probability
(
| )
i
P
x
ω
is computed by
Bayes’ rule as shown in Eq. (4.1). If we make decision
iδ
when the true class is
j
ω , we
incur a loss
(
|
)
i
j
l δ
ω
. Since the posterior probability
(
| )
j
P
x
ω
is the probability that the
true class is
j
ω
after observing the data x, the expected loss associated with making deci-
sion
iδ is:
1
(
| )
(
|
) (
| )
s
i
i
j
j
j
R
x
l
P
x
δ
δ
ω
ω
=
= 
(4.4)
In decision-theoretic terminology, the above expression is called conditional risks.
The overall risk R is the expected loss associated with a given decision rule. The decision
rule is employed as a decision function
( )
x
δ
that maps the data x to one of the decisions
{
}
1,...,
t
δ
δ
∆=
. Since
(
| )
i
R
x
δ
is the conditional risk associated with decision
iδ , the over-
all risk is given by:
( ( ) | ) ( )
R
R
x
x p x dx
δ
∞
−∞
= 
(4.5)
If the decision function
( )
x
δ
is chosen so that the conditional risk
( ( ) | )
R
x
x
δ
is minimized
for every x, the overall risk is minimized. This leads to the Bayes’ decision rule: To mini-
mize the overall risk, we compute the conditional risk shown in Eq. (4.4) for
1,...,
i
t
=
and
select the decision
iδ
for which the conditional risk
(
| )
i
R
x
δ
is minimum. The resulting
minimum overall risk is known as Bayes’ risk that has the best performance possible.
The loss function (
|
)
i
j
l δ
ω
in the Bayes’ decision rule can be defined as:
0
(
|
)
,
1,
,
1
i
j
i
j
l
i j
s
i
j
δ
ω
=


=
=


≠


(4.6)
This loss function assigns no loss to a correct decision where the true class is
i
ω and the
decision is
iδ , which implies that the true class must be
i
ω . It assigns a unit loss to any er-
ror where i
j
≠
; i.e., all errors are equally costly. This type of loss function is known as a
symmetrical or zero-one loss function. The risk corresponding to this loss function equals
the classification error rate, as shown in the following equation.

Bayes Decision Theory
137
1
1
(
| )
(
|
) (
| )
(
| )
(
| )
(
| ) 1
(
| )
s
i
i
j
j
j
j
j i
s
j
i
i
j
R
x
l
P
x
P
x
P
x
P
x
P
x
δ
δ
ω
ω
ω
ω
ω
ω
=
≠
=
=
=
=
−
= −



(4.7)
Here
(
| )
i
P
x
ω
is the conditional probability that decision
iδ
is correct after observing the
data x. Therefore, in order to minimize classification error rate, we have to choose the deci-
sion of class i that maximizes the posterior probability
(
| )
i
P
x
ω
. Furthermore, since
( )
p x is
a constant, the decision is equivalent to picking the class i that maximizes
( |
) (
)
i
i
p x
P
ω
ω
.
The Bayes’ decision rule can be formulated as follows:
( )
argmax
(
| )
argmax
( |
) (
)
i
i
i
i
i
x
P
x
P x
P
δ
ω
ω
ω
=
=
(4.8)
This decision rule, which is based on the maximum of the posterior probability
(
| )
i
P
x
ω
, is called the minimum-error-rate decision rule. It minimizes the classification
error rate. Although our description is for random variable x, Bayes’ decision rule is appli-
cable to multivariate random vector x without loss of generality.
p(x|ω 1 ) P(ω 1)
p(x|ω 2 ) P(ω 2)
optimal
decision
boundary
x
decision
boundary
1
2
Figure 4.1 Calculation of the likelihood of classification error [22]. The shaded area represents
the integral value in Eq. (4.9).
A pattern classifier can be regarded as a device for partitioning the feature space into
decision regions. Without loss of generality, we consider a two-class case. Assume that the
classifier divides the space ℜinto two regions,
1
ℜand
2
ℜ. To compute the likelihood of
errors, we need to consider two cases. In the first case, x falls in
1
ℜ, but the true class is
2
ω .
In the other case, x falls in
2
ℜ, but the true class is
1
ω . Since these two cases are mutually
exclusive, we have

138
Pattern Recognition
1
2
1
2
2
1
1
2
2
2
1
1
2
2
1
1
(
)
(
,
)
(
,
)
=
(
|
) (
)
(
|
) (
)
=
( |
) (
)
( |
) (
)
P error
P x
P x
P x
P
P x
P
P x
P
dx
P x
P
dx
ω
ω
ω
ω
ω
ω
ω
ω
ω
ω
ℜ
ℜ
=
∈ℜ
+
∈ℜ
∈ℜ
+
∈ℜ
+


(4.9)
Figure 4.1 illustrates the calculation of the classification error in Eq. (4.9). The two
terms in the summation are merely the tail areas of the function
( |
) (
)
i
i
P x
P
ω
ω
. It is clear
that this decision boundary is not optimal. If we move the decision boundary a little bit to
the left, so that the decision is made to choose the class i based on the maximum value
of
( |
) (
)
i
i
P x
P
ω
ω
, the tail integral area
(
)
P error
becomes minimum, which is the Bayes’
decision rule.
4.1.2.
Discriminant Functions
The decision problem above can also be viewed as a pattern classification problem where
unknown data x1 are classified into known categories, such as the classification of sounds
into phonemes using spectral data x. A classifier is designed to classify data x into s catego-
ries by using s discriminant functions,
( )
id x , computing the similarities between the un-
known data x and each class
i
ω and assigning x to class
j
ω if
( )
( )
j
i
d
d
i
j
>
∀≠
x
x
(4.10)
This representation of a classifier is illustrated in Figure 4.2.
d1
d2
ds
MAX
x1
x2
xd
d1(x)
d2(x)
ds(x)
δ(x)
Feature
Vector
Discriminate
Function
Maximum
Selector
Decision
Figure 4.2 Block diagram of a classifier based on discriminant functions [22].
1 Assuming x is a d-dimensional vector.

Bayes Decision Theory
139
A Bayes’ classifier can be represented in the same way. Based on the Bayes’ classi-
fier, unknown data x are classified on the basis of Bayes’ decision rule, which minimizes the
conditional risk
(
| )
i
R a
x . Since the classification decision of a pattern classifier is based on
the maximum discriminant function shown in Eq. (4.10), we define our discriminant func-
tion as:
( )
(
| )
i
i
d
R a
= −
x
x
(4.11)
As such, the maximum discriminant function corresponds to the minimum conditional risk.
In the minimum-error-rate classifier, the decision rule is to maximize the posterior probabil-
ity
(
| )
i
P ω
x . Thus, the discriminant function can be written as follows:
1
( |
) (
)
( |
) (
)
( )
(
| )
( )
( |
) (
)
i
i
i
i
i
i
s
j
j
j
p
P
p
P
d
P
p
p
P
ω
ω
ω
ω
ω
ω
ω
=
=
=
=

x
x
x
x
x
x
(4.12)
There is a very interesting relationship between Bayes’ decision rule and the hypothe-
ses testing method described in Chapter 3. For a two-class pattern recognition problem, the
Bayes’ decision rule in Eq. (4.2) can be written as follows:
1
2
1
1
2
2
( |
) (
)
( |
) (
)
p
P
p
P
ω
ω
ω
ω
ω
ω
>
<
x
x
(4.13)
Eq. (4.13) can be rewritten as:
1
2
1
2
2
1
( |
)
(
)
( )
( |
)
(
)
p
P
x
p
P
ω
ω
ω
ω
ω
ω
>
=
<
x
x

(4.14)
The term
( )
x

is called likelihood ratio and is the basic quantity in hypothesis testing [73].
The term
2
1
(
)
(
)
P
P
ω
ω
is called the threshold value of the likelihood ratio for the decision.
Often it is convenient to use the log-likelihood ratio instead of the likelihood ratio for the
decision rule. Namely, the following single discriminant function can be used instead of
1( )
d x
and
2( )
d
x
for:
1
2
1
2
2
1
( )
log ( )
log
( |
)
log
( |
)
log
(
)
log
(
)
d
p
p
P
P
ω
ω
ω
ω
ω
ω
>
=
=
−
−
<
x
x
x
x

(4.15)
As the classifier assigns data x to class
i
ω , the data space is divided into s regions,
1
2
,
,
,
d
d
d
s
ℜ
ℜ
ℜ

, called decision regions. The boundaries between decision regions are called
decision boundaries and are represented as follows (if they are contiguous):
( )
( )
i
j
d
d
i
j
=
≠
x
x
(4.16)

140
Pattern Recognition
For points on the decision boundary, the classification can go either way. For a Bayes’ clas-
sifier, the conditional risk associated with either decision is the same and how to break the
tie does not matter. Figure 4.3 illustrates an example of decision boundaries and regions for
a three-class classifier on a scalar data sample x.
p(x|ω 1 ) P(ω 1)
p(x|ω 3 ) P(ω 3)
1
3
p(x|ω 2 ) P(ω 2)
x
2
3
Figure 4.3 An example of decision boundaries and regions. For simplicity, we use scalar vari-
able x instead of a multi-dimensional vector [22].
4.2.
HOW TO CONSTRUCT CLASSIFIERS
In the Bayes classifier, or the minimum-error-rate classifier, the prior probability
(
)
i
P ω
and
class-conditional pdf
( |
)
i
p
ω
x
are known. Unfortunately, in pattern recognition domains,
we rarely have complete knowledge of class-conditional pdf’s and/or prior probability
(
)
i
P ω
. They often must be estimated or learned from the training data. In practice, the esti-
mation of the prior probabilities is relatively easy. Estimation of the class-conditional pdf is
more complicated. There is always concern to have sufficient training data relative to the
tractability of the huge dimensionality of the sample data x. In this chapter we focus on es-
timation methods for the class-conditional pdf.
The estimation of the class-conditional pdfs can be nonparametric or parametric. In
nonparametric estimation, no model structure is assumed and the pdf is directly estimated
from the training data. When large amounts of sample data are available, nonparametric
learning can accurately reflect the underlying probabilistic structure of the training data.
However, available sample data are normally limited in practice, and parametric learning
can achieve better estimates if valid model assumptions are made. In parametric learning,
some general knowledge about the problem space allows one to parameterize the class-
conditional pdf, so the severity of sparse training data can be reduced significantly. Suppose
the pdf
(
)
i
p
ω
x |
is assumed to have a certain probabilistic structure, such as the Gaussian
pdf. In such cases, only the mean vector
iµ
(or mean
iµ ) and covariance matrix
i
 (or
variance
2
σ ) need to be estimated.

How to Construct Classifiers
141
When the observed data x only takes discrete values from a finite set of N values, the
class-conditional pdf is often assumed nonparametric, so there will be
1
N −
free parameters
in the probability function
(
)
i
p
ω
x |
2. When the observed data x takes continuous values,
parametric approaches are usually necessary. In many systems, the continuous class-
conditional pdf (likelihood)
(
)
i
p
ω
x |
is assumed to be a Gaussian distribution or a mixture
of Gaussian distributions.
In pattern recognition, the set of data samples, which is often collected to estimate the
parameters of the recognizer (including the prior and class-conditional pdf), is referred to as
the training set. In contrast to the training set, the testing set is referred to the independent
set of data samples, which is used to evaluate the recognition performance of the recognizer.
For parameter estimation or learning, it is also important to distinguish between super-
vised learning and unsupervised learning. Let’s denote the pair ( ,
)
ω
x
as a sample, where x
is the observed data and ω is the class from which the data x comes. From the definition, it
is clear that ( ,
)
ω
x
are jointly distributed random variables. In supervised learning, ω , in-
formation about the class of the sample data x is given. Such sample data are usually called
labeled data or complete data, in contrast to incomplete data where the class information ω
is missing for unsupervised learning. Techniques for parametric unsupervised learning are
discussed in Section 4.4.
In Chapter 3 we introduced two most popular parameter estimation techniques –
maximum likelihood estimation (MLE) and maximum a posteriori probability estimation
(MAP). Both MLE and MAP are supervised learning methods since the class information is
required. MLE is the most widely used because of its efficiency. The goal of MLE is to find
the set of parameters that maximizes the probability of generating the training data actually
observed. The class-conditional pdf is typically parameterized. Let
i
Φ denote the parameter
vector for class i. We can represent the class-conditional pdf as a function of
i
Φ
as
( |
,
)
i
i
p
ω
x
Φ
. As stated earlier, in supervised learning, the class name
i
ω is given for each
sample data in training set {
}
1
2
,
,
,
n
x x
x

. We need to make an assumption3 that samples in
class
i
ω give no information about the parameter vector
j
Φ
of the other class
j
ω . This
assumption allows us to deal with each class independently, since the parameter vectors for
different categories are functionally independent. The class-conditional pdf can be rewritten
as
( |
)
p x Φ , where
{
}
1,
,
,
i
n
=
Φ
Φ Φ
Φ

. If a set of random samples
1
2
{
,
,
,
}
n
X X
X

is
drawn independently according to a pdf
( |
)
p x Φ , where the value of the parameter Φ is
unknown, the MLE method described in Chapter 3 can be directly applied to estimate Φ .
Similarly, MAP estimation can be applied to estimate Φ if knowledge about a prior
distribution is available. In general, MLE is used for estimating parameters from scratch
without any prior knowledge, and MAP estimation is used for parameter adaptation where
2 Since all the probabilities need to add up to one.
3 This assumption is only true for non-discriminant estimation. Samples in class
i
ω
may affect parameter vector
i
Φ of the other classes in discriminant estimation methods as described in Section 4.3

142
Pattern Recognition
the behavior of prior distribution is known and only small amount adaptation data is avail-
able. When the amount of adaptation data increases, MAP estimation converges to MLE.
4.2.1.
Gaussian Classifiers
A Gaussian classifier is a Bayes’ classifier where class-conditional probability density
( |
)
i
p
ω
x
for each class
i
ω is assumed to have a Gaussian distribution4:
(
)
1
1/ 2
/ 2
1
1
( |
)
exp
(
)
(
)
2
2
t
i
i
i
i
n
i
p
ω
π
−


=
−
−
Σ
−




Σ
x
x
µ
x
µ
(4.17)
As discussed in Chapter 3, the parameter estimation techniques are well suited for the
Gaussian family. The MLE of the Gaussian parameter is just its sample mean and variance
(or co-variance matrix). A Gaussian classifier is equivalent to the one using a quadratic dis-
criminant function. As noted in Eq. (4.12), the discriminant function for a Bayes’ decision
rule is the posterior probability
(
| )
i
p ω
x
or
( |
) (
)
i
i
p
P
ω
ω
x
as the discriminant function.
Assuming
( |
)
i
p
ω
x
is a multivariate Gaussian density as shown in Eq. (4.17), a discrimi-
nant function can be written as follows:
1
( )
log
( |
) (
)
1
1
(
)
(
)
log
(
)
log |
|
log2
2
2
2
i
i
i
t
i
i
i
i
d
p
p
d
p
ω
ω
ω
π
−
=
= −
−
Σ
−
+
−
Σ −
x
x
x
µ
x
µ
(4.18)
If we have a uniform prior
(
)
i
p ω
, it is clear that the above discriminant function
( )
id x is a quadratic function. Once we have the s Gaussian discriminant functions, the deci-
sion process simply assigns data x to class
j
ω iff
arg max
( )
i
i
j
d
=
x
(4.19)
When
all
the
Gaussian
pdfs
have
the
same
covariance
matrix
(
for
1,2,
,
i
i
s
Σ = Σ
=

), the quadratic term
1
t
−
Σ
x
x is independent of the class and can be
treated as a constant. Thus the following new discriminant function
( )
id x can be used [22];
( )
t
i
i
i
d
=
+
x
a x
c
(4.20)
where
1
i
i
−
= Σ
a
µ and
1
1
log
(
)
2
t
i
i
i
i
i
P ω
−
= −
Σ
+
c
µ
µ
.
( )
id x in Eq. (4.20) is a linear discrimi-
nant function. For linear discriminant functions, the decision boundaries are hyperplanes.
For the two-class case (
1
ω and
2
ω ), and assuming that data sample x is a real random vec-
tor, the decision boundary can be shown to be the following hyperplane:
4 The Gaussian distribution may include a mixture of Gaussian pdfs.

How to Construct Classifiers
143
(
)
0
t
−
=
A x
b
(4.21)
where
1
1
2
(
)
−
= Σ
−
A
µ
µ
(4.22)
and
[
]
1
2
1
2
1
2
1
1
2
1
2
(
)log
(
1 (
)
2
(
)
(
)
t
P ω ω
−
−
=
−
−
−
Σ
−
µ
µ
b
µ
µ
µ
µ
µ
µ
(4.23)
Figure 4.4 Decision boundary for a two-class Gaussian classifier. Gaussian distributions for
the two categories have the same covariance matrix Σ . Each ellipse represents the region with
the same likelihood probability [22].
Figure 4.4 shows a two dimensional decision boundary for a two-class Gaussian classifier
with the same covariance matrix. Please note that the decision hyperplane is generally not
orthogonal to the line between the means
1
µ
and
2
µ , although it does intersect that line at
the point b, which is halfway between
1
µ
and
2
µ . The analysis above is based on the case
of uniform priors (
1
2
(
)
(
)
p
p
ω
ω
=
). For nonuniform priors, the decision hyperplane moves
away from the more likely mean.
Finally, if each dimension of random vector x is statistically independent and has the
same variance
2
σ , i.e.,
2
1
2
σ
Σ = Σ =
I , Figure 4.4 becomes Figure 4.5. The ellipse in Figure
4.4 becomes a circle because the variance
2
σ
is the same for all dimensions [22].
decision boundary
1
µ
2
µ
2
ℜ
1
ℜ

144
Pattern Recognition
Figure 4.5 Decision boundary for a two-class Gaussian classifier. Gaussian distributions for
the two categories have the same covariance matrix
2
σ I . Each circle represents the region
with the same likelihood probability [22].
4.2.2.
The Curse of Dimensionality
More features (higher dimensions for sample x) and more parameters for the class-
conditional pdf
( |
)
p
Φ
x
may lead to lower classification error rate. If the features are statis-
tically independent, there are theoretical arguments that support better classification per-
formance with more features. Let us consider a simple two-class Gaussian classifier. Sup-
pose the prior probabilities
(
)
i
p ω
are equal and the class-conditional Gaussian pdf’s
( |
,
)
i
i
p
Σ
x m
share the same covariance matrix Σ . According to Eqs. (4.9) and (4.21), the
Bayes’ classification error rate is given by:
(
)
1
2
/ 2
1/ 2
2
1
2
2
1
2
1
1
1
2
(
) 0
1
2
xp
(
)
(
)
e
(
)
2
( |
) (
)
2
t
i
i
i
n
t
i
r
z
P error
P
P
d
d
e
dz
π
π
ω
ω
−
−
ℜ
∞
Σ
−
=
∞
−
−
Σ
−
=
=




=



A
x b
x
µ
x
µ
x
x
x
(4.24)
where
1
1
2
1
2
(
)
(
)
t
r
−
=
−
Σ
−
µ
µ
µ
µ
. When features are independent, the covariance matrix
becomes a diagonal one. The following equation shows that each independent feature helps
to reduce the error rate5:
5 When the means of a feature for the two classes are exactly the same, adding such a feature does not reduce the
Bayes’ error. Nonetheless, according to Eq. (4.25), the Bayes’ error cannot possibly be increased by incorporating
an additional independent feature.
decision boundary
1
µ
2
ℜ
1
ℜ
2
µ

How to Construct Classifiers
145
2
1
2
1
d
i
i
i
i
r
µ
µ
σ
=


−
=





(4.25)
where
1i
µ
and
2i
µ
are the ith -dimension of mean vectors
1
µ and
1
µ respectively.
Unfortunately, in practice, the inclusion of additional features may lead to worse clas-
sification results. This paradox is called the curse of dimensionality. The fundamental issue,
called trainability, refers to how well the parameters of the classifier are trained from the
limited training samples. Trainability can be illustrated by a typical curve-fitting (or regres-
sion) problem. Figure 4.6 shows a set of eleven data points and several polynomial fitting
curves with different orders. Both the first-order (linear) and second-order (quadratic) poly-
nomials shown provide fairly good fittings for these data points. Although the tenth-order
polynomial fits the data points perfectly, no one would expect such an under-determined
solution to fit the new data well. In general, many more data samples would be necessary to
get a good estimate of a tenth-order polynomial than of a second-order polynomial, because
reliable interpolation or extrapolation can be attained only with an over-determined solution.
Figure 4.6 Fitting eleven data points with polynomial functions of different orders [22].
Figure 4.7 shows the error rates for two-phonemes (/ae/ and /ih/) classification where
two phonemes are modeled by mixtures of Gaussian distributions. The parameters of mix-
tures of Gaussian are trained from a varied set of training samples via maximum likelihood
estimation. The curve illustrates the classification error rate as a function of the number of
training samples and the number of mixtures. For every curve associated with a finite num-
ber of samples, there are an optimal number of mixtures. This illustrates the importance of
trainability: it is critical to assure there are enough samples for training an increased number
of features or parameters. When the size of training data is fixed, increasing the number of
features or parameters beyond a certain point is likely to be counterproductive.
When you have an insufficient amount of data to estimate the parameters, some sim-
plification can be made to the structure of your models. In general, the estimation for higher-
order statistics, like variances or co-variance matrices, requires more data than that for
lower- order statistics, like mean vectors. Thus more attention often is paid to dealing with
the estimation of covariance matrices. Some frequent used heuristics for Gaussian distribu-

146
Pattern Recognition
tions include the use of the same covariance matrix for all mixture components [77], diago-
nal covariance matrix, and shrinkage (also referred to as regularized discriminant analysis),
where the covariance matrix is interpolated with the constant covariance matrix [23, 50].
2
10
50
500
1000
NUMBER OF
SAMPLES, n
10
100
1000
ERROR
RATE
1
NUMBER OF MIXTURES, m
10000
Figure 4.7 Two-phoneme (/ae/ and /ih/) classification results as a function of the number of
Gaussian mixtures and the number of training samples.
4.2.3.
Estimating the Error Rate
Estimating the error rate of a classifier is important. We want to see whether the classifier is
good enough to be useful for our task. For example, telephone applications show that some
accuracy is required before users would switch from using the touch-tone to the speech rec-
ognizer. It is also critical to compare the performance of a classifier (algorithm) against an
alternative. In this section we deal with how to estimate the true classification error rate.
One approach is to compute the theoretic error rate from the parametric model as
shown in Eq. (4.25). However, there are several problems with this approach. First, such an
approach almost always under-estimates, because the parameters estimated from the training
samples might not be realistic unless the training samples are representative and sufficient.
Second, all the assumptions about models and distributions might be severely wrong. Fi-
nally, it is very difficult to compute the exact error rate, as in the simple case illustrated in
Eq. (4.25).
Instead, you can estimate the error rate empirically. In general, the recognition error
rate on the training set should be viewed only as a lower bound, because the estimate can be
made to minimize the error rate on the training data. Therefore, a better estimate of the rec-
ognition performance should be obtained on an independent test set. The question now is
how representative is the error rate computed from an arbitrary independent test set. The

How to Construct Classifiers
147
common process of using some of the data samples for design and reserving the rest for test
is called the holdout or H method.
Suppose the true but unknown classification error rate of the classifier is p, and one
observes that k out of n independent randomly drawn test samples are misclassified. The
random variable K should have a binomial distribution
( , )
B n p . The maximum likelihood
estimation for p should be
ˆ
k
p
n
=
(4.26)
The statistical test for binomial distribution is discussed in Chapter 3. For a 0.05 signifi-
cance level, we can compute the following equations to get the range
1
2
(
,
)
p p
:
(
) (
)
1
1
1
2 (
)
2
1
0.05
when
n
m
n m
m k
n
P k
m
n
p
p
k
np
m
−
=


≤
≤
=
−
=
>





(4.27)
(
) (
)
2
2
2
0
2 (0
)
2
1
0.05
when
k
m
n m
m
n
P
m
k
p
p
k
np
m
−
=


≤
≤
=
−
=
<





(4.28)
Equations (4.27) and (4.28) are cumbersome to solve, so the normal test described in
Chapter 3 can be used instead. The null hypothesis
0
H
is
0
H
:
ˆ
p
p
=
We can use the normal test to find the two boundary points
1p and
2p at which we would
not reject the null hypothesis
0
H .
The range
1
2
(
,
)
p p
is called the 0.95 confidence intervals because one can be 95%
confident that the true error rate p falls in the range
1
2
(
,
)
p p
. Figure 4.8 illustrates 95% con-
fidence intervals as a function of ˆp and n. The curve certainly agrees with our intuition –
the larger the number of test samples n, the more confidence we have in the MLE estimated
error rate ˆp ; otherwise, the ˆp can be used only with caution.
Based on the description in the previous paragraph, the larger the test set is, the better
it represents the recognition performance of possible data. On one hand, we need more train-
ing data to build a reliable and consistent estimate. On the other hand, we need a large inde-
pendent test set to derive a good estimate of the true recognition performance. This creates a
contradictory situation for dividing the available data set into training and independent test
set. One way to effectively use the available database is V-fold cross validation. It first splits
the entire database into V equal parts. Each part is used in turn as an independent test set
while the remaining (V - 1) parts are used for training. The error rate can then be better esti-
mated by averaging the error rates evaluated on the V different testing sets. Thus, each part
can contribute to both training and test sets during V-fold cross validation. This procedure,
also called the leave-one-out or U method [53], is particularly attractive when the number of
available samples are limited.

148
Pattern Recognition
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
n= 10
15
30
20
50
100
250
10
100
30
50
20
15
250
p
k/n
Figure 4.8 95% confidence intervals for classification error rate estimation with normal test
4.2.4.
Comparing Classifiers
Given so many design alternatives, it is critical to compare the performance of different
classifiers so that the best classifier can be used for real-world applications. It is common for
designers to test two classifiers on some test samples and decide if one is superior to the
other. Relative efficacy can be claimed only if the difference in performance is statistically
significant. In other words, we establish the null hypothesis
0
H
that the two classifiers have
the same error rates. Based on the observed error patterns, we decide whether we could re-
ject
0
H
at the 0.05 level of significance. The test for different classifiers falls into the cate-
gory of matched-pairs tests described in Chapter 3. Classifiers are compared with the same
test samples.
We present an effective matched-pairs test - McNemar’s test [66] which is particularly
suitable for comparing classification results. Suppose there are two classifiers:
1
Q and
2
Q .
The estimated classification error rates on the same test set for these two classifiers are
1p
and
2p
respectively. The null hypothesis
0
H
is
1p =
2p . The classification performance of
the two classifiers can be summarized as in Table 4.1. We define
'
ij
q
s as follows:
00
q
=
1
2
(
and
classify data sample correctly)
P Q
Q
01
q
=
1
2
(
classifies data sample correctly, but
incorrectly)
P Q
Q

How to Construct Classifiers
149
10
q
=
2
1
(
classifies data sample correctly, but
incorrectly)
P Q
Q
11
q
=
1
2
(
and
classify data sample incorrectly)
P Q
Q
Table 4.1 Classification performance table for classifier
1
Q and
2
Q .
00
N
is the number of
samples which
1
Q and
2
Q classify correctly,
01
N
is the number of samples which
1
Q classi-
fies correctly, but
2
Q incorrectly,
10
N
is the of samples which
2
Q classifies correctly, but
1
Q
incorrectly, and
11
N
is the number of samples which
1
Q and
2
Q classify incorrectly [30].
2
Q
Correct
Incorrect
Correct
00
N
01
N
1
Q
Incorrect
10
N
11
N
The null hypothesis
0
H
is equivalent to
1
0
H
:
01
q
=
10
q . If we define
(
)
10
01
10
q
q
q
q
=
+
,
0
H
is equivalent to
2
0
H
:
12
q =
.
2
0
H
represents the hypothesis that,
given only one of the classifiers makes an error, it is equally likely to be either one. We can
test
2
0
H
based on the data samples on which only one of the classifiers made an error. Let
01
10
n
N
N
=
+
. The observed random variable
01
N
should have a binomial distribution
12
( ,
)
B n
. Therefore, the normal test (z-test) described in Chapter 3 can be applied directly to
test the null hypothesis
2
0
H .
The above procedure is called the McNemar’s test [66]. If we view the classification
results as N (the total number of test samples) independent matched pairs, the sign test as
described in Chapter 3 can be directly applied to test the null hypothesis that classifier
1
Q is
not better than classifier
2
Q , that is, the probability that classifier
1
Q performs better than
classifier
2
Q , p, is smaller than or equal to ½.
McNemar’s test is applicable when the errors made by a classifier are independent
among different test samples. Although this condition is true for most static pattern recogni-
tion problems, it is not the case for most speech recognition problems. In speech recognition,
the errors are highly inter-dependent because of the use of higher-order language models
(described in Chapter 11).
The solution is to divide the test data stream into segments in such a way that errors in
one segment are statistically independent of errors in any other segment [30]. A natural can-
didate for such a segment is a sentence or a phrase after which the speaker pauses. Let
1
i
N
the number of errors6 made on the ith segment by classifier
1
Q and
2
i
N be the number of er-
rors made on the ith segment by classifier
2
Q . Under this formulation, the magnitude-
difference test described in Chapter 3 can be applied directly to test the null hypothesis that
6 The errors for speech recognition include substitutions, insertions and deletions as discussed in Chapter 8.

150
Pattern Recognition
classifiers
1
Q and
2
Q have on the average the same error rate on the pairs of n independent
segments.
4.3.
DISCRIMINATIVE TRAINING
Both MLE and MAP criteria maximize the probability of the model associated with the cor-
responding data. Only data labeled as belonging to class
i
ω are used to train the parameters.
There is no guarantee that the observed data x from class
i
ω actually have a higher likeli-
hood probability
( |
)
i
P
ω
x
than the likelihood
( |
)
j
P
ω
x
associated with class j, given j
i
≠.
Models generated by MLE or MAP have a loose discriminant nature. Several estimation
methods aim for maximum discrimination among models to achieve best pattern recognition
performance.
4.3.1.
Maximum Mutual Information Estimation
The pattern recognition problem can be formalized as an information channel, as illustrated
in Figure 4.9. The source symbol ω is encoded into data x and transmitted through an in-
formation channel to the observer. The observer utilizes pattern recognition techniques to
decode x into source symbol ˆω . Consistent with the goal of communication channels, the
observer hopes the decoded symbol ˆω is the same as the original source symbol ω . Maxi-
mum mutual information estimation tries to improve channel quality between input and out-
put symbols.
Figure 4.9 An information channel framework for pattern recognition.
As described in Section 4.1.1, the decision rule for the minimum-error-rate classifier
selects the class
i
ω with maximum posterior probability
(
| )
i
P ω
x . It is a good criterion to
maximize the posterior probability
(
| )
i
P ω
x
for parameter estimation. Recalling Bayes’ rule
in Section 4.1, the posterior probability
(
| )
i
p ω
x
(assuming x belongs to class
i
ω ) is:
( |
) (
)
(
| )
( )
i
i
i
p
P
P
p
ω
ω
ω
=
x
x
x
(4.29)
Communication Channel
x
Data
Generator
Pattern
Decoder
ω
ˆω

Discriminative Training
151
and
( )
p x can be expressed as follows:
( )
( |
) (
)
k
k
k
p
p
p
ω
ω
= 
x
x
(4.30)
In the classification stage,
( )
p x can be considered as a constant. However, during training,
the value of
( )
p x depends on the parameters of all models and is different for different x.
Equation (4.29) is referred to as conditional likelihood. A conditional maximum likelihood
estimator (CMLE)
CMLE
θ
is defined as follows:
( )
argmax
(
| )
CMLE
MAP
i
p
θ
ω
Φ
Φ
= Φ
=
x
x
(4.31)
The summation in Eq. (4.30) extends over all possible classes that include the correct model
and all the possible competing models. The parameter vector Φ in Eq. (4.31) includes not
only the parameter
i
Φ corresponding to class
i
ω , but also those for all other classes.
Note that in Chapter 3, the mutual information between random variable X (observed
data) and Ω(class label) is defined as:
( ,
)
(
|
) ( )
( ,
)
log
log
( ) ( )
( ) ( )
p
p
P
I
E
E
p
P
p
P




Ω
Ω
Ω
Ω=
=




Ω
Ω




X
X
X
X
X
(4.32)
Since we don’t know the probability distribution for
( ,
)
p
Ω
X
, we assume our sample
( ,
)
i
ω
x
is representative and define the following instantaneous mutual information:
( ,
)
( ,
)
log
( ) (
)
i
i
i
p
I
p
P
ω
ω
ω
=
x
x
x
(4.33)
If equal prior
(
)
i
p ω
is assumed for all classes, maximizing the conditional likelihood
in Eq. (4.29) is equivalent to maximizing the mutual information defined in Eq. (4.33).
CMLE becomes maximum mutual information estimation (MMIE). It is important to note
that, in contrast to MLE, MMIE is concerned with distributions over all possible classes.
Equation (4.30) can be rewritten as two terms, one corresponding to the correct one, and the
other corresponding to the competing models:
( )
( |
) (
)
( |
) (
)
i
i
k
k
k i
p
p
P
p
P
ω
ω
ω
ω
≠
=
+
x
x
x
(4.34)
Based on the new expression of
( )
p x shown in Eq. (4.34), the posterior probability
(
| )
i
p ω
x in Eq. (4.29) can be rewritten as:
( |
) (
)
(
| )
( |
) (
)
( |
) (
)
i
i
i
i
i
k
k
k i
p
P
P
p
P
p
P
ω
ω
ω
ω
ω
ω
ω
≠
=
+
x
x
x
x
(4.35)

152
Pattern Recognition
Now, maximization of the posterior probability
(
| )
i
p ω
x
with respect to all models leads to
a discriminant model.7 It implies that the contribution of
( |
) (
)
i
i
p
P
ω
ω
x
from the true model
needs to be enforced, while the contribution of all the competing models, specified by
( |
) (
)
k
k
k i
p
P
ω
ω
≠
x
, needs to be minimized. Maximization of Eq. (4.35) can be further re-
written as:
1
(
| )
( |
) (
)
1
( |
) (
)
i
i
i
k i
i
i
P
p
p
p
p
ω
ω
ω
ω
ω
≠
=
+ 
x
x
x
(4.36)
Maximization is thus equivalent to maximization of the following term, which is clearly a
discriminant criterion between model
i
ω and the sum of all other competing models.
( |
) (
)
( |
) (
)
i
i
k
k
k i
p
p
p
p
ω
ω
ω
ω
≠
x
x
(4.37)
Equation (4.37) also illustrates a fundamental difference between MLE and MMIE. In MLE,
only the correct model needs to be updated during training. However, every MMIE model is
updated even with one training sample. Furthermore, the greater the prior probability
(
)
k
p ω
for class
k
ω , the more effect it has on the maximum mutual information estimator
MMIE
θ
.
This makes sense, since the greater the prior probability
(
)
k
p ω
, the greater the chance for
the recognition system to mis-recognize
i
ω as
k
ω . MLE is a simplified version of MMIE by
restricting the training of model using the data for the model only. This simplification allows
the denominator term in Eq. (4.35) to contain the correct model so that it can be dropped as
a constant term. Thus, maximization of the posterior probability
(
| )
i
p ω
x
can be trans-
formed into maximization of the likelihood
( |
)
i
p
ω
x
.
Although likelihood and posterior probability are transformable based on Bayes’ rule,
MLE and MMIE often generate different results. Discriminative criteria like MMIE attempt
to achieve minimum error rate. It might actually produce lower likelihood than for the un-
derlying probability density
( |
)
k
p
ω
x
. However, if the assumption of the underlying distri-
butions is correct and there are enough (or infinite) training data, the estimates should con-
verge to the true underlying distributions. Therefore, Bayes’ rule should be satisfied and
MLE and MMIE should produce the same estimate.
Arthur Nadas [71] showed that if the prior distribution (language model) and the as-
sumed likelihood distribution family are correct, both MLE and MMIE are consistent esti-
mators, but MMIE has a greater variance. However, when some of those premises are not
valid, it is desirable to use MMIE to find the estimate that maximizes the mutual information
(instead of likelihood) between sample data and its class information. The difference be-
7 General minimum-error-rate estimation is described in Section 4.3.2.

Discriminative Training
153
tween these two estimation techniques is that MMIE not only aims to increase the likelihood
for the correct class, but also tries to decrease the likelihood for the incorrect classes. Thus,
MMIE in general possesses more discriminating power among different categories. Al-
though MMIE is theoretically appealing, computationally it is very expensive. Comparing
with MLE, every data sample needs to train all the possible models instead of the corre-
sponding model. It also lacks an efficient maximization algorithm. You need to use a gradi-
ent descent algorithm.
4.3.1.1.
Gradient Descent
To maximize Eq. (4.37) over the entire parameter space
{
}
1
2
,
,
,
M
=
Φ
Φ Φ
Φ

with M
classes, we define the mutual information term in Eq. (4.37) to be a function of Φ . To fit
into the traditional gradient descent framework, we take the inverse of Eq. (4.37) as our op-
timization function to minimize the following function:8
( |
) (
)
1
(
)
(
| )
( |
) (
)
k
i
k
k
k i
i
i
i
p
p
F
P
p
p
ω
ω
ω
ω
ω
≠
=
= 
Φ
Φ
Φ
x
Φ
x
x
(4.38)
The gradient descent algorithm starts with some initial estimate
0
Φ
and computes the
gradient vector
(
)
F
∇
Φ
(∇is defined in Chapter 3). It obtains a new estimate
1
Φ by mov-
ing
0
Φ in the direction of the steepest descent, i.e., along the negative of the gradient. Once
it obtains the new estimate, it can perform the same gradient descent procedure iteratively
until
(
)
F Φ
converges to the local minimum. In summary, it obtains
1
t+
Φ
from
t
Φ by the
following formula:
1
(
) |
t
t
t
t F
ε
+
Φ=Φ
Φ
= Φ −
∇
Φ
(4.39)
where
tε is the learning rate (or step size) for the gradient descent.
Why can gradient descent lead
(
)
F Φ
to a local minimum? Based on the definition of
gradient vector,
(
)
F Φ
can be approximated by the first order expansion if the correction
term ∆Φ is small enough.
1
(
)
(
)
(
) |
t
t
t
F
F
F
+
=
≈
+ ∆
∗∇
Φ Φ
Φ
Φ
Φ
Φ
(4.40)
∆Φ can be expressed as the following term based on Eq. (4.39)
1
(
) |
t
t
t
t F
ε
+
=
∆
=
−
= −∇
Φ Φ
Φ
Φ
Φ
Φ
(4.41)
Thus, we can obtain the following equation:
8 You can use the logarithm of the object function to make it easier to compute the derivative in gradient descent.

154
Pattern Recognition
1
2
(
)
(
)
(
) |
,
(
) |
=
(
) |
0
t
t
t
t
t
t
t
F
F
F
F
F
ε
ε
+
=
=
=
−
= −
∇
∇
Φ
−
∇
<
Φ Φ
Φ Φ
Φ Φ
Φ
Φ
Φ
Φ
(4.42)
where
,x y
represents the inner product of two vectors, and
x
represents the Euclidean
norm of the vector. Equation (4.42) means that the gradient descent finds a new estimate
1
t+
Φ
that makes the value of the function
(
)
F Φ
decrease.
The gradient descent algorithm needs to go through an iterative hill-climbing proce-
dure to converge to the local minimum (estimate). Gradient descent usually requires many
iterations to converge. The algorithm usually stops when the change of the parameter
∆Φ becomes small enough. That is,
(
) |
t
t F
ε
λ
=
∇
<
Φ Φ
Φ
(4.43)
where λ is a preset threshold.
Based on the derivation above, the learning rate coefficient
tε must be small enough
for gradient descent to converge. However, if
tε
is too small, convergence is needlessly
slow. Thus, it is very important to choose an appropriate
tε . It is proved [47] [48] that gra-
dient converges almost surely if the learning rate coefficient
tε satisfies the following condi-
tion:
2
0
0
,
, and
>0
t
t
t
t
t
ε
ε
ε
∞
∞
=
=
= ∞
< ∞


(4.44)
One popular choice of
tε satisfying the above condition is
1
1
t
t
ε = +
(4.45)
Another way to find an appropriate
tε is through the second-order expansion:
1
1
(
)
(
)
(
) |
(
)
2
t
t
t
t
F
F
F
+
=
≈
+ ∆
∇
+
∆
∆
Φ Φ
Φ
Φ
Φ
Φ
Φ D Φ
(4.46)
where D is the Hessian matrix [23] of the second-order gradient operator where the i-th row
and j-th element
,i j
D
are given by the following partial derivative:
2
,
(
)
i j
i
j
F
D
∂
= ∂
∂
Φ
Φ Φ
(4.47)
By substituting ∆Φ from Eq. (4.41) into Eq. (4.46), we can obtain
2
1
2
1
(
)
(
)
2
t
t
t
t
t
F
F
F
F
F
ε
ε
+
≈
−
∇
+
∇
∇
Φ
Φ
D
(4.48)

Discriminative Training
155
From this, it follows that
tε can be chosen as follows to minimize
(
)
F Φ [23]:
2
t
t
F
F
F
ε
∇
= ∇
∇
D
(4.49)
Sometimes it is desirable to impose a different learning rate for the correct model vs. com-
peting models. Therefore re-estimation Eq. (4.39) can be generalized to the following form
[19, 48]:
1
(
) |
t
t
t
t
t
U
F
ε
+
=
=
−
∇
Φ Φ
Φ
Φ
Φ
(4.50)
where
t
U
is the learning bias matrix which is a positive definite matrix. One particular
choice of
t
U is
-1
D , where D is the Hessian matrix defined in Eq. (4.47). When the learning
rate is set to be 1.0, Eq. (4.50) becomes Newton’s algorithm, where the gradient descent is
chosen to minimize the second-order expansion. Equation (4.50) becomes:
1
-1
(
) |
t
t
t
F
+
=
=
−
∇
Φ Φ
Φ
Φ
D
Φ
(4.51)
When probabilistic parameters are iteratively re-estimated, probabilistic constraints
must be satisfied in each iteration as probability measure, such as:
1. For discrete distributions, all the values of the probability function ought to
be nonnegative. Moreover the sum of all discrete probability values needs to
be one, i.e.,
1
i
i
a =

2. For continuous distributions (assuming Gaussian density family), the vari-
ance needs to be nonnegative. For Gaussian mixtures, the diagonal covari-
ance entries need to be nonnegative and the sum of mixture weights needs to
be one, i.e.,
1
i
i
c =

In general, gradient descent is an unconstrained minimization (or maximization) proc-
ess that needs to be modified to accommodate constrained minimization (or maximization)
problems. The tricks to use are parameter transformations that implicitly maintain these con-
straints during gradient descent. The original parameters are updated through the inverse
transform from the transformed parameter space to the original parameter space. The trans-
formation is done in such a way that constraints on the original parameter are always main-
tained. Some of these transformations are listed as follows [48]:
1. For probabilities which need to be nonnegative and sum to one, like discrete
probability function and mixture weight, the following transformation can be
performed:
a
a
a
i
i
k
k
= 
exp(~ )
exp(~ )
(4.52)

156
Pattern Recognition
2. For mean µ and variance (or diagonal covariance entries)
2
σ , the following
transformation can be used.
µ
µσ
= 
(4.53)
exp( )
σ
σ
=

(4.54)
After the transformations, we can now compute the gradient with respect to the trans-
formed parameters (
, ,
)
ia µ σ

 
using the chain rules. Once the new estimate for the trans-
formed parameters is obtained through gradient descent, one can easily transform them back
to the original parameter domain.
4.3.2.
Minimum-Error-Rate Estimation
Parameter estimation techniques described so far aim to maximize either the likelihood
(class-conditional probability) (MLE and MAP) or the posterior probability (MMIE) in
Bayes’ equation, Eq. (4.1). Although the criteria used in those estimation methods all have
their own merit and under some conditions should lead to satisfactory results, the ultimate
parameter estimation criterion for pattern recognition should be made to minimize the rec-
ognition error rate (or the Bayes’ risk) directly. Minimum-error-rate estimation is also called
minimum-classification-error (MCE) training, or discriminative training. Similar to MMIE,
the algorithm generally tests the classifier using re-estimated models in the training proce-
dure, and subsequently improves the correct models and suppresses mis-recognized or near-
miss models.9 Neural networks are in this class. Although minimum-error-rate estimation
cannot be easily applied, it is still attractive that the criterion is identical to the goal of the
spoken language systems.
We have used the posterior probability
(
| )
i
p ω
x
in Bayes’ rule as the discriminant
function. In fact, just about any discriminant function can be used for minimum-error-rate
estimation. For example, as described in Section 4.1.2, a Bayes’ Gaussian classifier is
equivalent to a quadratic discriminant function. The goal now is to find the estimation of
parameters for a discriminant function family {
}
( )
id x
to achieve the minimum error rate.
One such error measure is defined in Eq. (4.5). The difficulty associated with the discrimina-
tive training approach lies in the fact that the error function needs to be consistent with the
true error rate measure and also suitable for optimization.10 Unfortunately, the error function
defined in Section 4.1.1 [Eq. (4.5)] is based on a finite set, which is a piecewise constant
function of the parameter vector Φ . It is not suitable for optimization.
To find an alternative smooth error function for MCE, let us assume that the discrimi-
nant function family contains s discriminant functions
( ,
)
id x Φ , i = 1, 2,…, s. Φ denotes
9 A near-miss model is the incorrect model that has highest likelihood other than that of the correct model.
10 In general, a function is optimizable if it is a smooth function and has a derivative.

Discriminative Training
157
the entire parameter set for s discriminant functions. We also assume that all the discrimi-
nant functions are nonnegative. We define the following error (misclassification) measure:
1/
1
( )
( ,
)
( ,
)
1
i
i
j
j i
e
d
d
s
η
η
≠


= −
+ 

−



x
x Φ
x Φ
(4.55)
where η is a positive number. The intuition behind the above measure is the attempt to
enumerate the decision rule. For a
i
ω
class input x,
( )
0
ie
>
x
implies recognition error;
while
( )
0
ie
≤
x
implies correct recognition. The number η can be thought to be a coeffi-
cient to select competing classes in Eq. (4.55). When
1
η = , the competing class term is the
average of all the competing discriminant function scores. When η →∞, the competing
class term becomes max
( ,
)
j
j i d
≠
x Φ
representing the discriminant function score for the top
competing class. By varying the value of η , one can take all the competing classes into ac-
count based on their individual significance.
To transform
( )
ie x
into a normalized smooth function, we can use the sigmoid func-
tion to embed
( )
ie x
in a smooth zero-one function. The loss function can be defined as fol-
lows:
( ;
)
( ( ))
i
i
l
sigmoid e
=
x Φ
x
(4.56)
where
1
( )
1
x
sigmoid x
e−
=
+
(4.57)
When
( )
ie x
is a big negative number, which indicates correct recognition, the loss function
( ;
)
il x Φ
has a value close to zero, which implies no loss incurred. On the other hand, when
( )
ie x
is a positive number, it leads to a value between zero and one that indicates the likeli-
hood of an error. Thus
( ;
)
il x Φ essentially represents a soft recognition error count.
For any data sample x, the recognizer’s loss function can be defined as:
1
( ,
)
( ,
) (
)
s
i
i
i
l
l
δ ω
ω
=
=
=

x Φ
x Φ
(4.58)
where
( )
δ •
is a Boolean function which will return 1 if the argument is true and 0 if the
argument is false. Since X is a random vector, the expected loss according to Eq. (4.58) can
be defined as:
1
(
)
( ( ,
))
( ,
) ( )
i
s
i
L
E
l
l
p
d
ω ω
=
=
=
= 
X
Φ
x Φ
x Φ
x x
(4.59)

158
Pattern Recognition
Since max
( ,
)
max
( ,
)
f
d
f
d



 =






Φ
Φ
x Φ
x
x Φ
x , Φ can be estimated by gradient descent
over
( ,
)
l x Φ
instead of expected loss
(
)
L Φ . That is, minimum classification error training
of parameter Φ can be estimated by first choosing an initial estimate
0
Φ and the following
iterative estimation equation:
1
( ,
) |
t
t
t
t l
ε
+
=
=
−
∇
Φ Φ
Φ
Φ
x Φ
(4.60)
You can follow the gradient descent procedure described in Section 4.3.1.1 to achieve the
MCE estimate of Φ .
Both MMIE and MCE are much more computationally intensive than MLE, owing to
the inefficiency of gradient descent algorithms. Therefore, discriminant estimation methods,
like MMIE and MCE, are usually used for tasks containing few classes or data samples. A
more pragmatic approach is corrective training [6], which is based on a very simple error-
correcting procedure. First, a labeled training set is used to train the parameters for each
corresponding class by standard MLE. For each training sample, a list of confusable classes
is created by running the recognizer and kept as its near-miss list. Then, the parameters of
the correct class are moved in the direction of the data sample, while the parameters of the
“near-miss” class are moved in the opposite direction of the data samples. After all training
samples have been processed; the parameters of all classes are updated. This procedure is
repeated until the parameters for all classes converge. Although there is no theoretical proof
that such a process converges, some experimental results show that it outperforms both
MLE and MMIE methods [4].
We have described various estimators: minimum mean square estimator, maximum
likelihood estimator, maximum posterior estimator, maximum mutual information estimator,
and minimum error estimator. Although based on different training criteria, they are all
powerful estimators for various pattern recognition problems. Every estimator has its
strengths and weaknesses. It is almost impossible always to favor one over the others. In-
stead, you should study their characteristics and assumptions and select the most suitable
ones for the domains you are working on.
In the following section we discuss neural networks. Both neural networks and MCE
estimations follow a very similar discriminant training framework.
4.3.3.
Neural Networks
In the area of pattern recognition, the advent of new learning procedures and the availability
of high-speed parallel supercomputers have given rise to a renewed interest in neural net-
works.11 Neural networks are particularly interesting for speech recognition, which requires
massive constraint satisfaction, i.e., the parallel evaluation of many clues and facts and their
interpretation in the light of numerous interrelated constraints. The computational flexibility
of the human brain comes from its large number of neurons in a mesh of axons and den-
drites. The communication between neurons is via the synapse and afferent fibers. There are
11 A neural network is sometimes called an artificial neural network (ANN), a neural net, or a connectionist model.

Discriminative Training
159
many billions of neural connections in the human brain. At a simple level it can be consid-
ered that nerve impulses are comparable to the phonemes of speech, or to letters, in that they
do not themselves convey meaning but indicate different intensities [95, 101] that are inter-
preted as meaningful units by the language of the brain. Neural networks attempt to achieve
real-time response and human like performance using many simple processing elements
operating in parallel as in biological nervous systems. Models of neural networks use a par-
ticular topology for the interactions and interrelations of the connections of the neural units.
In this section we describe the basics of neural networks, including the multi-layer percep-
trons and the back-propagation algorithm for training neural networks.
4.3.3.1.
Single-Layer Perceptrons
Figure 4.10 shows a basic single-layer perceptron. Assuming there are N inputs, labeled as
1
2
,
,
,
N
x x
x

, we can form a linear function with weights
0
1
2
,
,
,
,
j
j
j
Nj
w
w
w
w

to give the
output
jy , defined as
0
1
( )
N
t
j
ij
i
j
j
i
y
w
w x
g
=
=
+
=
=

w x
x
(4.61)
where
0
1
2
(
,
,
,
,
)
j
j
j
j
Nj
w
w
w
w
=
w

and
1
2
(1,
,
,
,
)
N
x x
x
=
x

.
For pattern recognition purposes, we associate each class
j
ω
out of s classes
1
2
(
,
,
,
)
s
ω ω
ω

with such a linear discriminant function
( )
j
g
x . By collecting all the dis-
criminant functions, we will have the following matrix representation:
( )
t
=
=
y
g x
W x
(4.62)
where
1
2
1
2
( )
(
( ),
( ),
,
( )) ;
(
,
,
,
)
t
t
t
t
s
s
g
g
g
=
=
g x
x
x
x
W
w w
w


and
1
2
(
,
,
,
)t
s
y y
y
=
y

. The
pattern recognition decision can then be based on these discriminant functions as in Bayes’
decision theory. That is,
iff
arg max
( )
k
i
i
k
g
ω
∈
=
x
x
(4.63)
The perceptron training algorithm [68], guaranteed to converge for linearly separable
classes, is often used for training the weight matrix W . The algorithm basically divides the
sample space
N
ℜ
into regions of corresponding classes. The decision boundary is character-
ized by hyper-planes of the following form:
( )
( )
0
i
j
g
g
i
j
−
=
∀≠
x
x
(4.64)
Using linear discriminant functions is equivalent to estimating Bayes’ Gaussian densi-
ties in which all densities are sharing the same covariance matrix. Unfortunately, for data
samples that are not linearly separable, the perceptron algorithm does not converge. How-
ever, if we can relax the definition of classification errors in this case, we can still use a
powerful algorithm to train the weight matrix W . This approach is the Least Square Error

160
Pattern Recognition
(LSE) algorithm described in Chapter 3, which aims at minimizing sum-of-squared-error
(SSE) criterion, instead of minimizing the classification errors. The sum-of-squared-error is
defined as:
2
SSE
|| ( )
||
i
i
i
ω
δ
∈
=
−

x
g x
(4.65)
where
iδ is an M-dimensional index vector with all zero components except that the ith one
is 1.0, since the desired output for
( )
g x
is typically equal to 1.0 if
i
ω
∈
x
and 0 if
i
ω
∉
x
.
Figure 4.10 A single-layer perceptron.
The use of LSE leads to discriminant functions that have real outputs approximating
the values 1 or 0. Suppose there are S input vectors
1
2
(
,
,
,
)
t
t
t
S
=
X
x x
x

in the training set.
Similar to the LSE for linear functions described in Chapter 3 (cf. Section 3.2.1.2), the LSE
estimate of weight matrix W will have the following closed form:
1
((
))
t
−
=
Σ
W
XX
L
(4.66)
where
L
is
a
(
1)
N
s
+
×
matrix
where
the
k-th
column
is
the
mean
vector
1
2
(1,
,
,
,
)t
k
k
k
kN
µ
µ
µ
=
µ

of all the vectors classified into class
k
ω , and Σ is an s
s
×
di-
agonal matrix with diagonal entry
,
k k
c
representing the number of vectors classified into
k
ω . LSE estimation using linear discriminant functions is equivalent to estimating Bayes’
Gaussian densities where all the densities are assumed to share the same covariance matrix
[98].
Although the use of LSE algorithm solves the convergence problems, it loses the
power of nonlinear logical decision (i.e., minimizing the classification error rate), since it is
only approximating the simple logical decision between alternatives. An alternative ap-
proach is to use a smooth and differential sigmoid function as the threshold function:
w j
0
wNj
w j
2
w j
1

y j
x1
x2
xN
input layer
output layer

Discriminative Training
161
1
2
1
2
( ( ))
((
( ),
( ),
,
( )) )
(
(
( )),
(
( )),
,
(
( )))
t
s
t
s
sigmoid
sigmoid
g
g
g
sigmoid g
sigmoid g
sigmoid g
=
=
=
y
g x
x
x
x
x
x
x


(4.67)
where
( )
sigmoid x is the sigmoid function defined in Eq. (4.57). With the sigmoid function,
the following new sum-of-squared-error term closely tracks the classification error:
2
NSSE
||
( ( ))
||
i
i
i
sigmoid
ω
δ
∈
=
−
 
x
g x
(4.68)
where
iδ is the same index vector defined above. Since there is no analytic way of minimiz-
ing a nonlinear function, the use of the sigmoid threshold function requires an iterative gra-
dient descent algorithm, back-propagation, which will be described in the next section.
Figure 4.11 A multi-layer perceptron with four total layers. The middle two layers are hidden.
4.3.3.2.
Multi-Layer Perceptron
One of the technical developments sparking the recent resurgence of interest in neural net-
works has been the popularization of multi-layer perceptrons (MLP) [37, 90]. Figure 4.11
shows a multi-layer perceptron. In contrast to a single-layer perceptron, it has two hidden
x2


y j

x1
xN
hidden layer
hidden layer
input layer
output layer

162
Pattern Recognition
layers. The hidden layers can be viewed as feature extractors. Each layer has the same com-
putation models as the single-layer perceptron; i.e., the value of each node is computed as a
linear weighted sum of the input nodes and passed to a sigmoid type of threshold function.
1
1
1
2
2
1
2
1
2
2
(
( ))
(
)
(
(
))
(
)
(
(
))
(
)
t
h
h
t
h
h
t
y
y
sigmoid
sigmoid
sigmoid
sigmoid
sigmoid
sigmoid
=
=
=
=
=
=
h
g
x
W x
h
g
h
W h
y
g
h
W h
(4.69)
where
( )
sigmoid x is the sigmoid function defined in Eq. (4.57).
According to Eq. (4.69), we can propagate the computation from input layer to output
layer and denote the output layer as a nonlinear function of the input layer.
( )
MLP
=
Y
x
(4.70)
Let’s denote
( )
O x
as the desired output for input vector x . For pattern classification,
( )
O x
will be an s-dimensional vector with the desired output pattern set to one and the re-
maining patterns set to zero. As we mentioned before, there is no analytic way to minimize
the mean square error
2
||
( )
( ) ||
E
MLP
O
=
−

x
x
x
. Instead, an iterative gradient descent algo-
rithm called back propagation [89, 90] needs to be used to reduce to error. Without loss of
generality, we assume there is only one input vector
1
2
(1,
,
,
,
)
N
x x
x
=
x

with desired output
1
2
( ,
,
,
)
s
o o
o
=
o

. All the layers in the MLP are numbered 0, 1, 2,… upward from the input
layer. The back propagation algorithm can then be described as in Algorithm 4.1.
In computing the partial derivative
( )
k
ij
E
w t
∂
∂
, you need to use the chain rule.
K
ij
w
is the
weight connecting the output layer and the last hidden layer; the partial derivative is:
2
1
2
1
0
1
1
1
0
1
1
(
(
) )
(
(
) )
(
)
(
)
2(
)
(
1)
s
i
i
i
K
K
ij
ij
s
N
K
K
K
i
i
j
ij
i
j
i
i
N
K
K
K
K
j
ij
j
ij
i
i
K
i
i
j
i
i
y
o
E
w
w
y
o
w
w v
y
y
w
w
w v
y
o y
y
v
=
−
=
=
−
=
−
∂
−
∂
=
∂
∂
∂
−
∂
+
∂
=
×
×
∂
∂
∂
+
=
−
−




(4.71)
For layers
1,
2,
k
K
K
=
−
−
, one can apply chain rules similarly for gradient
( )
k
ij
E
w t
∂
∂
.
The back propagation algorithm is a generalization of the minimum mean squared er-
ror (MMSE) algorithm. It uses a gradient search to minimize the difference between the

Unsupervised Estimation Methods
163
desired outputs and the actual net outputs, where the optimized criterion is directly related to
pattern classification. With initial parameters for the weights, the training procedure is then
repeated to update the weights until the cost function is reduced to an acceptable value or
remains unchanged. In the algorithm described above, we assume a single training example.
In real-world application, these weights are estimated from a large number of training ob-
servations in a manner similar to hidden Markov modeling. The weight updates in the Step 3
are accumulated over all the training data. The actual gradient is then estimated for the com-
plete set of training data before the starting of the next iteration. Note that the estimation
criterion for neural networks is directly related to classification rather than the maximum
likelihood.
ALGORITHM 4.1 THE BACK PROPAGATION ALGORITHM
Step 1: Initialization: Set
0
t =
and choose initial weight matrices W for each layer. Let’s de-
note
( )
k
ij
w t as the weighting coefficients connecting
thi
input node in layer
1
k −
and
thj
out-
put node in layer k at time t .
Step 2: Forward Propagation: Compute the values in each node from input layer to output layer
in a propagating fashion, for k = 1 to K
1
0
1
(
( )
( )
)
N
k
k
k
j
j
ij
i
i
v
sigmoid w
t
w t v
j
−
=
=
+
∀

(4.72)
where
1
( )
1
x
sigmoid x
e−
=
+
and
k
jv
is denotes as the
thj
node in the
th
k
layer
Step 3: Back Propagation: Update the weights matrix for each layer from output layer to input
layer according to:
(
1)
( )
( )
k
k
ij
ij
k
ij
E
w t
w t
w t
α
∂
+
=
−
∂
(4.73)
where
2
1
||
||
s
i
i
i
E
y
o
=
=
−

and
1
2
(
,
,
)
s
y y
y

is the computed output vector in Step 2.
α is referred to as the learning rate and has to be small enough to guarantee
convergence. One popular choice is 1 (
1)
t +
.
Step 4: Iteration: Let t = t +1 Repeat Steps 2 and 3 until some convergence condition is met.
4.4.
UNSUPERVISED ESTIMATION METHODS
As described in Section 4.2, in unsupervised learning, information about class ω of the data
sample x is unavailable. Data observed are incomplete since the class data ω is missing.
One might wonder why we are interested in such an unpromising problem, and whether or
not it is possible to learn anything from incomplete data. Interestingly enough, the formal

164
Pattern Recognition
solution to this problem is almost identical to the solution for the supervised learning case –
MLE. We discuss vector quantization (VQ), which uses principles similar to the EM algo-
rithm. It is important in its own right in spoken language systems.
4.4.1.
Vector Quantization
As described in Chapter 3, source coding refers to techniques that convert the signal source
into a sequence of bits that are transmitted over a communication channel and then used to
reproduce the original signal at a different location or time. In speech communication, the
reproduced sound usually allows some acceptable level of distortion to achieve low bit rate.
The goal of source coding is to reduce the number of bits necessary to transmit or store data,
subject to a distortion or fidelity criterion, or equivalently, to achieve the minimum possible
distortion for a prescribed bit rate. Vector quantization (VQ) is one of the most efficient
source-coding techniques
Quantization is the process of approximating continuous amplitude signals by discrete
symbols. The quantization of a single signal value or parameter is referred to as scalar quan-
tization. In contrast, joint quantization of multiple signal values or parameters is referred to
as vector quantization. Conventional pattern recognition techniques have been used effec-
tively to solve the quantization or data compression problem with successful application to
speech coding, image coding, and speech recognition [36, 85]. In both speech recognition
and synthesis systems, vector quantization serves an important role in many aspects of the
systems, ranging from discrete acoustic prototypes of speech signals for the discrete HMM,
to robust signal processing and data compression.
A vector quantizer is described by a codebook, which is a set of fixed prototype vec-
tors or reproduction vectors. Each of these prototype vectors is also referred to as a code-
word. To perform the quantization process, the input vector is matched against each code-
word in the codebook using some distortion measure. The input vector is then replaced by
the index of the codeword with the smallest distortion. Therefore, a description of the vector
quantization process includes:
1. the distortion measure;
2. the generation of each codeword in the codebook.
4.4.1.1.
Distortion Measures
Since vectors are replaced by the index of the codeword with smallest distortion, the trans-
mitted data can be recovered only by replacing the code index sequence with the corre-
sponding codeword sequence. This inevitably causes distortion between the original data
and the transmitted data. How to minimize the distortion is thus the central goal of vector
quantization. This section describes a couple of the most common distortion measures.
Assume that
(
)
1
2
,
,
,
t
d
d
x x
x
R
=
∈
x

is a d-dimensional vector whose components
{
}
,1
kx
k
d
≤
≤
are real-valued, continuous-amplitude random variables. After vector quanti-

Unsupervised Estimation Methods
165
zation, the vector x is mapped (quantized) to another discrete-amplitude d-dimensional vec-
tor z.
( )
q
=
z
x
(4.74)
In Eq. (4.74) q() is the quantization operator. Typically, z is a vector from a finite set
{
}
,1
j
j
M
=
≤
≤
Z
z
, where
jz
is also a d-dimensional vector. The set Z is referred to as the
codebook, M is the size of the codebook, and
jz
is jth codeword. The size M of the code-
book is also called the number of partitions (or levels) in the codebook. To design a code-
book, the d-dimensional space of the original random vector x can be partitioned into M
regions or cells {
}
,1
i
C
i
M
≤≤
, and each cell
i
C is associated with a codeword vector
iz .
VQ then maps (quantizes) the input vector x to codeword
iz if x lies in
i
C . That is,
( )
if
i
i
q
C
=
∈
x
z
x
(4.75)
An example of partitioning of a two-dimensional space (d = 2) for the purpose of vec-
tor quantization is shown in Figure 4.12. The shaded region enclosed by the dashed lines is
the cell
i
C . Any input vector x that lies in the cell
i
C is quantized as
iz . The shapes of the
various cells can be different. The positions of the codewords within each cell are shown by
dots in Figure 4.12. The codeword
iz
is also referred to as the centroid of the cell
i
C be-
cause it can be viewed as the central point of the cell
i
C .
When x is quantized as z, a quantization error results. A distortion measure d(x, z) can
be defined between x and z to measure the quantization quality. Using this distortion meas-
ure, Eq. (4.75) can be reformulated as follows:
argmin
( )
if and only if
( ,
)
k
i
k
q
i
d
=
=
x
z
x z
(4.76)
The distortion measure between x and z is also known as a distance measure in the
speech context. The measure must be tractable in order to be computed and analyzed, and
also must be subjectively relevant so that differences in distortion values can be used to in-
dicate differences in original and transmitted signals. The most commonly used measure is
the Euclidean distortion measure, which assumes that the distortions contributed by quantiz-
ing the different parameters are equal. Therefore, the distortion measure d(x, z) can be de-
fined as follows:
2
1
( , )
(
) (
)
(
)
d
t
i
i
i
d
x
z
=
=
−
−
=
−

x z
x
z
x
z
(4.77)
The distortion defined in Eq. (4.77) is also known as sum of squared error. In general,
unequal weights can be introduced to weight certain contributions to the distortion more
than others. One choice for weights that is popular in many practical applications is to use
the inverse of the covariance matrix of z.
1
( , )
(
)
(
)
t
d
−
=
−
Σ
−
x z
x
z
x
z
(4.78)

166
Pattern Recognition
This distortion measure, known as the Mahalanobis distance, is actually the exponential
term in a Gaussian density function.
zi
Ci
Figure 4.12 Partitioning of a two-dimensional space into 16 cells.
Another way to weight the contributions to the distortion measure is to use perceptu-
ally-based distortion measures. Such distortion measures take advantage of subjective judg-
ments of perceptual difference caused by two different signals. A perceptually-based distor-
tion measure has the property that signal changes that make the sounds being perceived dif-
ferent should be associated with large distances. Similarly signal changes that keep the
sound perceived the same should be associated with small distances. A number of perceptu-
ally based distortion measures have been used in speech coding [3, 75, 76].
4.4.1.2.
The K-Means Algorithm
To design an M-level codebook, it is necessary to partition d-dimensional space into M cells
and associate a quantized vector with each cell. Based on the source-coding principle, the
criterion for optimization of the vector quantizer is to minimize overall average distortion
over all M-levels of the VQ. The overall average distortion can be defined by
[
]
[
]
1
1
1
(
)
(
)
( ,
)|
(
)
( ,
) ( |
)
i
M
i
i
i
i
M
M
i
i
i
i
x C
i
i
D
E d
p
C E d
C
p
C
d
p
C d
D
=
∈
=
=
=
=
∈
∈
=
∈
∈
=




x,z
x
x z
x
x
x z
x x
x
(4.79)

Unsupervised Estimation Methods
167
where the integral is taken over all components of vector x;
(
)
i
p
C
∈
x
denotes the prior
probability of codeword
iz ;
( |
)
i
p
C
∈
x x
denotes the multidimensional probability density
function of x in cell
i
C ; and
i
D is the average distortion in cell
i
C . No analytic solution
exists to guarantee global minimization of the average distortion measure for a given set of
speech data. However, an iterative algorithm, which guarantees a local minimum, exists and
works well in practice.
We say a quantizer is optimal if the overall average distortion is minimized over all M-
levels of the quantizer. There are two necessary conditions for optimality. The first is that
the optimal quantizer is realized by using a nearest-neighbor selection rule as specified by
Eq. (4.76). Note that the average distortion for each cell
i
C
[
]
( ,
) |
i
i
E d
C
∈
x z
x
(4.80)
can be minimized when
iz is selected such that
( ,
)
i
d x z
is minimized for x. This means that
the quantizer must choose the codeword that results in the minimum distortion with respect
to x. The second condition for optimality is that each codeword
iz is chosen to minimize the
average distortion in cell
i
C . That is,
iz is the vector that minimizes
[
]
(
)
(
)|
i
i
i
D
p
E d
C
=
∈
z
x,z x
(4.81)
Since the overall average distortion D is a linear combination of average distortions in
i
C , they can be independently computed after classification of x. The vector
iz is called the
centroid of the cell
i
C and is written
cent(
)
i
i
C
=
z
(4.82)
The centroid for a particular region (cell) depends on the definition of the distortion
measure. In practice, given a set of training vectors {
}
, 1
t
t
T
≤≤
x
, a subset of
i
K vectors
will be located in cell
i
C . In this case,
( |
)
i
p x z
can be assumed to be 1/
i
K , and
(
)
i
p z
be-
comes
/
i
K
T . The average distortion
i
D in cell
i
C can then be given by
1
(
)
i
i
i
C
D
d
T
∈
=

x
x,z
(4.83)
The second condition for optimality can then be rewritten as follows:
1
ˆ
(
)
(
)
arg min
argmin
i
i
i
i
i
i
i
C
D
d
T
∈
=
=

z
z
x
z
z
x,z
(4.84)
When the sum of squared error in Eq. (4.77) is used for the distortion measure, the at-
tempt to find such ˆ iz
to minimize the sum of squared error is equivalent to least squared

168
Pattern Recognition
error estimation, which was described in Chapter 3. Minimization of
i
D in Eq. (4.84) with
respect to
iz is given by setting the derivative of
i
D to zero:
1
(
) (
)
1
(
) (
)
2
(
)
0
i
i
i
i
i
i
t
i
i
i
C
t
i
i
C
i
C
D
T
T
T
∈
∈
∈
∇
= ∇
−
−
=
∇
−
−
−
=
−
=



z
z
x
z
x
x
x
z
x
z
x
z
x
z
x
z
(4.85)
By solving Eq. (4.85), we obtain the least square error estimate of centroid ˆ iz simply as the
sample mean of all the training vectors x, quantized to cell
i
C :
1
ˆ
i
i
C
i
K
∈
=

x
z
x
(4.86)
If the Mahalanobis distance measure (Eq. (4.78)) is used, minimization of
i
D in Eq.
(4.84) can be done similarly:
1
1
1
1
(
)
(
)
1
(
)
(
)
2
(
)
0
i
i
i
i
i
i
t
i
i
i
C
t
i
i
C
i
C
D
T
T
T
−
∈
−
∈
−
∈
∇
= ∇
−
Σ
−
=
∇
−
Σ
−
−
=
Σ
−
=



z
z
x
z
x
x
x
z
x
z
x
z
x
z
x
z
(4.87)
and centroid ˆiz is obtained from
1
ˆ
i
i
C
i
K
∈
=

x
z
x
(4.88)
One can see that ˆ iz is again the sample mean of all the training vectors x, quantized to cell
i
C . Although Eq. (4.88) is obtained based on the Mahalanobis distance measure, it also
works with a large class of Euclidean-like distortion measures [61]. Since the Mahalanobis
distance measure is actually the exponential term in a Gaussian density, minimization of the
distance criterion can be easily translated into maximization of the logarithm of the Gaussian
likelihood. Therefore, similar to the relationship between least square error estimation for
the linear discrimination function and the Gaussian classifier described in Section 4.3.3.1,
the distance minimization process (least square error estimation) above is in fact a maximum
likelihood estimation.

Unsupervised Estimation Methods
169
According to these two conditions for VQ optimality, one can iteratively apply the
nearest-neighbor selection rule and Eq. (4.88) to get the new centroid ˆ iz
for each cell in
order to minimize the average distortion measure. This procedure is known as the k-means
algorithm or the generalized Lloyd algorithm [29, 34, 56]. In the k-means algorithm, the
basic idea is to partition the set of training vectors into M clusters
i
C (
)
1
i
M
≤≤
in such a
way that the two necessary conditions for optimality described above are satisfied. The k-
means algorithm can be described as follows:
ALGORITHM 4.2: THE K-MEANS ALGORITHM
Step 1: Initialization: Choose some adequate method to derive initial VQ codewords
(
, 1
i
i
M
≤≤
z
) in the codebook.
Step 2: Nearest-neighbor Classification: Classify each training vector {
kx } into one of the cell
s
i
C by choosing the closest codeword
iz (
, iff
(
)
(
)
i
i
j
C
d
d
∈
≤
x
x,z
x,z
for all j
i
≠). This
classification is also called minimum-distance classifier.
Step 3: Codebook Updating: Update the codeword of every cell by computing the centroid of
the training vectors in each cell according to Eq. (4.84) ( ˆ
(
), 1
i
i
cent C
i
M
=
≤≤
z
).
Step 4: Iteration: Repeat steps 2 and 3 until the ratio of the new overall distortion D at the cur-
rent iteration relative to the overall distortion at the previous iteration is above a preset thresh-
old.
In the process of minimizing the average distortion measure, the k-means procedure
actually breaks the minimization process into two steps. Assuming that the centroid
iz
(or
mean) for each cell
i
C has been found, then the minimization process is found simply by
partitioning all the training vectors into their corresponding cells according to the distortion
measure. After all of the new partitions are obtained, the minimization process involves
finding the new centroid within each cell to minimize its corresponding within-cell average
distortion
i
D based on Eq. (4.84). By iterating over these two steps, a new overall distortion
D smaller than that of the previous step can be obtained.
Theoretically, the k-means algorithm can converge only to a local optimum [56].
Furthermore, any such solution is, in general, not unique [33]. Initialization is often critical
to the quality of the eventual converged codebook. Global optimality may be approximated
by repeating the k-means algorithm for several sets of codebook initialization values, and
then one can choose the codebook that produces the minimum overall distortion. In the next
subsection we will describe methods for finding a decent initial codebook.
4.4.1.3.
The LBG Algorithm
Since the initial codebook is critical to the ultimate quality of the final codebook, it has been
shown that it is advantageous to design an M-vector codebook in stages. This extended k-

170
Pattern Recognition
means algorithm is known as the LBG algorithm proposed by Linde, Buzo, and Gray [56].
The LBG algorithm first computes a 1-vector codebook, then uses a splitting algorithm on
the codewords to obtain the initial 2-vector codebook, and continues the splitting process
until the desired M-vector codebook is obtained. The procedure is formally implemented by
the following algorithm:
ALGORITHM 4.3: THE LBG ALGORITHM
Step 1: Initialization: Set M (number of partitions or cells) =1. Find the centroid of all the train-
ing data according to Eq. (4.84).
Step 2: Splitting: Split M into 2M partitions by splitting each current codeword by finding two
points that are far apart in each partition using a heuristic method, and use these two points as
the new centroids for the new 2M codebook. Now set M = 2M.
Step 3: K-means Stage: Now use the k-means iterative algorithm described in the previous
section to reach the best set of centroids for the new codebook.
Step 4: Termination: If M equals the VQ codebook size required, STOP; otherwise go to Step
2.
4.4.2.
The EM Algorithm
We introduce the EM algorithm that is important to hidden Markov models and other learn-
ing techniques. It discovers model parameters by maximizing the log-likelihood of incom-
plete data and by iteratively maximizing the expectation of log-likelihood from complete
data. The EM algorithm is a generalization of the VQ algorithm described above.
The EM algorithm can also be viewed as a generalization of the MLE method, when
the data observed is incomplete. Without loss of generality, we use scale random variables
here to describe the EM algorithm. Suppose we observe training data y. In order to deter-
mine the parameter vector Φ that maximizes
(
)
|
P Y
y
=
Φ , we would need to know some
hidden data x (that is unobserved). For example, x may be a hidden number that refers to
component densities of observable data y, or x may be the underlying hidden state sequence
in hidden Markov models (as discussed in Chapter 8). Without knowing this hidden data x,
we could not easily use the maximum likelihood estimation to estimate
ˆΦ , which maxi-
mizes
(
)
|
P Y
y
=
Φ . Instead, we assume a parameter vector Φ and estimate the probability
that each x occurred in the generation of y. This way we can pretend that we had in fact ob-
served a complete data pair (x, y), with frequency proportional to the probability
(
)
,
|
P X
x Y
y
=
=
Φ , to compute a new Φ , the maximum likelihood estimate of Φ . We
can then set the parameter vector Φ to be this new Φ and repeat the process to iteratively
improve our estimate.
The issue now is whether or not the process (EM algorithm) described above con-
verges. Without loss of generality, we assume that both random variables X (unobserved)
and Y (observed) are discrete random variables. According to Bayes rule,

Unsupervised Estimation Methods
171
(
,
|
)
(
|
,
) (
|
)
P X
x Y
y
P X
x Y
y
P Y
y
=
=
Φ =
=
=
Φ
=
Φ
(4.89)
Our goal is to maximize the log-likelihood of the observable, real data y generated by pa-
rameter vector Φ . Based on Eq. (4.89), the log-likelihood can be expressed as follows:
log
(
|
)
log
(
,
|
)
log
(
|
,
)
P Y
y
P X
x Y
y
P X
x Y
y
=
Φ =
=
=
Φ −
=
=
Φ
(4.90)
Now, we take the conditional expectation of log
(
|
)
P Y
y
=
Φ
over X computed with pa-
rameter vector Φ :
(
)
|
[log
(
|
)]
(
|
,
)log
(
|
)
log
(
|
)
X Y
y
x
E
P Y
y
P X
x Y
y
P Y
y
P Y
y
Φ
=
=
Φ
=
=
=
Φ
=
Φ
=
=
Φ

(4.91)
where we denote
|
[ ]X Y
y
E
f
Φ
=
as the expectation of function f over X computed with parame-
ter vector Φ . Then using Eq. (4.90) and (4.91) , the following expression is obtained:
|
|
log
(
|
)
[log
(
,
|
)]
[log
(
|
,
)]
( ,
)
( ,
)
X Y
y
X Y
y
P Y
y
E
P X Y
y
E
P X Y
y
Q
H
Φ
=
Φ
=
=
Φ =
=
Φ
−
=
Φ
=
Φ Φ −
Φ Φ
(4.92)
where
(
)
|
( ,
)
[log
(
,
|
)]
(
|
,
)log
(
,
|
)
X Y
y
x
Q
E
P X Y
y
P X
x Y
y
P X
x Y
y
Φ
=
Φ Φ =
=
Φ
=
=
=
Φ
=
=
Φ

(4.93)
and
(
)
|
( ,
)
[log
(
|
,
)]
(
|
,
)log
(
|
,
)
X Y
y
x
H
E
P X Y
y
P X
x Y
y
P X
x Y
y
Φ
=
Φ Φ =
=
Φ
=
=
=
Φ
=
=
Φ

(4.94)
The convergence of the EM algorithm lies in the fact that if we choose Φ so that
( ,
)
( ,
)
Q
Q
Φ Φ ≥
Φ Φ
(4.95)
then
log
(
|
)
log
(
|
)
P Y
y
P Y
y
=
Φ ≥
=
Φ
(4.96)
since it follows from Jensen's inequality that
( ,
)
( ,
)
H
H
Φ Φ ≤
Φ Φ
[21]. The function
( ,
)
Q Φ Φ
is known as the Q-function or auxiliary function. This fact implies that we can
maximize the Q-function, which is the expectation of log-likelihood from complete data pair
(x, y), to update parameter vector from Φ to Φ , so that the incomplete log-likelihood

172
Pattern Recognition
( ,
)
L x Φ
increases monotonically. Eventually, the likelihood will converge to a local maxi-
mum if we iterate the process.
The name of the EM algorithm comes from E for expectation and M for maximization.
The implementation of the EM algorithm includes the E (expectation) step, which calculates
the auxiliary Q-function
( ,
)
Q Φ Φ
and the M (maximization) step, which maximizes
( ,
)
Q Φ Φ over Φ to obtain ˆΦ . The general EM algorithm can be described in the following
way.
ALGORITHM 4.4: THE EM ALGORITHM
Step 1: Initialization: Choose an initial estimate Φ .
Step 2: E-Step: Compute auxiliary Q-function
( ,
)
Q Φ Φ
(which is also the expectation of log-
likelihood from complete data) based on Φ .
Step 3: M-Step: Compute ˆ
arg max
( ,
)
Q
Φ
Φ =
Φ Φ to maximize the auxiliary Q-function.
Step 4: Iteration: Set
ˆ
Φ = Φ , repeat from Step 2 until convergence.
The M-step of the EM algorithm is actually a maximum likelihood estimation of com-
plete data (assuming we know the unobserved data x based on observed data y and initial
parameter vector Φ ). The EM algorithm is usually used in applications where no analytic
solution exists for maximization of log-likelihood of incomplete data. Instead, the Q-
function is iteratively maximized to obtain the estimation of parameter vector.
4.4.3.
Multivariate Gaussian Mixture Density Estimation
The vector quantization process described in Section 4.4.1 partitions the data space into
separate regions based on some distance measure regardless of the probability distributions
of original data. This process may introduce errors in partitions that could potentially de-
stroy the original structure of data. An alternative way for modeling a VQ codebook is to use
a family of Gaussian probability density functions, such that each cell will be represented by
a (Gaussian) probability density function as shown in Figure 4.13. These probability density
functions can then overlap, rather than partition, in order to represent the entire data space.
The objective for a mixture Gaussian VQ is to maximize the likelihood of the observed data
(represented by the product of the Gaussian mixture scores) instead of minimizing the over-
all distortion. The centroid of each cell (the mean vectors of each Gaussian pdf) obtained via
such a representation may be quite different from that obtained using the traditional k-mean
algorithm, since the distribution properties of the data are taken into account.

Unsupervised Estimation Methods
173
zi
Ci
Figure 4.13 Partitioning of a two-dimensional space with 12 Gaussian density functions.
There should be an obvious analogy between the EM algorithm and the k-means algo-
rithm described in the Section 4.4.1.2. In the k-means algorithm, the class information for
the observed data samples is hidden and unobserved, so an EM-like algorithm instead of
maximum likelihood estimate needs to be used. Therefore, instead of a single process of
maximum likelihood estimation, the k-means algorithm first uses the old codebook to find
the nearest neighbor for each data sample followed by maximum likelihood estimation of
the new codebook and iterates the process until the distortion stabilizes. The steps 2 and 3 in
the k-means algorithm are actually the E and M steps in the EM algorithm respectively.
Mixture density estimation [41] is a typical example of EM estimation. In the mixtures
of Gaussian density, the probability density for observable data y is the weighted sum of
each Gaussian component.
1
1
( |
)
( |
)
( |
,
)
K
K
k
k
k
k
k
k
k
k
k
p
c p
c N
=
=
=
=
Σ


y Φ
y Φ
y µ
(4.97)
where
1
0
1, for 1
and
1
K
k
k
k
c
k
K
c
=
≤
≤
≤
≤
=

.
Unlike the case of a single Gaussian estimation, we also need to estimate the mixture
weight
kc . In order to do so, we can assume that observable data y come from one of the
component densities
( |
)
X
X
p
φ
y
, where X is a random variable taking value from {1,2,
}
K

to indicate the Gaussian component. It is clear that x is unobserved and used to specify the

174
Pattern Recognition
pdf component
X
φ . Assuming that the probability density function for complete data (x,y) is
given by the joint probability:
( ,
|
)
(
)
( |
)
(
)
( |
,
)
x
x
k
k
k
p
x
P X
x p
P X
x N
=
=
=
=
Σ
y
Φ
y Φ
y µ
(4.98)
(
)
P X
x
=
can be regarded as the probability of the unobserved data x used to specify the
component density
( |
)
x
x
p y Φ
from which the observed data y is drawn. If we assume the
number of components is K and
Φ
is the vector of all probability parameters
(
1
2
(
),
,
,
,
K
P X
Φ Φ
Φ

), the probability density function of incomplete (observed) data y
can be specified as the following marginal probability:
( |
)
( ,
|
)
(
)
( |
)
x
x
x
x
p
p
x
P X
x p
=
=
=


y Φ
y
Φ
y Φ
(4.99)
By comparing Eq. (4.97) and (4.99), we can see that the mixture weight is represented as the
probability function
(
)
P X
x
=
. That is,
(
)
kc
P X
k
=
=
(4.100)
According to the EM algorithm, the maximization of the logarithm of the likelihood
function log
( |
)
p y Φ
can be performed by iteratively maximizing the conditional expecta-
tion of the logarithm of Eq. (4.98), i.e., log
( ,
|
)
p
x
y
Φ . Suppose we have observed N inde-
pendent samples: {
}
1
2
,
,
,
N
y y
y

with hidden unobserved data {
}
1
2
,
,
,
N
x x
x

; the Q-
function can then be written as follows:
1
1
1
(
,
)
(
,
)
(
|
,
)log
(
,
|
)
(
,
|
)
=
log
(
,
|
)
(
|
)
i
i
N
N
i
i
i
i
i
i
i
x
N
i
i
i
i
i
x
i
Q
Q
P x
p
x
p
x
p
x
p
=
=
=
=
=



Φ Φ
Φ Φ
y Φ
y
Φ
y
Φ
y
Φ
y
Φ
(4.101)
By replacing items in Eq. (4.101) with Eqs. (4.98) and (4.100), the following equation can
be obtained:
1
1
(
,
)
log
(
,
)
K
K
k
k
k
k
k
Q
c
Qλ
γ
=
=
=
+


Φ Φ
Φ Φ
(4.102)
where
(
|
)
(
|
)
i
k
k
i
k
k
i
c p
P
γ
=
y
Φ
y
Φ
(4.103)
1
1
(
|
)
(
|
)
N
N
i
k
k
i
k
k
k
i
i
i
c p
P
γ
γ
=
=
=
=


y
Φ
y
Φ
(4.104)

Unsupervised Estimation Methods
175
1
1
(
|
)
( ,
)
log
(
|
)
log
(
|
)
(
|
)
N
N
i
k
k
i
k
k
k
k
i
k
k
i
k
i
i
i
c p
Q
p
p
P
λ
γ
=
=
Φ
=
=


y
Φ
Φ
y
Φ
y
Φ
y
Φ
(4.105)
Now we can perform a maximum likelihood estimation on the complete data (x, y)
during the M-step. By taking the derivative with respect to each parameter and setting it to
zero, we obtain the following EM re-estimate of
,m ,
k
k
c
and
k
Σ :
1
ˆ
k
k
k
K
k
k
c
N
γ
γ
γ
=
=
=

(4.106)
1
1
1
1
(
|
)
(
|
)
ˆ
(
|
)
(
|
)
N
N
k
k
i
k
i
i
k
i
i
i
i
k
N
N
i
k
k
i
k
k
i
i
i
c p
P
c p
P
γ
γ
=
=
=
=
=
= 



y
Φ
y
y
y
Φ
µ
y
Φ
y
Φ
(4.107)
1
1
1
1
(
|
)(
)(
)
(
)(
)
(
|
)
ˆ
(
|
)
(
|
)
t
N
N
k
k
i
k
i
k
i
k
i
t
k
i
k
i
k
i
i
i
k
N
N
i
k
k
i
k
k
i
i
i
c p
P
c p
P
γ
γ
=
=
=
=
−
−
−
−
Σ =
=




y
Φ
y
µ
y
µ
y
µ
y
µ
y
Φ
y
Φ
y
Φ
(4.108)
The quantity
i
k
γ
defined in Eq. (4.103) can be interpreted as the posterior probability
that the observed data
iy belong to Gaussian component k (
( |
,
)
k
k
k
N
Σ
y µ
). This informa-
tion as to whether the observed data
iy should belong to Gaussian component k is hidden
and can only be observed through the hidden variable
(
)
k
x c
. The EM algorithm described
above is used to uncover how likely the observed data
iy are expected to be in each Gaus-
sian component. The re-estimation formulas are consistent with our intuition. These MLE
formulas calculate the weighted contribution of each data sample according to the mixture
posterior probability
i
k
γ .
In fact, VQ is an approximate version of EM algorithms. A traditional VQ with the
Mahalanobis distance measure is equivalent to a mixture Gaussian VQ with the following
conditions
1/
kc
K
=
(4.109)
1,
0, otherwise
i
k
i
k
C
γ
∈

= 

y
(4.110)

176
Pattern Recognition
The difference between VQ and the EM algorithm is that VQ performs a hard assignment of
the data sample
iy to clusters (cells) while the EM algorithm performs a soft assignment of
the data sample
iy to clusters. As discussed in Chapter 8, this difference carries over to the
case of the Viterbi algorithm vs. the Baum-Welch algorithm in hidden Markov models.
4.5.
CLASSIFICATION AND REGRESSION TREES
Classification and regression trees (CART) [15, 82] have been used in a variety of pattern
recognition applications. Binary decision trees, with splitting questions attached to each
node, provide an easy representation that interprets and predicts the structures of a set of
data. The application of binary decision trees is much like playing the number-guessing
game, where the examinee tries to deduce the chosen number by asking a series of binary
number-comparing questions.
Consider a simple binary decision tree for height classification. Every person’s data in
the study may consist of several measurements, including race, gender, weight, age, occupa-
tion, and so on. The goal of the study is to develop a classification method to assign a person
one of the following five height classes: tall (T), medium-tall (t), medium (M), medium-shor
t(s) and short (S). Figure 4.14 shows an example of such a binary tree structure. With this
binary decision tree, one can easily predict the height class for any new person (with all the
measured data, but no height information) by traversing the binary trees. Traversing the bi-
nary tree is done through answering a series of yes/no questions in the traversed nodes with
the measured data. When the answer is yes, the right branch is traversed next; otherwise, the
left branch will be traversed instead. When the path ends at a leaf node, you can use its at-
tached label as the height class for the new person. If you have the average height for each
leaf node (computed by averaging the heights from those people who fall in the same leaf
node during training), you can actually use the average height in the leaf node to predict the
height for the new person.
This classification process is similar to a rule-based system where the classification is
carried out by a sequence of decision rules. The choice and order of rules applied in a rule-
based system is typically designed subjectively by hand through an introspective analysis
based on the impressions and intuitions of a limited number of data samples. CART, on the
other hand, provides an automatic and data-driven framework to construct the decision proc-
ess based on objective criteria. Most statistical pattern recognition techniques are designed
for data samples having a standard structure with homogeneous variables. CART is designed
instead to handle data samples with high dimensionality, mixed data types, and nonstandard
data structure. It has the following advantages over other pattern recognition techniques:
 CART can be applied to any data structure through appropriate formulation of
the set of potential questions.
 The binary tree structure allows for compact storage, efficient classification, and
easily understood interpretation of the predictive structure of the data.
 It often provides, without additional effort, not only classification and recogni-
tion, but also an estimate of the misclassification rate for each class.

Classification and Regression Trees
177
 It not only handles missing data, but also is very robust to outliers and misla-
beled data samples.
Figure 4.14 A binary tree structure for height classification
To construct a CART from the training samples with their classes (let’s denote the set
as ℑ), we first need to find a set of questions regarding the measured variables; e.g., “Is age
> 12?”, “Is occupation = professional basketball player?”, “Is gender = male?” and so on.
Once the question set is determined, CART uses a greedy algorithm to generate the decision
trees. All training samples ℑare placed in the root of the initial tree. The best question is
then chosen from the question set to split the root into two nodes. Of course, we need a
measurement of how well each question splits the data samples to pick the best question.
The algorithm recursively splits the most promising node with the best question until the
right-sized tree is obtained. We describe next how to construct the question set, how to
measure each split, how to grow the tree, and how to choose the right-sized tree.
4.5.1.
Choice of Question Set
Assume that the training data has the following format:
1
2
(
,
,
)
N
x x
x
=
x

(4.111)
N
N
N
N
Y
Y
Y
Y
S
T
t
M
m
Is age > 12?
Is occupation = professional
basketball player?
Is milk consumption > more
than 5 quarts per week?
Is gender = male?

178
Pattern Recognition
where each variable
ix is a discrete or continuous data type. We can construct a standard
set of questions Q as follows:
1. Each question is about the value of only a single variable. Questions of this
type are called simple or singleton questions.
2. If
ix is a discrete variable from the set{
}
1
2
,
,
,
K
c c
c

, Q includes all ques-
tions of the following form:
{Is
?}
ix
S
∈
(4.112)
where S is any subset of {
}
1
2
,
,
,
,
K
c c
c

3. If
ix
is a continuous variable, Q includes all questions of the following
form:
{Is
?} for
(
,
)
ix
c
c
≤
∈−∞∞
(4.113)
The question subset generated from discrete variables (in condition 2 above) is clearly
a finite set (
1
2
1
K −−). On the other hand, the question subset generated from continuous
variables (in condition 3 above) seems to be an infinite set based on the definition. Fortu-
nately, since the training data samples are finite, there are only finite number of distinct
splits for the training data. For a continuous variable
ix , the data points in ℑcontain at
most M distinct values
1
2
,
,
,
M
v v
v

. There are only at most M different splits generated by
the set of questions in the form:
{
}
Is
1,2,
,
i
n
x
c
n
M
≤
=

(4.114)
where
1
2
n
n
n
v
v
c
−+
=
and
0
0
v =
. Therefore, questions related to a continuous variable also
form a finite subset. The fact that Q is a finite set allows the enumerating of all possible
questions in each node during tree growing.
The construction of a question set is similar to that of rules in a rule-based system. In-
stead of using the all-possible question set Q , some people use knowledge selectively to
pick a subset of Q , which is sensitive to pattern classification. For example, the vowel sub-
set and consonant subset are a natural choice for these sensitive questions for phoneme clas-
sification. However, the beauty of CART is the ability to use all possible questions related to
the measured variables, because CART has a statistical data-driven framework to determine
the decision process (as described in subsequent Sections). Instead of setting some con-
straints on the questions (splits), most CART systems use all the possible questions for Q .

Classification and Regression Trees
179
4.5.2.
Splitting Criteria
A question in CART framework represents a split (partition) of data samples. All the leaf
nodes ( L in total) represent L disjoint subsets
1
2
,
,
,
L
A A
A

. Now we have the entire po-
tential question set Q , the task is how to find the best question for a node split. The selec-
tion of the best question is equivalent to finding the best split for the data samples of the
node.
Since each node t in the tree contains some training samples, we can compute the cor-
responding class probability density function
(
| )
P
t
ω
. The classification process for the
node can then be interpreted as a random process based on
(
| )
P
t
ω
. Since our goal is classi-
fication, the objective of a decision tree is to reduce the uncertainty of the event being de-
cided upon. We want the leaf nodes to be as pure as possible in terms of the class distribu-
tion. Let Y be the random variable of classification decision for data sample X . We could
define the weighted entropy for any node t as follows:
( )
( ) ( )
t
t
H Y
H Y P t
=
(4.115)
( )
(
| )log
(
| )
t
i
i
i
H Y
P
t
P
t
ω
ω
= −
(4.116)
where
(
| )
i
P
t
ω
is the percentage of data samples for class i in node t; and
( )
P t
is the prior
probability of visiting node t (equivalent to the ratio of number of data samples in node t and
the total number of training data samples). With this weighted entropy definition, the split-
ting criterion is equivalent to finding the question which gives the greatest entropy reduc-
tion, where the entropy reduction for a question q to split a node t into leaves l and r can be
defined as:
( )
( )
(
( )
( ))
( )
(
| )
t
t
l
r
t
t
H q
H Y
H Y
H Y
H Y
H Y q
∆
=
−
+
=
−
(4.117)
The reduction in entropy is also the mutual information between Y and question q .
The task becomes that of evaluating the entropy reduction
q
H
∆
for each potential question
(split), and picking the question with the greatest entropy reduction, that is,
*
argmax (
( ))
t
q
q
H q
=
∆
(4.118)
If we define the entropy for a tree, T , as the sum of weighted entropies for all the terminal
nodes, we have:
is terminal
( )
( )
t
t
H T
H Y
= 
(4.119)
It can be shown that the tree-growing (splitting) process repeatedly reduces the en-
tropy of the tree. The resulting tree thus has a better classification power. For continuous
pdf, likelihood gain is often used instead, since there is no straightforward entropy meas-

180
Pattern Recognition
urement [43]. Suppose one specific split divides the data into two groups,
1
X
and
2
X ,
which can then be used to train two Gaussian distributions
1
1
(
,
)
N
Σ
µ
and
2
2
(
,
)
N
Σ
µ
. The
log-likelihoods for generating these two data groups are:
1
1
1
1
1
1
1
(
|
)
log
(
,
,
)
( log2
log
) / 2
L
N
N
d
d a
π
=
Σ
=
+
Σ +
∏
x
X
x µ
(4.120)
2
2
2
2
2
2
1
(
|
)
log
(
,
,
)
( log 2
log
) / 2
L
N
N
d
d b
π
=
Σ
=
+
Σ +
∏
x
X
x µ
(4.121)
where d is the dimensionality of the data; and a and b are the sample counts for the data
groups
1
X
and
2
X
respectively. Now if the entire data
1
X
and
2
X
are merged into one
group and modeled by one Gaussian
( , )
N
Σ
µ
, according to MLE, we have
1
2
a
b
a
b
a
b
=
+
+
+
µ
µ
µ
(4.122)
1
1
1
1
2
2
2
(
)(
)
(
)(
)
t
t
a
b
a
b
a
b




Σ =
Σ +
−
−
Σ +
Σ +
−
−




+
+
µ
µ µ
µ
µ
µ µ
µ
(4.123)
Thus, the likelihood gain of splitting the data X into two groups
1
X and
2
X
is:
1
1
2
2
1
2
( )
(
|
)
(
|
)
(
|
)
=(
)log
log
log
tL q
L
N
L
N
L
N
a
b
a
b
∆
=
+
−
+
Σ −
Σ −
Σ
x
X
X
X
(4.124)
For regression purposes, the most popular splitting criterion is the mean squared error
measure, which is consistent with the common least squared regression methods. For in-
stance, suppose we need to investigate the real height as a regression function of the meas-
ured variables in the height study. Instead of finding height classification, we could simply
use the average height in each node to predict the height for any data sample. Suppose Y is
the actual height for training data X , then overall regression (prediction) error for a node t
can be defined as:
2
( )
|
( ) |
t
E t
Y
d
∈
=
−

X
X
(4.125)
where
( )
d X is the regression (predictive) value of Y
Now, instead of finding the question with greatest entropy reduction, we want to find
the question with largest squared error reduction. That is, we want to pick the question
*
q
that maximizes:
( )
( )
( ( )
( ))
tE q
E t
E l
E r
∆
=
−
+
(4.126)
where l and r are the leaves of node t. We define the expected square error
( )
V t
for a node t
as the overall regression error divided by the total number of data samples in the node.

Classification and Regression Trees
181
2
2
1
( )
|
( ) |
|
( ) |
( )
t
t
V t
E
Y
d
Y
d
N t
∈
∈


=
−
=
−






X
X
X
X
(4.127)
Note that
( )
V t is actually the variance estimate of the height, if
( )
d X is made to be the aver-
age height of data samples in the node. With
( )
V t , we define the weighted squared error
( )
V t
for a node t as follows.
2
1
( )
( ) ( )
|
( ) |
( )
( )
t
V t
V t P t
Y
d
P t
N t
∈


=
=
−





X
X
(4.128)
Finally, the splitting criterion can be rewritten as:
( )
( )
( ( )
( ))
tV q
V t
V l
V r
∆
=
−
+
(4.129)
Based on Eqs. (4.117) and (4.129), one can see the analogy between entropy and variance in
the splitting criteria for CART. The use of entropy or variance as splitting criteria is under
the assumption of uniform misclassification costs and uniform prior distributions. When
nonuniform misclassification costs and prior distributions are used, some other splitting
might be used for splitting criteria. Noteworthy ones are Gini index of diverity and twoing
rule. Those interested in alternative splitting criteria can refer to [11, 15].
For a wide range of splitting criteria, the properties of the resulting CARTs are empiri-
cally insensitive to these choices. Instead, the criterion used to get the right-sized tree is
much more important. We discuss this issue in Section 4.5.6.
4.5.3.
Growing the Tree
Given the question set Q and splitting criteria
( )
t
H q
∆
, the tree-growing algorithm starts
from the initial root-only tree. At each node of tree, the algorithm searches through the vari-
ables one by one, from
1x to
N
x . For each variable, it uses the splitting criteria to find the
best question (split). Then it can pick the best question out of the N best single-variable
questions. The procedure can continue splitting each node until either of the following con-
ditions is met for a node:
1. No more splits are possible; that is, all the data samples in the node belong to
the same class;
2. The greatest entropy reduction of best question (split) fall below a pre-set
threshold β , i.e.:
max
( )
t
q Q
H q
β
∈
∆
<
(4.130)

182
Pattern Recognition
3. The number of data samples falling in the leaf node t is below some threshold
α . This is to assure that there are enough training tokens for each leaf node if
one needs to estimate some parameters associated with the node.
When a node cannot be further split, it is declared a terminal node. When all active (non-
split) nodes are terminal, the tree-growing algorithm stops.
The algorithm is greedy because the question selected for any given node is the one
that seems to be the best, without regard to subsequent splits and nodes. Thus, the algorithm
constructs a tree that is locally optimal, but not necessarily globally optimal (but hopefully
globally good enough). This tree-growing algorithm has been successfully applied in many
applications [5, 39, 60]. A dynamic programming algorithm for determining global optimal-
ity is described in [78]; however, it is suitable only in restricted applications with relatively
few variables.
4.5.4.
Missing Values and Conflict Resolution
Sometimes, the available data sample
1
2
(
,
,
)
N
x x
x
=
X

has some value
jx
missing. This
missing-value case can be handled by the use of surrogate questions (splits). The idea is
intuitive. We define a similarity measurement between any two questions (splits) q and q
of a node t . If the best question of node t is the question q on the variable
ix , find the
question q that is most similar to q on a variable other than
ix . q is our best surrogate
question. Similarly, we find the second-best surrogate question, third-best and so on. The
surrogate questions are considered as the backup questions in the case of missing
ix values
in the data samples. The surrogate question is used in the descending order to continue tree
traversing for those data samples. The surrogate question gives CART unique ability to han-
dle the case of missing data. The similarity measurement is basically a measurement reflect-
ing the similarity of the class probability density function [15].
When choosing the best question for splitting a node, several questions on the same
variable
ix may achieve the same entropy reduction and generate the same partition. As in
rule-based problem solving systems, a conflict resolution procedure [99] is needed to decide
which question to use. For example, discrete questions
1q and
2q
have the following for-
mat:
1
1
: {Is
?}
i
q
x
S
∈
(4.131)
2
2
: {Is
?}
i
q
x
S
∈
(4.132)
Suppose
1S
is a subset of
2
S , and one particular node contains only data samples
whose
ix value contains only values in
1S , but no other. Now question
1q or
2q
performs
the same splitting pattern and therefore achieves exactly the same amount of entropy reduc-

Classification and Regression Trees
183
tion. In this case, we call
1q a sub-question of question
2q , because
1q is a more specific
version.
A specificity ordering conflict resolution strategy is used to favor the discrete question
with fewer elements because it is more specific to the current node. In other words, if the
elements of a question are a subset of the elements of another question with the same en-
tropy reduction, the question with the subset of elements is preferred. Preferring more spe-
cific questions will prevent decision trees from over-generalizing. The specificity ordering
conflict resolution can be implemented easily by presorting the set of discrete questions by
the number of elements they contain in descending order, before applying them to decision
trees. A similar specificity ordering conflict resolution can also be implemented for continu-
ous-variable questions.
4.5.5.
Complex Questions
One problem with allowing only simple questions is that the data may be over-fragmented,
resulting in similar leaves in different locations of the tree. For example, when the best ques-
tion (rule) to split a node is actually a composite question of the form “Is
1
ix
S
∈
?” or “Is
2
ix
S
∈
?”, a system allowing only simple questions will generate two separate questions to
split the data into three clusters rather than two as shown in Figure 4.15. Also data for which
the answer is yes are inevitably fragmented across two shaded nodes. This is inefficient and
ineffective since these two very similar data clusters may now both contain insufficient
training examples, which could potentially handicap the future tree growing. Splitting data
unnecessarily across different nodes leads to unnecessary computation, redundant clusters,
reduced trainability, and less accurate entropy reduction.
We deal with this problem by using a composite-question construction [38, 40]. It in-
volves conjunctive and disjunctive combinations of all questions (and their negations). A
composite question is formed by first growing a tree with simple questions only and then
clustering the leaves into two sets. Figure 4.16 shows the formation of one composite ques-
tion. After merging, the structure is still a binary question. To construct the composite ques-
tion, multiple OR operators are used to describe the composite condition leading to either
one of the final clusters, and AND operators are used to describe the relation within a par-
ticular route. Finally, a Boolean reduction algorithm is used to simplify the Boolean expres-
sion of the composite question.
To speed up the process of constructing composite questions, we constrain the number
of leaves or the depth of the binary tree through heuristics. The most frequently used heuris-
tics is the limitation of the depth when searching a composite question. Since composite
questions are essentially binary questions, we use the same greedy tree-growing algorithm to
find the best composite question for each node and keep growing the tree until the stop crite-
rion is met. The use of composite questions not only enables flexible clustering, but also
improves entropy reduction. Growing the sub-tree a little deeper before constructing the
composite question may achieve longer-range optimum, which is preferable to the local op-
timum achieved in the original greedy algorithm that used simple questions only.

184
Pattern Recognition
Figure 4.15 A over-split tree for question “Is
1
ix
S
∈
?” or “Is
2
ix
S
∈
?”
Figure 4.16 The formation of a composite question from simple questions
The construction of composite questions can also be applied to continuous variables to
obtained complex rectangular partitions. However, some other techniques are used to obtain
general partitions generated by hyperplanes not perpendicular to the coordinate axes. Ques-
tions typically have a linear combination of continuous variables in the following form [15]:
{Is
?}
i
i
i
a x
c
≤

(4.133)
1
Is
?
ix
S
∈
2
Is
?
ix
S
∈
N
N
Y
Y
N
Y
N
Y
N
Y
N
Y

Classification and Regression Trees
185
4.5.6.
The Right-Sized Tree
One of the most critical problems for CART is that the tree may be strictly tailored to the
training data that has no generalization capability. When you split a leaf node in the tree to
get entropy reduction until each leaf node contains data from one single class, that tree pos-
sesses a zero percent classification error on the training set. This is an over-optimistic esti-
mate of the test-set misclassification rate. Independent test sample estimation or cross-
validation is often used to prevent decision trees from over-modeling idiosyncrasies of the
training data. To get a right-sized tree, you can minimize the misclassification rate for future
independent test data.
Before we describe the solution for finding the right sized tree, let’s define a couple of
useful terms. Naturally we will use the plurality rule
( )t
δ
to choose the class for a node t:
( )
argmax
(
| )
i
i
t
P
t
δ
ω
=
(4.134)
Similar to the notation used in Bayes’ decision theory, we can define the misclassification
rate
( )
R t
for a node t as:
( )
( ) ( )
R t
r t P t
=
(4.135)
where
( )
1
max
(
| )
i
i
r t
P
t
ω
= −
and
( )
P t
is the frequency (probability) of the data falling in
node t. The overall misclassification rate for the whole tree T is defined as:
( )
( )
t T
R T
R t
∈
= 

(4.136)
where T represents the set of terminal nodes. If a nonuniform misclassification cost
( | )
c i j ,
the cost of misclassifying class j data as class i data, is used,
( )
r t
is redefined as:
( )
min
( | ) ( | )
i
j
r t
c i
j P j t
=

(4.137)
As we mentioned,
( )
R T
can be made arbitrarily small (eventually reduced to zero) for
the training data if we keep growing the tree. The key now is how we choose the tree that
can minimize
*( )
R T , which is denoted as the misclassification rate of independent test data.
Almost no tree initially grown can perform well on independent test data. In fact, using more
complicated stopping rules to limit the tree growing seldom works, and it is either stopped
too soon at some terminal nodes, or continued too far in other parts of the tree. Instead of
inventing some clever stopping criteria to stop the tree growing at the right size, we let the
tree over-grow (based on rules in Section 4.5.3). We use a pruning strategy to gradually cut
back the tree until the minimum
*( )
R T
is achieved. In the next section we describe an algo-
rithm to prune an over-grown tree, minimum cost-complexity pruning.

186
Pattern Recognition
4.5.6.1.
Minimum Cost-Complexity Pruning
To prune a tree, we need to find a subtree (or branch) that makes the least impact in terms of
a cost measure, whether it is pruned or not. This candidate to be pruned is called the weakest
subtree. To define such a weakest subtree, we first need to define the cost measure.
DEFINITION 1: For any sub-tree T of
max
T
(
max
T
T

), let |
|
T
denote the number of ter-
minal nodes in tree T .
DEFINITION 2: Let
0
α ≥
be a real number called the complexity parameter. The cost-
complexity measure can be defined as:
( )
( )
|
|
R T
R T
T
α
α
=
+

(4.138)
DEFINITION 3: For each α , define the minimal cost-complexity subtree
max
( )
T
T
α 
that
minimizes
( )
R T
α
, that is,
max
( )
argmin
( )
T
T
T
R T
α
α =

(4.139)
Based on DEFINITION 3, if α is small, the penalty for having a large tree is small
and
( )
T α
will be large. In fact,
(0)
T
=
max
T
because
max
T
has a zero misclassification rate,
so it will minimize
( )
o
R T . On the other hand, when α increases,
( )
T α
becomes smaller
and smaller. For a sufficient large α ,
( )
T α
may collapse into a tree with only the root. The
increase of α produces a sequence of pruned trees and it is the basis of the pruning process.
The pruning algorithm rests on two theorems. The first is given as follows.
THEOREM 1: For every value of α , there exists a unique minimal cost-complexity sub-
tree
( )
T α
as defined in Definition 3.12
To progressively prune the tree, we need to find the weakest subtree (node). The idea
of a weakest subtree is the following: if we collapse the weakest subtree into a single termi-
nal node, the cost-complexity measure would increase least. For any node t in the tree T ,
let { }t
denote the subtree containing only the node t , and
tT denote the branch starting at
node t . Then we have
( )
( )
|
|
t
t
t
R T
R T
T
α
α
=
+

(4.140)
({ })
( )
R
t
R t
α
α
=
+
(4.141)
When α is small,
tT has a smaller cost-complexity than the single-node tree { }t .
However, when α increases to a point where the cost-complexity measures for
tT and { }t
12 You can find the proof to this in [15].

Classification and Regression Trees
187
are the same, it makes sense to collapse
tT into a single terminal node { }t . Therefore, we
decide the critical value for α by solving the following inequality:
( )
({ })
t
R T
R
t
α
α
≤
(4.142)
We obtain:
( )
(
)
|
| 1
t
t
R t
R T
T
α
−
≤
−

(4.143)
Based on Eq. (4.143), we define a measurement
( )t
η
for each node t in tree T:
( )
(
) ,
|
| 1
( )
,
t
t
R t
R T
t
T
T
t
t
T
η
−

∉

−
= 
+∞
∈




(4.144)
Based on measurement
( )t
η
, we then define the weakest subtree
1tT as the tree branch start-
ing at the node
1t such that
1
arg min
( )
t T
t
t
η
∈
=
(4.145)
1
1
( )
t
α
η
=
(4.146)
As α increases, the node
1t is the first node such that
({ })
R
t
α
becomes equal to
( )
t
R T
α
. At this point, it would make sense to prune subtree
1tT (collapse
1tT
into a single-
node subtree
1
{ }
t
), and
1
α is the value of α where the pruning occurs.
Now the tree T after pruning is referred to as
1T , i.e.,
1
1
t
T
T
T
=
−
(4.147)
We then use the same process to find the weakest subtree
2tT
in
1T and the new pruning
point
2
α . After pruning away
2tT
from
1T to form the new pruned tree
2T , we repeat the
same process to find the next weakest subtree and pruning point. If we continue the process,
we get a sequence of decreasing pruned trees:
1
2
2
{ }
T
T
T
T
r



 
(4.148)
where { }
r
is the single-node tree containing the root of tree T with corresponding pruning
points:
0
1
2
3
α
α
α
α
<
<
<
<
(4.149)
where
0
0
α =

188
Pattern Recognition
With the process above, the following theorem (which is basic for the minimum cost-
complexity pruning) can be proved.
THEOREM 2 : Let
0T be the original tree T .
For
0
k ≥
,
1
k
k
α
α
α +
≤
<
,
( )
(
)
k
k
T
T
T
α
α
=
=
(4.150)
4.5.6.2.
Independent Test Sample Estimation
The minimum cost-complexity pruning algorithm can progressively prune the over-grown
tree to form a decreasing sequence of subtrees
1
2
2
{ }
T
T
T
T
r



 
, where
(
)
k
k
T
T α
=
,
0
0
α =
and
0T
T
=
. The task now is simply to choose one of those subtrees as the optimal-
sized tree. Our goal is to find the optimal-sized tree that minimizes the misclassification for
independent test set
*( )
R T . When the training set ℑis abundant, we can afford to set aside
an independent test set ℜfrom the training set. Usually ℜis selected as one third of the
training set ℑ. We use the remaining two thirds of the training set ℑ−ℜ(still abundant) to
train the initial tree T and apply the minimum cost-complexity pruning algorithm to attain
the decreasing sequence of subtrees
1
2
2
{ }
T
T
T
T
r



 
. Next, the test set ℜis run
through the sequence of subtrees to get corresponding estimates of test-set misclassification
*
*
*
*
1
2
( ),
( ),
(
),
,
({ })
R T
R T
R T
R
r

. The optimal-sized tree
*
kT
is then picked as the one with
minimum test-set misclassification measure, i.e.:
*
*
arg min
(
)
k
k
k
R T
=
(4.151)
The independent test sample estimation approach has the drawback that it reduces the
effective training sample size. This is why it is used only when there is abundant training
data. Under most circumstances where training data is limited, cross-validation is often
used.
4.5.6.3.
Cross-Validation
CART can be pruned via v-fold cross-validation. It follows the same principle of cross vali-
dation described in Section 4.2.3. First it randomly divides the training set ℑinto v disjoint
subset
1
2
,
,
,
v
ℑℑ
ℑ

, each containing roughly the same data samples. It then defines the ith
training set
1,2,
,
i
i
i
v
ℑ= ℑ−ℑ
=

(4.152)
so that
i
ℑcontains the fraction (
1)
v
v
−
of the original training set. v is usually chosen to
be large, like 10.
In v-fold cross-validation, v auxiliary trees are grown together with the main tree T
grown on ℑ. The ith tree is grown on the ith training set
i
ℑ. By applying minimum cost-

Classification and Regression Trees
189
complexity pruning, for any given value of the cost-complexity parameter α , we can obtain
the corresponding minimum cost-complexity subtrees
( )
T α
and
( )
i
T α ,
1,2,
,
i
v
=

. Ac-
cording to Theorem 2 in Section 4.5.6.1, those minimum cost-complexity subtrees will form
1
v +
sequences of subtrees:
1
2
2
{ }
T
T
T
T
r



 
and
(4.153)
1
2
3
{ }
1,2,
,
i
i
i
i
i
T
T
T
T
r
i
v
=



 

(4.154)
ALGORITHM 4.5 THE CART ALGORITHM
Step 1: Question Set: Create a standard set of questions Q that consists of all possible ques-
tions about the measure variables.
Step 2: Splitting Criterion: Pick a splitting criterion that can evaluate all the possible questions in
any node. Usually it is either entropy-like measurement for classification trees or mean square
errors for regression trees.
Step 3: Initialization: Create a tree with one (root) node, consisting of all training samples.
Step 4: Split Candidates: Find the best composite question for each terminal node:
a. Generate a tree with several simple-question splits as described in Section 4.5.3.
b. Cluster leaf nodes into two classes according to the same splitting criterion.
c. Based on the clustering done in (b), construct a corresponding composite question.
Step 5: Split: Out of all the split candidates in Step 4, split the one with best criterion.
Step 6: Stop Criterion: If all the leaf node containing data samples from the same class or all
the potential splits generate improvement fall below a pre-set threshold β , go to Step 7; oth-
erwise go to Step 4.
Step 7: Use independent test sample estimate or cross-validation estimate to prune the original
tree into the optimal size.
The basic assumption of cross-validation is that the procedure is stable if v is large.
That is,
( )
T α
should have the same classification accuracy as
( )
i
T α . Although we cannot
directly estimate the test-set misclassification for the main tree
*( ( ))
R T α
, we could ap-
proximate it via the test-set misclassification measure
*(
( ))
i
R T α
, since each data sample in
ℑoccurs in one and only one test set
i
ℑ. The v-fold cross-validation estimate
( ( ))
CV
R
T α
can be computed as:
*
1
1
( ( ))
(
( ))
v
CV
i
i
R
T
R T
v
α
α
=
= 
(4.155)

190
Pattern Recognition
Similar to Eq. (4.151), once
( ( ))
CV
R
T α
is computed, the optimal v-fold cross-validation
tree
CV
kT
can be found through
arg min
(
)
CV
CV
k
k
k
R
T
=
(4.156)
Cross-validation is computationally expensive in comparison with independent test
sample estimation, though it makes more effective use of all training data and reveals useful
information regarding the stability of the tree structure. Since the auxiliary trees are grown
on a smaller training set (a fraction
1
v
v
−
of the original training data), they tend to have a
higher misclassification rate. Therefore, the cross-validation estimates
( )
CV
R
T
tend to be an
over-estimation of the misclassification rate. The algorithm for generating a CART tree is
illustrated in Algorithm 4.5.
4.6.
HISTORICAL PERSPECTIVE AND FURTHER READING
Pattern recognition is a multidisciplinary field that comprises a broad body of loosely related
knowledge and techniques. Historically, there are two major approaches to pattern recogni-
tion – the statistical and the syntactical approaches. Although this chapter is focused on the
statistical approach, syntactical pattern recognition techniques, which aim to address the
limitations of the statistical approach in handling contextual or structural information, can be
complementary to statistical approaches for spoken language processing, such as parsing.
Syntactic pattern recognition is based on the analogy that complex patterns can be decom-
posed recursively into simpler subpatterns, much as a sentence can be decomposed into
words and letters. Fu [24] provides an excellent book on syntactic pattern recognition.
The foundation of statistical pattern recognition is Bayesian theory, which can be
traced back to the 18th century [9, 54] and its invention by the British mathematician Tho-
mas Bayes (1702-1761). Chow [20] was the first to use Bayesian decision theory for pattern
recognition. Statistical pattern recognition has been used successfully in a wide range of
applications, from optical/handwritten recognition [13, 96], to speech recognition [7, 86] and
to medical/machinery diagnosis [1, 27]. The books by Duda et al. [22] and Fukunaga [25]
are two classic treatments of statistical pattern recognition. Duda et al. have a second edition
of the classic pattern recognition book [23] that includes many up-to-date topics.
MLE and MAP are two most frequently used estimation methods for pattern recogni-
tion because of their simplicity and efficiency. In Chapters 8 and 9, they play an essential
role in model parameter estimation. Estimating the recognition performance and comparing
different recognition systems are important subjects in pattern recognition. The importance
of a large number of test samples was reported in [49]. McNemar’s test is dated back to the
1940s [66]. The modification of the test for continuous speech recognition systems pre-
sented in this chapter is based on an interesting paper [30] that contains a general discussion
on using hypothesis-testing methods for continuous speech recognition.
Gradient descent is fundamental for most discriminant estimation methods, including
MMIE, MCE, and neural networks. The history of gradient descent can be traced back to

Historical Perspective and Further Reading
191
Newton’s method for root finding [72, 81]. Both the book by Duda et al. [23] and the paper
by Juang et al. [48] provide a good description of gradient descent. MMIE was first pro-
posed in [16, 71] for the speech recognition problem. According to these two works, MMIE
is more robust than MLE to incorrect model assumptions. MCE was first formulated by
Juang et al. [48] and successfully applied to small-vocabulary speech recognition [47].
The modern era of neural networks was brought to the scientific community by
McCulloch and Pitts. In the pioneering paper [64], McCulloch and Pitts laid out the mathe-
matical treatment of the behavior of networks of simple neurons. The most important result
they showed is that a network would compute any computable function. John von Neumann
was influenced by this paper to use switch-delay elements derived from the McCulloch-Pitts
neuron in the construction of the EDVAC (Electronic Discrete Variable Automatic Com-
puter) that was developed based on ENIAC (Electronic Numerical Integrator and Computer)
[2, 35]. The ENIAC was the famous first general-purpose electronic computer built at the
Moore School of Electrical Engineering at the University of Pennsylvania from 1943 to
1946 [31]. The two-layer perceptron work [87] by Rosenblatt, was the first to provide rigor-
ous proofs about perceptron convergence. A 1969 book by Minsky and Papert [68] reveals
that there are fundamental limits to what single-layer perceptrons can compute. It was not
until the 1980s that the discovery of multi-layer perceptrons (with hidden layers and nonlin-
ear threshold functions) and back-propagation [88] reawakened interest in neural networks.
The two-volume PDP book [90, 91], Parallel Distributed Processing: Explorations in the
Microstructures of Cognition, edited by Rummelhart and McClelland, brought the back-
propagation learning method to the attention of the widest audience. Since then, various
applications of neural networks in diverse domains have been developed, including speech
recognition [14, 58], speech production and perception [93, 94], optical and handwriting
character recognition [55, 92], visual recognition [26], game playing [97], and natural lan-
guage processing [63]. There are several good textbooks for neural networks. In particular,
the book by Haykin [35] provides a very comprehensive coverage of all foundations of neu-
ral networks. Bishop [12] provides a thoughtful treatment of neural networks from the per-
spective of pattern recognition. Short, concise tutorial papers on neural networks can be
found in [44, 57].
Vector quantization originated from speech coding [17, 32, 45, 61]. The k-means algo-
rithm was introduced by Lloyd [59]. Over the years, there have been many variations of VQ,
including fuzzy VQ [10], learning VQ (LVQ) [51], and supervised VQ [18, 42]. The first
published investigation toward the EM-like algorithm for incomplete data learning can be
attributed to Pearson [79]. The modern EM algorithm is formalized by Dempster, Laird, and
Rubin [21]. McLachlan and Krishnan [65] provide a thorough overview and history of the
EM algorithm. The convergence of the EM algorithm is an interesting research topic and
Wu [100] has an extensive description of the rate of convergence. The EM algorithm is the
basis for all unsupervised learning that includes hidden variables. The famous HMM train-
ing algorithm, as described in Chapter 8, is based on the EM algorithm.
CART uses a very intuitive and natural principle of sequential questions and answers,
which can be traced back to 1960s [70]. The popularity of CART is attributed to the book by
Breiman et al. [15]. Quinlan proposed some interesting variants of CART, like ID3 [82] and
C4.5 [84]. CART has recently been one of the most popular techniques in machine learning.

192
Pattern Recognition
Mitchell includes a good overview chapter on the latest CART techniques in his machine-
learning book [69]. In addition to the strategies of node splitting and pruning mentioned in
this chapter, [62] used a very interesting approach for splitting and pruning criteria based on
a statistical significance testing of the data’s distributions. Moreover, [28] proposed an itera-
tive expansion pruning algorithm which is believed to perform as well as cross-validation
pruning and yet is computationally cheaper [52]. CART has been successfully used in a va-
riety of spoken language applications such as letter-to-sound conversion [46, 60], allophone
model clustering [8, 38, 39], language models [5], automatic rule generation [83], duration
modeling of phonemes [74, 80], and supervised vector quantization [67].
REFERENCES
[1]
Albert, A. and E.K. Harris, Multivariate Interpretation of Clinical Laboratory
Data, 1987, New York, Marcel Dekker.
[2]
Aspray, W. and A. Burks, "Papers of John von Neumann on Computing and Com-
puter Theory" in Charles Babbage Institute Reprint Series for the History of Com-
puting 1986, Cambridge, MA, MIT Press.
[3]
Atal, B.S. and M.R. Schroeder, "Predictive Coding of Speech Signals and Subjec-
tive Error Criteria," IEEE Trans. on Acoustics, Speech and Signal Processing,
1979, ASSP-27(3), pp. 247-254.
[4]
Bahl, L.R., et al., "A New Algorithm for the Estimation of Hidden Markov Model
Parameters," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Process-
ing, 1988, New York, NY pp. 493-496.
[5]
Bahl, L.R., et al., "A Tree-Based Statistical Language Model for Natural Language
Speech Recognition," IEEE Trans. on Acoustics, Speech, and Signal Processing,
1989, 37(7), pp. 1001-1008.
[6]
Bahl, L.R., et al., "Estimating Hidden Markov Model Parameters so as to Maxi-
mize Speech Recognition Accuracy," IEEE Trans. on Speech and Audio Process-
ing, 1993, 1(1), pp. 77-83.
[7]
Bahl, L.R., F. Jelinek, and R.L. Mercer, "A Maximum Likelihood Approach to
Continuous Speech Recognition," IEEE Trans. on Pattern Analysis and Machine
Intelligence, 1983, 5(2), pp. 179-190.
[8]
Bahl, L.R., et al., "Decision Trees for Phonological Rules in Continuous Speech" in
Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing 1991, To-
ronto, Canada, pp. 185-188.
[9]
Bayes, T., "An Essay Towards Solving a Problem in the Doctrine Of Chances,"
Philosophical Tansactions of the Royal Society, 1763, 53, pp. 370-418.
[10]
Bezdek, J., Pattern Recognition with Fuzzy Objective Function Algorithms, 1981,
New York, NY, Plenum Press.
[11]
Bhargava, T.N. and V.R.R. Uppuluri, "Sampling Distribution of Gini's Index of
Diversity," Applied Mathematics and Computation, 1977, 3, pp. 1-24.
[12]
Bishop, C.M., Neural Networks for Pattern Recognition, 1995, Oxford, UK, Ox-
ford University Press.

Historical Perspective and Further Reading
193
[13]
Blesser, B., et al., "A Theoretical Approach for Character Recognition Based on
Phenomenological Attributes," Int. Journal of Man-Machine Studies, 1974, 6(6),
pp. 701-714.
[14]
Bourlard, H. and N. Morgan, Connectionist Speech Recognition - A Hybrid Ap-
proach, 1994, Boston, MA, Kluwer Academic Publishers.
[15]
Breiman, L., et al., Classification and Regression Trees, 1984, Pacific Grove, CA,
Wadsworth.
[16]
Brown, P.F., The Acoustic-Modeling Problem in Automatic Speech Recognition,
PhD Thesis in Computer Science Department 1987, Carnegie Mellon University,
Pittsburgh, PA.
[17]
Buzo, A., et al., "Speech Coding Based upon Vector Quantization," IEEE Trans. on
Acoustics, Speech and Signal Processing, 1980, 28(5), pp. 562-574.
[18]
Cerf, P.L., W. Ma, and D.V. Compernolle, "Multilayer Perceptrons as Labelers for
Hidden Markov Models," IEEE Trans. on Speech and Audio Processing, 1994,
2(1), pp. 185-193.
[19]
Chang, P.C. and B.H. Juang, "Discriminative Training of Dynamic Programming
Based Speech Recognizers," IEEE Int. Conf. on Acoustics, Speech and Signal
Processing, 1992, San Fancisco.
[20]
Chow, C.K., "An Optimum Character Recognition System Using Decision Func-
tions," IRE Trans., 1957, pp. 247-254.
[21]
Dempster, A.P., N.M. Laird, and D.B. Rubin, "Maximum-Likelihood from Incom-
plete Data via the EM Algorithm," Journal of Royal Statistical Society ser. B, 1977,
39, pp. 1-38.
[22]
Duda, R.O. and P.E. Hart, Pattern Classification and Scene Analysis, 1973, New
York, N.Y., John Wiley and Sons.
[23]
Duda, R.O., D.G. Stork, and P.E. Hart, Pattern Classification and Scene Analysis :
Pattern Classification, 2nd ed, 1999, John Wiley & Sons.
[24]
Fu, K.S., Syntactic Pattern Recognition and Applications, 1982, Englewood Cliffs,
NJ, Prentice Hall.
[25]
Fukunaga, K., Introduction to Statistical Pattern Recognition, Second ed, 1990,
Orlando, FL, Academic Press.
[26]
Fukushima, K., S. Miykake, and I. Takayuki, "Neocognition: A Neural Network
Model for a Mechanism of Visual Pattern Recognition," IEEE Trans. on Systems,
Man and Cybernetics, 1983, SMC-13(5), pp. 826-834.
[27]
Gastwirth, J.L., "The Statistical Precision of Medical Screening Procedures: Appli-
cation to Polygraph and AIDS Antibodies Test Data (with Discussion)," Statistics
Science, 1987, 2, pp. 213-238.
[28]
Gelfand, S., C. Ravishankar, and E. Delp, "An Iterative Growing and Pruning Al-
gorithm for Classification Tree Design," IEEE Trans. on Pattern Analysis and Ma-
chine Intelligence, 1991, 13(6), pp. 163-174.
[29]
Gersho, A., "On the Structure of Vector Quantization," IEEE Trans. on Information
Theory, 1982, IT-28, pp. 256-261.

194
Pattern Recognition
[30]
Gillick, L. and S.J. Cox, "Some Statistical Issues in the Comparison of Speech
Recognition Algorithms," IEEE Int. Conf. on Acoustics, Speech and Signal Proc-
essing, 1989, Glasgow, Scotland, UK, IEEE pp. 532-535.
[31]
Goldstine, H., The Computer from Pascal to von Neumann, 2nd ed, 1993, Prince-
ton, NJ, Princeton University Press.
[32]
Gray, R.M., "Vector Quantization," IEEE ASSP Magazine, 1984, 1(April), pp. 4-
29.
[33]
Gray, R.M. and E.D. Karnin, "Multiple Local Optima in Vector Quantizers," IEEE
Trans. on Information Theory, 1982, IT-28, pp. 256-261.
[34]
Hartigan, J.A., Clustering Algorithm, 1975, New York, J. Wiley.
[35]
Haykin, S., Neural Networks: A Comprehensive Foundation, 2dn ed, 1999, Upper
Saddle River, NJ, Prentice-Hall.
[36]
Hedelin, P., P. Knagenhjelm, and M. Skoglund, "Vector Quantization for Speech
Transmission" in Speech Coding and Synthesis, W.B. Kleijn and K.K. Paliwal, eds.
1995, Amsterdam, pp. 311-396, Elsevier.
[37]
Hinton, G.E., "Connectionist Learning Procedures," Artificial Intelligence, 1989,
40, pp. 185--234.
[38]
Hon, H.W., Vocabulary-Independent Speech Recognition: The VOCIND System,
Ph.D Thesis in Department of Computer Science 1992, Carnegie Mellon Univer-
sity, Pittsburgh.
[39]
Hon, H.W. and K.F. Lee, "On Vocabulary-Independent Speech Modeling," IEEE
Int. Conf. on Acoustics, Speech and Signal Processing, 1990, Albuquerque, NM pp.
725-728.
[40]
Hon, H.W. and K.F. Lee, "Vocabulary-Independent Subword Modeling and Adap-
tation," IEEE Workshop on Speech Recognition, 1991, Arden House, Harriman,
NY.
[41]
Huang, X.D., Y. Ariki, and M.A. Jack, Hidden Markov Models for Speech
Recognition, 1990, Edinburgh, U.K., Edinburgh University Press.
[42]
Hunt, M.J., et al., "An Investigation of PLP and IMELDA Acoustic Representa-
tions and of Their Potential for Combination" in Proc. of the IEEE Int. Conf. on
Acoustics, Speech and Signal Processing 1991, Toronto, Canada, pp. 881-884.
[43]
Hwang, M.Y. and X.D. Huang, "Dynamically Configurable Acoustic Modelings
for Speech Recognition," IEEE Int. Conf. on Acoustics, Speech and Signal Process-
ing, 1998, Seattle, WA.
[44]
Jain, A., J. Mao, and K.M. Mohiuddin, "Artifical Neural Networks: A Tutorial,"
Compter, 1996, 29(3), pp. 31-44.
[45]
Jayant, N.S. and P. Noll, Digital Coding of Waveforms, 1984, Englewood Cliffs,
NJ, Prentice-Hall.
[46]
Jiang, L., H.W. Hon, and X.D. Huang, "(don't use) Improvements on a Trainable
Letter-to-Sound Converter," Eurospeech'97, 1997, Rhodes, Greece.
[47]
Juang, B.H., W. Chou, and C.H. Lee, "Statistical and Discriminative Methods for
Speech Recognition" in Automatic Speech and Speaker Recognition - Advanced
Topics, C.H. Lee, F.K. Soong, and K.K. Paliwal, eds. 1996, Boston, pp. 109-132,
Kluwer Academic Publishers.

Historical Perspective and Further Reading
195
[48]
Juang, B.H. and S. Katagiri, "Discriminative Learning for Minimum Error Classifi-
cation," IEEE Trans. on Acoustics, Speech and Signal Processing, 1992, SP-
40(12), pp. 3043-3054.
[49]
Kanal, L.N. and B. Chandrasekaran, "On Dimensionality and Sample Size in Statis-
tical Pattern Classification," Proc. of NEC, 1968, 24, pp. 2-7.
[50]
Kanal, L.N. and N.C. Randall, "Recognition System Design by Statistical Analy-
sis," ACM Proc. of 19th National Conf., 1964 pp. D2.5-1-D2.5-10.
[51]
Kohonen, T., Learning Vector Quantization for Pattern Recognition, 1986, Hel-
sinki University of Technology, Finland.
[52]
Kuhn, R. and R.D. Mori, "The Application of Semantic Classification Trees to
Natural Language Understanding," IEEE Trans. on Pattern Analysis and Machine
Intelligence, 1995(7), pp. 449-460.
[53]
Lachenbruch, P.A. and M.R. Mickey, "Estimation Of Error Rates In Discriminant
Analysis," Technometrics, 1968, 10, pp. 1-11.
[54]
Laplace, P.S., Theorie Analytique des Probabilities, 1812, Paris, Courcier.
[55]
Lee, D.S., S.N. Srihari, and R. Gaborski, "Bayesian and Neural Network Pattern
Recognition: A Theoretical Connection and Empirical Results with Handwriting
Characters" in Artificial Neural Network and Statistical Pattern Recognition: Old
and New Connections, I.K. Sethi and A.K. Jain, eds. 1991, Amsterdam, North-
Holland.
[56]
Linde, Y., A. Buzo, and R.M. Gray, "An Algorithm for Vector Quantizer Design,"
IEEE Trans. on Communication, 1980, COM-28(1), pp. 84-95.
[57]
Lippmann, R.P., "An Introduction to Computing with Neural Nets," IEEE ASSP
Magazine, 1987, pp. 4--22.
[58]
Lippmann, R.P., "Review of Neural Nets for Speech Recognition," Neural Compu-
tation, 1989, 1, pp. 1-38.
[59]
Lloyd, S.P., "Least Squares Quantization in PCM," IEEE Trans. on Information
Theory, 1982, IT-2, pp. 129-137.
[60]
Lucassen, J.M. and R.L. Mercer, "An Information-Theoretic Approach to the
Automatic Determination of Phonemic Baseforms," Proc. of the IEEE Int. Conf. on
Acoustics, Speech and Signal Processing, 1984, San Diego, California pp. 42.5.1-
42.5.4.
[61]
Makhoul, J., S. Roucos, and H. Gish, "Vector Quantization in Speech Coding,"
Proc. of the IEEE, 1985, 73(11), pp. 1551-1588.
[62]
Martin, J.K., "An Exact Probability Metric For Decision Tree Splitting And Stop-
ping," Articial Intelligence and Statistics, 1995, 5, pp. 379-385.
[63]
McClelland, J., "The Programmable Blackboard Model" in Parallel Distributed
Processing - Explorations in the Microstructure of Cognition, Volume II: Psycho-
logical and Biological Models 1986, Cambridge, MA, MIT Press.
[64]
McCulloch, W.S. and W. Pitts, "A Logical Calculus of Ideas Immanent in Nervous
Activity," Bulletin of Mathematical Biophysics, 1943.
[65]
McLachlan, G. and T. Krishnan, The EM Algorithm and Extensions, 1996, New
York, NY, Wiley Interscience.

196
Pattern Recognition
[66]
McNemar, E.L., "Note on the Sampling Error of The Difference Between Corre-
lated Proportions or Percentages," Psychometrika, 1947, 12, pp. 153-157.
[67]
Meisel, W.S., et al., "The SSI Large-Vocabulary Speaker-Independent Continuous
Speech Recognition System" 1991, pp. 337-340.
[68]
Minsky, M. and S. Papert, Perceptrons, 1969, Cambridge, MA, MIT Press.
[69]
Mitchell, T., Machine Learning, McGraw-Hill Series in Computer Science, 1997,
McGraw-Hill.
[70]
Morgan, J.N. and J.A. Sonquist, "Problems in the Analysis of Survey Data and a
Proposal," Journal of American Statistics Association, 1962, 58, pp. 415-434.
[71]
Nadas, A., "A Decision-Theoretic Formulation of a Training Problem in Speech
Recognition and a Comparison of Training by Unconditional Versus Conditional
Maximum Likelihood," IEEE Trans. on Acoustics, Speech and Signal Processing,
1983, 4, pp. 814-817.
[72]
Newton, I., Philosophiae Naturalis Principlea Mathematics, 1687, London, Royal
Society Press.
[73]
Neyman, J. and E.S. Pearson, "On the Problem of the Most Efficient Tests of
Statistical Hypotheses," Philosophical Trans. of Royal Society, 1928, 231, pp. 289-
337.
[74]
Ostendorf, M. and N. Velleux, "A Hierarchical Stochastic Model for Automatic
Prediction of Prosodic Boundary Location," Computational Linguistics, 1995,
20(1), pp. 27--54.
[75]
Paliwal, K. and W.B. Kleijn, "Quantization of LPC Parameters" in Speech Coding
and Synthesis, W.B. Kleijn and K.K. Paliwal, eds. 1995, Amsterdam, pp. 433-466,
Elsevier.
[76]
Paul, D.B., "An 800 bps Adaptive Vector Quantization in Speech Coding," IEEE
Int. Conf. on Acoustics, Speech and Signal Processing, 1983 pp. 73-76.
[77]
Paul, D.B., "The Lincoln Tied-Mixture HMM Continuous Speech Recognizer" in
Morgan Kaufmann Publishers 1990, San Mateo, CA, pp. 332-336.
[78]
Payne, H.J. and W.S. Meisel, "An Algorithm for Constructing Optimal Binary De-
cision Trees," IEEE Trans. on Computers, 1977, C-26(September), pp. 905-916.
[79]
Pearson, K., "Contributions to The Mathematical Theorem of Evolution," Philoso-
phical Trans. of Royal Society, 1894, 158A, pp. 71-110.
[80]
Pitrelli, J. and V. Zue, "A Hierarchical Model for Phoneme Duration in American
English" in Proc. of Eurospeech 1989.
[81]
Press, W.H., et al., Numerical Receipes in C, 1988, Cambridge, UK, Cambridge
University Press.
[82]
Quinlan, J.R., "Introduction of Decision Trees" in Machine Learning : An Artifical
Intelligence Approach, R. Michalski, J. Carbonell, and T. Mitchell, eds. 1986, Bos-
ton, M.A., pp. 1-86, Kluwer Academic Publishers.
[83]
Quinlan, J.R., "Generating Production Rules from Decision Trees," Int. Joint Conf.
on Artifical Intelligence, 1987 pp. 304-307.
[84]
Quinlan, J.R., C4.5: Programs for machine learning, 1993, San Francisco, Morgan
Kaufmann.

Historical Perspective and Further Reading
197
[85]
Rabiner, L. and B.H. Juang, "Speech Recognition System Design and Implementa-
tion Issues" in Fundamental of Speech Recognition, L. Rabiner and B.H. Juang,
eds. 1993, Englewood Cliffs, NJ, pp. 242-340, Prentice Hall.
[86]
Reddy, D.R., "Speech Recognition by Machine: A Review," IEEE Proc., 1976,
64(4), pp. 502-531.
[87]
Rosenblatt, F., "The Perceptron --- A Probabilistic Model for Information Storage
and Organization in the Brain," Psychological Review, 1958, 65, pp. 386-408.
[88]
Rummelhart, D.E., G.E. Hinton, and R.J. Williams, "Learning Internal Representa-
tions by Error Propagation" in Parallel Distributed Processing, D.E. Rumelhart and
J.L. McClelland, eds. 1986, Cambridge, MA, pp. 318-362, MIT Press.
[89]
Rummelhart, D.E., G.E. Hinton, and R.J. Williams, "Learning Representations by
Back-Propagating Errors," Nature, 1986, 323, pp. 533-536.
[90]
Rummelhart, D.E. and J.L. McClelland, Parallel Distributed Processing - Explora-
tions in the Microstructure of Cognition, Volume I : Foundations, 1986, Cam-
bridge, MA, MIT Press.
[91]
Rummelhart, D.E. and J.L. McClelland, Parallel Distributed Processing - Explora-
tions in the Microstructure of Cognition, Volume II: Psychological and Biological
Models, 1986, Cambridge, MA, MIT Press.
[92]
Schalkoff, R.J., Digital Image Processing and Computer Vision, 1989, New York,
NY, John Wiley & Sons.
[93]
Sejnowski, T.J. and C.R. Rosenberg, "Parallel Networks that Learn to Pronounce
English Text," Complex Systems, 1987, 1, pp. 145-168.
[94]
Sejnowski, T.J., et al., "Combining Visual And Acoustic Speech Signals with a
Neural Network Improve Intelligibility" in Advances in Neural Information Proc-
essing Systems 1990, San Mateo, CA, pp. 232-239, Morgan Kaufmann.
[95]
Sparkes, J.J., "Pattern Recognition and a Model of the Brain," Int. Journal of Man-
Machine Studies, 1969, 1(3), pp. 263-278.
[96]
Tappert, C., C.Y. Suen, and T. Wakahara, "The State of the Art in On-Line Hand-
writing Recognition," IEEE Trans. on Pattern Analysis and Machine Intelligence,
1990, 12(8), pp. 787-808.
[97]
Tesauro, G. and T.J. Sejnowski, "A Neural Network That Learns to Play Back-
gammon," Neural Information Processing Systems, American Institute of Physics,
1988, pp. 794-803.
[98]
White, H., "Learning in Artificial Neural Networks: A Statistical Perspective,"
Neural Computation, 1989, 1(4), pp. 425--464.
[99]
Winston, P.H., Artificial Intelligence, 1984, Reading, MA, Addison-Wesley.
[100]
Wu, C.F.J., "On The Convergence Properties of the EM Algorithm," The Annals of
Statistics, 1983, 11(1), pp. 95-103.
[101]
Young, J.Z., Programmes of the Brain, 1975, Oxford, England, Oxford University
Press.

201
C H A P T E R
5
Digital Signal ProcessingEquation Section 5
One of the most popular ways of characteriz-
ing speech is in terms of a signal or acoustic waveform. Shown in Figure 5.1 is a representa-
tion of the speech signal that ensures that the information content can be easily extracted by
human listeners or computers. This is why digital signal processing plays a fundamental role
for spoken language processing. We describe here the fundamentals of digital signal proc-
essing: digital signals and systems, frequency-domain transforms for both continuous and
discrete frequencies, digital filters, the relationship between analog and digital signals, fil-
terbanks, and stochastic processes. In this chapter we set the mathematical foundations of
frequency analysis that allow us to develop specific techniques for speech signals in Chapter
6.
The main theme of this chapter is the development of frequency-domain methods
computed through the Fourier transform. When we boost the bass knob in our amplifier we
are increasing the gain at low frequencies, and when we boost the treble knob we are in-
creasing the gain at high frequencies. Representation of speech signals in the frequency do-
main is especially useful because the frequency structure of a phoneme is generally unique.

202
Digital Signal Processing
Figure 5.1 Signal processing is both a representation and a transformation that allows a useful
information extraction from a source. The representation and transformation are based on a
model of the signal, often parametric, that is convenient for subsequent processing.
5.1.
DIGITAL SIGNALS AND SYSTEMS
To process speech signals, it is convenient to represent them mathematically as functions of
a continuous variable t, which represents time. Let us define an analog signal
( )
ax t
as a
function varying continuously in time. If we sample the signal x with a sampling period T
(i.e., t
nT
=
), we can define a discrete-time signal as
[ ]
(
)
a
x n
x nT
=
, also known as digital
signal1. In this book we use parentheses to describe an analog signal and brackets for digital
signals. Furthermore we can define the sampling frequency
sF as
1/
sF
T
=
, the inverse of
the sampling period T. For example, for a sampling rate
8kHz
sF =
, its corresponding sam-
pling period is 125 microseconds. In Section 5.5 it is shown that, under some circumstances,
the analog signal
( )
ax t
can be recovered exactly from the digital signal
[ ]
x n . Figure 5.2
shows an analog signal and its corresponding digital signal. In subsequent figures, for con-
venience, we will sometimes plot digital signals as continuous functions.
1 Actually the term digital signal is defined as a discrete-time signal whose values are represented by integers within
a range, whereas a general discrete-time signal would be represented by real numbers. Since the term digital signal
is much more commonly used, we will use that term, except when the distinction between them is necessary.
Information
Source
Measurement
/Observation
Signal
Representation
Signal
Transformation
Extraction/
Utilization of
Information
Signal
Processing

Digital Signals and Systems
203
Figure 5.2 Analog signal and its corresponding digital signal.
The term Digital Signal Processing (DSP) refers to methods for manipulating the se-
quence of numbers
[ ]
x n
in a digital computer. The acronym DSP is also used to refer to a
Digital Signal Processor, i.e., a microprocessor specialized to perform DSP operations.
We start with sinusoidal signals and show they are the fundamental signals for linear
systems. We then introduce the concept of convolution and linear time-invariant systems.
Other digital signals and nonlinear systems are also introduced.
5.1.1.
Sinusoidal Signals
One of the most important signals is the sine wave or sinusoid
0
0
0
0
[ ]
cos(
)
x n
A
n
ω
φ
=
+
(5.1)
where
0
A is the sinusoid’s amplitude,
0
ω
the angular frequency and
0
φ the phase. The an-
gle in the trigonometric functions is expressed in radians, so that the angular frequency
0
ω
is related to the normalized linear frequency
0f
by the relation
0
0
2
f
ω
π
=
, and
0
0
1
f
≤
≤
.
This signal is periodic2 with period
0
0
1/
T
f
=
. In Figure 5.3 we can see an example of a
sinusoid with frequency
0
0.04
f =
, or a period of
0
25
T =
samples.
Sinusoids are important because speech signals can be decomposed as sums of sinu-
soids. When we boost the bass knob in our amplifier we are increasing the gain for sinusoids
of low frequencies, and when we boost the treble knob we are increasing the gain for sinu-
soids of high frequencies.
2 A signal x[n] is periodic with period N if and only if x[n]=x[n+N], which requires
0
2
/ N
ω
π
=
. This means that
the digital signal in Eq. (5.1) is not periodic for all values of
0
ω , even though its continuous signal counterpart
0
0
0
( )
cos(
)
x t
A
t
ω
φ
=
+
is periodic for all values of
0
ω
(see Section 5.5).

204
Digital Signal Processing
0
10
20
30
40
50
60
70
80
-1
-0.5
0
0.5
1
Time (samples)
Amplitude
Figure 5.3 A digital sinusoid with a period of 25 samples.
What is the sum of two sinusoids
0[ ]
x n
and
1[ ]
x n
of the same frequency
0
ω
but dif-
ferent amplitudes
0
A and
1A , and phases
0
φ and
1φ ? The answer is another sinusoid of the
same frequency but a different amplitude A and phase φ. While this can be computed
through trigonometric identities, it is somewhat tedious and not very intuitive. For this rea-
son we introduce another representation based on complex numbers, which proves to be
very useful when we study digital filters.
Figure 5.4 Complex number representation in Cartesian form z
x
jy
=
+
and polar form
j
z
Ae φ
=
. Thus
cos
x
A
φ
=
and
sin
y
A
φ
=
.
A complex number x can be expressed as z = x+jy, where
1
j =
−, x is the real part
and y is the imaginary part, with both x and y being real numbers. Using Euler’s relation,
given a real number φ, we have
cos
sin
je
j
φ
φ
φ
=
+
(5.2)
so that the complex number z can also be expressed in polar form as
j
z
Ae φ
=
, where A is
the amplitude and φ is the phase. Both representations can be seen in Figure 5.4, where the
real part is shown in the abscissa (x-axis) and the imaginary part in the ordinate (y-axis).
Using complex numbers, the sinusoid in Eq. (5.1) can be expressed as the real part of
the corresponding complex exponential
0
0
(
)
0
0
0
0
0
[ ]
cos(
)
Re{
}
j
n
x n
A
n
A e
ω
φ
ω
φ
+
=
+
=
(5.3)
and thus the sum of two complex exponential signals equals
y
A
φ
x

Digital Signals and Systems
205
(
)
0
0
0
1
0
0
0
0
1
(
)
(
)
(
)
0
1
0
1
j
n
j
n
j
n
j
j
n
j
n
j
j
A e
Ae
e
A e
Ae
e
Ae
Ae
ω
φ
ω
φ
ω
φ
ω
ω
φ
φ
φ
+
+
+
+
=
+
=
=
(5.4)
Taking the real part in both sides results in
0
0
0
2
0
1
0
cos(
)
cos(
)
cos(
)
A
n
A
n
A
n
ω
φ
ω
φ
ω
φ
+
+
+
=
+
(5.5)
or in other words, the sum of two sinusoids of the same frequency is another sinusoid of the
same frequency.
To compute A and φ, dividing Eq. (5.4) by
0
j
n
e
ω
leads to a relationship between the
amplitude A and phase φ :
0
1
0
1
j
j
j
A e
A e
Ae
φ
φ
φ
+
=
(5.6)
Equating real and imaginary parts in Eq. (5.6) and dividing them we obtain:
0
0
1
1
0
0
1
1
sin
sin
tan
cos
cos
A
A
A
A
φ
φ
φ
φ
φ
+
=
+
(5.7)
and adding the squared of real and imaginary parts and using trigonometric identities3
2
2
2
0
1
0
1
0
1
2
cos(
)
A
A
A
A A
φ
φ
=
+
+
−
(5.8)
Figure 5.5 Geometric representation of the sum of two sinusoids of the same frequency. It fol-
lows the complex number representation in Cartesian form of Figure 5.4.
This complex representation of Figure 5.5 lets us analyze and visualize the amplitudes
and phases of sinusoids of the same frequency as vectors. The sum of N sinusoids of the
same frequency is another sinusoid of the same frequency that can be obtained by adding the
real and imaginary parts of all complex vectors. In Section 5.1.3.3 we show that the output
of a linear time-invariant system to a sinusoid is another sinusoid of the same frequency.
3
2
2
sin
cos
1
φ
φ
+
=
and cos(
)
cos cos
sin sin
a
b
a
b
a
b
−
=
+
 0
1

A0
A1
A

206
Digital Signal Processing
5.1.2.
Other Digital Signals
In the field of digital signal processing there are other signals that repeatedly arise and that
are shown in Table 5.1.
Table 5.1 Some useful digital signals: the Kronecker delta, unit step, rectangular signal, real
exponential (
1
a <
) and real part of a complex exponential (
1
r < ).
Kronecker
delta,
or unit impulse
1
0
[ ]
0
n
n
otherwise
δ
=

= 

Unit step
1
0
[ ]
0
0
n
u n
n
≥

= 
<

Rectangular
signal
1
0
rect [ ]
0
N
n
N
n
otherwise
≤
<

= 

Real exponential
[ ]
[ ]
n
x n
a u n
=
Complex
exponential
0
0
0
[ ]
[ ]
[ ]
(cos
sin
) [ ]
jn
n
n
n
x n
a u n
r e
u n
r
n
j
n
u n
ω
ω
ω
=
=
=
+
If
1
r =
and
0
0
ω ≠
we have a complex sinusoid as shown in Section 5.1.1. If
0
0
ω =
we have a real exponential signal, and if
1
r <
and
0
0
ω ≠
we have an exponentially decay-
ing oscillatory sequence, also known as a damped sinusoid.
5.1.3.
Digital Systems
A digital system is a system that, given an input signal x[n], generates an output signal y[n]:
[ ]
{ [ ]}
y n
T x n
=
(5.9)
whose input/output relationship can be seen in Figure 5.6.
Figure 5.6 Block diagram of a digital system whose input is digital signal x[n], and whose
output is digital signal y[n].
In general, a digital system T is defined to be linear iff (if and only if)
1 1
2
2
1
1
2
2
{
[ ]
[ ]}
{ [ ]}
{
[ ]}
T a x n
a x n
a T x n
a T x n
+
=
+
(5.10)
T{}
y[n]
x[n]
…
Re{x[n]}
n
n
n
n
n

Digital Signals and Systems
207
for any values of
1a ,
2a and any signals
1[ ]
x n and
2[ ]
x n .
Here, we study systems according to whether or not they are linear and/or time invari-
ant.
5.1.3.1.
Linear Time-Invariant Systems
A system is time-invariant if given Eq. (5.9), then
0
0
[
]
{ [
]}
y n
n
T x n
n
−
=
−
(5.11)
Linear digital systems of a special type, the so-called linear time-invariant (LTI)4, are de-
scribed by
[ ]
[ ] [
]
[ ]
[ ]
k
y n
x k h n
k
x n
h n
∞
=−∞
=
−
=
∗

(5.12)
where ∗is defined as the convolution operator. It is left to the reader to show that the linear
system in Eq. (5.12) indeed satisfies Eq. (5.11).
LTI systems are completely characterized by the signal
[ ]
h n , which is known as the
system’s impulse response because it is the output of the system when the input is an im-
pulse
[ ]
[ ]
x n
n
δ
=
. Most of the systems described in this book are LTI systems.
Table 5.2 Properties of the convolution operator.
Commutative
[ ]
[ ]
[ ]
[ ]
x n
h n
h n
x n
∗
=
∗
Associative
(
)
(
)
1
2
1
2
1
2
[ ]
[ ]
[ ]
[ ]
[ ]
[ ]
[ ]
[ ]
[ ]
x n
h n
h n
x n
h n
h n
x n
h n
h n
∗
∗
=
∗
∗
=
∗
∗
Distributive
(
)
1
2
1
2
[ ]
[ ]
[ ]
[ ]
[ ]
[ ]
[ ]
x n
h n
h n
x n
h n
x n
h n
∗
+
=
∗
+
∗
The convolution operator is commutative, associative and distributive as shown in
Table 5.2 and Figure 5.7.
Figure 5.7 The block diagrams on the left, representing the commutative property, are equiva-
lent. The block diagrams on the right, representing the distributive property, are also equiva-
lent.
4 Actually the term linear time-invariant (LTI) systems is typically reserved for continuous or analog systems, and
linear shift-invariant system is used for discrete-time signals, but we will use LTI for discrete-time signals too since
it is widely used in this context.
h1[n]
h2[n]
h2[n]
h1[n]
h n
h n
1
2
[ ]
[ ]

h1[n]
h2[n]
h1[n]+h2[n]

208
Digital Signal Processing
5.1.3.2.
Linear Time-Varying Systems
An interesting type of digital systems is that whose output is a linear combination of the
input signal at different times:
[ ]
[ ] [ ,
]
k
y n
x k g n n
k
∞
=−∞
=
−

(5.13)
The digital system in Eq. (5.13) is linear, since it satisfies Eq. (5.10). The Linear
Time-Invariant systems of Section 5.1.3.1 are a special case of Eq. (5.13) when
[ ,
]
[
]
g n n
k
h n
k
−
=
−
. The systems in Eq. (5.13) are called linear time-varying (LTV) sys-
tems, because the weighting coefficients can vary with time.
A useful example of such system is the so-called amplitude modulator
0
[ ]
[ ]cos
y n
x n
n
ω
=
(5.14)
used in AM transmissions. As we show in Chapter 6, speech signals are the output of LTV
systems. Since these systems are difficult to analyze, we often approximate them with linear
time-invariant systems.
Table 5.3 Examples of nonlinear systems for speech processing. All of them are memoryless
except for the median smoother.
Nonlinear System
Equation
Median Smoother
of order (2N+1)
[ ]
median{ [
],
, [ ],
, [
]}
y n
x n
N
x n
x n
N
=
−
+


Full-Wave Rectifier
[ ]
[ ]
y n
x n
=
Half-Wave Rectifier
[ ]
[ ]
0
[ ]
0
[ ]
0
x n
x n
y n
x n
≥

= 
<

Frequency Modulator
(
)
0
[ ]
cos
[ ]
y n
A
x n
n
ω
ω
=
+ ∆
Hard-Limiter
[ ]
[ ]
[ ]
[ ]
[ ]
A
x n
A
y n
x n
x n
A
A
x n
A
≥


=
<

 −
≤−

Uniform Quantizer
(L-bit) with 2
2L
N =
intervals of width ∆
(
)
(
)
(
)
(
)
1/ 2
[ ]
(
1)
1/ 2
[ ]
(
1)
0
1
[ ]
1/ 2
[ ]
(
1)
0
1
1/ 2
[ ]
(
1)
N
x n
N
m
m
x n
m
m
N
y n
m
m
x n
m
m
N
N
x n
N

−
∆
≥
−
∆

+
∆
∆≤
<
+
∆
≤
<
−

=  −
+
∆
−∆≤
< −
−
∆
<
<
−

 −
+
∆
< −
−
∆


Continuous-Frequency Transforms
209
5.1.3.3.
Nonlinear Systems
Many nonlinear systems do not satisfy Eq. (5.10). Table 5.3 includes a list of typical nonlin-
ear systems used in speech processing. All these nonlinear systems are memoryless, because
the output at time n depends only on the input at time n, except for the median smoother of
order (2N + 1) whose output depends also on the previous and the following N samples.
5.2.
CONTINUOUS-FREQUENCY TRANSFORMS
A very useful transform for LTI systems is the Fourier transform, because it uses complex
exponentials as its basis functions, and its generalization: the z-transform. In this section we
cover both transforms, which are continuous functions of frequency, and their properties.
5.2.1.
The Fourier Transform
It is instructive to see what the output of a LTI system with impulse response
[ ]
h n
is when
the input is a complex exponential. Substituting
0
[ ]
j
n
x n
e
ω
=
in Eq. (5.12) and using the
commutative property of the convolution we obtain
0
0
0
0
0
(
)
[ ]
[ ]
[ ]
(
)
j
n k
j
n
j
k
j
n
j
k
k
y n
h k e
e
h k e
e
H e
ω
ω
ω
ω
ω
∞
∞
−
−
=−∞
=−∞
=
=
=


(5.15)
which is another complex exponential of the same frequency and amplitude multiplied by
the complex quantity
0
(
)
j
H e
ω
given by
(
)
[ ]
j
j n
n
H e
h n e
ω
ω
∞
−
=−∞
= 
(5.16)
Since the output of a LTI system to a complex exponential is another complex exponential,
it is said that complex exponentials are eigensignals of LTI systems, with the complex quan-
tity
0
(
)
j
H e
ω
being their eigenvalue.
The quantity
(
)
j
H e ω
is defined as the discrete-time Fourier transform of h[n]. It is
clear from Eq. (5.16) that
(
)
j
H e ω
is a periodic function of ω with period 2π , and there-
fore we need to keep only one period to fully describe it, typically
π
ω
π
−
<
<
(Figure 5.8).
(
)
j
H e ω
is a complex function of ω which can be expressed in terms of the real and
imaginary parts:
(
)
(
)
(
)
j
j
j
r
i
H e
H
e
jH e
ω
ω
ω
=
+
(5.17)
or in terms of the magnitude and phase as

210
Digital Signal Processing
arg[
(
)]
(
)
(
)
j
j
j
j
H e
H e
H e
e
ω
ω
ω
=
(5.18)
Thus if the input to the LTI system is a sinusoid as in Eq. (5.1), the output will be
(
)
0
0
0
0
0
0
[ ]
(
) cos
arg{
(
)}
j
j
y n
A H e
n
H e
ω
ω
ω
φ
=
+
+
(5.19)
according to Eq. (5.15). Therefore if
0
(
)
1
j
H e
ω
> , the LTI system will amplify that fre-
quency, and likewise it will attenuate, or filter it, it if
0
(
)
1
j
H e
ω
< . That is one reason why
these systems are also called filters. The Fourier transform
(
)
j
H e ω
of a filter h[n] is called
the system’s frequency response or transfer function.
Figure 5.8
(
)
j
H e ω
is a periodic function of ω .
The angular frequency ω is related to the normalized linear frequency f by the sim-
ple relation
2
f
ω
π
=
. We show in Section 5.5 that linear frequency
lf and normalized fre-
quency f are related by
l
s
f
fF
=
, where
sF is the sampling frequency.
The inverse discrete-time Fourier transform is defined as
1
[ ]
(
)
2
j
j n
h n
H e
e
d
π
ω
ω
π
ω
π
−
=

(5.20)
The Fourier transform is invertible, and Eq. (5.16) and (5.20) are transform pairs:
(
)
1
1
[ ]
(
)
[ ]
2
2
1
[ ]
[ ] [
]
[ ]
2
j
j n
j m
j n
m
j
n m
m
m
h n
H e
e
d
h m e
e
d
h m
e
d
h m
n
m
h n
π
π
ω
ω
ω
ω
π
π
π
ω
π
ω
ω
π
π
ω
δ
π
∞
−
−
−
=−∞
∞
∞
−
−
=−∞
=−∞


=
=




=
=
−
=






(5.21)
since
(
)
1
[
]
2
j
n m
e
d
n
m
π
ω
π
ω δ
π
−
−
=
−

(5.22)
A sufficient condition for the existence of the Fourier transform is
H e j
(
)


2π
-2π
π
-π

Continuous-Frequency Transforms
211
[ ]
n
h n
∞
=−∞
< ∞

(5.23)
Although we have computed the Fourier transform of the impulse response of a filter
h[n], Eq. (5.16) and (5.20) can be applied to any signal x[n].
5.2.2.
Z-Transform
The z-transform is a generalization of the Fourier transform. The z-transform of a digital
signal
[ ]
h n is defined as
( )
[ ]
n
n
H z
h n z
∞
−
=−∞
= 
(5.24)
where z is a complex variable. Indeed, the Fourier transform of
[ ]
h n
equals its z-transform
evaluated at
j
z
e ω
=
. While the Fourier and z-transforms are often used interchangeably, we
normally use the Fourier transform to plot the filter’s frequency response, and the z-
transform to analyze more general filter characteristics, given its polynomial functional
form. We can also use the z-transform for unstable filters, which do not have Fourier trans-
forms.
Since Eq. (5.24) is an infinite sum, it is not guaranteed to exist. A sufficient condition
for convergence is:
[ ]
n
n
h n
z
∞
−
=−∞
< ∞

(5.25)
which is true only for a region of convergence (ROC) in the complex z-plane
1
2
R
z
R
<
<
as indicated in Figure 5.9.
Figure 5.9 Region of convergence of the z-transform in the complex plane.
For a signal
[ ]
h n to have a Fourier transform, its z-transform
( )
H z
has to include the
unit circle, |
| 1
z = , in its convergence region. Therefore, a sufficient condition for the exis-
tence of the Fourier transform is given in Eq. (5.23) by applying Eq. (5.25) to the unit circle.
An LTI system is defined to be causal if its impulse response is a causal signal, i.e.
[ ]
0
h n =
for
0
n <
. Similarly, a LTI system is anti-causal if
[ ]
0
h n =
for
0
n >
. While all
R1
R2

212
Digital Signal Processing
physical systems are causal, noncausal systems are still useful since causal systems could be
decomposed into causal and anti-causal systems.
A system is defined to be stable if for every bounded input it produces a bounded out-
put. A necessary and sufficient condition for an LTI system to be stable is
[ ]
n
h n
∞
=−∞
< ∞

(5.26)
which means, according to Eq. (5.23), that
[ ]
h n
has a Fourier transform, and therefore that
its z-transform includes the unit circle in its region of convergence.
Just as in the case of Fourier transforms, we can use the z-transform for any signal, not
just for a filter’s impulse response.
The inverse z-transform is defined as
1
1
[ ]
( )
2
n
h n
H z z
dz
j
π
−
=

(5.27)
where the integral is performed along a closed contour that is within the region of conver-
gence. Eqs. (5.24) and (5.27) plus knowledge of the region of convergence form a transform
pair: i.e. one can be exactly determined if the other is known. If the integral is performed
along the unit circle (i.e., doing the substitution
j
z
e ω
=
) we obtain Eq. (5.20), the inverse
Fourier transform.
5.2.3.
Z-Transforms of Elementary Functions
In this section we compute the z-transforms of the signals defined in Table 5.1. The z-
transforms of such signals are summarized in Table 5.4. In particular we compute the z-
transforms of left-sided and right-sided complex exponentials, which are essential to com-
pute the inverse z-transform of rational polynomials. As we see in Chapter 6, speech signals
are often modeled as having z-transforms that are rational polynomials.
Table 5.4 Z-transforms of some useful signals together with their region of convergence.
Signal
Z-Transform
Region of Convergence
1[ ]
[
]
h n
n
N
δ
=
−
1( )
N
H z
z−
=
0
z ≠
2[ ]
[ ]
[
]
h n
u n
u n
N
=
−
−
2
1
1
( )
1
N
z
H
z
z
−
−
−
=
−
0
z ≠
3[ ]
[ ]
n
h n
a u n
=
3
1
1
( )
1
H
z
az−
=
−
|
| |
|
a
z
<
4[ ]
[
1]
n
h n
a u
n
= −
−−
4
1
1
( )
1
H
z
az−
=
−
|
| |
|
z
a
<

Continuous-Frequency Transforms
213
5.2.3.1.
Right-Sided Complex Exponentials
A right-sided complex exponential sequence
3[ ]
[ ]
n
h n
a u n
=
(5.28)
has a z-transform given by
1
3
1
1
0
1 (
)
1
( )
lim
1
1
N
n
n
N
n
az
H
z
a z
az
az
−
∞
−
−
−
→∞
=
−
=
=
=
−
−

for
|
| |
|
a
z
<
(5.29)
by using the sum of the terms of a geometric sequence and making N →∞. This region of
convergence (|
| |
|
a
z
<
) is typical of causal signals (those that are zero for
0
n <
).
When a z-transform is expressed as the ratio of two polynomials, the roots of the nu-
merator are called zeros, and the roots of the denominator are called poles. Zeros are the
values of z for which the z-transform equals 0, and poles are the values of z for which the z-
transform equals infinity.
3( )
H
z
has a pole at z
a
=
, because its value goes to infinity at z
a
=
. According to
Eq. (5.26),
3[ ]
h n is a stable signal if and only if |
| 1
a < , or in other words, if its pole is in-
side the unit circle. In general, a causal and stable system has all its poles inside the unit
circle. As a corollary, a system which has poles outside the unit circle is either noncausal or
unstable or both. This is a very important fact, which we exploit throughout the book.
5.2.3.2.
Left-Sided Complex Exponentials
A left-sided complex exponential sequence
4[ ]
[
1]
n
h n
a u
n
= −
−−
(5.30)
has a z-transform given by
1
4
1
0
1
1
1
1
( )
1
1
1
1 1
1
1
n
n
n
n
n
n
n
n
n
H
z
a z
a
z
a
z
a z
a z
a z
az
−
∞
∞
−
−
−
=−∞
=
=
−
−
−
−
= −
= −
= −
−
= −
=
=
−
−
−



for |
| |
|
z
a
<
(5.31)
This region of convergence (|
| |
|
z
a
<
) is typical of noncausal signals (those that are nonzero
for
0
n <
). Observe that
3( )
H
z
and
4( )
H
z
are functionally identical and only differ in the
region of convergence. In general, the region of convergence of a signal that is nonzero for
n
−∞<
< ∞is
1
2
|
|
R
z
R
<
<
.

214
Digital Signal Processing
5.2.3.3.
Inverse Z-Transform of Rational Functions
Integrals in the complex plane such as Eq. (5.27) are not easy to do, but fortunately they are
not necessary for the special case of
( )
H z
being a rational polynomial transform. In this
case, partial fraction expansion can be used to decompose the signal into a linear combina-
tion of signals like
1[ ]
h n ,
3[ ]
h n and
4[ ]
h n as in Table 5.4.
For example,
1
5
1
2
2
8
( )
2
5
3
z
H
z
z
z
−
−
−
+
=
−
−
(5.32)
has as roots of its denominator
3, 1/ 2
z =
−
. Therefore it can be decomposed as
1
5
1
1
1
2
(2
2 )
(
6 )
( )
1 3
1
(1/ 2)
2
5
3
A
B
A
B
A
B z
H
z
z
z
z
z
−
−
−
−
−
+
+
−
=
+
=
−
+
−
−
(5.33)
so that A and B are the solution of the following set of linear equations:
2
2
2
6
8
A
B
A
B
+
=
−
=
(5.34)
whose solution is
2
A =
and
1
B = −, and thus Eq. (5.33) is expressed as
5
1
1
1
1
( )
2 1 3
1
(1/ 2)
H
z
z
z
−
−




=
−



−
+




(5.35)
However, we cannot compute the inverse z-transform unless we know the region of
convergence. If, for example, we are told that the region of convergence includes the unit
circle (necessary for the system to be stable), then the inverse transform of
4
1
1
( )
1 3
H
z
z−
=
−
(5.36)
must have a region of convergence of |
| 3
z <
according to Table 5.4, and thus be a left-sided
complex exponential:
4[ ]
3
[
1]
n
h n
u
n
= −
−−
(5.37)
and the transform of
3
1
1
( )
1
(1/ 2)
H
z
z−
=
+
(5.38)
must have a region of convergence of 1/ 2 |
|
z
<
according to Table 5.4, and thus be a right-
sided complex exponential:

Continuous-Frequency Transforms
215
3[ ]
( 1/ 2)
[ ]
n
h n
u n
= −
(5.39)
so that
5[ ]
2 3
[
1]
( 1/ 2)
[ ]
n
n
h n
u
n
u n
= −⋅
−−
−−
(5.40)
While we only showed an example here, the method used generalizes to rational trans-
fer functions with more poles and zeros.
5.2.4.
Properties of the Z and Fourier Transform
In this section we include a number of properties that are used throughout the book and that
can be derived from the definition of Fourier and z-transforms. Of special interest are the
convolution property and Parseval’s theorem, which are described below.
5.2.4.1.
The Convolution Property
The z-transform of
[ ]
y n , convolution of
[ ]
x n
and
[ ]
h n , can be expressed as a function of
their z-transforms:
(
)
( )
[ ]
[ ] [
]
[ ]
[
]
[ ]
[ ]
[ ]
( )
( )
( )
n
n
n
n
k
n
n k
k
n
k
n
k
k
Y z
y n z
x k h n
k
z
x k
h n
k z
x k
h n z
x k z
H z
X z H z
∞
∞
∞
−
−
=−∞
=−∞
=−∞
∞
∞
∞
∞
−
−
+
=−∞
=−∞
=−∞
=−∞
∞
−
=−∞


=
=
−








=
−
=








=
=

 





(5.41)
which is the fundamental property of LTI systems: “The z-transform of the convolution of
two signals is the product of their z-transforms.” This is also known as the convolution
property. The ROC of
( )
Y z
is now the intersection of the ROCs of
( )
X z
and
( )
H z
and
cannot be empty for
( )
Y z
to exist.
Likewise, we can obtain a similar expression for the Fourier transforms:
(
)
(
)
(
)
j
j
j
Y e
X e
H e
ω
ω
ω
=
(5.42)
A dual version of the convolution property can be proven for the product of digital
signals:
1
[ ] [ ]
(
)
(
)
2
j
j
x n y n
X e
Y e
ω
ω
π
↔
∗
(5.43)
whose transform is the continuous convolution of the transforms with a scale factor. The
convolution of functions of continuous variables is defined as

216
Digital Signal Processing
( )
( )* ( )
( ) (
)
y t
x t
h t
x
h t
d
τ
τ
τ
∞
−∞
=
=
−

(5.44)
Note how this differs from the discrete convolution of Eq. (5.12).
5.2.4.2.
Power Spectrum and Parseval’s Theorem
Let’s define the autocorrelation of signal x[n] as
[ ]
[
]
[ ]
[ ]
[ (
)]
[ ]
[
]
xx
m
l
R
n
x m
n x m
x l x
n
l
x n
x
n
∞
∞
∗
∗
∗
=−∞
=−∞
=
+
=
−
−
=
∗
−


(5.45)
where the superscript asterisk (*) means complex conjugate5 and should not be confused
with the convolution operator.
Using the fundamental property of LTI systems in Eq. (5.42) and the symmetry prop-
erties in Table 5.5, we can express its Fourier transform
( )
xx
S
ω
as
2
( )
( )
( )
( )
xx
S
X
X
X
ω
ω
ω
ω
∗
=
=
(5.46)
which is the power spectrum. The Fourier transform of the autocorrelation is the power
spectrum:
[ ]
( )
xx
xx
R
n
S
ω
↔
(5.47)
or alternatively
1
[ ]
( )
2
j n
xx
xx
R
n
S
e
d
π
ω
π
ω
ω
π
−
=

(5.48)
If we set n = 0 in Eq. (5.48) and use Eq. (5.45) and (5.46), we obtain
2
2
1
[ ]
( )
2
n
x n
X
d
π
π
ω
ω
π
∞
−
=−∞
=


(5.49)
which is called Parseval’s theorem and says that we can compute the signal’s energy in the
time domain or in the frequency domain.
In Table 5.5 we list, in addition to the convolution property and Parseval’s theorem, a
number of properties that can be derived from the definition of Fourier and z-transforms.
5.3.
DISCRETE-FREQUENCY TRANSFORMS
Here we describe transforms, including the DFT, DCT and FFT, that take our discrete-time
signal into a discrete frequency representation. Discrete-frequency transforms are the natural
5 If
j
z
x
jy
Ae φ
=
+
=
, its complex conjugate is defined as
j
z
x
jy
Ae
φ
∗
−
=
−
=

Discrete-Frequency Transforms
217
transform for periodic signals, though we show in Section 5.7 and Chapter 6 how they are
also useful for aperiodic signals such as speech.
Table 5.5 Properties of the Fourier and z-transforms.
Property
Signal
Fourier Transform
z-Transform
Linearity
1
2
[ ]
[ ]
ax n
bx n
+
1
2
(
)
(
)
j
j
aX e
bX
e
ω
ω
+
1
2
( )
( )
aX
z
bX
z
+
[
]
x
n
−
(
)
j
X e
ω
−
1
(
)
X z−
[ ]
x n
∗
(
)
j
X
e
ω
∗
−
(
)
X
z
∗
∗
[
]
x
n
∗−
(
)
j
X
e ω
∗
(1/
)
X
z
∗
∗
[ ]
x n real
(
)
j
X e ω
is Hermitian
(
)
(
)
j
j
X e
X
e
ω
ω
−
∗
=
(
)
j
X e ω
is even6
Re{
(
)}
j
X e ω
is even
{
}
arg
(
)
j
X e ω
is odd7
{
}
Im
(
)
j
X e ω
is odd
(
)
( )
X z
X
z
∗
∗
=
Even{ [ ]}
x n
Re{
(
)}
j
X e ω
Symmetry
Odd{ [ ]}
x n
Im{
(
)}
j
j
X e ω
Time-shifting
0
[
]
x n
n
−
0
(
)
j n
j
X e
e
ω
ω
−
0
( )
n
X z z
−
0
[ ]
j
n
x n e
ω
0
(
)
(
)
j
X e
ω ω
−
0
(
)
j
X e
z
ω
−
Modulation
0
[ ]
n
x n z
0
( /
)
X z z
[ ]
[ ]
x n
h n
∗
(
)
(
)
j
j
X e
H e
ω
ω
( )
( )
X z H z
Convolution
[ ] [ ]
x n y n
1
(
)
(
)
2
j
j
X e
Y e
ω
ω
π
∗
Parseval’s
Theorem
[ ]
[
]
[ ]
xx
m
R
n
x m
n x m
∞
∗
=−∞
=
+

2
( )
( )
xx
S
X
ω
ω
=
( )
(1/
)
X z X
z
∗
∗
A discrete transform of a signal
[ ]
x n is another signal defined as
[ ]
{ [ ]}
X k
x n
= Τ
(5.50)
Linear transforms are special transforms that decompose the input signal
[ ]
x n
into a
linear combination of other signals:
6 A function f(x) is called even if and only if
( )
(
)
f x
f
x
=
−
.
7 A function f(x) is called odd if and only if
( )
(
)
f x
f
x
= −
−
.

218
Digital Signal Processing
[ ]
[ ]
[ ]
k
k
x n
X k
n
ϕ
∞
=−∞
= 
(5.51)
where
[ ]
k n
ϕ
is a set of orthonormal functions
[ ],
[ ]
[
]
k
l
n
n
k
l
ϕ
ϕ
δ
<
>=
−
(5.52)
with the inner product defined as
[ ],
[ ]
[ ]
[ ]
k
l
k
l
n
n
n
n
n
ϕ
ϕ
ϕ
ϕ
∞
∗
=−∞
<
>= 
(5.53)
With this definition, the coefficients
[ ]
X k
are the projection of
[ ]
x n onto
[ ]
k n
ϕ
:
[ ]
[ ],
[ ]
k
X k
x n
n
ϕ
=<
>
(5.54)
as illustrated in Figure 5.10.
Figure 5.10 Orthonormal expansion of a signal x[n] in a two-dimensional space.
5.3.1.
The Discrete Fourier Transform (DFT)
If a
[ ]
N
x
n signal is periodic with period N then
[ ]
[
]
N
N
x
n
x
n
N
=
+
(5.55)
and the signal is uniquely represented by N consecutive samples. Unfortunately, since Eq.
(5.23) is not met, we cannot guarantee the existence of its Fourier transform. The Discrete
Fourier Transform (DFT) of a periodic signal
[ ]
N
x
n is defined as
1
2
/
0
[ ]
[ ]
N
j
nk N
N
N
n
X
k
x
n e
π
−
−
=
= 
0
k
N
≤
<
(5.56)
1
2
/
0
1
[ ]
[ ]
N
j
nk N
N
N
k
x
n
X
k e
N
π
−
=
=

0
n
N
≤
<
(5.57)
which are transform pairs. Equation (5.57) is also referred as a Fourier series expansion.
X0
0

X1
1

 0
1
x

Discrete-Frequency Transforms
219
-150
-100
-50
0
50
100
150
-0.5
0
0.5
1
1.5
In Figure 5.11 we see the approximation of a periodic square signal with period
100
N =
as a sum of 19 harmonic sinusoids, i.e., we used only the first 19
[ ]
N
X
k
coeffi-
cients in Eq. (5.57).
18
18
2
/
18
1
[0]
1
2
[ ]
[ ]
[ ]cos(2
/
)
j
nk N
N
N
N
N
k
k
X
x
n
X
k e
X
k
nk N
N
N
N
π
π
=−
=
=
=
+



(5.58)
Had we used 100 harmonic sinusoids, the periodic signal would have been reproduced
exactly. Nonetheless, retaining a smaller number of sinusoids can provide a decent approxi-
mation for a periodic signal.
Figure 5.11 Decomposition of a periodic square signal with period 100 samples as a sum of 19
harmonic sinusoids with frequencies
2
/100
k
k
ω
π
=
.
5.3.2.
Fourier Transforms of Periodic Signals
Using the DFT, we now discuss how to compute the Fourier transforms of a complex expo-
nential, an impulse train, and a general periodic signal, since they are signals often used in
DSP. We also present a relationship between the continuous-frequency Fourier transform
and the discrete Fourier transform.
5.3.2.1.
The Complex Exponential
One of the simplest periodic functions is the complex exponential
0
[ ]
j
n
x n
e
ω
=
. Since it has
infinite energy, we cannot compute its Fourier transform in its strict sense. Since such sig-
nals are so useful, we devise an alternate formulation.
First, let us define the function
1/
0
( )
0
d
otherwise
ω
ω
∆
∆
≤
< ∆

= 

(5.59)
which has the following property

220
Digital Signal Processing
( )
1
d
d
ω
ω
∞
∆
−∞
=

(5.60)
for all values of
0
∆>
.
It is useful to define the continuous delta function
( )
δ ω , also known as the Dirac
delta, as
0
( )
lim
( )
d
δ ω
ω
∆
∆→
=
(5.61)
which is a singular function and can be seen in Figure 5.12. The Dirac delta is a function of
a continuous variable and should not be confused with the Kronecker delta, which is a func-
tion of a discrete variable.
Figure 5.12 Representation of the
( )
δ ω
function and its approximation
( )
d
ω
∆
.
Using Eqs. (5.59) and (5.61) we can then see that
0
( ) ( )
lim
( )
( )
(0)
X
d
X
d
d
X
ω δ ω
ω
ω
ω
ω
∞
∞
∆
−∞
−∞
∆→
=
=


(5.62)
and similarly
0
0
( ) (
)
(
)
X
d
X
ω δ ω
ω
ω
ω
∞
−∞
−
=

(5.63)
so that
0
0
0
( ) (
)
(
) (
)
X
X
ω δ ω
ω
ω δ ω
ω
−
=
−
(5.64)
because the integrals on both sides are identical.
Using Eq. (5.63), we see that the convolution of
( )
X ω
and
0
(
)
δ ω
ω
−
is
0
0
0
( )
(
)
( ) (
)
(
)
X
X u
u du
X
ω
δ ω
ω
δ ω
ω
ω
ω
∞
−∞
∗
−
=
−
−
=
−

(5.65)
For the case of a complex exponential, inserting
( )
j n
X
e ω
ω =
into Eq. (5.63) results in
0
0
(
)
j
n
j n
e
d
e ω
ω
δ ω
ω
ω
∞
−∞
−
=

(5.66)
By comparing Eq. (5.66) with (5.20) we can then obtain
∆
1/∆
ω
ω
δ(ω)
d∆(ω)

Discrete-Frequency Transforms
221
0
0
2
(
)
j
n
e
ω
πδ ω
ω
↔
−
(5.67)
so that the Fourier transform of a complex exponential is an impulse concentrated at fre-
quency
0
ω .
5.3.2.2.
The Impulse Train
Since the impulse train
[ ]
[
]
N
k
p
n
n
kN
δ
∞
=−∞
=
−

(5.68)
is periodic with period N, it can be expanded in Fourier series according to (5.56) as
[ ]
1
N
P k =
(5.69)
so that using the inverse Fourier series Eq. (5.57),
[ ]
N
p
n can alternatively be expressed as
1
2
/
0
1
[ ]
N
j
kn N
N
k
p
n
e
N
π
−
=
=

(5.70)
which is an alternate expression to Eq. (5.68) as a sum of complex exponentials. Taking the
Fourier transform of Eq. (5.70) and using Eq. (5.67) we obtain
1
0
2
(
)
(
2
/
)
N
j
N
k
P e
k N
N
ω
π
δ ω
π
−
=
=
−

(5.71)
which is another impulse train in the frequency domain (See Figure 5.13). The impulse train
in the time domain is given in terms of the Kronecker delta, and the impulse train in the fre-
quency domain is given in terms of the Dirac delta.
Figure 5.13 An impulse train signal and its Fourier transform, which is also an impulse train.
5.3.2.3.
General Periodic Signals
We now compute the Fourier transform of a general periodic signal using the results of Sec-
tion 5.3.2.2 and show that, in addition to being periodic, the transform is also discrete. Given
a periodic signal
[ ]
N
x
n with period N, we define another signal
[ ]
x n :
[ ]
0
[ ]
0
N
x
n
n
N
x n
otherwise
≤
<

= 

(5.72)

222
Digital Signal Processing
so that
[ ]
[
]
[ ]
[
]
[ ]
[ ]
N
N
k
k
x
n
x n
kN
x n
n
kN
x n
p
n
δ
∞
∞
=−∞
=−∞
=
−
=
∗
−
=
∗


(5.73)
which is the convolution of
[ ]
x n with an impulse train
[ ]
N
p
n as in Eq. (5.68). Since
[ ]
x n is
of finite length, it has a Fourier transform
(
)
j
X e ω . Using the convolution property
(
)
(
)
(
)
j
j
j
N
N
X
e
X e
P e
ω
ω
ω
=
, where
(
)
j
N
P e ω
is the Fourier transform of
[ ]
N
p
n
as given by
Eq. (5.71), we obtain another impulse train:
2
/
2
(
)
(
) (
2
/
)
j
j
k N
N
k
X
e
X e
k N
N
ω
π
π
δ ω
π
∞
=−∞
=
−

(5.74)
Therefore the Fourier transform
(
)
j
N
X
e ω
of a periodic signal
[ ]
N
x
n can be expressed
in terms of samples
2
/
k
k N
ω
π
=
, spaced 2 / N
π
apart, of the Fourier transform
(
)
j
X e ω
of
[ ]
x n , one period of the signal
[ ]
N
x
n . The relationships between
[ ]
x n ,
[ ]
N
x
n ,
(
)
j
X e ω
and
(
)
j
N
X
e ω
are shown in Figure 5.14.
Figure 5.14 Relationships between finite and periodic signals and their Fourier transforms. On
one hand,
[ ]
x n
is a length N discrete signal whose transform
(
)
j
X e ω
is continuous and peri-
odic with period 2π . On the other hand,
[ ]
N
x
n
is a periodic signal with period N whose trans-
form
(
)
j
N
X
e ω
is discrete and periodic.
5.3.3.
The Fast Fourier Transform (FFT)
There is a family of fast algorithms to compute the DFT, which are called Fast Fourier
Transforms (FFT). Direct computation of the DFT from Eq. (5.56) requires
2
N
operations,
assuming that the trigonometric functions have been pre-computed. The FFT algorithm only
requires on the order of
2
log
N
N operations, so it is widely used for speech processing.
…
…
…
…
…
…
[ ]
x n
[ ]
N
x
n
(
)
j
X e ω
(
)
j
N
X
e ω

Discrete-Frequency Transforms
223
5.3.3.1.
Radix-2 FFT
Let’s express the discrete Fourier transform of x[n]
1
1
2
/
0
0
[ ]
[ ]
[ ]
N
N
j
nk N
nk
N
n
n
X k
x n e
x n W
π
−
−
−
=
=
=
=


0
k
N
≤
<
(5.75)
where we have defined for convenience
2
/
j
N
N
W
e
π
−
=
(5.76)
Equation (5.75) requires
2
N
complex multiplies and adds. Now, let’s suppose N is
even,
and
let
[ ]
[2 ]
f n
x
n
=
represent
the
even-indexed
samples
of
[ ]
x n ,
and
[ ]
[2
1]
g n
x
n
=
+
the odd-indexed samples. We can express Eq. (5.75) as
/ 2 1
/ 2 1
/ 2
/ 2
0
0
[ ]
[ ]
[ ]
[ ]
[ ]
N
N
nk
k
nk
k
N
N
N
N
n
n
X k
f n W
W
g n W
F k
W G k
−
−
=
=
=
+
=
+


(5.77)
where F[k] and G[k] are the N/2 point DFTs of
[ ]
f n
and
[ ]
g n , respectively. Since both
[ ]
F k
and
[ ]
G k
are defined for
0
/ 2
k
N
≤
<
, we need to also evaluate them for
/ 2
N
k
N
≤
<
, which is straightforward, since
[
/ 2]
[ ]
F k
N
F k
+
=
(5.78)
[
/ 2]
[ ]
G k
N
G k
+
=
(5.79)
If N/2 is also even, then both
[ ]
f n
and
[ ]
g n
can be decomposed into sequences of
even and odd indexed samples and therefore its DFT can be computed using the same proc-
ess. Furthermore, if N is an integer power of 2, this process can be iterated and it can be
shown that the number of multiplies and adds is
2
log
N
N , which is a significant saving
from
2
N . This is the decimation-in-time algorithm and can be seen in Figure 5.15. A dual
algorithm called decimation-in-frequency can be derived by decomposing the signal into its
first N/2 and its last N/2 samples.
5.3.3.2.
Other FFT Algorithms
Although the radix-2 FFT is the best known algorithm, there are other variants that are faster
and are more often used in practice. Among those are the radix-4, radix-8, split-radix and
prime-factor algorithm.
The same process used in the derivation of the radix-2 decimation-in-time algorithm
applies
if
we
decompose
the
sequences
into
four
sequences:
1[ ]
[4 ]
f n
x
n
=
,
2[ ]
[4
1]
f n
x
n
=
+
,
3[ ]
[4
2]
f n
x
n
=
+
, and
4[ ]
[4
3]
f n
x
n
=
+
. This is the radix-4 algorithm,

224
Digital Signal Processing
which can be applied when N is a power of 4, and is generally faster than an equivalent
radix-2 algorithm.
Similarly there are radix-8 and radix-16 algorithms for N being powers of 8 and 16 re-
spectively, which use fewer multiplies and adds. But because of possible additional control
logic, it is not obvious that they will be faster, and every algorithm needs to be optimized for
a given processor.
There are values of N, such as
128
N =
, for which we cannot use radix-4, radix-8 nor
radix-16, so we have to use the less efficient radix-2. A combination of radix-2 and radix-4,
called split-radix [5], has been shown to have fewer multiplies than both radix-2 and radix-
4, and can be applied to N being a power of 2.
Finally, another possible decomposition is
1
2
L
N
p p
p
=

with
ip being prime num-
bers. This leads to the prime-factor algorithm [2]. While this family of algorithms offers a
similar number of operations as the algorithms above, it offers more flexibility in the choice
of N.
Figure 5.15 Decimation in time radix-2 algorithm for an 8-point FFT.
5.3.3.3.
FFT Subroutines
Typically, FFT subroutines are computed in-place to save memory and have the form
fft (float *xr, float *xi, int n)
where xr and xi are the real and imaginary parts respectively of the input sequence, before
calling the subroutine, and the real and imaginary parts of the output transform, after return-
WN
0
x[0]
x[2]
x[4]
x[6]
x[1]
x[5]
x[3]
x[7]
X[0]
X[1]
X[2]
X[3]
X[4]
X[5]
X[6]
X[7]
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0
WN
0

Discrete-Frequency Transforms
225
ing from it. C code that implements a decimation-in-time radix-2 FFT of Figure 5.15 is
shown in Figure 5.16.
void fft2 (float *x, float *y, int n, int m)
{
int n1, n2, i, j, k, l;
float
xt, yt, c, s;
double
e, a;
/* Loop through all m stages */
n2 = n;
for (k = 0; k < m; k++) {
n1 = n2;
n2 = n2 / 2;
e = PI2 / n1;
for (j = 0; j < n2; j++) {
/* Compute Twiddle factors */
a = j * e;
c = (float) cos (a);
s = (float) sin (a);
/* Do the butterflies */
for (i = j; i < n; i += n1) {
l = i + n2;
xt = x[i] - x[l];
x[i] = x[i] + x[l];
yt = y[i] - y[l];
y[i] = y[i] + y[l];
x[l] = c * xt + s * yt;
y[l] = c * yt - s * xt;
}
}
}
/* Bit reversal: descrambling */
j = 0;
for (i = 0; i < n - 1; i++) {
if (i < j) {
xt = x[j];
x[j] = x[i];
x[i] = xt;
xt = y[j];
y[j] = y[i];
y[i] = xt;
}
k = n / 2;
while (k <= j) {
j -= k;
k /= 2;
}
j += k;
}
}
Figure 5.16 C source for a decimation-in-time radix-2 FFT. Before calling the subroutine, x
and y contain the real and imaginary parts of the input signal respectively. After returning from
the subroutine, x and y contain the real and imaginary parts of the Fourier transform of the in-
put signal. n is the length of the FFT and is related to m by
2m
n =
.
The first part of the subroutine in Figure 5.16 is doing the so-called butterflies, which
use the trigonometric factors, also called twiddle factors. Normally, those twiddle factors are

226
Digital Signal Processing
pre-computed and stored in a table. The second part of the subroutine deals with the fact that
the output samples are not linearly ordered (see Figure 5.15), in fact the indexing has the bits
reversed, which is why we need to do bit reversal, also called descrambling.
To compute the inverse FFT an additional routine is not necessary; it can be computed
with the subroutine above. To see that, we expand the DFT in Eq. (5.56) into its real and
imaginary parts:
(
)
1
2
/
0
[ ]
[ ]
[ ]
[ ]
N
j
nk N
R
I
R
I
n
X
k
jX
k
x
n
jx n
e
π
−
−
=
+
=
+

(5.80)
take complex conjugate and multiply by j to obtain
(
)
1
2
/
0
[ ]
[ ]
[ ]
[ ]
N
j
nk N
I
R
I
R
n
X
k
jX
k
x n
jx
n
e
π
−
=
+
=
+

(5.81)
which has the same functional form as the expanded inverse DFT of Eq. (5.57)
(
)
1
2
/
0
1
[ ]
[ ]
[ ]
[ ]
N
j
nk N
R
I
R
I
n
x
k
jx k
X
n
jX
n
e
N
π
−
=
+
=
+

(5.82)
so that the inverse FFT can be computed by calling fft (xi, xr, n) other than the
(1/N) factor.
Often the input signal
[ ]
x n
is real, so that we know from the symmetry properties of
Table 5.5 that its Fourier transform is Hermitian. This symmetry can be used to compute the
length-N FFT more efficiently with a length (N/2) FFT. One way of doing so is to define
[ ]
[2 ]
f n
x
n
=
to represent the even-indexed samples of
[ ]
x n , and
[ ]
[2
1]
g n
x
n
=
+
the odd-
indexed samples. We can then define a length (N/2) complex signal
[ ]
h n as
[ ]
[ ]
[ ]
[2 ]
[2
1]
h n
f n
jg n
x
n
jx
n
=
+
=
+
+
(5.83)
whose DFT is
[ ]
[ ]
[ ]
[ ]
[ ]
R
I
H k
F k
jG k
H
k
jH k
=
+
=
+
(5.84)
Since
[ ]
f n and
[ ]
g n are real, their transforms are Hermitian and thus
[
]
[
]
[
]
[ ]
[ ]
H
k
F
k
jG
k
F k
jG k
∗
∗
∗
−
=
−
−
−
=
−
(5.85)
Using Eqs. (5.84) and (5.85), we can obtain
[ ]
F k
and
[ ]
G k
as a function of
[ ]
R
H
k
and
[ ]
I
H k :
[ ]
[
]
[ ]
[
]
[ ]
[
]
[ ]
2
2
2
R
R
I
I
H
k
H
k
H k
H
k
H k
H
k
F k
j
∗
+
−
−
−
+
−




=
=
+








(5.86)

Discrete-Frequency Transforms
227
[ ]
[
]
[ ]
[
]
[ ]
[
]
[ ]
2
2
2
I
I
R
R
H k
H
k
H
k
H
k
H k
H
k
G k
j
j
∗
+
−
−
−
−
−




=
=
−








(5.87)
As shown in Eq. (5.77),
[ ]
X k
can be obtained as a function of
[ ]
F k
and
[ ]
G k
[ ]
[ ]
[ ]
k
N
X k
F k
G k W −
=
+
(5.88)
so that the DFT of the real sequence
[ ]
x n is obtained through Eqs. (5.83), (5.86), (5.87) and
(5.88). The computational complexity is a length (N/2) complex FFT plus N real multiplies
and 3N real adds.
5.3.4.
Circular Convolution
The convolution of two periodic signals is not defined according to Eq. (5.12). Given two
periodic signals
1[ ]
x n and
2[ ]
x n with period N, we define their circular convolution as
1
1
2
1
2
1
2
0
[ ]
[ ]
[ ]
[ ]
[
]
[ ]
[
]
N
m
m
N
y n
x n
x n
x m x n
m
x m x n
m
−
=
=<
>
=
⊗
=
−
=
−


(5.89)
where m
N
=<
> in Eq. (5.89) means that the sum lasts only one period. In fact, the sum
could be over any N consecutive samples, not just the first N. Moreover,
[ ]
y n
is also peri-
odic with period N. Furthermore, it is left to the reader to show that
1
2
[ ]
[ ]
[ ]
Y k
X k X
k
=
(5.90)
i.e., the DFT of
[ ]
y n
is the product of the DFTs of
1[ ]
x n and
2[ ]
x n .
An important application of the above result is the computation of a regular convolu-
tion using a circular convolution. Let
1[ ]
x n
and
2[ ]
x n
be two signals such that
1[ ]
0
x n =
outside
1
0
n
N
≤
<
, and
2[ ]
0
x n =
outside
2
0
n
N
≤
<
. We know that their regular convolu-
tion
1
2
[ ]
[ ]
[ ]
y n
x n
x n
=
∗
is zero outside
1
2
0
1
N
N
≤
+
−. If we choose an integer N such that
1
2
1
N
N
N
≥
+
−, we can define two periodic signals
1[ ]
x n

and
2[ ]
x n

with period N such
that
1
1
1
1
[ ]
0
[ ]
0
x n
n
N
x n
N
n
N
≤
<

= 
≤
<


(5.91)
2
2
2
2
[ ]
0
[ ]
0
x n
n
N
x n
N
n
N
≤
<

= 
≤
<


(5.92)
where
1[ ]
x n and
2[ ]
x n have been zero padded. It can be shown that the circular convolution
1
2
[ ]
[ ]
[ ]
y n
x n
x n
=
⊗



is identical to
[ ]
y n
for 0
n
N
≤
<
, which means that
[ ]
y n
can be ob-

228
Digital Signal Processing
tained as the inverse DFT of
1
2
[ ]
[ ]
[ ]
Y k
X k X
k
=



. This method of computing the regular
convolution of two signals is more efficient than the direct calculation when N is large.
While the crossover point will depend on the particular implementations of the FFT and
convolution, as well as the processor, in practice this has been found beneficial for
1024
N ≥
.
5.3.5.
The Discrete Cosine Transform (DCT)
The Discrete Cosine Transform (DCT) is a widely used for speech processing. It has several
definitions. The DCT-II
[ ]
C k
of a real signal
[ ]
x n is defined by:
(
)
1
0
[ ]
[ ]cos
(
1/ 2)/
N
n
C k
x n
k n
N
π
−
=
=
+

for 0
k
N
≤
<
(5.93)
with its inverse given by
(
)
1
1
1
[ ]
[0]
2
[ ]cos
(
1/ 2)/
N
k
x n
C
C k
k n
N
N
π
−
=


=
+
+





for 0
n
N
≤
<
(5.94)
The DCT-II can be derived from the DFT by assuming
[ ]
x n
is a real periodic se-
quence with period 2N and with an even symmetry
[ ]
[2
1
]
x n
x
N
n
=
−−
. It is left to the
reader to show, that
[ ]
X k
and
[ ]
C k
are related by
/ 2
[ ]
2
[ ]
j k
N
X k
e
C k
π
=
for 0
k
N
≤
<
(5.95)
/ 2
[2
]
2
[ ]
j k
N
X
N
k
e
C k
π
−
−
=
for 0
k
N
≤
<
(5.96)
It is left to the reader to prove Eq. (5.94) is indeed the inverse transform using Eqs.
(5.57), (5.95), and (5.96). Other versions of the DCT-II have been defined that differ on the
normalization constants but are otherwise the same.
There are eight different ways to extend an N-point sequence and make it both peri-
odic and even, such that can be uniquely recovered. The DCT-II is just one of the ways, with
three others being shown in Figure 5.17.
The DCT-II is the most often used discrete cosine transform because of its energy
compaction, which results in its coefficients being more concentrated at low indices than the
DFT. This property allows us to approximate the signal with fewer coefficients [10].
From Eq. (5.95) and (5.96) we see that the DCT-II of a real sequence can be computed
with a length-2N FFT of a real and even sequence, which in turn can be computed with a
length (N/2) complex FFT and some additional computations. Other fast algorithms have
been derived to compute the DCT directly [15], using the principles described in Section
5.3.3.1. Two-dimensional transforms can also be used for image processing.

Digital Filters and Windows
229
Figure 5.17 Four ways to extend a four-point sequence x[n] to make it both periodic and have
even symmetry. The figures in (a), (b), (c) and (d) correspond to the DCT-I, DCT-II, DCT-III
and DCT-IV respectively.
5.4.
DIGITAL FILTERS AND WINDOWS
We describe here the fundamentals of digital filter design and study finite-impulse response
(FIR) and infinite-impulse response (IIR) filters, which are special types of linear time-
invariant digital filters. We establish the time-frequency duality and study the ideal low-pass
filter (frequency limited) and its dual window functions (time limited). These transforms are
applied to stochastic processes.
5.4.1.
The Ideal Low-Pass Filter
It is useful to find an impulse response
[ ]
h n whose Fourier transform is
0
0
1
|
|
(
)
0
|
|
j
H e ω
ω
ω
ω
ω
π
<

= 
<
<

(5.97)
which is the ideal low-pass filter because it lets all frequencies below
0
ω pass through unaf-
fected and completely blocks frequencies above
0
ω . Using the definition of Fourier trans-
form, we obtain
(
)
(
)
0
0
0
0
0
0
0
sin
1
[ ]
sinc
2
2
j
n
j
n
j n
e
e
n
h n
e
d
n
jn
n
ω
ω
ω
ω
ω
ω
ω
ω
ω
π
π
π
π
−
−
−


=
=
=
= 




(5.98)
where we have defined the so-called sinc function as
sin
sinc( )
x
x
x
π
π
=
(5.99)
(a)
(b)
(c)
(d)

230
Digital Signal Processing
which is a real and even function of x and is plotted in Figure 5.18. Note that the sinc func-
tion is 0 when x is a nonzero integer.
-8
-6
-4
-2
0
2
4
6
8
-0.5
0
0.5
1
Figure 5.18 A sinc function, which is the impulse response of the ideal low-pass filter with a
scale factor.
Thus, an ideal low-pass filter is noncausal since it has an impulse response with an in-
finite number of nonzero coefficients.
5.4.2.
Window Functions
Window functions are signals that are concentrated in time, often of limited duration. While
window functions such as triangular, Kaiser, Barlett, and prolate spheroidal occasionally
appear in digital speech processing systems, the rectangular, Hanning, and Hamming are the
most widely used. Window functions are also concentrated in low frequencies. These win-
dow functions are useful in digital filter design and all throughout Chapter 6.
5.4.2.1.
The Rectangular Window
The rectangular window is defined as
[ ]
[ ]
[
]
h n
u n
u n
N
π
=
−
−
(5.100)
and we refer to it often in this book. Its z-transform is given by
1
0
( )
N
n
n
H
z
z
π
−
−
=
= 
(5.101)
which results in a polynomial of order (N – 1). Multiplying both sides of Eq. (5.101) by
1
z−,
we obtain
1
1
( )
( ) 1
N
n
N
n
z H
z
z
H
z
z
π
π
−
−
−
=
=
=
−+

(5.102)
and therefore the sum of the terms of a geometric series can also be expressed as

Digital Filters and Windows
231
1
1
( )
1
N
z
H
z
z
π
−
−
−
=
−
(5.103)
Although
1
z =
appears to be a pole in Eq. (5.103), it actually isn’t because it is can-
celed by a zero at
1
z = . Since
[ ]
h n
π
has finite length, Eq. (5.25) must be satisfied for
0
z ≠
, so the region of convergence is everywhere but at
0
z =
. Moreover, all finite-length
sequences have a region of convergence that is the complete z-plane except for possibly
0
z =
.
The Fourier transform of the rectangular window is, using Eq. (5.103):
(
)
(
)
/ 2
/ 2
/ 2
/ 2
/ 2
/ 2
(
1)/ 2
(
1)/ 2
1
(
)
1
sin
/ 2
( )
sin
/ 2
j
N
j
N
j
N
j N
j
j
j
j
j
j
N
j
N
e
e
e
e
H
e
e
e
e
e
N
e
A
e
ω
ω
ω
ω
ω
π
ω
ω
ω
ω
ω
ω
ω
ω
ω
−
−
−
−
−
−
−
−
−
−
−
−
=
=
−
−
=
=
(5.104)
where
( )
A ω
is real and even. The function
( )
A ω , plotted in Figure 5.19 in dB,8 is 0 for
2
/
k
k N
ω
π
=
with
{
}
0,
, 2
,
k
N
N
≠
±
±
 , and is the discrete-time equivalent of the sinc
function.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-100
-80
-60
-40
-20
0
(dB)
Figure 5.19 Frequency response (magnitude in dB) of the rectangular window with N = 50,
which is a digital sinc function.
5.4.2.2.
The Generalized Hamming Window
The generalized Hamming window is defined as
8 An energy value E is expressed is decibels (dB) as E
E
 10
10
log
. If the energy value is 2E, it is therefore 3dB
higher. Logarithmic measurements like dB are useful because they correlate well with how the human auditory
system perceives volume.
Normalized Frequency

232
Digital Signal Processing
(
)
(1
)
cos 2
/
0
[ ]
0
h
n N
n
N
h n
otherwise
α
α
π
−
−
≤
<

= 

(5.105)
and can be expressed in terms of the rectangular window in Eq. (5.100) as
[ ]
(1
)
[ ]
[ ]cos(2
/
)
hh n
h n
h n
n N
π
π
α
α
π
=
−
−
(5.106)
whose transform is
(
2
/
)
(
2
/
)
(
)
(1
)
(
)
(
/ 2)
(
)
(
/ 2)
(
)
j
j
j
N
j
N
h
H
e
H
e
H
e
H
e
ω
ω
ω
π
ω
π
π
π
π
α
α
α
−
+
=
−
−
−
(5.107)
after using the modulation property in Table 5.5. When
0.5
α =
the window is known as the
Hanning window, whereas for
0.46
α =
it is the Hamming window. Hanning and Hamming
windows and their magnitude frequency responses are plotted in Figure 5.20.
The main lobe of both Hamming and Hanning is twice as wide as that of the rectangu-
lar window, but the attenuation is much greater than that of the rectangular window. The
secondary lobe of the Hanning window is 31 dB below the main lobe, whereas for the
Hamming window it is 44 dB below. On the other hand, the attenuation of the Hanning win-
dow decays with frequency quite rapidly, which is not the case for the Hamming window,
whose attenuation stays approximately constant for all frequencies.
0
10
20
30
40
0
0.5
1
(a)
0
0.1
0.2
0.3
0.4
0.5
-100
-50
0
(dB)
(b)
0
10
20
30
40
0
0.5
1
(c)
time
0
0.1
0.2
0.3
0.4
0.5
-100
-50
0
Normalized Frequency
(dB)
(d)
Figure 5.20 (a) Hanning window and (b) the magnitude of its frequency response in dB; (c)
Hamming window and (d) the magnitude of its frequency response in dB for N = 50.
5.4.3.
FIR Filters
From a practical point of view, it is useful to consider LTI filters whose impulse responses
have a limited number of nonzero coefficients:

Digital Filters and Windows
233
0
[ ]
0
nb
n
M
h n
otherwise
≤
≤

= 

(5.108)
These types of LTI filters are called finite-impulse response (FIR) filters. The in-
put/output relationship in this case is
0
[ ]
[
]
M
r
r
y n
b x n
r
=
=
−

(5.109)
The z-transform of
[
]
x n
r
−
is
(
)
[
]
[ ]
( )
n
n r
r
n
n
x n
r z
x n z
z
X z
∞
∞
−
−
+
−
=−∞
=−∞
−
=
=


(5.110)
Therefore, given that the z-transform is linear,
( )
H z
is
(
)
1
0
1
( )
( )
1
( )
M
L
M
r
L
r
r
r
r
Y z
H z
b z
Az
c z
X z
−
−
−
−
=
=
=
=
=
−

∏
(5.111)
whose region of convergence is the whole z-plane except for possibly
0
z =
. Since
0
M
r
r
b
=
is
finite, FIR systems are always stable, which makes them very attractive. Several special
types of FIR filters will be analyzed below: linear-phase, first-order and low-pass FIR filters.
5.4.3.1.
Linear-Phase FIR Filters
Linear-phase filters are important because, other than a delay, the phase of the signal is un-
changed. Only the magnitude is affected. Therefore, the temporal properties of the input
signal are preserved. In this section we show that linear-phase FIR filters can be built if the
filter exhibits symmetry.
Let’s explore the particular case of
[ ]
h n
real,
2
M
L
=
, an even number, and
[ ]
[
]
h n
h M
n
=
−
(called a Type-I filter). In this case
(
)
(
)
(
)
1
(2
)
0
0
1
(
)
(
)
0
1
(
)
[ ]
[ ]
[ ]
[
]
[ ]
[ ]
[ ]
2 [
]cos
( )
M
L
j
j n
j L
j n
j
L n
n
n
L
j L
j
n L
j
n L
j L
n
L
j L
j L
n
H e
h n e
h L e
h n e
h M
n e
h L e
h n e
e
e
h L
h n
L
n
e
A
e
ω
ω
ω
ω
ω
ω
ω
ω
ω
ω
ω
ω
ω
−
−
−
−
−
−
=
=
−
−
−
−
−
−
=
−
−
=
=
=
+
+
−
=
+
+


=
+
+
=








(5.112)
where
( )
A ω
is a real and even function of ω , since the cosine is an even function, and
( )
A ω
is a
linear
combination
of cosines.
Furthermore,
we
see
that
the
phase

234
Digital Signal Processing
{
}
arg
(
)
j
H e
L
ω
ω
=
, which is a linear function of ω , and therefore
[ ]
h n
is called a linear-
phase system. It can be shown that if
[ ]
[
]
h n
h M
n
= −
−
, we also get a linear phase system
but
( )
A ω
this time is a pure imaginary and odd function (Type III filter). It is left to the
reader to show that in the case of M being odd the system is still linear phase (Types II and
IV filters). Moreover,
[ ]
h n doesn’t have to be real and:
*
[ ]
[
]
h n
h M
n
= ±
−
(5.113)
is a sufficient condition for
[ ]
h n to be linear phase.
5.4.3.2.
First-Order FIR Filters
A special case of FIR filters is the first-order filter:
[ ]
[ ]
[
1]
y n
x n
x n
α
=
+
−
(5.114)
for real values of α , which, unless
1
α = , is not linear phase. Its z-transform is
1
( )
1
H z
z
α
−
= +
(5.115)
It is of interest to analyze the magnitude and phase of its frequency response
2
2
2
2
2
|
(
) |
|1
(cos
sin
) |
(1
cos
)
( sin
)
1
2 cos
j
H e
j
ω
α
ω
ω
α
ω
α
ω
α
α
ω
=
+
−
=
+
+
= +
+
(5.116)
sin
(
)
arctan 1
cos
je ω
α
ω
θ
α
ω


= −


+


(5.117)
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-20
-15
-10
-5
0
5
10
normalized frequency
(dB)
α = -0.9
α = 0.9
α = -0.5
α = 0.5
Figure 5.21 Frequency response of the first order FIR filter for various values of α .

Digital Filters and Windows
235
It is customary to display the magnitude response in decibels (dB):
2
2
10log |
(
) |
10log (1
)
2 cos
j
H e ω
α
α
ω


=
+
+


(5.118)
as shown in Figure 5.21 for various values of α .
We see that for
0
α >
we have a low-pass filter whereas for
0
α <
it is a high-pass
filter, also called a pre-emphasis filter, since it emphasizes the high frequencies. In general,
filters that boost the high frequencies and attenuate the low frequencies are called high-pass
filters, and filters that emphasize the low frequencies and de-emphasize the high frequencies
are called low-pass filters. The parameter α controls the slope of the curve.
5.4.3.3.
Window Design FIR Lowpass Filters
The ideal lowpass filter lets all frequencies below
0
ω
go through and eliminates all energy
from frequencies above that range. As we described in Section 5.4.1, the ideal lowpass filter
has an infinite impulse response, which poses difficulties for implementation in a practical
system, as it requires an infinite number of multiplies and adds.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0
0.5
1
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-100
-50
0
(dB)
Figure 5.22 Magnitude frequency response of the truncated sinc signal (N=200) for
0
/ 4
ω
π
=
. It is an approximation to the ideal low-pass filter, though we see that overshoots
are present near the transition. The first graph is linear magnitude and the second is in dB.
Since we know that the sinc function decays over time, it is reasonable to assume that
a truncated sinc function that keeps a large enough number of samples N could be a good
approximation to the ideal low-pass filter. Figure 5.22 shows the magnitude of the frequency
response of such a truncated sinc function for different values of N. While the approximation
gets better for larger N, the overshoot near
0
ω
doesn’t go away and it facts stays at about
Normalized Frequency

236
Digital Signal Processing
9% of the discontinuity even for large N. This is known as the Gibbs phenomenon, since
Yale professor Josiah Gibbs first noticed it in 1899.
In computing the truncated sinc function, we have implicitly multiplied the ideal low-
pass filter, the sinc function, by a rectangular window. In the so-called window design filter
design method, the filter coefficients are obtained by multiplying the ideal sinc function by a
tapering window function, such as the Hamming window. The resulting frequency response
is the convolution of the ideal lowpass filter function with the transform of the window
(shown in Figure 5.23), and it does not exhibit the overshoots seen above, at the expense of
a slower transition.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0
0.5
1
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-100
-50
0
(dB)
Figure 5.23 Magnitude frequency response of a low-pass filter obtained with the window de-
sign method and a Hamming window (N = 200). The first graph is linear magnitude and the
second is in dB.
5.4.3.4.
Parks McClellan Algorithm
While the window design method is simple, it is hard to predict what the final response will
be. Other methods have been proposed whose coefficients are obtained to satisfy some con-
straints. If our constraints are a maximum ripple of
p
δ
in the passband ( 0
p
ω
ω
≤
<
), and a
minimum attenuation of
s
δ
in the stopband (
s
ω
ω
π
≤
<
), the optimal solution is given by
the Parks McClellan algorithm [14].
The transformation
cos
x
ω
=
(5.119)
maps the interval 0
ω
π
≤
≤
into
1
1
x
−≤
≤. We note that
Normalized Frequency

Digital Filters and Windows
237
cos(
)
(cos
)
n
n
T
ω
ω
=
(5.120)
where
( )
nT x
is the nth-order Chebyshev polynomial. The first two Chebychev polynomials
are given by
0( )
1
T x =
and
1( )
T x
x
=
. If we add the following trigonometric identities
cos(
1)
cos
cos
sin
sin
cos(
1)
cos
cos
sin
sin
n
n
n
n
n
n
ω
ω
ω
ω
ω
ω
ω
ω
ω
ω
+
=
−
−
=
+
(5.121)
and use Eqs. (5.119) and (5.120), we obtain the following recursion formula:
1
1
( )
2
( )
( )
n
n
n
T
x
xT x
T
x
+
−
=
−
for
1
n >
(5.122)
Using Eq. (5.120), the magnitude response of a linear phase Type-I filter in Eq.
(5.112) can be expressed as an Lth-order polynomial in cosω :
0
( )
(cos
)
L
k
k
k
A
a
ω
ω
=
= 
(5.123)
which, using Eq. (5.119) results in a polynomial
0
( )
L
k
k
k
P x
a x
=
= 
(5.124)
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-100
-80
-60
-40
-20
0
(dB)
Figure 5.24 Magnitude frequency response of a length-19 lowpass filter designed with the
Parks McClellan algorithm.
Given that a desired response is
( )
(cos
)
D x
D
ω
=
, we define the weighted squared er-
ror as
( )
(cos
)
(cos
)[
(cos
)
(cos
)]
( )[
( )
( )]
E x
E
W
D
P
W x D x
P x
ω
ω
ω
ω
=
=
−
=
−
(5.125)
where
(cos
)
W
ω
is the weighting in ω . A necessary and sufficient condition for this
weighted squared error to be minimized is to have P(x) alternate between minima and
maxima. For the case of a low-pass filter,
Normalized Frequency

238
Digital Signal Processing
1
cos
cos
1
(cos
)
0
1
cos
cos
p
s
D
ω
ω
ω
ω
ω
≤
≤

= 
−≤
≤

(5.126)
and the weight in the stopband is several times larger that in the passband.
These constraints and the response of a filter designed with such a method are shown
in Figure 5.24. We can thus obtain a similar transfer function with fewer coefficients using
this method.
5.4.4.
IIR Filters
Other useful filters are a function of past values of the input and also the output
1
0
[ ]
[
]
[
]
N
M
k
r
k
r
y n
a y n
k
b x n
r
=
=
=
−
+
−


(5.127)
whose z-transform is given by
0
1
( )
( )
( )
1
M
r
r
r
N
k
k
k
b z
Y z
H z
X z
a z
−
=
−
=
=
=
−


(5.128)
which in turn can be expressed as a function of the roots of the numerator
rc (called zeros),
and denominator
k
d
(called poles) as
(
)
(
)
1
1
1
1
1
( )
1
M
L
L
r
r
N
k
k
Az
c z
H z
d z
−
−
−
=
−
=
−
=
−
∏
∏
(5.129)
It is not obvious what the impulse response of such a system is by looking at either Eq.
(5.128) or Eq. (5.129). To do that, we can compute the inverse z-transform of Eq. (5.129). If
M
N
<
in Eq. (5.129),
( )
H z
can be expanded into partial fractions (see Section 5.2.3.3) as
1
1
( )
1
N
k
k
k
A
H z
d z−
=
=
−

(5.130)
and if M
N
≥
1
1
0
( )
1
N
M
N
k
k
k
k
k
k
A
H z
B z
d z
−
−
−
=
=
=
+
−


(5.131)

Digital Filters and Windows
239
which
we
can
now
compute,
since
we
know
that
the
inverse
z-transform
of
1
( )
/(1
)
k
k
k
H
z
A
d z−
=
−
is
[ ]
|
| 1
[ ]
[
1]
|
| 1
n
k
k
k
k
n
k
k
k
A d u n
d
h n
A d u
n
d

<

= −
−−
>

(5.132)
so that the convergence region includes the unit circle and therefore
[ ]
kh n
is stable. There-
fore, a necessary and sufficient condition for
( )
H z
to be stable and causal simultaneously is
that all its poles be inside the unit circle: i.e., |
| 1
k
d
<
for all k, so that its impulse response is
given by
1
[ ]
[ ]
N
n
n
k
k
k
h n
B
A d u n
=
=
+
(5.133)
which has an infinite impulse response, and hence its name.
Since IIR systems may have poles outside the unit circle, they are not guaranteed to be
stable and causal like their FIR counterparts. This makes IIR filter design more difficult,
since only stable and causal filters can be implemented in practice. Moreover, unlike FIR
filters, IIR filters do not have linear phase. Despite these difficulties, IIR filters are popular
because they are more efficient than FIR filters in realizing steeper roll-offs with fewer coef-
ficients. In addition, as shown in Chapter 6, they represent many physical systems.
5.4.4.1.
First-Order IIR Filters
An important type of IIR filter is the first-order filter of the form
[ ]
[ ]
[
1]
y n
Ax n
y n
α
=
+
−
(5.134)
for α real. Its transfer function is given by
1
( )
1
A
H z
z
α
−
=
−
(5.135)
This system has one pole and no zeros. As we saw in our discussion of z-transforms in
Section 5.2.3, a necessary condition for this system to be both stable and causal is that
|
| 1
α < . Since for the low-pass filter case 0
1
α
<
< , it is convenient to define
b
e
α
−
=
where
0
b >
. In addition, the corresponding impulse response is infinite:
[ ]
[ ]
n
h n
u n
α
=
(5.136)
whose Fourier transform is
(
)
1
1
j
j
b
j
A
A
H e
e
e
ω
ω
ω
α
−
−−
=
=
−
−
(5.137)

240
Digital Signal Processing
and magnitude square is given by
2
2
2
|
|
|
(
) |
1
2 cos
j
A
H e ω
α
α
ω
=
+
−
(5.138)
which is shown in Figure 5.25 for
0
α >
, which corresponds to a low-pass filter.
Figure 5.25 Magnitude frequency response of the first-order IIR filter.
The bandwidth of a low-pass filter is defined as the point where its magnitude square
is half of its maximum value. Using the first-order Taylor approximation of the exponential
function, the following approximation can be used when
0
b →
:
2
2
0
2
2
2
|
(
) |
|1
|
j
b
A
A
H e
e
b
−
=
≈
−
(5.139)
If the bandwidth
b
ω is also small, we can similarly approximate
(
)
2
2
2
2
2
2
2
2
|
(
) |
|
|
|1
|
b
b
j
b
j
b
b
A
A
A
H e
b
j
e
b
ω
ω
ω
ω
−−
=
≈
=
+
−
+
(5.140)
so that for
b
b
ω =
we have
2
0
2
|
(
) |
0.5 |
(
) |
jb
j
H e
H e
≈
. In other words, the bandwidth of this
filter equals b , for small values of b . The relative error in this approximation9 is smaller
than 2% for
0.5
b <
, which corresponds to 0.6
1
α
<
< . The relationship with the unnormal-
ized bandwidth B is
2
/
s
B F
e
π
α
−
=
(5.141)
9 The exact value is
arccos 2
cosh
b
b
ω =
−



 , where
(
)
cosh
/ 2
b
b
b
e
e−
=
+
is the hyperbolic cosine.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-10
-5
0
5
10
15
20
(dB)
normalized frequency
α = 0.9
α = 0.6
α = 0.3

Digital Filters and Windows
241
For
0
α <
it behaves as a high-pass filter, and a similar discussion can be carried out.
5.4.4.2.
Second-Order IIR Filters
An important type of IIR filters is the set of second-order filters of the form
1
2
[ ]
[ ]
[
1]
[
2]
y n
Ax n
a y n
a y n
=
+
−
+
−
(5.142)
whose transfer function is given by
1
2
1
2
( )
1
A
H z
a z
a z
−
−
=
−
−
(5.143)
This system has two poles and no zeros. A special case is when the coefficients A ,
1a
and
2a are real. In this case the two poles are given by
2
1
1
2
4
2
a
a
a
z
±
+
=
(5.144)
which for the case of
2
1
2
4
0
a
a
+
>
yields two real roots, and is a degenerate case of two
first-order systems. The more interesting case is when
2
1
2
4
0
a
a
+
<
. In this case we see that
the two roots are complex conjugates of each other, which can be expressed in their magni-
tude and phase notation as
0
j
z
e
σ
ω
−
±
=
(5.145)
As we mentioned before,
0
σ >
is a necessary and sufficient condition for the poles to be
inside the unit circle and thus for the system to be stable. With those values, the z-transform
is given by
0
0
1
2
2
1
1
0
( )
1
2
cos(
)
(1
)(1
)
j
j
A
A
H z
e
z
e
z
e
z
e
z
σ
ω
σ
ω
σ
σ
ω
−
+
−
−
−
−
−
−
−
−
=
=
−
+
−
−
(5.146)
In Figure 5.26 we show the magnitude of its Fourier transform for a value of σ and
0
ω . We see that the response is centered around
0
ω
and is more concentrated for smaller
values of σ . This is a type of bandpass filter, since it favors frequencies in a band around
0
ω . It is left to the reader as an exercise to show that the bandwidth10 is approximately 2σ .
The smaller the ratio
0
/
σ ω , the sharper the resonance. The filter coefficients can be ex-
pressed as a function of the unnormalized bandwidth B and resonant frequency F and the
sampling frequency
sF (all expressed in Hz) as
(
)
/
1
2
cos 2
/
s
B F
s
a
e
F F
π
π
−
=
(5.147)
10 The bandwidth of a bandpass filter is the region between half maximum magnitude squared values.

242
Digital Signal Processing
2
/
2
s
B F
a
e
π
−
= −
(5.148)
These types of systems are also known as second-order resonators and will be of great
use for speech synthesis (Chapter 16), particularly for formant synthesis.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-20
-10
0
10
20
30
(dB)
Figure 5.26 Frequency response of the second-order IIR filter for center frequency of
0.1
s
F
F
=
and bandwidth
0.01
s
B
F
=
.
5.5.
DIGITAL PROCESSING OF ANALOG SIGNALS
To use the digital signal processing methods, it is necessary to convert the speech signal
( )
x t , which is analog, to a digital signal
[ ]
x n , which is formed by periodically sampling the
analog signal
( )
x t
at intervals equally spaced T seconds apart:
[ ]
(
)
x n
x nT
=
(5.149)
where T is defined as the sampling period, and its inverse
1/
sF
T
=
as the sampling fre-
quency. In the speech applications considered in this book,
sF can range from 8000 Hz for
telephone applications to 44,100 Hz for high-fidelity audio applications. This section ex-
plains the sampling theorem, which essentially says that the analog signal
( )
x t
can be
uniquely recovered given its digital signal
[ ]
x n
if the analog signal
( )
x t
has no energy for
frequencies above the Nyquist frequency
/ 2
sF
.
We not only prove the sampling theorem, but also provide great insight into the ana-
log-digital conversion, which is used in Chapter 7.
5.5.1.
Fourier Transform of Analog Signals
The Fourier transform of an analog signal
( )
x t
is defined as
Normalized Frequency

Digital Processing of Analog Signals
243
( )
( )
j
t
X
x t e
dt
∞
−Ω
−∞
Ω= 
(5.150)
with its inverse transform being
1
( )
( )
2
j
t
x t
X
e
d
π
∞
Ω
−∞
=
Ω
Ω

(5.151)
They are transform pairs. You can prove similar relations for the Fourier transform of
analog signals as for their digital signals counterpart.
5.5.2.
The Sampling Theorem
Let’s define
( )
px
t
( )
( ) ( )
px
t
x t p t
=
(5.152)
as a sampled version of
( )
x t , where
( )
(
)
n
p t
t
nT
δ
∞
=−∞
=
−

(5.153)
where
( )t
δ
is the Dirac delta defined in Section 5.3.2.1. Therefore,
( )
px
t
can also be ex-
pressed as
( )
( ) (
)
(
) (
)
[ ] (
)
p
n
n
n
x
t
x t
t
nT
x nT
t
nT
x n
t
nT
δ
δ
δ
∞
∞
∞
=−∞
=−∞
=−∞
=
−
=
−
=
−



(5.154)
after using Eq. (5.149). In other words,
( )
px
t
can be uniquely specified given the digital
signal
[ ]
x n .
Using the modulation property of Fourier transforms of analog signals, we obtain
1
( )
( )
( )
2
p
X
X
P
π
Ω=
Ω∗
Ω
(5.155)
Following a derivation similar to that in Section 5.3.2.2, one can show that the trans-
form of the impulse train
( )
p t
is given by
2
( )
(
)
s
k
P
k
T
π
δ
∞
=−∞
Ω=
Ω−Ω

(5.156)
where
2
s
sF
π
Ω=
and
1/
sF
T
=
, so that
1
( )
(
)
p
s
k
X
X
k
T
∞
=−∞
Ω=
Ω−Ω

(5.157)

244
Digital Signal Processing
From Figure 5.27 it can be seen that if
( )
0
X Ω=
for |
|
/ 2
s
Ω> Ω
(5.158)
then X(Ω) can be completely recovered from
( )
p
X
Ω
as follows
( )
( )
( )
s
p
X
R
X
Ω
Ω=
Ω
Ω
(5.159)
where
1
|
|
/ 2
( )
0
s
s
R
otherwise
Ω
Ω< Ω

Ω= 

(5.160)
is an ideal lowpass filter. We can also see that if Eq. (5.158) is not met, then aliasing will
take place and X(Ω) can no longer be recovered from
( )
p
X
Ω. Since, in general, we cannot
be certain that Eq. (5.158) is true, the analog signal is low-pass filtered with an ideal filter
given by Eq. (5.160), which is called anti-aliasing filter, prior to sampling. Limiting the
bandwidth of our analog signal is the price we have to pay to be able to manipulate it digi-
tally.
Figure 5.27
( )
X Ω,
( )
p
X
Ω
for the case of no aliasing and aliasing.
The inverse Fourier transform of Eq. (5.160), computed through Eq. (5.151), is a sinc
function
(
)
sin
/
( )
sinc( /
)
/
T
t T
r t
t T
t T
π
π
=
=
(5.161)
so that using the convolution property in Eq. (5.159) we obtain
…
…
Ωs/2
-Ωs/2
Ωs
-Ωs
( )
X Ω
( )
p
X
Ω
…
Ωs/2
-Ωs/2
Ωs
-Ωs
…
( )
p
X
Ω

Digital Processing of Analog Signals
245
( )
( )
( )
( )
[ ] (
)
[ ] (
)
T
p
T
T
k
k
x t
r t
x
t
r t
x k
t
kT
x k r t
kT
δ
∞
∞
=−∞
=−∞
=
∗
=
∗
−
=
−


(5.162)
The sampling theorem states that we can recover the continuous time signal
( )
x t
just
from its samples
[ ]
x n
using Eqs. (5.161) and (5.162). The angular frequency
2
s
sF
π
Ω=
is
expressed in terms of the sampling frequency
S
F .
1/
s
T
F
=
is the sampling period, and
/ 2
sF
the Nyquist frequency. Equation (5.162) is referred to as bandlimited interpolation
because
( )
x t
is reconstructed by interpolating
[ ]
x n with sinc functions that are bandlimited.
Now let’s see the relationship between
( )
p
X
Ω
and
(
)
j
X e ω , the Fourier transform of
the discrete sequence
[ ]
x n . From Eq. (5.154) we have
( )
[ ]
j
nT
p
n
X
x n e
∞
−Ω
=−∞
Ω= 
(5.163)
so that the continuous transform
( )
p
X
Ω
equals the discrete Fourier transform
(
)
j
X e ω
at
T
ω = Ω
.
5.5.3.
Analog-to-Digital Conversion
The process of converting an analog signal
( )
x t
into a digital signal
[ ]
x n
is called Analog-
to-Digital conversion, or A/D for short, and the device that does it called an Analog-to-
Digital Converter. In Section 5.5.2 we saw that an ideal low-pass anti-aliasing filter was
required on the analog signal, which of course is not realizable in practice so that an ap-
proximation has to be used. In practice, sharp analog filters can be implemented on the same
chip using switched capacitor filters, which have attenuations above 60 dB in the stop band
so that aliasing tends not to be an important issue for speech signals. The passband is not
exactly flat, but this again does not have much significance for speech signals (for other sig-
nals, such as those used in modems, this issue needs to be studied more carefully).
Although such sharp analog filters are possible, they can be expensive and difficult to
implement. One common solution involves the use of a simple analog low-pass filter with a
large attenuation at
/ 2
s
MF
, a multiple of the required cutoff frequency. Then over-
sampling is done at the new rate
s
MF , followed by a sharper digital filter with a cut-off fre-
quency of
/ 2
sF
and downsampling (see Section 5.6). This is equivalent to having used a
sharp analog filter, with the advantage of a lower-cost implementation. This method also
allows variable sampling rates with minimal increase in cost and complexity. This topic is
discussed in more detail in Chapter 7 in the context of sigma-delta modulators.
In addition, the pulses in Eq. (5.59) cannot be zero length in practice, and therefore the
sampling theorem does not hold. However, current hardware allows the pulses to be small
enough that the analog signal can be approximately recovered. The signal level is then main-
tained during T seconds, while the conversion to digital is being carried out.

246
Digital Signal Processing
A real A/D converter cannot provide real numbers for
[ ]
x n , but rather a set of integers
typically represented with 16 bits, which gives a range between –32,768 and 32,767. Such
conversion is achieved by comparing the analog signal to a number of different signal levels.
This means that quantization noise has been added to the digital signal. This is typically not
a big problem for speech signals if using 16 bits or more since, as is shown in Chapter 7,
other noises will mask the quantization noise anyway. Typically, quantization noise be-
comes an issue only if 12 or fewer bits are used. A more detailed study of the effects of
quantization is presented in Chapter 7.
Finally, A/D subsystems are not exactly linear, which adds another source of distor-
tion. This nonlinearity can be caused by, among things, jitter and drift in the pulses and un-
evenly spaced comparators. For popular A/D subsystems, such as sigma-delta A/D, an offset
is typically added to
[ ]
x n , which in practice is not very important, because speech signals
do not contain information at
0
f =
, and thus can be safely ignored.
5.5.4.
Digital-to-Analog Conversion
The process of converting the digital signal
[ ]
x n
back into an analog
( )
x t
is called digital-
to-analog conversion, or D/A for short. The ideal band-limited interpolation requires ideal
sinc functions as shown in Eq. (5.162), which are not realizable. To convert the digital signal
to analog, a zero-order hold filter
0
1
0
( )
0
otherwise
t
T
h t
< <

= 

(5.164)
is often used, which produces an analog signal as shown in Figure 5.28. The output of such a
filter is given by
0
0
0
( )
( )
[ ] (
)
[ ]
(
)
n
n
x t
h t
x n
t
nT
x n h t
nT
δ
∞
∞
=−∞
=−∞
=
∗
−
=
−


(5.165)
-3T
-2T
-T
0
T
2T
3T
x0(t)
xa(t)
t
Figure 5.28 Output of a zero-order hold filter.
The Fourier transform of the zero-hold filter in Eq. (5.164) is, using Eq. (5.150),

Multirate Signal Processing
247
/ 2
0
2sin(
/ 2)
( )
j
T
T
H
e−Ω
Ω
Ω=
Ω
(5.166)
and, since we need an ideal lowpass filter to achieve the band-limited interpolation of Eq.
(5.162), the signal
0( )
x t
has to be filtered with a reconstruction filter with transfer function
/ 2
/ 2
/
sin(
/ 2)
( )
0
/
j
T
r
T
e
T
T
H
T
π
π
Ω
Ω

Ω<

Ω
Ω= 

Ω>

(5.167)
In practice, the phase compensation is ignored, as it amounts to a delay of T/2 seconds. Its
magnitude response can be seen in Figure 5.29. In practice, such an analog filter is not real-
izable and an approximation is made. Since the zero-order hold filter is already low-pass, the
reconstruction filter doesn’t need to be that sharp.
−π
T
π
T
H
j
r (
)
Ω
1
Figure 5.29 Magnitude frequency response of the reconstruction filter used in digital-to-
analog converters after a zero-hold filter.
In the above discussion we note that practical A/D and D/A systems introduce distor-
tions, which causes us to wonder whether it is a good idea to go through this process just to
manipulate digital signals. It turns out that for most speech processing algorithms described
in Chapter 6, the advantages of operating with digital signals outweigh the disadvantage of
the distortions described above. Moreover, commercial A/D and D/A systems are such that
the errors and distortions can be arbitrarily small. The fact that music in digital format (as in
compact discs) has won out over analog format (cassettes) shows that this is indeed the case.
Nonetheless, it is important to be aware of the above limitations when designing a system.
5.6.
MULTIRATE SIGNAL PROCESSING
The term Multirate Signal Processing refers to processing of signals sampled at different
rates. A particularly important problem is that of sampling-rate conversion. It is often the
case that we have a digital signal
[ ]
x n sampled at a sampling rate
sF , and we want to obtain
an equivalent signal
[ ]
y n
but at a different sampling rate
sF′ . This often occurs in A/D sys-
tems that oversample in order to use smaller quantizers, such as a delta or sigma delta-

248
Digital Signal Processing
quantizer (see Chapter 7), and a simpler analog filter, and then have to downsample the sig-
nal. Other examples include mixing signals of different sampling rates and downsampling to
reduce computation (many signal processing algorithms have a computational complexity
proportional to the sampling rate or its square).
A simple solution is to convert the digital signal
[ ]
x n into an analog signal
( )
x t
with
a D/A system running at
sF and then convert it back to digital with an A/D system running
at
sF′ . An interesting problem is whether this could be done in the digital domain directly,
and the techniques to do so belong to the general class of multi-rate processing.
5.6.1.
Decimation
If we want to reduce the sampling rate by a factor of M, i.e., T
MT
′ =
, we take every M
samples. In order to avoid aliasing, we need to lowpass filter the signal to bandlimit it to
frequencies 1/T′ . This is shown in Figure 5.30, where the arrow pointing down indicates
the decimation.
Figure 5.30 Block diagram of the decimation process.
Since the output is not desired at all instants n, but only every M samples, the compu-
tation can be reduced by a factor of M over the case where lowpass filtering is done first and
decimation later. To do this we express the analog signal
( )
lx t
at the output of the lowpass
filter as
( )
[ ]
(
)
l
T
k
x t
x k r
t
kT
∞
′
=−∞
=
−

(5.168)
and then look at the value t
nT
′
′
=
. The decimated signal
[ ]
y n is then given by
(
)
[ ]
(
)
[ ]
[ ]sinc
l
T
k
k
Mn
k
y n
x nT
x k r
nT
kT
x k
M
∞
∞
′
=−∞
=−∞
−


′
′
=
=
−
=






(5.169)
which can be expressed as
[
]
[ ]
[ ]
k
y n
x k h Mn
k
∞
=−∞
=
−

(5.170)
where
[ ]
sinc( /
)
h n
n M
=
(5.171)
rT’[n]
y[n]
M
x[n]

Multirate Signal Processing
249
In practice, the ideal lowpass filter h[n] is approximated by an FIR filter with a cutoff
frequency of 1/(2M).
5.6.2.
Interpolation
If we want to increase the sampling rate by a factor of N, so that
/
T
T N
′ =
, we do not have
any aliasing and no further filtering is necessary. In fact we already know one out of every N
output samples
[
]
[ ]
y Nn
x n
=
(5.172)
and we just need to compute the (
1)
N −
samples in-between. Since we know that
[ ]
x n is a
bandlimited signal, we can use the sampling theorem in Eq. (5.162) to reconstruct the analog
signal as
( )
[ ] (
)
l
T
k
x t
x k r t
kT
∞
=−∞
=
−

(5.173)
and thus the interpolated signal y[n] as
(
)
[ ]
(
)
[ ]
[ ]sinc
T
k
k
n
kN
y n
x nT
x k r
nT
kT
x k
N
∞
∞
=−∞
=−∞
−


′
′
=
=
−
=






(5.174)
Now let’s define
[
]
[ ]
0
x Nk
k
Nk
x k
otherwise
′ =

′
′ = 

(5.175)
which, inserted into Eq. (5.174), gives
(
)
[ ]
[ ]sinc (
)/
k
y n
x k
n
k
N
∞
′=−∞
′
′
′
=
−

(5.176)
This can be seen in Figure 5.31, where the block with the arrow pointing up imple-
ments Eq. (5.175).
Figure 5.31 Block diagram of the interpolation process.
Equation (5.174) can be expressed as
[
]
[ ]
[ ]
k
y n
x k h n
kN
∞
=−∞
=
−

(5.177)
rT[n]
y[n]
N
x[n]

250
Digital Signal Processing
where we have defined
[ ]
sinc( /
)
h n
n N
=
(5.178)
Again, in practice, the ideal low-pass filter h[n] is approximated by an FIR filter with a
cutoff frequency of 1/(2N).
5.6.3.
Resampling
To resample the signal so that
/
T
TM
N
′ =
, or
(
)
/
s
s
F
F
N M
′ =
, we can first upsample the
signal by N and then downsample it by M. However, there is a more efficient way. Proceed-
ing similarly to decimation and interpolation, one can show the output is given by
[ ]
[ ] [
]
k
y n
x k h nM
kN
∞
=−∞
=
−

(5.179)
where
[ ]
sinc max(
,
)
n
h n
N M


=




(5.180)
for the ideal case. In practice, h[n] is an FIR filter with a cutoff frequency of
(
)
1/ 2max(
,
)
N M
. We can see that Eq. (5.179) is a superset of Eqs. (5.170) and (5.177).
5.7.
FILTERBANKS
A filterbank is a collection of filters that span the whole frequency spectrum. In this section
we describe the fundamentals of filterbanks, which are used in speech and audio coding,
echo cancellation, and other applications. We first start with a filterbank with two equal
bands, then explain multi-resolution filterbanks, and present the FFT as a filterbank. Finally
we introduce the concept of lapped transforms and wavelets.
5.7.1.
Two-Band Conjugate Quadrature Filters
A two-band filterbank is shown in Figure 5.32, where the filters
0[ ]
f n
and
0[ ]
g n
are low-
pass filters, and the filters
1[ ]
f n
and
1[ ]
g n
are high-pass filters, as shown in Figure 5.33.
Since the output of
0[ ]
f n
has a bandwidth half of that of x[n], we can sample it at half the
rate of x[n]. We do that by decimation (throwing out every other sample), as shown in
Figure 5.32. The output of such a filter plus decimation is
0[ ]
x m . Similar results can be
shown for
1[ ]
f n and
1[ ]
x n .

Filterbanks
251
For reconstruction, we upsample
0[ ]
x m , by inserting a 0 between every sample. Then
we low-pass filter it with filter
0[ ]
g n
to complete the interpolation, as we saw in Section
5.6. A similar process can be done with the high pass filters
1[ ]
f n
and
1[ ]
g n . Adding the
two bands produces
[ ]
x n

, which is identical to x[n] if the filters are ideal.
Figure 5.32 Two-band filterbank.
In practice, however, ideal filters such as those in Figure 5.33 are not achievable, so
we would like to know if it is possible to build a filterbank that has perfect reconstruction
with FIR filters. The answer is affirmative, and in this section we describe conjugate quadra-
ture filters, which are the basis for the solutions.
Figure 5.33 Ideal frequency responses of analysis and synthesis filters for the two-band filter-
bank.
To investigate this, let’s analyze the cascade of a downsampler and an upsampler
(Figure 5.34). The output y[n] is a signal whose odd samples are zero and whose even sam-
ples are the same as those of the input signal x[n].
Figure 5.34 Cascade of a downsampler and an upsampler.
0
π
π / 2
f0(n)
g0(n)
Lowpass filter
f1(n)
g1(n)
Highpass filter
0
1
Gain
Frequency
x[n]
y[n]
2
2
x(n)
f0(n)
x0[m]
f1(n)
x1[m]
g0(n)
g1(n)
+
~[ ]
x n
2
2
2
2
Analysis
Synthesis

252
Digital Signal Processing
The z-transform of the output is given by
1
1
( )
[ ]
[ ]
( 1)
[ ]
2
2
( )
(
)
2
neven
n
n
n
n
n
n
n
Y z
x n z
x n z
x n z
X z
X
z
∞
∞
∞
−
−
−
=−∞
=−∞
=−∞
=
=
+
−
+
−
=



(5.181)
Using Eq. (5.181) and the system in Figure 5.32, we can express the z-transform of the
output in Figure 5.32 as
0
0
1
1
0
0
1
1
0
0
1
1
0
1
( )
( )
( )
( )
( )
( )
2
(
)
( )
(
)
( )
(
)
2
( )
( )
(
)
(
)
( )
( )
(
)
(
)
( )
( )
2
2
F z G z
F z G z
X z
X z
F
z G
z
F
z G z
X
z
F z X z
F
z X
z
F z X z
F
z X
z
G
z
G z
+


= 



−
+
−


+
−




+
−
−
+
−
−




=
+ 








(5.182)
which for perfect reconstruction requires the output to be a delayed version of the input, and
thus
(
1)
0
0
1
1
0
0
1
1
( )
( )
( )
( )
2
(
)
( )
(
)
( )
0
L
F z G
z
F z G z
z
F
z G z
F
z G z
−
−
+
=
−
+
−
=
(5.183)
These conditions are met if we select the so-called Conjugate Quadrature Filters
(CQF) [17], which are FIR filters that specify
1[ ]
f n ,
0[ ]
g n , and
1[ ]
g n
as a function of
0[ ]
f n :
1
0
0
0
1
1
[ ]
( 1)
[
1
]
[ ]
[
1
]
[ ]
[
1
]
n
f n
f L
n
g n
f L
n
g n
f L
n
= −
−−
=
−−
=
−−
(5.184)
where
0[ ]
f n is an FIR filter of even length L. The z-transforms of Eq. (5.184) are
(
1)
1
1
0
(
1)
1
0
0
1
0
( )
(
)
( )
(
)
( )
(
)
L
L
F z
z
F
z
G z
z
F z
G z
F
z
−
−
−
−
−
−
=
−
=
=
−
(5.185)
so that the second equation in Eq. (5.183) is met if L is even. In order to analyze the first
equation in Eq. (5.183), let’s define
( )
P z as

Filterbanks
253
1
0
0
0
0
( )
( )
(
)
[ ]
[ ] [
]
m
P z
F z F z
p n
f m f m
n
−
=
=
+

(5.186)
then insert Eq. (5.185) into (5.183), use Eq. (5.186), and obtain the following condition:
( )
(
)
2
P z
P
z
+
−
=
(5.187)
Taking the inverse z-transform of Eq. (5.186) and using Eq. (5.181), we obtain
1
0
[ ]
0
2
n
p n
n
k
=

= 
=

(5.188)
so that all even samples of the autocorrelation of
0[ ]
f n
are zero, except for n = 0. Since
0[ ]
f n is a half-band low-pass filter,
[ ]
p n
is also a half-band low-pass filter. The ideal half-
band filter h[n]
sin(
/ 2)
[ ]
n
h n
n
π
π
=
(5.189)
satisfies Eq. (5.188), as does any half-band zero-phase filter (a linear phase filter with no
delay). Therefore, the steps to build CQF are
1. Design a (2L - 1) tap11 half-band linear-phase low-pass filter
[ ]
p n
with any avail-
able technique, for an even value of L. For example, one could use the Parks
McClellan algorithm, constraining the passband and stopband cutoff frequencies
so that
p
s
ω
π
ω
=
−
and using an error weighting that is the same for the passband
and stopband. This results in a half-band linear-phase filter with equal ripple δ in
both bands. Another possibility is to multiply the ideal half-band filter in Eq.
(5.189) by a window with low-pass characteristics.
2. Add a value δ to
[0]
p
so that we can guarantee that
(
)
0
j
P e ω ≥
for all ω and
thus is a legitimate power spectral density.
3. Spectrally factor
1
0
0
( )
( )
(
)
P z
F z F z−
=
by computing its roots.
4. Compute
1[ ]
f n ,
0[ ]
g n and
1[ ]
g n from Eq. (5.184).
5.7.2.
Multiresolution Filterbanks
While the above filterbank has equal bandwidth for both filters, it may be desirable to have
varying bandwidths, since it has been proven to work better in speech recognition systems.
In this section we show how to use the two-band conjugate quadrature filters described in
the previous section to design a filterbank with more than two bands. In fact, multi-
11 A filter with N taps is a filter of length N.

254
Digital Signal Processing
resolution analysis such as that of Figure 5.35, are possible with bands of different band-
widths (see Figure 5.36).
Figure 5.35 Analysis part of a multi-resolution filterbank designed with conjugate quadrature
filters. Only
0[ ]
f n
needs to be specified.
Figure 5.36 Ideal frequency responses of the multi-resolution filterbank of Figure 5.35. Note
that
0[ ]
x n
and
1[ ]
x n
occupy 1/8 of the total bandwidth.
Figure 5.37 Two different time-frequency tilings: the non-uniform filterbank and that obtain
through a short-time Fourier transform. Notice that the area of each tile is constant.
x(n)
f0
f1
2
2
f0
f1
2
2
f0
f1
2
2
x0[n]
x1[n]
x2[n]
x3[n]
0
π
π / 2
0
1
π / 4 
π / 8 
0
1
2
3
t
f
t
f

Filterbanks
255
One interesting result is that the product of time resolution and frequency resolution is
constant (all the tiles in Figure 5.37 have the same area), since filters with smaller band-
widths do not need to be sampled as often. Instead of using Fourier basis for decomposition,
multi-resolution filterbanks allow more flexibility in the tiling of the time-frequency plane.
5.7.3.
The FFT as a Filterbank
It turns out that we can use the Fourier transform to construct a filterbank. To do that, we
decompose the input signal
[ ]
x n as a sum of short-time signals
[ ]
m
x
n
[ ]
[ ]
m
m
x n
x
n
∞
=−∞
= 
(5.190)
where
[ ]
m
x
n is obtained as
[ ]
[ ]
[ ]
m
m
x
n
x n w
n
=
(5.191)
the product of
[ ]
x n by a window function
[ ]
m
w
n of length N. From Eqs. (5.190) and (5.191)
we see that the window function has to satisfy
[ ]
1
m
m
w
n
∞
=−∞
=

n
∀
(5.192)
If the short-term signals
[ ]
m
x
n
are spaced M samples apart, we define the window
[ ]
m
w
n as:
[ ]
[
]
m
w
n
w n
Mm
=
−
(5.193)
where w[n] = 0 for
0
n <
and n
N
>
. The windows
[ ]
m
w
n
overlap in time while satisfying
Eq. (5.192).
Since
[ ]
m
x
n has N nonzero values, we can evaluate its length-N DFT as
1
0
1
1
0
0
[ ]
[
]
[
] [ ]
[
]
[
]
k
k
N
j
l
m
m
l
N
N
j
l
k
l
l
X
k
x
Mm
l e
x Mm
l w l e
x Mm
l f
l
ω
ω
−
−
=
−
−
−
=
=
=
+
=
+
=
+
−



(5.194)
where
2
/
k
k N
ω
π
=
and the analysis filters
[ ]
kf l are given by
[ ]
[
]
k
j
l
kf l
w
l e
ω
=
−
(5.195)
If we define
[ ]
k
X
n

as

256
Digital Signal Processing
1
0
[ ]
[ ]
[ ]
[
]
[ ]
[
]
[
]
N
k
k
k
k
r
l
X
n
x n
f n
x n
r f r
x n
l f
l
∞
−
=−∞
=
=
∗
=
−
=
+
−



(5.196)
then Eqs. (5.194) and (5.196) are related by
[ ]
[
]
m
k
X
k
X
mM
= 
(5.197)
This manipulation is shown in Figure 5.38, so that the DFT output
[ ]
m
X
k
is
[ ]
k
X
n

decimated by M.
Figure 5.38 Fourier analysis used to build a linear filter.
The short-time signal
[ ]
m
x
n can be recovered through the inverse DFT of
[ ]
m
X
k
as
1
0
[
]
[ ]
[ ]
k
N
j
l
m
m
k
x
mM
l
h l
X
k e
ω
−
=
+
=

(5.198)
where h[n] has been defined as
1/
0
[ ]
0
otherwise
N
n
N
h n
≤
<

= 

(5.199)
so that Eq. (5.198) is valid for all values of l, and not just 0
l
N
≤<
.
Making the change of variables mM
l
n
+ =
in Eq. (5.198) and inserting it into Eq.
(5.190) results in
1
(
)
0
1
0
[ ]
[
]
[ ]
[ ]
[
]
k
N
j
n mM
m
m
k
N
m
k
k
m
x n
h n
mM
X
k e
X
k g n
mM
ω
∞
−
−
=−∞
=
−
∞
=
=−∞
=
−
=
−


 
(5.200)
where the synthesis filters
[ ]
k
g n are defined as
[ ]
[ ]
k
j
n
k
g n
h n e
ω
=
(5.201)
Now, let’s define the upsampled version of
[ ]
m
X
k
as
[ ]
ˆ [ ]
0
otherwise
m
k
X
k
l
mM
X
l
=

= 

(5.202)
x[n]
[ ]
m
X
k
[ ]
kf
n
M
[ ]
k
X
n


Filterbanks
257
which, inserted into Eq. (5.200), yields
1
1
0
0
ˆ
ˆ
[ ]
[ ]
[
]
[ ]
[ ]
N
N
k
k
k
k
k
l
k
x n
X
l g n
l
X
n
g n
−
∞
−
=
=−∞
=
=
−
=
∗
 

(5.203)
Thus, the signal can be reconstructed. The block diagram of the analysis/resynthesis
filterbank implemented by the DFT can be seen in Figure 5.39, where
[ ]
[ ]
k
m
x m
X
k
=
and
[ ]
[ ]
x n
x n
=

.
Figure 5.39 A filterbank with N analysis and synthesis filters.
For perfect reconstruction we need N
M
≥
. If w[n] is a rectangular window of length
N, the frame rate has to be M
N
=
. We can also use overlapping windows with
2
N
M
=
(50% overlap), such as Hamming or Hanning windows, and still get perfect reconstruction.
The use of such overlapping windows increases the data rate by a factor of 2, but the analy-
sis filters have much less spectral leakage because of the higher attenuation of the Ham-
ming/Hanning window outside the main lobe.
5.7.4.
Modulated Lapped Transforms
The filterbank of Figure 5.39 is useful because, as we see in Chapter 7, it is better to quan-
tize the spectral coefficients than the waveform directly. If the DFT coefficients are quan-
tized, there will be some discontinuities at frame boundaries. To solve this problem we can
distribute the window w[n] between the analysis and synthesis filters so that
[ ]
[ ]
[ ]
a
s
w n
w n w n
=
(5.204)
x[n]
f0[n]
x0[m]
f1[n]
x1[m]
g0[n]
g1[n]
+
~[ ]
x n
M
Analysis DFT
Synthesis DFT
fN-1[n]
xN-1[m]
gN-1[n]
…
…
M
M
M
M
M

258
Digital Signal Processing
so that the analysis filters are given by
[ ]
[
]
k
j
n
k
a
f n
w
n e
ω
=
−
(5.205)
and the synthesis filters by
[ ]
[ ]
k
j
n
k
s
g n
w n e
ω
−
=
(5.206)
This way, if there is a quantization error, the use of a tapering synthesis window will
substantially decrease the border effect. A common choice is
[ ]
[ ]
a
s
w n
w n
=
, which for the
case of w[n] being a Hanning window divided by N, results in
1
[ ]
[ ]
sin
a
s
n
w n
w n
N
N
π


=
=




for
0
n
N
≤
<
(5.207)
so that the analysis and synthesis filters are the reversed versions of each other:
2
/
sin(
/
)
[
]
[ ]
[ ]
[ ]
j
nk N
N
k
k
N
k
n N
f
n
g n
e
n
h
n
N
π
π
−
=
=
Π
=
(5.208)
whose frequency response can be seen in Figure 5.40.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-50
-40
-30
-20
-10
0
normalized frequency
dB
Figure 5.40 Frequency response of the Lapped Orthogonal Transform filterbank.
The functions
[ ]
N
kh
n in Eq. (5.208) are sine modulated complex exponentials, which
have the property
/ 2
1/ 2
[ ]
2
[2 ]
N
N
k
k
h
n
h
n
−
=
(5.209)

Stochastic Processes
259
which is a property typical of functions called wavelets, i.e., they can be obtained from each
other by stretching by 2 and scaling them appropriately. Such wavelets can be seen in Figure
5.41.
0
5 0
1 0 0
- 0 . 0 5
0
0 . 0 5
k =
3
0
5 0
1 0 0
- 0 . 0 5
0
0 . 0 5
0
5 0
1 0 0
- 0 . 0 5
0
0 . 0 5
k =
1 0
0
5 0
1 0 0
- 0 . 0 5
0
0 . 0 5
Figure 5.41 Iterations of the wavelet
[ ]
N
kh
n for several values of k and N.
If instead of modulating a complex exponential we use a cosine sequence, we obtain
the Modulated Lapped Transform (MLT) [7], also known as the Modified Discrete Cosine
Transform (MDCT):
2
1
1
[2
1
]
[ ]
[ ]
cos
2
2
kn
k
k
M
p
f
M
n
g n
h n
k
n
M
M
π

+




=
−−
=
=
+
+





	

	



(5.210)
for
0,1,
,
1
k
M
=
−

and
0,1,
,2
1
n
M
=
−

. There are M filters with 2M taps each, and
h[n] is a symmetric window
[ ]
[2
1
]
h n
h
M
n
=
−−
that satisfies
2
2
[ ]
[
]
1
h n
h n
M
+
+
=
(5.211)
where the most common choice for h[n] is
1
[ ]
sin
2 2
h n
n
M
π




=
+




	



(5.212)
A fast algorithm can be used to compute these filters based on the DCT, which is called the
Lapped Orthogonal Transform (LOT).
5.8.
STOCHASTIC PROCESSES
While in this chapter we have been dealing with deterministic signals, we also need to deal
with noise, such as the static present in a poorly tuned AM station. To analyze noise signals
we need to introduce the concept of stochastic processes, also known as random processes.
A discrete-time stochastic process
[ ]
n
x
, also denoted by
n
x , is a sequence of random vari-
ables for each time instant n. Continuous-time stochastic processes
( )t
x
, random variables
for each value of t, will not be the focus of this book, though their treatment is similar to that
of discrete-time processes. We use bold for random variables and regular text for determi-
nistic signals.

260
Digital Signal Processing
Here, we cover the statistics of stochastic processes, defining stationary and ergodic
processes and the output of linear systems to such processes.
Example
We can define a random process x[n] as
[ ]
cos[
]
n
n
ω
=
+
x
φ
(5.213)
where φ is real random variable with a uniform pdf in the interval (
, )
π π
−
. Several realiza-
tions of this random process are displayed in Figure 5.42.
Figure 5.42 Several realizations of a sinusoidal random process with a random phase.
5.8.1.
Statistics of Stochastic Processes
In this section we introduce several statistics of stochastic processes such as distribution,
density function, mean and autocorrelation. We also define several types of processes de-
pending on these statistics.
For a specific n,
[ ]
n
x
is a random variable with distribution
( , )
{ [ ]
}
F x n
P
n
x
=
≤
x
(5.214)
Its first derivative with respect to x is the first-order density function, or simply the probabil-
ity density function (pdf)
( , )
( , )
dF x n
f x n
dx
=
(5.215)
The second-order distribution of the process
[ ]
n
x
is the joint distribution
1
2
1
2
1
1
2
2
(
,
;
,
)
{ [
]
, [
]
}
F x x n n
P
n
x
n
x
=
≤
≤
x
x
(5.216)
-4
-3
-2
-1
0
1
2
3
4
-1
-0.5
0
0.5
1

Stochastic Processes
261
of the random variables
1
[
]
n
x
and
2
[
]
n
x
. The corresponding density equals
2
1
1
1
2
1
2
1
2
1
2
(
,
;
,
)
(
,
;
,
)
F x x n n
f x x n n
x x
∂
=
∂∂
(5.217)
A complex random process
[ ]
[ ]
[ ]
r
i
n
n
j
n
=
+
x
x
x
is specified in terms of the joint sta-
tistics of the real processes
[ ]
r n
x
and
[ ]
i n
x
.
The mean
[ ]
n
µ
of
[ ]
n
x
, also called first-order moment, is defined as the expected
value of the random variable
[ ]
n
x
for each value of n:
{
}
[ ]
[ ]
[ ] ( , )
x n
E
n
n f
n d
µ
∞
−∞
=
= 
x
x
x
x
(5.218)
The autocorrelation of complex random process
[ ]
n
x
, also called second-order moment, is
defined as
{
}
1
2
1
2
2
1
[
,
]
[
]
[
]
[
,
]
xx
xx
R
n n
E
n
n
R
n n
∗
∗
=
=
x
x
(5.219)
which is a statistical average, unlike the autocorrelation of a deterministic signal defined in
Eq. (5.45), which was an average over time.
Example
Let’s look at the following sinusoidal random process
[ ]
cos[
]
n
n
ω
=
+
x
r
φ
(5.220)
where r and φ are independent and φ is uniform in the interval (
, )
π π
−
. This process is
zero-mean because
{
}
{ } {
}
[ ]
cos[
]
cos[
]
0
x n
E
n
E
E
n
µ
ω
ω
=
+
=
+
=
r
φ
r
φ
(5.221)
since r and φ are independent and
{
}
1
cos[
]
cos[
]
0
2
E
n
n
d
π
π
ω
ω
π
−
+
=
+
=

φ
φ
φ
(5.222)
Its autocorrelation is given by
{
}
2
1
2
1
2
2
1
2
2
1
2
2
1
1
[
,
]
{
}
cos[
]cos[
] 2
1
1
{
}
cos[ (
)
]
cos[ (
)]
2
2
1
{
}cos[ (
)]
2
xx
R
n n
E
n
n
d
E
n
n
n
n
d
E
n
n
π
π
π
π
ω
ω
π
ω
ω
π
ω
−
−
=
+
+
=
+
+
+
−
=
−


r
φ
φ
φ
r
φ
φ
r
(5.223)

262
Digital Signal Processing
which only depends on the time difference
2
1
n
n
−
.
An important property of a stochastic process is that its autocorrelation
1
2
[
,
]
xx
R
n n
is a
positive-definite function, i.e., for any
,
i
j
a a
[ ,
]
0
i
j
xx
i
j
i
j
a a R
n n
∗
≥

(5.224)
which is a consequence of the identity
{
}
2
0
[
]
[
]
[
]
i
i
i
j
i
j
i
i
j
E
a
n
a a E
n
n
∗
∗




≤
=








x
x
x
(5.225)
Similarly, the autocovariance of a complex random process is defined as
(
)(
)
{
}
1
2
1
1
2
2
1
2
1
2
[
,
]
[
]
[
]
[
]
[
]
[
,
]
[
]
[
]
xx
x
x
xx
x
x
C
n n
E
n
n
n
n
R
n n
n
n
µ
µ
µ
µ
∗
∗
=
−
−
=
−
x
x
(5.226)
The correlation coefficient of process
[ ]
n
x
is defined as
1
2
1
2
1
1
2
2
[
,
]
[
,
]
[
,
]
[
,
]
xx
xx
xx
xx
C
n n
r
n n
C
n n C
n n
=
(5.227)
An important property of the correlation coefficient is that it is bounded by 1:
1
2
[
,
]
1
xx
r
n n
≤
(5.228)
which is the Cauchy-Schwarz inequality. To prove it, we note that for any real number a
{
}
2
1
1
2
2
2
1
1
1
2
2
2
0
( [
]
[
])
( [
]
[
])
[ ,
]
2
[
,
]
[
,
]
xx
xx
xx
E a
n
n
n
n
a C
n n
aC
n n
C
n n
µ
µ
≤
−
+
−
=
+
+
x
x
(5.229)
Since the quadratic function in Eq. (5.229) is positive for all a , its roots have to be complex,
and thus its discriminant has to be negative:
2
1
2
1
1
2
2
[
,
]
[
,
]
[
,
]
0
xx
xx
xx
C
n n
C
n n C
n n
−
≤
(5.230)
from which Eq. (5.228) is derived.
The cross-correlation of two stochastic processes
[ ]
n
x
and
[ ]
n
y
is defined as
{
}
1
2
1
2
2
1
[
,
]
[
]
[
]
[
,
]
xy
yx
R
n n
E
n
n
R
n n
∗
∗
=
=
x
y
(5.231)
where we have explicitly indicated with subindices the random process. Similarly, their
cross-covariance is

Stochastic Processes
263
1
2
1
2
1
2
[
,
]
[
,
]
[
]
[
]
xy
xy
x
y
C
n n
R
n n
n
n
µ
µ ∗
=
−
(5.232)
Two processes
[ ]
n
x
and
[ ]
n
y
are called orthogonal iff
1
2
[
,
]
0
xy
R
n n
=
for every
1n and
2n
(5.233)
They are called uncorrelated iff
1
2
[
,
]
0
xy
C
n n
=
for every
1n and
2n
(5.234)
Independent processes. If two processes
[ ]
n
x
and
[ ]
n
y
are such that the random vari-
ables
1
2
[
], [
],
, [
]
m
n
n
n
x
x
x

, and
1
2
[
], [
],
, [
]
m
n
n
n
′
′
′
y
y
y

are mutually independent, then these
processes are called independent. If two processes are independent, then they are also uncor-
related, though the converse is not generally true.
Gaussian processes. A process
[ ]
n
x
is called Gaussian if the random variables
1
2
[
], [
],
, [
]
m
n
n
n
x
x
x

are jointly Gaussian for any m and
1
2
,
,
,
m
n n
n

. If two processes are
Gaussian and also uncorrelated, then they are also statistically independent.
5.8.2.
Stationary Processes
Stationary processes are those whose statistical properties do not change over time. While
truly stationary processes do not exist in speech signals, they are a reasonable approximation
and have the advantage of allowing us to use the Fourier transforms defined in Section
5.1.3.3. In this section we define stationarity and analyze some of its properties.
A stochastic process is called strict-sense stationary (SSS) if its statistical properties
are invariant to a shift of the origin: i.e., both processes
[ ]
n
x
and
[
]
n
l
+
x
have the same
statistics for any l . Likewise, two processes
[ ]
n
x
and
[ ]
n
y
are called jointly strict-sense
stationary if their joint statistics are the same as those of
[
]
n
l
+
x
and
[
]
n
l
+
y
for any l .
From the definition, it follows that the mth-order density of an SSS process must be
such that
1
1
1
1
(
,
,
;
,
,
)
(
,
,
;
,
,
)
m
m
m
m
f x
x
n
n
f x
x
n
l
n
l
=
+
+




(5.235)
for any l . Thus the first-order density satisfies
( , )
( ,
)
f x n
f x n
l
=
+
for any l , which means
that it is independent of n:
( , )
( )
f x n
f x
=
(5.236)
or, in other words, the density function is constant with time.
Similarly,
1
2
1
2
(
,
;
,
)
f x x n
l n
l
+
+
is independent of l , which leads to the conclusion
1
2
1
2
1
2
(
,
;
,
)
( ,
;
)
f x x n n
f x x m
=
1
2
m
n
n
=
−
(5.237)

264
Digital Signal Processing
or, in other words, the joint density of
[ ]
n
x
and
[
]
n
m
+
x
is not a function of n, only of m,
the time difference between the two samples.
Let’s compute the first two moments of a SSS process:
{ [ ]}
[ ] ( [ ])
( )
E x n
x n f x n
xf x
µ
=
=
=


(5.238)
{ [
]
[ ]}
[
]
[ ] ( [
], [ ])
[ ]
xx
E x n
m x n
x n
m x n f x n
m x n
R
m
∗
∗
+
=
+
+
=

(5.239)
or, in other words, its mean is not a function of time and its autocorrelation depends only on
m.
A stochastic process
[ ]
n
x
that obeys Eq. (5.238) and (5.239) is called wide-sense sta-
tionary (WSS). From this definition, a SSS process is also a WSS process but the converse
is not true in general. Gaussian processes are an important exception, and it can be proved
that a WSS Gaussian process is also SSS.
For example, the random process of Eq. (5.213) is WSS, because it has zero mean and
its autocorrelation function, as given by Eq. (5.223), is only a function of
1
2
m
n
n
=
−
. By
setting
0
m =
in Eq. (5.239) we see that the average power of a WSS stationary process
2
{ [ ] }
[0]
E x n
R
=
(5.240)
is independent of n.
The autocorrelation of a WSS process is a conjugate-symmetric function, also referred
to as a Hermitian function:
[
]
{ [
]
[ ]}
{ [ ]
[
]}
[ ]
R
m
E x n
m x n
E x n x n
m
R m
∗
∗
∗
−
=
−
=
+
=
(5.241)
so that if x[n] is real, R[m] is even.
From Eqs. (5.219), (5.238), and (5.239) we can compute the autocovariance as
2
[ ]
[ ]
C m
R m
µ
=
−
(5.242)
and its correlation coefficient as
[ ]
[ ]/
[0]
r m
C m
C
=
(5.243)
Two processes
[ ]
n
x
and
[ ]
n
y
are called jointly WSS if both are WSS and their cross-
correlation depends only on
1
2
m
n
n
=
−
:
[ ]
{ [
]
[ ]}
xy
R
m
E x n
m y n
∗
=
+
(5.244)
[ ]
[ ]
xy
xy
x
y
C
m
R
m
µ µ ∗
=
−
(5.245)

Stochastic Processes
265
5.8.2.1.
Ergodic Processes
A critical problem in the theory of stochastic processes is the estimation of their various
statistics, such as the mean and autocorrelation given that often only one realization of the
random process is available. The first approximation would be to replace the expectation in
Eq. (5.218) with its ensemble average:
1
0
1
[ ]
[ ]
M
i
i
n
x n
M
µ
−
=
≅

(5.246)
where
[ ]
ix n are different samples of the random process.
As an example, let
[ ]
n
x
be the frequency-modulated (FM) random process received
by a FM radio receiver:
[ ]
[ ]
[ ]
n
a n
n
=
+
x
v
(5.247)
which contains some additive noise v[n]. The realization
[ ]
ix n received by receiver i will be
different from the realization
[ ]
jx n
for receiver j. We know that each signal has a certain
level of noise, so one would hope that by averaging them, we could get the mean of the
process for a sufficiently large number of radio receivers.
In many cases, however, only one sample of the process is available. According to Eq.
(5.246) this would mean that that the sample signal equals the mean, which does not seem
very robust. We could also compute the signal’s time average, but this may not tell us much
about the random process in general. However, for a special type of random processes called
ergodic, their ensemble averages equal appropriate time averages.
A process
[ ]
n
x
with constant mean
{
}
[ ]
E
n
µ
=
x
(5.248)
is called mean-ergodic if, with probability 1, the ensemble average equals the time average
when N approaches infinity:
lim
N
N
µ
µ
→∞
=
(5.249)
where
N
µ
is the time average
/ 2 1
/ 2
1
[ ]
N
N
n
N
n
N
µ
−
=−
=
 x
(5.250)
which, combined with Eq. (5.248), indicates that
N
µ
is a random variable with mean µ .
Taking expectations in Eq. (5.250) and using Eq. (5.248), it is clear that
{
}
N
E µ
µ
=
(5.251)
so that proving Eq. (5.249) is equivalent to proving

266
Digital Signal Processing
2
lim
0
N
N
σ
→∞
=
(5.252)
with
2
N
σ
being the variance of
N
µ . It can be shown [12] that a process
[ ]
n
x
is mean ergodic
iff
/ 2 1
/ 2 1
2
/ 2
/ 2
1
lim
[ ,
]
0
N
N
xx
N
n
N
m
N
C
n m
N
−
−
→∞
=−
=−
=


(5.253)
It can also be shown [12] that a sufficient condition for a WSS process to be mean ergodic is
to satisfy
lim
[ ]
0
xx
m
C
m
→∞
=
(5.254)
which means that if the random variables
[ ]
n
x
and
[
]
n
m
+
x
are uncorrelated for large m,
then process
[ ]
n
x
is mean ergodic. This is true for many regular processes.
A similar condition can be proven for a WSS process to be covariance ergodic. In
most cases in this book we assume ergodicity, first because of convenience for mathematical
tractability, and second because it is a good approximation to assume that samples that are
far apart are uncorrelated. Ergodicity allows us to compute means and covariances of ran-
dom processes by their time averages.
5.8.3.
LTI Systems with Stochastic Inputs
If the WSS random process
[ ]
n
x
is the input to an LTI system with impulse response
[ ]
h n ,
the output
[ ]
[ ] [
]
[
] [ ]
m
m
n
h m
n
m
h n
m
m
∞
∞
=−∞
=−∞
=
−
=
−


y
x
x
(5.255)
is another WSS random process. To prove this we need to show that the mean is not a func-
tion of n:
{
}
{
}
[ ]
[ ]
[ ]
[
]
[ ]
y
x
m
m
n
E
n
h m E
n
m
h m
µ
µ
∞
∞
=−∞
=−∞
=
=
−
=


y
x
(5.256)
The cross-correlation between input and output is given by
{
}
{
}
[ ]
[
]
[ ]
[ ]
[
]
[
]
[ ]
[
]
[
]
[
]
[
]
[ ]
xy
l
xx
xx
xx
l
l
R
m
E
n
m
n
h l E
n
m
n
l
h l R
m
l
h
l R
m
l
h
m
R
m
∞
∗
=−∞
∞
∞
∗
∗
∗
=−∞
=−∞
=
+
∗
=
+
∗
−
=
+
=
−
−
=
−
∗



x
y
x
x
(5.257)
and the autocorrelation of the output

Stochastic Processes
267
{
}
{
}
[ ]
[
]
[ ]
[ ]
[
]
[ ]
[ ]
[
]
[ ]
[ ]
[ ]
[
]
[ ]
yy
l
xy
xy
xx
l
R
m
E
n
m
n
h l E
n
m
l
n
h l R
m
l
h m
R
m
h m
h
m
R
m
∞
=−∞
∞
∗
=−∞
=
+
∗
=
+
−
∗
=
−
=
∗
=
∗
−
∗


y
y
x
y
(5.258)
is only a function of m.
5.8.4.
Power Spectral Density
The Fourier transform of a WSS random process
[ ]
n
x
is a stochastic process in the variable
ω
( )
[ ]
j n
n
n e
ω
ω
∞
−
=−∞
= 
X
x
(5.259)
whose autocorrelation is given by
{
}
(
)
(
)
(
)
( )
[ ]
[ ]
{ [
]
[ ]}
j
u l
j m
l
m
j
u n
jum
n
m
E
u
E
l e
m e
e
E
m
n
m e
ω
ω
ω
ω
ω
∞
∞
∗
−
+
∗
=−∞
=−∞
∞
∞
−
+
∗
−
=−∞
=−∞


+
=
=




=
+




X
X
x
x
x
x
(5.260)
where we made a change of variables l
n
m
=
+
and changed the order of expectation and
summation. Now, if x[n] is WSS
{
}
[ ]
[
]
[ ]
xx
R
n
E
m
n
m
•
=
+
x
x
(5.261)
and if we set
0
u =
in Eq. (5.260) together with Eq. (5.261), then we obtain
{
}
2
( )
( )
[ ]
j n
xx
xx
n
S
E
R
n e
ω
ω
ω
∞
−
=−∞
=
= 
X
(5.262)
( )
xx
S
ω
is called the power spectral density of the WSS random process
[ ]
n
x
, and it is the
Fourier transform of its autocorrelation function
[ ]
xx
R
n , with the inversion formula being
1
[ ]
( )
2
j n
xx
xx
R
n
S
e
d
ω
ω
ω
π
∞
−∞
=

(5.263)
Note that Eqs. (5.48) and (5.263) are identical, though in one case we compute the
autocorrelation of a signal as a time average, and the other is the autocorrelation of a random
process as an ensemble average. For an ergodic process both are the same.

268
Digital Signal Processing
Just as we take Fourier transforms of deterministic signals, we can also compute the
power spectral density of a random process as long as it is wide-sense stationary, which is
why these wide-sense stationary processes are so useful.
If the random process
[ ]
n
x
is real then
[ ]
xx
R
n is real and even and, using properties in
Table 5.5,
( )
xx
S
ω
is also real and even.
Parseval’s theorem for random processes also applies here:
{
}
2
1
[ ]
[0]
( )
2
xx
xx
E
n
R
S
d
π
π
ω
ω
π
−
=
=

x
(5.264)
so that we can compute the signal’s energy from the area under
( )
xx
S
ω . Let’s get a physical
interpretation of
( )
xx
S
ω . In order to do that we can similarly derive the cross-power spec-
trum
( )
xy
S
ω
of two WSS random processes
[ ]
n
x
and
[ ]
n
y
as the Fourier transform of their
cross-correlation:
( )
[ ]
j n
xy
xy
n
S
R
n e
ω
ω
∞
−
=−∞
= 
(5.265)
which allows us, taking Fourier transforms in Eq. (5.257), to obtain the cross-power spec-
trum between input and output to a linear system as
( )
( )
( )
xy
xx
S
S
H
ω
ω
ω
∗
=
(5.266)
Now, taking the Fourier transform of Eq. (5.258), the power spectrum of the output is
thus given by
2
( )
( )
( )
( )
( )
yy
xy
xx
S
S
H
S
H
ω
ω
ω
ω
ω
=
=
(5.267)
Finally, suppose we filter
[ ]
n
x
through the ideal bandpass filter
0
0
/
( )
0
b
c
c
c
H
otherwise
π
ω
ω
ω
ω

−
<
<
+

= 
(5.268)
The energy of the output process is
{
}
0
0
2
1
1
0
[ ]
[0]
( )
( )
2
2
c
yy
yy
xx
c
E
n
R
S
d
S
d
c
π
ω
π
ω
ω
ω
ω
ω
π
+
−
−
≤
=
=
=


y
(5.269)
so that taking the limit when
0
c →
results in
0
0
0
0
1
0
lim
( )
(
)
2
c
xx
xx
c
c
S
d
S
c
ω
ω
ω
ω
ω
+
−
→
≤
=

(5.270)
which is the Wiener-Khinchin theorem and says that the power spectrum of a WSS process
[ ]
n
x
, real or complex, is always positive for any ω . Equation (5.269) also explains the

Historical Perspective And Further Reading
269
name power spectral density, because
( )
xx
S
ω
represents the density of power at any given
frequency ω .
5.8.5.
Noise
A process
[ ]
n
x
is white noise if, and only if, its samples are uncorrelated:
1
2
1
1
2
[
,
]
[
] [
]
xx
C
n n
C n
n
n
δ
=
−
(5.271)
and is zero-mean
[ ]
0
x n
µ
=
.
If in addition
[ ]
n
x
is WSS, then
[ ]
[ ]
[ ]
xx
xx
C
n
R
n
q
n
δ
=
=
(5.272)
which has a flat power spectral density
( )
xx
S
q
ω =
for all ω
(5.273)
The thermal noise phenomenon in metallic resistors can be accurately modeled as
white Gaussian noise. White noise doesn’t have to be Gaussian (white Poisson impulse
noise is one of many other possibilities).
Colored noise is defined as a zero-mean WSS process whose samples are correlated
with autocorrelation
[ ]
xx
R
n . Colored noise can be generated by passing white noise through
a filter
[ ]
h n
such that
2
( )
( )
xx
S
H
ω
ω
=
. A type of colored noise that is very frequently en-
countered in speech signals is the so-called pink noise, whose power spectral density decays
with ω . A more in-depth discussion of noise and its effect on speech signals is included in
Chapter 10.
5.9.
HISTORICAL PERSPECTIVE AND FURTHER READING
It is impossible to cover the field of Digital Signal Processing in just one chapter. The book
by Oppenheim and Shafer [10] is one of the most widely used as a comprehensive treatment.
For a more in-depth coverage of digital filter design, you can read the book by Parks and
Burrus [13]. A detailed study of the FFT is provided by Burrus and Parks [2]. The theory of
signal processing for analog signals can be found in Oppenheim and Willsky [11]. The the-
ory of random signals can be found in Papoulis [12]. Multirate processing is well studied in
Crochiere and Rabiner [4]. Razavi [16] covers analog-digital conversion. Software pro-
grams, such as MATLAB [1], contain a large number of packaged subroutines. Malvar [7]
has extensive coverage of filterbanks and lapped transforms.
The field of Digital Signal Processing has a long history. The greatest advances in the
field started in the 17th century. In 1666, English mathematician and physicist Sir Isaac
Newton (1642-1727) invented differential and integral calculus, which was independently

270
Digital Signal Processing
discovered in 1675 by German mathematician Gottfried Wilhelm Leibniz (1646-1716). They
both developed discrete mathematics and numerical methods to solve such equations when
closed-form solutions were not available. In the 18th century, these techniques were further
extended. Swiss brothers Johann (1667-1748) and Jakob Bernoulli (1654-1705) invented the
calculus of variations and polar coordinates. French mathematician Joseph Louis Lagrange
(1736-1813) developed algorithms for numerical integration and interpolation of continuous
functions. The famous Swiss mathematician Leonhard Euler (1707-1783) developed the
theory of complex numbers and number theory so useful in the DSP field, in addition to the
first full analytical treatment of algebra, the theory of equations, trigonometry and analytical
geometry. In 1748, Euler examined the motion of a vibrating string and discovered that si-
nusoids are eigenfunctions for linear systems. Swiss scientist Daniel Bernoulli (1700-1782),
son of Johann Bernoulli, also conjectured in 1753 that all physical motions of a string could
be represented by linear combinations of normal modes. However, both Euler and Bernoulli,
and later Lagrange, discarded the use of trigonometric series because it was impossible to
represent signals with corners. The 19th century brought us the theory of harmonic analysis.
One of those who contributed most to the field of Digital Signal Processing is Jean Baptiste
Joseph Fourier (1768-1830), a French mathematician who in 1822 published The Analytical
Theory of Heat, where he derived a mathematical formulation for the phenomenon of heat
conduction. In this treatise, he also developed the concept of Fourier series and harmonic
analysis and the Fourier transform. One of Fourier’s disciples, the French mathematician
Simeon-Denis Poisson (1781-1840), studied the convergence of Fourier series together with
countryman Augustin Louis Cauchy (1789-1857). Nonetheless, it was German Peter
Dirichlet (1805-1859) who gave the first set of conditions sufficient to guarantee the con-
vergence of a Fourier series. French mathematician Pierre Simon Laplace (1749-1827) in-
vented the Laplace transform, a transform for continuous-time signals over the whole com-
plex plane. French mathematician Marc-Antoine Parseval (1755-1836) derived the theorem
that carries his name. German Leopold Kronecker (1823-1891) did work with discrete delta
functions. French mathematician Charles Hermite (1822-1901) discovered complex conju-
gate matrices. American Josiah Willard Gibbs (1839-1903) studied the phenomenon of Fou-
rier approximations to periodic square waveforms.
Until the early 1950s, all signal processing was analog, including the long-playing
(LP) record first released in 1948. Pulse Code Modulation (PCM) had been invented by Paul
M. Rainey in 1926 and independently by Alan H. Reeves in 1937, but it wasn’t until 1948
when Oliver, Pierce, and Shannon [9] laid the groundwork for PCM (see Chapter 7 for de-
tails). Bell Labs engineers developed a PCM system in 1955, the so-called T-1 carrier sys-
tem, which was put into service in 1962 as the world’s first common-carrier digital commu-
nications system and is still used today. The year 1948 also saw the invention of the transis-
tor at Bell Labs and a small prototype computer at Manchester University and marked the
birth of modern Digital Signal Processing. In 1958, Jack Kilby of Texas Instruments in-
vented the integrated circuit and in 1970, researchers at Lincoln Laboratories developed the
first real-time DSP computer, which performed signal processing tasks about 100 times
faster than general-purpose computers of the time. In 1978, Texas Instruments introduced
Speak & Spell, a toy that included an integrated circuit especially designed for speech
synthesis. Intel Corporation introduced in 1971 the 4-bit Intel 4004, the first general-purpose

Historical Perspective And Further Reading
271
microprocessor chip, and in 1972 they introduced the 8-bit 8008. In 1982 Texas Instruments
introduced the TMS32010, the first commercially viable single-chip Digital Signal Proces-
sor (DSP), a microprocessor specially designed for fast signal processing operations. At a
cost of about $100, the TMS32010 was a 16-bit fixed-point chip with a hardware multiplier
built-in that executed 5 million instructions per second (MIPS). Gordon Moore, Intel’s
founder, came up with the law that carries his name stating that computing power doubles
every 18 months, allowing ever faster processors. By the end of the 20th century, DSP chips
could perform floating-point operations at a rate over 1000MIPS and had a cost below $5, so
that today they are found in many devices from automobiles to cellular phones.
While hardware improvements significantly enabled the development of the field,
digital algorithms were also needed. The 1960s saw the discovery of many of the concepts
described in this chapter. In 1965, James W. Cooley and John W. Tukey [3] discovered the
FFT, although it was later found [6] that German mathematician Carl Friedrich Gauss
(1777-1855) had already invented it over a century earlier. The FFT sped up calculations by
orders of magnitude, which opened up many possible algorithms for the slow computers of
the time. James F. Kaiser, Bernard Gold, and Charles Rader published key papers on digital
filtering. John Stockham and Howard Helms independently discovered fast convolution by
doing convolution with FFTs.
An association that has had a large impact on the development of modern Digital Sig-
nal Processing is the Institute of Electrical and Electronic Engineers (IEEE), which has over
350,000 members in 150 nations and is the world’s largest technical organization. It was
founded in 1884 as the American Institute of Electrical Engineers (AIEE). IEEE’s other par-
ent organization, the Institute of Radio Engineers (IRE), was founded in 1912, and the two
merged in 1963. The IEEE Signal Processing Society is a society within the IEEE devoted
to Signal Processing. Originally founded on 1948 as the Institute of Radio Engineers Profes-
sional Group on Audio, it was later renamed the IEEE Group on Audio (1964), the IEEE
Audio and Electroacoustics group (1965), the IEEE group on Acoustics Speech and Signal
Processing (1974), the Acoustic, Speech and Signal Processing Society (1976), and finally
IEEE Signal Processing Society (1989). In 1976 the society initiated its practice of holding
an annual conference, the International Conference on Acoustic, Speech and Signal Process-
ing (ICASSP), which has been held every year since, and whose proceedings constitute an
invaluable reference. Frederik Nebeker [8] provides a history of the society’s first 50 years
rich in insights from the pioneers.
REFERENCES
[1]
Burrus, C.S., et al., Computer-Based Exercises for Signal Processing Using Mat-
lab, 1994, Upper Saddle River, NJ, Prentice Hall.
[2]
Burrus, C.S. and T.W. Parks, DFT/FFT and Convolution Algorithms: Theory and
Implementation, 1985, New York, John Wiley.
[3]
Cooley, J.W. and J.W. Tukey, "An Algorithm for the Machine Calculation of
Complex Fourier Series," Mathematics of Computation, 1965, 19(Apr.), pp. 297-
301.
[4]
Crochiere, R.E. and L.R. Rabiner, Multirate Digital Signal Processing, 1983, Up-
per Saddle River, NJ, Prentice-Hall.

272
Digital Signal Processing
[5]
Duhamel, P. and H. Hollman, "Split Radix FFT Algorithm," Electronic Letters,
1984, 20(January), pp. 14-16.
[6]
Heideman, M.T., D.H. Johnson, and C.S. Burrus, "Gauss and the History of the
Fast Fourier Transform," IEEE ASSP Magazine, 1984, 1(Oct), pp. pp 14-21.
[7]
Malvar, H., Signal Processing with Lapped Transforms, 1992, Artech House.
[8]
Nebeker, F., Fifty Years of Signal Processing: The IEEE Signal Processing Society
and its Technologies, 1998, IEEE.
[9]
Oliver, B.M., J.R. Pierce, and C. Shannon, "The Philosophy of PCM," Proc. Insti-
tute of Radio Engineers, 1948, 36, pp. pp 1324-1331.
[10]
Oppenheim, A.V. and R.W. Schafer, Discrete-Time Signal Processing, 1999, Pren-
tice-Hall, Upper Saddle River, NJ.
[11]
Oppenheim, A.V. and A.S. Willsky, Signals and Systems, 1997, Upper Saddle
River, NJ, Prentice-Hall.
[12]
Papoulis, A., Probability, Random Variables, and Stochastic Processes, 3rd ed,
1991, New York, McGraw-Hill.
[13]
Parks, T.W. and C.S. Burrus, Digital Filter Design, 1987, New York, John Wiley.
[14]
Parks, T.W. and J.H. McClellan, "A Program for the Design of Linear Phase Finite
Impulse Response Filters," IEEE Trans. on
Audio Electroacoustics, 1972, AU-
20(Aug), pp. 195-199.
[15]
Rao, K.R. and P. Yip, Discrete Cosine Transform: Algorithms, Advantages and
Applications, 1990, San Diego, CA, Academic Press.
[16]
Razavi, B., Principles of Data Conversión System Design, 1995, IEEE Press.
[17]
Smith, M.J.T. and T.P. Barnwell, "A Procedure for Designing Exact Reconstruction
Filter Banks for Tree Structured Subband Coders," Int. Conf. on Acoustics, Speech
and Signal Processing, 1984, San Diego, Calif pp. 27.1.1-27.1.4.

273
C H A P T E R
6
Speech Signal RepresentationsEquation Section 6
This chapter presents several representations
for speech signals useful in speech coding, synthesis and recognition. The central theme is
the decomposition of the speech signal as a source passed through a linear time-varying fil-
ter. This filter can be derived from models of speech production based on the theory of
acoustics where the source represents the air flow at the vocal cords, and the filter represents
the resonances of the vocal tract which change over time. Such a source-filter model is illus-
trated in Figure 6.1. We describe methods to compute both the source or excitation e[n] and
the filter h[n] from the speech signal x[n].
Figure 6.1 Basic source-filter model for speech signals.
To estimate the filter we present methods inspired by speech production models (such
as linear predictive coding and cepstral analysis) as well as speech perception models (such
x[n]
e[n]
h[n]

274
Speech Signal Representations
as mel-frequency cepstrum). Once the filter has been estimated, the source can be obtained
by passing the speech signal through the inverse filter. Separation between source and filter
is one of the most difficult challenges in speech processing.
It turns out that phoneme classification (either by human or by machines) is mostly
dependent on the characteristics of the filter. Traditionally, speech recognizers estimate the
filter characteristics and ignore the source. Many speech synthesis techniques use a source-
filter model because it allows flexibility in altering the pitch and the filter. Many speech
coders also use this model because it allows a low bit rate.
We first introduce the spectrogram as a representation of the speech signal that high-
lights several of its properties and describe the short-time Fourier analysis, which is the ba-
sic tool to build the spectrograms of Chapter 2. We then introduce several techniques used to
separate source and filter: LPC and cepstral analysis, perceptually motivated models, for-
mant tracking, and pitch tracking.
6.1.
SHORT-TIME FOURIER ANALYSIS
In Chapter 2, we demonstrated how useful spectrograms are to analyze phonemes and their
transitions. A spectrogram of a time signal is a special two-dimensional representation that
displays time in its horizontal axis and frequency in its vertical axis. A gray scale is typically
used to indicate the energy at each point (t, f) with white representing low energy and black
high energy. In this section we cover short-time Fourier analysis, the basic tool with which
to compute them.
0
0.1
0.2
0.3
0.4
0.5
0.6
-0.5
0
0.5
Time (seconds)
Frequency (Hz)
0
0.1
0.2
0.3
0.4
0.5
0.6
0
1000
2000
3000
4000
Figure 6.2 (a) Waveform with (b) its corresponding wideband spectrogram. Darker areas
mean higher energy for that time and frequency. Note the vertical lines spaced by pitch peri-
X
Y
Z
W
H
G

Short-Time Fourier Analysis
275
ods.
The idea behind a spectrogram, such as that in Figure 6.2, is to compute a Fourier
transform every 5 milliseconds or so, displaying the energy at each time/frequency point.
Since some regions of speech signals shorter than, say, 100 milliseconds often appear to be
periodic, we use the techniques discussed in Chapter 5. However, the signal is no longer
periodic when longer segments are analyzed, and therefore the exact definition of Fourier
transform cannot be used. Moreover, that definition requires knowledge of the signal for
infinite time. For both reasons, a new set of techniques called short-time analysis, are pro-
posed. These techniques decompose the speech signal into a series of short segments, re-
ferred to as analysis frames, and analyze each one independently.
In Figure 6.2 (a), note the assumption that the signal can be approximated as periodic
within X and Y is reasonable. In regions (Z, W) and (H, G), the signal is not periodic and
looks like random noise. The signal in (Z, W) appears to have different noisy characteristics
than those of segment (H, G). The use of an analysis frame implies that the region is short
enough for the behavior (periodicity or noise-like appearance) of the signal to be approxi-
mately constant. If the region where speech seems periodic is too long, the pitch period is
not constant and not all the periods in the region are similar. In essence, the speech region
has to be short enough so that the signal is stationary in that region: i.e., the signal character-
istics (whether periodicity or noise-like appearance) are uniform in that region. A more for-
mal definition of stationarity is given in Chapter 5.
Similarly to the filterbanks described in Chapter 5, given a speech signal
[ ]
x n , we de-
fine the short-time signal
[ ]
m
x
n of frame m as
[ ]
[ ]
[ ]
m
m
x
n
x n w
n
=
(6.1)
the product of
[ ]
x n
by a window function
[ ]
m
w
n , which is zero everywhere except in a
small region.
While the window function can have different values for different frames m, a popular
choice is to keep it constant for all frames:
[ ]
[
]
m
w
n
w m
n
=
−
(6.2)
where
[ ]
0
w n =
for |
|
/ 2
n
N
>
. In practice, the window length is on the order of 20 to 30
ms.
With the above framework, the short-time Fourier representation for frame m is de-
fined as
(
)
[ ]
[
] [ ]
j
j n
j n
m
m
n
n
X
e
x
n e
w m
n x n e
ω
ω
ω
∞
∞
−
−
=−∞
=−∞
=
=
−


(6.3)
with all the properties of Fourier transforms studied in Chapter 5.

276
Speech Signal Representations
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0.05
-5000
0
5000
(a)
0
1000
2000
3000
4000
20
40
60
80
100
120
dB
(b)
0
1000
2000
3000
4000
20
40
60
80
100
120
dB
(c)
0
1000
2000
3000
4000
20
40
60
80
100
120
dB
(d)
0
1000
2000
3000
4000
20
40
60
80
100
120
dB
(e)
Figure 6.3 Short-time spectrum of male voiced speech (vowel /ah/ with local pitch of 110Hz):
(a) time signal, spectra obtained with (b) 30ms rectangular window and (c) 15 ms rectangular
window, (d) 30 ms Hamming window, (e) 15ms Hamming window. The window lobes are not
visible in (e), since the window is shorter than 2 times the pitch period. Note the spectral leak-
age present in (b).
In Figure 6.3 we show the short-time spectrum of voiced speech. Note that there are a
number of peaks in the spectrum. To interpret this, assume the properties of
[ ]
m
x
n
persist
outside the window, and that, therefore, the signal is periodic with period M in the true
sense. In this case, we know (see Chapter 5) that its spectrum is a sum of impulses
(
)
[ ] (
2
/
)
j
m
m
k
X
e
X
k
k M
ω
δ ω
π
∞
=−∞
=
−

(6.4)
Given that the Fourier transform of
[ ]
w n is
(
)
[ ]
j
j n
n
W e
w n e
ω
ω
∞
−
=−∞
= 
(6.5)
so that the transform of
[
]
w m
n
−
is
(
)
j
j m
W e
e
ω
ω
−
−
. Therefore, using the convolution prop-
erty, the transform of
[ ] [
]
x n w m
n
−
for fixed m is the convolution in the frequency domain

Short-Time Fourier Analysis
277
(
2
/
)
(
2
/
)
(
)
[ ] (
)
j
j
k N
j
k N m
m
m
k
X
e
X
k W e
e
ω
ω
π
ω
π
∞
−
−
=−∞
= 
(6.6)
which is a sum of weighted
(
)
j
W e ω , shifted on every harmonic, the narrow peaks seen in
Figure 6.3 (b) with a rectangular window. The short-time spectrum of a periodic signal ex-
hibits peaks (equally spaced 2 / M
π
apart) representing the harmonics of the signal. We
estimate
[ ]
m
X
k
from the short-time spectrum
(
)
j
m
X
e ω , and we see the importance of the
length and choice of window.
Equation (6.6) indicates that one cannot recover
[ ]
m
X
k
by simply retrieving
(
)
j
m
X
e ω , although the approximation can be reasonable if there is a small value of λ such
that
(
)
0
j
W e ω ≈
for
k
ω
ω
λ
−
>
(6.7)
which is the case outside the main lobe of the window’s frequency response.
Recall from Section 5.4.2.1 that, for a rectangular window of length N,
2 / N
λ
π
=
.
Therefore, Eq. (6.7) is satisfied if N
M
≥
, i.e., the rectangular window contains at least one
pitch period. The width of the main lobe of the window’s frequency response is inversely
proportional to the length of the window. The pitch period in Figure 6.3 is M = 71 at a sam-
pling rate of 8 kHz. A shorter window is used in Figure 6.3 (c), which results in wider analy-
sis lobes, though still visible.
Also recall from Section 5.4.2.2 that for a Hamming window of length N,
4
/ N
λ
π
=
:
twice as wide as that of the rectangular window, which entails
2
N
M
≥
. Thus, for Eq. (6.7)
to be met, a Hamming window must contain at least two pitch periods. The lobes are visible
in Figure 6.3 (d) since N = 240, but they are not visible in Figure 6.3 (e) since N = 120, and
2
N
M
<
.
In practice, one cannot know what the pitch period is ahead of time, which often
means you need to prepare for the lowest pitch period. A low-pitched voice with a
0
50Hz
F =
requires a rectangular window of at least 20 ms and a Hamming window of at
least 40 ms for the condition in Eq. (6.7) to be met. If speech is non-stationary within 40ms,
taking such a long window implies obtaining an average spectrum during that segment in-
stead of several distinct spectra. For this reason, the rectangular window provides better time
resolution than the Hamming window. Figure 6.4 shows analysis of female speech for which
shorter windows are feasible.
But the frequency response of the window is not completely zero outside its main
lobe, so one needs to see the effects of this incorrect assumption. From Section 5.4.2.1 note
that the second lobe of a rectangular window is only approximately 17 dB below the main
lobe. Therefore, for the kth harmonic the value of
2
/
(
)
j
k M
m
X
e
π
contains not
[ ]
m
X
k , but also
a weighted sum of
[ ]
m
X
l . This phenomenon is called spectral leakage because the ampli-
tude of one harmonic leaks over the rest and masks its value. If the signal’s spectrum is
white, spectral leakage does not cause a major problem, since the effect of the second lobe

278
Speech Signal Representations
on a harmonic is only
17 /10
10
10log (1 10
)
0.08dB
−
+
=
. On the other hand, if the signal’s spec-
trum decays more quickly in frequency than the decay of the window, the spectral leakage
results in inaccurate estimates.
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0.05
-5000
0
5000
(a)
0
1000
2000
3000
4000
20
40
60
80
100
120
dB
(b)
0
1000
2000
3000
4000
20
40
60
80
100
120
dB
(c)
0
1000
2000
3000
4000
20
40
60
80
100
120
dB
(d)
0
1000
2000
3000
4000
20
40
60
80
100
120
dB
(e)
Figure 6.4 Short-time spectrum of female voiced speech (vowel /aa/ with local pitch of
200Hz): (a) time signal, spectra obtained with (b) 30 ms rectangular window and (c) 15 ms
rectangular window, (d) 30 ms Hamming window, (e) 15 ms Hamming window. In all cases
the window lobes are visible, since the window is longer than 2 times the pitch period. Note
the spectral leakage present in (b) and (c).
From Section 5.4.2.2, observe that the second lobe of a Hamming window is approxi-
mately 43 dB, which means that the spectral leakage effect is much less pronounced. Other
windows, such as Hanning, or triangular windows, also offer less spectral leakage than the
rectangular window. This important fact is the reason why, despite their better time resolu-
tion, rectangular windows are rarely used for speech analysis. In practice, window lengths
are on the order of 20 to 30 ms. This choice is a compromise between the stationarity as-
sumption and the frequency resolution.
In practice, the Fourier transform in Eq. (6.3) is obtained through an FFT. If the win-
dow has length N, the FFT has to have a length greater than or equal to N. Since FFT algo-
rithms often have lengths that are powers of 2 (
2R
L =
), the windowed signal with length N
is augmented with (
)
L
N
−
zeros either before, after, or both. This process is called zero-
padding. A larger value of L provides a finer description of the discrete Fourier transform;
but it does not increase the analysis frequency resolution: this is the sole mission of the win-
dow length N.

Short-Time Fourier Analysis
279
In Figure 6.3, observe the broad peaks, resonances or formants, which represent the
filter characteristics. For voiced sounds there is typically more energy at low frequencies
than at high frequencies, also called roll-off. It is impossible to determine exactly the filter
characteristics, because we know only samples at the harmonics, and we have no knowledge
of the values in between. In fact, the resonances are less obvious in Figure 6.4 because the
harmonics sample the spectral envelope less densely. For high-pitched female speakers and
children, it is even more difficult to locate the formant resonances from the short-time spec-
trum.
Figure 6.5 shows the short-time analysis of unvoiced speech, for which no regularity is
observed.
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0.05
-500
0
500
(a)
0
1000
2000
3000
4000
20
40
60
80
100
120
dB
(b)
0
1000
2000
3000
4000
20
40
60
80
100
120
dB
(c)
0
1000
2000
3000
4000
20
40
60
80
100
120
dB
(d)
0
1000
2000
3000
4000
20
40
60
80
100
120
dB
(e)
Figure 6.5 Short-time spectrum of unvoiced speech. (a) time signal, (b) 30 ms rectangular
window (c) 15 ms rectangular window, (d) 30 ms Hamming window (e) 15 ms Hamming win-
dow.
6.1.1.
Spectrograms
Since the spectrogram displays just the energy and not the phase of the short-term Fourier
transform, we compute the energy as
(
)
2
2
2
log |
[ ]|
log
[ ]
[ ]
r
i
X k
X
k
X
k
=
+
(6.8)

280
Speech Signal Representations
with this value converted to a gray scale according to Figure 6.6. Pixels whose values have
not been computed are interpolated. The slope controls the contrast of the spectrogram,
while the saturation points for white and black control the dynamic range.
Figure 6.6 Conversion between log-energy values (in the x-axis) and gray scale (in the y-axis).
Larger log-energies correspond to a darker gray color. There is a linear region for which more
log-energy corresponds to darker gray, but there is saturation at both ends. Typically there is
40 to 60 dB between the pure white and the pure black.
There are two main types of spectrograms: narrow-band and wide-band. Wide-band
spectrograms use relatively short windows (< 10 ms) and thus have good time resolution at
the expense of lower frequency resolution, since the corresponding filters have wide band-
widths (> 200 Hz) and the harmonics cannot be seen. Note the vertical stripes in Figure 6.2,
due to the fact that some windows are centered at the high part of a pitch pulse, and others in
between have lower energy. Spectrograms can aid in determining formant frequencies and
fundamental frequency, as well as voiced and unvoiced regions.
0
0.1
0.2
0.3
0.4
0.5
0.6
-0.5
0
0.5
Time (seconds)
Frequency (Hz)
0
0.1
0.2
0.3
0.4
0.5
0.6
0
2000
4000
Figure 6.7 Waveform (a) with its corresponding narrowband spectrogram (b). Darker areas
mean higher energy for that time and frequency. The harmonics can be seen as horizontal lines
spaced by fundamental frequency. The corresponding wideband spectrogram can be seen in
Figure 6.2.
Log-energy (dB)
Gray intensity
black
white

Acoustical Model of Speech Production
281
Narrow-band spectrograms use relatively long windows (> 20 ms), which lead to fil-
ters with narrow bandwidth (< 100 Hz). On the other hand, time resolution is lower than for
wide-band spectrograms (see Figure 6.7). Note that the harmonics can be clearly seen, be-
cause some of the filters capture the energy of the signal’s harmonics, and filters in between
have little energy.
Some implementation details also need to be taken into account. Since speech signals
are real, the Fourier transform is Hermitian, and its power spectrum is also even. Thus, it is
only necessary to display values for 0
/ 2
k
N
≤
≤
for N even. In addition, while the tradi-
tional spectrogram uses a gray scale, a color scale can also be used, or even a 3-D represen-
tation. In addition, to make the spectrograms easier to read, sometimes the signal is first pre-
emphasized (typically with a first-order difference FIR filter) to boost the high frequencies
to counter the roll-off of natural speech.
By inspecting both narrow-band and wide-band spectrograms, we can learn the filter’s
magnitude response and whether the source is voiced or not. Nonetheless it is very difficult
to separate source and filter due to nonstationarity of the speech signal, spectral leakage, and
the fact that only the filter’s magnitude response can be known at the signal’s harmonics.
6.1.2.
Pitch-Synchronous Analysis
In the previous discussion, we assumed that the window length is fixed, and we saw the
tradeoffs between a window that contained several pitch periods (narrow-band spectro-
grams) and a window that contained less than a pitch period (wide-band spectrograms). One
possibility is to use a rectangular window whose length is exactly one pitch period; this is
called pitch-synchronous analysis. To reduce spectral leakage a tapering window, such as
Hamming or Hanning, can be used, with the window covering exactly two pitch periods.
This latter option provides a very good compromise between time and frequency resolution.
In this representation, no stripes can be seen in either time or frequency. The difficulty in
computing pitch synchronous analysis is that, of course, we need to know the local pitch
period, which, as we see in Section 6.7, is not an easy task.
6.2.
ACOUSTICAL MODEL OF SPEECH PRODUCTION
Speech is a sound wave created by vibration that is propagated in the air. Acoustic theory
analyzes the laws of physics that govern the propagation of sound in the vocal tract. Such a
theory should consider three-dimensional wave propagation, the variation of the vocal tract
shape with time, losses due to heat conduction and viscous friction at the vocal tract walls,
softness of the tract walls, radiation of sound at the lips, nasal coupling and excitation of
sound. While a detailed model that considers all of the above is not yet available, some
models provide a good approximation in practice, as well as a good understanding of the
physics involved.

282
Speech Signal Representations
6.2.1.
Glottal Excitation
As discussed in Chapter 2, the vocal cords constrict the path from the lungs to the vocal
tract. This is illustrated in Figure 6.8. As lung pressure is increased, air flows out of the
lungs and through the opening between the vocal cords (glottis). At one point the vocal
cords are together, thereby blocking the airflow, which builds up pressure behind them.
Eventually the pressure reaches a level sufficient to force the vocal cords to open and thus
allow air to flow through the glottis. Then, the pressure in the glottis falls and, if the tension
in the vocal cords is properly adjusted, the reduced pressure allows the cords to come to-
gether, and the cycle is repeated. This condition of sustained oscillation occurs for voiced
sounds. The closed-phase of the oscillation takes place when the glottis is closed and the
volume velocity is zero. The open-phase is characterized by a non-zero volume velocity, in
which the lungs and the vocal tract are coupled.
Figure 6.8 Glottal excitation: volume velocity is zero during the closed-phase, during which
the vocal cords are closed.
Rosenberg’s glottal model [39] defines the shape of the glottal volume velocity with
the open quotient, or duty cycle, as the ratio of pulse duration to pitch period, and the speed
quotient as the ratio of the rising to falling pulse durations.
6.2.2.
Lossless Tube Concatenation
A widely used model for speech production is based on the assumption that the vocal tract
can be represented as a concatenation of lossless tubes, as shown in Figure 6.9. The constant
cross-sectional areas {
}
kA
of the tubes approximate the area function A(x) of the vocal tract.
If a large number of tubes of short length are used, we reasonably expect the frequency re-
sponse of the concatenated tubes to be close to those of a tube with continuously varying
area function.
For frequencies corresponding to wavelengths that are long compared to the dimen-
sions of the vocal tract, it is reasonable to assume plane wave propagation along the axis of
the tubes. If in addition we assume that there are no losses due to viscosity or thermal con-
duction, and that the area A does not change over time, the sound waves in the tube satisfy
the following pair of differential equations:
Closed glottis
Open glottis
t
u
t
G ( )

Acoustical Model of Speech Production
283
2
( , )
( , )
( , )
( , )
p x t
u x t
x
A
t
u x t
A
p x t
x
t
c
ρ
ρ
∂
∂
−
=
∂
∂
∂
∂
−
=
∂
∂
(6.9)
where
( , )
p x t
is the sound pressure in the tube at position x and time t,
( , )
u x t
is the volume
velocity flow in the tube at position x and time t, ρ is the density of air in the tube, c is the
velocity of sound and A is the cross-sectional area of the tube.
Figure 6.9 Approximation of a tube with continuously varying area A(x) as a concatenation of
5 lossless acoustic tubes.
Since Eqs. (6.9) are linear, the pressure and volume velocity in tube kth are related by
( , )
(
/ )
(
/ )
( , )
(
/ )
(
/ )
k
k
k
k
k
k
k
u
x t
u
t
x c
u
t
x c
c
p
x t
u
t
x c
u
t
x c
A
ρ
+
−
+
−
=
−
−
+


=
−
+
+


(6.10)
where
(
/ )
ku
t
x c
+
−
and
(
/ )
ku
t
x c
−
−
are the traveling waves in the positive and negative di-
rections respectively and x is the distance measured from the left-hand end of tube kth:
0
x
l
≤
≤. The reader can prove that this is indeed the solution by substituting Eq. (6.10) into
(6.9).
Figure 6.10 Junction between two lossless tubes.
A1
A2
A3
A4
A5
l
Glottis
Lips
l
l
l
l
x
0
l
A(x)
Glottis
Lips
l
u
t
k
( )
u
t
k
( )
u
t
k 

1( )
u
t
k 

1( )
u
t
k 


1(
)

u
t
k 


1(
)

u
t
k


(
)

u
t
k


(
)

l
Ak
Ak+

284
Speech Signal Representations
When there is a junction between two tubes, as in Figure 6.10, part of the wave is re-
flected at the junction, as measured by
kr , the reflection coefficient
1
1
k
k
k
k
k
A
A
r
A
A
+
+
−
=
+
(6.11)
so that the larger the difference between the areas the more energy is reflected. The proof [9]
is beyond the scope of this book. Since
kA and
1
kA + are positive, it is easy to show that
kr
satisfies the condition
1
1
kr
−≤
≤
(6.12)
A relationship between the z-transforms of the volume velocity at the glottis
[ ]
G
u
n
and the lips
[ ]
L
u n
for a concatenation of N lossless tubes can be derived [9] using a dis-
crete-time version of Eq. (6.10) and taking into account boundary conditions for every junc-
tion:
(
)
(
)
[
]
/ 2
1
1
1
1
0.5
1
1
( )
( )
( )
1
1
1
0
N
N
G
k
k
L
N
G
k
G
k
k
z
r
r
U
z
V z
U
z
r
r
r z
z
−
=
−
−
=
+
+
=
=


−

  
−



  


−
	 
	



∏
∏
(6.13)
where
Gr
is the reflection coefficient at the glottis and
N
L
r
r
=
is the reflection coefficient at
the lips. Equation (6.11) is still valid for the glottis and lips, where
0
/
G
A
c Z
ρ
=
is the
equivalent area at the glottis and
1
/
N
L
A
c Z
ρ
+ =
the equivalent area at the lips.
G
Z
and
L
Z
are the equivalent impedances at the glottis and lips, respectively. Such impedances relate
the volume velocity and pressure, for the lips the expression is
( )
( ) /
L
L
L
U
z
P z
Z
=
(6.14)
In general, the concatenation of N lossless tubes results in an N-pole system as shown
in Eq. (6.13). For a concatenation of N tubes, there are at most N/2 complex conjugate poles,
or resonances or formants. These resonances occur when a given frequency gets trapped in
the vocal tract because it is reflected back at the lips and then again back at the glottis.
Since each tube has length l and there are N of them, the total length is L
lN
=
. The
propagation delay in each tube
/l c
τ =
, and the sampling period is
2
T
τ
=
, the round trip in
a tube. We can find a relationship between the number of tubes N and the sampling fre-
quency
1/
sF
T
=
:
2
s
LF
N
c
=
(6.15)

Acoustical Model of Speech Production
285
For example, for
sF = 8000 kHz, c = 34000 cm/s, and L = 17 cm, the average length
of a male adult vocal tract, we obtain N = 8, or alternatively 4 formants. Experimentally, the
vocal tract transfer function has been observed to have approximately 1 formant per kilo-
hertz. Shorter vocal tract lengths (females or children) have fewer resonances per kilohertz
and vice versa.
The pressure at the lips has been found to approximate the derivative of volume veloc-
ity, particularly at low frequencies. Thus,
( )
L
Z
z
can be approximated by
1
0
( )
(1
)
L
Z
z
R
z−
≈
−
(6.16)
which is 0 for low frequencies and reaches
0
R
asymptotically. This dependency upon fre-
quency results in a reflection coefficient that is also a function of frequency. For low fre-
quencies,
1
Lr = , and no loss occurs. At higher frequencies, loss by radiation translates into
widening of formant bandwidths.
Similarly, the glottal impedance is also a function of frequency in practice. At high
frequencies,
G
Z
is large and
1
Gr ≈
so that all the energy is transmitted. For low frequen-
cies,
1
Gr < , whose main effect is an increase of bandwidth for the lower formants.
1
2
3
4
5
6
7
8
9
10
11
0
10
20
30
Area (cm2)
Distance d = 1.75cm
0
500
1000
1500
2000
2500
3000
3500
4000
-20
0
20
40
60
Frequency (Hz)
(dB)
Figure 6.11 Area function and frequency response for vowel /a/ and its approximation as a
concatenation of 10 lossless tubes. A reflection coefficient at the load of k = 0.72 (dotted line)
is displayed. For comparison, the case of k = 1.0 (solid line) is also shown.

286
Speech Signal Representations
Moreover, energy is lost as a result of vibration of the tube walls, which is more pro-
nounced at low frequencies. Energy is also lost, to a lesser extent, as a result of viscous fric-
tion between the air and the walls of the tube, particularly at frequencies above 3kHz. The
yielding walls tend to raise the resonance frequencies while the viscous and thermal losses
tend to lower them. The net effect in the transfer function is a broadening of the resonances’
bandwidths.
Despite thermal losses, yielding walls in the vocal tract, and the fact that both
Lr and
Gr
are functions of frequency, the all-pole model of Eq. (6.13) for V(z) has been found to be
a good approximation in practice [13]. In Figure 6.11 we show the measured area function
of a vowel and its corresponding frequency response obtained using the approximation as a
concatenation of 10 lossless tubes with a constant
Lr . The measured formants and corre-
sponding bandwidths match quite well with this model despite all the approximations made.
Thus, this concatenation of lossless tubes model represents reasonably well the acoustics
inside the vocal tract. Inspired by the above results, we describe in Section 6.3 “Linear Pre-
dictive Coding,” an all-pole model for speech.
In the production of the nasal consonants, the velum is lowered to trap the nasal tract
to the pharynx, whereas a complete closure is formed in the oral tract (/m/ at the lips, /n/ just
back of the teeth and /ng/ just forward of the velum itself. This configuration is shown in
Figure 6.12, which shows two branches, one of them completely closed. For nasals, the ra-
diation occurs primarily at the nostrils. The set of resonances is determined by the shape and
length of the three tubes. At certain frequencies, the wave reflected in the closure cancels the
wave at the pharynx, preventing energy from appearing at nostrils. The result is that for na-
sal sounds, the vocal tract transfer function V(z) has anti-resonances (zeros) in addition to
resonances. It has also been observed that nasal resonances have broader bandwidths than
non-nasal voiced sounds, due to the greater viscous friction and thermal loss because of the
large surface area of the nasal cavity.
Figure 6.12 Coupling of the nasal cavity with the oral cavity.
6.2.3.
Source-Filter Models of Speech Production
As shown in Chapter 10, speech signals are captured by microphones that respond to
changes in air pressure. Thus, it is of interest to compute the pressure at the lips
( )
LP z ,
which can be obtained as
( )
( )
( )
( ) ( )
( )
L
L
L
G
L
P z
U
z Z
z
U
z V z Z
z
=
=
(6.17)
Closure
Nostrils
Pharynx
Glottis

Acoustical Model of Speech Production
287
For voiced sounds we can model
[ ]
G
u
n
as an impulse train convolved with g[n], the
glottal pulse (see Figure 6.13). Since g[n] is of finite length, its z-transform is an all-zero
system.
Figure 6.13 Model of the glottal excitation for voiced sounds.
The complete model for both voiced and unvoiced sounds is shown in Figure 6.14. We
have modeled
[ ]
G
u
n in unvoiced sounds as random noise.
Figure 6.14 General discrete-time model of speech production. The excitation can be either an
impulse train with period T and amplitude
vA driving a filter G(z) or random noise with am-
plitude
n
A .
We can simplify the model in Figure 6.14 by grouping G(z), V(z), and ZL(z) into H(z)
for voiced sounds, and V(z) and ZL(z) into H(z) for unvoiced sounds. The simplified model is
shown in Figure 6.15, where we make explicit the fact that the filter changes over time.
Figure 6.15 Source-filter model for voiced and unvoiced speech.
This model is a decent approximation, but fails on voiced fricatives, since those
sounds contain both a periodic component and an aspirated component. In this case, a mixed
excitation model can be applied, using for voiced sounds a sum of both an impulse train and
colored noise (Figure 6.16).
The model in Figure 6.15 is appealing because the source is white (has a flat spec-
trum) and all the coloring is in the filter. Other source-filter decompositions attempt to
model the source as the signal at the glottis, in which the source is definitely not white.
Since G(z), ZL(z) contain zeros, and V(z) can also contain zeros for nasals,
( )
H z
is no
z( )
( )
L
Z
z
G z( )
Av
An
g[n]
uG[n]
( )
H z
s[n]

288
Speech Signal Representations
longer all-pole. However, recall from in Chapter 5, we state that the z-transform of
[ ]
[ ]
n
x n
a u n
=
is
1
0
1
( )
1
n
n
n
X z
a z
az
∞
−
−
=
=
=
−

for
a
z
<
(6.18)
so that by inverting Eq. (6.18) we see that a zero can be expressed with infinite poles. This is
the reason why all-pole models are still reasonable approximations as long as a large enough
number of poles is used. Fant [12] showed that on the average the speech spectrum contains
one pole per kHz. Setting the number of poles p to
2
sF +
, where
sF is the sampling fre-
quency expressed in kHz, has been found to work well in practice.
Figure 6.16 A mixed excitation source-filter model of speech.
6.3.
LINEAR PREDICTIVE CODING
A very powerful method for speech analysis is based on linear predictive coding (LPC) [4,
7, 19, 24, 27], also known as LPC analysis or auto-regressive (AR) modeling. This method
is widely used because it is fast and simple, yet an effective way of estimating the main pa-
rameters of speech signals.
As shown in Section 6.2, an all-pole filter with a sufficient number of poles is a good
approximation for speech signals. Thus, we could model the filter H(z) in Figure 6.15 as
1
( )
1
1
( )
( )
( )
1
p
k
k
k
X z
H z
E z
A z
a z−
=
=
=
=
−
(6.19)
where p is the order of the LPC analysis. The inverse filter A(z) is defined as
1
( )
1
p
k
k
k
A z
a z−
=
= −
(6.20)
Taking inverse z-transforms in Eq. (6.19) results in
1
[ ]
[
]
[ ]
p
k
k
x n
a x n
k
e n
=
=
−
+

(6.21)
( )
H z
s[n]
+

Linear Predictive Coding
289
Linear predictive coding gets its name from the fact that it predicts the current sample
as a linear combination of its past p samples:
1
[ ]
[
]
p
k
k
x n
a x n
k
=
=
−


(6.22)
The prediction error when using this approximation is
1
[ ]
[ ]
[ ]
[ ]
[
]
p
k
k
e n
x n
x n
x n
a x n
k
=
=
−
=
−
−


(6.23)
6.3.1.
The Orthogonality Principle
To estimate the predictor coefficients from a set of speech samples, we use the short-term
analysis technique. Let’s define
[ ]
m
x
n
as a segment of speech selected in the vicinity of
sample m:
[ ]
[
]
m
x
n
x m
n
=
+
(6.24)
We define the short-term prediction error for that segment as
(
)
2
2
2
1
[ ]
[ ]
[ ]
[ ]
[
]
p
m
m
m
m
m
j
m
n
n
n
j
E
e
n
x
n
x
n
x
n
a x
n
j
=


=
=
−
=
−
−









(6.25)
Figure 6.17 The orthogonality principle. The prediction error is orthogonal to the past sam-
ples.
In the absence of knowledge about the probability distribution of
ia , a reasonable es-
timation criterion is minimum mean squared error, introduced in Chapter 4. Thus, given a
signal
[ ]
m
x
n , we estimate its corresponding LPC coefficients as those that minimize the
total prediction error
m
E . Taking the derivative of Eq. (6.25) with respect to
ia and equat-
ing to 0, we obtain:
1
m
x
2
m
x
m
x
em
m
x

290
Speech Signal Representations
,
[ ]
[
]
0
i
m
m
m
m
n
e
n x
n
i
<
>=
−
=

e
x
1
i
p
≤≤
(6.26)
where we have defined
m
e
and
i
m
x
as vectors of samples, and their inner product has to be
0. This condition, known as orthogonality principle, says that the predictor coefficients that
minimize the prediction error are such that the error must be orthogonal to the past vectors,
and is seen in Figure 6.17.
Equation (6.26) can be expressed as a set of p linear equations
1
[
]
[ ]
[
]
[
]
p
m
m
j
m
m
n
j
n
x
n
i x
n
a
x
n
i x
n
j
=
−
=
−
−

 
1,2,
,
i
p
=

(6.27)
For convenience, we can define the correlation coefficients as
[ , ]
[
]
[
]
m
m
m
n
i j
x
n
i x
n
j
φ
=
−
−

(6.28)
so that Eqs. (6.27) and (6.28) can be combined to obtain the so-called Yule-Walker equa-
tions:
1
[ , ]
[ ,0]
p
j
m
m
j
a
i j
i
φ
φ
=
=

1,2,
,
i
p
=

(6.29)
Solution of the set of p linear equations results in the p LPC coefficients that minimize
the prediction error. With
ia satisfying Eq. (6.29), the total prediction error in Eq. (6.25)
takes on the following value:
2
1
1
[ ]
[ ]
[
]
[0,0]
[0, ]
p
p
m
m
j
m
m
j
n
j
n
j
E
x
n
a
x
n x
n
j
a
j
φ
φ
=
=
=
−
−
=
−

 

(6.30)
It is convenient to define a normalized prediction error u[n] with unity energy
2 [ ]
1
m
n
u
n =

(6.31)
and a gain G, such that
[ ]
[ ]
m
m
e
n
Gu
n
=
(6.32)
The gain G can be computed from the short-term prediction error
2
2
2
2
[ ]
[ ]
m
m
m
n
n
E
e
n
G
u
n
G
=
=
=


(6.33)

Linear Predictive Coding
291
6.3.2.
Solution of the LPC Equations
The solution of the Yule-Walker equations in Eq. (6.29) can be achieved with any standard
matrix inversion package. Because of the special form of the matrix here, some efficient
solutions are possible, as described below. Also, each solution offers a different insight so
we present three different algorithms: the covariance method, the autocorrelation method,
and the lattice method.
6.3.2.1. Covariance Method
The covariance method [4] is derived by defining directly the interval over which the
summation in Eq. (6.28) takes place:
1
2
0
[ ]
N
m
m
n
E
e
n
−
=
= 
(6.34)
so that
[ , ]
m i j
φ
in Eq. (6.28) becomes
1
1
0
[ , ]
[
]
[
]
[ ]
[
]
[ , ]
N
j
N
m
m
m
m
m
m
n
n
i
i j
x
n
i x
n
j
x
n x
n
i
j
j i
φ
φ
−−
−
=
=−
=
−
−
=
+ −
=


(6.35)
and Eq. (6.29) becomes
1
2
3
[1,1]
[1,2]
[1,3]
[1, ]
[1,0]
[2,1]
[2,2]
[2,3]
[1, ]
[2,0]
[3,1]
[3,2]
[3,3]
[3, ]
[3,0]
[ ,1]
[ ,2]
[ ,3]
[ , ]
[ ,0]
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
p
m
m
m
m
m
a
p
a
p
a
p
a
p
p
p
p p
p
φ
φ
φ
φ
φ
φ
φ
φ
φ
φ
φ
φ
φ
φ
φ
φ
φ
φ
φ
φ




















=






































(6.36)
which can be expressed as the following matrix equation
ψ
Φ =
a
(6.37)
where the matrix Φ in Eq. (6.37) is symmetric and positive definite, for which efficient
methods are available, such as the Cholesky decomposition. For this method, also called the
squared root method, the matrix Φ is expressed as
t
Φ = VDV
(6.38)
where V is a lower triangular matrix (whose main diagonal elements are 1’s), and D is a
diagonal matrix. So each element of Φ can be expressed as
1
[ , ]
j
ik
k
jk
k
i j
V d V
φ
=
= 
1
j
i
≤
<
(6.39)

292
Speech Signal Representations
or alternatively
1
1
[ , ]
j
ij
j
ik
k
jk
k
V d
i j
V d V
φ
−
=
=
−
1
j
i
≤
<
(6.40)
and for the diagonal elements
1
[ , ]
i
ik
k
ik
k
i i
V d V
φ
=
= 
(6.41)
or alternatively
1
2
1
[ , ]
i
i
ik
k
k
d
i i
V d
φ
−
=
=
−
,
2
i ≥
(6.42)
with
1
[1,1]
d
φ
=
(6.43)
The Cholesky decomposition starts with Eq. (6.43) then alternates between Eqs. (6.40)
and (6.42). Once the matrices V and D have been determined, the LPC coefficients are
solved in a two-step process. The combination of Eqs. (6.37) and (6.38) can be expressed as
ψ
=
VY
(6.44)
with
t
=
Y
DV a
(6.45)
or alternatively
1
t
−
=
V a
D Y
(6.46)
Therefore, given matrix V and Eq. (6.44), Y can be solved recursively as
1
1
i
i
i
ij
j
j
Y
V Y
ψ
−
=
=
−
,
2
i
p
≤≤
(6.47)
with the initial condition
1
1
Y
ψ
=
(6.48)
Having determined Y , Eq. (6.46) can be solved recursively in a similar way
1
/
p
i
i
i
ji
j
j i
a
Y
d
V a
= +
=
−
,
1
i
p
≤<
(6.49)
with the initial condition
/
p
p
p
a
Y
d
=
(6.50)

Linear Predictive Coding
293
where the index i in Eq. (6.49) proceeds backwards.
The term covariance analysis is somewhat of a misnomer, since we know from Chap-
ter 5 that the covariance of a signal is the correlation of that signal with its mean removed. It
was so called because the matrix in Eq. (6.36) has the properties of a covariance matrix,
though this algorithm is more like a cross-correlation.
6.3.2.2. Autocorrelation Method
The summation in Eq. (6.28) had no specific range. In the autocorrelation method [24, 27],
we assume that
[ ]
m
x
n is 0 outside the interval 0
n
N
≤
<
:
[ ]
[
] [ ]
m
x
n
x m
n w n
=
+
(6.51)
with
[ ]
w n
being a window (such as a Hamming window) which is 0 outside the interval
0
n
N
≤
<
. With this assumption, the corresponding prediction error
[ ]
m
e
n
is non-zero over
the interval 0
n
N
p
≤
<
+
, and, therefore, the total prediction error takes on the value
1
2
0
[ ]
N
p
m
m
n
E
e
n
+
−
=
= 
(6.52)
With this range, Eq. (6.28) can be expressed as
1
1 (
)
0
0
[ , ]
[
]
[
]
[ ]
[
]
N
p
N
i
j
m
m
m
m
m
n
n
i j
x
n
i x
n
j
x
n x
n
i
j
φ
+
−
−−
−
=
=
=
−
−
=
+ −


(6.53)
or alternatively
[ , ]
[
]
m
m
i j
R i
j
φ
=
−
(6.54)
with
[ ]
m
R
k
being the autocorrelation sequence of
[ ]
m
x
n :
1
0
[ ]
[ ]
[
]
N
k
m
m
m
n
R
k
x
n x
n
k
−−
=
=
+

(6.55)
Combining Eqs. (6.54) and (6.29), we obtain
1
[|
|]
[ ]
p
j
m
m
j
a R
i
j
R i
=
−
=

(6.56)
which corresponds to the following matrix equation

294
Speech Signal Representations
1
2
3
[0]
[1]
[2]
[
1]
[1]
[1]
[0]
[1]
[
2]
[2]
[2]
[1]
[0]
[
3]
[3]
[
1]
[
2]
[
3]
[0]
[ ]
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
p
m
m
m
m
m
a
R
R
R
R
p
R
a
R
R
R
R
p
R
a
R
R
R
R
p
R
a
R
p
R
p
R
p
R
R
p


−










−












=
−


















−
−
−

















(6.57)
The matrix in Eq. (6.57) is symmetric and all the elements in its diagonals are identical.
Such matrices are called Toeplitz. Durbin’s recursion exploits this fact resulting in a very
efficient algorithm (for convenience, we omit the subscript m of the autocorrelation func-
tion), whose proof is beyond the scope of this book:
1. Initialization
0
[0]
E
R
=
(6.58)
2. Iteration. For
1,
,
i
p
=

do the following recursion:
1
1
1
1
[ ]
[
] /
i
i
i
i
j
j
k
R i
a
R i
j
E
−
−
−
=


=
−
−





(6.59)
i
i
i
a
k
=
(6.60)
1
1
i
i
i
j
j
i
i
j
a
a
k a
−
−
−
=
−
,
1
j
i
≤
<
(6.61)
2
1
(1
)
i
i
i
E
k
E −
=
−
(6.62)
3. Final solution:
p
j
j
a
a
=
1
j
p
≤
≤
(6.63)
where the coefficients
ik , called reflection coefficients, are bounded between –1 and 1 (see
Section 6.3.2.3). In the process of computing the predictor coefficients of order p, the recur-
sion finds the solution of the predictor coefficients for all orders less than p.
Replacing
[ ]
R j
by the normalized autocorrelation coefficients
[ ]
r j , defined as
[ ]
[ ]/ [0]
r j
R j
R
=
(6.64)
results in identical LPC coefficients, and the recursion is more robust to problems with
arithmetic precision. Likewise, the normalized prediction error at iteration i is defined by
dividing Eq. (6.30) by R[0], which, using Eq. (6.54), results in
1
1
[ ]
[0]
i
i
i
j
j
E
V
a r j
R
=
=
= −
(6.65)

Linear Predictive Coding
295
The normalized prediction error is, using Eqs. (6.62) and (6.65),
2
1
(1
)
p
p
i
i
V
k
=
=
−
∏
(6.66)
6.3.2.3. Lattice Formulation
In this section we derive the lattice formulation [7, 19], an equivalent algorithm to the Lev-
inson Durbin recursion, which has some precision benefits. It is advantageous to define the
forward prediction error obtained at stage i of the Levinson Durbin procedure as
1
[ ]
[ ]
[
]
i
i
i
k
k
e n
x n
a x n
k
=
=
−
−

(6.67)
whose z-transform is given by
( )
( )
( )
i
i
E z
A z X z
=
(6.68)
with
( )
iA z
being defined by
1
( )
1
i
i
i
k
k
k
A z
a z−
=
= −
(6.69)
which, combined with Eq. (6.61), results in the following recursion:
1
1
1
( )
( )
(
)
i
i
i
i
i
A z
A
z
k z A
z
−
−
−
−
=
−
(6.70)
Similarly, we can define the so-called backward prediction error as
1
[ ]
[
]
[
]
i
i
i
k
k
b n
x n
i
a x n
k
i
=
=
−
−
+
−

(6.71)
whose z-transform is
1
( )
(
)
( )
i
i
i
B z
z A z
X z
−
−
=
(6.72)
Now combining Eqs. (6.68), (6.70), and (6.72), we obtain
1
1
1
1
1
( )
( )
( )
(
)
( )
( )
( )
i
i
i
i
i
i
i
i
E z
A
z X z
k z A
z
X z
E
z
k B
z
−
−
−
−
−
−
=
−
=
−
(6.73)
whose inverse z-transform is given by
1
1
[ ]
[ ]
[
1]
i
i
i
i
e n
e
n
k b
n
−
−
=
−
−
(6.74)
Also, substituting Eqs. (6.70) into (6.72) and using Eq. (6.68), we obtain
1
1
1
( )
( )
( )
i
i
i
i
B z
z B
z
k E
z
−
−
−
=
−
(6.75)

296
Speech Signal Representations
whose inverse z-transform is given by
1
1
[ ]
[
1]
[ ]
i
i
i
i
b n
b
n
k e
n
−
−
=
−
−
(6.76)
Equations (6.74) and (6.76) define the forward and backward prediction error sequences for
an ith-order predictor in terms of the corresponding forward and backward prediction errors
of an (i - 1)th-order predictor. We initialize the recursive algorithm by noting that the 0th-
order predictor is equivalent to using no predictor at all; thus
0
0
[ ]
[ ]
[ ]
e n
b n
x n
=
=
(6.77)
and the final prediction error is
[ ]
[ ]
p
e n
e
n
=
.
A block diagram of the lattice method is given in Figure 6.18, which resembles a lat-
tice, whence its name.
Figure 6.18 Block diagram of the lattice filter.
While the computation of the
ik coefficients can be done through the Levinson Durbin
recursion of Eqs. (6.59) through (6.62), it can be shown that an equivalent calculation can be
found as a function of the forward and backward prediction errors. To do so we minimize
the sum of the forward prediction errors
(
)
1
2
0
[ ]
N
i
i
n
E
e n
−
=
= 
(6.78)
by substituting Eq. (6.74) in (6.78), taking the derivative with respect to
ik , and equating to
0:
(
)
1
1
1
0
1
2
1
0
[ ]
[
1]
[
1]
N
i
i
n
i
N
i
n
e
n b
n
k
b
n
−
−
−
=
−
−
=
−
=
−


(6.79)
Using Eqs. (6.67) and (6.71), it can be shown that
x[n]
z-1
+
+
e0[n]
b0[n]
-k1
-k1
z-1
+
+
e1[n]
b1[n]
-k2
-k2
z-1
+
+
ep-1[n]
bp-1[n]
-kp
-kp
ep[n]

Linear Predictive Coding
297
(
)
(
)
1
1
2
2
1
1
0
0
[ ]
[
1]
N
N
i
i
n
n
e
n
b
n
−
−
−
−
=
=
=
−


(6.80)
since minimization of both yields identical Yule-Walker equations. Thus Eq. (6.79) can be
alternatively expressed as
(
)
(
)
1
1
1
1
1
0
1
1
1
1
2
2
1
1
0
0
[ ]
[
1]
,
[ ]
[
1]
N
i
i
i
i
n
i
i
i
N
N
i
i
n
n
e
n b
n
k
e
n
b
n
−
−
−
−
−
=
−
−
−
−
−
−
=
=
−
<
>
=
=
−



e
b
e
b
(6.81)
where we have defined the vectors
(
)
[0]
[
1]
i
i
i
e
e N
=
−
e

and
(
)
[0]
[
1]
i
i
i
b
b N
=
−
b

. The
inner product of two vectors x and y is defined as
1
0
,
[ ] [ ]
N
n
x n y n
−
=
<
>= 
x y
(6.82)
and its norm as
1
2
2
0
,
[ ]
N
n
x n
−
=
=<
>= 
x
x x
(6.83)
Equation (6.81) has the form of a normalized cross-correlation function, and, there-
fore, the reason the reflection coefficients are also called partial correlation coefficients
(PARCOR). As with any normalized cross-correlation function, the
ik
coefficients are
bounded by
1
1
ik
−≤
≤
(6.84)
This is a necessary and sufficient condition for all the roots of the polynomial
( )
A z
to
be inside the unit circle, therefore guaranteeing a stable filter. This condition can be checked
to avoid numerical imprecision by stopping the recursion if the condition is not met. The
inverse lattice filter can be seen in Figure 6.19, which resembles the lossless tube model.
This is why the
ik are also called reflection coefficients.
Figure 6.19 Inverse lattice filter used to generate the speech signal, given its residual.
x[n]
b0[n]
ep[n]
bp[n]
+
+
kp
-kp
z-1
ep-1[n]
bp-1[n]
+
+
kp-1
-kp-1
z-1
e1[n]
b1[n]
+
+
k1
-k1
z-1

298
Speech Signal Representations
Lattice filters are often used in fixed-point implementation, because lack of precision
doesn’t result in unstable filters. Any error that may take place – for example due to quanti-
zation – is generally not be sufficient to cause
ik to fall outside the range in Eq. (6.84). If,
owing to round-off error, the reflection coefficient falls outside the range, the lattice filter
can be ended at the previous step.
More importantly, linearly varying coefficients can be implemented in this fashion.
While, typically, the reflection coefficients are constant during the analysis frame, we can
implement a linear interpolation of the reflection coefficients to obtain the error signal. If the
coefficients of both frames are in the range in Eq. (6.84), the linearly interpolated reflection
coefficients also have that property, and thus the filter is stable. This is a property that the
predictor coefficients don’t have.
6.3.3.
Spectral Analysis via LPC
Let’s now analyze the frequency-domain behavior of the LPC analysis by evaluating
1
(
)
(
)
1
j
p
j
j k
k
k
G
G
H e
A e
a e
ω
ω
ω
−
=
=
=
−
(6.85)
which is an all-pole or IIR filter. If we plot
(
)
j
H e ω , we expect to see peaks at the roots of
the denominator. Figure 6.20 shows the 14-order LPC spectrum of the vowel of Figure 6.3
(d).
0
500
1000
1500
2000
2500
3000
3500
4000
20
30
40
50
60
70
80
90
100
dB
Hz
Figure 6.20 LPC spectrum of the /ah/ phoneme in the word lifes of Figure 6.3. Used here are a
30-ms Hamming window and the autocorrelation method with p = 14. The short-time spectrum
is also shown.
For the autocorrelation method, the squared error of Eq. (6.52) can be expressed, using
Eq. (6.85) and Parseval’s theorem, as

Linear Predictive Coding
299
2
2
2
|
(
) |
2
|
(
) |
j
m
m
j
X
e
G
E
d
H e
ω
π
ω
π
ω
π
−
=

(6.86)
Since the integrand in Eq. (6.86) is positive, minimizing
m
E
is equivalent to minimizing the
ratio of the energy spectrum of the speech segment
2
|
(
) |
j
m
X
e ω
to the magnitude squared of
the frequency response of the linear system
2
|
(
) |
j
H e ω
. The LPC spectrum matches more
closely the peaks than the valleys (see Figure 6.20), because the regions where
|
(
) | |
(
) |
j
j
m
X
e
H e
ω
ω
>
contribute more to the error than those where |
(
) | |
(
) |
j
j
m
H e
X
e
ω
ω
>
.
Even nasals, which have zeros in addition to poles, can be represented with an infinite
number of poles. In practice, if p is large enough we can approximate the signal spectrum
with arbitrarily small error. Figure 6.21 shows different fits for different values of p. The
higher p, the more details of the spectrum are preserved.
0
500
1000
1500
2000
2500
3000
3500
4000
20
30
40
50
60
70
80
90
100
dB
Hz
p=4
p=8
p=14
Figure 6.21 LPC spectra of Figure 6.20 for various values of the predictor order p.
The prediction order is not known for arbitrary speech, so we need to set it to balance
spectral detail with estimation errors.
6.3.4.
The Prediction Error
So far, we have concentrated on the filter component of the source-filter model. Using Eq.
(6.23), we can compute the prediction error signal, also called the excitation, or residual
signal. For unvoiced speech synthetically generated by white noise following an LPC filter
we expect the residual to be approximately white noise. In practice, this approximation is
quite good, and replacement of the residual by white noise followed by the LPC filter typi-
cally results in no audible difference. For voiced speech synthetically generated by an im-
pulse train following an LPC filter, we expect the residual to approximate an impulse train.
In practice, this is not the case, because the all-pole assumption is not altogether valid; thus,
the residual, although it contains spikes, is far from an impulse train. Replacing the residual
by an impulse train, followed by the LPC filter, results in speech that sounds somewhat ro-

300
Speech Signal Representations
botic, partly because real speech is not perfectly periodic (it has a random component as
well), and because the zeroes are not modeled with the LPC filter. Residual signals com-
puted from inverse LPC filters for several vowels are shown in Figure 6.22.
5 0
10 0
15 0
2 00
-20 0
0
20 0
S ig nal
"ah"
50
1 00
150
200
0
0.2
0.4
P red ic tion E rror
5 0
10 0
15 0
2 00
-10 0
-5 0
0
5 0
"ee"
50
1 00
150
200
-0.1
0
0.1
0.2
0.3
5 0
10 0
15 0
2 00
-30 0
-20 0
-10 0
0
10 0
"oh"
50
1 00
150
200
-0.1
0
0.1
0.2
5 0
10 0
15 0
2 00
-20 0
-10 0
0
10 0
"ay "
50
1 00
150
200
0
0.2
0.4
Figure 6.22 LPC prediction error signals for several vowels.
How do we choose p? This is an important design question. Larger values of p lead to
lower prediction errors (see Figure 6.23). Unvoiced speech has higher error than voiced
speech, because the LPC model is more accurate for voiced speech. In general, the normal-
ized error rapidly decreases, and then converges to a value of around 12 - 14 for 8 kHz
speech. If we use a large value of p, we are fitting the individual harmonics; thus the LPC
filter is modeling the source, and the separation between source and filter is not going to be
so good. The more coefficients we have to estimate, the larger the variance of their esti-
mates, since the number of available samples is the same. A rule of thumb is to use 1 com-
plex pole per kHz plus 2 - 4 poles to model the radiation and glottal effects.
For unvoiced speech, both the autocorrelation and the covariance methods provide
similar results. For voiced speech, however, the covariance method can provide better esti-
mates if the analysis window is shorter than the local pitch period and the window only in-
cludes samples from the closed phase (when the vocal tract is closed at the glottis and
speech signal is due mainly to free resonances). This is called pitch synchronous analysis
and results in lower prediction error, because the true excitation is close to zero during the
whole analysis window. During the open phase, the trachea, the vocal folds, and the vocal
tract are acoustically coupled, and this coupling will change the free resonances. Addition-
ally, the prediction error is higher for both the autocorrelation and the covariance methods if
samples from the open phase are included in the analysis window, because the prediction
during those instants is poor.

Linear Predictive Coding
301
0
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1
p
RMS Prediction Error
Unvoiced Speech
Voiced Speech
Figure 6.23 Variation of the normalized prediction error with the number of prediction coeffi-
cients p for the voiced segment of Figure 6.3 and the unvoiced speech of Figure 6.5. The auto-
correlation method was used with a 30 ms Hamming window, and a sampling rate of 8 kHz.
6.3.5.
Equivalent Representations
There are a number of alternate useful representations of the predictor coefficients. The most
important are the line spectrum pairs, reflection coefficients, log-area ratios, and the roots of
the predictor polynomial.
6.3.5.1.
Line Spectral Frequencies
Line Spectral Frequencies (LSF) [18] provide an equivalent representation of the predictor
coefficients that is very popular in speech coding. It is derived from computing the roots of
the polynomials P(z) and Q(z) defined as
(
1)
1
( )
( )
(
)
p
P z
A z
z
A z
−
+
−
=
+
(6.87)
(
1)
1
( )
( )
(
)
p
Q z
A z
z
A z
−
+
−
=
−
(6.88)
To gain insight on these roots, look at a second-order predictor filter with a pair of
complex roots:
1
2
1
2
2
1
2
0
0
0
( )
1
1
2
cos(2
)
A z
a z
a z
f
z
z
ρ
π
ρ
−
−
−
−
= −
−
= −
+
(6.89)
where
0
0
1
ρ
<
<
and
0
0
0.5
f
<
<
. Inserting Eq. (6.89) into (6.87) and (6.88) results in
1
2
3
1
2
1
2
1
2
3
1
2
1
2
( )
1 (
)
(
)
( )
1 (
)
(
)
P z
a
a
z
a
a
z
z
Q z
a
a
z
a
a
z
z
−
−
−
−
−
−
= −
+
−
+
+
= −
−
+
−
−
(6.90)
From Eq. (6.90) we see that
1
z = −
is a root of P(z) and
1
z =
a root of Q(z), which can be
divided out and results in

302
Speech Signal Representations
1
1
2
1
1
1
2
2
( )
(1
)(1
2
)
( )
(1
)(1
2
)
P z
z
z
z
Q z
z
z
z
β
β
−
−
−
−
−
−
=
+
−
+
=
−
−
+
(6.91)
where
1
β and
2
β are given by
2
0
1
2
1
0
0
2
0
1
2
2
0
0
1
1
cos(2
)
2
2
1
1
cos(2
)
2
2
a
a
f
a
a
f
ρ
β
ρ
π
ρ
β
ρ
π
−
+
+
=
=
+
−
−
−
=
=
−
(6.92)
It can be shown that
1
1
β <
and
2
1
β
<
for all possible values of
0f
and
0
ρ . With
this property, the roots of P(z) and Q(z) in Eq. (6.91) are complex and given by
2
1
1
1
j
β
β
±
−
and
2
2
2
1
j
β
β
±
−
, respectively. Because they lie in the unit circle, they can
be uniquely represented by their angles
2
0
1
0
0
2
0
2
0
0
1
cos(2
)
cos(2
)
2
1
cos(2
)
cos(2
)
2
f
f
f
f
ρ
π
ρ
π
ρ
π
ρ
π
−
=
+
−
=
−
(6.93)
where
1f
and
2f
are
the
line
spectral
frequencies
of
A(z).
Since
0
1
ρ
< ,
2
0
cos(2
)
cos(2
)
f
f
π
π
<
, and thus
2
0
f
f
>
. It’s also the case that
1
0
cos(2
)
cos(2
)
f
f
π
π
>
and thus
1
0
f
f
<
. Furthermore, as
0
1
ρ →, we see from Eq. (6.93) that
1
0
f
f
→
and
2
0
f
f
→
. We conclude that, given a pole at
0f , the two line spectral frequencies bracket it,
i.e.,
1
0
2
f
f
f
<
<
, and that they are closer together as the pole of the second-order resonator
gets closer to the unit circle.
We have proven that for a second-order predictor, the roots of P(z) and Q(z) lie in the
unit circle, that
1
±
are roots, and that, once sorted, the roots of P(z) and Q(z) alternate. Al-
though we do not prove it here, it can be shown that these conclusions hold for other predic-
tor orders, and, therefore, the p predictor coefficients can be transformed into p line spectral
frequencies. We also know that
1
z =
is always a root of Q(z), whereas
1
z = −
is a root of
P(z) for even p and a root of Q(z) for odd p.
To compute the LSF for
2
p >
, we replace
cos( )
z
ω
=
and compute the roots of
( )
P ω
and
( )
Q ω
by any available root finding method. A popular technique, given that
there are p roots which are real in ω and bounded between 0 and 0.5, is to bracket them by
observing changes in sign of both functions in a dense grid. To compute the predictor coef-
ficients from the LSF coefficients we can factor P(z) and Q(z) as a product of second-order
filters as in Eq. (6.91), and then
(
)
( )
( )
( ) / 2
A z
P z
Q z
=
+
.
In practice, LSF are useful because of sensitivity (a quantization of one coefficient
generally results in a spectral change only around that frequency) and efficiency (LSF result

Linear Predictive Coding
303
in low spectral distortion). This doesn’t occur with other representations. As long as the LSF
coefficients are ordered, the resulting LPC filter is stable, though the proof is beyond the
scope of this book. LSF coefficients are used extensively in Chapter 7.
6.3.5.2.
Reflection Coefficients
For the autocorrelation method, the predictor coefficients may be obtained from the reflec-
tion coefficients by the following recursion:
1
1
1,
,
1
i
i
i
i
i
i
j
j
i
i
j
a
k
i
p
a
a
k a
j
i
−
−
−
=
=
=
−
≤
<

(6.94)
where
p
i
i
a
a
=
. Similarly, the reflection coefficients may be obtained from the prediction
coefficients using a backward recursion of the form
1
2
,
,1
1
1
i
i
i
i
i
i
j
i
i
j
i
j
i
k
a
i
p
a
a a
a
j
i
k
−
−
=
=
+
=
≤
<
−

(6.95)
where we initialize
p
i
i
a
a
=
.
Reflection coefficients are useful when implementing LPC filters whose values are in-
terpolated over time, because, unlike the predictor coefficients, they are guaranteed to be
stable at all times as long as the anchors satisfy Eq. (6.84).
6.3.5.3.
Log-Area Ratios
The log-area ratio coefficients are defined as
1
ln 1
i
i
i
k
g
k


−
=


+


(6.96)
with the inverse being given by
1
1
i
i
g
i
g
e
k
e
−
=
+
(6.97)
The log-area ratio coefficients are equal to the natural logarithm of the ratio of the ar-
eas of adjacent sections of a lossless tube equivalent of the vocal tract having the same trans-
fer function. Since for stable predictor filters
1
1
ik
−<
< , we have from Eq. (6.96) that
ig
−∞<
< ∞. For speech signals, it is not uncommon to have some reflection coefficients
close to 1, and quantization of those values can cause a large change in the predictor’s trans-
fer function. On the other hand, the log-area ratio coefficients have relatively flat spectral

304
Speech Signal Representations
sensitivity (i.e., a small change in their values causes a small change in the transfer function)
and thus are useful in coding.
6.3.5.4.
Roots of Polynomial
An alternative to the predictor coefficients results from computing the complex roots of the
predictor polynomial:
1
1
1
( )
1
(1
)
p
p
k
k
k
k
k
A z
a z
z z
−
−
=
=
= −
=
−

∏
(6.98)
These roots can be represented as
(
2
) /
k
k
s
b
j
f
F
kz
e
π
π
−
+
=
(6.99)
where
kb ,
kf , and
sF represent the bandwidth, center frequency, and sampling frequency,
respectively. Since
ka
are real, all complex roots occur in conjugate pairs so that if (
,
)
k
k
b
f
is a root, so is (
,
)
k
k
b
f
−
. The bandwidths
kb are always positive, because the roots are in-
side the unit circle (
1
kz
< ) for a stable predictor. Real roots
/
k
s
b
F
kz
e
π
−
=
can also occur.
While algorithms exist to compute the complex roots of a polynomial, in practice there are
sometimes numerical difficulties in doing so.
If the roots are available, it is straightforward to compute the predictor coefficients by
using Eq. (6.98). Since the roots of the predictor polynomial represent resonance frequencies
and bandwidths, they are used in formant synthesizers of Chapter 16.
6.4.
CEPSTRAL PROCESSING
A homomorphic transformation
(
)
ˆ[ ]
[ ]
x n
D x n
=
is a transformation that converts a convolu-
tion
[ ]
[ ]
[ ]
x n
e n
h n
=
∗
(6.100)
into a sum
ˆ
ˆ
ˆ
[ ]
[ ]
[ ]
x n
e n
h n
=
+
(6.101)
In this section we introduce the cepstrum as one homomorphic transformation [32]
that allows us to separate the source from the filter. We show that we can find a value N
such that the cepstrum of the filter ˆ[ ]
0
h n ≈
for n
N
≥
, and that the cepstrum of the excita-
tion ˆ[ ]
0
e n ≈
for n
N
<
. With this assumption, we can approximately recover both
[ ]
e n
and
[ ]
h n
from ˆ[ ]
x n
by homomorphic filtering. In Figure 6.24, we show how to recover
[ ]
h n with a homomorphic filter:

Cepstral Processing
305
1
[ ]
0
n
N
l n
n
N

<

= 
≥

(6.102)
where D is the cepstrum operator.
Figure 6.24 Homomorphic filtering to recover the filter’s response from a periodic signal. We
have used the homomorphic filter of Eq. (6.102).
The excitation signal can similarly recovered with a homomorphic filter given by
1
[ ]
0
n
N
l n
n
N

≥

= 
<

(6.103)
6.4.1.
The Real and Complex Cepstrum
The real cepstrum of a digital signal
[ ]
x n is defined as
1
[ ]
ln |
(
) |
2
j
j n
c n
X e
e
d
π
ω
ω
π
ω
π
−
=

(6.104)
and the complex cepstrum of
[ ]
x n is defined as
1
ˆ[ ]
ln
(
)
2
j
j n
x n
X e
e
d
π
ω
ω
π
ω
π
−
=

(6.105)
where the complex logarithm is used:
ˆ (
)
ln
(
)
ln |
(
) |
( )
j
j
j
X e
X e
X e
j
ω
ω
ω
θ ω
=
=
+
(6.106)
and the phase
( )
θ ω
is given by
( )
arg
(
)
j
X e ω
θ ω


=


(6.107)
You can see from Eqs. (6.104) and (6.105) that both the real and the complex cep-
strum satisfy Eq. (6.101) and thus they are homomorphic transformations.
If the signal
[ ]
x n
is real, both the real cepstrum
[ ]
c n and the complex cepstrum ˆ[ ]
x n
are also real signals. Therefore the term complex cepstrum doesn’t mean that it is a complex
signal but rather that the complex logarithm is taken.
x
D[ ]
[ ]
s n
[ ]
x n
w n
[ ]
ˆ[ ]
x n
x
l n
[ ]
D-1[ ]
[ ]
h n
h n
[ ]

306
Speech Signal Representations
It can easily be shown that
[ ]
c n is the even part of ˆ[ ]
x n :
ˆ
ˆ
[ ]
[
]
[ ]
2
x n
x
n
c n
+
−
=
(6.108)
From here on, when we refer to cepstrum without qualifiers, we are referring to the
real cepstrum, since it is the most widely used in speech technology.
The cepstrum was invented by Bogert et al. [6], and its term was coined by reversing
the first syllable of the word spectrum, given that it is obtained by taking the inverse Fourier
transform of the log-spectrum. Similarly, they defined the term quefrency to represent the
independent variable n in c[n]. The quefrency has dimension of time.
6.4.2.
Cepstrum of Pole-Zero Filters
A very general type of filters are those with rational transfer functions
1
1
1
1
1
1
(1
)
(1
)
( )
(1
)
(1
)
i
o
i
o
M
M
r
k
k
k
k
N
N
k
k
k
k
Az
a z
u z
H z
b z
v z
−
=
=
−
=
=
−
−
=
−
−
∏
∏
∏
∏
(6.109)
with the magnitudes of
ka ,
kb ,
ku
and
kv
all less than 1. Therefore,
1
(1
)
ka z−
−
and
1
(1
)
kb z−
−
represent the zeros and poles inside the unit circle, whereas (1
)
ku z
−
and
(1
)
kv z
−
represent the zeros and poles outside the unit circle, and
rz
is a shift from the time
origin. Thus, the complex logarithm is
1
1
1
1
1
1
ˆ ( )
ln[ ]
ln[
]
ln(1
)
ln(1
)
ln(1
)
ln(1
)
i
i
o
o
M
r
k
k
N
M
N
k
k
k
k
k
k
H z
A
z
a z
b z
u z
v z
−
=
−
=
=
=
=
+
+
−
−
−
+
−
−
−




(6.110)
where the term log[
]
rz
contributes to the imaginary part of the complex cepstrum only with
a term j r
ω . Since it just carries information about the time origin, it’s typically ignored.
We use the Taylor series expansion
1
ln(1
)
n
n
x
x
n
∞
=
−
= −
(6.111)
in Eq. (6.110) and take inverse z-transforms to obtain

Cepstral Processing
307
1
1
1
1
log[ ]
0
ˆ[ ]
0
0
i
i
o
o
n
n
N
M
k
k
k
k
n
n
M
N
k
k
k
k
A
n
b
a
h n
n
n
n
u
v
n
n
n
=
=
=
=


=


=
−
>



−
<





(6.112)
If the filter’s impulse response doesn’t have zeros or poles outside the unit circle, the
so-called minimum phase signals, then ˆ[ ]
0
h n =
for
0
n <
. Maximum phase signals are those
with
ˆ[ ]
0
h n =
for
0
n >
. If a signal is minimum phase, its complex cepstrum can be
uniquely determined from its real cepstrum:
0
0
ˆ[ ]
[ ]
0
2 [ ]
0
n
h n
c n
n
c n
n
<


=
=


>

(6.113)
It is easy to see from Eq. (6.112) that both the real and complex cepstrum are decaying
sequences, which is the reason why, typically, a finite number of coefficients are sufficient
to approximate it, and, therefore, people refer to the truncated cepstrum signal as a cepstrum
vector.
6.4.2.1.
LPC-Cepstrum
The case when the rational transfer function in Eq. (6.109) has been obtained with an LPC
analysis is particularly interesting, since LPC analysis is such a widely used method. While
Eq. (6.112) applies here, too, it is useful to find a recursion which doesn’t require us to com-
pute the roots of the predictor polynomial. Given the LPC filter
1
( )
1
p
k
k
k
G
H z
a z−
=
=
−
(6.114)
we take the logarithm
1
ˆ
ˆ ( )
ln
ln 1
[ ]
p
l
k
l
l
k
H z
G
a z
h k z
∞
−
−
=
=−∞


=
−
−
=






(6.115)
and the derivative of both sides with respect to z

308
Speech Signal Representations
1
1
1
1
ˆ[ ]
1
p
n
n
k
n
p
k
l
l
l
na z
kh k z
a z
−−
∞
−−
=
=−∞
−
=
−
= −
−



(6.116)
Multiplying both sides by
1
1
p
l
l
l
z
a z−
=


−
−





, we obtain
1
1
ˆ
ˆ
[ ]
[ ]
p
p
n
n
k l
n
l
n
n
l
k
na z
nh n z
kh k a z
∞
∞
−
−
−−
=
=−∞
=
=−∞
=
−


 
(6.117)
which, after replacing l
n
k
=
−
, and equating terms in
1
z−, results in
1
1
1
ˆ
ˆ
[ ]
[ ]
0
ˆ
ˆ
0
[ ]
[ ]
n
n
n k
k
n
n k
k n
p
na
nh n
kh k a
n
p
nh n
kh k a
n
p
−
−
=
−
−
= −
=
−
<
≤
=
−
>


(6.118)
so that the complex cepstrum can be obtained from the LPC coefficients by the following
recursion:
1
1
1
0
0
ln
0
ˆ
ˆ
[ ]
0
[ ]
ˆ[ ]
n
n
n k
k
n
n k
k n
p
n
G
n
k
a
h k a
n
p
h n
n
k h k a
n
p
n
−
−
=
−
−
= −
<


=





+
<
≤
= 



	




>




	



(6.119)
where the value for
0
n =
can be obtained from Eqs. (6.115) and (6.111). We note that,
while there are a finite number of LPC coefficients, the number of cepstrum coefficients is
infinite. Speech recognition researchers have shown empirically that a finite number is suffi-
cient: 12 - 20 depending on the sampling rate and whether or not frequency warping is done.
In Chapter 8 we discuss the use of the cepstrum in speech recognition.
This recursion should not be used in the reverse mode to compute the LPC coefficients
from any set of cepstrum coefficients, because the recursion in Eq. (6.119) assumes an all-
pole model with all poles inside the unit circle, and that might not be the case for an arbi-
trary cepstrum sequence, so that the recursion might yield a set of unstable LPC coefficients.
In some experiments it has been shown that quantized LPC-cepstrum can yield unstable
LPC coefficients over 5% of the time.

Cepstral Processing
309
6.4.3.
Cepstrum of Periodic Signals
It is important to see what the cepstrum of periodic signals looks like. To do so, let’s con-
sider the following signal:
1
0
[ ]
[
]
M
k
k
x n
n
kN
α δ
−
=
=
−

(6.120)
which can be viewed as an impulse train of period N multiplied by an analysis window, so
that only M impulses remain. Its z-transform is
1
0
( )
M
kN
k
k
X z
z
α
−
−
=
= 
(6.121)
which is a polynomial in
N
z−
rather than
1
z−. Therefore,
( )
X z
can be expressed as a prod-
uct of factors of the form (1
)
Nk
ka z−
−
and (1
)
Nk
ku z
−
. Following the derivation in Section
6.4.2, it is clear that its complex cepstrum is nonzero only at integer multiples of N:
ˆ[ ]
[
]
k
k
x n
n
kN
β δ
∞
=−∞
=
−

(6.122)
A particularly interesting case is when
k
k
α
α
=
with 0
1
α
<
< , so that Eq. (6.121)
can be expressed as
1
1 (
)
( )
1
(
)
1
N
M
N
N
M
N
z
X z
z
z
z
α
α
α
α
−
−
−
−
−
−
= +
+
+
=
−

(6.123)
so that taking the logarithm of Eq. (6.123) and expanding it in Taylor series using Eq.
(6.111) results in
1
1
1
ˆ
ˆ
( )
ln
( )
[ ]
r
lM
rN
lMN
n
r
l
n
X z
X z
z
z
x n z
r
l
α
α
∞
∞
∞
−
−
−
=
=
=
=
=
−
=



(6.124)
which lets us compute the complex cepstrum as
1
1
ˆ[ ]
[
]
[
]
r
lM
r
l
x n
n
rN
n
lMN
r
l
α
α
δ
δ
∞
∞
=
=
=
−
−
−


(6.125)
An infinite impulse train can be obtained by making
1
α →
and M →∞in Eq.
(6.125):
1
[
]
ˆ[ ]
r
n
rN
x n
r
δ
∞
=
−
= 
(6.126)
We see from Eq. (6.126) that the cepstrum of an impulse train goes to 0 as n increases.
This justifies our assumption of homomorphic filtering.

310
Speech Signal Representations
6.4.4.
Cepstrum of Speech Signals
We can compute the cepstrum of a speech segment by windowing the signal with a window
of length N. In practice, the cepstrum is not computed through Eq. (6.112), since root-
finding algorithms are slow and offer numerical imprecision for the large values of N used.
Instead, we can compute the cepstrum directly through its definition of Eq. (6.105), using
the DFT as follows:
1
2
/
0
[ ]
[ ]
N
j
nk N
a
n
X
k
x n e
π
−
−
=
= 
,
0
k
N
≤
<
(6.127)
ˆ [ ]
ln
[ ]
a
a
X
k
X
k
=
,
0
k
N
≤
<
(6.128)
1
2
/
0
1
ˆ
ˆ [ ]
[ ]
N
j
nk N
a
a
n
x n
X
k e
N
π
−
−
=
=

,
0
n
N
≤
<
(6.129)
The subscript a means that the new complex cepstrum ˆ [ ]
ax n is an aliased version of ˆ[ ]
x n
given by
ˆ
ˆ
[ ]
[
]
a
r
x n
x n
rN
∞
=−∞
=
+

(6.130)
which can be derived by using the sampling theorem of Chapter 5, by reversing the concepts
of time and frequency.
This aliasing introduces errors in the estimation that can be reduced by choosing a
large value for N.
Computation of the complex cepstrum requires computing the complex logarithm and,
in turn, the phase. However, given the principal value of the phase
[ ]
p k
θ
, there are infinite
possible values for
[ ]
k
θ
:
[ ]
[ ]
2
p
k
k
k
n
θ
θ
π
=
+
(6.131)
From Chapter 5 we know that if
[ ]
x n is real, arg
(
)
j
X e ω



 is an odd function and also con-
tinuous. Thus we can do phase unwrapping by choosing
kn
to guarantee that
[ ]
k
θ
is a
smooth function, i.e., by forcing the difference between adjacent values to be small:
[ ]
[
1]
k
k
θ
θ
π
−
−
<
(6.132)
A linear phase term r as in Eq. (6.110), would contribute to the phase difference in Eq.
(6.132) with 2
/
r N
π
, which may result in errors in the phase unwrapping if
[ ]
k
θ
is chang-
ing sufficiently rapidly. In addition, there could be large changes in the phase difference if
[ ]
a
X
k
is noisy. To guarantee that we can track small phase differences, a value of N several

Cepstral Processing
311
times larger than the window size is required: i.e., the input signal has to be zero-padded
prior to the FFT computation. Finally, the delay r in Eq. (6.109), can be obtained by forcing
the phase to be an odd function, so that:
[
/ 2]
N
r
θ
π
=
(6.133)
For unvoiced speech, the unwrapped phase is random, and therefore only the real cep-
strum has meaning. In practical situations, even voiced speech has some frequencies at
which noise dominates (typically very low and high frequencies), which results in phase
[ ]
k
θ
that changes drastically from frame to frame. Because of this, the complex cepstrum in
Eq. (6.105) is rarely used for real speech signals. Instead, the real cepstrum is used much
more often:
[ ]
ln
[ ]
a
a
C k
X
k
=
,
0
k
N
≤
<
(6.134)
1
2
/
0
1
[ ]
[ ]
N
j
nk N
a
a
n
c n
C k e
N
π
−
−
=
=

,
0
n
N
≤
<
(6.135)
Similarly, it can be shown that for the new real cepstrum
[ ]
ac n
is an aliased version of
[ ]
c n given by
[ ]
[
]
a
r
c n
c n
rN
∞
=−∞
=
+

(6.136)
which again has aliasing that can be reduced by choosing a large value for N.
6.4.5.
Source-Filter Separation via the Cepstrum
We have seen that, if the filter is a rational transfer function, and the source is an impulse
train, the homomorphic filtering of Figure 6.24 can approximately separate them. Because of
problems in estimating the phase in speech signals (see Section 6.4.4), we generally com-
pute the real cepstrum using Eqs. (6.127), (6.134) and (6.135), and then compute the com-
plex cepstrum under the assumption of a minimum phase signal according to Eq. (6.113).
The result of separating source and filter using this cepstral deconvolution is shown in
Figure 6.25 for voiced speech and Figure 6.26 for unvoiced speech.
The real cepstrum of white noise
[ ]
x n
with an expected magnitude spectrum
|
(
) | 1
j
X e ω
=
is 0. If colored noise is present, the cepstrum of the observed colored noise
ˆ[ ]
y n
is identical to the cepstrum of the coloring filter ˆ[ ]
h n , except for a gain factor. The
above is correct if we take an infinite number of noise samples, but in practice, this cannot
be done and a limited number have to be used, so that this is only an approximation, though
it is often used in speech processing algorithms.

312
Speech Signal Representations
0
50
100
150
200
-2
0
2
(a)
0
1000
2000
3000
4000
-5
0
5
dB
(b)
0
50
100
150
200
-2
0
2
(c)
0
1000
2000
3000
4000
-5
0
5
dB
(d)
0
50
100
150
200
-0.5
0
0.5
(e)
time
0
1000
2000
3000
4000
-5
0
5
dB
(f)
Frequency (Hz)
Figure 6.25 Separation of source and filter using homomorphic filtering for voiced speech
with the scheme of Figure 6.24 with N = 20 in the homomorphic filter of Eq. (6.102) with the
real cepstrum: (a) windowed signal, (b) log-spectrum, (c) filter’s impulse response, (d)
smoothed log-spectrum, (e) windowed excitation signal, (f) log-spectrum of high-part of cep-
strum. Note that the windowed excitation is not a windowed impulse train because of the
minimum phase assumption.
0
50
100
150
200
-2
-1
0
1
2
(a)
0
1000
2000
3000
4000
-5
0
5
dB
(b)
0
50
100
150
200
-2
-1
0
1
2
(c)
tim e
0
1000
2000
3000
4000
-5
0
5
Frequency (Hz)
dB
(d)
Figure 6.26 Separation of source and filter using homomorphic filtering for unvoiced speech
with the scheme of Figure 6.24 with N = 20 in the homomorphic filter of Eq. (6.102) with the
real cepstrum: (a) windowed signal, (b) log-spectrum, (c) filter’s impulse response, (d)
smoothed log-spectrum.

Perceptually-Motivated Representations
313
6.5.
PERCEPTUALLY-MOTIVATED REPRESENTATIONS
In this section we describe some aspects of human perception, and methods motivated by the
behavior of the human auditory system: Mel-Frequency Cepstrum Coefficients (MFCC) and
Perceptual Linear Prediction (PLP). These methods have been successfully used in speech
recognition. First we present several nonlinear frequency scales that have been used in such
representations.
6.5.1.
The Bilinear Transform
The transformation
1
1
1
z
s
z
α
α
−
−
−
=
−
(6.137)
for 0
1
α
<
<
belongs to the class of bilinear transforms. It is a mapping in the complex
plane that maps the unit circle onto itself. The frequency transformation is obtained by mak-
ing the substitution
j
z
e ω
=
and
j
s
e Ω
=
:
sin( )
2arctan 1
cos( )
α
ω
ω
α
ω


Ω=
+


−


(6.138)
This transformation is very similar to the Bark and mel scale for an appropriate choice
of the parameter α (see Chapter 2). Oppenheim [31] showed that the advantage of this
transformation is that it can be used to transform a time sequence in the linear frequency into
another time sequence in the warped frequency, as shown in Figure 6.27. This bilinear trans-
form has been successfully applied to cepstral and autocorrelation coefficients.
Figure 6.27 Implementation of the frequency-warped cepstral coefficients as a function of the
linear-frequency cepstrum coefficients. Both sets of coefficients are causal. The input is the
time-reversed cepstrum sequence, and the output can be obtained by sampling the outputs of
the filters at time n = 0. The filters used for w[m] m > 2 are the same. Note that, for a finite-
length cepstrum, an infinite-length warped cepstrum results.
1
1
1


z
(
)
1
1
2
1
1






z
z
z
z




1
1
1


z
z




1
1
1


c[-n]
n=0
n=0
n=0
n=0
w[0]
w[1]
w[2]
w[3]

314
Speech Signal Representations
For a finite number of cepstral coefficients the bilinear transform in Figure 6.27 results
in an infinite number of warped cepstral coefficients. Since truncation is usually done in
practice, the bilinear transform is equivalent to a matrix multiplication, where the matrix is a
function of the warping parameter α . Shikano [43] showed these warped cepstral coeffi-
cients were beneficial for speech recognition.
6.5.2.
Mel-Frequency Cepstrum
The Mel-Frequency Cepstrum Coefficients (MFCC) is a representation defined as the real
cepstrum of a windowed short-time signal derived from the FFT of that signal. The differ-
ence from the real cepstrum is that a nonlinear frequency scale is used, which approximates
the behavior of the auditory system. Davis and Mermelstein [8] showed the MFCC represen-
tation to be beneficial for speech recognition.
Given the DFT of the input signal
1
2
/
0
[ ]
[ ]
N
j
nk N
a
n
X
k
x n e
π
−
−
=
= 
,
0
k
N
≤
<
(6.139)
we define a filterbank with M filters (
1,2,
,
m
M
=

), where filter m is triangular filter given
by:
(
)
(
)(
)
(
)
(
)(
)
0
[
1]
2
[
1]
[
1]
[ ]
[
1]
[
1]
[ ]
[
1]
[ ]
2
[
1]
[ ]
[
1]
[
1]
[
1]
[
1]
[ ]
0
[
1]
m
k
f m
k
f m
f m
k
f m
f m
f m
f m
f m
H
k
f m
k
f m
k
f m
f m
f m
f m
f m
k
f m
<
−


−
−

−
≤
≤

+
−
−
−
−

= 
+
−

≤
≤
+

+
−
−
+
−


>
+

(6.140)
Such filters compute the average spectrum around each center frequency with increasing
bandwidths, and they are displayed in Figure 6.28.
Figure 6.28 Triangular filters used in the computation of the mel-cepstrum using Eq. (6.140).
Alternatively, the filters can be chosen as
…
1[ ]
H k
3[ ]
H k
4[ ]
H
k
5[ ]
H k
6[ ]
H k
2[ ]
H
k
f[0] f[1] f[2]
f[3]
f[4]
f[5]
f[6]
f[7]
k

Perceptually-Motivated Representations
315
(
)
(
)
(
)
(
)
'
0
[
1]
[
1]
[
1]
[ ]
[ ]
[
1]
[ ]
[
1]
[ ]
[
1]
[
1]
[ ]
0
[
1]
m
k
f m
k
f m
f m
k
f m
f m
f m
H
k
f m
k
f m
k
f m
f m
f m
k
f m
<
−


−
−

−
≤
≤

−
−

= 
+
−

≤
≤
+

+
−


>
+

(6.141)
which satisfies
1
'
0
[ ]
1
M
m
m
H
k
−
=
=

. The mel-cepstrum computed with
[ ]
m
H
k
or
' [ ]
m
H
k
will dif-
fer by a constant vector for all inputs, so the choice becomes unimportant when used in a
speech recognition system that has trained with the same filters.
Let’s define
lf
and
hf
to be the lowest and highest frequencies of the filterbank in
Hz, Fs the sampling frequency in Hz, M the number of filters, and N the size of the FFT. The
boundary points f[m] are uniformly spaced in the mel-scale:
1
(
)
(
)
[ ]
(
)
1
h
l
l
s
B f
B f
N
f m
B
B f
m
F
M
−


−


=
+




+




(6.142)
where the mel-scale B is given by Eq. (2.6), and B-1 is its inverse
(
)
1( )
700 exp( /1125) 1
B
b
b
−
=
−
(6.143)
We then compute the log-energy at the output of each filter as
1
2
0
[ ]
ln
[ ]
[ ]
N
a
m
k
S m
X
k
H
k
−
=


=





,
0
m
M
≤
<
(6.144)
The mel frequency cepstrum is then the discrete cosine transform of the M filter out-
puts:
(
)
1
0
[ ]
[ ]cos
(
1/ 2)/
M
m
c n
S m
n m
M
π
−
=
=
+

0
n
M
≤
<
(6.145)
where M varies for different implementations from 24 to 40. For speech recognition, typi-
cally only the first 13 cepstrum coefficients are used. It is important to note that the MFCC
representation is no longer a homomorphic transformation. It would be if the order of sum-
mation and logarithms in Eq. (6.144) were reversed:
(
)
1
2
0
[ ]
ln
[ ]
[ ]
N
a
m
k
S m
X
k
H
k
−
=
= 
0
m
M
≤
<
(6.146)
In practice, however, the MFCC representation is approximately homomorphic for fil-
ters that have a smooth transfer function. The advantage of the MFCC representation using

316
Speech Signal Representations
(6.144) instead of (6.146) is that the filter energies are more robust to noise and spectral es-
timation errors. This algorithm has been used extensively as a feature vector for speech rec-
ognition systems.
While the definition of cepstrum in Section 6.4.1 uses an inverse DFT, since S[m] is
even, a DCT-II can be used instead (see Chapter 5).
6.5.3.
Perceptual Linear Prediction (PLP)
Perceptual Linear Prediction (PLP) [16] uses the standard Durbin recursion of Section
6.3.2.2 to compute LPC coefficients, and typically the LPC coefficients are transformed to
LPC-cepstrum using the recursion in Section 6.4.2.1. But unlike standard linear prediction,
the autocorrelation coefficients are not computed in the time domain through Eq. (6.55).
The autocorrelation
[ ]
x
R n
is the inverse Fourier transform of the power spectrum
2
( )
X ω
of the signal. We cannot compute the continuous-frequency Fourier transform eas-
ily, but we can take an FFT to compute X[k], so that the autocorrelation can be obtained as
the inverse Fourier transform of
2
[ ]
X k
. Since the discrete Fourier transform is not per-
forming linear convolution but circular convolution, we need to make sure that the FFT size
is larger than twice the window length (see Section 5.3.4) for this to hold. This alternate way
of computing autocorrelation coefficients, entailing two FFTs and N multiplies and adds,
should yield identical results. Since normally only a small number p of autocorrelation coef-
ficients are needed, this is generally not a cost-effective way to do it, unless the first FFT has
to be computed for other reasons.
Perceptual linear prediction uses the above method, but replaces
2
[ ]
X k
by a percep-
tually motivated power spectrum. The most important aspect is the non-linear frequency
scaling, which can be achieved through a set of filterbanks similar to those described in Sec-
tion 6.5.2, so that this critical-band power spectrum can be sampled in approximately 1-bark
intervals. Another difference is that, instead of taking the logarithm on the filterbank energy
outputs, a different non-linearity compression is used, often the cubic root. It is reported [16]
that the use of this different non-linearity is beneficial for speech recognizers in noisy condi-
tions.
6.6.
FORMANT FREQUENCIES
Formant frequencies are the resonances in the vocal tract and, as we saw in Chapter 2, they
convey the differences between different sounds. Expert spectrogram readers are able to
recognize speech by looking at a spectrogram, particularly at the formants. It has been ar-
gued that they are very useful features for speech recognition, but they haven’t been widely
used because of the difficulty in estimating them.
One way of obtaining formant candidates at a frame level is to compute the roots of a
pth-order LPC polynomial [3, 26]. There are standard algorithms to compute the complex

Formant Frequencies
317
roots of a polynomial with real coefficients [36], though convergence is not guaranteed.
Each complex root zi can be represented as
z
b
j
f
i
i
i



exp(
)


2
(6.147)
where fi and bi are the formant frequency and bandwidth, respectively, of the ith root. Real
roots are discarded and complex roots are sorted by increasing f, discarding negative values.
The remaining pairs ( fi,bi) are the formant candidates. Traditional formant trackers discard
roots whose bandwidths are higher than a threshold [46], say 200 Hz.
Closed-phase analysis of voiced speech [5] uses only the regions for which the glottis
is closed and thus there is no excitation. When the glottis is open, there is a coupling of the
vocal tract with the lungs and the resonance bandwidths are somewhat larger. Determination
of the closed-phase regions directly from the speech signal is difficult, so often an elec-
troglottograph (EGG) signal is used [23]. EGG signals, obtained by placing electrodes at the
speaker’s throat, are very accurate in determining the times when the glottis is closed. Using
samples in the closed-phase covariance analysis can yield accurate results [46]. For female
speech, the closed-phase is short, and sometimes non-existent, so such analysis can be a
challenge. EGG signals are useful also for pitch tracking and are described in more detail in
Chapter 16.
Another common method consists of finding the peaks on a smoothed spectrum, such
as that obtained through an LPC analysis [26, 40]. The advantage of this method is that you
can always compute the peaks and it is more computationally efficient than extracting the
complex roots of a polynomial. On the other hand, this procedure generally doesn’t estimate
the formant’s bandwidth. The first three formants are typically estimated this way for for-
mant synthesis (see Chapter 16), since they are the ones that allow sound classification,
whereas the higher formants are more speaker dependent.
Sometimes, the signal goes through some conditioning, which includes sampling rate
conversion to remove frequencies outside the range we are interested in. For example, if we
are interested only in the first three formants, we can safely downsample the input signal to
8 kHz, since we know all three formants should be below 4 kHz. This downsampling re-
duces computation and the chances of the algorithm to find formant values outside the ex-
pected range (otherwise peaks or roots could be chosen above 4 kHz which we know do not
correspond to any of the first three formants). Pre-emphasis filtering is also often used to
whiten the signal.
Because of the thresholds imposed above, it is possible that the formants are not con-
tinuous. For example, when the vocal tract’s spectral envelope is changing rapidly, band-
widths obtained through the above methods are overestimates of the true bandwidths, and
they may exceed the threshold and thus be rejected. It is also possible for the peak-picking
algorithm to classify a harmonic as a formant during some regions where it is much stronger
than the other harmonics. Due to the thresholds used, a given frame could have no formants,
only one formant (either first, second, or third), two, three, or more. Formant alignment from
one frame to another has often been done using heuristics to prevent such discontinuities.

318
Speech Signal Representations
6.6.1.
Statistical Formant Tracking
It is desirable to have an approach that does not use any thresholds on formant candidates
and uses a probabilistic model to do the tracking instead of heuristics [1]. The formant can-
didates can be obtained from roots of the LPC polynomial, peaks in the smoothed spectrum
or even from a dense sample of possible points. If the first n formants are desired, and we
have (p/2) formant candidates, a maximum of r n-tuples are considered, where r is given by
r
p
n
FHG
IKJ
/ 2
(6.148)
A Viterbi search (see Chapter 8) is then carried out to find the most likely path of for-
mant n-tuples given a model with some a priori knowledge of formants. The prior distribu-
tion for formant targets is used to determine which formant candidate to use of all possible
choices for the given phoneme (i.e., we know that F1 for an AE should be around 800 Hz).
Formant continuity is imposed through the prior distribution of the formant slopes. This al-
gorithm produces n formants for every frame, including silence.
Since we are interested in obtaining the first three formants (n=3) and F3 is known to
be lower than 4 kHz, it is advantageous to downsample the signal to 8 kHz in order to avoid
obtaining formant candidates above 4 kHz and to let us use a lower-order analysis which
offers fewer numerical problems when computing the roots. With p = 14, it results in a
maximum of r = 35 triplets for the case of no real roots.
Let X be a sequence of T feature vectors
tx of dimension n:
X
x x
x


(
,
,
,
)
1
2 
T
(6.149)
where the prime denotes transpose.
We estimate the formants with the knowledge of what sound occurs at that particular
time, for example by using a speech recognizer that segments the waveform into different
phonemes (see Chapter 9) or states
tq within a phoneme. In this case we assume that the
output distribution of each state i is modeled by one Gaussian density function with a mean
i and covariance matrix i . We can define up to N states, with λ being the set of all means
and covariance matrices for all:




 (
,
,
,
,
,
,
)
1
1
2
2




N
N
(6.150)
Therefore, the log-likelihood for X is given by
(
)
1
1
1
1
1
ˆ
ln
(
| , )
ln 2
ln |
|
(
)
(
)
2
2
2
t
t
t
t
T
T
q
t
q
q
t
q
t
t
TM
p
λ
π
µ
µ
−
=
=
′
= −
−
Σ
−
−
Σ
−


X q
x
x
(6.151)
Maximizing X in Eq. (6.151) leads to the trivial solution

(
,
,
,
)
X 




q
q
qT
1
2 
, a
piecewise function whose value is that of the best n-tuple candidate. This function has dis-
continuities at state boundaries and thus is not likely to represent well the physical phenom-
ena of speech.

Formant Frequencies
319
This problem arises because the slopes at state boundaries do not match the slopes of
natural speech. To avoid these discontinuities, we would like to match not only the target
formants at each state, but also the formant slopes at each state. To do that, we augment the
feature vector xt at frame t with the delta vector x
x
t
t

1. Thus, we increase the parameter
space of λ with the corresponding means  i and covariance matrices i of these delta pa-
rameters, and assume statistical independence among them. The corresponding new log-
likelihood has the form
1
2
1
1
1
1
1
2
1
1
ˆ
ln
(
| , )
ln |
|
ln |
|
2
2
1
1
(
)
(
)
(
)
(
)
2
2
t
t
t
t
t
t
t
t
T
T
q
q
t
t
T
T
t
q
q
t
q
t
t
q
q
t
t
q
t
t
p
K
λ
µ
µ
δ
δ
=
=
−
−
−
−
=
=
=
−
Σ
−
Γ
′
′
−
−
Σ
−
−
−
−
Γ
−
−




X q
x
x
x
x
x
x
(6.152)
Time (seconds)
Frequency (Hz)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
0
500
1000
1500
2000
2500
3000
3500
4000
Figure 6.29 Spectrogram and three smoothed formants.
Maximization of Eq. (6.152) with respect to xt requires solving several sets of linear
equations. If i and i are diagonal covariance matrices, it results in a set of linear equations
for each of the M dimensions
BX
c

(6.153)
where B is a tridiagonal matrix (all values are zero except for those in the main diagonal and
its two adjacent diagonals), which leads to a very efficient solution [36]. For example, the
values of B and c for T = 3 are given by

320
Speech Signal Representations
B 








F
H
GGGGGGG
I
K
JJJJJJJ
1
1
1
0
1
1
1
1
1
0
1
1
1
1
2
2
2
2
2
3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2











q
q
q
q
q
q
q
q
q
q
q
(6.154)
c 




F
HG
I
KJ















q
q
q
q
q
q
q
q
q
q
q
q
q
q
1
1
2
2
2
2
2
2
3
3
3
3
3
3
2
2
2
2
2
2
2
(6.155)
where just one dimension is represented, and the process is repeated for all dimensions with
a computational complexity of O(TM).
0
500
1000
1500
2000
2500
3000
3500
4000
0.7
0.9
1.1
1.3
1.5
1.7
1.9
2.1
2.3
Time (seconds)
Frequency (Hz)
Figure 6.30 Raw formants (ragged gray line) and smoothed formants (dashed line).
The maximum likelihood sequence xt is close to the targets i while keeping the
slopes close to  i for a given state i, thus estimating a continuous function. Because of the
delta coefficients, the solution depends on all the parameters of all states and not just the
current state. This procedure can be performed for the formants as well as the bandwidths.

The Role of Pitch
321
The parameters i , i ,  i , and i can be re-estimated using the EM algorithm de-
scribed in Chapter 8. In [1] it is reported that two or three iterations are sufficient for
speaker-dependent data.
The formant track obtained through this method can be rough, and it may be desired to
smooth it. Smoothing without knowledge about the speech signal would result in either blur-
ring the sharp transitions that occur in natural speech, or maintaining ragged formant tracks
where the underlying physical phenomena vary slowly with time. Ideally we would like a
larger adjustment to the raw formant when the error in the estimate is large relative to the
variance of the corresponding state within a phoneme. This can be done by modeling the
formant measurement error as a Gaussian distribution. Figure 6.29 shows an utterance from
a male speaker with the smoothed formant tracks, and Figure 6.30 compares the raw and
smoothed formants. When no real formant is visible from the spectrogram, the algorithm
tends to assign a large bandwidth (not shown in the figure).
6.7.
THE ROLE OF PITCH
Pitch determination is very important for many speech processing algorithms. The concate-
native speech synthesis methods of Chapter 16 require pitch tracking on the desired speech
segments if prosody modification is to be done. Chinese speech recognition systems use
pitch tracking for tone recognition, which is important in disambiguating the myriad of
homophones. Pitch is also crucial for prosodic variation in text-to-speech systems (see
Chapter 15) and spoken language systems (see Chapter 17). While in the previous sections
we have dealt with features representing the filter, pitch represents the source of the model
illustrated in Figure 6.1.
Pitch determination algorithms also use short-term analysis techniques, which means
that for every frame
m
x
we get a score
(
|
)
m
f T x
that is a function of the candidate pitch
periods T. These algorithms determine the optimal pitch by maximizing
arg max
(
|
)
m
m
T
T
f T
=
x
(6.156)
We describe several such functions computed through the autocorrelation method and
the normalized cross-correlation method, as well as the signal conditioning that is often per-
formed. Other approaches based on cepstrum [28] have also been used successfully. a good
summary of techniques used for pitch tracking is provided by [17, 45].
Pitch determination using Eq. (6.156) is error prone, and a smoothing stage is often
done. This smoothing, described in Section 6.7.4, takes into consideration that the pitch does
not change quickly over time.
6.7.1.
Autocorrelation Method
A commonly used method to estimate pitch is based on detecting the highest value of the
autocorrelation function in the region of interest. This region must exclude
0
m =
, as that is

322
Speech Signal Representations
the absolute maximum of the autocorrelation function [37]. As discussed in Chapter 5, the
statistical autocorrelation of a sinusoidal random process
0
[ ]
cos(
)
n
n
ω
ϕ
=
+
x
(6.157)
is given by
0
1
[ ]
{
[ ] [
]}
cos(
)
2
R m
E
n
n
m
m
ω
∗
=
+
=
x
x
(6.158)
which has maxima for
0
m
lT
=
, the pitch period and its harmonics, so that we can find the
pitch period by computing the highest value of the autocorrelation. Similarly, it can be
shown that any WSS periodic process x[n] with period
0T also has an autocorrelation R[m]
which exhibits its maxima at
0
m
lT
=
.
In practice, we need to obtain an estimate ˆ[ ]
R m from knowledge of only N samples. If
we use a window w[n] of length N on x[n] and assume it to be real, the empirical autocorre-
lation function is given by
1 | |
0
1
ˆ[ ]
[ ] [ ] [
] [
]
N
m
n
R m
w n
n w n
m
n
m
N
−−
=
=
+
+

x
x
(6.159)
whose expected value can be shown to be
{
}
(
)
ˆ[ ]
[ ]
[ ]
[
]
E R m
R m
w m
w
m
=
∗
−
(6.160)
where
| | 1
0
[ ]
[
]
[ ] [
|
|]
N
m
n
w m
w
m
w n w n
m
−
−
=
∗
−
=
+

(6.161)
which, for the case of a rectangular window of length N, is given by
|
|
1
[ ]
[
]
0
m
m
N
w m
w
m
N
m
N
 −
<

∗
−
= 

≥

(6.162)
which means that ˆ[ ]
R m is a biased estimator of R[m]. So, if we compute the peaks based on
Eq. (6.159), the estimate of the pitch will also be biased. Although the variance of the esti-
mate is difficult to compute, it is easy to see that as m approaches N, fewer and fewer sam-
ples of x[n] are involved in the calculation, and thus the variance of the estimate is expected
to increase. If we multiply Eq. (6.159) by
/(
)
N
N
m
−
, the estimate will be unbiased but the
variance will be larger.
Using the empirical autocorrelation in Eq. (6.159) for the random process in Eq.
(6.157) results in an expected value of

The Role of Pitch
323
{
}
0
cos(
)
|
|
ˆ[ ]
1
,
2
m
m
E R m
m
N
N
ω


=
−
<




(6.163)
whose maximum coincides with the pitch period for
0
m
m
>
.
Since pitch periods can be as low as 40 Hz (for a very low-pitched male voice) or as
high as 600 Hz (for a very high-pitched female or child’s voice), the search for the maxi-
mum is conducted within a region. This F0 detection algorithm is illustrated in Figure 6.31
where the lag with highest autocorrelation is plotted for every frame. In order to see perio-
dicity present in the autocorrelation, we need to use a window that contains at least two pitch
periods, which, if we want to detect a 40Hz pitch, implies 50ms (see Figure 6.32). For win-
dow lengths so long, the assumption of stationarity starts to fail, because a pitch period at
the beginning of the window can be significantly different than at the end of the window.
One possible solution to this problem is to estimate the autocorrelation function with differ-
ent window lengths for different lags m.
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
-0.5
0
0.5
10
20
30
40
50
60
0
50
100
Figure 6.31 Waveform and unsmoothed pitch track with the autocorrelation method. A frame
shift of 10 ms, a Hamming window of 30 ms, and a sampling rate of 8kHz were used. Notice
that two frames in the voiced region have an incorrect pitch. The pitch values in the unvoiced
regions are essentially random.
The candidate pitch periods in Eq. (6.156) can be simply
m
T
m
=
; i.e., the pitch period
is any integer number of samples. For low values of
m
T , the frequency resolution is lower
than for high values. To maintain a relatively constant frequency resolution, we do not have
to search all the pitch periods for large
m
T . Alternatively, if the sampling frequency is not
high, we may need to use fractional pitch periods (often done in the speech coding algo-
rithms of Chapter 7)
The autocorrelation function can be efficiently computed by taking a signal, window-
ing it, and taking an FFT and then the square of the magnitude.

324
Speech Signal Representations
50
100
150
200
250
300
-1
0
1
50
100
150
200
250
300
-1
0
1
Figure 6.32 Autocorrelation function for frame 40 in Figure 6.31. The maximum occurs at 89
samples. A sampling frequency of 8 kHz and window shift of 10ms are used. The top figure is
using a window length of 30 ms, whereas the bottom one is using 50 ms. Notice the quasi-
periodicity in the autocorrelation function.
6.7.2.
Normalized Cross-Correlation Method
A method that is free from these border problems and has been gaining in popularity is
based on the normalized cross-correlation [2]
,
( )
cos( )
t
t T
t
t
t T
T
α
θ
−
−
<
>
=
=
x x
x
x
(6.164)
where
{ [
/ 2], [
/ 2 1],
, [
/ 2 1]}
t
x t
N
x t
N
x t
N
=
−
−
+
+
−
x

is a vector of N samples centered
at time t, and
,
t
t T
−
<
>
x x
is the inner product between the two vectors defined as
/ 2 1
/ 2
,
[
] [
]
N
n
l
m
N
x n
m y l
m
−
=−
<
>
+
+

x
y
(6.165)
so that, using Eq. (6.165), the normalized cross-correlation can be expressed as
/ 2 1
/ 2
/ 2 1
/ 2 1
2
2
/ 2
/ 2
[
] [
]
( )
[
]
[
]
N
n
N
t
N
N
n
N
m
N
x t
n x t
n
T
T
x t
n
x t
m
T
α
−
=−
−
−
=−
=−
+
+
−
=
+
+
+



(6.166)

The Role of Pitch
325
where we see that the numerator in Eq. (6.166) is very similar to the autocorrelation in Sec-
tion 6.7.1, but where N terms are used in the addition for all values of T.
The maximum of the normalized cross-correlation method is shown in Figure 6.33 (b).
Unlike the autocorrelation method, the estimate of the normalized cross-correlation is not
biased by the term (1
/
)
m N
−
. For perfectly periodic signals, this results in identical values
of the normalized cross-correlation function for kT. This can result in pitch halving, where
2T can be chosen as the pitch period, which happens in Figure 6.33 (b) at the beginning of
the utterance. Using a decaying bias (1
/
)
m M
−
with M
N

, can be useful in reducing
pitch halving, as we see in Figure 6.33 (c).
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
-0.5
0
0.5
(a)
10
20
30
40
50
60
0
100
200
(b)
10
20
30
40
50
60
0
100
200
(c)
Figure 6.33 (a) Waveform and (b) (c) unsmoothed pitch tracks with the normalized cross-
correlation method. A frame shift of 10 ms, window length of 10 ms, and sampling rate of 8
kHz were used. (b) is the standard normalized cross-correlation method, whereas (c) has a de-
caying term. If we compare it to the autocorrelation method of Figure 6.31, the middle voiced
region is correctly identified in both (b) and (c), but two frames at the beginning of (b) that
have pitch halving are eliminated with the decaying term. Again, the pitch values in the un-
voiced regions are essentially random.
Because the number of samples involved in the calculation is constant, this estimate is
unbiased and has lower variance than that of the autocorrelation. Unlike the autocorrelation
method, the window length could be lower than the pitch period, so that the assumption of
stationarity is more accurate and it has more time resolution. While pitch trackers based on
the normalized cross-correlation typically perform better than those based on the autocorre-
lation, they also require more computation, since all the autocorrelation lags can be effi-
ciently computed through 2 FFTs and N multiplies and adds (see Section 5.3.4).
Let’s gain some insight about the normalized cross-correlation. If x[n] is periodic with
period T, then we can predict it from a vector T samples in the past as:

326
Speech Signal Representations
t
t T
t
ρ
−
=
+
x
x
e
(6.167)
where ρ is the prediction gain. The normalized cross-correlation measures the angle between
the two vectors, as can be seen in Figure 6.34, and since it is a cosine, it has the property that
1
( )
1
n P
α
−≤
≤.
Figure 6.34 The prediction of
tx
with
t T
−
x
results in an error
te .
If we choose the value of the prediction gain ρ so as to minimize the prediction error
2
2
2
2
2
2
2
cos ( )
( )
t
t
t
t
t
t T
θ
α
=
−
=
−
e
x
x
x
x
(6.168)
and assume
te is a zero-mean Gaussian random vector with a standard deviation
|
|
t
σ x
,
then
2
2
( )
ln
(
|
)
2
t
t
T
f
T
K
α
σ
=
+
x
(6.169)
so that the maximum likelihood estimate corresponds to finding the value T with highest
normalized cross-correlation. Using Eq. (6.166), it is possible that
( )
0
t T
α
<
. In this case,
there is negative correlation between
tx
and
t T
−
x
, and it is unlikely that T is a good choice
for pitch. Thus, we need to force
0
ρ >
, so that Eq. (6.169) is converted into
(
)
2
2
max(0,
( ))
ln
(
|
)
2
t
t
T
f
T
K
α
σ
=
+
x
(6.170)
The normalized cross-correlation of Eq. (6.164) predicts the current frame with a
frame that occurs T samples before. Voiced speech may exhibit low correlation with a pre-
vious frame at a spectral discontinuity, such as those appearing at stops. To account for this,
an enhancement can be done to consider not only the backward normalized cross-
correlation, but also the forward normalized cross-correlation, by looking at a frame that
occurs T samples ahead of the current frame, and taking the highest of both.
(
)
2
2
max(0,
( ),
(
))
ln
(
|
)
2
t
t
t
T
T
f
T
K
α
α
σ
−
=
+
x
(6.171)
xt-T
et
xt
ρxt-T
θ

The Role of Pitch
327
6.7.3.
Signal Conditioning
Noise in the signal tends to make pitch estimation less accurate. To reduce this effect, signal
conditioning or pre-processing has been proposed prior to pitch estimation [44]. Typically
this involves bandpass filtering to remove frequencies above 1 or 2 kHz, and below 100 Hz
or so. High frequencies do not have much voicing information and have significant noise
energy, whereas low frequencies can have 50/60 Hz interference from power lines or non-
linearities from some A/D subsystems that can also mislead a pitch estimation algorithm.
In addition to the noise in the very low frequencies and aspiration at high bands, the
stationarity assumption is not so valid at high frequencies. Even a slowly changing pitch,
say, nominal 100 Hz increasing 5 Hz in 10 ms, results in a fast-changing harmonic: the 30th
harmonic at 3000 Hz changes 150 Hz in 10 ms. The corresponding short-time spectrum no
longer shows peaks at those frequencies.
Because of this, it is advantageous to filter out such frequencies prior to the computa-
tion of the autocorrelation or normalized cross-correlation. If an FFT is used to compute the
autocorrelation, this filter is easily done by setting to 0 the undesired frequency bins.
6.7.4.
Pitch Tracking
Pitch tracking using the above methods typically fails in several cases:
 Sub-harmonic errors. If a signal is periodic with period T, it is also periodic
with period 2T, 3T, etc. Thus, we expect the scores to be also high for the multi-
ples of T, which can mislead the algorithm. Because the signal is never perfectly
stationary, those multiples, or sub-harmonics, tend to have slightly lower scores
than the fundamental. If the pitch is identified as 2T, pitch halving is said to oc-
cur.
 Harmonic errors. If harmonic M dominates the signal’s total energy, the score at
pitch period T/M will be large. This can happen if the harmonic falls in a for-
mant frequency that boosts its amplitude considerably compared to that of the
other harmonics. If the pitch is identified as T/2, pitch doubling is said to occur.
 Noisy conditions. When the SNR is low, pitch estimates are quite unreliable for
most methods.
 Vocal fry. While pitch is generally continuous, for some speakers it can sud-
denly change and even halve, particularly at the end of an unstressed voiced re-
gion. The pitch here is really not well defined and imposing smoothness con-
straints can hurt the system.
 F0 jumps up or down by an octave occasionally.
 Breathy voiced speech is difficult to distinguish from periodic background noise.
 Narrow-band filtering of unvoiced excitations by certain vocal tract configura-
tions can lead to signals that appear periodic.

328
Speech Signal Representations
For these reasons, pitch trackers do not determine the pitch value at frame m based ex-
clusively on the signal at that frame. For a frame where there are several pitch candidates
with similar scores, the fact that pitch does not change abruptly with time is beneficial in
disambiguation, because possibly the following frame has a clearer pitch candidate, which
can help.
To integrate the normalized cross-correlation into a probabilistic framework, you can
combine
tracking
with
the
use
of
a
priori
information
[10].
Let’s
define
}
,
,
,
{
1
1
0
−
=
M
x
x
x
X

as a sequence of input vectors for M consecutive frames centered at
equally spaced time instants, say 10 ms. Furthermore, if we assume that the
ix
are inde-
pendent of each other, the joint distribution takes on the form:
1
0
(
|
)
(
|
)
M
i
i
i
f
f
T
−
=
=∏
X T
x
(6.172)
where
0
1
1
{
,
,
,
}
M
T T
T
−
=
T

is the pitch track for the input. The maximum a posteriori (MAP)
estimate of the pitch track is:
( ) (
|
)
max
(
|
)
max
max
( ) (
|
)
( )
MAP
f
f
f
f
f
f
=
=
=
T
T
T
T
X T
T
T X
T
X T
X
(6.173)
according to Bayes’ rule, with the term
(
|
)
f X T
being given by Eq. (6.172) and
(
|
)
i
i
f
T
x
by Eq. (6.169), for example.
The function
( )
f T constitutes the a priori statistics for the pitch and can help disam-
biguate the pitch, by avoiding pitch doubling or halving given knowledge of the speaker’s
average pitch, and by avoiding rapid transitions given a model of how pitch changes over
time. One possible approximation is given by assuming that the a priori probability of the
pitch period at frame i depends only on the pitch period for the previous frame:
0
1
1
1
2
2
3
1
0
0
( )
(
,
,
,
)
(
|
) (
|
)
(
|
) (
)
M
M
M
M
M
f
f T T
T
f T
T
f T
T
f T T
f T
−
−
−
−
−
=
=
T


(6.174)
One possible choice for
1
(
|
)
t
t
f T T −
is to decompose it into a component that depends
on
tT and another that depends on the difference
1
(
)
t
t
T
T −
−
. If we approximate both as
Gaussian densities, we obtain
(
)
(
)
2
2
1
1
2
2
ln
(
|
)
2
2
t
t
t
t
t
T
T
T
f T T
K
µ
δ
β
γ
−
−
−
−
−
′
=
−
−
(6.175)
so that when Eqs. (6.170) and (6.175) are combined, the log-probability of transitioning to
iT at time t from pitch
jT at time t - 1 is given by
(
)
(
)
(
)
2
2
2
2
2
2
max(0,
( ))
( ,
)
2
2
2
i
j
t
i
i
t
i
j
T
T
T
T
S T T
δ
α
µ
σ
β
γ
−
−
−
=
−
−
(6.176)

Historical Perspective And Future Reading
329
so that the log-likelihood in Eq. (6.173) can be expressed as
(
)
1
1
2
0
0
1
ln
( ) (
|
)
max(0,
(
))
max
(
,
)
t
t
t
M
t
i
i
i
t
f
f
T
S T T
α
−
−
=
=
+

T
X T
(6.177)
which can be maximized through dynamic programming. For a region where pitch is not
supposed to change,
0
δ =
, the term
2
(
)
i
j
T
T
−
in Eq. (6.176) acts as a penalty that keeps the
pitch track from jumping around. A mixture of Gaussians can be used instead to model dif-
ferent rates of pitch change, as in the case of Mandarin Chinese with four tones character-
ized by different slopes. The term (
)
2
iT
µ
−
attempts to get the pitch close to its expected
value to avoid pitch doubling or halving, with the average µ being different for male and
female speakers. Pruning can be done during the search without loss of accuracy (see Chap-
ter 12).
Pitch trackers also have to determine whether a region of speech is voiced or un-
voiced. A good approach is to build a statistical classifier with techniques described in
Chapter 8 based on energy and the normalized cross-correlation described above. Such clas-
sifiers, i.e., an HMM, penalize jumps between voiced and unvoiced frames to avoid voiced
regions having isolated unvoiced frame inside and vice versa. A threshold can be used on
the a posteriori probability to distinguish voiced from unvoiced frames.
6.8.
HISTORICAL PERSPECTIVE AND FUTURE READING
In 1978, Lawrence R. Rabiner and Ronald W. Schafer [38] wrote a book summarizing the
work to date on digital processing of speech, which remains a good source for the reader
interested in further reading in the field. The book by Deller, Hansen and Proakis [9] in-
cludes more recent work and is also an excellent reference. O’Shaughnessy [33] also has a
thorough description of the subject. Malvar [25] covers filterbanks and lapped transforms
extensively.
The extensive wartime interest in sound spectrography led Koenig and his colleagues
at Bell Laboratories [22] in 1946 to the invaluable development of a tool that has been used
for speech analysis since then: the spectrogram. Potter et al. [35] showed the usefulness of
the analog spectrogram in analyzing speech. The spectrogram facilitated research in the field
and led Peterson and Barney [34] to publish in 1952 a detailed study of formant values of
different vowels. The development of computers and the FFT led Oppenheim, in 1970 [30],
to develop digital spectrograms, which imitated the analog counterparts.
The MIT Acoustics Lab started work in speech in 1948 with Leo R. Beranek, who in
1954 published the seminal book Acoustics, where he studied sound propagation in tubes. In
1950, Kenneth N. Stevens joined the lab and started work on speech perception. Gunnar
Fant visited the lab at that time and as a result started a strong speech production effort at
KTH in Sweden.
The 1960s marked the birth of digital speech processing. Two books, Gunnar Fant’s
Acoustical Theory of Speech Production [13] in 1960 and James Flanagan’s Speech Analy-
sis: Synthesis and Perception [14] in 1965, had a great impact and sparked interest in the

330
Speech Signal Representations
field. The advent of the digital computer prompted Kelly and Gertsman to create in 1961 the
first digital speech synthesizer [21]. Short-time Fourier analysis, cepstrum, LPC analysis,
pitch and formant tracking, and digital filterbanks were the fruit of that decade.
Short-time frequency analysis was first proposed for analog signals by Fano [11] in
1950 and later by Schroeder and Atal [42].
The mathematical foundation behind linear predictive coding dates to the auto-
regressive models of George Udny Yule (1927) and Gilbert Walker (1931), which led to the
well-known Yule-Walker equations. These equations resulted in a Toeplitz matrix, named
after Otto Toeplitz (1881 - 1940) who studied it extensively. N. Levinson suggested in 1947
an efficient algorithm to invert such a matrix, which J. Durbin refined in 1960 and is now
known as the Levinson-Durbin recursion. The well-known LPC analysis consisted of the
application of the above results to speech signals, as developed by Bishnu Atal [4], J. Burg
[7], Fumitada Itakura and S. Saito [19] in 1968, Markel [27] and John Makhoul [24] in
1973.
The cepstrum was first proposed in 1964 by Bogert, Healy and John Tukey [6] and
further studied by Alan V. Oppenheim [29] in 1965. The popular mel-frequency cepstrum
was proposed by Davis and Mermelstein [8] in 1980, combining the advantages of cepstrum
with knowledge of the non-linear perception of frequency by the human auditory system that
had been studied by E. Zwicker [47] in 1961.
The study of digital filterbanks was first proposed by Schafer and Rabiner in 1971 for
IIR and in 1975 for FIR filters.
Formant tracking was first investigated by Ken Stevens and James Flanagan in the late
1950s, with the foundations for most modern techniques being developed by Schafer and
Rabiner [40], Itakura [20], and Markel [26]. Pitch tracking through digital processing was
first studied by B. Gold [15] in 1962 and then improved by A. M. Noll [28], M. Schroeder
[41], and M. Sondhi [44] in the late 1960s.
REFERENCES
[1]
Acero, A., "Formant Analysis and Synthesis using Hidden Markov Models," Eu-
rospeech, 1999, Budapest pp. 1047-1050.
[2]
Atal, B.S., Automatic Speaker Recognition Based on Pitch Contours, PhD Thesis
1968, Polytechnic Institute of Brooklyn, .
[3]
Atal, B.S. and L. Hanauer, "Speech Analysis and Synthesis by Linear Prediction of
the Speech Wave," Journal of the Acoustical Society of America, 1971, 50, pp. 637-
655.
[4]
Atal, B.S. and M.R. Schroeder, "Predictive Coding of Speech Signals," Report of
the 6th Int. Congress on Acoustics, 1968, Tokyo, Japan.
[5]
Berouti, M.G., D.G. Childers, and A. Paige, "Glottal Area versus Glottal Volume
Velocity," Int. Conf. on Acoustics, Speech and Signal Processing, 1977, Hartford,
Conn pp. 33-36.
[6]
Bogert, B., M. Healy, and J. Tukey, "The Quefrency Alanysis of Time Series for
Echoes," Proc. Symp. on Time Series Analysis, 1963, New York, J. Wiley pp. 209-
243.

Historical Perspective And Future Reading
331
[7]
Burg, J., "Maximum Entropy Spectral Analysis," Proc. of the 37th Meeting of the
Society of Exploration Geophysicists, 1967.
[8]
Davis, S. and P. Mermelstein, "Comparison of Parametric Representations for
Monosyllable Word Recognition in Continuously Spoken Sentences," IEEE Trans.
on Acoustics, Speech and Signal Processing, 1980, 28(4), pp. 357-366.
[9]
Deller, J.R., J.H.L. Hansen, and J.G. Proakis, Discrete-Time Processing of Speech
Signals, 2000, IEEE Press.
[10]
Droppo, J. and A. Acero, "Maximum a Posteriori Pitch Tracking," Int. Conf. on
Spoken Language Processing, 1998, Sydney, Australia pp. 943-946.
[11]
Fano, R.M., "Short-time Autocorrelation Functions and Power Spectra," Journal of
the Acoustical Society of America, 1950, 22(Sep), pp. 546-550.
[12]
Fant, G., "On the Predictability of Formant Levels and Spectrum Envelopes from
Formant Frequencies" in For Roman Jakobson, M. Halle, Editor 1956, The Hague,
NL, pp. 109-120, Mouton & Co.
[13]
Fant, G., Acoustic Theory of Speech Production, 1970, The Hague, NL, Mouton.
[14]
Flanagan, J., Speech Analysis Synthesis and Perception, 1972, New York,
Springer-Verlag.
[15]
Gold, B., "Computer Program for Pitch Extraction," Journal of the Acoustical Soci-
ety of America, 1962, 34(7), pp. 916-921.
[16]
Hermansky, H., "Perceptual Linear Predictive (PLP) Analysis of Speech," Journal
of the Acoustical Society of America, 1990, 87(4), pp. 1738-1752.
[17]
Hess, W., Pitch Determination of Speech Signals, 1983, New York, Springer-
Verlag.
[18]
Itakura, F., "Line Spectrum Representation of Linear Predictive Coefficients,"
Journal of the Acoustical Society of America, 1975, 57(4), pp. 535.
[19]
Itakura, F. and S. Saito, "Analysis Synthesis Telephony Based on the Maximum
Likelihood Method," Proc. 6th Int. Congress on Acoustics, 1968, Tokyo, Japan.
[20]
Itakura, F. and S. Saito, "A Statistical Method for Estimation of Speech Spectral
Density and Formant Frequencies," Elec. and Comm. in Japan, 1970, 53-A(1), pp.
36-43.
[21]
Kelly, J.L. and L.J. Gerstman, "An Artificial Talker Driven From Phonetic Input,"
Journal of Acoustical Society of America, 1961, 33, pp. 835.
[22]
Koenig, R., H.K. Dunn, and L.Y. Lacy, "The Sound Spectrograph," Journal of the
Acoustical Society of America, 1946, 18, pp. 19-49.
[23]
Krishnamurthy, A.K. and D.G. Childers, "Two Channel Speech Analysis," IEEE
Trans. on Acoustics, Speech and Signal Processing, 1986, 34, pp. 730-743.
[24]
Makhoul, J., "Spectral Analysis of Speech by Linear Prediction," IEEE Trans. on
Acoustics, Speech and Signal Processing, 1973, 21(3), pp. 140-148.
[25]
Malvar, H., Signal Processing with Lapped Transforms, 1992, Artech House.
[26]
Markel, J.D., "Digital Inverse Filtering - A New Tool for Formant Trajectory Esti-
mation," IEEE Trans. on Audio and Electroacoustics, 1972, AU-20(June), pp. 129-
137.

332
Speech Signal Representations
[27]
Markel, J.D. and A.H. Gray, "On Autocorrelation Equations as Applied to Speech
Analysis," IEEE Trans. on Audio and Electroacoustics, 1973, AU-21(April), pp.
69-79.
[28]
Noll, A.M., "Cepstrum Pitch Determination," Journal of the Acoustical Society of
America, 1967, 41, pp. 293--309.
[29]
Oppenheim, A.V., Superposition in a Class of Nonlinear Systems, 1965, Research
Lab. Of Electronics, MIT, Cambridge, Massachusetts.
[30]
Oppenheim, A.V., "Speech Spectrograms Using the Fast Fourier Transform," IEEE
Spectrum, 1970, 7(Aug), pp. 57-62.
[31]
Oppenheim, A.V. and D.H. Johnson, "Discrete Representation of Signals," The
Proc. of the IEEE, 1972, 60(June), pp. 681-691.
[32]
Oppenheim, A.V., R.W. Schafer, and T.G. Stockham, "Nonlinear Filtering of Mul-
tiplied and Convolved Signals," Proc. of the IEEE, 1968, 56, pp. 1264-1291.
[33]
O'Shaughnessy, D., Speech Communication -- Human and Machine, 1987, Addi-
son-Wesley.
[34]
Peterson, G.E. and H.L. Barney, "Control Methods Used in a Study of the Vowels,"
Journal of the Acoustical Society of America, 1952, 24(2), pp. 175-184.
[35]
Potter, R.K., G.A. Kopp, and H.C. Green, Visible Speech, 1947, New York, D. Van
Nostrand Co. Republished by Dover Publications, Inc. 1966.
[36]
Press, W.H., et al., Numerical Recipes in C, 1988, New York, NY, Cambridge
University Press.
[37]
Rabiner, L.R., "On the Use of Autocorrelation Analysis for Pitch Detection," IEEE
Trans. on Acoustics, Speech and Signal Processing, 1977, 25, pp. 24-33.
[38]
Rabiner, L.R. and R.W. Schafer, Digital Processing of Speech Signals, 1978,
Englewood Cliffs, NJ, Prentice-Hall.
[39]
Rosenberg, A.E., "Effect of Glottal Pulse Shape on the Quality of Natural Vowels,"
Journal of the Acoustical Society of America, 1971, 49, pp. 583-590.
[40]
Schafer, R.W. and L.R. Rabiner, "System for Automatic Formant Analysis of
Voiced Speech," Journal of the Acoustical Society of America, 1970, 47, pp. 634--
678.
[41]
Schroeder, M., "Period Histogram and Product Spectrum: New Methods for Fun-
damental Frequency Measurement," Journal of the Acoustical Society of America,
1968, 43(4), pp. 829-834.
[42]
Schroeder, M.R. and B.S. Atal, "Generalized Short-Time Power Spectra and Auto-
correlation," Journal of the Acoustical Society of America, 1962, 34(Nov), pp.
1679-1683.
[43]
Shikano, K., K.-F. Lee, and R. Reddy, "Speaker Adaptation through Vector Quan-
tization," IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1986, To-
kyo, Japan pp. 2643-2646.
[44]
Sondhi, M.M., "New Methods for Pitch Extraction," IEEE Trans. on Audio and
Electroacoustics, 1968, 16(June), pp. 262-268.
[45]
Talkin, D., "A Robust Algorithm for Pitch Tracking" in Speech Coding and Synthe-
sis, W.B. Kleijn and K.K. Paliwal, eds. 1995, Amsterdam, pp. 485-518, Elsevier.

Historical Perspective And Future Reading
333
[46]
Yegnanarayana, B. and R.N.J. Veldhuis, "Extraction of Vocal-Tract System Char-
acteristics from Speech Signals," IEEE Trans. on Speech and Audio Processing,
1998, 6(July), pp. 313-327.
[47]
Zwicker, E., "Subdivision of the Audible Frequency Range into Critical Bands,"
Journal of the Acoustical Society of America, 1961, 33(Feb), pp. 248.

335
C H A P T E R
7
Speech CodingEquation Section 7
Transmission of speech using data networks
requires the speech signal to be digitally encoded. Voice over IP has become very popular
because of the Internet, where bandwidth limitations make it necessary to compress the
speech signal. Digital storage of audio signals, which can result in higher quality and smaller
size than the analog counterpart, is commonplace in compact discs, digital video discs, and
MP3 files. Many spoken language systems also use coded speech for efficient communica-
tion. For these reasons we devote a chapter to speech and audio coding techniques.
Rather than exhaustively cover all the existing speech and audio coding algorithms we
uncover their underlying technology and enumerate some of the most popular standards. The
coding technology discussed in this chapter has a strong link to both speech recognition and
speech synthesis. For example, the speech synthesis algorithms described in Chapter 16 use
many techniques described here.

336
Speech Coding
7.1.
SPEECH CODERS ATTRIBUTES
How do we compare different speech or audio coders? We can refer to a number of factors,
such as signal bandwidth, bit rate, quality of reconstructed speech, noise robustness, compu-
tational complexity, delay, channel-error sensitivity and standards.
Speech signals can be bandlimited to 10 kHz without significantly affecting the
hearer’s perception. The telephone network limits the bandwidth of speech signals to be-
tween 300 and 3400 Hz, which gives telephone speech a lower quality. Telephone speech is
typically sampled at 8 kHz. The term wideband speech is used for a bandwidth of 50–7000
Hz and a sampling rate of 16 kHz. Finally, audio coding is used in dealing with high-fidelity
audio signals, in which case the signal is sampled at 44.1 kHz.
Reduction in bit rate is the primary purpose of speech coding. The previous bit stream
can be compressed to a lower rate by removing redundancy in the signal, resulting in sav-
ings in storage and transmission bandwidth. If only redundancy is removed, the original
signal can be recovered exactly (lossless compression). In lossy compression, the signal
cannot be recovered exactly, though hopefully it will sound similar to the original.
Depending on system and design constraints, fixed-rate or variable-rate speech coders
can be used. Variable-rate coders are used for non-real time applications, such as voice stor-
age (silence can be coded with fewer bits than fricatives, which in turn use fewer bits than
vowels), or for packet voice transmissions, such as CDMA cellular for better channel utiliza-
tion. Transmission of coded speech through a noisy channel may require devoting more bits
to channel coding and fewer to source coding. For most real-time communication systems, a
maximum bit rate is specified.
The quality of the reconstructed speech signal is a fundamental attribute of a speech
coder. Bit rate and quality are intimately related: the lower the bit rate, the lower the quality.
While the bit rate is inherently a number, it is difficult to quantify the quality. The most
widely used measure of quality is the Mean Opinion Score (MOS) [25], which is the result
of averaging opinion scores for a set of between 20 and 60 untrained subjects. Each listener
characterizes each set of utterances with a score on a scale from 1 (unacceptable quality) to
5 (excellent quality), as shown in Table 7.1. An MOS of 4.0 or higher defines good or toll
quality, where the reconstructed speech signal is generally indistinguishable from the origi-
nal signal. An MOS between 3.5 and 4.0 defines communication quality, which is sufficient
for telephone communications. We show in Section 7.2.1 that if each sample is quantized
with 16 bits, the resulting signal has toll quality (essentially indistinguishable from the un-
quantized signal). See Chapter 16 for more details on perceptual quality measurements.
Table 7.1 Mean Opinion Score (MOS) is a numeric value computed as an average for a num-
ber of subjects, where each number maps to the above subjective quality.
Excellent
Good
Fair
Poor
Bad
5
4
3
2
1
Another measure of quality is the signal-to-noise ratio (SNR), defined as the ratio be-
tween the signal’s energy and the noise’s energy in terms of dB:

Speech Coders Attributes
337
SNR
E x
n
E e n
x
e




2
2
2
2
{
[ ]}
{
[ ]}
(7.1)
The MOS rating of a codec on noise-free speech is often higher than its MOS rating
for noisy speech. This is generally caused by specific assumptions in the speech coder that
tend to be violated when a significant amount of noise is present in the signal. This phe-
nomenon is more accentuated for lower-bit-rate coders that need to make more assumptions.
The computational complexity and memory requirements of a speech coder determine
the cost and power consumption of the hardware on which it is implemented. In most cases,
real-time operation is required at least for the decoder. Speech coders can be implemented in
inexpensive Digital Signal Processors (DSP) that form part of many consumer devices, such
as answering machines and DVD players, for which storage tends to be relatively more ex-
pensive than processing power. DSPs are also used in cellular phones because bit rates are
limited.
All speech coders have some delay, which, if excessive, can affect the dynamics of a
two-way communication. For instance, delays over 150 ms can be unacceptable for highly
interactive conversations. Coder delay is the sum of different types of delay. The first is the
algorithmic delay arising because speech coders usually operate on a block of samples,
called a frame, which needs to be accumulated before processing can begin. Often the
speech coder requires some additional look-ahead beyond the frame to be encoded. The
computational delay is the time that the speech coder takes to process the frame. For real-
time operation, the computational delay has to be smaller than the algorithmic delay. A
block of bits is generally assembled by the encoder prior to transmission, possibly to add
error-correction properties to the bit stream, which cause multiplexing delay. Finally, there is
the transmission delay, due to the time it takes for the frame to traverse the channel. The
decoder will incur a decoder delay to reconstruct the signal. In practice, the total delay of
many speech coders is at least three frames.
If the coded speech needs to be transmitted over a channel, we need to consider possi-
ble channel errors, and our speech decoder should be insensitive to at least some of them.
There are two types of errors: random errors and burst errors, and they could be handled
differently. One possibility to increase the robustness against such errors is to use channel
coding techniques, such as those proposed in Chapter 3. Joint source and channel coding
allows us to find the right combination of bits to devote to speech coding with the right
amount devoted to channel coding, adjusting this ratio adaptively depending on the channel.
Since channel coding will only reduce the number of errors, and not eliminate them, grace-
ful degradation of speech quality under channel errors is typically a design factor for speech
coders. When the channel is the Internet, complete frames may be missing because they
have not arrived in time. Therefore, we need techniques that degrade gracefully with missing
frames.

338
Speech Coding
7.2.
SCALAR WAVEFORM CODERS
In this section we describe several waveform coding techniques, such as linear PCM, µ-law,
and A-law PCM, APCM, DPCM, DM, and ADPCM, that quantize each sample using scalar
quantization. These techniques attempt to approximate the waveform, and, if a large enough
bit rate is available, will get arbitrarily close to it.
7.2.1.
Linear Pulse Code Modulation (PCM)
Analog-to-digital converters perform both sampling and quantization simultaneously. To
better understand how this process affects the signal it’s better to study them separately. We
analyzed the effects of sampling in Chapter 5, so now we analyze the effects of quantization,
which encodes each sample with a fixed number of bits. With B bits, it is possible to repre-
sent 2 B separate quantization levels. The output of the quantizer [ ]
x n is given by
[ ]
{ [ ]}
x n
Q x n

(7.2)
Linear Pulse Code Modulation (PCM) is based on the assumption that the input dis-
crete signal x n
[ ] is bounded
x n
X
[ ]
max

(7.3)
and that we use uniform quantization with quantization step size  which is constant for all
levels
ix
x
x
i
i


1

(7.4)
The input/output characteristics are shown by Figure 7.1 for the case of a 3-bit uni-
form quantizer. The so-called mid-riser quantizer has the same number of positive and nega-
tive levels, whereas the mid-tread quantizer has one more negative than positive levels. The
code c[n] is expressed in two’s complement representation, which for Figure 7.1 varies be-
tween –4 and +3. For the mid-riser quantizer the output [ ]
x n can be obtained from the code
c[n] through
ˆ[ ]
( [ ])
[ ]
2
x n
sign c n
c n
∆
=
+
∆
(7.5)
and for the mid-tread quantizer
ˆ[ ]
[ ]
x n
c n
=
∆
(7.6)
which is often used in computer systems that use two’s complement representation.
There are two independent parameters for a uniform quantizer: the number of levels
N
B
 2 , and the step size ∆. Assuming Eq. (7.3), we have the relationship
2
2
X
B
max  
(7.7)

Scalar Waveform Coders
339
Figure 7.1 Three-bit uniform quantization characteristics: (a) mid-riser, (b) mid-tread.
In quantization, it is useful to express the relationship between the unquantized sample
x[n] and the quantized sample [ ]
x n as
[ ]
[ ]
[ ]
x n
x n
e n


(7.8)
with e[n] being the quantization noise. If we choose ∆and B to satisfy Eq. (7.7), then





2
2
e n
[ ]
(7.9)
While there is obviously a deterministic relationship between e[n] and x[n], it is con-
venient to assume a probabilistic model for the quantization noise:
1. e[n] is white: E e n e n
m
m
e
{ [ ] [
]}
[ ]

  
2
2. e[n] and x[n] are uncorrelated: E x n e n
m
{ [ ] [
]}

 0
3. e[n] is uniformly distributed in the interval (
/ ,
/ )


2
2
These assumptions are unrealistic for some signals, except in the case of speech sig-
nals, which rapidly fluctuate between different quantization levels. The assumptions are
reasonable if the step size ∆is a small enough, or alternatively the number of levels is large
enough (say more than 26).
The variance of such uniform distribution (see Chapter 3) is
 e
B
X
2
2
2
2
12
3
2




max
(7.10)
after using Eq. (7.7). The SNR is given by
SNR dB
B
X
x
e
x
(
)
log
log
log
log
max

FHG IKJ 


FHG
IKJ
10
20
2
10
3
20
10
2
2
10
10
10



b
g
(7.11)
which implies that each bit contributes to 6 dB of SNR, since
10
20log
2
6
≅
.
-3∆
-∆
∆
3∆
-3.5∆
-2.5∆
-1.5∆
x
x
3.5∆
2.5∆
000
001
010
011
100
101
110
111
-3.5∆-1.5∆
1.5∆2.5∆
-4∆
-3∆
-2∆
x
x
3∆
2∆
000
001
010
011
100
101
110
111

340
Speech Coding
Speech samples can be approximately described as following a Laplacian distribution
[40]
p x
e
x
x
x
( ) 

1
2
2


(7.12)
and the probability of x falling outside the range (
,
)
4
4
2
2


x
x
is 0.35%. Thus, using
X
x
max  4 , B = 7 bits in Eq. (7.11) results in an SNR of 35 dB, which would be acceptable
in a communications system. Unfortunately, signal energy can vary over 40 dB, due to vari-
ability from speaker to speaker as well as variability in transmission channels. Thus, in prac-
tice, it is generally accepted that 11 bits are needed to achieve an SNR of 35dB while keep-
ing the clipping to a minimum.
Digital audio stored in computers (Windows WAV, Apple AIF, Sun AU, and SND
formats among others) use 16-bit linear PCM as their main format. The Compact Disc-
Digital Audio (CD-DA or simply CD) also uses 16-bit linear PCM. Invented in the late
1960s by James T. Russell, it was launched commercially in 1982 and has become one of
the most successful examples of consumer electronics technology: there were about 700
million audio CD players in 1997. A CD can store up to 74 minutes of music, so the total
amount of digital data that must be stored on a CD is 44,100 samples/(channel*second) * 2
bytes/sample * 2 channels * 60 seconds/minute * 74 minutes = 783,216,000 bytes. This 747
MB are stored in a disk only 12 centimeters in diameter and 1.2 mm thick. CD-ROMs can
record only 650 MB of computer data because they use the remaining bits for error correc-
tion.
7.2.2.
µµµµ-law and A-law PCM
Human perception is affected by SNR, because adding noise to a signal is not as noticeable
if the signal energy is large enough. Ideally, we want SNR to be constant for all quantization
levels, which requires the step size to be proportional to the signal value. This can be done
by using a logarithmic compander1
y n
x n
[ ]
ln [ ]

(7.13)
followed by a uniform quantizer on y[n] so that
ˆ[ ]
[ ]
[ ]
y n
y n
n
ε
=
+
(7.14)
and, thus,
ˆ
ˆ
[ ]
exp{ [ ]}sign{ [ ]}
[ ]exp{ [ ]}
x n
y n
x n
x n
n
ε
=
=
(7.15)
after using Eq. (7.13) and (7.14). If [ ]
n is small, then Eq. (7.15) can be expressed as
[ ]
[ ](
[ ])
[ ]
[ ] [ ]
x n
x n
n
x n
x n
n




1


(7.16)
1 A compander is a nonlinear function that compands one part of the x-axis.

Scalar Waveform Coders
341
and, thus, the SNR  1
2
/  is constant for all levels. This type of quantization is not practi-
cal, because an infinite number of quantization steps would be required. An approximation
is the so-called µ-law [51]:
y n
X
x n
X
x n
[ ]
log
| [ ]|
log
{ [ ]}
max
max

L
NM
O
QP

1
1


sign
(7.17)
which is approximately logarithmic for large values of x[n] and approximately linear for
small values of x[n]. A related compander called A-law is also used
y n
X
A x n
X
A
x n
[ ]
log
| [ ]|
log
{ [ ]}
max
max

 L
NM
O
QP

1
1
sign
(7.18)
which has greater resolution than µ-law for small sample values, but a range equivalent to
12 bits. In practice, they both offer similar quality. The µ-law curve can be seen in Figure
7.2.
x
y
Xmax
-Xmax
Xmax
Xmax
Figure 7.2 Nonlinearity used in the µ-law compression.
In 1972 the ITU-T2 recommendation G.711 standardized telephone speech coding at
64 kbps for digital transmission of speech through telephone networks. It uses 8 bits per
sample and an 8-kHz sampling rate with either µ-law or A-law. In North America and Japan,
µ-law with µ = 255 is used, whereas, in the rest of the world, A-law with A = 87.56 is used.
Both compression characteristics are very similar and result in an approximate SNR of 35
dB. Without the logarithmic compressor, a uniform quantizer requires approximately 12 bits
per sample to achieve the same level of quality. All the speech coders for telephone speech
described in this chapter use G.711 as a baseline reference, whose quality is considered toll,
2 The International Telecommunication Union (ITU) is a part of the United Nations Economic, Scientific and Cul-
tural Organization (UNESCO). ITU-T is the organization within ITU responsible for setting global telecommunica-
tion standards. Within ITU-T, Study Group 15 (SG15) is responsible for formulating speech coding standards. Prior
to 1993, telecommunication standards were set by the Comité Consultatif International Téléphonique et Té-
légraphique (CCITT), which was reorganized into the ITU-T that year.

342
Speech Coding
and an MOS of about 4.0. G.711 is used by most digital central office switches, so that when
you make a telephone call using your plain old telephone service (POTS), your call is en-
coded with G.711. G.711 has an MOS of about 4.3.
7.2.3.
Adaptive PCM
When quantizing speech signals we confront a dilemma. On the one hand, we want the
quantization step size to be large enough to accommodate the maximum peak-to-peak range
of the signal and avoid clipping. On the other hand, we need to make the step size small to
minimize the quantization noise. One possible solution is to adapt the step size to the level
of the input signal.
The basic idea behind Adaptive PCM (APCM) is to let the step size ∆[n] be propor-
tional to the standard deviation of the signal [ ]
n :


[ ]
[ ]
n
n

0
(7.19)
An equivalent method is to use a fixed quantizer but have a time-varying gain G[n],
which is inversely proportional to the signal’s standard deviation
G n
G
n
[ ]
/
[ ]

0 
(7.20)
Estimation of the signal’s variance, or short-time energy, is typically done by low-pass
filtering x
n
2[ ]. With a first-order IIR filter, the variance  2[ ]
n is computed as



2
2
2
1
1
1
[ ]
[
]
(
)
[
]
n
n
x
n





(7.21)
with  controlling the time constant of the filter T
Fs
 1/ (
ln
)
 , Fs the sampling rate, and
0
1



. In practice,  is chosen so that the time constant ranges between 1 ms (  088
.
at 8 kHz) and 10 ms (  0987
.
at 8 kHz).
Alternatively,  2[ ]
n can be estimated from the past M samples:
 2
2
1
1
[ ]
[ ]
n
M
x
m
m n
M
n

 


(7.22)
In practice, it is advantageous to set limits on the range of values of ∆[n] and G[n]:



min
max
[ ]


n
(7.23)
G
G n
G
min
max
[ ]


(7.24)
with the ratios 

max
min
/
and G
G
max
min
/
determining the dynamic range of the system. If
our objective is to obtain a relatively constant SNR over a range of 40 dB, these ratios can
be 100.
Feedforward adaptation schemes require us to transmit, in addition to the quantized
signal, either the step size ∆[n] or the gain G[n]. Because these values evolve slowly with
time, they can be sampled and quantized at a low rate. The overall rate will be the sum of the

Scalar Waveform Coders
343
bit rate required to transmit the quantized signal plus the bit rate required to transmit either
the gain or the step size.
Another class of adaptive quantizers use feedback adaptation to avoid having to send
information about the step size or gain. In this case, the step size and gain are estimated from
the quantizer output, so that they can be recreated at the decoder without any extra informa-
tion. The corresponding short-time energy can then be estimated through a first-order IIR
filter as in Eq. (7.21) or a rectangular window as in Eq. (7.22), but replacing x
n
2[ ] by  [ ]
x
n
2
.
Another option is to adapt the step size


[ ]
[
]
n
P
n

1
(7.25)
where P  1 if the previous codeword corresponds to the largest positive or negative quan-
tizer level, and P  1 if the previous codeword corresponds to the smallest positive or nega-
tive quantizer level. A similar process can be done for the gain.
APCM exhibits an improvement between 4–8 dB over µ-law PCM for the same bit
rate.
7.2.4.
Differential Quantization
Speech coding is about finding redundancy in the signal and removing it. We know that
there is considerable correlation between adjacent samples, because on the average the sig-
nal doesn’t change rapidly from sample to sample. A simple way of capturing this is to
quantize the difference d n
[ ] between the current sample x n
[ ] and its predicted value ~[ ]
x n
d n
x n
x n
[ ]
[ ]
~[ ]


(7.26)
with its quantized value represented as
[ ]
{ [ ]}
[ ]
[ ]
d n
Q d n
d n
e n



(7.27)
where e[n] is the quantization error. Then, the quantized signal is the sum of the predicted
signal ~[ ]
x n and the quantized difference [ ]
d n
[ ]
~[ ]
[ ]
[ ]
[ ]
x n
x n
d n
x n
e n




(7.28)
Figure 7.3 Block diagram of a DPCM encoder and decoder with feedback prediction.
Predictor
Quantizer
_
+
x[n]
d[n]
[ ]
x n

ˆ[ ]
x n
[ ]
d n
Predictor
+
~[ ]
x n
[ ]
x n

344
Speech Coding
If the prediction is good, Eq. (7.28) tells us that the quantization error will be small.
Statistically, we need the variance of e n
[ ] to be lower than that of x n
[ ] for differential cod-
ing to provide any gain. Systems of this type are generically called Differential Pulse Code
Modulation (DPCM) [11] and can be seen in Figure 7.3.
Delta Modulation (DM) [47] is a 1-bit DPCM, which predicts the current sample to be
the same as the past sample:
~[ ]
[
]
x n
x n

1
(7.29)
so that we transmit whether the current sample is above or below the previous sample.
d n
x n
x n
x n
x n
[ ]
[ ]
[
]
[ ]
[
]






RST


1
1
(7.30)
with ∆being the step size. If ∆is too small, the reconstructed signal will not increase as fast
as the original signal, a condition known as slope overload distortion. When the slope is
small, the step size ∆also determines the peak error; this is known as granular noise. Both
quantization errors can be seen in Figure 7.4. The choice of ∆that minimizes the mean
squared error will be a tradeoff between slope overload and granular noise.
xa(t)
xa(t)
~
Figure 7.4 An example of slope overload distortion and granular noise in a DM encoder.
If the signal is oversampled by a factor N, and the step size is reduced by the same
amount (i.e., ∆/N), the slope overload will be the same, but the granular noise will decrease
by a factor N. While the coder is indeed very simple, sampling rates of over 200 kbps are
needed for SNRs comparable to PCM, so DM is rarely used as a speech coder.
However, delta modulation is useful in the design of analog-digital converters, in a
variant called sigma-delta modulation [44] shown in Figure 7.5. First the signal is lowpass
filtered with a simple analog filter, and then it is oversampled. Whenever the predicted sig-
nal
[ ]
x n

is below the original signal x[n], the difference d[n] is positive. This difference d[n]
is averaged over time with a digital integrator whose output is e[n]. If this situation persists,
the accumulated error e[n] will exceed a positive value A, which causes a 1 to be encoded
into the stream q[n]. A digital-analog converter is used in the loop which increments by one
the value of the predicted signal
[ ]
x n

. The system acts in the opposite way if the predicted
signal
[ ]
x n

is above the original signal x[n] for an extended period of time. Since the signal
is oversampled, it changes very slowly from one sample to the next, and this quantization

Scalar Waveform Coders
345
can be accurate. The advantages of this technique as an analog-digital converter are that in-
expensive analog filters can be used and only a simple 1-bit A/D is needed. The signal can
next be low-passed filtered with a more accurate digital filter and then downsampled.
Figure 7.5 A sigma-delta modulator used in an oversampling analog-digital converter.
Adaptive Delta Modulation (ADM) combines ideas from adaptive quantization and
delta modulation with the so-called Continuously Variable Slope Delta Modulation
(CVSDM) [22] having a step size that increases
1
2
[
1]
if [ ], [
1] and [
2] have same sign
[ ]
[
1]
otherwise
n
k
e n e n
e n
n
n
k
α
α
∆
−
+
−
−

∆
=  ∆
−
+

(7.31)
with 0
1



and 0
2
1


k
k . The step size increases if the last three errors have the same
sign and decreases otherwise.
Improved DPCM is achieved through linear prediction in which ~[ ]
x n is a linear com-
bination of past quantized values [ ]
x n
~[ ]
[
]
x n
a x n
k
k
k
p



1
(7.32)
DPCM systems with fixed prediction coefficients can provide from 4 to 11 dB im-
provement over direct linear PCM, for prediction orders up to p = 4, at the expense of in-
creased computational complexity. Larger improvements can be obtained by adapting the
prediction coefficients. The coefficients can be transmitted in a feedforward fashion or not
transmitted if the feedback scheme is selected.
ADPCM [6] combines differential quantization with adaptive step-size quantization.
ITU-T recommendation G.726 uses ADPCM at bit rates of 40, 32, 24, and 16 kbps, with 5,
4, 3, and 2 bits per sample, respectively. It employs an adaptive feedback quantizer and an
adaptive feedback pole-zero predictor. Speech at bit rates of 40 and 32 kbps offer toll qual-
ity, while the other rates don’t. G.727 is called embedded ADPCM because the 2-bit quan-
tizer is embedded into the 3-bit quantizer, which is embedded into the 4-bit quantizer, and
into the 5-bit quantizer. This makes it possible for the same codec to use a lower bit rate,
with a graceful degradation in quality, if channel capacity is temporarily limited. Earlier
+
1
z−
DAC
1
z−
x[n]
d[n]
ˆ[ ]
x n
[ ]
x n

S/H
x(t)
LPF
integrator
e[n]
LPF
M

346
Speech Coding
standards G.721 [7, 13] (created in 1984) and G.723 have been subsumed by G.726 and
G.727. G.727 has a MOS of 4.1 for 32 kbps and is used in submarine cables. The Windows
WAV format also supports a variant of ADPCM. These standards are shown in Table 7.2.
Table 7.2 Common scalar waveform standards used.
Standard
Bit Rate
(kbits/sec)
MOS
Algorithm
Sampling Rate
(kHz)
Stereo CD Audio
1411
5.0
16-bit linear PCM
44.1
WAV, AIFF, SND
Variable
-
16/8-bit linear PCM
8, 11.025, 16,
22.05, 44.1, 48
G.711
64
4.3
µ-law/A-law PCM
8
G.727
40, 32, 24, 16
4.2 (32k)
ADPCM
8
G.722
64, 56, 48
Subband ADPCM
16
Wideband speech (50–7000 Hz) increases intelligibility of fricatives and overall per-
ceived quality. In addition, it provides more subject presence and adds a feeling of transpar-
ent communication. ITU-T Recommendation G.722 encodes wideband speech with bit rates
of 48, 56, and 64 kbps. Speech is divided into two subbands with QMF filters (see Chapter
5). The upper band is encoded using a 16-kbps ADPCM similar to the G.727 standard. The
lower band is encoded using a 48-kbps ADPCM with the 4- and 5-bit quantizers embedded
in the 6-bit quantizer. The quality of this system scores almost 1 MOS higher than that of
telephone speech.
7.3.
SCALAR FREQUENCY DOMAIN CODERS
Frequency domain is advantageous because:
1. The samples of a speech signal have a great deal of correlation among them,
whereas frequency domain components are approximately uncorrelated and
2. The perceptual effects of masking described in Chapter 2 can be more easily
implemented in the frequency domain. These effects are more pronounced for
high-bandwidth signals, so frequency-domain coding has been mostly used
for CD-quality signals and not for 8-kHz speech signals.
7.3.1.
Benefits of Masking
As discussed in Chapter 2, masking is a phenomenon by which human listeners cannot per-
ceive a sound if it is below a certain level. The consequence is that we don’t need to encode

Scalar Frequency Domain Coders
347
such sound. We now illustrate how this masked threshold is computed for MPEG3-1 layer 1.
Given an input signal s[n] quantized with b bits, we obtain the normalized signal x[n] as
1
[ ]
[ ]
2b
s n
x n
N
−
=
(7.33)
where N = 512 is the length of the DFT. Then, using a Hanning window,
(
)
[ ]
0.5
0.5cos 2
/
w n
n N
π
=
−
(7.34)
we obtain the log-power spectrum as
1
2
/
0
10
0
[ ]
10log
[ ] [ ]
N
j
nk N
n
P k
P
w n x n e
π
−
−
=


=
+





(7.35)
where
0P is the playback SPL, which, in the absence of any volume information, is defined
as 90 dB.
Tonal components are identified in Eq. (7.35) as local maxima, which exceed neigh-
boring components within a certain bark distance by at least 7 dB. Specifically, bin k is tonal
if and only if
[ ]
[
1]
P k
P k
>
±
(7.36)
and
[ ]
[
]
7
P k
P k
l
dB
>
±
+
(7.37)
where 1
k
l
< ≤∆, and
k
∆
is given by
2
2
63
(170Hz
5.5kHz)
3
63
127
(5.5kHz,11kHz)
6
127
256
(11kHz,22kHz)
k
k
k
k
<
<
−


∆=
≤
<


≤
≤

(7.38)
so that the power of that tonal masker is computed as the sum of the power in that bin and its
left and right adjacent bins:
0.1 [
]
10
1
[ ]
10log
10
j
P k
j
TM
j
P
k
+
=−


=





(7.39)
The noise maskers are computed from as the sum of power spectrum of the remaining
frequency bins k in a critical band not within a neighborhood
k
∆
of the tonal maskers:
3 MPEG (Moving Picture Experts Group) is the nickname given to a family of International Standards for coding
audiovisual information.

348
Speech Coding
0.1 [ ]
10
[ ]
10log
10
P j
NM
j
P
k


=





(7.40)
where j spans a critical band.
To compute the overall masked threshold we need to sum all masking thresholds con-
tributed by each frequency bin i, which is approximately equal to the maximum (see Chapter
2):
(
)
(
)
[ ]
max
[ ],max
[ ]
h
i
i
T k
T k
T k
=
(7.41)
In Chapter 2 we saw that whereas temporal postmasking can last from 50 to 300 ms,
temporal premasking tends to last about 5 ms. This is also important because when a fre-
quency transform is quantized, the blocking effects of transform’s coders can introduce
noise above the temporal premasking level that can be audible, since 1024 points corre-
sponds to 23 ms at a 44-kHz sampling rate. To remove this pre-echo distortion, audible in
the presence of castanets and other abrupt transient signals, subband filtering has been pro-
posed, whose time constants are well below the 5-ms premasking time constant.
7.3.2.
Transform Coders
We now use the Adaptive Spectral Entropy Coding of High Quality Music Signals (ASPEC)
algorithm, which is the basis for the MPEG1 Layer 1 audio coding standard [24], to illus-
trate how transform coders work. The DFT coefficients are grouped into 128 subbands, and
128 scalar quantizers are used to transmit all the DFT coefficients. It has been empirically
found that a difference of less than 1 dB between the original amplitude and the quantized
value cannot be perceived. Each subband j has a quantizer having
jk
levels and step size of
jT as
(
)
1
2 rnd
/
j
j
j
k
P
T
= + ×
(7.42)
where
jT is the quantized JND threshold,
jP is the quantized magnitude of the largest real
or imaginary component of the jth subband, and rnd( ) is the nearest integer rounding func-
tion. Entropy coding is used to encode the coefficients of that subband. Both
jT and
jP are
quantized on a dB scale using 8-bit uniform quantizers with a 170-dB dynamic range, thus
with a step size of 0.66 dB. Then they are transmitted as side information.
There are two main methods of obtaining a frequency-domain representation:
1. Through subband filtering via a filterbank (see Chapter 5). When a filterbank
is used, the bandwidth of each band is chosen to increase with frequency fol-
lowing a perceptual scale, such as the Bark scale. As shown in Chapter 5,
such filterbanks yield perfect reconstruction in the absence of quantization.

Scalar Frequency Domain Coders
349
2. Through frequency-domain transforms. Instead of using a DFT, higher effi-
ciency can be obtained by the use of an MDCT (see Chapter 5).
The exact implementation of the MPEG1 Layer 1 standard is much more complicated
and beyond the scope of this book, though it follows the main ideas described here; the same
is true for the popular MPEG1 layer III, also known as MP3. Implementation details can be
found in [42].
7.3.3.
Consumer Audio
Dolby Digital, MPEG, DTS and the Perceptual Audio Coder (PAC) [28] are all audio coders
based on frequency-domain coding. Except for MPEG-1, which supports only stereo signals,
the rest support multichannel.
Dolby Digital is multichannel digital audio, using lossy AC-3 [54] coding technology
from original PCM with a sample rate of 48 kHz at up to 24 bits. The bit rate varies from 64
to 448 kbps, with 384 being the normal rate for 5.1 channels and 192 the normal rate for
stereo (with or without surround encoding). Most Dolby Digital decoders support up to 640
kbps. Dolby Digital is the format used for audio tracks on almost all Digital Video/Versatile
Discs (DVD). A DVD-5 with only one surround stereo audio stream (at 192 kbps) can hold
over 55 hours of audio. A DVD-18 can hold over 200 hours.
MPEG was established in 1988 as part of the joint ISO (International Standardization
Organization) / IEC (International Electrotechnical Commission) Technical Committee on
Information technology. MPEG-1 was approved in 1992 and MPEG-2 in 1994. Layers I to
III define several specifications that provide better quality at the expense of added complex-
ity. MPEG-1 audio is limited to 384 kbps. MPEG1 Layer III audio [23], also known as MP3,
is very popular on the Internet, and many compact players exist.
MPEG-2 Audio, one of the audio formats used in DVD, is multichannel digital audio,
using lossy compression from 16-bit linear PCM at 48 kHz. Tests have shown that for nearly
all types of speech and music, at a data rate of 192 kbps and over, on a stereo channel,
scarcely any difference between original and coded versions was observable (ranking of
coded item > 4.5), with the original signal needing 1.4 Mbps on a CD (reduction by a factor
of 7). One advantage of the MPEG Audio technique is that future findings regarding psy-
choacoustic effects can be incorporated later, so it can be expected that today’s quality level
using 192 kbps will be achievable at lower data rates in the future. A variable bit rate of 32
to 912 kbps is supported for DVDs.
DTS (Digital Theater Systems) Digital Surround is another multi-channel (5.1) digital
audio format, using lossy compression derived from 20-bit linear PCM at 48 kHz. The com-
pressed data rate varies from 64 to 1536 kbps, with typical rates of 768 and 1536 kbps.
7.3.4.
Digital Audio Broadcasting (DAB)
Digital Audio Broadcasting (DAB) is a means of providing current AM and FM listeners
with a new service that offers: sound quality comparable to that of compact discs, increased

350
Speech Coding
service availability (especially for reception in moving vehicles), flexible coverage scenar-
ios, and high spectrum efficiency.
Different approaches have been considered for providing listeners with such a service.
Currently, the most advanced system is one commonly referred to as Eureka 147 DAB,
which has been under development in Europe under the Eureka Project EU147 since 1988.
Other approaches include various American in-band systems (IBOC, IBAC, IBRC, FMDigi-
tal, and FMeX) still in development, as well as various other systems promising satellite
delivery, such as WorldSpace and CD Radio, still in development as well. One satellite-
delivery system called MediaStar (formerly Archimedes) proposes to use the Eureka 147
DAB signal structure, such that a single receiver could access both terrestrial and satellite
broadcasts.
DAB has been under development since 1981 at the Institut für Rundfunktechnik
(IRT) and since 1987 as part of a European research project (Eureka 147). The Eureka 147
DAB specification was standardized by the European Telecommunications Standards Insti-
tute (ETSI) in February 1995 as document ETS 300 401, with a draft second edition issued
in
June
1996.
In
December
1994,
the
International
Telecommunication
Union—
Radiocommunication (ITU-R) recommended that this technology, referred to as Digital Sys-
tem A, be used for implementing DAB services.
The Eureka 147 DAB signal consists of multiple carriers within a 1.536-MHz channel
bandwidth. Four possible modes of operation define the channel coding configuration, speci-
fying the total number of carriers, the carrier spacing, and also the guard interval duration.
Each channel provides a raw data rate of 2304 kbps; after error protection, a useful data rate
of anywhere between approximately 600 kbps up to 1800 kbps is available to the service
provider, depending on the user-specified multiplex configuration. This useful data rate can
be divided into an infinite number of possible configurations of audio and data programs.
All audio programs are individually compressed using MUSICAM (MPEG-1 Layer II).
For each useful bit, 1 1/3 ... 4 bits are transmitted. This extensive redundancy makes it
possible to reconstruct the transmitted bit sequence in the receiver, even if part of it is dis-
rupted during transmission (FEC—forward error correction). In the receiver, error conceal-
ment can be carried out at the audio reproduction stage, so that residual transmission errors
which could not be corrected do not always cause disruptive noise.
7.4.
CODE EXCITED LINEAR PREDICTION (CELP)
The use of linear predictors removes redundancy in the signal, so that coding of the residual
signal can be done with simpler quantizers. We first introduce the LPC vocoder and then
introduce coding of the residual signal with a very popular technique called CELP.
7.4.1.
LPC Vocoder
A typical model for speech production is shown in Figure 7.6, which has a source, or excita-
tion, driving a linear time-varying filter. For voiced speech, the excitation is an impulse train
spaced P samples apart. For unvoiced speech, the source is white random noise. The filter

Code Excited Linear Prediction (CELP)
351
h
n
m[ ] for frame m changes at regular intervals, say every 10 ms. If this filter is represented
with linear predictive coding, it is called an LPC vocoder [3].
Figure 7.6 Block diagram of an LPC vocoder.
In addition to transmitting the gain and LPC coefficients, the encoder has to determine
whether the frame is voiced or unvoiced, as well as the pitch period P for voiced frames.
The LPC vocoder produces reasonable quality for unvoiced frames, but often results in
somewhat mechanical sound for voiced sounds, and a buzzy quality for voiced fricatives.
More importantly, the LPC vocoder is quite sensitive to voicing and pitch errors, so that an
accurate pitch tracker is needed for reasonable quality. The LPC vocoder also performs
poorly in the presence of background noise. Nonetheless, it can be highly intelligible. The
Federal Standard 1015 [55], proposed for secure communications, is based on a 2.4-kbps
LPC vocoder.
It’s also possible to use linear predictive coding techniques together with Huffman
coding [45] to achieve lossless compression of up to 50%.
7.4.2.
Analysis by Synthesis
Code Excited Linear Prediction (CELP) [5] is an umbrella for a family of techniques that
quantize the LPC residual using VQ, thus the term code excited, using analysis by synthesis.
In addition CELP uses the fact that the residual of voiced speech has periodicity and can be
used to predict the residual of the current frame. In CELP coding the LPC coefficients are
quantized and transmitted (feedforward prediction), as well as the codeword index. The pre-
diction using LPC coefficients is called short-term prediction. The prediction of the residual
based on pitch is called long-term prediction. To compute the quantized coefficients we use
an analysis-by-synthesis technique, which consists of choosing the combination of parame-
ters whose reconstructed signal is closest to the analysis signal. In practice, not all coeffi-
cients of a CELP coder are estimated in an analysis-by-synthesis manner.
We first estimate the pth-order LPC coefficients from the samples x[n] for frame t us-
ing the autocorrelation method, for example. We then quantize the LPC coefficients to
1
2
(
,
,
,
)
p
a a
a

with the techniques described in Section 7.4.5. The residual signal e[n] is
obtained by inverse filtering x[n] with the quantized LPC filter
1
[ ]
[
]
p
i
i
e n
a x n
i
=
=
−

(7.43)
Given the transfer function of the LPC filter
h
n
m[ ]

352
Speech Coding
0
1
1
1
( )
( )
1
i
i
p
i
i
i
i
H z
h z
A z
a z
∞
−
=
−
=
=
=
=
−


(7.44)
we can obtain the first M coefficients of the impulse response h[n] of the LPC filter by driv-
ing it with an impulse as
1
1
1
0
[ ]
[
]
0
[
]
n
i
i
p
i
i
n
h n
a h n
i
n
p
a h n
i
p
n
M
=
=


=


=
−
<
<



−
≤
<



(7.45)
so that if we quantize a frame of M samples of the residual
( [0], [1],
[
1])T
e
e
e M
=
−
e

to
( [0], [1],
[
1])T
i
i
i
i
e
e
e M
=
−
e

, we can compute the reconstructed signal ˆ [ ]
ix n as
0
1
ˆ [ ]
[ ] [
]
[ ] [
]
n
i
i
m
m n
x n
h m e n
m
h m e n
m
∞
=
= +
=
−
+
−


(7.46)
where the second term in the sum depends on the residual for previous frames, which we
already have. Let’s define signal
0[ ]
r n as the second term of Eq. (7.46):
0
1
[ ]
[ ] [
]
m n
r n
h m e n
m
∞
= +
=
−

(7.47)
which is the output of the LPC filter when there is no excitation for frame t. The important
thing to note is that
0[ ]
r n does not depend on
[ ]
ie n
It is convenient to express Eqs. (7.46) and (7.47) in matrix form as
0
ˆ i
i
=
+
x
He
r
(7.48)
where matrix H corresponds to the LPC filtering operation with its memory set to 0:
H 
L
N
MMMMMM
O
Q
PPPPPP



h
h
h
h
h
h
h
h
h
h
M
M
M
M
0
1
0
1
2
0
1
1
0
0
0
0
0
0
0




 



(7.49)
Given the large dynamic range of the residual signal, we use gain-shape quantization,
where we quantize the gain and the gain-normalized residual separately:

Code Excited Linear Prediction (CELP)
353
i
i
λ
=
e
c
(7.50)
where  is the gain and ci is the codebook entry i. This codebook is known as the fixed
codebook because its vectors do not change from frame to frame. Usually the size of the
codebook is selected as 2 N so that full use is made of all N bits. Codebook sizes typically
vary from 128 to 1024. Combining Eq. (7.48) with Eq. (7.50), we obtain
0
ˆ i
i
λ
=
+
x
Hc
r
(7.51)
The error between the original signal x and the reconstructed signal ˆ ix is
ˆ i
=
−
ε
x
x
(7.52)
The optimal gain  and codeword index i are the ones that minimize the squared error
between the original signal and the reconstructed4 signal:
2
2
2
2
0
0
0
ˆ
( , )
2
(
)
T
T
T
T
i
i
i
i
i
E i λ
λ
λ
λ
=
−
=
−
−
=
−
+
−
−
x
x
x
Hc
r
x
r
c H Hc
c H
x
r
(7.53)
where the term
2
0
−
x
r
does not depend on  or i and can be neglected in the minimization.
For a given
ic , the gain  i that minimizes Eq. (7.53) is given by
0
(
)
T
T
i
i
T
T
i
i
λ
−
= c H
x
r
c H Hc
(7.54)
Inserting Eq. (7.54) into (7.53) lets us compute the index j as the one that minimizes
(
)
2
0
(
)
arg min
T
T
i
T
T
i
i
i
j


−


=
−






c H
x
r
c H Hc
(7.55)
So we first obtain the codeword index j according to Eq. (7.55) and then the gain  j
according to Eq. (7.54), which is scalarly quantized to  j . Both codeword index j and  j
are transmitted. In the algorithm described here, we first chose the quantized LPC coeffi-
cients
1
2
(
,
,
,
)
p
a a
a

independently of the gains and codeword index, and then we chose the
codeword index independently of the quantized gain  j . This procedure is called open-loop
estimation, because some parameters are obtained independently of the others. This is
shown in Figure 7.7. Closed-loop estimation [49] means that all possible combinations of
quantized parameters are explored. Closed-loop is more computationally expensive but
yields lower squared error.
4 A beginner’s mistake is to find the codebook index that minimizes the squared error of the residual. This does not
minimize the difference between the original signal and the reconstructed signal.

354
Speech Coding
Figure 7.7 Analysis-by-synthesis principle used in a basic CELP.
7.4.3.
Pitch Prediction: Adaptive Codebook
The fact that speech is highly periodic during voiced segments can also be used to reduce
redundancy in the signal. This can be done by predicting the residual signal e[n] at the cur-
rent vector with samples from the past residual signal shifted a pitch period t:
[ ]
[
]
[ ]
[ ]
[ ]
a
f
f
a
a
f
f
t
i
i
t
t
i
i
e n
e n
t
c
n
c n
c
n
λ
λ
λ
λ
=
−
+
=
+
(7.56)
Using the matrix framework we described before, Eq. (7.56) can be expressed as
a
a
f
f
ti
t
t
i
i
λ
λ
=
+
e
c
c
(7.57)
where we have made use of an adaptive codebook [31], where
a
tc
is the adaptive codebook
entry j with corresponding gain a , and ci
f is the fixed or stochastic codebook entry i with
corresponding gain  f . The adaptive codebook entries are segments of the recently synthe-
sized excitation signal
( [
], [1
],
, [
1
])
a
T
t
e
t e
t
e M
t
=
−
−
−−
c

(7.58)
where t is the delay which specifies the start of the adaptive codebook entry t. The range of t
is often between 20 and 147, since this can be encoded with 7 bits. This corresponds to a
range in pitch frequency between 54 and 400 Hz for a sampling rate of 8 kHz.
The contribution of the adaptive codebook is much larger than that of the stochastic
codebook for voiced sounds. So we generally search for the adaptive codebook first, using
Eq. (7.58) and a modified version of Eqs. (7.55), (7.54), replacing i by t. Closed-loop search
of both t and gain here often yields a much larger error reduction.
iλ
…
codebook
+
A(z)
+
x[n]
[ ]
ix n
Error minimization
[ ]
i n
ε
VQ index
[ ]
ic n
Short-term predictor

Code Excited Linear Prediction (CELP)
355
7.4.4.
Perceptual Weighting and Postfiltering
The objective of speech coding is to reduce the bit rate while maintaining a perceived level
of quality; thus, minimization of the error is not necessarily the best criterion. A perceptual
weighting filter tries to shape the noise so that it gets masked by the speech signal (see
Chapter 2). This generally means that most of the quantization noise energy is located in
spectral regions where the speech signal has most of its energy. A common technique [4]
consists in approximating this perceptual weighting with a linear filter
W z
A z
A z
( )
( /
)
( / )



(7.59)
where A(z) is the predictor polynomial
A z
a z
i
i
i
p
( )  


1
1
(7.60)
Choosing  and  so that and 0
1
γ
β
<
<
≤, implies that the roots of A z( /
)

and A z( / )

will move closer to the origin of the unit circle than the roots of A z( ), thus resulting in a
frequency response with wider resonances. This perceptual filter therefore deemphasizes the
contribution of the quantization error near the formants. A common choice of parameters is
  10.
and   08. , since it simplifies the implementation. This filter can easily be included
in the matrix H, and a CELP coder incorporating the perceptual weighting is shown in
Figure 7.8.
Figure 7.8 Diagram of a CELP coder. Both long-term and short-term predictors are used, to-
gether with a perceptual weighting.
Despite the perceptual weighting filter, the reconstructed signal still contains audible
noise. This filter reduces the noise in those frequency regions that are perceptually irrelevant
without degrading the speech signal. The postfilter generally consists of a short-term postfil-
ter to emphasize the formant structure and a long-term postfilter to enhance the periodicity
+
A(z)
+
x[n]
ˆ[ ]
x n
Error minimization
[ ]
i n
ε
f
iλ
…
Stochastic
codebook
VQ index
[ ]
ic n
t
z−
a
tλ
Long term Predictor
Short term Predictor
W(z)

356
Speech Coding
of the signal [10]. One possible implementation follows Eq. (7.59) with values of   05.
and   0 75
.
.
7.4.5.
Parameter Quantization
To achieve a low bit rate, all the coefficients need to be quantized. Because of its coding
efficiency, vector quantization is the compression technique of choice to quantize the predic-
tor coefficients. The LPC coefficients cannot be quantized directly, because small errors
produced in the quantization process may result in large changes in the spectrum and possi-
bly unstable filters. Thus, equivalent representations that guarantee stability are used, such
as reflection coefficients, log-area ratios, and the line spectral frequencies (LSF) described
in Chapter 6. LSF are used most often, because it has been found empirically that they be-
have well when they are quantized and interpolated [2]. For 8 kHz, 10 predictor coefficients
are often used, which makes using a single codebook impractical because of the large di-
mension of the vector. Split-VQ [43] is a common choice, where the vectors are divided into
several subvectors, and each is vector quantized. Matrix quantization can also be used to
exploit the correlation of these subvectors across consecutive time frames. Transparent
quality, defined as average spectral distortion below 1 dB with no frames above 4 dB, can be
achieved with fewer than 25 bits per frame.
A frame typically contains around 20 to 30 milliseconds, which at 8 kHz represents
160–240 samples. Because of the large vector dimension, it is impractical to quantize a
whole frame with a single codebook. To reduce the dimensionality, the frame is divided into
four or more nonoverlapping sub-frames. The LSF coefficients for each subframe are line-
arly interpolated between the two neighboring frames.
A typical range of the pitch prediction for an 8-kHz sampling rate goes from 2 to 20
ms, from 20 to 147 samples, 2.5 ms to 18.375 ms, which can be encoded with 7 bits. An
additional bit is often used to encode fractional delays for the lower pitch periods. These
fractional delays can be implemented through upsampling as described in Chapter 5. The
subframe gain of the adaptive codebook can be effectively encoded with 3 or 4 bits. Alterna-
tively, the gains of all sub-frames within a frame can be encoded through VQ, resulting in
more efficient compression.
The fixed codebook can be trained from data using the techniques described in Chap-
ter 4. This will offer the lowest distortion for the training set but doesn’t guarantee low dis-
tortion for mismatched test signals. Also, it requires additional storage, and full search in-
creases computation substantially.
Since subframes should be approximately white, the codebook can be populated from
samples of a white process. A way of reducing computation is to let those noise samples be
only +1, 0, or –1, because only additions are required. Codebooks of a specific type, known
as algebraic codebooks [1], offer even more computational savings because they contain
many 0s. Locations for the 4 pulses per subframe under the G.729 standard are shown in
Table 7.3.

Code Excited Linear Prediction (CELP)
357
Full search can efficiently be done with this codebook structure. Algebraic codebooks
can provide almost as low distortion as trained codebooks can, with low computational
complexity.
Table 7.3 Algebraic codebooks for the G.729 standard. Each of the four codebooks has one
pulse in one possible location indicated by 3 bits for the first three codebooks and 4 bits for the
last codebook. The sign is indicated by an additional bit. A total of 17 bits are needed to en-
code a 40-sample subframe.
Amplitude
Positions
±1
0, 5, 10, 15, 20, 25, 30, 35
±1
1, 6, 11, 16, 21, 26, 31, 36
±1
2, 7, 12, 17, 22, 27, 32, 37
±1
3, 8, 13, 18, 23, 28, 33, 38
4, 9, 14, 19, 24, 29, 34, 39
7.4.6.
CELP Standards
There are many standards for speech coding based on CELP, offering various points in the
bit-rate/quality plane, mostly depending on when they were created and how refined the
technology was at that time.
Voice over Internet Protocol (Voice over IP) consists of transmission of voice through
data networks such as the Internet. H.323 is an umbrella standard which references many
other ITU-T recommendations. H.323 provides the system and component descriptions, call
model descriptions, and call signaling procedures. For audio coding, G.711 is mandatory,
while G.722, G.728, G.723.1, and G.729 are optional. G.728 is a low-delay CELP coder that
offers toll quality at 16 kbps [9], using a feedback 50th-order predictor, but no pitch predic-
tion. G.729 [46] offers toll quality at 8 kbps, with a delay of 10 ms. G.723.1, developed by
DSP Group, including Audiocodes Ltd., France Telecom, and the University of Sherbrooke,
has slightly lower quality at 5.3 and 6.3 kbps, but with a delay of 30 ms. These standards are
shown in Table 7.4.
Table 7.4 Several CELP standards used in the H.323 specification used for teleconferencing
and voice streaming through the internet.
Standard
Bit Rate
(kbps)
MOS
Algorithm
H.323
Comments
G.728
16
4.0
No pitch prediction
Optional
Low -delay
G.729
8
3.9
ACELP
Optional
G.723.1
5.3, 6.3
3.9
ACELP for 5.3k
Optional

358
Speech Coding
In 1982, the Conference of European Posts and Telegraphs (CEPT) formed a study
group called the Groupe Spécial Mobile (GSM) to study and develop a pan-European public
land mobile system. In 1989, GSM responsibility was transferred to the European Tele-
communication Standards Institute (ETSI), and the phase I GSM specifications were pub-
lished in 1990. Commercial service was started in mid 1991, and by 1993 there were 36
GSM networks in 22 countries, with 25 additional countries considering or having already
selected GSM. This is not only a European standard; South Africa, Australia, and many
Middle and Far East countries have chosen GSM.
The acronym GSM now stands for
Global System for Mobile telecommunications. The GSM group studied several voice cod-
ing algorithms on the basis of subjective speech quality and complexity (which is related to
cost, processing delay, and power consumption once implemented) before arriving at the
choice of a Regular Pulse Excited–Linear Predictive Coder (RPE-LPC) with a Long Term
Predictor loop [56]. Neither the original full-rate at 13 kbps [56] nor the half-rate at 5.6
kbps [19] achieves toll quality, though the enhanced full-rate (EFR) standard based on
ACELP [26] has toll quality at the same rates.
The Telecommunication Industry Association (TIA) and the Electronic Industries Alli-
ance (EIA) are organizations accredited by the American National Standards Institute
(ANSI) to develop voluntary industry standards for a wide variety of telecommunication
products. TR-45 is the working group within TIA devoted to mobile and personal communi-
cation systems. Time Division Multiple Access (TDMA) is a digital wireless technology that
divides a narrow radio channel into framed time slots (typically 3 or 8) and allocates a slot to
each user. The TDMA Interim Standard 54, or TIA/EIA/IS54, was released in early 1991 by
both TIA and EIA. It is available in North America at both the 800-MHz and 1900-MHz
bands. IS54 [18] at 7.95 kbps is used in North America’s TDMA (Time Division Multiple
Access) digital telephony and has quality similar to the original full-rate GSM. TDMA IS-
136 is an update released in 1994.
Table 7.5 CELP standards used in cellular telephony.
Standard
Bit Rate
(kbps)
MOS
Algorithm
Cellular
Comments
Full-rate GSM
13
3.6
VSELP
RTE-LTP
GSM
EFR GSM
12.2
4.5
ACELP
GSM
IS-641
7.4
4.1
ACELP
PCS1900
IS-54
7.95
3.9
VSELP
TDMA
IS-96a
max 8.5
3.9
QCELP
CDMA
Variable-rate
Code Division Multiple Access (CDMA) is a form of spread spectrum, a family of
digital communication techniques that have been used in military applications for many
years. The core principle is the use of noiselike carrier waves, and, as the name implies,
bandwidths much wider than that required for simple point-to-point communication at the
same data rate. Originally there were two motivations: either to resist enemy efforts to jam

Low-Bit Rate Speech Coders
359
the communications (anti-jam, or AJ) or to hide the fact that communication was even tak-
ing place, sometimes called low probability of intercept (LPI). The service started in 1996 in
the United States, and by the end of 1999 there were 50 million subscribers worldwide. IS-
96 QCELP [14], used in North America’s CDMA, offers variable-rate coding at 8.5, 4, 2
and 0.8 kbps. The lower bit rate is transmitted when the coder detects background noise.
TIA/EIA/IS-127-2 is a standard for an enhanced variable-rate codec, whereas TIA/EIA/IS-
733-1 is a standard for high-rate. Standards for CDMA, TDMA, and GSM are shown in
Table 7.5.
Third generation (3G) is the generic term used for the next generation of mobile
communications systems. 3G systems will provide enhanced services to those—such as
voice, text, and data—predominantly available today. The Universal Mobile Telecommuni-
cations System (UMTS) is a part of ITU's International Mobile Telecommunications (IMT)-
2000 vision of a global family of third-generation mobile communications systems. It has
been assigned to the frequency bands 1885–2025 and 2110–2200 MHz. The first networks
are planned to launch in Japan in 2001, with European countries following in early 2002. A
major part of 3G is General Packet Radio Service (GPRS), under which carriers charge by
the packet rather than by the minute. The speech coding standard for CDMA2000, the um-
brella name for the third-generation standard in the United States, is expected to gain ap-
proval late in 2000. An adaptive multi rate wideband speech codec has also been proposed
for the GSM’s 3G [16], which has five modes of operation from 24 kbps down to 9.1 kbps.
While most of the work described above uses a sampling rate of 8 kHz, there has been
growing interest in using CELP techniques for high bandwidth and particularly in a scalable
way so that a basic layer contains the lower frequency and the higher layer either is a full-
band codec [33] or uses a parametric model [37].
7.5.
LOW-BIT RATE SPEECH CODERS
In this section we describe a number of low-bit-rate speech coding techniques including the
mixed-excitation LPC vocoder, harmonic coding, and waveform interpolation. These coding
techniques are also used extensively in speech synthesis.
Waveform-approximating coders are designed to minimize the difference between the
original signal and the coded signal. Therefore, they produce a reconstructed signal whose
SNR goes to infinity as the bit rate increases, and they also behave well when the input sig-
nal is noisy or music. In this category we have the scalar waveform coders of Section 7.2,
the frequency-domain coders of Section 7.3, and the CELP coders of Section 7.4.
Low-bit-rate coders, on the other hand, do not attempt to minimize the difference be-
tween the original signal and the quantized signal. Since these coders are designed to operate
at low bit rates, their SNR does not generally approach infinity even if a large bit rate is
used. The objective is to compress the original signal with another one that is perceptually
equivalent. Because of the reliance on an inaccurate model, these low-bit-rate coders often
distort the speech signal even if the parameters are not quantized. In this case, the distortion
can consist of more than quantization noise. Furthermore, these coders are more sensitive to
the presence of noise in the signal, and they do not perform as well on music.

360
Speech Coding
In Figure 7.9 we compare the MOS of waveform approximating coders and low-bit-
rate coders as a function of the bit rate. CELP uses a model of speech to obtain as much pre-
diction as possible, yet allows for the model not to be exact, and thus is a waveform-
approximating coder. CELP is a robust coder that works reasonably well when the assump-
tion of only a clean speech signal breaks either because of additive noise or because there is
music in the background. Researchers are working on the challenging problem of creating
more scalable coders that offer best performance at all bit rates.
Figure 7.9 Typical subjective performance of waveform-approximating and low-bit-rate cod-
ers as a function of the bit rate. Note that waveform-approximating coders are a better choice
for bit rates higher than about 3 kbps, whereas parametric coders are a better choice for lower
bit rates. The exact cutoff point depends on the specific algorithms compared.
7.5.1.
Mixed-Excitation LPC Vocoder
The main weakness of the LPC vocoder is the binary decision between voiced and unvoiced
speech, which results in errors especially for noisy speech and voiced fricatives. By having a
separate voicing decision for each of a number of frequency bands, the performance can be
enhanced significantly [38]. The new proposed U.S. Federal Standard at 2.4 kbps is a Mixed
Excitation Linear Prediction (MELP) LPC vocoder [39], which has a MOS of about 3.3.
This exceeds the quality of the older 4800-bps federal standard 1016 [8] based on CELP.
The bit rate of the proposed standard can be reduced while maintaining the same quality by
jointly quantizing several frames together [57]. A hybrid codec that uses MELP in strongly
voiced regions and CELP in weakly voiced and unvoiced regions [53] has shown to yield
lower bit rates. MELP can also be combined with the waveform interpolation technique of
Section 7.5.3 [50].
7.5.2.
Harmonic Coding
Sinusoidal coding decomposes the speech signal [35] or the LP residual signal [48] into a
sum of sinusoids. The case where these sinusoids are harmonically related is of special in-
1
2
4
8
32
16
64
Bit rate (kbps)
poor
fair
good
excellent
Waveform-approximating coder
Low-bit-rate coder

Low-Bit Rate Speech Coders
361
terest for speech synthesis (see Chapter 16), so we will concentrate on it in this section, even
though a similar treatment can be followed for the case where the sinusoids are not harmoni-
cally related. In fact, a combination of harmonically related and nonharmonically related
sinusoids can also be used [17]. We show in Section 7.5.2.2 that we don’t need to transmit
the phase of the sinusoids, only the magnitude.
As shown in Chapter 5, a periodic signal ~[ ]
s n with period T0 can be expressed as a
sum of T0 harmonic sinusoids
~[ ]
cos(
)
s n
A
nl
l
l
l
T







0
0
1
0
(7.61)
whose frequencies are multiples of the fundamental frequency 

0
0
2

/ T , and where Al
and  l are the sinusoid amplitudes and phases, respectively. If the pitch period T0 has frac-
tional samples, the sum in Eq. (7.61) includes only the integer part of T0 in the summation.
Since a real signal s n
[ ] will not be perfectly periodic in general, we have a modeling error
e n
s n
s n
[ ]
[ ]
~[ ]


(7.62)
We can use short-term analysis to estimate these parameters from the input signal s n
[ ]
at frame k, in the neighborhood of t
kN

, where N is the frame shift:
s n
s n w n
s n w kN
n
k
k
[ ]
[ ]
[ ]
[ ] [
]



(7.63)
if we make the assumption that the sinusoid parameters for frame k ( 0
k , Al
k and  l
k ) are
constant within the frame.
At resynthesis time, there will be discontinuities at unit boundaries, due to the block
processing, unless we specifically smooth the parameters over time. One way of doing this
is with overlap-add method between frames (k – 1) and k:
[ ]
[ ]~
[ ]
[
]~ [
]
s n
w n s
n
w n
N s
n
N
k
k




1
(7.64)
where the window w n
[ ] must be such that
w n
w n
N
[ ]
[
]


 1
(7.65)
to achieve perfect reconstruction. This is the case for the common Hamming and Hanning
windows.
This harmonic model [35] is similar to the classic filterbank, though rather than the
whole spectrum we transmit only the fundamental frequency  0 and the amplitudes Al and
phases  l of the harmonics. This reduced representation doesn’t result in loss of quality for
a frame shift N that corresponds to 12 ms or less. For unvoiced speech, using a default pitch
of 100 Hz results in acceptable quality.
7.5.2.1.
Parameter Estimation
For simplicity in the calculations, let’s define ~[ ]
s n as a sum of complex exponentials

362
Speech Coding
~[ ]
exp{ (
)}
s n
A
j nl
l
l
l
T







0
0
1
0
(7.66)
and perform short-time Fourier transform with a window w n
[ ]
~ ( )
(
)
S
A e
W
l
W
l
l
T
j
l









0
1
0
0
(7.67)
where W( )

is the Fourier transform of the window function. The goal is to estimate the
sinusoid parameters as those that minimize the squared error:
E
S
SW


| ( )
~ ( )|


2
(7.68)
If the main lobes of the analysis window do not overlap, we can estimate the phases  l
as


l
S l
 arg (
)
0
(7.69)
and the amplitudes Al as
A
S l
W
l 
(
)
( )
 0
0
(7.70)
For example, the Fourier transform of a (2N + 1) point rectangular window centered
around the origin is given by
W
N
( )
sin (
)
/
sin
/





2
1
2
2
b
g
b
g
(7.71)
whose main lobes will not overlap in Eq. (7.67) if 2
2
1
0T
N

 : i.e., the window contains at
least two pitch periods. The implicit assumption in the estimates of Eqs. (7.69) and (7.70) is
that there is no spectral leakage, but a rectangular window does have significant spectral
leakage, so a different window is often used in practice. For windows such as Hanning or
Hamming, which reduce the leakage significantly, it has been found experimentally that
these estimates are correct if the window contains at least two and a half pitch periods.
Typically, the window is centered around 0 (nonzero in the interval 


N
n
N ) to
avoid numerical errors in estimating the phases.
Another implicit assumption in Eqs. (7.69) and (7.70) is that we know the fundamental
frequency  0 ahead of time. Since, in practice, this is not the case, we can estimate it as the
one which minimizes Eq. (7.68). This pitch-estimation method can generate pitch doubling
or tripling when a harmonic falls within a formant that accounts for the majority of the sig-
nal’s energy.
Voiced/unvoiced decisions can be computed from the ratio between the energy of the
signal and that of the reconstruction error

Low-Bit Rate Speech Coders
363
SNR
s n
s n
s n
n
N
N
n
N
N






| [ ]|
| [ ]
~[ ]|
2
2
(7.72)
where it has been empirically found that frames with SNR higher than 13 dB are generally
voiced and lower than 4 dB unvoiced. In between, the signal is considered to contain a
mixed excitation. Since speech is not perfectly stationary within the analysis frame, even
noise-free periodic signals will yield finite SNR.
For unvoiced speech, a good assumption is to default to a pitch of 100 Hz. The use of
fewer sinusoids leads to perceptual artifacts.
Improved quality can be achieved by using an analysis-by-synthesis framework [17,
34] since the closed-loop estimation is more robust to pitch-estimation and voicing decision
errors.
7.5.2.2.
Phase Modeling
An impulse train e[n], a periodic excitation, can be expressed as a sum of complex exponen-
tials
e n
T
n
n
kT
e
k
j n n
l
l
T
[ ]
[
]
(
)











0
0
0
0
1
0
0
0


(7.73)
which, if passed through a filter H
A
( )
( )exp
( )





, will generate
s n
A l
j n
n
l
l
l
T
[ ]
(
)exp{ [(
)
(
)]}









0
0
0
0
0
1
0

(7.74)
Comparing Eq. (7.66) with (7.74), the phases of our sinusoidal model are given by



l
n
l
l
 

0
0
0
(
)
(7.75)
Since the sinusoidal model has too many parameters to lead to low-rate coding, a
common technique is to not encode the phases. In Chapter 6 we show that if a system is con-
sidered minimum phase, the phases can be uniquely recovered from knowledge of the mag-
nitude spectrum.
The magnitude spectrum is known at the pitch harmonics, and the remaining values
can be filled in by interpolation: e.g., linear or cubic splines [36]. This interpolated magni-
tude spectrum can be approximated through the real cepstrum:
| ~( )|
cos(
)
A
c
c
k
k
k
K





0
1
2
(7.76)
and the phase, assuming a minimum phase system, is given by

364
Speech Coding
~( )
sin(
)
 

 

2
1
c
k
k
k
K
(7.77)
The phase  0( )t
of the first harmonic between frames (k – 1) and k can be obtained
from the instantaneous frequency  0( )t



0
0
0
1
1
( )
((
)
)
( )
(
)
t
k
N
t dt
k
N
t



z
(7.78)
if we assume the frequency  0( )t in that region to vary linearly between frames (k – 1) and
k:




0
0
1
0
0
1
( )t
N
t
k
k
k





(7.79)
and insert Eq. (7.79) into (7.78), evaluating at t
kN

, to obtain





0
0
0
0
1
0
1
2
k
k
k
kN
k
N
N






(
)
((
)
)
(
)(
/ )
(7.80)
the phase of the sinusoid at  0 as a function of the fundamental frequencies at frames (k –
1), k and the phase at frame (k – 1):



l
k
k
k
l
l


 (
)
0
0
(7.81)
The phases computed by Eqs. (7.80) and (7.81) are a good approximation in practice
for perfectly voiced sounds. For unvoiced sounds, random phases are needed, or else the
reconstructed speech sounds buzzy. Voiced fricatives and many voiced sounds have an aspi-
ration component, so that a mixed excitation is needed to represent them. In these cases, the
source is split into different frequency bands and each band is classified as either voiced or
unvoiced. Sinusoids in voiced bands use the phases described above, whereas sinusoids in
unvoiced bands have random phases.
7.5.2.3.
Parameter Quantization
To quantize the sinusoid amplitudes, we can use an LPC fitting and then quantize the line
spectral frequencies. Also we can do a cepstral fit and quantize the cepstral coefficients. To
be more effective, a mel scale should be used.
While these approaches help in reducing the number of parameters and in quantizing
those parameters, they are not the most effective way of quantizing the sinusoid amplitudes.
A technique called Variable-Dimension Vector Quantization (VDVQ) [12] has been devised
to address this. Each codebook vector ci has a fixed dimension N determined by the length
of the FFT used. The vector of sinusoid amplitudes A has a dimension l that depends on the
number of harmonics and thus the pitch of the current frame. To compute the distance be-
tween A and ci , the codebook vectors are resampled to a size l and the distance is computed
between two vectors of dimension l. Euclidean distance of the log-amplitudes is often used.
In this method, only the distance at the harmonics is evaluated instead of the distance at the
points in the envelope that are actually not present in the signal. Also, this technique does

Low-Bit Rate Speech Coders
365
not suffer from inaccuracies of the model used, such as the inability of linear predictive cod-
ing to model nasals.
7.5.3.
Waveform Interpolation
The main idea behind waveform interpolation (WI) [29] is that the pitch pulse changes
slowly over time for voiced speech. During voiced segments, the speech signal is nearly
periodic. WI coders can operate as low as 2.4 kbps.
Starting at an arbitrary time instant, it is easy to identify a first pitch cycle x n
1[ ], a sec-
ond x n
2[ ], a third x n
3[ ], and so on. We then express our signal x n
[ ] as a function of these
pitch cycle waveforms x n
m[ ]
x n
x
n
t
m
m
m
[ ]
[
]





(7.82)
where P
t
t
m
m
m


1 is the pitch period at time tm in samples, and the pitch cycle is a win-
dowed version of the input
x n
w n x n
m
m
[ ]
[ ] [ ]

(7.83)
—for example, with a rectangular window. To transmit the signal in a lossless fashion we
need to transmit all pitch waveforms x n
m[ ].
If the signal is perfectly periodic, we need to transmit only one pitch waveform x n
m[ ]
and the pitch period P. In practice, voiced signals are not perfectly periodic, so that we need
to transmit more than just one pitch waveform. On the other hand, voiced speech is nearly
periodic, and consecutive pitch waveforms are very similar. Thus, we probably do not need
to transmit all, and we could send every other pitch waveform, for example.
It is convenient to define a two-dimensional surface u n l
[ , ] (shown in Figure 7.10) such
that the pitch waveform x n
m[ ] can be obtained as
x n
u n t
m
m
[ ]
[ ,
]

(7.84)
so that u n l
[ , ] is defined for l
tm

, with the remaining points been computed through interpo-
lation. A frequency representation of the pitch cycle can also be used instead of the time
pitch cycle.
This surface can then be sampled at regular time intervals l
sT

. It has been shown
empirically that transmitting the pitch waveform x n
s[ ] about 40 times per second (a 25-ms
interval is equivalent to T = 200 samples for an
8000 Hz
sF =
sampling rate) is sufficient
for voiced speech. The so-called slowly evolving waveform (SEW) ~[ , ]
u n l can be generated
by low-pass filtering u n l
[ , ] along the l-axis:
x n
u n sT
h sT
t
u n t
h sT
t
s
m
m
m
m
m
[ ]
~[ ,
]
[
] [ ,
]
[
]






(7.85)

366
Speech Coding
where h n
[ ] is a low-pass filter and x n
s[ ] is a sampled version of ~[ , ]
u n l .
Figure 7.10 LP residual signal and its associated surface
( , )
u t φ . In the φ axis we have a
normalized pitch pulse at every given time t. Decomposition of the surface into a slowly evolv-
ing waveform and a rapidly evolving waveform (After Kleijn [30], reprinted by permission of
IEEE).

Low-Bit Rate Speech Coders
367
The decoder has to reconstruct each pitch waveform x n
m[ ] from the SEW x n
s[ ] by in-
terpolation between adjacent pitch waveforms, and thus the name waveform interpolation
(WI) coding:
~ [ ]
~[ ,
]
[
]
[ ]
[
]
w n
u n t
h t
sT w n
h t
sT
m
m
m
s
s
m
s






(7.86)
If the sampling period is larger than the local pitch period (T
Pm

), perfect reconstruc-
tion will not be possible, and there will be some error in the approximation
x
n
x n
x n
m
m
m
[ ]
~ [ ]
 [ ]


(7.87)
or alternatively in the two-dimensional representation
u n l
u n l
u n l
[ , ]
~[ , ]
[ , ]


(7.88)
where  [ ]
x n
m
and [ , ]
u n l represent the rapidly evolving waveforms (REW).
Since this technique can also be applied to unvoiced speech, where the concept of
pitch waveform doesn’t make sense, the more general term characteristic waveform is used
instead. For unvoiced speech, an arbitrary period of around 100 Hz can be used.
For voiced speech, we expect the rapidly varying waveform [ , ]
u n l in Eq. (7.88) to
have much less energy than the slowly evolving waveform ~[ , ]
u n l . For unvoiced speech the
converse is true: [ , ]
u n l has more energy than ~[ , ]
u n l . For voiced fricatives, both components
may be comparable and thus we want to transmit both.
In Eqs. (7.85) and (7.86) we need to average characteristic waveforms that have, in
general, different lengths. To handle this, all characteristic waveforms are typically normal-
ized in length prior to the averaging operation. This length normalization is done by padding
with zeros x n
m[ ] to a certain length M, or truncating x n
m[ ] if P
M
m 
. Another possible nor-
malization is done via linear resampling. This decomposition is shown in Figure 7.10.
Another representation uses the Fourier transform of x n
m[ ]. This case is related to the
harmonic model of Section 7.5.2. In the harmonic model, a relatively long window is needed
to average the several pitch waveforms within the window, whereas this waveform interpo-
lation method has higher time resolution. In constructing the characteristic waveforms we
have implicitly used a rectangular window of length one pitch period, but other windows can
be used, such as a Hanning window that covers two pitch periods. This frequency-domain
representation offers advantages in coding both the SEW and the REW, because properties
of the human auditory system can help reduce the bit rate. This decomposition is often done
on the LPC residual signal.
In particular, the REW [ , ]
u n l has the characteristics for noise, and as such only a rough
description of its power spectral density is needed. At the decoder, random noise is gener-
ated with the transmitted power spectrum. The spectrum of [ , ]
u n l can be vector quantized to
as few as eight shapes with little or no degradation.
The SEW ~[ , ]
u n l is more important perceptually, and for high quality the whole shape
needs to be transmitted. Higher accuracy is desired at lower frequencies so that a perceptual

368
Speech Coding
frequency scale (mel or Bark) is often used. Since the magnitude of ~[ , ]
u n l is perceptually
more important than the phase, for low bit rates the phase of the SEW is not transmitted. The
magnitude spectrum can be quantized with the VDVQ described in Section 7.5.2.3.
To obtain the characteristic waveforms, the pitch needs to be computed. We can find
the pitch period such that the energy of the REW is minimized. To do this we use the ap-
proaches described in Chapter 6. Figure 7.11 shows a block diagram of the encoder and
Figure 7.12 of the decoder.
Figure 7.11 Block diagram of the WI encoder.
Figure 7.12 Block diagram of the WI decoder.
Parameter estimation using an analysis-by-synthesis framework [21] can yield better
results than the open-loop estimation described above.
LSF
LPC filtering
LSF quantization
LPC
analysis
Pitch
tracking
Characteristic
waveform
extraction
residual
speech
pitch
SEW
REW
Quantization
gain
Lowpass
Highpass
Quantization
Quantization
Quantization
LPC filtering
speech
REW
Random phase
+
SEW
LSF
+
gain
Pitch pulse
pitch

Historical Perspective and Further Reading
369
7.6.
HISTORICAL PERSPECTIVE AND FURTHER READING
This chapter is only an introduction to speech and audio coding technologies. The reader is
referred to [27, 32, 41, 52] for coverage in greater depth. A good source of the history of
speech coding can be found in [20].
In 1939, Homer Dudley of AT&T Bell Labs first proposed the channel vocoder [15],
the first analysis-by-synthesis system. This vocoder analyzed slowly varying parameters for
both the excitation and the spectral envelope. Dudley thought of the advantages of band-
width compression and information encryption long before the advent of digital communica-
tions.
PCM was first conceived in 1937 by Alex Reeves at the Paris Laboratories of AT&T,
and it started to be deployed in the United States Public Switched Telephone Network in
1962. The digital compact disc, invented in the late 1960s by James T. Russell and intro-
duced commercially in 1984, also uses PCM as coding standard. The use of µ-law encoding
was proposed by Smith [51] in 1957, but it wasn’t standardized for telephone networks
(G.711) until 1972. In 1952, Schouten et al. [47] proposed delta modulation and Cutler [11]
invented differential PCM. ADPCM was developed by Barnwell [6] in 1974.
Speech coding underwent a fundamental change with the development of linear pre-
dictive coding in the early 1970s. Atal [3] proposed the LPC vocoder in 1971, and then
CELP [5] in 1984. The majority of coding standards for speech signals today use a variation
on CELP.
Sinusoidal coding [35] and waveform interpolation [29] were developed in 1986 and
1991, respectively, for low-bit-rate telephone speech. Transform coders such as MP3 [23],
MPEG II, and Perceptual Audio Coder (PAC) [28] have been used primarily in audio coding
for high-fidelity applications.
Recently, researchers have been improving the technology for cellular communica-
tions by trading off source coding and channel coding. For poor channels more bits are allo-
cated to channel coding and fewer to source coding to reduce dropped calls. Scalable coders
that have different layers with increased level of precision, or bandwidth, are also of great
interest.
REFERENCES
[1]
Adoul, J.P., et al., "Fast CELP Coding Based on Algebraic Codes," Int. Conf. on Acoustics,
Speech and Signal Processing, 1987, Dallas, TX pp. 1957-1960.
[2]
Atal, B.S., R.V. Cox, and P. Kroon, "Spectral Quantization and Interpolation for CELP Cod-
ers," Int. Conf. on Acoustics, Speech and Signal Processing, 1989, Glasgow pp. 69-72.
[3]
Atal, B.S. and L. Hanauer, "Speech Analysis and Synthesis by Linear Prediction of the
Speech Wave," Journal of the Acoustical Society of America, 1971, 50, pp. 637-655.
[4]
Atal, B.S. and M.R. Schroeder, "Predictive Coding of Speech Signals and Subjective Error
Criteria," IEEE Trans. on Acoustics, Speech and Signal Processing, 1979, ASSP-27(3), pp.
247-254.
[5]
Atal, B.S. and M.R. Schroeder, "Stochastic Coding of Speech at Very Low Bit Rates," Proc.
Int. Conf. on Comm., 1984, Amsterdam pp. 1610-1613.

370
Speech Coding
[6]
Barnwell, T.P., et al., Adaptive Differential PCM Speech Transmission, 1974, Rome Air
Development Center.
[7]
Benvenuto, N., G. Bertocci, and W.R. Daumer, "The 32-kbps ADPCM Coding Standard,"
AT&T Technical Journal, 1986, 65, pp. 12-22.
[8]
Campbell, J.P., T.E. Tremain, and V.C. Welch, "The DoD 4.8 kbps Standard (Proposed Fed-
eral Standard 1016)" in Advances in Speech Coding, B. Atal, V. Cuperman, and A. Gersho,
eds. 1991, pp. 121-133, Kluwer Academic Publishers.
[9]
Chen, J.H., et al., "A Low-Delay CELP Coder for the CCITT 16 kbps Speech Coding Stan-
dard," IEEE Journal on Selected Areas Communcations, 1992, 10(5), pp. 830-849.
[10]
Chen, J.H. and A. Gersho, "Adaptive Postfiltering for Quality Enhancement of Coded
Speech," IEEE Trans. on Speech and Audio Processing, 1995, 3(1), pp. 59-71.
[11]
Cutler, C.C., Differential Quantization for Communication Signals, , 1952, US Patent
2,605,361.
[12]
Das, A. and A. Gersho, "Variable Dimension Vector Quantization," IEEE Signal Processing
Letters, 1996, 3(7), pp. 200-202.
[13]
Daumer, W.R., et al., "Overview of the 32kbps ADPCM Algorithm," Proc. IEEE Global
Telecomm, 1984 pp. 774-777.
[14]
DeJaco, P.J.A., W. Gardner, and C. Lee, "QCELP: The North American CDMA Digital
Cellular Variable Speech Coding Standard," Proc. Workshop on Speech Coding for Tele-
communications, 1993, Sainte Adele, Quebec pp. 5-6.
[15]
Dudley, H., "The Vocoder," Bell Labs Record, 1939, 17, pp. 122-126.
[16]
Erdmann, C., et al., "An Adaptive Rate Wideband Speech Codec with Adaptive Gain Re-
Quantization," IEEE Workshop on Speech Coding, 2000, Delavan, Wisconsin.
[17]
Etemoglu, C.O., V. Cuperman, and A. Gersho, "Speech Coding with an Analysis-by-
Synthesis Sinusoidal Model," Int. Conf. on Acoustics, Speech and Signal Processing, 2000,
Istanbul, Turkey pp. 1371-1374.
[18]
Gerson, I.A. and M.A. Jasiuk, "Vector Sum Excited Linear Prediction (VSELP)" in Ad-
vances in Speech Coding, B.S. Atal, V. Cuperman, and A. Gersho, eds. 1991, Boston, MA,
pp. 69-79, Kluwer Academic Publishers.
[19]
Gerson, I.A. and M.A. Jasiuk., "Techniques for Improving the Performance of CELP-type
Speech Coders," IEEE Journal Selected Areas Communications, 1991, 10(5), pp. 858-865.
[20]
Gold, B. and N. Morgan, Speech and Audio Signal Processing: Processing and Perception
of Speech and Music, 2000, New York, John Wiley.
[21]
Gottesman, O. and A. Gersho, "High Quality Enhanced Waveform Interpolative Coding at
2.8 kbps," Int. Conf. on Acoustics, Speech and Signal Processing, 2000, Istanbul, Turkey pp.
1363-1366.
[22]
Greefkes, J.A., "A Digitally Companded Delta Modulation Modem for Speech Transmis-
sion," Proc. Int. Conf. on Communications, 1970 pp. 7.33-7.48.
[23]
ISO, Coding of Moving Pictures and Associated Audio - Audio Part, 1993, Int. Standards
Organization.
[24]
ISO/IEC, Information Technology - Coding of Moving Pictures and Associated Audio for
Digital Storage Media at up to about 1.5 Mbps, Part 3: Audio (MPEG-1), 1992, Int. Stan-
dards Organization.
[25]
ITU-T, Methods for Subjective Determination of Transmission Quality, 1996, Int. Telecom-
munication Unit.
[26]
Jarvinen, K., et al., "GSM Enhanced Full Rate Speech Codec," Int. Conf. on Acoustics,
Speech and Signal Processing, 1997, Munich, Germany pp. 771-774.
[27]
Jayant, N.S. and P. Noll, Digital Coding of Waveforms, 1984, Upper Saddle River, NJ, Pren-
tice Hall.

Historical Perspective and Further Reading
371
[28]
Johnston, J.D., et al., "ATT Perceptual Audio Coding (PAC)" in Audio Engineering Society
(AES) Collected Papers on Digital Audio Bit Rate Reduction, N. Gilchrist and C. Grewin,
eds. 1996, pp. 73-82.
[29]
Kleijn, W.B., "Continuous Representations in Linear Predictive Coding," Int. Conf. on
Acoustics, Speech and Signal Processing, 1991, Toronto, Canada pp. 201-204.
[30]
Kleijn, W.B. and J. Haagen, "Transformation and Decomposition of the Speech Signal for
Coding," IEEE Signal Processing Letters, 1994, 1, pp. 136-138.
[31]
Kleijn, W.B., D.J. Krasinski, and R.H. Ketchum, "An Efficient Stochastically Excited Linear
Predictive Coding Algorithm for High Quality Low Bit Rate Transmission of Speech,"
Speech Communication, 1988, 7, pp. 305-316.
[32]
Kleijn, W.B. and K.K. Paliwal, Speech Coding and Synthesis, 1995, Amsterdam, Nether-
lands, Elsevier.
[33]
Koishida, K., V. Cuperman, and A. Gersho, "A 16-KBIT/S Bandwidth Scalable Audio
Coder Based on the G.729 Standard," Int. Conf. on Acoustics, Speech and Signal Processing,
2000, Istanbul, Turkey pp. 1149-1152.
[34]
Li, C. and V. Cuperman, "Analysis-by-Synthesis Multimode Harmonic Speech Coding at 4
kbps," Int. Conf. on Acoustics, Speech and Signal Processing, 2000, Istanbul, Turkey pp.
1367-1370.
[35]
McAulay, R.J. and T.F. Quateri, "Speech Analysis/Synthesis Based on a Sinusoidal Repre-
sentation," IEEE Trans. on Acoustics, Speech and Signal Processing, 1986, 34, pp. 744-754.
[36]
McAulay, R.J. and T.F. Quateri, "Sinusoidal Coding" in Speech Coding and Synthesis, W.B.
Kleijn and K.K. Paliwal, eds. 1995, pp. 121-174, Elsevier.
[37]
McCree, A., "A 14 kbps Wideband Speech Coder with a Parametric Highband Model," Int.
Conf. on Acoustics, Speech and Signal Processing, 2000, Istanbul, Turkey pp. 1153-1156.
[38]
McCree, A.V. and T.P. Barnwell, "Improving the Performance of a Mixed-Excitation LPC
Vocoder in Acoustic Noise," Int. Conf. on Acoustics, Speech and Signal Processing, 1992,
San Francisco pp. II-137-138.
[39]
McCree, A.V., et al., "A 2.4 kbit/s MELP Coder Candidate for the New U.S. Federal Stan-
dard," Int. Conf. on Acoustics, Speech and Signal Processing, 1996, Atlanta, GA pp. 200-
203.
[40]
Paez, M.D. and T.H. Glisson, "Minimum Squared-Error Quantization in Speech," IEEE
Trans. on Comm, 1972, 20, pp. 225-230.
[41]
Painter, T. and A. Spanias, "A Review of Algorithms for Perceptual Coding of Digital Audio
Signals," Proc. Int. Conf. on DSP, 1997 pp. 179-205.
[42]
Painter, T. and A. Spanias, "Perceptual Coding of Digital Audio," Proc. of IEEE,
2000(April), pp. 451-513.
[43]
Paliwal, K.K. and B. Atal, "Efficient Vector Quantization of LPC Parameters at 24
Bits/Frame," IEEE Trans. on Speech and Audio Processing, 1993, 1(1), pp. 3-14.
[44]
Prevez, M.A., H.V. Sorensen, and J.V.D. Spiegel, "An Overview of Sigma-Delta Convert-
ers," IEEE Signal Processing Magazine, 1996, 13(1), pp. 61-84.
[45]
Robinson, T., Simple Lossless and Near-Lossless Waveform Compression, 1994, Cambridge
University Engineering Department.
[46]
Salami, R., C. Laflamme, and B. Bessette, "Description of ITU-T Recommendation G.729
Annex A: Reduced Complexity 8 kbps CS-ACELP Codec," Int. Conf. on Acoustics, Speech
and Signal Processing, 1997, Munich, Germany pp. 775-778.
[47]
Schouten, J.S., F.E. DeJager, and J.A. Greefkes, Delta Modulation, a New Modulation Sys-
tem for Telecommunications, 1952, Phillips, pp. 237-245.

372
Speech Coding
[48]
Shlomot, E., V. Cuperman, and A. Gersho, "Combined Harmonic and Waveform Coding of
Speech at Low Bit Rates," Int. Conf. on Acoustics, Speech and Signal Processing, 1998, Se-
attle, WA pp. 585-588.
[49]
Singhal, S. and B.S. Atal, "Improving Performance of Multi-Pulse LPC Coders at Low Bit
Rates," Int. Conf. on Acoustics, Speech and Signal Processing, 1984, San Diego pp. 1.3.1-
1.3.4.
[50]
Skoglund, J., R. Cox, and J. Collura, "A Combined WI and MELP Coder at 5.2KBPS," Int.
Conf. on Acoustics, Speech and Signal Processing, 2000, Istanbul, Turkey pp. 1387-1390.
[51]
Smith, B., "Instantaneous Companding of Quantized Signals," Bell Systems Technical Jour-
nal, 1957, 36(3), pp. 653-709.
[52]
Spanias, A.S., "Speech Coding: A Tutorial Review," Proc. of the IEEE, 1994, 82(10), pp.
1441-1582.
[53]
Stachurski, J. and A. McCree, "A 4 kbps Hybrid MELP/CELP Coder with Alignment Phase
Encoding and Zero Phase Equalization," Int. Conf. on Acoustics, Speech and Signal Process-
ing, 2000, Istanbul, Turkey pp. 1379-1382.
[54]
Todd, C., "AC-3: Flexible Perceptual Coding for Audio Transmission and Storage," Audio
Engineering Society 96th Convention, 1994.
[55]
Tremain, T.E., The Government Standard Linear Predictive Coding Algorithm, in Speech
Technology Magazine, 1982. pp. 40-49.
[56]
Vary, P., et al., "A Regular-Pulse Excited Linear Predictive Code," Speech Communication,
1988, 7(2), pp. 209-215.
[57]
Wang, T., et al., "A 1200 BPS Speech Coder Based on MELP," Int. Conf. on Acoustics,
Speech and Signal Processing, 2000, Istanbul, Turkey pp. 1375-1378.

375
C H A P T E R
8
Hidden Markov ModelsEquation Section 8
The hidden Markov model (HMM) is a very
powerful statistical method of characterizing the observed data samples of a discrete-time
series. Not only can it provide an efficient way to build parsimonious parametric models, but
can also incorporate the dynamic programming principle in its core for a unified pattern
segmentation and pattern classification of time-varying data sequences. The data samples in
the time series can be discretely or continuously distributed; they can be scalars or vectors.
The underlying assumption of the HMM is that the data samples can be well characterized as
a parametric random process, and the parameters of the stochastic process can be estimated
in a precise and well-defined framework. The basic HMM theory was published in a series
of classic papers by Baum and his colleagues [4]. The HMM has become one of the most
powerful statistical methods for modeling speech signals. Its principles have been success-
fully used in automatic speech recognition, formant and pitch tracking, speech enhancement,
speech synthesis, statistical language modeling, part-of-speech tagging, spoken language
understanding, and machine translation [3, 4, 8, 10, 12, 18, 23, 37].

376
Hidden Markov Models
8.1.
THE MARKOV CHAIN
A Markov chain models a class of random processes that incorporates a minimum amount of
memory without being completely memoryless. In this subsection we focus on the discrete-
time Markov chain only.
Let
1
2
,
,
n
X
X
X
=
X

be a sequence of random variables from a finite discrete alpha-
bet O
o
o
o M

1
2
,
,
,

l
q. Based on the Bayes rule, we have
1
1
2
1
1
2
(
,
,
,
)
(
)
(
|
)
n
i
n
i
i
P X
X
X
P X
P X
X −
=
=
∏

(8.1)
where
1
1
1
2
1
,
,
,
i
i
X
X
X
X
−
−
=

. The random variables X are said to form a first-order Markov
chain, provided that
P X
X
P X
X
i
i
i
i
(
|
)
(
|
)
1
1
1



(8.2)
As a consequence, for the first-order Markov chain, Eq. (8.1) becomes
1
2
1
1
2
(
,
,
,
)
(
)
(
|
)
n
n
i
i
i
P X
X
X
P X
P X
X −
=
=
∏

(8.3)
Equation (8.2) is also known as the Markov assumption. This assumption uses very lit-
tle memory to model dynamic data sequences: the probability of the random variable at a
given time depends only on the value at the preceding time. The Markov chain can be used
to model time-invariant (stationary) events if we discard the time index i,
P X
s X
s
P s s
i
i
(
|
')
( | ')



1
(8.4)
If we associate
i
X
to a state, the Markov chain can be represented by a finite state
process with transition between states specified by the probability function P s s
( | '). Using
this finite state representation, the Markov assumption (Eq. (8.2)) is translated to the follow-
ing: the probability that the Markov chain will be in a particular state at a given time depends
only on the state of the Markov chain at the previous time.
Consider a Markov chain with N distinct states labeled by {
}
1,
, N

, with the state at
time t in the Markov chain denoted as st ; the parameters of a Markov chain can be described
as follows:
1
(
|
)
1
,
ij
t
t
a
P s
j s
i
i j
N
−
=
=
=
≤
≤
(8.5)
1
(
)
1
i
P s
i
i
N
π =
=
≤≤
(8.6)
where aij is the transition probability from state i to state j; and π i is the initial probability
that the Markov chain will start in state i. Both transition and initial probabilities are bound
to the constraints:

The Markov Chain
377
1
1
1;
1
1
N
ij
j
N
j
j
a
i
N
π
=
=
=
≤≤
=


(8.7)
The Markov chain described above is also called the observable Markov model be-
cause the output of the process is the set of states at each time instance t, where each state
corresponds to an observable event
i
X . In other words, there is one-to-one correspondence
between the observable event sequence
X
and the Markov chain state sequence
1
2
,
,
n
s s
s
=
S

Consider a simple three-state Markov chain for the Dow Jones Industrial
average as shown in Figure 8.1. At the end of each day, the Dow Jones Industrial average
may correspond to one of the following states:
state 1 – up (in comparison to the index of previous day)
state 2 – down (in comparison to the index of previous day)
state 3 – unchanged (in comparison to the index of previous day)
up
down
unch.
0.2
0.5
0.6
0.3
0.2
0.1
0.4
0.2
0.5
1
2
3
Figure 8.1 A Markov chain for the Dow Jones Industrial average. Three states represent up,
down, and unchanged respectively.
The parameter for this Dow Jones Markov chain may include a state-transition prob-
ability matrix
A
aij

L
N
MMM
O
Q
PPP
n s
0 6
0 2
02
05
0 3
02
0 4
01
05
.
.
.
.
.
.
.
.
.
and an initial state probability matrix

378
Hidden Markov Models
(
)
0.5
0.2
0.3
t
iπ




=
= 





π
Suppose you would like to find out the probability for five consecutive up days. Since
the observed sequence of up-up-up-up-up corresponds to the state sequence of (1, 1, 1, 1, 1),
the probability will be
4
1
11 11
11
11
(5 consecutive
days)
(1,1,1,1,1)
0.5 (0.6)
0.0648
P
up
P
a a a a
π
=
=
=
×
=
8.2.
DEFINITION OF THE HIDDEN MARKOV MODEL
In the Markov chain, each state corresponds to a deterministically observable event; i.e., the
output of such sources in any given state is not random. A natural extension to the Markov
chain introduces a non-deterministic process that generates output observation symbols in
any given state. Thus, the observation is a probabilistic function of the state. This new model
is known as a hidden Markov model, which can be viewed as a double-embedded stochastic
process with an underlying stochastic process (the state sequence) not directly observable.
This underlying process can only be probabilistically associated with another observable
stochastic process producing the sequence of features we can observe.
1
2
3
0.2
0.5
0.6
0.3
0.2
0.1
0.4
0.2
0.5
0 7
01
0 2
.
.
.










01
0 6
0 3
.
.
.










03
03
0 4
.
.
.










initial state prob. =
05
0 2
0 3
.
.
.










P up
P down
P unchanged
(
)
(
)
(
)










output
pdf =
Figure 8.2 A hidden Markov model for the Dow Jones Industrial average. The three states no
longer have deterministic meanings as the Markov chain illustrated in Figure 8.1.
A hidden Markov model is basically a Markov chain where the output observation is a
random variable X generated according to a output probabilistic function associated with

Definition of the Hidden Markov Model
379
each state. Figure 8.2 shows a revised hidden Markov model for the Dow Jones Industrial
average. You see that each state now can generate all three output observations: up, down,
and unchanged according to its output pdf. This means that there is no longer a one-to-one
correspondence between the observation sequence and the state sequence, so you cannot
unanimously determine the state sequence for a given observation sequence; i.e., the state
sequence is not observable and therefore hidden. This is why the world hidden is placed in
front of Markov models. Although the state of an HMM is hidden, it often contains salient
information about the data we are modeling. For example, the first state in Figure 8.2 indi-
cates a bull market, and the second state indicates a bear market as specified by the output
probability in each state. Formally speaking, a hidden Markov model is defined by:
 Ο = o o
oM
1
2
,
,
,

l
q- An output observation alphabet.1 The observation symbols
correspond to the physical output of the system being modeled. In the case of the
Dow Jones Industrial average HMM, the output observation alphabet is the col-
lection of three categories -
{
}
,
,
O
up down unchanged
=
.
   12, ,
,
 N
l
q - A set of states representing the state space. Here st is denoted
as the state at time t . In the case of the Dow Jones Industrial average HMM, the
state may indicate a bull market, a bear market, and a stable market.
 A  aijn s - A transition probability matrix, where aij is the probability of taking
a transition from state i to state j, i.e.,
a
P s
j s
i
ij
t
t




(
|
)
1
(8.8)

{
}
( )
ib k
=
B
- An output probability matrix,2 where
( )
ib k
is the probability of
emitting symbol ok when state i is entered. Let X  X
X
X t
1
2
,
,
,
,

 be the
observed output of the HMM. The state sequence S
s
s
st

1
2
,
,
,
,

 is not ob-
served (hidden), and
( )
ib k
can be rewritten as follows:
( )
(
|
)
i
t
k
t
b k
P X
o
s
i
=
=
=
(8.9)

{ }
i
π
=
π
- A initial state distribution where
π i
P s
i
=
=
(
)
0
1 
i
N
(8.10)
Since aij , b
k
ij ( ), and π i are all probabilities, they must satisfy the following proper-
ties:
1 Although we use the discrete output observation here, we can extend it to the continuous case with a continuous
pdf. You can also use vector quantization to map a continuous vector variable into a discrete alphabet set.
2 The output distribution can also be transition-dependent. Although these two formations look different, the state-
dependent one can be reformulated as a transition-dependent one with the constraint of all the transitions entering
the same state sharing the same output distribution.

380
Hidden Markov Models
0,
( )
0,
0
all , ,
ij
i
i
a
b k
i j k
π
≥
≥
≥
∀
(8.11)
1
1
N
ij
j
a
=
=

(8.12)
1
( )
1
M
i
k
b k
=
=

(8.13)
π i
i
N
=
=
1
1
(8.14)
To sum up, a complete specification of an HMM includes two constant-size parame-
ters, N and M, representing the total number of states and the size of observation alphabets,
observation alphabet O, and three sets (matrices) of probability measures A, B, π . For con-
venience, we use the following notation
( , , )
Φ = A B π
(8.15)
to indicate the whole parameter set of an HMM and sometimes use the parameter set  to
represent the HMM interchangeably without ambiguity.
In the first-order hidden Markov model discussed above, there are two assumptions.
The first is the Markov assumption for the Markov chain.
P s s
P s s
t
t
t
t
(
|
)
(
|
)
1
1
1



(8.16)
where st
1
1

represents the state sequence s s
st
1
2
1
,
,
,

−. The second is the output-
independence assumption:
1
1
1
(
|
,
)
(
|
)
t
t
t
t
t
P X
X
s
P X
s
−
=
(8.17)
where X t
1
1
 represents the output sequence X
X
X t
1
2
1
,
,
,

 . The output-independence as-
sumption states that the probability that a particular symbol is emitted at time t depends only
on the state st and is conditionally independent of the past observations.
You might argue that these assumptions limit the memory of the first-order hidden
Markov models and may lead to model deficiency. However, in practice, they make evalua-
tion, decoding, and learning feasible and efficient without significantly affecting the model-
ing capability, since those assumptions greatly reduce the number of parameters that need to
be estimated.
Given the definition of HMMs above, three basic problems of interest must be ad-
dressed before they can be applied to real-world applications.

Definition of the Hidden Markov Model
381
1. The Evaluation Problem – Given a model  and a sequence of observa-
tions X  (
,
,
,
)
X
X
X T
1
2 
, what is the probability P( |
)
X  ; i.e., the prob-
ability of the model that generates the observations?
2. The Decoding Problem – Given a model  and a sequence of observations
X  (
,
,
,
)
X
X
X T
1
2 
,
what
is
the
most
likely
state
sequence
S  (
,
,
,
,
)
s
s
s
sT
0
1
2 
in the model that produces the observations?
3. The Learning Problem – Given a model  and a set of observations, how
can we adjust the model parameter  to maximize the joint probability (like-
lihood)
(
|
)
P
Φ
∏
X
X
?
If we could solve the evaluation problem, we would have a way of evaluating how
well a given HMM matches a given observation sequence. Therefore, we could use HMM to
do pattern recognition, since the likelihood P( |
)
X  can be used to compute posterior prob-
ability P( |
)
 X , and the HMM with highest posterior probability can be determined as the
desired pattern for the observation sequence. If we could solve the decoding problem, we
could find the best matching state sequence given an observation sequence, or, in other
words, we could uncover the hidden state sequence. As discussed in Chapters 12 and 13,
these are the basics for the decoding in continuous speech recognition. Last but not least, if
we could solve the learning problem, we would have the means to automatically estimate the
model parameter Φ from an ensemble of training data. These three problems are tightly
linked under the same probabilistic framework. The efficient implementation of these algo-
rithms shares the principle of dynamic programming that we briefly discuss next.
8.2.1.
Dynamic Programming and DTW
The dynamic programming concept, also known as dynamic time warping (DTW) in speech
recognition [40], has been widely used to derive the overall distortion between two speech
templates. In these template-based systems, each speech template consists of a sequence of
speech vectors. The overall distortion measure is computed from the accumulated distance
between two feature vectors that are aligned between two speech templates with minimal
overall distortion. The DTW method can warp two speech templates (
1
2...
N
x x
x ) and
(
1
2...
M
y y
y
) in the time dimension to alleviate nonlinear distortion as illustrated in Figure
8.3.
This is roughly equivalent to the problem of finding the minimum distance in the trellis
between these two templates. Associated with every pair (i, j) is a distance
( , )
d i j
between
two speech vectors
ix and
j
y . To find the optimal path between starting point (1, 1) and end
point (N, M) from left to right, we need to compute the optimal accumulated dis-
tance
(
,
)
D N M . We can enumerate all possible accumulated distance from (1,1) to (N, M)
and identify the one that has the minimum distance. Since there are M possible moves for

382
Hidden Markov Models
each step from left to right in Figure 8.3, all the possible paths from (1, 1) to (N, M) will be
exponential. Dynamic programming principle can drastically reduce the amount of computa-
tion by avoiding the enumeration of sequences that cannot possibly be optimal. Since the
same optimal path after each step must be based on the previous step, the minimum distance
( , )
D i j
must satisfy the following equation:
[
]
( , )
min
(
1, )
( , )
k
D i j
D i
k
d k j
=
−
+
(8.18)
y1
y2
yM
x2
x1
xN
d(2,2)
Optimal alignment
between X and Y
Figure 8.3 Direct comparison between two speech templates X=
1
2...
N
x x
x
and Y=
1
2...
M
y y
y
.
Equation (8.18) indicates you only need to consider and keep only the best move for
each pair although there are M possible moves. The recursion allows the optimal path search
to be conducted incrementally from left to right. In essence, dynamic programming delegates
the solution recursively to its own sub-problem. The computation proceeds from the small
sub-problem (
(
1, )
D i
k
−
) to the larger sub-problem (
( , )
D i j ). We can identify the optimal
match
j
y with respect to
ix and save the index in a back pointer table B(i, j) as we move
forward. The optimal path can be back traced after the optimal path is identified. The algo-
rithm is described in Algorithm 8.1.
The advantage of the dynamic programming lies in the fact that once a sub-problem is
solved, the partial result can be stored and never needs to be recalculated. This is a very im-
portant principle that you see again and again in building practical spoken language systems.
Speech recognition based on DTW is simple to implement and fairly effective for
small-vocabulary speech recognition. Dynamic programming can temporally align patterns
to account for differences in speaking rates across talkers as well as across repetitions of the
word by the same talker. However, it does not have a principled way to derive an averaged
template for each pattern from a large amount of training samples. A multiplicity of refer-

Definition of the Hidden Markov Model
383
ence training tokens is typically required to characterize the variation among different utter-
ances. As such, the HMM is a much better alternative for spoken language processing.
ALGORITHM 8.1: THE DYNAMIC PROGRAMMING ALGORITHM
Step 1: Initialization
(1,1)
(1,1), (1,1)
1
D
d
B
=
= , for
2,
,
j
M
=

compute
(1, )
D
j = ∞
Step 2: Iteration
for
2,
,
i
N
=

{
for
1,
,
j
M
=

compute {
[
]
1
( , )
min
(
1, )
( , )
p M
D i j
D i
p
d p j
≤≤
=
−
+
[
]
1
( , )
argmin
(
1, )
( , )
p M
B i j
D i
p
d p j
≤≤
=
−
+
}}
Step 3: Backtracking and Termination
The optimal (minimum) distance is
(
,
)
D N M and the optimal path is
1
2
( ,
,
,
)
N
s s
s

where
Ns
M
=
and
1
(
1,
)
i
i
s
B i
s +
=
+
,
1,
2,
,1
i
N
N
=
−
−

8.2.2.
How to Evaluate an HMM – The Forward Algorithm
To
calculate
the
probability
(likelihood)
P( |
)
X 
of
the
observation
sequence
X  (
,
,
,
)
X
X
X T
1
2 
, given the HMM , the most intuitive way is to sum up the prob-
abilities of all possible state sequences:
all
(
|
)
( |
) (
| ,
)
P
P
P
Φ =
Φ
Φ

S
X
S
X S
(8.19)
In other words, to compute P( |
)
X  , we first enumerate all possible state sequences S
of length T, that generate observation sequence X, and then sum all the probabilities. The
probability of each path S is the product of the state sequence probability (first factor) and
the joint output probability (the second factor) along the path.
For one particular state sequence
1
2
( ,
,
,
)
T
s s
s
=
S

, where
1s is the initial state, the
state-sequence probability in Eq. (8.19) can be rewritten by applying Markov assumption:
1
1 2
1
0 1
1 2
1
1
1
2
( |
)
(
|
)
(
|
,
)
T
T
T
T
T
t
t
s
s s
s
s
s s
s s
s
s
t
P
P s
P s s
a
a
a
a
a
π
−
−
−
=
Φ =
Φ
Φ =
=
∏
S


(8.20)
where
0 1
s s
a
denotes
1s
π
for simplicity. For the same state sequence S , the joint output prob-
ability along the path can be rewritten by applying the output-independent assumption:

384
Hidden Markov Models
1
2
1
1
1
1
2
(
| ,
)
(
|
,
)
(
| ,
)
(
)
(
)
(
)
T
T
T
T
t
t
t
s
s
s
T
P
P X
S
P X s
b
X b
X
b
X
=
Φ =
Φ =
Φ
=
∏
X S

(8.21)
Substituting Eq. (8.20) and (8.21) into (8.19), we get:
0 1
1
1 2
2
1
all
1
2
all
(
|
)
( |
) (
| ,
)
(
)
(
)
(
)
T
T
T
s s
s
s s
s
s
s
s
T
P
P
P
a
b
X a
b
X
a
b
X
−
Φ =
Φ
Φ
=


S
S
X
S
X S

(8.22)
Equation (8.22) can be interpreted as follows. First we enumerate all possible state se-
quence with length T+1. For any given state sequence, we start from initial state
1s with
probability
1s
π
or
0 1
s s
a
. We take a transition from st1 to st with probability as
s
t
t
1
and gen-
erate the observation X t with probability
(
)
ts
t
b
X
until we reach the last transition.
However, direct evaluation of Eq. (8.22) according to the interpretation above requires
enumeration of
(
)
T
O N
possible state sequences, which results in exponential computational
complexity. Fortunately, a more efficient algorithm can be used to calculate Eq. (8.22). The
trick is to store intermediate results and use them for subsequent state-sequence calculations
to save computation. This algorithm is known as the forward algorithm.
Based on the HMM assumptions, the calculation of
1
(
|
,
) (
|
,
)
t
t
t
t
P s
s
P X
s
−Φ
Φ involves
only st1,st , and X t , so, it is possible to compute the likelihood with P( |
)
X  with recur-
sion on t. Let’s define forward probability:
 t
t
t
i
P X
s
i
( )
(
,
|
)


1

(8.23)
 t i( ) is the probability that the HMM is in state i having generated partial observation
1
t
X (namely
1
2...
t
X X
X ).  t i( ) can be calculated inductively as illustrated in Algorithm 8.2.
This inductive procedure shown in Eq. (8.24) can be illustrated in a trellis. Figure 8.4 illus-
trates the computation of forward probabilities  via a trellis framework for the Dow Jones
Industrial average HMM shown in Figure 8.2. Given two consecutive up days for the Dow
Jones Industrial average, we can find the forward probability  based on the model of
Figure 8.2. An arrow in Figure 8.4 indicates a transition from its origin state to its destination
state. The number inside each cell indicates the forward probability  . We start the  cells
from t = 0, where the  cells contains exactly the initial probabilities. The other cells are
computed in a time-synchronous fashion from left to right, where each cell for time t is com-
pletely computed before proceeding to time t+1. When the states in the last column have
been computed, the sum of all probabilities in the final column is the probability of generat-
ing the observation sequence. For most speech problems, we need to have the HMM end in
some particular exit state (a.k.a final state,
F
S ), and we thus have
(
|
)
(
)
T
F
P
s
α
Φ =
X
.

Definition of the Hidden Markov Model
385
It is easy to show that the complexity for the forward algorithm is O N T
(
)
2
rather than
the exponential one. This is because we can make full use of partially computed probabilities
for the improved efficiency.
ALGORITHM 8.2 THE FORWARD ALGORITHM
Step 1: Initialization
1
1
( )
(X )
1
i
i
i
b
i
N
α
π
=
≤≤
Step 2: Induction
1
1
( )
( )
(
)
2
; 1
N
t
t
ij
j
t
i
j
i a
b X
t
T
j
N
α
α −
=


=
≤≤
≤
≤





(8.24)
Step 3: Termination
P
i
T
i
N
( |
)
( )
X  

1
If it is required to end in the final state,
(
|
)
(
)
T
F
P
s
α
Φ =
X
0.35
0.02
0.09
t = 1
X1 = up
0.179
0.008
0.036
t = 2
X2 = up
state 2
state 3
state 1
Figure 8.4 The forward trellis computation for the HMM of the Dow Jones Industrial average.
8.2.3.
How to Decode an HMM - The Viterbi Algorithm
The forward algorithm, in the previous section, computes the probability that an HMM gen-
erates an observation sequence by summing up the probabilities of all possible paths, so it
does not provide the best path (or state sequence). In many applications, it is desirable to
find such a path. As a matter of fact, finding the best path (state sequence) is the cornerstone
for searching in continuous speech recognition. Since the state sequence is hidden (unob-
served) in the HMM framework, the most widely used criterion is to find the state sequence

386
Hidden Markov Models
that has the highest probability of being taken while generating the observation sequence. In
other words, we are looking for the state sequence
1
2
( ,
,
,
)
T
s s
s
=
S

that maximizes
( ,
|
)
P
Φ
S X
. This problem is very similar to the optimal-path problem in dynamic program-
ming. As a consequence, a formal technique based on dynamic programming, known as
Viterbi algorithm [43], can be used to find the best state sequence for an HMM. In practice,
the same method can be used to evaluate HMMs that offers an approximate solution close to
the case obtained using the forward algorithm described in Section 8.2.2.
The Viterbi algorithm can be regarded as the dynamic programming algorithm applied
to the HMM or as a modified forward algorithm. Instead of summing up probabilities from
different paths coming to the same destination state, the Viterbi algorithm picks and remem-
bers the best path. To define the best-path probability:
1
1
1
( )
(
,
,
|
)
t
t
t
t
V i
P X
S
s
i
−
=
=
Φ
(8.24)
( )
tV i
is the probability of the most likely state sequence at time t, which has generated the
observation
1
t
X (until time t) and ends in state i. A similar induction procedure for the
Viterbi algorithm can be described in Algorithm 8.3.
ALGORITHM 8.3 THE VITERBI ALGORITHM
Step 1: Initialization
1
1
( )
1
(
)
i
i
V i
i
N
b X
π
=
≤≤
1( )
0
B i =
Step 2: Induction
1
1
( )
( )
2
; 1
(
)
t
t
ij
i N
j
t
V
j
V
i a
t
T
j
N
Max
b
X
−
≤≤
=
≤
≤
≤
≤




(8.25)
1
1
max
( )
( )
2
; 1
t
t
ij
i N
Arg
B
j
V
i a
t
T
j
N
−
≤≤
=
≤
≤
≤
≤




(8.26)
Step 3: Termination
[
]
1
The best score
( )
t
i N
V i
Max
≤≤
=
[
]
*
1
( )
max
T
T
i N
s
B
i
Arg
≤≤
=
Step 4: Backtracking
*
*
1
1
(
)
1,
2,
,1
t
t
t
s
B
s
t
T
T
+
+
=
=
−
−

*
*
*
*
1
2
(
,
,
,
)
T
s
s
s
=
S

is the best sequence
This Viterbi algorithm can also be illustrated in a trellis framework similar to the one
for the forward algorithm shown in Figure 8.4. Instead of summing up all the paths, Figure
8.5 illustrates the computation of
t by picking the best path in each cell. The number inside

Definition of the Hidden Markov Model
387
each cell indicates the best score
t and the best path leading to each cell is indicated by
solid lines while the rest of the paths are indicated by dashed line. Again, the computation is
done in a time-synchronous fashion from left to right. The complexity for the Viterbi algo-
rithm is also O N T
(
)
2
.
0.35
0.02
0.009
t = 1
X1 = up
0.147
0.007
0.021
t = 2
state 1
X2 = up
state 2
state 3
Figure 8.5 The Viterbi trellis computation for the HMM of the Dow Jones Industrial average.
8.2.4.
How to Estimate HMM Parameters – Baum-Welch Algorithm
It is very important to estimate the model parameters
( , , )
Φ = A B π
to accurately describe
the observation sequences. This is by far the most difficult of the three problems, because
there is no known analytical method that maximizes the joint probability of the training data
in a closed form. Instead, the problem can be solved by the iterative Baum-Welch algorithm,
also known as the forward-backward algorithm.
The HMM learning problem is a typical case of unsupervised learning discussed in
Chapter 4, where the data is incomplete because of the hidden state sequence. The EM algo-
rithm is perfectly suitable for this problem. As a matter of fact, Baum and colleagues used
the same principle as that of the EM algorithm. Before we describe the formal Baum-Welch
algorithm, we first define a few useful terms. In a manner similar to the forward probability,
we define backward probability as:
 t
t
T
t
i
P X
s
i
( )
(
|
,
)


1

(8.27)
where  t i( ) is the probability of generating partial observation
1
T
t
X + (from t+1 to the end)
given that the HMM is in state i at time t,  t i( ) can then be calculated inductively;
Initialization:
( )
1/
1
T i
N
i
N
β
=
≤≤

388
Hidden Markov Models
Induction:
1
1
1
( )
(
)
( )
=
-1
1; 1
N
t
ij
j
t
t
j
i
a b X
j
t T
i
N
β
β
+
+
=


=
…
≤≤





(8.28)
The relationship of adjacent  and  ( t1 &  t and  t &  t1) can be best illus-
trated as shown in Figure 8.6.  is computed recursively from left to right, and  recur-
sively from right to left.
s1
s2
s3
sN
a1j
a2j
a3j
aNj
ai1
ai2
ai3
aiN
s1
s2
s3
sN
si
t-1
t+1
t
αt-1(i)
αt(i)
βt(i)
βt+1(i)
output = Xt
output = Xt+1
Figure 8.6 The relationship of
1
t
α −&
t
α and
tβ &
1
tβ + in the forward-backward algorithm.
Next, we define γ t i j
( , ) , which is the probability of taking the transition from state i
to state j at time t, given the model and observation sequence, i.e.
1
1
1
1
1
1
1
( , )
(
,
|
,
)
(
,
,
|
)
(
|
)
( )
(
)
( )
( )
T
t
t
t
T
t
t
T
t
ij
j
t
t
N
T
k
i j
P s
i s
j X
P s
i s
j X
P X
i a b
X
j
k
γ
α
β
α
−
−
−
=
=
=
=
Φ
=
=
Φ
=
Φ
=

(8.29)
The equation above can be best illustrated as shown in Figure 8.7.
We can iteratively refine the HMM parameter vector
( , , )
Φ = A B π
by maximizing the
likelihood P( |
)
X 
for each iteration. We use ˆΦ to denote the new parameter vector de-
rived from the parameter vector Φ in the previous iteration. According to the EM algorithm

Definition of the Hidden Markov Model
389
of Chapter 4, the maximization process is equivalent to maximizing the following Q-
function:
all
( ,
|
)
ˆ
ˆ
(
,
)
log
( ,
|
)
(
|
)
P
Q
P
P
Φ
Φ Φ =
Φ
Φ

S
X S
X S
X
(8.30)
where P( , |
)
X S  and log
( , |  )
P X S  can be expressed as:
1
1
( ,
|
)
(
)
t
t
t
T
s
s
s
t
t
P
a
b
X
−
=
Φ =∏
X S
(8.31)
1
1
1
log
( ,
|
)
log
log
(
)
t
t
t
T
T
s
s
s
t
t
t
P
a
b
X
−
=
=
Φ =
+


X S
(8.32)
Equation (8.30) can thus be rewritten as
ˆ
ˆ
ˆ
(
,
)
(
,
)
(
,
)
i
j
i
j
Q
Q
Q
Φ Φ =
Φ
+
Φ
a
b
a
b
(8.33)
where
1
(
,
,
|
)
ˆ
ˆ
(
,
)
log
(
|
)
i
t
t
i
ij
i
j
t
P
s
i s
j
Q
a
P
−=
=
Φ
Φ
=
Φ

a
X
a
X
(8.34)
( ,
|
)
ˆ
ˆ
(
,
)
log
( )
(
|
)
j
t
k
t
j
j
j
k
t X
o
P
s
j
Q
b k
P
∈
=
=
Φ
Φ
=
Φ
 
b
X
b
X
(8.35)
s1
s2
s3
sN
a1j
a2j
a3j
aNj
ai1
ai2
ai3
aiN
s1
s2
s3
sN
si
t-1
t+1
αt-1(i)
αt-1(i)
βt(i)
βt+1(i)
output = Xt
aijbj(Xt+1)
sj
t
t-2
Figure 8.7 Illustration of the operations required for the computation of
( , )
t i j
γ
, which is the
probability of taking the transition from state i to state j at time t.

390
Hidden Markov Models
Since we separate the Q-function into three independent terms, the maximization pro-
cedure on Q(
,  )
 
can be done by maximizing the individual terms separately, subject to
probability constraints.
a
i
ij
j
N



1
1
all
(8.36)
1
( )
1
all
M
j
k
b k
j
=
=
∀

(8.37)
Moreover, all these terms in Eqs. (8.34) and (8.35) have the following form:
F x
y
x
i
i
i
( )
log

(8.38)
where
xi
i
 1
By using the Lagrange multipliers, the function above can be proved to achieve maxi-
mum value at
x
y
y
i
i
i
i
 
(8.39)
Using this formation, we obtain the model estimate as3:
1
1
1
1
=1
1
1
1
( ,
,
|
)
( , )
(
|
)
ˆ
1
( ,
|
)
( , )
(
|
)
T
T
t
t
t
t
t
ij
T
T
N
t
t
t
t
k
P
s
i s
j
i j
P
a
P
s
i
i k
P
γ
γ
−
=
=
−
=
=
=
=
Φ
Φ
=
=
=
Φ
Φ




X
X
X
X
(8.40)
1
=1
1
1
( , )
( ,
|
) (
,
)
(
|
)
ˆ ( )
=
1
( ,
|
)
( , )
(
|
)
t
k
T
t
t
t
k
t X
o
i
t
j
T
T
t
t
t
t
i
i j
P
s
j
X o
P
b k
P
s
j
i j
P
γ
δ
γ
∈
=
=
=
=
Φ
Φ
=
=
Φ
Φ
 



X
X
X
X
(8.41)
By carefully examining the HMM re-estimation Eqs. (8.40) and (8.41), you can see
that Eq. (8.40) is essentially the ratio between the expected number of transition from state i
to state j and the expected number of transitions from state i. For the output probability re-
estimation Eq. (8.41), the numerator is the expected number of times the observation data
3 Notice that the initial probability
ˆiπ
can be derived as a special case of the transition probability.
ˆiπ is often
fixed (i.e., ˆ
1
iπ = for the initial state) for most speech applications.

Definition of the Hidden Markov Model
391
emitted from state j with the observation symbol
ko , and the denominator is the expected
number of times the observation data emitted from state j.
According to the EM algorithm, the forward-backward (Baum-Welch) algorithm guar-
antees a monotonic likelihood improvement on each iteration, and eventually the likelihood
converges to a local maximum. The forward-backward algorithm can be described in a way
similar to the general EM algorithm as shown in Algorithm 8.4.
ALGORITHM 8.4 THE FORWARD-BACKWARD ALGORITHM
Step 1: Initialization: Choose an initial estimate Φ .
Step 2: E-step: Compute auxiliary function Q(
,  )
  based on Φ .
Step 3: M-step: Compute ˆΦ according to the re-estimation Eqs. (8.40) and (8.41) to
maximize the auxiliary Q-function.
Step 4: Iteration: Set

  , repeat from step 2 until convergence.
Although the forward-backward algorithm described above is based on one training
observation sequence, it can be easily generalized to multiple training observation sequences
under the independence assumption between these sequences. To train an HMM from M
data sequences is equivalent to finding the HMM parameter vector Φ that maximizes the
joint likelihood:
1
(
|
)
M
i
i
P
=
Φ
∏
X
(8.42)
The training procedure performs the forward-backward algorithm on each independent
observation sequence to calculate the expectations (or sometimes referred to as counts) in
Eqs. (8.40) and (8.41). These counts in the denominator and numerator, respectively, can be
added across M data sequences respectively. Finally, all the model parameters (probabilities)
are normalized to make them sum up to one. This constitutes one iteration of Baum-Welch
re-estimation; iteration continues until convergence. This procedure is practical and useful
because it allows you to train a good HMM in a typical speech recognition scenario where a
large amount of training data is available.
For example, we let
( , )
m
t i j
γ
denote the
( , )
t i j
γ
from the mth data sequence and
m
T
denote the corresponding length, Equation (8.40) can be extended as:
1
1
1
1
1
( , )
ˆ
( , )
m
m
M
T
m
t
m
t
ij
M
T
N
m
t
m
t
k
i j
a
i k
γ
γ
=
=
=
=
=
= 

(8.43)

392
Hidden Markov Models
8.3.
CONTINUOUS AND SEMI-CONTINUOUS HMMS
If the observation does not come from a finite set, but from a continuous space, the discrete
output distribution discussed in the previous sections needs to be modified. The difference
between the discrete and the continuous HMM lies in a different form of output probability
functions. For speech recognition, use of continuous HMMs implies that the quantization
procedure to map observation vectors from the continuous space to the discrete space for the
discrete HMM is no longer necessary. Thus, the inherent quantization error can be elimi-
nated.
8.3.1.
Continuous Mixture Density HMMs
In choosing continuous output probability density functions
( )
jb x , the first candidate is mul-
tivariate Gaussian mixture density functions. This is because they can approximate any con-
tinuous probability density function, as discussed in Chapter 3. With M Gaussian mixture
density functions, we have:
1
1
( )
( ,
,
)
( )
M
M
j
jk
jk
jk
jk
jk
k
k
b
c N
c b
=
=
=
=


x
x µ
Σ
x
(8.44)
where
( ,
,
)
jk
jk
N x µ
Σ
or
( )
jk
b
x
denotes a single Gaussian density function with mean vector
jk
µ
and covariance matrix
jk
Σ
for state j, M denotes the number of mixture-components,
and
jk
c
is the weight for the kth mixture component satisfying
1
1
M
jk
k
c
=
=

(8.45)
To take the same divide and conquer approach as Eq. (8.33), we need to express
( )
jb x
with respect to each single mixture component as:
1
1
1
2
1
1
1
1
1
( ,
|
)
(
)
...
{
(
)
}
t
t
t
t
t
t t
t
t t
T
T
s
s
s
t
t
M
M
M
T
s
s
s k
s
s k
k
k
k
t
p
a
b
a
b
c
−
−
=
=
=
=
=
Φ =
=
∏
  ∏
X S
x
x
(8.46)
In the summand of Eq. (8.46) it can be considered as the summation of density with all
the possible state sequences S and all the possible mixture components K, defined in T as
the Tth Cartesian product of  ={1, 2, ..., M}, as follows:
1
1
( ,
|
)
(
)
t
t
t t
t t
T
s
s
s k
t
s k
t
p
a
b
c
−
=
Φ = ∏
X S,K
x
(8.47)

Continuous and Semi-continuous HMMs
393
Therefore, the joint probability density is
(
|
)
( ,
|
)
T
p
p
∈Ω
Φ =
Φ
 
S
K
X
X S,K
(8.48)
An auxiliary function Q(
,  )
 
of two model points, Φ and ˆΦ , given an observation
X, can be written as:
( ,
|
)
ˆ
ˆ
(
,
)
log (
,
|
)
(
|
)
T
p
Q
p
p
∈Ω
Φ
Φ Φ =
Φ
Φ
 
S
K
X S,K
X S,K
X
(8.49)
From (8.47), the following decomposition can be shown:
1
1
1
1
ˆ
log ( , ,
|
)
ˆ
ˆ
ˆ
log
log
(
)
log
t
t
t t
t t
T
T
T
s
s
s k
t
s k
t
t
t
p
a
b
c
−
=
=
=
Φ
=
+
+



X S K
x
(8.50)
Maximization of the likelihood by way of re-estimation can be accomplished on indi-
vidual parameter sets owing to the separability shown in (8.50). The separation of (8.50) is
the key to the increased versatility of a re-estimation algorithm in accommodating mixture
observation densities. The auxiliary function can be rewritten in a separated form in a similar
manner as Eq. (8.33):
1
1
1
1
1
( , ,
|
)
ˆ
ˆ
(
,
)
log ( , ,
|
)
(
|
)
ˆ
ˆ
ˆ
(
,
)
(
,
)
(
,
)
i
jk
jk
N
N
M
N
M
i
jk
jk
i
j
k
j
k
p
Q
p
p
Q
Q
Q
=
=
=
=
=
Φ
Φ Φ =
Φ
Φ
=
Φ
+
Φ
+
Φ




S
K
a
b
c
X S K
X S K
X
a
b
c
(8.51)
The only difference we have is:
1
ˆ
ˆ
(
,
)
(
,
|
,
)log
(
)
jk
T
jk
t
t
jk
t
t
Q
p s
j k
k
b
=
Φ
=
=
=
Φ

b
b
X
x
,
(8.52)
and
1
ˆ
ˆ
(
,
)
(
,
|
,
)log
jk
T
jk
t
t
jk
t
Q
p s
j k
k
c
=
Φ
=
=
=
Φ

c
c
X
(8.53)
The optimization procedure is similar to what is discussed in the discrete HMM. The
only major difference is
ˆ
(
,
)
jk
jk
Q
Φ
b
b
. Maximization of
ˆ
(
,
)
jk
jk
Q
Φ
b
b
with respect to ˆ
jk
b
is
obtained through differentiation with respect to {
,
}
jk
jk
µ
Σ
that satisfies:
ˆ
ˆ
(
,
)
0
ijk
jk
jk
Q
∇
Φ
=
b
b
b
(8.54)
The solutions are:

394
Hidden Markov Models
1
1
1
1
( ,
,
|
)
( , )
(
|
)
ˆ
(
,
,
|
)
( , )
(
|
)
T
T
t
t
t
t
t
t
t
jk
T
T
t
t
t
t
t
p
s
j k
k
j k
p
p
s
j k
k
j k
p
ζ
ζ
=
=
=
=
=
=
Φ
Φ
=
=
=
=
Φ
Φ




X
x
x
X
µ
X
X
(8.55)
1
1
1
1
(
,
,
|
)
ˆ
ˆ
(
)(
)
(
|
)
ˆ
(
,
,
|
)
(
|
)
ˆ
ˆ
( , )(
)(
)
( , )
T
t
t
t
t
jk
t
jk
t
jk
T
t
t
t
T
t
t
t
jk
t
jk
t
T
t
t
p X s
j k
k
p
p X s
j k
k
p
j k
j k
ζ
ζ
=
=
=
=
=
=
Φ
−
−
Φ
=
=
=
Φ
Φ
−
−
=




x
µ
x
µ
X
Σ
X
x
µ
x
µ
(8.56)
where
( , )
t j k
ζ
is computed as:
1
1
( )
(
)
( )
( ,
,
|
)
( , )
(
|
)
( )
t
ij
jk
jk
t
t
t
t
i
t
N
T
i
i a c b
j
p
s
j k
k
j k
p
i
α
β
ζ
α
−
=
=
=
Φ
=
=
Φ


x
X
X
(8.57)
In a similar manner to the discrete HMM, we can derive reestimation equation for
jk
c
as follows:
1
1
( , )
ˆ
( , )
T
t
t
jk
T
t
t
k
j k
c
j k
ζ
ζ
=
=
= 

(8.58)
8.3.2.
Semi-continuous HMMs
Traditionally, the discrete and the continuous mixture density HMMs have been treated
separately. In fact, the gap between them can be bridged under some minor assumptions with
the so-called semi-continuous or tied-mixture HMM. It assumes the mixture density func-
tions are tied together across all the models to form a set of shared kernels. In the discrete
HMM, the VQ codebook is typically used to map the continuous input feature vector x to
ko , so we can use the discrete output probability distribution
( )
jb k . The codebook can be
essentially regarded as one of such shared kernels. Accordingly, Eq. (8.44) can be modified
as:

Continuous and Semi-continuous HMMs
395
1
1
( )
( ) ( |
)
( ) ( ,
,
)
M
M
j
j
k
j
k
k
k
k
b
b k f
o
b k N
=
=
=
=


x
x
x µ
Σ
(8.59)
where
ko
is the kth codeword,
( )
jb k
is the same output probability distribution in the dis-
crete HMM or the mixture weights for the continuous mixture density function, and
( ,
,
)
k
k
N x µ
Σ
are assumed to be independent of the Markov model and they are shared
across all the Markov models with a very large number of mixtures M.
From the discrete HMM point of view, the needed VQ codebook consists of M con-
tinuous probability density functions, and each codeword has a mean vector and a covari-
ance matrix. Typical quantization produces a codeword index that has minimum distortion to
the given continuous observation x. In the semi-continuous HMM, the quantization opera-
tion produces values of continuous probability density functions
( |
)
k
f
o
x
for all the code-
words
ko . The structure of the semi-continuous model can be roughly the same as that of
the discrete one. However, the output probabilities are no longer used directly as in the dis-
crete HMM. In contrast, the VQ codebook density functions,
( ,
,
)
k
k
N x µ
Σ
, are combined
with the discrete output probability as in Eq. (8.59). This is a combination of discrete model-
dependent weighting coefficients with the continuous codebook probability density func-
tions. Such a representation can be used to re-estimate the original VQ codebook together
with the HMM.
The semi-continuous model also resembles the M-mixture continuous HMM with all
the continuous output probability density functions shared among all Markov states. Com-
pared with the continuous mixture HMM, the semi-continuous HMM can maintain the mod-
eling ability of large-mixture probability density functions. In addition, the number of free
parameters and the computational complexity can be reduced, because all the probability
density functions are tied together, thus providing a good compromise between detailed
acoustic modeling and trainability.
In practice, because M is large, Eq. (8.59) can be simplified by using the L most sig-
nificant values
( |
)
k
f
o
x
for each x without affecting the performance. Experience has
shown that values of L in the range of 1-3% of M are adequate. This can be conveniently
obtained during the VQ operations by sorting the VQ output and keeping the L most signifi-
cant values. Let ( )
x denote the set of L VQ codewords that has the largest
( |
)
k
f
o
x
for the
given x. Then we have:
( )
( )
( |
)
( )
j
k
j
k
b
p
o b k
η
∈
≅
x
x
x
(8.60)
Since the number of mixture components in ( )
x is of lower order than M, Eq. (8.60)
can significantly reduce the amount of computation. In fact, ( )
x is the key to bridge the gap
between the continuous and discrete HMM. If ( )
x contains only the most significant
( |
)
k
p
v
x
(i.e., only the closest codeword to x), the semi-continuous HMM degenerates to
the discrete HMM. On the other hand, a large VQ codebook can be used such that each
Markov state could contain a number of its own codewords (a mixture of probability density

396
Hidden Markov Models
functions). The discrete output probability b
k
ij ( ) thus becomes the mixture weights for each
state. This would go to the other extreme, a standard continuous mixture density model. We
can also define ( )
x
in such a way that we can have partial tying of
( |
)
k
f
o
x
for different
phonetic classes. For example, we can have a phone-dependent codebook.
When we have a tied VQ codebook, re-estimation of these mean vectors and covari-
ance matrices of different models will involve interdependencies. If any observation xt (no
matter what model it is designated for) has a large value of posterior probability
( , )
t j k
ζ
, it
will have a large contribution on re-estimation of parameters of codeword
ko . We can com-
pute the posterior probability for each codeword from
( , )
t j k
ζ
as defined in Eq. (8.57).
( )
(
|
,
)
( , )
t
t
k
t
j
k
p
o
j k
ζ
ζ
=
=
Φ = 
x
X
(8.61)
The re-estimation formulas for the tied mixture can be written as:
1
1
( )
ˆ
( )
T
t
t
t
k
T
t
t
k
k
ζ
ζ
=
=
= 

x
µ
(8.62)
1
1
ˆ
ˆ
( )(
)(
)
ˆ
( )
T
t
t
t
k
t
k
t
k
T
t
t
k
k
ζ
ζ
=
=
−
−
Σ = 

x
µ
x
µ
(8.63)
8.4.
PRACTICAL ISSUES IN USING HMMS
While the HMM provides a solid framework for speech modeling, there are a number of
issues you need to understand to make effective use of spoken language processing. In this
section we point out some of the key issues related to practical applications. For expedience,
we mostly use the discrete HMM as our example here.
8.4.1.
Initial Estimates
In theory, the re-estimation algorithm of the HMM should reach a local maximum for the
likelihood function. A key question is how to choose the right initial estimates of the HMM
parameters so that the local maximum becomes the global maximum.
In the discrete HMM, if a probability is initialized to be zero, it will remain zero for-
ever. Thus, it is important to have a reasonable set of initial estimates. Empirical study has
shown that, for discrete HMMs, you can use a uniform distribution as the initial estimate. It

Practical Issues in Using HMMs
397
works reasonably well for most speech applications, though good initial estimates are always
helpful to compute the output probabilities.
If continuous mixture density HMMs are used, good initial estimates for the Gaussian
density functions are essential. There are a number of ways to obtain such initial estimates:
 You can use the k-means clustering procedure, as used in vector quantization
clustering. The Markov state segmentation can be derived from the discrete
HMM, since it is not very sensitive to the initial parameters. Based on the seg-
mented data, you can use the k-means algorithm to derive needed Gaussian mean
and covariance parameters. The mixture coefficients can be based on the uni-
form distribution.
 You can estimate the semi-continuous HMM from the discrete HMM. You sim-
ply need to estimate an additional covariance matrix for each VQ codeword and
run an additional four or five iterations to refine the semi-continuous HMM
based on the discrete HMM, which typically requires four or five iterations from
the uniform distribution. When the semi-continuous HMM is trained, you take
the top M codewords, and in each Markov state use them as the initial Gaussian
density functions for the continuous density mixture model.
 You can start training a single mixture Gaussian model. You can compute the
parameters from previously segmented data. You can then iteratively split the
Gaussian density function in a way similar to VQ codebook generation. You
typically need two or three iterations to refine the continuous density after each
splitting.
8.4.2.
Model Topology
Speech is a time-evolving nonstationary signal. Each HMM state has the ability to capture
some quasi-stationary segment in the non-stationary speech signal. A left-to-right topology,
as illustrated in Figure 8.8, is a natural candidate to model the speech signal. It has a self-
transition to each state that can be used to model contiguous speech features belonging to the
same state. When the quasi-stationary speech segment evolves, the left-to-right transition
enables a natural progression of such evolution. In such a topology, each state has a state-
dependent output probability distribution that can be used to interpret the observable speech
signal. This topology is, in fact, one of the most popular HMM structures used in state-of-
the-art speech recognition systems.
The state-dependent output probability distribution can be either discrete distributions
or a mixture of continuous density functions. This is a special case of the transition-
dependent output probability distributions. The state-dependent output probabilities can be
regarded as if the transition arch-dependent output probability distributions were tied to-
gether for each state.
For the state-dependent left-to-right HMM, the most important parameter in determin-
ing the topology is the number of states. The choice of model topology depends on available

398
Hidden Markov Models
training data and what the model is used for. If each HMM is used to represent a phone, you
need to have at least three to five output distributions. If such a model is used to represent a
word, more states are generally required, depending on the pronunciation and duration of the
word. For example, the word tetrahydrocannabino should have a large number of states in
comparison to the word a. You may use at least 24 states for the former and threestates for
the latter. If you have the number of states depending on the duration of the signal, you may
want to use 15 to 25 states for each second of speech signal. One exception is that, for si-
lence, you may want to have a simpler topology. This is because silence is stationary, and
one or two states will be sufficient.
Figure 8.8 A typical hidden Markov model used to model phoneme. There are three states (0-
2) and each state has an associated output probability distribution.
In practice, it is convenient to define a null transition. This is convenient if we want to
simply traverse the HMM without consuming any observation symbol. To incorporate the
null arc, you need to slightly modify the basic forward-backward or Viterbi probability equa-
tions, provided that no loops of empty transitions exist. If we denote the empty transition
between state i and j as
ij
aε , they need to satisfy the following constraints:
1,
ij
ij
j
a
a
i
ε
+
=
∀

(8.64)
The forward probability can be augmented as follows:
1
1
1
( )
( )
(
)
( )
1
; 1
N
N
t
t
ij
i
t
t
ij
i
i
j
i a b
i a
t
T
j
N
ε
α
α
α
−
=
=


=
+
≤≤
≤
≤






x
(8.65)
Equation (8.65) appears to have a recursion, but it actually uses the value of the same
time column of
( )
t i
α
, provided that i is already computed, which is easily achievable if we
have a left-to-right empty transitions without loops of empty transitions.
1
a00
a11
a01
a12
b
k
0 ( )
b
k
1( )
b
k
2 ( )
0
2
a22

Practical Issues in Using HMMs
399
8.4.3.
Training Criteria
The argument for maximum likelihood estimation (MLE) is based on an assumption that the
true distribution of speech is a member of the family of distributions used. This amounts to
the assertion that the observed speech is genuinely produced by the HMM being used, and
the only unknown parameters are the values. However, this can be challenged. Typical
HMMs make many inaccurate assumptions about the speech production process, such as the
output-independence assumption, the Markov assumption, and the continuous probability
density assumption. Such inaccurate assumptions substantially weaken the rationale for
maximu likelihood criteria. For instance, although maximum likelihood estimation is consis-
tent (convergence to the true value), it is meaningless to have such a property if the wrong
model is used. The true parameters in such cases will be the true parameters of the wrong
models. Therefore, an estimation criterion that can work well in spite of these inaccurate
assumptions should offer improved recognition accuracy compared with the maximum like-
lihood criterion. These alternative criteria include the MCE and MMIE, as discussed in
Chapter 4. Finally, if we have prior knowledge about the model distribution, we can employ
the Bayes method such as MAP that can effectively combine both the prior and posterior
distributions in a consistent way, which is particularly suitable for adaptation or dealing with
insufficient training data.
Among all these criteria, MLE remains one of the most widely used, because of its
simplicity and superior performance when appropriate assumptions are made about the sys-
tem design. MCE and MMIE work well for small- to medium-vocabulary speech recognition
[2, 26, 36]. You can train a number of other iterations based on the ML estimates. Neither
MCE nor MMIE has been found extremely effective for large-vocabulary speech recogni-
tion. However, it is possible to combine the MMIE or MCE model with the MLE model for
improved performance. This is because the error patterns generated from these different
models are not the same. We can decode the test utterance with these different models and
vote for the most consistent results [15, 25]. We discuss MAP methods in Chapter 9, since it
is mostly helpful for speaker adaptive speech recognition.
8.4.4.
Deleted Interpolation
For improved robustness, it is often necessary to combine well-trained general models (such
as speaker-independent) with those that are less well-trained but more detailed (such as
speaker-dependent). For example, you can typically improve speech recognition accuracy
with speaker-dependent training. Nevertheless, you may not have sufficient data for a par-
ticular speaker so it is desirable to use a speaker-independent model that is more general but
less accurate to smooth the speaker-dependent model. One effective way to achieve robust-
ness is to interpolate both models with a technique called deleted interpolation, in which the
interpolation weights are estimated using cross-validation data. The objective function is to
maximize the probability of the model generating the held-out data.

400
Hidden Markov Models
Now, let us assume that we want to interpolate two sets of models [
( )
A
P x and
( )
B
P x ,
which can be either discrete probability distributions or continuous density functions] to
form an interpolated model
( )
DI
P
x . The interpolation procedure can be expressed as fol-
lows:
(
)
( )
( )
1
( )
DI
A
B
P
P
P
λ
λ
=
+
−
x
x
x
(8.66)
where the interpolation weight λ is what we need to derive from the training data.
Consider that we want to interpolate a speaker-independent model
( )
A
P x
with a
speaker-dependent model
( )
B
P x . If we use speaker-independent data to estimate the interpo-
lation weight, we may not capture needed speaker-specific information that should be re-
flected in the interpolation weights. What is worse is that the interpolation weight for the
speaker-independent model should be equal to 1.0 if we use the same speaker-independent
data from which the model was derived to estimate the interpolation weight. This is because
of the MLE criterion. If we use speaker-dependent data instead, we have the weight for the
speaker-dependent model equal 1.0 without achieving the desired smoothing effect. Thus the
interpolation weights need to be trained using different data or deleted data with the so
called cross-validation method.
ALGORITHM 8.5 DELETED INTERPOLATION PROCEDURE
Step 1: Initialize λ with a guessed estimate.
Step 2: Update λ by the following formula:
(
)
1
1
(
)
1
(
)
1
(
)
ˆ
j
j
n
M
A
j
t
j
j
j
t
A
j
t
B
j
t
P
M
P
P
λ
λ
λ
λ
−
=
=
−
−
=
 
+
−
x
x
x
(8.67)
where
(
)
j
A
j
t
P −
x
and
(
)
j
B
j
t
P −
x
is
( )
A
P x and
( )
B
P x estimated by the entire training corpus
except part j, the deleted part, respectively.;
jn is the total number of data points in part j that
have been aligned to estimate the model; and
j
tx indicates the t-th data point in the j-ths set of
the aligned data.
Step 3: If the new value ˆλ is sufficiently close to the previous value λ , stop. Otherwise, go to
Step 2.
We can have the training data normally divided into M parts, and train a set of
( )
A
P x
and
( )
B
P x
models using the standard EM algorithm from each combination of M-1 parts,
with the deleted part serving as the unseen data to estimate the interpolation weights λ .
These M sets of interpolation weights are then averaged to obtain the final weights.

Practical Issues in Using HMMs
401
In fact, the interpolation weights in Eq. (8.66) are similar to the Gaussian mixture
weights, although
( )
A
P x
and
( )
B
P x
may not be Gaussian density functions. When we have
M sets of data, we can use the same EM algorithm to estimate the interpolation weights as
illustrated in Algorithm 8.5.
The deleted interpolation procedure described above can be applied after each training
iteration. Then, for the following iteration of training, the learned interpolation weights can
be used as illustrated in Eq. (8.66) to compute the forward-backward paths or the Viterbi
maximum path. We can also have more than two distributions interpolated together. Deleted
interpolation has been widely used in both acoustic and language modeling where smoothing
is needed.
8.4.5.
Parameter Smoothing
One simple reality for probabilistic modeling is that as many observations as possible are
required to reliably estimate model parameters. However, in reality, only a finite amount of
training data is available. If the training data are limited, this will result in some parameters
being inadequately trained, and classification based on poorly trained models will result in
higher recognition error rate. There are many possible solutions to address the problem of
insufficient training data:
 You can increase the size of the training data. There is no data like more data.
 You can reduce the number of free parameters to be re-estimated. This has its
limitations, because a number of significant parameters are always needed to
model physical events.
 You can interpolate one set of parameter estimates with another set of parameter
estimates, for which an adequate amount of training data exists. Deleted interpo-
lation discussed in Section 8.4.4, can be used effectively. In the discrete HMM,
one simple approach is to set a floor to both the transition probability and the
output probability in order to eliminate possible zero estimates. The same prin-
ciple applies to the SCHMM as well as the mixing coefficients of the continuous
density HMM. Parameter flooring can be regarded as a special case of interpola-
tion with the uniform distribution.
 You can tie parameters together to reduce the number of free parameters. The
SCHMM is a typical example of such parameter-tying techniques.
For the continuous mixture HMM, you need to pay extra attention to smoothing the
covariance matrices. There are a number of techniques you can use:
 You can interpolate the covariance matrix with those that are better trained or a
priori via the MAP method.
 You can tie the Gaussian covariance matrices across different mixture compo-
nents or across different Markov states. A very general shared Gaussian density
model is discussed in [20].

402
Hidden Markov Models
 You can use the diagonal covariance matrices if the correlation among feature
coefficients is weak, which is the case if you use uncorrelated features such as
the MFCC.
 You can combine these methods together.
In practice, we can reduce the speech recognition error rate by 5-20% with various
smoothing techniques, depending on the available amount of training data.
8.4.6.
Probability Representations
When we compute the forward and backward probabilities in the forward-backward algo-
rithm, they will approach zero in exponential fashion if the observation sequence length, T,
becomes large enough. For sufficiently large T, the dynamic range of these probabilities will
exceed the precision range of essentially any machine. Thus, in practice, it will result in un-
derflow on the computer if probabilities are represented directly. We can resolve this imple-
mentation problem by scaling these probabilities with some scaling coefficient so that they
remain within the dynamic range of the computer. All of these scaling coefficients can be
removed at the end of the computation without affecting the overall precision.
For example, let  t i( ) be multiplied by a scaling coefficient, St:
S
i
t
t
i
 
1/
( )

(8.68)
so that
( )
1
t
t
i
S
i
α
=

for 1

t
T .  t i( ) can also be multiplied by St for 1

t
T . The
recursion involved in computing the forward and backward variables can be scaled at each
stage of time t by St. Notice that  t i( ) and  t i( )are computed recursively in exponential
fashion; therefore, at time t, the total scaled factor applied to the forward variable  t i( ) is
1
( )
t
k
k
Scale
t
S
α
=
=∏
(8.69)
and the total scaled factor applied to the backward variable  t i( ) is
( )
T
k
k t
Scale
t
S
β
=
=∏
(8.70)
This is because the individual scaling factors are multiplied together in the forward
and backward recursion. Let

 t i( ) ,

 t i( ), and
( , )
t i j
γ ′
denote their corresponding scaled
variables, respectively. Note that
( )
( )
( )
( ) (
|
)
T
T
i
i
i
Scale T
i
Scale
T P
α
α
α
α
′
=
=
Φ


X
(8.71)
The scaled intermediate probability,

 t i j
( , ), can then be written as:

HMM Limitations
403
1
1
(
1)
( )
(
)
( )
( )
( , )
( , )
( )
( )
t
ij
j
t
t
t
t
N
T
i
Scale
t
i a b
X
j Scale
t
i j
i j
Scale T
i
α
β
α
α
β
γ
γ
α
−
=
−
′
=
=

(8.72)
Thus, the intermediate probabilities can be used in the same way as unscaled probabili-
ties, because the scaling factor is cancelled out in Eq. (8.72). Therefore, re-estimation formu-
las can be kept exactly except that
(
|
)
P
Φ
X
should be computed according to
(
|
)
( ) /
( )
T
i
P
i
Scale
T
α
α′
Φ = 
X
(8.73)
In practice, the scaling operation need not be performed at every observation time. It
can be used at any scaling interval for which the underflow is likely to occur. In the unscaled
interval, Scale can be kept as unity.
An alternative way to avoid underflow is to use a logarithmic representation for all the
probabilities. This not only ensures that scaling is unnecessary, as underflow cannot happen,
but also offers the benefit that integers can be used to represent the logarithmic values,
thereby changing floating point operations to fixed point ones, which is particularly suitable
for Viterbi-style computation, as Eq. (8.25) requires no probability addition.
In the forward-backward algorithm we need to have probability addition. We can keep
a table on log
log
b
b
P
P
2
1

. If we represent probability P by logb P, more precision can be
obtained by setting b closer to unity. Let us assume that we want to add P1 and P2 and that
P
P
1
2

. We have:
log (
)
log
log (
)
log
log
b
b
b
P
P
P
P
P
b
b
b
1
2
1
1
2
1





(8.74)
If P2 is so many orders of magnitude smaller than P1, adding the two numbers will just
result in P1. We could store all possible values of
2
1
(log
log
)
b
b
P
P
−
. Using logarithms in-
troduces errors for addition operation. In practice, double-precision float representation can
be used to minimize the impact of the precision problems.
8.5.
HMM LIMITATIONS
There are a number of limitations in the conventional HMMs. For example, HMMs assume
the duration follows an exponential distribution, the transition probability depends only on
the origin and destination, and all observation frames are dependent only on the state that
generated them, not on neighboring observation frames. Researchers have proposed a num-
ber of techniques to address these limitations, albeit these solutions have not significantly
improved speech recognition accuracy for practical applications.

404
Hidden Markov Models
8.5.1.
Duration Modeling
One major weaknesses of conventional HMMs is that they do not provide an adequate repre-
sentation of the temporal structure of speech. This is because the probability of state occu-
pancy decreases exponentially with time as shown in Eq. (8.75). The probability of t con-
secutive observations in state i is the probability of taking the self-loop at state i for t times,
which can be written as
d
t
a
a
i
ii
t
ii
( )
(
)


1
(8.75)
1
2
3
1
2
3
(a)
(b)
Figure 8.9 A standard HMM (a) and its corresponding explicit duration HMM (b) where the
self transitions are replaced with the explicit duration probability distribution for each state.
An improvement to the standard HMM results from the use of HMMs with an explicit
time duration distribution for each state [30, 39]. To explain the principle of time duration
modeling, a conventional HMM with exponential state duration density and a time duration
HMM with specified state duration densities (which can be either a discrete distribution or a
continuous density) are illustrated in Figure 8.9. In (a), the state duration probability has an
exponential form as in Eq. (8.75). In (b), the self-transition probabilities are replaced with an
explicit duration probability distribution. At time t, the process enters state i for duration 
with probability density di ( )
 , during which the observations X
X
X
t
t
t



1
2
,
....
 are gener-
ated. It then transfers to state j with transition probability aij only after the appropriate 
observations have occurred in state i. Thus, by setting the time duration probability density
to be the exponential density of Eq. (8.75) the time duration HMM can be made equivalent
to the standard HMM. The parameters di ( )

can be estimated from observations along with
the other parameters of the HMM. For expedience, the duration density is usually truncated
at a maximum duration value Td . To re-estimate the parameters of the HMM with time dura-
tion modeling, the forward recursion must be modified as follows:

HMM Limitations
405







t
t
ij
j
j
t
l
l
i i
j
j
i a d
b
X
( )
( )
( )
(
)
,


 





1
(8.76)
where the transition from state i to state j depends not only upon the transition probability aij
but also upon all the possible time periods  that may occur in state j. Intuitively, Eq. (8.76)
illustrates that when state j is reached from previous states i, the observations may stay in
state j for a period of  with duration density d j ( )
 , and each observation emits its own
output probability. All possible durations must be considered, which leads to summation
with respect to  . The independence assumption of observations results in the  term of the
output probabilities. Similarly, the backward recursion can be written as:






t
ij
j
j
t l
t
l
j j i
i
a d
b
X
j
( )
( )
(
)
( )
,








1
(8.77)
The modified Baum-Welch algorithm can then be used based on Eq. (8.76) and (8.77).
The proof of the re-estimation algorithm can be based on the modified Q-function except
that P(
|
)
X,S  should be replaced with P(
|
)
X,S, T  , which denotes the joint probability of
observation, X, state sequence, S = {s
s
s
s
k
Ns
1
2
,
...,
...
} in terms of state sk with time dura-
tion  k , and the corresponding duration sequence, T = {



1
2
,
,...
...
k
Ns }.
1
ˆ
ˆ
(
,
)
(
|
)log (
|
)
(
|
)
Q
P
P
P
Φ Φ =
Φ
Φ
Φ 
T
S
X,S,T
X,S,T
X
(8.78)
In a manner similar to the standard HMM, 

t
i j
, ( , ) can be defined as the transition
probability from state i at time t to state j with time duration  in state j. 

t
i j
, ( , ) can be
written as:
,
1
1
( , )
( )
( )
(
)
( ) /
( )
N
t
t
ij
j
j
t l
t
T
k
l
i j
i a d
b
X
j
k
τ
τ
τ
γ
α
τ
β
α
+
+
=
=
=

∏
(8.79)
Similarly, the probability of being in state j at time t with duration  can be computed
as:
,
,
( )
( , )
t
t
i
j
i j
τ
τ
γ
γ
= 
(8.80)
The re-estimation algorithm can be derived from Eq. (8.80), the Viterbi decoding algo-
rithm can be used for the time duration model, and the optimal path can be determined ac-
cording to:
j
Max Max V
i a d
b
X
t
i
t
ij
j
j
t
l
l
( )
[
( )
( )
(
)]


 






1
(8.81)

406
Hidden Markov Models
There are drawbacks to the use of the time duration modeling discussed here. One is
the great increase in computational complexity by a factor of O D
(
)
2 , where D is the time
duration distribution length. Another problem is the large number of additional parameters D
that must be estimated. One proposed remedy is to use a continuous density function instead
of the discrete distribution d j ( )
 .
In practice, duration models offered only modest improvement for speaker-
independent continuous speech recognition. Many systems have even eliminated the transi-
tion probability completely because output probabilities are so dominant. Nevertheless, dura-
tion information is very effective for pruning unlikely candidates during the large-vocabulary
speech recognition decoding process.
8.5.2.
First-Order Assumption
As you can see from the previous section, the duration of each stationary segment captured
by a single state is inadequately modeled. Another way to alleviate the duration problem is to
eliminate the first-order transition assumption and to make the underlying state sequence a
second-order Markov chain [32]. As a result, the transition probability between two states at
time t depends on the states in which the process was at time t-1 and t-2. For a given state
sequence S  {
...
}
,
,
s s
sT
1
2
, the probability of the state should be computed as:
P
as
s
s
t
t
t
t
( )
S 



2
1
(8.82)
where a
P s s
s
s
s
s
t
t
t
t
t
t





2
1
2
1
(
|
) is the transition probability at time t, given the two-order
state history. The re-estimation procedure can be readily extended based on Eq. (8.82). For
example, the new forward probability can be re-defined as:



t
t
t
t
t
ijk
i
k
t
j k
P X
s
j s
k
i j a
b
X
( , )
(
,
,
| )
( , )
(
)







1
1
1
(8.83)
where a
P s
k s
i s
j
ijk
t
t
t






(
|
,
)
2
1
. Similarly, we can define the backward probability
as:



t
t
T
t
t
ijk
k
k
t
t
i j
P X
s
i s
j
a
b
X
j k
( , )
(
|
,
, )
(
)
( , )









1
1
1
1
(8.84)
With Eq. (8.83) and (8.84), the MLE estimates can be derived easily based on the
modified  t i j k
( , , ):



t
t
t
t
t
ijk
k
t
t
i j k
P s
i s
j s
k
i j a
b
X
j k
P
( , , )
(
,
,
,
|
)
( , )
(
)
( , ) /
( |
)









1
1
1
1
X
X


(8.85)
In practice, the second-order model is computationally very expensive as we have to
consider the increased state space, which can often be realized with an equivalent first-order

Historical Perspective and Further Reading
407
hidden Markov model on the two-fold product state space. It has not offered significantly
improved accuracy to justify its increase in computational complexity for most applications.
8.5.3.
Conditional Independence Assumption
The third major weakness in HMMs is that all observation frames are dependent only on the
state that generated them, not on neighboring observation frames. The conditional independ-
ence assumption makes it hard to effectively handle nonstationary frames that are strongly
correlated. There are a number of ways to alleviate the conditional independence assumption
[34]. For example, we can assume the output probability distribution depends not only on the
state but also on the previous frame. Thus, the probability of a given state sequence can be
rewritten as:
P
P X X
s
t
t
T
t
t
( | ,
)
(
|
,
)
,
X S 





1
1
(8.86)
As the parameter space becomes huge, we often need to quantize X t1 into a smaller
set of codewords so that we can keep the number of free parameters under control. Thus, Eq.
(8.86) can be simplified as:
1
1
(
| ,
)
(
|
(
),
,
)
T
t
t
t
t
P
P X
X
s
−
=
Φ =
ℜ
Φ
∏
X S
(8.87)
where
()
ℜ
denotes the quantized vector that has a small codebook size, L. Although this can
dramatically reduce the space of the free conditional output probability distributions, the
total number of free parameters will still increase by L times.
The re-estimation for conditional dependent HMMs can be derived with the modi-
fied Q-function, as discussed in the previous sections. In practice, it has not demonstrated
convincing accuracy improvement for large-vocabulary speech recognition.
8.6.
HISTORICAL PERSPECTIVE AND FURTHER READING
The Markov chain was named after Russian scientist A. Markov for his pioneering work in
analyzing the letter sequence in the text of a literary work in 1913 [33]. In the 1960s, Baum
and others further developed efficient methods for training the model parameters [4, 5].
When the observation is real valued, the use of continuous or semi-continuous HMMs can
improve the overall performance. Baum et al. also developed the method to use continuous
density functions that are strictly log concave [5], which was relaxed by Liporace [31] and
expanded by Juang to include mixture density functions [27].
The Viterbi algorithm shares the same concept that was independently discovered by
researchers in many separate fields [28], including Vintsyuk [42], Needleman and Wunsch
[35], Sankoff [41], Sakoe and Chiba [40], and Wagner and Fischer [44].

408
Hidden Markov Models
Jim Baker did his Ph.D. thesis under Raj Reddy at Carnegie Mellon using HMMs for
speech recognition [3]. At the same time Fred Jelinek and his colleagues at IBM Research
pioneered widespread applications [23]. Since the 1980s, partly because of the DARPA-
funded speech projects, HMMs have become a mainstream technique for modeling speech,
as exemplified by advanced systems developed at BBN, Bell Labs, Carnegie Mellon, IBM,
Microsoft, SRI, and others [9, 17, 29, 46]. The Ph.D. theses from Kai-Fu Lee [29], Hsiao-
Wuen Hon [16], and Mei-Yuh Hwang [22] at Carnegie Mellon addressed many important
practical issues in using HMMs for speech recognition. There are also a number of good
books on the practical use of HMMs [18, 24, 38, 45].
The choice of different output probabilities depends on a number of factors such as the
availability of training data, the feature characteristics, the computational complexity, and
the number of free parameters [19] [34]. The semi-continuous model, also known as the tied-
mixture model, was independently proposed by Huang and Jack [21] and Bellegarda and
Nahamoo [6]. Other improvements include explicit duration modeling [1, 11, 13, 14, 30,
39], high-order and conditional models [7, 32, 34], which have yet to be shown effective for
practical speech recognition.
Both Carnegie Mellon University’s open speech software4 and Cambridge University’s
HTK5 are a good starting point for those interested in using the existing tools for running
experiments.
The HMM have become the most prominent techniques for speech recognition today.
Most of the state-of-the-art speech recognition systems on the market are based on HMMs
described in this chapter.
4 http://www.speech.cs.cmu.edu/sphinx/
5 http://htk.eng.cam.ac.uk/

Historical Perspective and Further Reading
409
REFERENCES
[1]
Anastasakos, A., R. Schwartz, and H. Sun, "Duration Modeling in Large Vocabu-
lary Speech Recognition" in Proc. of the IEEE Int. Conf. on Acoustics, Speech and
Signal Processing 1995, Detroit, MI, pp. 628-631.
[2]
Bahl, L.R., et al., "Speech Recognition with Continuous-Parameter Hidden Markov
Models," Computer Speech and Language, 1987, 2, pp. 219-234.
[3]
Baker, J.K., "The DRAGON System - An Overview," Trans. on Acoustics, Speech
and Signal Processing, 1975, 23(1), pp. 24-29.
[4]
Baum, L.E. and J.A. Eagon, "An Inequality with Applications to Statistical Estima-
tion for Probabilistic Functions of Markov Processes and to a Model for Ecology,"
Bulletin of American Mathematical Society, 1967, 73, pp. 360-363.
[5]
Baum, L.E., et al., "A Maximization Technique Occurring in the Statistical Analy-
sis of Probabilistic Functions of Markov Chains," Annals of Mathematical Statis-
tics, 1970, 41, pp. 164-171.
[6]
Bellegarda, J.R. and D. Nahamoo, "Tied Mixture Continuous Parameter Models for
Large Vocabulary Isolated Speech Recognition," Int. Conf. on Acoustics, Speech
and Signal Processing, 1989 pp. 13-16.
[7]
Brown, P., The Acoustic-Modeling Problem in Automatic Speech Recognition,
Ph.D. Thesis in Computer Science 1987, Carnegie Mellon University, Pittsburgh.
[8]
Brown, P.F., et al., "The Mathematics of Statistical Machine Translation: Parameter
Estimation," Computational Linguistics, 1995, 19(2), pp. 263--312.
[9]
Chou, W., C.H. Lee, and B.H. Juang, "Minimum Error Rate Training of Inter-Word
Context Dependent Acoustic Model Units in Speech Recognition" in Proc. of the
Int. Conf. on Spoken Language Processing 1994, Yokohama, Japan, pp. 439-442.
[10]
Church, K., "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted
Text," Proc. of the Second Conf. on Applied Natural Language Processing, 1988,
Austin, Texas pp. 136-143.
[11]
Deng, L., M. Lennig, and P. Mermelstein, "Use of Vowel Duration Information in a
Large Vocabulary Word Recognizer," Journal of the Acoustical Society of Amer-
ica, 1989, 86(2
August), pp. 540-548.
[12]
DeRose, S.J., "Grammatical Category Disambiguation by Statistical Optimization,"
Computational Linguistics, 1988(1), pp. 31-39.
[13]
Dumouchel, P. and D. Shaughnessy, "Segmental Duration and HMM Modeling,"
Proc. of the European Conf. on Speech Communication and Technology, 1995,
Madrid, Spain pp. 803-806.
[14]
Ferguson, J.D., "Variable Duration Models for Speech" in Proc. of the Symposium
on the Application of Hidden Markov Models to Text and Speech, J.D. Ferguson,
Editor 1980, New Jersey, pp. 143-179, Princeton.
[15]
Fiscus, J., "A Post-Processing System to Yield Reduced Word Error Rates: Recog-
nizer Output Voting Error Reduction (ROVER)," IEEE Workshop on Automatic
Speech Recognition and Understanding, 1997, Santa Barbara, CA pp. 347-352.

410
Hidden Markov Models
[16]
Hon, H.W., Vocabulary-Independent Speech Recognition: The VOCIND System,
Ph.D Thesis in Department of Computer Science 1992, Carnegie Mellon Univer-
sity, Pittsburgh.
[17]
Huang, X.D., et al., "The SPHINX-II Speech Recognition System: An Overview,"
Computer Speech and Language, 1993 pp. 137-148.
[18]
Huang, X.D., Y. Ariki, and M.A. Jack, Hidden Markov Models for Speech Recog-
nition, 1990, Edinburgh, U.K., Edinburgh University Press.
[19]
Huang, X.D., et al., "A Comparative Study of Discrete, Semicontinuous, and Con-
tinuous Hidden Markov Models," Computer Speech and Language, 1993, 7(4), pp.
359-368.
[20]
Huang, X.D., et al., "Deleted Interpolation and Density Sharing for Continuous
Hidden Markov Models," IEEE Int. Conf. on Acoustics, Speech and Signal Proc-
essing, 1996.
[21]
Huang, X.D. and M.A. Jack, "Semi-Continuous Hidden Markov Models with
Maximum Likelihood Vector Quantization" in IEEE Workshop on Speech Recogni-
tion 1988.
[22]
Hwang, M., Subphonetic Modeling in HMM-based Speech Recognition Systems,
Ph.D. Thesis Thesis in Computer Science 1994, Carnegie Mellon University, Pitts-
burgh.
[23]
Jelinek, F., "Continuous Speech Recognition by Statistical Methods," Proc. of the
IEEE, 1976, 64(4), pp. 532-556.
[24]
Jelinek, F., Statistical Methods for Speech Recognition, 1998, Cambridge, MA,
MIT Press.
[25]
Jiang, L. and X. Huang, "Unified Decoding and Feature Representation for Im-
proved Speech Recognition," Proc. of the 6th European Conf. on Speech Commu-
nication and Technology, 1999, Budapest, Hungary pp. 1331-1334.
[26]
Juang, B.H., W. Chou, and C.H. Lee, "Statistical and Discriminative Methods for
Speech Recognition" in Automatic Speech and Speaker Recognition - Advanced
Topics, C.H. Lee, F.K. Soong, and K.K. Paliwal, eds. 1996, Boston, pp. 109-132,
Kluwer Academic Publishers.
[27]
Juang, B.H., S.E. Levinson, and M.M. Sondhi, "Maximum Likelihood Estimation
for Multivariate Mixture Observations of Markov Chains," IEEE Trans. on Infor-
mation Theory, 1986, IT-32(2), pp. 307-309.
[28]
Kruskal, J., "An Overview of Sequence Comparison" in Time Warps, String Edits,
and Macromolecules: The Theory and Practice of Sequence Comparison, D.
Sankoff and J. Kruskal, eds. 1983, Reading, MA., pp. 1-44, Addison-Wesley.
[29]
Lee, K.F., Large-Vocabulary Speaker-Independent Continuous Speech Recogni-
tion: The SPHINX System, Ph.D. Thesis in Computer Science Dept. 1988, Carne-
gie Mellon University, Pittsburgh.
[30]
Levinson, S.E., "Continuously Variable Duration Hidden Markov Models for
Automatic Speech Recognition," Computer Speech and Language, 1986, pp. 29-
45.

Historical Perspective and Further Reading
411
[31]
Liporace, L.R., "Maximum Likelihood Estimation for Multivariate Observations of
Markov Sources," IEEE Trans. on Information Theory, 1982, 28, pp. 729-734.
[32]
Mari, J., J. Haton, and A. Kriouile, "Automatic Word Recognition Based on Sec-
ond-Order Hidden Markov Models," IEEE Trans. on Speech and Audio Process-
ing, 1977, 5(1), pp. 22-25.
[33]
Markov, A.A., "An Example of Statistical Investigation in the Text of 'Eugene On-
yegin', Illustrating Coupling of Tests in Chains," Proc. of the Academy of Sciences
of St. Petersburg, 1913, Russia pp. 153-162.
[34]
Ming, J. and F. Smith, "Modelling of the Interframe Dependence in an HMM Using
Conditional Gaussian Mixtures," Computer Speech and Language, 1996, 10(4), pp.
229-247.
[35]
Needleman, S. and C. Wunsch, "A General Method Applicable to the Search for
Similarities in the Amino-acid Sequence of Two Proteins," Journal of Molecular
Biology, 1970, 48, pp. 443-453.
[36]
Normandin, Y., "Maximum Mutual Information Estimation of Hidden Markov
Models" in Automatic Speech and Speaker Recognition, C.H. Lee, F.K. Soong, and
K.K. Paliwal, eds. 1996, Norwell, MA, Kluwer Academic Publishers.
[37]
Rabiner, L.R., "A Tutorial on Hidden Markov Models and Selected Applications in
Speech Recognition," Proc. of IEEE, 1989, 77(2), pp. 257-286.
[38]
Rabiner, L.R. and B.H. Juang, Fundamentals of Speech Recognition, Prentice Hall
Signal Processing Series, eds. A.V. Oppenheim, 1993, Englewood Cliffs, NJ, Pren-
tice-Hall.
[39]
Russell, M.J. and R.K. Moore, "Explicit Modeling of State Occupancy in Hidden
Markov Models for Automatic Speech Recognition," Int. Conf. on Acoustics,
Speech and Signal Processing, 1985 pp. 5-8.
[40]
Sakoe, H. and S. Chiba, "Dynamic Programming Algorithm Optimization for Spo-
ken Word Recognition," IEEE Trans. on Acoustics, Speech and Signal Processing,
1978, 26(1), pp. 43-49.
[41]
Sankoff, D., "Matching Sequences under Deletion-Insertion Constraints," Proc. of
the National Academy of Sciences, 1972, 69, pp. 4-6.
[42]
Vintsyuk, T.K., "Speech Discrimination by Dynamic Programming," Cybernetics,
1968, 4(1), pp. 52-57.
[43]
Viterbi, A.J., "Error Bounds for Convolutional Codes and an Asymptotically Opti-
mum Decoding Algorithm," IEEE Trans. on Information Theory, 1967, 13(2), pp.
260-269.
[44]
Wagner, R. and M. Fischer, "The String-to-String Correction Problem," Journal of
the ACM, 1974, 21, pp. 168-173.
[45]
Waibel, A.H. and K.F. Lee, Readings in Speech Recognition, 1990, San Mateo,
CA, Morgan Kaufman Publishers.
[46]
Young, S.J. and P.C. Woodland, "The Use of State Tying in Continuous Speech
Recognition," Proc. of Eurospeech, 1993, Berlin pp. 2203-2206.

413
C H A P T E R
9
Acoustic ModelingEquation Section 9
After years of research and development, ac-
curacy of automatic speech recognition remains one of the most important research chal-
lenges. A number of well-known factors determine accuracy; those most noticeable are
variations in context, in speaker, and in environment. Acoustic modeling plays a critical role
in improving accuracy and is arguably the central part of any speech recognition system.
For the given acoustic observation
1
2...
n
X X
X
=
X
, the goal of speech recognition is to
find out the corresponding word sequence
1
2
ˆ
...
m
w w
w
=
W
that has the maximum posterior
probability P(W | X) as expressed by Eq. (9.1).
(
) (
|
)
argmax
(
|
)
argmax
(
)
P
P
P
P
=
=
^
w
w
W
X W
W
W X
X
(9.1)
Since the maximization of Eq. (9.1) is carried out with the observation X fixed, the
above maximization is equivalent to maximization of the following equation:

414
Acoustic Modeling
argmax
(
) (
|
)
P
P
=
^
w
W
W
X W
(9.2)
The practical challenge is how to build accurate acoustic models, P(X | W), and lan-
guage models, P(W), that can truly reflect the spoken language to be recognized. For large-
vocabulary speech recognition, since there are a large number of words, we need to decom-
pose a word into a subword sequence. Thus P(X | W) is closely related to phonetic model-
ing. P(X | W) should take into account speaker variations, pronunciation variations, envi-
ronmental variations, and context-dependent phonetic coarticulation variations. Last, but not
least, any static acoustic or language model will not meet the needs of real applications. So it
is vital to dynamically adapt both P(W) and P(X | W) to maximize P(W | X) while using the
spoken language systems. The decoding process of finding the best matched word sequence
W to match the input speech signal X in speech recognition systems is more than a simple
pattern recognition problem, since in continuous speech recognition you have an infinite
number of word patterns to search, as discussed in detail in Chapters 12 and 13.
In this chapter we focus on discussing solutions that work well in practice. To high-
light solutions that are effective, we use the Whisper speech recognition system [49] devel-
oped at Microsoft Research as a concrete example to illustrate how to build a working sys-
tem and how various techniques can help to reduce speech recognition errors.1 We hope that
by studying what worked well in the past we can illuminate the possibilities for further im-
provement of the state of the art.
The hidden Markov model we discussed in Chapter 8 is the underpinning for acoustic
phonetic modeling. It provides a powerful way to integrate segmentation, time warping, pat-
tern matching, and context knowledge in a unified manner. The underlying technologies are
undoubtedly evolving, and the research community is aggressively searching for more pow-
erful solutions. Most of the techniques discussed in this chapter can be readily derived from
the fundamentals discussed in earlier chapters.
9.1.
VARIABILITY IN THE SPEECH SIGNAL
The research community has produced technologies that, with some constraints, can accu-
rately recognize spoken input. Admittedly, today’s state-of-the-art systems still cannot match
humans’ performance. Although we can build a very accurate speech recognizer for a par-
ticular speaker, in a particular language and speaking style, in a particular environment, and
limited to a particular task, it remains a research challenge to build a recognizer that can es-
sentially understand anyone’s speech, in any language, on any topic, in any free-flowing
style, and in almost any speaking environment.
1 Most of the experimental results used here are based on a development test set for the 60,000-word speaker-
independent continuous dictation task. The training set consists of 35,000 utterances from about 300 speakers. The
test set consists of 410 utterances from 10 speakers that were not used in the training data. The language model is
derived from 2 billion words of English text corpora.

Variability in the Speech Signal
415
Accuracy and robustness are the ultimate measures for the success of speech recogni-
tion algorithms. There are many reasons why existing algorithms or systems did not deliver
what people want. In the sections that follow we summarize the major factors involved.
9.1.1.
Context Variability
Spoken language interaction between people requires knowledge of word meanings, com-
munication context, and common sense. Words with widely different meanings and usage
patterns may have the same phonetic realization. Consider the challenge represented by the
following utterance:
Mr. Wright should write to Ms. Wright right away about his Ford or four door
Honda.
For a given word with the same pronunciation, the meaning could be dramatically dif-
ferent, as indicated by Wright, write, and right. What makes it even more difficult is that
Ford or and Four Door are not only phonetically identical, but also semantically relevant.
The interpretation is made within a given word boundary. Even with smart linguistic and
semantic information, it is still impossible to decipher the correct word sequence, unless the
speaker pauses between words or uses intonation to set apart these semantically confusable
phrases.
In addition to the context variability at word and sentence level, you can find dramatic
context variability at phonetic level. As illustrated in Figure 9.1, the acoustic realization of
phoneme /ee/ for word peat and wheel depends on its left and right context. The dependency
becomes more important in fast speech or spontaneous speech conversation, since many
phonemes are not fully realized.
0
0 . 1
0 . 2
0 . 3
-2 0 0 0
-1 0 0 0
0
1 0 0 0
0
0 . 1
0 . 2
0 . 3
-2 0 0 0
-1 0 0 0
0
1 0 0 0
2 0 0 0
T im e (s e c o n d s )
Frequency (Hz)
0
0 . 1
0 . 2
0 . 3
0
1 0 0 0
2 0 0 0
3 0 0 0
4 0 0 0
T im e (s e c o n d s )
Frequency (Hz)
0
0 . 1
0 . 2
0 . 3
0
1 0 0 0
2 0 0 0
3 0 0 0
4 0 0 0
Figure 9.1 Waveforms and spectrograms for words peat (left) and wheel (right). The phoneme
/ee/ is illustrated with two different left and right contexts. This illustrates that different con-
texts may have different effects on a phone.

416
Acoustic Modeling
9.1.2.
Style Variability
To deal with acoustic realization variability, a number of constraints can be imposed on the
use of the speech recognizer. For example, we can have an isolated speech recognition sys-
tem, in which users have to pause between each word. Because the pause provides a clear
boundary for the word, we can easily eliminate errors such as Ford or and Four Door. In
addition, isolated speech provides a correct silence context to each word so that it is easier to
model and decode the speech, leading to a significant reduction in computational complexity
and error rate. In practice, the word-recognition error rate of an isolated speech recognizer
can typically be reduced by more than a factor of three (from 7% to 2%) as compared with to
a comparable continuous speech recognition system [5]. The disadvantage is that such an
isolated speech recognizer is unnatural to most people. The throughput is also significantly
lower than that for continuous speech.
In continuous speech recognition, the error rate for casual, spontaneous speech, as oc-
curs in our daily conversation, is much higher than for carefully articulated read-aloud
speech. The rate of speech also affects the word recognition rate. It is typical that the higher
the speaking rate (words/minute), the higher the error rate. If a person whispers, or shouts, to
reflect his or her emotional changes, the variation increases more significantly.
9.1.3.
Speaker Variability
Every individual speaker is different. The speech he or she produces reflects the physical
vocal tract size, length and width of the neck, a range of physical characteristics, age, sex,
dialect, health, education, and personal style. As such, one person’s speech patterns can be
entirely different from those of another person. Even if we exclude these interspeaker differ-
ences, the same speaker is often unable to precisely produce the same utterance. Thus, the
shape of the vocal tract movement and rate of delivery may vary from utterance to utterance,
even with dedicated effort to minimize the variability.
For speaker-independent speech recognition, we typically use more than 500 speakers
to build a combined model. Such an approach exhibits large performance fluctuations among
new speakers because of possible mismatches in the training data between exiting speakers
and new ones [50]. In particular, speakers with accents have a tangible error-rate increase of
2 to 3 times.
To improve the performance of a speaker-independent speech recognizer, a number of
constraints can be imposed on its use. For example, we can have a user enrollment that re-
quires the user to speak for about 30 minutes. With the speaker-dependent data and training,
we may be able to capture various speaker-dependent acoustic characteristics that can sig-
nificantly improve the speech recognizer’s performance. In practice, speaker-dependent
speech recognition offers not only improved accuracy but also improved speed, since decod-
ing can be more efficient with an accurate acoustic and phonetic model. A typical speaker-
dependent speech recognition system can reduce the word recognition error by more than
30% as compared with a comparable speaker-independent speech recognition system.

How to Measure Speech Recognition Errors
417
The disadvantage of speaker-dependent speech recognition is that it takes time to col-
lect speaker-dependent data, which may be impractical for some applications such as an
automatic telephone operator. Many applications have to support walk-in speakers, so
speaker-independent speech recognition remains an important feature. When the amount of
speaker-dependent data is limited, it is important to make use of both speaker-dependent and
speaker-independent data using speaker-adaptive training techniques, as discussed in Section
9.6. Even for speaker-independent speech recognition, you can still use speaker-adaptive
training based on recognition results to quickly adapt to each individual speaker during the
usage.
9.1.4.
Environment Variability
The world we live in is full of sounds of varying loudness from different sources. When we
interact with computers, we may have people speaking in the background. Someone may
slam the door, or the air conditioning may start humming without notice. If speech recogni-
tion is embedded in mobile devices, such as PDAs (personal digital assistants) or cellular
phones, the spectrum of noises varies significantly because the owner moves around. These
external parameters, such as the characteristics of the environmental noise and the type and
placement of the microphone, can greatly affect speech recognition system performance. In
addition to the background noises, we have to deal with noises made by speakers, such as lip
smacks and noncommunication words. Noise may also be present from the input device it-
self, such as the microphone and A/D interference noises.
In a similar manner to speaker-independent training, we can build a system by using a
large amount of data collected from a number of environments; this is referred to as
multistyle training [70]. We can use adaptive techniques to normalize the mismatch across
different environment conditions in a manner similar to speaker-adaptive training, as dis-
cussed in Chapter 10. Despite the progress being made in the field, environment variability
remains as one of the most severe challenges facing today’s state-of-the-art speech systems
9.2.
HOW TO MEASURE SPEECH RECOGNITION ERRORS
It is critical to evaluate the performance of speech recognition systems. The word recogni-
tion error rate is widely used as one of the most important measures. When you compare
different acoustic modeling algorithms, it is important to compare their relative error reduc-
tion. Empirically, you need to have a test data set that contains more than 500 sentences
(with 6 to 10 words for each sentence) from 5 to 10 different speakers to reliably estimate
the recognition error rate. Typically, you need to have more than 10% relative error reduc-
tion to consider adopting a new algorithm.
As a sanity check, you may want to use a small sample from the training data to meas-
ure the performance of the training set, which is often much better than what you can get
from testing new data. Training-set performance is useful in the development stage to iden-
tify potential implementation bugs. Eventually, you need to use a development set that typi-

418
Acoustic Modeling
cally consists of data never used in training. Since you may tune a number of parameters with
your development set, it is important to evaluate performance of a test set after you decide
the optimal parameter setting. The test set should be completely new with respect to both
training and parameter tuning.
There are typically three types of word recognition errors in speech recognition:
 Substitution: an incorrect word was substituted for the correct word
 Deletion: a correct word was omitted in the recognized sentence
 Insertion: an extra word was added in the recognized sentence2
For instance, a speech recognition system may produce an incorrect result as follows,
where substitutions are bold, insertions are underlined, and deletions are denoted as **.
There are four errors in this example.
Correct: Did mob mission area of the Copeland ever go to m4 in nineteen eighty one
Recognized: Did mob mission area ** the copy land ever go to m4 in nineteen east
one
To determine the minimum error rate, you can’t simply compare two word sequences
one by one. For example, suppose you have utterance The effect is clear recognized as Effect
is not clear. If you compare word to word, the error rate is 75% (The vs. Effect, effect vs. is,
is vs. not). In fact, the error rate is only 50% with one deletion (The) and one insertion (not).
In general, you need to align a recognized word string against the correct word string and
compute the number of substitutions (Subs), deletions (Dels), and insertions (Ins). The Word
Error Rate is defined as:
Word Error Rate
No.of word in the correct sentence




100%
Subs
Dels
Ins
(9.3)
This alignment is also known as the maximum substring matching problem, which can
be easily handled by the dynamic programming algorithm discussed in Chapter 8.
Let the correct word string be
1
2
n
w w
w

, where
iw denotes the ith word in the correct
word string, and the recognized word string be
1
2
ˆ ˆ
ˆ m
w w
w

, where ˆiw denotes the ith word in
the recognized word string. We denote
[ , ]
R i j
as the minimum error of aligning substring
1
2
n
w w
w

against substring
1
2
ˆ ˆ
ˆ m
w w
w

. The optimal alignment and the associated word
error rate
[ ,
]
R n m
for correct word string
1
2
n
w w
w

and the recognized word string
1
2
ˆ ˆ
ˆ m
w w
w

are obtained via the dynamic programming algorithm illustrated in ALGORITHM
9.1. The accumulated cost function
[ , ]
R i j
progresses from R[1, 1] to
[ ,
]
R n m
correspond-
ing to the minimum distance from (1, 1) to (n, m). We store the back pointer information B[i,
2 Even for isolated speech recognition, you may still have the insertion error, since the word boundary
needs to be detected in most applications. It is possible that one isolated utterance is recognized into two words.

Signal Processing—Extracting Features
419
j] as we move along. When we reach the final grid (n, m), we back trace along the optimal
path to find out if there are substitutions, deletions, or insertions on the matched path, as
stored in B[i, j].
ALGORITHM 9.1: THE ALGORITHM TO MEASURE THE WORD ERROR RATE
Step 1: Initialization
R[ , ]
0 0
0
=
R i j
i
j
[ , ]
)
)
= ∞
<
<
if (
or (
0
0
B[ , ]
0 0
0
=
Step 2: Iteration
for
1,
,
i
n
=

{
for
1,
,
j
m
=

{
[
1, ] 1 (deletion)
[
1,
1] (match)
[
1,
1] 1 (substitution)
[ , ]
min
[ ,
1] 1 (insertion)
R i
j
R i
j
R i
j
R i j
R i j
−
+




−
−




−
−
+


=
−
+


B i j
[ , ]
1 if deletion
2 if insertion
3 if match
4 if substitution
=
R
S||
T||
} }
Step 3: Backtracking and termination
word error rate =100%  R n m
n
( ,
)
optimal backward path = (
,
,
, )
s
s
1
2
0

where s
B n m
1 =
[ , ] , s
B i
j
s
B i j
s
B i
j
s
t
t
t
t
=
−
−
−
−
L
N
MMM
O
Q
PPP
−
−
−
[
, ]
[ ,
]
[
,
]
1
1
1
1
1
1
1
if
= 1
if
= 2
if
= 3 or 4
for
2,...
t =
until
0
ts =
For applications involved with rejection, such as word confidence measures as dis-
cussed in Section 9.7, you need to measure both false rejection rate and false acceptance
rate. In speaker or command verification, the false acceptance of a valid user/command is
also referred to as Type I error, as opposed to the false rejection of a valid user/command
(Type II) [17]. A higher false rejection rate generally leads to a lower false acceptance rate.
A plot of the false rejection rate versus the false acceptance rate, widely used in communica-
tion theory, is called the receiver operating characteristic (ROC) curve.
9.3.
SIGNAL PROCESSING—EXTRACTING FEATURES
The role of a signal processing module, as illustrated in Figure 1.2, is to reduce the data rate,
to remove noises, and to extract salient features that are useful for subsequent acoustic

420
Acoustic Modeling
matching. Using as building blocks the topics we discussed in earlier chapters, we briefly
illustrate here what is important in modeling speech to deal with variations we must address.
More advanced environment normalization techniques are discussed in Chapter 10.
9.3.1.
Signal Acquisition
Today’s computers can handle most of the necessary speech signal acquisition tasks in soft-
ware. For example, most PC sound cards have direct memory access, and the speech can be
digitized to the memory without burdening the CPU with input/output interruptions. The
operating system can correctly handle most of the necessary AD/DA functions in real time.
To perform speech recognition, a number of components—such as digitizing speech,
feature extraction and transformation, acoustic matching, and language model-based
search—can be pipelined time-synchronously from left to right. Most operating systems can
supply mechanisms for organizing pipelined programs in a multitasking environment. Buff-
ers must be appropriately allocated so that you can ensure time-synchronous processing of
each component. Large buffers are generally required on slow machines because of potential
delays in processing an individual component. The right buffer size can be easily determined
by experimentally tuning the system with different machine load situations to find a balance
between resource use and relative delay.
For speech signal acquisition, the needed buffer typically ranges from 4 to 64 kB with
16-kHz speech sampling rate and 16-bit A/D precision. In practice, 16-kHz sampling rate is
sufficient for the speech bandwidth (8 kHz). Reduced bandwidth, such as telephone channel,
generally increases speech recognition error rate. Table 9.1 shows some empirical relative
word recognition error increase using a number of different sampling rates. If we take the 8-
kHz sampling as our baseline, we can reduce the word recognition error with a comparable
recognizer by about 10% if we increase the sampling rate to 11 kHz. If we further increase
the sampling rate to 16 kHz, the word recognition error rate can be further reduced by addi-
tional 10%. Further increasing the sampling rate to 22 kHz does not have any additional im-
pact on the word recognition errors, because most of the salient speech features are within 8-
kHz bandwidth.
Table 9.1 Relative error rate reduction with different sampling rates.
Sampling Rate
Relative Error-Rate Reduction
8 kHz
Baseline
11 kHz
+10%
16 kHz
+10%
22 kHz
+0%

Signal Processing—Extracting Features
421
9.3.2.
End-Point Detection
To activate speech signal capture, you can use a number of modes including either push to
talk or continuously listening. The push-to-talk mode uses a special push event to activate or
deactivate speech capturing, which is immune to the potential background noise and can
eliminate unnecessary use of processing resources to detect speech events. This mode some-
times also requires you to push and hold while talking. You push to indicate speech’s begin-
ning and then release to indicate the end of speech capture. The disadvantage is the necessity
to activate the application each time the person speaks.
The continuously listening model listens all the time and automatically detects whether
there is a speech signal or not. It needs a so-called speech end-point detector, which is typi-
cally based on an extremely efficient two-class pattern classifier. Such a classifier is used to
filter out obvious silence, but the ultimate decision on the utterance boundary is left to the
speech recognizer. In comparison to the push-to-talk mode, the continuously listening mode
requires more processing resources, also with potential classification errors.
The endpoint detector is often based on the energy threshold that is a function of time.
The logarithm of the energy threshold can be dynamically generated based on the energy
profiles across a certain period of time. Constraints on word duration can also be imposed to
better classify a sequence of frames so that extremely short spikes can be eliminated.
It is not critical for the automatic end-point detector to offer exact end-point accuracy.
The key feature required of it is a low rejection rate (i.e., the automatic end-point detector
should not interpret speech segments as silence/noise segments). Any false rejection leads to
an error in the speech recognizer. On the other hand, a possible false acceptance (i.e., the
automatic end-point detector interprets noise segments as speech segments) may be rescued
by the speech recognizer later if the recognizer has appropriate noise models, such as spe-
cific models for clicks, lip smacks, and background noise.
Explicit end-point detectors work reasonably well with recordings exhibiting a signal-
to-noise ratio of 30 dB or greater, but they fail considerably on noisier speech. As discussed,
speech recognizers can be used to determine the end points by aligning the vocabulary words
preceded and followed by a silence/noise model. This scheme is generally much more reli-
able than any threshold-based explicit end-point detection, because recognition can jointly
detect both the end points and words or other explicit noise classes, but requires more com-
putational resources. A compromise is to use a simple adaptive two-class (speech vs. si-
lence/noise) classifier to locate speech activities (with enough buffers at both ends) and no-
tify the speech recognizer for subsequent processing. For the two-class classifier, we can use
both the log-energy and delta log-energy as the feature. Two Gaussian density functions,
1
2
{
,
}
Φ Φ
= Φ , can be used to model the background stationary noise and speech, respec-
tively. The parameters of the Gaussian density can be estimated using the labeled speech and
noise data or estimated in an unsupervised manner.
When enough frames are classified as speech segments by the efficient two-class clas-
sifier, the speech recognizer is notified to start recognizing the signal. As shown in Figure
9.2, we should include enough frames before the beginning frame,
bt , for the speech recog-
nizer to minimize the possible detection error. In the same manner, when enough

422
Acoustic Modeling
noise/silence frames are detected at
et , we should keep providing the speech recognizer with
enough frames for processing before declaring that the end of the utterance has been
reached.
Figure 9.2 End-point detection boundary
bt
and
et
may need extra buffering for subsequent
speech recognition.
Since there are only two classes, these parameters can be dynamically adapted using
the EM algorithm during runtime. As discussed in Chapter 4, the EM algorithm can itera-
tively estimate the Gaussian parameters without having a precise segmentation between
speech and noise segments. This is very important, because we need to keep the parameters
dynamic for robust end-point detection in constantly changing environments.
To track the varying background noises, we use an exponential window to give weight
to the most recent signal:
w
k
k 

exp(
)

(9.4)
where α is a constant that controls the adaptation rate, and k is the index of the time. In fact,
you could use different rates for noise and speech when you use the EM algorithm to esti-
mate the two-class Gaussian parameters. It is advantageous to use a smaller time constant for
noise than for speech. With such a weighting window, the means of the Gaussian density, as
discussed in Chapter 4, can be rewritten as:
2
1
2
1
(
|
)
(
|
)
ˆ
,
{0,1}
(
|
)
(
|
)
t
k
i
k
i
i
i
i
k
k
k
t
k
i
k
i
i
i
k
k
c P
w
P
k
c P
w
P
=−∞
=
=−∞
=
Φ
Φ
=
∈
Φ
Φ




x
x
x
µ
x
x
(9.5)
bt
et

Signal Processing—Extracting Features
423
9.3.3.
MFCC and Its Dynamic Features
The extraction of reliable features is one of the most important issues in speech recognition.
There are a large number of features we can use. However, as discussed in Chapter 4, the
curse-of-dimensionality problem reminds us that the amount of training data is always lim-
ited. Therefore, incorporation of additional features may not lead to any measurable error
reduction. This does not necessarily mean that the additional features are poor ones, but
rather that we may have insufficient data to reliably model those features.
The first feature we use is the speech waveform itself. In general, time-domain features
are much less accurate than frequency-domain features such as the mel-frequency cepstral
coefficients (MFCC) discussed in Chapter 6 [23]. This is because many features such as
formants, useful in discriminating vowels, are better characterized in the frequency domain
with a low-dimension feature vector.
As discussed in Chapter 2, temporal changes in the spectra play an important role in
human perception. One way to capture this information is to use delta coefficients that meas-
ure the change in coefficients over time. Temporal information is particularly complemen-
tary to HMMs, since HMMs assume each frame is independent of the past, and these dy-
namic features broaden the scope of a frame. It is also easy to incorporate new features by
augmenting the static feature.
When 16-kHz sampling rate is used, a typical state-of-the-art speech system can be
build based on the following features.
 13th-order MFCC
kc ;
 13th-order 40-msec 1st-order delta MFCC computed from
2
2
k
k
k
+
−
=
−
c
c
c

;
 13th-order 2nd-order delta MFCC computed from
1
1
k
k
k
+
−
=
−
c
c
c



;
The short-time analysis Hamming window of 256 ms is typically used to compute the
MFCC
kc . The window shift is typically 10 ms. Please note that
[0]
kc
is included in the
feature vector, which has a role similar to that of the log power. The feature vector used for
speech recognition is typically a combination of these features
k
k
k
k
=










c
x
c
c


(9.6)
The relative error reduction with a typical speech recognition system is illustrated in
Table 9.2. As you can see from the table, the 13th-order MFCC outperforms 13th-order LPC
cepstrum coefficients, which indicates that perception-motivated mel-scale representation
indeed helps recognition. In a similar manner, perception-based LPC features such as PLP
can achieve similar improvement. The MFCC order has also been studied experimentally for
speech recognition. The higher-order MFCC does not further reduce the error rate in com-
parison with the 13th-order MFCC, which indicates that the first 13 coefficients already con-

424
Acoustic Modeling
tain most salient information needed for speech recognition. In addition to mel-scale repre-
sentation, another perception-motivated feature such as the first- and second-order delta fea-
tures can significantly reduce the word recognition error, while the higher-order delta fea-
tures provide no further information.
Feature extraction in these experiments is typically optimized together with the classi-
fier, since there are a number of modeling assumptions, such as the diagonal covariance in
the Gaussian density function, that are closely related to what features to use. It is possible
that these relative error reductions would vary if a different speech recognizer were used.
Table 9.2 Relative error reduction with different features.
Feature set
Relative error reduction
13th-order LPC cepstrum coefficients
Baseline
13th-order MFCC
+10%
16th-order MFCC
+0%
+1st- and 2nd-order dynamic features
+20%
+3rd-order dynamic features
+0%
9.3.4.
Feature Transformation
Before you use feature vectors such as MFCC for recognition, you can preprocess or trans-
form them into a new space that alleviates environment noise, channel distortion, and
speaker variations. You can also transform the features that are most effective for preserving
class separability so that you can further reduce the recognition error rate. Since we devote
Chapter 10 completely to environment and channel normalization, we briefly discuss here
how we can transform the feature vectors to improve class separability.
To further reduce the dimension of the feature vector, you can use a number of dimen-
sion reduction techniques to map the feature vector into more effective representations. If the
mapping is linear, the mapping function is well defined and you can find the coefficients of
the linear function so as to optimize your objective functions. For example, when you com-
bine the first- and second-order dynamic features with the static MFCC vector, you can use
the principal-component analysis (PCA) (also known as Karhunen-Loeve transform) [32] to
map the combined feature vector into a smaller dimension. The optimum basis vectors of the
principal-component analysis are the eigenvectors of the covariance matrix of a given distri-
bution. In practice, you can compute the eigenvectors of the autocorrelation matrix as the
basis vectors. The effectiveness of the transformed vector, in terms of representing the origi-
nal feature vector, is determined by the corresponding eigenvalue of each value in the vector.
You can discard the feature with the smallest eigenvalue, since the mean-square error be-
tween the transformed vector and the original vector is determined by the eigenvalue of each
feature in the vector. In addition, the transformed feature vector is uncorrelated. This is par-
ticularly suitable for the Gaussian probability density function with a diagonal covariance
matrix.

Signal Processing—Extracting Features
425
The recognition error is the best criterion for deciding what feature sets to use. How-
ever, it is hard to obtain such an estimate to evaluate feature sets systematically. A simpler
approach is to use within-class and between-class scatter matrices to formulate criteria of
class separability, which is also called as Linear Discriminant Analysis (LDA) transforma-
tion. We can compute the within-class scatter matrix as:
(
) {(
)(
) |
}
(
)
i
i
t
w
i
i
i
i
i
i
S
P
E
P
ω
ω
ω
ω
ω
∈
∈
=
−
−
=
Σ


x
x
x
µ
x
µ
(9.7)
where the sum is for all the data x within the class
i
ω . This is the scatter of samples around
their respective class mean. On the other hand, the between-class scatter matrix is the scatter
of the expected vectors around the mixture mean:
0
0
(
)(
)(
)
i
i
t
B
i
i
i
S
P
ω
ω
∈
=
−
−

µ
µ
m
µ
m
(9.8)
where
0
m represents the expected mean vector of the mixture distribution:
0
{ }
(
)
i
i
i
E
P
ω
ω
=
= 
m
x
m
(9.9)
To formulate criteria to transform feature vector x, we need to derive the linear trans-
formation matrix A. One of the measures can be the trace of
1
w
B
S S
−
:
1
(
)
w
B
J
tr S S
−
=
(9.10)
The trace is the sum of the eigenvalues of
1
w
B
S S
−
and hence the sum of the variances in
the principal directions. The number is larger when the between-class scatter is large or the
within-class scatter is small. You can derive the transformation matrix based on the eigen-
vectors of
1
w
B
S S
−
. In a manner similar to PCA, you can reduce the dimension of the original
input feature vector by discarding the smallest eigenvalues [16, 54].
Researchers have used the LDA method to measure the effectiveness of several feature
vectors for speaker normalization [41]. Other feature processing techniques designed for
speaker normalization include neural-network-based speaker mapping [51], frequency warp-
ing for vocal tract normalization (VTN) via mel-frequency scaling [67, 100], and bilinear
transformation [2].
To reduce interspeaker variability by a speaker-specific frequency warping, you can
simply shift the center frequencies of the mel-spaced filter bank. Let
mel
k f
∆
, k = 1, …, K,
denote the center frequencies in mel-scale. Then the center frequencies in hertz for a warping
factor of α are computed by Eq. (9.11) before the cosine transformation of the MFCC fea-
ture vector.
/2595
(
)
700(10
1) /
mel
k f
Hz
mel
f
k f
α
α
∆
∆
=
−
(9.11)

426
Acoustic Modeling
The warping factor is estimated for each speaker by computing the likelihood of the
training data for feature sets obtained with different warping factors using the HMM. The
relative error reduction based on the feature transformation method has been limited, typi-
cally under 10%.
9.4.
PHONETIC MODELING—SELECTING APPROPRIATE
UNITS
As discussed in Chapter 2, the phonetic system is related to a particular language. We focus
our discussion on language-independent technologies but use English in our examples to
illustrate how we can use the language-independent technologies to model the salient pho-
netic information in the language. For general-purpose large-vocabulary speech recognition,
it is difficult to build whole-word models because:
 Every new task contains novel words without any available training data, such as
proper nouns and newly invented jargons.
 There are simply too many words, and these different words may have different
acoustic realizations, as illustrated in Chapter 2. It is unlikely that we have suffi-
cient repetitions of these words to build context-dependent word models.
How to select the most basic units to represent salient acoustic and phonetic informa-
tion for the language is an important issue in designing a workable system. At a high level,
there are a number of issues we must consider in choosing appropriate modeling units.
 The unit should be accurate, to represent the acoustic realization that appears in
different contexts.
 The unit should be trainable. We should have enough data to estimate the pa-
rameters of the unit. Although words are accurate and representative, they are the
least trainable choice in building a working system, since it is nearly impossible
to get several hundred repetitions for all the words, unless we are using a speech
recognizer that is domain specific, such as a recognizer designed for digits only.
 The unit should be generalizable, so that any new word can be derived from a
predefined unit inventory for task-independent speech recognition. If we have a
fixed set of word models, there is no obvious way for us to derive the new word
model.
A practical challenge is how to balance these selection criteria for speech recognition.
In this section we compare a number of units and point out their strengths and weaknesses in
practical applications.

Phonetic Modeling—Selecting Appropriate Units
427
9.4.1.
Comparison of Different Units
What is a unit of language? In English, words are typically considered as a principal carrier
of meaning and are seen as the smallest unit that is capable of independent use. As the most
natural unit of speech, whole-word models have been widely used for many speech recogni-
tion systems. A distinctive advantage of using word models is that we can capture phonetic
coarticulation inherent within these words. When the vocabulary is small, we can create
word models that are context dependent.
For example, if the vocabulary is English digits, we can have different word models for
the word one to represent the word in different contexts. Thus each word model is dependent
on its left and right context. If someone says three one two, the recognizer uses the word
model one that specifically depends on the left context three and right context two. Since the
vocabulary is small (10), we need to have only 10*10*10=1000 word models, which is
achievable when you collect enough training data. With context-dependent, or even context-
independent, word models, a wide range of phonological variations can be automatically
accommodated. When these word models are adequately trained, they usually yield the best
recognition performance in comparison to other modeling units. Therefore, for small vocabu-
lary recognition, whole-word models are widely used, since they are both accurate and
trainable, and there is no need to be generalizable.
While words are suitable units for small-vocabulary speech recognition, they are not a
practical choice for large-vocabulary continuous speech recognition. First, each word has to
be treated individually, and data cannot be shared across word models; this implies a pro-
hibitively large amount of training data and storage. Second, for some task configurations,
the recognition vocabulary may consist of words that never appeared in the training data. As
a result, some form of word-model composition technique is required to generate word mod-
els. Third, it is very expensive to model interword coarticulation effects or adapt a word-
based system for a new speaker, a new channel, or new context usage.
To summarize, word models are accurate if enough data are available. Thus, they are
trainable only for small tasks. They are typically not generalizable.
Alternatively, there are only about 50 phones in English, and they can be sufficiently
trained with just a few hundred sentences. Unlike word models, phonetic models provide no
training problem. Moreover, they are also vocabulary independent by nature and can be
trained on one task and tested on another. Thus, phones are more trainable and generaliz-
able. However, the phonetic model is inadequate because it assumes that a phoneme in any
context is identical. Although we may try to say each word as a concatenated sequence of
independent phonemes, these phonemes are not produced independently, because our articu-
lators cannot move instantaneously from one position to another. Thus, the realization of a
phoneme is strongly affected by its immediately neighboring phonemes. For example, if con-
text-independent phonetic models are used, the same model for t must capture various
events, such as flapping, unreleased stops, and realizations in /t s/ and /t r/. Then, if /t s/ is
the only context in which t occurs in the training, while /t r/ is the only context in the testing,
the model used is highly inappropriate. While word models are not generalizable, phonetic
models overgeneralize and, thus, lead to less accurate models.

428
Acoustic Modeling
A compromise between the word and phonetic model is to use larger units such as syl-
lables. These units encompass phone clusters that contain the most variable contextual ef-
fects. However, while the central portions of these units have no contextual dependencies,
the beginning and ending portions are still susceptible to some contextual effects. There are
only about 1200 tone-dependent syllables in Chinese and approximately 50 syllables in Japa-
nese, which makes syllable a suitable unit for these languages. Unfortunately, the large
number of syllables (over 30,000) in English presents a challenge in terms of trainability.
9.4.2.
Context Dependency
If we make units context dependent, we can significantly improve the recognition accuracy,
provided there are enough training data to estimate these context-dependent parameters.
Context-dependent phonemes have been widely used for large-vocabulary speech recogni-
tion, thanks to its significantly improved accuracy and trainability. A context usually refers
to the immediately left and/or right neighboring phones.
A triphone model is a phonetic model that takes into consideration both the left and the
right neighboring phones. If two phones have the same identity but different left or right con-
texts, they are considered different triphones. We call different realizations of a phoneme
allophones. Triphones are an example of allophones.
The left and right contexts used in triphones, while important, are only two of many
important contributing factors that affect the realization of a phone. Triphone models are
powerful because they capture the most important coarticulatory effects. They are generally
much more consistent than context-independent phone models. However, as context-
dependent models generally have increased parameters, trainability becomes a challenging
issue. We need to balance the trainability and accuracy with a number of parameter-sharing
techniques.
Modeling interword context-dependent phones is complicated. For example, in the
word speech, pronounced /s p iy ch/, both left and right contexts for /p/ and /iy/ are known,
while the left context for /s/ and the right context for /ch/ are dependent on the preceding and
following words in actual sentences. The juncture effect on word boundaries is one of the
most serious coarticulation phenomena in continuous speech, especially with short function
words like the or a. Even with the same left and right context identities, there may be signifi-
cantly different realizations for a phone at different word positions (the beginning, middle, or
end of a word). For example, the phone /t/ in that rock is almost extinct, while the phone /t/
in the middle of theatrical sounds like /ch/. This implies that different word positions have
effects on the realization of the same triphone.
In addition to the context, stress also plays an important role in the realization of a par-
ticular phone. Stressed vowels tend to have longer duration, higher pitch, and more intensity,
while unstressed vowels appear to move toward a neutral, central schwa-like phoneme.
Agreement about the phonetic identity of a syllable has been reported to be greater in
stressed syllables for both humans and automatic phone recognizers. In English, word-level
stress is referred to as free stress, because the stressed syllable can take on any position
within a word, in contrast to bound stress found in languages such as French and Polish,

Phonetic Modeling—Selecting Appropriate Units
429
where the position of the stressed syllable is fixed within a word. Therefore, stress in Eng-
lish can be used as a constraint for lexical access. In fact, stress can be used as a unique fea-
ture to distinguish a set of word pairs, such as import vs. import, and export vs. export. For
example, the phone set used for Whisper, such as /er/-/axr/ and /ah/-/ix/-/ax/, describes these
stressed and unstressed vowels. One example illustrating how stress can significantly affect
the realization of phone is demonstrated in Figure 9.3, where phone /t/ in word Italy vs Ital-
ian is pronounced differently in American English due the location of the stress, albeit the
triphone context is identical for both words.
0
0.1
0.2
0.3
-1500
-1000
-500
0
500
1000
Time (seconds)
Frequency (Hz)
0
0.1
0.2
0.3
0
1000
2000
3000
4000
0
0.2
0.4
0.6
-3000
-2000
-1000
0
1000
2000
Time (seconds)
Frequency (Hz)
0
0.2
0.4
0.6
0
1000
2000
3000
4000
Figure 9.3 The importance of stress is illustrated in Italy vs. Italian for phone /t/.
Sentence-level stress, on the other hand, represents the overall stress pattern of con-
tinuous speech. While sentence-level stress does not change the meaning of any particular
lexicon item, it usually increases the relative prominence of portions of the utterance for the
purpose of contrast or emphasis. Contrastive stress is normally used to coordinate construc-
tions such as there are import records and there are domestic ones, as well as for the pur-
pose of correction, as in I said import, not export. Emphatic stress is commonly used to dis-
tinguish a sentence from its negation, e.g., I did have dinner. Sentence-level stress is very
hard to model without incorporating high-level semantic and pragmatic knowledge. In most
state-of- the-art speech recognition systems, only word-level stress is used for creating allo-
phones.

430
Acoustic Modeling
9.4.3.
Clustered Acoustic-Phonetic Units
Triphone modeling assumes that every triphone context is different. Actually, many phones
have similar effects on the neighboring phones. The position of our articulators has an
important effect on how we pronounce neighboring vowels. For example, /b/ and /p/ are
both labial stops and have similar effects on the following vowel, while /r/ and /w/ are
both liquids and have similar effects on the following vowel. Contrary to what we illustrate
in Figure 9.1, Figure 9.4 illustrates this phenomenon. It is desirable to find instances of simi-
lar contexts and merge them. This would lead to a much more manageable number of models
that can be better trained.
Figure 9.4 The spectrograms for the phoneme /iy/ with two different left-contexts are illus-
trated. Note that /r/ and /w/ have similar effects on /iy/. This illustrates that different left-
contexts may have similar effects on a phone.
The trainability and accuracy balance between phonetic and word models can be gen-
eralized further to model subphonetic events. In fact, both phonetic and subphonetic units
have the same benefits, as they share parameters at unit level. This is the key benefit in com-
parison to the word units. Papers by [11, 45, 57, 66, 111] provide examples of the applica-
tion of this concept to cluster hidden Markov models. For subphonetic modeling, we can
treat the state in phonetic HMMs as the basic subphonetic unit. Hwang and Huang further
generalized clustering to the state-dependent output distributions across different phonetic
models [57]. Each cluster thus represents a set of similar Markov states and is called a
senone [56]. A subword model is thus composed of a sequence of senones after the cluster-
0
0.1
0.2
0.3
0.4
-600
-400
-200
0
200
400
Time (seconds)
Frequency
(Hz)
0
0.1
0.2
0.3
0.4
0
1000
2000
3000
4000
0
0.1
0.2
0.3
0.4
-600
-400
-200
0
200
400
Time (seconds)
Frequency
(Hz)
0
0.1
0.2
0.3
0.4
0
1000
2000
3000
4000

Phonetic Modeling—Selecting Appropriate Units
431
ing is finished. The optimal number of senones for a system is mainly determined by the
available training corpus and can be tuned on a development set.
Each allophone model is an HMM made of states, transitions, and probability distribu-
tions. To improve the reliability of the statistical parameters of these models, some distribu-
tions can be tied. For example, distributions for the central portion of an allophone may be
tied together to reflect the fact that they represent the stable (context-independent) physical
realization of the central part of the phoneme, uttered with a stationary configuration of the
vocal tract. Clustering at the granularity of the state rather than the entire model can keep the
dissimilar states of two models apart while the other corresponding states are merged, thus
leading to better parameter sharing.
Figure 9.5 illustrates how state-based clustering can lead to improved representations.
These two HMMs come from the same phone class with a different right context, leading to
very different output distributions in the last state. As the left contexts are identical, the first
and second output distributions are almost identical. If we measure the overall model similar-
ity based on the accumulative overall output distribution similarities of all states, these two
models may be clustered, leading to a very inaccurate distribution for the last state. Instead,
we cluster the first two output distributions while leaving the last one intact.
Figure 9.5 State-based vs. model-based clustering. These two models are very similar ,as both
the first and the second output distributions are almost identical. The key difference is the out-
put distribution of the third state. If we measure the overall model similarity, which is often
based on the accumulative output distribution similarities of all states, these two models may
be clustered, leading to a very inaccurate distribution for the last state. If we cluster output dis-
tributions at state level, we can cluster the first two output distributions while leaving the last
ones intact, leading to more accurate representations.
0
0
1
2
1
b
k
0 ( )
b
k
1( )
b
k
2 ( )
0( )
b k
′
1( )
b k
′
2( )
b k
′
2

432
Acoustic Modeling
Table 9.3 Some example questions used in building senone trees.
Questions
Phones in Each Question Category
Aspseg
Sil
Alvstp
Dental
Labstp
Liquid
Lw
S/Sh
Sylbic
Velstp
Affric
Lqgl-B
Nasal
Retro
Schwa
Velar
Fric2
Fric3
Lqgl
S/Z/Sh/Zh
Wglide
Labial
Palatl
Yglide
High
Lax
Low
Orstp2
Orstp3
Alvelr
Diph
Fric1
Round
Frnt-R
Tense
Back-L
Frnt-L
Back-R
Orstp1
Vowel
Son
Voiced
hh
sil
d t
dh th
b p
l r
l w
s sh
er axr
g k
ch jh
l r w
m n ng
r er axr
ax ix axr
ng g k
th s sh f
dh z zh v
l r w y
s z sh zh
uw aw ow w
w m b p v
y ch jh sh zh
iy ay ey oy y
ih ix iy uh uw y
eh ih ix uh ah ax
ae aa ao aw ay oy
p t k
b d g
n d t s z
uw aw ay ey iy ow oy
dh th s sh z zh v f
uh ao uw ow oy w axr er
ae eh ih ix iy ey ah ax y aw
iy ey ae uw ow aa ao ay oy aw
uh ao uw ow aa er axr l r w aw
ae eh ih ix iy ey ah ax y oy ay
uh ao uw ow aa er axr oy l r w ay
b d g p t k ch jh
ae eh ih ix iy uh ah ax aa ao uw aw ay ey ow oy er axr
ae eh ih ix iy ey ah ax oy ay uh ao uw ow aa er axr aw l r w y
ae eh ih ix iy uh ah ax aa ao uw aw ay ey ow oy l r w y er axr m
n ng jh b d dh g v z zh

Phonetic Modeling—Selecting Appropriate Units
433
There are two key issues in creating trainable context-dependent phonetic or subpho-
netic units:
 We need to enable better parameter sharing and smoothing. As Figure 9.4 illus-
trates, many phones have similar effects on the neighboring phones. If the acous-
tic realization is indeed identical, we tie them together to improve trainability
and efficiency.
 Since the number of triphones in English is very large (over 100,000), there are
many new or unseen triphones that are in the test set but not in the training set. It
is important to map these unseen triphones into appropriately trained triphones.
senone 2
welcome
senone 5
senone 6
Is left phone a sonorant or nasal?
Is right phone a back-R?
Is left phone /s,z,sh,zh/?
Is right phone voiced?
Is left phone a back-L or
(is left phone neither a nasal nor a Y-
glide and right phone a LAX-vowel)?
senone 1
senone 3
yes
yes
yes
no
Figure 9.6 A decision tree for classifying the second state of K-triphone HMMs [48].
As discussed in Chapter 4, a decision tree is a binary tree to classify target objects by
asking binary questions in a hierarchical manner. Modeling unseen triphones is particularly
important for vocabulary independence, since it is difficult to collect a training corpus which
covers enough occurrences of every possible subword unit. We need to find models that are
accurate, trainable, and especially generalizable. The senonic decision tree classifies Markov
states of triphones represented in the training corpus by asking linguistic questions composed
of conjunctions, disjunctions, and/or negations of a set of predetermined simple categorical
linguistic questions. Examples of these simple categorical questions are: Is the left-context

434
Acoustic Modeling
phone a fricative? Is the right-context phone a front vowel? The typical question set used in
Whisper to generate the senone tree is shown in Table 9.3. So, for each node in the tree, we
check whether its left or right phone belongs to one of the categories. As discussed in Chap-
ter 4, we measure the corresponding entropy reduction or likelihood increase for each ques-
tion and select the question that has the largest entropy decrease to split the node. Thus, the
tree can be automatically constructed by searching, for each node with the best question that
renders the maximum entropy decrease. Alternatively, complex questions can be formed for
each node for improved splitting.
When we grow the tree, it needs to be pruned using cross-validation as discussed in
Chapter 4. When the algorithm terminates, the leaf nodes of the tree represent the senones to
be used. Figure 9.6 shows an example tree we built to classify the second state of all /k/
triphones seen in a training corpus. After the tree is built, it can be applied to the second state
of any /k/ triphone, thanks to the generalizability of the binary tree and the general linguistic
questions. Figure 9.6 indicates that the second state of the /k/ triphone in welcome is mapped
to the second senone, no matter whether this triphone occurs in the training corpus or not.
In practice, senone models significantly reduce the word recognition error rate in com-
parison with the model-based clustered triphone models, as illustrated in Table 9.4. It is the
senonic model’s significant reduction of the overall system parameters that enables the con-
tinuous mixture HMMs to perform well for large-vocabulary speech recognition [56].
Table 9.4 Relative error reductions for different modeling units.
Units
Relative Error Reductions
Context-independent phone
Baseline
Context-dependent phone
+25%
Clustered triphone
+15%
Senone
+24%
9.4.4.
Lexical Baseforms
When appropriate subword units are used, we must have the correct pronunciation for each
word so that concatenation of the subword unit can accurately represent the word to be rec-
ognized. The dictionary represents the standard pronunciation used as a starting point for
building a workable speech recognition system. We also need to provide alternative pronun-
ciations to words such as tomato that may have very different pronunciations. For example,
the COMLEX dictionary from LDC has about 90,000 baseforms that cover most words used
in many years of The Wall Street Journal. The CMU Pronunciation Dictionary, which was
optimized for continuous speech recognition, has about 100,000 baseforms.
In continuous speech recognition, we must also use phonologic rules to modify inter-
word pronunciations or to have reduced sounds. Assimilation is a typical coarticulation phe-
nomenon—a change in a segment to make it more like a neighboring segment. Typical ex-
amples include phrases such as did you /d ih jh y ah/, set you /s eh ch er/, last year / l ae s ch

Phonetic Modeling—Selecting Appropriate Units
435
iy r/, because you’ve /b iy k ah zh uw v/, etc. Deletion is also common in continuous speech.
For example, /t/ and /d/ are often deleted before a consonant. Thus, in conversational speech,
you may find examples like find him /f ay n ix m/, around this /ix r aw n ih s/, and Let me in
/l eh m eh n/.
Dictionaries often don’t include proper names. For example, the 20,000 names in-
cluded in the COMPLEX dictionary are a small fraction of 1–2 million names in the USA.
To deal with these new words, we often have to derive their pronunciation automatically.
These new words have to be added on the fly, either by the user or through interface from
speech-aware applications. Unlike Spanish or Italian, the rule-based letter-to-sound (LTS)
conversion for English is often impractical, since so many words in English don’t follow the
phonological rules. A trainable LTS converter is attractive, since its performance can be im-
proved by constantly learning from examples so that it can generalize rules for the specific
task. Trainable LTS converters can be based on neural networks, HMMs, or the CART de-
scribed in Chapter 4. In practice, the CART-based LTS has a very accurate performance [10,
61, 71, 89].
When the CART is used, the basic YES-NO question for LTS conversion looks like: Is
the second right letter ‘p’? or: Is the first left output phone /ay/? The question for letters and
phones can be on either the left or the right side. The range of question positions should be
long enough to cover the most important phonological variations. Empirically, the 10-letter
window (5 for left letter context and 5 for right letter context) and 3-phone window context
is generally sufficient. A primitive set of questions can include all the singleton questions
about each letter or phone identity. If we allow the node to have a complex question—that is,
a combination of primitive questions—the depth of the tree can be greatly reduced and per-
formance improved. For example, a complex question: Is the second left letter ‘t’ and the
first left letter ‘i’ and the first right letter ‘n’? can capture o in the common suffix tion and
convert it to the correct phone. Complex questions can also alleviate possible data-
fragmentation problems caused by the greedy nature of the CART algorithm.
Categorical questions can be formed in both the letter and phone domains with our
common linguistic knowledge. For example, the most often used set includes the letter or
phone clusters for vowels, consonants, nasals, liquids, fricatives, and so on. In growing the
decision tree, the context distance also plays a major role in the overall quality. It is very
important to weight the entropy reduction according to the distance (either letter or pho-
neme) to avoid overgeneralization, which forces the tree to look more carefully at the nearby
context than at the far-away context. Each leaf of the tree has a probability distribution for
letter-to-phoneme mapping.
There are a number of ways to improve the effectiveness of the decision tree. First,
pruning controls the tree’s depth. For example, certain criteria have to be met for a node to
be split. Typically splitting requires a minimum number of counts and a minimum entropy
reduction. Second, the distribution at the leaves can be smoothed. For example, a leaf distri-
bution can be interpolated with the distributions of its ancestor nodes using deleted-
interpolation. Finally, we can partition the training data and build multiple trees with differ-
ent prediction capabilities. These trees accommodate different phonological rules with dif-
ferent language origins.

436
Acoustic Modeling
When the decision tree is used to derive the phonetic pronunciation, the phonetic con-
version error is about 8% for the Wall Street Journal newspaper text corpora [61]. These
errors can be broadly classified into two categories. The first includes errors of proper nouns
and foreign words. For example, Pacino can be mistakenly converted to /p ax s iy n ow /
instead of /p ax ch iy n ow/. The second category includes generalization errors. For exam-
ple, shier may be converted to /sh ih r/ instead of the correct pronunciation /sh ay r/ if the
word cashier /k ae sh ih r/ appears in the training data. The top three phone confusion pairs
are /ix/ax/, /dx/t/, and /ae/ax/. The most confusing pair is /ix/ax/. This is not surprising, be-
cause /ix/ax/ is among the most inconsistent transcriptions in most of the published dictionar-
ies. There is no consensus for /ix/ax/ transcription among phoneticians.
Although automatic LTS conversion has a reasonable accuracy, it is hardly practical if
you don’t use an exception dictionary. This is especially true for proper nouns. In practice,
you can often ask the person who knows how to pronounce the word to either speak or write
down the correct phonetic pronunciation, updating the exception dictionary if the correct one
disagrees with what the LTS generates. When acoustic examples are available, you can use
the decision tree to generate multiple results and use these results as a language model to
perform phone recognition on the acoustic examples. The best overall acoustic and LTS
probability can be used as the most likely candidate in the exception dictionary. Since there
may be many ways to pronounce a word, you can keep multiple pronunciations in the dic-
tionary with a probability for each possible one. If the pronunciation probability is inaccu-
rate, an increase in multiple pronunciations essentially increases the size and confusion of the
vocabulary, leading to increased speech recognition error rate.
Even if you have accurate phonetic baseform, pronunciations in spontaneous speech
differ significantly from the standard baseform. Analysis of manual phonetic transcription of
conversational speech reveals a large number (> 20%) of cases of genuine ambiguity: in-
stances where human labelers disagree on the identity of the surface form [95]. For example,
the word because has more than 15 different pronunciation variations, such as /b iy k ah z/,
/b ix k ah z/, /k ah z/, /k ax, z/, /b ix k ax z/, /b ax k ah z/, /b ih k ah z/, /k s/, /k ix z/, /k ih z/,/b
iy k ah s/, /b iy k ah/, /b iy k ah zh/, /ax z/, etc., in the context of conversational speech [39].
To characterize the acoustic evidence in the context of this ambiguity, you can partly resolve
the ambiguity by deriving a suitable phonetic baseform from speech data [29, 95, 97]. This is
because the widespread variation can be due either to a lexical fact (such as that the word
because can be ’cause with informal speech) or to the dialect differences. African American
Vernacular English has many vowels different from general American English.
To incorporate widespread pronunciations, we can use a probabilistic finite state ma-
chine to model each word’s pronunciation variations, as shown in Figure 9.7. The probability
with each arc indicates how likely that path is to be taken, with all the arcs that leave a node
summing to 1. As with HMMs, these weights can be estimated from real corpus for im-
proved speech recognition [20, 85, 102, 103, 110]. In practice, the relative error reduction of
using probabilistic finite state machines is very modest (5–10%).

Acoustic Modeling—Scoring Acoustic Features
437
/ow/
/dx/
/t/
/aa/
/ey/
/m/
/ow/
/t/
/ax/
0.5
0.3
0.2
0.7
0.3
0.8
0.2
0.8
0.2
Figure 9.7 A possible pronunciation network for word tomato. The vowel /ey/ is more likely
to flap, thereby having a higher transition probability into /dx/.
9.5.
ACOUSTIC MODELING—SCORING ACOUSTIC FEATURES
After feature extraction, we have a sequence of feature vectors, X, such as the MFCC vector,
as our input data. We need to estimate the probability of these acoustic features, given the
word or phonetic model, W, so that we can recognize the input data for the correct word.
This probability is referred to as acoustic probability, P(X | W). In this section we focus our
discussion on the HMM. As discussed in Chapter 8, it is the most successful method for
acoustic modeling. Other emerging techniques are discussed in Section 9.8.
9.5.1.
Choice of HMM Output Distributions
As discussed in Chapter 8, you can use discrete, continuous, or semicontinuous HMMs.
When the amount of training data is sufficient, parameter tying becomes unnecessary. A con-
tinuous model with a large number of mixtures offers the best recognition accuracy, although
its computational complexity also increases linearly with the number of mixtures. On the
other hand, the discrete model is computationally efficient, but has the worst performance
among the three models. The semicontinuous model provides a viable alternative between
system robustness and trainability.
When either the discrete or the semicontinuous HMM is employed, it is helpful to use
multiple codebooks for a number of features for significantly improved performance. Each
codebook then represents a set of different speech parameters. One way to combine these
multiple output observations is to assume that they are independent, computing the output
probability as the product of the output probabilities of each codebook. For example, the
semicontinuous HMM output probability of multiple codebooks can be computed as the
product of each codebook:
1
( )
(
|
)
(
)
m
L
m
m
m
m
m
i
k
i
k
m
k
b
f
o
b
o
=
=∏
x
x
(9.12)

438
Acoustic Modeling
where superscript m denotes the codebook-m related parameters. Each codebook consists of
m
L -mixture continuous density functions.
Following our discussion in Chapter 8, the re-estimation algorithm for the multiple-
codebook-based HMM could be extended. Since multiplication of the output probability
density of each codebook leads to several independent terms in the Q-function, for codebook
m,
( ,
)
m
t j k
ζ
can be modified as follows:
1( )
(
)
(
|
)
(
)
(
|
)
( )
( ,
)
( )
m
m
m
m
n
n
n
n
t
ij
j
t
k
j
t
k
t
m
i
m n
k
t
m
T
k
i a b
k
f
v
b
k
f
v
j
j k
k
α
β
ζ
α
−
≠
= 
∏

x
x
(9.13)
Other intermediate probabilities can also be computed in a manner similar to what we
discussed in Chapter 8.
Multiple codebooks can dramatically increase the representation power of the VQ
codebook and can substantially improve speech recognition accuracy. You can typically
build a codebook for
kc ,
kc

, and
kc

, respectively. As energy has a very different dy-
namic range, you can further improve the performance by building a separate codebook for
[0]
kc
,
[0]
kc

, and
[0]
kc

. In comparison to building a single codebook for
k
x
as illus-
trated in Eq. (9.6), the multiple-codebook system can reduce the error rate by more than
10%.
In practice, the most important parameter for the output probability distribution is the
number of mixtures or the size of the codebooks. When there are sufficient training data,
relative error reductions with respect to the discrete HMM are those shown in Figure 9.8.
0
2
4
6
8
10
12
1
2
3
4
5
6
Training Set Size (thousands)
Word Error Rate
DHMM
SCHMM
CHMM
Figure 9.8 Continuous speaker-independent word recognition error rates of the discrete HMM
(DHMM), SCHMM, and the continuous HMM (CHMM) with respect to the training set sizes
(thousands of training sentences). Both the DHMM and SCHMM have multiple codebooks.
The CHMM has 20 mixture diagonal Gaussian density functions.
As you can see from Figure 9.8, the SCHMM offers improved accuracy in comparison
with the discrete HMM or the continuous HMM when the amount of training data is limited.

Acoustic Modeling—Scoring Acoustic Features
439
When we increase the training data size, the continuous mixture density HMM starts to out-
perform both the discrete and the semicontinuous HMM, since the need to share the model
parameters becomes less critical.
The performance is also a function of the number of mixtures. With a small number of
mixtures, the continuous HMM lacks the modeling power and it actually performs worse
than the discrete HMM across the board. Only after we dramatically increase the number of
mixtures does the continuous HMM start to offer improved recognition accuracy. The
SCHMM can typically reduce the discrete HMM error rate by 10–15% across the board. The
continuous HMM with 20 diagonal Gaussian density functions performed worse than either
the discrete or the SCHMM when the size of training data was small. It outperformed either
the discrete HMM or the SCHMM when sufficient amounts of training data became avail-
able. When the amount of training data is sufficiently large, it can reduce the error rate of the
semicontinuous HMM by 15–20%.
9.5.2.
Isolated vs. Continuous Speech Training
If we build a word HMM for each word in the vocabulary for isolated speech recognition,
the training or recognition can be implemented directly, using the basic algorithms intro-
duced in Chapter 8. To estimate model parameters, examples of each word in the vocabulary
are collected. The model parameters are estimated from all these examples using the for-
ward-backward algorithm and the reestimation formula. It is not necessary to have precise
end-point detection, because the silence model automatically determines the boundary if we
concatenate silence models with the word model in both ends.
If subword units,3 such as phonetic models, are used, we need to share them across dif-
ferent words for large-vocabulary speech recognition. These subword units are concatenated
to form a word model, possibly adding silence models at the beginning and end, as illustrated
in Figure 9.9.
To concatenate subword units to form a word model, you can have a null transition
from the final state of the previous subword HMM to the initial state of the next subword
HMM, as indicated by the dotted line in Figure 9.9. As described in Chapter 8, you can esti-
mate the parameters of the concatenated HMM accordingly. Please notice that the added null
transition arc should satisfy the probability constraint with the transition probability of each
phonetic HMM. The self-loop transition probability of the last state in each individual HMM
has the topology illustrated in Figure 9.9. If we estimate these parameters with the concate-
nated model, the null arc transition probability,
ij
aε , should satisfy the constraint
(
)
1
ij
ij
j a
aε
+
=

such that the self-loop transition probability of the last state is no longer
equal to 1. For interword concatenation or concatenation involving multiple pronunciations,
you can use multiple null arcs to concatenate individual models together.
3 We have a detailed discussion on word models vs. subword models in Section 9.4.1.

440
Acoustic Modeling
Figure 9.9 The construction of an isolated word model by concatenating multiple phonetic
models based on the pronunciation dictionary.
In the example given here, we have ten English digits in the vocabulary. We build an
HMM for each English phone. The dictionary provides the information on each word’s pro-
nunciation. We have a special wordn, Silence, that maps to a /sil/ HMM that has the same
topology as the standard phonetic HMM. For each word in the vocabulary we first derive the
phonetic sequence for each word from the dictionary. We link these phonetic models to-
gether to form a word HMM for each word in the vocabulary. The link between two phonetic
models is shown in the figure as the dotted arrow.
For example, for word two, we create a word model based on the beginning silence
/sil/, phone /t/, phone /uw/, and ending silence /sil/. The concatenated word model is then
treated in the same manner as a standard large composed HMM. We use the standard for-
ward-backward algorithm to estimate the parameters of the composed HMM from multiple
sample utterances of the word two. After several iterations, we automatically get the HMM
parameters for /sil/, /t/, and /uw/. Since a phone can be shared across different words, the
phonetic parameters may be estimated from acoustic data in different words.
The ability to automatically align each individual HMM to the corresponding unseg-
mented speech observation sequence is one of the most powerful features in the forward-
backward algorithm. When the HMM concatenation method is used for continuous speech,
you need to compose multiple words to form a sentence HMM based on the transcription of
the utterance. In the same manner, the forward-backward algorithm absorbs a range of possi-
ble word boundary information of models automatically. There is no need to have a precise
segmentation of the continuous speech.
/aa/
/ae/
…
Phonetic HMMs
Dictionary
One
/w ah n/
Two
/t uw/
…
Zero
/z ih r ow/
Silence
/sil/
A composed HMM for word two:
/sil/
/t/
/uw/
/sil/

Acoustic Modeling—Scoring Acoustic Features
441
In general, to estimate the parameters of the HMM, each word is instantiated with its
concatenated word model (which may be a concatenation of subword models). The words in
the sentence are concatenated with optional silence models between them. If there is a need
to modify interword pronunciations due to interword pronunciation change, such as want
you, you can add a different optional phonetic sequence for t-y in the concatenated sentence
HMM.
In the digit recognition example, if we have a continuous training utterance one three,
we compose a sentence HMM, as shown in Figure 9.10, where we have an optional silence
HMM between the words one and three, linked with a null transition from the last state of
the word model one to the thirst state of the word model three. There is also a direct null arc
connection between the models one and three because a silence may not exist in the training
example. These optional connections ensure that all the possible acoustic realizations of the
natural continuous speech are considered, so that the forward-backward algorithm can auto-
matically discover the correct path and accurately estimate the corresponding HMM from the
given speech observation.
Figure 9.10 A composed sentence HMM. Each word can be a word HMM or a composed
phonetic word HMM, as illustrated in Figure 9.9.
In general, the concatenated sentence HMM can be trained using the forward-
backward algorithm with the corresponding observation sequence. Since the entire sentence
HMM is trained on the entire observation sequence for the corresponding sentence, most
possible word boundaries are inherently considered. Parameters of each model are based on
those state-to-speech alignments. It does not matter where the word boundaries are. Such a
training method allows complete freedom to align the sentence model against the observa-
tion, and no explicit effort is needed to find word boundaries.
In speech decoding, a word may begin and end anywhere within a given speech signal.
As the word boundaries cannot be detected accurately, all possible beginning and end points
have to be accounted for. This converts a linear search (as for isolated word recognition) to a
tree search, and a polynomial recognition algorithm to an exponential one. How to design an
efficient decoder is discussed in Chapters 12 and 13.
/sil/
one
three
/sil/
/sil/

442
Acoustic Modeling
9.6.
ADAPTIVE TECHNIQUES—MINIMIZING MISMATCHES
As Figure 1.2 illustrated, it is important to adapt both acoustic models and language models
for new situations. A decent model can accommodate a wide range of variabilities. However,
the mismatch between the model and operating conditions always exists. One of the most
important factors in making a speech system usable is to minimize the possible mismatch
dynamically with a small amount of calibration data. Adaptive techniques can be used to
modify system parameters to better match variations in microphone, transmission channel,
environment noise, speaker, style, and application contexts. As a concrete example, speaker-
dependent systems can provide a significant word error-rate reduction in comparison to
speaker-independent systems if a large amount of speaker-dependent training data exists
[50]. Speaker-adaptive techniques can bridge the gap between these two configurations with
a small fraction of the speaker-specific training data needed to build a full speaker-dependent
system. These techniques can also be used incrementally as more speech is available from a
particular speaker. When speaker-adaptive models are build, you can have not only im-
proved accuracy but also improved speed and potentially reduced model parameter sizes
because of accurate representations, which is particularly appealing for practical speech rec-
ognition.
There are a number of ways to use adaptive techniques to minimize mismatches. You
can have a nonintrusive adaptation process that works in the background all the time. This is
typically unsupervised, using only the outcome of the recognizer (with a high confidence
score, as discussed in Section 9.7) to guide the model adaptation. This approach can con-
tinuously modify the model parameters so that any nonstationary mismatches can be elimi-
nated. As discussed in Chapter 13, systems that are required to transcribe speech in a non-
real-time fashion may use multiple recognition passes. You can use unsupervised adaptation
on the test data to improve the models after each pass to improve performance for a subse-
quent recognition pass.
Since the use of recognition results may be imperfect, there is a possibility of diver-
gence if the recognition error rate is high. If the error rate is low, the adaptation results may
still not be as good as supervised adaptation in which the correct transcription is provided for
the user to read, a process referred to as the enrollment process. In this process you can
check a wide range of parameters as follows:
 Check the background noise by asking the user not to speak.
 Adjust the microphone gain by asking the user to speak normally.
 Adapt the acoustic parameters by asking the user to read several sentences.
 Change the decoder parameters for the best speed with no loss of accuracy.
 Compose dynamically new enrollment sentences based on the user-specific error
patterns.
The challenge for model adaptation is that we can use only a small amount of observ-
able data to modify model parameters. This constraint requires different modeling strategies
from the ones we discussed in building the baseline system, as the amount of training data is

Adaptive Techniques—Minimizing Mismatches
443
generally sufficient for offline training. In this section we focus on a number of adaptive
techniques that can be applied to compensate either speaker or environment variations. Most
of these techniques are model-based, since the acoustic model parameters rather than the
acoustic feature vectors are adapted. We use speaker-adaptation examples to illustrate how
these techniques can be used to improve system performance. We can generalize to envi-
ronment adaptation by using environment-specific adaptation data and a noise-compensation
model, which we discuss in Chapter 10. In a similar manner, we can modify the language
model as discussed in Chapter 11.
9.6.1.
Maximum a Posteriori (MAP)
Maximum a posteriori (MAP) estimation, as discussed in Chapter 4, can effectively deal
with data-sparse problems, as we can take advantage of prior information about existing
models. We can adjust the parameters of pretrained models in such a way that limited new
training data would modify the model parameters guided by the prior knowledge to compen-
sate for the adverse effect of a mismatch [35]. The prior density prevents large deviations of
the parameters unless the new training data provide strong evidence.
More specifically, we assume that an HMM is characterized by a parameter vector 
that is a random vector, and that prior knowledge about the random vector is available and
characterized by a prior probability density function p(
)
 , whose parameters are to be de-
termined experimentally.
With the observation data X , the MAP estimate is expressed as follows:
ˆ
argmax [ (
|
)]
argmax [ (
|
) (
)]
p
p
p
Φ
Φ
Φ =
Φ
=
Φ
Φ
X
X
(9.14)
If we have no prior information, p(
)
 is the uniform distribution, and the MAP esti-
mate becomes identical to the ML estimate. We can use the EM algorithm as the ML to es-
timate the parameters of HMMs. The corresponding Q-function can be defined as:
ˆ
ˆ
ˆ
(
,
)
log (
)
(
,
)
MAP
Q
p
Q
Φ Φ =
Φ +
Φ Φ
(9.15)
The EM algorithm for the ML criterion can be applied here directly. The actual ex-
pression depends on the assumptions made about the prior density. For the widely used con-
tinuous Gaussian mixture HMM, there is no joint conjugate prior density. We can assume
different components of the HMM to be mutually independent, so that the optimization can
be split into different subproblems involving only a single component of the parameter set.
For example, the prior density function for the mixture Gaussian can be as follows:
( ,
,
)
(
)
(
,
)
i
i
ik
b
i
i
i
c
i
b
ik
ik
k
p
p
p
=
∏
c µ Σ
c
µ
Σ
(9.16)
where
(
)
ic
i
p
c
is a Dirichlet prior density for the mixing coefficient vector of all mixture
components in the Markov state i, and
(
,
)
ik
b
ik
ik
p
µ
Σ
denotes the prior density for parameters

444
Acoustic Modeling
of the kth Gaussian component in the state i. The Dirichlet prior density
(
)
ic
i
p
c
is character-
ized by a vector
iυ of positive hyperparameters such that:
1
(
)
ik
ic
i
ik
k
p
c
υ −
∝∏
c
(9.17)
For full covariance D-dimensional Gaussian densities, the prior density can be a nor-
mal-Wishart density parameterized by two values 




D
1
0
,
, the vector
nw
µ
, and the
symmetric positive definite matrix S as follows:
1
1
(
,
)
1
det(
)
exp(
(
)
(
)
(
))
2
2
ik
b
ik
ik
D
t
ik
ik
nw
ik
ik
nw
ik
p
tr
η
τ
−
−
−
∝
−
−
−
−
µ
Σ
Σ
µ
µ
Σ
µ
µ
SΣ
(9.18)
We can apply the same procedure as the MLE Baum-Welch reestimation algorithm.
For example, with the Q-function defined in Eq. (9.15), we can apply the Lagrange method
to derive the mixture coefficients as follows:
ˆ
ˆ
(log
( )
( , )log
)
0,
ˆ
ˆ
1
ic
i
t
ik
k
t
ik
ik
k
p
i k
c
k
c
c
ξ
λ
∂

+
+
=
∀
∂


=



c
(9.19)
Based on Eqs. (9.17) and (9.19), the solution is:
1
( , )
ˆ
(
1
( , ))
ik
t
t
ik
il
t
l
t
i k
c
i l
υ
ξ
υ
ξ
−+
=
−+



(9.20)
A comparison between Eq. (9.20) and the ML estimate Eq. (8.58) shows that the MAP
estimate is a weighted average between the mode of the prior density and the ML estimate
with proportions given by  ijk 1 and
( , )
t
t
i k
ξ

, respectively.
We can optimize Eq. (9.15) with respect to mean and covariance parameters in a simi-
lar fashion. For example, the solution of these estimates is:
1
1
( , )
ˆ
( , )
ik
T
ik
nw
t
t
t
ik
T
ik
t
t
i k
i k
τ
ζ
τ
ζ
=
=
+
=
+


µ
x
µ
(9.21)

Adaptive Techniques—Minimizing Mismatches
445
1
1
ˆ
ˆ
ˆ
ˆ
(
)(
)
( , )(
)(
)
ˆ
( , )
ik
ik
T
t
t
ik
ik
ik
nw
ik
nw
t
ik
ik
t
ik
T
ik
t
t
i k
D
i k
τ
ζ
η
ζ
=
=
+
−
−
+
−
−
=
−
+


S
µ
µ
µ
µ
x
µ
x
µ
Σ
(9.22)
where
ik
τ is the parameter in the normal-gamma density for the corresponding state i.
Thus, the reestimation formula for the Gaussian mean is a weighted sum of the prior
mean with the ML mean estimate mean
1
1
( , )
/
( , )
T
T
t
t
t
t
t
i k
i k
ζ
ζ
=
=


x
.
ik
τ
is a balancing factor
between prior mean and the ML mean estimate. When
ik
τ
is large, the variance of the prior
knowledge is small and the value of the mean
ik
nw
µ
is assumed to have high certainty, leading
to the dominance of the final estimate. When the amount of adaptation data increases, the
MAP estimate approaches the ML estimate, as the adaptation data overwrite any important
prior that may influence the final estimate. Similarly, the covariance estimation formula has
the same interpretation of the balance between the prior and new data.
One major limitation of the MAP-based approach is that it requires an accurate initial
guess for the prior p(
)
 , which is often difficult to obtain. We can use the already trained
initial models that embody some characteristics of the original training conditions. A typical
way to generate an initial Gaussian prior is to cluster the initial training data based on
speaker or environment similarity measures. We can derive a set of models based on the par-
tition, which can be seen as a set of observations drawn from a distribution having p(
)
 . We
can, thus, estimate the prior based on the sample moments to derive the corresponding prior
parameters.
Another major limitation is that the MAP-based approach is a local approach to updat-
ing the model parameters. Namely, only model parameters that are observed in the adapta-
tion data can be modified from the prior value. When the system has a large number of free
parameters, the adaptation can be very slow. Thus in practice we need to find correlations
between the model parameters, so that the unobserved or poorly adapted parameters can be
altered [3, 22]. Another possibility is to impose structural information so the model parame-
ters can be shared for improved adaptation speed [96].
The MAP training can be iterative, too, which requires an initial estimate of model pa-
rameters. A careful initialization for the Gaussian densities is also very important. Unlike the
discrete distributions, there is no such a thing as a uniform density for a total lack of informa-
tion about the value of the parameters. We need to use the same initialization procedure as
discussed in Chapter 8.
For speaker-adaptive speech recognition, it has been experimentally found that
ik
τ
can
be a fixed constant value for all the Gaussian components across all the dimensions. Thus the
MAP HMM can be regarded as an interpolated model between the speaker-independent and
speaker-dependent HMM. Both are derived from the standard ML forward-backward algo-
rithm. Experimental performance of MAP training is discussed in Section 9.6.3.

446
Acoustic Modeling
9.6.2.
Maximum Likelihood Linear Regression (MLLR)
When the continuous HMM is used for acoustic modeling, the most important parameter set
to adapt is the output Gaussian density parameters, i.e., the mean vector and the covariance
matrix. We can use a set of linear regression transformation functions to map both means and
covariances in order to maximize the likelihood of the adaptation data [68]. The maximum
likelihood linear regression (MLLR) mapping is consistent with the underlying criterion for
building the HMM while keeping the number of free parameters under control. Since the
transformation parameters can be estimated from a relatively small amount of adaptation
data, it is very effective for rapid adaptation. MLLR has been widely used to obtain adapted
models for either a new speaker or a new environment condition.
More specifically, in the mixture Gaussian density functions, the kth mean vector
ik
µ
for each state i can be transformed using following equation:
ik
c
ik
c
=
+
µ
A µ
b

(9.23)
where A c is a regression matrix and
c
b
is an additive bias vector associated with some
broad class c, which can be either a broad phone class or a set of tied Markov states. The
goal of Eq. (9.23) is to map the mean vector into a new space such that the mismatch can be
eliminated. Because the amount of adaptation data is small, we need to make sure the num-
ber of broad classes c is small so we have only a small number of free parameters to esti-
mate. Equation (9.23) can be simplified into:
ik
c
ik
=
µ
W µ

(9.24)
where
ik
µ
is extended as [1,
]
t
t
ik
µ
and Wc is the extended transform, [ ,
]
b A .
This mapping approach is based on the assumption that Wc can be tied for a wide
range of broad phonetic classes so that the overall free parameters are significantly less than
the number of the mean vectors. Therefore, the same transformation can be used for several
distributions if they represent similar acoustic characteristics.
To estimate these transformation parameters in the MLE framework, we can use the
same Q-function we discussed in Chapter 8. We need to optimize only
1
ˆ
(
,
)
i
M
b
ik
i
k
Q
=
Φ

b
(9.25)
with respect to Wc. Maximization of
ˆ
(
,
)
ib
ik
Q
Φ b
with respect to Wc can be achieved by
computing the partial derivatives. For the Gaussian mixture density function, the partial de-
rivative with respect to Wc is:
1
ˆ ( )
ˆ
( ,
,
)
(
)
t
ik
ik
ik
ik
c
ik
ik
c
b
N
−
∂
=
−
∂
x
x µ
Σ
Σ
x
W µ
µ
W

(9.26)

Adaptive Techniques—Minimizing Mismatches
447
Let us denote the set of Gaussian components forming the broad transformation classes
as C; we use
ik
b
C
∈
to denote that the kth Gaussian density in state i belongs to the class.
We can expand the Q-function with the partial derivatives and set it to zero, leading to the
following equation:
1
1
1
1
( , )
( , )
ik
ik
T
T
t
t
t
ik
t
ik
t
ik
c
ik
ik
t
b
C
t
b
C
i k
i k
ζ
ζ
−
−
=
∈
=
∈
=
 
 
Σ x µ
Σ W µ µ
(9.27)
We can rewrite Eq. (9.27) as:
ik
ik
c
ik
b
C
∈
= 
Z
V W D
(9.28)
where
1
1
( , )
ik
T
t
t
ik
t
ik
t
b
C
i k
ζ
−
=
∈
=  
Z
Σ x µ ,
(9.29)
1
1
( , )
T
ik
t
ik
t
i k
ζ
−
=
= 
V
Σ
,
(9.30)
and
t
ik
ik
ik
=
D
µ µ .
(9.31)
Estimating Wc for Eq. (9.28) is computationally expensive, as it requires solving si-
multaneous equations. Nevertheless, if we assume that the covariance matrix is diagonal, we
can have a closed-form solution that is computationally efficient. Thus, we can define
ijk
q
qq
ik
b
C
v
∈
= 
G
D
(9.32)
where vqq denotes the qth diagonal element of matrix
ik
V . The transformation matrix can be
computed row by row. So for the qth row of the transformation matrix Wq , we can derive it
from the qth row of Zq [defined in Eq. (9.29)] as follows:
W
Z G
q
q
q
=
−1
(9.33)
Since Gq may be a singular matrix, we need to make sure we have enough training
data for the broad class. Thus, if the amount of training data is limited, we must tie a number
of transformation classes together.
We can run several iterations to maximize the likelihood for the given adaptation data.
At each iteration, transformation matrices can be initialized to identity transformations. We
can iteratively repeat the process to update the means until convergence is achieved. We can

448
Acoustic Modeling
also incrementally adapt the mean vectors after each observation sequence or set of observa-
tion sequences while the required statistics are accumulated over time. Under the assumption
that the alignment of each observation sequence against the model is reasonably accurate, we
can accumulate these estimated counts over time and use them incrementally. In order to deal
with the tradeoff between specificity and robust estimation, we can dynamically generate
regression classes according to the senone tree. Thus, we can incrementally increase the
number of regression classes when more and more data become available.
MLLR adaptation can be generalized to include the variances with the ML framework,
although the additional gain after mean transformation is often less significant (less than rela-
tive 2% error reduction). When the user donates about 15 sentences for enrollment training,
Table 9.5 illustrates how the MLLR adaptation technique can be used to further reduce the
word recognition error rate for a typical dictation application. Here, there is only one con-
text-independent phonetic class for all the context-dependent Gaussian densities. As we can
see, most of the error reduction came from adapting the mean vector.
We can further extend MLLR to speaker-adaptive training (SAT) [6, 74]. In conven-
tional speaker-independent training, we simply use data from different speakers to build a
speaker-independent model. An inherent difficulty in this approach is that spectral variations
of different speakers give the speaker-independent acoustic model higher variance than the
corresponding speaker-dependent model. We can include MLLR transformation in the proc-
ess of training to derive the MLLR parameters for each individual speaker. Thus the training
data are transformed to maximize the likelihood for the overall speaker-independent model.
This process can be run iteratively to reduce mismatches of different speakers. By explicitly
accounting for the interspeaker variations during training and decoding, SAT reduces the
error rate by an additional 5–10%.
Table 9.5 Relative error reductions with MLLR methods.
Models
Relative Error Reduction
CHMM
Baseline
MLLR on mean only
12%
MLLR on mean and variance
+2%
MLLR SAT
+8%
9.6.3.
MLLR and MAP Comparison
The MLLR method can be combined with MAP. This guarantees that with the increased
amount of training data, we can have, not only a set of compact MLLR transformation func-
tions for rapid adaptation, but also directly modified model parameters that converge with
ML estimates. We can use MAP to adapt the model parameters and then add MLLR to trans-
form these adapted models. It is also possible to incorporate the MAP principle directly into
MLLR [18, 19].

Adaptive Techniques—Minimizing Mismatches
449
As an example, the result of a 60,000-word dictation application using various adapta-
tion methods is shown in Figure 9.11.4 The speaker-dependent model used 1000 utterances.
Also included as a reference is the speaker-independent result, which is used as the starting
point for adaptive training. When the speaker-independent model is adapted with about 200
utterances, the speaker-adaptive model has already outperformed both speaker-independent
and speaker-dependent systems. The results clearly demonstrate that we have insufficient
training data for speaker-dependent speech recognition, as MAP-based outperform ML-
based models. This also illustrates that we can make effective use of speaker-independent
data for speaker-dependent speech recognition. Also, notice that the MLLR method has a
faster adaptation rate than the MAP method. The MLLR method has context-independent
phonetic classes. So, when the amount of adaptation data is limited, the MLLR method of-
fers better overall performance.
However, the MAP becomes more accurate when the amount of adaptation data in-
creases to 600 per speaker. This is because we can modify all the model parameters with the
MAP training, and the MLLR transformation can never have the same degree of freedom as
the MAP method. When the MLLR is combined with MAP, we can have not only rapid ad-
aptation but also superior performance over either the MLLR or MAP method across a wide
range of adaptation data points. There are a number of different ways to combine both
MLLR and MAP for improved performance [4, 98].
Figure 9.11 Comparison of Whisper with MLLR, MAP, and combined MLLR and MAP. The
error rate is shown for a different amount of adaptation data. The speaker-dependent and -
independent models are also included. The speaker-dependent model was trained with 1000
sentences.
4 In practice, if a large, well-trained, speaker-independent model is used, the baseline performance may be very
good and hence the relative error reduction from speaker adaptation may be smaller than for smaller and simpler
models.
10.0
10.5
11.0
11.5
12.0
12.5
13.0
0
200
400
600
800
1000
Num ber of A daptation U tterances
Error Rate
M LL R + M A P
M LL R O nly
M ap O nly
Speaker-
Independent
Speaker-D ependent

450
Acoustic Modeling
9.6.4.
Clustered Models
Both MAP and MLLR techniques are based on using an appropriate initial model for adap-
tive modeling. How accurate we make the initial model directly affects the overall perform-
ance. An effective way to minimize the mismatch is, thus, to cluster similar speakers and
environments in the training data, building a set of models for each cluster that has minimal
mismatch for different conditions. When we have enough training data, and enough coverage
for a wide range of conditions, this approach ensures a significantly improved robustness.
For example, we often need a set of clustered models for different telephone channels,
including different cellular phone standards. We also need to build gender-dependent models
or speaker-clustered models for improved performance. In fact, when we construct speaker-
clustered models, we can apply MMLR transformations or neural networks to minimize
speaker variations such that different speakers can be mapped to the same golden speaker
that is the representative of the cluster.
Speaker clusters can be created based on the information of each speaker-dependent
HMM. The clustering procedure is similar to the decision-tree procedure discussed in Sec-
tion 9.4.3. Using clustered models increases the amount of computational complexity. It also
fragments the training data. Clustering is often needed to combine other smoothing tech-
niques, such as deleted interpolation or MLLR transformation, in order to create clustered
models from the pooled model. We can also represent a speaker as a weighted sum of indi-
vidual speaker cluster models with the cluster adaptive training [33] or eigenvoice tech-
niques [64].
When we select an appropriate model, we can compute the likelihood of the test
speech against all the models and select the model that has the highest likelihood. Alterna-
tively, we can compute likelihoods as part of the decoding process and prune away less
promising models dynamically without significantly increasing the computational load.
When multiple models are plausible, we can compute the weighted sum of the clustered
models with pretrained mixing coefficients for different clusters, much as we train the de-
leted interpolation weights.
Traditionally speaker clustering is performed across different speakers without consid-
ering phonetic similarities across different speakers. In fact, clustered speaker groups may
have very different degrees of variations for different phonetic classes. You can further gen-
eralize speaker clustering to the subword or subphonetic level [62]. With multiple instances
derived from clustering for each subword or subphonetic unit, you can model speaker varia-
tion explicitly across different subword or subphonetic models.
In practice, gender-dependent models can reduce the word recognition error by 10%.
More refined speaker-clustered models can further reduce the error rate, but not as much as
the gain from gender-dependent models, unless we have a large number of clustered speak-
ers. If the new user happens to be similar to one of these speaker clusters, we can approach
speaker-dependent speech recognition without enrollment. For environment-dependent mod-
els, clustering is more critical. The challenge is to anticipate the kind of environment or
channel distortions the system will have to deal with. Since this is often unpredictable, we

Confidence Measures: Measuring the Reliability
451
need to use adaptive techniques such as MAP and MLLR to minimize the mismatch. We
discuss this in more detail in Chapter 10.
9.7.
CONFIDENCE MEASURES: MEASURING THE RELIABILITY
One of the most critical components in a practical speech recognition system is a reliable
confidence measure. With an accurate confidence measure for each recognized word, the
conversational back end can repair potential speech recognition errors, can reject out-of-
vocabulary words, and can identify key words (perform word spotting) that are relevant to
the back end. In a speaker-dependent or speaker-adaptive system, the confidence measure
can help user enrollment (to eliminate mispronounced words). It is also critical for unsuper-
vised speaker adaptation, allowing selective use of recognition results so that transcriptions
with lower confidence can be discarded for adaptation.
In theory, an accurate estimate of
(
|
)
P W X , the posterior probability, is itself a good
confidence measure for word W given the acoustic input X . Most practical speech recogni-
tion systems simply ignore
( )
P X , as it is a constant in evaluating
(
) (
|
)/
( )
P
P
P
W
X W
X
across different words.
(
|
)
P W X can be expressed:
(
) (
|
)
(
) (
|
)
(
|
)
( )
(
) (
|
)
P
P
P
P
P
P
P
P
=
= 
W
W
X W
W
X W
W X
X
W
X W
(9.34)
Equation (9.34) essentially provides a solid framework for measuring confidence lev-
els. It is the ratio between the score for the word hypothesis
(
) (
|
)
P
P
W
X W
and the acous-
tic probability
(
) (
|
)
P
P

W
W
X W . In the sections that follow we discuss a number of ways
to model and use such a ratio in practical systems.
9.7.1.
Filler Models
You can compute
( )
P X
in Eq. (9.34) with a general-purpose recognizer. It should be able to
recognize anything such that it can fill the holes of the grammar in the normal speech recog-
nizer. The filler model has various forms [7, 63]. One of the most widely used is the so-
called all-phone network, in which all the possible phonetic and nonspeech HMMs are con-
nected to each other, and with which any word sequence can be recognized.
In addition to evaluating
(
) (
|
)
P
P
W
X W
as needed in normal speech recognition, a
separate decoding process is used to evaluate
(
) (
|
)
P
P
 W
W
X W . Here W is either a pho-
netic or a word model. You can also apply phonetic n-gram probabilities that are derived
from a lexicon targeted for possible new words. The best path from the all-phone network is
compared with the best path from the normal decoder. The ratio between the two, as ex-

452
Acoustic Modeling
pressed in Eq. (9.34), is used to measure the confidence for either word or phone. In the de-
coding process (see Chapters 12 and 13), you can accumulate the phonetic ratio derived from
Eq. (9.34) on a specific word. If the accumulative
(
|
)
P W X
for the word is less than a pre-
determined threshold, the word is rejected as either a new word or a nonspeech event.
Both context-independent and context-dependent phonetic models can be used for the
fully connected network. When context-dependent phonetic models are used, you need to
make sure that only correct contextual phonetic connections are made. Although context-
dependent models offer significant improvement for speech recognition, the filler phonetic
network seems to be insensitive to context-dependency in empirical experiments.
There are word-spotting applications that need to spot just a small number of key
words. You can use the filler models described here for word spotting. You can also build
antiword models trained with all the data that are not associated with the key words of inter-
est. Empirical experiments indicate that large-vocabulary speech recognition is the most suit-
able choice for word spotting. You can use a general-purpose n-gram to generate recognition
results and identify needed key words from the word lattice. This is because a large-
vocabulary system provides a better estimate of
(
) (
|
)
P
P
 W
W
X W
with a more accurate
language model probability. In practice, we don’t need to use all the hypotheses to compute
(
) (
|
)
P
P
 W
W
X W . Instead, n-best lists and scores [40] can be used to provide an effective
estimate of
(
) (
|
)
P
P
 W
W
X W .
9.7.2.
Transformation Models
To determine the confidence level for each word, subword confidence information is often
helpful. Different phones have different impact on our perception of words. The weight for
each subword confidence score can be optimized from the real training data. If a word w has
N phones, we can compute the confidence score of the word as follows:
1
( )
(
) /
N
i
i
i
CS w
x
N
=
=
℘

(9.35)
where CS(w) is the confidence score for word w, xi is the confidence score for subword unit
i in word w, and
i
℘is the mapping function that may be tied across a number of subword
units. The transformation function can be defined as:
( )
i x
ax
b
℘
=
+
(9.36)
We can use discriminative training to optimize the parameters a and b, respectively. A
cost function can be defined as a sigmoid function of CS(w). As shown in Figure 9.12, the
optimal transformation parameters vary substantially across different phones. The weight for
consonants is also typically larger than that of vowels.

Confidence Measures: Measuring the Reliability
453
Figure 9.12 Transformation parameter a for each context-independent phone class. The
weight of consonants is typically larger than that of vowels [63].
Figure 9.13 The ROC curve of phonetic filler models with and without optimal feature
transformation [63].
0
0.5
1
1.5
2
AA
AW
B
EH
G
IY
M
OY
SH
UW
Z
0
0.1
0.2
0.3
0.4
0.1
0.12
0.16
0.2
0.24
0.28
0.32
0.36
0.4
0.44
0.48
0.52
0.56
0.6
0.64
0.68
0.72
0.76
False Rejection
False Acceptance
T ransform ation
W ithout T ransform ation

454
Acoustic Modeling
The transformation function can be context dependent. Figure 9.13 illustrates the ROC
curve of the context-dependent transformation model in comparison with the corresponding
phonetic filler model. The filler model essentially has a uniform weight across all the phones
in a given word. The estimated transformation model has 15–40% false-acceptance error
reduction at various fixed false-rejection rates. The false-acceptance rate of the transforma-
tion model is consistently lower than that of the filler model [63].
9.7.3.
Combination Models
In practical systems, there are a number of features you can use to improve the performance
of confidence measures. For example, the following features are helpful:
 Word stability ratio from different language model weights (WrdStabRatio).
This is obtained by applying different language weights to see how stably each
word shows up in the recognition n-best list.
 Logarithm of the average number of active words around the ending frame of the
word (WrdCntEnd).
 Acoustic score per frame within the word normalized by the corresponding ac-
tive senone scores (AscoreSen).
 Logarithm of the average number of active words within the word (WrdCntW).
 Acoustic score per frame within the word normalized by the phonetic filler
model (AscoreFiller).
 Language model score (LMScore).
 Language model back-off (trigram, bigram, or unigram hit) for the word
(LMBackOff).
 Logarithm of the average number of active states within the word (StateCnt).
 Number of phones in the word (Nphones).
 Logarithm of the average number of active words around the beginning frame of
the word (WrdCntBeg).
 Whether the word has multiple pronunciations (Mpron).
 Word duration (WordDur).
To clarify each feature’s relative importance, Table 9.6 shows its linear correlation co-
efficient against the correct/incorrect tag for each word in the training set. Word stability
ratio (WrdStabRatio) has the largest correlation value.
Several kinds of classifiers can be used to compute the confidence scores. Previous re-
search has shown that the difference between classifiers, such as linear classifiers, general-
ized linear models, decision trees, and neural networks, is insignificant. The simplest linear
classifier based on the discriminative training performs well in practice. As some features are
highly correlated, you can iteratively remove features to combat the curse of dimensionality.

Other Techniques
455
The combination model can have up to 40–80% false-acceptance error reduction at fixed
false-rejection rate in comparison to the single-feature approach.
Table 9.6 Correlation coefficients of several features against correct/incorrect tag.
Feature
Correlation
WrdStabRatio
0.590
WrdCntW
–0.223
LMBackOff
0.171
AscoreSen
0.250
LMScore
0.175
Nphones
0.091
WordDur
0.012
WrdCntEnd
–0.321
AscoreFiller
0.219
StateCnt
–0.155
Mpron
0.057
WrdCntBeg
–0.067
9.8.
OTHER TECHNIQUES
In addition to HMMs, a number of interesting alternative techniques are being actively inves-
tigated by researchers. We briefly review two promising methods here.
9.8.1.
Neural Networks
You have seen both single-layer and multilayer neural nets in Chapter 4 for dealing with
static patterns. In dealing with nonstationary signals, you need to address how to map an
input sequence properly to an output sequence when two sequences are not synchronous,
which should include proper alignment, segmentation, and classification. The basic neural
networks are not well equipped to address these problems in a unified way.
Recurrent neural networks have an internal state that is a function of the current input
and the previous internal state. A number of them use time-step delayed recurrent loops on
the hidden or output units of a feedforward network, as discussed in earlier chapters. For
sequences of finite numbers of delays, we can transform these networks into equivalent feed-
forward networks by unfolding them over the time period. They can be trained with the stan-
dard back propagation procedure, with the following modifications:
 The desired outputs are functions of time, and error functions have to be com-
puted for every copy of the output layer. This requires the selection of an appro-
priate time-dependent target function, which is often difficult to define.

456
Acoustic Modeling
 All copies of the unfolded weights are constrained to be identical during the
training. We can compute the correction terms separately for each weight and
use the average to update the final estimate.
In most of these networks, you can have a partially recurrent network that has feedback
of the hidden and output units to the input layer. For example, the feedforward network can
be used in a set of local feedforward connections with one time-step delay. These networks
are usually implemented by extending the input field with additional feedback units contain-
ing both the hidden and output values generated by the preceding input. You can encode the
past nonstationary information that is often required to generate the correct output, given the
current input, as illustrated in Figure 9.14.
xn - 3
z-1
xn - 2
z-1
xn - 1
z-1
xn
z-1
z-1
Hidden Layer
Output Layer
Input Layer
Figure 9.14 A recurrent network with contextual inputs, hidden vector feedback, and output
vector feedback.
One of the popular neural networks is the Time Delay Neural Network (TDNN) [105].
Like static networks, the TDNN can be trained to recognize a sequence of predefined length
(defined by the width of the input window). The activation in the hidden layer is computed
from the current and multiple time-delayed values of the preceding layer, and the output
units are activated only when a complete speech segment has been processed. A typical
TDNN is illustrated in Figure 9.15. The TDNN has been successfully used to classify pre-
segmented phonemes.
All neural networks have been shown to yield good performance for small-vocabulary
speech recognition. Sometimes they are better than HMMs for short, isolated speech units.
By recurrence and the use of temporal memory, they can perform some kind of integration
over time. It remains a challenge for neural networks to demonstrate that they can be as ef-
fective as HMMs for dealing with nonstationary signals, as is often required for large-
vocabulary speech recognition.
To deal with the continuous speech, the most effective solution is to integrate neural
nets with HMMs [91, 113]. The neural network can be used as the output probabilities to
replace the Gaussian mixture densities. Comparable results can be obtained with the inte-

Other Techniques
457
grated approach. These HMM output probabilities could be estimated by applying the Bayes
rule to the output of neural networks that have been trained to classify HMM state catego-
ries. The neural networks can consist either of a single large trained network or of a group of
separately trained small networks [21, 31, 75, 90].
A number of techniques have been developed to improve the performance of training
these networks. Training can be embedded in an EM-style process. For example, dynamic
programming can be used to segment the training data. The segmented data are then used to
retrain the network. It is also possible to have Baum-Welch style training [14, 42].
xn - 2
z-1
xn - 1
z-1
xn
Output
Input Layer
hn - 2
z-1
hn - 1
z-1
hn
Figure 9.15 A time-delay neural network (TDNN), where the box ht denotes the hidden vec-
tor at time t, the box xt denotes the input vector at time t, and the box
1
z−
denotes a delay of
one sample.
9.8.2.
Segment Models
As discussed in Chapter 8, the HMM output-independence assumption results in a piecewise
stationary process within an HMM state. Although the nonstationary speech may be modeled
sufficiently with a large number of states, the states in which salient speech features are pre-
sent are far from stationary [25, 99]. While the use of time-derivative features (e.g., delta
and/or delta-delta features) alleviates these limitations, the use of such longer-time-span fea-
tures may invalidate the conditional independence assumption.
Figure 9.16 Diagram illustrating that HMM's output observation can hop between two unex-
pected quasi-stationary states [46].
state 1
state 2

458
Acoustic Modeling
The use of Gaussian mixtures for continous or semicontinuous HMMs, as described in
Chapter 8, could introduce another potential problem, where arbitrary transitions among the
Gaussian mixture components between adjacent HMM states are allowed [59]. Figure 9.16
illustrates two HMM states with two mixture components. The solid lines denote the valid
trajectories actually observed in the training data. However, in modeling these two
trajectories, the Gaussian mixtures inadvertently allow two phantom trajectories, shown in
dashed lines in Figure 9.16, because no constraint is imposed on the mixture transitions
across the state. It is possible that such phantom trajectories degrade recognition
performance, because the models can overrepresent speech signals that should be modeled
by other acoustic units. Segment models can alleviate such HMM modeling deficiencies [77,
79].
In the standard HMM, the output probability distribution is modeled by a quasi-
stationary process, i.e.,
1
1
(
| )
(
)
L
L
s
i
i
P
s
b
=
=∏
x
x
(9.37)
For the segment model (SM), the output observation distribution is modeled by two stochas-
tic processes:
1
1
(
| )
(
| , ) (
| )
L
L
P
s
P
s L P L s
=
x
x
(9.38)
The first term of Eq. (9.38) is no longer decomposable in the absence of the output-
independence assumption. The second term is similar to the duration model described in
Chapter 8. In contrast to the HMM whose quasi-stationary process for each state s generates
one frame
ix , a state in a segment model can generate a variable-length observation se-
quence
1
2
{
,
,
}
L
x x
x

with random length L.
Since the likelihood evaluation of segment models cannot be decomposed, the compu-
tation of the evaluation is not shareable between different segments (even for the case where
two segments differ only by one frame). This results in a significant increase in computation
for both training and decoding [77]. In general, the search state space is increased by a factor
of
max
L
, the maximum segment duration. If segment models are used for phone segments,
max
L
could be as large as 60. On top of this large increase in search state space, the evalua-
tion of segment models is usually an order of magnitude more expensive than for HMM,
since the evaluation involves several frames. Thus, the segment model is often implemented
in a multipass search framework, as described in Chapter 13.
Segment models have produced encouraging performance for small-vocabulary or iso-
lated recognition tasks [25, 44, 79]. However, their effectiveness on large-vocabulary con-
tinuous speech recognition remains an open issue because of necessary compromises to re-
duce the complexity of implementation.

Other Techniques
459
9.8.2.1.
Parametric Trajectory Models
Parametric trajectory models [25, 37] were first proposed to model a speech segment with
curve-fitting parameters. They approximate the D-dimensional acoustic observation vector
1
2
(
,
,
,
)
T
=
X
x x
x

by a polynomial function. Specifically, the observation vector
tx can be
represented as
0
1
0
1
1
1
0
1
1
2
2
2
0
1
2
( )
( )
( )
( )
( )
N
N
t
t
t
t
N
N
D
D
D
D
f t
c
c
c
f t
c
c
c
F
f
t
c
c
c
c









=
×
+
Σ =
+
Σ













x
C
e
e







(9.39)
where the matrix C is the trajectory parameter matrix,
tF
is the family of Nth-order
polynomial functions, and
( )
t Σ
e
is the residual fitting error. Equation (9.39) can be regarded
as modeling the time-varing mean in the output distribution for an HMM state. To simplify
computation, the distribution of the residual error is often assumed to be an independent and
identically distributed random process with a normal distribution
(0, )
N
Σ . To accommodate
diverse durations for the same segment, the relative linear time sampling of the fixed
trajectory is assumed [37].
Each segment M is characterized by a trajectory parameter matrix
m
C
and covariance
matrix
m
Σ . The likelihood for each frame can be specified [46] as
{
}
1
1
2
2
exp
(
)
(
)
2
(
|
,
)
(2 )
|
|
t
t
m
t
m
t
m
t
t
m
m
D
m
tr
F
F
P
π
−


−
−
Σ
−


Σ
=
Σ
x
C
x
C
x
C
(9.40)
If we let
0
1
1
(
,
,
,
)t
T
T
F F
F −
=
F

, then the likelihood for the whole acoustic observation vector
can be expressed as
{
}
1
2
2
exp
(
)
(
)
2
(
|
,
)
(2 )
|
|
t
m
T
m
m
T
m
m
DT
T
m
tr
P
π
−


−
−
Σ
−


Σ
=
Σ
X
C F
X
C F
X C
(9.41)
Multiple mixtures can also be applied to SM. Suppose segment M is modeled by K tra-
jectory mixtures. The likelihood for the acoustic observation vector X becomes
1
(
|
,
)
K
k
k
k
k
k
w p
=
Σ

X C
(9.42)
Hon et al. [47] showed that only a handful of target trajectories are needed for speaker-
independent recognition, in contrast to the many mixtures required for continuous Gaussian
HMMs. This should support the phantom-trajectory argument involved in Figure 9.16.

460
Acoustic Modeling
The estimation of segment parameters can be accomplished by the EM algorithm
described in Chapter 4. Assume a sample of L observation segments
1
2
,
,
,
L
X X
X

, with
corresponding duration
1
2
,
,
,
L
T T
T

, are generated by the segment model M. The MLE for-
mulae using the EM algorithm are:
(
|
,
)
(
|
)
i
k
k
i
k
k
k
i
m
w p
P
γ
Σ
=
Φ
X
C
X
(9.43)
1
m
1
(
|
,
)
ˆ
(
|
)
L
k
k
i
k
k
k
i
i
w p
w
L
P
=
Σ
=
Φ

X
C
X
(9.44)
1
1
ˆ
/
i
i
i
L
L
i
t
i
t
k
k
i
T
k
T
T
i
i
γ
γ
=
=

 

= 
 


 



C
X F
F F
(9.45)
1
1
ˆ
(
)(
)
i
i
L
L
i
t
i
k
k
i
k
T
i
k
T
k
i
i
i
T
γ
γ
=
=
Σ =
−
−


X
C F
X
C F
(9.46)
Parametric trajectory models have been successfully applied to phone classification
[25, 46] and word spotting [37], which offers a modestly improved performance over
HMMs.
9.8.2.2.
Unified Frame- and Segment-Based Models
The strengths of the HMM and the segment-model approaches are complementary. HMMs
are very effective in modeling the subtle details of speech signals by using one state for each
quasi-stationary region. However, the transitions between quasi-stationary regions are largely
neglected by HMMs because of their short durations. In contrast, segment models are power-
ful in modeling the transitions and longer-range speech dynamics, but might need to give up
the detailed modeling to assure trainability and tractability. It is possible to have a unified
framework to combine both methods [47].
In the unified complementary framework, the acoustic model
1
(
|
)
T
p X
W , can be con-
sidered as two joint hidden processes, as in the following equation:
(
|
)
,
)
|
) (
|
) (
|
)
(
,
|
(
,
h
s
h
s
h
s
h
s
s
h
h
p
p
p
p
p
=
=


q
q
q
q
X W
X
W
X
W
q
q
q
q
q
q
q
(9.47)
where
h
q
represents the hidden process of the HMM and
s
q , the segment model. The condi-
tional probability of the acoustic signal
(
|
,
)
s
h
p X q q
can be further decomposed into two
separate terms:

Other Techniques
461
(
|
,
)
(
|
) (
|
)
s
h
h
s
a
p
p
p
=
X
X
X
q q
q
q
(9.48)
where a is a constant that is called segment-model weight. The first term is the contribution
from normal frame-based HMM evaluation. We further assume for the second term that rec-
ognition of segment units can be performed by detecting and decoding a sequence of salient
events in the acoustic stream that are statistically independent. In other words,
(
|
)
(
|
)
s
s
i
i
i
p
p
q
=∏
X q
X
(9.49)
where
i
X denotes the ith segment.
We assume that the phone sequence and the phone boundaries hypothesized by HMMs
and segment models agree with each other. Based on the independent-segment assumption,
this leads to a segment duration model as
1
(
|
)
( ,
1|
)
s
h
i
i
i
i
p
p t t +
=
−
∏
q
q
X
(9.50)
By treating the combination as a hidden-data problem, we can apply the decoding and
iterative EM reestimation techniques here. This unified framework enables both frame- and
segment-based models to jointly contribute to optimal segmentations, which leads to more
efficient pruning during the search. The inclusion of the segment models does not require
massive revisions in the decoder, because the segment model scores can be handled in the
same manner as the language model scores; whereas the segment evaluation is performed at
each segment boundary.
Figure 9.17 Overlapped evaluation using (a) a phone-pair segment model, or (b) back-off to
two phone units when the phone-pair (ei-1, ei) segment model does not exist [47].
Since subphonetic units are often used in HMMs to model the detailed quasi-stationary
speech region, the segment units should be used to model long-range transition. As studies
have shown that phone transitions play an essential role in humans’ perception, the phone-
pair segment unit that spans over two adjacent phones can be used [47]. Let
ie and ti denote
the phone and the starting time of the ith segment, respectively. For a phone-pair (ei-1, ei)
segment between ti and ti+1, the segment likelihood can be computed as follows:
1
1
1
(
|
)
(
|
, )
i
i
t
s
i
i
t
i
i
p
q
p
e
e
+
−
−
=
X
x
(9.51)
Rather than applying segment evaluation for every two phones, an overlapped evalua-
tion scheme can be used, as shown in Figure 9.17 (a), where a phone-pair segment model
(a) phone-pair segment models
(b) two phone (monophone or gen.
triphone) segment models

462
Acoustic Modeling
evaluation is applied at each phone boundary. The overlapped evaluation implies that each
phone is evaluated twice in the overall score. Most importantly, the overlapped evaluation
places constraints on overlapped regions to assure consistent trajactory transitions. This is an
important feature, as trajectory mixtures prohibit phantom trajectories within a segment unit,
but there is still no mechanism to prevent arbitrary trajactory transitions between adjacent
segment units.
Some phone-pairs might not have sufficient training data. Units containing silence
might also have obscure trajectories due to the arbitrary duration of silence. As a result, a
phone-pair unit (ei-1, ei) can be backed off with two phone units as shown in Figure 9.17 (b).
The phone units can be context independent or context dependent [46]. Thus, the back-off
segment-model evaluation becomes:
1
1
1
(
|
)
(
|
) (
|
)
i
i
i
i
t
t
s
i
i
t
i
t
i
p
q
p
e
p
e
β
+
−
−
=
∗
X
x
x
(9.52)
where β is the back-off weight, generally smaller than 1.0. The use of back-off weight has
the effect of giving more preference to phone-pair segment models than to two-phone-based
back-off segment models.
The phone-pair segment model outperformed the phone-pair HMM by more than 20%
in a phone-pair classification experiment [46]. The unified framework achieved about 8%
word-error-rate reduction on the WSJ dictation task in comparison to the HMM-based Whis-
per [47].
9.9.
CASE STUDY: WHISPER
Microsoft’s Whisper engine offers general-purpose speaker-independent continuous speech
recognition [49]. Whisper can be used for command and control, dictation, and conversa-
tional applications. Whisper offers many features such as continuous speech recognition,
speaker-independence with adaptation, and dynamic vocabulary. Whisper has a unified ar-
chitecture that can be scaled to meet different application and platform requirements.
The Whisper system uses MFCC representations (see Chapter 6) and both first- and
second-order delta MFCC coefficients. Two-mean cepstral normalization discussed in Chap-
ter 10 is used to eliminate channel distortion for improved robustness.
The HMM topology is a three-state left-to-right model for each phone. Senone models
discussed in Section 9.4.3 are derived from both inter- and intraword context-dependent
phones. The generic shared density function architecture can support either semicontinuous
or continuous density hidden Markov models.
The SCHMM has a multiple-feature front end. Independent codebooks are built for the
MFCC, first-order delta MFCC, second-order delta MFCC, and power and first and second
power, respectively. Deleted interpolation is used to interpolate output distributions of con-
text-dependent and context-independent senones. All codebook means and covariance matri-
ces are reestimated together with the output distributions except the power covariance matri-
ces, which are fixed.

Historical Perspective and Further Reading
463
The overall senone models can reduce the error rate significantly in comparison to the
triphone or clustered triphone model. The shared Markov state also makes it possible to use
continuous-density HMMs efficiently for large-vocabulary speech recognition. When a suffi-
cient amount of training data becomes available, the best performance is obtained with the
continuous-density mixture HMM. Each senone has 20 mixtures, albeit such an error reduc-
tion came at the cost of significantly increased computational complexity.
We can further generalize sharing to the level of each individual Gaussian probability
density function. Each Gaussian function is treated as the basic unit to be shared across any
Markov state. At this extreme, there is no need to use senones or shared states any more, and
the shared probability density functions become the acoustic kernels that can be used to form
any mixture function for any Markov state with appropriate mixture weights. Parameter shar-
ing is, thus, advanced from a phone unit to a Markov state unit (senones) to a density com-
ponent unit.
Regarding lexicon modeling, most words have one pronunciation in the lexicon. For
words that are not in the dictionary, the LTS conversion is based on the decision tree that is
trained form the existing lexicon. For the purpose of efficiency, the dictionary is used to
store the most frequently used words. The LTS is only used for new words that need to be
added on the fly.
For speaker adaptation, the diagonal variances and means are adapted using the MAP
method. Whisper also uses MLLR to modify the mean vectors only. The MLLR classes are
phone dependent. The transition probabilities are context independent and they are not modi-
fied during the adaptation stage.
The language model used in Whisper can be either the trigram or the context-free
grammar. The difference is largely related to the decoder algorithm, as discussed in Chapter
13. The trigram lexicon has 60,000 most-frequent words extracted from a large text corpus.
Word selection is based on both the frequency and the word’s part-of-speech information.
For example, verbs and the inflected forms have a higher weight than proper nouns in the
selection process.
Whisper’s overall word recognition error rate for speaker-independent continuous
speech recognition is about 7% for the standard DARPA business-news dictation test set. For
isolated dictation with similar materials, the error rate is less than 3%. If speaker-dependent
data are available, we can further reduce the error rate by 15–30%, with less than 30 minutes
speech from each person. The performance can be obtained real-time on today’s PC systems.
9.10.
HISTORICAL PERSPECTIVE AND FURTHER READING
The first machine to recognize speech was a commercial toy named Radio Rex manufactured
in the 1920s. Fueled by increased computing resources, acoustic-phonetic modeling has pro-
gressed significantly since then. Relative word error rates have been reduced by 10% every
year, as illustrated in Figure 9.18, thanks to the use of HMMs, the availability of large
speech and text corpora, the establishment of standards for performance evaluation, and ad-
vances in computer technology. Before the HMM was established as the standard, there were

464
Acoustic Modeling
many competing techniques, which can be traced back to the 1950s. Gold and Morgan’s
book provides an excellent historical perspective [38].
The HMM is powerful in that, with the availability of training data, the parameters of
the model can be estimated and adapted automatically to give optimal performance. There
are many HMM-based state-of-the-art speech recognition systems [1, 12, 27, 34, 49, 55, 72,
73, 93, 108, 109, 112]. Alternatively, we can first identify speech segments, then classify the
segments and use the segment scores to recognize words. This approach has produced com-
petitive recognition performance that is similar to HMM-based systems in several small- to
medium-vocabulary tasks [115].
Speech recognition systems attempt to model the sources of variability in several ways.
At the level of signal representation, in addition to MFCC, researchers have developed rep-
resentations that emphasize perceptually important speaker-independent features of the sig-
nal, and deemphasize speaker-dependent characteristics [43]. Other methods based on linear
discriminant analysis to improve class separability [28, 54] and speaker normalization trans-
formation to minimize speaker variations [51, 67, 86, 106, 107, 114] have achieved limited
success. Linear discriminant analysis can be traced back to Fisher’s linear discriminant [30],
which projects a dimensional vector onto a single line that is oriented to maximize the class
separability. Its extension to speech recognition can be found in [65].
0%
5%
10%
15%
20%
25%
1988
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
RM C&C
ATIS Spontaneous Speech
WSJ Read Speech
NAB Broadcast Speech
Figure 9.18 History of DARPA speech recognition word-error-rate benchmark evaluation re-
sults from 1988 to 1999. There are four major tasks: the Resource Management command and
control task (RM C&C, 1000 words), the Air Travel Information System spontaneous speech
understanding task (ATIS, 2000 words), the Wall Street Journal dictation task (WSJ, 20,000
words), and the Broadcast News Transcription Task (NAB, 60,000 words) [80-84].
At the level of acoustic-phonetic modeling, we need to provide an accurate distance
measure of the input feature vectors against the phonetic or word models from the signal-

Historical Perspective and Further Reading
465
processing front end. Before the HMM was used, the most successful acoustic-phonetic
model was based on the speech template where the feature vectors are stored as the model
and dynamic-programming-based time warping was used to measure the distance between
the input feature vectors and the word or phonetic templates [88, 94]. The biggest problem
for template-based systems is that they are not as trainable as HMMs, since it is difficult to
generate a template that is as representative as all the speech samples we have for the par-
ticular units of interest.
Another approach that attracted many researchers is the knowledge-based one that
originated from the Artificial Intelligence research community. This approach requires ex-
tensive knowledge engineering, which often led to many inconsistent rules. Due to the com-
plexity of speech recognition, rule-based approaches generally cannot match the perform-
ance of data-driven approaches such as HMMs, which can automatically extract salient rules
from a large amount of training data [105].
Senones are now widely used in many state-of-the-art systems. Word models or allo-
phone models can also be built by concatenation of basic structures made by states, transi-
tions, and distributions such as fenones [8, 9] or senones [58].
Segment models, as proposed by Roucos and Ostendorf [79, 92], assume that each
variable-length segment is mapped to a fixed number of representative frames. The resulting
model is very similar to the HMM with a large number of states. Ostendorf published a com-
prehensive survey paper [77] on segment models. The parametric trajectory segment model
was introduced by Deng et al. [25] and Gish et al. [37] independently. Gish’s work is very
similar to our description in Section 9.8.2.1, which is based on Hon et al. [46, 47], where the
evaluation and estimation are more efficient because no individual polynormial fitting is
required for likelihood computation. In addition to the phone-pair units described in this
chapter, segment models have also been applied to phonetic units [25], subphonetic units
[25], diphones [36], and syllables [78]. The dynamic model [24, 26] is probably the most
aggressive attempt to impose a global transition constrain on the speech model. It uses the
phonetic target theories on unobserved vocal-tract parameters, which are fed to an MLP to
produce the observed acoustic data.
Today, it is not uncommon to have tens of thousands of sentences available for system
training and testing. These corpora permit researchers to quantify the acoustic cues important
for phonetic contrasts and to determine parameters of the recognizers in a statistically mean-
ingful way. While many of these corpora were originally collected under the sponsorship of
the U.S. Defense Advanced Research Projects Agency (ARPA) to spur human language
technology development among its contractors [82], they have nevertheless gained interna-
tional acceptance. Recognition of the need for shared resources led to the creation of the
Linguistic Data Consortium (LDC)5 in the United States in 1992 to promote and support the
widespread development and sharing of resources for human language technology. The LDC
supports various corpus development activities and distributes corpora obtained from a vari-
ety of sources. Currently, LDC distributes about twenty different speech corpora including
those cited above, comprising many hundreds of hours of speech. The availability of a large
body of data in the public domain, coupled with the specification of evaluation standards,
5 http://www.cis.upenn.edu/ldc

466
Acoustic Modeling
has resulted in uniform documentation of test results, thus contributing to greater reliability
in monitoring progress.
To further improve the performance of acoustic-phonetic models, we need a robust
system so that performance degrades gracefully (rather than catastrophically) as conditions
diverge from those under which it was trained. The best approach is likely to have systems
continuously adapted to changing conditions (new speakers, microphone, task, etc). Such
adaptation can occur at many levels in systems, subword models, word pronunciations, lan-
guage models, and so on. We also need to make the system portable, so that we can rapidly
design, develop, and deploy systems for new applications. At present, systems tend to suffer
significant degradation when moved to a new task. In order to retain peak performance, they
must be trained on examples specific to the new task, which is time consuming and expen-
sive. In the new task, system users may not know exactly which words are in the system vo-
cabulary. This leads to a certain percentage of out-of-vocabulary words in natural conditions.
Currently, systems lack a very robust method of detecting such out-of-vocabulary words.
These words often are inaccurately mapped into the words in the system, causing unaccept-
able errors.
An introduction to all aspects of acoustic modeling can be found in Spoken Dialogues
with Computers [76] and Fundamentals of Speech Recognition [87]. A good treatment of
HMM-based speech recognition is given in [52, 60, 105]. Bourlard and Morgan’s book [15]
is a good introduction to speech recognition based on neural networks. There are a range of
applications such as predictive networks that estimate each frame’s acoustic vector, given the
history [69, 104] and nonlinear transformation of observation vectors [13, 53, 101].
You can find tools to build acoustic models from Carnegie Mellon University’s speech
open source Web site.6 This site contains the release of CMU’s Sphinx acoustic modeling
toolkit and documentation. A version of Microsoft’s Whisper system can be found in the
Microsoft Speech SDK.7
REFERENCES
[1]
Abrash, V., et al., "Acoustic Adaptation Using Nonlinear Transformations of HMM Parame-
ters" in Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing 1996, At-
lanta, pp. 729-732.
[2]
Acero, A., Acoustical and Environmental Robustness in Automatic Speech Recognition,
1993, Boston, Kluwer Academic Publishers.
[3]
Ahadi-Sarkani, S.M., Bayesian and Predictive Techniques for Speaker Adaptation, Ph. D.
Thesis 1996, Cambridge University, .
[4]
Ahn, S., S. Kang, and H. Ko, "Effective Speaker Adaptations For Speaker Verification,"
IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 2000, Istanbul, Turkey pp.
1081-1084.
[5]
Alleva, F., et al., "Can Continuous Speech Recognizers Handle Isolated Speech?," Speech
Communication, 1998, 26, pp. 183-189.
6 http://www.speech.cs.cmu.edu/sphinx/
7 http://www.microsoft.com/speech

Historical Perspective and Further Reading
467
[6]
Anastasakos, T., et al., "A Compact Model for Speaker Adaptive Training," Int. Conf. on
Spoken Language Processing, 1996, Philadelphia pp. 1137-1140.
[7]
Asadi, A., R. Schwartz, and J. Makhoul, "Automatic Modeling for Adding New Words to a
Large-Vocabulary Continuous Speech Recognition System" in Proc. of the IEEE Int. Conf.
on Acoustics, Speech and Signal Processing 1991, Toronto, pp. 305-308.
[8]
Bahl, L.R., et al., "Multonic Markov Word Models for Large Vocabulary Continuous
Speech Recognition," IEEE Trans. on Speech and Audio Processing, 1993, 1(3), pp. 334-
344.
[9]
Bahl, L.R., et al., "A Method for the Construction of Acoustic Markov Models for Words,"
IEEE Trans. on Speech and Audio Processing, 1993, 1(4), pp. 443-452.
[10]
Bahl, L.R., et al., "Automatic Phonetic Baseform Determination," Proc. IEEE Int. Conf. on
Acoustics, Speech and Signal Processing, 1991, Toronto pp. 173-176.
[11]
Bahl, L.R., et al., "Decision Trees for Phonological Rules in Continuous Speech," Proc.
IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1991, Toronto, Canada pp.
185-188.
[12]
Bellegarda, J.R., et al., "Experiments Using Data Augmentation for Speaker Adaptation,"
Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1995, Detroit pp.
692-695.
[13]
Bengio, Y., et al., "Global Optimization of a Neural Network-Hidden Markov Model Hy-
brid," IEEE Trans. on Neural Networks, 1992, 3(2), pp. 252--259.
[14]
Bourlard, H., "A New Training Algorithm for Statistical Sequence Recognition with Appli-
cations to Transition-Based Speech Recognition," IEEE Signal Processing Letters, 1996, 3,
pp. 203-205.
[15]
Bourlard, H. and N. Morgan, Connectionist Speech Recognition - A Hybrid Approach, 1994,
Boston, MA, Kluwer Academic Publishers.
[16]
Brown, P.F., The Acoustic-Modeling Problem in Automatic Speech Recognition, PhD Thesis
in Computer Science Department 1987, Carnegie Mellon University, Pittsburgh, PA.
[17]
Campbell, J., "Speaker Recognition: A Tutorial," Proc. of the IEEE, 1997, 85(9), pp. 1437-
1462.
[18]
Chesta, C., O. Siohan, and C.H. Lee, "Maximum A Posteriori Linear Regression for Hidden
Markov Model Adaptation," Eurospeech, 1999, Budapest pp. 211-214.
[19]
Chou, W., "Maximum A Posteriori Linear Regression with Elliptically Symmetric Matrix
Priors," Eurospeech, 1999, Budapest pp. 1-4.
[20]
Cohen, M., Phonological Structures for Speech Recognition, Ph.D. Thesis
1989, Univer-
sity of California, Berkeley, .
[21]
Cook, G. and A. Robinson, "Transcribing Broadcast News with the 1997 Abbot System,"
Int. Conf. on Acoustics, Speech and Signal Processing, 1998, Seattle, WA pp. 917-920.
[22]
Cox, S., "Predictive Speaker Adaptation in Speech Recognition," Computer Speech and
Language, 1995, 9, pp. 1-17.
[23]
Davis, S. and P. Mermelstein, "Comparison of Parametric Representations for Monosyllable
Word Recognition in Continuously Spoken Sentences," IEEE Trans. on Acoustics, Speech
and Signal Processing, 1980, 28(4), pp. 357-366.
[24]
Deng, L., "A Dynamic, Feature-based Approach to the Interface Between Phonology and
Phonetics for Speech Modeling and Recognition," Speech Communication, 1998, 24(4), pp.
299-323.
[25]
Deng, L., et al., "Speech Recognition Using Hidden Markov Models with Polynomial Re-
gression Functions as Nonstationary States," IEEE Trans. on Speech and Audio Processing,
1994, 2(4), pp. 507-520.

468
Acoustic Modeling
[26]
Digalakis, V., Segment-based Stochastic Models of Spectral Dynamics for Continuous
Speech Recognition, Ph.D. Thesis in Electrical Computer System Engineering 1992, Boston
University, .
[27]
Digalakis, V. and H. Murveit, "Genones: Optimizing the Degree of Mixture Tying in a Large
Vocabulary Hidden Markov Model Based Speech Recognizer" in Proc. of the IEEE Int.
Conf. on Acoustics, Speech and Signal Processing 1994, Adelaide, Australia, pp. 537-540.
[28]
Doddington, G.R., "Phonetically Sensitive Discriminants for Improved Speech Recogni-
tion," Int. Conf. on Acoustics, Speech and Signal Processing, 1989, Glasgow, Scotland pp.
556-559.
[29]
Eichner, M. and M. Wolff, "Data-Driven Generation of Pronunciation Dictionaries In The
German Verbmobil Project - Discussion of Experimental Results," IEEE Int. Conf. on
Acoustics, Speech and Signal Processing, 2000, Istanbul, Turkey pp. 1687-1690.
[30]
Fisher, R., "The Use of Multiple Measurements in Taxonomic Problems," Annals of Eugen-
ics, 1936, 7(1), pp. 179-188.
[31]
Fritsch, J. and M. Finke, "ACID/HNN: Clustering Hierarchies of Neural Networks for Con-
text-Dependent Connectionist Acoustic Modeling," Int. Conf. on Acoustics, Speech and Sig-
nal Processing, 1998, Seattle, WA pp. 505-508.
[32]
Fukunaga, K., Introduction to Statistical Pattern Recognition, 2nd ed, 1990, Orlando, FL,
Academic Press.
[33]
Gales, M., "Cluster Adaptive Training for Speech Recognition," Int. Conf. on Spoken Lan-
guage Processing, 1998, Sydney, Australia pp. 1783-1786.
[34]
Gauvain, J.L., L. Lamel, and M. Adda-Decker, "Developments in Continuous Speech Dicta-
tion using the ARPA WSJ Task," Proc. of the IEEE Int. Conf. on Acoustics, Speech and
Signal Processing, 1995, Detroit, MI pp. 65-68.
[35]
Gauvain, J.L. and C.H. Lee, "Bayesian Learning of Gaussian Mixture Densities for Hidden
Markov Models," Proc. of the DARPA Speech and Natural Language Workshop, 1991, Palo
Alto, CA pp. 272-277.
[36]
Ghitza, O. and M.M. Sondhi, "Hidden Markov Models with Templates as Non-Stationary
States: An Application To Speech Recognition," Computer Speech and Language, 1993,
7(2), pp. 101-120.
[37]
Gish, H. and K. Ng, "A Segmental Speech Model With Applications to Word Spotting,"
Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1993, Minneapo-
lis, MN pp. 447-450.
[38]
Gold, B. and N. Morgan, Speech and Audio Signal Processing: Processing and Perception
of Speech and Music, 2000, New York, John Wiley.
[39]
Greenberg, S., D. Ellis, and J. Hollenback, "Insights into Spoken Language Gleaned from
Phonetic Transcription of the Switchboard Corpus," Int. Conf. on Spoken Language Proc-
essing, 1996, Philadelphia, PA pp. addendum 24-27.
[40]
Gunawardana, A., H.W. Hon, and L. Jiang, "Word-Based Acoustic Confidence Measures for
Large-Vocabulary Speech Recognition," Int. Conf. on Spoken Language Processing, 1998,
Sydney, Australia pp. 791-794.
[41]
Haeb-Umbach, R., "Investigations on Inter-Speaker Variability in the Feature Space," IEEE
Int. Conf. on Acoustics, Speech and Signal Processing, 1999, Phoenix, AZ.
[42]
Hennebert, J., et al., "Estimation of Global Posteriors and Forward-Backward Training of
Hybrid HMM/ANN Systems," Proc. of the Eurospeech Conf., 1997, Rhodes, Greece pp.
1951-1954.
[43]
Hermansky, H., "Perceptual Linear Predictive (PLP) Analysis of Speech," Journal of the
Acoustical Society of America, 1990, 87(4), pp. 1738-1752.

Historical Perspective and Further Reading
469
[44]
Holmes, W. and M. Russell, "Probabilistic-Trajectory Segmental HMMs," Computer Speech
and Language, 1999, 13, pp. 3-37.
[45]
Hon, H.W. and K.F. Lee, "CMU Robust Vocabulary-Independent Speech Recognition Sys-
tem," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1991, To-
ronto pp. 889-892.
[46]
Hon, H.W. and K. Wang, "Combining Frame and Segment Based Models for Large Vocabu-
lary Continuous Speech Recognition," IEEE Workshop on Automatic Speech Recognition
and Understanding, 1999, Keystone, CO.
[47]
Hon, H.-W. and K. Wang, "Unified Frame and Segment Based Models for Automatic
Speech Recognition," Int. Conf. on Acoustic, Signal and Speech Processing, 2000, Istanbul,
Turkey, IEEE pp. 1017-1020.
[48]
Huang, X., et al., "From Sphinx II to Whisper: Making Speech Recognition Usable" in
Automatic Speech and Speaker Recognition, C.H. Lee, F.K. Soong, and K.K. Paliwal, eds.
1996, Norwell, MA, pp. 481-508, Kluwer Academic Publishers.
[49]
Huang, X., et al., "From Sphinx-II to Whisper - Make Speech Recognition Usable" in Auto-
matic Speech and Speaker Recognition, C.H. Lee, F.K. Soong, and K.K. Paliwal, eds. 1996,
Norwell, MA, Kluwer Academic Publishers.
[50]
Huang, X. and K.-F. Lee, "On Speaker-Independent, Speaker-Dependent, and Speaker-
Adaptive Speech Recognition," IEEE Trans. on Speech and Audio Processing, 1993, 1(2),
pp. 150-157.
[51]
Huang, X.D., "Speaker Normalization for Speech Recognition," Proc. of the IEEE Int. Conf.
on Acoustics, Speech and Signal Processing, 1992, San Francisco pp. 465-468.
[52]
Huang, X.D., Y. Ariki, and M.A. Jack, Hidden Markov Models for Speech Recognition,
1990, Edinburgh, U.K., Edinburgh University Press.
[53]
Huang, X.D., K. Lee, and A. Waibel, "Connectionist Speaker Normalization and its Applica-
tions to Speech Recognition," IEEE Workshop on Neural Networks for Signal Processing,
1991, New York pp. 357-366.
[54]
Hunt, M.J., et al., "An Investigation of PLP and IMELDA Acoustic Representations and of
Their Potential for Combination," Proc. of the IEEE Int. Conf. on Acoustics, Speech and
Signal Processing, 1991, Toronto pp. 881-884.
[55]
Huo, Q., C. Chan, and C.-H. Lee, "On-Line Adaptation of the SCHMM Parameters Based
on the Segmental Quasi-Bayes Learning for Speech Recognition," IEEE Trans. on Speech
and Audio Processing, 1996, 4(2), pp. 141-144.
[56]
Hwang, M.Y., X. Huang, and F. Alleva, "Predicting Unseen Triphones with Senones," Proc.
of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1993, Minneapolis pp.
311-314.
[57]
Hwang, M.Y. and X.D. Huang, "Acoustic Classification of Phonetic Hidden Markov Mod-
els" in Proc. of Eurospeech 1991.
[58]
Hwang, M.Y. and X.D. Huang, "Shared-Distribution Hidden Markov Models for Speech
Recognition," IEEE Trans. on Speech and Audio Processing, 1993, 1(4), pp. 414-420.
[59]
Iyer, R., et al., "Hidden Markov Models for Trajectory Modeling," Int. Conf. on Spoken
Language Processing, 1998, Sydney, Australia.
[60]
Jelinek, F., Statistical Methods for Speech Recognition, 1998, Cambridge, MA, MIT Press.
[61]
Jiang, L., H.W. Hon, and X. Huang, "Improvements on a Trainable Letter-to-Sound Con-
verter," Proc. of Eurospeech, 1997, Rhodes, Greece pp. 605-608.
[62]
Jiang, L. and X. Huang, "Subword-Dependent Speaker Clustering for Improved Speech
Recognition," Int Conf. on Spoken Language Processing, 2000, Beijing, China.

470
Acoustic Modeling
[63]
Jiang, L. and X.D. Huang, "Vocabulary-Independent Word Confidence Measure Using
Subword Features," Int. Conf. on Spoken Language Processing, 1998, Syndey, Austrilia.
[64]
Kuhn, R., et al., "Eigenvoices for Speaker Adaptation," Int. Conf. on Spoken Language
Processing, 1998, Sydney, Australia pp. 1771-1774.
[65]
Kumar, N. and A. Andreou, "Heteroscedastic Discriminant Analysis and Reduced Rank
HMMs for Improved Speech Recognition," Speech Communication, 1998, 26, pp. 283-297.
[66]
Lee, K.F., Large-Vocabulary Speaker-Independent Continuous Speech Recognition: The
SPHINX System, Ph.D. Thesis in Computer Science Dept. 1988, Carnegie Mellon Univer-
sity, Pittsburgh.
[67]
Lee, L. and R. Rose, "Speaker Normalization Using Efficient Frequency Warping Proce-
dures," IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1996, Atlanta, GA pp.
353-356.
[68]
Leggetter, C.J. and P.C. Woodland, "Maximum Likelihood Linear Regression for Speaker
Adaptation of Continuous Density Hidden Markov Models," Computer Speech and Lan-
guage, 1995, 9, pp. 171--185.
[69]
Levin, E., "Word Recognition Using Hidden Control Neural Architecture," Proc. of the
IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1990, Albuquerque pp. 433-
436.
[70]
Lippmann, R.P., E.A. Martin, and D.P. Paul, "Multi-Style Training for Robust Isolated-
Word Speech Recognition," Int. Conf. on Acoustics, Speech and Signal Processing, 1987,
Dallas, TX pp. 709-712.
[71]
Lucassen, J.M. and R.L. Mercer, "An Information-Theoretic Approach to the Automatic
Determination of Phonemic Baseforms," Proc. of the IEEE Int. Conf. on Acoustics, Speech
and Signal Processing, 1984, San Diego pp. 42.5.1-42.5.4.
[72]
Lyu, R.Y., et al., "Golden Mandarin (III) - A User-Adaptive Prosodic Segment-Based Man-
darin Dictation Machine For Chinese Language With Very Large Vocabulary" in Proc. of
the IEEE Int. Conf. on Acoustics, Speech and Signal Processing 1995, Detroit, MI, pp. 57-
60.
[73]
Matsui, T., T. Matsuoka, and S. Furui, "Smoothed N-best Based Speaker Adaptation for
Speech Recognition" in Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Proc-
essing 1997, Munich, Germany, pp. 1015--1018.
[74]
McDonough, J., et al., "Speaker-Adapted Training on the Switchboard Corpus" in Proc. of
the IEEE Int. Conf. on Acoustics, Speech and Signal Processing 1997, Munich, Germany,
pp. 1059--1062.
[75]
Morgan, N. and H. Bourlard, Continuous Speech Recognition: An Introduction to Hybrid
HMM/Connectionist Approach, in IEEE Signal Processing Magazine, 1995. pp. 25-42.
[76]
Mori, R.D., Spoken Dialogues with Computers, 1998, London, Academic Press.
[77]
Ostendorf, M., V.V. Digalakis, and O.A. Kimball, "From HMM's to Segment Models: a
Unified View of Stochastic Modeling for Speech Recognition," IEEE Trans. on Speech and
Audio Processing, 1996, 4(5), pp. 360-378.
[78]
Ostendorf, M. and K. Ross, "A Multi-Level Model for Recognition of Intonation Labels" in
Computing Prosody, Y. Sagisaka, W.N. Campell, and N. Higuchi, eds. 1997, New York, pp.
291-308, Springer Verlag.
[79]
Ostendorf, M. and S. Roukos, "A Stochastic Segment Model for Phoneme-Based Continu-
ous Speech Recognition," IEEE Trans. on Acoustics, Speech and Signal Processing, 1989,
37(1), pp. 1857-1869.

Historical Perspective and Further Reading
471
[80]
Pallett, D., J.G. Fiscus, and J.S. Garofolo, "DARPA Resource Management Benchmark Test
Results June 1990" in Proc. of the DARPA Speech and Natural Language Workshop 1990,
Hidden Valley, PA, pp. 298-305, Pallett.
[81]
Pallett, D., J.G. Fiscus, and J.S. Garofolo, "DARPA Resource Management Benchmark Test
Results," Proc. of the DARPA Speech and Natural Language Workshop, 1991, Morgan
Kaufmann Publishers pp. 49-58.
[82]
Pallett, D.S., et al., "The 1994 Benchmark Tests for the ARPA Spoken Language Program"
in Proc. of the ARPA Spoken Language Technology Workshop 1995, Austin, TX, pp. 5-38.
[83]
Pallett, D.S., et al., "1997 Broadcast News Benchmark Test Results: English and Non-
English," Proc. of the Broadcast News Transcription and Understanding Workshop, 1998,
Landsdowne, Virginia, Morgan Kaufmann Publishers.
[84]
Pallett, D.S., J.G. Fiscus, and M.A. Przybocki, "1996 Preliminary Broadcast News Bench-
mark Tests," Proc. of the DARPA Speech Recognition Workshop, 1997, Chantilly, VA,
Morgan Kaufmann Publishers.
[85]
Pereira, F., M. Riley, and R. Sproat, "Weighted Rational Transductions and Their Applica-
tion to Human Language Processing," Proc. of the ARPA Human Language Technology
Workshop, 1994, Plainsboro, NJ pp. 249-254.
[86]
Pye, D. and P.C. Woodland, "Experiments in Speaker Normalization and Adaptation for
Large Vocabulary Speech Recognition" in Proc. of the IEEE Int. Conf. on Acoustics, Speech
and Signal Processing 1997, Munich, Germany, pp. 1047--1050.
[87]
Rabiner, L.R. and B.H. Juang, Fundamentals of Speech Recognition, May, 1993, Prentice-
Hall.
[88]
Rabiner, L.R. and S.E. Levinson, "Isolated and Connected Word Recognition - Theory and
Selected Applications," IEEE Trans. on Communication, 1981, COM-29(5), pp. 621-659.
[89]
Riley, M. and A. Ljolje, eds. Automatic Generation of Detailed Pronunciation Lexicons, in
Automatic Speech and Speaker Recognition, ed. C. Lee, F. Soong, and K. Paliwal, 1996,
Kluwer Academic Publishers.
[90]
Robinson, A., "An Application of Recurrent Nets to Phone Probability Estimation," IEEE
Trans. on Neural Networks, 1994, 5, pp. 298-305.
[91]
Robinson, A.J., et al., "A Neural Network Based, Speaker Independent, Large Vocabulary,"
Proc. of the European Conf. on Speech Communication and Technology, 1999, Berlin pp.
1941--1944.
[92]
Roucos, S., et al., "Stochastic Segment Modeling Using the Estimate-Maximize Algorithm,"
Int. Conf. on Acoustic, Speech and Signal Processing, 1988, New York pp. 127-130.
[93]
Sagisaka, Y. and L.S. Lee, "Speech Recognition of Asian Languages" in Proc. IEEE Auto-
matic Speech Recognition Workshop 1995, Snowbird, UT, pp. 55-57.
[94]
Sakoe, H. and S. Chiba, "Dynamic Programming Algorithm Optimization for Spoken Word
Recognition," IEEE Trans. on Acoustics, Speech and Signal Processing, 1978, 26(1), pp.
43-49.
[95]
Saraclar, M. and S. Khudanpur, "Pronunciation Ambiguity vs. Pronunciation Variability in
Speech Recognition," IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 2000,
Istanbul, Turkey pp. 1679-1682.
[96]
Shinoda, K. and C. Lee, "Structural MAP Speaker Adaptation Using Hierarchical Priors,"
IEEE Workshop on Automatic Speech Recognition and Understanding, 1997, Santa Bar-
bara, CA pp. 381-388.
[97]
Singh, R., B. Raj, and R. Stern, "Automatic Generation of Phone Sets and Lexical Transcrip-
tions," IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 2000, Istanbul, Turkey
pp. 1691-1694.

472
Acoustic Modeling
[98]
Siohan, O., C. Chesta, and C. Lee, "Joint Maximum A Posteriori Estimation of Transforma-
tion and Hidden Markov Model Parameters," IEEE Int. Conf. on Acoustics, Speech and Sig-
nal Processing, 2000, Istanbul, Turkey pp. 965-968.
[99]
Siu, M., et al., "Parametric Trajectory Mixtures for LVCSR," Int. Conf. on Spoken Lan-
guage Processing, 1998, Sydney, Australia.
[100]
Sixtus, A., et al., "Recent Improvements of the RWTH Large Vocabulary Speech Recogni-
tion System on Spontaneous Speech," IEEE Int. Conf. on Acoustics, Speech and Signal
Processing, 2000, Istanbul, Turkey pp. 1671-1674.
[101]
Sorenson, H., "A Cepstral Noise Reduction Multi-Layer Network," Int. Conf. on Acoustics,
Speech and Signal Processing, 1991, Toronto pp. 933-936.
[102]
Sproat, R. and M. Riley, "Compilation of Weighted Finite-State Transducers from Decision
Trees," ACL-96, 1996, Santa Cruz pp. 215-222.
[103]
Tajchman, G., E. Fosler, and D. Jurafsky, "Building Multiple Pronunciation Models for
Novel Words Using Exploratory Computational Phonology," Eurospeech, 1995 pp. 2247-
2250.
[104]
Tebelskis, J. and A. Waibel, "Large Vocabulary Recognition Using Linked Predictive Neural
Networks," Int. Conf. on Acoustics, Speech and Signal Processing, 1990, Albuquerque, NM
pp. 437-440.
[105]
Waibel, A.H. and K.F. Lee, Readings in Speech Recognition, 1990, San Mateo, CA, Morgan
Kaufman Publishers.
[106]
Watrous, R., "Speaker Normalization and Adaptation Using Second-Order Connectionist
Networks," IEEE Trans. on Neural Networks, 1994, 4(1), pp. 21-30.
[107]
Welling, L., S. Kanthak, and H. Ney, "Improved Methods for Vocal Tract Normalization,"
IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1999, Phoenix, AZ.
[108]
Wilpon, J.G., C.H. Lee, and L.R. Rabiner, "Connected Digit Recognition Based on Im-
proved Acoustic Resolution," Computer Speech and Language, 1993, 7(1), pp. 15--26.
[109]
Woodland, P.C., et al., "The 1994 HTK Large Vocabulary Speech Recognition System,"
Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1995, Detroit pp.
73-76.
[110]
Wooters, C. and A. Stolcke, "Multiple-Pronunciation Lexical Modeling in a Speaker Inde-
pendent Speech Understanding System," Proc. of the Int. Conf. on Spoken Language Proc-
essing, 1994, Yokohama, Japan pp. 1363-1366.
[111]
Young, S.J. and P.C. Woodland, "The Use of State Tying in Continuous Speech Recogni-
tion," Proc. of Eurospeech, 1993, Berlin pp. 2203-2206.
[112]
Zavaliagkos, G., R. Schwartz, and J. Makhoul, "Batch, Incremental and Instantaneous Adap-
tation Techniques for Speech Recognition," Proc. of the IEEE Int. Conf. on Acoustics,
Speech and Signal Processing, 1995, Detroit pp. 676-679.
[113]
Zavaliagkos, G., et al., "A Hybrid Segmental Neural Net/Hidden Markov Model System for
Continuous Speech Recognition," IEEE Trans. on Speech and Audio Processing, 1994, 2,
pp. 151-160.
[114]
Zhan, P. and M. Westphal, "Speaker Normalization Based on Frequency Warping" in Proc.
of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing 1997, Munich, Germany,
pp. 1039--1042.
[115]
Zue, V., et al., "The MIT SUMMIT System: A Progress Report," Proc. of DARPA Speech
and Natural Language Workshop, 1989 pp. 179-189.

473
C H A P T E R
1 0
Environmental RobustnessEquation Section 10
A speech recognition systems trained in the
lab with clean speech may degrade significantly in the real world if the clean speech used in
training doesn’t match real-world speech. If its accuracy doesn’t degrade very much under
mismatched conditions, the system is called robust. There are several reasons why real-
world speech may differ from clean speech; in this chapter we focus on the influence of the
acoustical environment, defined as the transformations that affect the speech signal from the
time it leaves the mouth until it is in digital format.
Chapter 9 discussed a number of variability factors that are critical to speech recogni-
tion. Because the acoustical environment is so important to practical systems, we devote this
chapter to ways of increasing the environmental robustness, including microphone, echo
cancellation, and a number of methods that enhance the speech signal, its spectrum, and the
corresponding acoustic model in a speech recognition system.

474
Environmental Robustness
10.1.
THE ACOUSTICAL ENVIRONMENT
The acoustical environment is defined as the set of transformations that affect the speech
signal from the time it leaves the speaker’s mouth until it is in digital form. Two main
sources of distortion are described here: additive noise and channel distortion. Additive
noise, such as a fan running in the background, door slams, or other speakers’ speech, is
common in our daily life. Channel distortion can be caused by reverberation, the frequency
response of a microphone, the presence of an electrical filter in the A/D circuitry, the re-
sponse of the local loop of a telephone line, a speech codec, etc. Reverberation, caused by
reflections of the acoustical wave in walls and other objects, can also dramatically alter the
speech signal.
10.1.1.
Additive Noise
Additive noise can be stationary or nonstationary. Stationary noise, such as that made by a
computer fan or air conditioning, has a power spectral density that does not change over
time. Nonstationary noise, caused by door slams, radio, TV, and other speakers’ voices, has
statistical properties that change over time. A signal captured with a close-talking micro-
phone has little noise and reverberation, even though there may be lip smacks and breathing
noise. A microphone that is not close to the speaker’s mouth may pick up a lot of noise
and/or reverberation.
As described in Chapter 5, a signal x[n] is defined as white noise if its power spectrum
is flat,
( )
xx
S
f
q
=
, a condition equivalent to different samples’ being uncorrelated,
[ ]
[ ]
xx
R
n
q
n
δ
=
. Thus, a white noise signal has to have zero mean. This definition tells us
about the second-order moments of the random process, but not about its distribution. Such
noise can be generated synthetically by drawing samples from a distribution p(x); thus we
could have uniform white noise if p(x) is uniform, or Gaussian white noise if p(x) is Gaus-
sian. While typically subroutines are available that generate uniform white noise, we are
often interested in white Gaussian noise, as it resembles better the noise that tends to occur
in practice. See Algorithm 10.1 for a method to generate white Gaussian noise. Variable x is
normally continuous, but it can also be discrete.
White noise is useful as a conceptual entity, but it seldom occurs in practice. Most of
the noise captured by a microphone is colored, since its spectrum is not white. Pink noise is
a particular type of colored noise that has a low-pass nature, as it has more energy at the low
frequencies and rolls off at higher frequencies. The noise generated by a computer fan, an air
conditioner, or an automobile engine can be approximated by pink noise. We can synthesize
pink noise by filtering white noise with a filter whose magnitude squared equals the desired
pow*er spectrum.
A great deal of additive noise is nonstationary, since its statistical properties change
over time. In practice, even the noises from a computer, an air conditioning system, or an
automobile are not perfectly stationary. Some nonstationary noises, such as keyboard clicks,
are caused by physical objects. The speaker can also cause nonstationary noises such as lip
smacks and breath noise. The cocktail party effect is the phenomenon under which a human

The Acoustical Environment
475
listener can focus onto one conversation out of many in a cocktail party. The noise of the
conversations that are not focused upon is called babble noise. When the nonstationary noise
is correlated with a known signal, the adaptive echo-canceling (AEC) techniques of Section
10.3 can be used.
ALGORITHM 10.1 WHITE NOISE GENERATION
To generate white noise in a computer, we can first generate a random variable • with a
Rayleigh distribution:
2 / 2
( )
p
e ρ
ρ ρ
ρ
−
=
(10.1)
from another random variable r with a uniform distribution between (0, 1),
( )
1
rp r = , by simply
equating the probability mass
( )
( )
r
p
d
p r dr
ρ ρ
ρ =
so that
2 / 2
dr
e
d
ρ
ρ
ρ
−
=
; with integration, it
results in
2 / 2
r
e ρ
−
=
and the inverse is given by
2ln r
ρ =
−
(10.2)
If r is uniform between (0, 1), and • is computed through Eq. (10.2), it follows a Rayleigh distri-
bution as in Eq. (10.1). We can then generate Rayleigh white noise by drawing independent
samples from such a distribution.
If we want to generate white Gaussian noise, the method used above does not work, be-
cause the integral of the Gaussian distribution does not exist in closed form. However, if • fol-
lows a Rayleigh distribution as in Eq. (10.1), obtained using Eq. (10.2) where r is uniform be-
tween (0, 1), and • is uniformly distributed between (0, 2•), then the white Gaussian noise can
be generated as the following two variables x and y:
cos( )
sin( )
x
y
ρ
θ
ρ
θ
=
=
(10.3)
They are independent Gaussian random variables with zero mean and unity variance, since the
Jacobian of the transformation is given by
cos
sin
sin
cos
x
x
y
y
p
p
J
p
p
ρ
θ
θ
ρ
θ
ρ
θ
ρ
θ
ρ
θ
∂
∂
∂
∂
−
=
=
=
∂
∂
∂
∂
(10.4)
and the joint density
( , )
p x y
is given by
2
2
2
/ 2
(
) / 2
( , )
( ) ( )
1
( , )
2
1
( ,0,1)
( ,0,1)
2
x
y
p
p
p
p x y
e
J
e
N x
N y
ρ
ρ θ
ρ
θ
ρ
π
π
−
−
+
=
=
=
=
=
(10.5)

476
Environmental Robustness
The presence of additive noise can sometimes change the way the speaker speaks. The
Lombard effect [40] is a phenomenon by which a speaker increases his vocal effort in the
presence of background noise. When a large amount of noise is present, the speaker tends to
shout, which entails not only a higher amplitude, but also often higher pitch, slightly differ-
ent formants, and a different coloring of the spectrum. It is very difficult to characterize
these transformations analytically, but recently some progress has been made [36].
10.1.2.
Reverberation
If both the microphone and the speaker are in an anechoic1 chamber or in free space, a mi-
crophone picks up only the direct acoustic path. In practice, in addition to the direct acoustic
path, there are reflections of walls and other objects in the room. We are well aware of this
effect when we are in a large room, which can prevent us from understanding if the rever-
beration time is too long. Speech recognition systems are much less robust than humans and
they start to degrade with shorter reverberation times, such as those present in a normal of-
fice environment.
As described in Section 10.2.2, the signal level at the microphone is inversely propor-
tional to the distance r from the speaker for the direct path. For the kth reflected sound wave,
the sound has to travel a larger distance
kr , so that its level is proportionally lower. This
reflection also takes time
/
k
k
T
r
c
=
to arrive, where c is the speed of sound in air.2 More-
over, some energy absorption a takes place each time the sound wave hits a surface. The
impulse response of such filter looks like
0
0
1
[ ]
[
]
[
]
k
k
k
k
k
k
k
k
h n
n
T
n
T
r
c
T
ρ
ρ
δ
δ
∞
∞
=
=
=
−
=
−


(10.6)
where
k
ρ
is the combined attenuation of the kth reflected sound wave due to absorption.
Anechoic rooms have
0
k
ρ ≈
. In general
k
ρ
is a (generally decreasing) function of fre-
quency, so that instead of impulses
[ ]
n
δ
in Eq. (10.6), other (low-pass) impulse responses
are used.
Often we have available a large amount of speech data recorded with a close-talking
microphone, and we would like to use the speech recognition system with a far field micro-
phone. To do that we can filter the clean-speech training database with a filter h[n], so that
the filtered speech resembles speech collected with the far field microphone, and then retrain
the system. This requires estimating the impulse response h[n] of a room. Alternatively, we
can filter the signal from the far field microphone with an inverse filter to make it resemble
the signal from the close-talking microphone.
1 An anechoic chamber is a room that has walls made of special fiberglass or other sound-absorbing materials so
that absorbs all echoes. It is equivalent to being in free space, where there are neither walls nor reflecting surfaces.
2 In air at standard atmospheric pressure and humidity the speed of sound is
331.4
0.6
(
/ )
c
T
m s
=
+
is. It varies
with different media and different levels of humidity and pressure.

The Acoustical Environment
477
One way to estimate the impulse response is to play a white noise signal x[n] through
a loudspeaker or artificial mouth; the signal y[n] captured at the microphone is given by
[ ]
[ ]
[ ]
[ ]
y n
x n
h n
v n
=
∗
+
(10.7)
where v[n] is the additive noise present at the microphone. This noise is due to sources such
as air conditioning and computer fans and is an obstacle to measuring h[n]. The impulse
response can be estimated by minimizing the error over N samples
2
1
1
0
0
1
[ ]
[ ] [
]
N
M
n
m
E
y n
h m x n
m
N
−
−
=
=


=
−
−






(10.8)
which, taking the derivative with respect to h[m] and equating to 0, results in our estimate
ˆ[ ]
h l :
1
1
ˆ
0
0
[ ]
[ ]
1
1
1
0
0
0
1
1
1
0
0
0
1
ˆ
[ ]
[ ] [
]
[
]
[ ]
1
1
ˆ
[ ] [
]
[ ]
[
] [
]
1
1
ˆ
ˆ
[ ] [
]
[ ]
[ ]
[
] [
]
[
]
0
N
M
n
m
h l
h l
N
M
N
n
m
n
N
M
N
n
m
n
E
y n
h m x n
m x n
l
h l
N
y n x n
l
h m
x n
m x n
l
N
N
y n x n
l
h l
h m
x n
m x n
l
m
l
N
N
δ
−
−
=
=
=
−
−
−
=
=
=
−
−
−
=
=
=
∂


=
−
−
−


∂




=
−
−
−
−






=
−
−
−
−
−
−
−
=












(10.9)
Since we know our white process is ergodic, it follows that we can replace time averages by
ensemble averages as N →∞:
{
}
1
0
1
lim
[
] [
]
[
] [
]
[
]
N
N
n
x n
m x n
l
E x n
m x n
l
m
l
N
δ
−
→∞
=
−
−
=
−
−
=
−

(10.10)
so that we can obtain a reasonable estimate of the impulse response as
1
0
1
ˆ[ ]
[ ] [
]
N
n
h l
y n x n
l
N
−
=
=
−

(10.11)
Inserting Eq. (10.7) into Eq. (10.11) ,we obtain
ˆ[ ]
[ ]
[ ]
h l
h l
e l
=
+
(10.12)
where the estimation error e[n] is given by
1
1
1
0
0
0
1
1
[ ]
[ ] [
]
[ ]
[
] [
]
[
]
N
M
N
n
m
n
e l
v n x n
l
h m
x n
m x n
l
m
l
N
N
δ
−
−
−
=
=
=


=
−
+
−
−
−
−







(10.13)
If v[n] and x[n] are independent processes, then
{ [ ]}
0
E e l
=
, since x[n] is zero-mean,
so that the estimate of Eq. (10.11) is unbiased. The covariance matrix decreases to 0 as

478
Environmental Robustness
N →∞, with the dominant term being the noise v[n]. The choice of N for a low-variance
estimate depends on the filter length M and the noise level present in the room.
The filter h[n] could also be estimated by playing sine waves of different frequencies
or a chirp3 [52]. Since playing a white noise signal or sine waves may not be practical, an-
other method is based on collecting stereo recordings with a close-talking microphone and a
far field microphone. The filter h[n] of length M is estimated so that when applied to the
close-talking signal x[n] it minimizes the squared error with the far field signal y[n], which
results in the following set of M linear equations:
1
0
[ ]
[
]
[ ]
M
xx
xy
m
h m R
m
n
R
n
−
=
−
=

(10.14)
which is a generalization of Eq. (10.11) when x[n] is not a white noise signal.
0
200
400
600
800
1000
1200
1400
1600
1800
2000
-3000
-2000
-1000
0
1000
2000
3000
4000
5000
6000
7000
Time (samples)
Room Impulse Response
Figure 10.1 Typical impulse response of an average office. Sampling rate was 16 kHz. It was
estimated by driving a 4-minute segment of white noise through an artificial mouth and using
Eq. (10.11). The filter length is about 125 ms.
It is not uncommon to have reverberation times of over 100 milliseconds in office
rooms. In Figure 10.1 we show the typical impulse response of an average office.
10.1.3.
A Model of the Environment
A widely used model of the degradation encountered by the speech signal when it gets cor-
rupted by both additive noise and channel distortion is shown in Figure 10.2. We can derive
3 A chirp function continuously varies its frequency. For example, a linear chirp varies its frequency linearly with
time:
0
1
sin( (
))
n
n
ω
ω
+
.

The Acoustical Environment
479
the relationships between the clean signal and the corrupted signal both in power-spectrum
and cepstrum domains based on such a model [2].
Figure 10.2 A model of the environment.
In the time domain, additive noise and linear filtering results in
[ ]
[ ]
[ ]
[ ]
y m
x m
h m
n m
=
∗
+
(10.15)
It is convenient to express this in the frequency domain using the short-time analysis
methods of Chapter 6. To do that, we window the signal, take a 2K-point DFT in Eq. (10.15)
and then the magnitude squared:
{
}
2
2
2
2
2
2
2
(
)
(
)
(
)
(
)
2Re
(
)
(
)
(
)
(
)
(
)
(
)
2
(
)
(
)
(
) cos(
)
k
k
k
k
k
k
k
k
k
k
k
k
k
k
Y f
X f
H f
N f
X f
H f
N
f
X f
H f
N f
X f
H f
N f
θ
∗
=
+
+
=
+
+
(10.16)
where
0,1,
,
k
K
=

, we have used upper case for frequency domain linear spectra, and
k
θ
is the angle between the filtered signal and the noise for bin k.
The expected value of the cross-term in Eq. (10.16) is zero, since x[m] and n[m] are
statistically independent. In practice, this term is not zero for a given frame, though it is
small if we average over a range of frequencies, as we often do when computing the popular
mel-cepstrum (see Chapter 6). When using a filterbank, we can obtain a relationship for the
energies at each of the M filters:
2
2
2
2
(
)
(
)
(
)
(
)
i
i
i
i
Y f
X f
H f
N f
≈
+
(10.17)
where it has been shown experimentally that this assumption works well in practice.
Equation (10.17) is also implicitly assuming that the length of h[n], the filter’s impulse
response, is much shorter than the window length 2N. That means that for filters with long
reverberation times, Eq. (10.17) is inaccurate. For example, for
2
( )
0
N f
=
, a window shift
of T, and a filter’s impulse response
[ ]
[
]
h n
n
T
δ
=
−
, we have
1
[
]
[
]
t
m
t
m
Y f
X
f
−
=
; i.e., the
output spectrum at frame t does not depend on the input spectrum at that frame. This is a
more serious assumption, which is why speech recognition systems tend to fail under long
reverberation times.
By taking logarithms in Eq. (10.17), and after some algebraic manipulation, we obtain
n[m]
x[m]
y[m]
h[m]

480
Environmental Robustness
(
)
(
)
2
2
2
2
2
2
ln
(
)
ln
(
)
ln
(
)
ln 1
exp ln
(
)
ln
(
)
ln
(
)
i
i
i
i
i
i
Y f
X f
H f
N f
X f
H f
≈
+
+
+
−
−
(10.18)
Since most speech recognition systems use cepstrum features, it is useful to see the ef-
fect of the additive noise and channel distortion directly on the cepstrum. To do that, let’s
define the following length-(M + 1) cepstrum vectors:
(
)
(
)
(
)
(
)
2
2
2
0
1
2
2
2
0
1
2
2
2
0
1
2
2
2
0
1
ln
(
)
ln
(
)
ln
(
)
ln
(
)
ln
(
)
ln
(
)
ln
(
)
ln
(
)
ln
(
)
ln
(
)
ln
(
)
ln
(
)
M
M
M
M
X f
X f
X f
H f
H f
H f
N f
N f
N f
Y f
Y f
Y f
=
=
=
=
x
C
h
C
n
C
y
C




(10.19)
where C is the DCT matrix and we have used lower-case bold to represent cepstrum vectors.
Combining Eqs. (10.18) and (10.19) results in
ˆ
(
)
=
+
+
−
−
y
x
h
g n
x
h
(10.20)
where the nonlinear function g(z) is given by
(
)
1
( )
ln 1
e
−
=
+
C z
g z
C
(10.21)
Equations (10.20) and (10.21) say that we can compute the cepstrum of the corrupted
speech if we know the cepstrum of the clean speech, the cepstrum of the noise, and the cep-
strum of the filter. In practice, the DCT matrix C is not square, so that the dimension of the
cepstrum vector is much smaller than the number of filters. This means that we are losing
resolution when going back to the frequency domain, and thus Eqs. (10.20) and (10.21) rep-
resent only an approximation, though it has been shown to work reasonably well.
As discussed in Chapter 9, the distribution of the cepstrum of x can be modeled as a
mixture of Gaussian densities. Even if we assume that x follows a Gaussian distribution, y in
Eq. (10.20) is no longer Gaussian because of the nonlinearity in Eq. (10.21).
It is difficult to visualize the effect on the distribution, given the nonlinearity involved.
To provide some insight, let’s consider the frequency-domain version of Eq. (10.18) when
no filtering is done, i.e.,
( )
1
H f
= :
(
)
(
)
ln 1
exp
y
x
n
x
=
+
+
−
(10.22)
where x, n, and y represent the log-spectral energies of the clean signal, noise, and noisy
signal, respectively, for a given frequency. Using simulated data, not real speech, we can
analyze the effect of this transformation. Let’s assume that both x and n are Gaussian ran-
dom variables. We can use Monte Carlo simulation to draw a large number of points from
those two Gaussian distributions and obtain the corresponding noisy values y using Eq.

The Acoustical Environment
481
(10.22). Figure 10.3 shows the resulting distribution for several values of
x
σ . We fixed
0dB
n
µ =
, since it is only a relative level, and set
2dB
n
σ =
, a typical value. We also set
25dB
x
µ =
and see that the resulting distribution can be bimodal when
x
σ
is very large.
Fortunately, for modern speech recognition systems that have many Gaussian components,
x
σ
is never that large and the resulting distribution is unimodal.
0
50
100
0
0.01
0.02
0.03
0
20
40
60
0
0.01
0.02
0.03
0.04
0
20
40
60
0
0.02
0.04
0.06
0.08
Figure 10.3 Distributions of the corrupted log-spectra y of Eq. (10.22) using simulated data.
The distribution of the noise log-spectrum n is Gaussian with mean 0 dB and standard devia-
tion of 2 dB. The distribution of the clean log-spectrum x is Gaussian with mean 25 dB and
standard deviations of 25, 10, and 5 dB, respectively (the x-axis is expressed in dB). The first
distribution is bimodal, whereas the other two are approximately Gaussian. Curves are plotted
using Monte Carlo simulation.
Figure 10.4 shows the distribution of y for two values of
x
µ , given the same values for
the noise distribution,
0dB
n
µ =
and
2dB
n
σ =
, and a more realistic value for
5dB
x
σ =
.
We see that the distribution is always unimodal, though not necessarily symmetric, particu-
larly for low SNR (
x
n
µ
µ
−
).
0
10
20
30
0
0.02
0.04
0.06
0.08
0
10
20
30
0
0.05
0.1
Figure 10.4 Distributions of the corrupted log-spectra y of Eq. (10.22) using simulated data.
The distribution of the noise log-spectrum n is Gaussian with mean 0 dB and standard devia-
tion of 2 dB. The distribution of the clean log-spectrum is Gaussian with standard deviation of
5 dB and means of 10 and 5 dB, respectively. The first distribution is approximately Gaussian
while the second is nonsymmetric. Curves are plotted using Monte Carlo simulation.
The distributions used in an HMM are mixtures of Gaussians so that, even if each
Gaussian component is transformed into a non-Gaussian distribution, the composite distribu-
tion can be modeled adequately by another mixture of Gaussians. In fact, if you retrain the

482
Environmental Robustness
model using the standard Gaussian assumption on corrupted speech, you can get good re-
sults, so this approximation is not bad.
10.2.
ACOUSTICAL TRANSDUCERS
Acoustical transducers are devices that convert the acoustic energy of sound into electrical
energy (microphones) and vice versa (loudspeakers). In the case of a microphone this trans-
duction is generally realized with a diaphragm, whose movement in response to sound pres-
sure varies the parameters of an electrical system (a variable-resistance conductor, a con-
denser, etc.), producing a variable voltage that constitutes the microphone output. We focus
on microphones because they play an important role in designing speech recognition sys-
tems.
There are near field or close-talking microphones, and far field microphones. Close-
talking microphones, either head-mounted or telephone handsets, pick up much less back-
ground noise, though they are more sensitive to throat clearing, lip smacks, and breath noise.
Placement of such a microphone is often very critical, since, if it is right in front of the
mouth, it can produce pops in the signal with plosives such as /p/. Far field microphones can
be lapel mounted or desktop mounted and pick up more background noise than near field
microphones. Having a small but variable distance to the microphone could be worse than a
larger but more consistent distance, because the corresponding HMM may have lower vari-
ability.
When used in speech recognition systems, the most important measurement is the sig-
nal-to-noise ratio (SNR), since the lower the SNR the higher the error rate. In addition, dif-
ferent microphones have different transfer functions, and even the same microphone offers
different transfer functions depending on the distance between mouth and microphone.
Varying noise and channel conditions are a challenge that speech recognition systems have
to address, and in this chapter we present some techniques to combat them.
The most popular type of microphone is the condenser microphone. We shall study in
detail its directionality patterns, frequency response, and electrical characteristics.
10.2.1.
The Condenser Microphone
A condenser microphone has a capacitor consisting of a pair of metal plates separated by an
insulating material called a dielectric (see Figure 10.5). Its capacitance C is given by
2
0
/
C
b
h
ε π
=
(10.23)
where
0ε is a constant, b is the width of the plate, and h is the separation between the plates.
If we polarize the capacitor with a voltage
cc
V , it acquires a charge Q given by
cc
Q
CV
=
(10.24)

Acoustical Transducers
483
One of the plates is free to move in response to changes in sound pressure, which re-
sults in a change in the plate separation ∆h, thereby changing the capacitance and producing
a change in voltage
/
cc
V
hV
h
∆
= ∆
. Thus, the sensitivity4 of the microphone depends on the
polarizing voltage
cc
V , which is why this voltage can often be 100 V.
Figure 10.5 A diagram of a condenser microphone.
Electret microphones are a type of condenser microphones that do not require a spe-
cial polarizing voltage
cc
V , because a charge is impressed on either the diaphragm or the
back plate during manufacturing and it remains for the life of the microphone. Electret mi-
crophones are light and, because of their small size, they offer good responses at high
frequencies.
From the electrical point of view, a microphone is equivalent to a voltage source v(t)
with an impedance ZM, as shown in Figure 10.6. The microphone is connected to a preampli-
fier which has an equivalent impedance RL.
Figure 10.6 Electrical equivalent of a microphone.
From Figure 10.6 we can see that the voltage on RL is
( )
( ) (
)
L
R
M
L
R
v
t
v t
R
R
=
+
(10.25)
Maximization of
( )
R
v
t
in Eq. (10.25) results in
L
R = ∞, or in practice
L
M
R
R
>>
,
which is called bridging. Thus, for highest sensitivity the impedance of the amplifier has to
be at least 10 times higher than that of the microphone. If the microphone is connected to an
amplifier with lower impedance, there is a load loss of signal level. Most low-impedance
4 The sensitivity of a microphone measures the open-circuit voltage of the electric signal the microphone delivers
for a sound wave for a given sound pressure level, often 94 dB SPL, when there is no load or a high impedance.
This voltage is measured in dBV, where the 0-dB reference is 1 V rms.
b
b
h
~
ZM
RL
v(t)
G
+
-
Preamplifier
Microphone

484
Environmental Robustness
microphones are labeled as 150 ohms, though the actual values may vary between 100 and
300. Medium impedance is 600 ohms and high impedance is 600–10,000 ohms. In practice,
the microphone impedance is a function of frequency. Signal power is measured in dBm,
where the 0-dB reference corresponds to 1 mW dissipated in a 600-ohm resistor. Thus, 0
dBm is equivalent to 0.775 V.
Since the output impedance of a condenser microphone is very high (~ 1 Mohm), a
JFET transistor must be coupled to lower the equivalent impedance. Such a transistor needs
to be powered with DC voltage through a different wire, as in Figure 10.7. A standard sound
card has a jack with the audio on the tip, ground on the sleeve, DC bias VDD on the ring, and
a medium impedance. When using phantom power, the VCC bias is provided directly in the
audio signal, which must be balanced to ground.
Figure 10.7 Equivalent circuit for a condenser microphone with DC bias on a separate wire.
It is important to understand how noise affects the signal of a microphone. If thermal
noise arises in the resistor RL, it will have a power
4
N
P
kTB
=
(10.26)
where k = 1.38 × 10-23 J/K is the Bolzmann’s constant, T is the temperature in °K, and B is
the bandwidth in Hz. The thermal noise in Eq. (10.26) at room temperature (T = 297°K) and
for a bandwidth of 4 kHz is equivalent to –132 dBm. In practice, the noise is significantly
higher than this because of preamplifier noise, radio-frequency noise and electromagnetic
interference (poor grounding connections). It is, thus, important to keep the signal path be-
tween the microphone and the preamp as short as possible to avoid extra noise. It is desir-
able to have a microphone with low impedance to decrease the effect of noise due to radio-
frequency interference, and to decrease the signal loss if long cables are used. Most micro-
phones specify their SNR and range where they are linear (dynamic range). For condenser
microphones, a power supply is necessary (DC bias required). Microphones with balanced
output (the signal appears across two inner wires not connected to ground, with the shield of
the cable connected to ground) are more resistant to radio frequency interference.
10.2.2.
Directionality Patterns
A microphone’s directionality pattern measures its sensitivity to a particular direction. Mi-
crophones may also be classified by their directional properties as omnidirectional (or non-
RM
RL
Preamplifier
Microphone
G
+
-
C0
Vcc
Rc
CM
VDD
Cc

Acoustical Transducers
485
directional) and directional, the latter subdivided into bidirectional and unidirectional, based
upon their response characteristics.
10.2.2.1.
Omnidirectional Microphones
By definition, the response of an omnidirectional microphone is independent of the direction
from which the encroaching sound wave is coming. Figure 10.8 shows the polar response of
an omnidirectional mike. A microphone’s polar response, or pickup pattern, graphs its out-
put voltage for an input sound source with constant level at various angles around the mic.
Typically, a polar response assumes a preferred direction, called the major axis or front of
the microphone, which corresponds to the direction at which the microphone is most sensi-
tive. The front of the mike is labeled as zero degrees on the polar plot, but since an omnidi-
rectional mic has no particular direction at which it is the most sensitive, the omnidirectional
mike has no true front and hence the zero-degree axis is arbitrary. Sounds coming from any
direction around the microphone are picked up equally. Omnidirectional microphones pro-
vide no noise cancellation.
0.5
1
30
210
60
240
90
270
120
300
150
330
180
0
(a)
(b)
Figure 10.8 (a) Polar response of an ideal omnidirectional microphone and (b) its cross sec-
tion.
Figure 10.8 shows the mechanics of the ideal5 omnidirectional condenser microphone.
A sound wave creates a pressure all around the microphone. The pressure enters the opening
of the mic and the diaphragm moves. An electrical circuit converts the diaphragm movement
into an electrical voltage, or response. Sound waves impinging on the mic create a pressure
at the opening regardless of the direction from which they are coming; therefore we have a
nondirectional, or omnidirectional, microphone. As we have seen in Chapter 2, if the source
signal is
j t
Be ω , the signal at a distance r is given by (
/ )
j t
A r e ω independently of the angle.
This is the most inexpensive of the condenser microphones, and it has the advantage
of a flat frequency response that doesn’t change with the angle or distance to the micro-
phone. On the other hand, because of its uniform polar pattern, it picks up not only the de-
sired signal but also noise from any direction. For example, if a pair of speakers is monitor-
ing the microphone output, the sound from the speakers can reenter the microphone and
create an undesirable sound called feedback.
5 Ideal omnidirectional microphones do not exist.
Diaphragm
Mic opening

486
Environmental Robustness
10.2.2.2.
Bidirectional Microphones
The bidirectional microphone is a noise-canceling microphone; it responds less to sounds
incident from the sides. The bidirectional mic utilizes the properties of a gradient micro-
phone to achieve its noise-canceling polar response. You can see how this is accomplished
by looking at the diagram of a simplified gradient bidirectional condenser microphone, as
shown in Figure 10.9. A sound impinging upon the front of the microphone creates a pres-
sure at the front opening. A short time later, this same sound pressure enters the back of the
microphone. The sound pressure never arrives at the front and back at the same time. This
creates a displacement of the diaphragm and, just as with the omnidirectional mic, a corre-
sponding electrical signal. For sounds impinging from the side, however, the pressure from
an incident sound wave at the front opening is identical to the pressure at the back. Since
both openings lead to one side of the diaphragm, there is no displacement of the diaphragm,
and the sound is not reproduced.
Figure 10.9 Cross section of an ideal bidirectional microphone.
Figure 10.10 Approximation to the noise-canceling microphone of Figure 10.9.
To compute the polar response of this gradient microphone let’s make the approxima-
tion of Figure 10.10, where the microphone signal is the difference between the signal at the
front and rear of the diaphragm, the separation between plates is 2d, and r is the distance
between the source and the center of the microphone.
Speech sound wave
from the front
Noise sound wave
from the side
r
source
θ
(d, 0)
(–d,
r1
r2

Acoustical Transducers
487
You can see that
1r , the distance between the source and the front of the diaphragm, is
the norm of the vector specifying the source location minus the vector specifying the loca-
tion of the front of the diaphragm
1
j
r
re
d
θ
=
−
(10.27)
Similarly, you obtain the distance between the source and the rear of the diaphragm
2
j
r
re
d
θ
=
+
(10.28)
The source arrives at the front of the diaphragm with a delay
1
1 /
r c
δ =
, where c is the
speed of sound in air. Similarly, the delay to the rear of the diaphragm is
2
2 /
r
c
δ =
. If the
source is a complex exponential
j t
e ω , the difference signal between the front and rear is
given by
1
2
2
(
)
2
(
)
2
1
2
( )
( , )
j
f t
j
f t
j
ft
A
A
A
x t
e
e
e
G f
r
r
r
π
δ
π
δ
π
θ
−
−
=
−
=
(10.29)
where A is a constant and, using Eqs. (10.27), (10.28) and (10.29), the gain
( , )
G f θ
is given
by
2
2
( , )
j
j
j
e
f
j
e
f
j
j
e
e
G f
e
e
θ
θ
π
λ τ
π
λ τ
θ
θ
θ
λ
λ
−
−
−
+
=
−
−
+
(10.30)
where we have defined
/
d r
λ =
and
/
r c
τ =
.
5
10
15
20
25
30
210
60
240
90
270
120
300
150
330
180
0
Figure 10.11 Polar response of a bidirectional microphone obtained through (10.30) with d = 1
cm, r = 50 cm, c = 33,000 cm/s, and f = 1000 Hz.
The magnitude of Eq. (10.30) is used to plot the polar response of Figure 10.11. As
can be seen by the plot, the pattern resembles a figure eight. The bidirectional mic has an

488
Environmental Robustness
interchangeable front and back, since the response is a maximum in two opposite directions.
In practice, this bidirectional microphone is an ideal case, and the polar response has to be
measured empirically.
According to the idealized model, the frequency response of omnidirectional micro-
phones is constant with frequency, and this approximately holds in practice for real omnidi-
rectional microphones. On the other hand, the polar pattern of directional microphones is not
constant with frequency. Clearly it is a function of frequency, as can be seen in Eq. (10.32).
In fact, the frequency response of a bidirectional microphone at 0° is shown in Figure 10.12
for both near field and far field conditions.
10
2
10
3
10
4
-30
-25
-20
-15
-10
-5
0
Frequency (Hz)
Difference in air pressure (dB)
Figure 10.12 Frequency response of a bidirectional microphone with
1
d = cm at 0°. The lar-
ger the distance between plates, the lower the frequency. The highest values are obtained for
8250 Hz and 24,750 Hz and the null for 16,500 Hz. The solid line corresponds to far field con-
ditions (
0.02
λ =
) and the dotted line to near field conditions (
0.5
λ =
).
It can be shown, after taking the derivative of
( ,0)
G f
in Eq. (10.32) and equating to
zero, that the maxima are given by
(2
1)
4
n
c
f
n
d
=
−
(10.31)
with
1,2,
n =
. We can observe from Eq. (10.31) that the larger the width of the dia-
phragm, the lower the first maximum.
The increase in frequency response, or sensitivity, in the near field, compared to the
far field, is a measure of noise cancellation. Consequently the microphone is said to be noise
canceling. The microphone is also referred to as a differential or gradient microphone, since
it measures the gradient (difference) in sound pressure between two points in space. The
boost in low-frequency response in the near field is also referred to as the proximity effect,
often used by singers to boost their bass levels by getting the microphone closer to their
mouths.

Acoustical Transducers
489
By evaluating Eq. (10.30) it can be seen that low-frequency sounds in a bidirectional
microphone are not reproduced as well as higher frequencies, leading to a thin sounding
mic. The resistive material of a unidirectional microphone reduces the high-frequency re-
sponse and makes the microphone reproduce low and high frequencies more equally than
the bidirectional microphone.
Let’s interpret Figure 10.12. The net sound pressure between these two points, sepa-
rated by a distance D = 2d, is influenced by two factors: phase shift and inverse square law.
The influence of the sound-wave phase shift is less at low frequencies than at high,
because the distance D between the front and rear port entries becomes a small fraction of
the low-frequency wavelength. Therefore, there is little phase shift between the ports at low
frequencies, as the opposite sides of the diaphragm receive nearly equal amplitude and
phase. The result is slight diaphragm motion and a weak microphone output signal. At
higher frequencies, the distance D between sound ports becomes a larger fraction of the
wavelength. Therefore, more phase shift exists across the diaphragm. This causes a higher
microphone output.
The pressure difference caused by phase shift rises with frequency at a rate of 20 dB
per decade. As the frequency rises to where the microphone port spacing D equals half a
wavelength, the net pressure is at its maximum. In this situation, the diaphragm movement is
also at its maximum, since the front and rear see equal amplitude but in opposite polarities
of the wave front. This results in a peak in the microphone frequency response, as illustrated
in Figure 10.12. As the frequency continues to rise to where the microphone port spacing D
equals one complete wavelength, the net pressure is at its minimum. Here, the diaphragm
does not move at all, since the front and rear sides see equal amplitude at the same polarity
of the wave front. This results in a dip in the microphone frequency response, as shown in
Figure 10.12.
A second factor creating a net pressure difference across the diaphragm is the impact
of the inverse square law. If the sound-pressure difference between the front and rear ports
of a noise-canceling microphone were measured near the sound source and again further
from the source, the near field measurement would be greater than the far field. In other
words, the microphone's net pressure difference and, therefore, output signal, is greater in
the near sound field than in the far field. The inverse-square-law effect is independent of
frequency. The net pressure that causes the diaphragm to move is a combination of both the
phase shift and inverse-square-law effect. These two factors influence the frequency re-
sponse of the microphone differently, depending on the distance to the sound source. For
distant sound, the influence of the net pressure difference from the inverse-square-law effect
is weaker than the phase-shift effect; thus, the rising 20-dB-per-decade frequency response
dominates the total frequency response. As the microphone is moved closer to the sound
source, the influence of the net pressure difference from the inverse square law is greater
than that of the phase shift; thus the total microphone frequency response is largely flat.
The difference in near field to far field frequency response is a characteristic of all
noise-canceling microphones and applies equally to both acoustic and electronic types.

490
Environmental Robustness
10.2.2.3.
Unidirectional Microphones
Unidirectional microphones are designed to pick-up the speaker’s voice by directing the
audio reception toward the speaker, focusing on the desired input and rejecting sounds ema-
nating from other directions that can negatively impact clear communications, such as com-
puter noise from fans or other sounds.
Figure 10.13 Cross section of a unidirectional microphone.
Figure 10.13 shows the cross-section of a unidirectional microphone, which also relies
upon the principles of a gradient microphone. Notice that the unidirectional mic looks simi-
lar to the bidirectional, except that there is a resistive material (often cloth or foam) between
the diaphragm and the opening of one end. The material's resistive properties slow down the
pressure on its path from the back opening to the diaphragm, thus optimizing it so that a
sound pressure impinging on the back of the microphone takes equally long to reach the rear
of the diaphragm as to reach the front of the diaphragm. If the additional delay through the
back plate is given by
0
τ , the gain can be given by
(
)
0
2
2
( , )
j
j
j
e
f
j
e
f
j
j
e
e
G f
e
e
θ
θ
π τ
λ τ
π
λ τ
θ
θ
θ
λ
λ
−
+
+
−
−
=
−
−
+
(10.32)
which was obtained by modifying Eq. (10.30). Unidirectional microphones have the greatest
response to sound waves impinging from one direction, typically referred to as the front, or
major axis of the microphone. One typical response of a unidirectional microphone is the
cardioid pattern shown in the polar plot of Figure 10.14, plotted from Eq. (10.32). The fre-
quency response at 0° is similar to that of Figure 10.12. Because the cardioid pattern of polar
response is so popular among them, unidirectional mics are often referred to as cardioid
mics.
Speech sound wave
from the front
Noise sound wave
from the side

Acoustical Transducers
491
5
10
15
20
25
30
210
60
240
90
270
120
300
150
330
180
0
Figure 10.14 Polar response (a) of a unidirectional microphone and its cross section (b). The
polar response was obtained through Eq. (10.32) with d = 1 cm, r = 50 cm, c = 33,000 cm/s, f
= 1 kHz, and
0
0.06
τ =
ms.
Equation (10.32) was derived under a simplified schematic based on Figure 10.10,
which is an idealized model so that, in practice, the polar response of a real microphone has
to be measured empirically. The frequency response and polar pattern of a commercial mi-
crophone are shown in Figure 10.15.
Figure 10.15 Characteristics of an AKG C1000S cardioid microphone: (a) frequency response
for near and far field conditions (note the proximity effect) and (b) polar pattern for different
frequencies.
Although this noise cancellation decreases the overall response to sound pressure (sen-
sitivity) of the microphone, the directional and frequency-response improvements far out-

492
Environmental Robustness
weigh the lessened sensitivity. It is particularly well suited for use as a desktop mic or as
part of an embedded microphone in a laptop or desktop computer. Unidirectional micro-
phones achieve superior noise-rejection performance over omnidirectionals. Such perform-
ance is necessary for clean audio input and for audio signal processing algorithms such as
acoustic echo cancellation, which form the core of speakerphone applications.
10.2.3.
Other Transduction Categories
In a passive microphone, sound energy is directly converted to electrical energy, whereas an
active microphone requires an external energy source that is modulated by the sound wave.
Active transducers thus require phantom power, but can have higher sensitivity.
We can also classify microphones according to the physical property to which the
sound wave responds. A pressure microphone has an electrical response that corresponds to
the pressure in a sound wave, while a pressure gradient microphone has a response corre-
sponding to the difference in pressure across some distance in a sound wave. A pressure
microphone is a fine reproducer of sound, but a gradient microphone typically has a re-
sponse greatest in the direction of a desired signal or talker and rejects undesired back-
ground sounds. This is particularly beneficial in applications that rely upon the reproduction
of only a desired signal, where any undesired signal entering the reproduction severely de-
grades performance. Such is the case in voice recognition or speakerphone applications.
In terms of the mechanism by which they create an electrical signal corresponding to
the sound wave they detect, microphones are classified as electromagnetic, electrostatic, and
piezoelectric. Dynamic microphones are the most popular type of electromagnetic micro-
phone and condenser microphones the most popular type of electrostatic microphone.
Electromagnetic microphones induce voltage based on a varying magnetic field. Rib-
bon microphones are a type of electromagnetic microphones that employ a thin metal ribbon
suspended between the poles of a magnet. Dynamic microphones are electromagnetic mi-
crophones that employ a moving coil suspended by a light diaphragm (see Figure 10.16),
acting like a speaker but in reverse. The diaphragm moves with changes in sound pressure,
which in turns moves the coil, which causes current to flow as lines of flux from the magnet
are cut. Dynamic microphones need no batteries or power supply, but they deliver low sig-
nal levels that need to be preamplified.
Figure 10.16 Dynamic microphone schematics.
Piezoresistive and piezoelectric microphones are based on the variation of electric re-
sistance of their sensor induced by changes in sound pressure. Carbon button microphones
Output
voltage
Magnet
Coil
Diaphragm

Adaptive Echo Cancellation (AEC)
493
consist of a small cylinder packed with tiny granules of carbon that, when compacted by
sound pressure, reduce the electric resistance. Such microphones, often used in telephone
handsets, offer a worse frequency response than condenser microphones, and lower dynamic
range.
10.3.
ADAPTIVE ECHO CANCELLATION (AEC)
If a spoken language system allows the user to talk while speech is being output through the
loudspeakers, the microphone picks up not only the user’s voice, but also the speech from
the loudspeaker. This problem may be avoided with a half-duplex system that does not listen
when a signal is being played through the loudspeaker, though such systems offer an unnatu-
ral user experience. On the other hand, a full-duplex system that allows barge-in by the user
to interrupt the system offers a better user experience. For barge-in to work, the signal
played through the loudspeaker needs to be canceled. This is achieved with echo cancella-
tion (see Figure 10.17), as discussed in this section.
In hands-free conferencing the local user’s voice is output by the remote loudspeaker,
whose signal is captured by the remote microphone and after some delay is output by the
local loudspeaker. People are tolerant to these echoes if either they are greatly attenuated or
the delay is short. Perceptual studies have shown that the longer the delay, the greater the
attenuation needed for user acceptance.
Figure 10.17 Block diagram of an echo-canceling application. x[n] represents the signal from
the loudspeaker, s[n] the speech signal, v[n] the local background noise, and e[n] the signal
that goes to the microphone.
The use of echo cancellation is mandatory in telephone communications and hands-
free conferencing when it is desired to have full-duplex voice communication. This is par-
ticularly important when the call is routed through a satellite that can have delays larger than
200 ms. A block diagram is shown in Figure 10.18.
In Figure 10.17, the return signal r[n] is the sum
[ ]
[ ]
[ ]
r n
d n
s n
=
+
(10.33)
where s[n] is the speech signal and d[n] is the attenuated and possibly distorted version of
the loudspeaker’s signal x[n]. The purpose of the echo canceler is to remove the echo d[n]
from the return signal r[n], which is done by means of an adaptive FIR filter whose coeffi-
Adaptive
filter
Acoustic
path H
x[n]
s[n]
r[n]
Loudspeaker
e[n]
Speech
signal
Microphone
+
+
v[n]
Local
noise
d[n]
ˆ[ ]
d n

494
Environmental Robustness
cients are computed to minimize the energy of the canceled signal e[n]. The filter coeffi-
cients are reestimated adaptively to track slowly changing line conditions.
Figure 10.18 Block diagram of echo canceling for a telephone communication. x[n] represents
the remote call signal, s[n] the local outgoing signal. The hybrid circuit H does a 2-4 wire con-
version and is nonideal because of impedance mismatches.
This problem is essentially that of adaptive filtering only when
[ ]
0
s n =
, or in other
words when the user is silent. For this reason, you have to implement a double-talk detection
module that detects when the speaker is silent. This is typically feasible because the echo
d[n] is usually small, and if the return signal r[n] has high energy it means that the user is
not silent. Errors in double-talk detection result in divergence of the filter, so it is generally
preferable to be conservative in the decision and when in doubt not adapt the filter coeffi-
cients. Initialization could be done by sending a known signal with white spectrum.
The quality of the filtering is measured by the so-called echo-return loss enhancement
(ERLE):
2
10
2
{
[ ]}
(
)
10log
ˆ
{( [ ]
[ ]) }
E d
n
ERLE dB
E
d n
d n
=
−
(10.34)
The filter coefficients are chosen to maximize the ERLE. Since the telephone-line
characteristics, or the acoustic path (due to speaker movement), can change over time, the
filter is often adaptive. Another reason for adaptive filters is that reliable ERLE maximiza-
tion requires a large number of samples, and such a delay is not tolerable.
In the following sections, we describe the fundamentals of adaptive filtering. While
there are some nonlinear adaptive filters, the vast majority are linear FIR filters, with the
LMS algorithm being the most important. We introduce the LMS algorithm, study its con-
vergence properties, and present two extensions: the normalized LMS algorithm and trans-
form-domain LMS algorithms.
10.3.1.
The LMS Algorithm
Let’s assume that a desired signal d[n] is generated from an input signal x[n] as follows
1
0
[ ]
[
]
[ ]
[ ]
[ ]
L
T
k
k
d n
g x n
k
u n
n
u n
−
=
=
−
+
=
+

G X
(10.35)
Adaptive
filter
Hybrid
circuit H
x[n]
s[n]
r[n]
Speaker
A
e[n]
Speaker
B
ˆ[ ]
d n
+
+
v[n]
d[n]
Noise

Adaptive Echo Cancellation (AEC)
495
with
0
1
1
{
,
,
}
L
g
g
g −
=
G

, the input signal vector
[ ]
{ [ ], [
1],
[
1]}
n
x n x n
x n
L
=
−
−
+
X

, and
u[n] being noise that is independent of x[n].
We want to estimate d[n] in terms of the sum of previous samples of x[n]. To do that
we define the estimate signal y[n] as
1
0
[ ]
[ ] [
]
[ ] [ ]
L
T
k
k
y n
w n x n
k
n
n
−
=
=
−
=

W
X
(10.36)
where
0
1
1
[ ]
{
[ ],
[ ],
[ ]}
L
n
w n w n
w
n
−
=
W

is the time-dependent coefficient vector. The instan-
taneous error between the desired and the estimated signal is given by
[ ]
[ ]
[ ] [ ]
T
e n
d n
n
n
=
−W
X
(10.37)
The least mean square (LMS) algorithm updates the value of the coefficient vector in
the steepest descent direction
[
1]
[ ]
[ ] [ ]
n
n
e n
n
ε
+
=
+
W
W
X
(10.38)
where ε is the step size. This algorithm is very popular because of its simplicity and effec-
tiveness [58].
10.3.2.
Convergence Properties of the LMS Algorithm
The choice of ε is important: if it is too small, the adaptation rate will be slow and it might
not even track the nonstationary trends of
[ ]
x n , whereas if ε is too large, the error might
actually increase. We analyze the conditions under which the LMS algorithm converges.
Let’s define the error in the coefficient vector
[ ]
n
V
as
[ ]
[ ]
n
n
=
−
V
G
W
(10.39)
and combine Eqs. (10.37), (10.38), and (10.39) to obtain
[
1]
[ ]
[ ]
[ ] [ ]
[ ] [ ]
T
n
n
n
n
n
u n
n
ε
ε
+
=
−
−
V
V
X
X
V
X
(10.40)
Taking expectations in Eq. (10.40) results in
{ [
1]}
{ [ ]}
{ [ ]
[ ] [ ]}
T
E
n
E
n
E
n
n
n
µ
+
=
−
V
V
X
X
V
(10.41)
where we have assumed that u[n] and x[n] are independent and that either is a zero-mean
process. Finally, we express the autocorrelation of
[ ]
n
X
as
{ [ ]
[ ]}
T
T
xx
E
n
n
=
=
Λ
R
X
X
Q Q
(10.42)
where Q is a matrix of its eigenvectors and Λ is a diagonal matrix of its eigenvalues
0
1
1
{
,
,
,
}
L
λ λ
λ −

, which are all real valued because of the symmetry of
xx
R
.

496
Environmental Robustness
Although we know that
[ ]
n
X
and
[ ]
n
V
are not statistically independent, we assume
in this section that they are, so that we can obtain some insight on the convergence proper-
ties. With this assumption, Eq. (10.41) can be expressed as
{ [
1]}
{ [ ]}(1
)
xx
E
n
E
n
ε
+
=
−
V
V
R
(10.44)
which, applied recursively, leads to
{ [
1]}
{ [0]}(1
)n
xx
E
n
E
ε
+
=
−
V
V
R
(10.45)
Using Eqs. (10.39) and (10.42) in (10.45), we can express the (I + 1)th element of
{
[ ]}
E
n
W
as
1
0
{
[ ]}
(1
)
{ [0]}
L
n
i
i
ij
j
i
j
E w n
g
q
E v
ελ
−
=
=
+
−


(10.46)
where
ij
q is the (I + 1, j + 1)th element of the eigenvector matrix Q, and
[ ]
iv n

is the rotated
coefficient error vector defined as
[ ]
[ ]
T
n
n
=
V
Q V

(10.47)
From Eq. (10.46) we see that the mean value of the LMS filter coefficients converges
exponentially to the true value if
0
1/
j
ε
λ
<
<
(10.48)
so that the adaptation constant ε must be determined from the largest eigenvalue of
[ ]
n
X
for the mean LMS algorithm to converge.
In practice, mean convergence doesn’t tell us the nature of the fluctuations that the co-
efficients experience. Analysis of the variance of
[ ]
n
V
together with some more
approximations result in mean-squared convergence if
2
0
x
K
L
ε
σ
<
<
(10.49)
with
2
2
{
[ ]}
x
E x n
σ
=
being the input signal power and K a constant that depends weakly on
the nature of the input signal statistics but not on its power.
Because of the inaccuracies of the independence assumptions above, a rule of thumb
used in practice to determine the adaptation constant ε is
2
0.1
0
x
L
ε
σ
<
<
(10.50)
The choice of largest value for ε in Eq. (10.49) makes the LMS algorithm track non-
stationary variations in x fastest, and achieve faster convergence. On the other hand, the
misadjustment of the filter coefficients increases as both the filter length L and adaptation

Adaptive Echo Cancellation (AEC)
497
constant ε increase. For this reason, often the adaptation constant can be made a function of
n ( [ ]
n
ε
), with larger values at first and smaller values once convergence has been deter-
mined.
10.3.3.
Normalized LMS Algorithm
The normalized LMS algorithm (NLMS) uses the result of Eq. (10.49) and, therefore, de-
fines a normalized step size
2
[ ]
ˆ [ ]
x
n
L
n
ε
ε
δ
σ
=
+
(10.51)
where the constant δ avoids a division by 0 and
2ˆ [ ]
x n
σ
is an estimate of the input signal
power, which is typically done with an exponential window
2
2
2
ˆ
ˆ
[ ]
(1
)
[
1]
[ ]
x
x
n
n
x n
σ
β σ
β
=
−
−
+
(10.52)
or a sliding rectangular window
(
)
1
2
2
2
2
2
0
1
1
ˆ
ˆ
[ ]
[
]
[
1]
[ ]
[
]
N
x
x
i
n
x n
i
n
x n
x n
N
N
N
σ
σ
−
=
=
−
=
−
+
−
−

(10.53)
where both β and N control the effective memory of the estimators in Eqs. (10.52) and
(10.53), respectively. Finally, we need to pick ε so that 0
2
ε
<
<
to assure convergence.
Choice of the NLMS algorithm simplifies the selection of ε , and the NLMS often con-
verges faster than the LMS algorithm in practical situations.
10.3.4.
Transform-Domain LMS Algorithm
As discussed in Section 10.3.2, convergence of the LMS algorithm is determined by the
largest eigenvalue of the input. Since complex exponentials are approximate eigenvectors
for LTI systems, the LMS algorithm’s convergence is dominated by the frequency band with
largest energy, and convergence in other frequency bands is generally much slower. This is
the rationale for the subband LMS algorithm, which performs independent LMS algorithms
for different frequency bands, as proposed by Boll [14].
The block LMS (BLMS) algorithm keeps the coefficients unchanged for a block k of L
samples
1
0
[
1]
[ ]
[
] [
]
L
m
k
k
e kL
m
kL
m
ε
−
=
+
=
+
+
+

W
W
X
(10.54)
which is represented by a linear convolution and therefore can be implemented efficiently
using length-2N FFTs according to overlap-save method of Figure 10.19. Notice that im-

498
Environmental Robustness
plementing a linear convolution with a circular convolution operator such as the FFT re-
quires the use of the dashed box.
Figure 10.19 Block diagram of the constrained frequency-domain block LMS algorithm. The
unconstrained version of this algorithm eliminates the computation inside the dashed box.
An unconstrained frequency-domain LMS algorithm can be implemented by removing
the constraint in Figure 10.19, therefore implementing a circular instead of a linear convolu-
tion. While this is not exact, the algorithm requires only three FFTs instead of five. In some
practical applications, there is no difference in convergence between the constrained and
unconstrained cases.
10.3.5.
The RLS Algorithm
The search for the optimum filter can be accelerated when the gradient vector is properly
deviated toward the minimum. This approach uses the Newton-Raphson method to itera-
tively compute the root of f(x) (see Figure 10.20) so that the value at iteration i + 1 is given
by
Generate Length 2N
Data vector
Input
x[n]
FFT
x
FFT
Conjugate
Update weight
vector W
FFT
Save last
N samples
Force last N
elements to 0
IFFT
x
x
d[n]
-
+
FFT
Output y[n]
X[k]
E[k]
[Old x | New x]

Multimicrophone Speech Enhancement
499
1
( )
( )
i
i
i
i
f x
x
x
f
x
+ =
−
′
(10.55)
Figure 10.20 Newton-Raphson method to compute the roots of a function.
To minimize function f(x) we thus compute the roots of f’(x) through the above
method:
1
(
)
(
)
i
i
i
i
f
x
x
x
f
x
+
′
=
−
′′
(10.56)
In the case of a vector, Eq. (10.56) is transformed into
(
)
1
2
1
[ ]
(
)
(
)
i
i
i
i
n
e
e
ε
−
+ =
−
∇
∇
w
w
w
w
(10.57)
where we add a step size
[ ]
n
ε
, and where
2 (
)
i
e
∇
w
is the Hessian of the least-squares func-
tion which, for Eq. (10.37), equals the autocorrelation of x:
2 (
)
[ ]
{ [ ]
[ ]}
T
i
e
n
E
n
n
∇
=
=
w
R
x
x
(10.58)
The recursive least squares (RLS) algorithm specifies a method of estimating Eq.
(10.58) using an exponential window:
[ ]
[
1]
[ ]
[ ]
T
n
n
n
n
λ
=
−
+
R
R
x
x
(10.59)
While the RLS algorithm converges faster than the LMS algorithm, it also is more
computationally expensive, as it requires a matrix inversion for every sample. Several algo-
rithms have been derived to speed it up [54].
10.4.
MULTIMICROPHONE SPEECH ENHANCEMENT
The use of more than one microphone is motivated by the human auditory system, in which
the use of both ears has been shown to enhance detection of the direction of arrival, as well
as increase SNR when one ear is covered. The methods the human auditory system uses to
accomplish this task are still not completely known, and the techniques described in this
section do not mimic that behavior.
Microphone arrays use multiple microphones and knowledge of the microphone loca-
tions to predict delays and thus create a beam that focuses on the direction of the desired
x0
x1
f(x)

500
Environmental Robustness
speaker and rejects signals coming from other angles. Reverberation, as discussed in Section
10.1.2, can be combated with these techniques. Blind source separation techniques are an-
other family of statistical techniques that typically do not use spatial constraints, but rather
statistical independence between different sources.
While in this section we describe only linear processing, i.e., the output speech is a
linearly filtered version of the microphone signals, we could also combine these techniques
with the nonlinear methods of Section 10.5.
10.4.1.
Microphone Arrays
The goals of microphone arrays are twofold: finding the position of a sound source in a
room, and improving the SNR of the received signal. Steering is helpful in videoconferenc-
ing, where a camera has to follow the current speaker. Since the speaker is typically far
away from the microphone, the received signal likely contains a fair amount of additive
noise. Microphone arrays can also be used to increase the SNR.
Let x[n] be the signal at the source S. Microphone i picks up a signal
[ ]
[ ]
[ ]
[ ]
i
i
i
y n
x n
g n
v n
=
∗
+
(10.60)
that is a filtered version of the source plus additive noise
[ ]
iv n . If we have N such micro-
phones, we can attempt to recover s[n] because all the signals
[ ]
iy n should be correlated.
A typical assumption made is that all the filters
[ ]
ig n
are delayed versions of the
same filter g[n]
[ ]
[
]
i
i
g n
g n
D
=
−
(10.61)
with the delay
/
i
i
D
d
c
=
,
id being the distance between the source S and microphone i, and
c the speed of sound in air. We cannot recover signal x[n] without knowledge of g[n] or the
signal itself, so the goal is to obtain the filtered signal
[ ]
y n
[ ]
[ ]* [ ]
y n
x n
g n
=
(10.62)
so that, combining Eqs. (10.60), (10.61), and (10.62),
[ ]
[
]
[ ]
i
i
i
y n
y n
D
v n
=
−
+
(10.63)
Assuming
[ ]
iv n
are independent and Gaussianly distributed, the optimal estimate of
[ ]
x n is given by
1
0
1
[ ]
[
]
[ ]
[ ]
N
i
i
i
y n
y n
D
y n
v n
N
−
=
=
+
=
+


(10.64)
which is the so-called delay-and-sum beamformer [24, 29], where the residual noise v[n]

Multimicrophone Speech Enhancement
501
1
0
1
[ ]
[
]
N
i
i
i
v n
v n
D
N
−
=
=
+

(10.65)
has a variance that decreases as the number of microphones N increases, since the noises
[
]
i
i
v n
D
+
are uncorrelated.
Equation (10.65) requires estimation of the delays
i
D . To attenuate the additive noise
v[n], it is not necessary to identify the absolute delays, but rather the delays relative to one
reference microphone (for example, the center microphone). It can be shown that the maxi-
mum likelihood solution consists in maximizing the energy of
[ ]
y n

in Eq. (10.64), which is
the sum of cross-correlations:
1
1
0
0
arg max
[
]
0
i
N
N
i
ij
i
j
D
i
j
D
R D
D
i
N
−
−
=
=


=
−
≤<





(10.66)
This approach assumes that we know nothing about the geometry of the microphone
placement. In fact, given a point source and assuming no reflections, we can compute the
delay based on the distance between the source and the microphone. The use of geometry
allows us to reduce the number of parameters to estimate from (N - 1) to a maximum of 3, in
case we desire to estimate the exact location. This location is often described in spherical
coordinates ( , ,
)
ϕ θ ρ
with ϕ being the direction of arrival, θ the elevation angle, and ρ
the distance to the reference microphone, as shown in Figure 10.21.
Figure 10.21 Spherical coordinates (
)
, ,
ϕ θ ρ
with ϕ being the direction of arrival, θ the
elevation angle, and ρ the distance to the reference microphone.
While 2-D and 3-D microphone configurations can be used, which would allow us to
determine not just the steering angle ϕ , but also distance to the origin ρ and azimuth θ ,
linear microphone arrays are the most widely used configurations because they are the sim-
plest. In a linear array all the microphones are placed on a line (see Figure 10.22). In this
case, we cannot determine the elevation angle θ . To determine both ϕ and ρ we need at
least two microphones in the array.
If the microphones are relatively close to each other compared to the distance to the
source, the angle of arrival ϕ is approximately the same for all signals. With this assump-
tion, the normalized delay
i
D with respect to the reference microphone is given by
ϕ
θ
ρ
Microphone
Speaker

502
Environmental Robustness
sin( ) /
i
i
D
a
c
ϕ
= −
(10.67)
where
ia is the y-axis coordinate in Figure 10.22 for microphone i, where the reference mi-
crophone has
0
0
a =
and also
0
0
D =
.
Figure 10.22 Linear microphone array (five microphones). The source signal arrives at each
microphone with a different delay, which allows us to find the correct angle of arrival.
With approximation, we define
( )
i
D ϕ , the relative delay of the signal at microphone i
to the reference microphone, as a function of the direction of arrival angle ϕ and independ-
ent of ρ . The optimal direction of arrival ϕ is then that which maximizes the energy of the
estimated signal
[ ]
x n

over a set of samples
2
1
0
2
1
0
1
arg max
[
( )]
1
argmax
[
sin( )]
N
i
i
n
i
N
i
i
n
i
y n
D
N
y n
a
N
ϕ
ϕ
ϕ
ϕ
ϕ
−
=
−
=


=
+






=
−








(10.68)
The term beamforming entails that this array favors a specific direction of arrival ϕ
and that sources arriving from other directions are not in phase and therefore are attenuated.
Since the source can move over time, maximization of Eq. (10.68) can be done in an adap-
tive fashion.
As the beam is steered away from the broadside, the system exhibits a reduction in
spatial discrimination because the beam pattern broadens. Furthermore, beamwidth varies
with frequency, so an array has an approximate bandwidth given by the upper
uf
and lower
lf frequencies
,
max cos
cos
u
u
l
c
f
d
f
f
N
φ φ
ϕ
ϕ
′
=
′
−
=
(10.69)
M0
M1
M2
S
ϕ
M-2
M-1

Multimicrophone Speech Enhancement
503
with d being the sensor spacing, ϕ′ the steering angle measured with respect to the axis of
the array, and ϕ the direction of the source. For a desired range of
30
±
 and five sensors
spaced 5 cm apart, the range is approximately 880 to 4400 Hz. We see in Figure 10.23 that
at very low frequencies the response is essentially omnidirectional, since the microphone
spacing is small compared to the large wavelength. At high frequencies more lobes start
appearing, and the array steers toward not only the preferred direction but others as well.
For speech signals, the upshot is that we either need a lot of microphones to provide a direc-
tional polar pattern at low frequencies, or we need them to be spread far enough apart, or
both.
12.5
25
30
210
60
240
90
270
120
300
150
330
180
0
12.5
25
30
210
60
240
90
270
120
300
150
330
180
0
12.5
25
30
210
60
240
90
270
120
300
150
330
180
0
12.5
25
30
210
60
240
90
270
120
300
150
330
180
0
Figure 10.23 Polar pattern of a microphone array with steering angle of
0
ϕ′ =
, five micro-
phones spaced 5 cm apart for 400, 880, 4400, and 8000 Hz from left to right, respectively, for
a source located at 5 m.
The polar pattern in Figure 10.23 was computed as follows:
2
sin
|
| /
1
( , , )
j
i
i
j
f a
re
ja
c
N
j
i
i
e
P f r
re
ja
ϕ
π
ϕ
ϕ
ϕ


′
−
+
−


=
=
−

(10.70)
though the sensors could be spaced nonuniformly, as in Figure 10.24, allowing for better
behavior across the frequency spectrum.
Figure 10.24 Nonuniform linear microphone array containing three subarrays for the high,
mid, and low frequencies.
Mid-frequency array
High-frequency array
Low-frequency array

504
Environmental Robustness
Once a microphone array has been steered towars a direction ϕ′, it attenuates noise
source coming from other directions. The beamwidth depends not only on the frequency of
the signal, but also on the steering direction. If the beam is steered toward a direction ϕ′,
then the direction of the source for which the beam response fall to half its power has been
found empirically to be
1
3
( )
cos
cos
dB
K
f
Ndf
ϕ
ϕ
−

′
=
±




(10.71)
with K being a constant. Equation (10.71) shows that the smaller the array, the wider the
beam, and that lower frequencies yield wider beams also. Figure 10.25 shows that the band-
width of the array when steering toward a 30° direction is lower than when steering at 0°.
12.5
25
30
210
60
240
90
270
120
300
150
330
180
0
12.5
25
30
210
60
240
90
270
120
300
150
330
180
0
12.5
25
30
210
60
240
90
270
120
300
150
330
180
0
12.5
25
30
210
60
240
90
270
120
300
150
330
180
0
Figure 10.25 Polar pattern of a microphone array with steering angle of
30
ϕ′ =
 , five micro-
phones spaced 5 cm apart for 400, 880, 3000, and 4400 Hz from left to right, respectively, for
a source located at 5 m.
Microphone arrays have been shown to improve recognition accuracy when the mi-
crophones and the speaker are far apart [51]. Several companies are commercializing micro-
phone arrays for teleconferencing or speech recognition applications.
Only in anechoic chambers does the assumption in Eq. (10.61) hold, since in practice
many reflections take place, which are also different for different microphones. In addition,
the assumption of a common direction of arrival for all microphones may not hold either.
For this case of reverberant environments, single beamformers typically fail. While comput-
ing the direction of arrival is much more difficult in this case, the SNR can still be improved.
Let’s define the desired signal d[n] as that obtained in the reference microphone. We
can estimate the vector
11
1
21
2
(
1)1
(
1)
[ ]
{
,
,
,
,
,
,
,
,
,
}
L
L
N
N
L
n
h
h
h
h
h
h
−
−
=
H




for the (N-1) L-
tap filters that minimizes the error array [25]
[ ]
[ ]
[ ] [ ]
e n
d n
n
n
=
−H
Y
(10.72)
where the (N – 1) microphone signals are represented in the vector
1
1
2
2
1
1
[ ]
{ [ ],
,
[
1],
[ ],
,
[
1],
,
[ ],
,
[
1]}
N
N
n
y n
y n
L
y n
y n
L
y
n
y
n
L
−
−
=
−
−
−
−
−
−
Y




The filter coefficients
[ ]
n
G
can be estimated through the adaptive filtering techniques de-
scribed in Section 10.3. The clean signal is then estimated as

Multimicrophone Speech Enhancement
505
(
)
1
ˆ[ ]
[ ]
[ ] [ ]
2
x n
d n
n
n
=
+ H
Y
(10.73)
This last method does not assume anything about the geometry of the microphone ar-
ray.
10.4.2.
Blind Source Separation
The problem of separating the desired speech from interfering sources, the cocktail party
effect [15], has been one of the holy grails in signal processing. Blind source separation
(BSS) is a set of techniques that assume no information about the mixing process or the
sources, apart from their mutual statistical independence, hence is termed blind. Independent
component analysis (ICA), developed in the last few years [19, 38], is a set of techniques to
solve the BSS problem that estimate a set of linear filters to separate the mixed signals under
the assumption that the original sources are statistically independent.
Let’s first consider instantaneous mixing. Let’s assume that R microphone signals
[ ]
iy n , denoted by
(
)
1
2
[ ]
[ ],
[ ],
,
[ ]
R
n
y n
y n
y
n
=
y

, are obtained by a linear combination of R
unobserved source signals
[ ]
ix n , denoted by
(
)
1
2
[ ]
[ ],
[ ],
,
[ ]
R
n
x n x n
x
n
=
x

:
[ ]
[ ]
n
n
=
y
Gx
(10.74)
for all n, with G being the R x R mixing matrix. This mixing is termed instantaneous, since
the sensor signals at time n depend on the sources at the same, but no earlier, time point.
Had the mixing matrix been given, its inverse could have been applied to the sensor signals
to recover the sources by
1
[ ]
[ ]
n
n
−
=
x
G y
. In the absence of any information about the mix-
ing, the blind separation problem consists of estimating a separating matrix
1
−
=
H
G
from
the observed microphone signals alone. The source signals can then be recovered by
[ ]
[ ]
n
n
=
x
Hy
(10.75)
We’ll use here the probabilistic formulation of ICA, though alternate frameworks for
ICA have been derived also [18]. Let
( [ ])
p
n
x x
be the probability density function (pdf) of
the source signals, so that the pdf of microphone signals y[n] is given by
( [ ]) |
|
(
[ ])
p
n
p
n
=
y
x
y
H
Hy
(10.76)
and if we furthermore assume the sources x[n] are independent from themselves in time,
[
]
0
n
i
i
+
≠
x
, then the joint probability is given by
1
1
0
0
( [0], [1],
, [
1])
( [ ]) |
|
(
[ ])
N
N
N
n
n
p
N
p
n
p
n
−
−
=
=
−
=
=
∏
∏
y
y
x
y
y
y
y
H
Hy

(10.77)
whose normalized log-likelihood is given by

506
Environmental Robustness
1
0
1
1
ln
( [0], [1],
, [
1])
ln |
|
ln
(
[ ])
N
n
p
N
p
n
N
N
−
=
Ψ =
−
=
+

y
x
y
y
y
H
Hy

(10.78)
It can be shown that
(
)
1
ln
T
−
∂
=
∂
H
H
H
(10.79)
so that that the gradient of Ψ [38] in Eq. (10.78) is given by
(
)
(
)
1
1
0
1
(
[ ])
[ ]
N
T
T
n
n
n
N
φ
−
−
=
∂Ψ =
+
∂

H
Hy
y
H
(10.80)
where
( )
φ x
is expressed as
ln
( )
( )
p
φ
∂
=
∂
x x
x
x
(10.81)
If we further assume the distribution is a zero mean Gaussian distribution with stan-
dard deviation σ, then Eq. (10.81) results in
2
( )
φ
σ
= −x
x
(10.82)
which inserted into Eq. (10.80) yields
(
)
(
)
(
)
1
1
1
2
2
0
1
[ ]
[ ]
N
T
T
T
n
n
n
N
σ
σ
−
−
−
=
∂Ψ


=
−
=
−


∂



H
H
H
y
y
H
R
H
(10.83)
with R being the matrix of cross-correlations, i.e.,
1
0
1
[ ]
[ ]
N
ij
i
j
n
R
y n y n
N
−
=
=

(10.84)
Setting Eq. (10.83) to 0 results in maximization of Eq. (10.78) under the Gaussian as-
sumption:
2
1
T
σ
−
=
H H
R
(10.85)
which can be solved using the Cholesky decomposition described in Chapter 6.
Since σ is generally not known, it can be shown from Eq. (10.85) that the sources can
be recovered only to within a scaling factor [17]. Scaling is in general not a big problem,
since speech recognition systems perform automatic gain control (AGC). Moreover, the
sources can be recovered to within a permutation. To see this, let’s define a two-dimensional
matrix A

Multimicrophone Speech Enhancement
507
0
1
1
0


= 



A
(10.86)
which is orthogonal:
T
=
A A
I
(10.87)
If H is a solution of Eq. (10.85), then AH is also a solution. Thus, a permutation of the
sources yields the same correlation matrix in Eq. (10.84). Although we have shown it only
under the Gaussian assumption, separation up to a scaling and source permutation is a gen-
eral result in blind source separation [17].
Unfortunately, the Gaussian assumption does not guarantee separation. To see this, we
can define a two-dimensional rotation matrix A
cos
sin
sin
cos
θ
θ
θ
θ
−


= 



A
(10.88)
which is also orthogonal, so that if H is a solution of Eq. (10.85), then AH is also a solution.
The Gaussian assumption entails considering only second-order statistics, and to en-
sure separation we could consider higher-order statistics. Since speech signals do not follow
a Gaussian distribution, we could use a Laplacian distribution, as we saw in Chapter 7:
( )
2
x
xp
x
e
β
β
−
=
(10.89)
which, using Eq. (10.81), results in
0
( )
0
x
x
x
β
φ
β
−
>

= 
<

(10.90)
and thus a nonlinear function of H for Eq. (10.80). Since a closed-form solution is not possi-
ble, a common solution in this case is gradient descent, where the gradient is given by
(
)
1
(
[ ])( [ ])
T
T
n
n
n
n
n
φ
−
∂Ψ =
+
∂
H
H y
y
H
(10.91)
and the update formula by
(
)
1
1
(
[ ])( [ ])
T
T
n
n
n
n
n
n
n
n
ε
ε
φ
−
+
∂Ψ


=
−
=
−
+




∂
H
H
H
H
H y
y
H
(10.92)
which is the so-called infomax rule [10].
Often the nonlinearity in Eq. (10.90) is replaced by a sigmoid6 function:
6 The sigmoid function can be expressed in terms of the hyperbolic tangent tanh( )
sinh( )/ cosh( )
x
x
x
=
, where
sinh( )
(
)/ 2
x
x
x
e
e−
=
−
and cosh( )
(
)/ 2
x
x
x
e
e−
=
+
.

508
Environmental Robustness
( )
tanh(
)
x
x
φ
β
β
= −
(10.93)
which implies a density function
( )
2 cosh(
)
xp
x
x
β
π
β
=
(10.94)
The sigmoid converges to the Laplacian as β →∞. Nonlinear functions in Eqs.
(10.90) and (10.93) can be expanded in Taylor series so that all the moments of the observed
signals are used and not just the second order, as in the case of the Gaussian assumption.
These nonlinearities have been shown to be more effective in separating the sources. The
use of more accurate density functions for
( )
px x , such as a mixture of Gaussians [9], also
results in nonlinear
( )
φ x
functions that have shown better separation.
A problem of Eq. (10.92) is that it requires a matrix inversion at every iteration. The
so-called natural gradient [7] was suggested to avoid this, also providing faster conver-
gence. To do this we can multiply the gradient of Eq. (10.91) by a positive definite matrix,
the inverse of the Fisher’s information matrix
T
n
n
H H , for example, to whiten the signal:
1
T
n
n
n
n
ε
+
∂Ψ
=
−
∂
H
H
H H
H
(10.95)
which, combined with Eq. (10.91), results in
1
ˆ
ˆ
( [ ])( [ ])T
n
n
n
n
n
ε
φ
+


=
−
+


H
H
I
x
x
H
(10.96)
where the estimated sources are given by
ˆ[ ]
[ ]
n
n
n
=
x
H y
(10.97)
Notice the similarity of this approach to the RLS algorithm of Section 10.3.5. Similarly to
most Newton-Raphson methods, the convergence of this approach is quadratic instead of
linear as long as we are close enough to the maximum.
Another way of overcoming the lack of separation under the independent Gaussian as-
sumption is to make use of temporal information, which we know is important for speech
signals. If the model of Eq. (10.74) is extended to contain additive noise
[ ]
[ ]
[ ]
n
n
n
=
+
y
Gx
v
(10.98)
we can compute the autocorrelation of y[n] as
[ ]
[ ]
[ ]
T
n
n
n
=
+
y
x
v
R
GR
G
R
(10.99)
or, after some manipulation,
[ ]
(
[ ]
[ ])
T
n
n
n
=
−
x
y
v
R
H R
R
H
(10.100)

Multimicrophone Speech Enhancement
509
which we know must be diagonal because the sources x are independent, and thus H can be
estimated to minimize the squared error of the off-diagonal terms of
[ ]
n
x
R
for several val-
ues of n [11]. Equation (10.100) is a generalization of Eq. (10.85) when considering tempo-
ral correlation and additive noise.
Figure 10.26 Convolutional model for the case of two microphones.
The case of instantaneous mixing is not realistic, as we need to consider the transfer
functions between the sources and the microphones created by the room acoustics. It can be
shown that the reconstruction filters
[ ]
ijh n in Figure 10.26 will completely recover the origi-
nal signals
[ ]
ix n
if and only if their z-transforms are the inverse of the z-transforms of the
mixing filters
[ ]
ij
g
n :
1
11
12
11
12
21
22
21
22
11
12
21
22
11
22
12
21
( )
( )
( )
( )
( )
( )
( )
( )
( )
( )
1
( )
( )
( )
( )
( )
( )
H
z
H
z
G
z
G
z
H
z
H
z
G
z
G
z
G
z
G
z
G
z
G
z
G
z G
z
G
z G
z
−




=










=


−


(10.101)
If the matrix in Eq. (10.101) is not invertible, separability is impossible. This can hap-
pen if both microphones pick up the same signal, which could happen if either the two mi-
crophones are too close to each other or the two sources are too close to each other. It’s rea-
sonable to assume the mixing filters
[ ]
ij
g
n
to be FIR filters, whose length will generally
depend on the reverberation time, which in turn depends on the room size, microphone posi-
tion, wall absorbance, and so on. In general this means that the reconstruction filters
[ ]
ijh n
have an infinite impulse response. In addition, the filters
[ ]
ij
g
n
may have zeroes outside the
unit circle, so that perfect reconstruction filters would need to have poles outside the unit
circle. For this reason it is not possible, in general, to recover the original signals exactly.
In practice, it’s convenient to assume such filters to be FIR of length q, which means
that the original signals
1[ ]
x n
and
2[ ]
x n , will not be recovered exactly. Thus the problem
1[ ]
y n
2[ ]
y n
1[ ]
x n
2[ ]
x n
2ˆ [ ]
x n
1ˆ [ ]
x n
11[ ]
g
n
+
+
12[ ]
g
n
21[ ]
g
n
22[ ]
g
n
11[ ]
h
n
+
+
12[ ]
h
n
21[ ]
h
n
22[ ]
h
n

510
Environmental Robustness
consists in estimating the reconstruction filters
[ ]
ijh n
directly from the microphone signals
1[ ]
y n
and
2[ ]
y n , so that the estimated signals ˆ [ ]
ix n
are as close as possible to the original
signals. Often we are satisfied if the resulting signals are separated, even if they contain
some amount of reverberation.
An approach commonly used to combat this problem consists of taking a filterbank
and assuming instantaneous mixing within each filter [38]. This approach can separate real
sources much more effectively, but it suffers from the problem of permutations, which in
this case is more severe because frequencies from different sources can be mixed together.
To avoid this, we may need a probabilistic model of the sources that takes into account cor-
relations across frequencies [3]. Another problem occurs when the number of sources is lar-
ger than the number of microphones.
10.5.
ENVIRONMENT COMPENSATION PREPROCESSING
The goal of this section is to present a number of techniques used to clean up the signal of
additive noise and/or channel distortions prior to the speech recognition system. Although
the techniques presented here are developed for the case of one microphone, they can be
generalized to the case where several microphones are available using the approaches de-
scribed in Section 10.4. These techniques can also be used to enhance the signal captured
with a speakerphone or a desktop microphone in teleconferencing applications.
Since the use of human auditory system is so robust to changes in acoustical environ-
ment, many researchers have attempted to develop signal processing schemes that mimic the
functional organization of the peripheral auditory system [27, 49]. The PLP cepstrum de-
scribed in Chapter 6 has also been shown to be very effective in combating noise and chan-
nel distortions [60].
Another alternative is to consider the feature vector as an integral part of the recog-
nizer, and thus researchers have investigated its design so as to maximize recognition accu-
racy, as discussed in Chapter 9. Such approaches include LDA [34] and neural networks
[45]. These discriminatively trained features can also be optimized to operate better under
noisy conditions, thus possibly beating the standard mel-cepstrum, especially when several
independent features are combined [50]. The mel-cepstrum is the most popular feature vec-
tor for speech recognition. In this context we present a number of techniques that have been
proposed over the years to compensate for the effects of additive noise and channel distor-
tions on the cepstrum.
10.5.1.
Spectral Subtraction
The basic assumption in this section is that the desired clean signal x[m] has been corrupted
by additive noise n[m]:
[ ]
[ ]
[ ]
y m
x m
n m
=
+
(10.102)

Environment Compensation Preprocessing
511
and that both x[m] and n[m] are statistically independent, so that the power spectrum of the
output y[m] can be approximated as the sum of the power spectra:
2
2
2
( )
( )
( )
Y f
X f
N f
≈
+
(10.103)~
with equality if we take expected values, as the expected value of the cross term vanishes
(see Section 10.1.3).
Although we don’t know
2
( )
N f
, we can obtain an estimate using the average perio-
dogram over M frames that are known to be just noise (i.e., when no signal is present) as
long as the noise is stationary
1
2
2
0
1
ˆ ( )
( )
M
i
i
N f
Y f
M
−
=
=

(10.104)
Spectral subtraction supplies an intuitive estimate for
( )
X f
using Eqs. (10.103) and
(10.104) as
2
2
2
2
1
ˆ
ˆ
( )
( )
( )
( )
1
( )
X f
Y f
N f
Y f
SNR f


=
−
=
−




(10.105)
where we have define the frequency-dependent signal-to-noise ratio
( )
SNR f
as
2
2
( )
( )
ˆ ( )
Y f
SNR f
N f
=
(10.106)
Equation (10.105) describes the magnitude of the Fourier transform but not the phase.
This is not a problem if we are interested in computing the mel-cepstrum as discussed in
Chapter 6. We can just modify the magnitude and keep the original phase of
( )
Y f
using a
filter
( )
ss
H
f :
ˆ ( )
( )
( )
ss
X f
Y f H
f
=
(10.107)
where, according to Eq. (10.105),
( )
ss
H
f
is given by
1
( )
1
( )
ss
H
f
SNR f
=
−
(10.108)
Since
2
ˆ ( )
X f
is a power spectral density, it has to be positive, and therefore
( )
1
SNR f
≥
(10.109)

512
Environmental Robustness
but we have no guarantee that
( )
SNR f , as computed by Eq. (10.106), satisfies Eq. (10.109).
In fact, it is easy to see that noise frames do not comply. To enforce this constraint, Boll [13]
suggested modifying Eq. (10.108) as follows:
1
( )
max 1
,
( )
ss
H
f
a
SNR f


=
−




(10.110)
with
0
a ≥
, so that the quantity within the square root is always positive, and where
( )
ssf
x is
given by
1
( )
max 1
,
ssf
x
a
x


=
−




(10.111)
It is useful to express
( )
SNR f
in dB so that
10
10log
x
SNR
=
(10.112)
and the gain of the filter in Eq. (10.111) also in dB:
10
( )
20log
( )
ss
ss
g
x
f
x
=
(10.113)
Using Eqs. (10.111) and (10.112), we can express Eq. (10.113) by
(
)
(
)
/10
10
( )
max 10log
1 10
,
x
ss
g
x
A
−
=
−
−
(10.114)
after expressing the attenuation a in dB:
/10
10 A
a
−
=
(10.115)
Equation (10.114) is plotted in Figure 10.27 for A = 10 dB.
The spectral subtraction rule in Eq. (10.111) is quite intuitive. To implement it we can
do a short-time analysis, as shown in Chapter 6, by using overlapping windowed segments,
zero-padding, computing the FFT, modifying the magnitude spectrum, taking the inverse
FFT, and adding the resulting windows.
This implementation results in output speech that has significantly less noise, though it
exhibits what is called musical noise [12]. This is caused by frequency bands f for which
2
2
ˆ
( )
( )
Y f
N f
≈
. As shown in Figure 10.27, a frequency
0f
for which
2
2
0
0
ˆ
(
)
(
)
Y f
N f
<
is attenuated by A dB, whereas a neighboring frequency
1f , where
2
2
1
1
ˆ
(
)
(
)
Y f
N f
>
, has a
much smaller attenuation. These rapid changes with frequency introduce tones at varying
frequencies that appear and disappear rapidly.
The main reason for the presence of musical noise is that the estimates of
( )
SNR f
through Eqs. (10.104) and (10.106) are poor. This is partly because
( )
SNR f
is computed
independently for each frequency, whereas we know that
0
(
)
SNR f
and
1
(
)
SNR f
are corre-

Environment Compensation Preprocessing
513
lated if
0f
and
1f are close to each other. Thus, one possibility is to smooth the filter in Eq.
(10.114) over frequency. This approach suppresses a smaller amount of noise, but it does not
distort the signal as much, and thus may be preferred by listeners. Similarly, smoothing over
time
2
2
( )
( , )
( ,
1)
(1
)
ˆ ( )
Y f
SNR f t
SNR f t
N f
γ
γ
=
−
+
−
(10.116)
can also be done to reduce the distortion, at the expense of a smaller noise attenuation.
Smoothing over both time and frequency can be done to obtain more accurate SNR meas-
urements and thus less distortion. As shown in Figure 10.28, use of spectral subtraction can
reduce the error rate.
-5
0
5
10
15
20
-12
-10
-8
-6
-4
-2
0
Instantaneous SNR (dB)
Gain(dB)
spectral subtraction
magnitude subtraction
Oversubtraction
Figure 10.27 Magnitude of the spectral subtraction filter gain as a function of the input
instantaneous SNR for A = 10 dB, for the spectral subtraction of Eq. (10.114), magnitude
subtraction of Eq. (10.118), and oversubtraction of Eq. (10.119) with β = 2 dB.
Additionally, the attenuation A can be made a function of frequency. This is useful
when we want to suppress more noise at one frequency than another, which is a tradeoff
between noise reduction and nonlinear distortion of speech.
Other enhancements to the basic algorithm have been proposed to reduce the musical
noise. Sometimes Eq. (10.111) is generalized to
1/
/ 2
1
( )
max 1
,
ms
f
x
a
x
α
α




=
−








(10.117)
where
2
α =
corresponds to the power spectral subtraction rule in Eq. (10.111), and
1
α =
corresponds to the magnitude subtraction rule (plotted in Figure 10.27 for A = 10 dB):

514
Environmental Robustness
(
)
(
)
/ 5
10
( )
max 20log
1 10
,
x
ms
g
x
A
−
=
−
−
(10.118)
Another variation, called oversubtraction, consists of multiplying the estimate of the
noise power spectral density
2
ˆ ( )
N f
in Eq. (10.104) by a constant
/10
10β
, where
0
β >
,
which causes the power spectral subtraction rule in Eq. (10.114) to be transformed to an-
other function
(
)
(
)
(
) /10
10
( )
max 10log
1 10
,
x
os
g
x
A
β
−
−
=
−
−
(10.119)
This causes
2
2
ˆ
( )
( )
Y f
N f
<
to occur more often than
2
2
ˆ
( )
( )
Y f
N f
>
for frames for
which
2
2
ˆ
( )
( )
Y f
N f
≈
, and thus reduces the musical noise.
0
10
20
30
40
50
60
70
80
90
100
0
5
10
15
20
25
30
SNR (dB)
Word Error Rate (%)
Clean Speech Training
Spectral Subtraction
Matched Noisy Training
Figure 10.28 Word error rate as a function of SNR (dB) using Whisper on the Wall Street
Journal 5000-word dictation task. White noise was added at different SNRs, and the system
was trained with speech with the same SNR. The solid line represents the baseline system
trained with clean speech, the dotted line the use of spectral subtraction with the previous clean
HMMs. They are compared to a system trained on the same speech with the same SNR as the
speech tested on.
10.5.2.
Frequency-Domain MMSE from Stereo Data
You have seen that several possible functions, such as Eqs. (10.114), (10.118), or (10.119),
can be used to attenuate the noise, and it is not clear that any one of them is better than the
others, since each has been obtained through different assumptions. This opens the possibil-
ity of estimating the curve
( )
g x
using a different criterion, and, thus, different approxima-
tions than those used in Section 10.5.1.

Environment Compensation Preprocessing
515
Figure 10.29 Empirical curves for input-to-output instantaneous SNR. Eight different curves
for 0, 1, 2, 3, 4, 5, 6, 7 and 8 kHz are obtained following Eq. (10.121) [2] using speech re-
corded simultaneously from a close-talking microphone and a desktop microphone.
One interesting possibility occurs when we have pairs of stereo utterances that have
been recorded simultaneously in noise-free conditions in one channel and noisy conditions
in the other channel. In this case, we can estimate
( )
f x
using a minimum mean squared
criterion (Porter and Boll [47], Ephraim and Malah [23]), so that
(
)
(
)
1
1
2
( )
0
0
ˆ( )
argmin
(
)
(
)
(
)
N
M
i
j
j
i
j
f x
i
j
f x
X
f
f SNR f
Y f
−
−
=
=


=
−





(10.120)
or g(x) as
(
)
(
)
1
1
2
2
2
10
10
( )
0
0
ˆ( )
arg min
10log
(
)
(
)
10log
(
)
N
M
i
j
j
i
j
g x
i
j
g x
X
f
g SNR f
Y f
−
−
=
=


=
−
−





(10.121)

516
Environmental Robustness
which can be solved by discretizing f(x) and g(x) into several bins and summing over all M
frequencies and N frames. This approach results in a curve that is smoother and thus results
in less musical noise and lower distortion. Stereo utterances of noise-free and noisy speech
are needed to estimate f(x) and g(x) through Eqs. (10.120) and (10.121) for any given acous-
tical environment and can be collected with two microphones, or the noisy speech can be
obtained by adding to the clean speech artificial noise from the testing environment.
Another generalization of this approach is to use a different function f(x) or g(x) for
every frequency [2] as shown in Figure 10.29. This also allows for a lower squared error at
the expense of having to store more data tables. In the experiments of Figure 10.29, we note
that more subtraction is needed at lower frequencies than at higher frequencies in this case.
If such stereo data is available to estimate these curves, it makes the enhanced speech
sound better [23] than does spectral subtraction. When used in speech recognition systems, it
also leads to higher accuracies [2].
10.5.3.
Wiener Filtering
Let’s reformulate Eq. (10.102) from the statistical point of view. The process
[ ]
n
y
is the
sum of random process
[ ]
n
x
and the additive noise
[ ]
n
v
process:
[ ]
[ ]
[ ]
n
n
n
=
+
y
x
v
(10.122)
We wish to find a linear estimate ˆ[ ]
n
x
in terms of the process
[ ]
n
y
:
ˆ[ ]
[ ] [
]
m
n
h m
n
m
∞
=−∞
=
−

x
y
(10.123)
which is the result of a linear time-invariant filtering operation. The MMSE estimate of
[ ]
h n
in Eq. (10.123) minimizes the squared error
2
[ ]
[ ] [
]
m
E
n
h m
n
m
∞
=−∞






−
−



	







x
y
(10.124)
which results in the famous Wiener-Hopf equation
[ ]
[ ]
[
]
xy
yy
m
R
l
h m R
l
m
∞
=−∞
=
−

(10.125)
so that, taking Fourier transforms, the resulting filter can be expressed in the frequency do-
main as
( )
( )
( )
xy
yy
S
f
H f
S
f
=
(10.126)
If the signal
[ ]
n
x
and the noise
[ ]
n
v
are orthogonal, which is often the case, then

Environment Compensation Preprocessing
517
( )
( )
xy
xx
S
f
S
f
=
and
( )
( )
( )
yy
xx
vv
S
f
S
f
S
f
=
+
(10.127)
so that Eq. (10.126) is given by
( )
( )
( )
( )
xx
xx
vv
S
f
H f
S
f
S
f
=
+
(10.128)
Equation (10.128) is called the noncausal Wiener filter. This can be realized only if
we know the power spectra of both the noise and the signal. Of course, if
( )
xx
S
f
and
( )
vv
S
f
do not overlap, then
( )
1
H f
=
in the band of the signal and
( )
0
H f
=
in the band
of the noise.
In practice,
( )
xx
S
f
is unknown. If it were known, we could compute its mel-cepstrum,
which would coincide exactly with the mel-cepstrum before noise addition. To solve this
chicken-and-egg problem, we need some kind of model. Ephraim [22] proposed the use of
an HMM where, if we know what state the current frame falls under, we can use its mean
spectrum as
( )
xx
S
f . In practice we do not know what state each frame falls into either, so
he proposed to weigh the filters for each state by the a posterior probability that the frame
falls into each state. This algorithm, when used in speech enhancement, results in gains of
15 dB or more.
A causal version of the Wiener filter can also be derived. A dynamical state model al-
gorithm called the Kalman filter (see [42] for details) is also an extension of the Wiener fil-
ter.
10.5.4.
Cepstral Mean Normalization (CMN)
Different microphones have different transfer functions, and even the same microphone has
a varying transfer function depending on the distance to the microphone and the room
acoustics. In this section we describe a powerful and simple technique that is designed to
handle convolutional distortions and, thus, increases the robustness of speech recognition
systems to unknown linear filtering operations.
Given a signal x[n], we compute its cepstrum through short-time analysis, resulting in
a set of T cepstral vectors
0
1
1
{
,
,
,
}
T −
=
X
x
x
x

. Its sample mean x is given by
1
0
1 T
t
t
T
−
=
= 
x
x
(10.129)
Cepstral mean normalization (CMN) (Atal [8]) consists of subtracting x from each vector
tx to obtain the normalized cepstrum vector ˆ tx :
ˆ t
t
=
−
x
x
x
(10.130)

518
Environmental Robustness
Let’s now consider a signal y[n], which is the output of passing x[n] through a filter
h[n]. We can compute another sequence of cepstrum vectors
0
1
1
{
,
,
,
}
T −
=
Y
y
y
y

. Now
let’s further define a vector h as
(
)
2
2
0
ln
(
)
ln
(
)
M
H
H
ω
ω
=
h
C

(10.131)
where C is the DCT matrix. We can see that
t
t
=
+
y
x
h
(10.132)
and thus the sample mean
ty equals
1
1
0
0
1
1
(
)
T
T
t
t
t
t
T
T
−
−
=
=
=
= +
+
=
+


y
y
x
h
x
h
(10.133)
and its normalized cepstrum is given by
ˆ
ˆ
t
t
t
t
=
−
=
y
y
y
x
(10.134)
which indicates that cepstral mean normalization is immune to linear filtering operations.
This procedure is performed on every utterance for both training and testing. Intuitively, the
mean vector x conveys the spectral characteristics of the current microphone and room
acoustics. In the limit, when T →∞for each utterance, we should expect means from utter-
ances from the same recording environment to be the same. Use of CMN to the cepstrum
vectors does not modify the delta or delta-delta cepstrum.
Let’s analyze the effect of CMN on a short utterance. Assume that our utterance con-
tains a single phoneme, say /s/. The mean x will be very similar to the frames in this pho-
neme, since /s/ is quite stationary. Thus, after normalization, ˆ
0
t ≈
x
. A similar result will
happen for other fricatives, which means that it would be impossible to distinguish these
ultrashort utterances, and the error rate will be very high. If the utterance contains more than
one phoneme but is still short, this problem is not insurmountable, but the confusion among
phonemes is still higher than if no CMN had been applied. Empirically, it has been found
that this procedure does not degrade the recognition rate on utterances from the same acous-
tical environment, as long as they are longer than 2–4 seconds. Yet the method provides
significant robustness against linear filtering operations. In fact, for telephone recordings,
where each call has a different frequency response, the use of CMN has been shown to pro-
vide as much as 30% relative decrease in error rate. When a system is trained on one micro-
phone and tested on another, CMN can provide significant robustness.
Interestingly enough, it has been found in practice that the error rate for utterances
within the same environment is actually somewhat lower, too. This is surprising, given that
there is no mismatch in channel conditions. One explanation is that, even for the same mi-
crophone and room acoustics, the distance between the mouth and the microphone varies for
different speakers, which causes slightly different transfer functions, as we studied in Sec-
tion 10.2. In addition, the cepstral mean characterizes not only the channel transfer function,

Environment Compensation Preprocessing
519
but also the average frequency response of different speakers. By removing the long-term
speaker average, CMN can act as sort of speaker normalization.
0
2
4
6
8
10
12
14
16
10
15
20
30
SNR (dB)
Word Error Rate (%)
No CMN
CMN-2
Figure 10.30 Word error rate as a function of SNR (dB) for both no CMN (solid line) and
CMN-2 [5] (dotted line). White noise was added at different SNRs and the system was trained
with speech with the same SNR. The Whisper system is used on the 5000-word Wall Street
Journal task using a bigram language model.
One drawback of CMN is it does not discriminate silence and voice in computing the
utterance mean. An extension to CMN consists in computing different means for noise and
speech [5]:
(
1)
(
1)
1
1
s
n
j
t
t q
s
j
t
t q
n
N
N
+
∈
+
∈
=
−
=
−


s
n
h
x
m
n
x
m
(10.135)
i.e., the difference between the average vector for speech frames in the utterance and the
average vector
s
m for speech frames in the training data, and similarly for the noise frames
n
m . Speech/noise discrimination could be done by classifying frames into speech frames
and noise frames, computing the average cepstra for each, and subtracting them from the
average in the training data. This procedure works well as long as the speech/noise classifi-
cation is accurate. It’s best done by the recognizer, since other speech detection algorithms
can fail in high background noise (see Section 10.6.2). As shown in Figure 10.30, this algo-
rithm has been shown to improve robustness not only to varying channels but also to noise.

520
Environmental Robustness
10.5.5.
Real-Time Cepstral Normalization
CMN requires the complete utterance to compute the cepstral mean; thus, it cannot be used
in a real-time system, and an approximation needs to be used. In this section we discuss a
modified version of CMN that can address this problem, as well as a set of techniques called
RASTA that attempt to do the same thing.
We can interpret CMN as the operation of subtracting a low-pass filter d[n], where all
the T coefficients are identical and equal 1/T , which is a high-pass filter with a cutoff fre-
quency
c
ω
that is arbitrarily close to 0. This interpretation indicates that we can implement
other types of high-pass filters. One that has been found to work well in practice is the ex-
ponential filter, so the cepstral mean
tx is a function of time
1
(1
)
t
t
t
α
α
−
=
+
−
x
x
x
(10.136)
where α is chosen so that the filter has a time constant7 of at least 5 seconds of speech.
Other types of filters have been proposed in the literature. In fact, a popular approach
consists of an IIR bandpass filter with the transfer function:
1
3
4
4
1
2
2
( )
0.1
*
1 0.98
z
z
z
H z
z
z
−
−
−
−
+
−
−
=
−
(10.137)
which is used in the so-called relative spectral processing or RASTA [32]. As in CMN, the
high-pass portion of the filter is expected to alleviate the effect of convolutional noise intro-
duced in the channel. The low-pass filtering helps to smooth some of the fast frame-to-frame
spectral changes present. Empirically, it has been shown that the RASTA filter behaves
similarly to the real-time implementation of CMN, albeit with a slightly higher error rate.
Both the RASTA filter and real-time implementations of CMN require the filter to be prop-
erly initialized. Otherwise, the first utterance may use an incorrect cepstral mean. The origi-
nal derivation of RASTA includes a few stages prior to the bandpass filter, and this filter is
performed on the spectral energies, not the cepstrum.
10.5.6.
The Use of Gaussian Mixture Models
Algorithms such as spectral subtraction of Section 10.5.1 or the frequency-domain MMSE
of Section 10.5.2 implicitly assume that different frequencies are uncorrelated from each
other. Because of that, the spectrum of the enhanced signal may exhibit abrupt changes
across frequency and not look like spectra of real speech signals. Using the model of the
environment of Section 10.1.3, we can express the clean-speech cepstral vector x as a func-
tion of the observed noisy cepstral vector y as
7 The time constant τ of a low-pass filter is defined as the value for which the output is cut in half. For an exponen-
tial filter of parameter α and sampling rate Fs,
ln 2/(
)
s
TF
α =
.

Environment Compensation Preprocessing
521
(
)
1(
)
ln 1 e
−
−
=
−
−
−
C
n y
x
y
h
C
(10.138)
where the noise cepstral vector n is a random vector. The MMSE estimate of x is given by
(
)
{
}
1 (
)
ˆ
{ | }
ln 1
|
MMSE
E
E
e
−
−
=
=
−
−
−
C
n y
x
x y
y
h
C
y
(10.139)
where the expectation uses the distribution of n. Solution to Eq. (10.139) results in a nonlin-
ear function which can be learned, for example, with a neural network [53].
A popular model to attack this problem consists in modeling the probability distribu-
tion of the noisy speech y as a mixture of K Gaussians:
1
1
0
0
( )
( | ) [ ]
( ,
,
) [ ]
K
K
k
k
k
k
p
p
k P k
P k
−
−
=
=
=
=
Ν


y
y
y µ
Σ
(10.140)
where P[k] is the prior probability of each Gaussian component k. If x and y are jointly
Gaussian within class k, then
( | , )
p
k
x y
is also Gaussian [42] with mean:
(
)
1
{ | , }
(
)
k
k
k
k
k
k
E
k
−
=
+
−
=
+
x
xy
y
y
x y
µ
Σ
Σ
y
µ
C y
r
(10.141)
so that the joint distribution of x and y is given by
1
1
0
0
1
0
( , )
( ,
| ) [ ]
( | , ) ( | ) [ ]
( ,
,
)
( ,
,
) [ ]
K
K
k
k
K
k
k
k
k
k
k
p
p
k P k
p
k p
k P k
P k
−
−
=
=
−
=
=
=
=
Ν
+
Ν



x y
x y
x y
y
x C y
r Γ
y µ
Σ
(10.142)
where
kr
is called the correction vector,
k
C
is the rotation matrix, and the matrix
k
Γ
tells
us how uncertain we are about the compensation.
A maximum likelihood estimate of x maximizes the joint probability in Eq. (10.142).
Assuming the Gaussians do not overlap very much (as in the FCDCN algorithm [2]):
ˆ
argmax
( , , )
argmax
( ,
,
)
( ,
,
) [ ]
ML
k
k
k
k
k
k
k
p
k
P k
≈
=
Ν
Ν
+
x
x y
y µ
Σ
x C y
r Γ
(10.143)
whose solution is
ˆ
ˆ
ˆ ML
k
k
=
+
x
C y
r
(10.144)
where
ˆ
argmax
( ,
,
) [ ]
k
k
k
k
P k
=
Ν y µ
Σ
(10.145)
It is often more robust to compute the MMSE estimate of x (as in the CDCN [2] and
RATZ [43] algorithms):

522
Environmental Robustness
{
}
(
)
1
1
0
0
ˆ
{ | }
( | )
| ,
( | )
K
K
MMSE
k
k
k
k
E
p k
E
k
p k
−
−
=
=
=
=
=
+


x
x y
y
x y
y
C y
r
(10.146)
as a weighted sum for all mixture components, where the posterior probability
( | )
p k y
is
given by
1
0
( | ) [ ]
( | )
( | ) [ ]
K
k
p
k P k
p k
p
k P k
−
=
=

y
y
y
(10.147)
where the rotation matrix
k
C
in Eq. (10.144) can be replaced by I with a modest degrada-
tion in performance in return for faster computation [21].
A number of different algorithms [2, 43] have been proposed that vary in how the pa-
rameters
k
µ ,
k
Σ ,
kr , and
k
Γ
are estimated. If stereo recordings are available from both the
clean signal and the noisy signal, then we can estimate
k
µ ,
k
Σ by fitting a mixture Gaussian
model to y as described in Chapter 3. Then
k
C ,
kr
and
k
Γ
can be estimated directly by
linear regression of x and y. The FCDCN algorithm [2, 6] is a variant of this approach when
it is assumed that
2
k
σ
=
Σ
I ,
2
k
γ
=
Γ
I , and
k =
C
I , so that
k
µ
and
kr
are estimated
through a VQ procedure and
kr is the average difference (
)
−
y
x for vectors y that belong to
mixture component k. An enhancement is to use the instantaneous SNR of a frame, defined
as the difference between the log-energy of that frame and the average log-energy of the
background noise. It is advantageous to use different correction vectors for different instan-
taneous SNR levels. The log-energy can be replaced by the zeroth-order cepstral coefficient
with little change in recognition accuracy. It is also possible to estimate the MMSE solution
instead of picking the most likely codeword (as in the RATZ algorithm [43]). The resulting
correction vector is a weighted average of the correction vectors for all classes.
Often, stereo recordings are not available and we need other means of estimating pa-
rameters
k
µ ,
k
Σ ,
kr , and
k
Γ . CDCN [6] is one such algorithm that has a model of the en-
vironment as described in Section 10.1.3, which defines a nonlinear relationship between x,
y and the environmental parameters n and h for the noise and channel. This method also
uses an MMSE approach where the correction vector is a weighted average of the correction
vectors for all classes. An extension of CDCN using a vector Taylor series approximation
[44] for that nonlinear function has been shown to offer improved results. Other methods
that do not require stereo recordings or a model of the environment are presented in [43].
10.6.
ENVIRONMENTAL MODEL ADAPTATION
We describe a number of techniques that achieve compensation by adapting the HMM to the
noisy conditions. The most straightforward method is to retrain the whole HMM with the
speech from the new acoustical environment. Another option is to apply standard adaptive
techniques discussed in Chapter 9 to the case of environment adaptation. We consider a

Environmental Model Adaptation
523
model of the environment that allows constrained adaptation methods for more efficient
adaptation in comparison to the general techniques.
10.6.1.
Retraining on Corrupted Speech
If there is a mismatch between acoustical environments, it is sensible to retrain the HMM.
This is done in practice for telephone speech where only telephone speech, and no clean
high-bandwidth speech, is used in the training phase.
Unfortunately, training a large-vocabulary speech recognizer requires a very large
amount of data, which is often not available for a specific noisy condition. For example, it is
difficult to collect a large amount of training data in a car driving at 50 mph, whereas it is
much easier to record it at idle speed. Having a small amount of matched-conditions training
data could be worse than a large amount of mismatched-conditions training data. Often we
want to adapt our model given a relatively small sample of speech from the new acoustical
environment.
0
20
40
60
80
100
0
5
10
15
20
25
30
SNR (dB)
Word Error Rate (%)
Mismatched
Matched (Noisy)
Figure 10.31 Word error rate as a function of the testing data SNR (dB) for Whisper trained
on clean data (solid line) and a system trained on noisy data at the same SNR as the testing set
(dotted line). White noise at different SNRs is added.
One option is to take a noise waveform from the new environment, add it to all the ut-
terances in our database, and retrain the system. If the noise characteristics are known ahead
of time, this method allows us to adapt the model to the new environment with a relatively
small amount of data from the new environment, yet use a large amount of training data.
Figure 10.31 shows the benefit of this approach over a system trained on clean speech for
the case of additive white noise. If the target acoustical environment also has a different
channel, we can also filter all the utterances in the training data prior to retraining. This
method allows us to adapt the model to the new environment with a relatively small amount
of data from the new environment.
If the noise sample is available offline, this simple technique can provide good results
at no cost during recognition. Otherwise the noise addition and model retraining would need
to occur at runtime. This is feasible for speaker-dependent small-vocabulary systems where

524
Environmental Robustness
the training data can be kept in memory and where the retraining time can be small, but it is
generally not feasible for large-vocabulary speaker-independent systems because of memory
and computational limitations.
0
5
10
15
20
25
30
5
10
15
20
25
30
SNR (dB)
Word Error Rate (%)
Matched Noise
Multistyle
Figure 10.32 Word error rates of multistyle training compared to matched-noise training as a
function of the SNR in dB for additive white noise. Whisper is trained as in Figure 10.30. The
error rate of multistyle training is between 12% (for low SNR) and 25% (for high SNR) higher
in relative terms than that of matched-condition training. Nonetheless, multistyle training does
better than a system trained on clean data for all conditions other than clean speech.
One possibility is to create a number of artificial acoustical environments by corrupt-
ing our clean database with noise samples of varying levels and types, as well as varying
channels. Then all those waveforms from multiple acoustical environments can be used in
training. This is called multistyle training [39], since our training data represents a different
condition. Because of the diversity of the training data, the resulting recognizer is more ro-
bust to varying noise conditions. In Figure 10.32 we see that, though generally the error-rate
curve is above the matched-condition curve, particularly for clean speech, multistyle training
does not require knowledge of the specific noise level and thus is a viable alternative to the
theoretical lower bound of matched conditions.
10.6.2.
Model Adaptation
We can also use the standard adaptation methods used for speaker adaptation, such as MAP
or MLLR described in Chapter 9. Since MAP is an unstructured method, it can offer results
similar to those of matched conditions, but it requires a significant amount of adaptation
data. MLLR can achieve reasonable performance with about a minute of speech for minor
mismatches [41]. For severe mismatches, MLLR also requires a large number of transforma-
tions, which, in turn, require a larger amount of adaptation data as discussed in Chapter 9.
Let’s analyze the case of a single MLLR transform, where the affine transformation is
simply a bias. In this case the MLLR transform consists only of a vector h that, as in the
case of CMN described in Section 10.5.4, can be estimated from a single utterance. Instead
of estimating h as the average cepstral mean, this method estimates h as the maximum like-

Environmental Model Adaptation
525
lihood estimate, given a set of sample vectors
0
1
1
{
,
,
,
}
T −
=
X
x
x
x

and an HMM model λ
[48], and it is a version of the EM algorithm where all the vector means are tied together
(see Algorithm 10.2). This procedure for estimating the cepstral bias has a very slight reduc-
tion in error rates over CMN, although the improvement is larger for short utterances [48].
ALGORITHM 10.2 MLE SIGNAL BIAS REMOVAL
Step 1: Initialize
(0) =
h
0 at iteration
0
j =
Step 2: Obtain model
( )j
λ
by updating the means from
k
m
to
( )j
k +
m
h
, for all Gaussians k.
Step 3: Run recognition with model
( )j
λ
on the current utterance and determine a state seg-
mentation
[ ]t
θ
for each frame t.
Step 4: Estimate
(
1)
j+
h
as the vector that maximizes the likelihood, which, using covariance
matrices
k
Σ , is given by:
(
)
1
1
1
(
1)
1
1
[ ]
[ ]
[ ]
0
0
T
T
j
t
t
t
t
t
t
θ
θ
θ
−
−
−
+
−
−
=
=


=
−






h
Σ
Σ
x
m
(10.148)
Step 5: If converged, stop; otherwise, increment j and go to Step 2. In practice two iterations
are often sufficient.
If both additive noise and linear filtering are applied, the cepstrum for the noise and
that for most speech frames are affected differently. The speech/noise mean normalization
[5] algorithm can be extended similarly, as shown in Algorithm 10.3. The idea is to estimate
a vector n and h , such that all the Gaussians associated to the noise model are shifted by
n , and all remaining Gaussians are shifted by h .
We can make Eq. (10.150) more efficient by tying all the covariance matrices. This
transforms Eq. (10.150) into
(
1)
(
1)
1
1
s
n
j
t
t q
s
j
t
t q
n
N
N
+
∈
+
∈
=
−
=
−


s
n
h
x
m
n
x
m
(10.149)
i.e., the difference between the average vector for speech frames in the utterance and the
average vector
s
m for speech frames in the training data, and similarly for the noise frames
n
m . This is essentially the same equation as in the speech-noise cepstral mean normaliza-
tion described in Section 10.5.4. The difference is that the speech/noise discrimination is
done by the recognizer instead of by a separate classifier. This method is more accurate in
high-background-noise conditions where traditional speech/noise classifiers can fail. As a
compromise, a codebook with considerably fewer Gaussians than a recognizer can be used
to estimate n and h .

526
Environmental Robustness
ALGORITHM 10.3 SPEECH/NOISE MEAN NORMALIZATION
Step 1: Initialize
(0) =
h
0 ,
(0) =
n
0 at iteration
0
j =
Step 2: Obtain model
( )j
λ
by updating the means of speech Gaussians from
k
m
to
( )j
k +
m
h
, and of noise Gaussians from
l
m to
( )j
l +
m
n
.
Step 3: Run recognition with model
( )j
λ
on the current utterance and determine a state seg-
mentation
[ ]t
θ
for each frame t.
Step 4: Estimate
(
1)
j+
h
and
(
1)
j+
n
as the vectors that maximize the likelihood for speech
frames (
s
t
q
∈
) and noise frames (
n
t
q
∈
), respectively:
(
)
(
)
1
(
1)
1
1
[ ]
[ ]
[ ]
1
(
1)
1
1
[ ]
[ ]
[ ]
s
s
n
n
j
t
t
t
t
t q
t q
j
t
t
t
t
t q
t q
θ
θ
θ
θ
θ
θ
−
+
−
−
∈
∈
−
+
−
−
∈
∈


=
−






=
−








h
Σ
Σ
x
m
n
Σ
Σ
x
m
(10.150)
Step 5: If converged, stop; otherwise, increment j and go to
Step 2
10.6.3.
Parallel Model Combination
By using the clean-speech models and a noise model, we can approximate the distributions
obtained by training a HMM with corrupted speech. The memory requirements for the algo-
rithm are then significantly reduced, as the training data is not needed online. Parallel model
combination (PMC) is a method to obtain the distribution of noisy speech given the distribu-
tion of clean speech and noise as mixture of Gaussians. As discussed in Section 10.1.3, if the
clean-speech cepstrum follows a Gaussian distribution and the noise cepstrum follows an-
other Gaussian distribution, the noisy speech has a distribution that is no longer Gaussian.
The PMC method nevertheless makes the assumption that the resulting distribution is Gaus-
sian whose mean and covariance matrix are the mean and covariance matrix of the resulting
non-Gaussian distribution. If it is assumed that the distribution of clean speech is a mixture
of N Gaussians, and the distribution of the noise is a mixture of M Gaussians, the distribu-
tion of the noisy speech contains NM Gaussians. The feature vector is often composed of the
cepstrum, delta cepstrum, and delta-delta cepstrum. The model combination can be seen in
Figure 10.33.
If the mean and covariance matrix of the cepstral noise vector n are given by
c
n
µ
and
c
n
Σ , respectively, we first compute the mean and covariance matrix in the log-spectral do-
main:

Environmental Model Adaptation
527
1
1
1
(
)
l
c
l
c
T
−
−
−
=
=
n
n
n
n
µ
C µ
Σ
C Σ
C
(10.151)
Figure 10.33 Parallel model combination for the case of one-state noise HMM.
In the linear domain
e
=
n
N
, the distribution is lognormal, whose mean vector
N
µ
and
covariance matrix
N
Σ
can be shown (see Chapter 3) to be given by
{
}
{
}
(
)
[ ]
exp
[ ]
[ , ]/ 2
[ , ]
[ ]
[ ] exp
[ , ]
1
l
l
l
i
i
i i
i j
i
j
i j
=
+
=
−
N
n
n
N
N
N
n
µ
µ
Σ
Σ
µ
µ
Σ
(10.152)
with expressions similar to Eqs. (10.151) and (10.152) for the mean and covariance matrix
of X.
Using the model of the environment with no filter is equivalent to obtaining a random
linear spectral vector Y given by (see Figure 10.33)
=
+
Y
X
N
(10.153)
and, since X and N are independent, we can obtain the mean and covariance matrix of Y as
C-1
exp ( )
+
log ( )
C-1
C-1
exp ( )
Clean-speech
HMM
Noise HMM
Noisy speech
HMM
Log-spectral domain
Cepstral domain
Linear domain
Cepstral domain
Log-spectral domain
g

528
Environmental Robustness
=
+
=
+
Y
X
N
Y
X
N
µ
µ
µ
Σ
Σ
Σ
(10.154)
Although the sum of two lognormal distributions is not lognormal, the popular log-
normal approximation [26] consists in assuming that Y is lognormal. In this case we can
apply the inverse formulae of Eq. (10.152) to obtain the mean and covariance matrix in the
log-spectral domain:
[ , ]
[ , ]
ln
1
[ ]
[ ]
[ , ]
1
[ ]
ln
[ ]
ln
1
2
[ ]
[ ]
l
l
i j
i j
i
j
i j
i
i
i
j


=
+






=
−
+




Y
y
Y
Y
Y
y
Y
Y
Y
Σ
Σ
µ
µ
Σ
µ
µ
µ
µ
(10.155)
and finally return to the cepstrum domain applying the inverse of Eq. (10.151):
c
l
c
l
T
=
=
y
y
y
y
µ
Cµ
Σ
CΣ C
(10.156)
The lognormal approximation cannot be used directly for the delta and delta-delta cep-
strum. Another variant that can be used in this case and is more accurate than the lognormal
approximation is the data-driven parallel model combination (DPMC) [26], which uses
Monte Carlo simulation to draw random cepstrum vectors from both the clean-speech HMM
and noise distribution to create cepstrum of the noisy speech by applying Eqs. (10.20) and
(10.21) to each sample point. These composite cepstrum vectors are not kept in memory,
only their means and covariance matrices are, therefore reducing the required memory
though still requiring a significant amount of computation. The number of vectors drawn
from the distribution was at least 100 in [26]. A way of reducing the number of random vec-
tors needed to obtain good Monte Carlo simulations is proposed in [56]. A version of PMC
using numerical integration, which is very computationally expensive, yielded the best re-
sults.
Figure 10.34 and Figure 10.35 compare the values estimated through the lognormal
approximation to the true value, where for simplicity we deal with scalars. Thus x, n, and y
represent the log-spectral energies of the clean signal, noise, and noisy signal, respectively,
for a given frequency. Assuming x and n to be Gaussian with means
x
µ
and
n
µ
and vari-
ances
x
σ
and
x
σ
respectively, we see that the lognormal approximation is accurate when
the standard deviations
x
σ
and
n
σ
are small.
10.6.4.
Vector Taylor Series
The model of the acoustical environment described in Section 10.1.3 describes the relation-
ship between the cepstral vectors x, n, and y of the clean speech, noise, and noisy speech,
respectively:

Environmental Model Adaptation
529
(
)
=
+
+
−
−
y
x
h
g n
x
h
(10.157)
where h is the cepstrum of the filter, and the nonlinear function g(z) is given by
(
)
1
( )
ln 1
e
−
=
+
C z
g z
C
(10.158)
Moreno [44] suggests the use of Taylor series to approximate the nonlinearity in Eq.
(10.158), though he applies it in the spectral instead of the cepstral domain. We follow that
approach to compute the mean and covariance matrix of y [4].
Assume that x, h, and n are Gaussian random vectors with means
x
µ ,
h
µ , and
n
µ
and
covariance matrices
x
Σ ,
h
Σ , and
n
Σ , respectively, and furthermore that x, h, and n are
independent. After algebraic manipulation it can be shown that the Jacobian of Eq. (10.157)
with respect to x, h, and n evaluated at
=
−
−
n
x
h
µ
µ
µ
µ can be expressed as
(
,
,
)
(
,
,
)
(
,
,
)
n
x
h
n
x
h
n
x
h
∂
∂
=
=
∂
∂
∂
=
−
∂
µ
µ
µ
µ
µ
µ
µ
µ
µ
y
y
A
x
h
y
I
A
n
(10.159)
where the matrix A is given by
1
−
=
A
CFC
(10.160)
and F is a diagonal matrix whose elements are given by vector
( )
f µ , which in turn is given
by
1
1
( )
1
e
−
=
+
C µ
f µ
(10.161)
Using Eq. (10.159) we can then approximate Eq. (10.157) by a first-order Taylor se-
ries expansion around (
,
,
)
n
x
h
µ
µ
µ
as
(
)
(
)
(
)
(
)(
)
x
h
n
x
h
x
h
n
≈
+
+
−
−
+
−
+
−
+
−
−
y
µ
µ
g µ
µ
µ
A x
µ
A h
µ
I
A n
µ
(10.162)
The mean of y,
y
µ , can be obtained from Eq. (10.162) as
(
)
y
x
h
n
x
h
≈
+
+
−
−
µ
µ
µ
g µ
µ
µ
(10.163)
and its covariance matrix
y
Σ by
(
)
(
)
T
T
T
≈
+
+
−
−
y
x
h
n
Σ
AΣ A
AΣ A
I
A Σ
I
A
(10.164)

530
Environmental Robustness
so that even if
x
Σ ,
h
Σ , and
n
Σ are diagonal,
y
Σ is no longer diagonal. Nonetheless, we can
assume it to be diagonal, because this way we can transform a clean HMM to a corrupted
HMM that has the same functional form and use a decoder that has been optimized for di-
agonal covariance matrices.
It is difficult to visualize how good the approximation is, given the nonlinearity in-
volved. To provide some insight, let’s consider the frequency-domain version of Eqs.
(10.157) and (10.158) when no filtering is done:
(
)
(
)
ln 1
exp
y
x
n
x
=
+
+
−
(10.165)
where x, n, and y represent the log-spectral energies of the clean signal, noise, and noisy
signal, respectively, for a given frequency. In Figure 10.34 we show the mean and standard
deviation of the noisy log-spectral energy y in dB as a function of the mean of the clean log-
spectral energy x with a standard deviation of 10 dB. The log-spectral energy of the noise n
is Gaussian with mean 0 dB and standard deviation 2 dB. We compare the correct values
obtained through Monte Carlo simulation (or DPMC) with the values obtained through the
lognormal approximation of Section 10.6.3 and the first-order VTS approximation described
here. We see that the VTS approximation is more accurate than the lognormal approxima-
tion for the mean and especially for the standard deviation of y, assuming the model of the
environment described by Eq. (10.165).
-20
-10
0
10
20
0
5
10
15
20
25
x mean (dB)
y mean (dB)
Montecarlo
1st order VTS
PMC
-20
-10
0
10
20
0
2
4
6
8
10
12
x mean (dB)
y std dev (dB)
Montecarlo
1st order VTS
PMC
Figure 10.34 Means and standard deviation of noisy speech y in dB according to Eq. (10.165).
The distribution of the noise log-spectrum n is Gaussian with mean 0 dB and standard devia-
tion 2 dB. The distribution of the clean log-spectrum x is Gaussian, having a standard deviation
of 10 dB and a mean varying from –25 to 25 dB. Both the mean and the standard deviation of y
are more accurate in first-order VTS than in PMC.
Figure 10.35 is similar to Figure 10.34 except that the standard deviation of the clean
log-energy x is only 5 dB, a more realistic number in speech recognition systems. In this
case, both the lognormal approximation and the first-order VTS approximation are good
estimates of the mean of y, though the standard deviation estimated through the lognormal

Environmental Model Adaptation
531
approximation in PMC is not as good as that obtained through first-order VTS, again assum-
ing the model of the environment described by Eq. (10.165). The overestimate of the vari-
ance in the lognormal approximation might, however, be useful if the model of the environ-
ment is not accurate.
-20
-10
0
10
20
0
5
10
15
20
25
x mean (dB)
y mean (dB)
Montecarlo
1st order VTS
PMC
-20
-10
0
10
20
0
1
2
3
4
5
6
x mean (dB)
y std dev (dB)
Montecarlo
1st order VTS
PMC
Figure 10.35 Means and standard deviation of noisy speech y in dB according to Eq. (10.165).
The distribution of the noise log-spectrum n is Gaussian with mean 0 dB and standard devia-
tion of 2 dB. The distribution of the clean log-spectrum x is Gaussian with a standard deviation
of 5 dB and a mean varying from –25 dB to 25 dB. The mean of y is well estimated in both
PMC and first-order VTS. The standard deviation of y is more accurate in first-order VTS than
in PMC.
To compute the means and covariance matrices of the delta and delta-delta parameters,
let’s take the derivative of the approximation of y in Eq. (10.162) with respect to time:
t
t
∂
∂
≈
∂
∂
y
x
A
(10.166)
so that the delta-cepstrum computed through
2
2
t
t
t
+
−
∆
=
−
x
x
x
, is related to the derivative
[28] by
4
t
t
∂
∆≈
∂
x
x
(10.167)
so that
∆
∆
≈
y
x
µ
Aµ
(10.168)
and similarly
(
)
(
)
T
T
∆
∆
∆
≈
+
−
−
y
x
n
Σ
AΣ A
I
A Σ
I
A
(10.169)
where we assumed that h is constant within an utterance, so that
0
∆
=
h
.
Similarly, for the delta-delta cepstrum, the mean is given by

532
Environmental Robustness
2
2
∆
∆
≈
y
x
µ
Aµ
(10.170)
and the covariance matrix by
2
2
2
(
)
(
)
T
T
∆
∆
∆
≈
+
−
−
y
x
n
Σ
AΣ
A
I
A Σ
I
A
(10.171)
where we again assumed that h is constant within an utterance, so that
2
0
∆
=
h
.
Equations (10.163), (10.168), and (10.170) resemble the MLLR adaptation formulae
of Chapter 9 for the means, though in this case the matrix is different for each Gaussian and
is heavily constrained.
We are interested in estimating the environmental parameters
n
µ ,
h
µ , and
n
Σ , given
a set of T observation frames
ty . This estimation can be done iteratively using the EM algo-
rithm on Eq. (10.162). If the noise process is stationary,
∆n
Σ
could be approximated, assum-
ing independence between
2
t+
n
and
2
t−
n
, by
2
∆=
n
n
Σ
Σ . Similarly,
2
∆n
Σ
could be ap-
proximated, assuming independence between
1
t+
∆n
and
1
t−
∆n
, by
2
4
∆
=
n
n
Σ
Σ . If the noise
process is not stationary, it is best to estimate
∆n
Σ
and
2
∆n
Σ
from input data directly.
If the distribution of x is a mixture of N Gaussians, each Gaussian is transformed ac-
cording to the equations above. If the distribution of n is also a mixture of M Gaussians, the
composite distribution has NM Gaussians. While this increases the number of Gaussians, the
decoder is still functionally the same as for clean speech. Because normally you do not want
to alter the number of Gaussians of the system when you do noise adaptation, it is often as-
sumed that n is a single Gaussian.
10.6.5.
Retraining on Compensated Features
We have discussed adapting the HMM to the new acoustical environment using the standard
front-end features, in most cases the mel-cepstrum. Section 10.5 dealt with cleaning the
noisy feature without retraining the HMMs. It’s logical to consider a combination of both,
where the features are cleaned to remove noise and channel effects and then the HMMs are
retrained to take into account that this processing stage is not perfect. This idea is illustrated
in Figure 10.36, where we compare the word error rate of the standard matched-noise-
condition training with the matched-noise-condition training after it has been compensated
by a variant of the mixture Gaussian algorithms described in Section 10.5.6 [21].
The low error rates of both curves in Figure 10.36 are hard to obtain in practice, be-
cause they assume we know exactly what the noise level and type are ahead of time, which
in general is hard to do. On the other hand, this could be combined with the multistyle train-
ing discussed in Section 10.6.1 or with a set of clustered models discussed in Chapter 9.

Modeling Nonstationary Noise
533
0
5
10
15
20
25
30
5
10
15
20
25
30
SNR (dB)
Word Error Rate (%)
SPLICE-processed matched condition
Unprocessed matched condition
Figure 10.36 Word error rates of matched-noise training without feature preprocessing and
with the SPLICE algorithm [21] as a function of the SNR in dB for additive white noise. Error
rate with the mixture Gaussian model is up to 30% lower than that of standard noisy matched
conditions for low SNRs while it is about the same for high SNRs.
10.7.
MODELING NONSTATIONARY NOISE
The previous sections deal mostly with stationary noise. In practice, there are many nonsta-
tionary noises that often match a random word in the system’s lexicon better than the silence
model. In this case, the benefit of using speech recognition vanishes quickly.
The most typical types of noise present in desktop applications are mouth noise (lip
smacks, throat clearings, coughs, nasal clearings, heavy breathing, uhms and uhs, etc), com-
puter noise (keyboard typing, microphone adjustment, computer fan, disk head seeking,
etc.), and office noise (phone rings, paper rustles, shutting door, interfering speakers, etc.).
We can use a simple method that has been successful in speech recognition [57] as shown in
Algorithm 10.4.
In practice, updating the transcription turns out to be important, because human label-
ers often miss short noises that the system can uncover. Since the noise training data are
often limited in terms of coverage, some noises can be easily matched to short word models,
such as: if, two. Due to the unique characteristics of noise rejection, we often need to further
augment confidence measures such as those described in Chapter 9. In practice, we need an
additional classifier to provide more detailed discrimination between speech and noise. We
can use a two-level classifier for this purpose. The ratio between the all-speech model score
(fully connected context-independent phone models) and the all-noise model score (fully
connected silence and noise phone models) can be used.
Another approach [55] consists of having an HMM for noise with several states to
deal with nonstationary noises. A decoder needs to be a three-dimensional Viterbi search
which evaluates at each frame every possible speech state as well as every possible noise

534
Environmental Robustness
state to achieve the speech/noise decomposition (see Figure 10.37). The computational com-
plexity of such an approach is very large, though it can handle nonstationary noises quite
well in theory.
ALGORITHM 10.4 EXPLICIT NOISE MODELING
Step 1: Augmenting the vocabulary with noise words (such as ++SMACK++), each composed
of a single noise phoneme (such as +SMACK+), which are thus modeled with a single HMM.
These noise words have to be labeled in the transcriptions so that they can be trained.
Step 2: Training noise models, as well as the other models, using the standard HMM training
procedure.
Step 3: Updating the transcription. To do that, convert the transcription into a network, where
the noise words can be optionally inserted between each word in the original transcription. A
forced alignment segmentation is then conducted with the current HMM optional noise words
inserted. The segmentation with the highest likelihood is selected, thus yielding an optimal tran-
scription.
Step 4: If converged, stop; otherwise go to Step 2.
Figure 10.37 Speech noise decomposition and a three-dimensional Viterbi decoder.
10.8.
HISTORICAL PERSPECTIVE AND FURTHER READING
This chapter contains a number of diverse topics that are often described in different fields;
no single reference covers it all. For further reading on adaptive filtering, you can check the
books by Widrow and Stearns [59] and Haykin [30]. Theodoridis and Bellanger provide [54]
a good summary of adaptive filtering, and Breining et al. [16] a good summary of echo-
canceling techniques. Lee [38] has a good summary of independent component analysis for
blind source separation. Deller et al. [20] provide a number of techniques for speech en-
hancement. Juang [35] and Junqua [37] survey techniques used in improving the robustness
Observations
Speech
HMM
Noise
HMM

Historical Perspective and Further Reading
535
of speech recognition systems to noise. Acero [2] compares a number of feature transforma-
tion techniques in the cepstral domain and introduces the model of the environment.
Adaptive filtering theory emerged early in the 1900s. The Wiener and LMS filters
were derived by Wiener and Widrow in 1919 and 1960, respectively. Norbert Wiener joined
the MIT faculty in 1919 and made profound contributions to generalized harmonic analysis,
the famous Wiener-Hopf equation, and the resulting Wiener filter. The LMS algorithm was
developed by B Widrow and his colleagues at Stanford University in the early 1960s.
From a practical point of view, the use of gradient microphones (Olsen [46]) has
proven to be one of the more important contributions to increased robustness. Directional
microphones are commonplace today in most speech recognition systems.
Boll [13] first suggested the use of spectral subtraction. This has been the cornerstone
for noise suppression, and many systems nowadays still use a variant of Boll’s original algo-
rithm.
The Cepstral mean normalization algorithm was proposed by Atal [8] in 1974, al-
though it wasn’t until the early 1990s that it became commonplace in most speech recogni-
tion systems evaluated in the DARPA speech programs [33]. Hermansky proposed PLP [31]
in 1990. The work of Rich Stern’s robustness group at CMU (especially the Ph.D. thesis
work of Acero [1] and Moreno [43]) and the Ph.D. thesis of Gales [26] also represented ad-
vances in the understanding of the effect of noise in the cepstrum.
Bell and Sejnowski [10] gave the field of independent component analysis a boost in
1995 with their infomax rule. The field of source separation is a promising alternative to
improve the robustness of speech recognition systems when more than one microphone is
available.
REFERENCES
[1]
Acero, A., Acoustical and Environmental Robustness in Automatic Speech Recognition, PhD
Thesis in Electrical and Computer Engineering 1990, Carnegie Mellon University, Pitts-
burgh.
[2]
Acero, A., Acoustical and Environmental Robustness in Automatic Speech Recognition,
1993, Boston, Kluwer Academic Publishers.
[3]
Acero, A., S. Altschuler, and L. Wu, "Speech/Noise Separation Using Two Microphones and
a VQ Model of Speech Signals," Int. Conf. on Spoken Language Processing, 2000, Beijing,
China.
[4]
Acero, A., et al., "HMM Adaptation Using Vector Taylor Series for Noisy Speech Recogni-
tion," Int. Conf. on Spoken Language Processing, 2000, Beijing, China.
[5]
Acero, A. and X.D. Huang, "Augmented Cepstral Normalization for Robust Speech Recog-
nition," Proc. of the IEEE Workshop on Automatic Speech Recognition, 1995, Snowbird,
UT.
[6]
Acero, A. and R. Stern, "Environmental Robustness in Automatic Speech Recognition," Int.
Conf. on Acoustics, Speech and Signal Processing, 1990, Albuquerque, NM pp. 849-852.
[7]
Amari, S., A. Cichocki, and H.H. Yang, eds. A New Learning Algorithm for Blind Separa-
tion, Advances in Neural Information Processing Systems, 1996, Cambridge, MA, MIT
Press.
[8]
Atal, B.S., "Effectiveness of Linear Prediction Characteristics of the Speech Wave for
Automatic Speaker Identification and Verification," Journal of the Acoustical Society of
America, 1974, 55(6), pp. 1304--1312.

536
Environmental Robustness
[9]
Attias, H., "Independent Factor Analysis," Neural Computation, 1998, 11, pp. 803-851.
[10]
Bell, A.J. and T.J. Sejnowski, "An Information Maximisation Approach to Blind Separation
and Blind Deconvolution," Neural Computation, 1995, 7(6), pp. 1129-1159.
[11]
Belouchrani, A., et al., "A Blind Source Separation Technique Using Second Order Statis-
tics," IEEE Trans. on Signal Processing, 1997, 45(2), pp. 434-444.
[12]
Berouti, M., R. Schwartz, and J. Makhoul, "Enhancement of Speech Corrupted by Acoustic
Noise," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1979 pp.
208-211.
[13]
Boll, S.F., "Suppression of Acoustic Noise in Speech Using Spectral Subtraction," IEEE
Trans. on Acoustics, Speech and Signal Processing, 1979, 27(Apr.), pp. 113-120.
[14]
Boll, S.F. and D.C. Pulsipher, "Suppression of Acoustic Noise in Speech Using Two Micro-
phone Adaptive Noise Cancellation," IEEE Trans. on Acoustics Speech and Signal Process-
ing, 1980, 28(December), pp. 751-753.
[15]
Bregman, A.S., Auditory Scene Analysis, 1990, Cambridge MA, MIT Press.
[16]
Breining, C., Acoustic Echo Control, in IEEE Signal Processing Magazine, 1999. pp. 42-69.
[17]
Cardoso, J., "Blind Signal Separation: Statistical Principles," Proc. of the IEEE, 1998, 9(10),
pp. 2009-2025.
[18]
Cardoso, J.F., "Infomax and Maximum Likelihood for Blind Source Separation," IEEE Sig-
nal Processing Letters, 1997, 4, pp. 112-114.
[19]
Comon, P., "Independent Component Analysis: A New Concept," Signal Processing, 1994,
36, pp. 287-314.
[20]
Deller, J.R., J.H.L. Hansen, and J.G. Proakis, Discrete-Time Processing of Speech Signals,
2000, IEEE Press.
[21]
Deng, L., et al., "Large-Vocabulary Speech Recognition Under Adverse Acoustic Environ-
ments," Int. Conf. on Spoken Language Processing, 2000, Beijing, China.
[22]
Ephraim, Y., "Statistical Model-Based Speech Enhancement System," Proc. of the IEEE,
1992, 80(1), pp. 1526-1555.
[23]
Ephraim, Y. and D. Malah, "Speech Enhancement Using Minimum Mean Square Error
Short Time Spectral Amplitude Estimator," IEEE Trans. on Acoustics, Speech and Signal
Processing, 1984, 32(6), pp. 1109-1121.
[24]
Flanagan, J.L., et al., "Computer-Steered Microphone Arrays for Sound Transduction in
Large Rooms," Journal of the Acoustical Society of America, 1985, 78(5), pp. 1508--1518.
[25]
Frost, O.L., "An Algorithm for Linearly Constrained Adaptive Array Processing," Proc. of
the IEEE, 1972, 60(8), pp. 926--935.
[26]
Gales, M.J., Model Based Techniques for Noise Robust Speech Recognition, PhD Thesis in
Engineering Department 1995, Cambridge University, .
[27]
Ghitza, O., "Robustness against Noise: The Role of Timing-Synchrony Measurement," Proc.
of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1987 pp. 2372-2375.
[28]
Gopinath, R.A., et al., "Robust Speech Recognition in Noise - Performance of the IBM Con-
tinuous Speech Recognizer on the ARPA Noise Spoke Task," Proc. ARPA Workshop on
Spoken Language Systems Technology, 1995 pp. 127-133.
[29]
Griffiths, L.J. and C.W. Jim, "An Alternative Approach to Linearly Constrained Adaptive
Beamforming," IEEE Trans. on Antennas and Propagation, 1982, 30(1), pp. 27-34.
[30]
Haykin, S., Adaptive Filter Theory, 2nd ed, 1996, Upper Saddle River, NJ, Prentice-Hall.
[31]
Hermansky, H., "Perceptual Linear Predictive (PLP) Analysis of Speech," Journal of the
Acoustical Society of America, 1990, 87(4), pp. 1738-1752.
[32]
Hermansky, H. and N. Morgan, "RASTA Processing of Speech," IEEE Trans. on Speech
and Audio Processing, 1994, 2(4), pp. 578-589.

Historical Perspective and Further Reading
537
[33]
Huang, X.D., et al., "The SPHINX-II Speech Recognition System: An Overview," Computer
Speech and Language, 1993 pp. 137-148.
[34]
Hunt, M. and C. Lefebre, "A Comparison of Several Acoustic Representations for Speech
Recognition with Degraded and Undegraded Speech," Int. Conf. on Acoustic, Speech and
Signal Processing, 1989 pp. 262-265.
[35]
Juang, B.H., "Speech Recognition in Adverse Environments," Computer Speech and Lan-
guage, 1991, 5, pp. 275-294.
[36]
Junqua, J.C., "The Lombard Reflex and Its Role in Human Listeners and Automatic Speech
Recognition," Journal of the Acoustical Society of America, 1993, 93(1), pp. 510--524.
[37]
Junqua, J.C. and J.P. Haton, Robustness in Automatic Speech Recognition, 1996, Kluwer
Academic Publishers.
[38]
Lee, T.W., Independent Component Analysis: Theory and Applications, 1998, Kluwer Aca-
demic Publishers.
[39]
Lippmann, R.P., E.A. Martin, and D.P. Paul, "Multi-Style Training for Robust Isolated-
Word Speech Recognition," Int. Conf. on Acoustics, Speech and Signal Processing, 1987,
Dallas, TX pp. 709-712.
[40]
Lombard, E., "Le Signe de l'élévation de la Voix," Ann. Maladies Oreille, Larynx, Nez,
Pharynx, 1911, 37, pp. 101-119.
[41]
Matassoni, M., M. Omologo, and D. Giuliani, "Hands-Free Speech Recognition Using a
Filtered Clean Corpus and Incremental HMM Adaptation," Proc. Int. Conf. on Acoustics,
Speech and Signal Processing, 2000, Istanbul, Turkey pp. 1407-1410.
[42]
Mendel, J.M., Lessons in Estimation Theory for Signal Processing, Communications, and
Control, 1995, Upper Saddle River, NJ, Prentice Hall.
[43]
Moreno, P., Speech Recognition in Noisy Environments, PhD Thesis in Electrical and Com-
puter Engineering 1996, Carnegie Mellon University, Pittsburgh.
[44]
Moreno, P.J., B. Raj, and R.M. Stern, "A Vector Taylor Series Approach for Environment
Independent Speech Recognition," Int. Conf. on Acoustics, Speech and Signal Processing,
1996, Atlanta pp. 733-736.
[45]
Morgan, N. and H. Bourlard, Continuous Speech Recognition: An Introduction to Hybrid
HMM/Connectionist Approach, in IEEE Signal Processing Magazine, 1995. pp. 25-42.
[46]
Olsen, H.F., "Gradient Microphones," Journal of the Acoustical Society of America, 1946,
17,(3), pp. 192-198.
[47]
Porter, J.E. and S.F. Boll, "Optimal Estimators for Spectral Restoration of Noisy Speech,"
Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1984, San Diego,
CA pp. 18.A.2.1-4.
[48]
Rahim, M.G. and B.H. Juang, "Signal Bias Removal by Maximum Likelihood Estimation
for Robust Telephone Speech Recognition," IEEE Trans. on Speech and Audio Processing,
1996, 4(1), pp. 19-30.
[49]
Seneff, S., "A Joint Synchrony/Mean-Rate Model of Auditory Speech Processing," Journal
of Phonetics, 1988, 16(1), pp. 55-76.
[50]
Sharma, S., et al., "Feature Extraction Using Non-Linear Transformation for Robust Speech
Recognition on the Aurora Database," Int. Conf. on Acoustics, Speech and Signal Process-
ing, 2000, Istanbul, Turkey pp. 1117-1120.
[51]
Sullivan, T.M. and R.M. Stern, "Multi-Microphone Correlation-Based Processing for Robust
Speech Recognition," Int. Conf. on Acoustics, Speech and Signal Processing, 1993, Minnea-
polis pp. 2091-2094.
[52]
Suzuki, Y., et al., "An Optimum Computer-Generated Pulse Signal Suitable for the Meas-
urement of Very Long Impulse Responses," Journal of the Acoustical Society of America,
1995, 97(2), pp. 1119-1123.

538
Environmental Robustness
[53]
Tamura, S. and A. Waibel, "Noise Reduction Using Connectionist Models," Int. Conf. on
Acoustics, Speech and Signal Processing, 1988, New York pp. 553-556.
[54]
Theodoridis, S. and M.G. Bellanger, Adaptive Filters and Acoustic Echo Control, in IEEE
Signal Processing Magazine, 1999. pp. 12-41.
[55]
Varga, A.P. and R.K. Moore, "Hidden Markov Model Decomposition of Speech and Noise,"
Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1990 pp. 845-848.
[56]
Wan, E.A., R.V.D. Merwe, and A.T. Nelson, "Dual Estimation and the Unscented Transfor-
mation" in Advances in Neural Information Processing Systems, S.A. Solla, T.K. Leen, and
K.R. Muller, eds. 2000, Cambridge, MA, pp. 666-672, MIT Press.
[57]
Ward, W., "Modeling Non-Verbal Sounds for Speech Recognition," Proc. Speech and Natu-
ral Language Workshop, 1989, Cape Cod, MA, Morgan Kauffman pp. 311-318.
[58]
Widrow, B. and M.E. Hoff, "Adaptive Switching Algorithms," IRE Wescon Convention
Record, 1960 pp. 96-104.
[59]
Widrow, B. and S.D. Stearns, Adaptive Signal Processing, 1985, Upper Saddle River, NJ,
Prentice Hall.
[60]
Woodland, P.C., "Improving Environmental Robustness in Large Vocabulary Speech Rec-
ognition," Int. Conf. on Acoustics, Speech and Signal Processing, 1996, Atlanta, Georgia pp.
65-68.

539
C H A P T E R
1 1
Language ModelingEquation Section 11
Acoustic pattern matching, as discussed in
Chapter 9, and knowledge about language are equally important in recognizing and under-
standing natural speech. Lexical knowledge (i.e., vocabulary definition and word pronuncia-
tion) is required, as are the syntax and semantics of the language (the rules that determine
what sequences of words are grammatically well-formed and meaningful). In addition,
knowledge of the pragmatics of language (the structure of extended discourse, and what
people are likely to say in particular contexts) can be important to achieving the goal of spo-
ken language understanding systems. In practical speech recognition, it may be impossible to
separate the use of these different levels of knowledge, since they are often tightly inte-
grated.
In this chapter we review the basic concept of Chomsky’s formal language theory and
the probabilistic language model. For the formal language model, two things are fundamen-
tal: the grammar and the parsing algorithm. The grammar is a formal specification of the
permissible structures for the language. The parsing technique is the method of analyzing the
sentence to see if its structure is compliant with the grammar. With the advent of bodies of

540
Language Modeling
text (corpora) that have had their structures hand-annotated, it is now possible to generalize
the formal grammar to include accurate probabilities. Furthermore, the probabilistic relation-
ship among a sequence of words can be directly derived and modeled from the corpora with
the so-called stochastic language models, such as n-gram, avoiding the need to create broad
coverage formal grammars. Stochastic language models play a critical role in building a
working spoken language system, and we discuss a number of important issues associated
with them.
11.1.
FORMAL LANGUAGE THEORY
In constructing a syntactic grammar for a language, it is important to consider the generality,
the selectivity, and the understandability of the grammar. The generality and selectivity basi-
cally determine the range of sentences the grammar accepts and rejects. The understandabil-
ity is important, since it is up to the authors of the system to create and maintain the gram-
mar. For SLU systems described in Chapter 17, we need to have a grammar that covers and
generalizes to most of the typical sentences for an application. The system also needs to dis-
tinguish the kind of sentences for different actions in a given application. Without under-
standability, it is almost impossible to improve a practical SLU system that typically in-
volves a large number of developers to maintain and refine the grammar.
S
VP
NP
V
NP
ADJ
NAME
Mary
loves
that
person
Rewrite Rules:
1. S
NP VP
2. VP
V NP
3. VP
AUX VP
4. NP
ART NP1
5. NP
ADJ NP1
7. NP1
N
6. NP1
ADJ NP1
8. NP
NAME
9. NP
PRON
10. NAME
Mary
11. V
loves
12. ADJ
that
13. N
person
NP1
N
Figure 11.1 A tree representation of a sentence and its corresponding grammar
The most common way of representing the grammatical structure of a sentence, “Mary
loves that person”, is by using a tree, as illustrated in Figure 11.1. The node labeled S is the
parent node of the nodes labeled NP and VP for noun phrase and verb phrase respectively.
The VP node is the parent node of node V and N - for verb and noun, respectively. Each leaf
is associated with the word in the sentence to be analyzed. To construct such a tree for a sen-
tence, we must know the structure of the language so that a set of rewrite rules can be used to

Formal Language Theory
541
describe what tree structures are allowable. These rules, as illustrated in Figure 11.1, deter-
mine that a certain symbol may be expanded in the tree by a sequence of symbols. The
grammatical structure helps in determining the meaning of the sentence. It tells us that that in
the sentence modifies person. Mary loves that person.
11.1.1.
Chomsky Hierarchy
In Chomsky's formal language theory [1, 14, 15], a grammar is defined as G = (V, T, P, S),
where V and T are finite sets of non-terminals and terminals, respectively. V contains all the
non-terminal symbols. We often use upper-case symbols to denote them. In the example
discussed here, S, NP, NP1, VP, NAME, ADJ, N, and V are non-terminal symbols. The ter-
minal set T contains Mary, loves, that, and person, which are often denoted with lower-case
symbols. P is a finite set of production (rewrite) rules, as illustrated in the rewrite rules in
Figure 11.1. S is a special non-terminal, called the start symbol.
Table 11.1 Chomsky hierarchy and the corresponding machine that accepts the language.
Types
Constraints
Automata
Phase structure
grammar
α
β
→
. This is the most general
grammar.
Turing machine
Context-sensitive
grammar
A subset of the phrase structure
grammar. α
β
≤
, where |.| indicates
the length of the string.
Linear bounded
automata
Context-free gram-
mar (CFG)
A subset of the context sensitive
grammar. The production rule is
A
β
→
, where A is a non-terminal.
This production rule is shown to be
equivalent to Chomsky normal form:
A
w
→
and A
BC
→
, where w is a
terminal and B, C are non-terminals.
Push down automata
Regular grammar
A subset of the CFG. The production
rule is expressed as: A
w
→
and A
wB
→
.
Finite-state automata
The language to be analyzed is essentially a string of terminal symbols, such as
“Mary loves that person.” It is produced by applying production rules sequentially to the
start symbol. The production rule is of the form α
β
→
, where α and β are arbitrary
strings of grammar symbols V and T, and the α must not be empty. In formal language the-
ory, four major languages and their associated grammars are hierarchically structured. They
are referred to as the Chomsky hierarchy [1] as defined in Table 11.1. There are four kinds
of automata that can accept the languages produced by these four types of grammars. Among
these automata, the finite-state automaton is not only the mathematical device used to im-

542
Language Modeling
plement the regular grammar but also one of the most significant tools in computational lin-
guistics. Variations of automata such as finite-state transducers, hidden Markov models, and
n-gram models are important examples in spoken language processing.
These grammatical formulations can be compared according to their generative ca-
pacity, i.e., the range that the formalism can cover. While there is evidence that natural lan-
guages are at least weakly context sensitive, the context-sensitive requirements are rare in
practice. The context-free grammar (CFG) is a very important structure for dealing with both
machine language and natural language. CFGs are not only powerful enough to describe
most of the structure in spoken language,1 but also restrictive enough to have efficient pars-
ers to analyze natural sentences. Since CFGs offer a good compromise between parsing effi-
ciency and power in representing the structure of the language, they have been widely ap-
plied to natural language processing. Alternatively, regular grammars, as represented with a
finite-state machine, can be applied to more restricted applications. Since finite-state gram-
mars are a subset of the more general context-free grammar, we focus our discussion on con-
text free grammars only, although the parsing algorithm for finite-state grammars can be
more efficient.
As discussed in Section 11.1.2, a parsing algorithm offers a procedure that searches
through various ways of combining grammatical rules to find a combination that generates a
tree to illustrate the structure of the input sentence, which is similar to the search problem in
speech recognition. The result of the parsing algorithm is a parse tree,2 which can be re-
garded as a record of the CFG rules that account for the structure of the sentence. In other
words, if we parse the sentence, working either top-down from S or bottom-up from each
word, we automatically derive something that is similar to the tree representation, as illus-
trated in Figure 11.1.
A push-down automaton is also called a recursive transition network (RTN), which is
an alternative formalism to describe context-free grammars. A transition network consists of
nodes and labeled arcs. One of the nodes is specified as the initial state S. Starting at the ini-
tial state, we traverse an arc if the current word in the sentence is in the category on the arc.
If the arc is followed, the current word is updated to the next word. A phrase can be parsed if
there is a path from the starting node to a pop arc that indicates a complete parse for all the
words in the phrase. Simple transition networks without recursion are often called finite-state
machines (FSM). Finite-state machines are equivalent in expressive power to regular gram-
mars and, thus, are not powerful enough to describe all languages that can be described by
CFGs. Chapter 12 has a more detailed discussion on RTNs and FSMs into speech recogni-
tion.
1 The effort to prove natural languages is noncontext free is summarized in Pullman and Gazdar [54].
2The result can be more than one parse tree since natural language sentences are often ambiguous. In practice, a
parsing algorithm should not only consider all the possible parse trees but also provide a ranking among them, as
discussed in Chapter 17.

Formal Language Theory
543
11.1.2.
Chart Parsing for Context-Free Grammars
Since Chomsky introduced the notion of context-free grammars in the 1950s, a vast literature
has arisen on the parsing algorithms. Most parsing algorithms were developed in computer
science to analyze programming languages that are not ambiguous in the way that spoken
language is [1, 32]. We discuss only the most relevant materials that are fundamental to
building spoken language systems, namely the chart parser for the context-free grammar.
This algorithm has been widely used in state-of-the-art spoken language understanding sys-
tems.
11.1.2.1.
Top Down or Bottom Up?
Parsing is a special case of the search problem generally encountered in speech recognition.
A parsing algorithm offers a procedure that searches through various ways of combining
grammatical rules to find a combination that generates a tree to illustrate the structure of the
input sentence, as illustrated in Figure 11.1. The search procedure can start from the root of
the tree with the S symbol, attempting to rewrite it into a sequence of terminal symbols that
matches the words in the input sentence, which is based on goal-directed search. Alterna-
tively, the search procedure can start from the words in the input sentence and identify a
word sequence that matches some non-terminal symbol. The bottom-up procedure can be
repeated with partially parsed symbols until the root of the tree or the start symbol S is iden-
tified. This data-directed search has been widely used in practical SLU systems.
A top-down approach starts with the S symbol, then searches through different ways to
rewrite the symbols until the input sentence is generated, or until all possibilities have been
examined. A grammar is said to accept a sentence if there is a sequence of rules that allow us
to rewrite the start symbol into the sentence. For the grammar in Figure 11.1, a sequence of
rewrite rules can be illustrated as follows:
S
→NP VP (rewriting S using S→NP)
→NAME VP (rewriting NP using NP→NAME)
→Mary VP (rewriting NAME using NAME→Mary)
…
→Mary loves that person (rewriting N using N→person)
Alternatively, we can take a bottom-up approach to start with the words in the input
sentence and use the rewrite rules backward to reduce the sequence of symbols until it be-
comes S. The left-had side or each rule is used to rewrite the symbol on the right-hand side
as follows:
→NAME loves that person (rewriting Mary using NAME→Mary)
→NAME V that person (rewriting loves using B→loves)
…
→NP VP (rewriting NP using S→NP VP)
→S

544
Language Modeling
A parsing algorithm must systematically explore every possible state that represents
the intermediate node in the parsing tree. As discussed in Chapter 12, if a mistake occurs
early on in choosing the rule that rewrites S, the intermediate parser results can be quite
wasteful if the number of rules becomes large.
The main difference between top-down and bottom-up parsers is the way the grammar
rules are used. For example, consider the rule NP→ADJ NP1. In a top-down approach, the
rule is used to identify an NP by looking for the sequence ADJ NP1. Top-down parsing can
be very predictive. A phrase or a word may be ambiguous in isolation. The top-down ap-
proach may prevent some ungrammatical combinations from consideration. It never wastes
time exploring trees that cannot result in an S. On the other hand, it may predict many differ-
ent constituents that do not have a match to the input sentence and rebuild large constituents
again and again. For example, when the grammar is left-recursive (i.e., it contains a non-
terminal category that has a derivation that includes itself anywhere along its leftmost
branch), the top-down approach can lead a top-down, depth-first left-to-right parser to recur-
sively expand the same non-terminal over again in exactly the same way. This causes an in-
finite expansion of trees. In contrast, a bottom-up parser takes a sequence ADJ NP1 and
identifies it as an NP according to the rule. The basic operation in bottom-up parsing is to
take a sequence of symbols and match it to the right-hand side of the rules. It checks the in-
put only once, and only builds each constituent exactly once. However, it may build up trees
that have no hope of leading to S since it never suggests trees that are not at least locally
grounded in the actual input. Since bottom-up parsing is similar to top-down parsing in terms
of overall performance and is particularly suitable for robust spoken language processing as
described in Chapter 17, we use the bottom-up method as our example to understand the key
concept in the next section.
11.1.2.2.
Bottom-Up Chart Parsing
As a standard search procedure, the state of the search consists of a symbol list, starting with
the words in the sentence. Successor states can be generated by exploring all possible ways
to replace a sequence of symbols that matches the right-hand side of a grammar rule with its
left-hand side symbol. A simple-minded solution enumerates all the possible matches, lead-
ing to prohibitively expensive computational complexity. To avoid this problem, it is neces-
sary to store partially parsed results of the matching, thereby eliminating duplicate work.
This is the same technique that has been widely used in dynamic programming, as described
in Chapter 8. Since chart parsing does not need to be from left to right, it is more efficient
than the graph search algorithm discussed in Chapter 12, which can be used to parse the in-
put sentence from left to right.
A data structure, called a chart, is used to allow the parser to store the partial results of
the matching. The chart data structure maintains not only the records of all the constituents
derived from the sentence so far in the parse tree, but also the records of rules that have
matched partially but are still incomplete. These are called active arcs. Here, matches are
always considered from the point of view of some active constituents, which represent the
subparts that the input sentence can be divided into according to the rewrite rules. Active

Formal Language Theory
545
constituents are stored in a data structure called an agenda. To find grammar rules that
match a string involving the active constituent, we need to identify rules that start with the
active constituent or rules that have already been started by earlier active constituents and
require the current constituent to complete the rule or to extend the rule. The basic operation
of a chart-based parser involves combining these partially matched records (active arcs) with
a completed constituent to form either a new completed constituent or a new partially
matched (but incomplete) constituent that is an extension of the original partially matched
constituent. Just like the graph search algorithm, we can use either a depth-first or breadth-
first search strategy, depending on how the agenda is implemented. If we use probabilities or
other heuristics, we take a best-first strategy discussed in Chapter 12 to select constituents
from the agenda. The chart-parser process is defined more precisely in Algorithm 11.1. It is
possible to combine both top-down and bottom-up. The major difference is how the con-
stituents are used.
ALGORITHM 11.1 A BOTTOM-UP CHART PARSER
Step1: Initialization: Define a list called chart to store active arcs, and a list called an agenda to
store active constituents until they are added to the chart.
Step 2: Repeat: Repeat Step 2 to 7 until there is no input left.
Step 3: Push and pop the agenda: If the agenda is empty, look up the interpretations of the
next word in the input and push them to the agenda. Pop a constituent C from the agenda. If C
corresponds to position from
iw to
j
w of the input sentence, we denote it C[i,j].
Step 4: Add C to the chart: Insert C[i,j] into the chart.
Step 5: Add key-marked active arcs to the chart: For each rule in the grammar of the form
X→C Y, add to the chart an active arc (partially matched constituent) of the form X[i,j]→°CY,
where ° denotes the critical position called the key that indicates that everything before ° has
been seen, but things after ° are yet to be matched (incomplete constituent).
Step 6: Move ° forward: For any active arc of the form X[1,j]→Y…°C…Z (everything before
iw ) in the chart, add a new active arc of the form X[1,j] →Y…C°…Z to the chart.
Step 7: Add new constituents to the agenda: For any active arc of the form X[1,I]→Y…#C,
add a new constituent of type X[1,j] to the agenda.
Step 8: Exit: If S[1,n] is in the chart, where n is the length of the input sentence, we can exit
successfully unless we want to find all possible interpretations of the sentence. The chart may
contain many S structures covering the entire set of positions.
Let us look at an example to see how the chart parser parses the sentence Mary loves
that person using the grammar specified in Figure 11.1. We first create the chart and agenda
data structure as illustrated in Figure 11.2 (a), in which the leaves of the tree-like chart data
structure corresponds to the position of each input word. The parent of each block in the
chart covers from the position of the left child’s corresponding starting word position to the
right child’s corresponding ending word position. Thus, the root block in the chart covers the
whole sentence from the first word Mary to the last word person. The chart parser scans

546
Language Modeling
through the input words to match against possible rewrite rules in the grammar. For the first
word, the rule Name→Mary can be matched, so it is added to the agenda according to Step 3
in Algorithm 11.1. In Step 4, Name→Mary is added to the chart from the agenda. After the
word Mary is processed, we have Name→Mary, NP→Name, and S→NP°VP in the chart, as
illustrated in Figure 11.2 (b). NP°VP in the chart indicates that ° has reached the point at
which everything before ° has been matched (in this case Mary matched NP) but everything
after ° is yet to be parsed. The completed parsed chart is illustrated in Figure 11.2 (c).
Name[1,1]
Mary
Mary
loves
that
person
(a) The chart is illustrated on the left, and the agenda is on the right. The agenda now has one
rule in it according to Step 3, since the agenda is empty.
S
NP ° VP
V[2,2]
loves
Name
Mary
NP
Name
Mary
loves
that
person

Formal Language Theory
547
(b) After Mary, the chart now has rules Name→Mary, NP→Name, and S→NP°VP.
ADJ
that
NP
ADJ ° NP1
S
NP °VP
V
loves
VP
V °NP
Name
Mary
NP
Name
S
NP °VP
Mary
loves
that
person
N
person
NP1
N
NP
ADJ NP1
VP
V NP
VP
V NP
S
NP VP
S
NP VP
(c) The chart after the whole sentence is parsed. S→NP VP covers the whole sentence, indicating that
the sentence is parsed successfully by the grammar.
Figure 11.2 An example of a chart parser with the grammar illustrated in Figure 11.1. Parts (a)
and (b) show the initial chart and agenda to parse the first word.; part (c) shows the chart after
the sentence is completely parsed.
A parser may assign one or more parsed structures to the sentence in the language it
defines. If any sentence is assigned more than one such structure, the grammar is said to be
ambiguous. Spoken language is, of course, ambiguous by nature.3 For example, we can have
a sentence like Mary sold the student bags. It is unclear whether student should be the modi-
fier for bags or whether it means that Mary sold the bags to the student.
Chart parsers can be fairly efficient simply because the same constituent is never con-
structed more than once. In the worst case, the chart parser builds every possible constituent
between every possible pair of positions, leading to the worst-case computational complexity
of O(n3), where n is the length of the input sentence. This is still far more efficient than a
straightforward brute-force search.
In many practical tasks, we need only a partial parse or shallow parse of the input sen-
tence. You can use cascades of finite-state automata instead of CFGs. Relying on simple
finite-state automata rather than full parsing makes such systems more efficient, although
finite-state systems cannot model certain kinds of recursive rules, so that efficiency is traded
for a certain lack of coverage.
3 The same parse tree can also mean multiple things, so a parse tree itself does not define meaning. “Mary loves
that person” could be sarcastic and mean something different.

548
Language Modeling
11.2.
STOCHASTIC LANGUAGE MODELS
Stochastic language models (SLM) take a probabilistic viewpoint of language modeling. We
need to accurately estimate the probability P(W) for a given word sequence
1
2...
n
w w
w
=
W
.
In the formal language theory discussed in Section 11.1, P(W) can be regarded as 1 or 0 if
the word sequence is accepted or rejected, respectively, by the grammar. This may be inap-
propriate for spoken language systems, since the grammar itself is unlikely to have a com-
plete coverage, not to mention that spoken language is often ungrammatical in real conversa-
tional applications.
The key goal of SLM is to provide adequate probabilistic information so that the likely
word sequences should have a higher probability. This not only makes speech recognition
more accurate but also helps to dramatically constrain the search space for speech recogni-
tion (see Chapters 12 and 13). Notice that SLM can have a wide coverage on all the possible
word sequences, since probabilities are used to differentiate different word sequences. The
most widely used SLM is the so call n-gram model discussed in this chapter. In fact, the
CFG can be augmented as the bridge between the n-gram and the formal grammar if we can
incorporate probabilities into the production rules, as discussed in the next section.
11.2.1.
Probabilistic Context-Free Grammars
The CFG can be augmented with probability for each production rule. The advantages of the
probabilistic CFGs (PCFGs) lie in their ability to more accurately capture the embedded
usage structure of the spoken language to minimize the syntactic ambiguity. The use of
probability becomes increasingly important to discriminate many competing choices when
the number of rules is large.
In the PCFG, we have to address the parallel problems we discussed for HMMs in
Chapter 8. The recognition problem is concerned with the computation of the probability of
the start symbol S generating the word sequence W = w w
wT
1
2
,
....
, given the grammar G:
P S
G
(
|
)
 W
(11.1)
where  denotes a derivation sequence consisting of one or more steps. This is equivalent
to the chart parser augmented with probabilities, as discussed in Section 11.1.2.2.
The training problem is concerned with determining a set of rules in G based on the
training corpus and estimating the probability of each rule. If the set of rules is fixed, the
simplest approach to deriving these probabilities is to count the number of times each rule is
used in a corpus containing parsed sentences. We denote the probability of a rule A   by
P A
G
(
|
)
 
. For instance, if there are m rules for left-hand side non-terminal node
A: A
A
A
m






1
2
,
...
,
, we can estimate the probability of these rules as follows:
1
(
|
)
(
) /
(
)
m
j
j
i
i
P A
G
C A
C A
α
α
α
=
→
=
→
→

(11.2)

Stochastic Language Models
549
where C(.) denotes the number of times each rule is used.
When you have hand-annotated corpora, you can use the maximum likelihood estima-
tion as illustrated by Eq. (11.2) to derive the probabilities. When you don’t have hand-
annotated corpora, you can extend the EM algorithm (see Chapter 4) to derive these prob-
abilities. The algorithm is also known as the inside-outside algorithm. As what we discussed
in Chapter 8, you can develop algorithms similar to the Viterbi algorithm to find the most
likely parse tree that could have generated the sequence of words P(W) after these probabili-
ties are estimated.
We can make certain independence assumptions about rule usage. Namely, we assume
that the probability of a constituent being derived by a rule is independent of how the con-
stituent is used as a subconstituent. For instance, we assume that the probabilities of NP rules
are the same no matter whether the NP is used for the subject or the object of a verb, al-
though the assumptions are not valid in many cases. More specifically, let the word sequence
W=w w
wT
1
2
,
....
be generated by a PCFG G, with rules in Chomsky normal form as dis-
cussed in Section 11.1.1:
A
A A
i
m
n

and
i
l
A
w
→
(11.3)
where
m
A
and
n
A are two possible non-terminal that expand
iA at different locations. The
probability for these rules must satisfy the following constraint:
,
(
|
)
(
|
)
1, for all
i
m
n
i
l
m n
l
P A
A A
G
P A
w G
i
→
+
→
=


(11.4)
Equation (11.4) simply means that all non-terminals can generate either pairs of non-terminal
symbols or a single terminal symbol, and all these production rules should satisfy the prob-
ability constraint. Analogous to the HMM forward and backward probabilities discussed in
Chapter 8, we can define the inside and outside probabilities to facilitate the estimation of
these probabilities from the training data.
Figure 11.3 Inside probability is computed recursively as sum of all the derivations.
A
A
A
...
...
...
...
w
w
w
w
j
l
l
k
1

550
Language Modeling
A non-terminal symbol Ai can generate a sequence of words w w
w
j
j
k
1...
; we define
the probability of
1
( ,
, )
(
...
|
)
i
i
j
j
k
Inside j A k
P A
w w
w
G
+
=

as the inside constituent prob-
ability, since it assigns a probability to the word sequence inside the constituent. The inside
probability can be computed recursively. When only one word is emitted, the transition rule
of the form A
w
i
m

applies. When there is more than one word, rules of the form
A
A A
i
j
k

must apply. The inside probability of inside j A k
i
( ,
, ) can be expressed recur-
sively as follows:
inside j A k
P A
w w
w
P A
A A
P A
w
w
P A
w
w
P A
A A
inside j A
l inside l
A
k
i
i
j
j
k
i
m
n
l
j
k
m
j
l
n
l
k
n m
i
m
n
l
j
k
m
n
n m
( ,
)
(
...
)
(
) (
...
) (
...
)
(
)
( ,
, )
(
,
, )
,
,
,



















1
1
1
1
1
(11.5)
The inside probability is the sum of the probabilities of all derivations for the section
over the span of j to k. One possible derivation of the form can be drawn as a parse tree
shown in Figure 11.3.
Another useful probability is the outside probability for a non-terminal node
iA cover-
ing
s
w to
tw , in which they can be derived from the start symbol S, as illustrated in Figure
11.4, together with the rest of the words in the sentence:
outside s A t
P S
w
w
A w
w
i
s
i
t
T
( ,
, )
(
...
...
)




1
1
1
(11.6)
After the inside probabilities are computed bottom-up, we can compute the outside
probabilities top-down. For each non-terminal symbol Ai, there are one of two possible con-
figurations A
A A
m
n
i

or A
A A
m
i
n

as illustrated in Figure 11.5. Thus, we need to con-
sider all the possible derivations of these two forms as follows:
1
1
1
1
1
1
1
1
1
,
1
1
1
1
1
1
1
( ,
, )
(
...
...
)
(
) (
...
) (
...
...
)
(
) (
...
) (
...
...
)
(
)
i
s
i
t
T
s
m
n
i
n
l
s
l
m
t
T
l
T
m n
m
i
n
n
t
l
s
m
l
T
l t
s
m
n
i
l
outside s A t
P S
w
w
A w
w
P A
A A P A
w
w
P S
w
w
A w
w
P A
A A P A
w
w P S
w
w
A w
w
P A
A A
−
+
−
−
−
+
=
+
−
+
= +
−
=
=



→


+




=




+
→






→
=




,
1
( ,
,
1)
( ,
, )
(
)
(
1,
, )
( ,
, )
n
m
T
m n
m
i
n
n
m
l t
inside l A s
outside l A t
P A
A A inside t
A l outside s A l
= +


−
+








+
→
+






(11.7)

Stochastic Language Models
551
The inside and outside probabilities are used to compute the sentence probability as
follows:
P S
w
w
inside s A t outside s A t
s
t
T
i
i
i
(
...
)
( ,
, )
( ,
, )




1
for any
(11.8)
Figure 11.4 Definition of the outside probability.
Figure 11.5 Two possible configurations for a non-terminal node
m
A .
Since outside
A T
i
( ,
, )
1
is equal to 1 for the starting symbol only, the probability for the
whole sentence can be conveniently computed using the inside probability alone as
P S
G
inside
S T
(
|
)
( , , )


W
1
(11.9)
We are interested in the probability that a particular rule, A
A A
i
m
n

is used to cover
a span w
w
s
t
...
, given the sentence and the grammar:
1
( ,
, , , )
(
...
,
|
,
)
1
(
|
)
( ,
, )
(
1,
, )
( ,
, )
(
|
)
i
s
t
i
m
n
t
i
m
n
m
n
i
k s
i m n s t
P A
w
w A
A A
S
G
P A
A A
G inside s A k inside k
A t outside s A t
P S
G
ξ
−
=
=

→

=
→
+


W
W
(11.10)
These conditional probabilities form the basis of the inside-outside algorithm, which is
similar to the forward-backward algorithm discussed in Chapter 8. We can start with some
initial probability estimates. For each sentence of training data, we determine the inside and
Ai
S
w
w
w
w
w
w
s
s
t
t
T
1
1
1
...
...
...


S
m
A
iA
n
A
S
m
A
n
A
iA

552
Language Modeling
outside probabilities in order to compute, for each production rule, how likely it is that the
production rule is used as part of the derivation of that sentence. This gives us the number of
counts for each production rule in each sentence. Summing these counts across sentences
gives us an estimate of the total number of times each production rule is used to produce the
sentences in the training corpus. Dividing by the total counts of productions used for each
non-terminal gives us a new estimate of the probability of the production in the MLE frame-
work. For example, we have:
P A
A A G
i m n s t
i m n s t
i
m
n
t s
T
s
T
t s
T
s
T
m n
(
|
)
( ,
, , , )
( ,
, , , )
,


 


 









1
1
1
1
1
1
(11.11)
In a similar manner, we can estimate
(
|
)
i
m
P A
w
G
→
. It is also possible to let the in-
side-outside algorithm formulate all the possible grammar production rules so that we can
select rules with sufficient probability values. If there is no constraint, we may have too
many greedy symbols that serve as possible non-terminals. In addition, the algorithm is guar-
anteed only to find a local maximum. It is often necessary to use prior knowledge about the
task and the grammar to impose strong constraints to avoid these two problems. The chart
parser discussed in Section 11.1.2 can be modified to accommodate PCFGs [29, 45].
One problem with the PCFG is that it assumes that the expansion of any one non-
terminal is independent of the expansion of other non-terminals. Thus each PCFG rule prob-
ability is multiplied together without considering the location of the node in the parse tree.
This is against our intuition since there is a strong tendency toward the context-dependent
expansion. Another problem is its lack of sensitivity to words, although lexical information
plays an important role in selecting the correct parsing of an ambiguous prepositional phrase
attachment. In the PCFG, lexical information can only be represented via the probability of
pre-terminal nodes, such as verb or noun, to be expanded lexically. You can add lexical de-
pendencies to PCFGs and make PCFG probabilities more sensitive to surrounding syntactic
structure [6, 11, 19, 31, 45].
11.2.2.
N-gram Language Models
As covered earlier, a language model can be formulated as a probability distribution P(
)
W
over word strings W that reflects how frequently a string W occurs as a sentence. For ex-
ample, for a language model describing spoken language, we might have P(hi) = 0.01, since
perhaps one out of every hundred sentences a person speaks is hi. On the other hand, we
would have P(lid gallops Changsha pop) = 0, since it is extremely unlikely anyone would
utter such a strange string.
P(
)
W can be decomposed as

Stochastic Language Models
553
P
P w
w
w
P w
P w
w
P w w
w
P w
w
w
w
P w w
w
w
n
n
n
i
n
i
i
(
)
(
,
,
,
)
(
) (
|
) (
|
,
)
(
|
,
,
,
)
(
|
,
,
,
)
W 






1
2
1
2
1
3
1
2
1
2
1
1
1
2
1




(11.12)
where P w w
w
w
i
i
(
|
,
,
,
)
1
2
1


is the probability that wi will follow, given that the word se-
quence w
w
wi
1
2
1
,
,
,

 was presented previously. In Eq. (11.12), the choice of wi thus de-
pends on the entire past history of the input. For a vocabulary of size v there are vi1 differ-
ent histories and so, to specify P w w
w
w
i
i
(
|
,
,
,
)
1
2
1


completely, vi values would have to
be estimated. In reality, the probabilities P w w
w
w
i
i
(
|
,
,
,
)
1
2
1


are impossible to estimate
for even moderate values of i , since most histories w
w
wi
1
2
1
,
,
,

 are unique or have oc-
curred only a few times. A practical solution to the above problems is to assume that
P w w
w
w
i
i
(
|
,
,
,
)
1
2
1


depends only on some equivalence classes. The equivalence class
can be simply based on the several previous words w
w
w
i
N
i
N
i





1
2
1
,
,
,

. This leads to an
n-gram language model. If the word depends on the previous two words, we have a tri-
gram: P w w
w
i
i
i
(
|
,
)


2
1 . Similarly, we can have unigram: P wi
(
), or bigram: P w w
i
i
(
|
)
1
language models. The trigram is particularly powerful, as most words have a strong depend-
ence on the previous two words, and it can be estimated reasonably well with an attainable
corpus.
In bigram models, we make the approximation that the probability of a word depends
only on the identity of the immediately preceding word. To make P w w
i
i
(
|
)
1
meaningful
for i = 1, we pad the beginning of the sentence with a distinguished token <s>; that is, we
pretend w0 = <s>. In addition, to make the sum of the probabilities of all strings equal 1, it is
necessary to place a distinguished token </s> at the end of the sentence. For example, to
calculate P(Mary loves that person) we would take
P(Mary loves that person) =
P(Mary|<s>)P(loves|Mary)P(that|loves)P(person|that)P(</s>|person)
To estimate P w w
i
i
(
|
)
1 , the frequency with which the word wi occurs given that the
last word is wi1, we simply count how often the sequence
1
(
,
)
i
i
w
w
−
occurs in some text and
normalize the count by the number of times
1
iw −occurs.
In general, for a trigram model, the probability of a word depends on the two preced-
ing words. The trigram can be estimated by observing the frequencies or counts of the word
pair C w
w
i
i
(
,
)


2
1
and triplet C w
w
w
i
i
i
(
,
,
)


2
1
as follows:
P w w
w
C w
w
w
C w
w
i
i
i
i
i
i
i
i
(
|
)
(
)
(
)
,
,
,
,







2
1
2
1
2
1
(11.13)
The text available for building a model is called a training corpus. For n-gram models,
the amount of training data used is typically many millions of words. The estimate of Eq.
(11.13) is based on the maximum likelihood principle, because this assignment of probabili-

554
Language Modeling
ties yields the trigram model that assigns the highest probability to the training data of all
possible trigram models.
We sometimes refer to the value n of an n-gram model as its order. This terminology
comes from the area of Markov models, of which n-gram models are an instance. In particu-
lar, an n-gram model can be interpreted as a Markov model of order n-1.
Consider a small example. Let our training data S be comprised of the three sentences
John read her book. I read a different book. John read a book by Mulan and let us calculate
P(John read a book) for the maximum likelihood bigram model. We have
(
,
)
2
(
|
)
(
)
3
(
,
)
2
(
|
)
(
)
2
C
s
John
P John
s
C
s
C John read
P read John
C John
<
>
<
> =
=
<
>
=
=
(
, )
2
( |
)
(
)
3
( ,
)
1
(
| )
( )
2
(
,
/
)
2
(
/
|
)
(
)
3
C read a
P a read
C read
C a book
P book a
C a
C book
s
P
s
book
C book
=
=
=
=
<
>
<
>
=
=
These trigram probabilities help us estimate the probability for the sentence as:
(
)
(
|
) (
|
) ( |
) (
| ) (
/
|
)
0.148
P John read a book
P John
s
P read John P a read P book a P
s
book
=
<
>
<
>
≈
(11.14)
If these three sentences are all the data we have available to use in training our lan-
guage model, the model is unlikely to generalize well to new sentences. For example, the
sentence “Mulan read her book” should have a reasonable probability, but the trigram will
give it a zero probability simply because we do not have a reliable estimate for
P(read|Mulan).
Unlike linguistics, grammaticality is not a strong constraint in the n-gram language
model. Even though the string is ungrammatical, we may still assign it a high probability if n
is small.
11.3.
COMPLEXITY MEASURE OF LANGUAGE MODELS
Language can be thought of as an information source whose outputs are words wi belonging
to the vocabulary of the language. The most common metric for evaluating a language model
is the word recognition error rate, which requires the participation of a speech recognition
system. Alternatively, we can measure the probability that the language model assigns to test
word strings without involving speech recognition systems. This is the derivative measure of
cross-entropy known as test-set perplexity.

Complexity Measure of Language Models
555
The measure of cross-entropy is discussed in Chapter 3. Given a language model that
assigns probability
(
)
P W
to a word sequence W , we can derive a compression algorithm
that encodes the text W using
2
log
(
)
P
−
W
bits. The cross-entropy
(
)
H W
of a model
P w w
w
i
i n
i
(
|
...
)
 

1
1 on data W , with a sufficiently long word sequence, can be simply ap-
proximated as
2
1
(
)
log
(
)
H
P
N
= −
W
W
W
(11.15)
where NW is the length of the text W measured in words.
The perplexity
(
)
PP W
of a language model
(
)
P W is defined as the reciprocal of the
(geometric) average probability assigned by the model to each word in the test set W . This
is a measure, related to cross-entropy, known as test-set perplexity:
(
)
(
)
2H
PP
=
W
W
(11.16)
The perplexity can be roughly interpreted as the geometric mean of the branching fac-
tor of the text when presented to the language model. The perplexity defined in Eq. (11.16)
has two key parameters: a language model and a word sequence. The test-set4 perplexity
evaluates the generalization capability of the language model. The training-set perplexity
measures how the language model fits the training data, like the likelihood. It is generally
true that lower perplexity correlates with better recognition performance. This is because the
perplexity is essentially a statistically weighted word branching measure on the test set. The
higher the perplexity, the more branches the speech recognizer needs to consider statisti-
cally.
While the perplexity [Eqs. (11.16) and (11.15)] is easy to calculate for the n-gram [Eq.
(11.12)], it is slightly more complicated to compute for a probabilistic CFG. We can first
parse the word sequence and use Eq. (11.9) to compute
(
)
P W
for the test-set perplexity.
The perplexity can also be applied to nonstochastic models such as CFGs. We can assume
they have a uniform distribution in computing
(
)
P W .
A language with higher perplexity means that the number of words branching from a
previous word is larger on average. In this sense, the perplexity is an indication of the com-
plexity of the language if we have an accurate estimate of
(
)
P W . For a given language, the
difference between the perplexity of a language model and the true perplexity of the lan-
guage is an indication of the quality of the model. The perplexity of a particular language
model can change dramatically in terms of the vocabulary size, the number of states or
grammar rules, and the estimated probabilities. A language model with perplexity X has
roughly the same difficulty as another language model in which every word can be followed
by X different words with equal probabilities. Therefore, in the task of continuous digit rec-
ognition, the perplexity is 10. Clearly, lower perplexity will generally have less confusion in
4 We often distinguish between the word sequence from the unseen test data and that from the training data to
derive the language model.

556
Language Modeling
recognition. Typical perplexities yielded by n-gram models on English text range from about
50 to almost 1000 (corresponding to cross-entropies from about 6 to 10 bits/word), depend-
ing on the type of text. In the task of 5,000-word continuous speech recognition for the Wall
Street Journal, the test-set perplexities of the trigram grammar and the bigram grammar are
reported to be about 128 and 176 respectively5. In the tasks of 2000-word conversational Air
Travel Information System (ATIS), the test-set perplexity of the word trigram model is typi-
cally less than 20.
Since perplexity does not take into account the acoustic confusability, we eventually
have to measure speech recognition accuracy. For example, if the vocabulary of a speech
recognizer contains the E-set of English alphabet: B, C, D, E, G, and T, we can define a CFG
that has a low perplexity value of 6. Such a low perplexity does not guarantee we will have
good recognition performance, because of the intrinsic acoustic confusability of the E-set.
11.4.
N-GRAM SMOOTHING
One of the key problems in n-gram modeling is the inherent data sparseness of real training
data. If the training corpus is not large enough, many actually possible word successions may
not be well observed, leading to many extremely small probabilities. For example, with sev-
eral-million-word collections of English text, more than 50% of trigrams occur only once,
and more than 80% of trigrams occur less than five times. Smoothing is critical to make es-
timated probabilities robust for unseen data. If we consider the sentence Mulan read a book
in the example we discussed in Section 11.2.2, we have:
P read Mulan
C Mulan read
C Mulan w
w
(
|
)
(
,
)
(
,
)



0
1
giving us P(Mulan read a book) = 0.
Obviously, this is an underestimate for the probability of “Mulan read a book” since
there is some probability that the sentence occurs in some test set. To show why it is impor-
tant to give this probability a nonzero value, we turn to the primary application for language
models, speech recognition. In speech recognition, if P(W) is zero, the string W will never
be considered as a possible transcription, regardless of how unambiguous the acoustic signal
is. Thus, whenever a string W such that P(W) = 0 occurs during a speech recognition task,
an error will be made. Assigning all strings a nonzero probability helps prevent errors in
speech recognition. This is the core issue of smoothing. Smoothing techniques adjust the
maximum likelihood estimate of probabilities to produce more robust probabilities for un-
seen data, although the likelihood for the training data may be hurt slightly.
The name smoothing comes from the fact that these techniques tend to make distribu-
tions more uniform, by adjusting low probabilities such as zero probabilities upward, and
5 Some experimental results show that the test-set perplexities for different languages are comparable. For example,
French, English, Italian and German have a bigram test-set perplexity in the range of 95 to 133 for newspaper
corpora. Italian has a much higher perplexity reduction (a factor of 2) from bigram to trigram because of the high
number of function words. The trigram perplexity of Italian is among the lowest in these languages [34].

N-Gram Smoothing
557
high probabilities downward. Not only do smoothing methods generally prevent zero prob-
abilities, but they also attempt to improve the accuracy of the model as a whole. Whenever a
probability is estimated from few counts, smoothing has the potential to significantly im-
prove the estimation so that it has better generalization capability.
To give an example, one simple smoothing technique is to pretend each bigram occurs
once more than it actually does, yielding
P w w
C w
w
C w
w
C w
w
V
C w
w
i
i
i
i
i
i
w
i
i
i
i
w
i
i
(
|
)
(
,
)
(
(
,
))
(
,
)
(
,
)













1
1
1
1
1
1
1
1
(11.17)
where V is the size of the vocabulary. In practice, vocabularies are typically fixed to be tens
of thousands of words or less. All words not in the vocabulary are mapped to a single word,
usually called the unknown word. Let us reconsider the previous example using this new
distribution, and let us take our vocabulary V to be the set of all words occurring in the train-
ing data S, so that we have V = 11 (with both <s> and </s>).
For the sentence John read a book, we now have
(
)
(
|
) (
|
) ( |
) (
| ) (
|
)
0.00035
P John read a book
P John
bos
P read John P a read P book a P
eos
book
=
<
>
<
>
≈
(11.18)
In other words, we estimate that the sentence John read a book occurs about once
every three thousand sentences. This is more reasonable than the maximum likelihood esti-
mate of 0.148 of Eq. (11.14). For the sentence Mulan read a book, we have
(
)
(
|
) (
|
) ( |
) (
| ) (
|
)
0.000084
P Mulan read a book
P Mulan
bos
P read Mulan P a read P book a P
eos
book
=
<
>
<
>
≈
(11.19)
Again, this is more reasonable than the zero probability assigned by the maximum
likelihood model. In general, most existing smoothing algorithms can be described with the
following equation:
1
1
1
1
1
1
1
2
1
1
(
|
...
)
(
|
...
)
if
(
...
)>0
(
...
)
(
|
...
)
if
(
...
)=0
smooth
i
i n
i
i
i n
i
i n
i
i n
i
smooth
i
i n
i
i n
i
P
w
w
w
w
w
w
C w
w
w
w
P
w
w
w
C w
w
α
γ
−+
−
−+
−
−+
−+
−
−+
−
−+

= 

(11.20)
That is, if an n-gram has a nonzero count we use the distribution (
|
...
)
w w
w
i
i n
i
 

1
1 .
Otherwise, we backoff to the lower-order n-gram distribution P
w w
w
smooth
i
i n
i
(
|
...
)
 

2
1 ,
where the scaling factor  (
...
)
w
w
i n
i
 

1
1
is chosen to make the conditional distribution sum
to one. We refer to algorithms that fall directly in this framework as backoff models.
Several other smoothing algorithms are expressed as the linear interpolation of higher-
and lower-order n-gram models as:

558
Language Modeling
1
1
1
1
2
1
(
|
...
)
(
|
...
)
(1
)
(
|
...
)
smooth
i
i n
i
ML
i
i n
i
smooth
i
i n
i
P
w
w
w
P
w
w
w
P
w
w
w
λ
λ
−+
−
−+
−
−+
−
=
+
−
(11.21)
where λ is the interpolation weight that depends on
1
1
...
i n
i
w
w
−+
−.We refer to models of this
form as interpolated models.
The key difference between backoff and interpolated models is that for the probability
of n-grams with nonzero counts, interpolated models use information from lower-order dis-
tributions while backoff models do not. In both backoff and interpolated models, lower-order
distributions are used in determining the probability of n-grams with zero counts. Now, we
discuss several backoff and interpolated smoothing methods. Performance comparison of
these techniques in real speech recognition applications is discussed in Section 11.4.4.
11.4.1.
Deleted Interpolation Smoothing
Consider the case of constructing a bigram model on training data where we have that
C(enliven you) = 0 and C(enliven thou) = 0. Then, according to both additive smoothing of
Eq. (11.17), we have P(you|enliven) = P(thou|enliven). However, intuitively we should have
P(you|enliven) > P(thou|enliven), because the word you is much more common than the
word thou in modern English. To capture this behavior, we can interpolate the bigram model
with a unigram model. A unigram model conditions the probability of a word on no other
words, and just reflects the frequency of that word in text. We can linearly interpolate a bi-
gram model and a unigram model as follows:
P
w w
P w w
P w
I
i
i
i
i
i
(
|
)
(
|
)
(
)
(
)





1
1
1


(11.22)
where 0
1



. Because P(you|enliven) = P(thou|enliven)=0 while presumably P(you) >
P(thou), we will have that
(
|
)
(
|
)
I
I
P you enliven
P thou enliven
>
as desired.
In general, it is useful to interpolate higher-order n-gram models with lower-order n-
gram models, because when there is insufficient data to estimate a probability in the higher-
order model, the lower-order model can often provide useful information. An elegant way of
performing this interpolation is given as follows
P
w w
w
P w w
w
P
w w
w
I
i
i n
i
w
w
i
i n
i
w
w
I
i
i n
i
i
n
i
i
n
i
(
|
...
)
(
|
...
)
(
)
(
|
...
)
...
...
 

 

 




 

 

1
1
1
1
2
1
1
1
1
1
1


(11.23)
That is, the nth-order smoothed model is defined recursively as a linear interpolation
between the nth-order maximum likelihood model and the (n-1)th-order smoothed model.
To end the recursion, we can take the smoothed first-order model to be the maximum likeli-
hood distribution (unigram), or we can take the smoothed zeroth-order model to be the uni-
form distribution. Given a fixed P w w
w
i
i n
i
(
|
...
)
 

1
1 , it is possible to search efficiently for
the interpolation parameters using the deleted interpolation method discussed in Chapter 9.

N-Gram Smoothing
559
Notice that the optimal
1
1
...
i n
i
w
w
λ
−+
−
is different for different histories
1
1
...
i n
i
w
w
−+
−. For
example, for a context we have seen thousands of times, a high λ will be suitable, since the
higher-order distribution is very reliable; for a history that has occurred only once, a lower λ
is appropriate. Training each parameter
1
1
...
i n
i
w
w
λ
−+
−independently can be harmful; we need an
enormous amount of data to train so many independent parameters accurately. One possibil-
ity is to divide the
1
1
...
i n
i
w
w
λ
−+
−into a moderate number of partitions or buckets, constraining all
1
1
...
i n
i
w
w
λ
−+
−
in the same bucket to have the same value, thereby reducing the number of inde-
pendent parameters to be estimated. Ideally, we should tie together those
1
1
...
i n
i
w
w
λ
−+
−
that we
have a prior reason to believe should have similar values.
11.4.2.
Backoff Smoothing
Backoff smoothing is attractive because it is easy to implement for practical speech recogni-
tion systems. The Katz backoff model is the canonical example we discuss in this section. It
is based on the Good-Turing smoothing principle.
11.4.2.1.
Good-Turing Estimates and Katz Smoothing
The Good-Turing estimate is a smoothing technique to deal with infrequent n-grams. It is not
used by itself for n-gram smoothing, because it does not include the combination of higher-
order models with lower-order models necessary for good performance. However, it is used
as a tool in several smoothing techniques. The basic idea is to partition n-grams into groups
depending on their frequency (i.e. how many time the n-grams appear in the training data)
such that the parameter can be smoothed based on n-gram frequency.
The Good-Turing estimate states that for any n-gram that occurs r times, we should
pretend that it occurs r * times as follows:
r
r
n
n
r
r
*
(
)



1
1
(11.24)
where nr is the number of n-grams that occur exactly r times in the training data. To convert
this count to a probability, we just normalize: for an n-gram a with r counts, we take
P a
r
N
( )
*

(11.25)
where N
n r
r
r




0
*. Notice that
*
1
0
0
0
(
1)
r
r
r
r
r
r
N
n r
r
n
n r
∞
∞
∞
+
=
=
=
=
=
+
=



i.e. N is equal to the
original number of counts in the distribution [28].
Katz smoothing extends the intuitions of the Good-Turing estimate by adding the
combination of higher-order models with lower-order models [38]. Take the bigram as our

560
Language Modeling
example, Katz smoothing suggested using the Good-Turing estimate for nonzero counts as
follows:
*
1
1
if >0
(
)
(
) (
)
if =0
r
i
i
i
i
d r
r
C
w w
w
P w
r
α
−
−

= 

(11.26)
where dr is approximately equal to
* /
r
r . That is, all bigrams with a nonzero count r are
discounted according to a discount ratio dr , which implies that the counts subtracted from
the nonzero counts are distributed among the zero-count bigrams according to the next
lower-order distribution, e.g., the unigram model. The value (
)
wi1
is chosen to equalize
the total number of counts in the distribution, i.e.,
*
1
1
(
)
(
)
i
i
i
i
i
i
w
w
C
w
w
C
w w
−
−
=


. The
appropriate value for (
)
wi1
is computed so that the smoothed bigram satisfies the prob-
ability constraint:
(
)
(
|
)
(
)
(
|
)
(
)
*
: (
)
: (
)
*
: (
)
: (
)
w
P
w w
P w
P
w w
P w
i
w C w
w
i
i
w C w
w
i
w C w
w
i
i
w C w
w
i
i
i
i
i
i
i
i
i
i
i
i
i




















1
0
1
0
0
1
0
1
1
1
1
1
1
1
(11.27)
To calculate P
w w
i
i
*(
|
)
1
from the corrected count, we just normalize:
*
*
1
1
*
1
(
)
(
|
)
(
)
i
i
i
i
i
k
wk
C
w w
P w
w
C
w w
−
−
−
= 
(11.28)
In Katz implementation, the dr are calculated as follows: large counts are taken to be
reliable, so they are not discounted. In particular, Katz takes dr = 1 for all r > k for some k,
say k in the range of 5 to 8. The discount ratios for the lower counts r
k

are derived from
the Good-Turing estimate applied to the global bigram distribution; that is, nr in Eq. (11.24)
denotes the total number of bigrams that occur exactly r times in the training data. These dr
are chosen such that
 the resulting discounts are proportional to the discounts predicted by the Good-
Turing estimate, and
 the total number of counts discounted in the global bigram distribution is equal
to the total number of counts that should be assigned to bigrams with zero counts
according to the Good-Turing estimate.
The first constraint corresponds to the following equation:
*
r
r
d
r
µ
=
(11.29)

N-Gram Smoothing
561
for r
k
{ ,... }
1
with some constant µ . The Good-Turing estimate predicts that the total
mass assigned to bigrams with zero counts is
1
0
1
0
n
n
n
n =
, and the second constraint corre-
sponds to the equation
n
d
r
n
r
r
r
k
(
)
1
1
1



(11.30)
Based on Eq. (11.30), the unique solution is given by:
d
r
r
k
n
n
k
n
n
r
k
k







*
(
)
(
)
1
1
1
1
1
1
1
(11.31)
Katz smoothing for higher-order n-gram models is defined analogously. The Katz n-
gram backoff model is defined in terms of the Katz (n-1)-gram model. To end the recursion,
the Katz unigram model is taken to be the maximum likelihood unigram model. It is usually
necessary to smooth nr when using the Good-Turing estimate, e.g., for those nr that are very
low. However, in Katz smoothing this is not essential because the Good-Turing estimate is
used only for small counts r<=k, and nr is generally fairly high for these values of r. The
procedure of Katz smoothing can be summarized as in Algorithm 11.2.
In fact, the Katz backoff model can be expressed in terms of the interpolated model
defined in Eq. (11.23), in which the interpolation weight is obtained via Eq. (11.26) and
(11.27).
ALGORITHM 11.2 KATZ SMOOTHING
1
1
1
1
1
1
(
) /
(
)
if
(
|
)
(
)/
(
)
if
0
(
) (
)
if
0
i
i
i
Katz
i
i
r
i
i
i
i
i
C w w
C w
r
k
P
w
w
d C w w
C w
k
r
w
P w
r
α
−
−
−
−
−
−
>


=
≥
>


=

where d
r
r
k
n
n
k
n
n
r
k
k







*
(
)
(
)
1
1
1
1
1
1
1
and
:
1
:
0
1
0
1
(
|
)
(
)
1
(
)
i
i
Katz
i
i
w r
i
i
w r
P
w
w
w
P w
α
−
>
−
>
−
=
−



562
Language Modeling
11.4.2.2.
Alternative Backoff Models
In a similar manner to the Katz backoff model, there are other ways to discount the probabil-
ity mass. For instance, absolute discounting involves subtracting a fixed discount D <= 1
from each nonzero count. If we express the absolute discounting in term of interpolated
models, we have the following:
P
w w
w
C w
w
D
C w
w
P
w w
w
abs
i
i n
i
i n
i
i n
i
w
w
w
abs
i
i n
i
i
i
n
i
(
|
...
)
max{ (
...
)
, }
(
...
)
(
)
(
|
...
)
...
 

 
 
 






 

1
1
1
1
2
1
0
1
1
1

(11.32)
To make this distribution sum to 1, we normalize it to determine  w
w
i n
i
 

1
1
...
. Absolute
discounting is explained with the Good-Turing estimate. Empirically the average Good-
Turing discount r
r

* associated with n-grams of larger counts (r over 3) is largely constant
over r.
Consider building a bigram model on data where there exists a word that is very com-
mon, say Francisco, that occurs only after a single word, say San. Since C(Francisco) is
high, the unigram probability P(Francisco) will be high, and an algorithm such as absolute
discounting or Katz smoothing assigns a relatively high probability to occurrence of the
word Francisco after novel bigram histories. However, intuitively this probability should not
be high, since in the training data the word Francisco follows only a single history. That is,
perhaps Francisco should receive a low unigram probability, because the only time the word
occurs is when the last word is San, in which case the bigram probability models its prob-
ability well.
Extending this line of reasoning, perhaps the unigram probability used should not be
proportional to the number of occurrences of a word, but instead to the number of different
words that it follows. To give an intuitive argument, imagine traversing the training data
sequentially and building a bigram model on the preceding data to predict the current word.
Then, whenever the current bigram does not occur in the preceding data, the unigram prob-
ability becomes a large factor in the current bigram probability. If we assign a count to the
corresponding unigram whenever such an event occurs, then the number of counts assigned
to each unigram is simply the number of different words that it follows. In Kneser-Ney
smoothing [40], the lower-order n-gram is not proportional to the number of occurrences of
a word, but instead to the number of different words that it follows. We summarize the Kne-
ser-Ney backoff model in Algorithm 11.3.

N-Gram Smoothing
563
ALGORITHM 11.3 KNESER-NEY BIGRAM SMOOTHING
1
1
1
1
1
max{ (
)
,0}
if
(
)
0
(
)
(
|
)
(
)
(
)
otherwise
i
i
i
i
i
KN
i
i
i
KN
i
C w w
D
C w w
C w
P
w
w
w
P
w
α
−
−
−
−
−
−

>

= 

where
(
)
(
)/
(
)
i
KN
i
i
i
w
P
w
w
w
=
•
•



,
(
)
iw
•

is the number of unique words preceding
iw .
1
(
)
iw
α
−
is
chosen
to
make
the
distribution
sum
to
1
so
that
we
have:
1
:
1
1
: (
) 0
1
1
(
) 0
max{ (
)
,0}
1
(
)
(
)
1
(
)
i
i
i
i
i
i
i
i
w C w
w
i
i
KN
i
w C w
w
C w w
D
C w
w
P
w
α
−
−
−
>
−
−
>
−
−
=
−


Kneser-Ney smoothing is an extension of other backoff models. Most of the previous
models used the lower-order n-grams trained with ML estimation. Kneser-Ney smoothing
instead considers a lower-order distribution as a significant factor in the combined model
such that they are optimized together with other parameters. To derive the formula, more
generally, we express it in terms of the interpolated model specified in Eq. (11.23) as:
1
1
1
1
1
...
2
1
1
(
|
...
)
max{ (
...
)
,0}
(1
)
(
|
...
)
(
...
)
i n
i
i
KN
i
i n
i
i n
i
w
w
KN
i
i n
i
i n
i
w
P
w
w
w
C w
w
D
P
w
w
w
C w
w
λ
−+
−
−+
−
−+
−+
−
−+
−
=
+
−

(11.33)
To make this distribution sum to 1, we have:
1
1
...
1
1
1
1
(
...
)
(
...
)
i n
i
i
w
w
i n
i
i n
i
w
D
w
w
C w
w
λ
−+
−
−+
−
−+
−
=
•


(11.34)
where
1
1
(
...
)
i n
i
w
w
−+
−•

is the number of unique words that follow the history w
w
i n
i
 

1
1
...
.
This equation enables us to interpolate the lower-order distribution with all words, not just
with words that have zero counts in the higher-order distribution.
Now, take the bigram case as an example. We need to find a unigram distribution
P
w
KN
i
(
) such that the marginal of the bigram smoothed distributions should match the mar-
ginal of the training data:
C w
C w
P
w
w
P
w w
P w
i
i
w
KN
i
i
w
KN
i
i
i
w
i
i
i
(
)
(
)
(
)
(
|
) (
)










1
1
1
1
1
(11.35)
For P wi
(
)
1 , we simply take the distribution found in the training data

564
Language Modeling
P w
C w
C w
i
i
i
wi
(
)
(
)
(
)






1
1
1
1
(11.36)
We substitute Eq. (11.33) in Eq. (11.35). For the bigram case, we have:
1
1
1
1
1
1
1
1
1
1
1
1
1
: (
) 0
1
1
1
1
(
)
max{ (
)
,0}
(
)[
(
)
(
)]
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
i
i
i
i
i
i
i
i
i
i
i
i
KN
i
w
i
i
i
i
w
w
i
i
i
i
i
KN
i
w
C w
w
w
i
i
i
i
KN
i
KN
i
i
C w
C w w
D
D
C w
w
P
w
C w w
C w w
C w w
D
D
C w
C w
w
P
w
C w
C w
C w
w
D
DP
w
DP
w
w
−
−
−
−
−
−
−
−
−
−
−
−
−
>
−
−
−
−
−
=
+
•
−
=
+
•
=
−
•
+
+
•









1
i
w −
(11.37)
Solving the equation, we get
(
)
(
)
(
)
i
i
KN
i
i
w
w
P
w
w
•
=
•



(11.38)
which can be generalized to higher-order models:
2
2
1
2
(
...
)
(
|
...
)
(
...
)
i
i n
i
KN
i
i n
i
i n
i
w
w
w
P
w
w
w
w
w
−+
−+
−
−+
•
=
•



(11.39)
where
2
(
...
)
i n
i
w
w
−+
•

is the number of different words that precede
2...
i n
i
w
w
−+
.
In practice, instead of using a single discount D for all nonzero counts as in Kneser-
Ney smoothing, we can have a number of different parameters (Di ) that depend on the range
of counts:
1
1
1
1
1
1
1
2
1
(
|
...
)
(
...
)
( (
...
))
(
...
)
(
...
)
(
|
...
)
i
KN
i
i n
i
i n
i
i n
i
i n
i
w
i n
i
KN
i
i n
i
P
w
w
w
C w
w
D C w
w
C w
w
w
w
P
w
w
w
γ
−+
−
−+
−+
−+
−+
−
−+
−
−
=
+
+

(11.40)
This modification is motivated by evidence that the ideal average discount for n-grams
with one or two counts is substantially different from the ideal average discount for n-grams
with higher counts.

N-Gram Smoothing
565
11.4.3.
Class n-grams
As discussed in Chapter 2, we can define classes for words that exhibit similar semantic or
grammatical behavior. This is another effective way to handle the data sparsity problem.
Class-based language models have been shown to be effective for rapid adaptation, training
on small data sets, and reduced memory requirements for real-time speech applications.
For any given assignment of a word wi to class ci , there may be many-to-many map-
pings, e.g., a word wi may belong to more than one class, and a class ci may contain more
than one word. For the sake of simplicity, assume that a word wi can be uniquely mapped to
only one class ci . The n-gram model can be computed based on the previous n-1 classes:
P w c
c
P w
c
P c c
c
i
i n
i
i
i
i
i n
i
(
|
...
) =
(
|
) (
|
...
)
 

 

1
1
1
1
(11.41)
where P w c
i
i
(
|
) denotes the probability of word wi given class ci in the current position,
and P c c
c
i
i n
i
(
|
...
)
 

1
1
denotes the probability of class ci given the class history. With such
a model, we can learn the class mapping w→c from either a training text or task knowledge
we have about the application. In general, we can express the class trigram as:
1
2,
1
...
(
)
(
|
) (
|
)
n
i
i
i
i
i
c
c
i
P
P w c P c
c
c
−
−
=  ∏
W
(11.42)
If the classes are nonoverlapping, i.e. a word may belong to only one class, then Eq.
(11.42) can be simplified as:
2,
1
(
)
(
|
) (
|
)
i
i
i
i
i
i
P
P w c P c
c
c
−
−
=∏
W
(11.43)
If we have the mapping function defined, we can easily compute the class n-gram. We
can estimate the empirical frequency of each word C wi
(
), and of each class C ci
(
). We can
also compute the empirical frequency that a word from one class will be followed immedi-
ately by a word from another C c
c
i
i
(
)
1
. As a typical example, the bigram probability of a
word given the prior word (class) can be estimated as
-1
1
1
1
1
(
)
(
)
(
|
)
(
|
) = (
|
)
( |
) =
( )
(
)
i
i
i
i
i
i
i
i
i
i
i
i
i
C w
C c c
P w w
P w c
P w
c
P c c
C c
C c
−
−
−
−

(11.44)
For general-purpose large vocabulary dictation applications, the class-based n-gram
has not significantly improved the recognition accuracy. It is mainly used as a backoff model
to complement the lower-order n-grams for better smoothing. Nevertheless, for limited do-
main speech recognition, the class-based n-gram is very helpful as the class can efficiently
encode semantic information for improved key word spotting and speech understanding ac-
curacy.

566
Language Modeling
11.4.3.1.
Rule-Based Classes
There are a number of ways to cluster words together based on the syntactic-semantic infor-
mation that exists for the language and the task. For example, part-of-speech can be gener-
ally used to produce a small number of classes although this may lead to significantly in-
creased perplexity. Alternatively, if we have domain knowledge, it is often advantageous to
cluster together words that have a similar semantic functional role. For example, if we need
to build a conversational system for air travel information systems, we can group the name of
different airlines such as United Airlines, KLM, and Air China, into a broad airline class.
We can do the same thing for the names of different airports such as JFK, Narita, and
Heathrow, the names of different cities like Beijing, Pittsburgh, and Moscow, and so on.
Such an approach is particularly powerful, since the amount of training data is always lim-
ited. With generalized broad classes of semantically interpretable meaning, it is easy to add a
new airline such as Redmond Air into the classes if there is indeed a start-up airline named
Redmond Air that the system has to incorporate. The system is now able to assign a reason-
able probability to a sentence like “Show me all flights of Redmond Air from Seattle to Bos-
ton” in a similar manner as “Show me all flights of United Airlines from Seattle to Boston”.
We only need to estimate the probability of Redmond Air, given the airline class ci . We can
use the existing class n-gram model that contains the broad structure of the air travel infor-
mation system as it is.
Without such a broad interpretable class, it would be extremely difficult to deal with
new names the system needs to handle, although these new names can always be mapped to
the special class of the unknown word or proper noun classes. For these new words, we can
alternatively map them into a word that has a similar syntactic and semantic role. Thus, the
new word inherits all the possible word trigram relationships that may be very similar to
those of the existing word observed with the training data.
11.4.3.2.
Data-driven Classes
For a general-purpose dictation application, it is impractical to derive functional classes in
the same manner as the domain-specific conversational system that focuses on a narrow task.
Instead, data-driven clustering algorithms have been used to generalize the concept of word
similarities, which is in fact a search procedure to find a class label for each word with a pre-
defined objective function. The set of words with the same class label is called a cluster. We
can use the maximum likelihood criterion as the objective function for a given training cor-
pus and a given number of classes, which is equivalent to minimizing the perplexity for the
training corpus. Once again, the EM algorithm can be used here. Each word can be initial-
ized to a random cluster (class label). At each iteration, every word is moved to the class that
produces the model with minimum perplexity [9, 48]. The perplexity modifications can be
calculated independently, so that each word is evaluated as if all other word classes were
held fixed. The algorithm converges when no single word can be moved to another class in a
way that reduces the perplexity of the clustered n-gram model.

N-Gram Smoothing
567
One special kind of class n-gram models is based on the decision tree as discussed in
Chapter 4. We can use it to create equivalent classes for words in the history, so that can we
have a compact long-distance n-gram language model [2]. The sequential decomposition, as
expressed in Eq. (11.12), is approximated as:
P
P w E w
w
w
P w E
i
n
i
i
i
n
i
(
)
(
|
(
,
,
,
))
(
|
( ))
W
h







1
1
2
1
1

(11.45)
where E(h) denotes a many-to-one mapping function that groups word histories h into some
equivalence classes. It is important to have a scheme that can provide adequate information
about the history so it can serve as a basis for prediction. In addition, it must yield a set of
classes that can be reliably estimated. The decision tree method uses entropy as a criterion in
developing the equivalence classes that can effectively incorporate long-distance informa-
tion. By asking a number of questions associated with each node, the decision tree can clas-
sify the history into a small number of equivalence classes. Each leaf of the tree, thus, has the
probability P w E w
w
i
i
(
|
(
...
))
1
1

that is derived according to the number of times the word
wi is found in the leaf. The selection of questions in building the tree can be infinite. We can
consider, not only the syntactic structure, but also semantic meaning to derive permissible
questions from which the entropy criterion would choose. A full-fledged question set that is
based on detailed analysis of the history is beyond the limit of our current computing re-
sources. As such, we often use the membership question to check each word in the history.
11.4.4.
Performance of n-gram Smoothing
The performance of various smoothing algorithms depends on factors such as the training-set
sizes. There is a strong correlation between the test-set perplexity and word error rate.
Smoothing algorithms leading to lower perplexity generally result in a lower word error rate.
Among all the methods discussed here, the Kneser-Ney method slightly outperforms other
algorithms over a wide range of training-set sizes and corpora, and for both bigram and tri-
gram models. Albeit the difference is not large, the good performance of the Kneser-Ney
smoothing is due to the modified backoff distributions. The Katz algorithms and deleted
interpolation smoothing generally yield the next best performance. All these three smoothing
algorithms perform significantly better than the n-gram model without any smoothing. The
deleted interpolation algorithm performs slightly better than the Katz method in sparse data
situations, and the reverse is true when data are plentiful. Katz’s algorithm is particularly
good at smoothing larger counts; these counts are more prevalent in larger data sets.
Class n-grams offer different kind of smoothing. While clustered n-gram models often
offer no significant test-set perplexity reduction in comparison to the word n-gram model, it
is beneficial to smooth the word n-gram model via either backoff or interpolation methods.
For example, the decision-tree based long-distance class language model does not offer sig-
nificantly improved speech recognition accuracy until it is interpolated with the word tri-
gram. They are effective as a domain-specific language model if the class can accommodate
domain-specific information.

568
Language Modeling
Smoothing is a fundamental technique for statistical modeling, important not only for
language modeling but for many other applications as well. Whenever data sparsity is an
issue, smoothing can help performance, and data sparsity is almost always an issue in statis-
tical modeling. In the extreme case, where there is so much training data that all parameters
can be accurately trained without smoothing, you can almost always expand the model, such
as by moving to a higher-order n-gram model, to achieve improved performance. With more
parameters, data sparsity becomes an issue again, but a proper smoothing model is usually
more accurate than the original model. Thus, no matter how much data you have, smoothing
can almost always help performance, and for a relatively small effort.
11.5.
ADAPTIVE LANGUAGE MODELS
Dynamic adjustment of the language model parameter, such as n-gram probabilities, vocabu-
lary size, and the choice of words in the vocabulary, is important, since the topic of conver-
sation is highly nonstationary [4, 33, 37, 41, 46]. For example, in a typical dictation applica-
tion, a particular set of words in the vocabulary may suddenly burst forth and then become
dormant later, based on the current conversation. Because the topic of the conversation may
change from time to time, the language model should be dramatically different based on the
topic of the conversation. We discuss several adaptive techniques that can improve the qual-
ity of the language model based on the real usage of the application.
11.5.1.
Cache Language Models
To adjust word frequencies observed in the current conversation, we can use a dynamic
cache language model [41]. The basic idea is to accumulate word n-grams dictated so far in
the current document and use these to create a local dynamic n-gram model such as bigram
P
w w
cache
i
i
(
|
)
1 . Because of limited data and nonstationary nature, we should use a lower-
order language model that is no higher than a trigram model P
w w
w
cache
i
i
i
(
|
)


2
1 , which can
be interpolated with the dynamic bigram and unigram. Empirically, we need to normally give
a high weight to the unigram cache model, because it is better trained with the limited data in
the cache.
With
the
cache
trigram,
we
interpolate
it
with
the
static
n-gram
model
P w w
w
s
i
i n
i
(
|
...
)
 

1
1 . The interpolation weight can be made to vary with the size of the
cache.
1
1
1
1
2
1
(
|
...
)
(
|
...
)
(1
)
(
|
)
cache
i
i n
i
c
s
i
i n
i
c
cache
i
i
i
P
w
w
w
P w
w
w
P
w
w
w
λ
λ
−+
−
−+
−
−
−
=
+
−
(11.46)
The cache model is desirable in practice because of its impressive empirical perform-
ance improvement. In a dictation application, we often encounter new words that are not in
the static vocabulary. The same words also tend to be repeated in the same article. The cache
model can address this problem effectively by adjusting the parameters continually as recog-

Adaptive Language Models
569
nition and correction proceed for incrementally improved performance. A noticeable benefit
is that we can better predict words belonging to fixed phrases such as Windows NT, and Bill
Gates.
11.5.2.
Topic-Adaptive Models
The topic can change over time. Such topic or style information plays a critical role in im-
proving the quality of the static language model. For example, the prediction of whether the
word following the phrase the operating is system or table can be improved substantially by
knowing whether the topic of discussion is related to computing or medicine.
Domain or topic-clustered language models split the language model training data ac-
cording to topic. The training data may be divided using the known category information or
using automatic clustering. In addition, a given segment of the data may be assigned to mul-
tiple topics. A topic-dependent language model is then built from each cluster of the training
data. Topic language models are combined using linear interpolation or other methods such
as maximum entropy techniques discussed in Section 11.5.3.
We can avoid any pre-defined clustering or segmentation of the training data. The rea-
son is that the best clustering may become apparent only when the current topic of discussion
is revealed. For example, when the topic is hand-injury to baseball player, the pre-segmented
clusters of topic baseball & hand-injuries may have to be combined. This leads to a union of
the two clusters, whereas the ideal dataset is obtained by the intersection of these clusters. In
general, various combinations of topics lead to a combinatorial explosion in the number of
compound topics, and it appears to be a difficult task to anticipate all the needed combina-
tions beforehand.
We base our determination of the most suitable language model data to build a model
upon the particular history of a given document. For example, we can use it as a query
against the entire training database of documents using information retrieval techniques
[57]. The documents in the database can be ranked by relevance to the query. The most rele-
vant documents are then selected as the adaptation set for the topic-dependent language
model. The process can be repeated as the document is updated.
There are two major steps we need to consider here. The first involves using the avail-
able document history to retrieve similar documents from the database. The second consists
of using the similar document set retrieved in the first step to adapt the general or topic-
independent language model. Available document history depends upon the design and the
requirements of the recognition system. If the recognition system is designed for live-mode
application, where the recognition results must be presented to the user with a small delay,
the available document history will be the history of the document user created so far. On the
other hand, in a recognition system designed for batch operation, the amount of time taken
by the system to recognize speech is of little consequence to the user. In the batch mode,
therefore, a multi-pass recognition system can be used, and the document history will be the
recognizer transcript produced in the current pass.

570
Language Modeling
The well-known information retrieval measure called TFIDF can be used to locate
similar documents in the training database [57]. The term frequency (TF) tf ij is defined as
the frequency of the jth term in the document Di , the unigram count of the term j in the
document Di . The inverse document frequency (IDF) idf j is defined as the frequency of the
jth term over the entire database of documents, which can be computed as:
idf
j
j 
Total number of documents
Number of documents containing term
(11.47)
The combined TF-IDF measure is defined as:
TFIDF
tf
idf
ij
ij
i

log(
)
(11.48)
The combination of TF and IDF can help to retrieve similar documents. It highlights
words of particular interest to the query (via TF), while de-emphasizing common words that
appear across different documents (via IDF). Each document including the query itself, can
be represented by the TFIDF vector. Each element of the vector is the TFIDF value that
corresponds to a word (or a term) in the vocabulary. Similarity between the two documents
is then defined to be the cosine of the angle between the corresponding vectors. Therefore,
we have:
,
2
2
*
(
)
(
) *
(
)
ik
jk
k
i
j
ik
jk
k
k
tfidf
tfidf
Similarity D D
tfidf
tfidf
=



(11.49)
All the documents in the training database are ranked by the decreasing similarity be-
tween the document and the history of the current document dictated so far, or by a topic of
particular interest to the user. The most similar documents are selected as the adaptation set
for the topic-adaptive language model [46].
11.5.3.
Maximum Entropy Models
The language model we have discussed so far combines different n-gram models via linear
interpolation. A different way to combine sources is the maximum entropy approach. It con-
structs a single model that attempts to capture all the information provided by the various
knowledge sources. Each such knowledge source is reformulated as a set of constraints that
the desired distribution should satisfy. These constraints can be, for example, marginal dis-
tributions of the combined model. Their intersection, if not empty, should contain a set of
probability functions that are consistent with these separate knowledge sources. Once the
desired knowledge sources have been incorporated, we make no other assumption about
other constraints, which leads to choosing the flattest of the remaining possibilities, the one
with the highest entropy. The maximum entropy principle can be stated as follows:
 Reformulate different information sources as constraints to be satisfied by the
target estimate.

Adaptive Language Models
571
 Among all probability distributions that satisfy these constraints, choose the one
that has the highest entropy.
Given a general event space {X}, let P(X) denote the combined probability function.
Each constraint is associated with a characteristic function of a subset of the sample space,
f i ( )
X . The constraint can be written as:
P
f
E
i
i
( )
( )
X
X
X


(11.50)
where Ei is the corresponding desired expectation for f i ( )
X , typically representing the re-
quired marginal probability of P(X). For example, to derive a word trigram model, we can
reformulate Eq. (11.50) so that constraints are introduced for unigram, bigram, and trigram
probabilities. These constraints are usually set only where marginal probabilities can be es-
timated from a corpus. For example, the unigram constraint can be expressed as
1
1
1 if
=
( )
0 otherwise
w
w w
f
w

= 

(11.51)
The
desired
value
E w1
can
be
the
empirical
expectation
in
the
training
data,
1 ( )/
w
w training data
f
w
N
∈
, and the associated constraint is
1
1
( )
(
| )
( )
w
w
w
P
P w
f
w
E
=


h
h
h
(11.52)
where h is the word history preceding word w.
We can choose P(X) to diverge minimally from some other known probability func-
tion Q(X), that is, to minimize the divergence function:
P
P
Q
( ) log
( )
( )
X
X
X
X
(11.53)
When Q(X) is chosen as the uniform distribution, the divergence is equal to the nega-
tive of entropy with a constant. Thus minimizing the divergence function leads to maximiz-
ing the entropy. Under a minor consistent assumption, a unique solution is guaranteed to
exist in the form [20]:
(
)
(
)
if
i
i
P
µ
∝∏
X
X
(11.54)
where  i is an unknown constant to be found. To search the exponential family de-
fined by Eq. (11.54) for the  i that make P(X) satisfy all the constraints, an iterative algo-
rithm called generalized iterative scaling exists [20]. It guarantees to converge to the solution

572
Language Modeling
with some arbitrary initial  i. Each iteration creates a new estimate P( )
X , which is im-
proved in the sense that it matches the constraints better than its [20]. One of the most effec-
tive applications of the maximum entropy model is to integrate the cache constraint into the
language model directly, instead of interpolating the cache n-gram with the static n-gram.
The new constraint is that the marginal distribution of the adapted model is the same as the
lower-order n-gram in the cache [56]. In practice, the maximum entropy method has not of-
fered any significant improvement in comparison to the cache modle discussed in Section
11.5.1.
11.6.
PRACTICAL ISSUES
In a speech recognition system, every string of words W = w w
wn
1
2...
taken from the pre-
scribed vocabulary can be assigned a probability, which is interpreted as the a priori prob-
ability to guide the recognition process and is a contributing factor in the determination of
the final transcription from a set of partial hypothesis. Without language modeling, the entire
vocabulary must be considered at every decision point. It is impossible to eliminate many
candidates from consideration, or alternatively to assign higher probabilities to some candi-
dates than others to considerably reduce recognition costs and errors.
11.6.1.
Vocabulary Selection
For most speech recognition systems, an inflected form is considered as a different word.
This is because these inflected forms typically have different pronunciations, syntactic roles,
and usage patterns. So the words work, works, worked, and working are counted as four dif-
ferent words in the vocabulary.
We prefer to have a smaller vocabulary size, since this eliminates potential confusable
candidates in speech recognition, leading to improved recognition accuracy. However, the
limited vocabulary size imposes a severe constraint on the users and makes the system less
flexible. In practice, the percentage of the Out-Of-Vocabulary (OOV) word rate directly
affects the perceived quality of the system. Thus, we need to balance two kinds of errors, the
OOV rate and the word recognition error rate. We can have a larger vocabulary to minimize
the OOV rate if the system resources permit. We can minimize the expected OOV rate of the
test data with a given vocabulary size. A corpus of text is used in conjunction with dictionar-
ies to determine appropriate vocabularies.
The availability of various types and amounts of training data, from various time peri-
ods, affects the quality of the derived vocabulary. Given a collection of training data, we can
create an ordered word list with the lowest possible OOV curve, such that, for any desired
vocabulary size V, a minimum-OOV-rate vocabulary can be derived by taking the most fre-
quent V words in that list. Viewed this way, the problem becomes one of estimating unigram
probabilities of the test distribution, and then ordering the words by these estimates.

Practical Issues
573
As illustrated in Figure 11.6, the perplexity generally increases with the vocabulary
size, albeit it really does not make much sense to compare the perplexity of different vocabu-
lary sizes. There are generally more competing words for a given context when the vocabu-
lary size becomes big, which leads to increased recognition error rate. In practice, this is
offset by the OOV rate, which decreases with the vocabulary size as illustrated in Figure
11.7. If we keep the vocabulary size fixed, we need more than 200,000 words in the vocabu-
lary to have 99.5% English words coverage. For more inflectional languages such as Ger-
man, larger vocabulary sizes are required to achieve coverage similar to that of English.6
Figure 11.6 The perplexity of bigram with different vocabulary sizes. The training set consists
of 500 million words derived from various sources, including newspapers and email. The test
set comes from the whole Microsoft Encarta, an encyclopedia that has a wide coverage of dif-
ferent topics.
In practice, it is far more important to use data from a specific topic or domain, if we
know in what domain the speech recognizer is used. In general, it is also important to con-
sider coverage of a specific time period. We should use training data from that period, or as
close to it as possible. For example, if we know we will talk only about air travel, we benefit
from using the air-travel related vocabulary and language model. This point is well illus-
trated by the fact that the perplexity of the domain-dependent bigram can be reduced by
more than a factor of five over the general-purpose English trigram.
For a user of a speech recognition system, a more personalized vocabulary can be
much more effective than a general fixed vocabulary. The coverage can be dramatically im-
proved as customized new words are added to a starting static vocabulary of 20,000. Typi-
cally, the coverage of such a system can be improved from 93% to more than 98% after
1000-4000 customized words are added to the vocabulary [18].
6 The OOV rate of German is about twice as high as that of English with a 20k-word vocabulary [34].
0
100
200
300
400
500
10k
30k
40k
60k
Vocabulary Size
Perplexity

574
Language Modeling
Figure 11.7 The OOV rate with different vocabulary size. The training set consists of 500 mil-
lion words derived from various sources including newspaper and email. The test set came
from the whole Microsoft Encarta encyclopedia.
In North American general business English, the least frequent words among the most
frequent 60,000 have a frequency of about 1:7,000,000. In optimizing a 60,000-word vo-
cabulary we need to distinguish words with frequency of 1:7,000,000 from those that are
slightly less frequent. To differentiate somewhat reliably between a 1:7,000,000 word and,
say, a 1:8,000,000 word, we need to observe them enough times for the difference in their
counts to be statistically reliable. For constructing a decent vocabulary, it is important that
most such words are ranked correctly. We may need 100,000,000 words to estimate these
parameters. This agrees with the empirical results, in which as more training data is used, the
OOV curve improves rapidly up to 50,000,000 words and then more slowly beyond that
point.
11.6.2.
N-gram Pruning
When high order n-gram models are used, the model sizes typically become too large for
practical applications. It is necessary to prune parameters from n-gram models such that the
relative entropy between the original and the pruned model is minimized. You can chose n-
grams so as to maximize performance (i.e., minimize perplexity) while minimizing the
model size [39, 59, 64].
The criterion to prune n-grams can be based on some well-understood information-
theoretic measure of language model quality. For example, the pruning method by Stolcke
[64] removes some n-gram estimates while minimizing the performance loss. After pruning,
the retained explicit n-gram probabilities are unchanged, but backoff weights are
recomputed. Stolcke pruning uses the criterion that minimizes the distance between the
0
5
10
15
20
25
10k
30k
40k
60k
Vocabulary Size
OOV Rate

Practical Issues
575
distribution embodied by the original model and that of the pruned model based on the
Kullback-Leibler distance defined in Eq. (3.175). Since it is infeasible to maximize over all
possible subsets of n-grams, Stolcke prunning assumes that the n-grams affect the relative
entropy roughly independently, and compute the distance due to each individual n-gram. The
n-grams are thus ranked by their effect on the model entropy, and those that increase relative
entropy the least are pruned accordingly. The main approximation is that we do not consider
possible interactions between selected n-grams, and prune based solely on relative entropy
due to removing a single n-gram. This avoids searching the exponential space of n-gram
subsets.
To compute the relative entropy, KL(p || p'), between the original and pruned n-gram
models p and p′ , there is no need to sum over the vocabulary. By plugging in the terms for
the backed-off estimates, the sum can be factored as shown in Eq. (11.55) for a more
efficient computation.
(
)
(
||
)
( ){ (
| )[log
(
|
)
log
( )
log
(
| )]
[log
( )
log ( )](1
(
| ))}
i
i
i
w
Backoff w h
KL p
p
P h
P w h
P w h
h
P w h
h
h
P w h
α
α
α
∈¬
′
′
′
= −
+
−
′
+
−
−

(11.55)
where the sum in
(
)
(
| )
i
i
i
w
Backoff w h
P w h
∈¬ 
is over all non-backoff estimates. To compute the
revised backoff weights
( )
h
α′
, you can simply drop the term for the pruned n-gram from the
summation (Backoff weight computation is illustrated in Algorithm 11.1).
In practice, pruning is highly effective. Stolcke reported that the trigram model can be
compressed by more than 25% without degrading recognition performance. Comparing the
pruned 4-gram model to the unpruned trigram model, it is better to use pruned 4-grams than
to use a much larger number of trigrams.
11.6.3.
CFG vs n-gram Models
This chapter has discussed two major language models. While CFGs remain one of the most
important formalisms for interpreting natural language, word n-gram models are surprisingly
powerful for domain-independent applications. These two formalisms can be unified for both
speech recognition and spoken language understanding. To improve portability of the do-
main-independent n-gram, it is possible to incorporate domain-specific CFGs into the do-
main-independent n-gram that can improve generalizability of the CFG and specificity of the
n-gram.
The CFG is not only powerful enough to describe most of the structure in spoken lan-
guage, but also restrictive enough to have efficient parsers. P(W) is regarded as 1 or 0 de-
pending upon whether the word sequence is accepted or rejected by the grammar. The prob-
lem is that the grammar is almost always incomplete. A CFG-based system is good only
when you know what sentences to speak, which diminishes the system’s value and usability
of the system. The advantage of CFG’s structured analysis is, thus, nullified by the poor cov-
erage in most real applications. On the other hand, the n-gram model is trained with a large

576
Language Modeling
amount of data, the n-word dependency can often accommodate both syntactic and semantic
structure seamlessly. The prerequisite of this approach is that we have enough training data.
The problem for n-gram models is that we need a lot of data and the model may not be spe-
cific enough.
It is possible to take advantage of both rule-based CFGs and data-driven n-grams.
Let’s consider the following training sentences:
Meeting at three with Zhou Li.
Meeting at four PM with Derek.
If we use a word trigram, we estimate P(Zhou|three with) and P(Derek|PM with), etc.
There is no way we can capture needed long-span semantic information in the training data.
A unified model has a set of CFGs that can capture the semantic structure of the domain. For
the example listed here, we have a CFG for {name} and {time}, respectively. We can use the
CFG to parse the training data to spot all the potential semantic structures in the training
data. The training sentences now look like:
Meeting {at three:TIME} with {Zhou Li:NAME}
Meeting {at four PM:TIME} with {Derek: NAME}
With analyzed training data, we can estimate our n-gram probabilities as usual. We
have probabilities, such as P({name}|{time} with), instead of P(Zhou|three with), which is
more meaningful and accurate. Inside each CFG we also derive P("Zhou Li"|{name}) and
P("four PM”|{time}) from the existing n-gram (n-gram probability inheritance) so that they
are normalized. If we add a new name to the existing {name} CFG, we use the existing n-
gram probabilities to renormalize our CFGs for the new name. The new approach can be
regarded as a standard n-gram in which the vocabulary consists of words and structured
classes, as discussed in Section 11.4.3. The structured class can be very simple, such as
{date}, {time}, and {name}, or can be very complicated, such as a CFG that contains deep
structured information. The probability of a word or class depends on the previous words or
CFG classes.
It is possible to inherit probability from a word n-gram LM. Let’s take word trigram as
our example here. An input utterance
1
2...
n
w w
w
=
W
can be segmented into a sequence
1 2... m
t t
t
=
T
, where each
it is either a word in W or a CFG non-terminal that covers a se-
quence of words
itu in W. The likelihood of W under the segmentation T is, therefore,
-1
-2
(
)
( |
,
)
(
| )
i
i
i
i
t
i
i
i
P
P t
t
t
P u
t
=∏
∏
W, T
(11.56)
(
| ),
it
i
P u
t
the likelihood of generating a word sequence
1
2
[
...
]
i
i
i
i
t
t
t
t k
u
u u
u
=
from the CFG
non-terminal
it , can be inherited from the domain-independent word trigram. We can essen-
tially use the CFG constraint to condition the domain-independent trigram into a domain-

Practical Issues
577
specific trigram. Such a unified language model can dramatically improve cross-domain per-
formance using domain-specific CFGs [66].
In summary, the CFG is widely used to specify the permissible word sequences in
natural language processing when training corpora are unavailable. It is suitable for dealing
with structured command and control applications in which the vocabulary is small and the
semantics of the task is well defined. The CFG either accepts the input sentence or rejects it.
There is a serious coverage problem associated with CFGs. In other words, the accuracy for
the CFG can be extremely high when the test data are covered by the grammar. Unfortu-
nately, unless the task is narrow and well-defined, most users speak sentences that may not
be accepted by the CFG, leading to word recognition errors.
Statistical language models such as trigrams assign an estimated probability to any
word that can follow a given word history without parsing the structure of the history. Such
an approach contains some limited syntactic and semantic information, but these probabili-
ties are typically trained from a large corpus. Speech recognition errors are much more likely
to occur within trigrams and (especially) bigrams that have not been observed in the training
data. In these cases, the language model typically relies on lower-order statistics. Thus, in-
creased n-gram coverage translates directly into improved recognition accuracy, but usually
at the cost of increased memory requirements.
It is interesting to compute the true entropy of the language so that we understand what
a solid lower bound is for the language model. For English, Shannon [60] used human sub-
jects to guess letters by looking at how many guesses it takes people to derive the correct one
based on the history. We can thus estimate the probability of the letters and hence the en-
tropy of the sequence. Shannon computed the per-letter entropy of English with an entropy
of 1.3 bits for 26 letters plus space. This may be an underestimate, since it is based on a sin-
gle text. Since the average length of English written words (including space) is about 5.5
letters, the Shannon estimate of 1.3 bits per letter corresponds to a per-word perplexity of
142 for general English.
Table 11.2 N-gram perplexity and its corresponding speaker-independent speech recognition
word error rate.
Models
Perplexity
Word Error Rate
Unigram Katz
1196.45
14.85%
Unigram Kneser-Ney
1199.59
14.86%
Bigram Katz
176.31
11.38%
Bigram Kneser-Ney
176.11
11.34%
Trigram Katz
95.19
9.69%
Trigram Kneser-Ney
91.47
9.60%
Table 11.2 summarizes the performance of several different n-gram models on a
60,000-word continuous speech dictation application. The experiments used about 260 mil-
lion words from a newspaper such as The Wall Street Journal. The speech recognizer is
based on Whisper described in Chapter 9. As you can see from the table, when the amount of
training data is sufficient, both Katz and Kneser-Ney smoothing offer comparable recogni-

578
Language Modeling
tion performance, although Kneser-Ney smoothing offers a modest improvement when the
amount of training data is limited.
In comparison to Shannon’s estimate of general English word perplexity, the trigram
language for The Wall Street Journal is lower (91.4 vs. 142). This is because the text is
mostly business oriented with a fairly homogeneous style and word usage pattern. For exam-
ple, if we use the trigram language for data from a new domain that is related to personal
information management, the test-set word perplexity can increase to 378 [66].
11.7.
HISTORICAL PERSPECTIVE AND FURTHER READING
There is a large and active area of research in both speech and linguistics. These two distinc-
tive communities worked on the problem with very different paths, leading to the stochastic
language models and the formal language theory. The linguistics community has developed
tools for tasks like parsing sentences, assigning semantic relations to the parts of a sentence,
and so on. Most of these parser algorithms have the same characteristics, that is, they tabu-
late each sub-derivation and reuse it in building any derivation that shares that sub-derivation
with appropriate grammars [22, 65, 67]. They have polynomial complexity with respect to
sentence length because of dynamic programming principles to search for optimal deriva-
tions with respect to appropriate evaluation functions on derivations. There are three well-
known dynamic programming parsers with a worst-case behavior of O(
3
n ), where n is the
number of words in the sentence: the Cocke-Younger-Kasami (CYK) algorithm (a bottom-
up parser, proposed by J. Cocke, D. Younger, and T. Kasami) [32, 67], the Graham-
Harrison-Ruzzo algorithm (bottom-up) [30], and the Earley algorithm (top-down) [21].
On the other hand, the speech community has developed tools to predict the next word
on the basis of what has been said, in order to improve speech recognition accuracy [35].
Neither approach has been completely successful. The formal grammar and the related pars-
ing algorithms are too brittle for comfort and require a lot of human retooling to port from
one domain to another. The lack of structure and deep understanding has taken its toll on
statistical technology’s ability to choose the right words to guide speech recognition.
In addition to those discussed in this chapter, many alternative formal techniques are
available. Augmented context-free grammars are used for natural language to capture gram-
matical natural languages such as agreement and subcategorization. Examples include gener-
alized phrase structure grammars and head-driven phrase structure grammars [26, 53]. You
can further generalize the augmented context-free grammar to the extent that the requirement
of context free becomes unnecessary. The entire grammar, known as the unification gram-
mar, can be specified as a set of constraints between feature structures [62]. Most of these
grammars have only limited success when applied to spoken language systems. In fact, no
practical domain-independent parser of unrestricted text has been developed for spoken lan-
guage systems, partly because disambiguation requires the specification of detailed semantic
information. Analysis of the Susanne Corpus with a crude parser suggests that over 80% of
sentences are structurally ambiguous. More recently, large treebanks of parsed texts have
given impetus to statistical approaches to parsing. Probabilities can be estimated from tree-

Historical Perspective and Further Reading
579
banks or plain text [6, 8, 24, 61] to efficiently rank analyses produced by modified chart
parsing algorithms. These systems have yielded results of around 75% accuracy in assigning
analyses to (unseen) test sentences from the same source as the unambiguous training mate-
rial. Attempts have also been made to use statistical induction to learn the correct grammar
for a given corpus of data [7, 43, 51, 58]. Nevertheless, these techniques are limited to sim-
ple grammars with category sets of a dozen or so non-terminals, or to training on manually
parsed data. Furthermore, even when parameters of the grammar and control mechanism can
be learned automatically from training corpora, the required corpora do not exist or are too
small for proper training. In practice, we can devise grammars that specify directly how rela-
tionships relevant to the task may be expressed. For instance, one may use a phrase-structure
grammar in which nonterminals stand for task concepts and relationships and rules specify
possible expressions of those concepts and relationships. Such semantic grammars have
been widely used for spoken language applications as discussed in Chapter 17.
It is worthwhile to point out that many natural language parsing algorithms are NP-
complete, a term for a class of problems that are suspected to be particularly difficult to
process. For example, maintaining lexical and agreement features over a potentially infinite-
length sentence causes the unification-based formalisms to be NP-complete [3].
Since the predictive power of a general-purpose grammar is insufficient for reasonable
performance, n-gram language models continue to be widely used. A complete proof of
Good-Turing smoothing was presented by Church et al. [17]. Chen and Goodman [13] pro-
vide a detailed study on different n-gram smoothing algorithms. Jelinek’s Eurospeech tuto-
rial paper [35] provides an interesting historical perspective on the community’s efforts to
improve trigrams. Mosia and Giachin’s paper [48] has detailed experimental results on class-
based language models. Class-based model may be based on parts of speech or morphology
[10, 16, 23, 47, 63]. More detailed discussion of the maximum entropy language model can
be found in [5, 36, 42, 44, 52, 55, 56].
One interesting research area is to combine both n-grams and the structure that is pre-
sent in language. A concerted research effort to explore structure-based language model may
be the key for significant progress to occur in language modeling. This can be done as anno-
tated data becomes available. Nasr et al. [50] have considered a new unified language model
composed of several local models and a general model linking the local models together.
The local model used in their system is based on the stochastic FSA, which is estimated from
the training corpora. Other efforts to incorporate structured information are described in [12,
25, 27, 49, 66].
You can find tools to build n-gram language models at the CMU open source Web
site7 that contains the release of CMU’s language modeling toolkit and documentation of
SRI’s language modeling toolkit.8
7 http://www.speech.cs.cmu.edu/sphinx/
8 http://www.speech.sri.com/projects/srilm/download.html

580
Language Modeling
REFERENCES
[1]
Aho, A.V. and J.D. Ullman, The Theory of Parsing, Translation and Compiling,
1972, Englewood Cliffs, NJ, Prentice-Hall.
[2]
Bahl, L.R., et al., "A Tree-Based Statistical Language Model for Natural Language
Speech Recognition," IEEE Trans. on Acoustics, Speech, and Signal Processing,
1989, 37(7), pp. 1001-1008.
[3]
Barton, G., R. Berwick, and E. Ristad, Computational Complexity and Natural
Language, 1987, Cambridge, MA, MIT Press.
[4]
Bellegarda, J., "A Latent Semantic Analysis Framework for Large-Span Language
Modeling," Eurospeech, 1997, Rhodes, Greece pp. 1451-1454.
[5]
Berger, A., S. DellaPietra, and V. DellaPietra, "A Maximum Entropy Approach to
Natural Language Processing," Computational Linguistics, 1996, 22(1), pp. 39-71.
[6]
Black, E., et al., "Towards History-based Grammars: Using Richer Models for
Probabilistic Parsing," Proc. of the Annual Meeting of the Association for Compu-
tational Linguistics, 1993, Columbus, Ohio, USA pp. 31--37.
[7]
Briscoe, E.J., ed. Prospects for Practical Parsing: Robust Statistical Techniques, in
Corpus-based Research into Language: A Feschrift for Jan Aarts, ed. P.d. Haan and
N. Oostdijk, 1994, Amsterdam. 67-95, Rodopi.
[8]
Briscoe, E.J. and J. Carroll, "Generalized Probabilistic LR Parsing of Natural Lan-
guage (Corpora) with Unification-based Grammars," Computational Linguistics,
1993, 19, pp. 25-59.
[9]
Brown, P.F., et al., "Class-Based N-Gram Models of Natural Language," Computa-
tional Linguistics, 1992(4), pp. 467-479.
[10]
Cerf-Danon, H. and M. El-Bèze, "Three Different Probabilistic Language Models:
Comparison and Combination," Proc. of the IEEE Int. Conf. on Acoustics, Speech
and Signal Processing, 1991, Toronto, Canada pp. 297-300.
[11]
Charniak, E., "Statistical Parsing with a Context-Free Grammar and Word Statis-
tics," AAAI-97, 1997, Menlo Park pp. 598-603.
[12]
Chelba, C., A. Corazza, and F. Jelinek, "A Context Free Headword Language
Model" in Proc. of IEEE Automatic Speech Recognition Workshop" 1995, Snow-
bird, Utah, pp. 89--90.
[13]
Chen, S. and J. Goodman, "An Empirical Study of Smoothing Techniques for Lan-
guage Modeling," Proc. of Annual Meeting of the ACL, 1996, Santa Cruz, CA.
[14]
Chomsky, N., Syntactic Structures, 1957, The Hague: Mouton.
[15]
Chomsky, N., Aspects of the Theory of Syntax, 1965, Cambridge, MIT Press.
[16]
Church, K., "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted
Text," Proc. of 2nd Conf. on Applied Natural Language Processing., 1988, Austin,
Texas pp. 136-143.
[17]
Church, K.W. and W.A. Gale, "A Comparison of the Enhanced Good-Turing and
Deleted Estimation Methods for Estimating Probabilities of English Bigrams,"
Computer Speech and Language, 1991, pp. 19-54.

Historical Perspective and Further Reading
581
[18]
Cole, R., et al., Survey of the State of the Art in Human Language Technology, eds.
http://cslu.cse.ogi.edu/HLTsurvey/HLTsurvey.html, 1996, Cambridge University
Press.
[19]
Collins, M., "A New Statistical Parser Based on Bigram Lexical Dependencies,"
ACL-96, 1996, pp. 184-191.
[20]
Darroch, J.N. and D. Ratcliff, "Generalized Iterative Scaling for Log-Linear Mod-
els," The Annals of Mathematical Statistics, 1972, 43(5), pp. 1470-1480.
[21]
Earley, J., An Efficient Context-Free Parsing Algorithm, PhD Thesis
1968, Car-
negie Mellon University, Pittsburgh.
[22]
Earley, J., "An Efficient Context-Free Parsing Algorithm," Communications of the
ACM, 1970, 6(8), pp. 451-455.
[23]
El-Bèze, M. and A.-M. Derouault, "A Morphological Model for Large Vocabulary
Speech Recognition," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal
Processing, 1990, Albuquerque, NM pp. 577-580.
[24]
Fujisaki, T., et al., "A probabilistic parsing method for sentence disambiguation,"
Proc. of the Int. Workshop on Parsing Technologies, 1989, Pittsburgh.
[25]
Galescu, L., E.K. Ringger, and a.F. Allen, "Rapid Language Model Development
for New Task Domains," Proc. of the ELRA First Int. Conf. on Language Re-
sources and Evaluation (LREC), 1998, Granada, Spain.
[26]
Gazdar, G., et al., Generalized Phrase Structure Grammars, 1985, Cambridge,
MA, Harvard University Press.
[27]
Gillett, J. and W. Ward, "A Language Model Combining Trigrams and Stochastic
Context-Free Grammars," Int. Conf. on Spoken Language Processing, 1998, Syd-
ney, Australia.
[28]
Good, I.J., "The Population Frequencies of Species and the Estimation of Popula-
tion Parameters," Biometrika, 1953, pp. 237-264.
[29]
Goodman, J., Parsing Inside-Out, PhD Thesis in Computer Science 1998, Harvard
University, Cambridge.
[30]
Graham, S.L., M.A. Harrison, and W. L.Ruzzo, "An Improved Context-Free Rec-
ognizer," ACM Trans. on Programming Languages and Systems, 1980, 2(3), pp.
415-462.
[31]
Hindle, D. and M. Rooth, "Structural Ambiguity and Lexical Relations," DARPA
Speech and Natural Language Workshop, 1990, Hidden Valley, PA, Morgan
Kaufmann.
[32]
Hopcroft, J.E. and J.D. Ullman, Introduction to Automata Theory, Languages, and
Computation, 1979, Reading, MA, Addision Wesley.
[33]
Iyer, R., M. Ostendorf, and J.R. Rohlicek, "Language Modeling with Sentence-
Level Mixtures," Proc. of the ARPA Human Language Technology Workshop,
1994, Plainsboro, NJ pp. 82-86.
[34]
Jardino, M., "Multilingual Stochastic N-gram Class Language Models," Proc. of
the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1996, Atlanta, GA
pp. 161-163.

582
Language Modeling
[35]
Jelinek, F., "Up From Trigrams! The Struggle for Improved Language Models" in
Proc. of the European Conf. on Speech Communication and Technology 1991,
Genoa, Italy, pp. 1037-1040.
[36]
Jelinek, F., Statistical Methods for Speech Recognition, 1998, Cambridge, MA,
MIT Press.
[37]
Jelinek, F., et al., "A dynamic language model for speech recognition" in Proc. of
the DARPA Speech and Natural Language Workshop 1991, Asilomar, CA.
[38]
Katz, S.M., "Estimation of Probabilities from Sparse Data for the Language Model
Component of a Speech Recognizer," IEEE Trans. Acoustics, Speech and Signal
Processing, 1987(3), pp. 400-401.
[39]
Kneser, R., "Statistical Language Modeling using a Variable Context" in Proc. of
the Int. Conf. on Spoken Language Processing 1996, Philadelphia, PA, pp. 494.
[40]
Kneser, R. and H. Ney, "Improved Backing-off for M-gram Language Modeling" in
Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing 1995, De-
troit, MI, pp. 181-184.
[41]
Kuhn, R. and R.D. Mori, "A Cache-Based Natural Language Model for Speech
Recognition," IEEE Trans. on Pattern Analysis and Machine Intelligence, 1990(6),
pp. 570-582.
[42]
Lafferty, J.D. and B. Suhm, "Cluster Expansions and Iterative Scaling for Maxi-
mum Entropy Language Models" in Maximum Entropy and Bayesian Methods, K.
Hanson and R. Silver, eds. 1995, Kluwer Academic Publishers.
[43]
Lari, K. and S.J. Young, "Applications of Stochastic Context-free Grammars Using
the Inside-Outside Algorithm," Computer Speech and Language, 1991, 5(3), pp.
237-257.
[44]
Lau, R., R. Rosenfeld, and S. Roukos, "Trigger-Based Language Models: A Maxi-
mum Entropy Approach," Int. Conf. on Acoustics, Speech and Signal Processing,
1993, Minneapolis, MN pp. 108-113.
[45]
Magerman, D.M. and M.P. Marcus, "Pearl: A Probabilistic Chart Parser," Proc. of
the Fourth DARPA Speech and Natural Language Workshop, 1991, Pacific Grove,
California.
[46]
Mahajan, M., D. Beeferman, and X.D. Huang, "Improved Topic-Dependent Lan-
guage Modeling Using Information Retrieval Techniques," IEEE Int. Conf. on
Acoustics, Speech and Signal Processing, 1999, Phoenix, AZ pp. 541-544.
[47]
Maltese, G. and F. Mancini, "An Automatic Technique to Include Grammatical and
Morphological Information in a Trigram-based Statistical Language Model," Proc.
of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1992, San
Francisco, CA pp. 157-160.
[48]
Moisa, L. and E. Giachin, "Automatic Clustering of Words for Probabilistic Lan-
guage Models" in Proc. of the European Conf. on Speech Communication and
Technology 1995, Madrid, Spain, pp. 1249-1252.
[49]
Moore, R., et al., "Combining Linguistic and Statistical Knowledge Sources in
Natural-Language Processing for ATIS," Proc. of the ARPA Spoken Language Sys-

Historical Perspective and Further Reading
583
tems Technology Workshop, 1995, Austin, Texas, Morgan Kaufmann, Los Altos,
CA.
[50]
Nasr, A., et al., "A Language Model Combining N-grams and Stochastic Finitie
State Automata," Proc. of the Eurospeech, 1999, Budapest, Hungary pp. 2175-
2178.
[51]
Pereira, F.C.N. and Y. Schabes, "Inside-Outside Reestimation from Partially
Bracketed Corpora," Proc. of the 30th Annual Meeting of the Association for Com-
putational Linguistics, 1992 pp. 128-135.
[52]
Pietra, S.A.D., et al., "Adaptive Language Model Estimation using Minimum Dis-
crimination Estimation," Proc. of the IEEE Int. Conf. on Acoustics, Speech and
Signal Processing, 1992, San Francisco, CA pp. 633-636.
[53]
Pollard, C. and I.A. Sag, Head-Driven Phrase Structure Grammar, 1994, Chicago,
University of Chicago Press.
[54]
Pullum, G. and G. Gazdar, "Natural Languages and Context-Free Languages," Lin-
guistics and Philosophy, 1982, 4, pp. 471-504.
[55]
Ratnaparkhi, A., S. Roukos, and R.T. Ward, "A Maximum Entropy Model for Pars-
ing," Proc. of the Int. Conf. on Spoken Language Processing, 1994, Yokohama,
Japan pp. 803--806.
[56]
Rosenfeld, R., Adaptive Statistical Language Modeling: A Maximum Entropy Ap-
proach, Ph.D. Thesis in School of Computer Science 1994, Carnegie Mellon Uni-
versity, Pittsburgh, PA.
[57]
Salton, G. and M.J. McGill, Introduction to Modern Information Retrieval, 1983,
New York, McGraw-Hill.
[58]
Schabes, Y., M. Roth, and R. Osborne, "Parsing the Wall Street Journal with the
Inside-Outside Algorithm," Proc. of the Sixth Conf. of the European Chapter of the
Association for Computational Linguistics, 1993 pp. 341-347.
[59]
Seymore, K. and R. Rosenfeld, "Scalable Backoff Language Models," Proc. of the
Int. Conf. on Spoken Language Processing, 1996, Philadelphia, PA pp. 232.
[60]
Shannon, C.E., "Prediction and Entropy of Printed English," Bell System Technical
Journal, 1951, pp. 50-62.
[61]
Sharman, R., F. Jelinek, and R.L. Mercer, "Generating a Grammar for Statistical
Training," Proc. of the Third DARPA Speech and Natural Language Workshop,
1990, Hidden Valley, Pennsylvania pp. 267-274.
[62]
Shieber, S.M., An Introduction to Unification-Based Approaches to Grammars,
1986, Cambridge, UK, CSLI Publication, Leland Stanford Junior University.
[63]
Steinbiss, V., et al., "A 10,000-word Continuous Speech Recognition System,"
Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1990,
Albuquerque, NM pp. 57-60.
[64]
Stolcke, A., "Entropy-based Pruning of Backoff Language Models," DARPA
Broadcast News Transcription and Understanding Workshop, 1998, Lansdowne,
VA.
[65]
Tomita, M., "An Efficient Augmented-Context-Free Parsing Algorithm," Computa-
tional Linguistics, 1987, 13(1-2), pp. 31-46.

584
Language Modeling
[66]
Wang, Y., M. Mahajan, and X. Huang, "A Unified Context-Free Grammar and N-
Gram Model for Spoken Language Processing," Int. Conf. on Acoustics, Speech
and Signal Processing, 2000, Istanbul, Turkey pp. 1639-1642.
[67]
Younger, D.H., "Recognition and Parsing of Context-Free Languages in Time n^3,"
Information and Control, 1967, 10, pp. 189-208.

585
C H A P T E R
1 2
Basic Search Algorithms
Continuous speech recognition (CSR) is both
a pattern recognition and search problem. As described in previous chapters, the acoustic
and language models are built upon a statistical pattern recognition framework. In speech
recognition, making a search decision is also referred to as decoding. In fact, decoding got
its name from information theory (see chapter 3) where the idea is to decode a signal that
has presumably been encoded by the source process and has been transmitted through the
communication channel, as depicted in Figure 1.1. In this chapter, we first review the gen-
eral decoder architecture that is based on such a source-channel model.
The decoding process of a speech recognizer is to find a sequence of words whose cor-
responding acoustic and language models best match the input signal. Therefore, the process
of such a decoding process with trained acoustic and language models is often referred to as
just a search process. Graph search algorithms have been explored extensively in the fields
of artificial intelligence, operation research, and game theory. In this chapter we present
several basic search algorithms, which serve as the basic foundation for CSR.

586
Basic Search Algorithms
The complexity of a search algorithm is highly correlated with the search space, which
is determined by the constraints imposed by the language models. We discuss the impact of
different language models, including finite-state grammars, context-free grammars, and n-
grams.
Speech recognition search is usually done with the Viterbi or A* stack decoders. The
reasons for choosing the Viterbi decoder involve arguments that point to speech as a left-to-
right process and to the efficiencies afforded by a time-synchronous process. The reasons for
choosing a stack decoder involve its ability to more effectively exploit the A* criteria, which
holds out the hope of performing an optimal search as well as the ability to handle huge
search spaces. Both algorithms have been successfully applied to various speech recognition
systems. The relative meritsof both search algorithms were quite controversial in the 1980s.
Lately, with the help of efficient pruning techniques, Viterbi beam search (described in de-
tail in Chapter 13) has been the preferred method for almost all speech recognition tasks.
Stack decoding, on the other hand, remains an important strategy to uncover the n-best and
lattice structures.
12.1.
BASIC SEARCH ALGORITHMS
Search is a subject of interest in artificial intelligence and has been well studied for
expert systems, game playing, and information retrieval. We discuss several general graph
search methods that are fundamental to spoken language systems. Although the basic con-
cept of graph search algorithms is independent of any specific task, the efficiency often de-
pends on how we exploit domain-specific knowledge.
The idea of search implies moving around, examining things, and making decisions
about whether the sought object has yet been found. In general, search problems can be rep-
resented using the state-space search paradigm. It is defined by a triplet (S, O, G), where S
is a set of initial states, O a set of operators (or rules) applied on a state to generate a transi-
tion with its corresponding cost to another state, and G a set of goal states. A solution in the
state-space search paradigm consists in finding a path from an initial state to a goal state.
The state-space representation is commonly identified with a directed graph in which each
node corresponds to a state and each arc to an application of an operator (or a rule), which
transitions from one state to another. Thus the state-space search is equivalent to searching
through the graph with some objective function.
Before we present any graph search algorithms, we need to remind the readers of the
importance of the dynamic programming algorithm described in Chapter 8. Dynamic pro-
gramming should be applied whenever possible and as early as possible because (1) unlike
any heuristics, it will not sacrifice optimality; (2) it can transform an exponential search into
a polynomial search.
12.1.1.
General Graph Searching Procedures
Although dynamic programming is a powerful polynomial search algorithm, many interest-
ing problems cannot be handled by it. A classical example is the traveling-salesman’s prob-

Basic Search Algorithms
587
lem. We need to find a shortest-distance tour, starting at one of many cites, visiting each city
exactly once, and returning to the starting city. This is one of the most famous problems in
the NP-hard class [1, 32]. Another classical example is the N-queens problem (typically 8-
queens), where the goal is to place N queens on an N
N
×
chessboard in such a way that no
queen can capture any other queen; i.e., there is no more than one queen in any given row,
column, or diagonal. Many of these puzzles have the same characteristics. As we know, the
best algorithms currently known for solving the NP-hard problem are exponential in the
problem size. Most graph search algorithms try to solve those problems using heuristics to
avoid or moderate such a combinatorial explosion.
S
E
C
A
B
F
D
3
2
4
3
3
3
4
5
5
G
Figure 12.1 A highway distance map for cities S, A, B, C, D, E, F and G. The salesman needs
to find a path to travel from city S to city G [42].
Let’s start our discussion of graph search procedure with a simple city-traveling prob-
lem [42]. Figure 12.1 shows a highway distance map for all the cities. A salesman John
needs to travel from the starting city S to the end city G. One obvious way to find a path is
to derive a graph that allows orderly exploration of all possible paths. Figure 12.2 shows the
graph that traces out all possible paths in the city-distance map shown in Figure 12.1. Al-
though the city-city connection is bi-directional, we should note that the search graph in this
case must not contain cyclic paths, because they would not lead to any progress in this sce-
nario.
If we define the search space as the potential number of nodes (states) in the graph
search procedure, the search space for finding the optimal state sequence in the Viterbi algo-
rithm (described in Chapter 8) is N
T
×
, where N is the number of states for the HMM and T
is length of the observation. Similary, the search space for John’s traveling problem will be
27.
Another important measure for a search graph is the branching factor, defined as the
average number of successors for each node. Since the number of nodes of a search graph
(or tree) grows exponentially with base equal to this branching factor, we certainly need to
watch out for search graphs (or trees) with a large branching factor. Sometimes they can be
too big to handle (even infinite, as in game playing). We often trade the optimal solution for
improved performance and feasibility. That is, the goal for such search problems is to find

588
Basic Search Algorithms
one satisfactory solution instead of the optimal one. In fact, most AI (artifical intelligence)
search problems belong to this category.
The search tree in Figure 12.2 may be implemented either explicitly or implicitly. In
an explicit implementation, the nodes and arcs with their corresponding distances (or costs)
are explicitly specified by a table. However, an explicit implementation is clearly impracti-
cal for large search graphs and impossible for those with infinite nodes. In practice, most
parts of the graph may never be explored before a solution is found. Therefore, a sensible
strategy is to dynamically generate the search graph. The part that becomes explicit is often
referred to as an active search space. Throughout the discussion here, it is important to keep
in mind this distinction between the implicit search graph that is specified by the start node
S and the explicit partial search graphs that are actually constructed by the search algo-
rithm.
G
G
G
G
S
B
A
B
C
A
D
F
E
C
E
D
E
F
D
E
C
A
D
B
F
C
F
Figure 12.2 The search tree (graph) for the salesman problem illustrated in Figure 12.1. The
number next to each goal nodes is the accumulated distance from start city to end city [42].
To expand the tree, the term successor operator (or move generator, as it is often
called in game search) is defined as an operator that is applied to a node to generate all of
the successors of that node and to compute the distance associated with each arc. The suc-
cessor operator obviously depends on the topology (or rules) of the problem space. Expand-
ing the starting node S, and successors of S, ad infinitum, gradually makes the implicitly
defined graph explicit. This recursive procedure is straightforward, and the search graph
(tree) can be constructed without the extra book-keeping. However, this process would only
generate a search tree where the same node might be generated as a part of several possible
paths.
For example, node E is being generated in four different paths. If we are interested in
finding an optimal path to travel from S to G, it is more efficient to merge those different
paths that lead to the same node E. We can pick the shortest path up to C, since everything

Basic Search Algorithms
589
following E is the same for the rest of the paths. This is consistent with the dynamic pro-
gramming principle – when looking for the best path from S to G, all partial paths from S to
any node E, other than the best path from S to E, should be discarded. The dynamic pro-
gramming merge also eliminates cyclic paths implicitly, since a cyclic path cannot be the
shortest path. Performing this extra bookkeeping (merging different paths leading into the
same node) generates a search graph rather than a search tree.
Although a graph search has the potential advantage over a tree search of being more
efficient, it does require extra bookkeeping. Whether this effort is justified depends on the
individual problem one has to address.
Figure 12.3 A bad case for bi-directional search, where the forward search and the backward
search crossed each other [42].
Most search strategies search in a forward direction, i.e., build the search graph (or
tree) by starting with the initial configuration (the starting state S) from the root. In the gen-
eral AI literature, this is referred to as forward reasoning [43], because it performs rule-base
reasoning by matching the left side of rules first. However, for some specific problem do-
mains, it might be more efficient to use backward reasoning [43], where the search graph is
built from the bottom up (the goal state G). Possible scenarios include:
 There are more initial states than goal states. Obviously it is easy to start with a
small set of states and search for paths leading to one of the bigger sets of states.
For example, suppose the initial state S is the hometown for John in the city-
traveling problem in Figure 12.1 and the goal state G is an unfamiliar city for
him. In the absence of a map, there are certainly more locations (neighboring
cities) that John can identify as being close1 to his home city S than those he can
identify as being close to an unfamiliar location. In a sense, all of those locations
being identified as close to John’s home city S are equivalent to the initial state
S. This means John might want to consider reasoning backward from the unfa-
miliar goal city G for the trip planning.
1 Being close means that, once John reaches one of those neighboring cities, he can easily remember the best path to
return home. It is similar to the killer book for chess play. Once the player reaches a particular board configuration,
he can follow the killer book for moves that can guarantee a victory.
Backward search explored area
Forward search explored area

590
Basic Search Algorithms
 The branching factor for backward reasoning is smaller than that for forward
reasoning. In this case it makes sense to search in the direction with lower
branching factor.
It is in principle possible to search from both ends simultaneously, until two partial
paths meet somewhere in the middle. This strategy is called bi-directional search [43]. Bi-
directional search seems particularly appealing if the number of nodes at each step grows
exponentially with the depth that needs to be explored. However, sometimes bi-directional
search can be devastating. The two searches may cross each other, as illustrated in Figure
12.3.
ALGORITHM 12.1: THE GRAPH-SEARCH ALGORITHM
1. Initialization: Put S in the OPEN list and create an initially empty CLOSE list
2. If the OPEN list is empty, exit and declare failure.
3. Pop up the first node N in the OPEN list, remove it from the OPEN list and put it into the
CLOSE list.
4. If node N is a goal node, exit successfully with the solution obtained by tracing back the path
along the pointers from N to S.
5. Expand node N by applying the successor operator to generate the successor set SS(N) of
node N. Be sure to eliminate the ancestors of N, from SS(N).
6.
(
)
SS N
ν
∀∈
do
6a. (optional) If
OPEN
ν ∈
and the accumulated distance of the new path is smaller
than that for the one in the OPEN list, do
(i) change the traceback (parent) pointer of ν to N and adjust the accumulated
distance for ν .
(ii) goto Step 7.
6b. (optional) If
CLOSE
ν ∈
and the accumulated distance of the new path is small
than the partial path ending atν in the CLOSE list, do
(i) change the trace back (parent) pointer of ν to N and adjust the accumu-
lated distance for all paths that containν .
(ii) goto Step 7.
6c. Create a pointer pointing to N and push it into the OPEN list
7. Reorder the OPEN list according to search strategy or some heuristic measurement.
8. Goto Step 2.
The process of explicitly generating part of an implicitly defined graph forms the es-
sence of our general graph search procedure. The procedure is summarized in Algorithm
12.1. It maintains two lists: OPEN, which stores the nodes waiting for expansion; and
CLOSE, which stores the already expanded nodes . Steps 6a and 6b are basically the book-
keeping process to merge different paths going into the same node by picking the one that
has the minimum distance. Step 6a handles the case where ν is in the OPEN list and thus is

Basic Search Algorithms
591
not expanded. The merging process is straightforward, with a single comparison and change
of trace-back pointer if necessary. However, when ν is in the CLOSE list and thus is al-
ready expanded in Step 6b, the merging requires additional forward propagation of the new
score if the current path is found to be better than the best subpath already in the CLOSE list.
This forward propagation could be very expensive. Fortunately, most of the search strategy
can avoid such a procedure if we know that the already expanded node must belong in the
best path leading to it. We discuss this in Section 12.5.
As described earlier, it may not be worthwhile to perform bookkeeping for a graph
search, so Steps 6a and 6b are optional. If both steps are omitted, the graph search algorithm
described above becomes a tree search algorithm. To illustrate different search strategies,
tree search is used as the basic graph search algorithm in the sections that follows. However,
you should note that all the search methods described here could be easily extended to graph
search with the extra bookkeeping (merging) process as illustrated in Steps 6a and 6b of
Algorithm 12.1.
12.1.2.
Blind Graph Search Algorithms
If the aim of the search problem is to find an acceptable path instead of the best path, blind
search is often used. Blind search treats every node in the OPEN list the same and blindly
decides the order to be expanded without using any domain knowledge. Since blind search
treats every node equally, it is often referred to as uniform search or exhaustive search, be-
cause it exhaustively tries out all possible paths. In AI, people are typically not interested in
blind search. However, it does provide a lot of insight into many sophisticated heuristic
search algorithms. You should note that blind search does not expand nodes randomly. In-
stead, it follows some systematic way to explore the search graph. Two popular types of
blind search are depth-first search and breadth-first search.
12.1.2.1.
Depth-First Search
When we are in a maze, the most natural way to find a way out is to mark the branch we
take whenever we reach a branching point. The marks allow us to go back to a choice point
with an unexplored alternative; withdraw the most recently made choice and undo all conse-
quences of the withdrawn choice whenever a dead end is reached. Once the alternative
choice is selected and marked, we go forward based on the same procedure. This intuitive
search strategy is called backtracking. The famous N-queens puzzle [32] can be handily
solved by the backtracking strategy.
Depth-first search picks an arbitrary alternative at every node visited. The search
sticks with this partial path and works forward from the partial path. Other alternatives at the
same level are ignored completely (for the time being) in the hope of finding a solution
based on the current choice. This strategy is equivalent to ordering the nodes in the OPEN
list by their depth in the search graph (tree). The deepest nodes are expanded first and nodes
of equal depth are ordered arbitrarily.

592
Basic Search Algorithms
Although depth-first search hopes the current choice leads to a solution, sometimes the
current choice could lead to a dead-end (a node which is neither a goal node nor can be ex-
panded further). In fact, it is desirable to have many short dead-ends. Otherwise the algo-
rithm may search for a very long time before it reaches a dead-end, or it might not ever
reach a solution if the search space is infinite. When the search reaches a dead-end, it goes
back to the last decision point and proceeds with another alternative.
Figure 12.4 shows all the nodes being expanded under the depth-first search algorithm
for the city-traveling problem illustrated in Figure 12.1. The only differences between the
graph search and the depth-first search algorithms are:
1. The graph search algorithm generates all successors at a time (although all
except one are ignored first), while backtracking generates only one succes-
sor at a time.
2. The graph search, when successfully finding a path, saves only one path from
the starting node to the goal node, while depth-first search in general saves
the entire record of the search graph.
G
G
G
G
S
B
A
B
C
A
D
F
E
C
E
D
E
F
D
E
C
A
D
B
F
C
F
Figure 12.4 The node-expanding procedure of the depth-first search for the path search prob-
lem in Figure 12.1. When it fails to find the goal city in node C, it backtracks to the parent and
continues the search until it finds the goal city. The gray nodes are those that explored. The
dotted nodes are not visited during the search [42].
Depth-first search could be dangerous because it might search an impossible path that
is actually an infinite dead end. To prevent exploring of paths that are too long, a depth
bound can be placed to constraint the nodes to be expanded, and any node reaching that
depth limit is treated as a terminal node (as if it had no successor).
The general graph search algorithm can be modified into a depth-first search algorithm
as illustrated in Algorithm 12.2.

Basic Search Algorithms
593
ALGORITHM 12.2 THE DEPTH-FIRST SEARCH ALGORITHM
1. Initialization: Put S in the OPEN list and create an initially empty the CLOSE list
2. If the OPEN list is empty, exit and declare failure.
3. Pop up the first node N in the OPEN list, remove it from the OPEN list and put it into the
CLOSE list.
4. If node N is a goal node, exit successfully with the solution obtained by tracing back the path
along the pointers from N to S.
4a. If the depth of node N is equal to the depth bound, goto Step 2.
5. Expand node N by applying the successor operator to generate the successor set SS(N) of
node N. Be sure to eliminate the ancestors of N, from SS(N).
6.
(
)
SS N
ν
∀∈
do
6c. Create a pointer pointing to N and push it into the OPEN list
7. Reorder the the OPEN list in descending order of the depth of the nodes.
8. Goto Step 2.
12.1.2.2.
Breadth-first Search
One natural alternative to the depth frist search strategy is breadth-first search. Breadth-first
search examines all the nodes on one level before considering any of the nodes on the next
level (depth). As shown in Figure 12.5, node B would be examined just after node A. The
search moves on level-by-level, finally discovering G on the fourth level.
G
G
G
G
S
B
A
B
C
A
D
F
E
C
E
D
E
F
D
E
C
A
D
B
F
C
F
Figure 12.5 The node-expanding procedure of a breadth-first search for the path search prob-
lem in Figure 12.1. It searches through each level until the goal is identified. The gray nodes
are those that are explored. The dotted nodes are not visited during the search [42].
1
2
3
4

594
Basic Search Algorithms
Breadth-first search is guaranteed to find a solution if one exists, assuming that a finite
number of successors (branches) always follow any node. The proof is straightforward. If
there is a solution, its path length must be finite. Let’s assume the length of the solution is
M. Breadth-first search explores all paths of the same length increasingly. Since the number
of paths of fixed length N is always finite, it eventually explores all paths of length M. By
that time it should find the solution.
It is also easy to show that a breadth-first search can work on a search tree (graph)
with infinite depth on which an unconstrained depth-first search will fail. Although a
breadth-first might not find a shortest-distance path for the city-travel problem, it is guaran-
teed to find the one with fewest cities visited (minimum-length path). In some cases, it is a
very desirable solution. On the other hand, a breadth-first search may be highly inefficient
when all solutions leading to the goal node are at approximately the same depth. The
breadth-first search algorithm is summarized in Algorithm 12.3.
ALGORITHM 12.3: THE BREADTH-FIRST SEARCH ALGORITHM
1. Initialization: Put S in the OPEN list and create an initially empty the CLOSE list
2. If the OPEN list is empty, exit and declare failure.
3. Pop up the first node N in the OPEN list, remove it from the OPEN list and put it into the
CLOSE list.
4. If node N is a goal node, exit successfully with the solution obtained by tracing back the path
along the pointers from N to S.
5. Expand node N by applying the successor operator to generate the successor set SS(N) of
node N. Be sure to eliminate the ancestors of N, from SS(N).
6.
(
)
SS N
ν
∀∈
do
6c. Create a pointer pointing to N and push it into the OPEN list
7. Reorder the OPEN list in increasing order of the depth of the nodes
8. Goto Step 2.
12.1.3.
Heuristic Graph Search
Blind search methods, like depth-first search and breadth-first search, have no sense (or
guidance) of where the goal node lies ahead. Consequently, they often spend a lot of time
searching in hopeless directions. If there is guidance, the search can move in the direction
that is more likely to lead to the goal. For example, you may want to find a driving route to
the World Trade Center in New York. Without a map at hand, you can still use a straight-
line distance estimated by eye as a hint to see if you are closer to the goal (World Trade
Center). This hill-climbing style of guidance can help you to find the destination much more
efficiently.
Blind search only finds one arbitrary solution instead of the optimal solution. To find
the optimal solution with depth-first or breadth-first search, you must not stop searching
when the first solution is discovered. Instead, the search needs to continue until it reaches all

Basic Search Algorithms
595
the solutions, so you can compare them to pick the best. This strategy for finding the optimal
solution is called British Museum search or brute-force search. Obviously, it is unfeasible
when the search space is large. Again, to conduct selective search and yet still be able to find
the optimal solution, some guidance on the search graph is necessary.
The guidance obviously comes from domain-specific knowledge. Such knowledge is
usually referred to as heuristic information, and search methods taking advantage of it are
called heuristic search methods. There is usually a wide variety of different heuristics for
the problem domain. Some heuristics can reduce search effort without sacrificing optimality,
while other can greatly reduce search effort but provide only sub-optimal solutions. In most
practical problems, the choice of different heuristics is usually a tradeoff between the quality
of the solution and the cost of finding the solution.
Heuristic information works like an evaluation function
(
)
h N
that maps each node N
to a real number, and which serves to indicate the relative goodness (or cost) of continuing
the search path from that node. Since in our city-travel problem, straight-line distance is a
natural way of measuring the goodness of a path, we can use the heuristic function
(
)
h N
for
the distance evaluation as:
h(N)=Heuristic estimate of the remaining distance from node N to goal G
(12.1)
Since
(
)
g N , the distance of the partial path to the current node N is generally known, we
have:
g(N)=The distance of the partial path already traveled from root S to node N
(12.2)
We can define a new heuristic function,
(
)
f N , which estimates the total distance for the
path (not yet finished) going through node N.
(
)
(
)
(
)
f N
g N
h N
=
+
(12.3)
A heuristic search method basically uses the heuristic function
(
)
f N
to re-order the
OPEN list in the Step 7 of Algorithm 12.1. The node with the best heuristic value is ex-
plored first (expanded first). Some heuristic search strategies also prune some unpromising
partial paths forever to save search space. This is why heuristic search is often referred to as
heuristic pruning.
The choice of the heuristic function is critical to the search results. If we use one that
overestimates the distance of some nodes, the search results may be suboptimal. Therefore,
heuristic functions that do not overestimate the distance are often used in search methods
aiming to find the optimal solution.
To close this section, we describe two of the most popular heuristic search methods: .
best-first (or A* Search) [32, 43] and beam search [43]. They are widely used in many com-
ponents of spoken language systems.

596
Basic Search Algorithms
12.1.3.1.
Best-First (A* Search)
Once we have a reasonable heuristic function to evaluate the goodness of each node in the
OPEN list, we can explore the best node (the node with smallest
(
)
f N
value) first, since it
offers the best hope of leading to the best path. This natural search strategy is called best-
first search. To implement best-first search based on the Algorithm 12.1, we need to first
evaluate
(
)
f N
for each successor before putting the successors in the OPEN list in Step 6.
We also need to sort the elements in the OPEN list based on
(
)
f N
in Step 7, so that the best
node is in the front-most position waiting to be expanded in Step 3. The modified procedure
for performing best-first search is illustrated in Algorithm 12.4. To avoid duplicating nodes
in the OPEN list, we include Steps 6a and 6b to take advantage of the dynamic programming
principle. They perform the needed bookkeeping process to merge different paths leading
into the same node.
ALGORITHM 12.4: THE BEST-FIRST SEARCH ALGORITHM
1. Initialization: Put S in the OPEN list and create an initially empty the CLOSE list
2. If the OPEN list is empty, exit and declare failure.
3. Pop up the first node N in the OPEN list, remove it from the OPEN list and put it into the
CLOSE list.
4. If node N is a goal node, exit successfully with the solution obtained by tracing back the path
along the pointers from N to S.
5. Expand node N by applying the successor operator to generate the successor set SS(N) of
node N. Be sure to eliminate the ancestors of N, from SS(N).
6.
(
)
SS N
ν
∀∈
do
6a. (optional) If
OPEN
ν ∈
and the accumulated distance of the new path is smaller
than that for the one in the the OPEN list, do
(i) Change the traceback (parent) pointer of ν
to N and adjust the
accumulated distance for ν .
(ii) Evaluate heuristic function
( )
f ν
for ν and goto Step 7.
6b. (optional) If
CLOSE
ν ∈
and the accumulated distance of the new path is small
than the partial path ending atν in the the CLOSE list,
(i) Change the traceback (parent) pointer of ν to N and adjust the accumu-
lated distance and heuristic function f for all the path containing ν .
(ii) goto Step 7.
6c. Create a pointer pointing to N and push it into the OPEN list
7. Reorder the the OPEN list in the increasing order of the heuristic function
(
)
f N
8. Goto Step 2.

Basic Search Algorithms
597
A search algorithm is said to be admissible if it can guarantee to find an optimal solu-
tion, if one exists. Now we show that if the heuristic function
(
)
h N
of estimating the re-
maining distance from N to goal node G is not an underestimate2 of the true distance from
N to goal node G , the best-first search illustrated in Algorithm 12.4 is admissible. In fact,
when
(
)
h N
satisfies the above criterion, the best-first algorithm is called A* (pronounced as
/eh/-star) Search.
The proof can be carried out informally as follows. When the frontmost node in the
OPEN list is the goal node G in Step 4, it immediately implies that
( )
( )
( )
( )
( )
OPEN
f
f G
g G
h G
g G
ν
ν
∀∈
≥
=
+
=
(12.4)
Equation (12.4) says that the distance estimate of any incomplete path are no shorter
than the first found complete path. Since the distance estimate for any incomplete path is
underestimated, the first found complete path in Step 4 must be the optimal path. A similar
argument can also be used to prove that the Step 6b is actually not necessary for admissible
heuristic functions; that is, there cannot be another path with a shorter distance from the
starting node to a node that has been expanded. This is a very important feature since Step
6b is in general very expensive and it requires significant updates of many already expanded
paths.
The A* search method is actually a family of search algorithms. When
(
)
0
h N =
for
all N, the search degenerates into an uninformed search3 [40]. In fact, this type of unin-
formed search is the famous branch-and-bound search algorithm that is often used in many
operations research problems. Branch-and-bound search always expands the shortest path
leading into an open node until there is a path reaching the goal that is of length no longer
than all incomplete paths terminating at open nodes. If
(
)
g N
is further defined as the depth
of the node N, the use of heuristic function
(
)
f N
makes the search method identical to
breadth-first search. In Section 12.1.2.2, we mention that breadth-first search is guaranteed
to find minimum length path. This can certainly be derived from the admissibility of the A*
search method.
When the heuristic function is close to the true remaining distance, the search can usu-
ally find the optimal solution without too much effort. In fact, when the true remaining dis-
tances for all nodes are known, the search can be done in a totally greedy fashion without
any search at all; i.e., the only path explored is the solution. Any non-zero heuristic function
is then called an informed heuristic function, and the search using such a function is called
informed search. A heuristic function
1h is said to be more informed than a heuristic func-
tion
2h if the estimate
1h is everywhere larger than
2h and yet still admissible (underesti-
mate). Finding an informed admissible heuristic function (guaranteed to underestimate for
all nodes) is in general a difficult task. The heuristic often requires extensive analysis of the
domain-specific knowledge and knowledge representation.
2 For admissibility, we actually require only that the heuristic function not overestimate the distance from N to G.
Since it is very rare to have an exact estimate, we use underestimate throughout this chapter without loss of general-
ity. Sometimes we refer to an underestimate function as a lower-bound estimate of the true value.
3 In some literature an uninformed search is referred to as uniform-cost search.

598
Basic Search Algorithms
Figure 12.6 Initial and goal configurations for the 8-puzzle problem.
Let’s look at a simple example – the 8-puzzle problem. The 8-puzzle consists of eight
numbered, movable tiles set in a 3 3
×
frame. One cell of this frame is always empty, so it is
possible to move an adjacent numbered tile into the empty cell. A solution for the 8-puzzle
is to find a sequence of moves to change the initial configuration into a given goal configu-
ration as shown in Figure 12.6. One choice for an informed admissible heuristic function
1h
is the number of misplaced tiles associated with the current configuration. Since each mis-
placed tile needs to move at least once to be in the right position, this heuristic function is
clearly a lower bound of the true movements remaining. Based on this heuristic function, the
value for the initial configuration will be 7 in Figure 12.6. If we examine this problem fur-
ther, a more informed heuristic function
2h can be defined as the sum of all row and column
distances of all misplaced tiles and their goal positions. For example, the row and column
distance between the tile 8 in the initial configuration and the goal position is 2 + 1= 3 which
indicates that one must to move tile 8 at least 3 times in order for it to be in the right posi-
tion. Based on the heuristic function
2h , the value for the initial configuration will be 16 in
Figure 12.6, so
2h is again admissible.
In our city-travel problem, one natural choice for the underestimating heuristic func-
tion of the remaining distance between node N and goal G is the straight-line distance since
the true distance must be no shorter than the straight-line distance.
Figure 12.7 shows an augmented city-distance map with straight-line distance to goal
node attached to each node. Accordingly, the heuristic search tree can be easily constructed
for improved efficiency. Figure 12.8 shows the search progress of applying the A* search
algorithm for the city-traveling problem by using the straight-line distance heuristic function
to estimate the remaining distances.
8
2
1
5
3
4
6
7
1
2
3
5
8
6
4
7

Basic Search Algorithms
599
D
10
8.5
5.7
2.8
10.3
7
3
S
E
C
A
B
F
3
2
4
3
3
3
4
5
5
G
Figure 12.7 The city-travel problem augmented with heuristic information. The numbers be-
side each node indicate the straight-line distance to the goal node G [42].
G
G
A
C
E
D
B
E
11.5
17.3
11.7
12.3
11.8
12
21
Figure 12.8 The search progress of applying A* search for the city-travel problem. The search
determines that path S-A-C-E-G is the optimal one. The number beside the node is f values on
which the sorting of the OPEN list is based [42].
12.1.3.2.
Beam Search
Sometimes, it is impossible to find any effective heuristic estimate, as required in A* search,
particularly when there is very little (or no) information about the remaining paths. For ex-
ample, in real-time speech recognition, there is little information about what the speaker will
utter for the remaining speech. Therefore, an efficient uninformed search strategy is very
important to tackle this type of problem.

600
Basic Search Algorithms
Breadth-first style search is an important strategy for heuristic search. A breadth-first
search virtually explores all the paths with the same depth before exploring deeper paths. In
practice, paths of the same depth are often easier to compare. It requires fewer heuristics to
rank the goodness of each path. Even with uninformed heuristic function ( (
)
0
h N =
), the
direct comparison of g (distance so far) of the paths with the same length should be a rea-
sonable choice.
Beam search is a widely used search technique for speech recognition systems [26, 31,
37]. It is a breadth-first style search and progresses along with the depth. Unlike traditional
breadth-first search, however, beam search only expands nodes that are likely to succeed at
each level. Only these nodes are kept in the beam, and the rest are ignored (pruned) for im-
proved efficiency.
In general, a beam search only keeps up to w best paths at each stage (level), and the
rest of the paths are discarded. The number w is often referred to as beam width. The num-
ber of nodes explored remains manageable in beam search even if the whole search space is
gigantic. If a beam width w is used in a beam search with an average branching factor b,
only w b
×
nodes need to be explored at any depth, instead of the exponential number
needed for breadth-first search. Suppose that a beam width of 2 is used for the city-travel
problem and the same kind of heuristic function (straight-line distance) is used; Figure 12.9
illustrates how beam search progresses to find the path. We can also see that the beam
search saved a large number of unneeded nodes, as shown by the dotted lines.
F
3
7
6
6
9
9
2
6
11
11
12
12
14
G
G
G
G
S
B
A
B
C
A
D
F
E
C
E
D
E
F
D
E
C
A
D
B
F
C
Figure 12.9 Beam search for the city-travel problem. The nodes with gray color are the ones
kept in the beam. The dashed nodes were explored but pruned because of higher cost. The dot-
ted nodes indicate all the savings because of pruning [42].
The beam search algorithm can be easily modified from the breadth-first search algo-
rithm and is illustrated in Algorithm 12.5. For simplicity, we do not include the merging step
here. In Algorithm 12.5, Step 4 obviously requires sorting, which is time consuming if the

Search Algorithms For Speech Recognition
601
number w b
×
is huge. In practice, the beam is usually implemented as a flexible list where
nodes are expanded if their heuristic functions
(
)
f N are within some threshold (a.k.a beam
threshold) of the best node (the smallest value) at the same level. Thus, we only need to
identify the best node and then prune away nodes that are outside of the threshold. Although
this makes the beam size change dynamically, it significantly reduces the effort for sorting
of the Beam-Candidate list. In fact, by adjusting the beam threshold, the beam size can be
controlled indirectly and yet kept manageable.
Unlike A* search, beam search is an approximate heuristic search method that is not
admissible. However, it has a number of unique merits. Because of its simplicity in both its
search strategy and its requirement of domain-specific heuristic information, it has become
one of the most popular methods for complicated speech recognition problems. It is particu-
larly attractive when integration of different knowledge sources is required in a time-
synchronous fashion. It has the advantages of providing a consistent way of exploring nodes
level by level and of offering minimally needed communication between different paths. It
is also very suitable for parallel implementation because of its breath-first search nature.
ALGORITHM 12.5: THE BEAM SEARCH ALGORITHM
1. Initialization: Put S in the OPEN list and create an initially empty the CLOSE list
2. If the OPEN list is empty, exit and declare failure.
3.
N
OPEN
∀
∈
do
3a. Pop up node N in the OPEN list, remove it from the OPEN list and put it into the
CLOSE list.
3b. If node N is a goal node, exit successfully with the solution obtained by tracing back
the path along the pointers from N to S.
3c. Expand node N by applying a successor operator to generate the successor set
SS(N) of node N. Be sure to eliminate the successors, which are ancestors of N, from SS(N).
3d.
(
)
SS N
ν
∀∈
Create a pointer pointing to N and push it into Beam-Candidate list
4. Sort the Beam-Candidate list according to the heuristic function
(
)
f N
so that the best w
nodes can be push into the the OPEN list. Prune the rest of nodes in the Beam-Candidate list.
5. Goto Step 2.
12.2.
SEARCH ALGORITHMS FOR SPEECH RECOGNITION
As described in Chapter 9, the decoder is basically a search process to uncover the word
sequence
1
2
ˆ
...
m
w w
w
=
W
that has the maximum posterior probability P(W|X) for the given
acoustic observation
1
2...
n
X X
X
=
X
. That is,
(
) (
|
)
argmax
(
|
)
argmax
argmax
(
) (
|
)
(
)
P
P
P
P
P
P
=
=
=
^
w
w
w
W
X W
W
W X
W
X W
X
(12.5)

602
Basic Search Algorithms
One obvious way is to search all possible word sequences and select the one with best poste-
rior probability score.
The unit of acoustic model P(X|W) is not necessary a word model. For large-
vocabulary speech recognition systems, subword models, which include phonemes, demisyl-
lables, and syllable are often used. When subword models are used, the word model P(X|W)
is then obtained by concatenating the subword models according to the pronunciation tran-
scription of the words in a lexicon or dictionary.
When word models are available, speech recognition becomes a search problem. The
goal for speech recognition is thus to find a sequence of word models that best describes the
input waveform against the word models. As neither the number of words nor the boundary
of each word or phoneme in the input waveform is known, appropriate search strategies to
deal with these variable-length nonstationary patterns are extremely important.
When HMMs are used for speech recognition systems, the states in the HMM can be
expanded to form the state-search space in the search. In this chapter, we use HMMs as our
speech models. Although the HMM framework is used to describe the search algorithms, all
techniques mentioned in this and the following chapter can be used for systems based on
other modeling techniques, including template matching and neural networks. In fact, many
search techniques had been invented before HMMs were applied to speech recognition.
Moreover, the HMMs state transition network is actually general enough to represent the
general search framework for all modeling approaches.
12.2.1.
Decoder Basics
The lessons learned from dynamic programming or the Viterbi algorithm introduced in
Chapter 8 tell us that the exponential blind search can be avoided if we can store some in-
termediate optimal paths (results). Those intermediate paths are used for other paths without
being recomputed each time. Moreover, the beam search described in the previous section
shows you that efficient search is possible if appropriate pruning is employed to discard
highly unlikely paths. In fact, all the search techniques use two strategies: sharing and prun-
ing. Sharing means that intermediate results can be kept, so that they can be used by other
paths without redundant re-computation. Pruning means that unpromising paths can be dis-
carded reliably without wasting time in exploring them further.
Search strategies based on dynamic programming or the Viterbi algorithm with the
help of clever pruning, have been applied successfully to a wide range of speech recognition
tasks [31], ranging from small-vocabulary tasks, like digit recognition, to unconstraint large-
vocabulary (more than 60,000 words) speech recognition. All the efficient search algorithms
we discuss in this chapter and the next are considered as variants of dynamic programming
or the Viterbi search algorithm.
In Section 12.1, cost (distance) is used as the measure of goodness for graph search al-
gorithms. With Bayes’ formulation, searching the minimum-cost path (word sequence) is
equivalent to finding the path with maximum probability. For the sake of consistency, we
use the inverse of Bayes' posterior probability as our objective function. Furthermore, loga-

Search Algorithms For Speech Recognition
603
rithms are used on the inverse posterior probability to avoid multiplications. That is, the fol-
lowing new criterion is used to find the optimal word sequence ˆW :
[
]
1
(
|
)
log
log
(
) (
|
)
(
) (
|
)
C
P
P
P
P


=
= −




W X
W
X W
W
X W
(12.6)
arg min
(
|
)
C
=
^
W
W
W X
(12.7)
For simplicity, we also define the following cost measures to mirror the likelihood for
acoustic models and language models:
[
]
(
|
)
log
(
|
)
C
P
= −
X W
X W
(12.8)
[
]
(
)
log
(
)
C
P
= −
W
W
(12.9)
12.2.2.
Combining Acoustic And Language Models
Although Bayes’ equation [Eq. (12.5)] suggests that the acoustic model probability (condi-
tional probability) and language model probability (prior probability) can be combined
through simple multiplication, in practice some weighting is desirable. For example, when
HMMs are used for acoustic models, the acoustic probability is usually underestimated, ow-
ing to the fallacy of the Markov and independence assumptions. Combining the language
model probability with an underestimated acoustic model probability according to Eq. (12.5)
would give the language model too little weight. Moreover, the two quantities have vastly
different dynamic ranges particularly when continuous HMMs are used. One way to balance
the two probability quantities is to add a language model weight LW to raise the language
model probability P(W) to that power
(
)LW
P W
[4, 25]. The language model weight LW is
typically determined empirically to optimize the recognition performance on a development
set. Since the acoustic model probabilities are underestimated, the language model weight
LW is typically
1
> .
Language model probability has another function as a penalty for inserting a new word
(or existing words). In particular, when a uniform language model (every word has a equal
probability for any condition) is used, the language model probability here can be viewed as
purely the penalty of inserting a new word. If this penalty is large, the decoder will prefer
fewer longer words in general, and if this penalty is small, the decoder will prefer a greater
number of shorter words instead. Since varying the language model weight to match the
underestimated acoustic model probability will have some side effect of adjusting the pen-
alty of inserting a new word, we sometimes use another independent insertion penalty to
adjust the issue of longer or short words. Thus the language model contribution will be-
comes:

604
Basic Search Algorithms
(
)
(
)LW
N
P
IP
W
W
(12.10)
where IP is the insertion penalty (generally 0
1.0
IP
<
≤
) and N(W) is the number of words
in sentence W. According to Eq. (12.10), insertion penalty is generally a constant that is
added to the negative-logarithm domain when extending the search to another new word. In
Chapter 9, we described how to compute errors in a speech recognition system and intro-
duced three types of error: substitutions, deletions and insertions. Insertion penalty is so
named because it usually affects only insertions. Similar to language model weight, the in-
sertion penalty is determined empirically to optimize the recognition performance on a de-
velopment set.
12.2.3.
Isolated Word Recognition
With isolated word recognition, word boundaries are known. If word HMMs are available,
the acoustic model probability P(X|W) can be computed using the forward algorithm intro-
duced in Chapter 8. The search becomes a simple pattern recognition problem, and the word
ˆW with highest forward probability is then chosen as the recognized word. When subword
models are used, word HMMs can be easily constructed by concatenating corresponding
phoneme HMMs or other types of subword HMMs according to the procedure described in
Chapter 9.
12.2.4.
Continuous Speech Recognition
Search in continuous speech recognition is rather complicated, even for a small vocabulary,
since the search algorithm has to consider the possibility of each word starting at any arbi-
trary time frame. Some of the earliest speech recognition systems took a two-stage approach
towards continuous speech recognition, first hypothesizing the possible word boundaries and
then using pattern matching techniques for recognizing the segmented patterns. However,
due to significant cross-word co-articulation, there is no reliable segmentation algorithm for
detecting word boundaries other than doing recognition itself.
Let’s illustrate how you can extend the isolated-word search technique to continuous
speech recognition by a simple example, as shown in Figure 12.10. This system contains
only two words, w1 and w2. We assume the language model used here is an uniform unigram
(
1
2
(
)
(
)
1 2
P w
P w
=
=
).
It is important to represent the language structures in the same HMM framework. In
Figure 12.10, we add one starting state S and one collector state C. The starting state has a
null transition to the initial state of each word HMM with corresponding language model
probability (1/2 in this case). The final state of each word HMM has a null transition to the
collector state. The collector state then has a null transition back to the starting state in order
to allow recursion. Similar to the case of embedding the phoneme (subword) HMMs into the
word HMM for isolated speech recognition, we can embed the word HMMs for w1 and w2

Search Algorithms For Speech Recognition
605
into a new HMM corresponding to structure in Figure 12.10. Thus, the continuous speech
search problem can be solved by the standard HMM formulations.
HMM of
W1
HMM of
W2
C
S
Figure 12.10 A simple example of continuous speech recognition task with two words w1 and
w2. A uniform unigram language model is assumed for these words. State S is the starting state
while state C is a collector state to save full expanded links between every word pairs.
W2
W1
0
1
2
3
t
Time
Figure 12.11 HMM trellis for continuous speech recognition example in Figure 12.10. When
the final state of the word HMM is reached, a null arc (indicated by a dashed line) is linked
from it to the initial state of the following word.
The composite HMMs shown in Figure 12.10 can be viewed as a stochastic finite state
network with transition probabilities and output distributions. The search algorithm is essen-

606
Basic Search Algorithms
tially producing a match between the acoustic the observation X and a path4 in the stochastic
finite state network. Unlike isolated word recognition, continuous speech recognition needs
to find the optimal word sequence
ˆW . The Viterbi algorithm is clearly a natural choice for
this task since the optimal state sequence ˆS corresponds to the optimal word sequence
ˆW .
Figure 12.11 shows the HMM Viterbi trellis computation for the two-word continuous
speech recognition example in Figure 12.10. There is a cell for each state in the stochastic
finite state network and each time frame t in the trellis. Each cell
,s t
C
in the trellis can be
connected to a cell corresponding to time t or t+1 and to states in the stochastic finite state
network that can be reached from s. To make a word transition, there is a null transition to
connect the final state of each word HMM to the initial state of the next word HMM that can
be followed. The trellis computation is done time-synchronously from left to right; i.e., the
each cell for time t is completely computed before proceeding to time t+1.
12.3.
LANGUAGE MODEL STATES
The state-space is a good indicator of search complexity. Since the HMM representation for
each word in the lexicon is fixed, the state-space is determined by the language models. Ac-
cording to Chapter 11, every language model (grammar) is associated with a state machine
(automata). Such a state machine is expanded to form the state-space for the recognizer. The
states in such a state machine are referred to as language models states. For simplicity, we
will use the concepts of state-space and language model states interchangeably. The expan-
sion of language model states to HMM states will be done implicitly. The language model
states for isolated word recognition are trivial. They are just the union of the HMM states of
each word. In this section we look at the language model states for various grammars for
continuous speech recognition.
12.3.1.
Search Space with FSM and CFG
As described in Chapter 8, the complexity for the Viterbi algorithm is
2
(
)
O N T where N is
the total numberof states in the composite HMM and T is the length of input observation. A
full time-synchronous Viterbi search is quite efficient for moderate tasks (vocabulary ≤
500). We have already demonstrated in Figure 12.11 how to search for a two-word continu-
ous speech recognition task with a uniform language model. The uniform language model,
which allows all words in the vocabulary to follow every word with the same probability, is
suitable for connected-digit task. In fact, most small vocabulary tasks in speech recognition
applications usually use a finite state grammar (FSG).
Figure 12.12 shows a simple example of an FSM. Similar to the process described in
Section 12.2.3 and 12.2.4, each of the word arcs in an FSG can be expanded as a network of
phoneme (subword) HMMs. The word HMMs are connected with null transitions with the
4 A path here means a sequence of states and transitions.

Language Model States
607
grammar state. A large finite state HMM network that encodes all the legal sentences can be
constructed based on the expansion procedure. The decoding process is achieved by per-
forming time-synchronous Viterbi search on this composite finite state HMM.
In practice, FSGs are sufficient for simple tasks. However, when an FSG is made to
satisfy the constraints of sharing of different sub-grammars for compactness and support for
dynamic modifications, the resulting non-deterministic FSG is very similar to context-free
grammar (CFG) in terms of implementation. The CFG grammar consists of a set of produc-
tions or rules, which expand nonterminals into a sequence of terminals and nonterminals.
Nonterminals in the grammar tend to refer to high-level task specific concepts such as dates,
names, and commands. The terminals are words in the vocabulary. A grammar also has a
non-terminal designated as its start state.
Although efficient parsing algorithms, like chart parsing (described in Chapter 11), are
available for CFG, they are not suitable for speech recognition, which requires left-to-right
processing. A context-free grammar can be formulated with a recursive transition network
(RTN). RTNs are more powerful and complicated than the finite state machines described in
Chapter 11 because they allow arc labels to refer to other networks as well as words. We use
Figure 12.13 to illustrate how to embed HMMs into a recursive transition network.
/w/
/ah/
/w/ + /ah/ + /t/
/t/
What
/silence/
is
Seattle's
weather
Boston's
Denver's
population
lattitude
Show
/silence/
(optional)
Figure 12.12 An illustration of how to compile a speech recognition task with finite state
grammar into a composite HMM.
Figure 12.13 is an RTN representation of the following CFG:
S→NP VP
NP→sam | sam davis
VP →VERB tom
VERB →likes | hates

608
Basic Search Algorithms
There are three types of arcs in an RTN, as shown in Figure 12.13: CAT(x), PUSH (x),
and POP(x). The CAT(x) arc indicates that x is a terminal node (which is equivalent to a
word arc). Therefore, all the CAT(x) arcs can be expanded by the HMM network for x. The
word HMM can again be a composite HMM built from phoneme (or subword) HMMs.
Similar to the finite state grammar case in Figure 12.12, each grammar state acts as a state
with incoming and outgoing null transitions to connect word HMMs in the CFG.
Figure 12.13 An simple RTN example with three types of arcs: CAT(x), PUSH (x), POP.
During decoding, the search pursues several paths through the CFG at the same time.
Associated with each of the paths is a grammar state that describes completely how the path
can be extended further. When the decoder hypothesizes the end of the current word of a
path, it asks the CFG module to extend the path further by one word. There may be several
alternative successor words for the given path. The decoder considers all the successor word
possibilities. This may cause the path to be extended to generate several more paths to be
considered ,each with its own grammar state.
Readers should note that the same word might be under consideration by the decoder
in the context of different paths and grammar states at the same time. For example, there are
two word arcs CAT (Sam) in Figure 12.13. Their HMM states should be considered as dis-
tinct states in the trellis because they are in completely different grammar states. Two differ-
ent states in the trellis also means that different paths going into these two states cannot be
merged. Since these two partial paths will lead to different successive paths, the search deci-
PUSH(VP)
PUSH(NP)
pop
CAT (tom)
CAT (hates)
VP2
S
S2
S1
S:
NP2
NP1
CAT (sam)
pop
CAT (Sam)
NP
CAT (davis)
NP:
VP1
CAT (likes)
pop
VP
VP:
CAT

Language Model States
609
sion needs to be postponed until the end of search. Therefore, when embedding HMMs into
word arcs in the grammar network, the HMM state will be assigned a new state identity,
although the HMM parameters (transition probabilities and output distributions) can still be
shared across different grammar arcs.
Each path consists of a stack of production rules. Each element of the stack also con-
tains the position within the production rule of the symbol that is currently being explored.
The search graph (trellis) started from the initial state of CFG (state S). When the path needs
to be extended, we look at the next arc (symbol in CFG) in the production. When the search
enters a CAT(x) arc (terminal), the path gets extended with the terminal, and the HMM trel-
lis computation is performed on the CAT(x) arc to match the model x against the acoustic
data. When the final state of the HMM for x is reached, the search moves on via the null
transition to the destination of the CAT(x) arc. When the search enters a PUSH(x) arc, it
indicates a nonterminal symbol x is encountered. In effect, the search is about to enter a sub-
network of x, the destination of the PUSH(x) arc is stored in a last-in first-out (LIFO) stack.
When the search reaches a POP arc that signals the end of the current network, the control
should jump back to the calling network. In the other words, the search returns to the state
extracted from the top of the LIFO stack. Finally, when we reach the end of the production
rule at the very bottom of the stack, we have reached an accepting state in which we have
seen a complete grammatical sentence. For our decoding purpose, that is the state we want
to pick as the best score at the end of time frame T to get the search result.
The problem of connected word recognition by finite state or context-free grammars is
that the number of states increases enormously when it is applied to more complex gram-
mars. Moreover it remains a challenge to generate such FSGs or CFGs from a large corpus,
either manually or automatically. As mentioned in Chapter 11, it is questionable whether
FSG or CFG is adequate to describe natural languages or unconstrained spontaneous lan-
guages. Instead, n-gram language models are often used for natural languages or uncon-
strained spontaneous languages. In the next section we investigate how to integrate various
n-grams into continuous speech recognition.
12.3.2.
Search Space with the Unigram
The simplest n-gram is the unigram that is memory-less and only depends on the current
word.
1
(
)
(
)
n
i
i
P
P w
=
=∏
W
(12.11)
Figure 12.14 shows such a unigram grammar network. The final state of each word
HMM is connected to the collector state by a null transition, with probability 1.0. The col-
lector state is then connected to the starting state by another null transition, with transition
probability equal to 1.0. For word expansion, the starting state is connected to the initial
state of each word HMM by a null transition, with transition probability equal to the corre-
sponding unigram probability. Using the collector state and starting state for word expansion

610
Basic Search Algorithms
allows efficient expansion because it first merges all the word-ending paths5 (only the best
one survives) before expansion. It can cut the total cross-word expansion from
2
N
to N.
W1
W2
WN
P(W1)
P(W2)
P(WN)
Figure 12.14 A unigram grammar network where the unigram probability is attached as the
transition probability from starting state S to the first state of each word HMM.
12.3.3.
Search Space with Bigrams
When the bigram is used, the probability of a word depends only on the immediately preced-
ing word. Thus, the language model score is:
1
1
2
(
)
(
|
)
(
|
)
n
i
i
i
P
P w
s
P w
w −
=
=
<
> ∏
W
(12.12)
where
s
<
> represents the symbol of starting of a sentence.
Figure 12.15 shows a grammar network using a bigram language model. Because of
the bigram constraint, the merge-and-expand framework for unigram search no longer ap-
plies here. Instead, the bigram search needs to perform expand-and-merge. Thus, bigram
expansion is more expensive than unigram expansion. For a vocabulary size N, the bigram
would need
2
N
word-to-word transitions in comparison to N for the unigram. Each word
transition has a transition probability equal to the correspondent bigram probability. Fortu-
nately, the total number of states for bigram search is still proportional to the vocabulary
size N.
5 In graph search, a partial path still under consideration is also referred as a theory, although we will use paths
instead of theories in this book.

Language Model States
611
W1
W2
WN
P(WN | W1)
P(WN | W2)
P(WN | WN)
P(W2 | WN)
P(W1 | WN)
P(W2 | W1)
P(W2 | W2)
P(W1 | W1)
P(W1 | W2)
Figure 12.15 A bigram grammar network where the bigram probability
(
|
)
j
i
P w
w
is at-
tached as the transition probability from word
iw to
j
w [19].
Because the search space for bigram is kept manageable, bigram search can be imple-
mented very efficiently. Bigram search is a good compromise between efficient search and
effective language models. Therefore, bigram search is arguably the most widely used
search technique for unconstrained large-vocabulary continuous speech recognition. Particu-
larly for the multiple-pass search techniques described in Chapter 13, a bigram search is
often used in the first pass search.
12.3.3.1.
Backoff Paths
When the vocabulary size N is large, the total bigram expansion
2
N
can become computa-
tionally prohibitive. As described in Chapter 11, only a limited number of bigrams are ob-
servable in any practical corpora for a large vocabulary size. Suppose the probabilities for
unseen bigrams are obtained through Katz’s backoff mechanism. That is, for unseen bigram
(
|
)
j
i
P w
w ,
(
|
)
(
) (
)
j
i
i
j
P w
w
w P w
α
=
(12.13)
where
(
)
iw
α
is the backoff weight for word
iw .
Using the backoff mechanism for unseen bigrams, the bigram expansion can be sig-
nificantly reduced [12]. Figure 12.16 shows the new word expansion scheme. Instead of full

612
Basic Search Algorithms
bigram expansion, only observed bigrams are connected by direct word transitions with cor-
respondent bigram probabilities. For backoff bigrams, the last state of word
iw is first con-
nected to a central backoff node with transition probability equal to backoff weight
(
)
iw
α
.
The backoff node is then connected to the beginning of each word
j
w with transition prob-
ability equal to its corresponding unigram probability
(
)
j
P w
. Readers should note that there
are now two paths from
iw to
j
w for an observed bigram
(
|
)
j
i
P w
w . One is the direct link
representing the observable bigram
(
|
)
j
i
P w
w , and the other is the two-link backoff path
representing
(
) (
)
i
j
w P w
α
. For a word pair whose bigram exists, the two-link backoff path is
likely to be ignored since the backoff unigram probability is almost always smaller than the
observed bigram
(
|
)
j
i
P w
w . Suppose there are only
b
N
different observable bigrams, this
scheme requires
2
b
N
N
+
instead of
2
N
word transitions. Since under normal circum-
stance,
2
b
N
N
<<
, this backoff scheme significantly reduce the cost of word expansion.
Wk
Wi
Wj
P(Wj | Wi)
α(Wi)
α(Wk)
backoff node
P(Wj)
Figure 12.16 Reducing bigram expansion in a search by using the backoff node. In additional
to normal bigram expansion arcs for all observed bigrams, the last state of word
iw is first
connected to a central backoff node with transition probability equal to backoff weight
(
)
iw
α
.
The backoff node is then connected to the beginning of each word
j
w
with its corresponding
unigram probability
(
)
j
P w
[12].
12.3.4.
Search Space with Trigrams
For a trigram language model, the language model score is:
1
2
1
2
1
3
(
)
(
|
) (
|
,
)
(
|
,
)
n
i
i
i
i
P
P w
s
P w
s
w
P w
w
w
−
−
=
=
<
>
<
>
∏
W
(12.14)

Language Model States
613
The search space is considerably more complex, as shown in Figure 12.17. Since the equiva-
lence grammar class is the previous two words
iw and
j
w , the total number of grammar
states is
2
N . From each of these grammar states, there is a transition to the next word [19].
Obviously, it is very expensive to implement large-vocabulary trigram search given
the complexity of the search space. It becomes necessary to dynamically generate the tri-
gram search graph (trellis) via a graph search algorithm. The other alternative is to perform a
multiple-pass search strategy, in which the first-pass search uses less detailed language
models, like bigrams to generate an n-best list or word lattice, and then a second pass de-
tailed search can use trigrams on a much smaller search space. Multiple-pass search strategy
is discussed in Chapter 13.
W1
W2
P(W1 | W1 , W1)
W1
W2
P(W1 | W2 , W1)
P(W2 | W1 , W1)
P(W2 | W2 , W1)
P(W1 | W1 , W2)
P(W1 | W2 , W2)
P(W2 | W1 , W2)
P(W1 | W2 , W2)
Figure 12.17 A trigram grammar network where the trigram probability
(
|
,
)
k
i
j
P w
w w
is at-
tached to transition from grammar state word
iw ,
j
w
to the next word
k
w . Illustrated here is a
two-word vocabulary, so there are four grammar states in the trigram network [19].
12.3.5.
How to Handle Silences Between Words
In continuous speech recognition, there are unavoidable pauses (silences) between words or
sentences. The pause is often referred to as silence or a non-speech event in continuous

614
Basic Search Algorithms
speech recognition. Acoustically, the pause is modeled by a silence model6 that models
background acoustic phenomena. The silence model is usually modeled with a topology
flexible enough to accommodate a wide range of lengths, since the duration of a pause is
arbitrary.
It can be argued that silences (pauses) are actually linguistic distinguishable events,
which contribute to prosodic and meaning representation. For example, people are likely to
pause more often in phrasal boundaries. However, these patterns are so far not well under-
stood for unconstrained natural speech (particularly for spontaneous speech). Therefore, the
design of almost all automatic speech recognition systems today allows silences (pauses)
occurring just about anywhere between two lexical tokens or between sentences. It is rela-
tively safe to assume that people pause a little bit between sentences to catch breath, so the
silences between sentences are assumed mandatory while silences between words are op-
tional. In most systems, silence (any pause) is often modeled as a special lexicon entry with
special language model probability. This special language model probability is also referred
to as silence insertion penalty that is set to adjust the likelihood of inserting such an optional
silence between words.
Figure 12.18 Incorporating optional silence (a non-speech event) in the grammar search net-
work where the grammar state connecting different words are replaced by two parallel paths.
One is the original null transition directly from one word to the other, while the other first goes
through the silence word to accommodate the optional silence.
It is relatively straightforward to handle the optional silence between words. We need
only to replace all the grammar states connecting words with a small network like the one
shown in Figure 12.18. This arrangement is similar to that of the optional silence in training
continuous speech, described in Chapter 9. The small network contains two parallel paths.
One is the original null transition acting as the direct transition from one word to another,
while the other path will need to go through a silence model with the silence insertion pen-
alty attached in the transition probability before going to the next word.
6 Some researchers extend the context-dependent modeling to silence models. In that case, there are several silence
models based on surrounding contexts.
Wi
Wj
/sil/
Wi
Wj

Time-Synchronous Viterbi Beam Search
615
One thing to clarify in the implementation of Figure 12.18 is that this silence expan-
sion needs to be done for every grammar state connecting words. In the unigram grammar
network of Figure 12.14, since there is only one collector node to connect words, the silence
expansion is required only for this collector node. On the other hand, in the bigram grammar
network of Figure 12.15, there is a collector node for every word before expanding to the
next word. In this case, the silence expansion is required for every collector node. For a vo-
cabulary size |
|
V , this means there are |
|
V
numbers of silence networks in the grammar
search network. This requirement lies in the fact that in bigram search we cannot merge
paths before expanding into the next word. Optional silence can then be regarded as part of
the search effort for the previous word, so the word expansion needs to be done after finish-
ing the optional silence. Therefore, we treat each word having two possible pronunciations,
one with the silence at the end and one without. This viewpoint integrates silence in the
word pronunciation network like the example shown in Figure 12.19.
Figure 12.19 An example of treating silence is part of the pronunciation network of word
TWO. The shaded nodes represent possible word-ending nodes; one without silence and the
other one with silence.
For efficiency reasons, a single silence is sometimes used for large-vocabulary con-
tinuous speech recognition using higher order n-gram language model. Theoretically, this
could be a source of pruning errors.7 However, the error could turn out to be so small as to
be negligible because there are, in general, very few pauses between word for continuous
speech. On the other hand, the overhead of using multiple silences should be very minimal
because it is less likely to visit those silence models at the end of words due to pruning.
12.4.
TIME-SYNCHRONOUS VITERBI BEAM SEARCH
When HMMs are used for acoustic models, the acoustic model score (likelihood) used in
search is by definition the forward probability. That is, all possible state sequences must be
considered. Thus,
0
0
possible
(
|
)
( ,
|
)
T
T
all
s
P
P
s
=

X W
X
W
(12.15)
where the summation is to be taken over all possible state sequences S with the word se-
quence W under consideration. However, under the trellis framework (as in Figure 12.11),
7 Speech recognition errors due to sub-optimal search or heuristic pruning are referred to as pruning errors, which
will be described in details in Chapter 13.
/t/
/uw/
/sil/

616
Basic Search Algorithms
more bookkeeping must be performed since we cannot add scores with different word se-
quence history. Since the goal of decoding is to uncover the best word sequence, we could
approximate the summation with the maximum to find the best state sequence instead. The
Bayes decision rule, Eq. (12.5) becomes
{
}
0
0
argmax
(
) (
|
)
argmax
(
)max
( ,
|
)
T
T
S
P
P
P
P
s
=
≅
^
w
w
W
W
X W
W
X
W
(12.16)
Equation (12.16) is often referred to as the Viterbi approximation. It can be literally
translated to “the most likely word sequence is approximated by the most likely state se-
quence”. Viterbi search is then sub-optimal. Although the search results by using forward
probability and Viterbi probability could in principle be different, in practice this is rarely
the case. We use this approximation for the rest of this chapter.
The Viterbi search has already been discussed as a solution to one of the three funda-
mental HMM problems in Chapter 8. It can be executed very efficiently via the same trellis
framework. To briefly reiterate, the Viterbi search is a time-synchronous search algorithm
that completely processes time t before going on to time t+1. For time t, each state is up-
dated by the best score (instead of the sum of all incoming paths) from all states in at time t-
1. This is why it is often called time-synchronous Viterbi search. When one update occurs, it
also records the backtracking pointer to remember the most probable incoming state. At the
end of search, the most probable state sequence can be recovered by tracing back these
backtracking pointers. The Viterbi algorithm provides an optimal solution for handling
nonlinear time warping between hidden Markov models and acoustic observation, word
boundary detection and word identification in continuous speech recognition. This unified
Viterbi search algorithm serves as the basis for all search algorithms as described in the rest
of the chapter.
It is necessary to clarify the backtracking pointer for time-synchronous Viterbi search
for continuous word recognition. We are generally not interested in the optimal state se-
quence for speech recognition.8 Instead, we are only interested in the optimal word sequence
indicated by Eq. (12.16). Therefore, we use the backtrack pointer just to remember the word
history for the current path, so the optimal word sequence can be recovered at the end of
search. To be more specific, when we reach the final state of a word, we create a history
node containing the word identity and current time index and append this history node to the
existing backtrack pointer. This backtrack pointer is then passed onto the successor node if it
is the optimal path leading to the successor node for both intra-word and inter-word transi-
tion. The side benefit of keeping this backtrack pointer is that we no longer need to keep the
entire trellis during the search. Instead, we only need space to keep two successive time
slices (columns) in the trellis computation (the previous time slice and the current time slice)
because all the backtracking information is now kept in the backtrack pointer. This simplifi-
cation is a significant benefit in the implement of a time-synchronous Viterbi search.
Time-synchronous Viterbi search can be considered as a breadth-first search with dy-
namic programming. Instead of performing a tree search algorithm, the dynamic program-
8 While we are not interested in optimal state sequences for ASR, they are very useful to derive phonetic segmenta-
tion, which could be provide important information for developing ASR systems.

Time-Synchronous Viterbi Beam Search
617
ming principle helps create a search graph where multiple paths leading to the same search
state are merged by keeping the best path (with minimum cost). The Viterbi trellis is a repre-
sentation of the search graph. Therefore, all the efficient techniques for graph search algo-
rithms can be applied to time-synchronous Viterbi search. Although so far we have de-
scribed the trellis in an explicit fashion – the whole search space needs to be explored before
the optimal path can be found, it is not necessary to do so. When the search space contains
an enormous number of states, it becomes impractical to pre-compile the composite HMM
entirely and store it in the memory. It is preferable to dynamically build and allocate por-
tions of the search space sufficient to search the promising paths. By using the graph search
algorithm described in Section 12.1.1, only part of the entire Viterbi trellis is generated ex-
plicitly. By constructing the search space dynamically, the computation cost of the search is
proportional only to the number of active hypotheses, independent of the overall size of the
potential search space. Therefore, dynamically generated trellises are key to heuristic Viterbi
search for efficient large-vocabulary continuous speech recognition, as described in Chapter
13.
12.4.1.
The Use of Beam
Based on Chapter 8, the search space for Viterbi search is
(
)
O NT
and the complexity is
2
(
)
O N T , where N is the total number of HMM states and T is the length of the utterance.
For large-vocabulary tasks these numbers are astronomically large even with the help of
dynamic programming. In order to avoid examining the overwhelming number of possible
cells in the HMM trellis, a heuristic search is clearly needed. Different heuristics generate or
explore portions of the trellis in different ways.
A simple way to prune the search space for breadth-first search is the beam search de-
scribed in Section 12.1.3.2. Instead of retaining all candidates (cells) at every time frame, a
threshold T is used to keep only a subset of promising candidates. The state at time t with the
lowest cost
min
D
is first identified. Then each state at time t with cost
min
D
T
>
+
is dis-
carded from further consideration before moving on to the next time frame t+1. The use of
the beam alleviates the need to process all the cells. In practice, it can lead to substantial
savings in computation with little or no loss of accuracy.
Although beam search is a simple idea, the combination of time-synchronous Viterbi
and beam search algorithms produces the most powerful search strategy for large-
vocabulary speech recognition. Comparing paths with equal length under a time-
synchronous search framework makes beam search possible. That is, for two different word
sequences
1
W and
2
W , the posterior probability
1
0
(
|
)
t
P W
x
and
2
0
(
|
)
t
P W
x
are always
compared based on the same partial acoustic observation
0
tx . This makes the comparison
straightforward because the denominator
0
(
)
t
P x
in Eq. (12.5) is the same for both terms and
can be ignored. Since the score comparison for each time frame is fair, the only assumption
of beam search is that an optimal path should have good enough partial-path score for each
time frame to survive under beam pruning.

618
Basic Search Algorithms
The time-synchronous framework is one of the aspects of Viterbi beam search that is
critical to its success. Unlike the time-synchronous framework, time-asynchronous search
algorithms such as stack decoding require the normalization of likelihood scores over fea-
ture streams of different time lengths. This, as we will see in Section 12.5, is the Achilles
heel of that approach.
The straightforward time-synchronous Viterbi beam search is ineffective in dealing
with the gigantic search space of high perplexity tasks. However, with a better understand-
ing of the linguistic search space and the advent of techniques for obtaining n-best lists from
time-synchronous Viterbi search, described in Chapter 13, time-synchronous Viterbi beam
search has turned out to be surprisingly successful in handling tasks of all sizes and all dif-
ferent types of grammars, including FSG, CFG, and n-gram [2, 14, 18, 28, 34, 38, 44].
Therefore, it has become the predominant search strategy for continuous speech recognition.
12.4.2.
Viterbi Beam Search
To explain the time-synchronous Viterbi beam search in a formal way [31], we first define
some quantities:
( ;
; )
t
D t s w ≡total cost of the best path up to time t that ends in state
ts of gram-
mar word state w.
( ; ; )
t
h t s w ≡backtrack pointer for the best path up to time t that ends in state
ts of
grammar word state w.
Readers should be aware that w in the two quantities above represents a grammar
word state in the search space. It is different from just the word identity since the same word
could occur in many different language model states, as in the trigram search space shown in
Figure 12.17.
There are two types of dynamic programming (DP) transition rules [30], namely intra-
word and inter-word transition. The intra-word transition is just like the Viterbi rule for
HMMs and can be expressed as follows:
{
}
1
1
1
( ;
; )
min
(
,
|
; )
(
1;
; )
t
t
t
t
t
t
s
D t s w
d
s
s
w
D t
s
w
−
−
−
=
+
−
x
(12.17)
min
( ; ; )
(
1,
( ; ; ); )
t
t
h t s w
h t
b
t s w w
=
−
(12.18)
where
1
(
,
|
; )
t
t
t
d
s
s
w
−
x
is the cost associated with taking the transition from state
1
ts −
to
state
ts while generating output observation
tx , and
min ( ;
; )
t
b
t s w
is the optimal predecessor
state of cell
( ; ; )
t
D t s w . To be specific, they can be expressed as follows:
1
1
(
,
|
; )
log
(
|
; )
log
(
|
; )
t
t
t
t
t
t
t
d
s
s
w
P s
s
w
P
s w
−
−
= −
−
x
x
(12.19)
{
}
1
min
1
1
( ;
; )
arg min
(
,
|
; )
(
1;
; )
t
t
t
t
t
t
s
b
t s w
d
s
s
w
D t
s
w
−
−
−
=
+
−
x
(12.20)

Stack decoding (A* Search)
619
The inter-word transition is basically a null transition without consuming any observa-
tion. However, it needs to deal with creating a new history node for the backtracking
pointer. Let’s define
( )
F w as the final state of word HMM w and
( )
I w as the initial state of
word HMM w. Moreover, state η is denoted as the pseudo initial state. The inter-word tran-
sition can then be expressed as follows:
{
}
( ; ; )
min log
(
| )
( ;
( ); )
v
D t
w
P w v
D t F v v
η
=
+
(12.21)
min
min
min
( ; ; )
,
:: ( ,
(
);
)
h t
w
v
t
h t F v
v
η
=
(12.22)
where
{
}
min
arg min log
(
| )
( ;
( ); )
v
v
P w v
D t F v v
=
+
and :: is a link appending operator.
The time-synchronous Viterbi beam search algorithm assumes that all the intra-word
transitions are evaluated before inter-word null transitions take place. The same time index
is used intentionally for inter-word transition since the null language model state transition
does not consume an observation vector. Since the initial state
( )
I w for word HMM w could
have a self-transition, the cell
( ; ( ); )
D t I w w
might already have active path. Therefore, we
need to perform the following check to advance the inter-word transitions.
if
( ; ; )
( ; ( ); )
( ; ( ); )
( ; ; ) and ( ; ( ); )
( ; ; )
D t
w
D t I w w
D t I w w
D t
w
h t I w w
h t
w
η
η
η
<
=
=
(12.23)
The time-synchronous Viterbi beam search can be summarized as in Algorithm 12.6.
For large-vocabulary speech recognition, the experimental results shows that only a small
percentage of the entire search space (the beam) needs to be kept for each time interval t
without increasing error rates. Empirically, the beam size has typically been found to be
between 5% and 10% of the entire search space. In Chapter 13 we describe strategies of
using different level of beams for more effectively pruning.
12.5.
STACK DECODING (A* SEARCH)
If some reliable heuristics are available to guide the decoding, the search can be done in a
depth-first fashion around the best path early on, instead of wasting efforts on unpromising
paths via the time-synchronous beam search. Stack decoding represents the best attempt to
use A* search instead of time-synchronous beam search for continuous speech recognition.
Unfortunately, as we will discover in this section, such a heuristic function
( )
h • (defined in
Section 12.1.3) is very difficult to attain in continuous speech recognition, so search algo-
rithms based on A* search are in general less efficient than time-synchronous beam search.
Stack decoding is a variant of the heuristic A* search based on the forward algorithm,
where the evaluation function is based on the forward probability. It is a tree search algo-
rithm, which takes a slightly different viewpoint than the time-synchronous Viterbi search.
Time-synchronous beam search is basically a breadth-first search, so it is crucial to control

620
Basic Search Algorithms
the number of all possible language model states as described in Section 12.3. In a typical
large-vocabulary Viterbi search with n-gram language models, this number is determined by
the equivalent classes of language model histories. On the other hand, stack decoding as a
tree search algorithm treats the search as a task for finding a path in a tree whose branches
correspond to words in the vocabulary V, non-terminal nodes correspond to incomplete sen-
tences, and terminal nodes correspond to complete sentences. The search tree has a constant
branching factor of |V|, if we allow every word to be followed by every word. Figure 12.20
illustrated such a search tree for a vocabulary with three words [19].
ALGORITHM 12.6 TIME-SYNCHRONOUS VITERBI BEAM SEARCH
Initialization: For all the grammar word states w which can start a sentence,
(0; ( ); )
0
D
I w w =
(0; ( ); )
h
I w w
null
=
Induction: For time
1 to
t
T
=
do
For all active states do
Intra-word transitions according to Eq. (12.17) and (12.18)
{
}
1
1
1
( ;
; )
min
(
,
|
; )
(
1;
; )
t
t
t
t
t
t
s
D t s w
d
s
s
w
D t
s
w
−
−
−
=
+
−
x
min
( ; ; )
(
1,
( ; ; ); )
t
t
h t s w
h t
b
t s w w
=
−
For all active word-final states do
Inter-word transitions according to Eq. (12.21), (12.22) and (12.23)
{
}
( ; ; )
min log
(
| )
( ;
( ); )
v
D t
w
P w v
D t F v v
η
=
+
min
min
min
( ; ; )
,
:: ( ,
(
);
)
h t
w
v
t
h t F v
v
η
=
if
( ; ; )
( ; ( ); )
( ; ( ); )
( ; ; ) and ( ; ( ); )
( ; ; )
D t
w
D t I w w
D t I w w
D t
w
h t I w w
h t
w
η
η
η
<
=
=
Pruning: Find the cost for the best path and decide the beam threshold
Prune unpromising hypotheses
Termination: Pick the best path among all the possible final states of grammar at time T
Obtain the optimal word sequence according to the backtracking pointer
( ; ; )
h t
w
η
An important advantage of stack decoding is its consistency with the forward-
backward training algorithm. Viterbi search is a graph search, and paths cannot be easily
summed because they may have different word histories. In general, the Viterbi search finds
the optimal state sequence instead of optimal word sequence. Therefore, Viterbi approxima-
tion is necessary to make the Viterbi search feasible, as described in Section 12.4. Stack
decoding is a tree search, so each node has a unique history, and the forward algorithm can
be used within word model evaluation. Moreover, all possible beginning and ending times
(shaded areas in Figure 12.21) beginning and ending times are considered [24]. With stack

Stack decoding (A* Search)
621
decoding, it is possible to use an objective function that searches for the optimal word string,
rather than the optimal state sequence. Furthermore, it is in principle natural for stack decod-
ing to accommodate long-range language models if the heuristics can guide the search to
avoid exploring the overwhelmingly large unpromising grammar states.
W1
W2
W3
W1
W2
W3
W1
W2
W3
W1
W2
W3
Figure 12.20 A stack decoding search tree for a vocabulary size of three [19].
By formulating stack decoding in a tree search framework, the graph search algo-
rithms described in Section 12.1 can be directly applied to stack decoding. Obviously, blind-
search methods, like depth-first and breadth-first search, that do not take advantage of good-
ness measurement of how close we are getting to the goal, are usually computationally in-
feasible in practical speech recognition systems. A* search is clearly attractive for speech
recognition given the hope of a sufficient heuristic function to guide the tree search in a fa-
vorable direction without exploring too many unpromising branches and nodes. In contrast
to the Viterbi search, it is not time-synchronous and extends paths of different lengths.
The search begins by adding all possible one-word hypotheses to the OPEN list. Then
the best path is removed from the OPEN list, and all paths from it are extended, evaluated,

622
Basic Search Algorithms
and placed back in the OPEN list. This search continues until a complete path that is guaran-
teed to be better than all paths in the OPEN list has been found.
W1
W2
W3
W4
Figure 12.21 The forward trellis space for stack decoding. Each grid point corresponds to a
trellis cell in the forward computation. The shaded area represents the values contributing to
the computation of the forward score for the optimal word sequence
1
2
3
,
,
,
w w w  [24].
Unlike Viterbi search, where the acoustic probabilities being compared are always
based on the same partial input, it is necessary to compare the goodness of partial paths of
different lengths to direct the A* tree search. Moreover, since stack decoding is done asyn-
chronously, we need an effective mechanism to determine when to end a phone/word
evaluation and move on to the next phone/word. Therefore, the heart and soul of the stack
decoding is clearly in
1. Finding an effective and efficient heuristic function for estimating the future
remaining input feature stream and
2. Determining when to extend the search to the next word/phone.
In the following section we describe these two critical components. Readers will note that
the solution to these two issues are virtually the same - using a normalization scheme to
compare paths of different lengths.
12.5.1.
Admissible Heuristics for Remaining Path
The key issue in heuristic search is the selection of an evaluation function. As described in
Section 12.1.3, the heuristic function of the path
N
H
going through node N includes the cost
up to the node and the estimate of the cost to the target node from node N. Suppose path
N
H

Stack decoding (A* Search)
623
is going through node N at time t; then the evaluation for path
N
H
can be expressed as fol-
lows:
,
(
)
(
)
(
)
t
t
t T
N
N
N
f H
g H
h H
=
+
(12.24)
where
(
)
t
N
g H
is the evaluation function for the partial path of
N
H
up to time t, and
,
(
)
t T
N
h H
is the heuristic function of the remaining path from
1
t +
to T for path
N
H . The
challenge for stack decoders is to devise an admissible function for
( )
h • .
According to Section 12.1.3.1, an admissible heuristic function is one that always un-
der estimates the true cost of the remaining path from
1
t +
to T for path
N
H . A trivially
admissible function is the zero function. In this case, it results in a very large OPEN list. In
addition, since the longer paths tend to have higher cost because of the gradually accumu-
lated cost, the search is likely to be conducted in a breadth-first fashion, which functions
very much like a plain Viterbi search. The evaluation function ( )
g • can be obtained easily by
using the HMM forward score as the true cost up to current time t. However, how can we
find an admissible heuristic function
( )
h • ? We present the basic concept here [19, 35].
The goal of
( )
h • is to find the expected cost for the remaining path. If we can obtain
the expected cost per frame ψ for the remaining path, the total expected cost, (
)
T
t
ψ
−
∗
, is
simply the product of ψ and the length of the remaining path. One way to find such ex-
pected cost per frame is to gather statistics empirically from training data.
1. After the final training iteration, perform Viterbi forced alignment9 with each
training utterance to get an optimal time alignment for each word.
2. Randomly select an interval to cover the number of words ranging from two
to ten. Denote this interval as [
]
i
j

3. Compute the average acoustic cost per frame within this selected interval ac-
cording to the following formula and save the value in a set Λ .
1 log
(
|
)
j
i
i
j
P
j
i
−
−
x
w 
(12.25)
where
i
j
w  is the word string corresponding to interval [
]
i
j

4. Repeat Steps 2 and 3 for the entire training set.
5. Define
min
ψ
and
avg
ψ
as the minimum and average value found in set Λ .
9 Viterbi forced alignment means that the Viterbi is performed on the HMM model constructed from the known
word transcription. The term “forced” is used because the Viterbi alignment is forced to be performed on the cor-
rect model. Viterbi forced alignment is a very useful tool in spoken language processing as it can provide the opti-
mal state-time alignment with the utterances. This detailed alignment can then be used for different purposes, in-
cluding discriminant training, concatenated speech synthesis, etc.

624
Basic Search Algorithms
Clearly,
min
ψ
should be a good under-estimate of the expected cost per frame for the
future unknown path. Therefore, the heuristic function
,
(
)
t T
N
h H
can be derived as:
,
min
(
)
(
)
t T
N
h H
T
t ψ
=
−
(12.26)
Although
min
ψ
is obtained empirically, stack decoding based on Eq. (12.26) will generally
find the optimal solution. However, the search using
min
ψ
usually runs very slowly, since
Eq. (12.26) always under-estimates the true cost for any portion of speech. In practice, a
heuristic function like
avg
ψ
that may over-estimate has to be used to prune more hypotheses.
This speeds up the search at the expense of possible search errors, because
avg
ψ
should rep-
resent the average cost per frame for any portion of speech. In fact, there is an argument that
one might be able to use a heuristic function even less than
avg
ψ
. The argument is that
avg
ψ
is derived from the correct path (training data) and the average cost per frame for all paths
during search should be less than
avg
ψ
because the paths undoubtedly include correct and
incorrect ones.
12.5.2.
When to Extend New Words
Since stack decoding is executed asynchronously, it becomes necessary to detect when a
phone/word ends, so that the search can extend to the next phone/word. If we have a cost
measure that indicates how well an input feature vector of any length matches the evaluated
model sequence, this cost measure should drop slowly for the correct phone/word and rise
sharply for an incorrect phone/word. In order to do so, it implies we must be able to compare
hypotheses of different lengths.
The first thing that comes to mind for this cost measure is simply the forward score
1
1
log (
,
|
)
t
k
t
P
s
w
−
x
, which represents the likelihood of producing acoustic observation
1
tx
based on word sequence
1
k
w
and ending at state
ts . However, it is definitely not suitable
because it is deemed to be smaller for a shorter acoustic input vector. This causes the search
to almost always prefer short phones/words, resulting in many insertion errors. Therefore,
we must derive some normalized score that satisfies the desired property described above.
The normalized cost
1
1
ˆ(
,
|
)
t
k
t
C
s
w
x
can be represented as follows [6, 24]:
1
1
1
1
1
1
(
,
|
)
ˆ(
,
|
)
log
log
(
,
|
)
log
t
k
t
k
t
k
t
t
t
t
P
s
w
C
s
w
P
s
w
t
γ
γ




= −
= −
+






x
x
x
(12.27)
where γ ( 0
1
γ
<
< ) is a constant normalization factor.

Stack decoding (A* Search)
625
Suppose the search is now evaluating a particular word
k
w ; we can define
min
ˆ
( )
C
t
as
the minimum cost for
1
1
ˆ(
,
|
)
t
k
t
C
s
w
x
for all the states of
k
w , and
max ( )t
α
as the maximum
forward probability for
1
1
(
,
|
)
t
k
t
P
s
w
x
for all the states of
k
w . That is,
min
1
1
ˆ
ˆ
( )
min
(
,
|
)
t
k
t
k
t
s
w
C
t
C
s
w
∈


=


x
(12.28)
max
1
1
( )
max
(
|
,
)
t
k
t
k
t
s
w
t
P
w
s
α
∈


=


x
(12.29)
We want
min
ˆ
( )
C
t
to be near 0 just so long as the phone/word we are evaluating is the correct
one and we have not gone beyond its end. On the other hand, if the phone/word we are
evaluating is the incorrect one or we have already passed its end, we want the
min
ˆ
( )
C
t
to be
rising sharply. Similar to the procedure of finding the admissible heuristic function, we can
set the normalized factor γ empirically during training so that
min
ˆ
( )
0
C
T =
when we know
the correct word sequence W that produces acoustic observation sequence
1
T
x . Based on Eq.
(12.27), γ should be set to:
max ( )
T
T
γ
α
=
(12.30)
Figure 12.22
min
ˆ
( )
C
t
and
1
1
ˆ(
,
(
)|
)
t
k
t
k
C
s
FS w
w
=
x
as functions of time t. The valley region
represents possible ending times for the correct phone/word.
Significant range
of ending time
Most likely
word-ending
1
1
ˆ(
,
(
)|
)
t
k
t
k
C
s
FS w
w
=
x
Significant
threshold
t
min
ˆ
( )
C
t

626
Basic Search Algorithms
Figure 12.22 shows a plot of
min
ˆ
( )
C
t
as a function of time for correct match. In addi-
tion, the cost for the final state
(
)
k
FS w
of word
k
w ,
1
1
ˆ(
,
(
)|
)
t
k
t
k
C
s
FS w
w
=
x
, which is the
score for
k
w -ending path, is also plotted. There should be a valley centered around 0 for
1
1
ˆ(
,
(
)|
)
t
k
t
k
C
s
FS w
w
=
x
, which indicates the region of possible ending time for the correct
phone/word. Sometimes a stretch of acoustic observations match may better than the aver-
age cost, pushing the curve below 0. Similarly, a stretch of acoustic observations may match
worse than the average cost, pushing the curve above 0.
There is an interesting connection between the normalized factor γ and the heuristic
estimate of the expected cost per frame, ψ , defined in Eq. (12.25). Since the cost is simply
the logarithm on the inverse posterior probability, we get the following equation.
1
1
max
1
ˆ
log
(
|
)
log
( )
log
T
T
P
T
T
ψ
α
γ
−


=
= −
= −


x
W
(12.31)
Equation (12.31) reveals that these two quantities are basically the same estimate. In fact, if
we subtract the heuristic function
(
)
t
N
f H
defined in Eq. (12.24) by the constant
(
)
log
Te
,
we get exactly the same quantity as the one defined in Eq. (12.27). Decisions on which path
to extend first based on the heuristic function and when to extend the search to the next
word/phone are basically centered on comparing partial theories with different lengths.
Therefore, the normalized cost
1
1
ˆ(
,
|
)
t
k
t
C
s
w
x
can be used for both purposes.
Figure 12.23 Unnormalized cost
1
1
(
,
|
)
t
k
t
C
s
w
x
for optimal path and other competing paths as a
function of time.
Based on the connection we have established, the heuristic function,
(
)
t
N
f H
, which
estimate the goodness of a path is simply replaced by the normalized evaluation function
optimal path
t
w5
w4
w3
w2
w1

Stack decoding (A* Search)
627
1
1
ˆ(
,
|
)
t
k
t
C
s
w
x
. If we plot the un-normalized cost
1
1
(
,
|
)
t
k
t
C
s
w
x
for the optimal path and other
competing paths as the function time t, the cost values increase as paths get longer (illus-
trated in Figure 12.23) because every frame adds some non-negative cost to the overall cost.
It is clear that using un-normalized cost function
1
1
(
,
|
)
t
k
t
C
s
w
x
generally results in a breadth-
first search. What we want is an evaluation that decreases slightly along the optimal path,
and hopefully increases along other competing paths. Clearly, the normalized cost function
1
1
ˆ(
,
|
)
t
k
t
C
s
w
x
fulfills this role, as shown in Figure 12.24.
Figure 12.24 Normalized cost
1
1
ˆ(
,
|
)
t
k
t
C
s
w
x
for the optimal path and other competing paths as
a function of time.
Equation (12.30) is a context-less estimation of the normalized factor, which is also re-
ferred to as zero-order estimate. To improve the accuracy of the estimate, you can use con-
text-dependent higher-order estimates like [24]:
(
)
i
i
γ
γ
=
x
first-order estimate
1
(
,
)
i
i
i
γ
γ
−
=
x x
second-order estimate
1
1
(
,
,
,
)
i
i
i
i N
γ
γ
−
−
+
=
x x
x

n-order estimate
Since the normalized factor e is estimated from the training data that is also used to
train the parameters of the HMMs, the normalized factor
ie tends to be an over estimate. As
a result,
max ( )t
α
might rise slowly for test data even when the correct phone/word model is
evaluated. This problem is alleviated by introducing some other scaling factor
1
δ <
so that
max ( )t
α
falls slowly for test data for when evaluating the correct phone/word model. The
best solution for this problem is to use an independent data set other than the training data to
derive the normalized factor
iγ .
t
w5
w4
w3
w2
w1
optimal path

628
Basic Search Algorithms
12.5.3.
Fast Match
Even with an efficient heuristic function and mechanism to determine the ending time for a
phone/word, stack decoding could still be too slow for large-vocabulary speech recognition
tasks. As described in Section 12.5.1, an effective underestimated heuristic function for the
remaining portion of speech is very difficult to derive. On the other hand, a heuristic esti-
mate for the immediate short segment that usually corresponds to the next phone or word
may be feasible to attain. In this section, we describe the fast-match mechanism that reduces
phone/word candidates for detailed match (expansion).
In asynchronous stack decoding, the most expensive step is to extend the best subpath.
For a large-vocabulary search, it implies the calculation of
(
|
)
t k
t
P
w
+
x
over the entire vo-
cabulary size |
|
V . It is desirable to have a fast computation to quickly reduce the possible
words starting at a given time t to reduce the search space. This process is often referred to
as fast match [15, 35]. In fact, fast match is crucial to stack decoding, of which it becomes
an integral part. Fast match is a method for the rapid computation of a list of candidates that
constrain successive search phases. The expensive detailed match can then be performed
after fast match. In this sense, fast match can be regarded as an additional pruning threshold
to meet before new word/phone can be started.
Fast match, by definition, needs to use only a small amount of computation. However,
it should also be accurate enough not to prune away any word/phone candidates that partici-
pate in the best path eventually. Fast match is in general characterized by the approximations
that are made in the acoustic/language models in order to reduce computation. There is an
obvious trade-off between these two objectives. Fortunately, many systems [15] have dem-
onstrated that one needs to sacrifice very little accuracy in order to speed up the computation
considerably.
Similar to admissibility in A* search, there is also an admissibility property in fast
match. A fast match method is called admissible if it never prunes away the word/phone
candidates that participate in the optimal path. In order words, a fast match is admissible if
the recognition errors that appear in a system using the fast match followed by a detailed
match are those that would appear if the detailed match was carried out for all words/phones
in the vocabulary. Since fast match can be applied to either word or phone level, as we de-
scribe in the next section, we explain the admissibility for the case of word-level fast match
for simplicity. The same principle can be easily extended to phone-level fast match.
Let V be the vocabulary and
(
|
)
C
w
X
be the cost of a detailed match between input X
and word w. Now
(
|
)
F
w
X
is an estimator of
(
|
)
C
w
X
that is accurate enough and fast to
compute. A word list selected by fast match estimator can be attained by first computing
(
|
)
F
w
X
for each word w of the vocabulary. Suppose
b
w is the word for which the fast
match has a minimum cost value:
arg min
(
|
)
b
w V
w
F
w
∈
=
X
(12.32)

Stack decoding (A* Search)
629
After computing
(
|
)
b
C
w
X
, the detailed match cost for
b
w , we form the fast match word
list, Λ , from the word w in the vocabulary such that
(
|
)
F
w
X
is no greater than
(
|
)
b
C
w
X
.
In other words,
{
}
|
(
|
)
(
|
)
b
w
V F
w
C
w
Λ =
∈
≤
X
X
(12.33)
Similar to the admissibility condition for A* search [3, 33], the fast match estimator
( )
F •
conducted in the way described above is admissible if and only if
(
|
)
F
w
X
is always
an under-estimator (lower bound) of detailed match
(
|
)
C
w
X
. That is,
(
|
)
(
|
)
,
F
w
C
w
w
≤
∀
X
X
X
(12.34)
The proof is straightforward. If the word
c
w has a lower detailed match cost
(
|
)
c
C
w
X
, you
can prove that it must be included in the fast match list Λ because
(
|
)
(
|
) and
(
|
)
(
|
)
(
|
)
(
|
)
c
b
c
c
c
b
C
w
C
w
F
w
C
w
F
w
C
w
≤
≤

≤
X
X
X
X
X
X
Therefore, based on the definition of
,
c
w
Λ
∈Λ
Now the task is to find an admissible fast match estimator. Bahl et al. [6] proposed one
fast match approximation for discrete HMMs. As we will see later, this fast match approxi-
mation is indeed equivalent to a simplification of the HMM structure. Given the HMM for
word w and an input sequence
1
Tx
of codebook symbols describing the input signal, the
probability that the HMM w produces the VQ sequence
1
Tx
is given by (according to Chap-
ter 8):
1
2
1
1
2
,
,
1
(
|
)
( ,
,
)
(
|
)
T
T
T
w
T
w
i
i
s s
s
i
P x
w
P s s
s
P x
s
=


=





∏


(12.35)
Since we often use Viterbi approximation instead of the forward probability, the equation
above can be approximated by:
1
2
1
1
2
,
,
1
(
|
)
max
( ,
,
)
(
|
)
T
T
T
w
T
w
i
i
s s
s
i
P x
w
P s s
s
P x
s
=


≅




∏


(12.36)
The detailed match cost
(
|
)
C
w
X
can now be represented as:
1
2
1
2
,
,
1
(
|
)
min
log
( ,
,
)
(
|
)
T
T
w
T
w
i
i
s s
s
i
C
w
P s s
s
P x
s
=






=
−



	






∏
X


(12.37)
Since the codebook size is finite, it is possible to compute, for each model w, the high-
est output probability for every VQ label c among all states
ks
in HMM w. Let’s define
( )
w
m
c
to be the following:

630
Basic Search Algorithms
( )
max
( |
)
max
( )
k
k
w
w
k
k
s
w
s
w
m
c
P c s
b c
∈
∈
=
=
(12.38)
We can further define the
max ( )
q
w as the maximum state sequence with respect to T, i.e., the
maximum probability of any complete path in HMM w.
[
]
max
1
2
( )
max
( ,
,
)
w
T
T
q
w
P s s
s
=

(12.39)
Now let’s define the fast match estimator
(
|
)
F
w
A
as the following:
max
1
(
|
)
log
( )
(
)
T
w
i
i
F
w
q
w
m
x
=


= −




∏
X
(12.40)
It is easy to show the fast match estimator
(
|
)
(
|
)
F
w
C
w
≤
X
X
is admissible based on Eq.
(12.38) to Eq. (12.40).
Figure 12.25 The equivalent one-state HMM corresponding to fast match computation defined
in Eq. (12.40) [15].
The fast match estimator defined in Eq. (12.40) requires T+1 additions for a vector se-
quence of length T. The operation can be viewed as equivalent to the forward computation
with a one-state HMM of the form shown in Figure 12.25. This correspondence can be in-
terpreted as a simplification of the original multiple-state HMM into such a one-state HMM.
It thus explains why fast match can be computed much faster than detailed match. Readers
should note that this HMM is not actually a true HMM by strict definition, because the out-
put probability distribution
( )
w
m c
and the transition probability distribution do not add up
to one.
The fast match computation defined in Eq. (12.40) discards the sequence information
with the model unit since the computation is independent of the order of input vectors.
Therefore, one needs to decide the acoustic unit for fast match. In general, the longer the
unit, the faster the computation is, and, therefore, the larger the under-estimation of detailed
match scores
(
|
)
C
w
X
. It thus becomes a trade-off between accuracy and speed.
Now let’s analyze the real speedup by using fast match to reduce the vocabulary V to
the list Λ , followed by the detailed match. Let |V| and | Λ | be the sizes for the vocabulary V
and the fast match short list Λ . Suppose
ft
and
dt
are the times required to compute one
fast match score and one detailed match score for one word, respectively. Then, the total
time required for the fast match followed by the detailed match is
|
|
|
|
f
d
t
V
t
+
Λ
whereas
max( )
q
w
1.0
(
)
w
i
m
x

Stack decoding (A* Search)
631
the time required in doing the detailed match alone for the entire vocabulary is
|
|
dt
V . The
speed up ratio is then given as follows:
1
|
|
|
|
f
d
t
t
V


Λ
+




(12.41)
We need
ft
to be much smaller than
dt
and | Λ | to be much smaller than |V| to have a sig-
nificant speed-up using fast match. Using our admissible fast match estimator in Eq. (12.40),
the time complexity of the computation for
(
|
)
F
w
X
is T instead of
2
N T for
(
|
)
D
w
X
,
where N is the number of states in the detailed acoustic model. Therefore, the
f
d
t
t
saving
is about
2
N .
In general, in order to make | Λ | much smaller than |V|, one needs a very accurate fast
match estimator that could result in
f
d
t
t
≈
. This is why we often relax the constraint of
admissibility, although it is a nice principle to adhere to. In practice, most real-time speech
recognition systems don’t necessarily obey the admissibility principle with the fast match.
For example, Bahl et al. [10], Laface et al., [22] and Roe et al., [36] used several techniques
to construct off-line groups of acoustically similar words. Armed with this grouping, they
can use an aggressive fast match to select only a very short list of words and words acousti-
cally similar to the words in this list are added to form the NV list for further detailed match
processing. By doing so, they are able to report a very efficient fast match method that
misses the correct word only 2% of the time. When non-admissible fast match is used, one
needs to minimize the additional search error introduced by fast match empirically.
Bahl et al. [6] use a one-state HMM as their fast match units and a tree-structure lexi-
con similar to the lexical tree structures introduced in Chapter 13 to construct the short word
list Λ for next-word expansion in stack decoding. Since the fast match tree search is also
done in an asynchronous way, the ending time of each phone is detected using normalized
scores similar to those described in Section 12.5.2. It is based on the same idea that this
normalized score rises slowly for the correct phone, while it drops rapidly once the end of
phone is encountered (so the model is starting to go toward the incorrect phones). During the
asynchronous lexical tree search, the unpromising hypotheses are also pruned away by a
pruning threshold that is constantly changing once a complete hypothesis (a leaf node) is
obtained. On a 20,000-word dictation task, such a fast match scheme was about 100 times
faster than detailed match and achieved real-time performance on a commercial workstation
with only 0.34% increase in the word error rate being introduced by the fast match process.
12.5.4.
Stack Pruning
Even with efficient heuristic functions, mechanism to determine the ending time for
phone/word, and fast match, stack decoding might still be too slow for large-vocabulary
speech recognition tasks. A beam within the stack, which saves only a small number of

632
Basic Search Algorithms
promising hypotheses in the OPEN list, is often used to reduce search effort. This stack
pruning is very similar to beam search. A predetermined threshold ε is used to eliminate
hypotheses whose cost score is much worse than the best path so far.
Both fast match and stack pruning could introduce search errors where the eventual
optimal path is thrown away prematurely. However, the impact could be reduced to a mini-
mum by empirically adjusting the thresholds in both methods.
The implementation of stack decoding is, in general, more complicated, particularly
when some inevitable pruning strategies are incorporated to make the search more efficient.
The difficulty of devising both an effectively admissible heuristic function for
( )
h •
and an
effective estimation of normalization factors for boundary determination have limited the
advantage that stack decoders have over Viterbi decoders. Unlike stack decoding, time-
synchronous Viterbi beam search can use an easy comparison of same-length path without
heuristic determination of word boundaries. As described in the earlier sections, these sim-
ple and unified features of Viterbi beam search allows researchers to incorporate various
sound techniques to improve the efficiency of search. Therefore, time-synchronous Viterbi
Beam search enjoys a much broader popularity in the speech community. However, the
principle of stack decoding is essential particularly for n-best and lattice search. As we de-
scribe in Chapter 13, stack decoding plays a very crucial part in multiple-pass search strate-
gies for n-best and lattice search because the early pass is able to establish a near-perfect
estimate of the remaining path.
12.5.5.
Multistack Search
Even with the help of normalized factor γ or heuristic function
( )
h • , it is still more effec-
tive to compare hypotheses of the same length than those of different lengths, because hy-
potheses with the same length are compared based on the true forward matching score. In-
spired by the time-synchronous principle in Viterbi beam search, researchers [8, 35] propose
a variant stack decoding based on multiple stacks.
Multistack search is equivalent to a best-first search algorithm running on multiple
stacks time-synchronously. Basically, the search maintains a separate stack for each time
frame t, so it never needs to compare hypotheses of different lengths. The search runs time-
synchronously from left to right just like time-synchronous Viterbi search. For each time
frame t, multistack search extracts the best path out the t-stack, computes one-word exten-
sions, and places all the new theories into the corresponding stacks. When the search fin-
ishes, the top path in the last stack is our optimal path. Algorithm 12.7 illustrates the mutlis-
tack search algorithm.
This time-synchronous multistack search is designed based on the fact that by the time
the
tht
stack is extended; it already contains the best path that could ever be placed into it.
This phenomenon is virtually a variant of the dynamic programming principle introduced in
Chapter 8. To make multistack more efficient, some heuristic pruning can be applied to re-
duce the computation. For example, when the top path of each stack is extended for one
more word, we could only consider extensions between minimum and maximum duration.
On the other hand, when some heuristic pruning is integrated into the multistack search, one

Historical Perspective and Further Reading
633
might need to use a small beam in Step 2 of Algorithm 12.7 to extend more than just the best
path to guarantee the admissibility.
ALGORITHM 12.7 MULTISTACK SEARCH
1.
Initialization: for each word v in vocabularyV
for
1,2,
,
t
T
=

Compute
1
(
| )
t
C
v
x
and insert it to
tht
stack
2.
Iteration: for
1,2,
,
1
t
T
=
−

Sort the
tht
stack and pop the top path
1
1
(
|
)
t
k
C
w
x
out the stack
for each word v in vocabularyV
for
1,
2,
,
t
t
T
τ = +
+

Extend the path
1
1
(
|
)
t
k
C
w
x
by word v to get
1
1
1
(
|
)
k
C
w
τ
+
x
where
1
1
1 ||
k
k
w
w
v
+ =
and || means string concatenation
Place
1
1
1
(
|
)
k
C
w
τ
+
x
in
th
τ
stack
3.
Termination: Sort the
th
T
stack and the top path is the optimal word sequence
12.6.
HISTORICAL PERSPECTIVE AND FURTHER READING
Search has been one of the most important topics in artificial intelligence (AI) since the ori-
gins of the field. It plays the central role in general problem solving [29] and computer
games. [43], Nilsson’s Principles of Artificial Intelligence [32] and Barr and Feigenbaum’s
The Handbook of Artificial Intelligence [11] contain a comprehensive introduction to state-
space search algorithms. A* search was first proposed by Hart et al. [17]. A* was thought to
be derived from Dijkstra’s algorithm [13] and Moore’s algorithm [27]. A* search is similar
to the branch-and-bound algorithm [23, 39], widely used in operations research. The proof
of admissibility of A* search can be found in [32].
The application of beam search in speech recognition was first introduced by the
HARPY system [26]. It wasn’t widely popular until BBN used it for their BYBLOS system
[37]. There are some excellent papers with detailed description of the use of time-
synchronous Viterbi beam search for continuous speech recognition [24, 31]. Over the years,
many efficient implementations and improvements have been introduced for time-
synchronous Viterbi beam search, so real-time large-vocabulary continuous speech recogni-
tion can be realized on a general-purpose personal computer.
On the other hand, stack decoding was first developed by IBM [9]. It is successfully
used in IBM’s large-vocabulary continuous speech recognition systems [3, 16]. Lacking of
time-synchronous framework, comparing theories of different lengths and extending theo-
ries is more complex as described in this chapter. Because of the complexity of stack decod-
ing, far fewer publications and systems are based on it than on Viterbi beam search [16, 19,

634
Basic Search Algorithms
20, 35]. With the introduction of multi-stack search [8], stack decoding in essence has actu-
ally come very close to time-synchronous Viterbi beam search.
Stack decoding is typically integrated with fast match methods to improve its effi-
ciency. Fast match was first implemented for isolated word recognition to obtain a list of
potential word candidates [5, 7]. The paper by Gopalakrishnan et al.’s paper [15] contains a
comprehensive description of fast match techniques to reduce the word expansion for stack
decoding. Besides the fast match techniques described in this chapter , there are a number of
alternative approaches [5, 21, 41]. Waast’s fast match [41], for example, is based on a binary
classification tree built automatically from data that comprise both phonetic transcription
and acoustic sequence.
REFERENCES
[1]
Aho, A., J. Hopcroft, and J. Ullman, The Design and Analysis of Computer Algo-
rithms, 1974, Addison-Wesley Publishing Company.
[2]
Alleva, F., X. Huang, and M. Hwang, "An Improved Search Algorithm for Con-
tinuous Speech Recognition," Int. Conf. on Acoustics, Speech and Signal Process-
ing, 1993, Minneapolis, MN pp. 307-310.
[3]
Bahl, L.R. and e. al, "Large Vocabulary Natural Language Continuous Speech
Recognition," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Proc-
essing, 1989, Glasgow, Scotland pp. 465-467.
[4]
Bahl, L.R., et al., "Language-Model/Acoustic Channel Balance Mechanism," IBM
Technical Disclosure Bulletin, 1980, 23(7B), pp. 3464-3465.
[5]
Bahl, L.R., et al., "Obtaining Candidate Words by Polling in a Large Vocabulary
Speech Recognition System," Proc. of the IEEE Int. Conf. on Acoustics, Speech
and Signal Processing, 1988 pp. 489-492.
[6]
Bahl, L.R., et al., "A Fast Approximate Acoustic Match for Large Vocabulary
Speech Recognition," IEEE Trans. on Speech and Audio Processing, 1993(1), pp.
59-67.
[7]
Bahl, L.R., et al., "Matrix Fast Match: a Fast Method for Identifying a Short List of
Candidate Words for Decoding," Proc. of the IEEE Int. Conf. on Acoustics, Speech
and Signal Processing, 1989, Glasgow, Scotland pp. 345-347.
[8]
Bahl, L.R., P.S. Gopalakrishnan, and R.L. Mercer, "Search Issues in Large Vocabu-
lary Speech Recognition," Proc. of the 1993 IEEE Workshop on Automatic Speech
Recognition, 1993, Snowbird, UT.
[9]
Bahl, L.R., F. Jelinek, and R. Mercer, "A Maximum Likelihood Approach to Con-
tinuous Speech Recognition," IEEE Trans. on Pattern Analysis and Machine Intel-
ligence, 1983(2), pp. 179-190.
[10]
Bahl, L.R., et al., "Constructing Candidate Word Lists Using Acoustically Similar
Word Groups," IEEE Trans. on Signal Processing, 1992(1), pp. 2814-2816.
[11]
Barr, A. and E. Feigenbaum, The Handbook of Artificial Intelligence: Volume I,
1981, Addison-Wesley.
[12]
Cettolo, M., R. Gretter, and R.D. Mori, "Knowledge Integration" in Spoken Dia-
logues with Computers, R.D. Mori, Editor 1998, London, pp. 231-256, Academic
Press.

Historical Perspective and Further Reading
635
[13]
Dijkstra, E.W., "A Note on Two Problems in Connection with Graphs," Nu-
merische Mathematik, 1959, 1, pp. 269-271.
[14]
Gauvain, J.L., et al., "The LIMSI Speech Dictation System: Evaluation on the
ARPA Wall Street Journal Corpus," Proc. of the IEEE Int. Conf. on Acoustics,
Speech and Signal Processing, 1994, Adelaide, Australia pp. 129-132.
[15]
Gopalakrishnan, P.S. and L.R. Bahl, "Fast Match Techniques" in Automatic Speech
and Speaker Recognition, C.H. Lee, F.K. Soong, and K.K. Paliwal, eds. 1996,
Norwell, MA, pp. 413-428, Kluwer Academic Publishers.
[16]
Gopalakrishnan, P.S., L.R. Bahl, and R.L. Mercer, "A Tree Search Strategy for
Large-Vocabulary Continuous Speech Recognition," Proc. of the IEEE Int. Conf.
on Acoustics, Speech and Signal Processing, 1995, Detroit, MI pp. 572-575.
[17]
Hart, P.E., N.J. Nilsson, and B. Raphael, "A Formal Basis for the Heuristic Deter-
mination of Minimum Cost Paths," IEEE Trans. on Systems Science and Cybernet-
ics, 1968, 4(2), pp. 100-107.
[18]
Huang, X., et al., "Microsoft
Windows Highly Intelligent Speech Recognizer:
Whisper," IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1995 pp.
93-96.
[19]
Jelinek, F., Statistical Methods for Speech Recognition, 1998, Cambridge, MA,
MIT Press.
[20]
Kenny, P., et al., "A* -Admissible Heuristics for Rapid Lexical Access," IEEE
Trans. on Speech and Audio Processing, 1993, 1, pp. 49-58.
[21]
Kenny, P., et al., "A New Fast Match for Very Large Vocabulary Continuous
Speech Recognition," IEEE Int. Conf. on Acoustics, Speech and Signal Processing,
1993, Minneapolis, MN pp. 656-659.
[22]
Laface, P., L. Fissore, and F. Ravera, "Automatic Generation of Words toward
Flexible Vocabulary Isolated Word Recognition," Proc. of the Int. Conf. on Spoken
Language Processing, 1994, Yokohama, Japan pp. 2215-2218.
[23]
Lawler, E.W. and D.E. Wood, "Branch-and-Bound Methods: A Survey," Opera-
tions Research, 1966(14), pp. 699-719.
[24]
Lee, K.F. and F.A. Alleva, "Continuous Speech Recognition" in Recent Progress in
Speech Signal Processing, S. Furui and M. Sondhi, eds. 1990, Marcel Dekker, Inc.
[25]
Lee, K.F., H.W. Hon, and R. Reddy, "An Overview of the SPHINX Speech Recog-
nition System," IEEE Trans. on Acoustics, Speech and Signal Processing, 1990,
38(1), pp. 35-45.
[26]
Lowerre, B.T., The HARPY Speech Recognition System, PhD Thesis in Computer
Science Department 1976, Carnegie Mellon University, .
[27]
Moore, E.F., "The Shortest Path Through a Maze," Int. Symp. on the Theory of
Switching, 1959, Cambridge, MA, Harvard University press pp. 285-292.
[28]
Murveit, H., et al., "Large Vocabulary Dictation Using SRI's DECIPHER Speech
Recognition System: Progressive Search Techniques," Proc. of the IEEE Int. Conf.
on Acoustics, Speech and Signal Processing, 1993, Minneapolis, MN pp. 319-322.
[29]
Newell, A. and H.A. Simon, Human Problem Solving, 1972, Englewood Cliffs, NJ,
Prentice Hall.

636
Basic Search Algorithms
[30]
Ney, H. and X. Aubert, "Dynamic Programming Search: From Digit Strings to
Large Vocabulary Word Graphs" in Automatic Speech and Speaker Recognition,
C.H. Lee, F. Soong, and K.K. Paliwal, eds. 1996, Boston, pp. 385-412, Kluwer
Academic Publishers.
[31]
Ney, H. and S. Ortmanns, Dynamic Programming Search for Continuous Speech
Recognition, in IEEE Signal Processing Magazine, 1999. pp. 64-83.
[32]
Nilsson, N.J., Principles of Artificial Intelligence, 1982, Berlin, Germany, Springer
Verlag.
[33]
Nilsson,
N.J.,
Artificial
Intelligence:
A
New
Synthesis,
1998,
Academic
Press/Morgan Kaufmann.
[34]
Normandin, Y., R. Cardin, and R.D. Mori, "High-Performance Connected Digit
Recognition Using Maximum Mutual Information Estimation," IEEE Trans. on
Speech and Audio Processing, 1994, 2(2), pp. 299-311.
[35]
Paul, D.B., "An Efficient A* Stack Decoder Algorithm for Continuous Speech
Recognition with a Stochastic Language Model," Proc. of the IEEE Int. Conf. on
Acoustics, Speech and Signal Processing, 1992, San Francisco, California pp. 25-
28.
[36]
Roe, D.B. and M.D. Riley, "Prediction of Word Confusabilities for Speech Recog-
nition," Proc. of the Int. Conf. on Spoken Language Processing, 1994, Yokohama,
Japan pp. 227-230.
[37]
Schwartz, R., et al., "Context-Dependent Modeling for Acoustic-Phonetic Recogni-
tion of Speech Signals," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Sig-
nal Processing, 1985, Tampa, FLA pp. 1205-1208.
[38]
Steinbiss, V., et al., "The Philips Research System for Large-Vocabulary Continu-
ous-Speech Recognition," Proc. of the European Conf. on Speech Communication
and Technology, 1993, Berlin, Germany pp. 2125-2128.
[39]
Taha, H.A., Operations Research: An Introduction, 6th ed, 1996, Prentice Hall.
[40]
Tanimoto, S.L., The Elements of Artificial Intelligence : An Introduction Using
Lisp, 1987, Computer Science Press, Inc.
[41]
Waast, C. and L.R. Bahl, "Fast Match Based on Decision Tree," Proc. of the Euro-
pean Conf. on Speech Communication and Technology, 1995, Madrid, Spain pp.
909-912.
[42]
Winston, P.H., Artificial Intelligence, 1984, Reading, MA, Addison-Wesley.
[43]
Winston, P.H., Artificial Intelligence, 3rd ed, 1992, Reading, MA, Addison-
Wesley.
[44]
Woodland, P.C., et al., "Large Vocabulary Continuous Speech Recognition Using
HTK," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing,
1994, Adelaide, Australia pp. 125-128.

637
C H A P T E R
1 3
Large Vocabulary Search Algorithms
Chapter 12 discussed the basic search tech-
niques for speech recognition. However, the search complexity for large-vocabulary speech
recognition with high-order language models is still difficult to handle. In this chapter we
describe efficient search techniques in the context of time-synchronous Viterbi beam search,
which becomes the choice for most speech recognition systems because it is very efficient.
We use Microsoft Whisper as our case study to illustrate the effectiveness of various search
techniques. Most of the techniques discussed here can also be applied to stack decoding.
With the help of beam search, it is unnecessary to explore the entire search space or
the entire trellis. Instead, only the promising search state-space needs to be explored. Please
keep in mind the distinction between the implicit search graph specified by the grammar
network and the explicit partial search graph that is actually constructed by the Viterbi beam
search algorithm.
In this chapter we first introduce the most critical search organization for large-
vocabulary speech recognition—tree lexicons. Tree lexicons significantly reduce potential
search space, although they introduce many practical problems. In particular, we need to

638
Large Vocabulary Search Algorithms
address problems such as reentrant lexical trees, factored language model probabilities, sub-
tree optimization, and subtree polymorphism.
Various other efficient techniques also are introduced. Most of these techniques aim
for clever pruning with the hope of sparing the correct paths. For more effective pruning,
different layers of beams are usually used. While fast match techniques described in Chapter
12 are typically required for stack decoding, similar concepts and techniques can be applied
to Viterbi beam search. In practice, the look-ahead strategy is equally effective for Viterbi
beam search.
Although it is always desirable to use all the knowledge sources (KSs) in the search
algorithm, some are difficult to integrate into the left-to-right time-synchronous search
framework. One alternative strategy is to first produce an ordered list of sentence hypotheses
(a.k.a. n-best list), or a lattice of word hypotheses (a.k.a. word lattice) using relatively inex-
pensive KSs. More expensive KSs can be used to rescore the n-best list or the word lattice to
obtain the refined result. Such a multipass strategy has been explored in many large-
vocabulary speech recognition systems. Various algorithms to generate sufficient n-best lists
or the word lattices are described in the section on multipass search strategies.
Most of the techniques described in this chapter rely on nonadmissible heuristics.
Thus, it is critical to derive a framework to evaluate different search strategies and pruning
parameters.
13.1.
EFFICIENT MANIPULATION OF TREE LEXICON
The lexicon entry is the most critical component for large-vocabulary speech recognition,
since the search space grows linearly along with increased linear vocabulary. Thus an effi-
cient framework for handling large vocabulary undoubtedly becomes the most critical issue
for efficient search performance.
13.1.1.
Lexical Tree
The search space for n-gram discussed in Chapter 12 is organized based on a straightforward
linear lexicon; i.e., each word is represented as a linear sequence of phonemes, independent
of other words. For example, the phonetic similarity between the words task and tasks is not
leveraged. In a large-vocabulary system, many words may share the same beginning pho-
nemes. A tree structure is a natural representation for a large-vocabulary lexicon, as many
phonemes can be shared to eliminate redundant acoustic evaluations. The lexical tree-based
search is thus essential for building a real-time1 large-vocabulary speech recognizer.
Figure 13.1 shows an example of such a lexical tree, where common beginning pho-
nemes are shared. Each leaf corresponds to a word in the vocabulary. Please note that an
1 The term real-time means the decoding process takes no longer than the duration of the speech. Since the decod-
ing process can take place as soon as the speech starts, such a real-time decoder can provide real instantaneous
responses after speakers finish talking.

Efficient Manipulation of Tree Lexicon
639
extra null arc is used to form the leaf node for each word. This null arc has the following
two functions:
1. When the pronunciation transcription of a word is a prefix of other ones, the
null arc can function as one branch to end the word.
2. When there are homophones in the lexicon, the null arcs can function as lin-
guistic branches to represent different words such as two and to.
/a/
/d/
/b/
/c/
/c/
/z/
/c/
W1 = /ab/
W2 = /abc/
W2 = /abc/
W4 = /acz/
W5 = /ade/
Figure 13.1 An example of a lexical tree, where each branch corresponds to a shared phoneme
and the leaf corresponds to a word.
The advantage of using such a lexical tree representation is obvious: it can effectively
reduce the state search space of the trellis. Ney et al. [32] reported that a lexical tree repre-
sentation of a 12,306-word lexicon with only 43,000 phoneme arcs had a saving of factor of
2.5 over the linear lexicon with 100,800 phoneme arcs. Lexical trees are also referred to as
prefix trees, since they are efficient representations of lexicons with sharing among lexical
entries that have a common prefix. Table 13.1 shows the distribution of phoneme arcs for
this 12,306-word lexical tree. As one can see, even in the fourth level the number of pho-
neme arcs is only about one-third of the total number of words in the lexicon.
Table 13.1 Distribution of the tree phoneme arcs and active tree phoneme arc for a 12,306-
word lexicon using a lexical tree representation [32].
Level
1
2
3
4
5
6
≥7
Phoneme arcs
28
331
1511
3116
4380
4950
29.200
Average active arcs
23
233
485
470
329
178
206
The saving by using a lexical tree is substantial, because it not only results in consid-
erable memory saving for representing state search space but also saves tremendous time by
searching far fewer potential paths. Ney et al. [32] report that a tree organization of the lexi-
con reduces the total search effort by a factor of 7 over the linear lexicon organization. This
is because the lion’s share of hypotheses during a typical large-vocabulary search is on the

640
Large Vocabulary Search Algorithms
first and second phonemes of a word. Haeb-Umbach et al. [23] report that for a 12,306-word
dictation task, 79% and 16% of the state hypotheses are in the first and second phonemes,
when analyzing the distribution of the state hypotheses over the state position within a word.
Obviously, the effect is caused by the ambiguities at the word boundaries. The lexical tree
representation reduces that effort by evaluating common phonetic prefixes only once. Table
13.1 also shows the average number of active phoneme arcs in the layers of the lexical tree
[32]. Based on this table, you can expect that the overall search cost is far less than the size
of the vocabulary. This is the key reason why lexical tree search is widely used for large-
vocabulary continuous speech recognition systems.
The lexical tree search requires a sophisticated implementation because of a funda-
mental deficiency—a branch in a lexical tree representation does not correspond to a single
word with the exception of branches ending in a leaf. This deficiency translates to the fact
that a unique word identity is not determined until a leaf of the tree is reached. This means
that any decision about the word identity needs to be delayed until the leaf node is reached,
which results in the following complexities.
 Unlike a linear lexicon, where the language model score can be applied when
starting the acoustic search of a new word, the lexical tree representation has to
delay the application of the language model probability until the leaf is reached.
This may result in an increased search effort, because the pruning needs to be
done on a less reliable measure, unless a factored language model is used as dis-
cussed in Section 13.1.3
 Because of the delay of language model contribution by one word, we need to
keep a separate copy of an entire lexical tree for each unique language model
history.
13.1.2.
Multiple Copies of Pronunciation Trees
A simple lexical tree is sufficient if no language model or a unigram is used. This is because
the decision at time t depends on the current word only. However, for higher-order n-gram
models, the linguistic state cannot be determined locally. A tree copy is required for each
language model state. For bigrams, a tree copy is required for each predecessor word. This
may seem to be astonishing, because the potential search space is increased by the vocabu-
lary size. Fortunately, experimental results show only a small number of tree copies are re-
quired, because efficient pruning can eliminate most of the unneeded ones. Ney el al [32]
report that the search effort using bigrams is increased by only a factor of 2 over the unigram
case. In general, when more detailed (better) acoustic and/or language models are used, the
effect of a potentially increased search space is often compensated by a more focused beam
search from the use of more accurate models. In other words, although the static search
space might increase significantly by using more accurate models, the dynamic search space
can be under control (sometimes even smaller), thanks to improved evaluation functions.
To deal with tree copies [19, 23, 37], you can create redundant subtrees. When copies
of lexical trees are used to disambiguate active linguistic contexts, many of the active state

Efficient Manipulation of Tree Lexicon
641
hypotheses correspond to the same redundant unigram state due to the postponed application
of language models. To apply the language model sooner, and to eliminate redundant uni-
gram state computations, a successor tree, Ti,, can be created for each linguistic context i. Ti
encodes the nonzero n-grams of the linguistic context i as an isomorphic subgraph of the
unigram tree, T0. Figure 13.2 shows the organization of such successor trees and unigram
tree for bigram search. For each word w a successor tree,
w
T
is created with the set of suc-
cessor words that have nonzero bigram probabilities. Suppose u is a successor of w; the bi-
gram probability
( |
)
P u w is attached to the transition connecting the leaf corresponding to u
in the successor tree
w
T , with the root of the successor tree
uT . The unigram tree is a full-
size lexical tree and is shared by all words as the back-off lexical tree. Each leaf of the uni-
gram tree corresponds to one of |V| words in the vocabulary and is linked to the root of its
bigram successor tree (
uT ) by an arc with the corresponding unigram probability
( )
P u . The
backoff weight,
( )
u
α
, of predecessor u is attached to the arc which links the root of succes-
sor tree
uT to the root of the unigram tree.
unigram tree
P(w)
w
u
v
P(u)
P(v)
α(u)
Tw
Tu
Tv
u
u
P(u|w)
P(u|v)
bigram successor
trees
Figure 13.2 Successor trees and unigram trees for bigram search [13].
A careful search organization is required to avoid computational overhead and to
guarantee a linear time complexity for exploring state hypotheses. In the following sections
we describe techniques to achieve efficient lexical tree recognizers. These techniques in-
clude factorization of language model probabilities, tree optimization, and exploiting subtree
dominance.

642
Large Vocabulary Search Algorithms
13.1.3.
Factored Language Probabilities
As mentioned in Section 13.1.2, search is more efficient if a detailed knowledge source can
be applied at an early stage. The idea of factoring the language model probabilities across
the tree is one such example [4, 19]. When more than one word shares a phoneme arc, the
upper bound of their probability can be associated to that arc.2 The factorization can be ap-
plied to both the full lexical tree (unigram) and successor trees (bigram or other higher-order
language models).
An unfactored tree only has language model probabilities attached to the leaf nodes,
and all the internal nodes have probability 1.0. The procedure for factoring the probabilities
across the tree computes the maximum of each node n in the tree according to Eq. (13.1).
The tree can then be factored according to Eq. (13.2) so when you traverse the tree you can
multiply
*( )
F
n
along the path to get the needed language probability.
*
( )
( )
max
( )
x child n
P n
P x
∈
=
(13.1)
*
*
*
( )
( )
(
( ))
P n
F
n
P
parent n
=
(13.2)
An illustration of the factored probabilities is shown in Table 13.2. Using this lexicon,
we create the tree depicted in Figure 13.3(a). In this figure the unlabeled internal nodes have
a probability of 1.0. We distribute the probabilities according to Eq. (13.1) in Figure 13.3(b),
which is factored according to Eq. (13.2), resulting in Figure 13.3(c).
Table 13.2 Sample probabilities P(wj) and their pseudoword pronunciations [4].
wj
Pronunciation
P(wj)
w0
/a b c/
0.1
w1
/a b c/
0.4
w2
/a c z/
0.3
w3
/d e/
0.2
Using the upper bounds in the factoring algorithm is not an approximation, since the
correct language model probabilities are calculated by the product of values traversed along
each path from the root to the leaves. However, you should note that the probabilities of all
the branches of a node do not sum to one. This can solved by replacing the upper-bound
(max) function in Eq. (13.1) with the sum.
*
( )
( )
( )
x child n
P n
P x
∈
= 
(13.3)
2 The choice of upper bound is because it is an admissible estimate of the path no matter which word will be chosen
later.

Efficient Manipulation of Tree Lexicon
643
Figure 13.3 (a) Unfactored lexical tree; (b) distributed probabilities with computed
*( )
P n ; (c)
factored tree
*( )
F
n
[4].
To guarantee that all the branches sum to one, Eq. (13.2) should also be replaced by
the following equation:
*
*
*
(
( ))
( )
( )
( )
x child parent n
P n
F
n
P
x
∈
=

(13.4)
A new illustration of the distribution of LM probabilities by using sum instead of up-
per bound is shown in Figure 13.4. Experimental results have shown that the factoring
method with either sum or upper bound has comparable search performance.
Figure 13.4 Using sum instead of upper bound when factoring tree, the corresponding (a) un-
factored lexical tree; (b) distributed probabilities with computed
*( )
P n ; (c) factored tree with
computed
*( )
F
n
[4].
One interesting observation is that the language model score can be regarded as a heu-
ristic function to estimate the linguistic expectation of the current word to be searched. In a
linear representation of the pronunciation lexicon, application of the linguistic expectation
was straightforward, since each state is associated with a unique word. Therefore, given the
context defined by the hypothesis under consideration, the expectation for the first phone of
1.0
.25
1.0
1.0
1.0
1.0
0.4
1.0
.75
0.5
1.0
a
d
b
c
e
z
c
w1
0.4
w0
0.1
w2
0.2
w3
0.3
(a)
(b)
0.4
0.1
0.2
0.3
0.4
0.4
0.4
0.3
0.3
0.2
0.4
(c)
0.8
0.2
1.0
1.0
1.0
0.8
1.0
1.0
0.375
0.2
.625
a
d
b
c
e
z
c
w1
0.4
w0
0.1
w2
0.2
w3
0.3
(a)
(b)
0.4
0.1
0.2
0.3
0.5
0.8
1.0
0.3
0.3
0.2
0.5
(c)

644
Large Vocabulary Search Algorithms
word
iw is just
1
1
(
|
)
i
i
P w
w −
. After the first phone, the expectation for the rest of the phones
becomes 1.0, since there is only one possible phone sequence when searching the word
iw .
However, for the tree lexicon, it is necessary to compute
1
1
1
(
|
,
)
j
i
j
i
E p
p
w
−
−
, the expectation
of phone
jp given the phonetic prefix
1
1
j
p −and the linguistic context
1
1
i
w −. Let
( ,
)
k
j w
φ
denote the phonetic prefix of length j for wk . Based on Eqs. (13.1) and (13.2), we can com-
pute the expectation as:
1
1
1
1
1
1
1
1
(
|
)
(
|
,
)
(
|
)
i
j
i
c
j
i
p
P w
w
E p
p
w
P w
w
−
−
−
−
=
(13.5)
where
1
1
1
arg max(
|
, ( ,
)
)
i
j
k
k
k
c
w
w
j w
p
φ
−
=
=
and
1
1
1
1
arg max(
|
, (
1,
)
)
i
j
k
k
k
p
w
w
j
w
p
φ
−
−
=
−
=
.
Based on Eq. (13.5), an arbitrary n-gram model or even a stochastic context-free grammar
can be factored accordingly.
13.1.3.1.
Efficient Memory Organization of Factored Lexical Trees
A major drawback to the use of successor trees is the large memory overhead required to
store the additional information that encodes the structure of the tree and the factored lin-
guistic probabilities. For example, the 5.02 million bigrams in the 1994 NABN (North
American Business News) model require 18.2 million nodes. Given a compact binary tree
representation that uses 4 bytes of memory per node, 72.8 million bytes are required to store
the predecessor-dependent lexical trees. Furthermore, this tree representation is not as ame-
nable to data compression techniques as the linear bigram representation.
ALGORITHM 13.1 ENCODING THE LEXICAL SUCCESSOR TREES (LST)
For each linguistic context
1. Distribute the probabilities according to Eq. (13.1).
2. Factor the probabilities according to Eq. (13.2).
3. Perform a depth-first traversal of the LST and encode each leaf record,
(a) the depth of the most recently visited node that is not a direct ancestor,
(b) the probability,
(c) the word identity.
The factored probability of successor trees can be encoded as efficiently as the n-gram
model based on Algorithm 13.1, i.e., one n-gram record results in one constant-sized record.
Step 3 is illustrated in Figure 13.5(b), where the heavy line ends at the most recently visited
node that is not a direct ancestor. The encoding result is shown in Table 13.3.
Clearly the new data structure meets the requirements set forth, and, in fact, it only re-
quires additional log( )
n
bits per record (n is the depth of the tree). These bits encode the
common prefix length for each word. Naturally this requires some modification to the de-

Efficient Manipulation of Tree Lexicon
645
coding procedure. In particular the decoder must scan a portion of the n-gram successor list
in order to determine which tree nodes should be activated. Depending on the structure of
the tree (which is determined by the acoustic model, the lexicon, and language model) the
tree structure can be interpreted at runtime or cached for rapid access if memory is available.
Figure 13.5 (a) Factored tree; (b) tree with common prefix-length annotation.
Table 13.3 Encoded successor lexical tree; each record corresponds to one augmented factored
n-gram.
wj
Depth
F*(wj)
w1
0
0.4
w0
4
0.25
w3
2
0.75
w2
1
0.5
13.1.4.
Optimization of Lexical Trees
We now investigate ways to handle the huge search network formed by the multiple copies
of lexical trees in different linguistic contexts. The factorization of lexical trees actually
makes it easier to search. First, after the factorization of the language model, the intertree
transitions shown in Figure 13.2 no longer have the language model scores attached because
they are already applied completely before leaving the leaves. Moreover, as illustrated in
Figure 13.3, many transitions toward the end of a single-word path now have an associated
transition probability that is equal to 1. This observation implies that there could be many
duplicated subtrees in the network. Those duplicated subtrees can then be merged to save
both space and computation by eliminating redundant (unnecessary) state evaluation. Unlike
pruning, this saving is based on the dynamic programming principle, without introducing
any potential error.
(a)
(b)
1.0
.25
1.0
1.0
1.0
1.0
0.4
1.0
.75
.5
1.0
w1
0
w0
4
w2
1
w3
2

646
Large Vocabulary Search Algorithms
13.1.4.1.
Optimization of Finite State Network
One way to compress the lexical tree network is to use a similar algorithm for optimizing the
number of states in a deterministic finite state automaton. The optimization algorithm is
based on the indistinguishable property of states in a finite state automaton. Suppose that
1s
and
2s
are the initial states for automata
1T and
2T , then
1s and
2s
are said to be indistin-
guishable if the languages accepted by automata
1T and
2T are exactly the same. If we con-
sider our lexical tree network as a finite state automaton, the symbol emitted from the transi-
tion arc includes not only the phoneme identity, but also the factorized language model
probability.
The general set-partitioning algorithm [1] can be used for the reduction of finite state
automata. The algorithm starts with an initial partition of the automaton states and iteratively
refines the partition so that two states
1s and
2s
are put in the same block
iB if and only if
1
( )
f s
and
2
(
)
f s
are both in the same block
j
B . For our purpose,
1
( )
f s
and
2
(
)
f s
can be
defined as the destination state given a phone symbol (in the factored trees, the pair <phone,
LM-probability> can be used). Each time a block is partitioned, the smaller subblock is used
for further partitioning. The algorithm stops when all the states that transit to some state in a
particular block with arcs labeled with the same symbol are in the same block. When the
algorithm halts, each block of the resulting partition is composed of indistinguishable states,
and those states within each block can then be merged. The algorithm is guaranteed to find
the automaton with the minimum number of states. The algorithm has a time complexity of
(
log
)
O MN
N , where M is the maximum number of branching (fan-out) factors in the lexi-
cal tree and N is the number of states in the original tree network.
Although the above algorithm can give optimal finite state networks in terms of num-
ber of states, such an optimized network may be difficult to maintain, because the original
lexical tree structure could be destroyed and it may be troublesome to add any new word
into the tree network [1].
13.1.4.2.
Subtree Isomorphism
The finite state optimization algorithm described above does not take advantage of the tree
structure of the finite state network, though it generates a network with a minimum number
of states. Since our finite state network is a network of trees, the indistinguishablility prop-
erty is actually the same as the definition of subtree isomorphism. Two subtrees are said to
be isomorphic to each other if they can be made equivalent by permuting the successors. It
should be straightforward to prove that two states are indistinguishable, if and only if their
subtrees are isomorphic.
There are efficient algorithms [1] to detect whether two subtrees are isomorphic. For
all possible pairs of states u and v, if the subtrees starting at u and v,
( )
ST u
and
( )
ST v , are
isomorphic, v is merged into u and
( )
ST v
can be eliminated. Note that only internal nodes

Efficient Manipulation of Tree Lexicon
647
need to be considered for subtree isomorphism check. The time complexity for this algo-
rithm is
2
(
)
O N
[1].
13.1.4.3.
Sharing Tails
A linear tail in a lexical tree is defined as a subpath ending in a leaf and going through states
with a unique successor. It is often referred as a single-word subpath. It can be proved that
such a linear tail has unit probability attached to its arcs according to Eqs. (13.1) and (13.2).
This is because LM probability factorization pushes forward the LM probability attached to
the last arc of the linear tail, leaving arcs with unit probability. Since all the tails correspond-
ing to the same word w in different successor trees are linked to the root of successor tree
w
T ,3 the subtree starting from the first state of each linear tail is isomorphic to the subtree
starting from one of the states forming the longest linear tail of w. A simple algorithm to
take advantage of this share-tail topology can be employed to reduce the lexical tree net-
work.
Figure 13.6 and Figure 13.7 show a lexical tree network before and after shared-tail
optimization. For each word, only the longest linear tail is kept. All other tails can be re-
moved by linking them to an appropriate state in the longest tail, as shown in Figure 13.7.
a
c
d
e
a
b
a
a
b
b
c
d
d
e
c
Tu = {u, y}
Ty = {y, z}
Tz = {u}
u = /ab/
y = /acd/
z = /ace/
Figure 13.6 An example of a lexical tree network without shared-tail optimization [12]. The
vocabulary includes three words, u, v, and z. Tu, Tv, and Tz are the successor trees for u, v, and
z, respectively [13].
Shared-tail optimization is not global optimization, because it considers only some
special topology optimization. However, there are some advantages associated with shared-
tail optimization. First, in practice, duplicated linear tails account for most of the redundancy
in lexical tree networks [12]. Moreover, shared-tail optimization has a nice property of
maintaining the basic lexical tree structure for the optimized tree network.
3 We assume bigram is used in the discussion of "sharing tails."

648
Large Vocabulary Search Algorithms
u:
a
b
y:
z:
a
c
d
a
c
d
e
a
c
e
a
b
a
a
b
b
c
d
d
e
c
lexicon
tree
linear
transcriptions
successor
trees
Figure 13.7 The lexical tree network in Figure 13.6 after shared-tail optimization [12].
13.1.5.
Exploiting Subtree Polymorphism
The techniques of optimizing the network of successor lexical trees can only eliminate iden-
tical subtrees in the network. However, there are still many subtrees that have the same
nodes and topology but with different language model scores attached to the arcs. The
acoustic evaluation for those subtrees is unnecessarily duplicated. In this section we exploit
subtree dominance for additional saving.
A subtree instance is dominated when the best outcome in that subtree is not better
than the worst outcome in another instance of that subtree. The evaluation becomes redun-
dant for the dominated subtree instance. Subtree isomorphism and shared-tail are cases of
subtree dominance, but they require prearrangement of the lexical tree network as described
in the previous section.
If we need to implement lexical tree search dynamically, the network optimization al-
gorithms are not suitable. Although subtree dominance can be computed using minimax
search [35] during runtime, this requires that information regarding subtree isomorphism be
available for all corresponding pairs of states for each successor tree
w
T . Unfortunately it is
not practical in terms of either computation or space.
In place of computing strict subtree dominance, a polymorphic linguistic context as-
signment to reduce redundancy is employed by estimating subtree dominance based on local
information and ignoring the subgraph isomorphism problem. Polymorphic context assign-
ment involves keeping a single copy of the lexical tree and allowing each state to assume the
linguistic context of the most promising history. The advantage of this approach is that it
employs maximum sharing of data structures and information, so each node in the tree is
evaluated at most once. However, the use of local knowledge to determine the dominant
context could introduce significant errors because of premature pruning. Whisper [4] reports

Efficient Manipulation of Tree Lexicon
649
a 65.7% increase in error rate when only the dominant context is kept, based on local
knowledge.
ALGORITHM 13.2 HANDLING MULTIPLE LINGUISTIC CONTEXTS IN LEXICAL
TREE
1.
(
, )
( log
(
|
, ))
n
m
n
d
Cost s c
P s
s c
=
+ −
2. if
(
, )
m
InHeap s
c
then
(a) If
(
, )
m
d
Cost s
c
<
then
(i)
(
, )
m
Cost s
c
d
=
(ii)
(
, )
(
, )
m
n
StateInfo s
c
StateInfo s c
=
3. elseif d
ε
<
then
(a)
(
, );
(
, )
(
, )
m
m
n
Add s
c
StateInfo s
c
StateInfo s c
=
(b)
(
, )
m
Cost s
c
d
=
4. else
(a)
(
)
m
w
WorstContext s
=
(b) if
(
,
)
m
d
Cost s
w
<
then
(i)
(
,
)
m
Delete s
w
(ii)
(
, );
(
, )
(
, )
m
m
n
Add s
c
StateInfo s
c
StateInfo s c
=
(iii)
(
, )
m
Cost s
c
d
=
To recover the errors created by using local linguistic information to estimate subtree
dominance, you need to delay the decision regarding which linguistic context is most prom-
ising. This can be done by keeping a heap of contexts at each node in the tree. The heap
maintains all contexts (linguistic paths) whose probabilities are within a constant threshold
ε , comprised of the best global path plus the best path whose probability is less than ε but
better than the global pruning threshold β . The effect of the ε -heap is that more contexts
are retained for high-probability states in the lexical tree. The pseudocode fragment in
Algorithm 13.2 [3] illustrates a transition from state
ns
in context c to state
m
s . The termi-
nology used in Algorithm 13.2 is listed as follows:
 ( log
(
|
, ))
m
n
P s
s c
−
is the cost associated with applying acoustic model matching
and language model probability of state
m
s
given state
ns in context c.

(
, )
m
InHeap s
c
is true if context c is in the heap corresponding to state
m
s .

(
, )
m
Cost s
c is the score for context c in state
m
s .

(
, )
m
StateInfo s
c
is the auxiliary state information associated with context c in
state
m
s .

650
Large Vocabulary Search Algorithms

(
, )
m
Add s
c
adds context c to the state
m
s
heap.

(
, )
m
Delete s
c
deletes context c from state
m
s
heap.

(
)
m
WorstContext s
retrieves the worst context from the heap of state
m
s .
When higher-order n-gram is used for lexical tree search, the potential heap size for
lexical tree nodes (some also refer to prefix nodes) could be unmanageable. With decent
acoustic models and efficient pruning, as illustrated in Algorithm 13.2, the average heap size
for active nodes in the lexical tree is actually very modest. For example, Whisper’s average
heap size for active nodes in the 20,000-word WSJ lexical tree decoder is only about 1.6 [3].
13.1.6.
Context-Dependent Units and Inter-Word Triphones
So far, we have implicitly assumed that context-independent models are used in the lexical
tree search. When context-dependent phonetic or subphonetic models, as discussed in Chap-
ter 9, are used for better acoustic models, the construction and use of a lexical tree becomes
more complicated.
Since senones represent both subphonetic and context-dependent acoustic models, this
presents additional difficulty for use in lexical trees. Let's assume that a three-state context-
dependent HMM is formed from three senones, one for each state. Each senone is context-
dependent and can be shared by different allophones. If we use allophones as the units for
lexical tree, the sharing may be poor and fan-out unmanageable. Fortunately, each HMM is
uniquely identified by the sequence of senones used to form the HMM. In this way, different
context-dependent allophones that share the same senone sequence can be treated as the
same. This is especially important for lexical tree search, since it reduces the order of the
fan-out in the tree.
Interword triphones that require significant fan-ins for the first phone of a word and
fan-outs for the last phones usually present an implementation challenge for large -
vocabulary speech recognition. A common approach is to delay full interword modeling
until a subsequent rescoring phase.4 Given a sufficiently rich lattice or word graph, this is a
reasonable approach, because the static state space in the successive search has been reduced
significantly. However, as pointed out in Section 13.1.2, the size of the dynamic state space
can remain under control when detailed models are used to allow effective pruning. In addi-
tion, a multipass search requires an augmented set of acoustic models to effectively model
the biphone contexts used at word boundaries for the first pass. Therefore, it might be desir-
able to use genuine interword acoustic models in the single-pass search.
Instead of expanding all the fan-ins and fan-outs for inter-word context-dependent
phone units in the lexical tree, three metaunits are created.
1. The first metaunit, which has a known right context corresponding to the sec-
ond phone in the word, but uses open left context for the first phone of a
word (sometimes referred to as the word-initial unit). In this way, the fan-in
4 Multipass search strategy is described in Section 13.3.5.

Other Efficient Search Techniques
651
is represented as a subgraph shared by all words with the same initial left-
context-dependent phone.
2. Another metaunit, which has a known left context corresponding to the sec-
ond-to-last phone of the word, but uses open right context for the last phone
of a word (sometimes referred to as the word-final unit). Again, the fan-out is
represented as a subgraph shared by all words with the same final right-
context-dependent phone.
3. The third metaunit, which has both open left and right contexts, and is used
for single-phone word unit.
By using these metaunits we can keep the states for the lexical trees under control, because
the fan-in and fan-out are now represented as a single node.
During recognition, different left or right contexts within the same metaunit are han-
dled using Algorithm 13.2, where the different acoustic contexts are treated similarly as dif-
ferent linguistic contexts. The open left-context metaunit (fan-ins) can be dealt with in a
straightforward way using Algorithm 13.2, because the left context is always known (the last
phone of the previous word) when it is initiated. On the other hand, the open right-context
metaunit (fan-out) needs to explore all possible right contexts because the next word is not
known yet. To reduce unnecessary computation, fast match algorithms (described in 13.2.3)
can be used to provide both expected acoustic and language scores for different context-
dependent units to result in early pruning of unpromising contexts.
13.2.
OTHER EFFICIENT SEARCH TECHNIQUES
Tree structured lexicon represents an efficient framework of manipulation of search space.
In this section we present some additional implementation techniques, which can be used to
further improve the efficiency of search algorithms. Most of these techniques can be applied
to both Viterbi beam search and stack decoding. They are essential ingredients for a practi-
cal large-vocabulary continuous speech recognizer.
13.2.1.
Using Entire HMM as a State in Search
The state in state-search space based on HMM-trellis computation is by definition a Markov
state. Phonetic HMM models are the basic unit in most speech recognizers. Even though
subphonetic HMMs, like senones, might be used for such a system, the search is often based
on phonetic HMMs.
Treating the entire phonetic HMM as a state in state-search has many advantages. The
first obvious advantage is that the number of states the search program needs to deal with is
smaller. Note that using the entire phonetic HMM does not in effect reduce the number of
states in the search. The entire search space is unchanged. All the states within a phonetic
HMM are now bundled together. This means that all of them are either kept in the beam, if
the phonetic HMM is regarded as promising, or all of them are pruned away. For any given

652
Large Vocabulary Search Algorithms
time, the minimum cost among all the states within the phonetic HMM is used as the cost
for the phonetic HMM. For pruning purposes, this cost score is used to determine the prom-
ising degree of this phonetic HMM, i.e., the fate of all the states within this phonetic HMM.
Although this does not actually reduce the beam beyond normal pruning, it has the effect of
processing fewer candidates in the beam. In programming, this means less checking and
bookkeeping, so some computation savings can be expected.
You might wonder if this organization might be ineffective for beam search, since it
forces you to keep or prune all the states within a phonetic HMM. In theory it is possible
that only one or two states in the phonetic HMM need to be kept, while other states can be
pruned due to high cost score. However, this is in reality very rare, since a phone is a small
unit and all the states within a phonetic HMM should be relatively promising when the
search is near the acoustic region corresponding to the phone.
During the trellis computation, all the phonetic HMM states need to advance one time
step when processing one input vector. By performing HMM computation for all states to-
gether, the new organization can reduce memory accesses and improve cache locality, since
the output and transition probabilities are held in common by all states. Combining this or-
ganization strategy with lexical tree search further enhances the efficiency. In lexical tree
search, each hypothesis in the beam is associated with a particular node in the lexical tree.
These hypotheses are linked together in the heap structure described in Algorithm 13.2 for
the purposes of efficient evaluation and heuristic pruning. Since the node corresponds to a
phonetic HMM, the HMM evaluation is guaranteed to execute once for each hypothesis
sharing this node.
In summary, treating the entire phonetic HMM as a state in state-search space allows
you to explore the effective data structure for better sharing and improved memory locality.
13.2.2.
Different Layers of Beams
Because of the complexity of search, it often requires pruning of various levels of search to
make search feasible. Most systems thus employ different pruning thresholds to control what
states participate. The most frequently used thresholds are listed below:
 ττττs controls what states (either phone states or senone states) to retain. This is the
most fundamental beam threshold.
 ττττp controls whether the next phone is extended. Although this might not be nec-
essary for both stack decoding and linear Viterbi beam search, it is crucial for
lexical tree search, because pruning unpromising phonetic prefixes in the lexical
trees could improve search efficiency significantly.
 ττττh controls whether hypotheses are extended for the next word. Since the branch-
ing factor for word boundaries is very large, we need this threshold to limit
search to only the promising ones.
 ττττc controls where a linguistic context is created in a lexical tree search using
higher-order language models. This is also known as ε -heap in Algorithm 13.2.

Other Efficient Search Techniques
653
Pruning can introduce search errors if a state is pruned that would have been on the
globally best path. The principle applied here is that the more constraints you have available,
the more aggressively you decide whether this path will participate in the globally best path.
In this case, at the state level, you have the least constraints. At the phonetic level there are
more, and there are most at the word level. In general the number of word hypotheses tends
to drop significantly at word boundaries. Different thresholds for different levels allow the
search designer to fine-tune those thresholds for their tasks to achieve best search perform-
ance without significant increase in error rates.
13.2.3.
Fast Match
As described in Chapter 12, fast match is a crucial part of stack decoding, which mainly
reducew the number of possible word expansions for each path. Similarly, fast match can be
applied to the most expensive part—extending the phone HMM fan-outs within or between
lexical trees. Fast match is a method for rapidly deriving a list of candidates that constrain
successive search phases in which a computationally expensive detailed match is performed.
In this sense, fast match can be regarded as an additional pruning threshold to meet before a
new word/phone can be started.
Fast match is typically characterized by the approximations that are made in the acous-
tic/language models to reduce computation. The factorization of language model scores
among tree branches in lexical trees described in Section 13.1.3 can be viewed as fast match
using a language model. The factorized method is also an admissible estimate of the lan-
guage model scores for the future word. In this section we focus on acoustic model fast
match.
13.2.3.1.
Look-Ahead Strategy
Fast match, when applied in time-synchronous search, is also called look-ahead strategy.
since it basically searches ahead of the time-synchronous search by a few frames to deter-
mine which words or phones are likely to extend. Typically the look-ahead frames are fixed,
and the fast match is also done in time-synchronous fashion with another specialized beam
for efficient pruning. You can also use simplified models, like the one-state HMMs or con-
text-independent models [4, 32]. Some systems [21, 22] have tried to simplify the level of
details in the input feature vectors by aggregating information from several frames into one.
A straightforward way for compressing the feature stream is to skip every other frame of
speech for fast match. This allows a longer-range look-ahead, while keeping computation
under control. The approach of simplifying the input feature stream instead of simplifying
the acoustic models can reuse the fast match results for detailed match.
Whisper [4] uses phoneme look-ahead fast match in lexical tree search, in which prun-
ing is applied based on the estimation of the score of possible phone fan-outs that may fol-
low a given phone. A context-independent phone-net is searched synchronously with the
search process but offset N frames into the future. In practice, significant savings can be
obtained in search efforts without increase in error rates.

654
Large Vocabulary Search Algorithms
The performance of word and phoneme look-ahead clearly depends on the length of
the look-ahead frames. In general, the larger the look-ahead window, the longer is the com-
putation and the shorter the word/phone Λ list. Empirically, the window is a few tens of
milliseconds for phone look-ahead and a few hundreds of milliseconds for word look-ahead.
13.2.3.2.
The Rich-Get-Richer Strategy
For systems employing continuous-density HMMs, tens of mixtures of Gaussians are often
used for the output probability distribution for each state. The computation of the mixtures is
one of the bottlenecks when many context-dependent models are used. For example, Whis-
per uses about 120,000 Gaussians. In addition to using various beam pruning thresholds in
the search, there could be significant savings if we have a strategy to limit the number of
Gaussians to be computed.
The Rich-Get-Richer (RGR) strategy enables us to focus on most promising paths and
treat them with detailed acoustic evaluations and relaxed path-pruning thresholds. On the
contrary, the less promising paths are extended with less expensive acoustic evaluations and
less forgiving path-pruning thresholds. In this way, locally optimal candidates continue to
receive the maximum attention while less optimal candidates are retained but evaluated us-
ing less precise (computationally expensive) acoustic and/or linguistic models. The RGR
strategy gives us finer control in the creation of new paths that has potential to grow expo-
nentially.
RGR is used to control the level of acoustic details in the search. The goal is to reduce
the number of context-dependent senone probability (Gaussian) computations required. The
context-dependent senones associated with a phone instance p would be evaluated according
to the following condition:
[
]
[
]
[
]
{
}
[
]
( ) *
+
( )
threshold
where
( )
min cos ( ) |
_
( )
and
( ) = look-ahead estimate of
( )
s
Min ci p
LookAhead ci p
Min ci p
t s
s
ci
phone p
LookAhead ci p
ci p
α
<
=
∈
(13.6)
These conditions state that the context-dependent senones associated with p should be
evaluated if there exists a state s corresponding to p, whose cost score in linear combination
with a look-ahead cost score corresponding to p falls within a threshold. In the event that p
does not fall within the threshold, the senone scores corresponding to p are estimated using
the context-independent senones corresponding to p. This means the context-dependent
senones are evaluated only if the corresponding context-independent senones and the look-
ahead start showing promise. RGR strategy should save significant senone computation for
clearly unpromising paths. Whisper [26] reports that 80% of senone computation can be
avoided without introducing significant errors for a 20,000-word WSJ dictation task.

N-best and Multipass Search Strategies
655
13.3.
N-BEST AND MULTIPASS SEARCH STRATEGIES
Ideally, a search algorithm should consider all possible hypotheses based on a unified prob-
abilistic framework that integrates all knowledge sources (KSs).5 These KSs, such as acous-
tic models, language models, and lexical pronunciation models, can be integrated in an
HMM state search framework. It is desirable to use the most detailed models, such as con-
text-dependent models, interword context-dependent models, and high-order n-grams, in the
search as early as possible. When the explored search space becomes unmanageable, due to
the increasing size of vocabulary or highly sophisticated KSs, search might be infeasible to
implement.
As we develop more powerful techniques, the complexity of models tends to increase
dramatically. For example, language understanding models in Chapter 17 require long-
distance relationships. In addition, many of these techniques are not operating in a left-to-
right manner. A possible alternative is to perform a multipass search and apply several KSs
at different stages, in the proper order to constrain the search progressively. In the initial
pass, the most discriminant and computationally affordable KSs are used to reduce the num-
ber of hypotheses. In subsequent passes, progressively reduced sets of hypotheses are exam-
ined, and more powerful and expensive KSs are then used until the optimal solution is
found.
The early passes of multipass search can be considered fast match that eliminates
those unlikely hypotheses. Multipass search is, in general, not admissible because the opti-
mal word sequence could be wrongly pruned prematurely, due to the fact that not all KSs are
used in the earlier passes. However, for complicated tasks, the benefits of computation com-
plexity reduction usually outweigh the nonadmissibility. In practice, multipass search strat-
egy using progressive KSs could generate better results than a search algorithm forced to use
less powerful models due to computation and memory constraints.
The most straightforward multipass search strategy is the so-called n-best search para-
digm. The idea is to use affordable KSs to first produce a list of n most probable word se-
quences in a reasonable time. Then these n hypotheses are rescored using more detailed
models to obtain the most likely word sequence. The idea of the n-best list can be further
extended to create a more compact hypotheses representation—namely word lattice or
graph. A word lattice is a more efficient way to represent alternative hypotheses. N-best or
lattice search is used for many large-vocabulary continuous speech recognition systems [20,
30, 44].
In this section we describe the representation of the n-best list and word lattice. Sev-
eral algorithms to generate such an n-best-list or word lattice are discussed.
13.3.1.
N-Best Lists and Word Lattices
Table 13.4 shows an example n-best (10-best) list generated for a North American Business
(NAB) sentence. N-best search framework is effective only for n of the order of tens or hun-
5 In the field of artificial intelligence, the process of performing search through an integrated network of various
knowledge sources is called constraint satisfaction.

656
Large Vocabulary Search Algorithms
dreds. If the short n-best list that is generated by using less optimal models does not include
the correct word sequence, the successive rescoring phases have no chance to generate the
correct answer. Moreover, in a typical n-best list like the one shown in Table 13.4, many of
the different word sequences are just one-word variations of each other. This is not surpris-
ing, since similar word sequences should achieve similar scores. In general, the number of n-
best hypotheses might grow exponentially with the length of the utterance. Word lattices and
word graphs are thus introduced to replace n-best list with a more compact representation of
alternative hypotheses.
Table 13.4 An example 10-best list for a North American Business sentence.
1. I will tell you would I think in my office
2. I will tell you what I think in my office
3. I will tell you when I think in my office
4. I would sell you would I think in my office
5. I would sell you what I think in my office
6. I would sell you when I think in my office
7. I will tell you would I think in my office
8. I will tell you why I think in my office
9. I will tell you what I think on my office
10. I Wilson you I think on my office
Word lattices are composed by word hypotheses. Each word hypothesis is associated
with a score and an explicit time interval. Figure 13.8 shows an example of a word lattice
corresponding to the n-best list example in Table 13.4. It is clear that a word lattice is more
efficient representation. For example, suppose the spoken utterance contains 10 words and
there are 2 different word hypotheses for each word position. The n-best list would need to
have
10
2
1024
=
different sentences to include all the possible permutations, whereas the
word lattice requires only 20 different word hypotheses.
Figure 13.8 A word lattice example. Each word has an explicit time interval associated with it.
I
will
would
tell
sell
Wilson
you
what
when
why
would
I
think
in
my
office
I

N-best and Multipass Search Strategies
657
Word graphs, on the other hand, resemble finite state automata, in which arcs are la-
beled with words. Temporal constraints between words are implicitly embedded in the to-
pology. Figure 13.9 shows a word graph corresponding to the n-best list example in Figure
13.12. Word graphs in general have an explicit specification of word connections that don't
allow overlaps or gaps along the time axis. Nonetheless, word lattices and graphs are simi-
lar, and we often use these terms interchangeably.6 Since an n-best list can be treated as a
simple word lattice, word lattices are a more general representation of alternative hypothe-
ses. N-best lists or word lattices are generally evaluated on the following two parameters:
 Density: In the n-best case, it is measured by how many alternative word se-
quences are kept in the n-best list. In the word lattice case, it is measured by the
number of word hypotheses or word arcs per uttered word. Obviously, we want
the density to be as small as possible for successive rescoring modules, provided
the correct word sequence is included in the n-best list or word lattice.
 The lower bound word error rate: It is the lowest word error rate for any word
sequence in the n-best list or the word lattice.
Figure 13.9 A word graph example for the n-best list in Table 13.4. Temporal constraints are
implicit in the topology.
Rescoring with highly similar n-best alternatives duplicates computation on common
parts. The compact representation of word lattices allows both data structure and computa-
tion sharing of the common parts among similar alternative hypotheses, so it is generally
computationally less expensive to rescore the word lattice.
6 We use the term word lattice exclusively for both representations.
I
will
would
Wilson
tell
sell
you
when
why
what
would
I
think
in
my
office

658
Large Vocabulary Search Algorithms
Figure 13.10 illustrates the general n-best/lattice search framework. Those KSs pro-
viding most constraints, at a lesser cost, are used first to generate the n-best list or word lat-
tice. The n-best list or word lattice is then passed to the rescoring module, which uses the
remaining KSs to select the optimal path. You should note that the n-best and word-lattice
generator sometimes involve several phases of search mechanisms to generate the n-best list
or word lattice. Therefore, the whole search framework in Figure 13.10 could involve sev-
eral (> 2) phases of search mechanism.
Figure 13.10 N-best/lattice search framework. The most discriminant and inexpensive knowl-
edge sources (KSs 1) are used first to generate the n-best/lattice. The remaining knowledge
sources (KSs 2, usually expensive to apply) are used in the rescoring phase to pick up the op-
timal solution [40].
Does the compact n-best or word-lattice representation impose constraints on the
complexity of the acoustic and language models applied during successive rescoring mod-
ules? The word lattice can be expanded for higher-order language models and detailed con-
text-dependent models, like inter-word triphone models. For example, to use higher-order
language models for word lattice entails copying each word in the appropriate context of
preceding words (in the trigram case, the two immediately preceding words). To use inter-
word triphone models entails replacing the triphones for the beginning and ending phone of
each word with appropriate interword triphones. The expanded lattice can then be used with
detailed acoustic and language models. For example, Murveit et al. [30] report this can
achieve trigram search without exploring the enormous trigram search space.
13.3.2.
The Exact N-best Algorithm
Stack decoding is the choice of generating n-best candidates because of its best-first princi-
ple. We can keep it generating results until it finds n complete paths; these n complete sen-
tences form the n-best list. However, this algorithm usually cannot generate the n best can-
didates efficiently. The efficient n-best algorithm for time-synchronous Viterbi search was
first introduced by Schwartz and Chow [39]. It is a simple extension of time-synchronous
Viterbi search. The fundamental idea is to maintain separate records for paths with distinct
histories. The history is defined as the whole word sequence up to the current time t and
word w. This exact n-best algorithm is also called sentence-dependent n-best algorithm.
When two or more paths come to the same state at the same time, paths having the same
history are merged and their probabilities are summed together; otherwise only the n best
Results
Input
Speech
Word Lattice
N-Best list
N-Best or
Lattice Generator
Rescoring
KS Set 1
KS Set 2

N-best and Multipass Search Strategies
659
paths are retained for each state. As commonly used in speech recognition, a typical HMM
state has 2 or 3 predecessor states within the word HMM. Thus, for each time frame and
each state, the n-best search algorithm needs to compare and merge 2 or 3 sets of n paths
into n new paths. At the end of the search, the n paths in the final state of the trellis are sim-
ply re-ordered to obtain the n best word sequences.
This straightforward n-best algorithm can be proved to be admissible7 in normal cir-
cumstances [40]. The complexity of the algorithm is proportional to O(n), where n is the
number of paths kept at each state. This is often too slow for practical systems.
13.3.3.
Word-Dependent N-Best and Word-Lattice Algorithm
Since many of the different entries in the n-best list are just one-word variations of each
other, as shown in Table 13.4, one efficient algorithm can be derived from the normal 1-best
Viterbi algorithm to generate the n best hypotheses. The algorithm runs just like the normal
time-synchronous Viterbi algorithm for all within-word transitions. However for each time
frame t, and each word-ending state, the algorithm stores all the different words that can end
at current time t and their corresponding scores in a traceback list. At the same time, the
score of the best hypothesis at each grammar state is passed forward, as in the normal time-
synchronous Viterbi search. This obviously requires almost no extra computation above the
normal time-synchronous Viterbi search. At the end of search, you can simply search
through the stored traceback list to get all the permutations of word sequences with their
corresponding scores. If you use a simple threshold, the traceback can be implemented very
efficiently to only uncover the word sequences with accumulated cost scores below the
threshold. This algorithm is often referred as traceback-based n-best algorithm [29, 42] be-
cause of the use of the traceback list in the algorithm.
However, there is a serious problem associated with this algorithm. It could easily
miss some low-cost hypotheses. Figure 13.11 illustrates an example in which word
k
w
can
be preceded by two different words
iw and
j
w in different time frames. Assuming path
iw -
k
w
has a lower cost than path
j
w -
k
w when both paths meet during the trellis search of
k
w ,
the path
j
w -
k
w
will be pruned away. During traceback for finding the n best word se-
quences, there is only one best starting time for word
k
w , determined by the best boundary
between the best preceding word
iw and it. Even though path
j
w -
k
w might have a very low
cost (let’s say only marginally higher than that of
iw -
k
w ), it could be completely over-
looked, since the path has a different starting time for word
k
w .
7 Although one can show in the worst case, when paths with different histories have near identical scores for each
state, the search actually needs to keep all paths (> N) in order to guarantee absolute admissibility. Under this worst
case, the admissible algorithm is clearly exponential in the number of words for the utterance, since all permuta-
tions of word sequences for the whole sentence need to be kept.

660
Large Vocabulary Search Algorithms
time
best path
2nd best path
Can only keep one
path within a word so
this path is lost.
Ph3
Ph2
Ph1
Figure 13.11 Deficiency in traceback-based n-best algorithm. The best subpath,
j
w -
k
w , will
prune away subpath
iw -
k
w
while searching the word
k
w ; the second-best subpath cannot be
recovered [40].
time
best path
2nd best path with
Preceding word is
different so both
theories are kept.
Ph3
Ph2
Ph1
different ending word
Figure 13.12 Word-dependent n-best algorithm. Both subpaths
j
w -
k
w
and
iw -
k
w
are kept
under the word-dependent assumption [40].

N-best and Multipass Search Strategies
661
The word-dependent n-best algorithm [38] can alleviate the deficiency of the trace-
back-based n-best algorithm, in which only one starting time is kept for each word, so the
starting time is independent of the preceding words. On the other hand, in the sentence-
dependent n-best algorithm, the starting time for a word clearly depends on all the preceding
words, since different histories are kept separately. A good compromise is the so-called
word-dependent assumption: The starting time of a word depends only on the immediate
preceding word. That is, given a word pair and its ending time, the boundary between these
two words is independent of further predecessor words.
In the word-dependent assumption, the history to be considered for a different path is
no longer the entire word sequence; instead, it is only the immediately preceding word. This
allows you to keep k (<< n) different records for each state and each time frame in Viterbi
search. Differing slightly from the exact n-best algorithm, a traceback must be performed to
find the n-best list at the end of search. The algorithm is illustrated in Figure 13.12. A word-
dependent n-best algorithm has a time complexity proportional to k. However, it is no longer
admissible because of the word-dependent approximation. In general, this approximation is
quite reasonable if the preceding word is long. The loss it entails is insignificant [6].
13.3.3.1.
One-Pass N-Best and Word-Lattice Algorithm
As presented in Section 13.1, one-pass Viterbi beam search can be implemented very effi-
ciently using a tree lexicon. Section 13.1.2 states that multiple copies of lexical trees are
necessary for incorporating language models other than the unigram. When bigram is used
in lexical tree search, the successor lexical tree is predecessor-dependent. This predecessor-
dependent property immediately translates into the word-dependent property,8 as defined in
Section 13.3.3, because the starting time of a word clearly depends on the immediately pre-
ceding word. This means that different word-dependent partial paths are automatically saved
under the framework of predecessor-dependent successor trees. Therefore, one-pass prede-
cessor-dependent lexical tree search can be modified slightly to output n-best lists or word
graphs.
Ney el al. [31] used a word graph builder with a one-pass predecessor-dependent lexi-
cal tree search. The idea is to exploit the word-dependent property inherited from the prede-
cessor-dependent lexical tree search. During predecessor-dependent lexical tree search, two
additional quantities are saved whenever a word ending state is processed.
( ;
,
)
i
j
t w w
τ
—Representing the optimal word boundary between word
iw and
j
w , given word
j
w ending at time t.
(
; ( ;
,
), )
log (
|
)
t
j
i
j
j
h w
t w w
t
P
w
τ
τ
= −
x
—Representing the cumulative score that
word
j
w produces acoustic vector
1
,
,
t
τ
τ +
x x
x

.
8 When higher order n-gram models are used, the boundary dependence will be even more accurate. For example,
when trigrams are used, the boundary for a word juncture depends on the previous two words. Since we generally
want a fast method of generating word lattices/graphs, bigram is often used instead of higher order n-gram to gen-
erate word lattices/graphs.

662
Large Vocabulary Search Algorithms
At the end of the utterance, the word graph or n-best list is constructed by tracing back
all the permutations of word pairs recorded during the search. The algorithm is summarized
in Algorithm 13.3.
ALGORITHM 13.3 ONE-PASS PREDECESSOR-DEPENDENT LEXICAL TREE
SEARCH FOR N-BEST OR WORD-LATTICE CONSTRUCTION
For
1..
t
T
=
,
1-best predecessor-dependent lexical tree search;
(
,
) ending at
i
j
w w
t
∀
record word-dependent crossing time
( ;
,
)
i
j
t w w
τ
;
record cumulative word score
(
; ( ;
,
), )
j
i
j
h w
t w w
t
τ
;
Output 1-best result;
Construct n-best or word-lattice by tracing back the word-pair records (
and h
τ
).
13.3.4.
The Forward-Backward Search Algorithm
As described Chapter 12, the ability to predict how well the search fares in the future for the
remaining portion of the speech helps to reduce the search effort significantly. The one-pass
search strategy in general has very little chance of predicting the cost for the portion that it
has not seen. This difficulty can be alleviated by multipass search strategies. In successive
phases the search should be able to provide good estimates for the remaining paths, since the
entire utterance has been examined by the earlier passes. In this section we investigate a
special type of multipass search strategy—forward-backward search.
The idea is to first perform a forward search, during which partial forward scores α
for each state can be stored. Then perform a second pass search backward—that is, the sec-
ond pass starts by taking the final frame of speech and searches its way back until it reaches
the start of the speech. During the backward search, the partial forward scores α can be
used as an accurate estimate of the heuristic function or the fast match score for the remain-
ing path. Even though different KSs might be used in forward and backward phases, this
estimate is usually close to perfect, so the search effort for the backward phase can be sig-
nificantly reduced.
The forward search must be very fast and is generally a time-synchronous Viterbi
search. As in the multipass search strategy, simplified acoustic and language models are
often used in forward search. For backward search, either time-synchronous search or time-
asynchronous A* search can be employed to find the n best word sequences.
13.3.4.1.
Forward-Backward Search
Stack decoding, as described in Chapter 12, is based on the admissible A* search, so the first
complete hypothesis found with a cost below that of all the hypotheses in the stack is guar-

N-best and Multipass Search Strategies
663
anteed to be the best word sequence. It is straightforward to extend stack decoding to pro-
duce the n best hypotheses by continuing to extend the partial hypotheses according to the
same A* criterion until n different hypotheses are found. These n different hypotheses are
destined to be the n best hypotheses under a proof similar to that presented in Chapter 12.
Therefore, stack decoding is a natural choice for producing the n best hypotheses.
However, as described in Chapter 12, the difficulty of finding a good heuristic func-
tion that can accurately under-estimate the remaining path has limited the use of stack de-
coding. Fortunately, this difficulty can be alleviated by tree-trellis forward-backward search
algorithms [41]. First, the search performs a time-synchronous forward search. At each time
frame t, it records the score of the final state of each word ending. The set of words whose
final states are active (surviving in the beam) at time t is denoted as
t
∆. The score of the
final state of each word w in
t
∆is denoted as
( )
t w
α
, which represents the sum of the cost
of matching the utterance up to time t given the most likely word sequence ending with
word w and the cost of the language model score for that word sequence. At the end of the
forward search, the best cost is obtained and denoted as
T
α .
After the forward pass is completed, the second search is run in reverse (backward),
i.e., considering the last frame T as the beginning one and the first frame as the final one.
Both the acoustic models and language models need to be reversed. The backward search is
based on A* search. At each time frame t, the best path is removed from the stack and a list
of possible one-word extensions for that path is generated. Suppose this best path at time t is
j
w
ph , where
j
w is the first word of this partial path (the last expanded during backward A*
search). The exit score of path
j
w
ph
at time t, which now corresponds to the score of the
initial state of the word HMM
j
w , is denoted as
(
)
j
t
w
ph
β
.
Let us now assume we are concerned about the one-word extension of word
iw for
path
j
w
ph . Remember that there are two fundamental issues for the implementation of A*
search algorithm—(1) finding an effective and efficient heuristic function for estimating the
future remaining input feature stream and (2) finding the best crossing time between
iw and
j
w .
The stored forward score α can be used for solving both issues effectively and effi-
ciently. For each time t, the sum
(
)
(
)
j
t
i
t
w
w
ph
α
β
+
represents the cost score of the best
complete path including word
iw and partial path
j
w
ph .
(
)
t
iw
α
clearly represents a very
good heuristic estimate of the remaining path from the start of the utterance until the end of
the word
iw , because it is indeed the best score computed in the forward path for the same
quantity. Moreover, the optimal crossing time
*t
between
iw and
j
w can be easily com-
puted by the following equation:
*
arg min
(
)
(
)
j
t
i
t
w
t
t
w
ph
α
β


=
+


(13.7)

664
Large Vocabulary Search Algorithms
Finally, the new path
'
ph including the one-word (
iw ) extension is inserted into the stack,
ordered by the cost score
*
*
(
)
(
)
j
i
w
t
t
w
ph
α
β
+
. The heuristic function (forward scores α )
allows the backward A* search to concentrate search on extending only few truly promising
paths.
As a matter of fact, if the same acoustic and language models are used in both the for-
ward and backward search, this heuristic estimate (forward scores α ) is indeed a perfect
estimate of the best score the extended path will achieve. The first complete hypothesis gen-
erated by backward A* search coincides with the best one found in the time-synchronous
forward search and is truly the best hypothesis. Subsequent complete hypotheses correspond
sequentially to the n-best list, as they are generated in increasing order of cost. Under this
condition, the size of the stack in the backward A* search need only be N. Since the estimate
of future is exact, the (
1)
N +
th path in the stack has no chance to become part of the n-best
list. Therefore, the backward search is executed very efficiently to obtain the n best hypothe-
ses without exploring many unpromising branches. Of course, tree-trellis forward-backward
search can also be used like most other multipass search strategies—inexpensive KSs are
used in the forward search to get an estimate of α , and more expensive KSs are used in the
backward A* search to generate the n-best list.
The same idea of using forward score α can be applied to time-synchronous Viterbi
search in the backward search instead of backward A* search [7, 34]. For large-vocabulary
tasks, the backward search can run 2 to 3 orders of magnitude faster than a normal Viterbi
beam search. To obtain the n-best list from time-synchronous forward-backward search, the
backward search can also be implemented in a similar way as a time-synchronous word-
dependent n-best search.
13.3.4.2.
Word-Lattice Generation
The forward-backward n-best search algorithm can be easily modified to generate word lat-
tices instead of n-best lists. A forward time-synchronous Viterbi search is performed first to
compute
( )
t
α ω , the score of each word ω ending at time t. At the end of the search, this
best score
T
α
is also recorded to establish the global pruning threshold. Then, a backward
time-synchronous Viterbi search is performed to compute
( )
tβ ω , the score of each word ω
beginning at time t. To decide whether to include word juncture
i
j
ω
ω
−
in the word lat-
tice/graph at time t, we can check whether the forward-backward score is below a global
pruning threshold. Specifically, supposed bigram probability
(
|
)
j
i
P ω
ω
is used, if
( )
( )
log
(
|
)
T
t
t
j
i
P
α ω
β ω
ω
ω
α
θ


+
+ −
<
+


(13.8)
where θ is the pruning threshold, we will include
i
j
ω
ω
−
in the word lattice/graph at time
t. Once word juncture
i
j
ω
ω
−
is kept, the search continues looking for the next word-pair,
where the first word
i
ω will be the second word of the next word-pair.

N-best and Multipass Search Strategies
665
The above formulation is based on the assumption of using the same acoustic and lan-
guage models in both forward and backward search. If different KSs are used in forward and
backward search, the normalized α and β scores should be used instead.
13.3.5.
One-Pass vs. Multipass Search
There are several real-time one-pass search engines [4, 5]. Is it necessary to build a multi-
pass search engine based on n-best or word-lattice rescoring? We address this issue by dis-
cussing the disadvantages and advantages of multipass search strategies.
One criticism of multipass search strategies is that they are not suitable for real-time
applications. No matter how fast the first pass is, the successive (backward) passes cannot
start until users finish speaking. Thus, the search results need to be delayed for at least the
time required to execute the successive (backward) passes. This is why the successive passes
must be extremely fast in order to shorten the delay. Fortunately, it is possible to keep the
delays minimum (under one second) with clever implementation of multipass search algo-
rithms, as demonstrated by Nguyen et al. [18].
Another criticism for multipass search strategies is that each pass has the potential to
introduce inadmissible pruning, because decisions made in earlier passes are based on sim-
plified models (KSs). Search is a constraint-satisfaction problem. When a pruning decision
in each search pass is made on a subset of constraints (KSs), pruning error is inevitable and
is unrecoverable by successive passes. However, inadmissible pruning, like beam pruning
and fast match, is often necessary to implement one-pass search in order to cope with the
large active search space caused jointly by complex KSs and large-vocabulary tasks. Thus,
the problem of inadmissibility is actually shared by both real-time one-pass search and mul-
tipass search for different reasons. Fortunately, in both cases, search errors can be reduced to
minimum by clever implementation and by empirically designing all the pruning thresholds
carefully, as demonstrated in various one-pass and multipass systems [4, 5, 18].
Despite these concerns regarding multipass search strategies, they remain important
components in developing spoken language systems. We list here several important aspects:
1. It might be necessary to use multipass search strategies to incorporate very
expensive KSs. Higher-order n-gram, long-distance context-dependent mod-
els, and natural language parsing are examples that make the search space
unmanageable for one-pass search. Multipass search strategies might be
compelling even for some small-vocabulary tasks. For example, there are
only a couple of million legal credit card numbers for the authentication task
of 16-digit credit card numbers. However, it is very expensive to incorporate
all the legal numbers explicitly in the recognition grammar. To first reduce
search space down to an n-best list or word lattice/graph might be a desirable
approach.
2. Multipass search strategies could be very compelling for spoken language
understanding systems. It is problematic to incorporate most natural language
understanding technologies in one-pass search. On the other hand, n-best lists
or word lattices provide a trivial interface between speech recognition and

666
Large Vocabulary Search Algorithms
natural language understanding modules. Such an interface also provides a
convenient mechanism for integrating different KSs in a modular way. This
is important because the KSs could come from different modalities (like
video or pen) that make one-pass integration almost infeasible. This high de-
gree of modality allows different component subsystems to be optimized and
implemented independently.
3. N-best lists or word lattices are very powerful offline tools for developing
new algorithms for spoken language systems. It is often a significant task to
fully integrate new modeling techniques, such as segment models, into a one-
pass search. The complexity could sometimes slow down the progress of the
development of such techniques, since recognition experiments are difficult
to conduct. Rescoring of n-best list and lattice provides a quick and conven-
ient alternative for running recognition experiments. Moreover, the computa-
tion and storage complexity can be kept relatively constant for offline n-best
or word lattice/graph search strategies even when experimenting with highly
expensive new modeling techniques. New modeling techniques can be ex-
perimented with using n-best/word-graph framework first, being integrated
into the system only after significant improvement is demonstrated.
4. Besides being an alternative search strategy, n-best generation is also essen-
tial for discriminant training. Discriminant training techniques, like MMIE,
and MCE described in Chapter 4, often need to compute statistics of all pos-
sible rival hypotheses. For isolated word recognition using word models, it is
easy to enumerate all the word models as the rival hypotheses. However, for
continuous speech recognition, one needs to use an all-phone or all-word
model to generate all possible phone sequences or all possible word se-
quences during training. Obviously, that is too expensive. Instead, one can
use n-best search to find all the near-miss sentence hypotheses that we want
to discriminate against [15, 36].
13.4.
SEARCH-ALGORITHM EVALUATION
Throughout this chapter we are careful in following dynamic programming principles, using
admissible criteria as much as possible. However, many heuristics are still unavoidable to
implement large-vocabulary continuous speech recognition in practice. Those nonadmissible
heuristics include:
 Viterbi score instead of forward score described in Chapter 12.
 Beam pruning or stack pruning described in Section 13.2.2 and Chapter 12.
 Subtree dominance pruning described in Section 13.1.5.
 Fast match pruning described in Section 13.2.3.
 Rich-get-richer pruning described in Section 13.2.3.2.
 Multipass search strategies described in Section 13.3.5.

Case Study—Microsoft Whisper
667
Nonadmissible heuristics generate suboptimal searches where the found path is not
necessarily the path with the minimum cost. The question is, how different is this subopti-
mal from the true optimal path? Unfortunately, there is no way to know the optimal path
unless an exhaustive search is conducted. The practical question is whether the suboptimal
search hurts the search result. In a test condition where the true result is specified, you can
easily compare the search result with the true result to find whether any error occurs. Errors
could be due to inaccurate models (including acoustic and language models), suboptimal
search, or end-point detection. The error caused by a suboptimal search algorithm is referred
to as search error or pruning error.
How can we find out whether the search commits a pruning error? One of the proce-
dures most often used is straightforward. Let
ˆW be the recognized word sequence from the
recognizer and W
be the true word sequence. We need to compare the cost for these two
word sequences:
ˆ
ˆ
ˆ
- log
(
|
)
log
(
) (
|
)
P
P
P


∝−


W X
W
X W
(13.9)
- log
(
|
)
log
(
) (
|
)
P
P
P


∝−


W X
W
X W



(13.10)
The quantity in Eq. (13.9) is supposed to be minimum among all possible word sequences if
the search is admissible. Thus, if the quantity in Eq. (13.10) is greater than that in Eq. (13.9),
the error is not attributed to search pruning. On the other hand, if the quantity in Eq. (13.10)
is smaller than that in Eq. (13.9), there is a search error. The rationale behind the procedure
described here is obvious. In the case of search errors, the suboptimal search (or nonadmis-
sible pruning) has obviously pruned the correct path, because the cost of the correct path is
smaller than the one found by the recognizer. Although we can conclude that search errors
are found in this case, it does not guarantee that the search result is correct if the search can
be made optimal. The reason is simply that there might be one pruned path with an incorrect
word sequence and lower cost under the same suboptimal search. Therefore, the search er-
rors represent only the upper bound that one can improve on if an optimal search is carried
out. Nonetheless, finding search errors by comparing quantities in Eqs. (13.9) and (13.10) is
a good measure in different search algorithms.
During the development of a speech recognizer, it is a good idea to always include the
correct path in the search space. By including such a path, and some bookkeeping, one can
use the correct path to help in determining all the pruning thresholds. If the correct path is
pruned away during search by some threshold, some adjustment can be made to relax such a
threshold to retain the correct path. For example, one can adjust the pruning threshold for
fast match if a word in W fails to appear on the list supplied by the fast match.
13.5.
CASE STUDY—MICROSOFT WHISPER
We use the decoder of Microsoft’s Whisper [26, 27] discussed in Chapter 9 as a case study
for reviewing the search techniques we have presented in this chapter. Whisper can handle

668
Large Vocabulary Search Algorithms
both context-free grammars for small-vocabulary tasks and n-gram language models for
large-vocabulary tasks. We describe these two different cases.
13.5.1.
The CFG Search Architecture
Although context-free grammars (CFGs) have the disadvantage of being too restrictive and
unforgiving, particularly with novice users, they are still one of the most popular configura-
tions for building limited-domain applications because of following advantages:
 Compact representation results in a small memory footprint.
 Efficient operation during decoding in terms of both space and time.
 Ease of grammar creation and modification for new tasks.
As mentioned in Chapter 12, the CFG grammar consists of a set of productions or
rules that expand nonterminals into a sequence of terminals and nonterminals. Nonterminals
in the grammar tend to refer to high-level task-specific concepts such as dates, font names,
and commands. The terminals are words in the vocabulary. A grammar also has a nontermi-
nal designated as its start state. Whisper also allows some regular expression operators on
the right-hand side of the production for notational convenience. These operators are: or ‘|’;
repeat zero or more times ‘*’; repeat one or more times ‘+’; and optional ([ ]). The following
is a simple CFG example for binary number:
%start BINARY_NUMBER
BINARY_NUMBER: (zero | one)*
Without losing generality, Whisper disallows the left recursion for ease of implemen-
tation [2]. The grammar is compiled into a binary linked list format. The binary format cur-
rently has a direct one-to-one correspondence with the text grammar components, but is
more compact. The compiled format is used by the search engine during decoding. The bi-
nary representation consists of variable-sized nodes linked together. The grammar format
achieves sharing of subgrammars through the use of shared nonterminal definition rules.
The CFG search is conducted according to RTN framework (see Chapter 12). During
decoding, the search engine pursues several paths through the CFG at the same time. Asso-
ciated with each of the paths is a grammar state that describes completely how the path can
be extended further. When the decoder hypothesizes the end of the current word of a path, it
asks the grammar module to extend the path further by one word. There may be several al-
ternative successor words for the given path. The decoder considers all the successor word
possibilities. This may cause the path to be extended to generate several more paths to be
considered, each with its own grammar state. Also note that the same word might be under
consideration by the decoder in the context of different paths and grammar states at the same
time.
The decoder uses beam search to prune unpromising paths with three different beam
thresholds. The state pruning threshold
sτ
and new phone pruning threshold
p
τ
work as
described in Section 13.2.2. When extending a path, if the score of the extended path does

Case Study—Microsoft Whisper
669
not exceed the threshold
h
τ , the path to be extended is put into a pool. At each frame, for
each word in the vocabulary, a winning path that extends to that word is picked from the
pool, based on the score. All the remaining paths in the pool are pruned. This level of prun-
ing gives us finer control in the creation of new paths that have potential to grow exponen-
tially.
When two paths representing different word sequences thus far reach the end of the
current word with the same grammar state at the same time, only the better path of the two is
allowed to continue on. This optimization is safe, except that it does not take into account
the effect of different interword left acoustic contexts on the scores of the new word that is
started.
Besides beam pruning, the RGR strategy, described in Section 13.2.3.2, is used to
avoid unnecessary senone computation. The basic idea is to use the linear combination of
context-independent senone score and context-independent look-ahead score to determine
whether the context-dependent senone evaluation is worthwhile to pursue.
All of these pruning techniques enable Whisper to perform typical 100- to 200-word
CFG tasks in real time running on a 486 PC with 2 MB RAM. Readers might think it is not
critical to make CFG search efficient on such a low-end platform.9 However, it is indeed
important to keep the CFG engine fast and lean. The speech recognition engine is eventually
only part of an integrated application. The application will benefit if the resources (both
CPU and memory) used by the speech decoder are kept as small as possible, so there are
more resources left for the application module to use. Moreover, in recognition server appli-
cations, several channels of speech recognition can be performed on a single server platform
if each speech recognition engine takes only a small portion of the total resources.
13.5.2.
The N-Gram Search Architecture
The CFG decoder is best suited for limited domain command and control applications. For
dictation or natural conversational systems, a stochastic grammar such as n-grams provides a
more natural choice. Using bigrams or trigrams leads to a large number of states to be con-
sidered by the search process, requiring an alternative search architecture.
Whisper’s n-gram search architecture is based on lexical tree search as described in
Section 13.1. To keep the runtime memory10 as small as possible, Whisper does not to allo-
cate the entire lexical tree network statically. Instead it dynamically builds only the portion
that needs to be active. To cope with the problem of delayed application of language model
scores, Whisper uses the factorization algorithm described in Section 13.1.3 to distribute the
language model probabilities through the tree branches. To reduce the memory overhead of
the factored language model probabilities, an efficient data structure is used for representing
the lexical tree as described in Section 13.1.3.1. This data structure allows Whisper to en-
code factored language model probabilities in no more than the space required for the origi-
9 Thanks to the progress predicted by Moore’s law, the current mainstream PC configuration is an order of magni-
tude more powerful than the configuration we list here (486 PC with 2 MB RAM) in both speed and memory.
10 Here the runtime memory means the virtual memory for the decoder that is the entire image of the decoder.

670
Large Vocabulary Search Algorithms
nal n-gram probabilities. Thus, there is absolutely no storage overhead for using factored
lexical trees.
Table 13.5 Configuration of the first seven levels of the 20,000-word WSJ (Wall Street Jour-
nal) tree;.the large initial fan-out is due to the use of context-dependent acoustic models [4].
Tree Level
Number of Nodes
Fan-Out
1
655
655.0
2
3174
4.85
3
9388
2.96
4
13,703
1.46
5
14,918
1.09
6
13,907
0.93
7
11,389
0.82
The basic acoustic subword model in Whisper is a context-dependent senone. It also
incorporates inter-word triphone models in the lexical tree search as described in Section
13.1.6. Table 13.5 shows the distribution of phoneme arcs for 20,000-word WSJ lexical tree
using senones as acoustic models. Context-dependent units certainly prohibit more prefix
sharing when compared with Table 13.1. However, the overall arcs in the lexical tree still
represent quite a saving when compared with a linear lexicon with about 140,000 phoneme
arcs. Most importantly, similar to the case in Table 13.1, most sharing is realized in the be-
ginning prefixes where most computation resides. Moreover, with the help of context-
dependent and interword senone models, the search is able to use more reliable knowledge
to perform efficient pruning. Therefore, lexical tree with context-dependent models can still
enjoy all the benefits associated with lexical tree search.
The search organization is evaluated on the 1992 development test set for the Wall
Street Journal corpus with a back-off trigram language model. The trigram language model
has on the order of 107 linguistic equivalent classes, but the number of classes generated is
far fewer due to the constraints provided by the acoustic model. Figure 13.13(a) illustrates
that the relative effort devoted to the trigram, bigram, and unigram is constant regardless of
total search effort, across a set of test utterances. This is because the ratio of states in the
language model is constant. The language model is using ~2 ×106 trigrams, ~2 × 106 bi-
grams, and 6 × 104 unigrams. Figure 13.13(b) illustrates different relative order when word
hypotheses are considered. The most common context for word hypotheses is the unigram
context, followed by the bigram and trigram contexts. The reason for the reversal from the
state-level transitions is the partially overlapping evaluations required by each bigram con-
text. The trigram context is more common than the bigram context for utterances that gener-
ate few hypotheses overall. This is likely because the language model models those utter-
ances well.

Case Study—Microsoft Whisper
671
(a)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
utt id.
%effort
0
100
200
300
400
500
states/frame
(b)
0
0.5
utt id.
%hypothesis
0
5
10
tot. hyp./frame
Hyps
Tg
Bg
Ug
Figure 13.13 (a) Search effort for different linguistic contexts measured by number of active
states in each of the three different linguistic contexts. The top series is for the bigram, then the
unigram and trigram. The remaining series is the effort per utterance and is plotted on the sec-
ondary y-axis. (b) The distribution of word hypothesis with respect to their context. The top
line is the unigram context, then the bigram and trigram. The remaining series is the average
number of hypotheses per frame for each utterance and is plotted on the secondary y-axis [3].
Table 13.6 Effect of heap threshold on contexts/node, states/frame-of-speech (fos), word error
rate, and search time [4].
ε
Context / node
states / fos
%error
search time
0
1.000
8805
16.4
1.0x
1.0
1.001
8808
15.5
1.0x
2.0
1.008
8898
14.4
1.0x
3.0
1.018
9252
12.4
1.07x
4.0
1.056
10224
10.5
1.16x
5.0
1.147
11832
10.3
1.36x
6.0
1.315
13749
10.0
1.60x
7.0
1.528
15342
9.9
1.81x
8.0
1.647
15984
9.9
1.86x
To improve efficiency in dealing with tree copies due to the use of higher-order n-
gram, one needs to reduce redundant computations in subtrees that are not explicitly part of
the given linguistic context. One solution is to use successor trees to include only nonzero
successors, as described in Section 13.1.2. Since Whisper builds the search space dynami-
cally, it is not effective for Whisper to use the optimization techniques of the successor-tree
network, such as FSN optimization, subtree isomorphism, and sharing tail optimization.
Instead, Whisper uses polymorphic linguistic context assignment to reduce redundancy, as
described in Section 13.1.5. This involves keeping a single copy of the lexical tree, so that
each node in the tree is evaluated at most once. To avoid early inadmissible pruning of dif-
ferent linguistic contexts, an ε -heap of storing paths of different linguistic contexts is cre-

672
Large Vocabulary Search Algorithms
ated for each node in the tree. The operation of such ε -heaps is in accordance with
Algorithm 13.2. The depth of each heap varies dynamically according to a changing thresh-
old that allows more contexts to be retained for promising nodes.
Table 13.6 illustrates how the depth of the ε -heap, the active states per frame of
speech, word error rate, and search time change when the value of threshold ε increases for
the 20,000-word WSJ dictation task. As we can see from the table, the average heap size for
active nodes is only about 1.6 for the most accurate configuration. Figure 13.14(a) illustrates
the distribution of stack depths for a large data set, showing that the stack depth is small
even for tree initial nodes. Figure 13.14(b) illustrates the profile of the average stack depth
for a sample utterance, showing that the average stack depth remains small across an utter-
ance.
(a)
1
2
3
4
5
6
7
prefix length
prefix count
(b)
0
400
800
1200
1600
2000
100
200
300
400
time
prefix count
0
0.5
1
1.5
2
2.5
stack depth
Figure 13.14 (a) A cumulative graph of the prefix count for each stack depth, starting with
depth 1, showing the distribution according to prefix length. (b) The prefix count and the aver-
age stack depth with respect to one utterance. The vertical bars show the word boundaries [3].
Whisper also employs look-ahead techniques to further reduce the search effort. The
acoustic look-ahead technique described in Section 13.2.3.1 attempts to estimate the
probability that a phonetic HMM will participate in the final result [3]. Whisper implements
acoustic look-ahead by running a CI phone-net synchronously with the search process but
offset N frames in the future. One side effect of the acoustic look-ahead is to provide infor-
mation for the RGR strategy, as described in Section 13.2.3.2, so the search can avoid un-
necessary Gaussian computation. Figure 13.15 demonstrates the effectiveness of varying the
frame look-ahead from 0 to N frames in terms of states evaluated.
When the look-ahead is increased from 0 to 3 frames, the search effort, in terms of real
time, is reduced by ~40% with no loss in accuracy; however most of that is due to reducing
the number of states evaluated per frame. There is no effect on the number of Gaussians
evaluated per frame (the system using continuous density) until we begin to negatively im-
pact error rate, indicating that the acoustic space represented by the pruned states is redun-
dant and adequately covered by the retained states prior to the introduction of search errors.
With the techniques discussed here, Whisper is able to achieved real-time performance
for the continuous WSJ dictation task (60,000-word) on Pentium-class PCs. The recognition
accuracy is identical to that of a standard Viterbi beam decoder with a linear lexicon.

Historical Perspectives and Further Reading
673
0
500
1000
1500
2000
2500
0
2
4
6
8
10
look-ahead frame count
gaussians -- states / frame
0.0
1.0
2.0
3.0
4.0
5.0
6.0
%error -- x real time
gaussians
states
error rate
x real time
Figure 13.15 Search effort, percent error rate, and real-time factor as a function of the acoustic
look-ahead. Note that search effort is the number of Gaussians evaluated per frame and the
number of states evaluated per frame [3].
13.6.
HISTORICAL PERSPECTIVES AND FURTHER READING
Large-vocabulary continuous speech recognition is a computationally intensive task. Real-
time systems started to emerge in the late 1980s. Before that, most systems achieved real-
time performance with the help of special hardware [11, 16, 25, 28]. Thanks to Moore’s law
and various efficient search techniques, real-time systems became a reality on a single-chip
general-purpose personal computer in 1990s [4, 34, 43].
Common wisdom in 1980’s saw stack decoding as more efficient for large-vocabulary
continuous speech recognition with higher-order n-grams. Time-synchronous Viterbi beam
search, as described in Sections 13.1 and 13.2, emerged as the most efficient search frame-
work. It has become the most widely used search technique today. The lexical tree represen-
tation was first used by IBM as part of its allophonic fast match system [10]. Ney proposed
the use of the lexical tree as the primary representation for the search space [32]. The ideas
of language model factoring [4, 19] [5] and subtree polymorphism [4] enabled real-time
single-pass search with higher-order language models (bigrams and trigrams). Alleva [3]
and Ney [33] are two excellent articles regarding the detailed Viterbi beam search algorithm
with lexical tree representation.
As mentioned in Chapter 12, fast match was first invented to speed up stack decoding
[8, 9]. Ney et al. and Alleva et al. extended the fast match idea to phone look-ahead in time-
synchronous search by using context-independent model evaluation. In Haeb-Umbach et al.
[22] a word look-ahead is implemented for a 12.3k-word speaker-dependent continuous
speech recognition task. The look-ahead is performed on a lexical tree, with beam search
executed every other frame. The results show a factor of 3–5 of reduction for search space

674
Large Vocabulary Search Algorithms
compared to the standard Viterbi beam search, while only 1—2% extra errors are introduced
by word look-ahead.
The idea of multipass search strategy has long existed for knowledge-based speech
recognition systems [17], where first a phone recognizer is performed, then a lexicon hy-
pothesizer is used to locate all the possible words to form a word lattice, and finally a lan-
guage model is used to search for the most possible word sequence. However, HMM’s
popularity predominantly shifted the focus to the unified search approach to achieve global
optimization. Computation concerns led many researchers to revisit the multipass search
strategy. The first n-best algorithm, described in Section 13.3.2, was published by research-
ers at BBN [39]. Since then, n-best and word-lattice based multipass search strategies have
become important search frameworks for rapid system deployment, research tools, and spo-
ken language understanding systems. Schwartz et al.’s paper [40] is a good tutorial on the n-
best or word-lattice generation algorithms. Most of the n-best search algorithms can be made
to generate word lattices/graphs with minor modifications. Other excellent discussions of
multipass search can be found in [14, 24, 30].
REFERENCES
[1]
Aho, A., J. Hopcroft, and J. Ullman, The Design and Analysis of Computer Algorithms,
1974, Addison-Wesley Publishing Company.
[2]
Aho, A.V., R. Sethi, and J.D. Ullman, Compilers : Principles, Techniques, and Tools, 1985,
Addison-Wesley.
[3]
Alleva, F., "Search Organization in the Whisper Continuous Speech Recognition System,"
IEEE Workshop on Automatic Speech Recognition, 1997.
[4]
Alleva, F., X. Huang, and M.Y. Hwang, "Improvements on the Pronunciation Prefix Tree
Search Organization," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Process-
ing, 1996, Atlanta, Georgia pp. 133-136.
[5]
Aubert, X., et al., "Large Vocabulary Continuous Speech Recognition of Wall Street Journal
Corpus," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1994,
Adelaide, Australia pp. 129-132.
[6]
Aubert, X. and H. Ney, "Large Vocabulary Continuous Speech Recognition Using Word
Graphs," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1995,
Detroit, MI pp. 49-52.
[7]
Austin, S., R. Schwartz, and P. Placeway, "The Forward-Backward Search Algorithm for
Real-Time Speech Recognition," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Sig-
nal Processing, 1991, Toronto, Canada pp. 697-700.
[8]
Bahl, L.R., et al., "Obtaining Candidate Words by Polling in a Large Vocabulary Speech
Recognition System," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Process-
ing, 1988 pp. 489-492.
[9]
Bahl, L.R., et al., "Matrix Fast Match: a Fast Method for Identifying a Short List of Candi-
date Words for Decoding," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal
Processing, 1989, Glasgow, Scotland pp. 345-347.
[10]
Bahl, L.R., P.S. Gopalakrishnan, and R.L. Mercer, "Search Issues in Large Vocabulary
Speech Recognition," Proc. of the 1993 IEEE Workshop on Automatic Speech Recognition,
1993, Snowbird, UT.
[11]
Bisiani, R., T. Anantharaman, and L. Butcher, "BEAM: An Accelerator for Speech Recogni-
tion," Int. Conf. on Acoustics, Speech and Signal Processing, 1989 pp. 782-784.

Historical Perspectives and Further Reading
675
[12]
Brugnara, F. and M. Cettolo, "Improvements in Tree-Based Language Model Representa-
tion," Proc. of the European Conf. on Speech Communication and Technology, 1995, Ma-
drid, Spain pp. 1797-1800.
[13]
Cettolo, M., R. Gretter, and R.D. Mori, "Knowledge Integration" in Spoken Dialogues with
Computers, R.D. Mori, Editor 1998, London, pp. 231-256, Academic Press.
[14]
Cettolo, M., R. Gretter, and R.D. Mori, "Search and Generation of Word Hypotheses" in
Spoken Dialogues with Computers, R.D. Mori, Editor 1998, London, pp. 257-310, Academic
Press.
[15]
Chou, W., C.H. Lee, and B.H. Juang, "Minimum Error Rate Training Based on N-best String
Models," IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1993, Minneapolis,
MN pp. 652-655.
[16]
Chow, Y.L., et al., "BYBLOS: The BBN Continuous Speech Recognition System," Proc. of
the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1987 pp. 89-92.
[17]
Cole, R.A., et al., "Feature-Based Speaker Independent Recognition of English Letters," Int.
Conf. on Acoustics, Speech and Signal Processing, 1983 pp. 731-734.
[18]
Davenport, J.C., R. Schwartz, and L. Nguyen, "Towards A Robust Real-Time Decoder,"
IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1999, Phoenix, Arizona pp.
645-648.
[19]
Federico, M., et al., "Language Modeling for Efficient Beam-Search," Computer Speech and
Language, 1995, pp. 353-379.
[20]
Gauvain, J.L., L. Lamel, and M. Adda-Decker, "Developments in Continuous Speech Dicta-
tion using the ARPA WSJ Task," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Sig-
nal Processing, 1995, Detroit, MI pp. 65-68.
[21]
Gillick, L.S. and R. Roth, "A Rapid Match Algorithm for Continuous Speech Recognition,"
Proc. of the Speech and Natural Language Workshop, 1990, Hidden Valley, PA pp. 170-
172.
[22]
Haeb-Umbach, R. and H. Ney, "A Look-Ahead Search Technique for Large Vocabulary
Continuous Speech Recognition," Proc. of the European Conf. on Speech Communication
and Technology, 1991, Genova, Italy pp. 495-498.
[23]
Haeb-Umbach, R. and H. Ney, "Improvements In Time-Synchronous Beam-Search For
10000-Word Continuous Speech Recognition," IEEE Trans. on Speech and Audio Process-
ing, 1994, 2(4), pp. 353-365.
[24]
Hetherington, I.L., et al., "A* Word Network Search for Continuous Speech Recognition,"
Proc. of the European Conf. on Speech Communication and Technology, 1993, Berlin, Ger-
many pp. 1533-1536.
[25]
Hon, H.W., A Survey of Hardware Architectures Designed for Speech Recognition, 1991,
Carnegie Mellon University, Pittsburgh, PA.
[26]
Huang, X., et al., "From Sphinx II to Whisper: Making Speech Recognition Usable" in
Automatic Speech and Speaker Recognition, C.H. Lee, F.K. Soong, and K.K. Paliwal, eds.
1996, Norwell, MA, pp. 481-508, Kluwer Academic Publishers.
[27]
Huang, X., et al., "Microsoft Windows Highly Intelligent Speech Recognizer: Whisper,"
IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1995 pp. 93-96.
[28]
Jelinek, F., "The Development of an Experimental Discrete Dictation Recognizer," Proc. of
the IEEE, 1985, 73(1), pp. 1616-1624.
[29]
Marino, J. and E. Monte, "Generation of Multiple Hypothesis in Connected Phonetic-Unit
Recognition by a Modified One-Stage Dynamic Programming Algorithm," Proc. of Eu-
roSpeech, 1989, Paris pp. 408-411.

676
Large Vocabulary Search Algorithms
[30]
Murveit, H., et al., "Large Vocabulary Dictation Using SRI's DECIPHER Speech Recogni-
tion System: Progressive Search Techniques," Proc. of the IEEE Int. Conf. on Acoustics,
Speech and Signal Processing, 1993, Minneapolis, MN pp. 319-322.
[31]
Ney, H. and X. Aubert, "A Word Graph Algorithm for Large Vocabulary," Proc. of the Int.
Conf. on Spoken Language Processing, 1994, Yokohama, Japan pp. 1355-1358.
[32]
Ney, H., et al., "Improvements in Beam Search for 10000-Word Continuous Speech Recog-
nition," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1992, San
Francisco, California pp. 9-12.
[33]
Ney, H. and S. Ortmanns, Dynamic Programming Search for Continuous Speech Recogni-
tion, in IEEE Signal Processing Magazine, 1999. pp. 64-83.
[34]
Nguyen, L., et al., "Search Algorithms for Software-Only Real-Time Recognition with Very
Large Vocabularies," Proc. of ARPA Human Language Technology Workshop, 1993, Plains-
boro, NJ pp. 91-95.
[35]
Nilsson, N.J., Problem-Solving Methods in Artificial Intelligence, 1971, New York,
McGraw-Hill.
[36]
Normandin, Y., "Maximum Mutual Information Estimation of Hidden Markov Models" in
Automatic Speech and Speaker Recognition, C.H. Lee, F.K. Soong, and K.K. Paliwal, eds.
1996, Norwell, MA, Kluwer Academic Publishers.
[37]
Odell, J.J., et al., "A One Pass Decoder Design for Large Vocabulary Recognition," Proc. of
the ARPA Human Language Technology Workshop, 1994, Plainsboro, New Jersey pp. 380-
385.
[38]
Schwartz, R. and S. Austin, "A Comparison of Several Approximate Algorithms for Finding
Multiple (N-BEST) Sentence Hypotheses," Proc. of the IEEE Int. Conf. on Acoustics,
Speech and Signal Processing, 1991, Toronto, Canada pp. 701-704.
[39]
Schwartz, R. and Y.L. Chow, "The N-Best Algorithm: an Efficient and Exact Procedure for
Finding the N Most Likely Sentence Hypotheses," Proc. of the IEEE Int. Conf. on Acoustics,
Speech and Signal Processing, 1990, Albuquerque, New Mexico pp. 81-84.
[40]
Schwartz, R., L. Nguyen, and J. Makhoul, "Multiple-Pass Search Strategies" in Automatic
Speech and Speaker Recognition, C.H. Lee, F.K. Soong, and K.K. Paliwal, eds. 1996, Nor-
well, MA, pp. 57-81, Klewer Academic Publishers.
[41]
Soong, F.K. and E.F. Huang, "A Tree-Trellis Based Fast Search for Finding the N Best Sen-
tence Hypotheses in Continuous Speech Recognition," Proc. of the IEEE Int. Conf. on
Acoustics, Speech and Signal Processing, 1991, Toronto, Canada pp. 705-708.
[42]
Steinbiss, V., "Sentence-Hypotheses Generation in a Continuous Speech Recognition Rec-
ognition," Proc. of EuroSpeech, 1989, Paris pp. 51-54.
[43]
Steinbiss, V., et al., "The Philips Research System for Large-Vocabulary Continuous-Speech
Recognition," Proc. of the European Conf. on Speech Communication and Technology,
1993, Berlin, Germany pp. 2125-2128.
[44]
Woodland, P.C., et al., "Large Vocabulary Continuous Speech Recognition Using HTK,"
Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1994, Adelaide,
Australia pp. 125-128.

679
C H A P T E R
1 4
Text and Phonetic Analysis
Text-to-speech can be viewed as a speech
coding system that yields an extremely high compression ratio coupled with a high degree of
flexibility in choosing style, voice, rate, pitch range, and other playback effects. In this view
of TTS, the text file that is input to a speech synthesizer is a form of coded speech. Thus,
TTS subsumes coding technologies discussed in Chapter 7 with following goals:
 Compression ratios superior to digitized wave files—Compression yields bene-
fits in many areas, including fast Internet transmission of spoken messages.
 Flexibility in output characteristics—Flexibility includes easy change of gender,
average pitch, pitch range, etc., enabling application developers to give their
systems’ spoken output a unique individual personality. Flexibility also implies
easy change of message content; it is generally easier to retype text than it is to
record and deploy a digitized speech file.
 Ability for perfect indexing between text and speech forms—Preservation of the
correspondence between textual representation and the speech wave form allows

680
Text and Phonetic Analysis
synchronization with other media and output modes, such as word-by-word re-
verse video highlighting in a literacy tutor reading aloud.
 Alternative access of text content—TTS is the most effective alternative access
of text for the blind, hands-free/eyes-free and displayless scenarios.
At first sight, the process of converting text into speech looks straightforward. How-
ever, when we analyze how complicated speakers read a text aloud, this simplistic view
quickly falls apart. First, we need to convert words in written forms into speakable forms.
This process is clearly nontrivial. Second, to sound natural, the system needs to convey the
intonation of the sentences properly. This second process is clearly an extremely challenging
one. One good analogy is to think how difficult to drop foreign accent when speaking a sec-
ond language—a process still not quite understood by human beings.
The ultimate goal of simulating the speech of an understanding, effective human
speaker from plain text is as distant today as the corresponding Holy Grail goals of the fields
of speech recognition and machine translation. This is because such humanlike rendition
depends on common-sense reasoning about the world and the text’s relation to it, deep
knowledge of the language itself in all its richness and variability, and even knowledge of
the actual or expected audience—its goals, assumptions, presuppositions, and so on. In typi-
cal audio books or recordings for the visually challenged today, the human reader has
enough familiarity with and understanding of the text to make appropriate choices for rendi-
tion of emotion, emphasis, and pacing, as well as handling both dialog and exposition.
While computational power is steadily increasing, there remains a substantial knowledge
gap that must be closed before fully human-sounding simulated voices and renditions can be
created.
While no TTS system to date has approached optimal quality in the Turing test,1 a
large number of experimental and commercial systems have yielded fascinating insights.
Even the relatively limited-quality TTS systems of today have found practical applications.
The basic TTS system architecture is illustrated in Chapter 1. In the present chapter we
discuss text analysis and phonetic analysis whose objective is to convert words into speak-
able phonetic representation. The techniques discussed here are relevant to what we dis-
cussed for language modeling in Chapter 11 (like text normalization before computing n-
gram) and for pronunciation modeling in Chapter 9. The next two modules—prosodic analy-
sis and speech synthesis—are treated in the next two chapters.
14.1.
MODULES AND DATA FLOW
The text analysis component, guided by presenter controls, is typically responsible for de-
termining document structure, conversion of nonorthographic symbols, and parsing of lan-
guage structure and meaning. The phonetic analysis component converts orthographic words
to phones (unambiguous speech sound symbols). Some TTS systems assume dependency
between text analysis, phonetic analysis, prosodic analysis, and speech synthesis, particu-
1 1 A test proposed by British mathematician Allan Turing of the ability of a computer to flawlessly imitate human
performance on a given speech or language task [[29]].

Modules and Data Flow
681
larly systems based on very large databases containing long stretches of original, unmodified
digitized speech with their original pitch contours. We discuss our high-level linguistic de-
scription of those modules, based on modularity, transparency, and reusability of compo-
nents, although some aspects of text and phonetic analysis may be unnecessary for some
particular systems.
We assume that the entire text (word, sentence, paragraph, document) to be spoken is
contained in a single, wholly visible buffer. Some systems may be faced with special re-
quirements for continuous flow-through or visibility of only small (word, phrase, sentence)
chunks at a time, or extremely complex timing and synchronization requirements. The basic
functional processes within the text and phonetic analysis are shown schematically in Figure
14.1.
Figure 14.1 Modularized functional blocks for text and phonetic analysis components.
Phonetic Analysis
raw text
or tagged text
tagged text & phones
tagged text
Text Analysis
Document Structure Detection
Lexicon
Text Normalization
Linguistic Analysis
Homograph Disambiguation
Morphological Analysis
Letter-to-Sound Conversion

682
Text and Phonetic Analysis
The architecture in Figure 14.1 brings the standard benefits of modularity and trans-
parency. Modularity in this case means that the analysis at each level can be supplied by the
most expert knowledge source, or a variety of different sources, as long as the markup con-
ventions for expressing the analysis are uniform. Transparency means that the results of
each stage could be reused by other processes for other purposes.
14.1.1.
Modules
The text analysis module (TAM) is responsible for indicating all knowledge about the text or
message that is not specifically phonetic or prosodic in nature. Very simple systems do little
more than convert nonorthographic items, such as numbers, into words. More ambitious
systems attempt to analyze whitespaces and punctuations to determine document structure,
and perform sophisticated syntax and semantic analysis on sentences to determine attributes
that help the phonetic analysis to generate correct phonetic representation and prosodic gen-
eration to construct superior pitch contours. As shown in Figure 14.1, text analysis for TTS
involves three related processes:
 Document structure detection—Document structure is important to provide a
context for all later processes. In addition, some elements of document structure,
such as sentence breaking and paragraph segmentation, may have direct implica-
tions for prosody.
 Text normalization—Text normalization is the conversion from the variety sym-
bols, numbers, and other nonorthographic entities of text into a common ortho-
graphic transcription suitable for subsequent phonetic conversion.
 Linguistic analysis—Linguistic analysis recovers the syntactic constituency and
semantic features of words, phrases, clauses, and sentences, which is important
for both pronunciation and prosodic choices in the successive processes.
The task of the phonetic analysis is to convert lexical orthographic symbols to phone-
mic representation along with possible diacritic information, such as stress placement. Pho-
netic analysis is thus often referred to grapheme-to-phoneme conversion. The purpose is
obvious, since phonemes are the basic units of sound, as described in Chapter 2. Even
though future TTS systems might be based on word sounding units with increasing storage
technologies, homograph disambiguation and phonetic analysis for new words (either true
new words being invented over time or morphologically transformed words) are still neces-
sary for systems to correctly utter every word.
Grapheme-to-phoneme conversion is trivial for languages where there is a simple rela-
tionship between orthography and phonology. Such a simple relationship can be well cap-
tured by a handful of rules. Languages such as Spanish and Finnish belong to this category
and are referred to as phonetic languages. English, on the other hand, is remote from pho-
netic language because English words often have many distinct origins. It is generally be-
lieved that the following three services are necessary to produce accurate pronunciations.

Modules and Data Flow
683
 Homograph disambiguation—It is important to disambiguate words with differ-
ent senses to determine proper phonetic pronunciations, such as object (/ah b jh
eh k t/) as a verb or as a noun (/aa b jh eh k t/).
 Morphological analysis—Analyzing the component morphemes provides im-
portant cues to attain the pronunciations for inflectional and derivational words.
 Letter-to-sound conversion—The last stage of the phonetic analysis generally
includes general letter-to-sound rules (or modules) and a dictionary lookup to
produce accurate pronunciations for any arbitrary word.
All the processes in text and phonetic analysis phases above need not to be determinis-
tic, although most TTS systems today have deterministic processes. What we mean by not
deterministic is that each of the above processes can generate multiple hypotheses with the
hope that the later process can disambiguate those hypotheses by using more knowledge.
For example, sometimes it might not be trivial to decide whether the punctuation “.” is a
sentence ending mark or abbreviation mark during document structure detection. The docu-
ment structure detection process can pass both hypotheses to the later processes, and the
decision can then be delayed until there is enough information to make an informed decision
in later modules, such as the text normalization or linguistic analysis phases. When generat-
ing multiple hypotheses, the process can also assign probabilistic information if it compre-
hends the underlying probabilistic structure. This flexible pipeline architecture avoids the
mistakes made by early processes based on insufficient knowledge.
Much of the work done by the text/phonetic analysis phase of a TTS system mir-
rors the processing attempted by natural language process (NLP) systems for other pur-
poses, such as automatic proofreading, machine translation, database document indexing,
and so on. Increasingly sophisticated NL analysis is needed to make certain TTS processing
decisions in the examples illustrated in Table 14.1. Ultimately all decisions are context
driven and probabilistic in nature, since, for example, dogs might be cooked and eaten in
some cultures.
Table 14.1 Examples of several ambiguous text normalization cases.
Examples
Alternatives
Techniques
Dr. Smith
doctor or drive?
abbreviation analysis, case analysis
Will you go?
yes-no or wh-question?
syntactic analysis
I ate a hot dog.
accent on dog?
semantic, verb/direct object likelihood
I saw a hot dog.
accent on dog?
discourse, pragmatic analysis
Most TTS systems today employ specialized natural language processing modules for
front-end analysis. In the future, it is likely that less emphasis will be placed on construction
of TTS-specific text/phonetic analysis components such as those described in [27], while
more resources will likely go to general-purpose NLP systems with cross-functional poten-
tial [23]. In other words, all the modules above only perform simple processing and pass all
possible hypotheses to the later modules. At the end of the text/phonetic phase, a unified
NLP module then performs extensive syntactic/semantic analysis for the best decisions. The

684
Text and Phonetic Analysis
necessity for such an architectural approach is already visible in markets where language
issues have forced early attention to common lexical and tokenization resources, such as
Japan. Japanese system services and applications can usually expect to rely on common
cross-functional linguistic resources, and many benefits are reaped, including elimination of
bulk, reduction of redundancy and development time, and enforcement of systemwide con-
sistent behavior. For example, under Japanese architectures, TTS, recognition, sorting, word
processing, database, and other systems are expected to share a common language and
dictionary service.
14.1.2.
Data Flows
It is arguable that text input alone does not give system enough information to express and
render the intention of the text producer. Thus, more and more TTS systems focus on pro-
viding an infrastructure of standard set of markups (tags), so that the text producer can better
express their semantic intention with these markups in addition to plain text. These kinds of
markups have different levels of granularity, ranging from simple speed settings specified in
words per minute up to elaborate schemes for semantic representation of concepts that may
bypass the ordinary text analysis module altogether.2 The markup can be done by internal
proprietary conventions or by some standard markup, such as XML (Extensible Markup
Language [35]). Some of these markup capabilities will be discussed in Sections 14.3 and
14.4.
For example, an application may know a lot about the structure and content of the text
to be spoken, and it can apply this knowledge to the text, using common markup conven-
tions, to greatly improve spoken output quality. On the other hand, some applications may
have certain broad requirements such as rate, pitch, callback types, etc. For engines provid-
ing such supports, the text and/or phonetic analysis phase can be skipped, in whole or in
part. Whether the application or the system has provided the text analysis markup, the struc-
tural conventions should be identical and must be sufficient to guide the phonetic analysis.
The phonetic analysis module should be presented only with markup tags indicating struc-
ture or functions of textual chunks, and words in standard orthography. The similar phonetic
markups could also be presented to the phonetic analysis module, the module could be
skipped.
Internal architectures, data structures, and interfaces may vary widely from system to
system. However, most modern TTS systems initially construct a simple description of an
utterance or paragraph based on observable attributes, typically text words and punctuation,
perhaps augmented by control annotations. This minimal initial skeleton is then augmented
with many layers of structure hypothesized by the TTS system’s internal analysis modules.
Beginning with a surface stream of words, punctuation, and other symbols, typical layers of
detected structure that may be added include:
 Phonemes
2 This latter type of system is sometimes called concept-to-speech or message-to-speech, which is described in
Chapter 17. It generally generates better speech rendering when domain-specific knowledge is provided to the
system.

Modules and Data Flow
685
 Syllables
 Morphemes
 Words derived from nonwords (such as dates like “9/10/99”)
 Syntactic constituents
 Relative importance of words and phrases
 Prosodic phrasing
 Accentuation
 Duration controls
 Pitch controls
We can now consider how the information needed to support synthesis of a sentence is
developed in processing an example sentence such as: “A skilled electrician reported.”
Figure 14.2 Annotation tiers indicating incremental analysis based on an input (text) sentence
“A skilled electrician reported.” Flow of incremental annotation is indicated by arrows on the
left side.
In Figure 14.2, the information that must be inferred from text is diagrammed. The
flow proceeds as follows:
 W(ords) 


 Σ, C(ontrols): the syllabic structure (Σ) and the basic phonemic
form of a word are derived from lexical lookup and/or the application of rules.
The Σ tier shows the syllable divisions (written in text form for convenience).
The C tier, at this stage, shows the basic phonemic symbols for each word’s syl-
lables.
F2
S [ f1, f2, …, fn ]
S
NP [ f1, f2, …, fn ]
VP [ f1, f2, …, fn ]
W4
W
W1
W2
W3
e
por
ted
Σ
A
skilled
lec
tri
cian
re
r
iy
p
ao
r
t
ax
d
C
Ih
l
eh
k
t
r
ih
sh
ax
n
s
k
ih
l
d
F1
P
F3
F5
F4
IP1 [f1, f2, …, fn]
IP2 [f1, f2, … , fn]
U [f1, f2, …, fn]
ax

686
Text and Phonetic Analysis
 W(ords) 


 S(yntax/semantics): The word stream from text is used to infer a
syntactic and possibly semantic structure (S tier) for an input sentence. Syntactic
and semantic structure above the word would include syntactic constituents such
as Noun Phrase (NP), Verb Phrase (VP), etc. and any semantic features that can
be recovered from the current sentence or analysis of other contexts that may be
available (such as an entire paragraph or document). The lower-level phrases
such as NP and VP may be grouped into broader constituents such as Sentence
(S), depending on the parsing architecture.
 S(yntax/semantics) 


 P(rosody): The P(rosodic) tier is also called the sym-
bolic prosodic module. If a word is semantically important in a sentence, that
importance can be reflected in speech with a little extra phonetic prominence,
called an accent. Some synthesizers begin building a prosodic structure by plac-
ing metrical foot boundaries to the left of every accented syllable. The resulting
metrical foot structure is shown as F1, F2, etc. in Figure 14.2 (some feet lack an
accented head and are ‘degenerate’). Over the metrical foot structure, higher-
order prosodic constituents, with their own characteristic relative pitch ranges,
boundary pitch movements, etc. can be constructed, shown in the figure as into-
national phrases IP1, IP2. The details of prosodic analysis, including the mean-
ing of those symbols, are described in Chapter 15.
The final phonetic form of the words to be spoken will reflect not only the original
phonetics, but decisions made in the S and P tiers as well. For example, the P(rosody) tier
adds detailed pitch and duration controls to the C(ontrol) specification that is passed to the
voice synthesis component. Obviously, there can be a huge variety of particular architec-
tures and components involved in the conversion process. Most systems, however, have
some analog to each of the components presented above.
14.1.3.
Localization Issues
A major issue in the text and phonetic analysis components of a TTS system is internation-
alization and localization. While most of the language processing technologies in this book
are exemplified by English case studies, an internationalized TTS architecture enabling
minimal expense in localization is highly desirable. From a technological point of view, the
text conventions and writing systems of language communities may differ substantially in
arbitrary ways, necessitating serious effort in both specifying an internationalized architec-
ture for text and phonetic analysis, and localizing that architecture for any particular lan-
guage.
For example, in Japanese and Chinese, the unit of word is not clearly identified by
spaces in text. In French, interword dependencies in pronunciation realization exist (liaison).
Conventions for writing numerical forms of dates, times, money, etc. may differ across lan-
guages. In French, number groups separated by spaces may need to be integrated as single
amounts, which rarely occurs in English. Some of these issues may be more serious for cer-
tain types of TTS architectures than others. In general, it is best to specify a rule architecture
for text processing and phonetic analysis based on some fundamental formalism that allows

Lexicon
687
for language-particular data tables, and which is powerful enough to handle a wide range of
relations and alternatives.
14.2.
LEXICON
The most important resource for text and phonetic analysis is the TTS system lexicon (also
referred to as a dictionary). As illustrated in Figure 14.1, the TTS system lexicon is shared
with almost all components. The lexical service should provide the following kinds of con-
tent in order to support a TTS system:
 Inflected forms of lexicon entries
 Phonetic pronunciations (support multiple pronunciations), stress and syllabic
structure features for each lexicon entry
 Morphological analysis capability
 Abbreviation and acronym expansion and pronunciation
 Attributes indicating word status, including proper-name tagging, and other spe-
cial properties
 List of speakable names of all common single characters. Under modern operat-
ing systems, the characters should include all Unicode characters.
 Word part-of-speech (POS) and other syntactic/semantic attributes
 Other special features, e.g., how likely a word is to be accented, etc.
It should be clear that the requirements for a TTS system lexical service overlap heavily
with those for more general-purpose NLP.
Traditionally, TTS systems have been rule oriented, in particular for grapheme-to-
phoneme conversion. Often, tens of so called letter-to-sound (LTS) rules (described in detail
in Section 14.8) are used first for grapheme-to-phoneme conversion, and the role of the lexi-
con has been minimized as an exception list, whose pronunciations cannot be predicted on
the basis of such LTS rules. However, this view of the lexicon’s role has increasingly been
adjusted as the requirement of a sophisticated NLP analysis for high-quality TTS systems
has become apparent. There are a number of ways to optimize a dictionary system. For a
good overview of lexical organization issues, please see [4].
To expose different contents about a lexicon entry listed above for different TTS mod-
ule, it calls for a consistent mechanism. It can be done either through a database query or a
function call in which the caller sends a key (usually the orthographic representation of a
word) and the desired attribute. For example, a TTS module can use the following function
call to look up a particular attribute (like phonetic pronunciations or POS) by passing the
attribute att and the result will be stored in the pointer val upon successful lookup. More-
over, when the lookup is successful (the word is found in the dictionary) the function returns
true, otherwise it will return false instead.
BOOLEAN
DictLookup (string word, ATTTYPE att, (VOID *) val)

688
Text and Phonetic Analysis
We should also point out that this functional view of dictionary could further expand
the physical dictionary as a service. The morphological analysis and letter-to-sound modules
(described in Sections 14.7 and 14.8) can all be incorporated into the same lexical service.
That is, underneath dictionary lookup, operation and analysis is encapsulated from users to
form a uniform service.
Another consideration in the system’s runtime dictionary is compression. While many
standard compression algorithms exist, and should be judiciously applied, the organization
and extent of the vocabulary itself can also be optimized for small space and quick search.
The kinds of American English vocabulary relevant to a TTS system include:
 Grammatical function words (closed class)—about several hundred)
 Very common vocabulary—about 5,000 or more
 College-level core vocabulary base forms—about 60,000 or more
 College-level core vocabulary inflected form—about 120,000 or more
 Scientific and technical vocabulary, by field—e.g., legal, medical, engineering,
etc.
 Personal names—e.g., family, given, male, female, national origin, etc.
 Place names—e.g., countries, cities, rivers, mountains, planets, stars, etc.
 Slang
 Archaisms
The typical sizes of reasonably complete lists of the above types of vocabulary run
from a few hundred function or closed-class words (such as prepositions and pronouns) to
120,000 or so inflected forms of college-level vocabulary items, up to several million sur-
names and place names. Careful analysis of the likely needs of typical target applications
can potentially reduce the size of the runtime dictionary. In general, most TTS systems
maintain a system dictionary with a size between 5000 and 200,000 entries. With advanced
technologies in database and hashing, search is typically a nonissue for dictionary lookup. In
addition, since new forms are constantly produced by various creative processes, such as
acronyms, borrowing, slang acceptance, compounding, and morphological manipulation,
some means of analyzing words that have not been stored must be provided. This is the topic
of Sections 14.7 and 14.8.
14.3.
DOCUMENT STRUCTURE DETECTION
For the purpose of discussion, we assume that all input to the TAM is an XML document,
though perhaps largely unmarked, and the output is also a (more extensively marked) XML
document. That is to say, all the knowledge recovered during the TAM phase is to be ex-
pressed as XML markup. This confirms the independence of the TAM from phonetic and
prosodic considerations, allowing a variety of resources, some perhaps not crafted with TTS
in mind, to be brought to bear by the TAM on the text. It also implies that that output of the
TAM is potentially usable by other, non-TTS processes, such as normalization of language-

Document Structure Detection
689
model training data for building statistical language models (see Chapter 11). This fully
modular and transparent view of TTS allows the greatest flexibility in document analysis,
provides for direct authoring of structure and other customization, while allowing a split
between expensive, multipurpose natural language analysis and the core TTS functionality.
Although other text format or markup language, such as Adobe Acrobat or Microsoft Word,
can be used for the same purpose, the choice of XML is obvious because it is the widely
open standard, particularly for the Internet.
Figure 14.3 A documentcentric view of TTS.
XML is a set of conventions for indicating the semantics and scope of various entities
that combine to constitute a document. It is conceptually somewhat similar to Hypertext
Markup Language (HTML), which is the exchange code for the World Wide Web. In these
markup systems, properties are identified by tags with explicit scope, such as “<b>make
this phrase bold</b>” to indicate a heavy, dark print display. XML in particular
attempts to enforce a principled separation between document structure and content, on one
hand, and the detailed formatting or presentation requirements of various uses of documents,
on the other. Since we cannot provide a tutorial on XML here, we freely introduce example
tags that indicate document and linguistic structure. The interpretations of these are intuitive
to most readers, though, of course, the analytic knowledge underlying decisions to insert
Content
Authoring
Automatic Language
Generation
Automatic Structure
Detection/creation
XML
Document
Sound Docu-
ment (TTS)
LM Training
Doc (ASR)
Database Doc
(IR)
Print Doc,
Screen Doc,
Groupware
Doc, etc.
Aural Stylesheet
LM stylesheet
DB stylesheet
Other
Stylesheets…
Automatic
Parsing
TTS engine
Document
Structure
Natural
Language
Structure

690
Text and Phonetic Analysis
tags may be very sophisticated. It will be some time before commercial TTS engines come
to a common understanding on the wide variety of text attributes that should be marked, and
accept a common set of conventions. Nevertheless, it is reasonable to adopt the idea that
TAM should be independent and reusable, thus allowing XML documents (which are ex-
pected to proliferate) to function for speech just as for other modalities, as indicated
schematically in Figure 14.3.
TTS is regarded in Figure 14.3as a factored process, with the text analysis perhaps car-
ried out by human editors or by natural language analysis systems. The role of the TTS en-
gine per se may eventually be reduced to the interpretation of structural tags and provision
of phonetic information. While commercial engines of the present day are not structured
with these assumptions in mind, modularity and transparency are likely to become increas-
ingly important. The increasing acceptance of the basic ideas underlying an XML docu-
mentcentric approach to text and phonetic analysis for TTS can be seen in the recent prolif-
eration of XML-like speech markup proposals [24, 33]. While not presenting any of these in
detail, in the discussion below we adopt informal conventions that reflect and extend their
basic assumptions. The structural markup exploited by the TTS systems of the future may be
imposed by XML authoring systems at document creation time, or may be inserted by inde-
pendent analytical procedures. In any case the distinction between purely automatic struc-
ture creation/detection and human annotation and authoring will increasingly blur—just as
in natural language translation and information retrieval domains, the distinction between
machine-produced results and human-produced results has begun to blur.
14.3.1.
Chapter and Section Headers
Section headers are a standard convention in XML document markup, and TTS systems can
use the structural indications to control prosody and to regulate prosodic style, just as a pro-
fessional reader might treat chapter headings differently. Increasingly, a document created
on computer or intended for any kind of electronic circulation incorporates structural
markup, and the TTS and audio human-computer-interface systems of the future learn to
exploit this (in longer documents, the document structure markup assists in audio naviga-
tion, speedup, and skipping). For example, the XML annotation of a book at a high level
might follow conventions as shown in Figure 14.4. Viewing a document in this way might
lead a TTS system to insert pauses and emphasis correctly, in accordance with the structure
marked. Furthermore, an audio interface system would work jointly with a TTS system to
allow easy navigation and orientation within such a structure. If future documents are
marked up in this fashion, the concept of audio books, for example, would change to rely
less on unstructured prerecorded speech and more on smart, XML-aware, high-quality audio
navigation and TTS systems, with the output customization flexibility they provide.
For documents without explicit markup information for section and chapter headers, it
is in general a nontrivial task to detect them automatically. Therefore, most TTS systems
today do not make such an attempt.

Document Structure Detection
691
<Book>
<Title>The Pity of War</Title>
<Subtitle>Explaining World War I</Subtitle>
<Author>Niall Ferguson</Author>
<TableOfContents>…</TableOfContents>
<Introduction>
<Para>…</Para>
…
</Introduction>
<Chapter>
<ChapterTitle>The Myths of Militarism</ChapterTitle>
<Section>
<SectionTitle>Prophets</SectionTitle>
<Para> … </Para>
…
</Section>
</Chapter>
…
</Book>
Figure 14.4 An example of the XML annotation of a book.
14.3.2.
Lists
Lists or bulleted items may be rendered with distinct intonational contours to indicate au-
rally their special status. This kind of structure might be indicated in XML as shown in
Figure 14.5. Again, TTS engine designers need to get used to the idea of accepting such
markup for interpretation, or incorporating technologies that can detect and insert such
markup as needed by the downstream phonetic processing modules. Similar to chapter and
section headers, most TTS systems today do not make an attempt to detect list structures
automatically.
<UL>
<LI>compression</LI>
<LI>flexibility</LI>
<LI>text-waveform correspondence</LI>
</UL>
<Caption>The advantages of TTS</Caption>
Figure 14.5 An example of a list marked by XML.

692
Text and Phonetic Analysis
14.3.3.
Paragraphs
The paragraph has been shown to have direct and distinctive implications for pitch assign-
ment in TTS [26]. The pitch range of good readers or speakers in the first few clauses at the
start of a new paragraph is typically substantially higher than that for mid-paragraph sen-
tences, and it narrows further in the final few clauses, before resetting for the next para-
graph. Thus, to mimic a high-quality reading style in future TTS systems, the paragraph
structure has to be detected from XML tagging or inferred from inspection of raw format-
ting. Obviously, relying on independently motivated XML tagging is, as always, the supe-
rior option, especially since this is a very common structural annotation in XML documents.
In contrast to other document structure information, paragraphs are probably among
the easiest to detect automatically. The character <CR> (carriage return) or <NL> (new line)
is usually a reliable clue for paragraphs.
14.3.4.
Sentences
While sentence breaks are not normally indicated in XML markup today, there is no reason
to exclude them, and knowledge of the sentence unit can be crucial for high-quality TTS. In
fact, some XML-like conventions for text markup of documents to be rendered by synthe-
sizers (e.g., SABLE) provide for a DIV (division) tag that could take paragraph, sentence,
clause, etc. as attribute [24]. If we define sentence broadly as a primal linguistic unit that
makes up paragraphs, attributes could be added to a Sent tag to express whatever linguistic
knowledge exists about the type of the sentence as a whole:
<Sent type=”yes-no question”>
Is life so dear, or peace so sweet, as to be purchased at the price of chains and slavery?
</Sent>
Again, as emphasized throughout this section, such annotation could be either applied
during creation of the XML documents (of the future) or inserted by independent processes.
Such structure-detection processes may be motivated by a variety of needs and may exist
outside the TTS system per se.
If no independent markup of sentence structure is available from an external, inde-
pendently motivated document analysis or natural language system, a TTS system typically
relies on simple internal heuristics to guess at sentence divisions. In e-mail and other rela-
tively informal written communications, sentence boundaries may be very hard to detect. In
contrast to English, sentence breaking could be trivial for some other written languages. In
Chinese, there is a designated symbol (a small circle ) for marking the end of a sentence, so
the sentence breaking could be done in a totally straightforward way. However, for most
Asian languages, such as Chinese, Japanese, and Thai, there is in general no space within a
sentence. Thus, tokenization is an important issue for Asian languages.
In more formal English writing, sentence boundaries are often signaled by terminal
punctuation from the set: {.!?} followed by whitespaces and an upper-case initial word.
Sometimes additional punctuation may trail the ‘?’ and ‘!’ characters, such as close

Document Structure Detection
693
quotation marks and/or close parenthesis. The character ‘.’ is particularly troubling, because
it is, in programming terms, heavily overloaded. Apart from its uses in numerical
expressions and internet addresses, its other main use is as a marker of abbreviation, itself a
difficult problem for text normalization (see Section 14.4). Consider this pathological
jumble of potentially ambiguous cases:
Mr. Smith came by. He knows that it costs $1.99, but I don’t know when he’ll
be
back
(he
didn’t
ask,
“when
should
I
return?”)…
His
website
is
www.mrsmithhhhhh.com. The car is 72.5 in. long (we don’t know which park-
ing space he’ll put his car in.) but he said “…and the truth shall set you free," an
interesting quote.
Some of these can be resolved in the linguistic analysis module. However for some
cases, only probabilistic guesses can be made, and even a human reader may have difficulty.
The ambiguous sentence breaking can also be resolved in an abbreviation-processing mod-
ule (described in Section 14.4.1). Any period punctuation that is not taken to signal an ab-
breviation and is not part of a number can be taken as end-of-sentence. Of course, as we
have seen above, abbreviations are also confusable with words that can naturally end sen-
tences, e.g., “in.” For the measure abbreviations, an examination of the left context (check-
ing for numeric) may be sufficient. In any case, the complexity of sentence breaking illus-
trates the value of passing multiple hypotheses and letting later, more knowledgeable mod-
ules (such as an abbreviation or linguistic analysis module) make decisions. Algorithm 14.1
shows a simple sentence-breaking algorithm that should be able to handle most cases.
ALGORITHM 14.1 A SIMPLE SENTENCE-BREAKING ALGORITHM
1.
If found punctuation ./!/? advance one character and goto 2.
else advance one character and goto 1.
2.
If not found whitespace advance one character and goto 1.
3.
If the character is period (.) goto 4.
else goto 5.
4.
Perform abbreviation analysis.
If not an abbreviation goto 5.
else advance one character and goto 1.
5.
Declare a sentence boundary and sentence type ./!/?
Advance one character and goto 1.
For advanced sentence breakers, a weighted combination of the following kinds of
considerations may be used in constructing algorithms for determining sentence boundaries
(ordered from easiest/most common to most sophisticated):
 Abbreviation processing—Abbreviation processing is one of the most important
tasks in text normalization and will be described in detail in Section 14.4.
 Rules or CART built (Chapter 4) upon features based on: document structure,
whitespace, case conventions, etc.

694
Text and Phonetic Analysis
 Statistical frequencies on sentence-initial word likelihood
 Statistical frequencies of typical lengths of sentences for various genres
 Streaming syntactic/semantic (linguistic) analysis—Syntactic/semantic analysis
is also essential for providing critical information for phonetic and prosodic
analysis. Linguistic analysis will be described in Section 14.5.
As you can see, a deliberate sentence breaking requires a fair amount of linguistic process-
ing, like abbreviation processing and syntactic/semantic analysis. Since this type of analysis
is typically included in the later modules (text normalization or linguistic analysis), it might
be a sensible decision to delay the decision for sentence breaking until later modules, either
text normalization or linguistic analysis. In effect, this arrangement can be treated as the
document structure module passing along multiple hypotheses of sentence boundaries, and it
allows later modules with deeper linguistic knowledge (text normalization or linguistic
analysis) to make more intelligent decisions.
Finally, if a long buffer of unpunctuated words is presented, TTS systems may impose
arbitrary limits on the length of a sentence for later processing. For example, the writings of
the French author Marcel Proust contain some sentences that are several hundred words long
(average sentence length for ordinary prose is about 15 to 25 words).
<message>
<header>
<date>11 June 1998</date>
<from>Leslie</from>
<to>Jo</to>
<subject>Surf’s Up!</subject>
</header>
<body> … </body>
<sig>Freedom’s just another word for nothing left to lose</sig>
</message>
Figure 14.6 An example of e-mail marked by XML.
14.3.5.
E-mail
TTS could be ideal for reading e-mail over the phone or in an eyes-busy situation such as
when driving a motor vehicle. Here again we can speculate that XML-tagged e-mail struc-
ture, minimally something like the example in Figure 14.6, will be essential for high-quality
prosody, and for controlling the audio interface, allowing skips and speedups of areas the
user has defined as less critical, and allowing the system to announce the function of each
block. For example, the sig (signature) portion of e-mail certainly has a different semantic
function than the main message text and should be clearly identified as such, or skipped, at
the listener’s discretion. Modern e-mail systems are providing increasingly sophisticated
support for structure annotation such as that exemplified in Figure 14.6. Obviously, the e-

Document Structure Detection
695
mail document structure can be detected only with appropriate tags (like XML). It is very
difficult for a TTS system to detect it automatically.
14.3.6.
Web Pages
All the comments about TTS reliance on XML markup of document structure can be applied
to the case of HTML-marked Web page content as well. In addition to sections, headers,
lists, paragraphs, etc., the TTS systems should be aware of XML/HTML conventions such
as links (<a href=“…”>link name</a>) and perhaps apply some distinctive voice quality or
prosodic pitch contour to highlight these. The size and color of the section of text also pro-
vides useful hints for emphasis. Moreover, the TTS system should also integrate the render-
ing of audio and video contents on the Web page to create a genuine multimedia experience
for the users. More could be said about the rendition of Web content, whether from underly-
ing XML documents or HTML-marked documents prepared specifically for Web presenta-
tion. In addition, the World Wide Web Consortium has begun work on standards for aural
stylesheets that can work in conjunction with standard HTML to provide special direction in
aural rendition [33].
14.3.7.
Dialog Turns and Speech Acts
Not all text to be rendered by a TTS system is standard written prose. The more expressive
TTS systems could be tasked with rendering natural conversation and dialog in a spontane-
ous style. As with written documents, the TTS system has to be guided by XML markup of
its input. Various systems for marking dialog turns (change of speaker) and speech acts (the
mood and functional intent of an utterance)3 are used for this purpose, and these annotations
will trigger particular phonetic and prosodic rules in future TTS systems. The speech act
coding schemes can help, for example, in identifying the speaker’s intent with respect to an
utterance, as opposed to the utterance’s structural attributes. The prosodic contour and voice
quality selected by the TTS system might be highly dependent on this functional knowledge.
For example, a syntactically well-formed question might be used as information
solicitation, with the typical utterance-final pitch upturn as shown in the following:
<REQUEST_INFO>Can you hand me the wrench?</REQUEST_INFO>
But if the same utterance is used as a command, the prosody may change drastically.
<DIRECTIVE>Can you hand me the wrench.</DIRECTIVE>
Research on speech act markup-tag inventories (see Chapter 17) and automatic meth-
ods for speech act annotation of dialog is ongoing, and this research has the property consid-
ered desirable here, in that it is independently motivated (useful for enhancing speech rec-
ognition and language understanding systems). Thus, an advanced TTS system should be
expected to exploit dialog and speech act markups extensively.
3 Dialog modeling and the concepts of dialog turns and speech acts are described in detail in Chapter 17.

696
Text and Phonetic Analysis
14.4.
TEXT NORMALIZATION
Text often include abbreviations (e.g., FDA for Food and Drug Administration) and acro-
nyms (SWAT for Special Weapons And Tactics). Novels and short stories may include spo-
ken dialog interspersed with exposition; technical manuals may include mathematical for-
mulae, graphs, figures, charts and tables, with associated captions and numbers; e-mail may
require interpretation of special conventional symbols such as emoticons [e.g., :-) means
smileys], as well as Web and interInternet address formats, and special abbreviations (e.g.,
IMHO means in my humble opinion). Again, any text source may include part numbers,
stock quotes, dates, times, money and currency, and mathematical expressions, as well as
standard ordinal and cardinal formats. Without context analysis or prior knowledge, even a
human reader would sometimes be hard pressed to give a perfect rendition of every se-
quence of nonalphabetic characters or of every abbreviation. Text normalization (TN) is the
process of generating normalized orthography (or, for some systems, direct generation of
phones) from text containing words, numbers, punctuation, and other symbols. For example,
a simple example is given as follows:
The 7% Solution  THE SEVEN PER CENT SOLUTION
Text normalization is an essential requirement not only for TTS, but also for the
preparation of training text corpora for acoustic-model and language-model construction.4 In
addition, speech dictation systems face an analogous problem of inverse text normalization
for document creation from recognized words, and such systems may depend on knowledge
sources similar to those described in this section. The example of an inverse text normaliza-
tion for the example above is given as follows:
THE SEVEN PER CENT SOLUTION  The 7% Solution
Modular text normalization components, which may produce output for multiple down-
stream consumers, mark up the exemplary text along the following lines:
The <tn snor=”SEVEN PER CENT”>7%</tn> Solution
The snor tag stands for Standard Normalized Orthographic Representation.5 For
TTS, input text may include multisentence paragraphs, numbers, dates, times, punctuation,
symbols of all kinds, as well as interpretive annotations in a TTS markup language, such as
tags for word emphasis or pitch range. Text analysis for TTS is the work of converting such
text into a stream of normalized orthography, with all relevant input tagging preserved and
new markup added to guide the subsequent modules. Such interpretive annotations added by
text analysis are critical for phonetic and prosodic generation phases to produce desired out-
put. The output of the text normalizer may be deterministic, or may preserve a full set of
interpretations and processing history with or without probabilistic information to be passed
along to later stages. We once again assume that XML markup is an appropriate format for
4 For details of acoustic and language modeling, please refer to Chapters 9 and 11.
5 SNOR, or Standard Normalized Orthographic Representation, is a uniform way of writing words and sentences
that corresponds to spoken rendition. SNOR-format sentence texts are required as reference material for many
Defense Advanced Research Project Agency and National Institutes of Standards and Technology-sponsored stan-
dard speech technology evaluation procedures.

Text Normalization
697
expressing knowledge that can be created by a variety of external processes and exploited by
a number of technologies in addition to TTS.
Since today’s TTS systems typically cannot expect that their input be independently
marked up for text normalization, they incorporate internal technology to perform this func-
tion. Future systems may piggyback on full natural language processing solutions developed
for independent purposes. Presently, many incorporate minimal, TTS-specific hand-written
rules [1], while others are loose agglomerations of modular, task-specific statistical evalua-
tors [3].
For some purposes, an architecture that allows for a set or lattice of possible alterna-
tive expansions may be preferable to deterministic text normalization, like the n-best lists or
word graph offered by the speech recognizers described in Chapter 13. Alternatives known
to the system can be listed and ranked by probabilities that may be learnable from data.
Later stages of processing (linguistic analysis or speech synthesis) can either add knowledge
to the lattice structure or recover the best alternative, if needed. Consider the fragment “at 8
am I . . . ” in some informal writing such as e-mail. Given the flexibility of writing conven-
tions for pronunciation, am could be realized as either A. M. (the numeric context seems to
cue at times) or the auxiliary verb am. Both alternatives could be noted in a descriptive lat-
tice of covering interpretations, with confidence measures if known.
Table 14.2 Two alternative interpretations for sentence fragment “At 8 am I …”.
At 8 am I …
At <time> eight am </time> I …
At 8 am I …
At <number> eight </number> am I …
If the potential ambiguity in the interpretation of am in the above pair of examples is
simply noted, and the alternatives retained rather than suppressed, the choice can be made
by a later stage of syntactic/semantic processing. Note another feature of this example—the
rough irregular abbreviation form for antemeridian, which by prescriptive convention hopes
that high-quality TTS processing can rely entirely on standard stylistic conventions. That
observation also applies to the obligatory use of “?” for all questions.
Specific architectures for the text normalization component of TTS may be highly
variable, depending on the system architect’s answers to the following questions:
 Are cross-functional language processing resources mandated, or available?
 If so, are phonetic forms, with stress or accent, and normalized orthography,
available?
 Is a full syntactic and semantic analysis of input text mandated, or available?
 Can the presenting application add interpretive knowledge to structure the input
(text)?
 Are there interface or pipelining requirements that preclude lattice alternatives at
every stage?
Because of this variability in requirements and resources, we do not attempt to for-
mally specify a single, all-purpose architectural solution here. Rather, we concentrate on

698
Text and Phonetic Analysis
describing the text normalization challenges any system has to face. We note where solu-
tions to these challenges are more readily realized under particular architectural assump-
tions.
All text normalization consists of two phases: identification of type, and expansion to
SNOR or other unambiguous representation. Much of the identification phase, dealing with
phenomena of sentence boundary determination, abbreviation expansion, number spell-out,
etc. can be modeled as regular processes (see Chapter 11). This raises an interesting archi-
tectural issue. Imagine a system based entirely on regular finite state transducers (FST, see
Chapter 11), as in [27], which enforces an appealing uniformity of processing mechanism
and internal structure description. The FST permits a lattice-style representation that does
not require premature resolution of any structural choice. An entire text analysis system can
be based on such a representation. However, as long as a system confines its attention to
issues that commonly come under the heading of text normalization, such as number for-
mats, abbreviations, and sentence breaking, a simpler regular-expression-based uniform
mechanism for rule specification and structure representation may be adequate.
Alternatively, TTS systems could make use of advanced tools such as, for example,
the lex and yacc tools [17], which provide frameworks for writing customized lexical ana-
lyzers and context-free grammar parsers, respectively. In the discussion of typical text nor-
malization requirements below, examples will be provided and then a fragment of Perl pat-
tern-matching code will be shown that allows matching of the examples given. Perl notation
[36] is used as a convenient short-hand representing any equivalent regular expression pars-
ing system and can be regarded as a subset of the functionality provided by any regular ex-
pression, FST, or context-free grammar tool set that a TTS software architect may choose to
employ. Only a small subset of the simple, fairly standard Perl conventions for regular ex-
pression matching are used, and comments are provided in our discussion of text normaliza-
tion.
A text normalization system typically adds identification information to assist subse-
quent stages in their tasks. For example, if the TN subsystem has determined with some con-
fidence that a given digit string is a phone number, it can associate XML-like tags with its
output, identifying the corresponding normalized orthographic chunk as a candidate for spe-
cial phone-number intonation. In addition, the identification tags can guide the lexical dis-
ambiguation of terms for other processes, like phonetic analysis in TTS systems and training
data preparation for speech recognition.
Table 14.3 shows some examples of input fragments with a relaxed form of output
normalized orthography. It illustrates a possible ambiguity in TN output. In the (contrived)
example, the ambiguity is between a place name and a hypothetical individual named per-
haps Steve or Samuel Asia. Two questions arise in such cases. The first is format of specifi-
cation. The data between submodules in a TTS system can be passed (or be placed in a cen-
trally viewable blackboard location) as tagged text or in a binary format. This is an imple-
mentation detail. Most important is that all possibilities known to the TN system be speci-
fied in the output, and that confidence measures from the TN, if any, be represented. For
example, in many contexts, South Asia is the more likely spell-out of S. Africa, and this
should be indicated implicitly by ordering output strings, or explicitly with probability num-

Text Normalization
699
bers. The decision could then be delayed until one has enough information in the later mod-
ule (like linguistic analysis) to make the decision in an informed manner.
Table 14.3 Examples of the normalized output using XML-like tags for text normalization.
Dr. King
<title> DOCTOR </title> KING
7%
<number>SEVEN<ratio>PERCENT</ratio> </number>
S. Asia
<toponym> SOUTH ASIA </toponym>
OR <psn_name><initial>S</initial>ASIA</psn_name>
14.4.1.
Abbreviations and Acronyms
As noted above, a period is an important but not completely reliable clue to the presence of
an abbreviation. Periods may be omitted or misplaced in text for a variety of reasons. For
similar reasons of stylistic variability and a writer’s (lack of) care and skill, capitalization,
another potentially important clue, can be variable as well. For example, all the representa-
tions of the abbreviation for post script listed below have been observed in actual mail and
e-mail. A system must therefore combine knowledge from a variety of contextual sources,
such as document structure and origin, when resolving abbreviations:
PS. Don't forget your hat.
Ps. Don't forget your hat.
P.S. Don't forget your hat.
P.s. Don't forget your hat.
And P.S., when examined out of context, could be personal name initials as well. Of
course, a given TTS system’s user may be satisfied with the simple spoken output /p iy ae s/
in cases such as the above, obviating the need for full interpretation. But at a minimum,
when fallback to letter pronunciation is chosen, the TTS system must attempt to ensure that
some obvious spell-out is not being overlooked. For example, a system should not render the
title in Dr. Jones as letter names /d iy aa r/.
Actually, any abbreviation is potentially ambiguous, and there are several distinct
types of ambiguity. For example, there are abbreviations, typically quantity and measure
terms, which can be realized in English as either plural or singular depending on their nu-
meric coefficient, such as mm for millimeter(s). This type of ambiguity can get especially
tricky in the context of conventionally frozen items. For example, 9mm ammunition is typi-
cally spoken as nine millimeter ammunition rather than nine millimeters ammunition.
Next, there are forms that can, with appropriate syntactic context, be interpreted either
as abbreviations or as simple English words, such as in (inches), particularly at the end of
sentences.
Finally, many, perhaps most, abbreviations have entirely different abbreviation spell-
outs depending on semantic context, such as DC for direct current or District of Columbia.
This variability makes it unlikely that any system ever performs perfectly. However, with

700
Text and Phonetic Analysis
sufficient training data, some statistical guidelines for interpretation of common abbrevia-
tions in context can be derived. Table 14.4 shows a few more examples of this most difficult
type of ambiguity:
Table 14.4 Some ambiguous abbreviations.
Colorado
commanding officer
CO
conscientious objector
carbon monoxide
IRA
Individual Retirement Account
Irish Republican Army
Maryland
doctor of medicine
MD
muscular dystrophy
An advanced TTS system should attempt to convert reliably at least the following ab-
breviations:
 Title—Dr., MD, Mr., Mrs, Ms., St. (Saint), … etc.
 Measure—ft., in., mm, cm (centimeter), kg (kilogram), … etc.
 Place names—CO, LA, CA, DC, USA, st. (street), Dr. (Drive), … etc.
Abbreviation disambiguation usually can be resolved by POS (part-of-speech) analy-
sis. For example, whether Dr. is Doctor or Drive can be resolved by examining the POS
features of the previous and following words. If the abbreviation is followed by a capitalized
personal name, it can be expanded as Doctor, whereas if the abbreviation is preceded by a
capitalized place name, a number, or an alphanumeric (like 120th), it will be expanded as
Drive. Although the example above is resolved via a series of heuristic rules, the disam-
biguation (POS analysis) can also be done by a statistical approach. In [6], the POS tags are
determined based on the most likely POS sequence using POS trigram and lexical-POS uni-
gram. Since an abbreviation can often be distinguished by its POS feature, the most likely
POS sequence of the sentence discovered by the trigram search then provides the best guess
of the POS (thus the usage) for abbreviations [6]. We describe POS tagging in more detail in
Section 14.5.
Other than POS information, the lexical entries for abbreviations should include all
features and alternatives necessary to generate a lattice of possible analyses. For example, a
typical abbreviation’s entry might include information as to whether it could be a word (like
in), whether period(s) are optional or required, whether plural variants must be generated
and if so under what circumstances, whether numerical specification is expected or required,
etc.
Acronyms are words created from the first letters or parts of other words. For exam-
ple, SCUBA is an acronym for self-contained underwater breathing apparatus. Generally to
qualify as a true acronym, a letter sequence should reflect normal language phonotactics,
such as a reasonable alternation of consonants and vowels. From a TTS system’s point of
view, the distinctions between acronyms, abbreviations, and plain new or unknown words
can be unclear. Many acronyms can be entered into the TTS system lexicon just as ordinary
words would be. However, unknown acronyms (not listed in the lexicon) may occasionally

Text Normalization
701
be encountered. Although an acronym’s case property can be a significant clue to identifica-
tion, it is often unclear how to speak a given sequence of upper-case letters. Most TTS sys-
tems, failing to locate the sequence in the acronym dictionary, spell it out letter-by-letter.
Other systems attempt to determine whether the sequence is inherently speakable. For ex-
ample, DEC might be inherently speakable, while FCC is not formed according to normal
word phonotactics. When something speakable is found, it is processed via the normal letter-
to-sound rules, while something unspeakable would be spelled out letter-by-letter. Yet other
systems might simply feed the sequence directly to the letter-to-sound rules (see Section
14.8), just as they would any other unknown word. As with all such problems, a larger lexi-
con usually provides superior results.
ALGORITHM 14.2 ABBREVIATIONS AND ACRONYMS EXPANSION
1.
If word token w is not in abbreviation table goto 3.
2.
Abbreviation Expansion
If the POS tag of w and the correspondent abbreviation match
Abbreviation expansion by inserting SNOR and interpretive annotation tags
Advance one word and goto 1.
3.
Acronym Expansion
If w contains only capital letters
If w is in the predefined acronym table
Acronym expansion by inserting SNOR and interpretive annotation tags
according to acronym expansion table
else spell out w letter-by-letter
4.
Advance one word and goto 1.
The general algorithm for abbreviations and acronyms expansion in text normalization
is summarized in Algorithm 14.2. The algorithm assumes that tokenization and POS tagging
have been done for the whole sentence. Abbreviation expansion is determined by the POS
tags of the potential abbreviation candidates. Acronym expansion is done exclusively by
table lookup, and letter-by-letter spell-out is used when acronyms cannot be found in the
acronym table.
14.4.2.
Number Formats
Numbers occur in a wide variety of formats and have a wide variety of contextually depend-
ent reading styles. For example, the digits 370 in the context of the product name IBM 370
mainframe computer typically are read as three seventy, while in other contexts 370 would
be read as three hundred seventy or three hundred and seventy. In a phone number, such as
370-1111, the string would normally be read as three seven oh, while in still other contexts it
might be rendered as three seven zero. A text analysis system can incorporate rules, perhaps
augmented by probabilities, for these situations, but might never achieve perfection in all

702
Text and Phonetic Analysis
cases. Phone numbers are a practical place to start, and their treatment illustrates some of the
general issues relevant to the other number formats which are covered below.
14.4.2.1.
Phone Numbers
Phone numbers may include prefixes and area codes and may have dashes and parentheses
as separators. Examples are shown in Table 14.5.
Table 14.5 Some different written representations of phone numbers.
9-999-4118
9 345-5555
(617) 932-9209
(617) 932-9209
716-123-4568
409/845-2274
+49 (228) 550-381
+49-228-550-381
The first two examples have prefix codes, while the next three have area codes with
minor formatting differences. The final two examples are possible international-format
phone numbers. A basic Perl regular expression pattern to subsume the commonality in all
the local domestic numbers can be defined as follows:
$us_basic = '([0-9]{3}\-[0-9]{4})';
This defines a pattern subpart to match 3 digits, followed by a separator dash, fol-
lowed by another 4 digits. Then the pattern to match the prefix type would be:
/([0-9]{1})[\/ -]($us_basic)/
In the example above, this leaves the system pattern variable $1 (corresponding to the
first set of capture parentheses in the pattern) set to 1, and $2 (the second set of capture pa-
rentheses) set to 999-4118. Then a separate set of tables, indexed by the rule name and the
pattern variable contents, could provide orthographic spell-outs for the digits. Clearly a bal-
ance has to be struck between the number of pattern variables provided in the expression and
the overall complexity of the expression, vis-à-vis the complexity and sophistication of the
indexing scheme of the spell-out tables. For example, the $us_basic could be defined to in-
corporate parentheses capture on the first three digits and the remaining four separately,
which might lead to a simpler spell-out table in some cases.
The pattern to match the area code types could be:
/(\([0-9]{3}\))[\/ -]($us_basic)/
These patterns could be endlessly refined, expanded, and layered to match strings of almost
arbitrary complexity. A balance has to be struck between number and complexity of distinct

Text Normalization
703
patterns. In any case, no matter how sophisticated the matching mechanism, arbitrary or at
best probabilistic decisions have to be made in constructing a TTS system. For example, in
matching an area code type, the rule architect must decide how much and what kind of
whitespace separation the matching system tolerates between the area code and the rest of
the number before a phone-number match is considered unlikely. Or, as another example,
does the rule architect allow new lines or other formatting characters to appear between the
area code and the basic phone number? These kinds of decisions must be explicitly consid-
ered, or made by default, and should be specified to a reasonable degree in user documenta-
tion. There are a great many other phone number formats and issues that are beyond the
scope of this treatment.
Once a certain type of pattern requires a conversion to normalized orthography, the
question of how to perform the conversion arises. The conversion characters can be aligned
with the identification, so that conversion occurs implicitly during the pattern matching pro-
cess. Another way is to separate the conversion from the identification phase. This may or
may not lead to gains in efficiency and elimination of redundancy, depending on the overall
architecture of the system and whether and how components are expected to be reused. A
version of this second approach is sketched here.
Suppose that the pattern match variable $1 has been set to 617 by one of the identifica-
tion-phase pattern matches described above. Another list can provide pointers to conversion
tables, indexed by the rule name or number and the variable name. So for the rule that can
match area codes formatted as in (c) above, the relevant entry would be:
Identification rule name
Variable
Spellout table
name
Area-Phone
$1
LITERAL_DIGIT
The LITERAL_DIGIT spell-out rule set, when presented with the 617 character se-
quence (the value of $1), simply generates the normalized orthography six one seven, by
table lookup. In this simple and straightforward approach, spell-out tables such as LIT-
ERAL_DIGIT can be reused for portions of a wide variety of identification rules. Other
simple numeric spell-out tables would cover different styles of numeric reading, such as
pairwise style (e.g., six seventeen), full decimal with tens, hundreds, thousands units (six
hundred seventeen), and so on. Some spellout tables may require processing code to sup-
plement the basic table lookup. Additional examples of spell-out tables are not provided for
the various other types of text normalization entities exemplified below, but would function
similarly.
14.4.2.2.
Dates
Dates may be specified in a wide variety of formats, sometimes with a mixture of ortho-
graphic and numeric forms. Note that dates in TTS suffer from a mild form of the century-
date-change uncertainty (the infamous Y2K bug), so a form such as 5/7/37 may in the future
be ambiguous, in its full form, between 1937 and 2037. The safest course is to say as little as
possible, i.e., “five seven thirty seven”, or even “May seventh, thirty seven”, rather than at-

704
Text and Phonetic Analysis
tempt “May seventh, nineteen thirty seven”. Table 14.6 shows a variety of date formats and
associated normalized orthography.
Table 14.6 Various date formats.
12/19/94 (US)
December nineteenth ninety four
19/12/94 (European)
December nineteenth ninety four
04/27/1992
April twenty seventh nineteen ninety two
May 27, 1995
May twenty seventh nineteen ninety five
July 4, 94
July fourth ninety four
1,994
one thousand nine hundred and ninety four
1994
nineteen ninety four
One issue that comes up with certain number formats, including dates, is range check-
ing. A form like 13/19/94 is basically uninterpretable as a date. This kind of checking, if
included in the initial pattern matching, may be slow and may increase formal requirements
for power of the pattern matching system. Therefore, range checking can be done at spell-
out time (see below) during normalized orthography generation, as long as a backtracking or
redo option is present. If range checking is desired as part of the basic identification phase of
text normalization, some regular expression matching systems allow for extensions. For
example, the following pattern variable matches only numbers less than or equal to 12, the
valid month specifications. It can be included as part of a larger, more complex date match-
ing pattern:
$month = ‘/(0[123456789]/1[012]/’
14.4.2.3.
Times
Times may include hours, minute, seconds, and duration specifications as shown in Table
14.7. Time formats exemplify yet another area where linguistic concerns have to intersect
with architecture. If simple, flat normalized orthography is generated during a text normali-
zation phase, a later stage may still find a form like am ambiguous in pronunciation. If a
lattice of alternative interpretations is provided, it should be supplemented with interpretive
information on the linguistic status of the alternative text analyses. Alternatively, a single
best guess can be made, but even in this case, some kind of interpretive information indicat-
ing the status of the choice as, e.g., a time expression should be provided for later stages of
syntactic, semantic, and prosodic interpretation. This reiterates the importance of TTS text
analysis systems to generate interpretive annotations tags for subsequent modules’ use
whenever possible, as discussed in Section 14.4. In some cases, unique text formatting of the
choice, corresponding to the system’s lexical contents, may be sufficient. That is, in some
systems, generation of A.M., for example, may uniquely correspond to the lexicon’s entry
for that portion of a time expression, which specifies the desired pronunciation and gram-
matical treatment.

Text Normalization
705
Table 14.7 Several examples for time expressions.
11:15
eleven fifteen
8:30 pm
eight thirty pm
5:20 am
five twenty am
12:15:20
twelve hours fifteen minutes and twenty seconds
07:55:46
seven hours fifty-five minutes and forty-six seconds
14.4.2.4.
Money and Currency
As illustrated in Table 14.8, money and currency processing should correctly handle at least
the currency indications $, £, DM, ¥, and €, standing for dollars, British pounds, Deutsche
marks, Japanese yen, and euros, respectively. In general, $ and £ have to precede the nu-
meral; DM, ¥, and € have to follow the numeral. Other currencies are often written in full
words and have to follow the numeral, though abbreviations for these are sometimes found,
such as 100 francs and 20 lira.
Table 14.8 Several money and currency expressions.
$40
FORTY DOLLARS
£200
TWO HUNDRED POUNDS
5 ¥
FIVE YEN
25 DM
TWENTY FIVE DEUTSCH MARKS
300€
THREE HUNDRED EUROS
14.4.2.5.
Account Numbers
Account numbers may refer to bank accounts or social security numbers. Commercial prod-
uct part numbers often have these kinds of formats as well. In some cases these cannot be
readily distinguished from mathematical expressions or even phone numbers. Some exam-
ples are shown below:
123456-987-125456
000-1254887-87
049-85-5489
The other popular number format is that of credit card number, such as
4446-2289-2465-7065
3745-122267-22465
To process formats like these, it may eventually be desirable for TTS systems to pro-
vide customization capabilities analogous to the pronunciation customization features for

706
Text and Phonetic Analysis
words found in current TTS systems. Regular expression formalisms of the type exemplified
above for phone number, would, if exposed to applications and developers through suitable
editors, be adequate for most such needs.
14.4.2.6.
Ordinal Numbers
Ordinal numbers are those referring to rank or placement in a series. Examples include:
1st, 2nd, 3rd, 4th, 10th, 11th , 12th, 20th, 100th, 1000th, etc.
1th, 2nd, 3rd, 4th, 10th, 11th, 12th, 20th, 21st, 32nd, 100th, 1000th, etc.
The system’s ordinal processing may also be used to generate the denominators of
fractions, except for halves, as shown in Table 14.9. Notice that the ordinal must be plural
for numerators other than 1.
Table 14.9 Some examples of fractions.
1/2
one half
1/3
one third
1/4
one quarter or one fourth
1/10
one tenth
3/10
three tenths
14.4.2.7.
Cardinal Numbers
Cardinal numbers are, loosely speaking, those forms used in simple counting or the state-
ment of amounts. If a given sequence of digits fails to fit any of the more complex formats
above, it may be a simple cardinal number. These may be explicitly negative or positive or
assumed positive. They may include decimal or fractional specifications. They may be read
in several different styles, depending on context and/or aesthetic preferences. Table 14.10
gives some examples of cardinal numbers and alternatives for normalized orthography.
Table 14.10 Some cardinal number types.
123
one two three
one hundred (and) twenty three
1,230
one thousand two hundred (and) thirty
two four two six
twenty four twenty six
2426
two thousand (and) twenty six
The number-expansion algorithm is summarized in Algorithm 14.3. In this algorithm
the text normalization module maintains an extensive pattern table. Each pattern in the table
contains its associated pattern in regular expression or Perl format along with a pointer to a
rule in the conversion table, which guides the expansion process.

Text Normalization
707
A regular expression to match well-formed cardinals with commas grouping chunks of
three digits of the type from 1,000,000 to 999,999,999 might appear as:
if ($item =~ /^([0-9]{1,3}),([0-9]{3}),([0-9]{3})/
{
$NewFrame->{"millions"} = $1;
$NewFrame->{"thousands"} = $2;
$NewFrame->{"hundreds"} = $3;
print "Grouped cardinal found: $item\n";
return $NewFrame;
}
ALGORITHM 14.3 NUMBER EXPANSION
1.
Pattern Matching
If a match is found goto 2.
else goto 3.
2.
Number Expansion
Insert SNOR and interpretive annotation tags according to the associated rule
Advance the pointer to the right of the match pattern and goto 1.
3.
Finish
14.4.3.
Domain-Specific Tags
In keeping with the theme of this section—that is, the increasing importance of independ-
ently generated precise markup of text entities—we present a little-used but interesting ex-
ample.
14.4.3.1.
Mathematical Expressions
Mathematical expressions are regarded by some systems as the domain of special-purpose
processors. It is a serious question how far to go in mathematical expression parsing, since
providing some capability in this area may raise users’ expectations to an unrealistic level.
The World Wide Web Consortium has developed MathML (mathematical markup language)
[34], which provides a standard way of describing math expressions. MathML is an XML
extension for describing mathematical expression structure and content to enable mathemat-
ics to be served, received, and processed on the Web, similar to the function HTML has per-
formed for text. As XML becomes increasingly pervasive, MathML could possibly be used
to guide interpretation of mathematical expressions. For the notation (x + 2)2 a possible
MathML representation such as that below might serve as an initial guide for a spoken
rendition.
<EXPR>
<EXPR>
x

708
Text and Phonetic Analysis
<PLUS/>
2
</EXPR>
<POWER/>
2
</EXPR>
This might be generated by an application or by a specialized preprocessor within the TTS
system itself. Prosodic rules or data tables appropriate for math expressions could then be
triggered.
14.4.3.2.
Chemical Formulae
As XML becomes increasingly common and exploitable by TTS text normalization, other
areas follow. For example, Chemical Markup Language (CML [22]) now provides a stan-
dard way to describe molecular structure or chemical formulae. CML is an example of how
standard conventions for text markup are expected increasingly to replace ad hoc, TTS-
internal heuristics.
In CML, the chemical formula C2OCOH4 would appear as:
<FORMULA>
<XVAR BUILTIN=”STOICH”>
C C O C O H H H H
</XVAR>
</FORMULA>
It seems reasonable to expect that TTS engines of the future will be increasingly de-
voted to interpreting such precise conventions in high-quality speech renditions rather than
endlessly replicating NL heuristics that fail as often as they succeed in guessing the identity
of raw text strings.
14.4.4.
Miscellaneous Formats
A random list illustrating the range of other types of phenomena for which an English-
oriented TTS text analysis module must generate normalized orthography might include:
 Approximately/tilde: The symbol ~ is spoken as approximately before (Arabic)
numeral or currency amount, otherwise it is the character named tilde.
 Folding of accented Roman characters to nearest plain version: If the TTS sys-
tem has no knowledge of dealing with foreign languages, like French or Ger-
man, a table of folding characters can be provided so that for a term such as
Über-mensch, rather than spell out the word Über, or ignore it, the system can
convert it to its nearest English-orthography equivalent: Uber. The ultimate way
to process such foreign words should integrate a language identification module

Linguistic Analysis
709
with a multi-lingual TTS system, so that language-specific knowledge can be
utilized to produce appropriate text normalization of all text.
 Rather than simply ignore high ASCII characters in English (characters from
128 to 255), the text analysis lexicon can incorporate a table that gives character
names to all the printable high ASCII characters. These names are either the full
Unicode character names, or an abbreviated form of the Unicode names. This
would allow speaking the names of characters like (copyright sign), 
(trademark), @ (at), (registered mark), and so on.
 Asterisk: in email, the symbol ‘*’ may be used for emphasis and for setting off
an item for special attention. The text analysis module can introduce a little
pause to indicate possible emphasis when this situation is detected. For the ex-
ample of “Larry has *never* been here,” this may be suppressed for asterisks
spanning two or more words. In some texts, a word or phrase appearing com-
pletely in UPPER CASE may also be a signal for special emphasis.
 Emoticons: There are several possible emoticons (emotion icons).
1. :-) or :)
SMILEY FACE (humor, laughter, friendliness, sarcasm)
2. :-( or :(
FROWNING FACE (sadness, anger, or disapproval)
3. ;-) or ;)
WINKING SMILEY FACE (naughty)
4. :-D
OPEN-MOUTHED SMILEY FACE (laughing out loud)
Smileys, of which there are dozens of types, may be tacked onto word start or word
end or even occur interword without spaces, as in the following examples.
:)hi!
Hi:)
Hi:)Hi!
14.5.
LINGUISTIC ANALYSIS
Linguistic analysis (sometimes also referred to as syntactic and semantic parsing) of natural
language (NL) constitutes a major independent research field. Often commercial TTS sys-
tems incorporate some minimal parsing heuristics developed strictly for TTS. Alternatively,
the TTS systems can also take advantage of independently motivated natural language proc-
essing (NLP) systems, which can produce structural and semantic information about sen-
tences. Such linguistically analyzed documents can be used for many purposes other than
TTS—information retrieval, machine translation system training, etc.
Provision of some parsing capability is useful to TTS systems in several areas. Parsers
may be used in disambiguating the text normalization alternatives described above. Addi-
tionally, syntactic/semantic analysis can help to resolve grammatical features of individual
words that may vary in pronunciation according to sense or abstract inflection, such as read.

710
Text and Phonetic Analysis
Finally, parsing can lay a foundation for derivation of a prosodic structure useful in deter-
mining segmental duration and pitch contour.
The fundamental types of information desired for TTS from a parsing analysis are
summarized below:
 Word part of speech (POS) or word type, e.g., proper name or verb.
 Word sense, e.g., river bank vs. money bank.
 Phrasal cohesion of words, such as idioms, syntactic phrases, clauses, sentences.
 Modification relations among words.
 Anaphora (co-reference) and synonymy among words and phrases.
 Syntactic type identification, such as questions, quotes, commands, etc.
 Semantic focus identification (emphasis).
 Semantic type and speech act identification, such as requesting, informing, nar-
rating, etc.
 Genre and style analysis.
Here we confine ourselves to discussion of the kind of information that a good parser could,
in principle, provide to enable the TTS-specific functionality.
Linguistic analysis supports the phonetic analysis and prosodic generation phases. The
modules of phonetic analysis are covered in Sections 14.6, 14.7, and 14.8. A linguistic
parser can contribute in several ways to the process of generating (symbolic) phonetic forms
from orthographic words found in text. One function of a parser is to provide accurate part-
of-speech (POS) labels. This can aid in resolving the pronunciation of several hundred
American English homographs, such as object and absent. Homographs are discussed in
greater detail in Section 14.6 below. Parsers can also aid in identifying names and other spe-
cial classes of vocabulary for which specialized pronunciation rule sets may exist [32].
Prosody generation deals mainly with assignment of segmental duration and pitch con-
tour that have close relationship with prosodic phrasing (pause placement) and accentuation.
Parsing can contribute useful information, such as the syntactic type of an utterance. (e.g.,
yes/no question contours typically differ from wh-question contours, though both are
marked simply by ‘?’ in text), as well as semantic relations of synonymy, anaphora, and
focus that may affect accentuation and prosodic phrasing. Information from discourse analy-
sis and text genre characterization may affect pitch range and voice quality settings. Further
examination of the contribution of parsing specifically to prosodic phrasing, accentuation,
and other prosodic interpretation is provided in Chapter 15.
As mentioned earlier, TTS can employ either a general-purpose NL analysis engine or
a pipeline of a number of very narrowly targeted, special-purpose NL modules together for
the requirement of TTS linguistic analysis. Although we focus on linguistic information for
supporting phonetic analysis and prosody generation here, a lot of the information and ser-
vices are beneficial to document structure detection and text normalization described in pre-
vious sections.
The minimum requirement for such a linguistic analysis module is to include a lexicon
of the closed-class function words, of which only several hundred exist in English (at most),

Linguistic Analysis
711
and perhaps homographs. In addition, a minimal set of modular functions or services would
include:
 Sentence breaking—Sentence breaking has been discussed in Section 14.3.4
above.
 POS tagging—POS tagging can be regarded as a two-stage process. The first is
POS guessing, which is the process of determining, through a combination of a
(possibly small) dictionary and some morphological heuristics or a specialized
morphological parser, the POS categories that might be appropriate for a given
input term in isolation. The second is POS choosing—that is, the resolution of
the POS in context, via local short-window syntactic rules, perhaps combined
with probabilistic distribution for the POS guesses of a given word. Sometimes
the guessing and choosing functions are combined in a single statistical frame-
work. In [6], lexical probabilities are unigram frequencies of assignments of
categories to words estimated from corpora. For example, in the original formu-
lation of the model, the lexical probabilities [
(
|
)
i
i
P c
w
, where
ic
is the hy-
pothesized POS for word
iw ], were estimated from the hand-tagged Brown cor-
pus [8], and the word see appeared 771 times as a verb and once as an interjec-
tion. Thus the probability that see is a verb is estimated to be 771/772 or 0.99.
Trigrams are used for contextual probability [
1
2
1
1
2
(
|
)
(
|
)
i
i
i
i
i
i
P c
c c
c
P c
c c
−
−
−
−
=

]. Lexical probabilities and trigrams over category sequences are used to score
all possible assignments of categories to words for a given input word sequence.
The entire set of possible assignments of categories to words in sequence is cal-
culated ,and the best-scoring sequence is used. Likewise, simple methods have
been used to detect noun phrases (NPs), which can be useful in assigning pro-
nunciation, stress, and prosody. The method described in [6] relies on a table of
probabilities for inserting an NP begin bracket ‘[’ between any two POS catego-
ries, and similarly for an NP end bracket ‘]’. This was also trained on the POS-
labeled Brown corpus, with further augmentation for the NP labels. For exam-
ple, the probability of inserting an NP begin bracket after an article was found to
be much lower than that of begin-bracket insertion between a verb and a noun,
thus automatically replicating human intuition.
 Homograph disambiguation—Homograph disambiguation in general refers to
the case of words with the same orthographic representation (written form) but
having different semantic meanings and sometimes even different pronuncia-
tions. Sometimes it is also referred as sense disambiguation. Examples include
“The boy used the bat to hit a home run” vs. “We saw a large bat in the zoo”
(the pronunciation is the same for two bat) and “You record your voice” vs. “I’d
like to buy that record” (the pronunciations are different for the two record).
The linguistic analysis module should at least try to resolve the ambiguity for
the case of different pronunciations because it is absolutely required for correct
phonetic rendering. Typically, the ambiguity can be resolved based on POS and

712
Text and Phonetic Analysis
lexical features. Homograph disambiguation is described in detail in Section
14.6.
 Noun phrase (NP) and clause detection—Basic NP and clause information
could be critical for a prosodic generation module to generate segmental dura-
tions. It also provides useful cues to introduce necessary pauses for intelligibility
and naturalness. Phrase and clause structure are well covered in any parsing
techniques.
 Sentence type identification—Sentence types (declarative, yes-no question, etc.)
are critical for macro-level prosody for the sentence. Typical techniques for
identifying sentence types have been covered in Section 14.3.4.
If a more sophisticated parser is available, a richer analysis can be derived. A so-called
shallow parse is one that shows syntactic bracketing and phrase type, based on the POS of
words contained in the phrases. A training corpus of shallow-parsed sentences has been cre-
ated for the Linguistic Data Consortium [16]. The following example illustrates a shallow
parse for sentence : “For six years, Marshall Hahn Jr. has made corporate acquisitions in the
George Bush mode: kind and gentle.”
For/IN[six/CD years/NNS],/,[T./NNP Marshall/NNP
Hahn/NNP Jr./NNP]has/VBZ made/VBN[corporate/JJ acquisi-
tions/NNS]in/IN[the/DT George/NNP Bush/NNP mode/NN]
:/:[kind/JJ]and/CC[gentle/JJ]./.
The POS labels used in this example are described in Chapter 2 (Table 2.14). A TTS
system uses the POS labels in the parse to decide alternative pronunciations and to assign
differing degrees of prosodic prominence. Additionally, the bracketing might assist in decid-
ing where to place pauses for great intelligibility. A fuller parse would incorporate more
higher-order structure, including sentence type identification, and more semantic analysis,
including co-reference.
14.6.
HOMOGRAPH DISAMBIGUATION
For written languages, sense ambiguities occur when words have different syntac-
tic/semantic meanings. Those words with different senses are called polysemous words. For
example, bat could mean either a kind of animal or the equipment to hit a baseball. Since the
pronunciations for the two different senses of bat are identical, we are in general only con-
cerned6 about the other type of polysemous words that are homographs (spelled alike but
vary in pronunciation), such as bass for a kind of fish (/b ae s/) or an instrument (/b ey s/).
Homograph variation can often be resolved on POS (grammatical) category. Examples
include object, minute, bow, bass, absent, etc. Unfortunately, correct determination of POS
(whether by a parsing system or statistical methods) is not always sufficient to resolve pro-
6 Sometimes, a polysemous word with the same pronunciation could have impact for prosodic generation because
different semantic properties could have different accentuation effects. Therefore, a high-quality TTS system can
definitely be benefited from word-sense disambiguation beyond homograph disambiguation.

Homograph Disambiguation
713
nunciation alternatives. For example, simply knowing that the form bow is a noun does not
allow us to distinguish the pronunciation appropriate for the instrument of archery from that
for the front part of a boat. Even more subtle is the pronunciation of read in “If you read the
book, he’ll be angry.” Without contextual clues, even human readers cannot resolve the pro-
nunciation of read from the given sentence alone. Even though the past tense is more likely
in some sense, deep semantic and/or discourse analysis would be required to resolve the
tense ambiguity.
Several hundred English homographs extracted from the 1974 Oxford Advanced
Learners Dictionary are listed in [10]. Here are some examples:
 Stress homographs: noun with front-stress vowel, verb with end-stress vowel
“an absent boy” vs. “Do you choose to absent yourself?”
 Voicing: noun/verb or adjective/verb distinction made by voice final consonant
“They will abuse him.” vs. “They won’t take abuse.”
 –ate words: noun/adjective sense uses schwa, verb sense uses a full vowel
“He will graduate.” vs. “He is a graduate.”
 Double stress: front-stressed before noun, end-stressed when final in phrase
“an overnight bag” vs. “Are you staying overnight?”
 -ed adjectives with matching verb past tenses
“He is a learned man.” vs. “He learned to play piano.”
 Ambiguous abbreviations: already described in Section 14.4.1
in, am, SAT (Saturday vs. Standard Aptitude Test)
 Borrowed words from other languages—They could sometimes be distinguish-
able based on capitalization.
“El Camino Real road in California” vs. “real world”
“polish shoes” vs. “Polish accent”
 Miscellaneous
“The sewer overflowed.” vs. “a sewer is not a tailor.”
“He moped since his parents refused to buy a moped.”
“Agape is a Greek word.” vs. “His mouth was agape.”
As discussed earlier, abbreviation/acronym expansion and linguistic analysis described
in Sections 14.4.1 and 14.5 are two main sources of information for TTS systems to resolve
homograph ambiguities.
We close this section by introducing two special sources of pronunciation ambiguity
that are not fully addressed by current TTS systems. The first one is a variation of dialects
(or even personal dialect—idiolect). For example, some might say tom[ey]to, while some
others might say tom[aa]to. Another example is that Boston natives tends to reduce the /r/
sound in sentences like “Park your car in Harvard yard.” Similarly, some people use the
spelling pronunciation in-ter-es-ting as opposed to intristing. Finally, speech rate and for-
mality level can influence pronunciation. For example, the /g/ sound in recognize may be
omitted in faster speech. It might be a sensible decision to output all possible pronunciations
as a multiple pronunciation list and hope the synthesis back end picks the one with better

714
Text and Phonetic Analysis
acoustic/prosodic voice rendition. While true homographs may be resolved by linguistic and
discourse analysis, achieving a consistent presentation of dialectal and stylistic variation is
an even more difficult research challenge.
The other special source of ambiguity in TTS is somewhat different from what we
have considered so far, but may be a concern in some markets. Most borrowed or foreign
single words and place names are realized naturally with pronunciation normalized to the
main presentation language. Going beyond that, language detection refers to the ability of a
TTS system to recognize the intended language of a multiword stretch of text. For example,
consider the fragment “Well, as for the next department head, that is simply une chose en-
tendue.” The French phrase “une chose entendue” (something clearly understood) might be
realized in a proper French accent and phone pronunciation by a bilingual English/French
reader. For a TTS system to mimic the best performance, the system must have:
 language identification capability
 dictionaries and rules for both languages
 voice rendition capability for both languages
14.7.
MORPHOLOGICAL ANALYSIS
General issues in morphology are covered in Chapter 2. Here, we consider issues of relating
a surface orthographic form to its pronunciation by analyzing its component morphemes,
which are minimal, meaningful elements of words, such as prefixes, suffixes, and stem
words themselves. This decomposition process is referred as morphological analysis [28].
When a dictionary does not list a given orthographic form explicitly, it is sometimes possi-
ble to analyze the new word in terms of shorter forms already present. These shorter forms
may combine as prefixes, one or more stems or roots, and suffixes to generate new forms. If
a word can be so analyzed, the listed pronunciations of the pieces can be combined, perhaps
with some adjustment (phonological rules), to yield a phonetic form for the word as a whole.
The prefixes and suffixes are generally considered bound, in the sense that they cannot
stand alone but must combine with a stem. A stem, however, can stand alone. A word such
as establishment may be decomposed into a “stem” establish and a suffix -ment. In practice,
it is not always clear where this kind of analysis should stop. That is, should a system at-
tempt to further decompose the stem establish into establ and -ish? These kinds of questions
ultimately belong to etymology, the study of word origins, and there is no final answer.
However, for practical purposes, having three classes of entries corresponding to prefixes,
stems, and suffixes, where the uses of the affixes are intuitively obvious to educated native
speakers, is usually sufficient. In practical language engineering, a difference that makes no
difference is no difference, and unless there is a substantial gain in compression or analytical
power, it is best to be conservative and list only obvious and highly productive affixes.
The English language presents numerous genuine puzzles in morphological analysis.
For example, there is the issue of abstraction: is the word geese one morpheme, or two (base
goose + abstract pluralizing morpheme)? For practical TTS systems, relying on large dic-
tionaries, it is generally best to deal with concrete, observable forms where possible. In such

Morphological analysis
715
a lexically oriented system, the word geese probably should appear in the lexicon as such,
with attached grammatical features including plurality. Likewise, it is simpler to include
children in the lexical listing rather than create a special pluralizing suffix -ren whose use is
restricted to the single base child.
The morphological analyzer must attempt to cover an input word in terms of the af-
fixes and stems listed in the morphological lexicon. The covering(s) proposed must be legal
sequences of forms, so that often a word grammar is supplied to express the allowable pat-
terns of combinations. A word grammar might, for example, restrict suffixation to the final
or rightmost stem of a compound, thus allowing plurality on the final element of business-
men but not in the initial stem (businessesman). In support of the word grammar, all stems
and affixes in the lexicon would be listed with morphological combinatory class specifica-
tions, usually subtyped in accordance with the base POS categories of the lexicon entries.
That is, verbs would typically accept a different set of affixes than nouns or adjectives. In
addition, spelling changes that sometimes accompany affixation must be recognized and
undone during analysis. For example, the word stopping has undergone final consonant
doubling as part of accommodating the suffix ing.
A morphological analysis system might be as simple as a set of suffix-stripping rules
for English. If a word cannot be found in the lexicon, a suffix-stripping rule can be applied
to first strip out the possible suffix, including –s, -’s, -ing, -ed, -est, -ment, etc. If the stripped
form can be found in the lexicon, a morphological decomposition is attained. Similarly, pre-
fix-stripping rules can be applied to find prefix-stem decomposition for prefixes like in-, un-,
non-, pre-, sub-, etc., although in general prefix stripping is less reliable.
Suffix and prefix stripping gives an analysis for many common inflected and some de-
rived words such as helped, cats, establishment, unsafe, predetermine, subword, etc. It helps
in saving system storage. However, it does not account for compounding, issues of legality
of sequence (word grammar), or spelling changes. It can also make mistakes (from a syn-
chronic point of view: basement is not base + -ment), some of which will have consequences
in TTS rendition. A more sophisticated version could be constructed by adding elements
such as POS type on each suffix/prefix for a rudimentary legality check on combinations.
However, a truly robust morphological capability would require more powerful formal ma-
chinery and a more thorough analysis. Therefore, adding irregular morphological formation
into a system dictionary is always a desirable solution.
Finally, sometimes in commercial product names the compounding structure is sig-
naled by word-medial case differences, e.g., AltaVistaTM, which can aid phonetic conversion
algorithms. These can be treated as two separate words and will often sound more natural if
rendered with two separate main stresses. This type of decomposition can be expanded to
find compound words that are formed by two separate nouns. Standard morphological
analysis algorithms employing suffix/prefix stripping and compound word decomposition
are summarized in Algorithm 14.4.

716
Text and Phonetic Analysis
ALGORITHM 14.4 MORPHOLOGICAL ANALYSIS
1.
Dictionary Lookup
Look up word w in lexicon
If found
Output attributes of the found lexical entry and exit
2.
Suffix Stripping
If word ends in –s, -’s, -ing, -ed, -est, -ment, etc
Strip the suffix from word w to form u
If stripped form u found in lexicon
Output attributes of the stem and suffix and exit
3.
Prefix Stripping
If word begins with in-, un-, non-, pre-, sub-, etc
Strip the prefix from word w to form u
If stripped form u found in lexicon
Output attributes of the prefix and stem and exit
4.
Compound word decomposition
If detect word-medial case differences within word w
Break word w into a multiple words u1, u2, u3, …according to case changes
For words u1, u2, u3, goto 1.
Else if word w can be decomposed into two nouns u1, u2
Output attributes of the u1, u2 and exit
5.
Pass word w to letter-to-sound module
14.8.
LETTER-TO-SOUND CONVERSION
The best resource for generating (symbolic) phonetic forms from words is an extensive word
list. The accuracy and efficiency of such a solution is limited only by the time, effort, and
knowledge brought to bear on the dictionary construction process. As described in Section
14.2, a general lexicon service is a critical resource for the TTS system. Thus, the first and
the most reliable way for grapheme-to-phoneme conversion is via dictionary lookup.
Where direct dictionary lookup fails, rules may be used to generate phonetic forms.
Under earlier naïve assumptions about the regularity and coverage of simple descriptions of
English orthography, rules have traditionally been viewed as the primary source of phonetic
conversion knowledge, since no dictionary covers every input form and the TTS system
must always be able to speak any word. A general letter-to-sound (LTS) conversion is thus
required to provide phonetic pronunciation for any sequence of letters.
Inspired by the phonetic languages, letter-to-sound conversion is usually carried out
by a set of rules. These rules can be thought of as dictionaries of fragments with some spe-
cial conventions about lookup and context. Typically, rules for phonetic conversion have
mimicked phonological rewriting in phonological theory [5], including conventions of or-
dering, such as most specific first. In phonological rules, a target is given and the rewrite is

Letter-to-Sound Conversion
717
indicated, with context following. For example, a set of rules that changes orthographic k to
a velar plosive /k/ except when the k is word-initial (‘[‘) followed by n might appear as:
k -> /sil/ % [ _ n
k -> /k/
The rule above reads that k is rewritten as (phonetic) silence when in word initial position
and followed by n, otherwise k is rewritten as (phonetic) /k/. The underscore in the first line
is a placeholder for the k itself in specifying the context. This little set properly treats k in
knight, darkness, and kitten. These are formally powerful, context-sensitive rules. Generally
a TTS system require hundreds or even thousands of such rules to cover words not appear-
ing in the system dictionary or exception list. Typically rules are specified in terms of single-
letter targets, such as the example for k above. However, some systems may have rules for
longer fragments, such as the special vowel and consonant combinations in words like
neighbor and weigh. In practice, a binary format for compression, a corresponding fragment
matching capability, and a rule index must be defined for efficient system deployment.
Rules of this type are tedious to develop manually. As with any expert system, it is dif-
ficult to anticipate all possible relevant cases and sometimes hard to check for rule interfer-
ence and redundancy. In any case, the rules must be verified over a test list of words with
known transcriptions. Generally, if prediction of main stress location is not attempted, such
rules might account for up to 70% of the words in a test corpus of general English. If predic-
tion of main stress is attempted, the percentage of correct phonetic pronunciations is much
lower, perhaps below 50%. The correct prediction of stress depends in part on morphology,
which is not typically explicitly attempted in this type of simple rule system (though frag-
ments corresponding to affixes are frequently used, such as tion -> /ah ax n/). Certainly,
such rules can be made to approach dictionary accuracy, as longer and more explicit mor-
phological fragments are included. One extreme case is to create one specific rule (contain-
ing exact contexts for the whole word) for each word in the dictionary. Obviously this is not
desirable, since it is equivalent to putting the word along with its phonetic pronunciation in
the dictionary.
In view of how costly it is to develop LTS rules, particularly for a new language, at-
tempts have been made recently to automate the acquisition of LTS conversion rules. These
self-organizing methods believe that, given a set of words with correct phonetic transcrip-
tions (the offline dictionary), an automated learning system could capture significant gener-
alizations. Among them, classification and regression trees (CART) have been demonstrated
to give satisfactory performances for letter-to-sound conversion. For basic and theoretic de-
scription of CART, please refer to Chapter 4.
In the system described in [14], CART methods and phoneme trigrams were used to
construct an accurate conversion procedure. All of the experiments were carried on two da-
tabases. The first is the NETALK [25], which has hand-labeled alignment between letter and
phoneme transcriptions. The second is the CMU dictionary, which does not have any align-
ment information. The NETALK database consists of 19,940 entries, of which 14,955 were
randomly selected as atraining set and the remaining 4951 were reserved for testing. Those
4951 words correspond to 4985 entries in the database because of multiple pronunciations.
The hand-labeled alignments were used directly to train the CART for LTS conversion. The

718
Text and Phonetic Analysis
CMU dictionary has more than 100,000 words, of which the top 60,000 words were selected
based on unigram frequencies trained from North American Business News. Among them,
52,415 were used for training and 9719 reserved for testing. Due to multiple pronunciations,
those 9719 words have 10,520 entries in the dictionary. Due to lack of alignment informa-
tion, dynamic programming was used to align each letter to the corresponding phoneme be-
fore training the LTS CART.
The basic CART component includes a set of yes-no questions and a procedure to se-
lect the best question at each node to grow the tree from the root. The basic yes-no question
for LTS conversion looks like “Is the second right letter ‘p’?” or “Is the first left output
phoneme /ay/?” The questions for letters could be on either the left or the right side. For
phones, only questions on the left side were used, for simplicity. The range of question posi-
tions must be long enough to cover the long-distance phonological variations. It was found
that the 11-letter window (5 for left letter context and 5 for right letter context) and 3-
phoneme window for left phoneme context are generally sufficient. A primitive set of ques-
tions would be the set of all the singleton questions about each letter or phoneme identity.
When growing the tree, the question that had the best entropy reduction was chosen at each
node. We observed that if we allow the node to have a complex question that is a combina-
tion of primitive questions, the depth of the tree will be greatly reduced and the performance
improved. For example, the complex question “Is the second left letter ‘t’ and the first left
letter ‘i’ and the first right letter ‘n’?” can capture ‘o’ in common suffix “tion” and convert
it to the right phoneme. Complex questions can also alleviate the data fragment problem
caused by greedy nature of the CART algorithm. This way of finding such complex ques-
tions is similar to those used in Chapter 4. The baseline system built using the above tech-
niques has error rates as listed in Table 14.11.
Table 14.11 LTS baseline results using CART [13].
Database
Phoneme
Word
CMU Lexicon
9.7%
35.0%
NETTALK
9.5%
42.3%
The CART LTS system [14] further improved the accuracy of the system via the fol-
lowing extensions and refinements:
 Phoneme trigram rescoring: A statistical model of phoneme co-occurrence, or
phonotactics, was constructed over the training set. A phonemic trigram was
generated from the training samples with back-off smoothing, and this was used
to rescore the n-best list generated by LTS.
 Multiple tree combination: The training data was partitioned into two parts and
two trees were trained. When the performance of these two trees was tested, it
was found that they had a great overlap but also behaved differently, as each had
a different focus region. Combining them together greatly improved the cover-
age. To get a better overall accuracy, the tree trained by all the samples was used
together with two other trees, each trained by half of the samples. The leaf dis-

Evaluation
719
tributions of three trees were interpolated together with equal weights and then
phonemic trigram was used to rescore the n-best output lists.
By incrementally experimenting with addition of these extensions and refinements, the re-
sults improved, as shown in Table 14.12:
Table 14.12 LTS using multiple trees and phonemic trigram rescoring [13].
Database
Phoneme
Word
CMU Lexicon
8.2%
26.9%
NETTALK
8.1%
34.2%
These experiments did not include prediction of stress location. Stress prediction is a
difficult problem, as we pointed out earlier. It requires information beyond the letter string.
In principle, one can incorporate more lexical information, including POS and morphologic
information, into the CART LTS framework, so it can be more powerful of learning the
phonetic correspondence between the letter string and lexical properties.
14.9.
EVALUATION
Ever since the early days of TTS research [21, 31], evaluation has been considered an inte-
gral part of the development of TTS systems. End users and application developers are
mostly interested the end-to-end evaluation of TTS systems. This monolithic type of whole-
system evaluation is often referred to as black-box evaluation. On the other hand, modular
(component) testing is more appropriate for TTS researchers when working with isolated
components of the TTS system, for diagnosis or regression testing. We often refer to this
type of evaluation as glass-box evaluation. We discuss the modular evaluations in each
modular TTS chapter, while leaving the evaluation of the whole system to Chapter 16.
For text and phonetic analysis, automated, analytic, and objective evaluation is usually
feasible, because the input and output of such module is relatively well defined. The evalua-
tion focuses mainly on symbolic and linguistic level in contrast to the acoustic level, with
which prosodic generation and speech synthesis modules need to deal. Such tests usually
involve establishing a test corpus of correctly tagged examples of the tested materials, which
can be automatically checked against the output of a text analysis module. It is not particu-
larly productive to discuss such testing in the abstract, since the test features must closely
track each system’s design and implementation. Nevertheless, a few typical areas for testing
can be noted. In general, tests are simultaneously testing the linguistic model and content as
well as the software implementation of a system, so whenever a discrepancy arises, both
possible sources of error must be considered.
For automatic detection of document structures, the evaluation typically focuses on
sentence breaking and sentence type detection. Since the definitions of these two types of
document structures are straightforward, a standard evaluation database can be easily estab-
lished.

720
Text and Phonetic Analysis
In the basic level, the evaluation for the text normalization component should include
large regression test databases of text micro-entities: addresses, Internet and e-mail entities,
numbers in many formats (ordinal, cardinal, mathematical, phone, currency, etc.), titles, and
abbreviations in a variety of contexts. These would be paired with the correct reference
forms in something like the SNOR used in ASR output evaluation. In its simplest form, this
would consist of a database of automatically checkable paired entries like 7% vs. seven per-
cent, and $1.20 vs. one dollar and twenty cents. If you want to evaluate the semantic capa-
bility of text normalization, the regression database might include markups for semantic
tags, so that we have 7% vs. “<number>SEVEN<ratio>PERCENT</ratio></number>”, and
$1.20 vs. “<money>ONE DOLLAR AND TWENTY CENTS</money>”. The regression
database could include domain-specific entries. This implies some dependence on the sys-
tem’s API—its markup capabilities or tag set. In the examples given in Table 14.13, the first
one is a desirable output for domain-independent input, while the second one is suitable for
normalization of the same expression in mathematical formula domain.
Table 14.13 Two examples to test domain independent/dependent text normalization.
three to four
3-4
three four
<math_exp> 3-4 </math_exp>
three minus four
Some systems may not have a discrete level of orthographic or SNOR representation
that easily lends itself to the type of evaluation described in this section. Such systems may
have to evaluate their text normalization component in terms of LTS conversion.
An automated test framework for the LTS conversion analysis minimally includes a
set of test words and their phonetic transcriptions for automated lookup and comparison
tests. The problem is the infinite nature of language: there are always new words that the
system does not convert correctly, and many of these will initially lack a transcription of
record even to allow systematic checking. Therefore, a comprehensive test program for test
of phonetic conversion accuracy needs to be paired with a data development effort. The data
effort has two goals: to secure a continuous source of potential new words, such as a 24-hour
newswire feed, and to maintain and construct an offline test dictionary, where reference
transcriptions for new words are constantly created and maintained by human experts. This
requirement illustrates the codependence of automated and manual aspects of evaluation.
Different types and sources of vocabulary need to be considered separately, and they may
have differing testing requirements, depending, again, on the nature of the particular system
to be evaluated. For example, some systems have elaborate subsystems targeted specifically
for name conversion. Such systems may depend on other kinds of preprocessing technolo-
gies, such as name identification modules, that might be tested independently.
The correct phonetic representation of a word usually depends on its sentence and
even discourse contexts, as described in Section 14.6. Therefore, the adequacy of LTS con-
version should not, in principle, be evaluated on the basis of isolated word pronunciations.
However, a list of isolated word pronunciations is often used in LTS conversion because of
its simplicity. Discourse contexts are, in general, difficult to represent unless specific appli-
cations and markup tags are available to the evaluation database. A reasonable compromise

Case Study: Festival
721
is to use a list of independent sentences with their corresponding phonetic representation for
the evaluation of grapheme-to-phoneme conversion.
Error analysis should be treated as equally important as the evaluation itself. For ex-
ample, if a confusability matrix shows that a given system frequently confuses central and
schwa-like unstressed vowels, this may be viewed as less serious than other kinds of errors.
Other subareas of LTS conversion that could be singled out for special diagnosis and testing
include morphological analysis and stress placement. Of course, testing with phonemic tran-
scriptions is the ultimate unit test in the sense that it contains nothing to insure that the cor-
rectly transcribed words, when spoken by the system’s artificial voice and prosody, are intel-
ligible or pleasant to hear. Phone transcription accuracy is, thus, a necessary but not a suffi-
cient condition of quality.
14.10. CASE STUDY: FESTIVAL
The University of Edinburgh’s Festival [3] has been designed to take advantage of modular
subcomponents for various standard functions. Festival provides a complete text and pho-
netic analysis with modules organized in sequence roughly equivalent to Figure 14.1. Festi-
val outputs speech of quality comparable to many commercial synthesizers. While default
routines are provided for each stage of processing, the system is architecturally designed to
accept alternative routines in modular fashion, as long as the data transfer protocols are fol-
lowed. This variant of the traditional TTS architecture is particularly attractive for commer-
cial purposes (development, maintenance, testing, scalability) as well as research. Festival
can be called in various ways with a variety of switches and filters, set from a variety of
sanctioned programming and scripting languages. These control options are beyond the
scope of this overview.
14.10.1. Lexicon
Festival employs phonemes as the basic sounding units, which are used not only as the at-
oms of word transcriptions in the lexicons, but also as the organizing principle for unit selec-
tion (see Chapter 16) in the synthesizer itself. Festival can support a number of distinct
phone sets and it supports mapping from one to another. A phone defined in a set can have
various associated phonological features, such as vowel, high, low, etc.
The Festival lexicon, which may contain several components, provides pronunciations
for words. The addenda is an optional list of words that are unique to a particular user,
document, or application. The addenda is searched linearly. The main system lexicon is ex-
pected to be large enough to require compression and is assumed to reside on a disk or other
external storage. It is accessed via binary search. The lexical entry also contains POS infor-
mation, which can be modified according to the preference of the system configurer. A typi-
cal lexical entry consists of the word key, a POS tag, and phonetic pronunciation (with stress
and possible syllabification indicated in parentheses):
("walkers" N ((( w ao ) 1) (( k er z ) 0)) )

722
Text and Phonetic Analysis
If the syllables structure is not shown with parentheses, a syllabification rule component can
be invoked. Separate entry lines are used for words with multiple pronunciations and/or
POS, which can be resolved by later processing.
14.10.2.
Text Analysis
Festival has been partially integrated with research on the use of automatic identification of
document and discourse structures. The discourse tagging is done by a separate component,
called SOLE [11]. The tags produced by SOLE indicate features that may have relevance for
pitch contour and phrasing in later stages of synthesis (see Chapter 15). These must be rec-
ognized and partially interpreted at the text analysis phrase. The SOLE tags tell Festival
when the text is comparing or contrasting two objects, when it's referring to old or new in-
formation, when it's using a parenthetical or starting a new paragraph, etc., and Festival will
decide, based on this information, that it needs to pause, to emphasize or deemphasize, to
modify its pitch range, etc.
Additionally, as discussed in Section 14.3, when document creators have knowledge
about the structure or content of documents, they can express the knowledge through an
XML-based synthesis markup language. A document to be spoken is first analyzed for all
such tags, which can indicate alternative pronunciations, semantic or quasi-semantic attrib-
utes (different uses of numbers by context for example), as well as document structures,
such as explicit sentence or paragraph divisions. The kinds of information potentially sup-
plied by the SABLE tags7 is exemplified in Figure 14.7.
<SABLE>
<SPEAKER NAME="male1">
The boy saw the girl in the park <BREAK/> with the telescope.
The boy saw the girl <BREAK/> in the park with the telescope.
Good morning <BREAK /> My name is Stuart, which is spelled
<RATE SPEED="-40%">
<SAYAS MODE="literal">stuart</SAYAS> </RATE>
though some people pronounce it
<PRON SUB="stoo art">stuart</PRON>. My telephone number
is <SAYAS MODE="literal">2787</SAYAS>.
I used to work in <PRON SUB="Buckloo">Buccleuch</PRON> Place,
but no one can pronounce that.
</SPEAKER>
</SABLE>
Figure 14.7 A document fragment augmented with SABLE tags can be processed by the Fes-
tival system [3].
7 Sable and other TTS markup systems are discussed further in Chapter 15.

Case Study: Festival
723
For untagged input, or for input inadequately tagged for text division (<BREAK/>),
sentence breaking is performed by heuristics, similar to Algorithm 14.1, which observe
whitespace, punctuation, and capitalization. A linguistic unit roughly equivalent to a sen-
tence is created by the system for the subsequent stages of processing.
Tokenization is performed by system or user-supplied routines. The basic function is
to recognize potentially speakable items and to strip irrelevant whitespace or other non-
speakable text features. Note that some punctuation is retained as a feature on its nearest
word.
Text normalization is implemented by token-to-word rules, which return a standard or-
thographic form that can, in turn, be input to the phonetic analysis module. The token-to-
word rules have to deal with text normalization issues similar to those presented in Section
14.4. As part of this process, token-type-specific rule sets may be applied to disambiguate
tokens whose pronunciations are highly context dependent. For example, a disambiguation
routine may be required to examine context for deciding whether St. should be realized as
Saint or street. For general English-language phenomena, such as numbers and various
symbols, a standard token-to-word routine is provided. One interesting feature of the Festi-
val system is a utility for helping to automatically construct decision trees to serve text nor-
malization rules, when system integrators can gather some labeled training data.
The linguistic analysis module for the Festival system is mainly a POS analyzer. An n-
gram based trainable POS tagger is used to predict the likelihoods of POS tags from a lim-
ited set given an input sentence. The system uses both a priori probabilities of tags given a
word and n-grams for sequences of tags. The basic underlying technology is similar to the
work in [6] and is described in Section 14.5. When lexical lookup occurs, the predicted most
likely POS tag for a given word is input with the word orthography, as a compound lookup
key. Thus, the POS tag acts as a secondary selection mechanism for the several hundred
words whose pronunciation may differ by POS categories.
14.10.3. Phonetic Analysis
The homograph disambiguation is mainly resolved by POS tags. When lexical lookup oc-
curs, the predicted most likely POS tag for a given word is input with the word orthography
as a compound lookup key. Thus, the POS tag acts as a secondary selection mechanism for
the several hundred words whose pronunciation may differ by POS categories.
If a word fails lexical lookup, LTS rules may be invoked. These rules may be created
by hand, formatted as shown below:
( # [ c h ] C = /k /)
// ch at word start, followed by a consonant, is /k/, e.g.
Chris
Alternatively, LTS rules may be constructed by automatic statistical methods, much as de-
scribed in Section 14.8 above, where CART LTS systems were introduced. Utility routines
are provided to assist in using a system lexicon as a training database for CART rule con-
struction.

724
Text and Phonetic Analysis
In addition, Festival system employs post-lexical rules to handle context coarticula-
tion. Context coarticulation occurs when surrounding words and sounds, as well as speech
style, affect the final form of pronunciation of a particular phoneme. Examples include re-
duction of consonants and vowels, phrase final devoicing, and r-insertion. Some coarticula-
tion rules are provided for these processes, and users may also write additional rules.
14.11. HISTORICAL PERSPECTIVE AND FURTHER READING
Text-to-speech has a long and rich history. You can hear samples and review almost a cen-
tury’s worth of work at the Smithsonian’s Speech Synthesis History Project [19]. A good
source for multilingual samples of various TTS engines is [20].
The most influential single published work on TTS has been From Text to Speech:
The MITalk System [1]. This book describes the MITalk system, from which a large number
of research and commercial systems were derived during the 1980s, including the widely
used DECTalk system [9]. The best compact overall historical survey is Klatt’s Review of
Text-to-Speech Conversion for English [15]. For deeper coverage of more recent architec-
tures, refer to [7]. For an overview of some of the most promising current approaches and
pressing issues in all areas of TTS and synthesis, see [30]. One of the biggest upcoming is-
sues in TTS text processing is the architectural relation of specialized TTS text processing as
opposed to general-purpose natural language or document structure analysis. One of the
most elaborate and interesting TTS-specific architectures is the multilingual text processing
engine described in [27]. This represents a commitment to providing exactly the necessary
and sufficient processing that speech synthesis requires, when a general-purpose language
processor is unavailable.
However, it is expected that natural language and document analysis technology
will become more widespread and important for a variety of other applications. To get an
idea of what capabilities the natural language analysis engines of the future may incorporate,
refer to [12] or [2]. Such generalized engines would serve a variety of clients, including
TTS, speech recognition, information retrieval, machine translation, and other services
which may seem exotic and isolated now but will increasingly share core functionality. This
convergence of NL services can be seen in a primitive form today in Japanese input method
editors (IME), which offload many NL analysis tasks from individual applications, such as
word processors and spreadsheets, and unify these functions in a single common processor
[18].
For letter-to-sound rules, NETalk [25], which describes automatic learning of LTS
processes via neural network, was highly influential. Now, however, most systems have
converged on decision-tree systems similar to those described in [14].
REFERENCES
[1]
Allen, J., M.S. Hunnicutt, and D.H. Klatt, From Text to Speech: the MITalk System, 1987,
Cambridge, UK, University Press.
[2]
Alshawi, H., The Core Language Engine, 1992, Cambridge, US, MIT Press.

Historical Perspective and Further Reading
725
[3]
Black, A.W., P. Taylor, and R. Caley, "The architecture of the Festival Speech Synthesis
System," 3rd ESCA Workshop on Speech Synthesis, 1998, Jenolan Caves, Australia, Univer-
sity of Edinburgh pp. 147-151.
[4]
Boguraev, B. and E.J. Briscoe, Computational Lexicography for Natural Language Process-
ing, 1989, London, Longmans.
[5]
Chomsky, N. and M. Halle, The Sound Patterns of English, 1968, Cambridge, MIT Press.
[6]
Church, K., "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text,"
Proc. of the Second Conf. on Applied Natural Language Processing, 1988, Austin, Texas pp.
136-143.
[7]
Dutoit, T., An Introduction to Text-to-Speech Synthesis, 1997, Kluwer Academic Publishers.
[8]
Francis, W. and H. Kucera, Frequency Analysis of English Usage, 1982, New York, N.Y.,
Houghton Mifflin.
[9]
Hallahan, W.I., "DECtalk Software: Text-to-Speech Technology and Implementation," Digit
Technical Journal, 1995, 7(4), pp. 5-19.
[10]
Higgins,
J.,
Homographs,
2000,
http://www.stir.ac.uk/celt/staff/higdox/wordlist/homogrph.htm.
[11]
Hitzeman, J., et al., "On the Use of Automatically Generated Discourse-Level Information in
a Concept-to-Speech Synthesis System," Proc. of the Int. Conf. on Spoken Language Proc-
essing, 1998, Sydney, Australia pp. 2763-2766.
[12]
Jensen, K., G. Heidorn, and S. Richardson, Natural Language Processing: the PLNLP Ap-
proach, 1993, Boston, US, Kluwer Academic Publishers.
[13]
Jiang, L., H.W. Hon, and X. Huang, "Improvements on a Trainable Letter-to-Sound Con-
verter," Proc. of Eurospeech, 1997, Rhodes, Greece pp. 605-608.
[14]
Jiang, L., H.W. Hon, and X.D. Huang, "(don't use) Improvements on a Trainable Letter-to-
Sound Converter," Eurospeech'97, 1997, Rhodes, Greece.
[15]
Klatt, D., "Review of Text-to-Speech Conversion for English," Journal of Acoustical Society
of America, 1987, 82, pp. 737-793.
[16]
LDC, Linguistic Data Consortium, 2000, http://www.ldc.upenn.edu/ldc/noframe.html.
[17]
Levine, J., Mason, T., Brown, D., Lex and Yacc, 1992, Sebastopol, CA, O'Rielly & Associ-
ates.
[18]
Lunde, K., CJKV Information Processing Chinese, Japanese, Korean & Vietnamese Com-
puting, 1998, O'Reilly.
[19]
Maxey,
H.,
Smithsonian
Speech
Synthesis
History
Project,
2000,
http://www.mindspring.com/~dmaxey/ssshp/.
[20]
Möhler,
G.,
Examples
of
Synthesized
Speech,
1999,
http://www.ims.uni-
stuttgart.de/phonetik/gregor/synthspeech/.
[21]
Nye, P.W., et al., "A Plan for the Fiield Evaluation of an Automated Reading System for the
Blind," IEEE Trans. on Audio and Electroacoustics, 1973, 21, pp. 265-268.
[22]
OMF, CML - Chemical Markup Language, 1999, http://www.xml-cml.org/.
[23]
Richardson, S.D., W.B. Dolan, and L. Vanderwende, "MindNet: Acquiring and Structuring
Semantic Information from Text," ACL'98: 36th Annual Meeting of the Assoc. for Computa-
tional Linguistics and 17th Int. Conf. on Computational Linguistics, 1998 pp. 1098-1102.
[24]
Sable,
The
Draft
Specification
for
Sable
version
0.2,
1998,
http://www.cstr.ed.ac.uk/projects/sable_spec2.html.
[25]
Sejnowski, T.J. and C.R. Rosenberg, NETtalk: A Parallel Network that Learns to Read
Aloud, 1986, Johns Hopkins University.
[26]
Sluijter, A.M.C. and J.M.B. Terken, "Beyond Sentence Prosody: Paragraph Intonation in
Dutch," Phonetica, 1993, 50, pp. 180-188.

726
Text and Phonetic Analysis
[27]
Sproat, R., Multilingual Text-To-Speech Synthesis: The Bell Labs Approach, 1998,
Dordrecht, Kluwer Academic Publishers.
[28]
Sproat, R. and J. Olive, "An Approach to Text-to-Speech Synthesis" in Speech Coding and
Synthesis, W.B. Kleijn and K.K. Paliwal, eds. 1995, Amsterdam, pp. 611-634, Elsevier Sci-
ence.
[29]
Turing, A.M., "Computing Machinery and Intelligence," Mind, 1950, LIX(236), pp. 433-
460.
[30]
van Santen, J., et al., Progress in Speech Synthesis, 1997, New York, Springer-Verlag.
[31]
Van-Santen, J., et al., "Report on the Third ESCA TTS Workshop Evaluation Procedure,"
Third ESCA Workshop on Speech Synthesis, 1998, Sydney, Australia.
[32]
Vitale, T., "An Algorithm for High Accuracy Name Pronunciation by Parametric Speech
Synthesizer," Computational Linguistics, 1991, 17(3), pp. 257-276.
[33]
W3C, Aural Cascading Style Sheets (ACSS), 1997, http://www.w3.org/TR/WD-acss-970328.
[34]
W3C, W3C's Math Home Page, 1998, http://www.w3.org/Math/.
[35]
W3C, Extensible Markup Language (XML), 1999, http://www.w3.org/XML/.
[36]
Wall, L., Christiansen, T., Schwartz, R., Programming Perl, 1996, Sebastopol, CA, O'Rielly
& Associates.

727
C H A P T E R
1 5
ProsodyEquation Section 15
It isn’t what you said; it’s how you said it!
Sheridan pointed out the importance of prosody more than 200 years ago [53]:
Children are taught to read sentences, which they do not understand; and as it
is impossible to lay the emphasis right, without perfectly comprehending the
meaning of what one reads, they get a habit either of reading in a monotone, or
if they attempt to distinguish one word from the rest, as the emphasis falls at
random, the sense is usually perverted, or changed into nonsense.
Prosody is a complex weave of physical, phonetic effects that is being employed to
express attitude, assumptions, and attention as a parallel channel in our daily speech com-
munication. The semantic content of a spoken or written message is referred to as its denota-
tion, while the emotional and attentional effects intended by the speaker or inferred by a
listener are part of the message’s connotation. Prosody has an important supporting role in
guiding a listener’s recovery of the basic messages (denotation) and a starring role in signal-

728
Prosody
ing connotation, or the speaker’s attitude toward the message, toward the listener(s), and
toward the whole communication event.
From the listener’s point of view, prosody consists of systematic perception and re-
covery of a speaker’s intentions based on:
 Pauses: to indicate phrases and to avoid running out of air.
 Pitch: rate of vocal-fold cycling (fundamental frequency or F0) as a function of
time.
 Rate/relative duration: phoneme durations, timing, and rhythm.
 Loudness: relative amplitude/volume.
Pitch is the most expressive of the prosodic phenomena. As we speak, we systemati-
cally vary our fundamental frequency to express our feelings about what we are saying, or to
direct the listener’s attention to especially important aspects of our spoken message. If a
paragraph is spoken on a constant, uniform pitch with no pauses, or with uniform pauses
between words, it sounds highly unnatural.
In some languages, the pitch variation is partly constrained by lexical and syntactic
conventions. For example, Japanese words usually exhibit a sharp pitch fall at a certain
vowel on a consistent, word-specific basis. In Mandarin Chinese [52], word meaning de-
pends crucially on shape and register distinctions among four highly stylized syllable pitch
contour types. This is a grammatical and lexical use of pitch. However, every language, and
especially English, allows some range of pitch variation that can be exploited for emotive
and attentional purposes. While this chapter concentrates primarily on American English,
the use of some prosodic effects to indicate emotion, mood, and attention is probably uni-
versal, even in languages that also make use of pitch for signaling word identity, such as
Chinese. It is tempting to speculate that speakers of some languages use expressive and af-
fective lexical particles and interjections to express some of the same emotive effects for
which American English speakers typically rely on prosody.
We discuss pausing, pitch generation, and duration separately, because it is convenient
to separate them when building systems. Bear in mind, however, that all the prosodic quali-
ties are highly correlated in human speech production. The effect of loudness is not nearly as
important in synthesizing speech as the effect of the other two factors and thus is not dis-
cussed here. In addition, for many concatenative systems this is generally embedded in the
speech segment.
15.1.
THE ROLE OF UNDERSTANDING
To date, most work on prosody for TTS has focused exclusively on the utterance, which is
the literal content of the message. That is, a TTS system learns whatever it can from the iso-
lated, textual representation of a single sentence or phrase to aid in prosodic generation.
Typically a TTS system may rely on word identity, word part-of-speech, punctuation, length
of a sentence or phrase, and other superficial characteristics. As more sophisticated NLP

The Role of Understanding
729
capabilities are deployed for use by TTS systems, deeper properties of an utterance, includ-
ing its document or discourse context, can be taken into account.
Good prosody depends on a speaker or reader’s understanding of the text’s or mes-
sage’s meaning. As noted in [64], the golden rule of the Roman orator Quintilian (c. A.D.
90) states [32] “That to make a man speak well, and pronounce with a right emphasis, he
ought thoroughly to understand all that he says, be fully persuaded of it, and bring himself
to have, those affections which he desires to infuse in others.” This is clearly a tall order for
today’s computers! How important is understanding of the text’s meaning, in generation of
appropriately engaging prosody? Consider a stanza from Lewis Carroll’s nonsense poem
Jabberwocky [10]:
Twas brillig, and the slithy toves
Did gyre and gimble in the wabe;
All mimsy were the borogoves,
And the mome raths outgrabe.
Here, a full interpretation is not possible, owing primarily to lexical uncertainty (our
ignorance of the meaning of words like brillig). However, you can recover a great deal of
information from this passage that aids prosodic rendition. Foremost is probably the metrical
structure of the poetic meter. This imposes a rhythmic constraint on prosodic phrasing (ca-
dence, timing, and pause placement). Second, the function words such as and, the, in, etc.
are interpretable and give rich clues about the general type and direction of action being
specified. They also give us contextual hints about the part-of-speech of the neighboring
nonsense words, which is a first crude step in interpreting those words’ meaning. Third,
punctuation is also important in this case. Using these three properties, with some analogical
guesses about LTS conversions and stress locations in the nonsense words, would allow
most speakers of English to render fairly pleasant and appropriate prosody for this poem.
Can a computer do the same? Where will a computer fall short of a human’s perform-
ance on this task, and why? First, the carrier voice quality of a human reader is generally
superior to synthesized voices. The natural human voice is more pleasant to a listener, all
else being equal. As for the prosody per se, most TTS systems today use a fairly simple
method to derive prosody, based on a distinction between closed-class function words, such
as determiners and prepositions, which are thought to receive lesser emphasis, and open-
ended sets of content words such as nouns like wabe, which are more likely to be accented.
For this nonsense poem, that is essentially what most human readers do. Thus, if accurate
LTS conversions are supplied, including main stress locations, a TTS system with a good
synthetic voice and a reasonable default pitch algorithm of this type could probably render
this stanza fairly well. Again, though the computer does not recognize it explicitly, the con-
strained rhythmic structure of the poem may be assisting.
But listeners to nonsense poems are generally not fully participating in the uncon-
scious interpretive dialog, the attempt on the part of the listener to actively construct useful
meaning from prosodic and message-content cues supplied in good faith by the speaker.
Therefore, judgments of the prosodic quality of uninterpretable nonsense materials must
always be suspect. In ordinary prose, the definition and recovery of meaning remains a slip-
pery question. Consider the passage below [56], which is not metrically structured, has few

730
Prosody
or no true nonsense words, and, yet, was deliberately constructed to be essentially meaning-
less.
In mathematical terms, Derrida's observation relates to the invariance of the
Einstein field equation under nonlinear space-time diffeomorphisms (self-
mappings of the space-time manifold which are infinitely differentiable but not
necessarily analytic). The key point is that this invariance group ‘acts transi-
tively’: this means that any space-time point, if it exists at all, can be trans-
formed into any other. In this way the infinite-dimensional invariance group
erodes the distinction between observer and observed; the pi of Euclid and the
G of Newton, formerly thought to be constant and universal, are now perceived
in their ineluctable historicity; and the putative observer becomes fatally de-
centered, disconnected from any epistemic link to a space-time point that can no
longer be defined by geometry alone.
Should the fact that, say, a professional news broadcaster with no prior knowledge of
the author’s intent could render this supposedly meaningless passage rather well, make us
suspicious of any claims regarding the necessity of deep semantic analysis for high-quality
prosody? Though perhaps meaningless when taken as a whole, once again, the educated
human reader can certainly recover fragments of meaning from this text sufficient to support
reasonable prosody. The morphology and syntax is all standard English, which takes us a
long way. The quality of the announcer’s rendition degrades somewhat under the condition
the computer truly faces, which can be simulated by replacing the content words of a sen-
tence above with content words randomly chosen from Jabberwocky:
In brillig toves, Derrida's wabe gimbles to the bandersnatch of the Tumtum
whiffling raths under frumious slithy diffeomorphisms (borogoves of the mimsy
mome which are beamishly vorpal but not frabjously uffish).
It is likely the human reader can still outperform the computer by reliance on morpho-
logical and syntactic cues, such as the parallelism determining the accent placements in the
contrastive structure “…which ARE…but NOT …” Nevertheless, the degree of understand-
ing of a message’s content that is required for convincing prosodic rendition remains a sub-
tle question. Clearly, the more the machine or human reader knows, the better the prosodic
rendition, but some of the most important knowledge is surprisingly shallow and accessible.
There is no rigorous specification or definition of meaning. The meaning of the rendi-
tion event itself is more significant than the inherent meaning of the text, if any. The mean-
ing of the rendition event is determined primarily by the goals of the speaker and listener(s).
While textual attributes such as metrical conventions, syntax, morphology, lexical seman-
tics, topic, etc. contribute to the construction of both kinds of meaning, the meaning of the
rendition event incorporates more important pragmatic and contextual elements, such as
goals of the communication event, and speaker identity and attitude projection. Thus the
concept-to-speech discussed in Chapter 17 has a much better chance of generating good
prosody, since the content of the sentence is known by the SLU system.

Prosody Generation Schematic
731
15.2.
PROSODY GENERATION SCHEMATIC
Figure 15.1 shows schematically the elements of prosodic generation in TTS, from prag-
matic abstraction to phonetic realization. The input of the prosody module in Figure 15.1 is
parsed text with a phoneme string, and the output specifies the duration of each phoneme
and the pitch contour. One possible output representation of that output prosody is shown in
Figure 15.2 for the sentence The cat sat. Up to four points per phoneme were included in
this example. Often one point per phoneme is more than sufficient, except for words like
john, where two points are needed for the phoneme /ao/ to achieve a natural prosody.
Figure 15.1 Block diagram of a prosody generation system; different prosodic representations
are obtained depending on the speaking style we use.
DH, 24
(0,178);
AH0, 104
;
#;
K,
80
(25,178)
(50,184)
(75,201);
AE1, 152
(0,214)
(25,213)
(50,204)
(75,193);
T,
40
(0,175)
(25,175)
(50,174)
(75,172);
#;
S,
104
(0,171)
(25,172)
(50,180)
(75,189);
AE1, 104
(0,198)
(25,196)
(50,168)
(75,137);
T,
112
(0,120)
(100,120);
#;
Figure 15.2 Enriched prosody representation, where each line contains one phoneme contain-
ing the phoneme identity, the phoneme duration in milliseconds, and a number of prosody
points specifying pitch and possibly volume. Each prosody point is determined by a time point,
expressed as a percentage of the phoneme’s duration, and its corresponding pitch value in Hz.
The symbol # is a word delimiter. For example, the fourth line specifies values for phoneme K,
which lasts 80 ms and has three prosody points: the first is located at 25% of the phoneme du-
ration, i.e., 20 ms into the phoneme, and has a pitch value of 178 Hz. Pitch in this case is speci-
fied in absolute terms in Hz, but it could also be in a logarithmic scale such as quarter-
semitones relative to a base pitch.
Pause insertion and Prosodic phrasing
Enriched Prosodic Representation
Parsed text and Phone string
Duration
F0 Contour
Volume
Speaking Style

732
Prosody
In the next sections we describe the modules of Figure 15.1: the speaking style, sym-
bolic prosody (including pause insertion), duration assignment, and pitch generation in that
order, as usually followed by most TTS systems.
15.3.
SPEAKING STYLE
Prosody depends not only on the linguistic content of a sentence. Different people generate
different prosody for the same sentence. Even the same person generates a different prosody
depending on his or her mood. The speaking style of the voice in Figure 15.1 can impart an
overall tone to a communication. Examples of such global settings include a low register,
voice quality (falsetto, creaky, breathy, etc), narrowed pitch range indicating boredom, de-
pression, or controlled anger, as well as more local effects, such as notable excursion of
pitch, higher or lower than surrounding syllables, for a syllable in a word chosen for special
emphasis. Another example of a global effect is a very fast speaking rate that might signal
excitement, while an example of a local effect would be the typical short, extreme rise in
pitch on the last syllable of a yes-no question in American English.
15.3.1.
Character
Character, as a determining element in prosody, refers primarily to long-term, stable, extral-
inguistic properties of a speaker, such as membership in a group and individual personality.
It also includes sociosyncratic features such as a speaker’s region and economic status, to
the degree that these influence characteristic speech patterns. In addition, idiosyncratic fea-
tures such as gender, age, speech defects, etc. affect speech, and physical status may also be
a background determiner of prosodic character. Finally, character may sometimes include
temporary conditions such as fatigue, inebriation, talking with mouth full, etc. Since many
of these elements have implications for both the prosodic and voice quality of speech output,
they can be very challenging to model jointly in a TTS system. The current state of the art is
insufficient to convincingly render most combinations of the character features listed above.
15.3.2.
Emotion
Temporary emotional conditions such as amusement, anger, contempt, grief, sympathy, sus-
picion, etc. have an effect on prosody. Just as a film director explains the emotional context
of a scene to her actors to motivate their most convincing performance, so TTS systems
need to provide information on the simulated speaker’s state of mind. These are relatively
unstable properties, somewhat independent of character as defined above. That is, one could
imagine a speaker with any combination of social/dialect/gender/age characteristics being in
any of a number of emotional states that have been found to have prosodic correlates, such
as anger, grief, happiness, etc. Emotion in speech is actually an important area for future
research. A large number of high-level factors go into determining emotional effects in
speech. Among these are point of view (can the listener interpret what the speaker is really

Symbolic Prosody
733
feeling or expressing?); spontaneous vs. symbolic (e.g., acted emotion vs. real feeling); cul-
ture-specific vs. universal; basic emotions and compositional emotions that combine basic
feelings and effects; and strength or intensity of emotion. We can draw a few preliminary
conclusions from existing research on emotion in speech [34]:
 Speakers vary in their ability to express emotive meaning vocally in controlled
situations.
 Listeners vary in their ability to recognize and interpret emotions from recorded
speech.
 Some emotions are more readily expressed and identified than others.
 Similar intensity of two emotions can lead to confusing one with the other.
An additional complication in expressing emotion is that the phonetic correlates ap-
pear not to be limited to the major prosodic variables (F0, duration, energy) alone. Besides
these, phonetic effects in the voice such as jitter (inter-pitch-period microvariation), or the
mode of excitation may be important [24]. In a formant synthesizer supported by extremely
sophisticated controls [59], and with sufficient data for automatic learning, such voice ef-
fects might be simulated. In a typical time-domain synthesizer (see Chapter 16), the lower-
level phonetic details are not directly accessible, and only F0, duration, and energy are
available.
Some basic emotions that have been studied in speech include:
 Anger, though well studied in the literature, may be too broad a category for
coherent analysis. One could imagine a threatening kind of anger with a tightly
controlled F0, low in the range and near monotone; while a more overtly expres-
sive type of tantrum could be correlated with a wide, raised pitch range.
 Joy is generally correlated with increase in pitch and pitch range, with increase
in speech rate. Smiling generally raises F0 and formant frequencies and can be
well identified by untrained listeners.
 Sadness generally has normal or lower than normal pitch realized in a narrow
range, with a slow rate and tempo. It may also be characterized by slurred
pronunciation and irregular rhythm.
 Fear is characterized by high pitch in a wide range, variable rate, precise pro-
nunciation, and irregular voicing (perhaps due to disturbed respiratory pattern).
15.4.
SYMBOLIC PROSODY
Abstract or symbolic prosodic structure is the link between the infinite multiplicity of prag-
matic, semantic, and syntactic features of an utterance and the relatively limited F0, phone
durations, energy, and voice quality. The output of the prosody module of Figure 15.2 is a
set of real values of F0 over time and real values for phoneme durations. Symbolic prosody
deals with:

734
Prosody
 Breaking the sentence into prosodic phrases, possibly separated by pauses, and
 Assigning labels, such as emphasis, to different syllables or words within each
prosodic phrase.
Words are normally spoken continuously, unless there are specific linguistic reasons
to signal a discontinuity. The term juncture refers to prosodic phrasing—that is, where do
words cohere, and where do prosodic breaks (pauses and/or special pitch movements) occur.
Juncture effects, expressing the degree of cohesion or discontinuity between adjacent words,
are determined by physiology (running out of breath), phonetics, syntax, semantics, and
pragmatics. The primary phonetic means of signaling juncture are:
 Silence insertion. This is discussed in Section 15.4.1.
 Characteristic pitch movements in the phrase-final syllable. This is discussed in
Section 15.4.4.
 Lengthening of a few phones in the phrase-final syllable. This is discussed in
Section 15.5.
 Irregular voice quality such as vocal fry. This is discussed in Chapter 16.
Figure 15.3 Pitch generation decomposed in symbolic and phonetic prosody.
Symbolic Prosody
Prosody Attributes
Pitch Range
Prominence
Declination
Speaking Style
F0 Contour
Parsed text and phone string
Pauses
Prosodic Phrases
Accent
Tone
F0 Contour Generation

Symbolic Prosody
735
Abstract prosodic structure or annotation typically specifies all the elements in the top
block of the pitch-generation schematic in Figure 15.3, including accents (corresponding
conceptually to heads in standard syntactic structure). The accent types are selected from a
small inventory of tones for American English (e.g., high, low, rising, late-rising, scooped).
The sequence of accent and juncture tones in a given prosodic structure may cohere to yield
tune-like effects that have some holistic semantic interpretation. While we center our de-
scription in an abstract representation called ToBI, we also describe other alternate represen-
tations in Section 15.4.6. Finally, though in principle the prosody attributes module applies
to all prosody variables, it is mostly used for F0 generation in practice, and as such is dis-
cussed in Section 15.6.1.
15.4.1.
Pauses
In a long sentence, speakers normally and naturally pause a number of times. These pauses
have traditionally been thought to correlate with syntactic structure but might more properly
be thought of as markers of information structure [58]. They may also be motivated by
poorly understood stylistic idiosyncrasies of the speaker, or physical constraints. In sponta-
neous speech, there is also the possibility that some pauses serve no linguistic function but
are merely artifacts of hesitation.
In a typical system, the most reliable indicator of pause location is punctuation. After
resolution of abbreviations and special symbols relevant to text normalization (Chapter 14),
the remaining punctuation can be reclassified as essentially prosodic in nature. This includes
periods, commas, exclamation points, parentheses, ellipsis points, colons, dashes, etc. Each
of these can be taken to correspond to a prosodic phrase boundary and can be given a special
pitch movement at its end-point.
In predicting pauses, although you have to consider both their occurrence and their du-
ration, the simple presence or absence of a silence (of greater than 30 ms) is the most sig-
nificant decision, and its exact duration is secondary, based partially on the current rate set-
ting and other extraneous factors.
There are many reasonable places to pause in a long sentence, but a few where it is
critical not to pause. The goal of a TTS system should be to avoid placing pauses anywhere
that might lead to ambiguity, misinterpretation, or complete breakdown of understanding.
Fortunately, most decent writing (apart from email) incorporates punctuation according to
exactly this metric: no need to punctuate after every word, just where it aids interpretation.
Therefore, by simply following punctuation in many writing styles, the TTS system will not
go far wrong.
Consider the opening passage from Edgar Allan Poe’s classic story The Cask of Amon-
tillado (1846) arranged sentence-by-sentence:
1. The thousand injuries of Fortunato I had borne as I best could, but when he
ventured upon insult, I vowed revenge.
2. You, who so well know the nature of my soul, will not suppose, however, that
I gave utterance to a threat.

736
Prosody
3. At length I would be avenged; this was a point definitively settled—but the
very definitiveness with which it was resolved precluded the idea of risk.
If we place prosodic pauses at all and only the punctuation sites, the result is accept-
able to most listeners, and no definite mistakes occur. Some stretches seem a bit too long,
however. Perhaps the second part of sentence 3 could be broken up as follows:
but the very definitiveness with which it was resolved PAUSE precluded the idea
of risk.
While commas are particularly useful in signaling pause breaks, as seen above, pauses
may be optional following comma-delimited listed words (berries, melons, and cheese.),
though the special small pitch rise typical of a minor (nonpause) break is often present.
Cases where placing a boundary in certain locations critically affects interpretation in-
clude tag questions and verb particle constructions (where the verb must not be separated
from its particle), such as:
Why did you hit Joe?
Why did you hit PAUSE Joe?
He distractedly threw out the trash.
(NOT … threw PAUSE out …)
He distractedly gazed PAUSE out the window.
(NOT … out PAUSE the …)
Supplying junctures at the optimal points sometimes requires deep semantic analysis
provided by the module described in Chapter 14. The need for independent methods for
pause insertion has motivated some researchers to assume that no independent source of
natural language analysis is available. The CART discussed in Chapter 4 can be used for
pause assignment [36]. You can use POS categories of words, punctuation, and a few struc-
tural measures, such as overall length of a phrase, and length relative to neighboring phrases
to construct the classification tree. The decision-tree-based system can have correct predic-
tion of 81% for pauses over test sentences with only 4% false prediction rates. As the algo-
rithm proceeds successively left to right through each pair of words, the following questions
can be used:
 Is this a sentence boundary (marked by punctuation)?
 Is the left word a content word and the right word a function word?
 What is the function word type of word to the right? (Certain function words are
more likely to signal a break)
 Is either adjacent word a proper name (capitalized)?
 How many content words have occurred since the previous function word (If > 4
or 5 words, a break more likely)

Symbolic Prosody
737
 Is there a comma at this location?
 What is the current location in the sentence?
 What is the length of current proposed major phrase?
These questions summarize the relevant knowledge, which could be formulated in expert-
system rules, and augmented by high-quality syntactic knowledge if available, or trained
statistically from tagged corpora.
15.4.2.
Prosodic Phrases
An end-of-sentence period may trigger an extreme lowering of pitch, a comma-terminated
prosodic phrase may exhibit a small continuation rise at its end, signaling more to come, etc.
Rules based on these kinds of simple observations are typically found in commercial TTS
systems. Certain pitch -range effects over the entire clause or utterance can also be based on
punctuation—for example, the range in a parenthetical restrictive clause is typically nar-
rower than that of surrounding material, while exclamations may have a heightened range,
or at least higher accent targets throughout.
Prosodic junctures that are clearly signaled by silence (and usually by characteristic
pitch movement as well), also called intonational phrases, are required between utterances
and usually at punctuation boundaries. Prosodic junctures that are not signaled by silence
but rather by characteristic pitch movement only, also called phonological phrases, may be
harder to place with certainty and to evaluate. In fast speech, the silence demarcating fruits
in the sentence ‘We have blueberries, raspberries, gooseberries, and blackberries.’ may dis-
appear, yet a trace of the continuation rise on each ‘berries’ typically remains. These loca-
tions would then still qualify as minor intonation phrases, or phonological phrases.
In analyzing spontaneous speech, the nature and extent of the signaling pitch move-
ment may vary from speaker to speaker. A further consideration for practical TTS systems is
a user’s preferred rate setting: blind people who depend on TTS to access information in a
computer usually prefer a fast rate, at which most sentence-internal pauses should disappear.
To discuss linguistically significant juncture types and pitch movement, it helps to
have a simple standard vocabulary. ToBI (for Tones and Break Indices) [4, 55] is a proposed
standard for transcribing symbolic intonation of American English utterances, though it can
be adapted to other languages as well. The Tones part of ToBI is considered in greater detail
in Section 15.4.4.
The Break Indices part of ToBI specifies an inventory of numbers expressing the
strength of a prosodic juncture. The Break Indices are marked for any utterance on their own
discrete break index tier (or layer of information), with the BI notations aligned in time with
a representation of the speech phonetics and pitch track. On the break index tier, the pro-
sodic association of words in an utterance is shown by labeling the end of each word for the
subjective strength of its association with the next word, on a scale from 0 (strongest per-
ceived conjoining) to 4 (most disjoint), defined as follows:

738
Prosody
 0 for cases of clear phonetic marks of clitic1 groups (phrases with ap-
pended reduced function words); e.g., the medial affricate in contractions
of did you or a flap as in got it.
 1 most phrase-medial word boundaries.
 2 a strong disjuncture marked by a pause or virtual pause, but with no to-
nal marks; i.e., a well-formed tune continues across the juncture. OR, a
disjuncture that is weaker than expected at what is tonally a clear
intermediate or full intonation phrase boundary.
 3 intermediate intonation phrase boundary; i.e., marked by a single phrase
tone affecting the region from the last pitch accent to the boundary.
 4 full intonation phrase boundary; i.e., marked by a final boundary tone af-
ter the last phrase tone.
For example, a typical fluent utterance of the following sentence: Did you want an ex-
ample? might have a 0 between Did and you, indicating palatalization of the /d j/ sequence
across the boundary between these words. Similarly, the break index value between want
and an might again be 0, indicating deletion of /t/ and subsequent flapping of /n/. The re-
maining break index values would probably be 1 between you and want and between an and
example, indicating the presence of a mere word boundary, and 4 at the end of the utterance,
indicating the end of a well-formed intonation phrase. The annotation is thus:
Did-0 you-1 want-0 an-1 example-4?
Without reference to any other knowledge, therefore, a system would place a 1 after
every word, except where utterance-final punctuation motivates placement of 4. Perhaps
comma boundaries would be marked by 4. Need any more be done? A BI of 0 correlates
with any special phone substitution or modification rules for reduction in clitic groups that a
TTS system may attempt. By marking the location of clitic (close association) phonetic re-
duction, such a BI can serve as a trigger for special duration rules that shorten the segments
of the cliticized word. Whatever syntactic/semantic processing was done to propose the cli-
ticization can serve to trigger assignment of 1. The 2 mark is generally more useful for
analysis than for speech generation. You may observe that in the literature on intonation, a 3
break is sometimes referred to as an intermediate phrase break, or a minor phrase break,
while a 4 break is sometimes called an intonational phrase break or a major phrase break.
15.4.3.
Accent
We should briefly clarify use of terms such as stress and accent. Stress generally refers to an
idealized location in an English word that is a potential site for phonetic prominence effects,
such as extruded pitch and/or lengthened duration. This information comes from our stan-
dard lexicon. Thus, the second syllable of the word em-ploy-er is said to have the abstract
1 Pronounced as part of another word, as in ve in I’ve.

Symbolic Prosody
739
property of lexical stress. In an actual utterance, if the word as a whole is sufficiently impor-
tant, phonetic highlighting effects are likely to fall on the lexically stressed syllable:
Acme Industries is the biggest employer in the area.
Accent is the signaling of semantic salience by phonetic means. In American English,
accent is typically realized via extruded pitch (higher or lower than the general trend) and
possibly extended phone duration. Although lexical stress as noted in dictionaries is strictly
an abstract property, these accent-signaling phonetic effects are usually strongest on the
lexically stressed syllable of the word that is singled out for accentuation (e.g., employer).
In the sentence above, the word employer is not specially focused or contrasted, but it
is an important word in the utterance, so its lexically stressed syllable typically receives a
prosodic accent (via pitch/duration phonetic mechanisms), along with the other syllables in
boldface. In cases of special emphasis or contrast, the lexically specified preferred location
of stress in a word may be overridden in utterance accent placement:
I didn’t say employer, I said employee.
It is also possible to override the primary stress of a word with the secondary stress
where a neighboring word is accented. While normally we would say Massachusetts, we
might say Massachusetts legislature [51].
Let’s consider what might make a word accentable in context. A basic rule based on
the use of POS category is to decide accentuation by accenting all and only the content
words. Such rule is used in the baseline F0 generation system of Section 15.6.2. The content
words are major open-class categories such as noun, verb, adjective, adverb, and certain
strong closed-class words such as negatives and some quantifiers. Thus, the function words,
made up of closed-class categories such as prepositions, conjunctions, etc., end up on a kind
of stop list for accentuation, analogous to the stop lists used traditionally in simple document
indexing schemes for information retrieval. This works adequately for many short, isolated
sentences, such as “The cat sat on the mat”, where the words selected for accentuation ap-
pear in boldface. For more complex sentences, appearing in document or dialog context,
such an algorithm will sometimes fail.
How often does the POS class-based stop-list approach fail? Let’s consider a slightly
more elaborate variant on the theme. A model was created using the Lancaster/IBM Spoken
English Corpus (SEC) [3]. This includes a variety of text types, including news, academic
lectures, commentary, and magazine articles. Each word in the corpus has a POS tag auto-
matically assigned by an independent process. The model predicts the probability of a word
of POS having accent status. The probability is computed based on POS class of a sequence
of words in the history in the similar way as n-gram models discussed in Chapter 11. This
simple model performed at or above 90% correct predictions for all text types. As for stress
predictions that were incorrect, we should note that in many cases accents are optional—it is
more a game of avoiding plain wrong predictions than it is of finding optimal ones. Clearly,
however, there are situations that call for greater power than a simple POS-based model can
provide. Even different readings of the exact same text can result in different accents [46].
Consider a simple case where a word or its base form is repeated within a short para-
graph. Such words may have the necessary POS to trigger accentuation, but, since they have

740
Prosody
already been mentioned (perhaps with varying morphological inflection), it can sound
strange to highlight them again with accentuation. They are given or old information the
second time around and may be deaccented. For example, the second occurrence of the noun
‘switch’ below is best not accented:
At the corner of the keyboard are two switches.
The top switch is user-defined.
To achieve this, the TTS system can keep a queue of most recently used words or
normalized base forms (if morphological capability is present), and block accentuation when
the next word has been used recently. The queue should be reset periodically, perhaps at
paragraph boundaries [54].
Of course, the surface form of words, even if reduced to a base form or lemma by
morphology, won’t always capture the deeper semantic relations that govern accentuation.
Consider the following fragment extracted from Roger Rosenblatt’s essay:
Kids today are being neglected by the older generation.
Adults spend hours every day on the StairMaster, trying to
become the youth they should be attending to.
A simple content-word-based accentuation algorithm accents the word youth, because
it is a noun. In context, however, it is not optimal to accent youth, because it is co-referent
with the subject of the fragment, which is kids today. Thus it is, by some metrics, old or
given information, and it had better remain unaccented. The surrounding verbs become,
should, and attending may get extra prominence. The degree to which coreference relations,
from surface identity to deep anaphora, can be exploited depends on the power of the NL
analysis supporting the TTS function.
Other confusions can arise in word accentuation due to English complex nominals,
where lack of, or location of, an accent may be a lexical (static) rather than a syntactic or
dynamic property. Consider:
I invited her to my birthday party, but she said she can’t attend any parties until
her grades improve.
One possible accent structure is indicated in boldface. Here birthday party functions as
a complex nominal, with lexical stress on birthday. The word party should not receive stress
at all, nor should its later stand-alone form parties. Accentuation of improve is optional: it is
a full content word, yet somehow it also feels predictable from the context, so deaccentua-
tion is possible. Some of the complex nominals, like birthday party, are fully fixed and can
be entered into the lexicon as such. Others form small families of binary or n-ary phrases,
which may be detected by local syntactic and lexical analysis. Ambiguous cases such as
moving van or hot dog, which could be either nominals or adjective-noun phrases, may have
to be resolved by user markup or text understanding processes.

Symbolic Prosody
741
Dwight Bolinger opined that Accent is predictable—if you’re a mind reader [6], as-
serting that accentuation algorithms will never achieve perfect performance, because a
writer’s exact intentions cannot be inferred from text alone, and understanding is needed.
However, work in [20] (similar to [3] but incorporating more sophisticated mechanics for
name identification and memory of recent accented items), showed that reasonably straight-
forward procedures, if applied separately and combined intelligently, can yield adequate
results on the accentuation task. This research has also determined that improvement occurs
when the system learns that not all ‘closed-class’ categories are equally likely to be deac-
cented. For example, closed accented items include the negative article, negative modals,
negative do, most nominal pronouns, most nominative and all reflexive pronouns, pre- and
postqualifiers (e.g., quite), prequantifiers (e.g., all), postdeterminers (e.g., next), nominal
adverbials (e.g., here), interjections, particles, most wh-words, plus some prepositions (e.g.,
despite, unlike).
One area of current and future development is the introduction of discourse analysis to
synthesis of dialog. Discourse analysis algorithms attempt to delimit the time within which a
given word/concept can be considered newly introduced, given, old, or reintroduced, and
combined with analysis of segments within discourse and their boundary cues (turn-taking,
digressions, interruptions, summarization, etc.) can supplement algorithms for accent as-
signment. This kind of work improves the naturalness of computer responses in human-
computer dialog, as well as the accentuation in TTS renditions of pure text, when dialog
must be performed (e.g., in reading a novel out loud) [44].
As noted above, user- or application-supplied annotations, based on intimate knowl-
edge of the purpose and content of the speech event, can greatly enhance the quality by off-
loading the task of automatic accent prediction. The /EMPHASIS/ tag described in Section
15.7, with several levels of strength including reduced accent and no accent, is ideally suited
for this purpose.
15.4.4.
Tone
Tones can be understood as labels for perceptually salient levels or movements of F0 on
syllables. Pitch levels and movements on accented and phrase-boundary syllables can ex-
hibit a bewildering diversity, based on the speaker’s characteristics, the nature of the speech
event, and the utterance itself, as discussed above. For modeling purposes, it is useful to
have an inventory of basic, abstract pitch types that could in principle serve as the base in-
ventory for expression of linguistically significant contrasts. Chinese, a lexical tone lan-
guage, is said to have an inventory of 4 lexical tones (5 if neutral tone is included), as shown
in Figure 15.4. Different speakers can realize these tones differently according to their
physiology, mood, utterance content, and the speech occasion. But the variance in the tones’
shapes, and contrasts with one another, remain fairly predictable, within broad limits.

742
Prosody
Figure 15.4 The four Chinese tones.
By analogy, linguists have proposed a relatively small set of tonal primitives for Eng-
lish, which can be used, in isolation or in combination, to specify the gross phonological
typology of linguistically relevant contrasts found in theories of English intonational mean-
ing [17, 28, 39]. A basic set of tonal contrasts has been codified for American English as
part of the Tones and Break Indices (ToBI) system [4, 55]. These categories can be used for
annotation of prosodic training data for machine learning, and also for internal modular con-
trol of F0 generation in a TTS system. The set specifies 2 abstract levels, H(igh) and L(ow),
indicating a relatively higher or lower point in a speaker’s range. The H/L primitive distinc-
tions form the foundations for 2 types of entities: pitch accents, which signal prominence or
culmination; and boundary tones, which signal unit completion, or delimitation. The bound-
ary tones are further divided into phrase types and full boundary types, which would mark
the ends of intonational phrases or whole utterances.
While useful as a link to syntax/semantics, the term accent as defined in Section
15.4.3 is a bit too abstract, even for symbolic prosody. What is required is a way of labeling
linguistically significant types of pitch contrast on accented syllables. Such a system could
serve as the basis for a theory of intonational meaning. The ToBI standard specifies six
types of pitch accents (see Table 15.1) in American English, where the * indicates direct
alignment with an accented syllable, two intermediate phrasal tones (see Table 15.2), and
five boundary tones [4] (see Table 15.3).
In American English one sometimes hears a string of strictly descending pitch accent
levels across a short phrase. When judiciously applied, this downstep effect can be pleas-
antly natural, as in the following sentence:
“I saw a big-H*
fat-!H*
pig-!H* (L-L%)”
A basic rule used in the baseline F0 generation system of Section 15.6.2 consists in
having all the pitch accents realized as H*, associated with the lexically stressed syllable of
accented words. In general, ToBI representations of intonation should be sparse, specifying
only what is linguistically significant. So, words lacking accent should not receive ToBI
pitch accent annotations, and their pitch must be derived via interpolation over neighbors, or
by some other default means. Low excursions can be linguistically significant also, in the
crude sense that if I dip very low in my range on a given word, it may be perceived as
prominent by listeners. L*+H and L+H* are both F0 rises on the accented syllable, but in the
1st
2nd
3rd
4th
t
F0

Symbolic Prosody
743
case of L*+H, the association of the starred tone (L*) with the accented syllable may push
the realization of H off to the following syllable. !H* can be used for successively lowered
high accents, such as might be found on big red car, or tall, dark, and handsome. A ToBI
labeled utterance is shown in Figure 15.5.
Table 15.1 ToBI pitch accent tones.
ToBI tone
Description
Graph
H*
peak accent—a tone target on an accented syllable which is
in the upper part of the speaker's pitch range.
L*
low accent—a tone target on an accented syllable which is
in the lowest part of the speaker's pitch range
L*+H
scooped accent—a low tone target on an accented syllable
which is immediately followed by relatively sharp rise to a
peak in the upper part of the speaker's pitch range.
L*+!H
Scooped downstep accent—a low tone target on an accented
syllable which is immediately followed by relatively flat rise
to a downstep peak.
L+H*
rising peak accent—a high peak target on an accented sylla-
ble which is immediately preceded by a relatively sharp rise
from a valley in the lowest part of the speaker's pitch range.
!H*
Downstep high tone—a clear step down onto an accented
syllable from a high pitch which itself cannot be accounted
for by an H phrasal tone ending the preceding phrase or by a
preceding H pitch accent in the same phrase.
A typical boundary tone is the final lowering, the marked tendency for the final sylla-
ble in all kinds of noninterrogative utterances to be realized on a pitch level close to the ab-
solute bottom of a speaker’s range. The final low (L-L%) may ‘pull down’ the height of
some few accents to its left as well [41].
Table 15.2 ToBI intermediate phrasal tones.
ToBI tone
Description
L-
Phrase accent, which occurs at an intermediate phrase boundary (level 3
and above).
H-
Phrase accent, which occurs at an intermediate phrase boundary (level 3
and above).
Ultimately, abstract linguistic categories should correlate with, or provide labels for
expressing, contrasts in meaning. While the ToBI pitch accent inventory is useful for gener-
ating a variety of Englishlike F0 effects, the distinction between perceptual contrast, func-
tional contrast, and semantic contrast is particularly unclear in the case of prosody [41]. For

744
Prosody
example, whether or not the L*, an alternative method of signaling accentual prominence,
functions in full linguistic contrast to H* is unclear.
Table 15.3 ToBI boundary tones.
ToBI tone
Description
L-L%
For a full intonation phrase with an L phrase accent ending its final intermedi-
ate phrase and a L% boundary tone falling to a point low in the speaker's
range, as in the standard `declarative' contour of American English.
L-H%
For a full intonation phrase with an L phrase accent closing the last intermedi-
ate phrase, followed by an H boundary tone, as in ‘continuation rise.’
H-H%
For an intonation phrase with a final intermediate phrase ending in an H phrase
accent and a subsequent H boundary tone, as in the canonical ‘yes-no ques-
tion’ contour. Note that the H- phrase accent causes ‘upstep’ on the following
boundary tone, so that the H% after H- rises to a very high value.
H- L%
For an intonation phrase in which the H phrase accent of the final intermediate
phrase upsteps the L% to a value in the middle of the speaker's range, produc-
ing a final level plateau.
%H
High initial boundary tones; marks a phrase that begins relatively high in the
speaker's pitch range when not explained by an initial H* or preceding H%.
Figure 15.5 “Marianna made the marmalade”, with an H* accent on Marianna and marma-
lade, and a final L-L% marking the characteristic sentence-final pitch drop. Note the use of 1
for the weak inter-word breaks, and 4 for the sentence-final break (after Beckman [4]).

Symbolic Prosody
745
In addition, we have mentioned that junctures are typically marked with perceptible
pitch movements that are independent of accent. The ToBI specification also allows for
combinations of the H and L primitives that signal phrase, clause, and utterance boundaries.
These are called phrasal tones. The ToBI specification further points out that since intona-
tion phrases are composed of one or more intermediate phrases plus a boundary tone, full
intonation phrase boundaries have two final tones. Both the intermediate phrasal tones and
the boundary tones are shown in Table 15.1.
The symbolic ToBI transcription alone is not sufficient to generate a full F0 contour.
The remaining components are discussed in Section 15.6.
15.4.5.
Tune
Nyaah
nuh nyaah
you
get
nyaah
nyaah,
can’t
me!
- Children’s chant
Some pitch contours appear to be immediately recognizable and emotionally interpret-
able, independent of lexical content, such as the English children’s chant above [40]. Can
this idea of stylized tunes, perhaps decomposable into the tones we examined above, be ap-
plied to the intonation of ordinary speech? In fact, the ideal use of the ToBI pitch accent
labels above would be as primitive elements in holistic prosodic contour descriptions, analo-
gous to the role of phonemes in words. Ultimately, a dictionary of meaningful contours,
described abstractly by ToBI tone symbols to allow for variable phonetic realization, would
constitute a theory of intonational meaning for American English. Ideally, the meanings of
such contours could perhaps be derived compositionally from the meanings of their con-
stituent pitch accent and boundary tones, thus allowing us to dispense with the dictionary
altogether. Contour stylization approaches describe contours holistically and index them for
application on the basis of utterance type, usually based on a naïve syntactic typology, e.g.,
question, declarative, etc.
The holistic representation of contours can perhaps be defended, but the categorizing
of types via syntactic description (usually triggered by punctuation) is questionable. Typi-
cally, use of punctuation as a rule trigger for pitch effects is making certain hidden assump-
tions about the relation between punctuation and syntax, and in turn between syntax and
prosody. An obvious example is question intonation. If you find a question mark at the end
of a sentence, are you justified in applying a final high rise (which might be denoted as H-
H% in ToBI)? First, the intonation of yes-no questions in general differs from that of wh-
questions. Wh-questions usually lack the extreme final upturn of F0 heard in some yes-no
questions:
i. Are you going?
ii. Where are you going?
However, there are cases where an extreme final upturn is acceptable on ii. As [8] puts
it, “It has been emphasized repeatedly … that no intonation is an infallible clue to any sen-

746
Prosody
tence type: any intonation that can occur with a statement, a command, or an exclamation
can also occur with a question.”
Admittedly, there is a rough correspondence between syntactic types and speech acts,2
as shown in Table 15.4. Nevertheless, the correspondence between syntactic types and acts
is not deterministic, and prosody in spontaneous speech is definitely mediated via speech
acts (the pragmatic context and use of an utterance) rather than syntactic types. Thus, it is
difficult to obtain high-quality simulation of spontaneous speech based on linguistic descrip-
tions that do not include speech acts and pragmatics. Likewise, even simulation of prose
reading without due consideration of pragmatics and speech acts, and based solely on syn-
tactic types, is difficult because prose that is read may:
 Include acted dialog
 Have limited occurrence of most types other than declarative, lessening variety
in practice
 Include long or complex sentences, blunting ‘stereotypical’ effects based on ut-
terance type
 Lack text cues as to syntactic type, or analysis grammar may be incomplete
Table 15.4 Relationship between syntactic types and speech acts.
Type
Speech Act
Example
interrogative
Questioning
Is it good?
declarative
Stating
It’s good.
imperative
Commanding
Be good!
exclamatory
Exclaiming
How good it is!
Thus, description of an entire speech event, rather than inferences about text content,
is again the ultimate guarantor of quality. This is why the future of automatic prosody lies
with concept-to-speech systems (see Chapter 17) incorporating explicit pragmatic and se-
mantic context specification to guide message rendition.
For commercial TTS systems that must infer structure from raw text, there are a few
characteristic fragmentary pitch patterns that can be taken as tunes and applied to special
segments of utterances. These include:
 Phone numbers—downstepping with pauses
 List intonation—downstepping with pauses (melons, pears, and eggplants)
 Tag and quotative tag intonation—low rise on tag (Never! he blurted. Come
here, Jonathan.)
2 For a more in-depth coverage of speech acts, consult Chapter 17.

Symbolic Prosody
747
15.4.6.
Prosodic Transcription Systems
ToBI, introduced above, can be used as a notation for transcription of prosodic training data
and as a high-level specification for the symbolic phase of prosodic generation. Alternatives
to ToBI also exist for these purposes, and some of them are amenable to automated prosody
annotation of corpora. Some examples of this type of system are discussed in this section.
PROSPA was developed specially to meet the needs of discourse and conversation
analysis, and it has also influenced the Prosody Group in the European ESPRIT 2589 SAM
(Multilingual Speech Input/Output Assessment, Methodology and Standardization) project
[50]. The system has annotations for general or global trends over long spans shown in
Table 15.5, short, accent-lending pitch movements on particular vowels are transcribed in
Table 15.6, and the pitch shape after the last accent in a () sequence, or tail, is indicated in
Table 15.7.
Table 15.5 Annotations for general or global trends over long spans.
( )
extent of a sequence of cohesive accents
F
globally falling intonation
R
globally rising intonation
H
level intonation on high tone level
M
level intonation on middle tone level
L
level intonation on low tone level
H/F
falling intonation on a globally high tone level
…
sequence of weakly accented or unaccented syllables
Table 15.6 Annotations for accent-lending pitch movements on particular vowels.
+
Upward pitch movement
-
Downward pitch movement
=
level pitch accent
Table 15.7 Annotations for pitch shape after the last accent in a () sequence, or tail.
falling tails
/
rising tails
-
level tails
/`
combinations of tails (rising-falling here)
INTSINT is a coding system of intonation described in [22]. It provides a formal en-
coding of the symbolic or phonologically significant events on a pitch curve. Each such tar-
get point of the stylized curve is coded by a symbol, either as an absolute tone, scaled glob-

748
Prosody
ally with respect to the speakers pitch range, or as a relative tone, defined locally in conjunc-
tion with the neighboring target points. Absolute tones in INSINT are defined according to
the speaker’s pitch range as shown in Table 15.8: Relative tones are notated in INTSINT
with respect to the height of the preceding and following target points.
Table 15.8 The definition of absolute tones in INSINT.
T
top of the speaker's pitch range
M
initial, mid value
B
bottom of the speaker's pitch range
In a transcription, numerical values are retained for all F0 target points. TILT [60] is
one of the most interesting models of prosodic annotation. It can represent a curve in both its
qualitative (ToBI-like) and quantitative (parametrized) aspects. Generally any ‘interesting’
movement (potential pitch accent or boundary tone) in a syllable can be described in terms
of TILT events, and this allows annotation to be done quickly by humans or machines with-
out specific attention to linguistic/functional considerations, which are paramount for ToBI
labeling. The linguistic/functional correlations of TILT events can be linked by subsequent
analysis of the pragmatic, semantic, and syntactic properties of utterances.
Table 15.9 The definition of relative tones in INSINT.
H
target higher than both immediate neighbours
L
target lower than both immediate neighbours
S
target not dif/ferent from preceding target
U
target in a rising sequence
D
target in a falling sequence
The automatic parametrization of a pitch event on a syllable is in terms of:
 starting f0 value (Hz)
 duration
 amplitude of rise (Arise, in Hz)
 amplitude of fall (Afall, in Hz)
 starting point, time aligned with the signal and with the vowel onset
The tone shape, mathematically represented by its tilt, is a value computed directly
from the f0 curve by the following formula:
rise
fall
rise
fall
A
A
tilt
A
A
−
=
+
(15.1)

Duration Assignment
749
Table 15.10 Label scheme for syllables.
sil
Silence
c
Connection
a
Major pitch accent
fb
Falling boundary
rb
Rising boundary
afb
Accent+falling boundary
arb
Accent+rising boundary
m
Minor accent
mfb
Minor accent+falling boundary
mrb
Minor accent+rising boundary
l
Level accent
lrb
Level accent+rising boundary
lfb
Level accent+falling boundary
A likely syllable for tilt analysis in the contour can be automatically detected based on
high energy and relatively extreme F0 values or movements. Human annotators can select
syllables for attention and label their qualities according to Table 15.10.
15.5.
DURATION ASSIGNMENT
Pitch and duration are not entirely independent, and many of the higher-order semantic fac-
tors that determine pitch contours may also influence durational effects. The relation be-
tween duration and pitch events is a complex and subtle area, in which only initial explora-
tion has been done [63]. Nonetheless, most systems often treat duration and pitch independ-
ently because of practical considerations [61].
Numerous factors, including semantics and pragmatic conditions, might ultimately in-
fluence phoneme durations. Some factors that are typically neglected include:
 The issue of speech rate relative to speaker intent, mood, and emotion.
 The use of duration and rhythm to possibly signal document structure above the
level of phrase or sentence (e.g., paragraph).
 The lack of a consistent and coherent practical definition of the phone such that
boundaries can be clearly located for measurement.

750
Prosody
15.5.1.
Rule-Based Methods
Klatt [1] identified a number of first-order perceptually significant effects that have largely
been verified by subsequent research. These effects are summarized in Table 15.11.
Table 15.11. Perceptually significant effects for duration. After Klatt [1].
Lengthening of the final vowel and following consonants in prepausal
syllables.
Shortening of all syllabic segments3 in nonprepausal position.
Shortening of syllabic segments if not in a word final syllable.
Consonants in non-word-initial position are shortened.
Unstressed and secondary stressed phones are shortened.
Emphasized vowels are lengthened.
Vowels may be shortened or lengthened according to phonetic features
of their context.
Consonants may be shortened in clusters.
The rule-based duration-modeling mechanism involves table lookup of minimum and
inherent durations for every phone type. The minimum duration is rate dependent, so all
phones could be globally scaled in their minimum durations for faster or slower rates. The
inherent duration is the raw material for the rules above: it may be stretched or contracted by
a prespecified percentage attached to each rule type above applied in sequence, then it is
finally added back onto the minimum duration to yield a millisecond time for a given phone.
The duration of a phone is expressed as
(
)
min
min
d
d
r d
d
=
+
−
(15.2)
where
min
d
is the minimum duration of the phoneme, d is the average duration of the pho-
neme, and the correction r is given by
1
N
i
i
r
r
=
=∏
(15.3)
for the case of N rules being applied where each rule has a correction
ir . At the very end, a
rule may apply that lengthens vowels when they are preceded by voiceless plosives (/p/, /t/,
/k/). This is also the basis for the additive-multiplicative duration model [49] that has been
widely used in the field.
3 Syllabic segments include vowels and syllabic consonants.

Pitch Generation
751
15.5.2.
CART-Based Durations
A number of generic machine-learning methods have been applied to the duration assign-
ment problem, including CART and linear regression [43, 62]. The voice datasets generally
rely on less than the full set of possible joint duration predictors implied in the rule list of
Table 15.11. It has been shown that a model restricted to the following features and contexts
can compare favorably, in listeners’ perceptions, with durations from natural speech [43]:
 Phone identity
 Primary lexical stress (binary feature)
 Left phone context (1 phone)
 Right phone context (1 phone)
In addition, a single rule of vowel and post-vocalic consonant lengthening (rule 1 in Table
15.11) is applied in prepausal syllables. The restriction of phone context to immediate left
and right neighbors results in a triphone duration model, congruent with the voice triphone
model underlying the basic synthesis in the system [23]. In perceptual testing this simple
triphone duration model yielded judgments nearly identical to those elicited by utterances
with phone durations from natural speech [43]. From this result, you may conjecture that
even the simplified list of first-order factors above may be excessive, and that only the
handful of factors implicit in the triphones themselves, supplemented by a single-phrase
final-syllable coda lengthening rule, is required. This would simplify data collection and
analysis for system construction.
15.6.
PITCH GENERATION
We now describe the issues involved in generating synthetic pitch contours. Pitch, or F0, is
probably the most characteristic of all the prosody dimensions. As discussed in Section 15.8,
the quality of a prosody module is dominated by the quality of its pitch-generation compo-
nent.
Since generating pitch contours is an incredibly complicated problem, pitch generation
is often divided into two levels, with the first level computing the so-called symbolic pros-
ody described in Section 15.4 and the second level generating pitch contours from this sym-
bolic prosody. This division is somewhat arbitrary since, as we shall see below, a number of
important prosodic phenomena do not fall cleanly on one side or the other but seem to in-
volve aspects of both. Often it is useful to add several other attributes of the pitch contour
prior to its generation, which are discussed in Section 15.6.1.
15.6.1.
Attributes of Pitch Contours
A pitch contour is characterized not only by its symbolic prosody but also by several other
attributes such as pitch range, gradient prominence, declination, and microprosody. Some of

752
Prosody
these attributes often cross into the realm of symbolic prosody. These attributes are also
known in the field as phonetic prosody (termed as an analogy to phonology and phonemics).
15.6.1.1.
Pitch Range
Pitch range refers to the high and low limits within which all the accent and boundary tones
must be realized: a floor and ceiling, so to speak, which are typically specified in Hz. This
may be considered in terms of stable, speaker-specific limits as well as in terms of an utter-
ance or passage. For a TTS system, each voice typically has a characteristic pitch range rep-
resenting some average of the pitch extremes over test utterances. This speaker-specific
range can be set as an initial default for the voice or character. These limits may be changed
by an application.
Another sense of pitch range is the actual exploitation of zones within the hard limits
at any point in time for linguistic purposes, having to do with expression of the content or
feeling of the message. Pitch-range variation that is correlated with emotion or other aspects
of the speech event is sometimes called paralinguistic. This linguistic and paralinguistic use
of pitch range includes aspects of both symbolic and phonetic prosody. Since it is quantita-
tive, it certainly is a phonetic property of an utterance’s F0 contour. Furthermore, it seems
that most linguistic contrasts involving pitch accents, boundary tones, etc. can be realized in
any pitch range. These settings can be estimated from natural speech (for research purposes)
by calculating F0 mean and variance over an utterance or set of utterances, or by simply
adopting the minimum and maximum measurements (perhaps the 5th and 95th percentile to
minimize the effect of pitch tracker error).
But, although pitch range is a phonetic property, it can be systematically manipulated
to express states of mind and feeling in ways that other strictly phonetic properties, such as
characteristic formant values, rarely are. Pitch range interacts with all the prosodic attributes
you have examined above, and certain pitch-range settings may be characteristic of particu-
lar styles or utterance events. For example, it is noted [8] that: “we cannot speak of an into-
nation of exclamation … Exclamation draws impartially upon the full repertory of up-down
patterns. What characterizes the class is not shape but range: exclamations reach for the
extreme—usually higher but sometimes lower.” In this sense, then, pitch range cannot be
considered an arbitrary or physiological attribute—it is directly manipulated for communica-
tive purposes.
In prosodic research, distinguishing emotive and iconic use of pitch (analogous to ges-
ture) from strictly linguistic (logical, syntactic, and semantic expression, with arbitrary rela-
tion between signifier and signified) prosodic phenomena has been difficult. Pitch-range
variation seems to straddle emotional, linguistic, and phonetic expression.
A linguistic pitch range may be narrowed or widened, and the zone of current pitch
variation may be placed anywhere within a speaker’s wider, physically determined range.
So, for example, a male speaker might adopt a falsetto speaking style for some purpose, with
his pitch range actually narrowed, but with all pitch variation realized in a high portion of
his overall range, close to his physical limits.
Pitch range is a gradient property, without categorical bounds. It seems to trade off
with other model components: accent, relative prominence, downstep, and declination. For

Pitch Generation
753
example, if our model of prosody incorporates, say, an accent-strength component, but if we
also recognize that pitch range can be manipulated for linguistic purposes, we may have
difficulty determining, in analysis, whether a given accent is at partial strength in a wide
range or at full strength in a reset, narrower range. This analytic uncertainty may be reflected
in the quality of models based on the analysis.
A practical TTS system has to stay within, and make some attempt to maximize the
exploitation of, the current system default or user-specified range. Thus, for general TTS
purposes, the simplest approach is to use about 90% of the user-set or system default range
for general prose reading, most of the time, and use the reserved 10% in special situations,
such as the paragraph initial resets, exclamations, and emphasized words and phrases.
15.6.1.2.
Gradient Prominence
Gradient prominence refers to the relative strength of a given accent position with respect to
its neighbors and the current pitch-range setting. The simplest approach, where every ac-
cented syllable is realized as a H(igh) tone, at uniform strength, within an invariant range,
can sound unnatural. At first glance, the prominence property of accents might appear to be
a phonetic detail, in that it is quantitative, and certainly any single symbolic tonal transcrip-
tion can be realized in a wide variety of relative per-accent prominence settings. However,
the relative height of accents can fundamentally alter the information content of a spoken
message by determining focus, contrast, and emphasis. You would hope that such linguistic
content would be determined by the presence and absence, or perhaps the types (H, L, etc.),
of the symbolic accents themselves. But an accented syllable at a low prominence might be
perceived as unaccented in some contexts, and there is no guaranteed minimum degree of
prominence for accent perception. Furthermore, as noted above, the realization of promi-
nence of an accent is context-sensitive, depending on the current pitch-range setting.
The key knowledge deficit here is a theory of the interpretation of prominence that
would allow designers to make sensible decisions. It appears that relative prominence is
related to the information status of accent-bearing words and is in that sense linguistic, yet
there is no theory of prominence categories that would license any abstraction. For the pre-
sent, many commercial TTS systems adopt a pseudorandom pattern of alternating
stronger/weaker prominence, simply to avoid monotony. If a word is tagged for emphasis, or
if its information status can otherwise be inferred, its prominence can be heightened within
the local range.
In the absence of information on the relative semantic salience of accented words in
the utterance, successive prominence levels are varied in some simple alternating pattern, to
avoid monotony. Rather than limiting the system to a single peak F0 value per accented syl-
lable, several points could be specified, which, when connected by interpolation and
smoothing, could give varied effects within the syllable, such as rising, falling, and scooped
accents.

754
Prosody
15.6.1.3.
Declination
Related to both pitch range and gradient prominence is the long-term downward trend of
accent heights across a typical reading-style, semantically neutral, declarative sentence. This
is called declination. Although this tendency, if overdone, can simply give the effect of a
bored or uncomprehending reader, it is a favorite prosodic effect for TTS systems, because it
is simple to implement and licenses some pitch change across a single sentence. If a system
uses a ‘top line’ as a reference for calculating the height of every accent, the slope of that top
line can simply be declined across the utterance. Otherwise, each accent’s prominence can
be realized as a certain percentage of the height of the preceding one. Declination can be
reset at utterance boundaries, or within an utterance at the boundaries of certain linguistic
structures, such as the beginning of quoted speech. Intrasentence phrase and clause types
that typically narrow the pitch range, such as parentheticals and certain relative clauses, can
be modeled by suspending the declination, or adopting a new declination line for the tempo-
rary narrowed range, then resuming the suspended longer-term trend as the utterance pro-
gresses. Needless to say, declination is not a prominent feature of spontaneous speech and in
any case should not be overdone.
The minor effect of declination should not be confused with the tendency in all kinds
of nonquestioning utterances to end with a very low pitch, close to the bottom of the
speaker’s range. In prosodic research this is called final lowering and is well-attested as a
phenomenon that is independent of declination [29]. The ToBI notation used to specify final
lowering is the complex boundary tone L-L%. In Figure 15.6 we show the declination line
together with the other two downers of intonation: downstep and final lowering described in
Section 15.4.4.
Figure 15.6 The three downers of intonation: the declination line, a downstep (!H*), and the
final lowering (L-L%).
15.6.1.4.
Phonetic F0—Microprosody
Microprosody refers to those aspects of the pitch contour that are unambiguously phonetic
and that often involve some interaction with the speech carrier phones. These may be re-
!H*
L-L%
Declination line
F0 (Hz)
Time (s)

Pitch Generation
755
garded as second-order effects, in the sense that rendering them well cannot compensate for
incorrect accentuation or other mistakes at the symbolic level. Conversely, making no at-
tempt to model these but putting a great deal of care into the semantic basis for determining
accentuation, contrast, focus, emphasis, phrasing, etc. can result in a system of reasonable
quality. Nevertheless, all else being equal, it is advisable to make some attempt to capture
the local phonetic properties of natural pitch contours.
If the strength of accents is controlled semantically, by having equal degrees of focus
on words of differing phonetic makeup, it has been observed that high vowels described in
Chapter 2 carrying H* accents are uniformly higher in the phonetic pitch range than low
vowels with the same kinds of accent. The distinction between high and low vowels corre-
lates with the position of the tongue in articulation (high or low in the mouth). The highest
English vowels by this metric are /iy/ (as in bee) and /uw/ (as in too), while the lowest vowel
is /aa/ as in father. The predictability of F0 under these conditions may relate to the degree
of tension placed on the laryngeal mechanisms by the raised tongue position in the high
vowels as opposed to the low. In any case, this effect, while probably perceptually important
in natural speech, is challenging for a synthesizer. The reason relates again to the issue of
gradient prominence, discussed above. Apart from experimental prompts in the lab, there is
currently no principled way to assign prominence for accent height realization based on ut-
terance content in general TTS. It may therefore be difficult for a listener to correctly factor
pitch accent height that is due to correctly (or incorrectly) assigned gradient prominence
from height variation related to the lower-level phonetic effects of vowel height.
Another phonetic effect is the level F0 in the early portion of a vowel that follows a
voiced obstruent such as /b/, contrasted with the typical fall in F0 following a voiceless ob-
struent such as /p/. This phonetic conditioning effect, of both preceding and following con-
sonants, can be observed most clearly when identical underlying accent types are assigned to
the carrier vowel, and may persist as long as 50 ms or more into the vowel. The exact con-
tribution of the pre-vocalic consonant, the postvocalic consonant, and the underlying accent
type are difficult to untangle, though [54] is a good survey of all research in this area and
adds new experimental results. For commercial synthesizers, this is definitely a second-order
effect and is probably more important for rule-based formant synthesizers (see Chapter 16),
which need to use every possible cue to enforce distinctions among consonants in phoneme
perception, than for strictly intonational synthesis. However, in order to achieve completely
natural prosody in the future, this area will have to be addressed.
Last, and perhaps least, jitter is a variation of individual cycle lengths in pitch-period
measurement, and shimmer is variation in energy values of the individual cycles. These are
distinct concepts, though somewhat correlated. Obviously, this is an influence of glottal
pulse shape and strength on the quality of vowels. Speech with jitter and shimmer over 15%
sounds pathological, but complete regularity in the glottal pulse may sound unnatural. For a
deeper understanding of how these could be controlled, see Chapter 16.
15.6.2.
Baseline F0 Contour Generation
We now examine a simple system that generates F0 contours. Although each stage of an F0
contour algorithm ideally requires a complete natural language and semantic analysis sys-

756
Prosody
tem, in practice a number of rules are often used. The system described here illustrates most
of the important features common to the pitch-generation systems of commercial synthesiz-
ers.
First, let’s consider a natural speech sample and describe what initial information is
needed to characterize it, and how an artificial pitch contour can be synthesized based on the
input analysis. The chosen sample is the utterance “Don’t hit it to Joey!”, an exclamation,
from the ToBI Labeling Guidelines sample utterance set [4]. The natural waveform, aligned
pitch contour, and abstract ToBI labels are shown in Figure 15.7. This utterance is about
1.63 seconds and it has three major ToBI pitch events:
H*
high pitch accent on Don’t
L*+!H
low pitch accent with following downstepped high on Joey
L-L%
low utterance-final boundary tone at the very end of utter-
ance
Figure 15.7 Time waveform, segmentation, TOBI marks, and pitch contour for the utterance
“Don’t hit it to Joey!” spoken by a female speaker (after Beckman [4]).
The input to the F0 contour generator includes:

Pitch Generation
757
 Word segmentation.
 Phone labels within words.
 Durations for phones, in milliseconds.
 Utterance type and/or punctuation information.
 Relative salience of words as determined by grammatical/semantic analysis.
 Current pitch-range settings for voice.
15.6.2.1.
Accent Determination
Although accent determination ideally requires a complete natural language and semantic
analysis system (see Section 15.4.3), in practice a number of rules are often used. The first
rule is: Content word categories of noun, verb, adjective, and adverb are to be accented,
while the function word categories (everything else, such as pronoun, preposition, conjunc-
tion, etc.) are to be left unaccented. Rules can be used to tune this by specifying which POS
is accented or not and in which context.
If we apply that simple metric to the natural sample of Figure 15.7, we see that it does
not account for the accentuation of ‘hit’, which, as a verb, should have been accented. In a
real system perhaps we would have accented it, and this might have resulted in the typical
overaccented quality of synthetic prosody. For this sample discussion, let’s adopt a simpli-
fied version of a rule found in some commercial synthesizers: Monosyllabic common verbs
are left unaccented.
What about “Don’t”? A simplistic view would state that the POS-based policy has
done the right thing, after all “Don’t” can be regarded as a verbal form. However, usually do
is considered an auxiliary verb and is not accented. For now we adopt another rule that says:
In a negative imperative exclamation, determined by presence of a second-person negative
auxiliary form and a terminal exclamation point, the negative term gets accented. The adop-
tion of these corollaries to the simple POS-based accentuation rule accounts for our accent
placement in the present example, but of course it sometimes fails, as does any rigid policy.
So our utterance would now appear (with words selected for accent in upper case) as
“DON’T hit it to JOEY!”
15.6.2.2.
Tone Determination
In the limit, tone determination (see Section 15.4.4) also requires a complete natural lan-
guage and semantic analysis system, but in practice a number of rules are often used. Gener-
ally, in working systems, H* is used for all pitch accent tones, and this is actually very real-
istic, as H* is the most frequent tone in natural speech.
Sometimes complex tones of the type L*+!H are thrown in for a kind of pseudovariety
in TTS. In our sample natural utterance this is the tone that is used, so here we assume that
this is the accent type assigned.
We also need to mark punctuation-adjacent and utterance-final phonemes as rise, con-
tinue, or fall boundaries. In this case we mark it as L-L%.

758
Prosody
15.6.2.3.
Pitch Range
To determine the pitch range, we are going to make use of three lines as a frame within
which all pitches are calculated. The top line and bottom line would presumably be derived
from the current or default pitch-range settings as controlled by an application or user. Here
we set them in accord with the limits of our natural sample. Note that while, for this exam-
ple, the pitch contour is generated within an actual pitch range, it could also be done within
an abstract range of, say, 1–100, which the voice-generation module could map to the cur-
rent actual setting. So we set the top line at T = 375 Hz and the base line at B = 100 Hz.
It is more advantageous to work in a logarithmic scale, because it is more easily ported
from males to females, and because this better represents human prosody. There are 24
semitones in an octave; thus a semitone corresponds to a ratio of
1/ 24
2
a =
. The pitch range
can be expressed in semitones as
(
)
(
)
2
10
24log
/
80log
/
n
T B
T B
=

(15.4)
so that we can express frequencies in semitones as
0
10
0
80log
f
F
=
(15.5)
and its inverse
0 /80
0
10
f
F =
(15.6)
Using Eq. (15.5), the top line is t = 206 and the base line is b = 160. The reference line
is a kind of midline for the range, used in the accent scaling calculations, and is set halfway
between the bottom and top lines, i.e., r = 183, and using Eq. (15.6), R = 194 Hz.
15.6.2.4.
Prominence Determination
The relative prominence of the words (see Section 15.6.1.2) allows the pitch module to scale
the pitch within any given pitch range. Here we assume (arbitrarily) that N = 5 degrees of
abstract relative prominence are sufficient. This means that, e.g., an H* pitch accent with
prominence 5 will be at or near the very top of the current pitch-range setting, while an L*
with the same prominence will be at or near the very bottom of the range. Smaller promi-
nence numbers indicate less salience, placing their pitch events closer to the middle of the
range.
Converting the abstract tone types plus prominence into pitch numbers is more art than
science (but see Section 15.6.4 for a discussion of data-based methods for this process).
Here we assume a simple linear relationship between the tone’s type and relative promi-
nence:
0[ ]
(
)* [ ]/
f i
r
t
r
p i
N
=
+
−
(15.7)
In the limit, prominence determination also requires a complete natural language and
semantic analysis system, but in practice a number of heuristics are often used. One such

Pitch Generation
759
heuristic is: In a negative imperative exclamation, the negative term gets the most emphasis,
leading to a relative prominence assignment of 5 on ‘don’t.’ Using Eqs. (15.7) and (15.6),
the anchor equals the top range of 375 Hz.
Then, since the L*+!H involves a downstepped term, it must by definition be lower
than the preceding H* accent, so we arbitrarily assign it a relative prominence of ‘2’. The
L*+!H is more complex, requiring calculation and placement of two separate anchor points.
For simplicity we are using a single prominence value for complex tones like L*+!H, but we
could also use a value-per-tone approach, at the cost of greater analytical complexity. Using
Eq. (15.7), it corresponds to 192 semitones, and with Eq. (15.6), the value of !H is 251 Hz.
For L*, we use a prominence of –2 (we use negative values for L tones), which, using Eq.
(15.7), results in an anchor of 174, or alternatively 149 Hz.
The L-L% tone is a boundary tone, so it always goes on the final frame of a syllable-
final (in this case, utterance-final) phone. The L-L% in most ToBI systems is not treated as a
two-part complex tone but rather as a super low L% boundary, falling at the very bottom of
the speaker’s pitch range, i.e., prominence of 5, for a few frames. Thus the F0 value of these
anchor point is 100 Hz.
We also need to set anchors for the initial point. The initial anchor is usually set at
some arbitrary but high place within the speaker’s range (perhaps a rule looking at utterance
type can be used). A prominence of 4 can be used, yielding a value of 329 Hz.
Finally we need to determine where to place the anchors within the accented syllable.
Often they are placed in the middle of the vowel. All the anchor points are shown in Figure
15.8.
0
50
100
150
200
250
300
350
400
0
20
40
60
80
100
120
140
160
Time (10ms frames)
F0 (Hz)
Figure 15.8 Anchor points of the F0 contour.
15.6.2.5.
F0 Contour Interpolation
To obtain the full F0 contour we need some kind of interpolation. One way is to interpolate
linearly and follow with a multipoint moving-average window over the resulting (angular)
contour to smooth it out. Another possibility is a higher-order interpolation polynomial. In

760
Prosody
this case a cubic interpolation routine is called, which has the advantage of retaining the
exact anchor points in a smoothed final contour (as opposed to moving average, which
smears the anchor points). In general the choice of interpolation algorithm makes little per-
ceptual difference, as long as no sharp ‘corners’ remain in the contour. In Figure 15.9 the
contour was interpolated fully, without regard to voicing properties of underlying phones. In
the graph, the sections corresponding to unvoiced phones have been replaced with zero, for
ease of comparison to the sample in Figure 15.7. The interpolation can be done in the linear
frequency, as in Figure 15.9, or in the log-frequency.
In order for the interpolation algorithm to operate properly we need to have phone du-
rations so that the anchor points are appropriately spaced apart. In this baseline algorithm,
we followed the algorithm described in Section 15.5.2.
0
50
100
150
200
250
300
350
400
0
50
100
150
Time (10ms frames)
F0 (Hz)
Figure 15.9 F0 contour of Figure 15.8 after cubic interpolation. Sections corresponding to un-
voiced phones have been replaced with zero.
15.6.2.6.
Interface to Synthesis Module
Finally, most synthesizers cannot accept an arbitrary number of pitch controls on a given
phoneme, nor it this necessary. We can downsample the pitch buffer to allocate a few char-
acteristic points per phoneme record, and, if the synthesizer can interpolate pitch, it may be
desirable to skip pitch controls for unvoiced phones altogether. The F0 targets can be placed
at default locations (such as the left edge and middle of each phone), or the placements can
be indicated by percent values on each target, depending on what the synthesizer supports.
This has to be in agreement with the specific interface between the prosody module and the
synthesis module as described in Section 15.2.

Pitch Generation
761
15.6.2.7.
Evaluation and Possible Improvements
In comparing the output contour of Figure 15.9 to the natural one of Figure 15.7, how well
have we done? As a first-order approximation, from visual inspection, it is somewhat similar
to the original. Of course, we have used hand-coded information for the accent property,
accent type, and prominence! However, these choices were reasonable, and could apply
them as defaults to many other utterances. At a minimum, almost exactly the code given
above would apply without change and give a decent contour for a whole ‘family’ of similar
utterances, such as “Don’t hit the ball to Joey!” or “Never give the baseball to Henry!” A
higher-order discourse module would need to determine that ball and baseball are not ac-
cented, however, in order to use the given contour with the same rhetorical effect (presuma-
bly ball and baseball in these cases could be given/understood information).
Something very much like the system described here has been used in most commer-
cially marketed synthesizers throughout the 1990s. This model seems overly simple, even
crude, and presumably it could be substantially augmented, or completely replaced by some-
thing more sophisticated.
However, many weaknesses are apparent also. For one thing, the contour appears very
smooth. The slight jitter of real contours can be easily simulated at a final stage of pitch
buffer processing by modifying +/- 3 or 4 Hz to the final value of each frame. The degree to
which such niceties actually affect listener perceptions depend entirely on the quality of the
synthetic speech and the quality of the pitch-modification algorithms in the synthesizer.
The details of peak placement obviously differ between the natural and synthetic con-
tours. This is partly due to the crude uniform durations used, but in practice synthesizers
incorporate large batteries of rules to decide exactly (for example) which frame of a phone
the H* definition point should appear in—early, middle, late? Sometimes this decision is
based on surrounding phonetic structure, word and syllable structure, and prosodic context.
The degree to which this matters in perception depends partly on synthetic speech quality
overall.
15.6.3.
Parametric F0 Generation
To realize all the prosodic effects discussed above, some systems make almost direct use of
a real speaker’s measured data, via table lookup methods. Other systems use data indirectly,
via parametrized algorithms with generic structure. The simplest systems use an invariant
algorithm that has no particular connection to any single speaker’s data, such as the algo-
rithm described in the baseline F0 generation system of Section 15.6.2. Each of these ap-
proaches has advantages and disadvantages, and none of them has resulted in a system that
fully mimics human prosodic performance to the satisfaction of all listeners. As in other
areas of TTS, researchers have not converged on any single standard family of approaches.
Once we venture beyond the simplest approaches, we find an interesting variety of systems,
based on different assumptions, with differing characteristics. We now discuss a few of the
more representative approaches.

762
Prosody
Even models that make little or no attempt to analyze the internal components of an F0
contour must be indexed somehow. System designers should choose indexing or predictive
factors that are derivable from text analysis, are general enough to cover most prosodic
situations, and are powerful enough to specify high-quality prosody. In practice, most mod-
els’ predictive factors have a rough correspondence to, or are an elaboration of, the elements
of the baseline algorithm of Section 15.6.2. A typical list might include the following:
 Word structure (stress, phones, syllabification)
 Word class and/or POS
 Punctuation and prosodic phrasing
 Local syntactic structure
 Clause and sentence type (declarative, question, exclamation, quote, etc.)
 Externally specified focus and emphasis
 Externally specified speech style, pragmatic style, emotional tone, and speech
act goals
These factors jointly determine an output contour’s characteristics, as listed below.
Ideally, any or all of these may be externally and directly specified, or they may be inferred
or implied within the F0 generation model itself:
 Pitch-range setting
 Gradient, relative prominence on each syllable
 Global declination trend, if any
 Local shape of F0 movement
 Timing of F0 events relative to phone (carrier) structure
The combinatorial complexity of these predictive factors, and the size of the resulting
models, can be serious issues for practical systems that strive for high coverage of prosodic
variability and high-quality output. The possibility of using externally specified symbolic
markups gives the whole system a degree of modularity, in that prosodic annotation can be
specified directly by an authoritative outside source or can be derived automatically by the
symbolic prosody prediction process that precedes F0 contour generation.
Parametric models propose an underlying architecture of prosodic production or per-
ception that constrains the set of possible outputs to conform to universal constants of the
human speech mechanism. Naturally, these models need settings to distinguish different
speakers, different styles, and the specifics of utterances. We describe superposition models
and ToBI Realization models.
15.6.3.1.
Superposition Models
An influential class of parametric models was initiated by the work [35] for Swedish,
which proposed additive superposition of component contours to synthesize a complex final
F0 track. In the version refined and elaborated in [14], the component contours, which may

Pitch Generation
763
all have different strengths and decay characteristics, may correspond to longer-term trends,
such as phrase or utterance declination, as well as shorter-time events, such as pitch accents
on words. The component contours are modeled as the critically damped responses of sec-
ond-order systems to impulse functions for the longer-term, slowly decaying phrasal trends,
and step or rectangular functions of shorter-term accent events. The components so gener-
ated are added and ride a baseline that is speaker specific. The basic ingredients of the sys-
tem, known as Fujisaki’s model [15, 19], are shown in Figure 15.10. The resulting contour is
shown in Figure 15.11. Obviously, similar effects can be generated with linear accent shapes
as described in the simpler model above, with smoothing. However, there are some plausible
claims for the articulatory correlates of the constraints imposed in the second-order damping
and superposition effects of this model [33].
Figure 15.10 Fujisaki pitch model [15]. F0 is a superposition of phrase effects with accent ef-
fects. The phrase mechanism controls things like the declination of a declarative sentence or a
question, whereas the accent mechanism accounts for accents in individual syllables.
Figure 15.11 Composite contour obtained by low-pass filtering the impulses and boxes in the
Fujisaki model of Figure 15.10.
Superposition models of this type can, if supplied with accurate parameters in the form
of time alignments and strengths of the impulses and steps, generate contours closely mim-
icking natural examples. In this respect, the remaining quality gap for general application is
in the parametric knowledge driving the model, not in the model structure per se. These
kinds of models have been particularly successful in replicating the relatively constrained
Japanese reading-style. Whether these models can account straightforwardly for the im-
mense variety of a large range of English speakers and text genre, or whether, on the con-
Phrase
Control
Mechanism
Accent
Control
Mechanism
+
Glottal
Oscillation
Mechanism
F0(t)

764
Prosody
trary, the parameters proliferate and the settings become increasingly arbitrary, remains to
be seen.
15.6.3.2.
ToBI Realization Models
One simple parametric model, which in its inherent structure makes only modest claims for
principled correspondence to perceptual or articulatory reality, is designed to support pro-
sodic symbols such as the Tones and Break Indices (ToBI) system. This model, variants of
which are developed in [2, 54], posits two or three control lines, by reference to which
ToBI-like prosody symbols can be scaled. This provides for some independence between
symbolic and phonetic prosodic subsystems. In the model shown in Figure 15.12, the top
line is an upper limit of the pitch range. It can be slanted down to simulate declination. The
bottom line represents the bottom of the speaker’s range. Pitch accents and boundary tones
(as in ToBI) are scaled from a reference line, which is often midway in the range in a loga-
rithmic scale of the pitch range, as described in the baseline algorithm of 15.6. You can
think of this scaling as operating within a percentage of the current range, rather than abso-
lute values, so a generic method can be applied to any arbitrary pitch-range setting. The
quantitative instantiation of accent height is done at the final stage. The accents and bound-
ary tones consist of one or more points, which can be aligned with the carrier phones; then
interpolation is applied between points, and smoothing is performed over the resulting con-
tour.
Figure 15.12 A typical model of tone scaling with an abstract pitch range.
In Figure 15.12 above, t, r, and b are the top, reference, and baseline pitch values, re-
spectively. They are set from the defaults of the voice character and by user choice. The
base b is considered a physiological constraint of voice. P is the prominence of the accent
and N is the number of prominence steps. Declination can be modeled by slanting the top
and/or reference lines down. The lowered position of the reference in Figure 15.12 reflects
the observation that the realization of H(igh) and L(ow) ToBI abstract tones in a given pitch
range is asymmetric, with a greater portion available for H, while L saturates more quickly.
This is why placing the reference line midway between the top and base lines in a log-
frequency scale automatically takes care of this phenomenon. After target points are located
and scaled according to their gradient prominence specifications, the (hopefully sparse) tar-
gets can be interpolated and the resulting contour smoothed. If the contour is calculated in,
say, 10-ms frames, two pitch targets sampled from the contour vector per phone usually suf-
fice to reproduce the intended prosodic effects faithfully.
b
r
t
*
(
)*
/
H
r
t
r
p N
=
+
−
*
(
)*
/
L
r
t
r
p N
=
−
−

Pitch Generation
765
If a database of recorded utterances with phone labeling and F0 measurements has
been reliably labeled with ToBI pitch annotations, it may be possible to automate the im-
plementation of the ToBI-style parametrized model. This was attempted with some success
in [5], where linear regression was used to predict syllable initial, vowel medial, and syllable
final F0 based on simple, accurately measurable factors such as:
 ToBI accent type of target and neighbor syllables
 ToBI boundary pitch type of target and neighbor syllables
 Break index on target and neighbor syllables
 Lexical stress of target and neighbor syllables
 Number of syllables in phrase
 Target syllable position phrase
 Number and location of stressed syllable(s)
 Number and location of accented syllable(s)
Models of this sort do not incorporate an explicit mechanism (like the scaling direction
from r in Figure 15.12) to distinguish H(igh) from L(ow) tone space, beyond what the data
and its annotations imply.
The work in [47] consists of a ToBI realization model in which the ‘smoothing’
mechanism is built-in as a dynamical system whose parameters are also learnt from data.
This work could be viewed as a stochastic realization of Fujisaki’s superposition model
without the phrase controls and where the accents are given by ToBI labels.
Both the ToBI realization models and the superposition models could, if supplied with
sufficiently accurate measurements of an example contour, reproduce it fairly accurately.
Both models require much detailed knowledge (global and local pitch range; location, type,
and relative strength of accents and boundary tones; degree of declination; etc.) to function
at human-level quality for a given utterance. If a system designer is in possession of a com-
pletely annotated, high-quality database of fully representative prosodic forms for his/her
needs, the question of deployment of the database in a model can be made based on per-
formance tradeoffs, maintenance issues, and other engineering considerations. If, on the
other hand, no such database is available for the given application purpose, extremely high
prosodic quality, including lively yet principled variation, should not be expected to result
simply from choosing the ‘mathematically correct’ model type.
15.6.4.
Corpus-Based F0 Generation
It is possible to have F0 parameters trained from a corpus of natural recordings. The sim-
plest models are the direct models, where an exact match is required. Models that offer more
generalization have a library of F0 contours that are indexed either from features from the
parse tree or from ToBI labels. Finally, there are F0 generation models from a statistical
network such as a neural network or an HMM. In all cases, once the model is set, the pa-
rameters are learned automatically from data.

766
Prosody
15.6.4.1.
Direct Models
The most direct approach of all is to store a single contour from a real speaker’s utterance
corresponding to every possible input utterance that one’s TTS system will ever face. This
seems to limit the ability to freely synthesize any input text. However, this approach can be
viable under certain special conditions and limitations. These controls are so detailed that
they are tedious to write manually. Fortunately, they can be generated automatically by
speech recognition algorithms.
When these controls (transplanted prosody), taken from an authentic digitized utter-
ance, are applied to synthetic voice units, the results can be very convincing, sometimes
nearly as good as the original digitized samples [43]. A system with this capability can mix
predefined utterances having natural-quality prosody, such as greetings, with flexible syn-
thesis capabilities for system response, using a consistent synthetic voice. The transplanted
prosody for the frozen phrases can be derived either from the original voice data donor used
to create the synthetic voice model, or any other speaker, with global adjustment for pitch-
range differences. Another use of the transplanted prosody capability is to compress a spo-
ken message into ASCII (phone labels plus the prosodic controls) for playback, preserving
much of the quality, if not the full individuality, of the original speaker’s recording.
15.6.4.2.
F0 Contours Indexed by Parsed Text
In a more generalized variant of the direct approach, once could imagine collecting and in-
dexing a gigantic database of clauses, phrases, words, or syllables, and then annotating all
units with their salient prosodic features. If the terms of annotation (word structure, POS,
syntactic context, etc.) can be applied to new utterances at runtime, a prosodic description
for the closest matching database unit can be recovered and applied to the input utterance
[23]. The advantages here are that prosodic quality can be made arbitrarily high, by collect-
ing enough exemplars to cover arbitrarily large quantities of input text, and that detailed
analysis of the deeper properties of the prosodic phenomena can be sidestepped. The poten-
tial disadvantages are:
 Data-collection time is long (which affects the capability to create new voices).
 A large amount of runtime storage is required (presumably less important as
technology progresses).
 Database annotation may have to be manual, or if automated, may be of poor
quality.
 The model cannot be easily modified/extended, owing to lack of fundamental
understanding.
 Coverage can never be complete, therefore rulelike generalization, fuzzy match
capability, or back-off, is needed.
 Consistency control for the prosodic attributes (to prevent unit boundary mis-
matches) can be difficult.

Pitch Generation
767
The first two disadvantages are self-explanatory. The difficulty of annotating the data-
base, to form the basis of the indexing and retrieval scheme, depends on the type and depth
of the indexing parameters chosen. Any such scheme requires annotations to identify the
component phones of each unit (syllable, word, or phrase) and their durations. This can usu-
ally be obtained from speech recognition tools [23], which may be independently required to
create a synthetic voice (see Chapter 16). Lexical or word stress attributes can be extracted
from an online dictionary or NLP system, though, as we have seen above, lexical stress is
neither a necessary nor a sufficient condition for predicting pitch accent placement.
If only a very high level of description is sought, based primarily on the pragmatics of
utterance use and some syntactic typology, it may not be necessary to recover a detailed
symbolic pitch analysis. An input text can be described in high-level pragmatic/semantic
terms, and pitch from the nearest matching word or phrase from the database can be applied
with the expectation that its contour is likely correct. For example, such a system might have
multiple prosodic versions of a word that can be used in different pragmatic senses, such as
ok, which could be a question, a statement, an exclamation, a signal of hesitation or uncer-
tainty, etc. The correct version must be selected based on the runtime requirements of the
application.
Direct prosody schemes of this type often preserve the original phone carrier material
of each instance in order to assure optimal match between prosody and spectrum. However,
with DSP techniques enabling arbitrary modifications of waveforms (see Chapter 16), this is
not strictly necessary; the prosodic annotations could stand alone, with phone label annota-
tion only. If more detailed prosodic control is required, such as being aware of the type of
accent, its pitch range, prominence, and other features, the annotation task is much more
difficult.
A straightforward and elegant formulation of the lookup-table direct model approach
can be found in [30]. This system, created for Spanish but generally adaptable, is based on a
large single-speaker recorded database of a variety of sentence types. The sentences are lin-
guistically analyzed, and prosodic structure is hypothesized based on syllables, accent
groups (groups of syllables with one lexical stress), breath groups (groups of accent-groups
between pauses regardless of the duration of the pause), and sentences. Note that these struc-
tures are hypothesized based on the textual material alone, and the speaker will not always
perform accordingly. Pitch (initial, mid, and final F0) and duration data for each spoken
syllable is automatically measured and stored. At runtime, the input sentence is analyzed
using the same set of structural attributes inferred from the text, and a vector of candidate
syllables from the database is constructed with identical, or similar, structural context and
attributes for each successive input syllable position.
The best path through the set of such candidates is selected by minimizing the F0 dis-
tance and disjuncture across the utterance. This is a clean and simple approach to jointly
utilizing both shallow linguistic features and genuine phonetic data (duration and F0), with
dynamic programming to smooth and integrate the output F0 contour. However, as with any
direct modeling approach, it lacks significant generalization capabilities outside the textual
material and speaking style specified during the data collection phase, so a number of sepa-
rate models may have to be constructed.

768
Prosody
The CHATR system of ATR (Japan) [9] takes a similar approach, in that optimal
prosody is selected from prerecorded units, rather than synthesized from a more general
model. The CHATR system consists of a large database of digitized speech, indexed by the
speaker identity, the phoneme sequences of the words, and some pragmatic and semantic
attributes. Selection of phonemes proceeds by choosing the minimal-cost path from among
the similarly indexed database candidate units available for each phoneme or longer segment
of speech to be synthesized. This system achieves high quality by allowing the carrier
phones to bear only their original prosody—pitch modification of the contour is minimized
or eliminated. Of course, the restriction of DSP modification implies a limitation of the gen-
eralizability of the database. This type of approach obtains the prosody implicitly from the
database [31], and as such combines both the prosody and speech synthesis modules. This
type of minimal-cost search is described in more detail in Chapter 16.
15.6.4.3.
F0 Contours Indexed by ToBI
The architecture for a simple and straightforward direct model indexed by ToBI is dia-
grammed in Figure 15.13.
Figure 15.13 A corpus-based prosodic generation model.
This model combines the two often-conflicting goals: it is empirically (corpus) based,
but it permits specification in terms of principled abstract prosodic categories. In this model,
ToBI Symbol
Generator
Tone lattice of possible renderings
Corpus (b):
ToBI
Auto-annotated
Contours
Long-Unit Voice String
with Unmodified Tone
Linguistic Features
Corpus (a):
Linguistic Feature
Auto-annotated
ToBI strings
Contour Candidate List
Statistical Long Voice-Units
Matcher/Extractor

Prosody Markup Languages
769
an utterance to be synthesized is annotated in terms of its linguistic features—perhaps POS,
syntactic structure, word emphasis (based on information structure), etc. The utterance so
characterized is matched against a corpus of actual utterances that are annotated with lin-
guistic features and ToBI symbols, Corpus (a). A fuzzy matching capability based on edit
distance or dynamic programming can be incorporated. If Corpus (a) is sufficiently large
and varied, a number of possible ToBI renderings of either the entire utterance or selected
parts of it may be recovered. At this level of abstraction, the ToBI labels would not encode
relative prominence specifications (strength of pitch extrusions) or pitch range. The set of
such abstractly described contours can then be fuzzy matched into Corpus (b), a set of ToBI
annotated actual contours, and the best set of matches recovered.
Note that while it is possible that Corpus (a) is the exact same base material as Corpus
(b), the model does not enforce an identity, and there may be reasons to desire such flexibil-
ity and modularity, depending on the degree and quality of data and annotation at each level.
Once a number of likely actual contours have been identified, they can be passed to a voice-
unit selection module. The module can select that combination of segmental strings (some-
times called ‘long units,’ since they may combine more than one phoneme) from the voice
database whose original prosody is closest to one of the candidate contours, using root-
mean-square-error, correlation, or other statistical tests. Those units are then concatenated
(with their prosody unmodified) and sent to the application or played out.
A model of this type has some of the disadvantages of direct models as listed above. It
also assumes availability of large and varied databases of both prosodic contours and seg-
mental (phone) long units for concatenation (see Chapter 16). It further requires that these
databases be annotated, either by human labelers or automated systems. However, it has
certain advantages as well:
 It allows for symbolic, phonological coding of prosody.
 It has high-quality natural contours.
 It has high-quality phonetic units, with unmodified pitch.
 Its modular architecture can work with user-supplied prosodic symbols.
It also allows the immediate, temporary use of data that is collected for deeper analy-
sis, in the hope of eventual construction of smaller, parametrized models. The model of
Figure 15.13 is a generalization of the prosody system described in [23].
15.7.
PROSODY MARKUP LANGUAGES
Chapter 14 discussed generalized document markup schemes for text analysis. Most TTS
engines provide simple text tags and application programming interface controls that allow
at least rudimentary hints to be passed along from an application to a TTS engine. We ex-
pect to see more sophisticated speech-specific annotation systems, which eventually incor-
porate current research on the use of semantically structured inputs to synthesizers, some-
times called concept-to-speech systems. A standard set of prosodic annotation tags would
likely include tags for insertion of silence pause, emotion, pitch baseline and range, speed in

770
Prosody
words-per-minute, and volume. This would be in addition to general tags for specifying the
language of origin if not predictable, character of the voice, and text normalization context
such as address, date, e-mail, etc.
For prosodic processing, text may be marked with tags that have scope, in the general
fashion of XML. Some examples of the form and function of a few common TTS tags for
prosodic processing, based loosely on the proposals of [65], are introduced below. Other
tags can be added by intermediate subcomponents to indicate variables such as accents and
tones. This extension allows for even finer research and prosody models.
 Pause or Break commands might accept either an absolute duration of silence
in milliseconds, or, as in the W3C proposal, a mnemonic describing the relative
salience of the pause (Large, Medium, Small, None), or a prosodic punctuation
symbol from the set ‘,’, ‘.’, ‘?’, ‘!’, ‘…’, etc., which not only indicates a pause
insertion but also influences the typical pitch contour of the phone segments en-
tering and leaving the pause area. For example, specifying ‘,’ as the argument of
a Pause command might determine the use of a continuation rise on the phones
immediately preceding the pause, indicating incompletion or listing intonation.
 Rate controls the speed of output. The usual measurement is words per minute,
which can be a bit vague, since words are of very different durations. However,
this metric is familiar to many TTS users and works reasonably well in practice.
For non-IndoEuropean languages, different metrics must be contemplated. Some
power listeners who use a TTS system routinely can tolerate (in fact, demand)
rates of over 300 words per minute, while 150 or fewer might be all that a nov-
ice listener could expect to reliably comprehend.
 Baseline Pitch specifies the desired average pitch: a level around which, or up
from which, pitch is to fluctuate.
 Pitch Range specifies within what bounds around the baseline pitch level line
the pitch is to fluctuate.
 Pitch commands can override the system’s default prosody, giving an applica-
tion or document author greater control. Generally, TTS engines require some
freedom to express their typical pitch patterns within the broad limits specified
by a Pitch markup.
 Emphasis emphasizes or deemphasizes one or more words, signaling their rela-
tive importance in an utterance. Its scope could be indicated by XML style. Con-
trol over emphasis brings up a number of interesting considerations. For one
thing, it may be desirable to have degrees of emphasis. The notion of gradient
prominence—the apparent fact that there are no categorical constraints on levels
of relative emphasis or accentuation—has been a perpetual thorn in the side for
prosodic researchers. This means that in principle any positive real number
could be used as an argument to this tag. In practice, most TTS engines would
artificially constrain the range of emphasis to a smaller set of integers, or per-
haps use semantic labels, such as strong, moderate, weak, none for degree of
emphasis. Emphasis may be realized with multiple phonetic cues. Thus, if the
user or application has, for example, set the pitch range very narrowly, the em-

Prosody Evaluation
771
phasis effect may be achieved by manipulation of segmental duration or even
relative amplitude. The implementation of emphasis by a TTS engine for a given
word may involve manipulation (e.g., de-accentuation) of surrounding words as
much as it involves heightening the pitch or volume, or stretching the phone du-
rations, of the target word itself. In most cases the main phonetic and perceptual
effect of emphasis or accentuation is heard on the lexically main stressed sylla-
ble of the word, but this can be violated under special conditions of semantic fo-
cus, e.g., “I didn’t say employer, I said employee.” This would require a more
powerful emphasis specification than is currently provided in most TTS sys-
tems, but alternatively it could be specified using phone input commands such
as “The <emp>truth</emp>, the <emp>whole truth</emp>, and nothing
<emp>but</emp> the truth.” For more control, future TTS systems may sup-
port degree emphasis: “… nothing <emp level=“strong”>but</emp> the truth”
or even deemphasis: “… nothing <emp level= “reduced”>but</emp> the
truth”. Emphasis is related to prominence, discussed in Section 15.6.1.2.
15.8.
PROSODY EVALUATION
Evaluation of a complete TTS system is discussed in Chapter 16. We limit ourselves here to
evaluating the prosody component. We assume that the text analysis module has done a per-
fect job, and that the synthesis module does perfect job, which cannot be done in general, so
that approximations need to be made.
Evaluation can be done automatically or by using listening tests with human subjects.
In both cases it’s useful to start with some natural recordings with their associated text. We
start by replacing the natural prosody with the system’s synthetic prosody. In the case of
automatic evaluation, we can compare the enriched prosodic representations described in
Section 15.2 for both the natural recording and the synthetic prosody. The reference en-
riched prosodic representation can be obtained either manually or by using a pitch tracker
and a speech recognizer.
Automated testing of prosody involves the following:
 Duration. It can be performed by measuring the average squared difference be-
tween each phone’s actual duration in a real utterance and the duration predicted
by the system.
 Pitch contours. It can be performed by using standard statistical measures over a
system contour and a natural one. When this is done, duration and phoneme
identity should be completely controlled. Measures such as root-mean-square er-
ror indicate the characteristic divergence between two contours, while correla-
tion indicates the similarity in shape across difference pitch ranges. In general,
RMSE scores of 15 Hz or less for male speech over a long sentence, with corre-
lation of .8 or above, indicate quality that may be close to perceptually identical
to the natural reference utterance. In general, such exactness of match is useful

772
Prosody
only during model training and testing and cannot be expected during training
on entirely new utterances from random text.
Listening tests can be performed to evaluate a prosody module. This involves subjects
listening to the natural recording and the synthetic speech, or to synthetic speech generated
with two different prosody modules. This can lead to a more precise evaluation, as humans
are the final consumer of this technology. However, such tests are more expensive to carry
out. Furthermore, this method results in testing both the prosody module and the synthesis
components together. To avoid this, the original waveform can be modified to have the syn-
thetic prosody using the signal processing techniques described in Chapter 16. Since such
techniques introduce some distortions, this measuring method is still somewhat biased. In
practice, it has been shown that its effect is much smaller than that of the synthetic prosody
[43].
It is shown that synthesizing pitch is more difficult than duration [43]. Subjects scored
significantly higher utterances that had natural pitch and synthetic duration than utterances
with synthetic duration and natural pitch. In fact, using only synthetic duration had a score
quite close to that of the original recording. While duration modeling is not a solved prob-
lem, this indicates that generation of pitch contours is more difficult.
15.9.
HISTORICAL PERSPECTIVE AND FURTHER READING
Prosodic methods have been incorporated within the traditional fields of rhetoric and elocu-
tion for centuries. In ancient Greece, at the time of Plato, written documentation in support
of claims in legal disputes was rare. To help litigants plead their cases persuasively, system-
atic programs of rhetorical instruction were established, which included both content and
form of verbal argument. This ‘prescriptive’ tradition of systematic instruction in verbal
style uncovered issues that remain central to the descriptively oriented prosodic research of
today. A masterful and entertaining discussion of this tradition and its possible relevance to
the task of teaching computers to plead a case can be found in [64]. The Greeks were par-
ticularly concerned about an issue that, as usual, is still important for us today: the separa-
tion of rhetorical effectiveness from considerations of truth. If you are interested in this, you
cannot do better than to begin with Plato’s dialog Phaedrus [42].
Modern linguists have also considered a related, but more narrowly formulated ques-
tion: Should prosody be treated as a logical, categorical analog to phonological and syntactic
processes? The best discussion of these issues from a prosodic (as opposed to strictly neuro-
logical) point of view is found in [7, 8]. If you are interested in the neurological side, you
can begin with [13]. For emotional modeling, before slogging through the scattered and
somewhat disjointed papers on emotion in speech that have appeared sporadically for years,
the reader would be well advised to get a basic grounding in some of the issues related to
emotion in computation, as treated in [38].
Going in the other direction, there are many subtle interactions in the phonetics of
prosody: the various muscles, their joint possibilities of operation in phonation and articula-
tion, as well as the acoustics properties of the vocal chambers. For an excellent introduction
to the whole field, start with [27].

Historical Perspective and Further Reading
773
The most complete and accessible overview of modern prosodic analysis as embedded
in mainstream linguistic theory is Ladd’s Intonational Phonology [26], which covers the
precursors, current standard practice, and remaining unsolved issues of the highly influential
auto segmental theory of intonational phonology, from which ToBI has arisen. ToBI was
devised by speech scientists who wanted a prosodic transcription standard to enable sharing
of databases [4]. For most practical purposes, the ToBI definitions are sufficient as a starting
point for both research and applications, but for those who prefer alternative annotation sys-
tems aligned with the British tradition, conversion guidelines have been attempted [45]. An-
other major phonological approach to English intonation has been the British school de-
scribed in [11]. Bridging the two is IViE, a labeling system that is philosophically aligned
with ToBI but may be more appropriate for non-U.S. dialects of English [18].
The first prosodic synthesis by rule was developed by Ignatius Mattingly in 1968 in
Haskins Laboratories. In 1971, Fujisaki [15] developed his superposition model that has
been used for many years. The development of the ToBI in 1992 [55] marked a milestone in
automatic prosody generation. The application of statistical techniques, such as CART, for
phoneme durations during the 1990s constituted a significant step beyond the rule-based
methods. Finally, the development of the CHATR system in the mid-1990s ignited interest
in the indexing of massive databases. It is possible to attempt smoothing over both the index
space and the resulting prosodic data tracks by means of generalized learning methods, such
as neural nets or HMMs. These models have built-in generalization over unseen inputs, and
built-in smoothing over the concatenated outputs of unit selection. The network described in
[57] codes every syllable in a training database in terms of perceived prominence (human
judged), a number from 1 to 31, as well as the syllable’s phonemes, rising/falling boundary
type for phrase-edge syllables, and distance from preceding and following phrase bounda-
ries, for all syllables. When tested with reasonably simple text material of similar type, these
networks yielded high-quality simulations.
A potential research area for future generalizations of this system is to increase the de-
gree and accuracy of automation in labeling the training features of the recordings, such as
perceived prominence. Another area is to either expand the inventory of model types, or to
determine adequate generalization mechanisms. By training HMMs on accented syllables of
differing phonetic structure, some of this fine alignment information can be automatically
captured [16]. Another approach consists in generating pitch contours directly from a hidden
Markov model, which is run in generation mode [66].
Recently, just as in speech synthesis for voice, there has been a realization that the di-
rect and parametric prosodic models have a great deal in common. Direct models require
huge databases of indexed exemplars for unmediated concatenation and playback of con-
tours, in addition to generalized back-off methods, while parametric models are generalized
for any input, but also require phonetic databases of sufficient variety to support statistical
learning of parameter settings for high quality. We can, therefore, expect to see increasing
numbers of hybrid systems. One such system is described in [47], which could be viewed as
a stochastic realization of Fujisaki’s superposition model without the phrase controls, where
the accents are given by ToBI labels and the smoothing is done by means of a dynamical
system.

774
Prosody
While this chapter has focused on U.S. English, many similar issues arise in prosodic
modeling of other languages. An excellent survey of the prosodic systems of every major
European language, as well as Arabic and several major East Asian languages, can be found
in [21].
Though not explicitly covered in this chapter, analysis of prosody for speech recogni-
tion is a small but growing area of study. Anyone who has digested this chapter should be
prepared to approach the more specialized work of [25, 37] and the speech recognition pros-
ody studies collected in [48]. Those with a psycholinguistic bent can begin with [12].
REFERENCES
[1]
Allen, J., M.S. Hunnicutt, and D.H. Klatt, From Text to Speech: the MITalk System, 1987,
Cambridge, UK, University Press.
[2]
Anderson, M.D., J.B. Pierrehumbert, and M.Y. Liberman, "Synthesis by Rule of English
Intonation Patterns," Proc. of Int. Conf. on Acoustics, Speech and Signal Processing, 1984
pp. 2.8.1-2.8.4.
[3]
Arnfield, S., "Word Class Driven Synthesis of Prosodic Annotations," Proc. of the Int. Conf.
on Spoken Language Processing, 1996, Philadelphia, PA pp. 1978-1981.
[4]
Beckman, M.E. and G.M. Ayers, Guidelines for ToBI Labelling, 1994, http://www.ling.ohio-
state.edu/phonetics/ToBI/main.html.
[5]
Black, A. and A. Hunt, "Generating FO Contours from ToBI labels using Linear Regres-
sion," Proc. of the Int. Conf. on Spoken Language Processing, 1996 pp. 1385-1388.
[6]
Bolinger, D., "Accent is predictable (if you're a mind-reader)," Language, 1972, 48, pp. 633-
44.
[7]
Bolinger, D., Intonation and its parts, 1986, Stanford, Stanford University Press.
[8]
Bolinger, D., Intonation and its uses, 1989, Stanford, Stanford University Press.
[9]
Campbell, N., "CHATR: A High-Definition Speech Re-sequencing System," ASA/ASJ Joint
Meeting, 1996, Honolulu, Hawaii pp. 1223-1228.
[10]
Carroll, L., Alice in Wonderland, Unabridged ed., 1997, Penguin USA.
[11]
Crystal, D., Prosodic Systems and Intonation in English, 1969, Cambridge University Press.
[12]
Crystal, D., "Prosody and Parsing" , P. Warren, Editor 1996, Lawrence Erlbaum Associates.
[13]
Emmorey, K., "The Neurological Substrates for Prosodic Aspects of Speech," Brain and
Language, 1987, 30, pp. 305-320.
[14]
Fujisaki, H., "Prosody, Models, and Spontaneous Speech" in Computing Prosody, Y. Sagi-
saka, Nick Campbell, Norio Higuchi, Editor 1997, New York, Springer.
[15]
Fujisaki, H. and H. Sudo, "A Generative Model of the Prosody of Connected Speech in
Japanese," Annual Report of Eng. Research Institute, 1971, 30, pp. 75-80.
[16]
Fukada, T., et al., "A Study on Pitch Pattern Generation Using HMM-based Statistical In-
formation," Proc. Int. Conf. on Spoken Language Processing, 1994, Yokohama, Japan pp.
723-726.
[17]
Goldsmith, J., "English as a Tone Language" in Phonology in the 1980's, D. Goyvaerts, Edi-
tor 1980, Ghent, Story-Scientia.
[18]
Grabe, E., F. Nolan, and K. Farrar, "IViE - a Comparative Transcription System for Intona-
tional Variation in English," Proc. of the Int. Conf. on Spoken Language Processing, 1998,
Sydney, Australia.
[19]
Hirose, H. and H. Fujisaki, "Analysis and Synthesis of Voice Fundamental Frequency Con-
tours of Spoken Sentences," IEEE Int. Conf. on Acoustics, Speech and Signal Processing,
1982 pp. 950-953.

Historical Perspective and Further Reading
775
[20]
Hirschberg, J., "Pitch Accent in Context: Predicting Intonational Prominence from Text,"
Artificial Intelligence, 1993, 63, pp. 305-340.
[21]
Hirst, D., A.D. Cristo, and A. Cruttenden, Intonation Systems: A Survey of Twenty Lan-
guages, 1998, Cambridge, U.K., Cambridge University Press.
[22]
Hirst, D.J., "The Symbolic Coding of Fundamental Frequency Curves: from Acoustics to
Phonology," Proc. of Int. Symposium on Prosody, 1994, Yokohama, Japan.
[23]
Huang, X., et al., "Whistler: A Trainable Text-to-Speech System," Int. Conf. on Spoken
Language Processing, 1996, Philadephia, PA pp. 2387-2390.
[24]
Klasmeyer, G. and W.F. Sendlmeier, "The Classification of Different Phonation Types in
Emotional and Neutral Speech," Forensic Linguistics, 1997, 4(1), pp. 104-125.
[25]
Kompe, R., Prosody in Speech Understanding Systems, 1997, Berlin, Springer.
[26]
Ladd, R.D., Intonational Phonology, Cambridge Studies in Linguistics, 1996, Cambridge,
Cambridge University Press.
[27]
Ladefoged, P., A Course in Phonetics, 1993, Harcourt Brace Johanovich.
[28]
Liberman, M., The Intonation System of English, PhD Thesis in Linguistics and Philosophy
1975, MIT, Cambridge.
[29]
Liberman, M. and J. Pierrehumbert, "Intonational Invariance under Changes in Pitch Range
and Length" in Language and Sound Structure, M. Aronoff, Oerhle, R., Editor 1984, Cam-
bridge, MA, pp. 157-233, MIT Press.
[30]
Lopez-Gonzalo, E., and J.M. Rodriguez-Garcia, "Statistical Methods in Data-Driven Model-
ing of Spanish Prosody for Text to Speech," in Proc. ICSLP 1996, 1996, pp. 1373-1376.
[31]
Malfrere, F., T. Dutoit, and P. Mertens, "Automatic Prosody Generation Using Supra-
Segmental Unit Selection," Third ESCA/COCOSCA Workshop on Speech Synthesis, 1998,
Jenolan Caves, Australia pp. 323-328.
[32]
Mason, J., An Essay on Elocution, 1st ed, 1748, London.
[33]
Möbius, B., "Analysis and Synthesis of German F0 Contours by Means of Fujisaki's Model,"
Speech Communication, 1993, 13(53-61).
[34]
Murray, I. and J. Arnott, "Toward the Simulation of Emotion in Synthetic Speech: A Review
of the Literature on Human Vocal Emotion," Journal Acoustical Society of America, 1993,
93(2), pp. 1097-1108.
[35]
Öhman, S., Word and Sentence Intonation: A Quantitative Model, 1967, KTH, pp. 20-54.
[36]
Ostendorf, M., and N. Veilleux, "A Hierarchical Stochastic Model for Automatic Prediction
of Prosodic Boundary Location," Computational Linguistics, 1994, 20(1), pp. 27-54.
[37]
Ostendorf, M., "Linking Speech Recognition and Language Processing Through Prosody,"
Journal for the Integrated Study of Artificial Intelligence, Cognitive Science and Applied
Epistemology, 1998, 15(3), pp. 279-303.
[38]
Picard, R.W., Affective Computing, 1997, MIT Press.
[39]
Pierrehumbert, J., The Phonology and Phonetics of English Intonation, PhD Thesis in Lin-
guistics and Philosophy 1980, MIT, Cambridge, MA.
[40]
Pierrehumbert, J., and M. Beckman, Japanese Tone Structure, 1988, Cambridge, MA, MIT
Press.
[41]
Pierrehumbert, J. and J. Hirschberg, "The Meaning of Intonational Contours in the Interpre-
tation of Discourse" in Intentions in Communication, P.R. Cohen, J. Morgan, and M. E. Pol-
lack, Editor 1990, Cambridge, MA, MIT Press.
[42]
Plato, The Symposium and The Phaedrus: Plato's Erotic Dialogues, 1994, State University
of New York Press.
[43]
Plumpe, M. and S. Meredith, "Which is More Important in a Concatenative Text-to-Speech
System: Pitch, Duration, or Spectral Discontinuity," Third ESCA/COCOSDA Int. Workshop
on Speech Synthesis, 1998, Jenolan Caves, Australia pp. 231-235.

776
Prosody
[44]
Prevost, S. and M. Steedman, "Specifying Intonation from Context for Speech Synthesis,"
Speech Communication, 1994, 15, pp. 139-153.
[45]
Roach, P., "Conversion between Prosodic Transcription Systems: 'Standard British' and
ToBI," Speech Communication, 1994, 15, pp. 91-97.
[46]
Ross, K. and M. Ostendorf, "Prediction of Abstract Prosodic Labels for Speech Synthesis,"
Computer, Speech and Language, 1996, 10, pp. 155-185.
[47]
Ross, K. and M. Ostendorf, "A Dynamical System Model for Generating Fundamental Fre-
quency for Speech Synthesis," IEEE Trans. on Speech and Audio Processing, 1999, 7(3), pp.
295-309.
[48]
Sagisaka, Y., W.N. Campbell, and N. Higuchi, Computing Prosody, 1997, Springer-Verlag.
[49]
Santen, J.V., "Contextual Effects on Vowel Duration," Speech Communication, 1992, 11(6),
pp. 513-546.
[50]
Selting, M., Prosodie im Gespräch, 1995, Max Niemeyer Verlag.
[51]
Shattuck-Hufnagel, S. and M. Ostendorf, "Stress Shift and Early Pitch Accent Placement in
Lexical Items in American English," Journal of Phonetics, 1994, 22, pp. 357-388.
[52]
Shen, X.-n.S., The Prosody of Mandarin Chinese, 1990, Berkeley, University of California
Press.
[53]
Sheridan, T., Lectures on the Art of Reading, 3rd ed, 1787, London, Dodsley.
[54]
Silverman, K., The Structure and Processing of Fundamental Frequency Contours, Ph.D.
Thesis 1987, University of Cambridge, Cambridge, UK.
[55]
Silverman, K., "ToBI: A Standard for Labeling English Prosody," Int. Conf. on Spoken Lan-
guage Processing, 1992, Banff, Canada pp. 867-870.
[56]
Sokal, A.D., "Transgressing the Boundaries: Towards a Transformative Hermeneutics of
Quantum Gravity," Social Text, 1996, 46/47, pp. 217-252.
[57]
Sonntag, G., T. Portele, and B. Heuft, "Prosody Generation with a Neural Network: Weigh-
ing the Importance of Input Parameters," Proc. Int. Conf. on Acoustics, Speech and Signal
Processing, 1997 pp. 930-934.
[58]
Steedman, M., "Information Structure and the Syntax-Phonology Interface," Linguistic In-
quiry, 2000.
[59]
Stevens, K.N., "Control Parameters for Synthesis by Rule," Proc. of the ESCA Tutorial Day
on Speech Synthesis, 1990 pp. 27-37.
[60]
Taylor, P.A., "The Tilt Intonation Model," Proc. Int. Conf. on Spoken Language Processing,
1998, Sydney, Australia.
[61]
van Santen, J., "Assignment of Segmental Duration in Text-to-Speech Synthesis," Computer
Speech and Language, 1994, 8, pp. 95-128.
[62]
van Santen, J., "Segmental Duration and Speech Timing" in Computing Prosody, Y. Sagi-
saka, Nick Campbell, and Norio Higuchi, Editor 1997, New York, pp. 225-250, Springer.
[63]
van Santen, J. and J. Hirschberg, "Segmental Effects of Timing and Height of Pitch Con-
tours," Proc. of the Int. Conf. on Spoken Language Processing, 1994 pp. 719-722.
[64]
Vanderslice, R.L., Synthetic Elocution: Considerations in Automatic Orthographic-to-
Phonetic Conversion of English with Special Reference to Prosody, PhD Thesis
1968,
UCLA, Los Angeles.
[65]
W3C, Speech Synthesis Markup Requirements for Voice Markup Languages, 2000,
http://www.w3.org/TR/voice-tts-reqs/.
[66]
Yoshimura, T., et al., "Simultaneous Modeling of Spectrum, Pitch and Duration in HMM-
Based Speech Synthesis," EuroSpeech, 1999, Budapest, Hungary pp. 2347-2350.

777
C H A P T E R
1 6
Speech SynthesisEquation Section 16
The speech synthesis module of a TTS sys-
tem is the component that generates the waveform. The input of traditional speech synthesis
systems is a phonetic transcription with its associated prosody. The input can also include
the original text with tags, as this may help in producing higher-quality speech.
Speech synthesis systems can be classified into three types according to the model
used in the speech generation. Articulatory synthesis, described in Section 16.2.4, uses a
physical model of speech production that includes all the articulators described in Chapter 2.
Formant synthesis uses a source-filter model, where the filter is characterized by slowly
varying formant frequencies; it is the subject of Section 16.2. Concatenative synthesis gen-
erates speech by concatenating speech segments and is described in Section 16.3. To allow
more flexibility in concatenative synthesis, a number of prosody modification techniques are
described in Sections 16.4 and 16.5. Finally, a guide to evaluating speech synthesis systems
is included in Section 16.6.
Speech synthesis systems can also be classified according to the degree of manual in-
tervention in the system design into synthesis by rule and data-driven synthesis. In the for-

778
Speech Synthesis
mer, a set of manually derived rules is used to drive a synthesizer, and in the latter the syn-
thesizer’s parameters are obtained automatically from real speech data. Concatenative sys-
tems are, thus, data driven. Formant synthesizers have traditionally used synthesis by rule,
since the evolution of formants in a formant synthesizer has been done with hand-derived
rules. Nonetheless, formant transitions can also be trained from data, as we show in Section
16.2.3.
Figure 16.1 Quality and task-independence in speech synthesis approaches.
16.1.
ATTRIBUTES OF SPEECH SYNTHESIS
The most important attribute of a speech synthesis system is the quality of its output speech.
It is often the case that a single system can sound beautiful on one sentence and terrible on
the next. For that reason we need to consider the quality of the best sentences and the per-
centage of sentences for which such quality is achieved. This tradeoff is illustrated in Figure
16.1, where we compare four different families of speech generation approaches:
 Limited-domain waveform concatenation. For a given limited domain, this ap-
proach can generate very high quality speech with only a small number of re-
corded segments. Such an approach, used in most interactive voice response
systems, cannot synthesize arbitrary text. Many concept-to-speech systems, de-
scribed in Chapter 17, use this approach.
 Concatenative synthesis with no waveform modification. Unlike the previous
approach, these systems can synthesize speech from arbitrary text. They can
achieve good quality on a large set of sentences, but the quality can be mediocre
for many other sentences where poor concatenations take place.
 Concatenative systems with waveform modification. These systems have more
flexibility in selecting the speech segments to concatenate because the wave-
forms can be modified to allow for a better prosody match. This means that the
number of sentences with mediocre quality is lower than in the case where no
prosody modification is allowed. On the other hand, replacing natural with syn-
high
Best quality
Limited-domain
concatenation
Concatenative
(No wave mod.)
Concatenative
(wave mod.)
Rule-based
low
High
Low
Percentage of sentences
with maximum quality

Attributes of Speech Synthesis
779
thetic prosody can hurt the overall quality. In addition, the prosody modification
process also degrades the overall quality.
 Rule-based systems. Such systems tend to sound uniformly across different sen-
tences, albeit with quality lower than the best quality obtained in the systems
above.
Best-quality and quality variability are possibly two of the most important attributes of
a speech synthesis system, but not the only ones. Measuring quality, difficult to do in an
objective way, is the main subject of Section 16.6. Other attributes of a speech synthesis
system include:
 Delay. The time it takes for the synthesizer to start speaking is important for in-
teractive applications and should be less than 200 ms. This delay is composed of
the algorithmic delays of the front end and of the speech synthesis module, as
well as the computation involved.
 Memory resources. Rule-based synthesizers require, on the average, less than
200 KB, so they are a widely used option whenever memory is at a premium.
However, required RAM can be an issue for concatenative systems, some of
which may require over 100 MB of storage.
 CPU resources. With current CPUs, processing time is typically not an issue,
unless many channels need to run in the same CPU. Nonetheless, some concate-
native synthesizers may require a large amount of computation when searching
for the optimal sequence.
 Variable speed. Some applications may require the speech synthesis module to
generate variable speed, particularly fast speech. This is widely used by blind
people who need TTS systems to obtain their information and can accept fast
speech because of the increased throughput. Fast speech is also useful when
skimming material. Concatenative systems that do not modify the waveform
cannot achieve variable speed control, unless a large number of segments are re-
corded at different speeds.
 Pitch control. Some spoken language systems require the output speech to have
a specific pitch. This is the case if you want to generate voice for a song. Again,
concatenative systems that do not modify the waveform cannot do this, unless a
large number of speech segments are recorded at different pitch.
 Voice characteristics. Other spoken language systems require specific voices,
such as that of a robot, that cannot be recorded naturally, or some, such as
monotones, that are tedious to record. Since rule-based systems are so flexible,
they are able to do many such modifications.
The approaches described in this chapter assume as input a phonetic string, durations,
a pitch contour, and possibly volume. Pauses are signaled by the default phoneme SIL with
its corresponding duration. If the parsed text is available, it is possible to do even better in a
concatenative system by conducting a matching with all the available information.

780
Speech Synthesis
16.2.
FORMANT SPEECH SYNTHESIS
As discussed in Chapter 6, we can synthesize a stationary vowel by passing a glottal peri-
odic waveform through a filter with the formant frequencies of the vocal tract. For the case
of unvoiced speech we can use random noise as the source instead. In practice, speech sig-
nals are not stationary, and we thus need to change the pitch of the glottal source and the
formant frequencies over time. The so-called synthesis-by-rule refers to a set of rules on how
to modify the pitch, formant frequencies, and other parameters from one sound to another
while maintaining continuity present in physical systems like the human production system.
Such a system is described in the block diagram of Figure 16.2.
Figure 16.2 Block diagram of a synthesis-by-rule system. Pitch and formants are listed as the
only parameters of the synthesizer for convenience. In practice, such system has about 40 pa-
rameters.
In Section 16.2.1 we describe the second block of Figure 16.2, the formant synthesizer
that generates a waveform from a set of parameters. In Section 16.2.2 we describe the first
block of Figure 16.2, the set of rules that can generate such parameters. This approach was
the one followed by Dennis Klatt and his colleagues [4, 30]. A data-driven approach to this
first block is studied in Section 16.2.3. Finally, articulatory synthesis is the topic of Section
16.2.4.
16.2.1.
Waveform Generation from Formant Values
To be able to synthesize speech by rule, a simple model for the filter is needed. Most rule-
based synthesizers use the so-called formant synthesis, which is derived from models of
speech production. The model explicitly represents a number of formant resonances (from 2
to 6). A formant resonance can be implemented (see Chapter 6) with a second-order IIR
filter
2
1
2
1
( )
1
2
cos(2
)
i
i
i
b
b
i
H z
e
f z
e
z
π
π
π
−
−
−
−
=
−
+
(16.1)
with
/
i
i
s
f
F
F
=
and
/
i
i
s
b
B
F
=
, where
iF ,
iB , and
sF are the formant’s center frequency,
formant’s bandwidth, and sampling frequency, respectively, all in Hz. A filter with several
resonances can be constructed by cascading several such second-order sections (cascade
model) or by adding several such sections together (parallel model). Formant synthesizers
typically use the parallel model to synthesize fricatives and stops and the cascade model for
all voiced sounds.
Formant
synthesizer
Pitch
contour
Formant
tracks
Phonemes
+ prosodic tags
Rule-based
system

Formant Speech Synthesis
781
Unlike the cascade model, the parallel model requires gains to be specified for each
second-order section, which often are chosen proportional to the formant’s frequency and
inversely proportional to the formant’s bandwidth. The cascade model results in an all-pole
filter, whereas the parallel model has zeros in addition to poles. Such a combination is
shown in Figure 16.3, where R1 through R6 are the resonances 1 to 6 and each one repre-
sents a second-order IIR filter like that in Eq. (16.1). RNP represents the nasal resonance,
and RNZ is an FIR filter with the nasal zero. A1 through AB are the gains for each filter,
used for the parallel model. Switch SW controls whether the cascade model or parallel
model is used.
Figure 16.3 Block diagram of the Klatt formant synthesizer (after Klatt [4]).
For voiced sounds the excitation model consists of an impulse train driving a low-pass
filter RGP and then a bandpass filter created by the parallel of RGZ and RGS. For unvoiced
sounds the excitation consists of white noise driving a low-pass filter LPF. The excitation
for voiced fricatives is a combination of the two sources. In practice, this mixed excitation is
Impulse
Gen.
RGP
AVS
RGZ
RGS
+
RNP
RNZ
R1
R2
R3
R4
R5
R1
A1
RNP
AN
R2
A2
R3
A3
R4
A4
R5
A5
R6
A6
AB
+
+
First
Diff.
First
Diff.
AF
AH
F0
Random
Number
Gen.
X
MOD.
LPF
SW
AV
Parallel Transfer Function
Cascade Transfer Function
+

782
Speech Synthesis
used for all voiced sounds to add some breathiness. Klatt [30] showed that this model could
reproduce quite faithfully a natural recording if the parameters had been manually selected.
Table 16.1 Parameter values for Klatt’s cascade/parallel formant synthesizer with the parame-
ter symbol, full name, minimum, maximum, and typical values (after Allen [4]).
N
Symbol
Name
Min
Max
Typ
1
AV
Amplitude of voicing (dB)
0
80
0
2
AF
Amplitude of frication (dB)
0
80
0
3
AH
Amplitude of aspiration (dB)
0
80
0
4
AVS
Amplitude of sinusoidal voicing (dB)
0
80
0
5
F0
Fundamental frequency (Hz)
0
500
0
6
F1
First formant frequency (Hz)
150
900
500
7
F2
Second formant frequency (Hz)
500
2500
1500
8
F3
Third formant frequency (Hz)
130
3500
2500
9
F4
Fourth formant frequency (Hz)
250
4500
3500
10
FNZ
Nasal zero frequency (Hz)
200
700
250
11
AN
Nasal formant amplitude (Hz)
0
80
0
12
A1
First formant amplitude (Hz)
0
80
0
13
A2
Second formant amplitude (Hz)
0
0
0
14
A3
Third formant amplitude (Hz)
0
80
0
15
A4
Fourth formant amplitude (Hz)
0
80
0
16
A5
Fifth formant amplitude (Hz)
0
80
0
17
A6
Sixth formant amplitude (Hz)
0
80
0
18
AB
Bypass path amplitude (Hz)
0
80
0
19
B1
First formant bandwidth (Hz)
40
500
50
20
B2
Second formant bandwidth (Hz)
40
500
70
21
B3
Third formant bandwidth (Hz)
40
500
110
22
SW
Cascade/parallel switch
0
1
0
23
FGP
Glottal resonator 1 frequency (Hz)
0
600
0
24
BGP
Glottal resonator 1 bandwidth (Hz)
100
2000
100
25
FGZ
Glottal zero frequency (Hz)
0
500
1500
26
BGZ
Glottal zero bandwidth (Hz)
100
9000
6000
27
B4
Fourth formant bandwidth (Hz)
100
500
250
28
F5
Fifth formant frequency (Hz)
350
4900
3850
29
B5
Fifth formant bandwidth (Hz)
150
700
200
30
F6
Sixth formant frequency (Hz)
400
4999
4900
31
B6
Sixth formant bandwidth (Hz)
200
2000
100
32
FNP
Nasal pole frequency (Hz)
200
500
250
33
BNP
Nasal pole bandwidth (Hz)
50
500
100
34
BNZ
Nasal zero bandwidth (Hz)
50
500
100
35
BGS
Glottal resonator 2 bandwidth (Hz)
100
1000
200
36
SR
Sampling rate (Hz)
500
2000
1000
37
NWS
Number of waveform samples per chunk
1
200
50
38
G0
Overall gain control (dB)
0
80
48
39
NFC
Number of cascaded formants
4
6
5

Formant Speech Synthesis
783
The parameter names and their minimum and maximum values are listed in Table
16.1, where the switch SW can be in voiced (V) or consonant (C) mode. In Figure 16.3, R2
is the resonator corresponding to the second formant, whose center frequency F2 and band-
width B2 are given in Table 16.1. In addition to the six resonances associated to the six for-
mants, there are other resonances: RGP, RGZ, RGS, RNP, and RNZ. Other source models
are also possible [43].
16.2.2.
Formant Generation by Rule
As described in Chapter 2, formants are one of the main features of vowels. Because of the
physical limitations of the vocal tract, formants do not change abruptly with time. Rule-
based formant synthesizers enforce this by generating continuous values for f n
i[ ] and b n
i[ ],
typically every 5–10 milliseconds. Continuous values can be implemented through the
above structures if a lattice filter is used, because it allows its reflection coefficients to vary
at every sample (see Chapter 6). In practice, the values can be fixed within a frame as long
as frames are smaller than 5 ms.
Rules on how to generate formant trajectories from a phonetic string are based on the
locus theory of speech production. The locus theory specifies that formant frequencies
within a phoneme tend to reach a stationary value called the target. Targets for formant fre-
quencies and bandwidths for a male speaker are shown in Table 16.2 (nonvocalic segments)
and Table 16.3 (vocalic segments). This target is reached if either the phoneme is suffi-
ciently long or the previous phoneme’s target is close to the current phoneme’s target. The
maximum slope at which the formants move is dominated by the speed of the articulators,
dominated by physical constraints. Since each formant is primarily caused by the position of
a given articulator, formants caused by the body of the tongue do not vary as rapidly as for-
mants caused by the tip of the tongue or the lips. Thus, rule-based systems store targets for
each phoneme as well as maximum allowable slopes and transition times.
For example, a transition between a vowel and a sonorant can follow the rule shown in
Figure 16.4 with a1 being the target of unit 1 and a2 the target of unit 2. The values of Tcb
and Tcf are 40 and 80 ms, respectively, and
2
1
2
0.75(
)
ba
a
a
a
=
+
−
. The time
cb
cf
T
T
+
speci-
fies how rapid the transition is. While linear interpolation could be used,
ba
and the ratio
/
cb
cf
T
T
are sometimes used to further refine the shape of the formant transition.
Figure 16.4 Transition between two vowels in a formant synthesizer.
t0
t1
t2
Tcb
Tcf
t
a
a1
a2
ab

784
Speech Synthesis
Other rules can allow a discontinuity, for example when a transition out of an un-
voiced segment takes place. To improve naturalness, all these parameters can be made de-
pendent on the immediate phonetic context.
Table 16.2 Targets used in the Klatt synthesizers: formant frequencies and bandwidths for
nonvocalic segments of a male speaker. Note that in addition to the phoneme set used in Chap-
ter 2, there are several additional phonetic segments here such as axp, dx, el, em, en, gp, hx, kp,
lx, qq, rx, tq, wh, that allow more control (after Allen [4]).
F1
F2
F3
B1
B2
B3
axp
430
1500
2500
120
60
120
b
200
900
2100
65
90
125
ch
300
1700
2400
200
110
270
d
200
1400
2700
70
115
180
dh
300
1150
2700
60
95
185
dx
200
1600
2700
120
140
250
el
450
800
2850
65
60
80
em
200
900
2100
120
60
70
en
200
1600
2700
120
70
110
ff
400
1130
2100
225
120
175
g
250
1600
1900
70
145
190
gp
200
1950
2800
120
140
250
h
450
1450
2450
300
160
300
hx
450
1450
2450
200
120
200
j
200
1700
2400
50
110
270
k
350
1600
1900
280
220
250
kp
300
1950
2800
150
140
250
l
330
1050
2800
50
100
280
lx
450
800
2850
65
60
80
m
480
1050
2100
40
175
120
ng
480
1600
2050
160
150
100
n
480
1400
2700
40
300
260
p
300
900
2100
300
190
185
qq
400
1400
2450
120
140
250
r
330
1060
1380
70
100
120
rx
460
1260
1560
60
60
70
sh
400
1650
2400
200
110
280
sil
400
1400
2400
120
140
250
s
400
1400
2700
200
95
220
th
400
1150
2700
225
95
200
tq
200
1400
2700
120
140
250
t
300
1400
2700
300
180
220
v
300
1130
2100
55
95
125
wh
330
600
2100
150
60
60
w
285
610
2150
50
80
60
y
240
2070
3020
40
250
500
zh
300
1650
2400
220
140
250
z
300
1400
2700
70
85
190

Formant Speech Synthesis
785
Klatt showed that for a given natural utterance, he could manually obtain a sequence
of formant tracks f n
i[ ] and b n
i[ ], such that the synthesized utterance not only had good
quality but also sounded very similar to the original. This shows that the formant synthesizer
of Section 16.2.1 appears to be sufficient for generation. On the other hand, when the for-
mant tracks are obtained automatically through rules such as that of Figure 16.4 and Table
16.2 and Table 16.3, the output speech does not sound that natural, and the voice does not
resemble the voice of the original recording.
Table 16.3 Targets used in the Klatt synthesizers: formant frequencies and bandwidths for vo-
calic segments of a male speaker. Note that in addition to the phoneme set used in Chapter 2,
there are several additional phonetic segments here such as axr, exr, ix, ixr, oxr, uxr, yu that al-
low more control (after Allen [4]).
F1
F2
F3
B1
B2
B3
aa
700
1220
2600
130
70
160
ae
620
1660
2430
70
130
300
ah
620
1220
2550
80
50
140
ao
600
990
2570
90
100
80
aw
640
1230
2550
80
70
110
ax
550
1260
2470
80
50
140
axr
680
1170
2380
60
60
110
ay
660
1200
2550
100
120
200
eh
530
1680
2500
60
90
200
er
470
1270
1540
100
60
110
exr
460
1650
2400
60
80
140
ey
480
1720
2520
70
100
200
ih
400
1800
2670
50
100
140
ix
420
1680
2520
50
100
140
ixr
320
1900
2900
70
80
120
iy
310
2200
2960
50
200
400
ow
540
1100
2300
80
70
70
oxr
550
820
2200
60
60
60
oy
550
960
2400
80
120
160
uh
450
1100
2350
80
100
80
uw
350
1250
2200
65
110
140
uxr
360
800
2000
60
60
80
yu
290
1900
2600
70
160
220
Formant synthesis is very flexible because it can generate intelligible speech with rela-
tively few parameters (about 40). The use of context-dependent rules can improve the qual-
ity of the synthesizer at the expense of a great deal of manual tuning. The synthesized
speech is, by design, smooth, although it may not resemble any given speaker and may not
sound very natural.

786
Speech Synthesis
Because of their flexibility, formant synthesizers can often generate many different
voices and effects. While not as flexible, voice effects can also be produced in concatenative
speech systems (see Section 16.5.3).
16.2.3.
Data-Driven Formant Generation
While, in general, formant synthesis assumes the formant model of Section 16.2.1 driven by
parameter values generated by rules, as in Section 16.2.2, data-driven methods to generate
the formant values have also been proposed [3]. An HMM running in generation mode emits
three formant frequencies and their bandwidths every 10 ms, and these values are used in a
cascade formant synthesizer similar to that described in Section 16.2.1. Like the speech rec-
ognition counterparts, this HMM has many decision-tree context-dependent triphones and
three states per triphone. A Gaussian distribution per state is used in this work. The baseline
system uses a six-dimensional vector that includes the first three formant frequencies and
their bandwidths. Initially it is assumed that the input to the synthesizer includes, in addition
to the duration of each phoneme, the duration of each state. In this case, the maximum like-
lihood formant track is a sequence of the state means and, therefore, is discontinuous at state
boundaries.
The key to obtaining a smooth formant track is to augment the feature vector with the
corresponding delta formants and bandwidths (the difference between the feature at time t
and that feature at time t – 1) to complete a twelve-dimensional vector. The maximum like-
lihood solution now entails solving a tridiagonal set of linear equations (see the discussion
on statistical formant tracking in Chapter 6). The resulting formant track is smooth, as it
balances formant values that are close to the state means with delta values that are also
within the state means. In addition, the synthesized speech resembles that of the donor
speaker. More details on the analysis and model training can be found in the formant track-
ing section of Chapter 6.
16.2.4.
Articulatory Synthesis
Articulatory synthesis is another model that has been used to synthesize speech by rule, by
using parameters that model the mechanical motions of the articulators and the resulting
distributions of volume velocity and sound pressure in the lungs, larynx, and vocal and nasal
tracts. Because the human speech production articulators do not have that many degrees of
freedom, articulatory models often use as few as 15 parameters to drive a formant synthe-
sizer.
The relationship between articulators and acoustics is many-to-one [17]. For example,
a ventriloquist can produce speech sounds with very different articulator positions than those
of normal speech. The speech inversion problem is therefore ill posed. By using the assump-
tion that the articulators do not change rapidly over time, it is possible, however, to estimate
the vocal-tract area from formant frequencies [37]. In [8] the model uses five articulatory
parameters: area of lip opening, constriction formed by the tongue blade, opening to the na-
sal cavities, average glottal area, and rate of active expansion or contraction of the vocal

Concatenative Speech Synthesis
787
tract volume behind a constriction. These five parameters are augmented with the first four
formant frequencies and F0.
Those area parameters can be obtained from real speech through X-rays and magnetic
resonance imaging (MRI), though positioning such sensors in the vocal tract alters the way
speech is produced (such as the sensors in the lips) and impedes completely natural sounds.
The relationship between articulatory parameters and acoustic values has typically been
done using a nonlinear mapping such as a neural network or a codebook.
While one day this may be the best way to synthesize speech, the state-of-the-art in ar-
ticulatory synthesis does not generate speech with quality comparable to that of formant or
concatenative systems.
16.3.
CONCATENATIVE SPEECH SYNTHESIS
While state-of-the-art synthesis by rule is quite intelligible, it sounds unnatural, because it is
very difficult to capture all the nuances of natural speech in a small set of manually derived
rules. In concatenative synthesis, a speech segment is synthesized by simply playing back a
waveform with matching phoneme string. An utterance is synthesized by concatenating to-
gether several speech fragments. The beauty of this approach is that unlike synthesis-by-
rule, it requires neither rules nor manual tuning. Moreover, each segment is completely natu-
ral, so we should expect very natural output.
Unfortunately, this is equivalent to assembling an automobile with parts of different
colors: each part is very good yet there is a color discontinuity from part to part that makes
the whole automobile unacceptable. Speech segments are greatly affected by coarticulation
[42], so if we concatenate two speech segments that were not adjacent to each other, there
can be spectral or prosodic discontinuities. Spectral discontinuities occur when the formants
at the concatenation point do not match. Prosodic discontinuities occur when the pitch at the
concatenation point does not match. A listener rates as poor any synthetic speech that con-
tains large discontinuities, even if each segment is very natural.
Thus, when designing a concatenative speech synthesis system we need to address the
following issues:
1. What type of speech segment to use? We can use diphones, syllables, pho-
nemes, words, phrases, etc.
2. How to design the acoustic inventory, or set of speech segments, from a set
of recordings? This includes excising the speech segments from the set of re-
cordings as well as deciding how many are necessary. This is similar to the
training problem in speech recognition.
3. How to select the best string of speech segments from a given library of seg-
ments, and given a phonetic string and its prosody? There may be several
strings of speech segments that produce the same phonetic string and pros-
ody. This is similar to the search problem in speech recognition.
4. How to alter the prosody of a speech segment to best match the desired out-
put prosody. This is the topic of Section 16.4.

788
Speech Synthesis
Generally, these concatenative systems suffer from great variability in quality: often
they can offer excellent quality in one sentence and terrible quality in the next one. If
enough good units are available, a given test utterance can sound almost as good as a re-
corded utterance. However, if several discontinuities occur, the synthesized utterance can
have very poor quality. While synthesizing arbitrary text is still a challenge with these tech-
niques, for restrictive domains this approach can yield excellent quality. We examine all
these issues in the following sections.
We define unit as an abstract representation of a speech segment, such as its phonetic
label, whereas we use instance as a speech segment from an utterance that belongs to the
same unit. Thus, a system can keep several instances of a given unit to select among them to
better reduce the discontinuities at the boundaries. This abstract representation consists of
the unit’s phonetic transcription at the minimum in such a way that the concatenation of a
set of units matches the target phonetic string. In addition to the phonetic string, this repre-
sentation can often include prosodic information.
Table 16.4 Unit types in English assuming a phone set of 42 phonemes. Longer units produce
higher quality at the expense of more storage. The number of units is generally below the abso-
lute maximum in theory: i.e., out of the 423 = 74,088 possible triphones, only about 30,000
occur in practice.
Unit length
Unit type
#Units
Quality
Phoneme
42
Diphone
~1500
Triphone
~30K
Demisyllable
~2000
Syllable
~11K
Word
100K–1.5M
Phrase
∞
Short
Long
Sentence
∞
Low
High
16.3.1.
Choice of Unit
A number of units that have been used in the field, including context-independent pho-
nemes, diphones, context-dependent phonemes, subphonetic units, syllables, words, and
phrases. A compilation of unit types for English is shown in Table 16.4 with their coverage
in Figure 16.5.
The issues in choosing appropriate units for synthesis are similar to those in choosing
units for speech recognition (described in Chapter 9):
 The unit should lead to low concatenation distortion. A simple way of minimiz-
ing this distortion is to have fewer concatenations and thus use long units such
as words, phrases or even sentences. But since some concatenations are un-
avoidable, we also want to use units that naturally lead to “small” discontinuities

Concatenative Speech Synthesis
789
at the concatenation points. For example, it has been observed that spectral dis-
continuities at vowels are much more noticeable than at fricatives, or that a dis-
continuity within a syllable is more perceptually noticeable than a discontinuity
across syllable boundaries, and similarly for within-word and across-word dis-
continuities [55]. Having several instances per unit is an alternative to long units
that allows the choice of instances with low concatenation distortion.
 The unit should lead to low prosodic distortion. While it is not crucial to have
units with slightly different prosody than the desired target, replacing a unit with
a rising pitch with another with a falling pitch may result in an unnatural sen-
tence. Altering the pitch and or duration of a unit is possible (see Section 16.4)
at the expense of additional distortion.
 The unit should be generalizable, if unrestricted text-to-speech is required. If we
choose words or phrases as our units, we cannot synthesize arbitrary speech
from text, because it’s almost guaranteed that the text will contain words not in
our inventory. As an example, the use of arbitrarily long units in such a way that
no concatenation between voiced sounds occurs by cutting at obstruents results
in low concatenation distortion but it is shown [47] that over 180,000 such units
would be needed to cover 75% of a random corpus. The longer the speech seg-
ments are, the more of them we need to be able to synthesize speech from arbi-
trary text. This generalization property is not needed if closed-domain synthesis
is desired.
 The unit should be trainable. Our training data should be sufficient to estimate
all our units. Since the training data is usually limited, having fewer units leads
to better trainability in general. So the use of words, phrases, or sentences may
be prohibitive other than for closed-domain synthesis. The other units in Table
16.4 can be considered trainable depending on the limitations on the size of our
acoustic inventory.
A practical challenge is how to balance these selection criteria. In this section we
compare a number of units and point out their strengths and weaknesses.
16.3.1.1.
Context-Independent Phonemes
The most straightforward unit is the phoneme. Having one instance of each phoneme, inde-
pendent of the neighboring phonetic context, is very generalizable, since it allows us to gen-
erate every word/sentence. It is also very trainable and we could have a system that is very
compact. For a language with N phonemes, say N = 42 for English, we would need only N
unit instances. The problem is that using context-independent phones results in many audible
discontinuities. Such a system is not intelligible.

790
Speech Synthesis
16.3.1.2.
Diphones
A type of subword unit that has been extensively used is the so-called dyad or diphone [41].
A diphone S-IH includes from the middle of the S phoneme to the middle of the IH pho-
neme, so diphones are, on the average, one phoneme long. The word hello /hh ax l
ow/ can be mapped into the diphone sequence: /sil-hh/,
/hh-ax/,
/ax-l/,
/l-ow/, /ow-sil/. If our language has N phonemes, there are potentially N 2 diphones.
In practice, many such diphones never occur in the language, so that a smaller number is
sufficient. For example, the phonetic alphabet of Chapter 2 has 42 phonemes for English,
and only about 1300 diphones are needed. Diphone units were among the first type of unit
used in concatenative systems because it yields fairly good quality. While diphones retain
the transitional information, there can be large distortions due to the difference in spectra
between the stationary parts of two units obtained from different contexts. For example,
there is no guarantee that the spectra of /ax-l/ and /l-ow/ will match at the junction
point, since /ax-l/ could have a very different right context than /ow/ or /l-ow/ could
have a very different left context than /ax/.
0
5000
10000
15000
20000
25000
30000
35000
40000
45000
50000
0
8000
16000
32000
50000
Top N surnames in English
Number of Units
word
syllables
triphones
diphones
Figure 16.5 Coverage with different number of units displays the number of units of different
types required to generate the top N surnames in the United States [34].
For this reason, many practical diphone systems are not purely diphone based: they do
not store transitions between fricatives, or between fricatives and stops, while they store
longer units that have a high level of coarticulation [48]. If only one representative of a dyad
is available, there are pitch discontinuities. Prosody modification techniques, such as those
described in Section 16.4, can be applied to correct this problem. Otherwise many instances
of each diphone are needed for good prosodic coverage. Diphones are trainable, generaliz-
able and offer better quality than context-independent phones.

Concatenative Speech Synthesis
791
16.3.1.3.
Context-Dependent Phoneme
Another subword unit used in the literature [24] is the context-dependent phoneme. If the
context is limited to the immediate left and right phonemes, the unit is known as triphone.
As in speech recognition, not all N 3 need to be stored, because not all combinations will
occur in practice. For English, typically there can be in excess of 25,000 triphones: 12,000
within-word triphones and another 12,000 across-word triphones. Because of the increased
number of units, more contextual variations can be accommodated this way. Drawing from
experience in speech recognition, we know that many different contexts have a similar effect
on the phoneme; thus, several triphones can be clustered together into a smaller number of
generalized triphones, typically between 500 and 3000. All the clustering procedures de-
scribed in Chapter 9 can be used here as well. In particular, decision-tree clustered phones
have been successfully used. Because a larger number of units can be used, discontinuities
can be smaller than in the case of diphones while making the best use of the available data.
In addition to only considering the immediate left and right phonetic context, we could also
add stress for the current phoneme and its left and right context, word-dependent phones
(where phones in particular words are considered distinct context-dependent phones), quin-
phones (where two immediate left and right phones are used), and different prosodic pat-
terns (pitch ranges and/or durations). As in speech recognition, clustered context-dependent
triphones are trainable and generalizable.
Traversing the tree for a given phoneme is equivalent to following the answers for the
branching nodes from root to leaves, which determines the clusters for similar context-
dependent phones. The decision trees are generated automatically from the analysis database
to obtain minimum within-unit distortion (or entropy) for each split. Therefore, one must be
able to acquire a large inventory of context-dependent phone HMMs with a decent coverage
of the contexts one wishes to model. All the context-dependent phone units can be well re-
placed by any other units within the same cluster. This method generalizes to contexts not
seen in the training data, because the decision tree uses questions involving broad phonetic
categories of neighboring contexts, yet provides detailed models for contexts that are repre-
sented in the database. Given the assumption that these clustering decision trees should be
consistent across different speakers, the use of ample speaker-independent databases instead
of limited speaker-dependent databases allows us to model more contexts as well as deeper
trees to generate a high-quality TTS voice. These techniques also facilitate the creation of
acoustic inventories with a scalable number of units that trade off size with quality. Thus, we
can use questions (about the immediate left/right phonetic contexts, stress, pitch, duration,
word, etc) in the decision-tree clustering methods of Chapter 4 to reduce all the possible
combinations to a manageable number.
16.3.1.4.
Subphonetic Unit
Subphonetic units, or senones, have also been used with some success [13]. Typically, each
phoneme can be divided into three states, which are determined by running a speech recog-
nition system in forced alignment mode. These states can also be context dependent and can
also be clustered using decision trees like the context-dependent phonemes. The HMM state

792
Speech Synthesis
has proved to be more effective than the context-dependent phone in speech recognition,
also trainable and generalizable, but for synthesis it means having more concatenations and
thus possibly more discontinuities. If multiple instances per subphonetic unit are used,
higher quality can be obtained.
A half phone goes either from the middle of a phone to the boundary between phones
or from the boundary between phones to the middle of the phone. This unit offers more
flexibility than a phone and a diphone and has been shown useful in systems that use multi-
ple instances of the unit [7].
16.3.1.5.
Syllable
It has been observed that discontinuities across syllables stand out more than discontinuities
within syllables [55], so syllables are natural units. There may be around 10,000 syllables in
English, depending on the exact definition of syllable, so even a context-independent sylla-
ble system needs to store at least as many if one instance per syllable is needed for full gen-
eralizability. There will still be spectral discontinuities, though hopefully not too noticeable.
More than one instance per unit may be needed to account for varying acoustic contexts or
varying prosodic patterns, particularly if no waveform modification is to be used.
16.3.1.6.
Word and Phrase
The unit can be as large as a word or even a phrase. While using these long units can in-
crease naturalness significantly, generalizability and trainability are poor, so that it is diffi-
cult to have all the instances desired to synthesize an output utterance. One advantage of
using a word or longer unit over its decomposition in phonemes, as in the above units, is that
there is no dependence on a phonetically transcribed dictionary. It is possible that the pho-
neme string associated to a word by our dictionary is not correct, or not fluent enough, so
that using a whole-word model can solve this problem. Of course, the system may have a
combination of all units: a set of the most frequent words, sentences, or phrases for best
quality some percentage of the time, and some smaller units for full generalizability and
trainability.
16.3.2.
Optimal Unit String: The Decoding Process
The goal of the decoding process is to choose the optimal string of units for a given phonetic
string that best matches the desired prosody. Sometimes there is only one possible string, so
that this process is trivial, but in general there are several strings of units that result in the
same phonetic string yet some of them sound better than others. The goal is to come up with
an objective function that approximates this sound quality that allows us to select the best
string. The quality of a unit string is typically dominated by spectral and pitch discontinui-
ties at unit boundaries. Discontinuities can occur because of:
1. Differences in phonetic contexts. A speech unit was obtained from a different
phonetic context than that of the target unit.

Concatenative Speech Synthesis
793
2. Incorrect segmentation. Such segmentation errors can cause spectral discon-
tinuities even if they had the same phonetic context.
3. Acoustic variability. Units can have the same phonetic context and be prop-
erly segmented, but variability from one repetition to the next can cause small
discontinuities. A unit spoken in fast speech is generally different from an-
other in slow or normal speech. Different recording conditions (amplitude,
microphone, sound card) can also cause spectral discontinuities.
4. Different prosody. Pitch discontinuity across unit boundaries is also a cause
for degradation.
The severity of such discontinuities generally decreases as the number of units in-
creases. More importantly, the prosody of the concatenation has, in general, no resemblance
with the prosody specified by the TTS front-end unless we have several instances of each
unit, each with a different prosody, or use a prosody modification algorithm (see Section
16.4).
16.3.2.1.
Objective Function
Our goal is to come up with a numeric measurement for a concatenation of speech segments
that correlates well with how well they sound. To do that we define unit cost and transition
cost between two units.
Let
θ
be
a
speech
segment
whose
phonetic
transcription
( )
p
p θ
=
.
Let
1
2
{ ,
,
,
}
N
θ θ
θ
Θ =

be a concatenation of N speech segments whose combined phonetic
transcription is
1
2
{
,
,
,
}
N
P
p p
p
=

. P is a string of M phonemes, and since each segment
has at least one phoneme, it holds that M
N
≥
.
For example, the phonetic string P = “hh ax l ow” corresponding to the word hello has
M = 4 phonemes and can be decomposed in N = 4 segments
1
1
2
3
4
{ ,
,
,
}
θ θ θ θ
Θ =
, where
1
(
)
/
/
p
hh
θ
=
,
2
(
)
/
/
p
ax
θ
=
,
3
(
)
/ /
p
l
θ
=
,
4
(
)
/
/
p
ow
θ
=
, each segment being a phoneme.
Or it can be decomposed into N = 2 segments
2
5
6
{
,
}
θ θ
Θ =
, where
5
(
)
/
/
p
hh ax
θ
=
,
5
(
)
/
/
p
l ow
θ
=
, so that each segment has two phonemes. There are 8 possible such decom-
positions for this example (in general there are
1
2M −possible decompositions1).
The distortion or cost function between the segment concatenation Θ and the target T
can be expressed as a sum of the corresponding unit costs and transition costs [27, 46] as
follows:
1
1
1
1
( , )
(
, )
(
,
)
N
N
u
j
t
j
j
j
j
d
T
d
T
d
θ
θ θ
−
+
=
=
Θ
=
+


(16.2)
1 This assumes that one instance per unit is available. If there are several instances per unit, the number of decom-
positions grows exponentially.

794
Speech Synthesis
where
(
, )
u
j
d
T
θ
is the unit cost of using speech segment
j
θ
within target T and
1
(
,
)
t
j
j
d θ θ +
is the transition cost in concatenating speech segments
j
θ
and
1
j
θ + . The optimal speech
segment sequence of units ˆΘ can be found as the one that minimizes the overall cost
ˆ
argmin ( , )
d
T
Θ
Θ =
Θ
(16.3)
over sequences with all possible number of units. Transition cost and unit costs are de-
scribed in Sections 16.3.2.2 and 16.3.2.3.
Let’s analyze the second term in the sum of Eq. (16.2), also shown in Figure 16.6. If
all transition costs were identical, the word string with fewest units would have lowest dis-
tortion. In practice transition costs are different and, thus, the string with fewest units is not
necessarily the best, though there is clearly a positive correlation.
Figure 16.6 Tradeoff between unit and transition costs.
When a large number of speech segments are available, finding the segment sequence
with lowest cost is a search problem like those analyzed in Chapter 12. Often a Viterbi algo-
rithm is needed to make this efficient.
The art in this procedure is in the exact definition of both transition and unit costs, for
which no standard has been defined that works best to date. In Sections 16.3.2.2 and
16.3.2.3 we present an approach for which both transition and unit costs are empirically set
after perceptual experiments. Such a system is easy to build, and study of those costs gives
insight into the perceptual effects.
Costs obtained using a data-driven criterion are described in Sections 16.3.2.4 and
16.3.2.5. While more complicated than that of empirical costs, this method addresses the
shortcomings of the previous method. Finally, we need to estimate some weights to combine
the different costs for spectrum and prosody, which can be done empirically or by regression
[26].
16.3.2.2.
Empirical Transition Cost
If spoken in succession, two speech segments have a zero transition cost. But, when they are
excised from separate utterances, their concatenation can have varying degrees of natural-
Transition cost
Unit cost
Target
units
Selected
units
tj
θj
θj+1

Concatenative Speech Synthesis
795
ness. The transition cost incorporates two types of continuity measures: coarticulatory and
prosodic.
An approximation to the prosodic continuity measure is to make it proportional to the
absolute difference of the F0 or log F0 at the boundaries, if the boundary is voiced for both
units. If we use the prosody modification techniques of Section 16.4, this cost could be set to
a small value to reflect the fact that prosody modification is not a perfect process. More so-
phisticated cost functions can be used to account for prosody mismatches [10].
Regarding the coarticulatory effect, it has been empirically observed that a concatena-
tion within a syllable is more perceptible than when the concatenation is at the syllable
boundary. Yi [55] proposed an empirical cost matrix for the concatenation of two speech
segments when that concatenation occurs within a syllable (Table 16.5) or at a syllable
boundary (Table 16.6). Phonemes are grouped by manner of articulation: vowel/semivowels,
fricatives, stops, and nasals. The rows represent the left side of the transition and the col-
umns represent the right side, and NA represents a case that does not occur. These costs re-
flect perceptual ratings by human listeners to unit concatenations between different
phonemes. Values of 10, 2000, 5000, 7500, and 10,000 were used to indicate different de-
grees of goodness from very good to very bad concatenations.
Table 16.5 Cost matrix for intrasyllable concatenations (after Yi [55]). The rows represent the
left side of the transition and the columns represent the right side, and NA represents a case
that does not occur.
vowel
semivowel
nasal
obstruent
/h/
vowel
10,000
10,000
7500
10
NA
semivowel
10,000
7500
7500
10
NA
nasal
5000
10
NA
10
NA
/h/
5000
NA
NA
NA
NA
obstruent
10
10
10
10,000
NA
Table 16.6 Cost matrix for intersyllable concatenations (after Yi [55]). The rows represent the
left side of the transition and the columns represent the right side, and NA represents a case
that does not occur.
vowel
semivowel
nasal
obstruent
/h/
Silence
vowel
NA
7500
5000
10
5000
10
semivowel
7500
7500
2000
10
10
10
nasal
2000
10
10
10
10
10
obstruent
10
10
10
5000
10
10
/h/
NA
NA
NA
NA
NA
NA
silence
10
10
10
10
10
10

796
Speech Synthesis
16.3.2.3.
Empirical Unit Cost
The unit cost is generally a combination of the coarticulation cost and the prosodic cost.
Prosodic mismatches can be made proportional to the F0 difference between the candidate
unit and the target unit or set to a fixed low value if the prosody modification techniques of
Section 16.4 are used.
A way of determining the cost associated with replacing a phonetic context with an-
other was proposed by Yi [55], who empirically set cost matrices for phone classes by listen-
ing to concatenations where such contexts were replaced. These ad hoc values also bring
some sense of where the coarticulation problems are. Replacing a vowel or semivowel by
another with a context that has a different place of articulation or nasalization results in au-
dible discontinuities. The rows represent the context class of the target phoneme and the
columns represent the context class of the proposed unit. Table 16.7, Table 16.8, Table 16.9,
Table 16.10, Table 16.11, and Table 16.12 include an empirical set of costs for such mis-
matches between the target’s context and a candidate unit’s context for the case of
vowel/semivowels, fricatives, stops, and nasals. These costs reflect human listeners’ percep-
tual ratings of speech units with an incorrect phonetic context. Values of 10, 100, 500, and
1000 were used to indicate very good, good, bad, and very bad units. These values are cho-
sen to match the values for transition costs of Section 16.3.2.2.
Table 16.7 Unit coarticulation cost matrix (after Yi [55]) for left and right context replace-
ments for vowels and semivowels.
labial
alv/den/pal
velar
m
n
ng
front
back
none
labial
10
1000
1000
1000
1000
1000
1000
1000
1000
alv/den/pal
1000
10
1000
1000
1000
1000
1000
1000
1000
velar
1000
1000
10
1000
1000
1000
1000
1000
1000
m
1000
1000
1000
10
1000
1000
1000
1000
1000
n
1000
1000
1000
1000
10
1000
1000
1000
1000
ng
1000
1000
1000
1000
1000
10
1000
1000
1000
front
1000
1000
1000
1000
1000
1000
10
1000
1000
back
1000
1000
1000
1000
1000
1000
1000
10
1000
none
1000
1000
1000
1000
1000
1000
1000
1000
10
Table 16.8 Unit coarticulation cost matrix (after Yi [55]) for left and right context replace-
ments for fricatives.
retroflex
round
sonorant
other
retroflex
10
100
100
100
round
100
10
100
100
sonorant
100
100
10
100
other
100
100
100
10

Concatenative Speech Synthesis
797
Table 16.9 Unit coarticulation cost matrix (after Yi [55]) for left context replacements for
stops.
front
back
retroflex
round
other
front
10
10
10
10
10
back
10
10
10
10
10
retroflex
10
10
10
10
10
round
10
10
10
10
10
other
500
500
500
500
10
Table 16.10 Unit coarticulation cost matrix (after Yi [55]) for right context replacements for
stops.
front
back
retroflex
round
schwa
other
Front
10
100
100
100
500
100
Back
100
10
100
100
500
100
Retroflex
100
100
10
100
500
100
Round
100
100
100
10
500
100
Schwa
500
500
500
500
10
500
Other
100
100
100
100
500
10
Table 16.11 Unit coarticulation cost matrix (after Yi [55]) for left context replacements for
nasals.
obstruent
sonorant
obstruent
10
1000
sonorant
1000
10
Table 16.12 Unit coarticulation cost matrix (after Yi [55]) for right context replacements for
nasals.
voiced
unvoiced
sonorant
voiced
10
100
1000
unvoiced
100
10
1000
sonorant
1000
1000
10

798
Speech Synthesis
16.3.2.4.
Data-Driven Transition Cost
The empirical transition costs of Section 16.3.2.2 do not necessarily mean that a spectral
discontinuity will take place, only that one is likely, and that if it occurs within a syllable it
will have a larger perceptual effect than if it occurs across syllable boundaries. While that
method can result in a good system, the cost is done independently of whether there is a true
spectral discontinuity or not. Thus, it has been also proposed to use a measurement of the
spectral discontinuity directly. This is often estimated as:
2
( ,
)
( ( ) 1)
(0)
t
i
j
i
i
j
d
l
θ θ
θ
=
−
−
x
x
(16.4)
the magnitude squared of the difference between the cepstrum at the last frame of
iθ and the
first frame of
j
θ . The quantity
( )
i
l θ
denotes the number of frames of speech segment
iθ ,
and
( )
i k
x
the cepstrum of segment
iθ at frame k.
This technique can effectively measure a spectral discontinuity in a region with slowly
varying spectrum, but it can fail when one of the segments is a nasal, for example, for which
a sharp spectral transition is expected and desired. A better way of measuring this disconti-
nuity is shown in Figure 16.7, in which we measure the cepstral distance in an overlap re-
gion:2 the last frame of segment 1 and the first frame before the beginning of segment 2:
2
( ,
)
( ( ) 1)
( 1)
t
i
j
i
i
j
d
l
θ θ
θ
=
−
−
−
x
x
(16.5)
When many speech segments are considered, a large number of cepstral distances
need to be computed, which in turn may result in a slow process. To speed it up an approxi-
mation can be made where all possible cepstral vectors at the boundaries are vector quan-
tized first, so that the distances between all codebook entries can be precomputed and stored
in a table.
Figure 16.7 Measurement of the spectral discontinuity in the overlap region. The dark gray
area is the speech region that precedes segment 2 and does not form part of segment 2. This
area should match the last part of the segment 1.
A spectral discontinuity across, say, fricatives is perceptually not as important as if
they happen across vowels [48]. For this reason, the cepstral distance described above does
2 This means extra frames need to be stored.
Segment 1
Segment 2
Overlap
Region

Concatenative Speech Synthesis
799
not correlate well with perceptual distances. To solve this problem, it is possible to combine
both methods, for example by weighting the spectral/cepstral distance by different values.
Even if no spectral discontinuity is present, a phase discontinuity may take place. The
pitch periodicity may be lost at the boundary. This can be generally solved by fine adjust-
ment of the boundary using a correlation approach as described in Section 16.4. You need to
keep in mind that such methods are not perfect.
16.3.2.5.
Data-Driven Unit Cost
Spectral discontinuities across concatenations are often the result of using a speech segment
with a different phonetic context than the target. One possibility is, then, to consider only
speech segments where the phonetic contexts to the left and right match exactly. For exam-
ple, if we use a phoneme as the basic speech segment, a perfect match would require on the
order of at least 25000 different segments. In this case, the coarticulation unit cost is zero if
the target and candidate segment have the same phonetic context and infinite otherwise.
When longer segments are desired, this number explodes exponentially. The problem with
this approach is that it severely reduces the number of potential speech segments that can be
used.
Generalized triphones, as described in Section 16.3.1.3, are used in [24]. In this ap-
proach, if the speech segments have the same generalized triphone contexts as the target
utterance, the unit cost is zero, otherwise the cost is infinite. The technique allows us to use
many more possible speech segments than the case above, yet it eliminates those speech
segments that presumably have context mismatches that in turn lead to unnatural concatena-
tions. When using a large training database, it was found that bringing the number of tri-
phones from 25,000 down to about 2000 did not adversely impact the quality, whereas some
degradation was perceived when using only 500 phoneme-length segments. Thus, this
technique allows us to reduce the size of the speech segment inventory without severely
degrading the voice quality.
If we set the number of decision-tree clustered context-dependent phonemes to be
large, there will be fewer choices of long speech segments that match. For instance, in a
system with 2000 generalized triphones, the phonetic context of the last phoneme of a long
segment and the context of the target phoneme may be clustered together, whereas in a
3000-generalized-triphone system, both contexts may not be clustered together, so that the
long segment cannot be used. This would be one example where using a larger number of
generalized triphones hurts speech naturalness because the database of speech segments is
limited. This problem could have been avoided if we didn’t have to match generalized tri-
phones and instead allowed context substitutions, yet penalized them with a corresponding
cost. In the framework of decision-tree clustered context-dependent phonemes, this cost can
be computed as the increase in entropy when those contexts are merged, using the methods
described in Chapter 9. The larger the increase in entropy, the larger the penalty is when
doing that context substitution between the candidate segment and the target segment. This
approach gives more flexibility in the number of speech segments to be considered. In this
case, there is a nonzero unit coarticulation cost associated with replacing one phonetic con-
text with another.

800
Speech Synthesis
Speech segments that have low HMM probability can be discarded, as they are proba-
bly not representative enough for that unit. Moreover, we can eliminate outliers: those units
that have parameters too far away from the mean. Eliminating pitch outliers helps if prosody
modification is to be done, as modifying pitch by more than a factor of 2 typically yields a
decrease of quality, and by keeping units with average pitch, this is less likely to occur.
Eliminating duration or amplitude outliers may signal an incorrect segmentation or a bad
transcription [13].
16.3.3.
Unit Inventory Design
The minimal procedure to obtain an acoustic inventory for a concatenative speech synthe-
sizer consists of simply recording a number of utterances from a single speaker and labeling
them with the corresponding text.
Since recording is often done in several sessions, it is important to maintain the re-
cording conditions constant to avoid spectral or amplitude discontinuities caused by changes
in recording conditions. The same microphone, room, and sound card should be used
throughout all sessions [49].
Not all donor speakers are created equal. The choice of donor speaker can have a sig-
nificant effect in voice quality (up to 0.3 MOS points on a 5-MOS scale) [7, 51, 52].
We can obtain higher-quality concatenative synthesis if the text read by the target
speaker is representative of the text to appear in our application. This way we will be able to
use longer units, and few concatenations will be needed.
Then the waveforms have to be segmented into phonemes, which is generally done
with a speech recognition system operating in forced-alignment mode. Phonetic transcrip-
tion, including alternate pronunciations, is generated automatically from text by the phonetic
analysis module of Chapter 14. A large part of the inventory preparation includes checking
correspondence between the text and corresponding waveform. Possible transcription errors
may be flagged by phonemes whose durations are too far away from the mean (outliers) [13,
24].
Once we have the segmented and labeled recordings, we can use them as our inven-
tory, or create smaller inventories as subsets that trade off memory size with quality [21,
25]. A database with a large number of utterances is generally required to obtain high-
quality synthesis. It is noteworthy to analyze whether we can reduce the size of our database
while obtaining similar synthesis quality on a given set of utterances. To do this, we can
measure the cost incurred when we use a subset of the units in the database to synthesize our
training database. A greedy algorithm can be used that at each stage eliminates the speech
unit that increases the total distortion the least, repeating the approach until the desired size
is achieved. This is an iterative analysis-by-synthesis algorithm.
The above procedure can also be used to find the set of units that have lowest cost in
synthesizing a given text. For efficiency, instead of a large training text, we could use repre-
sentative information from such text corpus, like the word trigrams with their corresponding
counts, as an approximation.
In concatenative systems, you need to store a large number of speech segments, which
could be compressed using any of the speech coding techniques described in Chapter 7.

Prosodic Modification of Speech
801
Since many such coders encode a frame of speech based on the previous one, you need to
store this context for every segment you want to encode if you are to use such systems.
16.4.
PROSODIC MODIFICATION OF SPEECH
One problem of segment concatenation is that it doesn’t generalize well to contexts not in-
cluded in the training process, partly because prosodic variability is very large. There are
techniques that allow us to modify the prosody of a unit to match the target prosody. These
prosody-modification techniques degrade the quality of the synthetic speech, though the
benefits are often greater than the distortion introduced by using them because of the added
flexibility.
The objective of prosodic modification is to change the amplitude, duration, and pitch
of a speech segment. Amplitude modification can be easily accomplished by direct multipli-
cation, but duration and pitch changes are not so straightforward.
We first present OLA and SOLA, two algorithms to change the duration of a speech
segment. Then we introduce PSOLA, a variant of the above that allows for pitch modifica-
tion as well.
16.4.1.
Synchronous Overlap and Add (SOLA)
Time-scale modification of speech is very useful, particularly voice compression, as it al-
lows a user to listen to a voice mail or taped lecture in a fraction of the time taken by the
original segment user to listen to information The overlap-and-add (OLA) technique [12]
shown in Figure 16.8 shows the analysis and synthesis windows used in the time compres-
sion. Given a Hanning window of length 2N and a compression factor of f, the analysis win-
dows are spaced fN. Each analysis window multiplies the analysis signal, and at synthesis
time they are overlapped and added together. The synthesis windows are spaced N samples
apart. The use of windows such as Hanning allows perfect reconstruction when f equals 1.
In Figure 16.8, some of the signal appearance has been lost; note particularly some ir-
regular pitch periods. To solve this problem, the synchronous overlap-and-add (SOLA) [45]
allows for a flexible positioning of the analysis window by searching the location of the
analysis window i around fNi in such a way that the overlap region had maximum correla-
tion. The SOLA algorithm produces high-quality time compression. A mathematical formu-
lation of PSOLA, an extension of both OLA and SOLA, is presented in Section 16.4.2.
While typically compression algorithms operate at a uniform rate, they have also been
used in a nonuniform rate to take into account human perception, so that rapid transitions are
compressed only slightly, steady sounds are compressed more, and pauses are compressed
the most. It’s reported in [11], that while uniform time compression can achieve a factor of
2.5 at most without degradation in intelligibility, a nonuniform compression allows up to an
average compression factor of 4.

802
Speech Synthesis
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
-1
-0.5
0
0.5
1
analysis signal
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
-1
-0.5
0
0.5
1
synthesis signal
Figure 16.8 Overlap-and-add (OLA) method for time compression. Hanning windows, N =
330, are used to multiply the analysis signal, and resulting windowed signals are added. The
analysis windows, spaced 2N samples, and the analysis signal x[n] are shown on the top. The
synthesis windows, spaced N samples apart, and the synthesis signal y[n] are shown below.
Time compression is uniform with a factor of 2. Pitch periodicity is somewhat lost, particularly
around the fourth window.
16.4.2.
Pitch Synchronous Overlap and Add (PSOLA)
Both OLA and SOLA do duration modification but cannot do pitch modification. On the
other hand, they operate without knowledge of the signal’s pitch. The most widely used
method to do pitch modification is called pitch synchronous overlap and add (PSOLA) [38,
39], though to do so it requires knowledge of the signal’s pitch. This process is illustrated in
Figure 16.9.
Let’s assume that our input signal x n
[ ] is voiced, so that it can be expressed as a func-
tion of pitch cycles x n
i[ ]
x n
x n
t i
i
a
i
[ ]
[
[ ]]





(16.6)
where t i
a[ ] are the epochs of the signal, so that the difference between adjacent epochs
P i
t i
t i
a
a
a
[ ]
[ ]
[
]


1 is the pitch period at time t i
a[ ] in samples. The pitch cycle is a windowed
version of the input
x n
w n x n
i
i
[ ]
[ ] [ ]

(16.7)

Prosodic Modification of Speech
803
which requires the windows w n
i[ ] to meet the following condition:
w n
t i
i
a
i
[
[ ]]





1
(16.8)
which can be accomplished with a Hanning window, or a trapezoidal window that spans two
pitch periods.
Figure 16.9 Mapping between five analysis epochs
[ ]
at i
and three synthesis epochs
[ ]
st
j .
Duration has been shortened by 40% and pitch period increased by 60%. Pitch cycle
2[ ]
x n
is
the product of the analysis window
2[ ]
w n , in dotted line, with the analysis signal x[n], which
is aligned with analysis epochs
[ ]
at i . In this case, synthesis pitch cycle
1[ ]
y n
equals
2[ ]
x n
and also
0
0
[ ]
[ ]
y n
x n
=
and
2
5
[ ]
[ ]
y n
x n
=
. Pitch is constant over time in this case.
Our goal is to synthesize a signal y n
[ ], which has the same spectral characteristics as
x n
[ ] but with a different pitch and/or duration. To do this, we replace the analysis epoch
sequence t i
a[ ] with the synthesis epochs t
j
s[ ], and the analysis pitch cycles x n
i[ ] with the
synthesis pitch cycles y n
j[ ]:
[ ]
[
[ ]]
j
s
j
y n
y n
t
j
∞
=−∞
=
−

(16.9)
The synthesis epochs are computed so as to meet a specified duration and pitch con-
tour, as shown in Figure 16.9. This is equivalent to an impulse train with variable spacing
[ ]
at i
[ ]
st
j
x[n]
y[n]
2
2
[ ]
[ ] [ ]
x n
w n x n
=
1[ ]
y n
2[ ]
w n

804
Speech Synthesis
driving a time-varying filter x n
t[ ] which is known for t
t i
a

[ ], as shown in Figure 16.10. The
synthesis pitch cycle y n
j[ ] is obtained via a mapping from the closest corresponding analysis
pitch cycle x n
i[ ]. In the following sections we detail how to calculate the synthesis epochs
and the synthesis pitch-cycle waveforms.
Figure 16.10 PSOLA technique as an impulse train driving a time-varying filter.
The term overlap-and-add derives from the fact that we use overlapping windows that
we add together. The pitch-synchronous aspect comes from the fact that the windows are
spaced a pitch period apart and are two pitch periods long. As you can see from Figure 16.9,
the synthesis waveform has a larger pitch period than the analysis waveform and is shorter
in duration.
For unvoiced speech, a set of epochs that are uniformly spaced works well in practice,
as long as the spacing is smaller than 10 ms. If the segment needs to be stretched in such a
way that these characteristic waveforms are repeated, an artificial periodicity would appear.
To avoid this, the characteristic waveform that was to be repeated is flipped in time [38].
This approach is remarkably simple, yet it leads to high-quality prosody modification,
as long as the voiced/unvoiced decision is correct and the epoch sequence is accurate.
To do prosody modification, the PSOLA approach requires keeping the waveform of
the speech segment and its corresponding set of epochs, or time marks. As you can see from
Eq. (16.6), if no prosody modification is done, the original signal is recovered exactly.
16.4.3.
Spectral Behavior of PSOLA
Let’s analyze why this simple technique works and how. To do that let’s consider the case of
a speech signal x[n] that is exactly periodic with period T0 and can be created by passing an
impulse train through a filter with impulse response s[n]:
x n
s n
n
iT
s n
iT
i
i
[ ]
[ ]
[
]
[
]












0
0
(16.10)
If we know the impulse response s[n], then we could change the pitch by changing
0T .
The problem is how to estimate it from x[n]. Let’s assume we want to build an estimate ~[ ]
s n
by multiplying x[n]by a window w[n]:
~[ ]
[ ] [ ]
s n
w n x n

(16.11)
The Fourier transform of x[n] in Eq. (16.10) is given by
X
S
k
S
k
k
k
( )
( )
(
)
(
) (
)


 


 











0
0
0
(16.12)
x n
t[ ]

Prosodic Modification of Speech
805
where 

0
0
2

/ T . The Fourier transform of ~[ ]
s n can be obtained using Eqs. (16.11) and
(16.12):
~( )
( )
( )
(
)
(
)
S
W
X
S
W
k
k













0
0
(16.13)
If the window w[n] is pitch synchronous, a rectangular window with length T0 or a
Hanning window with length 2 0T , for example, then the above estimate is exact at the har-
monics, i.e., ~(
)
(
)
S k
S k


0
0

, because the window leakage terms are zero at the harmonics.
In-between harmonics, ~( )
S 
is an interpolation using W( )
 , the transfer function of the
window. If we use a rectangular window, the values of S( )

in between S k
(
)
 0
and
S
k
((
)
)
1
0

are not determined only by those two harmonics, because the leakage from the
other harmonics are not negligible. The use of a Hanning window drastically attenuates this
leakage, so the estimate of the spectral envelope is better. This is what PSOLA is doing:
getting an estimate of the spectral envelope by using a pitch-synchronous window.
Since it is mathematically impossible to recover S( )

for a periodic signal, it is rea-
sonable to fill in the remaining values by interpolation with the main lobes of the transform
of the window. This approach works particularly well if the harmonics form a dense sam-
pling of the spectral envelope, which is the case for male speakers. For female speakers,
where the harmonics may be spaced far apart, the estimated spectral envelope could be far
different from the real envelope.
16.4.4.
Synthesis Epoch Calculation
In practice, we want to generate a set of synthesis epochs t
j
s[ ] given a target pitch period
P t
s( ). If the desired pitch period
( )
sP t
P
=
is constant, then the synthesis epochs are given
by
[ ]
st
j
jP
=
.
In general the desired pitch period P t
s( ) is a function of time. Intuitively, we could
compute
[
1]
st
j +
in terms of the previous epoch
[ ]
st
j and the pitch period at that time:
[
1]
[ ]
( [ ])
s
s
s
s
t
j
t
j
P t
j
+
−
=
(16.14)
though this is an approximation, which happens to work well if P t
s( ) changes slowly over
time.
Now we derive an exact equation, which also can help us understand pitch-scale and
time-scale modifications of the next few sections. Epoch
[
1]
st
j +
can be computed so that
the distance between adjacent epochs
[
1]
[ ]
s
s
t
j
t
j
+
−
equals the average pitch period in the
region
[ ]
[
1]
s
s
t
j
t
t
j
≤<
+
between them (see Figure 16.11). This can be done by the follow-
ing expression

806
Speech Synthesis
[
1]
[ ]
1
[
1]
[ ]
( )
[
1]
[ ]
s
s
t
j
s
s
s
t
j
s
s
t
j
t
j
P t dt
t
j
t
j
+
+
−
=
+
−

(16.15)
Figure 16.11 The desired pitch period
( )
sP t
is a linearly increasing function of time such that
the pitch period is doubled by the end of the segment. The four synthesis epochs
[ ]
st
j
are
computed to satisfy Eq. (16.15). In particular,
[2]
st
is computed such that
[2]
[1]
s
s
t
t
−
equals
the average pitch period in that region. Note that the growing spacing between epochs indi-
cates that pitch is growing over time.
It is useful to consider the case of P t
s( ) being linear with t in that interval:
(
)
(
)
( )
[ ]
[ ]
s
s
s
s
P t
P t
j
b t
t
j
=
+
−
(16.16)
so that the integral in Eq. (16.15) is given by
[
1]
[ ]
( )
( [ ])
2
s
s
t
j
j
s
j
s
t
j
P t dt
P t i
b δ
δ
+


=
+





(16.17)
where we have defined  j as
[
1]
[ ]
j
s
s
t
j
t
j
δ =
+
−
(16.18)
Inserting Eqs. (16.17) and (16.18) into Eq. (16.15), we obtain
( [ ])
2
j
j
s
P t i
b δ
δ =
+
(16.19)
which, using Eq. (16.18), gives a solution for epoch
[
1]
st
j +
as
(
)
( [ ])
[
1]
[ ]
1
/ 2
s
s
s
s
j
P t
j
t
j
t
j
b
δ
+
−
=
=
−
(16.20)
( )
sP t
t
[0]
st
[1]
st
[2]
st
[3]
st
[2]
[1]
s
s
t
t
−

Prosodic Modification of Speech
807
from the previous epoch t
j
s[ ], the target pitch at that epoch P t
j
s
s
( [ ]) , and the slope b. We
see that Eq. (16.14) is a good approximation to Eq. (16.20) if the slope b is small.
Evaluating Eq. (16.16) for
[
1]
st
j +
results in an expression for
( [
1])
s
s
P t
j +
(
)
(
)
( [
1])
[ ]
[
1]
[ ]
s
s
s
s
s
s
P t
j
P t
j
b t
j
t
j
+
=
+
+
−
(16.21)
Equations (16.20) and (16.21) can be used iteratively. It is important to note that Eq. (16.20)
requires
2
b <
in order to obtain meaningful results, which fortunately is always the case in
practice.
When synthesizing excitations for speech synthesis, it is convenient to specify the syn-
thesis pitch period P t
s( ) as a piecewise linear function of time. In this case, Eq. (16.20) is
still valid as long as
[
1]
st
j +
falls within the same linear segment. Otherwise, the integral in
Eq. (16.17) has two components, and a second-order equation needs to be solved to obtain
[
1]
st
j +
.
16.4.5.
Pitch-Scale Modification Epoch Calculation
Sometimes, instead of generating an epoch sequence given by a function P t
s( ), we want to
modify the epoch sequence of an analysis signal with epochs t i
a[ ] by changing its pitch
while maintaining its duration intact. This is called pitch-scale modification. To obtain the
corresponding synthesis epochs, let’s assume that the pitch period P t
a ( ) of the analysis
waveform at time t is constant and equals the difference between both epochs
( )
[
1]
[ ]
a
a
a
P t
t i
t i
=
+
−
(16.22)
as seen in Figure 16.12.
Figure 16.12 Pitch period of the analysis waveform as a function of time. It is a piece-wise
constant function of time.
The pitch period of the synthesis waveform P t
s( ) at the same time t now falls in be-
tween epochs j and j + 1
[ ]
[
1]
s
s
t
j
t
t
j
≤<
+
(16.23)
P t
a ( )
t
[
1]
at i −
t i
a[ ]
[
1]
at i +
[
1]
[ ]
a
a
t i
t i
+
−
[ ]
[
1]
a
a
t i
t i
−
−

808
Speech Synthesis
with t
j
s[ ] being the time instant of the j epoch of the synthesis waveform. Now, let’s define
a relationship between analysis and synthesis pitch periods
( )
( )
( )
s
a
P t
t P t
β
=
(16.24)
where ( )t
reflects the pitch-scale modification factor, which, in general, is a function of
time. Following the derivation in Section 16.4.3, we compute the synthesis epoch
[
1]
st
j +
so that
[
1]
[ ]
1
[
1]
[ ]
( )
( )
[
1]
[ ]
s
s
t
j
s
s
a
t
j
s
s
t
j
t
j
t P t dt
t
j
t
j
β
+
+
−
=
+
−

(16.25)
which reflects the fact that the synthesis pitch period at time t is the average pitch period of
the analysis waveform times the pitch-scale modification factor. Since ( )
( )
t P t
a
is piece-
wise linear, we can use the results of Section 16.4.3 to solve for
[
1]
st
j +
. In general, it
needs to be solved recursively, which results in a second-order equation if ( )t
is a constant
or a linear function of t.
16.4.6.
Time-Scale Modification Epoch Calculation
Time-scale modification of speech involves changing the duration of a speech segment
while maintaining its pitch intact. This can be realized by defining a mapping
( )
s
a
t
D t
=
, a
time-warping function, between the original signal and the modified signal. It is useful to
define the duration modification rate
( )t
α
from which the mapping function can be derived:
0
( )
( )
t
D t
d
α τ
τ
= 
(16.26)
Let’s now assume that the duration modification rate
( )t
α
α
=
is constant, so that the
mapping D(t) in Eq. (16.26) is linear. If
1
α > , we are slowing down the speech, whereas if
1
α < , we are speeding it up. Let’s consider time t in between epochs i and I + 1 so that
[ ]
[
1]
a
a
t i
t
t i
≤<
+
:
( [0])
0
( )
( [ ])
(
[ ])
a
a
a
D t
D t
D t i
t
t i
α
=
=
+
−
(16.27)
So that the relationship between analysis and synthesis pitch periods is given by
(
( ))
( )
s
a
P D t
P t
=
(16.28)
To solve this it is useful to define a stream of virtual time instants
, [ ]
at
j
in the analysis
signal related to the synthesis time instants by
,
[ ]
( [ ])
s
a
t
j
D t
j
=
(16.29)

Prosodic Modification of Speech
809
as shown in Figure 16.13.
Figure 16.13 Time-scale modification of speech. The five analysis epochs
[ ]
at i
are shown in
the x-axis and the four synthesis epochs
[ ]
st i
in the ordinate. Duration is shortened by 25%
while maintaining the same pitch period. The corresponding virtual analysis epochs
[ ]
at i
′
are
obtained through the mapping D(t), a linear transformation with
0.75
α =
.
Now we try to determine
[
1]
st
j +
such that
[
1]
[ ]
s
s
t
j
t
j
+
−
is equal to the average
pitch period in the original time signal between
, [ ]
at
j
and
, [
1]
at
j +
:
'
,
[
1]
,
,
[ ]
1
[
1]
[ ]
( )
[
1]
[ ]
a
a
t
j
s
s
a
t
j
a
a
t
j
t
j
P t dt
t
j
t
j
+
+
−
=
+
−

(16.30)
which, using Eqs. (16.27) and (16.29), results in
[
1]/
[ ]/
[
1]
[ ]
( )
[
1]
[ ]
s
s
t
j
s
s
a
t
j
s
s
t
j
t
j
P t dt
t
j
t
j
α
α
α
+
+
−
=
+
−

(16.31)
which again results in a second-order equation if P t
a ( ) is piecewise constant or linear in t.
[ ]
t i
( )
D t
[1]
t′
[2]
t′
[0]
t′
[3]
t′
[0]
t
[1]
t
[2]
t
[3]
t
( )
P t
t
[2]
[1]
t
t
−

810
Speech Synthesis
16.4.7.
Pitch-Scale Time-Scale Epoch Calculation
The case of both pitch-scale and time-scale modification results in a combination of Eqs.
(16.25) and (16.31):
[
1]/
[ ]/
[
1]
[ ]
( )
( )
[
1]
[ ]
s
s
t
j
s
s
a
t
j
s
s
t
j
t
j
t P t dt
t
j
t
j
α
α
α
β
+
+
−
=
+
−

(16.32)
which again results in a second-order equation if
( )
( )
a
t P t
β
is piecewise constant or linear in
t.
16.4.8.
Waveform Mapping
The synthesis pitch waveforms can be computed through linear interpolation. Suppose that
[ ]
[ ]
[
1]
a
a
a
t i
t
j
t i
′
≤
<
+
, then y n
j[ ] is given by
1
[ ]
(1
) [ ]
[ ]
j
j
i
j
i
y n
x n
x
n
γ
γ
+
=
−
+
(16.33)
where
j
γ
is given by
[ ]
[ ]
[
1]
[ ]
a
a
j
a
a
t
j
t i
t i
t i
γ
′
−
=
+
−
(16.34)
Using this interpolation for voiced sounds results in smooth speech. For unvoiced
speech, this interpolation results in a decrease of the amount of aspiration. Since smoothness
is not a problem in those cases, the interpolation formula above is not used for unvoiced
frames. A simplification of this linear interpolation consists of rounding
j
γ
to 0 or 1 and,
thus, selecting the closest frame.
16.4.9.
Epoch Detection
In the PSOLA approach, the analysis epochs t i
a[ ] were assumed known. In practice this is
not the case and we need to estimate them from the speech signal. There can be errors if the
pitch period is not correctly estimated, which results in a rough, noisy voice quality. But
estimating the epochs is not a trivial task, and this is the most sensitive part of achieving
prosody modification in PSOLA.
Most pitch trackers attempt to determine F0 and not the epochs. From the t i
a[ ] se-
quence it is easy to determine the pitch, since
( )
[
1]
[ ]
a
a
P t
t i
t i
=
+
−
for
[ ]
[
1]
a
a
t i
t
t i
< <
+
.
But from the pitch P(t) the epoch placement is not uniquely determined, since the time ori-
gin is unspecified.
Common pitch tracking errors, such as pitch doubling, subharmonic selection, or er-
rors in voiced/unvoiced decisions, result in rough speech. While manual pitch marking can

Prosodic Modification of Speech
811
result in accurate pitch marks, it is time consuming and error prone as well, so automatic
methods have receivedt a great deal of attention.
Figure 16.14 Speech signal, laryngograph signal, and its corresponding epochs.
A method that attains very high accuracy has been proposed through the use of an e-
lectroglottograph (EGG) [32]. It consists of a pair of electrodes strapped around the neck at
both sides of the larynx that measures the impedance of the larynx. Such a device, also
called laryngograph, delivers a periodic signal when the vocal cords are vibrating and no
signal otherwise. The period of a laryngograph signal is fairly stationary, which makes it
relatively easy to determine the epochs from it (see Figure 16.14).
High-quality epoch extraction can be achieved by performing peak picking on the de-
rivative of the laryngograph signal. Often, the derivative operation is accomplished by a
first-order preemphasis filter
1
1
]
[
−
−
=
z
z
H
α
, with α being close to 1 (0.95 is a good
choice).
In practice, the signal is preprocessed to filter out the low frequencies (lower than 100
Hz) and high frequencies (higher than 4 kHz). This can be done with rectangular window
filters that are quite efficient and easy to implement. There is a significant amount of energy
outside this band that does not contribute to epoch detection, yet it can complicate the proc-
ess, as can be seen in Figure 16.14, so this bandpass filtering is quite important.
The preemphasized signal exhibits peaks that are found by thresholding. The quality
of this epoch detector has been evaluated on recordings from two female and four male
speakers, and the voiced/unvoiced decision errors are lower than 1%. This is definitely ac-
ceptable for our prosody-modification algorithms. The quality of prosody modification with
the epochs computed by this method vastly exceeded the quality achieved when standard
pitch trackers (as described in Chapter 6) were used on the original speech signal [2].

812
Speech Synthesis
16.4.10.
Problems with PSOLA
The PSOLA approach is very effective in changing the pitch and duration of a speech seg-
ment if the epochs are determined accurately. Even assuming there are no pitch tracking
errors, there can be problems when concatenating different segments:
1000
1050
1100
1150
1200
1250
1300
1350
1400
1450
1500
-1
0
1
1000
1050
1100
1150
1200
1250
1300
1350
1400
1450
1500
-1
0
1
1000
1050
1100
1150
1200
1250
1300
1350
1400
1450
1500
-1
0
1
Figure 16.15 Phase mismatches in unit concatenation. Waveforms are identical, but windows
are not centered on the same relative positions within periods.
Phase mismatches. Even if the pitch period is accurately estimated, mismatches in the
positioning of the epochs in the analysis signal can cause glitches in the output, as can be
seen in Figure 16.15.
The MBROLA [15] technique, an attempt to overcome phase mismatches, uses the
time-domain PSOLA method for prosody modification, but the pitch cycles have been pre-
processed so that they have a fixed phase. The advantage is that the spectral smoothing can
be done by directly interpolating the pitch cycles in the time domain without adding any
extra complexity. Since MBROLA sets the phase to a constant, the algorithm is more robust
to phase errors in the epoch detection. Unfortunately, setting the phases constant incurs the
added perceived noise described before.
Pitch mismatches. These occur even if there are no pitch or phase errors during the
analysis phase. As shown in Section 16.4.3, if two speech segments have the same spectral
envelope but different pitch, the estimated spectral envelopes are not the same, and, thus, a
discontinuity occurs (see Figure 16.16).

Prosodic Modification of Speech
813
350
400
450
500
550
600
650
700
-1
0
1
350
400
450
500
550
600
650
700
-1
0
1
350
400
450
500
550
600
650
700
-1
0
1
Figure 16.16 Pitch mismatches in unit concatenation. Two synthetic vowels were generated
with a pitch of 138 Hz (top) and 137 Hz (middle) and exactly the same transfer function. There
is no pitch tracking error, and windows are positioned coherently (no phase mismatch). The
pitch of the second wave is changed through PSOLA to match the pitch of the first wave.
There is a discontinuity in the resulting waveform and its spectrum (see Section 16.4.3), which
is an artifact of the way the PSOLA approach estimates the spectral envelope.
In addition, pitch and timbre are not independent. Even when producing the same
sound in the same phonetic context, a vastly different pitch will likely result in a different
spectral envelope. This effect is particularly accentuated in the case of opera singers, who
move their formants around somewhat so that the harmonics fall near the formant values and
thus produce higher output.
Amplitude mismatch. A mismatch in amplitude across different units can be corrected
with an appropriate amplification, but it is not straightforward to compute such a factor.
More importantly, the timbre of the sound will likely change with different levels of loud-
ness.
The PSOLA approach doesn’t handle well voiced fricatives that are stretched consid-
erably because of added buzziness (repeating frames induces periodicity at the high fre-
quency that wasn’t present in the original signal) or attenuation of the aspirated component
(if frames are interpolated).

814
Speech Synthesis
16.5.
SOURCE-FILTER MODELS FOR PROSODY MODIFICATION
The largest problem in concatenative synthesis occurs because of spectral discontinuities at
unit boundaries. The methods described in Section 16.3 significantly reduce this problem
but do not eliminate it. While PSOLA can do high-quality prosody modification on speech
segments, it doesn’t address these spectral discontinuities occurring at unit boundaries. It
would be useful to come up with a technique that allows us to smooth these spectral discon-
tinuities. In addition, PSOLA introduces buzziness for overstretched voiced fricatives. In the
following sections we describe a number of techniques that have been proposed to cope with
these problems and that are based on source-filter models.
The use of source-filter models allow us to modify the source and filter separately and
thus maintain more control over the resulting synthesized signal. In Section 16.5.1 we study
an extension of PSOLA that allows filter modification as well for smoothing purposes. Sec-
tion 16.5.2 describes mixed excitation models that also allow for improved voiced fricatives.
Finally, Section 16.5.3 studies a number of voice effects that can be achieved with a source-
filter model.
16.5.1.
Prosody Modification of the LPC Residual
A method known as LP-PSOLA that has been proposed to allow smoothing in the spectral
domain is to do PSOLA on LPC residual. This approach, thus, implicitly uses the LPC spec-
trum as the spectral envelope instead of the spectral envelope interpolated from the harmon-
ics (see Section 16.4.3) when doing F0 modification. If the LPC spectrum is a better fit to
the spectral envelope, this approach should reduce the spectral discontinuities due to differ-
ent pitch values at the unit boundaries. LP-PSOLA reduces the bandwidth widening. In prac-
tice, however, this hasn’t proven to offer a significant improvement in quality, possibly
because the spectral discontinuities due to coarticulation dominate the overall quality.
The main advantage of this approach is that it allows us to smooth the LPC parameters
around a unit boundary and thus obtain smoother speech. Since smoothing the LPC parame-
ters directly may lead to unstable frames, other equivalent representations, such as line spec-
tral frequencies, reflection coefficients, log-area ratios, or autocorrelation coefficients, are
used instead. The use of a long window for smoothing may blur sharp spectral changes that
occur in natural speech. In practice, a window of 20–50 ms centered around the boundary
has been proven useful.
While time-domain PSOLA has low computational complexity, its use in a concatena-
tive speech synthesizer generally requires a large acoustic inventory. In some applications
this is unacceptable, and it needs to be compressed using any of the coding techniques de-
scribed in Chapter 7. You need to keep in mind that to use such encoders you need to store
the coder’s memory so that the first frame of the unit can be accurately encoded. The com-
bined decompression and prosody modification is not as computationally efficient as time-
domain PSOLA alone, so that the LP-PSOLA approach may offer an effective tradeoff,
given that many speech coders encode the LPC parameters anyway.

Source-Filter Models for Prosody Modification
815
16.5.2.
Mixed Excitation Models
The block diagram of PSOLA shown in Figure 16.10 for voiced sounds also works for un-
voiced sounds by choosing arbitrary epochs. The time-varying filter of Figure 16.10 can be
kept in its time-domain form or in the frequency domain X k
t[ ] by taking the FFT of x n
t[ ].
It has been empirically shown that for unvoiced frames, the phase of X k
t[ ] is unim-
portant as long as it is random. Thus, we can pass random noise through a filter with magni-
tude response |
[ ]|
X k
t
and obtain perceptually indistinguishable results. This reduced
representation is shown in Figure 16.17. Moreover, it has been shown that the magnitude
spectrum does not need to be encoded accurately, because it doesn’t affect the synthesized
speech. The only potential problem with this model occurs when voiced frames are incor-
rectly classified as unvoiced.
Maintaining the phase of X k
t[ ] is perceptually important for voiced sounds. If it is set
to 0, two audible distortions appear: the reconstructed speech exhibits a noisy quality, and
voiced fricatives sound buzzy.
Figure 16.17 Speech synthesis model with white noise or an impulse train driving a time-
varying filter.
The perceived noise may come from the fact that a listener who hears a formant, be-
cause of its amplitude spectrum, also expects the 180° phase shift associated with a complex
pole. In fact, it is not the absolute phase, but the fact that if the formant frequency/bandwidth
changes with time, there is a phase difference over time. If such a phase is not present, scene
analysis done in the auditory system may match this to noise. This effect can be greatly at-
tenuated if the phase of the residual in LP-PSOLA is set to 0, possibly because the LPC co-
efficients carry most of the needed phase information.
The buzziness in voiced fricatives is the result of setting phase coherence not only at
low frequencies but also at high frequencies, where the aspiration component dominates.
This is the result of treating the signal as voiced, when it has both a voiced and an unvoiced
component. In fact, most voiced sounds contain some aperiodic component, particularly at
high frequencies. The amount of aspiration present in a sound is called breathiness. Female
speech tends to be more breathy than male speech [29]. Mixed-excitation models, such as
those in Figure 16.18, are then proposed to more accurately represent speech.
Such a model is very similar to the waveform-interpolation coding approach of Chap-
ter 7, and, hence, we can leverage much of what was described there regarding the estima-
tion of x n
t
v[ ] and x n
t
u[ ]. This approach allows us to integrate compression with prosody
modification.
x n
t[ ]

816
Speech Synthesis
Figure 16.18 Mixed excitation model for speech synthesis.
The harmonic-plus-noise [50] model decomposes the speech signal s(t) as a sum of a
random component
( )
rs t
and a harmonic component
( )
ps
t
( )
( )
( )
r
p
s t
s t
s
t
=
+
(16.35)
where
( )
ps
t
uses the sinusoidal model described in Chapter 7:
( )
1
( )
( )cos(
( )
( ))
K t
p
k
k
k
s
t
A t
k
t
t
θ
φ
=
=
+

(16.36)
where
( )
kA t
and
( )
k t
φ
are the amplitude and phase at time t of the kth harmonic, and
( )t
θ
is given by
0
( )
( )
t
t
l dl
θ
ω
−∞
= 
(16.37)
16.5.3.
Voice Effects
One advantage of using a spectral representation like those described in this section is that
several voice effects can be achieved relatively easily, such as whisper, voice conversion,
and echo/reverberation.
A whispering effect can be achieved by replacing the voiced component by random
noise. Since the power spectrum of the voiced signal is a combination of the vocal tract and
the glottal pulse, we would need to remove the spectral roll-off of the glottal pulse. This
means that the power spectrum of the noise has to be high-pass in nature. Using white noise
results unnatural speech.
Voice conversion can be accomplished by altering the power spectrum [6, 28]. A
warping transformation of the frequency scale can be achieved by shifting the LSF or the
LPC roots, if using an LPC approach, or a warping curve if using an FFT representation.
Adding a controlled number of delayed and attenuated echoes can enhance an other-
wise dry signal. If the delay is longer, it can simulate the room acoustics of a large hall.
x n
t
u[ ]
x n
t
v[ ]
+

Evaluation of TTS Systems
817
16.6.
EVALUATION OF TTS SYSTEMS
How do we determine whether one TTS system is better than another? Being able to evalu-
ate TTS systems allows a customer to select the best system for his or her application. TTS
evaluation is also important for developers of such systems to set some numerical goals in
their design. As in any evaluation, we need to define a metric, which generally is dependent
on the particular application for which the customer wants the TTS system. Such a metric
consists of one or several variables of a system that are measured. Gibbon et al. [19] present
a good summary of techniques used in evaluation of TTS systems.
Here we present a taxonomy of a TTS evaluation:
 Glass-box vs. black-box evaluation. There are two types of evaluation of TTS
systems according to whether we evaluate the whole system or just one of its
components: black-box and glass-box. A black-box evaluation treats the TTS
system as a black box and evaluates the system in the context of a real-world
application. Thus, a system may do very well on a flight reservation application
but poorly on an e-mail reading application. In a glass-box evaluation, we at-
tempt to obtain diagnostics by evaluating the different components that make up
a TTS system.
 Laboratory vs. field. We can also conduct the study in a laboratory or in the
field. While the former is generally easier to do, the latter is generally more ac-
curate.
 Symbolic vs. acoustic level. In general, TTS evaluation is normally done by ana-
lyzing the output waveform, the so-called acoustic level. Glass-box evaluation at
the symbolic level is useful for the text analysis and phonetic module, for exam-
ple.
 Human vs. automated. There are two fundamentally distinct ways of evaluating
speech synthesizers, according to how a given attribute of the system is esti-
mated. One is to use human subjects; the other to automate the evaluation proc-
ess. Both types have some issues in common and a number of dimensions of
systematic variation. But the fundamental distinction is one of cost. In system
development, and particularly in research on high-quality systems, it can be pro-
hibitively expensive to run continuously a collection of human assessments of
every algorithmic change or idea. Though human-subject checkpoints are
needed throughout the development process, human testing is of greatest impor-
tance for the integrated, functionally complete system in the target field setting.
At all earlier stages of development, automated testing should be substituted for
human-subject testing wherever possible. The hope is that someday TTS re-
search can be conducted as ASR research is today: algorithms are checked for
accuracy or performance improvements automatically in the lab, while human
subjects are mainly used when the final integrated system is deployed for field
testing. This allows for rapid progress in the basic algorithms contributing to ac-
curacy on any given dimension.

818
Speech Synthesis
 Judgment vs. functional testing. Judgment tests are those that measure the TTS
system in the context of the application where it is used, such as what percent-
age of the time users hang up an IVR system. System A may be more appropri-
ate than system B for a banking application where most of the speech consists of
numerical values, and system B may be better than system A for reading e-mail
over the phone. Nonetheless, it is useful to use functional tests that measure
task-independent variables of a TTS system, since such tests allow an easier
comparison among different systems, albeit a nonoptimal one. Since a human
listener is the consumer of a TTS system, tests have been designed to determine
the following characteristics of synthesized speech: intelligibility, overall qual-
ity, naturalness, suitability for a given task, and pleasantness. In addition, testing
has been used for ranking and comparing a number of competing speech synthe-
sizers, and for comparing synthetic with natural speech.
 Global vs. analytic assessment. The tests can measure such global aspects as
overall quality, naturalness, and acceptability. Analytic tests can measure the
rate, the clarity of vowels and consonants, the appropriateness of stresses and
accents, pleasantness of voice quality, and tempo. Functional tests have been de-
signed to test the intelligibility of individual sounds (phoneme monitoring), of
combinations of sounds (syllable monitoring), and of whole words (word
monitoring) in isolation as well as in various types of context.
It should be noted that all the above tests focus on segments, words, and sentences.
This is a historical artifact, and as the field evolves, we should see an emphasis on testing of
higher-order units. The diagnostic categories mentioned above can be used as a basis for
developing tests of systems that take other structure into account. Such systems might in-
clude document-to-speech, concept-to-speech, and simulated conversation or dialog. A good
system will reflect document and paragraph structure in the pausing and rhythm. Concept-
to-speech systems claim to bring fuller knowledge of the intended use of information to bear
in message generation and synthesis. Simulated dialog systems, or human-computer dialog
systems, have to mimic a more spontaneous style, which is a subtle quality to evaluate. The
tricky issue with higher-order units is the difficulty of simple choice or transcription-
oriented measures. To develop tests of higher-order synthesizers, the word and sentence
metrics can be applied to components and the overall system until reasonable intelligibility
can be verified. Then tests of the special issues raised by higher-order systems can be con-
ducted. Appropriate measures might be MOS overall ratings, preference between systems,
summarization/gist transcription with subjective scoring, and task-based measures such as
following directions. With task-based testing of higher-order units, both the correctness of
direction-following and the time to completion, an indirect measure of intelligibility, pleas-
antness, and fatigue, can be recorded.
Furthermore, speech perception is not simply auditory. As discussed in Chapter 2, the
McGurk effect [36] shows that perception of a speech sound is heavily influenced by visual
cues. Synthetic speech is thus perceived with higher quality when a talking head is added as
a visual cue [9, 18].

Evaluation of TTS Systems
819
Glass-box evaluation of the text analysis and phonetic analysis modules, requiring
evaluation at the symbolic level, is done in Chapter 14. A glass-box evaluation of the pros-
ody module is presented in Chapter 15. In this section we include glass-box evaluation of
the synthesis module, as well as a black-box evaluation of the whole system.
16.6.1.
Intelligibility Tests
A critical measurement of a TTS system is whether or not human listeners can understand
the text read by the system. Tests that measure this are called intelligibility tests. In this sec-
tion we describe the Diagnostic Rhyme Test, the Modified Rhyme Test, the Phonetically
Balanced word list test, the Haskins Syntactic Sentence Test, and the Semantically Unpre-
dictable Sentence Test. The first three were described in a procedure approved by the
American National Standards Institute [5].
Table 16.13 The 192 stimulus words of the Diagnostic Rhyme Test (DRT).
Voicing
Nasality
Sustenation
Sibilation
Graveness
Compactness
veal
feel
meat
beat
vee
bee
zee
thee
weed
reed
yield
wield
bean
peen
need
deed
sheet
cheat
cheep
keep
peak
teak
key
tea
gin
chin
mitt
bit
vill
bill
jilt
gilt
bid
did
hit
fit
dint
tint
nip
dip
thick
tick
sing
thing
fin
thin
gill
dill
zoo
sue
moot
boot
foo
pooh
juice
goose
moon
noon
coop
poop
dune
tune
news
dues
shoes
choose
chew
coo
pool
tool
you
rue
vole
foal
moan
bone
those
doze
joe
go
bowl
dole
ghost
boast
goat
coat
note
dote
though
dough
sole
thole
fore
thor
show
so
zed
said
mend
bend
then
den
jest
guest
met
net
keg
peg
dense
tense
neck
deck
fence
pence
chair
care
pent
tent
yen
wren
vast
fast
mad
bad
than
dan
jab
gab
bank
dank
gat
bat
gaff
calf
nab
dab
shad
chad
sank
thank
fad
thad
shag
sag
vault
fault
moss
boss
thong
tong
jaws
gauze
fought
thought
yawl
wall
daunt
taunt
gnaw
daw
shaw
chaw
saw
thaw
bong
dong
caught
thought
jock
chock
mom
bomb
von
bon
jot
got
wad
rod
hop
fop
bond
pond
knock
dock
vox
box
chop
cop
pot
tot
got
dot
Among the best known and most mature of these tests is the Diagnostic Rhyme Test
(DRT) proposed by Voiers [54], which provides for diagnostic and comparative evaluation
of the intelligibility of single initial consonants. The test runs twice through the list of 96
rhyming pairs shown in Table 16.13. The test consists of identification choice between two
alternative English (or target-language) words, differing by a single phonetic feature in the
initial consonant. For English the test includes contrasts among easily confusable paired
consonant sounds such as veal/feel, meat/beat, fence/pence, cheep/keep, weed/reed, and
hit/fit. In the test, both veal and feel are presented with the response alternatives veal and

820
Speech Synthesis
feel. Six contrasts are represented, namely voicing, nasality, sustention, sibilation, graveness,
and compactness. Each contrast is included 32 times in the test, combined with 8 different
vowels. The percentage of right answers is used as an indicator of speech synthesizer intelli-
gibility. The tests use a minimum of five talkers and five listeners; larger subject groups
reduce the margin of error. Even for high-quality speech coders, 100% correct responses are
rarely achieved, so synthesizer results should be interpreted generously.
Table 16.14 The 300 stimulus words of the Modified Rhyme Test (MRT).
went
sent
bent
dent
tent
rent
same
name
game
tame
came
fame
hold
cold
told
fold
sold
gold
peel
reel
feel
eel
keel
heel
pat
pad
pan
path
pack
pass
hark
dark
mark
bark
park
lark
lane
lay
late
lake
lace
lame
heave
hear
heat
heal
heap
heath
kit
bit
fit
hit
wit
sit
cup
cut
cud
cuff
cuss
cud
must
bust
gust
rust
dust
just
thaw
law
raw
paw
jaw
saw
teak
team
teal
teach
tear
tease
pen
hen
men
then
den
ten
din
dill
dim
dig
dip
did
puff
puck
pub
pus
pup
pun
bed
led
fed
red
wed
shed
bean
beach
beat
beak
bead
beam
pin
sin
tin
fin
din
win
heat
neat
feat
seat
meat
beat
dug
dung
duck
dud
dub
dun
dip
sip
hip
tip
lip
rip
sum
sun
sung
sup
sub
sud
kill
kin
kit
kick
king
kid
seep
seen
seethe
seek
seem
seed
hang
sang
bang
rang
fang
gang
not
tot
got
pot
hot
lot
took
cook
look
hook
shook
book
vest
test
rest
best
west
nest
mass
math
map
mat
man
mad
pig
pill
pin
pip
pit
pick
ray
raze
rate
rave
rake
race
back
bath
bad
bass
bat
ban
save
same
sale
sane
sake
safe
way
may
say
pay
day
gay
fill
kill
will
hill
till
bill
pig
big
dig
wig
rig
fig
sill
sick
sip
sing
sit
sin
pale
pace
page
pane
pay
pave
bale
gale
sale
tale
pale
male
cane
case
cape
cake
came
cave
wick
sick
kick
lick
pick
tick
shop
mop
cop
top
hop
pop
peace
peas
peak
peach
peat
peal
coil
oil
soil
toil
boil
foil
bun
bus
but
bug
buck
buff
tan
tang
tap
tack
tam
tab
sag
sat
sass
sack
sad
sap
fit
fib
fizz
fill
fig
fin
fun
sun
bun
gun
run
nun
A variant of this is the Modified Rhyme Test (MRT) proposed by House [22], which
also uses a 300-entry word list for subjective intelligibility testing. The modified Rhyme
Test (shown in Table 16.14) uses 50 six-word lists of rhyming or similar-sounding monosyl-
labic English words, e.g., went, sent, bent, dent, tent, rent. Each word is basically Conso-
nant-Vowel-Consonant (with a few consonant clusters), and the six words in each list differ
only in the initial or final consonant sound(s). Listeners are asked to identify which of the
words was spoken by the synthesizer (closed response), or in some cases to enter any word
they thought they heard (open response). A carrier sentence, such as “Would you write <test
word> now,” is usually used for greater naturalness in stimulus presentation. Listener re-
sponses can be scored as the number of words heard correctly; or the frequency of confu-
sions of particular consonant sounds. This can be viewed as intelligibility of the synthesizer.

Evaluation of TTS Systems
821
Though this is a nice isolation of one property, and as such is particularly appropriate
for diagnostic use, it is not intended to substitute for fuller evaluation under more realistic
listening conditions involving whole sentences. Segmental intelligibility is somewhat over-
estimated in these tests, because all the alternatives are real words and the subjects can ad-
just their perception to match the closest word. A typical human voice gives an MRT score
of about 99%, with that of TTS systems generally ranging from 70% to 95%.
The set of twenty phonetically balanced (PB) word lists was developed during World
War II and has been used very widely since then in subjective intelligibility testing. In Table
16.15 we include the first four PB word lists [20]. The words in each list are presented in a
new, random order each time the list is used, each spoken in the same carrier sentence. The
PB intelligibility test requires more training of listeners and talkers than other subjective
tests and is particularly sensitive to SNR: a relatively small change causes a large change in
the intelligibility score.
Tests using the Haskins Syntactic Sentences [40] go somewhat farther toward more re-
alistic and holistic stimuli. This test set consists of 100 semantically unpredictable sentences
of the form The <Adjective> <Noun1> <Verb> the <Noun2>, such as “The old farm cost
the blood,” using high-frequency words. Compared with the rhyme tests, contextual predict-
ability based on meaning is largely lacking, the longer speech streams are more realistic, and
more coarticulation is present. Intelligibility is indicated by percentage of words correct.
Table 16.15 Phonetically balanced word lists.
List 1
are, bad, bar, bask, box, cane, cleanse, clove, crash, creed, death, deed, dike, dish,
end, feast, fern, folk, ford, fraud, fuss, grove, heap, hid, hive, hunt, is, mange, no,
nook, not, pan, pants, pest, pile, plush, rag, rat, ride, rise, rub, slip, smile, strife,
such, then, there, toe, use, wheat
List 2
awe, bait, bean, blush, bought, bounce, bud, charge, cloud, corpse, dab, earl, else,
fate, five, frog, gill, gloss, hire, hit, hock, job, log, moose, mute, nab, need, niece,
nut, our, perk, pick, pit, quart, rap, rib, scythe, shoe, sludge, snuff, start, suck, tan,
tang, them, trash, vamp, vast, ways, wish
List 3
ache, air, bald, barb, bead, cape, cast, check, class, crave, crime, deck, dig, dill,
drop, fame, far, fig, flush, gnaw, hurl, jam, law, leave, lush, muck, neck, nest, oak,
path, please, pulse, rate, rouse, shout, sit, size, sob, sped, stag, take, thrash, toil,
trip, turf, vow, wedge, wharf, who, why
List 4
bath, beast, bee, blonde, budge, bus, bush, cloak, course, court, dodge, dupe, earn,
eel, fin, float, frown, hatch, heed, hiss, hot, how, kite, merge, lush, neat, new, oils,
or, peck, pert, pinch, pod, race, rack, rave, raw, rut, sage, scab, shed, shin, sketch,
slap, sour, starve, strap, test, tick, touch
Another test minimizing predictability is Semantically Unpredictable Sentences [23],
with test sets for Dutch, English, French, German, Italian, and Swedish. A short template of
syntactic categories provides a frame, into which words are randomly slotted from the lexi-
con. For example, the template <Subject> <Verb> <Adverbial> might appear as “The chair
ate through the green honesty.” Fifty sentences (10 per syntactic template) are considered
adequate to test a synthesizer. Open transcription is requested, and sentences correct is used

822
Speech Synthesis
to score a synthesizer’s intelligibility. Other such tests exist, and some include systematic
variation of prosody on particular words or phrases as well.
The Harvard Psychoacoustic Sentences [16] is a set of 100 meaningful, syntactically
varied, phonetically balanced sentences, such as “Add salt before you fry the egg,” requiring
an open response identification, instead of a multiple-choice test.
16.6.2.
Overall Quality Tests
While a TTS system has to be intelligible, this does not guarantee user acceptance, because
its quality may be far from that of a human speaker. In this section we describe the Mean
Opinion Score and the Absolute Category Ratings.
Human-subject judgment testing for TTS can adapt methods from speech-coding
evaluation (see Chapter 7). With speech coders, Mean Opinion Score (MOS) is administered
by asking 10 to 30 listeners to rate several sentences of coded speech on a scale of 1 to 5 (1
= Bad, 2 = Poor, 3 = Fair, 4 = Good, 5 = Excellent). The scores are averaged, resulting in an
overall MOS rating for the coder. This kind of methodology can be applied to speech syn-
thesizers as well. Of course, as with any human subject test, it is essential to carefully design
the listening situation and carefully select the subject population, controlling for education,
experience, physiological disorders, dialect, etc. As with any statistically interpreted test, the
standard analyses of score distributions, standard deviation, and confidence intervals must
be performed. The range of quality in coder evaluations by MOS are shown in Table 16.16.
Table 16.16 Mean opinion score (MOS) ratings and typical interpretations.
MOS Scores
Quality
Comments
4.0–4.5
Toll/Network
Near-transparent, “in-person” quality
3.5–4.0
Communications
Natural, highly intelligible, adequate for telecommu-
nications, changes and degradation of quality very
noticeable
2.5–3.5
Synthetic
Usually intelligible, can be unnatural, loss of speaker
recognizability, inadequate levels of naturalness
Since we are making the analogy to coders, certain ironies can be noted. Note the low-
est-range descriptor for coder evaluation: synthetic. In using MOS for synthesis testing, out-
put is being evaluated by implicit reference to real human speech, and the upper range in the
coder MOS interpretations above (3.5–4.5) is probably not applicable to the output of most
TTS systems. Even a good TTS system might fare poorly on such a coder MOS evaluation.
Therefore, the MOS interpretive scale, when applied to synthesis, cannot be absolute as the
above coding-based interpretive table would imply. Furthermore, subjects participating in
MOS-like tests of synthesizers should be made aware of the special nature of the speech
(synthetic) and adjust their expectations accordingly. Finally, no matter how carefully the
test is designed and administered, it is difficult to correlate, compare, and scale such meas-
ures. Nevertheless, MOS tests are perhaps suited to relative ranking of various synthesizers.
The 1-to-5 scale is categorical, but similar judgment tests can be run in magnitude mode,

Evaluation of TTS Systems
823
with the strength of the quality judgment being indicated along a continuous scale, such as a
moving slider bar.
Table 16.17 Listening Quality Scale.
Quality of the Speech
Score
Excellent
5
Good
4
Fair
3
Poor
2
Bad
1
The International Telecommunication Union (ITU) has attempted to specify some
standards for assessing synthetic speech, including spliced digitized words and phrases,
typically with the expectation of delivery over the phone. The Absolute Category Rating
(ACR) system recommended by ITU P.800 offers instructions to be given to subjects for
making category judgments in MOS-style tests of the type discussed here. The first is the
Listening Quality Scale, shown in Table 16.17, and the second the Listening Effort Scale
shown in Table 16.18.
Table 16.18 Listening Effort Scale.
Effort Required to Understand the Meanings of Sentences
Score
Complete relaxation possible; no effort required
5
Attention necessary; no appreciable effort required
4
Moderate effort required
3
Considerable effort required
2
No meaning understood with any feasible effort
1
It is sometimes possible to get subjects to pay particular attention to various particular
features of the utterance, which may be called analytic as opposed to global listening. The
desired features generally have to be described somehow, and these descriptions can be a bit
vague. Thus, standard measures of reliability and validity, as well as result normalization,
must be applied. Typical descriptors for important factors in analytic listening might be:
smoothness, naturalness, pleasantness, clarity, appropriateness, etc., each tied to a particu-
lar underlying target quality identified by the system designers. For example, smoothness
might be a descriptor used when new algorithms for segment concatenation and blending are
being evaluated in a concatenative system. Naturalness might be the quality descriptor when
a formant-based system has been made more natural by incorporation of a richer glottal
source function. Some elements of the speech can be more directly identified to the subject
in familiar terms. For example, Pleasantness might be a way of targeting the pitch contours
for attention, or the subject could be specifically asked to rank the pitch contours per se, in
terms of naturalness, pleasantness, etc. Appropriateness might be a way of getting at judg-

824
Speech Synthesis
ments of accentuation: e.g., a stimulus that was accented as “… birthday PARTY” might be
judged less appropriate, in a neutral semantic context, than one that was perceived as “…
BIRTHDAY party.” But no matter how the attributes are described, in human-subject MOS-
style testing there cannot be a clear and consistent separation of effects.
16.6.3.
Preference Tests
Normalized MOS scores for different TTS systems can be obtained without any direct pref-
erence judgments. If direct comparisons are desired, especially for systems that are infor-
mally judged to be fairly close in quality, another ITU recommendation, the Comparison
Category Rating (CCR) method, may be used. In this method, listeners are presented with a
pair of speech samples on each trial. The order of the system A system B samples is chosen at
random for each trial. On half of the trials, the system A sample is followed by the system B
sample. On the remaining trials, the order is reversed. Listeners use the instructions in Table
16.19 to judge the quality of the second sample relative to that of the first. Sometimes the
granularity can be reduced as much as simply “prefer A/prefer B.”
Assuming (A,B) is the reference presentation order, scores for the (B,A) presentations
may be normalized by reversing their signs (e.g., –1 in B,A order becomes 1, etc.). Subse-
quently, standard statistical summarizations may be performed, like the one described in
Chapter 3.
Table 16.19 Preference ratings between two systems. The quality of the second utterance is
compared to the quality of the first by means of 7 categories. Sometimes only better, same, or
worse are used.
3
Much Better
2
Better
1
Slightly Better
0
About the Same
-1
Slightly Worse
-2
Worse
-3
Much Worse
16.6.4.
Functional Tests
Functional testing places the human subject in the position of carrying out some task related
to, or triggered by, the speech. This can simulate a full field deployment, with a usercentric
task, or can be more of a laboratory situation, with a testcentric task. In the laboratory situa-
tion, various kinds of tasks have been proposed. In analytic mode, functional testing can
enforce isolation of the features to be attended to in the structure of the test stimuli them-
selves. This can lead to a more precise form of result than the MOS judgment approach.
There have been a wide variety of proposals and experiments of this type.

Evaluation of TTS Systems
825
One of the well-known facts in TTS evaluation is that the quality of a system is domi-
nated by the quality of its worst component. While it may be argued that it is impossible to
separate the effects of the front-end analysis and back-end synthesis, it is convenient to do
so to gain a better understanding of each component. An attempt to study the quality of the
speech synthesis module has been done via the use of natural instead of synthetic prosody.
This way, it is presumed that the prosody module is doing the best possible job, and that any
problem is then due to a deficient speech synthesis. The natural pitch contour can be ob-
tained with a pitch tracker (or using a laryngograph signal), and the natural durations can be
obtained either through manual segmentation or through the use of a speech recognition
system used in forced-alignment mode. Plumpe and Meredith [44] conducted a preference
test between original recordings and waveforms created when one of the modules of a con-
catenative TTS system used synthetically generated values instead of the natural values. The
results indicated that using synthetic pitch instead of natural pitch was the cause of largest
degradation according to listeners, and, thus, that pitch generation was the largest bottleneck
in the system. The pitch-generation module was followed by the spectral discontinuities at
the concatenation points, with duration being the least damaging.
Some functional tests are much more creative than simple transcription, however.
They could, in theory, border on related areas, such as memory testing, involving summariz-
ing passages, or following synthesized directions, such as a route on a map. The ultimate test
of synthesis, in conjunction with all other language interface components, is said to be the
Turing test [53]. In this amusing scenario, a human being is placed into conversation with a
computational agent, represented vocally for our purposes, perhaps over the telephone. As
Turing put it: “It is proposed that a machine may be deemed intelligent, if it can act in such a
manner that a human cannot distinguish the machine from another human merely by asking
questions via a mechanical link.” Turing predicted that in the future “an average interrogator
will not have more than a 70 percent chance of making the right identification, human or
computer on the other end, after five minutes of questioning” in this game. A little reflection
might raise objections to this procedure as a check on speech output quality per se, since
some highly intelligent people have speech disabilities, but the basic idea should be clear,
and it remains an amusing Holy Grail for the artificial intelligence field generally. Of
course, no automated or laboratory test can substitute for a real-world trial with paying cus-
tomers.
16.6.5.
Automated Tests
The tests described above always involved the use of human subjects and are the best tests
that can be used to evaluate a TTS system. Unfortunately, they are time consuming and ex-
pensive to conduct. This limits their application to an infrequent use, which can hardly have
any diagnostic value. Automated objective tests usually involve establishing a test corpus of
correctly tagged examples of the tested phenomena, which can be automatically checked.
This style of testing is particularly appropriate when working with isolated components of
the TTS system, for diagnosis or regression testing (glass-box testing). It is not particularly
productive to discuss such testing in the abstract, as the test features must closely track each
system’s design and implementation. Nevertheless, a few typical areas for testing can be

826
Speech Synthesis
noted. In general, tests are simultaneously testing the linguistic model and content as well as
the software implementation of a system, so whenever a discrepancy arises, both possible
sources of error must be considered.
Several automated tests for text analysis and letter-to-sound conversion are presented
in Chapter 14. A number of automated tests for prosody are discussed in Chapter 15. Here
we touch on automated tests for the synthesis module.
The ITU has also created the P.861 proposal for estimating perceptual scores using
automated signal-based measurements. The P.861 specifies a particular technique known as
Perceptual Speech Quality Measurement (PSQM). In this method, for each analysis frame,
various quantified measures based on the time signal, the power spectrum, the Bark power
spectrum, the excitation spectrum, the compressed loudness spectrum, etc. of both the refer-
ence and the test signal can be computed. In some cases the PSQM score can be converted
to an estimated MOS score, with interpretations similar to those of Table 16.16. At present
such methods are limited primarily to analysis of telephone-quality speech (300–3400 Hz
bandwidth), to be compared with closely related reference utterances. This method could
perhaps be adapted to stand in for human judgments during system development of new
versions of modules, say glottal source functions in a formant synthesizer, comparing the
resulting synthetic speech to a standard reference system’s output on a given test sample.
16.7.
HISTORICAL PERSPECTIVE AND FUTURE READING3
In 1779 in St. Petersburg, Russian Professor Christian Kratzenstein explained physiological
differences between five long vowels (/a/, /e/, /i/, /o/, and /u/) and made apparatus to pro-
duce them artificially. He constructed acoustic resonators similar to the human vocal tract
and activated the resonators with vibrating reeds as in music instruments. Von Kempelen
(1734–1804) proposed in 1791 in Vienna a mechanical speaking machine that could produce
not just vowels but whole words and sentences (see Figure 16.19). While working with his
speaking machine, he demonstrated a speaking chess-playing machine. Unfortunately, the
main mechanism of the machine was a concealed, legless chess-player expert. Therefore, his
real speaking machine was not taken as seriously as it should have been. In 1846, Joseph
Faber developed a synthesizer, called speech organ, that had more control of pitch to the
extent it could sing God Save the Queen in a performance in London.
The first electrical synthesis device was introduced by Stewart in 1922 [4]. The device
had a buzzer as excitation and two resonant circuits to model the acoustic resonances of the
vocal tract and was able to generate single static vowel sounds with the first two formants.
In 1932 Japanese researchers Obata and Teshima added a third formant for more intelligible
vowels.
Homer Dudley of Bell Laboratories demonstrated at the 1939 New York World’s Fair
the Voder, the first electrical speech synthesizer, which was human-controlled. The operator
worked at a keyboard, with a wrist bar to control the voicing parameter and a pedal for pitch
control (see Figure 16.20 and Figure 16.21), and it was able to synthesize continuous
3 Chapter 6 includes a historical prospective on representation of speech signals that is intimately tied to speech
synthesis.

Historical Perspective And Future Reading
827
speech. The Pattern Playback is an early talking machine that was built by Franklin S. Coo-
per and his colleagues at Haskins Laboratories in the late 1940s. This device synthesized
sound by passing light through spectrograms that in turn modulated an oscillator with a
fixed F0 of 120 Hz and 50 harmonics.
Figure 16.19 Wheatstone's reconstruction of von Kempelen's speaking machine [14] (after
Flanagan [17]).
Figure 16.20 The Voder developed by Homer Dudley of Bell Labs at the 1939 World’s Fair in
New York. The operator worked at a keyboard, with a wrist bar to control the voicing parame-
ter and a pedal for pitch control.
The first analog parallel formant synthesizer, the Parametric Artificial Talker (PAT),
was developed in 1953 by Walter Lawrence of the Signals Research and Development Es-
tablishment of the British Government. Gunnar Fant of the KTH in Sweden developed an
analog cascade formant synthesizer, the OVE II. Both Lawrence and Fant showed in 1962
that by manually tuning the parameters, a natural sentence could be reproduced reasonably

828
Speech Synthesis
faithfully. Acoustic analog synthesizers were also known as terminal analogs, resonance-
synthesizers. John Holmes tuned by hand the parameters of his formant synthesizer so well
that the average listener could not tell the difference between the synthesized sentence "I
enjoy the simple life" and the natural one [31].
Figure 16.21 Block diagram of the Voder by Homer Dudley, 1939 (after Flanagan [17]).
The first articulatory synthesizer, the DAVO, was developed in 1958 by George Rosen
at M.I.T. Cecil Coker designed rules to control a low-dimensionality articulatory model in
1968. Paul Mermelstein and James Flanagan from Bell Labs also used articulatory synthesis
in 1976. Articulatory synthesis, however, never took off, because formant synthesis was
better understood at the time.
The advent of the digital computer prompted John Kelly and Louis Gerstman to create
in 1961 the first phonemic-synthesis-by-rule program. John Holmes and his colleagues Igna-
tius Mattingly and John Shearme developed a rule program for a formant synthesizer at
JSRU in England. The first full text-to-speech system was developed by Noriko Umeda in
1968 at the Electrotechnical Laboratory of Japan. It was based on an articulatory model and
included a syntactic analysis module with sophisticated heuristics. The speech was quite
intelligible, but monotonous and far from the quality of present systems.
In 1976, Raymond Kurzweil developed a unique reading machine for the blind, a
computer-based device that read printed pages aloud. It was an 80-pound device that shot a
beam of light across each printed page, converting the reflected light into digital data that
was transformed by a computer into synthetic speech. It made reading of all printed material
possible for blind people, whose reading has previously been limited to material translated
into Braille. The work of Dennis Klatt of MIT had a large influence in the field. In 1979
together with Jonathan Allen and Sheri Hunnicut he developed the MITalk system. Two
years later Klatt introduced his famous Klattalk system, which used a new sophisticated
voicing source.
The early 1980s marked the beginning of commercial TTS systems. The Klattalk sys-
tem was the basis of Telesensory Systems’ Prose-2000 commercial TTS system in 1982. It
also formed the basis for Digital Equipment Corporation's DECtalk commercial system in
1983, probably the most widely used TTS system of the twentieth century. The Infovox TTS

Historical Perspective And Future Reading
829
system, the first multilanguage formant synthesizer, was developed in Sweden by Rolf Carl-
son, Bjorn Granstrom, and Sheri Hunnicutt in 1982, and it was a descendant of Gunnar
Fant’s OVE system. The first integrated circuit for speech synthesis was probably the Vo-
trax chip, which consisted of cascade formant synthesizer and simple low-pass smoothing
circuits. In 1978 Richard Gagnon introduced an inexpensive Votrax-based Type-n-Talk sys-
tem. The first work in concatenative speech synthesis was done in 1968 by Rex Dixon and
David Maxey, where diphones were parametrized with formant frequencies and then con-
catenated. In 1977, Joe Olive and his colleagues at Bell Labs [41] concatenated linear-
prediction diphones. In 1982 Street Electronics introduced the Echo system, a diphone con-
catenation synthesizer which was based on a newer version of the same chip as in the Speak-
n-Spell toy introduced by Texas Instruments in 1980.
Concatenative systems started to gain momentum in 1985 with the development of the
PSOLA prosody modification technique by France Telecom’s Charpentier and Moulines.
PSOLA increased the text coverage of concatenative systems by allowing diphones to have
their prosody modified. The hybrid Harmonic/Stochastic (H/S) model of Abrantes [1] has
also been successfully used for prosody modification. The foundation of corpus-based con-
catenative systems was developed by a team of researchers at ATR in Japan in the early
1990s [10, 27]. The use of a large database of long units was also pioneered by researchers
at AcuVoice Inc. Other corpus-based systems have made use of HMMs to automatically
segment speech databases, as well as to serve as units in concatenative synthesis [13, 24]. A
significant milestone for concatenative TTS is that Microsoft integrated it [24] for the mass
market in Windows 2000.
For more detailed description of speech synthesis development and history see, for ex-
ample, [31] and [17] and references in these. A number of audio clips are available in Klatt
[31] showing the progress through the early years. You can hear samples at the Smith-
sonian’s Speech Synthesis History Project [35]. A Web site with comparison of recent TTS
systems can be found at [33].
REFERENCES
[1]
Abrantes, A.J., J.S. Marques, and I.M. Trancoso, "Hybrid Sinusoidal Modeling of Speech
without Voicing Decision," Proc. Eurospeech, 1991, Genoa, Italy pp. 231-234.
[2]
Acero, A., "Source-Filter Models for Time-Scale Pitch-Scale Modification of Speech," Int.
Conf. on Acoustics, Speech and Signal Processing, 1998, Seattle pp. 881-884.
[3]
Acero, A., "Formant Analysis and Synthesis Using Hidden Markov Models," Eurospeech,
1999, Budapest pp. 1047-1050.
[4]
Allen, J., M.S. Hunnicutt, and D.H. Klatt, From Text to Speech: the MITalk System, 1987,
Cambridge, UK, University Press.
[5]
ANSI, Method for Measuring the Intelligibility of Speech Over Communication Systems,
1989, American National Standards Institute.
[6]
Arslan, L.M. and D. Talkin, "Speaker Transformation Using Sentence HMM Based Align-
ments and Detailed Prosody Modification," Int. Conf. on Acoustics, Speech and Signal Proc-
essing, 1998, Seattle pp. 289-292.
[7]
Beutnagel, M., et al., "The AT&T Next-Gen TTS System," Joint Meeting of ASA, 1999,
Berlin pp. 15-19.

830
Speech Synthesis
[8]
Bickley, C.A., K.N. Stevens, and D.R. Williams, "A Framework for Synthesis of Segments
Based on Pseudoarticulatory Parameters" in Progress in Speech Synthesis 1997, New York,
pp. 211-220, Springer-Verlag.
[9]
Bregler, C., M. Covell, and M. Slaney, "Video Rewrite: Driving Visual Speech with Audio,"
ACM Siggraph, 1997, Los Angeles pp. 353-360.
[10]
Campbell, W.N. and A.W. Black, "Prosody and the Selection of Source Units for Concatena-
tive Synthesis" in Progress in Speech Synthesis, J.V. Santen, et al., eds. 1996, pp. 279-292,
Springer Verlag.
[11]
Covell, M., M. Withgott, and M. Slaney, "Mach1: Nouniform Time-Scale Modification of
Speech," Proc. of IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1998, Seattle
pp. 349-352.
[12]
Crochiere, R., "A Weighted Overlap-Add Method of Short Time Fourier Analy-
sis/Synthesis," IEEE Trans. on Acoustics, Speech and Signal Processing, 1980, 28(2), pp.
99-102.
[13]
Donovan, R. and P. Woodland, "Improvements in an HMM-based Speech Synthesizer,"
Proc. of the EuroSpeech Conf., 1995, Madrid pp. 573-576.
[14]
Dudley, H. and T.H. Tarnoczy, "The Speaking Machine of Wolfgang von Kempelen," Jour-
nal of the Acoustical Society of America, 1950, 22, pp. 151-166.
[15]
Dutoit, T., An Introduction to Text-to-Speech Synthesis, 1997, Kluwer Academic Publishers.
[16]
Egan, J., "Articulation Testing Methods," Laryngoscope, 1948, 58, pp. 955-991.
[17]
Flanagan, J., Speech Analysis Synthesis and Perception, 1972, New York, Springer-Verlag.
[18]
Galanes, F.G., et al., "Generation of Lip-Synched Synthetic Faces from Phonetically Clus-
tered Face Movement Data," AVSP, 1998, Terrigal, Australia.
[19]
Gibbon, D., R. Moore, and R. Winski, Handbook of Standards and Resources for Spoken
Language Systems, 1997, Berlin & New York, Walter de Gruyter Publishers.
[20]
Goldstein, M., "Classification of Methods Used for Assesment of Text-to-Speech Systems
According to the Demands Placed on the Listener," Speech Communication, 1995, 16, pp.
225-244.
[21]
Hon, H., et al., "Automatic Generation of Synthesis Units for Trainable Text-to-Speech Sys-
tems," Int. Conf. on Acoustics, Signal and Speech Processing, 1998, Seattle pp. 293-296.
[22]
House, A., et al., "Articulation Testing Methods: Consonantal Differentiation with a Closed
Response Set," Journal of the Acoustical Society of America, 1965, 37, pp. 158-166.
[23]
Howard-Jones, P., SOAP, Speech Output Assessment Package, , 1992, ESPRIT SAM-UCL-
042.
[24]
Huang, X., et al., "Whistler: A Trainable Text-to-Speech System," Int. Conf. on Spoken
Language Processing, 1996, Philadephia pp. 2387-2390.
[25]
Huang, X., et al., "Recent Improvements on Microsoft's Trainable Text-To-Speech System -
Whistler," Int. Conf. on Acoustics, Signal and Speech Processing, 1997, Munich, Germany
pp. 959-962.
[26]
Hunt, A.J. and A.W. Black, "Unit Selection in a Concatenative Speech Synthesis System
Using a Large Speech Database," Int. Conf. on Acoustics, Speech and Signal processing,
1996, Atlanta pp. 373-376.
[27]
Iwahashi, N., N. Kaiki, and Y. Sagisaka, "Concatenation Speech Synthesis by Minimum
Distortion Criteria," IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 1992, San
Francisco pp. 65-68.
[28]
Kain, A. and M. Macon, "Text-to-Speech Voice Adaptation from Sparse Training Data," Int.
Conf. on Spoken Language Systems, 1998, Sydney, Australia pp. 2847-2850.

Historical Perspective And Future Reading
831
[29]
Klatt, D. and L. Klatt, "Analysis, Synthesis and Perception of Voice Quality Variations
among Female and Male Talkers," Journal of the Acoustical Society of America, 1990, 87,
pp. 737-793.
[30]
Klatt, D.H., "Software for a Cascade/Parallel Formant Synthesizer," Journal of Acoustical
Society of America, 1980, 67, pp. 971-995.
[31]
Klatt, D.H., "Review of Text to Speech Conversion for English," Journal of the Acoustical
Society of America, 1987, 82, pp. 737-793.
[32]
Krishnamurthy, A.K. and D.G. Childers, "Two Channel Speech Analysis," IEEE Trans. on
Acoustics, Speech and Signal Processing, 1986, 34, pp. 730-743.
[33]
LDC, Interactive Speech Synthesizer Comparison Site, 2000, http://morph.ldc.upenn.edu.
[34]
Maachi, M., "Coverage of Names," Journal of the Acoustical Society of America, 1993,
94(3), pp. 1842.
[35]
Maxey,
H.,
Smithsonian
Speech
Synthesis
History
Project,
2000,
http://www.mindspring.com/~dmaxey/ssshp/.
[36]
McGurk, H. and J. MacDonald, "Hearing Lips and Seeing Voices," Nature, 1976, 264, pp.
746-748.
[37]
Mermelstein, P. and M.R. Schroeder, "Determination of Smoothed Cross-Sectional Area
Functions of the Vocal Tract from Formant Frequencies," Fifth Int. Congress on Acoustics,
1965.
[38]
Moulines, E. and F. Charpentier, "Pitch-Synchronous Waveform Processing Techniques for
Text-to-Speech Synthesis Using Diphones," Speech Communication, 1990, 9(5), pp. 453-
467.
[39]
Moulines, E. and W. Verhelst, "Prosodic Modifications of Speech" in Speech Coding and
Synthesis, W.B. Kleijn and K.K. Paliwal, eds. 1995, pp. 519-555, Elsevier.
[40]
Nye, P. and J. Gaitenby, The Intelligibility of Synthetic Monosyllabic Words in Short, Syn-
tactically Normal Sentences, 1974, Haskins Laboratories.
[41]
Olive, J., "Rule Synthesis of Speech from Dyadic Units," Int. Conf. on Acoustics, Speech
and Signal Processing, 1977, Hartford, CT pp. 568-570.
[42]
Olive, J.P., A. Greenwood, and J.S. Coleman, Acoustics of American English Speech: a Dy-
namic Approach, 1993, New York, Springer-Verlag.
[43]
Oliveira, L., "Estimation of Source Parameters by Frequency Analysis," Proc. of the Eu-
rospeech Conf., 1993, Berlin pp. 99-102.
[44]
Plumpe, M. and S. Meredith, "Which is More Important in a Concatenative Text-to-Speech
System: Pitch, Duration, or Spectral Discontinuity," Third ESCA/COCOSDA Int. Workshop
on Speech Synthesis, 1998, Jenolan Caves, Australia pp. 231-235.
[45]
Roucos, S. and A. Wilgus, "High Quality Time-Scale Modification of Speech," Int. Conf. on
Acoustics, Speech and Signal Processing, 1985 pp. 493-496.
[46]
Sagisaka, Y. and N. Iwahashi, "Objective Optimization in Algorithms for Text-to-Speech
Synthesis" in Speech Coding and Synthesis, W.B. Kleijn and K.K. Paliwal, eds. 1995, pp.
685-706, Elsevier.
[47]
Santen, J.V., "Combinatorial Issue in Text-to-Speech Synthesis," Proc. of the Eurospeech
Conf., 1997, Rhodes, Greece pp. 2511-2514.
[48]
Sproat, R., Multilingual Text-To-Speech Synthesis: The Bell Labs Approach, 1998,
Dordrecht, Kluwer Academic Publishers.
[49]
Stylianou, Y., "Assessment and Correction of Voice Quality Variabilities in Large Speech
Databases for Concatenative Speech Synthesis," Int. Conf. on Acoustics, Speech and Signal
Processing, 1999, Phoenix, AZ pp. 377-380.
[50]
Stylianou, Y., J. Laroche, and E. Moulines, "High Quality Speech Modification based on a
Harmonic+ Noise mode," Proc Eurospeech, 1995, Madrid.

832
Speech Synthesis
[51]
Syrdal, A., A. Conkie, and Y. Stylianou, "Exploration of Acoustic Correlates in Speaker
Selection for Concatenative Synthesis," Int. Conf. on Spoken Language Processing, 1998,
Sydney, Australia pp. 2743-2746.
[52]
Syrdal, A., et al., "Voice Selection for Speech Synthesis," Journal of the Acoustical Society
of America, 1997, 102, pp. 3191.
[53]
Turing, A.M., "Computing Machinery and Intelligence," Mind, 1950, LIX(236), pp. 433-
460.
[54]
Voiers, W., A. Sharpley, and C. Hehmsoth, Research on Diagnostic Evaluation of Speech
Intelligibility, 1975, Air Force Cambridge Research Laboratories, Bedford, MA.
[55]
Yi, J., Natural Sounding Speech Synthesis Using Variable-Length Units, Masters' Thesis in
EECS Dept. 1998, MIT, Cambridge, MA.

835
C H A P T E R
1 7
Spoken Language Understanding
Formal methods for describing sentences are
discussed in Chapter 11. While the context-free grammars and n-gram models have mathe-
matically well-understood formulations and bounded processing complexity, they are only
partial aids in interpreting semantic meaning of the sentences. Suppose a recognizer cor-
rectly transcribes a series of spoken words into the written form—the system still has no
idea what to do, because there is often no direct mapping between a sequence of words (or
the syntactic structure of the sentence) and the functions that the system provides. The prob-
lem can also be approached from the opposite direction, i.e., solving the recognition problem
itself may require semantic analysis, or domain and language knowledge for perplexity
reduction.
What is meant by meaning or understanding? We could define it operationally: under-
standing is when a computer we interact with understands our desires and delivers the
goods. Or we could define it propositionally: the computer has an accurate and unambiguous
representation of who did what to whom corresponding to a real-world situation. In practice,
the concept of understanding is situation dependent, and both conceptions above have their

836
Spoken Language Understanding
places. Meaning is often a constellation that emerges from a conversational environment.
There are four main interacting areas in spoken language understanding (SLU) systems
from which meaning arises:
 Intent: goals of listener and speaker in the interaction
 Context: the pressures, opportunities, interruptions, etc. of the interaction scene
and communication media
 Content: the propositional or literal content of each utterance and the discourse
as a whole
 Assumptions: what each participant can assume about other participants’ men-
tal state, abilities, limitations, etc.
In this chapter we take a functional view of SLU systems, where the basic principle is
to link linguistic expressions to concrete real-world entities. Currently, only with systems
that are restricted to limited domains can understanding be attempted in practice. The do-
main restrictions allow the creation of specific, highly restricted language models and fully
interpretable semantic descriptions that enable high accuracy and usability. Such systems are
in contrast to speech recognition approaches that use large dictionaries, but make relatively
loose or probabilistic predictions of word sequences for general dictation/transcription.
The need for spoken language understanding is double-edged. We generally want
more than a string of word choices as a system’s output. Instead, we want some interpreta-
tion of the word string that helps in accomplishing complex tasks. At the same time, being
able to determine what makes sense in context, what is more or less likely as a speaker’s
input, could make a major contribution toward improving speech recognition word accuracy
and search efficiency. SLU systems that combine the semantic precision of grammars with
the probabilistic coverage of statistical language models can guide recognition and simulta-
neously control interpretation.
Figure 1.4 in Chapter 1 illustrates a basic SLU system architecture. The SLU problem
can be broadly viewed as yet another pattern recognition problem. Namely, given a speech
input X, the objective of the system is to arrive at actions A (including dialog messages and
necessary operations) so that the cost of choosing A is minimized. Assuming uniform cost,
the optimal solution, known as the maximum a posteriori (MAP) decision, can be expressed
as
*
1
1
,
1
(
|
,
)
(
|
)
argmax
arg max
(
|
,
) (
|
,
)
n
n
n
n
n
S
n
P
S
P
S
P S
S
P
S
−
−
−
=
≈

A
A
F
A
A X
A
F
F X
(17.1)
where F denotes semantic interpretation of X and
n
S , the discourse semantics for the nth
dialog turn.
Based on the formulation in Eq. (17.1), a dialog system is basically three pattern rec-
ognition components:

Written vs. Spoken Languages
837
 Semantic parser—use semantic model
1
(
|
,
)
n
P
S −
F X
to convert X into a collec-
tion of semantic objects F. This component is often further decomposed into
speech recognition module (converting speech signal X into textual sentence W)
and sentence interpretation module (parsing sentence W into semantic objects
F). Since the collection of semantic objects F is in the linguistic level, it is often
referred to as surface semantics.
 Discourse analysis—use discourse model
1
(
|
,
)
n
n
P S
S −
F
to derive new dialog
context
n
S based on the per-turn semantic parse and the previous context
1
n
S −.
 Dialog manager—iterate through the possible actions and pick the most suit-
able one. The quantitative measures governing operations for dialog manager is
called the behavior model,
(
|
)
n
P
S
A
.
The pattern recognition framework can be generalized to multimodal systems as well.
For input other than speech signal, you only need to replace the input X in the semantic
parser with input from an associated modality, e.g., X could be input from keyboard typing,
mouse clicking, pen, video, etc. As long as the new semantic parser (replacing speech rec-
ognizer and sentence interpretation modules in Figure 1.4) can convert it into appropriate
semantic representation, the rest of the system can be identical. Similarly, for different out-
put modality, you just need to replace message generation and text-to-speech modules with
a new rendering mechanism.
In this chapter we first describe the characteristics of spoken languages in comparison
with written languages. The structure of dialog is discussed in Section 17.2. Understanding
is the most fundamental issue in the field of artificial intelligence. The kernel of understand-
ing lies on the representation of semantics (knowledge). Several state-of-the-art semantic
representation schemes are discussed in Section 17.3. Based on the architecture of SLU sys-
tems illustrated in Chapter 1 (Figure 1.4), major modules are discussed in detail, with the Dr.
Who SLU system serving as an example to illustrate important issues.
17.1.
WRITTEN VS. SPOKEN LANGUAGES
To construct SLU systems, we need to understand the characteristics of spoken languages. It
is worth thinking about possible differences between spoken and written use of language
that could be relevant to developing spoken language systems. The following is a typical
example of two-agent, task-oriented dialog in action:
Sys:
Flight reservation service, how can I help you?
User:
One ticket to Honolulu, please
Sys:
Anchorage to Honolulu, when would you like to leave?
User:
Next Thursday
Sys:
Next Tuesday, the 30th of November; and at what time?
User:
No, Thursday, December 2nd, late in the evening, and make it first class.
Sys:
OK, December 2nd United flight 291, first class. Will you need a car or hotel?

838
Spoken Language Understanding
User:
No.
17.1.1.
Style
In both spoken and written forms, a communicative setting is established. Both forms in-
volve participants. In the case of written language, we normally expect passivity on the part
of the addressee(s), though with e-mail bulletin boards, Web chat rooms, and the like, this
assumption can be challenged. The communicative event emerges from personal characteris-
tics of the participants—their mood, goals, and interests. Communication depends both on
the actual world knowledge and shared knowledge of the participants and on their beliefs
about one another’s knowledge. Communication can be influenced by the setting in which it
takes place, whether in spoken or written mode. Also, different subchannels of supportive
communication, such as visual aids, gesture, etc., may be available.
A number of grammatical and stylistic attributes have been found to distinguish con-
versational from written forms. Biber’s analysis [8] distinguishes not only a dimension of
modality, but also formality; for example a panel discussion is a relatively formal, yet spo-
ken, modality. Some typical features for which distinctions can be measured include the
number of passives, the number of pronouns, the use of contractions, and the use of nomi-
nalized forms.1 An example of the grammatical and stylistic difference continuum that Biber
uses is illustrated in Figure 17.1. The variation can be measured along multiple orthogonal
scales for different genres. In the SLU case, style can be orthogonal to the modality (dialog
or dictation, spoken or written). A crossover case is speech dictation used to create a written
document that may never be orally rendered again.
Many nominalizations
and passives
Few nominalizations
and passives
Many pronouns
and
contractions
Few pronouns and
contractions
PANEL
DISCUSSION
SCIENTIFIC
TEXT
CONVERSATION
FICTION
Figure 17.1 Dimensions of written vs spoken language variation.
Fortunately, much of the disjuncture between spoken and written forms in grammati-
cal style and lexical choice can be handled by training task-specific and modality-specific
1 Nominalization is stylistic device whereby a main verb is converted to a noun. For example, The dean rejected the
application unexpectedly may become: The rejection of the application by the dean was unexpected.

Written vs. Spoken Languages
839
language models for the recognizer. For this, only the data need vary, not necessarily the
modeling methods. In Figure 17.1, the right-hand side is toward the spoken style, while the
left-hand side is toward the written one. The difference in styles is best illustrated by the fact
that the statistical n-gram trained from newspaper text exhibits a very high perplexity when
evaluated on conversational Air Travel Information Service (ATIS) texts.
17.1.2.
Disfluency
Another issue for spoken language processing is disfluency. Spoken dialogs show a large set
of problems such as interruptions, corrections, filled pauses, ungrammatical sentences, ellip-
ses, and unconnected phrases. These challenges are unique to spontaneous spoken input and
represent a possible further degradation of speech recognizer performance, as current sys-
tems often rely on acoustic models trained from read speech, and language models trained
on written text corpora. When speech input is used as dictation for document creation, of
course, the models would presumably be most appropriate.
There are a number of types of disfluencies in human-human dialog, and, possibly to a
lesser extent, in human-computer dialog as well. The more common types of nonlinguistic
disfluencies are listed below:
 Filled pauses: um
 Repetitions: the – the
 Repairs: on Thursday – on Friday
 False Starts: I like – what I always get is …
Early work in discourse led to the determination that discourses are divided into dis-
course segments, much as sentences are divided into phrases [18]. In the experiments of
[46], CART methods (see Chapter 4) were used to predict occurrence and location of each
of the above types of disfluency. A tree was trained from labeled corpora for each type, and
the resulting system classified each interword boundary as having no disfluency or one or
more of the above types. The feature types used to derive the classification questions in-
cluded duration of vocalic regions and pauses, fundamental frequency and its derivatives,
signal-to-noise ratios, and distance of the boundary from silence pauses. The basic classifi-
cation task consisted in selecting each of the four disfluency types listed above (D), given
the list of prosodic features (X), by computing the maximum of
(
|
)
P D X . When decision
trees were used to supplement the language-model scoring of hypothesis word strings, per-
formance improved.
A number of intriguing regularities were also observed in this work. For example, it
was noted that the marked (less common) pronunciation of the - /dh iy/ was often used just
prior to a production problem, e.g., a disfluent silent pause. Also, it has been noted that the
leftmost word of a major phrase or clause is likeliest to be repeated, as in their example “I’ll
I’ll do what I can.” Continued research on disfluencies may contribute an important secon-
dary knowledge source to supplement text-based language models and ‘read speech’ acous-
tic models in the future.

840
Spoken Language Understanding
17.1.3.
Communicative Prosody
Prosodic attributes of utterances, such as fundamental frequency and timing (cf. Chapter 15),
are crucial cues for detecting disfluency. However, prosody can be deliberately manipulated
by speakers for deep communicative purposes as well. The speaker may intentionally or
subconsciously manipulate the fundamental frequency, timing, and other aspects of voice
quality to communicate attitude and emotion. If a conversational interface is equipped to
recognize and interpret prosodic effects, these can be taken into account for understanding.
In addition to serving as a disfluency detector, as described above, prosodic analysis
modules could aid recognition of:
 Utterance type—declarative, yes-no questions, wh-question, etc.
 Speech act type—directive, commissive, expressive, representative, declarative,
etc. Different speech acts will be described in Section 17.2.2.
 Speaker’s attentional state.
 Speaker’s attitude toward his/her utterance(s).
 Speaker’s attitude to system presentations.
 Speaker’s mood or emotional state.
Consider the simple utterance OK. This may be used along a range of attitudes and
meanings, from bored contempt, to enthusiastic agreement, to questioning and uncertainty.
The interpretation will depend on both the dialog state context of expectations-to-date and
the prosody. Generally, a higher relative F0 in a wider range correlates with submission,
involvement, questioning, and uncertainty, while a lower relative F0 in a narrower range
correlates with dominance, detachment, assertion, and certainty. Even though acknowl-
edgement words such as yeah and ok are potentially ambiguous among: true agreement;
intention of the listener to initiate a new turn; and simple passive encouragement from lis-
tener to speaker, the system may rely on a longer duration and greater pitch excursion of a
lexical item such as yeah or ok to hypothesize genuine agreement with a speaker statement,
as opposed to mere acknowledgement.
In addition to correlating with speech acts, F0 and timing can be used to demarcate ut-
terance and turn segments. For example, certain boundary pitch movements and phonemic
lengthening systematically signal termination of clauses. In general, a fall to the very bottom
of a speaker’s range, in a prepausal location, coincides with a clause or sentence boundary.
A sharp upturn preceding a significant silence gives an impression of incompletion, perhaps
signaling a yes-no question, or may signal an intention by the speaker to carry on with fur-
ther information, as in the case of list intonation.
The disfluent and prosodic characteristics of the conversational speech are in general
very distinct from those of read speech. Thus, we often refer conversational speech as spon-
taneous speech.

Dialog Structure
841
17.2.
DIALOG STRUCTURE
The analysis methods discussed in Chapter 11 are focused on single sentences. They are
steps along the way, helping to map vague and ambiguous natural language constructions
into precise logical forms of propositions. In reality, however, the communicative function
of language is not a simple, uncomplicated assembly of discrete logical propositions derived
from sentences in a one-to-one fashion. In discourse, each sentence or utterance contributes
to a larger abstract information structure that the user is attempting to construct. Sometimes
feedback is directly available to the user or can be inferred. These considerations take us
beyond the process of mapping of isolated utterances into logically structured propositions
(with simple truth-values).
A set of principles, known collectively as the cooperative principle, is introduced by
Grice [9]. It consists of a set of conversational maxims, the violation of which may lead to a
breakdown in communication.
GRICE'S MAXIMS
Quantity: speaker tries to be as informative as possible, and gives only as much infor-
mation as needed
Quality: speaker tries to be truthful, and does not give information that is false or that
is not supported by evidence
Relevance: speaker tries to be relevant, and says things that are pertinent to the discus-
sion
Manner: speaker tries to be as clear, as brief, and as orderly as possible, and avoids
obscurity and ambiguity
In general, there are five main domains of operation that must be modeled for intelli-
gent conversation systems, although all these areas are linked:
 Linguistic forms: all the knowledge a human-computer dialog system requires
to perform semantic and syntactic analysis and generation of actual utterances.
 Intentional state: goals related to both the task (Show me all flights …), and the
dialog process itself (Please repeat …) of the users.
 Attentional state: the set of entities at any point in time that can be felicitously
discussed and referred to, i.e., the main topic of any stage of interaction.
 World knowledge: common sense knowledge and inference. Examples include
temporal and spatial concepts and the relation of these to linguistic forms.
 Task knowledge: all information relevant to achieving the user’s goal in a com-
plete, correct, and efficient fashion.
Human-computer dialog is multiagent communication. Each agent has to form a no-
tion of the other’s beliefs, desires, and knowledge, all of which underlie their intentions,
plans, and actions. In a limited application, deep inference may not be possible, and the sys-

842
Spoken Language Understanding
tem may have more or less hardwired assumptions about the user, the interaction, and the
flow of action. An interaction may be controlled by the system’s own rigid schedule of in-
formation acquisition. In the research community, such a dialog system—always leading the
interaction flow control and not allowing the user to digress—is called system initiative. On
the other hand, a dialog system is called user initiative if it always lets the user decide what
to do next. It is often more natural, however, to allow for mixed initiative systems, where
interaction starts with a user’s query or command and the system attempts to derive, via in-
ference or further questioning of the user, all information needed to understand and process
a complete transaction. When the user knows clearly what he wants and the system has no
trouble catching up, the user is in the driver’s seat. However, when the system detects that
the user is in a state of confusion, or when it has trouble getting user’s intention, the machine
will offer guidance or negotiate with the user to steer the dialog back on track.
Whether it is system-initiative, user-initiative, or mixed-initiative, however, the fun-
damental structure of dialog consists of initiative-response pairs as indicated in Figure 17.2.
The Initiatives (I) are often issued by users while the Responses (R) are issued by the sys-
tem. As shown in Section 17.2.2, there are many types of Initiatives and Responses and
there may also be higher-order structure subsuming a number of I/R pairs in a dialog.
Figure 17.2 The fundamental structure of dialog: initiative and response.
17.2.1.
Units of Dialog
The words uttered in a dialog are the surface manifestation of a complex underlying layer of
participants’ shared interaction knowledge and desires, even when one participant is a com-
puter simulation. It is natural to assume that the sentence is a clear and simple chunking unit
for dialog, by analogy with written communication. However, since sentences are artificially
delimited in written text, researchers in dialog communication usually speak of the utterance
as the basic unit. An Initiative or Response could consist of one or more utterances. The
utterance, however, is not necessarily trivial to define.
It is tempting to posit an equivalence of the notion utterance with turn, i.e., an uninter-
rupted stream of speech from one participant in a dialog. This formulation makes it easy to
segment dialog data into utterance units—they are just each speaker’s turns. The downside
is that this kind of utterance possibly spans grammatical units that really do have some
rough correspondence to traditional sentences (predicate-argument structures), and to which
much of the hard-won gains in natural language processing would apply fairly directly.
Initiative
(statement or question)
Response

Dialog Structure
843
Thus, the use of turn as synonymous with utterance unit is probably too broad, though the
turn may be independently useful for higher-level segmentation.
Turns are building blocks for constructing a common task-oriented understanding
among participants. This process is called grounding, a set of discourse strategies by which
dialog actors (humans in most current research) attempt to achieve a common understand-
ing, and come to feel confident of the other participants’ understanding. In other words,
conversational partners are finding or establishing common ground.
Turns may have their own typology. For example, a speaking turn conveys new in-
formation, while a back-channel turn is limited to acknowledgement or encouragement,
such as OK, really?, etc. The turns themselves consist of linguistic substructures, such as
sentences, clauses, and phrases. If we assume that turns can be segmented, by grammatical
and/or prosodic criteria, into utterances, we can then begin to explore distinct types of utter-
ances, their properties, and their communicative functions.
Finally, dialogs are not flat streams of unrelated turns or utterances. The utterances
that make up a dialog have higher-order affiliations with one another. A discourse segment
would thus consist of groups of related utterances organized around a common dialog sub-
task, perhaps spanning turns.
17.2.2.
Dialog (Speech) Acts
In simpler applications, the amount and sophistication of world knowledge can be kept to a
minimum, and attentional state can be modeled simply as the complete set of task-specific
entities. A layer of structure has therefore been sought to link linguistic forms with task
knowledge or operations in a theoretically appropriate fashion, which also yields an implicit
understanding of intentional state. This is necessary because the function of utterances in
discourse cannot be predicted strictly on the basis of their surface grammatical form. The
layer of structure that can abstract away from linguistic details and can map well to formula-
tion of goals is called dialog acts [42]. Dialog acts are also often referred to as speech acts
that group infinite families of surface utterances into abstract functional classes. They are
traditionally classified into five broad categories:
 Directive: The speaker wants the listener to do something.
 Commissive: The speaker indicates that s/he herself will do something in future.
 Expressive: The speaker expresses his or her feelings or emotional response.
 Representative: The speaker expresses his or her belief about the truth of a
proposition.
 Declarative: Speaker’s utterance causes a change in external, nonlinguistic
situation.
While this analysis is somewhat coarse, speech act theory has influenced all current
work on human-computer dialog, except the very simplest and most rigid systems. Because
dialog functions can be realized with a bewildering variety of linguistic forms, researchers
have posited systems of functional abstractions. Speech acts are functional abstractions over

844
Spoken Language Understanding
variation in utterance form and content. Declare, request, accept, contradict, withdraw, ac-
knowledge, confirm, and assert are all examples of speech acts—things we are attempting to
do with speech. An example of dialog acts and their relation to syntactic form is shown in
the two-turn dialog in Table 17.1.
Table 17.1 A simple dialog analyzed with dialog acts.
Utterance
Form
Function
Do you have the butter?
Y/N-question
REQUEST-ACT
Sure. (passes butter)
statement
COMMIT-TO-ACTION-ACT
The relation between speech acts and linguistic forms (utterances) is a many-to-many
mapping. That is, a single linguistic form, such as OK, could realize a large number of
speech acts, such as request for acknowledgement or confirm, etc. Likewise, a single speech
act, such as agreement, could be realized by a variety of linguistic forms, such as ok, yes,
you bet, etc. In a particular application, special task-specific speech acts may be used to sup-
plement the universal inventory.
Tagging of dialog utterance data with speech-act labels can add useful information for
training models. There are a number of ways that dialo- act analysis could be useful:
 Speech recognition: Given a history, we can predict the most likely dialog act
type for the next utterance, so that specialized language models may be applied.
 Spoken language understanding: Given a history, and a transcription/parse of
the current utterance, we can identify the user’s intentions, so that the system
can respond appropriately.
 Semantic authoring: It is tedious for each team designing or customizing a new
application area for SLU to have to wrack their brains for all the ways a given
generic function, such as request or confirm, might be realized linguistically.
Libraries of speech acts (form-to-function mappings) may reduce the work in
new-domain adaptation of systems.
An example of a practical dialog tagging system that could be the foundation of
speech-act analysis is the Dialog Act Markup in Several Layers (DAMSL) system [14],
which has been used and adapted for a variety of projects. This is a system for annotating
dialog transcriptions with speech-act labels and corresponding structural elements. The
structuring is based on a loose hierarchy of: discourse segment, turn, utterance, and speech
act. The tags applied to utterances fall into three basic categories:
 Communicative Status: records whether the utterance is intelligible and
whether it was successfully completed. It is mainly used to flag problematic ut-
terances that
should
be
used
for
data
modeling only with caution—
Uninterpretable, Abandoned, or Self-talk. Uninterpretable is self-explanatory.
Abandoned marks utterances that were broken off without, crucially, adding any
information to the dialog. Self-talk is a note that, while an utterance may contain

Dialog Structure
845
useful information, it did not appear to be intentionally communicated. Self-talk
can be considered reliable only when the annotator is working from speech data.
 Information Level: a characterization of the semantic content of the utterance.
This is used to specify the kind of information the utterance mainly conveys. It
includes Task (Doing the task), Task-management (Talking about the task),
Communication-management (Maintaining the communication), and Other-
level. Task utterances relate directly to the business of the transaction and move
it toward completion. Task-management utterances ask or tell about the task,
explain it perhaps, but do not materially move it forward. Communication-
management utterances are about the dialog process and capabilities. The Other
level is for unclear cases.
 The Forward/Backward Looking Function: how the current utterance con-
strains the future/previous beliefs and actions of the participants and affects the
discourse. Forward Looking functions introduce new information or otherwise
move the dialog or task completion forward, while Backward Looking Func-
tions are tied to an antecedent, a prior utterance which they respond to or com-
plete. This distinction is the DAMSL reflection of the common observation that
dialogs have a tendency to consist of Initiation/Response pairs. The core of the
system is the set of particular act types. The core Forward/backward Looking
tags are listed in Table 17.2 and Table 17.3.
Table 17.2 Forward looking tags.
Forward Looking Tags
Example
assert
I always fly first class.
reassert
No, as I said, I always fly first class.
action-directive
Book me a flight to Chicago.
open-option
There’s a red-eye flight tonight …
info-request
What time is it?, Tell me the time.
offer
I can meet at 3 if you're free.
commit
I'll come to your party.
conventional opening
May I help you?
conventional closing
Goodbye.
explicit-performative
Thank you, I apologize.
exclamation
Ouch! Darn!
Multiple tags may appear on any given utterance. In the example shown in Figure
17.3, B’s utterance is coded as opening the option of buying (from B), asserting the exis-
tence of the sofas, and functioning as an offer or solicitation:

846
Spoken Language Understanding
Action-directive
A: Let’s buy the living room furniture
first.
Open-option/Assert/Offer
B: OK, I have a red sofa for $150 or a blue one
for $200
Figure 17.3 A tagged dialog fragment
Table 17.3 Backward looking tags.
Backward Looking Tags
Example
accept
(Will you come ?)
Yes. [and/or, I’ll be there at 10.]
accept-part
(Will you come with your wife?)
I’ll come, she may be busy.
reject
(Will you come?)
No.
reject-part
(Want fries and a shake with that burger?)
Just the hamburger and fries, please.
maybe
Maybe.
signal-nonunderstanding
What did you say?
acknowledgment
OK.
answer
(Can I fly nonstop from Anchorage to Kabul?)
No.
The DAMSL system is actually more complex than the example demonstrated above,
since subsets of the tags are grouped into mutually exclusive options for a given general
speech function. For example, there is a general Agreement function, under which the ac-
cept, accept-part, reject, and reject-part tags are grouped as mutually exclusive options.
Above the level of those groupings, however, a single utterance can receive multiple nonex-
clusive tags. For example, as illustrated in Figure 17.4, the assistant may respond with a
countersuggestion (a kind of action-directive) that rejects part of the original command.
Action-directive
utt1 oper: Take the train to Avon via Bath
Action-directive/Reject-part(utt1)
utt2 asst: Go via Corning instead.
Figure 17.4 A tagged dialog fragment, showing that utterances can be tagged with multiple
nonexclusive tags.
The prototypical dialog turn unit in simple applications would be the I/R pair info-
request/answer, as in the interaction shown in Figure 17.5 between an operator (planner)
and an assistant regarding railroad transport scheduling [1].
The example in Figure 17.5 illustrates a dialog for a railway-scheduling task. The
turns are numbered T1–T4, the utterances within turns are also numbered sequentially, and

Dialog Structure
847
the speaker identity alternates between oper: and asst:. The tagging is incomplete, because,
for example, within the ans| sequence, each utterance is performing a function, asserting,
acknowledging, etc. The example in Figure 17.6 is a more completely annotated fragment,
omitting turn numbers.
info-req
T1 utt1 oper: where are the engines?
ans|
T2 utt2 asst: there's an engine at Avon
|
T3 utt3 oper: okay
|
T4 utt4 asst: and we need
utt5 asst: I mean there's another in Corning
Figure 17.5 A tagged dialog fragment in railroad transport scheduling.
info-req/assert
utt1 oper: and it's gonna take us also an hour
to load boxcars right
ans/accept(utt1) utt2 asst: right
assert
utt3 oper: and it's gonna take us also an hour to
load boxcars
accept(utt1)
utt4 asst: right
Figure 17.6 A tagged dialog fragment, showing backward-looking utterances.
The example in Figure 17.6 shows backward-looking utterances, where the relevant antece-
dent in the dialog is shown (in parentheses) as part of the dialog coding.
More elaborate variants of DAMSL have been developed that extend the basic system
presented here. Consider, for example, the SWITCHBOARD Shallow-Discourse-Function
Annotation SWBD-DAMSL [27]. This project used a shallow discourse tag set of 42 basic
tags (frequent composed tags from the large set of possible multitags) to tag 1155 5-minute
conversations, comprising 205,000 utterances and 1.4 million words, from the SWITCH-
BOARD corpus of telephone conversations. Distributed by the Linguistic Data Consortium2
[28], SWITCHBOARD is a corpus of spontaneous conversations that addresses the growing
need for large multispeaker databases of telephone bandwidth speech. The corpus contains
2430 conversations averaging 6 minutes in length—in other words, over 240 hours of
recorded speech, and about 3 million words of text, spoken by over 500 speakers of both
sexes from every major dialect of American English.
More detailed tags are added to DAMSL to create SWBD-DAMSL, most of which are
elaborations of existing DAMSL broad categories. For example, where DAMSL has the
simple category answer, SWBD-DAMSL has: yes answer, no answer, affirmative non-yes
answer, negative non-no answers, other answers, no plus expansion, yes plus expansion,
statement expanding y/n answer, expansions of y/n answers, and dispreferred answer.
SWBD-DAMSL is intended for the annotation and learning of structure in human-human
dialog, and could be considered overkill as a basis for describing or constructing grammars
for most limited-domain human-computer interactions of today. But the more sophisticated
agent-based services of the future will need to assume ever-greater linguistic sophistication
along these lines.
2 http://www.ldc.upenn.edu

848
Spoken Language Understanding
One fact noted by the SWBD-DAMSL researchers, which may not apply directly to
task-directed human-computer interactions but which casts interesting light on human com-
munication patterns, is that out of 1115 conversations studied, simple nonopinion statements
and brief acknowledgements together constituted 55% of the conversational material! If
statements of opinion (including simple stuff like I think it’s great!), expressions of agree-
ment (That’s right!), turn breakoffs and no-content utterances (So…), and appreciative ac-
knowledgements (I can imagine.) are added to this base, 80% of the utterances are ac-
counted for. This relative poverty of types may bode well for future attempts to annotate and
predict utterance function automatically. The DAMSL scheme is challenging to apply auto-
matically, because it relies on complex linguistic and pragmatic judgments of the trained
annotators.
17.2.3.
Dialog Control
The system’s view of how the dialog should proceed is embodied in its management strat-
egy. Strategy is closely connected to the concept of initiative in dialog, meaning basically
who is controlling the interaction. Different dialog initiatives are defined in Section 17.2.
Initiative can be seen as a continuum from system controlled to user controlled. As back-
ground for the dialog management discussion, some important steps along this continuum
can be identified:
 System directs—The system retains complete dialog control throughout the in-
teraction. The system chooses the content and sequence of all sugoals and initi-
ates any dialog necessary to obtain completion of information from the user for
each transaction. This style is often referred as system initiative.
 System guides—The system may initiate dialog and may maintain a general
plan, but the sequence of information acquisition from the user may be flexible,
and system subgoals and plans may be modified in response to the user’s input.
This style is often referred as mixed initiative.
 System inform—The user directs the dialog and the system responds as helpfully
as possible, which may include presentation of relevant data not specifically re-
quested by the user but which the system believes could be helpful. This style
also belongs to mixed initiative, though users control most of the dialog flows.
 System accepts—This is the typical human-computer interaction in traditional
systems (whether it is a GUI-, command-line-, or natural language-based sys-
tem). The system interprets each command without any attempted inference of a
deeper user plan, or recommendation of any suitable course of action. This style
is referred as user initiative.

Semantic Representation
849
17.3.
SEMANTIC REPRESENTATION
Most SLU systems require an internal representation of meaning that lends itself to com-
puter processing. In other words, we need a way of representing semantic entities, which are
used at every possible step. In general, an SLU system needs to deal with two types of se-
mantic entities. The first type is physical entities, which correspond to the real-world enti-
ties. Such representation is often referred as knowledge representation in the field of artifi-
cial intelligence. The second type is functional entities, which correspond to a way of unam-
biguously representing the meaning or structure of situations, events, and concepts that can
be expressed in natural language. Such representations are often similar to the logical form
introduced in Chapter 2. Processing may include operations such as determining similarity
or identity of events or entities, inference from a state of affairs to its logical consequences,
and so on. Here, we briefly review some general properties of the common semantic repre-
sentation frameworks.
17.3.1.
Semantic Frames
Semantic objects are used to represent real world entities. Here, we assume that the domain
knowledge conforms to a relational or objected-oriented database, of which the schema is
clearly defined. We use the term entity to refer to a data item in the domain (a row in a data-
base table), or a function (command or query) that can be fulfilled in the domain. A column
in the database table is called an entity attribute, and each database table is given an entity
type. Through a small subset of its attributes, an entity can be realized linguistically in many
fashions. We call each of them a semantic class. For example, a person can be referred to in
terms of her full name (Angela), a pronoun anaphora (her), or her relationships to others
(Christina’s manager). In this case, one can derive three semantic classes for the entity type.
Semantic classes can be viewed as a type definition to denote the objects and describe
the relations that hold among them. One of the most popular representations for semantic
classes is the semantic frame [31]—a type of representation in which a semantic class (con-
cept) is defined by an entity and relations represented by a number of attributes (or slots)
with certain values (the attributes are filled in for each instance). Thus, frames are also
known as slot-and-filler structures.
[DOG:]-
[SUPERTYPE]->[mammal]
[NAME]->()
[BREED]->()
[LOC]->()
[Color]->()
Figure 17.7 A semantic frame representation for dog
We could, for example, define a generalized frame for the concept dog, with attributes
that must be filled in for each particular instance of a particular dog. A type definition for
the concept dog appears in Figure 17.7. Many different notational systems have been used

850
Spoken Language Understanding
for frames [51]. For these introductory examples, we use a simple declarative notation that
should be fairly intuitive.
When we need to describe a particular dog, say Lassie, we create an instance defini-
tion, as shown in Figure 17.8. The knowledge base supporting a typical dialog system con-
sists of a set of type definitions, perhaps arranged in an inheritance hierarchy, and a set of
instances.
[DOG:]-
[NAME]->(Lassie)
[BREED]->(Collie)
[LOC]->()
[Color]-()
Figure 17.8 A instance of semantic frame dog.
Fillers in semantic frames can be attained by attachment of inheritance, procedures or
default. Attributes in frame can typically be inherited, as the Lassie instance inherits mam-
malian properties from the DOG type definition. In some cases, properties of a particular
dog may be dynamic. Sometimes attached procedures are used to fill dynamic slots. For
example, the location of a dog may be variable, and if the dog has a Global Positioning Sys-
tem (GPS) chip in its collar, the LOC property could be continually updated by reference to
the GPS calculations. Furthermore, procedures of the type when-needed or when-
filled can also be attached to slots. Finally, some default value could provide a typical
value for a slot when the information for that slot is not yet available. For example, it might
be appropriate to set the default color for dog frame as white when such information is not
available. For frames without a default-value slot, it is natural to define mandatory slots
(slots’ values must be filled) and optional slots (slots could have null value). For the dog
frame, it is reasonable to assume the NAME slot should be mandatory while the COLOR
slot can be optional.
Often descriptions can be attached to slots to establish constraints within or between
frames. Description may have connectives, co-referential (description attached to a slot are
attached to another) and declarative conditions. For example, the return-date slot of a round-
trip itinerary frame must be no earlier than the departure-date slot, and this constraint can be
specified by descriptions in both slots. Descriptions can also be inherited and are often im-
plemented by a special procedure (different from the slot-filling procedure) attached to the
slot.
The main motivation for having multiple semantic classes for each entity type is to
better encapsulate the language, semantic, and behavior models based on the domain knowl-
edge. While the entity relationships capture the domain knowledge, the semantic class hier-
archy represents how knowledge can be expressed in the semantics of a language and thus
can cover linguistic variation. The concept of semantic objects/classes is similar to that of
objects/classes in modern object-oriented programming. The semantic classes3 in Dr. Who
[61] are good illustration of borrowing some important concepts from object-oriented pro-
3 The representation of semantic classes is also referred to as semantic grammars in Dr. Who.

Semantic Representation
851
gramming to enhance the effectiveness and efficiency of using semantic objects/classes to
represent domain knowledge and linguistic expressions.
<!-- semantic class definition for ByRel that has type PER-
SON -->
<class type=”PERSON” name=”ByRel”>
<slot type=”PERSON” name=”person”/>
<slot type=”P_RELATION” name=”p_relation”/>
<cfg>
<prod> [person] [p_relation] </prod>
<prod> [p_relation] of [person] </prod>
</cfg>
</class>
<!-- semantic class definition for ByName that has type PERSON too -->
<class type=”PERSON” name=”ByName”>
<slot type=”FIRSTNAME” name=”firstname”/>
<slot type=”LASTNAME” name=”lastname”/>
<cfg>
<prod> [firstname] [lastname] </prod>
<prod> [firstname] </prod>
<prod> [lastname] </prod>
</cfg>
</class>
<!-- semantic class definition for FIRSTNAME and LASTNAME -->
<verbatim type=”FIRSTNAME”
<cfg>
<prod> john | john’s | peter … </prod>
</cfg>
</verbatim >
<verbatim type=”FIRSTNAME”
<cfg>
<prod> smith | smith’s | shaw … </prod>
</cfg>
</verbatim >
<!-- semantic class definition for P_RELATION -->
<verbatim type=”P_RELATION”
<cfg>
<prod> manager | father | mother | … </prod>
</cfg>
</verbatim >
Figure 17.9 The semantic classes of type PERSON as implemented in Dr. Who.
The semantic grammar used in the Dr Who Project [58] contains the definitions of
semantic classes that refers to real-world or functional entities. A semantic class is defined
as a semantic frame containing set of slots that need to be filled with terminal (verbatim)
words or with recursive semantic class objects. Here ByRel is a semantic class that has the
type PERSON. The semantic grammar specifies that it has two slots—one has to be filled
with an object of a semantic class having the type PERSON, and the other has to be filled

852
Spoken Language Understanding
with an object of a semantic class having the type P_RELATION. On the other hand, the
syntax grammar for this semantic class is specified by the <cfg> tags. Within <cfg> tags,
several production rules can be specify to provide linguistic constraints (orders) of possible
expressions for this semantic class. The syntactic aspect of semantic classes will be de-
scribed further in Section 17.4.1.
17.3.1.1.
Type Abstraction
As described above, an entity is an element in the real world that an application has to deal
with and wishes to expose to the user via natural language. Since an entity can be referred to
in many different ways, different semantic classes may have the same type. In Figure 17.9, a
person can be referred to in terms of his name (Peter) or his relation to another person (Pe-
ter’s manager); therefore, both semantic classes ByName and ByRel can share the same
type, PERSON.
Semantic classes are designed to separate the essential attributes of a semantic object
from its physical realizations. A semantic class may refer to an entity, and the entity is called
the type of the semantic class. The attributes of a semantic class can, in turn, be semantic
classes themselves. The concept behind semantic classes is identical to the mechanism
known as type abstraction commonly employed in software engineering using a strongly
typed programming language. Semantic class can be recursive, as demonstrated in Figure
17.9; a ByRel semantic class of type PERSON contains an attribute of PERSON type. Since
the entities can be nested, i.e., a database column can in turn refer to another table, an attrib-
ute in the semantic class can also be an entity type. From an understanding point of view, a
semantic class is an abstraction of the collection of semantic objects that have the same at-
tributes and usually can be expressed, and hence be understood, in similar manners. Under
this view, a semantic object is just an instantiation.
Another argument for type abstraction is that the multitude of semantic objects is usu-
ally a result of the numerous ways and perspectives that can be used to describe a physical
entity. Quite often in an understanding system it is more important to correctly identify the
entity of interest than to capture the mechanism that describes it. For instance, one may refer
to a person by his name, job function, relations to others, or, in a multimodal environment,
by pointing to his photo on a display. All these references lead to semantic objects that are
apparently distinct yet should be associated with the same physical entity. Accordingly, it is
often useful to segregate the conceptual manifestation and its realizations into different lay-
ers of abstraction so that the semantic objects can be better organized and managed. Type
abstraction allows the discourse sentence interpretation module to perform robust parsing,
since sentence fragments can be parsed into its semantic class type that can be filled into
slots with the same correspondent semantic type, as discussed in Section 17.4.1.
Finally, type abstraction provides a unified framework for resolving relative expres-
sions in the discourse analysis module. Type matching often serves to impose strong con-
straints between real-world entities and relative expressions. The resolution of relative ex-
pressions is discussed in Section 17.5.

Semantic Representation
853
17.3.1.2.
Property Inheritance
Introducing inheritance into the semantic class hierarchy further augments the multilayer
abstraction mentioned above. Class A is said to inherit or be derived from class B if class A
possesses all the attributes of class B. In this case, class A is called the derived class and
class B the base class. Inheritance is a mechanism to propagate knowledge and properties
through the structural relationships of semantic classes. It is crucial for many types of intel-
ligent behavior, such as deducing presumed facts from general knowledge and assuming
default values in lieu of explicit and specific facts.
Perhaps the strongest motivation to employ inheritance is to facilitate the multilayer
abstraction mentioned above. Very often, a base class is constructed with the general proper-
ties of a type of semantic objects, and a collection of more specific classes are derived from
the base class to support the various embodiments of the underlying type of the semantic
objects. For example, a semantic class hierarchy for the reference to a person can have the
methods (e.g., by name, job function) and the media (e.g., speech, handwriting) of reference
as the first layer of derived classes. One can then cross-match the viable means (e.g., by
name via speech, by name via handwriting) and develop the second layer of derived classes
for use in the real applications.
17.3.1.3.
Functionality Encapsulation
The goal of abstraction is to reduce the complexity in describing the world—in this case, the
semantic objects and their relations. One can inspect the quality of abstraction by examining
the extent to which the constructs, i.e., semantic classes, are self-contained, and how prolif-
erating they have to become in order to account for novel scenarios. Studies in data structure
and software engineering propose the notion of encapsulation, which suggest that individual
attributes have local rather than global impacts. This principle also serves as a guideline in
designing the semantic class.
Semantic class encapsulation can be elaborated in two aspects: syntactic and semantic.
The syntactic encapsulation refers to the constraint that each attribute in a semantic class can
only have relations to others from the same class. The collection for these relations is called
the semantic grammar, which specifies how a semantic object of this type can be identified.
In Figure 17.9, the tag <CFG> specifies how the semantic class can be referred to syntacti-
cally via a context-free grammar (CFG). For the class ByRel, the specified syntax indicates
that expressions like Peter’s manager and manager of Peter are legal references to semantic
class ByName. The semantic encapsulation, on the other hand, dictates the actions and the
discourse context under which they may be taken by a semantic class. This is discussed fur-
ther in Section 17.5.
As described in 17.2.2, it is a nontrivial task to determine the types of speech acts. The
semantic frame is an abstraction of the speech acts, the domain knowledge, and sometimes
even the application logic. Once we have this rich semantic representation, how to parse
spoken utterances into the semantic frames becomes the critical task. Nonetheless, the com-
bination of semantic frames and the semantic parser alleviates the need for a dedicated mod-
ule for determining speech acts.

854
Spoken Language Understanding
Semantic frames and associated robust parsing (described in Section 17.4.1) have been
widely used in spoken language understanding. For detailed description of semantic classes
and frames, you can refer to [58, 65].
17.3.2.
Conceptual Graphs
The semantic-representation requirement has led to development of a proposal to standard-
ize the logical form that may form the basis of the internal semantics and semantic inter-
change of natural language systems, including dialog processing, information retrieval, and
machine translation. The proposal is based on conceptual graphs derived from Charles
Sanders Peirce [38] and the various types of semantic networks used in artificial intelligence
research.
[Fly] –
(Agent)->[Person: Eric]
(Dest)->[City: Boston]
(Inst)->[Airplane]
Figure 17.10 A linear form representation of Fly has an agent who is a person, Eric, and a des-
tination Boston.
The conceptual graph (CG) proposal [53] specifies the syntax and semantics of con-
ceptual graphs as well as formats for graphical and character-based representation and ma-
chine-based exchange. In the terms of the proposed standard, a conceptual graph (CG) is an
abstract representation for logic with nodes called concepts and conceptual relations, linked
together by arcs. In the graphical representation of a CG, concepts are represented by rec-
tangles, and conceptual relations are represented by circles or ovals. The ordinary phrasing
for the association of relations (circles) to concepts (rectangles) is has a(n) for arrows point-
ing toward the circle and is a(n) for arrows pointing away.
Figure 17.11 CG display form for Lars is going to Oslo by train [53].
Figure 17.11 illustrates a conceptual graph for the sentence Eric is flying to Boston by
a airplane. The mnemonic meaning of the arrows is: Fly has an agent who is a person, Eric,
and a destination Boston. The proposal also specifies a linear form, as shown in Figure
17.10. In the form, concepts are in square brackets and conceptual relations are in parenthe-
Person:
Eric
AGENT
Fly
DEST
City:
Boston
INST
Airplane

Sentence Interpretation
855
ses. The hyphen means that relations of a given concept continue on subsequent lines, as
shown in Figure 17.11.
Figure 17.12 CG Display Form for a person is between a rock and a hard place [53].
Each concept has a type and a (possibly empty) referent. An empty referent means that
at least one, unspecified example of the type is assumed to exist somewhere (an existential
quantifier). So, in Figure 17.10, the type is present, but the referent is left unspecified. In an
application, the referent can be completed by referring to a train-schedule database and in-
serting a particular instance of a scheduled train departure time, location, and number. The
valence of a relation is the number of required concepts that it links. For example, as shown
in Figure 17.12, the relation between would be a conceptual relation of valence 3, because
typically (something/somebody) is between one (something/somebody) and another (some-
thing/somebody), as in the familiar English idiom “somebody is between a rock and a hard
place” (meaning, to be in great difficulty). This corresponds to the linear form, as shown in
Figure 17.13.
[Person]<-(Betw) –
<-1-[Rock]
<-2-[Place]->(Attr)->[Hard]
Figure 17.13 A linear form representation of A person is between a rock and a hard place.
17.4.
SENTENCE INTERPRETATION
We follow the convention of most modern SLU systems—treating semantic parser as a two-
step pattern recognition problem (speech recognition followed by sentence interpretation).
This convention has the advantage of modular design of SLU systems. Thus, the same SLU
system can be used for text input. However, a unified semantic parser [50, 62] may achieve
better accuracy, because no hard decision needs to be made before picking the optimal se-
mantic representation.
The heart and soul of the sentence interpretation module is how to convert (translate)
a user’s query (sentence) into the semantic representation. In other words, one has to fill the
semantic slots with information derived from the content (words) in the sentence. In this
Person
BETW
Rock
Place
ATTR
Hard

856
Spoken Language Understanding
section we describe two popular approaches to accomplish this task. Although they can be
perceived as pattern matching methods, they differ in the matching mechanism.
17.4.1.
Robust Parsing
Due to the nested nature of semantic classes, a semantic object F in Eq. (17.1) can itself be a
tree of semantic objects. A user’s utterance may consist of disjoint fragments that may make
sense at the discourse level. For instance, in the context of setting up a meeting, the utter-
ance “Peter Duke at a quarter to two” can be parsed into two semantic objects: a person and
the meeting time. Therefore, the sentence interpretation module must deal with sentence
fragments.
The analysis of spoken language is a more challenging task than the analysis of writ-
ten text. The major issues that come to play in parsing spontaneous speech are speech dis-
fluencies, the looser notion of grammaticality that is characteristic of spoken language, and
the lack of clearly marked sentence boundaries. The contamination of the input with errors
of a speech recognizer can further exacerbate these problems. Most natural language parsing
algorithms are designed to analyze grammatical input. These algorithms are designed to
detect ungrammatical input at the earliest possible opportunity and to reject any input that is
found to be ungrammatical in even the slightest way. This property, which requires the
parser to make a complete and absolute distinction between grammatical and ungrammatical
input, makes such formal parsers fragile for spontaneous speech, where completely gram-
matical input is the exception more than the rule. This is why a robust parser is needed.
In Chapter 11, context-free grammars (CFG) can be written to analyze the structure of
entire sentences. It is natural to extend CFG as a pattern matching vehicle. For example, a
question such as “Where would you like to go?” might be used to solicit a response from a
user, who might respond, “I would like to fly to Boston.” The following grammar might be
used to parse the response:
S →NP VP
NP →N
VP →VCluster PP
VCluster →would like to V
V →go | fly
PP →prep NP
N →Boston | I
Prep →to
The resulting phrase structure, characterizing the entire sentence, would be:
[S [NP [N I ] ] [VP [VCluster would like to [V fly ] ] [PP
[prep to ] [NP [N Boston]]]]]]
This structure in turn can provide the foundation for subsequent semantic analysis.
Thus, the grammar is adequate for the example response and can be easily extended to cover

Sentence Interpretation
857
more city names by expanding the N →rule, i.e., by enlarging the lexicon. It has some defi-
ciencies, however. Some of the problems are purely formal or logical in nature, such as the
fact that “Boston would like to go to I” can be equally parsed. These flaws can be addressed
with formal fixes (e.g., a more refined category system), but, in any case, they are not cru-
cial for the practical system designer, because pathological examples are rare in real life.
The deeper problem is how to deal with legitimate, natural variations.
The user might respond with any of the following:
To Boston
I’m going to Boston.
Well, I want to start in New York and get to Boston by the
day after tomorrow.
I’m in a big hurry; I’ve got a meeting in Boston.
OK, um, wait a second… OK, I think I’ve gotta head for Bos-
ton.
The above sentences incorporate different kinds of variation for which a sentence cov-
erage grammar typically has trouble accounting. For this reason, dialog system designers
have gravitated to the idea of robust parsing. Robust parsing is the idea of extracting all and
only the usable chunks of simple meaning from an utterance, ignoring the rest or treating it
as noise or filler. Small grammars can be written that scan a word lattice (see Chapter 13) or
a word sequence for just those particular items in which they specialize. For example, a Des-
tination grammar, not intended to span an entire utterance, can skim each of the complex
utterances above and find the Destination in each case:
Destination →Preposition CityName
Preposition →to | for | in
CityName →Boston | …
The noise or filler elements might include nonspeech noise (cough, laugh, breath, hesi-
tation), elements of phatic communication (greetings, polite constructions), irrelevant com-
ments, unnecessary detail, etc. As a user becomes accustomed to the limited yet practical
domain of a system’s operations, it is expected that variant phrasings would diminish, since
they take longer to utter and contribute very little, though disfluencies would always be an
issue.
The original word graph or lattice from the speech recognizer might consist of nodes,
representing points in time, and edges representing word hypotheses and acoustic scores for
a given span in the utterance. Figure 17.14 illustrates a sample of word graph for the exam-
ple “I would like to fly to Boston” with competing hypotheses. Using the Destination gram-
mar on the word graph in Figure 17.14 will skip the earlier parts of the possible sentence
hypotheses and identify the short fragment from node 6 to node 8 as a destination. If only
the Destination grammar were active, a new view of the word graph would result in Figure
17.15.

858
Spoken Language Understanding
This example shows that potential and legitimate ambiguities can persist even with
flexible grammars of this type, but the key potential meanings have been identified. A robust
parser that is capable of handling the example needs to solve the following three problems:
 Chunking: appropriate segmentation of text into syntactically meaningful units;
 Disambiguation: selecting the unique semantically and pragmatically correct
analysis from the potentially large number of syntactically legitimate ones re-
turned; and
• Undergeneration: dealing with cases of input outside the system’s lexical or
syntactic coverage.
Figure 17.14 Word graph for hypotheses [61].
Grammars developed for spontaneous speech should concentrate on describing the
structure of the meaningful clauses and sentences that are embedded in the spoken utterance.
The goal of the parser is to facilitate the extraction of these meaningful clauses from the
utterance, while disregarding the surrounding disfluencies. We use the semantic grammar in
the Dr. Who SLU engine [61] to illustrate how this works.
As mentioned in Section 17.3.1, a Dr. Who grammar mostly contains set of slots that
need to be filled with terminal words (verbatim) or with recursive nonterminal semantic
classes. Strictly speaking, this semantic class grammar can hardly be called a grammar,
since it is primarily used to define the conceptual relations among Dr. Who entities rather
than the language expressions that are used to refer to the entities. The syntactic expression
is specified by optional CFGs associated with each semantic class. In general, the syntactic
grammars need to deal with three kinds of variation in surface linguistic form:
1. Variation within a slot—When CFG is missing in the definition of semantic
classes, the grammar could allow flexible assembly of an expression. For ex-
ample, if the <cfg> tags in Figure 17.9 are omitted, any sequence that con-
1
3
4
2
5
6
7
8
I
would
like
to
fly
to
Boston
Inglewood
right
today
to
Austin

Sentence Interpretation
859
tains a word of a P_RELATION typed class and a word of a PERSON typed
class can be an expression referring to a semantic object of ByRel such as
John’s father, father of John or even John loves his father. Thus, CFGs are
often specified within the semantic slot to provide linguistic constraints with-
out over-generating.
2. Variation in the order of frame presentation—Many systems [64, 66] employ
an island-driven robust parsing strategy where the slots in the semantic
frames are filled by language fragments from parsing. Parsing of the slots is
order independent. Thus utterances “Schedule a meeting with John at 3 PM”
and “Schedule a meeting at 3 PM with John” can be processed without prob-
lems.
3. Disfluencies and irrelevancies—Disfluencies and irrelevancies are unavoid-
able for spoken language input. The system has to deal with real utterances
such as “I’d really like to know whether a meeting by 3 PM would be at all
possible for John.”
Figure 17.15 Word graph for hypotheses if only the Destination grammar is active [61].
To cope with these requirements, the robust parsing algorithm [61] is typically imple-
mented as an extension of the bottom-up chart-parsing algorithm discussed in Chapter 11.
There are a number of improvements:
 The requirement that a hypothesis and a partial parse have to cover adjacent
words in the input is relaxed here. This effectively skips the words and enables
the parser to omit unwanted words in input sentences.
 The combination of a hypothesis with a new partial parse taken from agenda re-
sults in multiple new hypotheses. Those hypotheses may have different critical
position number. In other words, they are expecting different partial parses. This
1
3
4
2
5
6
7
8
FILLER
Destination: Boston
Destination: Austin

860
Spoken Language Understanding
effectively skips the symbols in a rule, so the parser can continue its operation
even if something expected by the grammar does not exist.
 The sequential order in which the partial parses are taken out from the agenda is
crucial here. A partial parse that has the minimum span and highest score and
that covers the word closest to the sentence start position (in that order) has the
highest priority.
In a robust parser, if there is already a parse g that has the same symbol and span as
the new parse h, we need to compare their scores so we only keep the better one. The parse
scoring can be the likelihood of the parse with respect to a heuristic CFG enhanced with a
mechanism of assigning probability for insertions and deletions. It can also be based on heu-
ristics when no training data is available. The typical heuristic values may include the num-
ber of words covered by a parse; the number of rule symbols skipped in the parse tree; the
number of nodes in the parse tree; the depth of the parse tree; and the leftmost position of
the word covered by the parse.
17.4.2.
Statistical Pattern Matching
The use of CFGs to capture the semantic meaning of an utterance can be augmented with
probabilistic CFGs or the unified language model described in Chapter 11. In the statistical
parser, the application developers first define semantic nonterminal and preterminal nodes.
A large number of sentences are then collected and annotated with these semantic nodes.
The statistical training methods are used to build the parser to extract semantic meaning
from an utterance.
For example, a statistical parsing algorithm [15, 26] takes one step further toward
automatic discovery of complex CFG rules. Instead of relying on hand-written CFG rules, it
builds a statistical parser based on the tree-banked data where sentences are labeled with
parsing-tree structure. It identifies simple named classes like Date, Amount, Fund, or Per-
cent and only handles simple classes using the local context. Words that are not part of a
class are tagged as word, indicating that the word is passed on to the subsequent parser. The
subsequent statistical parser takes a classed sentence. It generates the most likely semantic
parse in a bottom-up leftmost derivation order. At each step in the derivation, the parsers use
CART (see Chapter 4) to assign probabilities to primitive parser actions such as assigning a
tag to a word or deciding when to begin a new constituent. A beam search is used to find the
parse with highest probability. The two-step parsing for the sentence “Please transfer one
hundred dollars from voyager fund to fidelity fund” is illustrated in Figure 17.16.
The hidden understanding model (HUM) [29, 30] is another statistical pattern match-
ing techniques. Let W denote the sequence of words and S denote the meaning of the utter-
ance. According to Bayes’ rule, we have the following equation:
(
|
) ( )
(
|
)
(
)
P
S P S
P S
P
=
W
W
W
(17.2)

Sentence Interpretation
861
The task of sentence interpretation can then be translated into finding the meaning represen-
tation ˆS , such that
ˆ
arg max
(
|
) ( )
S
S
P
S P S
=
W
(17.3)
(a)
(b)
Figure 17.16 An example class tree in IBM’s statistical class parser. (a) The sentence is classi-
fied into semantic classes. (b) The classed sentence is parsed into the semantic tree based on
CART [15].
transfer
S
hundred
one
dollars
from
voyager
fund
to
fidelity
fund
please
word
word
amount
amount
amount
word
fund
fund
word
fund
fund
AMOUNT
FUND
FUND
transfer

862
Spoken Language Understanding
( )
P S
is the semantic language model that specifies the prior statistical distribution of
meaning expressions. The semantic language model is based on a tree-structured meaning
representation where concepts are represented as nodes in a semantic tree with subconcepts
represented as child nodes. Figure 17.17 illustrates such a tree-structured meaning represen-
tation for the sentence “United flight 203 from Dallas to Atlanta.” The Flight concept has
Airline, Flight_Ind, Flt_Num, Origin, and Destination subconcepts. Origin and Destination
subconcepts have terminal nodes Origin_Ind and City; Dest_Ind ,and City, respectively.
Each terminal node (like City) could be composed of a word or of a sequence of words.
FLIGHT
[AIRLINE[United]
FLIGHT_IND[flight]
FLIGHT_NUM[203]
ORIGIN[ORIGIN_IND[from] CITY[DALLAS]]
DESTINATION[DEST_IND[to] CITY[Atlanta]]]
Figure 17.17 A tree-structured meaning representation for “United flight 203 from Dallas to
Atlanta in BBN’s HUM system [29]..
Semantic language model
( )
P S
is modeled as
1
(
|
,
)
i
i
P S
S
concept
−
, where concept is
the
parent
concept
for
iS
and
1
iS −.
Based
on
this
definition,
the
probability
(Destination | Origin,Flight)
P
is bigger than
(Origin | Destination,Flight)
P
, since users
often omit the origin for a flight in an airline reservation system.
(
|
)
P
S
W
is called a lexical realization model, which is basically a word bigram
model augmented with the context of the parent concept:
1
(
|
)
(
|
,
)
i
i
P
S
P w
w
concept
−
= ∏
W
(17.4)
Both the semantic language model and lexical realization model are estimated from a la-
beled corpus. Viterbi search is applied to find the best path of meaning representation ˆS
according to Eq. (17.3).
17.5.
DISCOURSE ANALYSIS
The sentence interpretation module only attempts to interpret each sentence without knowl-
edge about the current dialog status or discourse. As we mentioned in Section 17.2, some-
times it is impossible to get the right interpretation without discourse knowledge. For exam-
ple, in the sentence “Show me the morning flight” one must have the knowledge what “the
morning flight” refers to in order to derive the real-world entitiy, even though the sentence
interpretation module comprehends perfectly what morning flight means.
Discourse information formed by dialog history is necessary not only for semantic in-
ference but also for inconsistency detection. Inconsistency detection is important in a dialog
system, since the dialog management module (described in Section 17.6) needs such infor-
mation to disambiguate the dialog flow when needed. For example, in an airline reservation

Discourse Analysis
863
system, the returning date should not proceed the departure date, which may be conveyed in
the previous dialog turns. The discourse analysis module needs to maintain a stack of dis-
course trees so that the semantic representation remains the same whether the information is
obtained through several dialog turns or a single one.
The goal of the discourse analysis module is to collapse the discourse tree by resolv-
ing the semantic objects into the domain entities. This process is also called semantic
evaluation. When the resolution is successful, the semantic object is officially bound to the
domain entities. The last process is often called semantic binding. Because an entity can be
identified by partial information (e.g., last name of a person), binding is necessary for the
system to grasp the whole attributes of the objects the dialog is concerned with. Semantic
binding is also critical for intelligent behaviors such as setting the discourse context for ref-
erence resolution. The semantic evaluation and binding are the basics for driving the dialog
flow. The communication mechanism between discourse analysis and dialog manager is
typically event driven. Events that can be passed to the dialog manager are evaluation suc-
ceeded, evaluation failed, invalid information, and value to be determined. The discourse
analysis module often needs to tap into the knowledge base with the semantic object attrib-
utes and entity memory for semantic evaluation. The semantic evaluation usually proceeds
from the leaves up toward the root of the discourse tree. The process ends when the root
node is converted, which indicates the dialog goal has been achieved. The functions of Dis-
course analysis module are the following:
 Converting the relative expressions (like tomorrow, next week, he, it, the morn-
ing flight etc) in the semantic slots into real-world objects or concepts (such as
1/5/2000, the week of 2/7/2000, John, John’s dog, etc).
 Automatic inference—Based on dialog history, the module may decide some
missing information for certain slots. For example, an airline reservation system
could infer the destination city for the origin of the return flight even though it is
not specified.
 Inconsistency/ambiguity detection—Since the discourse analysis module can
perform automatic inference for some slots, it can perform consistency checking
when it is explicitly specified during the current dialog turn.
17.5.1.
Resolution of Relative Expression
There are two types of relative expression. The first type is the reference, relating linguistic
expressions to real-world entities. This may involve disambiguation, by inference or direct
user query. When a user says, “Give me Eric’s phone number,” many people with first name
Eric may exist in the database. The second type of relative expression is the co-reference.
Co-reference occurs when different names or referring expressions are used to signify the
same real-world entity. For example, in the sentence “Nelson Mandela has a long history of
leadership within the African National Congress, but he is aging and nobody was surprised
yesterday when Mandela announced his successor” the terms Nelson Mandela and Mandela
refer to the same person.

864
Spoken Language Understanding
In linguistics, there are three different types of co-references. The example above is an
ellipsis, where the omitted word(s) can be understood from the context. The other type is
deixis. A deixis refers to the use of a word such as that, now, tomorrow, or here, whose full
meaning depends on the extralinguistic context in which it is used. Location deictic co-
references are very common for multimodal applications where pointing devices (modali-
ties) like pens can be used to indicate the real locations. The most common type of co-
reference is anaphora, which is a special type of co-reference, where a word or phrase has
an indirect, dependent meaning, standing for another word or concept previously introduced.
The pronoun he in the sentence above is an anaphor referring to Nelson Mandela too.
Time deictic co-references like tomorrow, next week, the week of 2/7/2000, etc., are
among the easiest category for resolution (requiring only simple domain knowledge). The
resolution of other relative expressions usually requires deep natural language processing.
We focus our discussion on anaphora resolution, since it represents the most challenge one
among others and approaches of solving this problem are typical of the kind of methods
appropriate for resolving a variety of other relative expressions.
17.5.1.1.
Priority Entity Memory
We introduce a simple resolution method [60] that is based on semantic class type abstrac-
tion and priority entity memory. This method is straightforward and is very powerful to han-
dle most cases even without complex natural language processing.
Whenever a conversion of a relative expression occurs, the consequent entity is added
to the entity memory. The entity memory consists of turn and discourse memories. Either
type of memory consists of a number of priority queues that are delineated by entity types.
An entity can only be remembered into the queue of compatible types (e.g., through inheri-
tance). When referred to, the memory item increases its priority in the queue. This treatment
resembles the cache language model described in Chapter 11.
The turn memory is a cache for holding entities in each turn. There are two types of
turn memories. The explicit memory holds the entities that are resolved directly from seman-
tic objects. In contrast, the implicit memory is for entities that are deduced from relative
expressions. In accessing the memory, the explicit turn memory takes precedent over the
discourse memory, which in turn has a higher priority than the implicit. At the end of the
system’s turn, all the turn memory items are moved and sorted into the discourse memory.
The distinctions between the three kinds of memories and the rules to operate them are
designed as a simple mechanism for most common but not all possible scenarios. It is worth
noting that the design has a bias toward direct and backward reference. For example, in the
expression “Forward this mail to John, his manager, and his assistant,” the second his will
be evaluated as referring to John, not to his manager. The implicit memory, however, pro-
vides a back-off for expressions like “Send email to John, his manager, and her assistant”
in which the pronoun her should be taken as indicating John’s manager is a female and re-
solved accordingly. However, since we store only the entities and not the semantic objects
into the memory, the mechanism is not suitable for forward or pleonastic references, as in
the examples like “Since his promotion last May, John has been working very hard” or “It

Discourse Analysis
865
being so nice, John moved the meeting outside.” Fortunately, these natural language phe-
nomena are rare in a spoken dialog environment.
It is sensible to confirm4 the resolved entities with users due to possible resolution er-
rors. In cases where many entities in the entity memory can be matched with a semantic
object, a decision of not performing any resolution and directly inquiring the user for disam-
biguation may be a better solution. In general, name references can be resolved by a se-
quence of simple rules. In the example of “Give me Eric’s phone number” the SLU system
may just generate the query message “What is Eric’s last name?” when many people in the
entity memory have the same name Eric.
17.5.1.2.
Resolution by Full NLP
Extensive understanding is crucial for perfect resolution for relative expressions (in particu-
lar, anaphora). Though morphology, lexical semantics, and syntax can be helpful for disam-
biguation, ultimately it is a problem of inference using real-world knowledge and dialog
state or context. In a discourse model of focus, it is assumed that speakers usually center
their attention on a single main topic called the focus. Some utterances introduce or reintro-
duce a focus; others elaborate on it. Focus elements typically change (by being suspended,
interrupted, resumed, etc.) over the course of a dialog. Once a focus element has been intro-
duced, anaphora is usually used to represent it, making dialog more efficient.
Anaphora resolution specifies the referent of a pronoun or other anaphoric expression.
This association should be supported by inference about properties and probabilities in the
real world. Anaphora resolution can be done with a simple entity focus principle. For exam-
ple, in the very common schedule a meeting type of dialog application, an exchange such as
that shown in Figure 17.18 is centered on the initial focus element—the proposed meeting—
and anaphora are likely to relate to that central topic, at least early on in the exchange. The
subscript indicates the co-reference to the same entity. The focus is the meeting proposed in
(1). The pronoun it in (2), by the very simple mechanism discussed here, can be interpreted
as referring to the meeting. Some grammatical knowledge and the semantic class type
should help the system to resolve him in (3) as Jim rather than the meeting. In sentence (3)
the focus has shifted to the action of taking a cab, to which that refers in sentence (4). The
locative here in (3) must also be resolved to the speaker’s location.
(1) I’d like to schedule a [meeting]i with [Christoph]j.
(2) [It]i can be anytime after 4.
(3) Tell [him]j [he]j can [grab a cab over here]k.
(4) [That]k should be only if he’s running late.
Figure 17.18 A schedule a meeting dialog example showing different anaphora usage.
Most formal models of anaphora resolution originated from research into discourse
and human-human dialog. They tend to be overpowered, in making elaborate provision for
greater topic and reference variation than exists in typical computer speech dialog applica-
4 One might decide which confirmation strategies (explicit or implicit confirmation) to use based on the confidence
of the resolutions. The details of confirmation strategies are described in Section 17.6.

866
Spoken Language Understanding
tions of the present time. On the other hand, while they can provide resolution for some
complicated situations, they tend to be underpowered, in failing to deal robustly with the
realities of imperfect speech recognition and parsing.
Some of the work on anaphora resolution in dialog relies on elaborate focus-tracking
mechanisms [47]. These tend to be somewhat circular in nature, in that anaphoric reference
resolution is required for the focus-tracking algorithms to operate, while the anaphoric reso-
lution itself relies on the currently identified focus structure of the dialog or discourse.
Rather than elaborate on these possibilities, we instead present a number of relatively
straightforward heuristics for anaphora resolution, some of which have been developed
based on textual studies, but which may be relevant to increasingly complex human-
computer dialog in the future. The discussion here is limited to the resolution of intersenten-
tial and intrasentential pronominal anaphora. Full noun-phrase anaphora, where one syn-
onymous noun phrase is co-referent with another, requires even more powerful grammatical
and semantic resources.
Syntactic conditions can be tested when a parse tree showing syntactic constituency is
available. The most obvious syntactic filter for disallowing co-reference is simple gram-
matical feature agreement. For example, the following proposed co-indexed relation is not
semantically possible in ordinary discourse, and the restriction is explicitly provided through
the lexical morphology and syntax of the language:
The [girl]i thought [he]i was frightening.
Though the theoretical details can be complex [37], the basic intuition of syntax-based
anaphoric resolution is that nonreflexive pronouns that are syntactically too close to a candi-
date co-referential NP (antecedent) are disfavored. For example, in a sentence such as:
[Bill’s]i photo of [him]i is offensive.
the coindexing of Bill with him is disallowed. By disallowed, we mean that your innate
sense of proper English grammar and interpretation will balk at the proposed relation. The
language provides a mechanism to override some proximity restrictions, as in the following
repaired version:
[Bill’s]i photo of [himself]i is offensive.
So, when is a pronoun too close to a possible antecedent? The most important syntac-
tic concepts for determining anaphoric relations rely on structural attributes of parse trees. In
fact, treatment of this problem represents a very large and highly argumentative subfield
within theoretical linguistics. Nevertheless, any treatment of anaphora resolution on purely
syntactic grounds is very likely to end with a list of conditions that can mostly be subsumed
under some form of x-bar theory [25], as it is called in the theoretical linguistics.
17.5.2.
Automatic Inference and Inconsistency Detection
Automatic inference can be carried out through the same framework of priority entity mem-
ory described in Section 17.5.1.1. During semantic evaluation, a partially filled semantic
object is first compared with the entities in the memory based on the type compatibility. If a

Dialog Management
867
candidate is found, the discourse analysis module then computes a goodness-of-fit score by
consulting the knowledge base and considering the position of the entity in the memory list.
The semantic object is converted immediately to the entity from the memory if the score
exceeds the threshold. In the process, all the actions implied by the entities are carried out
following the order in which the corresponding semantic objects are converted.
In general, automatic inference can be implemented as description procedures attached
to semantic slots as described in Section 17.3.1. In the example of an airline reservation sys-
tem, a procedure or rule can be attached to automatically infer the destination city for the
returning flight. The other powerful strategy for automatic inference is slot inheritance.
When changing dialog turn for different semantic objects under the same service, the system
may allow such slot inheritance to free users from repeating the same attributes. For exam-
ple, after a user asks “What is Peter Hon’s office number?” he may abbreviate his next query
to “How about Derek Acero’s?” Slot inheritance will allow the second semantic object re-
garding Derek Acero to inherit the office number slot even though it is not explicitly speci-
fied.
Inconsistency checking is crucial to initiate necessary events for dialog repair. A dia-
log may be diverted away from the ideal flow for various reasons (e.g., misrecognition, out-
of-domain reference, conflicting information), many of which require domain- and applica-
tion-specific knowledge to guide the dialog back to the desired course. This process is called
dialog repair. Similar to automatic inference, inconsistency checking can be implemented as
description procedures attached to semantic slots. In addition, inconsistency checking can
also be triggered when semantic binding for a partially filled semantic object fails (e.g., in-
dicated by a failed database lookup). The discourse analysis module is responsible only for
sending the dialog repair events to the dialog manager, and it leaves the realization of the
repair strategy to the corresponding event handler in the dialog manager.
For example, consider a query: “Find me the cheapest flight from Seattle to Memphis
on Sunday.” The semantic binding fails because there is actually no flight available on Sun-
day from Seattle to Memphis based on the flight database. Thus, the discourse manager
passes such event to the dialog manager, and the dialog manager will generate an appropri-
ate message to let the users be aware of this fact.
17.6.
DIALOG MANAGEMENT
For most applications, it is highly unlikely that a user can access or retrieve the desired in-
formation with just a single query. The query might be incomplete, imprecise, and some-
timed inconsistent with respect to the discourse history. Even if the query is unambiguous,
the speech recognition and sentence interpretation modules in a SLU system may make mis-
takes. Thus the SLU system needs to provide an interactive mechanism to perform clarifica-
tion, completion, confirmation, and negotiation dialogs with users. By default, the objective
of such a dialog is to help users accomplish the required tasks more efficiently. Being user-
friendly is also one of the major objectives for dialog systems as discussed in Chapter 18.
Since the goal of a SLU system is to provide a natural conversation interface for users, the
ultimate SLU system should act like a real human, yet still possessing perfect memory and

868
Spoken Language Understanding
superfast computation. Based on these criteria, it is not hard to see why mix-initiative sys-
tems are preferred over system-initiative systems.
The dialog manager controls the interactive strategy and flow once the semantic mean-
ing of the query is extracted and stored in the system’s representation (discourse trees). The
architecture of SLU dialog systems resembles the one used in event-driven GUI systems. In
the same way that GUI events are assigned to graphical objects, the dialog events are as-
signed to semantic objects that encapsulate the knowledge for handling events under various
discourse contexts. As mentioned in Section 17.5, the discourse tree with domain entity
binding is passed along with necessary dialog events generated from the discourse analysis
module to the dialog manager. The dialog manager acts as an intelligent domain knowledge
handler that uses the semantic meaning of the query to check against domain-specific
knowledge (including domain database and application logic) and generates the desired an-
swer for the query or produces other necessary dialog strategy.
In this sense, the dialog manager functions as a GUI application that contains an event
handler. The event handler handles dialog events passed from the discourse analysis module
and generates appropriate responses to engage users to solve the problems. In addition, the
dialog manager needs to implement the application logic to generate appropriate actions
(e.g., make real airline and hotel reservation). In this section we discuss two modeling tech-
niques for implementing application logic, and different dialog behaviors related to event
handling.
17.6.1.
Dialog Grammars
Dialog grammars use constrained, well-understood formalisms such as finite state machines
to express sequencing regularities in dialogs, termed adjacency pairs. The rules state se-
quential and hierarchical constraints on acceptable dialogs, just as syntactic grammar rules
state constraints on grammatically acceptable strings. For example, an answer or a request
for clarification is likely to follow a question, just as a finite state grammar might provide
for a noun or an adjective, but not a verb, to follow a determiner such as the. In most dialog
grammar systems, dialog-act types (explain, complain, request, etc. cf. Section 17.2.2) are
categorized ,and the categories are used as terminals in the dialog grammar. This approach
has the advantage that the formalism is simple and tractable. At every stage of processing
the system has a basis for setting expectations, which may correspond to activating state-
dependent language models, and for setting thresholds for rejection and requests for clarifi-
cation.
In its essence, the dialog grammar model is exemplified by a rigid flowchart dia-
gramming system control of the type and sequence of interaction. Figure 17.19 shows a fi-
nite state dialog grammar for an airline reservation SLU system. In this simple example,
dialog-act categorization is omitted, and the interactions are controlled based on bare infor-
mation items. This grammar makes simple claims: the interaction is basically question-
answer; the topic queries are answered on-topic if possible, and presumably with a confir-
mation statement to catch the existence of a problem.
This system is easily programmed. The challenge lies in providing tools to application
authors to ease the tedium and minimize the errors in the construction of grammars, and to

Dialog Management
869
allow for more flexibility and spontaneous deviations from the expected transitions in the
grammar. Such deviations may be important for novice users, who may more naturally tend
to give their information (origin, destination, time) in one single utterance or in a different
order.
Figure 17.19 A finite state dialog grammar for airline reservation (after [19]).
In general, dialog grammar approach has the following potential disadvantages
 The interaction may be experienced by a user as brittle, inflexible, and unforgiv-
ing, since it is difficult to support mix-initiative systems.
 Dialog grammars have difficulty with nonliteral language (indirection, irony,
etc.).
Is it a one-way
trip?
where you want to leave from?
where you want to go?
Did you say
when you want to leave?
Did you say
<TIME>?
No
No
Did you say
<FROM>?
No
No
Stop

870
Spoken Language Understanding
 A speech act might be expressed by several utterances, complicating the gram-
mar.
 A single utterance might express several speech acts, complicating the grammar.
To address these issues, more sophisticated approaches to enhance hand-built finite state
dialog grammars have been attempted. For example, once can add statistical knowledge
based on realistic data to dialog grammars. The statistical learning methods, like come
CART, n-grams, or neural networks [3] can be used to learn the association between utter-
ances and states in the training data.
17.6.2.
Plan-Based Systems
Plan-based approaches [2, 41] seek to overcome the rigidity and shallowness of dialog
grammars and templates. They are based on the observation that humans plan their actions
to achieve various goals. Thus, plans and goals are in some degree of correspondence. A
system operating under these assumptions needs to infer goals, construct and activate plans.
A user may have a preconceived plan for achieving his/her goals or may need to rely on the
system to supplement or construct appropriate plans.
Plan-based systems are well studied in artificial intelligence (AI) [32, 65]. The
mathematical foundation of the plan-based approach is inference. The behaviors of the sys-
tem and the knowledge of the domain are programmed as a set of logical rules and axioms.
The system interacts with the user to gather facts, which consequently trigger rules and gen-
erate more facts as the interaction progresses. As illustrated in Eq. (17.1), the goal of the
dialog manager is to derive the action A based on discourse semantic
n
S . Taking this view,
the dialog manager is a natural outgrow of the semantic evaluation process. It is the step
where the system’s intent is computed. The outcome of the dialog manager is a message (via
different rendering) the system conveys to the user.
In essence, a plan-based system is an embodiment of a state machine for which dif-
ferent discourse semantics are regarded as states. The difference, however, is that the states
for the plan-based system are generated dynamically and not limited to a predetermined fi-
nite set. This capability of handling an unbounded number of states is a key strength of plan-
based systems in terms of scalability.
Even a simple interaction can involve a variety of complex subgoals and pragmatic in-
ferences. A partial plan for the airline reservation example in Section 17.6.1 is illustrated in
Figure 17.20. One wants to know if a flight itinerary (F12) is an available one. The relation-
ships among the goals and actions that compose a plan can be represented as a directed
graph, with goals, preconditions, actions, and effects as nodes and relationships among these
as arcs. These graphs illustrate the compositional nature of plans, which always include
nested subplans, down to an almost infinite level of detail. The appropriate level of planning
specification is thus a judgment call and must be application dependent.
The arcs are labeled with the relationship that holds between any two nodes. SUB
shows that the child arc is the beginning of a subplan for the parent. At some point appropri-
ate to the domain of the planning application, the SUBs will be suspended and represented

Dialog Management
871
as a single subsuming node. In Figure 17.20, ENABLE indicates a precondition on a goal or
action. EFFECT indicates the result of an action. ENABLE indicates an enabling relation-
ship between parent and child nodes.
Figure 17.20 A partial plan for the airline reservation example in Figure 17.19 represented as a
graph.
Plan-based approaches incorporate a rich and deep model of rational behavior and,
thus, in theory, permit a more flexible mode of interaction than do dialog grammar ap-
proaches. However, they can be complex to construct and operate in practice, due to reliance
on logical and pragmatic inference, and due to the fact that no fully understood theoretical
underpinning exists for their specification. The complexity of the domain of modeling often
requires significant efforts from human experts to author the logical rules and axioms.
In plan-based theories of agent interaction, each dialog participant needs to construct
and maintain a model of all participants’ goals, commitments, and beliefs. Plans are, thus, a
relatively abstract notion, leading to the hope that plans could be designed in an application-
independent fashion, which would permit the development of plan libraries. Such libraries
could be easily adapted to a variety of domains; just as specific entity models are derived
from generic classes via inheritance in object-oriented programming.
The following operational cycle exemplifies the plan approach, describing interaction
of two agents, X (the helpful assisting agent) and Y (the client). Interaction is stated from
X’s point of view [10].
Available_Fligh(F12)
Origin(F12,C1)
Outbound_Leg(F12,L1)
Same_city (C1,C4)
Dest(F12,C2)
Time(F12,T1)
Roundtrip(F12)
SUB
SUB
ENABLE
EFFECT
Inbound_Leg(F12,L2)
SUB
Origin(F12,C3)
Dest(F12,C4)
Time(F12,T2)
SUB
SUB
EFFECT
EFFECT
SUB
SUB
SUB
Later (T2,T1)
Same_city (C2,C3)

872
Spoken Language Understanding
 Observe Y’s act(s)
 Infer Y’s plan (using X’s model of Y’s beliefs and goals)
 Debug Y’s plan, finding obstacles to success of plan, based on X’s beliefs
 Adopt the negation of the obstacles as X’s goal
 Plan to achieve those goals, and execute the plan
A flight itinerary that at least contains an Outbound_Leg subgoal and another possible
Inbound_Leg subgoal is a round trip. Let’s assume F12 is a round trip itinerary. At the In-
bound_Leg
node,
the
interesting question is
how
much of the
underlying
goal
(Time(F12,T2), Origin(F12,C3) and Dest(F12,C4)) can be inferred by the information pro-
vided by the system from the dialog so far, or from other known conditions. For example,
the destination of the Inbound_Leg can be inferred from the origin in the outbound leg. The
origin city can be inferred similarly. Going one step further, you can also infer that the de-
parture time for the inbound leg must occur after the departure time of the outbound leg (T1
< T2). Those three inference sare shown in the Effect arcs in Figure 17.20.
The goal inference could be a cooperative process, with the system making the mini-
mal queries needed to verify and choose among alternative hypotheses. Or, it could be based
on pure inference, with perhaps a confirmation step. Inference modeling can get very com-
plicated. The technologies of inference are complex models of the beliefs, desires, and inten-
tions of agents, making use of generic logical systems, which operate over the propositions
corresponding to the nodes in a plan structure such as shown in Figure 17.20. Both user and
system are assumed to be operating from partially shared world and discourse models con-
sisting of beliefs about all relevant entities and their relationships. If utterances and speech
acts are not in conflict with the constraints implied by the world models, communication and
action can proceed. Otherwise, either the utterance itself must be further interpreted, sup-
plemented, or clarified, or the world models need to be changed.
The natural expression of rational behavior, communication, and cooperation is some
form of first-order logic. We define axioms and inference rules for Belief and Intention. If
the modal operator for belief is B, axioms and inference rules for an agent i with respect to
proposition schemata φ or ψ could be formalized in the following logical expression.
(
( )
(
))
( )
( )
( )
(
( ))
( )
(
( ))
( )
(
( ))
( )
(
)
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
B
B
B
B
B
B
B B
B
B
B
B
B B
xB
B
x
φ
φ
ψ
ψ
φ
φ
φ
φ
φ
φ
φ
φ
φ
φ
∧


 ¬ ¬

¬

¬
¬
 ¬
∀

∀
(17.5)
These describe appropriate conditions on beliefs of rational agents, such as entailment
and consistency. Intentions, in turn, are formalized with respect to beliefs. For example, if an
agent is to form an intention to bring about a state of affairs, it is reasonable that s/he be-
lieves this state of affairs is not currently in force:

Dialog Management
873
( )
(
)
i
i
I
B
φ
φ

¬
(17.6)
Other such axioms formalize related constraints on intentions, e.g., having an intention
entails a commitment to achieving any preconditions, and belief in the possibility of doing
so. Many more axioms involving all aspects of rational behavior, and formalizing, to some
extent, the Gricean Maxims can be devised. For example, a kind of conversational coopera-
tion occurs when a participant i is willing to come to believe what i believes his/her conver-
sational partner j is attempting to communicate (at least for the limited operational domains
in question!), unless i holds beliefs to the contrary:
(
(
( ( ))))
(
( ))
( ( ))
i
j
i
i
i
B I
B
j
B
j
B
j
φ
φ
φ
∧¬
¬

(17.7)
When beliefs and intentions are modeled in this fashion, it may be possible to directly con-
struct the core of a dialog engine based on rational principles as a theorem prover. Such a
treatment is, however, beyond the scope of this discussion.
A few desirable system behaviors that would naturally follow from limited infer-
ence and goal tracking can be briefly examined. Unlike the dialog grammar approach, a
plan-based system allows digression, since the user’s intention model has been built into the
plan. When a system is confused about a user’s input, a cooperative system could begin to
perform the critical pragmatic steps that uniquely distinguish the conversational interface. A
chain of inferring the user’s goal, based on the system’s axioms, dialog history, and current
knowledge, would be triggered.
It is essential for a system to track the dialog focus, or temporary centers of atten-
tion, in order to understand things that are unspoken but assumed to be salient across utter-
ances. In this case, the user’s input is ambiguous—June 22 is for outbound or inbound
flight? If the dialog architecture provides a method of tracking focus, it may be simple to
resolve the legs from an earlier query.
Focus is a useful concept in dialog understanding. The basic idea is similar to the
entity memory tracking in anaphora resolution (see Section 17.5.1.1)—at any given point in
a conversational exchange, a few items are at the center of attention and are given prefer-
ence in disambiguation. Other items are in the background but may be revitalized as centers
of attention at some later point. A static area can be used to contain items that are assumed
background knowledge throughout the exchange. The main goal of conversation can initial-
ize the stack. As subgoals are elaborated, new focus sets are pushed on the stack, and when
these subgoals are exhausted, the corresponding focus object is popped from the stack and
earlier, presumably broader topics are resumed. Focus shifts that are not naturally character-
ized as refinements of a broader current topic may be modeled by initiating a new independ-
ent focus stack. Focus shifts may be cued by characteristic linguistic signals, such as cue
words and phrases (well now, ok!, by the way, wait!, hey, etc.). In many cases, focus struc-
ture tracks the recursively embedded plan structures, such as that shown in Figure 17.20.

874
Spoken Language Understanding
17.6.3.
Dialog Behavior
Even though the behavior of the dialog manager is highly dependent on the domain knowl-
edge and the applications, some general styles of dialog behavior are worth investigating.
The first important dialog behavior is the dialog initiative strategies. System initiative sys-
tems have the advantage of narrowing the possible inputs from users, while paying the price
for extreme inflexibility. Although user initiative strategy is often adopted for GUI-based
systems, it is seldom implemented for SLU systems, since total flexibility is translated into
high perplexity (resulting low system performance). For many applications, a flexible mixed
initiative style is preferred over a rigidly controlled one. Although it is possible to imple-
ment a mixed initiative system using either dialog grammars or plan-based approach, the
latter is more flexible because it can handling an unbounded number of states.
Most often, the response generated by the dialog manager is either a confirmation or a
negotiation. Confirmation is important due to possible SLU errors. There are two major con-
firmation strategies—explicit or implicit confirmations. An explicit confirmation is a re-
sponse solely for confirmation of what the system has heard. On the other hand, an implicit
confirmation is a response containing new input query and embedded confirmation with the
hope that the user can catch and correct the errors if the embedded confirmation is wrong.
The examples in Figure 17.21 illustrate both confirmation strategies.
I:
I would like to fly to Boston.
R1: Do you want to fly to Boston?
(explicit confirmation)
R2: When do you want the flight to Boston?
(implicit confirmation)
Figure 17.21 With the input I would like to fly to Boston, explicit confirmation response R1
Do you want to fly to Boston? only allows the user to confirm the destination, while implicit
confirmation response R2 When do you want the flight to Boston? allows the user to provide
departure-time information and have a chance to confirm the destination as well.
SLU systems usually use a confidence measure as to when to use explicit and implicit
confirmation. Obviously, explicit confirmation is used for low-confidence semantic objects
while implicit confirmation is for high-confidence ones.
A negotiation response can arise whether a semantic object is fully filled or not. In the
case of underspecification, there are some attributes of the semantic objects that cannot be
inferred by the discourse manager. Possible actions range from simply pursuing the unfilled
attributes in a predefined order, to gathering the entities in the knowledge base sorted by
various keys. For cases of ill specification, an entity that matches the semantic object attrib-
utes does not exist. The planner can simply report such fact, or suggest removal or replace-
ment of certain attributes, depending on how much domain knowledge is to be included in
the planning process.
Often in the design process, we find it desirable to segregate a dialog into several self-
contained sessions, each of which can employ specialized language, semantic, and even be-
havior models to further improve the system performance. Basically, these sessions are sub-
goals of the dialog, which usually manifest themselves as trunk nodes on the discourse tree.
We implement a tree stack in which each trunk node is treated as the root for a discourse
tree. The stack is managed in a first-in last-out fashion, as currently no digression is allowed

Dialog Management
875
from one subdialog to another. So far, the no-digression rule is considered to be a reasonable
trade-off for dynamic model swapping.
Consider the example domain of travel itinerary planning [13]. At the top level is the
scenario, which is the intended output of the interaction. The scenario is the entire itinerary,
consisting of reservations for flights, hotels, rental cars, etc., all booked for the user at
workable, coordinated times and acceptable prices and quality levels. A scenario might be: a
flight out of the user’s home city of Boston, from Logan airport, on April 2, at 4:00 PM on a
particular flight, connecting in Dallas-Ft. Worth to another flight to a regional airport, an
overnight hotel stay, a meeting the next day in the morning, a drive to a second local after-
noon meeting, a flight from the regional airport in the evening to LA for a late meeting, an-
other overnight stay in LA, a morning meeting at the hotel, and a return flight back to Bos-
ton later that same morning.
Figure 17.22 A dialog structure hierarchy for travel.
Creating the finished itinerary for this scenario involves goals, generated by the sys-
tem or the user. Goals might include: access user travel profile, book outbound and inbound
flights, and make local arrangement (hotel reservation and car rental). Goals in turn may
subsume subgoals. Subgoals are concerned with the details of planning. These would in-
clude establishing particular desired cities and airports for the flights, price investigation,
queries about hotel location and quality, etc. The subgoals in their turn are generally realized
via speech acts forming I/R pairs. A simplified schematic of the structure of the itinerary
structure described above might appear as shown in Figure 17.22.
This structure lends itself to a variety of control mechanisms, including system-led and
mixed initiative. For example, the system may ask guiding questions such as “Where would
you like to go?” followed by “What day would you like to leave?” or the system could begin
Scenario
Itinerary
Profile
Outbound
Inbound
Local
Goals
Subgoals
Flight
…
Flight
…
Local
…
Dest
Date
Time
I/R

876
Spoken Language Understanding
processing from the user’s point of view by accepting an utterance like I want to go from
Boston to LA, corresponding to the Dest node of a flight on the outbound flight, and re-
sponding with a query about the next needed item, e.g., What day would you like to leave?
This system can also accommodate a user who may wish to talk about his or her hotel reser-
vation immediately after making the outbound flight reservation, before arranging the in-
bound flight.
17.7.
RESPONSE GENERATION AND RENDITION
Response generation, also known as the message generation, is the process in which the
message is physically presented to the user. This is the stage that significantly involves hu-
man-factor issues, as discussed in Chapter 18. It is more susceptible to application-specific
or user interface considerations. For example, to handle a message requesting the user to
select a sizable list of alternatives, a system with a suitable visual display might choose to
present the whole list, while a speech-only system might require a more clever way. In this
section we mainly focus on speech output modality and provide some thoughts on other
popular output modalities.
A conversational interactive system requires a speech-output capability. The speech
output may be comprised of system requests for clarification, disambiguation or repeat of
garbled input; confirmation; prompting for missing information; statements of system capa-
bilities or expectations; and presentation of results. At the lowest level, this is done via a
text-to-speech engine, as discussed in Part III (Chapters 14, 15 and 16) and shown as a com-
ponent in Figure 1.4. However, most text-to-speech engines have been designed for a read
speech style. Moreover, such systems typically perform only shallow syntactic and semantic
analysis of their input texts to recover some text features that may have prosodic correlates.
Because the topic space of a task-oriented dialog system is narrower, there are opportunities
to tune prosodic and other attributes of the speech output for better quality.
There are two major concerns in voice-response rendering. First is the creation or se-
lection of the content to be spoken, and second is the rendition of it, which may include spe-
cial prosodic markups as guidance to a TTS engine.
17.7.1.
Response Content Generation
The response content can be explicitly tied to the semantic representation of the domain task
and objects. The semantic grammar could incorporate custom prompts for specific slots or
even for whole semantic classes. Whenever the dialog manager finds that specification for a
particular slot is missing from a semantic class represented as a frame, it can consult the
grammar to see if it contains prompts. If prompts are present, one could be selected at ran-
dom for presentation to the user.
Response prompts can be embedded in semantic representation. Prompts are usually
provided for each slot to provide direction for users to fill the slot in the next dialog term.

Response Generation And Rendition
877
For example, the semantic class ByName defined in Figure 17.9 can be enhanced with the
prompts in Figure 17.23.
<!-- semantic class definition for ByName that has type PER-
SON too -->
<class type=”PERSON” name=”ByName”>
<slot type=”FIRSTNAME” name=”firstname”
prompt=”Please specify the last name for [firstname]/>
<slot type=”LASTNAME” name=”lastname”
prompt=”Please specify the first name for [lasttname]/>
/>
<cfg>
……………
</cfg>
</class>
Figure 17.23 Semantic class ByName in Figure 17.9 is enhanced with prompts specified for
the case of missing a particular slot information.
Prompts could be associated with conditions. For example, in a flight information sys-
tem, a conditional prompt can be inserted into the semantic class definition to inform users
of the flight arrival time based on whether the flight has landed or not, as shown in Figure
17.24.
<class type=”FLIGHT” name=”Flight”>
<slot type=”FLIGHTNO” name=”flight_no”>
<slot type=”TIME” name=”sch_time”>
<slot type=”TIME” name=”actu_time”>
<slot type=”CITY” name=”dep_city”>
<slot type=”CITY” name=”arr_city”>
<slot type=”AIRLINE” name=”airline”>
<prompt condition= “$SYS_TIME > [actu_time]”>
Flight [flight_no] is landed at [actu_time]
</prompt>
<prompt condition= “default”>
Flight [flight_no] is schedule to land at [sch_time]
</prompt>
<cfg>
………………………
</cfg>
</class>
Figure 17.24 A semantic class Flight contains a conditional prompt to inform users when inva-
lid [depart_time] is detected.
Other systems may include some categorization of prompts for different functions. For
example, at the task level of an airline reservation system, the categorized message list
might appear as shown in Figure 17.25. The grammar format makes provision for conven-
ient authoring of messages that can be specified and accessed by functional type at runtime.
The BEMsg is a special type of message. In this particular architecture, communication with

878
Spoken Language Understanding
the database engine (cf. the boxes application and database in Figure 17.2) is controlled by
messages that are authored in the task specification. The URL attribute indicates a database
access. The /rclist is the set of possible return codes from the back-end application (as it
attempts to perform the specified command from the message). Again, every return condi-
tion is associated with a message by the task specification author. Those shown here include
a simple confirmation of a successful completion, as well as a warning for flight sold out and
a generic failure of transaction message.
<messages>
<msg id=”Help”> Please specify the flight time, origin and destination </msg>
<msg id=”Cancel”> Canceling itinerary… </msg>
<msg id=”Confirm”> Buying ticket from [origin] to [dest] on [time]? </msg>
<msg id=”BEMsg” url=”http://server/…?op=buy&time=[time]&flight=[flight]..”>
<rclist>
<rc id=”OK”> Complete buying </rc>
<rc id=”SO”> The flight is sold out </rc>
<rc id=”ERROR”> Cannot complete transaction </rc>
</rclist>
</msg>
</message>
Figure 17.25 An example of categorization of prompts for an airline reservation SLU system.
Such systems can incorporate other kinds of categorization as well. For example, a
system might provide a battery of responses to a given task or subtask situation, varying
depending on a speech recognition confidence metric. Thus a set of utterances ordered by
decreasing confidence might appear as:
You want to flight to Boston?
Did you say the Boston?
Could you repeat that, please?
Please state a flight reservation.
Systems of this type are sometimes referred to as template systems for response gen-
eration. They have the advantages of direct authoring and simplicity of implementation and
may provide very high quality if the message templates of the application can be mirrored
with matching digitized speech utterances or carrier phrases in the synthesizer.
The specificity and application-dependent qualities of template-based systems are
sometimes perceived as weaknesses that could potentially be overcome by more general,
flexible, and intelligent systems. In these systems the message generation box could sub-
sume discrete modules, as shown in Figure 17.26. The semantic representation would typi-
cally be akin to logical forms (see Chapter 2) expressed via semantic frames or conceptual
graphs. The representation would include abstract expression of content as well as speech-
act type and other information to guide the tactical or low-level aspects of utterance genera-
tion, such as word choice, sentence type choice, grammatical arrangement, etc.

Response Generation And Rendition
879
Natural language generation from abstract semantic input is a deep and complex field.
Let us briefly consider a slightly more abstract form of template-selection mechanism that
could gracefully either accommodate a simple set of static, authored response utterances or,
alternatively, serve as a form of semantic input to a generalized, NLP-based utterance gen-
eration module. Imagine that instead of simply providing lists of prompt strings with em-
bedded slot identifiers, a system of parameterization can be used [24]. The parameters could
be at varying levels of abstraction and would function as descriptors of static content when
preauthored prompts were being used, or would serve as a kind of input semantic representa-
tion when a general natural language was used. The set of parameters might include attrib-
utes of utterances such as the following:
Figure 17.26 Subsystems of message generation and rendition.
 Utterance type: mood of the sentence, i.e., declarative, wh-question, yes/no
question, or imperative.
 Dialog or speech act: confirmation, suggestion, request, command, warning, etc.
 Body: some characteristic lexical content for the utterance, apart from any situa-
tion-dependent words and concepts. This could serve as a hint to a generator. In
many cases this would be the main verb of a sentence and might also include
characteristic cue words, especially for functional transitions, e.g., however,
now, etc.
Message (synthetic speech)
Lexicon
Grammar
Phonological &
Prosodic Rules
Utterance Generation
Phonological &
Prosodic Processing
Semantic Representation, from Dialog manager
Templates,
Units,
Rules for Speech
Synthesis
Speech Synthesis
tagges text

880
Spoken Language Understanding
 Given: information that is understood from the discourse history. This is usually
represented as pronouns or other anaphora in the generated utterance.
 New: anything that is in the informational foreground, due to lack of prior men-
tion, but may not be precisely the purpose of the prompt, per se. New material
typically receives some kind of prosodic prominence in speech.
Examples of these parameter indices for templates from a theater ticket-reservation
domain might appear as in Table 17.4. The basic idea of the parametric approach is that such
a level of medium abstraction allows for flexibility in the choice of deployment tactics. If a
full set of static prompts and response utterances is available for all cases, then this approach
reduces to a template system, though it does provide the potential for separation of gram-
mars and prompt files. If, however, a natural language generation component is available for
dynamic message generation, a parameter set like that above can serve as input.
Table 17.4 Sentence generation indices for an airline reservation SLU system.
Act
Type
Body
Given
New
Example
Meta [sorry]
Decl
no
-
-
No, sorry.
Verify
Y/N-Q
Boston
Boston?
Request-info
WH-Q
fly
you
thing
When do you want to fly?
Request-info
WH-Q
want
you
airline
tomorrow
Which airline would you like
to fly tomorrow?
Stmt[sorry]
Decl
sold out
it
-
Sorry it is sold out.
Stmt
Decl
sold out
USAir
-
Sorry, USAir is sold out.
17.7.2.
Concept-to-Speech Rendition
Once the response content is generated, the SLU system needs to render it into a waveform
to play to the users. The task is naturally assigned to a text-to-speech component. However,
the response generated in the previous session is more than text message. It contains the
underlying semantic information, because it is usually embedded in the semantic representa-
tion as shown in Figure 17.23 and Figure 17.24. This is why the speech rendition is often
done through a concept-to-speech module. A concept-to-speech system can be considered as
a text-to-speech system with input text enhanced with domain knowledge tags. With these
extra tags, a concept-to-speech system should be able to generate tailored speech output to
better convey the system intention.
Chapter 15 discussed the role of prosody in human perception. When messages are
generated, it is expected that they are supplemented with hints as to their information struc-
ture. At a minimum, the message generation component can identify which parts of the ut-
terance constitute the theme, which is material understood, previously mentioned, or some-
how extending a longer thread of coherence in the dialog, from the rheme, which is the
unique contribution of the present utterance to the discourse [36]. If such a distinction is

Response Generation And Rendition
881
marked on the generated utterances, or templates, it can be associated with characteristic
pitch contour, prosodic phrasing, and other effects (see Chapter 15).
For example, in the question-answer pair shown in Figure 17.27 (from ordinary human
conversation), the theme and rheme components are bracketed. The theme of the answer
consists of a mention of Mary, and the act of driving, both carried forward from the ques-
tion. The theme consists of new information, the answer to the question, embedded in a kind
of placeholder noun phrase. Clearly, the input to the message generation component requires
some indication of which entities of the input semantic representation are linked to discourse
history.
Q: Which car did Mary drive?
A: (Mary drove)th (the RED car.)rh
Figure 17.27 A question-answer pair with theme and rheme components marked.
Prosodic rules are triggered by information structure. In general, a theme in the early
part of a statement may be realized with a rise-fall-rise pitch contour, often with turning
points in the contour aligned with lexically stressed or other salient syllables of the words in
the theme. Rheme marking by pitch contour is also essential for naturalness, and a common
rheme tune in English declaratives is a slight rise up to the final lexically stressed syllable,
followed by a fall to the bottom of the speaker’s pitch range. The actual alignment of pitch
extrema will depend on the position of focus, or maximum contrast and information value,
within either the theme or the rheme.
In Figure 17.27, the word RED is in focus within the rheme. If the question had im-
plied a contrast between Mary’s car and other people’s cars, it would be acceptable to estab-
lish a focus on Mary in the theme as well, marked by a pitch accent (see Chapter 15). Some-
times the portion of either theme or rheme that is not in focus (e.g., drove or car) is called
the ground [54, 55].
The response generator could add such rheme-theme information that may be used to
trigger more specialized prosodic rules. For example, one experimental system is based on a
message generator that dynamically creates concise descriptions of individual museum ob-
jects during a tour, while attempting to maximize correlations to objects a museum visitor
has already seen [21]. During the response generation phase, simple entities and factual
statements are combined, first into a semantic graph and then into a text, in which the rhe-
torical functions of utterances and clauses, and their relations to one another, are known.
This information can be passed along to a synthesizer in the form of markup tags within the
text. A synthesizer can then select appropriately interesting pitch contours that indirectly
reflect rhetorical functions.
In a dialog system, other attributes beyond rheme-theme kinds of information struc-
ture, such as speech-act type, may have characteristic intonation patterns. This might include
a regretful-sounding contour (perhaps sampled from real speaker data) applied when apolo-
gizing (Sorry, that flight is sold out) or a cheerful-sounding greeting. Although the concept-
to-speech module can be implemented as just a text-to-speech system that take the advan-
tage of the extra semantic knowledge to generate appropriate prosody, the most natural
speech rendition is still to play back a prestored waveform for the entire message. This is
why the concept-to-speech module usually relies heavily on playback of template waveform.

882
Spoken Language Understanding
However, it is obvious that we can’t record every possible message like “Flight [flight_no]
is schedule to land at [sch_time]” in Figure 17.24. Instead, a carrier sentence can be re-
corded and the slots can then be replaced with real information. The slot can be synthesized
with an adapted TTS, which essentially eliminates the need for a front end in the TTS sys-
tem.
One problem of this approach is that the same prosody is used for a word regardless of
where it appears, which results in lower naturalness, because prosodic context is important
for natural speech. Enhanced quality can be achieved by having different instances of those
slot words, depending their contexts. For example, we can have different one recordings
depending on whether it is the first digit on a flight number, the second, or the last. Deter-
mining the number of different contexts where a slot needs to be recorded is typically done
much like the context-dependent acoustic modeling discussed in Chapter 9. This technique
increases the naturalness, at the expense of increasing the number of necessary recordings.
17.7.3.
Other Renditions
So far, we have assumed that a dialog system may be used only in a speech-only modality.
Although such systems have found many applications, multi+modal interaction may be
more compelling, as discussed in Chapter 18. In fact, voice output might not be the best in-
formation carrier in such an environment. For example, the latest wireless phones are
equipped with an LCD screen that allows for e-mail and Web access. If a high-resolution
screen is available, the renditions mechanism will likely be visually oriented.
When renditions become visually oriented, the message generation component needs
to be replaced by a graphic display component. Since GUI has been the dominant platform
for deploying major computer applications today, the behavior and technique of such a dis-
play component is well studied and documented [17]. The SLU system needs only to pass
the semantic representation from the dialog management module to a GUI rendering mod-
ule. Of course, the GUI rendering module should also be equipped with domain knowledge
to generate best rendering to convey the dialog message. MiPad [22] is such an example and
is discussed in Chapter 18.
17.8.
EVALUATION
How do we define a quantitative measure for understanding? Evaluation of understanding
and dialog is a research topic on its own. We review a number of research techniques being
pursued.
17.8.1.
Evaluation in the ATIS Task
An application used for development, testing, and demonstration of a wide variety of dialog
systems is the Air Travel Information Service (ATIS) task, sponsored by the DARPA Spo-
ken Language Systems program [20]. In this task, users ask about flight information and

Evaluation
883
make travel arrangements. To enable consistent evaluation of progress across systems, a
corpus of data for this task has been collected and shared among research sites.
The application database contains information about flights, fares, airlines, cities, air-
ports, and ground services, organized in a relational schema. Most user queries, though they
may require some system interaction in order to specify fully, can be answered with a single
relational query. The ATIS data collection is done using the wizard-of-oz framework.5 A
user interacts with the system as though working with a fully automated travel planner. Hid-
den human wizards were used in the data-collection process to provide efficient and correct
responses to the subjects. A typical scenario presented as a task for a subject to accomplish
by means of the automated assistant is as follows:
Plan the travel arrangements for a small family reunion:
First pick a city where the get-together will be held. From three different cities (of your
choice), find travel arrangements that are suitable for the family members who typify the econ-
omy, high class, and adventurous life styles.
After data collection, each query was classified as context dependent or context inde-
pendent. A context-dependent query relies partially on past queries for specification, such as
“Is that a non-stop flight?” Many of the system tests based on ATIS require not only accu-
racy of speech recognition (the user’s spoken query), but also semantic interpretation suffi-
cient to construct an SQL query to the database and correctly complete the desired transac-
tion. Evaluation of ATIS was based on three benchmarks: SPREC (speech recognition per-
formance), NL (natural language understanding for text transcription of spoken utterances),
and SLU (spoken language understanding). For SLU systems we are interested only in the
last two benchmarks.
With the help of constrained domain of ATIS, correct understanding can be translated
into correct database access. Since database access is usually done via SQL database query,
the evaluation of understanding can be performed in the domain of generated SQL queries.
However, it is still ambiguous when someone would like to query flights around 11:00 a.m.
For the purpose of understanding, how wide a time frame is around considered to be?
Many examples of queries contain some ambiguities. For instance, when querying
about the flights between city X and Y, should the system display only the flights from X to
Y; or flights in both directions. To alleviate the ambiguity, each release of ATIS training
corpus was accompanied by a Principles of Interpretation document that has standard defi-
nitions of the meaning of such terms like around (means within a 15-minute window) and
between (means only from).
Once the correct understanding is represented as an SQL query, ATIS can be easily
evaluated by comparing the SQL queries generated by SLU systems against the standard
labeled SQL queries. The utterances in ATIS are classified into three types:
 A—semantically independent of earlier utterances, so per-turn semantic inter-
pretation can uniquely identify the semantic intent.
5 The wizard-of-oz data collection framework is described in Chapter 18.

884
Spoken Language Understanding
 D—semantically dependent upon earlier utterances, so discourse knowledge is
required to provide full interpretation.
 X—unevaluatable, so a response such as No answer or I don’t understand you,
could you repeat yourself is considered a right answer.
The other debatable item is whether a No answer output for type A and D utterances
should be treated equally as a false SQL query. In the original 1991 ATIS evaluation, a false
SQL query for type A and D utterances is penalized twice as heavily as a No answer output
for type A and D utterances. However, the decision was dropped for the 1993 ATIS evalua-
tion. ATIS decided not to evaluate dialog component for two reasons. First, dialog alters
users’ behavior during data collection. Users’ utterances are highly contingent on the per-
formance of the wizard-of-oz system, so the data collected has little use for systematic train-
ing and testing. Moreover, the SLU systems would likely have to be tested by real subjects.
Second, the evaluation of dialog behavior is highly subjective, since effectiveness and user
friendliness are generally vaguely defined.
Figure 17.28 PARADISE’s structure of objectives for spoken dialog performance [57].
17.8.2.
PARADISE Framework
The evaluation of a dialog system is subjective in nature and is typically done in an end-to-
end fashion. In such a framework, objective criteria like number of dialog turns and system
throughput, and subjective measures like user satisfaction, are typically used.
One of the most sophisticated systems for evaluating dialog systems ever developed is
the PARAdigm for Dialog System Evaluation (PARADISE) [57]. The designers of this
framework took a comprehensive view of the many potential factors affecting dialog evalua-
tion, in particular the distinction between measuring success of transaction (quality) and cost
Maximize User Satisfaction
Maximize Task Success
Minimize Costs
Kappa
Measure
Efficiency Measures
Qualitative Measures
Number of Turns,
Dialog Time,
Etc.
Agent response delay,
Inappropriate utterance ratio,
Repair ratio, Etc.

Evaluation
885
of the dialog, both in human and system terms. A decision-theoretic method, as shown in
Figure 17.28, is used to explicitly weight these various disparate factors to achieve a unified
measure. In addition, the PARADISE metrics can derive discrete scores for subdialogs,
which is useful for diagnosis, comparison across systems, and tuning.
A simple measure for task success can be the following question: “Was all the needed
information exchanged, in the correct direction (user to system, system to user) at each
step?” PARADISE provides a framework for defining, for any interaction in a limited do-
main, a simplified representation of the minimal required information and its directionality.
In PARADISE terms, this is an attribute-value matrix (AVM) showing the names and in-
stantiations of required elements at dialog completion. This could be derived from reference
frames for each required concept in a dialog exchange, with mandatory slots marked for
legal completions. Once such reference frames or matrices are available, different dialog
strategies that address the same function can be compared over many instantiations (test
dialog sessions), using statistical measures that assess confusability and length.
Table 17.5 Attribute-value table [57].
Attribute
Possible Values
Depart-City (DC)
Milan, Rome, Torin, Trento
Arrival-City (AC)
Milan, Rome, Torin, Trento
Depart-Range (DR)
Morning, evening
Depart-Time (DT)
6am, 8am, 6pm, 8pm
For example, imagine an ATIS-like application that had the following information at-
tributes, with the possible values listed in Table 17.5. An utterance such as “I want to go
from Torin to Milan” communicates legal DC and AC attribute values from user to system.
This is a limited-domain system by assumption, so confusions are assumed to occur within
the possible values of the application. For example, if the system instantiates the Depart-
City (DC) slot with Trento instead of Torin after processing the given sample utterance, it is
a confusion that can be recorded in a confusability matrix over all dialog test sessions. A
subsection of such a possible confusability matrix, covering only the DC and AC attributes,
is shown in Table 17.6, which shows only confusion within an attribute type that covers a
consistent vocabulary (city names, instantiating the DC and AC attributes). In practice, how-
ever, the full matrix might show confusions across attribute types, such as morning for Mi-
lan, etc.
Given a confusability matrix M over all possible attributes in the application, we can
apply the Kappa coefficient [48] to measure the quality characterizing the task’s success at
meeting the information requirements of the application:
( )
( )
=
1-
( )
P A
P E
P E
κ
−
(17.8)

886
Spoken Language Understanding
where
( )
P A is the proportion of times that the AVMs for the actual set of dialog agree with
the AVMs for the interpreted results, and
( )
P E
is the proportion of times that AVMs for the
dialog and interpreted results are expected to agree by chance.
( )
P E
can be estimated by
2
1
( )
(
)
n
i
i
t
P E
T
=
= 
, where
it is the sum of the frequencies in column i of M and T is total
frequencies ( 1
n
t
t
+
+

) in M . The measure of
( )
P A
(how well or poorly the application
did in information extraction) is calculated simply by examining how much of the total
count occurs on the diagonal:
1
( )
( , )
n
i
P A
M i i
T
=
= 
.
Table 17.6 Confusability matrix for city identification [57].
Depart-City
Arrival-City
Data
Milan
Rome
Torin
Trento
Milan
Rome
Torin
Trento
Milan (depart)
22
1
3
Rome (depart)
29
Torin (depart)
4
16
4
1
Trento (depart)
1
1
5
11
1
Milan (arrive)
3
20
Rome (arrive)
22
Torin (arrive)
2
1
1
20
5
Trento (arrive)
1
1
2
8
15
sum
30
30
25
15
25
25
30
20
In addition to task success, system performance is also a function of several cost
measures. Cost measures include efficiency measures, such as the number of dialog turns or
task completion time; as well as qualitative measures, such as style of dialog or how good
the repair mechanism is. If a set of test dialogs is available, with experimentally measured
user satisfaction (the predicted categories), the kappa measure, and quantitative measures of
cost (denoted as
ic , such as counts of repetitions, repairs etc.), linear regression can be used,
over the z-score normalization of these predictor terms, to identify and weight the most im-
portant predictors of satisfaction for a given system. Thus, the performance can be defined
as:
1
Performance =
( )
( )
n
i
i
i
w
c
α
κ
=
∗
−
∗

A
A
(17.9)
where A is the z-score normalization function
( )
x
x
x
x
σ
−
=
A
.

Case Study—Dr. Who
887
Evaluating a dialog system involves having a group of users perform tasks with ideal
outcomes. Then the cost measures and task success kappa measure are estimated. These
measures are used to derive the regression weights in Eq. (17.9). Once the regression
weights are attained, one could possibly predict the user satisfaction when a subpart of the
dialog system is improved.
17.9.
CASE STUDY—DR. WHO
Dr. Who is a project at Microsoft Research on its multimodal dialog system development. It
incorporates many of the dialog technologies described in this chapter. We use Dr. Who’s
SLU engine as an example to illustrate how to effectively create practical systems [22, 58-
60]. It follows the mathematical framework illustrated in Eq. (17.1). The system architecture
is shown in Figure 17.29. Since it intends to serve as a general architecture for multimodal
dialog systems, it makes some simple assumptions at the architecture level. First, it replaces
the speech recognizer and sentence interpretation modules with a semantic parser for each
modality. The response rendering is merged into dialog manager with different XSL style
sheets for each media output.
Figure 17.29 The Dr. Who system architecture [60].
17.9.1.
Semantic Representation
Semantic representation is a critical part in Dr. Who’s SLU engine design. Essentially, the
semantic objects are an abstraction of the speech acts, the domain knowledge, and the appli-
cation logic. They are designed to encapsulate the respective language models and dialog
actions that govern their creation and behaviors. The system components communicate with
one another through events surrounding the semantic objects. In this view, the dialog (in-
cluding logic inferences) is an integral part of the discourse semantic evaluation process.
Discourse
Semantics (SML)
Surface
Semantics (SML)
Response
Semantic
Parser
Discourse
Analysis
Dialog
Manager
CFG Language
Model
Semantic Model
(SDL)
Behavior Model
(XSLT)

888
Spoken Language Understanding
There are two types of semantic objects in Dr. Who. The first type is the functional
semantic object that is used to represent linguistic expressions in the user’s utterance. The
second type is the physical semantic object that is used to represent real-world entities re-
lated to the application domain. Both types of semantic objects are represented by semantic
frames and specified in the semantic markup language (SML), which is an extension of
XML. Following the principles of the XML schema, Dr. Who defines the schema of SML in
another XML called semantic definition language (SDL). SDL is designed to support many
discourse and dialog features. In addition, SDL is suited to represent the domain knowledge
via the application schema, the hierarchy of the semantic objects, and the semantic inference
rules.
The format of various semantic classes follows SDL representations in Dr. Who. The
terminal and nonterminal nodes on the parse are denoted in SDL with tags <verbatim>
and <class>, respectively. These tags refer to the semantic objects and have the name and
type attributes. The type attribute corresponds to the entity type the semantic object even-
tually would be converted to; it plays a key role in inheritance and polymorphism, as de-
scribed in Section 17.3.1. When a semantic object is unique in its type, SDL can automati-
cally assume its type as the name. In addition, SDL defines a <cfg> tag for the language
model that governs the instantiation of a semantic object ,and the language model could be
stored in another file. An <expert> tag can be defined for the system resource to physi-
cally convert a semantic object to a domain entity. Finally, the tag <slot> in SDL defines
the descendant for a nonterminal node.
Take the semantic class for Microsoft employee directory as an example. The simple
application answers queries on an employee’s data such as office location, phone number,
hiring date, etc. An item that can be asked is a semantic terminal DirectoryItem as de-
fined in Figure 17.30. To allow users to ask more than one directory item at one dialog turn,
a multiple semantic class DirectoryItems
is also defined recursively, as shown in
Figure 17.30.
<verbatim type=”DirectoryItem” …>
<prod name=”office”/>
<prod name=”phone”/>
<prod name=”hiring date”/>
…
</verbatim>
<class type=”DirectoryItems” …>
<slot type=”DirectoryItem”/>
<slot type=”DirectoryItems”/>
<cfg ref=”DirectoryItems.cfg”/>
</class>
Figure 17.30 The terminal semantic class DirectoryItem and nonterminal semantic class
defined in Dr. Who using SDL. Note that the definition of DirectoryItems contains a re-
cursive style, which can accommodate more than one DirectoryItem [59].
The <prod> tags inside a terminal semantic object indicate that the terminal is of an
enumeration type, and all the possible values are text normalized to the string values of the

Case Study—Dr. Who
889
name attribute. The main speech act, the query, is modeled by the semantic class Direc-
toryQuery, as shown in Figure 17.31.
<class type=”DirectoryQuery” …>
<slot type=”Person”/>
<slot type=”DirectoryItems”/>
<expert clsid=”…”/>
<cfg ref=”Directory.cfg”/>
</class>
<include ref=”PeopleGrammar.sdl”/>
Figure 17.31 The main semantic class DirectoryQuery defined in Dr. Who using SDL [59].
The semantic object can be instantiated following the language model in “Direc-
tory.cfg” and, once instantiated, is handled by a system object identified by its class id
(clsid). The system object then formulates the query language that retrieves the data from the
database. It is also possible to embed the XML version of the query language (e.g., XQL)
within the <expert> tag. Semantic models can be nested and reused, as shown in the
<include> tag in the above example, where the semantic model for people is referred.
17.9.2.
Semantic Parser (Sentence Interpretation)
For speech modality, Dr. Who employs a speech recognizer with unified language models
[62] that take advantage of both rule-based and data-driven approaches, as discussed Chap-
ter 11. Once we have text transcription of user’s utterances, a robust chart parser similar to
the one described in Section 17.4.1 is used for sentence interpretation.
The emphasis of sentence interpretation is to annotate the user’s utterance in a mean-
ingful way to generate functional semantic entities. Essentially, the surface SML represents
a semantic parse. Thus, after a successful parse, the corresponding surface semantic objects
are instantiated based on the semantic classes whose CFG grammars are fired. While in SDL
we use static tags such as <class> and <verbatim> for the semantic classes, the in-
stances of a semantic object use the object name as the tag in SML. For example, the surface
SML for an utterance “What is the phone number for Kuansan” is
<DirectoryQuery …>
<PersonByName type=”Person” parse=”kuansan”>
Kuansan
</PersonByName>
<DirectoryItem type=”DirectoryItem” parse=”phone number”>
phone
</DirectoryItem>
</DirectoryQuery>
Figure 17.32 The surface semantic object DirectoryQuery represented in SML after a
successful parse [59].

890
Spoken Language Understanding
17.9.3.
Discourse Analysis
As mentioned in Section 17.5, the goal of discourse analysis is to resolve surface semantic
objects to discourse semantic objects. For the surface semantic object in Figure 17.32, the
discourse engine binds the three semantic objects (i.e., the person, the directory item, and the
directory query itself) to real word entities represented in the SML example, as shown in
Figure 17.33.
<DirectoryQuery …>
<Person id=”kuansanw” parse=”kuansan”>
<First>Kuansan</First>
<Last>Wang</Last>
…
</Person>
<DirectoryItem parse=”phone number”>
<phone>+1(425)703-8377</phone>
</DirectoryItem>
</DirectoryQuery>
Figure 17.33 The discourse semantic objects for the surface semantic object illustrated in
Figure 17.32 [59].
Note that the parse string from the user’s original utterance is kept so that the render-
ing engine can choose to rephrase the response using the user’s wording.
When an error occurs, the semantic engine inserts an <error> tag in the offending
semantic objects with a code indicating the error condition. For example, if the query is for a
person named Derek, the discourse SML might appear as shown in Figure 17.34.
<DirectoryQuery status=”TBD” focus=”Person” …>
<PersonByName type=”Person” parse=”Derek” status=”TBD”…>
<error scode=”1” count=”27”/>
<Person id=”derekba”>
<First>Derek</First>
<Last>Baines</Last>
…
</Person>
<Person id=”dbevan”>
<First>Derek</First>
<Last>Bevan</Last>
…
</Person>
…
</PersonByName>
…
</DirectoryQuery>
Figure 17.34 A discourse semantic object in Dr. Who contains an <error> tag indicating
the error condition [59].

Case Study—Dr. Who
891
In Figure 17.34, semantic objects that cannot be converted (e.g., DirectoryQuery
and PersonByName) are flagged with a status “TBD”. Discourse SML also marks the dia-
log focus, as in the DirectoryQuery, that indicates the places where the semantic
evaluation process fails to continue. These two cues assist the behavior model in deciding
the appropriate error-repair responses.
Dr. Who uses three priority types of entity memory (discourse memory, explicit, and
implicit turn memory) to resolve relative expressions. Anaphora and deixis are treated as
common semantic classes, so they can be resolved according to the algorithm described in
Section 17.5.1.1. Ellipsis is treated as an automatic inference. Unless marked as NO
_INFER in the semantic class definition, every slot in a semantic class can be automatically
inferred. The strategy to automatically resolve partially specified entities is as follows.
During the evaluation stage, a partially filled semantic object is first compared with
the entities in the three-entity memory based on the type compatibility. If a candidate is
found, the discourse analysis module then computes a goodness-of-fit score by consulting
the knowledge base and considering the position of the entity in the memory list. The se-
mantic object is converted immediately to the entity from the memory if the score exceeds
the threshold. In the process, all the actions implied by the entities are carried out following
the order in which the corresponding semantic objects are converted. For example, the sec-
ond user’s query in the dialog illustrated in Figure 17.35 contains an ellipsis reference to
DirectoryItem office, which can be resolved using the discourse entity memory.
U: Where is his office?
S: The office is in building 31, room 1362.
U: How about Kuansan’s?
S: The office is in building 31, room 1363.
Figure 17.35 A dialog example in the Dr Who system. The second user’s query contains an el-
lipsis reference to DirectoryItem office [59].
17.9.4.
Dialog Manager
To support mixed-initiative multimodal dialogs, Dr. Who employs a plan-based approach
instead of dialog grammars. The dialog manager that handles dialog events surrounding se-
mantic objects is very similar to a GUI program that handles GUI events surrounding
graphical objects. These events can be handled synchronously or asynchronously based on
various implementation considerations. In addition, the design enables a seamlessly inte-
grated GUI and speech interface for multimodal applications to embrace the same human-
computer interaction model.
Dr. Who SLU engine can use XSL-transformations (XSLT) [62] for specifying the
behavior of a plan-based dialog system. XSLT, a recent World Wide Web Consortium
(W3C) standard, is a specialized XML intended for describing the rules of how a structured
document in XML can be transformed into another, say in a text-to-speech markup language
for speech rendering or the hypertext markup language (HTML) for visual rendering. Its
core construct is a collection of predicate-action pairs: each predicate specifies a textual pat-

892
Spoken Language Understanding
tern in the source document, and the corresponding action will produce a text segment in the
output whenever the pattern specified by the predicate is seen in the source document. The
output segment is specified through a programmable, context-sensitive template. XSLT de-
fines a rich set of logical controls for composing the templates. The basic programming
paradigm bears close resemblance to a logical programming language, such as Prolog,
which facilitates logic inference in plan-based systems. As a result, XSLT possesses suffi-
cient expressive power for implementing crucial dialog components, ranging from defining
dialog plans, realizing dialog strategies, and generating natural language, to manipulating
prosodic markup for text-to-speech synthesis and creating dynamic HTML pages for multi-
modal applications.
Assuming TTS output, the planning rules that render the discourse SML of Figure
17.33 in text can be expressed in XSLT as shown in Figure 17.36.
<xsl:template match=”DirectoryQuery[@not(status)]”>
For <xsl:apply-templates select=”Person”/>, the
<xsl:apply-templates select=”DirectoryItem”/>.
</xsl:template>
<xsl:template match=”Person”>
<xsl:value-of select=”First”/>
<xsl:value-of select=”Last”/>
</xsl:template>
<xsl:template match=”DirectoryItem”>
<xsl:apply-templates/>
</xsl:template>
<xsl:template match=”phone”>
phone number is <xsl:value-of/>
</xsl:template>
Figure 17.36 A TTS response-rendering rule for discourse SML of Figure 17.33. This rule
generates a text message For Kuansan Wang, the phone number is +1(425)703-8377 ” [59].
<xsl:template match=”DirectoryQuery[@not(status)]”>
<TABLE border=”1”>
<THEAD><TR>
<TH>Properties</TH>
<TH><xsl:apply-templates select=”Person”/> </TH>
</TR></THEAD>
<TBODY><xsl:apply-templates select=”DirectoryItem”/>
</TBODY>
</TABLE>
</xsl:template>
<xsl:template match=”phone”>
<TR> <TD>phone</TD> <TD> <xsl:value-of /> </TD> </TR>
</xsl:template>
Figure 17.37 An HTML response-rendering rule for discourse SML of Figure 17.33. It gener-
ates a visual table representation rather than a text message ” [59].

Case Study—Dr. Who
893
This rule leads to a response For Kuansan Wang, the phone number is +1(425)703-
8377. Elaborated functions, such as prosodic manipulations in text to speech markup, can be
included accordingly. To change the output to Web presentation, the above XSLT style
sheet can be slightly modified for rendering in HTML as a table, as shown in Figure 17.37.
The Dr. Who SLU engine has a concept called logical container as a dialog property
to be encapsulated in a semantic class. Three types of logical containers can be accessed in
the definition of semantic classes. A semantic class is an AND type container if all its attrib-
utes must be evaluated successfully. If this requirement is not met, the evaluation of the
AND type semantic object is considered failed, which will prompt the system to post a dia-
log-repair event. An OR type container requires at least one attribute to be successfully
evaluated. Similarly, for an exclusive or (XOR) type container, one and only one attribute
must be successfully evaluated.
Figure 17.38 shows a semantic class hierarchy corresponding to the partial plan shown
in Figure 17.20. The dialog goal—to gather information for booking a flight—corresponds
to the highest-level semantic class Book Flight. Evaluating this semantic class drives the
dialog system to traverse down the semantic class structure, eventually fulfilling all the steps
necessary to achieve the dialog goal. This is achieved by recursively evaluating the attrib-
utes, instantiating semantic objects actively if necessary. The logical relation of each seman-
tic class determines the rules of instantiation and dialog repair. For instance, if the user
specifies the trip to be one way only, the evaluation of the One Way Flag semantic class
becomes successful. As the Inbound Trip semantic class is an XOR container, the dialog
system bypasses the evaluation of the Itinerary attribute in the Inbound Trip semantic class.
Figure 17.38 An semantic tree hierarchy corresponding to the partial plan shown in Figure
17.20 in an airline reservation application [58].
The Itinerary semantic class encapsulates the basic elements to specify a one-way trip.
Since it is designated as an AND type container, the dialog manager tries to acquire any
missing information by actively instantiating the corresponding semantic classes it contains.
City Name
Airport Name
Book Flight (AND)
Itinerary (AND)
Inbound Trip (XOR)
One Way Flag
Place (OR)
origin
Place (OR)
destination
Time(OR)
Outbound Trip

894
Spoken Language Understanding
The active instantiation event handlers for these classes solicit information from the user by
implementing certain prompting strategy. On the other hand, the Place semantic class,
which is used to denote both the origin and the destination, is implemented as an OR con-
tainer. The user may specify the location by either the city name or the airport name.
17.10. HISTORICAL PERSPECTIVE AND FURTHER READING
Traditional natural language research has its roots in symbolic systems. Motivated by the
desire to understand cognitive processes, the underlying theories tend to be from linguistics
and psychology. As a result, coverage of phenomena of theoretical interest (usually a rare
occurrence) has traditionally been more important than developing systems with a broad
coverage.
On the other hand, speech recognition research is driven to produce practical usable
applications. Techniques motivated by knowledge of human processes have been less
important than techniques that can be used for real applications. In recent decades, interest
has grown in the use of engineering techniques in computational language processing,
although the use of linguistic knowledge and techniques in engineering has lagged
somewhat.
The ATIS program sponsored by DARPA had a very significant influence upon the
SLU research community [34]. For the first time, the research community started seriously
evaluating SLU systems on a quantitative basis, which revealed that many traditional NL
techniques designed for written language failed to deal with spoken language in practice.
For limited-domain SLU applications, vocabularies are typically about 2000 words.
CMU’s Phoenix SLU system [63] set the benchmark for domain-specific spoken language
understanding in the DARPA ATIS programs. It is based on an island-driven semantic pars-
ing approach. After years of engineering, the speech understanding error rate ranges from
6% to 41%. Since conversational repairs in human-human dialog can often be in the same
range for these systems, the determining factor in these domain-specific SLU applications
may not be the error rates but instead the ability of the system to manage and recover from
errors. Many of these were described in detail in the Proceedings of the DARPA Spoken
Language Systems Technology Workshop published by Morgan Kaufmann from 1991 to
1995. The special issue of Speech Communication on Spoken Dialog [45] also includes sev-
eral state-of-the-art system descriptions.
Allen’s Natural Language Understanding [1] is a good book on natural language un-
derstanding with a comprehensive coverage of syntactic processing, semantic processing,
discourse analysis, and dialog agent. Knowledge and semantic representation comprise the
most import fundamental issue for symbolic artificial intelligence. Several AI textbooks [33,
56, 65] contain comprehensive description of knowledge representation. The use of semantic
frames can be traced back to case frames or structures proposed by Fillmore [16]. SAM [44]
is among the first systems using semantic frames and template matcher for natural language
processing. The description of semantic classes and frames in this book mostly follows the
systematic treatment of semantic classes in the Dr Who system [58].
Speech-act (sometimes called dialog-act) theory was first proposed by Austin [4] and
further developed by Searle [42]. It is an important concept in dialog systems. You can ac-

Historical Perspective and Further Reading
895
quire more information about speech-act theory and its application to dialog systems from
[12, 40, 43]. Cohen [10] provides a good comparison of different approaches for dialog
modeling, including dialog grammar (finite state), plan-based and agent-based (dialog as
teamwork). We treat agent-based dialog modeling as an extension of plan-based dialog
modeling, as described in Section 17.6.2. Agent-based approach is a very popular frame-
work for multimodal user interface, and interested readers can refer to [11]. Hudson and
Newell [23] incorporate probability into finite state dialog management to handle uncer-
tainty in input modalities, such as pen-based interface, gesture recognition, and speech rec-
ognition. J. Allen’s book [1] has a systematic description of plan-based dialog systems. De
Mori’s Spoken Dialogs with Computers [39] is another excellent book that contains dialog
systems and related technologies.
Much of the content in this chapter follows the architecture and implementation of
semantic frame based approaches. In particular, we use plenty of descriptions and examples
of the Dr. Who SLU engine developed at Microsoft Research [22, 58-60]. The description of
plan-based systems is based on semantic frame representation and pattern matching. There is
no need for explicit dialog-act analysis and logic reasoning, since these important knowl-
edge sources are encapsulated in the semantic frames.
In addition to the semantic frame-based approach, there other approaches that rely on
formal NL parsing, logic form representation, speech acts, and logic inference [2, 41]. Mes-
sage generation for telephone application is well studied and reported in [5, 6, 49], which
provide experimental results for various prompting strategies. Most evaluation schemes for
the SLU systems focus on the end-to-end system. Human factors are important in overall
evaluation [7, 35, 52].
REFERENCES
[1]
Allen, J., Natural Language Understanding, 2nd ed, 1995, Menlo Park CA, The Benja-
min/Cummings Publishing Company.
[2]
Allen, J.F., et al., "Trains as an Embodied Natural Language System," AAAI-95 Symposium
on Embodied Language and Action, 1995.
[3]
Andernach, T., M. Poel, and E. Salomons, "Finding Classes of Dialogue Utterances with
Kohonen Networks," Proc. of the NLP Workshop of the European Conf. on Machine Learn-
ing (ECML), 1997, Prague, Czech Republic.
[4]
Austin, J.L., How to Do Things with Words., 1962, Cambridge, MA, Harvard University
Press.
[5]
Basson, S., "Integrating Speech Recognition and Speech Synthesis in the Telephone Net-
work," Proc. of the Human Factors Society 36th Annual Meeting, 1992.
[6]
Basson, S., "Prompting The User in ASR Applications," Proc. of COST232 (European Co-
operation in Science and Technology) Workshop, 1992.
[7]
Basson, S., et al., "User Participation and Compliance in Speech Automated Telecommuni-
cations Applications," Proc. of the Int. Conf. on Spoken Language Processing, 1996 pp.
1680-1683.
[8]
Biber, D., Variation Across Speech and Writing, 1988, Cambridge University Press.
[9]
Clark, H.H. and S.E. Haviland, "Comprehension and the Given-New Contract" in Discourse
production and comprehension, R.O. Freedle, Editor 1977, Norwood, NJ, pp. 1-38, Ablex
Publishing Corporation.

896
Spoken Language Understanding
[10]
Cohen, P., "Models of Dialogue," Proc.s Fourth NEC Research Symposium, 1994, SIAM
Press.
[11]
Cohen, P.R., "The Role of Natural Language in a Multimodal Interface," Proc. of the ACM
Symposium on User Interface Software and Technology, 1992 pp. 143-149.
[12]
Cohen, P.R. and C.R. Perrault, "Elements of a Plan-Based Theory of Speech Acts," Cogni-
tive Science, 1979, 3(3), pp. 177-212.
[13]
Constantinides, P.H., S Tchou, C Rudnicky, A, "A Schema Based Approach to Dialog Con-
trol," Proc. of the Int. Conf. on Spoken Language Processing, 1998 pp. 409-412.
[14]
Core, M. and J. Allen, "Coding Dialogs with the DAMSL Annotation Scheme," Proc. AAAI
Fall Symposium on Communicative Action in Humans and Machines, 1997.
[15]
Davies, K., et al., "The IBM Conversational Telephony System For Financial Applications,"
EuroSpeech'99, 1999, Budapest, Hungary pp. 275-278.
[16]
Fillmore, C.J., "The Case for Case" in Universals in Linguistic Theory, E. Bach and R.
Harms, eds. 1968, New York, NY, Holt, Rinehart and Winston.
[17]
Galitz, W.O., The Essential Guide to User Interface Design : An Introduction to Gui Design
Principles and Techniques, 1996, John Wiley & Sons.
[18]
Grosz, B., M. Pollack, and C. Sidner, eds. Discourse, in Foundations of Cognitive Science,
ed. M. Posner, 1989, MIT Press.
[19]
Heeman, P.A., et al., "Beyond Structured Dialogues: Factoring Out Grounding," Proc. of the
Int. Conf. on Spoken Language Processing, 1998, Sydney, Australia.
[20]
Hemphill, C.T., J.J. Godfrey, and G.R. Doddington, "The ATIS Spoken Language Systems
Pilot Corpus," Proc. of the Speech and Natural Language Workshop, 1990 pp. 96-101.
[21]
Hitzeman, J., et al., "On the Use of Automatically Generated Discourse-Level Information in
a Concept-to-Speech Synthesis System," Proc. of the Int. Conf. on Spoken Language Proc-
essing, 1998, Sydney, Australia pp. 2763-2766.
[22]
Huang, X., et al., "MIPAD: A Next Generation PDA Prototype," Int. Conf. on Spoken Lan-
guage Processing, 2000, Beijing, China.
[23]
Hudson, S.E. and G.L. Newell, "Probabilistic State Machines: Dialog Management for In-
puts with Uncertainty," Proc. of the ACM Symposium on User Interface Software and Tech-
nology, 1992 pp. 199-208.
[24]
Hulstijn, J. and A.V. Hessen, "Utterance Generation for Trans. Dialogues," Int. Conf. on
Spoken Language Processing, 1998, Sydney, Australia.
[25]
Jackendoff, R.S., X' Syntax: A Study of Phrase Structure, 1977, Cambridge, MA, MIT press.
[26]
Jelinek, F., et al., "Decision Tree Parsing using Hidden Derivational Model," Proc. of the
ARPA Human Language Technology Workshop, 1994 pp. 272-277.
[27]
Jurafsky, D., L. Shriberg, and D. Biasca, Switchboard SWBD-DAMSL Shallow-Discourse-
Function
Annotation
Coders
Manual,
Draft
13,
1997,
http://www.colorado.edu/linguistics/jurafsky/manual.august1.html.
[28]
LDC, Linguistic Data Consortium, 2000, http://www.ldc.upenn.edu/ldc/noframe.html.
[29]
Miller, S., et al., "Recent Progress in Hidden Understanding Models," Proc. of the ARPA
Spoken Language Systems Technology Workshop, 1995, Austin, Texas, Morgan Kaufmann,
Los Altos, CA pp. 22-25.
[30]
Miller, S. and R. Bobrow, "Statistical Language Processing Using Hidden Understanding
Models," Proc. of the Spoken Language Technology Workshop, 1994, Plainsboro, New Jer-
sey, Morgan Kaufmann, Los Altos, CA pp. 48-52.
[31]
Minsky, M., "A Framework for Representing Knowledge" in The Psychology for Computer
Vision, P.H. Winston, Editor 1975, New York, NY, McGraw-Hill.
[32]
Nilsson, N.J., Principles of Artificial Intelligence, 1982, Berlin, Germany, Springer Verlag.

Historical Perspective and Further Reading
897
[33]
Nilsson, N.J., Artificial Intelligence: A New Synthesis, 1998, Academic Press/Morgan Kauf-
mann.
[34]
Pallett, D.S., et al., "1994 Benchmark Tests for the ARPA Spoken Language Program,"
Proc. of the 1995 ARPA Human Language Technology Workshop, 1995 pp. 5-36.
[35]
Polifroni, J., et al., "Evaluation Methodology for a Telephone-Based Conversational Sys-
tem," The First Int. Conf. on Language Resources and Evaluation, 1998, Granada, Spain pp.
42-50.
[36]
Prevost, S. and M. Steedman, "Specifying Intonation from Context for Speech Synthesis,"
Speech Communication, 1994, 15, pp. 139-153.
[37]
Reinhart, T., Anaphora and Semantic Interpretation, Croom Helm Linguistics Series, 1983,
University of Chicago Press.
[38]
Roberts, D., The Existential Graphs of Charles S. Peirce, 1973, Mouton and Co.
[39]
Sadek, D. and R.D. Mori, "Dialogue Systems" in Spoken Dialogues with Computers, R.D.
Mori, Editor 1998, London, UK, pp. 523-561, Academic Press.
[40]
Sadek, M.D., "Dialogue Acts are Rational Plans," Proc. of the ESCA/ETRW Workshop on
the Structure of Multimodal Dialogue, 1991, Maratea, Italy pp. 1-29.
[41]
Sadek, M.D., et al., "Effective Human-Computer Cooperative Spoken Dialogue: The AGS
Demonstrator," Proc. of the Int. Conf. on Spoken Language Processing, 1996, Philadel-
phia,Pennsylvania pp. 546-549.
[42]
Searle, J.R., Speech Acts: An Essay in the Philosophy of Language, 1969, UK, Cambridge
University Press.
[43]
Searle, J.R. and D. Vanderveken, Foundations of Illocutionary Logic, 1985, Cambridge Uni-
versity Press.
[44]
Shank, R., Conceptual Information Processing, 1975, North Holland, Amsterdam, The
Netherlands.
[45]
Shirai, K. and S. Furui, "Special Issue on Spoken Dialogue," Speech Communication, 1994,
15.
[46]
Shriberg, E.E., R. Bates, and A. Stolcke, "A Prosody-only Decision-tree Model for Disflu-
ency Detection," Proc. Eurospeech, 1997, Rhodes, Greece pp. 2383-2386.
[47]
Sidner, C., "Focusing in the Comprehension of Definite Anaphora" in Computational Model
of Discourse, M. Brady, Berwick, R., Editor 1983, Cambridge, MA, pp. 267-330, The MIT
Press.
[48]
Sidney, S. and N.J. Castellan, Nonparametric Statistics for the Behavioral Sciences, 1988,
McGraw Hill.
[49]
Sorin, C. and R.D. Mori, "Sentence Generation" in Spoken Dialogues with Computers, R.D.
Mori, Editor 1998, London, UK, pp. 563-582, Academic Press.
[50]
Souvignier, B., et al., "The Thoughtful Elephant: Strategies for Spoken Dialog Systems,"
IEEE Trans. on Speech and Audio Processing, 2000, 8(1), pp. 51-62.
[51]
Sowa, J.F., Knowledge Representation: Logical, Philosophical, and Computational Founda-
tions, 1999, Brooks Cole Publishing Co.
[52]
Springer, S., S. Basson, and J. Spitz, "Identification of Principal Ergonomic Requirements
for Interactive Spoken Language Systems," Int. Conf. on Spoken Language Processing, 1992
pp. 1395-1398.
[53]
Standards,
N.C.f.I.T.,
Conceptual
Graph
Standard
Information,
1999,
http://www.bestweb.net/~sowa/cg/cgdpansw.htm.
[54]
Steedman, M., ed. Parsing Spoken Language using Combinatory Grammars, in Current
Issues in Parsing Technology, ed. M. Tomita, 1991, Kluwer Academic Publishers.
[55]
Steedman, M., "Information Structure and the Syntax-Phonology Interface," Linguistic In-
quiry, 2000.

898
Spoken Language Understanding
[56]
Tanimoto, S.L., The Elements of Artificial Intelligence : An Introduction Using Lisp, 1987,
Computer Science Press, Inc.
[57]
Walker, M., et al., "PARADISE: A Framework for Evaluating Spoken Dialogue Agents.,"
Proc. of the 35th Annual Meeting of the Association for Computational Linguistics (ACL-
97), 1997 pp. 271-280.
[58]
Wang, K., "An Event Driven model for Dialogue Systems," Int. Conf. on Spoken Language
Processing, 1998, Sydney, Australia pp. 393-396.
[59]
Wang, K., "Implementation od Dr. Who Dialog System Using Extended Markup Lan-
guages," Int. Conf. on Spoken Language Processing, 2000, Beijing, China.
[60]
Wang, K., "A Plan-Based Dialog System With Probabilistic Inferences," ICSLP, 2000, Bei-
jing, China.
[61]
Wang, Y., "A Robust Parser For Spoken Language Understanding," Eurospeech, 1999, Bu-
dapest, Hungary pp. 2055-2058.
[62]
Wang, Y., M. Mahajan, and X. Huang, "A Unified Context-Free Grammar and N-Gram
Model for Spoken Language Processing," Int. Conf. on Acoustics, Speech and Signal Proc-
essing, 2000, Istanbul, Turkey pp. 1639-1642.
[63]
Ward, W., "Understanding spontaneous speech: the Phoenix system," Proc. Int. Conf. on
Acoustics, Speech and Signal Processing, 1991, Toronto, Canada pp. 365-367.
[64]
Ward, W. and S. Issar, "The CMU ATIS System," Proc. of the ARPA Spoken Language
Systems Technology Workshop, 1995, Austin, Texas, Morgan Kaufmann, Palo Alto, CA.
[65]
Winston, P.H., Artificial Intelligence, 3rd ed, 1992, Reading, MA, Addison-Wesley.
[66]
Young, S.R., et al., "High Level Knowledge Sources in Usable Speech Recognition Sys-
tems," Communications of the Association for Computing Machines , 1989, 32(2), pp. 183-
194.

899
C H A P T E R
1 8
Applications and User InterfacesEquation Section 18
The ultimate impact of spoken language
technologies depends on whether you can fully integrate the enabling technologies with
applications so that users find it easy to communicate with computers. How to effectively
integrate speech into applications often depends on the nature of the user interface and
application. This is why we group user interface and application together in this chapter. In
discussing some general principles and guidelines in developing spoken language
applications, we must look closely at designing the user interface.
A well-designed user interface entails carefully considering the particular user group
of the application and delivering an application that works effectively and efficiently. As a
general guideline, you need to make sure that the interface matches the way users want to
accomplish a task. You also need to use the most appropriate modality at the appropriate
time to assist users to achieve their goals. One unique challenge in spoken language
applications is that neither speech recognition nor understanding is perfect. In addition, the
spoken command can be ambiguous, so the dialog strategy described in Chapter 17 is
necessary to clarify the goal of the speaker. There are always mistakes you have to deal

Application Architecture
900
with. It is critical that applications employ necessary interactive error-handling techniques to
minimize the impact of these errors. Application developers should therefore fully
understand the strengths and weaknesses of the underlying speech technologies and identify
the appropriate place to use the spoken language technology effectively.
This chapter mirrors Chapter 2, in the sense that you need to incorporate all the needed
components of speech communication to make a spoken language system work well. It is
important also to have your applications developed based on some standard application
programming interfaces (API), which ensures that multiple applications work well with a
wide range of speech components provided by different speech technology developers.
18.1.
APPLICATION ARCHITECTURE
A typical spoken language application has three key components. It needs an engine that can
be either a speech recognizer or a spoken language understanding system. An application
programming interface (API) is often used to facilitate the communication between the
engine and application, as illustrated in Figure 18.1. Multiple applications can interact with a
shared speech engine via the speech API. The speech engine may be a CSR engine, a TTS
engine, or an SLU engine. The interface between the application and the engine can be
distributed. For example, you can have a client-server model in which the engine is running
remotely on the server.
Figure 18.1 In a typical spoken language application architecture, multiple applications can
interact with a shared speech engine via the speech API. The speech engine may be a speech
recognizer, a TTS converter, or an SLU engine.
For a given API, there is typically an associated toolkit that provides a good
development environment and the tools you need in order to build speech applications. You
don’t need to understand the underlying speech technologies to fully take advantage of state-
of-the-art speech engines. Industry-standard based applications can draw upon support from
many different speech engine vendors, thus significantly minimizing the cost of your
applications development. For the widely used Microsoft Windows, Microsoft’s speech API
Speech API
Application 1
Application 2
Engine 1
Engine 2
Engine 3

Typical Applications
901
(SAPI) brings both speech and application developers together.1 Alternative standards are
available, such as VoiceXML2 and JSAPI3.
18.2.
TYPICAL APPLICATIONS
There are three broad classes of applications that require different UI design:
 Office: This includes the widely used desktop applications such as Microsoft
Windows and Office.
 Home: TV and kitchen are the centers for home applications. Since home
appliances and TV don’t have a keyboard or mouse, the traditional GUI
application can’t be directly extended for this category.
 Mobile: Cell phone and car are the two most important mobile scenarios.
Because of the physical size and the hands-busy and eyes-busy constraints, the
traditional GUI application interaction model requires significant modification.
This section provides descriptions of typical spoken language applications in these three
broad classes. Spoken language has the potential to provide a consistent and unified
interaction model across these three classes, albeit for these different application scenarios
you still need to apply different user interface design principles.
18.2.1.
Computer Command and Control
One of the earliest prototypes for speech recognition is command and control, which is
mainly used to navigate through operating system interfaces and applications running under
them. For example, Microsoft Agent is a set of software services that supports the
presentation of software agents as interactive personalities within the Microsoft Windows or
the Internet Explorer interface. Its command-and-control speech interface is an extension
and enhancement of the existing interactive modalities of the Windows interface. It has a
character called Peedy, shown in Figure 18.2, which recognizes your speech and talks back
to you using a Microsoft SAPI compliant command-and-control speech recognizer and text-
to-speech synthesizer.
The speech recognizer used in these command-and-control systems is typically based
on a context-free grammar (CFG) decoder. Either developers or users can define these
grammars. Associated with each legal path in the grammar is a corresponding executable
event that can map a user’s command into appropriate control actions the user may want.
They possess a built-in vocabulary for the menus and other components. The vocabulary can
also be dynamically provided by the application. Command-and-control speech recognition
allows the user to speak a word, phrase, or sentence from a list of phrases that the computer
1 http://www.microsoft.com/speech
2 http://www.voicexml.org/
3 http://java.sun.com/products/java-media/speech/

Typical Applications
902
is expecting to hear. The number of different commands a user might speak at any time can
be in the hundreds. Furthermore, the commands are not just limited to a list but can also
contain other fields, such as “Send mail to <Name>” or “Call <digits>”. With all of the
possibilities, the user is able to speak thousands of different commands. As discussed in
Chapter 17, a CFG-based recognizer is often very rigid, since it may reject the input
utterance that contains a sentence slightly different from what the CFG defines, leading to an
unfriendly user experience.
How are you?
Figure 18.2 A talking character Peedy4 as used in Microsoft Agent. Reprinted with permission
from Microsoft Corporation.
Command-and-control recognition might be used in some of the following situations:
 Answering questions. An application can easily be designed to accept voice
responses to message boxes and wizard screens. Most speech recognition
engines can easily identify Yes, No, and a few other short responses.
 Accessing large lists. In general, it's faster for a user to speak one of the names
on a list, such as “Start running calculator,” than to scroll through the list to
find it. It assumes that the user knows what is in the list. Laurila and Haavisto
[23] summarized their usability study of inexperienced users on name dialing.
Although the study is based on the telephone handset, it has a similar
implication for computer desktop applications.
 Activating macros. Speech recognition lets a user speak a more natural word or
phrase to activate a macro. For example, “Spell check the second paragraph” is
easier for most users to remember than the CTRL+F5 key combination after
selecting the second paragraph. But again, the user must know the command.
This is where most simple speech applications fail. The competition is not
CTRL+F5 itself, it is the recognition memory for most users.
 Facilitating dialog between the user and the computer. As discussed in Chapter
17, speech recognition works well in situations where the computer essentially
asks the user: “What do you want to do?” and branches according to the reply
4 Peedy © 1993-1998 Microsoft Corporation

Typical Applications
903
(somewhat like a wizard). For example, the user might reply, “I want to book a
flight from New York to Boston.” After the computer analyzes the reply, it
clarifies any ambiguous words (Did you say New York?). Finally, the computer
asks for any information that the user did not supply, such as “At what day and
time do you want to leave?”
 Providing hands-free computing. Speech recognition is an essential component
of any application that requires hands-free operation; it also can provide an
alternative to the keyboard for users who are unable to or prefer not to use one.
Users with repetitive-stress injuries or those who cannot type may use speech
recognition as the sole means of controlling the computer. As discussed in later
sections, hands-free computing is important for accessibility and mobility.
 Humanizing the computer. Speech recognition can make the computer seem
more like a person — that is, like someone whom the user talks to and who
speaks back. This capability can make games more realistic and make
educational or entertainment applications friendlier.
The specific use of command and control depends on the application. Here are some
sample ideas and their uses:
 Games and entertainment: Software games are some of the heaviest users of
command-and-control speech recognition. One of the most compelling uses of
speech
recognition
technology
is
in
interactive
verbal
exchanges
and
conversation with the computer. With games such as flight simulators, for
example, traditional computer-based characters can now evolve into characters
the user can actually talk to. While speech recognition enhances the realism and
fun in many computer games, it also provides a useful alternative to keyboard-
based control of games, and applications-voice commands provide new freedom
for the user.
 Document editing: Command and control is useful for document editing when
you wish to keep your hands on the keyboard to type, or on the mouse to drag
and select. This is especially true when you have to do a lot of editing that
requires you to move to menus frequently. You can simultaneously speak
commands for manipulating the data that you are working on. A word processor
might provide commands like “bold, italic” and “change font.” A paint package
might have “select eraser” or “choose a wider brush.” Of course, there are
users who won't prefer speaking a command to using keyboard equivalents, as
they have been using the latter for so long that the combinations have become
for them a routine part of program control. But for many people, keyboard
equivalents are a lot of unused shortcuts. Voice commands provide these users
with the means to execute a command directly.
For most of the existing applications, before an application starts a command-and-
control recognizer listening, it must first give the recognizer a list of commands to listen for.
The list might include commands like “minimize window, make the font bold, call extension
<digit> <digit> <digit>,” and “send mail to <name>.” If the user speaks the command as

Typical Applications
904
it is designed, he/she typically gets very good accuracy. However, if the user speaks the
command differently, the system typically either does not recognize anything or erroneously
recognizes something completely different. Applications can work around this problem by:
 Making sure the command names are intuitive to users. For many operations
like minimizing a window, nine out of ten users will say minimize window
without prompting.
 Showing the command on the screen. Sometimes an application displays a list of
commands on the screen. Users naturally speak the same text they see.
 Using word spotting as discussed in Chapter 9. Many speech recognizers can be
told to just listen for one keyword, like mail. This way the user can speak, “Send
mail,” or “Mail a letter,” and the recognizer will get it. Of course, the user
might say, “I don't want to send any mail,” and the computer will still end up
sending mail.
 Employing spoken language understanding components as discussed in Chapter
17.
 Employing user studies to collect data on frequently spoken variations on
commands so that the coverage is enhanced.
18.2.2.
Telephony Applications
Speech is the only major modality for telephony applications. The earliest uses of speech
technology in business were interactive voice response (IVR) systems. These systems
include infoline services in the ad-supported local newspapers, offering everything from
world news to school homework assignments at the touch of a few buttons. So what's the big
deal with a speech telephony application? It offers greater breadth, ease of use, and
interactivity. Navigating by voice rather than by keypad offers more options and quicker
navigation. It also works better while you’re driving.
To make a successful IVR application, you need to have speech input, output, and
related dialog control. People have used IVR systems over the telephone to navigate the
application based on the menu option to provide digit strings, such as the credit card
numbers, to the application. Such system typically has a small to medium vocabulary.
Today, you can use IVR to get stock quotes, people’s telephone number, and other
directory-related information. For example, you can call AT&T universal card services and
the application asks you to speak your 16-digit card number. Most of these IVR systems use
recorded messages instead of synthetic speech because the quality of TTS is still far from
humanlike. Since speech output is a slow method to present information, it is important to be
as brief as possible. Reducing the presentation of repetitive data can shorten the speech
output significantly.
Voice portals that let you talk your way to Web-based information from any phone are
one class of compelling telephony applications. Linked to specially formatted Web sites and
databases, the portals deliver what amounts to customized real-time news radio. You can

Typical Applications
905
tailor voice portals much as you do Web portals like Yahoo, AOL, or MSN. But surfing
is restricted to the very limited subsets of information the portals choose to offer. These
services typically avoid using synthesized speech. For options like news updates they rely
on sound bites recorded by announcers. There are a number of free voice portals available,
including TellMe, BeVocal, HeyAnita, Quack.com. Table 18.1 illustrates some of their
features.
Table 18.1 Some free voice portal features. These portals are being developed and will roll out
more features.
Category
Audiopoint5
Tellme6
Traffic
Yes
Yes
Weather
U.S. and world cities
U.S.
News
Yes
Yes
Financial
Yes
Yes
Sports
Yes
Yes
Airline info
No
Yes
Restaurants
No
US
Entertainment
Yes
Yes
Personalization
Yes
Yes
Some digital wireless telephony applications make full use of a client-server
architecture because of limited computing resource on the client. The server performs most
of needed processing. The client can either send the speech waveform (as used in standard
telephone) or the spectral parameters such as MFCC coefficients. Using a quantized MFCC
(see Chapter 6) at 4.5 kbps, no loss of accuracy can be achieved [6]. The Aurora project tries
to standardize the client server communication protocol based on the quantized MFCC
coefficients.7
When people are engaged in a conversation, even if they have the graphical interface
in front of them, they seldom use the vocabulary from the interface (unless prompted by
TTS or the speaker). This has an important implication for the UI design. The use of a
discourse segment pop cue such as “What now?” or “Do you want to check messages?”
could reorient users (especially after a subdialog) and help them figure out what to say next.
Wildfire8 uses such pop cues extensively. The right feedback is essential, because speech
recognition is not perfect. Designers should verify only those commands that might destroy
data or trigger future events. People become frustrated very quickly if the error feedback is
repetitive. It would be nice to design the error messages with different prompts according to
recognition confidence measures and the dialog context.
Most current telephony systems do not have much intelligent dialog control. Speech-
based dialog control as discussed in Chapter 17 is critical for error repairs, anaphora
resolution, and context management. There are a number of institutions pursuing advanced
5 http://www.myaudiopoint.com or call 1-888-38-AUDIO.
6 http://www.tellme.com or call 1-800-555-TELL.
7 http://www.etsi.org/stq
8 http://www.wildfire.com/

Typical Applications
906
telephony spoken language interface. You can find some of these excellent programs at
Carnegie Mellon University9 and Massachusetts Institute of Technology.10
18.2.3.
Dictation
Dictation is attractive to many people, since speaking is generally faster than typing,
especially in East Asian languages such as Chinese. There are a number of general-purpose
dictation software products such as IBM’s ViaVoice, Dragon’s NaturallySpeaking.
Lernout&Hauspie’s Voice Xpress, as well as the dictation capability built into Microsoft
Office. There are also vertical markets for dictation applications. Radiologists and lawyers
are two examples of such niche markets. They are generally highly paid and pressed for
time. Radiologists are often hands- and eyes-busy when work. Lawyers’ work is often
language intensive and they need to write long documents and reports. Both radiologists and
lawyers have been using dictation devices, and they have a strong need to have customized
language models for their specialized vocabulary. In fact, because of the constraints of the
language they use, the perplexity is often much smaller than in the general business articles,
leading to improved performance for both the medical and legal segments.
In dictation applications, it is important to convert speech into the text you would like
to see in its written form. This implies that punctuation should be added automatically, dates
and times should be converted into the form that is conventionally used, and appropriate
capitalization and homonym disambiguation should all be seamless to users. Dictation is
typically associated with word processing applications, which provide for a nice separation
of input modes based upon the division of the primary task into subactivities:
 Text entry;
 Command execution, such as cross-out to delete the previous word, or
capitalize-that.
 Direct manipulation activities such as cursor positioning and text selection.
It is likely that separate single input modes for each activity can increase efficiency. In
a study conducted by Morrison et al. [28], subjects switched modality from speech input to
typed input while issuing text-editing commands and found the switch of modality
disruptive. Karat et al. [19] compared the state-of-the-art dictation product with keyboard
input. They reported that several of the initial-use subjects commented that keyboard entry
seemed much more natural. This reflects the degree to which some people have become
used to using keyboards and the fact that speech recognition still makes too many mistakes.
Experienced users can enter transcription text at an average rate of 107 uncorrected words
per minute. However, correction took them over three times as long as entry time on
average, leading to an average input rate nearly twice as slow as keyboard (including
correction). Thus, unless the accuracy of speech recognition and its error correction can be
significantly improved for the mass market, text entry is best performed by keyboard.
9 http://www.speech.cs.cmu.edu/speech/
10 http://www.sls.lcs.mit.edu/sls/

Typical Applications
907
The mouse is generally accepted as being well suited to direct manipulation activities.
Further, based on human-factors studies, full typed input of commands, single keyboard
presses, and accelerator keys is not as efficient as speech-activated commands [26, 33]. The
case for the utility of speech input in word-processing applications relies upon its superiority
over the mouse with respect to the activation of commands. Word-processing and text-entry
applications are also naturally hands-busy, eyes-busy applications. It is inconvenient and
time consuming for the user to have to interrupt the typing of text or to move his/her eyes
from the work in order to execute word-processing commands. This is not necessary with
speech activation of commands.
To improve dictation throughput, you should make it as easy as possible for users to
correct mistakes. As illustrated in Figure 18.3, you can provide easy access to the Correction
Window so the user can correct mistakes that the recognizer made. Thus the quality of n-best
or word-lattice decoding discussed in Chapter 13 is critical. We use examples drawn from
Microsoft Dictation11 to illustrate these concepts.
Figure 18.3 Correction window in Microsoft Dictation. By clicking the dictated word, the
alternative words are listed in the correction window. If the dictated word is wrong, you can
click the word in the correction window to easily replace the misrecognized word. Reprinted
with permission from Microsoft Corporation.
You should also allow the user to train the system so it can adapt to different accent
and environment conditions. A typical training wizard, illustrated in Figure 18.4, asks users
to read a number of sentences. It is important to use a confidence measure, as discussed in
Chapter 9, to filter out unwanted training data so that you can eliminate possible divergences
of training the acoustic model. In Figure 18.4, the user is prompted with the sentence shown
in the window. As the user reads the sentence, the black progress bar covers the
11 Microsoft Dictation was developed by the authors at Microsoft Research, and was included for free in the
SDK4.0.

Typical Applications
908
corresponding word. If the user misreads the word, the progress bar typically stays where it
is.
Figure 18.4 Training wizard in Microsoft Dictation. Reprinted with permission from
Microsoft Corporation.
Figure 18.5 New word addition in Microsoft Dictation. Reprinted with permission from
Microsoft Corporation.
Another practical problem for most dictation products is that they have about 60,000
to 100,000 words in the vocabulary, which contains all the inflected forms, so work and

Typical Applications
909
works are considered as two different words. As the vocabulary is limited (for detailed
discussion on vocabulary selection, see Chapter 11), you should provide a capability to
dynamically add new words to the system. This capability is illustrated in Figure 18.5. You
can listen to the new word you added to see if the computer guessed the pronunciation of the
word correctly or not. If you want to pronounce the word differently, you can either type in
the words that sound the way you want it to or type in the phonetic sequence.
18.2.4.
Accessibility
Approximately 10–15 percent of the population has an impairment that affects their ability
to use computers efficiently. Accessibility is about designing applications that everyone can
use. The GUI actually makes accessibility a more critical issue. People with disabilities face
more usability issues than those without. There are a number of Web sites dedicated to
accessibility.12
People who cannot effectively use a keyboard or a mouse can use speech commands
to control the computer and use the dictation software to input text. For example, Dragon’s
dictation software lets you control everything by voice, making it extremely useful for
people with repetitive stress injury (RSI). A screen reader is a program that uses TTS to help
people who have reading problems or are vision impaired. Windows 2000® includes one
such screen reader, called Narrator, which reads what is displayed on the screen: the
contents of the active window, menu options, and text that the user types. Users can
customize the way these screen elements are read. They can also configure Narrator to make
the mouse pointer follow the active item on the screen and can adjust the speed, volume, and
pitch of the narrator voice.
18.2.5.
Handheld Devices
For handheld devices such as smart phones or Personal Digital Assistants (PDA), speech,
pen, and display can be used effectively to improve usability. The small form-factor implies
that most of the functions have to be hidden. Speech is particularly suitable to access
information that can’t be seen on the display. Current input methods are generally pen- or
soft keyboard-based. They are slower than dictation-based text input. For form filling,
speech is also particularly suitable to combine multiple parameters in a single phrase. The
increasing popularity of cell phones is likely to be the major thrust for using handheld
devices to access information.
Potential usability problems of handheld devices are the lack of privacy and the
problem of environmental noise. It is hard to imagine that we can have 100 people in a
conference room talking to their handheld devices simultaneously. Since these devices are
also often used in a noisy environment, you need to have a very robust speech recognition
12 http://www.el.net/CAT/index.html (The Center for Accessible Technology).
http://www.lighthouse.org/print_leg.htm (Lighthouse International).
http://www.w3c.org/wai (World Wide Web Consortium’s Accessibility Page).
http://www.microsoft.com/enable (Microsoft Accessibility Home Page).

Typical Applications
910
system to deal with signals around 15 dB. In Section 18.5 we illustrate the process of
designing and developing an effective speech interface and application for a handheld
device.
18.2.6.
Automobile Applications
People already use cell phones extensively when driving. Safety is an important issue
whenever a driver takes his/her eyes off the road or his/her hands off the steering wheel. In
many countries it is illegal to use a cellular phone while driving. Speech-based interaction is
particularly suitable for this eyes-busy and hands-busy environment. However, it is a
challenge to use speech recognition in the car due to the relative high noisy environment.
Environment robustness algorithms, discussed in Chapter 10, are necessary to make
automobile applications compelling.
Figure 18.6 Clarion in-dash AutoPC.
One such system is Clarion’s AutoPC, shown in Figure 18.6. It enables drivers to
access information by speech while driving. You can tell your car what to do by speech.
AutoPC has a vocabulary of more than 1200 words. It understands commands to change
radio stations, provide turn-by-turn directions based on its Global Positioning System (GPS)
navigation unit, or read e-mail subject lines using TTS. Running on Microsoft's Windows
CE operating system, AutoPC's control module fits in a single dashboard slot, with the
computer and six-disc CD changer stored in the trunk.
18.2.7.
Speaker Recognition
The HMM-based speech recognition algorithms discussed in earlier chapters can be
modified for speaker recognition, where a speaker model is needed for each speaker to be
recognized. The term speaker recognition is an umbrella that includes speaker identification,
speaker detection, and speaker verification. Speaker identification consists in deciding who
the person is, what group the person is a member of, or that the person is unknown. Speaker
detection consists in detecting if there is a speaker change. This is often needed in speech
recognition, as a better speaker model can be used for improved accuracy. Speaker
verification consists in verifying a person’s claimed identify from his/her voice, which is
also called speaker authentication, talker verification, or voice authentication.
In text-dependent approaches, the system knows the phrase, which can be either fixed
or prompted to the user. The speaker speaks the required phrase to be identified or verified.

Speech Interface Design
911
The speaker typically has to enroll in the system by presenting a small number of speech
samples to build the corresponding speaker model. Since the amount of training data is
small, the parameter tying techniques discussed in Chapter 9 are very important.
Speaker recognition is a performance biometric; i.e., you perform a task to be
recognized. It can be made fairly robust against noise and channel variations, ordinary
human changes, and mimicry of humans and tape recorders. Campbell [3] and Furui [9]
provided two excellent tutorial papers on recent advances in speaker recognition.
In general, accuracy is much higher for speaker recognition than for speech
recognition. The best speaker recognition systems typically use the same algorithms, such as
the ways to deal with environment noises and hidden Markov modeling, found in speech
recognition systems. Applicable speaker recognition services include voice dialing, banking
over a telephone network, telephone shopping, database access services, information and
reservation services, forensic application, and voice login. There are a number of
commercial speaker verification systems, including Apple’s voice login for its iMacand
Sprint’s Voice FONCARD.
18.3.
SPEECH INTERFACE DESIGN
No single unified interaction theory has emerged so far to guide user interface design. Study
of the interaction has been largely associated with developing practical applications, having
its roots in human factors and user studies. Practitioners have postulated general principles
that we discuss in this section.
18.3.1.
General Principles
There are a number of books on graphical user interface (GUI) design [10, 11, 17, 29, 41].
Usability research also generated a large body of data that is useful to UI designers. The
Human Factors and Ergonomic Society (HFES) is developing the HFES-200 standard—
Ergonomics of Software User Interface, which has many insightful implications for interface
design. The most important criteria, in order of importance, are:
 Effectiveness or usefulness
 Efficiency or usability
 User satisfaction or desirability
These three general points mean that, first and foremost, a user interface must work, so
the user can get the job done. After that, it is important that the user interface accomplishes
its task as productively as possible. Last, but not least, the user interface can be further
improved by focusing on user satisfaction as measured by means of questionnaires and user
feedback.
The major difference between GUI and spoken language interface is that speech
recognition or understanding can never be perfect. Imagine you are designing for a GUI and
your input method is a keyboard and a mouse where the error rate is about 5% for the key or

Speech Interface Design
912
mouse presses. These 5% errors make it much more difficult to have an effective user
interface. In GUI applications, when you mistype or misclick, you regard these errors as
your own mistakes. In spoken language applications, all the misrecognition errors are
regarded as system errors. This psychological effort is a very important factor in the UI
design. Thus it is not stretching to say that most of the design work for spoken language
applications should be on how to effectively manage what to do when something goes
wrong. This should be the most important design principle to remember.
We also want to stress the importance of iteratively testing the UI design. In practice,
users, their tasks, and the work environment are often too varied to be able to have a good
design without going through a detailed and thorough test program, especially for new areas
like spoken language applications.
18.3.1.1.
Human Limitations
Applications make constant demands on human capabilities. When we design the user
interface, we must understand that humans have cognitive, auditory, visual, tactile, and
motor limits. As a general principle, we should make the user interface easy to use, and not
overload any of these limits.
For example, if the font is too small, the speech volume is too low, or the application
requires people to constantly switch between a mouse and a keyboard, we have a poor
interface, leading to lowered productivity. People have trouble remembering long complex
messages spoken to them in a continuous fashion. If these messages are delivered with
synthesized speech, the problem is worsened, because of the greater attention required to
understand the synthesized speech. Effective user interface design requires a balance of easy
access to information and an awareness of human limitations in processing this information.
In general, people tend to handle well 7 (+ or – 2), pieces of information at a time.
18.3.1.2.
User Accommodation
The interface and application should accommodate users as much as possible. This means
that the design needs to match the way users want to do their work, not the other way
around. People always have, or quickly create, a mental model of how the interface operates.
You need to design and communicate a conceptual model that works well with the users’
mental model. Their mental model should match the actual interface and the task, which is
very important for the designer to consider consciously.
To accommodate users needs, you ‘must ask whether the interface you are designing
helps people get their work done efficiently. Will it be easy to learn and use? Will it have an
attractive and appropriate design? Will the interface fit individual tasks with a suitable
modality? Will the interface allow the user to perceive that they are in control? Will the
interface be flexible enough to allow the user to adjust the design for custom use? Does the
interface have an appropriate tempo? Notice that speech is temporal. Does the interface
contain sufficient user support if the user needs or requests it? Does the interface respond to
the use in a timely fashion? Does the interface provide appropriate feedback to the user
about the results of the actions and application status?

Speech Interface Design
913
18.3.1.3.
Modes of Interaction
The interface is unimodal if only one mode is used as both input and output modality. By
contrast, multimodal systems use additional modalities in exchanging information with their
users. When we interact with each other, we often rely on multimodal interaction, which is
normal for us.
When speech is combined with other modalities of information presentation and
exchange, it is generally more effective than a unimodal interface, especially for dealing
with errors from speech recognition and understanding.
As an input modality, speech is generally not as precise as mouse or pen to perform
position-related operations. Speech interaction can also be adversely affected by the ambient
noise. When privacy is of concern, speech is also disadvantageous, since others can overhear
the conversation. Despite these disadvantages, speech communication is not only natural but
also provides a powerful complementary modality to enhance the existing keyboard- and
mouse-based graphical user interface. As an input-output modality, speech has a number of
unique characteristics in comparison with the traditional modalities:
 Speech does not require any real estate in the interface. For ubiquitous
computing, this is particularly suitable for devices that are as small as our
watches.
 Speech can be used at a distance, which makes it ideal for hands-busy and eyes-
busy situations such as driving.
 Speech is expressive and very suitable to bring information that is hidden to the
forefront. This particularly suitable in the GUI environment or with handheld
devices as the monitor space is always limited.
 Speech is omnidirectional and can communicate information to multiple users
easily. This also has implications relating to privacy.
 Speech is temporal and sequential. Once uttered, auditory information is no
longer available. This can place extra memory burden on the user and severely
limit the ability to scan, review, and cross-reference information.
 For social interfaces, speech plays a very important role. For example, if we
need to pose a question, make a remark, and delegate a task, there is no more
natural way than speech itself.
Because of these unique features, we need to leverage the strengths and overcome the
technology limitations that are associated with the speech modality. The conventional GUI
relies on the visual display of objects of interest, the selection by pointing, and continuous
feedback.
Direct
manipulation
is
generally inadequate
for
supporting
fundamental
transactions in applications such as word processing and database queries, as there are only
limited means to identify objects, not to mention that ambiguity in the meanings of icons
and limitations in screen display space further complicate the usability of the interface.
Generally, multimodality can dramatically enhance the usability of speech, because
GUI and speech have complementary strengths and weaknesses. The strengths of one can be
used to offset the weaknesses of others. For example, pen and speech can be complementary

Speech Interface Design
914
and they can be used very effectively for handheld devices. You can use Tap and Talk as
discussed in Section 18.4 to activate microphone and select appropriate context for speech
recognition. The respective strengths and weaknesses are summarized in Table 18.2. The
advantage of pen is typically the weakness of speech and vice versa. This implies that user
interface performance and acceptance could increase by combining both. Thus, visible,
limited, and simple actions can be enhanced by nonvisible, unlimited, and complex actions.
The multimodal interface is particularly suitable to people for completing contrastive
functions or when task attributes are perceived as separable. For example, speech and pen
can be used in different ways to designate a shift in context or functionality. Although most
people may not prefer to use speech and pen simultaneously, they like to use speech to enter
data and pen for corrections. Other contrastive tasks may include data versus commands and
digits versus text. When inputs are perceived as integral, unimodal interfaces are efficient
and work best.
Table 18.2 Complementary strengths of pen and speech for multimodal user interface.
Pen
Speech
Direct manipulation, requires hands and
eyes
Hands/eyes-free manipulation
Good at performing simple actions
Good at performing complex actions
Visual feedback
Visual and location independent
references
No reference ambiguity on the display
Multiple ways to refer to objects
18.3.1.4.
Technology Considerations
Speech recognition applications can be divided into many classes, as illustrated in Figure
18.7. The easiest category is the system that can only deal with isolated speech with limited
vocabulary for a command-and-control application, where typically a context-free grammar
is used to constrain the search space, resulting in a limited number of ways users can issue a
command. The hardest is the one that can support channel- and speaker-independent
continuous
speech
recognition
with
both
dictation
and
robust
application-specific
understanding capability. This is also the one that can really add value to a wide range of
speech applications. In Figure 18.7, a command-and-control application implies application-
specific restrictive understanding capability—typically with less than 1000-word vocabulary
and with a lower perplexity context-free grammar. A dictation application implies general-
purpose dictation capability—typically with more than 60,000 words in the lexicon and a
stochastic n-gram language model. A dictation and SLU application implies both general-
purpose dictation and less restrictive application-specific spoken language understanding
capability—typically with 60,000 words in the lexicon with both n-gram and semantic
grammar language models for both recognition and understanding.
Although a number of parameters can be used to characterize the capability of speech
recognition systems, user-centered design often dictates that you need to use the most
challenging technology solutions. The speaking style (from isolated speech to continuous
speech) can dramatically change the performance of a speech recognition system. Natural,

Speech Interface Design
915
spontaneous, continuous speech remains the most difficult to recognize; but it is more user
friendly. Small vocabulary (less than 50 words) is generally more accurate than large
vocabulary (greater than 20,000 words); but the small vocabulary limits the options the user
has on the task. The lower-perplexity language model generally has a great recognition
accuracy; but it imposes a considerable constraint on how the user can issue a command.
Most systems may work in the high-SNR (>30 dB) environment; but it is desirable that the
system should still work when the SNR is below 10 dB.
Command &
Control
Dictation only
Dictation &
SLU
Easiest
Isolated Speech
+Continuous speech
+Environment-independence
Hardest
+Speaker-independence
Figure 18.7 Illustrative degrees of difficulty for different classes of speech recognition (gray
level from easy to hard).
Associated with the input modality is the speech output modality for many speech
applications. Speech as an output medium taxes user memory and may even cause loss of
context and distraction from the task if it is too verbose. Most of the text-to-speech systems
have a problem generating a prosody pattern that is dynamic and accurate enough to convey
the underlying semantic message. Recorded speech still has the best quality; but it is
generally inflexible in comparison to TTS. There are many ways to provide additional
information to the TTS so that the final synthesized quality can be improved dramatically.
Examples include the use of recorded speech for constant messages, transplanted prosody,
and concept-to-speech synthesis as discussed in Chapter 16.
Improved quality and ease of use often need expensive authoring. This is true for
speech recognition, synthesis, and understanding. For example, to support channel
independence, you need not only additional channel normalization algorithms to deal with
both additive and convolutional noises but also realistic training data to estimate model
parameters. To support robust understanding, you need to author semantic grammars that
have broad coverage. This often requires tedious authoring and labor-intensive data
collection and annotation to augment the grammar. Ultimately, you need a modular and
reusable system that can warrant the development costs. To make effective use of dialog and
error-repair strategies, you need to adopt tools for preventing dialog design problems during
the early stages, ones that guide the choice of words in dialog and feedback and ensure
usability and correctness in the context, etc. In addition, as discussed in Chapter 17, whether
you have a system-directed dialog, user-directed dialog, or mixed-initiative dialog will have
significant ramifications on the authoring cost.
A spoken language application requires certain hardware and software on the user's
computer in order to run.13 Most new sound cards work well for speech recognition and text-
13 Speech recognition engines currently on the market typically require a Pentium 200 or faster processor. Speech
recognition for command and control consumes 1 to 4 MB RAM, and dictation requires an additional 16-32 MB.
Text-to-speech engines use about 1-3 MB, but high-quality TTS can require more than 64 MB.

Speech Interface Design
916
to-speech, including Sound Blaster™. Some old sound cards are only half duplex (as
opposed to full duplex). If a sound card is half duplex, it cannot record and play audio at the
same time. Thus it cannot be used for barge-in and echo cancellation. The user can choose
among different kinds of microphones: a close-talk headset microphone that is held close to
the mouth, a medium-distance microphone, or a microphone array device that rests on the
computer 30 to 60 centimeters away from the speaker. A headset microphone is often
needed for noisy environments, although microphone array or blind separation techniques
have the potential to close the gap in the future (see Chapter 10). Most speech products also
need to calibrate the microphone gain by speaking one or two utterances, as illustrated in
Figure 18.8. It adjusts the gain of the amplifier to make sure there is an appropriate gain
without clipping the signal. This can be done in the background without distracting the user.
Figure 18.8 Microphone setup wizard used in Microsoft Speech SDK 4.0. Reprinted with
permission from Microsoft Corporation.
18.3.2.
Handling Errors
Speech recognition and understanding can never be perfect. Current spoken language
processing work is very much a matter of minimizing errors. Together with the underlying
speech-engine technologies, the role of the interface is to help reduce the errors or minimize
their severity and consequences. As discussed in earlier chapters, there are errors associated
with word deletion, word insertion, and word substitution. The recognizer may be unable to
reject noises and classify them as regular words. The speaker may also have new words that
are not in the lexicon, or may have less clear articulation due to accent, stress, sickness, and
excitement.

Speech Interface Design
917
To repair errors in the system, we need to take each individual application into
consideration, since the strategy is often application-dependent. To have a compelling
application, we must design the application with both the limitation of the engine and the
interaction of the engine and UI in mind. For example, multimodal applications are
generally more compelling and useful than speech-only solutions as far as error handling is
concerned. A multimodal application can overcome many limitations of today’s SLU
technologies, but it may be inappropriate for hands-busy or eyes-buy scenarios.
18.3.2.1.
Error Detection and Correction
We need to detect errors before we can repair them. This is a particularly difficult problem
that often requires high-level context and discourse information to decipher whether there
are errors. We can certainly use confidence measures, as discussed in Chapter 9, to detect
potential errors, although confidence measures alone are not particularly reliable.
There are many ways to correct the errors. The choice of which one to use depends on
the types of the applications you are developing. We want to emphasize that correction
based on another modality is often desirable, since it is unusual that two independent
modalities make the same mistake. For example, speech recognition errors are often
different from handwriting recognition errors. In addition to the complementary nature of
these different errors, we can also leverage other modalities for improved error-correction
efficacy. For example, a pen is particularly suitable for pointing, and we can use that to
locate misrecognized words. A camera can be used to track whether the speaker is speaking
in order to eliminate background noise generated from other sound sources.
Let’s consider a concrete example. If you have a handheld device, you may be able to
see the errors and respeak the desired commands or use a different modality such as pen to
help task completion. When you want to create an e-mail using speech, and a dictation error
occurs, you can click the misrecognized word or phrases to display a list of n-best
alternatives. You can select the correct word or phrase from the n-best list with the pen.
Alternatively, you can respeak the highlighted word or phrase. Since the correct answer
should not be the previous misrecognized word or phrase, you can exclude them in the
second trial. If it is an isolated word in the correction stage, you can also use the correct
silence context triphone models for the word boundary as the correct acoustic model. You
can also use the correct n-gram language model from the left context with the assumption
that the left words are all correct. With these correct acoustic and language models, it is
expected that the second trial would have a better chance to get the right answer.
If you have telephony-based applications, you can count on keywords such as no, I
meant to identify possible recognition or understanding errors. When errors occur, you
should provide specific error-handling messages so that the user knows what response the
system wants. An adequate error message should tell the user what is wrong, how to correct
it, and where to get help if needed. Let’s consider an example of the following directory
assistant application:

Speech Interface Design
918
Computer: Please speak the name
User: Mike
Computer: Please speak the name
User: Mike
Clearly, the user does not know what responses the computer wants. An improved
dialog can be:
Computer: Please speak the first and last name
User: Mike Miller
Computer: Which division is Mike Miller working?
User: Research group
You can use alternative guesses intelligently for telephony applications, too. For
example, if the user says “Mike Miller” and the computer is unsure of what was said, it can
prompt the user to repeat with a constrained grammar. In the example above, if the computer
is unsure whether the user said Mike or Mark, it may want to respond with “Did you say
Mike Miller or Mark Miller?” before it proceeds to clarify which division Miller is working
in.
When an error is detected, the system should be adapted to learn from the error. The
best error-repair strategy is to use an adaptive acoustic and language model so that the
system can learn from the errors and seldom repeat the same mistake. This is why adaptive
acoustic and language modeling, as discussed in Chapters 9 and 11, is critical for real
acceptance of speech applications. When an error occurs and the computer understands the
error via user correction, you can use the corrected words or phrases to adapt related
components in the system.
18.3.2.2.
Feedback and Confirmation
Users need feedback from the system so that they understand the status of the interaction.
When a command is issued, the user expects the system to acknowledge that the command
is heard. It is also important to tell the users if the system is busy so that they know when to
wait. Otherwise, the users may interpret the lack of response or feedback as errors of
recognition or understanding.
If the user has requested that an action to be taken, the best response is to carry out the
action as requested but if the action cannot be carried out, the user must be notified.
Sometimes, the user may need this feedback to know whether it is her turn to speak again. If
the interface is multimodal, you can use visual feedback to indicate the status of the
interaction. For example, in Microsoft’s MiPad research prototype, when the microphone is
activated with a pen pointing to a specific field, the microphone levels are indicated in the

Speech Interface Design
919
feedback rectangle in strip-chart fashion as shown in Figure 18.9 for the corresponding field.
Initially this wipes from left to right, but when the right edge is reached, the contents scroll
to the left as each new sample is added. The height of each sample indicates the audio level;
there is some indication given for the redline level for a possible saturation. It also shows the
total number of frames as well as the number of remaining frames to be processed. Then, as
the recognizer processes, the black bar wipes from left to right to indicate its progress. When
the recognizer completes (or is canceled by the user), the dynamic volume meter is removed.
The volume meter feedback described addresses a number of issues that are critical to most
users: if the computer is listening or not; if the volume is too high or not, and when the
computer finishes processing the speech data.
Figure 18.9 MiPad’s dynamic volume meter. Reprinted with permission from Microsoft
Corporation.
If the interface has no visual component, you can use auditory cues or speech
feedback. If there is more than a 3-second delay after the user issues a command and the
system responds, an auditory icon during the delay, such as music, may be used. A delay of
less than 2-3 seconds is acceptable without using any feedback.
Since there are errors in spoken language systems, you may have to use confirmation
questions to assure that the computer heard the correct message. You can associate risks
with different commands, so you can have a different level of strategy for confirmation. For
example, format disk should have a higher threshold or may not be allowed by voice at all.
You can have an explicit confirmation before executing the command. You can also have
implicit confirmation based on the level of the risks in the corresponding confidence scores
from the recognizer. For example, if the user’s response has more than one possible meaning
(X and Y) or the computer is not confident about the recognized result (X or Y), the
computer could ask, “Do you want to do X or Y?” You want to be specific about what the
system needs. A prompt like “Do you mean X or Y?” is always better than Please repeat.
You have to balance the cost of making an error with the extra time and annoyance in
requiring the user to confirm a number of statements.
In a telephony-based application, you need to tell the users specifically that they are
talking to a computer, not a real person. When AT&T used a long greeting for its telephone
customer service, their prompts were as follows:
“AT&T Automated Customer Service. This service listens to your speech and sends
your call to the appropriate operator. How many I help you?”
The longer version resulted in shorter utterances from the people calling in. The short
automated version did not help shorten the utterances, because people did not seem to catch
the automated connotation.
You can also design the prompt in such a way that you can constrain how people
respond or use their words. Let’s consider an example of the following directory assistant
application:

Speech Interface Design
920
Computer: Welcome to Directory Assistant. We look forward to serving you.
How can I help you?
User: I am interested in knowing if you can tell me the office number of Mr.
Derek Smith?
To have the user follow the lead of the computer, an improved dialog can be:
Computer: Welcome to Directory Assistant. Please say the full name of the
person.
User: Derek Smith
When the system expects the user to respond with both the first name and last name,
you should still design the grammar so that the system can recognize a partial name with
lower probabilities. This makes the system far more robust—a good example of combining
engine technology and interface seamlessly for improved user satisfaction.
18.3.3.
Other Considerations
When speech is used as a modality, you should remember the following general principles:
18.3.3.1.
Don't Use Speech as an Add-On Feature
It's poor design to just bolt speech recognition onto an application that is designed for a
mouse and keyboard. Such applications get little benefit from speech recognition. Speech
recognition is not a replacement for the keyboard and mouse, but in some circumstances it is
a better input device than either. Speech recognition makes a terrible pointing device, just as
the mouse makes a terrible text entry device, or the keyboard is bad for drawing. When
speech recognition systems were first bolted onto the PC, it was thought that speaking menu
names would be really useful. As it turns out, very few users use speech recognition to
access a window menu.
18.3.3.2.
Give Users Options
Every feature in an application should be accessible from all input devices: keyboard,
mouse, and speech recognition. Users naturally use whichever input mechanism provides
them the quickest or easiest access to the feature. The ideal input device for a given feature
may vary from user to user.

Speech Interface Design
921
18.3.3.3.
Respect Technology Limitations
There are a number of examples of poor use of speech recognition. Having a user spell out
words is a bad idea for most recognizers, because they are too inaccurate unless you
constrain the recognizer for spelling. An engine generally has a hard time to detect multiple
speakers talking over each other in the same digital-audio stream. This means that a
dictation system used to transcribe a meeting will not perform accurately during times when
two or more people are talking at once. An engine cannot hear a new word and guess its
spelling unless the word is specified in the vocabulary. Speakers with accents or those
speaking in nonstandard dialects can expect more errors until they train the engine to
recognize their speech. Even then, the engine accuracy will not be as high as it would be for
someone with the expected accent or dialect. An engine can be designed to recognize
different accents or dialects, but this requires almost as much effort as porting the engine to
a new language.
18.3.3.4.
Manage User Expectations
When you design a speech recognition application, it is important to communicate to the
user that your application is speech aware and to provide him or her with the commands it
understands. It is also important to provide command sets that are consistent and complete.
When users hear that they can speak to their computers, they instantly think of Star Trek and
2001: A Space Odyssey, expecting that the computer will correctly transcribe every word
that they speak, understand it, and then act upon it in an intelligent manner. You should also
convey as clearly as possible exactly what an application can and cannot do and emphasize
that the user should speak clearly, using words the application understands.
18.3.4.
Dialog Flow
Dialog flow is as important to speech interface as screen design is to GUI. A robust dialog
system is a prerequisite to the success of the speech interface. The dialog flow design and
the prompting strategy can dramatically improve user experience and reduce the task
complexity of the spoken language system.
18.3.4.1.
Spoken Menus
Many speech-only systems rely on spoken menus as the main navigation vehicle when no
sophisticated dialog system is used. The design of these menus is, therefore, critical to the
usability of the system. If there are only two options, you can have “Say 1 for yes, or 2 for
no.” However, if a menu has more than two options, you should describe the result before
you specify the menu option. For example, you want to have system say “For email say 1.
For address book say 2…” instead of “Say 1 for email. Say 2 for address book….” This is
because the user may forget which choice he wants before hearing all the menu options.

Speech Interface Design
922
You should not use more than five options at each level. If there are more than five,
you should consider submenus. It is important to place the most frequently used options at
the top of the menu hierarchy.
You should also design the menu so that commands sound different. As a rule of
thumb, the more different phonemes you have between two commands, the more different
they sound to the computer. The two commands, go and no, only differ by one phoneme, so
when the user says, “Go” the computer is likely to recognize “No.” However, if the
commands were “Go there” and “No way” instead, recognition would be much better.
18.3.4.2.
Prompting Strategy
When you design the prompt, use as few words as possible. You should also avoid using a
personal pronoun when asking the user to respond to a question. This gives the user less to
remember, as speech output is slow and one-dimensional. Typical users are able to
remember three to four menu or prompt options at a time. You should also allow users to
interrupt the computer with barge-in techniques, as discussed in Chapter 10.
It is often a good idea to use small steps to query the user at each step if application
has a speech-only interface progressively. You can start with short high-level prompts such
as “How can I help you?” If the user does not respond appropriately, the system can provide
a more detailed prompt such as “You can check your email, calendar, address book, and
your home page.” If the user still does not respond correctly, you can give a more detailed
response to tell the user how to speak the appropriate commands that the computer can
understand.
If the application contains users’ personal information, you may want to treat novice
users and experienced users differently. Since the novice users are unfamiliar with the
system and dialog flow, you can use more detailed instructions and then reduce the number
of words used the next time when the user is going through the same scenario.
18.3.4.3.
Prosody
For TTS, prosodic features, including pitch, volume, speed, and pause, are very important
for the overall user experience. You can vary pitch and volume to introduce new topics and
emphasize important sections. Increased pitch dynamic range also makes the system sound
lively.
Users tend to mimic the speed of the system. If the system is reacting and speaking
slowly, the user may tend to slow down. If the computer is speaking quickly, the user may
speed up. An appropriate speech rate for natural English speech is about 150 to 170 words
per minute.
Pauses are important in conversation, but they can be ambiguous, too. We generally
use pauses to suggest that it is the user’s turn to act. On the other hand, if the system has
requested a response from the user and the user has not responded, you should use the time-
out period (typically 10 seconds) to take appropriate actions, which may be a repeat of a
more detailed prompt or may be a connection to an operator. People are generally

Internationalization
923
uncomfortable with long pauses in conversation. If there are long pauses in the dialog flow,
people tend to talk more and use less meaningful words, resulting in more potential errors.
18.4.
INTERNATIONALIZATION
To meet the demands of international markets you need to support a number of languages.
For example, Microsoft Windows 2000 supports more than 40 languages. Software
internationalization
typically
involves
both
globalization
and
localization
phases.
Globalization is the process of defining and developing a product such that its core features
and code design do not make assumptions about a single language. To adapt the application
for a specific market, you need to modify the interface, resize the dialog boxes, and translate
text to a specific market. This process is called localization. The globalization phase of
software internationalization typically focuses on the design and development of the
product. The product designers plan for locale-neutral features to support multicultural
conventions such as address formats and monetary units. Development of the source code is
typically based on a single code base to have the capability to turn locale-specific features
on and off without modifying the source code, and the ability to correctly process different
character encoding methods. Products developed with the single worldwide binary model
typically include complete global functionality, such as support for Input Method Editors
used in Asian languages and bidirectional support for Arabic and Hebrew languages.
In addition to the most noticeable change in the translated UI, spoken language
engines such as speech recognition or text-to-speech synthesis require significant
undertakings. You need to have not only the lexicon defined for the specific language but
also a large amount of speech and text data to build your acoustic and language model.
There are many exceptional rules for different languages. Let’s take dictation application as
an example. When you have inverse text normalization, whole numbers may be grouped
differently from country to country, as shown in Table 18.3. In the United States, we group
three digits separated by a comma. However, many European locales use a period as the
number separator, and Chinese conventions typically do not group numbers at all.
Table 18.3 Examples of how numbers are grouped by different locales for inverse text
normalization.
123,456,789.0
123456789.0
12,34,56,789,0
United States and most locales
Chinese
Hindi
For the speech recognition engine localization, there are a number of excellent studies
on both Asian [5, 15, 16, 21, 24, 32, 34, 37] and European languages [7, 8, 20, 22, 27, 31,
40]. Many spoken language processing components, such as the speech signal processing
module, the speech decoder, and the parser, are language independent. Major changes are in
the content of the engine, such as the lexicon and its associated processing algorithm, the
grammar, and the acoustic model or language model parameter.

Case Study—MiPad
924
In general, it is easier to localize speech recognition than either text-to-speech or
spoken language understanding. This is because a speech recognition engine can be mostly
automatically derived from a large amount of speech and text data, as discussed in Chapters
9 through 13. For Chinese, you need to have special processing for tones, and the signal-
processing component needs to be modified accordingly [4]. For both Chinese and Japanese
the lexicon also needs to be carefully selected, since there is no space between two words in
the lexicon [12, 46]. As discussed in Chapter 17, the parser can be used without much
modification for SLU systems. Most of localization efforts would be on the semantic
grammar and dialog design, which is language specific by nature.
As discussed in Chapter 14 and 15, the TTS front end and symbolic prosody
components are much harder to localize than the TTS back end. The TTS back end can be
easily localized if you use the trainable approach discussed in Chapter 16 on the assumption
that the speech recognizer for the language is available. The internationalization process
mainly consists of creating tools that can generate training phrase lists from target language
texts and dictionaries, and ensuring that any reasonable-sized set of symbolic distinctive
language sound inventories (phone sets) can be accepted by the system. The human vocal
tract is largely invariant across language communities, and this relative invariance is
reflected in the universality of the speech synthesis tools and methods.
When the semantic and lexicon content is localized, you should be aware that the
incorrect use of sensitive terms can lead to outright product bans in some countries, while
other misuses are offensive to the local culture. For example, Turkey forbids the use of the
term Kurdistan in text and any association of Kurdistan with Turkey. Knowing proper
terminology is critical to the success of your application internationalization.
18.5.
CASE STUDY—MIPAD
An effective speech interface design requires a rigorous process that typically consists of
three steps with a number of iterations based on the user evaluation:
 Specifying the application
 Rapid prototyping
 Evaluation
We use Microsoft’s research prototype MiPad [18] as an example to illustrate how
you can use the process to create an effective speech application. As a wireless Personal
Digital Assistant (PDA), MiPad fully integrates continuous speech recognition (CSR) and
spoken language understanding (SLU) to enable users to accomplish many common tasks
using a multimodal interface and wireless technologies. It tries to solve the problem of
pecking with tiny styluses or typing on minuscule keyboards in today’s PDAs or smart
phones. It also avoids the problem of being a cellular telephone that depends on speech-only
interaction. MiPad incorporates a built-in microphone that activates whenever a field is
selected. As a user taps the screen or uses a built-in roller to navigate, the tapping action
narrows the number of possible instructions for spoken language processing. MiPad runs on
a Windows CE Pocket PC with a Windows 2000 Server where speech recognition is

Case Study—MiPad
925
performed. The CSR engine has a 64,000 word vocabulary with a unified context-free
grammar and n-gram language model as discussed in Chapter 11. The SLU engine is based
on a robust chart parser and a plan-based dialog manager discussed in Chapter 17.
18.5.1.
Specifying the Application
The first step to develop an application is to identify key features that can help users. You
need to understand what work has been done, and how that work can be used or modified.
You need to ask yourself why you need to use speech for the applications and review project
goals and time frames. The first deliverable will be the spec of the application that includes a
documented interface design and usability engineering project plan. Some of the key
components should include:
 Define usability goal, not technology
 Identify the user groups and characteristics, interview potential users, and
conduct focus-group studies
 Create system technology feasibility reports
 Specify system requirements and application architecture
 Develop development plan, QA test plan, and marketing plan
You should develop task scenarios that adopt a user’s point of view. Scenarios should
provide detail for each user task and be in a prose or dialog format. You may also want to
include frequency information as a percentage to illustrate whether a particular step is
performed more than 20% or 80% of the time, which helps you focus on the most common
features first.
To create a conceptual interaction model, not the software architecture, we need to
have a mental model for the users. Users have to manipulate interface objects either literally
or figuratively in the navigation. Interfaces without clear and consistent user objects are
difficult to use. Generally, you need a script flow diagram if speech is used. This describes
how the user moves through dialogs with the system. To create a script flow diagram from
your scenarios, you can start at the beginning of a scenario and derive a set of possible
dialogs the user needs to have with the system to complete the scenario. You can draw a box
representing a dialog with arrows that shows the order and relationship among the dialogs. If
your interface involves graphic display, you need to include window flow diagrams as well,
which organize objects and views and define how the user moves from one object to
another. Of course, these two diagrams may be combined for effective multimodal
interaction.
When the initial spec is in place, you can use the conceptual model to create actual
paper windows and dialogs for the interface. You must iterate the design several times and
get feedback early. Paper prototyping allows you to do these tasks most efficiently. It is
useful to conduct walkthroughs with a number of people as though they were performing the
task. You may want to start usability testing to collect real data on real users before the
software is implemented. Even a system that is well designed and follows the best proactive

Case Study—MiPad
926
approachmay reveal some usability problems during testing. Usability testing has
established protocols, procedures, and methodologies to get reliable data for design feedback
[36, 42]. Generally you need to decide what to test, create usability specifications for the
tasks, select the right user groups, specify exact testing scenarios, conduct the actual tests
with participants, analyze the data, and report critical findings.
How did we do this for MiPad? Since MiPad is a small handheld device, the present
pen-based methods for getting text into a PDA (Graffiti, Jot, soft keyboard) are barriers to
broad market acceptance. As an input modality, speech is generally not as precise as mouse
or pen to perform position-related operations. Speech interaction can be adversely affected
by the ambient noise. When privacy is of concern, speech is also disadvantageous, since
others can overhear the conversation. Despite these disadvantages, speech communication is
not only natural but also provides a powerful complementary modality to enhance the pen-
based interface. Because of these unique features, we need to leverage the strengths and
overcome the technology limitations that are associated with the speech modality. Pen and
speech can be complementary and they can be used very effectively for handheld devices.
You can tap to activate a microphone and select appropriate context for speech recognition.
The advantage of pen is typically the weakness of speech and vice versa. This implies that
user interface performance and acceptance could increase by combining both. Thus, visible,
limited, and simple actions can be enhanced by nonvisible, unlimited, and complex actions.
Table 18.4 Benefits of having speech and pen for MiPad.
Action
Benefit
Ed uses MiPad to read an e-mail, which
reminds him to schedule a meeting. Ed taps
to activate microphone and says Meet with
Peter on Friday.
Using speech, information can be accessed
directly, even if not visible. Tap and talk also
provides increased reliability for speech
detection.
The screen shows a new appointment to
meet with Peter at 10:00 on Friday for an
hour.
An action and multiple parameters can be
specified in only a few words.
Ed taps Time field and says Noon to one
thirty
Field values can be easily changed using field-
specific language and semantic models
Ed taps Subject field dictates and corrects a
couple of sentences explaining the purpose
of the meeting.
Bulk text can be entered easily and faster.
People tend to like to use speech to enter data and pen for corrections and pointing. As
illustrated in Table 18.4, MiPad’s Tap and Talk interface offers a number of benefits. MiPad
has a Tap and Talk field that is always present on the screen, as illustrated in MiPad’s start
page in Figure 18.10 (a) (the bottom gray window is always on the screen).
The user can give spontaneous commands by tapping the Tap and Talk field and
talking to it. The system recognizes and parses the command, such as showing a new
appointment form. The appointment form shown on MiPad’s display is similar to the
underlying semantic objects. The user can have conversation by tapping and talking to any
subfield as well. By tapping to the attendees field in the calendar card shown in Figure 18.10
(b), for example, the semantic information related to potential attendees is used to constrain

Case Study—MiPad
927
both CSR and SLU, leading to a significantly reduced error rate and dramatically improved
throughput. This is because the perplexity is much smaller for each subfield-dependent
language and semantic model. General text fields, such as the title or body of an e-mail, call
for general dictation. Dictation is handled by the same mechanism as other speech entries:
The user dictates while tapping the pen where the words should go. To correct a
misrecognized word, a tap on the word invokes a correction window, which contains the n-
best list and a soft keyboard. The user may hold the pen on the misrecognized word to
dictate its replacement.
(a)
(b)
Figure 18.10 Concept design for (a) MiPad’s main card and (b) MiPad’s calendar card.
From the start page, you can reach most application states. There are also a number of
short-cut states that can be reached via the hardware buttons. The design is refined
incrementally in a number of iterations.
18.5.2.
Rapid Prototyping
When you have the spec for the application, you need to communicate the design to the
development team so they can rapidly develop a prototype for further iterations. In practice,
handing off an interface to a development team without involvement in its implementation
seldom works. This is because the developers always have questions about the design
document. You need to be available to them to interpret your design. Technical and practical
issues may also come up during development, requiring changes to the interface design.
For speech applications, there are a number of key components that have a significant
implication for the user interface design. As your design iterations progress, it is useful to
track the changes in these components and the effect that these changes have on the user
experience of the prototype.
 Semantic objects: Each semantic object, as discussed in Chapter 17, is
associated with the action that applications can take. We need to abstract the
application to a level such that we can represent all the actions that the

Case Study—MiPad
928
application can take with a number of semantic objects. Each semantic object
has needed slots that require a context-free grammar with a decent coverage.
You can start with example sentences and reuse some of the predefined task-
independent CFGs, such as date and time. The interface design can help to
constrain users to say the sentences that are within the domain.
 Dialog control: When appropriate semantic frames are selected, you need to
decide the flow of these semantic frames with an appropriate dialog strategy,
either speech-based or GUI-based, which should include both inter- and intra-
frame control and error-repair strategy. In addition, appropriate use of
confidence measures is also critical.
 Language models: You need to use appropriate language models for the speech
recognizer. Context-free grammars used for the semantic frames can be used for
the recognizer. As coverage is generally limited, it is desirable to use the n-gram
as an alternative. You can use a unified context-free grammar and n-gram for
improved robustness, as introduced in Chapter 11.
 Acoustic models: In comparison to the semantic frames and language models,
subphonetic acoustic models can be used relatively effectively for task-
independent prototyping. Nevertheless, mismatches such as microphones,
environment, vocabulary, and speakers are critical to the overall performance.
You may want to consider speaker-dependent or environment-dependent
prototyping initially, as this minimizes the impact of various mismatches.
 TTS models: If the speech output is well defined, you can adapt the general-
purpose TTS systems with the correct prosody patterns. This can be achieved
with the transplanted prosody in which variables can be reserved for TTS to deal
only with proper nouns or phrases that cannot be predecided. You can also adapt
acoustic units such that the concatenation errors can be minimized.
The work needed for rapid prototyping is significant, but less than it might at first
appear to be. Earlier tasks can be repeated in later iterations when more data become
available in order to further improve the usability of the overall system.
18.5.3.
Evaluation
It is vital to have a rigorous evaluation and testing program to measure the usability of the
prototype. It is a good practice to evaluate not only the entire system but also each individual
component. The most important measure should be whether users are satisfied with the
entire system. You can ask questions such as: Is the task completion time is much better? Is
it easier to get the job done? Did we solve the major problem existing in the previous
design? Most of the time, we may find that the overall system can’t meet your goal. This is
why you need to further analyze your design and evaluate each individual component from
UI to the underlying engines.
Before the prototype becomes fully functioning, you may want to use Wizard-of-Oz
(WOZ) experimentation to study the impact of the prototype. That is, you have a real person

Case Study—MiPad
929
(the wizard) to control some of the functions that you have not implemented yet. The person
can carry out spoken interactions with users, who are made to believe that they are
interacting with a real computer system. The goal of WOZ is to study the behavior of
human-computer interactions. By producing data on the interaction between a simulated
system and its users, WOZ provides the basis for early tests of the system and its feasibility
prior to implementation. It can simulate the error patterns in the existing spoken language
system.
WOZ is still expensive, as you have the cost of training a wizard in addition to the
usual costs of selecting experimental subjects, transcribing the session, and analyzing overall
results. The use of WOZ has so far been justified in terms of relatively higher cost of having
to revise an already implemented system that turned out to be flawed in the usability study.
In practice, WOZ may be replaced by the implementation-test-revise approach based
on available tools. Whether or not WOZ is preferable depends on several factors, such as the
methods and tools available, the complexity of the application to be designed, and the risk
and cost of implementation failure. Low complexity speaks in favor of directly
implementing the interface and application without interposing a simulation phase. On the
other hand, high complexity may advocate iterative simulations before a full-fledged
implementation.
To evaluate each component, you often need to select users that typify a wide range of
potential users. You can collect representative data from these typical users based on the
prototype we have. The most important measures include language-model perplexity, speech
recognition accuracy and speed, parser accuracy and speed (on text and speech recognition
output), dialog control performance, prompting strategy and quality, error-repair efficacy,
impact of adaptive modeling, and interactions among all these components.
Effective evaluation of a user interface depends on a mix of objective and subjective
measures. One of the most important characteristics is repeatability of your results. If you
can repeat them, then any difference is likely due to the changes in the design that you are
testing. You will want to establish reference tasks for your application and then measure the
time that it takes subjects to complete each of those tasks. The consistent selection of
subjects is important; as is the use of enough subjects to account for individual differences in
the way they complete the tasks.
How do we evaluate MiPad? For our preliminary user study, we did a benchmark
study to assess the performance in terms of task-completion time, text throughput (WPM),
and user satisfaction. The focal question of this study is whether the MiPad interface can
provide added value to the existing PDA interface.
Is the task-completion time much better? We had 20 computer-savvy users test the
partially implemented MiPad prototype. These people had no experience with PDA or
speech recognition software. The tasks we evaluated include creating a new e-mail,
checking a calendar, and creating a new appointment. Task order was randomized. We
alternated tasks to different user groups using either pen-only or Tap and Talk interfaces.
The text throughput was calculated during e-mail paragraph transcription tasks.14 Compared
to using the pen-only iPaq interface, we observed that the Tap and Talk interface is about
14 Transcription may not be a realistic task and gives an advantage to speech, because speech avoids a lot of
attention shifting that is involved with pen input.

Case Study—MiPad
930
twice as fast transcribing appointments15. For the overall command-and-control operations
such scheduling appointments, the Tap and Talk interface was about 33% faster than the
existing pen-only interface16. Error correction for the Tap and Talk interface was one of the
most unsatisfactory features. In our user study, calendar access time using the Tap and Talk
methods was about the same as pen-only methods, which suggests that simple actions are
very suitable for pen-based interaction.
Is it easier to get the job done? Most users we tested stated that they preferred using
the Tap and Talk interface. The preferences were consistent with the task completion times.
Indeed, most users’ comments concerning preference were based on ease of use and time to
complete the task, as shown in Figure 18.11.
Figure 18.11 Task completion time of e-mail transcription between the pen-only interface and
Tap and Talk interface.
18.5.4.
Iterations
With rigorous evaluations, you can revisit the components that are on the critical path. In
practice, you need to run several iterations to improve user interface design, semantic
frames, slot-associated CFGs, n-gram, dialog control, and acoustic models. Iterative
experiments are particularly important for deployment, as the coverage of the grammars can
be dramatically improved only after we accumulate enough realistic data from end users.
For MiPad, the performance improvement from using real data has been more
dramatic than any other new technologies we can think of. When we collected domain-
specific data, MiPad’s SLU error rate was reduced by more than a factor of two. The major
15 The corresponding speech recognition error rate for the tasks is about 14%, which is based on using a close-talk
microphone and a speaker-dependent acoustic model trained from about 20 minutes of speech.
16 The SLU (at card level) error rate for the task is about 4%.
email transcription tasks
00:00.0
01:26.4
02:52.8
04:19.2
05:45.6
07:12.0
08:38.4
10:04.8
11:31.2
211 212 221 222 231 232 241 242 251 252 261 262 271 272 281 282
task
time (s)
Tap & Talk
pen-only
s

Historical Perspective and Further Reading
931
improvement came from broadening the semantic grammars from real data. We believe that
data collection and iterative improvement should be an integral part of the process of
developing interactive systems.
Despite unquestionable progress in the last decade, there are many unsolved problems
in the procedures, concepts, theory, methods, and tools. Designing an effective speech
application and interface remains as much of an art as it is an exact science with established
procedures of good engineering practice. The quest for an effective speech application has
been the goal for many speech and interface researchers. A successful speech application
needs to significantly improve people’s productivity. We believe that speech will be one of
the most important technology components to enable people access information more
effectively in the near future.
MiPad is a work in progress to develop a consistent interaction model and engine
technologies for three broad classes of applications. A number of discussed features are yet
to be fully implemented and iteratively tested. Our currently tested features include PIM
functions only. Despite our incomplete implementation, we observed in our preliminary user
study that speech and pen have the potential to significantly improve user experience.
Thanks to the multimodal interaction, MiPad also offers a far more compelling user
experience than standard telephony interaction. The success of MiPad depends on spoken
language technology and always-on wireless connection. With upcoming 3G wireless
deployments in sight,17 the critical challenge for MiPad remains the accuracy and efficiency
of our spoken language systems, since MiPad may be used in noisy environments without a
close-talk microphone, and the server also needs to support a large number of MiPad clients.
18.6.
HISTORICAL PERSPECTIVE AND FURTHER READING
User interface design is particularly resistant to rigid methodologies that attempt to define it
simply as another branch of engineering. This is because design deals with human
perception and human performance. Issues affecting the user interface design typically
include a wide range of topics such as technology capability and limitations, product
productivity (speed, ease of use, range of functions, flexibility), esthetics (look and feel, user
familiarity, user impression), ergonomics (cognitive load, memory), and user education [2,
13, 14, 30, 35, 38, 44, 45]. The spoken language interface is a unique and specialized human
interface medium that differs considerably from the traditional graphical user interface, and
there are a number of excellent books discuss how to build an effective speech interface [1,
25, 39, 43].
When we communicate with speech, we must hear and comprehend all information
sequentially. This very serial or sequential nature makes speech less effective as an
information presentation medium than the graphics. As an input medium, speech must be
supplied one command or word at a time, requiring the user to remember and order
information in a way that is more taxing than it is with visual and mechanical interfaces. The
powerful and parallel presentation of information in today’s GUI in the form of menus,
icons, and windows makes GUI one of the most effective media for people to communicate.
17 http://www.wirelessweek.com/issues/3G

Historical Perspective and Further Reading
932
GUI is not only parallel but also persistent. That is, once presented, information remains on
the screen until replaced as desired. Speech has no such luxury, which places the burden of
remembering machine output onto the user. It is no wonder that a picture is worth a
thousand words. We believe this is one of the key reasons why the spoken language
interface has not taken off in comparison to the graphical user interface.
Although speech is natural, as humans already know how to talk effortlessly, this
common shared experience of humans may not necessarily translate to human-machine
interaction. This is because machines do not share a common cultural heritage with humans
and they do not posses certain assumptions about the reality of the world in which we live.
We have no precedent for effective speech interaction with nonsentient devices that are self-
aware. Our experience with natural speech interaction is based on the assumption that when
we talk to another person, the other person has some stake in the outcome of the interaction.
The result of this expectation is that structured and goal-oriented conventions are necessary
to steer people away from social speech behaviors toward task-oriented protocols, which
essentially eliminate our expectation of naturalness.
The fact that speech interactions are one dimensional and time consuming has
significant ramifications for interface design. The major challenge is to help the user
accomplish as much as possible in as little time as possible. The designer thus must
encourage the user to construct a mental model of the task and the interface that serves it,
thereby creating an illusion of forward movement that exploits time by managing and
responding to the user goals. Many telephony applications have carefully considered these
constraints and provided a viable alternative to GUI-based applications and interfaces.
Despite the challenges, a number of excellent applications set milestones for the
spoken language industry. Widely used examples include DECTalk (TTS), AT&T universal
card services, Dragon’s NaturallySpeaking dictation software, and Microsoft Windows 2000
TTS accessibility services. With the wireless smart devices such as smart phones, we are
confident that MiPad-like devices will find their way to empower people to access
information anywhere and anytime, leading to dramatically improved productivity.
There are a number of companies offering a variety of new speech products. Their
Web sites are the best reference points for further reading. The following Web sites contain
relevant spoken language applications and user interface technologies:
 AOL (http://www.aol.com)
 Apple (http://www.apple.com/macos/speech)
 AT&T (http://www.att.com/aspg)
 BBN (http://www.bbn.com)
 Dragon (http://www.dragonsystems.com)
 IBM (http://www.ibm.com/software/speech)
 Lernout & Hauspie (http://www.lhs.com)
 Lucent Bell Labs (http://www.lucent.com/speech)
 Microsoft (http://www.microsoft.com/speech)
 Nuance (http://www.nuance.com)

Historical Perspective and Further Reading
933
 NTT (http://www.ntt.com)
 Philips (http://www.speech.be.philips.com)
 Speechworks (http://www.speechworks.com)
 Tellme (http://www.tellme.com)
 Wildfire (http://www.wildfire.com)
REFERENCES
[1]
Balentine, B. and D. Morgan, How to Build a Speech Recognition Application,
1999, Enterprise Integration Group.
[2]
Beyer, H. and K. Holtzblatt, Contextual Design: Defining Customer-Centered
Systems, 1998, San Francisco, Morgan Kaufmann Publishers.
[3]
Campbell, J., "Speaker Recognition: A Tutorial," Proc. of the IEEE, 1997, 85(9),
pp. 1437-1462.
[4]
Chang, E., et al., "Large Vocabulary Mandarin Speech Recognition with Different
Approaches in Modeling Tones," Int. Conf. on Spoken Language Processing, 2000,
Beijing, China.
[5]
Chien, L.F., K.J. Chen, and L.S. Lee, "A Best-First Language Processing Model
Integrating the Unification Grammar and Markov Language Model for Speech
Recognition Applications," IEEE Trans. on Speech and Audio Processing, 1993,
1(2), pp. 221--240.
[6]
Dobler, S., "Speech Control In The Mobile Communications Environment," IEEE
ASRU Workshop, 1999, Keystone, CO.
[7]
Dugast, C., X. Aubert, and R. Kneser, "The Philips Large-Vocabulary Recognition
System for American English, French and German," Proc. of the European Conf.
on Speech Communication and Technology, 1995, Madrid pp. 197-200.
[8]
Eichner, M. and M. Wolff, "Data-Driven Generation of Pronunciation Dictionaries
In The German Verbmobil Project - Discussion of Experimental Results," IEEE Int.
Conf. on Acoustics, Speech and Signal Processing, 2000, Istanbul pp. 1687-1690.
[9]
Furui, S., "Recent Advances in Speaker Recognition," Pattern Recognition Letters,
1997, 18, pp. 859-872.
[10]
Galitz, W.O., Handbook of Screen Format Design, 1985, Wellesley, MA, Q. E. D.
Information Sciences Inc.
[11]
Galitz, W.O., User-Interface Screen Design, 1993, Wellesley, MA, Q. E. D.
Information Sciences Inc.
[12]
Gao, J., et al., "A Unified Approach to Statistical Language Modeling for Chinese,"
Int. Conf. on Acoustics, Speech and Signal Processing, 2000, Istanbul pp. 1703-
1706.
[13]
Hackos, J. and J. Redish, User and Task Anaysis for User Interface Design, 1998,
New York, John Wiley.
[14]
Helander, M., Handbook of Human-Computer Interaction, 1997, Amsterdam,
North-Holland.

Historical Perspective and Further Reading
934
[15]
Hon, H.W., et al., "Towards Large Vocabulary Mandarin Chinese Speech
Recognition," Int. Conf. on Acoustics, Signal and Speech Processing, 1994,
Adelaide, Australia pp. 545-548.
[16]
Hon, H.-W., Y. Ju, and K. Otani, "Japanese Large-Vocabulary Continuous Speech
Recognition System Based on Microsoft Whisper," The 5th Int. Conf. on Spoken
Language Processing, 1998, Sydney, Australia.
[17]
Horton, W., The Icon Book, 1994, New York, John Wiley.
[18]
Huang, X., et al., "MIPAD: A Next Generation PDA Prototype," Int. Conf. on
Spoken Language Processing, 2000, Beijing, China.
[19]
Karat, C., et al., "Patterns of Entry and Correction in Large Vocabulary Continuous
Speech Recognition Systems," CHI'99, 1999, Pittsburgh pp. 15-20.
[20]
Karat, J., "Int. Perspectives: Some Thoughts on Differences between North
American and European HCI," ACM SIGCHI Bulletin, 1991, 23(4), pp. 9-10.
[21]
Kumamoto, T. and A. Ito, "Structural Analysis of Spoken Japanese Language and
its Application to Communicative Intention Recognition," Proc. of the Fifth Int.
Conf. on Human-Computer Interaction -- Poster Sessions: Abridged Proc., 1993
pp. 284.
[22]
Lamel, L. and R.D. Mori, "Speech Recognition of European Languages" in Proc.
IEEE Automatic Speech Recognition Workshop 1995, Snowbird, UT, pp. 51-54.
[23]
Laurila, K. and P. Haavisto, "Name Dialing - How Useful Is It?," IEEE Int. Conf.
on Acoustics, Speech and Signal Processing, 2000, Istanbul pp. 3731-3734.
[24]
Lyu, R.Y., et al., "Golden Mandarin (III) - A User-Adaptive Prosodic Segment-
Based Mandarin Dictation Machine for Chinese Language with Very Large
Vocabulary," Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal
Processing, 1995, Detroit pp. 57-60.
[25]
Markowitz, J., Using Speech Recognition, 1996, Upper Saddle River, Prentice Hall.
[26]
Martin, G.L., "The Utility of Speech Input in User-Computer Interfaces," Int.
Journal of Man-Machine Studies, 1989, 30(4), pp. 355-375.
[27]
Möbius, B., "Analysis and Synthesis of German F0 Contours by Means of
Fujisaki's Model," Speech Communication, 1993, 13(53-61).
[28]
Morrison, D.L., et al., "Speech-Controlled Text-Editing: Effects of Input Modality
and of Command Structure," Int. Journal of Man-Machine Studies, 1984, 21(1), pp.
49-63.
[29]
Nielsen, J., Usability Engineering, 1993, Boston, MA, Academic Press.
[30]
Norman, D.A., Things That Make Us Smart: Defending Human Attributes in the
Age of the Machine, 1993, Reading, MA, Addison-Wesley.
[31]
O'Shaughnessy, D., "Specifying Accent Marks in French Text for Teletext and
Speech Synthesis," Int. Journal of Man-Machine Studies, 1989, 31(4), pp. 405-414.
[32]
Pierrehumbert, J., and M. Beckman, Japanese Tone Structure, 1988, Cambridge,
MA, MIT Press.
[33]
Poock, G.K., "Voice Recognition Boosts Command Terminal Throughput," Speech
Technology, 1982, 1, pp. 36-39.
[34]
Rao, P.V.S., "VOICE: An Integrated Speech Recognition Synthesis System for the
Hindi Language," Speech Communication, 1993, 13, pp. 197-205.

Historical Perspective and Further Reading
935
[35]
Royer, T., "Using Scenario-Based Designs to Review User Interface Changes and
Enhancements," Proc. of DIS'95 Symposium on Designing Interactive Systems:
Processes, Practices, Methods, & Techniques, 1995 pp. 237-246.
[36]
Rubin, J., Handbook of Usability Testing: How to Plan, Design, and Conduct
Effective Tests, 1994, New York, John Wiley.
[37]
Sagisaka, Y. and L.S. Lee, "Speech Recognition of Asian Languages" in Proc.
IEEE Automatic Speech Recognition Workshop 1995, Snowbird, UT, pp. 55-57.
[38]
Salvendy, G., Handbook of Human Factors and Ergonomics, 1997, New York,
John Wiley.
[39]
Schmandt, C., Voice Communication with Computers, 1994, New York, Van
Nostrand Reinhold.
[40]
Schmidt, M., et al., "Phonetic Transcription Standards for European Names
(ONOMASTICA)," Proc. of the European Conf. on Speech Communication and
Technology, 1993, Berlin pp. 279-283.
[41]
Shneiderman, B., Designing the User Interface: Strategies for Effective Human-
Computer Interaction, 1997, Reading, MA, Addison-Wesley.
[42]
Spencer, R.H., Computer Usability Testing and Evaluation, 1985, Englewood-
Cliffs, NJ, Prentice-Hall.
[43]
Weinschenk, S. and D. Barker, Designing Effective Speech Interfaces, 2000, New
York, John Wiley.
[44]
Wixon, D. and J. Ramey, Field Methods Casebook for Software Design, 1996, New
York, John Wiley.
[45]
Wood, L., User Interface Design: Bridging the Gap from User Requirements to
Design, 1997, CRC Press.
[46]
Yang, K.C., et al., "Statistics-Based Segment Pattern Lexicon - A New Direction
for Chinese Language Modeling," Int. Conf. on Acoustics, Speech and Signal
Processing, 1998 pp. 169-172.



























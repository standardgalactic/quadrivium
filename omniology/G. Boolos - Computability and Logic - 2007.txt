
This page intentionally left blank

Computability and Logic, Fifth Edition
Computability and Logic has become a classic because of its accessibility to students with-
out a mathematical background and because it covers not simply the staple topics of an
intermediate logic course, such as G¨odel’s incompleteness theorems, but also a large num-
ber of optional topics, from Turing’s theory of computability to Ramsey’s theorem. This ﬁfth
edition has been thoroughly revised by John P. Burgess. Including a selection of exercises,
adjusted for this edition, at the end of each chapter, it offers a new and simpler treatment
of the representability of recursive functions, a traditional stumbling block for students on
the way to the G¨odel incompleteness theorems. This new edition is also accompanied by a
Web site as well as an instructor’s manual.
“[This book] gives an excellent coverage of the fundamental theoretical results about logic
involving computability, undecidability, axiomatization, deﬁnability, incompleteness, and
so on.”
– American Math Monthly
“The writing style is excellent: Although many explanations are formal, they are perfectly
clear. Modern, elegant proofs help the reader understand the classic theorems and keep the
book to a reasonable length.”
– Computing Reviews
“A valuable asset to those who want to enhance their knowledge and strengthen their ideas
in the areas of artiﬁcial intelligence, philosophy, theory of computing, discrete structures,
and mathematical logic. It is also useful to teachers for improving their teaching style in
these subjects.”
– Computer Engineering


Computability and Logic
Fifth Edition
GEORGE S. BOOLOS
JOHN P. BURGESS
Princeton University
RICHARD C. JEFFREY

CAMBRIDGE UNIVERSITY PRESS
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, São Paulo
Cambridge University Press
The Edinburgh Building, Cambridge CB2 8RU, UK
First published in print format
ISBN-13    978-0-521-87752-7
ISBN-13    978-0-521-70146-4
ISBN-13
978-0-511-36668-0
© George S. Boolos, John P. Burgess, Richard C. Jeffrey 1974, 1980, 1990, 2002, 2007
2007
Information on this title: www.cambridge.org/9780521877527
This publication is in copyright. Subject to statutory exception and to the provision of 
relevant collective licensing agreements, no reproduction of any part may take place 
without the written permission of Cambridge University Press.
ISBN-10    0-511-36668-X
ISBN-10    0-521-87752-0
ISBN-10    0-521-70146-5
Cambridge University Press has no responsibility for the persistence or accuracy of urls 
for external or third-party internet websites referred to in this publication, and does not 
guarantee that any content on such websites is, or will remain, accurate or appropriate.
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
hardback
paperback
paperback
eBook (EBL)
eBook (EBL)
hardback

For
SALLY
and
AIGLI
and
EDITH


Contents
Preface to the Fifth Edition
page xi
COMPUTABILITY THEORY
1 Enumerability
3
1.1
Enumerability
3
1.2
Enumerable Sets
7
2 Diagonalization
16
3 Turing Computability
23
4 Uncomputability
35
4.1
The Halting Problem
35
4.2
The Productivity Function
40
5 Abacus Computability
45
5.1
Abacus Machines
45
5.2
Simulating Abacus Machines by Turing Machines
51
5.3
The Scope of Abacus Computability
57
6 Recursive Functions
63
6.1
Primitive Recursive Functions
63
6.2
Minimization
70
7 Recursive Sets and Relations
73
7.1
Recursive Relations
73
7.2
Semirecursive Relations
80
7.3
Further Examples
83
8 Equivalent Deﬁnitions of Computability
88
8.1
Coding Turing Computations
88
8.2
Universal Turing Machines
94
8.3
Recursively Enumerable Sets
96
vii

viii
CONTENTS
BASIC METALOGIC
9 A Pr´ecis of First-Order Logic: Syntax
101
9.1
First-Order Logic
101
9.2
Syntax
106
10 A Pr´ecis of First-Order Logic: Semantics
114
10.1 Semantics
114
10.2 Metalogical Notions
119
11 The Undecidability of First-Order Logic
126
11.1 Logic and Turing Machines
126
11.2 Logic and Primitive Recursive Functions
132
12 Models
137
12.1 The Size and Number of Models
137
12.2 Equivalence Relations
142
12.3 The L¨owenheim–Skolem and Compactness Theorems
146
13 The Existence of Models
153
13.1 Outline of the Proof
153
13.2 The First Stage of the Proof
156
13.3 The Second Stage of the Proof
157
13.4 The Third Stage of the Proof
160
13.5 Nonenumerable Languages
162
14 Proofs and Completeness
166
14.1 Sequent Calculus
166
14.2 Soundness and Completeness
174
14.3 Other Proof Procedures and Hilbert’s Thesis
179
15 Arithmetization
187
15.1 Arithmetization of Syntax
187
15.2 G¨odel Numbers
192
15.3 More G¨odel Numbers
196
16 Representability of Recursive Functions
199
16.1 Arithmetical Deﬁnability
199
16.2 Minimal Arithmetic and Representability
207
16.3 Mathematical Induction
212
16.4 Robinson Arithmetic
216
17 Indeﬁnability, Undecidability, Incompleteness
220
17.1 The Diagonal Lemma and the Limitative Theorems
220
17.2 Undecidable Sentences
224
17.3 Undecidable Sentences without the Diagonal Lemma
226
18 The Unprovability of Consistency
232

CONTENTS
ix
FURTHER TOPICS
19 Normal Forms
243
19.1 Disjunctive and Prenex Normal Forms
243
19.2 Skolem Normal Form
247
19.3 Herbrand’s Theorem
253
19.4 Eliminating Function Symbols and Identity
255
20 The Craig Interpolation Theorem
260
20.1 Craig’s Theorem and Its Proof
260
20.2 Robinson’s Joint Consistency Theorem
264
20.3 Beth’s Deﬁnability Theorem
265
21 Monadic and Dyadic Logic
270
21.1 Solvable and Unsolvable Decision Problems
270
21.2 Monadic Logic
273
21.3 Dyadic Logic
275
22 Second-Order Logic
279
23 Arithmetical Deﬁnability
286
23.1 Arithmetical Deﬁnability and Truth
286
23.2 Arithmetical Deﬁnability and Forcing
289
24 Decidability of Arithmetic without Multiplication
295
25 Nonstandard Models
302
25.1 Order in Nonstandard Models
302
25.2 Operations in Nonstandard Models
306
25.3 Nonstandard Models of Analysis
312
26 Ramsey’s Theorem
319
26.1 Ramsey’s Theorem: Finitary and Inﬁnitary
319
26.2 K¨onig’s Lemma
322
27 Modal Logic and Provability
327
27.1 Modal Logic
327
27.2 The Logic of Provability
334
27.3 The Fixed Point and Normal Form Theorems
337
Annotated Bibliography
341
Index
343


Preface to the Fifth Edition
The original authors of this work, the late George Boolos and Richard Jeffrey, stated in the
preface to the ﬁrst edition that the work was intended for students of philosophy, mathe-
matics, or other ﬁelds who desired a more advanced knowledge of logic than is supplied by
an introductory course or textbook on the subject, and added the following:
The aim has been to present the principal fundamental theoretical results about logic, and to
cover certain other meta-logical results whose proofs are not easily obtainable elsewhere. We
have tried to make the exposition as readable as was compatible with the presentation of complete
proofs, to use the most elegant proofs we knew of, to employ standard notation, and to reduce
hair (as it is technically known).
Such have remained the aims of all subsequent editions.
The “principal fundamental theoretical results about logic” are primarily the theorems of
G¨odel, the completeness theorem, and especially the incompleteness theorems, with their
attendant lemmas and corollaries. The “other meta-logical results” included have been of
two kinds. On the one hand, ﬁlling roughly the ﬁrst third of the book, there is an extended
exposition by Richard Jeffrey of the theory of Turing machines, a topic frequently alluded
to in the literature of philosophy, computer science, and cognitive studies but often omitted
in textbooks on the level of this one. On the other hand, there is a varied selection of
theorems on (in-)deﬁnability, (un-)decidability, (in-)completeness, and related topics, to
which George Boolos added a few more items with each successive edition, until by the
third, the last to which he directly contributed, it came to ﬁll about the last third of the book.
When I undertook a revised edition, my special aim was to increase the pedagogical
usefulness of the book by adding a selection of problems at the end of each chapter and by
making more chapters independent of one another, so as to increase the range of options
available to the instructor or reader as to what to cover and what to defer. Pursuit of the latter
aim involved substantial rewriting, especially in the middle third of the book. A number of
the new problems and one new section on undecidability were taken from Boolos’s Nach-
lass, while the rewriting of the pr´ecis of ﬁrst-order logic – summarizing the material typically
covered in a more leisurely way in an introductory text or course and introducing the more
abstract modes of reasoning that distinguish intermediate- from introductory-level logic –
was undertaken in consultation with Jeffrey. Otherwise, the changes have been my respon-
sibility alone.
The book runs now in outline as follows. The basic course in intermediate logic culmi-
nating in the ﬁrst incompleteness theorem is contained in Chapters 1, 2, 6, 7, 9, 10, 12, 15,
16, and 17, minus any sections of these chapters starred as optional. Necessary background
xi

xii
PREFACE TO THE FIFTH EDITION
on enumerable and nonenumerable sets is supplied in Chapters 1 and 2. All the material
on computability (recursion theory) that is strictly needed for the incompletness theorems
has now been collected in Chapters 6 and 7, which may, if desired, be postponed until after
the needed background material in logic. That material is presented in Chapters 9, 10, and
12 (for readers who have not had an introductory course in logic including a proof of the
completeness theorem, Chapters 13 and 14 will also be needed). The machinery needed
for the proof of the incompleteness theorems is contained in Chapter 15 on the arithme-
tization of syntax (though the instructor or reader willing to rely on Church’s thesis may
omit all but the ﬁrst section of this chapter) and in Chapter 16 on the representability of
recursive functions. The ﬁrst completeness theorem itself is proved in Chapter 17. (The
second incompleteness theorem is discussed in Chapter 18.)
A semester course should allow time to take up several supplementary topics in addition
to this core material. The topic given the fullest exposition is the theory of Turing machines
and their relation to recursive functions, which is treated in Chapters 3 through 5 and 8 (with
an application to logic in Chapter 11). This now includes an account of Turing’s theorem
on the existence of a universal Turing machine, one of the intellectual landmarks of the last
century. If this material is to be included, Chapters 3 through 8 would best be taken in that
order, either after Chapter 2 or after Chapter 12 (or 14).
Chapters 19 through 21 deal with topics in general logic, and any or all of them might
be taken up as early as immediately after Chapter 12 (or 14). Chapter 19 is presupposed by
Chapters 20 and 21, but the latter are independent of each other. Chapters 22 through 26, all
independent of one another, deal with topics related to formal arithmetic, and any of them
could most naturally be taken up after Chapter 17. Only Chapter 27 presupposes Chapter 18.
Users of the previous edition of this work will ﬁnd essentially all the material in it still here,
though not always in the same place, apart from some material in the former version of
Chapter 27 that has, since the last edition of this book, gone into The Logic of Provablity.
All these changes were made in the fourth edition. In the present ﬁfth edition, the main
change to the body of the text (apart from correction of errata) is a further revision and
simpliﬁcationofthetreatmentoftherepresentabilityofrecursivefunctions,traditionallyone
ofthegreatestdifﬁcultiesforstudents.Theversionnowtobefoundinsection16.2represents
the distillation of more than twenty years’ teaching experience trying to ﬁnd ever easier ways
over this hump. Section 16.4 on Robinson arithmetic has also been rewritten. In response
to a suggestion from Warren Goldfarb, an explicit discussion of the distinction between
two different kinds of appeal to Church’s thesis, avoidable and unavoidable, has been
inserted at the end of section 7.2. The avoidable appeals are those that consist of omitting
the veriﬁcation that certain obviously effectively computable functions are recursive; the
unavoidable appeals are those involved whenever a theorem about recursiveness is converted
into a conclusion about effective computability in the intuitive sense.
On the one hand, it should go without saying that in a textbook on a classical subject,
only a small number of the results presented will be original to the authors. On the other
hand, a textbook is perhaps not the best place to go into the minutiæ of the history of a ﬁeld.
Apart from a section of remarks at the end of Chapter 18, we have indicated the history of
the ﬁeld for the student or reader mainly by the names attached to various theorems. See
also the annotated bibliography at the end of the book.

PREFACE TO THE FIFTH EDITION
xiii
There remains the pleasant task of expressing gratitude to those (beyond the dedicatees)
to whom the authors have owed personal debts. By the third edition of this work the
original authors already cited Paul Benacerraf, Burton Dreben, Hartry Field, Clark Glymour,
Warren Goldfarb, Simon Kochen, Paul Kripke, David Lewis, Paul Mellema, Hilary Putnam,
W.V.Quine,T.M.Scanlon,JamesThomson,andPeterTovey,withspecialthankstoMichael
J. Pendlebury for drawing the “mop-up” diagram in what is now section 5.2.
In connection with the fourth edition, my thanks were due collectively to the students
who served as a trial audience for intermediate drafts, and especially to my very able
assistants in instruction, Mike Fara, Nick Smith, and Caspar Hare, with special thanks to
the last-named for the “scoring function” example in section 4.2. In connection with the
present ﬁfth edition, Curtis Brown, Mark Budolfson, John Corcoran, Sinan Dogramaci,
Hannes Eder, Warren Goldfarb, Hannes Hutzelmeyer, David Keyt, Brad Monton, Jacob
Rosen, Jada Strabbing, Dustin Tucker, Joel Velasco, Evan Williams, and Richard Zach are
to be thanked for errata to the fourth edition, as well as for other helpful suggestions.
Perhaps the most important change connected with this ﬁfth edition is one not visible
in the book itself: It now comes supported by an instructor’s manual. The manual contains
(besides any errata that may come to light) suggested hints to students for odd-numbered
problems and solutions to all problems. Resources are available to students and instructors
at www.cambridge.org/us/9780521877527.
January 2007
JOHN P. BURGESS


Computability Theory


1
Enumerability
Our ultimate goal will be to present some celebrated theorems about inherent limits on
what can be computed and on what can be proved. Before such results can be established,
we need to undertake an analysis of computability and an analysis of provability. Com-
putations involve positive integers 1, 2, 3, . . . in the ﬁrst instance, while proofs consist of
sequences of symbols from the usual alphabet A, B, C, . . . or some other. It will turn out
to be important for the analysis both of computability and of provability to understand
the relationship between positive integers and sequences of symbols, and background
on that relationship is provided in the present chapter. The main topic is a distinction
between two different kinds of inﬁnite sets, the enumerable and the nonenumerable. This
material is just a part of a larger theory of the inﬁnite developed in works on set theory:
the part most relevant to computation and proof. In section 1.1 we introduce the concept
of enumerability. In section 1.2 we illustrate it by examples of enumerable sets. In the
next chapter we give examples of nonenumerable sets.
1.1 Enumerability
An enumerable, or countable, set is one whose members can be enumerated: arranged
in a single list with a ﬁrst entry, a second entry, and so on, so that every member of
the set appears sooner or later on the list. Examples: the set P of positive integers is
enumerated by the list
1, 2, 3, 4, . . .
and the set N of natural numbers is enumerated by the list
0, 1, 2, 3, . . .
while the set P−of negative integers is enumerated by the list
−1, −2, −3, −4, . . . .
Note that the entries in these lists are not numbers but numerals, or names of
numbers. In general, in listing the members of a set you manipulate names, not the
things named. For instance, in enumerating the members of the United States Senate,
you don’t have the senators form a queue; rather, you arrange their names in a list,
perhaps alphabetically. (An arguable exception occurs in the case where the members
3

4
ENUMERABILITY
of the set being enumerated are themselves linguistic expressions. In this case we can
plausiblyspeakofarrangingthemembersthemselvesinalist.Butwemightalsospeak
of the entries in the list as names of themselves so as to be able to continue to insist
that in enumerating a set, it is names of members of the set that are arranged in a list.)
By courtesy, we regard as enumerable the empty set, ∅, which has no members.
(The empty set; there is only one. The terminology is a bit misleading: It suggests
comparison of empty sets with empty containers. But sets are more aptly compared
with contents, and it should be considered that all empty containers have the same,
null content.)
A list that enumerates a set may be ﬁnite or unending. An inﬁnite set that is
enumerable is said to be enumerably inﬁnite or denumerable. Let us get clear about
what things count as inﬁnite lists, and what things do not. The positive integers can be
arranged in a single inﬁnite list as indicated above, but the following is not acceptable
as a list of the positive integers:
1, 3, 5, 7, . . . , 2, 4, 6, . . .
Here, all the odd positive integers are listed, and then all the even ones. This will not
do. In an acceptable list, each item must appear sooner or later as the nth entry, for
some ﬁnite n. But in the unacceptable arrangement above, none of the even positive
integers are represented in this way. Rather, they appear (so to speak) as entries
number ∞+ 1, ∞+ 2, and so on.
To make this point perfectly clear we might deﬁne an enumeration of a set not as a
listing, but as an arrangement in which each member of the set is associated with one
of the positive integers 1, 2, 3, . . . . Actually, a list is such an arrangement. The thing
named by the ﬁrst entry in the list is associated with the positive integer 1, the thing
named by the second entry is associated with the positive integer 2, and in general,
the thing named by the nth entry is associated with the positive integer n.
In mathematical parlance, an inﬁnite list determines a function (call it f ) that takes
positiveintegersasargumentsandtakesmembersofthesetasvalues.[Shouldwehave
written: ‘call it “ f ”,’ rather than ‘call it f ’? The common practice in mathematical
writing is to use special symbols, including even italicized letters of the ordinary
alphabet when being used as special symbols, as names for themselves. In case the
special symbol happens also to be a name for something else, for instance, a function
(as in the present case), we have to rely on context to determine when the symbol is
being used one way and when the other. In practice this presents no difﬁculties.] The
value of the function f for the argument n is denoted f (n). This value is simply the
thing denoted by the nth entry in the list. Thus the list
2, 4, 6, 8, . . .
which enumerates the set E of even positive integers determines the function f for
which we have
f (1) = 2,
f (2) = 4,
f (3) = 6,
f (4) = 8,
f (5) = 10, . . . .
And conversely, the function f determines the list, except for notation. (The same list
would look like this, in Roman numerals: II, IV, VI, VIII, X, . . . , for instance.) Thus,

1.1. ENUMERABILITY
5
we might have deﬁned the function f ﬁrst, by saying that for any positive integer n,
the value of f is f (n) = 2n; and then we could have described the list by saying that
for each positive integer n, its nth entry is the decimal representation of the number
f (n), that is, of the number 2n.
Then we may speak of sets as being enumerated by functions, as well as by lists.
Instead of enumerating the odd positive integers by the list 1, 3, 5, 7, . . . , we may
enumerate them by the function that assigns to each positive integer n the value
2n −1. And instead of enumerating the set P of all positive integers by the list 1, 2,
3, 4, . . . , we may enumerate P by the function that assigns to each positive integer n
the value n itself. This is the identity function. If we call it id, we have id(n) = n for
each positive integer n.
If one function enumerates a nonempty set, so does some other; and so, in fact,
do inﬁnitely many others. Thus the set of positive integers is enumerated not only
by the function id, but also by the function (call it g) determined by the following
list:
2, 1, 4, 3, 6, 5, . . . .
This list is obtained from the list 1, 2, 3, 4, 5, 6, . . . by interchanging entries in pairs:
1 with 2, 3 with 4, 5 with 6, and so on. This list is a strange but perfectly acceptable
enumeration of the set P: every positive integer shows up in it, sooner or later. The
corresponding function, g, can be deﬁned as follows:
g(n) =
n + 1
if n is odd
n −1
if n is even.
Thisdeﬁnitionisnotasneatasthedeﬁnitions f (n) = 2n andid(n) = n ofthefunctions
f and id, but it does the job: It does indeed associate one and only one member of P
with each positive integer n. And the function g so deﬁned does indeed enumerate
P: For each member m of P there is a positive integer n for which we have g(n) = m.
In enumerating a set by listing its members, it is perfectly all right if a member
of the set shows up more than once on the list. The requirement is rather that each
member show up at least once. It does not matter if the list is redundant: All we
require is that it be complete. Indeed, a redundant list can always be thinned out to
get an irredundant list, since one could go through and erase the entries that repeat
earlier entries. It is also perfectly all right if a list has gaps in it, since one could
go through and close up the gaps. The requirement is that every element of the set
being enumerated be associated with some positive integer, not that every positive
integer have an element of the set associated with it. Thus ﬂawless enumerations of
the positive integers are given by the following repetitive list:
1, 1, 2, 2, 3, 3, 4, 4, . . .
and by the following gappy list:
1, −, 2, −, 3, −, 4, −, . . . .
The function corresponding to this last list (call it h) assigns values corresponding
to the ﬁrst, third, ﬁfth, . . . entries, but assigns no values corresponding to the gaps

6
ENUMERABILITY
(second, fourth, sixth, . . . entries). Thus we have h(1) = 1, but h(2) is nothing at all,
for the function h is undeﬁned for the argument 2; h(3) = 2, but h(4) is undeﬁned;
h(5) = 3, but h(6) is undeﬁned. And so on: h is a partial function of positive integers;
that is, it is deﬁned only for positive integer arguments, but not for all such arguments.
Explicitly, we might deﬁne the partial function h as follows:
h(n) = (n + 1)/2
if n is odd.
Or, to make it clear we haven’t simply forgotten to say what values h assigns to even
positive integers, we might put the deﬁnition as follows:
h(n) =
(n + 1)/2
if n is odd
undeﬁned
otherwise.
Now the partial function h is a strange but perfectly acceptable enumeration of the
set P of positive integers.
It would be perverse to choose h instead of the simple function id as an enumeration
of P; but other sets are most naturally enumerated by partial functions. Thus, the set
E of even integers is conveniently enumerated by the partial function (call it j) that
agrees with id for even arguments, and is undeﬁned for odd arguments:
j(n) =
n
if n is even
undeﬁned
otherwise.
The corresponding gappy list (in decimal notation) is
−, 2, −, 4, −, 6, −, 8, . . . .
Of course the function f considered earlier, deﬁned by f (n) = 2n for all positive
integers n, was an equally acceptable enumeration of E, corresponding to the gapless
list 2, 4, 6, 8, and so on.
Any set S of positive integers is enumerated quite simply by a partial function s,
which is deﬁned as follows:
s(n) =
n
if n is in the set S
undeﬁned
otherwise.
It will be seen in the next chapter that although every set of positive integers is
enumerable, there are sets of others sorts that are not enumerable. To say that a set
A is enumerable is to say that there is a function all of whose arguments are positive
integers and all of whose values are members of A, and that each member of A is a
value of this function: For each member a of A there is at least one positive integer
n to which the function assigns a as its value.
Notice that nothing in this deﬁnition requires A to be a set of positive integers
or of numbers of any sort. Instead, A might be a set of people; or a set of linguistic
expressions; or a set of sets, as when A is the set {P, E,∅}. Here A is a set with
three members, each of which is itself a set. One member of A is the inﬁnite set
P of all positive integers; another member of A is the inﬁnite set E of all even
positive integers; and the third is the empty set ∅. The set A is certainly enumerable,
for example, by the following ﬁnite list:P, E,∅. Each entry in this list names a

1.2. ENUMERABLE SETS
7
member of A, and every member of A is named sooner or later on this list. This
list determines a function (call it f ), which can be deﬁned by the three statements:
f (1) = P, f (2) = E, f (3) = ∅. To be precise, f is a partial function of positive
integers, being undeﬁned for arguments greater than 3.
In conclusion, let us straighten out our terminology. A function is an assignment
of values to arguments. The set of all those arguments to which the function assigns
values is called the domain of the function. The set of all those values that the function
assigns to its arguments is called the range of the function. In the case of functions
whose arguments are positive integers, we distinguish between total functions and
partial functions. A total function of positive integers is one whose domain is the
whole set P of positive integers. A partial function of positive integers is one whose
domain is something less than the whole set P. From now on, when we speak simply
of a function of positive integers, we should be understood as leaving it open whether
the function is total or partial. (This is a departure from the usual terminology, in
which function of positive integers always means total function.) A set is enumerable
if and only if it is the range of some function of positive integers. We said earlier
we wanted to count the empty set ∅as enumerable. We therefore have to count as
a partial function the empty function e of positive integers that is undeﬁned for all
arguments. Its domain and its range are both ∅.
It will also be important to consider functions with two, three, or more positive
integers as arguments, notably the addition function sum(m, n) = m + n and the
multiplication function prod(m, n) = m · n. It is often convenient to think of a two-
argument or two-place function on positive integers as a one-argument function on
ordered pairs of positive integers, and similarly for many-argument functions. A few
more notions pertaining to functions are deﬁned in the ﬁrst few problems at the end
of this chapter. In general, the problems at the end should be read as part of each
chapter, even if not all are going to be worked.
1.2 Enumerable Sets
We next illustrate the deﬁnition of the preceding section by some important examples.
The following sets are enumerable.
1.1 Example (The set of integers). The simplest list is 0, 1, −1, 2, −2, 3, −3, . . . . Then if
the corresponding function is called f , we have f (1) = 0, f (2) = 1, f (3) = −1, f (4) =
2, f (5) = −2, and so on.
1.2 Example (The set of ordered pairs of positive integers). The enumeration of pairs
will be important enough in our later work that it may be well to indicate two different
ways of accomplishing it. The ﬁrst way is this. As a preliminary to enumerating them,
we organize them into a rectangular array. We then traverse the array in Cantor’s zig-zag
manner indicated in Figure 1.1. This gives us the list
(1, 1), (1, 2), (2, 1), (1, 3), (2, 2), (3, 1), (1, 4), (2, 3), (3, 2), (4, 1), . . . .
If we call the function involved here G, then we have G(1) = (1, 1), G(2) = (1, 2), G(3) =
(2, 1), and so on. The pattern is: First comes the pair the sum of whose entries is 2, then

8
ENUMERABILITY
(1, 1) —(1, 2)     (1, 3)     (1, 4)     (1, 5)     …
(2, 1)     (2, 2)     (2, 3)     (2, 4)     (2, 5)     …
(3, 1)     (3, 2)     (3, 3)     (3, 4)     (3, 5)     …
(4, 1)     (4, 2)     (4, 3)     (4, 4)     (4, 5)     …
(5, 1)     (5, 2)     (5, 3)     (5, 4)     (5, 5)     …
                              
Figure 1-1. Enumerating pairs of positive integers.
come the pairs the sum of whose entries is 3, then come the pairs the sum of whose entries
is 4, and so on. Within each block of pairs whose entries have the same sum, pairs appear
in order of increasing ﬁrst entry.
As for the second way, we begin with the thought that while an ordinary hotel may have
to turn away a prospective guest because all rooms are full, a hotel with an enumerable
inﬁnity of rooms would always have room for one more: The new guest could be placed
in room 1, and every other guest asked to move over one room. But actually, a little more
thought shows that with foresight the hotelier can be prepared to accommodate a busload
with an enumerable inﬁnity of new guests each day, without inconveniencing any old guests
by making them change rooms. Those who arrive on the ﬁrst day are placed in every other
room, those who arrive on the second day are placed in every other room among those
remaining vacant, and so on. To apply this thought to enumerating pairs, let us use up every
other place in listing the pairs (1, n), every other place then remaining in listing the pairs
(2, n), every other place then remaining in listing the pairs (3, n), and so on. The result will
look like this:
(1, 1), (2, 1), (1, 2), (3, 1), (1, 3), (2, 2), (1, 4), (4, 1), (1, 5), (2, 3), . . . .
If we call the function involved here g, then g(1) = (1, 1), g(2) = (2, 1), g(3) = (1, 2), and
so on.
Given a function f enumerating the pairs of positive integers, such as G or g
above, an a such that f (a) = (m, n) may be called a code number for the pair (m, n).
Applying the function f may be called decoding, while going the opposite way, from
the pair to a code for it, may be called encoding. It is actually possible to derive
mathematical formulas for the encoding functions J and j that go with the decoding
functions G and g above. (Possible, but not necessary: What we have said so far more
than sufﬁces as a proof that the set of pairs is enumerable.)
Let us take ﬁrst J. We want J(m, n) to be the number p such that G(p) = (m, n),
which is to say the place p where the pair (m, n) comes in the enumeration corre-
sponding to G. Before we arrive at the pair (m, n), we will have to pass the pair whose
entries sum to 2, the two pairs whose entries sum to 3, the three pairs whose entries
sum to 4, and so on, up through the m + n −2 pairs whose entries sum to m + n −1.

1.2. ENUMERABLE SETS
9
The pair (m, n) will appear in the mth place after all of these pairs. So the position
of the pair (m, n) will be given by
[1 + 2 + · · · + (m + n −2)] + m.
At this point we recall the formula for the sum of the ﬁrst k positive integers:
1 + 2 + · · · + k = k(k + 1)/2.
(Never mind, for the moment, where this formula comes from. Its derivation will be
recalled in a later chapter.) So the position of the pair (m, n) will be given by
(m + n −2)(m + n −1)/2 + m.
This simpliﬁes to
J(m, n) = (m2 + 2mn + n2 −m −3n + 2)/2.
For instance, the pair (3, 2) should come in the place
(32 + 2 · 3 · 2 + 22 −3 −3 · 2 + 2)/2 = (9 + 12 + 4 −3 −6 + 2)/2 = 18/2 = 9
as indeed it can be seen (looking back at the enumeration as displayed above) that it
does: G(9) = (3, 2).
Turning now to j, we ﬁnd matters a bit simpler. The pairs with ﬁrst entry 1 will
appear in the places whose numbers are odd, with (1, n) in place 2n −1. The pairs
with ﬁrst entry 2 will appear in the places whose numbers are twice an odd number,
with (2, n) in place 2(2n −1). The pairs with ﬁrst entry 3 will appear in the places
whose numbers are four times an odd number, with (3, n) in place 4(2n −1). In
general, in terms of the powers of two (20 = 1, 21 = 2, 22 = 4, and so on), (m, n)
will appear in place j(m, n) = 2m−1(2n −1). Thus (3, 2) should come in the place
23−1(2 · 2 −1) = 22(4 −1) = 4 · 3 = 12, as indeed it does: g(12) = (3, 2).
The series of examples to follow shows how more and more complicated objects
can be coded by positive integers. Readers may wish to try to ﬁnd proofs of their own
before reading ours; and for this reason we give the statements of all the examples
ﬁrst, and collect all the proofs afterwards. As we saw already with Example 1.2,
several equally good codings may be possible.
1.3 Example. The set of positive rational numbers
1.4 Example. The set of rational numbers
1.5 Example. The set of ordered triples of positive integers
1.6 Example. The set of ordered k-tuples of positive integers, for any ﬁxed k
1.7 Example. The set of ﬁnite sequences of positive integers less than 10
1.8 Example. The set of ﬁnite sequences of positive integers less than b, for any ﬁxed b
1.9 Example. The set of ﬁnite sequences of positive integers
1.10 Example. The set of ﬁnite sets of positive integers

10
ENUMERABILITY
1.11 Example. Any subset of an enumerable set
1.12 Example. The union of any two enumerable sets
1.13 Example. The set of ﬁnite strings from a ﬁnite or enumerable alphabet of symbols
Proofs
Example 1.3. A positive rational number is a number that can be expressed as a
ratio of positive integers, that is, in the form m/n where m and n are positive integers.
Therefore we can get an enumeration of all positive rational numbers by starting with
our enumeration of all pairs of positive integers and replacing the pair (m, n) by the
rational number m/n. This gives us the list
1/1, 1/2, 2/1, 1/3, 2/2, 3/1, 1/4, 2/3, 3/2, 4/1, 1/5, 2/4, 3/3, 4/2, 5/1, 1/6, . . .
or, simpliﬁed,
1, 1/2, 2, 1/3, 1, 3, 1/4, 2/3, 3/2, 4, 1/5, 1/2, 1, 2, 5/1, 1/6, . . . .
Every positive rational number in fact appears inﬁnitely often, since for instance
1/1 = 2/2 = 3/3 = · · · and 1/2 = 2/4 = · · · and 2/1 = 4/2 = · · · and similarly for
every other rational number. But that is all right: our deﬁnition of enumerability
permits repetitions.
Example 1.4. We combine the ideas of Examples 1.1 and 1.3. You know from
Example 1.3 how to arrange the positive rationals in a single inﬁnite list. Write a zero
in front of this list, and then write the positive rationals, backwards and with minus
signs in front of them, in front of that. You now have
. . . , −1/3, −2, −1/2, −1, 0, 1, 1/2, 2, 1/3, . . .
Finally, use the method of Example 1.1 to turn this into a proper list:
0, 1, −1, 1/2, −1/2, 2, −2, 1/3, −1/3, . . .
Example 1.5. In Example 1.2 we have given two ways of listing all pairs of positive
integers. For deﬁniteness, let us work here with the ﬁrst of these:
(1, 1), (1, 2), (2, 1), (1, 3), (2, 2), (3, 1), . . . .
Now go through this list, and in each pair replace the second entry or component n
with the pair that appears in the nth place on this very list. In other words, replace
each 1 that appears in the second place of a pair by (1, 1), each 2 by (1, 2), and so on.
This gives the list
(1, (1, 1)), (1, (1, 2)), (2, (1, 1)), (1, (2, 1)), (2, (1, 2)), (3, (1, 1)), . . .
and that gives a list of triples
(1, 1, 1), (1, 1, 2), (2, 1, 1), (1, 2, 1), (2, 1, 2), (3, 1, 1), . . . .
In terms of functions, this enumeration may be described as follows. The original
enumeration of pairs corresponds to a function associating to each positive integer n

1.2. ENUMERABLE SETS
11
a pair G(n) = (K(n), L(n)) of positive integers. The enumeration of triples we have
just deﬁned corresponds to assigning to each positive integer n instead the triple
(K(n), K(L(n)), L(L(n))).
We do not miss any triples (p, q,r) in this way, because there will always be an
m = J(q,r) such that (K(m), L(m)) = (q,r), and then there will be an n = J(p, m)
such that (K(n), L(n)) = (p, m), and the triple associated with this n will be precisely
(p, q,r).
Example 1.6. The method by which we have just obtained an enumeration of
triples from an enumeration of pairs will give us an enumeration of quadruples from
an enumeration of triples. Go back to the original enumeration pairs, and replace
each second entry n by the triple that appears in the nth place in the enumeration of
triples, to get a quadruple. The ﬁrst few quadruples on the list will be
(1, 1, 1, 1), (1, 1, 1, 2), (2, 1, 1, 1), (1, 2, 1, 1), (2, 1, 1, 2), . . . .
Obviously we can go on from here to quintuples, sextuples, or k-tuples for any ﬁxed
k.
Example 1.7. A ﬁnite sequence whose entries are all positive integers less than 10,
such as (1, 2, 3), can be read as an ordinary decimal or base-10 numeral 123. The
number this numeral denotes, one hundred twenty-three, could then be taken as a
code number for the given sequence. Actually, for later purposes it proves convenient
to modify this procedure slightly and write the sequence in reverse before reading it
as a numeral. Thus (1, 2, 3) would be coded by 321, and 123 would code (3, 2, 1). In
general, a sequence
s = (a0, a1, a2, . . . , ak)
would be coded by
a0 + 10a1 + 100a2 + · · · + 10kak
which is the number that the decimal numeral ak · · · a2a1a0 represents. Also, it will
be convenient henceforth to call the initial entry of a ﬁnite sequence the 0th entry, the
next entry the 1st, and so on. To decode and obtain the ith entry of the sequence coded
by n, we take the quotient on dividing by 10i, and then the remainder on dividing by
10. For instance, to ﬁnd the 5th entry of the sequence coded by 123 456 789, we divide
by 105 to obtain the quotient 1234, and then divide by 10 to obtain the remainder 4.
Example 1.8. We use a decimal, or base-10, system ultimately because human
beings typically have 10 ﬁngers, and counting began with counting on ﬁngers. A
similar base-b system is possible for any b > 1. For a binary, or base-2, system only
the ciphers 0 and 1 would be used, with ak . . . a2a1a0 representing
a0 + 2a1 + 4a2 + · · · + 2kak.
So, for instance, 1001 would represent 1 + 23 = 1 + 8 = 9. For a duodecimal, or
base-12, system, two additional ciphers, perhaps * and # as on a telephone, would be
needed for ten and eleven. Then, for instance, 1*# would represent 11 + 12 · 10 +
144 · 1 = 275. If we applied the idea of the previous problem using base 12 instead

12
ENUMERABILITY
of base 10, we could code ﬁnite sequences of positive integers less than 12, and not
just ﬁnite sequences of positive integers less than 10. More generally, we can code a
ﬁnite sequence
s = (a0, a1, a2, . . . , ak)
of positive integers less than b by
a0 + ba1 + b2a2 + · · · + bkak.
To obtain the ith entry of the sequence coded by n, we take the quotient on dividing
by bi and then the remainder on dividing by b. For example, when working with
base 12, to obtain the 5th entry of the sequence coded by 123 456 789, we divide
123 456 789 by 125 to get the quotient 496. Now divide by 12 to get remainder 4. In
general, working with base b, the ith entry—counting the initial one as the 0th—of
the sequence coded by (b, n) will be
entry(i, n) = rem(quo(n, bi), b)
where quo(x, y) and rem(x, y) are the quotient and remainder on dividing x by y.
Example 1.9. Coding ﬁnite sequences will be important enough in our later work
that it will be appropriate to consider several different ways of accomplishing this
task. Example 1.6 showed that we can code sequences whose entries may be of
any size but that are of ﬁxed length. What we now want is an enumeration of all
ﬁnite sequences—pairs, triples, quadruples, and so on—in a single list; and for good
measure, let us include the 1-tuples or 1-term sequences (1), (2), (3), . . . as well. A
ﬁrst method, based on Example 1.6, is as follows. Let G1(n) be the 1-term sequence
(n). Let G2 = G, the function enumerating all 2-tuples or pairs from Example 1.2.
Let G3 be the function enumerating all triples as in Example 1.5. Let G4, G5, . . . ,
be the enumerations of triples, quadruples, and so on, from Example 1.6. We can get
a coding of all ﬁnite sequences by pairs of positive integers by letting any sequence
s of length k be coded by the pair (k, a) where Gk(a) = s. Since pairs of positive
integers can be coded by single numbers, we indirectly get a coding of sequences of
numbers. Another way to describe what is going on here is as follows. We go back
to our original listing of pairs, and replace the pair (k, a) by the ath item on the list
of k-tuples. Thus (1, 1) would be replaced by the ﬁrst item (1) on the list of 1-tuples
(1), (2), (3), . . . ; while (1, 2) would be replaced by the second item (2) on the same
list; whereas (2, 1) would be replaced by the ﬁrst item (1, 1) on the list of all 2-tuples
or pairs; and so on. This gives us the list
(1), (2), (1, 1), (3), (1, 2), (1, 1, 1), (4), (2, 1), (1, 1, 2), (1, 1, 1, 1), . . . .
(If we wish to include also the 0-tuple or empty sequence ( ), which we may take to
be simply the empty set ∅, we can stick it in at the head of the list, in what we may
think of as the 0th place.)
Example 1.8 showed that we can code sequences of any length whose entries
are less than some ﬁxed bound, but what we now want to do is show how to code
sequences of any length whose entries may be of any size. A second method, based

1.2. ENUMERABLE SETS
13
on Example 1.8, is to begin by coding sequences by pairs of positive integers. We
take a sequence
s = (a0, a1, a2, . . . , ak)
to be coded by any pair (b, n) such that all ai are less than b, and n codes s in the
sense that
n = a0 + b · a1 + b2a2 + · · · + bkak.
Thus (10, 275) would code (5, 7, 2), since 275 = 5 + 7 · 10 + 2 · 102, while (12, 275)
would code (11, 10, 1), since 275 = 11 + 10 · 12 + 1 · 122. Each sequence would
have many codes, since for instance (10, 234) and (12, 328) would equally code (4,
3, 2), because 4 + 3 · 10 + 2 · 102 = 234 and 4 + 3 · 12 + 2 · 122 = 328. As with the
previous method, since pairs of positive integers can be coded by single numbers, we
indirectly get a coding of sequences of numbers.
A third, and totally different, approach is possible, based on the fact that every
integer greater than 1 can be written in one and only one way as a product of powers
of larger and larger primes, a representation called its prime decomposition. This fact
enables us to code a sequence s = (i, j, k, m, n, . . . ) by the number 2i3 j5k7m11n . . .
. Thus the code number for the sequence (3, 1, 2) is 233152 = 8 · 3 · 25 = 600.
Example 1.10. It is easy to get an enumeration of ﬁnite sets from an enumeration
of ﬁnite sequences. Using the ﬁrst method in Example 1.9, for instance, we get the
following enumeration of sets:
{1}, {2}, {1, 1}, {3}, {1, 2}, {1, 1, 1}, {4}, {2, 1}, {1, 1, 2}, {1, 1, 1, 1}, . . . .
The set {1, 1} whose only elements are 1 and 1 is just the set {1} whose only element
is 1, and similarly in other cases, so this list can be simpliﬁed to look like this:
{1}, {2}, {1}, {3}, {1, 2}, {1}, {4}, {1, 2}, {1, 2}, {1}, {5}, . . . .
The repetitions do not matter.
Example 1.11. Given any enumerable set A and a listing of the elements of A:
a1, a2, a3, . . .
we easily obtain a gappy listing of the elements of any subset B of A simply by
erasing any entry in the list that does not belong to B, leaving a gap.
Example 1.12. Let A and B be enumerable sets, and consider listings of their
elements:
a1, a2, a3, . . .
b1, b2, b3, . . . .
Imitating the shufﬂing idea of Example 1.1, we obtain the following listing of the
elements of the union A ∪B (the set whose elements are all and only those items that
are elements either of A or of B or of both):
a1, b1, a2, b2, a3, b3, . . . .

14
ENUMERABILITY
If the intersection A ∩B (the set whose elements of both A and B) is not empty, then
there will be redundancies on this list: If am = bn, then that element will appear both
at place 2m −1 and at place 2n, but this does not matter.
Example 1.13. Given an ‘alphabet’ of any ﬁnite number, or even an enumerable
inﬁnity, of symbols S1, S2, S3, . . . we can take as a code number for any ﬁnite string
Sa0 Sa1 Sa2 · · · Sak
the code number for the ﬁnite sequence of positive integers
(a1, a2, a3, . . ., ak)
under any of the methods of coding considered in Example 1.9. (We are usually going
to use the third method.) For instance, with the ordinary alphabet of 26 symbols letters
S1 = ‘A’, S2 = ‘B’, and so on, the string or word ‘CAB’ would be coded by the code
for (3, 1, 2), which (on the third method of Example 1.9) would be 23 · 3 · 52 = 600.
Problems
1.1 A (total or partial) function f from a set A to a set B is an assignment for (some
or all) elements a of A of an associated element f (a) of B. If f (a) is deﬁned for
every element a of A, then the function f is called total. If every element b of B
is assigned to some element a of A, then the function f is said to be onto. If no
element b of B is assigned to more than one element a of A, then the function
f is said to be one-to-one. The inverse function f −1 from B to A is deﬁned by
letting f −1(b) be the one and only a such that f (a) = b, if any such a exists;
f −1(b) is undeﬁned if there is no a with f (a) = b or more than one such a. Show
that if f is a one-to-one function and f −1its inverse function, then f −1 is total
if and only if f is onto, and conversely, f −1 is onto if and only if f is total.
1.2 Let f be a function from a set A to a set B, and g a function from the set B to a
set C. The composite function h = gf from A to C is deﬁned by h(a) = g( f (a)).
Show that:
(a) If f and g are both total, then so is gf.
(b) If f and g are both onto, then so is gf.
(c) If f and g are both one-to-one, then so is gf.
1.3 A correspondence between sets A and B is a one-to-one total function from A
onto B. Two sets A and B are said to be equinumerous if and only if there is a
correspondence between A and B. Show that equinumerosity has the following
properties:
(a) Any set A is equinumerous with itself.
(b) If A is equinumerous with B, then B is equinumerous with A.
(c) If A is equinumerous with B and B is equinumerous with C, then A is
equinumerous with C.
1.4 A set A has n elements, where n is a positive integer, if it is equinumerous
with the set of positive integers up to n, so that its elements can be listed as
a1, a2, . . . , an. A nonempty set A is ﬁnite if it has n elements for some positive
integer n. Show that any enumerable set is either ﬁnite or equinumerous with

PROBLEMS
15
the set of all positive integers. (In other words, given an enumeration, which is
to say a function from the set of positive integers onto a set A, show that if A
is not ﬁnite, then there is a correspondence, which is to say a one-to-one, total
function, from the set of positive integers onto A.)
1.5 Show that the following sets are equinumerous:
(a) The set of rational numbers with denominator a power of two (when written
in lowest terms), that is, the set of rational numbers ±m/n where n = 1 or 2
or 4 or 8 or some higher power of 2.
(b) The set of those sets of positive integers that are either ﬁnite or coﬁnite,
where a set S of positive integers is coﬁnite if the set of all positive integers
n that are not elements of S is ﬁnite.
1.6 Show that the set of all ﬁnite subsets of an enumerable set is enumerable.
1.7 Let A = {A1, A2, A3, . . .} be an enumerable family of sets, and suppose that each
Ai for i = 1, 2, 3, and so on, is enumerable. Let ∪A be the union of the family
A, that is, the set whose elements are precisely the elements of the elements of
A. Is ∪A enumerable?

2
Diagonalization
In the preceding chapter we introduced the distinction between enumerable and nonenu-
merable sets, and gave many examples of enumerable sets. In this short chapter we give
examples of nonenumerable sets. We ﬁrst prove the existence of such sets, and then look
a little more closely at the method, called diagonalization, used in this proof.
Not all sets are enumerable: some are too big. For example, consider the set of all sets
of positive integers. This set (call it P*) contains, as a member, each ﬁnite and each
inﬁnite set of positive integers: the empty set ∅, the set P of all positive integers, and
every set between these two extremes. Then we have the following celebrated result.
2.1 Theorem (Cantor’s Theorem). The set of all sets of positive integers is not enu-
merable.
Proof: We give a method that can be applied to any list L of sets of positive integers
in order to discover a set (L) of positive integers which is not named in the list. If
you then try to repair the defect by adding (L) to the list as a new ﬁrst member, the
same method, applied to the augmented list L* will yield a different set (L*) that
is likewise not on the augmented list.
The method is this. Confronted with any inﬁnite list L
S1, S2, S3. . . .
of sets of positive integers, we deﬁne a set (L) as follows:
For each positive integer n, n is in (L) if and only if n is not in Sn.
(∗)
It should be clear that this genuinely deﬁnes a set (L); for, given any positive inte-
ger n, we can tell whether n is in (L) if we can tell whether n is in the nth set in the
list L. Thus, if S3 happens to be the set E of even positive integers, the number 3 is
not in S3 and therefore it is in (L). As the notation (L) indicates, the composition
of the set (L) depends on the composition of the list L, so that different lists L may
yield different sets (L).
To show that the set (L) that this method yields is never in the given list L,
we argue by reductio ad absurdum: we suppose that (L) does appear somewhere
in list L, say as entry number m, and deduce a contradiction, thus showing that the
16

DIAGONALIZATION
17
supposition must be false. Here we go. Supposition: For some positive integer m,
Sm = (L).
[Thus, if 127 is such an m, we are supposing that (L) and S127 are the same set
under different names: we are supposing that a positive integer belongs to (L) if
and only if it belongs to the 127th set in list L.] To deduce a contradiction from this
assumption we apply deﬁnition (*) to the particular positive integer m: with n = m,
(*) tells us that
m is in (L) if and only if m is not in Sm.
Now a contradiction follows from our supposition: if Sm and (L) are one and the
same set we have
m is in (L) if and only if m is in Sm.
Since this is a ﬂat self-contradiction, our supposition must be false. For no positive
integer m do we have Sm = (L). In other words, the set (L) is named nowhere in
list L.
So the method works. Applied to any list of sets of positive integers it yields a
set of positive integers which was not in the list. Then no list enumerates all sets of
positive integers: the set P* of all such sets is not enumerable. This completes the
proof.
Note that results to which we might wish to refer back later are given reference
numbers 1.1, 1.2, . . . consecutively through the chapter, to make them easy to locate.
Different words, however, are used for different kinds of results. The most important
general results are digniﬁed with the title of ‘theorem’. Lesser results are called
‘lemmas’ if they are steps on the way to a theorem, ‘corollaries’ if they follow
directly upon some theorem, and ‘propositions’ if they are free-standing. In contrast
to all these, ‘examples’ are particular rather than general. The most celebrated of the
theorems have more or less traditional names, given in parentheses. The fact that 2.1
has been labelled ‘Cantor’s theorem’ is an indication that it is a famous result. The
reason is not—we hope the reader will agree!—that its proof is especially difﬁcult,
but that the method of the proof (diagonalization) was an important innovation. In
fact, it is so important that it will be well to look at the proof again from a slightly
different point of view, which allows the entries in the list L to be more readily
visualized.
Accordingly, we think of the sets S1, S2, . . . as represented by functions s1,
s2, . . . of positive integers that take the numbers 0 and 1 as values. The relationship
between the set Sn and the corresponding function sn is simply this: for each positive
integer p we have
sn(p) =
1
if p is in Sn
0
if p is not in Sn.
Then the list can be visualized as an inﬁnite rectangular array of zeros and ones, in
which the nth row represents the function sn and thus represents the set Sn. That is,

18
DIAGONALIZATION
1
2
3
4
s1
s1(1)
s1(2)
s1(3)
s1(4)
s2
s2(1)
s2(2)
s2(3)
s2(4)
s3
s3(1)
s3(2)
s3(3)
s3(4)
s4
s4(1)
s4(2)
s4(3)
s4(4)
Figure 2-1. A list as a rectangular array.
the nth row
sn(1)sn(2)sn(3)sn(4) . . .
is a sequence of zeros and ones in which the pth entry, sn(p), is 1 or 0 according as
the number p is or is not in the set Sn. This array is shown in Figure 2-1.
The entries in the diagonal of the array (upper left to lower right) form a sequence
of zeros and ones:
s1(1) s2(2) s3(3) s4(4) . . .
This sequence of zeros and ones (the diagonal sequence) determines a set of positive
integers (the diagonal set). The diagonal set may well be among those listed in L. In
other words, there may well be a positive integer d such that the set Sd is none other
than our diagonal set. The sequence of zeros and ones in the dth row of Figure 2-1
would then agree with the diagonal sequence entry by entry:
sd(1) = s1(1),
sd(2) = s2(2),
sd(3) = s3(3), . . . .
That is as may be: the diagonal set may or may not appear in the list L, depending
on the detailed makeup of the list. What we want is a set we can rely upon not to appear
in L, no matter how L is composed. Such a set lies near to hand: it is the antidiagonal
set, which consists of the positive integers not in the diagonal set. The corresponding
antidiagonal sequence is obtained by changing zeros to ones and ones to zeros in the
diagonal sequence. We may think of this transformation as a matter of subtracting
each member of the diagonal sequence from 1: we write the antidiagonal sequence as
1 −s1(1), 1 −s2(2), 1 −s3(3), 1 −s4(4), . . . .
This sequence can be relied upon not to appear as a row in Figure 2-1, for if it did
appear—say, as the mth row—we should have
sm(1) = 1 −s1(1),
sm(2) = 1 −s2(2), . . . ,
sm(m) = 1 −sm(m), . . . .
But the mth of these equations cannot hold. [Proof: sm(m) must be zero or one. If zero,
the mth equation says that 0 = 1. If one, the mth equation says that 1 = 0.] Then the
antidiagonal sequence differs from every row of our array, and so the antidiagonal set
differs from every set in our list L. This is no news, for the antidiagonal set is simply
the set (L). We have merely repeated with a diagram—Figure 2-1—our proof that
(L) appears nowhere in the list L.
Of course, it is rather strange to say that the members of an inﬁnite set ‘can be
arranged’ in a single list. By whom? Certainly not by any human being, for nobody

DIAGONALIZATION
19
has that much time or paper; and similar restrictions apply to machines. In fact, to
call a set enumerable is simply to say that it is the range of some total or partial
function of positive integers. Thus, the set E of even positive integers is enumerable
because there are functions of positive integers that have E as their range. (We had
two examples of such functions earlier.) Any such function can then be thought of as
a program that a superhuman enumerator can follow in order to arrange the members
of the set in a single list. More explicitly, the program (the set of instructions) is:
‘Start counting from 1, and never stop. As you reach each number n, write a name of
f (n) in your list. [Where f (n) is undeﬁned, leave the nth position blank.]’ But there
is no need to refer to the list, or to a superhuman enumerator: anything we need to say
about enumerability can be said in terms of the functions themselves; for example, to
say that the set P* is not enumerable is simply to deny the existence of any function
of positive integers which has P* as its range.
Vivid talk of lists and superhuman enumerators may still aid the imagination, but
in such terms the theory of enumerability and diagonalization appears as a chapter
in mathematical theology. To avoid treading on any living toes we might put the
whole thing in a classical Greek setting: Cantor proved that there are sets which even
Zeus cannot enumerate, no matter how fast he works, or how long (even, inﬁnitely
long).
If a set is enumerable, Zeus can enumerate it in one second by writing out an
inﬁnite list faster and faster. He spends 1/2 second writing the ﬁrst entry in the list;
1/4 second writing the second entry; 1/8 second writing the third; and in general, he
writes each entry in half the time he spent on its predecessor. At no point during the
one-second interval has he written out the whole list, but when one second has passed,
the list is complete. On a time scale in which the marked divisions are sixteenths of
a second, the process can be represented as in Figure 2-2.
0
1/16
2/16
3/16
4/16
5/16
6/16
7/16
8/16
9/16
10/16
11/16
12/16
13/16
14/16
15/16
1
Zeus makes 1st entry
2nd entry
3rd entry
&c.
Figure 2-2. Completing an inﬁnite process in ﬁnite time.
To speak of writing out an inﬁnite list (for example, of all the positive integers, in
decimal notation) is to speak of such an enumerator either working faster and faster
as above, or taking all of inﬁnite time to complete the list (making one entry per
second, perhaps). Indeed, Zeus could write out an inﬁnite sequence of inﬁnite lists
if he chose to, taking only one second to complete the job. He could simply allocate
the ﬁrst half second to the business of writing out the ﬁrst inﬁnite list (1/4 second for
the ﬁrst entry, 1/8 second for the next, and so on); he could then write out the whole
second list in the following quarter second (1/8 for the ﬁrst entry, 1/16 second for the
next, and so on); and in general, he could write out each subsequent list in just half
the time he spent on its predecessor, so that after one second had passed he would
have written out every entry in every list, in order. But the result does not count as a

20
DIAGONALIZATION
single inﬁnite list, in our sense of the term. In our sort of list, each entry must come
some ﬁnite number of places after the ﬁrst.
As we use the term ‘list’, Zeus has not produced a list by writing inﬁnitely many
inﬁnite lists one after another. But he could perfectly well produce a genuine list
which exhausts the entries in all the lists, by using some such device as we used
in the preceeding chapter to enumerate the positive rational numbers. Nevertheless,
Cantor’s diagonal argument shows that neither this nor any more ingenious device
is available, even to a god, for arranging all the sets of positive integers into a sin-
gle inﬁnite list. Such a list would be as much an impossibility as a round square:
the impossibility of enumerating all the sets of positive integers is as absolute as the
impossibility of drawing a round square, even for Zeus.
Once we have one example of a nonenumerable set, we get others.
2.2 Corollary. The set of real numbers is not enumerable.
Proof: If ξ is a real number and 0 < ξ < 1, then ξ has a decimal expansion
.x1x2x3. . . where each xi is one of the cyphers 0–9. Some numbers have two decimal
expansions, since for instance .2999. . . = .3000. . . ; so if there is a choice, choose
the one with the 0s rather than the one with the 9s. Then associate to ξ the set of all
positive integers n such that a 1 appears in the nth place in this expansion. Every set
of positive integers is associated to some real number (the sum of 10−n for all n in
the set), and so an enumeration of the real numbers would immediately give rise to
an enumeration of the sets of positive integers, which cannot exist, by the preceding
theorem.
Problems
2.1 Show that the set of all subsets of an inﬁnite enumerable set is nonenumerable.
2.2 Show that if for some or all of the ﬁnite strings from a given ﬁnite or enumerable
alphabet we associate to the string a total or partial function from positive
integers to positive integers, then there is some total function on positive integers
taking only the values 1 and 2 that is not associated with any string.
2.3 In mathematics, the real numbers are often identiﬁed with the points on a line.
Show that the set of real numbers, or equivalently, the set of points on the line,
is equinumerous with the set of points on the semicircle indicated in Figure 2-3.
0
1
Figure 2-3. Interval, semicircle, and line.

PROBLEMS
21
2.4 Show that the set of real numbers ξ with 0 < ξ < 1, or equivalently, the set
of points on the interval shown in Figure 2-3, is equinumerous with the set of
points on the semicircle.
2.5 Show that the set of real numbers ξ with 0 < ξ < 1 is equinumerous with the
set of all real numbers.
2.6 A real number x is called algebraic if it is a solution to some equation of the
form
cdxd + cd−1xd−1 + cd−2xd−2 + · · · + c2x2 + c1x + c0 = 0
where the ci are rational numbers and cd ̸= 0. For instance, for any rational
number r, the number r itself is algebraic, since it is the solution to x −r = 0;
and the square root √r of r is algebraic, since it is a solution to x2 −r = 0.
(a) Use the fact from algebra that an equation like the one displayed has at
most d solutions to show that every algebraic number can be described by
a ﬁnite string of symbols from an ordinary keyboard.
(b) A real number that is not algebraic is called transcendental. Prove that
transcendental numbers exist.
2.7 Each real number ξ with 0 < ξ < 1 has a binary representation 0 · x1x2x3 . . .
where each xi is a digit 0 or 1, and the successive places represent halves,
quarters, eighths, and so on. Show that the set of real numbers, ξ with 0 < ξ < 1
and ξ not a rational number with denominator a power of two, is equinumerous
with the set of those sets of positive integers that are neither ﬁnite nor coﬁnite.
2.8 Show that if A is equinumerous with C and B is equinumerous with D, and the
intersections A ∩B and C ∩D are empty, then the unions A ∪B and C ∪D
are equinumerous.
2.9 Show that the set of real numbers ξ with 0 < ξ< 1 (and hence by an earlier
problem the set of all real numbers) is equinumerous with the set of all sets of
positive integers.
2.10 Show that the following sets are equinumerous:
(a) the set of all pairs of sets of positive integers
(b) the set of all sets of pairs of positive integers
(c) the set of all sets of positive integers.
2.11 Show that the set of points on a line is equinumerous with the set of points on
a plane.
2.12 Show that the set of points on a line is equinumerous with the set of points in
space.
2.13 (Richard’s paradox) What (if anything) is wrong with the following argument?
The set of all ﬁnite strings of symbols from the alphabet, including the space,
capital letters, and punctuation marks, is enumerable; and for deﬁniteness let us use
the speciﬁc enumeration of ﬁnite strings based on prime decomposition. Some strings
amount to deﬁnitions in English of sets of positive integers and others do not. Strike
out the ones that do not, and we are left with an enumeration of all deﬁnitions in
English of sets of positive integers, or, replacing each deﬁnition by the set it deﬁnes,
an enumeration of all sets of positive integers that have deﬁnitions in English. Since
somesetshavemorethanonedeﬁnition,therewillberedundanciesinthisenumeration

22
DIAGONALIZATION
of sets. Strike them out to obtain an irredundant enumeration of all sets of positive
integers that have deﬁnitions in English. Now consider the set of positive integers
deﬁned by the condition that a positive integer n is to belong to the set if and only if
it does not belong to the nth set in the irredundant enumeration just described.
This set does not appear in that enumeration. For it cannot appear at the nth place
for any n, since there is a positive integer, namely n itself, that belongs to this set if
and only if it does not belong to the nth set in the enumeration. Since this set does
not appear in our enumeration, it cannot have a deﬁnition in English. And yet it does
have a deﬁnition in English, and in fact we have just given such a deﬁnition in the
preceding paragraph.

3
Turing Computability
A function is effectively computable if there are deﬁnite, explicit rules by following which
one could in principle compute its value for any given arguments. This notion will be
further explained below, but even after further explanation it remains an intuitive notion.
In this chapter we pursue the analysis of computability by introducing a rigorously
deﬁned notion of a Turing-computable function. It will be obvious from the deﬁnition that
Turing-computablefunctionsareeffectivelycomputable.Thehypothesisthat,conversely,
every effectively computable function is Turing computable is known as Turing’s thesis.
This thesis is not obvious, nor can it be rigorously proved (since the notion of effective
computability is an intuitive and not a rigorously deﬁned one), but an enormous amount
of evidence has been accumulated for it. A small part of that evidence will be presented
in this chapter, with more in chapters to come. We ﬁrst introduce the notion of Turing
machine, give examples, and then present the ofﬁcial deﬁnition of what it is for a function
to be computable by a Turing machine, or Turing computable.
A superhuman being, like Zeus of the preceding chapter, could perhaps write out the
whole table of values of a one-place function on positive integers, by writing each
entry twice as fast as the one before; but for a human being, completing an inﬁnite
process of this kind is impossible in principle. Fortunately, for human purposes we
generally do not need the whole table of values of a function f , but only need the
values one at a time, so to speak: given some argument n, we need the value f (n). If
it is possible to produce the value f (n) of the function f for argument n whenever
such a value is needed, then that is almost as good as having the whole table of values
written out in advance.
A function f from positive integers to positive integers is called effectively com-
putable if a list of instructions can be given that in principle make it possible to
determine the value f (n) for any argument n. (This notion extends in an obvious
way to two-place and many-place functions.) The instructions must be completely
deﬁnite and explicit. They should tell you at each step what to do, not tell you to go
ask someone else what to do, or to ﬁgure out for yourself what to do: the instructions
should require no external sources of information, and should require no ingenuity
to execute, so that one might hope to automate the process of applying the rules, and
have it performed by some mechanical device.
There remains the fact that for all but a ﬁnite number of values of n, it will be
infeasible in practice for any human being, or any mechanical device, actually to carry
23

24
TURING COMPUTABILITY
out the computation: in principle it could be completed in a ﬁnite amount of time if we
stayed in good health so long, or the machine stayed in working order so long; but in
practice we will die, or the machine will collapse, long before the process is complete.
(There is also a worry about ﬁnding enough space to store the intermediate results
of the computation, and even a worry about ﬁnding enough matter to use in writing
down those results: there’s only a ﬁnite amount of paper in the world, so you’d have to
writer smaller and smaller without limit; to get an inﬁnite number of symbols down on
paper, eventually you’d be trying to write on molecules, on atoms, on electrons.) But
our present study will ignore these practical limitations, and work with an idealized
notion of computability that goes beyond what actual people or actual machines can
be sure of doing. Our eventual goal will be to prove that certain functions are not
computable, even if practical limitations on time, speed, and amount of material could
somehow be overcome, and for this purpose the essential requirement is that our
notion of computability not be too narrow.
So far we have been sliding over a signiﬁcant point. When we are given as argument
a number n or pair of numbers (m, n), what we in fact are directly given is a numeral for
n or an ordered pair of numerals for m and n. Likewise, if the value of the function
we are trying to compute is a number, what our computations in fact end with is a
numeral for that number. Now in the course of human history a great many systems
of numeration have been developed, from the primitive monadic or tally notation,
in which the number n is represented by a sequence of n strokes, through systems
like Roman numerals, in which bunches of ﬁve, ten, ﬁfty, one-hundred, and so forth
strokes are abbreviated by special symbols, to the Hindu–Arabic or decimal notation
in common use today. Does it make a difference in a deﬁnition of computability
which of these many systems we adopt?
Certainly computations can be harder in practice with some notations than with
others. For instance, multiplying numbers given in decimal numerals (expressing the
product in the same form) is easier in practice than multiplying numbers given in
something like Roman numerals. Suppose we are given two numbers, expressed in
Roman numerals, say XXXIX and XLVIII, and are asked to obtain the product, also
expressed in Roman numerals. Probably for most us the easiest way to do this would
be ﬁrst to translate from Roman to Hindu–Arabic—the rules for doing this are, or at
least used to be, taught in primary school, and in any case can be looked up in reference
works—obtaining 39 and 48. Next one would carry out the multiplication in our own
more convenient numeral system, obtaining 1872. Finally, one would translate the
result back into the inconvenient system, obtaining MDCCCLXXII. Doing all this
is, of course, harder than simply performing a multiplication on numbers given by
decimal numerals to begin with.
But the example shows that when a computation can be done in one notation, it
is possible in principle to do in any other notation, simply by translating the data
from the difﬁcult notation into an easier one, performing the operation using the
easier notation, and then translating the result back from the easier to the difﬁcult
notation. If a function is effectively computable when numbers are represented in
one system of numerals, it will also be so when numbers are represented in any other
system of numerals, provided only that translation between the systems can itself be

TURING COMPUTABILITY
25
carried out according to explicit rules, which is the case for any historical system of
numeration that we have been able to decipher. (To say we have been able to decipher
it amounts to saying that there are rules for translating back and forth between it and
the system now in common use.) For purposes of framing a rigorously deﬁned notion
of computability, it is convenient to use monadic or tally notation.
A Turing machine is a speciﬁc kind of idealized machine for carrying out computa-
tions, especially computations on positive integers represented in monadic notation.
We suppose that the computation takes place on a tape, marked into squares, which
is unending in both directions—either because it is actually inﬁnite or because there
is someone stationed at each end to add extra blank squares as needed. Each square
either is blank, or has a stroke printed on it. (We represent the blank by S0 or 0 or
most often B, and the stroke by S1 or | or most often 1, depending on the context.)
And with at most a ﬁnite number of exceptions, all squares are blank, both initially
and at each subsequent stage of the computation.
At each stage of the computation, the computer (that is, the human or mechanical
agent doing the computation) is scanning some one square of the tape. The computer
is capable of erasing a stroke in the scanned square if there is one there, or of printing
a stroke if the scanned square is blank. And he, she, or it is capable of movement:
one square to the right or one square to the left at a time. If you like, think of the
machine quite crudely, as a box on wheels which, at any stage of the computation,
is over some square of the tape. The tape is like a railroad track; the ties mark the
boundaries of the squares; and the machine is like a very short car, capable of moving
along the track in either direction, as in Figure 3-1.
Figure 3-1. A Turing machine.
At the bottom of the car there is a device that can read what’s written between
the ties, and erase or print a stroke. The machine is designed in such a way that
at each stage of the computation it is in one of a ﬁnite number of internal states,
q1, . . . , qm. Being in one state or another might be a matter of having one or another
cog of a certain gear uppermost, or of having the voltage at a certain terminal inside
the machine at one or another of m different levels, or what have you: we are not
concerned with the mechanics or the electronics of the matter. Perhaps the simplest
way to picture the thing is quite crudely: inside the box there is a little man, who
does all the reading and writing and erasing and moving. (The box has no bottom:
the poor mug just walks along between the ties, pulling the box along.) This operator
inside the machine has a list of m instructions written down on a piece of paper and
is in state qi when carrying out instruction number i.
Each of the instructions has conditional form: it tells what to do, depending on
whether the symbol being scanned (the symbol in the scanned square) is the blank or

26
TURING COMPUTABILITY
stroke, S0 or S1. Namely, there are ﬁve things that can be done:
(1) Erase: write S0 in place of whatever is in the scanned square.
(2) Print: write S1 in place of whatever is in the scanned square.
(3) Move one square to the right.
(4) Move one square to the left.
(5) Halt the computation.
[In case the square is already blank, (1) amounts to doing nothing; in case the
square already has a stroke in it, (2) amounts to doing nothing.] So depending on
what instruction is being carried out (= what state the machine, or its operator, is
in) and on what symbol is being scanned, the machine or its operator will perform
one or another of these ﬁve overt acts. Unless the computation has halted (overt act
number 5), the machine or its operator will perform also a covert act, in the privacy
of box, namely, the act of determining what the next instruction (next state) is to be.
Thus the present state and the presently scanned symbol determine what overt act is
to be performed, and what the next state is to be.
The overall program of instructions can be speciﬁed in various ways, for example,
by a machine table, or by a ﬂow chart (also called a ﬂow graph), or by a set of
quadruples. For the case of a machine that writes three symbols S1 on a blank tape
and then halts, scanning the leftmost of the three, the three sorts of description are
illustrated in Figure 3-2.
Figure 3-2. A Turing machine program.
3.1 Example (Writing a speciﬁed number of strokes). We indicate in Figure 3-2 a ma-
chine that will write the symbol S1 three times. A similar construction works for any
speciﬁed symbol and any speciﬁed number of times. The machine will write an S1 on the
square it’s initially scanning, move left one square, write an S1 there, move left one more
square, write an S1 there, and halt. (It halts when it has no further instructions.) There
will be three states—one for each of the symbols S1 that are to be written. In Figure 3-2,
the entries in the top row of the machine table (under the horizontal line) tell the ma-
chine or its operator, when following instruction q1, that (1) an S1 is to be written and
instruction q1 is to be repeated, if the scanned symbol is S0, but that (2) the machine is
to move left and follow instruction q2 next, if the scanned symbol is S1. The same infor-
mation is given in the ﬂow chart by the two arrows that emerge from the node marked
q1; and the same information is also given by the ﬁrst two quadruples. The signiﬁcance

TURING COMPUTABILITY
27
in general of a table entry, of an arrow in a ﬂow chart, and of a quadruple is shown in
Figure 3-3.
Figure 3-3. A Turing machine instruction.
Unless otherwise stated, it is to be understood that a machine starts in its lowest-numbered
state. The machine we have been considering halts when it is in state q3 scanning S1, for
there is no table entry or arrow or quadruple telling it what to do in such a case. A virtue
of the ﬂow chart as a way of representing the machine program is that if the starting state
is indicated somehow (for example, if it is understood that the leftmost node represents
the starting state unless there is an indication to the contrary), then we can dispense with
the names of the states: It doesn’t matter what you call them. Then the ﬂow chart could be
redrawn as in Figure 3-4.
Figure 3-4. Writing three strokes.
We can indicate how such a Turing machine operates by writing down its sequence
of conﬁgurations. There is one conﬁguration for each stage of the computation, showing
what’s on the tape at that stage, what state the machine is in at that stage, and which square
is being scanned. We can show this by writing out what’s on the tape and writing the name
of the present state under the symbol in the scanned square; for instance,
1100111
2
shows a string or block of two strokes followed by two blanks followed by a string or block
of three strokes, with the machine scanning the leftmost stroke and in state 2. Here we have
written the symbols S0 and S1 simply as 0 and 1, and similarly the state q2 simply as 2,
to save needless fuss. A slightly more compact representation writes the state number as a
subscript on the symbol scanned: 12100111.
This same conﬁguration could be written 012100111 or 121001110 or 0121001110 or
0012100111 or . . . —a block of 0s can be written at the beginning or end of the tape, and can
be shorted or lengthened ad lib. without changing the signiﬁcance: the tape is understood
to have as many blanks as you please at each end.
We can begin to get a sense of the power of Turing machines by considering some
more complex examples.

28
TURING COMPUTABILITY
3.2 Example (Doubling the number of strokes). The machine starts off scanning the left-
most of a block of strokes on an otherwise blank tape, and winds up scanning the leftmost
of a block of twice that many strokes on an otherwise blank tape. The ﬂow chart is shown
in Figure 3-5.
Figure 3-5. Doubling the number of strokes.
How does it work? In general, by writing double strokes at the left and erasing single
strokes at the right. In particular, suppose the initial conﬁguration is 1111, so that we start
in state 1, scanning the leftmost of a block of three strokes on an otherwise blank tape. The
next few conﬁgurations are as follows:
02111
030111
130111
0410111
1410111.
So we have written our ﬁrst double stroke at the left—separated from the original block
111 by a blank. Next we go right, past the blank to the right-hand end of the original block,
and erase the rightmost stroke. Here is how that works, in two phases. Phase 1:
1150111
1105111
1101611
1101161
1101116
11011106.
Now we know that we have passed the last of the original block of strokes, so (phase 2) we
back up, erase one of them, and move one more square left:
1101117
1101107
1101180.
Now we hop back left, over what is left of the original block of strokes, over the blank
separating the original block from the additional strokes we have printed, and over those
additional strokes, until we ﬁnd the blank beyond the leftmost stroke:
110191
110911
1110011
1101011
01011011.
Now we will print another two new strokes, much as before:
0121011
0311011
1311011
04111011
14111011.
We are now back on the leftmost of the block of newly printed strokes, and the process
that led to ﬁnding and erasing the rightmost stroke will be repeated, until we arrive at the
following:
11110117
11110107
11110180.
Another round of this will lead ﬁrst to writing another pair of strokes:
141111101.

TURING COMPUTABILITY
29
It will then lead to erasing the last of the original block of strokes:
111111017
111111007
111111080.
And now the endgame begins, for we have what we want on the tape, and need only move
back to halt on the leftmost stroke:
11111111
11111111
11111111
11111111
11111111
11111111
011111111
11211111.
Now we are in state 12, scanning a stroke. Since there is no arrow from that node telling us
what to do in such a case, we halt. The machine performs as advertised.
(Note: The fact that the machine doubles the number of strokes when the original number
is three is not a proof that the machine performs as advertised. But our examination of the
special case in which there are three strokes initially made no essential use of the fact that
the initial number was three: it is readily converted into a proof that the machine doubles
the number of strokes no matter how long the original block may be.)
Readers may wish, in the remaining examples, to try to design their own machines
before reading our designs; and for this reason we give the statements of all the
examples ﬁrst, and collect all the proofs afterward.
3.3 Example (Determining the parity of the length of a block of strokes). There is a Turing
machine that, started scanning the leftmost of an unbroken block of strokes on an otherwise
blank tape, eventually halts, scanning a square on an otherwise blank tape, where the square
contains a blank or a stroke depending on whether there were an even or an odd number of
strokes in the original block.
3.4 Example (Adding in monadic (tally) notation). There is a Turing machine that does
the following. Initially, the tape is blank except for two solid blocks of strokes, say a left
block of p strokes and a right block of q strokes, separated by a single blank. Started on the
leftmost blank of the left block, the machine eventually halts, scanning the leftmost stroke
in a solid block of p + q stokes on an otherwise blank tape.
3.5 Example (Multiplying in monadic (tally) notation). There is a Turing machine that
does the same thing as the one in the preceding example, but with p · q in place of p + q.
Proofs
Example 3.3. A ﬂow chart for such a machine is shown in Figure 3-6.
Figure 3-6. Parity machine.
If there were 0 or 2 or 4 or . . . strokes to begin with, this machine halts in state 1,
scanning a blank on a blank tape; if there were 1 or 3 or 5 or . . . , it halts in state 5,
scanning a stroke on an otherwise blank tape.

30
TURING COMPUTABILITY
Example 3.4. The object is to erase the leftmost stroke, ﬁll the gap between the
two blocks of strokes, and halt scanning the leftmost stroke that remains on the tape.
Here is one way of doing it, in quadruple notation: q1S1S0q1; q1S0Rq2; q2S1Rq2;
q2S0S1q3; q3S1Lq3; q3S0Rq4.
Example 3.5. A ﬂow chart for a machine is shown in Figure 3-7.
Figure 3-7. Multiplication machine.
Here is how the machine works. The ﬁrst block, of p strokes, is used as a counter,
to keep track of how many times the machine has added q strokes to the group at the
right. To start, the machine erases the leftmost of the p strokes and sees if there are
any strokes left in the counter group. If not, pq = q, and all the machine has to do is
position itself over the leftmost stroke on the tape, and halt.

TURING COMPUTABILITY
31
But if there are any strokes left in the counter, the machine goes into a leapfrog
routine: in effect, it moves the block of q strokes (the leapfrog group) q places to
the right along the tape. For example, with p = 2 and q = 3 the tape looks like this
initially:
11B111
and looks like this after going through the leapfrog routine:
B1BBBB111.
The machine will then note that there is only one 1 left in the counter, and will ﬁnish
up by erasing that 1, moving right two squares, and changing all Bs to strokes until
it comes to a stroke, at which point it continues to the leftmost 1 and halts.
The general picture of how the leapfrog routine works is shown in Figure 3-8.
Figure 3-8. Leapfrog.
In general, the leapfrog group consists of a block of 0 or 1 or . . . or q strokes,
followed by a blank, followed by the remainder of the q strokes. The blank is there
to tell the machine when the leapfrog game is over: without it the group of q strokes
would keep moving right along the tape forever. (In playing leapfrog, the portion of
the q strokes to the left of the blank in the leapfrog group functions as a counter:
it controls the process of adding strokes to the portion of the leapfrog group to the
right of the blank. That is why there are two big loops in the ﬂow chart: one for each
counter-controlled subroutine.)
We have not yet given an ofﬁcial deﬁnition of what it is for a numerical function
to be computable by a Turing machine, specifying how inputs or arguments are to be
represented on the machine, and how outputs or values represented. Our speciﬁcations
for a k-place function from positive integers to positive integers are as follows:
(a) The arguments m1, . . . , mk of the function will be represented in monadic notation
by blocks of those numbers of strokes, each block separated from the next by a
single blank, on an otherwise blank tape. Thus, at the beginning of the
computation of, say, 3 + 2, the tape will look like this: 111B11.
(b) Initially, the machine will be scanning the leftmost 1 on the tape, and will be in its
initial state, state 1. Thus in the computation of 3 + 2, the initial conﬁguration will
be 1111B11. A conﬁguration as described by (a) and (b) is called a standard initial
conﬁguration (or position).
(c) If the function that is to be computed assigns a value n to the arguments that are
represented initially on the tape, then the machine will eventually halt on a tape

32
TURING COMPUTABILITY
containing a block of that number of strokes, and otherwise blank. Thus in the
computation of 3 + 2, the tape will look like this: 11111.
(d) In this case, the machine will halt scanning the leftmost 1 on the tape. Thus in the
computation of 3 + 2, the ﬁnal conﬁguration will be 1n1111, where nth state is one
for which there is no instruction what to do if scanning a stroke, so that in this
conﬁguration the machine will be halted. A conﬁguration as described by (c) and
(d) is called a standard ﬁnal conﬁguration (or position).
(e) If the function that is to be computed assigns no value to the arguments that are
represented initially on the tape, then the machine either will never halt, or will
halt in some nonstandard conﬁguration such as Bn11111 or B11n111 or B11111n.
The restriction above to the standard position (scanning the leftmost 1) for starting
and halting is inessential, but some speciﬁcations or other have to be made about
initial and ﬁnal positions of the machine, and the above assumptions seem especially
simple.
With these speciﬁcations, any Turing machine can be seen to compute a function of
one argument, a function of two arguments, and, in general, a function of k arguments
for each positive integer k. Thus consider the machine speciﬁed by the single quadru-
ple q111q2. Started in a standard initial conﬁguration, it immediately halts, leaving the
tape unaltered. If there was only a single block of strokes on the tape initially, its ﬁnal
conﬁguration will be standard, and thus this machine computes the identity function id
of one argument: id(m) = m for each positive integer m. Thus the machine computes
a certain total function of one argument. But if there were two or more blocks of
strokes on the tape initially, the ﬁnal conﬁguration will not be standard. Accordingly,
the machine computes the extreme partial function of two arguments that is undeﬁned
for all pairs of arguments: the empty function e2 of two arguments. And in general,
for k arguments, this machine computes the empty function ek of k arguments.
Figure 3-9. A machine computing the value 1 for all arguments.
By contrast, consider the machine whose ﬂow chart is shown in Figure 3-9. This
machine computes for each k the total function that assigns the same value, namely 1,
to each k-tuple. Started in initial state 1 in a standard initial conﬁguration, this machine
erases the ﬁrst block of strokes (cycling between states 1 and 2 to do so) and goes to
state 3, scanning the second square to the right of the ﬁrst block. If it sees a blank there,
it knows it has erased the whole tape, and so prints a single 1 and halts in state 4, in
a standard conﬁguration. If it sees a stroke there, it re-enters the cycle between states
1 and 2, erasing the second block of strokes and inquiring again, in state 3, whether
the whole tape is blank, or whether there are still more blocks to be dealt with.

TURING COMPUTABILITY
33
A numerical function of k arguments is Turing computable if there is some Turing
machine that computes it in the sense we have just been specifying. Now computation
in the Turing-machine sense is certainly one kind of computation in the intuitive
sense, so all Turing-computable functions are effectively computable. Turing’s thesis
is that, conversely, any effectively computable function is Turing computable, so that
computation in the precise technical sense we have been developing coincides with
effective computability in the intuitive sense.
It is easy to imagine liberalizations of the notion of the Turing machine. One could
allow machines using more symbols than just the blank and the stroke. One could
allow machines operating on a rectangular grid, able to move up or down a square as
wellasleftorright.Turing’sthesisimpliesthatnoliberalizationofthenotionofTuring
machine will enlarge the class of functions computable, because all functions that are
effectively computable in any way at all are already computable by a Turing machine
of the restricted kind we have been considering. Turing’s thesis is thus a bold claim.
It is possible to give a heuristic argument for it. After all, effective computation
consists of moving around and writing and perhaps erasing symbols, according to
deﬁnite, explicit rules; and surely writing and erasing symbols can be done stroke by
stroke, and moving from one place to another can be done step by step. But the main
argument will be the accumulation of examples of effectively computable functions
that we succeed in showing are Turing computable. So far, however, we have had
just a few examples of Turing machines computing numerical functions, that is,
of effectively computable functions that we have proved to be Turing computable:
additionandmultiplicationintheprecedingsection,andjustnowtheidentityfunction,
the empty function, and the function with constant value 1.
Now addition and multiplication are just the ﬁrst two of a series of arithmetic
operations all of which are effectively computable. The next item in the series is ex-
ponentiation. Just as multiplication is repeated addition, so exponentiation is repeated
multiplication. (Then repeated exponentiation gives a kind of super-exponentiation,
and so on. We will investigate this general process of deﬁning new functions from
old in a later chapter.) If Turing’s thesis is correct, there must be a Turing machine
for each of these functions, computing it. Designing a multiplier was already difﬁcult
enough to suggest that designing an exponentiator would be quite a challenge, and
in any case, the direct approach of designing a machine for each operation would
take us forever, since there are inﬁnitely many operations in the series. Moreover,
there are many other effectively computable numerical functions besides the ones in
this series. When we return, in the chapter after next, to the task of showing vari-
ous effectively computable numerical functions to be Turing computable, and thus
accumulating evidence for Turing’s thesis, a less direct approach will be adopted,
and all the operations in the series that begins with addition and multiplication will
be shown to be Turing computable in one go.
For the moment, we set aside the positive task of showing functions to be Turing
computable and instead turn to examples of numerical functions of one argument
that are Turing uncomputable (and so, if Turing’s thesis is correct, effectively uncom-
putable).

34
TURING COMPUTABILITY
Problems
3.1 Consider a tape containing a block of n strokes, followed by a space, followed
by a block of m strokes, followed by a space, followed by a block of k strokes,
and otherwise blank. Design a Turing machine that when started on the leftmost
stroke will eventually halt, having neither printed nor erased anything . . .
(a) . . . on the leftmost stroke of the second block.
(b) . . . on the leftmost stroke of the third block.
3.2 Continuing the preceding problem, design a Turing machine that when started
on the leftmost stroke will eventually halt, having neither printed nor erased
anything . . .
(a) . . . on the rightmost stroke of the second block.
(b) . . . on the rightmost stroke of the third block.
3.3 Design a Turing machine that, starting with the tape as in the preceding problems,
will eventually halt on the leftmost stroke on the tape, which is now to contain a
block of n strokes, followed by a blank, followed by a block of m + 1 strokes,
followed by a blank, followed by a block of k strokes.
3.4 Design a Turing machine that, starting with the tape as in the preceding problems,
will eventually halt on the leftmost stroke on the tape, which is now to contain a
block of n strokes, followed by a blank, followed by a block of m −1 strokes,
followed by a blank, followed by a block of k strokes.
3.5 Design a Turing machine to compute the function min(x, y) = the smaller of x
and y.
3.6 Design a Turing machine to compute the function max(x, y) = the larger of
x and y.

4
Uncomputability
In the preceding chapter we introduced the notion of Turing computability. In the present
short chapter we give examples of Turing-uncomputable functions: the halting function
in section 4.1, and the productivity function in the optional section 4.2. If Turing’s thesis
is correct, these are actually examples of effectively uncomputable functions.
4.1 The Halting Problem
There are too many functions from positive integers to positive integers for them all
to be Turing computable. For on the one hand, as we have seen in problem 2.2, the
set of all such functions is nonenumerable. And on the other hand, the set of Turing
machines, and therefore of Turing-computable functions, is enumerable, since the re-
presentationofaTuringmachineintheformofquadruplesamountstoarepresentation
of it by a ﬁnite string of symbols from a ﬁnite alphabet; and we have seen in Chapter 1
that the set of such strings is enumerable. These considerations show us that there must
exist functions that are not Turing computable, but they do not provide an explicit
example of such a function. To provide explicit examples is the task of this chapter.
We begin simply by examining the argument just given in slow motion, with careful
attention to details, so as to extract a speciﬁc example of a Turing-uncomputable
function from it.
To begin with, we have suggested that we can enumerate the Turing-computable
functions of one argument by enumerating the Turing machines, and that we can enu-
merate the Turing machines using their quadruple representations. As we turn to de-
tails, it will be convenient to modify the quadruple representation used so far some-
what. To indicate the nature of the modiﬁcations, consider the machine in Figure 3-9
in the preceding chapter. Its quadruple representation would be
q1S0Rq3, q1S1S0q2, q2S0Rq1, q3S0S1q4, q3S1S0q2.
We have already been taking the lowest-numbered state q1 to be the initial state.
We now want to assume that the highest-numbered state is a halted state, for which
there are no instructions and no quadruples. This is already the case in our example,
and if it were not already so in some other example, we could make it so by adding
one additional state.
35

36
UNCOMPUTABILITY
We now also want to assume that for every state qi except this highest-numbered
halted state, and for each of the two symbols Sj we are allowing ourselves to use,
namely S0 = B and S1 = 1, there is a quadruple beginning qi Sj. This is not so in our
example as it stands, where there is no instruction for q2S1. We have been interpreting
the absence of an instruction for qi Sj as an instruction to halt, but the same effect
could be achieved by giving an explicit instruction to keep the same symbol and then
go to the highest-numbered state. When we modify the representation by adding this
instruction, the representation becomes
q1S0Rq3, q1S1S0q2, q2S0Rq1, q2S1S1q4, q3S0S1q4, q3S1S0q2.
Now taking the quadruples beginning q1S0, q1S1, q2S0, . . . in that order, as we
have done, the ﬁrst two symbols of each quadruple are predictable and therefore do
not need to be written. So we may simply write
Rq3, S0q2, Rq1, S1q4, S1q4, S0q2.
Representing qi by i, and Sj by j + 1 (so as to avoid 0), and L and R by 3 and 4, we
can write still more simply
4, 3, 1, 2, 4, 1, 2, 4, 2, 4, 1, 2.
Thus the Turing machine can be completely represented by a ﬁnite sequence of
positive integers—and even, if desired, by a single positive integer, say using the
method of coding based on prime decomposition:
24 · 33 · 5 · 72 · 114 · 13 · 172 · 194 · 232 · 294 · 31 · 372.
Not every positive integer will represent a Turing machine: whether a given posi-
tive integer does so or not depends on what the sequence of exponents in its prime
decomposition is, and not every ﬁnite sequence represents a Turing machine. Those
that do must have length some multiple 4n of 4, and have among their odd-numbered
entries only numbers 1 to 4 (representing B, 1, L, R) and among their even-numbered
entries only numbers 1 to n + 1 (representing the initial state q1, various other states
qi, and the halted state qn+1). But no matter: from the above representation we at least
get a gappy listing of all Turing machines, in which each Turing machine is listed
at least once, and on ﬁlling in the gaps we get a gapless list of all Turing machines,
M1, M2, M3, . . . , and from this a similar list of all Turing-computable functions of
oneargument, f1, f2, f3, . . . ,where fi isthetotalorpartialfunctioncomputedby Mi.
To give a trivial example, consider the machine represented by (1, 1, 1, 1), or
2 · 3 · 5 · 7 = 210. Started scanning a stroke, it erases it, then leaves the resulting
blank alone and remains in the same initial state, never going to the halted state,
which would be state 2. Or consider the machine represented by (2, 1, 1, 1) or
22 · 3 · 5 · 7 = 420. Started scanning a stroke, it erases it, then prints it back again,
then erases it, then prints it back again, and so on, again never halting. Or consider
the machine represented by (1, 2, 1, 1), or 2 · 32 · 5 · 7 = 630. Started scanning a
stroke, it erases it, then goes to the halted state 2 when it scans the resulting blank,
which means halting in a nonstandard ﬁnal conﬁguration. A little thought shows that
210, 420, 630 are the smallest numbers that represent Turing machines, so the three

4.1. THE HALTING PROBLEM
37
machines just described will be M1, M2, M3, and we have f1 = f2 = f3 = the empty
function.
WehavenowindicatedanexplicitenumerationoftheTuring-computablefunctions
of one argument, obtained by enumerating the machines that compute them. The fact
that such an enumeration is possible shows, as we remarked at the outset, that there
must exist Turing-uncomputable functions of a single argument. The point of actually
specifying one such enumeration is to be able to exhibit a particular such function.
To do so, we deﬁne a diagonal function d as follows:
d(n) =
2
if fn(n) is deﬁned and = 1
1
otherwise.
(1)
Now d is a perfectly genuine total function of one argument, but it is not Turing
computable, that is, d is neither f1 nor f2 nor f3, and so on. Proof: Suppose that d is
one of the Turing computable functions—the mth, let us say. Then for each positive
integer n, either d(n) and fm(n) are both deﬁned and equal, or neither of them is
deﬁned. But consider the case n = m:
fm(m) = d(m) =
 2
if fm(m) is deﬁned and = 1
1
otherwise.
(2)
Then whether fm(m) is or is not deﬁned, we have a contradiction: Either fm(m) is
undeﬁned, in which case (2) tells us that it is deﬁned and has value 1; or fm(m)
is deﬁned and has a value ̸=1, in which case (2) tells us it has value 1; or fm(m)
is deﬁned and has value 1, in which case (2) tells us it has value 2. Since we
have derived a contradiction from the assumption that d appears somewhere in the
list f1, f2, . . . , fm, . . . , we may conclude that the supposition is false. We have
proved:
4.1 Theorem. The diagonal function d is not Turing computable.
According to Turing’s thesis, since d is not Turing computable, d cannot be
effectively computable. Why not? After all, although no Turing machine computes
the function d, we were able compute at least its ﬁrst few values. For since, as we
have noted, f1 = f2 = f3 = the empty function we have d(1) = d(2) = d(3) = 1.
And it may seem that we can actually compute d(n) for any positive integer n—if we
don’t run out of time.
Certainly it is straightforward to discover which quadruples determine Mn for n =
1, 2, 3, and so on. (This is straightforward in principle, though eventually humanly
infeasible in practice because the duration of the trivial calculations, for large n,
exceeds the lifetime of a human being and, in all probability, the lifetime of the
human race. But in our idealized notion of computability, we ignore the fact that
human life is limited.)
And certainly it is perfectly routine to follow the operations of Mn, once the initial
conﬁguration has been speciﬁed; and if Mn does eventually halt, we must eventually
get that information by following its operations. Thus if we start Mnwith input n and
it does halt with that input, then by following its operations until it halts, we can see
whether it halts in nonstandard position, leaving fn(n) undeﬁned, or halts in standard

38
UNCOMPUTABILITY
position with output fn(n) = 1, or halts in standard position with output fn(n) ̸= 1.
In the ﬁrst or last cases, d(n) = 1, and in the middle case, d(n) = 2.
But there is yet another case where d(n) = 1; namely, the case where Mn never
halts at all. If Mn is destined never to halt, given the initial conﬁguration, can we
ﬁnd that out in a ﬁnite amount of time? This is the essential question: determining
whether machine Mn, started scanning the leftmost of an unbroken block of n strokes
on an otherwise blank tape, does or does not eventually halt.
Is this perfectly routine? Must there be some point in the routine process of fol-
lowing its operations at which it becomes clear that it will never halt? In simple cases
this is so, as we saw in the cases of M1, M2, and M3 above. But for the function d to
be effectively computable, there would have to be a uniform mechanical procedure,
applicable not just in these simple cases but also in more complicated cases, for dis-
covering whether or not a given machine, started in a given conﬁguration, will ever
halt.
Thus consider the multiplier in Example 3.5. Its sequential representation would
be a sequence of 68 numbers, each ≤18. It is routine to verify that it represents
a Turing machine, and one can easily enough derive from it a ﬂow chart like the
one shown in Figure 3-7, but without the annotations, and of course without the
accompanying text. Suppose one came upon such a sequence. It would be routine
to check whether it represented a Turing machine and, if so, again to derive a ﬂow
chart without annotations and accompanying text. But is there a uniform method
or mechanical routine that, in this and much more complicated cases, allows one to
determine from inspecting the ﬂow chart, without any annotations or accompanying
text, whether the machine eventually halts, once the initial conﬁguration has been
speciﬁed?
If there is such a routine, Turing’s thesis is erroneous: if Turing’s thesis is correct,
there can be no such routine. At present, several generations after the problem was
ﬁrst posed, no one has yet succeeded in describing any such routine—a fact that must
be considered some kind of evidence in favor of the thesis.
Let us put the matter another way. A function closely related to d is the halting
function h of two arguments. Here h(m, n) = 1 or 2 according as machine m, started
with input n, eventually halts or not. If h were effectively computable, d would
be effectively computable. For given n, we could ﬁrst compute h(n, n). If we got
h(n, n) = 2, we would know that d(n) = 1. If we got h(n, n) = 1, we would know
that we could safely start machine Mn in stardard initial conﬁguration for input n, and
that it would eventually halt. If it halted in nonstandard conﬁguration, we would again
have d(n) = 1. If it halted in standard ﬁnal conﬁguration giving an output fn(n), it
would have d(n) = 1 or 2 according as fn(n) ̸= 1 or = 1.
This is an informal argument showing that if h were effectively computable, then d
would be effectively computable. Since we have shown that d is not Turing com-
putable, assuming Turing’s thesis it follows that d is not effectively computable, and
hence that h is not effectively computable, and so not Turing computable. It is also
possible to prove rigorously, though we do not at this point have the apparatus needed
to do so, that if h were Turing computable, then d would be Turing computable, and
since we have shown that d is not Turing computable, this would show that h is not

4.1. THE HALTING PROBLEM
39
Turing computable. Finally, it is possible to prove rigorously in another way, not
involving d, that h is not Turing computable, and this we now do.
4.2 Theorem. The halting function h is not Turing computable.
Proof: By way of background we need two special Turing machines. The ﬁrst is
a copying machine C, which works as follows. Given a tape containing a block of n
strokes, and otherwise blank, if the machine is started scanning the leftmost stroke
on the tape, it will eventually halt with the tape containing two blocks of n strokes
separated by a blank, and otherwise blank, with the machine scanning the leftmost
stroke on the tape. Thus if the machine is started with
. . . BBB1111BBB . . .
it will halt with
. . . BBB1111B1111BBB . . .
eventually. We ask you to design such a machine in the problems at the end of this
chapter (and give you a pretty broad hint how to do it at the end of the book).
The second is a dithering machine D. Started on the leftmost of a block of n strokes
on an otherwise blank tape, D eventually halts if n > 1, but never halts if n = 1. Such
a machine is described by the sequence
1, 3, 4, 2, 3, 1, 3, 3.
Started on a stroke in state 1, it moves right and goes into state 2. If it ﬁnds itself on a
stroke, it moves back left and halts, but if it ﬁnds itself on a blank, it moves back left
and goes into state 1, starting an endless back-and-forth cycle.
Now suppose we had a machine H that computed the function h. We could combine
the machines C and H as follows: if the states of C are numbered 1 through p,
and the states of H are numbered 1 through q, renumber the latter states p + 1
through r = p + q, and write these renumbered instructions after the instructions
for C. Originally, C tells us to halt by telling us to go into state p + 1, but in the
new combined instructions, going into state p + 1 means not halting, but beginning
the operations of machine H. So the new combined instructions will have us ﬁrst
go through the operations of C, and then, when C would have halted, go through
the operations of H. The result is thus a machine G that computes the function
g(n) = h(n, n).
We now combine this machine G with the dithering machine D, renumbering the
states of the latter as r + 1 and r + 2, and writing its instructions after those for G.
The result will be a machine M that goes through the operations of G and then the
operations of D. Thus if machine number n halts when started on its own number,
that is, if h(n, n) = g(n) = 1, then the machine M does not halt when started on
that number n, whereas if machine number n does not halt when started on its own
number, that is, if h(n, n) = g(n) = 2, then machine M does halt when started on n.
But of course there can be no such machine as M. For what would it do if started
with input its own number m? It would halt if and only if machine number m, which is

40
UNCOMPUTABILITY
to say itself, does not halt when started with input the number m. This contradiction
shows there can be no such machine as H.
Thehaltingproblemistoﬁndaneffectiveprocedurethat,givenanyTuringmachine
M, say represented by its number m, and given any number n, will enable us to
determine whether or not that machine, given that number as input, ever halts. For
the problem to be solvable by a Turing machine would require there to be a Turing
machine that, given m and n as inputs, produces as its output the answer to the question
whether machine number m with input n ever halts. Of course, a Turing machine of
the kind we have been considering could not produce the output by printing the word
‘yes’ or ‘no’ on its tape, since we are considering machines that operate with just
two symbols, the blank and the stroke. Rather, we take the afﬁrmative answer to be
presented by an output of 1 and the negative by an output of 2. With this understanding,
the question whether the halting problem can be solved by a Turing machine amounts
to the question whether the halting function h is Turing computable, and we have just
seen in Theorem 4.2 that it is not. That theorem, accordingly, is often quoted in the
form: ‘The halting problem is not solvable by a Turing machine.’ Assuming Turing’s
thesis, it follows that it is not solvable at all.
Thus far we have two examples of functions that are not Turing computable—
or problems that are not solvable by any Turing machine—and if Turing’s thesis is
correct, these functions are not effectively computable. A further example is given
in the next section. Though working through the example will provide increased
familiarity with the potential of Turing machines that will be desirable when we
come to the next chapter, and in any case the example is a beautiful one, still none of
the material connected with this example is strictly speaking indispensable for any
of our further work; and therefore we have starred the section in which it appears as
optional.
4.2* The Productivity Function
Consider a k-state Turing machine, that is, a machine with k states (not counting the
halted state). Start it with input k, that is, start it in its initial state on the leftmost
of a block of k strokes on an otherwise blank tape. If the machine never halts, or
halts in nonstandard position, give it a score of zero. If it halts in standard position
with output n, that is, on the leftmost of a block of n strokes on an otherwise blank
tape, give it a score of n. Now deﬁne s(k) = the highest score achieved by any k-state
Turing machine. This function can be shown to be Turing uncomputable.
We ﬁrst show that if the function s were Turing computable, then so would be the
function t given by t(k) = s(k) + 1. For supposing we have a machine that computes
s, we can modify it as follows to get a machine, having one more state than the original
machine, that computes t. Where the instructions for the original machine would have
it halt, the instructions for the new machine will have it go into the new, additional
state. In this new state, if the machine is scanning a stroke, it is to move one square
to the left, remaining in the new state; while if it is scanning a blank, it is to print a
stroke and halt. A little thought shows that a computation of the new machine will

4.2. THE PRODUCTIVITY FUNCTION
41
go through all the same steps as the old machine, except that, when the old machine
would halt on the leftmost of a block of n strokes, the new machine will go through
two more steps of computation (moving left and printing a stroke), leaving it halted
on the leftmost of a block of n + 1 strokes. Thus its output will be one more than
the output of the original machine, and if the original machine, for a given argument,
computes the value of s, the new machine will compute the value of t.
Thus, to show that no Turing machine can compute s, it will now be enough to show
that no Turing machine can compute t. And this is not hard to do. For suppose there
were a machine computing t. It would have some number k of states (not counting the
halted state). Started on the leftmost of a block of k strokes on an otherwise blank
tape, it would halt on the leftmost of a block of t(k) strokes on an otherwise blank
tape. But then t(k) would be the score of this particular k-state machine, and that is
impossible, since t(k) > s(k) = the highest score achieved by any k-state machine.
Thus we have proved:
4.3 Proposition. The scoring function s is not Turing computable.
Let us have another look at the function s in the light of Turing’s thesis. Accord-
ing to Turing’s thesis, since s is not Turing computable, s cannot be effectively
computable. Why not? After all there are (ignoring labelling) only ﬁnitely many
quadruple representations or ﬂow charts of k-place Turing machines for a given k. We
could in principle start them all going in state 1 with input k and await developments.
Some machines will halt at once, with score 0. As time passes, one or another of the
other machines may halt; then we can check whether or not it has halted in standard
position. If not, its score is 0; if so, its score can be determined simply by counting
the number of strokes in a row on the tape. If this number is less than or equal
to the score of some k-state machine that stopped earlier, we can ignore it. If it is
greater than the score of any such machine, then it is the new record-holder. Some
machines will run on forever, but since there are only ﬁnitely many machines, there
will come a time when any machine that is ever going to halt has halted, and the
record-holding machine at that time is a k-state machine of maximum score, and
its score is equal to s(k). Why doesn’t this amount to an effective way of comput-
ing s(k)?
It would, if we had some method of effectively determining which machines are
eventually going to halt. Without such a method, we cannot determine which of the
machines that haven’t halted yet at a given time are destined to halt at some later
time, and which are destined never to halt at all, and so we cannot determine whether
or not we have reached a time when all machines that are ever going to halt have
halted. The procedure outlined in the preceding paragraph gives us a solution to the
scoring problem, the problem of computing s(n), only if we already have a solution
to the halting problem, the problem of determining whether or not a given machine
will, for given input, eventually halt. This is the ﬂaw in the procedure.
There is a related Turing-uncomputable function that is even simpler to describe
than s, called the Rado or busy-beaver function, which may be deﬁned as follows.
Consider a Turing machine started with the tape blank (rather than with input equal
to the number of states of the machine, as in the scoring-function example). If the

42
UNCOMPUTABILITY
machine eventually halts, scanning the leftmost of an unbroken block of strokes on
an otherwise blank tape, its productivity is said to be the length of that block. But if
the machine never halts, or halts in some other conﬁguration, its productivity is said
to be 0. Now deﬁne p(n) = the productivity of the most productive Turing machine
having no more than n states (not counting the halted state).
This function also can be shown to be Turing uncomputable.
The facts needed about the function p can be conveniently set down in a series of
examples. We state all the examples ﬁrst, and then give our proofs, in case the reader
wishes to look for a proof before consulting ours.
4.4 Example. p(1) = 1
4.5 Example. p(n + 1) > p(n) for all n
4.6 Example. There is an i such that p(n + i) ≥2p(n) for all n
Proofs
Example 4.4. There are just 25 Turing machines with a single state q1. Each may
be represented by a ﬂow chart in which there is just one node, and 0 or 1 or 2 arrows
(from that node back to itself). Let us enumerate these ﬂow charts.
Consider ﬁrst the ﬂow chart with no arrows at all. (There is just one.) The
corresponding machine halts immediately with the tape still blank, and thus has
productivity 0.
Consider next ﬂow charts with two arrows, labelled ‘B:—’ and ‘1 : . . . ,’ where
each of ‘—’ and ‘. . .’ may be ﬁlled in with R or L or B or 1. There are 4 · 4 = 16
such ﬂow charts, corresponding to the 4 ways of ﬁlling in ‘—’ and the 4 ways of
ﬁlling in ‘. . .’. Each such ﬂow chart corresponds to a machine that never halts, and
thus has productivity 0. The machine never halts because no matter what symbol it is
scanning, there is always an instruction for it to follow, even if it is an instruction like
‘print a blank on the (already blank) square you are scanning, and stay in the state
you are in’.
Consider ﬂow charts with one arrow. There are four of them where the arrow is
labelled ‘1: . . . ’. These all halt immediately, since the machine is started on a blank,
and there is no instruction what to do when scanning a blank. So again the productivity
is 0.
Finally, consider ﬂow charts with one arrow labelled ‘B:—’. Again there are four
of them. Three of them have productivity 0: the one ‘B:B’, which stays put, and the
two labelled ‘B:R’ and ‘B:L’, which move endlessly down the tape in one direction
or the other (touring machines). The one labelled ‘B:1’ prints a stroke and then halts,
and thus has productivity 1. Since there is thus a 1-state machine whose productivity
is 1, and every other 1-state machine has productivity 0, the most productive 1-state
machine has productivity 1.
Example 4.5. Choose any of the most productive n-state machines, and add one
more state, as in Figure 4-1.
The result is an (n + 1)-state machine of productivity n + 1. There may be (n + 1)-
state machines of even greater productivity than this, but we have established that

4.2. THE PRODUCTIVITY FUNCTION
43
Figure 4-1. Increasing productivity by 1.
the productivity of the most productive (n + 1)-state machines is at least greater by
1 than the productivity of the most productive n-state machine.
Example 4.6. We can take i = 11. To see this, plug together an n-state machine for
writing a block of n strokes (Example 3.1) with a 12-state machine for doubling the
length of a row of strokes (Example 3.2). Here ‘plugging together’ means superim-
posing the starting node of one machine on the halting node of the other: identifying
the two nodes. [Number the states of the ﬁrst machine 1 through n, and those of the
second machine (n −1) + 1 through (n −1) + 12, which is to say n through n + 11.
This is the same process we described in terms of lists of instructions rather than ﬂow
charts in our proof of Theorem 4.2.] The result is shown in Figure 4-2.
Figure 4-2. Doubling productivity.
The result is an (n + 11)-state machine with productivity 2n. Since there may
well be (n + 11)-state machines with even greater productivity, we are not entitled
to conclude that the most productive (n + 11)-state machine has productivity exactly
2n, but we are entitled to conclude that the most productive (n + 11)-state machine
has productivity at least 2n.
So much for the pieces. Now let us put them together into a proof that the function
p is not Turing computable. The proof will be by reductio ad absurdum: we deduce an
absurd conclusion from the supposition that there is a Turing machine computing p.
The ﬁrst thing we note is that if there is such a machine, call it BB, and the number
of its states is j, then we have
p(n + 2 j) ≥p(p(n))
(1)
for any n. For given a j-state machine BB computing p, we can plug together an
n-state machine writing a row of n strokes with two replicas of BB as in Figure 4-3.
Figure 4-3. Boosting productivity using the hypothetical machine BB.
The result is an (n + 2 j)-state machine of productivity p(p(n)). Now from
Example 4.5 above it follows that if a < b, then p(a) < p(b). Turning this around,

44
UNCOMPUTABILITY
if p(b) ≤p(a), we must have b ≤a. Applying this observation to (1), we have
n + 2 j ≥p(n)
(2)
for any n. Letting i be as in Example 4.6 above, we have
p(m + i) ≥2m
(3)
for any m. But applying (2) with n = m + i, we have
m + i + 2 j ≥p(m + i)
(4)
for any m. Combining (3) and (4), we have
m + i + 2 j ≥2m
(5)
for any m. Setting k = i + 2 j, we have
m + k ≥2m
(6)
for any m. But this is absurd, since clearly (6) fails for any m > k. We have proved:
4.7 Theorem. The productivity function p is Turing uncomputable.
Problems
4.1 Is there a Turing machine that, started anywhere on the tape, will eventually halt
if and only if the tape originally was not completely blank? If so, sketch the
design of such a machine; if not, brieﬂy explain why not.
4.2 Is there a Turing machine that, started anywhere on the tape, will eventually halt
if and only if the tape originally was completely blank? If so, sketch the design
of such a machine; if not, brieﬂy explain why not.
4.3 Design a copying machine of the kind described at the beginning of the proof of
theorem 4.2.
4.4 Show that if a two-place function g is Turing computable, then so is the one-
place function f given by f (x) = g(x, x). For instance, since the multiplication
function g(x, y) = xy is Turing computable, so is the square function f (x) = x2.
4.5 A universal Turing machine is a Turing machine U such that for any other Turing
machine Mn and any x, the value of the two-place function computed by U for
arguments n and x is the same as the value of the one-place function computed
by Mn for argument x. Show that if Turing’s thesis is correct, then a universal
Turing machine must exist.

5
Abacus Computability
Showing that a function is Turing computable directly, by giving a table or ﬂow chart
for a Turing machine computing the function, is rather laborious, and in the preceding
chapters we did not get beyond showing that addition and multiplication and a few
other functions are Turing computable. In this chapter we provide a less direct way of
showing functions to be Turing computable. In section 5.1 we introduce an ostensibly
more ﬂexible kind of idealized machine, an abacus machine, or simply an abacus. In
section 5.2 we show that despite the ostensible greater ﬂexibility of these machines, in
fact anything that can be computed on an abacus can be computed on a Turing machine.
In section 5.3 we use the ﬂexibility of these machines to show that a large class of
functions, including not only addition and multiplication, but exponentiation and many
other functions, are computable on a abacus. In the next chapter functions of this class
will be called recursive, so what will have been proved by the end of this chapter is that
all recursive functions are Turing computable.
5.1 Abacus Machines
We have shown addition and multiplication to be Turing-computable functions, but
notmuchbeyondthat.Actually,thesituationisevenabitworse.Itseemedappropriate,
when considering Turing machines, to deﬁne Turing computability for functions
on positive integers (excluding zero), but in fact it is customary in work on other
approaches to computability to consider functions on natural numbers (including
zero). If we are to compare the Turing approach with others, we must adapt our
deﬁnition of Turing computability to apply to natural numbers, as can be accomp-
lished (at the cost of some slight artiﬁciality) by the expedient of letting the number n
be represented by a string of n + 1 strokes, so that a single stroke now represents zero,
two strokes represent one, and so on. But with this change, the adder we presented in
the last chapter actually computes m + n + 1, rather than m + n, and would need to be
modiﬁed to compute the standard addition function; and similarly for the multiplier.
The modiﬁcations are not terribly difﬁcult to carry out, but they still leave us with
only a very few examples of interesting effectively computable functions that have
been shown to be Turing computable. In this chapter we greatly enlarge the number of
examples, but we do not do so directly, by giving tables or ﬂow charts for the relevant
Turing machines. Instead, we do so indirectly, by way of another kind of idealized
machine.
45

46
ABACUS COMPUTABILITY
Historically, the notion of Turing computability was developed before the age of
high-speed digital computers, and in fact, the theory of Turing computability formed
a not insigniﬁcant part of the theoretical background for the development of such
computers. The kinds of computers that are ordinary today are in one respect more
ﬂexible than Turing machines in that they have random-access storage. A Lambek or
abacus machine or simply abacus will be an idealized version of computer with this
‘ordinary’ feature. In contrast to a Turing machine, which stores information symbol
by symbol on squares of a one-dimensional tape along which it can move a single
step at a time, a machine of the seemingly more powerful ‘ordinary’ sort has access
to an unlimited number of registers R0, R1, R2, . . . , in each of which can be written
numbers of arbitrary size. Moreover, this sort of machine can go directly to register
Rn without inching its way, square by square, along the tape. That is, each register
has its own address (for register Rn it might be just the number n) which allows the
machine to carry out such instructions as
put the sum of the numbers in registers Rm and Rn into register Rp
which we abbreviate
[m] + [n] →p.
In general, [n] is the number in register Rn, and the number at the right of an arrow
identiﬁes the register in which the result of the operation at the left of the arrow is
to be stored. When working with such machines, it is natural to consider functions
on the natural numbers (including zero), and not just the positive integers (excluding
zero). Thus, the number [n] in register Rn at a given time may well be zero: the
register may be empty.
It should be noted that our ‘ordinary’ sort of computing machine is really quite
extraordinary in one respect: although real digital computing machines often have
random-access storage, there is always a ﬁnite upper limit on the size of the numbers
that can be stored; for example, a real machine might have the ability to store any
of the numbers 0, 1, . . . , 10 000 000 in each of its registers, but no number greater
than ten million. Thus, it is entirely possible that a function that is computable in
principle by one of our idealized machines is not computable in practice by any real
machine, simply because, for certain arguments, the computation would require more
capacious registers than any real machine possesses. (Indeed, addition is a case in
point: there is no ﬁnite bound on the sizes of the numbers one might think of adding,
and hence no ﬁnite bound on the size of the registers needed for the arguments and
the sum.) But this is in line with our objective of abstracting from technological
limitations so as to arrive at a notion of computability that is not too narrow. We seek
to show that certain functions are uncomputable in an absolute sense: uncomputable
even by our idealized machines, and therefore uncomputable by any past, present, or
future real machine.
In order to avoid discussion of electronic or mechanical details, we may imagine
the abacus machine in crude, Stone Age terms. Each register may be thought of as a
roomy, numbered box capable of holding any number of stones: none or one or two
or . . . , so that [n] will be the number of stones in box number n. The ‘machine’ can

5.1. ABACUS MACHINES
47
be thought of as operated by a little man who is capable of carrying out two sorts of
operations: adding a stone to the box of a speciﬁed number, and removing a stone
from the box of a speciﬁed number, if there are any stones there to be removed.
The table for a Turing machine is in effect a list of numbered instructions, where
‘attending to instruction q’ is called ‘being in state q’. The instructions all have the
following form:
(q)
if you are scanning a blank
then perform action a and go to r
if you are scanning a stroke
then perform action b and go to s.
Here each of the actions is one of the following four options: erase (put a blank in the
scanned square), print (put a stroke in the scanned square), move left, move right. It
is permitted that one or both of r or s should be q, so ‘go to r’ or ‘go to s’ amounts
to ‘remain with q’.
Turing machines can also be represented by ﬂow charts, in which the states or
instructions do not have to be numbered. An abacus machine program could also be
represented in a table of numbered instructions. These would each be of one or the
other of the following two forms:
(q)
add one to box m and go to r
(q)
 if box m is not empty
then subtract one from box m and go to r
if box m is empty
then go to s.
But in practice we are going to be working throughout with a ﬂow-chart represen-
tation. In this representation, the elementary operations will be symbolized as in
Figure 5-1.
Figure 5-1. Elementary operations in abacus machines.
Flow charts can be built up as in the following examples.
5.1 Example (Emptying box n). Emptying the box of a speciﬁed number n can be accom-
plished with a single instruction as follows:
 if box n is not empty
then subtract 1 from box n and stay with 1
if box n is empty
then halt.
(1)
The corresponding ﬂow chart is indicated in Figure 5-2.
In the ﬁgure, halting is indicated by an arrow leading nowhere. The block diagram also
shown in Figure 5-2 summarizes what the program shown in the ﬂow chart accomplishes,

48
ABACUS COMPUTABILITY
Figure 5-2. Emptying a box.
without indicating how it is accomplished. Such summaries are useful in showing how more
complicated programs can be put together out of simpler ones.
5.2 Example (Emptying box m into box n). The program is indicated in Figure 5-3.
Figure 5-3. Emptying one box into another.
The ﬁgure is intended for the case m ̸= n. (If m = n, the program halts—exits on the e
arrow—either at once or never, according as the box is empty or not originally.) In future we
assume, unless the contrary possibility is explicitly allowed, that when we write of boxes
m, n, p, and so on, distinct letters represent distinct boxes.
When as intended m ̸= n, the effect of the program is the same as that of carrying stones
from box m to box n until box m is empty, but there is no way of instructing the machine
or its operator to do exactly that. What the operator can do is (m−) take stones out of
box m, one at a time, and throw them on the ground (or take them to wherever unused
stones are stored), and then (n+) pick stones up off the ground (or take them from wherever
unused stones are stored) and put them, one at a time, into box n. There is no assurance that
the stones put into box n are the very same stones that were taken out of box m, but we
need no such assurance in order to be assured of the desired effect as described in the block
diagram, namely,
[m] + [n] →n:
the number of stones in box n after this move equals
the sum of the numbers in m and in n before the move
and then
0 →m:
the number of stones in box m after this move is 0.
5.3 Example (Adding box m to box n, without loss from m). To accomplish this we must
make use of an auxiliary register p, which must be empty to begin with (and will be empty
again at the end as well). Then the program is as indicated in Figure 5-4.

5.1. ABACUS MACHINES
49
Figure 5-4. Addition.
In case no assumption is made about the contents of register p at the beginning, the
operation accomplished by this program is the following:
[m] + [n] →n
[m] + [p] →m
0 →p.
Here, as always, the vertical order represents a sequence of moves, from top to bottom.
Thus, p is emptied after the other two moves are made. (The order of the ﬁrst two moves
is arbitrary: The effect would be the same if their order were reversed.)
5.4 Example (Multiplication). The numbers to be multiplied are in distinct boxes m1 and
m2; two other boxes, n and p, are empty to begin with. The product appears in box n. The
program is indicated in Figure 5-5.
Figure 5-5. Multiplication.
Instead of constructing a ﬂow chart de novo, we use the block diagram of the preceding
example a shorthand for the ﬂow chart of that example. It is then straightforward to draw
the full ﬂow chart, as in Figure 5-5(b), where the m of the preceding example is changed to
m2. The procedure is to dump [m2] stones repeatedly into box n, using box m1 as a counter:

50
ABACUS COMPUTABILITY
We remove a stone from box m1 before each dumping operation, so that when box m1 is
empty we have
[m2] + [m2] + · · · + [m2]
([m1] summands)
stones in box n.
5.5 Example (Exponentiation). Just as multiplication is repeated addition, so exponentia-
tion is repeated multiplication. The program is perfectly straightforward, once we arrange
the multiplication program of the preceding example to have [m2] · [n] →n. How that is to
be accomplished is shown in Figure 5-6.
Figure 5-6. Exponentiation.
The cumulative multiplication indicated in this abbreviated ﬂow chart is carried out in
two steps. First, use a program like Example 5.4 with a new auxiliary:
[n] · [m2] →q
0 →n.
Second, use a program like Example 5.2:
[q] + [n] →n
0 →q.
The result gives [n] · [m2] →n. Provided the boxes n, p, and q are empty initially, the
program for exponentiation has the effect
[m2][m1] →n
0 →m1
in strict analogy to the program for multiplication. (Compare the diagrams in the preceding
example and in this one.)
Structurally, the abbreviated ﬂow charts for multiplication and exponentiation
differ only in that for exponentiation we need to put a single stone in box n at the
beginning. If [m1] = 0 we have n = 1 when the program terminates (as it will
at once, without going through the multiplication routine). This corresponds to the
convention that x0 = 1 for any natural number x. But if [m1] is positive, [n] will
ﬁnally be a product of [m1] factors [m2], corresponding to repeated application of

5.2. SIMULATING ABACUS MACHINES BY TURING MACHINES
51
the rule x y+1 = x · x y, which is implemented by means of cumulative multiplication,
using box m1 as a counter.
It should now be clear that the initial restriction to two elementary sorts of acts,
n+ and n−, does not prevent us from computing fairly complex functions, including
all the functions in the series that begins sum, product, power, . . . , and where the
n + 1st member is obtained by iterating the nth member. This is considerably further
than we got with Turing machines in the preceding chapters.
5.2 Simulating Abacus Machines by Turing Machines
We now show that, despite the ostensible greater ﬂexibility of abacus machines,
all abacus-computable functions are Turing computable. Before we can describe a
method for transforming abacus ﬂow charts into equivalent Turing-machine ﬂow
charts, we need to standardize certain features of abacus computations, as we did
earlier for Turing computations with our ofﬁcial deﬁnition of Turing computability.
We must know where to place the arguments, initially, and where to look, ﬁnally,
for values. The following conventions will do as well as any, for a function f of r
arguments x1, . . . , xr:
(a) Initially, the arguments are the numbers of stones in the ﬁrst r boxes, and all other
boxes to be used in the computation are empty. Thus, x1 = [1], . . . , xr = [r],
0 = [r + 1] = [r + 2] = · · · .
(b) Finally, the value of the function is the number of stones is some previously
speciﬁed box n (which may but need not be one of the ﬁrst r). Thus,
f (x1, . . . , xr) = [n] when the computation halts, that is, when we come to an
arrow in the ﬂow chart that terminates in no node.
(c) If the computation never halts, f (x1, . . . , xr) is undeﬁned.
The computation routines for addition, multiplication, and exponentiation in the
preceding section were essentially in this form, with r = 2 in each case. They were
formulated in a general way, so as to leave open the question of just which boxes
are to contain the arguments and value. For example, in the adder we only speciﬁed
that the arguments are to be stored in distinct boxes numbered m and n, that the sum
will be found in box x, and that a third box, numbered p and initially empty, will
be used as an auxiliary in the course of the computation. But now we must specify
m, n, and p subject to the restriction that m and n must be 1 and 2, and p must be
some number greater than 2. Then we might settle on n = 1, m = 2, p = 3, to get a
particular program for addition in the standard format, as in Figure 5-7.
The standard format associates a deﬁnite function from natural numbers to nat-
ural numbers with each abacus, once we specify the number r of arguments and
the number n of the box in which the values will appear. Similarly, the standard
format for Turing-machine computations associates a deﬁnite function from natural
numbers to natural numbers (originally, from positive integers to positive integers,
but we have modiﬁed that above) with each Turing machine, once we specify the
number r of arguments. Observe that once we have speciﬁed the chart of an abacus

52
ABACUS COMPUTABILITY
Figure 5-7. Addition in standard format.
machine A in standard form, then for each register n that we might specify as hold-
ing the result of the computation there are inﬁnitely many functions Ar
n that we have
speciﬁed as computed by the abacus: one function for each possible number r of argu-
ments. Thus if A is determined by the simplest chart for addition, as in Example 5.2,
with n = 1 and m = 2, we have
A2
1(x, y) = x + y
for all natural numbers x and y, but we also have the identity function A1
1(x) = x
of one argument, and for three or more arguments we have A1
r(x1, . . . , xr) = x1 + x2.
Indeed, for r = 0 arguments we may think of A as computing a ‘function’ of a sort,
namely, the number A0
1 = 0 of strokes in box 1 when the computation halts, having
been started with all boxes (‘except the ﬁrst r’) empty. Of course, the case is entirely
parallel for Turing machines, each of which computes a function of r arguments in
standard format for each r = 0, 1, 2, . . . , the value for 0 being what we called the
productivity of the machine in the preceding chapter.
Having settled on standard formats for the two kinds of computation, we can turn
to the problem of designing a method for converting the ﬂow chart of an abacus An,
with n designated as the box in which the values will appear, into the chart of a
Turing machine that computes the same functions: for each r, the Turing machine
will compute the same function Ar
n of r arguments that the abacus computes. Our
method will specify a Turing-machine ﬂow chart that is to replace each node of type
n+ with its exiting arrow (as on the left in Figure 5-1, but without the entering arrow)
in the abacus ﬂow chart; a Turing-machine ﬂow chart that is to replace each node
of type n−with its two exiting arrows (as on the right in Figure 5-1, again without
the entering arrow); and a mop-up Turing-machine ﬂow chart that, at the end, makes
the machine erase all but the nth block of strokes on the tape and halt, scanning the
leftmost of the remaining strokes.
It is important to be clear about the relationship between boxes of the abacus and
corresponding parts of the Turing machine’s tape. For example, in computing A4
n(0,
2, 1, 0), the initial tape and box conﬁgurations would be as shown in Figure 5-8.
Boxes containing one or two or . . . stones are represented by blocks of two or three
or . . . strokes on the tape. Single blanks separate portions of the tape corresponding to
successive boxes. Empty boxes are always represented by single squares, which may
be blank (as with R5, R6, R7, . . . in the ﬁgure) or contain a single 1 (as with R1 and

5.2. SIMULATING ABACUS MACHINES BY TURING MACHINES
53
Figure 5-8. Correspondence between boxes and tape.
R4 in the ﬁgure). The 1 is mandatory if there are any strokes further to the right on the
tape, and is mandatory initially for empty argument boxes. The blank is mandatory
initially for Rr+1, Rr+2, . . . . Then at any stage of the computation we can be sure
that when in moving to the right or left we encounter two successive blanks, there
are no further strokes to be found anywhere to the right or left (as the case may be)
on the tape. The exact portion of the tape that represents a box will wax and wane with
the contents of that box as the execution of the program progresses, and will shift to
the right or left on the tape as stones are added to or removed from lower-numbered
boxes.
The ﬁrst step in our method for converting abacus ﬂow charts into equivalent
Turing-machine ﬂow charts can now be speciﬁed: replace each s+ node (consisting
of a node marked s+ and the arrow leading from it) by a copy of the s+ ﬂow chart
shown in Figure 5-9.
Figure 5-9. The s+ ﬂow chart.
The ﬁrst 2(s −1) nodes of the s+ chart simply take the Turing machine across the
ﬁrst s −1 blocks of strokes. In the course of seeking the sth block, the machine sub-
stitutes the 1-representation for the B-representation of any empty boxes encountered
along the way.
When it enters the node sa, the Turing machine has arrived at the sth block. Then
again substitutes the 1-representation for the B-representation of that box, if that box
is empty. On leaving node sb, the machine writes a stroke, moves 1 square right, and
does one thing or another (node x) depending on whether it is then scanning a blank
or a stroke.

54
ABACUS COMPUTABILITY
If it is scanning a blank, there can be no more strokes to the right, and it therefore
returns to standard position. But if it is scanning a stroke at that point, it has more
work to do before returning to standard position, for there are more blocks of strokes
to be dealt with, to the right on the tape. These must be shifted one square rightwards,
by erasing the ﬁrst 1 in each block and ﬁlling the blank to the block’s right with a
stroke—continuing this routine until it ﬁnds a blank to the right of the last blank it
has replaced by a stroke. At that point there can be no further strokes to the right, and
the machine returns to standard position.
Note that node 1a is needed in case the number r of arguments is 0: in case the
‘function’ that the abacus computes is a number A0
n. Note, too, that the ﬁrst s −1
pairs of nodes (with their efferent arrows) are identical, while the last pair is different
only in that the arrow from node sb to the right is labelled B:1 instead of B:R. What
the general s+ ﬂow chart looks like in the case s = 1 is shown in Figure 5-10.
Figure 5-10. The special case s = 1.
The second step in our method of converting abacus ﬂow charts into equivalent
Turing machine ﬂow charts can now be speciﬁed: replace each s−node (with the
two arrows leading from it) by a copy of an s−ﬂow chart having the general pattern
shown in Figure 5-11.
Readers may wish to try to ﬁll in the details of the design for themselves, as an
exercise. (Our design will be given later.) When the ﬁrst and second steps of the
method have been carried out, the abacus ﬂow chart will have been converted into
something that is not quite the ﬂow chart of a Turing machine that computes the same
function that the abacus does. The chart will (probably) fall short in two respects,
one major and one minor. The minor respect is that if the abacus ever halts, there
must be one or more ‘loose’ arrows in the chart: arrows that terminate in no node.
This is simply because that is how halting is represented in abacus ﬂow charts: by an
arrow leading nowhere. But in Turing-machine ﬂow charts, halting is represented in
a different way, by a node with no arrows leading from it. The major respect is that
in computing Ar
n(x1, . . . , xr) the Turing machine would halt scanning the leftmost 1
on the tape, but the value of the function would be represented by the nth block of
strokes on the tape. Even if n = 1, we cannot depend on there being no strokes on the
tape after the ﬁrst block, so our method requires one more step.
The third step: after completing the ﬁrst two steps, redraw all loose arrows so they
terminate in the input node of a mop-up chart, which makes the machine (which will
be scanning the leftmost 1 on the tape at the beginning of this routine) erase all but
the ﬁrst block of strokes if n = 1, and halt scanning the leftmost of the remaining
strokes. But if n ̸= 1, it erases everything on the tape except for both the leftmost

5.2. SIMULATING ABACUS MACHINES BY TURING MACHINES
55
Figure 5-11. Abbreviated s−ﬂow chart.
1 on the tape and the nth block, repositions all strokes but the rightmost in the nth
block immediately to the right of the leftmost 1, erases the rightmost 1, and then halts
scanning the leftmost 1. In both cases, the effect is to place the leftmost 1 in the block
representing the value just where the leftmost 1 was initially. Again readers may wish
to try to ﬁll in the details of the design for themselves, as an exercise. (Our design
will be given shortly.)
The proof that all abacus-computable functions are Turing computable is now
ﬁnished, except for the two steps that we have invited readers to try as exercises. For
the sake of completeness, we now present our own solutions to these exercises: our
own designs for the second and third stages of the construction reducing an abacus
computation to a Turing computation.
For the second stage, we describe what goes into the boxes in Figure 5-11. The
top block of the diagram contains a chart identical with the material from node 1a to
sa (inclusive) of the s+ ﬂow chart. The arrow labelled 1:R from the bottom of this
block corresponds to the one that goes right from node sa in the s+ ﬂow chart.
The ‘Is [s] = 0?’ box contains nothing but the shafts of the two emergent arrows:
They originate in the node shown at the top of that block.

56
ABACUS COMPUTABILITY
Figure 5-12. Detail of the s−ﬂow chart.
The ‘Return to standard position’ blocks contain replicas of the material to the
right of node x in the s+ chart: The B:L arrows entering those boxes correspond to
the B:L arrow from node x.
The only novelty is in the remaining block: ‘Find and erase the . . . ’ That block
contains the chart shown in Figure 5-12.
For the third stage, the mop-up chart, for n ̸= 1, is shown in Figure 5-13.
Figure 5-13. Mop-up chart.
We have proved:
5.6 Theorem. Every abacus-computable function is Turing computable.
We know from the preceding chapter some examples of functions that are not
Turing computable. By the foregoing theorem, these functions are also not abacus
computable. It is also possible to prove directly the existence of functions that are not
abacus computable, by arguments parallel to those used for Turing computability in
the preceding chapter.

5.3. THE SCOPE OF ABACUS COMPUTABILITY
57
5.3 The Scope of Abacus Computability
Wenowturnfromshowingthatparticularfunctionsareabacuscomputabletoshowing
thatcertainprocessesfordeﬁningnewfunctionsfromold,whenappliedtooldabacus-
computable functions, produce new abacus-computable functions. (These processes
will be explained and examined in more detail in the next chapter, and readers may
wish to defer reading this section until after that chapter.)
Now we initially indicated that to compute a function of r arguments on an abacus,
we must specify r registers or boxes in which the arguments are to be stored initially
(represented by piles of rocks) and we must specify a register or box in which the
value of the function is to appear (represented by a pile of rocks) at the end of
the computation. To facilitate comparison with computations by Turing machines in
standard form, we then insisted that the input or arguments were to be placed in the
ﬁrst r registers, but left it open in which register n the output or value would appear:
it was not necessary to be more speciﬁc, because the simulation of the operations
of an abacus by a Turing machine could be carried out wherever we let the output
appear. For the purposes of this section, we are therefore free now to insist that the
output register n, which we have heretofore left unspeciﬁed, be speciﬁcally register
r + 1. We also wish to insist that at the end of the computation the original arguments
should be back in registers 1 through r. In the examples considered earlier this last
condition was not met, but those examples are easily modiﬁed to meet it. We give
some further, trivial examples here, where all our speciﬁcations are exactly met.
5.7 Example (Zero, successor, identity). First consider the zero function z, the one-place
function that takes the value 0 for all arguments. It is computed by the vacuous program:
box 2 is empty anyway.
Next consider the successor function s, the one-place function that takes any natural
number x to the next larger natural number x + 1. It is computed by modifying the program
in Example 5.3, as shown in Figure 5-14.
Figure 5-14. Three basic functions.
Initially and ﬁnally, [1] = x; initially [2] = 0; ﬁnally, [2] = s(x). Finally consider identity
function idm
n , the n-place function whose value for n arguments x1, . . . , xn is the mth one
among them, xm. It is computed by the program of the same Example 5.3. Initially and
ﬁnally, [1] = x1, . . . , [n] = xn; initially, [n + 1] = 0; ﬁnally [n + 1] = xm.

58
ABACUS COMPUTABILITY
Threedifferentprocessesfordeﬁningnewfunctionsfromoldcanbeusedtoexpand
our initial list of examples. A ﬁrst process is composition, also called substitution.
Suppose we have two 3-place functions g1 and g2, and a 2-place function f . The
function h obtained from them by composition is the 3-place function given by
h(x1, x2, x3) = f (g1(x1, x2, x3), g2(x1, x2, x3)).
Suppose g1 and g2 and f are abacus computable according to our speciﬁcations, and
we are given programs for them.
↓
↓
↓
f ([1], [2]) →3
g1([1], [2], [3]) →4
g2([1], [2], [3]) →4
↓
↓
↓
We want to ﬁnd a program for h, to show it is abacus computable:
↓
h([1], [2], [3]) →4
.
↓
The thing is perfectly straightforward: It is a matter of shuttling the results of
subcomputations around so as to be in the right boxes at the right times.
First,weidentifyﬁveregisters,noneofwhichareusedinanyofthegivenprograms.
Let us call these registers p1, p2, q1, q2, and q3. They will be used for temporary
storage. In the single program which we want to construct, the 3 arguments are stored
initially in boxes 1, 2, and 3; all other boxes are empty initially; and at the end, we
want the n arguments back in boxes 1, 2, 3, and want the value f (g1([1], [2], [3]),
g2([1], [2], [3])) in box number 4. To arrange that, all we need are the three given
programs, plus the program of Example 5.2 for emptying one box into another.
We simply compute g1([1], [2], [3]) and store the result in box p1 (which ﬁgures
in none of the given programs, remember); then compute g2([1], [2], [3]) and store
the result in box p2; then store the arguments in boxes 1, 2, and 3 in boxes q1, q2,
and q3, emptying boxes 1 through 4; then get the results of the computations of g1
and g2 out of boxes p1 and p2 where they have been stored, emptying them into
boxes 1 and 2; then compute f ([1], [2]) = f [g1(original arguments), g2(original
arguments)], getting the result in box 3; and ﬁnally, tidy up, moving the overall result
of the computation from box 3 to box 4, emptying box 3 in the process, and reﬁlling
boxes 1 through 3 with the original arguments of the overall computation, which were
stored in boxes q1, q2, and q3. Now everything is as it should be. The structure of the
ﬂow chart is shown in Figure 5-15.
Another process, called (primitive) recursion, is what is involved in deﬁning mul-
tiplication as repeated addition, exponentiation as repeated multiplication, and so on.
Suppose we have a 1-place functions f and a 3-place function g. The function h
obtained from them by (primitive) recursion is the 2-place function h given by
h(x, 0) = f (x)
h(x, y + 1) = g(x, y, h(x, y)).

5.3. THE SCOPE OF ABACUS COMPUTABILITY
59
Figure 5-15. Composition.
For instance, if f (x) = x and g(x, y, z) = z + 1, then
h(x, 0) = f (x)
= x
= x + 0
h(x, 1) = g(x, 0, x)
= x + 1
h(x, 2) = g(x, 1, x + 1) = (x + 1) + 1 = x + 2
and in general h(x, y) = x + y. Suppose f and g are abacus computable according
to our speciﬁcations, and we are given programs for them:
↓
↓
f ([1]) →2
g1([1], [2], [3]) →4
.
↓
↓
We want to ﬁnd a program for h, to show it is abacus computable
↓
h([1], [2]) →3
.
↓
The thing is easily done, as in Figure 5-16.

60
ABACUS COMPUTABILITY
Figure 5-16. Recursion.
Initially, [1] = x, [2] = y, and [3] = [4] = · · · = 0. We use a register number p that
is not used in the f and g programs as a counter. We put y into it at the beginning, and
after each stage of the computation we see whether [p] = 0. If so, the computation
is essentially ﬁnished; if not, we subtract 1 from [p] and go through another stage.
In the ﬁrst three steps we calculate f (x) and see whether entry y was 0. If so, the
ﬁrst of the pair of equations for h is operative: h(x, y) = h(x, 0) = f (x), and the
computation is ﬁnished, with the result in box 3, as required. If not, the second of
the pair of equations for h is operative, and we successively compute h(x, 1),
h(x, 2), . . . (see the cyle in Figure 5-16) until the counter (box p) is empty. At that
point the computation is ﬁnished, with h(x, y) in box 3, as required.
A ﬁnal process is minimization. Suppose we have a 2-place function f ; then we
can deﬁne a 1-place function h as follows. If f (x, 0), . . . , f (x, i −1) are all deﬁned
and ̸= 0, and f (x, i) = 0, then h(x) = i. If there is no i with these properties, either
because for some i the values f (x, 0), . . . , f (x, j −1) are all deﬁned and ̸= 0 but
f (x, j) is not deﬁned, or because for all i the value f (x, i) is deﬁned and ̸= 0,
then h(x) is undeﬁned. The function h is called the function obtained from f by
minimization. If f is abacus computable, so is h, with a ﬂow chart as in Figure 5-17.
Initially, box 2 is empty, so that if f (x, 1) = 0, the program will halt with the
correct answer, h(x) = 0, in box 2. (Box 3 will be empty.) Otherwise, box 3 will be

PROBLEMS
61
Figure 5-17. Minimization.
emptied and a single rock place in box 2, preparatory to the computation of f (x, 1).
If this value is 0, the program halts, with the correct value, h(x) = 1, in box 2.
Otherwise, another rock is placed in box 2, and the procedure continues until such time
(if any) as we have a number y of rocks in box 2 that is enough to make f (x, y) = 0.
The extensive class of functions obtainable from the trivial functions considered
in the example at the beginning of this section by the kinds of processes considered
in the rest of this section will be studied in the next chapter, where they will be given
the name recursive functions. At this point we know the following:
5.8 Theorem. All recursive functions are abacus computable (and hence Turing
computable).
So as we produce more examples of such functions, we are going to be producing
more evidence for Turing’s thesis.
Problems
5.1 Design an abacus machine for computing the difference function .−deﬁned by
letting x .−y = x −y if y < x, and = 0 otherwise.
5.2 The signum function sg is deﬁned by letting sg(x) = 1 if x > 0, and = 0
otherwise. Give a direct proof that sg is abacus computable by designing an
abacus machine to compute it.
5.3 Give an indirect proof that sg is abacus computable by showing that sg is
obtainable by composition from functions known to be abacus computable.
5.4 Show (directly by designing an appropriate abacus machine, or indirectly) that
the function f deﬁned by letting f (x, y) = 1 if x < y, and = 0 otherwise, is
abacus computable.
5.5 The quotient and the remainder when the positive integer x is divided by the
positive integer y are the unique natural numbers q and r such that x = qy + r
and 0 ≤r < y. Let the functions quo and rem be deﬁned as follows: rem(x, y) =
the remainder on dividing x by y if y ̸= 0, and = x if y = 0; quo(x, y) = the
quotient on dividing x by y if y ̸= 0, and = 0 if y = 0. Design an abacus machine
for computing the remainder function rem.
5.6 Write an abacus-machine ﬂow chart for computing the quotient function quo
of the preceding problem.

62
ABACUS COMPUTABILITY
5.7 Show that for any k there is a Turing machine that, when started on the leftmost
1 on a tape containing k blocks of 1s separated by single blanks, halts on the
leftmost 1 on a tape that is exactly the same as the starting tape, except that
everything has been moved one square to the right, without the machine in the
course of its operations ever having moved left of the square on which it was
started.
5.8 Review the operations of a Turing machine simulating some give abacus ma-
chine according to the method of this chapter. What is the furthest to the left of
the square on which it is started that such a machine can ever go in the course
of its operations?
5.9 Show that any abacus-computable function is computable by a Turing machine
that never moves left of the square on which it is started.
5.10 Describe a reasonable way of coding abacus machines by natural numbers.
5.11 Given a reasonable way of coding abacus machines by natural numbers, let
d(x) = 1 if the one-place function computed by abacus number x is deﬁned and
has value 0 for argument x, and d(x) = 0 otherwise. Show that this function is
not abacus computable.

6
Recursive Functions
The intuitive notion of an effectively computable function is the notion of a function for
which there are deﬁnite, explicit rules, following which one could in principle compute its
value for any given arguments. This chapter studies an extensive class of effectively com-
putable functions, the recursively computable, or simply recursive, functions. According
to Church’s thesis, these are in fact all the effectively computable functions. Evidence
for Church’s thesis will be developed in this chapter by accumulating examples of ef-
fectively computable functions that turn out to be recursive. The subclass of primitive
recursive functions is introduced in section 6.1, and the full class of recursive functions
in section 6.2. The next chapter contains further examples. The discussion of recursive
computability in this chapter and the next is entirely independent of the discussion of
Turing and abacus computability in the preceding three chapters, but in the chapter
after next the three notions of computability will be proved equivalent to each other.
6.1 Primitive Recursive Functions
Intuitively, the notion of an effectively computable function f from natural numbers
to natural numbers is the notion of a function for which there is a ﬁnite list of
instructions that in principle make it possible to determine the value f (x1, . . . , xn)
for any arguments x1, . . . , xn. The instructions must be so deﬁnite and explicit that
they require no external sources of information and no ingenuity to execute. But the
determination of the value given the arguments need only be possible in principle,
disregarding practical considerations of time, expense, and the like: the notion of
effective computability is an idealized one.
For purposes of computation, the natural numbers that are the arguments and values
of the function must be presented in some system of numerals or other, though the
class of functions that is effectively computable will not be affected by the choice
of system of numerals. (This is because conversion from one system of numerals
to another is itself an effective process that can be carried out according to deﬁnite,
explicit rules.) Of course, in practice some systems of numerals are easier to work with
than others, but that is irrelevant to the idealized notion of effective computability.
Forpresentpurposesweadoptavariantoftheprimevalmonadicortallynotation,in
which a positive integer n is represented by n strokes. The variation is needed because
we want to consider not just positive integers (excluding zero) but the natural numbers
63

64
RECURSIVE FUNCTIONS
(including zero). We adopt the system in which the number zero is represented by
the cipher 0, and a natural number n > 0 is represented by the cipher 0 followed by
a sequence of n little raised strokes or accents. Thus the numeral for one is 0′, the
numeral for two is 0′′, and so on.
Two functions that are extremely easy to compute in this notation are the zero
function, whose value z(x) is the same, namely zero, for any argument x, and the
successor function s(x), whose value for any number x is the next larger number. In
our special notation we write:
z(0) = 0
z(0′) = 0
z(0′′) = 0
· · ·
s(0) = 0′
s(0′) = 0′′
s(0′′) = 0′′′
· · · ·
To compute the zero function, given any any argument, we simply ignore the argument
and write down the symbol 0. To compute the successor function in our special
notation, given a number written in that notation, we just add one more accent at the
right.
Some other functions it is easy to compute (in any notation) are the identity
functions. We have earlier encountered also the identity function of one argument,
id or more fully id1
1, which assigns to each natural number as argument that same
number as value:
id1
1(x) = x.
There are two identity functions of two arguments: id2
1 and id2
2. For any pair of
natural numbers as arguments, these pick out the ﬁrst and second, respectively, as
values:
id2
1(x, y) = x
id2
2(x, y) = y.
In general, for each positive integer n, there are n identity functions of n arguments,
which pick out the ﬁrst, second, . . . , and nth of the arguments:
idn
i (x1, . . . , xi, . . . , xn) = xi.
Identity functions are also called projection functions. [In terms of analytic geometry,
id2
1(x, y) and id2
2(x, y) are the projections x and y of the point (x, y) to the X-axis
and to the Y-axis respectively.]
The foregoing functions—zero, successor, and the various identity functions—are
together called the basic functions. They can be, so to speak, computed in one step,
at least on one way of counting steps.
The stock of effectively computable functions can be enlarged by applying certain
processes for deﬁning new functions from old. A ﬁrst sort of operation, composi-
tion, is familiar and straightforward. If f is a function of m arguments and each of
g1, . . . , gm is a function of n arguments, then the function obtained by composition
from f , g1, . . . , gm is the function h where we have
h(x1, . . . , xn) = f (g1(x1, . . . , xn), . . . , gm(x1, . . . , xn))
(Cn)

6.1. PRIMITIVE RECURSIVE FUNCTIONS
65
One might indicate this in shorthand:
h = Cn[ f, g1, . . . , gm].
Composition is also called substitution.
Clearly, if the functions gi are all effectively computable and the function f is
effectively computable, then so is the function h. The number of steps needed to
compute h(x1, . . . , xn) will be the sum of the number of steps needed to compute
y1 = g1(x1, . . . , xn), the number needed to compute y2 = g2(x1, . . . , xn), and so on,
plus at the end the number of steps needed to compute f (y1, . . . , ym).
6.1 Example (Constant functions). For any natural number n, let the constant function
constn be deﬁned by constn(x) = n for all x. Then for each n, constn can be obtained from
the basic functions by ﬁnitely many applications of composition. For, const0 is just the zero
function z, and Cn[s, z] is the function h with h(x) = s(z(x)) = s(0) = 0′ = 1 = const1(x)
for all x, so const1 = Cn[s, z]. (Actually, such notations as Cn[s, z] are genuine function
symbols, belonging to the same grammatical category as h, and we could have simply
written Cn[s, z](x) = s(z(x)) here rather than the more longwinded ‘if h = Cn[s, z], then
h(x) = z(x)′’.) Similarly const2 = Cn[s, const1], and generally constn+1 = Cn[s, constn].
Theexamplesofeffectivelycomputablefunctionswehavehadsofarareadmittedly
not very exciting. More interesting examples are obtainable using a different process
for deﬁning new functions from old, a process that can be used to deﬁne addition
in terms of successor, multiplication in terms of addition, exponentiation in terms of
multiplication, and so on. By way of introduction, consider addition. The rules for
computing this function in our special notation can be stated very concisely in two
equations as follows:
x + 0 = x
x + y′ = (x + y)′.
To see how these equations enable us to compute sums consider adding 2 = 0′′
and 3 = 0′′′. The equations tell us:
0′′ + 0′′′ = (0′′ + 0′′)′
by 2nd equation
with x = 0′′ and y = 0′′
0′′ + 0′′ = (0′′ + 0′)′
by 2nd equation
with x = 0′′ and y = 0′
0′′ + 0′ = (0′′ + 0)′
by 2nd equation
with x = 0′′ and y = 0
0′′ + 0
= 0′′
by 1st equation
with x = 0′′.
Combining, we have the following:
0′′ + 0′′′ = (0′′ + 0′′)′
= (0′′ + 0′)′′
= (0′′ + 0)′′′
= 0′′′′′.
So the sum is 0′′′′′ = 5. Thus we use the second equation to reduce the problem of
computing x + y to that of computing x + z for smaller and smaller z, until we arrive
at z = 0, when the ﬁrst equation tells us directly how to compute x + 0.

66
RECURSIVE FUNCTIONS
Similarly, for multiplication we have the rules or equations
x · 0 = 0
x · y′ = x + (x · y)
which enable us to reduce the computation of a product to the computation of sums,
which we know how to compute:
0′′ · 0′′′ = 0′′ + (0′′ · 0′′)
= 0′′ + (0′′ + (0′′ · 0′))
= 0′′ + (0′′ + (0′′ + (0′′ · 0)))
= 0′′ + (0′′ + (0′′ + 0))
= 0′′ + (0′′ + 0′′)
after which we would carry out the computation of the sum in the last line in the way
indicated above, and obtain 0′′′′′′.
Now addition and multiplication are just the ﬁrst two of a series of arithmetic
operations, all of which are effectively computable. The next item in the series is ex-
ponentiation. Just as multiplication is repeated addition, so exponentiation is repeated
multiplication. To compute x y, that is, to raise x to the power y, multiply together
y xs as follows:
x · x · x· · · · ·x
(a row of y xs).
Conventionally, a product of no factors is taken to be 1, so we have the equation
x0 = 0′.
For higher powers we have
x1 = x
x2 = x · x
...
x y = x · x· · · · ·x
(a row of y xs)
x y+1 = x · x· · · · ·x · x = x · x y
(a row of y + 1 xs).
So we have the equation
x y′ = x · x y.
Again we have two equations, and these enable us to reduce the computation of a
power to the computation of products, which we know how to do.
Evidently the next item in the series, super-exponentiation, would be deﬁned as
follows:
x x xx...
(a stack of y xs).
The alternative notation x ↑y may be used for exponentiation to avoid piling up of
superscripts. In this notation the deﬁnition would be written as follows:
x ↑x ↑x ↑. . . ↑x
(a row of y xs).

6.1. PRIMITIVE RECURSIVE FUNCTIONS
67
Actually, we need to indicate the grouping here. It is to the right, like this:
x ↑(x ↑(x ↑. . . ↑x . . .))
and not to the left, like this:
(. . . ((x ↑x) ↑x) ↑. . .) ↑x.
For it makes a difference: 3 ↑(3 ↑3) = 3 ↑(27) = 7 625 597 484 987; while (3 ↑3) ↑
3 = 27 ↑3 =19 683. Writing x ⇑y for the super-exponential, the equations would be
x ⇑0 = 0′
x ⇑y′ = x ↑(x ⇑y).
The next item in the series, super-duper-exponentiation, is analogously deﬁned, and
so on.
The process for deﬁning new functions from old at work in these cases is called
(primitive) recursion. As our ofﬁcial format for this process we take the following:
h(x, 0) = f (x), h(x, y′) = g(x, y, h(x, y))
(Pr).
Where the boxed equations—called the recursion equations for the function h—
hold, h is said to be deﬁnable by (primitive) recursion from the functions f and g. In
shorthand,
h = Pr[ f, g].
Functions obtainable from the basic functions by composition and recursion are called
primitive recursive.
All such functions are effectively computable. For if f and g are effectively com-
putable functions, then h is an effectively computable function. The number of steps
needed to compute h(x, y) will be the sum of the number of steps needed to com-
pute z0 = f (x) = h(x, 0), the number needed to compute z1 = g(x, 0, z0) = h(x, 1),
the number needed to compute z2 = g(x, 1, z1) = h(x, 2), and so on up to zy =
g(x, y −1, zy−1) = h(x, y).
The deﬁnitions of sum, product, and power we gave above are approximately in
our ofﬁcial boxed format. [The main difference is that the boxed format allows one,
in computing h(x, y′), to apply a function taking x, y, and h(x, y) as arguments. In the
examples of sum, product, and power, we never needed to use y as an argument.] By
fussing over the deﬁnitions we gave above, we can put them exactly into the format
(Pr), thus showing addition and multiplication to be primitive recursive.
6.2 Example (The addition or sum function). We start with the deﬁnition given by the
equations we had above,
x + 0 = x
x + y′ = (x + y)′.
As a step toward reducing this to the boxed format (Pr) for recursion, we replace the ordinary
plus sign, written between the arguments, by a sign written out front:
sum(x, 0) = x
sum(x, y′) = sum(x, y)′.

68
RECURSIVE FUNCTIONS
To put these equations in the boxed format (Pr), we must ﬁnd functions f and g for which
we have
f (x) = x
g(x, y, —) = s(—)
for all natural numbers x, y, and —. Such functions lie ready to hand: f = id1
1, g = Cn
[s, id3
3]. In the boxed format we have
sum(x, 0) = id1
1(x)
sum(x, s(y)) = Cn

s, id3
3

(x, y, sum(x, y))
and in shorthand we have
sum = Pr

id1
1, Cn

s, id3
3

.
6.3 Example (The multiplication or product function). We claim prod = Pr[z, Cn[sum,
id3
1, id3
3]]. To verify this claim we relate it to the boxed formats (Cn) and (Pr). In terms of
(Pr) the claim is that the equations
prod(x, 0) = z(x)
prod(x, s(y)) = g(x, y, prod(x, y))
hold for all natural numbers x and y, where [setting h = g, f = sum, g1 = id3
1, g2 = id3
3 in
the boxed (Cn) format] we have
g(x1, x2, x3) = Cn

sum, id3
1, id3
3

(x1, x2, x3)
= sum

id3
1(x1, x2, x3), id3
3(x1, x2, x3)

= x1 + x3
for all natural numbers x1, x2, x3. Overall, then, the claim is that the equations
prod(x, 0) = z(x)
prod(x, s(y)) = x + prod(x, y)
hold for all natural numbers x and y, which is true:
x · 0 = 0
x · y′ = x + x · y.
Our rigid format for recursion serves for functions of two arguments such as sum
and product, but we are sometimes going to wish to use such a scheme to deﬁne
functions of a single argument, and functions of more than two arguments. Where
there are three or more arguments x1, . . . , xn, y instead of just the two x, y that appear
in (Pr), the modiﬁcation is achieved by viewing each of the ﬁve occurrences of x in
the boxed format as shorthand for x1, . . . , xn. Thus with n = 2 the format is
h(x1, x2, 0) = f (x1, x2)
h(x1, x2, s(y)) = g(x1, x2, y, h(x1, x2, y)).
6.4 Example (The factorial function). The factorial x! for positive x is the product
1 · 2 · 3· · · · ·x of all the positive integers up to and including x, and by convention 0! = 1.
Thus we have
0! = 1
y′! = y! · y′.

6.1. PRIMITIVE RECURSIVE FUNCTIONS
69
To show this function is recursive we would seem to need a version of the format for
recursion with n = 0. Actually, however, we can simply deﬁne a two-argument function
with a dummy argument, and then get rid of the dummy argument afterwards by composing
with an identity function. For example, in the case of the factorial function we can deﬁne
dummyfac(x, 0) = const1(x)
dummyfac(x, y′) = dummyfac(x, y) · y′
sothatdummyfac(x, y) = y!regardlessofthevalueof x,andthendeﬁnefac(y)=dummyfac
(y, y). More formally,
fac = Cn

Pr

const1, Cn

prod, id3
3, Cn

s, id3
2

, id, id

.
(We leave to the reader the veriﬁcation of this fact, as well as the conversions of informal-
style deﬁnitions into formal-style deﬁnitions in subsequent examples.)
The example of the factorial function can be generalized.
6.5 Proposition. Let f be a primitive recursive function. Then the functions
g(x, y) = f (x, 0) + f (x, 1) + · · · + f (x, y) =
y

i=0
f (x, i)
h(x, y) = f (x, 0) · f (x, 1)· · · · · f (x, y) =
y
i=0
f (x, i)
are primitive recursive.
Proof: We have for the g the recursion equations
g(x, 0) = f (x, 0)
g(x, y′) = g(x, y) + f (x, y′)
and similarly for h.
Readers may wish, in the further examples to follow, to try to ﬁnd deﬁnitions of
their own before reading ours; and for this reason we give the description of the
functions ﬁrst, and our deﬁnitions of them (in informal style) afterwards.
6.6 Example. The exponential or power function.
6.7 Example (The (modiﬁed) predecessor function). Deﬁne pred (x) to be the predecessor
x −1 of x for x > 0, and let pred(0) = 0 by convention. Then the function pred is primitive
recursive.
6.8 Example (The (modiﬁed) difference function). Deﬁne x .−y to be the difference x −y
if x ≥y, and let x .−y = 0 by convention otherwise. Then the function .−is primitive
recursive.
6.9 Example (The signum functions). Deﬁne sg(0) = 0, and sg(x) = 1 if x > 0, and deﬁne
sg(0) = 1 and sg(x) = 0 if x > 0. Then sg and sg are primitive recursive.

70
RECURSIVE FUNCTIONS
Proofs
Example 6.6. x ↑0 = 1, x ↑s(y) = x · (x ↑y), or more formally,
exp = Pr

Cn[s, z], Cn

prod, id3
1, id3
3

.
Example 6.7. pred(0) = 0, pred(y′) = y.
Example 6.8. x .−0 = x, x .−y′ = pred(x .−y).
Example 6.9. sg(y) = 1 .−(1 .−y), sg(y) = 1 .−y.
6.2 Minimization
We now introduce one further process for deﬁning new functions from old, which
can take us beyond primitive recursive functions, and indeed can take us beyond
total functions to partial functions. Intuitively, we consider a partial function f to be
effectively computable if a list of deﬁnite, explicit instructions can be given, following
which one will, in the case they are applied to any x in the domain of f , arrive after
a ﬁnite number of steps at the value f (x), but following which one will, in the case
they are applied to any x not in the domain of f , go on forever without arriving at
any result. This notion applies also to two- and many-place functions.
Now the new process we want to consider is this. Given a function f of n + 1
arguments, the operation of minimization yields a total or partial function h of n
arguments as follows:
Mn[ f ](x1, . . . , xn) =
⎧
⎨
⎩
y
if f (x1, . . . , xn, y) = 0, and for all t < y
f (x1, . . . , xn, t) is deﬁned and ̸= 0
undeﬁned
if there is no such y.
If h = Mn[ f ] and f is an effectively computable total or partial function, then
h also will be such a function. For writing x for x1, . . . , xn, we compute h(x) by
successively computing f (x, 0), f (x, 1), f (x, 2), and so on, stopping if and when
we reach a y with f (x, y) = 0. If x is in the domain of h, there will be such a y, and
the number of steps needed to compute h(x) will be the sum of the number of steps
needed to compute f (x, 0), the number of steps needed to compute f (x, 1), and so
on, up through the number of steps needed to compute f (x, y) = 0. If x is not in the
domain of h, this may be for either of two reasons. On the one hand, it may be that
all of f (x, 0), f (x, 1), f (x, 2), . . . are deﬁned, but they are all nonzero. On the other
hand, it may be that for some i, all of f (x, 0), f (x, 1), . . . , f (x, i −1) are deﬁned
and nonzero, but f (x, i) is undeﬁned. In either case, the attempt to compute h(x) will
involve one in a process that goes on forever without producing a result.
In case f is a total function, we do not have to worry about the second of the two
ways in which Mn[ f ] may fail to be deﬁned, and the above deﬁnition boils down to
the following simpler form.
Mn[ f ](x1, . . . , xn) =
⎧
⎨
⎩
the smallest y for which
f (x1, . . . , xn, y) = 0
if such a y exists
undeﬁned
otherwise.

PROBLEMS
71
The total function f is called regular if for every x1, . . . , xn there is a y such that
f (x1, . . . , xn, y) = 0. In case f is a regular function, Mn[ f ] will be a total function.
In fact, if f is a total function, Mn[ f ] will be total if and only if f is regular.
For example, the product function is regular, since for every x, x · 0 = 0; and
Mn[prod] is simply the zero function. But the sum function is not regular, since
x + y = 0 only in case x = y = 0; and Mn[sum] is the function that is deﬁned only
for 0, for which it takes the value 0, and undeﬁned for all x > 0.
The functions that can be obtained from the basic functions z, s, idn
i by the pro-
cesses Cn, Pr, and Mn are called the recursive (total or partial) functions. (In the
literature, ‘recursive function’ is often used to mean more speciﬁcally ‘recursive
total function’, and ‘partial recursive function’ is then used to mean ‘recursive total
or partial function’.) As we have observed along the way, recursive functions are all
effectively computable.
The hypothesis that, conversely, all effectively computable total functions are re-
cursive is known as Church’s thesis (the hypothesis that all effectively computable
partial functions are recursive being known as the extended version of Church’s the-
sis). The interest of Church’s thesis derives largely from the following fact. Later
chapters will show that some particular functions of great interest in logic and mathe-
matics are nonrecursive. In order to infer from such a theoretical result the conclusion
that such functions are not effectively computable (from which may be inferred the
practical advice that logicians and mathematicians would be wasting their time look-
ing for a set of instructions to compute the function), we need assurance that Church’s
thesis is correct.
At present Church’s thesis is, for us, simply an hypothesis. It has been made some-
what plausible to the extent that we have shown a signiﬁcant number of effectively
computable functions to be recursive, but one can hardly on the basis of just these
few examples be assured of its correctness. More evidence of the correctness of the
thesis will accumulate as we consider more examples in the next two chapters.
Before turning to examples, it may be well to mention that the thesis that every ef-
fectively computable total function is primitive recursive would simply be erroneous.
Examples of recursive total functions that are not primitive recursive are described
in the next chapter.
Problems
6.1 Let f be a two-place recursive total function. Show that the following functions
are also recursive:
(a) g(x, y) = f (y, x)
(b) h(x) = f (x, x)
(c) k17(x) = f (17, x) and k17(x) = f (x, 17).
6.2 Let J0(a, b) be the function coding pairs of positive integers by positive integers
that was called J in Example 1.2, and from now on use the name J for the
corresponding function coding pairs of natural numbers by natural numbers, so
that J(a, b) = J0(a + 1, b + 1) −1. Show that J is primitive recursive.

72
RECURSIVE FUNCTIONS
6.3 Show that the following functions are primitive recursive:
(a) the absolute difference |x −y|, deﬁned to be x −y if y < x, and y −x
otherwise.
(b) the order characteristic, χ≤(x, y), deﬁned to be 1 if x ≤y, and 0 otherwise.
(c) the maximum max(x, y), deﬁned to be the larger of x and y.
6.4 Show that the following functions are primitive recursive:
(a) c(x, y, z) = 1 if yz = x, and 0 otherwise.
(b) d(x, y, z) = 1 if J(y, z) = x, and 0 otherwise.
6.5 Deﬁne K(n) and L(n) as the ﬁrst and second entries of the pair coded (under the
coding J oftheprecedingproblems)bythenumber n,so that J(K(n), L(n)) = n.
Show that the functions K and L are primitive recursive.
6.6 An alternative coding of pairs of numbers by numbers was considered in
Example 1.2, based on the fact that every natural number n can be written
in one and only one way as 1 less than a power of 2 times an odd number,
n = 2k(n)(2l(n) .−1) .−1. Show that the functions k and l are primitive recursive.
6.7 Devise some reasonable way of assigning code numbers to recursive functions.
6.8 Given a reasonable way of coding recursive functions by natural numbers, let
d(x) = 1 if the one-place recursive function with code number x is deﬁned and
has value 0 for argument x, and d(x) = 0 otherwise. Show that this function is
not recursive.
6.9 Let h(x, y) = 1 if the one-place recursive function with code number x is deﬁned
forargument y,andh(x, y) = 0otherwise.Showthatthisfunctionisnotrecursive.

7
Recursive Sets and Relations
In the preceding chapter we introduced the classes of primitive recursive and recursive
functions. In this chapter we introduce the related notions of primitive recursive and
recursive sets and relations, which help provide many more examples of primitive recur-
sive and recursive functions. The basic notions are developed in section 7.1. Section 7.2
introduces the related notion of a semirecursive set or relation. The optional section 7.3
presents examples of recursive total functions that are not primitive recursive.
7.1 Recursive Relations
A set of, say, natural numbers is effectively decidable if there is an effective procedure
that, applied to a natural number, in a ﬁnite amount of time gives the correct answer
to the question whether it belongs to the set. Thus, representing the answer ‘yes’ by 1
and the answer ‘no’ by 0, a set is effectively decidable if and only if its characteristic
function is effectively computable, where the characteristic function is the function
that takes the value 1 for numbers in the set, and the value 0 for numbers not in the
set. A set is called recursively decidable, or simply recursive for short, if its char-
acteristic function is recursive, and is called primitive recursive if its characteristic
function is primitive recursive. Since recursive functions are effectively computable,
recursive sets are effectively decidable. Church’s thesis, according to which all
effectively computable functions are recursive, implies that all effectively decidable
sets are recursive.
These notions can be generalized to relations. Ofﬁcially, a two-place relation R
among natural numbers will be simply a set of ordered pairs of natural numbers, and
we write Rxy—or R(x, y) if punctuation seems needed for the sake of readability—
interchangeably with (x, y) ∈R to indicate that the relation R holds of x and y,
which is to say, that the pair (x, y) belongs to R. Similarly, a k-place relation is a set
of ordered k-tuples. [In case k = 1, a one-place relation on natural numbers ought to
be a set of 1-tuples (sequences of length one) of numbers, but we will take it simply
to be a set of numbers, not distinguishing in this context between n and (n). We
thus write Sx or S(x) interchangeably with x ∈S.] The characteristic function of a
k-place relation is the k-argument function that takes the value 1 for a k-tuple if the
relation holds of that k-tuple, and the value 0 if it does not; and a relation is effectively
73

74
RECURSIVE SETS AND RELATIONS
decidable if its characteristic function is effectively computable, and is (primitive)
recursive if its characteristic function is (primitive) recursive.
7.1 Example (Identity and order). The identity relation, which holds if and only if x =
y, is primitive recursive, since a little thought shows its characteristic function is 1 −
(sg(x .−y) + sg(y .−x)). The strict less-than order relation, which holds if and only if x < y,
is primitive recursive, since its characteristic function is sg(y .−x).
We are now ready to indicate an important process for obtaining new recursive
functions from old. What follows is actually a pair of propositions, one about primitive
recursive functions, the other about recursive functions (according as one reads the
proposition with or without the bracketed word ‘primitive’). The same proof works
for both propositions.
7.2 Proposition (Deﬁnition by cases). Suppose that f is the function deﬁned in the
following form:
f (x, y) =
⎧
⎪⎨
⎪⎩
g1(x, y)
if C1(x, y)
...
...
gn(x, y)
if Cn(x, y)
where C1, . . . , Cn are (primitive) recursive relations that are mutually exclusive, meaning
that for no x, y do more than one of them hold, and collectively exhaustive, meaning that for
any x, y at least one of them holds, and where g1, . . . , gn are (primitive) recursive total
functions. Then f is (primitive) recursive.
Proof: Let ci be the characteristic function of Ci. By recursion, deﬁne hi(x, y, 0)
= 0, hi(x, y, z′) = gi(x, y). Let ki(x, y) = hi(x, y, ci(x, y)), so ki(x, y) = 0 un-
less Ci(x, y) holds, in which case ki(x, y) = gi(x, y). It follows that f (x, y) =
k1(x, y) + . . . + kn(x, y), and f is (primitive) recursive since it is obtainable by
primitive recursion and composition from the gi and the ci, which are (primitive)
recursive by assumption, together with the addition (and identity) functions.
7.3 Example (The maximum and minimum functions). As an example of deﬁnition by
cases, consider max(x, y) = the larger of the numbers x, y. This can be deﬁned as follows:
max(x, y) =
x
if x ≥y
y
if x < y
or in the ofﬁcial format of the proposition above with g1 = id2
1 and g2 = id2
2. Similarly,
function min(x, y) = the smaller of x, y is also primitive recursive.
These particular functions, max and min, can also be shown to be primitive re-
cursive in a more direct way (as you were asked to do in the problems at the end of
the preceding chapter), but in more complicated examples, deﬁnition by cases makes
it far easier to establish the (primitive) recursiveness of important functions. This is
mainly because there are a variety of processes for deﬁning new relations from old
that can be shown to produce new (primitive) recursive relations when applied to
(primitive) recursive relations. Let us list the most important of these.

7.1. RECURSIVE RELATIONS
75
Given a relation R(y1, . . . , ym) and total functions f1(x1, . . . , xn), . . . , fm(x1, . . . ,
xn), the relation deﬁned by substitution of the fi in R is the relation R∗(x1, . . . , xn)
that holds of x1, . . . , xn if and only if R holds of f1(x1, . . . , xn), . . . , fm(x1, . . . , xn),
or in symbols,
R∗(x1, . . . , xn) ↔R( f1(x1, . . . , xn), . . . , fm(x1, . . . , xn)).
If the relation R∗is thus obtained by substituting functions fi in the relation R, then
the characteristic function c∗of R∗is obtainable by composition from the fi and the
characteristic function c of R:
c∗(x1, . . . , xn) = c( f1(x1, . . . , xn), . . . , fn(x1, . . . , xn)).
Therefore, the result of substituting recursive total functions in a recursive relation is
itself a recursive relation. (Note that it is important here that the functions be total.)
An illustration may make this important notion of substitution clearer. For a given
function f , the graph relation of f is the relation deﬁned by
G(x1, . . . , xn, y) ↔f (x1, . . . , xn) = y.
Let f ∗(x1, . . . , xn, y) = f (x1, . . . , xn). Then f ∗is recursive if f is, since
f ∗= Cn

f, idn+1
1
, . . . , idn+1
n

.
Now f (x1, . . . , xn) = y if and only if
f ∗(x1, . . . , xn, y) = idn+1
n+1(x1, . . . , xn, y).
Indeed, the latter condition is essentially just a long-winded way of writing the former
condition. But this shows that if f is a recursive total function, then the graph relation
f (x1, . . . , xn) = y is obtainable from the identity relation u = v by substituting the
recursive total functions f ∗and idn+1
n+1. Thus the graph relation of a recursive total
function is a recursive relation. More compactly, if less strictly accurately, we can
summarize by saying that the graph relation f (x) = y is obtained by substituting the
recursive total function f in the identity relation. (This compact, slightly inaccurate
manner of speaking, which will be used in future, suppresses mention of the role of
the identity functions in the foregoing argument.)
Besides substitution, there are several logical operations for deﬁning new relations
from old. To begin with the most basic of these, given a relation R, its negation or
denial is the relation S that holds if and only if R does not:
S(x1, . . . , xn) ↔∼R(x1, . . . , xn).
Given two relations R1 and R2, their conjunction is the relation S that holds if and
only if R1 holds and R2 holds:
S(x1, . . . , xn) ↔R1(x1, . . . , xn) & R2(x1, . . . , xn)

76
RECURSIVE SETS AND RELATIONS
while their disjunction is the relation S that holds if and only if R1 holds or R2 holds
(or both do):
S(x1, . . . , xn) ↔R1(x1, . . . , xn) ∨R2(x1, . . . , xn).
Conjunction and disjunctions of more than two relations are similarly deﬁned. Note
that when, in accord with our ofﬁcial deﬁnition, relations are considered as sets of
k-tuples, the negation is simply the complement, the conjunction the intersection, and
the disjunction the union.
Given a relation R(x1, . . . , xn, u), by the relation obtained from R through bounded
universal quantiﬁcation we mean the relation S that holds of x1, . . . , xn, u if and only
if for all v < u, the relation R holds of x1, . . . , xn, v. We write
S(x1, . . . , xn, u) ↔∀v < u R(x1, . . . , xn, v)
or more fully:
S(x1, . . . , xn, u) ↔∀v(v < u →R(x1, . . . , xn, v)).
By the relation obtained from R through bounded existential quantiﬁcation we mean
the relation S that holds of x1, . . . , xn, u if and only if for some v < u, the relation
R holds of x1, . . . , xn, v. We write
S(x1, . . . , xn, u) ↔∃v < u R(x1, . . . , xn, v)
or more fully:
S(x1, . . . , xn, u) ↔∃v(v < u & R(x1, . . . , xn, v)).
The bounded quantiﬁers ∀v ≤u and ∃v ≤u are similarly deﬁned.
Thefollowingtheoremanditscorollaryarestatedforrecursiverelations(andrecur-
sive total functions), but hold equally for primitive recursive relations (and primitive
recursive functions), by the same proofs, though it would be tedious for writers and
readers alike to include a bracketed ‘(primitive)’ everywhere in the statement and
proof of the result.
7.4 Theorem (Closure properties of recursive relations).
(a) A relation obtained by substituting recursive total functions in a recursive relation
is recursive.
(b) The graph relation of any recursive total function is recursive.
(c) If a relation is recursive, so is its negation.
(d) If two relations are recursive, then so is their conjunction.
(e) If two relations are recursive, then so is their disjunction.
(f) If a relation is recursive, then so is the relation obtained from it by bounded
universal quantiﬁcation.
(g) If a relation is recursive, then so is the relation obtained from it by bounded
existential quantiﬁcation.

7.1. RECURSIVE RELATIONS
77
Proof:
(a), (b): These have already been proved.
(c): In the remaining items, we write simply x for x1, . . . , xn. The characteristic
function c∗of the negation or complement of R is obtainable from the characteristic
function c of R by c∗(x) = 1 .−c(x).
(d), (e): The characteristic function c∗of the conjunction or intersection of R1
and R2 is obtainable from the characteristic functions c1 and c2 of R1 and R2 by
c∗(x) = min(c1(x), c2(x)), and the characteristic function c† of the disjunction or
union is similarly obtainable using max in place of min.
(f), (g): From the characteristic function c(x, y) of the relation R(x, y) the charac-
teristic functions u and e of the relations ∀v ≤y R(x1, . . . , xn, v) and ∃v ≤y R(x1,
. . . , xn, v) are obtainable as follows:
u(x, y) =
y	
i=0
c(x, i)
e(x, y) = sg

 y

i=0
c(x, i)

where the summation

and product

notation is as in Proposition 6.5. For the
product will be 0 if any factor is 0, and will be 1 if and only if all factors are 1; while
the sum will be positive if any summand is positive. For the strict bounds ∀v < y and
∃v < y we need only replace y by y .−1.
7.5 Example (Primality). Recall that a natural number x is prime if x > 1 and there do
not exist any u, v both <x such that x = u · v. The set P of primes is primitive recursive,
since we have
P(x) ↔1 < x & ∀u < x ∀v < x(u · v ̸= x).
Here the relation 1 < x is the result of substituting const1 and id into the relation y < x,
which we know to be primitive recursive from Example 7.1, and so this relation is primitive
recursive by clause (a) of the theorem. The relation u · v = x is the graph of a primitive
recursive function, namely, the product function; hence this relation is primitive recursive by
clause (b) of the theorem. So P is obtained by negation, bounded universal quantiﬁcation,
and conjunction from primitive recursive relations, and is primitive recursive by clauses (c),
(d), and (f) of the theorem.
7.6 Corollary (Bounded minimization and maximization). Given a (primitive) recur-
sive relation R, let
Min[R](x1, . . . , xn, w) =
⎧
⎨
⎩
the smallest y ≤w for which
R(x1, . . . , xn, y)
if such a y exists
w + 1
otherwise
and
Max[R](x1, . . . , xn, w) =
⎧
⎨
⎩
the largest y ≤w for which
R(x1, . . . , xn, y)
if such a y exists
0
otherwise.
Then Min[R] and Max[R] are (primitive) recursive total functions.

78
RECURSIVE SETS AND RELATIONS
Proof: We give the proof for Min. Write x for x1, . . . , xn. Consider the (primitive)
recursive relation ∀t ≤y ∼R(x, t), and let c be its characteristic function. If there is
a smallest y ≤w such that R(x, y), then abbreviating c(x, i) to c(i) we have
c(0) = c(1) = · · · = c(y −1) = 1
c(y) = c(y + 1) = · · · = c(w) = 0.
So c takes the value 1 for the y numbers i < y, and the value 0 thereafter. If there is
no such y, then
c(0) = c(1) = · · · = c(w) = 1.
So c takes the value 1 for all w + 1 numbers i ≤w. In either case
Min[R](x, w) =
w

i=0
c(x, i)
and is therefore (primitive) recursive. The proof for Max is similar, and is left to the
reader.
7.7 Example (Quotients and remainders). Given natural numbers x and y with y > 0, there
are unique natural numbers q and r such that x = q · y + r and r < y. They are called the
quotient and remainder on division of x by y. Let quo(x, y) be the quotient on dividing x by
y if y > 0, and set quo(x, 0) = 0 by convention. Let rem(x, y) be the remainder on dividing
x by y if y > 0, and set rem(x, 0) = x by convention. Then quo is primitive recursive, as
an application of bounded maximization, since q ≤x and q is the largest number such
that q · y ≤x.
quo(x, y) =
the largest z ≤x such that y · z ≤x
if
y ̸= 0
0
otherwise.
We apply the preceding corollary (in its version for primitive recursive functions and re-
lations). If we let Rxyz be the relation y · z ≤x, then quo(x, y) = Max[R](x, y, x), and
therefore quo is primitive recursive. Also rem is primitive recursive, since rem(x, y) = x ˙−
(quo(x, y) · y). Another notation for rem(x, y) is x mod y.
7.8 Corollary. Suppose that f is a regular primitive function and that there is a primitive
recursive function g such that the least y with f (x1, . . . , xn, y) = 0 is always less than
g(x1, . . . , xn). Then Mn[ f ] is not merely recursive but primitive recursive.
Proof: Let R(x1, . . . , xn, y) hold if and only if f (x1, . . . , xn, y) = 0. Then
Mn[ f ](x1, . . . , xn) = Min[R](x1, . . . , xn, g(x1, . . . , xn)).
7.9 Proposition. Let R be an (n + 1)-place recursive relation. Deﬁne a total or partial
function r by
r(x1, . . . , xn) = the least y such that R(x1, . . . , xn, y).
Then r is recursive.
Proof: The function r is just Mn[c], where c is the characteristic function of ∼R.
Note that if r is a function and R its graph relation, then r(x) is the only y such
that R(x, y), and therefore a fortiori the least such y (as well as the greatest such y).

7.1. RECURSIVE RELATIONS
79
So the foregoing proposition tells us that if the graph relation of a function is recur-
sive, the function is recursive. We have not set this down as a numbered corollary
because we are going to be getting a stronger result at the beginning of the next
section.
7.10 Example (The next prime). Let f (x) = the least y such that x < y and y is prime.
The relation
x < y & y is prime
is primitive recursive, using Example 7.5. Hence the function f is recursive by the preceding
proposition. There is a theorem in Euclid’s Elements that tells us that for any given number
x there exists a prime y > x, from which we know that our function f is total. But actually,
the proof in Euclid shows that there is a prime y > x with y ≤x! + 1. Since the factorial
function is primitive recursive, the Corollary 7.8 applies to show that f is actually primitive
recursive.
7.11 Example (Logarithms). Subtraction, the inverse operation to addition, can take us
beyond the natural numbers to negative integers; but we have seen there is a reasonable
modiﬁed version ˙−that stays within the natural numbers, and that it is primitive recursive.
Division, the inverse operation to multiplication, can take us beyond the integers to fractional
rational numbers; but again we have seen there is a reasonable modiﬁed version quo that
is primitive recursive. Because the power or exponential function is not commutative, that
is, because in general x y ̸= yx, there are two inverse operations: the yth root of x is the z
such that zy = x, while the base-x logarithm of y is the z such that xz = y. Both can take
us beyond the rational numbers to irrational real numbers or even imaginary and complex
numbers. But again there is a reasonable modiﬁed version, or several reasonable modiﬁed
versions. Here is one for the logarithms
lo(x, y) =
the greatest z ≤x such that yz divides x
if x, y > 1
0
otherwise
where ‘divides x’ means ‘divides x without remainder’. Clearly if x, y > 1 and yz divides
x, z must be (quite a bit) less than x. So we can agrue as in the proof of 7.7 to show that lo
is a primitive recursive function. Here is another reasonable modiﬁed logarithm function:
lg(x, y) =
the greatest z such that yz ≤x
if x, y > 1
0
otherwise.
The proof that lg is primitive recursive is left to the reader.
The next series of examples pertain to the coding of ﬁnite sequences of natural
numbers by single natural numbers. The coding we adopt is based on the fact that
each positive integer can be written in one and only one way as a product of powers
of larger and larger primes. Speciﬁcally:
(a0, a1, . . . , an−1) is coded by 2n3a05a1 · · · π(n)an−1

80
RECURSIVE SETS AND RELATIONS
where π(n) is the nth prime (counting 2 as the 0th). (When we ﬁrst broached the
topic of coding ﬁnite sequences by single numbers in section 1.2, we used a slightly
different coding. That was because we were then coding ﬁnite sequences of positive
integers, but now want to code ﬁnite sequences of natural numbers.) We state the
examples ﬁrst and invite the reader to try them before we give our own proofs.
7.12 Example (The nth prime). Let π(n) be the nth prime, counting 2 as the 0th, so π(0) =
2, π(1) = 3, π(2) = 5, π(3) = 7, and so on. This function is primitive recursive.
7.13 Example (Length). There is a primitive recursive function lh such that if s codes a
sequence (a0, a1, . . . , an−1), then the value lh(s) is the length of that sequence.
7.14 Example (Entries). There is a primitive recursive function ent such that if s codes a
sequence (a0, a1, . . . , an−1), then for each i < n the value of ent(s, i) is the ith entry in that
sequence (counting a0 as the 0th).
Proofs
Example 7.12. π(0) = 2, π(x′) = f (π(x)), where f is the next prime function of
Example 7.10. The form of the deﬁnition is similar to that of the factorial function:
see Example 6.4 for how to reduce deﬁnitions of this form to the ofﬁcial format for
recursion.
Example 7.13. lh(s) = lo(s, 2) will do, where lo is as in Example 7.11. Applied to
2n3a05a1 · · · π(n)an−1
this function yields n.
Example 7.14. ent(s, i) = lo(s, π(i + 1)) will do. Applied to
2n3a05a1 · · · π(n)an−1
and i, this function yields ai.
There are some further examples pertaining to coding, but these will not be needed
till a much later chapter, and even then only in a section that is optional reading, so
we defer them to the optional ﬁnal section of this chapter. Instead we turn to another
auxiliary notion.
7.2 Semirecursive Relations
Intuitively, a set is (positively) effectively semidecidable if there is an effective pro-
cedure that, applied to any number, will if the number is in the set in a ﬁnite amount
of time give the answer ‘yes’, but will if the number is not in the set never give an
answer. For instance, the domain of an effectively computable partial function f is
always effectively semidecidable: the procedure for determining whether n is in the
domain of f is simply to try to compute the value f (n); if and when we succeed, we
know that n is in the domain; but if n is not in the domain, we never succeed.
The notion of effective semidecidability extends in the obvious way to relations.
When applying the procedure, after any number t of steps of computation, we can
tell whether we have obtained the answer ‘yes’ already, or have so far obtained no

7.2. SEMIRECURSIVE RELATIONS
81
answer. Thus if S is a semidecidable set we have
S(x) ↔∃t R(x, t)
where R is the effectively decidable relation ‘by t steps of computation we obtain the
answer “yes”’. Conversely, if R is an effectively decidable relation of any kind, and
S is the relation obtained from R by (unbounded) existential quantiﬁcation, then S is
effectively semidecidable: we can attempt to determine whether n is in S by checking
whether R(n, 0) holds, and if not, whether R(n, 1) holds, and if not, whether R(n, 2)
holds, and so on. If n is in S, we must eventually ﬁnd a t such that R(n, t), and will
thus obtain the answer ‘yes’; but if n is not in S, we go on forever without obtaining
an answer.
Thus we may characterize the effectively semidecidable sets as those obtained
from two-place effectively decidable relations by existential quantiﬁcation, and more
generally, the n-place effectively semidecidable relations as those obtained from
(n + 1)-place effectively decidable relations by existential quantiﬁcation. We deﬁne
an n-place relation S on natural numbers to be (positively) recursively semidecidable,
or simply semirecursive, if it is obtainable from an (n + 1)-place recursive relation
R by existential quantiﬁcation, thus:
S(x1, . . . , xn) ↔∃y R(x1, . . . , xn, y).
A y such that R holds of the xi and y may be called a ‘witness’ to the relation S hold-
ing of the xi (provided we understand that when the witness is a number rather than
a person, a witness only testiﬁes to what is true). Semirecursive relations are effec-
tively semidecidable, and Church’s thesis would imply that, conversely, effectively
semidecidable relations are semirecursive.
These notions should become clearer as we work out their most basic properties,
an exercise that provides an opportunity to review the basic properties of recursive
relations. The closure properties of recursive relations established in Theorem 7.4
can be used to establish a similar but not identical list of properties of semirecursive
relations.
7.15 Corollary (Closure properties of semirecursive relations).
(a) Any recursive relation is semirecursive.
(b) A relation obtained by substituting recursive total functions in a semirecursive
relation is semirecursive.
(c) If two relations are semirecursive, then so is their conjunction.
(d) If two relations are semirecursive, then so is their disjunction.
(e) If a relation is semirecursive, then so is the relation obtained from it by bounded
universal quantiﬁcation.
(f) If a relation is semirecursive, then so is the relation obtained from it by existential
quantiﬁcation.
Proof: We write simply x for x1, . . . , xn.
(a): If Rx is a recursive relation, then the relation S given by Sxy ↔(Rx & y = y)
is also recursive, and we have R(x) ↔∃y Sxy.

82
RECURSIVE SETS AND RELATIONS
(b): If Rx is a semirecursive relation, say Rx ↔∃ySxy where S is recursive, and
if R∗x ↔R f (x), where f is a recursive total function, then the relation S∗given
by S∗xy ↔Sf (x)y is also recursive, and we have R∗x ↔∃y S∗xy and R∗is semi-
recursive.
(c): If R1x and R2x are semirecursive relations, say Rix ↔∃y Sixy where S1 and
S2 are recursive, then the relation S given by Sxw ↔∃y1 < w ∃y2 < w(S1xy1 &
S2xy2) is also recursive, and we have (R1x & R2y) ↔∃w Sxw. We are using here
the fact that for any two numbers y1 and y2, there is a number w greater than both of
them.
(d): If Ri and Si are as in (c), then the relation S given by Sxy ↔(S1xy ∨S2xy) is
also recursive, and we have (R1y ∨R2x) ↔∃y Sxy.
(e): If Rx is a semirecursive relation, say Rx ↔∃y Sxy where S is recursive, and
if R∗x ↔∀u < x Ru, then the relation S∗given by S∗xw ↔∀u < x ∃y < w Suy
is also recursive, and we have R∗x ↔∃w S∗xw. We are using here the fact that for
any ﬁnite number of numbers y0, y1, . . . , yx there is a number w greater than all of
them.
(f): If Rxy is a semirecursive relation, say Rxy ↔∃z Sxyz where S is recursive, and
if R∗x ↔∃y Rxy, then the relation S∗given by S∗xw ↔∃y < w ∃z < w Sxyz is
also recursive, and we have R∗x ↔∃w S∗xw.
The potential for semirecursive relations to yield new recursive relations and func-
tions is suggested by the following propositions. Intuitively, if we have a procedure
that will eventually tell us when a number is in a set (but will tell us nothing if it is
not), and also have a procedure that will eventually tell us when a number is not in a
set (but will tell us nothing if it is), then by combining them we can get a procedure
that will tell us whether or not a number is in the set: apply both given procedures
(say by doing a step of the one, then a step of the other, alternately), and eventually
one or the other must give us an answer. In jargon, if a set and its complement are
both effectively semidecidable, the set is decidable. The next proposition is the formal
counterpart of this observation.
7.16 Proposition (Complementation principle, or Kleene’s theorem). If a set and
its complement are both semirecursive, then the set (and hence also its complement) is
recursive.
Proof: If Rx and ∼Rx are both semirecursive, say Rx ↔∃y S+xy and ∼Rx ↔
∃y S−xy, then the relation S∗given by S∗xy ↔(S+xy ∨S−xy) is recursive, and
if f is the function deﬁned by letting f (x) be the least y such that S∗xy, then f
is a recursive total function. But then we have Rx ↔S+x f (x), showing that R is
obtainable by substituting a recursive total function in a recursive relation, and is
therefore recursive.
7.17 Proposition (First graph principle). If the graph relation of a total or partial func-
tion f is semirecursive, then f is a recursive total or partial function.

7.3. FURTHER EXAMPLES
83
Proof: Suppose f (x) = y ↔∃z Sxyz, where S is recursive. We ﬁrst introduce
two auxiliary functions:
g(x) =
⎧
⎨
⎩
the least w such that
∃y < w ∃z < w Sxyz
if such a w exists
undeﬁned
otherwise
h(x, w) =
⎧
⎨
⎩
the least y < w such that
∃z < w Sxyz
if such a y exists
undeﬁned
otherwise.
Here the relations involved are recursive, and not just semirecursive, since they are
obtained from S by bounded, not unbounded, existential quantiﬁcation. So g and h
are recursive. And a little thought shows that f (x) = h(x, g(x)), so f is recursive also.
The converse of the foregoing proposition is also true—the graph relation of a
recursive partial function is semirecursive, and hence a total or partial function is
recursive if and only if its graph relation is recursive or semirecursive—but we are
not at this point in a position to prove it.
An unavoidable appeal to Church’s thesis is made whenever one passes from a
theorem about what is or isn’t recursively computable to a conclusion about what
is or isn’t effectively computable. On the other hand, an avoidable or lazy appeal
to Church’s thesis is made whenever, in the proof of a technical theorem, we skip
the veriﬁcation that certain obviously effectively computable functions are recur-
sively computable. Church’s thesis is mentioned in connection with omissions of
veriﬁcations only when writing for comparatively inexperienced readers, who cannot
reasonably be expected to be able to ﬁll in the gap for themselves; when writing for
the more experienced reader one simply says “proof left to reader” as in similar cases
elsewhere in mathematics. The reader who works through the following optional
section and/or the optional Chapter 8 and/or the optional sections of Chapter 15 will
be well on the way to becoming “experienced” enough to ﬁll in virtually any such
gap.
7.3* Further Examples
The list of recursive functions is capable of indeﬁnite extension using the machinery
developed so far. We begin with the examples pertaining to coding that were alluded
to earlier.
7.18 Example (First and last). There are primitive recursive functions fst and lst such that
if s codes a sequence (a0, a1, . . . , an−1), then fst(s) and lst(s) are the ﬁrst and last entries in
that sequence.
7.19 Example (Extension). There is a primitive recursive function ext such that if s
codes a sequence (a0, a1, . . . , an−1), then for any b, ext(s, b) codes the extended sequence
(a0, a1, . . . , an−1, b).

84
RECURSIVE SETS AND RELATIONS
7.20 Example (Concatenation). There is a primitive recursive function conc such that if
s codes a sequence (a0, a1, . . . , an−1) and t codes a sequence (b0, b1, . . . , bm−1), then
conc (s, t) codes the concatenation (a0, a1, . . . , an−1, b0, b1, . . . , bm−1) of the two sequences.
Proofs
Example 7.18. fst(s) = ent(s, 0) and lst(s) = ent(s, lh(s) .−1) will do.
Example 7.19. ext(s, b) = 2 · s · π(lh(s) + 1)b will do. Applied to
2n3a05a1 · · · π(n)an−1
this function yields
2n+13a05a1 · · · π(n)an−1π(n + 1)b.
Example 7.20. A head-on approach here does not work, and we must proceed a
little indirectly, ﬁrst introducing an auxiliary function such that
g(s, t, i) = the code for (a0, a1, . . . , an−1, b0, b1, . . . , bi−1).
We can then obtain the function we really want as conc(s, t) = g(s, t, lh(t)). The
auxiliary g is obtained by recursion as follows:
g(s, t, 0) = s
g(s, t, i′) = ext(g(s, t, i), ent(t, i)).
Two more we leave entirely to the reader.
7.21 Example (Truncation). There is a primitive recursive function tr such that if s codes
a sequence (a0, a1, . . . , an−1) and m ≤n, then tr(s, m) codes the truncated sequence (a0,
a1, . . . , am−1).
7.22 Example (Substitution). There is a primitive recursive function sub such that if
s codes a sequence (a1, . . . , ak), and c and d are any natural numbers, then sub(s, c, d)
codes the sequence that results upon taking s and substituting for any entry that is equal to
c the number d instead.
We now turn to examples, promised in the preceding chapter, of recursive total
functions that are not primitive recursive.
7.23 Example (The Ackermann function). Let ≪0≫be the operation of addition, ≪1≫
the operation of multiplication, ≪2≫the operation of exponentiation, ≪3≫the operation
of super-exponentiation, and so on, and let α(x, y, z) = x ≪y ≫z and γ (x) = α(x, x, x).
Thus
γ (0) = 0 + 0 = 0
γ (1) = 1 · 1
= 1
γ (2) = 22
= 4
γ (3) = 333
= 7 625 597 484 987

7.3. FURTHER EXAMPLES
85
after which the values of γ (x) begin to grow very rapidly. A related function δ is determined
as follows:
β0(0) = 2
β0(y′) = (β0(y))′
βx′(0) = 2
βx′(y′) = βx(βx′(y))
β(x, y) = βx(y)
δ(x) = β(x, x).
Clearly each of β0, β1, β2, . . . is recursive. The proof that β and hence δ are also recursive
is outlined in a problem at the end of the chapter. (The proof for α and γ would be similar.)
The proof that γ and hence α is not primitive recursive in effect proceeds by showing that
one needs to apply recursion at least once to get a function that grows as fast as the addition
function, at least twice to get one that grows as fast as the multiplication function, and so
on; so that no ﬁnite number of applications of recursion (and composition, starting with the
zero, successor, and identity functions) can give a function that grows as fast as γ . (The
proof for β and δ would be similar.) While it would take us too far aﬁeld to give the whole
proof here, working through the ﬁrst couple of cases can give insight into the nature of
recursion. We present the ﬁrst case next and outline the second in the problems at the end
of the chapter.
7.24 Proposition. It is impossible to obtain the sum or addition function from the basic
functions (zero, successor, and identity) by composition, without using recursion.
Proof: To prove this negative result we claim something positive, that if f belongs
to the class of functions that can be obtained from the basic functions using only
composition, then there is a positive integer a such that for all x1, . . . , xn we have
f (x1, . . . , xn) < x + a, where x is the largest of x1, . . . , xn. No such a can exist for
the addition function, since (a + 1) + (a + 1) > (a + 1) + a, so it will follow that
the addition function is not in the class in question—provided we can prove our claim.
The claim is certainly true for the zero function (with a = 1), and for the successor
function (with a = 2), and for each identity function (with a = 1 again). Since every
function in the class we are interested in is built up step by step from these functions
using composition, it will be enough to show if the claim holds for given functions,
it holds for the function obtained from them by composition.
So consider a composition
h(x1, . . . , xn) = f (g1(x1, . . . , xn), . . . , gm(x1, . . . , xn)).
Suppose we know
gi(x1, . . . , xn) < x + a j
where x is the largest of the x j
and suppose we know
f (y1, . . . , ym) < y + b
where y is the largest of the yi.
We want to show there is a c such that
h(x1, . . . , xn) < x + c
where x is the largest of the x j.
Let a be the largest of a1, . . . , am. Then where x is the largest of the x j, we have
gi(x1, . . . , xn) < x + a

86
RECURSIVE SETS AND RELATIONS
so if yi = gi(x1, . . . , xn), then where y is the largest of the yi, we have y < x + a.
And so
h(x1, . . . , xn) = f (y1, . . . , ym) < (x + a) + b = x + (a + b)
and we may take c = a + b.
Problems
7.1 Let R be a two-place primitive recursive, recursive, or semirecursive relation.
Show that the following relations are also primitive recursive, recursive, or
semirecursive, accordingly:
(a) the converse of R, given by S(x, y) ↔R(y, x)
(b) the diagonal of R, given by D(x) ↔R(x, x)
(c) for any natural number m, the vertical and horizontal sections of R at m,
given by
Rm(y) ↔R(m, y)
and
Rm(x) ↔R(x, m).
7.2 Prove that the function lg of Example 7.11 is, as there asserted, primitive
recursive.
7.3 For natural numbers, write u | v to mean that u divides v without remainder,
that is, there is a w such that u · w = v. [Thus u | 0 holds for all u, but 0 | v
holds only for v = 0.] We say z is the greatest common divisor of x and y, and
write z = gcd(x, y), if z | x and z | y and whenever w | x and w | y, then w ≤z
[except that, by convention, we let gcd(0, 0) = 0]. We say z is the least common
multiple of x and y, and write z = lcm(x, y), if x | z and y | z and whenever
x |w and y |w, then z ≤w. Show that the functions gcd and lcm are primitive
recursive.
7.4 For natural numbers, we say x and y are relatively prime if gcd(x, y) = 1, where
gcd is as in the preceding problem. The Euler φ-function φ(n) is deﬁned as the
number of m < n such that gcd(m, n) = 1. Show that φ is primitive recursive.
More generally, let Rxy be a (primitive) recursive relation, and let r(x) = the
number of y < x such that Rxy. Show that r is (primitive) recursive.
7.5 Let A be an inﬁnite recursive set, and for each n, let a(n) be the nth element
of A in increasing order (counting the least element as the 0th). Show that the
function a is recursive.
7.6 Let f be a (primitive) recursive total function, and let A be the set of all n such
that the value f (n) is ‘new’ in the sense of being different from f (m) for all
m < n. Show that A is (primitive) recursive.
7.7 Let f be a recursive total function whose range is inﬁnite. Show that there is a
one-to-one recursive total function g whose range is the same as that of f .
7.8 Let us deﬁne a real number ξ to be primitive recursive if the function f (x) = the
digit in the (x + 1)st place in the decimal expansion of ξ is primitive recursive.
[Thus if ξ =
√
2 = 1.4142 . . . , then f (0) = 4, f (1) = 1, f (2) = 4, f (3) = 2,
and so on.] Show that
√
2 is a primitive recursive real number.

PROBLEMS
87
7.9 Let f (n) be the nth entry in the inﬁnite sequence 1, 1, 2, 3, 5, 8, 13, 21,
. . . of Fibonacci numbers. Then f is determined by the conditions f (0) = f (1)
= 1, and f (n+ 2) = f (n) + f (n + 1). Show that f is a primitive recursive
function.
7.10 Show that the truncation function of Example 7.21 is primitive recursive.
7.11 Show that the substitution function of Example 7.22 is primitive recursive.
The remaining problems pertain to Example 7.23 in the optional section 7.3. If
you are not at home with the method of proof by mathematical induction, you
should probably defer these problems until after that method has been discussed
in a later chapter.
7.12 If f and g are n- (and n + 2)-place primitive recursive functions obtainable
from the initial functions (zero, successor, identity) by composition, without
use of recursion, we have shown in Proposition 7.24 that there are numbers a
and b such that for all x1, . . . , xn, y, and z we have
f (x1, . . . , xn) < x + a,
where x is the largest of x1, . . . , xn
g(x1, . . . , xn, y, z) < x + b,
where x is the largest of x1, . . . , xn, y, and z.
Show now that if h = Pr[ f , g], then there is a number c such that for all x1, . . . ,
xn and y we have
h(x1, . . . , xn, y) < cx + c,
where x is the largest of x1, . . . , xn and y.
7.13 Show that if f and g1, . . . , gm are functions with the property ascribed to the
function h in the preceding problem, and if j = Cn[ f , g1, . . . , gm], then j also
has that property.
7.14 Show that the multiplication or product function is not obtainable from the
initial functions by composition without using recursion at least twice.
7.15 Let β be the function considered in Example 7.23. Consider a natural number s
that codes a sequence (s0, . . . , sm) whose every entry si is itself a code for a
sequence (bi,0, . . . , bi,ni). Call such an s a β-code if the following conditions
are met:
if i < m, then bi,0 = 2
if j < n0, then b0, j+1 = b0, j
if i < m and j < ni+1, then c = bi+1, j ≤ni and bi+1, j+1 = bi,c.
Call such an s a β-code covering (p, q) if p ≤m and q ≤n p.
(a) Show that if s is a β-code covering (p, q), then bp,q = β(p, q).
(b) Show that for every p it is the case that for every q there exists a β-code
covering (p, q).
7.16 Continuing the preceding problem, show that the relation Rspqx, which we
deﬁne to hold if and only if s is a β-code covering (p, q) and bp,q = x, is a
primitive recursive relation.
7.17 Continuing the preceding problem, show that β is a recursive (total) function.

8
Equivalent Deﬁnitions of Computability
In the preceding several chapters we have introduced the intuitive notion of effective
computability, and studied three rigorously deﬁned technical notions of computability:
Turing computability, abacus computability, and recursive computability, noting along
the way that any function that is computable in any of these technical senses is com-
putable in the intuitive sense. We have also proved that all recursive functions are abacus
computable and that all abacus-computable functions are Turing computable. In this
chapter we close the circle by showing that all Turing-computable functions are recur-
sive, so that all three notions of computability are equivalent. It immediately follows that
Turing’sthesis,claimingthatalleffectivelycomputablefunctionsareTuringcomputable,
is equivalent to Church’s thesis, claiming that all effectively computable functions are
recursive. The equivalence of these two theses, originally advanced independently of
each other, does not amount to a rigorous proof of either, but is surely important evi-
dence in favor of both. The proof of the recursiveness of Turing-computable functions
occupies section 8.1. Some consequences of the proof of equivalence of the three notions
of computability are pointed out in section 8.2, the most important being the existence
of a universal Turing machine, a Turing machine capable of simulating the behavior of
any other Turing machine desired. The optional section 8.3 rounds out the theory of
computability by collecting basic facts about recursively enumerable sets, sets of natu-
ral numbers that can be enumerated by a recursive function. Perhaps the most basic fact
about them is that they coincide with the semirecursive sets introduced in the preceding
chapter, and hence, if Church’s (or equivalently, Turing’s) thesis is correct, coincide
with the (positively) effectively semidecidable sets.
8.1 Coding Turing Computations
At the end of Chapter 5 we proved that all abacus-computable functions are Turing
computable, and that all recursive functions are abacus computable. (To be quite
accurate, the proofs given for Theorem 5.8 did not consider the three processes in
their most general form. For instance, we considered only the composition of a two-
place function f with two three-place functions g1 and g2. But the methods of proof
used were perfectly general, and do sufﬁce to show that any recursive function can
be computed by some Turing machine.) Now we wish to close the circle by proving,
conversely, that every function that can be computed by a Turing machine is recursive.
We will concentrate on the case of a one-place Turing-computable function, though
our argument readily generalizes. Let us suppose, then, that f is a one-place function
88

8.1. CODING TURING COMPUTATIONS
89
computed by a Turing machine M. Let x be an arbitrary natural number. At the
beginning of its computation of f (x), M’s tape will be completely blank except
for a block of x + 1 strokes, representing the argument or input x. At the outset
M is scanning the leftmost stroke in the block. When it halts, it is scanning the
leftmost stroke in a block of f (x) + 1 strokes on an otherwise completely blank
tape, representing the value or output f (x). And throughout the computation there
are ﬁnitely many strokes to the left of the scanned square, ﬁnitely many strokes to
the right, and at most one stroke in the scanned square. Thus at any time during the
computation, if there is a stroke to the left of the scanned square, there is a leftmost
stroke to the left, and similarly for the right. We wish to use numbers to code a
description of the contents of the tape. A particularly elegant way to do so is through
the Wang coding. We use binary notation to represent the contents of the tape and
the scanned square by means of a pair of natural numbers, in the following manner:
If we think of the blanks as zeros and the strokes as ones, then the inﬁnite portion of
the tape to the left of the scanned square can be thought of as containing a binary nu-
meral (for example, 1011, or 1, or 0) preﬁxed by an inﬁnite sequence of superﬂuous 0s.
We call this numeral the left numeral, and the number it denotes in binary notation
the left number. The rest of the tape, consisting of the scanned square and the portion
to its right, can be thought of as containing a binary numeral written backwards, to
which an inﬁnite sequence of superﬂuous 0s is attached. We call this numeral, which
appears backwards on the tape, the right numeral, and the number it denotes the right
number. Thus the scanned square contains the digit in the unit’s place of the right
numeral. We take the right numeral to be written backwards to insure that changes
on the tape will always take place in the vicinity of the unit’s place of both numerals.
If the tape is completely blank, then the left numeral = the right numeral = 0, and
the left number = the right number = 0.
8.1 Example (The Wang coding). Suppose the tape looks as in Figure 8-1. Then the left
numeral is 11101, the right numeral is 10111, the left number is 29, and the right number
is 23. M now moves left, then the new left numeral is 1110, and the new left number is 14,
while the new right numeral is 101111, and the new right number is 47.
Figure 8-1. A Turing machine tape to be coded.
What are the left and right numbers when M begins the computation? The tape is
then completely blank to the left of the scanned square, and so the left numeral is
0 and the left number is 0. The right numeral is 11 . . . 1, a block of x + 1 digits 1.
A sequence of m strokes represents in binary notation
2m−1 + · · · + 22 + 2 + 1 = 2m −1.

90
EQUIVALENT DEFINITIONS OF COMPUTABILITY
Thus the right number at the start of M’s computation of f (x) will be
strt(x) = 2(x+1) .−1.
Note that strt is a primitive recursive function.
How do the left and right numbers change when M performs one step in the
computation? That depends, of course, on what symbol is being scanned, as well as
on what act is performed. How can we determine the symbol scanned? It will be a
blank, or 0, if the binary representation of the right number ends in a 0, as is the case
when the number is even, and a stroke, or 1, if the binary representation of the right
number ends in a 1, as is the case when the number is odd. Thus in either case it will
be the remainder on dividing the right number by two, or in other words, if the right
number is r, then the symbol scanned will be
scan(r) = rem(r, 2).
Note that scan is a primitive recursive function.
Suppose the act is to erase, or put a 0 on, the scanned square. If there was already
a 0 present, that is, if scan(r) = 0, there will be no change in the left or right number.
If there was a 1 present, that is, if scan(r) = 1, the left number will be unchanged, but
the right number will be decreased by 1. Thus if the original left and right numbers
were p and r respectively, then the new left and new right numbers will be given by
newleft0(p,r) = p
newrght0(p,r) = r .−scan(r).
If instead the act is to print, or put a 1 on, the scanned square, there will again be
no change in the left number, and there will be no change in the right number either
if there was a 1 present. But if there was a 0 present, then the right number will be
increased by 1. Thus the new left and new right number will be given by
newleft1(p,r) = p
newrght1(p,r) = r + 1 .−scan(r).
Note that all the functions here are primitive recursive.
What happens when M moves left or right? Let p and r be the old (pre-move) left
and right numbers, and let p* and r* be the new (post-move) left and right numbers.
We want to see how p* and r* depend upon p, r, and the direction of the move. We
consider the case where the machine moves left.
If p is odd, the old numeral ends in a one. If r = 0, then the new right numeral is
1, and r* = 1 = 2r + 1. And if r > 0, then the new right numeral is obtained from
the old by appending a 1 to it at its one’s-place end (thus lengthening the numeral);
again r* = 2r + 1. As for p*, if p = 1, then the old left numeral is just 1, the
new left numeral is 0, and p* = 0 = (p ˙−1)/2 = quo(p, 2). And if p is any odd
number greater than 1, then the new left numeral is obtained from the old by deleting
the 1 in its one’s place (thus shortening the numeral), and again p* = (p .−1)/2 =
quo(p, 2). [In Example 8.1, for instance, we had p = 29, p* = (29 −1)/2 = 14,

8.1. CODING TURING COMPUTATIONS
91
r = 23, r* = 2 · 23 + 1 = 47.] Thus we have established the ﬁrst of the following
two claims:
If M moves left
and p is odd
then p* = quo(p, 2)
and r* = 2r + 1
If M moves left
and p is even
then p* = quo(p, 2)
and r* = 2r.
The second claim is established in exactly the same way, and the two claims may be
subsumed under the single statement that when M moves left, the new left and right
numbers are given by
newleft2(p,r) = quo(p, 2)
newrght2(p,r) = 2r + rem(p, 2).
A similar analysis shows that if M moves right, then the new left and right numbers
are given by
newleft3(p,r) = 2p + rem(r, 2)
newrght3(p,r) = quo(r, 2).
Again all the functions involved are primitive recursive. If we call printing 0, print-
ing 1, moving left, and moving right acts numbers 0, 1, 2, and 3, then the new left
number when the old left and right numbers are p and r and the act number is a will
be given by
newleft(p,r, a) =
⎧
⎨
⎩
p
if a = 0 or a = 1
quo(p, 2)
if a = 2
2p + rem(r, 2)
if a = 3.
This again is a primitive recursive function, and there is a similar primitive recursive
function newrght(p,r, a) giving the new right number in terms of the old left and
right numbers and the act number.
And what are the left and right numbers when M halts? If M halts in standard
position (or conﬁguration), then the left number must be 0, and the right number must
be r = 2 f (x)+1 .−1, which is the number denoted in binary notation by a string of
f (x) + 1 digits 1. Then f (x) will be given by
valu(r) = lg(r, 2).
Here lg is the primitive recursive function of Example 7.11, so valu is also primitive
recursive. If we let nstd be the characteristic function of the relation
p ̸= 0 ∨r ̸= 2lg(r,2)+1 .−1
then the machine will be in standard position if and only if nstd(p,r) = 0. Again,
since the relation indicated is primitive recursive, so is the function nstd.
So much, for the moment, for the topic of coding the contents of a Turing tape.
Let us turn to the coding of Turing machines and their operations. We discussed the
coding of Turing machines in section 4.1, but there we were working with positive
integers and here we are working with natural numbers, so a couple of changes
will be in order. One of these has already been indicated: we now number the acts
0 through 3 (rather than 1 through 4). The other is equally simple: let us now use

92
EQUIVALENT DEFINITIONS OF COMPUTABILITY
0 for the halted state. A Turing machine will then be coded by a ﬁnite sequence
whose length is a multiple of four, namely 4k, where k is the number of states of
the machine (not counting the halted state), and with the even-numbered entries
(starting with the initial entry, which we count as entry number 0) being numbers
≤3 to represent possible acts, while the odd-numbered entries are numbers ≤k,
representing possible states. Or rather, a machine will be coded by a number coding
such a ﬁnite sequence.
The instruction as to what act to perform when in state q and scanning symbol i
will be given by entry number 4(q .−1) + 2i, and the instruction as to what state to
go into will be given by entry number 4(q .−1) + 2i + 1. For example, the 0th entry
tells what act to perform if in the initial state 1 and scanning a blank 0, and the 1st
entry what state then to go into; while the 2nd entry tells what act to perform if in
initial state 1 and scanning a stroke 1, and the 3rd entry what state then to go into. If
the machine with code number m is in state q and the right number is r, so that the
symbol being scanned is, as we have seen, given by scan(r), then the next action to
be performed and new state to go into will be given by
actn(m, q,r) = ent(m, 4(q .−1) + 2 · scan(r))
newstat(m, q,r) = ent(m, (4(q .−1) + 2 · scan(r)) + 1).
These are primitive recursive functions.
We have discussed representing the tape contents at a given stage of computation by
two numbers p and r. To represent the conﬁguration at a given stage of computation,
we need also to mention the state q the machine is in. The conﬁguration is then
represented by a triple (p, q,r), or by a single number coding such a triple. For
deﬁniteness let us use the coding
trpl(p, q,r) = 2p3q5r.
Then given a code c for the conﬁguration of the machine, we can recover the left,
state, and right numbers by
left(c) = lo(c, 2)
stat(c) = lo(c, 3)
rght(c) = lo(c, 5)
where lo is the primitive recursive function of Example 7.11. Again all the functions
here are primitive recursive.
Our next main goal will be to deﬁne a primitive recursive function conf(m, x, t)
that will give the code for the conﬁguration after t stages of computation when the
machine with code number m is started with input x, that is, is started in its initial
state 1 on the leftmost of a block of x + 1 strokes on an otherwise blank tape. It
should be clear already what the code for the conﬁguration will be at the beginning,
that is, after 0 stages of computation. It will be given by
inpt(m, x) = trpl(0, 1, strt(x)).
What we need to analyse is how to get from a code for the conﬁguration at time t to
the conﬁguration at time t′ = t + 1.
Giventhecodenumberm foramachineandthecodenumberc fortheconﬁguration
at time t, to obtain the code number c* for the conﬁguration at time t + 1, we may

8.1. CODING TURING COMPUTATIONS
93
proceed as follows. First, apply left, stat, and rght to c to obtain the left number, state
number, and right number p, q, and r. Then apply actn and newstat to m and r to
obtain the number a of the action to be performed, and the number q* of the state
then to enter. Then apply newleft and newrght to p, r, and a to obtain the new left and
right numbers p* and r*. Finally, apply trpl to p*, q*, and r* to obtain the desired
c*, which is thus given by
c* = newconf(m, c)
where newconf is a composition of the functions left, stat, rght, actn, newstat, newleft,
newrght, and trpl, and is therefore a primitive recursive function.
The function conf(m, x, t), giving the code for the conﬁguration after t stages of
computation, can then be deﬁned by primitive recursion as follows:
conf (m, x, 0) = inpt(m, x)
conf (m, x, t′) = newconf (m, conf(m, x, t)).
It follows that conf is itself a primitive recursive function.
The machine will be halted when stat(conf(m, x, t)) = 0, and will then be halted
in standard position if and only if nstd(conf(m, x, t)) = 0. Thus the machine will be
halted in standard position if and only if stdh(m, x, t) = 0, where
stdh(m, x, t) = stat(conf(m, x, t)) + nstd(conf(m, x, t)).
If the machine halts in standard conﬁguration at time t, then the output of the machine
will be given by
otpt(m, x, t) = valu(rght(conf(m, x, t))).
Note that stdh and otpt are both primitive recursive functions.
The time (if any) when the machine halts in standard conﬁguration will be given by
halt(m, x) =
the least t such that stdh(m, x, t) = 0
if such a t exists
undeﬁned
otherwise.
This function, being obtained by minimization from a primitive recursive function,
is a recursive partial or total function.
Putting everything together, let F(m, x) = otpt(m, x, halt(m, x)), a recursive func-
tion. Then F(m, x) will be the value of the function computed by the Turing machine
with code number m for argument x, if that function is deﬁned for that argument,
and will be undeﬁned otherwise. If f is a Turing-computable function, then for some
m—namely, for the code number of any Turing machine computing f—we have
f (x) = F(m, x) for all x. Since F is recursive, it follows that f is recursive. We have
proved:
8.2 Theorem. A function is recursive if and only if it is Turing computable.
The circle is closed.

94
EQUIVALENT DEFINITIONS OF COMPUTABILITY
8.2 Universal Turing Machines
The connection we have established between Turing computability and recursiveness
enables us to establish properties of each notion that it would have been more difﬁcult
to establish working with that notion in isolation. We begin with one example of this
phenomenon pertaining to Turing machines, and one to recursive functions.
8.3 Theorem. The same class of functions are Turing computable whether one deﬁnes
Turing machines to have a tape inﬁnite in both directions or inﬁnite in only one direction,
and whether one requires Turing machines to operate with only one symbol in addition to
the blank, or allows them to operate with any ﬁnite number.
Proof: Suppose we have a Turing machine M of the kind we have been working
with, with a two-way inﬁnite tape. In this chapter we have seen that the total or
partial function f computed by M is recursive. In earlier chapters we have seen how
a recursive function f can be computed by an abacus machine and hence by a Turing
machine simulating an abacus machine. But the Turing machines simulating abacus
machines are rather special: according to the problems at the end of Chapter 5, any
abacus-computable function can be computed by a Turing machine that never moves
left of the square on which it is started. Thus we have now shown that for any Turing
machine there is another Turing machine computing the same function that uses only
the right half of its tape. In other words, if we had begun with a more restrictive notion
of Turing machine, where the tape is inﬁnite in one direction only, we would have
obtained the same class of Turing-computable functions as with our ofﬁcial, more
liberal deﬁnition.
Inversely, suppose we allowed Turing machines to operate not only with the blank
0 and the stroke 1, but also with another symbol 2. Then in the proof of the preceding
sections we would need to work with ternary rather than binary numerals, to code
Turing machines by sequences of length a multiple of six rather than of four, and
make similar minor changes. But with such changes, the proof would still go through,
and show that any function computable by a Turing machine of this liberalized kind
is still recursive—and therefore was computable by a Turing machine of the original
kind already. The result generalizes to more than two symbols in an obvious way:
for n symbols counting the blank, we need n-ary numerals and sequences of length a
multiple of 2n.
Similar, somewhat more complicated arguments show that allowing a Turing ma-
chine to work on a two-dimensional grid rather than a one-dimensional tape would
not enlarge the class of functions that are computable. Likewise the class of functions
computable would not be changed if we allowed the use of blank, 0, and 1, and re-
deﬁned computations so that inputs and outputs are to be given in binary rather than
stroke notation. That class is, as is said, stable under perturbations of deﬁnition, one
mark of a natural class of objects.
8.4 Theorem (Kleene normal form theorem). Every recursive total or partial function
canbeobtainedfromthebasicfunctions(zero,successor,identity)bycomposition,primitive
recursion, and minimization, using this last process no more than once.

8.2. UNIVERSAL TURING MACHINES
95
Proof: Suppose we have a recursive function f . We have seen in earlier chapters
that f is computable by an abacus machine and hence by some Turing machine
M. We have seen in this chapter that if m is the code number of M, then f (x) =
F(m, x) for all x, from which it follows that f can be obtained by composition from
the constant function constm, the identity function id, and the function F [namely,
f (x) = F(constm(x), id(x)), and therefore f = Cn[F, constm, id].] Now constm and
id are primitive recursive, and so obtainable from basic functions by composition and
primitive recursion, without use of minimization. As for F, reviewing its deﬁnition,
we see that minimization was used just once (namely, in deﬁning halt(m, x)). Thus
any recursive function f can be obtained using minimization only once.
An (n + 1)-place recursive function F with the property that for every n-place
recursive function f there is an m such that
f (x1, . . . , xn) = F(m, x1, . . . , xn)
is called a universal function. We have proved the existence of a two-place universal
function, and remarked at the outset that our arguments would apply also to functions
with more places. A signiﬁcant property of our two-place universal function, shared
by the analogous many-place universal functions, is that its graph is a semirecursive
relation. For F(m, x) = y if and only if the machine with code number m, given input
x, eventually halts in standard position, giving output y, which is to say, if and only if
∃t(stdh(m, x, t) = 0 & otpt(m, x, t) = y).
Since what follows the existential quantiﬁer here is a primitive recursive relation, the
graph relation F(m, x) = y is obtainable by existential quantiﬁcation from a primi-
tive recursive relation, and therefore is semirecursive, as asserted. Thus we have the
following.
8.5 Theorem. For every k there exists a universal k-place recursive function (whose
graph relation is semirecursive).
This theorem has several substantial corollaries in the theory of recursive func-
tions, but as these will not be essential in our later work, we have relegated them to
an optional ﬁnal section—in effect, an appendix—to this chapter. In the closing para-
graphs of the present section, we wish to point out the implications of Theorem 8.5
for the theory of Turing machines. Of course, in the deﬁnition of universal function
and the statement of the foregoing theorem we could have said ‘Turing-computable
function’ in place of ‘recursive function’, since we now know these come to the same
thing.
A Turing machine for computing a universal function is called a universal Turing
machine. If U is such a machine (for, say, k = 1), then for any Turing machine M
we like, the value computed by M for a given argument x will also be computed
by U given a code m for M as a further argument in addition to x. Historically,
as we have already mentioned, the theory of Turing computability (including the
proof of the existence of a universal Turing machine) was established before (indeed,
a decade or more before) the age of general-purpose, programmable computers, and

96
EQUIVALENT DEFINITIONS OF COMPUTABILITY
in fact formed a signiﬁcant part of the theoretical background for the development of
such computers. We can now say more speciﬁcally that the theorem that there exists a
universalTuringmachine,togetherwithTuring’sthesisthatalleffectivelycomputable
functions are Turing computable, heralded the arrival of the computer age by giving
the ﬁrst theoretical assurance that in principle a general-purpose computer could be
designed that could be made to mimic any special-purpose computer desired, simply
by giving it coded instructions as to what machine it is to mimic as an additional
input along with the arguments of the function we want computed.
8.3∗Recursively Enumerable Sets
An immediate consequence of Theorem 8.5 is the following converse to Proposition
7.17.
8.6 Corollary (Second graph principle). The graph relation of a recursive function is
semirecursive.
Proof: If f is a recursive (total or partial) function, then there is an m such that
f (x) = F(m, x), where F is the universal function of the preceding section. For the
graph relation of f we have
f (x) = y ↔F(m, x) = y.
Hence, the graph relation of f is a section, in the sense of Problem 7.1, of the graph
relation of F, which is semirecursive, and is therefore itself semirecursive.
At the beginning of this book we deﬁned a set to be enumerable if it is the range
of a total or partial function on the positive integers; and clearly we could have said
‘natural numbers’ in place of ‘positive integers’. We now deﬁne a set of natural
numbers to be recursively enumerable if it is the range of a total or partial recursive
function on natural numbers. It turns out that we could say ‘domain’ here instead
of ‘range’ without changing the class of sets involved, and that this class is one we
have already met with under another name: the semirecursive sets. In the literature
the name ‘recursively enumerable’ or ‘r.e.’ is more often used than ‘semirecursive’,
though the two come to the same thing.
8.7 Corollary. Let A be a set of natural numbers. Then the following conditions are
equivalent:
(a) A is the range of some recursive total or partial function.
(b) A is the domain of some recursive total or partial function.
(c) A is semirecursive.
Proof: First suppose A is semirecursive. Then the relation
Rxy ↔Ax & x = y
is semirecursive, since A is semirecursive, the identity relation is semirecursive, and
semirecursive relations are closed under conjunction. But the relation R is the graph

PROBLEMS
97
relation of the restriction of the identity function to A, that is, of the function
idA(x) =
x
if Ax
undeﬁned
otherwise.
Since the graph relation is semirecursive, the function is recursive by Proposition
7.17. And A is both the range and the domain of idA. Hence A is both the range of a
recursive partial function and the domain of such a function.
Now suppose f is a recursive partial or total function. Then by Corollary 8.6 the
graph relation f (x) = y is semirecursive. Since semirecursive relations are closed
under existential quantiﬁcation, the following sets are also semirecursive:
Ry ↔∃x( f (x) = y)
Dx ↔∃y( f (x) = y).
But these sets are precisely the range and the domain of f . Thus the range and domain
of any recursive function are semirecursive.
We have said quite a bit about recursively enumerable (or equivalently, semirecur-
sive) sets without giving any examples of such sets. Of course, in a sense we have
given many examples, since every recursive set is recursively enumerable. But are
there any other examples? We are at last in a position to prove that there are.
8.8 Corollary. There exists a recursively enumerable set that is not recursive.
Proof: Let F be the universal function of Theorem 8.5, and let A be the set of
x such that F(x, x) = 0. Since the graph relation of F is semirecursive, this set is
also semirecursive (or equivalently, recursively enumerable). If it were recursive,
its complement would also be recursive, which is to say, the characteristic function
c of its complement would be a recursive function. But then, since F is a universal
function, there would be an m such that c(x) = F(m, x) for all x, and in particular,
c (m) = F(m, m). But since c is the characteristic function of the complement of A,
we have c (m) = 0 if and only if m is not in A, which, by the deﬁnition of A, means
if and only if F(m, m) is not = 0 (is either undeﬁned, or deﬁned and > 0). This is a
contradiction, showing that A cannot be recursive.
When we come to apply computability theory to logic, we are going to ﬁnd that
there are many more natural examples than this of recursively enumerable sets that
are not recursive.
Problems
8.1 We proved Theorem 8.2 for one-place functions. For two-place (or many-place)
functions, the only difference in the proof would occur right at the beginning,
in deﬁning the function strt. What is the right number at the beginning of a
computation with arguments x1 and x2?
8.2 Suppose we liberalized our deﬁnition of Turing machine to allow the machine to
operate on a two-dimensional grid, like graph paper, with vertical up and down

98
EQUIVALENT DEFINITIONS OF COMPUTABILITY
actions as well as horizontal left and right actions. Describe some reasonable
way of coding a conﬁguration of such a machine.
The remaining problems pertain to the optional section 8.3.
8.3 The (positive) semicharacteristic function of a set A is the function c such
that c(a) = 1 if a is in A, and c(a) is undeﬁned otherwise. Show that a set A is
recursively enumerable if and only if its semicharacteristic function is recursive.
8.4 A two-place relation S is called recursively enumerable if there are two recur-
sive total or partial functions f and g with the same domain such that for all
x and y we have Sxy ↔∃t( f (t) = x & g(t) = y). Show that S is recursively
enumerable if and only if the set of all J(x, y) such that Sxy is recursively
enumerable, where J is the usual primitive recursive pairing function.
8.5 Showthatanyrecursivelyenumerableset A canbedeﬁnedintheform Ay ↔∃w
Ryw for some primitive recursive relation R.
8.6 Show that any nonempty recursively enumerable set A is the range of some
primitive recursive function.
8.7 Show that any inﬁnite recursively enumerable set A is the range of some one-
to-one recursive total function.
8.8 A one-place total function f on the natural numbers is monotone if and only if
whenever x < y we have f (x) < f (y). Show that if A is the range of a monotone
recursive function, then A is recursive.
8.9 A pair of recursively enumerable sets A and B are called recursively inseparable
if they are disjoint, but there is no recursive set C that contains A and is disjoint
from B. Show that a recursively inseparable pair of recursively enumerable sets
exists.
8.10 Give an example of a recursive partial function f such that f cannot be extended
to a recursive total function, or in other words, such that there is no recursive
total function g such that g(x) = f (x) for all x in the domain of f .
8.11 Let R be a recursive relation, and A the recursively enumerable set given by
Ax ↔∃w Rxw. Show that if A is not recursive, then for any recursive total
function f there is an x in A such that the least ‘witness’ that x is in A (that is,
the least w such that Rxw) is greater than f (x).
8.12 Show that if f is a recursive total function, then there is a sequence of functions
f1, . . . , fn with last item fn = f , such that each either is a basic function
(zero, successor, identity) or is obtainable from earlier functions in the sequence
by composition, primitive recursion, or minimization, and all functions in the
sequence are total.

Basic Metalogic


9
A Pr´ecis of First-Order Logic: Syntax
This chapter and the next contain a summary of material, mainly deﬁnitions, needed for
later chapters, of a kind that can be found expounded more fully and at a more relaxed
pace in introductory-level logic textbooks. Section 9.1 gives an overview of the two
groups of notions from logical theory that will be of most concern: notions pertaining
to formulas and sentences, and notions pertaining to truth under an interpretation. The
former group of notions, called syntactic, will be further studied in section 9.2, and the
latter group, called semantic, in the next chapter.
9.1 First-Order Logic
Logic has traditionally been concerned with relations among statements, and with
properties of statements, that hold by virtue of ‘form’ alone, regardless of ‘content’.
For instance, consider the following argument:
(1) A mother or father of a person is an ancestor of that person.
(2) An ancestor of an ancestor of a person is an ancestor of that person.
(3) Sarah is the mother of Isaac, and Isaac is the father of Jacob.
(4) Therefore, Sarah is an ancestor of Jacob.
Logic teaches that the premisses (1)–(3) (logically) imply or have as a (logical)
consequence the conclusion (4), because in any argument of the same form, if the
premisses are true, then the conclusion is true. An example of another argument of
the same form would be the following:
(5) A square or cube of a number is a power of that number.
(6) A power of a power of a number is a power of that number.
(7) Sixty-four is the cube of four and four is the square of two.
(8) Therefore, sixty-four is a power of two.
Modern logic represents the forms of statements by certain algebraic-looking sym-
bolic expressions called formulas, involving special signs. The special signs we are
going to be using are shown in Table 9-1.
101

102
A PR ´ECIS OF FIRST-ORDER LOGIC: SYNTAX
Table 9-1. Logical symbols
∼
Negation
‘not . . . ’
&
Conjunction
‘. . . and . . . ’
∨
Disjunction
‘. . . or . . . ’
→
Conditional
‘if . . . then . . . ’
↔
Biconditional
‘. . . if and only if . . . ’
∀x, ∀y, ∀z, . . .
Universal quantiﬁcation
‘for every x’, ‘for every y’, ‘for every z’, . . .
∃x, ∃y, ∃z, . . .
Existential quantiﬁcation
‘for some x’, ‘for some y’, ‘for some z’, . . .
In this symbolism, the form shared by the arguments (1)–(4) and (5)–(8) above
might be represented as follows:
(9) ∀x∀y((Pyx ∨Qyx) →Ryx)
(10) ∀x∀y(∃z(Ryz & Rzx) →Ryx)
(11) Pab & Qbc
(12) Rac
Content is put back into the forms by providing an interpretation. Specifying
an interpretation involves specifying what sorts of things the xs and ys and zs are
supposed to stand for, which of these things a and b and c are supposed to stand for,
and which relations among these things P and Q and R are supposed to stand for. One
interpretation would let the xs and ys and zs stand for (human) persons, a and b and c
for the persons Sarah and Isaac and Jacob, and P and Q and R for the relations among
persons of mother to child, father to child, and ancestor to descendent, respectively.
With this interpretation, (9) and (10) would amount to the following more stilted
versions of (1) and (2):
(13) For any person x and any person y, if either y is the mother of x or y is the father
of x, then y is an ancestor of x.
(14) For any person x and any person y, if there is a person z such that y is an ancestor
of z and z is an ancestor of x, then y is an ancestor of x.
(11) and (12) would amount to (3) and (4).
Adifferentinterpretationwouldletthe xsand ysand zsstandfor(natural)numbers,
a and b and c for the numbers sixty-four and four and two, and P and Q and R for the
relations of the cube or the square or a power of a number to that number, respectively.
With this interpretation, (9)–(12) would amount to (5)–(8). We say that (9)–(11) imply
(12) because in any interpretation in which (9)–(11) come out true, (12) comes out
true.
Our goal in this chapter will be to make the notions of formula and interpretation
rigorous and precise. In seeking the degree of clarity and explicitness that will be
needed for our later work, the ﬁrst notion we need is a division of the symbols that
may occur in formulas into two sorts: logical and nonlogical. The logical symbols
are the logical operators we listed above, the connective symbols (the tilde ∼, the
ampersand &, the wedge ∨, the arrow →, the double arrow ↔), the quantiﬁer symbols
(the inverted ay ∀, the reversed ee ∃), plus the variables x, y, z, . . . that go with the
quantiﬁers, plus left and right parentheses and commas for punctuation.

9.1. FIRST-ORDER LOGIC
103
The nonlogical symbols are to begin with of two sorts: constants or individual
symbols, and predicates or relation symbols. Each predicate comes with a ﬁxed
positive number of places. (It is possible to consider zero-place predicates, called
sentence letters, but we have no need for them here.) As we were using them above,
a and b and c were constants, and P and Q and R were two-place predicates.
Especially though not exclusively when dealing with mathematical material, some
further apparatus is often necessary or useful. Hence we often include one more
logical symbol, a special two-place predicate, the identity symbol or equals sign =,
for ‘. . . is (the very same thing as) . . . ’. To repeat, the equals sign, though a two-
place predicate, is counted as a logical symbol, but it is the only exception: all other
predicates count as nonlogical symbols. Also, we often include one more category
of nonlogical symbols, called function symbols. Each function symbol comes with a
ﬁxed number of places. (Occasionally, constants are regarded as zero-place function
symbols, though usually we don’t so regard them.)
We conscript the word ‘language’ to mean an enumerable set of nonlogical sym-
bols. A special case is the empty language L∅, which is just the empty set under
another name, with no nonlogical symbols. Here is another important case.
9.1 Example (The language of arithmetic). One language that will be of especial interest
to us in later chapters is called the language of arithmetic, L*. Its nonlogical symbols are
the constant zero 0, the two-place predicate less-than <, the one-place function symbol
successor ′, and the two-place function symbols addition + and multiplication · .
Intuitively, formulasarejustthesequencesofsymbolsthatcorrespondtogrammat-
ically well-formed sentences of English. Those that, like (9)–(12) above, correspond
to English sentences that make a complete statement capable of being true or false
are called closed formulas. Those that, like (Pyz ∨Qyx), correspond to English
sentences involving unidentiﬁed xs and ys and zs that would have to be identiﬁed
before the sentences could be said to be true or false, are called open formulas.
The terms are sequences of symbols, such as 0 or 0 + 0 or x or x ′′, that correspond
to grammatically well-formed phrases of English of the kind that grammarians call
‘singular noun phrases’. The closed terms are the ones that involve no variables, and
the open terms are the ones that involve variables whose values would have to be
speciﬁed before the term as a whole could be said to have a denotation. When no
function symbols are present, the only closed terms are constants, and the only open
terms are variables. When function symbols are present, the closed terms also include
such expressions as 0 + 0, and the open terms such expressions as x′′.
The formulas and terms of a given language are simply the ones all of whose
nonlogical symbols belong to that language. Since languages are enumerable and
each formula of a language is a ﬁnite string of symbols from the language plus
variables and logical symbols, the set of formulas is enumerable, too. (One might at
ﬁrst guess that the empty language would have no formulas, but at least when identity
is present, in fact it has inﬁnitely many, among them ∀x x = x, ∀y y = y, ∀z z = z,
and so on.)
An interpretation M for a language L consists of two components. On the one
hand, there is a nonempty set |M| called the domain or universe of discourse of the

104
A PR ´ECIS OF FIRST-ORDER LOGIC: SYNTAX
interpretation, the set of things M interprets the language to be talking about. When
we say ‘for every x’ or ‘for some x’, what we mean, according to interpretation M,
is ‘for every x in |M|’ or ‘there exists an x in |M|’. On the other hand, there is for
each nonlogical symbol a denotation assigned to it. For a constant c, the denotation
cM is to be some individual in the domain |M|. For an n-place nonlogical predicate
R, the denotation RM is to be some n-place relation on |M| (which is ofﬁcially
just a set of n-tuples of elements of |M|, a one-place relation being simply a subset
of |M|).
For example, for the language LG with constants a and b and c and two-place
predicates P and Q and R, the genealogical interpretation G of LG indicated above
would now be described by saying that the domain |G| is the set of all persons, aG is
Sarah, bG is Isaac, cG is Jacob, PG is set of ordered pairs of persons where the ﬁrst is
the mother of the second, and analogously for QG and RG. Under this interpretation,
the open formula ∃z(Pyz & Qzx) amounts to ‘y is the paternal grandmother of x’,
while ∃z(Qyz & Pzx) amounts to ‘y is the maternal grandfather of x’. The closed
formula ∼∃x Pxx amounts to ‘no one is her own mother’, which is true, while
∃x Qxx amounts to ‘someone is his own father’, which is false.
When the identity symbol is present, it is not treated like the other, nonlogical
predicates: one is not free to assign it an arbitrary two-place relation on the domain
as its denotation; rather, its denotation must be the genuine identity relation on that
domain, the relation each thing bears to itself and to nothing else. When function
symbols are present, for an n-place function symbol f , the denotation f M is an
n-argument function from |M| to |M|.
9.2 Example (The standard interpretation of the language of arithmetic). One interpreta-
tion that will be of especial interest to us in later chapters is called the standard interpretation
N ∗of the language of the language of arithmetic L∗. Its domain |N ∗| is the set of natural
numbers; the denotation 0N ∗of the cipher 0 is the number zero; the denotation < N ∗of the
less-than sign is the usual strict less-than order relation; the denotation ′N ∗of the accent is
the successor function, which takes each number to the next larger number; and the denota-
tions +N ∗and ·N ∗of the plus sign and times sign are the usual addition and multiplication
functions. Then such an open term as x · y would stand for the product of x and y, whatever
they are; while such a closed term as 0′′ would stand for the successor of the successor of
zero, which is to say the successor of one, which is to say two. And such a closed formula as
(15) ∀x∀y(x · y = 0′′ →( x = 0′′ ∨y = 0′′))
would stand for ‘for every x and every y, if the product of x and y is two, then either x
is two or y is two’ or ‘a product is two only if one of the factors is two’. This happens to
be true (given that our domain consists of natural numbers, with no negatives or fractions).
Other closed formulas that come out true on this interpretation include the following:
(16) ∀x∃y(x < y&∼∃z(x < z&z < y))
(17) ∀x(x < x ′ & ∼∃z(x < z&z < x ′)).
Here (16) says that for any number x there is a next larger number, and (17) that
x′ is precisely this next larger number.

9.1. FIRST-ORDER LOGIC
105
(For the empty language L∅, there are no nonlogical symbols to be assigned
denotations, but an interpretation must still specify a domain, and that speciﬁcation
makes a difference as to truth for closed formulas involving =. For instance, ∃x∃y ∼
x = y will be true if the domain has at least two distinct elements, but false if it has
only one.)
Closed formulas, which are also called sentences, have truth values, true or false,
when supplied with an interpretation. But they may have different truth values un-
der different interpretations. For our original example (9)–(12), on the genealogical
interpretation we have since named G (and equally on the alternative arithmetical
interpretation that we have left nameless) all four sentences came out true. But alter-
native interpretations are possible. For instance, if we kept everything else the same
as in the genealogical interpretation, but took R to denote the relation of descendant
to ancestor rather than vice versa, (10) and (11) would remain true, but (9) and (12)
would become false: descendants of descendants are descendants, but parents and
grandparents are not descendants. Various other combinations are possible. What
one will not ﬁnd is any interpretation that makes (9)–(11) all true, but (12) false.
Precisely that, to repeat, is what is meant by saying that (9)–(11) imply (12).
9.3 Example (Alternative interpretations of the language of arithmetic). For the language
of arithmetic, there is an alternative interpretation Q in which the domain is the nonnegative
rational numbers, but the denotation of 0 is still zero, the denotation of ′ is still the function
thataddsonetoanumber,thedenotationsof + and·aretheusualadditionandmultiplication
operations, and the denotation of < is still the less-than relation among the numbers in
question. On this interpretation, (16) and (17) above are both false (because there are lots
of rational numbers between x and any larger y in general, and lots of rational numbers
between x and x plus one in particular). There is another alternative interpretation P in
which the domain consists of the nonnegative half integers 0, 1/2, 1, 11/2 , 2, 21/2, 3, and so
on, but the denotation of 0 is still zero, the denotation of ′ is still the function that adds one
to a number, the denotation of + is still the usual addition operation, and the denotation
of < is still the less-than relation among the numbers in question. (Multiplication cannot
be interpreted in the usual way, since a product of two half integers is not in general a half
integer, but for purposes of this example it does not matter how multiplication is interpreted.)
On this interpretation, (16) would be true (because there is no half integer between x and
y = x plus one-half), but (17) would be false (because there is a half integer between x and
x plus one, namely x plus one-half). What you won’t ﬁnd is an interpretation that makes
(17) true but (16) false. And again, that is what it means to say that (16) is a consequence
of (17).
The explanations given so far provide part of the precision and rigor that will be
needed in our later work, but only part. For they still rely on an intuitive understanding
of what it is to be a sentence of a language, and what it is for a sentence be true in an
interpretation. There are two reasons why we want to avoid this reliance on intuition.
The ﬁrst is that when we come to apply our work on computability to logic, we
are going to want the notion of sentence to be so precisely deﬁned that a machine
could tell whether or not a given string of symbols is a sentence. The second is that

106
A PR ´ECIS OF FIRST-ORDER LOGIC: SYNTAX
the notion of truth was historically under a certain cloud of suspicion, owing to the
occurrence of certain contradictions, euphemistically called ‘paradoxes’, such as the
ancient Epimenides or liar paradox: If I say, ‘what I am now saying is not true’, is
what I am saying true? We are therefore going to want to give, for sentences of the
kind of formal language we are considering, a deﬁnition of truth just as rigorous
as the deﬁnition of any other notion in mathematics, making the notion of truth, as
applied to the kind of formal language we are considering, as respectable as any other
mathematical notion.
The next section will be devoted to giving precise and rigorous deﬁnitions of the
notions of formula and sentence, and more generally to giving deﬁnitions of notions
pertaining to syntax, that is, pertaining to the internal structure of formulas. The
next chapter will be devoted to giving the deﬁnition of truth, and more generally to
giving deﬁnitions of notions pertaining to semantics, that is, pertaining to the external
interpretation of formulas.
9.2 Syntax
Ofﬁcially we think of ourselves as working for each k > 0 with a ﬁxed denumerable
stock of k-place predicates:
A1
0
A1
1
A1
2
· · ·
A2
0
A2
1
A2
2
· · ·
A3
0
A3
1
A3
2
· · ·
...
...
...
and with a ﬁxed denumerable stock of constants:
f 0
0
f 0
1
f 0
2
. . . .
Whenfunctionsymbolsarebeingused,wearealsogoingtowantforeachk > 0aﬁxed
denumerable stock of k-place function symbols:
f 1
0
f 1
1
f 1
2
. . .
f 2
0
f 2
1
f 2
2
. . .
f 3
0
f 3
1
f 3
2
. . .
...
...
...
.
Any language will be a subset of this ﬁxed stock. (In some contexts in later chapters
where we are working with a language L we will want to be able to assume that there
are inﬁnitely many constants available that have not been used in L. This is no real
difﬁculty, even if L itself needs to contain inﬁnitely many constants, since we can
either add the new constants to our basic stock, or assume that L used only every
other constant of our original stock to begin with.)
We also work with a ﬁxed denumerable stock of variables:
v0
v1
v2
. . . .

9.2. SYNTAX
107
Thus the more or less traditional 0 and < and ′ and + and · we have been writing—
and in practice, are going to continue to write—are in principle to be thought of as
merely nicknames for f 0
0 and A2
0 and f 1
0 and f 2
0 and f 2
1 ; while even writing x and y
and z rather than vi and v j and vk, we are using nicknames, too.
The ofﬁcial deﬁnition of the notion of formula begins by deﬁning the notion of
an atomic formula, which will be given ﬁrst for the case where identity and function
symbols are absent, then for the case where they are present. (If sentence letters were
admitted, they would count as atomic formulas, too; but, as we have said, we generally
are not going to admit them.) If identity and function symbols are absent, then an
atomic formula is simply a string of symbols R(t1 , . . . , tn) consisting of a predicate,
followed by a left parenthesis, followed by n constants or variables, where n is the
number of places of the predicate, with commas separating the successive terms, all
followed by a right parenthesis. Further, if F is a formula, then so is its negation
∼F, consisting of a tilde followed by F. Also, if F and G are formulas, then so is
their conjunction (F & G), consisting of a left parenthesis, followed by F, which is
called the left or ﬁrst conjunct, followed by the ampersand, followed by G, which
is called the right or second conjunct, followed by a right parenthesis. Similarly for
disjunction. Also, if F is a formula and x is a variable, the universal quantiﬁcation
∀xF is a formula, consisting of an inverted ay, followed by x, followed by F. Similarly
for existential quantiﬁcation.
And that is all: the deﬁnition of (ﬁrst-order) formula is completed by saying
that anything that is a (ﬁrst-order) formula can be built up from atomic formulas in a
sequenceofﬁnitelymanysteps—calledaformationsequence—byapplyingnegation,
junctions, and quantiﬁcations to simpler formulas. (Until a much later chapter, where
we consider what are called second-order formulas, ‘ﬁrst-order’ will generally be
omitted.)
Where identity is present, the atomic formulas will include ones of the kind
=(t1, t2). Where function symbols are present, we require a preliminary deﬁnition of
terms. Variables and constants are atomic terms. If f is an n-place function symbol
and t1 , . . . , tn are terms, then f (t1 , . . . , tn) is a term. And that is all: the deﬁnition
of term is completed by stipulating that anything that is a term can be built up from
atomic terms in a sequence of ﬁnitely many steps—called a formation sequence—by
applying function symbols to simpler terms. Terms that contain variables are said to
be open, while terms that do not are said to be closed. An atomic formula is now
something of the type R(t1 , . . . , tn) where the ti may be any terms, not just constants
or variables; but otherwise the deﬁnition of formula is unchanged.
Note that ofﬁcially predicates are supposed to be written in front of the terms to
which they apply, so writing x < y rather than < (x, y) is an unofﬁcial colloquial-
ism. We make use of several more such colloquialisms below. Thus we sometimes
omit the parentheses around and commas separating terms in atomic formulas, and
we generally write multiple conjunctions like (A & (B & (C & D))) simply as
(A & B & C & D), and similarly for disjunctions, as well as sometimes omitting
the outer parentheses on conjunctions and disjunctions (F & G) and (F ∨G) when
these stand alone rather than as parts of more complicated formulas. All this is slang,
from the ofﬁcial point of view. Note that →and ↔have been left out of the ofﬁcial

108
A PR ´ECIS OF FIRST-ORDER LOGIC: SYNTAX
Table 9-2. Some terms of the language of
arithmetic
v0
x
f 0
0
0
f 1
0( f 0
0 )
1
f 1
0( f 1
0 ( f 0
0 ))
2
f 2
1( f 1
0 ( f 1
0 ( f 0
0 )), v0)
2 · x
f 2
0( f 2
1 ( f 1
0 ( f 1
0 ( f 0
0 )), v0), f 2
1 ( f 1
0 ( f 1
0 ( f 0
0 )), v0))
2 · x + 2 · x
language entirely: (F →G) and (F ↔G) are to be considered unofﬁcial abbrevia-
tions for (∼F ∨G) and ((∼F ∨G) & (∼G ∨F)). In connection with the language of
arithmetic we allow ourselves two further such abbreviations, the bounded quantiﬁers
∀y < x for ∀y(y < x →. . . ) and ∃y < x for ∃y(y < x & . . . ).
Where identity is present, we also write x = y and x ̸= y rather than =(x, y) and
∼==(x, y). Where function symbols are present, they also are supposed to be written
in front of the terms to which they apply. So our writing x′ rather than ′(x) and x + y
and x · y rather than +(x, y) and ·(x, y) is a colloquial departure from ofﬁcialese.
And if we adopt—as we do—the usual conventions of algebra that allow us to omit
certain parenthesis, so that x + y · z is conventionally understood to mean x + (y · z)
rather than (x + y) · z without our having to write the parentheses in explicitly, that
is another such departure. And if we go further—as we do—and abbreviate 0′, 0′′,
0′′′, . . . , as 1, 2, 3, . . . , that is yet another departure.
Some terms of L* in ofﬁcial and unofﬁcial notation are shown in Table 9-2. The
left column is a formation sequence for a fairly complex term.
Some formulas of L∗in ofﬁcial (or rather, semiofﬁcial, since the the terms have
been written colloquially) notation are shown in Table 9-3. The left column is a
formation sequence for a fairly complex formula.
No one writing about anything, whether about family trees or natural numbers,
will write in the ofﬁcial notation illustrated above (any more than anyone ﬁlling out
a scholarship application or a tax return is going to do the necessary calculations in
the rigid format established in our chapters on computability). The reader may well
wonder why, if the ofﬁcial notation is so awkward, we don’t just take the abbreviated
Table 9-3. Some formulas of the language of arithmetic
A20(x, 0)
x < 0
A20(x, 1)
x < 1
A20(x, 2)
x < 2
A20(x, 3)
x < 3
∼A20(x, 3))
∼x < 3
(= (x, 1)∨= (x, 2))
x == 1 ∨x == 2
(= (x, 0) ∨(= (x, 1)∨= (x, 2)))
x == 0 ∨x == 1 ∨x == 2
(∼A20(x, 3) ∨(= (x, 0) ∨(= (x, 1)∨= (x, 2))))
x < 3 →(x == 0 ∨x == 1 ∨x == 2)
∀x((∼A20(x, 3) ∨(= (x, 0) ∨(= (x, 1)∨= (x, 2)))))
∀x < 3(x == 0 ∨x == 1 ∨x == 2)

9.2. SYNTAX
109
notation as the ofﬁcial one. The reason is that in proving things about the terms and
formulas of a language, it is easiest if the language has a very rigid format (just as,
in proving things about computability, it is easiest if the computations take place in
a very rigid format). In writing examples of terms and formulas in the language, it
is on the contrary easiest if the language has a very ﬂexible format. The traditional
strategy of logicians is to make the ofﬁcial language about which one proves theorems
a very austere and rigid one, and to make the unofﬁcial language in which one writes
examples a very generous and ﬂexible one. Of course, for the theorems proved about
the austere idiom to be applicable to the generous idiom, one has to have conﬁdence
that all the abbreviations permitted by the latter but not the former could in principle
be undone. But there is no need actually to undo them in practice.
The main method of proving theorems about terms and formulas in a language is
called induction on complexity. We can prove that all formulas have a property by
proving
Base Step: Atomic formulas have the property.
Induction Step: If a more complex formula is formed by applying a logical operator
to a simpler formula or formulas, then, assuming (as induction hypothesis) that the
simpler formula or formulas have the property, so does the more complex formula.
The induction step will usually be divided into cases, according as the operator is ∼
or & or ∨or ∀or ∃.
Typically the proof will ﬁrst be given for the situation where identity and func-
tion symbols are absent, then for the situation with identity present but function
symbols absent, and then for the case with both identity and function symbols present.
Identity typically requires very little extra work if any, but where function symbols
are present, we generally need to prove some preliminary result about terms, which is
also done by induction on complexity: we can prove that all terms have some property
by proving that atomic terms have the property, and that if a more complex term is
formed by applying a function symbol to simpler terms, then, assuming the simpler
terms have the property, so does the more complex term.
The method of proof by induction on complexity is so important that we want to
illustrate it now by very simple examples. The following lemma may tell us more
than we want to know about punctuation, but is good practice.
9.4 Lemma (Parenthesis lemma). When formulas are written in ofﬁcial notation the
following hold:
(a) Every formula ends in a right parenthesis.
(b) Every formula has equally many left and right parentheses.
(c) If a formula is divided into a left part and a right part, both nonempty, then there
are at least as many left as right parentheses in the left part, and more if that part
contains at least one parenthesis.
Proof: We give ﬁrst the proof for (a). Base step: An atomic formula R(t1 , . . . , tn)
or =(t1, t2) of course ends in a right parenthesis. Induction step, negation case: If
F ends in a right parenthesis, then so does ∼F, since the only new symbol is at
the beginning. Induction step, junction case: A conjunction (F & G) or disjunction
(F ∨G) of course ends in a right parenthesis. Induction step, quantiﬁcation case: If

110
A PR ´ECIS OF FIRST-ORDER LOGIC: SYNTAX
F ends in a right parenthesis, then so do ∀xF or ∃xF, for the same reason as in the
case of negation, namely, that the only new symbols are at the beginning.
In giving the proof for (b), we allow ourselves to be a little less rigid about the
format. We consider ﬁrst the case where function symbols are absent. First note
that an atomic formula R(t1 , . . . , tn) or =(t1, t2) has equal numbers of left and right
parentheses, namely, one of each. Then note that F has equal numbers of left and
right parentheses, then so does ∼F, since there are no new parentheses. Then note
that if F has m of each kind of parenthesis, and G has n of each, then (F & G) has
m + n + 1 of each, the only new ones being the outer ones. The proof for disjunction
is the same as for conjunction, and the proofs for quantiﬁcations essentially the same
as for negation.
If function symbols are present, we need the preliminary result that every term
has equally many left and right parentheses. This is established by induction on
complexity. An atomic term has equal numbers of left and right parentheses, namely
zero of each. The nonatomic case resembles the conjunction case above: if s has m
each of left and right parentheses, and t has n each, then f (s, t) has m + n + 1 each;
and similarly for f (t1 , . . . , tk) for values of k other than two. Having this preliminary
result, we must go back and reconsider the atomic case in the proof of (b). The
argument now runs as follows: if s has m each of left and right parentheses, and t has
n each, then R(s, t) has m + n + 1 each, and similarly for R(t1 , . . . , tk) for values of
k other than two. No change is needed in the nonatomic cases of the proof of (b).
In giving the proof for (c), we also ﬁrst consider the case where function symbols
are absent. First suppose an atomic formula R(t1, . . . , tn) or =(t1, t2) is divided into
a left part λ and a right part ρ, both nonempty. If λ is just R or =, it contains zero
parentheses of each kind. Otherwise, λ contains the one and only left parenthesis and
not the one and only right parenthesis. In either case, (c) holds. Next assume (c) holds
for F, and suppose ∼F is divided. If λ consists just of ∼, and ρ of all of F, then λ
contains zero parentheses of each kind. Otherwise, λ is of the form ∼λ0, where λ0
is a left part of F, and ρ is the right part of F. By assumption, then λ0 and hence λ
has at least as many left as right parentheses, and more if it contains any parentheses
at all. Thus in all cases, (c) holds for ∼F. Next assume (c) holds for F and G, and
suppose (F & G) is divided. The possible cases for the left part λ are:
Case1
Case 2
Case 3
Case 4
Case 5
Case 6
(
(λ0
(F
(F &
(F & λ1
(F & G
where in case 2, λ0 is a left part of F, and in case 5, λ1 is a left part of G. In every
case, the part of λ after the initial left parenthesis has at least as many left as right
parentheses: obviously in case 1, by the assumption of (c) for F in case (2), by part
(b) in case (3), and so on. So the whole left part λ has at least one more left than right
parenthesis, and (c) holds for (F & G). The proof for disjunction is the same as for
conjunction, and the proofs for quantiﬁcations essentially the same as for negation.
We leave the case where function symbols are present to the reader.
We conclude this section with the ofﬁcial deﬁnitions of four more important
syntactic notions. First, we ofﬁcially deﬁne a string of consecutive symbols within a

9.2. SYNTAX
111
given formula to be a subformula of the given formula if it is itself a formula. Where
function symbols are present, we can similarly deﬁne a notion of subterm. We stop
to note one result about subformulas.
9.5 Lemma (Unique readability lemma).
(a) The only subformula of an atomic formula R(t1 , . . . , tn) or =(t1, t2) is itself.
(b) The only subformulas of ∼F are itself and the subformulas of F.
(c) The only subformulas of (F & G) or (F ∨G) are itself and the subformulas of F
and G.
(d) The only subformulas of ∀xF or ∃xF are itself and the subformulas of F.
These assertions may seem obvious, but they only hold because we use enough
parentheses. If we used none at all, the disjunction of F & G with H, that is, F &
G ∨H, would have the subformula G ∨H, which is neither the whole conjunction
nor a subformula of either conjunct. Indeed, the whole formula would be the same
as the conjunction of F with G ∨H, and we would have a serious ambiguity. A
rigorous proof of the unique readability lemma requires the parenthesis lemma.
Proof: For (a), a subformula of R(t1, . . . , tn) or =(t1, t2) must contain the initial
predicate R or =, and so, if it is not the whole formula, it will be a left part of it.
Being a formula, it must contain (and in fact end in) a parenthesis by 9.4(a), and so,
if it is not the whole formula but only a left part, must contain an excess of left over
right parentheses by 9.4(c), which is impossible for a formula by 9.4(b).
For (b), a subformula of ∼F that is not a subformula of F must contain the
initial negation sign ∼, and so, if it is not the whole formula ∼F, it will be a left
part of it, and from this point the argument is essentially the same as in the atomic
case (a).
For (c), we relegate the proof to the problems at the end of the chapter.
For (d), the argument is essentially the same as for (b).
Resuming our series of deﬁnitions, second, using the notion of subformula, we
state the ofﬁcial deﬁnition of which occurrences of a variable x in a formula F are free
and which are bound: an occurrence of variable x is bound if it is part of a subformula
beginning ∀x . . . or ∃x. . . , in which case the quantiﬁer ∀or ∃in question is said to
bind that occurrence of the variable x, and otherwise the occurrence of the variable
x is free. As an example, in
x < y & ∼∃z(x < z & z < y)
all the occurrences of x and y are free, and all the occurrences of z are bound; while
in
Fx →∀xFx
the ﬁrst occurrence of x is free, and the other two occurrences of x are bound. [The
difference between the role of a free variable x and the role of a bound variable u in

112
A PR ´ECIS OF FIRST-ORDER LOGIC: SYNTAX
a formula like ∀u R(x, u) or ∃u R(x, u) is not unlike the difference between the roles
of x and of u in mathematical expressions like
 x
1
du
u
x

u=1
1
u
For some readers this analogy may be helpful, and those readers who do not ﬁnd it
so may ignore it.]
In general, any and all occurrences of variables in an atomic formula R(t1 , . . . , tn)
are free, since there are no quantiﬁers in the formula; the free occurrences of a variable
in a negation ∼F are just the free occurrences in F, since any subformula of ∼F
beginning ∀x or ∃x is a proper subformula of ∼F and so a subformula of F; and
similarly, the free occurrences of a variable in a junction (F & G) or (F ∨G) are just
those in F and G; and similarly, the free occurrences of a variable other than x in a
quantiﬁcation ∀xF or ∃xF are just those in F, while of course none of the occurrences
of x in ∀xF or ∃xF is free.
Third, using the notion of free and bound occurrence of variables, we state the
ofﬁcial deﬁnition of the notion of an instance of a formula. But before giving that
deﬁnition, let us mention a convenient notational convention. When we write some-
thing like ‘Let F(x) be a formula’, we are to be understood as meaning ‘Let F be a
formula in which no variables occur free except x’. That is, we indicate which vari-
ables occur free in the formula we are calling F by displaying them immediately after
the name F we are using for that formula. Similarly, if we go on to write something
like ‘Let c be a constant, and consider F(c)’, we are to be understood as meaning,
‘Let c be a constant, and consider the result of substituting c for all free occurrences
of x in the formula F’. That is, we indicate what substitution is to be made in the
formula we are calling F(x) by making that very substitution in the expression F(x).
Thus if F(x) is ∀y ∼y < x, then F(0) is ∀y ∼y < 0. Then the ofﬁcial deﬁnition of
instance is just this: an instance of a formula F(x) is any formula of form F(t) for
t a closed term. Similar notations apply where there is more than one free variable,
and to terms as well as formulas.
Fourth and ﬁnally, again using the notion of free and bound occurrence of variables,
we state the ofﬁcial deﬁnition of sentence: a formula is a sentence if no occurrence
of any variable in it is free. A subsentence is a subformula that is a sentence.
Problems
9.1 Indicate the form of the following argument—traditionally called ‘syllogism in
Felapton’—using formulas:
(a) No centaurs are allowed to vote.
(b) All centaurs are intelligent beings.
(c) Therefore, some intelligent beings are not allowed to vote.
Do the premisses (a) and (b) in the preceding argument imply the conclusion (c)?
9.2 Consider (9)–(12) of at the beginning of the chapter, and give an alternative to
the genealogical interpretation that makes (9) true, (10) false, (11) true, and (12)
false.

PROBLEMS
113
9.3 Consider a language with a two-place predicate P and a one-place predicate F,
and an interpretation in which the domain is the set of persons, the denotation
of P is the relation of parent to child, and the denotation of F is the set of all
female persons. What do the following amount to, in colloquial terms, under that
interpretation?
(a) ∃z∃u∃v(u ̸= v & Puy & Pvy & Puz & Pvz & Pzx & ∼F y)
(b) ∃z∃u∃v(u ̸= v & Pux & Pvx & Puz & Pvz & Pzy & F y)
9.4 Ofﬁcially, a formation sequence is a sequence of formulas in which each either
is atomic, or is obtained by some earlier formula(s) in the sequence by negation,
conjunction, disjunction, or universal or existential quantiﬁcation. A formation
sequence for a formula F is just a formation sequence whose last formula is F.
Prove that in a formation sequence for a formula F, every subformula of F must
appear.
9.5 Prove that every formula F has a formation sequence in which the only formulas
that appear are subformulas of F, and the number of formulas that appear is no
greater than the number of symbols in F.
9.6 Here is an outline of a proof that the only subformulas of (F & G) are itself and
the subformulas of F and of G. Suppose H is some other kind of subformula. If
H does not contain the displayed ampersand, then H must be of one of the two
forms:
(a) (λ where λ is a left part of F, or
(b) ρ) where ρ is a right part of G.
If H does contain the displayed ampersand, then some subformula of H (possibly
H itself) is a conjunction (A & B) where A and B are formulas and either
(c) A = F and B is a left part λ of G,
(d) A is a right part ρ of F and B = G, or
(e) A is a right part ρ of F and B is a left part λ of G.
Show that (a) and (b) are impossible.
9.7 Continuing the preceding problem, show that (c)–(e) are all impossible.
9.8 Our deﬁnition allows the same variable to occur both bound and free in a formula,
as in P(x) & ∀xQ(x). How could we change the deﬁnition to prevent this?

10
A Pr´ecis of First-Order Logic: Semantics
This chapter continues the summary of background material on logic needed for later
chapters. Section 10.1 studies the notions of truth and satisfaction, and section 10.2 the
so-called metalogical notions of validity, implication or consequence, and (un)satisﬁa-
bility.
10.1 Semantics
Let us now turn from the ofﬁcial deﬁnitions of syntactical notions in the preceding
chapter to the ofﬁcial deﬁnitions of semantic notions. The task must be to introduce
the same level of precision and rigor into the deﬁnition of truth of a sentence in or
on or under an interpretation as we have introduced into the notion of sentence itself.
The deﬁnition we present is a version or variant of the Tarski deﬁnition of what it
is for a sentence F to be true in an interpretation M, written M |= F. (The double
turnstile |= may be pronounced ‘makes true’.)
The ﬁrst step is to deﬁne truth for atomic sentences. The ofﬁcial deﬁnition will
be given ﬁrst for the case where identity and function symbols are absent, then for
the case where they are present. (If sentence letters were admitted, they would be
atomic sentences, and specifying which of them are true and which not would be part
of specifying an interpretation; but, as we have said, we generally are not going to
admit them.) Where identity and function symbols are absent, so that every atomic
sentence has the form R(t1, . . . , tn) for some nonlogical predicate R and constants
ti, the deﬁnition is straightforward:
M |= R(t1, . . . , tn)
if and only if
RM
tM
1 , . . . , tM
n

.
(1a)
The atomic sentence is true in the interpretation just in case the relation that the
predicate is interpreted as denoting holds of the individuals that the constants are
interpreted as denoting.
When identity is present, there is another kind of atomic sentence for which a
deﬁnition of truth must be given:
M |= =(t1, t2)
if and only if
tM
1 = tM
2 .
(1b)
114

10.1. SEMANTICS
115
The atomic sentence is true in the interpretation just in case the individuals the
constants are interpreted as denoting are the same.
When function symbols are present, we need a preliminary deﬁnition of the de-
notation tM of a closed term t of a language L under an interpretation M. Clauses
(1a) and (1b) then apply, where the ti may be any closed terms, and not just constants.
For an atomic closed term, that is, for a constant c, specifying the denotation cM of c
is part of what is meant by specifying an interpretation. For more complex terms, we
proceed as follows. If f is an n-place function symbol, then specifying the denota-
tion f M is again part of what is meant by specifying an interpretation. Suppose the
denotations tM
1 , . . . , tM
n
of terms t1, . . . , tn have been deﬁned. Then we deﬁne the
denotation of the complex term f (t1, . . . , tn) to be the value of the function f M that
is the denotation of f applied to the individuals tM
1 , . . . , tM
n
that are the denotations
of t1, . . . , tn as arguments:
( f (t1, . . . , tn))M = f M
tM
1 , . . . , tM
n

.
(1c)
Since every term is built up from constants by applying function symbols a ﬁnite
number of times, these speciﬁcations determine the denotation of every term.
So, for example, in the standard interpretation of the language of arithmetic, since
0 denotes the number zero and ′ denotes the successor function, according to (1c) 0′
denotes the value obtained on applying the successor function to zero as argument,
which is to say the number one, a fact we have anticipated in abbreviating 0′ as 1.
Likewise, the denotation of 0′′ is the value obtained on applying the successor func-
tion to the denotation of 0′, namely one, as argument, and this value is of course the
number two, again a fact we have been anticipating in abbreviating 0′′ as 2. Simi-
larly, the denotation of 0′′′ is three, as is, for instance, the denotation of 0′ + 0′′. No
surprises here.
According to (1b), continuing the example, since the denotations of 0′′′ or 3 and of
0′ + 0′′ or 1 + 2 are the same, 0′′′ = 0′ + 0′′ or 3 = 1 + 2 is true, while by contrast
0′′ = 0′ + 0′′ or 2 = 1 + 2 is false. Again no surprises. According to (1a), further
continuing the example, since the denotation of < is the strict less-than relation, and
the denotations of 0′′′ or 3 and of 0′ + 0′′ or 1 + 2 are both three, the atomic
sentence 0′′′ < 0′ + 0′′ or 3 < 1 + 2 is false, while by contrast 0′′ < 0′ + 0′′ is
true. Yet again, no surprises.
There is only one candidate for what the deﬁnition should be in each of the cases
of negation and of the two junctions:
M |= ∼F
if and only if
not M |= F
(2a)
M |= (F & G)
if and only if
M |= F and M |= G
(2b)
M |= (F ∨G)
if and only if
M |= F or M |= G.
(2c)
So, for example, in the standard interpretation of the language of arithmetic, since
0 = 0 and 0 < 0′ are true while 0 < 0 is false, we have that (0 = 0 ∨0 < 0′) is true,
(0 < 0 & 0 = 0) is false, (0 < 0 & (0 = 0 ∨0 < 0′)) is false, and ((0 < 0 & 0 = 0) ∨
0 < 0′) is true. Still no surprises.

116
A PR ´ECIS OF FIRST-ORDER LOGIC: SEMANTICS
One consequence of (2a)–(2c) worth mentioning is that (F & G) is true if and only
if ∼(∼F ∨∼G) is true, and (F ∨G) is true if and only if ∼(∼F & ∼G) is true. We
could therefore if we wished drop one of the pair &, ∨from the ofﬁcial language,
and treat it as an unofﬁcial abbreviation (for an expression involving ∼and the other
of the pair) on a par with →and ↔.
The only slight subtlety in the business arises at the level of quantiﬁcation. Here is a
simple, tempting, and wrong approach to deﬁning truth for the case of quantiﬁcation,
called the substitutional approach:
M |= ∀x F(x)
if and only if
for every closed term t, M |= F(t)
M |= ∃x F(x)
if and only if
for some closed term t, M |= F(t).
In other words, under this deﬁnition a universal quantiﬁcation is true if and only if
every substitution instance is true, and an existential quantiﬁcation is true if and only
if some substitution instance is true. This deﬁnition in general produces results not
in agreement with intuition, unless it happens that every individual in the domain of
the interpretation is denoted by some term of the language. If the domain of the inter-
pretation is enumerable, we could always expand the language to add more constants
and extend the interpretation so that each individual in the domain is the denotation
of one of them. But we cannot do this when the domain is nonenumerable. (At least
we cannot do so while continuing to insist that a language is supposed to involve
only a ﬁnite or enumerable set of symbols. Of course, to allow a ‘language’ with a
nonenumerable set of symbols would involve a considerable stretching of the con-
cept. We will brieﬂy consider this extended concept of ‘language’ in a later chapter,
but for the moment we set it aside.)
10.1 Example. Consider the language L* of arithmetic and three different inter-
pretations of it: ﬁrst, the standard interpretation N*; second, the alternative interpretation
Q we considered earlier, with domain the nonnegative rational numbers; third, the similar
alternative interpretation R with domain the nonnegative real numbers. Now in fact the
substitutional approach gives the intuitively correct results for N* in all cases. Not so, how-
ever, for the other two interpretations. For, all closed terms in the language have the same
denotation in all three interpretations, and from this it follows that all closed terms denote
natural numbers. And from this it follows that t + t = 1 is false for all closed terms t,
since there is no natural number that, added to itself, yields one. So on the substitutional
approach, ∃x(x + x = 1) would come out false on all three interpretations. But intuitively
‘there is something (in the domain) that added to itself yields one’ is false only on the
standard interpretation N*, and true on the rational and real interpretations Q and R.
We could try to ﬁx this by adding more constants to the language, so that there is one
denoting each nonnegative rational number. If this were done, then on the rational and
real interpretations, 1/2 + 1/2 = 1 would come out true, and hence ∃x(x + x = 1) would
come out true using the substitutional approach, and this particular example of a problem
with the substitutional approach would be ﬁxed. Indeed, the substitutional approach would
then give the intuitively correct results for Q in all cases. Not so, however, for R. For, all
terms in the language would denote rational numbers, and from this it would follow that
t · t = 2 is false for all terms t, since there is no rational number that, multiplied by itself,

10.1. SEMANTICS
117
yields two. So on the substitutional approach, ∃x(x · x = 2) would come out false. But
intuitively, though ‘there is something (in the domain) that multiplied by itself yields two’
is false on the rational interpretation, it is true on the real interpretation. We could try to ﬁx
this by adding yet more terms to the language, but by Cantor’s theorem there are too many
real numbers to add a term for each of them while keeping the language enumerable.
The right deﬁnition for the case of quantiﬁcation has to be a little more indirect.
In deﬁning when M |= ∀x F(x) we do not attempt to extend the given language L
so as to provide constants for every individual in the domain of the interpretation at
once. In general, that cannot be done without making the language nonenumerable.
However, if we consider any particular individual in the domain, we could extend the
language and interpretation to give just it a name, and what we do in deﬁning when
M |= ∀xF(x) is to consider all possible extensions of the language and interpretation
by adding just one new constant and assigning it a denotation.
Let us say that in the interpretation M the individual m satisﬁes F(x), and write
M |= F[m], to mean ‘if we considered the extended language L ∪{c} obtained by
adding a new constant c in to our given language L, and if among all the extensions
of our given interpretation M to an interpretation of this extended language we
considered the one Mc
m that assigns c the denotation m, then F(c) would be true’:
M |= F[m]
if and only if
Mc
m |= F(c).
(3*)
(For deﬁniteness, let us say the constant to be added should be the ﬁrst constant not
in L in our ﬁxed enumeration of the stock of constants.)
For example, if F(x) is x · x = 2, then on the real interpretation of the language
of arithmetic
√
2 satisﬁes F(x), because if we extended the language by adding a
constant c and extended the interpretation by taking c to denote
√
2, then c · c = 2
would be true, because the real number denoted by c would be one that, multiplied
by itself, yields two. This deﬁnition of satisfaction can be extended to formulas with
more than one free variable. For instance, if F(x, y, z) is x · y = z, then
√
2,
√
3,
√
6
satisfy F(x, y, z), because if we added c, d, e denoting them, c · d = e would be true.
Here, then, is the right deﬁnition, called the objectual approach:
M |= ∀xF(x)
if and only if
for every m in the domain, M |= F[m]
(3a)
M |= ∃xF(x)
if and only if
for some m in the domain, M |= F[m].
(3b)
So R |= ∃xF(x) under the above deﬁnitions, in agreement with intuition, even though
there is no term t in the actual language such that R |= F(t), because R |= F[
√
2].
One immediate implication of the above deﬁnitions worth mentioning is that ∀xF
turns out to be true just in case ∼∃x ∼F is true, and ∃x F turns out to be true just
in case ∼∀x ∼F is true, so it would be possible to drop one of the pair ∀, ∃from the
ofﬁcial language, and treat it as an unofﬁcial abbreviation.
The method of proof by induction on complexity can be used to prove semantic
as well as syntactic results. The following result can serve as a warm-up for more
substantial proofs later, and provides an occasion to review the deﬁnition of truth
clause by clause.

118
A PR ´ECIS OF FIRST-ORDER LOGIC: SEMANTICS
10.2 Proposition (Extensionality lemma).
(a) Whether a sentence A is true depends only on the domain and denotations of the
nonlogical symbols in A.
(b) Whether a formula F(x) is satisﬁed by an element m of the domain depends only
on the domain, the denotations of the nonlogical symbols in F, and the element m.
(c) Whether a sentence F(t) is true depends only on the domain, the denotations of
the nonlogical symbols in F(x), and the denotation of the closed term t.
Here (a), for instance, means that the truth value of A does not depend on what
the nonlogical symbols in A themselves are, but only on what their denotations are,
and does not depend on the denotations of nonlogical symbols not in A. (So a more
formal statement would be: If we start with a sentence A and interpretation I, and
change A to B by changing zero or more nonlogical symbols to others of the same
kind, and change I to J , then the truth value of B in J will be the same as the truth
value of A in I provided J has the same domain as I, J assigns each unchanged
nonlogical symbol the same denotation I did, and whenever a nonlogical symbol S
is changed to T , then J assigns to T the same denotation I assigned to S. The proof,
as will be seen, is hardly longer than this formal statement!)
Proof: In proving (a) we consider ﬁrst the case where function symbols are absent,
so the only closed terms are constants, and proceed by induction on complexity. By
the atomic clause in the deﬁnition of truth, the truth value of an atomic sentence
depends only on the denotation of the predicate in it (which in the case of the identity
predicate cannot be changed) and the denotations of the constants in it. For a negation
∼B, assuming as induction hypothesis that (a) holds for B, then (a) holds for ∼B
as well, since by the negation clause in the deﬁnition of truth, the truth value of ∼B
depends only on the truth value of B. The cases of disjunction and conjunction are
similar.
For a universal quantiﬁcation ∀x B(x), assuming as induction hypothesis that
(a) holds for sentences of form B(c), then (b) holds for B(x), for the following
reason. By the deﬁnition of satisfaction, whether m satisﬁes B(x) depends on the
truth value of B(c) where c is a constant not in B(x) that is assigned denotation m.
[For deﬁniteness, we speciﬁed which constant was to be used, but the assumption of
(a) for sentences of form B(c) implies that it does not matter what constant is used,
so long as it is assigned denotation m.] By the induction hypothesis, the truth value
of B(c) depends only on the domain and the denotations of the nonlogical symbols
in B(c), which is to say, the denotations of the nonlogical symbols in B(x) and the
element m that is the denotation of the nonlogical symbol c, just as asserted by (b) for
B(x). This preliminary observation made, (a) for ∀x B(x) follows at once, since by
the universal quantiﬁcation clause in the deﬁnition of truth, the truth value of ∀x B(x)
depends only on the domain and which of its elements satisfy B(x). The case of
existential quantiﬁcation is the same.
If function symbols are present, we must as a preliminary establish by induction
on complexity of terms that the denotation of a term depends only on the denotations
of the nonlogical symbols occurring in it. This is trivial in the case of a constant. If it

10.2. METALOGICAL NOTIONS
119
is true for terms t1, . . . , tn, then it is true for the term f (t1, . . . , tn), since the deﬁnition
of denotation of term mentions only the denotation of the nonlogical symbol f and
the denotations of the terms t1, . . . , tn. This preliminary observation made, (a) for
atomic sentences follows, since by the atomic clause in the deﬁnition of truth, the
truth value of an atomic sentence depends only on the denotation of its predicate and
the denotations of its terms. The nonatomic cases in the proof require no change.
We have proved (b) in the course of proving (a). Having (b), the proof of (c) reduces
to showing that whether a sentence F(t) is true depends only on whether the element
m denoted by t satisﬁes F(x), which by the deﬁnition of satisfaction is to say, on
whether F(c) is true, where c is a constant having the same denotation m as t. The
proof that F(c) and F(t) have the same truth value if c and t have the same denotation
is relegated to the problems at the end of the chapter.
It is also extensionality (speciﬁcally, part (c) of Proposition 10.2) that justiﬁes
our earlier passing remarks to the effect that the substitutional approach to deﬁning
quantiﬁcation does work when every element of the domain is the denotation of some
closed term. If for some closed term t the sentence B(t) is true, then letting m be the
denotation of t, it follows by extensionality that m satisﬁes B(x), and hence ∃x B(x) is
true; and conversely, if ∃x B(x) is true, then some m satisﬁes B(x), and assuming that
every element of the domain is the denotation of some closed term, then some term
t denotes m, and by extensionality, B(t) is true. Thus under the indicated assumption,
∃xB(x) is true if and only if for some term t, B(t) is true, and similarly ∀x B(x) is
true if and only if for every term t, B(t) is true.
Similarly, if every element of the domain is the denotation of a closed term of
some special kind then ∃x B(x) is true if and only if B(t) is true for some closed term
t that is of that special kind. In particular, for the standard interpretation N* of the
language of arithmetic L*, where every element of the domain is the denotation of
one of the terms 0, 1, 2, . . . , we have
N* |= ∀x F(x)
if and only if
for every natural number m, N* |= F(m)
N* |= ∃x F(x)
if and only if
for some natural number m, N* |= F(m)
where m is the numeral for the number m (that is, the term consisting of the cipher
0 followed by m copies of the accent ′).
10.2 Metalogical Notions
Now that rigorous deﬁnitions of formula and sentence, and of satisfaction and truth,
have been given, we can proceed to the deﬁnitions of the main notions of logical
theory. A set of sentences  implies or has as a consequence the sentence D if there
is no interpretation that makes every sentence in  true, but makes D false. This
is the same as saying that every interpretation that makes every sentence in  true
makes D true. (Or almost the same. Actually, if D contains a nonlogical symbol not
in , an interpretation might make  true but assign no denotation to this symbol and
therefore no truth value to D. But in such a case, however the denotation is extended
to assign a denotation to any such symbols and therewith a truth value to D,  will

120
A PR ´ECIS OF FIRST-ORDER LOGIC: SEMANTICS
still be true by the extensionality lemma, so D cannot be false and must be true. To
avoid fuss over such points, in future we tacitly understand ‘every interpretation’ to
mean ‘every interpretation that assigns denotations to all the nonlogical symbols in
whatever sentences we are considering’.) We use ‘makes every sentence in  true’
and ‘makes  true’ interchangeably, and likewise ‘the sentences in the set  imply
D’ and ‘ implies D’. When  contains but a single sentence C (in symbols, when
 = {C}), we use ‘ implies D’ and ‘C implies D’ interchangeably. Let us give a
few examples. There are more in the problems at the end of the chapter (and many,
many, many more in introductory textbooks).
10.3 Example. Some implication principles
(a)
∼∼B implies B.
(b)
B implies (B ∨C) and C implies (B ∨C).
(c)
∼(B ∨C) implies ∼B and ∼C.
(d)
B(t) implies ∃x B(x).
(e)
∼∃x B(x) implies ∼B(t).
(f)
s = t and B(s) imply B(t).
Proofs: For (a), by the negation clause in the deﬁnition of truth, in any interpre-
tation, if ∼∼B is true, then ∼B must be false, and B must be true. For (b), by the
disjunction clause in the deﬁnition of truth, in any interpretation, if B is true, then
(B ∨C) is true; similarly for C. For (c), by what we have just shown, any interpre-
tation that does not make (B ∨C) true cannot make B true; hence any intepretation
that makes ∼(B ∨C) true makes ∼B true; and similarly for ∼C. For (d), in any
interpretation, by the extensionality lemma B(t) is true if and only if the element m
of the domain that is denoted by t satisﬁes B(x), in which case ∃x B(x) is true. As
for (e), it follows from what we have just shown much as (c) follows from (b). For
(f), by the identity clause in the deﬁnition of truth, in any interpretation, if s = t is
true, then s and t denote the same element of the domain. Then by the extensionality
lemma B(s) is true if and only if B(t) is true.
There are two more important notions to go with implication or consequence.
A sentence D is valid if no interpretation makes D false. In this case, a fortiori
no interpretation makes  true and D false;  implies D for any . Conversely, if
every  implies D, then since for every interpretation there is a set of sentences  it
makes true, no interpretation can make D false, and D is valid. A set of sentences  is
unsatisﬁable if no interpretation makes  true (and is satisﬁable if some interpretation
does). In this case, a fortiori no interpretation makes  true and D false, so  implies
D for any D. Conversely, if  implies every D, then since for every interpretation
there is a sentence it makes false, there can be no interpretation making  true, and
 is unsatisﬁable.
Notions such as consequence, unsatisﬁability, and validity are often called ‘meta-
logical’ in contrast to the notions of negation, conjunction, disjunction, and univer-
sal and existential quantiﬁcation, which are simply called ‘logical’. Terminology
aside, the difference is that there are symbols ∼, &, ∨, ∀, ∃in our formal language

10.2. METALOGICAL NOTIONS
121
(the ‘object language’) for negation and the rest, whereas words like ‘consequence’
only appear in the unformalized prose, the mathematical English, in which we talk
about the formal language (the ‘metalanguage’).
Just as for implication or consequence, so for validity and for unsatisﬁability
and satisﬁability, there are innumerable little principles that follow directly from
the deﬁnitions. For instance: if a set is satisﬁable, then so is every subset (since an
interpretation making every sentence in the set true will make every sentence in the
subset true); no set containing both a sentence and its negation is satisﬁable (since no
interpretation makes them both true); and so on. The plain assertions of Example 10.3
can each be elaborated into fancier versions about validity and (un)satisﬁability, as
we next illustrate in the case of 10.3(a).
10.4 Example. Variations on a theme
(a)
∼∼B implies B.
(b)
If  implies ∼∼B, then  implies B.
(c)
If B implies D, then ∼∼B implies D.
(d)
If  ∪{B} implies D, then  ∪{∼∼B} implies D.
(e)
If ∼∼B is valid, then B is valid.
(f)
If  ∪{B} is unsatisﬁable, then  ∪{∼∼B} is unsatisﬁable.
(g)
If  ∪{∼∼B} is satisﬁable, then  ∪{B} is satisﬁable.
Proof: (a) is a restatement of 10.3(a). For (b), we are given that every interpretation
that makes  true makes ∼∼B true, and want to show that any interpretation that
makes  true makes B true. But this is immediate from (a), which says that any
interpretation that makes ∼∼B true makes B true. For (c), we are given that any
interpretation that makes B true makes D true, and want to show that any interpretation
that makes ∼∼B true makes D true. But again, this is immediate from the fact that
any interpretation that makes ∼∼B true makes B true. In (d),  ∪{B} denotes the
result of adding B to . The proof in this case is a combination of the proofs of (b)
and (c). For (e), we are given that every interpretation makes ∼∼B true, and want
to show that every interpretation makes B true, while for (f), we are given that no
interpretation makes  and B true, and want to show that no interpretation makes 
and ∼∼B true. But again both are immediate from (a), that is, from the fact that every
interpretation that makes ∼∼B true makes B true. Finally, (g) is immediate from (f).
We could play the same game with any of 10.3(b)–10.3(f). Some results exist only
in the fancy versions, so to speak.
10.5 Example. Some satisﬁability principles
(a)
If  ∪{(A ∨B)} is satisﬁable, then either  ∪{A} is satisﬁable, or  ∪{B} is
satisﬁable.
(b)
If  ∪{∃xB(x)} is satisﬁable, then for any constant c not occurring in  or ∃xB(x),
 ∪{B(c)} is satisﬁable.
(c)
If  is satisﬁable, then  ∪{t = t} is satisﬁable.

122
A PR ´ECIS OF FIRST-ORDER LOGIC: SEMANTICS
Proof: For (a), we are given that some interpretation makes  and A ∨B true,
and want to show that some interpretation makes  and A true, or some makes 
and B true. In fact, the same interpretation that makes  and A ∨B true either
makes A true or makes B true, by the disjunction clause in the deﬁnition of truth. For
(b), we are given that some interpretation makes  and ∃x B(x) true, and want to show
that some interpretation makes  and B(c) true, assuming c does not occur in  or
∃x B(x). Well, since ∃x B(x) is true, some element m of the domain satisﬁes B(x).
And since c does not occur in  or ∃x B(x), we can change the interpretation to make
m the denotation of c, without changing the denotations of any nonlogical symbols in
 or ∃x B(x), and so by extensionality not changing their truth values. But then  is
still true, and since m satisﬁes B(x), B(c) is also true. For (c), we are given that some
interpretation makes  true and want to show that some interpretation makes  and
t = t true. But any interpretation makes t = t true, so long as it assigns a denotation
to each nonlogical symbol in t, and if our given interpretation does not, it at least
assigns a denotation to every nonlogical symbol in t that occurs in , and if we extend
it to assign denotations to any other nonlogical symbols in t, by extensionality  will
still be true, and now t = t will be true also.
There is one more important metalogical notion: two sentences are equivalent over
an interpretation M if they have the same truth value. Two formulas F(x) and G(x)
are equivalent over M if, taking a constant c occurring in neither, the sentences F(c)
and G(c) are equivalent over every interpretation Mc
mobtained by extending M to
provide some denotation m for c. Two sentences are (logically) equivalent if they
are equivalent over all interpretations. Two formulas F(x) and G(x) are (logically)
equivalent if, taking a constant c occurring in neither, the sentences F(c) and G(c) are
(logically) equivalent. A little thought shows that formulas are (logically) equivalent
if they are equivalent over every interpretation. The deﬁnitions may be extended to
formulas with more than one free variable. We leave the development of the basic
properties of equivalence entirely to the problems.
Before closing this chapter and bringing on those problems, a remark will be in
order. The method of induction on complexity we have used in this chapter and
the preceding to prove such unexciting results as the parenthesis and extensionality
lemmas will eventually be used to prove some less obvious and more interesting
results. Much of the interest of such results about formal languages depends on their
being applicable to ordinary language. We have been concerned here mainly with
how to read sentences of our formal language in ordinary language, and much less
with writing sentences of ordinary language in our formal language, so we need to
say a word about the latter topic.
In later chapters of this book there will be many examples of writing assertions
from number theory, the branch of mathematics concerned with the natural numbers,
as ﬁrst-order sentences in the language of arithmetic. But the full scope of what can
be done with ﬁrst-order languages will not be apparent from these examples, or this
book, alone. Works on set theory give examples of writing assertions from other
branches of mathematics as ﬁrst-order sentences in a language of set theory, and
make it plausible that in virtually all branches of mathematics, what we want to say

PROBLEMS
123
can be said in a ﬁrst-order language. Works on logic at the introductory level contain
a wealth of examples of how to say what we want to say in a ﬁrst-order language
from outside mathematics (as in our genealogical examples).
But this cannot always be done outside of mathematics, and some of our results do
not apply unrestrictedly to ordinary language. A case in point is unique readability.
In ordinary language, ambiguous sentences of the type ‘A and B or C’ are perfectly
possible. Of course, though possible, they are not desirable: the sentence ought to
be rewritten to indicate whether ‘A, and either B or C’ or ‘Either A and B, or C’
is meant. A more serious case in point is extensionality. In ordinary language it is
not always the case that one expression can be changed to another denoting the same
thing without altering truth values. To give the classic example, Sir Walter Scott was
the author of the historical novel Waverley, but there was a time when this fact was
unknown, since the work was originally published anonymously. At that time, ‘It is
known that Scott is Scott’ was as always true, but ‘It is known that the author of
Waverley is Scott’ was false, even though ‘Scott’ and ‘the author of Waverly’ had the
same denotation.
To put the matter another way, writing s for ‘Scott’ and t for ‘the author of
Waverley’, and writing A(x) for ‘x is Scott’ and □for ‘it is known that’, what we have
just said is that s = t and □A(s) may be true without □A(t) being true, in contrast to
one of our examples above, according to which, in our formal languages, s = t and
B(s) always imply B(t). There is no contradiction with our example, of course, since
our formal languages do not contain any operator like □; but for precisely this reason,
not everything that can be expressed in ordinary language can be expressed in our
formal languages. There is a separate branch of logic, called modal logic, devoted to
operators like □, and we are eventually going to get a peek at a corner of this branch
of logic, though only in the last chapter of the book.
Problems
10.1 Complete the proof of the extensionality lemma (Proposition 10.2) by show-
ing that if c is a constant and t a closed term having the same denotation,
then substituting t for c in a sentence does not change the truth value of the
sentence.
10.2 Show that ∃y∀x R(x, y) implies ∀x∃yR(x, y).
10.3 Show that ∀x∃yF(x, y) does not imply ∃y∀x F(x, y) .
10.4 Show that:
(a) If the sentence E is implied by the set of sentences  and every sentence
D in  is implied by the set of sentences , then E is implied by .
(b) If the sentence E is implied by the set of sentences  ∪ and every
sentence D in  is implied by the set of sentences , then E is implied
by .
10.5 Let ∅be the empty set of sentences, and let ⊥be any sentence that is not true
on any interpretation. Show that:
(a) A sentence D is valid if and only if D is a consequence of ∅.
(b) A set of sentences  is unsatisﬁable if and only if ⊥is a consequence of .

124
A PR ´ECIS OF FIRST-ORDER LOGIC: SEMANTICS
10.6 Show that:
(a) {C1, . . . , Cm} is unsatisﬁable if and only if ∼C1 ∨· · · ∨∼Cm is valid.
(b) D is a consequence of {C1, . . . , Cm} if and only if ∼C1 ∨· · · ∨∼Cm∨D
is valid.
(c) D is a consequence of {C1, . . . , Cm} if and only if {C1, . . . , Cm, ∼D} is
unsatisﬁable.
(d) D is valid if and only if ∼D is unsatisﬁable.
10.7 Show that B(t) and ∃x(x = t & B(x)) are logically equivalent.
10.8 Show that:
(a) (B & C) implies B and implies C.
(b) ∼B implies ∼(B & C), and ∼C implies ∼(B & C).
(c) ∀x B(x) implies B(t).
(d) ∼B(t) implies ∼∀x B(x).
10.9 Show that:
(a) If  ∪{∼(B & C)} is satisﬁable, then either  ∪{∼B} is satisﬁable or
 ∪{∼C} is satisﬁable.
(b) If  ∪{∼∀x B(x)} is satisﬁable, then for any constant c not occurring in
 or ∀x B(x),  ∪{∼B(c)} is satisﬁable.
10.10 Showthatthefollowingholdforequivalenceoveranyinterpretation(andhence
for logical equivalence), for any sentences (and hence for any formulas):
(a) F is equivalent to F.
(b) If F is equivalent to G, then G is equivalent to F.
(c) If F is equivalent to G and G is equivalent to H, then F is equivalent to H.
(d) If F and G are equivalent, then ∼F and ∼G are equivalent.
(e) If F1 and G1 are equivalent, and F2 and G2 are equivalent, then F1 & F2
and G1 & G2 are equivalent, and similarly for ∨.
(f) If c does not occur in F(x) or G(x), and F(c) and G(c) are equivalent,
then ∀x F(x) and ∀xG(x) are equivalent, and similarly for ∃.
10.11 (Substitution of equivalents.) Show that the following hold for equivalence
over any interpretation (and hence for logical equivalence):
(a) If sentence G results from sentence F on replacing each occurrence of an
atomic sentence A by an equivalent sentence B, then F and G are
equivalent.
(b) Show that the same holds for an atomic formula A and an equivalent
formula B (provided, to avoid complications, that no variable occurring
in A occurs bound in B or F).
(c) Show that the same holds even when A is not atomic.
10.12 Showthat F(x)is(logically)equivalentto G(x)ifandonlyif∀x(F(x) ↔G(x))
is valid.
10.13 (Relettering bound variables.) Show that:
(a) If F is a formula and y a variable not occurring free in F, then F is
(logically) equivalent to a formula in which y does not occur at all. The
same applies to any number of variables y1, . . . , yn.
(b) Every formula is logically equivalent to a formula having no subformulas
in which the same variable occurs both free and bound.

PROBLEMS
125
10.14 Show that the following pairs are equivalent:
(a) ∀x F(x) & ∀yG(y) and ∀u(F(u) & G(u)).
(b) ∀x F(x) ∨∀yG(y) and ∀u∀v(F(u) ∨G(v)).
(c) ∃x F(x) & ∃yG(y) and ∃u∃v(F(u) & G(v)).
(d) ∃x F(x) ∨∃yG(y) and ∃u(F(u) ∨G(u)).
[In (a), it is to be understood that u may be a variable not occurring free in
∀x F(x) or ∀yG(y); in particular, if x and y are the same variable, u may be
that same variable. In (b) it is to be understood that u and v may be any distinct
variables not occurring free in ∀x F(x) ∨∀yG(y); in particular, if x does not
occur in free in ∀yG(y) and y does not occur free in ∀x F(x), then u may be
x and y may be v. Analogously for (d) and (c).]

11
The Undecidability of First-Order Logic
This chapter connects our work on computability with questions of logic. Section 11.1
presupposes familiarity with the notions of logic from Chapter 9 and 10 and of Turing
computability from Chapters 3–4, including the fact that the halting problem is not
solvable by any Turing machine, and describes an effective procedure for producing,
given any Turing machine M and input n, a set of sentences  and a sentence D such
that M given input n will eventually halt if and only if  implies D. It follows that if there
were an effective procedure for deciding when a ﬁnite set of sentences implies another
sentence, then the halting problem would be solvable; whereas, by Turing’s thesis, the
latter problem is not solvable, since it is not solvable by a Turing machine. The upshot
is, one gets an argument, based on Turing’s thesis for (the Turing–B¨uchi proof of)
Church’s theorem, that the decision problem for implication is not effectively solvable.
Section 11.2 presents a similar argument—the G¨odel-style proof of Church’s theorem—
this time using not Turing machines and Turing’s thesis, but primitive recursive and
recursive functions and Church’s thesis, as in Chapters 6–7. The constructions of the
two sections, which are independent of each other, are both instructive; but an entirely
different proof, not dependent on Turing’s or Church’s thesis, will be given in a later
chapter, and in that sense the present chapter is optional. (After the present chapter we
return to pure logic for the space of several chapters, to resume to the application of
computability theory to logic with Chapter 15.)
11.1 Logic and Turing Machines
We are going to show how, given the machine table or ﬂow chart or other suitable
presentation of a Turing machine, and any n, we can effectively write down a ﬁnite
set of sentences  and a sentence D such that  implies D if and only if the machine
in question does eventually halt when started with input n, that is, when started in its
initial state scanning the leftmost of a block of n strokes on an otherwise blank tape.
It follows that if the decision problem for logical implication could be solved, that is,
if an effective method could be devised that, applied to any ﬁnite set of sentences 
and sentence D, would in a ﬁnite amount of time tell us whether or not  implies D,
then the halting problem for Turing machines could be solved, or in other words, an
effective method would exist that, applied to any suitably presented Turing machine
andnumbern,wouldinaﬁniteamountoftimetelluswhetherornotthatmachinehalts
when started with input n. Since we have seen in Chapter 4 that, assuming Turing’s
126

11.1. LOGIC AND TURING MACHINES
127
thesis, the halting problem is not solvable, it follows that, again assuming Turing’s
thesis, the decision problem is unsolvable, or, as is said, that logic is undecidable.
In principle this section requires only the material of Chapters 3–4 and 9–10. In
practice some facility at recognizing simple logical implications will be required:
we are going to appeal freely to various facts about one sentence implying another,
leaving the veriﬁcation of these facts largely to the reader.
We begin by introducing simultaneously the language in which the sentences in 
and the sentence D will be written, and its standard interpretation M. The language
interpretation will depend on what machine and what input n we are considering. The
domain of M will in all cases be the integers, positive and zero and negative. The
nonnegative integers will be used to number the times when the machine is operating:
the machine starts at time 0. The integers will also be used to number the squares
on the tape: the machine starts at square 0, and the squares to the left and right are
numbered as in Figure 11-1.
…
-4
-3
-2
-1
0
1
2
3
4
…
Figure 11-1. Numbering the squares of a Turing tape.
There will be a constant 0, whose denotation in the standard interpretation will
be zero, and two-place predicates S and <, whose denotations will be the successor
relation (the relation an integer n bears to n + 1 and nothing else) and the usual order
relation, respectively. To save space, we write Suv rather than S(u, v), and similarly
for other predicates. As to such other predicates, there will further be, for each of the
(nonhalted) states of the machine, numbered let us say from 1 (the initial state) to k,
a one-place predicate. In the standard interpretation, Qi will denote the set of t ≥0
such that at the time numbered t the machine is in the state numbered i. Besides this
we need two more two-place predicates @ and M. The denotation of the former will
be the set of pairs of integers t ≥0 and x such that at the time number t, the machine
is at the square numbered x. The denotation of the latter will be the set of t ≥0 and
x such that at time t, square x is ‘marked’, that is, contains a stroke rather than a
blank. (We use t as the variable when a time is intended, and x and y when squares
are intended, as a reminder of the standard interpretation. Formally, the function of
a variable is signalled by its position in the ﬁrst or the second place of the predicate
@ or M.) It would be easy to adapt our construction to the case where more symbols
than just the stroke and the blank are allowed, but for present purposes there is no
reason to do so.
We must next describe the sentences that are to go into  and the sentence D.
The sentences in  will fall into three groups. The ﬁrst contains some ‘background
information’ about S and < that would be the same for any machine and any input. The
second consists of a single sentence speciﬁc to the input n we are considering. The
third consists of one sentence for each ‘normal’ instruction of the speciﬁc machine
we are considering, that is, for each instruction except for those telling us to halt.

128
THE UNDECIDABILITY OF FIRST-ORDER LOGIC
The ‘background information’ is provided by the following:
∀u∀v∀w(((Suv & Suw) →v = w) & ((Svu & Swu) →v = w))
(1)
∀u∀v(Suv →u < v) & ∀u∀v∀w((u < v & v < w) →u < w)
(2)
∀u∀v(u < v →u ̸= v).
(3)
These say that a number has only one successor and only one predecessor, that a
number is less than its predecessor, and so on, and are all equally true in the standard
interpretation.
It will be convenient to introduce abbreviations for the mth-successor relation,
writing
S0uv
for u = v
S1uv
for Suv
S2uv
for ∃y(Suy & Syv)
S3uv
for ∃y1∃y2(Suy1 & Sy1y2 & Sy2v)
and so on. (In S2, y may be any convenient variable distinct from u and v; for
deﬁniteness let us say the ﬁrst on our ofﬁcial list of variables. Similarly for S3.) The
following are then true in the standard interpretation.
∀u∀v∀w(((Smuv & Smuw) →v = w) & ((Smvu & Smwu) →v = w))
(4)
∀u∀v(Smuv →u < v)
if m ̸= 0
(5)
∀u∀v(Smuv →u ̸= v)
if m ̸= 0
(6)
∀u∀v∀w((Smwu & Suv) →Skwv)
if k = m + 1
(7)
∀u∀v∀w((Skwv & Suv) →Smwu)
if m = k −1.
(8)
Indeed, these are logical consequences of (1)–(3) and hence of , true in any in-
terpretation where  is true: (4) follows on repeated application of (1); (5) follows
on repeated application of (2); (6) follows from (3) and (5); (7) is immediate from
the deﬁnitions; and (8) follows from (7) and (1). If we also write S−muv for Smvu,
(4)–(8) still hold.
We need some further notational conventions before writing down the remaining
sentences of . Though ofﬁcially our language contains only the numeral 0 and
not numerals 1, 2, 3, or −1, −2, −3, it will be suggestive to write y = 1, y = 2,
y = −1, and the like for S1(0, y), S2(0, y), S−1(0, y), and so on, and to understand
the application of a predicate to a numeral in the natural way, so that, for instance,
Qi2 and S2u abbreviate ∃y(y = 2 & Qi y) and ∃y(y = 2 & Syu). A little thought
shows that with these conventions (6)–(8) above (applied with 0 for w) give us the
following wherein p, q, and so on, are the numerals for the numbers p, q, and so
on:
p ̸= q
if p ̸= q
(9)
∀v(Smv →v = k)
where
k = m + 1
(10)
∀u(Suk →u = m)
where
m = k −1.
(11)

11.1. LOGIC AND TURING MACHINES
129
These abbreviatory conventions enable us to write down the remaining sentences of
 comparatively compactly.
The one member of  pertaining to the input n is a description of (the conﬁguration
at) time 0, as follows:
Q00 & @00 & M00 & M01 & . . . & M0n &
(12)
∀x((x ̸= 0 & x ̸= 1 & . . . & x ̸= n −1) →∼M0x).
This is true in the standard interpretation, since at time 0 the machine is in state 1,
at square 0, with squares 0 through n marked to represent the input n, and all other
squares blank.
To complete the speciﬁcation of , there will be one sentence for each nonhalting
instruction, that is, for each instruction of the following form, wherein j is not the
halted state:
If you are in state i and are scanning symbol e,
(∗)
then —— and go into state j.
In writing down the corresponding sentence of , we use one further notational
convention, sometimes writing M as M1 and ∼M as M0. Thus Metx says, in the
standard interpretation, that at time t, square x contains symbol e (where e = 0
means the blank, and e = 1 means the stroke). Then the sentence corresponding to
(*) will have the form
∀t∀x((Qit &@tx & Metx) →
(13)
∃u(Stu & —— & Q ju &
∀y((y ̸= x & M1ty) →M1uy) & ∀y((y ̸= x & M0ty) →M0uy))).
The last two clauses just say that the marking of squares other than x remains un-
changed from one time t to the next time u.
Whatgoesintothespace‘——’in(13)dependsonwhatgoesintothecorresponding
space in (*). If the instruction is to (remain at the same square x but) print the symbol
d, the missing conjunct in (9) will be
@ux & Mdux.
If the instruction is to move one square to the right or left (leaving the marking of the
square x as it was), it will instead be
∃y(S±1xy & @uy & (Mux ↔Mtx))
(with the minus sign for left and the plus sign for right). A little thought shows that
when ﬁlled in after this fashion, (13) exactly corresponds to the instruction (*), and
will be true in the standard interpretation.
This completes the speciﬁcation of the set . The next task is to describe the
sentence D. To obtain D, consider a halting instruction, that is, an instruction of the
type
If you are in state i and are scanning symbol e, then halt.
(†)

130
THE UNDECIDABILITY OF FIRST-ORDER LOGIC
For each such instruction write down the sentence
∃t∃x(Qit & @tx & Metx).
(14)
This will be true in the standard interpretation if and only if in the course of its
operations the machine eventually comes to a conﬁguration where the applicable
instruction is (†), and halts for this reason. We let D be the disjunction of all sentences
of form (14) for all halting instructions (†). Since the machine will eventually halt if
and only if it eventually comes to a conﬁguration where the applicable instruction is
some halting instruction or other, the machine will eventually halt if and only if D is
true in the standard interpretation.
We want to show that  implies D if and only if the given machine, started with the
given input, eventually halts. The ‘only if’ part is easy. All sentences in  are true in
the standard interpretation, whereas D is true only if the given machine started with
the given input eventually halts. If the machine does not halt, we have an interpretation
where all sentences in  are true and D isn’t, so  does not imply D.
For the ‘if’ part we need one more notion. If a ≥0 is a time at which the machine
has not (yet) halted, we mean by the description of time a the sentence that does for
a what (12) does for 0, telling us what state the machine is in, where it is, and which
squares are marked at time a. In other words, if at time a the machine is in state i, at
square p, and the marked squares are q1, q2 , . . . , qm, then the description of time a
is the following sentence:
Qia & @ap & Maq1 & Maq2 & . . . & Maqm &
(15)
∀x((x ̸= q1 & x ̸= q2 & . . . & x ̸= qm) →∼Max).
It is important to note that (15) provides, directly or indirectly, the information
whether the machine is scanning a blank or a stroke at time a. If the machine is
scanning a stroke, then p is one of the qr for 1 ≤r ≤m, and M1ap, which is to
say Map, is actually a conjunct of (15). If the machine is scanning a blank, then p
is different from each of the various numbers q. In this case M0ap, which is to say
∼Map, is implied by (15) and . Brieﬂy put, the reason is that (9) gives p ̸= qr for
each qr, and then the last conjuct of (15) gives ∼Map.
[Less brieﬂy but more accurately put, what the last conjunct of (15) abbreviates
amounts to
∀x((∼Sq 10x & . . . ∼Sqm0x) →∼∃t(Sa0t & Mtx)).
What (9) applied to p and qr abbreviates is
∼∃x((Spx & Sqr x).
These together imply
∼∃t∃x(S0t & S0x & Mtx)
which amounts to what ∼Map abbreviates.]
If the machine halts at time b = a + 1, that means that at time a we had conﬁgura-
tion for which the applicable instruction as to what to do next was a halting instruction
of form (†). In that case, Qia and @ap will be conjuncts of the description of time

11.1. LOGIC AND TURING MACHINES
131
a, and Meap will be either a conjunct of the description also (if e = 1) or a logical
implication of the description and  (if e = 0). Hence (14) and therefore D will be a
logical implication of  together with the description of time a. What if the machine
does not halt at time b = a + 1?
11.1 Lemma. If a ≥0, and b = a +1 is a time at which the machine has not (yet)
halted, then  together with the description of time a implies the description of time b.
Proof: The proof is slightly different for each of the four types of instructions
(print a blank, print a stroke, move left, move right). We do the case of printing a
stroke, and leave the other cases to the reader. Actually, this case subdivides into
the unusual case where there is already a stroke on the scanned square, so that the
instruction is just to change state, and the more usual case where the scanned square
is blank. We consider only the latter subcase.
So the description of time a looks like this:
Qia & @ap & Maq1 & Maq2 & . . . & Maqm &
(16)
∀x((x ̸= q1 & x ̸= q2 & . . . & x ̸= qm) →∼Max)
where p ̸= qr for any r, so  implies p ̸= qr by (9), and, by the argument given earlier,
 and (16) together imply ∼Map. The sentence in  corresponding to the applicable
instruction looks like this:
∀t∀x((Qit & @tx & ∼Mtx) →
(17)
∃u(Stu & @ux & Mux & Q ju & ∀y((y ̸= x & Mty) →Muy)
& ∀y((y ̸= x & ∼Mty) →∼Muy))).
The description of time b looks like this:
Q jb & @bp & Mbp & Mbq1 & Mbq2 & . . . & Mbqm &
(18)
∀x((x ̸= p & x ̸= q1 & x ̸= q2 & . . . & x ̸= qm) →∼Mbx).
And, we submit, (18) is a consequence of (16), (17), and .
[Brieﬂy put, the reason is this. Putting a for t and p for x in (17), we get
(Qia & @ap & ∼Map) →
∃u(Sau & @up & Mup & Q ju &
∀y((y ̸= p & May) →Muy) & ∀y((y ̸= p & ∼May) →∼Muy)).
Since (16) and  imply Qia & @ap & ∼Map, we get
∃u(Sau & @up & Mup & Q ju &
∀y((y ̸= p & May) →Muy) & ∀y((y ̸= p & ∼May) →∼Muy)).
By (10), Sau gives u = b, where b = a + 1, and we get
@bp & Mbp & Q jb &
∀y((y ̸= p & May) →Mby) & ∀y((y ̸= p & ∼May) →∼Mby).
The ﬁrst three conjuncts of this last are the same, except for order, as the ﬁrst three
conjuncts of (18). The fourth conjunct, together with p ̸= qk from (9) and the conjunct

132
THE UNDECIDABILITY OF FIRST-ORDER LOGIC
Maqk of (16), gives the conjunct Mbqk of (18). Finally, the ﬁfth conjunct together
with the last conjunct of (16) gives the last conjunct of (18). The reader will see
now what we meant when we said at the outset, ‘Some facility at recognizing simple
logical implications will be required.’]
Now the description of time 0 is one of the sentences in . By the foregoing lemma,
if the machine does not stop at time 1, the description of time 1 will be a consequence
of , and if the machine then does not stop at time 2, the description of time 2 will
be a consequence of  together with the description of time 1 (or, as we can more
simply say, since the description of time 1 is a consequence of , the description of
time 2 will be a consequence of ), and so on until the last time a before the machine
halts, if it ever does. If it does halt at time a + 1, we have seen that the description
of time a, which we now know to be a consequence of , implies D. Hence if the
machine ever halts,  implies D.
Hence we have established that if the decision problem for logical implication were
solvable, the halting problem would be solvable, which (assuming Turing’s thesis)
we know it is not. Hence we have established the following result, assuming Turing’s
thesis.
11.2 Theorem (Church’s theorem). The decision problem for logical implication is
unsolvable.
11.2 Logic and Primitive Recursive Functions
By the nullity problem for a two-place primitive recursive function f we mean the
problem of devising an effective procedure that, given any m, would in a ﬁnite amount
of time tell us whether or not there is an n such that f (m, n) = 0. We are going to show
how, given f , to write down a certain ﬁnite set of sentences  and a certain formula
D(x) in a language that contains the constants 0 and the successor symbol ′ from
the language of arithmetic, and therefore contains the numerals 0′, 0′′, 0′′′, . . . or
1, 2, 3, . . . as we usually write them. And then we are going to show that for any m,
 implies D(m) if and only if there is an n such that f (m, n) = 0. It follows that if the
decision problem for logical implication could be solved, and an effective method
devised to tell whether or not a given ﬁnite set of sentences  implies a sentence D,
then the nullity problem for any f could be solved. Since it is known that, assuming
Church’s thesis, there is an f for which the nullity problem is not solvable, it follows,
again assuming Church’s thesis, that the decision problem for logical implication is
unsolvable, or, as is said, that logic is undecidable. The proof of the fact just cited
about the unsolvability of the nullity problem requires the apparatus of Chapter 8,
but for the reader who is willing to take this fact on faith, this section otherwise
presupposes only the material of Chapters 6–7 and 9–10.
To begin the construction, the function f , being primitive recursive, is built up from
the basic functions (successor, zero, the identity functions) by the two processes of
composition and primitive recursion. We can therefore make a ﬁnite list of primitive
recursive functions f0, f1, f2, . . . , fr, such that for each i from 1 to r, fi is either the
zero function or the successor function or one of the identity functions, or is obtained

11.2. LOGIC AND PRIMITIVE RECURSIVE FUNCTIONS
133
from earlier functions in the list by composition or primitive recursion, with the last
function fr being the function f . We introduce a language with the symbol 0, the
successor symbol ′, and a function symbol fi of the appropriate number of places for
each of the functions fi. In the standard interpretation of the language, the domain
will be the natural numbers, 0 will denote zero, ′ will denote the successor function,
and each fi will denote fi, so that in particular fr will denote f .
The set of sentences  will consist of one or two sentence for each fi for i > 0. In
case fi is the zero function, the sentence will be
∀x fi(x) = 0.
(1)
In case fi is the successor function, the sentence will be
∀x fi(x) = x′.
(2)
(In this case fi will be another symbol besides ′ for the successor function; but it does
not matter if we happen to have two symbols for the same function.) In case fi is the
identity function idn
k , the sentence will be
∀x1 · · · ∀xn fi(x1, . . . , xn) = xk.
(3)
If case fi is obtained from fk and f j1, . . . f jp, where j1 , . . . , jp and k are all < i, by
composition, the sentence will be
∀x fi(x) = fk(f j1(x), . . . f jp(x)).
(4)
In case fi is obtained from f j and fk, where j and k are <i, by primitive recursion,
there will be two sentences, as follows.
∀x fi(x, 0) = f j(x).
(5a)
∀x∀y fi(x, y′) = fk(x, y, fi(x, y)).
(5b)
[In (4) and (5) we have written x and ∀x for x1 , . . . , xn and ∀x1 · · · ∀xn.] Clearly all
these sentences are true in the intended interpretation. The formula D(x) will simply
be ∃y fr(x, y) = 0. For given m, the sentence D(m) will be true in the standard
interpretation if and only if there is an n with f (m, n) = 0.
We want to show that for any m, D(m) will be implied by  if and only if there is
an n with f (m, n) = 0. The ‘only if’ part is easy. All sentences in  are true in the
standard interpretation, whereas D(m) is true only if there is an n with f (m, n) = 0.
If there is no such n, we have an interpretation where also sentences in  are true and
D(m) isn’t, so  does not imply D(m).
For the ‘if’ part we need one more notion. Call  adequate for the function fi if
whenever fi(a) = b, then fi(a) = b is implied by . (We have written a for a1 , . . . , an
and a for a1 , . . . , an.) The presence of (1)–(3) in  guarantees that it is adequate for
any fi that is a basic function (zero, successor, or an identity function). What about
more complicated functions?

134
THE UNDECIDABILITY OF FIRST-ORDER LOGIC
11.3 Lemma
(a) If fi is obtained by composition from functions fk and f j1, . . . f jp for which  is
adequate, then  is adequate also for fi.
(b) If fi is obtained by primitive recursion from functions f j and fk for which  is
adequate, then  is adequate also for fi.
Proof: We leave (a) to the reader and do (b). Given a, b, and c with fi(a, b) = c, for
each p ≤b let cp = fi(a, p), so that cb = c. Note that since fi is obtained by primitive
recursion from f j and fk, we have
c0 = fi(a, 0) = f j(a)
and for all p < b we have
cp′ = fi(a, p′) = fk(a, p, fi(a, p)) = fk(a, p, cp).
Since  is adequate for f j and fk,
f j(a, 0) = c0
(6a)
fk(a, p, cp) = cp′
(6b)
are consequences of . But (6a) and (5a) imply
fi(a, 0) = c0
(7a)
while (6b) and (5b) imply
fi(a, p) = cp →fi(a, p′) = cp′.
(7b)
But (7a) and (7b) for p = 0 imply fi(a, 1) = c1, which with (7b) for p = 1 im-
plies fi(a, 2) = c2, which with (7b) for p = 2 implies fi(a, 3) = c3, and so on up
to fi(a, b) = cb = c, which is what needed to be proved to show  adequate for fi.
Since every fi is either a basic function or obtained from earlier functions on our
list by the processes covered by Lemma 11.3, the lemma implies that  is adequate
for all the functions on our list, including fr = f . In particular, if f (m, n) = 0, then
fr(m, n) = 0 is implied by , and hence so is ∀y fr(m, y) = 0, which is D(m).
Thus we have reduced the problem of determining whether for some n we have
f (m, n) = 0 to the problem of determining whether  implies D(m). That is, we
have established that if the decision problem for logical implication were solvable,
the nullity problem for f would be solvable, which it is known, as we have said, that
it is not, assuming Church’s thesis. Hence we have established the following result,
assuming Church’s thesis.
11.4 Theorem (Church’s theorem). The decision problem for logical implication is
unsolvable.
Problems
11.1 The decision problem for validity is the problem of devising an effective pro-
cedure that, applied to any sentence, would in a ﬁnite amount of time enable
one to determine whether or not it is valid. Show that the unsolvability of

PROBLEMS
135
the decision problem for implication (Theorem 11.2, or equivalently Theorem
11.4) implies the unsolvability of the decision problem for validity.
11.2 The decision problem for satisﬁability is the problem of devising an effective
procedure that, applied to any ﬁnite set of sentences, would in a ﬁnite amount
of time enable one to determine whether or not it is satisﬁable. Show that
the unsolvability of the decision problem for implication (Theorem 11.2, or
equivalently Theorem 11.4) implies the unsolvability of the decision problem
for satisﬁability.
The next several problems pertain speciﬁcally to section 11.1.
11.3 Show that
∀w∀v(T wv ↔∃y(Rwy & Syv))
and
∀u∀v∀y((Suv & Syv) →u = y)
together imply
∀u∀v∀w((T wv & Suv) →Rwu).
11.4 Show that
∀x(∼Ax →∼∃t(Bt & Rtx))
and
∼∃x(Cx & Ax)
together imply
∼∃t∃x(Bt & Cx & Rtx).
11.5 The foregoing two problems state (in slightly simpliﬁed form in the case of
the second one) two facts about implication that were used in the proof of
Theorem 11.2. Where?
11.6 The operating interval for a Turing machine’s computation beginning with
input n consists of the numbers 0 through n together with the number of any
time at which the machine has not (yet) halted, and of any square the machine
visitsduringthecourseofitscomputations.Showthatifthemachineeventually
halts, then the operating interval is the set of numbers between some a ≤0
and some b ≥0, and that if the machine never halts, then the operating interval
consists either of all integers, or of all integers ≥a for some a ≤0.
11.7 A set of sentences  ﬁnitely implies a sentence D if D is true in every interpre-
tation with a ﬁnite domain in which every sentence in  is true. Trakhtenbrot’s
theorem states that the decision problem for ﬁnite logical implication is un-
solvable. Prove this theorem, assuming Turing’s thesis.
The remaining problems pertain speciﬁcally to section 11.2.
11.8 Add to the theory  in the proof of Theorem 11.4 the sentence
∀x 0 ̸= x′ & ∀x∀y(x′ = y′ →x = y).
Show that m ̸= n is then implied by  for all natural numbers m ̸= n, where
m is the usual numeral for m.

136
THE UNDECIDABILITY OF FIRST-ORDER LOGIC
11.9 Add to the language of the theory  of the proof of Theorem 11.4 the symbol
<, and add to  itself the sentence indicated in the preceding problem as well
as the sentence
∀x ∼0 < x & ∀x∀y(y < x′ ↔(y < x ∨y = x)).
Show that m < n is implied by  whenever m < n and ∼m < n is implied by
 whenever m ≥n, and that
∀y (y < n →y = 0 ∨y = 1 ∨. . . ∨y = m)
is implied by  whenever n = m′.
11.10 Let f be a recursive total function, and f1 ,. . . , fr a sequence of functions
with last item fn = f , such that each is either a basic function or is obtain-
able by earlier functions in the sequence by composition, primitive recursion,
or minimization, and all functions in the sequence are total. (According to
Problem 8.13, such a sequence exists for any recursive total function f.) Con-
struct  and D as in the proof of Theorem 11.6, with the following modiﬁ-
cations. Include the symbol < in the language, and the sentences indicated in
the preceding two problems, and besides this, whenever fi is obtained from
f j by minimization, include the sentence
∀x∀y((f j(x, y) = 0 & ∀z(z < y →f j(x, z) ̸= 0)) →fi(x) = y).
Show that the modiﬁed  is adequate for fr = f .
11.11 (Requires the material of Chapter 8.) Show that there is a two-place primitive
recursive function f such that the nullity problem for f is recursively unsolv-
able, or in other words, such that the set of x such that ∃y( f (x, y) = 0) is not
recursive.
A distinction between unavoidable and lazy appeals to Church’s thesis was
made at the end of section 7.2; though phrased there for recursive computabil-
ity, it applies also to Turing computability.
11.12 Distinguish the unavoidable from the lazy appeals to Turing’s thesis in section
11.1.
11.13 Distinguish the unavoidable from the lazy appeals to Church’s thesis in section
11.2.

12
Models
A model of a set of sentences is any interpretation in which all sentences in the set are
true. Section 12.1 discusses the sizes of the models a set of sentences may have (where
by the size of a model is meant the size of its domain) and the number of models of a
given size a set of sentences may have, introducing in the latter connection the impor-
tant notion of isomorphism. Section 12.2 is devoted to examples illustrating the theory,
with most pertaining to the important notion of an equivalence relation. Section 12.3
includes the statement of two major theorems about models, the L¨owenheim–Skolem
(transfer) theorem and the (Tarski–Maltsev) compactness theorem, and begins to illus-
trate some of their implications. The proof of the compactness theorem will be postponed
until the next chapter. The L¨owenheim–Skolem theorem is a corollary of compactness
(though it also admits of an independent proof, to be presented in a later chapter, along
with some remarks on implications of the theorem that have sometimes been thought
‘paradoxical’).
12.1 The Size and Number of Models
By a model of a sentence or set of sentences we mean an interpretation in which the
sentence, or every sentence in the set, comes out true. Thus  implies D if every
model of  is a model of D, D is valid if every interpretation is a model of D, and 
is unsatisﬁable if no interpretation is a model of .
By the size of a model we mean the size of its domain. Thus a model is called
ﬁnite, denumerable, or whatever, if its domain is ﬁnite, denumerable, or whatever.
A set of sentences is said to have arbitrarily large ﬁnite models if for every positive
integer m there is a positive integer n ≥m such that the set has a model of size n.
Already in the empty language, with identity but no nonlogical symbols, where an
interpretation is just a domain, one can write down sentences that have models only
of some ﬁxed ﬁnite size.
12.1 Example (A sentence with models only of a speciﬁed ﬁnite size). For each positive
integer n there is a sentence In involving identity but no nonlogical symbols such that In
will be true in an interpretation if and only if there are at least n distinct individuals in
the domain of the interpretation. Then Jn = ∼In+1 will be true if and only if there are
at most n individuals, and Kn = In & Jn will be true if and only if there are exactly n
individuals.
137

138
MODELS
There are actually several different sentences that could be used for In. A comparatively
short one is the following:
∀x1∀x2 · · · ∀xn−1∃xn(xn ̸= x1 & xn ̸= x2 & . . . & xn ̸= xn−1).
Thus, for instance, I3 may be written ∀x∀y∃z(z ̸= x & z ̸= y). For this to be true in an
interpretation M, it must be the case that for every p in the domain, if we added a con-
stant c denoting p, then ∀y∃z(z ̸= c & z ̸= y) would be true. For that to be true, it must
be the case that for every q in the domain, if we added a constant d denoting q, then
∃z(z ̸= c & z ̸= d) would be true. For that to be true, it must be the case that for some r
in the domain, if we added a constant e denoting r, then e ̸= c & e ̸= d would be true. For
that, e ̸= c and e ̸= d would both have to be true, and for that, e = c and e = d would both
have to be untrue. For that, the denotation r of e must be different from the denotations p
and q of c and d. So for every p and q in the domain, there is an r in the domain different
from both of them. Starting from any m1 in the domain, and applying this last conclusion
with p = q = m1, there must be an r, which we call m2, different from m1. Applying the
conclusion again with p = m1 and q = m2, there must be an r, which we call m3, dif-
ferent from m1 and m2. So there are at least three distinct individuals m1, m2, m3 in the
domain.
The set  of all sentences In has only inﬁnite models, since the number of elements
in any model must be ≥n for each ﬁnite n. On the other hand, any ﬁnite subset 0
of  has a ﬁnite model, and indeed a model of size n, where n is the largest number
for which In is in . Can we ﬁnd an example of a ﬁnite set of sentences that has
only inﬁnite models? If so, then we can in fact ﬁnd a single sentence that has only
inﬁnite models, namely, the conjunction of all the sentences in the ﬁnite set. In fact,
examples of single sentences that have only inﬁnite models are known.
12.2 Example (A sentence with only inﬁnite models). Let R be a two-place predicate.
Then the following sentence A has a denumerable model but no ﬁnite models:
∀x∃y Rxy & ∀x∀y ∼(Rxy & Ryx) & ∀x∀y∀z((Rxy & Ryz) →Rxz).
A hasadenumerablemodelinwhichthedomainisthenaturalnumbersandtheinterpretation
of the predicate is the usual strict less-than order relation on natural numbers. For every
number there is one it is less than; no two numbers are less than each other; and if one
number is less than a second and the second less than a third, then the ﬁrst is less than the
third. So all three conjuncts of A are true in this interpretation.
Now suppose there were a ﬁnite model M of A. List the elements of |M| as m0,
m1, . . . , mk−1, where k is the number of elements in |M|. Let n0 = m0. By the ﬁrst conjunct
of A (that is, by the fact that this conjunct is true in the interpretation) there must be some
n in |M| such that RM(n0, n). Let n1 be the ﬁrst element on the list for which this is
the case. So we have RM(n0, n1). But by the second conjunct of A we do not have both
RM(n0, n1) and RM(n1, n0), and so we do not have RM(n1, n0). It follows that n1 ̸= n0.
By the ﬁrst conjunct of A again there must be some n in |M| such that RM(n1, n). Let n2
be the ﬁrst element on the list for which this is the case, so we have RM(n1, n2). By the

12.1. THE SIZE AND NUMBER OF MODELS
139
third conjunct of A either RM(n0, n1) fails or RM(n1, n2) fails or RM(n0, n2) holds, and
since we do not have either of the ﬁrst two disjuncts, we must have RM(n0, n2). But by the
second conjunct of A, RM(n0, n2) and RM(n2, n0) don’t both hold, nor do both RM(n1, n2)
and RM(n2, n1), so we have neither RM(n2, n0) nor RM(n2, n1). It follows that n2 ̸= n0
and n2 ̸= n1. Continuing in this way, we obtain n3 different from all of n0, n1, n2, then n4
different from all of n0, n1, n2, n3, and so on. But by the time we get to nk we will have
exceeded the number of elements of |M|. This shows that our supposition that |M| is ﬁnite
leads to a contradiction. Thus A has a denumerable but no ﬁnite models.
When we ask how many different models a sentence or set of sentences may have
of a given size, the answer is disappointing: there are always an unlimited number
(a nonenumerable inﬁnity) of models if there are any at all. To give a completely trivial
example, consider the empty language, with identity but no nonlogical predicates,
for which an interpretation is just a nonempty set to serve as domain. And consider
the sentence ∃x∀y(y = x), which says there is just one thing in the domain. For any
object a you wish, the interpretation whose domain is {a}, the set whose only element
is a, is a model of this sentence. So for each real number, or each point on the line,
we get a model.
Of course, these models all ‘look alike’: each consists of just one thing, sitting
there doing nothing, so to speak. The notion of isomorphism, which we are about
to deﬁne, is a technically precise way of saying what is meant by ‘looking alike’ in
the case of nontrivial languages. Two interpretations P and Q of the same language
L are isomorphic if and only if there is a correspondence j between individuals p
in the domain |P| and individuals q in the domain |Q| subject to certain conditions.
(The deﬁnition of correspondence, or total, one-to-one, onto function, has been given
in the problems at the end of Chapter 1.) The further conditions are that for every
n-place predicate R and all p1, . . . , pn in |P| we have
RP(p1, . . . , pn)
if and only if
RQ( j(p1), . . . , j(pn))
(I1)
and for every constant c we have
j(cP) = cQ.
(I2)
If function symbols are present, it is further required that for every n-place function
symbol f and all p1, . . . , pn in |P| we have
j( f P(p1, . . . , pn)) = f Q( j(p1), . . . , j(pn)).
(I3)
12.3 Example (Inverse order and mirror arithmetic). Consider the language with a single
two-place predicate <, the interpretation with domain the natural numbers {0, 1, 2, 3, . . .}
and with < denoting the usual strict less-than order relation, and by contrast the inter-
pretation with domain the nonpositive integers {0, −1, −2, −3, . . .} and with < denot-
ing the usual strict greater-than relation. The correspondence associating n with −n is an
isomorphism, since m is less than n if and only if −m is greater than −n, as required
by (I1).

140
MODELS
If we also let 0 denote zero, let ′ denote the predecessor function, which takes x to
x −1, let + denote the addition function, and let · denote the function taking x and y
to the negative of their product, −xy, then we obtain an interpretation isomorphic to the
standard interpretation of the language of arithmetic. For the following equations show (I3)
to be fulﬁlled:
−x −1 = −(x + 1)
(−x) + (−y) = −(x + y)
−(−x)(−y) = −xy.
Generalizing our completely trivial example, in the case of the empty language,
where an interpretation is just a domain, two interpretations are isomorphic if and
only if there is a correspondence between their domains (that is, if and only if they
are equinumerous, as deﬁned in the problems at the end of Chapter 1). The analogous
property for nonempty languages is stated in the next result.
12.4 Proposition. Let X and Y be sets, and suppose there is a correspondence j from
X to Y. Then if Y is any interpretation with domain Y, there is an interpretation X with
domain X such that X is isomorphic to Y. In particular, for any interpretation with a
ﬁnite domain having n elements, there is an isomorphic interpretation with domain the set
{0, 1, 2, . . . , n −1}, while for any interpretation with a denumerable domain there is an
isomorphic interpretation with domain the set {0, 1, 2, . . .} of natural numbers.
Proof: For each relation symbol R, let RX be the relation that holds for p1, . . . , pn
in X if and only if RY holds for j(p1), . . . , j(pn). This makes (I1) hold automatically.
For each constant c, let cX be the unique p in X such that j(p) = cY. (There will be
such a p because j is onto, and it will be unique because j is one-to-one.) This makes
(I2) hold automatically. If function symbols are present, for each function symbol f ,
let f X be the function on X whose value for p1, . . . , pn in X is the unique p such
that j(p) = f Y( j(p1), . . . , j(pn)). This makes (I3) hold automatically.
The next result is a little more work. Together with the preceding, it implies what we
hinted earlier, that a sentence or set of sentences has an unlimited number of models if
it has any models at all: given one model, by the preceding proposition there will be an
unlimited number of interpretations isomorphic to it, one for each set equinumerous
with its domain. By the following result, these isomorphic interpretations will all be
models of the given sentence or set of sentences.
12.5 Proposition (Isomorphism lemma). If there is an isomorphism between two in-
terpretations P and Q of the same language L, then for every sentence A of L we have
P |= A
if and only if
Q |= A.
(1)
Proof: We ﬁrst consider the case where identity and function symbols are absent,
and proceed by induction on complexity. First, for an atomic sentence involving a
nonlogical predicate R and constants t1, . . . , tn, the atomic clause in the deﬁnition of

12.1. THE SIZE AND NUMBER OF MODELS
141
truth gives
P |= R(t1, . . . , tn)
if and only if
RP
tP
1 , . . . , tP
n

Q |= R(t1, . . . , tn)
if and only if
RQ
tQ
1 , . . . , tQ
n

while the clause (I1) in the deﬁnition of isomorphism gives
RP
tP
1 , . . . , tP
n

if and only if
RQ
j

tP
1

, . . . , j

tP
n

and the clause (I2) in the deﬁnition of isomorphism gives
RQ
j

tP
1

, . . . , j

tP
n

if and only if
RQ
tQ
1 , . . . , tQ
n

.
Together the four displayed equivalences give (1) for R(t1, . . . , tn).
Second, suppose (1) holds for less complex sentences than ∼F, including the
sentence F. Then (1) for ∼F is immediate from this assumption together with the
negation clause in the deﬁnition of truth, by which we have
P |= ∼F
if and only if
not P |= F
Q |= ∼F
if and only if
not Q |= F.
The case of junctions is similar.
Third, suppose (1) holds for less complex sentences than ∀x F(x), including sen-
tences of the form F(c). For any element p of |P|, if we extend the language by
adding a new constant c and extend the interpretation P so that c denotes p, then
there is one and only one way to extend the interpretation Q so that j remains an
isomorphism of the extended interpretations; namely, we extend the interpretation Q
so that c denotes j(p), and therefore clause (I2) in the deﬁnition of isomorphism still
holds for the extended language. By our assumption that (1) holds for F(c) it follows
on the one hand that
P |= F[p]
if and only if
Q |= F[ j(p)].
(2)
By the universal quantiﬁer clause in the deﬁnition of truth
P |= ∀x F(x)
if and only if
P |= F[p] for all p in |P|.
Hence
P |= ∀x F(x)
if and only if
Q |= F[ j(p)] for all p in |P|.
On the other hand, again by the universal quantiﬁer clause in the deﬁnition of truth
we have
Q |= ∀x F(x)
if and only if
Q |= F[q] for all q in |Q|.
But since j is a correspondence, and therefore is onto, every q in |Q| is of the form
j(p), and (1) follows for ∀x F(x). The existential-quantiﬁer case is similar.
If identity is present, we have to prove (1) also for atomic sentences involving =.
That is, we have to prove
p1 = p2
if and only if
j(p1) = j(p2).

142
MODELS
But this is simply the condition that j is one-to-one, which is part of the deﬁnition of
beingacorrespondence,whichinturnispartofthedeﬁnitionofbeinganisomorphism.
If function symbols are present, we must ﬁrst prove as a preliminary that for any
closed term t we have
j(tP) = tQ.
(3)
This is proved by induction on complexity of terms. For constants we have (3) by
clause (I2) in the deﬁnition of isomorphism. And supposing (3) holds for t1, . . . , tn,
then it holds for f (t1, . . . , tn) since by clause (I3) in the deﬁnition of isomorphism
we have
j(( f (t1, . . . , tn))P) = j

f P
tP
1 , . . . , tP
n

= f Q
j

tP
1

, . . . , j

tP
n

= f Q
tQ
1 , . . . , tQ
n

= ( f (t1, . . . , tn))Q.
The proof given above for the atomic case of (1) now goes through even when the
ti are complex closed terms rather than constants, and no further changes are required
in the proof.
12.6 Corollary (Canonical-domains lemma).
(a) Any set of sentences that has a ﬁnite model has a model whose domain is the set
{0, 1, 2, . . . , n} for some natural number n.
(b) Any set of sentences having a denumerable model has a model whose domain is
the set {0, 1, 2, . . .} of natural numbers.
Proof: Immediate from Propositions 12.4 and 12.5.
Two models that are isomorphic are said to be of the same isomorphism type. The
intelligent way to count the models of a given size that a sentence has is to count
not literally the number of models (which is always a nonenumerable inﬁnity if it is
nonzero), but the number of isomorphism types of models. The import of the rather
abstract results of this section should become clearer as they are illustrated concretely
in the next section.
12.2 Equivalence Relations
Throughout this section we will work with a language whose only nonlogical symbol
is a single two-place predicate ≡. We will write x ≡y for what ofﬁcially ought to be
≡(x, y). Our interest will be in models—and especially in denumerable models—of
the following sentence Eq of the language:
∀xx ≡x &
∀x∀y(x ≡y →y ≡x) &
∀x∀y∀z((x ≡y & y ≡z) →x ≡z).
Such a model X will consist of a nonempty set X and a two-place relation ≡X or E on
X. In order to make the three clauses of Eq true, E will have to have three properties.

12.2. EQUIVALENCE RELATIONS
143
Namely, for all a, b, c in X we must have the following:
(E1) Reﬂexivity: a E a.
(E2) Symmetry: If a E b then b E a.
(E3) Transitivity: If a E b and b E c then a E c.
A relation with these properties is called an equivalence relation on X.
One way to get an equivalence relation on X is to start with what is called a
partition of X. This is a set  of nonempty subsets of X such that the following hold:
(P1) Disjointness: If A and B are in , then either A = B or A and B have no
elements in common.
(P2) Exhaustiveness: Every a in X belongs to some A in .
The sets in  are called the pieces of the partition.
Given a partition, deﬁne a E b to hold if a and b are in the same piece of the
partition, that is, if, for some A in , a and b are both in A. Now by (P2), a is in
some A in . To say a and a are ‘both’ in A is simply to say a is in A twice, and
since it was true the ﬁrst time, it will be true the second time also, showing that
a E a, and that (E1) holds. If a E b, then a and b are both in some A in , and to say
that b and a are both in A is to say the same thing in a different order, and is equally
true, showing that b E a, and that (E2) holds. Finally, if a E b and b E c, then a and
b are both in some A in  and b and c are both in some B in . But by (P1), since
A and B have the common element b, they are in fact the same, so a and c are both
in A = B, and a E c, showing that (E3) holds. So E is an equivalence relation, called
the equivalence relation induced by the partition.
Actually, this is in a sense the only way to get an equivalence relation: every
equivalence relation is induced by a partition. For suppose E is any such relation;
for any a in X let [a] be the equivalence class of a, the set of all b in X such that
a E b; and let  be the set of all these equivalence classes. We claim  is a partition.
Certainly any element a of X is in some A in , namely, a is in [a], by (E1). So (P2)
holds. As for (P1), if [a] and [b] have a common element c, we have a E c and b E c,
and having b E c, by (E2) we have also c E b, and then, having a E c and c E b, by
(E3) we have also a E b, and by (E2) again we have also b E a. But then if d is any
element of [a], having a E d and b E a, by (E3) again we have b E d, and d is in [b]. In
exactly the same way, any element of [b] is in [a], and [a] = [b]. So  is a partition,
as claimed. We also claim the original E is just the equivalence relation induced by
this partition . For along the way we have shown that if a E b then a and b belong
to the same piece [a] = [b] of the partition, while of course if b belongs to the same
piece [a] of the partition that a does, then we have a E b, so E is the equivalence
relation induced by this partition.
We can draw a picture of a denumerable model of Eq, by drawing dots to represent
elements of X with boxes around those that are in the same equivalence class. We
can also describe such a model by describing its signature, the inﬁnite sequence of
numbers whose 0th entry is the number (which may be 0, 1, 2, . . . , or inﬁnite) of
equivalence classes having inﬁnitely many elements and whose nth entry for n > 0 is

144
MODELS
the number of equivalence classes with exactly n elements. The examples to follow
are illustrated by pictures for equivalence relations of a variety of different signatures
in Figure 12-1.
(a) Signature (1, 0, 0, 0, 0, … )
…
(b) Signature (0, ∞, 0, 0, 0, … )
…
(c) Signature (0, 0, ∞, 0, 0, … )
…
(d)(i) Signatures (1, 1, 0, 0, 0, … ), (1, 2, 0, 0, 0, … ), (1, 3, 0, 0, 0, … ), and so on
…
…
…
(d)(ii) Signatures (0, ∞, 1, 0, 0, … ), (0, ∞, 0, 1, 0, …), (0, ∞, 0, 0, 1, … ) and so on
…
…
…
(d)(iii) Signature (1, ∞, 0, 0, 0, … )
…
…
(e) Signature  (0, 0, 1, 0, 1, 0, 1, 0, … )
…
Figure 12-1. Equivalence relations.
12.7 Example (A promiscuous model). Let a be the set containing Eq and the following
sentence Ea:
∀x∀y x ≡y.
A denumerable model of a consists of a denumerable set X with an equivalence relation
E in which all elements are in the same equivalence class, as in Figure 12-1(a). We claim
all such models are isomorphic. Indeed, if
X = {a1, a2, a3, . . .}
and
Y = {b1, b2, b3, . . .}
are any two denumerable sets, if X is the model with domain X and ≡X the relation that
holds among all pairs ai, a j of elements of X, and if Y is the model with domain Y and ≡Y
the relation that holds among all pairs bi, b j of elements of Y, then the function sending ai to

12.2. EQUIVALENCE RELATIONS
145
bi is an isomorphism between X and Y. Condition (I1) in the deﬁnition of isomorphism—
the only applicable condition—says that we must in all cases have ai ≡X a j if and only if
f (ai) ≡Y f (a j); and of course we do, since we always have both ai ≡X a j and bi ≡Yb j.
Thus a has only one isomorphism type of denumerable model.
12.8 Example (An eremitic model). Let b be the set containing Eq and the following
sentence Eb:
∀x∀y (x ≡y ↔x = y).
A denumerable model of b consists of a denumerable set X with an equivalence relation
E in which each element is equivalent only to itself, so each equivalence class consists
of but a single element, as in Figure 12-1(b). Again any two such models are isomorphic.
With the notation as in the preceding example, this time we have ai ≡X a j if and only if
f (ai) ≡Y f (a j), because we only have ai ≡X a j when i = j, which is precisely when we
have bi ≡Y b j.
12.9 Example (Two isomorphism types). Let ab be the set containing Eq and the dis-
junction Ea ∨Eb. Any model of ab must be either a model of a or one of b, and all
models of either are models of ab. Now all denumerable models of a are isomorphic to
each other, and all denumerable models of b are isomorphic to each other. But a model of
a cannot be isomorphic to a model of b, by the isomorphism lemma, since Ea is true in
the former and false in the latter, and inversely for Eb. So ab has exactly two isomorphism
types of denumerable model.
12.10 Example (An uxorious model). Let c be the set containing Eq and the following
sentence Ec:
∀x∃y(x ̸= y & x ≡y & ∀z(z ≡x →(z = x ∨z = y))).
A denumerable model of c consists of a denumerable set X with an equivalence relation
E in which each element is equivalent to just one other element than itself, so each equiva-
lence class consists of exactly two elements, as in Figure 12-1(c). Again there is only one
isomorphism type of denumerable model. If we renumber the elements of X so that a2 is
the equivalent of a1, a4 of a3, and so on, and if we similarly renumber the elements of Y,
again the function f (ai) = bi will be an isomorphism.
12.11 Example (Three isomorphism types). Let abc be the set containing Eq and the
disjunction Ea ∨Eb ∨Ec. Then abc has three isomorphism types of denumerable models.
The reader will see the pattern emerging: we can get an example with n isomorphism types
of denumerable models for any positive integer n.
12.12 Example (Denumerably many isomorphism types). Let d be the set containing Eq
and the following sentence Ed:
∀x∀y((∃u(u ̸= x & u ≡x) & ∃v(v ̸= y & v ≡y)) →x ≡y).
A denumerable model of d will consist of a denumerable set X with an equivalence
relation in which any two elements a and b that are not isolated, that is, that are such that
each is equivalent to something other than itself, are equivalent to each other. Here there
are a number of possible pictures. It could be that all elements are equivalent, or that all

146
MODELS
elements are isolated, as in Figure 12-1(a) or (b). It could also be the case that there is
one isolated element with all the other elements being equivalent. Or there could be two
isolated elements with all the other elements being equivalent. Or three, and so on, as in
Figure 12-1(d)(i).
There are further possibilities. For, supposing there are inﬁnitely many isolated ele-
ments, the remaining equivalence class, consisting of all nonisolated elements, may contain
two or three or . . . elements, as in Figure 12-1(d)(ii)—or it could contain zero, but that is
Figure 12-1(b) again. Finally there is the possibility (whose picture takes two lines to draw)
of inﬁnitely many isolated elements plus an inﬁnite class of other elements, all equivalent
to each other, as in Figure 12-1(d)(iii).
Any two models corresponding to the same picture (or, what comes to the same thing,
the same signature) are isomorphic. If there are only n isolated elements, renumber so that
these are a1 through an. If there are only n nonisolated elements, renumber so that these
are a1 through an instead. And if there are inﬁnitely many of each, renumber so that a1, a3,
a5, . . . are the isolated ones, and a2, a4, a6, . . . the nonisolated ones. Renumber the bi
similarly, and then, as always, the function f (ai) = bi can be checked to be an isomorphism.
No two models corresponding to different pictures are isomorphic, for if a is nonisolated,
a satisﬁes the formula
∃y(y ̸= x & y ≡x).
So by the isomorphism lemma, if f is an isomorphism, f (a) must also satisfy this formula,
and so must be nonisolated. And for the same reason, applied to the negation of this
formula, if a is isolated, f (a) must be isolated. So an isomorphism must carry nonisolated
to nonisolated and isolated to isolated elements, and the numbers of nonisolated and of
isolated elements must be the same in both models. Here, then, is an example where there
are denumerably many of isomorphism types of denumerable models.
12.13 Example (Nonenumerably many isomorphism types). The sentence Eq all by itself
has nonenumerably many isomorphism types of denumerable models. For any inﬁnite set
of positive integers S there is a model in which there is exactly one equivalence class with
exactly n elements for each n in S, and no equivalence class with exactly n elements for any n
not in S. For instance, if S is the set of even numbers, the model will look like Figure 12-1(e).
We leave it to the reader to show how the isomorphism lemma can be used to show that no
two models corresponding to different sets S are isomorphic. Since there are nonenumerably
many such sets, there are nonenumerably many isomorphism types of models.
12.3 The L¨owenheim–Skolem and Compactness Theorems
We have seen that there are sentences that have only inﬁnite models. One might
wonder whether there are sentences that have only nonenumerable models. We have
also seen that there are enumerable sets of sentences that have only inﬁnite models,
though every ﬁnite subset has a ﬁnite model. One might wonder whether there are
sets of sentences that have no models at all, though every ﬁnite subset has a model.
The answer to both these questions is negative, according to the following pair of
theorems. They are basic results in the theory of models, with many implications
about the existence, size, and number of models.

12.3. THE L ¨OWENHEIM–SKOLEM AND COMPACTNESS THEOREMS
147
12.14 Theorem (L¨owenheim–Skolem theorem). If a set of sentences has a model, then
it has an enumerable model.
12.15 Theorem (Compactness theorem). If every ﬁnite subset of a set of sentences has
a model, then the whole set has a model.
We explore a few of the implications of these theorems in the problems at the end
this chapter. We stop here just to note three immediate implications.
12.16 Corollary (Overspill principle). If a set of sentences has arbitrarily large ﬁnite
models, then it has a denumerable model.
Proof: Let  be a set of sentences having arbitrarily large ﬁnite models, and for
each m let Im be the sentence with identity but no nonlogical symbols considered in
Example 12.1, which is true in a model if and only if the model has size ≥m. Let
* =  ∪{I1, I2, I3, . . . }
be the result of adding all the Im to . Any ﬁnite subset of * is a subset of  ∪
{I1, I2, . . . , Im} for some m, and since  has a model of size ≥m, such a set has
a model. By the compactness theorem, therefore, * has a model. Such a model is
of course a model of , and being also a model of each Im, it has size ≥m for all
ﬁnite m, and so is inﬁnite. By the L¨owenheim–Skolem theorem, we could take it to
be enumerable.
A set  of sentences is (implicationally) complete if for every sentence A in its
language, either A or ∼A is a consequence of , and denumerably categorical if any
two denumerable models of  are isomorphic.
12.17 Corollary (Vaught’s test). If  is a denumerably categorical set of sentences
having no ﬁnite models, then  is complete.
Proof: Suppose  is not complete, and let A be some sentence in its language such
thatneither A nor∼A isaconsequenceof.Thenboth ∪{∼A}and ∪{A}aresatis-
ﬁable, and by the L¨owenheim–Skolem theorem they have enumerable models P−and
P+. Since  has no ﬁnite models, P−and P+ must be denumerable. Since  is denu-
merably categorical, they must be isomorphic. But by the isomorphism lemma, since
A is untrue in one and true in the other, they cannot be isomorphic. So the assumption
that  is not complete leads to a contradiction, and  must be complete after all.
Thus if  is any of the examples of the preceding section in which we found there
was only one isomorphism type of denumerable model, then adding the sentences
I1, I2, I3, . . . to  (in order to eliminate the possibility of ﬁnite models) produces an
example that is complete.
The L¨owenheim–Skolem theorem also permits a sharpening of the statement of
the canonical-domains lemma (Lemma 12.6).
12.18 Corollary (Canonical-domains theorem).
(a) Any set of sentences that has a model, has a model whose domain is either the set
of natural numbers <n for some positive n, or else the set of all natural numbers.

148
MODELS
(b) Any set of sentences not involving function symbols or identity that has a model,
has a model whose domain is the set of all natural numbers.
Proof: (a) is immediate from the L¨owenheim–Skolem theorem and Corollary 12.6.
For (b), given a set of sentences  not involving function symbols or identity, if
 has a model, apply part (a) to get, at worst, a model Y with domain the ﬁnite set
{0, 1, . . . , n −1} for some n. Let f be the function from the set of all natural numbers
to this ﬁnite set given by f (m) = min(m, n −1). Deﬁne an interpretation X with
domain the set of all natural numbers by assigning to each k-place relation symbol
R as denotation the relation RX that holds for p1, . . . , pk if and only if RY holds for
f (p1),. . . , f (pk). Then f has all the properties of an isomorphism except for not
being one-to-one. Examining the proof of the isomorphism lemma (Proposition 12.5),
which tells us the same sentences are true in isomorphic interpretations, we see that
the property of being one-to-one was used only in connection with sentences involving
identity. Since the sentences in  do not involve identity, they will be true in X because
they are true in Y.
The remainder of this section is devoted to an advance description of what will be
done in the following two chapters, which contain proofs of the L¨owenheim–Skolem
and compactness theorems and a related result. Our preview is intended to enable
the readers who are familiar with the contents of an introductory textbook to decide
how much of this material they need or want to read. The next chapter, Chapter 13,
is devoted to a proof of the compactness theorem. Actually, the proof shows that if
every ﬁnite subset of a set  has a model, then  has an enumerable model. This
version of the compactness theorem implies the L¨owenheim–Skolem theorem, since
if a set has a model, so does every subset, and in particular every ﬁnite subset. An
optional ﬁnal section 13.5 considers what happens if we admit nonenumerable lan-
guages. (It turns out that the compactness theorem still holds, but the ‘downward’
L¨owenheim–Skolem theorem fails, and one gets instead an ‘upward’ theorem to the
effect that any set of sentences having an inﬁnite model has a nonenumerable model.)
Every introductory textbook introduces some notion of a deduction of a sentence
D from a ﬁnite set of sentences . The sentence D is deﬁned to be deducible from
a ﬁnite set  if and only if there is a deduction of the sentence from the set. A
deduction from a subset of a set always counts as a deduction from that set itself,
and a sentence D is deﬁned to be deducible from an inﬁnite set  if and only if it
is deducible from some ﬁnite subset. A sentence D is deﬁned to be demonstrable
if it is deducible from the empty set of sentences ∅, and a set of sentences  is
deﬁned to be inconsistent if the constant false sentence ⊥is deducible from it. The
better introductory textbooks include proofs of the soundness theorem, according
to which if D is deducible from , then D is a consequence of  (from which it
follows that if D is demonstrable, then D is valid, and that if  is inconsistent,
then  is unsatisﬁable), and of the G¨odel completeness theorem, according to which,
conversely, if D is a consequence of , then D is deducible from  (from which it
follows that if D is valid, then D is demonstrable, and that if  is unsatisﬁable, then 
is inconsistent). Since by deﬁnition a set is consistent if and only if every ﬁnite subset
is, it follows that a set is satisﬁable if and only if every ﬁnite subset is: the compactness
theorem follows from the soundness and completeness theorems. Actually, the proof

PROBLEMS
149
of completeness shows that if  is consistent, then  has an enumerable model,
so the form of the compactness theorem implying the L¨owenheim–Skolem theorem
follows.
In Chapter 14 we introduce a notion of deduction of the kind used in advanced,
rather than introductory, works on logic, and prove soundness and completeness for
it. However, rather than derive the compactness theorem (and thence the L¨owenheim–
Skolem theorem) from soundness and completeness, we obtain completeness in
Chapter 14 from the main lemma used to obtain compactness in Chapter 13. Thus our
proof of the compactness theorem (and similarly the L¨owenheim–Skolem theorem)
does not mention the notion of deduction any more than does the statement of the theo-
rem itself. For the reader who is familiar with a proof of the soundness and complete-
ness theorems, however, Chapter 14 is optional and Chapter 13 (containing the main
lemma) with it, since the compactness theorem (and thence the L¨owenheim–Skolem
theorem) does follow. It does not matter if the notion of deduction with which such
a reader is familiar is different from ours, since no reference to the details of any
particular deduction procedure is made outside Chapter 14 (except in one optional
section at the end of the chapter after that, Chapter 15). All that matters for our later
work is that there is some procedure or other of deduction that is sound and complete,
and—for purposes of later application of our work on computability to logic—is
such that one can effectively decide whether or not a given ﬁnite object is or is not a
deduction of a given sentence D from a given ﬁnite set of sentences . And this last
feature is shared by all deduction procedures in all works on logic, introductory or
advanced, ours included.
Problems
12.1 By the spectrum of a sentence C (or set of sentences ) is meant the set of all
positive integers n such that C (or ) has a ﬁnite model with a domain having
exactly n elements. Consider a language with just two nonlogical symbols,
a one-place predicate P and a one-place function symbol f . Let A be the
following sentence:
∀x1∀x2( f (x1) = f (x2) →x1 = x2) &
∀y∃x( f (x) = y) &
∀x∀y( f (x) = y →(Px ↔∼Py)).
Show that the spectrum of A is the set of all even positive integers.
12.2 Give an example of a sentence whose spectrum is the set of all odd positive
integers.
12.3 Give an example of a sentence whose spectrum is the set of all positive integers
that are perfect squares.
12.4 Give an example of a sentence whose spectrum is the set of all positive integers
divisible by three.
12.5 Consider a language with just one nonlogical symbol, a two-place predicate
Q. Let U be the interpretation in which the domain consists of the four sides
of a square, and the denotation of Q is the relation between sides of being
parallel. Let V be the interpretation in which the domain consists of the four

150
MODELS
vertices of a square, and the denotation of Q is the relation between vertices
of being diagonally opposite. Show that U and V are isomorphic.
12.6 Consider a language with just one nonlogical symbol, a two-place predicate <.
Let Q be the interpretation in which the domain is the set of real numbers
strictly greater than zero and strictly less than one and the denotation of <
is the usual order relation. Let R be the interpretation in which the domain is
the set of all real numbers and the denotation of < is the usual order relation.
Show that Q and R are isomorphic.
12.7 Let L be a language whose only nonlogical symbols are a two-place function
symbol § and a two-place predicate <. Let P be the interpretation of this
language in which the domain is the set of positive real numbers, the denotation
of § is the usual multiplication operation, and the denotation of < is the usual
order relation. Let Q be the interpretation of this language in which the domain
is the set of all real numbers, the denotation of § is the usual addition operation,
and the denotation of < is the usual order relation. Show that P and Q are
isomorphic.
12.8 Write A ∼= B to indicate that A is isomorphic to B. Show that for all interpre-
tations A, B, C of the same language the following hold:
(a) A ∼= A;
(b) if A ∼= B, then B ∼= A;
(c) if A ∼= B and B ∼= C, then A ∼= C.
12.9 By true arithmetic we mean the set  of all sentences of the language of arith-
metic that are true in the standard interpretation. By a nonstandard model of
arithmetic we mean a model of this  that (unlike the model in Example 12.3)
is not isomorphic to the standard interpretation. Let  be the set of sentences
obtained by adding a constant c to the language and adding the sentences
c ̸= 0, c ̸= 1, c ̸= 2, and so on, to . Show that any model of  would give us
a nonstandard model of arithmetic.
12.10 Consider the language with just the one nonlogical symbol ≡and the sentence
Eq whose models are precisely the sets with equivalence relations, as in the
examples in section 12.2.
(a) For each n, indicate how to write down a sentence Bn such that the
models of Eq & Bn will be sets with equivalence relations having at least
n equivalence classes.
(b) For each n, indicate how to write down a formula Fn(x) such that in a
model of Eq, an element a of the domain will satisfy Fn(x) if and only if
there are at least n elements in the equivalence class of a.
(c) For each n, indicate how to write down a sentence Cn that is true in a
model of Eq if and only if there are exactly n equivalence classes.
(d) For each n, indicate how to write down a formula Gn(x) that is satisﬁed
by an element of the domain if and only if its equivalence class has
exactly n elements.
12.11 For each m and n indicate how to write down a sentence Dmn that is true in a
model of Eq if and only if there are at least m equivalence classes with exactly
n elements.

PROBLEMS
151
12.12 Show that if two models of Eq are isomorphic, then the equivalence relations
of the models have the same signature.
12.13 Suppose E1 and E2 are equivalence relations on denumerable sets X1 and
X2 both having the signature σ(n) = 0 for n ≥1 and σ(0) = ∞, that is, both
having inﬁnitely many equivalence classes, all inﬁnite. Show that the models
involved are isomorphic.
12.14 Show that two denumerable models of Eq are isomorphic if and only if they
have the same signature.
In the remaining problems you may, when relevant, use the L¨owenheim–Skolem
and compactness theorems, even though the proofs have been deferred to the
next chapter.
12.15 Show that:
(a)  is unsatisﬁable if and only if ∼C1 ∨· · · ∨∼Cm is valid for some
C1, . . . , Cm in .
(b) D is a consequence of  if and only if D is a consequence of some ﬁnite
subset of .
(c) D is a consequence of  if and only if ∼C1 ∨· · · ∨∼Cm ∨D is valid
for some C1, . . . , Cm in .
12.16 For any prime p = 2, 3, 5, . . . , let Dp(x) be the formula ∃y p · y = x of the
language of arithmetic, so that for any natural number n, Dp(n) is true if
and only if p divides n without remainder. Let S be any set of primes. Say
that a nonstandard model M of arithmetic encrypts S if there is an indi-
vidual m in the domain |M| such that M |= Dp[m] for all p belonging
to S, and M |= ∼Dp[m] for all p not belonging to S. Show that for any
set S of primes there is a denumerable nonstandard model of arithmetic that
encrypts S.
12.17 Show that there are nonenumerably many isomorphism types of denumerable
nonstandard models of arithmetic.
12.18 Show that if two sentences have the same enumerable models, then they are
logically equivalent.
12.19 Work with a language whose only nonlogical symbol is a single two-place
predicate <. Consider the set of sentences of this language that are true in the
interpretation where the domain is the set of real numbers and the denotation of
the predicate is the usual order on real numbers. According to the L¨owenheim–
Skolem theorem, there must be an enumerable model of this set of sentences.
Can you guess what one is?
The next several problems provide a signiﬁcant example of a denumerably
categorical set of sentences.
12.20 Work with a language whose only nonlogical symbol is a single two-place
predicate <. The models of the following sentence LO of the language are
called linear orders:
∀x ∼x < x &
∀x∀y∀z((x < y & y< z) →x < z) &
∀x∀y(x < y ∨x = y ∨y < x).

152
MODELS
Such a model A will consist of a nonempty set |A| or A and a two-place
relation <A or <A on it. Show that the above sentence implies
∀x∀y ∼(x < y & y < x).
12.21 Continuing the preceding problem, a ﬁnite partial isomorphism between linear
orders (A, <A) and (B, <B) is a function j from a ﬁnite subset of A onto a
ﬁnite subset of B such that for all a1 and a2 in the domain of j, a1 <A a2 if
and only if j(a1) <A j(a1). Show that if j is a ﬁnite partial isomorphism from
a linear order (A, <A) to the rational numbers with their usual order, and a is
any element of A not in the domain of j, then j can be extended to a ﬁnite
partial isomorphism whose domain is the domain of j together with a. (Here
extended means that the new isomorphism assigns the same rational numbers
as the old to elements of A there were already in the domain of the old.)
12.22 Continuing the preceding problem, if j0, j1, j2, . . . are ﬁnite partial isomor-
phisms from an enumerable linear order to the rational numbers with their
usual order, and if each ji+1 is an extension of the preceding ji, and if every
element of A is in the domain of one of the ji (and hence of all jk for k ≥i),
then (A, <A) is isomorphic to some suborder of the rational numbers with their
usual order. (Here suborder means a linear order (B, <B) where B is some
subset of the rational numbers, and <B the usual order on rational numbers as
it applies to elements of this subset.)
12.23 Continuing the preceding problem, show that every enumerable linear order
(A, <A) is isomorphic to a suborder of the rational numbers with their usual
order.
12.24 Continuing the preceding problem, a linear order is said to be dense if it is a
model of
∀x∀y(x < y →∃z(x < z & z < y)).
It is said to have no endpoints if it is a model of
∼∃x∀y(x < y ∨x = y) & ∼∃x∀y(x = y ∨y < x).
Which of the following is dense: the natural numbers, the integers, the rational
numbers, the real numbers, in each case with their usual order? Which have
no endpoints?
12.25 Continuing the preceding problem, show that the set of sentences whose mod-
els are the dense linear orders without endpoints is denumerably categorical.
12.26 A linear order is said to have endpoints if it is a model of
∃x∀y(x < y ∨x = y) & ∃x∀y(x = y ∨y < x).
Show that the set of sentences whose models are the dense linear orders with
endpoints is denumerably categorical.
12.27 How many isomorphism types of denumerable dense linear orders are there?

13
The Existence of Models
This chapter is entirely devoted to the proof of the compactness theorem. Section 13.1
outlines the proof, which reduces to establishing two main lemmas. These are then taken
up in sections 13.2 through 13.4 to complete the proof, from which the L¨owenheim–
Skolem theorem also emerges as a corollary. The optional section 13.5 discusses what
happens if nonenumerable languages are admitted: compactness still holds, but the
L¨owenheim–Skolem theorem in its usual ‘downward’ form fails, while an alternative
‘upward’ theorem holds.
13.1 Outline of the Proof
Our goal is to prove the compactness theorem, which has already been stated in the
preceding chapter (in section 12.3). For convenience, we work with a version of ﬁrst-
order logic in which the only logical operators are ∼, ∨, and ∃, that is, in which &
and ∀are treated as unofﬁcial abbreviations. The hypothesis of the theorem, it will
be recalled, is that every ﬁnite subset of a given set of sentences is satisﬁable, and
the conclusion we want to prove is that the set itself is satisﬁable, or, as we more
elaborately put it, belongs to the set S of all satisﬁable sets of sentences. As a ﬁrst
step towards the proof, we set down some properties enjoyed by this target set S.
The reason for not including & and ∀ofﬁcially in the language is simply that in this
and subsequent lemmas we would need four more clauses, two for & and two for ∀.
These would not be difﬁcult to prove, but they would be tedious.
13.1 Lemma (Satisfaction properties lemma). Let S be the set of all sets  of sentences
of a given language such that  is satisﬁable. Then S has the following properties:
(S0) If  is in S and 0 is a subset of , then 0 is in S.
(S1) If  is in S, then for no sentence A are both A and ∼A in .
(S2) If  is in S and ∼∼B is in , then  ∪{B} is in S.
(S3) If  is in S and (B ∨C) is in , then either  ∪{B} is in S or  ∪{C} is in S.
(S4) If  is in S and ∼(B ∨C) is in , then  ∪{∼B} is in S and  ∪{∼C} is in S.
(S5) If  is in S and {∃x B(x)} is in , and the constant c does not occur in  or
∃xB(x), then  ∪{B(c)} is in S.
(S6) If  is in S and ∼∃x B(x) is in , then for every closed term t,  ∪{∼B(t)} is in S.
153

154
THE EXISTENCE OF MODELS
(S7) If  is in S, then  ∪{t = t} is in S for any closed term t of the language of .
(S8) If  is in S and B(s) and s = t are in , then  ∪{B(t)} is in S.
Proof: These have been established in Chapter 10. (S0) and (S1) were men-
tioned just before Example 10.4. (S2) appeared as Example 10.4(g), where it was
derived from Example10.3(a). (S4), (S6), and (S8) can be derived in exactly the
same way from Example 10.3(c), 10.3(e), and 10.3(f), as remarked after the proof of
Example 10.4. (S3), (S5), and (S7) were established in Example 10.5.
We call (S0)–(S8) the satisfaction properties. Of course, at the outset we do not
know that the set we are interested in belongs to S. Rather, what we are given is that
it belongs to the set S* of all sets of sentences whose every ﬁnite subset belongs to S.
(Of course, once we succeed in proving the compactness theorem, S and S* will turn
out to be the same set.) It will be useful to note that S* shares the above properties of S.
13.2 Lemma (Finite character lemma). If S is a set of sets of sentences having the
satisfaction properties, then the set S* of all sets of formulas whose every ﬁnite subset is in
S also has properties (S0)–(S8).
Proof: To prove (S0) for S*, note that if every ﬁnite subset of  is in S, and 0 is
subset of , then every ﬁnite subset of 0 is in S, since any ﬁnite subset of 0 is a
ﬁnite subset of . To prove (S1) for S*, note that if every ﬁnite subset of  is in S,
then  cannot contain both A and ∼A, else {A, ∼A} would be a ﬁnite subset of ,
though {A, ∼A} is not in S by property (S1) of S. To prove (S2) for S*, note that if
every ﬁnite subset of  ∪{∼∼B} is in S, then any ﬁnite subset of  ∪{B} is either
a ﬁnite subset of  and hence of  ∪{∼∼B} and therefore is in S, or else is of form
0 ∪{B} where 0 is a ﬁnite subset of . In the latter case, 0 ∪{∼∼B} is a ﬁnite
subset of  ∪{∼∼B} and therefore in S, so  ∪{B} is in S by property (S2) of S.
Thus the ﬁnite subset 0 ∪{B} is in S*. (S4)–(S8) for S* follow from (S4)–(S8) for
S exactly as in the case of (S2). It remains only to prove (S3) for S*.
So suppose every ﬁnite subset of  ∪{(B ∨C)} is in S, but that it is not the case
that every ﬁnite subset of  ∪{B} is in S, or in other words that there is some ﬁnite
subset of  ∪{B} that is not in S. This cannot just be a subset of , since then it
would be a ﬁnite subset of  ∪{(B ∨C)} and would be in S. So it must be of the
form 0 ∪{B} for some ﬁnite subset 0 of . We now claim that every ﬁnite subset of
 ∪{C} is in S. For any such set is either a ﬁnite subset of  and therefore in S, or is of
form 1 ∪{C} for some ﬁnite subset 1 of . In the latter case, 0 ∪1 ∪{(B ∨C)}
is a ﬁnite subset of  ∪{(B ∨C)} and so is in S. It follows that either 0 ∪1 ∪{B}
or 0 ∪1 ∪{C} is in S by property (S3) of S. But if 0 ∪1 ∪{B} were in S, then
by property (S1) of S, 0 ∪{B} would be in S, which it is not. So it must be that
0 ∪1 ∪{C} is in S and hence 1 ∪{C} is in S by property (S0) of S.
By these preliminary manoeuvres, we have reduced proving the compactness the-
orem to proving the following lemma, which is a kind of converse to Lemma 13.1. In
stating it we suppose we have available an inﬁnite set of constants not occurring in
the set of sentences we are interested in.

13.1. OUTLINE OF THE PROOF
155
13.3 Lemma (Model existence lemma). Let L be a language, and L+ a language ob-
tained by adding inﬁnitely many new constants to L. If S* is a set of sets of sentences of
L+ having the satisfaction properties, then every set of sentences of L in S* has a model in
which each element of the domain is the denotation of some closed term of L+.
Note that the condition that every element of the domain is the denotation of some
closed term guarantees that, since we are working in an enumerable language, the do-
main will be enumerable, which means that we get not only the compactness but also
the L¨owenheim–Skolem theorem, as remarked in the preceding chapter (following
the statement of the two theorems in section 12.3).
So it ‘only’ remains to prove Lemma 13.3. The conclusion of Lemma 13.3 asserts
the existence of an interpretation in which every element of the domain is the deno-
tation of some closed term of the relevant language, and we begin by listing some
properties that the set of all sentences true in such an interpretation would have to
have.
13.4 Proposition (Closure properties lemma). Let L+ be a language and M an inter-
pretation thereof in which every element of the domain is the denotation of some closed
term. Then the set * of sentences true in M has the following properties:
(C1) For no sentence A are both A and ∼A in *.
(C2) If ∼∼B is in *, then B is in *.
(C3) If B ∨C is in *, then either B is in * or C is in *.
(C4) If ∼(B ∨C) is in *, then both ∼B and ∼C are in *.
(C5) If ∃xB(x) is in *, then for some closed term t of L+, B(t) is in *.
(C6) If ∼∃x B(x) is in *, then for every closed term t of L+, ∼B(t) is in *.
(C7) For every closed term t of L+, t = t is in *.
(C8) If B(s) and s = t are in *, then B(t) is in *.
Proof: For (C1), for no A are both A and ∼A true in the same interpretation. For
(C2), anything implied by anything true in a given interpretation is itself true in that
interpretation, and B is implied by ∼∼B. Similarly for (C4) and (C6)–(C8).
For (C3), any interpretation that makes a disjunct true must make at least one of
its disjuncts true.
For (C5), if ∃x B(x) is true in a given interpretation, then B(x) is satisﬁed by some
element m of the domain, and if that element m is the denotation of some closed term t,
then B(t) is true.
We call the properties (C1)–(C8) the closure properties. Actually, it is not
Proposition 13.4 itself that will be useful to us here, but the following converse.
13.5 Lemma (Term models lemma). Let * be a set of sentences with the closure
properties. Then there is an interpretation M in which every element of the domain is the
denotation of some closed term, such that every sentence in * is true in M.
To prove Lemma 13.3, it would sufﬁce to prove the foregoing lemma plus the
following one.

156
THE EXISTENCE OF MODELS
13.6 Lemma (Closure lemma). Let L be a language, and L+ a language obtained by
adding inﬁnitely many new constants to L. If S* is a set of sets of sentences of L+ having
the satisfaction properties, then every set  of sentences of L in S* can be extended to a set
* of sentences of L+ having the closure properties.
Sections 13.2 and 13.3 will be devoted to the proof of the term models lemma,
Lemma 13.5. As in so many other proofs, we consider ﬁrst, in section 13.2, the
case where identity and function symbols are absent, so that (C7) and (C8) may be
ignored, and the only closed terms are constants, and then, in section 13.3, consider the
additional complications that arise when identity is present, as well as those created by
the presence of function symbols. The proof of the closure lemma, Lemma 13.6, will
be given in section 13.4, with an alternative proof, avoiding any dependence on the
assumptionthatthelanguageisenumerable,tobeoutlinedintheoptionalsection13.5.
13.2 The First Stage of the Proof
In this section we are going to prove the term models lemma, Lemma 13.5, in the case
where identity and function symbols are absent. So let there be given a set * with
the closure properties (C1)–(C6), as in the hypothesis of the lemma to be proved. We
want to show that, as in the conclusion of that lemma, there is an interpretation in
which every element of the domain is the denotation of some constant of the language
of *, in which every sentence in * will be true.
To specify an interpretation M in this case, we need to do a number of things. To
begin with, we must specify the domain |M|. Also, we must specify for each constant
c of the language which element cM of the domain is to serve as its denotation.
Moreover, we must do all this in such a way that every element of the domain is the
denotation of some constant. This much is easily accomplished: simply pick for each
constant c some object cM, picking a distinct object for each distinct constant, and
let the domain consist of these objects.
To complete the speciﬁcation of the interpretation, we must specify for each pred-
icate R of the language what relation RM on elements of the domain is to serve as
its denotation. Moreover, we must do so in such a way that it will turn out that for
every sentence B in the language we have
if
B is in *
then
M |= B.
(1)
What we do is to specify RM in such a way that (1) automatically becomes true for
atomic B. We deﬁne RM by the following condition:
RM
cM
1 , . . . , cM
n

if and only if
R(c1, . . . , cn) is in *.
Now the deﬁnition of truth for atomic sentences reads as follows:
M |= R(c1, . . . , cn)
if and only if
RM
cM
1 , . . . , cM
n

.
We therefore have the following:
M |= R(c1, . . . , cn)
if and only if
R(c1, . . . , cn) is in *
(2)
and this implies (1) for atomic B.

13.3. THE SECOND STAGE OF THE PROOF
157
We also have (1) for negated atomic sentences. For if ∼R(c1, . . . , cn) is in *, then
by property (C1) of *, R(c1, . . . , cn) is not in *, and therefore by (2), R(c1, . . . , cn)
is not true in M, and so ∼R(c1, . . . , cn) is true in M, as required.
To prove (1) for other formulas, we proceed by induction on complexity. There are
three cases, according as A is a negation, a disjunction, or an existential quantiﬁcation.
However, we divide the negation case into subcases. Apart from the subcase of the
negation of an atomic sentence, which we have already handled, there are three of
these: the negation of a negation, the negation of a disjunction, and the negation of
an existential quantiﬁcation. So there are ﬁve cases in all:
to prove (1) for ∼∼B
assuming (1) for B
to prove (1) for B1 ∨B2
assuming (1) for each Bi
to prove (1) for ∼(B1 ∨B2)
assuming (1) for each ∼Bi
to prove (1) for ∃x B(x)
assuming (1) for each B(c)
to prove (1) for ∼∃x B(x)
assuming (1) for each ∼B(c).
The ﬁve cases correspond to the ﬁve properties (C2)–(C6), which are just what is
needed to prove them.
If ∼∼B is in *, then B is in * by property (C2). Assuming that (1) holds for B,
it follows that B is true in M. But then ∼B is untrue, and ∼∼B is true as required.
If B1 ∨B2 is in *, then Bi is in * for at least one of i = 1 or 2 by property (C3) of
*. Assuming (1) holds for this Bi, it follows that Bi is true in M. But then B1 ∨B2
is true as required. If ∼(B1 ∨B2) is in *, then each ∼Bi is in * for i = 1 or 2 by
property (C4) of *. Assuming (1) holds for the ∼Bi, it follows that each ∼Bi is
true in M. But then each Bi is untrue, so B1 ∨B2 is untrue, so ∼(B1 ∨B2) is true as
required.
In connection with existential quantiﬁcation, note that since every individual in the
domain is the denotation of some constant, ∃x B(x) will be true if and only if B(c) is
true for some constant c. If ∃x B(x) is in *, then B(c) is in * for some constant c
by property (C5) of *. Assuming (1) holds for this B(c), it follows that B(c) is true
in M. But then ∃x B(x) is true as required. If ∼∃x B(x) is in *, then ∼B(c) is in *
for every constant c by property (C6) of *. Assuming (1) holds for each ∼B(c), it
follows that ∼B(c) is true in M. But then B(c) is untrue for each c, and so ∃x B(x) is
untrue, and ∼∃x B(x) is true as required. We are done with the case without identity
or function symbols.
13.3 The Second Stage of the Proof
In this section we want to extend the result of the preceding section to the case where
identity is present, and then to the case where function symbols are also present.
Before describing the modiﬁcations of the construction of the preceding section
needed to accomplish this, we pause for a lemma.
13.7 Lemma. Let * be a set of sentences with properties (C1)–(C8). For closed terms
t and s write t ≡s to mean that the sentence t = s is in *. Then the following hold:
(E1) t ≡t.
(E2) If s ≡t, then t ≡s.

158
THE EXISTENCE OF MODELS
(E3) If t ≡s and s ≡r, then t ≡r.
(E4) If t1 ≡s1, . . . , tn ≡sn, then for any predicate R, R(t1, . . . , tn) is in * if and only
if R(s1, . . . , sn) is in *.
(E5) If t1 ≡s1, . . . , tn ≡sn, then for any function symbol f, f (t1, . . . , tn) = f (s1, . . . ,
sn) is in *.
Proof: (E1) is simply a restatement of (C7). For (E2), let B(x) be the formula
x = s. We now know that the sentence B(s), which is to say the sentence s = s, is in
*, so if s = t is in *, it follows by (C8) that the sentence B(t), which is to say the
sentence t = s, is in *. For (E3), let B(x) be the formula x = r. If t = s is in *, then
we now know s = t is in *, and if B(s), which is s = r, is in *, it follows by (C8)
that B(t), which is t = r, is in *. For (E4), if all ti = si are in * and R(s1, . . . , sn) is
in *, then repeated application of (C8) tells us that R(t1, s2, s3, . . . , sn) is in *, that
R(t1, t2, s3, . . . , sn) is in *, and so on, and ﬁnally that R(t1, . . . , tn) is in *. This
gives the ‘only if’ direction of (E4). For the ‘if’ direction, if all ti = si are in *, then
so are all si = ti, so if R(t1, . . . , tn) is in *, then by the direction we have already
proved, R(s1, . . . , sn) is in *. For (E5), the proof just given for (E4) applies not only
to atomic formulas R(x1, . . . , xn) but to arbitrary formulas F(x1, . . . , xn). Applying
this fact where F is the formula f (t1, . . . , tn) = f (x1, . . . , xn) gives (E5).
Note that (E1)–(E3) say that ≡is an equivalence relation. If we write [t] for the
equivalence class of t, then (E4) and (E5) may be rewritten as follows:
(E4′) If [t1] = [s1] , . . . , [tn] = [sn], then for any predicate R, R(t1, . . . , tn) is in * if
and only if R(s1, . . . , sn) is in *
(E5′) If [t1] = [s1], . . . , [tn] = [sn], then for any function symbol f, [ f (t1, . . . , tn)] =
[ f (s1, . . . , sn)].
We now return to the proof of the term models lemma, taking up the case where
identity is present but function symbols are absent, so the only closed terms are con-
stants. To specify the domain for our interpretation, instead of picking a distinct object
for each distinct constant, we pick a distinct object C* for each distinct equivalence
class C of constants. We let the domain of the interpretation consist of these objects,
and for the denotations of constants we specify the following:
cM = [c]*.
(3)
Since [c] = [d] if and only if c = d is in *, we then have:
cM = dM
if and only if
c = d is in *.
This is (the analogue of) (2) of the preceding section for atomic sentences involving
the logical predicate =, and gives us (1) of the preceding section for such sentences
and their negations.
What remains to be done is to deﬁne the denotation RM for a nonlogical predicate
R, in such a way that (2) of the preceding section will hold for atomic sentences

13.3. THE SECOND STAGE OF THE PROOF
159
involving nonlogical predicates. From that point, the rest of the proof will be exactly
the same as where identity was not present. Towards framing the deﬁnition of RM,
note that (E4′) allows us to give the following deﬁnition:
RM(C1*, . . . , Cn*)
if and only if
R(c1, . . . , cn) is in *
for some or equivalently any
ci with Ci = [ci].
Thus
RM([c1], . . . , [cn])
if and only if
R(c1, . . . , cn) is in *.
Together with (3), this gives (2) of the preceding section. Since as already indicated
the proof is the same from this point on, we are done with the case with identity but
without function symbols.
For the case with function symbols, we pick a distinct object T * for each equiva-
lence class of closed terms, and let the domain of the interpretation consist of these
objects. Note that (3) above still holds for constants. We must now specify for each
function symbol f what function f M on this domain is to serve as its denotation,
and in such a way that (3) will hold for all closed terms. From that point, the rest of
the proof will be exactly the same as in the preceding case where function symbols
were not present.
(E5′) allows us to give the following deﬁnition:
f M(T1*, . . . , Tn*) = T *
where
T = [ f (t1, . . . , tn)]
for some or equivalently any
ti with Ti = [ti].
Thus
f M([t1]*, . . . , [tn]*) = [ f (t1, . . . , tn)]*.
(4)
We can now prove by induction on complexity that (3) above, which holds by
deﬁnition for constants, in fact holds for any closed term t. For suppose (3) holds for
t1, . . . , tn, and consider f (t1, . . . , tn). By the general deﬁnition of the denotation of
a term we have
( f (t1, . . . , tn))M = f M
tM
1 , . . . , tM
n

.
By our induction hypothesis about the ti we have
tM
i
= [ti]*.
Putting these together, we get
( f (t1, . . . , tn))M = f M([t1]*, . . . , [tn]*).
And this together with the deﬁnition (4) above gives
( f (t1, . . . , tn))M = [ f (t1, . . . , tn)]*.

160
THE EXISTENCE OF MODELS
which is precisely (3) above for the closed term f (t1, . . . , tn). Since, as already
indicated, the proof is the same from this point on, we are done.
13.4 The Third Stage of the Proof
What remains to be proved is the closure lemma, Lemma 13.6. So let there be given
a language L, a language L+ obtained by adding inﬁnitely many new constants to
L, a set S* of sets of sentences of L+ having the satisfaction properties (S0)–(S8),
and a set  of sentences of L in S*, as in the hypotheses of the lemma to be proved.
We want to show that, as in the conclusion of that lemma,  can be extended to a set
* of sentences of L+ with closure properties (C1)–(C8).
The idea of the proof will be to obtain * as the union of a sequence of sets
0, 1, 2, . . . , where each n belongs to S* and each contains all earlier sets m for
m < n, and where 0 is just . (C1) will easily follow, because if A and ∼A were
both in *, A would be in some m and ∼A would be in some n, and then both
would be in k, where k is whichever of m and n is the larger. But since k is in S*,
this is impossible, since (S0) says precisely that no element of S* contains both A
and ∼A for any A.
What need to be worried about are (C2)–(C8). We have said that each k+1 will
be a set in S* containing k. In fact, each k+1 be obtained by adding to k a single
sentence Bk, so that k+1 = k ∪{Bk}. (It follows that each n will be obtained by
adding only ﬁnitely many sentences to , and therefore will involve only ﬁnitely
many of the constants of L+ that are not in the language L of , leaving at each
stage inﬁnitely many as yet unused constants.) At each stage, having k in S*, we
are free to choose as Bk any sentence such that k ∪{Bk} is still in S*. But we must
make the choices in such a way that in the end (C2)–(C8) hold.
Now how can we arrange that * fulﬁlls condition (C2), for example? Well, if
∼∼B is in *, it is in some m. If we can so arrange matters that whenever m and
B are such that ∼∼B is in m, then B is in k+1 for some k ≥m, then it will follow
that B is in *, as required by (C2). To achieve this, it will be more than enough if
we can so arrange matters that the following holds:
If
∼∼B is in m,
then
for some k ≥m,
k+1 = k ∪{B}.
But can we so arrange matters that this holds? Well, what does (S2) tell us? If ∼∼B
is in m, then ∼∼B will still be in k for any k ≥m, since the sets get larger. Since
each k is to be in S*, (S2) promises that k ∪{B} will be in S*. That is:
If
∼∼B is in m,
then
for any k ≥m,
k ∪{B} is in S*.
So we could take k+1 = k ∪{B} if we chose to do so.
To understand better what is going on here, let us introduce some suggestive
terminology. If ∼∼B is in m, let us say that the demand for admission of B is raised
at stage m; and if k+1 = k ∪{B}, let us say that the demand is granted at stage k.
What is required by (C2) is that any demand that is raised at any stage m should be
granted at some later stage k. And what is promised by (S2) is that at any stage k, any
one demand raised at any one earlier stage m could be granted. There is a gap here

13.4. THE THIRD STAGE OF THE PROOF
161
between what is demanded and what is promised, since it may well be that there are
inﬁnitely many demands raised at stage m, which is to say, inﬁnitely many sentences
of form ∼∼B in m, and in any case, there are inﬁnitely many stages m at which new
demands may arise—and all this only considering demands of the type associated
with condition (C2), whereas there are several other conditions, also raising demands,
that we also wish to fulﬁll.
Let us look at these. The relationship between (C3)–(C8) and (S3)–(S8) is exactly
the same as between (C2) and (S2). Each of (C2)–(C8) corresponds to a demand of
a certain type:
(C2) If ∼∼B is in m, then for some k ≥m, k+1 = k ∪{B}.
(C3) If B ∨C is in m, then for some k ≥m, k+1 = k ∪{B} or k ∪{C}.
(C4) If ∼(B ∨C) or ∼(C ∨B) is in m, then for some k ≥m, k+1 = k ∪{∼B}.
(C5) If ∃x B(x) is in m, then for some k ≥m, for some constant c,
k+1 = k ∪{B(c)}.
(C6) If ∼∃x B(x) is in m and t is a closed term in the language of m, then for some
k ≥m, k+1 = k ∪{∼B(t)}.
(C7) If t is a closed term in the language of m, then for some
k ≥m, k+1 = k ∪{t = t}.
(C8) If B(s) and s = t are in m, where s and t are closed terms B(x) a formula, then
for some k ≥m, k+1 = k ∪{B(t)}.
Each of (S2)–(S8) promises that any one demand of the relevant type can be granted:
(S2) If ∼∼B is in m, then for any k ≥m, k ∪{B} is in S*.
(S3) If B ∨C is in m, then for any k ≥m, k ∪{B} or k ∪{C} is in S*.
(S4) If ∼(B ∨C) or ∼(C ∨B) is in m, then for any k ≥m, k ∪{∼B} is in S*.
(S5) If ∃x B(x) is in m, then for any k ≥m, for any as yet unused constant c,
k ∪{B(c)} is in S*.
(S6) If ∼∃x B(x) is in m and t is a closed term in the language of m, then for any
k ≥m, k ∪{∼B(t)} is in S*.
(S7) If t is a closed term in the language of m, then for any k ≥m, k ∪{t = t} is in
S*.
(S8) If B(s) and s = t are in m, where s and t are closed terms B(x) a formula, then
for any k ≥m, k ∪{B(t)} is in S*.
At any stage k of the construction, we can grant any one demand we choose
from among those that have been raised at earlier stages, but for the construction
to succeed, we must make our successive choices so that in the end any demand
that is ever raised at any stage is granted at some later stage. Our difﬁculty is that
at each stage many different demands may be raised. Our situation is like that of
Herakles ﬁghting the hydra: every time we chop off one head (grant one demand),
multiple new heads appear (multiple new demands are raised). At least in one re-
spect, however, we have made progress: we have succeeded in redescribing our prob-
lem in abstract terms, eliminating all details about which particular formulas are of
concern.

162
THE EXISTENCE OF MODELS
And indeed, with this redescription of the problem we are now not far from a
solution. We need only recall two facts. First, our languages are enumerable, so that at
each stage, though an inﬁnity of demands may be raised, it is still only an enumerable
inﬁnity.Eachdemandmaybeworded‘admitsuch-and-suchasentence’(or‘admitone
or the other of two such-and-such sentences’), and an enumeration of the sentences
of our language therefore gives rise to an enumeration of all the demands raised at
any given stage. Thus each demand that is ever raised may be described as the ith
demand raised at stage m, for some numbers i and m, and so may be described by a
pair of numbers (i, m). Second, we have seen in Chapter 1 that there is a way—in fact,
there are many ways—of coding any pair of numbers by a single number j(i, m), and
if one looks closely at this coding, one easily sees that j(i, m) is greater than m (and
greater than i). We can solve our problem, then, by proceeding as follows. At stage k,
see what pair (i, m) is coded by k, and grant the ith demand that was raised at stage
m < k. In this way, though we grant only one demand at a time, all the demands that
are ever raised will eventually be granted.
The proof of the compactness theorem is now complete.
13.5* Nonenumerable Languages
In Chapter 12 we mentioned in passing the possibility of allowing nonenumerable
languages. The L¨owenheim–Skolem theorem would then fail.
13.8 Example (The failure of the downward L¨owenheim–Skolem theorem for a non-
enumerable language). Take one constant cξ for each real number ξ, and let  be the
set of all sentences cξ ̸= cη for ξ ̸= η. Clearly  has a model with domain the real numbers,
in which cξ denotes ξ. Equally clearly, any model of  will be nondenumerable.
However, it can be shown that the compactness theorem still holds. The proof
we have given does not work for a nonenumerable language: no essential use of the
enumerability of the language was made in the proof of the term models lemma, but
the proof given in the preceding section for the closure lemma did make heavy use at
the end of the enumerability assumption. In this section we outline a different proof
of the closure lemma, which can be generalized to cover nonenumerable languages,
and note one consequence of the generalized version of the compactness theorem.
Many veriﬁcations are relegated to the problems.
It is not hard to show that if  is a satisﬁable set of sentences, ∃x F(x) a sentence
of the language of , and c a constant not in the language of , then  ∪{∃x F(x) →
F(c)} is satisﬁable [imitating the proof of Example 10.5(b), which gave us (S5) in
Lemma 13.1]. Now let L be a language. Let L0 = L, and given Ln, let Ln+1 be the
result of adding to Ln a new constant cF for each formula ∃x F(x) of Ln. Let L+
be the union of all the Ln. The set of Henkin axioms is the set H of all sentences
∃x F(x) →F(cF) of L+. It is not hard to show that if  is a set of sentences of L and
every ﬁnite subset of  has a model then every ﬁnite subset of  ∪H has a model
(using the observation with which we began this paragraph). Let S* be the set of all
sets of sentences  of L+ such that every ﬁnite subset of  ∪H has a model. What

13.5. NONENUMERABLE LANGUAGES
163
we have just remarked is that if  is a set of sentences of L and every ﬁnite subset
of  has a model, then  is in S*. It is not hard to show that S* has the satisﬁability
properties (S1)–(S4) and (S6)–(S8) (imitating the proof of Lemma 13.2).
We now introduce some set-theoretic terminology. Let I be a nonempty set. A
family P of subsets of I is said to be of ﬁnite character provided that for each subset
 of I,  is in P if and only if each ﬁnite subset of  is in P. A subset * of I is
said to be maximal with respect to P if * is in P, but no subset  of I properly
including * is in P.
To apply this terminology to the situation we are considering, it is not hard to show
that S* is of ﬁnite character (essentially by deﬁnition). Nor is it hard to show that
any maximal element * of S* will contain H (by showing that adding a Henkin
axiom to a given set in S* produces a set still in S*, so that if the given set was
maximal, the Henkin axiom must already have belonged to it). Nor is it hard to
show that any maximal element * of S* will have closure properties (C1)–(C4) and
(C6)–(C8) [since, for instance, if ∼∼B is in *, then adding B to * produces a set
still in S* by (S2)]. Nor, for that matter, is it hard to show that such a * will also
have closure property (C5) [using the fact that whether or not ∃x F(x) is in *, *
contains the Henkin axioms ∼∃x F(x) ∨F(cF), and applying (C1) and (C3)]. Thus by
Lemma 13.5, whose proof made no essential use of enumerability, * will have a
model.
Putting everything together from the preceding several paragraphs, if  is a set of
sentences of L such that every ﬁnite subset of  has a model, then  itself will have
a model, provided we can prove that for every set  in S* there is a maximal element
* in S* that contains .
And this does follow using a general set-theoretic fact, the maximal principle,
according to which for any nonempty set I and any set P of subsets of I that has
ﬁnite character, and any  in P, there is a maximal element * of P that contains . It
is not hard to prove this principle in the case where I is enumerable (by enumerating
its elements i0, i1, i2, . . . , and building * as the union of sets n in P, where 0 = ,
and n+1 = n ∪{in} if n ∪{in} is in P, and = n otherwise). In fact, the maximal
principle is known to hold even for nonenumerable I, though the proof in this case
requires a formerly controversial axiom of set theory, the axiom of choice—indeed,
given the other, less controversial axioms of set theory, the maximal principle is
equivalent to the axiom of choice, a fact whose proof is given in any textbook on set
theory, but will not be given here.
Reviewing our work, one sees that using the maximal principle for a nonenumer-
able set, we get a proof of the compactness theorem for nonenumerable languages.
This general version of the compactness theorem has one notable consequence.
13.9 Theorem (The upward L¨owenheim–Skolem theorem). Any set of sentences that
has an inﬁnite model has a nonenumerable model.
The proof is not hard (combining the ideas of Corollary 12.16 and Example 13.8).
But like the proofs of several of our assertions above, we relegate this one to the
problems.

164
THE EXISTENCE OF MODELS
Problems
The ﬁrst several problems pertain to the optional section 13.5.
13.1 Prove the maximal principle for the case where I is enumerable.
13.2 Show that if  is a satisﬁable set of sentences, ∃x F(x) a sentence of the lan-
guageof,andc aconstantnotinthelanguageof,then ∪{∃x F(x)→F(c)}
is satisﬁable.
13.3 Let L be a language, and construct the language L+ and the set H of Henkin
axioms as in section 13.5. Let S* be the set of all sets of sentences  of L+
such that every ﬁnite subset of  ∪H has a model. Show that:
(a) Any set  of sentences of L whose every ﬁnite subset is satisﬁable is
in S*.
(b) S* has satisﬁability properties (S1)–(S4) and (S6)–(S8).
13.4 Continuing the notation of the preceding problem, show that:
(a) S* is of ﬁnite character.
(b) Any maximal set * in S* contains H.
13.5 Continuing the notation of the preceding problem, let * be a maximal set in
S*. Show that * has closure properties (C1)–(C4) and (C6)–(C8).
13.6 Continuing the notation of the preceding problem, let * be a set of sentences
of L+ containing H and having closure properties (C1)–(C4) and (C6)–(C8).
Show that * also has property (C5).
13.7 Use the compactness theorem for nonenumerable languages to prove the up-
ward L¨owenheim–Skolem theorem, Theorem 13.9.
In the remaining problems, for simplicity assume that function symbols are
absent, though the results indicated extend to the case where they are
present.
13.8 An embedding of one interpretation P in another interpretation Q is a function
j fulﬁlling all the conditions in the deﬁnition of isomorphism in section 13.1,
except that j need not be onto. Given an interpretation P, let LP be the result
of adding to the language a constant cp for each element p of the domain
|P|, and and let P* be the extension of P to an interpretation of LP in which
each cp denotes the corresponding p. The set (P) of all atomic and negated
atomic sentences of LP, whether involving a nonlogical predicate R or the
logical predicate =, that are true in P*, is called the diagram of P. Show that
if Q is any interpretation of the language of P that can be extended to a model
Q∗of (P), then there is an embedding of P into Q.
13.9 A sentence is called existential if and only if it is of the form ∃x1 . . . ∃xnF
where F contains no further quantiﬁers (universal or existential). A sentence
is said to be preserved upwards if and only if, whenever it is true in an
interpretation P, and there is an embedding of P in another interpretation
Q, then it is true in Q. Show that every existential sentence is preserved
upwards.
13.10 Let A be a sentence that is preserved upwards, P a model of A, and (P)
the diagram of P. Show that  ∪{∼A} is unsatisﬁable, and that some ﬁnite
subset of  ∪{∼A} is unsatisﬁable.

PROBLEMS
165
13.11 Let A be a sentence of a language L that is preserved upwards. Show that:
(a) P is a model of A if and only if there is a quantiﬁer-free sentence B of
the language LP such that B implies A and P∗is a model of B.
(b) P is a model of A if and only if there is an existential sentence B of the
language L such that B implies A and P is a model of B.
13.12 Let A be a sentence that is preserved upwards, and  the set of existential
sentencesofthelanguageof A thatimply A.Writing∼ forthesetofnegations
of elements of , show that:
(a) {A} ∪∼ is unsatisﬁable.
(b) {A} ∪∼0 is unsatisﬁable for some ﬁnite subset 0 of .
(c) {A} ∪{∼B} is unsatisﬁable for some single element of .
13.13 Let A be a sentence that is preserved upwards. Show that A is logically equiva-
lent to an existential sentence (in the same language).
13.14 A sentence is called universal if and only if it is of the form ∀x1 . . . ∀xn F
where F contains no further quantiﬁers (universal or existential). A sentence
is said to be preserved downwards if and only if, whenever it is true in an
interpretation Q, and there is an embedding of P in another interpretation Q,
then it is true in P. Prove that a sentence is preserved downwards if and only
if it is logically equivalent to a universal sentence (in the same language).
13.15 The proof in the preceding several problems involves (at the step of Problem
13.10) applying the compactness theorem to a language that may be nonenu-
merable. How could this feature be avoided?

14
Proofs and Completeness
Introductory textbooks in logic devote much space to developing one or another kind
of proof procedure, enabling one to recognize that a sentence D is implied by a set of
sentences , with different textbooks favoring different procedures. In this chapter we
introduce the kind of proof procedure, called a Gentzen system or sequent calculus, that
is used in more advanced work, where in contrast to introductory textbooks the emphasis
is on general theoretical results about the existence of proofs, rather than practice in
constructing speciﬁc proofs. The details of any particular procedure, ours included,
are less important than some features shared by all procedures, notably the features
that whenever there is a proof of D from , D is a consequence of , and conversely,
whenever D is a consequence of , there is a proof of D from . These features are called
soundness and completeness, respectively. (Another feature is that deﬁnite, explicit rules
can be given for determining in any given case whether a purported proof or deduction
really is one or not; but we defer detailed consideration of this feature to the next chapter.)
Section 14.1 introduces our version or variant of sequent calculus. Section 14.2 presents
proofs of soundness and completeness. The former is easy; the latter is not so easy, but
all the hard work for it has been done in the previous chapter. Section 14.3, which is
optional, comments brieﬂy on the relationship of our formal notion to other such formal
notions, as might be found in introductory textbooks or elsewhere, and of any formal
notion to the unformalized notion of a deduction of a conclusion from a set of premisses,
or proof of a theorem from a set of axioms.
14.1 Sequent Calculus
The idea in setting up a proof procedure is that even when it is not obvious that
 implies D, we may hope to break the route from  to D down into a series of small
steps that are obvious, and thus render the implication relationship recognizable.
Every introductory textbook develops some kind of formal notion of proof or deduc-
tion. Though these take different shapes in different books, in every case a formal
deduction is some kind of ﬁnite array of symbols, and there are deﬁnite, explicit rules
for determining whether a given ﬁnite array of symbols is or is not a formal deduc-
tion. The notion of deduction is ‘syntactic’ in the sense that these rules mention the
internal structure of formulas, but do not mention interpretations. In the end, though,
the condition that there exists a deduction of D from  turns out to be equivalent
to the condition that every interpretation making all sentences in  true makes the
sentence D true, which was the original ‘semantic’ deﬁnition of consequence. This
166

14.1. SEQUENT CALCULUS
167
equivalence has two directions. The result that whenever D is deducible from , D
is a consequence of , is the soundness theorem. The result that whenever D is a
consequence of , then D is deducible from , is the G¨odel completeness theorem.
Our goal in this chapter will be to present a particular system of deduction for which
soundness and completeness can be established. The proof of completeness uses the
main lemma from the preceding chapter. Our system, which is of the general sort used
in more advanced, theoretical studies, will be different from that used in virtually any
introductory textbook—or to put a positive spin on it, virtually no reader will have an
advantage over any other reader of previous acquaintance with the particular kind of
system we are going to be using. Largely for the beneﬁt of readers who have been or
will be looking at other books, in the ﬁnal section of the chapter we brieﬂy indicate the
kinds of variations that are possible and are actually to be met with in the literature.
But as a matter of fact, it is not the details of any particular system that really matter,
but rather the common features shared by all such systems, and except for a brief
mention at the end of the next chapter (in a section that itself is optional reading), we
will when this chapter is over never again have occasion to mention the details of our
particular system or any other. The existence of some proof procedure or other with
the properties of soundness and completeness will be the result that will matter.
[Let us indicate one consequence of the existence of such a procedure that will be
looked at more closely in the next chapter. It is known that the consequence relation
is not effectively decidable: that there cannot be a procedure, governed by deﬁnite
and explicit rules, whose application would, in every case, in principle enable one to
determine in a ﬁnite amount of time whether or not a given ﬁnite set  of sentences
implies a given sentence D. Two proofs of this fact appear in sections 11.1 and
11.2, with another to come in chapter 17. But the existence of a sound and complete
proof procedure shows that the consequence relation is at least (positively) effectively
semidecidable. There is a procedure whose application would, in case  does imply
D, in principle enable one to determine in a ﬁnite amount of time that it does so.
The procedure is simply to search systematically through all ﬁnite objects of the
appropriate kind, determining for each whether or not it constitutes a deduction of
D from . For it is part of the notion of a proof procedure that there are deﬁnite and
explicit rules for determining whether a given ﬁnite object of the appropriate sort does
or does not constitute such a deduction. If  does imply D, then checking through all
possible deductions one by one, one would by completeness eventually ﬁnd one that
is a deduction of D from , thus by soundness showing that  does imply D; but if
 does not imply D, checking through all possible deductions would go on forever
without result. As we said, these matters will be further discussed in the next chapter.]
At the same time one looks for a syntactic notion of deduction to capture and
make recognizable the semantic notion of consequence, one would like to have also
a syntactic notion of refutation to capture the semantic notion of unsatisﬁability,
and a syntactic notion of demonstration to capture the semantic notion of valid-
ity. At the cost of some very slight artiﬁciality, the three notions of consequence,
unsatisﬁability, and validity can be subsumed as special cases under a single, more
general notion. We say that one set of sentences  secures another set of sentences
 if every interpretation that makes all sentences in  true makes some sentence in

168
PROOFS AND COMPLETENESS
 true. (Note that when the sets are ﬁnite,  = {C1, . . . , Cm} and  = {D1, . . . , Dn},
this amounts to saying that every interpretation that makes C1 & . . . & Cm true makes
D1 ∨· · · ∨Dn true: the elements of  are being taken jointly as premisses, but the
elements of  are being taken alternatively as conclusions, so to speak.) When a set
contains but a single sentence, then of course making some sentence in the set true
and making every sentence in the set true come to the same thing, namely, making the
sentence in the set true; and in this case we naturally speak of the sentence as doing
the securing or as being secured. When the set is empty, then of course the condition
that some sentence in it is made true is not fulﬁlled, since there is no sentence in
it to be made true; and we count the condition that every sentence in the set is made
true as being ‘vacuously’ fulﬁlled. (After all, there is no sentence in the set that is not
made true.) With this understanding, consequence, unsatisﬁability, and validity can
be seen to be special cases of security in the way listed in Table 14-1.
Table 14-1. Metalogical notions
D is a consequence of 
if and only if
 secures {D}
 is unsatisﬁable
if and only if
 secures ∅
D is valid
if and only if
∅secures {D}
Correspondingly, our approach to deductions will subsume them along with refu-
tations and demonstrations under a more general notion of derivation. Thus for us the
soundness and completeness theorems will be theorems relating a syntactic notion of
derivability to a semantic notion of security, from which relationship various other
relationships between syntactic and semantic notions will follow as special cases.
The objects with which we are going to work in this chapter—the objects of which
derivations will be composed—are called sequents. A sequent  ⇒ consists of
a ﬁnite set of sentences  on the left, the symbol ⇒in the middle, and a ﬁnite set
of sentences  on the right. We call the sequent secure if its left side  secures its
right side . The goal will be to deﬁne a notion of derivation so that there will be a
derivation of a sequent if and only if it is secure.
Deliberately postponing the details of the deﬁnition, we just for the moment say
that a derivation will be a kind of ﬁnite sequence of sequents, called the steps
(or lines) of the derivation, subject to certain syntactic conditions or rules that re-
main to be stated. A derivation will be a derivation of a sequent  ⇒ if and only if
that sequent is its last step (or bottom line). A sequent will be derivable if and only
if there is some derivation of it. It is in terms of this notion of derivation that we will
deﬁne other syntactic notions of interest, as in Table 14-2.
Table 14-2. Metalogical notions
A deduction of D from 
is a
derivation of  ⇒{D}
A refutation of 
is a
derivation of  ⇒∅
A demonstration of D
is a
derivation of ∅⇒{D}

14.1. SEQUENT CALCULUS
169
We naturally say that D is deducible from  if there is a deduction of D from ,
that  is refutable if there is a refutation of , and that D is demonstrable if there
is a demonstration of D, where deduction, refutation, and demonstration are deﬁned
in terms of derivation as in Table 14-2. An irrefutable set of sentences is also called
consistent, and a refutable one inconsistent. Our main goal will be so to deﬁne the
notion of derivation that we can prove the following two theorems.
14.1 Theorem (Soundness theorem). Every derivable sequent is secure.
14.2 Theorem (G¨odel completeness theorem). Every secure sequent is derivable.
It will then immediately follow (on comparing Tables 14-1 and 14-2) that there is
an exact coincidence between two parallel sets of metalogical notions, the semantic
and the syntactic, as shown in Table 14-3.
Table 14-3. Correspondences between metalogical notions
D is deducible from 
if and only if
D is a consequence of 
 is inconsistent
if and only if
 is unsatisﬁable
D is demonstrable
if and only if
D is valid
To generalize to the case of inﬁnite sets of sentences, we simply deﬁne  to be
derivable from  if and only if some ﬁnite subset 0 of  is derivable from some
ﬁnite subset 0 of , and deﬁne deducibility and inconsistency in the inﬁnite case
similarly. As an easy corollary of the compactness theorem,  secures  if and only
if some ﬁnite subset 0 of  secures some ﬁnite subset 0 of . Thus Theorems 14.1
and 14.2 will extend to the inﬁnite case:  will be derivable from  if and only if 
is secured by , even when  and  are inﬁnite.
So much by way of preamble. It remains, then, to specify what conditions a
sequence of sequents must fulﬁll in order to count as a derivation. In order for a
sequence of steps to qualify as a derivation, each step must either be of the form
{A} ⇒{A} or must follow from earlier steps according to one of another of several
rules of inference permitting passage from one or more sequents taken as premisses
to some other sequent taken as conclusion. The usual way of displaying rules is to
write the premiss or premisses of the rule, a line below them, and the conclusion
of the rule. The provision that a step may be of the form {A} ⇒{A} may itself be
regarded as a special case of a rule of inference with zero premisses; and in listing the
rules of inference, we in fact list this one ﬁrst. In general, in the case of any rule, any
sentence that appears in a premiss but not the conclusion of a rule is said to be exiting,
any that appears in the conclusion but not the premisses is said to be entering, and
any that appears in both a premiss and the conclusion is said to be standing. In the
special case of the zero-premiss rule and steps of the form {A} ⇒{A}, the sentence
A counts as entering. It will be convenient in this chapter to work as in the preceding
chapter with a version of ﬁrst-order logic in which the only logical symbols are ∼,
∨, ∃, =, that is, in which & and ∀are treated as unofﬁcial abbreviations. (If we
admitted & and ∀, there would be a need for four more rules, two for each. Nothing

170
PROOFS AND COMPLETENESS
Table 14-4. Rules of sequent calculus
(R0)
{A} ⇒{A}
(R1)
 ⇒
′ ⇒′
 subset of ′,  subset of ′
(R2a)
 ∪{A} ⇒
 ⇒{∼A} ∪
(R2b)
 ⇒{A} ∪
 ∪{∼A} ⇒
(R3)
 ⇒{A, B} ∪
 ⇒{(A ∨B)} ∪
(R4)
 ∪{A} ⇒
 ∪{B} ⇒
 ∪{A ∨B} ⇒
(R5)
 ⇒{A(s)} ∪
 ⇒{∃x A(x)} ∪
(R6)
 ∪{A(c)} ⇒
 ∪{∃x A(x)} ⇒
c not in  or  or A(x)
(R7)
 ∪{s = s} ⇒
 ⇒
(R8a)
 ⇒{A(t)} ∪
 ∪{s = t} ⇒{A(s)} ∪
(R8b)
 ∪{A(t)} ⇒
 ∪{s = t, A(s)} ⇒
(R9a)
 ∪{∼A} ⇒
 ⇒{A} ∪
(R9b)
 ⇒{∼A} ∪
 ∪{A} ⇒
would be harder, but everything would be more tedious.) With this understanding,
the rules are those give in Table 14-4.
These rules roughly correspond to patterns of inference used in unformalized
deductive argument, and especially mathematical proof. (R2a) or right negation in-
troduction corresponds to ‘proof by contradiction’, where an assumption A is shown
to be inconsistent with background assumptions  and it is concluded that those
background assumptions imply its negation. (R2b) or left negation introduction cor-
responds to the inverse form of inference. (R3) or right disjunction introduction,
together with (R1), allows us to pass from  ⇒{A} ∪ or  ⇒{B} ∪ by way of
 ⇒{A, B} ∪ to  ⇒{(A ∨B)} ∪, which corresponds to inferring a disjunc-
tion from one disjunct. (R4) or left disjunction introduction corresponds to ‘proof by
cases’, where something that has been shown to follow from each disjunct is con-
cluded to follow from a disjunction. (R5) or right existential quantiﬁer introduction
corresponds to inferring an existential generalization from a particular instance. (R6)
or left existential-quantiﬁer introduction is a bit subtler: it corresponds to a common
procedure in mathematical proof where, assuming there is something for which a

14.1. SEQUENT CALCULUS
171
condition A holds, we ‘give it a name’ and say ‘let c be something for which the
condition A holds’, where c is some previously unused name, and thereafter proceed
to count whatever statements not mentioning c that can be shown to follow from the
assumption that condition A holds for c as following from the original assumption
that there is something for which condition A holds. (R8a, b) correspond to two forms
of ‘substituting equals for equals’.
A couple of trivial examples will serve show how derivations are written.
14.3 Example. The deduction of a disjunction from a disjunct.
(1)
A ⇒A
(R0)
(2)
A ⇒A, B
(R1), (1)
(3)
A ⇒A ∨B
(R3), (2)
The ﬁrst thing to note here is that though ofﬁcially what occur on the left and
right sides of the double arrow in a sequent are sets, and sets have no intrinsic order
among their elements, in writing a sequent, we do have to write those elements in
some order or other. {A, B} and {B, A} and for that matter {A, A, B} are the same set,
and therefore {A} ⇒{A, B} and {A} ⇒{B, A} and for that matter {A} ⇒{A, A, B}
are the same sequent, but we have chosen to write the sequent the ﬁrst way. Actually,
we have not written the braces at all, nor will they be written in future when writing
out derivations. [For that matter, have also been writing A ∨B for (A ∨B), and will
be writing Fx for F(x) below.] An alternative approach would be to have sequences
rather than sets of formulas on both sides of a sequent, and introduce additional
‘structural’ rules allowing one to reorder the sentences in a sequences, and for that
matter, to introduce or eliminate repetitions.
The second thing to note here is that the numbering of the lines on the left, and
the annotations on the right, are not ofﬁcially part of the derivation. In practice, their
presence makes it easier to check that a purported derivation really is one; but in
principle it can be checked whether a string symbols constituties a derivation even
without such annotation. For there are, after all, at each step only ﬁnitely many
rules that could possibly have been applied to get that step from earlier steps, and
only ﬁnitely many earlier steps any rule could possibly have been applied to, and in
principle we need only check through these ﬁnitely many possibilities to ﬁnd whether
there is a justiﬁcation for the given step.
14.4 Example. The deduction of a conjunct from a conjunction
(1)
A ⇒A
(R0)
(2)
A, B ⇒A
(R1), (1)
(3)
B ⇒A, ∼A
(R2a), (2)
(4)
⇒A, ∼A, ∼B
(R2a), (3)
(5)
⇒A, ∼A ∨∼B
(R3), (4)
(6)
∼(∼A ∨∼B) ⇒A
(R2b), (5)
(7)
A & B ⇒A
abbreviation, (6)
Here the last step, reminding us that ∼(∼A ∨∼B) is what A & B abbreviates, is
unofﬁcial, so to speak. We omit the word ‘abbreviation’ in such cases in the future. It

172
PROOFS AND COMPLETENESS
is because & is not in the ofﬁcial notation, and we do not directly have rules for it, that
the derivation in this example needs more steps than that in the preceding example.
Since the two examples so far have both been of derivations constituting deduc-
tions, let us give two equally short examples of derivations constituting refutations
and demonstrations.
14.5 Example. Demonstration of a tautology
(1)
A ⇒A
(R0)
(2)
⇒A, ∼A
(R2b), (1)
(3)
⇒A ∨∼A
(R3), (2)
14.6 Example. Refutation of a contradiction
(1)
∼A ⇒∼A
(R0)
(2)
⇒∼A, ∼∼A
(R2b), (1)
(3)
⇒∼A ∨∼∼A
(R3), (2)
(4)
∼(∼A ∨∼∼A) ⇒
(R2a), (3)
(5)
A & ∼A ⇒
(4)
The remarks above about the immateriality of the order in which sentences are
written are especially pertinent to the next example.
14.7 Example. Commutativity of disjunction
(1)
A ⇒A
(R0)
(2)
A ⇒B, A
(R1), (1)
(3)
A ⇒B ∨A
(R3), (2)
(4)
B ⇒B
(R0)
(5)
B ⇒B, A
(R1), (4)
(6)
B ⇒B ∨A
(R3), (5)
(7)
A ∨B ⇒B ∨A
(R4), (3), (6)
The commutativity of conjunction would be obtained similarly, though there would
be more steps, for the same reason that there are more steps in Examples 14.4 and
14.6 than in Examples 14.3 and 14.5. Next we give a couple of somewhat more
substantial examples, illustrating how the quantiﬁer rules are to be used, and a couple
of counter-examples to show how they are not to be used.
14.8 Example. Use of the ﬁrst quantiﬁer rule
(1)
Fc ⇒Fc
(R0)
(2)
⇒Fc, ∼Fc
(R2b), (1)
(3)
⇒∃x Fx, ∼Fc
(R5), (2)
(4)
⇒∃x Fx, ∃x ∼Fx
(R5), (3)
(5)
∼∃x ∼Fx ⇒∃x Fx
(R2a), (4)
(6)
∀x Fx ⇒∃x Fx
(5)

14.1. SEQUENT CALCULUS
173
14.9 Example. Proper use of the two quantiﬁer rules
(1)
Fc ⇒Fc
(R0)
(2)
Fc ⇒Fc, Gc
(R1), (1)
(3)
Gc ⇒Gc
(R0)
(4)
Gc ⇒Fc, Gc
(R1), (3)
(5)
Fc ∨Gc ⇒Fc, Gc
(R4), (2), (4)
(6)
Fc ∨Gc ⇒∃xF x, Gc
(R5), (5)
(7)
Fc ∨Gc ⇒∃xF x, ∃xGx
(R5), (6)
(8)
Fc ∨Gc ⇒∃xF x ∨∃xGx
(R3), (7)
(9)
∃x(F x ∨Gx) ⇒∃xF x ∨∃xGx
(R6), (8)
14.10 Example. Improper use of the second quantiﬁer rule
(1)
Fc ⇒Fc
(R0)
(2)
Fc, ∼Fc ⇒
(R2b), (1)
(3)
∃xF x, ∼Fc ⇒
(R6), (2)
(4)
∃xF x, ∃x ∼F x ⇒
(R6), (3)
(5)
∃xF x ⇒∼∃x ∼F x
(R2b), (4)
(6)
∃xF x ⇒∀xF x
(5)
Since ∃xFx does not imply ∀xFx, there must be something wrong in this last
example, either with our rules, or with the way they have been deployed in the
example.Infact,itisthedeploymentof(R6)atline(3)thatisillegitimate.Speciﬁcally,
the side condition ‘c not in ’ in the ofﬁcial statement of the rule is not met, since
the relevant  in this case would be {∼Fc}, which contains c. Contrast this with a
legitimate application of (R6) as at the last line in the preceding example. Ignoring
the side condition ‘c not in ’ can equally lead to trouble, as in the next example.
(Trouble can equally arise from ignoring the side condition ‘c not in A(x)’, but we
leave it to the reader to provide an example.)
14.11 Example. Improper use of the second quantiﬁer rule
(1)
Fc ⇒Fc
(R0)
(2)
∃xF x ⇒Fc
(R6), (1)
(3)
∃xF x, ∼Fc ⇒
(R2b), (2)
(4)
∃xF x, ∃x ∼F x ⇒
(R6), (3)
(5)
∃xF x ⇒∼∃x ∼F x
(R2a), (4)
(6)
∃xF x ⇒∀xF x
(5)
Finally, let us illustrate the use of the identity rules.
14.12 Example. Reﬂexivity of identity
(1)
c = c ⇒c = c
(R0)
(2)
⇒c = c
(R7), (1)
(3)
∼c = c ⇒
(R2b), (2)
(4)
∃x ∼x = x ⇒
(R6), (3)
(5)
⇒∼∃x ∼x = x
(R2a), (4)
(6)
⇒∀x x = x
(5)

174
PROOFS AND COMPLETENESS
14.13 Example. Symmetry of identity
(1)
d = d ⇒d = d
(R0)
(2)
d = d, c = d ⇒d = c
(R8a), (1)
(3)
c = d ⇒d = c
(R7), (2)
(4)
⇒∼c = d, d = c
(R2a), (3)
(5)
⇒∼c = d ∨d = c
(R3), (4)
(6)
⇒c = d →d = c
(5)
(7)
∼(c = d →d = c) ⇒
(R2b), (6)
(8)
∃y ∼(c = y →y = c) ⇒
(R6), (7)
(9)
⇒∼∃y ∼(c = y →y = c)
(R2a), (8)
(10)
⇒∀y(c = y →y = c)
(9)
(11)
∼∀y(c = y →y = c) ⇒
(R2b), (10)
(12)
∃x ∼∀y(x = y →y = x) ⇒
(R6), (11)
(13)
⇒∼∃x ∼∀y(x = y →y = x)
(R2a), (12)
(14)
⇒∀x∀y(x = y →y = x)
(13)
The formula A(x) to which (R8a) has been applied at line (2) is d = x.
14.2 Soundness and Completeness
Let us now begin the proof of soundness, Theorem 14.1, according to which every
derivable sequent is secure. We start with the observation that every (R0) sequent
{A} ⇒{A} is clearly secure. It will then sufﬁce to show that each rule (R1)–(R9) is
sound in the sense that when applied to secure premisses it yields secure conclusions.
Consider, for instance, an application of (R1). Suppose  ⇒ is secure, where 
is a subset of ′ and  is a subset of ′, and consider any interpretation that makes
all the sentences in ′ true. What (R1) requires is that it should make some sentence
in ′ true, and we show that it does as follows. Since  is a subset of ′, it makes all
the sentences in  true, and so by the security of  ⇒ it makes some sentence in
 true and, since  is a subset of ′, thereby makes some sentence of ′ true.
Each of the rules (R2)–(R9) must now be checked in a similar way. Since this proof
is perhaps the most tedious in our whole subject, it may be well to remark in advance
that it does have one interesting feature. The feature is this: that as we argue for the
soundness of the formal rules, we are going to ﬁnd ourselves using something like the
unformalized counterparts of those very rules in our argumentation. This means that
a mathematical heretic who rejected one of another of the usual patterns of argument
as employed in unformalized proofs in orthodox mathematics—and there have been
benighted souls who have rejected the informal counterpart of (R9), for example—
would not accept our proof of the soundness theorem. The point of the proof is not to
convince such dissenters, but merely to check that, in putting everything into symbols,
we have not made some slip and allowed some inference that, stated in unformalized
terms, we ourselves would recognize as fallacious. (This is a kind of mistake that it
is not hard to make, especially over the side conditions in the quantiﬁer rule, and it is
one that has been made in the past in some textbooks.) This noted, let us now return
to the proof.

14.2. SOUNDNESS AND COMPLETENESS
175
Consider (R2a). We suppose  ∪{A} ⇒ is secure, and consider any interpre-
tation that makes all the sentences in  true. What (R2a) requires is that it should
make some sentence in {∼A} ∪ true, and we show it does as follows. On the one
hand, if the given interpretation also makes A true, then it makes all the sentences in
 ∪{A} true, and therefore by the security of  ∪{A} ⇒ makes some sentence in
 true, and therefore makes some sentence in {∼A} ∪ true. On the other hand, if
the interpretation does not make A true, then it makes ∼A true, and therefore it again
makes some sentence in {∼A} ∪ true.
Consider (R2b). We suppose  ⇒{A} ∪ is secure, and consider any interpre-
tation making all sentences in  ∪{∼A} true. What (R2b) requires is that it should
make some sentence in  true, and we show it does as follows. The given interpre-
tation makes all sentences in  true, and so by the security of  ⇒{A} ∪ makes
some sentence in {A} ∪ true. But since the interpretation makes ∼A true, it does
not make A true, so it must be that it makes some sentence in  true.
For (R3), we suppose that  ⇒{A, B} ∪ is secure, and consider any interpreta-
tion making all sentences in  true. By the security of  ⇒{A, B} ∪ the interpreta-
tion makes some sentence in {A, B} ∪ true. This sentence must be either A or B or
some sentence in . If the sentence is A or B, then the interpretation makes (A ∨B)
true, and so makes a sentence in {(A ∨B)} ∪ true. If the sentence is one of those
in , then clearly the interpretation makes a sentence in {(A ∨B)} ∪ true. So in
any case, some sentence in {(A ∨B)} ∪ is made true, which is what (R3) requires.
For (R4), we suppose that  ∪{A} ⇒ and  ∪{B} ⇒ are secure, and consider
any interpretation that makes all sentences in  ∪{(A ∨B)} true. The interpretation
in particular makes (A ∨B) true, and so it must either make A true or make B true.
In the former case it makes all sentences in  ∪{A} true, and by the security of
 ∪{A} ⇒ it makes some sentence in  true. Similarly in the latter case. So in
either case it makes some sentence in  true, which is what (R4) requires.
For (R5), we suppose that  ⇒{A(s)} ∪ is secure and consider any interpreta-
tion that makes all sentences in  true. By the security of  ⇒{A(s)} ∪ it makes
some sentence in {A(s)} ∪ true. If the sentence is one in , then clearly the inter-
pretation makes some sentence in {∃x A(x)} ∪ true. If the sentence is A(s), then the
interpretation makes ∃x A(x) true, and so again the interpretation makes some sen-
tence in {∃x A(x)} ∪ true. This sufﬁces to show that  ⇒{∃x A(x)} ∪ is secure,
which is what (R5) requires.
For(R6),wesupposethat ∪{A(c)} ⇒issecureandconsideranyinterpretation
making all sentences in  ∪{∃x A(x)} true. Since the interpretation makes ∃x A(x)
true, there is some element i in the domain of the interpretation that satisﬁes A(x). If
c does not occur in  or  or A(x), then while leaving the denotations of all symbols
that occur in  and  and A(x) unaltered, we can alter the interpretation so that the
denotation of c becomes i. By extensionality, in the new interpretation every sentence
in  will still be true, i will still satisfy A(x) in the new interpretation, and every
sentence in  will have the same truth value as in the old interpretation. But since i
is now the denotation of c, and i satisﬁes A(x), it follows that A(c) will be true in
the new interpretation. And since the sentences in  are still true and A(c) is now
true, by the security of  ∪{A(c)} ⇒, some sentence in  true will be true in the

176
PROOFS AND COMPLETENESS
new interpretation and hence will have been true in old interpretation. This sufﬁces
to show that  ∪{∃x A(x)} ⇒ is secure.
For (R7), we suppose  ∪{s = s} ⇒ is secure, and consider any interpretation
of a language containing all symbols in  and  that makes all sentences in  true.
If there is some symbol in s not occurring in  or  to which this interpretation fails
to assign a denotation, alter it so that it does. The new interpretation will still make
every sentence in  true by extensionality, and will make s = s true. By the security
of  ∪{s = s} ⇒, the new interpretation will make some sentence in  true, and
extensionality implies that the original interpretation already made this same sentence
in  true. This sufﬁces to show that  ⇒ is secure.
For (R8a), we suppose  ⇒{A(t)} ∪ is secure and consider any interpretation
making all sentences in  ∪{s = t} true. Since it makes every sentence in  true, by
the security of  ⇒{A(t)} ∪ it must make some sentence in {A(t)} ∪ true. If
this sentence is one of those in , then clearly the interpretation makes a sentence
in {A(s)} ∪ true. If the sentence is A(t), then note that since the interpretation
makes s = t true, it must assign the same denotation to s and to t, and therefore by
the must also make A(s) true by extensionality. Thus again it makes some sentence
in {A(s)} ∪ true. This sufﬁces to show that  ∪{s = t} ⇒{A(s)} ∪ is secure.
(R8b) is entirely similar.
(R9) is just like (R2), to ﬁnish the proof of soundness.
Now, for completeness, Theorem 14.2, according to which every secure sequent
is derivable. We begin with a quick reduction of the problem. Write ∼ for the set
of negations of sentences in .
14.14 Lemma.  ⇒ is derivable if and only if  ∪∼ is inconsistent.
Proof: If
{C1, . . . , Cm} ⇒{D1, . . . , Dn}
is derivable, then
{C1, . . . , Cm, ∼D1} ⇒{D2, . . . , Dn}
{C1, . . . , Cm, ∼D1, ∼D2} ⇒{D3, . . . , Dn}
...
{C1, . . . , Cm, ∼D1, . . . , ∼Dn} ⇒∅
are derivable by repeated application of (R2b). If the last of these is derivable, then
{C1, . . . , Cm, ∼D2, . . . , ∼Dn} ⇒{D1}
{C1, . . . , Cm, ∼D3, . . . , ∼Dn} ⇒{D1, D2}
...
{C1, . . . , Cm} ⇒{D1, . . . , Dn}
are derivable by repeated application of (R9a).

14.2. SOUNDNESS AND COMPLETENESS
177
Since it is easily seen that  secures  if and only if  ∪∼ is unsatisﬁable,
proving that if  secures , then  ⇒ is derivable, which is what we want to do,
reduces to showing that any consistent set is satisﬁable. (For if  secures , then
 ∪∼ is unsatisﬁable, and supposing we have succeeded in showing that it would
be satisﬁable if it were consistent, it follows  ∪∼ is inconsistent, and so by the
preceding lemma  ⇒ is derivable.) By the main lemma of the preceding chapter,
in order to show every consistent set is satisﬁable, it will sufﬁce to show that the set S
of all consistent sets has the satisﬁability properties (S0)–(S8). (For any consistent
set  will by deﬁnition belong to S, and what Lemma 13.3 tells us is that if S has the
satisfaction properties, then any element of S is satisﬁable.) This we now proceed to
verify, recalling the statements of properties (S0)–(S8) one by one as we prove S has
them.
Consider (S0). This says that if  is in S and 0 is a subset of , then 0 is in S.
So what we need to prove is that if  ⇒∅is not derivable, and 0 is a subset of ,
then 0 ⇒∅is not derivable. Contraposing, this is equivalent to proving:
(S0) If 0 ⇒∅is derivable, and 0 is a subset of , then  ⇒∅is derivable.
We show this by indicating how to extend any given derivation of 0 ⇒∅to a
derivation of  ⇒∅. In fact, only one more step need be added, as follows:
...
0 ⇒∅
Given
 ⇒∅.
(R1)
(Here the three dots represent the earlier steps of the hypothetical derivation of
0 ⇒∅.)
For each of (S1)–(S8) we are going to give a restatement, in contraposed form, of
what is to be proved, and then show how to prove it by extending a given derivation
to a derivation of the sequent required. First (S1)
(S1) If A and ∼A are both in , then  ⇒∅is derivable.
The hypothesis may be restated as saying that {A, ∼A} is a subset of . We then have
{A} ⇒{A}
(R0)
{A, ∼A} ⇒∅
(R2a)
 ⇒∅.
(R1)
As for (S2), literally, this says that:
(S2) If  ⇒∅is not derivable and ∼∼B is in , then  ∪{B} ⇒∅is not derivable.
Contraposing, this says that if  ∪{B} ⇒∅is derivable and ∼∼B is in , then
 ⇒∅is derivable. What we actually show is that if  ∪{B} ⇒∅is derivable, then

178
PROOFS AND COMPLETENESS
whether or not ∼∼B is in ,  ∪{∼∼B} ⇒∅is derivable. In case ∼∼B is in , we
have  ∪{∼∼B} = , so what we actually show is something a little more general
than what we need:
...
 ∪{B} ⇒∅
Given
 ⇒{∼B}
(R2b)
 ∪{∼∼B} ⇒∅.
(R2a)
Analogous remarks apply to (S3)–(S8) below.
(S3) If  ∪{B} ⇒∅and  ∪{C} ⇒∅are both derivable, then  ∪{B ∨C} ⇒∅
is derivable.
Here we concatenate the two given derivations, writing one after the other:
...
 ∪{B} ⇒∅
Given
...
 ∪{C} ⇒∅
Given
 ∪{B ∨C} ⇒∅.
(R4)
(S4) If either  ∪{∼B} ⇒∅or  ∪{∼C} ⇒∅is derivable, then
 ∪{∼(B ∨C)} ⇒∅is derivable.
The two cases are exactly alike, and we do only the ﬁrst:
...
 ∪{∼B} ⇒∅
Given
 ⇒{B}
(R9a)
 ⇒{B, C}
(R1)
 ⇒{B ∨C}
(R3)
 ∪{∼(B ∨C)} ⇒∅.
(R2a)
(S5) If  ∪{B(c)} ⇒∅is derivable, where c does not occur in  ∪{∃x B(x)}, then
 ∪{∃x B(x)} ⇒∅is derivable:
...
 ∪{B(c)} ⇒∅
Given
 ∪{∃x B(x)} ⇒∅.
(R6)
Note that the hypothesis that c does not occur in  or ∃x B(x) (nor of course in ∅)
means that the side conditions for the proper application of (R6) are met.

14.3. OTHER PROOF PROCEDURES AND HILBERT’S THESIS
179
(S6) If  ∪{∼B(t)} ⇒∅is derivable for some closed term t, then
 ∪{∼∃x B(x)} ⇒∅is derivable:
...
 ∪{∼B(t)} ⇒∅
Given
 ⇒{B(t)}
(R9a)
 ⇒{∃x B(x)}
(R5)
 ∪{∼∃x B(x)} ⇒∅.
(R2a)
(S7)  ∪{t = t} ⇒∅derivable for some closed term t, then  ⇒∅is derivable:
...
 ∪{t = t} ⇒∅
Given
 ⇒∅.
(R7)
(S8) If  ∪{B(t)} ⇒∅is derivable, then  ∪{B(s), s = t} ⇒∅is derivable:
...
 ∪{B(t)} ⇒∅
Given
 ⇒{∼B(t)}
(R2b)
 ∪{s = t} ⇒{∼B(s)}
(R8a)
 ∪{s = t, B(s)} ⇒∅.
(R9b)
This veriﬁcation ﬁnishes the proof of completeness.
14.3* Other Proof Procedures and Hilbert’s Thesis
A great many other sound and complete proof procedures are known. We begin by
considering modiﬁcations of our own procedure that involve only adding or dropping
a rule or two, and ﬁrst of all consider the result of dropping (R9). The following
lemma says that it will not be missed. Its proof gives just a taste of the methods of
proof theory, a branch of logical studies that otherwise will be not much explored in
this book.
14.15 Lemma (Inversion lemma). Using (R0)–(R8):
(a) If there is a derivation of  ∪{∼A} ⇒, then there is a derivation of
 ⇒{A} ∪.
(b) If there is a derivation of  ⇒{∼A} ∪, then there is a derivation of
 ∪{A} ⇒.
Proof: The two parts are similarly proved, and we do only (a). A counterexample to
the lemma would be a derivation of a sequent  ∪{∼A} ⇒ for which no derivation
of  ⇒{A} ∪ is possible. We want to show there can be no counterexample by
showing that a contradiction follows from the supposition that there is one. Now if
there are any counterexamples, among them there must be one that is as short as

180
PROOFS AND COMPLETENESS
possible, so that no strictly shorter derivation would be a counterexample. So suppose
that  ∪{∼A} ⇒ is the sequent derived in such a shortest possible counterexample.
We ask by what rule the last step  ∪{∼A} ⇒ could have been justiﬁed.
Could it have been (R0)? If that were so, the counterexample would simply be
the one-step derivation of {∼A} ⇒{∼A}, and we would have  = ∅,  = {∼A}.
The sequent  ⇒{A} ∪ for which supposedly no derivation exists would then just
be ⇒{A, ∼A}. But there is a derivation of this sequent, in two steps, starting with
{A} ⇒{A} by (R0) and proceeding to ⇒{A, ∼A} by (R2a). So (R0) is excluded,
and  ∪{∼A} ⇒ must have been inferred from some earlier step or steps by one
of the other rules.
Could it have been (R3)? If that were so, the counterexample would be a derivation
of
 ∪{∼A} ⇒{(B ∨C)} ∪′
where the last step was obtained from
 ∪{∼A} ⇒{B, C} ∪′.
But then, since the derivation down to this last-displayed sequent is too short to be a
counterexample, there will be a derivation of
 ⇒{A} ∪{B, C} ∪′,
and by applying (R3) we can then get
 ⇒{A} ∪{(B ∨C)} ∪′,
which is precisely what we are supposed not to be able to get in the case of a
counterexample to the lemma. Thus (R3) is excluded. Moreover, every case where
∼A is not an entering sentence is excluded for entirely similar reasons.
There remain to be considered three cases where ∼A is an entering sentence. One
case where ∼A enters arises when  ∪{∼A} ⇒ is obtained by (R1) from ′ ⇒′,
where ′ is a subset of  not containing ∼A and ′ is a subset of . But in this case
 ⇒{A} ∪ equally follows by (R1) from ′ ⇒′, and we have no counterexample.
If ∼A enters when  ∪{∼A} ⇒ is obtained by (R2b), the premiss must be
 ⇒{A} ∪ itself or  ∪{∼A} ⇒{A} ∪, and in the latter case, since the deriva-
tion of the premiss is too short to be a counterexample, there must exist a derivation
of  ⇒{A} ∪{A} ∪ or  ⇒{A} ∪; so we have no counterexample.
The other case where ∼A enters arises when ∼A is of the form ∼B(s) and the last
lines of the derivation are
 ∪{∼B(t)} ⇒
 ∪{s = t, ∼B(s)} ⇒
using (R8b).

14.3. OTHER PROOF PROCEDURES AND HILBERT’S THESIS
181
to which may be added the step
 ∪{s = t} ⇒{B(s)} ∪
which follows by (R8a), and again we have no counterexample.
14.16 Corollary. Any sequent derivable using (R0)–(R9) is in fact derivable using only
(R0)–(R8).
Proof: Suppose there were a counterexample, that is, a derivation using (R0)–(R9)
the last step  ⇒ of which was not derivable using just (R0)–(R8). Among all such
derivations, consider a derivation  that is as short as possible for a counterexample.
 ⇒ is not of the form {A} ⇒{A}, since any sequent of that form can be derived
in one step by (R0). So in  the sequent  ⇒ is inferred by from one or more
premisses appearing as earlier steps. Since the derivation down to any earlier step is
too short to be a counterexample, for each premiss there is a derivation of it using
just (R0)–(R8). If there is only one premiss, let 0 be such a derivation of it. If there
are more than one premiss, let 0 be the result of concatenating such a derivation
for each premiss, writing one after the other. In either case, 0 is a derivation using
only (R0)–(R8) that includes any and all premisses among its steps. Let ′ be the
derivation that results on adding  ⇒ as one last step, inferred by the same rule as
in . If that rule was one of (R0)–(R8), we have a derivation of  ⇒ using only
(R0)–(R8). If the rule was (R9a), then  is of the form {A} ∪′, where we have a
derivation of  ∪{∼A} ⇒′ using only (R0)–(R8). In that case, the inversion lemma
tells us we have a derivation of  ⇒, that is, of  ⇒{A} ∪′, using only (R0)–
(R8). Likewise if the rule was (R9b). So in any case, we have a derivation of  ⇒
using only (R0)–(R8), and our original supposition that we had a counterexample has
led to a contradiction, completing the proof.
14.17 Corollary. The proof procedure consisting of rules (R0)–(R8) is sound and
complete.
Proof: Soundness is immediate from the soundness theorem for (R0)–(R9), since
taking away rules cannot make a sound system unsound. Completeness follows from
completeness for (R0)–(R9) together with the preceding corollary.
Instead of dropping (R9), one might consider adding the following.
 ⇒{(A →B)} ∪
(R10)
 ⇒{A} ∪
 ⇒{B} ∪
.
14.18 Lemma (Cut elimination theorem). Using (R0)–(R9), if there are derivations of
 ⇒{(A →B)} ∪ and of  ⇒{A} ∪, then there is a derivation of  ⇒{B} ∪.
14.19 Corollary. Any sequent derivable using (R0)–(R10) is in fact derivable using
only (R0)–(R9).
14.20 Corollary. The proof procedure consisting of rules (R0)–(R10) is sound and
complete.

182
PROOFS AND COMPLETENESS
Proofs: We begin with Corollary14.20. It is easily seen that rule (R10) is sound,
so soundness for (R0)–(R10) follows from the soundness theorem for (R0)–(R9).
Completeness for (R0)–(R10) follows from the completeness theorem for (R0)–(R9),
since adding rules cannot make a complete system incomplete.
Now Corollary14.19 follows, since the same sequents are derivable in any two
sound and complete proof procedures: by Corollary14.17 a sequent will be derivable
using (R0)–(R10) if and only if it is secure, and by Theorems 14.1 and 14.2 it will be
secure if and only if it is derivable using (R0)–(R9).
AndnowLemma14.18followsalso,sinceiftherearederivationsof ⇒{(A →B)}
∪ and of  ⇒{A} ∪ using (R0)–(R9), then there is certainly a derivation of
 ⇒{B} ∪ using (R0)–(R10) [namely, the one consisting simply of concatenating
the two given derivations and adding a last line inferring  ⇒{B} ∪ by (R10)],
and by Corollary 14.19, this implies there must be a derivation of  ⇒{B} ∪ using
only (R0)–(R9).
Note the contrast between the immediately foregoing proof of the cut elimina-
tion lemma, Lemma 14.18, and the earlier proof of the inversion lemma, Lemma
14.15. The inversion proof is constructive: it actually contains implicit instructions
for converting a derivation of  ∪{∼A} ⇒ into a derivation of  ⇒{A} ∪.
The cut elimination proof we have given is nonconstructive: it gives no hint how
to ﬁnd a derivation of  ⇒{B} ∪ given derivations of  ⇒{A} ∪ and  ⇒
{(A →B)} ∪, though it promises us that such a derivation exists.
A constructive proof of the corollary is known, Gentzen’s proof, but it is very
much more complicated than the proof of the inversion lemma, and the result is that
while the derivation of  ⇒{A} ∪ obtained from the proof of the inversion lemma
is about the same length as the given derivation of  ∪{∼A} ⇒, the derivation
of  ⇒{B} ∪ obtained from the constructive proof of the foregoing corollary
may be astronomically longer than the given derivations of  ⇒{(A →B)} ∪ and
 ⇒{A} ∪ combined.
So much for dropping (R9) or adding (R10). A great deal more adding and dropping
of rules could be done. If enough new rules are added, some of our original rules
(R0)–(R8) could then be dropped, since the effect of them could be achieved using
the new rules. If we allowed & and ∀ofﬁcially, we would want rules for them, and the
addition of these rules might make it possible to drop some of the rules for ∨and ∃,
if indeed we did not choose to drop ∨and ∃altogether from our ofﬁcial language,
treating them as abbreviations. Similarly for →and ↔.
In all the possible variations mentioned in the preceding paragraph, we were
assuming that the basic objects would still be sequents  ⇒. But variation is
possible in this respect as well. It is possible, with the right selection of rules, to get
by working only with sequents of form  ⇒{D} (in which case one would simply
write  ⇒D), making deduction the central notion. It is even possible to get by work-
ing only with sequents of form  ⇒∅(in which case one would simply write ),
making refutation the central notion. Indeed, it is even possible to get by working
only with sequents of form ∅⇒{D} (which in one would simply write D), making
demonstration the central notion.

14.3. OTHER PROOF PROCEDURES AND HILBERT’S THESIS
183
Just by way of illustration, the rules for a variant approach in which ∼and →and
∀and = are the ofﬁcial logical operators, and in which one works only with sequents
of form  ⇒D, are listed in Table 14-5.
Table 14-5. Rules of a variant sequent calculus
(Q0)
 ⇒A
A in 
(Q1)
 ⇒A →B
 ⇒A
 ⇒B
(Q2)
, A ⇒B
 ⇒A →B
(Q3)
 ⇒∼∼A
 ⇒A
(Q4)
, A ⇒B
, A ⇒∼B
 ⇒∼A
(Q5)
 ⇒∀x A(x)
 ⇒A(t)
(Q6)
 ⇒A(c)
 ⇒∀x A(x)
c not in  or A(x)
(Q7)
 ⇒s = t
 ⇒A(s)
 ⇒A(t)
(Q8)
 ⇒t = t
This variation can be proved sound and complete in the sense that a sequent  ⇒D
will be obtainable by these rules if and only if D is a consequence of . We give one
sample deduction to give some idea how the rules work.
14.21 Example. A deduction.
(1)
∼A →∼B, B, ∼A ⇒∼A →∼B
(Q0), (i)
(2)
∼A →∼B, B, ∼A ⇒∼A
(Q0), (iii)
(3)
∼A →∼B, B, ∼A ⇒∼B
(Q1), (1), (2)
(4)
∼A →∼B, B, ∼A ⇒B
(Q0), (ii)
(5)
∼A →∼B, B ⇒∼∼A
(Q4), (3), (4)
(6)
∼A →∼B, B ⇒A
(Q3), (5)
(7)
∼A →∼B ⇒B →A
(Q2), (6)
The lowercase Roman numerals (i)–(iii) associated with (Q0) indicate whether it is the
ﬁrst, second, or third sentence in = {∼A →∼B, B, ∼A} that is playing the role of A in
the rule (Q0).

184
PROOFS AND COMPLETENESS
In addition to such substantive variations as we have been discussing, considerable
variations in style are possible, and in particular in typographical layout. For instance,
if one opens an introductory textbook, one may well encounter something like what
appears in Figure 14-1.
(i)
~A → ~B
(ii)
B
(iii)
~A
(1)
~A → ~B
(Q0), (i)
(2)
~A
(Q0), (iii)
(3)
~B
(Q1), (1), (2)
(4)
B
(Q0), (ii)
(5)
~~A
(Q4), (3), (4)
(6)
A
(Q3), (5)
(7)
B → A
(Q2), (6)
Figure 14-1. A ‘natural deduction’.
What appears in Figure 14-1 is really the same as what appears in Example 14.21,
differently displayed. The form of display adopted in this book, as illustrated in
Example 14.21, is designed for convenience when engaged in theoretical writing
about deductions. But when engaged in the practical writing of deductions, as in
introductory texts, the form of display in Figure 14-1 is more convenient, because it
involves less rewriting of the same formula over and over again. In lines (1)–(7) in
Figure 14-1, one only writes the sentence D on the right of the sequent  ⇒D that
occurs at the corresponding line in Example 14.21. Which of the sentences (i), (ii),
(iii) occur in the set  on the left of that sequent is indicated by the spatial position
where D is written: if it is written in the third column, all of (i)–(iii) appear; if in the
second, only (i) and (ii) appear; if in the ﬁrst, only (i). Colloquially one sometimes
speaks of deducing a conclusion D ‘under’ certain hypotheses , but in the form of
display illustrated in Figure 14-1, the spatial metaphor is taken quite literally.
It would take us too far aﬁeld to enter into a detailed description of the conventions
of this form of display, which in any case can be found expounded in many introduc-
tory texts. The pair of examples given should sufﬁce to make our only real point here:
that what is substantively the same kind of procedure can be set forth in very different
styles, and indeed appropriately so, given the different purposes of introductory texts
and of more theoretical books like this one. Despite the diversity of approaches pos-
sible, the aim of any approach is to set up a system of rules with the properties that if
D is deducible from , then D is a consequence of  (soundness), and that if D is a
consequence of , then D is formally deducible from  (completeness). Clearly, all
systems of rules that achieve these aims will be equivalent to each other in the sense
that D will be deducible from  in the one system if and only if D is deducible from
 in the other system. Except for one optional section at the end of the next chapter,
there will be no further mention of the details of our particular proof procedure in the
rest of this book.
A word may now be said about the relationship between any formal notion, whether
ours or a variant, of deduction of a sentence from a set of sentences, and the notion in

PROBLEMS
185
unformalized mathematics of a proof of a theorem from a set of axioms. For in later
chapters we are going to be establishing results about the scope and limits of formal
deducibilitywhoseinterestlargelydependsontheirhavingsomethingtodowithproof
in a more ordinary sense (just as results about the scope and limits of computability
in one or another formal sense discussed in other chapters depend for their interest
on their having something to do with computation in a more ordinary sense).
We have already mentioned towards the end of Chapter 10 that theorems and
axioms in ordinary mathematics can virtually always be expressed as sentences of
a formal ﬁrst-order language. Suppose they are so expressed. Then if there is a
deduction in the logician’s formal sense of the theorem from the axioms, there will
be a proof in the mathematician’s ordinary sense, because, as indicated earlier, each
formal rule of inference in the deﬁnition of deduction corresponds to some ordinary
mode of argument as used in mathematics and elsewhere. It is the converse assertion,
that if there is a proof in the ordinary sense, then there will be a deduction in our very
restrictive format, that may well seem more problematic. This converse assertion is
sometimes called Hilbert’s thesis.
As the notion of ‘proof in the ordinary sense’ is an intuitive, not a rigorously deﬁned
one, there cannot be a rigorous proof of Hilbert’s thesis. Before the completeness
theorem was discovered, a good deal of evidence of two kinds had already been
obtained for the thesis. On the one hand, logicians produced vast compendia of
formalizations of ordinary proofs. On the other hand, various independently proposed
systems of formal deducibility, each intended to capture formally the ordinary notion
of provability, had been proved equivalent to each other by directly showing how to
convert formal deductions in one format into formal deductions in another format;
and such equivalence of proposals originally advanced independently of each other,
while it does not amount to a rigorous proof that either has succeeded in capturing
the ordinary notion of provability, is surely important evidence in favor of both.
The completeness theorem, however, makes possible a much more decisive argu-
ment in favor of Hilbert’s thesis. The argument runs as follows. Suppose there is a
proof in the ordinary mathematical sense of some theorem from some axioms. As
part-time orthodox mathematicians ourselves, we presume ordinary mathematical
methods of proof are sound, and if so, then the existence of an ordinary mathematical
proof means that the theorem really is a consequence of the axioms. But if the theo-
rem is a consequence of the axioms, then the completeness theorem tells us that, in
agreement with Hilbert’s thesis, there will be a formal deduction of the theorem from
the axioms. And when in later chapters we show that there can be no formal deduction
in certain circumstances, it will follow that there can be no ordinary proof, either.
Problems
14.1 Show that:
(a)  secures  if and only if  ∪∼ is unsatisﬁable.
(b)  secures  if and only if some ﬁnite subset of  secures some ﬁnite
subset of .
14.2 Explain why the problems following this one become more or less trivial if
one is allowed to appeal to the soundness and completeness theorems.

186
PROOFS AND COMPLETENESS
Unless otherwise speciﬁed, ‘derivable’ is to mean ‘derivable using (R0)–
(R8)’. All proofs should be constructive, not appealing to the soundness and
completeness theorems.
14.3 Show that if , A, B ⇒ is derivable, then , A & B ⇒ is derivable.
14.4 Show that if  ⇒A,  and  ⇒B,  are derivable, then  ⇒A & B,  is
derivable.
14.5 Show that if  ⇒A(c),  is derivable, then  ⇒∀x A(x),  is derivable,
provided c does not appear in , , or A(x).
14.6 Show that if , A(t) ⇒ is derivable, then , ∀x A(x) ⇒ is derivable.
14.7 Show that ∀x Fx & ∀xGx is deducible from ∀x(Fx & Gx).
14.8 Show that ∀x (Fx & Gx) is deducible from ∀xFx & ∀xGx.
14.9 Show that the transitivity of identity, ∀x∀y∀z(x = y & y = z →x = z) is
demonstrable.
14.10 Show that if , A(s) ⇒ is derivable, then , s = t, A(t) ⇒ is derivable.
14.11 Prove the following (left) inversion lemma for disjunction: if there is a deriva-
tion of  ⇒{(A ∨B)} ∪ using rules (R0)–(R8), then there is such a deriva-
tion of  ⇒{A, B} ∪.
14.12 Prove the following (right) inversion lemma for disjunction: if there is a deriva-
tion of  ∪{(A ∨B)} ⇒, then there is a derivation of  ∪{A} ⇒, and
there is a derivation of  ∪{B} ⇒.
14.13 Consider adding one or the other of the following rules to (R0)–(R8):
(R11)
 ∪{A} ⇒
 ⇒{A} ∪
 ⇒
.
(R12)
 ∪{(A ∨∼A)} ⇒
 ⇒.
Show that a sequent is derivable on adding (R11) if and only if it is derivable
on adding (R12).

15
Arithmetization
In this chapter we begin to bring together our work on logic from the past few chapters
with our work on computability from earlier chapters (speciﬁcally, our work on recursive
functions from Chapters 6 and 7). In section 15.1 we show how we can ‘talk about’ such
syntactic notions as those of sentence and deduction in terms of recursive functions, and
draw among others the conclusion that, once code numbers are assigned to sentences in
a reasonable way, the set of valid sentences is semirecursive. Some proofs are deferred
to sections 15.2 and 15.3. The proofs consist entirely of showing that certain effectively
computable functions are recursive. Thus what is being done in the two sections men-
tioned is to present still more evidence, beyond that accumulated in earlier chapters, in
favor of Church’s thesis that all effectively computable functions are recursive. Readers
who feel they have seen enough evidence for Church’s thesis for the moment may regard
these sections as optional.
15.1 Arithmetization of Syntax
A necessary preliminary to applying our work on computability, which pertained to
functions on natural numbers, to logic, where the objects of study are expressions of a
formal language, is to code expressions by numbers. Such a coding of expressions is
called a G¨odel numbering. One can then go on to code ﬁnite sequences of expressions
and still more complicated objects.
A set of symbols, or expressions, or more complicated objects may be called
recursive in a transferred or derivative sense if and only if the set of code numbers
of elements of the set in question is recursive. Similarly for functions. Ofﬁcially, a
language is just a set of nonlogical symbols, so a language may be called recursive
if and only if the set of code numbers of symbols in the language is recursive. In
what follows we tacitly assume throughout that the languages we are dealing with
are recursive: in practice we are going to be almost exclusively concerned with ﬁnite
languages, which are trivially so.
There are many reasonable ways to code ﬁnite sequences, and it does not really
matter which one we choose. Almost all that matters is that, for any reasonable choice,
the following concatenation function will be recursive: s ∗t = the code number for
the sequence consisting of the sequence with code number s followed by the sequence
with code number t. This is all that is needed for the proof of the next proposition,
187

188
ARITHMETIZATION
in which, as elsewhere in this section, ‘recursive’ could actually be strengthened to
‘primitive recursive’.
So that the reader may have something deﬁnite in mind, let us offer one example
of a coding scheme. It begins by assigning code numbers to symbols as in Table 15-1.
Table 15-1. G¨odel numbers of symbols (ﬁrst scheme)
Symbol
(
∼
∃
=
v0
A0
0
A1
0
A2
0
. . .
f 0
0
f 1
0
f 2
0
. . .
)
∨
v1
A0
1
A1
1
A2
1
. . .
f 0
1
f 1
1
f 2
1
. . .
,
v2
A0
2
A1
2
A2
2
. . .
f 0
2
f 1
2
f 2
2
. . .
...
...
...
...
...
...
...
Code
1
2
3
4
5
6
68
688
. . .
7
78
788
. . .
19
29
59
69
689
6889
. . .
79
789
7889
. . .
199
599
699
6899
68899
. . .
799
7899
78899
. . .
...
...
...
...
...
...
...
Thus for the language of arithmetic < or A2
0 has code number 688, 0 or f 0
0 has
code number 7′, or f 1
0 has code number 78, + or f 2
0 has code number 788, and · or
f 2
1 has code number 7889. We then extend the code numbering to all ﬁnite sequences
of symbols. The principle is that if the expression E has code number e and the
expression D has code number d, then the expression ED obtained by concatenating
them is to have the code number whose decimal numeral is obtained by concatenating
the decimal numeral for e and the decimal numeral for d. Thus (0 = 0 ∨∼0 = 0), the
sequence of symbols with code numbers
1, 7, 4, 7, 29, 2, 7, 4, 7, 19
has code number 174 729 274 719.
In general the code number for the concatenation of the expressions with code
numbers e and d can be obtained from e and d as e ∗d = e · 10lg(d,10)+1 + d, where
lg is the logarithm function of Example 7.11. For lg(d, 10) + 1 will be the least
power z such that d < 10z, or in other words, the number of digits in the decimal
numeral for d, and thus the decimal numeral for e · 10lg(d,10)+1 will be that for e
followed by as many 0 s as there are digits in that for d, and the decimal numeral for
e · 10lg(d,10)+1 + d will be that for e followed by that for d.
15.1 Proposition. The logical operations of negation, disjunction, existential quantiﬁ-
cation, substitution of a term for free occurrences of a variable, and so on, are recursive.
Proof: Let n be the code number for the tilde, and let neg be the recursive func-
tion deﬁned by letting neg(x) = n ∗x. Then if x is the code number for a formula,
neg(x) will be the code number for its negation. (We do not care what the function
does with numbers that are not code numbers of formulas.) This is what is meant by
saying that the operation of negation is recursive. Similarly, if l and d and r are the
code numbers for the left parenthesis and wedge and right parenthesis, disj(x, y) =
l ∗x ∗d ∗y ∗r will be the code number for the disjunction of the formulas coded by

15.1. ARITHMETIZATION OF SYNTAX
189
x and y. If e is the code number for the backwards E, then exquant(v, x) = e ∗v ∗x
will be the code number for the existential quantiﬁcation with respect to the variable
with code number v of the formula with code number x. And similarly for as many
other logical operations as one cares to consider. For instance, if ofﬁcially the con-
junction (X & Y) is an abbreviation for ∼(∼X ∨∼Y), the conjunction function is then
the composition conj (x, y) = neg(disj(neg (x), neg(y))). The case of substitution is
more complicated, but as we have no immediate need for this operation, we defer the
proof.
Among sets of expressions, the most important for us will be simply the sets of
formulas and of sentences. Among more complicated objects, the only important
ones for us will be deductions, on whatever reasonable proof procedure one prefers,
whether ours from the preceding chapter, or some other from some introductory
textbook. Now intuitively, one can effectively decide whether or not a given sequence
of symbols is a formula, and if so, whether it is a sentence. Likewise, as we mentioned
when introducing our own proof procedure, one can effectively decide whether a
given object D is a deduction of a given sentence from a given ﬁnite set of sentences
0. If  is an inﬁnite set of sentences, then a deduction of D from  is simply a
deduction of D from some ﬁnite subset of 0, and therefore, so long as one can
effectively decide whether a given sentence C belongs to , and hence can effectively
decide whether a given ﬁnite set 0 is a subset of , one can also effectively decide
whether a given object is a deduction of D from . Church’s thesis then implies the
following.
15.2 Proposition. The sets of formulas and of sentences are recursive.
15.3 Proposition. If  is a recursive set of sentences, then the relation ‘ is a deduction
of sentence D from ’ is recursive.
Collectively, Propositions 15.1–15.3 (and their various attendant lemmas and
corollaries) are referred to by the imposing title at the head of this section.
Before concerning ourselves with the proofs of these propositions, let us note a
couple of implications.
15.4 Corollary. The set of sentences deducible from a given recursive set of sentences
is semirecursive.
Proof: What is meant is that the set of code numbers of sentences deducible from
a given recursive set is semirecursive. To prove this we apply Proposition 15.3. What
is meant by the statement of that proposition is that if  is recursive, then the relation
Rsd ↔d is the code number of a sentence and
s is the code number of a deduction of it from 
is recursive. And then the set S of code numbers of sentences deducible from , being
given by Sd ↔∃s Rsd, will be semirecursive.

190
ARITHMETIZATION
15.5 Corollary (G¨odel completeness theorem, abstract form). The set of valid sen-
tences is semirecursive.
Proof: By the G¨odel completeness theorem, the set of valid sentences is the same
as the set of demonstrable sentences, that is, as the set of sentences deducible from
 =∅. Since the empty set ∅is certainly recursive, it follows from the preceding
corollary that the set of valid sentences is semirecursive.
The preceding corollary states as much of the content of the G¨odel completeness
theorem as it is possible to state without mentioning any particular proof procedure.
The next corollary is more technical, but will be useful later.
15.6 Corollary. Let  be a recursive set of sentences in the language of arithmetic, and
D(x) a formula of that language. Then:
(a) The set of natural numbers n such that D(n) is deducible from  is semirecursive.
(b) The set of natural numbers n such that ∼D(n) is deducible from  is semirecursive.
(c) If for every n either D(n) or ∼D(n) is deducible from , then the set of n such that
D(n) is deducible from  is recursive.
Proof: For (a), we actually show that the set R of pairs (d, n) such that d is the
code number for a formula D(x) and D(n) is deducible from  is semirecursive. It
immediately follows that for any one ﬁxed D(x), with code number d, the set of n such
that D(n) is deducible from  will be semirecursive, since it will simply be the set of n
such that Rdn. To avoid the need to consider substituting a term for the free occurrences
of a variable (the one operation mentioned in Proposition 15.1 the proof of whose
recursiveness we deferred), ﬁrst note that for any n, D(n) and ∃x(x = n & D(x)) are
logically equivalent, and one will be a consequence of, or equivalently, deducible
from,  if and only if the other is. Now note that the function taking a number
n to the code number num(n) for the numeral n is (primitive) recursive, for recalling
that ofﬁcially s′ is ′(s) we have
num(0) = z
num(n′) = a ∗b ∗num(n) ∗c
where z is the code number for the cipher 0 and a, b, and c are the code numbers for
the accent and the left and right parentheses. The function f taking the code number
d for a formula D(x) and a number n to the code number for ∃x(x = n & D(x)) is
recursive in consequence of Proposition 15.1, since we have
f (d, n) = exquant(ν, conj(i ∗b ∗ν ∗k ∗num(n) ∗c), d)
where v is the code number for the variable, i for the equals sign, k for the comma.
The set S of code numbers of sentences that are deducible from  is semirecursive
by Corollary 15.4. The set R of pairs is then given by
R(d, n) ↔S( f (d, n)).
In other words, R is obtained from the semirecursive set S by substituting the recursive
total function f , which implies that R is itself semirecursive.

15.1. ARITHMETIZATION OF SYNTAX
191
As for (b), we actually show that the set Q of pairs (d, n) such that d is the code
number for a formula D(x) and ∼D(n) is deducible from  is semirecursive. Indeed,
with R as in part (a) we have
Q(d, n) ↔R(neg(d), n).
So Q is obtained from the semirecursive R by substitution of the recursive total
function neg, which implies that Q is itself semirecursive.
Obviously there is nothing special about negation as opposed to other logical
constructions here. For instance, in the language of arithmetic, we could consider the
operation taking D(x) not to ∼D(x) but to, say,
D(x) & ∼∃y < x D(y)
and since the relevant function on code numbers would still, like neg, be recursive in
consequence of Proposition 15.1, so the set of pairs (d, n) such that
D(n) & ∼∃y < nD(y)
is deducible from  is also semirecursive. We are not going to stop, however, to try
to ﬁnd the most general formulation of the corollary.
As for (c), if for any n both D(n) and ∼D(n) are deducible from , then every
formula is deducible from , and the set of n such that D(n) is deducible from 
is simply the set of all natural numbers, which is certainly recursive. Otherwise, on
the assumption that for every n either D(n) or ∼D(n) is deducible from , the set
of n for which D(n) is deducible and the set of n for which ∼D(n) is deducible are
complements of each other. Then (c) follows from (a) and (b) by Kleene’s theorem
(Proposition 7.16).
There is one more corollary worth setting down, but before stating it we introduce
some traditional terminology. We use ‘ proves D’, written  ⊢D or ⊢ D, inter-
changeably with ‘D is deducible from ’. The sentences proved by  we call the
theorems of . We conscript the word theory to mean a set of sentences that contains
all the sentences of its language that are provable from it. Thus the theorems of a
theory T are just the sentences in T , and ⊢T B and B ∈T are two ways of writing
the same thing.
Note that we do not require that any subset of a theory T be singled out as ‘axioms’.
If there is a recursive set  of sentences such that T consists of all and only the senten-
ces provable from , we say T is axiomatizable. If the set  is ﬁnite, we say T is
ﬁnitely axiomatizable. We have already deﬁned a set  of sentences to be complete
if for every sentence B of its language, either B or ∼B is a consequence of , or
equivalently, is provable from . Note that for a theory T , T is complete if and only
if for every sentence B of its language, either B or ∼B is in T . Similarly, a set  is
consistent if not every sentence is a consequence of , so a theory T is consistent if
not every sentence of its language is in T. A set  of sentences is decidable if the set
of sentences of its language that are consequences of , or equivalently, are proved by
, is recursive. Note that for a theory T , T is decidable if and only if T is recursive.
This terminology is used in stating our next result.

192
ARITHMETIZATION
15.7 Corollary. Let T be an axiomatizable theory. If T is complete, then T is decidable.
Proof: Throughout, ‘sentence’ will mean ‘sentence of the language of T ’. The
assumption that T is an axiomatizable theory means that T is the set of sentences
provable from some recursive set of sentences . We write T* for the set of code
numbers of theorems of T . By Corollary 15.4, T* is semirecursive. To show that T is
decidable we need to show that T* is in fact recursive. By Proposition 15.2, T* will
be so if it is simply the set of all code numbers of sentences, so let us consider the case
where this is not so, that is, where not every sentence is a theorem of T . Since every
sentence would be a theorem of T if for any sentence D it happened that both D and
∼D were theorems of T , for no sentence D can this happen. On the other hand, the
hypothesis that T is complete means that for every sentence D, at least one of D and
∼D is a theorem of T . It follows that D is not a theorem of T if and only if ∼D is a
theorem of T . Hence the complement of T* is the union of the set X of those numbers
n that are not code numbers of sentences at all, and the set Y of code numbers of
sentences whose negations are theorems of T , or in other words, the set of n such
that neg(n) is in T*. X is recursive by Proposition 15.2. Y is semirecursive, since it is
obtainable by substituting the recursive function neg in the semirecursive set T*. So
the complement of T* is semirecursive, as was T* itself. That T* is recursive follows
by Kleene’s theorem (Proposition 7.16).
It ‘only’ remains to prove Propositions 15.2 and 15.3. In proving them we are once
again going to be presenting evidence for Church’s thesis: we are one more time going
to be showing that certain sets and functions that must be recursive if Church’s thesis
is correct are indeed recursive. Many readers may well feel that by this point they have
seen enough evidence, and such readers may be prepared simply to take Church’s
thesis on trust in future. There is much to be said for such an attitude, especially since
giving the proofs of these propositions requires going into details about the G¨odel
numbering, the scheme of coding sequences, and the like, that we have so far largely
avoided; and it is very easy to get bogged down in such details and lose sight of
larger themes. (There is serious potential for a woods–trees problem, so to speak.)
Readers who share the attitude described are therefore welcome to postpone sine die
reading the proofs that ﬁll the rest of this chapter. Section 15.2 concerns (the deferred
clause of Proposition 15.1 as well as) Proposition 15.2, while section 15.3 concerns
Proposition 15.3.
15.2* G¨odel Numbers
We next want to indicate the proof of Proposition 15.2 (also indicating, less fully, the
proof of the one deferred clause of Proposition 15.1, on the operation of substituting
a term for the free occurrences of a variable in a formula). The G¨odel numbering
we gave by way of illustration near the beginning of this chapter is not, in fact, an
especially convenient one to work with here, mainly because it is not so easy to show
that such functions as the one that gives the the length (that is, number of symbols)
in the expression with a given code number are primitive recursive. An alternative
way of assigning code numbers to expressions begins by assigning code numbers to
symbols as in Table 15-2.

15.2. G¨ODEL NUMBERS
193
Table 15-2. G¨odel numbers of symbols (second scheme)
Symbol
(
)
,
∼
∨
∃
=
vi
An
i
f n
i
Code
1
3
5
7
9
11
13
2 · 5i
22 · 3n · 5i
23 · 3n · 5i
Thus for the language of arithmetic < or A2
0 has code number 22 · 32 · 50 = 4 · 9 =
36, 0 or f 0
0 has code number 23 · 30 · 50 = 8, ′ or f 1
0 has code number 23 · 31 · 50 =
8 · 3 = 24, + or f 2
0 has code number 23 · 32 · 50 = 8 · 9 = 72, and similarly · has
code number 360. We then extend the code numbering to all ﬁnite sequences of sym-
bols by assigning to an expression E consisting of a sequence of symbols S1S2 · · · Sn
the code number # (E) for the sequence (|S1|, |S2|, . . . , |Sn|) according to the scheme
for coding ﬁnite sequences of numbers by single numbers based on prime decompo-
sition. [In contrast to the earlier scheme, we need to distinguish, in the case of the
expression consisting of a single symbol S, the code number # (S) of S qua expression
from the code number |S| of S qua symbol. In general the code number for a single-
term sequence (n) is 2 · 3n, so we get #(S) = 2 · 3|S|.] Thus the code number for the
sentence we have been writing 0 = 0, which is ofﬁcially =(0, 0), is that for (13, 1, 36,
5, 36, 3), which is 26 · 313 · 5 · 736 · 115 · 1336 · 173. This is a number of 89 digits. For-
tunately, our concern will only be with what kinds of calculations could in principle be
performed with such large numbers, not with performing such calculations in practice.
The calculation of the length lh(e) of the expression with code number e is espe-
cially simple on this scheme, since lh(e) = lo(e, 2), where lo is the logarithm function
in Example 7.11, or in other words, the exponent on the prime 2 in the prime de-
composition of e. What are not so easy to express as primitive recursive functions
on this coding scheme are such functions as the one that gives the code number for
the concatenation of the expressions with two given code numbers. But while such
functions may not have been so easy to prove primitive recursive, they have been
proved to be so in Chapter 7. We know from our work there that in addition to the
concatenation function ∗, several further cryptographic or code-related functions are
primitive recursive. Writing #(σ) for the code number of sequence σ, and §(s) for the
sequence with code number s, we list these functions in Table 15-3.
Table 15-3. Cryptographic functions
lh (s)
= the length of §(s)
ent(s, i)
= the ith entry of §(s)
last(s)
= the last entry of §(s)
ext(s, a)
= #(§(s) with a added at the end)
pre(a, s)
= #(§(s) with a added at the beginning)
sub(s, c, d)
= #(§(s) with c replaced by d throughout)
More complicated objects, such as ﬁnite sequences or ﬁnite sets of expressions, can
also be assigned code numbers. A code number for a ﬁnite sequence of expressions
is simply a code number for a ﬁnite sequence of natural numbers, whose entries are
themselves in turn code numbers for expressions. As a code number for a ﬁnite set of
expressions, we may take the code number for the ﬁnite sequence of expressions that
list the elements of the set (without repetitions) in order of increasing code number.

194
ARITHMETIZATION
This means that a code number of a ﬁnite set of expressions will be a code number for
a ﬁnite sequence of expressions whose entries are increasing, with later entries larger
than earlier ones. A virtue of this coding is that such relations as ‘the expression with
code number i belongs to the set with code number s’ and ‘the set with code number
t is a subset of the set with code number s’ will all be simply deﬁnable in terms of
the cryptographic functions, and hence recursive. (The ﬁrst amounts to ‘i is an entry
of the sequence coded by s’, and the second amounts to ‘every entry of the sequence
coded by t is an entry of the sequence coded by s’.) Similarly the coding can be
extended to ﬁnite sequences or ﬁnite sets of ﬁnite sequences or ﬁnite sets, and so on.
Towards proving Proposition 15.2, the ﬁrst thing to note is that one- and two-place
relations like those given by ‘a is the code number of a predicate’ and ‘a is the code
number of an n-place predicate’ are primitive recursive. For the former is equivalent
to the existence of n and i such that a = 22 · 3n · 5i, and the latter is equivalent to the
existence of i such that a = 22 · 3n · 5i. The function f given by f (n, i) = 22 · 3n · 5i
is primitive recursive, being a composition of exponentiation, multiplication, and
the constant functions with values 22, 3, and 5. So the relation ‘a = 22 · 3n · 5i’
is primitive recursive, being the graph relation ‘a = f (n, i)’. The two relations of
interest are obtained from the relation ‘a = 22 · 3n · 5i’ by existential quantiﬁcation,
and in each case the quantiﬁers can be taken to be bounded, since if a = 22 · 3n · 5i,
then certainly n and i are less than a. So the ﬁrst condition amounts to ∃n < a
∃i < a(a = 22 · 3n · 5i) and the second to ∃i < a(a = 22 · 3n · 5i).
Similar remarks apply to ‘a codes a variable’, ‘a codes a function symbol’, and ‘a
codes a constant (that is, a zero-place function symbol)’, ‘a codes an n-place function
symbol’, and ‘a codes an atomic term (that is, a variable or constant)’. These all give
primitive recursive relations. If we are interested only in formulas and sentences of
some language L less than the full language containing all nonlogical symbols, we
must add clauses ‘and a is in L’ to our various deﬁnitions of the items just listed.
So long as L is still primitive recursive, and in particular if L is ﬁnite, the relations
just listed will still be primitive recursive. (If L is only recursive and not primitive
recursive, we have to change ‘primitive recursive’ to ‘recursive’ both here and below.)
Considering only the case without identity and function symbols, the relation given
by ‘s codes an atomic formula’ is also primitive recursive, being obtainable by simple
operations (namely, substitution, conjunction, and bounded universal quantiﬁcations)
from the relations mentioned in the preceding paragraph and the graph relations of
the primitive recursive functions of some of the cryptographic functions listed earlier.
Speciﬁcally, s codes an atomic formula if and only if there is an n less than lh(s) such
that the following holds:
lh (s) = 2n + 2, and
ent (s, 0) is the code number for an n-place predicate, and
ent (s, 1) = 1 (the code number for a left parenthesis), and
for every i with 1 < i < lh (s) −1:
if i is odd then ent(s, i) = 5 (the code number for a comma), and
if i is even then ent(s, i) is the code number for an atomic term, and
last (s) = 3 (the code number for a right parenthesis).

15.2. G¨ODEL NUMBERS
195
Now s is the code number of a formula S if and only if there is some r that is the
code number for a formation sequence for S. In general, the relation given by ‘r is the
code number of a formation sequence for a formula with code number s’ is primitive
recursive, since this relation holds if and only if the following does:
For all j < lh(r) either:
ent (r, j) is the code number for an atomic sentence, or
for some k < j,
ent (r, j) = neg(ent (r, k)), or
for some k1 < jand some k2 < j,
ent (r, j) = disj(ent (r, k1), ent (r, k2)), or
for some k < j and some i < ent (r, j),
ent (r, j) = exquant (2 · 5i, ent (r, k))
and last (r) = s.
Here neg, disj, and exquant are as in the proof of Proposition 15.1.
We can give a rough upper bound on the code number for a formation sequence,
since we know (from the problems at the end of Chapter 9) that if S is a formula—
that is, if S has any formation sequence at all—then S has a formation sequence in
which every line is a substring of S, and the number of lines is less than the length
of S. Thus, if there is any formation sequence at all for s, letting n = lh(s), there will
be a formation sequence for s of length no greater than n with each entry of size
no greater than s. The code number for such a formation sequence will therefore
be less than the code number for a sequence of length n all of whose entries are
s, which would be 2n · 3s· · · · ·π(n)s, where π(n) is the nth prime, and this is less
that π(n)s(n+1). So there is a primitive recursive function g, namely the one given by
g(x) = π(lh(x))x[lh(x)+1], such that if s is the code number for a formula at all, then
there will be an r < g(s) such that r is a code number for a formation sequence for that
formula. In other words, the relation given by ‘s is the code number for a formula’
is obtainable by bounded quantiﬁcation from a relation we showed in the preceding
paragraph to be primitive recursive: ∃r < g(s) (r codes a formation sequence for s).
Thus the relation ‘s is the code number for a formula’ is itself primitive recursive.
In order to deﬁne sentencehood, we need to be able to check which occurrences of
variables in a formula are bound and which free. This is also what is needed in order
to deﬁne the one operation in Lemma 15.1 whose proof we deferred, substitution of a
term for the free occurrences of a variable in a formula. It is not the substitution itself
that is the problem here, so much as recognizing which occurrences of the variable
are to be substituted for and which not. The relation ‘s codes a formula and the eth
symbol therein is a free occurrence of the dth variable’ holds if and only if
s codes a formula and ent (s, e) = 2 · 5d and
for no t, u, v, w < s is it the case that
s = t ∗v ∗w and lh (t) < e and e < lh(t) + lh(v) and
u codes a formula and v = exquant (2 · 5d, u).

196
ARITHMETIZATION
For the ﬁrst clause says that s codes a formula and the eth symbol therein is the dth
variable, while the second clause says that the eth symbol does not fall within any
subsequence v of the formula that is itself a formula beginning with a quantiﬁcation
of the dth variable. This relation is primitive recursive. Since the relation ‘s codes a
sentence’ is then simply
s codes a formula and
for no d, e < s is the eth symbol therein a free occurrence of the dth variable
it is primitive recursive, too, as asserted.
So much for the proof in the case where identity and function symbols are absent.
If identity is present, but not function symbols, the deﬁnition of atomic formula will
be the disjunction of the clause above covering atomic formulas involving a nonlog-
ical predicate with a second, similar but simpler, clause covering atomic formulas
involving the logical predicate of identity. If function symbols are present, it will be
necessary to give a preliminary deﬁnitions of term formation sequence and term. The
deﬁnition for term formation sequence will have much the same gross form as the
deﬁnition above of formation sequence; the deﬁnition for term will be obtained from
it by a bounded existential quantiﬁcation. We suppress the details.
15.3* More G¨odel Numbers
We indicate the proof of Proposition 15.3, for the proof procedure used in the preced-
ing chapter, only in gross outline. Something similar can be done for any reasonable
proof procedure, though the details will be different.
We have already indicated how sets of sentences are to be coded: s is a code for a
set of sentences if and only if s is a code for a sequence and for all i < lh(s), ent(s, i)
is a code for a sentence, and in addition for all j < i, ent (s, j) < ent(s, i). It follows
that the set of such codes is primitive recursive. A derivation, on the approach we took
in the last chapter, is a sequence of sequents 1 ⇒1, 2 ⇒2, and so on, subject
to certain conditions. Leaving aside the conditions for the moment, a sequence of
sequents is most conveniently coded by a code for (c1, d1, c2, d2, . . .), where ci codes
i and di codes i. The set of such codes is again primitive recursive. The sequence
of sequents coded by the code for (c1, d1, . . . , cn, dn) will be a deduction of sentence
D from set  if and only if: ﬁrst, the sequence of sequents coded is a derivation; and
second, cn codes a sequences whose entries are all codes for sentences in , and dn
codes the sequence of length 1 whose sole entry is the code for D. Assuming  is
recursive, the second condition here deﬁnes a recursive relation.
The ﬁrst condition deﬁnes a primitive recursive set, and the whole matter boils
down to proving as much. Now the sequence of sequents coded by a code for
(c1, d1, . . . , cn, dn) will be derivation if for each i ≤n, the presence of ci and di
is justiﬁed by the presence of zero, one, or more earlier pairs, such that the sequent
i ⇒i coded by ci and di follows from the sequents  j ⇒ j coded by these
earlier c jand d j according to one or another rule. In gross form, then, the deﬁnition
of coding a derivation will resemble the deﬁnition of coding a formation sequence,

PROBLEMS
197
where the presence of any code for an expression must be justiﬁed by the presence
of zero, one, or more earlier codes for expressions from which the given expression
‘follows’ by or another ‘rule’ of formation. The rules of formation are just the rules
the zero-‘premiss’ rule allowing atomic formulas to appear, the one-‘premiss’ rule
allowing a negation to be ‘inferred’ from the expression it negates, the two-‘premiss’
rule allowing a disjunction to be ‘inferred’ from the two expressions it disjoins—and
so on. Deﬁnitions of this gross form deﬁne primitive recursive relations, provided the
individual rules in them do.
So, going back to derivations, let us look at a typical one-premiss rule. (The zero-
premiss rule would be a bit simpler, a two-premiss rule a bit more complicated.)
Take
(R2a)
 ∪{A} ⇒
 ⇒{∼A} ∪.
The relation we need to show to be primitive recursive is the relation ‘e and f code
a sequent that follows from the sequent coded by c and d according to (R2a)’. But
this can be deﬁned as follows:
c, d, e, f code sets of formulas, and ∃a < lh(c)
∃b < lh( f )
ent( f, b) = neg(ent(c, a)), and
∀i < lh(c) (i = a or ∃j < lh(e) ent(c, i) = ent(e, j)), and
∀i < lh(e)
∃j < lh(c) ent(e, i) = ent(c, j), and
∀i < lh(d)
∃j < lh( f ) ent(d, i) = ent( f, j), and
∀i < lh( f )(i = b or ∃j < lh(d) ent( f, i) = ent(d, j)).
Here the last four clauses just say that the only difference between the sets coded by
c and e is the presence of the sentence A coded by ent(c, a) in the former, and the
only difference between the sets coded by d and f is the presence of the sentence
B coded by ent( f, b) in the latter. The second clause tells us that B = ∼A. This is a
primitive recursive relation, since we known neg is a primitive recursive function.
To supply a full proof, each of the rules would have to be analyzed in this way. In
general, the analyses would be very similar, the main difference being in the second
clauses, stating how the ‘exiting’ and ‘entering’ sentences are related. In the case we
just looked at, the relationship was very simple: one sentence was the negation of the
other. In the case of some other rules, we would need to know that the function taking
a formula B(x) and a closed term t to the result B(t) of substituting t for all the free
occurrences of x is recursive, or rather, that the corresponding function on codes is.
We suppress the details.
Problems
15.1 On the ﬁrst scheme of coding considered in this chapter, show that the length
of, or number of symbols in, the expression with code number e is obtainable
by a primitive recursive function from e.

198
ARITHMETIZATION
15.2 Let  be a set of sentences, and T the set of sentences in the language of 
that are deducible from . Show that T is a theory.
15.3 Suppose an axiomatizable theory T has only inﬁnite models. If T has only one
isomorphism type of denumerable models, we know that it will be complete
by Corollary 12.17, and decidable by Corollary 15.7. But suppose T is not
complete, though it has only two isomorphism types of denumerable models.
Show that T is still decidable.
15.4 Give examples of theories that are decidable though not complete.
15.5 Suppose A1, A2, A3, . . . are sentences such that no An is provable from the
conjunctionofthe Am form < n.Let T bethetheoryconsistingofallsentences
provable from the Ai. Show that T is not ﬁnitely axiomatizable, or in other
words, that there are not some other, ﬁnitely many, sentences B1, B2, . . . , Bm
such that T is the set of consequences of the B j.
15.6 For a language with, say, just two nonlogical symbols, both two-place relation
symbols, consider interpretations where the domain consists of the positive
integers from 1 to n. How many such interpretations are there?
15.7 A sentence D is ﬁnitely valid if every ﬁnite interpretation is a model of D.
Outline an argument assuming Church’s thesis for the conclusion that the
set of sentences that are not ﬁnitely valid is semirecursive. (It follows from
Trakhtenbrot’s theorem, as in the problems at the end of chapter 11, that the
set of such sentences is not recursive.)
15.8 Show that the function taking a pair consisting of a code number a of a
sentence A and a natural number n to the code number for the conjunction
A & A & · · · & A of n copies of A is recursive.
15.9 The Craig reaxiomatization lemma states that any theory T whose set of the-
orems is semirecursive is axiomatizable. Prove this result.
15.10 Let T be an axiomatizable theory in the language of arithmetic. Let f be a
one-place total or partial function f of natural numbers, and suppose there is
a formula φ(x, y) such that for any a and b, φ(a, b) is a theorem of T if and
only if f (a) = b. Show that f is a recursive total or partial function.

16
Representability of Recursive Functions
In the preceding chapter we connected our work on recursion with our work on formulas
and proofs in one way, by showing that various functions associated with formulas and
proofs are recursive. In this chapter we connect the two topics in the opposite way, by
showing how we can ‘talk about’ recursive functions using formulas, and prove things
about them in theories formulated in the language of arithmetic. In section 16.1 we show
that for any recursive function f, we can ﬁnd a formula φ f such that for any natural
numbers a and b, if f (a) = b then ∀y(φ f (a, y) ↔y = b) will be true in the standard
interpretation of the language of arithmetic. In section 16.2 we strengthen this result,
by introducing a theory Q of minimal arithmetic, and showing that for any recursive
function f, we can ﬁnd a formula ψ f such that for any natural numbers a and b, if f (a) = b
then ∀y(ψ f (a, y) ↔y = b) will be not merely true, but provable in Q. In section 16.3
we brieﬂy introduce a stronger theory P of Peano arithmetic, which includes axioms
of mathematical induction, and explain how these axioms enable us to prove results not
obtainable in Q. The brief, optional section 16.4 is an appendix for readers interested in
comparing our treatment of these matters here with other treatments in the literature.
16.1 Arithmetical Deﬁnability
In Chapter 9, we introduced the language L* of arithmetic and its standard interpre-
tation N*. We now abbreviate ‘true in the standard interpretation’ to correct. Our
goal in this chapter is to show that we can ‘talk about’ recursive functions in the
language of arithmetic, and we begin by making talk about ‘talking about’ precise.
We say a formula F(x) of the language of arithmetic arithmetically deﬁnes a set
S of natural numbers if and only if for all natural numbers a we have Sa if and only if
F(a) is correct. We say the set S is arithmetically deﬁnable, or arithmetical for short,
if some formula arithmetically deﬁnes it. These notions naturally extend to two-place
or many-place relations. A formula F(x, y) arithmetically deﬁnes a relation R on
natural numbers if and only if for all natural numbers a and b we have Rab if and
only if F(a, b) is correct. The notions also naturally extend to functions, a function
being counted as arithmetical if and only if its graph relation is arithmetical. Thus
a one-place function f is arithmetical if and only if there is a formula F(x, y) of
the language of arithmetic such that for all a and b we have f (a) = b if and only if
F(a, b) is correct.
199

200
REPRESENTABILITY OF RECURSIVE FUNCTIONS
16.1 Examples (Basic functions). To give the most trivial example, the identity function
id = id1
1is arithmetically deﬁned by the formula y = x, and more generally, idn
i is arithmeti-
cally deﬁned by the formula y = xi, or if we want the other x j to be mentioned, by the
formula
x1 = x1 & . . . & xn = xn & y = xi.
The zero function const0(x) = 0 is also arithmetically deﬁnable, by the formula y = 0, or
if we want x to be mentioned, by the formula x = x & y = 0. The successor, addition, and
multiplication functions are arithmetically deﬁnable by the formulas y = x′, y = x1 + x2,
and y = x1 · x2.
16.2 Examples (Other arithmetical functions). Of course, it is no surprise that the functions
we have just been considering are arithmetically deﬁnable, since they are ‘built in’: we have
included in the language special symbols expressly for them. But their inverses, for which we
have not built in symbols, are also arithmetical. The predecessor function is arithmetically
deﬁnable by the following formula Fpred(x1, y):
(x1 = 0 & y = 0) ∨x1 = y′.
The difference function x1 .−x2 is arithmetically deﬁned by the following formula
Fdif(x1, x2, y):
(x1 < x2 & y = 0) ∨(x1 = x2 + y)
and the quotient and remainder functions quo(x1, x2) and rem(x1, x2) are arithmetically
deﬁned by the following formulas Fquo(x1, x2, y) and Frem(x1, x2, y):
(x2 = 0 & y = 0) ∨∃u < x2x1 = y · x2 + u
(x2 = 0 & y = x1) ∨(y < x2 & ∃u ≤≤x1 x1 = u · x2 + y).
On the other hand, it is not obvious how to deﬁne exponentiation, and as a tem-
porary expedient we now expand the language of arithmetic by adding a symbol
↑, thus obtaining the language of exponential arithmetic. Its standard interpretation
is like that of the original language arithmetic, with the denotation of ↑↑↑being the
usual exponentiation function. In terms of this expansion we deﬁne ↑-arithmetical
deﬁnability in the obvious way. (The expression ‘↑-arithmetical’ may be pronounced
‘exponential-arithmetical’ or ‘exp-arithmetical’ for short.)
16.3 Examples (↑-arithmetical functions). Examples of ↑-arithmetical functions include
the exponential function itself, its inverses the logarithm functions (lo and lg of Example
7.11), and, what will be more signiﬁcant for our present purposes, any number of functions
pertaining to the coding of ﬁnite sequences of numbers by single numbers or pairs of
numbers. For instance, in section 1.2 we found one serviceable if not especially elegant
way of coding sequences by pairs for which the ith entry of the sequence coded by the pair
(s, t) could be recovered using the function
entry(i, s, t) = rem(quo(s, ti), t)

16.1. ARITHMETICAL DEFINABILITY
201
This function is ↑-arithmetically deﬁnable by the following formula Fent(x1, x2, x3, y):
∃z ≤≤x3 ↑↑↑x1(Fquo (x2, x3 ↑↑↑x1, z) & Frem (z, x2, y)).
For this just says that there is something that is the quotient on dividing x2 by x x1
3 , and
whose remainder on dividing by x2 is y, adding that it will be less than or equal to x2
(as any quotient on dividing x2 by anything must be).
Even after helping ourselves to exponentiation, it is still not obvious how to deﬁne
super-exponentiation, but though not obvious, it is possible—in fact any recursive
function can now be deﬁned, as we next show.
16.4 Lemma. Every recursive function f is ↑-arithmetical.
Proof: Since we have already shown the basic functions to be deﬁnable, we need
only show that if any of the three processes of composition, primitive recursion, or
minimization is applied to ↑-arithmetical functions, the result is an ↑-arithmetical
function. We begin with composition, the idea for which was already encountered in
the last example. Suppose that f and g are one-place functions and that h is obtained
from them by composition. Then clearly c = h(a) if and only if
c = g( f (a))
which may be more long-windedly put as
there is something such that it is f (a) and g(it) is c.
It follows that if f and g are ↑-arithmetically deﬁned by φ f and φg, then h is
↑-arithmetically deﬁned by the following formula φh(x, z):
∃y (φ f (x, y) & φg(y, z)).
[To be a little more formal about it, given any a, let b = f (a) and let c = h(a) =
g( f (a)) = g(b). Since φ f and φg deﬁne f and g, φ f (a, b) and φg(b, c) are correct, so
φ f (a, b) & φg(b, c) is correct, so ∃y(φ f (a, y) & φg(y, c)) is correct, which is to say
φh(a, c) is correct. Conversely, if φh(a, c) is correct, φ f (a, b) & φg(b, c) and hence
φ f (a, b) and φg(b, c) must be correct for some b, and since φ f deﬁnes f , this b can
only be f (a), while since φg deﬁnes g, c then can only be g(b) = g( f (a)) = h(a).]
For the composition of a two-place function f with a one-place function g the
formula would be
∃y(φ f (x1, x2, y) & φg(y, z)).
For the composition of two one-place functions f1 and f2 with a two-place function
g, the formula would be
∃y1∃y2(φ f1(x, y1) & φ f2(x, y2) & φg(y1, y2, z))
and so on. The construction is similar for functions of more places.
Recursion is just a little more complicated. Suppose that f and g are one-place
and three-place functions, respectively, and that the two-place function h is obtained

202
REPRESENTABILITY OF RECURSIVE FUNCTIONS
from them by primitive recursion. Writing i′ for the successor of i, clearly c = h(a, b)
if and only if there exists a sequence σ with the following three properties:
entry 0 of σ is h(a, 0)
for all i < b, if entry i of σ is h(a, i), then entry i′ of σ is h(a, i′)
entry b of σ is c.
These conditions may be restated equivalently thus:
entry 0 of σ is f (a)
for all i < b, entry i′ of σ is g(a, i, entry i of σ)
entry b of σ is c.
These conditions may be restated more long-windedly thus:
there is something that is entry 0 of σ and is f (a)
for all i < b, there is something that is entry i of σ, and
there is something which is entry i′ of σ, and
the latter is g(a, i, the former)
entry b of σ is c.
Moreover, instead of saying ‘there is a sequence’ we may say ‘there are two numbers
coding a sequence’. It follows that if f and g are ↑-arithmetically deﬁned by φ f and
φg, then h is ↑-arithmetically deﬁned by the formula φh(x, y, z) = ∃s∃tφ, where φ is
the conjunction of the following three formulas:
∃u(Fent(0, s, t, u) & φ f (x, u))
∀w < y ∃u∃v(Fent(w, s, t, u) & Fent(w′, s, t, v) & φg(x, w, u, v))
Fent(y, s, t, z).
The construction is exactly the same for functions of more places.
Minimization is a little simpler. Suppose that f is a two-place function, and that
the one-place function g is obtained from it by minimization. Clearly g(a) = b if and
only if
f (a, b) = 0 and
for all c < b, f (a, c) is deﬁned and is not 0.
These conditions may be restated more long-windedly thus:
f (a, b) = 0 and
for all c < b, there is something that is f (a, c), and it is not 0.
It follows that if f is ↑-arithmetically deﬁned by φ f , then g is ↑-arithmetically deﬁned
by the following formula φg(x, y):
φ f (x, y, 0) & ∀z < y ∃u(φ f (x, z, u) & u ̸= 0).
The construction is exactly the same for functions of more places.
On reviewing the above construction, it will be seen that the presence of the
exponential symbol ↑↑↑in the language was required only for the formula Fent. If we

16.1. ARITHMETICAL DEFINABILITY
203
could ﬁnd some other way to code sequences by pairs for which the entry function
could be deﬁned without using exponentiation, then we could forget about ↑↑↑. And in
fact, a coding is possible for which
entry(i, s, t) = rem(s, t(i + 1) + 1)
so that for Fent we may take
Frem(x2, x3 · (x1 + 1)+ 1, y).
That such a coding is possible is the content of the following lemma.
16.5 Lemma (β-function lemma). For every k and every a0, a1, . . . , ak there exist
s and t such that for all i with 0 ≤i ≤k we have ai = rem(s, t(i + 1) + 1).
Proof: This result follows directly from the proofs of two ancient and famous
theorems of number theory, to be found in a prominent place in any textbook on that
subject. Since this is not a textbook on number theory, we are not going to develop the
whole subject from the foundations, but we do give an indication of the proof. The ﬁrst
ingredient is the Chinese remainder theorem, so called from the appearance (at least
of special cases) of the theorem in the ancient Mathematical Classic of Sun Zi and
the medieval Mathematical Treatise in Nine Sections of Qin Jiushao. This theorem
states that given any numbers t0, t1, . . . , tn no two of which have a common prime
factor, and given any numbers ai < ti, there is a number s such that rem(s, ti) = ai
for all i from 0 to n. The proof is sufﬁciently illustrated by the case of two numbers
t and u with no common prime factor, and two numbers a < t and b < u. Every one
of the tu numbers i with 0 ≤i < tu produces one of the tu pairs (a, b) with a < t and
b < u on taking the remainders rem (s, t) and rem (s, u). To show that, as asserted by
the theorem, every pair (a, b) is produced by some number s, it sufﬁces to show that
no two distinct numbers 0 ≤s <r < tu produce the same pair. If s and r do produce
the same pair, then they leave the same remainder when divided by t, and leave the
same remainder when divided by u. In that case, their difference q = r −s leaves
remainder zero when divided by either t or u. In other words, t and u both divide q.
But when numbers with no common prime factor both divide a number, so does their
product. Hence tu divides q. But this is impossible, since 0 < q < tu.
The second ingredient comes from the proof in Euclid’s Elements of Geometry that
there exist inﬁnitely many primes. Given any number n, we want to ﬁnd a prime p > n.
Well, let N = n!, so that in particular N is divisible by every prime ≤n. Then N + 1,
like any number >1, has a prime factor p. (Possibly N is itself prime, in which case
we have p = N.) But we cannot have p ≤n, since when N is divided by any number
≤n, there is a remainder of 1. A slight extension of the argument shows that any two
distinct numbers N · i + 1 and N · j + 1 with 0 < i < j ≤n have no common prime
factor. For if a prime p divides both numbers, it divides their difference N( j −i).
This is a product of factors ≤n, and when a prime divides a product of several factors,
it must divide one of the factors; so p itself must be a number ≤n. But then p cannot
divide N · i + 1 or N · j + 1. Now given k and every a0, a1,. . . , ak, taking n larger
than all of them, and letting t be a number divisible by every prime ≤n, no two of the

204
REPRESENTABILITY OF RECURSIVE FUNCTIONS
numbers ti = t(i + 1) + 1 will have a common prime factor, and we will of course
have ai < ti, so there will be an s such that rem(s, ti)= ai for all i with 0 ≤i ≤k.
Thus we have proved part (a) of the following.
16.6 Lemma
(a) Every recursive function f is arithmetical.
(b) Every recursive set is arithmetical.
Proof: As remarked just before the statement of the lemma, we already have (a).
For (b), if R is an n-place recursive relation and f its characteristic function, then
apply (a) to get a formula φ(x1, . . . , xn, y) arithmetically deﬁning f . Then the formula
φ(x1, . . . , xn, 1) arithmetically deﬁnes R.
Furthersharpeningoftheresultdependsondistinguishingdifferentkindsofformu-
las. By a rudimentary formula of the language of arithmetic we mean a formula built
up from atomic formulas using only negation, conjunction, disjunction, and bounded
quantiﬁcations ∀x < t and ∃x < t, where t may be any term of the language (not in-
volving x). (Conditionals and biconditionals are allowed, too, since these ofﬁcially
are just abbreviations for certain constructions involving negation, conjunction, and
disjunction. So are the bounded quantiﬁers ∀x ≤≤t and ∃x ≤≤t, since these are equi-
valent to ∀x < t′ and ∃x < t′). By an ∃-rudimentary formula we mean a formula of
form ∃xF where F is rudimentary, and similarly for an ∀-rudimentary formula. (The
negation of an ∃-rudimentary formula is equivalent to an ∀-rudimentary formula, and
conversely.) Many major theorems of number theory are naturally expressible by
∀-rudimentary formulas.
16.7 Examples (Theorems of number theory). Lagrange’s theorem that every natural num-
ber is the sum of four squares is naturally expressible by an ∀-rudimentary sentence as
follows:
∀x ∃y1 < x ∃y2 < x ∃y3 < x ∃y4 < x x = y1 · y1 + y2 · y2 + y3 · y3 + y4 · y4.
Bertrand’s postulate, or Chebyshev’s theorem, that there is a prime between any number
greater than one and its double, is naturally expressible by an ∀-rudimentary sentence as
follows:
∀x( 1 < x →∃y < 2 · x(x < y & ∼∃u < y ∃v < y y = u · v)).
Our present concern, however, will be with ∃-rudimentary formulas and with
generalized ∃-rudimentary formulas, which include all formulas obtainable from
rudimentary formulas by conjunction, disjunction, bounded universal quantiﬁcation,
bounded existential quantiﬁcation, and unbounded existential quantiﬁcation. Review-
ing the proof of Lemma 16.6, one ﬁnds that the formulas deﬁning the basic functions
and the formula Fent are rudimentary, and that the formula deﬁning a composition of
functions is obtained by conjunction, bounded quantiﬁcation, and existential quan-
tiﬁcation from rudimentary formulas and the formulas deﬁning the original functions,
and similarly for recursion and minimization. Hence we have proved:

16.1. ARITHMETICAL DEFINABILITY
205
16.8 Lemma. Every recursive function is arithmetically deﬁnable by a generalized
∃-rudimentary formula.
The next reﬁnement will be to get rid of the word ‘generalized’ here. Two formulas
with, say, two free variables, φ(x, y) and ψ(x, y), are called arithmetically equivalent
if for all numbers a and b, φ(a, b) is correct if and only if ψ(a, b) is correct. Clearly
arithmetically equivalent formulas deﬁne the same relation or function. The condition
that φ and ψ are arithmetically equivalent is equivalent to the condition that the
biconditional
∀x∀y(φ(x, y) ↔ψ(x, y))
is correct. In particular, if φ and ψ are logically equivalent—in which case the bicon-
ditional is true not just in the standard interpretation, but in any interpretation—then
they are arithmetically equivalent. The following lemma bears more than a passing
resemblance to Corollary 7.15.
16.9 Lemma (Closure properties of ∃-rudimentary formulas).
(a) Any rudimentary formula is arithmetically equivalent to an ∃-rudimentary formula.
(b) The conjunction of two ∃-rudimentary formulas is arithmetically equivalent to an
∃-rudimentary formula.
(c) The disjunction of two ∃-rudimentary formulas is arithmetically equivalent to an
∃-rudimentary formula.
(d) The result of applying bounded universal quantiﬁcation to an ∃-rudimentary
formula is arithmetically equivalent to an ∃-rudimentary formula.
(e) The result of applying bounded existential quantiﬁcation to an ∃-rudimentary
formula is arithmetically equivalent to an ∃-rudimentary formula.
(f) The result of applying (unbounded) existential quantiﬁcation to an ∃-rudimentary
formula is arithmetically equivalent to an ∃-rudimentary formula.
Proof: For (a), φ is logically equivalent to ∃w(w = w & φ) (and if φ is rudimentary,
so is w = w & φ).
For (b), ∃uφ(u) & ∃vψ(v) is arithmetically equivalent to
∃w ∃u < w ∃v < w (φ(u) & ψ(v)).
[and if φ(u) and ψ(v) are rudimentary, so is ∃u < w ∃v < w (φ(u) & ψ(v))]. The
implication in one direction is logical, and in the other direction we use the fact that
for any two natural numbers u and v, there is always a natural number w greater than
both.
For (c), ∃uφ(u) ∨∃vψ(v) is logically equivalent to ∃w(φ(w) ∨ψ(w)).
For (d), ∀z < y ∃uφ(u, z) is arithmetically equivalent to
∃w ∀z < y ∃u < w φ(u, z).
The implication in one direction is logical, and in the other direction we use the fact
that for any ﬁnitely many natural numbers u0, u1,. . . , uy−1 there is a number w that
is greater than all the uz.

206
REPRESENTABILITY OF RECURSIVE FUNCTIONS
For (e), ∃z < y ∃u φ(u, z) is logically equivalent to ∃u ∃z < y φ(u, z).
For (f), ∃u ∃v φ(u, v) is arithmetically equivalent to ∃w ∃u < w ∃v < w φ(u, v),
much as in part (b).
Repeated application of Lemma 16.9, followed by combination with Lemma 16.8
give the following:
16.10 Proposition. Every generalized ∃-rudimentary formula is arithmetically equiv-
alent to an ∃-rudimentary formula.
16.11 Lemma. Everyrecursivefunctionisarithmeticallydeﬁnablebyan∃-rudimentary
formula.
Call a function that is arithmetically deﬁnable by a rudimentary formula a rudimen-
tary function. Can we go further and show every recursive function to be rudimentary?
Not quite. The next lemma tells us how far we can go. It bears more than a passing
resemblance to Proposition 7.17.
16.12 Lemma. Every recursive function is obtainable by composition from rudimen-
tary functions.
Proof: Let f be a recursive function of, say, one place. (The proof for many-
place functions is exactly the same.) We know f is arithmetically deﬁnable by an
∃-rudimentary formula ∃zφ(x, y, z). Let S be the relation arithmetically deﬁned by
φ, so that we have
Sabc ↔φ(a, b, c) is correct.
We have
f (a) = b ↔∃c Sabc.
We now introduce two auxiliary functions:
g(a) =
⎧
⎨
⎩
the least d such that
∃b < d ∃c < d Sabc
if such a d exists
undeﬁned
otherwise
h(a, d) =
⎧
⎨
⎩
the least b < d such that
∃c < d Sabc
if such a b exists
0
otherwise.
(Note that if f is total, then g is total, while h is always total.) These functions are
rudimentary, being arithmetically deﬁnable by the following formulas φg(x, w) and
φh(x, w, y):
∃y < w ∃z < w φ(x, y, z) & ∀v < w ∀y < v ∀z < v ∼φ(x, y, z)
∃z < w φ(x, y, z) & ∀u < y ∀z < w ∼φ(x, u, z)
and a little thought shows that f (x) = h(x, g(x)) = h(id (x), g(x)), so f = Cn[h, id, g]
is a composition of rudimentary functions.

16.2. MINIMAL ARITHMETIC AND REPRESENTABILITY
207
If T is a consistent theory in the language of arithmetic, we say a set S is deﬁned
in T by D(x) if for all n, if n is in S, then D(n) is a theorem of T , and if n is not in S,
then ∼D(n) is a theorem of T . S is deﬁnable in T if S is deﬁned by some formula.
Arithmetical deﬁnability is simply the special case where T is true arithmetic, the
set of all correct sentences. The general notion of deﬁnability in a theory extends to
relations, but deﬁnability of a function turns out to be less useful than a related notion.
For the remainder of this chapter, unless otherwise noted, ‘function’ will mean ‘total
function’. Let f be a one-place function. (The deﬁnition we are about to give extends
easily to many-place functions.) We say f is representable in T if there is a formula
F(x, y) such that whenever f (a) = b, the following is a theorem of T :
∀y(F(a, y) ↔y = b).
This is logically equivalent to the conjunction of the positive assertion
F(a, b)
and the general negative assertion
∀y(y ̸= b →∼F(a, y)).
By contrast, deﬁnability would only require that we have the positive assertion and for
each particular c ̸= b the relevant particular instance of the general negative assertion,
namely, ∼F(a, c).
Now in the special case where T is true arithmetic, of course if each particular
numerical instance is correct, then the universal generalization is correct as well, so
representability and deﬁnability come to the same thing. But for other theories, each
particular numerical instance may be a theorem without the universal generalization
being a theorem, and representability is in general a stronger requirement than de-
ﬁnability. Note that if T is a weaker theory than T * (that is, if the set of theorems of
T is a subset of the set of theorems of T *), then the requirement that a function be
representable in T is a stronger requirement than that it be representable in T * (that
is, representability in T implies representability in T *). Thus far we have proved all
recursive functions to be representable in true arithmetic. If we are to strengthen our
results, we must consider weaker theories than that.
16.2 Minimal Arithmetic and Representability
We now introduce a ﬁnite set of axioms of minimal arithmetic Q, which, though
not strong enough to prove major theorems of number theory, at least are correct
and strong enough to prove all correct ∃-rudimentary sentences. By themselves, the
axioms of Q would not be adequate for number theory, but any set of adequate axioms
would have to include them, or at least to prove them (in which case the set might
as well include them). Our main theorems (Theorems 16.13 and 16.15) apply to any
theory T that contains Q, and since Q is weak, the theorems are correspondingly
strong.
In displaying the list of axioms we make use of a traditional convention, according
to which when displaying sentences of the language of arithmetic that begin with a

208
REPRESENTABILITY OF RECURSIVE FUNCTIONS
string of one or more universal quantiﬁers, one may omit to write the quantiﬁers and
write only the open formula that comes after them.
0 ̸= x′
(Q1)
x′ = y′ →x = y
(Q2)
x + 0 = x
(Q3)
x + y′ = (x + y)′
(Q4)
x · 0 = 0
(Q5)
x · y′ = (x · y)+ x
(Q6)
∼x < 0
(Q7)
x < y′ ↔(x < y ∨x = y)
(Q8)
0 < y ↔y =/= 0
(Q9)
x′ < y ↔(x < y & y =/= x′)
(Q10)
Thus axiom (Q1) is really ∀x 0 ̸= x′, axiom (Q2) is really ∀x∀y (x′ = y′ →x = y),
and so on. As is said, the real axioms are the universal closures of the formulas
displayed. The theory Q of minimal arithmetic is the set of all sentences of the
language of arithmetic that are provable from (or, equivalently, are true in all models
of) these axioms. The signiﬁcance of the various axioms will become clear as we
work through the steps of the proof of the main theorem of this section.
16.13 Theorem. An ∃-rudimentary sentence is correct if and only if it is a theorem
of Q.
Proof: Since every axiom of Q is correct, so is every theorem of Q, and hence
any ∃-rudimentary sentence provable from the axioms of Q is correct. All the work
will go into proving the converse. To begin with zero and sucessor, for any natural
number m, of course m = m (where m is as always the numeral for m, that is, is the
term 0′...′ with m accents ′) is provable even without any axioms, by pure logic.
All of 0 ̸= 1, 0 ̸= 2, 0 ̸= 3, . . . , are provable by (Q1) (since the numerals 1, 2,
3, . . . all end in accents). Then 1 = 2 →0 = 1, 1 = 3 →0 = 2, . . . are provable using
(Q2), and since 0 ̸= 1, 0 ̸= 2, . . . are provable, it follows by pure logic that 1 ̸= 2,
1 ̸= 3, . . . , are provable. Then 2 = 3 →1 = 2, 2 = 4 →1 = 3, . . . are provable, again
using (Q2), and since 1 ̸= 2, 1 ̸= 3, . . . , are provable, it follows by pure logic that
2 ̸= 3, 2 ̸= 4, . . . are provable. Continuing in the same way, if m < n, then m ̸= n is
provable.
It follows by pure logic (the symmetry of identity) that if m < n, then n ̸= m is
provable also. Since in general if m ̸= n we have either m < n or n < m, it follows
that if m ̸= n then both m ̸= n and n ̸= m are provable.
Turning now to order, note that using (Q8), x < 1 ↔(x < 0 ∨x = 0) is prov-
able, and (Q7) is ∼x < 0. By pure logic x < 1 ↔x = 0 is provable from these, so
that 0 < 1 is provable, and since we already know that 1 ̸= 0, 2 ̸= 0, . . . are provable,
it follows that ∼1 < 1, ∼2 < 1, . . . are provable. Then using (Q8) again, x < 2 ↔
(x < 1 ∨x = 1) is provable, from which, given what we already know to be provable,

16.2. MINIMAL ARITHMETIC AND REPRESENTABILITY
209
it follows that x < 2 ↔(x = 0 ∨x = 1) is provable, from which it follows that 0 < 2,
1 < 2, and also ∼2 < 2, ∼3 < 2, . . . are all provable. Continuing in the same way, for
any m the following is provable:
x < m ↔(x = 0 ∨x = 1 ∨. . . ∨x = m −1).
(1)
Moreover, whenever n < m, n < m is provable, and whenever m ≥n, ∼m < n is
provable.
Turning now to addition and multiplication, let us show how (Q3) and (Q4), which
are of course just the formal versions of the recursion equations for addition, can be
used to prove, for instance, 2 + 3 = 5, or 0′′ + 0′′′ = 0′′′′′. Using (Q4), the following
are all provable:
0′′ + 0′′′ = (0′′ + 0′′)′
0′′ + 0′′ = (0′′ + 0′)′
0′′ + 0′ = (0′′ + 0)′.
Using (Q3), 0′′ + 0 = 0′′ is provable. Working backwards, by pure logic the following
are all provable from what we have so far:
0′′ + 0′ = 0′′′
0′′ + 0′′ = 0′′′′
0′′ + 0′′′ = 0′′′′′
This is, in fact, just the formal calculation exhibited in section 6.1. Obviously this
method is perfectly general, and whenever a + b = c we can prove a+ b = c. Then
also, again as in section 6.1, the recursion equations (Q5) and (Q6) for multiplication
can be used to prove 2 · 3 = 6 and more generally, whenever a · b = c to prove a · b = c.
If we next consider more complex terms involving ′ and + and · , their correct
values are also provable. For example, consider (1+ 2) · (3+ 4). By what we have
already said, 1+ 2 = 3 and 3+ 4 = 7, as well as 3 · 7 = 21, are provable. From these it
is provable by pure logic that (1+ 2) · (3+ 4) = 21, and similarly for other complex
terms. Thus for any closed term t built up from 0 using ′, + , ·, it is provable what
is the correct value of the term. Suppose then we have two terms s, t that have the
same value m. Since by what we have just said s = m and t = m are provable, by pure
logic s = t is also provable. Suppose instead the two terms have different values m
and n. Then since s = m and t = n and m ̸= n are provable, again by pure logic s ̸= t
is also provable. A similar argument applies to order, so all correct formulas of types
s = t, s ̸= t, s < t, ∼s < t are provable. Thus all correct closed atomic and negated
atomic sentences are provable.
Now we move beyond atomic and negation-atomic sentences. First, by pure logic
the double negation of a sentence is provable if and only if the sentence itself is, and a
conjunction is provable if both its conjuncts are, a disjunction is provable if either of
its disjuncts is, a negated conjunction is provable if the negation of one of its conjuncts
is, and a negated disjunction is provable if the negations of both of its disjuncts are.
Since all correct atomic and negated atomic closed sentences are provable, so are
all correct sentences of types ∼S, ∼∼S, S1 & S2, ∼(S1 & S2), S1 ∨S2, ∼(S1 ∨S2),

210
REPRESENTABILITY OF RECURSIVE FUNCTIONS
where S, S1, S2 are atomic or negated atomic sentences. Continuing in this way, all
correct closed formulas built up from atomic formulas by negation, conjunction, and
disjunction are provable: All correct closed formulas without quantiﬁers are provable.
As for bounded quantiﬁers, using (1), for any formula A(x) and any m, the fol-
lowing are provable:
∀x < mA(x) ↔(A(0) & A(1) & . . . & A(m −1)),
∃x < mA(x) ↔(A(0) ∨A(1) ∨. . . ∨A(m −1)).
More generally, if t is a closed term whose correct value is m, since t = m is provable,
so are the following:
∀x < t A(x) ↔(A(0) & A(1) & . . . & A(m −1)),
∃x < t A(x) ↔(A(0) ∨A(1) ∨. . . ∨A(m −1)).
Thus any bounded universal or existential quantiﬁcation of formulas without quan-
tiﬁers can be proved equivalent to a conjunction or disjunction of sentences without
quantiﬁers, which is of course itself then a sentence without quantiﬁers, so that we
already know it can be proved if it is correct. Thus any correct sentence obtained by
applying bounded universal or existential quantiﬁcation to formulas without quanti-
ﬁers is provable, and repeating the argument, so is any correct sentence built up from
atomic formulas using negation, conjunction, disjunction, and bounded universal and
bounded existential quantiﬁcation: Any correct rudimentary sentence is provable.
Finally, consider now a correct ∃-rudimentary sentence ∃x A(x). Since it is correct,
there is some a such that A(a) is correct. Being correct and rudimentary, A(a) is
provable, and hence so is ∃x A(x), completing the proof.
Note that for a correct ∀-rudimentary sentence ∀x A(x), we can conclude that each
numerical instance A(0), A(1), A(2), . . . is provable from the axioms of Q, but this
is not to say that ∀xA(x) itself is provable from the axioms of Q, and in general it
is not. There are nonstandard interpretations of the language of arithmetic on which
all the axioms of Q come out true, but some very simple ∀-universal sentences that
are correct or true on the standard interpretation come out false. Works on set theory
develop an extremely natural nonstandard model of Q, called the system of ordinal
numbers, for which, among others, laws as simple as 1+ x = x + 1 fail. It would take
us too far aﬁeld to stop to develop this model here, but some of its features are hinted
at by the nonstandard interpretations of Q indicated in the problems at the end of
the chapter. As we have already said, the fact that Q is a weak theory makes the
following theorem (which automatically applies to any theory T containing Q) a
strong theorem.
16.14 Lemma. Every rudimentary function is representable in Q (and by a rudimentary
formula).
Proof: Inspection of the proof of the preceding lemma shows that it actually did
not require any use of (Q9) and (Q10), but the proof of the present lemma does. An
argument exactly like that used in the earlier proof to derive
x < m ↔(x = 0 ∨x = 1 ∨. . . ∨x = m −1)
(1)

16.2. MINIMAL ARITHMETIC AND REPRESENTABILITY
211
from (Q7) and (Q8) can be used to derive
m < y ↔(y =/= 0 & y =/= 1 & . . . & y =/= m)
(2)
from (Q9) and (Q10). An immediate consequence of (1) and (2) together is the
following:
z < m ∨z = m ∨m < z.
(3)
Now let f be a one-place rudimentary function. (The proof for many-place func-
tions is exactly the same.) Let φ(x, y) be a rudimentary formula arithmetically
deﬁning f. We do not claim that φ represents f in Q, but we do claim that φ can be
used to build another rudimentary formula ψ that does represent f in Q. The formula
ψ(x, y) is simply
φ(x, y) & ∀z < y ∼φ(x, z).
To show this formula represents f we must do two things. First, we must show
that if f (a) = b, then ψ(a, b) is a theorem of Q. But indeed, since φ arithmetically
deﬁnes f , if f (a) = b, then φ(a, b) is correct, and ∼φ(a, c) is correct for every c ̸= b,
and in particular for every c < b. Therefore ∀z < b ∼φ(a, z) is correct and ψ(a, b) is
correct, and being rudimentary, it is a theorem of Q by Theorem 16.13.
Second, we must show that the following is a theorem of Q:
y ̸= b →∼ψ(a, y),
which is to say
y ̸= b →∼(φ(a, y) & ∀z < y ∼φ(a, z))
or, what is logically equivalent,
φ(a, y) →(y = b ∨∃z < y φ(a, z)).
(4)
It will be sufﬁcent to show that the following is a theorem of Q, since together
with φ(a, b), which we know to be a theorem of Q, it logically implies (4):
φ(a, y) →(y = b ∨b < y).
(5)
But (3), together with ∀y < b ∼φ(a, y), which we know to be a theorem of Q, logically
implies (5), to compete the proof.
16.15 Lemma. Any composition of rudimentary functions is representable in Q (and
by an ∃-rudimentary formula).
Proof: We consider the composition of two one-place functions, the proof for
many-place functions being similar. Suppose f and g are rudimentary functions, rep-
resented in Q by the rudimentary formulas φ f and φg respectively. Let h(x) = g( f (x)),
and consider the (∃-rudimentary) formula φhwe get from the proof of Lemma 16.4:
∃y(φ f (x, y) & φg(y, z)) .

212
REPRESENTABILITY OF RECURSIVE FUNCTIONS
We claim φh represents h in Q. For let a be any number, b = f (a), and c = h(a) =
g( f (a)) = g(b). Since φ f represents f and f (a) = b, the following is a theorem
of Q:
∀y(φ f (a, y) ↔y = b) .
(1)
Since φg represents g and g(b) = c, the following is a theorem of Q:
∀z(φg(b, z) ↔z = c) .
(2)
What we need to show in order to establish that φh represents h in Q is that the
following is a theorem of Q:
∀z(∃y(φ f (a, y) & φg(y, z)) ↔z = c) .
(3)
But (3) is logically implied by (1) and (2)!
16.16 Theorem
(a) Every recursive function is representable in Q (and by an ∃-rudimentary formula).
(b) Every recursive relation is deﬁnable in Q (and by an ∃-rudimentary formula).
Proof: (a)isimmediatefromLemmas16.12,16.14,and16.15.For(b),weconsider
the case of a one-place relation or set, many-place relations being similar. Let P be
the recursive set, f its characteristic function, and ∃wφ(x, y, w) an ∃-rudimentary
formula representing f in Q. If n is in P, then f (n) = 1, and Q proves ∃wφ(n, 1, w).
If n is not in P, then f (n) = 0, and Q proves ∀y(y ̸= 0 →∼∃wφ(n, y, w)) and in
particular ∼∃wφ(n, 0, w). So the formula ∃wφ(x, 1, w) deﬁnes P in Q.
Careful review of the proof of Theorem 16.16(a) shows that it actually applies to
any recursive total or partial function f and gives a formula that both arithmetically
deﬁnes and represents f in Q. This reﬁnement will not be needed, however, for our
work in the next chapter.
We now have all the machinery we need for the proof of the ﬁrst G¨odel incom-
pleteness theorem, and readers impatient to see that famous result may skip ahead to
the next chapter. They should then return to the next brief section of this one before
going on to the second G¨odel incompleteness theorem in the chapter after next.
16.3 Mathematical Induction
The most immediate reason for the inadequacy of the axioms of minimal arithmetic
to prove many correct ∀-universal sentences is that they make no provision for proof
by mathematical induction, a method ubiquitously used in number theory and mathe-
matics generally, according to which we can prove that every number has some
property by proving that zero has it (the zero or basis step), and proving that, assuming
a number x has it (an assumption called the induction hypothesis) then the successor
of x also has it (the successor or induction step).
16.17 Example (Dichotomy). As the most trivial example, we can prove by mathematical
induction that every x is either 0 or the successor of some number. Basis. 0 is 0. Induction.
x′ is the successor of x.

16.3. MATHEMATICAL INDUCTION
213
Another example is the proof of the law
0 + 1 + 2 + · · · + x = x(x + 1)/2.
Basis. 0 = 0 · 1/2. Induction. Assuming the result for x, we have
0 + 1 + 2 + · · · + x + (x + 1) = x(x + 1)/2 + (x + 1)
= [x(x + 1) + 2(x + 1)]/2
= (x + 1)(x + 2)/2.
The algebraic manipulations in this proof depend on basic laws of arithmetic (asso-
ciative, commutative, distributive) which can be proved using mathematical induc-
tion.
16.18 Example (Additive identity). By mathematical induction one can prove (from the
recursion equations deﬁning addition) 0 + x = x + 0. Zero or basis step: for x = 0 we have
0 + 0 = 0 + 0 by pure logic. Successor or induction step: assuming 0 + x = x + 0, we have
0 + x′ = (0 + x)′
by the second recursion equation for addition
(0 + x)′ = (x + 0)′
by our assumption
(x + 0)′ = x′ = x′ + 0
by the ﬁrst recursion equation for addition.
16.19 Example (First case of the commutativity of addition). Similarly, we can prove
1 + x = x + 1, or 0′ + x = x + 0′. Basis: 0′ + 0 = 0 + 0′ by the preceding example. Induc-
tion: assuming 0′ + x = x + 0′, we have
0′ + x′ = (0′ + x)′
by the second recursion equation for addition
(0′ + x)′ = (x + 0′)′
by assumption
(x + 0′)′ = (x + 0)′′
by the second recursion equation for addition
(x + 0)′′ = x′′
by the ﬁrst recursion equation for addition
x′′ = (x′ + 0)′
by the ﬁrst recursion equation for addition
(x′ + 0)′ = x′ + 0′
by the second recursion equation for addition.
We relegate further examples of this kind to the problems at the end of the chapter.
Once we have the basic laws of arithmetic, we can go on to prove various ele-
mentary lemmas of number theory such as the facts that a divisor of a divisor of
a number is a divisor of that number, that every number has a prime factor, that
if a prime divides a product it divides one of its factors, and that if two numbers
with no common prime factor both divide a number, then so does their product.
(The reader may recognize these as results we took for granted in the proof of
Lemma 16.5.) Once we have enough elementary lemmas, we can go on to prove
more substantial theorems of number theory, such as Lagrange’s theorem from
Example 16.7.
Closely related to the principle of mathematical induction as stated above is the
principle of complete induction, according to which we can prove that every number
has some property P by proving that zero has P, and proving that, assuming every
number ≤x has P, then the successor of x also has P. Indeed, complete induction
for a property P follows on applying mathematical induction to the related property
‘every number ≤x has P,’ using the facts (Q7) that 0 is the only number ≤0, and
(Q8) that the only numbers ≤x′ are the numbers ≤x and x′ itself.

214
REPRESENTABILITY OF RECURSIVE FUNCTIONS
Another related principle is the least-number principle, according to which, if
there is some number that has a given property, then there is a least number having
the property, one such that no lesser number has it. This principle follows from the
principle of mathematical induction as follows. Consider some property P such that
there is no least number with the property P. Then we can use induction to show
that in fact no number has the property P. We do this a bit indirectly, showing ﬁrst
by induction that for any number x, there is no number less than x with the property
P. Basis: there is no number less than zero with the property P, because by (Q7)
there is no number less than zero at all. Induction: supposing there is no number
less than x with the property P, there can be no number less than the successor of
x with the property P, since by (Q8) the only numbers less than the successor
of x are the numbers less than x, which by assumption do not have the property,
and x itself, which if it had the property would be the least number having the prop-
erty. Now that we know that for any number x there is no number y less than x
with the property, it follows that there is no number y with the property, since, tak-
ing x to be the successor of y, y is less than x and therefore cannot have the pro-
perty.
(Conversely, the least-number principle together with the dichotomy of Example
16.17 yields the principle of mathematical induction. For if zero has a property and
the successor of any number having the property has it also, then neither zero nor any
successor can be the least number failing to have the property.)
All our argumentation in this section so far has been informal. A more adequate
set of formal axioms for number theory is provided by the set of axioms of Peano
arithmetic—aninﬁnite(butprimitiverecursive)setofaxiomsconsistingoftheaxioms
of Q plus all sentences of the following form:
(A(0) & ∀x(A(x) →A(x′))) →∀x A(x).
[Here A(x) may contain other free variables y1, . . . , yn, and what is really meant is
∀y1 . . . ∀yn((A(0, y1, . . . , yn) & ∀x(A(x, y1, . . . , yn) →A(x′, y1, . . . , yn))) →
∀x A(x, y1, . . . , yn))
in accordance with the traditional convention of suppressing initial universal quanti-
ﬁers in displayed formulas.]
The theory P of Peano arithmetic is the set of all sentences of the language of
arithmetic that are provable from (or equivalently, are consequences of) these axioms.
A rule to the effect that all sentences of a certain kind are to be taken as axioms is called
an axiom scheme. With this terminology it would be said that the axioms of Peano
arithmetic P consist of ﬁnitely many individual axioms (those of minimal arithmetic
Q) plus a single axiom scheme (the induction scheme as above). In practice, the
sets of axioms of most interest to logicians tend to consist of at most a dozen or so
individual axioms and at most a very few axiom schemes, and so in particular are
primitive recursive.

16.3. MATHEMATICAL INDUCTION
215
Among the axioms of P are for instance the following:
(0 + 0 = 0 + 0 &
∀x(0 + x = x + 0 →0 + x′ = x′ + 0)) →
∀x 0 + x = x + 0
and
(0′ + 0 = 0 + 0′ &
∀x(0′ + x = x + 0′ →0′ + x′ = x′ + 0′)) →
∀x 0′ + x = x + 0′.
And using these axioms in addition to the axioms of Q, the laws 0 + x = x + 0
and 1 + x = x + 1 are provable from the axioms of P, by ‘formalizing’ the proof of
these laws given above as Examples 16.18 and 16.19. Also, for any formula F(x) the
least-number principle for F, namely
∃x F(x) →∃x(F(x) & ∀y < x ∼F(y))
is provable from the axioms of P, again by ‘formalizing’ the proof given above;
and similarly for complete induction. Eventually the usual proofs of, say, Lagrange’s
theorem in textbooks on number theory can be ‘formalized’ to give proofs from the
axioms of P.
The method of proof by mathematical induction is indeed an ingredient in the
proofs of essentially all major theorems in mathematics, but it is perhaps especially
common in metamathematics, the branch of mathematics concerned with giving
proofs about what can be proved in mathematics—the branch to which the present
book belongs. We have been using this method of proof all along, often in disguise.
Consider, for instance, the proof by induction on complexity of formulas, of which we
have made considerable use. What one does with this method is, not to put too ﬁne a
point on it, prove (as base step) that any atomic formula, which is to say, any formula
containing 0 occurrences of the logical symbols (negation, junctions, quantiﬁers), has
a certain property, and then prove (as inductive step) that if all formulas containing no
more than n occurrences of the logical symbols have the property, then so does any for-
mula containing n′ such occurrences. The proof of the latter assertion is broken down
into cases according as the one extra symbol is a negation, a junction, or a quantiﬁer.
This method of proof is really a special form of proof by mathematical induction.
And in our proof of Theorem 16.13 in the preceding section, for instance, every
step involved some sort of induction, though we have expressed it very casually, using
such phrases as ‘continuing in the same way’. A less casual way of putting the second
paragraph of the proof, for instance, would be as follows:
It can be proved by mathematical induction that if m < n, then m ̸= n is provable from
the axioms of Q. Basis: if 0 < n then 0 ̸= n is provable by (Q0) (since the numeral n ends
in an accent). Induction: assuming m ̸= n is provable whenever m < n, if m′ < n, then we
show m′ ̸= n is provable as follows. Let n = k′. Then m < k, and by assumption m ̸= k
is provable. But m′ = k′ →m = k, which is to say m′ = n →m = k, is provable by (Q1).
It follows by pure logic that m ̸= n is provable.

216
REPRESENTABILITY OF RECURSIVE FUNCTIONS
In this example we are using induction (‘in the metalanguage’) to prove something
about a theory that does not have induction as an axiom (‘in the object language’):
we prove that something is a theorem of Q for every m by proving it is a theorem
for 0, and that if it is a theorem for m, then it is a theorem for m′. Again, this sort of
proof can be ‘formalized’ in P.
16.4* Robinson Arithmetic
This optional section is addressed to readers who wish to compare our treatment of
the matters with which we have been concerned in this chapter with other treatments
in the literature. In the literature, the label Q is often used to refer not to our minimal
arithmetic but to another system, called Robinson arithmetic, for which we use the
label R. To obtain the axioms of R from those of Q, add
x = 0 ∨∃y x = y′
(Q0)
and replace (Q7)–(Q10) by
x < y ↔∃z(z′ + x = y).
(Q11)
We have already mentioned an extremely natural nonstandard model for Q, called
the system of ordinal numbers, in which (Q0) fails. There is also an extremely nat-
ural nonstandard model for R, called the system of cardinal numbers, in which
(Q10) fails; though it would take us too far aﬁeld to develop this model here, a
simpliﬁed version sufﬁces to show that some theorems of Q are not theorems of R.
Thus Q is in some respects weaker and in some respects stronger than R, and vice
versa.
By Theorem 16.16, every recursive function is representable in Q. Careful reread-
ing of the proof reveals that all the facts it required about order are these, that the
following are theorems:
a < b, whenever a< b
(1)
∼x < 0
(2)
0 < y ↔y ̸= 0
(3)
and for any b the following:
x < b′ →x < b ∨x = b
(4)
b < y & y ̸= b′ →b′ < y.
(5)
Clearly (1) is a theorem of R, since if a < b, then for some c, c′+ a = b, and
then c′ + a = b is a consequence of (Q1)–(Q4). Also (2), which is axiom (Q7), is a
theorem of R. For ﬁrst z′ + 0 = z′ ̸= 0 by (Q3) and (Q1), which gives us ∼0 < 0,

PROBLEMS
217
and then second z′ + y′ = (z′ + y′) ̸= 0 by (Q4) and (Q1), which gives us ∼y′ < 0.
But these two, together with (Q0), give us (2). Also (3), which is axiom (Q9), is a
theorem of R. For 0 < y →y ̸= 0 follows from ∼0 < 0, and for the opposite direction,
(Q0) gives y ̸= 0→∃z(y = z′), while (Q3) gives y = z′ →z′ + 0 = y, and (Q11)
gives ∃z(z′ + 0 = y) →0 < y, and (3) is a logical consequence of these three.
It turns out that (4) and (5) are also theorems of R, and hence every recursive
function is representable in R. Proofs have been relegated to the problems at the end
of the chapter because we do not need any results about R for our later work. All we
need for the purposes of proving the celebrated G¨odel incompleteness theorems and
their attendant lemmas and corollaries in the next chapter is that there is some correct,
ﬁnitely axiomatizable theory in the language of arithmetic in which all recursive
functions are representable. We chose minimal arithmetic because it is easier to
prove representability for it; except in this regard Robinson arithmetic would really
have done no worse and no better.
Problems
16.1 Show that the class of arithmetical relations is closed under substitution of
recursive total functions. In other words, if P is an arithmetical set and f a
recursive total function, and if Q(x) ↔P( f (x)), then Q is an arithmetical set,
and similarly for n-place relations and functions.
16.2 Show that the class of arithmetical relations is closed under negation, conjunc-
tion, disjunction, and universal and existential quantiﬁcation, and in particular
that every semirecursive relation is arithmetical.
16.3 A theory T is inconsistent if for some sentence A, both A and ∼A are theorems
of T . A theory T in the language of arithmetic is called ω-inconsistent if
for some formula F(x), ∃x F(x) is a theorem of T , but so is ∼F(n) for each
natural number n. Let T be a theory in the language of arithmetic extending Q.
Show:
(a) If T proves any incorrect ∀-rudimentary sentence, then T is inconsistent.
(b) If T proves any incorrect ∃-rudimentary sentence, then T is
ω-inconsistent.
16.4 Extend Theorem 16.3 to generalized ∃-rudimentary sentences.
16.5 Let R be the set of triples (m, a, b) such that m codes a formula φ(x, y) and
Q proves
∀y(φ(a, y) ↔y = b).
Show that R is semirecursive.
16.6 For R as in the preceding problem, show that R is the graph of a two-place
partial function.
16.7 A universal function is a two-place recursive partial function F such that for
any one-place recursive total or partial function f there is an m such that
f (a) = F(m, a) for all a. Show that a universal function exists.

218
REPRESENTABILITY OF RECURSIVE FUNCTIONS
The result of the preceding problem was already proved in a completely
different way (using the theory of Turing machines) in Chapter 8 as Theorem
8.5. After completing the preceding problem, readers who skipped section 8.3
may turn to it, and to the related problems at the end of Chapter 8.
16.8 A set P is (positively) semideﬁnable in a theory T by a formula φ(x) if for every
n, φ(n) is a theorem of T if and only if n is in P. Show that every semirecursive
set is (positively) semideﬁnable in Q and any ω-consistent extension of Q.
16.9 Let T be a consistent, axiomatizable theory containing Q. Show that:
(a) Every set (positively) semi-deﬁnable in T is semirecursive.
(b) Every set deﬁnable in T is recursive.
(c) Every total function representable in T is recursive.
16.10 Using the recursion equations for addition, prove:
(a) x + (y + 0) = (x + y) + 0.
(b) x + (y + z) = (x + y) + z →x + (y + z′) = (x + y) + z′.
The associative law for addition,
x + (y + z) = (x + y) + z
then follows by mathematical induction (‘on z’). (You may argue informally,
as at the beginning of section 16.3. The proofs can be ‘formalized’ in P, but
we are not asking you to do so.)
16.11 Continuing the preceding problem, prove:
(c) x′ + y = (x + y)′,
(d) x + y = y + x.
The latter is the commutative law for addition.
16.12 Continuing the preceding problems, prove the associative and distributive and
commutative laws for multiplication:
(e) x · (y + z) = x · y + x · z,
(f) x · (y · z) = (x · y) · z,
(g) x · y = y · x.
16.13 (a) Consider the following nonstandard order relation on the natural numbers:
m <1 n if and only if m is odd and n is even, or m and n have the same
parity (are both odd or both even) and m < n. Show that if there is a natural
number having a property P then there is a <1-least such natural number.
(b) Consider the following order on pairs of natural numbers: (a, b) <2 (c, d)
if and only if either a < c or both a = c and b < d. Show that if there is a
pair of natural numbers having a property P then there is a <2-least such
pair.
(c) Consider the following order on ﬁnite sequences of natural numbers:
(a0, . . . , am) <3 (b0, . . . , bn) if and only if either m < n or both m = n
and the following condition holds: that either am < bm or else for some
i < m, ai < bi while for j > i we have a j = b j. Show that if there is a
sequence of natural numbers having a property P then there is a <3-least
such sequence.
16.14 Consider a nonstandard interpretation of the language {0, ′, <} in which the
domain is the set of natural numbers, but the denotation of < is taken to be the

PROBLEMS
219
relation <1 of Problem 16.13(a). Show that by giving suitable denotations to
0 and ′, axioms (Q1)–(Q2) and (Q7)–(Q10) of Q can be made true, while the
sentence ∀x(x = 0 ∨∃y x = y′) is made false.
16.15 Consider a nonstandard interpretation of the language {0, ′, <,+} in which
the domain is the set of pairs of natural numbers, and the denotation of < is
taken to be the relation <2 of Problem 16.13(b). Show that by giving suitable
denotations to 0 and ′ and +, axioms (Q1)–(Q4) and (Q7)–(Q10) of Q can be
made true, while both the sentence of the preceding problem and the sentence
∀y(1+ y = y + 1) are made false.
16.16 Consider an interpretation of the language {0, ′,+, ·, <} in which the domain
is the set of natural numbers plus one additional object called ∞, where the
relations and operations on natural numbers are as usual, ∞′ = ∞, x + ∞=
∞+ x = ∞for any x, 0 · ∞= ∞· 0 = 0 but x · ∞= ∞· x = ∞for any
x ̸= 0, and x < ∞for all x, but not ∞< y for any y ̸= ∞. Show that axioms
(Q0)–(Q9) and (Q11) are true on this interpretation, but not axiom (Q10).
16.17 Show that, as asserted in the proof of Lemma 16.14, for each m the following
is a theorem of Q:
m < y ↔(y ̸= 0 & y ̸= 1 & . . . & y ̸= m).
16.18 Show that if the induction axioms are added to (Q1)–(Q8), then (Q9) and (Q10)
become theorems. The following problems pertain to the optional section 16.4.
16.19 Show that the following are theorems of R for any b:
(a) x′ + b = x + b′.
(b) b < x →b′ < x′.
(c) x′ < y′ →x < y.
16.20 Show that the following are theorems of R for any b:
(a) x < b′ →x < b ∨x = b.
(b) b < y & y ̸= b′ →b′ < y.
16.21 Show that adding induction to R produces the same theory (Peano arithmetic
P) as adding induction to Q.

17
Indeﬁnability, Undecidability, Incompleteness
We are now in a position to give a uniﬁed treatment of some of the central negative
results of logic: Tarski’s theorem on the indeﬁnability of truth, Church’s theorem on the
undecidability of logic, and G¨odel’s ﬁrst incompleteness theorem, according to which,
roughly speaking, any sufﬁciently strong formal system of arithmetic must be incomplete
(if it is consistent). These theorems can all be seen as more or less direct consequences
of a single exceedingly ingenious lemma, the G¨odel diagonal lemma. This lemma, and
the various negative results on the limits of logic that follow from it, will be presented
in section 17.1. This presentation will be followed by a discussion in section 17.2 of
some classic particular examples of sentences that can be neither proved nor disproved
in theories of arithmetic like Q or P. Further such examples will be presented in the
optional section 17.3. According to G¨odel’s second incompleteness theorem, the topic
of the next chapter, such examples also include the sentence stating that P is consistent.
17.1 The Diagonal Lemma and the Limitative Theorems
By the results in the preceding chapter on the representability of recursive functions,
we can ‘talk about’ such functions within a formal system of arithmetic. By the
results of the chapter before that on the arithmetization of syntax, we can ‘talk about’
sentences and proofs in a formal system of arithmetic in terms of recursive functions.
Putting the two together, we can ‘talk about’ sentences and proofs in a formal system
of arithmetic within the formal system of arithmetic itself. This is the key to the main
lemma of this section, the diagonal lemma.
Until further notice, all formulas, sentences, theories, and so on, will be formulas,
sentences, theories, or whatever in the language of arithmetic. Given any expression
A of the language of arithmetic, we have introduced in Chapter 15 a code number
for A, called the G¨odel number of A. If a is this number, then the numeral a for a,
consisting of 0 followed by a accents ′, is naturally called the G¨odel numeral for A.
We write A for this code numeral for A. In what follows, A will be seen to function
somewhat like a name for A.
We deﬁne the diagonalization of A to be the expression ∃x(x = A & A). While
this notion makes sense for arbitrary expressions, it is of most interest in the case of a
formula A(x) with just the one variable x free. Since in general F(t) is equivalent to
∃x(x = t & F(x)), in case A is such a formula, the diagonalization of A is a sentence
220

17.1. THE DIAGONAL LEMMA AND THE LIMITATIVE THEOREMS
221
equivalent to A( A ), the result of substituting the code numeral for A itself for the
free variable in A. In this case the diagonalization ‘says that’ A is satisﬁed by its
own G¨odel number, or more precisely, the diagonalization will be true in the standard
interpretation if and only if A is satisﬁed by its own G¨odel number in the standard
interpretation.
17.1 Lemma (Diagonal lemma). Let T be a theory containing Q. Then for any formula
B(y) there is a sentence G such that ⊢T G ↔B( G ).
Proof: There is a (primitive) recursive function, diag, such that if a is the G¨odel
number of an expression A, then diag(a) is the G¨odel number of the diagonalization
of A. Indeed, we have seen almost exactly the function we want before, in the proof
of Corollary 15.6. Recalling that ofﬁcially x = y is supposed to be written =(x, y),
it can be seen to be
diag(y) = exquant(v, conj(i ∗l ∗v ∗c ∗num(y) ∗r, y))
where v is the code number for the variable, i,l,r, and c for the equals sign, left and
right parentheses, and comma, and exquant, conj, and num are as in Proposition 15.1
and Corollary 15.6.
If T is a theory extending Q, then diag is representable in T by Theorem 16.16. Let
Diag(x, y) be a formula representing diag, so that for any m and n, if diag (m) = n,
then ⊢T ∀y(Diag(m,y) ↔y = n).
Let A(x)betheformula∃y(Diag(x, y) & B(y)).Leta betheG¨odelnumberof A(x),
and a its G¨odel numeral. Let G be the sentence ∃x(x = a & ∃y(Diag(x, y) & B(y))).
Thus G is ∃x(x = a & A(x)), and is logically equivalent to A(a) or ∃y(Diag(a, y) &
B(y)). The biconditional G ↔∃y(Diag(a, y) & B(y)) is therefore valid, and as such
provable in any theory, so we have
⊢T G ↔∃y(Diag(a, y) & B(y)).
Let g be the G¨odel number of G, and g its G¨odel numeral. Since G is the diagonal-
ization of A(x), diag(a) = g and so we have
⊢T ∀y(Diag(a, y) ↔y = g).
It follows that
⊢T G ↔∃y(y = g & B(y)).
Since ∃y(y = g & B(y)) is logically equivalent to B(g), we have
⊢T ∃y(y = g & B(y)) ↔B(g).
It follows that
⊢T G ↔B(g)
or in other words, ⊢T G ↔B( G ), as required.
17.2 Lemma. Let T be a consistent theory extending Q. Then the set of G¨odel numbers
of theorems of T is not deﬁnable in T .

222
INDEFINABILITY, UNDECIDABILITY, INCOMPLETENESS
Proof: Let T be an extension of Q. Suppose θ(y) deﬁnes the set  of G¨odel
numbers of sentences in T . By the diagonal lemma there is a sentence G such that
⊢T G ↔∼θ( G ).
In other words, letting g be the G¨odel number of G, and g its G¨odel numeral, we have
⊢T G ↔∼θ(g).
Then G is a theorem of T . For if we assume G is not a theorem of T , then g is not
in , and since θ(y) deﬁnes , we have ⊢T ∼θ(g); but then since ⊢T G ↔∼θ(g), we
have ⊢T G and G is a theorem of T after all. But since G is a theorem of T , g is in
, and so we have ⊢T θ(g); but then, since ⊢T G ↔∼θ(g), we have ⊢T ∼G, and T is
inconsistent.
Now the ‘limitative theorems’ come tumbling out in rapid succession.
17.3 Theorem (Tarski’s theorem). The set of G¨odel numbers of sentences of the lan-
guageofarithmeticthatarecorrect,ortrueinthestandardinterpretation,isnotarithmetically
deﬁnable.
Proof: The set T in question is the theory we have been calling true arithmetic.
It is a consistent extension of Q, and arithmetic deﬁnability is simply deﬁnability in
this theory, so the theorem is immediate from Lemma 17.2.
17.4 Theorem (Undecidability of arithmetic). The set of G¨odel numbers of sentences
of the language of arithmetic that are correct, or true in the standard interpretation, is not
recursive.
Proof: This follows from Theorem 17.3 and the fact that all recursive sets are
deﬁnable in arithmetic.
Assuming Church’s thesis, this means that the set in question is not effectively
decidable: there are no rules—of a kind requiring only diligence and persistence, not
ingenuity and insight, to execute—with the property that applied to any sentence of
the language of arithmetic they will eventually tell one whether or not it is correct.
17.5 Theorem (Essential undecidability theorem). No consistent extension of Q is
decidable (and in particular, Q itself is undecidable).
Proof: Suppose T is a consistent extension of Q (in particular, T could just be Q
itself). Then by Lemma 17.2, the set  of G¨odel numbers of theorems of T is not
deﬁnable in T . Now, again as in the proof of Theorem 17.4, we invoke the fact that
every recursive set is deﬁnable in T . So the set  is not recursive, which is to say T
is not decidable.

17.1. THE DIAGONAL LEMMA AND THE LIMITATIVE THEOREMS
223
17.6 Theorem (Church’s theorem). The set of valid sentences is not decidable.
Proof: Let C be the conjunction of all the axioms of Q. Then a sentence A is a
theorem of Q if and only if A is a consequence of C, hence if and only if (∼C ∨A) is
valid. The function f taking the G¨odel number of A to that of (∼C ∨A) is recursive [it
being simply f (y) = disj(neg(c), y), in the notation of the proof of Proposition 15.1].
If the set  of logically valid sentences were recursive, the set K of G¨odel numbers of
theorems of Q would be obtainable from it by substitution of the recursive function
f , since a is in K if and only if f (n) is in , and so would be recursive, which it is
not by Theorem 17.4.
The sets of valid sentences, and of theorems of any axiomatizable theory, are
semirecursive by Corollaries 15.4 and 15.5, and intuitively, of course, both are posi-
tively effectively decidable: in principle, if not in practice, just by searching through
all demonstrations (or all proofs from the axioms of the theory), if a sentence is valid
(or a theorem of the theory), one will eventually ﬁnd that out. But Theorems 17.5 and
17.6 tell us these sets are not recursive, and so by Church’s thesis are not effectively
decidable.
17.7 Theorem (G¨odel’s ﬁrst incompleteness theorem). There is no consistent, com-
plete, axiomatizable extension of Q.
Proof: Any complete axiomatizable theory is decidable by Corollary 15.7, but no
consistent extension of Q is decidable by Theorem 17.5 above.
The import of G¨odel’s ﬁrst incompleteness theorem is sometimes expressed in
the words ‘any sufﬁciently strong formal system of arithmetic (or mathematics) is
incomplete, unless it is inconsistent’. Here by ‘formal system’ is meant a theory
whose theorems are derivable by the rules of logical derivation from a set of ax-
ioms that is effectively decidable, and hence (assuming Church’s thesis) recursive. So
‘formal system’ amounts to ‘axiomatizable theory’, and ‘formal system of arithmetic’
to ‘axiomatizable theory in the language of arithmetic’. G¨odel’s ﬁrst incompleteness
theorem in the version in which we have given it indicates a sufﬁcient condition
for being ‘sufﬁciently strong’, namely, being an extension of Q. Since Q is a com-
paratively weak theory, this version of G¨odel’s ﬁrst incompleteness theorem is a
correspondingly strong result.
Now a formal system of mathematics might well be such that the domain of its
intended interpretation was a more inclusive set than the set of natural numbers, and
it might well be such that it did not have symbols speciﬁcally for ‘less than’ and the
other items for which there are symbols in the language of arithmetic. So the principle
that any two natural numbers are comparable as to ordering might not be expressed,
as it is in the axioms of Q, by the sentence
∀x∀y(x < y ∨x = y ∨y < x).

224
INDEFINABILITY, UNDECIDABILITY, INCOMPLETENESS
Still, it is reasonable to understand ‘sufﬁciently strong’ as implying that this principle
can be somehow expressed in the language of the theory, perhaps by a sentence
∀x(N(x) →∀y(N(y) →(L(x, y) ∨x = y ∨L(y, x))))
where N(x) appropriately expresses ‘x is a natural number’ and L(x, y) appropriately
expresses ‘x is less than y’. Moreover, the sentence that thus ‘translates’ this or any
axiom of Q should be a theorem of the theory. Such is the case, for instance, with the
formal systems considered in works on set theory, such as the one known as ZFC,
which are adequate for formalizing essentially all accepted mathematical proofs.
When the notion of ‘translation’ is made precise, it can be shown that any ‘sufﬁciently
strong’ formal system of mathematics in the sense we have been indicating is still
subject to the limitative theorems of this chapter. In particular, if consistent, it will be
incomplete.
Perhaps the most important implication of the incompleteness theorem is what is
says about the notions of truth (in the standard interpretation) and provability (in a
formal system): that they are in no sense the same.
17.2 Undecidable Sentences
A sentence in the language of a theory T is said to be disprovable in T if its negation
is provable in T , and is said to be undecidable in or by or for T if it is neither provable
nor disprovable in T . (Do not confuse the notion of an undecidable sentence with that
of an undecidable theory. True arithmetic, for example, is an undecidable theory with
no undecidable sentences: the sentences of its language that are true in the standard
interpretation all being provable, and those that are false all being disprovable.) If
T is a theory in the language of arithmetic that is consistent, axiomatizable, and an
extension of Q, then T is an undecidable theory by Theorem 17.4, and there exist
undecidable sentences for T by Theorem 17.7. Our proof of the latter theorem did
not, however, exhibit any explicit example of a sentence that is undecidable for T .
An immediate question is: can we ﬁnd any such speciﬁc examples?
In order to do so, we use the fact that the set of sentences that are provable
and the set of sentences that are disprovable from any recursive set of axioms is
semirecursive, and that all recursive sets are deﬁnable by ∃-rudimentary formulas.
It follows that there are formulas PrvT (x) and DisprvT (x) of forms ∃y PrfT (x, y)
and ∃y DisprfT (x, y) respectively, with Prf and Disprf rudimentary, such that ⊢T A
if and only if the sentence PrvT ( A ) is correct or true in the standard interpretation,
and hence if and only if for some b the sentence PrfT ( A , b) is correct or—what
is equivalent for rudimentary sentences—provable in Q and in T ; and similarly for
disprovability. PrfT (x, y) could be read ‘y is a witness to the provability of x in T ’.
By the diagonalization lemma, there is a sentence GT such that
⊢T GT ↔∼∃y PrfT ( GT , y)
and a sentence RT such that
⊢T RT ↔∀y(PrfT ( RT , y) →∃z < y Disprf( RT , z)).

17.2. UNDECIDABLE SENTENCES
225
Such a GT is called a G¨odel sentence for T , and such an RT a Rosser sentence for
T . Thus a G¨odel sentence ‘says of itself that’ it is unprovable, and a Rosser sentence
‘says of itself that’ if there is a witness to its provability, then there is an earlier witness
to its disprovability.
17.8 Theorem. Let T be a consistent, axiomatizable extension of Q. Then a Rosser
sentence for T is undecidable for T .
Proof: Suppose the Rosser sentence RT is provable in T . Then there is some a
that witnesses the provability of RT . Since T is consistent, ∼RT is not also provable,
and so no m witnesses the disprovability of RT , and in particular, no m < n does so.
It follows that the rudimentary sentence
PrfT ( RT , n) & ∼∃z < n DisprfT ( RT , z)
is correct and as such is provable from the axioms of Q, and hence from T . In other
words, we have
⊢T PrfT ( RT , n) & ∼∃z < n DisprfT ( RT , z)
while, since RT is a Rosser sentence, we also have
⊢T RT ↔∀y(PrfT ( RT , y) →∃z < y Disprf( RT , z)).
By pure logic it follows that
⊢T ∼RT .
But then T is inconsistent, both RT and ∼RT being provable, contrary to assumption.
This contradiction shows that RT cannot be provable.
Suppose the Rosser sentence RT is disprovable in T . Then there is some m that
witnesses the disprovability of RT . Since T is consistent, RT is not also provable,
and so no n witnesses the provability of RT , and in particular, no n ≤m does so. It
follows that the rudimentary formulas
DisprfT ( RT , m)
∀x((x < m ∨x = m) →∼PrfT ( RT , x))
are correct and hence provable in T . In other words we have
⊢T DisprfT ( RT , m),
⊢T ∀y((y < m ∨y = m) →∼PrfT ( RT , y)).
By pure logic it follows from the former of these that
⊢T ∀y(m < y →∃z < y DisprfT ( RT , z)).
As a theorem of Q we also have
⊢T ∀y(y < m ∨y = m ∨m < y).

226
INDEFINABILITY, UNDECIDABILITY, INCOMPLETENESS
It follows by pure logic that
⊢T ∀y(PrfT ( RT , y) →∃z < y Disprf( RT , z))
and hence ⊢T RT , and T is inconsistent, a contradiction that shows that RT cannot be
disprovable in T .
Atheory T iscalledω-inconsistentifandonlyifforsomeformula F(x),⊢T ∃x F(x)
but ⊢T ∼F(n) for every natural number n, and is called ω-consistent if and only if
it is not ω-inconsistent. Thus an ω-inconsistent theory ‘afﬁrms’ there is some num-
ber with the property expressed by F, but then ‘denies’ that zero is such a number,
that one is such a number, than two is such a number, and so on. Since ∃x F(x) and
∼F(0), ∼F(1), ∼F(2), . . . cannot all be correct, any ω-inconsistent theory must have
some incorrect theorems. But an ω-inconsistent theory need not be inconsistent. (An
example of a consistent but ω-inconsistent theory will be given shortly.)
17.9 Theorem. Let T be a consistent, axiomatizable extension of Q. Then a G¨odel
sentence for T is unprovable in T , and if T is ω-consistent, it is also undisprovable in T .
Proof: Suppose the G¨odel sentence GT is provable in T . Then the ∃-rudimentary
sentence ∃y PrfT ( GT , y) is correct, and so provable in T . But since GT is a G¨odel
sentence, GT ↔∼∃y PrfT ( GT , y) is also provable in T . By pure logic it follows
that ∼GT is provable in T , and T is inconsistent, a contradiction, which shows that
GT is not provable in T .
Suppose the sentence GT is disprovable in T . Then ∼∼∃y PrfT ( GT , y) and hence
∃y PrfT ( GT , y) is provable in T . But by consistency, GT is not provable in T , and so
for any n, n is not a witness to the provability of GT , and so the rudimentary sentence
∼PrfT ( GT , n) is correct and hence provable in Q and hence in T . But this means
T is ω-inconsistent, a contradiction, which shows that GT is not disprovable in T .
For an example of a consistent but ω-inconsistent theory, consider the theory
T = Q + ∼GQ consisting of all consequences of the axioms of Q together with ∼GQ
or ∼∼∃y PrfQ(GQ, y). Since GQ is not a theorem of Q, this theory T is consistent. Of
course ∃y PrfQ(GQ, y) is a theorem of T . But for any particular n, the rudimentary
sentence ∼PrfQ(GQ, n) is correct, and therefore provable in any extension of Q,
including T .
Historically, Theorem 17.9 came ﬁrst, and Theorem 17.8 was a subsequent re-
ﬁnement. Accordingly, the Rosser sentence is sometimes called the G¨odel–Rosser
sentence. Subsequently, many other examples of undecidable sentences have been
brought forward. Several interesting examples will be discussed in the following,
optional section, and the most important example in the next chapter.
17.3* Undecidable Sentences without the Diagonal Lemma
The diagonal lemma, which was used to construct the G¨odel and Rosser sentences,
is in some sense the cleverest idea in the proof of the ﬁrst incompleteness theorem,
and is heavily emphasized in popularized accounts. However, the possibility of im-
plementing the idea of this lemma, of constructing a sentence that says of itself that
it is unprovable, depends on the apparatus of the arithmetization of syntax and the

17.3*. UNDECIDABLE SENTENCES WITHOUT THE DIAGONAL LEMMA
227
representability of recursive functions. Once that apparatus is in place, a version of
the incompleteness theorem, showing the existence of a true but unprovable sentence,
can be established without the diagonal lemma. One way to do so is indicated in the
ﬁrst problem at the end of this chapter. (This way uses the fact that there exist semire-
cursive sets that are not recursive, and though it does not use the diagonal lemma,
does involve a diagonal argument, buried in the proof of the fact just cited.) Some
other ways will be indicated in the present section.
Towards describing one such way, recall the Epimenides or liar paradox, involving
the sentence ‘This sentence is untrue’. A contradiction arises when we ask whether
this sentence is true: it seems that it is if and only if it isn’t. The G¨odel sentence
in effect results from this paradoxical sentence on substituting ‘provable’ for ‘true’
(a substitution that is crucial for establishing that we can actually construct a G¨odel
sentence in the language of arithmetic). Now there are other semantic paradoxes,
paradoxes in the same family as the liar paradox, involving other semantic notions
related to truth. One famous one is the Grelling or heterological paradox. Call an
adjective autological if it is true of itself, as ‘short’ is short, ‘polysyllabic’ is poly-
syllabic, and ‘English’ is English, and call it heterological if it is untrue of itself, as
‘long’ is not long, ‘monosyllabic’ is not monosyllabic, and ‘French’ is not French. A
contradiction arises when we ask whether ‘heterological’ is heterological: it seems
that it is if and only if it isn’t.
Let us modify the deﬁnition of heterologicality by substituting ‘provable’ for
‘true’. We then get the notion of self-applicability: a number m is self-applicable
in Q if it is the G¨odel number of a formula μ(x) such that μ(m) is provable in Q.
Now the same apparatus that allowed us to construct the G¨odel sentence allows us
to construct what may be called the G¨odel–Grelling formula GG(x) expressing ‘x
is not self-applicable’. Let m be its G¨odel number. If m were self-applicable, then
GG(m) would be provable, hence true, and since what it expresses is that m is not
self-applicable, this is impossible. So m is not self-applicable, and hence GG(m) is
true but unprovable.
Another semantic paradox, Berry’s paradox, concerns the least integer not namable
in fewer than nineteen syllables. The paradox, of course, is that the integer in question
appears to have been named just now in eighteen syllables. This paradox, too, can be
adapted to give an example of a sentence undecidable in Q. Let us say that a number
n is denominable in Q by a formula φ(x) if ∀x(φ(x) ↔x = n) is (not just true but)
provable in Q.
Every number n is denominable in Q, since if worse comes to worst, it can always
be denominated by the formula x = n, a formula with n + 3 symbols. Some numbers
n are denominable in Q by formulas with far fewer than n symbols. For example, the
number 10 ⇑10 is denominable by the formula φ(10, 10, x), where φ is a formula
representing the super-exponential function ⇑. We have not actually written out this
formula, but instructions for doing so are implicit in the proof that all recursive func-
tions are representable, and review of that proof reveals that writing out the formula
would not take more time or more paper than an ordinary homework assignment. By
contrast, 10 ⇑10 is larger than the number of particles in the visible universe. But
while big numbers can thus be denominated by comparatively short formulas, for any
ﬁxed k, only ﬁnitely many numbers can be denominated by formulas with fewer than

228
INDEFINABILITY, UNDECIDABILITY, INCOMPLETENESS
k symbols. For logically equivalent formulas denominate the same number (if they
denominate any number at all), and every formula with fewer than k symbols is log-
ically equivalent, by relettering bound variables, to one with only the ﬁrst k variables
on our ofﬁcial list of variables, and there are only ﬁnitely many of those.
Thus, there will be numbers not denominable using fewer than 10 ⇑10 symbols.
The usual apparatus allows us to construct a G¨odel–Berry formula GB(x, y), ex-
pressing ‘x is the least number not denominable by a formula with fewer than y ⇑y
symbols’. Writing out this formula would involve writing out not just the formula
representing the super-exponential function ⇑, but also the formulas relating to prov-
ability in Q. Again we have not actually written out these formulas, but only given
an outline of how to do so in our proofs of the arithmetizability of syntax and the
representability of recursive functions in Q. Review of those proofs reveals that writ-
ing out the formula GB(x, y) or GB(x, 10), though it would require more time and
paper than any reasonable homework assignment, would not require more symbols
than appear in an ordinary encyclopedia, which is far fewer than the astronomical
ﬁgure 10 ⇑10. Now there is some number not denominable by a formula with fewer
symbols than that astronomical ﬁgure, and among such numbers there is one and
only one least, call it n. Then GB(n, 10) and ∀x(GB(x, 10) ↔x = n) are true. But
if the latter were provable, the formula GB(x, 10) would denominate n, whereas n is
not denominable except by formulas much longer than that. Hence we have another
example of an unprovable truth.
This example is worth pressing a little further. The length of the shortest formula
denominating a number may be taken as a measure of the complexity of that number.
Just as we could construct the G¨odel–Berry formula, we can construct a formula
C(x, y, z) expressing ‘the complexity of x is y and y is greater than z ⇑z’, and
using it the G¨odel–Chaitin formula GC(x) or ∃yC(x, y, 10), expressing that x has
complexity greater than 10 ⇑10. Now for all but ﬁnitely many n, GC(n) is true.
Chaitin’s theorem tells us that no sentence of form GC(n) is provable.
The reason may be sketched as follows. Just as ‘y is a witness to the provability
of x in Q’ can be expressed in the language of arithmetic by a formula PrfQ(x,y), so
can ‘y is a witness to the provability of the result of subsituting the numeral for x for
the variable in GC’ be expressed by a formula PrfGCQ(x,y). Now if any sentence of
form GC(n) can be proved, there is a least m such that m witnesses the provability of
GC(n) for some n. Let us call m the ‘lead witness’ for short. And of course, since any
one number witnesses the provability of at most one sentence, there will be a least
n—in fact, there will be one and only one n—such that the lead witness is a witness to
the provability of GC(n). Call n the number ‘identiﬁed by the lead witness’ for short.
If one is careful, one can arrange matters so that the sentences K(m) and L(n)
expressing ‘m is the lead witness’ and ‘n is the number identiﬁed by the lead witness’
will be ∃-rudimentary, so that, being true, K(m) and L(n) will be provable. Moreover,
since it can be proved in Q that there is at most one least number fulﬁlling the condition
expressed by any formula, ∀x(x ̸= m →∼K(x)) and ∀x(x ̸= n →∼L(x)) will also
be provable. But this means that n is denominated by the formula L(x), and hence
has complexity less than the number of symbols in that formula. And though it might
take an encyclopedia’s worth of paper and ink to write the formula down, the number

17.3*. UNDECIDABLE SENTENCES WITHOUT THE DIAGONAL LEMMA
229
of symbols in any encyclopedia remains far less than 10 ⇑10. So if n is denominated
by the formula L(x), its complexity is less than 10 ⇑10. Since this is impossible, it
follows that no sentence of form GC(n) can be proved: no speciﬁc number n can be
proved to have complexity greater than 10 ⇑10. This reasoning can be adapted to
any other reasonable measure of complexity.
(For example, suppose we take the complexity of a number to be the smallest
number of states needed for a Turing machine that will produce that number as output
given zero as input. To establish that ‘the complexity of x is y’ and related formulas
can be expressed in the language of arithmetic we now need the fact that Turing
machines can be coded by recursive functions in addition to the fact that recursive
functions are representable. And to show that if there is any proof that some number
has complexity greater than 10 ⇑10, then the number n identiﬁed by the lead witness
can be generated as the output for input zero by some Turing machine, we need in
addition to the arithmetizability of syntax the fact also of the Turing computability of
recursive functions. Almost the whole of this book up to this point is involved just in
outlining how one would go about writing down the relevant formula and designing
the relevant Turing machine. But while ﬁlling in the details of this outline might ﬁll
an encyclopedia, still it would not require anything approaching 10 ⇑10 symbols,
and that is all that is essential to the argument. In the literature, the label Chaitin’s
theorem refers especially to this Turing-machine version, but as we have said, similar
reasoning applies to any reasonable notion of complexity.)
Thus on any reasonable measure of complexity, there is an upper bound b—we
have used 10 ⇑10, though a closer analysis would show that a much smaller number
would do, its exact value depending on the particular measure of complexity being
used—such that no speciﬁc number n can be proved in Q to have complexity greater
than b. Moreover, this applies not just to Q but to any stronger true theory, such as
P or the theories developed in works on set theory that are adequate for formalizing
essentially all ordinary mathematical proofs. Thus Chaitin’s theorem, whose proof we
have sketched, tells us that there is an upper bound such that no speciﬁc number can be
proved by ordinary mathematical means to have complexity greater than that bound.
Problems
17.1 Show that the existence of a semirecursive set that is not recursive implies
that any consistent, axiomatizable extension of Q fails to prove some correct
∀-rudimentary sentence.
17.2 Let T be a consistent, axiomatizable theory extending Q. Consider the set Pyes
of (code numbers of) formulas that are provable in T , and the set Pno of (code
numbers of) formulas that are disprovable in P. Show that there is no recursive
set R such that Pyes is a subset of R while no element of R is an element of Pno.
17.3 Let B1(y) and B2(y) be two formulas of the language of arithmetic. General-
izing the diagonal lemma, show that there are sentences G1 and G2 such that
⊢Q G1 ↔B2( G2 )
⊢Q G2 ↔B1( G1 ).

230
INDEFINABILITY, UNDECIDABILITY, INCOMPLETENESS
For instance, there are a pair of sentences such that the ﬁrst says the second is
provable, while the second says the ﬁrst is unprovable.
The set of (code numbers of) sentences of the language of arithmetic {<, 0, ′,
+, ·} that are correct, or true in the standard interpretation, is not recursive.
Actually, it can be shown that the set of (code numbers of) sentences of the
language {+, ·} that are true in the standard interpretation is not recursive.
The next few problems are pieces of the proof.
17.4 Explain why, to establish the stronger result just stated, it would sufﬁce to as-
sociate in a recursive way to every sentence A of the language {<, 0, ′, +, ·}
a sentence A† of the language {+, ·} such that A is correct if and only if A† is
correct.
17.5 Continuing the preceding problem, show that there is a formula D0(x) of the
language {+, ·} such that the following is correct: ∀x(x = 0 ↔D0(x)). Then
explain how to associate in an effective (and therefore, assuming Church’s
thesis, a recursive) way to every sentence A of the language {<, 0, ′, +, ·}
a sentence A† of the language {<, ′, +, ·} such that A is correct if and only
if A† is correct.
17.6 Continuing the preceding series of problems, exhibit a formula Ds(x, y) of the
language {+, ·} such that
∀x∀y(x′ = y ↔Ds(x, y))
is correct, and a formula D< (x, y) of the language {+, ·} such that
∀x∀y(x < y ↔D< (x, y))
is correct. Then show how, say, the statements in Example 16.7 can be naturally
expressed in the language {+, ·}.
17.7 Let T = Q, let R be the Rosser sentence of T , let T0 be T + {R}, the set of
consequences of T ∪{R}, and let T1 = T + {∼R}; then {T0, T1} is a set of two
consistent, axiomatizable extensions of Q that are inconsistent with each other
in the sense that their union is inconsistent. Show that for every n there is a set
of 2n consistent, axiomatizable extensions of Q that are pairwise inconsistent
in the sense that any two of them are inconsistent with each other.
17.8 Show that there is a nonenumerable set of consistent extensions of Q that are
pairwise inconsistent.
17.9 Let L1 and L2 be ﬁnite or recursive languages, and T a theory in L2. A trans-
lation of L1 into T is an assignment to each sentence S of L1 of a sentence S†
of L2 such that:
(T0) (∼A)† is logically equivalent to ∼(A)†.
(T1) The function taking the code number of a sentence of L1 to the code
number of its translation is recursive.
(T2) Whenever A1, . . . , Ak, B are sentences of L1 and B is a consequence
of A1, . . . , Ak, then B† is a consequence of T ∪{A†
1, . . . , A†
k}.
Show that if T is a consistent, axiomatizable theory in a language L, and if there
is a translation of the language of arithmetic into T such that the translation of

PROBLEMS
231
every axiom of Q is a theorem of T , then the set of sentences of the language of
arithmetic whose translations are theorems of T is a consistent, axiomatizable
extension of Q.
17.10 Show that under the hypotheses of the preceding problem, T is incomplete
and undecidable.
17.11 Let L be a language, N(u) a formula of L. For any sentence F of L, let the
relativization FN be the result of replacing each universal quantiﬁer ∀x in F
by ∀x(N(x) →· · ·) and each existential quantiﬁer ∃x by ∃x(N(x) & . . .). Let
T be a theory in L such that for every name c, N(c) is a theorem of T and for
every function symbol f the following is a theorem of T :
∀x1 . . . ∀xk((N(x1) & . . . & N(xk)) →N( f (x1, . . . , xk))).
Show that for any model M of T , the set of a in |M| that satisﬁes N(x) is the
domain of an interpretation N such that any sentence S of L is true in N if
and only if its relativization SN is true in M.
17.12 Continuing the preceding series of problem, show that the function assigning
each sentence S of L its relativization SN is a translation. (You may appeal to
Church’s thesis.)
17.13 Consider the interpretation Z of the language {<, 0, ′, +, ·} in which the
domain is the set of all integers (including the negative ones), and the deno-
tation of 0 is zero, of ′ is the function that adds one to a number, of + and ·
are the usual addition and multiplication functions, and of < is the usual order
relation. Show that the set of all sentences that are true in Z is undecidable,
and that this is still so if < is dropped.

18
The Unprovability of Consistency
According to G¨odel’s second incompleteness theorem, the sentence expressing that a
theory like P is consistent is undecidable by P, supposing P is consistent. The full proof
of this result is beyond the scope of a book on the level of the present one, but the overall
structure of the proof and main ingredients that go into the proof will be indicated in
this short chapter. In place of problems there are some historical notes at the end.
Ofﬁcially we deﬁned T to be inconsistent if every sentence is provable from T ,
though we know this is equivalent to various other conditions, notably that for some
sentence S, both S and ∼S are provable from T . If T is an extension of Q, then since
0 ̸= 1 is the simplest instance of the ﬁrst axiom of Q, 0 ̸= 1 is provable from T , and
if 0 = 1 is also provable from T , then T is inconsistent; while if T is inconsistent,
then 0 = 1 is provable from T , since every sentence is. Thus T is consistent if
and only if 0 = 1 is not provable from T . We call ∼PrvT ( 0 = 1 ), which is to
say ∼∃y PrfT ( 0 = 1 , y), the consistency sentence for T . Historically, the original
paper of G¨odel containing his original version of the ﬁrst incompleteness theorem
(corresponding to our Theorem 17.9) included towards the end a statement of a
version of the following theorem.
18.1 Theorem* (G¨odel’s second incompleteness theorem, concrete form). Let T be
a consistent, axiomatizable extension of P. Then the consistency sentence for T is not
provable in T .
We have starred this theorem because we are not going to give a full proof of it. In
gross outline, G¨odel’s idea for the proof of this theorem was as follows. The proof of
Theorem 17.9 shows that if the absurdity 0 = 1 is not provable in T then the G¨odel
sentence GT is not provable in T either, so the following is true: ∼PrvT ( 0 = 1 ) →
∼PrvT ( GT ). Now it turns out that the theory P of inductive arithmetic, and hence
any extension T thereof, is strong enough to ‘formalize’ the proof of Theorem 17.9,
so we have
⊢T ∼PrvT ( 0 = 1 ) →∼PrvT ( GT ).
But GT was a G¨odel sentence, so we have also
⊢T GT ↔∼PrvT ( GT ).
232

THE UNPROVABILITY OF CONSISTENCY
233
And so we have
⊢T ∼PrvT ( 0 = 1 ) →GT .
So if we had ⊢T ∼PrvT ( 0 = 1 ), then we would have ⊢T GT , which by Proposition
17.9 we do not.
Of course, the key step here, of which we have not given and are not going to
be giving the proof, is the claim that a theory like P is strong enough to ‘formalize’
the proof of a result like Theorem 17.9. G¨odel’s successors, beginning with Paul
Bernays, have analyzed just what properties of PrvT are actually essential to get the
second incompleteness theorem, ﬁnding that one does not really have to ‘formalize’
the whole proof of Theorem 17.9, but only certain key facts that serve as lemmas in
that proof. We summarize the results of the analysis in the next two propositions.
18.2 Lemma*. Let T be a consistent, axiomatizable extension of P, and let B(x) be
the formula PrvT (x). Then the following hold for all sentences:
If ⊢T A then ⊢T B( A )
(P1)
⊢T B( A1 →A2 ) →(B( A1 ) →B( A2 ))
(P2)
⊢T B( A ) →B( B( A ) ).
(P3)
Again we have starred the lemma because we are not going to give a full proof.
First we note a property not on the above list:
If ⊢T A1 →A2 and ⊢T A1, then ⊢T A2.
(P0)
This is a consequence of the G¨odel completeness theorem, according to which the
theorems of T are just the sentences implied by T , since if a conditional A1 →A2
and its antecedent A1 are both implied by a set of sentences, then so is its consequent
A2. Whatever notion of proof one starts with, so long as it is sound and complete, (P0)
will hold. One might therefore just as well build it into one’s notion of proof, adding
some appropriate version of it to the rules of one’s proof procedure. Of course, once it
is thus built in, the proof of (P0) no longer requires the completeness theorem, but be-
comes comparatively easy. [For the particular proof procedure we used in Chapter 14,
we discussed the possibility of doing this in section 14.3, where the version of (P0)
appropriate to our particular proof procedure was called rule (R10).]
(P1) holds for any extension of Q, since if ⊢T A, then PrvT ( A ) is correct, and
being an ∃-rudimentary sentence, it is therefore provable in Q. (P2) is essentially the
assertion that the proof of (P0) (which we have just said can be made comparatively
easy) can be ‘formalized’ in P. (P3) is essentially the assertion that the (by no means
so easy) proof of (P1) can also be ‘formalized’ in P. The proofs of the assertions
(P2) and (P3) of ‘formalizability’ are omitted from virtually all books on the level
of this one, not because they involve any terribly difﬁcult new ideas, but because the
innumerable routine veriﬁcations they—and especially the latter of them—require
would take up too much time and patience. What we can and do include is the proof
that the starred lemma implies the starred theorem. More generally, we have the
following:

234
THE UNPROVABILITY OF CONSISTENCY
18.3 Theorem (G¨odel’s second incompleteness theorem, abstract form). Let T be a
consistent, axiomatizable extension of P, and let B(x) be a formula having properties
(P1)–(P3) above. Then not ⊢T ∼B( 0 = 1 ).
The proof will occupy the remainder of this chapter. Throughout, let T be an
extension (not necessarily consistent) of Q. A formula B(x) with properties (P1)–
(P3) of Lemma 18.2 we call a provability predicate for T . We begin with a few
words about this notion. The formula PrvT (x) considered so far we call the traditional
provability predicate for T , though, as we have indicated, we are not going to give the
proof of Lemma 18.2, and so are not going to be giving the proof that the ‘traditional
provability predicate’ is a ‘provability predicate’ in the sense of our ofﬁcial deﬁnition
of the latter term.
If T is ω-consistent, taking the traditional PrvT (x) for B(x), we have also the
following property, the converse of (P1):
If ⊢T B( A ) then ⊢T A.
(P4)
[For if we had ⊢T PrvT ( A ), or in other words ⊢T ∃y PrfT ( A , y), but did not have
⊢T A, then for each b, ∼PrfT ( A , b) would be correct and hence provable in Q and
hence in T , and we would have an ω-inconsistency in T .] We do not, however, include
ω-consistency in our assumptions on T , or (P4) in our deﬁnition of the technical
term ‘provability predicate’. Without the assumption of (P4), which is not part of our
ofﬁcial deﬁnition, a ‘provability predicate’ need not have much to do with provability.
In fact, the formula x = x is easily seen to be a ‘provability predicate’ in the sense
of our deﬁnition.
On the other hand, a formula may arithmetically deﬁne the set of G¨odel numbers
of theorems of T without being a provability predicate for T . If T is consistent and
PrvT (x) is the traditional provability predicate for T , then not only does PrvT (x)
arithmetically deﬁne the set of G¨odel numbers of theorems of T , but so does the
formulaPrv*T (x),whichistheconjunctionofPrvT (x)with∼PrvT ( 0 = 1 ),sincethe
second conjunct is true. But notice that, in contrast to Theorem 18.1, ∼Prv*T ( 0 = 1 )
is provable in T . For it is simply
∼(PrvT ( 0 = 1 ) & ∼PrvT ( 0 = 1 ))
which is a valid sentence and hence a theorem of any theory. The formula Prv*T (x),
however, lacks property (P1) in the deﬁnition of provability predicate. That is, it
is not the case that if ⊢T A then ⊢T Prv*T ( A ). Indeed, it is never the case that
⊢T Prv*T ( A ), since it is not the case that ⊢T ∼PrvT ( 0 = 1 ), by Theorem 18.1.
The traditional provability predicate PrvT (x) has the further important, if nonmath-
ematical, property beyond (P0)–(P4), that intuitively speaking Prv(x) can plausibly
be regarded as meaning or saying (on the standard interpretation) that x is the G¨odel
number of a sentence that is provable in T . This is conspicuously not the case for
Prv*T (x), which means or says that x the G¨odel number of a sentence that is provable
in T and T is consistent.
The thought that whatever is provable had better be true might make it surprising
that a further condition was not included in the deﬁnition of provability predicate,

THE UNPROVABILITY OF CONSISTENCY
235
namely, that for every sentence A we have
⊢T B( A ) →A.
(P5)
But in fact, as we also show below, no provability predicate fulﬁlls condition (P5)
unless T is inconsistent.
Our next theorem will provide answers to three questions. First, just as the diagonal
lemma provides a sentence, the G¨odel sentence, that ‘says of itself’ that it is unprov-
able, so also it provides a sentence, the Henkin sentence, that ‘says of itself’ that it
is provable. In other words, given a provability predicate B(x), there is a sentence
HT such that ⊢T HT ↔B( HT ). G¨odel’s theorem was that, if T is consistent, then
the G¨odel sentence is indeed unprovable. Henkin’s question was whether the Henkin
sentence is indeed provable. This is the ﬁrst question our next theorem will answer.
Second, call a formula Tr(x) a truth predicate for T if and only if for every sentence
A of the language of T we have ⊢T A ↔Tr ( A ). Another question is whether, if T
is consistent, there can exist a truth predicate for T . (The answer to this question is
going to be negative. Indeed, the negative answer can actually be obtained directly
from the diagonal lemma of the preceding chapter.) Third, if B(x) is a provability
predicate, call ∼B( 0 = 1 ) the consistency sentence for T [relative to B(x)]. Yet
another question is whether, if T is consistent, the consistency sentence for T can be
provable in T . (We have already indicated in Theorem 18.3 that the answer to this
last question is going to be negative.)
The proof of the next theorem, though elementary, is somewhat convoluted, and
as warm-up we invite the reader to ponder the following paradoxical argument, by
which we seem to be able to prove from pure logic, with no special assumptions, the
existence of Santa Claus. (The argument would work equally well for Zeus.) Consider
the sentence ‘if this sentence is true, then Santa Claus exists’; or to put the matter
another way, let S be the sentence ‘if S is true, then Santa Claus exists’.
Assuming
S is true
(1)
by the logic of identity it follows that
‘If S is true, then Santa Claus exists’ is true.
(2)
From (2) we obtain
If S is true, then Santa Claus exists.
(3)
From (1) and (3) we obtain
Santa Claus exists.
(4)
Having derived (4) from the assumption (1) we infer that without the assumption (1),
indeed without any special assumption, that we at least have the conditional conclu-
sion that if (1), then (4), or in other words
If S is true, then Santa Claus exists.
(5)

236
THE UNPROVABILITY OF CONSISTENCY
From (5) we obtain
‘If S is true, then Santa Claus exists’ is true.
(6)
By the logic of identity again it follows that
S is true.
(7)
And from (5) and (7) we infer, without any special assumptions, the conclusion that
Santa Claus exists.
(8)
18.4 Theorem (L¨ob’s theorem). If B(x) is a provability predicate for T , then for any
sentence A, if ⊢T B( A ) →A, then ⊢T A.
Proof: Suppose that B is a provability predicate for T and that
⊢T B( A ) →A.
(1)
Let D(y) be the formula (B(y) →A), and apply the diagonal lemma to obtain a
sentence C such that
⊢T C ↔(B( C ) →A).
(2)
So
⊢T C →(B( C ) →A).
(3)
By virtue of property (P1) of a provability predicate,
⊢T B( C →(B( C ) →A)).
(4)
By virture of (P2),
⊢T B( C →(B( C ) →A) ) →(B( C ) →B( B( C ) →A )).
(5)
From (4) and (5) it follows that
⊢T B( C ) →B( B( C ) →A ).
(6)
By virtue of (P2) again,
⊢T B( B( C ) →A ) →(B( B( C ) ) →B( A )).
(7)
From (6) and (7) it follows that
⊢T B( C ) →(B( B( C ) ) →B( A )).
(8)
By virtue of (P3),
⊢T B( C ) →B( B( C ) ).
(9)
From (8) and (9) it follows that
⊢T B( C ) →B( A ).
(10)
From (1) and (10) it follows that
⊢T B( C ) →A.
(11)

HISTORICAL REMARKS
237
From (2) and (11) it follows that
⊢T C.
(12)
By virtue of (P1) again,
⊢T B( C ).
(13)
And so ﬁnally, from (11) and (13), we have
⊢T A.
(14)
Since the converse of L¨ob’s theorem is trivial (if ⊢T A, then ⊢T F →A for any
sentence F), a necessary and sufﬁcient condition for A to be a theorem of T is that
B( A ) →A is a theorem of T . Now for the promised derivation of the three results
mentioned earlier.
18.5 Corollary. Suppose that B(x) is a provability predicate for T . Then if ⊢T H ↔
B( H ), then ⊢T H.
Proof: Immediate from L¨ob’s theorem.
18.6 Corollary. If T is consistent, then T has no truth predicate.
Proof: Suppose that Tr(x) is a truth predicate for T . Then a moment’s thought
shows that Tr(x) is also a provability predicate for T . Moreover, since Tr(x) is a truth
predicate, for every A we have ⊢T Tr( A ) →A. But then by L¨ob’s theorem, for every
A we have ⊢T A, and T is inconsistent.
And ﬁnally, here is the proof of Theorem 18.3.
Proof: Suppose ⊢T ∼B( 0 = 1 ). Then ⊢T B( 0 = 1 ) →F for any sentence F,
and in particular ⊢T B( 0 = 1 ) →0 = 1, and hence ⊢T 0 = 1, and since T is an
extension of Q, T is inconsistent.
It is characteristic of important theorems to raise new questions even as they
answer old ones. G¨odel’s theorems (as well as some of the major recursion-theoretic
and model-theoretic results we have passed on our way to G¨odel’s theorems) are
a case in point. Several of the new directions of research they opened up will be
explored in the remaining chapters of this book. One such question is that of how
far one can go working just with the abstract properties (P1)–(P3), without getting
involved in the messy details about a particular predicate PrvT (x). That question will
be explored in the last chapter of this book.
Historical Remarks
We alluded in passing in an earlier chapter to the existence of heterodox mathemati-
cians who reject certain principles of logic. More speciﬁcally, in the late nineteenth
and early twentieth centuries there were a number of mathematicians who rejected
‘nonconstructive’ as opposed to ‘constructive’ existence proofs and were led by this

238
THE UNPROVABILITY OF CONSISTENCY
rejection to reject the method of proof by contradiction, which has been ubiqui-
tously used in orthodox mathematics since Euclid (and has been repeatedly used
in this book). The most extreme critics, the ‘ﬁnitists’, rejected the whole of estab-
lished ‘inﬁnitistic’ mathematics, declaring not only that the proofs of its theorems
were fallacious, but that the very statements of those theorems were meaningless.
Any mathematical assertion going beyond generalizations whose every instance can
be checked by direct computation (essentially, anything beyond ∀-rudimentary sen-
tences) was rejected.
In the 1920s, David Hilbert, the leading mathematician of the period, devised a
program he hoped would provide a decisive answer to these critics. On the plane of
philosophical principle, he in effect conceded that sentences going beyond
∀-rudimentary sentences are ‘ideal’ additions to ‘contentful’ mathematics. He com-
pared this addition to the addition of ‘imaginary’ numbers to the system of real
numbers, which had also raised doubts and objections when it was ﬁrst introduced.
On the plane of mathematical practice, Hilbert insisted, a detour through the ‘ideal’ is
often the shortest route to a ‘contentful’ result. (For example, Chebyshev’s theorem
that there is a prime between any number and its double was proved not in some
‘ﬁnitistic’, ‘constructive’, directly computational way, but by an argument involving
applying calculus to functions whose arguments and values are imaginary numbers.)
Needless to say, this reply wouldn’t satisfy a critic who doubted the correctness of
‘contentful’ results arrived at by such a detour. But Hilbert’s program was precisely
to prove that any ‘contentful’ result provable by orthodox, inﬁnitistic mathematics is
indeed correct. Needless to say, such a proof wouldn’t satisfy a critic if the proof itself
used the methods whose legitimacy was under debate. But more precisely Hilbert’s
program was to prove by ‘ﬁnitistic’ means that every ∀-rudimentary sentence proved
by ‘inﬁnitistic’ means is correct.
An important reduction of the problem was achieved. Suppose a mathematical
theory T proves some incorrect ∀-rudimentary sentence ∀xF(x). If this sentence is
incorrect, then some speciﬁc numerical instance F(n) for some speciﬁc number
n must be incorrect. Of course, if the theory proves ∀xF(x) it also proves each
instance F(n), since the instances follow from the generalization by pure logic.
But if F(n) is incorrect, then ∼F(n) is a correct rudimentary sentence, and as such
will be provable in T , for any ‘sufﬁciently strong’ T . Hence if such a T proves an
∀-rudimentary sentence ∀xF(x), it will prove an outright contradiction, proving both
F(n) and ∼F(n). So the problem of proving T yields only correct ∀-rudimentary
theorems reduces to the problem of showing T is consistent. Hilbert’s program was,
then, to prove ﬁnitistically the consistency of inﬁnitistic mathematics.
It can now be appreciated how G¨odel’s theorems derailed this program in its origi-
nal form just described. While it was never made completely explicit what ‘ﬁnitistic’
mathematics does and does not allow, its assumptions amounted to less than the as-
sumptions of inductive or Peano arithmetic P. On the other hand, the assumptions of
‘inﬁnitistic’ mathematics amount to more than the assumptions of P. So what Hilbert
was trying to do was prove, using a theory weaker than P, the consistency of a theory
stronger than P, whereas what G¨odel proved was that, even using the full strength of
P, one cannot prove the consistency of P itself, let alone anything stronger.

HISTORICAL REMARKS
239
In the course of this essentially philosophically motivated work, G¨odel introduced
the notion of primitive recursive function, and established the arithmetization of syn-
tax by primitive recursive functions and the representability in formal arithmetic of
primitive recursive functions. But though primitive recursive functions were thus orig-
inally introduced merely as a tool for the proof of the incompleteness theorems, it was
not long before logicians, G¨odel himself included, began to wonder how far beyond
the class of primitive recursive functions one had to go before one arrived at a class
of functions that could plausibly be supposed to include all effectively computable
functions. Alonzo Church was the ﬁrst to publish a deﬁnite proposal. A. M. Turing’s
proposal, involving his idealized machines, followed shortly thereafter, and with it
the proof of the existence of a universal machine, another intellectual landmark of
the last century almost on the level of the incompleteness theorems themselves.
G¨odel and others went on to show that various other mathematically interesting
statements, besides the consistency statement, are undecidable by P, assuming it to
be consistent, and even by stronger theories, such as are introduced in works on set
theory. In particular, G¨odel and Paul Cohen showed that the accepted formal set theory
of their day and ours could not decide an old conjecture of Georg Cantor, the creator of
the theory of enumerable and nonenumberable sets, which Hilbert in 1900 had placed
ﬁrst on a list of problems for the coming century. The conjecture, called the continuum
hypothesis, was that any nonenumerable set of real numbers is equinumerous with
the whole set of real numbers. Mathematicians would be, according to the results of
G¨odel and Cohen, wasting their time attempting to settle this conjecture on the basis
of currently accepted set-theoretic axioms, in the same way people who try to trisect
the angle or square the circle are wasting their time. They must either ﬁnd some way
to justify adopting new set-theoretic axioms, or else give up on the problem. (Which
they should do is a philosophical question, and like other philosophical questions,
it has been very differently answered by different thinkers. G¨odel and Cohen, in
particular, arrayed themselves on opposite sides of the question: G¨odel favored the
search for new axioms, while Cohen was for giving up.)


Further Topics


19
Normal Forms
A normal form theorem of the most basic type tells us that for every formula A there is
a formula A* of some special syntactic form such that A and A* are logically equiva-
lent. A normal form theorem for satisﬁability tells us that for every set  of sentences
there is a set * of sentences of some special syntactic form such that  and * are
equivalent for satisﬁability, meaning that one will be satisﬁable if and only if the other
is. In section 19.1 we establish the prenex normal form theorem, according to which
every formula is logically equivalent to one with all quantiﬁers at the beginning, along
with some related results. In section 19.2 we establish the Skolem normal form the-
orem, according to which every set of sentences is equivalent for satisﬁability to a
set of sentences with all quantiﬁers at the beginning and all quantiﬁers universal. We
then use this result to give an alternative proof of the L¨owenheim–Skolem theorem,
which we follow with some remarks on implications of the theorem that have sometimes
been thought ‘paradoxical’. In the optional section 19.3 we go on to sketch alternative
proofs of the compactness and G¨odel completeness theorems, using the Skolem normal
form theorem and an auxiliary result known as Herbrand’s theorem. In section 19.4
we establish that every set of sentences is equivalent for satisﬁability to a set of sen-
tences not containing identity, constants, or function symbols. Section 19.1 presupposes
only Chapters 9 and 10, while the rest of the chapter presupposes also Chapter 12.
Section 19.2 (with its pendant 19.3) on the one hand, and section 19.4 on the other
hand, are independent of each other. The results of section 19.4 will be used in the next
two chapters.
19.1 Disjunctive and Prenex Normal Forms
This chapter picks up where the problems at the end of Chapter 10 left off. There we
asked the reader to show that that every formula is logically equivalent to a formula
having no subformulas in which the same variable occurs both free and bound. This
result is a simple example of a normal form theorem, a result asserting that every
sentence is logically equivalent to one fulﬁlling some special syntactic requirement.
Ourﬁrstresulthereisanalmostequallysimpleexample.Wesayaformulaisnegation-
normal if it is built up from atomic and negated atomic formulas using ∨, & , ∃, and
∀alone, without further use of ∼.
19.1 Proposition (Negation-normal form). Every formula is logically equivalent to one
that is negation-normal.
243

244
NORMAL FORMS
Proof: The proof is by induction on complexity. The base step is trivial, since an
atomic formula is already negation-normal. Most cases of the induction step are trivial
as well. For instance, if A and B are equivalent respectively to negation-normal for-
mulas A* and B*, then A & B and A ∨B are equivalent respectively to A* & B* and
A* ∨B*, which are also negation-normal. The nontrivial case is to prove that if A is
equivalent to the negation-normal A* then ∼A is equivalent to some negation-normal
A†. This divides into six subcases according to the form of A*. The case where A* is
atomic is trivial, since we may simply let A† be ∼A*. In case A* is of form ∼B, so
that ∼A* is ∼∼B, we may let A† be B. In case A* is of form (B ∨C), so that ∼A* is
∼(B ∨C), which is logically equivalent to (∼B & ∼C), by the induction hypothesis
the simpler formulas ∼B and ∼C are equivalent to formulas B† and C† of the required
form, so we may let A† be (B† & C†). The case of conjunction is similar. In case A*
is of form ∃x B, so that ∼A* is ∼∃x B, which is logically equivalent to ∀x∼B, by the
induction hypothesis the simpler formula ∼B is equivalent to a formula B† of the re-
quired form, so we may let A† be ∀x B†. The case of universal quantiﬁcation is similar.
In the foregoing proof we have used such equivalences as that of ∼(B ∨C) to
∼B & ∼C, to show ‘from the bottom up’ that there exists a negation-normal equiv-
alent for any formula. What we show at the induction step is that if there exist
negation-normal equivalents for the simpler formulas ∼B and ∼C, then there ex-
ists a negation-normal equivalent for the more complex formula ∼(B ∨C). If we
actually want to ﬁnd a negation-normal equivalent for a given formula, we use the
same equivalences, but work ‘from the top down’. We reduce the problem of ﬁnding
a negation-normal equivalent for the more complex formula to that of ﬁnding such
equivalents for simpler formulas. Thus, for instance, if P, Q, and R are atomic, then
∼(P ∨(∼Q & R))
can be successively converted to
∼P & ∼(∼Q & R)
∼P & (∼∼Q ∨∼R)
∼P & (Q ∨∼R)
the last of which is negation-normal. In this process use such equivalences as that
of ∼(B ∨C) to ∼B & ∼C to ‘bring junctions out’ or ‘push negations in’ until we
get a formula equivalent to the original in which negation is applies only to atomic
subformulas.
The above result on negation-normal form can be elaborated in two different
directions. Let A1, A2, . . . , An be any formulas. A formula built up from them using
only ∼, ∨, and &, without quantiﬁers, is said to be a truth-functional compound of
the given formulas. A truth-functional compound is said to be in disjunctive normal
form if it is a disjunction of conjunctions of formulas from among the Ai and their
negations. (A notion of conjunctive normal form can be deﬁned exactly analogously.)
19.2 Proposition (Disjunctive normal form). Every formula is logically equivalent to
one that is in disjunctive normal form.

19.1. DISJUNCTIVE AND PRENEX NORMAL FORMS
245
Proof: Given any formula, ﬁrst replace it by a negation-normal equivalent. Then,
using the distributive laws, that is, the equivalence of (B & (C ∨D)) to ((B & C) ∨
(B & D)) and of ((B ∨C) & D) to ((B ∨D) & (C ∨D)), ‘push conjunction inside’
and ‘pull disjunction outside’ until a disjunctive normal equivalent is obtained.
(It would be a tedious but routine task to rewrite this ‘top down’ description of the
process of ﬁnding a disjunctive normal equivalent as a ‘bottom up’ proof the existence
of such an equivalent.)
If in a formula that is in disjunctive normal form each disjunction contains each
Ai exactly once, plain or negated, then the compound is said to be in full disjunctive
normal form. (A notion of full conjunctive normal form can be deﬁned exactly anal-
ogously.) In connection with such forms it is often useful to introduce, in addition to
the two-place connectives ∨and & , and the one-place connective ∼, the zero-place
connectives or constant truth ⊤and constant falsehood ⊥, counting respectively as
true in every interpretation and false in every interpretation. The disjunction of zero
disjuncts may by convention be understood to be ⊥, and the conjunction of zero con-
juncts to be ⊤(rather as, in mathematics, the sum of zero summands is understood
to be 0, and the product of zero factors to be 1).
In seeking a full disjunctive normal equivalent of a given disjunctive normal for-
mula, ﬁrst note that conjunctions (and analogously, disjunctions) can be reordered
and regrouped at will using the commutative and associative laws, that is, the equiva-
lence of (B & C) to (C & B), and of (B & C & D), which ofﬁcially is supposed to be
an abbreviation of (B & (C & D)), with grouping to the right, to ((B & C) & D), with
grouping to the left. Thus for instance (P & (Q & P)) is equivalent to (P & (P & Q))
and to ((P & P) & Q). Using the idempotent law, that is, the equivalence of B & B
to B, this last is equivalent to P & Q. This illustrates how repetitions of the same Ai
(or ∼Ai) within a conjunction can be eliminated. To eliminate the occurrence of the
same Ai twice, once plain and once negated, we can use the equivalence of B & ∼B to
⊥and of ⊥& C to ⊥, and of ⊥∨D to D, so that, for instance, (B & ∼B & C) ∨D is
equivalent simply to D: contradictory disjuncts can be dropped. These reductions will
convert a given formula to one that, like our earlier example (∼P & Q) ∨(∼P & R),
is a disjunction of conjunctions in which each basic formula occurs at most once,
plain or negated, in each conjunct.
To ensure that each occurs at least once in each conjunction, we use the equivalence
of B to (B & C) ∨(B & ∼C). Thus our example is equivalent to
(∼P & Q & R) ∨(∼P & Q & ∼R) ∨(∼P & ∼R)
and to
(∼P & Q & R) ∨(∼P & Q & ∼R) ∨(∼P&Q & ∼R) ∨(∼P & ∼Q & ∼R)
and, eliminating repetition, to
(∼P & Q & R) ∨(∼P & Q & ∼R) ∨(∼P & ∼Q & ∼R)
which is in full disjunctive normal form. The foregoing informal description can be
converted into a formal proof of the following result.

246
NORMAL FORMS
19.3 Theorem (Full disjunctive normal form). Every truth-functional compound of
given formulas is logically equivalent to one in full disjunctive normal form.
The theorem on negation-normal forms can be elaborated in another direction. A
formula A is said to be in prenex form if it is of the form
Q1x1Q2x2 . . . Qnxn B
where each Q is either ∃or ∀, and where B contains no quantiﬁers. The sequence of
quantiﬁers and variables at the beginning is called the preﬁx, and the quantiﬁer-free
formula that follows the matrix.
19.4 Example (Finding a prenex equivalent for a given formula). Consider (∀x Fx ↔Ga),
where F and G are one-place predicates. This is ofﬁcially an abbreviation for
(∼∀x Fx ∨Ga) & (∼Ga ∨∀x Fx).
Let us ﬁrst put this in negation-normal form
(∃x∼Fx ∨Ga) & (∼Ga ∨∀x Fx).
The problem now is to ‘push junctions in’. This may be done by noting that the displayed
negation-normal form is equivalent successively to
∃x(∼Fx ∨Ga) & (∼Ga ∨∀x Fx)
∃x(∼Fx ∨Ga) & ∀x(∼Ga ∨Fx)
∃y(∼Fy ∨Ga) & ∀x(∼Ga ∨Fx)
∀x(∃y(∼Fy ∨Ga) & (∼Ga ∨Fx))
∀x∃y((∼Fy ∨Ga) & (∼Ga ∨Fx)).
If we had ‘pulled quantiﬁers out’ in a different order, a different prenex equivalent would
have been obtained.
19.5 Theorem (Prenex normal form). Every formula is logically equivalent to one in
prenex normal form.
Proof: By induction on complexity. Atomic formulas are trivially prenex. The re-
sult of applying a quantiﬁer to a prenex formula is prenex (and hence the result of
applying a quantiﬁer to a formula equivalent to a prenex formula is equivalent to a
prenex formula). The equivalence of the negation of a prenex formula (or a formula
equivalent to one) to a prenex formula follows by repeated application of the equiva-
lence of ∼∀x and ∼∃x to ∃x∼and ∀x∼, respectively. The equivalence of a conjunc-
tion (or disjunction) of prenex formulas to a prenex formula follows on ﬁrst relettering
bound variables as in Problem 10.13, so the conjuncts or disjuncts have no variables
in common, and then repeatedly applying the equivalence of QxA(x) § B, where x
does not occur in B, to Qx(A(x) § B), where Q may be ∀or ∃and § may be & or ∨.
In the remainder of this chapter our concern is less with ﬁnding a logical equivalent
of a special kind for a given sentence or formula than with ﬁnding equivalents for
satisﬁability of a special kind for a given sentence or set of sentences. Two sets of
sentences  and * are equivalent for satisﬁability if and only if they are either

19.2. SKOLEM NORMAL FORM
247
both satisﬁable or both unsatisﬁable, though generally when we prove the existence
of such equivalents our proof will actually provide some additional information,
indicating a stronger relationship between the two sets. Two different results on the
existence of equivalents for satisﬁability will be established in sections 19.2 and
19.4. In each case,  will be shown to have an equivalent for satisﬁability * whose
sentenceswillbeofasimplertypesyntactically,butwhichwillinvolvenewnonlogical
symbols.
In this connection some terminology will be useful. Let L be any language, and L+
any language containing it. Let M be an interpretation of L, and M+ an interpretation
of L+. If the interpretations have the same domain and assign the same denotations to
nonlogical symbols in L (so that the only difference is that the one assigns denotations
to symbols of L+ not in L, while the other does not), then M+ is said to be an
expansion of M to L+, and M to be the reduct of M+ to L. Note that the notions
of expansion and reduct pertain to changing the language while keeping the domain
ﬁxed.
19.2 Skolem Normal Form
A formula in prenex form with all quantiﬁes universal (respectively, existential) may
be called a universal or ∀-formula (respectively, an existential or ∃-formula). Consider
a language L and a sentence of that language in prenex form, say
∀x1∃y1∀x2∃y2R(x1, y1, x2, y2).
(1)
Now for each existential quantiﬁer, let us introduce a new function symbol with
as many places as there are universal quantiﬁers to its left, to obtain an expanded
language L+. Thus in our example there would be two new function symbols, say
f1 and f2, corresponding to ∃y1 and ∃y2, the former having one place corresponding
to ∀x1, and the latter two places corresponding to ∀x1 and ∀x2. Let us replace each
existentially quantiﬁed variable by the term that results on applying the corresponding
function symbol to the universally quantiﬁed variable(s) to its left. The resulting
∀-formula, which in our example would be
∀x1∀x2R(x1, f1(x1), x2, f2(x1, x2))
(2)
is called the Skolem normal form of the original sentence, and the new function
symbols occurring in it the Skolem function symbols.
A little thought shows that (2) logically implies (1). In any interpretation of the
expanded language L+ with the new function symbols, it is the case that for every
element a1 of the domain there is an element b1, such that for every element a2 there
is an element b2, such that a1, b1, a2, b2 satify R(x1, y1, x2, y2): namely, take for
b1 the result of applying to a1 the function denoted by f1, and for b2 the result of
applying to a1 and a2 the function denoted by f2.
We cannot, of course, say that conversely (1) implies (2). What is true is that (2)
is implied by (1) together with the following:
∀x1(∃y1∀x2∃y2R(x1, y1, x2, y2) →∀x2∃y2R(x1, f1(x1), x2, y2))
(3.1)
∀x1∀x2(∃y2R(x1, f1(x1), x2, y2) →R(x1, f1(x1), x2, f2(x1, x2))).
(3.2)

248
NORMAL FORMS
For (1) and (3.1) imply
∀x1∀x2∃y2R(x1, f1(x1), x2, y2)
which with (3.2) implies (2). The sentences (3) are called the Skolem axioms.
For a prenex formula of a different kind, with different numbers of universal and
existential quantiﬁers, the number and number of places of the required Skolem func-
tions would be different, and the Skolem axioms correspondingly so. For instance, for
∃y0∀x1∀x2∃y1∃y2∀x3Q(y0, x1, x2, y1, y2, x3)
(1′)
we would need one zero-place function symbol (which is to say, one constant) f0 and
two two-place function symbols f1 and f2. The Skolem normal form would be
∀x1∀x2∀x3Q( f0, x1, x2, f1(x1, x2), f2(x1, x2), x3)
(2′)
and the Skolem axioms would be
∃y0∀x1∀x2∃y1∃y2∀x3Q(y0, x1, x2, y1, y2, x3) →
(3.0′)
∀x1∀x2∃y1∃y2∀x3Q( f0, x1, x2, y1, y2, x3)
∀x1∀x2 (∃y1∃y2∀x3Q( f0, x1, x2, y1, y2, x3) →
(3.1′)
∃y2∀x3Q( f0, x1, x2, f1(x1, x2), y2, x3))
∀x1∀x2 (∃y2∀x3Q( f0, x1, x2, f1(x1, x2), y2, x3) →
(3.2′)
∀x3Q( f0, x1, x2, f1(x1, x2), f2(x1, x2), x3)).
But in exactly the same way in any example, the Skolem normal form will imply
the original formula, and the original formula together with the Skolem axioms will
imply the Skolem normal form.
If L is a language and L+ is the result of adding Skolem functions for some or all of
its sentences, then an expansion M+ of an interpretation M of L to an interpretation
of L+ is called a Skolem expansion if it is a model of the Skolem axioms.
19.6 Lemma (Skolemization). Every interpretation of L has a Skolem expansion.
Proof: The essential idea of the proof is sufﬁciently illustrated by the case of our
original example (1) above. The proof uses a set-theoretic principle known as the
axiom of choice. According to this principle, given any family of nonempty sets,
there is a function ε whose domain is that family of sets, and whose value ε(X) for
any set Y in the family is some element of Y. Thus ε ‘chooses’ an element out of each
Y in the family. We apply this assumption to the family of nonempty subsets of |M|
and use ε to deﬁne a Skolem expansion N = M+ of M.
We ﬁrst want to assign a denotation f N
1
that will make the Skolem axiom (3.1)
come out true. To this end, for any element a1 in |M| consider the set B1 of all b1in
|M| such that a1 and b1 satisfy ∀x2∃y2R(x1, y1, x2, y2) in M. If B1 is empty, then no
matter what we take f N
1 (a1) to be, a1 will satisfy the conditional
∃y1∀x2∃y2 R(x1, y1, x2, y2) →∀x2∃y2R(x1, f1(x1), x2, y2)
since it will not satisfy the antecedent. But for deﬁniteness, let us say that if B1
is empty, then we are to take f N
1 (a1) to be ε(|M|). If B1 is nonempty, then we

19.2. SKOLEM NORMAL FORM
249
take f N
1 (a1) to be ε(B1): we use ε to choose one particular element b1 such that
a1 and b1 satisfy ∀x2∃y2R(x1, y1, x2, y2). Then since a1 and f N
1 (a1) will satisfy
∀x2∃y2R(x1, y1, x2, y2), it follows that a1 will satisfy the foregoing conditional, and
since this will be the case for any a1, it follows that (3.1) will be true.
We next want to assign a f N
2 that will make the Skolem axiom (3.2) come out true.
We proceed in exactly the same way. For any a1 and a2, consider the set B2 of all b2
such that a1, a2, and b2 satisfy R(x1, f1(x1), x2, y2). If B1 is empty, we take f N
2 (a1, a2)
to be ε(|M|), and otherwise take it to be ε(B2). The procedure would be the same no
matter how many Skolem function symbols we needed to assign denotations to, and
how many Skolem axioms we needed to make true.
Let  be any set of sentences of any language L, and for each sentence A in , ﬁrst
associate to it a logically equivalent prenex sentence A* as in the preceding section,
and then associate to A* its Skolem form A# as above, and let # be the set of all these
sentences A# for A in . Then # is a set of ∀-sentences equivalent for satisﬁability
to the original set . For if # is satisﬁable, there is an interpretation N in which
each A# in # comes out true, and since A# implies A* and A* is equivalent to A, we
thus have an interpretation in which each A in  comes out true, so  is satisﬁable.
Conversely, if  is satisﬁable, there is an interpretation M of the original language
in which each A in  and hence each A* comes out true. By the preceding lemma,
M has an expansion N to an interpretation in which each A* remains true and all
Skolem axioms come out true. Since A* together with the Skolem axioms implies
A#, each A# in # comes out true in N, and # is satisﬁable. We have thus shown
how we can associate to any set of sentences a set of ∀-sentences equivalent to it for
satisﬁability. This fact, however, does not exhaust the content of the Skolemization
lemma. For it can also be used to give a proof of the L¨owenheim–Skolem theorem,
and in a stronger version than that stated in chapter 12 (and proved in chapter 13).
To state the strong L¨owenheim–Skolem theorem we need the notion of what it is
for one interpretation B to be a subinterpretation of another interpretation A. Where
function symbols are absent, the deﬁnition is simply that (1) the domain |B| should be
a subset of the domain |A|, (2) for any b1, . . . , bn in |B| and any predicate R one has
RB(b1, . . . , bn)
if and only if
RA((b1), . . . , (bn))
(S1)
and (3) for every constant c one has
(cB) = cA.
(S2)
Thus, B is just like A, except that we ‘throw away’ the elements of |A| that are not
in |B|.
Where function symbols are present, we have also to require that for any b1, . . . ,
bn in |B| and any function symbol f the following should hold:
f B(b1, . . . , bn) = f A(b1, . . . , bn).
(S3)
Note that this last implies that f A(b1, . . . , bn) must be in |B|: Where function
symbols are absent, any nonempty subset B of A can be the domain of a subinterpre-
tation of A, but where function symbols are present, only those nonempty subsets B

250
NORMAL FORMS
can be the domains of subinterpretations that are closed under the functions f A that
are the denotations of function symbols of the language, or in other words, that contain
the value of any of these functions for given arguments if they contain the arguments
themselves.
When B is a subinterpretation of A, we say that A is an extension of B. Note that
the notions of extension and subinterpretation pertain to enlarging or contracting the
domain, while keeping the language ﬁxed.
Note that it follows from (S1) and (S3) by induction on complexity of terms that
every term has the same denotation in B as in A. It then follows by (S1) and (S2)
that any atomic sentence has the same truth value in B as in A. It then follows
by induction on complexity that every quantiﬁer-free sentence has the same truth
value in B as in A. Essentially the same argument shows that, more generally, any
given elements of B satisfy the same quantiﬁer-free formulas in B as in A. If an
∃-sentence ∃x1 . . . ∃xn R(x1, . . . , xn) is true in B, then there are elements b1, . . . , bn
of |B| that satisfy the quantiﬁer-free formula R(x1, . . . , xn) in B and hence, by what
has just been said, in A as well, so that the ∃-sentence ∃x1. . . ∃xn R(x1, . . . , xn) is
also true in A. Using the logical equivalence of the negation of an ∀-sentence to an
∃-sentence, we have the following result.
19.7 Proposition. Let A be any interpretation and B any subinterpretation thereof.
Then any ∀-sentence true in A is true in B.
19.8 Example (Subinterpretations). Proposition 19.7 is in general as far as one can go. For
consider the language with just the two-place predicate <. Let P, Q, and R have domains
the integers, rational numbers, and real numbers, respectively, and let the denotation of < in
each case be the usual order relation < on the numbers in question. Since the order of integers
quaintegersisthesameastheirorderquarationalnumbers,andtheorderofrationalnumbers
qua rational numbers is the same as their order qua real numbers, P is a subinterpretation
of Q and R, and Q is a subinterpretation of R. Consider, however, the sentence
∀x∀y(x < y →∃z(x < z & z < y))
or its prenex equivalent
∀x∀y∃z(x < y →(x < z & z < y)).
R and Q are models of this sentence, since between any two real numbers a and b with
a < b there is some other real number c with a < c and c < b, such as (b −a)/2, and
similarly for rational numbers. But P is not a model of the sentence, since between the
integers 0 and 1 there is no other integer. (Of course, the sentence here is not an ∀-sentence,
but it is, so to speak, just one step beyond, an ∀∃-sentence.)
Thus a subinterpretation of a model of a sentence C (or set of sentences ) may,
but in general need not, also be a model of C (or ): if it is, it is called a submodel.
Without further ado, here is the strong version of the L¨owenheim–Skolem theorem.
(The phrase ‘enumerable’ is redundant, given that we are restricting our attention
to enumerable languages, but we include it to emphasize that we are making this
restriction.)

19.2. SKOLEM NORMAL FORM
251
19.9 Theorem (Strong L¨owenheim–Skolem theorem). Let A be a nonenumerable
model of an enumerable set of sentences . Then A has an enumerable subinterpretation
that is also a model of .
Proof: It will sufﬁce toprovethetheoremforthespecialcaseofsetsof ∀-sentences.
For suppose we have proved the theorem in this special case, and consider the general
case where A is a model of some arbitrary set of sentences . Then as in our earlier
discussion, A has an expansion A# to a model of #, the set of Skolem forms of
the sentences in . Since Skolem forms are ∀-sentences, by the special case of the
theorem there will an enumerable subinterpretation B# that is also a model of #.
Then since the Skolem form of a sentence implies the original sentence, B# will also
be a model of , and so will be its reduct B to the original language. But this B will
be an enumerable subinterpretation of A.
To prove the theorem in the special case where all sentences in  are ∀-sentences,
consider the set B of all denotations in A of closed terms of the language of .
(We may assume there are some closed terms, since if not, we may add a constant
c to the language and the logically valid sentence c = c to .) Since that language
is enumerable, so is the set of closed terms, and so is B. Since B is closed under
the functions that are denotations of the function symbols of the language, it is the
domain of an enumerable subinterpretation B of A. And by Proposition 19.7, every
∀-sentence true in A is true in B, so B is a model of .
Two interpretations A and B for the same language are called elementarily equiv-
alent if every sentence true in the one is true in the other. Taking for  in the above
version of the L¨owenheim–Skolem theorem the set of all sentences true in A, the
theorem tells us that any interpretation has an enumerable subinterpretation that is
elementarily equivalent to it. A subinterpretation B of an interpretation A is called
an elementary subinterpretation if for any formula F(x1, . . . , xn) and any elements
b1, . . . , bn of |B|, the elements satisfy the formula in A if and only if they satisfy
it in B. This implies elementary equivalence, but is in general a stronger condition.
By extending the notion of Skolem normal form to formulas with free variables,
the above strong version of the L¨owenheim–Skolem theorem can be sharpened to
a still stronger one telling us that any interpretation has an enumerable elementary
subinterpretation.
Applications of Skolem normal form will be given in the next section. Since
we are not going to be needing the sharper result stated in the preceding para-
graph for these applications, we do not go into the (tedious but routine) details
of its proof. Instead, before turning to applications we wish to discuss another,
more philosophical issue. At one time the L¨owenheim–Skolem theorem (especially
in the strong form in which we have proved it in this section) was considered
philosophically perplexing because some of its consequences were perceived
as anomalous. The apparent anomaly, sometime called ‘Skolem’s paradox’, is that
there exist certain interpretations in which a certain sentence, which seems to say
that nonenumerably many sets of natural numbers exist, is true, even though the
domains of these interpretations contain only enumerably many sets of natural num-
bers, and the predicate in the sentence that we would be inclined to translate

252
NORMAL FORMS
as ‘set (of natural numbers)’ is true just of the sets (of natural numbers) in the
domains.
19.10 Example (The ‘Skolem paradox’). There is no denying that the state of affairs
thought to be paradoxical does obtain. In order to see how it arises, we ﬁrst need an
alternative account of what it is for a set E of sets of natural numbers to be enumerable,
and for this we need to use the coding of an ordered pair (m, n) of natural numbers by a
single number J(m, n), as described in section 1.2. We call a set w of natural numbers an
enumerator of a set E of sets of natural numbers if
∀z(z is a set of natural numbers & z is in E →
∃x(x is a natural number &
∀z(∀y(y is a natural number →(y is in z ↔J(x, y) is in w)))).
The fact about enumerators and enumerability we need is that a set E of sets of natural
numbers is enumerable if and only if E has an enumerator.
[The reason: suppose E is enumerable. Let e0, e1, e2, . . . be an enumeration of sets of
natural numbers that contains all the members of E, and perhaps other sets of natural
numbers also. Then the set of numbers J(x, y) such that y is in ex is an enumerator of E.
Conversely, if w is an enumerator of E, then letting ex be the set of those numbers y such
that J(x, y) is in w, we get an enumeration e0, e1, e2, . . . that contains all members of E,
and E is enumerable.]
We want now to look at a language and some of its interpretations. The language contains
just the following: constants 0, 1, 2, . . . , two one-place predicates N and S, a two-place
predicate ∈, and a two-place function symbol J. An interpretation I of the kind we are
interested in will have as the elements of its domain all the natural numbers, some or all of
the sets of natural numbers, and nothing else. The denotations of 0, 1, 2, and so on will be
the numbers 0, 1, 2, and so on. The denotation of N will be the set of all natural numbers,
and of S will be the set of all sets of natural numbers in the domain; while the denotation
of ∈will be the relation of membership between numbers and sets of numbers. Finally, the
denotation of J will be the function J, extended to give some arbitrary value—say 17—for
arguments that are not both numbers (that is, one or both of which are sets). Among such
interpretations, the standard interpretation J will be the one in which the domain contains
all sets of natural numbers.
Consider the sentence ∼∃w F(w) where F(w) is the formula
Sw & ∀z(Sz →∃x(Nx & ∀y(Ny →(y ∈z ↔J(x, y) ∈w)))).
In each of the interpretations I that concern us, ∼∃w F(w) will have a truth value. It will be
true in I if and only if there is set in the domain of I that is an enumerator of the set of all
sets of numbers that are in the domain of I, as can be seen by compairing the formula F(w)
with the deﬁnition of enumerator above. We cannot say, more simply, that the sentence
is true in the interpretation if and only if there is no enumerator of the set of all sets of
numbers in its domain, because the quantiﬁer ∃w only ‘ranges over’ or ‘refers to’ sets in
the domain.
There is, as we know, no enumerator of the set of all sets of numbers, so the sentence
∼∃w F(w) is true in the standard interpetation J , and can be said to mean ‘nonenumerably
many sets of numbers exist’ when interpreted ‘over’ J , since it then denies that there is an

19.3. HERBRAND’S THEOREM
253
enumerator of the set of all sets of numbers. By the L¨owenheim–Skolem theorem, there is
an enumerable subinterpretation K of J in which the sentence is also true. (Note that all
numbers will be in its domain, since each is the denotation of some constant.) Thus there is
an interpretation K whose domain contains only enumerably many sets of numbers, and in
which S is true of just the sets of numbers in its domain. This is the ‘Skolem paradox’.
How is the paradox to be resolved? Well, though the set of all sets of numbers in the
domain of K does indeed have an enumerator, since the domain is enumerable, none of its
enumerators can be in the domain of K. [Otherwise, it would satisfy F(x), and ∃w F(x)
would be true in K, as it is not.] So part of the explanation of how the sentence ∼∃w F(x)
can be true in K is that those sets that ‘witness’ that the set of sets of numbers in the domain
of K is enumerable are not themselves members of the domain of K.
A further part of the explanation is that what a sentence should be understood as saying
or meaning or denying is at least as much as function of the domain over which the sentence
is interpreted (and even of the way in which that interpretation is described or referred to)
as of the symbols that constitute the sentence. ∼∃w F(x) can be understood as saying
‘nonenumerably many sets of numbers exist’ when its quantiﬁers are understood as ranging
over a collection containing all numbers and all sets of numbers, as with the domain of the
standardinterpretationJ ;butitcannotbesounderstoodwhenitsquantiﬁersrangeoverother
domains, and in particular not when they range over the members of enumerable domains.
The sentence ∼∃w F(x)—that sequence of symbols—‘says’ something only when supplied
with an interpretation. It may be surprising and even amusing that it is true in all sorts of
interpretations, including perhaps some subinterpretations K of J that have enumerable
domains, but it should not a priori seem impossible for it to be true in these. Interpreted
over such a K, it will only say ‘the domain of K contains no enumerator of the set of sets
of numbers in K′. And this, of course, is true.
19.3 Herbrand’s Theorem
The applications of Skolem normal form with which we are going to be concerned
in this section require some preliminary machinery, with which we begin. We work
throughout in logic without identity. (Extensions of the results of this section to logic
with identity are possible using the machinery to be developed in the next section,
but we do not go into the matter.)
Let A1, . . . , An be atomic sentences. A (truth-functional) valuation of them is
simply a function ω assigning each of them one of the truth values, true or false
(represented by, say, 1 and 0). The valuation can be extended to truth-functional
compounds of the Ai (that is, quantiﬁer-free sentences built up from the Ai using ∼
and & and ∨) in the same way that the notion of truth in an interpretation is extended
from atomic to quantiﬁer-free sentences:
ω(∼B) = 1
if and only if
ω(B) = 0
ω(B & C) = 1
if and only if
ω(B) = 1 and ω(C) = 1
ω(B ∨C) = 1
if and only if
ω(B) = 1 or ω(C) = 1.
A set  of quantiﬁer-free sentences built up from the Ai is said to be truth-functionally
satisﬁable if there is some valuation ω giving every sentence S in  the value 1.

254
NORMAL FORMS
Now if  is satisﬁable in the ordinary sense, that is, if there is an interpretation A
in which every sentence in  comes out true, then certainly  is truth-functionally
satisﬁable. Simply take for ω the function that gives a sentence the value 1 if and
only if it is true in A.
The converse is also true. In other words, if there is a valuation ω that gives every
sentence in  the value 1, then there is an interpretation A in which every sentence in
 comes out true. To show this, it is enough to show that for any valuation ω, there is
an interpretation A such that each Ai comes out true in A just in case ω assigns it the
value 1. This is in fact the case even if we start with an inﬁnite set of atomic formulas
Ai. To specify A, we must specify a domain, and assign a denotation to each constant,
function symbol, and predicate occurring in the Ai. Well, simply take for each closed
term t in the language some object t*, with distinct terms corresponding to distinct
objects. We take the domain of our interpretation to consist of these objects t*. We
take the denotation of a constant c to be c*, and we take the denotation of a function
symbol f to be the function that given the objects t1*, . . . , tn* associated with terms
t1, . . . , tn as arguments, yields as value the object f (t1, . . . , tn)* associated with the
term f (t1, . . . , tn). It follows by induction on complexity that the denotation of an
arbitrary term t is the object t* associated with it. Finally, we take as the denotation
of a predicate P the relation that holds of objects the objects t1*, . . . , tn* associated
with terms t1, . . . , tn if and only if the sentence P(t1, . . . , tn) is one of the Ai and
ω assigns it the value 1. Thus truth-functional satisﬁability and satisﬁability in the
ordinary sense come to the same thing for quantiﬁer-free sentences.
Let now  be a set of ∀-formulas of some language L, and consider the set  of all
instances P(t1, . . . , tn) obtained by substituting in sentences ∀x1. . . ∀xn P(x1, . . . , xn)
of  terms t1, . . . , tn of L for the variables. If every ﬁnite subset of  is truth-
functionally satisﬁable, then every ﬁnite subset of  is satisﬁable, and hence so is ,
by the compactness theorem.
Moreover, by Proposition 12.7, if A is an interpretation in which every sentence in
 comes out true, and B is the subinterpretation of A whose domain is the set of all
denotations of closed terms, then every sentence in  also comes out true in B. Since
in B every element of the domain is the denotation of some term, from the fact that
every instance P(t1, . . . , tn) comes out true it follows that the ∀-formula ∀x1. . . ∀xn
P(x1, . . . , xn) comes out true, and thus B is a model of . Hence  is satisﬁable.
Conversely, if  is satisﬁable, then since a sentence implies all its substitution in-
stances,everyﬁniteorinﬁnitesetofsubstitutioninstancesofsentencesin willbesat-
isﬁable and hence truth-functionally satisﬁable. Thus we have proved the following.
19.11 Theorem (Herbrand’s theorem). Let  be a set of ∀-sentences. Then  is sat-
isﬁable if and only if every ﬁnite set of substitution instances of sentences in  is truth-
functionally satisﬁable.
It is possible to avoid dependence on the compactness theorem in the foregoing
proof, by proving a kind of compactness theorem for truth-functional valuations,
which is considerably easier than proving the ordinary compactness theorem. (Then,
starting with the assumption that every ﬁnite subset of  is truth-functionally sat-
isﬁable, instead of arguing that each ﬁnite subset is therefore satisﬁable, and hence

19.4. ELIMINATING FUNCTION SYMBOLS AND IDENTITY
255
that  is satisﬁable by compactness, instead one would apply compactness for truth-
functionalsatisﬁabilitytoconcludethatistruth-functionallysatisﬁable,fromwhich
it follows that  is satisﬁable.) Herbrand’s theorem actually then implies the compact-
nesstheorem:Givenaset ofsentences,let# bethesetofSkolemformsofsentences
in . We know from the preceding section that if every ﬁnite subset of  is satisﬁable,
then every ﬁnite subset of # is satisﬁable and hence truth-functionally satisﬁable,
and so by Herbrand’s theorem # is satisﬁable, whence the original  is satisﬁable.
Herbrand’s theorem also implies the soundness and G¨odel completeness theorems
for an appropriate kind of proof procedure (different from that used earlier in this book
and from those used in introductory textbooks), which we next describe. Suppose we
are given a ﬁnite set  of sentences and wish to know if  is unsatisﬁable. We ﬁrst
replace the sentences in  by Skolem forms: the proofs of the normal form theorems
given in the preceding two sections implicitly provide an effective method of doing
so. Now having the ﬁnite set S1, . . . , Sn, of ∀-sentences that are the Skolem forms
of our original sentences, and any effective enumeration t1, t2, t3, . . . of terms of the
language, we set about effectively generating all possible substitution instances. (We
could do this by ﬁrst substituting in each formula for each of its variables the term
t1, then substituting for each variable in each formula either t1 or t2, then substituting
for each variable in each formula either t1 or t2 or t3, and so on. At each stage we get
only ﬁnitely many substitution instances, namely, at stage m just km, where k is the
total number of variables; but in the end we get them all.)
Each time we generate a new substitution instance, we check whether the ﬁnitely
many instances we have generated so far are truth-functionally satisﬁable. This can
be done effectively, since on the one hand at any given stage we will have generated
so far only ﬁnitely many substitution instances, so that there are only ﬁnitely many
valuations to be considered (if the substitution instances generated so far involve m
distinct atomic sentences, the number of possible valuations will be 2m); while on the
other hand, given a valuation ω and a truth-functional compound B of given atomic
sentences Ai, we can effectively work out the value ω(B) required (the method of
truth tables expounded in introductory textbooks being a way of setting out the work).
If any ﬁnite set of Skolem instances (that is, of substitution instances of Skolem
forms) turns out to be truth-functionally unsatisﬁable, then the original set  is
unsatisﬁable: producing such a set of Skolem instances is a kind of refutation of
. Conversely, if  is unsatisﬁable, the above-described procedure will eventually
produce such a refutation. This is because we know from the preceding section that 
is unsatisﬁable if and only if # is, and so, by Herbrand’s theorem,  is unsatisﬁable
if and only if some ﬁnite set of substitution instances of Skolem forms is truth-
functionally unsatisﬁable. The refutation procedure just described is thus sound and
complete (hence so would be the proof procedure that proves  implies D by refuting
 ∪{∼D}).
19.4 Eliminating Function Symbols and Identity
While the presence of identity and function symbols is often a convenience, their
absence can often be a convenience, too, and in this section we show how they can,

256
NORMAL FORMS
in a sense to be made precise, be ‘eliminated’. (Constants will be treated as a special
sort of function symbol, namely 0-place function symbols. Whatever we say about
function symbols in this section goes for constants, too, and they will not be given
separate consideration.)
Let us take up the elimination of function symbols ﬁrst. The ﬁrst fact we need is
that any sentence is logically equivalent to one in which all function symbols occur
immediately to the right of the identity symbol. This means that no function symbols
occur in the blanks to the right of predicates other than the identity predicate, or
in the blanks to the right of a function symbol, or in the blank to the left of the
identity symbol, so the only occurrences of an n-place function symbol f are in
atomic subformulas of the type v = f (u1, . . . , un), where v and the ui are variables
(not necessarily all distinct).
The proof is quite simple: Suppose that S is a sentence with at least one occurrence
of a function symbol f in a position other than immediately to the right of the identity
symbol. In any such occurrence, f occurs as the ﬁrst symbol in some term t that
occurs (possibly as a subterm of a more complex term) in some atomic subformula
A(t). Let v be any variable not occurring in S, and let S−be the result of replacing
A(t) by the logically equivalent ∃v(v = t & A(v)). Then S is logically equivalent to
S−, and S−contains one fewer occurrence of function symbols in positions other
than immediately to the right of the identity symbol. Reducing the number of such
occurrences one at a time in this way, S is ultimately equivalent to a sentence with no
such occurrences. So for the remainder of this chapter, we consider only sentences
without such occurrences.
We show how to eliminate function symbols one at a time from such sentences.
(The process may be repeated until all function symbols, including constants, have
been eliminated.) If S is such a sentence and f an n-place function symbol occurring
in it, let R be a new (n + 1)-place predicate. Replace each subformula of the type
v = f (u1, . . . , un) in which f occurs—and remember, these are the only kind of
occurrences of f in S—by R(u1, . . . , un, v), and call the result S±. Let C be the
following sentence, which we call the functionality axiom:
∀x1 . . . ∀xn∃y∀z(R(x1, . . . , xn, z) ↔z = y).
Let D be the following sentence, which we call the auxiliary axiom:
∀x1 . . . ∀xn∀z(R(x1, . . . , xn, z) ↔z = f (x1, . . . , xn)).
The precise sense in which the symbol f is ‘dispensable’ is indicated by the following
proposition (and its proof).
19.12 Proposition. S is satisﬁable if and only if S± & C is satisﬁable.
Proof: Let us begin by sorting out the relationships among the various sentences
we have introduced. If we call the language to which the original sentence S belonged
L, the language obtained by adding the new predicate R to this language L+, and the
language obtained by removing the old function symbol f from the latter language
L±, then S belongs to L, D to L+, and S± and C to L±. Note that D implies C, and
D implies S ↔S±.

19.4. ELIMINATING FUNCTION SYMBOLS AND IDENTITY
257
Now note that every interpretation M of L has a unique expansion to an interpre-
tation N of L+ that is a model of D. The one and only way to obtain such an N is
to take as the denotation RN of the new predicate the relation that holds of a1, . . . ,
an, b if and only if b = f M(a1, . . . , an). Also, every interpretation P of L± that is a
model of C has a unique expansion to an interpretation N of L+ that is a model of D.
The one and only way to obtain such an N is to take as the denotation f N of the new
function symbol the function that given a1, . . . , an as arguments yields as value the
unique b such that RP(a1, . . . , an, b) holds. (The truth of C in P is need to guarantee
that there will exist such a b and that it will be unique.)
If S has a model M, by our observations in the preceding paragraph it has an
expansion to a model N of S & D. Then since D implies S ↔S± and C, N is a
model of S± & C. Conversely, if S± & C has a model P, then by our observations
in the preceding paragraph, it has an expansion to a model N of S± & D. Then since
D implies S ↔S±, N is a model of S.
We now turn to the matter of eliminating the identity symbol, supposing that
function symbols have already been eliminated. Thus we begin with a language L
whose only nonlogical symbols are predicates. We add a further two-place relation-
symbol ≡and consider the following sentence E, which we have already encountered
in chapter 12, and will call the equivalence axiom:
∀x x ≡x &
∀x∀y(x ≡y →y ≡x) &
∀x∀y∀z((x ≡y & y ≡z) →x ≡z).
In addition, for each predicate P of L we consider the following sentence CP, which
we will call the congruence axiom for P:
∀x1 . . . ∀xn∀y1 . . . ∀yn((x1 ≡y1 & . . . & xn ≡yn) →
(P(x1, . . . , xn) ↔P(y1, . . . , yn))).
Note that the result of replacing the new sign ≡by the identity sign = in E or any CP
is a logically valid sentence. For any sentence S, we let S* be the result of replacing
the identity sign = throughout by this new sign ≡, and CS the conjunction of the
CP for all predicates P occurring in S. The precise sense in which the symbol = is
‘dispensable’ is indicated by the following proposition (and its proof).
19.13 Proposition. S is satisﬁable if and only if S* & E & CS is satisﬁable.
Proof: One direction is easy. Given a model of S, we get a model of S* & E &
CS by taking the identity relation as the denotation of the new sign.
For the other direction, suppose we have a model A of S* & E & CS. We want to
show there is a model B of S. Since E is true in A, the denotation ≡A of the new sign
in A is an equivalence relation on the domain |A|. We now specify an interpretation
B whose domain |B| will be the set of all equivalence classes of elements of |A|. We
need to specify what the denotation PB of each predicate P of the original language
is to be. For any equivalence classes b1, . . . , bn in |B|, let PB hold of them if and only

258
NORMAL FORMS
if PA holds of a1, . . . , an for some a1 in b1, . . . , and an in bn. We also need to specify
what the denotation ≡B of the new sign is to be. We take it to be the genuine identity
relation.
Let now j be the function from |A| to |B| whose value for argument a is the equiv-
alence class of a. If PA(a1, . . . , an) holds, then by deﬁnition of PB, PB( j(a1), . . . ,
j(an)) holds; while if PB( j(a1), . . . , j(an)) holds, then again by deﬁnition of PB,
PA(a′
1, . . . , a′
n) holds for some a′
i, where each a′
i belongs to the same equivalence class
j(a′
i) = j(ai) as ai. The truth of CP in A guarantees that in that case PA(a1, . . . , an)
holds. Trivially, a1 ≡A a2 holds if and only if j(a1) = j(a2), which is to say, if and
only if j(a1) ≡B j(a2) holds. Thus the function j has all the properties of an isomor-
phism except for not being one-to-one. If we look at the proof of the isomorphism
lemma, according to which exactly the same sentences are true in isomorphic inter-
pretations, we see that the property of being one-to-one was used only in connection
with identity. Hence, so far as sentences not involving identity are concerned, by the
same proof as that of the isomorphism lemma, the same ones are true in B as in A.
(See Proposition 12.5 and its proof.) In particular S* is true in B. But since ≡B is
the genuine identity relation, it follows that the result of replacing ≡by = in S* will
also be true in B—and the result of this substitution is precisely the original S. So we
have a model B of S as required.
Propositions 19.12 and 19.13 can both be stated more generally. If  is any set
of sentences and ± the set of all S± for S in , together with all functionality
axioms, then  is satisﬁable if and only if ± is. If  is any set of sentences not
involving function symbols, and * is the set of all S* for S in  together with the
equivalence axiom and all congruence axioms, then  is satisﬁable if and only if *
is satisﬁable. Applications of the function-free and identity-free normal forms of the
present section will be indicated in the next two chapters.
Problems
19.1 Find equivalents
(a) in negation-normal form
(b) in disjunctive normal form
(c) in full disjunctive normal form
for ∼((∼A & B) ∨(∼B & C)) ∨∼(∼A ∨C).
19.2 Find equivalents in prenex form for
(a) ∃x(P(x) →∀x P(x))
(b) ∃x(∃x P(x) →P(x)).
19.3 Find an equivalent in prenex form for the following, and write out its Skolem
form:
∀x(Qx →∃y(Py & Ryx)) ↔∃x(Px & ∀y(Qy →Rxy).
19.4 Let T be a set of ﬁnite sequences of 0s and 1s such that any initial segment
(e0, . . . , em−1), m < n, of any element (e0, . . . , en−1) in T is in T . Let T * be

PROBLEMS
259
the subset of T consisting of all ﬁnite sequences s such that there are inﬁnitely
many ﬁnite sequences t in T with s is an initial segment of t. Show that if T is
inﬁnite, then there is an inﬁnite seqeunce e1, e2, . . . of 0s and 1s such that every
initial segment (e0, . . . , em−1) is in T *.
19.5 State and prove a compactness theorem for truth-functional valuations.

20
The Craig Interpolation Theorem
Suppose that a sentence A implies a sentence C. The Craig interpolation theorem tells us
that in that case there is a sentence B such that A implies B, B implies C, and B involves
no nonlogical symbols but such as occur both in A and in B. This is one of the basic
results of the theory of models, almost on a par with, say, the compactness theorem.
The proof is presented in section 20.1. The proof for the special case where identity and
function symbols are absent is an easy further application of the same lemmas that we
have applied to prove the compactness theorem in Chapter 13, and could have been
presented there. But the easiest proof for the general case is by reduction to this special
case, using the machinery for the elimination of function symbols and identity developed
in section 19.4. Sections 20.2 and 20.3, which are independent of each other, take up
two signiﬁcant corollaries of the interpolation theorem, Robinson’s joint consistency
theorem and Beth’s deﬁnability theorem.
20.1 Craig’s Theorem and Its Proof
We begin with a simple observation.
20.1 Proposition. If a sentence A implies a sentence C, then there is a sentence B that
A implies, that implies C, and that contains only such constants as are contained in both of
A and C.
Proof: The reason is clear: If there are no constants in A not in C, we may take A
for our B; otherwise, let a1, . . . , an be all the constants in A and not in C, and let A* be
the result of replacing each ai by some new variable vi. Then, since A →C is valid,
so is ∀v1 · · · ∀vn(A* →C), and hence so is ∃v1 · · · ∃vn A* →C. Then ∃v1 · · · ∃vn A*
is a suitable B, for A implies it, it implies C, and all constants in it are in both A and C.
It might occur to one to ask whether the fact just proved about constants can be
subsumed under one about constants, function symbols, and predicates; that is, to ask
whether if A implies C, there is always a sentence B that A implies, that implies C,
and that contains only constants, function symbols, and predicates that are in both A
and C. The answer to the question, as stated, is no.
260

20.1. CRAIG’S THEOREM AND ITS PROOF
261
20.2 Example (A failure of interpolation). Let A be ∃x Fx & ∃x ∼Fx, and let C be
∃x∃y x ̸= y. Then A implies C, but there is no sentence at all that contains only con-
stants, function symbols, and predicates that are in both A and C, and therefore there is no
such sentence that A implies and that implies C.
Suppose we do not count the logical predicate of identity, and ask whether, if
A implies C, there is always a sentence B that A implies, that implies C, and that
contains no nonlogical symbols (that is, no constants, no function symbols, and no
nonlogical predicates) except such as are both in A and in C. The Craig interpolation
theorem is the assertion that the answer to our question, thus restated, is yes.
20.3 Theorem (Craig interpolation theorem). If A implies C, then there is a sentence
B that A implies, that implies C, and that contains no nonlogical symbols except such as
are both in A and in C.
Such a B is called an interpolant between A and C. Before launching into the
proof, let us make one clariﬁcatory observation.
20.4 Example (Degenerate cases). It may happen that we need to allow the identity symbol
to appear in the interpolant even though it appears in neither A nor C. Such a situation can
arise if A is unsatisﬁable. For instance, if A is ∃x(Fx & ∼Fx) and C is ∃xGx, then ∃x x ̸= x
will do for B, but there are no sentences at all containing only predicates that occur in both
A and C, since there are no such predicates. A similar situation can arise if C is valid.
For instance, if A is ∃x Fx and C is ∃x(Gx ∨∼Gx), then ∃x x = x will do for B, but
again there are no predicates that occur in both A and C. Note that ∃x x ̸= x will do for
an interpolant in any case where A is unsatisﬁable, and ∃x x = x in any case where C is
valid. (We could avoid the need for identity if we admitted the logical constants ⊤and ⊥of
section 19.1.)
Proof, Part I:
This noted, we may restrict our attention to cases where A is
satisﬁable and C is not valid. The proof that, under this assumption, an interpolant
B exists will be, like so many other proofs, divided into two parts. First we consider
the case where identity and function symbols are absent, then reduce the general case
where they are present to this special case. (The proof, unlike that of Proposition 20.1,
will be nonconstructive. It will prove the existence of an interpolant, without showing
how to ﬁnd one. More constructive proofs are known, but are substantially longer and
more difﬁcult.)
Let us begin immediately on the proof of the special case. Considering only sen-
tences and formulas without identity or function symbols, let A be a sentence that
is satisﬁable and C a sentence that is not valid (which is equivalent to saying that
∼C is satisﬁable), such that A implies C (which is equivalent to saying {A, ∼C}
is unsatisﬁable). We want to show there is a sentence B containing only predicates
common to A and C, such that A implies B and B implies C (which is equivalent to
saying ∼C implies ∼B).
What we are going do is to show that if there is no such interpolant B, then
{A, ∼C} is after all satisﬁable. To show this we apply the model existence theorem

262
THE CRAIG INTERPOLATION THEOREM
(Lemma 13.3). This tells us that if L is a language containing all the nonlogical
symbols of A and of C, and if L* is a language obtained by adding inﬁnitely many
new constants to L, then {A, ∼C} will be satisﬁable provided it belongs to some set S
of sets of sentences of L* having certain properties. For the present situation, where
identity and function symbols are absent, these properties are as follows:
(S0) If  is in S and 0 is a subset of , then 0 is in S.
(S1) If  is in S, then for no sentence D are both D and ∼D in .
(S2) If  is in S and ∼∼D is in , then  ∪{D} is in S.
(S3) If  is in S and (D1 ∨D2) is in , then  ∪{Di} is in S for either i = 1 or i = 2.
(S4) If  is in S and ∼(D1 ∨D2) is in , then  ∪{∼Di} is in S for both i = 1 and
i = 2.
(S5) If  is in S and {∃x F(x)} is in , and then  ∪{F(b)} is in S for any constant b
not in  or ∃xF(x).
(S6) If  is in S and ∼∃x F(x) is in , then  ∪{∼F(b)} is in S for any constant b at
all.
What we need to do is to deﬁne a set S, use the hypothesis that there is no interpolant
B to show {A, ∼C} is in S, and establish properties (S0)–(S1) for S. Towards deﬁning
S, call a sentence D of L* a left formula (respectively, a right formula) if every
predicate in D is in A (respectively, is in C). If L is a satisﬁable set of left sentences
and R a satisﬁable set of right sentences, let us say that B bars the pair L, R if
B is both a left and a right sentence and L implies B while R implies ∼B. Our
assumption that there is no interpolant, restated in this terminology, is the assumption
that no sentence B of L bars {A}, {∼C}. It follows—by a proof quite like that of
Proposition 20.1—that no sentence B of L* bars {A}, {∼C}. Let S be the set of all 
that admit and unbarred division in the sense that we can write  as a union L ∪R
of two sets of sentences where L consists of left and R of right sentences, each of
L and R is satisﬁable, and no sentence bars the pair L, R. Then what we have
said so far is that {A, ∼C} is in S. What remains to be done is to establish properties
(S0)–(S6) for S.
(S0) is easy and left to the reader. For (S1), if  = L ∪R is an unbarred division,
then the assumptions that L is satisﬁable implies that there is no sentence D with
both D and ∼D in L. Similarly for R. Nor can there be a D with D in L and
∼D in R, for in that case D would be both a left sentence (since it belongs to L)
and a right sentence (since it belongs to R) and therefore would be a sentence that
is implied by L and whose negation is implied by R, and so would bar L, R.
Similarly, the reverse case with ∼D in L and D in R is impossible, since ∼D would
bar the pair L, R.
For (S2), suppose  = L ∪R is an unbarred division and ∼∼D is in . There
are two cases according as ∼∼D is in L or in R, but the two are just alike, and
we consider only the former. Then since ∼∼D is a left formula, so is D, and since
D is implied by L, adding D to L cannot make it unsatisﬁable if it was satisﬁable
before, nor can it make any sentence B a consequence that was not a consequence
before. So  ∪{D} = (L ∪{D}) ∪R is an unbarred division, and  ∪{D} is in S.
(S4)–(S6) are very similar.

20.1. CRAIG’S THEOREM AND ITS PROOF
263
(S3) is just slightly different. Suppose  = L ∪R is an unbarred division and
(D1 ∨D2) is in . Again there are two cases, and we consider only the one where
(D1 ∨D2) is in L. Each of the Di is of course a left sentence, since their disjunction
is. We claim that  ∪{Di} = (L ∪{Di}) ∪R is an unbarred division for at least
one i. Towards showing this, note that if L ∪{D1} is unsatisﬁable, then L implies
both (D1 ∨D2) and ∼D1, and hence implies D2. In this case, the proof that (L ∪
{D2}) ∪R is an unbarred division is just like the proof of the preceding paragraph.
Similarly, if L ∪{D2} is unsatisﬁable, then (L ∪{D2}) ∪R gives an unbarred
division. So we are left to treat the case where L ∪{Di} is satisﬁable for both i.
In this case, (L ∪{Di}) ∪R can fail to give an unbarred division only because
there is a sentence Bi that bars the pair L ∪{Di}, R. What we claim is that there
cannot exist such Bi both both i = 1 and i = 2. For suppose there did. Then since
Bi is implied by L ∪{Di} for each i, and L contains (D1 ∨D2), it follows that
B = (B1 ∨B2) is implied by L. Moreover, since each ∼Bi is implied by R, so
is ∼B. Finally, since each Bi is both a left and a right sentence, the same is true
of B. Thus there is a sentence B that bars L, R, contrary to hypothesis. This
contradiction completes the proof for the case where identity and function symbols
are absent.
Proof, Part II: We next consider the case where identity is present but func-
tion symbols are still absent. Suppose A implies C. As in section 18.4, we in-
troduce the new two-place predicate ≡. We write EL for the conjunction of the
equality axioms and the congruence axioms for predicates in A, and ER for the
corresponding sentence for C. We write * to indicate replacing = by ≡. Since A
implies C, A & ∼C is unsatisﬁable. What the proof of Proposition 19.13 tells us is
that therefore EL & ER & A* & ∼C* is unsatisﬁable. It follows that EL & A* implies
ER →C*. By the interpolation theorem for sentences without identity, there is a sen-
tence B* involving only ≡and nonlogical predicates common to A and C, such that
EL & A* implies B* and B* implies ER →C*. It follows that EL & A*& ∼B* and
ER & B* & C* are unsatisﬁable. Then we claim B, the result of replacing ≡by =
in B*, is the required interpolant between A and C. Certainly its nonlogical predi-
cates are common to A and C. Further, what the proof of Proposition 18.13 tell us
is is that A & ∼B and B & ∼C are unsatisﬁable, and therefore A implies B and B
implies C. The treatment of function symbols is much the same, but using the ma-
chinery of Proposition 19.12 rather than of Proposition 19.13. Details are left to the
reader.
In the remaining sections of this chapter we apply the interpolation theorem to
prove two results about theories: one about the conditions under which the union
of two theories is satisﬁable, the other about the conditions under which deﬁni-
tions are consequences of theories. (Throughout ‘theory’ is being used, as else-
where in this book, in a very broad way: A theory in a language is just a set of
sentences of the language that contains every sentence of that language that is a
logical consequence of the set. A theorem of a theory is just a sentence in that
theory.)

264
THE CRAIG INTERPOLATION THEOREM
20.2 Robinson’s Joint Consistency Theorem
We begin with a preliminary result.
20.5 Lemma. The union T1 ∪T2 of two theories T1 and T2 is satisﬁable if and only if
there is no sentence in T1 whose negation is in T2.
Proof: The ‘only if’ part is obvious: if there were a sentence in T1 whose negation
was T2, the union could not possibly be satisﬁable; for there could be no interpretation
in which both the sentence and its negation were true.
The ‘if’ part follows quickly from the compactness theorem and Craig’s theorem:
Suppose the union of T1 and T2 is unsatisﬁable. Then by the compactness theorem,
there is a ﬁnite subset S0 of the union which is unsatisﬁable. If there are no members
of S0 that belong to T1, then T2 is unsatisﬁable, and so ∀x x = x is a sentence in T1
whose negation is in T2; if no members of S0 belong to T2, then T1 is unsatisﬁable,
and so ∼∀x x = x is a sentence in T1 whose negation is in T2. So we may suppose
that S0 contains some members both of T1 and of T2. Let F1, . . . , Fm be the members
of S0 that are in T1; let G1, . . . , Gn be the members of S0 that are in T2.
Let A be (F1 & . . . & Fm), and let C be ∼(G1 & . . . & Gn). A implies C. By
Craig’s theorem, there is a sentence B implied by A, implying C, and containing
only nonlogical symbols contained in both A and C. B is therefore a sentence in
the languages of both T1 and T2. Since A is in T1 and implies B, B is in T1. Since
(G1 & . . . & Gn) is in T2, so is ∼B, as (G1 & . . . & Gn) implies ∼B. So B is a
sentence in T1 whose negation is in T2.
An extension T ′ of a theory T is just another theory containing it. The extension
is called conservative if every sentence of the language of T that is a theorem of T ′
is a theorem of T . We next prove a theorem about conservative extensions.
20.6 Theorem. Let L0, L1, L2 be languages, with L0 = L1 ∩L2. Let Ti be a theory
in Li for i = 0, 1, 2. Let T3 be the set of sentences of L1 ∪L2 that are consequences
of T1 ∪T2. Then if T1 and T2 are both conservative extensions of T0, then T3 is also a
conservative extension of T0.
Proof: Suppose B is a sentence of L0 that is a theorem of T3. We must show that
B is a theorem of T0. Let U2 be the set of sentences of L2 that are consequences of
T2 ∪{∼B}. Since B is a theorem of T3, T1 ∪T2 ∪{∼B} is unsatisﬁable, and therefore
T1 ∪U2 is unsatisﬁable. Therefore there is a sentence D in T1 whose negation ∼D
is in U2. D is a sentence of L1, and ∼D of L2. Thus D and ∼D are both in L0, and
hence so is (∼B →∼D). Since D is in T1, which is a conservative extension of T0,
D is in T0. And since ∼D is in U2, (∼B →∼D) is in T2, which is a conservative
extension of T0. Thus (∼B →∼D) is also in T0, and therefore so is B, which follows
from D and (∼B →∼D).
An immediate consequence is
20.7 Corollary (Robinson’s joint consistency theorem). Let L0, L1, L2 be languages,
with L0 = L1 ∩L2. Let Ti be a theory in Li for i = 0, 1, 2. If T0 is complete, and T1 and
T2 are satisﬁable extensions of T0, then T1 ∪T2 is satisﬁable.

20.3. BETH’S DEFINABILITY THEOREM
265
Proof: A satisﬁable extension of a complete theory is conservative, and a conserva-
tive extension of a satisﬁable theory is satisﬁable. Thus if the Ti satisfy the hypotheses
of Corollary 20.7, then T3 as deﬁned in Theorem 20.6 is a satisﬁable extension of T0,
and therefore T1 ∪T2 is satisﬁable.
20.8 Example (Failures of joint consistency). Let L0 = L1 = L2 = {P, Q}, where P and
Q are one-place predicates. Let T1 (respectively, T2) be the set of consequences in L1
(respectively, L2) of {∀x Px, ∀x Qx} (respectively, {∀x Px, ∀x ∼Qx}. Let T0 be the set of
consequences in L0 of ∀x Px. Then T1 ∪T2 is not satisﬁable, though each of T1 and T2 is
a satisﬁable extension of T0. This is not a counterexample to Robinson’s theorem, because
T0 is not complete. If instead we let L0 = {P}, then again we do not get a counterexample,
because then L0 is not the intersection of L1 and L2, while L is. This shows the hypotheses
in Corollary 20.7 are needed.
We have proved Robinson’s theorem using Craig’s theorem. Robinson’s theorem
can also be proved a different way, not using Craig’s theorem, and then used to prove
Craig’s theorem. Let us indicate how a ‘double compactness’ argument yields Craig’s
theorem from Robinson’s.
Suppose A implies C. Let L1 (L2) be the language consisting of the nonlogical
symbols occurring in A (C). Let L0 = L1 ∩L2. We want to show there is a sentence
B of L0 implied by A and implying C. Let  be the set of sentences of L0 that are
implied by A. We ﬁrst show that  ∪{∼C} is unsatisﬁable. Suppose that it is not and
that M is a model of  ∪{∼C}. Let T0 be the set of sentences of L0 that are true in M.
T0 is a complete theory whose langauge is L0. Let T1 (T2) be the set of sentences of
L1 (L2) that are consequences of T0 ∪{A} (T0 ∪{∼C}). T2 is a satisﬁable extension
of T0: M is a model of T0 ∪{∼C}, and hence of T2. But T1 ∪T2 is not satisﬁable: any
model of T1 ∪T2 would be a model of {A, ∼C}, and since A implies C, there is no
such model. Thus by the joint consistency theorem, T1 is not a satisﬁable extension
of T0, and therefore T0 ∪{A} is unsatisﬁable. By the compactness theorem, there is a
ﬁnite set of sentences in T0 whose conjunction D, which is in L0, implies ∼A. Thus
A implies ∼D, ∼D is in L0, ∼D is in , and ∼D is therefore true in M. But this is
a contradiction, as all of the conjuncts of D are in T0 and are therefore true in M. So
 ∪{∼C} is unsatisﬁable, and by the compactness theorem again, there is a ﬁnite
set of members of  whose conjunction B implies C. B is in L0, since its conjuncts
are, and as A implies each of these, A implies B.
20.3 Beth’s Deﬁnability Theorem
Beth’s deﬁnability theorem is a result about the relation between two different ex-
plications, or ways of making precise, the notion of a theory’s giving a deﬁnition of
one concept in terms of other concepts. As one might expect, each of the explica-
tions discusses a relation that may or may not hold between a theory, a symbol in the
language of that theory (which is supposed to ‘represent’ a certain concept), and other
symbols in the language of the theory (which ‘represent’ other concepts), rather than
directly discussing a relation that may or may not hold between a theory, a concept,

266
THE CRAIG INTERPOLATION THEOREM
and other concepts. The supposition of Beth’s theorem, then, is that α and β1, . . . , βn
are nonlogical symbols of the language L of some theory T and that α is not among
the βi.
The ﬁrst explication is straightforward and embodies the idea that a theory deﬁnes
a concept in terms of others when ‘a deﬁnition of that concept in terms of the others
is a consequence of the theory’. This sort of deﬁnition is called an explicit deﬁnition:
we say that α is explicitly deﬁnable in terms of the βi in T if a deﬁnition of α from
the βi is one of the sentences in T. What precisely is meant by a deﬁnition of α in
terms of the βi depends on whether α is a predicate or a function symbol. In the case
of a (k + 1)-place predicate, such a deﬁnition is a sentence of the form
∀x0∀x1 · · · ∀xk(α(x0, x1, . . . , xk) ↔B(x0, . . . , xk))
and in case of a k-place function symbol, such a deﬁnition is a sentence of the form
∀x0∀x1 · · · ∀xk(x0 = α(x1, . . . , xk) ↔B(x0, . . . , xk))
where in either case B is a formula whose only nonlogical symbols are among the βi.
(Constants may be regarded as 0-place function symbols, and do not require separate
discussion. In this case the right side of the biconditional would simply be x0 = α.)
The general form of a deﬁnition may be represented as
∀x0 · · · ∀xk(—α, x0, . . . , xk— ↔B(x0, . . . , xk)).
The second explication is rather more subtle, and incorporates the idea that a theory
deﬁnes a concept in terms of others if ‘any speciﬁcation of the universe of discourse
of the theory and the meanings of the symbols representing the other concepts (that
is compatible with the truth of all the sentences in the theory) uniquely determines
the meaning of the symbol representing that concept’. This sort of deﬁnition is called
implicit deﬁnition: we say that α is implicitly deﬁnable from the βi in T if any two
models of T that have the same domain and agree in what they assign to the βi also
agree in what they assign to α.
It will be useful to develop a more ‘syntactic’ reformulation of this ‘semantic’
deﬁnition of implicit deﬁnability. To this end, we introduce a new language L′ ob-
tained from L by replacing every nonlogical symbol γ of L, other than the βi, by a
new symbol γ ′ of the same kind: 17-place function symbols are replaced by 17-place
function symbols, 59-place predicates by 59-place predicates, and so on.
Given two models M and N of T that have the same domain and agree on what
they assign to the βi, we let M+ N be the interpretation of L ∪L′ that has the same
domain, and assigns the same denotations to the βi, and, for any other nonlogical
symbol γ of L, assigns to γ what M assigns to γ , and assigns to γ ′ what N assigns
to γ . Then M+ N is a model of T ∪T ′.
Conversely, if K is a model of T ∪T ′, then K can clearly be ‘decomposed’ into
two models M and N of T , which have the same domain (as each other, and as K)
and agree (with each other and with K) on what they assign to the βi, where for any
other nonlogical symbol γ of L, what M assigns to γ is what K assigns to γ , and
what N assigns to γ is what K assigns to γ ′.

20.3. BETH’S DEFINABILITY THEOREM
267
20.9 Lemma. α is implicitly deﬁnable from β1, . . . , βn in T if and only if
∀x0 · · · ∀xk(—α, x0, . . . , xk— ↔—α′, x0, . . . , xk—)
(1)
is a consequence of T ∪T ′.
Proof: Here, of course, by —α′, x0, . . . , xk— is meant the result of substituting α′
for α in —α, x0, . . . , xk—. Note that (1) will be true in a given interpretation if and
only if that interpretation assigns the same denotation to α and to α′.
For the left-to-right direction, suppose α is implicitly deﬁnable from the βi in T .
Suppose K is a model of T ∪T ′. Let M and N be the models into which K can be
decomposed as above, so that K = M + N. Then M and N have the same domain
and agree on what they assign to the βi. By the supposition of implicit deﬁnability,
they must therefore agree on what they assign to α. Therefore the biconditional (1)
is true in K. In other words, any model of T ∪T ′ is a model of (1), which therefore
is a consequence of T ∪T ′.
For the right-to-left direction, suppose that (1) follows from T ∪T ′. Suppose M
and N are models of T that have the same domain and agree on what they assign to
the βi. Then M + N is a model of T ∪T ′ and therefore of (1), by the supposition that
(1) is a consequence of T ∪T ′. It follows that M + N assigns the same denotation
to α and α′, and therefore that M and N assign the same denotation to α. Thus α is
implicitly deﬁnable from the βi in T .
One direction of the connection between implicit and explicit deﬁnability is now
easy.
20.10 Proposition (Padoa’s method). If α is not implicitly deﬁnable from the βi in T ,
then α is not explicitly deﬁnable from the βi in T .
Proof: Suppose α is explicitly deﬁnable from the βi in T . Then some deﬁnition
∀x0 · · · ∀xk(—α, x0, . . . , xk— ↔B(x0, . . . , xk))
(2)
of α from the βi is in T . Therefore
∀x0 · · · ∀xk(—α′, x0, . . . , xk— ↔B(x0, . . . , xk))
(3)
is in T ′. (Recall that B involves only the βi, which are not replaced by new nonlogical
symbols.) Since (1) of Lemma 20.9 is a logical consequence of (2) and (3), it is a
consequences of T ∪T ′, and by that lemma, α is implicitly deﬁnable from the βi
in T .
20.11 Theorem (Beth’s deﬁnability theorem). α is implicitly deﬁnable from the βi in
T if and only if α is explicitly deﬁnable from the βi in T .
Proof: The ‘if’ direction is the preceding proposition, so it only remains to prove
the ‘only if’ direction. So suppose α is implicitly deﬁnable from the βi in T. Then
(1) of Lemma 20.9 is a consequence of T ∪T ′. By the compactness theorem, it is a
consequence of some ﬁnite subset of T ∪T ′. By adding ﬁnitely many extra sentences
to it, if necessary, we can regard this ﬁnite subset as T0 ∪T ′
0, where T0 is a ﬁnite subset
of T , and T ′
0 comes from T0 on replacing each nonlogical symbol γ other than the βi

268
THE CRAIG INTERPOLATION THEOREM
by γ ′. Let A(A′) be the conjunction of the members of T0 (T ′
0). Then (1) is implied by
A & A′ Let c0, . . . , ck be constants not occurring in T ∪T ′, and hence not in A, A′,
—α, x0, . . . , xk—, or —α′, x0, . . . , xk—. Then
—α, c0, . . . , ck— ↔—α′, c0, . . . , ck—
is a consequence of (1) and therefore of A & A′. Here of course by —α, c0, . . . , ck—
is meant the result of substituting ci for xi in —α, x0, . . . , xk—for all i, and similarly
for —α′, c0, . . . , ck—. It follows that
(A & A′) →(—α, c0, . . . , ck— ↔—α′, c0, . . . , ck—)
is valid, and hence that
A & —α, c0, . . . , ck—
(4)
implies
A′ →—α′, c0, . . . , ck—.
(5)
We now apply the Craig interpolation lemma. It tells us that there is a sentence
B(c0, . . . , ck) implied by (4) and implying (5), such that the nonlogical symbols of
B are common to (4) and (5). This means that they can include only the ci, which
we have displayed, and the βi. Since (4) implies B(c0, . . . , ck), A and therefore T
implies
—α, c0, . . . , ck— →B(c0, . . . , ck)
and since the ci do not occur in T , this means T implies
∀x0 · · · ∀xk(—α, x0, . . . , xk— →B(x0, . . . , xk)).
(6)
Since B(c0, . . . , ck) implies (5), A′ and therefore T ′ implies
B(c0, . . . , ck) →—α′, c0, . . . , ck—
and since the ci do not occur in T ′, this means T ′ implies
∀x0 · · · ∀xk(B(x0, . . . , xk) →—α′, x0, . . . , xk—).
Replacing each symbol γ ′ by γ ′, it follows that T implies
∀x0 · · · ∀xk(B(x0, . . . , xk) →—α, x0, . . . , xk—).
(7)
But (6) and (7) together imply, and therefore T implies, the explicit deﬁnition
∀x0 · · · ∀xk(—α, x0, . . . , xk— ↔B(x0, . . . , xk)).
Thus, α is explicitly deﬁnable from the βi in T , and Beth’s theorem is proved.
Problems
20.1 (Lyndon’s interpolation theorem) Let A and C be sentences without constants
or function symbols and in negation-normal form. We say that an occurrence of
a predicate in such a sentence is positive if it is not preceded by ∼, and negative

PROBLEMS
269
if it is preceded by ∼. Show that if A implies C, and neither ∼A nor C is valid,
then there is another such sentence B such that: (i) A implies B; (ii) B implies
C; (iii) any predicate occurs positively in B only if it occurs positively in both
A and C, and occurs negatively in B if and only if it occurs negatively in both
A and C.
20.2 Give an example to show that Lyndon’s theorem does not hold if constants are
present.
20.3 (Kant’s theorem on the indeﬁnability of chirality). For points in the plane, we
say y is between x and z if the three points lie on a straight line and y is between
x and z on that line. We say w and x and y and z are equidistant if the distance
from w to x and the distance from y to z are the same. We say x and y and z form
a right-handed triple if no two distances between different pairs of them are the
same, and traversing the shortest side, then the middle side, then the longest side
of the triangle having them as vertices takes one around the triangle clockwise,
as on the right in Figure 20-1.
Figure 20-1. Right and left handed triangles.
Show that right-handedness cannot be deﬁned in terms of betweenness and
equidistance. (More formally, consider the language with a three-place predi-
cate P, a four-place predicate Q, and a three-place predicate R; consider the
interpretation whose domain is the set of the points in the plane and that assigns
betweenness and equidistance and right-handedness as the denotations of P and
Q and R; and ﬁnally consider the theory T whose theorems are all the sentences
of the language that come out true under this interpretation. Show that R is not
deﬁnable in terms of P and Q in this theory.)

21
Monadic and Dyadic Logic
We have given in earlier chapters several different proofs of Church’s theorem to the
effect that ﬁrst-order logic is undecidable: there is no effective procedure that applied
to any ﬁrst-order sentence will in a ﬁnite amount of time tell us whether or not it is
valid. This negative result leaves room on the one hand for contrasting positive results,
and on the other hand for sharper negative results. The most striking of the former is
the L¨owenheim–Behmann theorem, to the effect that the logic of monadic (one-place)
predicates is decidable, even when the two-place logical predicate of identity is admitted.
The most striking of the latter is the Church–Herbrand theorem that the logic of a single
dyadic (two-place) predicate is undecidable. These theorems are presented in sections
21.2 and 21.3 after some general discussion of solvable and unsolvable cases of the
decision problem for logic in section 21.1. While the proof of Church’s theorem requires
the use of considerable computability theory (the theory of recursive functions, or of
Turing machines), that is not so for the proof of the L¨owenheim–Behmann theorem or for
the proof that Church’s theorem implies the Church–Herbrand theorem. The former uses
only material developed by Chapter 11. The latter uses also the elimination of function
symbols and identity from section 19.4, but nothing more than this. The proofs of these
two results, positive and negative, are independent of each other.
21.1 Solvable and Unsolvable Decision Problems
Let K be some syntactically deﬁned class of ﬁrst-order sentences. By the decision
problem for K is meant the problem of devising an effective procedure that, applied
to any sentence S in K, will in a ﬁnite amount of time tell us whether or not S is
valid. Since S is valid if and only if ∼S is not satisﬁable, and S is satisﬁable if and
only if ∼S is not valid, for any class K that contains the negation of any sentence it
contains, the decision problem for K is equivalent to the satisﬁability problem for K,
the problem of devising an effective procedure that, applied to any sentence S in K,
will in a ﬁnite amount of time tell us whether or not S is satisﬁable, or has a model.
The formulation in terms of satisﬁability turns out to be the more convenient for our
purposes in this chapter.
The most basic result in this area is a negative one, Church’s theorem, which asserts
the unsolvability of the satisﬁability problem full ﬁrst-order logic, where K is the class
of all sentences. We have given three different proofs of this result, two in Chapter 11
and another in section 17.1; but none of the machinery from any of these proofs of
270

21.1. SOLVABLE AND UNSOLVABLE DECISION PROBLEMS
271
Church’s theorem need be recalled for purposes of this chapter. We are going to prove
sharper results than Church’s theorem, to the effect that the satisﬁability problem is
unsolvable for narrower classes K than the class of all ﬁrst-order sentences; but in
no case will we prove these sharper results by going back to the proof of Church’s
theorem and sharpening the proof. Instead, we are simply going to prove that if the
satisﬁability problem for K were solvable, then the satisﬁability problem for full
ﬁrst-order logic would be solvable, as Church’s theorem tells us it is not. And we
are going to prove this simply by showing how one can effectively associate to any
arbitrary sentence a sentence in K that is equivalent to it for satisﬁability.
We have in fact already done this in one case in section 19.4, where we showed
how one can effectively associate to any arbitrary sentence a sentence of predicate
logic (that is, one not involving constants or function symbols), and indeed one of
predicate logic without identity, that is equivalent to it for satisﬁability. Thus we have
already proved the following slight sharpening of Church’s theorem.
21.1 Lemma. The satisﬁability problem for predicate logic without identity is un-
solvable.
Sharper results will be obtained by considering narrower classes of sentences:
dyadic logic, the part of predicate logic without identity where only two-place pred-
icates are allowed; the logic of a triadic predicate, where only a single three-place
predicate is allowed; and ﬁnally the logic of a dyadic predicate, where only a single
two-place predicate is allowed. Section 21.3 will be devoted to proving the following
three results.
21.2 Lemma. The satisﬁability problem for dyadic logic is unsolvable.
21.3 Lemma. Thesatisﬁabilityproblemforthelogicofatriadicpredicateisunsolvable.
21.4 Theorem (The Church–Herbrand theorem). The satisﬁability problem for the
logic of a dyadic predicate is unsolvable.
Let us now turn to positive results. Call a sentence n-satisﬁable if has a model of
some size m ≤n. Now note three things. First, we know from section 12.2 that if a
sentence comes out true in some interpretation of size m, then it comes out true in
some interpretation whose domain is the set of natural numbers from 1 to m. Second,
for a given ﬁnite language, there are only ﬁnitely many interpretations whose domain
is the set of natural numbers from 1 to m. Third, for any given one of them we
can effectively determine for any sentence whether or not it comes out true in that
interpretation.
[It is easy to see this last claim holds for quantiﬁer-free sentences: the speciﬁcation
of the model tells us which atomic sentences are true, and then we can easily work out
whether a given truth-functional compound of them is true. Perhaps the easiest way to
see the claim holds for all sentences is to reduce the general case to the special case of
quantiﬁer-free sentences. To do so, for each 1 ≤k ≤m add to the language a constant k
denoting k. To any sentence A of the expanded language we can effectively associate
a quantiﬁer-free sentence A* as follows. If A is atomic, A* is A. If A is a truth-
functional compound, then A* is the same compound of the quantiﬁer-free sentences

272
MONADIC AND DYADIC LOGIC
associated with the sentences out of which it is compounded. For instance, (B & C)* is
B* & C*, and analogously for ∨. If A is ∀x F(x), then A* is (F(1)& . . . &F(m))*, and
analogously for ∃. Then A comes out true in the interpretation if and only if A* does.]
Putting our three observations together, we have proved the following.
21.5 Lemma. For each n, the n-satisﬁability problem for ﬁrst-order logic is solvable.
To show that the decision problem for a class K is solvable, it is sufﬁcent to show
how one can effectively calculate for any sentence S in K a number n such that if
S has a model at all, then it has a model of size ≤n. For if this can be shown, then
for K the satisﬁability problem is reduced to the n-satisﬁability problem. The most
basic positive result that can be proved in this way concerns monadic logic, where
only one-place predicates are allowed.
21.6 Theorem. The decision problem for monadic logic is solvable.
A stronger result concerns monadic logic with identity, where in addition to one-
place predicates, the two-place logical predicate of identity is allowed.
21.7 Theorem. The decision problem for monadic logic with identity is solvable.
These results are immediate from the following lemmas, whose proofs will occupy
section 21.2.
21.8 Lemma. If a sentence involving only monadic predicates is satisﬁable, then it has
a model of size no greater than 2k, where k is the number of predicates in the sentence.
21.9 Lemma. If a sentence involving only n monadic predicates and identity is satis-
ﬁable, then it has a model of size no greater than 2k ·r, where k is the number of monadic
predicates and r the number of variables in the sentence.
Before launching into the proofs, some brief historical remarks may be in order.
The ﬁrst logician, Aristotle, was concerned with arguments such as
All horses are mammals.
All mammals are animals.
Therefore, all horses are animals.
The form of such an argument would in modern notation be represented using one-
place predicates. Later logicians down through George Boole in the middle 19th
century considered more complicated arguments, but still ones involving only one-
place predicates. The existence had been noticed of intuitively valid arguments in-
volving many-place predicates, such as
All horses are animals.
Therefore, all who ride horses ride animals.
But until the later 19th century, and especially the work of Gottlob Frege, logi-
cians did not treat such arguments systematically. The extension of logic beyond the
monadic to the polyadic is indispensable if the forms of arguments used in mathemat-
ical proofs are to be represented, but the ability of contemporary logic to represent

21.2. MONADIC LOGIC
273
the forms of such arguments comes at a price, namely, that of the undecidability
of the contemporary notions of validity and satisﬁability. For, as the results listed
above make plain, undecidability sets in precisely when two-place predicates are
allowed.
21.2 Monadic Logic
Let us get straight to work.
Proof of Lemma 21.9: Let S be a sentence of monadic logic with identity involving
k one-place predicates (possibly k = 0) and r variables. Let P1, . . . , Pk be predicates
and v1, . . . , vr the variables. Suppose M is a model of S.
For each d in the domain M = |M| let the signature σ(d) of d be the sequence
( j1, . . . , jk) whose ith entry ji is 1 or 0 according as PM
i
does or does not hold
of d [if k = 0, then σ(d) is the empty sequence ( )]. There are at most 2k possible
signatures. Call e and d similar if they have the same signature. Clearly similarity is
an equivalence relation. There are at most 2k equivalence classes.
Now let N be a subset of M containing all the elements of any equivalence class
that has ≤r elements, and exactly r elements of any equivalence class that has ≥r
elements. Let N be the subinterpretation of M with domain |N| = N. Then N has
size ≤2k ·r. To complete the proof, it will sufﬁce to prove that N is a model of S.
Towards this end we introduce an auxiliary notion. Let a1, . . . , as and b1, . . . , bs
be sequences of elements of M. We say they match if for each i and j between 1
and n, ai and bi are similar, and ai = a j if and only if bi = b j. We claim that if
R(u1, . . . , us) is a subformula of S (which implies that s ≤r and that each of the us
is one of the vs) and a1, . . . , as and b1, . . . , bs are matching sequences of elements
of M, with the bi all belonging to N, then the ai satisfy R in M if and only if the bi
satisfy R in N. To complete the proof it will sufﬁce to prove this claim, since, applied
with s = 0, it tells us that since S is true in M, S is true in N, as desired.
The proof of the claim is by induction on complexity. If R is atomic, it is either of
the form Pj(ui) or of the form ui = u j. In the former case, the claim is true because
matching requires that ai and bi have the same signature, so that PM
j
holds of the
one if and only if it holds of the other. In the latter case, the claim is true because
matching requires that ai = a j if and only if bi = b j.
If R is of form ∼Q, then the as satisfy R in M if and only if they do not satisfy
Q, and by the induction hypothesis the as fail to satisfy Q in M if and only if the bs
fail to satisfy Q in N, which is the case if and only if the bs satisfy R in N, and we
are done. Similarly for other truth-functional compounds.
It remains to treat the case of universal quantiﬁcation (and of existential quantiﬁ-
cation, but that is similar and is left to the reader). So let R (u1, . . . , us) be of form
∀us+1Q(u1, . . . , us, us+1), where s + 1 ≤r and each of the us is one of the vs. We
need to show that a1, . . . , as satisfy R in M (which is to say that for any as+1 in
M, the longer sequence of elements a1, . . . , as, as+1 satisﬁes Q in M) if and only if
b1, . . . , bs satisfy R in N (which is to say that for all bs+1 in N, the longer sequence
of elements b1, . . . , bs, bs+1 satisﬁes Q in N). We treat the ‘if’ direction and leave
the ‘only if’ direction to the reader.

274
MONADIC AND DYADIC LOGIC
Our induction hypothesis is that if a1, . . . , as, as+1 and b1, . . . , bs, bs+1 match,
then a1, . . . , as, as+1 satisfy Q in M if and only if b1, . . . , bs, bs+1 satisfy Q in N.
What we want to show is that if b1, . . . , bs, bs+1 satisfy Q in N for all bs+1 in N,
then a1, . . . , as, as+1 satisfy Q in M for all as+1 in M. Therefore it will be enough
to show that if a1, . . . , as and b1, . . . , bs match, where s <r, then for any as+1 in M
there is a bs+1 in N such that a1, . . . , as, as+1 and b1, . . . , bs, bs+1 match.
In the degenerate case where as+1 is identical with one of the previous ai, we may
simply take bs+1 to be identical with the corresponding bi. In the non-degenerate case,
as+1 belongs to some equivalence class C and is distinct from any and all previous ai
that belong to C. Let the number of such ai be t (where possibly t = 0), so that there
are at least t + 1 elements in C, counting as+1. To ensure matching, it will sufﬁce
to choose bs+1 to be some element of C that is distinct from any and all previous bi
that belong to C. Since a1, . . . , as and b1, . . . , bs match, the number of such bi will
also be t. Since t ≤s < r, and there are at least t + 1 ≤r elements in C, there will
be at least that many elements of C in N, and so we can ﬁnd an appropriate bs+1, to
complete the proof.
21.10 Corollary. If a sentence involving no nonlogical symbols (but only identity) is
satisﬁable, then it has a model of size no greater than r, where r is the number of variables
in the sentence.
21.11 Corollary. If a sentence of monadic logic involving only one variable is satis-
ﬁable, then it has a model of size no greater than 2k, where k is the number of monadic
predicates in the sentence.
Proofs: These are simply the cases k = 0 and r = 1 of Lemma 21.9.
Proof of Lemma 21.7: This is immediate from Corollary 21.11 and the following, which
is a kind of normal form theorem.
21.12 Lemma. Any sentence of monadic logic without identity is logically equivalent
to one with the same predicates and only one variable.
Proof: Call a formula clear if in any subformula ∀x B(x) or ∃x B(x) that begins
with a quantiﬁer, no variable other than the variable x attached to the quantiﬁer
appears in F. Thus ∀x∃y(Fx & Gy) is not clear, but ∀x Fx & ∃yGy is clear. To prove
the lemma, we show how one can inductively associate to any formula A of monadic
logic without identity an equivalent formula A© with the same predicates that is clear
(as in our example the ﬁrst formula is equivalent to the second). We then note that any
clear sentence is equivalent to the result of rewriting all its variables to be the same (as
in our example the second sentence is equivalent to ∀zFz & ∃zGz). The presence of
identity would make such clearing impossible. (There is no clear sentence equivalent
to ∀x∃y x ̸= y, for instance.)
To an atomic formula we associate itself. To a truth-functional compound of for-
mulas to which clear equivalents have been associated, we associate the same truth-
functional compound of those equivalents. Thus (B ∨C)© is B© ∨C©, for instance,
and analogously for &. The only problem is how to deﬁne the associate (∃x B(x))©

21.3. DYADIC LOGIC
275
to a quantiﬁed formula ∃x B(x) in terms of the associate (B(x))© of the subformula
B(x), and analogously for ∀.
∃xB(x) will of course be equivalent to ∃x(B(x))©. And (B(x))© will be a truth-
functional compound of clear formulas A1, . . . , An, each of which either is atomic or
begins with a quantiﬁer. Consider a formula equivalent to (B(x))© that is in disjunctive
normal form in the Ai. It will be a disjunction B1 ∨· · · ∨Br of formulas B j, each of
which is a conjunction of some of the Ai and their negations. We may assume each
B j has the form
C j,1 & . . . & C j,r & D j,1 & . . . & D j,s
where the Cs are the conjuncts in which the variable x does occur and the Ds those in
which it does not; by clarity, the Ds will include all conjuncts that begin with or are
the negations of formulas beginning with quantiﬁers, and the Cs will all be atomic.
Then as ∃x(B(x))© we may take the disjunction B′
1 ∨· · · ∨B′
r, where B′
j is
∃x(C j,1 & · · · & C j,r) & D j,1 & . . . & D j,s.
(In the degenerate case where r = 0, B′
j is thus the same as B j.)
21.3 Dyadic Logic
Again we go straight to work.
Proof of Lemma 21.2: Lemma 21.1 tells us the satisﬁability problem is unsolvable
for predicate logic, and we want to show it is unsolvable for dyadic logic. It will be
enough to show how one can effectively associate to any sentence of predicate logic a
sentence of dyadic logic such that the former will be satisﬁable if and only if the latter
is. What we are going to do is to show how to eliminate one three-place predicate
(at the cost of introducing new two- and one-place predicates). The same method
will work for k-place predicates for any k ≥3, and applying it over and over we
can eliminate all but two- and one-place predicates. The one-place ones can also be
eliminated one at a time, since given a sentence S containing a one-place predicate
P, introducing a new two-place predicate P* and replacing each atomic subformula
Px by P*xx clearly produces a sentence S* that is satisﬁable if and only if S is. Thus
we can eliminate all but two-place predicates.
To indicate the method for eliminating a three-place predicate, let S be a sentence
containing such a predicate P. Let P* be a new one-place predicate, and Qi for
i = 1, 2, 3 a trio of new two-place predicates. Let w be a variable not appearing in
S, and let S* be the result of replacing each atomic subformula of form Px1x2x3
in S by
∃w(Q1wx1 & Q2wx2 & Q3wx3 & P∗w).
We claim S is satisﬁable if and only if S* is satisﬁable. The ‘if’ direction is easy.
For if S is unsatisﬁable, then ∼S is valid, and substitution (of a formula with the
appropriate free variables for a predicate) preserves validity, so ∼S* is valid, and S*
is unsatisﬁable.

276
MONADIC AND DYADIC LOGIC
For the ‘only if’ direction, suppose S has a model M. By the canonical domains
theorem(Corollary12.18)wemaytakethedomainofMbethesetofnaturalnumbers.
We want to show that S* has a model M*. We will take M* to have domain the
set of natural numbers, and to assign to every predicate in S other than P the same
denotation that M assigns. It will sufﬁce to show that we can assign denotations to
P* and the Qi in such a way that natural numbers a1, a2, a3 will satisfy ∃w(Q1wx1 &
Q2wx2 & Q3wx3 & P∗w) in M* if and only if they satisfy Px1x2x3 in M. To achieve
this, ﬁx a function f from the natural numbers onto the set of all triples of natural
numbers. It then sufﬁces to take as the denotation of P* in M* the relation that holds
of a number b if and only if f (b) is a triple a1, a2, a3 for which the relation that is the
denotation of P in M holds, and to take as the denotation of Qi in M* the relation
that holds of b and a if and only if a is the ith component of the triple f (b).
Proof of Lemma 21.3: We want next to show that we can eliminate any number
of two-place predicates P1, . . . , Pk in favour of a single three-place predicate Q. So
given a sentence S containing the Pi, let u1, . . . , uk be variables not occurring in S,
and let S* be the result of replacing each atomic subformula of form Pix1x2 in S by
Qvix1x2, and let S† be the result of preﬁxing S* by ∃v1 · · · ∃vk. For instance, if S is
∀x∃y(P2yx & ∀z(P1xz & P3zy))
then S† will be
∃v1∃v2∃v3∀x∃y(Qv2yx & ∀z(Qv1xz & Qv3zy)).
We claim S is satisﬁable if and only if S† is satisﬁable. As in the preceding proof,
the ‘if’ direction is easy, using the fact that substitution preserves validity. (More
explicitly, if there is a model M† of S†, some elements a1, . . . , ak of its domain
satisfy the formula S*. We can now get a model M of S by taking the same domain,
and assigning as denotation to Pi in M the relation that holds between b1 and b2
if and only if the relation that is the denotation of Q in M† holds among ai and b1
and b2.)
For the ‘only if’ direction, suppose M is a model of S. As in the preceding proof,
we may take the domain of M to be the set of natural numbers, using the canonical
domains theorem. We can now get a model M† of S†, also with domain the natural
numbers, by taking as the denotation of Q in M† the relation that holds among natural
numbers a and b1 and b2 if and only if 1 ≤a ≤k, and as the denotation of Pa in M
the relation that holds between b1 and b2. From the fact that M is a model of S, it
follows that 1, . . . , k satisfy S* in M†, and hence S† is true in M†.
Proof of Theorem 21.4: We want next to show that we can eliminate a single three-
place predicate P in favour of a single two-place predicate Q. So given a sentence
S containing the P, let u1, u2, u3, u4 be variables not occurring in S, and let S* be
the result of replacing each atomic subformula of form Pix1x2x3 in S by a certain
formula P∗(x1, x2, x3), namely
∃u1∃u2∃u3∃u4 (∼Qu1u1 & Qu1u2 & Qu2u3 & Qu3u4
& Qu4u1 & Qu1x1 & Qu2x2 & Qu3x3 & ∼Qx1u2 & ∼Qx2u3 & ∼Qx3u4 & Qu4x1).

21.3. DYADIC LOGIC
277
We then claim S is satisﬁable if and only if S* is satisﬁable. As in the preceding
proofs, the ‘if’ direction is easy, and for the ‘only if’ direction what we need to do
is to show, given a model M of S, which may be taken to have domain the natural
numbers, that we can deﬁne an interpretation M*, also with domain the natural
numbers, which will assign as denotation to Q in M* a relation such that for any
natural numbers b1, b2, b3, those numbers will satisfy P∗(x1, x2, x3) in M* if and
only if those numbers satisfy P(x1, x2, x3) in M. To accomplish this last and so
complete the proof, it will be enough to establish the following lemma.
21.13 Lemma. Let R be a three-place relation on the natural numbers. Then there is
a two-place relation S on the natural numbers such that if a, b, c are any natural numbers,
then we have Rabc if and only if for some natural numbers w, x, y, z we have
∼Sww & Swx & Sxy & Syz & Szw &
Swa & Sxb & Syc & ∼Sax & ∼Sby & ∼Scz & Sza.
(1)
Proof: One of the several ways of enumerating all triples of natural numbers is to
order them by their sums, and where these are the same by their ﬁrst components,
and where these also are the same by their second components, and where these also
are the same by their third components. Thus the ﬁrst few triples are
(0, 0, 0)
(0, 0, 1)
(0, 1, 0)
(1, 0, 0)
(0, 0, 2)
(0, 1, 1)
(0, 2, 0)
(1, 0, 1)
(1, 1, 0)
(2, 0, 0)
(0, 0, 3)
...
·
Counting the initial triple as the ﬁrst rather than the zeroth, it is clear that if the nth
triple is (a, b, c), then a, b, c are all <n. It follows that if w, x, y, and z are respectively
4n + 1, 4n + 2, 4n + 3, and 4n + 4, then a, b, c are all less than w −4, x −4, y −4,
and z −4. (For instance, a < n implies a + 1 ≤n, which implies 4a + 4 ≤4n <
4n + 1.)
Now to deﬁne S. If the nth triple is (a, b, c), we let Svu hold in each of the following
four cases:
v = 4n + 1
and
(u = 4n + 2 or u = a)
v = 4n + 2
and
(u = 4n + 2 or u = 4n + 3 or u = b)
v = 4n + 3
and
(u = 4n + 3 or u = 4n + 4 or u = c)
v = 4n + 4
and
(u = 4n + 4 or u = 4n + 1 or (u = a and Rabc)).

278
MONADIC AND DYADIC LOGIC
Svu is not to hold in any other cases. Note that
if Svu, then v + 1 ≥u
(2)
there is at most one u < v −4 such that Svu.
(3)
We must now show that Rabc holds if and only if there are w, x, y, z such that
(1) above holds. The ‘only if’ direction is immediate: if Rabc, take w, x, y, z to
be 4n + 1, 4n + 2, 4n + 3, 4n + 4, where (a, b, c) is the nth triple, and (1) will hold.
[∼Sax holds because a < x −4 holds, so a + 1 ≥x fails, so Sax fails by (2); similarly
for the other negations in (1).]
Now suppose (1) holds for some w, x, y, z. We must show that Rabc. To begin
with, (1) gives us ∼Sww, so w must be of the form 4n + 1 for some n ≥1.
Also, (1) gives us Swx, Sxy, Syz, Szw. Therefore, by (2), x + 3 ≥y + 2 ≥
z + 1 ≥w, whence x ≥w −3. Similarly, y ≥x −3 and z ≥y −3. So neither x <
w −4 nor y < x −4 nor z < y −4. Since Swx holds while x < w −4 fails, we
must have x = w + 1 = 4n + 2.
Also, since Sxy holds while y < x −4 fails, either y = x or y = x + 1. Similarly,
either z = y or z = y + 1. But if either y = x or z = y, then z = w + 1 = 4n + 2 or
z = w + 2 = 4n + 3. But this is impossible, since Svu never holds for u = 4m + 1
and v = 4n + 2 or 4n + 3, whereas we have Szw. It follows that y = x + 1 = 4n + 3
and z = y + 1 = 4n + 4.
If we can show that the nth triple is (a, b, c), then we can conclude that Rabc: for
if (a, b, c) is the nth triple, then Sza if and only if Rabc, and (1) gives Sza.
We have Swa, Swb, Syc and ∼Sax, ∼Sby, ∼Scz from (1). And since we know
w = 4n + 1, x = 4n + 2, y = 4n + 3, z = 4n + 4, we also have Sxx, Sxy, Syy, Syz,
and Szz from the deﬁnition of S. So a ̸= x, b ̸= x, b ̸= y, c ̸= y, and c ̸= z. So we
have Swa and a < w −4, Sxb and b < x −4, and Syc and c < y −4. If the nth triple
is (r, s, t), then we also have Swr, Sxs, Syt and r < w −4, s < x −4, t < y −4. So
by (3) above we must have r = a, s = b, and t = c. So (a, b, c) is the nth triple, and
the proof is complete.
Problems
21.1 Prove Lemma 21.8 directly, without deriving it from Lemma 21.9.
21.2 Show that the estimates 2k and 2k · r in Lemmas 21.8 and 21.9 cannot be
improved.
21.3 What happens if constants are added to monadic logic with identity?
21.4 The language of set theory has a single nonlogical symbol and two-place pre-
dicate ∈. ZFC is a certain theory in this language, of which it was asserted
towards the end of section 17.1 that it is ‘adequate for formalizing essen-
tially all accepted mathematical proofs’. What is the bearing of this fact on
Theorem 21.4?

22
Second-Order Logic
Suppose that, in addition to allowing quantiﬁcations over the elements of a domain,
as in ordinary ﬁrst-order logic, we allow also quantiﬁcation over relations and functions
on the domain. The result is called second-order logic. Almost all the major theorems
we have established for ﬁrst-order logic fail spectacularly for second-order logic, as
is shown in the present short chapter. This chapter and those to follow generally
presuppose the material in section 17.1. (They are also generally independent of each
other, and the results of the present chapter will not be presupposed by later ones.)
Let us begin by recalling some of the major results we have established for ﬁrst-order
logic.
The compactness theorem: If every ﬁnite subset of a set of sentences has a model,
the whole set has a model.
The (downward) L¨owenheim–Skolem theorem: If a set of sentences has a model,
it has an enumerable model.
The upward L¨owenheim–Skolem theorem: If a set of sentences has an inﬁnite
model, it has a nonenumerable model.
The (abstract) G¨odel completeness theorem: The set of valid sentences is semire-
cursive.
All of these results fail for second-order logic, which involves an extended notion
of sentence, with a corresponding extension of the notion of truth of a sentence in
an interpretation. In introducing these extended notions, we stress at the outset that
we change neither the deﬁnition of language nor the deﬁnition of interpretation:
a language is still an enumerable set of nonlogical symbols, and an interpretation
of a language is still a domain together with an assignment of a denotation to each
nonlogical symbol in the language. The only changes will be that we add some new
clauses to the deﬁnition of what it is to be a sentence of a language, and correspond-
ingly some new clauses to the deﬁnition of what it is for a sentence of a language to
be true in an interpretation.
What is a second-order sentence? Let us refer to what we have been calling ‘vari-
ables’ as individual variables. We now introduce some new kinds of variable: relation
variables and function variables. Just as we have one-, two-, three-, and more-place
predicates or relation symbols and function symbols, we have one-, two-, three-, and
more-place relation variables and function variables. (Since one-place relations are
279

280
SECOND-ORDER LOGIC
just sets, one-place relation variables may be called set variables.) We suppose that
no symbol of any sort is also a symbol of any other sort. We extend the deﬁnition of
formula by allowing relation or function variables to occur in those positions in formu-
las where previously only relation symbols (a.k.a. predicates) or function symbols
(respectively!) could occur, and also by allowing the new kinds of variable to oc-
cur after ∀and ∃in quantiﬁcations. Free and bound occurrences are deﬁned for
the new kinds of variable exactly as they were for deﬁned for individual variables.
Sentences, as always, are formulas in which no variables (individual, relation, or
function) occur free. A second-order formula, then, is a formula that contains at least
one occurrence of a relation or function variable, and a second-order sentence is a
second-order formula that is a sentence. A formula or sentence of a language, whether
ﬁrst- or second-order, is, as before, one whose nonlogical symbols all belong to the
language.
22.1 Example (Second-order sentences). (In the following examples we use u as a one-
place function variable, and X as a one-place relation variable.)
In ﬁrst-order logic we could identify a particular function as the identity function:
∀x f (x) = x. But in second-order logic we can assert the existence of the identity func-
tion: ∃u ∀x u(x) = x.
Similarly, where in ﬁrst-order logic we could assert that two particular indviduals share
a property (Pc & Pd), in second-order logic we can assert that every two individuals share
some property or other: ∀x∀y∃X(Xx & Xy).
Finally, in ﬁrst-order logic we can assert that if two particular individuals are identical,
then they must either both have or both lack a particular property: c = d →(Pc ↔Pd).
But in second-order logic we can deﬁne identity through Leibniz’s law of the identity of
indiscernibles: c = d ↔∀X (Xc ↔Xd).
Each of the three second-order sentences above is valid: true in each of its interpretations.
When is a second-order sentence S true in an interpretation M? We answer this
question by adding four more clauses (for universal and existential quantiﬁcations
involving relation and function variables) to the deﬁnition of truth in an interpreta-
tion given in section 9.3. For a universal quantiﬁcation ∀XF(X) involving a relation
variable, the clause reads as follows. First we deﬁne what it is for a relation R
(of the appropriate number of places) on the domain of M to satisfy F(X): R does
so if, on expanding the language by adding a new relation symbol P (of the appro-
priate number of places) to the language, and expanding the interpretation M to an
interpretation MP
R of the expanded language by taking R as the denotation of P, the
sentence F(P) becomes true. Then we deﬁne ∀XF(X) to be true in M if and only if
every relation R (of the appropriate number of places) on the domain of M satisﬁes
F(X). The clauses for existential quantiﬁcations and for function symbols are simi-
lar. The deﬁnitions of validity, satisﬁability, and implication are also unchanged for
second-order sentences. Any sentence, ﬁrst- or second-order, is valid if and only if
true in all its interpretations, and satisﬁable if and only if true in at least one of them.
A set  of sentences implies a sentence D if and only if there is no interpretation in
which all the sentences in  are true but D false.

SECOND-ORDER LOGIC
281
(The foregoing gives the standard notion of interpretation and truth for second-
order logic. In the literature nonstandard notions, euphemistically called ‘general’,
are sometimes considered, where an interpretation has separate domains of individ-
uals and of relations and functions. These will not be considered here.)
22.2 Example (The deﬁnition of identity). The Leibniz deﬁnition of identity in Example
22.1 is unnecessarily complicated, since the following simpler Whitehead–Russell deﬁnition
will do:
c = d ↔∀X(Xc →Xd)
We don’t need a biconditional on the right!
Proof: ∼Pc ∨Pd or Pc →Pd is true in an interpretation just in case the set P
denotes either fails to contain the individual c denotes or contains the one d denotes.
Hence a set R satisﬁes Xc →Xd just in case it either fails to contain the individual c
denotes or contains the one d denotes. Hence ∀X(Xc →Xd) is true just in case every
set either fails to contain the individual c denotes or contains the one d denotes. If c
and d denote the same individual, this must be so for every set, while if c and d do not
denote the same individual, then it will fail to be so for the set whose one and only
element is the individual c denotes. Thus ∀X(Xc →Xd) is true just in case c and d
denote the same individual, which is to say, if and only if c = d is true. (Intuitively,
the Whitehead–Russell deﬁnition is valid because among the properties of a is the
property of being identical with a; hence if the individual b is to have all the properties
of a, it must in particular have the property of being identical with a.)
22.3 Example (The ‘axiom’ of enumerability). Let Enum be the sentence
∃z∃u∀X((Xz & ∀x(Xx →Xu(x))) →∀x Xx).
Then Enum is true in an interpretation if and only if its domain is enumerable.
Proof: First suppose Enum is true in an interpretation M. This means there exists
an individual a in |M| and a one-place function f on |M| that satisfy
∀X((Xz & ∀x(Xx →Xu(x))) →∀x Xx).
Thus, if we add a constant 0 and let it denote a, and a one-place function symbol ′
and let it denote f , then
∀X((X0 & ∀x(Xx →Xx′)) →∀x Xx)
is true. This means every subset A of |M| satisﬁes
(X0 & ∀x(Xx →Xx′)) →∀x Xx.
In particular this is so for the enumerable subset A of |M| whose elements are all and
only a, f (a), f ( f (a)), f ( f ( f (a))), and so on. Thus if we add a one-place predicate
N and let it denote A, then
(N0 & ∀x(Nx →Nx′)) →∀xNx

282
SECOND-ORDER LOGIC
is true. But N0 is true, since the individual a that is the denotation of 0 is in the set
A that is the denotation of N, and ∀x(Nx →Nx′) is true, since if any individual is in
A, so is the value obtained when the function f that is the denotation of ′ is applied
to that individual as argument. Hence ∀xNx must be true, and this means that every
individual in the domain is in A, so the domain, being just A, is enumerable.
Conversely, suppose that the domain of an interpretation M is enumerable. Fix
an enumeration of its elements: m0, m1, m2, and so on. Let a be m0, and let f be the
function that given mi as argument yields mi+1 as value, and add a constant 0 and a
one-place function symbol ′ to denote a and f . Given any subset A of the domain,
suppose we add a one-place predicate N to denote A. Then if N0 is true, a = m0
must belong to A, and if ∀x(Nx →Nx′) is true, then whenever mi belongs to A,
f (mi) = mi+1 must belong to A. So if both are true, every element m0, m1, m2, . . .
of the domain must belong to A, and therefore ∀xNx is true. Thus
(N0 & ∀x(Nx →Nx′)) →∀xNx
is true if N is taken to denote A, and therefore A satisﬁes
(X0 & ∀x(Xx →Xx′)) →∀x Xx
and since this is true for any A,
∀X((X0 & ∀x(Xx →Xx′)) →∀x Xx)
is true, and therefore
∃z∃u∀X((Xz & ∀x(Xx →Xu(x))) →∀x Xx)
or Enum is true in M.
22.4 Example (The ‘axiom’ of inﬁnity). Let Inf be the sentence
∃z∃u(∀xz ̸= u(x) & ∀x∀y(u(x) = u(y) →x = y)).
Then Inf is true in an interpretation if and only if its domain is inﬁnite. The proof is left to
the reader.
22.5 Proposition. The downward and upward L¨owenheim–Skolem theorems both fail
for second-order logic.
Proof: Inf & ∼Enum and Inf & Enum are both second-order sentences having
inﬁnite but no ﬁnite models. The former has only nonenumerable models, contrary
to the downward L¨owenheim–Skolem theorem; the latter only denumerable models,
contrary to the upward L¨owenheim–Skolem theorem.
It is an immediate consequence of the downward and upward L¨owenheim–Skolem
theorems that if a ﬁrst-order sentence or set of such sentences has an inﬁnite model,
then it has nonisomorphic inﬁnite models. Even this corollary of the L¨owenheim–
Skolem theorems fails for second-order logic, as the next example shows.

SECOND-ORDER LOGIC
283
22.6 Example (Second-order arithmetic). Let PII be the conjunction of the axioms of Q
(as in section 16.2) with the following sentence Ind, called the axiom of induction:
∀X((X0 & ∀x(Xx →Xx′)) →∀x Xx).
Then an interpretation of the language of arithmetic is a model of PII if and only if it is
isomorphic to the standard interpretation.
Proof: We have already in effect seen in the proof of Example 22.3 that in any
model of Ind, the domain will consist precisely of the denotations of the terms
0, 0′, 0′′, . . . , which is to say, of the numerals 0, 1, 2, . . . , as we usually abbrevi-
ate those terms. We have also seen in section 16.2 that in any model of the axioms
of Q, all the following will be true for natural numbers m, n, and p:
m ̸= n
if
m ̸= n
m < n
if
m < n
∼m < n
if
m ≥n
m+ n = p
if
m + n = p
m+ n ̸= p
if
m + n ̸= p
m · n = p
if
m · n = p
m · n ̸= p
if
m · n ̸= p.
Now let M be a model of PII. Every element of |M| is the denotation of at least
one m, because M is a model of Ind, and of at most one m, because M is a model
of the axioms of Q and therefore of m ̸= n whenever m ̸= n, by the ﬁrst fact on the
list above. We can therefore deﬁne a function j from |M| to the natural numbers by
letting the value of j for the argument that is the denotation of m by m. By the other
six facts on the list above, j will be an isomorphism between M and the standard
interpretation.
Conversely, PII is easily seen to be true in the standard interpretation, and the proof
of the isomorphism theorem (Proposition 12.5) goes through essentially unchanged
for second-order logic, so any interpretation isomorphic to the standard interpretation
will also be an model of PII.
22.7 Proposition. The compactness theorem fails for second-order logic.
Proof: As in the construction of a nonstandard model of ﬁrst-order arithmetic, add
a constant c to the language of arithmetic and consider the set
 = {PII, c ̸= 0, c ̸= 1, c ̸= 2, . . . }.
Every ﬁnite subset 0 has a model obtained by expanding the standard interpretation
to assign a suitable denotation to c—any number bigger than all those mentioned in
0 will do. But  itself does not, because in any model of PII every element is the
denotation of one of the terms 0, 1, 2, and so on.
22.8 Proposition. The (abstract) G¨odel completeness theorem fails for second-order
logic: The set of valid sentences of second-order logic is not semirecursive (or even arith-
metical).

284
SECOND-ORDER LOGIC
Proof: A ﬁrst-order sentence A of the language of arithmetic is true in the standard
interpretation if and only if it is true in all interpretations isomorphic to the standard
one, and hence by the preceding example if and only if it is true in all models of
PII, or equivalently, if and only if PII →A is valid. The function taking (the code
number of) a ﬁrst-order sentence A to (the code number of) the second-order sentence
PII →A is clearly recursive. (Compare the proof of Theorem 17.6.) Hence if the set of
(code numbers of) valid second-order sentences were semirecursive, the set of (code
numbers of) sentences of the language of arithmetic true in the standard interpretation
would be also. But the latter set is not arithmetical (by Theorem 17.3) and a fortiori
not semirecursive.
Proposition 22.8 is sometimes formulated as follows: ‘Second-order logic is
incomplete’. A more accurate formulation would be: ‘No sound proof procedure
for second-order logic is complete’. (After all, it’s not the logic that’s incomplete, but
candidate proof procedures.)
We conclude this chapter with a preview of the next. Recall that a set S of natural
numbers is arithmetically deﬁnable, or simply arithmetical, if there is a ﬁrst-order
formula F(x) of the language of arithmetic such that S consists of just those m for
which F(m) is true in the standard interpretation, or equivalently, just those m that
satisfy F(x) in the standard interpretation. A set S of natural numbers is analytically
deﬁnable or analytical if there is a ﬁrst- or second-order formula φ(x) of the language
of arithmetic such that S consists of just those m that satisfy φ(x) in the standard
interpretation. Let us, for the space of this discussion, use the word class for sets of sets
of natural numbers. Then a class  of sets of natural numbers is arithmetical if there
is a second-order formula F(X) with no bound relation or function variables such that
 consists of just those sets M that satisfy F(X) in the standard intepretation. A class
 of sets of natural numbers is analytical if there is a second-order formula φ(X) such
that  consists of just those sets M that satisfy φ(X) in the standard interpretation.
We have seen that recursive and semirecursive sets are arithmetical, but that the set of
(code numbers of) ﬁrst-order sentences of the language of arithmetic that are true in
the standard interpretation is not arithmetical. It can similarly be shown that the set of
ﬁrst- and second-order sentences true in the standard interpretation is not analytical.
However, the set V of (code numbers of) ﬁrst-order sentences true in the standard
interpretation is analytical. This follows from the fact, to be proved in the next chapter,
that the class {V } of sets of natural numbers whose one and only member is the set
V is arithmetical. The latter result means that there is a second-order formula F(X)
with no bound relation or function variables such that V is the one and only set that
satisﬁes F(X) in the standard interpretation. From this it follows that V is precisely
the set of m that satisfy ∃X(F(X) & Xx); and this shows that, as asserted, V is
analytical. It will also be shown that the class of arithmetical sets of natural numbers
is not arithmetical. (Again, this class can be shown to be analytical.) In order to keep
the next chapter self-contained and independent of this one, a different deﬁnition of
arithmetical class will be given there, not presupposing familiarity with second-order
logic. However, the reader who is familiar with second-order logic should have no
difﬁculty recognizing that this deﬁnition is equivalent to the one given here.

PROBLEMS
285
Problems
22.1 Does it follow from the fact that ∃xFx & ∃x∼Fx is satisﬁable that ∃X(∃xXx &
∃x∼Xx) is valid?
22.2 Let us write R∗ab to abbreviate
∀X[Xa & ∀x∀y((Xx & Rxy) →Xy) →Xb].
Show that the following are valid:
(a) R∗aa
(b) Rab →R∗ab
(c) (R∗ab & R∗bc) →R∗ac
Suppose Rab if and only if a is a child of b. Under what conditions do we have
R∗ab?
22.3 (A theorem of Frege) Show that (a) and (b) imply (c):
(a) ∀x∀y∀z[(Rxy & Rxz) →y = z]
(b) ∃x(R∗xa & R∗xb)
(c) (R∗ab ∨a = b ∨R∗ba).
22.4 Write ♦(R) to abbreviate
∀x∀y(∃w(Rwx & Rwy) →∃z(Rxz & Ryz)).
Show that ♦(R) →♦(R*) is valid.
22.5 (The principle of Russell’s paradox) Show that ∃X∼∃y∀x(Xx ↔Rxy) is
valid.
22.6 (A problem of Henkin) Let Q1 and Q2 be as in section 16.2, and let I be the
induction axiom of Example 22.6. Which of the eight combinations
{(∼)Q1, (∼)Q2, (∼)I}, where on each of the three sentences the negation
sign may be present or absent, are satisﬁable?
22.7 Show that the set of (code numbers of) second-order sentences true in the
standard model of arithmetic is not analytical.
22.8 Show that PII is not logically equivalent to any ﬁrst-order sentence.
22.9 Show that for any ﬁrst- or second-order sentence A of the language of arith-
metic, either PII & A is equivalent to PII, or PII & A is equivalent to 0 ̸= 0.
22.10 Show that the set of (code numbers of) second-order sentences that are equiv-
alent to ﬁrst-order sentences is not analytical.
22.11 Prove the Craig interpolation theorem for second-order logic.

23
Arithmetical Deﬁnability
Tarski’s theorem tells us that the set V of (code numbers of) ﬁrst-order sentences of the
language in arithmetic that are true in the standard interpretation is not arithmetically
deﬁnable. In section 23.1 we show that this negative result is poised, so to speak, between
two positive results. One is that for each n the set Vn of sentences of the language of
arithmetic of degree of complexity n that are true in the standard interpretation is
arithmetically deﬁnable (in a sense of degree of complexity to be made precise). The
other is that the class {V} of sets of natural numbers whose one and only member is V
is arithmetically deﬁnable (in a sense of arithmetical deﬁnability for classes to be made
precise). In section 23.2 we take up the question whether the class of arithmetically
deﬁnable sets of numbers is an arithmetically deﬁnable class of sets. The answer is
negative, according to Addison’s theorem. This result is perhaps most interesting on
account of its method of proof, which is a comparatively simple application of the method
of forcing originally devised to prove the independence of the continuum hypothesis in
set theory (as alluded to in the historical notes to Chapter 18).
23.1 Arithmetical Deﬁnability and Truth
Throughout this chapter we use L and N for the language of arithmetic and its
standard interpretation (previously called L* and N*), and V for the set of code
numbers of ﬁrst-order sentences of L ture in N. It will be convenient to work with
a version of logic in which the only operators are ∼and ∨and ∃(& and ∀being
treated as unofﬁcial abbreviations). We measure the ‘complexity’ of a sentence by
the number of occurrences of logical operators ∼and ∨and ∃in it. (Our results
do, however, go through for other reasonable notions of measures of complexity: see
the problems at the end of the chapter). By Vn we mean the set of code numbers of
ﬁrst-order sentences of L of complexity ≤n that are true in N.
We are going to be discussing natural numbers, sets of natural numbers, and sets
of sets of natural numbers. To keep the levels straight, we generally use numbers for
the natural numbers, sets for the sets of natural numbers, and classes for the sets of
sets of natural numbers. We write Lc for the expansion of L by adding a constant c,
and N c
a for the expansion of N that assigns c as denotation the number a. Then a
set S of numbers is arithmetically deﬁnable if and only if there is a sentence F(c) of
Lc such that S is precisely the set of a for which F(c) is true in N c
a . Analogously,
we write LG for the expansion of L by adding a one-place predicate G, and N G
A for
286

23.1. ARITHMETICAL DEFINABILITY AND TRUTH
287
the expansion of N that assigns G as denotation the set A. And we say a class  of
sets of numbers is arithmetically deﬁnable if and only if there is a sentence F(G) of
LG such that  is precisely the set of A for which F(G) is true in N G
A .
The following two results contrast with Tarski’s theorem to the effect that V is not
arithmetically deﬁnable.
23.1 Theorem. For each n, Vn is arithmetically deﬁnable.
23.2 Theorem. The class {V } whose one and only member is V is arithmetically
deﬁnable.
This entire section will be devoted to the proofs. We are going to need certain facts
about recursiveness (or the ‘arithmetization of syntax’):
(1) The set S of code numbers of the sentences of L is recursive.
(2) For each n, the set Sn of code numbers of sentences of L with no more than n
occurrences of logical operators is recursive.
(3) There exists a recursive function ν such that if B is a sentence of L with code
number b, then ν(b) is the code number of ∼B.
(4) There exists a recursive function δ such that if B and C are sentences of L with
code numbers b and c, then δ(b, c) is the code number of (B ∨C).
(5) There exists a recursive function η such that if v is a variable with code numbers q
and F(v) is a formula with code number p, then η(p, q) is the code number of
∃v F(v).
(6) There exists a recursive function σ such that if v, F(v), q, p are as in (5), then for
any m, σ(p, q, m) is the code number of F(m).
(7) The set V0 of atomic sentences of L that are true in N is recursive.
[We may suppose that ν, δ, η, σ take the value 0 for inappropriate arguments; for
instance, if b is not the code number for a sentence, then ν(b) = 0.]
In every case, intuitively it is more or less clear that the set or function in question is
effectively decidable or computable, so according to Church’s thesis they should all be
recursive. Proofs not depending on appeal to Church’s thesis have been given for (1)
and (3)–(6) in Chapter 15; and the proof for (2) is very similar and could easily have
been included there as well (or placed among the problems at the end of that chapter).
As for (7), perhaps the simplest proof is to note that V0 consists of the elements
of the recursive set S0 that are theorems of Q, and equivalently whose negations
are not theorems of Q (since Q proves all true atomic sentences and disproves all
false ones). But we know from Chapter 15 that the set of theorems of Q or any
axiomatizable theory is a semirecursive set, and the set of sentences whose negations
are not theorems is the complement of a semirecursive set. It follows that V0 is both
a semirecursive set and the complement of one, and is therefore recursive.
The sets above all being recursive, they are deﬁnable in arithmetic and indeed in
Q, and the functions are representable. Let S(x), S0(x), S1(x), S2(x), . . . , Nu(x, y),
Delta(x, y, z), Eta(x, y, z), Sigma(x, y, z, w), and V 0(x) be deﬁning or representing
formulas for S, S0, S1, S2,. . . , ν, δ, η, σ, and V0. This machinery will be used in the
proofs of both theorems.

288
ARITHMETICAL DEFINABILITY
Proof of Theorem 23.1: A sentence A that contains n + 1 logical operators is
either the negation ∼B of a sentence B containing n operators, or the disjunction
B ∨C of two sentences B and C each containing at most n operators, or else an
existential quantiﬁcation ∃v F(v) of a formula F(v) containing n operators. In the last
case, for each m, the sentence F(m) also contains n operators. In the ﬁrst case A will
be true if and only if B is not true. In the second case, A will be true if and only if B
is true or C is true. In the third case, A will be true if and only if, for some m, F(m)
is true.
In terms of Vn, we can therefore characterize Vn+1 as the set of those numbers a
in Sn+1 such that either k is in Vn; or for some b, a = ν(b) and b is not in Vn; or for
some b and c, a = δ(b, c) and either b is in Vn or c is in Vn; or ﬁnally, for some p
and q, a = η(p, q), and for some m, σ(p, q, m) is in Vn. So if V n(x) arithmetically
deﬁnes Vn, the following formula V n+1(x) arithmetically deﬁnes Vn+1:
Sn+1(x) & {V n(x) ∨∃y[Nu(y, x) & ∼V n(y)]
∨∃y∃z[Delta(y, z, x) & (V n(y) ∨V n(z))]
∨∃y∃z[Eta(y, z, x) & ∃u∃w(Sigma(y, z, u, w) & V n(w))]}.
Since we know V0 is arithmetically deﬁnable, it follows by induction that Vn is
arithmetically deﬁnable for all n.
Proof of Theorem 23.2: The set of sentences true in N can be characterized as
the unique set  such that:
 contains only sentences of L.
For any atomic sentence A, A is in  if and only if A is a true atomic sentence.
For any sentence B, ∼B is in  if and only if B is not in .
For any sentences B and C, (B ∨C) is in  if and only if B is in  or C is in .
For any variable v and formula F(v), ∃v F(v) is in  if and only if, for some m,
F(m)is in .
The set V of code numbers of sentences true in N can therefore be characterized
as the unique set M such that:
For all b, if b is in M, then b is in S.
For all a, if a is in S0, then a is in M if and only if a is in V0.
For all b, if ν(b) is in S, then ν(b) is in M if and only if b is not in M.
For all b and c, if δ(b, c) is in S, then δ(b, c) is in M if and only if either b is in
M or c is in M.
For all p and q, if η(p, q) is in S, then η(p, q) is in M if and only if, for some
m, σ(p, q, m) is in M.
So on expanding L be adding the one-place predicate G, if we let F(G) be the
conjunction
∀x(Gx →S(x)) &
∀x(S0(x) →(Gx ↔V 0(x))) &
∀x∀y((Nu(x, y) & S(y)) →(Gy ↔∼Gx)) &
∀x∀y∀z((Delta(x, y, x) & S(z)) →(Gz ↔(Gx ∨Gy))) &
∀x∀y∀z((Eta(x, y, z) & S(z)) →(Gz ↔∃u∃w(Sigma(x, y, u, w) & Gw)))

23.2. ARITHMETICAL DEFINABILITY AND FORCING
289
then the only way to expand N to get a model of F(G) is to take V as the denotation
of G.
23.2 Arithmetical Deﬁnability and Forcing
We retain the terminology and notation of the preceding section. This entire section
will be devoted to the proof of the following result.
23.3 Theorem (Addison’s theorem). The class of arithmetically deﬁnable sets of num-
bers is not an arithmetically deﬁnable class of sets.
The ﬁrst notion we need is that of a condition, by which we mean a ﬁnite, consistent
set of sentences of the language LG each either of the form Gm or ∼Gm. The empty
set ∅is a condition. Other examples are {G17}, {G17, ∼G59}, and
{G0, G1, G2, . . . , G999 999, G1 000 000}.
We use p, q,r as variables for conditions. We say a condition q extends or is an
extension of a condition p if p is a subset of q. Thus every condition extends itself
and extends ∅.
Forcing is a relation between certain conditions and certain sentences of LG. We
write p ⊩S to mean that condition p forces sentence S. The relation of forcing is
inductively deﬁned by the following ﬁve stipulations:
(1) If S is an atomic sentence of L, then p ⊩S if and only if N |= S.
(2) If t is a term of L and m is the denotation of t in N, then if S is the sentence Gt,
then p ⊩S if and only if Gm is in p.
(3) If S is a disjunction (B ∨C), then p ⊩S if and only if either p ⊩B or p ⊩C.
(4) If S is an existential quantiﬁcation ∃x B(x), then p ⊩S if and only if, for some n,
p ⊩B(n).
(5) If S is a negation ∼B, then p ⊩S if and only if, for every q that extends p, it is not
the case that q ⊩S.
The last clause bears repeating: a condition forces the negation of a sentence
if and only if no extension forces the sentence. It follows that no condition forces
some sentence and its negation, and also that either a condition forces the negation
of a sentence or some extension forces the sentence. (It will soon be shown that if a
condition forces a sentence, so does every extension of it.)
It follows from (2) and (5) that p ⊩∼Gm if and only if ∼Gm is in p. For if ∼Gm
is not in p, then p ∪{Gm} is an extension of p that forces Gm. So if p ⊩∼Gm, that
is, if no extension of p forces Gm, then ∼Gm must be in p. Conversely, if ∼Gm is
in p, then Gm is in no extension of p, and hence no extension of p forces Gm, and
so p ⊩∼Gm.
Thus {G3} forces neither G11 nor ∼G11, and so does not force (G11 ∨∼G11).
Thus a condition may imply a sentence without forcing it. (It will soon be seen that
the inverse is also possible; that, for example, ∅forces ∼∼∃xGx, even though it
does not imply it, and does not force ∃xGx.)

290
ARITHMETICAL DEFINABILITY
23.4 Lemma. If p ⊩S and q extends p, then q ⊩S.
Proof: Suppose that p ⊩S and q extends p. The proof that q ⊩S is by induction
on complexity of S. The atomic case has two subcases. If S is an atomic sentence
of L, then since p ⊩S, S is true in N, and since S is true in N, q ⊩S. If S is an
atomic sentence of form Gt, then since p ⊩S, Gm is in p, where m is the denotation
of t in N, and since q extends p, Gm is also in q and q ⊩Gt. If S is (B ∨C), then
since p ⊩S, either p ⊩B or p ⊩C; so by the induction hypothesis, either q ⊩B or
q ⊩C, and so q ⊩(B ∨C). If S is ∃x B(x), then since p ⊩S, we have p ⊩B(m) for
some m; so by the induction hypothesis, q ⊩B(m) and q ⊩∃x B(x). Finally, if S is
∼B, then since p ⊩S, no extension of p forces B; and then, since q is an extension
of p, every extension of q is an extension of p, so no extension of q forces B, and so
q ⊩∼B.
Two observations, not worthy of being called lemmas, follow directly from the
preceding lemma. First, if p ⊩B, then p ⊩∼∼B; for any extension of p will force
B, hence no extension of p will force ∼B. Second, if p ⊩∼B and p ⊩∼C, then
p ⊩∼(B ∨C); for every extension of p will force both ∼B and ∼C, and so will
force neither B nor C, and so will not force (B ∨C).
A more complicated observation of the same kind may be recorded here for future
reference, concerning the sentence
∼(∼(∼B ∨∼C) ∨∼(B ∨C))
(∗)
which is a logical equivalent of ∼(B ↔C). Suppose p ⊩B and p ⊩∼C. Then
p ⊩(∼B ∨∼C), so by our ﬁrst observation in the preceding paragraph, p ⊩
∼∼(∼B ∨∼C). Also p ⊩(B ∨C), so p ⊩∼∼(B ∨C). Hence by our second ob-
servation, p ⊩(*). Similarly, if p ⊩∼B and p ⊩C, then again p ⊩(*).
23.5 Lemma. If S is a sentence of L, then for every p, p ⊩S if and only if N |= S.
Proof: The proof again is by induction on the complexity of S. If S is atomic, the
assertion of the lemma holds by the ﬁrst clause in the deﬁnition of forcing. If S is
(B ∨C), then p ⊩S if and only if p ⊩B or p ⊩C, which by the induction hypothesis
is so if and only N |= B or N |= C, which is to say, if and only if N |= (B ∨C). If
S is ∃x B(x), the proof is similar. If S is ∼B, then p ⊩S if and only if no extension
of p forces B, which by the induction hypothesis is so if and only if it is not the case
that N |= B, which is to say, if and only if N |= ∼B.
Forcing is a curious relation. Since ∅does not contain any sentence Gn, for no n
does ∅force Gn, and therefore ∅does not force ∃xGx. But ∅does force ∼∼∃xGx!
For suppose some p forces ∼∃xGx. Let n be the least number such that ∼Gn is not
in p. Let q be p ∪{Gn}. Then q is a condition, q extends p, and q forces Gn, so q
forces ∃xGx. Contradiction. Thus no p forces ∼∃xGx, which is to say, no extension
of ∅forces ∼∃xGx, so ∅forces ∼∼∃xGx.
We are going to need some more deﬁnitions. Let A be a set of numbers. First,
we call a condition p A-correct if for any m, if Gm is in p, then m is in A, while
if ∼Gm is in p, then m is not in A. In other words, p is A-correct if and only if

23.2. ARITHMETICAL DEFINABILITY AND FORCING
291
N G
A (the expansion of the standard interpretation N of the language of arithmetic
L to an interpretation of the language LG in which the new predicate G is taken to
denote A) is a model of p.
Further, say A FORCES S if some A-correct condition forces S. Note that the union
of any two A-correct conditions is still a condition and is still A-correct. It follows
that A cannot FORCE both S and ∼S, since the union of an A-correct condition forcing
S with one forcing ∼S would force both, which is impossible.
Finally, we call A generic if for every sentence S of LG, either A FORCES S or
A FORCES ∼S. If this is so at least for every sentence S with at most n occurrences
of logical operators, we call A n-generic. Thus a set is generic if and only if it is
n-generic for all n.
The ﬁrst fact about generic sets that we have to prove is that they exist.
23.6 Lemma. For any p, there is a generic set A such that p is A-correct.
Proof: Let S0, S1, S2, . . . be an enumeration of all sentences of LG. Let p0, p1,
p2, . . . be an enumeration of all conditions. We inductively deﬁne a sequence q0,
q1, q2, . . . of conditions, each an extension of those that come before it, as follows:
(0) q0 is p.
(1) If qi forces ∼Si, then qi+1 is qi.
(2) If qi does not force ∼Si, in which case there must be some q extending qi and
forcing Si, then qi+1 is the ﬁrst such q (in the enumeration p0, p1, p2, . . .).
Let A be the set of m such that Gm is in qi for some i.
We claim that p is A-correct and that A is generic. Since p = q0, and since for
each i, either qi+1 ⊩Si or qi+1 ⊩∼Si, it will be enough to show that for each i, qi is
A-correct. And since m is in A when Gm is in qi, it is enough to show that if ∼Gm
is in qi, then m is not in A. Well, suppose it were. Then Gm would be in q j for some
j. Letting k = max(i, j), both ∼Gm and Gm would be in qk, which is impossible.
This contradiction completes the proof.
The next fact about generic sets relates FORCING and truth.
23.7 Lemma. Let S be a sentence of LG, and A a generic set. Then A FORCES S if and
only if N G
A |= S.
Proof: The proof will be yet another by induction on complexity, with ﬁve cases,
one for each clause in the deﬁnition of forcing. We abbreviate ‘if and only if’ to ‘iff’.
Case 1. S is an atomic sentence of L. Then A FORCES S iff some A-correct p forces
S, iff (by Lemma 23.5) N |= S, iff N G
A |= S.
Case 2. S is an atomic sentence Gt. Let m be the denotation of t in N. Then A
FORCES S iff some A-correct p forces Gt, iff Gm is in some A-correct p, iff m is in
A, iff N G
A |= Gt.
Case 3. S is (B ∨C). Then A FORCES S iff some A-correct p forces (B ∨C),
iff some A-correct p forces B or forces C, iff either some A-correct p forces B or
some A-correct p forces C, iff A FORCES B or A FORCES C, iff (by the induction
hypothesis) N G
A |= B or N G
A |= C, iff N G
A |= (B ∨C).

292
ARITHMETICAL DEFINABILITY
Case 4. S is ∃x B(x). Then A FORCES S iff some A-correct p forces ∃x B(x), iff
for some A-correct p there is an m such that p forces B(m), iff for some m there is
an A-correct p such that p forces B(m), iff for some m, A forces B(m), iff (by the
induction hypothesis) for some m, N G
A |= B(m), iff N G
A |= ∃x B(x).
Case 5. S is ∼B. No set FORCES both B and ∼B. Since A is generic, A FORCES at
least one of B or ∼B. Hence A FORCES ∼B iff it is not the case that A FORCES B, iff
(by the induction hypothesis) not N G
A |= B, iff N G
A |= ∼B.
The last fact about generic sets that we have to prove is that none of them is
arithmetical.
23.8 Lemma. No generic set is arithmetical.
Proof: Suppose otherwise. Then there is a generic set A and a formula B(x) of L
such that for every n, n is in A if and only if N |= B(n). So N G
A |= ∀x(Gx ↔B(x)) or
N G
A |= ∼∃x F(x), where F(x) is the following logical equivalent of ∼(Gx ↔B(x)):
∼(∼(∼Gx ∨∼B(x)) ∨∼(Gx ∨B(x))).
By Lemma 23.7, A FORCES ∼∃x F(x), so some A-correct p forces ∼∃x F(x), so for
no q extending p and no n does q force F(n), which is to say
∼(∼(∼Gn ∨∼B(n)) ∨∼(Gn ∨B(n))).
(∗)
Let k be the least number such that neither Gk nor ∼Gk is in p. Deﬁne a condition
q extending p by letting q = p ∪{Gk} if N |= ∼B(k) and letting q = p ∪{∼Gk}
if N |= B(k). In the former case, q ⊩Gk, while by Lemma 23.5 q ⊩∼B(k). In the
latter case, q ⊩∼Gk while by Lemma 23.5 q ⊩B(k). In either case, q ⊩(*) by our
observations following Lemma 23.4, which is to say q ⊩F(n). Contradiction.
Suppose that at the beginning of the proof of Lemma 23.6, instead of enumerating
all sentences we enumerate those sentences of complexity ≤n (that is, having no
more than n occurrences of logical operators). Then the proof would establish the
existence of an n-generic set rather than of a generic set. Suppose that in the hypothesis
of Lemma 23.7 we only assume the set A is n-generic rather than generic. Then the
proof would establish the conclusion of Lemma 23.7 for sentences of complexity
≤n, rather than for all sentences. But suppose that in the hypothesis of Lemma 23.8
we only assume the set A is n-generic rather than generic. Then the proof would
break down entirely. And indeed, in contrast to Lemma 23.8, we have the following.
23.9 Lemma. For any n, there is an n-generic set A that is arithmetical.
Proof: The proof will be indicated only in outline. The idea is to carry out the
construction in the proof of Lemma 23.6, starting from an enumeration of all sentences
of complexity ≤n, and with p =∅. It is necessary to show that, if code numbers are
assigned in a suitable way, then various relations among code numbers connected with
the construction will be arithmetical, with the result that the generic set constructed
is arithmetical as well.
First note that, since we have seen in the preceding section that the set of code
numbers of sentences of complexity ≤n is recursive, the function enumerating the

23.2. ARITHMETICAL DEFINABILITY AND FORCING
293
elements of that set in increasing order is recursive. That is, if we enumerate the
sentences S0, S1, S2, . . . in order of increasing code number, then the function taking
us from i to the code number for Si will be recursive.
We also enumerate the conditions p0, p1, p2, . . . in order of increasing code num-
ber, where code numbers are assigned to ﬁnite sets of sentences—for that is what
conditions are—as in section 15.2. As we observed in section 15.2, the relation ‘the
sentence with code number i belongs to the set with code number s’ is recursive.
Using this fact and the fact that the function taking m to the code number for Gm—
essential the substitution function σ used in the preceding section—is recursive, it is
not hard to show that the set of code numbers of conditions is recursive, and that the
relation that holds between m and s if and only if s is the code number of a condition
containing Gm is recursive. We also observed in section 15.2 that the relation ‘the set
with code number s is a subset of the set with code number t’ is recursive. Hence the
relation that holds between s and t if and only if they are code numbers of conditions
p and q, with q an extension of p, is also recursive. Being recursive, the various
functions and relations we have mentioned are all arithmetical.
We also need one more fact: that for each n, the relation that holds between i and s
if and only if i is the code number of a sentence S of complexity ≤n and s is the code
number of a condition p, and p forces S, is arithmetical. The proof is very similar to
the proof in the preceding section that for each n the set Vn is arithmetical, and will
be left to the reader.
Now returning to the construction of an n-generic set A, by the method of the
proof of Lemma 23.6, we see that m is in A if and only if there exists a sequence s
of conditions such that the following hold (for each i less than the length of the
sequence):
(0) The 0th entry of the sequence is the empty condition ∅
(1) If the ith entry of the sequence forces the negation of the ith sentence in the
enumeration of sentences, then the (i + 1)st entry is the same as the ith.
(2) Otherwise, the (i + 1)st entry is a condition that extends the ith and that forces
the ith sentence in the enumeration of sentences, and is such that no condition
earlier in the enumeration of conditions (that is, no condition of smaller code
number) does both these things.
(3) The sentence Gm belongs to the last entry of the sequence.
We can, of course, replace ‘there exists a sequence. . .’ by ‘there exists a code
number for a sequence. . .’. When everything is thus reformulated in terms of code
numbers, what we get is a logical compound of relations that we have noted in the
preceding several paragraphs to be arithmetical. It follows that A itself is arithmetical.
At last we are in a position to prove Addison’s theorem.
Proof of Theorem 23.3: Suppose the theorem fails. Then there is a sentence S of
LG such that for any set A, N G
A |= S if and only if A is arithmetical. Let S be of
complexity n. By Lemma 23.9 there exists an n-generic set A that is arithmetical. So
N G
A |= S. So by Lemma 23.7 (or rather, the version for n-generic sets and sentences
of complexity ≤n, as in our remarks following Lemma 23.8), A FORCES S. So some

294
ARITHMETICAL DEFINABILITY
A-correct p forces S. By Lemma 23.6, there exists a (fully) generic set A* such that
p is A*-correct. Since p forces S, by Lemma 23.7 (in its original version), N G
A∗|= S.
But this means A* is arithmetical, contrary to Lemma 23.8.
Problems
23.1 Use Beth’s deﬁnability theorem, Tarski’s theorem on the ﬁrst-order indeﬁn-
ability of ﬁrst-order arithmetic truth, and the results of section 23.1 to obtain
another proof of the existence of nonstandard models of arithmetic.
23.2 Show that for each n the set of (code numbers of) true prenex sentences of the
language of arithmetic that contain at most n quantiﬁers is arithmetical. Show
the same with ‘prenex’ omitted.
23.3 Show that if p ⊩∼∼∼B, then p ⊩∼B.
23.4 Given an example of a sentence B such that the set of even numbers FORCES
neither B nor ∼B.
23.5 Show that the set of pairs (i, j) such that j codes a sentence of LG and i codes
a condition that forces that sentence is not arithmetical.
23.6 Where would the proof of Addison’s theorem have broken down if we had
worked with ∼, & , ∀rather than ∼, ∨, ∃(and made the obvious analogous
stipulations in the deﬁnition of forcing)?
23.7 Show that the only arithmetical subsets of a generic set are its ﬁnite subsets.
23.8 Show that if A is generic, then {A} is not arithmetical.
23.9 Show that {A : A is generic} is not arithmetical.
23.10 Show that every generic set contains inﬁnitely many prime numbers.
23.11 Show that the class of generic sets is nonenumerable.
23.12 A set of natural numbers is said to have density r, where r is a real number, if r
is the limit as n goes to inﬁnity of the ratio (number of members of A < n)/n.
Show that no generic set has a density.

24
Decidability of Arithmetic without Multiplication
Arithmetic is not decidable: the set V of code numbers of sentences of the language
L of arithmetic that are true in the standard interpretation is not recursive (nor even
arithmetical). But for some sublanguages L* of L, if we consider the elements of V that
are code numbers of sentences of L*, then the set V* of such elements is recursive:
arithmetic without some of the symbols of its language is decidable. A striking case
is Presburger arithmetic, or arithmetic without multiplication. The present chapter is
entirely devoted to proving its decidability.
We have used (true) arithmetic to mean the set of sentences of the language
of arithmetic L = {0, <, ′,+, ·} that are true in the standard interpretation N. By
arithmetic without multiplication we mean the set of sentences of (true) arithmetic
that do not contain the symbol ·. By arithmetic without addition we mean the set
of sentences of (true) arithmetic that do not contain the symbols <, ′,+. In contrast
to the undecidability of arithmetic stand Presburger’s theorem, to the effect that
arithmeticwithoutmultiplicationisdecidable,andSkolem’stheorem,totheeffectthat
arithmetic without addition is decidable. [Note in connection with the latter theorem
that ′ is easily deﬁnable in terms of < and that + is deﬁnable in terms of ′ and ·, as
follows:
x + y = z ↔(x′ · z′′) ′ · (y′ · z′′) = ((x′ · y′)′ · (z′′ · z′′))′.
That is why < and ′ have to be dropped along with +.] This chapter will be entirely
devoted to proving the former theorem, by describing an effective procedure for deter-
mining whether or not a given sentence of the language of arithmetic not involving ·
is true in the standard interpretation.
We begin with a reduction of the problem. Let K be the language with con-
stants 0 and 1, inﬁnitely many one-place predicates D2, D3, D4, . . . , the two-place
predicate <, and the two-place function symbols + and ---. Let M be the
interpretation with domain the set of all integers (positive, zero, negative), and with
the following denotations for the nonlogical symbols. 0, 1, <,+, --- will denote the
usual zero and unity elements, order relation, and addition and subtraction oper-
ations on integers. Dn will denote the set of integers divisible without remainder
by n.
295

296
DECIDABILITY OF ARITHMETIC WITHOUT MULTIPLICATION
Given a sentence S of L without ·, replace ′ everywhere in it by +1, and replace
every quantiﬁcation ∀x or ∃x by a relativized quantiﬁcation
∀x((x = 0 ∨0 < x) →· · ·)
∃x((x = 0 ∨0 < x) & . . .).
to obtain a sentence S* of K. Then S will be true in N if and only if S* is true in
M. Thus to prove Presburger’s theorem, it will be sufﬁcient to describe an effective
procedure for determining whether or not a given sentence of K is true in M.
For the remainder of this chapter, therefore, term or formula or sentence will al-
ways mean term or formula or sentence of K, while denotation or satisfaction or truth
will always mean denotation or satisfaction or truth in M. We call two terms r and
s coextensive if ∀v1 . . . ∀vn r = s is true, where the vi are all the variables occurring
in r or s. We call two formulas F and G coextensive if ∀v1. . .∀vn(F ↔G) is true,
where the vi are all the free variables occurring in F or G.
Given any closed term, we can effectively calculate its denotation. Given any
atomic sentence, we can effectively determine its truth value; and we can therefore
do the same for any quantiﬁer-free sentence. We are going to show how one can
effectively decide whether a given sentence S is true by showing how one can effec-
tively associate to S a coextensive quantiﬁer-free sentence T : once T is found, its
truth value, which is also the truth value of S, can be effectively determined.
The method to be used for ﬁnding T , given S, is called elimination of quanti-
ﬁers. It consists in showing how one can effectively associate to a quantiﬁer-free
formula F(x), which may contain other free variables besides x, and quantiﬁer-free
G such that G is coextensive with ∃x F(x) and G contains no additional free variables
beyond the free variables in ∃x F(x). This shown, given S, we put it in prenex form,
then replace each quantiﬁcation ∀x by ∼∃x ∼, and work from the inside outward,
successively replacing existential quantiﬁcations of quantiﬁer-free formulas by co-
extensive quantiﬁer-free formulas with no additional free variables, until at last a
sentence with no free variables, which is to say, a quantiﬁer-free sentence T , is
obtained.
So let F(x) be a quantiﬁer-free formula. We obtain G, coextensive with ∃x F(x)
and containing no addition free variables beyond those in ∃x F(x), by performing, in
order, a sequence of 30 operations, each of which replaces a formula by a coextensive
formula with no additional free variables.
In describing the operations to be gone through, we make use of certain notational
conventions. When writing of a positive integer k and a term t we allow ourselves to
write
−t
instead of
0 --- t
k
instead of
1+ 1 + · · ·+ 1 (k times)
kt
instead of
t + t + · · ·+ t (k times)
for instance. With such notation, the 30 operations are as follows:
(1) Put F into disjunctive normal form. (See section 19.1.) Thus we get a disjunction
of conjunctions of atomic formulas of the forms r = s or r < s or Dms (where r
and s are terms) and negations of such.

DECIDABILITY OF ARITHMETIC WITHOUT MULTIPLICATION
297
(2) Replace each formula of form r = s by (r < s + 1 & s < r + 1).
(3) Replace each formula of form r ̸= s by (r < s ∨s < r).
(4) Put the result back into disjunctive normal form.
(5) Replace each formula of form ∼r < s by s < r + 1.
(6) Replace each formula of form ∼Dms by the disjunction of Dm(s + i) for all i with
0 < i < m. The result is coextensive with the original, because for any number
a, m divides exactly one of a, a + 1, a + 2, . . . , a + m −1.
(7) Put the result back into disjunctive normal form.
(8) At this point we have a disjunction of conjunctions of atomic formulas of the
forms r < s and Dms. Replace each formula of form r < s by 0 < (s ---r).
(9) We say a term is in normal form if it has one of the ﬁve forms
kx,--- kx,kx+ t,--- kx + t, or t, wherein t is a term not containing the variable x.
For every term one can effectively ﬁnd a coextensive term in normal form by
ordinary algebraic operations, such as regrouping and reordering summands.
Replace each term in the formula that is not in normal form by a coextensive one
that is.
(10) Replace each formula of form
0 < ---kx,
0 < kx + t,
or
0 < ---kx + t
by
kx < 0,
---t < kx,
or
kx < t
as the case may be.
(11) At this point all atomic formulas with predicate < that contain the variable x have
either the form t < kx or the form kx < t, where k is positive and t does not
contain x. We call those of the former form lower inequalities and those of the
latter form upper inequalties. Rearrange the order of conjuncts in each disjunct so
that all lower inequalities occur on the left.
(12) Towards reducing the number of lower inequalities occurring in any disjunct, if a
conjunction of form t1 < k1x & t2 < k2x occurs in a disjunct, replace it by the
disjunction of the following three conjunctions:
(i) t1 < k1x & k1t2 < k2t1
(ii) t1 < k1x & k1t2 = k2t1
(iii) t2 < k2x & k2t1 < k1t2.
To see that this substitution is justiﬁed (that is, to see that it produces a result
coextensive with the original), note that exactly one of the second conjuncts in
(i)–(iii) must hold, and that (i) or (ii) holds, then so do k2t1 < k1k2x and
k1t2 < k1k2x, and hence so does t2 < k2x; while similarly (iii) yields t1 < k1x.
(13) Eliminate any occurrences of = introduced at the previous step by repeating steps
(2) and (4).
(14) The effect of the preceding three steps is to reduce by one the number of lower
inequalities in any disjunct where there were more than one to begin with. (Note
that k1t2 < k2t1, for instance, does not count as a lower inequality, since it does not

298
DECIDABILITY OF ARITHMETIC WITHOUT MULTIPLICATION
contain the variable x.) Repeat these three steps over and over until no disjunct has
more than one lower inequality among its conjuncts.
(15) Carry out an analogous process for upper inequalities, until no disjunct has more
than one lower or upper inequality among its conjuncts.
(16) Replace each formula of form
Dm(kx),
Dm(---kx),
Dm(kx + t),
or
Dm(---kx + t)
by
Dm(kx --- 0),
Dm(kx --- 0),
Dm(kx --- (---t)),
or
Dm(kx --- t)
as the case may be. This step is justiﬁed because for any number a, m divides a if
and only if m divides −a.
(17) At this point all atomic formulas with Dm and involving x have the form
Dm(kx --- t), where k is a positive integer. Replace any formula of this form by the
disjunction of all conjunctions
Dm(kx --- i) & Dm(t --- i)
for 0 ≤i < m. To see that this step is justiﬁed, note that m divides the difference of
two numbers a and b if and only if a and b leave the same remainder on division
by m, and that the remainder on dividing a (respectively, b) by m is the unique i
with 0 ≤i < m such that m divides a −i (respectively, b −i).
(18) Put the result back into disjunctive normal form.
(19) At this point all atomic formulas with Dm and involving x have the form
Dm(kx --- i), where k is a positive integer and 0 ≤i < m. Replace any formula of
this form with k >1 by the disjunction of the formulas Dm(x --- j) for all j with
0 ≤j < m such that m divides kj −i. This step is justiﬁed because for any
number a, ka leaves a remainder of i on division by m if and only if kj does, where
j is the remainder on dividing a by m.
(20) Put the result back into disjunctive normal form.
(21) At this point all atomic formulas with Dm and involving x have the form
Dm(x --- i), where i is a nonnegative integer. In any such case consider the prime
decomposition of m; that is, write
m = pe1
1 · · · pek
k
where
p1 < p2 < · · · < pk
and all ps are primes.
If k >1, then let m1 = pe1
1 , . . . , mk = pek
k , and replace Dm(x --- i) by
Dm1(x --- i) & . . . & Dmk(x --- i).
This step is justiﬁed because the product of two given numbers having no common
factor (such as powers of distinct primes) divides a given number if and only if
each of the two given numbers does.
(22) At this point all atomic formulas with Ds and involving x have the form
Dm(x −i), where i is a nonnegative integer, and m a power of a prime. If in a
given disjunct there are two conjuncts Dm(x −i) and Dn(x −j) where m and n
are powers of the same prime, say m = pd, n = pe, d ≤e, then drop Dm(x --- i) in
favor of Dn(i −j), which does not involve x. This step is justiﬁed because, since

DECIDABILITY OF ARITHMETIC WITHOUT MULTIPLICATION
299
m divides n, for any number a, if a leaves remainder j on division by n, a will
leave remainder i on division by m if and only if j does.
(23) Repeat the preceding step until for any two conjuncts Dm(x --- i) and Dn(x --- j) in a
single disjunct, m and n are powers of distinct primes, and therefore have no
common factors.
(24) Replace each Dm(x --- i) by Dm(x --- i*), where i* is the remainder on dividing i
by m.
(25) Rewrite each disjunct so that all atomic formulas with with Ds and involving x are
on the left.
(26) At this point each disjunct has the form
Dm1(x --- i1) & . . . & Dmk(x --- ik) & (other conjuncts)
where 0 ≤i1 < m1, . . . , 0 ≤ik < mk. Let m = m1 · · · · · mk. According to the
Chinese remainder theorem (see Lemma 15.5), there exists a (unique) i with
0 ≤i < m such that i leaves remainder i1 on division by m1, . . . , i leaves
remainder ik on division by mk. Replace the conjuncts involving Ds by the single
formula Dm(x --- i).
(27) At this point we have a disjunction F1 ∨· · · ∨Fk each of whose disjuncts is a
conjunction containing at most one lower inequality, at most one upper inequality,
and at most one formula of form Dm(x −i). Rewrite ∃x(F1 ∨· · · ∨Fk) as
∃x F1 ∨· · · ∨∃x Fk.
(28) Within each disjunct ∃xF, rewrite the conjunction F so that any and all conjuncts
involving x occur on the left, and conﬁne the quantiﬁer to these conjuncts, of
which there are at most three; if there are none, simply omit the quantiﬁer.
(29) At this point, the only occurrences of x are in sentences of one of the seven types
listed in Table 24-1. Replace these by the sentences listed on the right.
Table 24-1. Elimination of quantiﬁers
∃x s < jx
0 < 1
∃x kx < t
0 < 1
∃x Dm(x --- i)
0 < 1
∃x(Dm(x --- i) & s < jx)
0 < 1
∃x(Dm(x --- i) & kx < t)
0 < 1
∃x(s < jx & kx < t)
∃x(D jk(x --- 0) & ks < x & x < jt)
∃x(Dm (x --- i) & s < jx & kx < t)
∃x(D jkm(x --- jki) & ks < x & x < jt)
This step is justiﬁed in the ﬁrst ﬁve cases because in these cases the sentence on
the right is automatically true. (In the fourth case this is because there are
arbitrarily large integers leaving a prescribed remainder i on division by m, and
similarly in the ﬁfth case.) The sixth and seventh cases are similar to each other.
We discuss the latter because it is slightly more complicated. First note that the
sentence on the left,
∃x(Dm(x −i) & s < jx & kx < t)
(i)
is coextensive with
∃x(D jkm( jkx −jki) & ks < jkx & jkx < jt).
(ii)

300
DECIDABILITY OF ARITHMETIC WITHOUT MULTIPLICATION
This in turn is coextensive with
∃y(D jkm(y −jki) & ks < y & y < jt)
(iii)
which is the sentence on the right, except for relettering the variable. For if x is as
in (ii), then y = jkx will be as in (iii); and conversely, if y is as in (iii), then since
jk divides y −jki, jk must divide y, which is to say that y will be of the form jkx
for some x, which x will then be as in (ii).
(30) At this point, the only occurrences of x are in sentences of the form
∃x(Dm(x −i) & s < x & x < t).
Replace this by the disjunction of
Dm(s + j −i) & s + j < t
for all j with 1 ≤j ≤m. This step is justiﬁed because, given two integers a and b,
there will be an integer strictly between them that leaves the same remainder as i
when divided by m if and only if one of a + 1, . . . , a + m is such an integer.
We now have eliminated x altogether, and have obtained a quantiﬁer-free formula
coextensive with our original formula and involving no additional free variables, and
we are done.
Problems
24.1 Consider monadic logic without identity, and add to it a new quantiﬁer
(Mx)(A(x) > B(x)), which is to be true if and only if there are more x such
that A(x) than there are x such that B(x). Call the result comparative logic.
Show how to deﬁne in terms of M:
(a) ∀and ∃(so that these can be ofﬁcially dropped and treated as mere
abbreviations)
(b) ‘most x such that A(x) are such that B(x)’
24.2 Deﬁne a comparison to be a formula of the form (Mx)(A(x) > B(x)) where
A(x) and B(x) are quantiﬁer-free. Show that any sentence is equivalent to a
truth-functional compound of comparisons (which then by relettering may be
taken all to involve the same variable x).
24.3 As with sets of sentences of ﬁrst-order logic, a set of sentences of logic with
the quantiﬁer M is (ﬁnitely) satisﬁable if there is an interpretation (with a ﬁnite
domain) in which all sentences in the set come out true. Show that ﬁnite satis-
ﬁability for ﬁnite sets of sentences of logic with the quantiﬁer M is decidable.
(The same is true for satisﬁability, but this involves more set theory than we
wish to presuppose.)
24.4 For present purposes, by an inequality is meant an expression of the form
a1x1 + · · · + amxm § b
where the xi are variables, the ai and b are (numerals for) speciﬁc rational
numbers, and § may be any of <, ≤, >, ≥. A ﬁnite set of inequalities is coherent

PROBLEMS
301
if there are rational numbersri that if taken for the xi would make each inequality
in the set come out true (with respect to the usual addition operation and order
relation on rational numbers). Show that there is a decision procedure for the
coherence of ﬁnite sets of inequalities.
24.5 In sentential logic the only nonlogical symbols are an enumerable inﬁnity of
sentence letters, and the only logical operators are negation, conjunction, and
disjunction ∼, &, ∨. Let A1, . . . , An be sentence letters, and consider sentences
ofsententiallogicthatcontainnosentenceletters,butthe Ai,orequivalently,that
are truth-functional compounds of the Ai. For each sequence e = (e1, . . . , en)
of 0s and 1s, let Pe be (∼)A1 & . . . & (∼)An, where for each i, 1 ≤i ≤n, the
negation sign preceding Ai is present if ei = 0, and absent if ei = 1. For present
purposes a probability measure μ may be deﬁned as an assignment of a rational
number μ(Pe) to each Pe in such a way that the sum of all these numbers is 1.
For a truth-functional combination A of the Ai we deﬁne μ(A) to be the sum
of the μ(Pe) for those Pe that imply A, or equivalently, that are disjuncts in
the full disjunctive normal form of A). The conditional probability μ(A\B) is
deﬁned to be the quotient μ(A & B)/μ(A) if μ(A) ̸= 0, and is conventionally
taken to be 1 if μ(A) = 0. For present purposes, by a constraint is meant an
expression of the form μ(A) § b or μ(A\B) § b, where A and B are sentences
of sentential logic, b a nonnegative rational number, and § any of <, ≤, >, ≥.
A ﬁnite set of constraints is coherent if there exists a probability measure μ
that makes each constraint in the set come out true. Is the set of constraints
μ(A\B) = 3/4, μ(B\C) = 3/4, and μ(A\C) = 1/4 coherent?
24.6 Show that there is a decision procedure for the coherence of ﬁnite sets of
constraints.

25
Nonstandard Models
By a model of (true) arithmetic is meant any model of the set of all sentences of the
language L of arithmetic that are true in the standard interpretation N. By a nonstan-
dard model is meant one that is not isomorphic to N. The proof of the existence of an
(enumerable) nonstandard model of arithmetic is as an easy application of the compact-
ness theorem (and the L¨owenheim–Skolem theorem). Every enumerable nonstandard
model is isomorphic to a nonstandard model M whose domain is the same as that of
N, namely, the set of natural numbers; though of course such an M cannot assign
the same denotations as N to the nonlogical symbols of L. In section 25.1 we analyze
the structure of the order relation in such a nonstandard model. A consequence of this
analysis is that, though the order relation cannot be the standard one, it at least can be
a recursive relation. By contrast, Tennenbaum’s theorem tells us that it cannot happen
that the addition and multiplication relations are recursive. This theorem and related
results will be taken up in section 25.2. Section 25.3 is a sort of appendix (independent
of the other sections, but alluding to results from several earlier chapters) concerning
nonstandard models of an expansion of arithmetic called analysis.
25.1 Order in Nonstandard Models
Let M be a model of (true) arithmetic not isomorphic to the standard model N. (The
existence of such models was established in the problems at the end of Chapter 12, as
an application of the compactness theorem.) What does such a model look like? We’ll
call the objects in the domain |M| NUMBERS. M assigns as denotation to the symbol
0 some NUMBER O we’ll call ZERO, and to the symbol ′ some function † on NUMBERS
we’ll call SUCCESSOR. It assigns to < some relation ≺we’ll call LESS THAN, and
to + and · some functions ⊕and ⊗we’ll call ADDITION and MULTIPLICATION. Our
main concern in this section will be to understand the LESS THAN relation.
First of all, no NUMBER is LESS THAN itself. For no (natural) number is less than
itself. So ∀x ∼x < x is true in N, so it is true in M, and so as asserted no NUMBER
is LESS THAN itself. This argument illustrates our main technique for obtaining infor-
mation about the ‘appearance’ of M: observe that the natural numbers have a certain
property, conclude that a certain sentence of L is true in N, infer that it must also be
true in M (since the same sentences of L are true in M as in N), and decipher the
sentence ‘over’ M. In this way we can conclude that exactly one of any two NUMBERS
is LESS THAN the other, and that if one NUMBER is LESS THAN another, which is LESS
302

25.1. ORDER IN NONSTANDARD MODELS
303
THAN a third, then the ﬁrst is LESS THAN the third. LESS THAN is a linear ordering of
the NUMBERS, just as less than is a linear ordering of the numbers.
Zero is the least number, so ZERO is the LEAST NUMBER. Any number is less than
its successor, and there is no number between a given number and its successor (in
the sense of being greater than the former and less than the latter), so any NUMBER
is LESS THAN its SUCCESSOR, and there is no NUMBER between a given NUMBER and
its SUCCESSOR. In particular, 0′ (that is, 1 or one) is the next-to-least number, and O†
(which we may call I or ONE) is the next-to-LEAST NUMBER; 0′′ is next-to-next-to-least
and O†† is next-to-next-to-LEAST; and so on. So there is an initial segment O, O†,
O††, . . . of the relation LESS THAN that is isomorphic to the series 0, 0′′, 0′′, . . . of the
(natural) numbers.
We call O, O†, O††, . . . the standard NUMBERS. Any others are nonstandard. The
standard NUMBERS are precisely those that can be obtained from ZERO by applying
the SUCCESSOR operation a ﬁnite number of times. For any (natural) number n, let us
write h(n) for O††...†(n times), which is the denotation of the numeral n or 0′′...′(n times)
in M. Then the standard NUMBERS are precisely the h(n) for n a natural number. Any
others are nonstandard. Any standard NUMBER h(n) is LESS THAN any nonstandard
NUMBER m. This is because, being true in N, the sentence
∀z((z ̸= 0 & . . . & z ̸= n) →n < z)
must be true in M, so any NUMBER other than h(0), . . . , h(n) must be GREATER THAN
h(n).
[It is not quite trivial to show that there must be some nonstandard NUMBERS
in any nonstandard model M. If there were not, then h would be a function from
(natural) numbers onto the domain of M. We claim that in that case, h would be
an isomorphism between N and M, which it cannot be if M is nonstandard. First,
h would be one-to-one, because when m ̸= n, m ̸= n is true in N and so in M, so
the denotations of m and n in M are distinct, that is, h(m) ̸= h(n). Further, when
m + n = p, m + n = p is true in N and so in M, so h(m + n) = h(m) ⊕h(n).
Finally, h(m · n) = h(m) ⊗h(n) by a similar argument.]
Any number other than zero is the successor of some unique NUMBER, so any
NUMBER other than ZERO is the SUCCESSOR of some unique number. So we can deﬁne
a function ‡ from NUMBERS to NUMBERS by letting O‡ = O and otherwise letting m‡
be the unique NUMBER of which m is the SUCCESSOR. If n is standard, then n† and n‡
are standard, too, and if m is nonstandard, then m† and m‡ are nonstandard. Moreover,
if n is standard and m nonstandard, then n is LESS THAN m‡.
We’ll now deﬁne an equivalence relation ≈on NUMBERS. If a and b are NUMBERS,
we’ll say that a ≈b if for some standard(!) NUMBER c, either a ⊕c = b or b ⊕c = a.
Intuitively speaking, a ≈b if a and b are a ﬁnite distance away from each other, or
in other words, if one can get from a to b by applying † or ‡ a ﬁnite number of times.
Every standard NUMBER bears the relation ≈to all and only the standard NUMBERS.
We call the equivalence class under ≈of any NUMBER a the block of a. Thus a’s
block is
{. . . , a‡‡‡, a‡‡, a‡, a, a†, a††, a†††, . . .}.

304
NONSTANDARD MODELS
Note that a’s block is inﬁnite in both directions if a is nonstandard, and is ordered
like the integers (negative, zero, and positive).
Suppose that a is LESS THAN b and that a and b are in different blocks. Then since
a† is LESS THAN or equal to b, and a and a† are in the same block, a† is LESS THAN b.
Similarly, a is LESS THAN b‡. It follows that if there is even one member of a block A
that is LESS THAN some member of a block B, then every member of A is LESS THAN
every member of B. If this is the case, we’ll say that block A is LESS THAN block B.
A block is nonstandard if and only if it contains some nonstandard number. The
standard block is the LEAST block.
There is no LEAST nonstandard block, however. For suppose that b is a nonstandard
NUMBER. Then there is an a LESS THAN b such that either a ⊕a = b or a ⊕a ⊕I = b.
[Why? Because for any (natural) number b greater than zero there is an a less than b
such that either a + a = b or a + a + 1 = b.] Let’s suppose a ⊕a = b. (The other
case is similar.) If a is standard, so is a ⊕a. So a is nonstandard. And a is not in the
same block as b: for if a ⊕c = b for some standard c, then a ⊕c = a ⊕a, whence
c = a, contradicting the fact that a is nonstandard. (The laws of addition that hold in
N hold in M.) So a’s block is LESS THAN b’s block. Similarly, there is no GREATEST
block.
Finally, if one block A is LESS THAN another block C, then there is a third block
B that A is LESS THAN, and that is LESS THAN C. For suppose a is in A and c is in C,
and a is LESS THAN c. There there is an b such that a is LESS THAN b, b is LESS THAN
c, and either a ⊕c = b ⊕b or a ⊕c ⊕I = b ⊕b. (Averages, to within a margin of
error of one-half, always exist in N; b is the AVERAGE in M of a and c.) Suppose
a ⊕c = b ⊕b. (The argument is similar in the other case.) If b is in A, then b = a ⊕d
for some standard d, and so a ⊕c = a ⊕d ⊕a ⊕d, and so c = a ⊕d ⊕d (laws of
addition), from which it follows, as d ⊕d is standard, that c is in A. So b is not in A,
and, similarly not in C either. We may thus take as the desired B the block of b.
To sum up: the elements of the domain of any nonstandard model M of arithmetic
are going to be linearly ordered by LESS THAN. This ordering will have an initial
segment that is isomorphic to the usual ordering of natural numbers, followed by a
sequence of blocks, each of which is isomorphic to the usual ordering of the integers
(negative, zero, and positive). There is neither an earliest nor a latest block, and
between any two blocks there lies a third. Thus the ordering of the blocks is what
was called in the problems at the end of Chapter 12 a dense linear ordering without
endpoints, and so, as shown there, it is isomorphic to the usual ordering of the rational
numbers. This analysis gives us the following result.
25.1a Theorem. The order relations on any two enumerable nonstandard models of
arithmetic are isomorphic.
Proof: Let K be the set consisting of all natural numbers together with all pairs
(q, a) where q is a rational number and a and integer. Let <K be the order on K in
which the natural numbers come ﬁrst, in their usual order, and the pairs afterward,
ordered as follows: (q, a) <K (r, b) if and only if q < r in the usual order on ra-
tional numbers, or (q = r and a < b in the usual order on integers). Then what we
have shown above is that the order relation in any enumerable nonstandard model of

25.1. ORDER IN NONSTANDARD MODELS
305
arithmetic is isomorphic to the ordering <K of K. Hence the order relations in any
two such models are isomorphic to each other.
This result can be extended from models of (true) arithmetic to models of the
theory P (introduced in Chapter 16).
25.1b Theorem. The order relations on any two enumerable nonstandard models of P
are isomorphic.
Proof: We indicate the proof in outline. What one needs to do in order to extend
Theorem 25.1a from models of arithmetic to models of P is to replace every argument
‘S must be true in M because S is true in N’ that occurs above, by the argument ‘S
must be true in M because S is a theorem of P’. To show that S is indeed a theorem
of P, one needs to ‘formalize’ in P the ordinary, unformalized mathematical proof
that S is true in N. In some cases (for instance, laws of arithmetic) this has been done
already in Chapter 16; in the other cases (for instance, the existence of averages) what
needs to be done is quite similar to what was done in Chapter 16. Details are left to
the reader.
Any enumerable model of arithmetic or P (or indeed any theory) is isomorphic
to one whose domain is the set of natural numbers. Our interest in the remainder of
this chapter will be in the nature of the relations and functions that such a model
assigns as denotations to the nonlogical symbols of the language. A ﬁrst result on
this question is a direct consequence of Theorem 25.1a.
25.2 Corollary. There is a nonstandard model of arithmetic with domain the natural
numbers in which the order relation is a recursive relation (and the successor function a
recursive function).
Proof: We know the order relation on any nonstandard model of arithmetic is
isomorphic to the order <K on the set K deﬁned in the proof of Theorem 25.1a. The
main step in the proof of the corollary will be relegated to the problems at the end of the
chapter. It is to show that there is a recursive relation ≺on the natural numbers that is
also isomorphic to the order <K on the set K. Now, given any enumerable nonstandard
model M of arithmetic, there is a function h from the natural numbers to |M| that
is an isomorphism between the ordering ≺on the natural numbers and the ordering
<M on M. Much as in the proof of the canonical-domains lemma (Corollary 12.6),
deﬁne an operation † on natural numbers by letting n† be the (unique) m such that
h(m) = h(n)′M; and deﬁne functions ⊕and ⊗similarly. Then the interpretation with
domain the natural numbers and with ≺,† , ⊕, ⊗as the denotation of <, ′, +, · will
be isomorphic to M. It will thus be a model of arithmetic, with the order relation ≺
recursive.(Ifoneiscareful,onecangetthesuccessorfunction † toberecursiveaswell.)
The L¨owenheim–Skolem theorem tells us that any theory that has an inﬁnite model
has a model with domain the natural numbers. The arithmetical L¨owenheim–Skolem
theorem asserts that any axiomatizable theory that has an inﬁnite model has a model
with domain the natural numbers and the denotation of every nonlogical symbol an
arithmetical relation or function. The proof of this result requires careful review of

306
NONSTANDARD MODELS
the proof of the model existence lemma in Chapter 13. It is outlined in the problems
at the end of this chapter. While (true) arithmetic is not an axiomatizable theory, P
is, and so the arithmetical L¨owenheim–Skolem theorem gives us the following.
25.3 Corollary. There is a nonstandard model of P with domain the natural numbers
in which the denotation of every nonlogical symbol is an arithmetical relation or function.
Proof: As in the proof of the existence of nonstandard models of arithmetic, add a
constant ∞to the language of arithmetic and apply the compactness theorem to the
theory
P ∪{∞̸= n: n = 0, 1, 2, . . . }
to conclude that it has a model (necessarily inﬁnite, since all models of P are). The
denotation of ∞in any such model will be a nonstandard element, guaranteeing that
the model is nonstandard. Then apply the arithmetical L¨owenheim–Skolem theorem
to conclude that the model may be taken to have domain the natural numbers, and the
denotations of all nonlogical symbols arithmetical.
The results of the next section contrast sharply with Corollaries 25.2 and 25.3.
25.2 Operations in Nonstandard Models
Our goal in this section is to indicate the proof of two strengthenings of Tennenbaum’s
theorem to the effect that there is no nonstandard model of P with domain the natural
numbers in which the addition and multiplication functions are both recursive, along
with two analogues of these strengthened results. Speciﬁcally, the four results are as
follows.
25.4a Theorem. There is no nonstandard model of (true) arithmetic with domain the
natural numbers in which the addition function is arithmetical.
25.4b Theorem (Tennenbaum–Kreisel theorem). There is no nonstandard model of P
with domain the natural numbers in which the addition function is recursive.
25.4c Theorem. There is no nonstandard model of (true) arithmetic with domain the
natural numbers in which the multiplication function is arithmetical.
25.4d Theorem (Tennenbaum–McAloon theorem). There is no nonstandard model of
P with domain the natural numbers in which the multiplication function is recursive.
The proof of Theorem 25.4a will be given in some detail. The modiﬁcations
needed to prove Theorem 25.4b and those needed to prove Theorem 25.4c will both
be indicated in outline. A combination of both kind of modiﬁcations would be needed
for Theorem 25.4d, which will not be further discussed.
Throughout the remainder of this section, by formula we mean formula and sen-
tence of the language of arithmetic L, and by model we mean an interpretation of L
with domain the set of natural numbers. For the moment our concern will be with
models of (true) arithmetic. Let M be such a model that is not isomorphic to the

25.2. OPERATIONS IN NONSTANDARD MODELS
307
standard model N, and let us use ⊕and ⊗as in the preceding section for the deno-
tations it assigns to the addition and multiplication symbols.
A notational preliminary: Our usual notation for the satisfaction in a model M of
a formula F(x, y) by elements a and b of the domain has been M |= F[a, b]. For the
remainder of this chapter, rather than write, for instance, ‘let F(x, y) be the formula
∃z x = y · z and let M |= F[a, b]’, we are just going to write ‘let M |= ∃z x =
y · z[a, b]’. (Potentially this briefer notation is ambiguous where there is more than
one free variable, since nothing in it explicitly indicates that it is a that goes with x and
b with y rather than the other way around; actually, context and alphabetical order
should always be sufﬁcient to indicate what is intended.) Thus instead of writing
‘Let F(z) be the formula n < z and suppose M |= F[d]’, we just write ‘Suppose
M |= n < z[d]’. In this notation, a number d is a nonstandard element of M if and
only if for for every n, M |= n < z[d]. (If d is nonstandard, M |= d < z[d].)
We know from the previous section that nonstandard elements exist. The key to
proving Theorem 25.4a is a rather surprising result (Lemma 25.7a below) asserting
the existence of nonstandard elements with special properties. In the statement of this
result and the lemmas needed to prove it, we write π(n) for the nth prime (counting 2
as the zeroth, 3 as the ﬁrst, and so on). In order to be able to write about π in
the language of arithmetic, ﬁx a formula (x, y) representing the function π in Q
(and hence in P and in arithemetic), as in section 16.2. Also, abbreviate as x | y
the rudimentary formula deﬁning the relation ‘x divides y’. Here, then, are the key
lemmas.
25.5a Lemma. Let M be a nonstandard model of arithmetic. For any m > 0,
M |= ∀x m · x = x + · · · + x
(m xs).
25.6a Lemma. Let M be a nonstandard model of arithmetic. Let A(x) be any formula
of L. Then there is a nonstandard element d such that
M |= ∃y∀x < z (∃w((x, w) & w | y) ↔A(x))[d].
25.7a Lemma. Let M be a nonstandard model of arithmetic. Let A(x) be any formula
of L. Then there exists a b such that for every n,
M |= A(n)
if and only if
for some a,
b = a ⊕· · · ⊕a [π(n) as].
Proof of Lemma 25.5a: The displayed sentence is true in N, and hence is true in
M.
Proof of Lemma 25.6a: It is enough to show that the sentence
∀z∃y∀x < z (∃w((x, w) & w | y) ↔A(x))
is true in N, since it must then be true in M. Now what this sentence says, interpreted
over N, is just that for every z there exists a positive y such that for all x < z, the xth
prime divides y if and only if A(x) holds. It is enough to take for y the product of the
xth prime for all x < z such that A(x) holds.

308
NONSTANDARD MODELS
BeforegivingthedetailsoftheproofofLemma25.7a,letusindicate Tennenbaum’s
main idea. Lemma 25.6a can be regarded as saying that for every z there is a y that
encodes the answers to all questions A(x)? for x less than z. Apply this to a non-
standard element d in M. Then there is a b that encodes the answers to all questions
M |= A(x)[i]? for all i LESS THAN d. But since d is nonstandard, the denotations
of all numerals are LESS THAN d. So b codes the answers to all the inﬁnitely many
questions M |= A(n)? for n a natural number.
Proof of Lemma 25.7a: Let d be as in Lemma 25.6a, so we have
M |= ∃y∀x < z (∃w((x, w) & w | y) ↔A(x))[d].
Let b be such that
M |= ∀x < z (∃w((x, w) & w | y) ↔A(x))[b, d].
Since d is nonstandard, for every n, M |= n < z[d]. Thus we have
M |= (∃w((n, w) & w | y) ↔A(n))[b].
Since  represents π, we have
M |= ∀w((n, w) ↔w = pn).
Thus for every n,
M |= pn|y ↔A(n)[b].
That is,
M |= ∃x(pn · x = y) ↔A(n)[b].
It follows that, for every n,
M |= A(n)
if and only if
for some
a,
M |= pn · x = y[a, b].
By Lemma 25.5a this means
M |= A(n)
if and only if
for some
a,
b = a ⊕· · · ⊕a [π(n) as]
as required to complete the proof.
Proof of Theorem 25.4a: Suppose ⊕is arithmetical. Then, since it is obtainable
from ⊕by primitive recursion, the function f taking a to a ⊕· · · ⊕a(n as) is arith-
metical; and then, since it is obtainable from f and π by composition, the function g
taking a to a ⊕· · · ⊕a [π(n) as] is arithmetical. (The proof in section 16.1 that re-
cursive functions are arithmetical shows that processes of composition and primitive
recursion applied to arithmetical functions yield arithmetical functions.) Hence the
relation H given by
Hbn
if and only if
for some a,
b = a ⊕· · · ⊕a [π(n) as]
or in other words
Hbn
if and only if
∃a b = g(a, n)

25.2. OPERATIONS IN NONSTANDARD MODELS
309
is arithmetical, being obtainable by existential quantiﬁcation from the graph relation
of an arithmetical function.
So let B(x, y) be a formula arithmetically deﬁning H. Let A(x) be the formula
∼B(x, x). Apply Lemma 25.7a to obtain a b such that for all n, M |= A(n) if and
only if Hbn. Since the same sentences are true in M and N, for all n, N |= A(n) if
and only if Hbn. In particular, N |= A(b) if and only if Hbb, that is, N |= ∼B(b, b)
if and only if Hbb. But since B arithmetically deﬁnes H, we also have N |= B(b, b)
if and only if Hbb. Contradiction.
For the proof of Theorem 25.4b, we need extensions of the lemmas used for
Theorem 25.4a that will apply not just to models of arithmetic but to models of
P. We state these as Lemmas 25.5b through 25.7b below. As in the case of the
extension of Theorem 25.1a to Theorem 25.1b, some ‘formalizing’ of the kind done
in Chapter 16 is needed. What is needed for Lemma 25.6b, however, goes well beyond
this; so, leaving other details to the reader, we give the proof of that lemma, before
going on to give the derivation of Theorem 25.4b from the lemmas. The proof of
Lemma 25.6b itself uses an auxiliary lemma of some interest, Lemma 25.8 below.
25.5b Lemma. Let M be a nonstandard model of P. For any m > 0,
M |= ∀x m · x = x + · · · + x(m xs).
25.6b Lemma. Let M be a nonstandard model of P. Let A(x) be any formula of L.
Then there is a nonstandard element d such that
M |= ∃y∀x < z (∃w((x, w) & w | y) ↔A(x))[d].
25.7b Lemma. Let M be a nonstandard model of P. Let A(x) be any formula of L.
Then there exists a b such that for every n,
M |= A(n)
if and only if
for some a,
b = a ⊕· · · ⊕a [π(n) as].
25.8 Lemma (Overspill principle). Let M be a nonstandard model of P. Let B(x) be
any formula of L that is satisﬁed in M by all standard elements. Then B(x) is satisﬁed in
M by some nonstandard element.
Proof of Lemma 25.8: Assume not. Then for any d that satisﬁes B(x) in M, d is
standard, hence d† is standard, and hence d† satisﬁes B(x) in M. Thus
M |= ∀x(B(x) →B(x′))
since O, being standard, satisﬁes B(x) in M, M |= B(0). But also
M |= (B(0) & ∀x(B(x) →B(x′))) →∀x B(x)
since this is an axiom of P. So M |= ∀x B(x) and every element satisﬁes B(x) in M,
contrary to assumption.
Proof of Lemma 25.6b: It is possible to formalize the proof of
∀z∃y∀x < z (∃w((x, w) & w | y) ↔A(x))

310
NONSTANDARD MODELS
in P, but to do so would be both extremely tedious and entirely unnecessary, since in
view of the preceding lemma it is enough to show that
∃y∀x < z (∃w((x, w) & w | y) ↔A(x))
is satisﬁed by all standard elements, and for this it is enough to show that for every
n, the following is a theorem of P:
∃y∀x < n (∃w((x, w) & w | y) ↔A(x)).
(1)
Let n = m + 1. First recall that the following is a theorem of P:
∀x(x < n ↔(x = 0 ∨· · · ∨x = m)).
(2)
Since  represents π, writing pi for π(i), for all i < n the following is a theorem
of P:
∀w((i, w) ↔w = pi).
(3)
Using (2) and (3), (1) is provably equivalent in P to
∃y((p0 | y ↔A(0)) & . . . & (pm | y ↔A(m))).
(4)
For each sequence e = (e0, . . . , em) of length n of 0s and 1s, let Ae be the conjunction
of all (∼)A(i), where the negation sign is present if ei = 0 and absent if ei = 1. Let
Be(y) be the analogous formula with pi | y in place of A(i). Then the formula after
the initial quantiﬁer in (4) is logically equivalent to the disjunction of all conjunctions
Ae & Be(y). The existential quantiﬁer may be distributed through the disjunction, and
in each disjunct conﬁned to the conjuncts that involve the variable y. Thus (4) is
logically equivalent to the disjunction of all conjunctions Ae & ∃yBe(y). Hence (1)
is provably equivalent in P to this disjunction. But ∃yBe(y) is a true ∃-rudimentary
sentence, and so is provable in P. Hence (1) is provably equivalent in P to the dis-
junction of all Ae. But this disjunction is logically valid, hence provable in P or any
theory. So (1) is provable in P.
Proof of Theorem 25.4b: We need a fact established in the problems at the end
of Chapter 8 (and in a different way in those at the end of Chapter 16): there exist
disjoint semirecursive sets A and B such that there is no recursive set containing A and
disjoint from B. Since the sets are semirecursive, there are ∃-rudimentary formulas
∃yα(x, y) and ∃yβ(x, y) deﬁning them. Replacing these by
∃y(α(x, y)) & ∼∃z ≤yβ(x, y))
and
∃y(β(x, y) & ∼∃z ≤y α(x, y))
we get ∃-rudimentary formulas α*(x) and β*(x) also deﬁning A and B, and for which
∼∃x(α*(x) & β*(x)) is a theorem of P. If n is in A, then since α*(n) is ∃-rudimentary
and true, it is a theorem of P, and hence M |= α*(n); while if n is in B, then similarly
β*(n) is a theorem of P and hence so is ∼α*(n), so that M |= ∼α*(n).
Now by Lemma 25.7b, there are elements b+ and b−such that for every n,
M |= α*(n)
if and only if
for some a,
b+ = a ⊕· · · ⊕a (π(n) as)
M |= ∼α*(n)
if and only if
for some a,
b−= a ⊕· · · ⊕a (π(n) as).

25.2. OPERATIONS IN NONSTANDARD MODELS
311
Let Y + be {n: M |= α*(n)}, and let Y −be its complement, {n: M |= ∼α*(n)}. Then
we have
Y + = {n: for some a, b+ = a ⊕· · · ⊕a (π(n) as)}.
If the function ⊕is recursive, then (much as in the proof of Theorem 25.4a) since
the function g taking a to a ⊕· · · ⊕a (π(n) as) is obtainable from ⊕by primitive
recursion and composition with π, this g is recursive. Since
Y + = {n: ∃a b+ = g(a, n)}.
Y + is semirecursive. A similar argument with b−in place of b+ shows that the com-
plement Y −of Y + is also semirecursive, from which it follows that Y + is recursive.
But this is impossible, since Y + contains A and is disjoint from B.
For the proof of Theorem 25.4c, we need lemmas analogous to those used for
Theorem 25.4a, with ⊗in place of ⊕. We state these as Lemmas 25.5c through 25.7c
below. These lemmas pertain to exponentiation. Now the notation x y for exponentia-
tion is not available in L, any more than the notation π for the function enumerating
the primes. But we allow ourselves to use that in stating the lemmas, rather than use
a more correct but more cumbersome formulation in terms of a formula representing
the exponential function. We also write x ↓y for ‘y has an integral xth root’ or ‘y is
the xth power of some integer’. The only real novelty comes in the proof of Lemma
25.6c, so we give that proof, leaving other details to the reader.
25.5c Lemma. Let M be a nonstandard model of arithmetic. For any m > 0,
M |= ∀x xm = x · · · · · x (m xs).
25.6c Lemma. Let M be a nonstandard model of arithmetic. Let A(x) be any formula
of L. Then there is a nonstandard element d such that
M |= ∃y∀x < z (∃w((x, w) & w ↓y) ↔A(x))[d].
25.7c Theorem. Let M be a nonstandard model of arithmetic. Let A(x) be any formula
of L. Then there exists a b such that for every n,
M |= A(n)
if and only if
for some a,
b = a ⊗· · · ⊗a (π(n) as).
Proof of Lemma 25.6c: It is enough to show that
∀z∃y∀x < z (∃w((x, w) & W↓y) ↔A(x))
is true in N, since it must then be true in M. Recall that we have shown in the proof
of Lemma 25.6b that
∀z∃y∀x < z (∃w((x, w) & w | y) ↔A(x))
is true in N. It sufﬁces to show, therefore, that the following is true in N:
∀y∃v∀w(w ↓v ↔w | y).
In fact, given y, 2y will do for v (unless y = 0, in which case v = 0 will do). For
suppose w divides y, say y = uw. Then 2y = 2uw = (2u)w, and 2y is a wth power.

312
NONSTANDARD MODELS
And suppose conversely 2y is a wth power, say 2y = tw. Then t cannot be divisible
by any odd prime, and so must be a power of 2, say t = 2u. Then 2y = (2u)w = 2uw,
and y = uw, so y is divisible by w.
25.3 Nonstandard Models of Analysis
In the language L* of arithmetic, under its standard interpretation N* (to revert to
our former notation), we can directly ‘talk about’ natural numbers, and can indirectly,
through coding, ‘talk about’ ﬁnite sets of natural numbers, integers, rational numbers,
and more. We cannot, however, ‘talk about’ arbitrary sets of natural numbers or
objects that might be coded by these, such as real or complex numbers. The language
of analysis L**, and its standard interpretation N**, let us do so.
This language is an example of a two-sorted ﬁrst-order language. In two-sorted
ﬁrst-order logic there are two sorts of variables: a ﬁrst sort x, y, z, . . . , which may
be called lower variables, and a second sort X, Y, Z, . . . , which may be called upper
variables. For each nonlogical symbol of a two-sorted language, it must be speciﬁed
not only how many places that symbol has, but also which sorts of variables go into
which places. An interpretation of a two-sorted language has two domains, upper
and lower. A sentence ∀x F(x) is true in an interpretation if every element of the
lower domain satisﬁes F(x), while a sentence ∀XG(X) is true if every element of
the upper domain satisﬁes G(X). Otherwise the deﬁnitions of language, sentence,
formula, interpretation, truth, satisfaction, and so forth are unchanged from ordinary
or one-sorted ﬁrst-order logic.
An isomorphism between two interpretations of a two-sorted language consists of
a pair of correspondences, one between the lower domains and the other between
the upper domains of the two interpretations. The proof of the isomorphism lemma
(Proposition 12.5) goes through for two-sorted ﬁrst-order logic, and so do the proofs
of more substantial results such as the compactness theorem and the L¨owenheim–
Skolem theorem (including the strong L¨owenheim–Skolem theorem of Chapter 19).
Note that in the L¨owenheim–Skolem theorem, an interpretation of a two-sorted lan-
guage counts as enumerable only if both its domains are enumerable.
In the language of analysis L** the nonlogical symbols are those of L*, which
take only lower variables, plus a further two-place predicate ∈, which takes a lower
variable in its ﬁrst place but an upper in its second. Thus x ∈Y is an atomic formula,
but x ∈y, X ∈Y, and X ∈y are not. In the standard interpretation N** of L*, the
lower domain is the set of natural numbers and the interpretation of each symbol of
L is the same as in the standard interpretation N* of L. The upper domain is the
class of all sets of natural numbers, and the interpretation of ∈is the membership or
elementhood relation ∈between numbers and sets of numbers. As (true) arithmetic
is the set of sentences of L* true in N*, so (true) analysis is the set of all sentences
of L** true in N**. A model of analysis is nonstandard if it is not isomorphic to
N**. Our aim in this section is to gain some understanding of nonstandard models
of (true) analysis and some important subtheories thereof.
By the lower part of an interpretation of L**, we mean the interpretation of L*
whose domain is the lower domain of the given interpretation, and that assigns to each

25.3. NONSTANDARD MODELS OF ANALYSIS
313
nonlogical symbol of L* the same denotation as does the given interpretation. Thus
the lower part of N** is N*. A sentence of L* will be true in an interpretation of L**
if and only if it is true in the lower part of that interpretation. Thus a sentence of L* is
a theorem of (that is, is in) true arithmetic if and only if it is a theorem of true analysis.
Our ﬁrst aim in this section will be to establish the existence of nonstandard models
of analysis of two distinct kinds. An interpretation of L** is called an ∈-model if (as
in the standard interpretation) the elements of the upper domain are sets of elements
of the lower domain, and the interpretation of ∈is the membership or elementhood
relation ∈(between elements of the lower and the upper domain). The sentence
∀X∀Y(∀x(x∈X ↔x∈Y) →X = Y)
is called the axiom of extensionality. Clearly it is true in any ∈-model and hence in
any model isomorphic to an ∈-model. Conversely, any model M of extensionality
is isomorphic to an ∈-model M#. [To obtain M# from M, keep the same lower
domain and the same interpretations for symbols of L*, replace each element α of
the upper domain of M by the set α# of all elements a of the lower domain such that
a ∈M α, and interpret ∈not as the relation ∈M but as ∈. The identity function on the
lower domain together with the function sending α to α# is an isomorphism. The only
point that may not be immediately obvious is that the latter function is one-to-one. To
see this, note that if α# = β#, then α and β satisfy ∀x(x ∈X ↔x ∈Y) in M, and
since (2) is true in M, α and β must satisfy X = Y, that is, we must have α = β.]
Since we are going to be interested only in models of extensionality, we may restrict
our attention to ∈-models.
If the lower part of an ∈-model M is the standard model of arithmetic, we call
M an ω-model. The standard model of analysis is, of course, an ω-model. If an
ω-model of analysis is nonstandard, its upper domain must consist of some class
of sets properly contained in the class of all sets of numbers. If the lower part of an
∈-model M is isomorphic to the standard interpretation N* of L*, then M as a whole
is isomorphic to an ω-model M#. [If j is the isomorphism from N* to the lower part
of M, replace each element α of the upper domain of M by the set of n such that
j(n) ∈α, to obtain M#.] So we may restrict our attention to models that are of one of
two kinds, namely, those that either are ω-models, or have a nonstandard lower part.
Our ﬁrst result is that nonstandard models of analysis of both kinds exist.
25.9 Proposition. Both nonstandard models of analysis whose lower part is a nonstan-
dard model of arithmetic and nonstandard ω-models of analysis exist.
Proof: The existence of nonstandard models of arithmetic was established in the
problems at the end of Chapter 12 by applying the compactness theorem to the theory
that results upon adding to arithmetic a constant ∞and the sentences ∞̸= n for all
natural numbers n. The same proof, with analysis in place of arithmetic, establishes
the existence of a nonstandard model of analysis whose lower parts is a nonstandard
model of arithmetic. The strong L¨owenheim–Skolem theorem implies the existence
of an enumerable subinterpretation of the standard model of analysis that is itself
a model of analysis. This must be an ω-model, but it cannot be isomorphic to the
standard model, whose upper domain is nonenumerable.

314
NONSTANDARD MODELS
The axiomatizable theory in L* to which logicians have devoted the most attention
is P, which consists of the sentences deducible from the following axioms:
(0) The ﬁnitely many axioms of Q
(1) For each formula F(x) of L*, the sentence
(F(0) & ∀x(F(x) →F(x′))) →∀x F(x).
It is to be understood that in (1) there may be other free variables u, v, . . . present,
and that what is really meant by the displayed expression is the universal closure
∀u∀v · · · (F(0, u, v, . . .) & ∀x(F(x, u, v, . . .) →F(x′, u, v, . . .))
→∀x F(x, u, v, . . .)).
The sentence in (1) is called the induction axiom for F(x).
Theaxiomatizabletheoryin L**towhichlogicianshavedevotedthemostattention
is the theory P** consisting of the sentences deducible from the following axioms:
(0) The ﬁnitely many axioms of Q
(1*) ∀X(0 ∈X & ∀x(x ∈X →x′ ∈X) →∀x x ∈X)
(2) ∀X∀Y(∀x(x ∈X ↔x ∈Y) →X = Y)
(3) For each formula F(x) of L*, the sentence
∃X∀x(x∈X ↔F(x)).
It is to be understood that in (3) there may be other free variables u, v, . . . and/or
U, V, . . . present, and that what is really meant by the displayed expression is the
universal closure
∀u∀v · · · ∀U∀V · · · ∃X∀x(x ∈X ↔F(x, u, v, . . . , U, V, . . .)).
The sentence (1*) is called the induction axiom of P**, the extensionality axiom
(2) has already been encountered, and the sentence (3) is called the comprehension
axiom for F(x). We call P** axiomatic analysis.
Since the set of theorems of (true) arithmetic is not arithmetical, the set of theorems
of (true) analysis is not arithmetical, and a fortiori is not semirecursive. By contrast,
the set of theorems of axiomatic analysis P** is, like the set of theorems of any
axiomatizable theory, semirecursive. There must be many theorems of (true) analysis
that are not theorems of axiomatic analysis, and indeed (since the G¨odel theorems
apply to P**), among these are the G¨odel and Rosser sentences of P**, and the
consistency sentence for P**.
Note that the induction axiom (1) of P for F(x) follows immediately from the
induction axiom (1) of P** together with the comprehension axiom (3) for F(x).
Thus every theorem of P is a theorem of P**, and the lower part of any model of
P** is a model of P. We say a model of P is expandable to a model of P** if it is the
lower part of a model of P**. Our second result is to establish the nonexistence of
certain kinds of nonstandard models of P**.
25.10 Proposition
Not every model of P can be expanded to a model of P**.

25.3. NONSTANDARD MODELS OF ANALYSIS
315
Proof: We are not going to give a full proof, but let us indicate the main idea.
Any model of P that can be expanded to a model of P** must be a model of every
sentence of L* that is a theorem of P**. Let A be the consistency sentence for P
(or the G¨odel or Rosser sentence). Then A is not a theorem of P, and so there is a
model of P ∪{∼A}. We claim such a model cannot be expanded to a model of P**,
because A is provable in P**. The most simple-minded proof of the consistency of
P is just this: every axiom of P is true, only truths are deducible from truths, and
0 = 1 is not true; hence 0 = 1 is not deducible from P. In section 23.1 we in effect
produced a formula F(X) of L** which is satisﬁed in the standard model of analysis
by and only by the set code numbers of sentences of L* that are true in the lower
part of that model (that is, in the standard model of arithmetic). Working in P**,
we can introduce the abbreviation True(x) for ∃X(F(X) & x ∈X), and ‘formalize’
the simple-minded argument just indicated. (The work of ‘formalization’ required,
which we are omitting, is extensive, though not so extensive as would be required for
a complete proof of the second incompleteness theorem.)
Recall that if a language L1 is contained in a language L2, a theory T1 in L1 is
contained in a theory T2 in L2, then T2 is called a conservative extension of T1 if and
only if every sentence of L1 that is a theorem of T2 is a theorem of T1. What is shown
in the proof indicated for the preceding proposition is, in this terminology, that P**
is not a conservative extension of P.
A weaker variant P+ allows the comprehension axioms (3) only for formulas F(x)
not involving bound upper variables. [There may still be, in addition to free lower
variables u, v, . . . , free upper variables U, V, . . . in F(X).] P+ is called (strictly)
predicative analysis. When one speciﬁes a set by specifying a condition that is nec-
essary and sufﬁcient for an object to belong to the set, the speciﬁcation is called
impredicative if the condition involves quantiﬁcation over sets. Predicative analysis
does not allow impredicative speciﬁcations of sets. In ordinary, unformalized mathe-
matical argument, impredicative speciﬁcations of sets of numbers are comparatively
common: for instance, in the ﬁrst section of the next chapter, an ordinary, unfor-
malized mathematical proof of a principle about sets of natural numbers called the
‘inﬁnitary Ramsey’s theorem’ will be presented that is a typical example of a proof
that can be ‘formalized’ in P** but not in P+.
An innocent-looking instance of impredicative speciﬁcation of a set is implicitly
involved whenever we deﬁne a set S of numbers as the union S0 ∪S1 ∪S2 ∪· · · of
a sequence of sets that is deﬁned inductively. In an inductive deﬁnition, we specify
a condition F0(u) such that u belongs to S0 if and only if F0(u) holds, and specify a
condition F′(u, U) such that for all i, u belongs to Si+1 if and only if F′(u, Si) holds.
Such an inductive deﬁnition can be turned into a direct deﬁnition, since x ∈S if and
only if
there exists a ﬁnite sequence of sets U0, . . . , Un such that
for all u, u ∈U0, if and only if F0(u)
for all i < n, for all u, u ∈Ui+1 if and only if F′(u, Ui)
x ∈Un.

316
NONSTANDARD MODELS
But while the quantiﬁcation ‘there exists a ﬁnite sequence of sets’ can by suitable
coding be replaced by a quantiﬁcation ‘there exists a set’, in general the latter quan-
tiﬁcation cannot be eliminated. The inductive deﬁnition implicitly involves—what
the corresponding direct deﬁnition explicitly involves—an impredicative speciﬁca-
tion of a set. In general, one cannot ‘formalize’ in P+ arguments involving this kind
of inductive speciﬁcation of sets, even if the conditions F0 and F′ involve no bound
upper variables.
Also, one cannot ‘formalize’ in P+ the proof of the consistency sentence for P
indicated in the proof of the preceding proposition. [One can indeed introduce the
abbreviation True(x) for ∃X(F(X) & x ∈X), but one cannot in P prove the existence
of {x: True(x)}, and so cannot apply the induction axiom to prove assertions involving
the abbreviation True(x).] So the proof indicated for the preceding proposition fails
for P+ in place of P*. In fact, not only is the consistency sentence for P not an example
of a sentence of L* that is a theorem of P+ and not of P, but actually there can be no
example of such sentence: P+ is a conservative extension of P.
Our last result is a proposition immediately implying the fact just stated.
25.11 Proposition. Every model of P can be expanded to a model of P+.
Proof: Let M be a model of P. Call a subset S of the domain |M| parametri-
cally deﬁnable over M if there exist a formula F(x, y1, . . . , ym) of L* and elements
a1, . . . , am of |M| such that
S = {b: M |= F[b, a1, . . . , am]}.
Expand M to an interpretation of L** by taking as upper domain the class of all
parametrically deﬁnable subsets of M, and interpreting ∈as ∈.We claim the expanded
model M+ is a model of P+. The axioms that need checking are induction (1) and
comprehension (3) (with F having no bound upper variables). Leaving the former to
the reader, we consider an instance of the latter:
∀u1∀u2∀U1∀U2∃X∀x(x ∈X ↔F(x, u1, u2, U1, U2)).
(In general, there could be more than two us and more than two Us, but the proof
would be no different.) To show the displayed axiom is true in M+, we need to show
that for any elements s1, s2 of |M| and any parametrically deﬁnable subsets S1, S2 of
|M| there is a parametrically deﬁnable subset T of |M| such that
M+ |= ∀x(x ∈X ↔F(x, u1, u2, U1, U2))[s1, s2, S1, S2, T ].
Equivalently, what we must show is that for any such s1, s2, S1, S2, the set
T = {b: M+ |= F(x, u1, u2, U1, U2)[s1, s2, S1, S2, b]}
is parametrically deﬁnable. To this end, consider parametric deﬁnitions of U1, U2:
U1 = {b: M |= G1[b, a11, a12]}
U2 = {b: M |= G2[b, a21, a22]}.

PROBLEMS
317
(In general, there could be more than two as for each U, but the proof would be no
different.) Now let
H(x, u1, u2, v11, v12, v21, v22)
be the result of replacing any subformula of form Ui(w) by Gi(w, vi1, vi2). Then
T = {b: M |= H[b, s1, s2, a11, a12, a21, a22]}
and is parametrically deﬁnable as required.
Problems
25.1 Show how the proof of the existence of averages can be formalized in P, in
the style of Chapter 16.
25.2 Show that there is a recursive relation ≺on the natural numbers that is also
isomorphic to the order <K on the set K deﬁned in the proof of Theorem 25.1.
25.3 Show that the successor function † associated with ≺may also be taken to be
recursive.
25.4 Show that in an ∈-model that is not an ω-model, the upper domain cannot
contain all subsets of the lower domain.
The remaining problems outline the proof of the arithmetical L¨owenheim–
Skolem theorem, and refer to the alternative proof of the model existence lemma
in section 13.5 and the problems following it.
25.5 Assuming Church’s thesis, explain why, if  is a recursive set of (code numbers
of) sentences in a recursive language, the set * obtained by adding (the code
numbers of) the Henkin sentences to  is still recursive (assuming a suitable
coding of the language with the Henkin constants added).
25.6 Explain why, if  is an arithmetical set of sentences, then the relation
i codes a ﬁnite set of sentences 	,
j codes a sentence D,
and
 ∪	 implies D
is also arithmetical.
25.7 Suppose * is a set of sentences in a language L* and i0, i1, . . . an enumeration
of all the sentences of L*, and suppose we form # as the union of sets n,
where 0 = * and n+1 = n if n implies ∼in, while n+1 = n ∪{in}
otherwise. Explain why, if * is arithmetical, then # is arithmetical.
25.8 Suppose we have a language with relation symbols and enumerably many
constants c0, c1, . . . , but function symbols and identity are absent. Suppose
# is arithmetical and has the closure properties required for the construction
of section 13.2. In that construction take as the element cM
i
associated with
the constant ci the number i. Explain why the relation RM associated with
any relation symbol R will then be arithmetical.
25.9 Suppose we have a language with relation symbols and enumerably many
constants c0, c1, . . . , but that function symbols are absent, though identity
may be present. Suppose # is arithmetical has the closure properties required

318
NONSTANDARD MODELS
for the construction of section 13.3. Call i minimal if there is no j < i such
that ci = c j is in #. Show that the function δ taking n to the nth number i
such that ci is minimal is arithmetical.
25.10 Continuing the preceding problem, explain why for every constant c there
is a unique n such that c = δ(n) is in #, and that if in the construction of
section 13.3 we take as the element cM
i
associated with the constant ci this
number n, then the relation RM associated with any relation symbol R will
be arithmetical.
25.11 Explain how the arithmetical L¨owenheim–Skolem theorem for the case where
function symbols are absent follows on putting together the preceding six
problems, and indicate how to extend the theorem to the case where they are
present.

26
Ramsey’s Theorem
Ramsey’s theorem is a combinatorial result about ﬁnite sets with a proof that has inter-
esting logical features. To prove this result about ﬁnite sets, we are ﬁrst going to prove,
in section 26.1, an analogous result about inﬁnite sets, and are then going to derive, in
section 26.2, the ﬁnite result from the inﬁnite result. The derivation will be an application
of the compactness theorem. Nothing in the proof of Ramsey’s theorem to be presented
requires familiarity with logic beyond the statement of the compactness theorem, but at
the end of the chapter we indicate how Ramsey theory provides an example of a sentence
undecidable in P that is more natural mathematically than any we have encountered
so far.
26.1 Ramsey’s Theorem: Finitary and Inﬁnitary
There is an old puzzle about a party attended by six persons, at which any two of the
six either like each other or dislike each other: the problem is to show that at the party
there are three persons, any two of whom like each other, or there are three persons,
any two of whom dislike each other.
The solution: Let a be one of the six. Since there are ﬁve others, either there will be
(at least) three others that a likes or there will be three others that a dislikes. Suppose
a likes them. (The argument is similar if a dislikes them.) Call the three b, c, d. Then
if (case 1) b likes c or b likes d or c likes d, then a, b, and c, or a, b, and d, or a, c,
and d, respectively, are three persons any two of whom like each other; but if (case 2)
b dislikes c, b dislikes d, and c dislikes d, then b, c, and d are three persons, any two
of whom dislike each other. And either case 1 or case 2 must hold.
The number six cannot in general be reduced; if only ﬁve persons, a, b, c, d, e are
present, then the situation illustrated in Figure 26-1 can arise. (A broken line means
‘likes’; a solid line, ‘dislikes’.) In this situation there are no three of a, b, c, d, e any
two of whom like each other (a ‘clique’) and no three, any two of whom dislike each
other (an ‘anticlique’).
A harder puzzle of the same type is to prove that at any party such as the previous
one at which eighteen persons are present, either there are four persons, any two of
whom like each other, or four persons, any two of whom dislike each other. (This
puzzle has been placed among the problems at the end of this chapter.) It is known
that the number eighteen cannot be reduced.
319

320
RAMSEY’S THEOREM
Figure 26-1. A party with no clique or anticlique of three.
We are going to prove a theorem that bears on these puzzles. Recall that by a
partition of a nonempty set we mean a family of nonempty subsets thereof, called the
classes of the partition, such that every element of the original set belongs to exactly
one of these classes. By a size-k set we mean a set with exactly k elements.
26.1 Theorem (Ramsey’s theorem). Let r, s, n be positive integers with n ≥r. Then
there exists a positive integer m ≥n such that for X = {0, 1, . . . , m −1}, no matter how
the size-r subsets of X are partitioned into s classes, there will always be a size-n subset Y
of X such that all size-r subsets of Y belong to the same class.
A set Y all of whose size-r subsets belong to the same one of the s classes is called
a homogeneous set for the partition. Note that if the theorem holds as stated, then it
clearly holds for any other size-m set in place of {0, 1, . . . , m −1}.
For instance, it holds for the set of partiers at a party where m persons are present.
In the puzzles, the size-2 subsets of the set of persons at the party were partitioned
into two classes, one consisting of the pairs of persons who like each other, the other,
of the pairs of persons who dislike each other. So in both problems r = s = 2. In the
ﬁrst, where n = 3, we showed how to prove that m = 6 is large enough to guarantee
the existence of a homogeneous set of size n—a clique of three who like each other,
or an anticlique of three who dislike each other. We also showed that 6 is the least
number m that is large enough. In the second problem, where n = 4, we reported that
m = 18 is large enough, and that 18 is in fact the least value of m that is large enough.
In principle, since there are only ﬁnitely many size-r subsets of {0, . . . , m −1},
and only ﬁnitely many ways to partition these ﬁnitely many subsets into s classes,
and since there are only ﬁnitely many size-n subsets, we could set a computer to
work searching through all partitions, and for each looking for a homogeneous set.
If some partition were found without a homogeneous set, the computer could go on
to do a similar check for {0, . . . , m}. Continuing in this way, in a ﬁnite amount of
time it would ﬁnd the least m that is large enough to guarantee the existence of the
required homogeneous set.
In practice, the numbers of possibilities to be checked are so large that such a
procedure is hopelessly infeasible. We do not at present have sufﬁcient theoretical
insight into the problem to be able to reduce the number of possibilities that would
have to be checked to the point where a computer could feasibly be employed in
surveying them in order to pinpoint the least m. And it is entirely conceivable that
because of the such physical limitations as those imposed by the speed of light, the
atomic character of matter, and the short amount of time before the universe becomes
unable to sustain life, we are never going to know exact what the value of the least m
is, even for some quite small values of r, s, and n.

26.1. RAMSEY’S THEOREM: FINITARY AND INFINITARY
321
So let us set aside the difﬁcult problem of ﬁnding the least m that is large enough,
and turn to proving that there is some m that is large enough. The proof of Theorem
26.1 that we are going to present will make a ‘detour through the inﬁnite’. First we
prove the following inﬁnitary analogue:
26.2 Theorem (Inﬁnitary Ramsey’s theorem). Let r, s be positive integers. Then no
matter how the size-r subsets of the set X = {0, 1, 2, . . . } are partitioned into s classes,
there will always be an inﬁnite subset Y of X such that all size-r subsets of Y belong to the
same class.
Note that if the theorem holds as stated, then it clearly holds for any other enu-
merably inﬁnite set in place of {0, 1, 2, . . . }. (If Zeus threw a party for an enumerable
inﬁnity of guests, any two of whom either liked each other or disliked each other,
there would either be inﬁnitely many guests, any two of whom liked each other, or
inﬁnitely many, any two of whom disliked each other.) In fact Theorem 26.2 holds
for any inﬁnite set X, because any such set has an enumerably inﬁnite subset (though
it requires the axiom of choice to prove this, and we are not going to go into the
matter).
The proof of Theorem 26.2 will be given in this section, and the derivation of
Theorem 26.1 from it—which will involve an interesting application of the compact-
ness theorem—in the next. Before launching into the proof, let us introduce some
notation that will be useful for both proofs.
A partition of a set Z into s classes may be represented by a function f whose
arguments are the elements of Z and whose values are elements of {1, . . . , s}: the
ith class in the partition is just the set of those z in Z with f (z) = i. Let us write
f : Z →W to indicate that f is a function whose arguments are the elements of
Z and whose values are elements of W. Our interest is in the case where Z is the
collection of all the size-r subsets of some set X. Let us denote this collection [X]r.
Finally, let us write ω for the set of natural numbers. Then the inﬁntary version of
Ramsey’s theorem may be restated as follows: If f : [ω]r →{1, . . . , s}, then there is
an inﬁnite subset Y of ω and a j with 1 ≤j ≤s such that f : [Y]r →{ j} (that is, f
takes the value j for any size-r subset of Y as argument).
Proof of Theorem 26.2: Our proof will proceed as follows. For any ﬁxed s > 0,
we show by induction on r that for any r > 0 we can deﬁne an operation  such that
if f : [ω]r →{1, . . . , s}, then ( f ) is a pair ( j, Y) with f : [Y]r →{ j}.
Basis step: r = 1. In this case the deﬁnition of ( f ) = ( j, Y) is easy. For each of
the inﬁnitely many size-1 sets {b}, f ({b}) is one of the ﬁnitely many positive integers
k ≤s. We can thus deﬁne j as the least k ≤s such that f ({b}) = k for inﬁnitely many
b, and deﬁne Y as {b : f ({b}) = j}.
Induction step: We assume as induction hypothesis that  has been suitably deﬁned
for all g : [ω]r →{1, . . . , s}. Suppose f : [ω]r+1 →{1, . . . , s}. In order to deﬁne
( f ) = ( j, Y), we deﬁne, for each natural number i, a natural number bi, inﬁnite sets
Yi, Zi, Wi, a function fi : [ω]r →{1, . . . , s}, and a positive integer ji ≤s. Let Y0 = ω.
We now suppose Yi has been deﬁned, and show how to deﬁne bi, Zi, fi, ji, Wi, and
Yi+1.

322
RAMSEY’S THEOREM
Let bi be the least member of Yi.
Let Zi = Yi −{bi}. Since Yi is inﬁnite, so is Zi. Let the members of Zi in increasing
order be ai0, ai1, . . ..
For any size-r set x of natural numbers, where x = {k1, . . . , kr}, with k1 < · · · <
kr, let fi(x) = f ({bi, aik1, . . . , aikr }). Since bi is not one of the aik and f is deﬁned
on all size-(r + 1) sets of natural numbers, fi is well deﬁned.
By the induction hypothesis, for some positive integer ji ≤s and some inﬁnite set
Wi, ( fi) = ( ji, Wi) and for every size-r subset x of Wi, we have fi(x) = ji. We
have thus deﬁned ji and Wi, and we deﬁne Yi+1 = {aik : k ∈Wi}.
Since Wi is inﬁnite, Yi+1 is inﬁnite. Yi+1 ⊆Zi ⊆Yi, and thus if i1 ≤i2, then
Yi2 ⊆Yi1. And since bi is less than every member of Yi+1, we have bi < bi+1, which
is the least member of Yi+1. Thus if i1 < i2 then bi1 < bi2.
For each positive integer k ≤s, let Ek = {i: ji = k}. As in the basis step, some Ek
is inﬁnite, and we let j be the least k such that Ek is inﬁnite, and let Y = {bi: i ∈E j}.
This completes the deﬁnition of .
Since bi1 < bi2 if i1 < i2, Y is inﬁnite. In order to complete the proof, we must
show that if y is a size-(r + 1) subset of Y, then f (y) = j. So suppose that y =
{bi, bi1, . . . , bir }, with i < i1 < · · · < ir and i, i1, . . . , ir all in E j. Since the Yi are
nested, all of bi1, . . . , bir are in Yi. For each m, 1 ≤m ≤r, let km be the unique
member of Wi such that bim = aikm. And let x = {k1, . . . , kr}. Then x is a sub-
set of Wi, and since i1 < · · · < ir, we have bi1 < · · · < bir , aik1 < · · · < aikr , and
k1 < · · · < kr, and thus x is a size-r subset of Wi. But ( fi) = ( ji, Wi) and thus
fi(x) = ji. Since i is in Ej, ji = j. Thus
f (y) = f

bi, bi1, . . . , bir

= f

bi, aik1, . . . , aikr

= fi(x) = j
as required.
Before moving on to the next section and the proof of Theorem 26.3, let us point
out that the following strengthening of Theorem 26.2 is simply false: Let s be a
positive integer. Then no matter how the ﬁnite sets of natural numbers are partitioned
into s classes, there will always be an inﬁnite set Y of natural numbers such that all
positive integers r, all size-r subsets of Y belong to the same one of the s classes.
Indeed, this fails for s = 2. Let f (x) = 1 if the ﬁnite set x contains the number that
is the number of members in x; and f (x) = 2 otherwise. Then there is no inﬁnite set
Y such that for every r, either f (y) = 1 for all size-r subsets y of Y or f (y) = 2 for
all such y. For if r is a positive integer that belongs to Y and b1, . . . , br are r other
members of Y, then f ({r, b2, . . . , br}) = 1, while f ({b1, . . . , br}) = 2.
26.2 K¨onig’s Lemma
In order to derive the original, ﬁnitary version of Ramsey’s theorem from the inﬁnitary
version, we will establish a principle known as K¨onig’s lemma, concerning objects
called trees. For present purposes, a tree consists of: (i) a nonempty set T of elements,
called the nodes of the tree; (ii) a partition of T into ﬁnitely or inﬁnitely many sets
T = T0 ∪T1 ∪T2 ∪· · ·

26.2. K ¨ONIG’S LEMMA
323
called the levels of the tree; and (iii) a two-place relation R subject to the following
conditions:
(1) Rab never holds for b in T0.
(2) For b in Tn+1, Rab holds for exactly one a, and that a is in Tn.
When Rab holds, we say a is immediately below b, and b is immediately above a.
Figure 26-2 is a picture of a ﬁnite tree with ten nodes and four levels. Line segments
connect nodes immediately below and above each other.
Figure 26-2. A ﬁnite tree.
A branch through a tree is a sequence of nodes b0, b1, b2, . . . with each bn imme-
diately below bn+1. Obviously, an inﬁnite tree none of whose levels is inﬁnite must
have inﬁnitely many nonempty levels. The following is not so obvious.
26.3 Lemma (K¨onig’s lemma). An inﬁnite tree none of whose levels is inﬁnite must
have an inﬁnite branch.
Postponing the proof of this result, let us see how it can be used as a bridge between
the ﬁnite and the inﬁnite.
Proof of Theorem 26.1: Suppose that Theorem 26.1 fails. Then for some positive
integers r, s, n, with n ≥r, for every m ≥n there exists a partition
f : [{0, 1, . . . , m −1}]r →{1, . . . , s}
having no size-n homogeneous set Y. Let T be the set of all such partitions without
size-n homogeneous sets for all m, and let Tk be the subset of T consisting of those
f with m = n + k. Let Rfg hold if and only if for some k
f : [{0, 1, . . . , n + k −1}]r →{1, . . . , s}
g : [{0, 1, . . . , n + k}]r →{1, . . . , s}
and g extends f, in the sense that g assigns the same value as does f to any argument
in the domain of f . It is easily seen that for any g in Tk+1 there is exactly one f in Tk
that g extends, so what we have deﬁned is a tree.
There are only ﬁnitely many functions from a given ﬁnite set to a given ﬁnite set,
so there are only ﬁnitely many nodes f in any level Tk. But our initial supposition was
that for every m = n + k there exists a partition f in Tk, so the level Tk is nonempty for
all k, and the tree is inﬁnite. K¨onig’s lemma then tells us there will be an inﬁnite branch
f0, f1, f2, . . . , which is to say, an inﬁnite sequence of partitions, each extending the
one before, and none having a size-n homogenous set. We can then deﬁne a partition
F : [ω]r →{1, . . . , s}

324
RAMSEY’S THEOREM
as follows. For any size-r subset x of ω, let p be its largest element. Then for any k
large enough that p < n + k, x will be in the domain of fk, and we have
fk(x) = fk+1(x) = fk+2(x) = · · ·.
Let F(x) be this common value.
By the inﬁnitary version of Ramsey’s theorem, F has an inﬁnite homogeneous set.
That is, there is an inﬁnite Y and a j with 1 ≤j ≤s such that F: [Y]r →{ j}. Let Z
be the set of the ﬁrst n elements of Y, and take k large enough that the largest element
of Z is less than n + k. Then Z will be a size-n subset of {0, . . . , n + k −1}, with
fk(x) = F(x) = j for all size-r subsets x of Z. In other words, Z will be a size-n
homogeneous set for fk, which is impossible, since fk is in T . This contradiction
completes the proof.
Proof of Lemma 26.3: To prove K¨onig’s lemma we are going to use the compact-
ness theorem. Let LT be the language with one one-place predicate B and with one
constant t for each node t in the tree T . Let  consist of the following quantiﬁer-free
sentences:
Bs1 ∨· · · ∨Bsk
(1)
where s1, . . . , sk are all the nodes in T0;
∼(Bs & Bt)
(2)
for all pairs of nodes s, t belonging to the same level; and
∼Bs ∨Bu1 ∨· · · ∨Bum
(3)
for every node s, where u1, . . . , um are all the nodes immediately above s. [If there
are no nodes above s, the sentence (3) is just ∼Bs.]
We ﬁrst show that if  has a model M, then T has an inﬁnite branch. By (1) there
will be at least one node r in T0 such that Br is true in M. By (2) there will in fact
be exactly one such node, call it r0. By (3) applied with r0 as s, there will be at least
one node r immediately above r0 such that Br is true in M. By (2) there will in fact be
exactly one such node, call it r1. Repeating the process, we obtain r0, r1, r2, . . . , each
immediately above the one before, which is to say that we obtain an inﬁnite branch.
We next show that  does have a model. By the compactness theorem, it is enough
to show that any ﬁnite subset  of  has a model. In fact, we can show that for any
k, the set k containing the sentence (1), all the sentences (2), and all the sentences
(3) for s of level less than k has a model. We can then, given a ﬁnite , apply this fact
to the least k such that all s occurring in  are of level <k, to conclude that  has a
model.
To obtain a model of k, let the domain be T , and let the denotation of each
constant r be the node r. It remains to assign a denotation to B. Take any tk at level
Tk, and take as the denotation of B the set consisting of tk, the node tk−1 at level Tk−1
immediately below tk, the node tk−2at level Tk−2 immediately below tk−1, and so on
down until we reach a node t0 at level 0.

26.2. K ¨ONIG’S LEMMA
325
The presence of t0 in the denotation of B will make (1) true. Since we have included
in the denotation of B only one node ti at each level Ti for i ≤k and none at higher
levels, (2) will be true. For a sentence of form (3) with s of level i < k, the ﬁrst disjunct
will be true unless s is ti, in which case the Bti+1 will be among the other disjuncts,
and will be true. In either case, then, (3) will be true for every sentence of this type in
k. [The sentence of form (3) with tk as s will be false, but that sentence is not in k.]
Before indicating the connection of Ramsey’s theorem with the kind of logical
phenomena we have been concerned with in this book, we digress a moment to
present a pretty application of Ramsey’s theorem.
26.4 Corollary (Schur’s theorem). Suppose that each natural number is ‘painted’ ex-
actly one of some ﬁnite number of ‘colors’. Then there are positive integers x, y, z all the
same color such that x + y = z.
Proof: Suppose the number of colors is s. Paint each size-2 set {i, j}, i < j, the
same color as the natural number j −i. By Ramsey’s theorem (r = 2, n = 3), there
are a positive integer m ≥3 and a size-3 subset {i, j, k} of {0, 1, . . . , m −1} with
i < j < k, such that {i, j}, { j, k} and {i, k} are all the same color. Let x = j −i,
y = k −j, and z = k −i. Then x, y, z are positive integers all the same color, and
x + y = z.
Ramsey’s theorem is, in fact, just the starting point for a large body of results in
combinatorial mathematics. It is possible to add some bells and whistles to the basic
statement of the theorem. Call a nonempty set Y of natural numbers glorious if Y
has more than p elements, where p is the least element of Y. Since every inﬁnite set
is automatically glorious, it would add nothing to the inﬁnitary version of Ramsey’s
theorem to change ‘inﬁnite homogeneous set’ to ‘glorious inﬁnite homogeneous set’.
It does, however, add something to the statement of the original Ramsey’s theorem
to change ‘size-n homogeneous set’ to ‘glorious size-n homogeneous set’.
Let us call the result of this change the gloriﬁed Ramsey’s theorem. Essentially
the same proof we have given for Ramsey’s theorem proves the gloriﬁed Ramsey’s
theorem. (At the beginning, take T to be the set of partitions without glorious size-n
homogeneous sets, and towards the end, take Z to be the set of the ﬁrst q elements
of Y, where q is the maximum of n and p, p being the least element of Y.) There is,
however, an interesting difference in logical status between the two.
While the proof we have presented for Ramsey’s theorem involved a detour through
theinﬁnite,F.P.Ramsey’soriginalproofofRamsey’stheoremdidnot.Usingareason-
able coding of ﬁnite sets of natural numbers by natural numbers, Ramsey’s theorem
can be expressed in the language of arithmetic, and by ‘formalizing’ Ramsey’s proof,
it can be proved in P. By contrast, the gloriﬁed Ramsey’s theorem, though it can be
expressed in the language of arithmetic, cannot be proved in P.
It is an example of a sentence undecidable in P that is far more natural, mathe-
matically speaking, than any we have encountered so far. (The sentences involved in
G¨odel’s theorem or Chaitin’s theorem, for instance, are ‘metamathematical’, being
aboutprovabilityandcomputability,notordinarymathematicalnotionsontheorderof
those occurring in Ramsey’s theorem.) Unfortunately, the Paris–Harrington theorem,

326
RAMSEY’S THEOREM
which tells us that gloriﬁed Ramsey’s theorem is undecidable in P, requires a deeper
analysis of nonstandard models than that undertaken in the preceding chapter, and is
beyond the scope of a book such as this one.
Problems
26.1 Show that at a party attended by at least nine persons, any two of whom either
like each other or dislike each other, either there are four, any two of whom like
each other, or there are three, any two of whom dislike each other.
26.2 Show that at a party attended by at least eighteen persons, any two of whom
either like each other or dislike each other, either there are four, any two of
whom like each other, or there are four, any two of whom dislike each other.
26.3 A ﬁnite set of points in the plane, none lying on the line between any other two,
is said to be convex if no point lies in the interior of the triangle formed by any
three other points, as on the left in Figure 26-3. It is not hard to show that given
Figure 26-3. Convex and concave sets of points.
any set of ﬁve points in the plane, none lying on the line between any other
two, there is a convex subset of four points. The Erd¨os–Szekeres theorem states
that, more generally, for any number n > 4 there exists a number m such that
given a set of (at least) m points in the plane, none lying on the line between
any other two, there is a convex subset of (at least) n points. Show how this
theorem follows from Ramsey’s theorem.
26.4 Show that the general case of Ramsey’s theorem follows from the special case
with s = 2, by induction on s.
26.5 For r = s = 2 and n = 3, each node in the tree used in the proof of Theorem
26.1 in section 26.2 can be represented by a picture in the style of Figure 26-1.
How many such nodes will there be in the tree?
26.6 Prove K¨onig’s lemma directly, that is, without using the compactness theorem,
by considering the subtree T * of T consisting of all nodes that have inﬁnitely
many nodes above them (where above means either immediately above, or
immediately above something immediately above, or . . . ).

27
Modal Logic and Provability
Modal logic extends ‘classical’ logic by adding new logical operators □and ♦for
‘necessity’ and ‘possibility’. Section 27.1 is an exposition of the rudiments of (sentential)
modal logic. Section 27.2 indicates how a particular system of modal logic GL is related
to the kinds of questions about provability in P we considered in Chapters 17 and 18.
This connection motivates the closer examination of GL then undertaken in section 27.3.
27.1 Modal Logic
Introductory textbooks in logic devote considerable attention to a part of logic we
have not given separate consideration: sentential logic. In this part of logic, the
only nonlogical symbols are an enumerable inﬁnity of sentence letters, and the only
logical operators are negation, conjunction, and disjunction: ∼, &, ∨. Alternatively,
the operators may be taken to be the constant false (⊥) and the conditional (→). The
syntax of sentential logic is very simple: sentence letters are sentences, the constant ⊥
is a sentence, and if A and B are sentences, so is (A →B).
The semantics is also simple: an interpretation is simply an assignment ω of truth
values, true (represented by 1) or false (represented by 0), to the sentence letters. The
valuation is extended to formulas by letting ω(⊥) = 0, and letting ω(A →B) = 1 if
and only if, if ω(A) = 1, then ω(B) = 1. In other words, ω(A →B) = 1 if ω(A) = 0
or ω(B) = 1 or both, and ω(A →B) = 0 if ω(A) = 1 and ω(B) = 0. ∼A may be
considered an abbreviation for (A →⊥), which works out to be true if and only if
A is false. (A & B) may similarly be taken to be an abbreviation for ∼(A →∼B),
which works out to be true if and only if A and B are both true, and (A ∨B) may be
taken to be an abbreviation for (∼A →B).
Validity and implication are deﬁned in terms of interpretations: a sentence D is im-
plied by a set of sentences  if it is true in every interpretation in which all sentences
in  are true, and D is valid if it is true in all interpretations. It is decidable whether a
given sentence D is valid, since whether D comes out true on an interpretation ω de-
pends only on the values ω assigns to the ﬁnitely many sentence letters that occur in D.
If there are only k of these, this means that only a ﬁnite number of interpretations,
namely 2k of them, need to be checked to see if they make D true. Similar remarks
apply to implication.
327

328
MODAL LOGIC AND PROVABILITY
What is done in introductory textbooks that we have not done here is to work
out many particular examples of valid and invalid sentences, and implications and
nonimplications among sentences. We are simply going to presume a certain facility
with recognizing sentential validity and implication.
Modal sentential logic adds to the apparatus of ordinary or ‘classical’ sentential
logic one more logical operator, the box □, read ‘necessarily’ or ‘it must be the case
that’. One more clause is added to the deﬁnition of sentence: if A is a sentence, so
is □A. The diamond ♦, read ‘possibly’ or ‘it may be the case that’, is treated as an
abbreviation: ♦A abbreviates ∼□∼A.
A modal sentence is said to be a tautology if it can be obtained from a valid
sentence of nonmodal sentential logic by substituting modal sentences for sentence
letters. Thus, since p ∨∼p is valid for any sentence letter p, A ∨∼A is a tautology
for any modal sentence A. Analogously, tautological consequence for modal logic is
deﬁnableintermsofimplicationfornonmodalsententiallogic.Thussinceq isimplied
by p and p →q for any sentence letters p and q, B is a tautologous consequence of
A and A →B for any modal sentences A and B. The inference from A and A →B
to B is traditionally called modus ponens.
There is no single accepted view as to what modal sentences are to be considered
modally valid, beyond tautologies. Rather, there are a variety of systems of modal
logic, each with its own notion of a sentence being demonstrable.
The minimal system of modal sentential logic, K, may be described as follows.
The axioms of K include all tautologies, and all sentences of the form
□(A →B) →(□A →□B).
The rules of K allow one to pass from earlier sentences to any sentence that is a
tautologous consequence of them, and to pass
from A to □A.
The latter rule is called the rule of necessitation. A demonstration in K is a sequence
of sentences, each of which either is an axiom or follows from earlier ones by a
rule. A sentence is then demonstrable in K, or a theorem of K, if it is the last sen-
tence of some demonstration. Given a ﬁnite set  = {C1, . . . , Cn}, we write ∧C for
the conjunction of all its members, and say  is inconsistent if ∼∧C is a theorem.
We say a sentence D is deducible from  if ∧C →D is a theorem. The usual rela-
tionships hold.
Stronger systems can be obtained by adding additional classes of sentences as ax-
ioms, resulting in a larger class of theorems. The following are among the candidates:
□A →A
(A1)
A →□♦A
(A2)
□A →□□A
(A3)
□(□A →A) →□A.
(A4)
For any system S we write ⊢S A to mean that A is a theorem of S.

27.1. MODAL LOGIC
329
There is a notion of interpretation or model for K. We are going to be interested
only in ﬁnite models, so we build ﬁniteness into the deﬁnition. A model for K will
be a triple W = (W, >, ω), where W is a nonempty ﬁnite set, > a two-place relation
on it, and ω a valuation or assignment of truth values true or false (represented by
1 or 0) not to sentence letters but to pairs (w, p) consisting of an element w of W and
a sentence letter p. The notion W, w |= A of a sentence A being true in a model W
and an element w is deﬁned by induction on complexity. The clauses are as follows:
W, w |= p for p a sentence letter
iff
ω(w, p) = 1
not W, w |= ⊥
W, w |= (A →B)
iff
not W, w |= A or W, w |= B
W, w |= □A
iff
W, v |= A for all v < w.
(We have written v < w for w > v.) Note that the clauses for ⊥and →are just like
those for nonmodal sentential logic. We say a sentence A is valid in the model W if
W, w |= A for all w in W.
Stronger notions of model of can be obtained by imposing conditions that the
relation > must fulﬁll, resulting in a smaller class of models. The following are
among the candidates.
(W1)
Reﬂexivity:
for all w,
w > w
(W2)
Symmetry:
for all w and v,
if w > v, then v > w
(W3)
Transitivity:
for all w, v, and u,
if w > v > u, then w > u
(W4)
Irreﬂexivity:
for all w,
not w > w.
(We have written w > v > u for w > v and v > u.) For any class  of models, we say A
is valid in , and write |= A, if A is valid in all W in .
Let S be a system obtained by adding axioms and  a class obtained by imposing
conditions on >. If whenever ⊢S A we have |= A, we say S is sound for . If when-
ever |= A we have ⊢S A, we say S is complete for . A soundness and completeness
theorem relating the system S to a class of models  generally tells us that the (set
of theorems of) the system S is decidable: given a sentence A, to determine whether
or not A is a theorem, one can simultaneously run through all demonstrations and
through all ﬁnite models, until one ﬁnds either a demonstration of A or a model of ∼A.
A large class of such soundness and completeness theorems are known, of which we
state the most basic as our ﬁrst theorem.
27.1 Theorem (Kripke soundness and completeness theorems). Let S be obtained by
adding to K a subset of {(A1), (A2), (A3)}. Let  be obtained by imposing on <W the
corresponding subset of {(W1), (W2), (W3)}. Then S is sound and complete for .
Since there are eight possible subsets, we have eight theorems here. We are going
to leave most of them to the reader, and give proofs for just two: the case of the
empty set, and the case of the set {(A3)} corresponding to {(W3)}: K is sound and
complete for the class of all models, and K + (A3) is sound and complete for the class
of transitive models. Before launching into the proofs we need a couple of simple
facts.

330
MODAL LOGIC AND PROVABILITY
27.2 Lemma. For any extension S of K, if ⊢S A →B, then ⊢S □A →□B.
Proof: Suppose we have a proof of A →B. Then we can then extend it as follows:
(1)
A →B
G
(2)
□(A →B)
N(1)
(3)
□(A →B) →(□A →□B)
A
(4)
□A →□B
T(2), (3)
The annotations mean: G[iven], [by] N[ecessitation from step] (1), A[xiom], and
T[autological consequence of steps] (2), (3).
27.3 Lemma. ⊢K (□A & □B) ↔□(A & B), and similarly for more conjuncts.
Proof:
(1)
(A & B) →A
T
(2)
□(A & B) →□A
25.2(1)
(3)
□(A & B) →□B
S(2)
(4)
A →(B →(A & B))
T
(5)
□A →□(B →(A & B))
25.2(4)
(6)
□(B →(A & B)) →(□B →□(A & B))
A
(7)
(□A & □B) ↔□(A & B)
T(2), (3), (5), (6)
The ﬁrst three annotations mean: T[autology], [by Lemma] 25.2 [from] (1), and
S[imilar to] (2).
Proof of Theorem 27.1: There are four assertions to be proved.
K is sound for the class of all models. Let W be any model, and write w |= A
for W, w |= A. It will be enough to show that if A is an axiom, then for all w we
have w |= A, and that if A follows by a rule from B1, . . . , Bn, and for all w we have
w |= Bi for each i, then for all w we have w |= A.
Axioms. If A is tautologous, the clauses of the deﬁnition of |= for ⊥and →
guarantee that w |= A. As for axioms of the other kind, if w |= □(A →B) and w |=
□A, then for any v < w, v |= A →B and v |= A. Hence v |= B for any v < w, and
w |= □B. So w |= □(A →B) →(□A →□B).
Rules. If A is a tautologous consequence of the Bi and w |= Bi for each i, then
again the clauses of the deﬁnition of |= for ⊥and →guarantee that w |= A. For
the other rule, if w |= A for all w, then a fortiori for any w and any v < w, we have
v |= A. So w |= □A.
K is complete for the class of all models. Suppose A is not a theorem. We construct
amodelinwhich A isnotvalid.Wecallasentenceaformulaifitiseitherasubsentence
of A or the negation of one. We call a consistent set of formulas maximal if for every
formula B it contains one of every pair of formulas B, ∼B. First note that {∼A} is
consistent: otherwise ∼∼A is a theorem, and hence A is, as a tautologous conse-
quence. Further, note that every consistent set  is a subset of some maximal set:
∧ is equivalent to some nonempty disjunction each of whose conjuncts is a con-
junction of formulas that contains the members of  and contains every formula
exactly once, plain or negated. Further, note that a maximal set contains any formula

27.1. MODAL LOGIC
331
deducible from it: otherwise it would contain the negation of that formula; but a set
that contains the negation of a formula deducible from it is inconsistent.
Let W be the set of all maximal sets. W is not empty, since {∼A} is consistent and
therefore a subset of some maximal set. W is ﬁnite: if there are only k subsentences
of A, there are at most 2k maximal sets. Deﬁne a relation > on W by letting w > v
if and only if whenever a formula □A is in w, the formula A is in v. Finally, for w
in W and sentence letter p, let ω(w, p) = 1 if p is in w, and ω(w, p) = 0 if not. Let
W = (W, >, ω). We are going to show by induction on complexity that for any w in
W and any formula B we have W, w |= B if and only if B is in w. Since there is a
w containing ∼A rather than A, it follows that A is not valid in W.
For the base step, if B is a sentence letter p, then p is in w iff ω(w, p) = 1 iff
w |= p. If B is ⊥, then ⊥is not in w, since w is consistent, and also it is not the case
that w |= ⊥. For the induction step, if B is C →D, then C and D are subsentences
of A, and ∼B ↔(C & ∼D) is a theorem, being tautologous. Thus B is not in w iff
(by maximality) ∼B is in w, iff C and ∼D are in w, iff (by the induction hypothesis)
w |= C and not w |= D, iff not w |= C →D. If B is □C, the induction hypothesis is
that for any v, v |= C iff C is in v. We want to show that w |= □C iff □C is in w. For
the ‘if’ direction, suppose □C is in w. Then for any v < w, C is in v and so v |= C.
It follows that w |= □C.
For the ‘only if’ direction, suppose w |= □C. Let
V = {D1, . . . , Dm, ∼C}
where the □Di for 1 ≤i ≤m are all the formulas in w that begin with □. Is V
consistent? If it is, then it is contained in some maximal v. Since all Di are in v, we
have v < w. Since ∼C is in v, not v |= C, which is impossible, since w |= □C. So
V is inconsistent, and it follows that
(D1 & · · · & Dm) →C
is a theorem. By Lemma 27.2,
□(D1 & · · · & Dm) →□C
is a theorem, and so by Lemma 27.3,
(□D1 & · · · & □Dm) →□C
is a theorem. Hence, since each □Di is in w, □C is in w.
K + (A3) is sound for transitive models. If w |= □A, then for any v < w it is the
case that for any u < v we have by transitivity u < w, and so u |= A. Thus v |= □A
for any v < w , and w |= □□A. Thus w |= □A →□□A.
K + (A3) is complete for transitive models. The construction used to prove K
complete for the class of all models needs to be modiﬁed. Deﬁne w > v if and only
if whenever a formula □B is in w, the formulas □B are both B in v. Then > will be
transitive. For if w > v > u, then whenever □A is in w, □A and A will be in v, and
since the former is in v, both will also by in u, so w > u.

332
MODAL LOGIC AND PROVABILITY
Theonlyotherpartoftheproofthatneedsmodiﬁcationistheproofthatif w |= □C,
then □C is in w. So suppose w |= □C, and let
V = {□D1, D1, . . . , □Dm, Dm, ∼C}
where the □Di are all the formulas in w that begin with □. If V is consistent and
v is a maximal set containing it, then w > v and v |= ∼C, which is impossible. It
follows that
□D1 & D1 & · · · & □Dm & Dm →C
□(□D1 & D1 & · · · & □Dm & Dm) →□C
(□□D1 & □D1 & · · · & □□Dm & □Dm) →□C
are theorems, and hence any tautologous consequence of the last of these and the
axioms □Di →□□Di is a theorem, and this includes
(□D1 & · · · & □Dm) →□C
from which it follows that w |= □C.
Besides its use in proving decidability, the preceding theorem makes it possible
to prove syntactic results by semantic arguments. Let us give three illustrations. In
both the ﬁrst and the second, A and B are arbitrary sentences, q a sentence letter not
contained in either, F(q) any sentence, and F(A) and F(B) the results of substituting
A and B respectively for any and all occurrences of q in F. In the second and third,
□
■A abbreviates □A & A. In the third, •A is the result of replacing □by □
■throughout
A.
27.4 Proposition. If ⊢K A ↔B, then ⊢K F(A) ↔F(B).
27.5 Proposition. ⊢K+(A3) □
■(A ↔B) →□
■(F(A) ↔F(B)).
27.6 Proposition. If ⊢K+(A1)+(A3) A, then ⊢K + (A3) •A.
Proof: For Proposition 27.4, it is easily seen (by induction on complexity of F)
that if W = (W, >, ω) and we let W′ = (W, >, ω′), where ω′ is like ω except that for
all w
ω′(w, q) = 1
if and only
if W, w |= A
then for all w, we have
W, w |= F(A)
if and only
if W′, w |= F(q).
But if ⊢K A ↔B, then by soundness for all w we have
W, w |= A
if and only
if W, w |= B
and hence
W, w |= F(B)
if and only
if W′, w |= F(q)
W, w |= F(A)
if and only
if W, w |= F(B).
So by completeness we have ⊢K F(A) ↔F(B).

27.1. MODAL LOGIC
333
For Proposition 27.5, it is easily seen (by induction on complexity of A) that since
each clause in the deﬁnition of truth at w mentions only w and those v with w > v,
for any W = (W, >, ω) and any w in W, whether W, w |= A depends only on the
values of ω(v, p) for those v such that there is a sequence
w = w0 > w1 > · · · > wn = v.
If > is transitive, these are simply those v with w ≥v (that is, w = v or w > v).
Thus for any transitive model (W, >, ω) and any w, letting Ww = {v: w ≥v} and
Ww = (Ww, >, ω), we have
W, w |= A
if and only if
Ww, w |= A.
Now
W, w |= □
■C
if and only if
for all v ≤w
we have W, v |= C.
Thus if W, w |= □
■(A ↔B), then Ww, v |= A ↔B for all v in Ww. Then, arguing as
in the proof of Proposition 27.4, we have Ww, v |= F(A) ↔F(B) for all such v, and
so W, w |= □
■(F(A) ↔F(B)). This shows
W, w |= □
■(A ↔B) →□
■(F(A) ↔F(B))
for all transitive W and all w, from which the conclusion of the proposition follows
by soundness and completeness.
For Proposition 27.6, for any model W = (W, >, ω), let •W = (W, ≥, ω). It is
easily seen (by induction on complexity) that for any A and any w in W
W, w |= A
if and only if
•W, w |= •A.
•W is always reﬂexive, is the same as W if W was already reﬂexive, and is transitive
if and only if W was transitive. It follows that A is valid in all transitive models if and
only if •A is valid in all reﬂexive transitive models. The conclusion of the proposition
follows by soundness and completeness.
The conclusion of Proposition 27.4 actually applies to any system containing K
in place of K, and the conclusions of Propositions 27.5 and 27.6 to any system
containing K + (A3) in place of K + (A3). We are going to be especially interested
in the system GL = K + (A3) + (A4). The soundness and completeness theorems
for GL are a little tricky to prove, and require one more preliminary lemma.
27.7 Lemma. If ⊢GL (□A & A & □B & B & □C) →C,then⊢GL (□A & □B) →□C,
and similarly for any number of conjuncts.
Proof: The hypothesis of the lemma yields
⊢GL (□A & A & □B & B) →(□C →C).
Then, as in the proof of the completeness of K + (A3) for transitive models, we get
⊢GL (□A & □B) →□(□C →C).

334
MODAL LOGIC AND PROVABILITY
From this and the axiom □(□C →C) →□C we get as a tautologous consequence
the conclusion of the lemma.
27.8 Theorem (Segerberg soundness and completeness theorems). GL is sound and
complete for transitive, irreﬂexive models.
Proof: Soundness. We need only show, in addition to what has been shown in
the proof of the soundness of K + (A3) for transitive models, that if a model is also
irreﬂexive, then w |= □(□B →B) →□B for any w. To show this we need a notion
of rank.
First note that if > is a transitive, irreﬂexive relation on a nonempty set W, then
whenever w0 > w1 > · · · > wm, by transitivity we have wi > w j whenever i < j, and
hence by irreﬂexivity wi ̸= w j whenever i ̸= j. Thus if W has only m elements, we can
never have w0 > w1 > · · · > wm. Thus in any transitive, irreﬂexive model, there is for
any w a greatest natural number k for which there exists elements w = w0 > · · · > wk.
We call this k the rank rk(w) of w. If there is no v < w, then rk(w) = 0. If v < w,
then rk(v) < rk(w). And if j < rk(w), then there is an element v < w with rk(v) = j.
(If w = w0 > · · · > wrk(w), then wrk(w)−j is such a v.)
Now suppose w |= □(□B →B) but not w |= □B. Then there is some v < w such
that not v |= B. Take such a v of lowest possible rank. Then for all u < v, by transitivity
u < w, and since rk(u) < rk(v), u |= B. This shows v |= □B, and since not v |= B,
not v |= □B →B. But that is impossible, since v < w and w |= □(□B →B). Thus
if w |= □(□B →B) then w |= □B, so for all w, w |= □(□B →B) →□B.
Completeness. We modify the proof of the completeness of K + (A3) by letting W
be not the set of all maximal w, but only of those for which not w > w. This makes
the model irreﬂexive.
Theonlyotherpartoftheproofthatneedsmodiﬁcationistheproofthatifw |= □C,
then □C is in w. So suppose w |= □C, and let
V = {□D1, D1, . . . , □Dm, Dm, □C, ∼C}
where the □Di are all the formulas in w that begin with □. If V is consistent and v is
a maximal set containing it, then since □C is in v but C cannot be in v, we have not
v > v, and v is in W. Also w > v and v |= ∼C, which is impossible. It follows that
□D1 & D1 & · · · & □Dm & Dm & □C →C
is a theorem, and hence by the preceding lemma so is
(□D1 & · · · & □Dm) →□C
from which it follows that w |= □C.
27.2 The Logic of Provability
Let us begin by explaining why the system GL is of special interest in connection
with the matters with which we have been concerned through most of this book. Let L

27.2. THE LOGIC OF PROVABILITY
335
be the language of arithmetic, and φ a function assigning to sentence letters sentences
of L. We associate to any modal sentence A a sentence Aφ of L as follows:
pφ = φ(p)
for p a sentence letter
⊥φ = 0 = 1
(B →C)φ = Bφ →Cφ
(□B)φ = Prv( Bφ )
where Prv is a provability predicate for P, in the sense of chapter 18. Then we have
the following relationship between GL and P:
27.9 Theorem (Arithmetical soundness theorem). If ⊢GL A, then for all φ, ⊢P Aφ.
Proof: Fix any φ. It is sufﬁcient to show that ⊢P Aφ for each axiom of GL, and that
if B follows by rules of GL from A1, . . . , Am and ⊢P Aφ
i for 1 ≤i ≤m, then ⊢P Bφ.
This is immediate for a tautologous axioms, and for the rule permitting passage to tau-
tologous consequences, so we need only consider the three kinds of modal axioms, and
the one modal rule, necessitation. For necessitation, what we want to show is that if ⊢P
Bφ, then ⊢P (□B)φ, which is to say ⊢P Prv( Bφ ). But this is precisely property (P1)
in the deﬁnition of a provability predicate in Chapter 18 (Lemma 18.2). The axioms
□(B →C) →(□B →□C) and □B →□□B correspond in the same way to the
remaining properties (P2) and (P3) in that deﬁnition.
It remains to show that ⊢P Aφ where A is an axiom of the form
□(□B →B) →□B.
By L¨ob’s theorem it sufﬁces to show ⊢P Prv( Aφ ) →Aφ. To this end, write S for
Bφ, so that Aφ is
Prv( Prv( S ) →S ) →Prv( S ).
By (P2)
Prv( Aφ ) →[Prv( Prv( Prv( S ) →S ) ) →Prv( Prv( S ) )]
Prv( Prv( S ) →S ) →[Prv( Prv( S ) ) →Prv( S )]
are theorems of P, and by (P3)
Prv( Prv( S ) →S ) →Prv( Prv( Prv( S ) →S ) )
is also a theorem of P. And therefore
Prv( Aφ ) →[Prv( Prv( S ) →S ) →Prv( S )]
which is to say Prv( Aφ ) →Aφ, being a tautological consequences of these three
sentences, is a theorem of P as required.
The converse of Theorem 27.9 is the Solovay completeness theorem: if for all
φ, ⊢P Aφ, then ⊢GL A. The proof of this result, which will not be needed in what
follows, is beyond the scope of a book such as this.

336
MODAL LOGIC AND PROVABILITY
Theorem 27.9 enables us to establish results about provability in P by establishing
results about GL. The remainder of this section will be devoted to the statement
of two results about GL, the De Iongh–Sambin ﬁxed point theorem and a normal
form theorem for letterless sentences, with an indication of their consequences for P.
The proofs of these two results are deferred to the next section. Before stating the
theorems, a few preliminary deﬁnitions will be required.
We call a sentence A modalized in the sentence letter p if every occurrence of p
in A is part of a subsentence beginning with □. Thus if A is modalized in p, then
A is a truth-functional compound of sentences □Bi and sentence letters other than
p. (Sentences not containing p at all count vacuously as modalized in p, while
⊥and truth-functional compounds thereof count conventionally as truth-functional
compounds of any sentences.) A sentence is a p-sentence if it contains no sentence
letter but p, and letterless if it contains no sentence letters at all.
So for example □p →□∼p is a p-sentence modalized in p, as is (vacuously and
conventionally) the letterless sentence ∼⊥, whereas q →□p is not a p-sentence but
is modalized in p, and ∼p is a p-sentence not modalized in p, and ﬁnally q →p is
neither a p-sentence nor modalized in p.
A sentence H is a ﬁxed point of A (with respect to p) if H contains only sentence
letters contained in A, H does not contain p, and
⊢GL □
■(p ↔A) →(p ↔H).
For any A, □0A = A and □n + 1A = □□n A. A letterless sentence H is in normal
form if it is a truth-functional compound of sentences □n⊥. Sentences B and C are
equivalent in GL if ⊢GL (B ↔C).
27.10 Theorem (Fixed point theorem). If A is modalized in p, then there exists a ﬁxed
point H for A relative to p.
Several proofs along quite different lines are known. The one we are going to give
(Sambin’s and Reidhaar-Olson’s) has the advantage that it explicitly and effectively
associates to any A modalized in p a sentence A§, which is then proved to be a ﬁxed
point for A.
27.11 Theorem (Normal form theorem). If B is letterless, then there exists a letterless
sentence C in normal form equivalent to B in GL.
Again the proof we give will effectively associate to any letterless B a sentence
B# that in normal form equivalent to B in GL.
27.12 Corollary. If A is a p-sentence modalized in p, then there exists a letterless
sentence H in normal form that is a ﬁxed point for A relative to p.
The corollary follows at once from the preceding two theorems, taking as H the
sentence A§#. Some examples of the H thus associated with certain A are given in
Table 27-1.
What does all this tell us about P? Suppose we take some formula α(x) of L
‘built up from’ Prv using truth functions and applying the diagonal lemma to obtain

27.3. THE FIXED POINT AND NORMAL FORM THEOREMS
337
Table 27-1. Fixed points in normal form
A
□p
∼□p
□∼p
∼□∼p
∼□□p
□p →□∼p
H
∼⊥
∼□⊥
□⊥
⊥
∼□□⊥
□□⊥→□⊥
a sentence γ such that ⊢P πα ↔α( πα ). Let us call such a sentence π a sentence of
G¨odel type. Then α(x) corresponds to a p-sentence A(p), to which we may apply
Corollary 27.12 in order to obtain a ﬁxed point H in normal form. This H will in
turn correspond to a truth-functional compound η of the sentences
0 = 1,
Prv( 0 = 1 ),
Prv( Prv( 0 = 1 ) ), . . .
and we get ⊢P πα ↔η. Since moreover the association of A with H is effective, so
is the association of α with η. Since the sentences in the displayed sequence are all
false (in the standard interpretation), we can effectively determine the truth value of
η and so of πα. In other words, there is a decision procedure for sentences of G¨odel
type.
27.13 Example (‘Cashing out’ theorems about GL as theorems about P). When α(x) is
Prv(x), then πα is the Henkin sentence, A(p) is □p, and H is (according to Table 27-1)
∼⊥, so η is 0 ̸= 1, and since ⊢P πα ↔0 ̸= 1, we get the result that the Henkin sentence
is true—and moreover that it is a theorem of P, which was L¨ob’s answer to Henkin’s
question. When α(x) is ∼Prv(x), then πα is the G¨odel sentence, A(p) is ∼□p, and H is
(according to Table 27-1) ∼□⊥, so η is the consistency sentence ∼Prv( 0 = 1 ), and since
⊢P πα ↔∼Prv( 0 = 1 ),wegettheresultthattheG¨odelsentenceistrue,whichissomething
that we knew—and moreover that the G¨odel sentence is provably equivalent in P to the
consistency sentence, which is a connection between the ﬁrst and second incompleteness
theorems that we did not know of before.
Each column in Table 27-1 corresponds to another such example.
27.3 The Fixed Point and Normal Form Theorems
We begin with the normal form theorem.
Proof of Theorem 27.11: The proof is by induction on the complexity of B.
(Throughout we make free tacit use of Proposition 27.4, permitting substitution of
demonstrably equivalent sentences for each other.) It clearly sufﬁces to show how
to associate a letterless sentence in normal form equivalent to □C with a letterless
sentence C in normal form.
First of all, put C in conjunctive normal form, that is, rewrite C as a conjunction
D1 & · · · & Dk of disjunctions of sentences □i⊥and ∼□i⊥. Since □distributes over
conjunction by Lemma 27.3, it sufﬁces to ﬁnd a suitable equivalent for □D for any

338
MODAL LOGIC AND PROVABILITY
disjunction D of □i⊥and ∼□i⊥. So let D be
□n1⊥∨· · · ∨□n p⊥∨∼□m1⊥∨· · · ∨∼□mq⊥.
Wemayassume D hasatleastoneplaindisjunct:ifnot,justaddthedisjunct □0⊥= ⊥,
and the result will be equivalent to the original.
Using the axiom □B →□□B and Lemma 27.2, we see ⊢GL □i B →□i+1B for
all i, and hence
(∗)
⊢GL □i B →□j B and ⊢GL ∼□j B →∼□i B
whenever
i ≤j.
Sowemayreplace D by□n⊥∨∼□m⊥,wheren = max(n1, . . . , n p)andm = min(m1,
. . . , mq). If there were no negated disjuncts, this is just □n⊥, and we are done.
Otherwise, D is equivalent to □m⊥→□n⊥. If m ≤n, then this is a theorem, so we
may replace D by ∼⊥.
If m > n, then n + 1 ≤m. We claim in this case ⊢GL □D ↔□n+1⊥. In one direc-
tion we have
(1)
□n⊥→□n+1⊥
(∗)
(2)
(□m⊥→□n⊥) →(□m⊥→□n+1⊥)
T(1)
(3)
□(□m⊥→□n⊥) →□(□m⊥→□n+1⊥)
27.2(2)
(4)
□(□n + 1⊥→□n⊥) →□n+1⊥
A
(5)
□(□m⊥→□n⊥) →□n+1⊥
T(3), (4)
(6)
□n⊥→(□m⊥→□n⊥)
T
(7)
□n+1⊥→□(□m⊥→□n⊥)
27.2(6)
(8)
□(□m⊥→□n⊥) ↔□n+1⊥.
T(5), (7)
And (8) tells us ⊢GL □D ↔□n+1⊥.
Turning to the proof of Theorem 27.10, we begin by describing the transform
A§.Write ⊤for ∼⊥. Let us say that a sentence A is of grade n if for some distinct
sentence letters q1, . . . , qn (where possibly n = 0), and some sentence B(q1, . . . , qn)
not containing p but containing all the qi, and some sequence of distinct sentences
C1(p), . . . , Cn(p) all containing p, A is the result B(□C1(p), . . . , □Cn(p)) of substi-
tuting for each qi in B the sentence □Ci. If A is modalized in p, then A is of grade n
for some n.
If A is of grade 0, then A does not contain p, and is a ﬁxed point of itself. In this
case, let A§ = A. If
A = B(□C1(p), . . . , □Cn+1(p))
is of grade n + 1, for 1 ≤i ≤n + 1 let
Ai = B(□C1(p), . . . , □Ci−1(p), ⊤, □Ci+1(p), . . . , □Cn+1(p)).
Then Ai is of grade n, and supposing § to be deﬁned for sentences of grade n, let
A§ = B(□C1(A§
1), . . . , □Cn(A§
n+1)).
27.14 Examples (Calculating ﬁxed points). We illustrate the procedure by working out A§
in two cases (incidentally showing how substitution of demonstrably equivalent sentences
for each other can result in simpliﬁcations of the form of A§).

27.3. THE FIXED POINT AND NORMAL FORM THEOREMS
339
Let A = □∼p. Then A = B(□C1(p)), where B(q1) = q1 and C1(p) = ∼p. Now A1 = B
(⊤) = ⊤is of grade 0, so A§
1 = A1 = ⊤, and A§ = B(□C1(A§
1)) = □∼⊤, which is equivalent
to □⊥, the H associated with this A in Table 27-1.
Let A = □(p →q) →□∼p. Then A = B(□C1(p), □C2(p)), where B(q1, q2) =
(q1 →q2), C1(p) = (p →q), C2(p) = ∼p. Now A1 = (⊤→□∼p), which is equivalent to
□∼p, and A2 = □(p →q) →⊤, which is equivalent to ⊤. By the preceding example,
A§
1 = □∼⊤, and A§
2 is equivalent to ⊤. So A§ is equivalent to B(□C1(□⊥), □∼C2(⊤)) =
□(□∼⊤→q) →□∼⊤, or □(□⊥→q) →□∼⊥.
To prove the ﬁxed-point theorem, we show by induction on n that A§ is a ﬁxed
point of A for all formulas A modalized in p of grade n. The base step n = 0, where
A§ = A, is trivial. For the induction step, let A, B, Ci be as in the deﬁnition of §,
let i range over numbers between 1 and n + 1, write H for A§ and Hi for A§
i, and
assume as induction hypothesis that Hi is a ﬁxed point for Ai. Let W = (W, >, ω)
be a model, and write w |= D for W, w |= D. In the statements of the lemmas, w
may be any element of W.
27.15 Lemma. Suppose w |= □
■
(p ↔A) and w |= □Ci(p). Then w |= Ci(p) ↔
Ci(Hi) and w |= □Ci(p) ↔□Ci(Hi).
Proof: Sincew |= □Ci(p),byaxiom(A3)w |= □□Ci(p);henceforallv ≤w, v |=
□Ci(p). It follows that w |= □
■(Ci(p) ↔⊤). By Proposition 27.5, w |= □
■(A ↔Ai),
whence by Lemma 27.5 again w |= □
■(p ↔Ai), since w |= □
■(p ↔A). Since Hi
is a ﬁxed point for Ai, w |= □
■(p ↔Hi). The conclusion of the lemma follows on
applying Proposition 27.5 twice (once to Ci, once to □Ci).
27.16 Lemma. w |= □
■(p ↔A) →□
■(□Ci(p) →□Ci(Hi)).
Proof: Suppose w |= □
■(p ↔A). By Proposition 27.6, □
■D →□
■□
■D is a theorem,
so w |= □
■□
■(p ↔A), and if w ≥v, then v |= □
■(p ↔A). Hence if v |= □Ci(p), then
v |= □Ci(p) ↔□Ci(Hi) by the preceding lemma, and so v |= □Ci(Hi). Thus if
w ≥v, then v |= □Ci(p) ↔□Ci(Hi), and so w |= □
■(□Ci(p) →□Ci(Hi)).
27.17 Lemma. w |= □
■(p ↔A) →□
■(□Ci(Hi) →□Ci(p)).
Proof: Suppose w |= □
■(p ↔A), w ≥v, and v |= ∼□Ci(p). Then there exist u
with v ≥u and therefore w ≥u with u |= ∼Ci(p). Take u ≤v of least rank among
those such that u |= ∼Ci(p). Then for all t with u > t, we have t |= Ci(p). Thus
u |= □Ci(p). As in the proof of Lemma 27.16, u |= □
■(p ↔A), and so by that lemma,
u |= Ci(p) ↔Ci(Hi) and u |= ∼Ci(Hi). Thus v |= ∼□Ci(Hi) and v |= □Ci(Hi) →
□Ci(p) and w |= □
■(□Ci(Hi) →□Ci(p)).
The last two lemmas together tell us that
□
■(p ↔A) →□
■(□Ci(Hi) ↔□Ci(p))
is a theorem of GL. By repeated application of Proposition 27.5, we successively see
that □
■(p ↔A) →□
■(A ↔D) and therefore □
■(p ↔A) →□
■(p ↔D) is a theorem of

340
MODAL LOGIC AND PROVABILITY
GL for all the following sentences D, of which the ﬁrst is A and the last H:
B(□C1(p), □C2(p), . . . , □Cn+1(p))
B(□C1(H1), □C2(p), . . . , □Cn+1(p))
B(□C1(H1), □C2(H2), . . . , □Cn+1(p))
...
B(□C1(H1), □C2(H2), . . . , □Cn+1(Hn+1)).
Thus □
■(p ↔A) →(p ↔H) is a theorem of GL, to complete the proof of the ﬁxed
point theorem.
The normal form and ﬁxed point theorems are only two of the many results about
GL and related systems that have been obtained in the branch of logical studies known
as provability logic.
Problems
27.1 Prove the cases of Theorem 27.1 that were ‘left to the reader’.
27.2 Let S5 = K + (A1) + (A2) + (A3). Introduce an alternative notion of model for
S5 in which a model is just a pair W = (W, ω) and W, w |= □A iff W, v |= A
for all v in W. Show that S5 is sound and complete for this notion of model.
27.3 Show that in S5 every formula is provably equivalent to one such that in a
subformula of form □A, there are no occurrences of □in A.
27.4 Show that there is an inﬁnite transitive, irreﬂexive model in which the sentence
□(□p →p) →□p is not valid.
27.5 Verify the entries in Table 27-1.
27.6 Suppose for A in Table 27-1 we took □(∼p →□⊥) →□(p →□⊥). What
would be the corresponding H?
27.7 To prove that the G¨odel sentence is not provable in P, we have to assume
the consistency of P. To prove that the negation of the G¨odel sentence is not
provable in P, we assumed in Chapter 17 the ω-consistency of P. This is a
stronger assumption than is really needed for the proof. According to Table 27-1,
what assumption is just strong enough?

Annotated Bibliography
General Reference Works
BARWISE, JON (1977) (ed.), Handbook of Mathematical Logic (Amsterdam: North Holland). A col-
lection of survey articles with references to further specialist literature, the last article being an
exposition of the Paris–Harrington theorem.
GABBAY, DOV, and GUENTHNER, FRANZ (1983) (eds.), Handbook of Philosophical Logic (4 vols.)
(Dordrecht: Reidel). A collection of survey articles covering classical logic, modal logic and allied
subjects, and the relation of logical theory to natural language. Successive volumes of an open-
ended, much-expanded second edition have been appearing since 2001.
VAN HEIJENOORT, JEAN (1967) (ed.), From Frege to G¨odel: A Source Book in Mathematical Logic,
1879–1931 (Cambridge, Massachusetts: Harvard University Press). A collection of classic pa-
pers showing the development of the subject from the origins of truly modern logic through the
incompleteness theorems.
Textbooks and Monographs
ENDERTON, HERBERT (2001), A Mathematical Introduction to Logic, 2nd ed. (New York: Harcourt/
Academic Press). An undergraduate textbook directed especially to students of mathematics and
allied ﬁelds.
KLEENE, STEVEN COLE (1950), Introduction to Metamathematics (Princeton: D. van Nostrand). The
text from which many of the older generation ﬁrst learned the subject, containing many results still
not readily found elsewhere.
SHOENFIELD, JOSEPH R. (1967), Mathematical Logic (Reading, Massachusetts: Addison-Wesley).
The standard graduate-level text in the ﬁeld.
TARSKI, ALFRED, MOSTOWSKI, ANDRZEJ, and ROBINSON, RAPHAEL (1953), Undecidable Theories
(Amsterdam: North Holland). A treatment putting G¨odel’s ﬁrst incompleteness theorem in its most
general formulation.
By the Authors
BOOLOS,GEORGES.(1993),TheLogicofProvability(Cambridge,U.K.:CambridgeUniversityPress).
A detailed account of work on the modal approach to provability and unprovability introduced in
the last chapter of this book.
JEFFREY, RICHARD C. (1991), Formal Logic: Its Scope and Limits, 4th ed. (Indianapolis: Hackett).
An introductory textbook, supplying more than enough background for this book.
341


Index
abacus (machine), 45ff, simulation of Turing machine
by, 51ff
abacus-computable function, 46ff
abbreviation, 108ff
accent (′), 64
Ackermann, Wilhelm, see Ackermann function
Ackermann function, 84f
A-correct, 290
Addison, John, see Addison’s theorem
Addison’s theorem, 286, 289ff
addition, abacus computability of, 48f, laws of, 218,
Turing computability of, 29f
address of a register, 46
ampersand (&), 102
analysis, 312ff, axiomatic, 314, predicative, 315
analytical sets, relations, functions, 284
antidiagonal sequence, 18, set, 18
Arabic numerals, see decimal representation
argument(s) of a function, 4
Aristotle, 272
arithmetic (true arithmetic), 150, 207, non-standard
models of, 150f, 302ff, undecidability of, 222,
without addition, 295, without multiplication, 295;
see also P, Q, R
arithmetical classes, 287
arithmetical completeness theorem, see Solovay
completeness theorem
arithmetical deﬁnability, see arithmetical sets,
arithmetical classes
arithmetical equivalence of formulas, 205
arithmetical L¨owenheim–Skolem theorem, 305, 317f
arithmetical sets and relations, 199, 286ff
arithmetical soundness theorem, 335
arithmetization of syntax, 187ff
arrow (→), 102, 107f, 327
associative laws of addition and multiplication, 218,
of conjunction and disjunction, 245
atomic formula, 107
atomic term, 107
avoidable appeal to Church’s thesis, 83
axiom of choice (AC), 163, 248, 341
axiom of enumerability, 281, of induction, 214, 283,
314, of inﬁnity, 282
axioms of GL, 333, of K, 328, of P, 214–215, of Q,
207f, of R, 216, of S5, 340
axiom scheme, 214
axiomatizable theory, 191, ﬁnitely, 191
back-and-forth argument, 345
bars, 262
Barwise, Jon, 341
base-b representation of numbers, 11; see also binary,
decimal, duodecimal
base step in proof by induction, 109, 212–213
basic functions, 64
Behmann, Heinrich, see L¨owenheim–Behmann
theorem
Benacerraf, Paul, xii
Bernays, Paul, 233
Berry’s paradox, 227
Bertrand’s postulate, see Chebyshev’s theorem
beta function (β-function) lemma, 203
Beth, Ewart W., see Beth deﬁnability theorem
Beth deﬁnability theorem, 265ff
biconditional (↔), 102, 108
binary representation of numbers, 11, 21, 89
Boole, George, 272
box (□, □
■), 328, 332
box of an abacus, 46
bound variables, 111, relettering of, 124
bounded minimization and maximization, 77
bounded quantiﬁcation, 76
branch, 323
B¨uchi, J. R., see Turing–B¨uchi proof
busy beaver problem, see productivity
canonical domains, 142, 147
Cantor, Georg, 239; see also back-and-forth argument,
Cantor’s theorem, diagonalization method, zig-zag
enumeration
Cantor’s theorem, 16ff
caret (∧), 328
343

344
INDEX
categorical theory, see denumerably categorical
Chaitin, Gregory, see Chaitin’s theorem
Chaitin’s theorem, 228f
characteristic function, 73
Chebyshev’s theorem, 204, 238
Ch’in Chiu-shiao (Qin Jiushao), see Chinese
remainder theorem
Chinese remainder theorem, 203
choice, axiom of, see axiom of choice
Church, Alonzo, 239; see also Church–Herbrand
theorem, Church’s theorem, Church’s thesis
Church–Herbrand theorem, 270f; see also dyadic
logic
Church’s theorem, 120, 132, 134, G¨odel-style proof
of, 132ff, Turing–B¨uchi proof of, 126ff
Church’s thesis, 71, 134, 189, 192, avoidable and
unavoidable appeals to, 83, extended, 71
class, 286
clique, 319
closed formula or sentence, 103, 112
closed term, 103
closure properties of recursive relations, 76, of
semi-recursive relations, 81f
closure properties of a set of sentences, 155
code number, 8ff, of an expression, 188, 193, of a
sequence, 12f, of a Turing machine, 36ff
coding operations of a Turing machine, 88ff; see also
code number
coextensive, 296
coﬁnite, 15
Cohen, Paul, 239
coherence, 301
combining Turing machines, 39
commutative laws of addition and multiplication, 218,
of conjunction and disjunction, 245
compactness theorem, 137, 147ff, and second-order
logic, 279, 283, for truth-functional valuations, 254
complementation principle, 82
complete induction, 213
complete set of sentences, 147, theory, 191
completeness, 148, in modal logic, 329
completeness theorem, see G¨odel completeness
theorem, Kripke completeness theorems, Segerberg
completeness theorem, Solovay completeness
theorem
complexity, 228f
composition of functions, 14, 58, 64
comprehension, axiom, 314
concatenation function, 84, 187
conclusion of a rule, 169
condition, 289
conditional (→), 102, 108, 327
conditional probability, 301
conﬁguration of a Turing machine, 27, standard initial
and halting, 31f
congruence axioms, 257
conjunction (&), 75, 102, 107, 327, general (∧), 328
conjunctive normal form, 244, full, 245
connective, 102, zero-place, see constant truth and
falsehood
consequence, logical, 101, 119
conservative extension, 264, 315
consistency, unprovability of, see second
incompleteness theorem
consistency sentence, 232
consistent set of sentences, 169, theory, 191
constant functions, 65
constant symbol, 103, elimination of, 255ff
constant truth and falsehood (⊤, ⊥), 245, 327
constraint, 301
constructive proof, 182, 237f
continuum hypothesis (CH), 239
convex set of points, 326
copying machine, 39
correct, 199; see also A-correct
correspondence, 14
corners (⌜,⌝), see G¨odel numeral
countable, 3; see also enumerable
Craig, William, see Craig interpolation theorem,
Craig reaxiomatizability lemma
Craig interpolation theorem, 260ff
Craig reaxiomatizability lemma, 198
cryptographic functions, 193
cut elimination, 181
decidable, effectively, 73, recursively, see recursive
sets; semi-recursively, see semi-recursive sets
decidable set of sentences, 191, theory, 191
decimal representation of numbers, 11, 24f
decision problem, 126
decoding, 8
deduction, deducibility, 148, 168f, in modal logic,
328
deﬁnability, explicit, 266, implicit, 266; see also
Addison’s theorem, analytical sets, arithmetical
sets, Beth’s deﬁnability theorem, predicative and
impredicative, Richard’s paradox, Tarski’s theorem
deﬁnition by cases, 74
De Jongh, Dick, see de Jongh–Sambin theorem
De Jongh–Sambin theorem, 336
demonstration, demonstrability, 148, 168f, in modal
logic, 328
denial, see negation
denotation of a symbol, 104, of a term, 115
dense linear order, 152
density, 294
denumerable or enumerably inﬁnite, 4
denumerably categorical, 147
derivation and derivability, 168
description of a time, 130
diagonal lemma, 220f
diagonal sequence, 18, set, 18
diagonalization, method of, 17ff, of a relation, 86, of
an expression, 220
diagram, 164
diamond (♦), 328

INDEX
345
difference function ( .−), modiﬁed, 61, 69
disjunction (∨), 76, 102, 107, 372
disjunctive normal form, 244, full, 245
distributive law of addition and multiplication, 218, of
conjunction and disjunction, 245
dithering machine, 39
divisibility (|), 86
domain of a function, 7
domain of an interpretation or model, 103f, canonical,
142, 147
double arrow (↔), 102, 107f
double turnstile (|=), 114
Dreben, Burton, xii
duodecimal representation of numbers, 11
dyadic predicates and dyadic logic, 271, 275ff
effectively computable function, 23ff, 63
effectively decidable set or relation, 73
effectively semi-decidable set or relation, 80
Elements of Geometry, see Euclid’s Elements
elementarily equivalent, 251
elementary operation of an abacus, 47
elementary subinterpretation or submodel, 251
elimination of quantiﬁers, 296
empty function, 7
empty language, 103
empty set (Ø), 4
emptying a box, 47
encoding, 8
Enderton, Herbert, 341
entering sentence, 169
entry function, 80
enumerability, axiom of, 281
enumerable, 3
enumerably inﬁnite or denumerable, 4
enumerator, 252
Epimenides or liar paradox, 106, 227
epsilon model (∈-model), 313
equals sign, see identity symbol
equinumerous sets, 14
equivalence, arithmetical, 205
equivalence, axiom of, 257
equivalence, logical, 122, 124f
equivalence class, 143
equivalence relation, 142ff
erasure act of a Turing machine, 26
Erd¨os–Paul, see Erd¨os–Szekeres theorem
Erd¨os–Szekeres theorem, 326
essential undecidability, 222
Euclid of Alexandria, see Euclid’s Elements
Euclid’s Elements, 203, 238
Euler φ-function, 86
existential quantiﬁcation (∃), 103, 107, bounded, 76
existential sentences and formulas, 164, 247
existential, rudimentary (∃-rudimentary) sentences
and formulas, 204, generalized, 204
exiting sentence, 169
expansion, 247
explicit deﬁnability, 266
exponential function (↑), 66, abacus computability
of, 50
exponential-arithmetical (↑-arithmetical)
deﬁnability, 200
extended Church’s thesis, 71
extension of an interpretation or model, 250
extension of a set of sentences or theory, 264,
conservative, 264, 315
extensionality axiom, 313f
extensionality lemma, 118, 123
factorial, 68
falsehood, constant (⊥), 245, 327
Fara, Michael, xiii
Felapton, 112
Field, Hartry, xii
ﬁnite character, 154, 163
ﬁnitely axiomatizable theory, 191
ﬁnitely satisﬁable sentence or set, 271f, 300; see also
Trakhtenbrot’s theorem
ﬁnitism, 238
ﬁrst graph principle, 82
ﬁrst incompleteness theorem, 223f
ﬁrst-order logic, 101ff
ﬁxed point theorem, see De Jongh–Sambin theorem
ﬂow chart, 26
ﬂow graph, see ﬂow chart
forcing (⊢), 289ff, and FORCING, 291ff
formalization, 215
formation sequence, 107, 113, 195
formula, 103, 107f, second-order, 279f
free variables, 111, 195f
Frege, Gottlob, 272, 285
function, 4, one-to-one, onto, 14, partial and total, 7
function symbols, 103, elimination of, 255ff
function variable, 279
GL (system of modal logic), 333ff
Gabbay, Dov, 341
generalized existential-rudimentary (∃-rudimentary)
formula or sentence, 204
generic set, 291ff
Gentzen, Gerhard, see sequent calculus, cut
elimination
Gentzen system, see sequent calculus
gloriﬁed Ramsey’s theorem, 325
Glymour, Clark, xii
G¨odel, Kurt, 232ff–9, see also completeness theorem,
ﬁrst and second incompleteness theorems
G¨odel completeness theorem, 148, 163ff, 174ff,
abstract, 190, failure for second-order logic, 279,
283
G¨odel incompleteness theorems, see ﬁrst
incompleteness theorem, second incompleteness
theorem
G¨odel number, see code number of an expression
G¨odel numeral, 220

346
INDEX
G¨odel sentence, 225
G¨odel–Berry formula, 228
G¨odel–Chaitin formula, 228
G¨odel–Grelling formula, 227
G¨odel–Rosser sentence, 226
G¨odel-style proof of Church’s theorem, 126, 132ff
Goldfarb, Warren, xiii
graph principle, ﬁrst, 82, second, 96
graph relation of a function, 75
greatest common divisor, 86
Grelling or heterological paradox, 227
Guenthner, Franz, 341
halting, of a Turing machine, 26, in standard
conﬁguration or position, 32, 91
halting function, 38f
halting problem, 40
Hare, Caspar, xiii
Harrington, Leo, see Paris–Harrington theorem
Henkin, Leon, 285; see also Henkin axioms, Henkin
sentence
Henkin axioms, 162, 164
Henkin sentence, 235
Herbrand, Jacques, see Herbrand–Church theorem,
Herbrand’s theorem
Herbrand’s theorem, 253ff
heterological or Grelling paradox, 227
Hilbert, David, 238; see also Hilbert’s thesis
Hilbert’s thesis, 185
Hindu–Arabic numerals, see decimal representation
homogeneous set, 320
horizontal section, 86
identifying nodes of a ﬂow chart, 43
identity function(s), 5, 57, 64
identity of indiscernibles, 280
identity relation, 104, Whitehead–Russell deﬁnition
of, 281
identity symbol, 103, elimination of, 255ff
implication, logical, 101
implicit deﬁnability, 266
impredicative and predicative, 315f
incompleteness of second-order logic, see G¨odel
completeness theorem, failure for second-order
logic
incompleteness theorems, see ﬁrst incompleteness
theorem, second incompleteness theorem
inconsistent sentence or set, 148, theory, 191, in
modal logic, 328
individual symbol, see constant
individual variable, 278; see also variable
induction, mathematical, proof by, 212– 213,
complete, 213
induction axioms, 214, second-order, 283
induction hypothesis, 109
induction on complexity, proof by, 109ff
induction scheme, 214
induction step in a proof, 109, 213
inﬁnitary Ramsey’s theorem, 321
inﬁnity, axiom of, 282
instance of a formula, 112
interpolant, 261
interpolation theorem, see Craig interpolation
theorem, Lyndon interpolation theorem
interpretation of a language, 102, 103f, in modal
logic, 327
inverse function, 14
inversion lemma, 179f, 186
irrefutable, see consistent
isomorphism, 139ff
isomorphism lemma, 140
isomorphism type, 142
J, see pairing function
junction, see conjunction, disjunction
K (minimal system of modal logic), 328ff
Kant’s theorem, 269
Kleene normal form theorem, 94
Kleene, Steven Cole, 341; see also Kleene normal
form theorem, Kleene’s theorem
Kleene’s theorem, 82
Kochen, Simon, xiii
K¨onig’s lemma, 322ff
Kreisel, Georg, see Tennenbaum–Kreisel theorem
Kripke, Saul, xi; see also Kripke completeness
theorem
Kripke completeness theorems, 329ff
L∗, see language of arithmetic
L∗∗, see language of analysis
Lagrange’s theorem, 204, 213
Lambek, Joachim, see abacus
Lambek machine, see abacus
language, 103, empty, 103, meta-, 121, natural, 122f,
non-enumerable, 162f, object, 121, of analysis, 312,
of arithmetic, 103
leapfrog routine, 31
least common multiple, 86
least-number principle, 214
left introduction rules, 170
left movement of a Turing machine, 26
left number, 89
Leibniz’s law, 280
length function, 80
letterless sentence, 336, normal form theorem for, 336ff
level of a tree, 322f
Lewis, David, xiii
liar or Epimenides paradox, 106
linear order, 151f
lines or steps, 168
L¨ob, M. H., see L¨ob’s theorem
L¨ob’s theorem, 236
logarithm functions, 79

INDEX
347
logical consequence, see consequence
logical equivalence, see equivalence, logical
logical symbols, 102
L¨owenheim, Leopold, see L¨owenheim–Behmann
theorem, L¨owenheim–Skolem theorem
L¨owenheim–Behmann theorem, 270; see also
monadic logic
L¨owenheim–Skolem theorem, 137, 147ff,
arithmetical, 305, 317f, and second-order logic,
279, 282, strong, 251, upward, 163
lower domain, 312
lower inequality, 297
lower part, 312
Lyndon, Roger, see Lyndon interpolation theorem
Lyndon interpolation theorem, 268
machine table, 26
Maltsev (Malcev), A. I., see compactness theorem
mathematical induction, proof by, see induction,
mathematical
matrix, 246
max function, 34
maximal principle, 163
McAloon, Kenneth, see Tennenbaum–McAloon
theorem
Mellema, Paul, xiii
metalanguage, 121
metalogical, 120
min function, 34
minimal arithmetic, see Q
minimal system of modal logic, see K
minimization, 60f, 70ff, bounded, 77
modal logic, 123, 327ff
modalized, 336
models, 137ff, existence of, 153ff, number of, 139ff,
size of, 137, in modal logic, 329ff; see also epsilon
model, non-standard model, standard model
modus ponens, 328
monadic predicates and monadic logic, 272ff
monadic represention of numbers, 24, modiﬁed, 63f
monotone function, 98
mop-up, 54ff
Mostowski, Andrzej, 341
multiplication, abacus computability of, 49, laws of,
218, Turing computability of, 29ff
N*, see standard model of arithmetic
N**, see standard model of analysis
name, see constant
natural language, 122f; see also Hilbert’s thesis
necessity (□), 328
necessitation, 328
negation (∼), 75, 102, 107, 327
negation normal form, 243f
n-generic, 291
nonconstructive proof, 182, 237f
nonlogical symbols, 103
nonstandard models, of analysis, 312ff, of arithmetic,
150f, 302ff
normal form, for sentences and sets, 243, conjunctive,
244, disjunctive, 244, full conjunctive and
disjunctive, 245, for letterless sentences, 336ff,
negation, 243f, prenex, 246, Skolem, 247f
normal form, for terms, 297
nullity problem, 132
numerals, see base-b representation, monadic or tally
representation
object language, 121
objectual quantiﬁer, 117
ofﬁcial and unofﬁcial notation, see abbreviation
omega-consistency and -inconsistency (ω-consistency
and -inconsistency), 217, 226
omega-model (ω-model), 313
one-to-one function, 14
onto function, 14
open formula, 103, 112
open term, 103
ordinal numbers, 210
ordinary language, see natural language
overspill, 147, 309
P (Peano arithmetic), 214–215
Padoa, Alessandro, see Padoa’s method
Padoa’s method, 267
pairing function (J), 8f, 71
paradox, Berry, 227, Epimenides or liar, 106, 227,
Grelling or heterological, 227, Richard, 21f,
Russell’s, 285, Skolem, 252f
parentheses, 102, 109; see also abbreviation
parenthesis lemma, 109
Paris, Jeffrey, see Paris–Harrington theorem
Paris–Harrington theorem, 325, 341
partition, 143; see also Ramsey’s theorem
parity, 29
partial function, 7, recursive function, 71
Peano, Giuseppi, see P
Peano arithmetic, see P
Pendelbury, Michael J., xiii
pi () notation, 69
places, 103
positively semi-decidable, see semi-decidable
positively semi-deﬁnable, see semi-deﬁnable
possibility (♦), 328
power, see exponentiation
predecessor function, 69
predicate or relation symbol, 103, dyadic, 271, 275ff,
identity, 103, 255ff, monadic, 272ff
predicative and impredicative, 315f
preﬁx, 246
premiss of a rule, 169
prenex normal form, 246
Presburger arithmetic, see arithmetic without
multiplication

348
INDEX
Presburger, Max, see arithmetic without
multiplication
preservation upwards and downwards, 164f
prime decomposition, 13
primitive recursion, 58f, 67
primitive recursive functions, 67, 132ff, real numbers,
86, sets or relations, 73
print operation of a Turing machine, 26
probability measure, 301
product, see multiplication, pi notation
productivity, of a Turing machine, 42
projection functions, 64
proof by contradiction, 170, 238
proof by induction, see induction
proof procedure, 166ff
proof theory, 179
provability logic, 387; see also GL
provability predicate, 234, 335
provable (⊢), 191
power, see exponentiation
Putnam, Hilary, xiii
Q (minimal arithmetic), 207ff
Qin Jiushao (Ch’in Chiu-shao), see Chinese
remainder theorem
quadruples of a Turing machine, 26
quantiﬁer, 102, bounded, 76, existential, 102, 107,
universal, 102, 107
Quine, Willard Van Orman, xiii
quotient function, 12, 61
R (Robinson arithmetic), 216ff
Rado, Tibor, see productivity
Ramsey, Frank Plumpton, see Ramsey’s theorem
Ramsey’s theorem, 319ff, gloriﬁed, 325, inﬁnitary,
321
random access, 46
range of a function, 7
recursion, see primitive recursion
recursion equations, 67
recursive function, 61, 71, set or relation, 73
recursively enumerable sets, 96ff; see also
semi-recursive sets
recursively inseparable sets, 98
reduct, 247
reﬂexive relation, 143
refutation, refutability, 167ff; see also inconsistent
registers of an abacus, see box
regular function or relation, 71
Reidhaar-Olson, Lisa, 336
relation, 73, 104
relation symbol or predicate, 103
relation variable, 279
relatively prime, 86
relativized quantiﬁers, 296
relettering bound variables, 124
remainder function, 12, 61
representable, 207ff
Richard’s paradox, 21f
right introduction rules, 170
right movement of a Turing machine, 26
right number, 89
Robinson, Abraham, see Robinson’s joint
consistency theorem
Robinson arithmetic, see R
Robinson, Raphael, 341; see also R
Robinson’s joint consistency theorem, 264f
Rosser, J. Barkley, see Rosser sentence
Rosser sentence, 225
rudimentary formula, 204; see also existential-
rudimentary, universal-rudimentary
rudimentary function, 206
rule of inference, 169
Russell, Bertrand, see Russell’s paradox,
Whitehead–Russell deﬁnition of identity
Russell’s paradox, 285
S5, 340
Sambin, Giovanni, 336; see also De Jongh–Sambin
theorem
Santa Claus, 235
satisfaction of a formula, 117ff
satisfaction properties, 153f
satisﬁable sentence or set, 120
Scanlon, T. M., xiii
scanned symbol, 25
scheme, see axiom scheme
Schur’s theorem, 325
scoring function, 40f
second graph principle, 96
second incompleteness theorem, 232ff
second-order logic, 279ff
section of a relation, see horizontal section,
vertical section
secures (⇒), 167f
Segerberg, Krister, see Segerberg completeness
theorem
Segerberg completeness theorem, 334
semantics of ﬁrst-order logic, 114ff, distinguished
from syntax, 106
semi-decidable set or relation, effectively, 80,
recursively, see semi-recursive set or relation
semi-deﬁnable set or relation, 218
semi-recursive set or relation, 80ff; see also
recursively enumerable
sentence or closed formula, of ﬁrst-order logic, 103,
112, of modal logic, 328, of second-order logic, 280
sentence letter, 103, 107, 114, 327
sentential logic, 301, 327
sequents and sequent calculus, 166ff
Shoenﬁeld, J. R., 341
sigma () notation, 69
signature of an equivalence relation, 143, 151
size-n set, 320
Skolem, Thoralf, see L¨owenheim–Skolem theorem,
Skolem normal form

INDEX
349
Skolem axioms, 248
Skolem expansion, 248
Skolem function symbol, 247
Skolem normal form, 247
Skolem paradox, 252f
Smith, Nicholas, xiii
Solovay, Robert, see Solovay completeness theorem
Solovay completeness theorem, 335
soundness, 148, 167, 174ff, in modal logic, 329,
arithmetical, 335
spectrum, 149
standard conﬁguration or position, initial, 31, ﬁnal or
halting, 32, 91
standard element of a non-standard model, 303
standard interpretation or model, of the language of
analysis, 312, of the language of arithmetic, 104
standing sentence, 169
state of a Turing machine, 25
steps or lines, 168
subformula, 111
subinterpretation or submodel, 249f, elementary,
251
subsentence, 112
substitution, of equivalents, 124
substitution of functions, see composition
substitution of functions in relations, 75
substitution of terms for variables, 188f, 195;
see also instance
substitution function, 84
substitutional quantiﬁcation, 116
subterm, 111
successor function, 57, 64
successor step in proof by induction, see induction
step
sum, see addition, sigma notation
Sun Zi (Sun Tze), see Chinese remainder theorem
super-duper-exponentiation, 67
super-exponentiation (⇑), 66f
symmetric relation, 143
syntax of ﬁrst-order logic, 106ff, distinguished from
semantics, 106
tally representation of numbers, see monadic
representation
Tarski, Alfred, 341; see also compactness theorem,
Tarski’s theorem, truth
Tarski’s deﬁnition of truth, see truth
Tarski’s theorem, 222
tautology and tautological consequence, 328
Tennenbaum, Stanley, see Tennenbaum’s theorem,
Tennenbaum–Kreisel and Tennenbaum–McAloon
theorems
Tennenbaum’s theorem, 302
Tennenbaum–Kreisel theorem, 306
Tennenbaum–McAloon theorem, 306
term, 103, 107, atomic, 107, closed, 103, 107,
denotation of, 115, open, 103, 107
term model, 155
theorem, of a ﬁrst-order theory, 191, 263, in modal
logic, 328
theory, 191, 263, axiomatizable, 191, complete, 191,
consistent, 191, decidable, 191, ﬁnitely
axiomatizable, 191
Thomson, James, xiii
total function, 7
touring machines, 42
Tovey, Peter, xiii
Trakhtenbrot, Boris, see Trakhtenbrot’s theorem
Trakhtenbrot’s theorem, 135, 198
transfer theorem, see L¨owenheim–Skolem theorem
transitive relation, 143
trees, 322ff
true analysis, see analysis
true arithmetic, see arithmetic
truth, deﬁnability of, 286ff; see also Tarski’s theorem
truth in an interpretation (|=), 114, for modal logic,
329
truth predicate, see Tarski’s theorem
truth tables, 255
truth value, 105
truth-functional compound, 244
truth-function satisﬁability, 253
truth-functional valuation, 253, 327
Turing, Alan M., 239; see also Turing computability,
Turing machine, Turing’s thesis
Turing computability, 33
Turing machine, 25ff, 126ff, code number for, 36ff,
coding operations of, 88ff, simulation of abacus by,
51ff, universal, 44, 95f
Turing–B¨uchi proof of Church’s theorem, 126ff
Turing’s thesis, 33, 132; see also Church’s thesis
turnstile (⊢), 191, 328, double (|=), 114
two-sorted language, 312
unavoidable appeal to Church’s thesis, 83
unbarred, 262
uncomputability, 35ff
undecidability, of arithmetic, 222, essential, 222, of
ﬁrst-order logic, see Church’s theorem
undecidable sentence, 224, theory, see decidable
theory
undeﬁned, 6
unique readability lemma, 111, 123
universal closure, 208
universal function, 217
universal quantiﬁcation (∀), 102, 107, bounded, 76
universal sentences and formulas, 165, 247
universal Turing machine, 44, 95f
universal-rudimentary (∀-rudimentary), 204
universe of discourse, see domain of an
interpretation
unofﬁcial and ofﬁcial notation, see abbreviation
unsatisﬁable, see satisﬁable
upper domain, 312
upper inequality, 297
upward L¨owenheim–Skolem theorem, 163

350
INDEX
valid sentence, 120, 327
valuation, truth-functional, 327, 253
value of a function, 4
van Heijenoort, Jean, 341
variable, 102, 106, bound and free, 111, individual
and second-order, 279
Vaught, Robert, see Vaught’s test
Vaught’s test, 147
vertical section, 86
Wang, Hao, see coding operations of a Turing
machine
Wang coding, see coding operations of Turing
machine
wedge (∨), 76, 102, 107, 372
Whitehead, Alfred North, see Whitehead–Russell
deﬁnition of identity
Whitehead–Russell deﬁnition of identity, 281
zero function, 57, 64
zero step in proof by induction, see base step
Zeus, 19f, 23, 235, 321
ZFC, 278
zig-zag enumeration, 7


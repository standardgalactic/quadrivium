Algorithms and Combinatorics 21
Editorial Board
R.L. Graham, La Jolla     B. Korte, Bonn
L. Lovász, Budapest     A. Wigderson, Princeton
G.M. Ziegler, Berlin

Bernhard Korte
Jens Vygen
123
Combinatorial
Optimization
Theory and Algorithms
Third Edition

Bernhard Korte
Jens Vygen
Research Institute for Discrete Mathematics
University of Bonn
Lennéstraße 2
53113 Bonn, Germany
e-mail: dm@or.uni-bonn.de
 vygen@or.uni-bonn.de
This work is subject to copyright. All rights are reserved, whether the whole or part of the material
is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microfilms or in any other way, and storage in data banks.
Duplication of this publication or parts thereof is permitted only under the provisions of the
German Copyright Law of September 9, 1965, in its current version, and permission for use must
always be obtained from Springer. Violations are liable for prosecution under the German
Copyright Law.
Springer is a part of Springer Science+Business Media
springeronline.com
© Springer-Verlag Berlin Heidelberg 2000, 2002, 2006
Printed in Germany
The use of general descriptive names, registered names, trademarks, etc. in this publication does
not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
Typeset in          by the authors. Edited and reformatted by Kurt Mattes, Heidelberg, using the
MathTime fonts and a Springer             macro package.
Production: LE-TEX Jelonek, Schmidt & Vöckler GbR, Leipzig
Cover design: design & production GmbH, Heidelberg
Printed on acid-free paper         46/3142YL - 5 4 3 2 1 0
L T X
E
a
L T X
E
a
ISSN  0937-5511
ISBN-10  3-540-25684-9  Springer-Verlag Berlin Heidelberg New York
ISBN-13  978-3-540-25684-7  Springer-Verlag Berlin Heidelberg New York
ISBN 3-540-43154-3  2nd ed. Springer-Verlag Berlin Heidelberg New York
Library of Congress Control Number: 2005931374
Mathematics Subject Classification (2000):
90C27, 68R10, 05C85, 68Q25

Preface to the Third Edition
After ﬁve years it was time for a thoroughly revised and substantially extended
edition. The most signiﬁcant feature is a completely new chapter on facility loca-
tion. No constant-factor approximation algorithms were known for this important
class of NP-hard problems until eight years ago. Today there are several interesting
and very different techniques that lead to good approximation guarantees, which
makes this area particularly appealing, also for teaching. In fact, the chapter has
arisen from a special course on facility location.
Many of the other chapters have also been extended signiﬁcantly. The new ma-
terial includes Fibonacci heaps, Fujishige’s new maximum ﬂow algorithm, ﬂows
over time, Schrijver’s algorithm for submodular function minimization, and the
Robins-Zelikovsky Steiner tree approximation algorithm. Several proofs have been
streamlined, and many new exercises and references have been added.
We thank those who gave us feedback on the second edition, in particular
Takao Asano, Yasuhito Asano, Ulrich Brenner, Stephan Held, Tomio Hirata, Dirk
M¨uller, Kazuo Murota, Dieter Rautenbach, Martin Skutella, Markus Struzyna and
J¨urgen Werber, for their valuable comments. Eminently, Takao Asano’s notes and
J¨urgen Werber’s proofreading of Chapter 22 helped to improve the presentation at
various places.
Again we would like to mention the Union of the German Academies of
Sciences and Humanities and the Northrhine-Westphalian Academy of Sciences.
Their continuous support via the long-term project “Discrete Mathematics and Its
Applications” funded by the German Ministry of Education and Research and the
State of Northrhine-Westphalia is gratefully acknowledged.
Bonn, May 2005
Bernhard Korte and Jens Vygen

Preface to the Second Edition
It was more than a surprise to us that the ﬁrst edition of this book already went
out of print about a year after its ﬁrst appearance. We were ﬂattered by the many
positive and even enthusiastic comments and letters from colleagues and the gen-
eral readership. Several of our colleagues helped us in ﬁnding typographical and
other errors. In particular, we thank Ulrich Brenner, Andr´as Frank, Bernd G¨artner
and Rolf M¨ohring. Of course, all errors detected so far have been corrected in this
second edition, and references have been updated.
Moreover, the ﬁrst preface had a ﬂaw. We listed all individuals who helped
us in preparing this book. But we forgot to mention the institutional support, for
which we make amends here.
It is evident that a book project which took seven years beneﬁted from many
different grants. We would like to mention explicitly the bilateral Hungarian-
German Research Project, sponsored by the Hungarian Academy of Sciences and
the Deutsche Forschungsgemeinschaft, two Sonderforschungsbereiche (special re-
search units) of the Deutsche Forschungsgemeinschaft, the Minist`ere Franc¸ais de
la R´echerche et de la Technologie and the Alexander von Humboldt Foundation
for support via the Prix Alexandre de Humboldt, and the Commission of the Eu-
ropean Communities for participation in two projects DONET. Our most sincere
thanks go to the Union of the German Academies of Sciences and Humanities
and to the Northrhine-Westphalian Academy of Sciences. Their long-term project
“Discrete Mathematics and Its Applications” supported by the German Ministry
of Education and Research (BMBF) and the State of Northrhine-Westphalia was
of decisive importance for this book.
Bonn, October 2001
Bernhard Korte and Jens Vygen

Preface to the First Edition
Combinatorial optimization is one of the youngest and most active areas of discrete
mathematics, and is probably its driving force today. It became a subject in its
own right about 50 years ago.
This book describes the most important ideas, theoretical results, and algo-
rithms in combinatorial optimization. We have conceived it as an advanced gradu-
ate text which can also be used as an up-to-date reference work for current research.
The book includes the essential fundamentals of graph theory, linear and integer
programming, and complexity theory. It covers classical topics in combinatorial
optimization as well as very recent ones. The emphasis is on theoretical results
and algorithms with provably good performance. Applications and heuristics are
mentioned only occasionally.
Combinatorial optimization has its roots in combinatorics, operations research,
and theoretical computer science. A main motivation is that thousands of real-life
problems can be formulated as abstract combinatorial optimization problems. We
focus on the detailed study of classical problems which occur in many different
contexts, together with the underlying theory.
Most combinatorial optimization problems can be formulated naturally in terms
of graphs and as (integer) linear programs. Therefore this book starts, after an
introduction, by reviewing basic graph theory and proving those results in linear
and integer programming which are most relevant for combinatorial optimization.
Next, the classical topics in combinatorial optimization are studied: minimum
spanning trees, shortest paths, network ﬂows, matchings and matroids. Most of
the problems discussed in Chapters 6–14 have polynomial-time (“efﬁcient”) algo-
rithms, while most of the problems studied in Chapters 15–21 are NP-hard, i.e.
a polynomial-time algorithm is unlikely to exist. In many cases one can at least
ﬁnd approximation algorithms that have a certain performance guarantee. We also
mention some other strategies for coping with such “hard” problems.
This book goes beyond the scope of a normal textbook on combinatorial opti-
mization in various aspects. For example we cover the equivalence of optimization
and separation (for full-dimensional polytopes), O(n3)-implementations of match-
ing algorithms based on ear-decompositions, Turing machines, the Perfect Graph
Theorem, MAXSNP-hardness, the Karmarkar-Karp algorithm for bin packing, re-
cent approximation algorithms for multicommodity ﬂows, survivable network de-

X
Preface to the First Edition
sign and the Euclidean traveling salesman problem. All results are accompanied
by detailed proofs.
Of course, no book on combinatorial optimization can be absolutely compre-
hensive. Examples of topics which we mention only brieﬂy or do not cover at
all are tree-decompositions, separators, submodular ﬂows, path-matchings, delta-
matroids, the matroid parity problem, location and scheduling problems, non-
linear problems, semideﬁnite programming, average-case analysis of algorithms,
advanced data structures, parallel and randomized algorithms, and the theory of
probabilistically checkable proofs (we cite the PCP Theorem without proof).
At the end of each chapter there are a number of exercises containing additional
results and applications of the material in that chapter. Some exercises which
might be more difﬁcult are marked with an asterisk. Each chapter ends with a list
of references, including texts recommended for further reading.
This book arose from several courses on combinatorial optimization and from
special classes on topics like polyhedral combinatorics or approximation algo-
rithms. Thus, material for basic and advanced courses can be selected from this
book.
We have beneﬁted from discussions and suggestions of many colleagues and
friends and – of course – from other texts on this subject. Especially we owe sincere
thanks to Andr´as Frank, L´aszl´o Lov´asz, Andr´as Recski, Alexander Schrijver and
Zolt´an Szigeti. Our colleagues and students in Bonn, Christoph Albrecht, Ursula
B¨unnagel, Thomas Emden-Weinert, Mathias Hauptmann, Sven Peyer, Rabe von
Randow, Andr´e Rohe, Martin Thimm and J¨urgen Werber, have carefully read
several versions of the manuscript and helped to improve it. Last, but not least we
thank Springer Verlag for the most efﬁcient cooperation.
Bonn, January 2000
Bernhard Korte and Jens Vygen

Table of Contents
1.
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Enumeration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Running Time of Algorithms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3
Linear Optimization Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.4
Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.
Graphs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.1
Basic Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.2
Trees, Circuits, and Cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.3
Connectivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
2.4
Eulerian and Bipartite Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.5
Planarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.6
Planar Duality
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.
Linear Programming
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.1
Polyhedra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.2
The Simplex Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
3.3
Duality
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
3.4
Convex Hulls and Polytopes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
4.
Linear Programming Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.1
Size of Vertices and Faces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.2
Continued Fractions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
4.3
Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
4.4
The Ellipsoid Method
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.5
Khachiyan’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
4.6
Separation and Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90

XII
Table of Contents
5.
Integer Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
5.1
The Integer Hull of a Polyhedron . . . . . . . . . . . . . . . . . . . . . . . . . .
92
5.2
Unimodular Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
5.3
Total Dual Integrality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
5.4
Totally Unimodular Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
5.5
Cutting Planes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
5.6
Lagrangean Relaxation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
6.
Spanning Trees and Arborescences
. . . . . . . . . . . . . . . . . . . . . . . . . . . 119
6.1
Minimum Spanning Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
6.2
Minimum Weight Arborescences
. . . . . . . . . . . . . . . . . . . . . . . . . . 125
6.3
Polyhedral Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
6.4
Packing Spanning Trees and Arborescences
. . . . . . . . . . . . . . . . . 132
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
7.
Shortest Paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
7.1
Shortest Paths From One Source
. . . . . . . . . . . . . . . . . . . . . . . . . . 144
7.2
Shortest Paths Between All Pairs of Vertices . . . . . . . . . . . . . . . . . 148
7.3
Minimum Mean Cycles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
8.
Network Flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
8.1
Max-Flow-Min-Cut Theorem
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
8.2
Menger’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
8.3
The Edmonds-Karp Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
8.4
Blocking Flows and Fujishige’s Algorithm . . . . . . . . . . . . . . . . . . 166
8.5
The Goldberg-Tarjan Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
8.6
Gomory-Hu Trees
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
8.7
The Minimum Cut in an Undirected Graph . . . . . . . . . . . . . . . . . . 179
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
9.
Minimum Cost Flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
9.1
Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
9.2
An Optimality Criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
9.3
Minimum Mean Cycle-Cancelling Algorithm . . . . . . . . . . . . . . . . 195
9.4
Successive Shortest Path Algorithm . . . . . . . . . . . . . . . . . . . . . . . . 199
9.5
Orlin’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
9.6
Flows Over Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212

Table of Contents
XIII
10. Maximum Matchings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
10.1 Bipartite Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
10.2 The Tutte Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
10.3 Tutte’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
10.4 Ear-Decompositions of Factor-Critical Graphs
. . . . . . . . . . . . . . . 223
10.5 Edmonds’ Matching Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
11. Weighted Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
11.1 The Assignment Problem
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
11.2 Outline of the Weighted Matching Algorithm . . . . . . . . . . . . . . . . 247
11.3 Implementation of the Weighted Matching Algorithm
. . . . . . . . . 250
11.4 Postoptimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
11.5 The Matching Polytope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
12. b-Matchings and T -Joins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
12.1 b-Matchings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
12.2 Minimum Weight T -Joins. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
12.3 T -Joins and T -Cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
12.4 The Padberg-Rao Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
13. Matroids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
13.1 Independence Systems and Matroids
. . . . . . . . . . . . . . . . . . . . . . . 291
13.2 Other Matroid Axioms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
13.3 Duality
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
13.4 The Greedy Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
13.5 Matroid Intersection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
13.6 Matroid Partitioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
13.7 Weighted Matroid Intersection
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
14. Generalizations of Matroids
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
14.1 Greedoids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
14.2 Polymatroids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
14.3 Minimizing Submodular Functions . . . . . . . . . . . . . . . . . . . . . . . . . 331
14.4 Schrijver’s Algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
14.5 Symmetric Submodular Functions
. . . . . . . . . . . . . . . . . . . . . . . . . 337
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341

XIV
Table of Contents
15. NP-Completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
15.1 Turing Machines
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
15.2 Church’s Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
15.3 P and NP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
15.4 Cook’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
15.5 Some Basic NP-Complete Problems . . . . . . . . . . . . . . . . . . . . . . . . 358
15.6 The Class coNP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
15.7 NP-Hard Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374
16. Approximation Algorithms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
16.1 Set Covering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
16.2 Colouring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
16.3 Approximation Schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
16.4 Maximum Satisﬁability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
16.5 The PCP Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
16.6 L-Reductions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410
17. The Knapsack Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
17.1 Fractional Knapsack and Weighted Median Problem
. . . . . . . . . . 415
17.2 A Pseudopolynomial Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
17.3 A Fully Polynomial Approximation Scheme . . . . . . . . . . . . . . . . . 420
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
18. Bin-Packing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
18.1 Greedy Heuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
18.2 An Asymptotic Approximation Scheme . . . . . . . . . . . . . . . . . . . . . 431
18.3 The Karmarkar-Karp Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
19. Multicommodity Flows and Edge-Disjoint Paths . . . . . . . . . . . . . . . . 443
19.1 Multicommodity Flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444
19.2 Algorithms for Multicommodity Flows . . . . . . . . . . . . . . . . . . . . . 447
19.3 Directed Edge-Disjoint Paths Problem . . . . . . . . . . . . . . . . . . . . . . 451
19.4 Undirected Edge-Disjoint Paths Problem . . . . . . . . . . . . . . . . . . . . 455
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463

Table of Contents
XV
20. Network Design Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467
20.1 Steiner Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
20.2 The Robins-Zelikovsky Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 473
20.3 Survivable Network Design
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
20.4 A Primal-Dual Approximation Algorithm . . . . . . . . . . . . . . . . . . . 481
20.5 Jain’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 498
21. The Traveling Salesman Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501
21.1 Approximation Algorithms for the TSP . . . . . . . . . . . . . . . . . . . . . 501
21.2 Euclidean TSPs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 506
21.3 Local Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
21.4 The Traveling Salesman Polytope . . . . . . . . . . . . . . . . . . . . . . . . . . 519
21.5 Lower Bounds
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525
21.6 Branch-and-Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 532
22. Facility Location . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537
22.1 The Uncapacitated Facility Location Problem . . . . . . . . . . . . . . . . 537
22.2 Rounding Linear Programming Solutions
. . . . . . . . . . . . . . . . . . . 539
22.3 Primal-Dual Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541
22.4 Scaling and Greedy Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . 547
22.5 Bounding the Number of Facilities . . . . . . . . . . . . . . . . . . . . . . . . . 550
22.6 Local Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553
22.7 Capacitated Facility Location Problems. . . . . . . . . . . . . . . . . . . . . . 559
22.8 Universal Facility Location . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570
Notation Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577
Subject Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585

1. Introduction
Let us start with two examples.
A company has a machine which drills holes into printed circuit boards. Since
it produces many of these boards it wants the machine to complete one board as
fast as possible. We cannot optimize the drilling time but we can try to minimize
the time the machine needs to move from one point to another. Usually drilling
machines can move in two directions: the table moves horizontally while the
drilling arm moves vertically. Since both movements can be done simultaneously,
the time needed to adjust the machine from one position to another is proportional
to the maximum of the horizontal and the vertical distance. This is often called
the L∞-distance. (Older machines can only move either horizontally or vertically
at a time; in this case the adjusting time is proportional to the L1-distance, the
sum of the horizontal and the vertical distance.)
An optimum drilling path is given by an ordering of the hole positions
p1, . . . , pn such that n−1
i=1 d(pi, pi+1) is minimum, where d is the L∞-distance:
for two points p = (x, y) and p′ = (x′, y′) in the plane we write d(p, p′) :=
max{|x −x′|, |y −y′|}. An order of the holes can be represented by a permutation,
i.e. a bijection π : {1, . . . , n} →{1, . . . , n}.
Which permutation is best of course depends on the hole positions; for each list
of hole positions we have a different problem instance. We say that one instance
of our problem is a list of points in the plane, i.e. the coordinates of the holes to
be drilled. Then the problem can be stated formally as follows:
Drilling Problem
Instance:
A set of points p1, . . . , pn ∈R2.
Task:
Find a permutation π
:
{1, . . . , n}
→
{1, . . . , n} such that
n−1
i=1 d(pπ(i), pπ(i+1)) is minimum.
We now explain our second example. We have a set of jobs to be done,
each having a speciﬁed processing time. Each job can be done by a subset of
the employees, and we assume that all employees who can do a job are equally
efﬁcient. Several employees can contribute to the same job at the same time,
and one employee can contribute to several jobs (but not at the same time). The
objective is to get all jobs done as early as possible.

2
1. Introduction
In this model it sufﬁces to prescribe for each employee how long he or she
should work on which job. The order in which the employees carry out their jobs
is not important, since the time when all jobs are done obviously depends only on
the maximum total working time we have assigned to one employee. Hence we
have to solve the following problem:
Job Assignment Problem
Instance:
A set of numbers t1, . . . , tn ∈R+ (the processing times for n
jobs), a number m ∈N of employees, and a nonempty subset
Si ⊆{1, . . . , m} of employees for each job i ∈{1, . . . , n}.
Task:
Find numbers xi j ∈R+ for all i = 1, . . . , n and j ∈Si such
that 
j∈Si xi j = ti for i = 1, . . . , n and maxj∈{1,...,m}

i: j∈Si xi j is
minimum.
These are two typical problems arising in combinatorial optimization. How to
model a practical problem as an abstract combinatorial optimization problem is
not described in this book; indeed there is no general recipe for this task. Besides
giving a precise formulation of the input and the desired output it is often important
to ignore irrelevant components (e.g. the drilling time which cannot be optimized
or the order in which the employees carry out their jobs).
Of course we are not interested in a solution to a particular drilling problem
or job assignment problem in some company, but rather we are looking for a
way how to solve all problems of these types. We ﬁrst consider the Drilling
Problem.
1.1 Enumeration
How can a solution to the Drilling Problem look like? There are inﬁnitely
many instances (ﬁnite sets of points in the plane), so we cannot list an optimum
permutation for each instance. Instead, what we look for is an algorithm which,
given an instance, computes an optimum solution. Such an algorithm exists: Given
a set of n points, just try all possible n! orders, and for each compute the L∞-length
of the corresponding path.
There are different ways of formulating an algorithm, differing mostly in the
level of detail and the formal language they use. We certainly would not accept
the following as an algorithm: “Given a set of n points, ﬁnd an optimum path and
output it.” It is not speciﬁed at all how to ﬁnd the optimum solution. The above
suggestion to enumerate all possible n! orders is more useful, but still it is not
clear how to enumerate all the orders. Here is one possible way:
We enumerate all n-tuples of numbers 1, . . . , n, i.e. all nn vectors of {1, . . . ,
n}n. This can be done similarly to counting: we start with (1, . . . , 1, 1), (1, . . . ,
1, 2) up to (1, . . . , 1, n) then switch to (1, . . . , 1, 2, 1), and so on. At each step
we increment the last entry unless it is already n, in which case we go back to the
last entry that is smaller than n, increment it and set all subsequent entries to 1.

1.1 Enumeration
3
This technique is sometimes called backtracking. The order in which the vectors
of {1, . . . , n}n are enumerated is called the lexicographical order:
Deﬁnition 1.1.
Let x, y ∈Rn be two vectors. We say that a vector x is lexico-
graphically smaller than y if there exists an index j ∈{1, . . . , n} such that xi = yi
for i = 1, . . . , j −1 and xj < yj.
Knowing how to enumerate all vectors of {1, . . . , n}n we can simply check for
each vector whether its entries are pairwise distinct and, if so, whether the path
represented by this vector is shorter than the best path encountered so far.
Since this algorithm enumerates nn vectors it will take at least nn steps (in fact,
even more). This is not best possible. There are only n! permutations of {1, . . . , n},
and n! is signiﬁcantly smaller than nn. (By Stirling’s formula n! ≈
√
2πn nn
en
(Stirling [1730]); see Exercise 1.) We shall show how to enumerate all paths in
approximately n2 · n! steps. Consider the following algorithm which enumerates
all permutations in lexicographical order:
Path Enumeration Algorithm
Input:
A natural number n ≥3. A set {p1, . . . , pn} of points in the plane.
Output:
A permutation π∗: {1, . . . , n} →{1, . . . , n} with
cost(π∗) := n−1
i=1 d(pπ∗(i), pπ∗(i+1)) minimum.
1⃝
Set π(i) := i and π∗(i) := i for i = 1, . . . , n. Set i := n −1.
2⃝
Let k := min({π(i) + 1, . . . , n + 1} \ {π(1), . . . , π(i −1)}).
3⃝
If k ≤n then:
Set π(i) := k.
If i = n and cost(π) < cost(π∗) then set π∗:= π.
If i < n then set π(i + 1) := 0 and i := i + 1.
If k = n + 1 then set i := i −1.
If i ≥1 then go to 2⃝.
Starting with (π(i))i=1,...,n = (1, 2, 3, . . . , n−1, n) and i = n−1, the algorithm
ﬁnds at each step the next possible value of π(i) (not using π(1), . . . , π(i −1)).
If there is no more possibility for π(i) (i.e. k = n + 1), then the algorithm
decrements i (backtracking). Otherwise it sets π(i) to the new value. If i = n, the
new permutation is evaluated, otherwise the algorithm will try all possible values
for π(i + 1), . . . , π(n) and starts by setting π(i + 1) := 0 and incrementing i.
So all permutation vectors (π(1), . . . , π(n)) are generated in lexicographical
order. For example, the ﬁrst iterations in the case n = 6 are shown below:

4
1. Introduction
π := (1, 2, 3, 4, 5, 6),
i := 5
k := 6,
π := (1, 2, 3, 4, 6, 0),
i := 6
k := 5,
π := (1, 2, 3, 4, 6, 5),
cost(π) < cost(π∗)?
k := 7,
i := 5
k := 7,
i := 4
k := 5,
π := (1, 2, 3, 5, 0, 5),
i := 5
k := 4,
π := (1, 2, 3, 5, 4, 0),
i := 6
k := 6,
π := (1, 2, 3, 5, 4, 6),
cost(π) < cost(π∗)?
Since the algorithm compares the cost of each path to π∗, the best path en-
countered so far, it indeed outputs the optimum path. But how many steps will this
algorithm perform? Of course, the answer depends on what we call a single step.
Since we do not want the number of steps to depend on the actual implementation
we ignore constant factors. In any reasonable computer, 1⃝will take at least 2n+1
steps (this many variable assignments are done) and at most cn steps for some
constant c. The following common notation is useful for ignoring constant factors:
Deﬁnition 1.2.
Let f, g : D →R+ be two functions. We say that f is O(g) (and
sometimes write f = O(g)) if there exist constants α, β > 0 such that f (x) ≤
αg(x) + β for all x ∈D. If f = O(g) and g = O( f ) we also say that f = (g)
(and of course g = ( f )). In this case, f and g have the same rate of growth.
Note that the use of the equation sign in the O-notation is not symmetric. To
illustrate this deﬁnition, let D = N, and let f (n) be the number of elementary
steps in 1⃝and g(n) = n (n ∈N). Clearly we have f = O(g) (in fact f = (g))
in this case; we say that 1⃝takes O(n) time (or linear time). A single execution
of 3⃝takes a constant number of steps (we speak of O(1) time or constant time)
except in the case k ≤n and i = n; in this case the cost of two paths have to be
compared, which takes O(n) time.
What about
2⃝? A naive implementation, checking for each j ∈{π(i) +
1, . . . , n} and each h ∈{1, . . . , i −1} whether j = π(h), takes O((n −π(i))i)
steps, which can be as big as (n2). A better implementation of
2⃝uses an
auxiliary array indexed by 1, . . . , n:
2⃝
For j := 1 to n do aux( j) := 0.
For j := 1 to i −1 do aux(π( j)) := 1.
Set k := π(i) + 1.
While k ≤n and aux(k) = 1 do k := k + 1.
Obviously with this implementation a single execution of 2⃝takes only O(n)
time. Simple techniques like this are usually not elaborated in this book; we assume
that the reader can ﬁnd such implementations himself.
Having computed the running time for each single step we now estimate the
total amount of work. Since the number of permutations is n! we only have to
estimate the amount of work which is done between two permutations. The counter
i might move back from n to some index i′ where a new value π(i′) ≤n is found.
Then it moves forward again up to i = n. While the counter i is constant each
of
2⃝and
3⃝is performed once, except in the case k ≤n and i = n; in this

1.2 Running Time of Algorithms
5
case 2⃝and 3⃝are performed twice. So the total amount of work between two
permutations consists of at most 4n times 2⃝and 3⃝, i.e. O(n2). So the overall
running time of the Path Enumeration Algorithm is O(n2n!).
One can do slightly better; a more careful analysis shows that the running time
is only O(n · n!) (Exercise 4).
Still the algorithm is too time-consuming if n is large. The problem with the
enumeration of all paths is that the number of paths grows exponentially with the
number of points; already for 20 points there are 20! = 2432902008176640000 ≈
2.4 · 1018 different paths and even the fastest computer needs several years to
evaluate all of them. So complete enumeration is impossible even for instances of
moderate size.
The main subject of combinatorial optimization is to ﬁnd better algorithms for
problems like this. Often one has to ﬁnd the best element of some ﬁnite set of
feasible solutions (in our example: drilling paths or permutations). This set is not
listed explicitly but implicitly depends on the structure of the problem. Therefore
an algorithm must exploit this structure.
In the case of the Drilling Problem all information of an instance with n
points is given by 2n coordinates. While the naive algorithm enumerates all n!
paths it might be possible that there is an algorithm which ﬁnds the optimum path
much faster, say in n2 computation steps. It is not known whether such an algorithm
exists (though results of Chapter 15 suggest that it is unlikely). Nevertheless there
are much better algorithms than the naive one.
1.2 Running Time of Algorithms
One can give a formal deﬁnition of an algorithm, and we shall in fact give one
in Section 15.1. However, such formal models lead to very long and tedious
descriptions as soon as algorithms are a bit more complicated. This is quite similar
to mathematical proofs: Although the concept of a proof can be formalized nobody
uses such a formalism for writing down proofs since they would become very long
and almost unreadable.
Therefore all algorithms in this book are written in an informal language. Still
the level of detail should allow a reader with a little experience to implement the
algorithms on any computer without too much additional effort.
Since we are not interested in constant factors when measuring running times
we do not have to ﬁx a concrete computing model. We count elementary steps,
but we are not really interested in how elementary steps look like. Examples of
elementary steps are variable assignments, random access to a variable whose
index is stored in another variable, conditional jumps (if – then – go to), and
simple arithmetic operations like addition, subtraction, multiplication, division and
comparison of numbers.
An algorithm consists of a set of valid inputs and a sequence of instructions
each of which can be composed of elementary steps, such that for each valid input
the computation of the algorithm is a uniquely deﬁned ﬁnite series of elementary

6
1. Introduction
steps which produces a certain output. Usually we are not satisﬁed with ﬁnite
computation but rather want a good upper bound on the number of elementary
steps performed, depending on the input size.
The input to an algorithm usually consists of a list of numbers. If all these
numbers are integers, we can code them in binary representation, using O(log(|a|+
2)) bits for storing an integer a. Rational numbers can be stored by coding the
numerator and the denominator separately. The input size size(x) of an instance x
with rational data is the total number of bits needed for the binary representation.
Deﬁnition 1.3.
Let A be an algorithm which accepts inputs from a set X, and let
f : N →R+. If there exists a constant α > 0 such that A terminates its computa-
tion after at most αf (size(x)) elementary steps (including arithmetic operations)
for each input x ∈X, then we say that A runs in O( f ) time. We also say that the
running time (or the time complexity) of A is O( f ).
Deﬁnition 1.4.
An algorithm with rational input is said to run in polynomial
time if there is an integer k such that it runs in O(nk) time, where n is the input
size, and all numbers in intermediate computations can be stored with O(nk) bits.
An algorithm with arbitrary input is said to run in strongly polynomial time
if there is an integer k such that it runs in O(nk) time for any input consisting of
n numbers and it runs in polynomial time for rational input. In the case k = 1 we
have a linear-time algorithm.
Note that the running time might be different for several instances of the
same size (this was not the case with the Path Enumeration Algorithm). We
consider the worst-case running time, i.e. the function f : N →N where f (n) is
the maximum running time of an instance with input size n. For some algorithms
we do not know the rate of growth of f but only have an upper bound.
The worst-case running time might be a pessimistic measure if the worst case
occurs rarely. In some cases an average-case running time with some probabilistic
model might be appropriate, but we shall not consider this.
If A is an algorithm which for each input x ∈X computes the output f (x) ∈Y,
then we say that A computes f : X →Y. If a function is computed by some
polynomial-time algorithm, it is said to be computable in polynomial time.
Polynomial-time algorithms are sometimes called “good” or “efﬁcient”. This
concept was introduced by Cobham [1964] and Edmonds [1965]. Table 1.1 moti-
vates this by showing hypothetical running times of algorithms with various time
complexities. For various input sizes n we show the running time of algorithms
that take 100n log n, 10n2, n3.5, nlog n, 2n, and n! elementary steps; we assume that
one elementary step takes one nanosecond. As always in this book, log denotes
the logarithm with basis 2.
As Table 1.1 shows, polynomial-time algorithms are faster for large enough
instances. The table also illustrates that constant factors of moderate size are not
very important when considering the asymptotic growth of the running time.
Table 1.2 shows the maximum input sizes solvable within one hour with the
above six hypothetical algorithms. In (a) we again assume that one elementary step

1.2 Running Time of Algorithms
7
Table 1.1.
n
100n log n
10n2
n3.5
nlog n
2n
n!
10
3 µs
1 µs
3 µs
2 µs
1 µs
4 ms
20
9 µs
4 µs
36 µs
420 µs
1 ms
76 years
30
15 µs
9 µs
148 µs
20 ms
1 s
8 · 1015 y.
40
21 µs
16 µs
404 µs
340 ms
1100 s
50
28 µs
25 µs
884 µs
4 s
13 days
60
35 µs
36 µs
2 ms
32 s
37 years
80
50 µs
64 µs
5 ms
1075 s
4 · 107 y.
100
66 µs
100 µs
10 ms
5 hours
4 · 1013 y.
200
153 µs
400 µs
113 ms
12 years
500
448 µs
2.5 ms
3 s
5 · 105 y.
1000
1 ms
10 ms
32 s
3 · 1013 y.
104
13 ms
1 s
28 hours
105
166 ms
100 s
10 years
106
2 s
3 hours
3169 y.
107
23 s
12 days
107 y.
108
266 s
3 years
3 · 1010 y.
1010
9 hours
3 · 104 y.
1012
46 days
3 · 108 y.
takes one nanosecond, (b) shows the corresponding ﬁgures for a ten times faster
machine. Polynomial-time algorithms can handle larger instances in reasonable
time. Moreover, even a speedup by a factor of 10 of the computers does not in-
crease the size of solvable instances signiﬁcantly for exponential-time algorithms,
but it does for polynomial-time algorithms.
Table 1.2.
100n log n
10n2
n3.5
nlog n
2n
n!
(a)
1.19 · 109
60000
3868
87
41
15
(b)
10.8 · 109
189737
7468
104
45
16
(Strongly) polynomial-time algorithms, if possible linear-time algorithms, are
what we look for. There are some problems where it is known that no polynomial-
time algorithm exists, and there are problems for which no algorithm exists at all.
(For example, a problem which can be solved in ﬁnite time but not in polynomial
time is to decide whether a so-called regular expression deﬁnes the empty set; see
Aho, Hopcroft and Ullman [1974]. A problem for which there exists no algorithm
at all, the Halting Problem, is discussed in Exercise 1 of Chapter 15.)
However, almost all problems considered in this book belong to the follow-
ing two classes. For the problems of the ﬁrst class we have a polynomial-time

8
1. Introduction
algorithm. For each problem of the second class it is an open question whether a
polynomial-time algorithm exists. However, we know that if one of these prob-
lems has a polynomial-time algorithm, then all problems of this class do. A precise
formulation and a proof of this statement will be given in Chapter 15.
The Job Assignment Problem belongs to the ﬁrst class, the Drilling Prob-
lem belongs to the second class.
These two classes of problems divide this book roughly into two parts.
We ﬁrst deal with tractable problems for which polynomial-time algorithms are
known. Then, starting with Chapter 15, we discuss hard problems. Although no
polynomial-time algorithms are known, there are often much better methods than
complete enumeration. Moreover, for many problems (including the Drilling
Problem), one can ﬁnd approximate solutions within a certain percentage of the
optimum in polynomial time.
1.3 Linear Optimization Problems
We now consider our second example given initially, the Job Assignment Prob-
lem, and brieﬂy address some central topics which will be discussed in later
chapters.
The Job Assignment Problem is quite different to the Drilling Problem
since there are inﬁnitely many feasible solutions for each instance (except for
trivial cases). We can reformulate the problem by introducing a variable T for the
time when all jobs are done:
min
T
s.t.

j∈Si
xi j
=
ti
(i ∈{1, . . . , n})
xi j
≥
0
(i ∈{1, . . . , n}, j ∈Si)

i: j∈Si
xi j
≤
T
( j ∈{1, . . . , m})
(1.1)
The numbers ti and the sets Si (i = 1, . . . , n) are given, the variables xi j and
T are what we look for. Such an optimization problem with a linear objective
function and linear constraints is called a linear program. The set of feasible
solutions of (1.1), a so-called polyhedron, is easily seen to be convex, and one
can prove that there always exists an optimum solution which is one of the ﬁnitely
many extreme points of this set. Therefore a linear program can, theoretically, also
be solved by complete enumeration. But there are much better ways as we shall
see later.
Although there are several algorithms for solving linear programs in general,
such general techniques are usually less efﬁcient than special algorithms exploiting
the structure of the problem. In our case it is convenient to model the sets Si,

1.4 Sorting
9
i = 1, . . . , n, by a graph. For each job i and for each employee j we have a
point (called vertex), and we connect employee j with job i by an edge if he or she
can contribute to this job (i.e. if j ∈Si). Graphs are a fundamental combinatorial
structure; many combinatorial optimization problems are described most naturally
in terms of graph theory.
Suppose for a moment that the processing time of each job is one hour, and
we ask whether we can ﬁnish all jobs within one hour. So we look for numbers
xi j (i ∈{1, . . . , n}, j ∈Si) such that 0 ≤xi j ≤1 for all i and j, 
j∈Si xi j = 1 for
i = 1, . . . , n, and 
i: j∈Si xi j ≤1 for j = 1, . . . , n. One can show that if such a
solution exists, then in fact an integral solution exists, i.e. all xi j are either 0 or 1.
This is equivalent to assigning each job to one employee, such that no employee
has to do more than one job. In the language of graph theory we then look for a
matching covering all jobs. The problem of ﬁnding optimal matchings is one of
the best known combinatorial optimization problems.
We review the basics of graph theory and linear programming in Chapters 2
and 3. In Chapter 4 we prove that linear programs can be solved in polynomial
time, and in Chapter 5 we discuss integral polyhedra. In the subsequent chapters
we discuss some classical combinatorial optimization problems in detail.
1.4 Sorting
Let us conclude this chapter by considering a special case of the Drilling Prob-
lem where all holes to be drilled are on one horizontal line. So we are given just
one coordinate for each point pi, i = 1, . . . , n. Then a solution to the drilling
problem is easy, all we have to do is sort the points by their coordinates: the drill
will just move from left to right. Although there are still n! permutations, it is
clear that we do not have to consider all of them to ﬁnd the optimum drilling
path, i.e. the sorted list. It is very easy to sort n numbers in nondecreasing order
in O(n2) time.
To sort n numbers in O(n log n) time requires a little more skill. There are
several algorithms accomplishing this; we present the well-known Merge-Sort
Algorithm. It proceeds as follows. First the list is divided into two sublists of
approximately equal size. Then each sublist is sorted (this is done recursively by
the same algorithm). Finally the two sorted sublists are merged together. This
general strategy, often called “divide and conquer”, can be used quite often. See
e.g. Section 17.1 for another example.
We did not discuss recursive algorithms so far. In fact, it is not necessary to
discuss them, since any recursive algorithm can be transformed into a sequential
algorithm without increasing the running time. But some algorithms are easier to
formulate (and implement) using recursion, so we shall use recursion when it is
convenient.

10
1. Introduction
Merge-Sort Algorithm
Input:
A list a1, . . . , an of real numbers.
Output:
A permutation π : {1, . . . , n} →{1, . . . , n} such that aπ(i) ≤aπ(i+1)
for all i = 1, . . . , n −1.
1⃝
If n = 1 then set π(1) := 1 and stop (return π).
2⃝
Set m :=
 n
2

.
Let ρ :=Merge-Sort(a1, . . . , am).
Let σ :=Merge-Sort(am+1, . . . , an).
3⃝
Set k := 1, l := 1.
While k ≤m and l ≤n −m do:
If aρ(k) ≤am+σ(l) then set π(k + l −1) := ρ(k) and k := k + 1
else set π(k + l −1) := m + σ(l) and l := l + 1.
While k ≤m do: Set π(k + l −1) := ρ(k) and k := k + 1.
While l ≤n −m do: Set π(k + l −1) := m + σ(l) and l := l + 1.
As an example, consider the list “69,32,56,75,43,99,28”. The algorithm ﬁrst
splits this list into two, “69,32,56” and “75,43,99,28” and recursively sorts each
of the two sublists. We get the permutations ρ = (2, 3, 1) and σ = (4, 2, 1, 3)
corresponding to the sorted lists “32,56,69” and “28,43,75,99”. Now these lists
are merged as shown below:
k := 1,
l := 1
ρ(1) = 2,
σ(1) = 4,
aρ(1) = 32,
aσ(1) = 28,
π(1) := 7,
l := 2
ρ(1) = 2,
σ(2) = 2,
aρ(1) = 32,
aσ(2) = 43,
π(2) := 2,
k := 2
ρ(2) = 3,
σ(2) = 2,
aρ(2) = 56,
aσ(2) = 43,
π(3) := 5,
l := 3
ρ(2) = 3,
σ(3) = 1,
aρ(2) = 56,
aσ(3) = 75,
π(4) := 3,
k := 3
ρ(3) = 1,
σ(3) = 1,
aρ(3) = 69,
aσ(3) = 75,
π(5) := 1,
k := 4
σ(3) = 1,
aσ(3) = 75,
π(6) := 4,
l := 4
σ(4) = 3,
aσ(4) = 99,
π(7) := 6,
l := 5
Theorem 1.5.
The Merge-Sort Algorithm works correctly and runs in
O(n log n) time.
Proof:
The correctness is obvious. We denote by T (n) the running time (number
of steps) needed for instances consisting of n numbers and observe that T (1) = 1
and T (n) = T (⌊n
2⌋) + T (⌈n
2⌉) + 3n + 6. (The constants in the term 3n + 6 depend
on how exactly a computation step is deﬁned; but they do not really matter.)
We claim that this yields T (n) ≤12n log n + 1. Since this is trivial for n = 1
we proceed by induction. For n ≥2, assuming that the inequality is true for
1, . . . , n −1, we get
T (n)
≤
12
n
2

log
2
3n

+ 1 + 12
	n
2

log
2
3n

+ 1 + 3n + 6

Exercises
11
=
12n(log n + 1 −log 3) + 3n + 8
≤
12n log n −13
2 n + 3n + 8 ≤12n log n + 1,
because log 3 ≥37
24.
2
Of course the algorithm works for sorting the elements of any totally ordered
set, assuming that we can compare any two elements in constant time. Can there be
a faster, a linear-time algorithm? Suppose that the only way we can get information
on the unknown order is to compare two elements. Then we can show that any
algorithm needs at least (n log n) comparisons in the worst case. The outcome
of a comparison can be regarded as a zero or one; the outcome of all comparisons
an algorithm does is a 0-1-string (a sequence of zeros and ones). Note that two
different orders in the input of the algorithm must lead to two different 0-1-strings
(otherwise the algorithm could not distinguish between the two orders). For an
input of n elements there are n! possible orders, so there must be n! different 0-
1-strings corresponding to the computation. Since the number of 0-1-strings with
length less than
 n
2 log n
2

is 2⌊n
2 log n
2⌋−1 < 2
n
2 log n
2 = ( n
2)
n
2 ≤n! we conclude
that the maximum length of the 0-1-strings, and hence of the computation, must
be at least n
2 log n
2 = (n log n).
In the above sense, the running time of the Merge-Sort Algorithm is optimal
up to a constant factor. However, there is an algorithm for sorting integers (or
sorting strings lexicographically) whose running time is linear in the input size;
see Exercise 7. An algorithm to sort n integers in O(n log log n) time was proposed
by Han [2004].
Lower bounds like the one above are known only for very few problems (except
trivial linear bounds). Often a restriction on the set of operations is necessary to
derive a superlinear lower bound.
Exercises
1. Prove that for all n ∈N:
e
n
e
n
≤n! ≤en
n
e
n
.
Hint: Use 1 + x ≤ex for all x ∈R.
2. Prove that log(n!) = (n log n).
3. Prove that n log n = O(n1+ϵ) for any ϵ > 0.
4. Show that the running time of the Path Enumeration Algorithm is
O(n · n!).
5. Suppose we have an algorithm whose running time is (n(t + n1/t)), where
n is the input length and t is a positive parameter we can choose arbitrarily.
How should t be chosen (depending on n) such that the running time (as a
function of n) has a minimum rate of growth?

12
1. Introduction
6. Let s, t be binary strings, both of length m. We say that s is lexicographically
smaller than t if there exists an index j ∈{1, . . . , m} such that si = ti for
i = 1, . . . , j −1 and sj < tj. Now given n strings of length m, we want to
sort them lexicographically. Prove that there is a linear-time algorithm for this
problem (i.e. one with running time O(nm)).
Hint: Group the strings according to the ﬁrst bit and sort each group.
7. Describe an algorithm which sorts a list of natural numbers a1, . . . , an in linear
time; i.e. which ﬁnds a permutation π with aπ(i) ≤aπ(i+1) (i = 1, . . . , n −1)
and runs in O(log(a1 + 1) + · · · + log(an + 1)) time.
Hint: First sort the strings encoding the numbers according to their length.
Then apply the algorithm of Exercise 6.
Note: The algorithm discussed in this and the previous exercise is often called
radix sorting.
References
General Literature:
Knuth, D.E. [1968]: The Art of Computer Programming; Vol. 1. Addison-Wesley, Reading
1968 (3rd edition: 1997)
Cited References:
Aho, A.V., Hopcroft, J.E., and Ullman, J.D. [1974]: The Design and Analysis of Computer
Algorithms. Addison-Wesley, Reading 1974
Cobham, A. [1964]: The intrinsic computational difﬁculty of functions. Proceedings of the
1964 Congress for Logic Methodology and Philosophy of Science (Y. Bar-Hillel, ed.),
North-Holland, Amsterdam 1964, pp. 24–30
Edmonds, J. [1965]: Paths, trees, and ﬂowers. Canadian Journal of Mathematics 17 (1965),
449–467
Han, Y. [2004]: Deterministic sorting in O(n log log n) time and linear space. Journal of
Algorithms 50 (2004), 96–105
Stirling, J. [1730]: Methodus Differentialis. London 1730

2. Graphs
Graphs are a fundamental combinatorial structure used throughout this book. In
this chapter we not only review the standard deﬁnitions and notation, but also
prove some basic theorems and mention some fundamental algorithms.
After some basic deﬁnitions in Section 2.1 we consider fundamental objects
occurring very often in this book: trees, circuits, and cuts. We prove some important
properties and relations, and we also consider tree-like set systems in Section
2.2. The ﬁrst graph algorithms, determining connected and strongly connected
components, appear in Section 2.3. In Section 2.4 we prove Euler’s Theorem on
closed walks using every edge exactly once. Finally, in Sections 2.5 and 2.6 we
consider graphs that can be drawn in the plane without crossings.
2.1 Basic Deﬁnitions
An undirected graph is a triple (V, E, ), where V and E are ﬁnite sets and
 : E →{X ⊆V : |X| = 2}. A directed graph or digraph is a triple (V, E, ),
where V and E are ﬁnite sets and  : E →{(v, w) ∈V × V : v ̸= w}. By a
graph we mean either an undirected graph or a digraph. The elements of V are
called vertices, the elements of E are the edges.
Two edges e, e′ with (e) = (e′) are called parallel. Graphs without parallel
edges are called simple. For simple graphs we usually identify an edge e with its
image (e) and write G = (V (G), E(G)), where E(G) ⊆{X ⊆V (G) : |X| = 2}
or E(G) ⊆V (G)×V (G). We often use this simpler notation even in the presence
of parallel edges, then the “set” E(G) may contain several “identical” elements.
|E(G)| denotes the number of edges, and for two edge sets E and F we always
have |E
.
∪F| = |E| + |F| even if parallel edges arise.
We say that an edge e = {v, w} or e = (v, w) joins v and w. In this case,
v and w are adjacent. v is a neighbour of w (and vice versa). v and w are the
endpoints of e. If v is an endpoint of an edge e, we say that v is incident with e.
In the directed case we say that (v, w) leaves v and enters w. Two edges which
share at least one endpoint are called adjacent.
This terminology for graphs is not the only one. Sometimes vertices are called
nodes or points, other names for edges are arcs (especially in the directed case)
or lines. In some texts, a graph is what we call a simple undirected graph, in

14
2. Graphs
the presence of parallel edges they speak of multigraphs. Sometimes edges whose
endpoints coincide, so-called loops, are considered. However, unless otherwise
stated, we do not use them.
For a digraph G we sometimes consider the underlying undirected graph,
i.e. the undirected graph G′ on the same vertex set which contains an edge {v, w}
for each edge (v, w) of G. We also say that G is an orientation of G′.
A subgraph of a graph G = (V (G), E(G)) is a graph H = (V (H), E(H))
with V (H) ⊆V (G) and E(H) ⊆E(G). We also say that G contains H. H is an
induced subgraph of G if it is a subgraph of G and E(H) = {{x, y} or (x, y) ∈
E(G) : x, y ∈V (H)}. Here H is the subgraph of G induced by V (H). We also
write H = G[V (H)]. A subgraph H of G is called spanning if V (H) = V (G).
If v ∈V (G), we write G −v for the subgraph of G induced by V (G) \ {v}.
If e ∈E(G), we deﬁne G −e := (V (G), E(G) \ {e}). Furthermore, the addition
of a new edge e is abbreviated by G + e := (V (G), E(G)
.
∪{e}). If G and H
are two graphs, we denote by G + H the graph with V (G + H) = V (G) ∪V (H)
and E(G + H) being the disjoint union of E(G) and E(H) (parallel edges may
arise).
Two graphs G and H are called isomorphic if there are bijections 	V :
V (G) →V (H) and 	E : E(G) →E(H) such that 	E((v, w)) = (	V (v),
	V (w)) for all (v, w) ∈E(G), or 	E({v, w}) = {	V (v), 	V (w)} for all {v, w}
∈E(G) in the undirected case. We normally do not distinguish between isomorphic
graphs; for example we say that G contains H if G has a subgraph isomorphic
to H.
Suppose we have an undirected graph G and X ⊆V (G). By contracting (or
shrinking) X we mean deleting the vertices in X and the edges in G[X], adding
a new vertex x and replacing each edge {v, w} with v ∈X, w /∈X by an edge
{x, w} (parallel edges may arise). Similarly for digraphs. We often call the result
G/X.
For a graph G and X, Y ⊆V (G) we deﬁne E(X, Y) := {{x, y} ∈E(G) :
x ∈X \ Y, y ∈Y \ X} if G is undirected and E+(X, Y) := {(x, y) ∈E(G) :
x ∈X \ Y, y ∈Y \ X} if G is directed. For undirected graphs G and X ⊆V (G)
we deﬁne δ(X) := E(X, V (G) \ X). The set of neighbours of X is deﬁned by
(X) := {v ∈V (G) \ X : E(X, {v}) ̸= ∅}. For digraphs G and X ⊆V (G)
we deﬁne δ+(X) := E+(X, V (G) \ X), δ−(X) := δ+(V (G) \ X) and δ(X) :=
δ+(X)∪δ−(X). We use subscripts (e.g. δG(X)) to specify the graph G if necessary.
For singletons, i.e. one-element vertex sets {v} (v ∈V (G)) we write δ(v) :=
δ({v}), (v) := ({v}), δ+(v) := δ+({v}) and δ−(v) := δ−({v}). The degree of
a vertex v is |δ(v)|, the number of edges incident to v. In the directed case, the
in-degree is |δ−(v)|, the out-degree is |δ+(v)|, and the degree is |δ+(v)|+|δ−(v)|.
A vertex v with zero degree is called isolated. A graph where all vertices have
degree k is called k-regular.
For any graph, 
v∈V (G) |δ(v)| = 2|E(G)|. In particular, the number of vertices
with odd degree is even. In a digraph, 
v∈V (G) |δ+(v)| = 
v∈V (G) |δ−(v)|. To
prove these statements, please observe that each edge is counted twice on each

2.1 Basic Deﬁnitions
15
side of the ﬁrst equation and once on each side of the second equation. With just
a little more effort we get the following useful statements:
Lemma 2.1.
For a digraph G and any two sets X, Y ⊆V (G):
(a) |δ+(X)|+|δ+(Y)| = |δ+(X ∩Y)|+|δ+(X ∪Y)|+|E+(X, Y)|+|E+(Y, X)|;
(b) |δ−(X)| + |δ−(Y)| = |δ−(X ∩Y)| + |δ−(X ∪Y)| + |E+(X, Y)| + |E+(Y, X)|.
For an undirected graph G and any two sets X, Y ⊆V (G):
(c) |δ(X)| + |δ(Y)| = |δ(X ∩Y)| + |δ(X ∪Y)| + 2|E(X, Y)|;
(d) |(X)| + |(Y)| ≥|(X ∩Y)| + |(X ∪Y)|.
Proof:
All parts can be proved by simple counting arguments. Let Z := V (G) \
(X ∪Y).
To prove (a), observe that |δ+(X)|+|δ+(Y)| = |E+(X, Z)|+|E+(X, Y \ X)|+
|E+(Y, Z)| + |E+(Y, X \ Y)| = |E+(X ∪Y, Z)| + |E+(X ∩Y, Z)| + |E+(X, Y \
X)| + |E+(Y, X \ Y)| = |δ+(X ∪Y)| + |δ+(X ∩Y)| + |E+(X, Y)| + |E+(Y, X)|.
(b) follows from (a) by reversing each edge (replace (v, w) by (w, v)). (c)
follows from (a) by replacing each edge {v, w} by a pair of oppositely directed
edges (v, w) and (w, v).
To show (d), observe that |(X)| + |(Y)| = |(X ∪Y)| + |(X) ∩(Y)| +
|(X) ∩Y| + |(Y) ∩X| ≥|(X ∪Y)| + |(X ∩Y)|.
2
A function f : 2U →R (where U is some ﬁnite set and 2U denotes its power
set) is called
–
submodular if f (X ∩Y) + f (X ∪Y) ≤f (X) + f (Y) for all X, Y ⊆U;
–
supermodular if f (X ∩Y) + f (X ∪Y) ≥f (X) + f (Y) for all X, Y ⊆U;
–
modular if f (X ∩Y) + f (X ∪Y) = f (X) + f (Y) for all X, Y ⊆U.
So Lemma 2.1 implies that |δ+|, |δ−|, |δ| and || are submodular. This will be
useful later.
A complete graph is a simple undirected graph where each pair of vertices
is adjacent. We denote the complete graph on n vertices by Kn. The complement
of a simple undirected graph G is the graph H for which G + H is a complete
graph.
A matching in an undirected graph G is a set of pairwise disjoint edges (i.e.
the endpoints are all different). A vertex cover in G is a set S ⊆V (G) of vertices
such that every edge of G is incident to at least one vertex in S. An edge cover in
G is a set F ⊆E(G) of edges such that every vertex of G is incident to at least
one edge in F. A stable set in G is a set of pairwise non-adjacent vertices. A
graph containing no edges is called empty. A clique is a set of pairwise adjacent
vertices.
Proposition 2.2.
Let G be a graph and X ⊆V (G). Then the following three
statements are equivalent:
(a) X is a vertex cover in G,

16
2. Graphs
(b) V (G) \ X is a stable set in G,
(c) V (G) \ X is a clique in the complement of G.
2
If F is a family of sets or graphs, we say that F is a minimal element of F if
F contains F but no proper subset/subgraph of F. Similarly, F is maximal in F
if F ∈F and F is not a proper subset/subgraph of any element of F. When we
speak of a minimum or maximum element, we mean one of minimum/maximum
cardinality.
For example, a minimal vertex cover is not necessarily a minimum vertex
cover (see e.g. the graph in Figure 13.1), and a maximal matching is in general not
maximum. The problems of ﬁnding a maximum matching, stable set or clique, or
a minimum vertex cover or edge cover in an undirected graph will play important
roles in later chapters.
The line graph of a simple undirected graph G is the graph (E(G), F), where
F = {{e1, e2} : e1, e2 ∈E(G), |e1 ∩e2| = 1}. Obviously, matchings in a graph G
correspond to stable sets in the line graph of G.
For the following notation, let G be a graph, directed or undirected. An edge
progression W in G is a sequence v1, e1, v2, . . . , vk, ek, vk+1 such that k ≥0, and
ei = (vi, vi+1) ∈E(G) or ei = {vi, vi+1} ∈E(G) for i = 1, . . . , k. If in addition
ei ̸= ej for all 1 ≤i < j ≤k, W is called a walk in G. W is closed if v1 = vk+1.
A path is a graph P = ({v1, . . . , vk+1}, {e1, . . . , ek}) such that vi ̸= vj for
1 ≤i < j ≤k + 1 and the sequence v1, e1, v2, . . . , vk, ek, vk+1 is a walk. P is
also called a path from v1 to vk+1 or a v1-vk+1-path. v1 and vk+1 are the endpoints
of P. By P[x,y] with x, y ∈V (P) we mean the (unique) subgraph of P which is
an x-y-path. Evidently, there is an edge progression from a vertex v to another
vertex w if and only if there is a v-w-path.
A circuit or a cycle is a graph ({v1, . . . , vk}, {e1, . . . , ek}) such that the se-
quence v1, e1, v2, . . . , vk, ek, v1 is a (closed) walk and vi ̸= vj for 1 ≤i < j ≤k.
An easy induction argument shows that the edge set of a closed walk can be
partitioned into edge sets of circuits.
The length of a path or circuit is the number of its edges. If it is a subgraph
of G, we speak of a path or circuit in G. A spanning path in G is called a
Hamiltonian path while a spanning circuit in G is called a Hamiltonian circuit
or a tour. A graph containing a Hamiltonian circuit is a Hamiltonian graph.
For two vertices v and w we write dist(v, w) or distG(v, w) for the length of
a shortest v-w-path (the distance from v to w) in G. If there is no v-w-path at all,
i.e. w is not reachable from v, we set dist(v, w) := ∞. In the undirected case,
dist(v, w) = dist(w, v) for all v, w ∈V (G).
We shall often have a cost function c : E(G) →R. Then for F ⊆E(G) we
write c(F) := 
e∈F c(e) (and c(∅) = 0). This extends c to a modular function
c : 2E(G) →R. Moreover, dist(G,c)(v, w) denotes the minimum c(E(P)) over all
v-w-paths P in G.

2.2 Trees, Circuits, and Cuts
17
2.2 Trees, Circuits, and Cuts
Let G be some undirected graph. G is called connected if there is a v-w-path for
all v, w ∈V (G); otherwise G is disconnected. The maximal connected subgraphs
of G are its connected components. Sometimes we identify the connected com-
ponents with the vertex sets inducing them. A set of vertices X is called connected
if the subgraph induced by X is connected. A vertex v with the property that G−v
has more connected components than G is called an articulation vertex. An edge
e is called a bridge if G −e has more connected components than G.
An undirected graph without a circuit (as a subgraph) is called a forest. A
connected forest is a tree. A vertex of degree 1 in a tree is called a leaf. A star
is a tree where at most one vertex is not a leaf.
In the following we shall give some equivalent characterizations of trees and
their directed counterparts, arborescences. We need the following connectivity
criterion:
Proposition 2.3.
(a) An undirected graph G is connected if and only if δ(X) ̸= ∅for all ∅̸= X ⊂
V (G).
(b) Let G be a digraph and r ∈V (G). Then there exists an r-v-path for every
v ∈V (G) if and only if δ+(X) ̸= ∅for all X ⊂V (G) with r ∈X.
Proof:
(a): If there is a set X ⊂V (G) with r ∈X, v ∈V (G)\ X, and δ(X) = ∅,
there can be no r-v-path, so G is not connected. On the other hand, if G is not
connected, there is no r-v-path for some r and v. Let R be the set of vertices
reachable from r. We have r ∈R, v /∈R and δ(R) = ∅.
(b) is proved analogously.
2
Theorem 2.4.
Let G be an undirected graph on n vertices. Then the following
statements are equivalent:
(a) G is a tree (i.e. is connected and has no circuits).
(b) G has n −1 edges and no circuits.
(c) G has n −1 edges and is connected.
(d) G is a minimal connected graph (i.e. every edge is a bridge).
(e) G is a minimal graph with δ(X) ̸= ∅for all ∅̸= X ⊂V (G).
(f) G is a maximal circuit-free graph (i.e. the addition of any edge creates a
circuit).
(g) G contains a unique path between any pair of vertices.
Proof:
(a)⇒(g) follows from the fact that the union of two distinct paths with
the same endpoints contains a circuit.
(g)⇒(e)⇒(d) follows from Proposition 2.3(a).
(d)⇒(f) is trivial.
(f)⇒(b)⇒(c): This follows from the fact that for forests with n vertices, m
edges and p connected components n = m + p holds. (The proof is a trivial
induction on m.)

18
2. Graphs
(c)⇒(a): Let G be connected with n −1 edges. As long as there are any
circuits in G, we destroy them by deleting an edge of the circuit. Suppose we
have deleted k edges. The resulting graph G′ is still connected and has no circuits.
G′ has m = n −1 −k edges. So n = m + p = n −1 −k + 1, implying k = 0. 2
In particular, (d)⇒(a) implies that a graph is connected if and only if it contains
a spanning tree (a spanning subgraph which is a tree).
A digraph is called connected if the underlying undirected graph is connected.
A digraph is a branching if the underlying undirected graph is a forest and each
vertex v has at most one entering edge. A connected branching is an arborescence.
By Theorem 2.4 an arborescence with n vertices has n −1 edges, hence it has
exactly one vertex r with δ−(r) = ∅. This vertex is called its root; we also speak
of an arborescence rooted at r. The vertices v with δ+(v) = ∅are called leaves.
Theorem 2.5.
Let G be a digraph on n vertices. Then the following statements
are equivalent:
(a) G is an arborescence rooted at r (i.e. a connected branching with δ−(r) = ∅).
(b) G is a branching with n −1 edges and δ−(r) = ∅.
(c) G has n −1 edges and every vertex is reachable from r.
(d) Every vertex is reachable from r, but deleting any edge destroys this property.
(e) G is a minimal graph with δ+(X) ̸= ∅for all X ⊂V (G) with r ∈X.
(f) δ−(r) = ∅and there is a unique r-v-path for any v ∈V (G) \ {r}.
(g) δ−(r) = ∅, |δ−(v)| = 1 for all v ∈V (G) \ {r}, and G contains no circuit.
Proof:
(a)⇒(b) and (c)⇒(d) follow from Theorem 2.4.
(b)⇒(c): We have that |δ−(v)| = 1 for all v ∈V (G) \ {r}. So for any v we
have an r-v-path (start at v and always follow the entering edge until r is reached).
(d)⇒(e) is implied by Proposition 2.3(b).
(e)⇒(f): The minimality in (e) implies δ−(r) = ∅. Moreover, by Proposition
2.3(b) there is an r-v-path for all v. Suppose there are two r-v-paths P and Q for
some v. Let e be the last edge of P that does not belong to Q. Then after deleting
e, every vertex is still reachable from r. By Proposition 2.3(b) this contradicts the
minimality in (e).
(f)⇒(g)⇒(a): trivial
2
A cut in an undirected graph G is an edge set of type δ(X) for some ∅̸= X ⊂
V (G). In a digraph G, δ+(X) is a directed cut if ∅̸= X ⊂V (G) and δ−(X) = ∅,
i.e. no edge enters the set X.
We say that an edge set F ⊆E(G) separates two vertices s and t if t is
reachable from s in G but not in (V (G), E(G) \ F). In a digraph, an edge set
δ+(X) with s ∈X and t /∈X is called an s-t-cut. An s-t-cut in an undirected
graph is a cut δ(X) for some X ⊂V (G) with s ∈X and t /∈X. An r-cut in a
digraph is an edge set δ+(X) for some X ⊂V (G) with r ∈X.
By an undirected path, an undirected circuit, and an undirected cut in
a digraph, we mean a subgraph corresponding to a path, a circuit, and a cut,
respectively, in the underlying undirected graph.

2.2 Trees, Circuits, and Cuts
19
Lemma 2.6.
(Minty [1960]) Let G be a digraph and e ∈E(G). Suppose e is
coloured black, while all other edges are coloured red, black or green. Then exactly
one of the following statements holds:
(a) There is an undirected circuit containing e and only red and black edges such
that all black edges have the same orientation.
(b) There is an undirected cut containing e and only green and black edges such
that all black edges have the same orientation.
Proof:
Let e = (x, y). We label the vertices of G by the following procedure.
First label y. In case v is already labelled and w is not, we label w if there is a
black edge (v, w), a red edge (v, w) or a red edge (w, v). In this case, we write
pred(w) := v.
When the labelling procedure stops, there are two possibilities:
Case 1:
x has been labelled. Then the vertices x, pred(x), pred(pred(x)), . . . ,
y form an undirected circuit with the properties (a).
Case 2:
x has not been labelled. Then let R consist of all labelled vertices.
Obviously, the undirected cut δ+(R) ∪δ−(R) has the properties (b).
Suppose that an undirected circuit C as in (a) and an undirected cut δ+(X) ∪
δ−(X) as in (b) both exist. All edges in their (nonempty) intersection are black,
they all have the same orientation with respect to C, and they all leave X or all
enter X. This is a contradiction.
2
A digraph is called strongly connected if there is a path from s to t and a
path from t to s for all s, t ∈V (G). The strongly connected components of a
digraph are the maximal strongly connected subgraphs.
Corollary 2.7.
In a digraph G, each edge belongs either to a (directed) circuit
or to a directed cut. Moreover the following statements are equivalent:
(a) G is strongly connected.
(b) G contains no directed cut.
(c) G is connected and each edge of G belongs to a circuit.
Proof:
The ﬁrst statement follows directly from Minty’s Lemma 2.6 by colouring
all edges black. This also proves (b)⇒(c).
(a)⇒(b) follows from Proposition 2.3(b).
(c)⇒(a): Let r ∈V (G) be an arbitrary vertex. We prove that there is an r-v-
path for each v ∈V (G). Suppose this is not true, then by Proposition 2.3(b) there
is some X ⊂V (G) with r ∈X and δ+(X) = ∅. Since G is connected, we have
δ+(X) ∪δ−(X) ̸= ∅(by Proposition 2.3(a)), so let e ∈δ−(X). But then e cannot
belong to a circuit since no edge leaves X.
2
Corollary 2.7 and Theorem 2.5 imply that a digraph is strongly connected if
and only if it contains for each vertex v a spanning arborescence rooted at v.
A digraph is called acyclic if it contains no (directed) circuit. So by Corollary
2.7 a digraph is acyclic if and only if each edge belongs to a directed cut. Moreover,

20
2. Graphs
a digraph is acyclic if and only if its strongly connected components are the
singletons. The vertices of an acyclic digraph can be ordered in a nice way:
Deﬁnition 2.8.
Let G be a digraph. A topological order of G is an order of the
vertices V (G) = {v1, . . . , vn} such that for each edge (vi, vj) ∈E(G) we have
i < j.
Proposition 2.9.
A digraph has a topological order if and only if it is acyclic.
Proof:
If a digraph has a circuit, it clearly cannot have a topological order. We
show the converse by induction on the number of edges. If there are no edges,
every order is topological. Otherwise let e ∈E(G); by Corollary 2.7 e belongs to
a directed cut δ+(X). Then a topological order of G[X] followed by a topological
order of G −X (both exist by the induction hypothesis) is a topological order of
G.
2
Circuits and cuts also play an important role in algebraic graph theory. For a
graph G we associate a vector space RE(G) whose elements are vectors (xe)e∈E(G)
with |E(G)| real components. Following Berge [1985] we shall now brieﬂy discuss
two linear subspaces which are particularly important.
Let G be a digraph. We associate a vector ζ(C) ∈{−1, 0, 1}E(G) with each
undirected circuit C in G by setting ζ(C)e = 0 for e /∈E(C), and setting ζ(C)e ∈
{−1, 1} for e ∈E(C) such that reorienting all edges e with ζ(C)e = −1 results
in a directed circuit. Similarly, we associate a vector ζ(D) ∈{−1, 0, 1}E(G) with
each undirected cut D = δ(X) in G by setting ζ(D)e = 0 for e /∈D, ζ(D)e = −1
for e ∈δ−(X) and ζ(D)e = 1 for e ∈δ+(X). Note that these vectors are properly
deﬁned only up to multiplication by −1. However, the subspaces of the vector
space RE(G) generated by the set of vectors associated with the undirected circuits
and by the set of vectors associated with the undirected cuts in G are properly
deﬁned; they are called the cycle space and the cocycle space of G, respectively.
Proposition 2.10.
The cycle space and the cocycle space are orthogonal to each
other.
Proof:
Let C be any undirected circuit and D = δ(X) be any undirected cut.
We claim that the scalar product of ζ(C) and ζ(D) is zero. Since reorienting any
edge does not change the scalar product we may assume that D is a directed cut.
But then the result follows from observing that any circuit enters a set X the same
number of times as it leaves X.
2
We shall now show that the sum of the dimensions of the cycle space and the
cocycle space is |E(G)|, the dimension of the whole space. A set of undirected
circuits (undirected cuts) is called a cycle basis (a cocycle basis) if the associated
vectors form a basis of the cycle space (the cocycle space, respectively). Let G be
a graph (directed or undirected) and T a maximal subgraph without an undirected
circuit. For each e ∈E(G) \ E(T ) we call the unique undirected circuit in T + e
the fundamental circuit of e with respect to T . Moreover, for each e ∈E(T )

2.2 Trees, Circuits, and Cuts
21
there is a set X ⊆V (G) with δG(X) ∩E(T ) = {e} (consider a component of
T −e); we call δG(X) the fundamental cut of e with respect to T .
Theorem 2.11.
Let G be a digraph and T a maximal subgraph without an undi-
rected circuit. The |E(G) \ E(T )| fundamental circuits with respect to T form a
cycle basis of G, and the |E(T )| fundamental cuts with respect to T form a cocycle
basis of G.
Proof:
The vectors associated with the fundamental circuits are linearly inde-
pendent since each fundamental circuit contains an element not belonging to any
other. The same holds for the fundamental cuts. Since the vector spaces are or-
thogonal to each other by Proposition 2.10, the sum of their dimensions cannot
exceed |E(G)| = |E(G) \ E(T )| + |E(T )|.
2
The fundamental cuts have a nice property which we shall exploit quite often
and which we shall discuss now. Let T be a digraph whose underlying undirected
graph is a tree. Consider the family F := {Ce : e ∈E(T )}, where for e = (x, y) ∈
E(T ) we denote by Ce the connected component of T −e containing y (so δ(Ce)
is the fundamental cut of e with respect to T ). If T is an arborescence, then any
two elements of F are either disjoint or one is a subset of the other. In general F
is at least cross-free:
Deﬁnition 2.12.
A set system is a pair (U, F), where U is a nonempty ﬁnite set
and F a family of subsets of U. (U, F) is cross-free if for any two sets X, Y ∈F,
at least one of the four sets X \ Y, Y \ X, X ∩Y, U \ (X ∪Y) is empty. (U, F) is
laminar if for any two sets X, Y ∈F, at least one of the three sets X \ Y, Y \ X,
X ∩Y is empty.
In the literature set systems are also known as hypergraphs. See Figure 2.1(a)
for an illustration of the laminar family {{a}, {b, c}, {a, b, c}, {a, b, c, d}, { f },
{ f, g}}. Another word used for laminar is nested.
a
b
c
d
e
f
g
a
b, c
d
e
f
g
(a)
(b)
Fig. 2.1.

22
2. Graphs
Whether a set system (U, F) is laminar does not depend on U, so we some-
times simply say that F is a laminar family. However, whether a set system is
cross-free can depend on the ground set U. If U contains an element that does
not belong to any set of F, then F is cross-free if and only if it is laminar. Let
r ∈U be arbitrary. It follows directly from the deﬁnition that a set system (U, F)
is cross-free if and only if
F′ := {X ∈F : r ̸∈X} ∪{U \ X : X ∈F, r ∈X}
is laminar. Hence cross-free families are sometimes depicted similarly to laminar
families: for example, Figure 2.2(a) shows the cross-free family {{b, c, d, e, f }, {c},
{a, b, c}, {e}, {a, b, c, d, f }, {e, f }}; a square corresponds to the set containing all
elements outside.
a
b
c
d
e
f
a
b
c
d
e
f
(a)
(b)
Fig. 2.2.
While oriented trees lead to cross-free families the converse is also true: every
cross-free family can be represented by a tree in the following sense:
Deﬁnition 2.13.
Let T be a digraph such that the underlying undirected graph is
a tree. Let U be a ﬁnite set and ϕ : U →V (T ). Let F := {Se : e ∈E(T )}, where
for e = (x, y) we deﬁne
Se := {s ∈U : ϕ(s) is in the same connected component of T −e as y}.
Then (T, ϕ) is called a tree-representation of (U, F).
See Figures 2.1(b) and 2.2(b) for examples.
Proposition 2.14.
Let (U, F) be a set system with a tree-representation (T, ϕ).
Then (U, F) is cross-free. If T is an arborescence, then (U, F) is laminar. More-
over, every cross-free family has a tree-representation, and for laminar families,
an arborescence can be chosen as T .

2.2 Trees, Circuits, and Cuts
23
Proof:
If (T, ϕ) is a tree-representation of (U, F) and e = (v, w), f = (x, y) ∈
E(T ), we have an undirected v-x-path P in T (ignoring the orientations). There
are four cases: If w, y /∈V (P) then Se ∩Sf = ∅(since T contains no circuit).
If w /∈V (P) and y ∈V (P) then Se ⊆Sf . If y /∈V (P) and w ∈V (P) then
Sf ⊆Se. If w, y ∈V (P) then Se ∪Sf = U. Hence (U, F) is cross-free. If T is an
arborescence, the last case cannot occur (otherwise at least one vertex of P would
have two entering edges), so F is laminar.
To prove the converse, let F ﬁrst be a laminar family. We deﬁne V (T ) :=
F
.
∪{r},
E′ := {(X, Y) ∈F × F : X ⊃Y ̸= ∅and there is no Z ∈F with X ⊃Z ⊃Y}
and E(T ) := E′∪{(r, X) : X is a maximal element of F}. If ∅∈F and F ̸= {∅},
we choose a minimal nonempty element X ∈F arbitrarily and add the edge (X, ∅)
to E(T ). We set ϕ(x) := X, where X is the minimal set in F containing x, and
ϕ(x) := r if no set in F contains x. Obviously, T is an arborescence rooted at r,
and (T, ϕ) is a tree-representation of F.
Now let F be a cross-free family of subsets of U. Let r ∈U. As noted above,
F′ := {X ∈F : r ̸∈X} ∪{U \ X : X ∈F, r ∈X}
is laminar, so let (T, ϕ) be a tree-representation of (U, F′). Now for an edge
e ∈E(T ) there are three cases: If Se ∈F and U \ Se ∈F, we replace the edge
e = (x, y) by two edges (x, z) and (y, z), where z is a new vertex. If Se ̸∈F and
U \ Se ∈F, we replace the edge e = (x, y) by (y, x). If Se ∈F and U \ Se ̸∈F,
we do nothing. Let T ′ be the resulting graph. Then (T ′, ϕ) is a tree-representation
of (U, F).
2
The above result is mentioned by Edmonds and Giles [1977] but was probably
known earlier.
Corollary 2.15.
A laminar family of distinct subsets of U has at most 2|U| ele-
ments. A cross-free family of distinct subsets of U has at most 4|U| −2 elements.
Proof:
We ﬁrst consider a laminar family F of distinct nonempty proper subsets
of U. We prove that |F| ≤2|U| −2. Let (T, ϕ) be a tree-representation, where
T is an arborescence whose number of vertices is as small as possible. For every
w ∈V (T ) we have either |δ+(w)| ≥2 or there exists an x ∈U with ϕ(x) = w
or both. (For the root this follows from U /∈F, for the leaves from ∅/∈F, for all
other vertices from the minimality of T .)
There can be at most |U| vertices w with ϕ(x) = w for some x ∈U and at
most

|E(T )|
2

vertices w with |δ+(w)| ≥2. So |E(T )|+1 = |V (T )| ≤|U|+ |E(T )|
2
and thus |F| = |E(T )| ≤2|U| −2.
Now let (U, F) be a cross-free family with ∅, U /∈F, and let r ∈U. Since
F′ := {X ∈F : r ̸∈X} ∪{U \ X : X ∈F, r ∈X}
is laminar, we have |F′| ≤2|U| −2. Hence |F| ≤2|F′| ≤4|U| −4. The proof
is concluded by taking ∅and U as possible members of F into account.
2

24
2. Graphs
2.3 Connectivity
Connectivity is a very important concept in graph theory. For many problems it
sufﬁces to consider connected graphs, since otherwise we can solve the problem
for each connected component separately. So it is a fundamental task to detect the
connected components of a graph. The following simple algorithm ﬁnds a path
from a speciﬁed vertex s to all other vertices that are reachable from s. It works
for both directed and undirected graphs. In the undirected case it builds a maximal
tree containing s; in the directed case it constructs a maximal arborescence rooted
at s.
Graph Scanning Algorithm
Input:
A graph G (directed or undirected) and some vertex s.
Output:
The set R of vertices reachable from s, and a set T ⊆E(G) such
that (R, T ) is an arborescence rooted at s, or a tree.
1⃝
Set R := {s}, Q := {s} and T := ∅.
2⃝
If Q = ∅then stop,
else choose a v ∈Q.
3⃝
Choose a w ∈V (G) \ R with e = (v, w) ∈E(G) or e = {v, w} ∈E(G).
If there is no such w then set Q := Q \ {v} and go to 2⃝.
4⃝
Set R := R ∪{w}, Q := Q ∪{w} and T := T ∪{e}. Go to 2⃝.
Proposition 2.16.
The Graph Scanning Algorithm works correctly.
Proof:
At any time, (R, T ) is a tree or an arborescence rooted at s. Suppose
at the end there is a vertex w ∈V (G) \ R that is reachable from s. Let P be
an s-w-path, and let {x, y} or (x, y) be an edge of P with x ∈R and y /∈R.
Since x has been added to R, it also has been added to Q at some time during the
execution of the algorithm. The algorithm does not stop before removing x from
Q. But this is done in 3⃝only if there is no edge {x, y} or (x, y) with y /∈R. 2
Since this is the ﬁrst graph algorithm in this book we discuss some imple-
mentation issues. The ﬁrst question is how the graph is given. There are several
natural ways. For example, one can think of a matrix with a row for each vertex
and a column for each edge. The incidence matrix of an undirected graph G is
the matrix A = (av,e)v∈V (G), e∈E(G) where
av,e =
 1
if v ∈e
0
if v ̸∈e .
The incidence matrix of a digraph G is the matrix A = (av,e)v∈V (G), e∈E(G) where
av,(x,y) =
 −1
if v = x
1
if v = y
0
if v ̸∈{x, y}
.

2.3 Connectivity
25
Of course this is not very efﬁcient since each column contains only two nonzero
entries. The space needed for storing an incidence matrix is obviously O(nm),
where n := |V (G)| and m := |E(G)|.
A better way seems to be having a matrix whose rows and columns are indexed
by the vertex set. The adjacency matrix of a simple graph G is the 0-1-matrix
A = (av,w)v,w∈V (G) with av,w = 1 iff {v, w} ∈E(G) or (v, w) ∈E(G). For graphs
with parallel edges we can deﬁne av,w to be the number of edges from v to w.
An adjacency matrix requires O(n2) space for simple graphs.
The adjacency matrix is appropriate if the graph is dense, i.e. has (n2) edges
(or more). For sparse graphs, say with O(n) edges only, one can do much better.
Besides storing the number of vertices we can simply store a list of the edges, for
each edge noting its endpoints. If we address each vertex by a number from 1 to
n, the space needed for each edge is O(log n). Hence we need O(m log n) space
altogether.
Just storing the edges in an arbitrary order is not very convenient. Almost all
graph algorithms require ﬁnding the edges incident to a given vertex. Thus one
should have a list of incident edges for each vertex. In case of directed graphs,
two lists, one for entering edges and one for leaving edges, are appropriate. This
data structure is called adjacency list; it is the most customary one for graphs.
For direct access to the list(s) of each vertex we have pointers to the heads of all
lists; these can be stored with O(n log m) additional bits. Hence the total number
of bits required for an adjacency list is O(n log m + m log n).
Whenever a graph is part of the input of an algorithm in this book, we assume
that the graph is given by an adjacency list.
As for elementary operations on numbers (see Section 1.2), we assume that
elementary operations on vertices and edges take constant time only. This includes
scanning an edge, identifying its ends and accessing the head of the adjacency list
for a vertex. The running time will be measured by the parameters n and m, and
an algorithm running in O(m + n) time is called linear.
We shall always use the letters n and m for the number of vertices and the
number of edges. For many graph algorithms it causes no loss of generality to
assume that the graph at hand is simple and connected; hence n −1 ≤m < n2.
Among parallel edges we often have to consider only one, and different connected
components can often be analyzed separately. The preprocessing can be done in
linear time in advance; see Exercise 13 and the following.
We can now analyze the running time of the Graph Scanning Algorithm:
Proposition 2.17.
The Graph Scanning Algorithm can be implemented to run
in O(m+n) time. The connected components of a graph can be determined in linear
time.
Proof:
We assume that G is given by an adjacency list. For each vertex x we
introduce a pointer current(x), indicating the current edge in the list containing
all edges in δ(x) or δ+(x) (this list is part of the input). Initially current(x) is set
to the ﬁrst element of the list. In 3⃝, the pointer moves forward. When the end of

26
2. Graphs
the list is reached, x is removed from Q and will never be inserted again. So the
overall running time is proportional to the number of vertices plus the number of
edges, i.e. O(n + m).
To identify the connected components of a graph, we apply the algorithm
once and check if R = V (G). If so, the graph is connected. Otherwise R is a
connected component, and we apply the algorithm to (G, s′) for an arbitrary vertex
s′ ∈V (G) \ R (and iterate until all vertices have been scanned, i.e. added to R).
Again, no edge is scanned twice, so the overall running time remains linear.
2
An interesting question is in which order the vertices are chosen in 3⃝. Obvi-
ously we cannot say much about this order if we do not specify how to choose
a v ∈Q in 2⃝. Two methods are frequently used; they are called Depth-First
Search (DFS) and Breadth-First Search (BFS). In DFS we choose the v ∈Q
that was the last to enter Q. In other words, Q is implemented as a LIFO-stack
(last-in-ﬁrst-out). In BFS we choose the v ∈Q that was the ﬁrst to enter Q. Here
Q is implemented by a FIFO-queue (ﬁrst-in-ﬁrst-out).
An algorithm similar to DFS has been described already before 1900 by
Tr´emaux and Tarry; see K¨onig [1936]. BFS seems to have been mentioned ﬁrst
by Moore [1959]. Trees (in the directed case: arborescences) (R, T ) computed by
DFS and BFS are called DFS-tree and BFS-tree, respectively. For BFS-trees we
note the following important property:
Proposition 2.18.
A BFS-tree contains a shortest path from s to each vertex
reachable from s. The values distG(s, v) for all v ∈V (G) can be determined
in linear time.
Proof:
We apply BFS to (G, s) and add two statements: initially (in 1⃝of the
Graph Scanning Algorithm) we set l(s) := 0, and in 4⃝we set l(w) := l(v)+1.
We obviously have that l(v) = dist(R,T )(s, v) for all v ∈R, at any stage of the
algorithm. Moreover, if v is the currently scanned vertex (chosen in 2⃝), at this
time there is no vertex w ∈R with l(w) > l(v) + 1 (because the vertices are
scanned in an order with nondecreasing l-values).
Suppose that when the algorithm terminates there is a vertex w ∈V (G)
with distG(s, w) < dist(R,T )(s, w); let w have minimum distance from s in G
with this property. Let P be a shortest s-w-path in G, and let e = (v, w) or
e = {v, w} be the last edge in P. We have distG(s, v) = dist(R,T )(s, v), but e does
not belong to T . Moreover, l(w) = dist(R,T )(s, w) > distG(s, w) = distG(s, v) +
1 = dist(R,T )(s, v) + 1 = l(v) + 1. This inequality combined with the above
observation proves that w did not belong to R when v was removed from Q. But
this contradicts 3⃝because of edge e.
2
This result will also follow from the correctness of Dijkstra’s Algorithm
for the Shortest Path Problem, which can be thought of as a generalization of
BFS to the case where we have nonnegative weights on the edges (see Section
7.1).

2.3 Connectivity
27
We now show how to identify the strongly connected components of a digraph.
Of course, this can easily be done by using n times DFS (or BFS). However, it
is possible to ﬁnd the strongly connected components by visiting every edge only
twice:
Strongly Connected Component Algorithm
Input:
A digraph G.
Output:
A function comp : V (G) →N indicating the membership of the
strongly connected components.
1⃝
Set R := ∅. Set N := 0.
2⃝
For all v ∈V (G) do: If v /∈R then Visit1(v).
3⃝
Set R := ∅. Set K := 0.
4⃝
For i := |V (G)| down to 1 do:
If ψ−1(i) /∈R then set K := K + 1 and Visit2(ψ−1(i)).
Visit1(v)
1⃝
Set R := R ∪{v}.
2⃝
For all w ∈V (G) \ R with (v, w) ∈E(G) do Visit1(w).
3⃝
Set N := N + 1, ψ(v) := N and ψ−1(N) := v.
Visit2(v)
1⃝
Set R := R ∪{v}.
2⃝
For all w ∈V (G) \ R with (w, v) ∈E(G) do Visit2(w).
3⃝
Set comp(v) := K.
Figure 2.3 shows an example: The ﬁrst DFS scans the vertices in the order
a, g, b, d, e, f and produces the arborescence shown in the middle; the numbers
are the ψ-labels. Vertex c is the only one that is not reachable from a; it gets the
highest label ψ(c) = 7. The second DFS starts with c but cannot reach any other
vertex via a reverse edge. So it proceeds with vertex a because ψ(a) = 6. Now
b, g and f can be reached. Finally e is reached from d. The strongly connected
components are {c}, {a, b, f, g} and {d, e}.
In summary, one DFS is needed to ﬁnd an appropriate numbering, while in
the second DFS the reverse graph is considered and the vertices are processed
in decreasing order with respect to this numbering. Each connected component of
the second DFS-forest is an anti-arborescence, a graph arising from an arbores-
cence by reversing every edge. We show that these anti-arborescences identify the
strongly connected components.

28
2. Graphs
b
b
1
b
c
c
7
c
a
a
6
a
d
d
3
d
e
2
e
e
f
f 4
f
g
g
5
g
Fig. 2.3.
Theorem 2.19.
The Strongly Connected Component Algorithm identiﬁes
the strongly connected components correctly in linear time.
Proof:
The running time is obviously O(n + m). Of course, vertices of the
same strongly connected component are always in the same component of any
DFS-forest, so they get the same comp-value. We have to prove that two vertices
u and v with comp(u) = comp(v) indeed lie in the same strongly connected
component. Let r(u) and r(v) be the vertex reachable from u and v with the
highest ψ-label, respectively. Since comp(u) = comp(v), i.e. u and v lie in the
same anti-arborescence of the second DFS-forest, r := r(u) = r(v) is the root of
this anti-arborescence. So r is reachable from both u and v.
Since r is reachable from u and ψ(r) ≥ψ(u), r has not been added to R
after u in the ﬁrst DFS, and the ﬁrst DFS-forest contains an r-u-path. In other
words, u is reachable from r. Analogously, v is reachable from r. Altogether, u is
reachable from v and vice versa, proving that indeed u and v belong to the same
strongly connected component.
2
It is interesting that this algorithm also solves another problem: ﬁnding a
topological order of an acyclic digraph. Observe that contracting the strongly
connected components of any digraph yields an acyclic digraph. By Proposition
2.9 this acyclic digraph has a topological order. In fact, such an order is given
by the numbers comp(v) computed by the Strongly Connected Component
Algorithm:
Theorem 2.20.
The Strongly Connected Component Algorithm determines
a topological order of the digraph resulting from contracting each strongly con-
nected component of G. In particular, we can for any given digraph either ﬁnd a
topological order or decide that none exists in linear time.
Proof:
Let X and Y be two strongly connected components of a digraph
G, and suppose the Strongly Connected Component Algorithm computes
comp(x) = k1 for x ∈X and comp(y) = k2 for y ∈Y with k1 < k2. We claim
that E+
G(Y, X) = ∅.
Suppose that there is an edge (y, x) ∈E(G) with y ∈Y and x ∈X. All
vertices in X are added to R in the second DFS before the ﬁrst vertex of Y is

2.3 Connectivity
29
added. In particular we have x ∈R and y /∈R when the edge (y, x) is scanned
in the second DFS. But this means that y is added to R before K is incremented,
contradicting comp(y) ̸= comp(x).
Hence the comp-values computed by the Strongly Connected Component
Algorithm determine a topological order of the digraph resulting from contracting
the strongly connected components. The second statement of the theorem now
follows from Proposition 2.9 and the observation that a digraph is acyclic if and
only if its strongly connected components are the singletons.
2
The ﬁrst linear-time algorithm that identiﬁes the strongly connected compo-
nents was given by Tarjan [1972]. The problem of ﬁnding a topological order (or
deciding that none exists) was solved earlier (Kahn [1962], Knuth [1968]). Both
BFS and DFS occur as subroutines in many other combinatorial algorithms. Some
examples will reappear in later chapters.
Sometimes one is interested in higher connectivity. Let k ≥2. An undirected
graph with more than k vertices and the property that it remains connected even
if we delete any k −1 vertices, is called k-connected. A graph with at least two
vertices is k-edge-connected if it remains connected after deleting any k−1 edges.
So a connected graph with at least three vertices is 2-connected (2-edge-connected)
if and only if it has no articulation vertex (no bridge, respectively).
The largest k and l such that a graph G is k-connected and l-edge-connected
are called the vertex-connectivity and edge-connectivity of G. Here we say that
a graph is 1-connected (and 1-edge-connected) if it is connected. A disconnected
graph has vertex-connectivity and edge-connectivity zero.
The blocks of an undirected graph are its maximal connected subgraphs with-
out articulation vertex. Thus each block is either a maximal 2-connected subgraph,
or consists of a bridge or an isolated vertex. Two blocks have at most one vertex in
common, and a vertex belonging to more than one block is an articulation vertex.
The blocks of an undirected graph can be determined in linear time quite similarly
to the Strongly Connected Component Algorithm; see Exercise 16. Here we
prove a nice structure theorem for 2-connected graphs. We construct graphs from
a single vertex by sequentially adding ears:
Deﬁnition 2.21.
Let G be a graph (directed or undirected). An ear-decomposition
of G is a sequence r, P1, . . . , Pk with G = ({r}, ∅)+P1+· · ·+Pk, such that each Pi
is either a path where exactly the endpoints belong to {r} ∪V (P1) ∪· · · ∪V (Pi−1),
or a circuit where exactly one of its vertices belongs to {r}∪V (P1)∪· · ·∪V (Pi−1)
(i ∈{1, . . . , k}).
P1, . . . , Pk are called ears. If k ≥1, P1 is a circuit of length at least three, and
P2, . . . , Pk are paths, then the ear-decomposition is called proper.
Theorem 2.22.
(Whitney [1932]) An undirected graph is 2-connected if and only
if it has a proper ear-decomposition.
Proof:
Evidently a circuit of length at least three is 2-connected. Moreover, if G
is 2-connected, then so is G + P, where P is an x-y-path, x, y ∈V (G) and x ̸= y:

30
2. Graphs
deleting any vertex does not destroy connectivity. We conclude that a graph with
a proper ear-decomposition is 2-connected.
To show the converse, let G be a 2-connected graph. Let G′ be the maximal
simple subgraph of G; evidently G′ is also 2-connected. Hence G′ cannot be a
tree; i.e. it contains a circuit. Since it is simple, G′, and thus G, contains a circuit
of length at least three. So let H be a maximal subgraph of G that has a proper
ear-decomposition; H exists by the above consideration.
Suppose H is not spanning. Since G is connected, we then know that there
exists an edge e = {x, y} ∈E(G) with x ∈V (H) and y /∈V (H). Let z be a
vertex in V (H) \ {x}. Since G −x is connected, there exists a path P from y
to z in G −x. Let z′ be the ﬁrst vertex on this path, when traversed from y,
that belongs to V (H). Then P[y,z′] + e can be added as an ear, contradicting the
maximality of H.
Thus H is spanning. Since each edge of E(G)\ E(H) can be added as an ear,
we conclude that H = G.
2
See Exercise 17 for similar characterizations of 2-edge-connected graphs and
strongly connected digraphs.
2.4 Eulerian and Bipartite Graphs
Euler’s work on the problem of traversing each of the seven bridges of K¨onigsberg
exactly once was the origin of graph theory. He showed that the problem had
no solution by deﬁning a graph, asking for a walk containing all edges, and
observing that more than two vertices had odd degree. Although Euler neither
proved sufﬁciency nor considered the case explicitly in which we ask for a closed
walk, the following result is usually attributed to him.
Deﬁnition 2.23.
An Eulerian walk in a graph G is a closed walk containing
every edge. An undirected graph G is called Eulerian if the degree of each vertex
is even. A digraph G is Eulerian if |δ−(v)| = |δ+(v)| for each v ∈V (G).
Theorem 2.24.
(Euler [1736], Hierholzer [1873])
A connected graph has an
Eulerian walk if and only if it is Eulerian.
Proof:
The necessity of the degree conditions is obvious, the sufﬁciency is
proved by the following algorithm (Theorem 2.25).
2
The algorithm accepts as input only connected Eulerian graphs. Note that one
can check in linear time whether a given graph is connected (Theorem 2.17)
and Eulerian (trivial). The algorithm ﬁrst chooses an initial vertex, then calls a
recursive procedure. We ﬁrst describe it for undirected graphs:
Euler’s Algorithm
Input:
An undirected connected Eulerian graph G.
Output:
An Eulerian walk W in G.

2.4 Eulerian and Bipartite Graphs
31
1⃝
Choose v1 ∈V (G) arbitrarily. Return W := Euler(G, v1).
Euler(G, v1)
1⃝
Set W := v1 and x := v1.
2⃝
If δ(x) = ∅then go to 4⃝.
Else let e ∈δ(x), say e = {x, y}.
3⃝
Set W := W, e, y and x := y. Set E(G) := E(G) \ {e} and go to 2⃝.
4⃝
Let v1, e1, v2, e2, . . . , vk, ek, vk+1 be the sequence W.
For i := 1 to k do: Set Wi := Euler(G, vi).
5⃝
Set W := W1, e1, W2, e2, . . . , Wk, ek, vk+1. Return W.
For digraphs, 2⃝has to be replaced by:
2⃝
If δ+(x) = ∅then go to 4⃝.
Else let e ∈δ+(x), say e = (x, y).
Theorem 2.25.
Euler’s Algorithm works correctly. Its running time is O(m +
n), where n = |V (G)| and m = |E(G)|.
Proof:
We use induction on |E(G)|, the case E(G) = ∅being trivial.
Because of the degree conditions, vk+1 = x = v1 when 4⃝is executed. So at
this stage W is a closed walk. Let G′ be the graph G at this stage. G′ also satisﬁes
the degree constraints.
For each edge e ∈E(G′) there exists a minimum i ∈{1, . . . , k} such that e is
in the same connected component of G′ as vi. Then by the induction hypothesis
e belongs to Wi. So the closed walk W composed in 5⃝is indeed Eulerian.
The running time is linear, because each edge is deleted immediately after
being examined.
2
Euler’s Algorithm will be used several times as a subroutine in later chap-
ters. Sometimes one is interested in making a given graph Eulerian by adding or
contracting edges. Let G be an undirected graph and F a family of unordered
pairs of V (G) (edges or not). F is called an odd join if (V (G), E(G)
.
∪F) is
Eulerian. F is called an odd cover if the graph which results from G by suc-
cessively contracting each e ∈F is Eulerian. Both concepts are equivalent in the
following sense.
Theorem 2.26.
(Aoshima and Iri [1977]) Let G be an undirected graph.
(a) Every odd join is an odd cover.
(b) Every minimal odd cover is an odd join.
Proof:
To prove (a), let F be an odd join. We build a graph G′ by contracting the
connected components of (V (G), F) in G. Each of these connected components
contains an even number of odd-degree vertices (with respect to F and thus with

32
2. Graphs
respect to G, because F is an odd join). So the resulting graph has even degrees
only. Thus F is an odd cover.
To prove (b), let F be a minimal odd cover. Because of the minimality,
(V (G), F) is a forest. We have to show that |δF(v)| ≡|δG(v)|
(mod 2) for
each v ∈V (G). So let v ∈V (G). Let C1, . . . , Ck be the connected components
of (V (G), F) −v that contain a vertex w with {v, w} ∈F. Since F is a forest,
k = |δF(v)|.
As F is an odd cover, contracting X := V (C1)∪· · ·∪V (Ck)∪{v} in G yields
a vertex of even degree, i.e. |δG(X)| is even. On the other hand, because of the
minimality of F, F \ {{v, w}} is not an odd cover (for any w with {v, w} ∈F),
so |δG(V (Ci))| is odd for i = 1, . . . , k. Since
k

i=1
|δG(V (Ci))| = |δG(X)|+|δG(v)|−2|EG({v}, V (G)\X)|+2

1≤i< j≤k
|EG(Ci, Cj)|,
we conclude that k has the same parity as |δG(v)|.
2
We shall return to the problem of making a graph Eulerian in Section 12.2.
A bipartition of an undirected graph G is a partition of the vertex set V (G) =
A
.
∪B such that the subgraphs induced by A and B are both empty. A graph is
called bipartite if it has a bipartition. The simple bipartite graph G with V (G) =
A
.
∪B, |A| = n, |B| = m and E(G) = {{a, b} : a ∈A, b ∈B} is denoted by
Kn,m (the complete bipartite graph). When we write G = (A
.
∪B, E(G)), we
mean that G[A] and G[B] are both empty.
Proposition 2.27.
(K¨onig [1916]) An undirected graph is bipartite if and only if
it contains no circuit of odd length. There is a linear-time algorithm which, given
an undirected graph G, either ﬁnds a bipartition or an odd circuit.
Proof:
Suppose G is bipartite with bipartition V (G) = A
.
∪B, and the closed
walk v1, e1, v2, . . . , vk, ek, vk+1 deﬁnes some circuit in G. W.l.o.g. v1 ∈A. But
then v2 ∈B, v3 ∈A, and so on. We conclude that vi ∈A if and only if i is odd.
But vk+1 = v1 ∈A, so k must be even.
To prove the sufﬁciency, we may assume that G is connected, since a graph
is bipartite iff each connected component is (and the connected components can
be determined in linear time; Proposition 2.17). We choose an arbitrary vertex
s ∈V (G) and apply BFS to (G, s) in order to obtain the distances from s to v
for all v ∈V (G) (see Proposition 2.18). Let T be the resulting BFS-tree. Deﬁne
A := {v ∈V (G) : distG(s, v) is even} and B := V (G) \ A.
If there is an edge e = {x, y} in G[A] or G[B], the x-y-path in T together
with e forms an odd circuit in G. If there is no such edge, we have a bipartition.
2

2.5 Planarity
33
2.5 Planarity
We often draw graphs in the plane. A graph is called planar if it can be drawn such
that no pair of edges intersect. To formalize this concept we need the following
topological terms:
Deﬁnition 2.28.
A simple Jordan curve is the image of a continuous injective
function ϕ : [0, 1] →R2; its endpoints are ϕ(0) and ϕ(1). A closed Jordan curve
is the image of a continuous function ϕ : [0, 1] →R2 with ϕ(0) = ϕ(1) and
ϕ(τ) ̸= ϕ(τ ′) for 0 ≤τ < τ ′ < 1. A polygonal arc is a simple Jordan curve
which is the union of ﬁnitely many intervals (straight line segments). A polygon is
a closed Jordan curve which is the union of ﬁnitely many intervals.
Let R = R2 \ J, where J is the union of ﬁnitely many intervals. We deﬁne
the connected regions of R as equivalence classes where two points in R are
equivalent if they can be joined by a polygonal arc within R.
Deﬁnition 2.29.
A planar embedding of a graph G consists of an injective map-
ping ψ : V (G) →R2 and for each e = {x, y} ∈E(G) a polygonal arc Je with
endpoints ψ(x) and ψ(y), such that for each e = {x, y} ∈E(G):
(Je \ {ψ(x), ψ(y)}) ∩
⎛
⎝{ψ(v) : v ∈V (G)} ∪

e′∈E(G)\{e}
Je′
⎞
⎠= ∅.
A graph is called planar if it has a planar embedding.
Let G be a (planar) graph with some ﬁxed planar embedding 	 = (ψ,
(Je)e∈E(G)). After removing the points and polygonal arcs from the plane, the re-
mainder,
R := R2 \
⎛
⎝{ψ(v) : v ∈V (G)} ∪

e∈E(G)
Je
⎞
⎠,
splits into open connected regions, called faces of 	.
For example, K4 is obviously planar but it will turn out that K5 is not planar.
Exercise 23 shows that restricting ourselves to polygonal arcs instead of arbitrary
Jordan curves makes no substantial difference. We will show later that for simple
graphs it is indeed sufﬁcient to consider straight line segments only.
Our aim is to characterize planar graphs. Following Thomassen [1981], we
ﬁrst prove the following topological fact, a version of the Jordan curve theorem:
Theorem 2.30.
If J is a polygon, then R2 \ J splits into exactly two connected
regions, each of which has J as its boundary. If J is a polygonal arc, then R2 \ J
has only one connected region.
Proof:
Let J be a polygon, p ∈R2 \ J and q ∈J. Then there exists a polygonal
arc in (R2 \ J)∪{q} joining p and q: starting from p, one follows the straight line
towards q until one gets close to J, then one proceeds within the vicinity of J.

34
2. Graphs
(We use the elementary topological fact that disjoint compact sets have a positive
distance from each other.) We conclude that p is in the same connected region of
R2 \ J as points arbitrarily close to q.
J is the union of ﬁnitely many intervals; one or two of these intervals contain
q. Let ϵ > 0 such that the ball with center q and radius ϵ contains no other interval;
then clearly this ball intersects at most two connected regions. Since p ∈R2 \ J
and q ∈J were chosen arbitrarily, we conclude that there are at most two regions
and each region has J as its boundary.
Since the above also holds if J is a polygonal arc and q is an endpoint of J,
R2 \ J has only one connected region in this case.
Returning to the case when J is a polygon, it remains to prove that R2 \ J
has more than one region. For any p ∈R2 \ J and any angle α we consider the
ray lα starting at p with angle α. J ∩lα is a set of points or closed intervals. Let
cr(p,lα) be the number of these points or intervals that J enters from a different
side of lα than to which it leaves (the number of times J “crosses” lα; e.g. in
Figure 2.4 we have cr(p,lα) = 2).
p
lα
J
J
J
Fig. 2.4.
Note that for any angle α,

lim
ϵ→0, ϵ>0 cr(p,lα+ϵ) −cr(p,lα)
 +

lim
ϵ→0, ϵ<0 cr(p,lα+ϵ) −cr(p,lα)

is twice the number of intervals of J ∩lα that J enters from the same side as to
which it leaves. Therefore g(p, α) := (cr(p,lα) mod 2) is a continuous function
in α, so it is constant and we denote it by g(p). Clearly g(p) is constant for points
p on each straight line not intersecting J, so it is constant within each region.
However, g(p) ̸= g(q) for points p, q such that the straight line segment joining
p and q intersects J exactly once. Hence there are indeed two regions.
2
Exactly one of the faces, the outer face, is unbounded.
Proposition 2.31.
Let G be a 2-connected graph with a planar embedding 	.
Then every face is bounded by a circuit, and every edge is on the boundary of
exactly two faces. Moreover, the number of faces is |E(G)| −|V (G)| + 2.

2.5 Planarity
35
Proof:
By Theorem 2.30 both assertions are true if G is a circuit. For general
2-connected graphs we use induction on the number of edges, using Theorem 2.22.
Consider a proper ear-decomposition of G, and let P be the last ear, a path with
endpoints x and y, say. Let G′ be the graph before adding the last ear, and let 	′
be the restriction of 	 to G′.
Let 	 = (ψ, (Je)e∈E(G)). Let F′ be the face of 	′ containing 
e∈E(P) Je \
{ψ(x), ψ(y)}. By induction, F′ is bounded by a circuit C. C contains x and y, so
C is the union of two x-y-paths Q1, Q2 in G′. Now we apply Theorem 2.30 to
each of the circuits Q1 + P and Q2 + P. We conclude that
F′ ∪{ψ(x), ψ(y)} = F1
.
∪F2
.
∪

e∈E(P)
Je
and F1 and F2 are two faces of G bounded by the circuits Q1 + P and Q2 + P,
respectively. Hence G has one more face than G′. Using |E(G) \ E(G′)| =
|V (G) \ V (G′)| + 1, this completes the induction step.
2
This proof is due to Tutte. It also implies easily that the circuits bounding the
ﬁnite faces constitute a cycle basis (Exercise 24). The last statement of Proposition
2.31 is known as Euler’s formula; it holds for general connected graphs:
Theorem 2.32.
(Euler [1758], Legendre [1794])
For any planar connected
graph G with any embedding, the number of faces is |E(G)| −|V (G)| + 2.
Proof:
We have already proved the statement for 2-connected graphs (Propo-
sition 2.31). Moreover, the assertion is trivial if |V (G)| = 1 and follows from
Theorem 2.30 if |E(G)| = 1. If |V (G)| = 2 and |E(G)| ≥2, then we can sub-
divide one edge e, thereby increasing the number of vertices and the number of
edges by one and making the graph 2-connected, and apply Proposition 2.31.
So we may now assume that G has an articulation vertex x; we proceed by
induction on the number of vertices. Let 	 be an embedding of G. Let C1, . . . , Ck
be the connected components of G −x; and let 	i be the restriction of 	 to
Gi := G[V (Ci) ∪{x}] for i = 1, . . . , k.
The set of inner (bounded) faces of 	 is the disjoint union of the sets of
inner faces of 	i, i = 1, . . . , k. By applying the induction hypothesis to (Gi, 	i),
i = 1, . . . , k, we get that the total number of inner faces of (G, 	) is
k

i=1
(|E(Gi)|−|V (Gi)|+1) = |E(G)|−
k

i=1
|V (Gi)\{x}| = |E(G)|−|V (G)|+1.
Taking the outer face into account concludes the proof.
2
In particular, the number of faces is independent of the embedding. The average
degree of a simple planar graph is less than 6:
Corollary 2.33.
Let G be a 2-connected simple planar graph whose minimum
circuit has length k (we also say that G has girth k). Then G has at most (n−2)
k
k−2
edges. Any simple planar graph with n ≥3 vertices has at most 3n −6 edges.

36
2. Graphs
Proof:
First assume that G is 2-connected. Let some embedding 	 of G be
given, and let r be the number of faces. By Euler’s formula (Theorem 2.32),
r = |E(G)|−|V (G)|+2. By Proposition 2.31, each face is bounded by a circuit, i.e.
by at least k edges, and each edge is on the boundary of exactly two faces. Hence
kr ≤2|E(G)|. Combining the two results we get |E(G)|−|V (G)|+2 ≤2
k |E(G)|,
implying |E(G)| ≤(n −2)
k
k−2.
If G is not 2-connected we add edges between non-adjacent vertices to make it
2-connected while preserving planarity. By the ﬁrst part we have at most (n−2)
3
3−2
edges, including the new ones.
2
Now we show that certain graphs are non-planar:
Corollary 2.34.
Neither K5 nor K3,3 is planar.
Proof:
This follows directly from Corollary 2.33: K5 has ﬁve vertices but 10 >
3·5−6 edges; K3,3 is 2-connected, has girth 4 (as it is bipartite) and 9 > (6−2)
4
4−2
edges.
2
Fig. 2.5.
Figure 2.5 shows these two graphs, which are the smallest non-planar graphs.
We shall prove that every non-planar graph contains, in a certain sense, K5 or
K3,3. To make this precise we need the following notion:
Deﬁnition 2.35.
Let G and H be two undirected graphs. G is a minor of H if
there exists a subgraph H ′ of H and a partition V (H ′) = V1
.
∪· · ·
.
∪Vk of its
vertex set into connected subsets such that contracting each of V1, . . . , Vk yields a
graph which is isomorphic to G.
In other words, G is a minor of H if it can be obtained from H by a series
of operations of the following type: delete a vertex, delete an edge or contract an
edge. Since neither of these operations destroys planarity, any minor of a planar
graph is planar. Hence a graph which contains K5 or K3,3 as a minor cannot
be planar. Kuratowski’s Theorem says that the converse is also true. We ﬁrst
consider 3-connected graphs and start with the following lemma (which is the
heart of Tutte’s so-called wheel theorem):

2.5 Planarity
37
Lemma 2.36.
(Tutte [1961], Thomassen [1980]) Let G be a 3-connected graph
with at least ﬁve vertices. Then there exists an edge e such that G/e is also 3-
connected.
Proof:
Suppose there is no such edge. Then for each edge e = {v, w} there exists
a vertex x such that G −{v, w, x} is disconnected, i.e. has a connected component
C with |V (C)| < |V (G)| −3. Choose e, x and C such that |V (C)| is minimum.
x has a neighbour y in C, because otherwise C is a connected component
of G −{v, w} (but G is 3-connected). By our assumption, G/{x, y} is not 3-
connected, i.e. there exists a vertex z such that G −{x, y, z} is disconnected.
Since {v, w} ∈E(G), there exists a connected component D of G −{x, y, z}
which contains neither v nor w.
But D contains a neighbour d of y, since otherwise D is a connected com-
ponent of G −{x, z} (again contradicting the fact that G is 3-connected). So
d ∈V (D) ∩V (C), and thus D is a subgraph of C. Since y ∈V (C) \ V (D), we
have a contradiction to the minimality of |V (C)|.
2
Theorem 2.37.
(Kuratowski [1930], Wagner [1937])
A 3-connected graph is
planar if and only if it contains neither K5 nor K3,3 as a minor.
Proof:
As the necessity is evident (see above), we prove the sufﬁciency. Since
K4 is obviously planar, we proceed by induction on the number of vertices: let G
be a 3-connected graph with more than four vertices but no K5 or K3,3 minor.
By Lemma 2.36, there exists an edge e = {v, w} such that G/e is 3-connected.
Let 	 =

ψ, (Je)e∈E(G)

be a planar embedding of G/e, which exists by induction.
Let x be the vertex in G/e which arises by contracting e. Consider (G/e)−x with
the restriction of 	 as a planar embedding. Since (G/e)−x is 2-connected, every
face is bounded by a circuit (Proposition 2.31). In particular, the face containing
the point ψ(x) is bounded by a circuit C.
Let y1, . . . , yk ∈V (C) be the neighbours of v that are distinct from w, num-
bered in cyclic order, and partition C into edge-disjoint paths Pi, i = 1, . . . , k,
such that Pi is a yi-yi+1-path (yk+1 := y1).
Suppose there exists an index i ∈{1, . . . , k} such that (w) ⊆{v} ∪V (Pi).
Then a planar embedding of G can be constructed easily by modifying 	.
We shall prove that all other cases are impossible. First, if w has three neigh-
bours among y1, . . . , yk, we have a K5 minor (Figure 2.6(a)).
Next, if (w) = {v, yi, yj} for some i < j, then we must have i + 1 < j
and (i, j) ̸= (1, k) (otherwise yi and yj would both lie on Pi or Pj); see Figure
2.6(b). Otherwise there is a neighbour z of w in V (Pi) \ {yi, yi+1} for some i and
another neighbour z′ /∈V (Pi) (Figure 2.6(c)). In both cases, there are four vertices
y, z, y′, z′ on C, in this cyclic order, with y, y′ ∈(v) and z, z′ ∈(w). This
implies that we have a K3,3 minor.
2
The proof implies quite directly that every 3-connected simple planar graph
has a planar embedding where each edge is embedded by a straight line and
each face, except the outer face, is convex (Exercise 27(a)). The general case of

38
2. Graphs
(a)
v
w
C
(b)
v
w
C
yi
yj
(c)
v
w
C
yi+1
yi
z
z′
Fig. 2.6.
Kuratowski’s Theorem can be reduced to the 3-connected case by gluing together
planar embeddings of the maximal 3-connected subgraphs, or by the following
lemma:
Lemma 2.38.
(Thomassen [1980]) Let G be a graph with at least ﬁve vertices
which is not 3-connected and which contains neither K5 nor K3,3 as a minor.
Then there exist two non-adjacent vertices v, w ∈V (G) such that G + e, where
e = {v, w} is a new edge, does not contain a K5 or K3,3 minor either.
Proof:
We use induction on |V (G)|. Let G be as above. If G is disconnected,
we can simply add an edge e joining two different connected components. So
henceforth we assume that G is connected. Since G is not 3-connected, there
exists a set X = {x, y} of two vertices such that G −X is disconnected. (If G
is not even 2-connected we may choose x to be an articulation vertex and y a
neighbour of x.) Let C be a connected component of G −X, G1 := G[V (C)∪X]
and G2 := G −V (C). We ﬁrst prove the following:
Claim:
Let v, w ∈V (G1) be two vertices such that adding an edge e = {v, w}
to G creates a K3,3 or K5 minor. Then at least one of G1 + e + f and G2 + f
contains a K5 or K3,3 minor, where f is a new edge joining x and y.
To prove this claim, let v, w ∈V (G1), e = {v, w} and suppose that there are
disjoint connected vertex sets Z1, . . . , Zt of G +e such that after contracting each
of them we have a K5 (t = 5) or K3,3 (t = 6) subgraph.
Note that it is impossible that Zi ⊆V (G1) \ X and Zj ⊆V (G2) \ X for some
i, j ∈{1, . . . , t}: in this case the set of those Zk with Zk ∩X ̸= ∅(there are at
most two of these) separate Zi and Zj, contradicting the fact that both K5 and
K3,3 are 3-connected.
Hence there are two cases: If none of Z1, . . . , Zt is a subset of V (G2) \ X,
then G1 + e + f also contains a K5 or K3,3 minor: just consider Zi ∩V (G1)
(i = 1, . . . , t).
Analogously, if none of Z1, . . . , Zt is a subset of V (G1) \ X, then G2 + f
contains a K5 or K3,3 minor (consider Zi ∩V (G2) (i = 1, . . . , t)).
The claim is proved. Now we ﬁrst consider the case when G contains an
articulation vertex x, and y is a neighbour of x. We choose a second neighbour z

2.5 Planarity
39
of x such that y and z are in different connected components of G −x. W.l.o.g.
say that z ∈V (G1). Suppose that the addition of e = {y, z} creates a K5 or K3,3
minor. By the claim, at least one of G1 + e and G2 contains a K5 or K3,3 minor
(an edge {x, y} is already present). But then G1 or G2, and thus G, contains a K5
or K3,3 minor, contradicting our assumption.
Hence we may assume that G is 2-connected. Recall that x, y ∈V (G) were
chosen such that G −{x, y} is disconnected. If {x, y} /∈E(G) we simply add an
edge f = {x, y}. If this creates a K5 or K3,3 minor, the claim implies that G1 + f
or G2 + f contains such a minor. Since there is an x-y-path in each of G1, G2
(otherwise we would have an articulation vertex of G), this implies that there is a
K5 or K3,3 minor in G which is again a contradiction.
Thus we can assume that f = {x, y} ∈E(G). Suppose now that at least one
of the graphs Gi (i ∈{1, 2}) is not planar. Then this Gi has at least ﬁve vertices.
Since it does not contain a K5 or K3,3 minor (this would also be a minor of G),
we conclude from Theorem 2.37 that Gi is not 3-connected. So we can apply the
induction hypothesis to Gi. By the claim, if adding an edge within Gi does not
introduce a K3 or K5,5 minor in Gi, it cannot introduce such a minor in G either.
So we may assume that both G1 and G2 are planar; let 	1 and 	2 be planar
embeddings. Let Fi be a face of 	i with f on its boundary, and let zi be another
vertex on the boundary of Fi, zi /∈{x, y} (i = 1, 2). We claim that adding an edge
{z1, z2} (cf. Figure 2.7) does not introduce a K5 or K3,3 minor.
G1
G2
f
x
y
z1
z2
Fig. 2.7.
Suppose, on the contrary, that adding {z1, z2} and contracting some disjoint
connected vertex sets Z1, . . . , Zt would create a K5 (t = 5) or K3,3 (t = 6)
subgraph.
First suppose that at most one of the sets Zi is a subset of V (G1)\{x, y}. Then
the graph G′
2, arising from G2 by adding one vertex w and edges from w to x, y
and z2, also contains a K5 or K3,3 minor. (Here w corresponds to the contracted
set Zi ⊆V (G1)\{x, y}.) This is a contradiction since there is a planar embedding
of G′
2: just supplement 	2 by placing w within F2.
So we may assume that Z1, Z2 ⊆V (G1)\{x, y}. Analogously, we may assume
that Z3, Z4 ⊆V (G2) \ {x, y}. W.l.o.g. we have z1 /∈Z1 and z2 /∈Z3. Then we
cannot have a K5, because Z1 and Z3 are not adjacent. Moreover, the only possible

40
2. Graphs
common neighbours of Z1 and Z3 are Z5 and Z6. Since in K3,3 each stable set
has three common neighbours, a K3,3 minor is also impossible.
2
Theorem 2.37 and Lemma 2.38 yield Kuratowski’s Theorem:
Theorem 2.39.
(Kuratowski [1930], Wagner [1937]) An undirected graph is pla-
nar if and only if it contains neither K5 nor K3,3 as a minor.
2
Indeed, Kuratowski proved a stronger version (Exercise 28). The proof can be
turned into a polynomial-time algorithm quite easily (Exercise 27(b)). In fact, a
linear-time algorithm exists:
Theorem 2.40.
(Hopcroft and Tarjan [1974]) There is a linear-time algorithm
for ﬁnding a planar embedding of a given graph or deciding that it is not planar.
2.6 Planar Duality
We shall now introduce an important duality concept. This is the only place in this
book where we need loops. So in this section loops, i.e. edges whose endpoints
coincide, are allowed. In a planar embedding loops are of course represented by
polygons instead of polygonal arcs.
Note that Euler’s formula (Theorem 2.32) also holds for graphs with loops: this
follows from the observation that subdividing a loop e (i.e. replacing e = {v, v}
by two parallel edges {v, w}, {w, v} where w is a new vertex) and adjusting the
embedding (replacing the polygon Je by two polygonal arcs whose union is Je)
increases the number of edges and vertices each by one but does not change the
number of faces.
Deﬁnition 2.41.
Let G be a directed or undirected graph, possibly with loops, and
let 	 = (ψ, (Je)e∈E(G)) be a planar embedding of G. We deﬁne the planar dual
G∗whose vertices are the faces of 	 and whose edge set is {e∗: e ∈E(G)}, where
e∗connects the faces that are adjacent to Je (if Je is adjacent to only one face,
then e∗is a loop). In the directed case, say for e = (v, w), we orient e∗= (F1, F2)
in such a way that F1 is the face “to the right” when traversing Je from ψ(v) to
ψ(w).
G∗is again planar. In fact, there obviously exists a planar embedding

ψ∗, (Je∗)e∗∈E(G∗)

of G∗such that ψ∗(F) ∈F for all faces F of 	 and, for
each e ∈E(G), |Je∗∩Je| = 1 and
Je∗∩
⎛
⎝{ψ(v) : v ∈V (G)} ∪

f ∈E(G)\{e}
Jf
⎞
⎠= ∅.
Such an embedding is called a standard embedding of G∗.

2.6 Planar Duality
41
(a)
(b)
Fig. 2.8.
The planar dual of a graph really depends on the embedding: consider the two
embeddings of the same graph shown in Figure 2.8. The resulting planar duals are
not isomorphic, since the second one has a vertex of degree four (corresponding
to the outer face) while the ﬁrst one is 3-regular.
Proposition 2.42.
Let G be an undirected connected planar graph with a ﬁxed
embedding. Let G∗be its planar dual with a standard embedding. Then (G∗)∗= G.
Proof:
Let

ψ, (Je)e∈E(G)

be a ﬁxed embedding of G and

ψ∗, (Je∗)e∗∈E(G∗)

a
standard embedding of G∗. Let F be a face of G∗. The boundary of F contains
Je∗for at least one edge e∗, so F must contain ψ(v) for one endpoint v of e. So
every face of G∗contains at least one vertex of G.
By applying Euler’s formula (Theorem 2.32) to G∗and to G, we get that the
number of faces of G∗is |E(G∗)| −|V (G∗)| + 2 = |E(G)| −(|E(G)| −|V (G)| +
2) + 2 = |V (G)|. Hence each face of G∗contains exactly one vertex of G. From
this we conclude that the planar dual of G∗is isomorphic to G.
2
The requirement that G is connected is essential here: note that G∗is always
connected, even if G is disconnected.
Theorem 2.43.
Let G be a connected planar undirected graph with arbitrary
embedding. The edge set of any circuit in G corresponds to a minimal cut in G∗,
and any minimal cut in G corresponds to the edge set of a circuit in G∗.
Proof:
Let 	 = (ψ, (Je)e∈E(G)) be a ﬁxed planar embedding of G. Let C be a
circuit in G. By Theorem 2.30, R2 \ 
e∈E(C) Je splits into exactly two connected
regions. Let A and B be the set of faces of 	 in the inner and outer region,
respectively. We have V (G∗) = A
.
∪B and EG∗(A, B) = {e∗: e ∈E(C)}. Since
A and B form connected sets in G∗, this is indeed a minimal cut.
Conversely, let δG(A) be a minimal cut in G. Let 	∗= (ψ∗, (Je)e∈E(G∗)) be
a standard embedding of G∗. Let a ∈A and b ∈V (G) \ A. Observe that there is
no polygonal arc in
R := R2 \
⎛
⎝{ψ∗(v) : v ∈V (G∗)} ∪

e∈δG(A)
Je∗
⎞
⎠

42
2. Graphs
which connects ψ(a) and ψ(b): the sequence of faces of G∗passed by such a
polygonal arc would deﬁne an edge progression from a to b in G not using any
edge of δG(A).
So R consists of at least two connected regions. Then, obviously, the boundary
of each region must contain a circuit. Hence F := {e∗: e ∈δG(A)} contains the
edge set of a circuit C in G∗. We have {e∗: e ∈E(C)} ⊆{e∗: e ∈F} = δG(A),
and, by the ﬁrst part, {e∗: e ∈E(C)} is a minimal cut in (G∗)∗= G (cf.
Proposition 2.42). We conclude that {e∗: e ∈E(C)} = δG(A).
2
In particular, e∗is a loop if and only if e is a bridge, and vice versa. For
digraphs the above proof yields:
Corollary 2.44.
Let G be a connected planar digraph with some ﬁxed planar
embedding. The edge set of any circuit in G corresponds to a minimal directed cut
in G∗, and vice versa.
2
Another interesting consequence of Theorem 2.43 is:
Corollary 2.45.
Let G be a connected undirected graph with arbitrary planar
embedding. Then G is bipartite if and only if G∗is Eulerian, and G is Eulerian if
and only if G∗is bipartite.
Proof:
Observe that a connected graph is Eulerian if and only if every minimal
cut has even cardinality. By Theorem 2.43, G is bipartite if G∗is Eulerian, and
G is Eulerian if G∗is bipartite. By Proposition 2.42, the converse is also true. 2
An abstract dual of G is a graph G′ for which there is a bijection χ : E(G) →
E(G′) such that F is the edge set of a circuit iff χ(F) is a minimal cut in G′
and vice versa. Theorem 2.43 shows that any planar dual is also an abstract dual.
The converse is not true. However, Whitney [1933] proved that a graph has an
abstract dual if and only if it is planar (Exercise 34). We shall return to this duality
relation when dealing with matroids in Section 13.3.
Exercises
1. Let G be a simple undirected graph on n vertices which is isomorphic to its
complement. Show that n mod 4 ∈{0, 1}.
2. Prove that every simple undirected graph G with |δ(v)| ≥1
2|V (G)| for all
v ∈V (G) is Hamiltonian.
Hint: Consider a longest path in G and the neighbours of its endpoints.
(Dirac [1952])
3. Prove that any simple undirected graph G with |E(G)| >
|V (G)|−1
2

is con-
nected.
4. Let G be a simple undirected graph. Show that G or its complement is con-
nected.

Exercises
43
5. Prove that every simple undirected graph with more than one vertex contains
two vertices that have the same degree. Prove that every tree (except a single
vertex) contains at least two leaves.
6. Let G be a connected undirected graph, and let (V (G), F) be a forest in G.
Prove that there is a spanning tree (V (G), T ) with F ⊆T ⊆E(G).
7. Let (V, F1) and (V, F2) be two forests with |F1| < |F2|. Prove that there
exists an edge e ∈F2 \ F1 such that (V, F1 ∪{e}) is a forest.
8. Prove that any cut in an undirected graph is the disjoint union of minimal
cuts.
9. Let G be an undirected graph, C a circuit and D a cut. Show that |E(C)∩D|
is even.
10. Show that any undirected graph has a cut containing at least half of the edges.
11. Let (U, F) be a cross-free set system with |U| ≥2. Prove that F contains at
most 4|U| −4 distinct elements.
12. Let G be a connected undirected graph. Show that there exists an orientation
G′ of G and a spanning arborescence T of G′ such that the set of fundamental
circuits with respect to T is precisely the set of directed circuits in G′.
Hint: Consider a DFS-tree.
(Camion [1968], Crestin [1969])
13. Describe a linear-time algorithm for the following problem: Given an adja-
cency list of a graph G, compute an adjacency list of the maximal simple
subgraph of G. Do not assume that parallel edges appear consecutively in the
input.
14. Given a graph G (directed or undirected), show that there is a linear-time
algorithm to ﬁnd a circuit or decide that none exists.
15. Let G be a connected undirected graph, s ∈V (G) and T a DFS-tree resulting
from running DFS on (G, s). s is called the root of T . x is a predecessor of
y in T if x lies on the (unique) s-y-path in T . x is a direct predecessor of y
if the edge {x, y} lies on the s-y-path in T . y is a (direct) successor of x if x
is a (direct) predecessor of y. Note that with this deﬁnition each vertex is a
successor (and a predecessor) of itself. Every vertex except s has exactly one
direct predecessor. Prove:
(a) For any edge {v, w} ∈E(G), v is a predecessor or a successor of w in
T .
(b) A vertex v is an articulation vertex of G if and only if
– either v = s and |δT (v)| > 1
– or v ̸= s and there is a direct successor w of v such that no edge in
G connects a proper predecessor of v (that is, excluding v) with a
successor of w.
16.
∗
Use Exercise 15 to design a linear-time algorithm which ﬁnds the blocks of
an undirected graph. It will be useful to compute numbers
α(x) := min{ f (w) : w = x or {w, y} ∈E(G)\T for some successor y of x}
recursively during the DFS. Here (R, T ) is the DFS-tree (with root s), and
the f -values represent the order in which the vertices are added to R (see

44
2. Graphs
the Graph Scanning Algorithm). If for some vertex x ∈R \ {s} we have
α(x) ≥f (w), where w is the direct predecessor of x, then w must be either
the root or an articulation vertex.
17. Prove:
(a) An undirected graph is 2-edge-connected if and only if it has at least two
vertices and an ear-decomposition.
(b) A digraph is strongly connected if and only if it has an ear-decomposition.
(c) The edges of an undirected graph G with at least two vertices can be
oriented such that the resulting digraph is strongly connected if and only
if G is 2-edge-connected.
(Robbins [1939])
18. A tournament is a digraph such that the underlying undirected graph is a
(simple) complete graph. Prove that every tournament contains a Hamilto-
nian path (R´edei [1934]). Prove that every strongly connected tournament is
Hamiltonian (Camion [1959]).
19. Prove that if a connected undirected simple graph is Eulerian then its line
graph is Hamiltonian. What about the converse?
20. Prove that any connected bipartite graph has a unique bipartition. Prove that
any non-bipartite undirected graph contains an odd circuit as an induced sub-
graph.
21. Prove that a strongly connected digraph whose underlying undirected graph
is non-bipartite contains a (directed) circuit of odd length.
22.
∗
Let G be an undirected graph. A tree-decomposition of G is a pair (T, ϕ),
where T is a tree and ϕ : V (T ) →2V (G) satisﬁes the following conditions:
– for each e ∈E(G) there exists a t ∈V (T ) with e ⊆ϕ(t);
– for each v ∈V (G) the set {t ∈V (T ) : v ∈ϕ(t)} is connected in T .
We say that the width of (T, ϕ) is maxt∈V (T ) |ϕ(t)| −1. The tree-width of a
graph G is the minimum width of a tree-decomposition of G. This notion is
due to Robertson and Seymour [1986].
Show that the graphs of tree-width at most 1 are the forests. Moreover, prove
that the following statements are equivalent for an undirected graph G:
(a) G has tree-width at most 2;
(b) G does not contain K4 as a minor;
(c) G can be obtained from an empty graph by successively adding bridges
and doubling and subdividing edges. (Doubling an edge e = {v, w} ∈
E(G) means adding another edge with endpoints v and w; subdividing
an edge e = {v, w} ∈E(G) means adding a vertex x and replacing e by
two edges {v, x}, {x, w}.)
Note: Because of the construction in (c) such graphs are called series-parallel.
23. Show that if a graph G has a planar embedding where the edges are embedded
by arbitrary Jordan curves, then it also has a planar embedding with polygonal
arcs only.
24. Let G be a 2-connected graph with a planar embedding. Show that the set of
circuits bounding the ﬁnite faces constitute a cycle basis of G.

Exercises
45
25. Can you generalize Euler’s formula (Theorem 2.32) to disconnected graphs?
26. Show that there are exactly ﬁve Platonic graphs (corresponding to the Platonic
solids; cf. Exercise 11 of Chapter 4), i.e. 3-connected planar regular graphs
whose faces are all bounded by the same number of edges.
Hint: Use Euler’s formula (Theorem 2.32).
27. Deduce from the proof of Kuratowski’s Theorem 2.39:
(a) Every 3-connected simple planar graph has a planar embedding where
each edge is embedded by a straight line and each face, except the outer
face, is convex.
(b) There is a polynomial-time algorithm for checking whether a given graph
is planar.
28.
∗
Given a graph G and an edge e = {v, w} ∈E(G), we say that H results from
G by subdividing e if V (H) = V (G)
.
∪{x} and E(H) = (E(G) \ {e}) ∪
{{v, x}, {x, w}}. A graph resulting from G by successively subdividing edges
is called a subdivision of G.
(a) Trivially, if H contains a subdivision of G then G is a minor of H. Show
that the converse is not true.
(b) Prove that a graph containing a K3,3 or K5 minor also contains a subdi-
vision of K3,3 or K5.
Hint: Consider what happens when contracting one edge.
(c) Conclude that a graph is planar if and only if no subgraph is a subdivision
of K3,3 or K5.
(Kuratowski [1930])
29. Prove that each of the following statements implies the other:
(a) For every inﬁnite sequence of graphs G1, G2, . . . there are two indices
i < j such that Gi is a minor of Gj.
(b) Let G be a class of graphs such that for each G ∈G and each minor H
of G we have H ∈G (i.e. G is a hereditary graph property). Then there
exists a ﬁnite set X of graphs such that G consists of all graphs that do
not contain any element of X as a minor.
Note: The statements have been proved by Robertson and Seymour; they are
a main result of their series of papers on graph minors (not yet completely
published). Theorem 2.39 and Exercise 22 give examples of forbidden minor
characterizations as in (b).
30. Let G be a planar graph with an embedding 	, and let C be a circuit of G
bounding some face of 	. Prove that then there is an embedding 	′ of G
such that C bounds the outer face.
31. (a) Let G be disconnected with an arbitrary planar embedding, and let G∗
be the planar dual with a standard embedding. Prove that (G∗)∗arises
from G by successively applying the following operation, until the graph
is connected: Choose two vertices x and y which belong to different
connected components and which are adjacent to the same face; contract
{x, y}.

46
2. Graphs
(b) Generalize Corollary 2.45 to arbitrary planar graphs.
Hint: Use (a) and Theorem 2.26.
32. Let G be a connected digraph with a ﬁxed planar embedding, and let G∗be
the planar dual with a standard embedding. How are G and (G∗)∗related?
33. Prove that if a planar digraph is acyclic (strongly connected), then its planar
dual is strongly connected (acyclic). What about the converse?
34. (a) Show that if G has an abstract dual and H is a minor of G then H also
has an abstract dual.
(b)
∗
Show that neither K5 nor K3,3 has an abstract dual.
(c) Conclude that a graph is planar if and only if it has an abstract dual.
(Whitney [1933])
References
General Literature:
Berge, C. [1985]: Graphs. 2nd revised edition. Elsevier, Amsterdam 1985
Bollob´as, B. [1998]: Modern Graph Theory. Springer, New York 1998
Bondy, J.A. [1995]: Basic graph theory: paths and circuits. In: Handbook of Combinatorics;
Vol. 1 (R.L. Graham, M. Gr¨otschel, L. Lov´asz, eds.), Elsevier, Amsterdam 1995
Bondy, J.A., and Murty, U.S.R. [1976]: Graph Theory with Applications. MacMillan, Lon-
don 1976
Diestel, R. [1997]: Graph Theory. Springer, New York 1997
Wilson, R.J. [1972]: Introduction to Graph Theory. Oliver and Boyd, Edinburgh 1972 (3rd
edition: Longman, Harlow 1985)
Cited References:
Aoshima, K., and Iri, M. [1977]: Comments on F. Hadlock’s paper: ﬁnding a maximum
cut of a Planar graph in polynomial time. SIAM Journal on Computing 6 (1977), 86–87
Camion, P. [1959]: Chemins et circuits hamiltoniens des graphes complets. Comptes Rendus
Hebdomadaires des S´eances de l’Acad´emie des Sciences (Paris) 249 (1959), 2151–2152
Camion, P. [1968]: Modulaires unimodulaires. Journal of Combinatorial Theory A 4 (1968),
301–362
Dirac, G.A. [1952]: Some theorems on abstract graphs. Proceedings of the London Math-
ematical Society 2 (1952), 69–81
Edmonds, J., and Giles, R. [1977]: A min-max relation for submodular functions on graphs.
In: Studies in Integer Programming; Annals of Discrete Mathematics 1 (P.L. Hammer,
E.L. Johnson, B.H. Korte, G.L. Nemhauser, eds.), North-Holland, Amsterdam 1977, pp.
185–204
Euler, L. [1736]: Solutio Problematis ad Geometriam Situs Pertinentis. Commentarii
Academiae Petropolitanae 8 (1736), 128–140
Euler, L. [1758]: Demonstratio nonnullarum insignium proprietatum quibus solida hedris
planis inclusa sunt praedita. Novi Commentarii Academiae Petropolitanae 4 (1758), 140–
160
Hierholzer, C. [1873]: ¨Uber die M¨oglichkeit, einen Linienzug ohne Wiederholung und ohne
Unterbrechung zu umfahren. Mathematische Annalen 6 (1873), 30–32
Hopcroft, J.E., and Tarjan, R.E. [1974]: Efﬁcient planarity testing. Journal of the ACM 21
(1974), 549–568
Kahn, A.B. [1962]: Topological sorting of large networks. Communications of the ACM 5
(1962), 558–562

References
47
Knuth, D.E. [1968]: The Art of Computer Programming; Vol. 1; Fundamental Algorithms.
Addison-Wesley, Reading 1968 (third edition: 1997)
K¨onig, D. [1916]: ¨Uber Graphen und Ihre Anwendung auf Determinantentheorie und Men-
genlehre. Mathematische Annalen 77 (1916), 453–465
K¨onig, D. [1936]: Theorie der endlichen und unendlichen Graphen. Chelsea Publishing Co.,
Leipzig 1936, reprint New York 1950
Kuratowski, K. [1930]: Sur le probl`eme des courbes gauches en topologie. Fundamenta
Mathematicae 15 (1930), 271–283
Legendre, A.M. [1794]: ´El´ements de G´eom´etrie. Firmin Didot, Paris 1794
Minty, G.J. [1960]: Monotone networks. Proceedings of the Royal Society of London A
257 (1960), 194–212
Moore, E.F. [1959]: The shortest path through a maze. Proceedings of the International
Symposium on the Theory of Switching; Part II. Harvard University Press 1959, pp.
285–292
R´edei, L. [1934]: Ein kombinatorischer Satz. Acta Litt. Szeged 7 (1934), 39–43
Robbins, H.E. [1939]: A theorem on graphs with an application to a problem of trafﬁc
control. American Mathematical Monthly 46 (1939), 281–283
Robertson, N., and Seymour, P.D. [1986]: Graph minors II: algorithmic aspects of tree-
width. Journal of Algorithms 7 (1986), 309–322
Tarjan, R.E. [1972]: Depth ﬁrst search and linear graph algorithms. SIAM Journal on
Computing 1 (1972), 146–160
Thomassen, C. [1980]: Planarity and duality of ﬁnite and inﬁnite graphs. Journal of Com-
binatorial Theory B 29 (1980), 244–271
Thomassen, C. [1981]: Kuratowski’s theorem. Journal of Graph Theory 5 (1981), 225–241
Tutte, W.T. [1961]: A theory of 3-connected graphs. Konink. Nederl. Akad. Wetensch.
Proc. A 64 (1961), 441–455
Wagner, K. [1937]: ¨Uber eine Eigenschaft der ebenen Komplexe. Mathematische Annalen
114 (1937), 570–590
Whitney, H. [1932]: Non-separable and planar graphs. Transactions of the American Math-
ematical Society 34 (1932), 339–362
Whitney, H. [1933]: Planar graphs. Fundamenta Mathematicae 21 (1933), 73–84

3. Linear Programming
In this chapter we review the most important facts about Linear Programming.
Although this chapter is self-contained, it cannot be considered to be a compre-
hensive treatment of the ﬁeld. The reader unfamiliar with Linear Programming is
referred to the textbooks mentioned at the end of this chapter.
The general problem reads as follows:
Linear Programming
Instance:
A matrix A ∈Rm×n and column vectors b ∈Rm, c ∈Rn.
Task:
Find a column vector x ∈Rn such that Ax ≤b and c⊤x is maximum,
decide that {x ∈Rn : Ax ≤b} is empty, or decide that for all α ∈R
there is an x ∈Rn with Ax ≤b and c⊤x > α.
A linear program (LP) is an instance of the above problem. We often write
a linear program as max{c⊤x : Ax ≤b}. A feasible solution of an LP max{c⊤x :
Ax ≤b} is a vector x with Ax ≤b. A feasible solution attaining the maximum is
called an optimum solution.
Here c⊤x denotes the scalar product of the vectors. The notion x ≤y for vectors
x and y (of equal size) means that the inequality holds in each component. If no
sizes are speciﬁed, the matrices and vectors are always assumed to be compatible
in size. We often omit indicating the transposition of column vectors and write
e.g. cx for the scalar product.
As the problem formulation indicates, there are two possibilities when an LP
has no solution: The problem can be infeasible (i.e. P := {x ∈Rn : Ax ≤b} = ∅)
or unbounded (i.e. for all α ∈R there is an x ∈P with cx > α). If an LP is neither
infeasible nor unbounded it has an optimum solution, as we shall prove in Section
3.2. This justiﬁes the notation max{c⊤x : Ax ≤b} instead of sup{c⊤x : Ax ≤b}.
Many combinatorial optimization problems can be formulated as LPs. To do
this, we encode the feasible solutions as vectors in Rn for some n. In Section 3.4
we show that one can optimize a linear objective function over a ﬁnite set S of
vectors by solving a linear program. Although the feasible set of this LP contains
not only the vectors in S but also all their convex combinations, one can show
that among the optimum solutions there is always an element of S.
In Section 3.1 we compile some terminology and basic facts about polyhedra,
the sets P = {x ∈Rn : Ax ≤b} of feasible solutions of LPs. In Section 3.2 we

50
3. Linear Programming
present the Simplex Algorithm, which we also use to derive the Duality Theorem
and related results (Section 3.3). LP duality is a most important concept which
explicitly or implicitly appears in almost all areas of combinatorial optimization;
we shall often refer to the results in Sections 3.3 and 3.4.
3.1 Polyhedra
Linear Programming deals with maximizing or minimizing a linear objective func-
tion of ﬁnitely many variables subject to ﬁnitely many linear inequalities. So the
set of feasible solutions is the intersection of ﬁnitely many halfspaces. Such a set
is called a polyhedron:
Deﬁnition 3.1.
A polyhedron in Rn is a set of type P = {x ∈Rn : Ax ≤b} for
some matrix A ∈Rm×n and some vector b ∈Rm. If A and b are rational, then P
is a rational polyhedron. A bounded polyhedron is also called a polytope.
We denote by rank(A) the rank of a matrix A. The dimension dim X of a
nonempty set X ⊆Rn is deﬁned to be
n −max{rank(A) : A is an n × n-matrix with Ax = Ay for all x, y ∈X}.
A polyhedron P ⊆Rn is called full-dimensional if dim P = n.
Equivalently, a polyhedron is full-dimensional if and only if there is a point
in its interior. For most of this chapter it makes no difference whether we are in
the rational or real space. We need the following standard terminology:
Deﬁnition 3.2.
Let P := {x : Ax ≤b} be a nonempty polyhedron. If c is a
nonzero vector for which δ := max{cx : x ∈P} is ﬁnite, then {x : cx = δ} is
called a supporting hyperplane of P. A face of P is P itself or the intersection
of P with a supporting hyperplane of P. A point x for which {x} is a face is called
a vertex of P, and also a basic solution of the system Ax ≤b.
Proposition 3.3.
Let P = {x : Ax ≤b} be a polyhedron and F ⊆P. Then the
following statements are equivalent:
(a) F is a face of P.
(b) There exists a vector c such that δ := max{cx : x ∈P} is ﬁnite and
F = {x ∈P : cx = δ}.
(c) F = {x ∈P : A′x = b′} ̸= ∅for some subsystem A′x ≤b′ of Ax ≤b.
Proof:
(a) and (b) are obviously equivalent.
(c)⇒(b): If F = {x ∈P : A′x = b′} is nonempty, let c be the sum of the rows
of A′, and let δ be the sum of the components of b′. Then obviously cx ≤δ for
all x ∈P and F = {x ∈P : cx = δ}.
(b)⇒(c): Assume that c is a vector, δ := max{cx : x ∈P} is ﬁnite and
F = {x ∈P : cx = δ}. Let A′x ≤b′ be the maximal subsystem of Ax ≤b such
that A′x = b′ for all x ∈F. Let A′′x ≤b′′ be the rest of the system Ax ≤b.

3.1 Polyhedra
51
We ﬁrst observe that for each inequality a′′
i x ≤β′′
i of A′′x ≤b′′ (i = 1, . . . , k)
there is a point xi ∈F such that a′′
i xi < β′′
i . Let x∗:= 1
k
k
i=1 xi be the center of
gravity of these points (if k = 0, we can choose an arbitrary x∗∈F); we have
x∗∈F and a′′
i x∗< β′′
i for all i.
We have to prove that A′y = b′ cannot hold for any y ∈P\F. So let y ∈P\F.
We have cy < δ. Now consider z := x∗+ ϵ(x∗−y) for some small ϵ > 0; in
particular let ϵ be smaller than β′′
i −a′′
i x∗
a′′
i (x∗−y) for all i ∈{1, . . . , k} with a′′
i x∗> a′′
i y.
We have cz > δ and thus z /∈P. So there is an inequality ax ≤β of Ax ≤b
such that az > β. Thus ax∗> ay. The inequality ax ≤β cannot belong to A′′x ≤
b′′, since otherwise we have az = ax∗+ϵa(x∗−y) < ax∗+ β−ax∗
a(x∗−y)a(x∗−y) = β
(by the choice of ϵ). Hence the inequality ax ≤β belongs to A′x ≤b′. Since
ay = a(x∗+ 1
ϵ (x∗−z)) < β, this completes the proof.
2
As a trivial but important corollary we remark:
Corollary 3.4.
If max{cx : x ∈P} is bounded for a nonempty polyhedron P and
a vector c, then the set of points where the maximum is attained is a face of P. 2
The relation “is a face of” is transitive:
Corollary 3.5.
Let P be a polyhedron and F a face of P. Then F is again a
polyhedron. Furthermore, a set F′ ⊆F is a face of P if and only if it is a face
of F.
2
The maximal faces distinct from P are particularly important:
Deﬁnition 3.6.
Let P be a polyhedron. A facet of P is a maximal face distinct
from P. An inequality cx ≤δ is facet-deﬁning for P if cx ≤δ for all x ∈P and
{x ∈P : cx = δ} is a facet of P.
Proposition 3.7.
Let P ⊆{x ∈Rn : Ax = b} be a nonempty polyhedron of
dimension n −rank(A). Let A′x ≤b′ be a minimal inequality system such that
P = {x : Ax = b, A′x ≤b′}. Then each inequality of A′x ≤b′ is facet-deﬁning
for P, and each facet of P is deﬁned by an inequality of A′x ≤b′.
Proof:
If P = {x ∈Rn : Ax = b}, then there are no facets and the statement
is trivial. So let A′x ≤b′ be a minimal inequality system with P = {x : Ax =
b, A′x ≤b′}, let a′x ≤β′ be one of its inequalities and A′′x ≤b′′ be the rest of
the system A′x ≤b′. Let y be a vector with Ay = b, A′′y ≤b′′ and a′y > b′
(such a vector y exists as the inequality a′x ≤b′ is not redundant). Let x ∈P
such that a′x < b′ (such a vector must exist because dim P = n −rank(A)).
Consider z := x + β′−a′x
a′y−a′x (y −x). We have a′z = β and, since 0 <
β′−a′x
a′y−a′x < 1,
z ∈P. Therefore F := {x ∈P : a′x = β′} ̸= 0 and F ̸= P (as x ∈P \ F). Thus
F is a facet of P.
By Proposition 3.3 each facet is deﬁned by an inequality of A′x ≤b′.
2

52
3. Linear Programming
The other important class of faces (beside facets) are minimal faces (i.e. faces
not containing any other face). Here we have:
Proposition 3.8.
(Hoffman and Kruskal [1956])
Let P = {x : Ax ≤b} be
a polyhedron. A nonempty subset F ⊆P is a minimal face of P if and only if
F = {x : A′x = b′} for some subsystem A′x ≤b′ of Ax ≤b.
Proof:
If F is a minimal face of P, by Proposition 3.3 there is a subsystem
A′x ≤b′ of Ax ≤b such that F = {x ∈P : A′x = b′}. We choose A′x ≤b′
maximal. Let A′′x ≤b′′ be a minimal subsystem of Ax ≤b such that F = {x :
A′x = b′, A′′x ≤b′′}. We claim that A′′x ≤b′′ does not contain any inequality.
Suppose, on the contrary, that a′′x ≤β′′ is an inequality of A′′x ≤b′′. Since it
is not redundant for the description of F, Proposition 3.7 implies that F′ := {x :
A′x = b′, A′′x ≤b′′, a′′x = β′′} is a facet of F. By Corollary 3.5 F′ is also a
face of P, contradicting the assumption that F is a minimal face of P.
Now let ∅̸= F = {x : A′x = b′} ⊆P for some subsystem A′x ≤b′ of
Ax ≤b. Obviously F has no faces except itself. By Proposition 3.3, F is a face
of P. It follows by Corollary 3.5 that F is a minimal face of P.
2
Corollary 3.4 and Proposition 3.8 imply that Linear Programming can be
solved in ﬁnite time by solving the linear equation system A′x = b′ for each
subsystem A′x ≤b′ of Ax ≤b. A more intelligent way is the Simplex Algorithm
which is described in the next section.
Another consequence of Proposition 3.8 is:
Corollary 3.9.
Let P = {x ∈Rn : Ax ≤b} be a polyhedron. Then all minimal
faces of P have dimension n−rank(A). The minimal faces of polytopes are vertices.
2
This is why polyhedra {x ∈Rn : Ax ≤b} with rank(A) = n are called
pointed: their minimal faces are points.
Let us close this section with some remarks on polyhedral cones.
Deﬁnition 3.10.
A cone is a set C ⊆Rn for which x, y ∈C and λ, µ ≥0 implies
λx + µy ∈C. A cone C is said to be generated by x1, . . . , xk if x1, . . . , xk ∈C
and for any x ∈C there are numbers λ1, . . . , λk ≥0 with x = k
i=1 λixi. A cone
is called ﬁnitely generated if some ﬁnite set of vectors generates it. A polyhedral
cone is a polyhedron of type {x : Ax ≤0}.
It is immediately clear that polyhedral cones are indeed cones. We shall now
show that polyhedral cones are ﬁnitely generated. I always denotes an identity
matrix.
Lemma 3.11.
(Minkowski [1896]) Let C = {x ∈Rn : Ax ≤0} be a polyhedral
cone. Then C is generated by a subset of the set of solutions to the systems My = b′,
where M consists of n linearly independent rows of

A
I

and b′ = ±ej for some
unit vector ej.

3.2 The Simplex Algorithm
53
Proof:
Let A be an m × n-matrix. Consider the systems My = b′ where M
consists of n linearly independent rows of

A
I

and b′ = ±ej for some unit
vector ej. Let y1, . . . , yt be those solutions of these equality systems that belong
to C. We claim that C is generated by y1, . . . , yt.
First suppose C = {x : Ax = 0}, i.e. C is a linear subspace. Write C = {x :
A′x = 0} where A′ consists of a maximal set of linearly independent rows of A.
Let I ′ consist of some rows of I such that

A′
I ′

is a nonsingular square matrix.
Then C is generated by the solutions of

A′
I ′

x =

0
b

,
for b = ±ej, j = 1, . . . , dim C.
For the general case we use induction on the dimension of C. If C is not
a linear subspace, choose a row a of A and a submatrix A′ of A such that the
rows of

A′
a

are linearly independent and {x : A′x = 0, ax ≤0} ⊆C. By
construction there is an index s ∈{1, . . . , t} such that A′ys = 0 and ays = −1.
Now let an arbitrary z ∈C be given. Let a1, . . . , am be the rows of A and
µ := min

ai z
ai ys : i = 1, . . . , m, ai ys < 0

. We have µ ≥0. Let k be an index
where the minimum is attained. Consider z′ := z −µys. By the deﬁnition of µ
we have ajz′ = ajz −akz
ak ys aj ys for j = 1, . . . , m, and hence z′ ∈C′ := {x ∈
C : akx = 0}. C′ is a cone whose dimension is one less than that of C (because
ak ys < 0 and ys ∈C). By induction, C′ is generated by a subset of y1, . . . , yt, so
z′ = t
i=1 λi yi for some λ1, . . . , λt ≥0. By setting λ′
s := λs + µ (observe that
µ ≥0) and λ′
i := λi (i ̸= s), we obtain z = z′ + µys = t
i=1 λ′
i yi.
2
Thus any polyhedral cone is ﬁnitely generated. We shall show the converse at
the end of Section 3.3.
3.2 The Simplex Algorithm
The oldest and best known algorithm for Linear Programming is Dantzig’s
[1951] simplex method. We ﬁrst assume that the polyhedron has a vertex, and that
some vertex is given as input. Later we shall show how general LPs can be solved
with this method.
For a set J of row indices we write AJ for the submatrix of A consisting of
the rows in J only, and bJ for the subvector of b consisting of the components
with indices in J. We abbreviate ai := A{i} and βi := b{i}.

54
3. Linear Programming
Simplex Algorithm
Input:
A matrix A ∈Rm×n and column vectors b ∈Rm, c ∈Rn.
A vertex x of P := {x ∈Rn : Ax ≤b}.
Output:
A vertex x of P attaining max{cx : x ∈P} or a vector w ∈Rn with
Aw ≤0 and cw > 0 (i.e. the LP is unbounded).
1⃝
Choose a set of n row indices J such that AJ is nonsingular and AJ x = bJ.
2⃝
Compute c (AJ)−1 and add zeros in order to obtain a vector y with c = y A
such that all entries of y outside J are zero.
If y ≥0 then stop. Return x and y.
3⃝
Choose the minimum index i with yi < 0.
Let w be the column of −(AJ)−1 with index i, so AJ\{i}w = 0 and
aiw = −1.
If Aw ≤0 then stop.
Return w.
4⃝
Let λ := min
βj −ajx
ajw
: j ∈{1, . . . , m}, ajw > 0

,
and let j be the smallest row index attaining this minimum.
5⃝
Set J := (J \ {i}) ∪{ j} and x := x + λw.
Go to 2⃝.
Step
1⃝relies on Proposition 3.8 and can be implemented with Gaussian
Elimination (Section 4.3). The selection rules for i and j in 3⃝and 4⃝(often
called pivot rule) are due to Bland [1977]. If one just chose an arbitrary i with
yi < 0 and an arbitrary j attaining the minimum in 4⃝the algorithm would run
into cyclic repetitions for some instances. Bland’s pivot rule is not the only one
that avoids cycling; another one (the so-called lexicographic rule) was proved to
avoid cycling already by Dantzig, Orden and Wolfe [1955]. Before proving the
correctness of the Simplex Algorithm, let us make the following observation
(sometimes known as “weak duality”):
Proposition 3.12.
Let x and y be feasible solutions of the LPs
max{cx : Ax ≤b}
and
(3.1)
min{yb : y
⊤A = c
⊤, y ≥0},
(3.2)
respectively. Then cx ≤yb.
Proof:
cx = (y A)x = y(Ax) ≤yb.
2
Theorem 3.13.
(Dantzig [1951], Dantzig, Orden and Wolfe [1955], Bland [1977])
The Simplex Algorithm terminates after at most
m
n

iterations. If it returns x and
y in 2⃝, these vectors are optimum solutions of the LPs (3.1) and (3.2), respectively,
with cx = yb. If the algorithm returns w in 3⃝then cw > 0 and the LP (3.1) is
unbounded.

3.2 The Simplex Algorithm
55
Proof:
We ﬁrst prove that the following conditions hold at any stage of the
algorithm:
(a) x ∈P;
(b) AJ x = bJ;
(c) AJ is nonsingular;
(d) cw > 0;
(e) λ ≥0.
(a) and (b) hold initially. 2⃝and 3⃝guarantee cw = y Aw = −yi > 0. By 4⃝,
x ∈P implies λ ≥0. (c) follows from the fact that AJ\{i}w = 0 and ajw > 0. It
remains to show that 5⃝preserves (a) and (b).
We show that if x ∈P, then also x + λw ∈P. For a row index k we have
two cases: If akw ≤0 then (using λ ≥0) ak(x + λw) ≤akx ≤βk. Otherwise
λ ≤βk−akx
akw
and hence ak(x + λw) ≤akx + akw βk−akx
akw
= βk. (Indeed, λ is chosen
in 4⃝to be the largest number such that x + λw ∈P.)
To show (b), note that after
4⃝we have AJ\{i}w = 0 and λ =
βj−aj x
ajw , so
AJ\{i}(x + λw) = AJ\{i}x = bJ\{i} and aj(x + λw) = ajx + ajw βj−aj x
ajw
= βj.
Therefore after 5⃝, AJ x = bJ holds again.
So we indeed have (a)–(e) at any stage. If the algorithm returns x and y in
2⃝, x and y are feasible solutions of (3.1) and (3.2), respectively. x is a vertex of
P by (a), (b) and (c). Moreover, cx = y Ax = yb since the components of y are
zero outside J. This proves the optimality of x and y by Proposition 3.12.
If the algorithm stops in 3⃝, the LP (3.1) is indeed unbounded because in this
case x + µw ∈P for all µ ≥0, and cw > 0 by (d).
We ﬁnally show that the algorithm terminates. Let J (k) and x(k) be the set J
and the vector x in iteration k of the Simplex Algorithm, respectively. If the
algorithm did not terminate after
m
n

iterations, there are iterations k < l with
J (k) = J (l). By (b) and (c), x(k) = x(l). By (d) and (e), cx never decreases, and it
strictly increases if λ > 0. Hence λ is zero in all the iterations k, k + 1, . . . ,l −1,
and x(k) = x(k+1) = · · · = x(l).
Let h be the highest index leaving J in one of the iterations k, . . . ,l −1,
say in iteration p. Index h must also have been added to J in some iteration
q ∈{k, . . . ,l −1}. Now let y′ be the vector y at iteration p, and let w′ be the
vector w at iteration q. We have y′Aw′ = cw′ > 0. So let r be an index for which
y′
rarw′ > 0. Since y′
r ̸= 0, index r belongs to J (p). If r > h, index r would also
belong to J (q) and J (q+1), implying arw′ = 0. So r ≤h. But by the choice of i
in iteration p we have y′
r < 0 iff r = h, and by the choice of j in iteration q we
have arw′ > 0 iff r = h (recall that λ = 0 and arx(q) = arx(p) = βr as r ∈J (p)).
This is a contradiction.
2
Klee and Minty [1972] and Avis and Chv´atal [1978] found examples where
the Simplex Algorithm (with Bland’s rule) needs 2n iterations on LPs with n
variables and 2n constraints, proving that it is not a polynomial-time algorithm.
It is not known whether there is a pivot rule that leads to a polynomial-time

56
3. Linear Programming
algorithm. However, Borgwardt [1982] showed that the average running time (for
random instances in a certain natural probabilistic model) can be bounded by a
polynomial. Also in practice the Simplex Algorithm is quite fast if implemented
skilfully.
We now show how to solve general linear programs with the Simplex Al-
gorithm. More precisely, we show how to ﬁnd an initial vertex. Since there are
polyhedra that do not have vertices at all, we put a given LP into a different form
ﬁrst.
Let max{cx : Ax ≤b} be an LP. We substitute x by y −z and write it
equivalently in the form
max
 
c
−c
 
y
z

:

A
−A
 
y
z

≤b, y, z ≥0

.
So w.l.o.g. we assume that our LP has the form
max{cx : A′x ≤b′, A′′x ≤b′′, x ≥0}
(3.3)
with b′ ≥0 and b′′ < 0. We ﬁrst run the Simplex Algorithm on the instance
min{(1lA′′)x + 1ly : A′x ≤b′, A′′x + y ≥b′′, x, y ≥0},
(3.4)
where 1l denotes a vector whose entries are all 1. Since

x
y

= 0 deﬁnes a
vertex, this is possible. The LP is obviously not unbounded since the minimum
must be at least 1lb′′. For any feasible solution x of (3.3),

x
b′′ −A′′x

is an
optimum solution of (3.4) of value 1lb′′. Hence if the minimum of (3.4) is greater
than 1lb′′, then (3.3) is infeasible.
In the contrary case, let

x
y

be an optimum vertex of (3.4) of value 1lb′′.
We claim that x is a vertex of the polyhedron deﬁned by (3.3). To see this, ﬁrst
observe that A′′x+y = b′′. Let n and m be the dimensions of x and y, respectively;
then by Proposition 3.8 there is a set S of n + m inequalities of (3.4) satisﬁed
with equality, such that the submatrix corresponding to these n + m inequalities
is nonsingular.
Let S′ be the inequalities of A′x ≤b′ and of x ≥0 that belong to S. Let S′′
consist of those inequalities of A′′x ≤b′′ for which the corresponding inequalities
of A′′x+y ≥b′′ and y ≥0 both belong to S. Obviously |S′∪S′′| ≥|S|−m = n, and
the inequalities of S′∪S′′ are linearly independent and satisﬁed by x with equality.
Hence x satisﬁes n linearly independent inequalities of (3.3) with equality; thus
x is indeed a vertex. Therefore we can start the Simplex Algorithm with (3.3)
and x.

3.3 Duality
57
3.3 Duality
Theorem 3.13 shows that the LPs (3.1) and (3.2) are related. This motivates the
following deﬁnition:
Deﬁnition 3.14.
Given a linear program max{cx : Ax ≤b}, we deﬁne the dual
LP to be the linear program min{yb : y A = c, y ≥0}.
In this case, the original LP max{cx : Ax ≤b} is often called the primal LP.
Proposition 3.15.
The dual of the dual of an LP is (equivalent to) the original
LP.
Proof:
Let the primal LP max{cx : Ax ≤b} be given. Its dual is min{yb : y A =
c, y ≥0}, or equivalently
−max
⎧
⎪⎨
⎪⎩
−by :
⎛
⎜⎝
A⊤
−A⊤
−I
⎞
⎟⎠y ≤
⎛
⎜⎝
c
−c
0
⎞
⎟⎠
⎫
⎪⎬
⎪⎭
.
(Each equality constraint has been split up into two inequality constraints.) So the
dual of the dual is
−min
⎧
⎪⎨
⎪⎩
zc −z′c :

A
−A
−I

⎛
⎜⎝
z
z′
w
⎞
⎟⎠= −b, z, z′, w ≥0
⎫
⎪⎬
⎪⎭
which is equivalent to −min{−cx : −Ax −w = −b, w ≥0} (where we have
substituted x for z′ −z). By eliminating the slack variables w we see that this is
equivalent to the primal LP.
2
We now obtain the most important theorem in LP theory, the Duality Theorem:
Theorem 3.16.
(von Neumann [1947], Gale, Kuhn and Tucker [1951])
If the
polyhedra P := {x : Ax ≤b} and D := {y : y A = c, y ≥0} are both nonempty,
then max{cx : x ∈P} = min{yb : y ∈D}.
Proof:
If D is nonempty, it has a vertex y. We run the Simplex Algorithm
for min{yb : y ∈D} and y. By Proposition 3.12, the existence of some x ∈P
guarantees that min{yb : y ∈D} is not unbounded. Thus by Theorem 3.13, the
SimplexAlgorithm returns optimum solutions y and z of the LP min{yb : y ∈D}
and its dual. However, the dual is max{cx : x ∈P} by Proposition 3.15. We have
yb = cz, as required.
2
We can say even more about the relation between the optimum solutions of
the primal and dual LP:

58
3. Linear Programming
Corollary 3.17.
Let max{cx : Ax ≤b} and min{yb : y A = c, y ≥0} be a
primal-dual pair of LPs. Let x and y be feasible solutions, i.e. Ax ≤b, y A = c
and y ≥0. Then the following statements are equivalent:
(a) x and y are both optimum solutions.
(b) cx = yb.
(c) y(b −Ax) = 0.
Proof:
The Duality Theorem 3.16 immediately implies the equivalence of (a) and
(b). The equivalence of (b) and (c) follows from y(b−Ax) = yb−y Ax = yb−cx.
2
The property (c) of optimum solutions is often called complementary slack-
ness. Let us write the last result in another form:
Corollary 3.18.
Let min{cx : Ax ≥b, x ≥0} and max{yb : y A ≤c, y ≥0} be
a primal-dual pair of LPs. Let x and y be feasible solutions, i.e. Ax ≥b, y A ≤c
and x, y ≥0. Then the following statements are equivalent:
(a) x and y are both optimum solutions.
(b) cx = yb.
(c) (c −y A)x = 0 and y(b −Ax) = 0.
Proof:
The equivalence of (a) and (b) is obtained by applying the Duality The-
orem 3.16 to max

(−c)x :

−A
−I

x ≤

−b
0

.
To prove that (b) and (c) are equivalent, observe that we have y(b −Ax) ≤
0 ≤(c−y A)x for any feasible solutions x and y, and that y(b−Ax) = (c−y A)x
iff yb = cx.
2
The two conditions in (c) are sometimes called primal and dual complemen-
tary slackness conditions.
The Duality Theorem has many applications in combinatorial optimization.
One reason for its importance is that the optimality of a solution can be proved by
giving a feasible solution of the dual LP with the same objective value. We shall
show now how to prove that an LP is unbounded or infeasible:
Theorem 3.19.
There exists a vector x with Ax ≤b if and only if yb ≥0 for
each vector y ≥0 for which y A = 0.
Proof:
If there is a vector x with Ax ≤b, then yb ≥y Ax = 0 for each y ≥0
with y A = 0.
Consider the LP
−min{1lw : Ax −w ≤b, w ≥0}.
(3.5)
Writing it in standard form we have

3.3 Duality
59
max

0
−1
 
x
w

:

A
−I
0
−I
 
x
w

≤

b
0

.
The dual of this LP is
min

b
0
 
y
z

:

A⊤
0
−I
−I
 
y
z

=

0
−1

, y, z ≥0

,
or, equivalently,
min{yb : y A = 0, 0 ≤y ≤1}.
(3.6)
Since both (3.5) and (3.6) have a solution (x = 0, w = |b|, y = 0), we can apply
Theorem 3.16. So the optimum values of (3.5) and (3.6) are the same. Since the
system Ax ≤b has a solution iff the optimum value of (3.5) is zero, the proof is
complete.
2
So the fact that a linear inequality system Ax ≤b has no solution can be
proved by giving a vector y ≥0 with y A = 0 and yb < 0. We mention two
equivalent formulations of Theorem 3.19:
Corollary 3.20.
There is a vector x ≥0 with Ax ≤b if and only if yb ≥0 for
each vector y ≥0 with y A ≥0.
Proof:
Apply Theorem 3.19 to the system

A
−I

x ≤

b
0

.
2
Corollary 3.21.
(Farkas [1894])
There is a vector x ≥0 with Ax = b if and
only if yb ≥0 for each vector y with y A ≥0.
Proof:
Apply Corollary 3.20 to the system

A
−A

x ≤

b
−b

, x ≥0.
2
Corollary 3.21 is usually known as Farkas’ Lemma. The above results in turn
imply the Duality Theorem 3.16 which is interesting since they have quite easy
direct proofs (in fact they were known before the Simplex Algorithm); see
Exercises 6 and 7.
We have seen how to prove that an LP is infeasible. How can we prove that
an LP is unbounded? The next theorem answers this question.
Theorem 3.22.
If an LP is unbounded, then its dual LP is infeasible. If an LP has
an optimum solution, then its dual also has an optimum solution.
Proof:
The ﬁrst statement follows immediately from Proposition 3.12.
To prove the second statement, suppose that the (primal) LP max{cx : Ax ≤b}
has an optimum solution x∗, but the dual min{yb : y A = c, y ≥0} is infeasible
(it cannot be unbounded due to the ﬁrst statement).
If the dual is infeasible, i.e. there is no y ≥0 with A⊤y = c, we apply Farkas’
Lemma (Corollary 3.21) to get a vector z with zA⊤≥0 and zc < 0. But then x∗−z

60
3. Linear Programming
is feasible for the primal, because A(x∗−z) = Ax∗−Az ≤b. The observation
c(x∗−z) > cx∗therefore contradicts the optimality of x∗.
2
So there are four cases for a primal-dual pair of LPs: either both have an
optimum solution (in which case the optimum values are the same), or one is
infeasible and the other one is unbounded, or both are infeasible.
The following important fact will often be used:
Theorem 3.23.
Let P = {x ∈Rn : Ax ≤b} be a polyhedron and z ̸∈P.
Then there exists a separating hyperplane, i.e. there is a vector c ∈Rn with
cz > max{cx : Ax ≤b}.
Proof:
Since z ̸∈P, {x : Ax ≤b, I x ≤z, −I x ≤−z} is empty. So by Theorem
3.19, there are vectors y, λ, µ ≥0 with y A+(λ−µ)I = 0 and yb+(λ−µ)z < 0.
Then with c := µ −λ we have cz > yb ≥y(Ax) = (y A)x = cx for all x ∈P.
2
Farkas’ Lemma also enables us to prove that each ﬁnitely generated cone is
polyhedral:
Theorem 3.24.
(Minkowski [1896], Weyl [1935]) A cone is polyhedral if and
only if it is ﬁnitely generated.
Proof:
The only-if direction is given by Lemma 3.11. So consider the cone C
generated by a1, . . . , at. We have to show that C is polyhedral. Let A be the
matrix whose rows are a1, . . . , at.
By Lemma 3.11, the cone D := {x : Ax ≤0} is generated by some vectors
b1, . . . , bs. Let B be the matrix whose rows are b1, . . . , bs. We prove that C =
{x : Bx ≤0}.
As bjai = aibj ≤0 for all i and j, we have C ⊆{x : Bx ≤0}. Now suppose
there is a vector w /∈C with Bw ≤0. w ̸∈C means that there is no v ≥0
such that A⊤v = w. By Farkas’ Lemma (Corollary 3.21) this means that there
is a vector y with yw < 0 and Ay ≥0. So −y ∈D. Since D is generated by
b1, . . . , bs we have −y = zB for some z ≥0. But then 0 < −yw = zBw ≤0, a
contradiction.
2
3.4 Convex Hulls and Polytopes
In this section we collect some more facts on polytopes. In particular, we show
that polytopes are precisely those sets that are the convex hull of a ﬁnite number
of points. We start by recalling some basic deﬁnitions:
Deﬁnition 3.25.
Given vectors x1, . . . , xk ∈Rn and λ1, . . . , λk ≥0 with k
i=1 λi
= 1, we call x = k
i=1 λixi a convex combination of x1, . . . , xk. A set X ⊆Rn
is convex if λx + (1 −λ)y ∈X for all x, y ∈X and λ ∈[0, 1]. The convex hull
conv(X) of a set X is deﬁned as the set of all convex combinations of points in X.
An extreme point of a set X is an element x ∈X with x /∈conv(X \ {x}).

3.4 Convex Hulls and Polytopes
61
So a set X is convex if and only if all convex combinations of points in X are
again in X. The convex hull of a set X is the smallest convex set containing X.
Moreover, the intersection of convex sets is convex. Hence polyhedra are convex.
Now we prove the “ﬁnite basis theorem for polytopes”, a fundamental result which
seems to be obvious but is not trivial to prove directly:
Theorem 3.26.
(Minkowski [1896], Steinitz [1916], Weyl [1935]) A set P is a
polytope if and only if it is the convex hull of a ﬁnite set of points.
Proof:
(Schrijver [1986]) Let P = {x ∈Rn : Ax ≤b} be a nonempty polytope.
Obviously,
P =

x :

x
1

∈C

, where C =

x
λ

∈Rn+1 : λ ≥0, Ax −λb ≤0

.
C is a polyhedral cone, so by Theorem 3.24 it is generated by ﬁnitely many
nonzero vectors, say by

x1
λ1

, . . . ,

xk
λk

. Since P is bounded, all λi are
nonzero; w.l.o.g. all λi are 1. So x ∈P if and only if

x
1

= µ1

x1
1

+ · · · + µk

xk
1

for some µ1, . . . , µk ≥0. In other words, P is the convex hull of x1, . . . , xk.
Now let P be the convex hull of x1, . . . , xk ∈Rn. Then x ∈P if and only
if

x
1

∈C, where C is the cone generated by

x1
1

, . . . ,

xk
1

. By
Theorem 3.24, C is polyhedral, so
C =

x
λ

: Ax + bλ ≤0

.
We conclude that P = {x ∈Rn : Ax + b ≤0}.
2
Corollary 3.27.
A polytope is the convex hull of its vertices.
Proof:
Let P be a polytope. By Theorem 3.26, the convex hull of its vertices
is a polytope Q. Obviously Q ⊆P. Suppose there is a point z ∈P \ Q. Then,
by Theorem 3.23, there is a vector c with cz > max{cx : x ∈Q}. The supporting
hyperplane {x : cx = max{cy : y ∈P}} of P deﬁnes a face of P containing no
vertex. This is impossible by Corollary 3.9.
2
The previous two and the following result are the starting point of polyhedral
combinatorics; they will be used very often in this book. For a given ground set
E and a subset X ⊆E, the incidence vector of X (with respect to E) is deﬁned
as the vector x ∈{0, 1}E with xe = 1 for e ∈X and xe = 0 for e ∈E \ X.

62
3. Linear Programming
Corollary 3.28.
Let (E, F) be a set system, P the convex hull of the incidence
vectors of the elements of F, and c : E →R. Then max{cx : x ∈P} = max{c(X) :
X ∈F}.
Proof:
Since max{cx : x ∈P} ≥max{c(X) : X ∈F} is trivial, let x be an
optimum solution of max{cx : x ∈P} (note that P is a polytope by Theorem 3.26).
By deﬁnition of P, x is a convex combination of incidence vectors y1, . . . , yk of
elements of F: x = k
i=1 λi yi for some λ1, . . . , λk ≥0. Since cx = k
i=1 λicyi,
we have cyi ≥cx for at least one i ∈{1, . . . , k}. This yi is the incidence vector
of a set Y ∈F with c(Y) = cyi ≥cx.
2
Exercises
1. A set of vectors x1, . . . , xk is called afﬁnely independent if there is no λ ∈
Rk \ {0} with λ⊤1l = 0 and k
i=1 λixi = 0. Let ∅̸= X ⊆Rn. Show that the
maximum cardinality of an afﬁnely independent set of elements of X equals
dim X + 1.
2. Let P be a polyhedron. Prove that the dimension of any facet of P is one less
than the dimension of P.
3. Formulate the dual of the LP formulation (1.1) of the Job Assignment Prob-
lem. Show how to solve the primal and the dual LP in the case when there
are only two jobs (by a simple algorithm).
4. Let G be a digraph, c : E(G) →R+, E1, E2 ⊆E(G), and s, t ∈V (G).
Consider the following linear program
min

e∈E(G)
c(e)ye
s.t.
ye
≥
zw −zv
(e = (v, w) ∈E(G))
zt −zs
=
1
ye
≥
0
(e ∈E1)
ye
≤
0
(e ∈E2).
Prove that there is an optimum solution (y, z) and s ∈X ⊆V (G) \ {t} with
ye = 1 for e ∈δ+(X), ye = −1 for e ∈δ−(X) \ E1, and ye = 0 for all other
edges e.
Hint: Consider the complementary slackness conditions for the edges entering
or leaving {v ∈V (G) : zv ≤zs}.
5. Let Ax ≤b be a linear inequality system in n variables. By multiplying each
row by a positive constant we may assume that the ﬁrst column of A is a
vector with entries 0, −1 and 1 only. So can write Ax ≤b equivalently as
a′
ix′
≤
bi
(i = 1, . . . , m1),
−x1 + a′
jx′
≤
bj
( j = m1 + 1, . . . , m2),
x1 + a′
kx′
≤
bk
(k = m2 + 1, . . . , m),

References
63
where x′ = (x2, . . . , xn) and a′
1, . . . , a′
m are the rows of A without the ﬁrst
entry. Then one can eliminate x1: Prove that Ax ≤b has a solution if and
only if the system
a′
ix′
≤
bi
(i = 1, . . . , m1),
a′
jx′ −bj
≤
bk −a′
kx′
( j = m1 + 1, . . . , m2, k = m2 + 1, . . . , m)
has a solution. Show that this technique, when iterated, leads to an algorithm
for solving a linear inequality system Ax ≤b (or proving infeasibility).
Note: This method is known as Fourier-Motzkin elimination because it was
proposed by Fourier and studied by Motzkin [1936]. One can prove that it is
not a polynomial-time algorithm.
6. Use Fourier-Motzkin elimination (Exercise 5) to prove Theorem 3.19 directly.
(Kuhn [1956])
7. Show that Theorem 3.19 implies the Duality Theorem 3.16.
8. Prove the decomposition theorem for polyhedra: Any polyhedron P can be
written as P = {x + c : x ∈X, c ∈C}, where X is a polytope and C is a
polyhedral cone.
(Motzkin [1936])
9.
∗
Let P be a rational polyhedron and F a face of P. Show that
{c : cz = max {cx : x ∈P} for all z ∈F}
is a rational polyhedral cone.
10. Prove Carath´eodory’s theorem:
If X ⊆Rn and y ∈conv(X), then there are x1, . . . , xn+1 ∈X such that
y ∈conv({x1, . . . , xn+1}).
(Carath´eodory [1911])
11. Prove the following extension of Carath´eodory’s theorem (Exercise 10):
If X ⊆Rn and y, z ∈conv(X), then there are x1, . . . , xn ∈X such that
y ∈conv({z, x1, . . . , xn}).
12. Prove that the extreme points of a polyhedron are precisely its vertices.
13. Let P be a nonempty polytope. Consider the graph G(P) whose vertices are
the vertices of P and whose edges correspond to the 1-dimensional faces of
P. Let x be any vertex of P, and c a vector with c⊤x < max{c⊤z : z ∈P}.
Prove that then there is a neighbour y of x in G(P) with c⊤x < c⊤y.
14.
∗
Use Exercise 13 to prove that G(P) is n-connected for any n-dimensional
polytope P (n ≥1).
References
General Literature:
Chv´atal, V. [1983]: Linear Programming. Freeman, New York 1983
Padberg, M. [1995]: Linear Optimization and Extensions. Springer, Berlin 1995
Schrijver, A. [1986]: Theory of Linear and Integer Programming. Wiley, Chichester 1986

64
3. Linear Programming
Cited References:
Avis, D., and Chv´atal, V. [1978]: Notes on Bland’s pivoting rule. Mathematical Program-
ming Study 8 (1978), 24–34
Bland, R.G. [1977]: New ﬁnite pivoting rules for the simplex method. Mathematics of
Operations Research 2 (1977), 103–107
Borgwardt, K.-H. [1982]: The average number of pivot steps required by the simplex
method is polynomial. Zeitschrift f¨ur Operations Research 26 (1982), 157–177
Carath´eodory, C. [1911]: ¨Uber den Variabilit¨atsbereich der Fourierschen Konstanten von
positiven harmonischen Funktionen. Rendiconto del Circolo Matematico di Palermo 32
(1911), 193–217
Dantzig, G.B. [1951]: Maximization of a linear function of variables subject to linear
inequalities. In: Activity Analysis of Production and Allocation (T.C. Koopmans, ed.),
Wiley, New York 1951, pp. 359–373
Dantzig, G.B., Orden, A., and Wolfe, P. [1955]: The generalized simplex method for min-
imizing a linear form under linear inequality restraints. Paciﬁc Journal of Mathematics
5 (1955), 183–195
Farkas, G. [1894]: A Fourier-f´ele mechanikai elv alkalmaz´asai. Mathematikai ´es Term´esz-
ettudom´anyi ´Ertesit¨o 12 (1894), 457–472
Gale, D., Kuhn, H.W., and Tucker, A.W. [1951]: Linear programming and the theory of
games. In: Activity Analysis of Production and Allocation (T.C. Koopmans, ed.), Wiley,
New York 1951, pp. 317–329
Hoffman, A.J., and Kruskal, J.B. [1956]: Integral boundary points of convex polyhedra. In:
Linear Inequalities and Related Systems; Annals of Mathematical Study 38 (H.W. Kuhn,
A.W. Tucker, eds.), Princeton University Press, Princeton 1956, pp. 223–246
Klee, V., and Minty, G.J. [1972]: How good is the simplex algorithm? In: Inequalities III
(O. Shisha, ed.), Academic Press, New York 1972, pp. 159–175
Kuhn, H.W. [1956]: Solvability and consistency for linear equations and inequalities. The
American Mathematical Monthly 63 (1956), 217–232
Minkowski, H. [1896]: Geometrie der Zahlen. Teubner, Leipzig 1896
Motzkin, T.S. [1936]: Beitr¨age zur Theorie der linearen Ungleichungen (Dissertation).
Azriel, Jerusalem 1936
von Neumann, J. [1947]: Discussion of a maximum problem. Working paper. Published in:
John von Neumann, Collected Works; Vol. VI (A.H. Taub, ed.), Pergamon Press, Oxford
1963, pp. 27–28
Steinitz, E. [1916]: Bedingt konvergente Reihen und konvexe Systeme. Journal f¨ur die reine
und angewandte Mathematik 146 (1916), 1–52
Weyl, H. [1935]: Elementare Theorie der konvexen Polyeder. Commentarii Mathematici
Helvetici 7 (1935), 290–306

4. Linear Programming Algorithms
There are basically three types of algorithms for Linear Programming: the Sim-
plex Algorithm (see Section 3.2), interior point algorithms, and the Ellipsoid
Method.
Each of these has a disadvantage: In contrast to the other two, so far no variant
of the Simplex Algorithm has been shown to have a polynomial running time.
In Sections 4.4 and 4.5 we present the Ellipsoid Method and prove that it leads
to a polynomial-time algorithm for Linear Programming. However, the Ellip-
soid Method is too inefﬁcient to be used in practice. Interior point algorithms
and, despite its exponential worst-case running time, the Simplex Algorithm are
far more efﬁcient, and they are both used in practice to solve LPs. In fact, both
the Ellipsoid Method and interior point algorithms can be used for more gen-
eral convex optimization problems, e.g. for so-called semideﬁnite programming
problems. We shall not go into details here.
An advantage of the Simplex Algorithm and the Ellipsoid Method is that
they do not require the LP to be given explicitly. It sufﬁces to have an oracle (a
subroutine) which decides whether a given vector is feasible and, if not, returns a
violated constraint. We shall discuss this in detail with respect to the Ellipsoid
Method in Section 4.6, because it implies that many combinatorial optimization
problems can be solved in polynomial time; for some problems this is in fact
the only known way to show polynomial solvability. This is the reason why we
discuss the Ellipsoid Method but not interior point algorithms in this book.
A prerequisite for polynomial-time algorithms is that there exists an optimum
solution that has a binary representation whose length is bounded by a polyno-
mial in the input size. We prove this in Section 4.1. In Sections 4.2 and 4.3 we
review some basic algorithms needed later, including the well-known Gaussian
elimination method for solving systems of equations.
4.1 Size of Vertices and Faces
Instances of Linear Programming are vectors and matrices. Since no strongly
polynomial-time algorithm for Linear Programming is known we have to restrict
attention to rational instances when analyzing the running time of algorithms. We
assume that all numbers are coded in binary. To estimate the size (number of bits)
in this representation we deﬁne size(n) := 1+⌈log(|n|+1)⌉for integers n ∈Z and

66
4. Linear Programming Algorithms
size(r) := size(p) + size(q) for rational numbers r = p
q , where p, q are relatively
prime integers. For vectors x = (x1, . . . , xn) ∈Qn we store the components and
have size(x) := n + size(x1) + . . . + size(xn). For a matrix A ∈Qm×n with entries
ai j we have size(A) := mn + 
i, j size(ai j).
Of course these precise values are a somewhat random choice, but remember
that we are not really interested in constant factors. For polynomial-time algorithms
it is important that the sizes of numbers do not increase too much by elementary
arithmetic operations. We note:
Proposition 4.1.
If r1, . . . ,rn are rational numbers, then
size(r1 · · ·rn)
≤
size(r1) + · · · + size(rn);
size(r1 + · · · + rn)
≤
2(size(r1) + · · · + size(rn)).
Proof:
For integers s1, . . . , sn we obviously have size(s1 · · · sn) ≤size(s1) +
· · · + size(sn) and size(s1 + · · · + sn) ≤size(s1) + · · · + size(sn).
Let now ri = pi
qi , where pi and qi are nonzero integers (i = 1, . . . , n). Then
size(r1 · · ·rn) = size(p1 · · · pn) + size(q1 · · · qn) ≤size(r1) + · · · + size(rn).
For the second statement, observe that the denominator q1 · · · qn has size
at most size(q1) + · · · + size(qn). The numerator is the sum of the num-
bers q1 · · · qi−1 piqi+1 · · · qn (i = 1, . . . , n), so its absolute value is at most
(|p1| + · · · + |pn|)|q1 · · · qn|. Therefore the size of the numerator is at most
size(r1) + · · · + size(rn).
2
The ﬁrst part of this proposition also implies that we can often assume w.l.o.g.
that all numbers in a problem instance are integers, since otherwise we can multiply
each of them with the product of all denominators. For addition and inner product
of vectors we have:
Proposition 4.2.
If x, y ∈Qn are rational vectors, then
size(x + y)
≤
2(size(x) + size(y));
size(x
⊤y)
≤
2(size(x) + size(y)).
Proof:
Using Proposition 4.1 we have size(x + y) = n + n
i=1 size(xi + yi) ≤
n + 2 n
i=1 size(xi) + 2 n
i=1 size(yi) = 2(size(x) + size(y)) −n and size(x ⊤y) =
size
n
i=1 xi yi

≤2 n
i=1 size(xi yi) ≤2 n
i=1 size(xi) + 2 n
i=1 size(yi) =
2(size(x) + size(y)) −4n.
2
Even under more complicated operations the numbers involved do not grow
fast. Recall that the determinant of a matrix A = (ai j)1≤i, j≤n is deﬁned by
det A :=

π∈Sn
sgn(π)
n'
i=1
ai,π(i),
(4.1)
where Sn is the set of all permutations of {1, . . . , n} and sgn(π) is the sign of the
permutation π (deﬁned to be 1 if π can be obtained from the identity map by an
even number of transpositions, and −1 otherwise).

4.1 Size of Vertices and Faces
67
Proposition 4.3.
For any matrix A ∈Qm×n we have size(det A) ≤2 size(A).
Proof:
We write ai j = pi j
qi j with relatively prime integers pi j, qi j. Now let det A =
p
q where p and q are relatively prime integers. Then |det A| ≤(
i, j(|pi j| + 1) and
|q| ≤(
i, j |qi j|. We obtain size(q) ≤size(A) and, using |p| = |det A||q| ≤
(
i, j(|pi j| + 1)|qi j|,
size(p) ≤

i, j
(size(pi j) + 1 + size(qi j)) = size(A).
2
With this observation we can prove:
Theorem 4.4.
Suppose the rational LP max{cx : Ax ≤b} has an optimum solu-
tion. Then it also has an optimum solution x with size(x) ≤4n(size(A) + size(b)),
with components of size at most 4(size(A) + size(b)). If b = ei or b = −ei for
some unit vector ei, then there is a nonsingular submatrix A′ of A and an optimum
solution x with size(x) ≤4n size(A′).
Proof:
By Corollary 3.4, the maximum is attained in a face F of {x : Ax ≤b}.
Let F′ ⊆F be a minimal face. By Proposition 3.8, F′ = {x : A′x = b′} for some
subsystem A′x ≤b′ of Ax ≤b. W.l.o.g., we may assume that the rows of A′ are
linearly independent. We then take a maximal set of linear independent columns
(call this matrix A′′) and set all other components to zero. Then x = (A′′)−1b′, ﬁlled
up with zeros, is an optimum solution to our LP. By Cramer’s rule the entries of
x are given by xj = det A′′′
det A′′ , where A′′′ arises from A′′ by replacing the j-th column
by b′. By Proposition 4.3 we obtain size(x) ≤n + 2n(size(A′′′) + size(A′′)) ≤
4n(size(A′′) + size(b′)). If b = ±ei then | det(A′′′)| is the absolute value of a
subdeterminant of A′′.
2
The encoding length of the faces of a polytope given by its vertices can be
estimated as follows:
Lemma 4.5.
Let P ⊆Rn be a rational polytope and T ∈N such that size(x) ≤T
for each vertex x. Then P = {x : Ax ≤b} for some inequality system Ax ≤b,
each of whose inequalities ax ≤β satisﬁes size(a) + size(β) ≤75n2T .
Proof:
First assume that P is full-dimensional. Let F = {x ∈P : ax = β} be a
facet of P, where P ⊆{x : ax ≤β}.
Let y1, . . . , yt be the vertices of F (by Proposition 3.5 they are also vertices
of P). Let c be the solution of Mc = e1, where M is a t × n-matrix whose
i-th row is yi −y1 (i = 2, . . . , t) and whose ﬁrst row is some unit vector that
is linearly independent of the other rows. Observe that rank(M) = n (because
dim F = n −1). So we have c⊤= κa for some κ ∈R \ {0}.
By Theorem 4.4 size(c) ≤4n size(M′), where M′ is a nonsingular n × n-
submatrix of M. By Proposition 4.2 we have size(M′) ≤4nT and size(c⊤y1) ≤
2(size(c)+size(y1)). So the inequality c⊤x ≤δ (or c⊤x ≥δ if κ < 0), where δ :=
c⊤y1 = κβ, satisﬁes size(c) + size(δ) ≤3 size(c) + 2T ≤48n2T + 2T ≤50n2T .
Collecting these inequalities for all facets F yields a description of P.

68
4. Linear Programming Algorithms
If P = ∅, the assertion is trivial, so we now assume that P is neither full-
dimensional nor empty. Let V be the set of vertices of P. For s = (s1, . . . , sn) ∈
{−1, 1}n let Ps be the convex hull of V ∪{x + siei : x ∈V, i = 1, . . . , n}.
Each Ps is a full-dimensional polytope (Theorem 3.26), and the size of any of its
vertices is at most T + n (cf. Corollary 3.27). By the above, Ps can be described
by inequalities of size at most 50n2(T + n) ≤75n2T (note that T ≥2n). Since
P = )
s∈{−1,1}n Ps, this completes the proof.
2
4.2 Continued Fractions
When we say that the numbers occurring in a certain algorithm do not grow too
fast, we often assume that for each rational p
q the numerator p and the denominator
q are relatively prime. This assumption causes no problem if we can easily ﬁnd
the greatest common divisor of two natural numbers. This is accomplished by one
of the oldest algorithms:
Euclidean Algorithm
Input:
Two natural numbers p and q.
Output:
The greatest common divisor d of p and q, i.e. p
d and q
d are relatively
prime integers.
1⃝
While p > 0 and q > 0 do:
If p < q then set q := q −⌊q
p⌋p else set p := p −⌊p
q ⌋q.
2⃝
Return d := max{p, q}.
Theorem 4.6.
The Euclidean Algorithm works correctly. The number of iter-
ations is at most size(p) + size(q).
Proof:
The correctness follows from the fact that the set of common divisors
of p and q does not change throughout the algorithm, until one of the numbers
becomes zero. One of p or q is reduced by at least a factor of two in each iteration,
hence there are at most log p + log q + 1 iterations.
2
Since no number occurring in an intermediate step is greater than p and q, we
have a polynomial-time algorithm.
A similar algorithm is the so-called Continued Fraction Expansion. This
can be used to approximate any number by a rational number whose denominator is
not too large. For any positive real number x we deﬁne x0 := x and xi+1 :=
1
xi−⌊xi⌋
for i = 1, 2, . . ., until xk ∈N for some k. Then we have
x = x0 = ⌊x0⌋+ 1
x1
= ⌊x0⌋+
1
⌊x1⌋+ 1
x2
= ⌊x0⌋+
1
⌊x1⌋+
1
⌊x2⌋+ 1
x3
= · · ·

4.2 Continued Fractions
69
We claim that this sequence is ﬁnite if and only if x is rational. One direction
follows immediately from the observation that xi+1 is rational if and only if xi
is rational. The other direction is also easy: If x =
p
q , the above procedure is
equivalent to the Euclidean algorithm applied to p and q. This also shows that
for a given rational number p
q the (ﬁnite) sequence x1, x2, . . . , xk as above can be
computed in polynomial time. The following algorithm is almost identical to the
Euclidean Algorithm except for the computation of the numbers gi and hi; we
shall prove that the sequence

gi
hi

i∈N converges to x.
Continued Fraction Expansion
Input:
A rational number x = p
q .
Output:
The sequence

xi = pi
qi

i=0,1,... with x0 = p
q and xi+1 :=
1
xi−⌊xi⌋.
1⃝
Set i := 0, p0 := p and q0 := q.
Set g−2 := 0, g−1 := 1, h−2 := 1, and h−1 := 0.
2⃝
While qi ̸= 0 do:
Set ai := ⌊pi
qi ⌋.
Set gi := aigi−1 + gi−2.
Set hi := aihi−1 + hi−2.
Set qi+1 := pi −aiqi.
Set pi+1 := qi.
Set i := i + 1.
We claim that the sequence gi
hi yields good approximations of x. Before we
can prove this, we need some preliminary observations:
Proposition 4.7.
The following statements hold for all iterations i in the above
algorithm:
(a) ai ≥1 (except possibly for i = 0) and hi ≥hi−1.
(b) gi−1hi −gihi−1 = (−1)i.
(c)
pigi−1 + qigi−2
pihi−1 + qihi−2
= x.
(d)
gi
hi ≤x if i is even and gi
hi ≥x if i is odd.
Proof:
(a) is obvious. (b) is easily shown by induction: For i = 0 we have
gi−1hi −gihi−1 = g−1h0 = 1, and for i ≥1 we have
gi−1hi−gihi−1 = gi−1(aihi−1+hi−2)−hi−1(aigi−1+gi−2) = gi−1hi−2−hi−1gi−2.
(c) is also proved by induction: For i = 0 we have
pigi−1 + qigi−2
pihi−1 + qihi−2
= pi · 1 + 0
0 + qi · 1 = x.

70
4. Linear Programming Algorithms
For i ≥1 we have
pigi−1 + qigi−2
pihi−1 + qihi−2
=
qi−1(ai−1gi−2 + gi−3) + (pi−1 −ai−1qi−1)gi−2
qi−1(ai−1hi−2 + hi−3) + (pi−1 −ai−1qi−1)hi−2
=
qi−1gi−3 + pi−1gi−2
qi−1hi−3 + pi−1hi−2
.
We ﬁnally prove (d). We note
g−2
h−2 = 0 < x < ∞=
g−1
h−1 and proceed by
induction. The induction step follows easily from the fact that the function f (α) :=
αgi−1+gi−2
αhi−1+hi−2 is monotone for α > 0, and f ( pi
qi ) = x by (c).
2
Theorem 4.8.
(Khintchine [1956])
Given a rational number α and a natural
number n, a rational number β with denominator at most n such that |α −β| is
minimum can be found in polynomial time (polynomial in size(n) + size(α)).
Proof:
We run the Continued Fraction Expansion with x := α. If the al-
gorithm stops with qi = 0 and hi−1 ≤n, we can set β =
gi−1
hi−1 = α by
Proposition 4.7(c). Otherwise let i be the last index with hi ≤n, and let t be
the maximum integer such that thi + hi−1 ≤n (cf. Proposition 4.7(a)). Since
ai+1hi + hi−1 = hi+1 > n, we have t < ai+1. We claim that
y := gi
hi
or
z := tgi + gi−1
thi + hi−1
is an optimum solution. Both numbers have denominators at most n.
If i is even, then y ≤x < z by Proposition 4.7(d). Similarly, if i is odd,
we have y ≥x > z. We show that any rational number
p
q between y and z has
denominator greater than n.
Observe that
|z −y| = |higi−1 −hi−1gi|
hi(thi + hi−1)
=
1
hi(thi + hi−1)
(using Proposition 4.7(b)). On the other hand,
|z −y| =
z −p
q
 +

p
q −y
 ≥
1
(thi + hi−1)q +
1
hiq = hi−1 + (t + 1)hi
qhi(thi + hi−1) ,
so q ≥hi−1 + (t + 1)hi > n.
2
The above proof is from the book of Gr¨otschel, Lov´asz and Schrijver [1988],
which also contains important generalizations.
4.3 Gaussian Elimination
The most important algorithm in Linear Algebra is the so-called Gaussian elimi-
nation. It has been applied by Gauss but was known much earlier (see Schrijver
[1986] for historical notes). Gaussian elimination is used to determine the rank of

4.3 Gaussian Elimination
71
a matrix, to compute the determinant and to solve a system of linear equations. It
occurs very often as a subroutine in linear programming algorithms; e.g. in 1⃝of
the Simplex Algorithm.
Given a matrix A ∈Qm×n, our algorithm for Gaussian Elimination works with
an extended matrix Z = ( B
C ) ∈Qm×(n+m); initially B = A and C = I. The
algorithm transforms B to the form

I
R
0
0

by the following elementary oper-
ations: permuting rows and columns, adding a multiple of one row to another row,
and (in the ﬁnal step) multiplying rows by nonzero constants. At each iteration C
is modiﬁed accordingly, such that the property C ˜A = B is maintained throughout
where ˜A results from A by permuting rows and columns.
The ﬁrst part of the algorithm, consisting of 2⃝and 3⃝, transforms B to an
upper triangular matrix. Consider for example the matrix Z after two iterations; it
has the form
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
z11 ̸=0
z12
z13
·
·
·
z1n
1
0
0
·
·
·
0
0
z22 ̸=0
z23
·
·
·
z2n
z2,n+1
1
0
·
·
·
0
0
0
z33
·
·
·
z3n
z3,n+1
z3,n+2
1
0
·
·
0
·
·
·
·
·
·
0
·
·
·
·
·
·
·
·
I
·
·
·
·
·
·
·
·
0
0
0
zm3
·
·
·
zmn
zm,n+1
zm,n+2
0
·
·
0
1
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
If z33 ̸= 0, then the next step just consists of subtracting zi3
z33 times the third row
from the i-th row, for i = 4, . . . , m. If z33 = 0 we ﬁrst exchange the third row
and/or the third column with another one. Note that if we exchange two rows, we
have to exchange also the two corresponding columns of C in order to maintain the
property C ˜A = B. To have ˜A available at each point we store the permutations of
the rows and columns in variables row(i), i = 1, . . . , m and col( j), j = 1, . . . , n.
Then ˜A = (Arow(i),col( j))i∈{1,...,m}, j∈{1,...,n}.
The second part of the algorithm, consisting of 4⃝and 5⃝, is simpler since no
rows or columns are exchanged anymore.
Gaussian Elimination
Input:
A matrix A = (ai j) ∈Qm×n.
Output:
Its rank r,
a maximal nonsingular submatrix A′ = (arow(i),col( j))i, j∈{1,...,r} of A,
its determinant d = det A′, and its inverse (A′)−1 = (zi,n+ j)i, j∈{1,...,r}.
1⃝
Set r := 0 and d := 1.
Set zi j := ai j, row(i) := i and col( j) := j (i = 1, . . . , m, j = 1, . . . , n).
Set zi,n+ j := 0 and zi,n+i := 1 for 1 ≤i, j ≤m, i ̸= j.

72
4. Linear Programming Algorithms
2⃝
Let p ∈{r + 1, . . . , m} and q ∈{r + 1, . . . , n} with zpq ̸= 0. If no such p
and q exist, then go to 4⃝.
Set r := r + 1.
If p ̸= r then exchange zpj and zr j ( j = 1, . . . , n + m), exchange zi,n+p
and zi,n+r (i = 1, . . . , m), and exchange row(p) and row(r).
If q ̸= r then exchange ziq and zir (i = 1, . . . , m), and exchange col(q)
and col(r).
3⃝
Set d := d · zrr.
For i := r + 1 to m do:
For j := r to n + r do: zi j := zi j −zir
zrr zr j.
Go to 2⃝.
4⃝
For k := r down to 1 do:
For i := 1 to k −1 do:
For j := k to n + r do zi j := zi j −zik
zkk zkj.
5⃝
For k := 1 to r do:
For j := 1 to n + r do zkj := zkj
zkk .
Theorem 4.9.
Gaussian Elimination works correctly and terminates after
O(mnr) steps.
Proof:
First observe that each time before 2⃝we have zii ̸= 0 for i ∈{1, . . . ,r}
and zi j = 0 for all j ∈{1, . . . ,r} and i ∈{ j + 1, . . . , m}. Hence
det

(zi j)i, j∈{1,2,...,r}

= z11z22 · · · zrr = d ̸= 0.
Since adding a multiple of one row to another row of a square matrix does not
change the value of the determinant (this well-known fact follows directly from
the deﬁnition (4.1)) we have
det

(zi j)i, j∈{1,2,...,r}

= det

(arow(i),col( j))i, j∈{1,2,...,r}

at any stage before 5⃝, and hence the determinant d is computed correctly. A′
is a nonsingular r × r-submatrix of A. Since (zi j)i∈{1,...,m}, j∈{1,...,n} has rank r at
termination and the operations did not change the rank, A has also rank r.
Moreover, m
j=1 zi,n+ jarow( j),col(k) = zik for all i ∈{1, . . . , m} and k ∈
{1, . . . , n} (i.e. C ˜A = B in our above notation) holds throughout. (Note that
for j = r + 1, . . . , m we have at any stage zj j = 1 and zi j = 0 for i ̸= j.) Since

(zi j)i, j∈{1,2,...,r}

is the unit matrix at termination this implies that (A′)−1 is also
computed correctly. The number of steps is obviously O(rmn + r2(n + r)) =
O(mnr).
2
In order to prove that Gaussian Elimination is a polynomial-time algorithm
we have to guarantee that all numbers that occur are polynomially bounded by the
input size. This is not trivial but can be shown:

4.3 Gaussian Elimination
73
Theorem 4.10.
(Edmonds [1967]) GaussianElimination is a polynomial-time
algorithm. Each number occurring in the course of the algorithm can be stored with
O(m(m + n) size(A)) bits.
Proof:
We ﬁrst show that in 2⃝and 3⃝all numbers are 0, 1, or quotients of
subdeterminants of A. First observe that entries zi j with i ≤r or j ≤r are not
modiﬁed anymore. Entries zi j with j > n+r are 0 (if j ̸= n+i) or 1 (if j = n+i).
Furthermore, we have for all s ∈{r + 1, . . . , m} and t ∈{r + 1, . . . , n + m}
zst = det

(zi j)i∈{1,2,...,r,s}, j∈{1,2,...,r,t}

det

(zi j)i, j∈{1,2,...,r}

.
(This follows from evaluating the determinant det

(zi j)i∈{1,2,...,r,s}, j∈{1,2,...,r,t}

along
the last row because zsj = 0 for all s ∈{r + 1, . . . , m} and all j ∈{1, . . . ,r}.)
We have already observed in the proof of Theorem 4.9 that
det

(zi j)i, j∈{1,2,...,r}

= det

(arow(i),col( j))i, j∈{1,2,...,r}

,
because adding a multiple of one row to another row of a square matrix does not
change the value of the determinant. By the same argument we have
det

(zi j)i∈{1,2,...,r,s}, j∈{1,2,...,r,t}

= det

(arow(i),col( j))i∈{1,2,...,r,s}, j∈{1,2,...,r,t}

for s ∈{r + 1, . . . , m} and t ∈{r + 1, . . . , n}. Furthermore,
det

(zi j)i∈{1,2,...,r,s}, j∈{1,2,...,r,n+t}

= det

(arow(i),col( j))i∈{1,2,...,r,s}\{t}, j∈{1,2,...,r}

for all s ∈{r + 1, . . . , m} and t ∈{1, . . . ,r}, which is checked by evaluating the
left-hand side determinant (after 1⃝) along column n + t.
We conclude that at any stage in 2⃝and 3⃝all numbers zi j are 0, 1, or quotients
of subdeterminants of A. Hence, by Proposition 4.3, each number occurring in 2⃝
and 3⃝can be stored with O(size(A)) bits.
Finally observe that 4⃝is equivalent to applying 2⃝and 3⃝again, choosing p
and q appropriately (reversing the order of the ﬁrst r rows and columns). Hence
each number occurring in 4⃝can be stored with O

size

(zi j)i∈{1,...,m}, j∈{1,...,m+n}

bits, which is O(m(m + n) size(A)).
The easiest way to keep the representations of the numbers zi j small enough is
to guarantee that the numerator and denominator of each of these numbers are rel-
atively prime at any stage. This can be accomplished by applying the Euclidean
Algorithm after each computation. This gives an overall polynomial running
time.
2
In fact, we can easily implement Gaussian Elimination to be a strongly
polynomial-time algorithm (Exercise 4).
So we can check in polynomial time whether a set of vectors is linearly in-
dependent, and we can compute the determinant and the inverse of a nonsingular
matrix in polynomial time (exchanging two rows or columns changes just the sign
of the determinant). Moreover we get:

74
4. Linear Programming Algorithms
Corollary 4.11.
Given a matrix A ∈Qm×n and a vector b ∈Qm we can in
polynomial time ﬁnd a vector x ∈Qn with Ax = b or decide that no such vector
exists.
Proof: We compute a maximal nonsingular submatrix A′ = (arow(i),col( j))i, j∈{1,...,r}
of A and its inverse (A′)−1 = (zi,n+ j)i, j∈{1,...,r} by Gaussian Elimination.
Then we set xcol( j) := r
k=1 zj,n+kbrow(k) for j = 1, . . . ,r and xk := 0 for
k /∈{col(1), . . . , col(r)}. We obtain for i = 1, . . .r:
n

j=1
arow(i), jxj
=
r

j=1
arow(i),col( j)xcol( j)
=
r

j=1
arow(i),col( j)
r

k=1
zj,n+kbrow(k)
=
r

k=1
brow(k)
r

j=1
arow(i),col( j)zj,n+k
=
brow(i).
Since the other rows of A with indices not in {row(1), . . . ,row(r)} are linear
combinations of these, either x satisﬁes Ax = b or no vector satisﬁes this system
of equations.
2
4.4 The Ellipsoid Method
In this section we describe the so-called ellipsoid method, developped by Iudin
and Nemirovskii [1976] and Shor [1977] for nonlinear optimization. Khachiyan
[1979] observed that it can be modiﬁed in order to solve LPs in polynomial time.
Most of our presentation is based on (Gr¨otschel, Lov´asz and Schrijver [1981]);
(Bland, Goldfarb and Todd [1981]) and the book of Gr¨otschel, Lov´asz and Schri-
jver [1988], which is also recommended for further study.
The idea of the ellipsoid method is very roughly the following. We look for
either a feasible or an optimum solution of an LP. We start with an ellipsoid which
we know a priori to contain the solutions (e.g. a large ball). At each iteration k,
we check if the center xk of the current ellipsoid is a feasible solution. Otherwise,
we take a hyperplane containing xk such that all the solutions lie on one side of
this hyperplane. Now we have a half-ellipsoid which contains all solutions. We
take the smallest ellipsoid completely containing this half-ellipsoid and continue.
Deﬁnition 4.12.
An ellipsoid is a set E(A, x) = {z ∈Rn : (z −x)⊤A−1(z −x) ≤
1} for some symmetric positive deﬁnite n × n-matrix A.
Note that B(x,r) := E(r2I, x) (with I being the n × n unit matrix) is the
n-dimensional Euclidean ball with center x and radius r.

4.4 The Ellipsoid Method
75
The volume of an ellipsoid E(A, x) is known to be
volume (E(A, x)) =
√
det A volume (B(0, 1))
(see Exercise 7). Given an ellipsoid E(A, x) and a hyperplane {z : az = ax},
the smallest ellipsoid E(A′, x′) containing the half-ellipsoid E′ = {z ∈E(A, x) :
az ≥ax} is called the L¨owner-John ellipsoid of E′ (see Figure 4.1). It can be
computed by the following formulas:
A′
=
n2
n2 −1

A −
2
n + 1bb
⊤

,
x′
=
x +
1
n + 1b,
b
=
1
√
a⊤Aa
Aa.
E(A, x)
{z : az = ax}
E(A′, x′)
x
Fig. 4.1.
One difﬁculty of the ellipsoid method is caused by the square root in the
computation of b. Because we have to tolerate rounding errors, it is necessary to
increase the radius of the next ellipsoid a little bit. Here is an algorithmic scheme
that takes care of this problem:

76
4. Linear Programming Algorithms
Ellipsoid Method
Input:
A number n ∈N, n ≥2. A number N ∈N. x0 ∈Qn and R ∈Q+,
R ≥2.
Output:
An ellipsoid E(AN, xN).
1⃝
Set p := ⌈6N + log(9n3)⌉.
Set A0 := R2I, where I is the n × n unit matrix.
Set k := 0.
2⃝
Choose any ak ∈Qn \ {0}.
3⃝
Set bk :=
1
*
a⊤
k Akak
Akak.
Set xk+1 :≈x∗
k+1 := xk +
1
n + 1bk.
Set Ak+1 :≈A∗
k+1 := 2n2 + 3
2n2

Ak −
2
n + 1bkb
⊤
k

.
(Here :≈means computing the entries up to p decimal places, taking care
that Ak+1 is symmetric).
4⃝
Set k := k + 1.
If k < N then go to 2⃝else stop.
So in each of the N iterations an approximation E(Ak+1, xk+1) of the smallest
ellipsoid containing E(Ak, xk) ∩{z : akz ≥akxk} is computed. Two main issues,
how to obtain the ak and how to choose N, will be addressed in the next section.
But let us ﬁrst prove some lemmas.
Let ||x|| denote the Euclidean norm of vector x, while ||A|| := max{||Ax|| :
||x|| = 1} shall denote the norm of the matrix A. For symmetric matrices, ||A|| is
the maximum absolute value of the eigenvalue and ||A|| = max{x ⊤Ax : ||x|| = 1}.
The ﬁrst lemma says that each Ek := E(Ak, xk) is indeed an ellipsoid. Further-
more, the absolute values of the numbers involved remain smaller than R22N +
2size(x0). Therefore the running time of the Ellipsoid Method is O(n2(p + q))
per iteration, where q = size(ak) + size(R) + size(x0).
Lemma 4.13.
(Gr¨otschel, Lov´asz and Schrijver [1981])
The matrices A0, A1,
. . . , AN are positive deﬁnite. Moreover, for k = 0, . . . , N we have
||xk|| ≤||x0|| + R2k,
||Ak|| ≤R22k
and
||A−1
k || ≤R−24k.
Proof:
We use induction on k. For k = 0 all the statements are obvious. Assume
that they are true for some k ≥0. By a straightforward computation one veriﬁes
that
(A∗
k+1)−1 =
2n2
2n2 + 3

A−1
k
+
2
n −1
aka⊤
k
a⊤
k Akak

.
(4.2)
So (A∗
k+1)−1 is the sum of a positive deﬁnite and a positive semideﬁnite matrix;
thus it is positive deﬁnite. Hence A∗
k+1 is also positive deﬁnite.

4.4 The Ellipsoid Method
77
Note that for positive semideﬁnite matrices A and B we have ||A|| ≤||A+B||.
Therefore
||A∗
k+1|| = 2n2 + 3
2n2

Ak −
2
n + 1bkb
⊤
k

 ≤2n2 + 3
2n2
||Ak|| ≤11
8 R22k.
Since the n ×n all-one matrix has norm n, the matrix Ak+1 −A∗
k+1, each of whose
entries has absolute value at most 2−p, has norm at most n2−p. We conclude
||Ak+1|| ≤||A∗
k+1|| + ||Ak+1 −A∗
k+1|| ≤11
8 R22k + n2−p ≤R22k+1
(here we used the very rough estimate 2−p ≤1
n ).
It is well-known from linear algebra that for any symmetric positive deﬁnite
n × n-matrix A there exists a symmetric positive deﬁnite matrix B with A = BB.
Writing Ak = BB with B = B⊤we obtain
||bk|| =
||Akak||
*
a⊤
k Akak
=
+
a⊤
k A2
kak
a⊤
k Akak
=
+
(Bak)⊤Ak(Bak)
(Bak)⊤(Bak)
≤
*
||Ak|| ≤R2k−1.
Using this (and again the induction hypothesis) we get
||xk+1||
≤
||xk|| +
1
n + 1||bk|| + ||xk+1 −x∗
k+1||
≤
||x0|| + R2k +
1
n + 1 R2k−1 + √n2−p ≤||x0|| + R2k+1.
Using (4.2) and ||aka⊤
k || = a⊤
k ak we compute
(A∗
k+1)−1
≤
2n2
2n2 + 3
A−1
k
 +
2
n −1
a⊤
k ak
a⊤
k Akak

(4.3)
=
2n2
2n2 + 3

A−1
k
 +
2
n −1
a⊤
k B A−1
k Bak
a⊤
k BBak

≤
2n2
2n2 + 3
A−1
k
 +
2
n −1
A−1
k


< n + 1
n −1
A−1
k

≤
3R−24k.
Let λ be the smallest eigenvalue of Ak+1, and let v be a corresponding eigen-
vector with ||v|| = 1. Then – writing A∗
k+1 = CC for a symmetric matrix C – we
have
λ
=
v
⊤Ak+1v = v
⊤A∗
k+1v + v
⊤(Ak+1 −A∗
k+1)v
=
v⊤CCv
v⊤C

A∗
k+1
−1 Cv
+ v
⊤(Ak+1 −A∗
k+1)v
≥
(A∗
k+1)−1−1 −||Ak+1 −A∗
k+1|| > 1
3 R24−k −n2−p ≥R24−(k+1),

78
4. Linear Programming Algorithms
where we used 2−p ≤
1
3n 4−k. Since λ > 0, Ak+1 is positive deﬁnite. Furthermore,
(Ak+1)−1 = 1
λ ≤R−24k+1.
2
Next we show that in each iteration the ellipsoid contains the intersection of
E0 and the previous half-ellipsoid:
Lemma 4.14.
For k = 0, . . . , N −1 we have Ek+1 ⊇{x ∈Ek∩E0 : akx ≥akxk}.
Proof:
Let x ∈Ek ∩E0 with akx ≥akxk. We ﬁrst compute (using (4.2))
(x −x∗
k+1)
⊤(A∗
k+1)−1(x −x∗
k+1)
=
2n2
2n2+3

x −xk −
1
n+1bk
⊤
A−1
k
+
2
n−1
aka⊤
k
a⊤
k Akak
 
x −xk −
1
n+1bk

=
2n2
2n2 + 3

(x −xk)
⊤A−1
k (x −xk) +
2
n −1(x −xk)
⊤aka⊤
k
a⊤
k Akak
(x −xk)
+
1
(n + 1)2

b
⊤
k A−1
k bk +
2
n −1
b⊤
kaka⊤
k bk
a⊤
k Akak

−2(x −xk)⊤
n + 1

A−1
k bk +
2
n −1
aka⊤
k bk
a⊤
k Akak
 
=
2n2
2n2 + 3

(x −xk)
⊤A−1
k (x −xk) +
2
n −1(x −xk)
⊤aka⊤
k
a⊤
k Akak
(x −xk) +
1
(n + 1)2

1 +
2
n −1

−
2
n + 1
(x −xk)⊤ak
*
a⊤
k Akak

1 +
2
n −1

.
Since x ∈Ek, we have (x−xk)⊤A−1
k (x−xk) ≤1. By abbreviating t := a⊤
k (x−xk)
√
a⊤
k Akak
we obtain
(x −x∗
k+1)
⊤(A∗
k+1)−1(x −x∗
k+1) ≤
2n2
2n2 + 3

1 +
2
n −1t2 +
1
n2 −1 −
2
n −1t

.
Since b⊤
k A−1
k bk = 1 and b⊤
k A−1
k (x −xk) = t, we have
1
≥
(x −xk)
⊤A−1
k (x −xk)
=
(x −xk −tbk)
⊤A−1
k (x −xk −tbk) + t2
≥
t2,
because A−1
k
is positive deﬁnite. So (using akx ≥akxk) we have 0 ≤t ≤1 and
obtain
(x −x∗
k+1)
⊤(A∗
k+1)−1(x −x∗
k+1) ≤
2n4
2n4 + n2 −3.

4.4 The Ellipsoid Method
79
It remains to estimate the rounding error
Z
:=
(x −xk+1)
⊤(Ak+1)−1(x −xk+1) −(x −x∗
k+1)
⊤(A∗
k+1)−1(x −x∗
k+1)

≤
(x −xk+1)
⊤(Ak+1)−1(x∗
k+1 −xk+1)

+
(x∗
k+1 −xk+1)
⊤(Ak+1)−1(x −x∗
k+1)

+
(x −x∗
k+1)
⊤
(Ak+1)−1 −(A∗
k+1)−1
(x −x∗
k+1)

≤
||x −xk+1|| ||(Ak+1)−1|| ||x∗
k+1 −xk+1||
+||x∗
k+1 −xk+1|| ||(Ak+1)−1|| ||x −x∗
k+1||
+||x −x∗
k+1||2 ||(Ak+1)−1|| ||(A∗
k+1)−1|| ||A∗
k+1 −Ak+1||.
Using Lemma 4.13 and x ∈E0 we get ||x −xk+1|| ≤||x −x0|| + ||xk+1 −x0|| ≤
R + R2N and ||x −x∗
k+1|| ≤||x −xk+1|| + √n2−p ≤R2N+1. We also use (4.3)
and obtain
Z
≤
2

R2N+1 
R−24N √n2−p
+

R24N+1 
R−24N 
3R−24N−1 
n2−p
=
4R−123N√n2−p + 3R−226Nn2−p
≤
26Nn2−p
≤
1
9n2 ,
by deﬁnition of p. Altogether we have
(x −xk+1)
⊤(Ak+1)−1(x −xk+1) ≤
2n4
2n4 + n2 −3 +
1
9n2 ≤1.
2
The volumes of the ellipsoids decrease by a constant factor in each iteration:
Lemma 4.15.
For k = 0, . . . , N −1 we have volume (Ek+1)
volume (Ek) < e−1
5n .
Proof:
(Gr¨otschel, Lov´asz and Schrijver [1988])
We write
volume (Ek+1)
volume (Ek)
=
+
det Ak+1
det Ak
=
+
det A∗
k+1
det Ak
+
det Ak+1
det A∗
k+1
and estimate the two factors independently. First observe that
det A∗
k+1
det Ak
=
2n2 + 3
2n2
n
det

I −
2
n + 1
aka⊤
k Ak
a⊤
k Akak

.
The matrix aka⊤
k Ak
a⊤
k Akak has rank one and 1 as its only nonzero eigenvalue (eigenvector
ak). Since the determinant is the product of the eigenvalues, we conclude that
det A∗
k+1
det Ak
=
2n2 + 3
2n2
n 
1 −
2
n + 1

< e
3
2n e−2
n
= e−1
2n ,
where we used 1 + x ≤ex for all x and
 n−1
n+1
n < e−2 for n ≥2.

80
4. Linear Programming Algorithms
For the second estimation we use (4.3) and the well-known fact that det B ≤
||B||n for any matrix B:
det Ak+1
det A∗
k+1
=
det

I + (A∗
k+1)−1(Ak+1 −A∗
k+1)

≤
I + (A∗
k+1)−1(Ak+1 −A∗
k+1)
n
≤

||I|| + ||(A∗
k+1)−1|| ||Ak+1 −A∗
k+1||
n
≤

1 + (R−24k+1)(n2−p)
n
≤

1 +
1
10n2
n
≤
e
1
10n
(we used 2−p ≤
4
10n34N ≤
R2
10n34k+1 ).
We conclude that
volume (Ek+1)
volume (Ek)
=
+
det A∗
k+1
det Ak
+
det Ak+1
det A∗
k+1
≤e−1
4n e
1
20n
= e−1
5n .
2
4.5 Khachiyan’s Theorem
In this section we shall prove Khachiyan’s theorem: the EllipsoidMethod can be
applied to Linear Programming in order to obtain a polynomial-time algorithm.
Let us ﬁrst prove that it sufﬁces to have an algorithm for checking feasibility of
linear inequality systems:
Proposition 4.16.
Suppose there is a polynomial-time algorithm for the following
problem: “Given a matrix A ∈Qm×n and a vector b ∈Qm, decide if {x : Ax ≤b}
is empty.” Then there is a polynomial-time algorithm for Linear Programming
which ﬁnds an optimum basic solution if there exists one.
Proof:
Let an LP max{cx : Ax ≤b} be given. We ﬁrst check if the primal and
dual LPs are both feasible. If at least one of them is infeasible, we are done by
Theorem 3.22. Otherwise, by Corollary 3.17, it is sufﬁcient to ﬁnd an element of
{(x, y) : Ax ≤b, y A = c, y ≥0, cx = yb}.
We show (by induction on k) that a solution of a feasible system of k inequal-
ities and l equalities can be found by k calls to the subroutine checking emptiness
of polyhedra plus additional polynomial-time work. For k = 0 a solution can be
found easily by Gaussian Elimination (Corollary 4.11).
Now let k > 0. Let ax ≤β be an inequality of the system. By a call to the
subroutine we check whether the system becomes infeasible by replacing ax ≤β
by ax = β. If so, the inequality is redundant and can be removed (cf. Proposition
3.7). If not, we replace it by the equality. In both cases we reduced the number
of inequalities by one, so we are done by induction.

4.5 Khachiyan’s Theorem
81
If there exists an optimum basic solution, the above procedure generates one,
because the ﬁnal equality system contains a maximal feasible subsystem of Ax =
b.
2
Before we can apply the Ellipsoid Method, we have to take care that the
polyhedron is bounded and full-dimensional:
Proposition 4.17.
(Khachiyan [1979], G´acs and Lov´asz [1981]) Let A ∈Qm×n
and b ∈Qm. The system Ax ≤b has a solution if and only if the system
Ax ≤b + ϵ1l,
−R1l ≤x ≤R1l
has a solution, where 1l is the all-one vector, 1
ϵ = 2n24(size(A)+size(b)) and R =
1 + 24(size(A)+size(b)).
If Ax ≤b has a solution, then volume ({x ∈Rn : Ax ≤b + ϵ1l, −R1l ≤x ≤
R1l}) ≥

2ϵ
n2size(A)
n.
Proof:
The box constraints −R1l ≤x ≤R1l do not change the solvability by
Theorem 4.4. Now suppose that Ax ≤b has no solution. By Theorem 3.19 (a
version of Farkas’ Lemma), there is a vector y ≥0 with y A = 0 and yb = −1.
By applying Theorem 4.4 to min{1ly : y ≥0, A⊤y = 0, b⊤y = −1} we conclude
that y can be chosen such that its components are of absolute value at most
24(size(A)+size(b)). Therefore y(b + ϵ1l) ≤−1 + n24(size(A)+size(b))ϵ ≤−1
2. Again by
Theorem 3.19, this proves that Ax ≤b + ϵ1l has no solution.
For the second statement, if x ∈Rn with Ax ≤b has components of absolute
value at most R −1 (cf. Theorem 4.4), then {x ∈Rn : Ax ≤b + ϵ1l, −R1l ≤x ≤
R1l} contains all points z with ||z −x||∞≤
ϵ
n2size(A) .
2
Note that the construction of this proposition increases the size of the system
of inequalities by at most a factor of O(m + n).
Theorem 4.18.
(Khachiyan [1979])
There exists a polynomial-time algorithm
for Linear Programming (with rational input), and this algorithm ﬁnds an opti-
mum basic solution if there exists one.
Proof:
By Proposition 4.16 it sufﬁces to check feasibility of a system Ax ≤b.
We transform the system as in Proposition 4.17 in order to obtain a polytope P
which is either empty or has volume at least

2ϵ
n2size(A)
n.
We run the Ellipsoid Method with x0 = 0, R = n

1 + 24(size(A)+size(b))
,
N = ⌈10n2(2 log n + 5(size(A) + size(b)))⌉. Each time in 2⃝we check whether
xk ∈P. If yes, we are done. Otherwise we take a violated inequality ax ≤β of
the system Ax ≤b and set ak := −a.
We claim that if the algorithm does not ﬁnd an xk ∈P before iteration N,
then P must be empty. To see this, we ﬁrst observe that P ⊆Ek for all k: for
k = 0 this is clear by the construction of P and R; the induction step is Lemma
4.14. So we have P ⊆EN.

82
4. Linear Programming Algorithms
By Lemma 4.15, we have, abbreviating s := size(A) + size(b),
volume (EN)
≤
volume (E0)e−N
5n
≤(2R)ne−N
5n
<

2n

1 + 24sn n−4ne−10ns < n−2n2−5ns.
On the other hand, P ̸= ∅implies
volume (P) ≥
 2ϵ
n2s
n
=

1
n225s
n
= n−2n2−5ns,
which is a contradiction.
2
If we estimate the running time for solving an LP max{cx : Ax ≤b} with the
above method, we get the bound O((n+m)9(size(A)+size(b)+size(c))2) (Exercise
9), which is polynomial but completely useless for practical purposes. In practice,
either the Simplex Algorithm or interior point algorithms are used. Karmarkar
[1984] was the ﬁrst to describe a polynomial-time interior point algorithm for
Linear Programming. We shall not go into the details here.
A strongly polynomial-time algorithm for LinearProgramming is not known.
However, Tardos [1986] showed that there is an algorithm for solving max{cx :
Ax ≤b} with a running time that polynomially depends on size(A) only. For
many combinatorial optimization problems, where A is a 0-1-matrix, this gives
a strongly polynomial-time algorithm. Tardos’ result was extended by Frank and
Tardos [1987].
4.6 Separation and Optimization
The above method (in particular Proposition 4.16) requires that the polyhedron be
given explicitly by a list of inequalities. However, a closer look shows that this is
not really necessary. It is sufﬁcient to have a subroutine which – given a vector
x – decides if x ∈P or otherwise returns a separating hyperplane, i.e. a vector
a such that ax > max{ay : y ∈P} (recall Theorem 3.23). We shall prove this
for full-dimensional polytopes; for the general (more complicated) case we refer
to Gr¨otschel, Lov´asz and Schrijver [1988] (or Padberg [1995]). The results in this
section are due to Gr¨otschel, Lov´asz and Schrijver [1981] and independently to
Karp and Papadimitriou [1982] and Padberg and Rao [1981].
With the results of this section one can solve certain linear programs in poly-
nomial time although the polytope has an exponential number of facets. Examples
will be discussed later in this book; see e.g. Corollary 12.19. By considering the
dual LP one can also deal with linear programs with a huge number of variables.
Let P ⊆Rn be a full-dimensional polytope. We assume that we know the
dimension n and two balls B(x0,r) and B(x0, R) such that B(x0,r) ⊆P ⊆
B(x0, R). But we do not assume that we know a linear inequality system deﬁning
P. In fact, this would not make sense if we want to solve linear programs with
an exponential number of constraints in polynomial time.

4.6 Separation and Optimization
83
Below we shall prove that, under some reasonable assumptions, we can opti-
mize a linear function over a polyhedron P in polynomial time (independent of
the number of constraints) if we have a so-called separation oracle: a subroutine
for the following problem:
Separation Problem
Instance:
A polytope P. A vector y ∈Qn.
Task:
Either decide that y ∈P
or ﬁnd a vector d ∈Qn such that dx < dy for all x ∈P.
Given a polyhedron P by such a separation oracle, we look for an oracle
algorithm using this as a black box. In an oracle algorithm we may ask the oracle
at any time and we get a correct answer in one step. We can regard this concept
as a subroutine whose running time we do not take into account.
Indeed, it often sufﬁces to have an oracle which solves the Separation Prob-
lem approximately. More precisely we assume an oracle for the following prob-
lem:
Weak Separation Problem
Instance:
A polytope P, a vector c ∈Qn and a number ϵ > 0. A vector
y ∈Qn.
Task:
Either ﬁnd a vector y′ ∈P with cy ≤cy′ + ϵ
or ﬁnd a vector d ∈Qn such that dx < dy for all x ∈P.
Using a weak separation oracle we ﬁrst solve linear programs approximately:
Weak Optimization Problem
Instance:
A number n ∈N. A vector c ∈Qn. A number ϵ > 0.
A polytope P ⊆Rn given by an oracle for the Weak Separation
Problem for P, c and ϵ
2.
Task:
Find a vector y ∈P with cy ≥max{cx : x ∈P} −ϵ.
Note that the above two deﬁnitions differ from the ones given e.g. in Gro¨tschel,
Lov´asz and Schrijver [1981]. However, they are basically equivalent, and we shall
need the above form again in Section 18.3.
The following variant of the Ellipsoid Method solves the Weak Optimiza-
tion Problem:
Gro¨tschel-Lov´asz-Schrijver Algorithm
Input:
A number n ∈N, n ≥2. A vector c ∈Qn. A number 0 < ϵ ≤1.
A polytope P ⊆Rn given by an oracle for the Weak Separation
Problem for P, c and ϵ
2.
x0 ∈Qn and r, R ∈Q+ such that B(x0,r) ⊆P ⊆B(x0, R).
Output:
A vector y∗∈P with cy∗≥max{cx : x ∈P} −ϵ.

84
4. Linear Programming Algorithms
1⃝
Set R := max{R, 2}, r := min{r, 1} and γ := max{||c||, 1}.
Set N := 5n2 	
ln 4R2γ
rϵ

. Set y∗:= x0.
2⃝
Run the Ellipsoid Method, with ak in 2⃝being computed as follows:
Run the oracle for the Weak Separation Problem with y = xk.
If it returns a y′ ∈P with cy ≤cy′ + ϵ
2 then:
If cy′ > cy∗then set y∗:= y′.
Set ak := c.
If it returns a d ∈Qn with dx < dy for all x ∈P then:
Set ak := −d.
Theorem 4.19.
The Gro¨tschel-Lov´asz-SchrijverAlgorithm correctly solves
the Weak Optimization Problem. Its running time is bounded by
O

n6α2 + n4αf (size(c), size(ϵ), n size(x0) + n3α)

,
where α = log R2γ
rϵ and f (size(c), size(ϵ), size(y)) is an upper bound of the run-
ning time of the oracle for the Weak Separation Problem for P with input c, ϵ, y.
Proof:
(Gr¨otschel, Lov´asz and Schrijver [1981])
The running time in each of
the N = O(n2α) iterations of the Ellipsoid Method is O(n2(n2α + size(R) +
size(x0) + q)) plus one oracle call, where q is the size of the output of the oracle.
As size(y) ≤n(size(x0) + size(R) + N) by Lemma 4.13, the overall running time
is O(n4α(n2α + size(x0) + f (size(c), size(ϵ), n size(x0) + n3α))), as stated.
By Lemma 4.14, we have
{x ∈P : cx ≥cy∗+ ϵ
2} ⊆EN.
Let z be an optimum solution of max{cx : x ∈P}. We may assume that cz >
cy∗+ ϵ
2; otherwise we are done.
Consider the convex hull U of z and the (n−1)-dimensional ball B(x0,r)∩{x :
cx = cx0} (see Figure 4.2). We have U ⊆P and hence U ′ := {x ∈U : cx ≥
cy∗+ ϵ
2} is contained in EN. The volume of U ′ is
volume (U ′)
=
volume (U)
cz −cy∗−ϵ
2
cz −cx0
n
=
Vn−1rn−1 cz −cx0
n||c||
cz −cy∗−ϵ
2
cz −cx0
n
,
where Vn denotes the volume of the n-dimensional unit ball. Since volume (U ′) ≤
volume (EN), and Lemma 4.15 yields
volume (EN) ≤e−N
5n E0 = e−N
5n Vn Rn,
we have

4.6 Separation and Optimization
85
{x : cx = cy∗+ ϵ
2}
{x : cx = cx0}
r
r
x0
z
U ′
Fig. 4.2.
cz −cy∗−ϵ
2 ≤e−N
5n2 R
Vn(cz −cx0)n−1n||c||
Vn−1rn−1
 1
n
.
Since cz −cx0 ≤||c|| · ||z −x0|| ≤||c||R we obtain
cz −cy∗−ϵ
2 ≤||c||e−N
5n2 R
 nVn Rn−1
Vn−1rn−1
 1
n
< 2||c||e−N
5n2 R2
r
≤ϵ
2.
2
Of course we are usually interested in the exact optimum. To achieve this, we
need some assumption on the size of the vertices of the polytope.
Lemma 4.20.
Let n ∈N, let P ⊆Rn be a rational polytope, and let x0 ∈Qn be a
point in the interior of P. Let T ∈N such that size(x0) ≤log T and size(x) ≤log T
for all vertices x of P. Then B(x0,r) ⊆P ⊆B(x0, R), where r := 1
n T −379n2 and
R := 2nT .
Moreover, let K := 2T 2n+1. Let c ∈Zn, and deﬁne c′ := K nc + (1, K, . . . ,
K n−1). Then max{c′x : x ∈P} is attained by a unique vector x∗, for all other
vertices y of P we have c′(x∗−y) > T −2n, and x∗is also an optimum solution of
max{cx : x ∈P}.
Proof:
For any vertex x of P we have ||x|| ≤nT and ||x0|| ≤nT , so ||x−x0|| ≤
2nT and x ∈B(x0, R).
To show that B(x0,r) ⊆P, let F = {x ∈P : ax = β} be a facet of P, where
by Lemma 4.5 we may assume that size(a)+size(β) < 75n2 log T . Suppose there
is a point y ∈F with ||y −x0|| < r. Then
|ax0 −β| = |ax0 −ay| ≤||a|| · ||y −x0|| < n2size(a)r ≤T −304n2
But on the other hand the size of ax0 −β can by estimated by

86
4. Linear Programming Algorithms
size(ax0 −β)
≤
4(size(a) + size(x0) + size(β))
≤
300n2 log T + 4 log T ≤304n2 log T.
Since ax0 ̸= β (x0 is in the interior of P), this implies |ax0 −β| ≥T −304n2, a
contradiction.
To prove the last statements, let x∗be a vertex of P maximizing c′x, and let
y be another vertex of P. By the assumption on the size of the vertices of P we
may write x∗−y = 1
α z, where α ∈{1, 2, . . . , T 2n −1} and z is an integral vector
whose components are less than K
2 . Then
0 ≤c′(x∗−y) = 1
α

K ncz +
n

i=1
K i−1zi

.
Since K n > n
i=1 K i−1|zi|, we must have cz ≥0 and hence cx∗≥cy. So x∗
indeed maximizes cx over P. Moreover, since z ̸= 0, we obtain
c′(x∗−y) ≥1
α > T −2n,
as required.
2
Theorem 4.21.
Let n ∈N and c ∈Qn. Let P ⊆Rn be a rational polytope, and
let x0 ∈Qn be a point in the interior of P. Let T ∈N such that size(x0) ≤log T
and size(x) ≤log T for all vertices x of P.
Given n, c, x0, T and a polynomial-time oracle for the Separation Problem
for P, a vertex x∗of P attaining max{c⊤x : x ∈P} can be found in time polynomial
in n, log T and size(c).
Proof:
(Gr¨otschel, Lov´asz and Schrijver [1981])
We ﬁrst use the Gro¨tschel-
Lov´asz-Schrijver Algorithm to solve the Weak Optimization Problem; we
set c′, r and R according to Lemma 4.20 and ϵ :=
1
4nT 2n+3 . (We ﬁrst have to make
c integral by multiplying with the product of its denominators; this increases its
size by at most a factor 2n.)
The Gro¨tschel-Lov´asz-Schrijver Algorithm returns a vector y ∈P with
c′y ≥c′x∗−ϵ, where x∗is the optimum solution of max{c′x : x ∈P}. By Theorem
4.19 the running time is O

n6α2 + n4αf (size(c′), size(ϵ), n size(x0) + n3α)

=
O

n6α2 + n4αf (size(c′), 6n log T, n log T + n3α)

, where α = log R2 max{||c′||,1}
rϵ
≤
log(16n5T 400n22size(c′)) = O(n2 log T + size(c′)) and f is a polynomial upper
bound of the running time of the oracle for the Separation Problem for P. Since
size(c′) ≤6n2 log T +2 size(c), we have an overall running time that is polynomial
in n, log T and size(c).
We claim that ||x∗−y|| ≤
1
2T 2 . To see this, write y as a convex combination
of the vertices x∗, x1, . . . , xk of P:
y = λ0x∗+
k

i=1
λixi,
λi ≥0,
k

i=0
λi = 1.

4.6 Separation and Optimization
87
Now – using Lemma 4.20 –
ϵ ≥c′(x∗−y) =
k

i=1
λic′ 
x∗−xi

>
k

i=1
λiT −2n = (1 −λ0)T −2n,
so 1 −λ0 < ϵT 2n. We conclude that
||y −x∗|| ≤
k

i=1
λi||xi −x∗|| ≤(1 −λ0)R < 2nT 2n+1ϵ ≤
1
2T 2 .
So when rounding each entry of y to the next rational number with denominator at
most T , we obtain x∗. The rounding can be done in polynomial time by Theorem
4.8.
2
We have proved that, under certain assumptions, optimizing over a polytope
can be done whenever there is a separation oracle. We close this chapter by noting
that the converse is also true. We need the concept of polarity: If X ⊆Rn, we
deﬁne the polar of X to be the set
X◦:= {y ∈Rn : y
⊤x ≤1 for all x ∈X}.
When applied to full-dimensional polytopes, this operation has some nice proper-
ties:
Theorem 4.22.
Let P be a polytope in Rn with 0 in the interior. Then:
(a) P◦is a polytope with 0 in the interior;
(b) (P◦)◦= P;
(c) x is a vertex of P if and only if x ⊤y ≤1 is a facet-deﬁning inequality of P◦.
Proof:
(a): Let P be the convex hull of x1, . . . , xk (cf. Theorem 3.26). By def-
inition, P◦= {y ∈Rn : y⊤xi ≤1 for all i ∈{1, . . . , k}}, i.e. P◦is a polyhedron
and the facet-deﬁning inequalities of P◦are given by vertices of P. Moreover,
0 is in the interior of P◦because 0 satisﬁes all of the ﬁnitely many inequalities
strictly. Suppose P◦is unbounded, i.e. there exists a w ∈Rn \ {0} with αw ∈P◦
for all α > 0. Then αwx ≤1 for all α > 0 and all x ∈P, so wx ≤0 for all
x ∈P. But then 0 cannot be in the interior of P.
(b): Trivially, P ⊆(P◦)◦. To show the converse, suppose that z ∈(P◦)◦\ P.
Then, by Theorem 3.23, there is an inequality c⊤x ≤δ satisﬁed by all x ∈P
but not by z. We have δ > 0 since 0 is in the interior of P. Then 1
δ c ∈P◦but
1
δ c⊤z > 1, contradicting the assumption that z ∈(P◦)◦.
(c): We have already seen in (a) that the facet-deﬁning inequalities of P◦
are given by vertices of P. Conversely, if x1, . . . , xk are the vertices of P, then
¯P := conv({ 1
2x1, x2, . . . , xk}) ̸= P, and 0 is in the interior of ¯P. Now (b) implies
¯P◦̸= P◦. Hence {y ∈Rn : y⊤x1 ≤2, y⊤xi ≤1(i = 2, . . . , k)} = ¯P◦̸= P◦=
{y ∈Rn : y⊤xi ≤1(i = 1, . . . , k)}. We conclude that x ⊤
1 y ≤1 is a facet-deﬁning
inequality of P◦.
2
Now we can prove:

88
4. Linear Programming Algorithms
Theorem 4.23.
Let n ∈N and y ∈Qn. Let P ⊆Rn be a rational polytope, and
let x0 ∈Qn be a point in the interior of P. Let T ∈N such that size(x0) ≤log T
and size(x) ≤log T for all vertices x of P.
Given n, y, x0, T and an oracle which for any given c ∈Qn returns a vertex
x∗of P attaining max{c⊤x : x ∈P}, we can solve the Separation Problem for
P and y in time polynomial in n, log T and size(y). Indeed, in the case y /∈P we
can ﬁnd a facet-deﬁning inequality of P that is violated by y.
Proof:
Consider Q := {x −x0 : x ∈P} and its polar Q◦. If x1, . . . , xk are the
vertices of P, we have
Q◦= {z ∈Rn : z
⊤(xi −x0) ≤1 for all i ∈{1, . . . , k}}.
By Theorem 4.4 we have size(z) ≤4n(2n log T +3n) ≤20n2 log T for all vertices
z of Q◦.
Observe that the Separation Problem for P and y is equivalent to the Sep-
aration Problem for Q and y −x0. Since by Theorem 4.22
Q = (Q◦)◦= {x : zx ≤1 for all z ∈Q◦},
the SeparationProblem for Q and y−x0 is equivalent to solving max{(y−x0)⊤x :
x ∈Q◦}. Since each vertex of Q◦corresponds to a facet-deﬁning inequality of Q
(and thus of P), it remains to show how to ﬁnd a vertex attaining max{(y−x0)⊤x :
x ∈Q◦}.
To do this, we apply Theorem 4.21 to Q◦. By Theorem 4.22, Q◦is full-
dimensional with 0 in the interior. We have shown above that the size of the
vertices of Q◦is at most 20n2 log T . So it remains to show that we can solve the
Separation Problem for Q◦in polynomial time. However, this reduces to the
optimization problem for Q which can be solved using the oracle for optimizing
over P.
2
We ﬁnally mention that a new algorithm which is faster than the Ellipsoid
Method and also implies the equivalence of optimization and separation has
been proposed by Vaidya [1996]. However, this algorithm does not seem to be of
practical use either.
Exercises
1. Let A be a nonsingular rational n × n-matrix. Prove that size(A−1) ≤
4n2 size(A).
2.
∗
Let n ≥2, c ∈Rn and y1, . . . , yk ∈{−1, 0, 1}n such that 0 < c⊤yi+1 ≤1
2c⊤yi
for i = 1, . . . , k −1. Prove that then k ≤3n log n.
Hint: Consider the linear program max{y⊤
k x : (yi −2yi+1)⊤x ≥0, y⊤
k x =
1, x ≥0}.
(M. Goemans)

Exercises
89
3. Consider the numbers hi in the Continued Fraction Expansion. Prove that
hi ≥Fi+1 for all i, where Fi is the i-th Fibonacci number (F1 = F2 = 1 and
Fn = Fn−1 + Fn−2 for n ≥3). Observe that
Fn =
1
√
5

1 +
√
5
2
n
−

1 −
√
5
2
n
.
Conclude that the number of iterations of the Continued Fraction Expan-
sion is O(log q).
(Gr¨otschel, Lov´asz and Schrijver [1988])
4. Show that Gaussian Elimination can be made a strongly polynomial-time
algorithm.
Hint: First assume that A is integral. Recall the proof of Theorem 4.10 and
observe that we can choose d as the common denominator of the entries.
(Edmonds [1967])
5.
∗
Let x1, . . . , xk ∈Rl, d := 1 + dim{x1, . . . , xk}, λ1, . . . , λk ∈R+ with
k
i=1 λi = 1, and x := k
i=1 λixi. Show how to compute numbers µ1, . . . , µk
∈R+, at most d of which are nonzero, such that x = k
i=1 µixi (cf. Exercise
10 of Chapter 3). Show that all computations can be performed in O(n3) time.
Hint: Run Gaussian Elimination with the matrix A ∈R(l+1)×k whose i-th
column is

1
xi

. If d < k, let w ∈Rk be the vector with wcol(i) := zi,d+1
(i = 1, . . . , d), wcol(d+1) := −1 and wcol(i) := 0 (i = d + 2, . . . , k); observe
that Aw = 0. Add a multiple of w to λ, eliminate at least one vector and
iterate.
6. Let max{cx : Ax ≤b} be a linear program all whose inequalities are facet-
deﬁning. Suppose that we know an optimum basic solution x∗. Show how to
use this to ﬁnd an optimum solution to the dual LP min{yb : y A = c, y ≥0}
using Gaussian Elimination. What running time can you obtain?
7.
∗
Let A be a symmetric positive deﬁnite n×n-matrix. Let v1, . . . , vn be n orthog-
onal eigenvectors of A, with corresponding eigenvalues λ1, . . . , λn. W.l.o.g.
||vi|| = 1 for i = 1, . . . , n. Prove that then
E(A, 0) =

µ1
*
λ1v1 + · · · + µn
*
λnvn : µ ∈Rn, ||µ|| ≤1

.
(The eigenvectors correspond to the axes of symmetry of the ellipsoid.)
Conclude that volume (E(A, 0)) =
√
det A volume (B(0, 1)).
8. Let E(A, x) ⊆Rn be an ellipsoid and a ∈Rn, and let E(A′, x′)) be as deﬁned
on page 75. Prove that {z ∈E(A, x) : az ≥ax} ⊆E(A′, x′).
9. Prove that the algorithm of Theorem 4.18 solves a linear program max{cx :
Ax ≤b} in O((n + m)9(size(A) + size(b) + size(c))2) time.
10. Show that the assumption that P is bounded can be omitted in Theorem 4.21.
One can detect if the LP is unbounded and otherwise ﬁnd an optimum solution.

90
4. Linear Programming Algorithms
11.
∗
Let P ⊆R3 be a 3-dimensional polytope with 0 in its interior. Consider
again the graph G(P) whose vertices are the vertices of P and whose edges
correspond to the 1-dimensional faces of P (cf. Exercises 13 and 14 of Chapter
3). Show that G(P◦) is the planar dual of G(P).
Note: Steinitz [1922] proved that for every simple 3-connected planar graph
G there is a 3-dimensional polytope P with G = G(P).
12. Prove that the polar of a polyhedron is always a polyhedron. For which poly-
hedra P is (P◦)◦= P?
References
General Literature:
Gr¨otschel, M., Lov´asz, L., and Schrijver, A. [1988]: Geometric Algorithms and Combina-
torial Optimization. Springer, Berlin 1988
Padberg, M. [1995]: Linear Optimization and Extensions. Springer, Berlin 1995
Schrijver, A. [1986]: Theory of Linear and Integer Programming. Wiley, Chichester 1986
Cited References:
Bland, R.G., Goldfarb, D., and Todd, M.J. [1981]: The ellipsoid method: a survey. Opera-
tions Research 29 (1981), 1039–1091
Edmonds, J. [1967]: Systems of distinct representatives and linear algebra. Journal of Re-
search of the National Bureau of Standards B 71 (1967), 241–245
Frank, A., and Tardos, ´E. [1987]: An application of simultaneous Diophantine approxima-
tion in combinatorial optimization. Combinatorica 7 (1987), 49–65
G´acs, P., and Lov´asz, L. [1981]: Khachiyan’s algorithm for linear programming. Mathe-
matical Programming Study 14 (1981), 61–68
Gr¨otschel, M., Lov´asz, L., and Schrijver, A. [1981]: The ellipsoid method and its conse-
quences in combinatorial optimization. Combinatorica 1 (1981), 169–197
Iudin, D.B., and Nemirovskii, A.S. [1976]: Informational complexity and effective methods
of solution for convex extremal problems. Ekonomika i Matematicheskie Metody 12
(1976), 357–369 [in Russian]
Karmarkar, N. [1984]: A new polynomial-time algorithm for linear programming. Combi-
natorica 4 (1984), 373–395
Karp, R.M., and Papadimitriou, C.H. [1982]: On linear characterizations of combinatorial
optimization problems. SIAM Journal on Computing 11 (1982), 620–632
Khachiyan, L.G. [1979]: A polynomial algorithm in linear programming [in Russian]. Dok-
lady Akademii Nauk SSSR 244 (1979) 1093–1096. English translation: Soviet Mathe-
matics Doklady 20 (1979), 191–194
Khintchine, A. [1956]: Kettenbr¨uche. Teubner, Leipzig 1956
Padberg, M.W., and Rao, M.R. [1981]: The Russian method for linear programming III:
Bounded integer programming. Research Report 81-39, New York University 1981
Shor, N.Z. [1977]: Cut-off method with space extension in convex programming problems.
Cybernetics 13 (1977), 94–96
Steinitz, E. [1922]: Polyeder und Raumeinteilungen. Enzyklop¨adie der Mathematischen
Wissenschaften, Band 3 (1922), 1–139
Tardos, ´E. [1986]: A strongly polynomial algorithm to solve combinatorial linear programs.
Operations Research 34 (1986), 250–256
Vaidya, P.M. [1996]: A new algorithm for minimizing convex functions over convex sets.
Mathematical Programming 73 (1996), 291–341

5. Integer Programming
In this chapter, we consider linear programs with integrality constraints:
Integer Programming
Instance:
A matrix A ∈Zm×n and vectors b ∈Zm, c ∈Zn.
Task:
Find a vector x ∈Zn such that Ax ≤b and cx is maximum.
We do not consider mixed integer programs, i.e. linear programs with integral-
ity constraints for only a subset of the variables. Most of the theory of linear and
integer programming can be extended to mixed integer programming in a natural
way.
PI
P
Fig. 5.1.
Virtually all combinatorial optimization problems can be formulated as integer
programs. The set of feasible solutions can be written as {x : Ax ≤b, x ∈Zn}
for some matrix A and some vector b. {x : Ax ≤b} is a polyhedron P, so let us
deﬁne by PI = {x : Ax ≤b}I the convex hull of the integral vectors in P. We
call PI the integer hull of P. Obviously PI ⊆P.

92
5. Integer Programming
If P is bounded, then PI is also a polytope by Theorem 3.26 (see Figure
5.1). Meyer [1974] proved that PI is a polyhedron for arbitrary rational polyhedra
P. This does in general not hold for irrational polyhedra; see Exercise 1. We
prove a generalization of Meyer’s result in (Theorem 5.7) in Section 5.1. After
some preparation in Section 5.2 we study conditions under which polyhedra are
integral (i.e. P = PI) in Sections 5.3 and 5.4. Note that in this case the integer
linear program is equivalent to its LP relaxation (arising by omitting the integrality
constraints), and can hence be solved in polynomial time. We shall encounter this
situation for several combinatorial optimization problems in later chapters.
In general, however, Integer Programming is much harder than Linear
Programming, and polynomial-time algorithms are not known. This is indeed
not surprising since we can formulate many apparently hard problems as integer
programs. Nevertheless we discuss a general method for ﬁnding the integer hull
by successively cutting off parts of P \ PI in Section 5.5. Although it does not
yield a polynomial-time algorithm it is a useful technique in some cases. Finally
Section 5.6 contains an efﬁcient way of approximating the optimal value of an
integer linear program.
5.1 The Integer Hull of a Polyhedron
As linear programs, integer programs can be infeasible or unbounded. It is not
easy to decide whether PI = ∅for a polyhedron P. But if an integer program
is feasible we can decide whether it is bounded by simply considering the LP
relaxation.
Proposition 5.1.
Let P = {x : Ax ≤b} be some rational polyhedron whose
integer hull is nonempty, and let c be some vector. Then max {cx : x ∈P} is
bounded if and only if max {cx : x ∈PI} is bounded.
Proof:
Suppose max {cx : x ∈P} is unbounded. Then Theorem 3.22 the dual
LP min {yb : y A = c, y ≥0} is infeasible. Then by Corollary 3.21 there is a
rational (and thus an integral) vector z with cz < 0 and Az ≥0. Let y ∈PI be
some integral vector. Then y −kz ∈PI for all k ∈N, and thus max {cx : x ∈PI}
is unbounded. The other direction is trivial.
2
Deﬁnition 5.2.
Let A be an integral matrix. A subdeterminant of A is det B for
some square submatrix B of A (deﬁned by arbitrary row and column indices). We
write (A) for the maximum absolute value of the subdeterminants of A.
Lemma 5.3.
Let C = {x : Ax ≥0} be a polyhedral cone, A an integral matrix.
Then C is generated by a ﬁnite set of integral vectors, each having components
with absolute value at most (A).
Proof:
By Lemma 3.11, C is generated by some of the vectors y1, . . . , yt, such
that for each i, yi is the solution to a system My = b′ where M consists of n

5.1 The Integer Hull of a Polyhedron
93
linearly independent rows of

A
I

and b′ = ±ej for some unit vector ej. Set
zi := | det M|yi. By Cramer’s rule, zi is integral with ||zi||∞≤(A). Since this
holds for each i, the set {z1, . . . , zt} has the required properties.
2
A similar lemma will be used in the next section:
Lemma 5.4.
Each rational polyhedral cone C is generated by a ﬁnite set of in-
tegral vectors {a1, . . . , at} such that each integral vector in C is a nonnegative
integral combination of a1, . . . , at. (Such a set is called a Hilbert basis for C.)
Proof:
Let C be generated by the integral vectors b1, . . . , bk. Let a1, . . . , at be
all integral vectors in the polytope
{λ1b1 + . . . + λkbk : 0 ≤λi ≤1 (i = 1, . . . , k)}
We show that {a1, . . . , at} is a Hilbert basis for C. They indeed generate C, because
b1, . . . , bk occur among the a1, . . . , at.
For any integral vector x ∈C there are µ1, . . . , µk ≥0 with
x = µ1b1 + . . . + µkbk
=
⌊µ1⌋b1 + . . . + ⌊µk⌋bk +
(µ1 −⌊µ1⌋)b1 + . . . + (µk −⌊µk⌋)bk,
so x is a nonnegative integral combination of a1, . . . , at.
2
An important basic fact in integer programming is that optimum integral and
fractional solutions are not too far away from each other:
Theorem 5.5.
(Cook et al. [1986]) Let A be an integral m × n-matrix and b ∈
Rm, c ∈Rn arbitrary vectors. Let P := {x : Ax ≤b} and suppose that PI ̸= ∅.
(a) Suppose y is an optimum solution of max {cx : x ∈P}. Then there exists an
optimum integral solution z of max {cx : x ∈PI} with ||z −y||∞≤n (A).
(b) Suppose y is a feasible integral solution of max {cx : x ∈PI}, but not an
optimal one. Then there exists a feasible integral solution z ∈PI with cz > cy
and ||z −y||∞≤n (A).
Proof:
The proof is almost the same for both parts. Let ﬁrst y ∈P arbitrary.
Let z∗be an optimum integral solution of max {cx : x ∈PI}. We split Ax ≤b
into two subsystems A1x ≤b1, A2x ≤b2 such that A1z∗≥A1y and A2z∗< A2y.
Then z∗−y belongs to the polyhedral cone C := {x : A1x ≥0, A2x ≤0}.
C is generated by some vectors xi (i = 1, . . . , s). By Lemma 5.3, we may
assume that xi is integral and ||xi||∞≤(A) for all i.
Since z∗−y ∈C, there are nonnegative numbers λ1, . . . , λs with z∗−y =
s
i=1 λixi. We may assume that at most n of the λi are nonzero.
For µ = (µ1, . . . , µs) with 0 ≤µi ≤λi (i = 1, . . . , s) we deﬁne
zµ := z∗−
s

i=1
µixi = y +
s

i=1
(λi −µi)xi

94
5. Integer Programming
and observe that zµ ∈P: the ﬁrst representation of zµ implies A1zµ ≤A1z∗≤b1;
the second one implies A2zµ ≤A2y ≤b2.
Case 1:
There is some i ∈{1, . . . , s} with λi ≥1 and cxi > 0. Let z := y + xi.
We have cz > cy, showing that this case cannot occur in case (a). In case (b),
when y is integral, z is an integral solution of Ax ≤b such that cz > cy and
||z −y||∞= ||xi||∞≤(A).
Case 2:
For all i ∈{1, . . . , s}, λi ≥1 implies cxi ≤0. Let
z := z⌊λ⌋=
z∗−
s

i=1
⌊λi⌋xi.
z is an integral vector of P with cz ≥cz∗and
||z −y||∞≤
s

i=1
(λi −⌊λi⌋) ||xi||∞≤n (A).
Hence in both (a) and (b) this vector z does the job.
2
As a corollary we can bound the size of optimum solutions of integer pro-
gramming problems:
Corollary 5.6.
If P = {x ∈Qn : Ax ≤b} is a rational polyhedron and max{cx :
x ∈PI} has an optimum solution, then it also has an optimum integral solution x
with size(x) ≤13n(size(A) + size(b)).
Proof:
By Proposition 5.1 and Theorem 4.4, max{cx : x ∈P} has an optimum
solution y with size(y) ≤4n(size(A) + size(b)). By Theorem 5.5(a) there is
an optimum solution x of max{cx : x ∈PI} with ||x −y||∞≤n (A). By
Propositions 4.1 and 4.3 we have
size(x)
≤
2 size(y) + 2n size(n (A))
≤
8n(size(A) + size(b)) + 2n log n + 4n size(A)
≤
13n(size(A) + size(b)).
2
Theorem 5.5(b) implies the following: given any feasible solution of an integer
program, optimality of a vector x can be checked simply by testing x + y for a
ﬁnite set of vectors y that depend on the matrix A only. Such a ﬁnite test set
(whose existence has been proved ﬁrst by Graver [1975]) enables us to prove a
fundamental theorem on integer programming:
Theorem 5.7.
(Wolsey [1981], Cook et al. [1986])
For each integral m × n-
matrix A there exists an integral matrix M whose entries have absolute value at
most n2n(A)n, such that for each vector b ∈Qm there exists a vector d with
{x : Ax ≤b}I = {x : Mx ≤d}.

5.1 The Integer Hull of a Polyhedron
95
Proof:
We may assume A ̸= 0. Let C be the cone generated by the rows of A.
Let
L := {z ∈Zn : ||z||∞≤n(A)}.
For each K ⊆L, consider the cone
CK := C ∩{y : zy ≤0 for all z ∈K}.
By the proof of Theorem 3.24 and Lemma 5.3, CK = {y : Uy ≤0} for some
matrix U (whose rows are generators of {x : Ax ≤0} and elements of K) whose
entries have absolute value at most n(A). Hence, again by Lemma 5.3, there is
a ﬁnite set G(K) of integral vectors generating CK, each having components with
absolute value at most (U) ≤n!(n(A))n ≤n2n(A)n.
Let M be the matrix with rows 
K⊆L G(K). Since C∅= C, we may assume
that the rows of A are also rows of M.
Now let b be some ﬁxed vector. If Ax ≤b has no solution, we can complete
b to a vector d arbitrarily and have {x : Mx ≤d} ⊆{x : Ax ≤b} = ∅.
If Ax ≤b contains a solution, but no integral solution, we set b′ := b −A′1l,
where A′ arises from A by taking the absolute value of each entry. Then Ax ≤b′
has no solution, since any such solution yields an integral solution of Ax ≤b by
rounding. Again, we complete b′ to d arbitrarily.
Now we may assume that Ax ≤b has an integral solution. For y ∈C we
deﬁne
δy := max {yx : Ax ≤b, x integral}
(this maximum is bounded if y ∈C). It sufﬁces to show that
{x : Ax ≤b}I =

x : yx ≤δy for each y ∈

K⊆L
G(K)

.
(5.1)
Here “⊆” is trivial. To show the converse, let c be any vector for which
max {cx : Ax ≤b, x integral}
is bounded, and let x∗be a vector attaining this maximum. We show that cx ≤cx∗
for all x satisfying the inequalities on the right-hand side of (5.1).
By Proposition 5.1 the LP max {cx : Ax ≤b} is bounded, so by Theorem 3.22
the dual LP min {yb : y A = c, y ≥0} is feasible. Hence c ∈C.
Let ¯K := {z ∈L : A(x∗+ z) ≤b}. By deﬁnition cz ≤0 for all z ∈¯K, so
c ∈C ¯K. Thus there are nonnegative numbers λy (y ∈G( ¯K)) such that
c =

y∈G( ¯K)
λy y.
Next we claim that x∗is an optimum solution for
max {yx : Ax ≤b, x integral}
for each y ∈G( ¯K): the contrary assumption would, by Theorem 5.5(b), yield a
vector z ∈¯K with yz > 0, which is impossible since y ∈C ¯K. We conclude that

96
5. Integer Programming

y∈G( ¯K)
λyδy =

y∈G( ¯K)
λy yx∗=
⎛
⎝
y∈G( ¯K)
λy y
⎞
⎠x∗= cx∗.
Thus the inequality cx ≤cx∗is a nonnegative linear combination of the inequal-
ities yx ≤δy for y ∈G( ¯K). Hence (5.1) is proved.
2
See Lasserre [2004] for a similar result.
5.2 Unimodular Transformations
In this section we shall prove two lemmas for later use. A square matrix is called
unimodular if it is integral and has determinant 1 or −1. Three types of uni-
modular matrices will be of particular interest: For n ∈N, p ∈{1, . . . , n} and
q ∈{1, . . . , n} \ {p} consider the matrices (ai j)i, j∈{1,...,n} deﬁned in one of the
following ways:
ai j =
 1
if i = j ̸= p
−1
if i = j = p
0
otherwise
ai j =
 1
if i = j /∈{p, q}
1
if {i, j} = {p, q}
0
otherwise
ai j =
 1
if i = j
−1
if (i, j) = (p, q)
0
otherwise
These matrices are evidently unimodular. If U is one of the above matrices, then
replacing an arbitrary matrix A (with n columns) by AU is equivalent to applying
one of the following elementary column operations to A:
– multiply a column by −1;
– exchange two columns;
– subtract one column from another column.
A series of the above operations is called a unimodular transformation.
Obviously the product of unimodular matrices is unimodular. It can be shown
that a matrix is unimodular if and only if it arises from an identity matrix by a
unimodular transformation (equivalently, it is the product of matrices of the above
three types); see Exercise 5. Here we do not need this fact.
Proposition 5.8.
The inverse of a unimodular matrix is also unimodular. For each
unimodular matrix U the mappings x →Ux and x →xU are bijections on Zn.
Proof:
Let U be a unimodular matrix. By Cramer’s rule the inverse of a uni-
modular matrix is integral. Since (det U)(det U −1) = det(UU −1) = det I = 1,
U −1 is also unimodular. The second statement follows directly from this.
2
Lemma 5.9.
For each rational matrix A whose rows are linearly independent
there exists a unimodular matrix U such that AU has the form ( B
0 ), where
B is a nonsingular square matrix.

5.3 Total Dual Integrality
97
Proof:
Suppose we have found a unimodular matrix U such that
AU =

B
0
C
D

for some nonsingular square matrix B. (Initially U = I, D = A, and the parts B,
C and 0 have no entries.)
Let (δ1, . . . , δk) be the ﬁrst row of D. Apply unimodular transformations such
that all δi are nonnegative and k
i=1 δi is minimum. W.l.o.g. δ1 ≥δ2 ≥· · · ≥δk.
Then δ1 > 0 since the rows of A (and hence those of AU) are linearly independent.
If δ2 > 0, then subtracting the second column of D from the ﬁrst one would
decrease k
i=1 δi. So δ2 = δ3 = . . . = δk = 0. We can increase the size of B by
one and continue.
2
Note that the operations applied in the proof correspond to the Euclidean
Algorithm. The matrix B we get is in fact a lower diagonal matrix. With a
little more effort one can obtain the so-called Hermite normal form of A. The
following lemma gives a criterion for integral solvability of equation systems,
similar to Farkas’ Lemma.
Lemma 5.10.
Let A be a rational matrix and b a rational column vector. Then
Ax = b has an integral solution if and only if yb is an integer for each rational
vector y for which y A is integral.
Proof:
Necessity is obvious: if x and y A are integral vectors and Ax = b, then
yb = y Ax is an integer.
To prove sufﬁciency, suppose yb is an integer whenever y A is integral. We
may assume that Ax = b contains no redundant equalities, i.e. y A = 0 implies
yb ̸= 0 for all y ̸= 0. Let m be the number of rows of A. If rank(A) < m then
{y : y A = 0} contains a nonzero vector y′ and y′′ :=
1
2y′b y′ satisﬁes y′′A = 0 and
y′′b = 1
2 /∈Z. So the rows of A are linearly independent.
By Lemma 5.9 there exists a unimodular matrix U with AU = ( B
0 ),
where B is a nonsingular m × m-matrix. Since B−1AU = ( I
0 ) is an integral
matrix, we have for each row y of B−1 that y AU is integral and thus by Proposition
5.8 y A is integral. Hence yb is an integer for each row y of B−1, implying that
B−1b is an integral vector. So U

B−1b
0

is an integral solution of Ax = b. 2
5.3 Total Dual Integrality
In this and the next section we focus on integral polyhedra:
Deﬁnition 5.11.
A polyhedron P is integral if P = PI.

98
5. Integer Programming
Theorem 5.12.
(Hoffman [1974], Edmonds and Giles [1977]) Let P be a ratio-
nal polyhedron. Then the following statements are equivalent:
(a) P is integral.
(b) Each face of P contains integral vectors.
(c) Each minimal face of P contains integral vectors.
(d) Each supporting hyperplane contains integral vectors.
(e) Each rational supporting hyperplane contains integral vectors.
(f) max {cx : x ∈P} is attained by an integral vector for each c for which the
maximum is ﬁnite.
(g) max {cx : x ∈P} is an integer for each integral c for which the maximum is
ﬁnite.
Proof:
We ﬁrst prove (a)⇒(b)⇒(f)⇒(a), then (b)⇒(d)⇒(e)⇒(c)⇒(b), and ﬁ-
nally (f)⇒(g)⇒(e).
(a)⇒(b): Let F be a face, say F = P∩H, where H is a supporting hyperplane,
and let x ∈F. If P = PI, then x is a convex combination of integral points in P,
and these must belong to H and thus to F.
(b)⇒(f) follows directly from Proposition 3.3, because {y ∈P : cy =
max {cx : x ∈P}} is a face of P for each c for which the maximum is ﬁnite.
(f)⇒(a): Suppose there is a vector y ∈P\PI. Then (since PI is a polyhedron
by Theorem 5.7) there is an inequality ax ≤β valid for PI for which ay > β.
Then clearly (f) is violated, since max {ax : x ∈P} (which is ﬁnite by Proposition
5.1) is not attained by any integral vector.
(b)⇒(d) is also trivial since the intersection of a supporting hyperplane with
P is a face of P. (d)⇒(e) and (c)⇒(b) are trivial.
(e)⇒(c): Let P = {x : Ax ≤b}. We may assume that A and b are integral.
Let F = {x : A′x = b′} be a minimal face of P, where A′x ≤b′ is a subsystem of
Ax ≤b (we use Proposition 3.8). If A′x = b′ has no integral solution, then – by
Lemma 5.10 – there exists a rational vector y such that c := y A′ is integral but
δ := yb′ is not an integer. Adding integers to components of y does not destroy
this property (A′ and b′ are integral), so we may assume that all components of y
are positive. So H := {x : cx = δ} contains no integral vectors. Observe that H
is a rational hyperplane.
We ﬁnally show that H is a supporting hyperplane by proving that H ∩P = F.
Since F ⊆H is trivial, it remains to show that H ∩P ⊆F. But for x ∈H ∩P
we have y A′x = cx = δ = yb′, so y(A′x −b′) = 0. Since y > 0 and A′x ≤b′,
this implies A′x = b′, so x ∈F.
(f)⇒(g) is trivial, so we ﬁnally show (g)⇒(e). Let H = {x : cx = δ} be
a rational supporting hyperplane of P, so max{cx : x ∈P} = δ. Suppose H
contains no integral vectors. Then – by Lemma 5.10 – there exists a number γ
such that γ c is integral but γ δ /∈Z. Then
max{(|γ |c)x : x ∈P} = |γ | max{cx : x ∈P} = |γ |δ /∈Z,
contradicting our assumption.
2

5.3 Total Dual Integrality
99
See also Gomory [1963], Fulkerson [1971] and Chv´atal [1973] for earlier
partial results. By (a)⇔(b) and Corollary 3.5 every face of an integral polyhedron
is integral. The equivalence of (f) and (g) of Theorem 5.12 motivated Edmonds
and Giles to deﬁne TDI-systems:
Deﬁnition 5.13.
(Edmonds and Giles [1977])
A system Ax ≤b of linear in-
equalities is called totally dual integral (TDI) if the minimum in the LP duality
equation
max {cx : Ax ≤b} = min {yb : y A = c, y ≥0}
has an integral optimum solution y for each integral vector c for which the minimum
is ﬁnite.
With this deﬁnition we get an easy corollary of (g)⇒(a) of Theorem 5.12:
Corollary 5.14.
Let Ax ≤b be a TDI-system where A is rational and b is inte-
gral. Then the polyhedron {x : Ax ≤b} is integral.
2
But total dual integrality is not a property of polyhedra (cf. Exercise 7). In
general, a TDI-system contains more inequalities than necessary for describing the
polyhedron. Adding valid inequalities does not destroy total dual integrality:
Proposition 5.15.
If Ax ≤b is TDI and ax ≤β is a valid inequality for {x :
Ax ≤b}, then the system Ax ≤b, ax ≤β is also TDI.
Proof:
Let c be an integral vector such that min {yb + γβ : y A + γ a = c, y ≥
0, γ ≥0} is ﬁnite. Since ax ≤β is valid for {x : Ax ≤b},
min {yb : y A = c, y ≥0}
=
max {cx : Ax ≤b}
=
max {cx : Ax ≤b, ax ≤β}
=
min {yb + γβ : y A + γ a = c, y ≥0, γ ≥0}.
The ﬁrst minimum is attained by some integral vector y∗, so y = y∗, γ = 0 is an
integral optimum solution for the second minimum.
2
Theorem 5.16.
(Giles and Pulleyblank [1979]) For each rational polyhedron P
there exists a rational TDI-system Ax ≤b with A integral and P = {x : Ax ≤b}.
Here b can be chosen to be integral if and only if P is integral.
Proof:
Let P = {x : Cx ≤d} with C and d rational. Let F be a minimal face
of P. By Proposition 3.8, F = {x : C′x = d′} for some subsystem C′x ≤d′ of
Cx ≤d. Let
K F := {c : cz = max {cx : x ∈P} for all z ∈F}.
Obviously, K F is a cone. We claim that K F is the cone generated by the rows of
C′.
Obviously, the rows of C′ belong to K F. On the other hand, for all z ∈F,
c ∈K F and all y with C′y ≤0 there exists an ϵ > 0 with z + ϵy ∈P. Hence

100
5. Integer Programming
cy ≤0 for all c ∈K F and all y with C′y ≤0. By Farkas’ Lemma (Corollary
3.21), this implies that there exists an x ≥0 with c = xC′.
So K F is indeed a polyhedral cone (Theorem 3.24). By Lemma 5.4 there
exists an integral Hilbert basis a1, . . . , at generating K F. Let SF be the system of
inequalities
a1x ≤max {a1x : x ∈P} , . . . , atx ≤max {atx : x ∈P}.
Let Ax ≤b be the collection of all these systems SF (for all minimal faces F).
Note that if P is integral then b is integral. Certainly P = {x : Ax ≤b}. It remains
to show that Ax ≤b is TDI.
Let c be an integral vector for which
max {cx : Ax ≤b} = min {yb : y ≥0, y A = c}
is ﬁnite. Let F := {z ∈P : cz = max {cx : x ∈P}}. F is a face of P, so let
F′ ⊆F be a minimal face of P. Let SF′ be the system a1x ≤β1, . . . , atx ≤βt.
Then c = λ1a1 +· · ·+λtat for some nonnegative integers λ1, . . . , λt. We add zero
components to λ1, . . . , λt in order to get an integral vector ¯λ ≥0 with ¯λA = c
and thus ¯λb = ¯λ(Ax) = (¯λA)x = cx for all x ∈F′. So ¯λ attains the minimum
min {yb : y ≥0, y A = c}, and Ax ≤b is TDI.
If P is integral, we have chosen b to be integral. Conversely, if b can be
chosen integral, by Corollary 5.14 P must be integral.
2
Indeed, for full-dimensional rational polyhedra there is a unique minimal TDI-
system describing it (Schrijver [1981]). For later use, we prove that each “face”
of a TDI-system is again TDI:
Theorem 5.17.
(Cook [1983]) Let Ax ≤b, ax ≤β be a TDI-system, where a
is integral. Then the system Ax ≤b, ax = β is also TDI.
Proof:
(Schrijver [1986])
Let c be an integral vector such that
max {cx : Ax ≤b, ax = β}
=
min {yb + (λ −µ)β : y, λ, µ ≥0, y A + (λ −µ)a = c}
(5.2)
is ﬁnite. Let x∗, y∗, λ∗, µ∗attain these optima. We set c′ := c+⌈µ∗⌉a and observe
that
max {c′x : Ax ≤b, ax ≤β} = min {yb + λβ : y, λ ≥0, y A + λa = c′} (5.3)
is ﬁnite, because x := x∗is feasible for the maximum and y := y∗, λ :=
λ∗+ ⌈µ∗⌉−µ∗is feasible for the minimum.
Since Ax ≤b, ax ≤β is TDI, the minimum in (5.3) has an integral optimum
solution ˜y, ˜λ. We ﬁnally set y := ˜y, λ := ˜λ and µ := ⌈µ∗⌉and claim that
(y, λ, µ) is an integral optimum solution for the minimum in (5.2).
Obviously (y, λ, µ) is feasible for the minimum in (5.2). Furthermore,

5.4 Totally Unimodular Matrices
101
yb + (λ −µ)β
=
˜yb + ˜λβ −⌈µ∗⌉β
≤
y∗b + (λ∗+ ⌈µ∗⌉−µ∗)β −⌈µ∗⌉β
since (y∗, λ∗+ ⌈µ∗⌉−µ∗) is feasible for the minimum in (5.3), and ( ˜y, ˜λ) is an
optimum solution. We conclude that
yb + (λ −µ)β ≤y∗b + (λ∗−µ∗)β,
proving that (y, λ, µ) is an integral optimum solution for the minimum in (5.2).
2
The following statements are straightforward consequences of the deﬁnition
of TDI-systems: A system Ax = b, x ≥0 is TDI if min {yb : y A ≥c} has an
integral optimum solution y for each integral vector c for which the minimum is
ﬁnite. A system Ax ≤b, x ≥0 is TDI if min {yb : y A ≥c, y ≥0} has an
integral optimum solution y for each integral vector c for which the minimum is
ﬁnite. One may ask whether there are matrices A such that Ax ≤b, x ≥0 is
TDI for each integral vector b. It will turn out that these matrices are exactly the
totally unimodular matrices.
5.4 Totally Unimodular Matrices
Deﬁnition 5.18.
A matrix A is totally unimodular if each subdeterminant of A
is 0, +1, or −1.
In particular, each entry of a totally unimodular matrix must be 0, +1, or −1.
The main result of this section is:
Theorem 5.19.
(Hoffman and Kruskal [1956])
An integral matrix A is totally
unimodular if and only if the polyhedron {x : Ax ≤b, x ≥0} is integral for each
integral vector b.
Proof:
Let A be an m × n-matrix and P := {x : Ax ≤b, x ≥0}. Observe that
the minimal faces of P are vertices.
To prove necessity, suppose that A is totally unimodular. Let b be some integral
vector and x a vertex of P. x is the solution of A′x = b′ for some subsystem
A′x ≤b′ of

A
−I

x ≤

b
0

, with A′ being a nonsingular n × n-matrix.
Since A is totally unimodular, | det A′| = 1, so by Cramer’s rule x = (A′)−1b′ is
integral.
We now prove sufﬁciency. Suppose that the vertices of P are integral for each
integral vector b. Let A′ be some nonsingular k × k-submatrix of A. We have
to show | det A′| = 1. W.l.o.g., A′ contains the elements of the ﬁrst k rows and
columns of A.

102
5. Integer Programming
,
-.
/
z′′
k
n −k
k
m −k
k
m −k
A′
I
0
0
I
0
0
(A I)
z′
Fig. 5.2.
Consider the integral m × m-matrix B consisting of the ﬁrst k and the last
m −k columns of ( A
I ) (see Figure 5.2). Obviously, | det B| = | det A′|.
To prove | det B| = 1, we shall prove that B−1 is integral. Since det B det B−1
= 1, this implies that | det B| = 1, and we are done.
Let i ∈{1, . . . , m}; we prove that B−1ei is integral. Choose an integral vector
y such that z := y + B−1ei ≥0. Then b := Bz = By + ei is integral. We add
zero components to z in order to obtain z′ with
( A
I )z′ = Bz = b.
Now z′′, consisting of the ﬁrst n components of z′, belongs to P. Furthermore, n
linearly independent constraints are satisﬁed with equality, namely the ﬁrst k and
the last n −k inequalities of

A
−I

z′′ ≤

b
0

.
Hence z′′ is a vertex of P. By our assumption z′′ is integral. But then z′ must
also be integral: its ﬁrst n components are the components of z′′, and the last m
components are the slack variables b −Az′′ (and A and b are integral). So z is
also integral, and hence B−1ei = z −y is integral.
2
The above proof is due to Veinott and Dantzig [1968].
Corollary 5.20.
An integral matrix A is totally unimodular if and only if for all
integral vectors b and c both optima in the LP duality equation

5.4 Totally Unimodular Matrices
103
max {cx : Ax ≤b, x ≥0} = min {yb : y ≥0, y A ≥c}
are attained by integral vectors (if they are ﬁnite).
Proof:
This follows from the Hoffman-Kruskal Theorem 5.19 by using the fact
that the transpose of a totally unimodular matrix is also totally unimodular.
2
Let us reformulate these statements in terms of total dual integrality:
Corollary 5.21.
An integral matrix A is totally unimodular if and only if the
system Ax ≤b, x ≥0 is TDI for each vector b.
Proof:
If A (and thus A⊤) is totally unimodular, then by the Hoffman-Kruskal
Theorem min {yb : y A ≥c, y ≥0} is attained by an integral vector for each
vector b and each integral vector c for which the minimum is ﬁnite. In other
words, the system Ax ≤b, x ≥0 is TDI for each vector b.
To show the converse, suppose Ax ≤b, x ≥0 is TDI for each integral vector
b. Then by Corollary 5.14, the polyhedron {x : Ax ≤b, x ≥0} is integral for
each integral vector b. By Theorem 5.19 this means that A is totally unimodular.
2
This is not the only way how total unimodularity can be used to prove that
a certain system is TDI. The following lemma contains another proof technique;
this will be used several times later (Theorems 6.13, 19.10 and 14.12).
Lemma 5.22.
Let Ax ≤b, x ≥0 be an inequality system, where A ∈Rm×n and
b ∈Rm. Suppose that for each c ∈Zn for which min{yb : y A ≥c, y ≥0} has an
optimum solution, it has one y∗such that the rows of A corresponding to nonzero
components of y∗form a totally unimodular matrix. Then Ax ≤b, x ≥0 is TDI.
Proof:
Let c ∈Zn, and let y∗be an optimum solution of min{yb : y A ≥c, y ≥
0} such that the rows of A corresponding to nonzero components of y∗form a
totally unimodular matrix A′. We claim that
min{yb : y A ≥c, y ≥0} = min{yb′ : y A′ ≥c, y ≥0},
(5.4)
where b′ consists of the components of b corresponding to the rows of A′. To see
the inequality “≤” of (5.4), observe that the LP on the right-hand side arises from
the LP on the left-hand side by setting some variables to zero. The inequality “≥”
follows from the fact that y∗without zero components is a feasible solution for
the LP on the right-hand side.
Since A′ is totally unimodular, the second minimum in (5.4) has an integral
optimum solution (by the Hoffman-Kruskal Theorem 5.19). By ﬁlling this solution
with zeros we obtain an integral optimum solution to the ﬁrst minimum in (5.4),
completing the proof.
2
A very useful criterion for total unimodularity is the following:

104
5. Integer Programming
Theorem 5.23.
(Ghouila-Houri [1962])
A matrix A = (ai j) ∈Zm×n is totally
unimodular if and only if for every R ⊆{1, . . . , m} there is a partition R = R1
.
∪
R2 such that

i∈R1
ai j −

i∈R2
ai j ∈{−1, 0, 1}
for all j = 1, . . . , n.
Proof:
Let A be totally unimodular, and let R ⊆{1, . . . , m}. Let dr := 1 for
r ∈R and dr := 0 for r ∈{1, . . . , m} \ R. The matrix
⎛
⎜⎝
A⊤
−A⊤
I
⎞
⎟⎠is also totally
unimodular, so by Theorem 5.19 the polytope

x : x A ≤
01
2d A
1
, x A ≥
21
2d A
3
, x ≤d, x ≥0

is integral. Moreover it is nonempty because it contains 1
2d. So it has an integral
vertex, say z. Setting R1 := {r ∈R : zr = 0} and R2 := {r ∈R : zr = 1} we
obtain

i∈R1
ai j −

i∈R2
ai j

1≤j≤n
= (d −2z)A ∈{−1, 0, 1}n,
as required.
We now prove the converse. By induction on k we prove that every k × k-
submatrix has determinant 0, 1 or −1. For k = 1 this is directly implied by the
criterion for |R| = 1.
Now let k > 1, and let B = (bi j)i, j∈{1,...,k} be a nonsingular k × k-submatrix
of A. By Cramer’s rule, each entry of B−1 is
det B′
det B , where B′ arises from B
by replacing a column by a unit vector. By the induction hypothesis, det B′ ∈
{−1, 0, 1}. So B∗:= (det B)B−1 is a matrix with entries −1, 0, 1 only.
Let b∗
1 be the ﬁrst row of B∗. We have b∗
1 B = (det B)e1, where e1 is the ﬁrst
unit vector. Let R := {i : b∗
1i ̸= 0}. Then for j = 2, . . . , k we have 0 = (b∗
1 B)j =

i∈R b∗
1ibi j, so |{i ∈R : bi j ̸= 0}| is even.
By the hypothesis there is a partition R = R1
.
∪R2 with 
i∈R1 bi j −

i∈R2 bi j ∈{−1, 0, 1} for all j. So for j = 2, . . . , k we have 
i∈R1 bi j −

i∈R2 bi j = 0. If also 
i∈R1 bi1 −
i∈R2 bi1 = 0, then the sum of the rows
in R1 equals the sum of the rows in R2, contradicting the assumption that B is
nonsingular (because R ̸= ∅).
So 
i∈R1 bi1 −
i∈R2 bi1 ∈{−1, 1} and we have yB ∈{e1, −e1}, where
yi :=
 1
if i ∈R1
−1
if i ∈R2
0
if i ̸∈R
.

5.4 Totally Unimodular Matrices
105
Since b∗
1 B = (det B)e1 and B is nonsingular, we have b∗
1 ∈{(det B)y, −(det B)y}.
Since both y and b∗
1 are vectors with entries −1, 0, 1 only, this implies that
| det B| = 1.
2
We apply this criterion to the incidence matrices of graphs:
Theorem 5.24.
The incidence matrix of an undirected graph G is totally unimod-
ular if and only if G is bipartite.
Proof:
By Theorem 5.23 the incidence matrix M of G is totally unimodular
if and only if for any X ⊆V (G) there is a partition X = A
.
∪B such that
E(G[A]) = E(G[B]) = ∅. By deﬁnition, such a partition exists iff G[X] is
bipartite.
2
Theorem 5.25.
The incidence matrix of any digraph is totally unimodular.
Proof:
Using Theorem 5.23, it sufﬁces to set R1 := R and R2 := ∅for any
R ⊆V (G).
2
Applications of Theorems 5.24 and 5.25 will be discussed in later chapters.
Theorem 5.25 has an interesting generalization to cross-free families:
Deﬁnition 5.26.
Let G be a digraph and F a family of subsets of V (G). The
one-way cut-incidence matrix of F is the matrix M = (m X,e)X∈F, e∈E(G) where
m X,e =

1
if e ∈δ+(X)
0
if e /∈δ+(X) .
The two-way cut-incidence matrix of F is the matrix M = (m X,e)X∈F, e∈E(G)
where
m X,e =
 −1
if e ∈δ−(X)
1
if e ∈δ+(X)
0
otherwise
.
Theorem 5.27.
Let G be a digraph and (V (G), F) a cross-free set system. Then
the two-way cut-incidence matrix of F is totally unimodular. If F is laminar, then
also the one-way cut-incidence matrix of F is totally unimodular.
Proof:
Let F be some cross-free family of subsets of V (G). We ﬁrst consider
the case when F is laminar.
We use Theorem 5.23. To see that the criterion is satisﬁed, let R ⊆F, and
consider the tree-representation (T, ϕ) of R, where T is an arborescence rooted at
r (Proposition 2.14). With the notation of Deﬁnition 2.13, R = {Se : e ∈E(T )}.
Set R1 := {S(v,w) ∈R : distT (r, w) even} and R2 := R \ R1. Now for any edge
f ∈E(G), the edges e ∈E(T ) with f ∈δ+(Se) form a path Pf in T (possibly
of zero length). So
|{X ∈R1 : f ∈δ+(X)}| −|{X ∈R2 : f ∈δ+(X)}| ∈{−1, 0, 1},
as required for the one-way cut-incidence matrix.

106
5. Integer Programming
Moreover, for any edge f the edges e ∈E(T ) with f ∈δ−(Se) form a path
Q f in T . Since Pf and Q f have a common endpoint, we have
|{X ∈R1 : f ∈δ+(X)}| −|{X ∈R2 : f ∈δ+(X)}|
−|{X ∈R1 : f ∈δ−(X)}| + |{X ∈R2 : f ∈δ−(X)}|
∈
{−1, 0, 1},
as required for the two-way cut-incidence matrix.
Now if (V (G), F) is a general cross-free set system, consider
F′ := {X ∈F : r ̸∈X} ∪{V (G) \ X : X ∈F, r ∈X}
for some ﬁxed r ∈V (G). F′ is laminar. Since the two-way cut-incidence matrix
of F is a submatrix of

M
−M

, where M is the two-way cut-incidence matrix
of F′, it is totally unimodular, too.
2
For general cross-free families the one-way cut-incidence matrix is not totally
unimodular; see Exercise 12. For a necessary and sufﬁcient condition, see Schrijver
[1983]. The two-way cut-incidence matrices of cross-free families are also known
as network matrices (Exercise 13).
Seymour [1980] showed that all totally unimodular matrices can be constructed
in a certain way from these network matrices and two other totally unimodular
matrices. This deep result implies a polynomial-time algorithm which decides
whether a given matrix is totally unimodular (see Schrijver [1986]).
5.5 Cutting Planes
In the previous sections we considered integral polyhedra. For general polyhedra P
we have P ⊃PI. If we want to solve an integer linear program max {cx : x ∈PI},
it is a natural idea to cut off certain parts of P such that the resulting set is again a
polyhedron P′ and we have P ⊃P′ ⊃PI. Hopefully max {cx : x ∈P′} is attained
by an integral vector; otherwise we can repeat this cutting-off procedure for P′
in order to obtain P′′ and so on. This is the basic idea behind the cutting plane
method, ﬁrst proposed for a special problem (the TSP) by Dantzig, Fulkerson and
Johnson [1954].
Gomory [1958, 1963] found an algorithm which solves general integer pro-
grams with the cutting plane method. Since Gomory’s algorithm in its original
form has little practical relevance, we restrict ourselves to the theoretical back-
ground. The general idea of cutting planes is used very often, although it is in
general not a polynomial-time method. The importance of cutting plane methods
is mostly due to their success in practice. We shall discuss this in Section 21.6.
The following presentation is mainly based on Schrijver [1986].

5.5 Cutting Planes
107
Deﬁnition 5.28.
Let P = {x : Ax ≤b} be a polyhedron. Then we deﬁne
P′ :=
4
P⊆H
HI,
where the intersection ranges over all rational afﬁne half-spaces H = {x : cx ≤δ}
containing P. We set P(0) := P and P(i+1) :=

P(i)′. P(i) is called the i-th
Gomory-Chv´atal-truncation of P.
For a rational polyhedron P we obviously have P ⊇P′ ⊇P(2) ⊇· · · ⊇PI
and PI = (P′)I.
Proposition 5.29.
For any rational polyhedron P = {x : Ax ≤b},
P′ = {x : u Ax ≤⌊ub⌋for all u ≥0 with u A integral }.
Proof:
We ﬁrst make two observations. For any rational afﬁne half-space H =
{x : cx ≤δ} with c integral we obviously have
H ′ = HI ⊆{x : cx ≤⌊δ⌋}.
(5.5)
If in addition the components of c are relatively prime, we claim that
H ′ = HI = {x : cx ≤⌊δ⌋}.
(5.6)
To prove (5.6), let c be an integral vector whose components are relatively
prime. By Lemma 5.10 the hyperplane {x : cx = ⌊δ⌋} contains an integral vector
y. For any rational vector x ∈{x : cx ≤⌊δ⌋} let α ∈N such that αx is integral.
Then we can write
x = 1
α (αx −(α −1)y) + α −1
α
y,
i.e. x is a convex combination of integral points in H. Hence x ∈HI, implying
(5.6).
We now turn to the main proof. To see “⊆”, observe that for any u ≥0,
{x : u Ax ≤ub} is a half-space containing P, so by (5.5) P′ ⊆{x : u Ax ≤⌊ub⌋}
if u A is integral.
We now prove “⊇”. For P = ∅this is easy, so we assume P ̸= ∅. Let
H = {x : cx ≤δ} be some rational afﬁne half-space containing P. W.l.o.g. c is
integral and the components of c are relatively prime. We observe that
δ ≥max {cx : Ax ≤b} = min {ub : u A = c, u ≥0}.
Now let u∗be any optimum solution for the minimum. Then for any
z ∈{x : u Ax ≤⌊ub⌋for all u ≥0 with u A integral } ⊆{x : u∗Ax ≤⌊u∗b⌋}
we have:
cz = u∗Az ≤⌊u∗b⌋≤⌊δ⌋

108
5. Integer Programming
which, using (5.6), implies z ∈HI.
2
Below we shall prove that for any rational polyhedron P there is a number
t with PI = P(t). So Gomory’s cutting plane method successively solves the
linear programs over P, P′, P′′, and so on, until the optimum is integral. At each
step only a ﬁnite number of new inequalities have to be added, namely those
corresponding to a TDI-system deﬁning the current polyhedron (recall Theorem
5.16):
Theorem 5.30.
(Schrijver [1980]) Let P = {x : Ax ≤b} be a polyhedron with
Ax ≤b TDI, A integral and b rational. Then P′ = {x : Ax ≤⌊b⌋}. In particular,
for any rational polyhedron P, P′ is a polyhedron again.
Proof:
The statement is trivial if P is empty, so let P ̸= ∅. Obviously P′ ⊆{x :
Ax ≤⌊b⌋}. To show the other inclusion, let u ≥0 be a vector with u A integral.
By Proposition 5.29 it sufﬁces to show that u Ax ≤⌊ub⌋for all x with Ax ≤⌊b⌋.
We know that
ub ≥max {u Ax : Ax ≤b} = min {yb : y ≥0, y A = u A}.
Since Ax ≤b is TDI, the minimum is attained by some integral vector y∗. Now
Ax ≤⌊b⌋implies
u Ax = y∗Ax ≤y∗⌊b⌋≤⌊y∗b⌋≤⌊ub⌋.
The second statement follows from Theorem 5.16.
2
To prove the main theorem of this section, we need two more lemmas:
Lemma 5.31.
If F is a face of a rational polyhedron P, then F′ = P′ ∩F. More
generally, F(i) = P(i) ∩F for all i ∈N.
Proof:
Let P = {x : Ax ≤b} with A integral, b rational, and Ax ≤b TDI
(recall Theorem 5.16).
Now let F = {x : Ax ≤b, ax = β} be a face of P, where ax ≤β is a valid
inequality for P with a and β integral.
By Proposition 5.15, Ax ≤b, ax ≤β is TDI, so by Theorem 5.17, Ax ≤b,
ax = β is also TDI. As β is an integer,
P′ ∩F
=
{x : Ax ≤⌊b⌋, ax = β}
=
{x : Ax ≤⌊b⌋, ax ≤⌊β⌋, ax ≥⌈β⌉}
=
F′.
Here we have used Theorem 5.30 twice.
To prove F(i) = P(i) ∩F for i > 1 we observe that F′ is either empty or a
face of P′. Now the statement follows by induction on i.
2

5.5 Cutting Planes
109
Lemma 5.32.
Let P be a rational polyhedron in Rn and U a unimodular n × n-
matrix. For X ⊆Rn write f (X) := {Ux : x ∈X}. Then if X is a polyhedron, f (X)
is again a polyhedron. Moreover, we have ( f (P))′ = f (P′) and ( f (P))I = f (PI).
Proof:
Since f : Rn →Rn, x →Ux is a bijective linear function, the ﬁrst
statement is obviously true. Since also the restrictions of f and f −1 to Zn are
bijections (by Proposition 5.8) we have
( f (P))I = conv({x ∈Zn : U −1x ∈P}) = conv({x ∈Rn : U −1x ∈PI}) = f (PI).
Let P = {x : Ax ≤b} with Ax ≤b TDI, A integral, b rational (cf. Theorem
5.16). Then by deﬁnition AU −1x ≤b is also TDI. Therefore
( f (P))′ = {x : AU −1x ≤b}′ = {x : AU −1x ≤⌊b⌋} =
f (P′).
2
Theorem 5.33.
(Schrijver [1980]) For each rational polyhedron P there exists
a number t such that P(t) = PI.
Proof:
Let P be a rational polyhedron in Rn. We prove the theorem by induction
on n + dim P. The case P = ∅is trivial, the case dim P = 0 is easy.
First suppose that P is not full-dimensional. Then P ⊆K for some rational
hyperplane K.
If K contains no integral vectors, K = {x : ax = β} for some integral vector
a and some non-integer β (by Lemma 5.10). But then P′ ⊆{x : ax ≤⌊β⌋, ax ≥
⌈β⌉} = ∅= PI.
If K contains integral vectors, say K = {x : ax = β} with a integral, β an
integer, we may assume β = 0, because the theorem is invariant under translations
by integral vectors. By Lemma 5.9 there exists a unimodular matrix U with aU =
αe1. Since the theorem is also invariant under the transformation x →U −1x
(by Lemma 5.32), we may assume a = αe1. Then the ﬁrst component of each
vector in P is zero, and thus we can reduce the dimension of the space by one
and apply the induction hypothesis (observe that ({0} × Q)I = {0} × QI and
({0} × Q)(t) = {0} × Q(t) for any polyhedron Q in Rn−1 and any t ∈N).
Let now P = {x : Ax ≤b} be full-dimensional, and w.l.o.g. A integral. By
Theorem 5.7 there is some integral matrix C and some vector d with PI = {x :
Cx ≤d}. In the case PI = ∅we set C := A and d := b−A′1l, where A′ arises from
A by taking the absolute value of each entry. (Note that {x : Ax ≤b −A′1l} = ∅.)
Let cx ≤δ be an inequality of Cx ≤d. We claim that P(s) ⊆H := {x : cx ≤
δ} for some s ∈N. This claim obviously implies the theorem.
First observe that there is some β ≥δ such that P ⊆{x : cx ≤β}: in the case
PI = ∅this follows from the choice of C and d; in the case PI ̸= ∅this follows
from Proposition 5.1.
Suppose our claim is false, i.e. there is an integer γ with δ < γ ≤β for which
there exists an s0 ∈N with P(s0) ⊆{x : cx ≤γ }, but there is no s ∈N with
P(s) ⊆{x : cx ≤γ −1}.

110
5. Integer Programming
Observe that max{cx : x ∈P(s)} = γ for all s ≥s0, because if max{cx : x ∈
P(s)} < γ for some s, then P(s+1) ⊆{x : cx ≤γ −1}.
Let F := P(s0) ∩{x : cx = γ }. F is a face of P(s0), and dim F < n = dim P.
By the induction hypothesis, there is a number s1 such that
F(s1) = FI ⊆PI ∩{x : cx = γ } = ∅.
By applying Lemma 5.31 to F and P(s0) we obtain
∅= F(s1) = P(s0+s1) ∩F = P(s0+s1) ∩{x : cx = γ }.
Hence max{cx : x ∈P(s0+s1)} < γ , a contradiction.
2
This theorem also implies the following:
Theorem 5.34.
(Chv´atal [1973]) For each polytope P there is a number t such
that P(t) = PI.
Proof:
As P is bounded, there exists some rational polytope Q ⊇P with QI =
PI. By Theorem 5.33, Q(t) = QI for some t. Hence PI ⊆P(t) ⊆QI = PI,
implying P(t) = PI.
2
This number t is called the Chv´atal rank of P. If P is neither bounded nor
rational, one cannot have an analogous theorem: see Exercises 1 and 16.
A more efﬁcient algorithm which computes the integer hull of a two-dimen-
sional polyhedron has been found by Harvey [1999]. A version of the cutting plane
method which, in polynomial time, approximates a linear objective function over
an integral polytope given by a separation oracle was described by Boyd [1997].
5.6 Lagrangean Relaxation
Suppose we have an integer linear program max{cx : Ax ≤b, A′x ≤b′, x
integral} that becomes substantially easier to solve when omitting some of the
constraints A′x ≤b′. We write Q := {x ∈Rn : Ax ≤b, x integral} and assume
that we can optimize linear objective functions over Q (for example if conv(Q) =
{x : Ax ≤b}). Lagrangean relaxation is a technique to get rid of some troublesome
constraints (in our case A′x ≤b′). Instead of explicitly enforcing the constraints
we modify the objective function in order to punish infeasible solutions. More
precisely, instead of optimizing
max{c
⊤x : A′x ≤b′, x ∈Q}
(5.7)
we consider, for any vector λ ≥0,
L R(λ) := max{c
⊤x + λ
⊤(b′ −A′x) : x ∈Q}.
(5.8)

5.6 Lagrangean Relaxation
111
For each λ ≥0, L R(λ) is an upper bound for (5.7) which is relatively easy to
compute. (5.8) is called the Lagrangean relaxation of (5.7), and the components
of λ are called Lagrange multipliers.
Lagrangean relaxation is a useful technique in nonlinear programming; but
here we restrict ourselves to (integer) linear programming.
Of course one is interested in as good an upper bound as possible. Observe
that L R(λ) is a convex function. The following procedure (called subgradient
optimization) can be used to minimize L R(λ):
Start with an arbitrary vector λ(0) ≥0. In iteration i, given λ(i), ﬁnd a vector x(i)
maximizing c⊤x + (λ(i))⊤(b′ −A′x) over Q (i.e. compute L R(λ(i))). Set λ(i+1) :=
max{0, λ(i) −ti(b′ −A′x(i))} for some ti > 0. Polyak [1967] showed that if
limi→∞ti = 0 and ∞
i=0 ti = ∞, then limi→∞L R(λ(i)) = min{L R(λ) : λ ≥0}.
For more results on the convergence of subgradient optimization, see (Gofﬁn
[1977]).
The problem
min{L R(λ) : λ ≥0}
is sometimes called the Lagrangean dual of (5.7). The question remains how
good this upper bound is. Of course this depends on the structure of the original
problem. In Section 21.5 we shall meet an application to the TSP, where La-
grangean relaxation is very effective. The following theorem helps to estimate the
quality of the upper bound:
Theorem 5.35.
(Geoffrion [1974])
Let Q ⊂Rn be a ﬁnite set, c ∈Rn, A′ ∈
Rm×n and b′ ∈Rm. Suppose that {x ∈Q : A′x ≤b′} is nonempty. Then the
optimum value of the Lagrangean dual of max{c⊤x : A′x ≤b′, x ∈Q} is equal to
max{c⊤x : A′x ≤b′, x ∈conv(Q)}.
Proof:
By reformulating and using the LP Duality Theorem 3.16 we get
min{L R(λ) : λ ≥0}
=
min
5
max{c
⊤x + λ
⊤(b′ −A′x) : x ∈Q} : λ ≥0
6
=
min{η : λ ≥0, η + λ
⊤(A′x −b′) ≥c
⊤x for all x ∈Q}
=
max
⎧
⎨
⎩

x∈Q
αx(c
⊤x) : αx ≥0 (x ∈Q), 1l
⊤α = 1,

x∈Q
(A′x −b′)αx ≤0
⎫
⎬
⎭
=
max
⎧
⎨
⎩c
⊤
x∈Q
αxx : αx ≥0 (x ∈Q),

x∈Q
αx = 1, A′
⎛
⎝
x∈Q
αxx
⎞
⎠≤b′
⎫
⎬
⎭
=
max{c
⊤y : y ∈conv(Q), A′y ≤b′}.
2
In particular, if we have an integer linear program max{cx : A′x ≤b′, Ax ≤
b, x integral} where {x : Ax ≤b} is integral, then the Lagrangean dual (when
relaxing A′x ≤b′ as above) yields the same upper bound as the standard LP

112
5. Integer Programming
relaxation max{cx : A′x ≤b′, Ax ≤b}. If {x : Ax ≤b} is not integral, the upper
bound is in general stronger (but can be difﬁcult to compute). See Exercise 20 for
an example.
Lagrangean relaxation can also be used to approximate linear programs. For
example, consider the Job Assignment Problem (see Section 1.3, in particular
(1.1)). The problem can be rewritten equivalently as
min
⎧
⎨
⎩T :

j∈Si
xi j ≥ti (i = 1, . . . , n), (x, T ) ∈P
⎫
⎬
⎭
(5.9)
where P is the polytope

(x, T )
:
0 ≤xi j ≤ti (i = 1, . . . , n, j ∈Si),

i: j∈Si
xi j ≤T ( j = 1, . . . , m),
T ≤
n

i=1
ti

.
Now we apply Lagrangean relaxation and consider
L R(λ) := min
⎧
⎨
⎩T +
n

i=1
λi
⎛
⎝ti −

j∈Si
xi j
⎞
⎠: (x, T ) ∈P
⎫
⎬
⎭.
(5.10)
Because of its special structure this LP can be solved by a simple combinatorial
algorithm (Exercise 22), for arbitrary λ. If we let Q be the set of vertices of P (cf.
Corollary 3.27), then we can apply Theorem 5.35 and conclude that the optimum
value of the Lagrangean dual max{L R(λ) : λ ≥0} equals the optimum of (5.9).
Exercises
1. Let P :=

(x, y) ∈R2 : y ≤
√
2x

. Prove that PI is not a polyhedron.
2.
∗
Prove the following integer analogue of Carath´eodory’s theorem (Exercise 10
of Chapter 3): For each pointed polyhedral cone C = {x : Ax ≤0}, each
Hilbert basis {a1, . . . , at} of C, and each integral point x ∈C there are 2n −1
vectors among a1, . . . , at such that x is a nonnegative integer combination of
those.
Hint: Consider an optimum basic solution of the LP max{y1l : y A = x, y ≥0}
and round the components down.
(Cook, Fonlupt and Schrijver [1986])
3. Let C = {x : Ax ≥0} be a rational polyhedral cone and b some vector with
bx > 0 for all x ∈C \ {0}. Show that there exists a unique minimal integral
Hilbert basis generating C.
(Schrijver [1981])

Exercises
113
4. Let A be an integral m × n-matrix, and let b and c be vectors, and y an
optimum solution of max {cx : Ax ≤b, x integral}. Prove that there exists an
optimum solution z of max {cx : Ax ≤b} with ||y −z||∞≤n(A).
(Cook et al. [1986])
5. Prove that each unimodular matrix arises from an identity matrix by unimod-
ular transformations.
Hint: Recall the proof of Lemma 5.9.
6.
∗
Prove that there is a polynomial-time algorithm which, given an integral matrix
A and an integral vector b, ﬁnds an integral vector x with Ax = b or decides
that none exists.
Hint: See the proofs of Lemma 5.9 and Lemma 5.10.
7. Consider the two systems
⎛
⎜⎝
1
1
1
0
1
−1
⎞
⎟⎠

x1
x2

≤
⎛
⎜⎝
0
0
0
⎞
⎟⎠and

1
1
1
−1
 
x1
x2

≤

0
0

.
They clearly deﬁne the same polyhedron. Prove that the ﬁrst one is TDI but
the second one is not.
8. Let a be an integral vector and β a rational number. Prove that the inequality
ax ≤β is TDI if and only if the components of a are relatively prime.
9. Let Ax ≤b be TDI, k ∈N and α > 0 rational. Show that 1
k Ax ≤αb is again
TDI. Moreover, prove that αAx ≤αb is not necessarily TDI.
10. Use Theorem 5.24 in order to prove K¨onig’s Theorem 10.2 (cf. Exercise 2 of
Chapter 11):
The maximum cardinality of a matching in a bipartite graph equals the mini-
mum cardinality of a vertex cover.
11. Show that A =
⎛
⎜⎝
1
1
1
−1
1
0
1
0
0
⎞
⎟⎠is not totally unimodular, but {x : Ax = b}
is integral for all integral vectors b.
(Nemhauser and Wolsey [1988])
12. Let G be the digraph ({1, 2, 3, 4}, {(1, 3), (2, 4), (2, 1), (4, 1), (4, 3)}), and let
F := {{1, 2, 4}, {1, 2}, {2}, {2, 3, 4}, {4}}. Prove that (V (G), F) is cross-free
but the one-way cut-incidence matrix of F is not totally unimodular.
13.
∗
Let G and T be digraphs such that V (G) = V (T ) and the undirected graph
underlying T is a tree. For v, w ∈V (G) let P(v, w) be the unique undirected
path from v to w in T . Let M = (me, f )e∈E(G), f ∈E(T ) be the matrix deﬁned by
m(v,w),(x,y) :=
 1
if (x, y) ∈E(P(v, w)) and (x, y) ∈E(P(v, y))
−1
if (x, y) ∈E(P(v, w)) and (x, y) ∈E(P(v, x))
0
if (x, y) /∈E(P(v, w))
.
Matrices arising this way are called network matrices. Show that the network
matrices are precisely the two-way cut-incidence matrices.

114
5. Integer Programming
14. An interval matrix is a 0-1-matrix such that in each row the 1-entries are
consecutive. Prove that interval matrices are totally unimodular.
15.
∗
Consider the following interval packing problem: Given a list of intervals
[ai, bi], i = 1, . . . , n with weights c1, . . . , cn and a number k ∈N, ﬁnd a
maximum weight subset of the intervals such that no point is contained in
more than k of them.
(a) Find an LP formulation (without integrality constraints) of this problem.
(b) What combinatorial meaning has the dual LP? Show how to solve the
dual LP by a simple combinatorial algorithm.
(c) Use (b) to obtain a combinatorial algorithm for the interval packing prob-
lem. What running time do you obtain?
16. Let P := {(x, y) ∈R2 : y =
√
2x, x ≥0} and Q := {(x, y) ∈R2 : y =
√
2x}. Prove that P(t) = P ̸= PI for all t ∈N and Q′ = R2.
17. Let P be the convex hull of the three points (0, 0), (0, 1) and (k, 1
2) in R2,
where k ∈N. Show that P(2k−1) ̸= PI but P(2k) = PI.
18.
∗
Let P ⊆[0, 1]n be a polytope in the unit hypercube with PI = ∅. Prove that
then P(n) = ∅.
Note: Eisenbrand and Schulz [2003] proved that P(n2(1+log n)) = PI for any
polytope P ⊆[0, 1]n.
19. In this exercise we apply Lagrangean relaxation to linear equation systems.
Let Q be a ﬁnite set of vectors in Rn, c ∈Rn and A′ ∈Rm×n and b′ ∈Rm.
Prove that
min
5
max{c
⊤x + λ
⊤(b′ −A′x) : x ∈Q} : λ ∈Rm6
=
max{c
⊤y : y ∈conv(Q), A′y = b′}.
20. Consider the following facility location problem: Given a set of n customers
with demands d1, . . . , dn, and m optional facilities each of which can be
opened or not. For each facility i = 1, . . . , m we have a cost fi for opening
it, a capacity ui and a distance ci j to each customer j = 1, . . . , n. The task
is to decide which facilities should be opened and to assign each customer to
an open facility. The total demand of the customers assigned to one facility
must not exceed its capacity. The objective is to minimize the facility opening
costs plus the sum of the distances of each customer to its facility. In terms
of Integer Programming the problem can be formulated as
min
⎧
⎨
⎩

i, j
ci jxi j +

i
fi yi :

j
djxi j ≤ui yi,

i
xi j = 1, xi j, yi ∈{0, 1}
⎫
⎬
⎭.
Apply Lagrangean relaxation, once relaxing 
j djxi j ≤ui yi for all i, then
relaxing 
i xi j = 1 for all j. Which Lagrangean dual yields a tighter bound?
Note: Both Lagrangean relaxations can be dealt with: see Exercise 7 of Chapter
17.

References
115
21.
∗
Consider the Uncapacitated Facility Location Problem: given numbers
n, m, fi and ci j (i = 1, . . . , m, j = 1, . . . , n), the problem can be formulated
as
min
⎧
⎨
⎩

i, j
ci jxi j +

i
fi yi :

i
xi j = 1, xi j ≤yi, xi j, yi ∈{0, 1}
⎫
⎬
⎭.
For S ⊆{1, . . . , n} we denote by c(S) the cost of supplying facilities for the
customers in S, i.e.
min
⎧
⎨
⎩

i, j
ci jxi j +

i
fi yi :

i
xi j = 1 for j ∈S, xi j ≤yi, xi j, yi ∈{0, 1}
⎫
⎬
⎭.
The cost allocation problem asks whether the total cost c({1, . . . , n}) can be
distributed among the customers such that no subset S pays more than c(S). In
other words: are there numbers p1, . . . , pn such that n
j=1 pj = c({1, . . . , n})
and 
j∈S pj ≤c(S) for all S ⊆{1, . . . , n}? Show that this is the case if and
only if c({1, . . . , n}) equals
min
⎧
⎨
⎩

i, j
ci jxi j +

i
fi yi :

i
xi j = 1, xi j ≤yi, xi j, yi ≥0
⎫
⎬
⎭,
i.e. if the integrality conditions can be left out.
Hint: Apply Lagrangean relaxation to the above LP. For each set of Lagrange
multipliers decompose the resulting minimization problem to minimization
problems over polyhedral cones. What are the vectors generating these cones?
(Goemans and Skutella [2004])
22. Describe a combinatorial algorithm (without using Linear Programming) to
solve (5.10) for arbitrary (but ﬁxed) Lagrange multipliers λ. What running
time can you achieve?
References
General Literature:
Bertsimas, D., and Weismantel, R. [2005]: Optimization Over Integers. Dynamic Ideas,
Belmont 2005
Cook, W.J., Cunningham, W.H., Pulleyblank, W.R., and Schrijver, A. [1998]: Combinato-
rial Optimization. Wiley, New York 1998, Chapter 6
Nemhauser, G.L., and Wolsey, L.A. [1988]: Integer and Combinatorial Optimization. Wiley,
New York 1988
Schrijver, A. [1986]: Theory of Linear and Integer Programming. Wiley, Chichester 1986
Wolsey, L.A. [1998]: Integer Programming. Wiley, New York 1998

116
5. Integer Programming
Cited References:
Boyd, E.A. [1997]: A fully polynomial epsilon approximation cutting plane algorithm for
solving combinatorial linear programs containing a sufﬁciently large ball. Operations
Research Letters 20 (1997), 59–63
Chv´atal, V. [1973]: Edmonds’ polytopes and a hierarchy of combinatorial problems. Dis-
crete Mathematics 4 (1973), 305–337
Cook, W. [1983]: Operations that preserve total dual integrality. Operations Research Letters
2 (1983), 31–35
Cook, W., Fonlupt, J., and Schrijver, A. [1986]: An integer analogue of Carath´eodory’s
theorem. Journal of Combinatorial Theory B 40 (1986), 63–70
Cook, W., Gerards, A., Schrijver, A., and Tardos, ´E. [1986]: Sensitivity theorems in integer
linear programming. Mathematical Programming 34 (1986), 251–264
Dantzig, G., Fulkerson, R., and Johnson, S. [1954]: Solution of a large-scale traveling-
salesman problem. Operations Research 2 (1954), 393–410
Edmonds, J., and Giles, R. [1977]: A min-max relation for submodular functions on graphs.
In: Studies in Integer Programming; Annals of Discrete Mathematics 1 (P.L. Hammer,
E.L. Johnson, B.H. Korte, G.L. Nemhauser, eds.), North-Holland, Amsterdam 1977, pp.
185–204
Eisenbrand, F., and Schulz, A.S. [2003]: Bounds on the Chv´atal rank of polytopes in the
0/1-cube. Combinatorica 23 (2003), 245–261
Fulkerson, D.R. [1971]: Blocking and anti-blocking pairs of polyhedra. Mathematical Pro-
gramming 1 (1971), 168–194
Geoffrion, A.M. [1974]: Lagrangean relaxation for integer programming. Mathematical
Programming Study 2 (1974), 82–114
Giles, F.R., and Pulleyblank, W.R. [1979]: Total dual integrality and integer polyhedra.
Linear Algebra and Its Applications 25 (1979), 191–196
Ghouila-Houri, A. [1962]: Caract´erisation des matrices totalement unimodulaires. Comptes
Rendus Hebdomadaires des S´eances de l’Acad´emie des Sciences (Paris) 254 (1962),
1192–1194
Goemans, M.X., and Skutella, M. [2004]: Cooperative facility location games. Journal of
Algorithms 50 (2004), 194–214
Gofﬁn, J.L. [1977]: On convergence rates of subgradient optimization methods. Mathemat-
ical Programming 13 (1977), 329–347
Gomory, R.E. [1958]: Outline of an algorithm for integer solutions to linear programs.
Bulletin of the American Mathematical Society 64 (1958), 275–278
Gomory, R.E. [1963]: An algorithm for integer solutions of linear programs. In: Recent
Advances in Mathematical Programming (R.L. Graves, P. Wolfe, eds.), McGraw-Hill,
New York, 1963, pp. 269–302
Graver, J.E. [1975]: On the foundations of linear and integer programming I. Mathematical
Programming 9 (1975), 207–226
Harvey, W. [1999]: Computing two-dimensional integer hulls. SIAM Journal on Computing
28 (1999), 2285–2299
Hoffman, A.J. [1974]: A generalization of max ﬂow-min cut. Mathematical Programming
6 (1974), 352–359
Hoffman, A.J., and Kruskal, J.B. [1956]: Integral boundary points of convex polyhedra. In:
Linear Inequalities and Related Systems; Annals of Mathematical Study 38 (H.W. Kuhn,
A.W. Tucker, eds.) Princeton University Press, Princeton 1956, 223–246
Lasserre, J.B. [2004]: The integer hull of a convex rational polytope. Discrete & Compu-
tational Geometry 32 (2004), 129–139
Meyer, R.R. [1974]: On the existence of optimal solutions to integer and mixed-integer
programming problems. Mathematical Programming 7 (1974), 223–235

References
117
Polyak, B.T. [1967]: A general method for solving extremal problems. Doklady Akademii
Nauk SSSR 174 (1967), 33–36 [in Russian]. English translation: Soviet Mathematics
Doklady 8 (1967), 593–597
Schrijver, A. [1980]: On cutting planes. In: Combinatorics 79; Part II; Annals of Discrete
Mathematics 9 (M. Deza, I.G. Rosenberg, eds.), North-Holland, Amsterdam 1980, pp.
291–296
Schrijver, A. [1981]: On total dual integrality. Linear Algebra and its Applications 38
(1981), 27–32
Schrijver, A. [1983]: Packing and covering of crossing families of cuts. Journal of Combi-
natorial Theory B 35 (1983), 104–128
Seymour, P.D. [1980]: Decomposition of regular matroids. Journal of Combinatorial Theory
B 28 (1980), 305–359
Veinott, A.F., Jr., and Dantzig, G.B. [1968]. Integral extreme points. SIAM Review 10
(1968), 371–372
Wolsey, L.A. [1981]: The b-hull of an integer program. Discrete Applied Mathematics 3
(1981), 193–201

6. Spanning Trees and Arborescences
Consider a telephone company that wants to rent a subset from an existing set
of cables, each of which connects two cities. The rented cables should sufﬁce to
connect all cities and they should be as cheap as possible. It is natural to model
the network by a graph: the vertices are the cities and the edges correspond to
the cables. By Theorem 2.4 the minimal connected spanning subgraphs of a given
graph are its spanning trees. So we look for a spanning tree of minimum weight,
where we say that a subgraph T of a graph G with weights c : E(G) →R has
weight c(E(T )) = 
e∈E(T ) c(e).
This is a simple but very important combinatorial optimization problem. It is
also among the combinatorial optimization problems with the longest history; the
ﬁrst algorithm was given by Bor˚uvka [1926a,1926b]; see Neˇsetˇril, Milkov´a and
Neˇsetˇrilov´a [2001].
Compared to the Drilling Problem which asks for a shortest path containing
all vertices of a complete graph, we now look for a shortest tree. Although the
number of spanning trees is even bigger than the number of paths (Kn contains n!
2
Hamiltonian paths, but, by a theorem of Cayley [1889], as many as nn−2 different
spanning trees; see Exercise 1), the problem turns out to be much easier. In fact,
a simple greedy strategy works as we shall see in Section 6.1.
Arborescences can be considered as the directed counterparts of trees; by The-
orem 2.5 they are the minimal spanning subgraphs of a digraph such that all
vertices are reachable from a root. The directed version of the Minimum Span-
ning Tree Problem, the Minimum Weight Arborescence Problem, is more
difﬁcult since a greedy strategy no longer works. In Section 6.2 we show how to
solve this problem.
Since there are very efﬁcient combinatorial algorithms it is not recommended
to solve these problems with Linear Programming. Nevertheless it is interesting
that the corresponding polytopes (the convex hull of the incidence vectors of
spanning trees or arborescences; cf. Corollary 3.28) can be described in a nice
way, which we shall show in Section 6.3. In Section 6.4 we prove some classical
results concerning the packing of spanning trees and arborescences.
6.1 Minimum Spanning Trees
In this section, we consider the following two problems:

120
6. Spanning Trees and Arborescences
Maximum Weight Forest Problem
Instance:
An undirected graph G, weights c : E(G) →R.
Task:
Find a forest in G of maximum weight.
Minimum Spanning Tree Problem
Instance:
An undirected graph G, weights c : E(G) →R.
Task:
Find a spanning tree in G of minimum weight or decide that G is
not connected.
We claim that both problems are equivalent. To make this precise, we say that
a problem P linearly reduces to a problem Q if there are functions f and g,
each computable in linear time, such that f transforms an instance x of P to an
instance f (x) of Q, and g transforms a solution of f (x) to a solution of x. If P
linearly reduces to Q and Q linearly reduces to P, then both problems are called
equivalent.
Proposition 6.1.
The Maximum Weight Forest Problem and the Minimum
Spanning Tree Problem are equivalent.
Proof:
Given an instance (G, c) of the Maximum Weight Forest Problem,
delete all edges of negative weight, let c′(e) := −c(e) for all e ∈E(G′), and add
a minimum set F of edges (with arbitrary weight) to make the graph connected; let
us call the resulting graph G′. Then instance (G′, c′) of the Minimum Spanning
Tree Problem is equivalent in the following sense: Deleting the edges of F from
a minimum weight spanning tree in (G′, c′) yields a maximum weight forest in
(G, c).
Conversely, given an instance (G, c) of the Minimum Spanning Tree Prob-
lem, let c′(e) := K −c(e) for all e ∈E(G), where K = 1+maxe∈E(G) c(e). Then
the instance (G, c′) of the Maximum Weight Forest Problem is equivalent,
since all spanning trees have the same number of edges (Theorem 2.4).
2
We shall return to different reductions of one problem to another in Chapter
15. In the rest of this section we consider the Minimum Spanning Tree Problem
only. We start by proving two optimality conditions:
Theorem 6.2.
Let (G, c) be an instance of the Minimum Spanning Tree Prob-
lem, and let T be a spanning tree in G. Then the following statements are equiv-
alent:
(a) T is optimum.
(b) For every e = {x, y} ∈E(G)\ E(T ), no edge on the x-y-path in T has higher
cost than e.
(c) For every e ∈E(T ), e is a minimum cost edge of δ(V (C)), where C is a
connected component of T −e.

6.1 Minimum Spanning Trees
121
Proof:
(a)⇒(b): Suppose (b) is violated: Let e = {x, y} ∈E(G) \ E(T ) and let
f be an edge on the x-y-path in T with c( f ) > c(e). Then (T −f ) + e is a
spanning tree with lower cost.
(b)⇒(c): Suppose (c) is violated: let e ∈E(T ), C a connected component of
T −e and f = {x, y} ∈δ(V (C)) with c( f ) < c(e). Observe that the x-y-path
in T must contain an edge of δ(V (C)), but the only such edge is e. So (b) is
violated.
(c)⇒(a): Suppose T satisﬁes (c), and let T ∗be an optimum spanning tree with
E(T ∗)∩E(T ) as large as possible. We show that T = T ∗. Namely, suppose there
is an edge e = {x, y} ∈E(T )\ E(T ∗). Let C be a connected component of T −e.
T ∗+ e contains a circuit D. Since e ∈E(D) ∩δ(V (C)), at least one more edge
f ( f ̸= e) of D must belong to δ(V (C)) (see Exercise 9 of Chapter 2). Observe
that (T ∗+e)−f is a spanning tree. Since T ∗is optimum, c(e) ≥c( f ). But since
(c) holds for T , we also have c( f ) ≥c(e). So c( f ) = c(e), and (T ∗+ e) −f is
another optimum spanning tree. This is a contradiction, because it has one edge
more in common with T .
2
The following “greedy” algorithm for the Minimum Spanning Tree Problem
was proposed by Kruskal [1956]. It can be regarded as a special case of a quite
general greedy algorithm which will be discussed in Section 13.4. In the following
let n := |V (G)| and m := |E(G)|.
Kruskal’s Algorithm
Input:
A connected undirected graph G, weights c : E(G) →R.
Output:
A spanning tree T of minimum weight.
1⃝
Sort the edges such that c(e1) ≤c(e2) ≤. . . ≤c(em).
2⃝
Set T := (V (G), ∅).
3⃝
For i := 1 to m do:
If T + ei contains no circuit then set T := T + ei.
Theorem 6.3.
Kruskal’s Algorithm works correctly.
Proof:
It is clear that the algorithm constructs a spanning tree T . It also guar-
antees condition (b) of Theorem 6.2, so T is optimum.
2
The running time of Kruskal’s Algorithm is O(mn): the edges can be
sorted in O(m log m) time (Theorem 1.5), and testing for a circuit in a graph with
at most n edges can be implemented in O(n) time (just apply DFS (or BFS) and
check if there is any edge not belonging to the DFS-tree). Since this is repeated
m times, we get a total running time of O(m log m + mn) = O(mn). However, a
more efﬁcient implementation is possible:

122
6. Spanning Trees and Arborescences
Theorem 6.4.
Kruskal’s Algorithm can be implemented to run in O(m log n)
time.
Proof:
Parallel edges can be eliminated ﬁrst: all but the cheapest edges are re-
dundant. So we may assume that m = O(n2). Since the running time of 1⃝is
obviously O(m log m) = O(m log n) we concentrate on 3⃝. We study a data struc-
ture maintaining the connected components of T . In 3⃝we have to test whether
the addition of an edge ei = {v, w} to T results in a circuit. This is equivalent to
testing if v and w are in the same connected component.
Our implementation maintains a branching B with V (B) = V (G). At any time
the connected components of B will be induced by the same vertex sets as the
connected components of T . (Note however that B is in general not an orientation
of T .)
When checking an edge ei = {v, w} in 3⃝, we ﬁnd the root rv of the arbores-
cence in B containing v and the root rw of the arborescence in B containing w.
The time needed for this is proportional to the length of the rv-v-path plus the
length of the rw-w-path in B. We shall show later that this length is always at
most log n.
Next we check if rv = rw. If rv ̸= rw, we insert ei into T and we have to
add an edge to B. Let h(r) be the maximum length of a path from r in B. If
h(rv) ≥h(rw), then we add an edge (rv,rw) to B, otherwise we add (rw,rv) to
B. If h(rv) = h(rw), this operation increases h(rv) by one, otherwise the new root
has the same h-value as before. So the h-values of the roots can be maintained
easily. Of course initially B := (V (G), ∅) and h(v) := 0 for all v ∈V (G).
We claim that an arborescence of B with root r contains at least 2h(r) vertices.
This implies that h(r) ≤log n, concluding the proof. At the beginning, the claim
is clearly true. We have to show that this property is maintained when adding
an edge (x, y) to B. This is trivial if h(x) does not change. Otherwise we have
h(x) = h(y) before the operation, implying that each of the two arborescences
contains at least 2h(x) vertices. So the new arborescence rooted at x contains at
least 2 · 2h(x) = 2h(x)+1 vertices, as required.
2
The above implementation can be improved by another trick: whenever the root
rv of the arborescence in B containing v has been determined, all the edges on the
rv-v-path P are deleted and an edge (rx, x) is inserted for each x ∈V (P)\{rv}. A
complicated analysis shows that this so-called path compression heuristic makes
the running time of 3⃝almost linear: it is O(mα(m, n)), where α(m, n) is the
functional inverse of Ackermann’s function (see Tarjan [1975,1983]).
We now mention another well-known algorithm for the Minimum Spanning
Tree Problem, due to Jarn´ık [1930] (see Korte and Neˇsetˇril [2001]), Dijkstra
[1959] and Prim [1957]:
Prim’s Algorithm
Input:
A connected undirected graph G, weights c : E(G) →R.
Output:
A spanning tree T of minimum weight.

6.1 Minimum Spanning Trees
123
1⃝
Choose v ∈V (G). Set T := ({v}, ∅).
2⃝
While V (T ) ̸= V (G) do:
Choose an edge e ∈δG(V (T )) of minimum weight. Set T := T + e.
Theorem 6.5.
Prim’s Algorithm works correctly. Its running time is O(n2).
Proof:
The correctness follows from the fact that condition (c) of Theorem 6.2
is guaranteed.
To obtain the O(n2) running time, we maintain for each vertex v ∈V (G) \
V (T ) the cheapest edge e ∈E(V (T ), {v}). Let us call these edges the candidates.
The initialization of the candidates takes O(m) time. Each selection of the cheapest
edge among the candidates takes O(n) time. The update of the candidates can be
done by scanning the edges incident to the vertex which is added to V (T ) and
thus also takes O(n) time. Since the while-loop of 2⃝has n −1 iterations, the
O(n2) bound is proved.
2
The running time can be improved by efﬁcient data structures. Denote lT,v :=
min{c(e) : e ∈E(V (T ), {v})}. We maintain the set {(v,lT,v) : v ∈V (G) \
V (T ), lT,v < ∞} in a data structure, called priority queue or heap, that allows
inserting an element, ﬁnding and deleting an element (v,l) with minimum l, and
decreasing the so-called key l of an element (v,l). Then Prim’s Algorithm can
be written as follows:
1⃝
Choose v ∈V (G). Set T := ({v}, ∅).
Let lw := ∞for w ∈V (G) \ {v}.
2⃝
While V (T ) ̸= V (G) do:
For e = {v, w} ∈E({v}, V (G) \ V (T )) do:
If c(e) < lw < ∞then set lw := c(e) and decreasekey(w,lw).
If lw = ∞then set lw := c(e) and insert(w,lw).
(v,lv) := deletemin.
Let e ∈E(V (T ), {v}) with c(e) = lv. Set T := T + e.
There are several possible ways to implement a heap. A very efﬁcient way,
the so-called Fibonacci heap, has been proposed by Fredman and Tarjan [1987].
Our presentation is based on Schrijver [2003]:
Theorem 6.6.
It is possible to maintain a data structure for a ﬁnite set (initially
empty), where each element u is associated with a real number d(u), called its key,
and perform any sequence of
– p insert-operations (adding an element u with key d(u));
– n deletemin-operations (ﬁnding and deleting an element u with d(u) mini-
mum);
– m decreasekey-operations (decreasing d(u) to a speciﬁed value for an ele-
ment u)
in O(m + p + n log p) time.

124
6. Spanning Trees and Arborescences
Proof:
The set, denoted by U, is stored in a Fibonacci heap, i.e. a branching
(U, E) with a function ϕ : U →{0, 1} with the following properties:
(i) If (u, v) ∈E then d(u) ≤d(v). (This is called the heap order.)
(ii) For each u ∈U the children of u can be numbered 1, . . . , |δ+(u)| such that
the i-th child v satisﬁes |δ+(v)| + ϕ(v) ≥i −1.
(iii) If u and v are distinct roots (δ−(u) = δ−(v) = ∅), then |δ+(u)| ̸= |δ+(v)|.
Condition (ii) implies:
(iv) If a vertex u has out-degree at least k, then at least
√
2
k vertices are reachable
from u.
We prove (iv) by induction on k, the case k = 0 being trivial. So let u be a
vertex with |δ+(u)| ≥k ≥1, and let v be a child of u with |δ+(v)| ≥k −2
(v exists due to (ii)). We apply the induction hypothesis to v in (U, E) and to
u in (U, E \ {(u, v)}) and conclude that at least
√
2
k−2 and
√
2
k−1 vertices are
reachable. (iv) follows from observing that
√
2
k ≤
√
2
k−2 +
√
2
k−1.
In particular, (iv) implies that |δ+(u)| ≤2 log |U| for all u ∈U. Thus, using
(iii), we can store the roots of (U, E) by a function b : {0, 1, . . . , ⌊2 log |U|⌋} →U
with b(|δ+(u)|) = u for each root u.
In addition to this, we keep track of a doubly-linked list of children (in arbitrary
order), a pointer to the parent (if existent) and the out-degree of each vertex.
We now show how the insert-, deletemin- and decreasekey-operations are
implemented.
insert(v, d(v)) is implemented by setting ϕ(v) := 0 and applying
plant(v):
1⃝
Set r := b(|δ+(v)|).
if r is a root with r ̸= v and |δ+(r)| = |δ+(v)| then:
if d(r) ≤d(v) then add (r, v) to E and plant(r).
if d(v) < d(r) then add (v,r) to E and plant(v).
else set b(|δ+(v)|) := v.
As (U, E) is always a branching, the recursion terminates. Note also that (i),
(ii) and (iii) are maintained.
deletemin is implemented by scanning b(i) for i = 0, . . . , ⌊2 log |U|⌋in
order to ﬁnd an element u with d(u) minimum, deleting u and its incident edges
and successively applying plant(v) for each (former) child v of u.
decreasekey(v, (d(v)) is a bit more complicated. Let P be the longest path
in (U, E) ending in v such that each internal vertex u satisﬁes ϕ(u) = 1. We set
ϕ(u) := 1 −ϕ(u) for all u ∈V (P) \ {v}, delete all edges of P from E and apply
plant(z) for each deleted edge (y, z).
To see that this maintains (ii) we only have to consider the parent of the start
vertex x of P, if existent. But then x is not a root, and thus ϕ(x) changes from 0
to 1, making up for the lost child.

6.2 Minimum Weight Arborescences
125
We ﬁnally estimate the running time. As ϕ increases at most m times (at most
once in each decreasekey), ϕ decreases at most m times. Thus the sum of the
length of the paths P in all decreasekey-operations is at most m +m. So at most
2m +2n log p edges are deleted overall (as each deletemin-operation may delete
up to 2 log p edges). Thus at most 2m + 2n log p + p −1 edges are inserted in
total. This proves the overall O(m + p + n log p) running time.
2
Corollary 6.7.
Prim’s Algorithm implemented with Fibonacci heap solves the
Minimum Spanning Tree Problem in O(m + n log n) time.
Proof:
We have at most n −1 insert-, n −1 deletemin-, and m decreasekey-
operations.
2
With a more sophisticated implementation, the running time can be improved
to O (m log β(n, m)), where β(n, m) = min
5
i : log(i) n ≤m
n
6
; see Fredman and
Tarjan [1987], Gabow, Galil and Spencer [1989], and Gabow et al. [1986]. The
fastest known deterministic algorithm is due to Chazelle [2000] and has a running
time of O(mα(m, n)), where α is the functional inverse of Ackermann’s function.
On a different computational model Fredman and Willard [1994] achieved
linear running time. Moreover, there is a randomized algorithm which ﬁnds a
minimum weight spanning tree and has linear expected running time (Karger, Klein
and Tarjan [1995]; such an algorithm which always ﬁnds an optimum solution is
called a Las Vegas algorithm). This algorithm uses a (deterministic) procedure
for testing whether a given spanning tree is optimum; a linear-time algorithm for
this problem has been found by Dixon, Rauch and Tarjan [1992]; see also King
[1995].
The Minimum Spanning Tree Problem for planar graphs can be solved (de-
terministically) in linear time (Cheriton and Tarjan [1976]). The problem of ﬁnd-
ing a minimum spanning tree for a set of n points in the plane can be solved in
O(n log n) time (Exercise 9). Prim’s Algorithm can be quite efﬁcient for such
instances since one can use suitable data structures for ﬁnding nearest neighbours
in the plane effectively.
6.2 Minimum Weight Arborescences
Natural directed generalizations of the Maximum Weight Forest Problem and
the Minimum Spanning Tree Problem read as follows:
Maximum Weight Branching Problem
Instance:
A digraph G, weights c : E(G) →R.
Task:
Find a maximum weight branching in G.

126
6. Spanning Trees and Arborescences
Minimum Weight Arborescence Problem
Instance:
A digraph G, weights c : E(G) →R.
Task:
Find a minimum weight spanning arborescence in G or decide that
none exists.
Sometimes we want to specify the root in advance:
Minimum Weight Rooted Arborescence Problem
Instance:
A digraph G, a vertex r ∈V (G), weights c : E(G) →R.
Task:
Find a minimum weight spanning arborescence rooted at r in G or
decide that none exists.
As for the undirected case, these three problems are equivalent:
Proposition 6.8.
The Maximum Weight Branching Problem, the Minimum
Weight Arborescence Problem and the Minimum Weight Rooted Arbores-
cence Problem are all equivalent.
Proof:
Given an instance (G, c) of the Minimum Weight Arborescence Prob-
lem, let c′(e) := K −c(e) for all e ∈E(G), where K = 2 
e∈E(G) |c(e)|. Then the
instance (G, c′) of the Maximum Weight Branching Problem is equivalent, be-
cause for any two branchings B, B′ with |E(B)| > |E(B′)| we have c′(B) > c′(B′)
(and branchings with n −1 edges are exactly the spanning arborescences).
Given an instance (G, c) of the Maximum Weight Branching Problem, let
G′ := (V (G)
.
∪{r}, E(G)∪{(r, v) : v ∈V (G)}). Let c′(e) := −c(e) for e ∈E(G)
and c(e) := 0 for e ∈E(G′)\ E(G). Then the instance (G′,r, c′) of the Minimum
Weight Rooted Arborescence Problem is equivalent.
Finally, given an instance (G,r, c) of the Minimum Weight Rooted Ar-
borescence Problem, let G′ := (V (G)
.
∪{s}, E(G)∪{(s,r)}) and c((s,r)) := 0.
Then the instance (G′, c) of the Minimum Weight Arborescence Problem is
equivalent.
2
In the rest of this section we shall deal with the Maximum Weight Branching
Problem only. This problem is not as easy as its undirected version, the Maximum
Weight Forest Problem. For example any maximal forest is maximum, but the
bold edges in Figure 6.1 form a maximal branching which is not maximum.
Fig. 6.1.

6.2 Minimum Weight Arborescences
127
Recall that a branching is a graph B with |δ−
B (x)| ≤1 for all x ∈V (B), such
that the underlying undirected graph is a forest. Equivalently, a branching is an
acyclic digraph B with |δ−
B (x)| ≤1 for all x ∈V (B); see Theorem 2.5(g):
Proposition 6.9.
Let B be a digraph with |δ−
B (x)| ≤1 for all x ∈V (B). Then B
contains a circuit if and only if the underlying undirected graph contains a circuit.
2
Now let G be a digraph and c : E(G) →R+. We can ignore negative weights
since such edges will never appear in an optimum branching. A ﬁrst idea towards
an algorithm could be to take the best entering edge for each vertex. Of course the
resulting graph may contain circuits. Since a branching cannot contain circuits,
we must delete at least one edge of each circuit. The following lemma says that
one is enough.
Lemma 6.10.
(Karp [1972]) Let B0 be a maximum weight subgraph of G with
|δ−
B0(v)| ≤1 for all v ∈V (B0). Then there exists an optimum branching B of G
such that for each circuit C in B0, |E(C) \ E(B)| = 1.
a1
b1
a2
b2
a3
b3
C
Fig. 6.2.
Proof:
Let B be an optimum branching of G containing as many edges of B0 as
possible. Let C be some circuit in B0. Let E(C)\ E(B) = {(a1, b1), . . . , (ak, bk)};
suppose that k ≥2 and a1, b1, a2, b2, a3, . . . , bk lie in this order on C (see Figure
6.2).
We claim that B contains a bi-bi−1-path for each i = 1, . . . , k (b0 := bk). This,
however, is a contradiction because these paths form a closed edge progression in
B, and a branching cannot have a closed edge progression.
Let i ∈{1, . . . , k}. It remains to show that B contains a bi-bi−1-path. Consider
B′ with V (B′) = V (G) and E(B′) := {(x, y) ∈E(B) : y ̸= bi} ∪{(ai, bi)}.
B′ cannot be a branching since it would be optimum and contain more edges
of B0 than B. So (by Proposition 6.9) B′ contains a circuit, i.e. B contains a

128
6. Spanning Trees and Arborescences
bi-ai-path P. Since k ≥2, P is not completely on C, so let e be the last edge of
P not belonging to C. Obviously e = (x, bi−1) for some x, so P (and thus B)
contains a bi-bi−1-path.
2
The main idea of Edmonds’ [1967] algorithm is to ﬁnd ﬁrst B0 as above, and
then contract every circuit of B0 in G. If we choose the weights of the resulting
graph G1 correctly, any optimum branching in G1 will correspond to an optimum
branching in G.
Edmonds’ Branching Algorithm
Input:
A digraph G, weights c : E(G) →R+.
Output:
A maximum weight branching B of G.
1⃝
Set i := 0, G0 := G, and c0 := c.
2⃝
Let Bi be a maximum weight subgraph of Gi with |δ−
Bi(v)| ≤1 for all
v ∈V (Bi).
3⃝
If Bi contains no circuit then set B := Bi and go to 5⃝.
4⃝
Construct (Gi+1, ci+1) from (Gi, ci) by doing the following for each circuit
C of Bi:
Contract C to a single vertex vC in Gi+1
For each edge e = (z, y) ∈E(Gi) with z /∈V (C), y ∈V (C) do:
Set ci+1(e′) := ci(e) −ci(α(e, C)) + ci(eC) and 	(e′) := e,
where e′ := (z, vC), α(e, C) = (x, y) ∈E(C),
and eC is some cheapest edge of C.
Set i := i + 1 and go to 2⃝.
5⃝
If i = 0 then stop.
6⃝
For each circuit C of Bi−1 do:
If there is an edge e′ = (z, vC) ∈E(B)
then set E(B) := (E(B) \ {e′}) ∪	(e′) ∪(E(C) \ {α(	(e′), C)})
else set E(B) := E(B) ∪(E(C) \ {eC}).
Set V (B) := V (Gi−1), i := i −1 and go to 5⃝.
This algorithm was also discovered independently by Chu and Liu [1965] and
Bock [1971].
Theorem 6.11.
(Edmonds [1967])
Edmonds’ Branching Algorithm works
correctly.
Proof:
We show that each time just before the execution of 5⃝, B is an optimum
branching of Gi. This is trivial for the ﬁrst time we reach 5⃝. So we have to show
that 6⃝transforms an optimum branching B of Gi into an optimum branching B′
of Gi−1.
Let B∗
i−1 be any branching of Gi−1 such that |E(C) \ E(B∗
i−1)| = 1 for each
circuit C of Bi−1. Let B∗
i result from B∗
i−1 by contracting the circuits of Bi−1. B∗
i

6.3 Polyhedral Descriptions
129
is a branching of Gi. Furthermore we have
ci−1(B∗
i−1) = ci(B∗
i ) +

C: circuit of Bi−1
(ci−1(C) −ci−1(eC)).
By the induction hypothesis, B is an optimum branching of Gi, so we have
ci(B) ≥ci(B∗
i ). We conclude that
ci−1(B∗
i−1)
≤
ci(B) +

C: circuit of Bi−1
(ci−1(C) −ci−1(eC))
=
ci−1(B′).
This, together with Lemma 6.10, implies that B′ is an optimum branching of Gi−1.
2
This proof is due to Karp [1972]. Edmonds’ original proof was based on a
linear programming formulation (see Corollary 6.14). The running time of Ed-
monds’ Branching Algorithm is easily seen to be O(mn), where m = |E(G)|
and n = |V (G)|: there are at most n iterations (i.e. i ≤n at any stage of the
algorithm), and each iteration can be implemented in O(m) time.
The best known bound has been obtained by Gabow et al. [1986] using a
Fibonacci heap: their branching algorithm runs in O(m + n log n) time.
6.3 Polyhedral Descriptions
A polyhedral description of the Minimum Spanning Tree Problem is as follows:
Theorem 6.12.
(Edmonds [1970]) Given a connected undirected graph G, n :=
|V (G)|, the polytope P :=
⎧
⎨
⎩x ∈[0, 1]E(G) :

e∈E(G)
xe = n −1,

e∈E(G[X])
xe ≤|X| −1 for ∅̸= X ⊂V (G)
⎫
⎬
⎭
is integral. Its vertices are exactly the incidence vectors of spanning trees of G. (P
is called the spanning tree polytope of G.)
Proof:
Let T be a spanning tree of G, and let x be the incidence vector of E(T ).
Obviously (by Theorem 2.4), x ∈P. Furthermore, since x ∈{0, 1}E(G), it must
be a vertex of P.
On the other hand let x be an integral vertex of P. Then x is the incidence
vector of the edge set of some subgraph H with n −1 edges and no circuit. Again
by Theorem 2.4 this implies that H is a spanning tree.
So it sufﬁces to show that P is integral (recall Theorem 5.12). Let c :
E(G) →R, and let T be the tree produced by Kruskal’s Algorithm when
applied to (G, c) (ties are broken arbitrarily when sorting the edges). Denote

130
6. Spanning Trees and Arborescences
E(T ) = { f1, . . . , fn−1}, where the fi were taken in this order by the algorithm. In
particular, c( f1) ≤· · · ≤c( fn−1). Let Xk ⊆V (G) be the connected component
of (V (G), { f1, . . . , fk}) containing fk (k = 1, . . . , n −1).
Let x∗be the incidence vector of E(T ). We show that x∗is an optimum
solution to the LP
min

e∈E(G)
c(e)xe
s.t.

e∈E(G)
xe
=
n −1

e∈E(G[X])
xe
≤
|X| −1
(∅̸= X ⊂V (G))
xe
≥
0
(e ∈E(G)).
We introduce a dual variable zX for each ∅̸= X ⊂V (G) and one additional
dual variable zV (G) for the equality constraint. Then the dual LP is
max
−

∅̸=X⊆V (G)
(|X| −1)zX
s.t.
−

e⊆X⊆V (G)
zX
≤
c(e)
(e ∈E(G))
zX
≥
0
(∅̸= X ⊂V (G)).
Note that the dual variable zV (G) is not forced to be nonnegative. For k =
1, . . . , n −2 let z∗
Xk := c( fl) −c( fk), where l is the ﬁrst index greater than k
for which fl ∩Xk ̸= ∅. Let z∗
V (G) := −c( fn−1), and let z∗
X := 0 for all X ̸∈
{X1, . . . , Xn−1}.
For each e = {v, w} we have that
−

e⊆X⊆V (G)
z∗
X = c( fi),
where i is the smallest index such that v, w ∈Xi. Moreover c( fi) ≤c(e) since
v and w are in different connected components of (V (G), { f1, . . . , fi−1}). Hence
z∗is a feasible dual solution.
Moreover x∗
e > 0, i.e. e ∈E(T ), implies
−

e⊆X⊆V (G)
z∗
X = c(e),
i.e. the corresponding dual constraint is satisﬁed with equality. Finally, z∗
X > 0
implies that T [X] is connected, so the corresponding primal constraint is satisﬁed
with equality. In other words, the primal and dual complementary slackness con-
ditions are satisﬁed, thus (by Corollary 3.18) x∗and z∗are optimum solutions for
the primal and dual LP, respectively.
2

6.3 Polyhedral Descriptions
131
Indeed, we have proved that the inequality system in Theorem 6.12 is TDI. We
remark that the above is also an alternative proof of the correctness of Kruskal’s
Algorithm (Theorem 6.3). Another description of the spanning tree polytope is
the subject of Exercise 13.
If we replace the constraint 
e∈E(G) xe = n −1 by 
e∈E(G) xe ≤n −1,
we obtain the convex hull of the incidence vectors of all forests in G (Exercise
14). A generalization of these results is Edmonds’ characterization of the matroid
polytope (Theorem 13.21).
We now turn to a polyhedral description of the Minimum Weight Rooted
Arborescence Problem. First we prove a classical result of Fulkerson. Recall
that an r-cut is a set of edges δ+(S) for some S ⊂V (G) with r ∈S.
Theorem 6.13.
(Fulkerson [1974]) Let G be a digraph with weights c : E(G) →
Z+, and r ∈V (G) such that G contains a spanning arborescence rooted at r. Then
the minimum weight of a spanning arborescence rooted at r equals the maximum
number t of r-cuts C1, . . . , Ct (repetitions allowed) such that no edge e is contained
in more than c(e) of these cuts.
Proof:
Let A be the matrix whose columns are indexed by the edges and whose
rows are all incidence vectors of r-cuts. Consider the LP
min{cx : Ax ≥1l, x ≥0},
and its dual
max{1ly : y A ≤c, y ≥0}.
Then (by part (e) of Theorem 2.5) we have to show that for any nonnegative
integral c, both the primal and dual LP have integral optimum solutions. By
Corollary 5.14 it sufﬁces to show that the system Ax ≥1l, x ≥0 is TDI. We use
Lemma 5.22.
Since the dual LP is feasible if and only if c is nonnegative, let c : E(G) →Z+.
Let y be an optimum solution of max{1ly : y A ≤c, y ≥0} for which

∅̸=X⊆V (G)\{r}
yδ−(X)|X|2
(6.1)
is as large as possible. We claim that F := {X : yδ−(X) > 0} is laminar. To see
this, suppose X, Y ∈F with X ∩Y ̸= ∅, X \ Y ̸= ∅and Y \ X ̸= ∅(Figure
6.3). Let ϵ := min{yδ−(X), yδ−(Y)}. Set y′
δ−(X) := yδ−(X) −ϵ, y′
δ−(Y) := yδ−(Y) −ϵ,
y′
δ−(X∩Y) := yδ−(X∩Y) + ϵ, y′
δ−(X∪Y) := yδ−(X∪Y) + ϵ, and y′(S) := y(S) for all
other r-cuts S. Observe that y′A ≤y A, so y′ is a feasible dual solution. Since
1ly = 1ly′, it is also optimum and contradicts the choice of y, because (6.1) is
larger for y′. (For any numbers a > b ≥c > d > 0 with a + d = b + c we have
a2 + d2 > b2 + c2.)
Now let A′ be the submatrix of A consisting of the rows corresponding to the
elements of F. A′ is the one-way cut-incidence matrix of a laminar family (to be
precise, we must consider the graph resulting from G by reversing each edge). So
by Theorem 5.27 A′ is totally unimodular, as required.
2

132
6. Spanning Trees and Arborescences
X
Y
r
Fig. 6.3.
The above proof also yields the promised polyhedral description:
Corollary 6.14.
(Edmonds [1967]) Let G be a digraph with weights c : E(G) →
R+, and r ∈V (G) such that G contains a spanning arborescence rooted at r. Then
the LP
min
⎧
⎨
⎩cx : x ≥0,

e∈δ+(X)
xe ≥1 for all X ⊂V (G) with r ∈X
⎫
⎬
⎭
has an integral optimum solution (which is the incidence vector of a minimum
weight spanning arborescence rooted at r, plus possibly some edges of zero weight).
2
For a description of the convex hull of the incidence vectors of all branchings
or spanning arborescences rooted at r, see Exercises 15 and 16.
6.4 Packing Spanning Trees and Arborescences
If we are looking for more than one spanning tree or arborescence, classical
theorems of Tutte, Nash-Williams and Edmonds are of help. We ﬁrst give a proof
of Tutte’s Theorem on packing spanning trees which is essentially due to Mader
(see Diestel [1997]) and which uses the following lemma:
Lemma 6.15.
Let G be an undirected graph, and let F = (F1, . . . , Fk) be a k-
tuple of edge-disjoint forests in G such that |E(F)| is maximum, where E(F) :=
k
i=1 E(Fi). Let e ∈E(G) \ E(F). Then there exists a set X ⊆V (G) with e ⊆X
such that Fi[X] is connected for each i ∈{1, . . . , k}.
Proof:
For two k-tuples F′ = (F′
1, . . . , F′
k) and F′′ = (F′′
1 , . . . , F′′
k ) we say that
F′′ arises from F′ by exchanging e′ for e′′ if F′′
j = (F′
j \ e′)
.
∪e′′ for some j and
F′′
i = F′
i for all i ̸= j. Let F be the set of all k-tuples of edge-disjoint forests
arising from F by a sequence of such exchanges. Let E := E(G)\
)
F′∈F E(F′)

and G := (V (G), E). We have F ∈F and thus e ∈E. Let X be the vertex set

6.4 Packing Spanning Trees and Arborescences
133
of the connected component of G containing e. We shall prove that Fi[X] is
connected for each i.
Claim:
For any F′ = (F′
1, . . . , F′
k) ∈F and any ¯e = {v, w} ∈E(G[X])\ E(F′)
there exists a v-w-path in F′
i [X] for all i ∈{1, . . . , k}.
To prove this, let i ∈{1, . . . , k} be ﬁxed. Since F′ ∈F and |E(F′)| = |E(F)|
is maximum, F′
i + ¯e contains a circuit C. Now for all e′ ∈E(C) \ {¯e} we have
F′
e′ ∈F, where F′
e′ arises from F′ by exchanging e′ for ¯e. This shows that
E(C) ⊆E, and so C −¯e is a v-w-path in F′
i [X]. This proves the claim.
Since G[X] is connected, it sufﬁces to prove that for each ¯e = {v, w} ∈
E(G[X]) and each i there is a v-w-path in Fi[X].
So let ¯e = {v, w} ∈E(G[X]). Since ¯e ∈E, there is some F′ = (F′
1, . . . , F′
k) ∈
F with ¯e ̸∈E(F′). By the claim there is a v-w-path in F′
i [X] for each i.
Now there is a sequence F = F(0), F(1) . . . , F(s) = F′ of elements of F such
that F(r+1) arises from F(r) by exchanging one edge (r = 0, . . . , s −1). It sufﬁces
to show that the existence of a v-w-path in F(r+1)
i
[X] implies the existence of a
v-w-path in F(r)
i
[X] (r = 0, . . . , s −1).
To see this, suppose that F(r+1)
i
[X] arises from F(r)
i
[X] by exchanging er for
er+1, and let P be the v-w-path in F(r+1)
i
[X]. If P does not contain er+1 = {x, y},
it is also a path in F(r)
i
[X]. Otherwise er+1 ∈E(G[X]), and we consider the x-y-
path Q in F(r)
i
[X] which exists by the claim. Since (E(P) \ {er+1}) ∪Q contains
a v-w-path in F(r)
i
[X], the proof is complete.
2
Now we can prove Tutte’s theorem on disjoint spanning trees. A multicut in
an undirected graph G is a set of edges δ(X1, . . . , X p) := δ(X1) ∪· · · ∪δ(X p)
for some partition V (G) = X1
.
∪X2
.
∪· · ·
.
∪X p of the vertex set into nonempty
subsets. For p = 3 we also speak of 3-cuts. Observe that cuts are multicuts with
p = 2.
Theorem 6.16.
(Tutte [1961], Nash-Williams [1961])
An undirected graph G
contains k edge-disjoint spanning trees if and only if
|δ(X1, . . . , X p)| ≥k(p −1)
for every multicut δ(X1, . . . , X p).
Proof:
To prove necessity, let T1, . . . , Tk be edge-disjoint spanning trees in
G, and let δ(X1, . . . , X p) be a multicut. Contracting each of the vertex subsets
X1, . . . , X p yields a graph G′ whose vertices are X1, . . . , X p and whose edges
correspond to the edges of the multicut. T1, . . . , Tk correspond to edge-disjoint
connected subgraphs T ′
1, . . . , T ′
k in G′. Each of the T ′
1, . . . , T ′
k has at least p −1
edges, so G′ (and thus the multicut) has at least k(p −1) edges.
To prove sufﬁciency we use induction on |V (G)|. For n := |V (G)| ≤2
the statement is true. Now assume n > 2, and suppose that |δ(X1, . . . , X p)| ≥
k(p−1) for every multicut δ(X1, . . . , X p). In particular (consider the partition into
singletons) G has at least k(n −1) edges. Moreover, the condition is preserved

134
6. Spanning Trees and Arborescences
when contracting vertex sets, so by the induction hypothesis G/X contains k
edge-disjoint spanning trees for each X ⊂V (G) with |X| ≥2.
Let F = (F1, . . . , Fk) be a k-tuple of edge-disjoint forests in G such that
|E(F)| =

k
i=1 E(Fi)
 is maximum. We claim that each Fi is a spanning tree.
Otherwise E(F) < k(n −1), so there is an edge e ∈E(G) \ E(F). By Lemma
6.15 there is an X ⊆V (G) with e ⊆X such that Fi[X] is connected for each
i. Since |X| ≥2, G/X contains k edge-disjoint spanning trees F′
1, . . . , F′
k. Now
F′
i together with Fi[X] forms a spanning tree in G for each i, and all these k
spanning trees are edge-disjoint.
2
We now turn to the corresponding problem in digraphs, packing spanning
arborescences:
Theorem 6.17.
(Edmonds [1973]) Let G be a digraph and r ∈V (G). Then the
maximum number of edge-disjoint spanning arborescences rooted at r equals the
minimum cardinality of an r-cut.
Proof:
Let k be the minimum cardinality of an r-cut. Obviously there are at most
k edge-disjoint spanning arborescences. We prove the existence of k edge-disjoint
spanning arborescences by induction on k. The case k = 0 is trivial.
If we can ﬁnd one spanning arborescence A rooted at r such that
min
r∈S⊂V (G)
δ+
G(S) \ E(A)
 ≥k −1,
(6.2)
then we are done by induction. Suppose we have already found some arborescence
A rooted at r (but not necessarily spanning) such that (6.2) holds. Let R ⊆V (G)
be the set of vertices covered by A. Initially, R = {r}; if R = V (G), we are done.
If R ̸= V (G), we call a set X ⊆V (G) critical if
(a) r ∈X;
(b) X ∪R ̸= V (G);
(c) |δ+
G(X) \ E(A)| = k −1.
R
X
e
x
y
r
Fig. 6.4.
If there is no critical vertex set, we can augment A by any edge leaving R.
Otherwise let X be a maximal critical set, and let e = (x, y) be an edge such that

6.4 Packing Spanning Trees and Arborescences
135
x ∈R \ X and y ∈V (G) \ (R ∪X) (see Figure 6.4). Such an edge must exist
because
|δ+
G−E(A)(R ∪X)| = |δ+
G(R ∪X)| ≥k > k −1 = |δ+
G−E(A)(X)|.
We now add e to A. Obviously A + e is an arborescence rooted at r. We have
to show that (6.2) continues to hold.
Suppose there is some Y such that r ∈Y ⊂V (G) and |δ+
G(Y) \ E(A + e)| <
k −1. Then x ∈Y, y /∈Y, and |δ+
G(Y) \ E(A)| = k −1. Now Lemma 2.1(a)
implies
k −1 + k −1
=
|δ+
G−E(A)(X)| + |δ+
G−E(A)(Y)|
≥
|δ+
G−E(A)(X ∪Y)| + |δ+
G−E(A)(X ∩Y)|
≥
k −1 + k −1 ,
because r ∈X ∩Y and y ∈V (G) \ (X ∪Y). So equality must hold throughout, in
particular |δ+
G−E(A)(X ∪Y)| = k −1. Since y ∈V (G) \ (X ∪Y ∪R) we conclude
that X ∪Y is critical. But since x ∈Y \ X, this contradicts the maximality of X.
2
This proof is due to Lov´asz [1976]. A generalization of Theorems 6.16 and 6.17
was found by Frank [1978]. A good characterization of the problem of packing
spanning arborescences with arbitrary roots is given by the following theorem,
which we cite without proof:
Theorem 6.18.
(Frank [1979]) A digraph G contains k edge-disjoint spanning
arborescences if and only if
p

i=1
|δ−(Xi)| ≥k(p −1)
for every collection of pairwise disjoint nonempty subsets X1, . . . , X p ⊆V (G).
Another question is how many forests are needed to cover a graph. This is
answered by the following theorem:
Theorem 6.19.
(Nash-Williams [1964]) The edge set of an undirected graph G is
the union of k forests if and only if |E(G[X])| ≤k(|X|−1) for all ∅̸= X ⊆V (G).
Proof:
The necessity is clear since no forest can contain more than |X|−1 edges
within a vertex set X. To prove the sufﬁciency, assume that |E(G[X])| ≤k(|X|−1)
for all ∅̸= X ⊆V (G), and let F = (F1, . . . , Fk) be a k-tuple of disjoint forests in
G such that |E(F)| =

k
i=1 E(Fi)
 is maximum. We claim that E(F) = E(G).
To see this, suppose there is an edge e ∈E(G) \ E(F). By Lemma 6.15 there
exists a set X ⊆V (G) with e ⊆X such that Fi[X] is connected for each i. In
particular,

136
6. Spanning Trees and Arborescences
|E(G[X])| ≥
{e}
.
∪
k
i=1
E(Fi[X])
 ≥1 + k(|X| −1),
contradicting the assumption.
2
Exercise 21 gives a directed version. A generalization of Theorems 6.16 and
6.19 to matroids can be found in Exercise 18 of Chapter 13.
Exercises
1. Prove Cayley’s theorem, stating that Kn has nn−2 spanning trees, by showing
that the following deﬁnes a one-to-one correspondence between the spanning
trees in Kn and the vectors in {1, . . . , n}n−2: For a tree T with V (T ) =
{1, . . . , n}, n ≥3, let v be the leaf with the smallest index and let a1 be
the neighbour of v. We recursively deﬁne a(T ) := (a1, . . . , an−2), where
(a2, . . . , an−2) = a(T −v).
(Cayley [1889], Pr¨ufer [1918])
2. Let (V, T1) and (V, T2) be two trees on the same vertex set V . Prove that for
any edge e ∈T1 there is an edge f ∈T2 such that both (V, (T1 \ {e}) ∪{ f })
and (V, (T2 \ { f }) ∪{e}) are trees.
3. Given an undirected graph G with weights c : E(G) →R and a vertex
v ∈V (G), we ask for a minimum weight spanning tree in G where v is not
a leaf. Can you solve this problem in polynomial time?
4. We want to determine the set of edges e in an undirected graph G with weights
c : E(G) →R for which there exists a minimum weight spanning tree in G
containing e (in other words, we are looking for the union of all minimum
weight spanning trees in G). Show how this problem can be solved in O(mn)
time.
5. Given an undirected graph G with arbitrary weights c : E(G) →R, we
ask for a minimum weight connected spanning subgraph. Can you solve this
problem efﬁciently?
6. Consider the following algorithm (sometimes called Worst-Out-GreedyAl-
gorithm, see Section 13.4). Examine the edges in order of non-increasing
weights. Delete an edge unless it is a bridge. Does this algorithm solve the
Minimum Spanning Tree Problem?
7. Consider the following “colouring” algorithm. Initially all edges are un-
coloured. Then apply the following rules in arbitrary order until all edges
are coloured:
Blue rule: Select a cut containing no blue edge. Among the uncoloured edges
in the cut, select one of minimum cost and colour it blue.
Red rule: Select a circuit containing no red edge. Among the uncoloured edges
in the circuit, select one of maximum cost and colour it red.
Show that one of the rules is always applicable as long as there are uncoloured
edges left. Moreover, show that the algorithm maintains the “colour invariant”:

Exercises
137
there always exists an optimum spanning tree containing all blue edges but no
red edge. (So the algorithm solves the Minimum Spanning Tree Problem
optimally.) Observe that Kruskal’s Algorithm and Prim’s Algorithm are
special cases.
(Tarjan [1983])
8. Suppose we wish to ﬁnd a spanning tree T in an undirected graph such that
the maximum weight of an edge in T is as small as possible. How can this
be done?
9. For a ﬁnite set V ⊂R2, the Vorono¨ı diagram consists of the regions
Pv :=

x ∈R2 : ||x −v||2 = min
w∈V ||x −w||2

for v ∈V . The Delaunay triangulation of V is the graph
(V, {{v, w} ⊆V, v ̸= w, |Pv ∩Pw| > 1}) .
A minimum spanning tree for V is a tree T with V (T ) = V whose length

{v,w}∈E(T ) ||v −w||2 is minimum. Prove that every minimum spanning tree
is a subgraph of the Delaunay triangulation.
Note: Using the fact that the Delaunay triangulation can be computed in
O(n log n) time (where n = |V |; see e.g. Fortune [1987], Knuth [1992]),
this implies an O(n log n) algorithm for the Minimum Spanning Tree Prob-
lem for point sets in the plane.
(Shamos and Hoey [1975]); see also (Zhou, Shenoy and Nicholls [2002])
10. Can you decide in linear time whether a graph contains a spanning arbores-
cence?
Hint: To ﬁnd a possible root, start at an arbitrary vertex and traverse edges
backwards as long as possible. When encountering a circuit, contract it.
11. The Minimum Weight Rooted Arborescence Problem can be reduced to
the Maximum Weight Branching Problem by Proposition 6.8. However, it
can also be solved directly by a modiﬁed version of Edmonds’ Branching
Algorithm. Show how.
1
0
1
1
0
0
Fig. 6.5.

138
6. Spanning Trees and Arborescences
12. Prove that the spanning tree polytope of an undirected graph G (see Theorem
6.12) with n := |V (G)| is in general a proper subset of the polytope
⎧
⎨
⎩x ∈[0, 1]E(G) :

e∈E(G)
xe = n −1,

e∈δ(X)
xe ≥1 for ∅⊂X ⊂V (G)
⎫
⎬
⎭.
Hint: To prove that this polytope is not integral, consider the graph shown in
Figure 6.5 (the numbers are edge weights).
(Magnanti and Wolsey [1995])
13.
∗
In Exercise 12 we saw that cut constraints do not sufﬁce to describe the
spanning tree polytope. However, if we consider multicuts instead, we obtain
a complete description: Prove that the spanning tree polytope of an undirected
graph G with n := |V (G)| consists of all vectors x ∈[0, 1]E(G) with

e∈E(G)
xe = n −1 and

e∈C
xe ≥k −1 for all multicuts C = δ(X1, . . . , Xk).
(Magnanti and Wolsey [1995])
14. Prove that the convex hull of the incidence vectors of all forests in an undi-
rected graph G is the polytope
P :=
⎧
⎨
⎩x ∈[0, 1]E(G) :

e∈E(G[X])
xe ≤|X| −1 for ∅̸= X ⊆V (G)
⎫
⎬
⎭.
Note: This statement implies Theorem 6.12 since 
e∈E(G[X]) xe = |V (G)|−1
is a supporting hyperplane. Moreover, it is a special case of Theorem 13.21.
15.
∗
Prove that the convex hull of the incidence vectors of all branchings in a
digraph G is the set of all vectors x ∈[0, 1]E(G) with

e∈E(G[X])
xe ≤|X| −1 for ∅̸= X ⊆V (G) and

e∈δ−(v)
xe ≤1 for v ∈V (G).
Note: This is a special case of Theorem 14.13.
16.
∗
Let G be a digraph and r ∈V (G). Prove that the polytopes

x ∈[0, 1]E(G)
:
xe = 0 (e ∈δ−(r)),

e∈δ−(v)
xe = 1 (v ∈V (G) \ {r}),

e∈E(G[X])
xe ≤|X| −1 for ∅̸= X ⊆V (G)

and

References
139

x ∈[0, 1]E(G)
:
xe = 0 (e ∈δ−(r)),

e∈δ−(v)
xe = 1 (v ∈V (G) \ {r}),

e∈δ+(X)
xe ≥1 for r ∈X ⊂V (G)

are both equal to the convex hull of the incidence vectors of all spanning
arborescences rooted at r.
17. Let G be a digraph and r ∈V (G). Prove that G is the disjoint union of k
spanning arborescences rooted at r if and only if the underlying undirected
graph is the disjoint union of k spanning trees and |δ−(x)| = k for all x ∈
V (G) \ {r}.
(Edmonds)
18. Let G be a digraph and r ∈V (G). Suppose that G contains k edge-disjoint
paths from r to every other vertex, but removing any edge destroys this prop-
erty. Prove that every vertex of G except r has exactly k entering edges.
Hint: Use Theorem 6.17.
19.
∗
Prove the statement of Exercise 18 without using Theorem 6.17. Formulate
and prove a vertex-disjoint version.
Hint: If a vertex v has more than k entering edges, take k edge-disjoint r-v-
paths. Show that an edge entering v that is not used by these paths can be
deleted.
20. Give a polynomial-time algorithm for ﬁnding a maximum set of edge-disjoint
spanning arborescences (rooted at r) in a digraph G.
Note: The most efﬁcient algorithm is due to Gabow [1995]; see also (Gabow
and Manu [1998]).
21. Prove that the edges of a digraph G can be covered by k branchings if and
only if the following two conditions hold:
(a) |δ−(v)| ≤k for all v ∈V (G);
(b) |E(G[X])| ≤k(|X| −1) for all X ⊆V (G).
Hint: Use Theorem 6.17.
(Frank [1979])
References
General Literature:
Ahuja, R.K., Magnanti, T.L., and Orlin, J.B. [1993]: Network Flows. Prentice-Hall, Engle-
wood Cliffs 1993, Chapter 13
Balakrishnan, V.K. [1995]: Network Optimization. Chapman and Hall, London 1995, Chap-
ter 1
Cormen, T.H., Leiserson, C.E., and Rivest, R.L. [1990]: Introduction to Algorithms. MIT
Press, Cambridge 1990, Chapter 24
Gondran, M., and Minoux, M. [1984]: Graphs and Algorithms. Wiley, Chichester 1984,
Chapter 4

140
6. Spanning Trees and Arborescences
Magnanti, T.L., and Wolsey, L.A. [1995]: Optimal trees. In: Handbooks in Operations Re-
search and Management Science; Volume 7: Network Models (M.O. Ball, T.L. Magnanti,
C.L. Monma, G.L. Nemhauser, eds.), Elsevier, Amsterdam 1995, pp. 503–616
Schrijver, A. [2003]: Combinatorial Optimization: Polyhedra and Efﬁciency. Springer,
Berlin 2003, Chapters 50–53
Tarjan, R.E. [1983]: Data Structures and Network Algorithms. SIAM, Philadelphia 1983,
Chapter 6
Wu, B.Y., and Chao, K.-M. [2004]: Spanning Trees and Optimization Problems. Chapman
& Hall/CRC, Boca Raton 2004
Cited References:
Bock, F.C. [1971]: An algorithm to construct a minimum directed spanning tree in a directed
network. In: Avi-Itzak, B. (Ed.): Developments in Operations Research. Gordon and
Breach, New York 1971, 29–44
Bor˚uvka, O. [1926a]: O jist´em probl´emu minim´aln´ım. Pr´aca Moravsk´e P˘r´ırodov˘edeck´e
Spolne˘cnosti 3 (1926), 37–58
Bor˚uvka, O. [1926b]: P˘r´ıspev˘ek k ˘re˘sen´ı ot´azky ekonomick´e stavby. Elektrovodn´ıch s´ıt´ı.
Elektrotechnicky Obzor 15 (1926), 153–154
Cayley, A. [1889]: A theorem on trees. Quarterly Journal on Mathematics 23 (1889), 376–
378
Chazelle, B. [2000]: A minimum spanning tree algorithm with inverse-Ackermann type
complexity. Journal of the ACM 47 (2000), 1028–1047
Cheriton, D., and Tarjan, R.E. [1976]: Finding minimum spanning trees. SIAM Journal on
Computing 5 (1976), 724–742
Chu, Y., and Liu, T. [1965]: On the shortest arborescence of a directed graph. Scientia
Sinica 4 (1965), 1396–1400; Mathematical Review 33, # 1245
Diestel, R. [1997]: Graph Theory. Springer, New York 1997
Dijkstra, E.W. [1959]: A note on two problems in connexion with graphs. Numerische
Mathematik 1 (1959), 269–271
Dixon, B., Rauch, M., and Tarjan, R.E. [1992]: Veriﬁcation and sensitivity analysis of
minimum spanning trees in linear time. SIAM Journal on Computing 21 (1992), 1184–
1192
Edmonds, J. [1967]: Optimum branchings. Journal of Research of the National Bureau of
Standards B 71 (1967), 233–240
Edmonds, J. [1970]: Submodular functions, matroids and certain polyhedra. In: Combina-
torial Structures and Their Applications; Proceedings of the Calgary International Con-
ference on Combinatorial Structures and Their Applications 1969 (R. Guy, H. Hanani,
N. Sauer, J. Schonheim, eds.), Gordon and Breach, New York 1970, pp. 69–87
Edmonds, J. [1973]: Edge-disjoint branchings. In: Combinatorial Algorithms (R. Rustin,
ed.), Algorithmic Press, New York 1973, pp. 91–96
Fortune, S. [1987]: A sweepline algorithm for Voronoi diagrams. Algorithmica 2 (1987),
153–174
Frank, A. [1978]: On disjoint trees and arborescences. In: Algebraic Methods in Graph
Theory; Colloquia Mathematica; Soc. J. Bolyai 25 (L. Lov´asz, V.T. S´os, eds.), North-
Holland, Amsterdam 1978, pp. 159–169
Frank, A. [1979]: Covering branchings. Acta Scientiarum Mathematicarum (Szeged) 41
(1979), 77–82
Fredman, M.L., and Tarjan, R.E. [1987]: Fibonacci heaps and their uses in improved net-
work optimization problems. Journal of the ACM 34 (1987), 596–615
Fredman, M.L., and Willard, D.E. [1994]: Trans-dichotomous algorithms for minimum
spanning trees and shortest paths. Journal of Computer and System Sciences 48 (1994),
533–551

References
141
Fulkerson, D.R. [1974]: Packing rooted directed cuts in a weighted directed graph. Mathe-
matical Programming 6 (1974), 1–13
Gabow, H.N. [1995]: A matroid approach to ﬁnding edge connectivity and packing arbores-
cences. Journal of Computer and System Sciences 50 (1995), 259–273
Gabow, H.N., Galil, Z., and Spencer, T. [1989]: Efﬁcient implementation of graph algo-
rithms using contraction. Journal of the ACM 36 (1989), 540–572
Gabow, H.N., Galil, Z., Spencer, T., and Tarjan, R.E. [1986]: Efﬁcient algorithms for
ﬁnding minimum spanning trees in undirected and directed graphs. Combinatorica 6
(1986), 109–122
Gabow, H.N., and Manu, K.S. [1998]: Packing algorithms for arborescences (and spanning
trees) in capacitated graphs. Mathematical Programming B 82 (1998), 83–109
Jarn´ık, V. [1930]: O jist´em probl´emu minim´aln´ım. Pr´aca Moravsk´e P˘r´ırodov˘edeck´e Spole˘c-
nosti 6 (1930), 57–63
Karger, D., Klein, P.N., and Tarjan, R.E. [1995]: A randomized linear-time algorithm to
ﬁnd minimum spanning trees. Journal of the ACM 42 (1995), 321–328
Karp, R.M. [1972]: A simple derivation of Edmonds’ algorithm for optimum branchings.
Networks 1 (1972), 265–272
King, V. [1995]: A simpler minimum spanning tree veriﬁcation algorithm. Algorithmica
18 (1997), 263–270
Knuth, D.E. [1992]: Axioms and hulls; LNCS 606. Springer, Berlin 1992
Korte, B., and Neˇsetˇril, J. [2001]: Vojt˘ech Jarn´ık’s work in combinatorial optimization.
Discrete Mathematics 235 (2001), 1–17
Kruskal, J.B. [1956]: On the shortest spanning subtree of a graph and the travelling salesman
problem. Proceedings of the AMS 7 (1956), 48–50
Lov´asz, L. [1976]: On two minimax theorems in graph. Journal of Combinatorial Theory
B 21 (1976), 96–103
Nash-Williams, C.S.J.A. [1961]: Edge-disjoint spanning trees of ﬁnite graphs. Journal of
the London Mathematical Society 36 (1961), 445–450
Nash-Williams, C.S.J.A. [1964]: Decompositions of ﬁnite graphs into forests. Journal of
the London Mathematical Society 39 (1964), 12
Neˇsetˇril, J., Milkov´a, E., and Neˇsetˇrilov´a, H. [2001]: Otakar Bor˚uvka on minimum span-
ning tree problem. Translation of both the 1926 papers, comments, history. Discrete
Mathematics 233 (2001), 3–36
Prim, R.C. [1957]: Shortest connection networks and some generalizations. Bell System
Technical Journal 36 (1957), 1389–1401
Pr¨ufer, H. [1918]: Neuer Beweis eines Satzes ¨uber Permutationen. Arch. Math. Phys. 27
(1918), 742–744
Shamos, M.I., and Hoey, D. [1975]: Closest-point problems. Proceedings of the 16th Annual
IEEE Symposium on Foundations of Computer Science (1975), 151–162
Tarjan, R.E. [1975]: Efﬁciency of a good but not linear set union algorithm. Journal of the
ACM 22 (1975), 215–225
Tutte, W.T. [1961]: On the problem of decomposing a graph into n connected factor. Journal
of the London Mathematical Society 36 (1961), 221–230
Zhou, H., Shenoy, N., and Nicholls, W. [2002]: Efﬁcient minimum spanning tree construc-
tion without Delaunay triangulation. Information Processing Letters 81 (2002), 271–276

7. Shortest Paths
One of the best known combinatorial optimization problems is to ﬁnd a shortest
path between two speciﬁed vertices of a digraph:
Shortest Path Problem
Instance:
A digraph G, weights c : E(G) →R and two vertices s, t ∈V (G).
Task:
Find a shortest s-t-path P, i.e. one of minimum weight c(E(P)),
or decide that t is not reachable from s.
Obviously this problem has many practical applications. Like the Minimum
Spanning Tree Problem it also often appears as a subproblem when dealing with
more difﬁcult combinatorial optimization problems.
In fact, the problem is not easy to solve if we allow arbitrary weights. For
example, if all weights are −1 then the s-t-paths of weight 1 −|V (G)| are pre-
cisely the Hamiltonian s-t-paths. Deciding whether such a path exists is a difﬁcult
problem (see Exercise 14(b) of Chapter 15).
The problem becomes much easier if we restrict ourselves to nonnegative
weights or at least exclude negative circuits:
Deﬁnition 7.1.
Let G be a (directed or undirected) graph with weights c :
E(G) →R. c is called conservative if there is no circuit of negative total weight.
We shall present algorithms for the Shortest Path Problem in Section 7.1.
The ﬁrst one allows nonnegative weights only while the second algorithm can deal
with arbitrary conservative weights.
The algorithms of Section 7.1 in fact compute a shortest s-v-path for all v ∈
V (G) without using signiﬁcantly more running time. Sometimes one is interested
in the distance for every pair of vertices; Section 7.2 shows how to deal with this
problem.
Since negative circuits cause problems we also show how to detect them.
If none exists, a circuit of minimum total weight can be computed quite easily.
Another interesting problem asks for a circuit whose mean weight is minimum. As
we shall see in Section 7.3 this problem can also be solved efﬁciently by similar
techniques.
Finding shortest paths in undirected graphs is more difﬁcult unless the edge
weights are nonnegative. Undirected edges of nonnegative weights can be replaced

144
7. Shortest Paths
equivalently by a pair of oppositely directed edges of the same weight; this reduces
the undirected problem to a directed one. However, this construction does not
work for edges of negative weight since it would introduce negative circuits. We
shall return to the problem of ﬁnding shortest paths in undirected graphs with
conservative weights in Section 12.2 (Corollary 12.12).
Henceforth we work with a digraph G. Without loss of generality we may
assume that G is connected and simple; among parallel edges we have to consider
only the one with least weight.
7.1 Shortest Paths From One Source
All shortest path algorithms we present are based on the following observation,
sometimes called Bellman’s principle of optimality, which is indeed the core of
dynamic programming:
Proposition 7.2.
Let G be a digraph with conservative weights c : E(G) →R,
let k ∈N, and let s and w be two vertices. Let P be a shortest one among all
s-w-paths with at most k edges, and let e = (v, w) be its ﬁnal edge. Then P[s,v]
(i.e. P without the edge e) is a shortest one among all s-v-paths with at most k −1
edges.
Proof:
Suppose Q is a shorter s-v-path than P[s,v], and |E(Q)| ≤k −1. Then
c(E(Q)) + c(e) < c(E(P)). If Q does not contain w, then Q + e is a shorter
s-w-path than P, otherwise Q[s,w] has length c(E(Q[s,w])) = c(E(Q)) + c(e) −
c(E(Q[w,v] + e)) < c(E(P)) −c(E(Q[w,v] + e)) ≤c(E(P)), because Q[w,v] + e
is a circuit and c is conservative. In both cases we have a contradiction to the
assumption that P is a shortest s-w-path with at most k edges.
2
The same result holds for undirected graphs with nonnegative weights and
also for acyclic digraphs with arbitrary weights. It yields the recursion formulas
dist(s, s) = 0 and dist(s, w) = min{dist(s, v) + c((v, w)) : (v, w) ∈E(G)} for
w ∈V (G)\{s} which immediately solve the Shortest Path Problem for acyclic
digraphs (Exercise 6).
Proposition 7.2 is also the reason why most algorithms compute the shortest
paths from s to all other vertices. If one computes a shortest s-t-path P, one has
already computed a shortest s-v-path for each vertex v on P. Since we cannot
know in advance which vertices belong to P, it is only natural to compute shortest
s-v-paths for all v. We can store these s-v-paths very efﬁciently by just storing
the ﬁnal edge of each path.
We ﬁrst consider nonnegative edge weights, i.e. c : E(G) →R+. The Short-
est Path Problem can be solved by BFS if all weights are 1 (Proposition 2.18).
For weights c : E(G) →N one could replace an edge e by a path of length
c(e) and again use BFS. However, this might introduce an exponential number
of edges; recall that the input size is 

n log m + m log n + 
e∈E(G) log c(e)

,
where n = |V (G)| and m = |E(G)|.

7.1 Shortest Paths From One Source
145
A much better idea is to use the following algorithm, due to Dijkstra [1959]. It
is quite similar to Prim’s Algorithm for the Minimum Spanning Tree Problem
(Section 6.1).
Dijkstra’s Algorithm
Input:
A digraph G, weights c : E(G) →R+ and a vertex s ∈V (G).
Output:
Shortest paths from s to all v ∈V (G) and their lengths.
More precisely, we get the outputs l(v) and p(v) for all v ∈V (G).
l(v) is the length of a shortest s-v-path, which consists of a shortest
s-p(v)-path together with the edge (p(v), v). If v is not reachable
from s, then l(v) = ∞and p(v) is undeﬁned.
1⃝
Set l(s) := 0. Set l(v) := ∞for all v ∈V (G) \ {s}.
Set R := ∅.
2⃝
Find a vertex v ∈V (G) \ R such that l(v) =
min
w∈V (G)\R l(w).
3⃝
Set R := R ∪{v}.
4⃝
For all w ∈V (G) \ R such that (v, w) ∈E(G) do:
If l(w) > l(v) + c((v, w)) then
set l(w) := l(v) + c((v, w)) and p(w) := v.
5⃝
If R ̸= V (G) then go to 2⃝.
Theorem 7.3.
(Dijkstra [1959]) Dijkstra’s Algorithm works correctly.
Proof:
We prove that the following statements hold at any stage of the algorithm:
(a) For each v ∈V (G) \ {s} with l(v) < ∞we have p(v) ∈R, l(p(v)) +
c((p(v), v)) = l(v), and the sequence v, p(v), p(p(v)), . . . contains s.
(b) For all v ∈R: l(v) = dist(G,c)(s, v).
The statements trivially hold after 1⃝. l(w) is decreased to l(v) + c((v, w))
and p(w) is set to v in
4⃝only if v ∈R and w /∈R. As the sequence
v, p(v), p(p(v)), . . . contains s but no vertex outside R, in particular not w, (a)
is preserved by 4⃝.
(b) is trivial for v = s. Suppose that v ∈V (G) \ {s} is added to R in 3⃝, and
there is an s-v-path P in G that is shorter than l(v). Let y be the ﬁrst vertex on
P that belongs to (V (G) \ R) ∪{v}, and let x be the predecessor of y on P. Since
x ∈R, we have by 4⃝and the induction hypothesis:
l(y) ≤l(x) + c((x, y))
=
dist(G,c)(s, x) + c((x, y))
≤
c(E(P[s,y])) ≤c(E(P)) < l(v),
contradicting the choice of v in 2⃝.
2
The running time is obviously O(n2). Using a Fibonacci heap we can do better:

146
7. Shortest Paths
Theorem 7.4.
(Fredman and Tarjan [1987])
Dijkstra’s Algorithm imple-
mented with a Fibonacci heap runs in O(m + n log n) time, where n = |V (G)|
and m = |E(G)|.
Proof:
We apply Theorem 6.6 to maintain the set {(v,l(v)) : v ∈V (G) \
R, l(v) < ∞}. Then 2⃝and 3⃝are one deletemin-operation, while the update
of l(w) in
4⃝is an insert-operation if l(w) was inﬁnite and a decreasekey-
operation otherwise.
2
This is the best known strongly polynomial running time for the Shortest
Path Problem with nonnegative weights. (On different computational models,
Fredman and Willard [1994], Thorup [2000] and Raman [1997] achieved slightly
better running times.)
If the weights are integers within a ﬁxed range there is a simple linear-time
algorithm (Exercise 2). In general, running times of O(m log log cmax) (Johnson
[1982]) and O

m + n√log cmax

(Ahuja et al. [1990]) are possible for weights
c : E(G) →{0, . . . , cmax}. This has been improved by Thorup [2003] to O(m +
n log log cmax) and O(m +n log log n), but even the latter bound applies to integral
edge weights only, and the algorithm is not strongly polynomial.
For planar digraphs there is a linear-time algorithm due to Henzinger et al.
[1997]. Finally we mention that Thorup [1999] found a linear-time algorithm for
ﬁnding a shortest path in an undirected graph with nonnegative integral weights.
See also Pettie and Ramachandran [2002]; this paper also contains more references.
We now turn to an algorithm for general conservative weights:
Moore-Bellman-Ford Algorithm
Input:
A digraph G, conservative weights c : E(G) →R, and a vertex
s ∈V (G).
Output:
Shortest paths from s to all v ∈V (G) and their lengths.
More precisely, we get the outputs l(v) and p(v) for all v ∈V (G).
l(v) is the length of a shortest s-v-path which consists of a shortest
s-p(v)-path together with the edge (p(v), v). If v is not reachable
from s, then l(v) = ∞and p(v) is undeﬁned.
1⃝
Set l(s) := 0 and l(v) := ∞for all v ∈V (G) \ {s}.
2⃝
For i := 1 to n −1 do:
For each edge (v, w) ∈E(G) do:
If l(w) > l(v) + c((v, w)) then
set l(w) := l(v) + c((v, w)) and p(w) := v.
Theorem 7.5.
(Moore [1959], Bellman [1958], Ford [1956]) The Moore-Bell-
man-Ford Algorithm works correctly. Its running time is O(nm).
Proof:
The O(nm) running time is obvious. At any stage of the algorithm let
R := {v ∈V (G) : l(v) < ∞} and F := {(x, y) ∈E(G) : x = p(y)}. We claim:

7.1 Shortest Paths From One Source
147
(a) l(y) ≥l(x) + c((x, y)) for all (x, y) ∈F;
(b) If F contains a circuit C, then C has negative total weight;
(c) If c is conservative, then (R, F) is an arborescence rooted at s.
To prove (a), observe that l(y) = l(x) + c((x, y)) when p(y) is set to x and
l(x) is never increased.
To prove (b), suppose at some stage a circuit C in F was created by setting
p(y) := x. Then before the insertion we had l(y) > l(x) + c((x, y)) as well as
l(w) ≥l(v) + c((v, w)) for all (v, w) ∈E(C) \ {(x, y)} (by (a)). Summing these
inequalities (the l-values cancel), we see that the total weight of C is negative.
Since c is conservative, (b) implies that F is acyclic. Moreover, x ∈R \ {s}
implies p(x) ∈R, so (R, F) is an arborescence rooted at s.
Therefore l(x) is at least the length of the s-x-path in (R, F) for any x ∈R
(at any stage of the algorithm).
We claim that after k iterations of the algorithm, l(x) is at most the length
of a shortest s-x-path with at most k edges. This statement is easily proved by
induction: Let P be a shortest s-x-path with at most k edges and let (w, x) be the
last edge of P. Then, by Proposition 7.2, P[s,w] must be a shortest s-w-path with
at most k −1 edges, and by the induction hypothesis we have l(w) ≤c(E(P[s,w]))
after k −1 iterations. But in the k-th iteration edge (w, x) is also examined, after
which l(x) ≤l(w) + c((w, x)) ≤c(E(P)).
Since no path has more than n −1 edges, the above claim implies the correct-
ness of the algorithm.
2
This algorithm is still the fastest known strongly polynomial-time algorithm
for the Shortest Path Problem (with conservative weights). A scaling algorithm
due to Goldberg [1995] has a running time of O
√nm log(|cmin| + 2)

if the edge
weights are integral and at least cmin. For planar graphs, Fakcharoenphol and Rao
[2001] described an O(n log3 n)-algorithm.
If G contains negative circuits, no polynomial-time algorithm is known (the
problem becomes NP-hard; see Exercise 14(b) of Chapter 15). The main difﬁculty
is that Proposition 7.2 does not hold for general weights. It is not clear how to
construct a path instead of an arbitrary edge progression. If there are no negative
circuits, any shortest edge progression is a path, plus possibly some circuits of
zero weight that can be deleted. In view of this it is also an important question
how to detect negative circuits. The following concept due to Edmonds and Karp
[1972] is useful:
Deﬁnition 7.6.
Let G be a digraph with weights c : E(G) →R, and let π :
V (G) →R. Then for any (x, y) ∈E(G) we deﬁne the reduced cost of (x, y)
with respect to π by cπ((x, y)) := c((x, y)) + π(x) −π(y). If cπ(e) ≥0 for all
e ∈E(G), π is called a feasible potential.
Theorem 7.7.
Let G be a digraph with weights c : E(G) →R. There exists a
feasible potential of (G, c) if and only if c is conservative.
Proof:
If π is a feasible potential, we have for each circuit C:

148
7. Shortest Paths
0 ≤

e∈E(C)
cπ(e) =

e=(x,y)∈E(C)
(c(e) + π(x) −π(y)) =

e∈E(C)
c(e)
(the potentials cancel). So c is conservative.
On the other hand, if c is conservative, we add a new vertex s and edges (s, v)
of zero cost for all v ∈V (G). We run the Moore-Bellman-Ford Algorithm
on this instance and obtain numbers l(v) for all v ∈V (G). Since l(v) is the length
of a shortest s-v-path for all v ∈V (G), we have l(w) ≤l(v) + c((v, w)) for all
(v, w) ∈E(G). Hence l is a feasible potential.
2
This can be regarded as a special form of LP duality; see Exercise 8.
Corollary 7.8.
Given a digraph G with weights c : E(G) →R we can ﬁnd in
O(nm) time either a feasible potential or a negative circuit.
Proof:
As above, we add a new vertex s and edges (s, v) of zero cost for all
v ∈V (G). We run a modiﬁed version of the Moore-Bellman-Ford Algorithm
on this instance: Regardless of whether c is conservative or not, we run 1⃝and
2⃝as above. We obtain numbers l(v) for all v ∈V (G). If l is a feasible potential,
we are done.
Otherwise let (v, w) be any edge with l(w) > l(v) + c((v, w)). We claim that
the sequence w, v, p(v), p(p(v)), . . . contains a circuit. To see this, observe that
l(v) must have been changed in the ﬁnal iteration of 2⃝. Hence l(p(v)) has been
changed during the last two iterations, l(p(p(v))) has been changed during the
last three iterations, and so on. Since l(s) never changes, the ﬁrst |V (G)| places
of the sequence w, v, p(v), p(p(v)), . . . do not contain s, so a vertex must appear
twice in the sequence.
Thus we have found a circuit C in F := {(x, y) ∈E(G) : x = p(y)}∪{(v, w)}.
By (a) and (b) of the proof of Theorem 7.5, C has negative total weight.
2
In practice there are more efﬁcient methods to detect negative circuits; see
Cherkassky and Goldberg [1999].
7.2 Shortest Paths Between All Pairs of Vertices
Suppose we now want to ﬁnd a shortest s-t-path for all ordered pairs of vertices
(s, t) in a digraph:
All Pairs Shortest Paths Problem
Instance:
A digraph G and conservative weights c : E(G) →R.
Task:
Find numbers lst and vertices pst for all s, t ∈V (G) with s ̸= t,
such that lst is the length of a shortest s-t-path, and (pst, t) is the
ﬁnal edge of such a path (if it exists).

7.2 Shortest Paths Between All Pairs of Vertices
149
Of course we could run the Moore-Bellman-Ford Algorithm n times, once
for each choice of s. This immediately gives us an O(n2m)-algorithm. However,
one can do better:
Theorem 7.9.
The AllPairsShortestPathsProblem can be solved in O(mn+
n2 log n) time, where n = |V (G)| and m = |E(G)|.
Proof:
Let (G, c) be an instance. First we compute a feasible potential π, which
is possible in O(nm) time by Corollary 7.8. Then for each s ∈V (G) we do a
single-source shortest path computation from s using the reduced costs cπ instead
of c. For any vertex t the resulting s-t-path is also a shortest path with respect to
c, because the length of any s-t-path changes by π(s) −π(t), a constant. Since
the reduced costs are nonnegative, we can use Dijkstra’s Algorithm each time.
So, by Theorem 7.4, the total running time is O(mn + n(m + n log n)).
2
The same idea will be used again in Chapter 9 (in the proof of Theorem 9.12).
Pettie [2004] showed how to improve the running time to O(mn+n2 log log n);
this is the best known time bound. For dense graphs with nonnegative weights,
Zwick’s [2004] bound of O

n3√log log n/ log n

is slightly better. If all edge
weights are small integers, this can be improved using fast matrix multiplication;
see e.g. Zwick [2002].
The solution of the All Pairs Shortest Paths Problem also enables us to
compute the metric closure:
Deﬁnition 7.10.
Given a graph G (directed or undirected) with conservative
weights c : E(G) →R. The metric closure of (G, c) is the pair ( ¯G, ¯c), where ¯G
is the simple graph on V (G) that, for x, y ∈V (G) with x ̸= y, contains an edge
e = {x, y} (or e = (x, y) if G is directed) with weight ¯c(e) = dist(G,c)(x, y) if and
only if y is reachable from x in G.
Corollary 7.11.
Let G be a directed or undirected graph with conservative
weights c : E(G) →R. Then the metric closure of (G, c) can be computed in
O(mn + n2 log n) time.
Proof:
If G is undirected, we replace each edge by a pair of oppositely directed
edges. Then we solve the resulting instance of the All Pairs Shortest Paths
Problem.
2
The rest of the section is devoted to the Floyd-Warshall Algorithm, an-
other O(n3)-algorithm for the All Pairs Shortest Paths Problem. The main ad-
vantage of the Floyd-Warshall Algorithm is its simplicity. We assume w.l.o.g.
that the vertices are numbered 1, . . . , n.

150
7. Shortest Paths
Floyd-Warshall Algorithm
Input:
A digraph G with V (G) = {1, . . . , n} and conservative weights c :
E(G) →R.
Output:
Matrices (li j)1≤i, j≤n and (pi j)1≤i, j≤n where li j is the length of a short-
est path from i to j, and (pi j, j) is the ﬁnal edge of such a path (if
it exists).
1⃝
Set li j := c((i, j)) for all (i, j) ∈E(G).
Set li j := ∞for all (i, j) ∈(V (G) × V (G)) \ E(G) with i ̸= j.
Set lii := 0 for all i.
Set pi j := i for all i, j ∈V (G).
2⃝
For j := 1 to n do:
For i := 1 to n do: If i ̸= j then:
For k := 1 to n do: If k ̸= j then:
If lik > li j + ljk then set lik := li j + ljk and pik := pjk.
Theorem 7.12.
(Floyd [1962], Warshall [1962]) The Floyd-Warshall Algo-
rithm works correctly. Its running time is O(n3).
Proof:
The running time is obvious.
Claim:
After the algorithm has run through the outer loop for j = 1, 2, . . . , j0,
the variable lik contains the length of a shortest i-k-path with intermediate vertices
v ∈{1, . . . , j0} only (for all i and k), and (pik, k) is the ﬁnal edge of such a path.
This statement will be shown by induction for j0 = 0, . . . , n. For j0 = 0 it is
true by 1⃝, and for j0 = n it implies the correctness of the algorithm.
Suppose the claim holds for some j0 ∈{0, . . . , n −1}. We have to show that
it still holds for j0 + 1. For any i and k, during processing the outer loop for
j = j0 +1, lik (containing by the induction hypothesis the length of a shortest i-k-
path with intermediate vertices v ∈{1, . . . , j0} only) is replaced by li, j0+1 +lj0+1,k
if this value is smaller. It remains to show that the corresponding i-( j0 + 1)-path
P and the ( j0 + 1)-k-path Q have no inner vertex in common.
Suppose that there is an inner vertex belonging to both P and Q. By shortcut-
ting the maximal closed walk in P + Q (which by our assumption has nonnegative
weight because it is the union of circuits) we get an i-k-path R with intermediate
vertices v ∈{1, . . . , j0} only. R is no longer than li, j0+1 +lj0+1,k (and in particular
shorter than the lik before processing the outer loop for j = j0 + 1).
This contradicts the induction hypothesis since R has intermediate vertices
v ∈{1, . . . , j0} only.
2
Like the Moore-Bellman-Ford Algorithm, the Floyd-Warshall Algo-
rithm can also be used to detect the existence of negative circuits (Exercise 11).
The All Pairs Shortest Paths Problem in undirected graphs with arbitrary
conservative weights is more difﬁcult; see Theorem 12.13.

7.3 Minimum Mean Cycles
151
7.3 Minimum Mean Cycles
We can easily ﬁnd a circuit of minimum total weight in a digraph with conserva-
tive weights, using the above shortest path algorithms (see Exercise 12). Another
problem asks for a circuit whose mean weight is minimum:
Minimum Mean Cycle Problem
Instance:
A digraph G, weights c : E(G) →R.
Task:
Find a circuit C whose mean weight c(E(C))
|E(C)| is minimum, or decide
that G is acyclic.
In this section we show how to solve this problem with dynamic programming,
quite similar to the shortest path algorithms. We may assume that G is strongly
connected, since otherwise we can identify the strongly connected components
in linear time (Theorem 2.19) and solve the problem for each strongly connected
component separately. But for the following min-max theorem it sufﬁces to assume
that there is a vertex s from which all vertices are reachable. We consider not only
paths, but arbitrary edge progressions (where vertices and edges may be repeated).
Theorem 7.13.
(Karp [1978]) Let G be a digraph with weights c : E(G) →R.
Let s ∈V (G) such that each vertex is reachable from s. For x ∈V (G) and k ∈Z+
let
Fk(x) := min
 k

i=1
c((vi−1, vi)) : v0 = s, vk = x, (vi−1, vi) ∈E(G) for all i

be the minimum weight of an edge progression of length k from s to x (and ∞if
there is none). Let µ(G, c) be the minimum mean weight of a circuit in G (and
µ(G, c) = ∞if G is acyclic). Then
µ(G, c) =
min
x∈V (G)
max
0≤k≤n−1
Fk(x)<∞
Fn(x) −Fk(x)
n −k
.
Proof:
If G is acyclic, then Fn(x) = ∞for all x ∈V (G), so the theorem holds.
We now assume that µ(G, c) < ∞.
First we prove that if µ(G, c) = 0 then also
min
x∈V (G)
max
0≤k≤n−1
Fk(x)<∞
Fn(x) −Fk(x)
n −k
= 0.
Let G be a digraph with µ(G, c) = 0. G contains no negative circuit. Since c is
conservative, Fn(x) ≥dist(G,c)(s, x) = min0≤k≤n−1 Fk(x), so
max
0≤k≤n−1
Fk(x)<∞
Fn(x) −Fk(x)
n −k
≥0.

152
7. Shortest Paths
We show that there is a vertex x for which equality holds, i.e. Fn(x) =
dist(G,c)(s, x). Let C be any zero-weight circuit in G, and let w ∈V (C). Let
P be a shortest s-w-path followed by n repetitions of C. Let P′ consist of the ﬁrst
n edges of P, and let x be the end-vertex of P′. Since P is a minimum-weight
edge progression from s to w, any initial segment, in particular P′, must be a
minimum-weight edge progression. So Fn(x) = c(E(P′)) = dist(G,c)(s, x).
Having proved the theorem for the case µ(G, c) = 0, we now turn to the
general case. Note that adding a constant to all edge weights changes both µ(G, c)
and
min
x∈V (G)
max
0≤k≤n−1
Fk(x)<∞
Fn(x) −Fk(x)
n −k
by the same amount, namely this constant. By choosing this constant to be
−µ(G, c) we are back to the case µ(G, c) = 0.
2
This theorem suggests the following algorithm:
Minimum Mean Cycle Algorithm
Input:
A digraph G, weights c : E(G) →R.
Output:
A circuit C with minimum mean weight or the information that G is
acyclic.
1⃝
Add a vertex s and edges (s, x) with c((s, x)) := 0 for all x ∈V (G) to G.
2⃝
Set n := |V (G)|, F0(s) := 0, and F0(x) := ∞for all x ∈V (G) \ {s}.
3⃝
For k := 1 to n do:
For all x ∈V (G) do:
Set Fk(x) := ∞.
For all (w, x) ∈δ−(x) do:
If Fk−1(w) + c((w, x)) < Fk(x) then:
Set Fk(x) := Fk−1(w) + c((w, x)) and pk(x) := w.
4⃝
If Fn(x) = ∞for all x ∈V (G) then stop (G is acyclic).
5⃝
Let x be a vertex for which
max
0≤k≤n−1
Fk(x)<∞
Fn(x) −Fk(x)
n −k
is minimum.
6⃝
Let C be any circuit in the edge progression given by
pn(x), pn−1(pn(x)), pn−2(pn−1(pn(x))), . . ..
Corollary 7.14.
(Karp [1978]) The Minimum Mean Cycle Algorithm works
correctly. Its running time is O(nm).
Proof:
1⃝does not create any new circuit in G but makes Theorem 7.13 appli-
cable. It is obvious that 2⃝and 3⃝compute the numbers Fk(x) correctly. So if the
algorithm stops in 4⃝, G is indeed acyclic.

Exercises
153
Consider the instance (G, c′), where c′(e) := c(e) −µ(G, c) for all e ∈E(G).
On this instance the algorithm runs exactly the same way as with (G, c), the
only difference being the change of the F-values to F′
k(x) = Fk(x) −kµ(G, c).
By the choice of x in
5⃝, Theorem 7.13 and µ(G, c′) = 0 we have F′
n(x) =
min0≤k≤n−1 F′
k(x). Hence any edge progression from s to x with n edges and
length F′
n(x) in (G, c′) consists of a shortest s-x-path plus one or more circuits of
zero weight. These circuits have mean weight µ(G, c) in (G, c).
Hence each circuit on a minimum weight edge progression of length n from s
to x (for the vertex x chosen in 5⃝) is a circuit of minimum mean weight. In 6⃝
such a circuit is chosen.
The running time is dominated by 3⃝which obviously takes O(nm) time. Note
that 5⃝takes only O(n2) time.
2
This algorithm cannot be used for ﬁnding a circuit of minimum mean weight
in an undirected graph with edge weights. See Exercise 10 of Chapter 12.
Algorithms for more general minimum ratio problems have been proposed by
Megiddo [1979,1983] and Radzik [1993].
Exercises
1. Let G be a graph (directed or undirected) with weights c : E(G) →Z+, and
let s, t ∈V (G) such that t is reachable from s. Show that the minimum length
of an s-t-path equals the maximum number of cuts separating s and t such
that each edge e is contained in at most c(e) of them.
2. Suppose the weights are integers between 0 and C for some constant C.
Can one implement Dijkstra’s Algorithm for this special case with linear
running time?
Hint: Use an array indexed by 0, . . . , |V (G)|·C to store the vertices according
to their current l-value.
(Dial [1969])
3. Given a digraph G, weights c : E(G) →R+, and two vertices s, t ∈V (G).
Suppose there is only one shortest s-t-path P. Can one then ﬁnd the shortest
s-t-path different from P in polynomial time?
4. Modify Dijkstra’s Algorithm in order to solve the bottleneck path problem:
Given a digraph G, c : E(G) →R, and s, t ∈V (G), ﬁnd an s-t-path whose
longest edge is shortest possible.
5. Let G be a digraph with s, t ∈V (G). To each edge e ∈E(G) we assign a
number r(e) (its reliability), with 0 ≤r(e) ≤1. The reliability of a path P
is deﬁned to be the product of the reliabilities of its edges. The problem is to
ﬁnd an s-t-path of maximum reliability.
(a) Show that by taking logarithms one can reduce this problem to a Shor-
test Path Problem.
(b) Show how to solve this problem (in polynomial time) without taking
logarithms.

154
7. Shortest Paths
6. Given an acyclic digraph G, c : E(G) →R and s, t, ∈V (G). Show how to
ﬁnd a shortest s-t-path in G in linear time.
7. Given an acyclic digraph G, c : E(G) →R and s, t, ∈V (G). Show how to
ﬁnd the union of all longest s-t-paths in G in linear time.
8. Prove Theorem 7.7 using LP duality, in particular Theorem 3.19.
9. Let G be a digraph with conservative weights c : E(G) →R. Let s, t ∈V (G)
such that t is reachable from s. Prove that the minimum length of an s-t-path
in G equals the maximum of π(t) −π(s), where π is a feasible potential of
(G, c).
10. Let G be a digraph, V (G) = A
.
∪B and E(G[B]) = ∅. Moreover, suppose
that |δ(v)| ≤k for all v ∈B. Let s, t ∈V (G) and c : E(G) →R conservative.
Prove that then a shortest s-t-path can be found in O(|A|k|E(G)|) time, and
if c is nonnegative in O(|A|2) time.
(Orlin [1993])
11. Suppose that we run the Floyd-Warshall Algorithm on an instance (G, c)
with arbitrary weights c : E(G) →R. Prove that all lii (i = 1, . . . , n) remain
nonnegative if and only if c is conservative.
12. Given a digraph with conservative weights, show how to ﬁnd a circuit of
minimum total weight in polynomial time. Can you achieve an O(n3) running
time?
Hint: Modify the Floyd-Warshall Algorithm slightly.
Note: For general weights the problem includes the decision whether a given
digraph is Hamiltonian (and is thus NP-hard; see Chapter 15). How to ﬁnd
the minimum circuit in an undirected graph (with conservative weights) is
described in Section 12.2.
13. Let G be a complete (undirected) graph and c : E(G) →R+. Show that
(G, c) is its own metric closure if and only if the triangle inequality holds:
c({x, y})+c({y, z}) ≥c({x, z}) for any three distinct vertices x, y, z ∈V (G).
14. The timing constraints of a logic chip can be modelled by a digraph G with
edge weights c : E(G) →R+. The vertices represent the storage elements, the
edges represent paths through combinational logic, and the weights are worst-
case estimations of the propagation time of a signal. An important task in the
design of very large scale integrated (VLSI) circuits is to ﬁnd an optimum
clock schedule, i.e. a mapping a : V (G) →R such that a(v) + c((v, w)) ≤
a(w)+T for all (v, w) ∈E(G) and a number T which is as small as possible.
(T is the cycle time of the chip, and a(v) and a(v) + T are the “departure
time” and latest feasible “arrival time” of a signal at v, respectively.)
(a) Reduce the problem of ﬁnding the optimum T to a Minimum Mean
Cycle Problem.
(b) Show how the numbers a(v) of an optimum solution can be determined
efﬁciently.
(c) Typically, some of the numbers a(v) are ﬁxed in advance. Show how to
solve the problem in this case.
(Albrecht et al. [2002])

References
155
References
General Literature:
Ahuja, R.K., Magnanti, T.L., and Orlin, J.B. [1993]: Network Flows. Prentice-Hall, Engle-
wood Cliffs 1993, Chapters 4 and 5
Cormen, T.H., Leiserson, C.E., and Rivest, R.L. [1990]: Introduction to Algorithms. MIT
Press, Cambridge 1990, Chapters 23, 25 and 26
Dreyfus, S.E. [1969]: An appraisal of some shortest path algorithms. Operations Research
17 (1969), 395–412
Gallo, G., and Pallottino, S. [1988]: Shortest paths algorithms. Annals of Operations Re-
search 13 (1988), 3–79
Gondran, M., and Minoux, M. [1984]: Graphs and Algorithms. Wiley, Chichester 1984,
Chapter 2
Lawler, E.L. [1976]: Combinatorial Optimization: Networks and Matroids. Holt, Rinehart
and Winston, New York 1976, Chapter 3
Schrijver, A. [2003]: Combinatorial Optimization: Polyhedra and Efﬁciency. Springer,
Berlin 2003, Chapters 6–8
Tarjan, R.E. [1983]: Data Structures and Network Algorithms. SIAM, Philadelphia 1983,
Chapter 7
Cited References:
Ahuja, R.K., Mehlhorn, K., Orlin, J.B., and Tarjan, R.E. [1990]: Faster algorithms for the
shortest path problem. Journal of the ACM 37 (1990), 213–223
Albrecht, C., Korte, B., Schietke, J., and Vygen, J. [2002]: Maximum mean weight cycle
in a digraph and minimizing cycle time of a logic chip. Discrete Applied Mathematics
123 (2002), 103–127
Bellman, R.E. [1958]: On a routing problem. Quarterly of Applied Mathematics 16 (1958),
87–90
Cherkassky, B.V., and Goldberg, A.V. [1999]: Negative-cycle detection algorithms. Math-
ematical Programming A 85 (1999), 277–311
Dial, R.B. [1969]: Algorithm 360: shortest path forest with topological order. Communica-
tions of the ACM 12 (1969), 632–633
Dijkstra, E.W. [1959]: A note on two problems in connexion with graphs. Numerische
Mathematik 1 (1959), 269–271
Edmonds, J., and Karp, R.M. [1972]: Theoretical improvements in algorithmic efﬁciency
for network ﬂow problems. Journal of the ACM 19 (1972), 248–264
Fakcharoenphol, J., and Rao, S. [2001]: Planar graphs, negative edge weights, shortest
paths, and near-linear time. Proceedings of the 42nd Annual Symposium on Foundations
of Computer Science (2001), 232–241
Floyd, R.W. [1962]: Algorithm 97 – shortest path. Communications of the ACM 5 (1962),
345
Ford, L.R. [1956]: Network ﬂow theory. Paper P-923, The Rand Corporation, Santa Monica
1956
Fredman, M.L., and Tarjan, R.E. [1987]: Fibonacci heaps and their uses in improved net-
work optimization problems. Journal of the ACM 34 (1987), 596–615
Fredman, M.L., and Willard, D.E. [1994]: Trans-dichotomous algorithms for minimum
spanning trees and shortest paths. Journal of Computer and System Sciences 48 (1994),
533–551
Goldberg, A.V. [1995]: Scaling algorithms for the shortest paths problem. SIAM Journal
on Computing 24 (1995), 494–504
Henzinger, M.R., Klein, P., Rao, S., and Subramanian, S. [1997]: Faster shortest-path
algorithms for planar graphs. Journal of Computer and System Sciences 55 (1997), 3–23

156
7. Shortest Paths
Johnson, D.B. [1982]: A priority queue in which initialization and queue operations take
O(log log D) time. Mathematical Systems Theory 15 (1982), 295–309
Karp, R.M. [1978]: A characterization of the minimum cycle mean in a digraph. Discrete
Mathematics 23 (1978), 309–311
Megiddo, N. [1979]: Combinatorial optimization with rational objective functions. Mathe-
matics of Operations Research 4 (1979), 414–424
Megiddo, N. [1983]: Applying parallel computation algorithms in the design of serial al-
gorithms. Journal of the ACM 30 (1983), 852–865
Moore, E.F. [1959]: The shortest path through a maze. Proceedings of the International
Symposium on the Theory of Switching, Part II, Harvard University Press, 1959, 285–
292
Orlin, J.B. [1993]: A faster strongly polynomial minimum cost ﬂow algorithm. Operations
Research 41 (1993), 338–350
Pettie, S., and Ramachandran, V. [2002]: Computing shortest paths with comparisons and
additions. Proceedings of the 13th Annual ACM-SIAM Symposium on Discrete Algo-
rithms (2002), 267–276; to appear in SIAM Journal on Computing
Pettie, S. [2004]: A new approach to all-pairs shortest paths on real-weighted graphs.
Theoretical Computer Science 312 (2004), 47–74
Radzik, T. [1993]: Parametric ﬂows, weighted means of cuts, and fractional combinatorial
optimization. In: Complexity in Numerical Optimization (P.M. Pardalos, ed.), World
Scientiﬁc, Singapore 1993
Raman, R. [1997]: Recent results on the single-source shortest paths problem. ACM
SIGACT News 28 (1997), 81–87
Thorup, M. [1999]: Undirected single-source shortest paths with positive integer weights
in linear time. Journal of the ACM 46 (1999), 362–394
Thorup, M. [2000]: On RAM priority queues. SIAM Journal on Computing 30 (2000),
86–109
Thorup, M. [2003]: Integer priority queues with decrease key in constant time and the
single source shortest paths problem. Proceedings of the 35th Annual ACM Symposium
on Theory of Computing (2003), 149–158
Warshall, S. [1962]: A theorem on boolean matrices. Journal of the ACM 9 (1962), 11–12
Zwick, U. [2002]: All pairs shortest paths using bridging sets and rectangular matrix mul-
tiplication. Journal of the ACM 49 (2002), 289–317
Zwick, U. [2004]: A slightly improved sub-cubic algorithm for the all pairs shortest paths
problem with real edge lengths. Algorithms and Computation – ISAAC 2004; LNCS
3341 (R. Fleischer, G. Trippen, eds.), Springer, Berlin 2004, pp. 921–932

8. Network Flows
In this and the next chapter we consider ﬂows in networks. We have a digraph G
with edge capacities u : E(G) →R+ and two speciﬁed vertices s (the source)
and t (the sink). The quadruple (G, u, s, t) is sometimes called a network.
Our main motivation is to transport as many units as possible simultaneously
from s to t. A solution to this problem will be called a maximum ﬂow. Formally
we deﬁne:
Deﬁnition 8.1.
Given a digraph G with capacities u : E(G) →R+, a ﬂow is
a function f : E(G) →R+ with f (e) ≤u(e) for all e ∈E(G). We say that f
satisﬁes the ﬂow conservation rule at vertex v if
exf (v) :=

e∈δ−(v)
f (e) −

e∈δ+(v)
f (e) = 0.
A ﬂow satisfying the ﬂow conservation rule at every vertex is called a circulation.
Now given a network (G, u, s, t), an s-t-ﬂow is a ﬂow f satisfying exf (s) < 0
and exf (v) = 0 for all v ∈V (G) \ {s, t}. We deﬁne the value of an s-t-ﬂow f by
value ( f ) := −exf (s).
Now we can formulate the basic problem of this chapter:
Maximum Flow Problem
Instance:
A network (G, u, s, t).
Task:
Find an s-t-ﬂow of maximum value.
It causes no loss of generality to assume that G is simple as parallel edges can
be united beforehand.
This problem has numerous applications. For example, consider the Job As-
signment Problem: given n jobs, their processing times t1, . . . , tn ∈R+ and a
nonempty subset Si ⊆{1, . . . , m} of employees that can contribute to each job
i ∈{1, . . . , n}, we ask for numbers xi j ∈R+ for all i = 1, . . . , n and j ∈Si
(meaning how long employee j works on job i) such that all jobs are ﬁnished,
i.e. 
j∈Si xi j = ti for i = 1, . . . , n. Our goal was to minimize the amount of time
in which all jobs are done, i.e. T (x) := maxm
j=1

i: j∈Si xi j. Instead of solving this
problem with Linear Programming we look for a combinatorial algorithm.

158
8. Network Flows
We apply binary search for the optimum T (x). Then for one speciﬁc value T
we have to ﬁnd numbers xi j ∈R+ with 
j∈Si xi j = ti for all i and 
i: j∈Si xi j ≤T
for all j. We model the sets Si by a (bipartite) digraph with a vertex vi for each
job i, a vertex wj for each employee j and an edge (vi, wj) whenever j ∈Si. We
introduce two additional vertices s and t and edges (s, vi) for all i and (wj, t) for
all j. Let this graph be G. We deﬁne capacities u : E(G) →R+ by u((s, vi)) := ti
and u(e) := T for all other edges. Then the feasible solutions x with T (x) ≤T
evidently correspond to the s-t-ﬂows of value n
i=1 ti in (G, u). Indeed, these are
maximum ﬂows.
In Section 8.1 we describe a basic algorithm for the Maximum Flow Problem
and use it to prove the Max-Flow-Min-Cut Theorem, one of the best known results
in combinatorial optimization, which shows the relation to the problem of ﬁnding
a minimum capacity s-t-cut. Moreover we show that, for integral capacities, there
always exists an optimum ﬂow which is integral. The combination of these two re-
sults also implies Menger’s Theorem on disjoint paths as we discuss in Section 8.2.
Sections 8.3, 8.4 and 8.5 contain efﬁcient algorithms for the Maximum Flow
Problem. Then we shift attention to the problem of ﬁnding minimum cuts. Section
8.6 describes an elegant way to store the minimum capacity of an s-t-cut (which
equals the maximum value of an s-t-ﬂow) for all pairs of vertices s and t. Section
8.7 shows how the edge-connectivity, or the global minimum capacity cut in an
undirected graph can be determined more efﬁciently than by applying several
network ﬂow computations.
8.1 Max-Flow-Min-Cut Theorem
The deﬁnition of the Maximum Flow Problem suggests the following LP for-
mulation:
max

e∈δ+(s)
xe −

e∈δ−(s)
xe
s.t.

e∈δ−(v)
xe
=

e∈δ+(v)
xe
(v ∈V (G) \ {s, t})
xe
≤
u(e)
(e ∈E(G))
xe
≥
0
(e ∈E(G))
Since this LP is obviously bounded and the zero ﬂow f ≡0 is always feasible,
we have the following :
Proposition 8.2.
The MaximumFlowProblem always has an optimum solution.
2
Furthermore, by Theorem 4.18 there exists a polynomial-time algorithm. How-
ever, we are not satisﬁed with this, but will rather look for a combinatorial algo-
rithm (not using Linear Programming).

8.1 Max-Flow-Min-Cut Theorem
159
Recall that an s-t-cut in G is an edge set δ+(X) with s ∈X and t ∈V (G)\ X.
The capacity of an s-t-cut is the sum of the capacities of its edges. By a minimum
s-t-cut in (G, u) we mean an s-t-cut of minimum capacity (with respect to u) in
G.
Lemma 8.3.
For any A ⊆V (G) such that s ∈A, t /∈A, and any s-t-ﬂow f ,
(a) value ( f ) = 
e∈δ+(A) f (e) −
e∈δ−(A) f (e).
(b) value ( f ) ≤
e∈δ+(A) u(e).
Proof:
(a): Since the ﬂow conservation rule holds for v ∈A \ {s},
value ( f )
=

e∈δ+(s)
f (e) −

e∈δ−(s)
f (e)
=

v∈A
⎛
⎝
e∈δ+(v)
f (e) −

e∈δ−(v)
f (e)
⎞
⎠
=

e∈δ+(A)
f (e) −

e∈δ−(A)
f (e).
(b): This follows from (a) by using 0 ≤f (e) ≤u(e) for e ∈E(G).
2
In other words, the value of a maximum ﬂow cannot exceed the capacity of a
minimum s-t-cut. In fact, we have equality here. To see this, we need the concept
of augmenting paths which will reappear in several other chapters.
Deﬁnition 8.4.
For a digraph G we deﬁne
↔
G := (V (G), E(G)
.
∪{
←e : e ∈
E(G)}), where for e = (v, w) ∈E(G) we deﬁne
←e to be a new edge from w
to v. We call
←e the reverse edge of e and vice versa. Note that if e = (v, w), e′ =
(w, v) ∈E(G), then
←e and e′ are two parallel edges in
↔
G.
Given a digraph G with capacities u : E(G) →R+ and a ﬂow f , we deﬁne
residual capacities u f : E(
↔
G) →R+ by u f (e) := u(e)−f (e) and u f (
←e ) := f (e)
for all e ∈E(G). The residual graph G f is the graph (V (G), {e ∈E(
↔
G) : u f (e) >
0}).
Given a ﬂow f and a path (or circuit) P in G f , to augment f along P by γ
means to do the following for each e ∈E(P): if e ∈E(G) then increase f (e) by
γ , otherwise – if e =
←
e0 for e0 ∈E(G) – decrease f (e0) by γ .
Given a network (G, u, s, t) and an s-t-ﬂow f , an f-augmenting path is an
s-t-path in the residual graph G f .
Using this concept, the following algorithm for the Maximum Flow Problem,
due to Ford and Fulkerson [1957], is natural. We ﬁrst restrict ourselves to integral
capacities.

160
8. Network Flows
Ford-Fulkerson Algorithm
Input:
A network (G, u, s, t) with u : E(G) →Z+.
Output:
An s-t-ﬂow f of maximum value.
1⃝
Set f (e) = 0 for all e ∈E(G).
2⃝
Find an f -augmenting path P. If none exists then stop.
3⃝
Compute γ := min
e∈E(P) u f (e). Augment f along P by γ and go to 2⃝.
Edges where the minimum in 3⃝is attained are sometimes called bottleneck
edges. The choice of γ guarantees that f continues to be a ﬂow. Since P is an
s-t-path, the ﬂow conservation rule is preserved at all vertices except s and t.
To ﬁnd an augmenting path is easy (we just have to ﬁnd any s-t-path in
G f ). However, we should be careful how to do this. In fact, if we allow irrational
capacities (and have bad luck when choosing the augmenting paths), the algorithm
might not terminate at all (Exercise 2).
N
N
N
N
1
s
t
Fig. 8.1.
Even in the case of integer capacities, we may have an exponential number
of augmentations. This is illustrated by the simple network shown in Figure 8.1,
where the numbers are the edge capacities (N ∈N). If we choose an augmenting
path of length 3 in each iteration, we can augment the ﬂow by just one unit each
time, so we need 2N iterations. Observe that the input length is O(log N), since
capacities are of course encoded in binary form. We shall overcome these problems
in Section 8.3.
We now claim that when the algorithm stops, then f is indeed a maximum
ﬂow:
Theorem 8.5.
An s-t-ﬂow f is maximum if and only if there is no f -augmenting
path.
Proof:
If there is an augmenting path P, then 3⃝of the Ford-Fulkerson Al-
gorithm computes a ﬂow of greater value, so f is not maximum. If there is no
augmenting path, this means that t is not reachable from s in G f . Let R be the set
of vertices reachable from s in G f . By the deﬁnition of G f , we have f (e) = u(e)
for all e ∈δ+
G(R) and f (e) = 0 for all e ∈δ−
G(R).

8.1 Max-Flow-Min-Cut Theorem
161
Now Lemma 8.3 (a) says that
value ( f ) =

e∈δ+
G(R)
u(e)
which by Lemma 8.3 (b) implies that f is a maximum ﬂow.
2
In particular, for any maximum s-t-ﬂow we have an s-t-cut whose capacity
equals the value of the ﬂow. Together with Lemma 8.3 (b) this yields the central
result of network ﬂow theory, the Max-Flow-Min-Cut Theorem:
Theorem 8.6.
(Ford and Fulkerson [1956], Dantzig and Fulkerson [1956])
In
a network the maximum value of an s-t-ﬂow equals the minimum capacity of an
s-t-cut.
2
An alternative proof was proposed by Elias, Feinstein and Shannon [1956].
The Max-Flow-Min-Cut Theorem also follows quite easily from LP duality; see
Exercise 4 of Chapter 3.
If all capacities are integers, γ in 3⃝of the Ford-Fulkerson Algorithm is
always integral. Since there is a maximum ﬂow of ﬁnite value (Proposition 8.2),
the algorithm terminates after a ﬁnite number of steps. Therefore we have the
following important consequence:
Corollary 8.7.
(Dantzig and Fulkerson [1956])
If the capacities of a network
are integers, then there exists an integral maximum ﬂow.
2
This corollary – sometimes called the Integral Flow Theorem – can also be
proved easily by using the total unimodularity of the incidence matrix of a digraph
(Exercise 3).
We close this section with another easy but useful observation, the Flow De-
composition Theorem:
Theorem 8.8.
(Gallai [1958], Ford and Fulkerson [1962]) Let (G, u, s, t) be a
network and let f be an s-t-ﬂow in G. Then there exists a family P of s-t-paths
and a family C of circuits in G along with weights w : P ∪C →R+ such that
f (e) = 
P∈P∪C: e∈E(P) w(P) for all e ∈E(G), 
P∈P w(P) = value ( f ), and
|P| + |C| ≤|E(G)|.
Moreover, if f is integral then w can be chosen to be integral.
Proof:
We construct P, C and w by induction on the number of edges with
nonzero ﬂow. Let e = (v0, w0) be an edge with f (e) > 0. Unless w0 = t,
there must be an edge (w0, w1) with nonzero ﬂow. Set i
:= 1. If wi
∈
{t, v0, w0, . . . , wi−1} we stop. Otherwise there must be an edge (wi, wi+1) with
nonzero ﬂow; we set i := i + 1 and continue. The process must end after at most
n steps.
We do the same in the other direction: if v0 ̸= s, there must be an edge (v1, v0)
with nonzero ﬂow, and so on. At the end we have found either a circuit or an s-t-

162
8. Network Flows
path in G, and we have used edges with positive ﬂow only. Let P be this circuit
or path. Let w(P) := mine∈E(P) f (e). Set f ′(e) := f (e) −w(P) for e ∈E(P)
and f ′(e) := f (e) for e ̸∈E(P). An application of the induction hypothesis to f ′
completes the proof.
2
8.2 Menger’s Theorem
Consider Corollary 8.7 and Theorem 8.8 in the special case where all capacities are
1. Here integral s-t-ﬂows can be regarded as collections of edge-disjoint s-t-paths
and circuits. We obtain the following important theorem:
Theorem 8.9.
(Menger [1927])
Let G be a graph (directed or undirected), let
s and t be two vertices, and k ∈N. Then there are k edge-disjoint s-t-paths if and
only if after deleting any k −1 edges t is still reachable from s.
Proof:
Necessity is obvious. To prove sufﬁciency in the directed case, let
(G, u, s, t) be a network with unit capacities u ≡1 such that t is reachable from
s even after deleting any k −1 edges. This implies that the minimum capacity of
an s-t-cut is at least k. By the Max-Flow-Min-Cut Theorem 8.6 and Corollary 8.7
there is an integral s-t-ﬂow of value at least k. By Theorem 8.8 this ﬂow can be
decomposed into integral ﬂows on s-t-paths (and possibly some circuits). Since
all capacities are 1 we must have at least k edge-disjoint s-t-paths.
v
w
e
v
w
xe
ye
Fig. 8.2.
To prove sufﬁciency in the undirected case, let G be an undirected graph with
two vertices s and t such that t is reachable from s even after deleting any k −1
edges. This property obviously remains true if we replace each undirected edge
e = {v, w} by ﬁve directed edges (v, xe), (w, xe), (xe, ye), (ye, v), (ye, w) where
xe and ye are new vertices (see Figure 8.2). Now we have a digraph G′ and, by
the ﬁrst part, k edge-disjoint s-t-paths in G′. These can be easily transformed to
k edge-disjoint s-t-paths in G.
2
In turn it is easy to derive the Max-Flow-Min-Cut Theorem (at least for rational
capacities) from Menger’s Theorem. We now consider the vertex-disjoint version
of Menger’s Theorem. We call two paths vertex-disjoint if they have no edge and
no inner vertex in common (they may have one or two common endpoints).

8.2 Menger’s Theorem
163
Theorem 8.10.
(Menger [1927]) Let G be a graph (directed or undirected), let
s and t be two non-adjacent vertices, and k ∈N. Then there are k vertex-disjoint
s-t-paths if and only if after deleting any k −1 vertices (distinct from s and t) t is
still reachable from s.
Proof:
Necessity is again trivial. Sufﬁciency in the directed case follows from
the directed part of Theorem 8.9 by the following elementary construction: we
replace each vertex v of G by two vertices v′ and v′′ and an edge (v′, v′′). Each
edge (v, w) of G is replaced by (v′′, w′). Any set of k −1 edges in the new graph
G′ whose deletion makes t′ unreachable from s′′ implies a set of at most k −1
vertices in G whose deletion makes t unreachable from s. Moreover, edge-disjoint
s′′-t′-paths in the new graph correspond to vertex-disjoint s-t-paths in the old one.
The undirected version follows from the directed one by the same construction
as in the proof of Theorem 8.9 (Figure 8.2).
2
The following corollary is an important consequence of Menger’s Theorem:
Corollary 8.11.
(Whitney [1932]) An undirected graph G with at least two ver-
tices is k-edge-connected if and only if for each pair s, t ∈V (G) with s ̸= t there
are k edge-disjoint s-t-paths.
An undirected graph G with more than k vertices is k-connected if and only if
for each pair s, t ∈V (G) with s ̸= t there are k vertex-disjoint s-t-paths.
Proof:
The ﬁrst statement follows directly from Theorem 8.9.
To prove the second statement let G be an undirected graph with more than
k vertices. If G has k −1 vertices whose deletion makes the graph disconnected,
then it cannot have k vertex-disjoint s-t-paths for each pair s, t ∈V (G).
Conversely, if G does not have k vertex-disjoint s-t-paths for some s, t ∈
V (G), then we consider two cases. If s and t are non-adjacent, then by Theorem
8.10 G has k −1 vertices whose deletion separates s and t.
If s and t are joined by a set F of parallel edges, |F| ≥1, then G −F has no
k −|F| vertex-disjoint s-t-paths, so by Theorem 8.10 it has a set X of k −|F|−1
vertices whose deletion separates s and t. Let v ∈V (G) \ (X ∪{s, t}). Then v
cannot be reachable from s and from t in (G −F) −X, say v is not reachable
from s. Then v and s are in different connected components of G −(X ∪{t}). 2
In many applications one looks for edge-disjoint or vertex-disjoint paths be-
tween several pairs of vertices. The four versions of Menger’s Theorem (directed
and undirected, vertex- and edge-disjoint) correspond to four versions of the Dis-
joint Paths Problem:
Directed/UndirectedEdge-/Vertex-DisjointPathsProblem
Instance:
Two directed/undirected graphs (G, H) on the same vertices.
Task:
Find a family (Pf )f ∈E(H) of edge-disjoint/vertex-disjoint paths in G
such that for each f = (t, s) or f = {t, s} in H, Pf is an s-t-path.

164
8. Network Flows
Such a family is called a solution of (G, H). We say that Pf realizes f . The
edges of G are called supply edges, the edges of H demand edges. A vertex
incident to some demand edge is called a terminal.
Above we considered the special case when H is just a set of k parallel edges.
The general Disjoint Paths Problem will be discussed in Chapter 19. Here we
only note the following useful special case of Menger’s Theorem:
Proposition 8.12.
Let (G, H) be an instance of the Directed Edge-Disjoint
Paths Problem where H is just a set of parallel edges and G + H is Eulerian.
Then (G, H) has a solution.
Proof:
Since G+H is Eulerian, every edge, in particular any f ∈E(H), belongs
to some circuit C. We take C −f as the ﬁrst path of our solution, delete C, and
apply induction.
2
8.3 The Edmonds-Karp Algorithm
In Exercise 2 it is shown that it is necessary to specialize
2⃝of the Ford-
Fulkerson Algorithm. Instead of choosing an arbitrary augmenting path it is
a good idea to look for a shortest one, i.e. an augmenting path with a minimum
number of edges. With this simple idea Edmonds and Karp [1972] obtained the
ﬁrst polynomial-time algorithm for the Maximum Flow Problem.
Edmonds-Karp Algorithm
Input:
A network (G, u, s, t).
Output:
An s-t-ﬂow f of maximum value.
1⃝
Set f (e) = 0 for all e ∈E(G).
2⃝
Find a shortest f -augmenting path P. If there is none then stop.
3⃝
Compute γ := min
e∈E(P) u f (e). Augment f along P by γ and go to 2⃝.
This means that
2⃝of the Ford-Fulkerson Algorithm should be imple-
mented by BFS (see Section 2.3).
Lemma 8.13.
Let f1, f2, . . . be a sequence of ﬂows such that fi+1 results from fi
by augmenting along Pi, where Pi is a shortest fi-augmenting path. Then
(a) |E(Pk)| ≤|E(Pk+1)| for all k.
(b) |E(Pk)|+2 ≤|E(Pl)| for all k < l such that Pk ∪Pl contains a pair of reverse
edges.
Proof:
(a): Consider the graph G1 which results from Pk
.
∪Pk+1 by deleting
pairs of reverse edges. (Edges appearing both in Pk and Pk+1 are taken twice.)

8.3 The Edmonds-Karp Algorithm
165
Note that E(G1) ⊆E(G fk), since any edge in E(G fk+1) \ E(G fk) must be the
reverse of an edge in Pk.
Let H1 simply consist of two copies of (t, s). Obviously G1 + H1 is Eulerian.
Thus by Proposition 8.12 there are two edge-disjoint s-t-paths Q1 and Q2. Since
E(G1) ⊆E(G fk), both Q1 and Q2 are fk-augmenting paths. Since Pk was a
shortest fk-augmenting path, |E(Pk)| ≤|E(Q1)| and |E(Pk)| ≤|E(Q2)|. Thus,
2|E(Pk)| ≤|E(Q1)| + |E(Q2)| ≤|E(G1)| ≤|E(Pk)| + |E(Pk+1)|,
implying |E(Pk)| ≤|E(Pk+1)|.
(b): By part (a) it is enough to prove the statement for those k,l such that for
k < i < l, Pi ∪Pl contains no pair of reverse edges.
As above, consider the graph G1 which results from Pk
.
∪Pl by deleting pairs
of reverse edges. Again, E(G1) ⊆E(G fk): To see this, observe that E(Pk) ⊆
E(G fk), E(Pl) ⊆E(G fl), and any edge of E(G fl)\ E(G fk) must be the reverse of
an edge in one of Pk, Pk+1, . . . , Pl−1. But – due to the choice of k and l – among
these paths only Pk contains the reverse of an edge in Pl.
Let H1 again consist of two copies of (t, s). Since G1 + H1 is Eulerian,
Proposition 8.12 guarantees that there are two edge-disjoint s-t-paths Q1 and
Q2. Again Q1 and Q2 are both fk-augmenting. Since Pk was a a shortest fk-
augmenting path, |E(Pk)| ≤|E(Q1)| and |E(Pk)| ≤|E(Q2)|. We conclude that
2|E(Pk)| ≤|E(Q1)| + |E(Q2)| ≤|E(Pk)| + |E(Pl)| −2
(since we have deleted at least two edges). This completes the proof.
2
Theorem 8.14.
(Edmonds and Karp [1972]) Regardless of the edge capacities,
the Edmonds-Karp Algorithm stops after at most mn
2 augmentations, where m
and n denote the number of edges and vertices, respectively.
Proof:
Let P1, P2, . . . be the augmenting paths chosen during the Edmonds-
Karp Algorithm. By the choice of γ in 3⃝of the algorithm, each augmenting
path contains at least one bottleneck edge.
For any edge e, let Pi1, Pi2, . . . be the subsequence of augmenting paths con-
taining e as a bottleneck edge. Obviously, between Pij and Pij+1 there must be
an augmenting path Pk (ij < k < ij+1) containing
←e . By Lemma 8.13 (b),
|E(Pij)| + 4 ≤|E(Pk)| + 2 ≤|E(Pij+1)| for all j. If e has neither s nor t as
endpoint, we have 3 ≤|E(Pij)| ≤n −1, for all j, and there can be at most n
4
augmenting paths containing e as a bottleneck edge. Otherwise at most one of the
augmenting paths contains e or
←e as bottleneck edge.
Since any augmenting path must contain at least one edge of
↔
G as a bottleneck
edge, there can be at most |E(
↔
G)| n
4 = mn
2 augmenting paths.
2
Corollary 8.15.
The Edmonds-Karp Algorithm solves the Maximum Flow
Problem in O(m2n) time.
Proof:
By Theorem 8.14 there are at most mn
2 augmentations. Each augmentation
uses BFS and thus takes O(m) time.
2

166
8. Network Flows
8.4 Blocking Flows and Fujishige’s Algorithm
Around the time when Edmonds and Karp observed how to obtain a polynomial-
time algorithm for the Maximum Flow Problem, Dinic [1970] independently
found an even better algorithm. It is based on the following deﬁnition:
Deﬁnition 8.16.
Given a network (G, u, s, t) and an s-t-ﬂow f . The level graph
GL
f of G f is the graph

V (G),
5
(e = (x, y) ∈E(G f ) : distG f (s, x) + 1 = distG f (s, y)
6
.
Note that the level graph is acyclic. The level graph can be constructed easily
by BFS in O(m) time.
Lemma 8.13(a) says that the length of the shortest augmenting paths in the
Edmonds-Karp Algorithm is non-decreasing. Let us call a sequence of aug-
menting paths of the same length a phase of the algorithm. Let f be the ﬂow at
the beginning of a phase. The proof of Lemma 8.13 (b) yields that all augmenting
paths of this phase must already be augmenting paths in G f . Therefore all these
paths must be s-t-paths in the level graph of G f .
Deﬁnition 8.17.
Given a network (G, u, s, t), an s-t-ﬂow f is called blocking if
(V (G), {e ∈E(G) : f (e) < u(e)}) contains no s-t-path.
The union of the augmenting paths in a phase can be regarded as a blocking
ﬂow in GL
f . Note that a blocking ﬂow is not necessarily maximum. The above
considerations suggest the following algorithmic scheme:
Dinic’s Algorithm
Input:
A network (G, u, s, t).
Output:
An s-t-ﬂow f of maximum value.
1⃝
Set f (e) = 0 for all e ∈E(G).
2⃝
Construct the level graph GL
f of G f .
3⃝
Find a blocking s-t-ﬂow f ′ in GL
f . If f ′ = 0 then stop.
4⃝
Augment f by f ′ and go to 2⃝.
Since the length of a shortest augmenting path increases from phase to phase,
Dinic’s Algorithm stops after at most n −1 phases. So it remains to show how
a blocking ﬂow in an acyclic graph can be found efﬁciently. Dinic obtained an
O(nm) bound for each phase, which is not very difﬁcult to show (Exercise 14).
This bound has been improved to O(n2) by Karzanov [1974]; see also (Mal-
hotra, Kumar and Maheshwari [1978]). Subsequent improvements are due to
Cherkassky [1977], Galil [1980], Galil and Namaad [1980], Shiloach [1978],
Sleator [1980], and Sleator and Tarjan [1983]. The last two references describe

8.4 Blocking Flows and Fujishige’s Algorithm
167
an O(m log n)-algorithm for ﬁnding blocking ﬂows in an acyclic network using
a data structure called dynamic trees. Using this as a subroutine of Dinic’s Al-
gorithm one has an O(mn log n)-algorithm for the Maximum Flow Problem.
However, we do not describe any of the above-mentioned algorithms here (see
Tarjan [1983]), because an even faster network ﬂow algorithm will be the subject
of the next section.
We close this section by describing the weakly polynomial algorithm by Fu-
jishige [2003], mainly because of its simplicity:
Fujishige’s Algorithm
Input:
A network (G, u, s, t) with u : E(G) →Z+.
Output:
An s-t-ﬂow f of maximum value.
1⃝
Set f (e) = 0 for all e ∈E(G). Set α := max{u(e) : e ∈E(G)}.
2⃝
Set i := 1, v1 := s, X := ∅, and b(v) := 0 for all v ∈V (G).
3⃝
For e = (vi, w) ∈δ+
G f (vi) with w /∈{v1, . . . , vi} do:
Set b(w) := b(w) + u f (e). If b(w) ≥α then set X := X ∪{w}.
4⃝
If X = ∅then:
Set α := ⌊α
2 ⌋. If α = 0 then stop else go to 2⃝.
5⃝
Set i := i + 1. Choose vi ∈X and set X := X \ {vi}.
If vi ̸= t then go to 3⃝.
6⃝
Set β(t) := α and β(v) := 0 for all v ∈V (G) \ {t}.
While i > 1 do:
For e = (p, vi) ∈δ−
G f (vi) with p ∈{v1, . . . , vi−1} do:
Set β′ := min{β(vi), u f (e)}.
Augment f along e by β′.
Set β(vi) := β(vi) −β′ and β(p) := β(p) + β′.
Set i := i −1.
7⃝
Go to 2⃝.
Theorem 8.18.
Fujishige’s Algorithm correctly solves the Maximum Flow
Problem for integral capacities u : E(G) →Z+ in O(mn log umax) time, where
n := |V (G)|, m := |E(G)| and umax := max{u(e) : e ∈E(G)}.
Proof:
We may assume that G is simple as parallel edges can be united
beforehand. Let us call an iteration a sequence of steps ending with
4⃝or
7⃝. In
2⃝– 5⃝, v1, . . . , vi is always an order of a subset of vertices such that
b(vj) = u f (E+({v1, . . . , vj−1}, {vj})) ≥α for j = 2, . . . , i. In 6⃝the ﬂow f is
augmented with the invariant 
v∈V (G) β(v) = α, and by the above the result is
an s-t-ﬂow whose value is α units larger.
Thus after at most n −1 iterations, α will be decreased for the ﬁrst time. When
we decrease α to α′ = ⌊α
2 ⌋≥α
3 in 4⃝, we have an s-t-cut δ+
G f ({v1, . . . , vi}) in G f
of capacity less than α(|V (G)|−i) because b(v) = u f (E+({v1, . . . , vi}, {v})) < α

168
8. Network Flows
for all v ∈V (G) \ {v1, . . . , vi}. By Lemma 8.3(b), a maximum s-t-ﬂow in G f
has value less than α(n −i) < 3α′n. Hence after less than 3n iterations, α will be
decreased again. If α is decreased from 1 to 0, we have an s-t-cut of capacity 0
in G f , so f is maximum.
As α is decreased at most 1 + log umax times before it reaches 0, and each
iteration between two changes of α takes O(m) time, the overall running time is
O(mn log umax).
2
Such a scaling technique is useful in many contexts and will reappear in
Chapter 9. Fujishige [2003] also described a variant of his algorithm without
scaling, where vi in 5⃝is chosen as a vertex attaining max{b(v) : v ∈V (G) \
{v1, . . . , vi−1}}. The resulting order is called MA order and will reappear in Section
8.7. The running time of this variant is slightly higher than the above and not
strongly polynomial either (Shioura [2004]).
8.5 The Goldberg-Tarjan Algorithm
In this section we shall describe the Push-Relabel Algorithm due to Goldberg
and Tarjan [1988]. We shall derive an O(n2√m) bound for the running time.
Sophisticated implementations using dynamic trees (see Sleator and Tarjan
[1983]) result in network ﬂow algorithms with running time O

nm log n2
m

(Gold-
berg and Tarjan [1988]) and O

nm log
 n
m
√log umax + 2

, where umax is the
maximum (integral) edge capacity (Ahuja, Orlin and Tarjan [1989]). The best
known bounds today are O

nm log2+m/(n log n) n

(King, Rao and Tarjan [1994])
and
O

min{m1/2, n2/3}m log
n2
m

log umax

(Goldberg and Rao [1998]).
By deﬁnition and Theorem 8.5, a ﬂow f is a maximum s-t-ﬂow if and only
if the following conditions hold:
– exf (v) = 0
for all v ∈V (G) \ {s, t};
– There is no f -augmenting path.
In the algorithms discussed so far, the ﬁrst condition is always satisﬁed, and
the algorithms stop when the second condition is satisﬁed. The Push-Relabel
Algorithm starts with an f satisfying the second condition and maintains it
throughout. Naturally it stops when the ﬁrst condition is satisﬁed as well. Since f
will not be an s-t-ﬂow during the algorithm (except at termination), we introduce
the weaker term of an s-t-preﬂow.
Deﬁnition 8.19.
Given a network (G, u, s, t), an s-t-preﬂow is a function f :
E(G) →R+ satisfying f (e) ≤u(e) for all e ∈E(G) and exf (v) ≥0 for all
v ∈V (G) \ {s}. We call a vertex v ∈V (G) \ {s, t} active if exf (v) > 0.

8.5 The Goldberg-Tarjan Algorithm
169
Obviously an s-t-preﬂow is an s-t-ﬂow if and only if there are no active
vertices.
Deﬁnition 8.20.
Let (G, u, s, t) be a network and f an s-t-preﬂow. A distance
labeling is a function ψ : V (G) →Z+ such that ψ(t) = 0, ψ(s) = n and
ψ(v) ≤ψ(w) + 1 for all (v, w) ∈E(G f ). An edge e = (v, w) ∈E(
↔
G) is called
admissible if e ∈E(G f ) and ψ(v) = ψ(w) + 1.
If ψ is a distance labeling, ψ(v) (for v ̸= s) must be a lower bound on the
distance to t (number of edges in a shortest v-t-path) in G f .
The Push-Relabel Algorithm to be described below always works with an
s-t-preﬂow f and a distance labeling ψ. It starts with the preﬂow that is equal
to the capacity on each edge leaving s and zero on all other edges. The initial
distance labeling is ψ(s) = n and ψ(v) = 0 for all v ∈V (G) \ {s}.
Then the algorithm performs the update operations Push (updating f ) and
Relabel (updating ψ) in any order.
Push-Relabel Algorithm
Input:
A network (G, u, s, t).
Output:
A maximum s-t-ﬂow f .
1⃝
Set f (e) := u(e) for each e ∈δ+(s).
Set f (e) := 0 for each e ∈E(G) \ δ+(s).
2⃝
Set ψ(s) := n and ψ(v) := 0 for all v ∈V (G) \ {s}.
3⃝
While there exists an active vertex do:
Let v be an active vertex.
If no e ∈δ+
G f (v) is admissible
then Relabel(v),
else let e ∈δ+
G f (v) be an admissible edge and Push(e).
Push(e)
1⃝
Set γ := min{exf (v), u f (e)}, where v is the vertex with e ∈δ+
G f (v).
2⃝
Augment f along e by γ .
Relabel(v)
1⃝
Set ψ(v) := min{ψ(w) + 1 : e = (v, w) ∈E(G f )}.
Proposition 8.21.
During the execution of the Push-Relabel Algorithm f is
always an s-t-preﬂow and ψ is always a distance labeling with respect to f .
Proof:
We have to show that the procedures Push and Relabel preserve these
properties. It is clear that after a Push operation, f is still an s-t-preﬂow. A

170
8. Network Flows
Relabel operation does not even change f . Moreover, after a Relabel operation
ψ is still a distance labeling.
It remains to show that after a Push operation, ψ is still a distance labeling
with respect to the new preﬂow. We have to check ψ(a) ≤ψ(b) + 1 for all new
edges (a, b) in G f . But if we apply Push(e) for some e = (v, w), the only possible
new edge in G f is the reverse edge of e, and here we have ψ(w) = ψ(v) −1,
since e is admissible.
2
Lemma 8.22.
If f is an s-t-preﬂow and ψ is a distance labeling with respect to
f , then:
(a) s is reachable from any active vertex v in G f .
(b) If w is reachable from v in G f for some v, w ∈V (G), then ψ(v) ≤ψ(w) +
n −1.
(c) t is not reachable from s in G f .
Proof:
(a): Let v be an active vertex, and let R be the set of vertices reachable
from v in G f . Then f (e) = 0 for all e ∈δ−
G(R). So

w∈R
exf (w) =

e∈δ−
G(R)
f (e) −

e∈δ+
G(R)
f (e) ≤0.
But v is active, meaning exf (v) > 0, and therefore there must exist a vertex w ∈R
with exf (w) < 0. Since f is an s-t-preﬂow, this vertex must be s.
(b): Suppose there is a v-w-path in G f , say with vertices v = v0, v1, . . . , vk =
w. Since there is a distance labeling ψ with respect to f , ψ(vi) ≤ψ(vi+1) + 1
for i = 0, . . . , k −1. So ψ(v) ≤ψ(w) + k. Note that k ≤n −1.
(c): follows from (b) as ψ(s) = n and ψ(t) = 0.
2
Part (c) helps us to prove the following:
Theorem 8.23.
When the algorithm terminates, f is a maximum s-t-ﬂow.
Proof:
f is an s-t-ﬂow because there are no active vertices. Lemma 8.22(c)
implies that there is no augmenting path. Then by Theorem 8.5 we know that f
is maximum.
2
The question now is how many Push and Relabel operations are performed.
Lemma 8.24.
(a) For each v ∈V (G), ψ(v) is strictly increased by every Relabel(v), and is
never decreased.
(b) At any stage of the algorithm, ψ(v) ≤2n −1 for all v ∈V (G).
(c) No vertex is relabelled more than 2n −1 times. The total number of Relabel
operations is at most 2n2 −n.

8.5 The Goldberg-Tarjan Algorithm
171
Proof:
(a): ψ is changed only in the Relabel procedure. If no e ∈δ+
G f (v)
is admissible, then Relabel(v) strictly increases ψ(v) (because ψ is a distance
labeling at any time).
(b): We only change ψ(v) if v is active. By Lemma 8.22(a) and (b), ψ(v) ≤
ψ(s) + n −1 = 2n −1.
(c): follows directly from (a) and (b).
2
We shall now analyse the number of Push operations. We distinguish between
saturating pushes (where u f (e) = 0 after the push) and nonsaturating pushes.
Lemma 8.25.
The number of saturating pushes is at most 2mn.
Proof:
After each saturating push from v to w, another such push cannot occur
until ψ(w) increases by at least 2, a push from w to v occurs, and ψ(v) increases
by at least 2. Together with Lemma 8.24(a) and (b), this proves that there are at
most n saturating pushes on each edge (v, w) ∈E(
↔
G).
2
The number of nonsaturating pushes can be in the order of n2m in general
(Exercise 19). By choosing an active vertex v with ψ(v) maximum in 3⃝we can
prove a better bound. As usual we denote n := |V (G)|, m := |E(G)| and may
assume n ≤m ≤n2.
Lemma 8.26.
If we always choose v to be an active vertex with ψ(v) maximum
in 3⃝of the Push-Relabel Algorithm, the number of nonsaturating pushes is
at most 8n2√m.
Proof:
Call a phase the time between two subsequent changes of ψ∗:=
max{ψ(v) : v active}. As ψ∗can only increase by relabeling, its total increase
is at most 2n2. As ψ∗= 0 initially, it can decrease at most 2n2 times, and the
number of phases is at most 4n2.
Call a phase cheap if it contains at most √m nonsaturating pushes and expen-
sive otherwise. Clearly there are at most 4n2√m nonsaturating pushes in cheap
phases.
Let
	 :=

v∈V (G):v active
|{w ∈V (G) : ψ(w) ≤ψ(v)}|.
Initially 	 ≤n2. A relabeling step may increase 	 by at most n. A saturating push
may increase 	 by at most n. A nonsaturating push does not increase 	. Since
	 = 0 at termination, the total decrease of 	 is at most n2+n(2n2−n)+n(2mn) ≤
4mn2.
Now consider the nonsaturating pushes in an expensive phase. Each of them
pushes ﬂow along an edge (v, w) with ψ(v) = ψ∗= ψ(w) + 1, deactivating v
and possibly activating w.
As the phase ends by relabeling or by deactivating the last active vertex v
with ψ(v) = ψ∗, the set of vertices w with ψ(w) = ψ∗remains constant during
the phase, and it contains more than √m vertices as the phase is expensive.

172
8. Network Flows
Hence each nonsaturating push in an expensive phase decreases 	 by at least
√m. Thus the total number of nonsaturating pushes in expensive phases is at
most 4mn2
√m = 4n2√m.
2
This proof is due to Cheriyan and Mehlhorn [1999]. We ﬁnally get:
Theorem 8.27.
(Goldberg and Tarjan [1988], Cheriyan and Maheshwari [1989],
Tunc¸el [1994])
The Push-Relabel Algorithm solves the Maximum Flow
Problem correctly and can be implemented to run in O(n2√m) time.
Proof:
The correctness follows from Proposition 8.21 and Theorem 8.23.
As in Lemma 8.26 we always choose v in 3⃝to be an active vertex with ψ(v)
maximum. To make this easy we keep track of doubly-linked lists L0, . . . , L2n−1,
where Li contains the active vertices v with ψ(v) = i. These lists can be updated
during each Push and Relabel operation in constant time.
We can then start by scanning Li for i = 0. When a vertex is relabelled,
we increase i accordingly. When we ﬁnd a list Li for the current i empty (after
deactivating the last active vertex at that level), we decrease i until Li is nonempty.
As we increase i at most 2n2 times by Lemma 8.24(c), we also decrease i at most
2n2 times.
As a second data structure, we store a doubly-linked list Av containing the ad-
missible edges leaving v for each vertex v. They can also be updated in each Push
operation in constant time, and in each Relabel operation in time proportional
to the total number of edges incident to the relabelled vertex.
So Relabel(v) takes a total of O(|δG(v)|) time, and by Lemma 8.24(c) the
overall time for relabelling is O(mn). Each Push takes constant time, and by
Lemma 8.25 and Lemma 8.26 the total number of pushes is O(n2√m).
2
8.6 Gomory-Hu Trees
Any algorithm for the Maximum Flow Problem also implies a solution to the
following problem:
Minimum Capacity Cut Problem
Instance:
A network (G, u, s, t).
Task:
An s-t-cut in G with minimum capacity.
Proposition 8.28.
The Minimum Capacity Cut Problem can be solved in the
same running time as the Maximum Flow Problem, in particular in O(n2√m)
time.
Proof:
For a network (G, u, s, t) we compute a maximum s-t-ﬂow f and deﬁne
X to be the set of all vertices reachable from s in G f . X can be computed with the
Graph Scanning Algorithm in linear time (Proposition 2.17). By Lemma 8.3

8.6 Gomory-Hu Trees
173
and Theorem 8.5, δ+
G(X) constitutes a minimum capacity s-t-cut. The O(n2√m)
running time follows from Theorem 8.27 (and is not best possible).
2
In this section we consider the problem of ﬁnding a minimum capacity s-t-cut
for each pair of vertices s, t in an undirected graph G with capacities u : E(G) →
R+.
This problem can be reduced to the above one: For all pairs s, t ∈V (G) we
solve the Minimum Capacity Cut Problem for (G′, u′, s, t), where (G′, u′) arises
from (G, u) by replacing each undirected edge {v, w} by two oppositely directed
edges (v, w) and (w, v) with u′((v, w)) = u′((w, v)) = u({v, w}). In this way we
obtain minimum s-t-cuts for all s, t after
n
2

ﬂow computations.
This section is devoted to the elegant method of Gomory and Hu [1961], which
requires only n −1 ﬂow computations. We shall see some applications in Sections
12.3 and 20.3.
Deﬁnition 8.29.
Let G be an undirected graph and u : E(G) →R+ a capac-
ity function. For two vertices s, t ∈V (G) we denote by λst their local edge-
connectivity, i.e. the minimum capacity of a cut separating s and t.
The edge-connectivity of a graph is obviously the minimum local edge-
connectivity with respect to unit capacities.
Lemma 8.30.
For all vertices i, j, k ∈V (G) we have λik ≥min(λi j, λjk).
Proof:
Let δ(A) be a cut with i ∈A, k ∈V (G)\ A and u(δ(A)) = λik. If j ∈A
then δ(A) separates j and k, so u(δ(A)) ≥λjk. If j ∈V (G)\A then δ(A) separates
i and j, so u(δ(A)) ≥λi j. We conclude that λik = u(δ(A)) ≥min(λi j, λjk).
2
Indeed, this condition is not only necessary but also sufﬁcient for numbers
(λi j)1≤i, j≤n with λi j = λji to be local edge-connectivities of some graph (Exercise
23).
Deﬁnition 8.31.
Let G be an undirected graph and u : E(G) →R+ a capacity
function. A tree T is called a Gomory-Hu tree for (G, u) if V (T ) = V (G) and
λst =
min
e∈E(Pst) u(δG(Ce))
for all s, t ∈V (G),
where Pst is the (unique) s-t-path in T and, for e ∈E(T ), Ce and V (G) \ Ce are
the connected components of T −e (i.e. δG(Ce) is the fundamental cut of e with
respect to T ).
We shall see that every graph possesses a Gomory-Hu tree. This implies that
for any undirected graph G there is a list of n −1 cuts such that for each pair
s, t ∈V (G) a minimum s-t-cut belongs to the list.
In general, a Gomory-Hu tree cannot be chosen as a subgraph of G. For
example, consider G = K3,3 and u ≡1. Here λst = 3 for all s, t ∈V (G). It is
easy to see that the Gomory-Hu trees for (G, u) are exactly the stars with ﬁve
edges.

174
8. Network Flows
The main idea of the algorithm for constructing a Gomory-Hu tree is as follows.
First we choose any s, t ∈V (G) and ﬁnd some minimum s-t-cut, say δ(A). Let
B := V (G) \ A. Then we contract A (or B) to a single vertex, choose any
s′, t′ ∈B (or s′, t′ ∈A, respectively) and look for a minimum s′-t′-cut in the
contracted graph G′. We continue this process, always choosing a pair s′, t′ of
vertices not separated by any cut obtained so far. At each step, we contract – for
each cut E(A′, B′) obtained so far – A′ or B′, depending on which part does not
contain s′ and t′.
Eventually each pair of vertices is separated. We have obtained a total of n −1
cuts. The crucial observation is that a minimum s′-t′-cut in the contracted graph
G′ is also a minimum s′-t′-cut in G. This is the subject of the following lemma.
Note that when contracting a set A of vertices in (G, u), the capacity of each edge
in G′ is the capacity of the corresponding edge in G.
Lemma 8.32.
Let G be an undirected graph and u : E(G) →R+ a capacity
function. Let s, t ∈V (G), and let δ(A) be a minimum s-t-cut in (G, u). Let now
s′, t′ ∈V (G) \ A, and let (G′, u′) arise from (G, u) by contracting A to a single
vertex. Then for any minimum s′-t′-cut δ(K ∪{A}) in (G′, u′), δ(K ∪A) is a
minimum s′-t′-cut in (G, u).
Proof:
Let s, t, A, s′, t′, G′, u′ be as above. W.l.o.g. s ∈A. It sufﬁces to prove
that there is a minimum s′-t′-cut δ(A′) in (G, u) such that A ⊂A′. So let δ(C)
be any minimum s′-t′-cut in (G, u). W.l.o.g. s ∈C.
A
V (G) \ A
C
V (G) \ C
s
s′
t′
Fig. 8.3.
Since u(δ(·)) is submodular (cf. Lemma 2.1(c)), we have u(δ(A))+u(δ(C)) ≥
u(δ(A ∩C)) + u(δ(A ∪C)). But δ(A ∩C) is an s-t-cut, so u(δ(A ∩C)) ≥λst =
u(δ(A)). Therefore u(δ(A ∪C)) ≤u(δ(C)) = λs′t′ proving that δ(A ∪C) is a
minimum s′-t′-cut. (See Figure 8.3.)
2

8.6 Gomory-Hu Trees
175
Now we describe the algorithm which constructs a Gomory-Hu tree. Note that
the vertices of the intermediate trees T will be vertex sets of the original graph;
indeed they form a partition of V (G). At the beginning, the only vertex of T is
V (G). In each iteration, a vertex of T containing at least two vertices of G is
chosen and split into two.
Gomory-Hu Algorithm
Input:
An undirected graph G and a capacity function u : E(G) →R+.
Output:
A Gomory-Hu tree T for (G, u).
1⃝
Set V (T ) := {V (G)} and E(T ) := ∅.
2⃝
Choose some X ∈V (T ) with |X| ≥2. If no such X exists then go to 6⃝.
3⃝
Choose s, t ∈X with s ̸= t.
For each connected component C of T −X do: Let SC := 
Y∈V (C) Y.
Let (G′, u′) arise from (G, u) by contracting SC to a single vertex vC for
each connected component C of T −X.
(So V (G′) = X ∪{vC : C is a connected component of T −X}.)
4⃝
Find a minimum s-t-cut δ(A′) in (G′, u′). Let B′ := V (G′) \ A′.
Set A :=
⎛
⎝
vC∈A′\X
SC
⎞
⎠∪(A′ ∩X) and B :=
⎛
⎝
vC∈B′\X
SC
⎞
⎠∪(B′ ∩X).
5⃝
Set V (T ) := (V (T ) \ {X}) ∪{A ∩X, B ∩X}.
For each edge e = {X, Y} ∈E(T ) incident to the vertex X do:
If Y ⊆A then set e′ := {A ∩X, Y} else set e′ := {B ∩X, Y}.
Set E(T ) := (E(T ) \ {e}) ∪{e′} and w(e′) := w(e).
Set E(T ) := E(T ) ∪{{A ∩X, B ∩X}} and
w({A ∩X, B ∩X}) := u′(δG′(A′)).
Go to 2⃝.
6⃝
Replace all {x} ∈V (T ) by x and all {{x}, {y}} ∈E(T ) by {x, y}. Stop.
Figure 8.4 illustrates the modiﬁcation of T in 5⃝. To prove the correctness of
this algorithm, we ﬁrst show the following lemma:
Lemma 8.33.
Each time at the end of 4⃝we have
(a) A
.
∪B = V (G)
(b) E(A, B) is a minimum s-t-cut in (G, u).
Proof:
The elements of V (T ) are always nonempty subsets of V (G), indeed
V (T ) constitutes a partition of V (G). From this, (a) follows easily.
We now prove (b). The claim is trivial for the ﬁrst iteration (since here G′ =
G). We show that the property is preserved in each iteration.
Let C1, . . . , Ck be the connected components of T −X. Let us contract them
one by one; for i = 0, . . . , k let (Gi, ui) arise from (G, u) by contracting each

176
8. Network Flows
X
A ∩X
B ∩X
(a)
(b)
Fig. 8.4.
of SC1, . . . , SCi to a single vertex. So (Gk, uk) is the graph which is denoted by
(G′, u′) in 3⃝of the algorithm.
Claim:
For any minimum s-t-cut δ(Ai) in (Gi, ui), δ(Ai−1) is a minimum
s-t-cut in (Gi−1, ui−1), where
Ai−1 :=
 (Ai \ {vCi}) ∪SCi
if vCi ∈Ai
Ai
if vCi /∈Ai .
Applying this claim successively for k, k −1, . . . , 1 implies (b).
To prove the claim, let δ(Ai) be a minimum s-t-cut in (Gi, ui). By our as-
sumption that (b) is true for the previous iterations, δ(SCi) is a minimum si-ti-cut
in (G, u) for some appropriate si, ti ∈V (G). Furthermore, s, t ∈V (G) \ SCi. So
applying Lemma 8.32 completes the proof.
2
Lemma 8.34.
At any stage of the algorithm (until 6⃝is reached) for all e ∈E(T )

8.6 Gomory-Hu Trees
177
w(e) = u
⎛
⎝δG
⎛
⎝
Z∈Ce
Z
⎞
⎠
⎞
⎠,
where Ce and V (T ) \ Ce are the connected components of T −e. Moreover for all
e = {P, Q} ∈E(T ) there are vertices p ∈P and q ∈Q with λpq = w(e).
Proof:
Both statements are trivial at the beginning of the algorithm when T
contains no edges; we show that they are never violated. So let X be vertex of
T chosen in
2⃝in some iteration of the algorithm. Let s, t, A′, B′, A, B be as
determined in 3⃝and 4⃝next. W.l.o.g. assume s ∈A′.
Edges of T not incident to X are not affected by 5⃝. For the new edge {A ∩
X, B ∩X}, w(e) is clearly set correctly, and we have λst = w(e), s ∈A ∩X,
t ∈B ∩X.
So let us consider an edge e = {X, Y} that is replaced by e′ in 5⃝. We assume
w.l.o.g. Y ⊆A, so e′ = {A ∩X, Y}. Assuming that the assertions were true for e
we claim that they remain true for e′. This is trivial for the ﬁrst assertion, because
w(e) = w(e′) and u

δG

Z∈Ce Z

does not change.
To show the second statement, we assume that there are p ∈X, q ∈Y with
λpq = w(e). If p ∈A∩X then we are done. So henceforth assume that p ∈B ∩X
(see Figure 8.5).
A ∩X
B ∩X
s
q
t
p
Y
Fig. 8.5.
We claim that λsq = λpq. Since λpq = w(e) = w(e′) and s ∈A ∩X, this will
conclude the proof.
By Lemma 8.30,
λsq ≥min{λst, λtp, λpq}.
Since by Lemma 8.33(b) E(A, B) is a minimum s-t-cut, and since s, q ∈A, we
may conclude from Lemma 8.32 that λsq does not change if we contract B. Since

178
8. Network Flows
t, p ∈B, this means that adding an edge {t, p} with arbitrary high capacity does
not change λsq. Hence
λsq ≥min{λst, λpq}.
Now observe that λst ≥λpq because the minimum s-t-cut E(A, B) also separates
p and q. So we have
λsq ≥λpq.
To prove equality, observe that w(e) is the capacity of a cut separating X and
Y, and thus s and q. Hence
λsq ≤w(e) = λpq.
This completes the proof.
2
Theorem 8.35.
(Gomory and Hu [1961]) The Gomory-Hu Algorithm works
correctly. Every undirected graph possesses a Gomory-Hu tree, and such a tree is
found in O(n3√m) time.
Proof:
The complexity of the algorithm is clearly determined by n −1 times
the complexity of ﬁnding a minimum s-t-cut, since everything else can be imple-
mented in O(n3) time. By Proposition 8.28 we obtain the O(n3√m) bound.
We prove that the output T of the algorithm is a Gomory-Hu tree for (G, u).
It should be clear that T is a tree with V (T ) = V (G). Now let s, t ∈V (G). Let
Pst be the (unique) s-t-path in T and, for e ∈E(T ), let Ce and V (G) \ Ce be the
connected components of T −e.
Since δ(Ce) is an s-t-cut for each e ∈E(Pst),
λst ≤
min
e∈E(Pst) u(δ(Ce)).
On the other hand, a repeated application of Lemma 8.30 yields
λst ≥
min
{v,w}∈E(Pst) λvw.
Hence applying Lemma 8.34 to the situation before execution of 6⃝(where each
vertex X of T is a singleton) yields
λst ≥
min
e∈E(Pst) u(δ(Ce)),
so equality holds.
2
A similar algorithm for the same task (which might be easier to implement)
was suggested by Gusﬁeld [1990].

8.7 The Minimum Cut in an Undirected Graph
179
8.7 The Minimum Cut in an Undirected Graph
If we are only interested in a minimum capacity cut in an undirected graph G
with capacities u : E(G) →R+, there is a simpler method using n −1 ﬂow
computations: just compute the minimum s-t-cut for some ﬁxed vertex s and each
t ∈V (G) \ {s}. However, there are more efﬁcient algorithms.
Hao and Orlin [1994] found an O(nm log n2
m )-algorithm for determining the
minimum capacity cut. They use a modiﬁed version of the Push-Relabel Al-
gorithm.
If we just want to compute the edge-connectivity of the graph (i.e. unit capac-
ities), the currently fastest algorithm is due to Gabow [1995] with running time
O(m+λ2n log
n
λ(G)), where λ(G) is the edge-connectivity (observe that 2m ≥λn).
Gabow’s algorithm uses matroid intersection techniques. We remark that the Max-
imum Flow Problem in undirected graphs with unit capacities can also be solved
faster than in general (Karger and Levine [1998]).
Nagamochi and Ibaraki [1992] found a completely different algorithm to de-
termine the minimum capacity cut in an undirected graph. Their algorithm does
not use max-ﬂow computations at all. In this section we present this algorithm
in a simpliﬁed form due to Stoer and Wagner [1997] and independently to Frank
[1994]. We start with an easy deﬁnition.
Deﬁnition 8.36.
Given a graph G with capacities u : E(G) →R+, we call an
order v1, . . . , vn of the vertices an MA (maximum adjacency) order if for all
i ∈{2, . . . , n}:

e∈E({v1,...,vi−1},{vi})
u(e) =
max
j∈{i,...,n}

e∈E({v1,...,vi−1},{vj})
u(e).
Proposition 8.37.
Given a graph G with capacities u : E(G) →R+, an MA
order can be found in O(m + n log n) time.
Proof:
Consider the following algorithm. First set α(v) := 0 for all v ∈V (G).
Then for i := 1 to n do the following: choose vi from among V (G)\{v1, . . . , vi−1}
such that it has maximum α-value (breaking ties arbitrarily), and set α(v) :=
α(v) + 
e∈E({vi},{v}) u(e) for all v ∈V (G) \ {v1, . . . , vi}.
The correctness of this algorithm is obvious. By implementing it with a Fi-
bonacci heap, storing each vertex v with key −α(v) until it is selected, we get
a running time of O(m + n log n) by Theorem 6.6 as there are n insert-, n
deletemin- and (at most) m decreasekey-operations.
2
Lemma 8.38.
(Stoer and Wagner [1997], Frank [1994]) Let G be a graph with
n := |V (G)| ≥2, capacities u : E(G) →R+ and an MA order v1, . . . , vn. Then
λvn−1vn =

e∈E({vn},{v1,...,vn−1})
u(e).

180
8. Network Flows
Proof:
Of course we only have to show “≥”. We shall use induction on |V (G)|+
|E(G)|. For |V (G)| < 3 the statement is trivial. We may assume that there is no
edge e = {vn−1, vn} ∈E(G), because otherwise we would delete it (both left-hand
side and right-hand side decrease by u(e)) and apply the induction hypothesis.
Denote the right-hand side by R. Of course v1, . . . , vn−1 is an MA order in
G −vn. So by induction,
λG−vn
vn−2vn−1 =

e∈E({vn−1},{v1,...,vn−2})
u(e) ≥

e∈E({vn},{v1,...,vn−2})
u(e) = R.
Here the inequality holds because v1, . . . , vn was an MA order for G. The last
equality is true because {vn−1, vn} /∈E(G). So λG
vn−2vn−1 ≥λG−vn
vn−2vn−1 ≥R.
On the other hand v1, . . . , vn−2, vn is an MA order in G−vn−1. So by induction,
λG−vn−1
vn−2vn
=

e∈E({vn},{v1,...,vn−2})
u(e) = R,
again because {vn−1, vn} /∈E(G). So λG
vn−2vn ≥λG−vn−1
vn−2vn
= R.
Now by Lemma 8.30 λvn−1vn ≥min{λvn−1vn−2, λvn−2vn} ≥R.
2
Note that the existence of two vertices x, y with λxy = 
e∈δ(x) u(e) was al-
ready shown by Mader [1972], and follows easily from the existence of a Gomory-
Hu tree (Exercise 25).
Theorem 8.39.
(Nagamochi and Ibaraki [1992], Stoer and Wagner [1997]) The
minimum capacity cut in an undirected graph with nonnegative capacities can be
found in O(mn + n2 log n) time.
Proof:
We may assume that the given graph G is simple since we can unite
parallel edges. Denote by λ(G) the minimum capacity of a cut in G. The algorithm
proceeds as follows:
Let G0 := G. In the i-th step (i = 1, . . . , n−1) choose vertices x, y ∈V (Gi−1)
with
λGi−1
xy
=

e∈δGi−1(x)
u(e).
By Proposition 8.37 and Lemma 8.38 this can be done in O(m +n log n) time. Set
γi := λGi−1
xy , zi := x, and let Gi result from Gi−1 by contracting {x, y}. Observe
that
λ(Gi−1) = min{λ(Gi), γi},
(8.1)
because a minimum cut in Gi−1 either separates x and y (in this case its capacity
is γi) or does not (in this case contracting {x, y} does not change anything).
After arriving at Gn−1 which has only one vertex, we choose an k ∈{1, . . . , n−
1} for which γk is minimum. We claim that δ(X) is a minimum capacity cut in
G, where X is the vertex set in G whose contraction resulted in the vertex zk of
Gk−1. But this is easy to see, since by (8.1) λ(G) = min{γ1, . . . , γn−1} = γk and
γk is the capacity of the cut δ(X).
2

Exercises
181
A randomized contraction algorithm for ﬁnding the minimum cut (with high
probability) is discussed in Exercise 29. Moreover, we mention that the vertex-
connectivity of a graph can be computed by O(n2) ﬂow computations (Exercise
30).
In this section we have shown how to minimize f (X) := u(δ(X)) over ∅̸=
X ⊂V (G). Note that this f : 2V (G) →R+ is submodular and symmetric (i.e.
f (A) = f (V (G)\A) for all A). The algorithm presented here has been generalized
by Queyranne [1998] to minimize general symmetric submodular functions; see
Section 14.5.
Exercises
1. Let (G, u, s, t) be a network, and let δ+(X) and δ+(Y) be minimum s-t-cuts
in (G, u). Show that δ+(X ∩Y) and δ+(X ∪Y) are also minimum s-t-cuts in
(G, u).
2. Show that in case of irrational capacities, the Ford-Fulkerson Algorithm
may not terminate at all.
Hint: Consider the following network (Figure 8.6):
x1
x2
x3
x4
y1
y2
y3
y4
s
t
Fig. 8.6.
All lines represent edges in both directions. All edges have capacity S =
1
1−σ
except
u((x1, y1)) = 1,
u((x2, y2)) = σ,
u((x3, y3)) = u((x4, y4)) = σ 2
where σ =
√
5−1
2
. Note that σ n = σ n+1 + σ n+2.
(Ford and Fulkerson [1962])
3.
∗
Let G be a digraph and M the incidence matrix of G. Prove that for all
c,l, u ∈ZE(G) with l ≤u:
max
5
cx : x ∈ZE(G), l ≤x ≤u, Mx = 0
6
=
min

y′u −y′′l : y′, y′′ ∈ZE(G)
+
, zM + y′ −y′′ = c for some z ∈ZV (G)
.
Show how this implies Theorem 8.6 and Corollary 8.7.

182
8. Network Flows
4. Prove Hoffman’s circulation theorem: Given a digraph G and lower and upper
capacities l, u : E(G) →R+ with l(e) ≤u(e) for all e ∈E(G), there is
circulation f with l(e) ≤f (e) ≤u(e) for all e ∈E(G) if and only if

e∈δ−(X)
l(e) ≤

e∈δ+(X)
u(e)
for all X ⊆V (G).
Note: Hoffman’s circulation theorem in turn quite easily implies the Max-
Flow-Min-Cut Theorem.
(Hoffman [1960])
5. Consider a network (G, u, s, t), a maximum s-t-ﬂow f and the residual graph
G f . Form a digraph H from G f by contracting the set S of vertices reachable
from s to a vertex vS, contracting the set T of vertices from which t is
reachable to a vertex vT , and contracting each strongly connected component
X of G f −(S ∪T ) to a vertex vX. Observe that H is acyclic. Prove that there
is a one-to-one correspondence between the sets X ⊆V (G) for which δ+
G(X)
is a minimum s-t-cut in (G, u) and the sets Y ⊆V (H) for which δ+
H(Y) is a
directed vT -vS-cut in H (i.e. a directed cut in H separating vT and vS).
Note: This statement also holds for G f without any contraction instead of H.
However, we shall use the statement in the above form in Section 20.4.
(Picard and Queyranne [1980])
6. Let G be a digraph and c : E(G) →R. We look for a set X ⊂V (G) with
s ∈X and t /∈X such that 
e∈δ+(X) c(e) −
e∈δ−(X) c(e) is minimum. Show
how to reduce this problem to the Minimum Capacity Cut Problem.
Hint: Construct a network where all edges are incident to s or t.
7.
∗
Let G be an acyclic digraph with mappings σ, τ, c : E(G) →R+, and a
number C ∈R+. We look for a mapping x : E(G) →R+ such that σ(e) ≤
x(e) ≤τ(e) for all e ∈E(G) and 
e∈E(G)(τ(e) −x(e))c(e) ≤C. Among the
feasible solutions we want to minimize the length (with respect to x) of the
longest path in G.
The meaning behind the above is the following. The edges correspond to jobs,
σ(e) and τ(e) stand for the minimum and maximum completion time of job
e, and c(e) is the cost of reducing the completion time of job e by one unit.
If there are two jobs e = (i, j) and e′ = ( j, k), job e has to be ﬁnished before
job e′ can be processed. We have a ﬁxed budget C and want to minimize the
total completion time.
Show how to solve this problem using network ﬂow techniques. (This appli-
cation is known as PERT, program evaluation and review technique, or CPM,
critical path method.)
Hint: Introduce one source s and one sink t. Start with x = τ and successively
reduce the length of the longest s-t-path (with respect to x) at the minimum
possible cost. Use Exercise 7 of Chapter 7, Exercise 4 of Chapter 3, and
Exercise 6.
(Phillips and Dessouky [1977])
8.
∗
Let (G, c, s, t) be a network such that G is planar even when an edge e = (s, t)
is added. Consider the following algorithm. Start with the ﬂow f ≡0 and let

Exercises
183
G′ := G f . At each step consider the boundary B of a face of G′+e containing
e (with respect to some ﬁxed planar embedding). Augment f along B −e.
Let G′ consist of the forward edges of G f only and iterate as long as t is
reachable from s in G′.
Prove that this algorithm computes a maximum s-t-ﬂow. Use Theorem 2.40
to show that it can be implemented to run in O(n2) time.
(Ford and Fulkerson [1956], Hu [1969])
Note: The problem can be solved in O(n) time; for general planar networks
an O(n log n)-algorithm has been found by Weihe [1997].
9. Show that the directed edge-disjoint version of Menger’s Theorem 8.9 also
follows directly from Theorem 6.17.
10. Let G be a graph (directed or undirected), x, y, z three vertices, and α, β ∈N
with α ≤λxy, β ≤λxz and α+β ≤max{λxy, λxz}. Prove that there are α x-y-
paths and β x-z-paths such that these α + β paths are pairwise edge-disjoint.
11. Let G be a digraph that contains k edge-disjoint s-t-paths for any two vertices
s and t (such a graph is called strongly k-edge-connected).
Let H be any digraph with V (H) = V (G) and |E(H)| = k. Prove that
the instance (G, H) of the Directed Edge-Disjoint Paths Problem has a
solution.
(Mader [1981] and Shiloach [1979])
12. Let G be a digraph with at least k edges. Prove: G contains k edge-disjoint
s-t-paths for any two vertices s and t if and only if for any k distinct edges
e1 = (x1, y1), . . . , ek = (xk, yk), G −{e1, . . . , ek} contains k edge-disjoint
spanning arborescences T1, . . . , Tk such that Ti is rooted at yi (i = 1, . . . , k).
Note: This generalizes Exercise 11. Hint: Use Theorem 6.17.
(Su [1997])
13. Let G be a digraph with capacities c : E(G) →R+ and r ∈V (G). Can
one determine an r-cut with minimum capacity in polynomial time? Can one
determine a directed cut with minimum capacity in polynomial time (or decide
that G is strongly connected)?
Note: The answer to the ﬁrst question solves the Separation Problem for the
Minimum Weight Rooted Arborescence Problem; see Corollary 6.14.
14. Show how to ﬁnd a blocking ﬂow in an acyclic network in O(nm) time.
(Dinic [1970])
15. Let (G, u, s, t) be a network such that G −t is an arborescence. Show how
to ﬁnd a maximum s-t-ﬂow in linear time.
Hint: Use DFS.
16.
∗
Let (G, u, s, t) be a network such that the underlying undirected graph of
G −{s, t} is a forest. Show how to ﬁnd a maximum s-t-ﬂow in linear time.
(Vygen [2002])
17. Consider a modiﬁed version of Fujishige’s Algorithm where in
5⃝we
choose vi ∈V (G) \ {v1, . . . , vi−1} such that b(vi) is maximum, and
4⃝is
replaced by stopping if b(v) = 0 for all v ∈V (G) \ {v1, . . . , vi}. Then X

184
8. Network Flows
and α are not needed anymore. Show that the number of iterations is still
O(n log umax). Show how to implement one iteration in O(m + n log n) time.
18. Let us call a preﬂow f maximum if exf (t) is maximum.
(a) Show that for any maximum preﬂow f there exists a maximum ﬂow f ′
with f ′(e) ≤f (e) for all e ∈E(G).
(b) Show how a maximum preﬂow can be converted into a maximum ﬂow in
O(nm) time. (Hint: Use a variant of the Edmonds-Karp Algorithm.)
19. Prove that the Push-Relabel Algorithm performs O(n2m) nonsaturating
pushes, independent of the choice of v in 3⃝.
20. Given an acyclic digraph G with weights c : E(G) →R+, ﬁnd a maximum
weight directed cut in G. Show how this problem can be reduced to a minimum
s-t-cut problem and be solved in O(n3) time.
Hint: Use Exercise 6.
21. Let G be an acyclic digraph with weights c : E(G) →R+. We look for the
maximum weight edge set F ⊆E(G) such that no path in G contains more
than one edge of F. Show that this problem is equivalent to looking for the
maximum weight directed cut in G (and thus can be solved in O(n3) time by
Exercise 20).
22. Given an undirected graph G with capacities u : E(G) →R+ and a set
T ⊆V (G) with |T | ≥2. We look for a set X ⊂V (G) with T ∩X ̸= ∅
and T \ X ̸= ∅such that 
e∈δ(X) u(e) is minimum. Show how to solve this
problem in O(n4) time, where n = |V (G)|.
23. Let λi j, 1 ≤i, j ≤n, be nonnegative numbers with λi j = λji and λik ≥
min(λi j, λjk) for any three distinct indices i, j, k ∈{1, . . . , n}. Show that there
exists a graph G with V (G) = {1, . . . , n} and capacities u : E(G) →R+
such that the local edge-connectivities are precisely the λi j.
Hint: Consider a maximum weight spanning tree in (Kn, c), where c({i, j}) :=
λi j.
(Gomory and Hu [1961])
24. Let G be an undirected graph with capacities u : E(G) →R+, and let T ⊆
V (G) with |T | even. A T -cut in G is a cut δ(X) with |X ∩T | odd. Construct a
polynomial time algorithm for ﬁnding a T -cut of minimum capacity in (G, u).
Hint: Use a Gomory-Hu tree.
(A solution of this exercise can be found in Section 12.3.)
25. Let G be a simple undirected graph with at least two vertices. Suppose the
degree of each vertex of G is at least k. Prove that there are two vertices s
and t such that at least k edge-disjoint s-t-paths exist. What if there is exactly
one vertex with degree less than k?
Hint: Consider a Gomory-Hu tree for G.
26. Consider the problem of determining the edge-connectivity λ(G) of an undi-
rected graph (with unit capacities). Section 8.7 shows how to solve this prob-
lem in O(mn) time, provided that we can ﬁnd an MA order of an undirected
graph with unit capacities in O(m + n) time. How can this be done?

Exercises
185
27.
∗
Let G be an undirected graph with an MA order v1, . . . , vn. Let κuv de-
note the maximum number of vertex-disjoint u-v-paths. Prove κvn−1vn =
|E({vn}, {v1, . . . , vn−1})| (the vertex-disjoint counterpart of Lemma 8.38).
Hint: Prove by induction that κ
Gi j
vjvi = |E({vj}, {v1, . . . , vi})|, where Gi j =
G[{v1, . . . , vi}∪{vj}]. To do this, assume w.l.o.g. that {vj, vi} /∈E(G), choose
a minimal set Z ⊆{v1, . . . , vi−1} separating vj and vi (Menger’s Theorem
8.10), and let h ≤i be the maximum number such that vh /∈Z and vh is
adjacent to vi or vj.
(Frank [unpublished])
28.
∗
An undirected graph is called chordal if it has no circuit of length at least
four as an induced subgraph. An order v1, . . . , vn of an undirected graph G
is called simplicial if {vi, vj}, {vi, vk} ∈E(G) implies {vj, vk} ∈E(G) for
i < j < k.
(a) Prove that a graph with a simplicial order must be chordal.
(b) Let G be a chordal graph, and let v1, . . . , vn be an MA order. Prove that
vn, vn−1, . . . , v1 is a simplicial order.
Hint: Use Exercise 27 and Menger’s Theorem 8.10.
Note: The fact that a graph is chordal if and only if it has a simplicial order
is due to Rose [1970].
29. Let G an undirected graph with capacities u : E(G) →R+. Let ∅̸= A ⊂
V (G) such that δ(A) is a minimum capacity cut in G.
(a) Show that u(δ(A)) ≤2
n u(E(G)). (Hint: Consider the trivial cuts δ(x),
x ∈V (G).)
(b) Consider the following procedure: We randomly choose an edge which
we contract, each edge e is chosen with probability
u(e)
u(E(G)). We repeat
this operation until there are only two vertices. Prove that the probability
that we never contract an edge of δ(A) is at least
2
(n−1)n .
(c) Conclude that running the randomized algorithm in (b) kn2 times yields
δ(A) with probability at least 1−e−2k. (Such an algorithm with a positive
probability of a correct answer is called a Monte Carlo algorithm.)
(Karger and Stein [1996]; see also Karger [2000])
30. Show how the vertex-connectivity of an undirected graph can be determined
in O(n5) time.
Hint: Recall the proof of Menger’s Theorem.
Note: There exists an O(n4)-algorithm; see (Henzinger, Rao and Gabow
[2000]).
31. Let G be a connected undirected graph with capacities u : E(G) →R+. We
are looking for a minimum capacity 3-cut, i.e. an edge set whose deletion
splits G into at least three connected components.
Let δ(X1), δ(X2), . . . be a list of the cuts ordered by nondecreasing capacities:
u(δ(X1)) ≤u(δ(X2)) ≤· · ·. Assume that we know the ﬁrst 2n elements of this
list (note: they can be computed in polynomial time by a method of Vazirani
and Yannakakis [1992]).

186
8. Network Flows
(a) Show that for some indices i, j ∈{1, . . . , 2n} all sets Xi \ X j, X j \ Xi,
Xi ∩X j and V (G) \ (Xi ∪X j) are nonempty.
(b) Show that there is a 3-cut of capacity at most 3
2u(δ(X2n).
(c) For each i = 1, . . . , 2n consider δ(Xi) plus a minimum capacity cut of
G−Xi, and also δ(Xi) plus a minimum capacity cut of G[Xi]. This yields
a list of at most 4n 3-cuts. Prove that one of them is optimum.
(Nagamochi and Ibaraki [2000])
Note: The problem of ﬁnding the optimum 3-cut separating three given vertices
is much harder; see Dahlhaus et al. [1994] and Cunningham and Tang [1999].
References
General Literature:
Ahuja, R.K., Magnanti, T.L., and Orlin, J.B. [1993]: Network Flows. Prentice-Hall, Engle-
wood Cliffs 1993
Cook, W.J., Cunningham, W.H., Pulleyblank, W.R., and Schrijver, A. [1998]: Combinato-
rial Optimization. Wiley, New York 1998, Chapter 3
Cormen, T.H., Leiserson, C.E., and Rivest, R.L. [1990]: Introduction to Algorithms. MIT
Press, Cambridge 1990, Chapter 27
Ford, L.R., and Fulkerson, D.R. [1962]: Flows in Networks. Princeton University Press,
Princeton 1962
Frank, A. [1995]: Connectivity and network ﬂows. In: Handbook of Combinatorics; Vol. 1
(R.L. Graham, M. Gr¨otschel, L. Lov´asz, eds.), Elsevier, Amsterdam, 1995
Goldberg, A.V., Tardos, ´E., and Tarjan, R.E. [1990]: Network ﬂow algorithms. In: Paths,
Flows, and VLSI-Layout (B. Korte, L. Lov´asz, H.J. Pr¨omel, A. Schrijver, eds.), Springer,
Berlin 1990, pp. 101–164
Gondran, M., and Minoux, M. [1984]: Graphs and Algorithms. Wiley, Chichester 1984,
Chapter 5
Jungnickel, D. [1999]: Graphs, Networks and Algorithms. Springer, Berlin 1999
Phillips, D.T., and Garcia-Diaz, A. [1981]: Fundamentals of Network Analysis. Prentice-
Hall, Englewood Cliffs 1981
Ruhe, G. [1991]: Algorithmic Aspects of Flows in Networks. Kluwer Academic Publishers,
Dordrecht 1991
Schrijver, A. [2003]: Combinatorial Optimization: Polyhedra and Efﬁciency. Springer,
Berlin 2003, Chapters 9,10,13–15
Tarjan, R.E. [1983]: Data Structures and Network Algorithms. SIAM, Philadelphia 1983,
Chapter 8
Thulasiraman, K., and Swamy, M.N.S. [1992]: Graphs: Theory and Algorithms. Wiley,
New York 1992, Chapter 12
Cited References:
Ahuja, R.K., Orlin, J.B., and Tarjan, R.E. [1989]: Improved time bounds for the maximum
ﬂow problem. SIAM Journal on Computing 18 (1989), 939–954
Cheriyan, J., and Maheshwari, S.N. [1989]: Analysis of preﬂow push algorithms for max-
imum network ﬂow. SIAM Journal on Computing 18 (1989), 1057–1086
Cheriyan, J., and Mehlhorn, K. [1999]: An analysis of the highest-level selection rule in the
preﬂow-push max-ﬂow algorithm. Information Processing Letters 69 (1999), 239–242

References
187
Cherkassky, B.V. [1977]: Algorithm of construction of maximal ﬂow in networks with
complexity of O(V 2√
E) operations. Mathematical Methods of Solution of Economical
Problems 7 (1977), 112–125 [in Russian]
Cunningham, W.H., and Tang, L. [1999]: Optimal 3-terminal cuts and linear programming.
Proceedings of the 7th Conference on Integer Programming and Combinatorial Opti-
mization; LNCS 1610 (G. Cornu´ejols, R.E. Burkard, G.J. Woeginger, eds.), Springer,
Berlin 1999, pp. 114–125
Dahlhaus, E., Johnson, D.S., Papadimitriou, C.H., Seymour, P.D., and Yannakakis, M.
[1994]: The complexity of multiterminal cuts. SIAM Journal on Computing 23 (1994),
864–894
Dantzig, G.B., and Fulkerson, D.R. [1956]: On the max-ﬂow min-cut theorem of networks.
In: Linear Inequalities and Related Systems (H.W. Kuhn, A.W. Tucker, eds.), Princeton
University Press, Princeton 1956, pp. 215–221
Dinic, E.A. [1970]: Algorithm for solution of a problem of maximum ﬂow in a network
with power estimation. Soviet Mathematics Doklady 11 (1970), 1277–1280
Edmonds, J., and Karp, R.M. [1972]: Theoretical improvements in algorithmic efﬁciency
for network ﬂow problems. Journal of the ACM 19 (1972), 248–264
Elias, P., Feinstein, A., and Shannon, C.E. [1956]: Note on maximum ﬂow through a
network. IRE Transactions on Information Theory, IT-2 (1956), 117–119
Ford, L.R., and Fulkerson, D.R. [1956]: Maximal Flow Through a Network. Canadian
Journal of Mathematics 8 (1956), 399–404
Ford, L.R., and Fulkerson, D.R. [1957]: A simple algorithm for ﬁnding maximal network
ﬂows and an application to the Hitchcock problem. Canadian Journal of Mathematics 9
(1957), 210–218
Frank, A. [1994]: On the edge-connectivity algorithm of Nagamochi and Ibaraki. Labora-
toire Artemis, IMAG, Universit´e J. Fourier, Grenoble, 1994
Fujishige, S. [2003]: A maximum ﬂow algorithm using MA ordering. Operations Research
Letters 31 (2003), 176–178
Gabow, H.N. [1995]: A matroid approach to ﬁnding edge connectivity and packing arbores-
cences. Journal of Computer and System Sciences 50 (1995), 259–273
Galil, Z. [1980]: An O(V
5
3 E
2
3 ) algorithm for the maximal ﬂow problem. Acta Informatica
14 (1980), 221–242
Galil, Z., and Namaad, A. [1980]: An O(EV log2 V ) algorithm for the maximal ﬂow
problem. Journal of Computer and System Sciences 21 (1980), 203–217
Gallai, T. [1958]: Maximum-minimum S¨atze ¨uber Graphen. Acta Mathematica Academiae
Scientiarum Hungaricae 9 (1958), 395–434
Goldberg, A.V., and Rao, S. [1998]: Beyond the ﬂow decomposition barrier. Journal of the
ACM 45 (1998), 783–797
Goldberg, A.V., and Tarjan, R.E. [1988]: A new approach to the maximum ﬂow problem.
Journal of the ACM 35 (1988), 921–940
Gomory, R.E., and Hu, T.C. [1961]: Multi-terminal network ﬂows. Journal of SIAM 9
(1961), 551–570
Gusﬁeld, D. [1990]: Very simple methods for all pairs network ﬂow analysis. SIAM Journal
on Computing 19 (1990), 143–155
Hao, J., and Orlin, J.B. [1994]: A faster algorithm for ﬁnding the minimum cut in a directed
graph. Journal of Algorithms 17 (1994), 409–423
Henzinger, M.R., Rao, S., and Gabow, H.N. [2000]: Computing vertex connectivity: new
bounds from old techniques. Journal of Algorithms 34 (2000), 222–250
Hoffman, A.J. [1960]: Some recent applications of the theory of linear inequalities to
extremal combinatorial analysis. In: Combinatorial Analysis (R.E. Bellman, M. Hall,
eds.), AMS, Providence 1960, pp. 113–128
Hu, T.C. [1969]: Integer Programming and Network Flows. Addison-Wesley, Reading 1969

188
8. Network Flows
Karger, D.R. [2000]: Minimum cuts in near-linear time. Journal of the ACM 47 (2000),
46–76
Karger, D.R., and Levine, M.S. [1998]: Finding maximum ﬂows in undirected graphs seems
easier than bipartite matching. Proceedings of the 30th Annual ACM Symposium on the
Theory of Computing (1998), 69–78
Karger, D.R., and Stein, C. [1996]: A new approach to the minimum cut problem. Journal
of the ACM 43 (1996), 601–640
Karzanov, A.V. [1974]: Determining the maximal ﬂow in a network by the method of
preﬂows. Soviet Mathematics Doklady 15 (1974), 434–437
King, V., Rao, S., and Tarjan, R.E. [1994]: A faster deterministic maximum ﬂow algorithm.
Journal of Algorithms 17 (1994), 447–474
Mader, W. [1972]: ¨Uber minimal n-fach zusammenh¨angende, unendliche Graphen und ein
Extremalproblem. Arch. Math. 23 (1972), 553–560
Mader, W. [1981]: On a property of n edge-connected digraphs. Combinatorica 1 (1981),
385–386
Malhotra, V.M., Kumar, M.P., and Maheshwari, S.N. [1978]: An O(|V |3) algorithm for
ﬁnding maximum ﬂows in networks. Information Processing Letters 7 (1978), 277–278
Menger, K. [1927]: Zur allgemeinen Kurventheorie. Fundamenta Mathematicae 10 (1927),
96–115
Nagamochi, H., and Ibaraki, T. [1992]: Computing edge-connectivity in multigraphs and
capacitated graphs. SIAM Journal on Discrete Mathematics 5 (1992), 54–66
Nagamochi, H., and Ibaraki, T. [2000]: A fast algorithm for computing minimum 3-way
and 4-way cuts. Mathematical Programming 88 (2000), 507–520
Phillips, S., and Dessouky, M.I. [1977]: Solving the project time/cost tradeoff problem
using the minimal cut concept. Management Science 24 (1977), 393–400
Picard, J., and Queyranne, M. [1980]: On the structure of all minimum cuts in a network
and applications. Mathematical Programming Study 13 (1980), 8–16
Queyranne, M. [1998]: Minimizing symmetric submodular functions. Mathematical Pro-
gramming B 82 (1998), 3–12
Rose, D.J. [1970]: Triangulated graphs and the elimination process. Journal of Mathematical
Analysis and Applications 32 (1970), 597–609
Shiloach, Y. [1978]: An O(nI log2 I) maximum-ﬂow algorithm. Technical Report STAN-
CS-78-802, Computer Science Department, Stanford University, 1978
Shiloach, Y. [1979]: Edge-disjoint branching in directed multigraphs. Information Process-
ing Letters 8 (1979), 24–27
Shioura, A. [2004]: The MA ordering max-ﬂow algorithm is not strongly polynomial for
directed networks. Operations Research Letters 32 (2004), 31–35
Sleator, D.D. [1980]: An O(nm log n) algorithm for maximum network ﬂow. Technical
Report STAN-CS-80-831, Computer Science Department, Stanford University, 1978
Sleator, D.D., and Tarjan, R.E. [1983]: A data structure for dynamic trees. Journal of
Computer and System Sciences 26 (1983), 362–391
Su, X.Y. [1997]: Some generalizations of Menger’s theorem concerning arc-connected
digraphs. Discrete Mathematics 175 (1997), 293–296
Stoer, M., and Wagner, F. [1997]: A simple min cut algorithm. Journal of the ACM 44
(1997), 585–591
Tunc¸el, L. [1994]: On the complexity preﬂow-push algorithms for maximum ﬂow problems.
Algorithmica 11 (1994), 353–359
Vazirani, V.V., and Yannakakis, M. [1992]: Suboptimal cuts: their enumeration, weight,
and number. In: Automata, Languages and Programming; Proceedings; LNCS 623 (W.
Kuich, ed.), Springer, Berlin 1992, pp. 366–377
Vygen, J. [2002]: On dual minimum cost ﬂow algorithms. Mathematical Methods of Oper-
ations Research 56 (2002), 101–126

References
189
Weihe, K. [1997]: Maximum (s, t)-ﬂows in planar networks in O(|V | log |V |) time. Journal
of Computer and System Sciences 55 (1997), 454–475
Whitney, H. [1932]: Congruent graphs and the connectivity of graphs. American Journal
of Mathematics 54 (1932), 150–168

9. Minimum Cost Flows
In this chapter we show how we can take edge costs into account. For example, in
our application of the MaximumFlowProblem to the JobAssignmentProblem
mentioned in the introduction of Chapter 8 one could introduce edge costs to model
that the employees have different salaries; our goal is to meet a deadline when
all jobs must be ﬁnished at a minimum cost. Of course, there are many more
applications.
A second generalization, allowing several sources and sinks, is more due to
technical reasons. We introduce the general problem and an important special case
in Section 9.1. In Section 9.2 we prove optimality criteria that are the basis of
the minimum cost ﬂow algorithms presented in Sections 9.3, 9.4 and 9.5. These
use algorithms of Chapter 7 for ﬁnding a minimum mean cycle or a shortest
path as a subroutine. Section 9.6 concludes this chapter with an application to
time-dependent ﬂows.
9.1 Problem Formulation
We are again given a digraph G with capacities u : E(G) →R+, but in addition
numbers c : E(G) →R indicating the cost of each edge. Furthermore, we allow
several sources and sinks:
Deﬁnition 9.1.
Given a digraph G, capacities u : E(G) →R+, and numbers b :
V (G) →R with 
v∈V (G) b(v) = 0, a b-ﬂow in (G, u) is a function f : E(G) →
R+ with f (e) ≤u(e) for all e ∈E(G) and 
e∈δ+(v) f (e) −
e∈δ−(v) f (e) = b(v)
for all v ∈V (G).
Thus a b-ﬂow with b ≡0 is a circulation. b(v) is called the balance of vertex
v. |b(v)| is sometimes called the supply (if b(v) > 0) or the demand (if b(v) < 0)
of v. Vertices v with b(v) > 0 are called sources, those with b(v) < 0 sinks.
Note that a b-ﬂow can be found by any algorithm for the Maximum Flow
Problem: Just add two vertices s and t and edges (s, v), (v, t) with capacities
u((s, v)) := max{0, b(v)} and u((v, t)) := max{0, −b(v)} for all v ∈V (G) to G.
Then any s-t-ﬂow of value 
v∈V (G) u((s, v)) in the resulting network corresponds
to a b-ﬂow in G. Thus a criterion for the existence of a b-ﬂow can be derived
from the Max-Flow-Min-Cut Theorem 8.6 (see Exercise 2). The problem is to ﬁnd
a minimum cost b-ﬂow:

192
9. Minimum Cost Flows
Minimum Cost Flow Problem
Instance:
A digraph G, capacities u : E(G) →R+, numbers b : V (G) →R
with 
v∈V (G) b(v) = 0, and weights c : E(G) →R.
Task:
Find a b-ﬂow f whose cost c( f ) := 
e∈E(G) f (e)c(e) is minimum
(or decide that none exists).
Sometimes one also allows inﬁnite capacities. In this case an instance can be
unbounded, but this can be checked in advance easily; see Exercise 5.
The Minimum Cost Flow Problem is quite general and has a couple of
interesting special cases. The uncapacitated case (u ≡∞) is sometimes called
the transshipment problem. An even more restricted problem, also known as the
transportation problem, has been formulated quite early by Hitchcock [1941] and
others:
Hitchcock Problem
Instance:
A digraph G with V (G) = A
.
∪B and E(G) ⊆A × B. Supplies
b(v) ≥0 for v ∈A and demands −b(v) ≥0 for v ∈B with

v∈V (G) b(v) = 0. Weights c : E(G) →R.
Task:
Find a b-ﬂow f in (G, ∞) of minimum cost (or decide that none
exists).
In the Hitchcock Problem it causes no loss of generality to assume that c is
nonnegative: Adding a constant α to each weight increases the cost of each b-ﬂow
by the same amount, namely by α 
v∈A b(v). Often only the special case where
c is nonnegative and E(G) = A × B is considered.
Obviously, any instance of the Hitchcock Problem can be written as an in-
stance of the Minimum Cost Flow Problem on a bipartite graph with inﬁnite
capacities. It is less obvious that any instance of the Minimum Cost Flow Prob-
lem can be transformed to an equivalent (but larger) instance of the Hitchcock
Problem:
Lemma 9.2.
(Orden [1956], Wagner [1959]) An instance of the Minimum Cost
Flow Problem with n vertices and m edges can be transformed to an equivalent
instance of the Hitchcock Problem with n + m vertices and 2m edges.
Proof:
Let (G, u, b, c) be an instance of the Minimum Cost Flow Problem.
We deﬁne an equivalent instance (G′, A′, B′, b′, c′) of the Hitchcock Problem
as follows:
Let A′ := E(G), B′ := V (G) and G′ := (A′ ∪B′, E1 ∪E2), where
E1 := {((x, y), x) : (x, y) ∈E(G)} and E2 := {((x, y), y) : (x, y) ∈E(G)}.
Let c′((e, x)) := 0 for (e, x) ∈E1 and c′((e, y)) := c(e) for (e, y) ∈E2. Finally
let b′(e) := u(e) for e ∈E(G) and
b′(x) := b(x) −

e∈δ+
G(x)
u(e)
for x ∈V (G).

9.2 An Optimality Criterion
193
b′(e1) = 5
b′(e2) = 4
b′(e3) = 7
b′(x) = −1
b′(y) = −5
b′(z) = −10
0
c(e1)
0
c(e2)
c(e3)
0
b(x) = 4
b(y) = −1
b(z) = −3
e1
e2
e3
u(e1) = 5, u(e2) = 4, u(e3) = 7
u′ ≡∞
Fig. 9.1.
For an example, see Figure 9.1.
We prove that both instances are equivalent. Let f be a b-ﬂow in (G, u).
Deﬁne f ′((e, y)) := f (e) and f ′((e, x)) := u(e) −f (e) for e = (x, y) ∈E(G).
Obviously f ′ is a b′-ﬂow in G′ with c′( f ′) = c( f ).
Conversely, if f ′ is a b′-ﬂow in G′, then f ((x, y)) := f ′(((x, y), y)) deﬁnes
a b-ﬂow in G with c( f ) = c′( f ′).
2
The above proof is due to Ford and Fulkerson [1962].
9.2 An Optimality Criterion
In this section we prove some simple results, in particular an optimality criterion,
which will be the basis for the algorithms in the subsequent sections. We again
use the concepts of residual graphs and augmenting paths. We extend the weights
c to
↔
G by deﬁning c(
←e ) := −c(e) for each edge e ∈E(G). Our deﬁnition of a
residual graph has the advantage that the weight of an edge in a residual graph
G f is independent of the ﬂow f .
Deﬁnition 9.3.
Given a digraph G with capacities and a b-ﬂow f , an f-augment-
ing cycle is a circuit in G f .
The following simple observation will prove useful:
Proposition 9.4.
Let G be a digraph with capacities u : E(G) →R+. Let f and
f ′ be b-ﬂows in (G, u). Then g : E(
↔
G) →R+ deﬁned by g(e) := max{0, f ′(e) −
f (e)} and g(
←e ) := max{0, f (e) −f ′(e)} for e ∈E(G) is a circulation in
↔
G.
Furthermore, g(e) = 0 for all e /∈E(G f ) and c(g) = c( f ′) −c( f ).
Proof:
At each vertex v ∈V (
↔
G) we have

194
9. Minimum Cost Flows

e∈δ+
↔
G
(v)
g(e) −

e∈δ−
↔
G
(v)
g(e)
=

e∈δ+
G(v)
( f ′(e) −f (e)) −

e∈δ−
G(v)
( f ′(e) −f (e))
=
b(v) −b(v) = 0,
so g is a circulation in
↔
G.
For e ∈E(
↔
G) \ E(G f ) we consider two cases: If e ∈E(G) then f (e) = u(e)
and thus f ′(e) ≤f (e), implying g(e) = 0. If e =
←
e0 for some e0 ∈E(G) then
f (e0) = 0 and thus g(
←
e0) = 0.
The last statement is easily veriﬁed:
c(g) =

e∈E(
↔
G)
c(e)g(e) =

e∈E(G)
c(e) f ′(e) −

e∈E(G)
c(e) f (e) = c( f ′) −c( f ).
2
Just as Eulerian graphs can be partitioned into circuits, circulations can be
decomposed into ﬂows on single circuits:
Proposition 9.5.
(Ford and Fulkerson [1962]) For any circulation f in a digraph
G there is a family C of at most |E(G)| circuits in G and positive numbers h(C)
(C ∈C) such that f (e) = {h(C) : C ∈C, e ∈E(C)} for all e ∈E(G).
Proof:
This is a special case of Theorem 8.8.
2
Now we can prove an optimality criterion:
Theorem 9.6.
(Klein [1967])
Let (G, u, b, c) be an instance of the Minimum
Cost Flow Problem. A b-ﬂow f is of minimum cost if and only if there is no
f -augmenting cycle with negative total weight.
Proof:
If there is an f -augmenting cycle C with weight γ < 0, we can augment
f along C by some ε > 0 and get a b-ﬂow f ′ with cost decreased by −γ ε. So f
is not a minimum cost ﬂow.
If f is not a minimum cost b-ﬂow, there is another b-ﬂow f ′ with smaller cost.
Consider g as deﬁned in Proposition 9.4. Then g is a circulation with c(g) < 0. By
Proposition 9.5, g can be decomposed into ﬂows on single circuits. Since g(e) = 0
for all e /∈E(G f ), all these circuits are f -augmenting. At least one of them must
have negative total weight, proving the theorem.
2
This result gows back essentially to Tolsto˘ı [1930] and has been rediscovered
several times in different forms. One equivalent formulation is the following:
Corollary 9.7.
(Ford and Fulkerson [1962])
Let (G, u, b, c) be an instance of
the Minimum Cost Flow Problem. A b-ﬂow f is of minimum cost if and only if
there exists a feasible potential for (G f , c).

9.3 Minimum Mean Cycle-Cancelling Algorithm
195
Proof:
By Theorem 9.6 f is a minimum cost b-ﬂow if and only if G f contains
no negative circuit. By Theorem 7.7 there is no negative circuit in (G f , c) if and
only if there exists a feasible potential.
2
Feasible potentials can also be regarded as solutions of the linear programming
dual of the Minimum Cost Flow Problem. This is shown by the following
different proof of the above optimality criterion:
Second Proof of Corollary 9.7:
We write the Minimum Cost Flow Problem
as a maximization problem and consider the LP
max

e∈E(G)
−c(e)xe
s.t.

e∈δ+(v)
xe −

e∈δ−(v)
xe
=
b(v)
(v ∈V (G))
xe
≤
u(e)
(e ∈E(G))
xe
≥
0
(e ∈E(G))
(9.1)
and its dual
min

v∈V (G)
b(v)yv +

e∈E(G)
u(e)ze
s.t.
yv −yw + ze
≥
−c(e)
(e = (v, w) ∈E(G))
ze
≥
0
(e ∈E(G))
(9.2)
Let x be any b-ﬂow, i.e. any feasible solution of (9.1). By Corollary 3.18 x
is optimum if and only if there exists a feasible dual solution (y, z) of (9.2) such
that x and (y, z) satisfy the complementary slackness conditions
ze(u(e)−xe) = 0 and xe(c(e)+ze + yv −yw) = 0 for all e = (v, w) ∈E(G).
So x is optimum if and only if there exists a pair of vectors (y, z) with
0 = −ze ≤c(e) + yv −yw
for e = (v, w) ∈E(G) with xe < u(e)
and
c(e) + yv −yw = −ze ≤0
for e = (v, w) ∈E(G) with xe > 0.
This is equivalent to the existence of a vector y such that c(e) + yv −yw ≥0 for
all residual edges e = (v, w) ∈E(Gx), i.e. to the existence of a feasible potential
y for (Gx, c).
2
9.3 Minimum Mean Cycle-Cancelling Algorithm
Note that Klein’s Theorem 9.6 already suggests an algorithm: ﬁrst ﬁnd any b-ﬂow
(using a max-ﬂow algorithm as described above), and then successively augment
along negative weight augmenting cycles until no more exist. We must however
be careful in choosing the cycle if we want to have polynomial running time (see
Exercise 7). A good strategy is to choose an augmenting cycle with minimum
mean weight each time:

196
9. Minimum Cost Flows
Minimum Mean Cycle-Cancelling Algorithm
Input:
A digraph G, capacities u : E(G) →R+, numbers b : V (G) →R
with 
v∈V (G) b(v) = 0, and weights c : E(G) →R.
Output:
A minimum cost b-ﬂow f .
1⃝
Find a b-ﬂow f .
2⃝
Find a circuit C in G f whose mean weight is minimum.
If C has nonnegative total weight (or G f is acyclic) then stop.
3⃝
Compute γ := min
e∈E(C) u f (e). Augment f along C by γ .
Go to 2⃝.
As described in Section 9.1, 1⃝can be implemented with any algorithm for the
Maximum Flow Problem. 2⃝can be implemented with the algorithm presented in
Section 7.3. We shall now prove that this algorithm terminates after a polynomial
number of iterations. The proof will be similar to the one in Section 8.3. Let µ( f )
denote the minimum mean weight of a circuit in G f . Then Theorem 9.6 says that
a b-ﬂow f is optimum if and only if µ( f ) ≥0.
We ﬁrst show that µ( f ) is non-decreasing throughout the algorithm. Moreover,
we can show that it is strictly increasing with every |E(G)| iterations. As usual
we denote by n and m the number of vertices and edges of G, respectively.
Lemma 9.8.
Let f1, f2, . . . be a sequence of b-ﬂows such that fi+1 results from
fi by augmenting along Ci, where Ci is a circuit of minimum mean weight in G fi.
Then
(a) µ( fk) ≤µ( fk+1) for all k.
(b) µ( fk) ≤
n
n−2 µ( fl) for all k < l such that Ck ∪Cl contains a pair of reverse
edges.
Proof:
(a): Let fk, fk+1 be two subsequent ﬂows in this sequence. Consider the
Eulerian graph H resulting from (V (G), E(Ck)
.
∪E(Ck+1)) by deleting pairs of
reverse edges. (Edges appearing both in Ck and Ck+1 are counted twice.) H is a
subgraph of G fk because each edge in E(G fk+1)\ E(G fk) must be the reverse of an
edge in E(Ck). Since H is Eulerian, it can be decomposed into circuits, and each
of these circuits has mean weight at least µ( fk). So c(E(H)) ≥µ( fk)|E(H)|.
Since the total weight of each pair of reverse edges is zero,
c(E(H)) = c(E(Ck)) + c(E(Ck+1)) = µ( fk)|E(Ck)| + µ( fk+1)|E(Ck+1)|.
Since |E(H)| ≤|E(Ck)| + |E(Ck+1)|, we conclude
µ( fk)(|E(Ck)| + |E(Ck+1)|)
≤
µ( fk)|E(H)|
≤
c(E(H))
=
µ( fk)|E(Ck)| + µ( fk+1)|E(Ck+1)|,
implying µ( fk+1) ≥µ( fk).

9.3 Minimum Mean Cycle-Cancelling Algorithm
197
(b):
By (a) it is enough to prove the statement for those k,l such that for
k < i < l, Ci ∪Cl contains no pair of reverse edges.
As in the proof of (a), consider the Eulerian graph H resulting from (V (G),
E(Ck)
.
∪E(Cl)) by deleting pairs of reverse edges. H is a subgraph of G fk
because any edge in E(Cl) \ E(G fk) must be the reverse of an edge in one of
Ck, Ck+1, . . . , Cl−1. But – due to the choice of k and l – only Ck among these
contains the reverse of an edge of Cl.
So as in (a) we have c(E(H)) ≥µ( fk)|E(H)| and
c(E(H)) = µ( fk)|E(Ck)| + µ( fl)|E(Cl)|.
Since |E(H)| ≤|E(Ck)| + n−2
n |E(Cl)| (we deleted at least two edges) we get
µ( fk)

|E(Ck)| + n −2
n
|E(Cl)|

≤
µ( fk)|E(H)|
≤
c(E(H))
=
µ( fk)|E(Ck)| + µ( fl)|E(Cl)|,
implying µ( fk) ≤
n
n−2 µ( fl).
2
Corollary 9.9.
During the execution of the Minimum Mean Cycle-Cancelling
Algorithm, |µ( f )| decreases by at least a factor of 1
2 with every mn iterations.
Proof:
Let Ck, Ck+1, . . . , Ck+m be the augmenting cycles in consecutive itera-
tions of the algorithm. Since each of these circuits contains one edge as a bottleneck
edge (an edge removed afterwards from the residual graph), there must be two of
these circuits, say Ci and Cj (k ≤i < j ≤k + m) whose union contains a pair of
reverse edges. By Lemma 9.8 we then have
µ( fk) ≤µ( fi) ≤
n
n −2 µ( fj) ≤
n
n −2 µ( fk+m).
So |µ( f )| decreases by at least a factor of n−2
n
with every m iterations. The
corollary follows from this because of
 n−2
n
n < e−2 < 1
2.
2
This already proves that the algorithm runs in polynomial time provided that
all edge costs are integral: |µ( f )| is at most |cmin| at the beginning, where cmin is
the minimum cost of any edge, and decreases by at least a factor of 1
2 with every
mn iterations. So after O(mn log(n|cmin|)) iterations, µ( f ) is greater than −1
n . If
the edge costs are integral, this implies µ( f ) ≥0 and the algorithm stops. So by
Corollary 7.14, the running time is O

m2n2 log(n|cmin|)

.
Even better, we can also derive a strongly polynomial running time for the
Minimum Cost Flow Problem (ﬁrst obtained by Tardos [1985]):
Theorem 9.10.
(Goldberg and Tarjan [1989])
The Minimum Mean Cycle-
Cancelling Algorithm runs in O

m3n2 log n

time.

198
9. Minimum Cost Flows
Proof:
We show that every mn(⌈log n⌉+ 1) iterations at least one edge is ﬁxed,
i.e. the ﬂow on this edge will not change anymore. Therefore there are at most
O

m2n log n

iterations. Using Corollary 8.15 for 1⃝and Corollary 7.14 for 2⃝
then proves the theorem.
Let f be the ﬂow at some iteration, and let f ′ be the ﬂow mn(⌈log n⌉+ 1)
iterations later. Deﬁne weights c′ by c′(e) := c(e) −µ( f ′) (e ∈E(G f ′)). Let
π be a feasible potential of (G f ′, c′) (which exists by Theorem 7.7). We have
0 ≤c′
π(e) = cπ(e) −µ( f ′), so
cπ(e) ≥µ( f ′)
for all e ∈E(G f ′).
(9.3)
Now let C be the circuit of minimum mean weight in G f that is chosen in the
algorithm to augment f . Since by Corollary 9.9
µ( f ) ≤2⌈log n⌉+1µ( f ′) ≤2nµ( f ′)
(see Figure 9.2), we have

e∈E(C)
cπ(e) =

e∈E(C)
c(e) = µ( f )|E(C)| ≤2nµ( f ′)|E(C)|.
So let e0 = (x, y) ∈E(C) with cπ(e0) ≤2nµ( f ′). By (9.3) we have e0 /∈E(G f ′).
µ( f )
2nµ( f ′)
µ( f ′)
0
Fig. 9.2.
Claim:
For any b-ﬂow f ′′ with e0 ∈E(G f ′′) we have µ( f ′′) < µ( f ′).
By Lemma 9.8(a) the claim implies that e0 will never be in the residual graph
anymore, i.e. e0 and
←
e0 are ﬁxed mn(⌈log n⌉+ 1) iterations after e0 is used in C.
This completes the proof.
To prove the claim, let f ′′ be a b-ﬂow with e0 ∈E(G f ′′). We apply Proposition
9.4 to f ′ and f ′′ and obtain a circulation g with g(e) = 0 for all e /∈E(G f ′) and
g(
←
e0) > 0 (because e0 ∈E(G f ′′) \ E(G f ′)).
By Proposition 9.5, g can be written as the sum of ﬂows on f ′-augmenting cy-
cles. One of these circuits, say W, must contain
←
e0. By using cπ(
←
e0) = −cπ(e0) ≥
−2nµ( f ′) and applying (9.3) to all e ∈E(W) \ {
←
e0} we obtain a lower bound for
the total weight of W:
c(E(W)) =

e∈E(W)
cπ(e) ≥−2nµ( f ′) + (n −1)µ( f ′) > −nµ( f ′).
But the reverse of W is an f ′′-augmenting cycle (this can be seen by exchanging
the roles of f ′ and f ′′), and its total weight is less than nµ( f ′). This means that
G f ′′ contains a circuit whose mean weight is less than µ( f ′), and so the claim is
proved.
2

9.4 Successive Shortest Path Algorithm
199
9.4 Successive Shortest Path Algorithm
The following theorem gives rise to another algorithm:
Theorem 9.11.
(Jewell [1958], Iri [1960], Busacker and Gowen [1961])
Let
(G, u, b, c) be an instance of the Minimum Cost Flow Problem, and let f be a
minimum cost b-ﬂow. Let P be a shortest (with respect to c) s-t-path P in G f (for
some s and t). Let f ′ be a ﬂow obtained when augmenting f along P by at most
the minimum residual capacity on P. Then f ′ is a minimum cost b′-ﬂow (for some
b′).
Proof:
f ′ is a b′-ﬂow for some b′. Suppose f ′ is not a minimum cost b′-ﬂow.
Then by Theorem 9.6 there is a circuit C in G f ′ with negative total weight.
Consider the graph H resulting from (V (G), E(C)
.
∪E(P)) by deleting pairs of
reverse edges. (Again, edges appearing both in C and P are taken twice.)
For any edge e ∈E(G f ′)\ E(G f ), the reverse of e must be in E(P). Therefore
E(H) ⊆E(G f ).
We have c(E(H)) = c(E(C)) + c(E(P)) < c(E(P)). Furthermore, H is the
union of an s-t-path and some circuits. But since E(H) ⊆E(G f ), none of the
circuits can have negative weight (otherwise f would not be a minimum cost
b-ﬂow).
Therefore H, and thus G f , contains an s-t-path of less weight than P, contra-
dicting the choice of P.
2
If the weights are conservative, we can start with f ≡0 as an optimum
circulation (b-ﬂow with b ≡0). Otherwise we can initially saturate all edges of
negative cost and bounded capacity. This changes the b-values but guarantees that
there is no negative augmenting cycle (i.e. c is conservative for G f ) unless the
instance is unbounded.
Successive Shortest Path Algorithm
Input:
A digraph G, capacities u : E(G) →R+, numbers b : V (G) →R
with 
v∈V (G) b(v) = 0, and conservative weights c : E(G) →R.
Output:
A minimum cost b-ﬂow f .
1⃝
Set b′ := b and f (e) := 0 for all e ∈E(G).
2⃝
If b′ = 0 then stop,
else choose a vertex s with b′(s) > 0.
Choose a vertex t with b′(t) < 0 such that t is reachable
from s in G f .
If there is no such t then stop. (There exists no b-ﬂow.)
3⃝
Find an s-t-path P in G f of minimum weight.

200
9. Minimum Cost Flows
4⃝
Compute γ := min

min
e∈E(P) u f (e), b′(s), −b′(t)

.
Set b′(s) := b′(s) −γ and b′(t) := b′(t) + γ . Augment f along P by γ .
Go to 2⃝.
If we allow arbitrary capacities, we have the same problems as with the Ford-
Fulkerson Algorithm (see Exercise 2 of Chapter 8; set all costs to zero). So
henceforth we assume u and b to be integral. Then it is clear that the algorithm
stops after at most B := 1
2

v∈V (G) |b(v)| augmentations. By Theorem 9.11, the
resulting ﬂow is optimum if the initial zero ﬂow is optimum. This is true if and
only if c is conservative.
We remark that if the algorithm decides that there is no b-ﬂow, this decision
is indeed correct. This is an easy observation, left as Exercise 11.
Each augmentation requires a shortest path computation. Since negative weights
occur, we have to use the Moore-Bellman-Ford Algorithm whose running
time is O(nm) (Theorem 7.5), so the overall running time will be O(Bnm).
However, as in the proof of Theorem 7.9, it can be arranged that (except at the
beginning) the shortest paths are computed in a graph with nonnegative weights:
Theorem 9.12.
(Tomizawa [1971], Edmonds and Karp [1972])
For integral
capacities and supplies, the Successive Shortest Path Algorithm can
be implemented with a running time of
O (nm + B(m + n log n)), where
B = 1
2

v∈V (G) |b(v)|.
Proof:
It is convenient to assume that there is only one source s. Otherwise we
introduce a new vertex s and edges (s, v) with capacity max{0, b(v)} and zero
cost for all v ∈V (G). Then we can set b(s) := B and b(v) := 0 for each former
source v. In this way we obtain an equivalent problem with only one source.
Moreover, we may assume that every vertex is reachable from s (other vertices
can be deleted).
We introduce potentials πi : V (G) →R for each iteration i of the Successive
Shortest Path Algorithm. We start with any feasible potential π0 of (G, c). By
Corollary 7.8, this exists and can be computed in O(mn) time.
Now let fi−1 be the ﬂow before iteration i. Then the shortest path computation
in iteration i is done with the reduced costs cπi−1 instead of c. Let li(v) denote the
length of a shortest s-v-path in G fi−1 with respect to the weights cπi−1. Then we
set πi(v) := πi−1(v) + li(v).
We prove by induction on i that πi is a feasible potential for (G fi, c). This
is clear for i = 0. For i > 0 and any edge e = (x, y) ∈E(G fi−1) we have (by
deﬁnition of li and the induction hypothesis)
li(y) ≤li(x) + cπi−1(e) = li(x) + c(e) + πi−1(x) −πi−1(y),
so
cπi(e) = c(e) + πi(x) −πi(y) = c(e) + πi−1(x) +li(x) −πi−1(y) −li(y) ≥0.

9.4 Successive Shortest Path Algorithm
201
For any edge e = (x, y) ∈Pi (where Pi is the augmenting path in iteration i) we
have
li(y) = li(x) + cπi−1(e) = li(x) + c(e) + πi−1(x) −πi−1(y),
so cπi(e) = 0, and the reverse edge of e also has zero weight. Since each edge
in E(G fi) \ E(G fi−1) is the reverse of an edge in Pi, cπi is indeed a nonnegative
weight function on E(G fi).
We observe that, for any i and any t, the shortest s-t-paths with respect to c
are precisely the shortest s-t-paths with respect to cπi, because cπi(P) −c(P) =
πi(s) −πi(t) for any s-t-path P.
Hence we can use Dijkstra’s Algorithm – which runs in O (m + n log n)
time when implemented with a Fibonacci heap by Theorem 7.4 – for all shortest
path computations except the initial one. Since we have at most B iterations, we
obtain an overall running time of O (nm + B(m + n log n)).
2
Note that (in contrast to many other problems, e.g. the Maximum Flow Prob-
lem) we cannot assume without loss of generality that the input graph is simple
when considering the Minimum Cost Flow Problem. The running time of The-
orem 9.12 is still exponential unless B is known to be small. If B = O(n), this is
the fastest algorithm known. For an application, see Section 11.1.
In the rest of this section we show how to modify the algorithm in order
to reduce the number of shortest path computations. We only consider the case
when all capacities are inﬁnite. By Lemma 9.2 each instance of the Minimum
Cost Flow Problem can be transformed to an equivalent instance with inﬁnite
capacities.
The basic idea – due to Edmonds and Karp [1972] – is the following. In
early iterations we consider only augmenting paths where γ – the amount of ﬂow
that can be pushed – is large. We start with γ = 2⌊log bmax⌋and reduce γ by a
factor of two if no more augmentations by γ can be done. After ⌊log bmax⌋+ 1
iterations we have γ = 1 and stop (we again assume b to be integral). Such a
scaling technique has proved useful for many algorithms (see also Exercise 12).
A detailed description of the ﬁrst scaling algorithm reads as follows:
Capacity Scaling Algorithm
Input:
A digraph G with inﬁnite capacities u(e) = ∞(e ∈E(G)), numbers
b : V (G) →Z with 
v∈V (G) b(v) = 0, and conservative weights
c : E(G) →R.
Output:
A minimum cost b-ﬂow f .
1⃝
Set b′ := b and f (e) := 0 for all e ∈E(G).
Set γ = 2⌊log bmax⌋, where bmax = max{b(v) : v ∈V (G)}.

202
9. Minimum Cost Flows
2⃝
If b′ = 0 then stop,
else choose a vertex s with b′(s) ≥γ .
Choose a vertex t with b′(t) ≤−γ such that t is reachable
from s in G f .
If there is no such s or t then go to 5⃝.
3⃝
Find an s-t-path P in G f of minimum weight.
4⃝
Set b′(s) := b′(s) −γ and b′(t) := b′(t) + γ . Augment f along P by γ .
Go to 2⃝.
5⃝
If γ = 1 then stop. (There exists no b-ﬂow.)
Else set γ := γ
2 and go to 2⃝.
Theorem 9.13.
(Edmonds and Karp [1972])
The Capacity Scaling Algo-
rithm correctly solves the Minimum Cost Flow Problem for integral b, inﬁnite
capacities and conservative weights. It can be implemented to run in O(n(m +
n log n) log bmax) time, where bmax = max{b(v) : v ∈V (G)}.
Proof:
As above, the correctness follows directly from Theorem 9.11. Note that
at any time, the residual capacity of any edge is either inﬁnite or a multiple of γ .
To establish the running time, we call the period in which γ remains constant
a phase. We prove that there are less than 4n augmentations within each phase.
Suppose this is not true. For some value of γ , let f and g be the ﬂow at the
beginning and at the end of the γ -phase, respectively. g −f can be regarded as a
b′′-ﬂow in G f , where 
x∈V (G) |b′′(x)| ≥8nγ . Let S := {x ∈V (G) : b′′(x) > 0},
S+ := {x ∈V (G) : b′′(x) ≥2γ }, T := {x ∈V (G) : b′′(x) < 0}, T + := {x ∈
V (G) : b′′(x) ≤−2γ }. If there had been a path from S+ to T + in G f , the 2γ -
phase would have continued. Therefore the total b′′-value of all sinks reachable
from S+ in G f is greater than n(−2γ ). Therefore (note that there exists a b′′-ﬂow
in G f ) 
x∈S+ b′′(x) < 2nγ . Now we have

x∈V (G)
|b′′(x)| = 2

x∈S
b′′(x)
=
2
⎛
⎝
x∈S+
b′′(x) +

x∈S\S+
b′′(x)
⎞
⎠
<
2(2nγ + 2nγ ) = 8nγ,
a contradiction.
This means that the total number of shortest path computations is O(n log bmax).
Combining this with the technique of Theorem 9.12 we obtain the O(mn +
n log bmax(m + n log n)) bound.
2
This was the ﬁrst polynomial-time algorithm for the Minimum Cost Flow
Problem. By some further modiﬁcations we can even obtain a strongly polynomial
running time. This is the subject of the next section.

9.5 Orlin’s Algorithm
203
9.5 Orlin’s Algorithm
The Capacity Scaling Algorithm of the previous section can be improved
further. A basic idea is that if an edge carries more than 8nγ units of ﬂow at
any stage of the Capacity Scaling Algorithm, it may be contracted. Namely,
observe that such an edge will always keep a positive ﬂow (and thus zero reduced
cost with respect to any feasible potential in the residual graph): there are at most
4n more augmentations by γ , another 4n by γ
2 and so on; hence the total amount
of ﬂow moved in the rest of the algorithm is less than 8nγ .
We shall describe Orlin’s Algorithm without explicitly using contraction.
This simpliﬁes the description, especially from the point of view of implementing
the algorithm. A set F keeps track of the edges (and their reverse edges) that
can be contracted. A representative is chosen out of each connected component
of (V (G), F). The algorithm maintains the property that the representative of
a connected component is its only non-balanced vertex. For any vertex x, r(x)
denotes the representative of the connected component of (V (G), F) containing x.
Orlin’s Algorithm does not require that b is integral. However, it can deal
with uncapacitated problems only (but recall Lemma 9.2).
Orlin’s Algorithm
Input:
A digraph G with inﬁnite capacities u(e) = ∞(e ∈E(G)), numbers
b : V (G) →R with 
v∈V (G) b(v) = 0, and conservative weights
c : E(G) →R.
Output:
A minimum cost b-ﬂow f .
1⃝
Set b′ := b and f (e) := 0 for all e ∈E(G).
Set r(v) := v for all v ∈V (G). Set F := ∅.
Set γ = max
v∈V (G) |b′(v)|.
2⃝
If b′ = 0 then stop.
3⃝
Choose a vertex s with b′(s) > n−1
n γ .
If there is no such s then go to 4⃝.
Choose a vertex t with b′(t) < −1
n γ such that t is reachable from s in G f .
If there is no such t then stop. (There exists no b-ﬂow.)
Go to 5⃝.
4⃝
Choose a vertex t with b′(t) < −n−1
n γ .
If there is no such t then go to 6⃝.
Choose a vertex s with b′(s) > 1
n γ such that t is reachable from s in G f .
If there is no such s then stop. (There exists no b-ﬂow.)
5⃝
Find an s-t-path P in G f of minimum weight.
Set b′(s) := b′(s) −γ and b′(t) := b′(t) + γ . Augment f along P by γ .
Go to 2⃝.

204
9. Minimum Cost Flows
6⃝
If f (e) = 0 for all e ∈E(G) \ F then set γ := min
γ
2 , max
v∈V (G) |b′(v)|

,
else set γ := γ
2 .
7⃝
For all e = (x, y) ∈E(G) \ F with r(x) ̸= r(y) and f (e) > 8nγ do:
Set F := F ∪{e,
←e }.
Let x′ := r(x) and y′ := r(y). Let Q be the x′-y′-path in F.
If b′(x′) > 0 then augment f along Q by b′(x′),
else augment f along the reverse of Q by −b′(x′).
Set b′(y′) := b′(y′) + b′(x′) and b′(x′) := 0.
Set r(z) := y′ for all vertices z reachable from y′ in F.
8⃝
Go to 2⃝.
This algorithm is due to Orlin [1993]. See also (Plotkin and Tardos [1990]).
Let us ﬁrst prove its correctness. Let us call the time between two changes of γ
a phase.
Lemma 9.14.
Orlin’s Algorithm solves the uncapacitated Minimum Cost
Flow Problem with conservative weights correctly. At any stage f is a minimum-
cost (b −b′)-ﬂow.
Proof:
We ﬁrst prove that f is always a (b −b′)-ﬂow. In particular, we have to
show that f is always nonnegative. To prove this, we ﬁrst observe that at any time
the residual capacity of any edge not in F is either inﬁnite or an integer multiple
of γ . Moreover we claim that an edge e ∈F always has positive residual capacity.
To see this, observe that any phase consists of at most n −1 augmentations by
less than 2 n−1
n γ in 7⃝and at most 2n augmentations by γ in 5⃝; hence the total
amount of ﬂow moved after e has become a member of F in the γ -phase is less
than 8nγ .
Hence f is always nonnegative and thus it is always a (b −b′)-ﬂow. We now
claim that f is always a minimum cost (b −b′)-ﬂow and that each v-w-path in F
is a shortest v-w-path in G f . Indeed, the ﬁrst statement implies the second one,
since by Theorem 9.6 for a minimum cost ﬂow f there is no negative circuit in
G f . Now the claim follows from Theorem 9.11: P in 5⃝and Q in 7⃝are both
shortest paths.
We ﬁnally show that if the algorithm stops in 3⃝or 4⃝with b′ ̸= 0, then there
is indeed no b-ﬂow. Suppose the algorithm stops in 3⃝, implying that there is a
vertex s with b′(s) > n−1
n γ , but that no vertex t with b′(t) < −1
n γ is reachable
from s in G f . Then let R be the set of vertices reachable from s in G f . Since f
is a (b −b′)-ﬂow, 
x∈R(b(x) −b′(x)) = 0. Therefore we have

x∈R
b(x) =

x∈R
(b(x)−b′(x))+

x∈R
b′(x) =

x∈R
b′(x) = b′(s)+

x∈R\{s}
b′(x) > 0.
This proves that no b-ﬂow exists. An analogous proof applies in the case that the
algorithm stops in 4⃝.
2

9.5 Orlin’s Algorithm
205
We now analyse the running time.
Lemma 9.15.
(Plotkin and Tardos [1990])
If at some stage of the algorithm
|b′(s)| > n−1
n γ for a vertex s, then the connected component of (V (G), F) con-
taining s increases during the next ⌈2 log n + log m⌉+ 4 phases.
Proof:
Let |b′(s)| > n−1
n γ1 for a vertex s at the beginning of some phase of the
algorithm where γ = γ1. Let γ0 be the γ -value in the preceding phase, and γ2 the
γ -value ⌈2 log n + log m⌉+ 4 phases later. We have 1
2γ0 ≥γ1 ≥16n2mγ2. Let b′
1
and f1 be the b′ and f at the beginning of the γ1-phase, respectively, and let b′
2
and f2 be the b′ and f at the end of the γ2-phase, respectively.
Let S be the connected component of (V (G), F) containing s in the γ1-phase,
and suppose that this remains unchanged for the ⌈2 log n + log m⌉+ 4 phases
considered. Note that 7⃝guarantees b′(v) = 0 for all vertices v with r(v) ̸= v.
Hence b′(v) = 0 for all v ∈S \ {s} and

x∈S
b(x) −b′
1(s) =

x∈S
(b(x) −b′
1(x)) =

e∈δ+(S)
f1(e) −

e∈δ−(S)
f1(e).
(9.4)
We claim that


x∈S
b(x)
 ≥1
n γ1.
(9.5)
If γ1 < γ0
2 , then each edge not in F has zero ﬂow, so the right-hand side of (9.4)
is zero, implying

x∈S b(x)
 = |b′
1(s)| > n−1
n γ1 ≥1
n γ1.
In the other case (γ1 = γ0
2 ) we have
1
n γ1 ≤n −1
n
γ1 < |b′
1(s)| ≤n −1
n
γ0 = γ0 −2
n γ1.
(9.6)
Since the ﬂow on any edge not in F is a multiple of γ0, the expression in (9.4) is
also a multiple of γ0. This together with (9.6) implies (9.5).
Now consider the total f2-ﬂow on edges leaving S minus the total ﬂow on
edges entering S. Since f2 is a (b −b′
2)-ﬂow, this is 
x∈S b(x) −b′
2(s). Using
(9.5) and |b′
2(s)| ≤n−1
n γ2 we obtain

e∈δ+(S)∪δ−(S)
| f2(e)| ≥


x∈S
b(x)
 −|b′
2(s)|
≥
1
n γ1 −n −1
n
γ2
≥
(16nm −1)γ2 > m(8nγ2).
Thus there exists at least one edge e with exactly one end in S and f2(e) > 8nγ2.
By 7⃝of the algorithm, this means that S is increased.
2
Theorem 9.16.
(Orlin [1993])
Orlin’s Algorithm solves the uncapacitated
Minimum Cost Flow Problem with conservative weights correctly in O(n log m
(m + n log n)) time.

206
9. Minimum Cost Flows
Proof:
The correctness has been proved above (Lemma 9.14). 7⃝takes O(mn)
total time. Lemma 9.15 implies that the total number of phases is O(n log m).
Moreover, it says the following: For a vertex s and a set S ⊆V (G) there are at
most ⌈2 log n+log m⌉+4 augmentations in 5⃝starting at s while S is the connected
component of (V (G), F) containing s. Since all vertices v with r(v) ̸= v have
b′(v) = 0 at any time, there are at most ⌈2 log n + log m⌉+ 4 augmentations
for each set S that is at some stage of the algorithm a connected component of
F. Since the family of these sets is laminar, there are at most 2n −1 such sets
(Corollary 2.15) and thus O(n log m) augmentations in 5⃝altogether.
Using the technique of Theorem 9.12, we obtain an overall running time of
O (mn + (n log m)(m + n log n)).
2
This is the best known running time for the uncapacitated Minimum Cost
Flow Problem.
Theorem 9.17.
(Orlin [1993]) The general Minimum Cost Flow Problem can
be solved in O (m log m(m + n log n)) time, where n = |V (G)| and m = |E(G)|.
Proof:
We apply the construction given in Lemma 9.2. Thus we have to solve
an uncapacitated Minimum Cost Flow Problem on a bipartite graph H with
V (H) = A′
.
∪B′, where A′ = E(G) and B′ = V (G). Since H is acyclic, an
initial feasible potential can be computed in O(|E(H)|) = O(m) time. As shown
above (Theorem 9.16), the overall running time is bounded by O(m log m) shortest
path computations in a subgraph of
↔
H with nonnegative weights.
Before we call Dijkstra’s Algorithm we apply the following operation to
each vertex a ∈A′ that is not an endpoint of the path we are looking for: add
an edge (b, b′) for each pair of edges (b, a), (a, b′) and set its weight to the sum
of the weights of (b, a) and (a, b′); ﬁnally delete a. Clearly the resulting instance
of the Shortest Path Problem is equivalent. Since each vertex in A′ has four
incident edges in
↔
H, the resulting graph has O(m) edges and at most n+2 vertices.
The preprocessing takes constant time per vertex, i.e. O(m). The same holds for
the ﬁnal computation of the path in
↔
H and of the distance labels of the deleted
vertices. We get an overall running time of O ((m log m)(m + n log n)).
2
This is the fastest known strongly polynomial algorithm for the general Min-
imum Cost Flow Problem. An algorithm which achieves the same running time
but works directly on capacitated instances has been described by Vygen [2002].
9.6 Flows Over Time
We now consider ﬂows over time (also sometimes called dynamic ﬂows); i.e. the
ﬂow value on each edge may change over time, and ﬂow entering an edge arrives
at the endvertex after a speciﬁed delay:

9.6 Flows Over Time
207
Deﬁnition 9.18.
Let (G, u, s, t) be a network with transit times l : E(G) →R+
and a time horizon T ∈R+. Then an s-t-ﬂow over time f consists of a Lebesgue-
measurable function fe : [0, T ] →R+ for each e ∈E(G) with fe(τ) ≤u(e) for
all τ ∈[0, T ] and e ∈E(G) and
exf (v, a) :=

e∈δ−(v)
7 a−l(e)
0
fe(τ)dτ −

e∈δ+(v)
7 a
0
fe(τ)dτ ≥0
(9.7)
for all v ∈V (G) \ {s} and a ∈[0, T ].
fe(τ) is called the rate of ﬂow entering e at time τ (and leaving this edge l(e)
time units later). (9.7) allows intermediate storage at vertices, like in s-t-preﬂows.
It is natural to maximize the ﬂow arriving at sink t:
Maximum Flow Over Time Problem
Instance:
A network (G, u, s, t). Transit times l : E(G) →R+ and a time
horizon T ∈R+.
Task:
Find an s-t-ﬂow over time f such that value ( f ) := exf (t, T ) is
maximum.
Following Ford and Fulkerson [1958], we show that this problem can be re-
duced to the Minimum Cost Flow Problem.
Theorem 9.19.
The Maximum Flow Over Time Problem can be solved in the
same time as the Minimum Cost Flow Problem.
Proof:
Given an instance (G, u, s, t,l, T ) as above, deﬁne a new edge e′ = (t, s)
and G′ := G + e′. Set u(e′) := u(E(G)), c(e′) := −T and c(e) := l(e) for e ∈
E(G). Consider the instance (G′, u, 0, c) of the Minimum Cost Flow Problem.
Let f ′ be an optimum solution, i.e. a minimum cost (with respect to c) circulation
in (G′, u). By Proposition 9.5, f ′ can be decomposed into ﬂows on circuits, i.e.
there is a set C of circuits in G′ and positive numbers h : C →R+ such that
f ′(e) = {h(C) : C ∈C, e ∈E(C)}. We have c(C) ≤0 for all C ∈C as f ′ is a
minimum cost circulation.
Let C ∈C with c(C) < 0. C must contain e′. For e = (v, w) ∈E(C) \ {e′},
let dC
e be the distance from s to v in (C, c). Set
f ∗
e (τ) :=

{h(C) : C ∈C, c(C) < 0, e ∈E(C), dC
e ≤τ ≤dC
e −c(C)}
for e ∈E(G) and τ ∈[0, T ]. This deﬁnes an s-t-ﬂow over time without inter-
mediate storage (i.e. exf (v, a) = 0 for all v ∈V (G) \ {s, t} and all a ∈[0, T ]).
Moreover,
value ( f ∗) =

e∈δ−(t)
7 T −l(e)
0
f ∗
e (τ)dτ = −

e∈E(G′)
c(e) f ′(e).

208
9. Minimum Cost Flows
We claim that f ∗is optimum. To see this, let f be any s-t-ﬂow over time,
and set fe(τ) := 0 for e ∈E(G) and τ /∈[0, T ]. Let π(v) := dist(G′
f ′,c)(s, v) for
v ∈V (G). As G′
f ′ contains no negative circuit (cf. Theorem 9.6), π is a feasible
potential in (G′
f ′, c). We have
value ( f ) = exf (t, T ) ≤

v∈V (G)
exf (v, π(v))
because of (9.7), π(t) = T , π(s) = 0 and 0 ≤π(v) ≤T for all v ∈V (G). Hence
value ( f )
≤

e=(v,w)∈E(G)
7 π(w)−l(e)
0
fe(τ)dτ −
7 π(v)
0
fe(τ)dτ

≤

e=(v,w)∈E(G):π(w)−l(e)>π(v)
(π(w) −l(e) −π(v))u(e)
=

e=(v,w)∈E(G)
(π(w) −l(e) −π(v)) f ′(e)
=

e=(v,w)∈E(G′)
(π(w) −c(e) −π(v)) f ′(e)
=
−

e=(v,w)∈E(G′)
c(e) f ′(e)
=
value ( f ∗)
2
Other ﬂow over time problems are signiﬁcantly more difﬁcult. Hoppe and
Tardos [2000] solved the so-called quickest transshipment problem (with several
sources and sinks) with integral transit times using submodular function minimiza-
tion (see Chapter 14). Finding minimum cost ﬂows over time is NP-hard (Klinz
and Woeginger [2004]). See Fleischer and Skutella [2004] for approximation al-
gorithms and more information.
Exercises
1. Show that the Maximum Flow Problem can be regarded as a special case
of the Minimum Cost Flow Problem.
2. Let G be a digraph with capacities u : E(G) →R+, and let b : V (G) →R
with 
v∈V (G) b(v) = 0. Prove that there exists a b-ﬂow if and only if

e∈δ+(X)
u(e) ≥

v∈X
b(v)
for all X ⊆V (G).
(Gale [1957])

Exercises
209
3. Let G be a digraph with lower and upper capacities l, u : E(G) →R+, where
l(e) ≤u(e) for all e ∈E(G), and let b1, b2 : V (G) →R with

v∈V (G)
b1(v) ≤0 ≤

v∈V (G)
b2(v).
Prove that there exists a ﬂow f with l(e) ≤f (e) ≤u(e) for all e ∈E(G)
and
b1(v) ≤

e∈δ+(v)
f (e) −

e∈δ−(v)
f (e) ≤b2(v)
for all v ∈V (G)
if and only if

e∈δ+(X)
u(e) ≥max
⎧
⎨
⎩

v∈X
b1(v), −

v∈V (G)\X
b2(v)
⎫
⎬
⎭+

e∈δ−(X)
l(e)
for all X ⊆V (G). (This is a generalization of Exercise 4 of Chapter 8 and
Exercise 2 of this chapter.)
(Hoffman [1960])
4. Prove the following theorem of Ore [1956]. Given a digraph G and nonnega-
tive integers a(x), b(x) for each x ∈V (G), then G has a spanning subgraph
H with |δ+
H(x)| = a(x) and |δ−
H(x)| = b(x) for all x ∈V (G) if and only if

x∈V (G)
a(x) =

x∈V (G)
b(x)
and

x∈X
a(x) ≤

y∈V (G)
min{b(y), |EG(X, {y})|}
for all X ⊆V (G).
(Ford and Fulkerson [1962])
5. Consider the Minimum Cost Flow Problem where inﬁnite capacities (u(e) =
∞for some edges e) are allowed.
(a) Show that an instance is unbounded if and only if it is feasible and there
is a negative circuit all whose edges have inﬁnite capacity.
(b) Show how to decide in O(n3) time whether an instance is unbounded.
(c) Show that for an instance that is not unbounded each inﬁnite capacity can
be equivalently replaced by a ﬁnite capacity.
6.
∗
Let (G, u, c, b) be an instance of the Minimum Cost Flow Problem. We call
a function π : V (G) →R an optimal potential if there exists a minimum cost
b-ﬂow f such that π is a feasible potential with respect to (G f , c).
(a) Prove that a function π : V (G) →R is an optimal potential if and only
if for all X ⊆V (G):
b(X) +

e∈δ−(X):cπ(e)<0
u(e) ≤

e∈δ+(X):cπ(e)≤0
u(e).

210
9. Minimum Cost Flows
(b) Given π : V (G) →R, show how to ﬁnd a set X violating the condition
in (a) or determine that none exists.
(c) Suppose an optimal potential is given; show how to ﬁnd a minimum cost
b-ﬂow in O(n3) time.
Note: This leads to so-called cut cancelling algorithms for the Minimum Cost
Flow Problem.
(Hassin [1983])
7. Consider the following algorithm scheme for the Minimum Cost Flow Prob-
lem: ﬁrst ﬁnd any b-ﬂow, then as long as there is a negative augmenting cycle,
augment the ﬂow along it (by the maximum possible amount). We have seen
in Section 9.3 that we obtain a strongly polynomial running time if we always
choose a circuit of minimum mean weight. Prove that without this speciﬁca-
tion one cannot guarantee that the algorithm terminates.
(Use the construction in Exercise 2 of Chapter 8.)
8. Consider the problem as described in Exercise 3 with a weight function c :
E(G) →R. Can one ﬁnd a minimum cost ﬂow that satisﬁes the constraints
of Exercise 3? (Reduce this problem to a standard Minimum Cost Flow
Problem.)
9. The Directed Chinese Postman Problem can be formulated as follows:
given a strongly connected simple digraph G with weights c : E(G) →R+,
ﬁnd f : E(G) →N such that the graph which contains f (e) copies of each
edge e ∈E(G) is Eulerian and 
e∈E(G) c(e) f (e) is minimum. How can this
problem be solved by a polynomial-time algorithm?
(For the Undirected Chinese Postman Problem, see Section 12.2.)
10.
∗
The fractional b-matching problem is deﬁned as follows: Given an undirected
graph G, capacities u : E(G) →R+, numbers b : V (G) →R+ and weights
c : E(G) →R, we are looking for an f : E(G) →R+ with f (e) ≤u(e)
for all e ∈E(G) and 
e∈δ(v) f (e) ≤b(v) for all v ∈V (G) such that

e∈E(G) c(e) f (e) is maximum.
(a) Show how to solve this problem by reducing it to a Minimum Cost Flow
Problem.
(b) Suppose now b and u are integral. Show that then the fractional b-
matching problem always has a half-integral solution f (i.e. 2 f (e) ∈Z
for all e ∈E(G)).
Note: The (integral) Maximum Weight b-Matching Problem is the subject
of Section 12.1.
11. Show that the Successive Shortest Path Algorithm correctly decides
whether a b-ﬂow exists.
12. The scaling technique can be considered in a quite general setting: Let  be a
family of set systems each of which contains the empty set. Suppose that there
is an algorithm which solves the following problem: given an (E, F) ∈,
weights c : E →Z+ and a set X ∈F; ﬁnd a Y ∈F with c(Y) > c(X) or
assert that no such Y exists. Suppose this algorithm has a running time which
is polynomial in size(c). Prove that then there is an algorithm for ﬁnding a

Exercises
211
maximum weight set X ∈F for a given (E, F) ∈ and c : E →Z+, whose
running time is polynomial in size(c).
(Gr¨otschel and Lov´asz [1995]; see also Schulz, Weismantel and Ziegler [1995],
and Schulz and Weismantel [2002])
13. Let (G, u, c, b) be an instance of the Minimum Cost Flow Problem that has
a solution. We assume that G is connected. Prove that there is a set of edges
F ⊆E(G) such that when ignoring the orientations, F forms a spanning
tree in G, and there is an optimum solution f of the Minimum Cost Flow
Problem such that f (e) ∈{0, u(e)} for all e ∈E(G) \ F.
Note: Such a solution is called a spanning tree solution. Orlin’s Algorithm
in fact computes a spanning tree solution. These play a central role in the
network simplex method. This is a specialization of the simplex method to
the Minimum Cost Flow Problem, which can be implemented to run in
polynomial time; see Orlin [1997], Orlin, Plotkin and Tardos [1993], and
Armstrong and Jin [1997].
14. Prove that in 7⃝of Orlin’s Algorithm one can replace the 8nγ -bound by
5nγ .
15. Consider the shortest path computations with nonnegative weights (using Di-
jkstra’s Algorithm) in the algorithms of Section 9.4 and 9.5. Show that
even for graphs with parallel edges each of these computations can be per-
formed in O(n2) time, provided that we have the incidence list of G sorted by
edge costs. Conclude that Orlin’s Algorithm runs in O(mn2 log m) time.
16.
∗
The Push-Relabel Algorithm (Section 8.5) can be generalized to the
Minimum Cost Flow Problem. For an instance (G, u, b, c) with integral
costs c, we look for a b-ﬂow f and a feasible potential π in (G f , c).
We start by setting π := 0 and saturating all edges e with negative cost.
Then we apply
3⃝of the Push-Relabel Algorithm with the following
modiﬁcations: An edge e is admissible if e ∈E(G f ) and cπ(e) < 0. A
vertex v is active if b(v) + exf (v) > 0. Relabel(v) consists of setting
π(v) := max{π(w) −c(e) −1 : e = (v, w) ∈E(G f )}. In Push(e) for
e ∈δ+(v) we set γ := min{b(v) + exf (v), u f (e)}.
(a) Prove that the number of Relabel operations is O(n2|cmin|), where
cmin = mine∈E(G) c(e).
Hint: Some vertex w with b(w) + exf (w) < 0 must be reachable in G f
from any active vertex v. Note that b(w) has never changed and recall
the proofs of Lemmata 8.22 and 8.24.
(b) Show that the overall running time is O(n2mcmax).
(c) Prove that the algorithm computes an optimum solution.
(d) Apply scaling to obtain an O(n2m log cmax)-algorithm for the Minimum
Cost Flow Problem with integral costs c.
(Goldberg and Tarjan [1990])
17. Given a network (G, u, s, t) with integral transit times l : E(G) →Z+, a
time horizon T ∈N, a value V ∈R+, and costs c : E(G) →R+. We
look for an s-t-ﬂow over time f with value ( f ) = V and minimum cost

212
9. Minimum Cost Flows

e∈E(G) c(e)
8 T
0 fe(τ)dτ. Show how to solve this in polynomial time if T is
a constant.
Hint: Consider a time-expanded network with a copy of G for each discrete
time step.
References
General Literature:
Ahuja, R.K., Magnanti, T.L., and Orlin, J.B. [1993]: Network Flows. Prentice-Hall, Engle-
wood Cliffs 1993
Cook, W.J., Cunningham, W.H., Pulleyblank, W.R., and Schrijver, A. [1998]: Combinato-
rial Optimization. Wiley, New York 1998, Chapter 4
Goldberg, A.V., Tardos, ´E., and Tarjan, R.E. [1990]: Network ﬂow algorithms. In: Paths,
Flows, and VLSI-Layout (B. Korte, L. Lov´asz, H.J. Pr¨omel, A. Schrijver, eds.), Springer,
Berlin 1990, pp. 101–164
Gondran, M., and Minoux, M. [1984]: Graphs and Algorithms. Wiley, Chichester 1984,
Chapter 5
Jungnickel, D. [1999]: Graphs, Networks and Algorithms. Springer, Berlin 1999, Chapter
9
Lawler, E.L. [1976]: Combinatorial Optimization: Networks and Matroids. Holt, Rinehart
and Winston, New York 1976, Chapter 4
Ruhe, G. [1991]: Algorithmic Aspects of Flows in Networks. Kluwer Academic Publishers,
Dordrecht 1991
Cited References:
Armstrong, R.D., and Jin, Z. [1997]: A new strongly polynomial dual network simplex
algorithm. Mathematical Programming 78 (1997), 131–148
Busacker, R.G., and Gowen, P.J. [1961]: A procedure for determining a family of minimum-
cost network ﬂow patterns. ORO Technical Paper 15, Operational Research Ofﬁce, Johns
Hopkins University, Baltimore 1961
Edmonds, J., and Karp, R.M. [1972]: Theoretical improvements in algorithmic efﬁciency
for network ﬂow problems. Journal of the ACM 19 (1972), 248–264
Fleischer, L., and Skutella, M. [2004]: Quickest ﬂows over time. Manuscript, 2004
Ford, L.R., and Fulkerson, D.R. [1958]: Constructing maximal dynamic ﬂows from static
ﬂows. Operations Research 6 (1958), 419–433
Ford, L.R., and Fulkerson, D.R. [1962]: Flows in Networks. Princeton University Press,
Princeton 1962
Gale, D. [1957]: A theorem on ﬂows in networks. Paciﬁc Journal of Mathematics 7 (1957),
1073–1082
Goldberg, A.V., and Tarjan, R.E. [1989]: Finding minimum-cost circulations by cancelling
negative cycles. Journal of the ACM 36 (1989), 873–886
Goldberg, A.V., and Tarjan, R.E. [1990]: Finding minimum-cost circulations by successive
approximation. Mathematics of Operations Research 15 (1990), 430–466
Gr¨otschel, M., and Lov´asz, L. [1995]: Combinatorial optimization. In: Handbook of Com-
binatorics; Vol. 2 (R.L. Graham, M. Gr¨otschel, L. Lov´asz, eds.), Elsevier, Amsterdam
1995
Hassin, R. [1983]: The minimum cost ﬂow problem: a unifying approach to dual algorithms
and a new tree-search algorithm. Mathematical Programming 25 (1983), 228–239
Hitchcock, F.L. [1941]: The distribution of a product from several sources to numerous
localities. Journal of Mathematical Physics 20 (1941), 224–230

References
213
Hoffman, A.J. [1960]: Some recent applications of the theory of linear inequalities to
extremal combinatorial analysis. In: Combinatorial Analysis (R.E. Bellman, M. Hall,
eds.), AMS, Providence 1960, pp. 113–128
Hoppe, B., and Tardos, ´E. [2000]: The quickest transshipment problem. Mathematics of
Operations Research 25 (2000), 36–62
Iri, M. [1960]: A new method for solving transportation-network problems. Journal of the
Operations Research Society of Japan 3 (1960), 27–87
Jewell, W.S. [1958]: Optimal ﬂow through networks. Interim Technical Report 8, MIT 1958
Klein, M. [1967]: A primal method for minimum cost ﬂows, with applications to the
assignment and transportation problems. Management Science 14 (1967), 205–220
Klinz, B., and Woeginger, G.J. [2004]: Minimum cost dynamic ﬂows: the series-parallel
case. Networks 43 (2004), 153–162
Orden, A. [1956]: The transshipment problem. Management Science 2 (1956), 276–285
Ore, O. [1956]: Studies on directed graphs I. Annals of Mathematics 63 (1956), 383–406
Orlin, J.B. [1993]: A faster strongly polynomial minimum cost ﬂow algorithm. Operations
Research 41 (1993), 338–350
Orlin, J.B. [1997]: A polynomial time primal network simplex algorithm for minimum cost
ﬂows. Mathematical Programming 78 (1997), 109–129
Orlin, J.B., Plotkin, S.A., and Tardos, ´E. [1993]: Polynomial dual network simplex algo-
rithms. Mathematical Programming 60 (1993), 255–276
Plotkin, S.A., and Tardos, ´E. [1990]: Improved dual network simplex. Proceedings of the
1st Annual ACM-SIAM Symposium on Discrete Algorithms (1990), 367–376
Schulz, A.S., Weismantel, R., and Ziegler, G.M. [1995]: 0/1-Integer Programming: opti-
mization and augmentation are equivalent. In: Algorithms – ESA ’95; LNCS 979 (P.
Spirakis, ed.), Springer, Berlin 1995, pp. 473–483
Schulz, A.S., and Weismantel, R. [2002]: The complexity of generic primal algorithms
for solving general integer problems. Mathematics of Operations Research 27 (2002),
681–192
Tardos, ´E. [1985]: A strongly polynomial minimum cost circulation algorithm. Combina-
torica 5 (1985), 247–255
Tolsto˘ı, A.N. [1930]: Metody nakhozhdeniya naimen’shego summovogo kilometrazha
pri planirovanii perevozok v prostanstve. In: Planirovanie Perevozok, Sbornik pervy˘ı,
Transpechat’ NKPS, Moskow 1930, pp. 23–55. (See A. Schrijver, On the history of
the transportation and maximum ﬂow problems, Mathematical Programming 91 (2002)
437–445)
Tomizawa, N. [1971]: On some techniques useful for solution of transportation network
problems. Networks 1 (1971), 173–194
Vygen, J. [2002]: On dual minimum cost ﬂow algorithms. Mathematical Methods of Oper-
ations Research 56 (2002), 101–126
Wagner, H.M. [1959]: On a class of capacitated transportation problems. Management
Science 5 (1959), 304–318

10. Maximum Matchings
Matching theory is one of the classical and most important topics in combinatorial
theory and optimization. All the graphs in this chapter are undirected. Recall that
a matching is a set of pairwise disjoint edges. Our main problem is:
Cardinality Matching Problem
Instance:
An undirected graph G.
Task:
Find a maximum cardinality matching in G.
Since the weighted version of this problem is signiﬁcantly more difﬁcult we
postpone it to Chapter 11. But already the above cardinality version has applica-
tions: Suppose in the Job Assignment Problem each job has the same processing
time, say one hour, and we ask whether we can ﬁnish all the jobs within one hour.
In other words: given a bipartite graph G with bipartition V (G) = A
.
∪B, we
look for numbers x : E(G) →R+ with 
e∈δ(a) x(e) = 1 for each job a ∈A and

e∈δ(b) x(e) ≤1 for each employee b ∈B. We can write this as a linear inequal-
ity system x ≥0, Mx ≤1l, M′x ≥1l, where the rows of M and M′ are rows
of the node-edge incidence matrix of G. These matrices are totally unimodular
by Theorem 5.24. From Theorem 5.19 we conclude that if there is any solution
x, then there is also an integral solution. Now observe that the integral solutions
to the above linear inequality system are precisely the incidence vectors of the
matchings in G covering A.
Deﬁnition 10.1.
Let G be a graph and M a matching in G. We say that a vertex
v is covered by M if v ∈e for some e ∈M. M is called a perfect matching if all
vertices are covered by M.
In Section 10.1 we consider matchings in bipartite graphs. Algorithmically
this problem can be reduced to a Maximum Flow Problem as mentioned in the
introduction of Chapter 8. The Max-Flow-Min-Cut Theorem as well as the concept
of augmenting paths have nice interpretations in our context.
Matching in general, non-bipartite graphs, does not reduce directly to network
ﬂows. We introduce two necessary and sufﬁcient conditions for a general graph
to have a perfect matching in Sections 10.2 and 10.3. In Section 10.4 we consider
factor-critical graphs which have a matching covering all vertices but v, for each
v ∈V (G). These play an important role in Edmonds’ algorithm for the Cardi-

216
10. Maximum Matchings
nality Matching Problem, described in Section 10.5, and its weighted version
which we postpone to Sections 11.2 and 11.3.
10.1 Bipartite Matching
Since the Cardinality Matching Problem is easier if G is bipartite, we shall
deal with this case ﬁrst. In this section, a bipartite graph G is always assumed to
have the bipartition V (G) = A
.
∪B. Since we may assume that G is connected,
we can regard this bipartition as unique (Exercise 20 of Chapter 2).
For a graph G, let ν(G) denote the maximum cardinality of a matching in G,
while τ(G) is the minimum cardinality of a vertex cover in G.
Theorem 10.2.
(K¨onig [1931]) If G is bipartite, then ν(G) = τ(G).
Proof:
Consider the graph G′ = (V (G)
.
∪{s, t},
E(G) ∪{{s, a} : a ∈
A} ∪{{b, t} : b ∈B}). Then ν(G) is the maximum number of vertex-disjoint
s-t-paths, while τ(G) is the minimum number of vertices whose deletion makes t
unreachable from s. The theorem now immediately follows from Menger’s The-
orem 8.10.
2
ν(G) ≤τ(G) evidently holds for any graph (bipartite or not), but we do not
have equality in general (as the triangle K3 shows).
Several statements are equivalent to K¨onig’s Theorem. Hall’s Theorem is prob-
ably the best-known version.
Theorem 10.3.
(Hall [1935]) Let G be a bipartite graph with bipartition V (G) =
A
.
∪B. Then G has a matching covering A if and only if
|(X)| ≥|X|
for all X ⊆A.
(10.1)
Proof:
The necessity of the condition is obvious. To prove the sufﬁciency, as-
sume that G has no matching covering A, i.e. ν(G) < |A|. By Theorem 10.2 this
implies τ(G) < |A|.
Let A′ ⊆A, B′ ⊆B such that A′∪B′ covers all the edges and |A′∪B′| < |A|.
Obviously (A \ A′) ⊆B′. Therefore |(A \ A′)| ≤|B′| < |A| −|A′| = |A \ A′|,
and the Hall condition (10.1) is violated.
2
It is worthwhile to mention that it is not too difﬁcult to prove Hall’s Theorem
directly. The following proof is due to Halmos and Vaughan [1950]:
Second Proof of Theorem 10.3:
We show that any G satisfying the Hall condi-
tion (10.1) has a matching covering A. We use induction on |A|, the cases |A| = 0
and |A| = 1 being trivial.
If |A| ≥2, we consider two cases: If |(X)| > |X| for every nonempty
proper subset X of A, then we take any edge {a, b} (a ∈A, b ∈B), delete its
two vertices and apply induction. The smaller graph satisﬁes the Hall condition
because |(X)| −|X| can have decreased by at most one for any X ⊆A \ {a}.

10.1 Bipartite Matching
217
Now assume that there is a nonempty proper subset X of A with |(X)| = |X|.
By induction there is a matching covering X in G[X ∪(X)]. We claim that we
can extend this to a matching in G covering A. Again by the induction hypothesis,
we have to show that G[(A \ X) ∪(B \ (X))] satisﬁes the Hall condition. To
check this, observe that for any Y ⊆A \ X we have (in the original graph G):
|(Y) \ (X)| = |(X ∪Y)| −|(X)| ≥|X ∪Y| −|X| = |Y|.
2
A special case of Hall’s Theorem is the so-called “Marriage Theorem”:
Theorem 10.4.
(Frobenius [1917]) Let G be a bipartite graph with bipartition
V (G) = A
.
∪B. Then G has a perfect matching if and only if |A| = |B| and
|(X)| ≥|X| for all X ⊆A.
2
The variety of applications of Hall’s Theorem is indicated by Exercises 4–8.
The proof of K¨onig’s Theorem 10.2 shows how to solve the bipartite matching
problem algorithmically:
Theorem 10.5.
The Cardinality Matching Problem for bipartite graphs G
can be solved in O(nm) time, where n = |V (G)| and m = |E(G)|.
Proof:
Let G be a bipartite graph with bipartition V (G) = A
.
∪B. Add a vertex
s and connect it to all vertices of A, and add another vertex t connected to all
vertices of B. Orient the edges from s to A, from A to B, and from B to t. Let
the capacities be 1 everywhere. Then a maximum integral s-t-ﬂow corresponds to
a maximum cardinality matching (and vice versa).
So we apply the Ford-Fulkerson Algorithm and ﬁnd a maximum s-t-
ﬂow (and thus a maximum matching) after at most n augmentations. Since each
augmentation takes O(m) time, we are done.
2
This result is essentially due to Kuhn [1955]. In fact, one can use the con-
cept of shortest augmenting paths again (cf. the Edmonds-Karp Algorithm). In
this way one obtains the O
√n(m + n)

-algorithm of Hopcroft and Karp [1973].
This algorithm will be discussed in Exercises 9 and 10. Slight improvements of
the Hopcroft-Karp Algorithm yield running times of O

n
9
mn
log n

(Alt et al.
[1991]) and O

m√n
log n2
m
log n

(Feder and Motwani [1995]). The latter bound is the
best known for dense graphs.
Let us reformulate the augmenting path concept in our context.
Deﬁnition 10.6.
Let G be a graph (bipartite or not), and let M be some matching
in G. A path P is an M-alternating path if E(P) \ M is a matching. An M-
alternating path is M-augmenting if its endpoints are not covered by M.
One immediately checks that augmenting paths must have odd length.

218
10. Maximum Matchings
Theorem 10.7.
(Berge [1957])
Let G be a graph (bipartite or not) with some
matching M. Then M is maximum if and only if there is no M-augmenting path.
Proof:
If there is an M-augmenting path P, the symmetric difference M△E(P)
is a matching and has greater cardinality than M, so M is not maximum. On
the other hand, if there is a matching M′ such that |M′| > |M|, the symmetric
difference M△M′ is the vertex-disjoint union of alternating circuits and paths,
where at least one path must be M-augmenting.
2
In the bipartite case Berge’s Theorem of course also follows from Theorem
8.5.
10.2 The Tutte Matrix
We now consider maximum matchings from an algebraic point of view. Let G
be a simple undirected graph, and let G′ be the directed graph resulting from G
by arbitrarily orienting the edges. For any vector x = (xe)e∈E(G) of variables, we
deﬁne the Tutte matrix
TG(x) = (t x
vw)v,w∈V (G)
by
t x
vw :=
 x{v,w}
if (v, w) ∈E(G′)
−x{v,w}
if (w, v) ∈E(G′)
0
otherwise
.
(Such a matrix M, where M = −M ⊤, is called skew-symmetric.) det TG(x) is a
polynomial in the variables xe (e ∈E(G)).
Theorem 10.8.
(Tutte [1947]) G has a perfect matching if and only if det TG(x)
is not identically zero.
Proof:
Let V (G) = {v1, . . . , vn}, and let Sn be the set of all permutations on
{1, . . . , n}. By deﬁnition of the determinant,
det TG(x) =

π∈Sn
sgn(π)
n'
i=1
t x
vi,vπ(i).
Let S′
n :=

π ∈Sn : (n
i=1 t x
vi,vπ(i) ̸= 0

. Each permutation π ∈Sn corresponds
to a directed graph Hπ := (V (G), {(vi, vπ(i)) : i = 1, . . . , n}) where each vertex
x has |δ−
Hπ(x)| = |δ+
Hπ(x)| = 1. For permutations π ∈S′
n, Hπ is a subgraph of
↔
G′.
If there exists a permutation π ∈S′
n such that Hπ consists of even circuits only,
then by taking every second edge of each circuit (and ignoring the orientations)
we obtain a perfect matching in G.

10.2 The Tutte Matrix
219
Otherwise, for each π ∈S′
n there is a permutation r(π) ∈S′
n such that Hr(π)
is obtained by reversing the ﬁrst odd circuit in Hπ, i.e. the odd circuit containing
the vertex with minimum index. Of course r(r(π)) = π.
Observe that sgn(π) = sgn(r(π)), i.e. the two permutations have the same
sign: if the ﬁrst odd circuit consists of the vertices w1, . . . , w2k+1 with π(wi) =
wi+1 (i = 1, . . . , 2k) and π(w2k+1) = w1, then we obtain r(π) by 2k transpo-
sitions: for j = 1, . . . , k exchange π(w2 j−1) with π(w2k) and then π(w2 j) with
π(w2k+1).
Moreover, (n
i=1 t x
vi,vπ(i) = −(n
i=1 t x
vi,vr(π)(i). So the two corresponding terms in
the sum
det TG(x) =

π∈S′n
sgn(π)
n'
i=1
t x
vi,vπ(i)
cancel each other. Since this holds for all pairs π,r(π) ∈S′
n, we conclude that
det TG(x) is identically zero.
So if G has no perfect matching, det TG(x) is identically zero. On the other
hand, if G has a perfect matching M, consider the permutation deﬁned by π(i) :=
j and π( j) := i for all {vi, vj} ∈M. The corresponding term (n
i=1 t x
vi,vπ(i) =
(
e∈M

−xe2
cannot cancel out with any other term, so det TG(x) is not identically
zero.
2
Originally, Tutte used Theorem 10.8 to prove his main theorem on matchings,
Theorem 10.13. Theorem 10.8 does not provide a good characterization of the
property that a graph has a perfect matching. The problem is that the determinant
is easy to compute if the entries are numbers (Theorem 4.10) but difﬁcult to
compute if the entries are variables. However, the theorem suggests a randomized
algorithm for the Cardinality Matching Problem:
Corollary 10.9.
(Lov´asz [1979]) Let x = (xe)e∈E(G) be a random vector where
each coordinate is equally distributed in [0, 1]. Then with probability 1 the rank of
TG(x) is exactly twice the size of a maximum matching.
Proof:
Suppose the rank of TG(x) is k, say the ﬁrst k rows are linearly inde-
pendent. Since TG(x) is skew-symmetric, also the ﬁrst k columns are linearly
independent. So the principal submatrix (t x
vi,vj)1≤i, j≤k is nonsingular, and by The-
orem 10.8 the subgraph G[{v1, . . . , vk}] has a perfect matching. In particular, k is
even and G has a matching of cardinality k
2.
On the other hand, if G has a matching of cardinality k, the determinant of
the principal submatrix T ′ whose rows and columns correspond to the 2k vertices
covered by M is not identically zero by Theorem 10.8. The set of vectors x for
which det T ′(x) = 0 must then have measure zero. So with probability one, the
rank of TG(x) is at least 2k.
2
Of course it is not possible to choose random numbers from [0, 1] with a
digital computer. However, it can be shown that it sufﬁces to choose random
integers from the ﬁnite set {1, 2, . . . , N}. For sufﬁciently large N, the probability

220
10. Maximum Matchings
of error will become arbitrarily small (see Lov´asz [1979]). Lov´asz’ algorithm
can be used to determine a maximum matching (not only its cardinality). See
Rabin and Vazirani [1989], Mulmuley, Vazirani and Vazirani [1987], and Mucha
and Sankowski [2004] for further randomized algorithms for ﬁnding a maximum
matching in a graph. Moreover we note that Geelen [2000] has shown how to
derandomize Lov´asz’ algorithm. Although its running time is worse than that
of Edmonds’ matching algorithm (see Section 10.5), it is important for some
generalizations of the Cardinality Matching Problem (e.g., see Geelen and
Iwata [2005]).
10.3 Tutte’s Theorem
We now consider the Cardinality Matching Problem in general graphs. A
necessary condition for a graph to have a perfect matching is that every connected
component is even (i.e. has an even number of vertices). This condition is not
sufﬁcient, as the graph K1,3 (Figure 10.1(a)) shows.
(a)
(b)
Fig. 10.1.
The reason that K1,3 has no perfect matching is that there is one vertex (the
black one) whose deletion produces three odd connected components. The graph
shown in Figure 10.1(b) is more complicated. Does this graph have a perfect
matching? If we delete the three black vertices, we get ﬁve odd connected com-
ponents (and one even connected component). If there were a perfect matching, at
least one vertex of each odd connected component would have to be connected to
one of the black vertices. This is impossible because the number of odd connected
components exceeds the number of black vertices.
More generally, for X ⊆V (G) let qG(X) denote the number of odd connected
components in G −X. Then a graph for which qG(X) > |X| holds for some

10.3 Tutte’s Theorem
221
X ⊆V (G) cannot have a perfect matching: otherwise there must be, for each
odd connected component in G −X, at least one matching edge connecting this
connected component with X, which is impossible if there are more odd connected
components than elements of X. Tutte’s Theorem says that the above necessary
condition is also sufﬁcient:
Deﬁnition 10.10.
A graph G satisﬁes the Tutte condition if qG(X) ≤|X| for all
X ⊆V (G). A nonempty vertex set X ⊆V (G) is a barrier if qG(X) = |X|.
To prove the sufﬁciency of the Tutte condition we shall need an easy obser-
vation and an important deﬁnition:
Proposition 10.11.
For any graph G and any X ⊆V (G) we have
qG(X) −|X| ≡|V (G)|
(mod 2).
2
Deﬁnition 10.12.
A graph G is called factor-critical if G−v has a perfect match-
ing for each v ∈V (G). A matching is called near-perfect if it covers all vertices
but one.
Now we can prove Tutte’s Theorem:
Theorem 10.13.
(Tutte [1947]) A graph G has a perfect matching if and only if
it satisﬁes the Tutte condition:
qG(X) ≤|X|
for all X ⊆V (G).
Proof:
We have already seen the necessity of the Tutte condition. We now prove
the sufﬁciency by induction on |V (G)| (the case |V (G)| ≤2 being trivial).
Let G be a graph satisfying the Tutte condition. |V (G)| cannot be odd since
otherwise the Tutte condition is violated because qG(∅) ≥1.
So by Proposition 10.11, |X| −qG(X) must be even for every X ⊆V (G).
Since |V (G)| is even and the Tutte condition holds, every singleton is a barrier.
We choose a maximal barrier X. G −X has |X| odd connected components.
G −X cannot have any even connected components because otherwise X ∪{v},
where v is a vertex of some even connected component, is a barrier (G −(X ∪{v})
has |X| + 1 odd connected components), contradicting the maximality of X.
We now claim that each odd connected component of G −X is factor-critical.
To prove this, let C be some odd connected component of G −X and v ∈V (C).
If C −v has no perfect matching, by the induction hypothesis there is some
Y ⊆V (C) \ {v} such that qC−v(Y) > |Y|. By Proposition 10.11, qC−v(Y) −|Y|
must be even, so
qC−v(Y) ≥|Y| + 2.
Since X, Y and {v} are pairwise disjoint, we have

222
10. Maximum Matchings
qG(X ∪Y ∪{v})
=
qG(X) −1 + qC(Y ∪{v})
=
|X| −1 + qC−v(Y)
≥
|X| −1 + |Y| + 2
=
|X ∪Y ∪{v}|.
So X ∪Y ∪{v} is a barrier, contradicting the maximality of X.
We now consider the bipartite graph G′ with bipartition V (G′) = X
.
∪Z
which arises when we delete edges with both ends in X and contract the odd
connected components of G −X to single vertices (forming the set Z).
It remains to show that G′ has a perfect matching. If not, then by Frobenius’
Theorem 10.4 there is some A ⊆Z such that |G′(A)| < |A|. This implies
qG(G′(A)) ≥|A| > |G′(A)|, a contradiction.
2
This proof is due to Anderson [1971]. The Tutte condition provides a good
characterization of the perfect matching problem: either a graph has a perfect
matching or it has a so-called Tutte set X proving that it has no perfect match-
ing. An important consequence of Tutte’s Theorem is the so-called Berge-Tutte
formula:
Theorem 10.14.
(Berge [1958])
2ν(G) + max
X⊆V (G)(qG(X) −|X|) = |V (G)|.
Proof:
For any X ⊆V (G), any matching must leave at least qG(X) −|X|
vertices uncovered. Therefore 2ν(G) + qG(X) −|X| ≤|V (G)|.
To prove the reverse inequality, let
k :=
max
X⊆V (G)(qG(X) −|X|).
We construct a new graph H by adding k new vertices to G, each of which is
connected to all the old vertices.
If we can prove that H has a perfect matching, then
2ν(G) + k ≥2ν(H) −k = |V (H)| −k = |V (G)|,
and the theorem is proved.
Suppose H has no perfect matching, then by Tutte’s Theorem there is a set
Y ⊆V (H) such that qH(Y) > |Y|. By Proposition 10.11, k has the same parity
as |V (G)|, implying that |V (H)| is even. Therefore Y ̸= ∅and thus qH(Y) > 1.
But then Y contains all the new vertices, so
qG(Y ∩V (G)) = qH(Y) > |Y| = |Y ∩V (G)| + k,
contradicting the deﬁnition of k.
2
Let us close this section with a proposition for later use.

10.4 Ear-Decompositions of Factor-Critical Graphs
223
Proposition 10.15.
Let G be a graph and X ⊆V (G) with |V (G)| −2ν(G) =
qG(X) −|X|. Then any maximum matching of G contains a perfect matching in
each even connected component of G −X, a near-perfect matching in each odd
connected component of G −X, and matches all the vertices in X to vertices of
distinct odd connected components of G −X.
2
Later we shall see (Theorem 10.32) that X can be chosen such that each odd
connected component of G −X is factor-critical.
10.4 Ear-Decompositions of Factor-Critical Graphs
This section contains some results on factor-critical graphs which we shall need
later. In Exercise 17 of Chapter 2 we have seen that the graphs having an ear-
decomposition are exactly the 2-edge-connected graphs. Here we are interested in
odd ear-decompositions only.
Deﬁnition 10.16.
An ear-decomposition is called odd if every ear has odd length.
Theorem 10.17.
(Lov´asz [1972]) A graph is factor-critical if and only if it has
an odd ear-decomposition. Furthermore, the initial vertex of the ear-decomposition
can be chosen arbitrarily.
Proof:
Let G be a graph with a ﬁxed odd ear-decomposition. We prove that G
is factor-critical by induction on the number of ears. Let P be the last ear in the
odd ear-decomposition, say P goes from x to y, and let G′ be the graph before
adding P. We have to show for any vertex v ∈V (G) that G −v contains a perfect
matching. If v is not an inner vertex of P this is clear by induction (add every
second edge of P to the perfect matching in G′ −v). If v is an inner vertex of P,
then exactly one of P[v,x] and P[v,y] must be even, say P[v,x]. By induction there is
a perfect matching in G′ −x. By adding every second edge of P[y,v] and of P[v,x]
we obtain a perfect matching in G −v.
We now prove the reverse direction. Choose the initial vertex z of the ear-
decomposition arbitrarily, and let M be a near-perfect matching in G covering
V (G) \ {z}. Suppose we already have an odd ear-decomposition of a subgraph G′
of G such that z ∈V (G′) and M ∩E(G′) is a near-perfect matching in G′. If
G = G′, we are done.
If not, then – since G is connected – there must be an edge e = {x, y} ∈
E(G) \ E(G′) with x ∈V (G′). If y ∈V (G′), e is the next ear. Otherwise let N
be a near-perfect matching in G covering V (G) \ {y}. M△N obviously contains
the edges of a y-z-path P. Let w be the ﬁrst vertex of P (when traversed from y)
that belongs to V (G′). The last edge of P′ := P[y,w] cannot belong to M (because
no edge of M leaves V (G′)), and the ﬁrst edge cannot belong to N. Since P′ is
M-N-alternating, |E(P′)| must be even, so together with e it forms the next ear.
2
In fact, we have constructed a special type of odd ear-decomposition:

224
10. Maximum Matchings
Deﬁnition 10.18.
Given a factor-critical graph G and a near-perfect matching
M, an M-alternating ear-decomposition of G is an odd ear-decomposition such
that each ear is an M-alternating path or a circuit C with |E(C) ∩M| + 1 =
|E(C) \ M|.
It is clear that the initial vertex of an M-alternating ear-decomposition must
be the vertex not covered by M. The proof of Theorem 10.17 immediately yields:
Corollary 10.19.
For any factor-critical graph G and any near-perfect matching
M in G there exists an M-alternating ear-decomposition.
2
From now on, we shall only be interested in M-alternating ear-decompositions.
An interesting way to store an M-alternating ear-decomposition efﬁciently is due
to Lov´asz and Plummer [1986]:
Deﬁnition 10.20.
Let G be a factor-critical graph and M a near-perfect match-
ing in G. Let r, P1, . . . , Pk be an M-alternating ear-decomposition and µ, ϕ :
V (G) →V (G) two functions. We say that µ and ϕ are associated with the ear-
decomposition r, P1, . . . , Pk if
–
µ(x) = y if {x, y} ∈M,
–
ϕ(x) = y if {x, y} ∈E(Pi) \ M and x /∈{r} ∪V (P1) ∪· · · ∪V (Pi−1),
–
µ(r) = ϕ(r) = r.
If M is ﬁxed, we also say that ϕ is associated with r, P1, . . . , Pk.
If M is some ﬁxed near-perfect matching and µ, ϕ are associated with two
M-alternating ear-decompositions, they are the same up to the order of the ears.
Moreover, an explicit list of the ears can be obtained in linear time:
Ear-Decomposition Algorithm
Input:
A factor-critical graph G, functions µ, ϕ associated with an M-
alternating ear-decomposition.
Output:
An M-alternating ear-decomposition r, P1, . . . , Pk.
1⃝
Let initially be X := {r}, where r is the vertex with µ(r) = r.
Let k := 0, and let the stack be empty.
2⃝
If X = V (G) then go to 5⃝.
If the stack is nonempty
then let v ∈V (G) \ X be an endpoint of the topmost element of the
stack,
else choose v ∈V (G) \ X arbitrarily.

10.4 Ear-Decompositions of Factor-Critical Graphs
225
3⃝
Set x := v, y := µ(v) and P := ({x, y}, {{x, y}}).
While ϕ(ϕ(x)) = x do:
Set P := P + {x, ϕ(x)} + {ϕ(x), µ(ϕ(x))} and x := µ(ϕ(x)).
While ϕ(ϕ(y)) = y do:
Set P := P + {y, ϕ(y)} + {ϕ(y), µ(ϕ(y))} and y := µ(ϕ(y)).
Set P := P + {x, ϕ(x)} + {y, ϕ(y)}. P is the ear containing y as an inner
vertex. Put P on top of the stack.
4⃝
While both endpoints of the topmost element P of the stack are in X do:
Delete P from the stack, set k := k+1, Pk := P and X := X ∪V (P).
Go to 2⃝.
5⃝
For all {y, z} ∈E(G) \ (E(P1) ∪· · · ∪E(Pk)) do:
Set k := k + 1 and Pk := ({y, z}, {{y, z}}).
Proposition 10.21.
Let G be a factor-critical graph and µ, ϕ functions associated
with an M-alternating ear-decomposition. Then this ear-decomposition is unique
up to the order of the ears. The Ear-Decomposition Algorithm correctly deter-
mines an explicit list of these ears; it runs in linear time.
Proof:
Let D be an M-alternating ear-decomposition associated with µ and ϕ.
The uniqueness of D as well as the correctness of the algorithm follows from the
obvious fact that P as computed in 3⃝is indeed an ear of D. The running time
of 1⃝– 4⃝is evidently O(|V (G)|), while 5⃝takes O(|E(G)| time.
2
The most important property of the functions associated with an alternating
ear-decomposition is the following:
Lemma 10.22.
Let G be a factor-critical graph and µ, ϕ two functions associated
with an M-alternating ear-decomposition. Let r be the vertex not covered by M.
Then the maximal path given by an initial subsequence of
x, µ(x), ϕ(µ(x)), µ(ϕ(µ(x))), ϕ(µ(ϕ(µ(x)))), . . .
deﬁnes an M-alternating x-r-path of even length for all x ∈V (G).
Proof:
Let x ∈V (G)\{r}, and let Pi be the ﬁrst ear containing x. Clearly some
initial subsequence of
x, µ(x), ϕ(µ(x)), µ(ϕ(µ(x))), ϕ(µ(ϕ(µ(x)))), . . .
must be a subpath Q of Pi from x to y, where y ∈{r} ∪V (P1) ∪· · · ∪V (Pi−1).
Because we have an M-alternating ear-decomposition, the last edge of Q does not
belong to M; hence Q has even length. If y = r, we are done, otherwise we apply
induction on i.
2
The converse of Lemma 10.22 is not true: In the counterexample in Figure 10.2
(bold edges are matching edges, edges directed from u to v indicate ϕ(u) = v),

226
10. Maximum Matchings
Fig. 10.2.
µ and ϕ also deﬁne alternating paths to the vertex not covered by the matching.
However, µ and ϕ are not associated with any alternating ear-decomposition.
For the Weighted Matching Algorithm (Section 11.3) we shall need a fast
routine for updating an alternating ear-decomposition when the matching changes.
Although the proof of Theorem 10.17 is algorithmic (provided that we can ﬁnd a
maximum matching in a graph), this is far too inefﬁcient. We make use of the old
ear-decomposition:
Lemma 10.23.
Given a factor-critical graph G, two near-perfect matchings M
and M′, and functions µ, ϕ associated with an M-alternating ear-decomposition.
Then functions µ′, ϕ′ associated with an M′-alternating ear-decomposition can be
found in O(|V (G)|) time.
Proof:
Let v be the vertex not covered by M, and let v′ be the vertex not covered
by M′. Let P be the v′-v-path in M△M′, say P = x0, x1, . . . , xk with x0 = v′
and xk = v.
An explicit list of the ears of the old ear-decomposition can be obtained from µ
and ϕ by the Ear-Decomposition Algorithm in linear time (Proposition 10.21).
Indeed, since we do not have to consider ears of length one, we can omit 5⃝: then
the total number of edges considered is at most 3
2(|V (G)| −1) (cf. Exercise 19).
Suppose we have already constructed an M′-alternating ear-decomposition of a
spanning subgraph of G[X] for some X ⊆V (G) with v′ ∈X (initially X := {v′}).
Of course no M′-edge leaves X. Let p := max{i ∈{0, . . . , k} : xi ∈X} (illustrated
in Figure 10.3). At each stage we keep track of p and of the edge set δ(X) ∩M.
Their update when extending X is clearly possible in linear total time.
Now we show how to extend the ear-decomposition. We shall add one or more
ears in each step. The time needed for each step will be proportional to the total
number of edges in new ears.
Case 1:
|δ(X) ∩M| ≥2. Let f ∈δ(X) ∩M with xp /∈f . Evidently, f belongs
to an M-M′-alternating path which can be added as the next ear. The time needed
to ﬁnd this ear is proportional to its length.
Case 2:
|δ(X) ∩M| = 1. Then v /∈X, and e = {xp, xp+1} is the only edge in
δ(X) ∩M. Let R′ be the xp+1-v-path determined by µ and ϕ (cf. Lemma 10.22).
The ﬁrst edge of R′ is e. Let q be the minimum index i ∈{p + 2, p + 4, . . . , k}

10.4 Ear-Decompositions of Factor-Critical Graphs
227
v
v′
X
P
xp
e
xp+1
M
M′
Fig. 10.3.
with xi ∈V (R′) and V (R′
[xp+1,xi ]) ∩{xi+1, . . . , xk} = ∅(cf. Figure 10.4). Let
R := R′
[xp,xq]. So R has vertices xp, ϕ(xp), µ(ϕ(xp)), ϕ(µ(ϕ(xp))), . . . , xq, and
can be traversed in time proportional to its length.
x0 = v′
xk = v
xp
xp+1
xq
X
Fig. 10.4.
Let S := E(R) \ E(G[X]), D := (M△M′) \ (E(G[X]) ∪E(P[xq,v])), and
Z := S△D. S and D consist of M-alternating paths and circuits. Observe that
every vertex outside X has degree 0 or 2 with respect to Z. Moreover, for every
vertex outside X with two incident edges of Z, one of them belongs to M′. (Here
the choice of q is essential.)

228
10. Maximum Matchings
Hence all connected components C of (V (G), Z) with E(C) ∩δ(X) ̸= ∅can
be added as next ears, and after these ears have been added, S \ Z = S ∩(M△M′)
is the vertex-disjoint union of paths each of which can then be added as an ear.
Since e ∈D \ S ⊆Z, we have Z ∩δ(X) ̸= ∅, so at least one ear is added.
It remains to show that the time needed for the above construction is propor-
tional to the total number of edges in new ears. Obviously, it sufﬁces to ﬁnd S in
O(|E(S)|) time.
This is difﬁcult because of the subpaths of R inside X. However, we do not
really care what they look like. So we would like to shortcut these paths whenever
possible. To achieve this, we modify the ϕ-variables.
Namely, in each application of Case 2, let R[a,b] be a maximal subpath of R
inside X with a ̸= b. Let y := µ(b); y is the predecessor of b on R. We set
ϕ(x) := y for all vertices x on R[a,y] where R[x,y] has odd length. It does not
matter whether x and y are joined by an edge. See Figure 10.5 for an illustration.
X
y
x0 = v′
xp
xp+1
R
Fig. 10.5.
The time required for updating the ϕ-variables is proportional to the number
of edges examined. Note that these changes of ϕ do not destroy the property
of Lemma 10.22, and the ϕ-variables are not used anymore except for ﬁnding
M-alternating paths to v in Case 2.
Now it is guaranteed that the time required for ﬁnding the subpaths of R inside
X is proportional to the number of subpaths plus the number of edges examined
for the ﬁrst time inside X. Since the number of subpaths inside X is less than or
equal to the number of new ears in this step, we obtain an overall linear running
time.

10.5 Edmonds’ Matching Algorithm
229
Case 3:
δ(X) ∩M = ∅. Then v ∈X. We consider the ears of the (old) M-
alternating ear-decomposition in their order. Let R be the ﬁrst ear with V (R)\X ̸=
∅.
Similar to Case 2, let S := E(R) \ E(G[X]), D := (M△M′) \ E(G[X]), and
Z := S△D. Again, all connected components C of (V (G), Z) with E(C)∩δ(X) ̸=
∅can be added as next ears, and after these ears have been added, S \ Z is the
vertex-disjoint union of paths each of which can then be added as an ear. The
total time needed for Case 3 is obviously linear.
2
10.5 Edmonds’ Matching Algorithm
Recall Berge’s Theorem 10.7: A matching in a graph is maximum if and only if
there is no augmenting path. Since this holds for non-bipartite graphs as well, our
matching algorithm will again be based on augmenting paths.
However, it is not at all clear how to ﬁnd an augmenting path (or decide
that there is none). In the bipartite case (Theorem 10.5) it was sufﬁcient to mark
the vertices that are reachable from a vertex not covered by the matching via an
alternating edge progression. Since there were no odd circuits, vertices reachable
by an alternating edge progression were also reachable by an alternating path. This
is no longer the case when dealing with general graphs.
v1
v2
v3
v7
v6
v5
v4
v8
Fig. 10.6.
Consider the example in Figure 10.6 (the bold edges constitute a matching M).
When starting at v1, we have an alternating edge progression v1, v2, v3, v4, v5, v6,
v7, v5, v4, v8, but this is not a path. We have run through an odd circuit, namely
v5, v6, v7. Note that in our example there exists an augmenting path (v1, v2, v3, v7,
v6, v5, v4, v8) but it is not clear how to ﬁnd it.
The question arises what to do if we encounter an odd circuit. Surprisingly,
it sufﬁces to get rid of it by shrinking it to a single vertex. It turns out that the
smaller graph has a perfect matching if and only if the original graph has one.
This is the general idea of Edmonds’ Cardinality Matching Algorithm. We
formulate this idea in Lemma 10.25 after giving the following deﬁnition:

230
10. Maximum Matchings
Deﬁnition 10.24.
Let G be a graph and M a matching in G. A blossom in G with
respect to M is a factor-critical subgraph C of G with |M ∩E(C)| = |V (C)|−1
2
. The
vertex of C not covered by M ∩E(C) is called the base of C.
The blossom we have encountered in the above example (Figure 10.6) is
induced by {v5, v6, v7}. Note that this example contains other blossoms. Any single
vertex is also a blossom in terms of our deﬁnition. Now we can formulate the
Blossom Shrinking Lemma:
Lemma 10.25.
Let G be a graph, M a matching in G, and C a blossom in G
(with respect to M). Suppose there is an M-alternating v-r-path Q of even length
from a vertex v not covered by M to the base r of C, where E(Q) ∩E(C) = ∅.
Let G′ and M′ result from G and M by shrinking V (C) to a single vertex. Then
M is a maximum matching in G if and only if M′ is a maximum matching in G′.
Proof:
Suppose that M is not a maximum matching in G. N := M△E(Q)
is a matching of the same cardinality, so it is not maximum either. By Berge’s
Theorem 10.7 there then exists an N-augmenting path P in G. Note that N does
not cover r.
At least one of the endpoints of P, say x, does not belong to C. If P and C
are disjoint, let y be the other endpoint of P. Otherwise let y be the ﬁrst vertex
on P – when traversed from x – belonging to C. Let P′ result from P[x,y] when
shrinking V (C) in G. The endpoints of P′ are not covered by N ′ (the matching
in G′ corresponding to N). Hence P′ is an N ′-augmenting path in G′. So N ′ is
not a maximum matching in G′, and nor is M′ (which has the same cardinality).
To prove the converse, suppose that M′ is not a maximum matching in G′.
Let N ′ be a larger matching in G′. N ′ corresponds to a matching N0 in G which
covers at most one vertex of C in G. Since C is factor-critical, N0 can be extended
by k := |V (C)|−1
2
edges to a matching N in G, where
|N| = |N0| + k = |N ′| + k > |M′| + k = |M|,
proving that M is not a maximum matching in G.
2
It is necessary to require that the base of the blossom is reachable from a vertex
not covered by M by an M-alternating path of even length which is disjoint from
the blossom. For example, the blossom induced by {v4, v6, v7, v2, v3} in Figure
10.6 cannot be shrunk without destroying the only augmenting path.
When looking for an augmenting path, we shall build up an alternating forest:
Deﬁnition 10.26.
Given a graph G and a matching M in G. An alternating forest
with respect to M in G is a forest F in G with the following properties:
(a) V (F) contains all the vertices not covered by M. Each connected component
of F contains exactly one vertex not covered by M, its root.
(b) We call a vertex v ∈V (F) an outer (inner) vertex if it has even (odd) distance
to the root of the connected component containing v. (In particular, the roots
are outer vertices.) All inner vertices have degree 2 in F.

10.5 Edmonds’ Matching Algorithm
231
Fig. 10.7.
(c) For any v ∈V (F), the unique path from v to the root of the connected com-
ponent containing v is M-alternating.
Figure 10.7 shows an alternating forest. The bold edges belong to the matching.
The black vertices are inner, the white vertices outer.
Proposition 10.27.
In any alternating forest the number of outer vertices that are
not a root equals the number of inner vertices.
Proof:
Each outer vertex that is not a root has exactly one neighbour which is
an inner vertex and whose distance to the root is smaller. This is obviously a
bijection between the outer vertices that are not a root and the inner vertices.
2
Informally, Edmonds’ Cardinality Matching Algorithm works as fol-
lows. Given some matching M, we build up an M-alternating forest F. We start
with the set S of vertices not covered by M, and no edges.
At any stage of the algorithm we consider a neighbour y of an outer vertex x.
Let P(x) denote the unique path in F from x to a root. There are three interesting
cases, corresponding to three operations (“grow”, “augment”, and “shrink”):
Case 1:
y /∈V (F). Then the forest will grow when we add {x, y} and the
matching edge covering y.
Case 2:
y is an outer vertex in a different connected component of F. Then we
augment M along P(x) ∪{x, y} ∪P(y).
Case 3:
y is an outer vertex in the same connected component of F (with root
q). Let r be the ﬁrst vertex of P(x) (starting at x) also belonging to P(y). (r can

232
10. Maximum Matchings
be one of x, y.) If r is not a root, it must have degree at least 3. So r is an outer
vertex. Therefore C := P(x)[x,r] ∪{x, y} ∪P(y)[y,r] is a blossom with at least
three vertices. We shrink C.
If none of the cases applies, all the neighbours of outer vertices are inner. We
claim that M is maximum. Let X be the set of inner vertices, s := |X|, and let t
be the number of outer vertices. G −X has t odd components (each outer vertex
is isolated in G −X), so qG(X) −|X| = t −s. Hence by the trivial part of the
Berge-Tutte formula, any matching must leave at least t −s vertices uncovered.
But on the other hand, the number of vertices not covered by M, i.e. the number
of roots of F, is exactly t −s by Proposition 10.27. Hence M is indeed maximum.
Since this is not at all a trivial task, we shall spend some time on implemen-
tation details. The difﬁcult question is how to perform the shrinking efﬁciently so
that the original graph can be recovered afterwards. Of course, several shrinking
operations may involve the same vertex. Our presentation is based on the one
given by Lov´asz and Plummer [1986].
Rather than actually performing the shrinking operation, we allow our forest
to contain blossoms.
Deﬁnition 10.28.
Given a graph G and a matching M in G. A subgraph F
of G is a general blossom forest (with respect to M) if there exists a partition
V (F) = V1
.
∪V2
.
∪· · ·
.
∪Vk of the vertex set such that Fi := F[Vi] is a maximal
factor-critical subgraph of F with |M ∩E(Fi)| = |Vi|−1
2
(i = 1, . . . , k) and after
contracting each of V1, . . . , Vk we obtain an alternating forest F′.
Fi is called an outer blossom (inner blossom) if Vi is an outer (inner) vertex in
F′. All the vertices of an outer (inner) blossom are called outer (inner). A general
blossom forest where each inner blossom is a single vertex is a special blossom
forest.
Figure 10.8 shows a connected component of a special blossom forest with ﬁve
nontrivial outer blossoms. This corresponds to one of the connected components
of the alternating forest in Figure 10.7. The orientations of the edges will be
explained later. All vertices of G not belonging to the special blossom forest are
called out-of-forest.
Note that the Blossom Shrinking Lemma 10.25 applies to outer blossoms only.
However, in this section we shall deal only with special blossom forests. General
blossom forests will appear only in the Weighted Matching Algorithm in
Chapter 11.
To store a special blossom forest F, we introduce the following data structures.
For each vertex x ∈V (G) we have three variables µ(x), ϕ(x), and ρ(x) with the
following properties:
µ(x)
=
 x
if x is not covered by M
y
if {x, y} ∈M
(10.2)

10.5 Edmonds’ Matching Algorithm
233
y
x
Fig. 10.8.
ϕ(x)
=
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
x
if x /∈V (F) or x is the base of an outer blossom in F
y
for {x, y} ∈E(F) \ M if x is an inner vertex
y
for {x, y} ∈E(F) \ M according to an
M-alternating ear-decomposition of
the blossom containing x if x is an outer vertex
(10.3)
ρ(x)
=
 x
if x is not an outer vertex
y
if x is an outer vertex and y is the base of
the outer blossom in F containing x
(10.4)
For each outer vertex v we deﬁne P(v) to be the maximal path given by an initial
subsequence of
v, µ(v), ϕ(µ(v)), µ(ϕ(µ(v))), ϕ(µ(ϕ(µ(v)))), . . .
We have the following properties:
Proposition 10.29.
Let F be a special blossom forest with respect to a matching
M, and let µ, ϕ : V (G) →V (G) be functions satisfying (10.2) and (10.3). Then
we have:

234
10. Maximum Matchings
(a) For each outer vertex v, P(v) is an alternating v-q-path, where q is the root
of the tree of F containing v.
(b) A vertex x is
–
outer iff either µ(x) = x or ϕ(µ(x)) ̸= µ(x)
–
inner iff ϕ(µ(x)) = µ(x) and ϕ(x) ̸= x
–
out-of-forest iff µ(x) ̸= x and ϕ(x) = x and ϕ(µ(x)) = µ(x).
Proof:
(a): By (10.3) and Lemma 10.22, an initial subsequence of
v, µ(v), ϕ(µ(v)), µ(ϕ(µ(v))), ϕ(µ(ϕ(µ(v)))), . . .
must be an M-alternating path of even length to the base r of the blossom con-
taining v. If r is not the root of the tree containing v, then r is covered by M.
Hence the above sequence continues with the matching edge {r, µ(r)} and also
with {µ(r), ϕ(µ(r))}, because µ(r) is an inner vertex. But ϕ(µ(r)) is an outer
vertex again, and so we are done by induction.
(b): If a vertex x is outer, then it is either a root (i.e. µ(x) = x) or P(x) is a
path of length at least two, i.e. ϕ(µ(x)) ̸= µ(x).
If x is inner, then µ(x) is the base of an outer blossom, so by (10.3) ϕ(µ(x)) =
µ(x). Furthermore, P(µ(x)) is a path of length at least 2, so ϕ(x) ̸= x.
If x is out-of-forest, then by deﬁnition x is covered by M, so by (10.2) µ(x) ̸=
x. Of course µ(x) is also out-of-forest, so by (10.3) we have ϕ(x) = x and
ϕ(µ(x)) = µ(x).
Since each vertex is either outer or inner or out-of-forest, and each vertex
satisﬁes exactly one of the three right-hand side conditions, the proof is complete.
2
In Figure 10.8, an edge is oriented from u to v if ϕ(u) = v. We are now ready
for a detailed description of the algorithm.
Edmonds’ Cardinality Matching Algorithm
Input:
A graph G.
Output:
A maximum matching in G given by the edges {x, µ(x)}.
1⃝
Set µ(v) := v, ϕ(v) := v, ρ(v) := v and scanned(v) := false for all
v ∈V (G).
2⃝
If all outer vertices are scanned
then stop,
else let x be an outer vertex with scanned(x) = false.
3⃝
Let y be a neighbour of x such that y is out-of-forest or (y is outer and
ρ(y) ̸= ρ(x)).
If there is no such y then set scanned(x) := true and go to 2⃝.
4⃝
(“grow”)
If y is out-of-forest then set ϕ(y) := x and go to 3⃝.

10.5 Edmonds’ Matching Algorithm
235
5⃝
(“augment”)
If P(x) and P(y) are vertex-disjoint then
Set µ(ϕ(v)) := v, µ(v) := ϕ(v) for all v ∈V (P(x)) ∪V (P(y))
with odd distance from x or y on P(x) or P(y), respectively.
Set µ(x) := y.
Set µ(y) := x.
Set ϕ(v) := v, ρ(v) := v, scanned(v) := false for all v ∈V (G).
Go to 2⃝.
6⃝
(“shrink”)
Let r be the ﬁrst vertex on V (P(x)) ∩V (P(y)) with ρ(r) = r.
For v ∈V (P(x)[x,r]) ∪V (P(y)[y,r]) with odd distance from x or y on
P(x)[x,r] or P(y)[y,r], respectively, and ρ(ϕ(v)) ̸= r do: Set ϕ(ϕ(v)) :=
v.
If ρ(x) ̸= r then set ϕ(x) := y.
If ρ(y) ̸= r then set ϕ(y) := x.
For all v ∈V (G) with ρ(v) ∈V (P(x)[x,r])∪V (P(y)[y,r]) do: Set ρ(v) :=
r.
Go to 3⃝.
For an illustration of the effect of shrinking on the ϕ-values, see Figure 10.9,
where 6⃝of the algorithm has been applied to x and y in Figure 10.8.
Lemma 10.30.
The following statements hold at any stage of Edmonds’ Cardi-
nality Matching Algorithm:
(a) The edges {x, µ(x)} form a matching M;
(b) The edges {x, µ(x)} and {x, ϕ(x)} form a special blossom forest F with respect
to M (plus some isolated matching edges);
(c) The properties (10.2), (10.3) and (10.4) are satisﬁed with respect to F.
Proof:
(a): The only place where µ is changed is 5⃝, where the augmentation is
obviously done correctly.
(b): Since after 1⃝and 5⃝we trivially have a blossom forest without any edges
and 4⃝correctly grows the blossom forest by two edges, we only have to check
6⃝. r either is a root or must have degree at least three, so it must be outer. Let
B := V (P(x)[x,r]) ∪V (P(y)[y,r]). Consider an edge {u, v} of the blossom forest
with u ∈B and v /∈B. Since F[B] contains a near-perfect matching, {u, v} is a
matching edge only if it is {r, µ(r)}. Moreover, u has been outer before applying
6⃝. This implies that F continues to be a special blossom forest.
(c): Here the only nontrivial fact is that, after shrinking, µ and ϕ are associated
with an alternating ear-decomposition of the new blossom. So let x and y be two
outer vertices in the same connected component of the special blossom forest,
and let r be the ﬁrst vertex of V (P(x)) ∩V (P(y)) for which ρ(r) = r. The
new blossom consists of the vertices B := {v ∈V (G) : ρ(v) ∈V (P(x)[x,r]) ∪
V (P(y)[y,r])}.

236
10. Maximum Matchings
y
x
r
Fig. 10.9.
We note that ϕ(v) is not changed for any v ∈B with ρ(v) = r. So the
ear-decomposition of the old blossom B′ := {v ∈V (G) : ρ(v) = r} is the
starting point of the ear-decomposition of B. The next ear consists of P(x)[x,x′],
P(y)[y,y′], and the edge {x, y}, where x′ and y′ is the ﬁrst vertex on P(x) and P(y),
respectively, that belongs to B′. Finally, for each ear Q of an old outer blossom
B′′ ⊆B, Q \ (E(P(x)) ∪E(P(y))) is an ear of the new ear-decomposition of B.
2
Theorem 10.31.
(Edmonds [1965])
Edmonds’ Cardinality Matching Al-
gorithm correctly determines a maximum matching in O(n3) time, where n =
|V (G)|.
Proof:
Lemma 10.30 and Proposition 10.29 show that the algorithm works cor-
rectly. Consider the situation when the algorithm terminates. Let M and F be the
matching and the special blossom forest according to Lemma 10.30(a) and (b).
It is clear that any neighbour of an outer vertex x is either inner or a vertex y
belonging to the same blossom (i.e. ρ(y) = ρ(x)).

10.5 Edmonds’ Matching Algorithm
237
To show that M is a maximum matching, let X denote the set of inner vertices,
while B is the set of vertices that are the base of some outer blossom in F. Then
every unmatched vertex belongs to B, and the matched vertices of B are matched
with elements of X:
|B| = |X| + |V (G)| −2|M|.
(10.5)
On the other hand, the outer blossoms in F are odd connected components in
G −X. Therefore any matching must leave at least |B| −|X| vertices uncovered.
By (10.5), M leaves exactly |B| −|X| vertices uncovered and thus is maximum.
We now consider the running time. By Proposition 10.29(b), the status of each
vertex (inner, outer, or out-of-forest) can be checked in constant time. Each of
4⃝, 5⃝, 6⃝can be done in O(n) time. Between two augmentations, 4⃝or 6⃝are
executed at most O(n) times, since the number of ﬁxed points of ϕ decreases each
time. Moreover, between two augmentations no vertex is scanned twice. Thus the
time spent between two augmentations is O(n2), yielding an O(n3) total running
time.
2
Micali and Vazirani [1980] improved the running time to O
√n m

. They used
the results of Exercise 9, but the existence of blossoms makes the search for a
maximal set of disjoint minimum length augmenting paths more difﬁcult than in the
bipartite case (which was solved earlier by Hopcroft and Karp [1973], see Exercise
10). See also Vazirani [1994]. The currently best known time complexity for the
Cardinality Matching Problem is O

m√n
log n2
m
log n

, just as in the bipartite case.
This was obtained by Goldberg and Karzanov [2004] and by Fremuth-Paeger and
Jungnickel [2003].
With the matching algorithm we can easily prove the Gallai-Edmonds Structure
Theorem. This was ﬁrst proved by Gallai, but Edmonds’ Cardinality Matching
Algorithm turns out to be a constructive proof thereof.
W
X
Y
Fig. 10.10.

238
10. Maximum Matchings
Theorem 10.32.
(Gallai [1964])
Let G be any graph. Denote by Y the set of
vertices not covered by at least one maximum matching, by X the neighbours of Y
in V (G) \ Y, and by W all other vertices. Then:
(a) Any maximum matching in G contains a perfect matching of G[W] and near-
perfect matchings of the connected components of G[Y], and matches all ver-
tices in X to distinct connected components of G[Y];
(b) The connected components of G[Y] are factor-critical;
(c) 2ν(G) = |V (G)| −qG(X) + |X|.
We call W, X, Y the Gallai-Edmonds decomposition of G (see Figure 10.10).
Proof:
We apply Edmonds’ Cardinality Matching Algorithm and consider
the matching M and the special blossom forest F at termination. Let X′ be the
set of inner vertices, Y ′ the set of outer vertices, and W ′ the set of out-of-forest
vertices. We ﬁrst prove that X′, Y ′, W ′ satisfy (a)–(c), and then observe that X =
X′, Y = Y ′, and W = W ′.
The proof of Theorem 10.31 shows that 2ν(G) = |V (G)|−qG(X′)+|X′|. We
apply Proposition 10.15 to X′. Since the odd connected components of G −X′ are
exactly the outer blossoms in F, (a) holds for X′, Y ′, W ′. Since the outer blossoms
are factor-critical, (b) also holds.
Since part (a) holds for X′, Y ′, and W ′, we know that any maximum matching
covers all the vertices in V (G)\Y ′. In other words, Y ⊆Y ′. We claim that Y ′ ⊆Y
also holds. Let v be an outer vertex in F. Then M△E(P(v)) is a maximum
matching M′, and M′ does not cover v. So v ∈Y.
Hence Y = Y ′. This implies X = X′ and W = W ′, and the theorem is proved.
2
Exercises
1. Let G be a graph and M1, M2 two maximal matchings in G. Prove that
|M1| ≤2|M2|.
2. Let α(G) denote the size of a maximum stable set in G, and ζ(G) the minimum
cardinality of an edge cover. Prove:
(a) α(G) + τ(G) = |V (G)|
for any graph G.
(b) ν(G) + ζ(G) = |V (G)|
for any graph G with no isolated vertices.
(c) ζ(G) = α(G)
for any bipartite graph G.
(K¨onig [1933], Gallai [1959])
3. Prove that a k-regular bipartite graph has k disjoint perfect matchings. Deduce
from this that the edge set of a bipartite graph of maximum degree k can be
partitioned into k matchings.
(K¨onig [1916]); see Rizzi [1998] or Theorem 16.9.
4.
∗
A partially ordered set (or poset) is deﬁned to be a set S together with a
partial order on S, i.e. a relation R ⊆S × S that is reﬂexive ((x, x) ∈R for all
x ∈S), symmetric (if (x, y) ∈R and (y, x) ∈R then x = y), and transitive (if

Exercises
239
(x, y) ∈R and (y, z) ∈R then (x, z) ∈R). Two elements x, y ∈S are called
comparable if (x, y) ∈R or (y, x) ∈R, otherwise they are incomparable.
A chain (an antichain) is a subset of pairwise comparable (incomparable)
elements of S. Use K¨onig’s Theorem 10.2 to prove the following theorem of
Dilworth [1950]:
In a ﬁnite poset the maximum size of an antichain equals the minimum number
of chains into which the poset can be partitioned.
Hint: Take two copies v′ and v′′ of each v ∈S and consider the graph with
an edge {v′, w′′} for each (v, w) ∈R.
(Fulkerson [1956])
5. (a) Let S = {1, 2, . . . , n} and 0 ≤k < n
2. Let A and B be the collection of
all k-element and (k + 1)-element subsets of S, respectively. Construct a
bipartite graph
G = (A
.
∪B, {{a, b} : a ∈A, b ∈B, a ⊆b}).
Prove that G has a matching covering A.
(b)
∗
Prove Sperner’s Lemma: the maximum number of subsets of an n-element
set such that none is contained in any other is
 n
⌊n
2 ⌋

.
(Sperner [1928])
6. Let (U, S) be a set system. An injective function 	 : S →U such that
	(S) ∈S for all S ∈S is called a system of distinct representatives of S.
Prove:
(a) S has a system of distinct representatives if and only if the union of any
k of the sets in S has cardinality at least k.
(Hall [1935])
(b) For u ∈U let r(u) := |{S ∈S : u ∈S}|. Let n := |S| and N :=

S∈S |S| = 
u∈U r(u). Suppose |S| <
N
n−1 for S ∈S and r(u) <
N
n−1
for u ∈U. Then S has a system of distinct representatives.
(Mendelsohn and Dulmage [1958])
7. Let G be a bipartite graph with bipartition V (G) = A
.
∪B. Suppose that
S ⊆A, T ⊆B, and there is a matching covering S and a matching covering
T . Prove that then there is a matching covering S ∪T .
(Mendelsohn and Dulmage [1958])
8. Show that any graph on n vertices with minimum degree k has a matching of
cardinality min{k, ⌊n
2⌋}.
Hint: Use Berge’s Theorem 10.7.
9. Let G be a graph and M a matching in G that is not maximum.
(a) Show that there are ν(G) −|M| vertex-disjoint M-augmenting paths in
G.
Hint: Recall the proof of Berge’s Theorem 10.7.
(b) Prove that there exists an M-augmenting path of length at most ν(G)+|M|
ν(G)−|M|
in G.
(c) Let P be a shortest M-augmenting path in G, and P′ an (M△E(P))-
augmenting path. Then |E(P′)| ≥|E(P)| + |E(P ∩P′)|.

240
10. Maximum Matchings
Consider the following generic algorithm. We start with the empty matching
and in each iteration augment the matching along a shortest augmenting path.
Let P1, P2, . . . be the sequence of augmenting paths chosen. By (c), |E(Pk)| ≤
|E(Pk+1)| for all k.
(d) Show that if |E(Pi)| = |E(Pj)| for i ̸= j then Pi and Pj are vertex-
disjoint.
(e) Use (b) to prove that the sequence |E(P1)|, |E(P2)|, . . . contains at most
2√ν(G) + 2 different numbers.
(Hopcroft and Karp [1973])
10.
∗
Let G be a bipartite graph and consider the generic algorithm of Exercise 9.
(a) Prove that – given a matching M – the union of all shortest M-augmenting
paths in G can be found in O(n + m) time.
Hint: Use a kind of breadth-ﬁrst search with matching edges and non-
matching edges alternating.
(b) Consider a sequence of iterations of the algorithm where the length of
the augmenting path remains constant. Show that the time needed for the
whole sequence is no more than O(n + m).
Hint: First apply (a) and then ﬁnd the paths successively by DFS. Mark
vertices already visited.
(c) Combine (b) with Exercise 9(e) to obtain an O
√n(m + n)

-algorithm
for the Cardinality Matching Problem in bipartite graphs.
(Hopcroft and Karp [1973])
11. Let G be a bipartite graph with bipartition V (G) = A
.
∪B, A = {a1, . . . , ak},
B = {b1, . . . , bk}. For any vector x = (xe)e∈E(G) we deﬁne a matrix MG(x) =
(mx
i j)1≤i, j≤k by
mx
i j :=
 xe
if e = {ai, bj} ∈E(G)
0
otherwise
.
Its determinant det MG(x) is a polynomial in x = (xe)e∈E(G). Prove that G
has a perfect matching if and only if det MG(x) is not identically zero.
12. The permanent of a square matrix M = (mi j)1≤i, j≤n is deﬁned by
per(M) :=

π∈Sn
k'
i=1
mi,π(i),
where Sn is the set of permutations of {1, . . . , n}. Prove that a simple bipartite
graph G has exactly per(MG(1l)) perfect matchings, where MG(x) is deﬁned
as in the previous exercise.
13. A doubly stochastic matrix is a nonnegative matrix whose column sums and
row sums are all 1. Integral doubly stochastic matrices are called permutation
matrices.
Falikman [1981] and Egoryˇcev [1980] proved that for a doubly stochastic
n × n-matrix M,
per(M) ≥n!
nn ,

Exercises
241
and equality holds if and only if every entry of M is 1
n . (This was a famous
conjecture of van der Waerden; see also Schrijver [1998].)
Br`egman [1973] proved that for a 0-1-matrix M with row sums r1, . . . ,rn,
per(M) ≤(r1!)
1
r1 · . . . · (rn!)
1
rn .
Use these results and Exercise 12 to prove the following. Let G be a simple
k-regular bipartite graph on 2n vertices, and let 	(G) be the number of perfect
matchings in G. Then
n!
k
n
n
≤	(G) ≤(k!)
n
k .
14. Prove that every 3-regular graph with at most two bridges has a perfect match-
ing. Is there a 3-regular graph without a perfect matching?
Hint: Use Tutte’s Theorem 10.13.
(Petersen [1891])
15.
∗
Let G be a graph, n := |V (G)| even, and for any set X ⊆V (G) with |X| ≤3
4n
we have


x∈X
(x)
 ≥4
3|X|.
Prove that G has a perfect matching.
Hint: Let S be a set violating the Tutte condition. Prove that the num-
ber of connected components in G −S with just one element is at most
max
5
0, 4
3|S| −1
3n
6
. Consider the cases |S| ≥n
4 and |S| < n
4 separately.
(Anderson [1971])
16. Prove that an undirected graph G is factorcritical if and only if G is connected
and ν(G) = ν(G −v) for all v ∈V (G).
17. Prove that the number of ears in any two odd ear-decompositions of a factor-
critical graph G is the same.
18.
∗
For a 2-edge-connected graph G let ϕ(G) be the minimum number of even ears
in an ear-decomposition of G (cf. Exercise 17(a) of Chapter 2). Show that for
any edge e ∈E(G) we have either ϕ(G/e) = ϕ(G)+1 or ϕ(G/e) = ϕ(G)−1.
Note: The function ϕ(G) has been studied by Szigeti [1996] and Szegedy
[1999].
19. Prove that a minimal factor-critical graph G (i.e. after the deletion of any edge
the graph is no longer factor-critical) has at most 3
2(|V (G)| −1) edges. Show
that this bound is tight.
20. Show how Edmonds’ Cardinality Matching Algorithm ﬁnds a maximum
matching in the graph shown in Figure 10.1(b).
21. Given an undirected graph, can one ﬁnd an edge cover of minimum cardinality
in polynomial time?
22.
∗
Given an undirected graph G, an edge is called unmatchable if it is not con-
tained in any perfect matching. How can one determine the set of unmatchable
edges in O(n3) time?

242
10. Maximum Matchings
Hint: First determine a perfect matching in G. Then determine for each vertex
v the set of unmatchable edges incident to v.
23. Let G be a graph, M a maximum matching in G, and F1 and F2 two special
blossom forests with respect to M, each with the maximum possible number
of edges. Show that the set of inner vertices in F1 and F2 is the same.
24. Let G be a k-connected graph with 2ν(G) < |V (G)| −1. Prove:
(a) ν(G) ≥k;
(b) τ(G) ≤2ν(G) −k.
Hint: Use the Gallai-Edmonds Theorem 10.32.
(Erd˝os and Gallai [1961])
References
General Literature:
Gerards, A.M.H. [1995]: Matching. In: Handbooks in Operations Research and Management
Science; Volume 7: Network Models (M.O. Ball, T.L. Magnanti, C.L. Monma, G.L.
Nemhauser, eds.), Elsevier, Amsterdam 1995, pp. 135–224
Lawler, E.L. [1976]: Combinatorial Optimization; Networks and Matroids. Holt, Rinehart
and Winston, New York 1976, Chapters 5 and 6
Lov´asz, L., and Plummer, M.D. [1986]: Matching Theory. Akad´emiai Kiad´o, Budapest
1986, and North-Holland, Amsterdam 1986
Papadimitriou, C.H., and Steiglitz, K. [1982]: Combinatorial Optimization; Algorithms and
Complexity. Prentice-Hall, Englewood Cliffs 1982, Chapter 10
Pulleyblank, W.R. [1995]: Matchings and extensions. In: Handbook of Combinatorics; Vol.
1 (R.L. Graham, M. Gr¨otschel, L. Lov´asz, eds.), Elsevier, Amsterdam 1995
Schrijver, A. [2003]: Combinatorial Optimization: Polyhedra and Efﬁciency. Springer,
Berlin 2003, Chapters 16 and 24
Tarjan, R.E. [1983]: Data Structures and Network Algorithms. SIAM, Philadelphia 1983,
Chapter 9
Cited References:
Alt, H., Blum, N., Mehlhorn, K., and Paul, M. [1991]: Computing a maximum cardinality
matching in a bipartite graph in time O

n1.5√m/ log n

. Information Processing Letters
37 (1991), 237–240
Anderson, I. [1971]: Perfect matchings of a graph. Journal of Combinatorial Theory B 10
(1971), 183–186
Berge, C. [1957]: Two theorems in graph theory. Proceedings of the National Academy of
Science of the U.S. 43 (1957), 842–844
Berge, C. [1958]: Sur le couplage maximum d’un graphe. Comptes Rendus Hebdomadaires
des S´eances de l’Acad´emie des Sciences (Paris) S´er. I Math. 247 (1958), 258–259
Br`egman, L.M. [1973]: Certain properties of nonnegative matrices and their permanents.
Doklady Akademii Nauk SSSR 211 (1973), 27–30 [in Russian]. English translation:
Soviet Mathematics Doklady 14 (1973), 945–949
Dilworth, R.P. [1950]: A decomposition theorem for partially ordered sets. Annals of Math-
ematics 51 (1950), 161–166
Edmonds, J. [1965]: Paths, trees, and ﬂowers. Canadian Journal of Mathematics 17 (1965),
449–467
Egoryˇcev, G.P. [1980]: Solution of the van der Waerden problem for permanents. Soviet
Mathematics Doklady 23 (1982), 619–622

References
243
Erd˝os, P., and Gallai, T. [1961]: On the minimal number of vertices representing the edges of
a graph. Magyar Tudom´anyos Akad´emia; Matematikai Kutat´o Int´ezet´enek K¨ozlem´enyei
6 (1961), 181–203
Falikman, D.I. [1981]: A proof of the van der Waerden conjecture on the permanent of
a doubly stochastic matrix. Matematicheskie Zametki 29 (1981), 931–938 [in Russian].
English translation: Math. Notes of the Acad. Sci. USSR 29 (1981), 475–479
Feder, T., and Motwani, R. [1995]: Clique partitions, graph compression and speeding-up
algorithms. Journal of Computer and System Sciences 51 (1995), 261–272
Fremuth-Paeger, C., and Jungnickel, D. [2003]: Balanced network ﬂows VIII: a revised
theory of phase-ordered algorithms and the O(√nm log(n2/m)/ log n) bound for the
nonbipartite cardinality matching problem. Networks 41 (2003), 137–142
Frobenius, G. [1917]:
¨Uber zerlegbare Determinanten. Sitzungsbericht der K¨oniglich
Preussischen Akademie der Wissenschaften XVIII (1917), 274–277
Fulkerson, D.R. [1956]: Note on Dilworth’s decomposition theorem for partially ordered
sets. Proceedings of the AMS 7 (1956), 701–702
Gallai, T. [1959]: ¨Uber extreme Punkt- und Kantenmengen. Annales Universitatis Scien-
tiarum Budapestinensis de Rolando E¨otv¨os Nominatae; Sectio Mathematica 2 (1959),
133–138
Gallai, T. [1964]: Maximale Systeme unabh¨angiger Kanten. Magyar Tudom´anyos Akad´emia;
Matematikai Kutat´o Int´ezet´enek K¨ozlem´enyei 9 (1964), 401–413
Geelen, J.F. [2000]: An algebraic matching algorithm. Combinatorica 20 (2000), 61–70
Geelen, J. and Iwata, S. [2005]: Matroid matching via mixed skew-symmetric matrices.
Combinatorica 25 (2005), 187–215
Goldberg, A.V., and Karzanov, A.V. [2004]: Maximum skew-symmetric ﬂows and match-
ings. Mathematical Programming A 100 (2004), 537–568
Hall, P. [1935]: On representatives of subsets. Journal of the London Mathematical Society
10 (1935), 26–30
Halmos, P.R., and Vaughan, H.E. [1950]: The marriage problem. American Journal of
Mathematics 72 (1950), 214–215
Hopcroft, J.E., and Karp, R.M. [1973]: An n5/2 algorithm for maximum matchings in
bipartite graphs. SIAM Journal on Computing 2 (1973), 225–231
K¨onig, D. [1916]: ¨Uber Graphen und ihre Anwendung auf Determinantentheorie und Men-
genlehre. Mathematische Annalen 77 (1916), 453–465
K¨onig, D. [1931]: Graphs and matrices. Matematikai´es Fizikai Lapok 38 (1931), 116–119
[in Hungarian]
K¨onig, D. [1933]: ¨Uber trennende Knotenpunkte in Graphen (nebst Anwendungen auf De-
terminanten und Matrizen). Acta Litteratum ac Scientiarum Regiae Universitatis Hun-
garicae Francisco-Josephinae (Szeged). Sectio Scientiarum Mathematicarum 6 (1933),
155–179
Kuhn, H.W. [1955]: The Hungarian method for the assignment problem. Naval Research
Logistics Quarterly 2 (1955), 83–97
Lov´asz, L. [1972]: A note on factor-critical graphs. Studia Scientiarum Mathematicarum
Hungarica 7 (1972), 279–280
Lov´asz, L. [1979]: On determinants, matchings and random algorithms. In: Fundamentals
of Computation Theory (L. Budach, ed.), Akademie-Verlag, Berlin 1979, pp. 565–574
Mendelsohn, N.S., and Dulmage, A.L. [1958]: Some generalizations of the problem of
distinct representatives. Canadian Journal of Mathematics 10 (1958), 230–241
Micali, S., and Vazirani, V.V. [1980]: An O(V 1/2E) algorithm for ﬁnding maximum match-
ing in general graphs. Proceedings of the 21st Annual IEEE Symposium on Foundations
of Computer Science (1980), 17–27
Mucha, M., and Sankowski, P. [2004]: Maximum matchings via Gaussian elimination.
Proceedings of the 45th Annual IEEE Symposium on Foundations of Computer Science
(2004), 248–255

244
10. Maximum Matchings
Mulmuley, K., Vazirani, U.V., and Vazirani, V.V. [1987]: Matching is as easy as matrix
inversion. Combinatorica 7 (1987), 105–113
Petersen, J. [1891]: Die Theorie der regul¨aren Graphen. Acta Mathematica 15 (1891), 193–
220
Rabin, M.O., and Vazirani, V.V. [1989]: Maximum matchings in general graphs through
randomization. Journal of Algorithms 10 (1989), 557–567
Rizzi, R. [1998]: K¨onig’s edge coloring theorem without augmenting paths. Journal of
Graph Theory 29 (1998), 87
Schrijver, A. [1998]: Counting 1-factors in regular bipartite graphs. Journal of Combinatorial
Theory B 72 (1998), 122–135
Sperner, E. [1928]: Ein Satz ¨uber Untermengen einer Endlichen Menge. Mathematische
Zeitschrift 27 (1928), 544–548
Szegedy, C. [1999]: A linear representation of the ear-matroid. Report No. 99878, Research
Institute for Discrete Mathematics, University of Bonn, 1999; accepted for publication
in Combinatorica
Szigeti, Z. [1996]: On a matroid deﬁned by ear-decompositions. Combinatorica 16 (1996),
233–241
Tutte, W.T. [1947]: The factorization of linear graphs. Journal of the London Mathematical
Society 22 (1947), 107–111
Vazirani, V.V. [1994]: A theory of alternating paths and blossoms for proving correctness
of the O(
√
V E) general graph maximum matching algorithm. Combinatorica 14 (1994),
71–109

11. Weighted Matching
Nonbipartite weighted matching appears to be one of the “hardest” combinatorial
optimization problems that can be solved in polynomial time. We shall extend
Edmonds’ Cardinality Matching Algorithm to the weighted case and shall
again obtain an O(n3)-implementation. This algorithm has many applications,
some of which are mentioned in the exercises and in Section 12.2. There are two
basic formulations of the weighted matching problem:
Maximum Weight Matching Problem
Instance:
An undirected graph G and weights c : E(G) →R.
Task:
Find a maximum weight matching in G.
Minimum Weight Perfect Matching Problem
Instance:
An undirected graph G and weights c : E(G) →R.
Task:
Find a minimum weight perfect matching in G or decide that G has
no perfect matching.
It is easy to see that both problems are equivalent: Given an instance (G, c)
of the Minimum Weight Perfect Matching Problem, we set c′(e) := K −c(e)
for all e ∈E(G), where K := 1 + 
e∈E(G) |c(e)|. Then any maximum weight
matching in (G, c′) is a maximum cardinality matching, and hence gives a solution
of the Minimum Weight Perfect Matching Problem (G, c).
Conversely, let (G, c) be an instance of the Maximum Weight Matching
Problem. Then we add |V (G)| new vertices and all possible edges in order to
obtain a complete graph G′ on 2|V (G)| vertices. We set c′(e) := −c(e) for all
e ∈E(G) and c′(e) := 0 for all new edges e. Then a minimum weight perfect
matching in (G′, c′) yields a maximum weight matching in (G, c), simply by
deleting the edges not belonging to G.
So in the following we consider only the Minimum Weight Perfect Match-
ing Problem. As in the previous chapter, we start by considering bipartite graphs
in Section 11.1. After an outline of the weighted matching algorithm in Section
11.2 we spend some effort on implementation details in Section 11.3 in order
to obtain an O(n3) running time. Sometimes one is interested in solving many
matching problems that differ only on a few edges; in such a case it is not nec-

246
11. Weighted Matching
essary to solve the problem from scratch each time as is shown in Section 11.4.
Finally, in Section 11.5 we discuss the matching polytope, i.e. the convex hull
of the incidence vectors of matchings. We use a description of the related perfect
matching polytope already for designing the weighted matching algorithm; in turn,
this algorithm will directly imply that this description is complete.
11.1 The Assignment Problem
The Assignment Problem is just another name for the Minimum Weight Per-
fect Matching Problem in bipartite graphs.
As in the proof of Theorem 10.5, we can reduce the assignment problem to a
network ﬂow problem:
Theorem 11.1.
The Assignment Problem can be solved in O(nm + n2 log n)
time.
Proof:
Let G be a bipartite graph with bipartition V (G) = A
.
∪B. We assume
|A| = |B| = n. Add a vertex s and connect it to all vertices of A, and add another
vertex t connected to all vertices of B. Orient the edges from s to A, from A to
B, and from B to t. Let the capacities be 1 everywhere, and let the new edges
have zero cost.
Then any integral s-t-ﬂow of value n corresponds to a perfect matching with
the same cost, and vice versa. So we have to solve a Minimum Cost Flow
Problem. We do this by applying the Successive Shortest Path Algorithm
(see Section 9.4). The total demand is n. So by Theorem 9.12, the running time
is O(nm + n2 log n).
2
This is the fastest known algorithm. It is essentially equivalent to the “Hun-
garian method” by Kuhn [1955] and Munkres [1957], the oldest polynomial-time
algorithm for the Assignment Problem.
It is worthwhile looking at the linear programming formulation of the Assign-
ment Problem. It turns out that in the integer programming formulation
min
⎧
⎨
⎩

e∈E(G)
c(e)xe : xe ∈{0, 1} (e ∈E(G)),

e∈δ(v)
xe = 1 (v ∈V (G))
⎫
⎬
⎭
the integrality constraints can be omitted (replace xe ∈{0, 1} by xe ≥0):
Theorem 11.2.
Let G be a graph, and let
P
:=
⎧
⎨
⎩x ∈RE(G)
+
:

e∈δ(v)
xe ≤1 for all v ∈V (G)
⎫
⎬
⎭and
Q
:=
⎧
⎨
⎩x ∈RE(G)
+
:

e∈δ(v)
xe = 1 for all v ∈V (G)
⎫
⎬
⎭

11.2 Outline of the Weighted Matching Algorithm
247
be the fractional matching polytope and the fractional perfect matching poly-
tope of G. If G is bipartite, then P and Q are both integral.
Proof:
If G is bipartite, then the incidence matrix M of G is totally unimodular
due to Theorem 5.24. Hence by the Hoffman-Kruskal Theorem 5.19, P is integral.
Q is a face of P and thus it is also integral.
2
There is a nice corollary concerning doubly-stochastic matrices. A doubly
stochastic matrix is a nonnegative square matrix such that the sum of the entries
in each row and each column is 1. Integral doubly stochastic matrices are called
permutation matrices.
Corollary 11.3.
(Birkhoff [1946], von Neumann [1953])
Any doubly stochas-
tic matrix M can be written as a convex combination of permutation matri-
ces P1, . . . , Pk (i.e. M = c1P1 + . . . + ck Pk for nonnegative c1, . . . , ck with
c1 + . . . + ck = 1).
Proof:
Let M = (mi j)i, j∈{1,...,n} be a doubly stochastic n ×n-matrix, and let Kn,n
be the complete bipartite graph with colour classes {a1, . . . , an} and {b1, . . . , bn}.
For e = {ai, bj} ∈E(Kn,n) let xe = mi j. Since M is doubly stochastic, x is in the
fractional perfect matching polytope Q of Kn,n. By Theorem 11.2 and Corollary
3.27, x can be written as a convex combination of integral vertices of Q. These
obviously correspond to permutation matrices.
2
This corollary can also be proved directly (Exercise 3).
11.2 Outline of the Weighted Matching Algorithm
The purpose of this and the next section is to describe a polynomial-time algorithm
for the general Minimum Weight Perfect Matching Problem. This algorithm
was also developped by Edmonds [1965] and uses the concepts of his algorithm
for the Cardinality Matching Problem (Section 10.5).
Let us ﬁrst outline the main ideas without considering the implementation.
Given a graph G with weights c : E(G) →R, the Minimum Weight Perfect
Matching Problem can be formulated as the integer linear program
min
⎧
⎨
⎩

e∈E(G)
c(e)xe : xe ∈{0, 1} (e ∈E(G)),

e∈δ(v)
xe = 1 (v ∈V (G))
⎫
⎬
⎭.
If A is a subset of V (G) with odd cardinality, any perfect matching must
contain an odd number of edges in δ(A), in particular at least one. So adding the
constraint

e∈δ(A)
xe ≥1
does not change anything. Throughout this chapter we use the notation A := {A ⊆
V (G) : |A| odd}. Now consider the LP relaxation:

248
11. Weighted Matching
min

e∈E(G)
c(e)xe
s.t.
xe
≥
0
(e ∈E(G))

e∈δ(v)
xe
=
1
(v ∈V (G))

e∈δ(A)
xe
≥
1
(A ∈A, |A| > 1)
(11.1)
We shall prove later that the polytope described by (11.1) is integral; hence
this LP describes the Minimum Weight Perfect Matching Problem (this will
be Theorem 11.13, a major result of this chapter). In the following we do not need
this fact, but will rather use the LP formulation as a motivation.
To formulate the dual of (11.1), we introduce a variable z A for each primal
constraint, i.e. for each A ∈A. The dual linear program is:
max

A∈A
z A
s.t.
z A
≥
0
(A ∈A, |A| > 1)

A∈A:e∈δ(A)
z A
≤
c(e)
(e ∈E(G))
(11.2)
Note that the dual variables z{v} for v ∈V (G) are not restricted to be non-
negative. Edmonds’ algorithm is a primal-dual algorithm. It starts with the empty
matching (xe = 0 for all e ∈E(G)) and the feasible dual solution
z A :=
 1
2 min{c(e) : e ∈δ(A)}
if |A| = 1
0
otherwise
.
At any stage of the algorithm, z will be a feasible dual solution, and we have
xe > 0
⇒

A∈A:e∈δ(A)
z A = c(e);
z A > 0
⇒

e∈δ(A)
xe ≤1.
(11.3)
The algorithm stops when x is the incidence vector of a perfect matching (i.e.
we have primal feasibility). Due to the complementary slackness conditions (11.3)
(Corollary 3.18) we then have the optimality of the primal and dual solutions. As
x is optimal for (11.1) and integral, it is the incidence vector of a minimum weight
perfect matching.
Given a feasible dual solution z, we call an edge e tight if the corresponding
dual constraint is satisﬁed with equality, i.e. if

A∈A:e∈δ(A)
z A = c(e).
At any stage, the current matching will consist of tight edges only.

11.2 Outline of the Weighted Matching Algorithm
249
We work with a graph Gz which results from G by deleting all edges that are
not tight and contracting each set B with zB > 0 to a single vertex. The family
B := {B ∈A : zB > 0} will be laminar at any stage, and each element of B will
induce a factor-critical subgraph consisting of tight edges only. Initially B consists
of the singletons.
One iteration of the algorithm roughly proceeds as follows. We ﬁrst ﬁnd a
maximum cardinality matching M in Gz, using Edmonds’ Cardinality Match-
ing Algorithm. If M is a perfect matching, we are done: we can complete M
to a perfect matching in G using tight edges only. Since the conditions (11.3) are
satisﬁed, the matching is optimal.
W
X
Y
+ε
+ε
+ε
+ε
−ε
−ε
−ε
Fig. 11.1.
Otherwise we consider the Gallai-Edmonds decomposition W, X, Y of Gz (cf.
Theorem 10.32). For each vertex v of Gz let B(v) ∈B be the vertex set whose
contraction resulted in v. We modify the dual solution as follows (see Figure
11.1 for an illustration). For each v ∈X we decrease zB(v) by some positive
constant ε. For each connected component C of Gz[Y] we increase z A by ε,
where A = 
v∈C B(v).
Note that tight matching edges remain tight, since by Theorem 10.32 all match-
ing edges with one endpoint in X have the other endpoint in Y. (Indeed, all edges
of the alternating forest we are working with remain tight).
We choose ε maximum possible while preserving dual feasibility. Since the
current graph contains no perfect matching, the number of connected components
of Gz[Y] is greater than |X|. Hence the above dual change increases the dual
objective function value 
A∈A z A by at least ε. If ε can be chosen arbitrarily
large, the dual LP (11.2) is unbounded, hence the primal LP (11.1) is infeasible
(Theorem 3.22) and G has no perfect matching.
Due to the change of the dual solution the graph Gz will also change: new
edges may become tight, new vertex sets may be contracted (corresponding to
the components of Y that are not singletons), and some contracted sets may be

250
11. Weighted Matching
“unpacked” (non-singletons whose dual variables become zero, corresponding to
vertices of X).
The above is iterated until a perfect matching is found. We shall show later
that this procedure is ﬁnite. This will follow from the fact that between two
augmentations, each step (grow, shrink, unpack) increases the number of outer
vertices.
11.3 Implementation of the Weighted Matching Algorithm
After this informal description we now turn to the implementation details. As
with Edmonds’ Cardinality Matching Algorithm we do not explicitly shrink
blossoms but rather store their ear-decomposition. However, there are several dif-
ﬁculties.
The “shrink”-step of Edmonds’ Cardinality Matching Algorithm pro-
duces an outer blossom. By the “augment”-step two connected components of the
blossom forest become out-of-forest. Since the dual solution remains unchanged,
we must retain the blossoms: we get so-called out-of-forest blossoms. The “grow”-
step may involve out-of-forest blossoms which then become either inner or outer
blossoms. Hence we have to deal with general blossom forests.
Another problem is that we must be able to recover nested blossoms one by
one. Namely, if z A becomes zero for some inner blossom A, there may be subsets
A′ ⊆A with |A′| > 1 and z A′ > 0. Then we have to unpack the blossom A,
but not the smaller blossoms inside A (except if they remain inner and their dual
variables are also zero).
Throughout the algorithm we have a laminar family B ⊆A, containing at least
all singletons. All elements of B are blossoms. We have z A = 0 for all A /∈B. The
set B is laminar and is stored by a tree-representation (cf. Proposition 2.14). For
easy reference, a number is assigned to each blossom in B that is not a singleton.
We store ear-decompositions of all blossoms in B at any stage of the algorithm.
The variables µ(x) for x ∈V (G) again encode the current matching M. We denote
by b1(x), . . . , bkx(x) the blossoms in B containing x, without the singleton. bkx(x)
is the outermost blossom. We have variables ρi(x) and ϕi(x) for each x ∈V (G)
and i = 1, . . . , kx. ρi(x) is the base of the blossom bi(x). µ(x) and ϕ j(x), for all
x and j with b j(x) = i, are associated with an M-alternating ear-decomposition
of blossom i.
Of course, we must update the blossom structures (ϕ and ρ) after each aug-
mentation. Updating ρ is easy. Updating ϕ can also be done in linear time by
Lemma 10.23.
For inner blossoms we need, in addition to the base, the vertex nearest to the
root of the tree in the general blossom forest, and the neighbour in the next outer
blossom. These two vertices are denoted by σ(x) and χ(σ(x)) for each base x of
an inner blossom. See Figure 11.2 for an illustration.
With these variables, the alternating paths to the root of the tree can be de-
termined. Since the blossoms are retained after an augmentation, we must choose

11.3 Implementation of the Weighted Matching Algorithm
251
x0
x1 = µ(x0)
x2
x3
x4
x5 = σ(x1)
x6 = χ(x5)
y0
y1
y2 = ρ(y0)
y3 = µ(y2)
y4
y5 = σ(y3)
y6 = χ(y5)
Fig. 11.2.
the augmenting path such that each blossom still contains a near-perfect matching
afterwards.
Figure 11.2 shows that we must be careful: There are two nested inner blos-
soms, induced by {x3, x4, x5} and {x1, x2, x3, x4, x5}. If we just consider the ear-
decomposition of the outermost blossom to ﬁnd an alternating path from x0 to the
root x6, we will end up with (x0, x1, x4, x5 = σ(x1), x6 = χ(x5)). After augment-
ing along (y6, y5, y4, y3, y2, y1, y0, x0, x1, x4, x5, x6), the factor-critical subgraph
induced by {x3, x4, x5} no longer contains a near-perfect matching.
Thus we must ﬁnd an alternating path within each blossom which contains
an even number of edges within each sub-blossom. This is accomplished by the
following procedure:
BlossomPath
Input:
A vertex x0.
Output:
An M-alternating path Q(x0) from x0 to ρkx0(x0).
1⃝
Set h := 0 and B := {b j(x0) : j = 1, . . . , kx0}.
2⃝
While x2h ̸= ρkx0(x0) do:
Set x2h+1 := µ(x2h) and x2h+2 := ϕi(x2h+1), where
i = min
5
j ∈{1, . . . , kx2h+1} : b j(x2h+1) ∈B
6
.
Add all blossoms of B to B that contain x2h+2 but not x2h+1.
Delete all blossoms from B whose base is x2h+2.
Set h := h + 1.
3⃝
Let Q(x0) be the path with vertices x0, x1, . . . , x2h.
Proposition 11.4.
The procedure BlossomPath can be implemented in O(n)
time. M△E(Q(x0)) contains a near-perfect matching within each blossom.

252
11. Weighted Matching
Proof:
Let us ﬁrst check that the procedure indeed computes a path. In fact, if
a blossom of B is left, it is never entered again. This follows from the fact that
contracting the maximal sub-blossoms of any blossom in B results in a circuit (a
property which will be maintained).
At the beginning of each iteration, B is the list of all blossoms that either
contain x0 or have been entered via a non-matching edge and have not been left
yet. The constructed path leaves any blossom in B via a matching edge. So the
number of edges within each blossom is even, proving the second statement of
the proposition.
When implementing the procedure in O(n) time, the only nontrivial task is
the update of B. We store B as a sorted list. Using the tree-representation of B
and the fact that each blossom is entered and left at most once, we get a running
time of O(n + |B|). Note that |B| = O(n), because B is laminar.
2
Now determining an augmenting path consists of applying the procedure Blos-
somPath within blossoms, and using µ and χ between blossoms. When we ﬁnd
adjacent outer vertices x, y in different trees of the general blossom forest, we ap-
ply the following procedure to both x and y. The union of the two paths together
with the edge {x, y} will be the augmenting path.
TreePath
Input:
An outer vertex v.
Output:
An alternating path P(v) from v to the root of the tree in the blossom
forest.
1⃝
Let initially P(v) consist of v only. Let x := v.
2⃝
Let y := ρkx(x). Let Q(x) := BlossomPath(x). Append Q(x) to P(v).
If µ(y) = y then stop.
3⃝
Set P(v) := P(v) + {y, µ(y)}.
Let Q(σ(µ(y))) := BlossomPath(σ(µ(y))).
Append the reverse of Q(σ(µ(y))) to P(v).
Let P(v) := P(v) + {σ(µ(y)), χ(σ(µ(y)))}.
Set x := χ(σ(µ(y))) and go to 2⃝.
The second main problem is how to determine ε efﬁciently. The general blos-
som forest, after all possible grow-, shrink- and augment-steps are done, yields
the Gallai-Edmonds decomposition W, X, Y of Gz. W contains the out-of-forest
blossoms, X contains the inner blossoms, and Y consists of the outer blossoms.
For a simpler notation, let us deﬁne c({v, w}) := ∞if {v, w} /∈E(G). More-
over, we use the abbreviation
slack(v, w) := c({v, w}) −

A∈A, {v,w}∈δ(A)
z A.
So {v, w} is a tight edge if and only if slack(v, w) = 0. Then let

11.3 Implementation of the Weighted Matching Algorithm
253
ε1
:=
min{z A : A is a maximal inner blossom, |A| > 1};
ε2
:=
min {slack(x, y) : x outer, y out-of-forest} ;
ε3
:=
1
2 min {slack(x, y) : x, y outer, belonging to different blossoms} ;
ε
:=
min{ε1, ε2, ε3}.
This ε is the maximum number such that the dual change by ε preserves dual
feasibility. If ε = ∞, (11.2) is unbounded and so (11.1) is infeasible. In this case
G has no perfect matching.
Obviously, ε can be computed in ﬁnite time. However, in order to obtain an
O(n3) overall running time we must be able to compute ε in O(n) time. This is
easy as far as ε1 is concerned, but requires additional data structures for ε2 and
ε3.
For A ∈B let
ζA :=

B∈B:A⊆B
zB.
We shall update these values whenever changing the dual solution; this can easily
be done in linear time (using the tree-representation of B). Then
ε2
=
min
5
c({x, y}) −ζ{x} −ζ{y} : x outer, y out-of-forest
6
,
ε3
=
1
2 min
5
c({x, y}) −ζ{x} −ζ{y} : x, y outer, belonging to different
blossoms
6
.
To compute ε2, we store for each out-of-forest vertex v the outer neighbour w
for which slack(v, w) = c({v, w})−ζ{v}−ζ{w} is minimum. We call this neighbour
τv. These variables are updated whenever necessary. Then it is easy to compute
ε2 = min{c({v, τv}) −ζ{v} −ζ{τv} : v out-of-forest}.
To compute ε3, we introduce variables t A
v and τ A
v for each outer vertex v and
each A ∈B, unless A is outer but not maximal. τ A
v is the vertex in A minimizing
slack(v, τ A
v ), and t A
v = slack(v, τ A
v ) +  + ζA, where  denotes the sum of the
ε-values in all dual changes.
Although when computing ε3 we are interested only in the values t A
v for maxi-
mal outer blossoms of B, we update these variables also for inner and out-of-forest
blossoms (even those that are not maximal), because they may become maximal
outer later. Blossoms that are outer but not maximal will not become maximal
outer before an augmentation takes place. After each augmentation, however, all
these variables are recomputed.
The variable t A
v has the value slack(v, τ A
v )++ζA at any time. Observe that
this value does not change as long as v remains outer, A ∈B, and τ A
v is the vertex
in A minimizing slack(v, τ A
v ). Finally, we write t A := min{t A
v : v /∈A, v outer}.
We conclude that
ε3 = 1
2slack(v, τ A
v ) = 1
2(t A
v − −ζA) = 1
2(t A − −ζA),

254
11. Weighted Matching
where A is a maximal outer element of B for which t A −ζA is minimum, and v
is some outer vertex with v /∈A and t A
v = t A.
At certain stages we have to update τ A
v and t A
v for a certain vertex v and
all A ∈B (except those that are outer but not maximal), for example if a new
vertex becomes outer. The following procedure also updates the variables τw for
out-of-forest vertices w if necessary.
Update
Input:
An outer vertex v.
Output:
Updated values of τ A
v , t A
v and t A for all A ∈B and τw for all out-of-
forest vertices w.
1⃝
For each neighbour w of v that is out-of-forest do:
If c({v, w}) −ζ{v} < c({w, τw}) −ζ{τw} then set τw := v.
2⃝
For each x ∈V (G) do: Set τ {x}
v
:= x and t{x}
v
:= c({v, x}) −ζ{v} + .
3⃝
For A ∈B with |A| > 1 do:
Set inductively τ A
v := τ A′
v
and t A
v := t A′
v −ζA′ +ζA, where A′ is the one
among the maximal proper subsets of A in B for which t A′
v −ζA′ is
minimum.
4⃝
For A ∈B with v /∈A, except those that are outer but not maximal,
do: Set t A := min{t A, t A
v }.
Obviously this computation coincides with the above deﬁnition of τ A
v and t A
v .
It is important that this procedure runs in linear time:
Lemma 11.5.
If B is laminar, the procedure Update can be implemented with
O(n) time.
Proof:
By Proposition 2.15, a laminar family of subsets of V (G) has cardinality
at most 2|V (G)| = O(n). If B is stored by its tree-representation, then a linear-
time implementation is easy.
2
We can now go ahead with the formal description of the algorithm. Instead of
identifying inner and outer vertices by the µ-, φ- and ρ-values, we directly mark
each vertex with its status (inner, outer or out-of-forest).
Weighted Matching Algorithm
Input:
A graph G, weights c : E(G) →R.
Output:
A minimum weight perfect matching in G, given by the edges
{x, µ(x)}, or the answer that G has no perfect matching.
1⃝
Set B := {{v} : v ∈V (G)} and K := 0. Set  := 0.
Set z{v} := 1
2 min{c(e) : e ∈δ(v)} and ζ{v} := z{v} for all v ∈V (G).
Set kv := 0, µ(v) := v, ρ0(v) := v, and ϕ0(v) := v for all v ∈V (G).
Mark all vertices as outer.

11.3 Implementation of the Weighted Matching Algorithm
255
2⃝
For all v ∈V (G) do: Set scanned(v) := false.
For each out-of-forest vertex v do: Let τv be an arbitrary outer vertex.
Set t A := ∞for all A ∈B.
For all outer vertices v do: Update(v).
3⃝
If all outer vertices are scanned
then go to 8⃝,
else let x be an outer vertex with scanned(x) = false.
4⃝
Let y be a neighbour of x such that {x, y} is tight and either y is
out-of-forest or (y is outer and ρky(y) ̸= ρkx(x)). If there is no such y
then set scanned(x) := true and go to 3⃝.
5⃝
If y is not out-of-forest then go to 6⃝, else:
(“grow”)
Set σ(ρky(y)) := y and χ(y) := x.
Mark all vertices v with ρkv(v) = ρky(y) as inner.
Mark all vertices v with µ(ρkv(v)) = ρky(y) as outer.
For each new outer vertex v do: Update(v).
Go to 4⃝.
6⃝
Let P(x) := TreePath(x) be given by (x = x0, x1, x2, . . . , x2h).
Let P(y) := TreePath(y) be given by (y = y0, y1, y2, . . . , y2 j).
If P(x) and P(y) are not vertex-disjoint then go to 7⃝, else:
(“augment”)
For i := 0 to h −1 do: Set µ(x2i+1) := x2i+2 and µ(x2i+2) := x2i+1.
For i := 0 to j −1 do: Set µ(y2i+1) := y2i+2 and µ(y2i+2) := y2i+1.
Set µ(x) := y and µ(y) := x.
Mark all vertices v such that the endpoint of TreePath(v) is either x2h
or y2 j as out-of-forest.
Update all values ϕi(v) and ρi(v) for these vertices (using Lemma 10.23).
If µ(v) ̸= v for all v then stop, else go to 2⃝.
7⃝
(“shrink”)
Let r = x2h′ = y2 j′ be the ﬁrst outer vertex of V (P(x)) ∩V (P(y)) with
ρkr (r) = r.
Let A := {v ∈V (G) : ρkv(v) ∈V (P(x)[x,r]) ∪V (P(y)[y,r])}.
Set K := K + 1, B := B ∪{A}, z A := 0 and ζA := 0.
For all v ∈A do:
Set kv := kv + 1, bkv(v) := K, ρkv(v) := r, ϕkv(v) := ϕkv−1(v) and
mark v as outer.
For i := 1 to h′ do:
If ρkx2i (x2i) ̸= r then set ϕkx2i (x2i) := x2i−1.
If ρkx2i−1(x2i−1) ̸= r then set ϕkx2i−1(x2i−1) := x2i.
For i := 1 to j′ do:
If ρky2i (y2i) ̸= r then set ϕky2i (y2i) := y2i−1.
If ρky2i−1(y2i−1) ̸= r then set ϕky2i−1(y2i−1) := y2i.

256
11. Weighted Matching
If ρkx(x) ̸= r then set ϕkx(x) := y.
If ρky(y) ̸= r then set ϕky(y) := x.
For each outer vertex v do: Set t A
v := t A′
v −ζA′ and τ A
v := τ A′
v , where A′
is the one among the maximal proper subsets of A in B for which
t A′
v −ζA′ is minimum.
Set t A := min{t A
v : v outer, there is no ¯A ∈B with A ∪{v} ⊆¯A}.
For each new outer vertex v do: Update(v).
Go to 4⃝.
8⃝
(“dual change”)
Set ε1 := min{z A : A maximal inner element of B, |A| > 1}.
Set ε2 := min{c({v, τv} −ζ{v} −ζ{τv} : v out-of-forest}.
Set ε3 := min{ 1
2(t A − −ζA) : A maximal outer element of B}.
Set ε := min{ε1, ε2, ε3}. If ε = ∞, then stop (G has no perfect matching).
If ε = ε2 = c({v, τv}−ζ{v} −ζ{τv}), v outer then set scanned(τv) := false.
If ε = ε3 = 1
2(t A
v − −ζA), A maximal outer element of B, v outer
and v /∈A then set scanned(v) := false.
For each maximal outer element A of B do:
Set z A := z A + ε and ζA′ := ζA′ + ε for all A′ ∈B with A′ ⊆A.
For each maximal inner element A of B do:
Set z A := z A −ε and ζA′ := ζA′ −ε for all A′ ∈B with A′ ⊆A.
Set  :=  + ε.
9⃝
While there is a maximal inner A ∈B with z A = 0 and |A| > 1 do:
(“unpack”)
Set B := B \ {A}.
Let y := σ(ρkv(v)) for some v ∈A.
Let Q(y) := BlossomPath(y) be given by
(y = r0,r1,r2, . . . ,r2l−1,r2l = ρky(y)).
Mark all v ∈A with ρkv−1(v) /∈V (Q(y)) as out-of-forest.
Mark all v ∈A with ρkv−1(v) = r2i−1 for some i as outer.
For all v ∈A with ρkv−1(v) = r2i for some i (v remains inner) do:
Set σ(ρkv(v)) := rj and χ(rj) := rj−1, where
j := min{ j′ ∈{0, . . . , 2l} : ρkrj′ −1(rj′) = ρkv−1(v)}.
For all v ∈A do: Set kv := kv −1.
For each new out-of-forest vertex v do: Let τv be the outer vertex w
for which c({v, w}) −ζ{v} −ζ{w} is minimum.
For each new outer vertex v do: Update(v).
Go to 3⃝.
Note that in contrast to our previous discussion, ε = 0 is possible. The vari-
ables τ A
v are not needed explicitly. The “unpack”-step 9⃝is illustrated in Figure
11.3, where a blossom with 19 vertices is unpacked. Two of the ﬁve sub-blossoms
become out-of-forest, two become inner blossoms and one becomes an outer blos-
som.

11.3 Implementation of the Weighted Matching Algorithm
257
(a)
(b)
y = r0
r1
r2
r3
r4
r5
r6
r7
r8
r9
r10
Fig. 11.3.
(a)
(b)
8
8
10
8
4
13
10
8
8
14
11
4
9
13
12
12
A
B
C
D
E
F
G
H
2
0
4
2
0
5
4
0
0
4
5
0
3
5
2
2
A
B
C
D
E
F
G
H
Fig. 11.4.
In 6⃝, the connected components of the blossom forest F have to be deter-
mined. This can be done in linear time by Proposition 2.17.
Before analysing the algorithm, let us demonstrate its major steps by an exam-
ple. Consider the graph in Figure 11.4(a). Initially, the algorithm sets z{a} = z{d} =
z{h} = 2, z{b} = x{c} = z{ f } = 4 and z{e} = z{g} = 6. In Figure 11.4(b) the slacks
can be seen. So in the beginning the edges {a, d}, {a, h}, {b, c}, {b, f }, {c, f } are
tight.
We assume that the algorithm scans the vertices in alphabetical order. So the
ﬁrst steps are
augment(a, d),
augment(b, c),
grow( f, b).
Figure 11.5(a) shows the current general blossom forest.

258
11. Weighted Matching
(a)
(b)
E
F
G
H
A
D
E
F
G
H
B
C
A
D
B
C
Fig. 11.5.
(a)
(b)
2
0
4
0
0
3
2
0
0
2
3
0
1
3
0
0
A
B
C
D
E
F
G
H
E
G
A
H
D
C
B
F
Fig. 11.6.
The next steps are
shrink( f, c),
grow(h, a),
resulting in the general blossom forest shown in Figure 11.5(b). Now all the tight
edges are used up, so the dual variables have to change. We perform
8⃝and
obtain ε = ε3 = 1, say A = {b, c, f } and τ A
v = d. The new dual variables are
z{b,c, f } = 1, z{a} = 1, z{d} = z{h} = 3, z{b} = z{c} = z{ f } = 4, z{e} = z{g} = 7. The
current slacks are shown in Figure 11.6(a). The next step is
augment(d, c).
The blossom {b, c, f } becomes out-of-forest (Figure 11.6(b)). Now the edge
{e, f } is tight, but in the previous dual change we have only set scanned(d) :=
false. So we need to do 8⃝with ε = ε3 = 0 twice to make the next steps
grow(e, f ),
grow(d, a)
possible. We arrive at Figure 11.7(a).
No more edges incident to outer vertices are tight, so we perform
8⃝once
more. We obtain ε = ε1 = 1 and obtain the new dual solution z{b,c, f } = 0,
z{a} = 0, z{d} = z{h} = z{b} = x{c} = z{ f } = 4, z{e} = z{g} = 8. The new slacks are

11.3 Implementation of the Weighted Matching Algorithm
259
(a)
(b)
H
A
D
C
B
F
E
G
4
0
6
0
0
1
2
0
0
2
3
0
1
1
0
0
A
B
C
D
E
F
G
H
Fig. 11.7.
(a)
(b)
H
A
D
C
B
F
E
G
4
0
7
0
0
0
2
1
0
2
2
0
1
0
0
0
A
B
C
D
E
F
G
H
Fig. 11.8.
shown in Figure 11.7(b). Since the dual variable for the inner blossom {B, C, F}
becomes zero, we have to
unpack({b, c, f }).
The general blossom forest we get is shown in Figure 11.8(a). After another
dual variable change with ε = ε3 = 1
2 we obtain z{a} = −0.5, z{c} = z{ f } = 3.5,
z{b} = z{d} = z{h} = 4.5, z{e} = z{g} = 8.5 (the slacks are shown in Figure 11.8(b)).
The ﬁnal steps are

260
11. Weighted Matching
shrink(d, e),
augment(g, h),
and the algorithm terminates. The ﬁnal matching is M = {{e, f }, {b, c}, {a, d},
{g, h}}. We check that M has total weight 37, equal to the sum of the dual
variables.
Let us now check that the algorithm works correctly.
Proposition 11.6.
The following statements hold at any stage of the Weighted
Matching Algorithm:
(a) B is a laminar family. B =
5
{v ∈V (G) : bi(v) = j for some i} : j = 1, . . . ,
B
6
. The sets Vρkr (r) := {v : ρkv(v) = ρkr (r)} are exactly the maximal elements
of B. The vertices in each Vr are marked either all outer or all inner or all
out-of-forest. Each (Vr, {{v, ϕkv(v)} : v ∈Vr \{r}}∪{{v, µ(v)} : v ∈Vr \{r}})
is a blossom with base r.
(b) The edges {x, µ(x)} form a matching M. M contains a near-perfect matching
within each element of B.
(c) For each b ∈{1, . . . , K} let X(b) := {v ∈V (G) : bi(v) = b for some i}.
Then the variables µ(v) and ϕi(v), for those v and i with bi(v) = b, are
associated with an M-alternating ear-decomposition in G[X(b)].
(d) The edges {x, µ(x)} and {x, ϕi(x)} for all x and i, and the edges {σ(x),
χ(σ(x))} for all bases x of maximal inner blossoms, are all tight.
(e) The edges {x, µ(x)}, {x, ϕkx(x)} for all inner or outer x, together with the edges
{σ(x), χ(σ(x))} for all bases x of maximal inner blossoms, form a general
blossom forest F with respect to M. The vertex marks (inner, outer, out-of-
forest) are consistent with F.
(f) Contracting the maximal sub-blossoms of any blossom in B results in a circuit.
(g) For each outer vertex v, the procedure TreePath gives an M-alternating v-r-
path, where r is the root of the tree in F containing v.
Proof:
The properties clearly hold at the beginning (after 2⃝is executed the ﬁrst
time). We show that they are maintained throughout the algorithm. This is easily
seen for (a) by considering 7⃝and 9⃝. For (b), this follows from Proposition 11.4
and the assumption that (f) and (g) hold before augmenting.
The proof that (c) continues to hold after shrinking is the same as in the
non-weighted case (see Lemma 10.30 (c)). The ϕ-values are recomputed after
augmenting and not changed elsewhere. (d) is guaranteed by 4⃝.
It is easy to see that (e) is maintained by 5⃝: The blossom containing y was
out-of-forest, and setting χ(y) := x and σ(v) := y for the base v of the blossom
makes it inner. The blossom containing µ(ρky(y)) was also out-of-forest, and
becomes outer.
In 6⃝, two connected components of the general blossom forest clearly become
out-of-forest, so (e) is maintained. In 7⃝, the vertices in the new blossom clearly
become outer because r was outer before. In
9⃝, for the vertices v ∈A with
ρkv−1(v) /∈V (Q(y)) we also have µ(ρkv(v)) /∈V (Q(y)), so they become out-of-
forest. For each v ∈A with ρkv−1(v) = rk for some k. Since {ri,ri+1} ∈M iff i
is even, v becomes outer iff k is odd.

11.3 Implementation of the Weighted Matching Algorithm
261
(f) holds for any blossom, as any new blossom arises from an odd circuit in
7⃝. To see that (g) is maintained, it sufﬁces to observe that σ(x) and χ(σ(x)) are
set correctly for all bases x of maximal inner blossoms. This is easily checked for
both 5⃝and 9⃝.
2
Proposition 11.6(a) justiﬁes calling the maximal elements of B inner, outer or
out-of-forest in 8⃝and 9⃝of the algorithm.
Next we show that the algorithm maintains a feasible dual solution.
Lemma 11.7.
At any stage of the algorithm, z is a feasible dual solution. If ε = ∞
then G has no perfect matching.
Proof:
We always have z A = 0 for all A ∈A\B. z A is decreased only for those
A ∈B that are maximal in B and inner. So the choice of ε1 guarantees that z A
continues to be nonnegative for all A with |A| > 1.
How can the constraints 
A∈A:e∈δ(A) z A ≤c(e) be violated? If 
A∈A:e∈δ(A) z A
increases in
8⃝, e must either connect an outer and an out-of-forest vertex or
two different outer blossoms. So the maximal ε such that the new z still satisﬁes

A∈A:e∈δ(A) z A ≤c(e) is slack(e) in the ﬁrst case and 1
2slack(e) in the second
case.
We thus have to prove that ε2 and ε3 are computed correctly:
ε2 = min{slack(v, w) : v outer, w out-of-forest}
and
ε3 = 1
2 min{slack(v, w) : v, w outer, ρkv(v) ̸= ρkw(w)}.
For ε2 this is easy to see, since for any out-of-forest vertex v we always have
that τv is the outer vertex w minimizing slack(v, w) = c({v, w}) −ζ{v} −ζ{w}.
Now consider ε3. We claim that at any stage of the algorithm the following
holds for any outer vertex v and any A ∈B such that there is no ¯A ∈B with
A ∪{v} ⊆¯A:
(a) τ A
v ∈A.
(b) slack(v, τ A
v ) = min{slack(v, u) : u ∈A}.
(c) ζA = 
B∈B:A⊆B zB.  is the sum of the ε-values in all dual changes so far.
(d) slack(v, τ A
v ) = t A
v − −ζA.
(e) t A = min{t A
v : v outer and there is no ¯A ∈B with A ∪{v} ⊆¯A}.
(a), (c), and (e) are easily seen to be true. (b) and (d) hold when τ A
v is deﬁned
(in
7⃝or in Update(v)), and afterwards slack(v, u) decreases exactly by the
amount that  + ζA increases (due to (c)). Now (a), (b), (d), and (e) imply that ε3
is computed correctly.
Now suppose ε = ∞, i.e. ε can be chosen arbitrarily large without destroying
dual feasibility. Since the dual objective 1lz increases by at least ε in
8⃝, we
conclude that the dual LP (11.2) is unbounded. Hence by Theorem 3.22 the primal
LP (11.1) is infeasible.
2
Now the correctness of the algorithm follows:

262
11. Weighted Matching
Theorem 11.8.
If the algorithm terminates in 6⃝, the edges {x, µ(x)} form a min-
imum weight perfect matching in G.
Proof:
Let x be the incidence vector of M (the matching consisting of the edges
{x, µ(x)}). The complementary slackness conditions
xe > 0
⇒

A∈A:e∈δ(A)
z A = c(e)
z A > 0
⇒

e∈δ(A)
xe = 1
are satisﬁed: The ﬁrst one holds since all the matching edges are tight (Proposition
11.6(d)). The second one follows from Proposition 11.6(b).
Since we have feasible primal and dual solutions (Lemma 11.7), both must be
optimal (Corollary 3.18). So x is optimal for the LP (11.1) and integral, proving
that M is a minimum weight perfect matching.
2
Until now we have not proved that the algorithm terminates.
Theorem 11.9.
The running time of the Weighted Matching Algorithm be-
tween two augmentations is O(n2). The overall running time is O(n3).
Proof:
By Lemma 11.5 and Proposition 11.6(a), the Update procedure runs in
linear time.
Both 2⃝and 6⃝take O(n2) time, once per augmentation.
Each of 5⃝, 7⃝, and 9⃝can be done in O(nk) time, where k is the number
of new outer vertices. (In 7⃝, the number of maximal proper subsets A′ of A to
be considered is at most 2k + 1: every second sub-blossom of a new blossom
must have been inner.) Since an outer vertex continues to be outer until the next
augmentation, the total time spent by 5⃝, 7⃝, and 9⃝between two augmentations
is O(n2).
It remains to estimate the running time of 8⃝, 3⃝, and 4⃝. Suppose in 8⃝we
have ε ̸= ε1. Due to the variables tv and t A
v we then obtain a new tight edge in 8⃝.
We continue in 3⃝and 4⃝, where after at most O(n) time this edge is checked.
Since it either connects an outer vertex with an out-of-forest vertex or two different
outer connected components, we can apply one of 5⃝, 6⃝, 7⃝. If ε = ε1 we have
to apply 9⃝.
This consideration shows that the number of times 8⃝is executed is less than
or equal to the number of times one of 5⃝, 6⃝, 7⃝, 9⃝is executed. Since 8⃝takes
only O(n) time, the O(n2) bound between two augmentations is proved. Note that
the case ε = 0 is not excluded.
Since there are only n
2 augmentations, the total running time is O(n3).
2
Corollary 11.10.
The Minimum Weight Perfect Matching Problem can be
solved in O(n3) time.
Proof:
This follows from Theorems 11.8 and 11.9.
2

11.4 Postoptimality
263
The ﬁrst O(n3)-implementation of Edmonds’ algorithm for the Minimum
Weight Perfect Matching Problem was due to Gabow [1973] (see also
Gabow [1976] and Lawler [1976]). The theoretically best running time, namely
O(mn + n2 log n), has also been obtained by Gabow [1990]. For planar graphs
a minimum weight perfect matching can be found in O

n
3
2 log n

time, as Lip-
ton and Tarjan [1979,1980] showed by a divide and conquer approach, using the
fact that planar graphs have small “separators”. For Euclidean instances (a set of
points in the plane deﬁning a complete graph whose edge weights are given by
the Euclidean distances) Varadarajan [1998] found an O

n
3
2 log5 n

algorithm.
Probably the currently most efﬁcient implementations are described by Mehl-
horn and Sch¨afer [2000] and Cook and Rohe [1999]. They solve matching prob-
lems with millions of vertices optimally. A “primal version” of the Weighted
Matching Algorithm – always maintaining a perfect matching and obtaining a
feasible dual solution only at termination – has been described by Cunningham
and Marsh [1978].
11.4 Postoptimality
In this section we prove two postoptimality results which we shall need in Section
12.2.
Lemma 11.11.
(Weber [1981], Ball and Derigs [1983])
Suppose we have run
the Weighted Matching Algorithm for an instance (G, c). Let s ∈V (G), and
let c′ : E(G) →R with c′(e) = c(e) for all e ̸∈δ(s). Then a minimum weight
perfect matching with respect to (G, c′) can be determined in O(n2) time.
Proof:
Let t := µ(s). If s is not contained in any nontrivial blossom, i.e. ks = 0,
then the ﬁrst step just consists of setting µ(s) := s and µ(t) := t. Otherwise
we have to unpack all the blossoms containing s. To accomplish this, we shall
perform dual changes of total value 
A: s∈A, |A|>1 z A while s is inner all the time.
Consider the following construction:
Set V (G) := V (G)
.
∪{a, b} and E(G) := E(G) ∪{{a, s}, {b, t}}.
Set c({a, s}) := ζ{s} and c({b, t}) := 2

A: s∈A, |A|>1
z A + ζ{t}.
Set µ(a) := a and µ(b) := b. Mark a and b as outer.
Set B := B ∪{{a}, {b}}, z{a} := 0, z{b} := 0, ζ{a} := 0, ζ{b} := 0.
Set ka := 0, kb := 0, ρ0(a) := a, ρ0(b) := b, ϕ0(a) := a, ϕ0(b) := b.
Update(a). Update(b).
The result is a possible status if the algorithm was applied to the modi-
ﬁed instance (the graph extended by two vertices and two edges). In particular,
the dual solution z is feasible. Moreover, the edge {a, s} is tight. Now we set
scanned(a) := false and continue the algorithm starting with 3⃝. The algorithm
will do a Grow(a, s) next, and s becomes inner.

264
11. Weighted Matching
By Theorem 11.9 the algorithm terminates after O(n2) steps with an augmen-
tation. The only possible augmenting path is a, s, t, b. So the edge {b, t} must
become tight. At the beginning, slack(b, t) = 2
A∈A, s∈A, |A|>1 z A.
Vertex s will remain inner throughout. So ζ{s} will decrease at each dual change.
Thus all blossoms A containing s are unpacked at the end. We ﬁnally delete the
vertices a and b and the edges {a, s} and {b, t}, and set B := B \ {{a}, {b}} and
µ(s) := s, µ(t) := t.
Now s and t are outer, and there are no inner vertices. Furthermore, no edge
incident to s belongs to the general blossom forest. So we can easily change
weights of edges incident to s as well as z{s}, as long as we maintain the dual
feasibility. This, however, is easily guaranteed by ﬁrst computing the slacks ac-
cording to the new edge weights and then increasing z{s} by mine∈δ(s) slack(e).
We set scanned(s) := false and continue the algorithm starting with 3⃝. By The-
orem 11.9, the algorithm will terminate after O(n2) steps with a minimum weight
perfect matching with respect to the new weights.
2
The same result for the “primal version” of the Weighted Matching Al-
gorithm can be found in Cunningham and Marsh [1978]. The following lemma
deals with the addition of two vertices to an instance that has already been solved.
Lemma 11.12.
Let (G, c) be an instance of the Minimum Weight Perfect
Matching Problem, and let s, t ∈V (G). Suppose we have run the Weighted
Matching Algorithm for the instance (G −{s, t}, c). Then a minimum weight
perfect matching with respect to (G, c) can be determined in O(n2) time.
Proof:
The addition of two vertices requires the initialization of the data
structures (as in the previous proof). The dual variable zv is set such that
mine∈δ(v) slack(e) = 0 (for v ∈{s, t}). Then setting scanned(s) := scanned(t) :=
false and starting the Weighted Matching Algorithm with 3⃝does the job. 2
11.5 The Matching Polytope
The correctness of the Weighted Matching Algorithm also yields Edmonds’
characterization of the perfect matching polytope as a by-product. We again use
the notation A := {A ⊆V (G) : |A| odd}.
Theorem 11.13.
(Edmonds [1965]) Let G be an undirected graph. The perfect
matching polytope of G, i.e. the convex hull of the incidence vectors of all perfect
matchings in G, is the set of vectors x satisfying
xe
≥
0
(e ∈E(G))

e∈δ(v)
xe
=
1
(v ∈V (G))

e∈δ(A)
xe
≥
1
(A ∈A)

11.5 The Matching Polytope
265
Proof:
By Corollary 3.27 it sufﬁces to show that all vertices of the polytope de-
scribed above are integral. By Theorem 5.12 this is true if the minimization prob-
lem has an integral optimum solution for any weight function. But our Weighted
Matching Algorithm ﬁnds such a solution for any weight function (cf. the proof
of Theorem 11.8).
2
An alternative proof will be given in Section 12.3 (see the remark after The-
orem 12.16).
We can also describe the matching polytope, i.e. the convex hull of the inci-
dence vectors of all matchings in an undirected graph G:
Theorem 11.14.
(Edmonds [1965])
Let G be a graph. The matching polytope
of G is the set of vectors x ∈RE(G)
+
satisfying

e∈δ(v)
xe ≤1
for all v ∈V (G)
and

e∈E(G[A])
xe ≤|A| −1
2
for all A ∈A.
Proof:
Since the incidence vector of any matching obviously satisﬁes these in-
equalities, we only have to prove one direction. Let x ∈RE(G)
+
be a vector with

e∈δ(v) xe ≤1 for v ∈V (G) and 
e∈E(G[A]) xe ≤|A|−1
2
for A ∈A. We prove that
x is a convex combination of incidence vectors of matchings.
Let H be the graph with V (H) := {(v, i) : v ∈V (G), i ∈{1, 2}}, and
E(H) := {{(v, i), (w, i)} : {v, w} ∈E(G), i ∈{1, 2}} ∪{{(v, 1), (v, 2)} : v ∈
V (G)}. So H consists of two copies of G, and there is an edge joining the two
copies of each vertex. Let y{(v,i),(w,i)} := xe for each e = {v, w} ∈E(G) and
i ∈{1, 2}, and let y{(v,1),(v,2)} := 1 −
e∈δG(v) xe for each v ∈V (G). We claim
that y belongs to the perfect matching polytope of H. Considering the subgraph
induced by {(v, 1) : v ∈V (G)}, which is isomorphic to G, we then get that x is
a convex combination of incidence vectors of matchings in G.
Obviously, y ∈RE(H)
+
and 
e∈δH(v) ye = 1 for all v ∈V (H). To show that
y belongs to the perfect matching polytope of H, we use Theorem 11.13. So
let X ⊆V (H) with |X| odd. We prove that 
e∈δH(X) ye ≥1. Let A := {v ∈
V (G) : (v, 1) ∈X, (v, 2) /∈X}, B := {v ∈V (G) : (v, 1) ∈X, (v, 2) ∈X} and
C := {v ∈V (G) : (v, 1) /∈X, (v, 2) ∈X}. Since |X| is odd, either A or C must
have odd cardinality, w.l.o.g. |A| is odd. We write Ai := {(a, i) : a ∈A} and
Bi := {(b, i) : b ∈B} for i = 1, 2 (see Figure 11.9). Then

e∈δH(X)
ye
≥

v∈A1

e∈δH(v)
ye −2

e∈E(H[A1])
ye −

e∈EH(A1,B1)
ye +

e∈EH(B2,A2)
ye
=

v∈A1

e∈δH(v)
ye −2

e∈E(G[A])
xe
≥
|A1| −(|A| −1) = 1.
2
Indeed, we can prove the following stronger result:

266
11. Weighted Matching
V (G)
{(v, 1) : v ∈V (G)}
{(v, 2) : v ∈V (G)}
A
A1
A2
B
B1
B2
C
: X
Fig. 11.9.
Theorem 11.15.
(Cunningham and Marsh [1978]) For any undirected graph G
the linear inequality system
xe
≥
0
(e ∈E(G))

e∈δ(v)
xe
≤
1
(v ∈V (G))

e⊆A
xe
≤
|A|−1
2
(A ∈A, |A| > 1)
is TDI.
Proof:
For c : E(G) →Z we consider the LP max 
e∈E(G) c(e)xe subject to
the above constraints. The dual LP is:
min

v∈V (G)
yv +

A∈A, |A|>1
|A| −1
2
z A
s.t.

v∈e
yv +

A∈A, e⊆A
z A
≥
c(e)
(e ∈E(G))
yv
≥
0
(v ∈V (G))
z A
≥
0
(A ∈A, |A| > 1)
Let (G, c) be the smallest counterexample, i.e. there is no integral optimum
dual solution and |V (G)| + |E(G)| + 
e∈E(G) |c(e)| is minimum. Then c(e) ≥1
for all e (otherwise we can delete any edge of nonpositive weight).
Moreover, for any optimum solution y, z we claim that y = 0. To prove this,
suppose yv > 0 for some v ∈V (G). Then by complementary slackness (Corollary
3.18) 
e∈δ(v) xe = 1 for any primal optimum solution x. But then decreasing c(e)
by one for each e ∈δ(v) yields a smaller instance (G, c′), whose optimum LP

Exercises
267
value is one less (here we use primal integrality, i.e. Theorem 11.14). Since (G, c)
is the smallest counterexample, there exists an integral optimum dual solution
y′, z′ for (G, c′). Increasing y′
v by one yields an integral optimum dual solution
for (G, c), a contradiction.
Now let y = 0 and z be an optimum dual solution for which

A∈A, |A|>1
|A|2z A
(11.4)
is as large as possible. We claim that F := {A : z A > 0} is laminar. To see this,
suppose there are sets X, Y ∈F with X \ Y ̸= ∅, Y \ X ̸= ∅and X ∩Y ̸= ∅. Let
ϵ := min{zX, zY} > 0.
If |X ∩Y| is odd, then |X ∪Y| is also odd. Set z′
X := zX −ϵ, z′
Y := zY −ϵ,
z′
X∩Y := zX∩Y + ϵ (unless |X ∩Y| = 1), z′
X∪Y := zX∪Y + ϵ and z′(A) := z(A) for
all other sets A. y, z′ is also a feasible dual solution; moreover it is optimum as
well. This is a contradiction since (11.4) is larger.
If |X ∩Y| is even, then |X \ Y| and |Y \ X| are odd. Set z′
X := zX −ϵ,
z′
Y := zY −ϵ, z′
X\Y := zX\Y + ϵ (unless |X \ Y| = 1), z′
Y\X := zY\X + ϵ (unless
|Y \ X| = 1) and z′(A) := z(A) for all other sets A. Set y′
v := yv +ϵ for v ∈X ∩Y
and y′
v := yv for v /∈X ∩Y. Then y′, z′ is a feasible dual solution that is also
optimum. This contradicts the fact that any optimum dual solution must have
y = 0.
Now let A ∈F with z A /∈Z and A maximal. Set ϵ := z A −⌊z A⌋> 0.
Let A1, . . . , Ak be the maximal proper subsets of A in F; they must be disjoint
because F is laminar. Setting z′
A := z A −ϵ and z′
Ai := z Ai + ϵ for i = 1, . . . , k
(and z′(D) := z(D) for all other D ∈A) yields another feasible dual solution
y = 0, z′ (since c is integral). We have

B∈A, |B|>1
|B| −1
2
z′
B <

B∈A, |B|>1
|B| −1
2
zB,
contradicting the optimality of the original dual solution y = 0, z.
2
This proof is due to Schrijver [1983a]. For different proofs, see Lov´asz [1979]
and Schrijver [1983b]. The latter does not use Theorem 11.14. Moreover, replac-
ing 
e∈δ(v) xe ≤1 by 
e∈δ(v) xe = 1 for v ∈V (G) in Theorem 11.15 yields
an alternative description of the perfect matching polytope, which is also TDI (by
Theorem 5.17). Theorem 11.13 can easily be derived from this; however, the linear
inequality system of Theorem 11.13 is not TDI in general (K4 is a counterexam-
ple). Theorem 11.15 also implies the Berge-Tutte formula (Theorem 10.14; see
Exercise 14). Generalizations will be discussed in Section 12.1.
Exercises
1. Use Theorem 11.2 to prove a weighted version of K¨onig’s Theorem 10.2.
(Egerv´ary [1931])

268
11. Weighted Matching
2. Describe the convex hull of the incidence vectors of all
(a) vertex covers,
(b) stable sets,
(c) edge covers,
in a bipartite graph G. Show how Theorem 10.2 and the statement of Exercise
2(c) of Chapter 10 follow.
Hint: Use Theorem 5.24 and Corollary 5.20.
3. Prove the Birkhoff-von-Neumann Theorem 11.3 directly.
4. Let G be a graph and P the fractional perfect matching polytope of G. Prove
that the vertices of P are exactly the vectors x with
xe =
 1
2
if e ∈E(C1) ∪· · · ∪E(Ck)
1
if e ∈M
0
otherwise
,
where C1, . . . , Ck are vertex-disjoint odd circuits and M is a perfect matching
in G −(V (C1) ∪· · · ∪V (Ck)).
(Balinski [1972]; see Lov´asz [1979]).
5. Let G be a bipartite graph with bipartition V = A
.
∪B and A = {a1, . . . , ap},
B = {b1, . . . , bq}. Let c : E(G) →R be weights on the edges. We look for
the maximum weight order-preserving matching M, i.e. for any two edges
{ai, bj}, {ai′, bj′} ∈M with i < i′ we require j < j′. Solve this problem with
an O(n3)-algorithm.
Hint: Use dynamic programming.
6. Prove that, at any stage of the Weighted Matching Algorithm, |B| ≤3
2n.
7. Let G be a graph with nonnegative weights c : E(G) →R+. Let M be the
matching at any intermediate stage of the Weighted Matching Algorithm.
Let X be the set of vertices covered by M. Show that any matching covering
X is at least as expensive as M.
(Ball and Derigs [1983])
8. A graph with integral weights on the edges is said to have the even circuit
property if the total weight of every circuit is even. Show that the Weighted
Matching Algorithm applied to a graph with the even circuit property
maintains this property (with respect to the slacks) and also maintains a dual
solution that is integral. Conclude that for any graph there exists an optimum
dual solution z that is half-integral (i.e. 2z is integral).
9. When the Weighted Matching Algorithm is restricted to bipartite graphs,
it becomes much simpler. Show which parts are necessary even in the bipartite
case and which are not.
Note: One arrives at what is called the Hungarian Method for the Assignment
Problem (Kuhn [1955]). This algorithm can also be regarded as an equivalent
description of the procedure proposed in the proof of Theorem 11.1.
10. How can the bottleneck matching problem (ﬁnd a perfect matching M such
that max{c(e) : e ∈M} is minimum) be solved in O(n3) time?

References
269
11. Show how to solve the Minimum Weight Edge Cover Problem in polyno-
mial time: given an undirected graph G and weights c : E(G) →R, ﬁnd a
minimum weight edge cover.
12. Given an undirected graph G with weights c : E(G) →R+ and two vertices
s and t, we look for a shortest s-t-path with an even (or with an odd) number
of edges. Reduce this to a Minimum Weight Perfect Matching Problem.
Hint: Take two copies of G, connect each vertex with its copy by an edge of
zero weight and delete s and t (or s and the copy of t).
See (Gr¨otschel and Pulleyblank [1981]).
13. Let G be a k-regular and (k −1)-edge-connected graph, and c : E(G) →R+.
Prove that there exists a perfect matching M in G with c(M) ≥1
k c(E(G)).
Hint: Show that 1
k 1l is in the perfect matching polytope.
14.
∗
Show that Theorem 11.15 implies:
(a) the Berge-Tutte formula (Theorem 10.14);
(b) Theorem 11.13;
(c) the existence of an optimum half-integral dual solution to the dual LP
(11.2) (cf. Exercise 8).
Hint: Use Theorem 5.17.
15. The fractional perfect matching polytope Q of G is identical to the perfect
matching polytope if G is bipartite (Theorem 11.2). Consider the ﬁrst Gomory-
Chv´atal-truncation Q′ of Q (Deﬁnition 5.28). Prove that Q′ is always identical
to the perfect matching polytope.
References
General Literature:
Gerards, A.M.H. [1995]: Matching. In: Handbooks in Operations Research and Management
Science; Volume 7: Network Models (M.O. Ball, T.L. Magnanti, C.L. Monma, G.L.
Nemhauser, eds.), Elsevier, Amsterdam 1995, pp. 135–224
Lawler, E.L. [1976]: Combinatorial Optimization; Networks and Matroids. Holt, Rinehart
and Winston, New York 1976, Chapters 5 and 6
Papadimitriou, C.H., and Steiglitz, K. [1982]: Combinatorial Optimization; Algorithms and
Complexity. Prentice-Hall, Englewood Cliffs 1982, Chapter 11
Pulleyblank, W.R. [1995]: Matchings and extensions. In: Handbook of Combinatorics; Vol.
1 (R.L. Graham, M. Gr¨otschel, L. Lov´asz, eds.), Elsevier, Amsterdam 1995
Cited References:
Balinski, M.L. [1972]: Establishing the matching polytope. Journal of Combinatorial Theory
13 (1972), 1–13
Ball, M.O., and Derigs, U. [1983]: An analysis of alternative strategies for implementing
matching algorithms. Networks 13 (1983), 517–549
Birkhoff, G. [1946]: Tres observaciones sobre el algebra lineal. Revista Universidad Na-
cional de Tucum´an, Series A 5 (1946), 147–151
Cook, W., and Rohe, A. [1999]: Computing minimum-weight perfect matchings. INFORMS
Journal of Computing 11 (1999), 138–148

270
11. Weighted Matching
Cunningham, W.H., and Marsh, A.B. [1978]: A primal algorithm for optimum matching.
Mathematical Programming Study 8 (1978), 50–72
Edmonds, J. [1965]: Maximum matching and a polyhedron with (0,1) vertices. Journal of
Research of the National Bureau of Standards B 69 (1965), 125–130
Egerv´ary, E. [1931]: Matrixok kombinatorikus tulajdons´agairol. Matematikai ´es Fizikai
Lapok 38 (1931), 16–28 [in Hungarian]
Gabow, H.N. [1973]: Implementation of algorithms for maximum matching on non-bipartite
graphs. Ph.D. Thesis, Stanford University, Dept. of Computer Science, 1973
Gabow, H.N. [1976]: An efﬁcient implementation of Edmonds’ algorithm for maximum
matching on graphs. Journal of the ACM 23 (1976), 221–234
Gabow, H.N. [1990]: Data structures for weighted matching and nearest common ances-
tors with linking. Proceedings of the 1st Annual ACM-SIAM Symposium on Discrete
Algorithms (1990), 434–443
Gr¨otschel, M., and Pulleyblank, W.R. [1981]: Weakly bipartite graphs and the max-cut
problem. Operations Research Letters 1 (1981), 23–27
Kuhn, H.W. [1955]: The Hungarian method for the assignment problem. Naval Research
Logistics Quarterly 2 (1955), 83–97
Lipton, R.J., and Tarjan, R.E. [1979]: A separator theorem for planar graphs. SIAM Journal
on Applied Mathematics 36 (1979), 177–189
Lipton, R.J., and Tarjan, R.E. [1979]: Applications of a planar separator theorem. SIAM
Journal on Computing 9 (1980), 615–627
Lov´asz, L. [1979]: Graph theory and integer programming. In: Discrete Optimization I;
Annals of Discrete Mathematics 4 (P.L. Hammer, E.L. Johnson, B.H. Korte, eds.), North-
Holland, Amsterdam 1979, pp. 141–158
Mehlhorn, K., and Sch¨afer, G. [2000]: Implementation of O(nm log n) weighted matchings
in general graphs: the power of data structures. In: Algorithm Engineering; WAE-2000;
LNCS 1982 (S. N¨aher, D. Wagner, eds.), pp. 23–38; also electronically in The ACM
Journal of Experimental Algorithmics 7 (2002)
Munkres, J. [1957]: Algorithms for the assignment and transportation problems. Journal of
the Society for Industrial and Applied Mathematics 5 (1957), 32–38
von Neumann, J. [1953]: A certain zero-sum two-person game equivalent to the optimal
assignment problem. In: Contributions to the Theory of Games II; Ann. of Math. Stud.
28 (H.W. Kuhn, ed.), Princeton University Press, Princeton 1953, pp. 5–12
Schrijver, A. [1983a]: Short proofs on the matching polyhedron. Journal of Combinatorial
Theory B 34 (1983), 104–108
Schrijver, A. [1983b]: Min-max results in combinatorial optimization. In: Mathematical
Programming; The State of the Art – Bonn 1982 (A. Bachem, M. Gr¨otschel, B. Korte,
eds.), Springer, Berlin 1983, pp. 439–500
Varadarajan, K.R. [1998]: A divide-and-conquer algorithm for min-cost perfect matching in
the plane. Proceedings of the 39th Annual IEEE Symposium on Foundations of Computer
Science (1998), 320–329
Weber, G.M. [1981]: Sensitivity analysis of optimal matchings. Networks 11 (1981), 41–56

12. b-Matchings and T-Joins
In this chapter we introduce two more combinatorial optimization problems, the
Minimum Weight b-Matching Problem in Section 12.1 and the Minimum
Weight T -Join Problem in Section 12.2. Both can be regarded as generaliza-
tions of the Minimum Weight Perfect Matching Problem and also include
other important problems. On the other hand, both problems can be reduced to
the Minimum Weight Perfect Matching Problem. They have combinatorial
polynomial-time algorithms as well as polyhedral descriptions. Since in both cases
the Separation Problem turns out to be solvable in polynomial time, we obtain
another polynomial-time algorithm for the general matching problems (using the
Ellipsoid Method; see Section 4.6). In fact, the Separation Problem can be
reduced to ﬁnding a minimum capacity T -cut in both cases; see Sections 12.3 and
12.4. This problem, ﬁnding a minimum capacity cut δ(X) such that |X ∩T | is odd
for a speciﬁed vertex set T , can be solved with network ﬂow techniques.
12.1 b-Matchings
Deﬁnition 12.1.
Let G be an undirected graph with integral edge capacities u :
E(G) →N ∪{∞} and numbers b : V (G) →N. Then a b-matching in (G, u) is a
function f : E(G) →Z+ with f (e) ≤u(e) for all e ∈E(G) and 
e∈δ(v) f (e) ≤
b(v) for all v ∈V (G). In the case u ≡1 we speak of a simple b-matching in G.
A b-matching f is called perfect if 
e∈δ(v) f (e) = b(v) for all v ∈V (G).
In the case b ≡1 the capacities are irrelevant, and we are back to ordinary
matchings. A simple b-matching is sometimes also called a b-factor. It can be
regarded as a subset of edges. In Chapter 21 we shall be interested in perfect
simple 2-matchings, i.e. subsets of edges such that each vertex is incident to
exactly two of them.
Maximum Weight b-Matching Problem
Instance:
A graph G, capacities u : E(G) →N∪{∞}, weights c : E(G) →R,
and numbers b : V (G) →N.
Task:
Find a b-matching f in (G, u) whose weight 
e∈E(G) c(e) f (e) is
maximum.

272
12. b-Matchings and T -Joins
Edmonds’ Weighted Matching Algorithm can be extended to solve this
problem (Marsh[1979]). We shall not describe this algorithm here, but shall rather
give a polyhedral description and show that the Separation Problem can be
solved in polynomial time. This yields a polynomial-time algorithm via the El-
lipsoid Method (cf. Corollary 3.28).
The b-matching polytope of (G, u) is deﬁned to be the convex hull of the
incidence vectors of all b-matchings in (G, u). We ﬁrst consider the uncapacitated
case (u ≡∞):
Theorem 12.2.
(Edmonds [1965]) Let G be an undirected graph and b : V (G)
→N. The b-matching polytope of (G, ∞) is the set of vectors x ∈RE(G)
+
satisfying

e∈δ(v)
xe
≤
b(v)
(v ∈V (G));

e∈E(G[X])
xe
≤
:
1
2

v∈X
b(v)
;
(X ⊆V (G)).
Proof:
Since any b-matching obviously satisﬁes these constraints, we only have
to show one direction. So let x ∈RE(G)
+
with 
e∈δ(v) xe ≤b(v) for all v ∈V (G)
and 
e∈E(G[X]) xe ≤
 1
2

v∈X b(v)

for all X ⊆V (G). We show that x is a
convex combination of incidence vectors of b-matchings.
We deﬁne a new graph H by splitting up each vertex v into b(v) copies: we
deﬁne Xv := {(v, i) : i ∈{1, . . . , b(v)}} for v ∈V (G), V (H) := 
v∈V (G) Xv and
E(H) := {{v′, w′} : {v, w} ∈E(G), v′ ∈Xv, w′ ∈Xw}. Let ye :=
1
b(v)b(w)x{v,w}
for each edge e = {v′, w′} ∈E(H), v′ ∈Xv, w′ ∈Xw. We claim that y is a
convex combination of incidence vectors of matchings in H. By contracting the
sets Xv (v ∈V (G)) in H we then return to G and x, and conclude that x is a
convex combination of incidence vectors of b-matchings in G.
To prove that y is in the matching polytope of H we use Theorem 11.14.

e∈δ(v) ye ≤1 obviously holds for each v ∈V (H). Let C ⊆V (H) with |C| odd.
We show that 
e∈E(H[C]) ye ≤1
2(|C| −1).
If Xv ⊆C or Xv ∩C = ∅for each v ∈V (G), this follows directly from the
inequalities assumed for x. Otherwise let a, b ∈Xv, a ∈C, b ̸∈C. Then
2

e∈E(H[C])
ye
=

c∈C\{a}

e∈E({c},C\{c})
ye +

e∈E({a},C\{a})
ye
≤

c∈C\{a}

e∈δ(c)\{{c,b}}
ye +

e∈E({a},C\{a})
ye
=

c∈C\{a}

e∈δ(c)
ye −

e∈E({b},C\{a})
ye +

e∈E({a},C\{a})
ye
=

c∈C\{a}

e∈δ(c)
ye
≤
|C| −1.
2

12.1 b-Matchings
273
Note that this construction yields an algorithm which, however, in general has
an exponential running time. But we note that in the special case 
v∈V (G) b(v) =
O(n) we can solve the uncapacitated Maximum Weight b-Matching Problem
in O(n3) time (using the Weighted Matching Algorithm; cf. Corollary 11.10).
Pulleyblank [1973,1980] described the facets of this polytope and showed that the
linear inequality system in Theorem 12.2 is TDI. The following generalization
allows ﬁnite capacities:
Theorem 12.3.
(Edmonds and Johnson [1970]) Let G be an undirected graph,
u : E(G) →N ∪{∞} and b : V (G) →N. The b-matching polytope of (G, u) is
the set of vectors x ∈RE(G)
+
satisfying
xe
≤
u(e)
(e ∈E(G));

e∈δ(v)
xe
≤
b(v)
(v ∈V (G));

e∈E(G[X])
xe +

e∈F
xe
≤
:
1
2

v∈X
b(v) +

e∈F
u(e)
;
(X ⊆V (G),
F ⊆δ(X)).
Proof:
First observe that the incidence vector of any b-matching f satisﬁes the
constraints. This is clear except for the last one; here we argue as follows. Let
X ⊆V (G) and F ⊆δ(X). We have a budget of b(v) units at each vertex v ∈X
and a budget of u(e) units for each e ∈F. Now for each e ∈E(G[X]) we take
f (e) units from the budget at each vertex incident to e. For each e ∈F, say
e = {x, y} with x ∈X, we take f (e) units from the budget at x and f (e) units
from the budget at e. It is clear that the budgets are not exceeded, and we have
taken 2 
e∈E(G[X])∪F f (e) units. So

e∈E(G[X])
xe +

e∈F
xe ≤1
2

v∈X
b(v) +

e∈F
u(e)

.
Since the left-hand side is an integer, so is the right-hand side; thus we may round
down.
Now let x ∈RE(G)
+
be a vector with xe ≤u(e) for all e ∈E(G), 
e∈δ(v) xe ≤
b(v) for all v ∈V (G) and

e∈E(G[X])
xe +

e∈F
xe ≤
:
1
2

v∈X
b(v) +

e∈F
u(e)
;
for all X ⊆V (G) and F ⊆δ(X). We show that x is a convex combination of
incidence vectors of b-matchings in (G, u).
Let H be the graph resulting from G by subdividing each edge e = {v, w}
with u(e) ̸= ∞by means of two new vertices (e, v), (e, w). (Instead of e, H now
contains the edges {v, (e, v)}, {(e, v), (e, w)} and {(e, w), w}.) Set b((e, v)) :=
b((e, w)) := u(e) for the new vertices.

274
12. b-Matchings and T -Joins
For each subdivided edge e = {v, w} set y{v,(e,v)} := y{(e,w),w} := xe and
y{(e,v),(e,w)} := u(e) −xe. For each original edge e with u(e) = ∞set ye := xe.
We claim that y is in the b-matching polytope P of (H, ∞).
We use Theorem 12.2. Obviously y ∈RE(H)
+
and 
e∈δ(v) ye ≤b(v) for all
v ∈V (H). Suppose there is a set A ⊆V (H) with

e∈E(H[A])
ye >
:
1
2

a∈A
b(a)
;
.
(12.1)
Let B := A ∩V (G). For each e = {v, w} ∈E(G[B]) we may assume
(e, v), (e, w) ∈A, for otherwise the addition of (e, v) and (e, w) does not de-
stroy (12.1). On the other hand, we may assume that (e, v) ∈A implies v ∈A:
If (e, v), (e, w) ∈A but v /∈A, we can delete (e, v) and (e, w) from A without
destroying (12.1). If (e, v) ∈A but v, (e, w) /∈A, we can just delete (e, v) from
A. Figure 12.1 shows the remaining possible edge types.
A
Fig. 12.1.
Let F := {e = {v, w} ∈E(G) : |A ∩{(e, v), (e, w)}| = 1}. We have

e∈E(G[B])
xe +

e∈F
xe
=

e∈E(H[A])
ye −

e∈E(G[B]),
u(e)<∞
u(e)
>
:
1
2

a∈A
b(a)
;
−

e∈E(G[B]),
u(e)<∞
u(e)
=
:
1
2

v∈B
b(v) +

e∈F
u(e)
;
,
contradicting our assumption. So y ∈P, and in fact y belongs to the face
⎧
⎨
⎩z ∈P :

e∈δ(v)
ze = b(v) for all v ∈V (H) \ V (G)
⎫
⎬
⎭

12.2 Minimum Weight T -Joins
275
of P. Since the vertices of this face are also vertices of P, y is a con-
vex combination of b-matchings f1, . . . , fm in (H, ∞), each of which satisﬁes

e∈δ(v) fi(e) = b(v) for all v ∈V (H) \ V (G). This implies fi({v, (e, v)}) =
fi({(e, w), w}) ≤u(e) for each subdivided edge e = {v, w} ∈E(G). By returning
from H to G we obtain that x is a convex combination of incidence vectors of
b-matchings in (G, u).
2
The constructions in the proofs of Theorems 12.2 and 12.3 are both due to
Tutte [1954]. They can also be used to prove a generalization of Tutte’s Theorem
10.13 (Exercise 4):
Theorem 12.4.
(Tutte [1952])
Let G be a graph, u : E(G) →N ∪{∞} and
b : V (G) →N. Then (G, u) has a perfect b-matching if and only if for any two
disjoint subsets X, Y ⊆V (G) the number of connected components C in G−X−Y,
for which 
c∈V (C) b(c) + 
e∈EG(V (C),Y) u(e) is odd, does not exceed

v∈X
b(v) +

y∈Y
⎛
⎝
e∈δ(y)
u(e) −b(y)
⎞
⎠−

e∈EG(X,Y)
u(e).
12.2 Minimum Weight T-Joins
Consider the following problem: A postman has to deliver the mail within his
district. To do this, he must start at the post ofﬁce, walk along each street at least
once, and ﬁnally return to the post ofﬁce. The problem is to ﬁnd a postman’s tour
of minimum length. This is known as the Chinese Postman Problem (Guan
[1962]).
Of course we model the street map as a graph which we assume to be con-
nected. (Otherwise the problem becomes NP-hard; see Exercise 14(d) of Chapter
15.) By Euler’s Theorem 2.24 we know that there is a postman’s tour using each
edge exactly once (i.e. an Eulerian walk) if and only if every vertex has even
degree.
If the graph is not Eulerian, we have to use some edges several times. Knowing
Euler’s Theorem, we can formulate the Chinese Postman Problem as follows:
given a graph G with weights c : E(G) →R+, ﬁnd a function n : E(G) →N
such that G′, the graph which arises from G by taking n(e) copies of each edge
e ∈E(G), is Eulerian and 
e∈E(G) n(e)c(e) is minimum.
All this is true in the directed and undirected case. In the directed case, the
problem can be solved with network ﬂow techniques (Exercise 9 of Chapter 9).
Hence from now on we shall deal with undirected graphs only. Here we need the
Weighted Matching Algorithm.
Of course it makes no sense to walk through an edge e more than twice,
because then we may subtract 2 from some n(e) and obtain a solution that cannot
be worse. So the problem is to ﬁnd a minimum weight J ⊆E(G) such that

276
12. b-Matchings and T -Joins
(V (G), E(G)
.
∪J) (the graph we obtain by doubling the edges in J) is Eulerian.
In this section, we solve a generalization of this problem.
Deﬁnition 12.5.
Given an undirected graph G and a set T ⊆V (G) of even
cardinality. A set J ⊆E(G) is a T-join if |J ∩δ(x)| is odd if and only if x ∈T .
Minimum Weight T -Join Problem
Instance:
An undirected graph G, weights c : E(G) →R, and a set T ⊆
V (G) of even cardinality.
Task:
Find a minimum weight T -join in G or decide that none exists.
The Minimum Weight T -Join Problem generalizes several combinatorial
optimization problems:
– If c is nonnegative and T is the set of vertices having odd degree in G, then
we have the Undirected Chinese Postman Problem.
– If T = ∅, the T -joins are exactly the Eulerian subgraphs. So the empty set is
a minimum weight ∅-join if and only if c is conservative.
– If |T | = 2, say T = {s, t}, each T -join is the union of an s-t-path and
possibly some circuits. So if c is conservative, the minimum T -join problem
is equivalent to the Shortest Path Problem. (Note that we were not able to
solve the Shortest Path Problem in undirected graphs in Chapter 7, except
for nonnegative weights.)
– If T = V (G), the T -joins of cardinality |V (G)|
2
are exactly the perfect match-
ings. So the Minimum Weight Perfect Matching Problem can be reduced
to the Minimum Weight T -Join Problem by adding a large constant to each
edge weight.
The main purpose of this section is to give a polynomial-time algorithm for
the Minimum Weight T -Join Problem. The question whether a T -join exists at
all can be answered easily:
Proposition 12.6.
Let G be a graph and T ⊆V (G) with |T | even. There exists
a T -join in G if and only if |V (C) ∩T | is even for each connected component C
of G.
Proof:
If J is a T -join, then for each connected component C of G we have
that 
v∈V (C) |J ∩δ(v)| = 2|J ∩E(C)|, so |J ∩δ(v)| is odd for an even number
of vertices v ∈V (C). Since J is a T -join, this means that |V (C) ∩T | is even.
Conversely, let |V (C) ∩T | be even for each connected component C of G.
Then T can be partitioned into pairs {v1, w1}, . . . , {vk, wk} with k =
|T |
2 , such
that vi and wi are in the same connected component for i = 1, . . . , k. Let Pi be
some vi-wi-path (i = 1, . . . , k), and let J := E(P1)△E(P2)△· · · △E(Pk). Since
the degree of each vertex has the same parity with respect to the edge sets J and
E(P1)
.
∪E(P2)
.
∪· · ·
.
∪E(Pk), we conclude that J is a T -join.
2
A simple optimality criterion is:

12.2 Minimum Weight T -Joins
277
Proposition 12.7.
A T -join J in a graph G with weights c : E(G) →R has
minimum weight if and only if c(J ∩E(C)) ≤c(E(C)\ J) for each circuit C in G.
Proof:
If c(J ∩E(C)) > c(E(C)\ J), then J△E(C) is a T -join whose weight is
less than the weight of J. On the other hand, if J ′ is a T -join with c(J ′) < c(J),
J ′△J is Eulerian, i.e. the union of circuits, where for at least one circuit C we
have c(J ∩E(C)) > c(J ′ ∩E(C)) = c(E(C) \ J).
2
This proposition can be regarded as a special case of Theorem 9.6. We now
solve the Minimum Weight T -Join Problem with nonnegative weights by re-
ducing it to the Minimum Weight Perfect Matching Problem. The main idea
is contained in the following lemma:
Lemma 12.8.
Let G be a graph, c : E(G) →R+, and T ⊆V (G) with |T | even.
Every optimum T -join in G is the disjoint union of the edge sets of |T |
2 paths whose
ends are distinct and in T , and possibly some zero-weight circuits.
Proof:
By induction on |T |. The case T = ∅is trivial since the minimum weight
of an ∅-join is zero.
Let J be any optimum T -join in G; w.l.o.g. J contains no zero-weight circuit.
By Proposition 12.7 J contains no circuit of positive weight. As c is nonnegative,
J thus forms a forest. Let x, y be two leaves of the same connected component,
i.e. |J ∩δ(x)| = |J ∩δ(y)| = 1, and let P be the x-y-path in J. We have x, y ∈T ,
and J \ E(P) is a minimum cost (T \ {x, y})-join (a cheaper (T \ {x, y})-join J ′
would imply a T -join J ′△E(P) that is cheaper than J). The assertion now follows
from the induction hypothesis.
2
Theorem 12.9.
(Edmonds and Johnson [1973])
In the case of nonnegative
weights, the Minimum Weight T -Join Problem can be solved in O(n3) time.
Proof:
Let (G, c, T ) be an instance. We ﬁrst solve an All Pairs Shortest Paths
Problem in (G, c); more precisely: in the graph resulting by replacing each edge
by a pair of oppositely directed edges with the same weight. By Theorem 7.9 this
takes O(mn + n2 log n) time. In particular, we obtain the metric closure ( ¯G, ¯c) of
(G, c) (cf. Corollary 7.11).
Now we ﬁnd a minimum weight perfect matching M in ( ¯G[T ], ¯c). By Corol-
lary 11.10, this takes O(n3) time. By Lemma 12.8, ¯c(M) is at most the minimum
weight of a T -join.
We consider the shortest x-y-path in G for each {x, y} ∈M (which we have
already computed). Let J be the symmetric difference of the edge sets of all these
paths. Evidently, J is a T -join in G. Moreover, c(J) ≤¯c(M), so J is optimum.
2
This method no longer works if we allow negative weights, because we would
introduce negative circuits. However, we can reduce the Minimum Weight T -Join
Problem with arbitrary weights to that with nonnegative weights:

278
12. b-Matchings and T -Joins
Theorem 12.10.
Let G be a graph with weights c : E(G) →R, and T ⊆V (G)
a vertex set of even cardinality. Let E−be the set of edges with negative weight,
T −the set of vertices that are incident with an odd number of negative edges, and
d : E(G) →R+ with d(e) := |c(e)|.
Then J is a minimum c-weight T -join if and only if J△E−is a minimum d-
weight (T △T −)-join.
Proof:
For any subset J of E(G) we have
c(J)
=
c(J \ E−) + c(J ∩E−)
=
c(J \ E−) + c(J ∩E−) + c(E−\ J) + d(E−\ J)
=
d(J \ E−) + c(J ∩E−) + c(E−\ J) + d(E−\ J)
=
d(J△E−) + c(E−) .
Now J is a T -join if and only if J△E−is a (T △T −)-join, which together
with the above equality proves the theorem (since c(E−) is constant).
2
Corollary 12.11.
The Minimum Weight T -Join Problem can be solved in
O(n3) time.
Proof:
This follows directly from Theorems 12.9 and 12.10.
2
In fact, using the fastest known implementation of the Weighted Matching
Algorithm, a minimum weight T -join can be computed in O(nm + n2 log n)
time.
We are ﬁnally able to solve the Shortest Path Problem in undirected graphs:
Corollary 12.12.
The problem of ﬁnding a shortest path between two speciﬁed
vertices in an undirected graph with conservative weights can be solved in O(n3)
time.
Proof:
Let s and t be the two speciﬁed vertices. Set T := {s, t} and apply Corol-
lary 12.11. After deleting zero-weight circuits, the resulting T -join is a shortest
s-t-path.
2
Of course this also implies an O(mn3)-algorithm for ﬁnding a circuit of mini-
mum total weight in an undirected graph with conservative weights (and in partic-
ular to compute the girth). If we are interested in the All Pairs Shortest Paths
Problem in undirected graphs, we do not have to do
n
2

independent weighted
matching computations (which would give a running time of O(n5)). Using the
postoptimality results of Section 11.4 we can prove:
Theorem 12.13.
The problem of ﬁnding shortest paths for all pairs of vertices in
an undirected graph G with conservative weights c : E(G) →R can be solved in
O(n4) time.

12.3 T -Joins and T -Cuts
279
Proof:
By Theorem 12.10 and the proof of Corollary 12.12 we have to compute
an optimum

{s, t}△T −
-join with respect to the weights d(e) := |c(e)| for all
s, t ∈V (G), where T −is the set of vertices incident to an odd number of negative
edges. Let ¯d({x, y}) := dist(G,d)(x, y) for x, y ∈V (G), and let HX be the complete
graph on X△T −(X ⊆V (G)). By the proof of Theorem 12.9 it is sufﬁcient to
compute a minimum weight perfect matching in

H{s,t}, ¯d

for all s and t.
Our O(n4)-algorithm proceeds as follows. We ﬁrst compute ¯d (cf. Corollary
7.11) and run the Weighted Matching Algorithm for the instance (H∅, ¯d). Up
to now we have spent O(n3) time.
We show that we can now compute a minimum weight perfect matching of

H{s,t}, ¯d

in O(n2) time, for any s and t.
Let K := 
e∈E(G) ¯d(e), and let s, t ∈V (G). There are four cases:
Case 1:
s, t ∈T −. Then all we have to do is reduce the cost of the edge {s, t} to
−K. After reoptimizing (using Lemma 11.11), {s, t} must belong to the optimum
matching M, and M \{{s, t}} is a minimum weight perfect matching of

H{s,t}, ¯d

.
Case 2:
s ∈T −and t ̸∈T −. Then the cost of the edge {s, v} is set to ¯d({t, v})
for all v ∈T −\ {s}. Now s plays the role of t, and reoptimizing (using Lemma
11.11) does the job.
Case 3:
s ̸∈T −and t ∈T −. Symmetric to Case 2.
Case 4:
s, t ̸∈T −. Then we add these two vertices and apply Lemma 11.12.
2
12.3 T-Joins and T-Cuts
In this section we shall derive a polyhedral description of the Minimum Weight
T -Join Problem. In contrast to the description of the perfect matching polytope
(Theorem 11.13), where we had a constraint for each cut δ(X) with |X| odd, we
now need a constraint for each T -cut. A T-cut is a cut δ(X) with |X ∩T | odd.
The following simple observation is very useful:
Proposition 12.14.
Let G be an undirected graph and T ⊆V (G) with |T | even.
Then for any T -join J and any T -cut C we have J ∩C ̸= ∅.
Proof:
Suppose C = δ(X), then |X ∩T | is odd. So the number of edges in J ∩C
must be odd, in particular nonzero.
2
A stronger statement can be found in Exercise 11.
Proposition 12.14 implies that the minimum cardinality of a T -join is not less
than the maximum number of edge-disjoint T -cuts. In general, we do not have
equality: consider G = K4 and T = V (G). However, for bipartite graphs equality
holds:
Theorem 12.15.
(Seymour [1981])
Let G be a connected bipartite graph and
T ⊆V (G) with |T | even. Then the minimum cardinality of a T -join equals the
maximum number of edge-disjoint T -cuts.

280
12. b-Matchings and T -Joins
Proof:
(Seb˝o [1987])
We only have to prove “≤”. We use induction on
|V (G)|. If T = ∅(in particular if |V (G)| = 1), the statement is trivial. So we
assume |V (G)| ≥|T | ≥2. Denote by τ(G, T ) the minimum cardinality of a
T -join in G. Choose a, b ∈V (G), a ̸= b, such that τ(G, T △{a, b}) is minimum.
Let T ′ := T △{a, b}. Since we may assume T ̸= ∅, τ(G, T ′) < τ(G, T ).
Claim:
For any minimum T -join J in G we have |J ∩δ(a)| = |J ∩δ(b)| = 1.
To prove this claim, let J ′ be a minimum T ′-join. J△J ′ is the edge-disjoint
union of an a-b-path P and some circuits C1, . . . , Ck. We have |Ci ∩J| = |Ci ∩
J ′| for each i, because both J and J ′ are minimum. So |J△P| = |J ′|, and
J ′′ := J△P is also a minimum T ′-join. Now J ′′ ∩δ(a) = J ′′ ∩δ(b) = ∅,
because if, say, {b, b′} ∈J ′′, J ′′ \ {{b, b′}} is a (T △{a}△{b′})-join, and we have
τ(G, T △{a}△{b′}) < |J ′′| = |J ′| = τ(G, T ′), contradicting the choice of a and
b. We conclude that |J ∩δ(a)| = |J ∩δ(b)| = 1, and the claim is proved.
In particular, a, b ∈T . Now let J be a minimum T -join in G. Contract
B := {b} ∪(b) to a single vertex vB, and let the resulting graph be G∗. G∗is
also bipartite. Let T ∗:= T \ B if |T ∩B| is even and T ∗:= (T \ B) ∪{vB}
otherwise. The set J ∗, resulting from J by the contraction of B, is obviously a
T ∗-join in G∗. Since (b) is a stable set in G (as G is bipartite), the claim implies
that |J| = |J ∗| + 1.
It sufﬁces to prove that J ∗is a minimum T ∗-join in G∗, because then we have
τ(G, T ) = |J| = |J ∗| + 1 = τ(G∗, T ∗) + 1, and the theorem follows by induction
(observe that δ(b) is a T -cut in G disjoint from E(G∗)).
So suppose that J ∗is not a minimum T ∗-join in G∗. Then by Proposition 12.7
there is a circuit C∗in G∗with |J ∗∩E(C∗)| > |E(C∗)\ J ∗|. Since G∗is bipartite,
|J ∗∩E(C∗)| ≥|E(C∗) \ J ∗| + 2. E(C∗) corresponds to an edge set Q in G. Q
cannot be a circuit, because |J ∩Q| > |Q \ J| and J is a minimum T -join. Hence
Q is an x-y-path in G for some x, y ∈(b) with x ̸= y. Let C be the circuit in G
formed by Q together with {x, b} and {b, y}. Since J is a minimum T -join in G,
|J ∩E(C)| ≤|E(C) \ J| ≤|E(C∗) \ J ∗| + 2 ≤|J ∗∩E(C∗)| ≤|J ∩E(C)|.
Thus we must have equality throughout, in particular {x, b}, {b, y} /∈J and |J ∩
E(C)| = |E(C)\ J|. So ¯J := J△E(C) is also a minimum T -join and | ¯J ∩δ(b)| =
3. But this is impossible by the claim.
2
T -cuts are also essential in the following description of the T -join polyhedron:
Theorem 12.16.
(Edmonds and Johnson [1973])
Let G be an undirected graph,
c : E(G) →R+, and T ⊆V (G) with |T | even. Then the incidence vector of a
minimum weight T -join is an optimum solution of the LP
min

cx : x ≥0,

e∈C
xe ≥1 for all T -cuts C

.
(This polyhedron is called the T-join polyhedron of G.)

12.3 T -Joins and T -Cuts
281
Proof:
By Proposition 12.14, the incidence vector of a T -join satisﬁes the con-
straints. Let c : E(G) →R+ be given; we may assume that c(e) is an even integer
for each e ∈E(G). Let k be the minimum weight (with respect to c) of a T -join
in G. We show that the optimum value of the above LP is k.
We replace each edge e by a path of length c(e) (if c(e) = 0 we contract e
and add the contracted vertex to T iff |e ∩T | = 1). The resulting graph G′ is
bipartite. Moreover, the minimum cardinality of a T -join in G′ is k. By Theorem
12.15, there is a family C′ of k edge-disjoint T -cuts in G′. Back in G, this yields
a family C of k T -cuts in G such that every edge e is contained in at most c(e)
of these. So for any feasible solution x of the above LP we have
cx ≥

C∈C

e∈C
xe ≥

C∈C
1 = k,
proving that the optimum value is k.
2
This implies Theorem 11.13: let G be a graph with a perfect matching and
T := V (G). Then Theorem 12.16 implies that
min

cx : x ≥0,

e∈C
xe ≥1 for all T -cuts C

is an integer for each c ∈ZE(G) for which the minimum is ﬁnite. By Theorem
5.12, the polyhedron is integral, and so is its face
⎧
⎨
⎩x ∈RE(G)
+
:

e∈C
xe ≥1 for all T -cuts C,

e∈δ(v)
xe = 1 for all v ∈V (G)
⎫
⎬
⎭.
One can also derive a description of the convex hull of the incidence vectors
of all T -joins (Exercise 14). Theorems 12.16 and 4.21 (along with Corollary
3.28) imply another polynomial-time algorithm for the Minimum Weight T -Join
Problem if we can solve the Separation Problem for the above description. This
is obviously equivalent to checking whether there exists a T -cut with capacity less
than one (here x serves as capacity vector). So it sufﬁces to solve the following
problem:
Minimum Capacity T -Cut Problem
Instance:
A graph G, capacities u : E(G) →R+, and a set T ⊆V (G) of
even cardinality.
Task:
Find a minimum capacity T -cut in G.
Note that the Minimum Capacity T -Cut Problem also solves the Separation
Problem for the perfect matching polytope (Theorem 11.13; T := V (G)). The
following theorem solves the Minimum Capacity T -Cut Problem: it sufﬁces to
consider the fundamental cuts of a Gomory-Hu tree. Recall that we can ﬁnd a
Gomory-Hu tree for an undirected graph with capacities in O(n4) time (Theorem
8.35).

282
12. b-Matchings and T -Joins
Theorem 12.17.
(Padberg and Rao [1982]) Let G be an undirected graph with
capacities u : E(G) →R+. Let H be a Gomory-Hu tree for (G, u). Let T ⊆V (G)
with |T | even. Then there is a minimum capacity T -cut among the fundamental cuts
of H. Hence the minimum capacity T -cut can be found in O(n4) time.
Proof:
We consider the pair (G + H, u′) with u′(e) = u(e) for e ∈E(G)
and u′(e) = 0 for e ∈E(H). Let A ⊆E(G) ∪E(H) be a minimum T -cut in
(G + H, u′). Obviously u′(A) = u(A ∩E(G)) and A ∩E(G) is a minimum T -cut
in (G, u).
Let now J be the set of edges e of H for which δG(Ce) is a T -cut. It is easy
to see that J is a T -join (in G + H). By Proposition 12.14, there exists an edge
e = {v, w} ∈A ∩J. We have
u(A ∩E(G)) ≥λvw =

{x,y}∈δG(Ce)
u({x, y}),
showing that δG(Ce) is a minimum T -cut.
2
12.4 The Padberg-Rao Theorem
The solution of the Minimum Capacity T -Cut Problem also helps us to solve
the Separation Problem for the b-matching polytope (Theorem 12.3):
Theorem 12.18.
(Padberg and Rao [1982])
For undirected graphs G, u :
E(G) →N ∪{∞} and b : V (G) →N, the Separation Problem for the b-
matching polytope of (G, u) can be solved in polynomial time.
Proof:
We may assume u(e) < ∞for all edges e (we may replace inﬁnite
capacities by a large enough number, e.g. max{b(v) : v ∈V (G)}). We choose an
arbitrary but ﬁxed orientation of G; we will sometimes use the resulting directed
edges and sometimes the original undirected edges.
Given a vector x ∈RE(G)
+
with xe ≤u(e) for all e ∈E(G) and 
e∈δG(v) xe ≤
b(v) for all v ∈V (G) (these trivial inequalities can be checked in linear time), we
deﬁne a new bipartite graph H with edge capacities t : E(H) →R+ as follows:
V (H)
:=
V (G)
.
∪E(G)
.
∪{S},
E(H)
:=
{{v, e} : v ∈e ∈E(G)} ∪{{v, S} : v ∈V (G)},
t({v, e})
:=
u(e) −xe
(e ∈E(G), where v is the tail of e),
t({v, e})
:=
xe
(e ∈E(G), where v is the head of e),
t({v, S})
:=
b(v) −

e∈δG(v)
xe
(v ∈V (G)).
Deﬁne T ⊆V (H) to consist of
– the vertices v ∈V (G) for which b(v) + 
e∈δ+
G(v) u(e) is odd,

12.4 The Padberg-Rao Theorem
283
– the vertices e ∈E(G) for which u(e) is odd, and
– the vertex S if 
v∈V (G) b(v) is odd.
Observe that |T | is even.
We shall prove that there exists a T -cut in H with capacity less than one if
and only if x is not in the convex hull of the b-matchings in (G, u).
E3
E1
E4
E2
/∈F
∈F
X
Fig. 12.2.
We need some preparation. Let X ⊆V (G) and F ⊆δG(X). Deﬁne
E1
:=
{e ∈δ+
G(X) ∩F},
E2
:=
{e ∈δ−
G(X) ∩F},
E3
:=
{e ∈δ+
G(X) \ F},
E4
:=
{e ∈δ−
G(X) \ F},
(see Figure 12.2) and
W := X ∪E(G[X]) ∪E2 ∪E3 ⊆V (H).
Claim:
(a) |W ∩T | is odd if and only if

v∈X b(v) + 
e∈F u(e) is odd.
(b) 
e∈δH(W) t(e) < 1 if and only if

e∈E(G[X])
xe +

e∈F
xe > 1
2

v∈X
b(v) +

e∈F
u(e) −1

.
To prove (a), observe that by deﬁnition |W ∩T | is odd if and only if

v∈X
⎛
⎝b(v) +

e∈δ+
G(v)
u(e)
⎞
⎠+

e∈E(G[X])∪E2∪E3
u(e)
is odd. But this number is equal to

284
12. b-Matchings and T -Joins

v∈X
b(v) + 2

e∈E(G[X])
u(e) +

e∈δ+
G(X)
u(e) +

e∈E2∪E3
u(e)
=

v∈X
b(v) + 2

e∈E(G[X])
u(e) + 2

e∈δ+
G(X)
u(e) −2

e∈E1
u(e) +

e∈E1∪E2
u(e),
proving (a), because E1 ∪E2 = F. Moreover,

e∈δH(W)
t(e)
=

e∈E1∪E4
x∈e∩X
t({x, e}) +

e∈E2∪E3
y∈e\X
t({y, e}) +

x∈X
t({x, S})
=

e∈E1∪E2
(u(e) −xe) +

e∈E3∪E4
xe +

v∈X
⎛
⎝b(v) −

e∈δG(v)
xe
⎞
⎠
=

e∈F
u(e) +

v∈X
b(v) −2

e∈F
xe −2

e∈E(G[X])
xe,
proving (b).
Now we can prove that there exists a T -cut in H with capacity less than one if
and only if x is not in the convex hull of the b-matchings in (G, u). First suppose
that there are X ⊆V (G) and F ⊆δG(X) with

e∈E(G[X])
xe +

e∈F
xe >
:
1
2

v∈X
b(v) +

e∈F
u(e)
;
.
Then 
v∈X b(v) + 
e∈F u(e) must be odd and

e∈E(G[X])
xe +

e∈F
xe > 1
2

v∈X
b(v) +

e∈F
u(e) −1

.
By (a) and (b), this implies that δH(W) is a T -cut with capacity less than one.
To prove the converse, let δH(W) now be any T -cut in H with capacity
less than one. We show how to construct a violated inequality of the b-matching
polytope.
W.l.o.g. assume S ̸∈W (otherwise exchange W and V (H) \ W). Deﬁne X :=
W ∩V (G). Observe that {v, {v, w}} ∈δH(W) implies {v, w} ∈δG(X): If {v, w} /∈
W for some v, w ∈X, the two edges {v, {v, w}} and {w, {v, w}} (with total
capacity u({v, w})) would belong to δH(W), contradicting the assumption that this
cut has capacity less than one. The assumption {v, w} ∈W for some v, w /∈X
leads to the same contradiction.
Deﬁne
F := {(v, w) ∈E(G) : {v, {v, w}} ∈δH(W)}.
By the above observation we have F ⊆δG(X). We deﬁne E1, E2, E3, E4 as above
and claim that
W = X ∪E(G[X]) ∪E2 ∪E3
(12.2)

Exercises
285
holds. Again by the above observation, we only have to prove W ∩δG(X) =
E2 ∪E3. But e = (v, w) ∈E1 = δ+
G(X) ∩F implies e /∈W by the deﬁnition of
F. Similarly, e = (v, w) ∈E2 = δ−
G(X) ∩F implies e ∈W, e = (v, w) ∈E3 =
δ+
G(X) \ F implies e ∈W, and e = (v, w) ∈E4 = δ−
G(X) \ F implies e /∈W.
Thus (12.2) is proved.
So (a) and (b) again hold. Since |W ∩T | is odd, (a) implies that 
v∈X b(v) +

e∈F u(e) is odd. Then by (b) and the assumption that 
e∈δH(W) t(e) < 1, we get

e∈E(G[X])
xe +

e∈F
xe >
:
1
2

v∈X
b(v) +

e∈F
u(e)
;
,
i.e. a violated inequality of the b-matching polytope.
Let us summarize: We have shown that the minimum capacity of a T -cut in
H is less than one if and only if x violates some inequality of the b-matching
polytope. Furthermore, given some T -cut in H with capacity less than one, we
can easily construct a violated inequality. So the problem reduces to the Minimum
Capacity T -CutProblem with nonnegative weights. By Theorem 12.17, the latter
can be solved in O(n4) time, where n = |V (H)|.
2
A generalization of this result has been found by Caprara and Fischetti [1996].
Letchford, Reinelt and Theis [2004] showed that it sufﬁces to consider the Gomory-
Hu tree for (G, u). They reduce the Separation Problem for b-matching (and
more general) inequalities to |V (G)| maximum ﬂow computations on the original
graph and thus solve it in O(|V (G)|4) time.
The Padberg-Rao Theorem implies:
Corollary 12.19.
The Maximum Weight b-Matching Problem can be solved
in polynomial time.
Proof:
By Corollary 3.28 we have to solve the LP given in Theorem 12.3. By
Theorem 4.21 it sufﬁces to have a polynomial-time algorithm for the Separation
Problem. Such an algorithm is provided by Theorem 12.18.
2
Marsh [1979] extended Edmonds’ Weighted Matching Algorithm to the
Maximum Weight b-Matching Problem. This combinatorial algorithm is of
course more practical than using the Ellipsoid Method. But Theorem 12.18 is
also interesting for other purposes (see e.g. Section 21.4). For a combinatorial
algorithm with a strongly polynomial running time, see Anstee [1987] or Gerards
[1995].
Exercises
1. Show that a minimum weight perfect simple 2-matching in an undirected
graph G can be found in O(n6) time.

286
12. b-Matchings and T -Joins
2.
∗
Let G be an undirected graph and b1, b2 : V (G) →N. Describe the convex
hull of functions f : E(G) →Z+ with b1(v) ≤
e∈δ(v) f (e) ≤b2(v).
Hint: For X, Y ⊆V (G) with X ∩Y = ∅consider the constraint

e∈E(G[X])
f (e) −

e∈E(G[Y])∪E(Y,Z)
f (e) ≤
⎢⎢⎢⎣1
2
⎛
⎝
x∈X
b2(x) −

y∈Y
b1(y)
⎞
⎠
⎥⎥⎥⎦,
where Z := V (G) \ (X ∪Y). Use Theorem 12.3.
(Schrijver [1983])
3.
∗
Can one generalize the result of Exercise 2 further by introducing lower and
upper capacities on the edges?
Note: This can be regarded as an undirected version of the problem in Exercise
3 of Chapter 9. For a common generalization of both problems and also the
Minimum Weight T -Join Problem see the papers of Edmonds and Johnson
[1973], and Schrijver [1983]. Even here a description of the polytope that is
TDI is known.
4.
∗
Prove Theorem 12.4.
Hint: For the sufﬁciency, use Tutte’s Theorem 10.13 and the constructions in
the proofs of Theorems 12.2 and 12.3.
5. The subgraph degree polytope of a graph G is deﬁned to be the convex hull of
all vectors b ∈ZV (G)
+
such that G has a perfect simple b-matching. Prove that
its dimension is |V (G)| −k, where k is the number of connected components
of G that are bipartite.
6.
∗
Given an undirected graph, an odd cycle cover is deﬁned to be a subset of
edges containing at least one edge of each odd circuit. Show how to ﬁnd in
polynomial time the minimum weight odd cycle cover in a planar graph with
nonnegative weights on the edges. Can you also solve the problem for general
weights?
Hint: Consider the Undirected Chinese Postman Problem in the planar
dual graph and use Theorem 2.26 and Corollary 2.45.
7. Consider the Maximum Weight Cut Problem in planar graphs: Given an
undirected planar graph G with weights c : E(G) →R+, we look for the
maximum weight cut. Can one solve this problem in polynomial time?
Hint: Use Exercise 6.
Note: For general graphs this problem is NP-hard; see Exercise 3 of Chapter
16.
(Hadlock [1975])
8. Given a graph G with weights c : E(G) →R+ and a set T ⊆V (G) with |T |
even. We construct a new graph G′ by setting
V (G′)
:=
{(v, e) : v ∈e ∈E(G)} ∪
{¯v : v ∈V (G), |δG(v)| + |{v} ∩T | odd},
E(G′)
:=
{{(v, e), (w, e)} : e = {v, w} ∈E(G)} ∪
{{(v, e), (v, f )} : v ∈V (G), e, f ∈δG(v), e ̸= f } ∪

Exercises
287
{{¯v, (v, e)} : v ∈e ∈E(G), ¯v ∈V (G′)},
and deﬁne c′({(v, e), (w, e)}) := c(e) for e = {v, w} ∈E(G) and c′(e′) = 0
for all other edges in G′.
Show that a minimum weight perfect matching in G′ corresponds to a mini-
mum weight T -join in G. Is this reduction preferable to the one used in the
proof of Theorem 12.9?
9.
∗
The following problem combines simple perfect b-matchings and T -joins. We
are given an undirected graph G with weights c : E(G) →R, a partition of
the vertex set V (G) = R
.
∪S
.
∪T , and a function b : R →Z+. We ask for
a subset of edges J ⊆E(G) such that J ∩δ(v) = b(v) for v ∈R, |J ∩δ(v)|
is even for v ∈S, and |J ∩δ(v)| is odd for v ∈T . Show how to reduce this
problem to a Minimum Weight Perfect Matching Problem.
Hint: Consider the constructions in Section 12.1 and Exercise 8.
10. Consider the Undirected Minimum Mean Cycle Problem: Given an undi-
rected graph G and weights c : E(G) →R, ﬁnd a circuit C in G whose mean
weight c(E(C))
|E(C)| is minimum.
(a) Show that the Minimum Mean Cycle Algorithm of Section 7.3 cannot
be applied to the undirected case.
(b)
∗
Find a strongly polynomial algorithm for the Undirected Minimum
Mean Cycle Problem.
Hint: Use Exercise 9.
11. Let G be an undirected graph, T ⊆V (G) with |T | even, and F ⊆E(G).
Prove: F has nonzero intersection with every T -join if and only if F contains
a T -cut. F has nonzero intersection with every T -cut if and only if F contains
a T -join.
12.
∗
Let G be a planar 2-connected graph with a ﬁxed embedding, let C be the
circuit bounding the outer face, and let T be an even cardinality subset of
V (C). Prove that the minimum cardinality of a T -join equals the maximum
number of edge-disjoint T -cuts.
Hint: Colour the edges of C red and blue such that, when traversing C, colours
change precisely at the vertices in T . Consider the planar dual graph, split the
vertex representing the outer face into a red and a blue vertex, and apply
Menger’s Theorem 8.9.
13. Prove Theorem 12.16 using Theorem 11.13 and the construction of Exercise 8.
(Edmonds and Johnson [1973])
14. Let G be an undirected graph and T ⊆V (G) with |T | even. Prove that the
convex hull of the incidence vectors of all T -joins in G is the set of all vectors
x ∈[0, 1]E(G) satisfying

e∈δG(X)\F
xe +

e∈F
(1 −xe) ≥1
for all X ⊆V (G) and F ⊆δG(X) with |X ∩T | + |F| odd.
Hint: Use Theorems 12.16 and 12.10.

288
12. b-Matchings and T -Joins
15. Let G be an undirected graph and T ⊆V (G) with |T | = 2k even. Prove that
the minimum cardinality of a T -cut in G equals the maximum of mink
i=1 λsi,ti
over all pairings T = {s1, t1, s2, t2, . . . , sk, tk}. (λs,t denotes the maximum
number of edge-disjoint s-t-paths.) Can you think of a weighted version of
this min-max formula?
Hint: Use Theorem 12.17.
(Rizzi [2002])
16. This exercise gives an algorithm for the Minimum Capacity T -Cut Problem
without using Gomory-Hu trees. The algorithm is recursive and – given G, u
and T – proceeds as follows:
1. First we ﬁnd a set X ⊆V (G) with T ∩X ̸= ∅and T \ X ̸= ∅, such that
u(X) := 
e∈δG(X) u(e) is minimum (cf. Exercise 22 of Chapter 8). If |T ∩X|
happens to be odd, we are done (return X).
2. Otherwise we apply the algorithm recursively ﬁrst to G, u and T ∩X, and
then to G, u and T \ X. We obtain a set Y ⊆V (G) with |(T ∩X) ∩Y|
odd and u(Y) minimum and a set Z ⊆V (G) with |(T \ X) ∩Z| odd and
u(Z) minimum. W.l.o.g. T \ X ̸⊆Y and X ∩T ̸⊆Z (otherwise replace Y by
V (G) \ Y and/or Z by V (G) \ Z).
3. If u(X ∩Y) < u(Z \ X) then return X ∩Y else return Z \ X.
Show that this algorithm works correctly and that its running time is O(n5),
where n = |V (G)|.
17. Show how to solve the Maximum Weight b-Matching Problem for the
special case when b(v) is even for all v ∈V (G) in strongly polynomial time.
Hint: Reduction to a Minimum Cost Flow Problem as in Exercise 10 of
Chapter 9.
References
General Literature:
Cook, W.J., Cunningham, W.H., Pulleyblank, W.R., and Schrijver, A. [1998]: Combinato-
rial Optimization. Wiley, New York 1998, Sections 5.4 and 5.5
Frank, A. [1996]: A survey on T -joins, T -cuts, and conservative weightings. In: Combi-
natorics, Paul Erd˝os is Eighty; Volume 2 (D. Mikl´os, V.T. S´os, T. Sz˝onyi, eds.), Bolyai
Society, Budapest 1996, pp. 213–252
Gerards, A.M.H. [1995]: Matching. In: Handbooks in Operations Research and Management
Science; Volume 7: Network Models (M.O. Ball, T.L. Magnanti, C.L. Monma, G.L.
Nemhauser, eds.), Elsevier, Amsterdam 1995, pp. 135–224
Lov´asz, L., and Plummer, M.D. [1986]: Matching Theory. Akad´emiai Kiad´o, Budapest
1986, and North-Holland, Amsterdam 1986
Schrijver, A. [1983]: Min-max results in combinatorial optimization; Section 6. In: Mathe-
matical Programming; The State of the Art – Bonn 1982 (A. Bachem, M. Gr¨otschel, B.
Korte, eds.), Springer, Berlin 1983, pp. 439–500
Schrijver, A. [2003]: Combinatorial Optimization: Polyhedra and Efﬁciency. Springer,
Berlin 2003, Chapters 29–33

References
289
Cited References:
Anstee, R.P. [1987]: A polynomial algorithm for b-matchings: an alternative approach.
Information Processing Letters 24 (1987), 153–157
Caprara, A., and Fischetti, M. [1996]: {0, 1
2}-Chv´atal-Gomory cuts. Mathematical Program-
ming 74 (1996), 221–235
Edmonds, J. [1965]: Maximum matching and a polyhedron with (0,1) vertices. Journal of
Research of the National Bureau of Standards B 69 (1965), 125–130
Edmonds, J., and Johnson, E.L. [1970]: Matching: A well-solved class of integer linear pro-
grams. In: Combinatorial Structures and Their Applications; Proceedings of the Calgary
International Conference on Combinatorial Structures and Their Applications 1969 (R.
Guy, H. Hanani, N. Sauer, J. Schonheim, eds.), Gordon and Breach, New York 1970,
pp. 69–87
Edmonds, J., and Johnson, E.L. [1973]: Matching, Euler tours and the Chinese postman
problem. Mathematical Programming 5 (1973), 88–124
Guan, M. [1962]: Graphic programming using odd and even points. Chinese Mathematics
1 (1962), 273–277
Hadlock, F. [1975]: Finding a maximum cut of a planar graph in polynomial time. SIAM
Journal on Computing 4 (1975), 221–225
Letchford, A.N., Reinelt, G., and Theis, D.O. [2004]: A faster exact separation algorithm for
blossom inequalities. Proceedings of the 10th Conference on Integer Programming and
Combinatorial Optimization; LNCS 3064 (D. Bienstock, G. Nemhauser, eds.), Springer,
Berlin 2004, pp. 196–205
Marsh, A.B. [1979]: Matching algorithms. Ph.D. thesis, Johns Hopkins University, Balti-
more 1979
Padberg, M.W., and Rao, M.R. [1982]: Odd minimum cut-sets and b-matchings. Mathe-
matics of Operations Research 7 (1982), 67–80
Pulleyblank, W.R. [1973]: Faces of matching polyhedra. Ph.D. thesis, University of Water-
loo, 1973
Pulleyblank, W.R. [1980]: Dual integrality in b-matching problems. Mathematical Program-
ming Study 12 (1980), 176–196
Rizzi, R. [2002]: Minimum T -cuts and optimal T -pairings. Discrete Mathematics 257
(2002), 177–181
Seb˝o, A. [1987]: A quick proof of Seymour’s theorem on T -joins. Discrete Mathematics
64 (1987), 101–103
Seymour, P.D. [1981]: On odd cuts and multicommodity ﬂows. Proceedings of the London
Mathematical Society (3) 42 (1981), 178–192
Tutte, W.T. [1952]: The factors of graphs. Canadian Journal of Mathematics 4 (1952),
314–328
Tutte, W.T. [1954]: A short proof of the factor theorem for ﬁnite graphs. Canadian Journal
of Mathematics 6 (1954), 347–352

13. Matroids
Many combinatorial optimization problems can be formulated as follows. Given a
set system (E, F), i.e. a ﬁnite set E and some F ⊆2E, and a cost function c : F →
R, ﬁnd an element of F whose cost is minimum or maximum. In the following we
consider modular functions c, i.e. assume that c(X) = c(∅)+
x∈X(c({x})−c(∅))
for all X ⊆E; equivalently we are given a function c : E →R and write
c(X) = 
e∈X c(e).
In this chapter we restrict ourselves to those combinatorial optimization prob-
lems where F describes an independence system (i.e. is closed under subsets) or
even a matroid. The results of this chapter generalize several results obtained in
previous chapters.
In Section 13.1 we introduce independence systems and matroids and show that
many combinatorial optimization problems can be described in this context. There
are several equivalent axiom systems for matroids (Section 13.2) and an interesting
duality relation discussed in Section 13.3. The main reason why matroids are
important is that a simple greedy algorithm can be used for optimization over
matroids. We analyze greedy algorithms in Section 13.4 before turning to the
problem of optimizing over the intersection of two matroids. As shown in Sections
13.5 and 13.7 this problem can be solved in polynomial time. This also solves the
problem of covering a matroid by independent sets as discussed in Section 13.6.
13.1 Independence Systems and Matroids
Deﬁnition 13.1.
A set system (E, F) is an independence system if
(M1) ∅∈F;
(M2) If X ⊆Y ∈F then X ∈F.
The elements of F are called independent, the elements of 2E \ F dependent.
Minimal dependent sets are called circuits, maximal independent sets are called
bases. For X ⊆E, the maximal independent subsets of X are called bases of X.
Deﬁnition 13.2.
Let (E, F) be an independence system. For X ⊆E we deﬁne
the rank of X by r(X) := max{|Y| : Y ⊆X, Y ∈F}. Moreover, we deﬁne the
closure of X by σ(X) := {y ∈E : r(X ∪{y}) = r(X)}.

292
13. Matroids
Throughout this chapter, (E, F) will be an independence system, and c : E →
R will be a cost function. We shall concentrate on the following two problems:
Maximization Problem For Independence Systems
Instance:
An independence system (E, F) and c : E →R.
Task:
Find an X ∈F such that c(X) := 
e∈X c(e) is maximum.
Minimization Problem For Independence Systems
Instance:
An independence system (E, F) and c : E →R.
Task:
Find a basis B such that c(B) is minimum.
The instance speciﬁcation is somewhat vague. The set E and the cost function
c are given explicitly as usual. However, the set F is usually not given by an
explicit list of its elements. Rather one assumes an oracle which – given a subset
F ⊆E – decides whether F ∈F. We shall return to this question in Section 13.4.
The following list shows that many combinatorial optimization problems ac-
tually have one of the above two forms:
(1) Maximum Weight Stable Set Problem
Given a graph G and weights c : V (G) →R, ﬁnd a stable set X in G of
maximum weight.
Here E = V (G) and F = {F ⊆E : F is stable in G}.
(2) TSP
Given a complete undirected graph G and weights c : E(G) →R+, ﬁnd a
minimum weight Hamiltonian circuit in G.
Here E = E(G) and F = {F ⊆E : F is a subset of a Hamiltonian circuit in
G}.
(3) Shortest Path Problem
Given a digraph G, c : E(G) →R and s, t ∈V (G) such that t is reachable
from s, ﬁnd a shortest s-t-path in G with respect to c.
Here E = E(G) and F = {F ⊆E : F is a subset of an s-t-path}.
(4) Knapsack Problem
Given nonnegative numbers n, ci, wi (1 ≤i ≤n), and W, ﬁnd a subset
S ⊆{1, . . . , n} such that 
j∈S wj ≤W and 
j∈S cj is maximum.
Here E = {1, . . . , n} and F =

F ⊆E : 
j∈F wj ≤W

.
(5) Minimum Spanning Tree Problem
Given a connected undirected graph G and weights c : E(G) →R, ﬁnd a
minimum weight spanning tree in G.
Here E = E(G) and F is the set of forests in G.
(6) Maximum Weight Forest Problem
Given an undirected graph G and weights c : E(G) →R, ﬁnd a maximum
weight forest in G.
Here again E = E(G) and F is the set of forests in G.

13.1 Independence Systems and Matroids
293
(7) Steiner Tree Problem
Given a connected undirected graph G, weights c : E(G) →R+, and a set
T ⊆V (G) of terminals, ﬁnd a Steiner tree for T , i.e. a tree S with T ⊆V (S)
and E(S) ⊆E(G), such that c(E(S)) is minimum.
Here E = E(G) and F = {F ⊆E : F is a subset of a Steiner tree for T }.
(8) Maximum Weight Branching Problem
Given a digraph G and weights c : E(G) →R, ﬁnd a maximum weight
branching in G.
Here E = E(G) and F is the set of branchings in G.
(9) Maximum Weight Matching Problem
Given an undirected graph G and weights c : E(G) →R, ﬁnd a maximum
weight matching in G.
Here E = E(G) and F is the set of matchings in G.
This list contains NP-hard problems ((1),(2),(4),(7)) as well as polynomially
solvable problems ((5),(6),(8),(9)). Problem (3) is NP-hard in the above form but
polynomially solvable for nonnegative weights. (See Chapter 15.)
Deﬁnition 13.3.
An independence system is a matroid if
(M3) If X, Y ∈F and |X| > |Y|, then there is an x ∈X \ Y with Y ∪{x} ∈F.
The name matroid points out that the structure is a generalization of matrices.
This will become clear by our ﬁrst example:
Proposition 13.4.
The following independence systems (E, F) are matroids:
(a) E is a set of columns of a matrix A over some ﬁeld, and
F := {F ⊆E : The columns in F are linearly independent over that ﬁeld}.
(b) E is a set of edges of some undirected graph G and
F := {F ⊆E : (V (G), F) is a forest}.
(c) E is a ﬁnite set, k an integer and F := {F ⊆E : |F| ≤k}.
(d) E is a set of edges of some undirected graph G, S a stable set in G, ks integers
(s ∈S) and F := {F ⊆E : |δF(s)| ≤ks for all s ∈S}.
(e) E is a set of edges of some digraph G, S ⊆V (G), ks integers (s ∈S) and
F := {F ⊆E : |δ−
F (s)| ≤ks for all s ∈S}.
Proof:
In all cases it is obvious that (E, F) is indeed an independence system.
So it remains to show that (M3) holds. For (a) this is well known from Linear
Algebra, for (c) it is trivial.
To prove (M3) for (b), let X, Y ∈F and suppose Y ∪{x} ̸∈F for all x ∈
X \ Y. We show that |X| ≤|Y|. For each edge x = {v, w} ∈X, v and w are in
the same connected component of (V (G), Y). Hence each connected component
Z ⊆V (G) of (V (G), X) is a subset of a connected component of (V (G), Y). So
the number p of connected components of the forest (V (G), X) is greater than
or equal to the number q of connected components of the forest (V (G), Y). But
then |V (G)| −|X| = p ≥q = |V (G)| −|Y|, implying |X| ≤|Y|.

294
13. Matroids
To verify (M3) for (d), let X, Y ∈F with |X| > |Y|. Let S′ := {s ∈S :
|δY(s)| = ks}. As |X| > |Y| and |δX(s)| ≤ks for all s ∈S′, there exists an
e ∈X \ Y with e /∈δ(s) for s ∈S′. Then Y ∪{e} ∈F.
For (e) the proof is identical except for replacing δ by δ−.
2
Some of these matroids have special names: The matroid in (a) is called the
vector matroid of A. Let M be a matroid. If there is a matrix A over the ﬁeld
F such that M is the vector matroid of A, then M is called representable over
F. There are matroids that are not representable over any ﬁeld.
The matroid in (b) is called the cycle matroid of G and will sometimes be
denoted by M(G). A matroid which is the cycle matroid of some graph is called
a graphic matroid.
The matroids in (c) are called uniform matroids.
In our list of independence systems at the beginning of this section, the only
matroids are the graphic matroids in (5) and (6). To check that all the other
independence systems in the above list are not matroids in general is easily proved
with the help of the following theorem (Exercise 1):
Theorem 13.5.
Let (E, F) be an independence system. Then the following state-
ments are equivalent:
(M3)
If X, Y ∈F and |X| > |Y|, then there is an x ∈X \ Y with Y ∪{x} ∈F.
(M3′)
If X, Y ∈F and |X| = |Y|+1, then there is an x ∈X \Y with Y ∪{x} ∈F.
(M3′′) For each X ⊆E, all bases of X have the same cardinality.
Proof:
Trivially, (M3)⇔(M3′) and (M3)⇒(M3′′). To prove (M3′′)⇒(M3), let
X, Y ∈F and |X| > |Y|. By (M3′′), Y cannot be a basis of X ∪Y. So there must
be an x ∈(X ∪Y) \ Y = X \ Y such that Y ∪{x} ∈F.
2
Sometimes it is useful to have a second rank function:
Deﬁnition 13.6.
Let (E, F) be an independence system. For X ⊆E we deﬁne
the lower rank by
ρ(X) := min{|Y| : Y ⊆X, Y ∈F and Y ∪{x} /∈F for all x ∈X \ Y}.
The rank quotient of (E, F) is deﬁned by
q(E, F) := min
F⊆E
ρ(F)
r(F) .
Proposition 13.7.
Let (E, F) be an independence system. Then q(E, F) ≤1.
Furthermore, (E, F) is a matroid if and only if q(E, F) = 1.
Proof:
q(E, F) ≤1 follows from the deﬁnition. q(E, F) = 1 is obviously
equivalent to (M3′′).
2
To estimate the rank quotient, the following statement can be used:

13.2 Other Matroid Axioms
295
Theorem 13.8.
(Hausmann, Jenkyns and Korte [1980]) Let (E, F) be an inde-
pendence system. If, for any A ∈F and e ∈E, A ∪{e} contains at most p circuits,
then q(E, F) ≥1
p.
Proof:
Let F ⊆E and J, K two bases of F. We show |J|
|K| ≥1
p.
Let J \ K = {e1, . . . , et}. We construct a sequence K = K0, K1, . . . , Kt of
independent subsets of J ∪K such that J ∩K ⊆Ki, Ki ∩{e1, . . . , et} = {e1, . . . , ei}
and |Ki−1 \ Ki| ≤p for i = 1, . . . , t.
Since Ki ∪{ei+1} contains at most p circuits and each such circuit must meet
Ki \ J (because J is independent), there is an X ⊆Ki \ J such that |X| ≤p and
(Ki \ X) ∪{ei+1} ∈F. We set Ki+1 := (Ki \ X) ∪{ei+1}.
Now J ⊆Kt ∈F. Since J is a basis of F, J = Kt. We conclude that
|K \ J| =
t
i=1
|Ki−1 \ Ki| ≤pt = p |J \ K|,
proving |K| ≤p |J|.
2
This shows that in example (9) we have q(E, F) ≥1
2 (see also Exercise 1 of
Chapter 10). In fact q(E, F) = 1
2 iff G contains a path of length 3 as a subgraph
(otherwise q(E, F) = 1). For the independence system in example (1) of our list,
the rank quotient can become arbitrarily small (choose G to be a star). In Exercise
5, the rank quotients for other independence systems will be discussed.
13.2 Other Matroid Axioms
In this section we consider other axiom systems deﬁning matroids. They charac-
terize fundamental properties of the family of bases, the rank function, the closure
operator and the family of circuits of a matroid.
Theorem 13.9.
Let E be a ﬁnite set and B ⊆2E. B is the set of bases of some
matroid (E, F) if and only if the following holds:
(B1) B ̸= ∅;
(B2) For any B1, B2 ∈B and x ∈B1 \ B2 there exists a y ∈B2 \ B1 with
(B1 \ {x}) ∪{y} ∈B.
Proof:
The set of bases of a matroid satisﬁes (B1) (by (M1)) and (B2): For bases
B1, B2 and x ∈B1 \ B2 we have that B1 \ {x} is independent. By (M3) there is
some y ∈B2 \ B1 such that (B1 \ {x}) ∪{y} is independent. Indeed, it must be a
basis, because all bases of a matroid have the same cardinality.
On the other hand, let B satisfy (B1) and (B2). We ﬁrst show that all elements
of B have the same cardinality: Otherwise let B1, B2 ∈B with |B1| > |B2| such
that |B1 ∩B2| is maximum. Let x ∈B1 \ B2. By (B2) there is a y ∈B2 \ B1 with
(B1 \ {x}) ∪{y} ∈B, contradicting the maximality of |B1 ∩B2|.

296
13. Matroids
Now let
F := {F ⊆E : there exists a B ∈B with F ⊆B}.
(E, F) is an independence system, and B is the family of its bases. To show
that (E, F) satisﬁes (M3), let X, Y ∈F with |X| > |Y|. Let X ⊆B1 ∈B and
Y ⊆B2 ∈B, where B1 and B2 are chosen such that |B1 ∩B2| is maximum. If
B2 ∩(X \ Y) ̸= ∅, we are done because we can augment Y.
We claim that the other case, B2 ∩(X \ Y) = ∅, is impossible. Namely, with
this assumption we get
|B1 ∩B2| + |Y \ B1| + |(B2 \ B1) \ Y| = |B2| = |B1| ≥|B1 ∩B2| + |X \ Y|.
Since |X \ Y| > |Y \ X| ≥|Y \ B1|, this implies (B2 \ B1) \ Y ̸= ∅. So let
y ∈(B2 \ B1) \ Y. By (B2) there exists an x ∈B1 \ B2 with (B2 \ {y}) ∪{x} ∈B,
contradicting the maximality of |B1 ∩B2|.
2
A very important property of matroids is that the rank function is submodular:
Theorem 13.10.
Let E be a ﬁnite set and r : 2E →Z+. Then the following
statements are equivalent:
(a) r is the rank function of a matroid (E, F) (and F = {F ⊆E : r(F) = |F|}).
(b) For all X, Y ⊆E:
(R1) r(X) ≤|X|;
(R2)
If X ⊆Y then r(X) ≤r(Y);
(R3) r(X ∪Y) + r(X ∩Y) ≤r(X) + r(Y).
(c) For all X ⊆E and x, y ∈E:
(R1′) r(∅) = 0;
(R2′) r(X) ≤r(X ∪{y}) ≤r(X) + 1;
(R3′) If r(X ∪{x}) = r(X ∪{y}) = r(X) then r(X ∪{x, y}) = r(X).
Proof:
(a)⇒(b): If r is a rank function of an independence system (E, F), (R1)
and (R2) evidently hold. If (E, F) is a matroid, we can also show (R3):
Let X, Y ⊆E, and let A be a basis of X ∩Y. By (M3), A can be extended
to a basis A
.
∪B of X and to a basis (A ∪B)
.
∪C of X ∪Y. Then A ∪C is an
independent subset of Y, so
r(X) + r(Y)
≥
|A ∪B| + |A ∪C|
=
2|A| + |B| + |C| = |A ∪B ∪C| + |A|
=
r(X ∪Y) + r(X ∩Y).
(b)⇒(c): (R1′) is implied by (R1). r(X) ≤r(X ∪{y}) follows from (R2). By
(R3) and (R1),
r(X ∪{y}) ≤r(X) + r({y}) −r(X ∩{y}) ≤r(X) + r({y}) ≤r(X) + 1,
proving (R2′).

13.2 Other Matroid Axioms
297
(R3′) is trivial for x = y. For x ̸= y we have, by (R2) and (R3),
2r(X) ≤r(X) + r(X ∪{x, y}) ≤r(X ∪{x}) + r(X ∪{y}),
implying (R3′).
(c)⇒(a): Let r : 2E →Z+ be a function satisfying (R1′)–(R3′). Let
F := {F ⊆E : r(F) = |F|}.
We claim that (E, F) is a matroid. (M1) follows from (R1′). (R2′) implies
r(X) ≤|X| for all X ⊆E. If Y ∈F, y ∈Y and X := Y \ {y}, we have
|X| + 1 = |Y| = r(Y) = r(X ∪{y}) ≤r(X) + 1 ≤|X| + 1,
so X ∈F. This implies (M2).
Now let X, Y ∈F and |X| = |Y| + 1. Let X \ Y = {x1, . . . , xk}. Suppose
that (M3′) is violated, i.e. r(Y ∪{xi}) = |Y| for i = 1, . . . , k. Then by (R3′)
r(Y ∪{x1, xi}) = r(Y) for i = 2, . . . , k. Repeated application of this argument
yields r(Y) = r(Y ∪{x1, . . . , xk}) = r(X ∪Y) ≥r(X), a contradiction.
So (E, F) is indeed a matroid. To show that r is the rank function of this
matroid, we have to prove that r(X) = max{|Y| : Y ⊆X, r(Y) = |Y|} for all
X ⊆E. So let X ⊆E, and let Y a maximum subset of X with r(Y) = |Y|. For all
x ∈X \ Y we have r(Y ∪{x}) < |Y| + 1, so by (R2′) r(Y ∪{x}) = |Y|. Repeated
application of (R3′) implies r(X) = |Y|.
2
Theorem 13.11.
Let E be a ﬁnite set and σ : 2E →2E a function. σ is the
closure operator of a matroid (E, F) if and only if the following conditions hold
for all X, Y ⊆E and x, y ∈E:
(S1)
X ⊆σ(X);
(S2)
X ⊆Y ⊆E implies σ(X) ⊆σ(Y);
(S3)
σ(X) = σ(σ(X));
(S4)
If y /∈σ(X) and y ∈σ(X ∪{x}) then x ∈σ(X ∪{y}).
Proof:
If σ is the closure operator of a matroid, then (S1) holds trivially.
For X ⊆Y and z ∈σ(X) we have by (R3) and (R2)
r(X) + r(Y)
=
r(X ∪{z}) + r(Y)
≥
r((X ∪{z}) ∩Y) + r(X ∪{z} ∪Y)
≥
r(X) + r(Y ∪{z}),
implying z ∈σ(Y) and thus proving (S2).
By repeated application of (R3′) we have r(σ(X)) = r(X) for all X, which
implies (S3).
To prove (S4), suppose that there are X, x, y with y /∈σ(X), y ∈σ(X ∪{x})
and x /∈σ(X ∪{y}). Then r(X ∪{y}) = r(X) + 1, r(X ∪{x, y}) = r(X ∪{x}) and
r(X ∪{x, y}) = r(X ∪{y}) + 1. Thus r(X ∪{x}) = r(X) + 2, contradicting (R2′).

298
13. Matroids
To show the converse, let σ : 2E →2E be a function satisfying (S1)–(S4). Let
F := {X ⊆E : x /∈σ(X \ {x}) for all x ∈X}.
We claim that (E, F) is a matroid.
(M1) is trivial. For X ⊆Y ∈F and x ∈X we have x /∈σ(Y \ {x}) ⊇
σ(X \ {x}), so X ∈F and (M2) holds. To prove (M3) we need the following
statement:
Claim:
For X ∈F and Y ⊆E with |X| > |Y| we have X ̸⊆σ(Y).
We prove the claim by induction on |Y \ X|. If Y ⊂X, then let x ∈X \ Y.
Since X ∈F we have x /∈σ(X \ {x}) ⊇σ(Y) by (S2). Hence x ∈X \ σ(Y) as
required.
If |Y \ X| > 0, then let y ∈Y \ X. By the induction hypothesis there exists
an x ∈X \ σ(Y \ {y}). If x ̸∈σ(Y), then we are done. Otherwise x /∈σ(Y \ {y})
but x ∈σ(Y) = σ((Y \ {y}) ∪{y}), so by (S4) y ∈σ((Y \ {y}) ∪{x}). By (S1)
we get Y ⊆σ((Y \ {y}) ∪{x}) and thus σ(Y) ⊆σ((Y \ {y}) ∪{x}) by (S2) and
(S3). Applying the induction hypothesis to X and (Y \{y})∪{x} (note that x ̸= y)
yields X ̸⊆σ((Y \ {y}) ∪{x}), so X ̸⊆σ(Y) as required.
Having proved the claim we can easily verify (M3). Let X, Y ∈F with
|X| > |Y|. By the claim there exists an x ∈X \ σ(Y). Now for each z ∈Y ∪{x}
we have z /∈σ(Y \ {z}), because Y ∈F and x /∈σ(Y) = σ(Y \ {x}). By (S4)
z /∈σ(Y \ {z}) and x /∈σ(Y) imply z /∈σ((Y \ {z}) ∪{x}) ⊇σ((Y ∪{x}) \ {z}).
Hence Y ∪{x} ∈F.
So (M3) indeed holds and (E, F) is a matroid, say with rank function r and
closure operator σ ′. It remains to prove that σ = σ ′.
By deﬁnition, σ ′(X) = {y ∈E : r(X ∪{y}) = r(X)} and
r(X) = max{|Y| : Y ⊆X, y /∈σ(Y \ {y}) for all y ∈Y}
for all X ⊆E.
Let X ⊆E. To show σ ′(X) ⊆σ(X), let z ∈σ ′(X) \ X. Let Y be a basis
of X. Since r(Y ∪{z}) ≤r(X ∪{z}) = r(X) = |Y| < |Y ∪{z}| we have y ∈
σ((Y ∪{z}) \ {y}) for some y ∈Y ∪{z}. If y = z, then we have z ∈σ(Y).
Otherwise (S4) and y /∈σ(Y \ {y}) also yield z ∈σ(Y). Hence by (S2) z ∈σ(X).
Together with (S1) this implies σ ′(X) ⊆σ(X).
Now let z /∈σ ′(X), i.e. r(X ∪{z}) > r(X). Let now Y be a basis of X ∪{z}.
Then z ∈Y and |Y \ {z}| = |Y| −1 = r(X ∪{z}) −1 = r(X). Therefore Y \ {z} is
a basis of X, implying X ⊆σ ′(Y \{z}) ⊆σ(Y \{z}), and thus σ(X) ⊆σ(Y \{z}).
As z /∈σ(Y \ {z}), we conclude that z ̸∈σ(X).
2
Theorem 13.12.
Let E be a ﬁnite set and C ⊆2E. C is set of circuits of an in-
dependence system (E, F), where F = {F ⊂E : there exists no C ∈C with C ⊆
F}, if and only if the following conditions hold:
(C1) ∅/∈C;
(C2) For any C1, C2 ∈C, C1 ⊆C2 implies C1 = C2.

13.3 Duality
299
Moreover, if C is set of circuits of an independence system (E, F), then the follow-
ing statements are equivalent:
(a)
(E, F) is a matroid.
(b)
For any X ∈F and e ∈E, X ∪{e} contains at most one circuit.
(C3) For any C1, C2 ∈C with C1 ̸= C2 and e ∈C1 ∩C2 there exists a C3 ∈C
with C3 ⊆(C1 ∪C2) \ {e}.
(C3′) For any C1, C2 ∈C, e ∈C1 ∩C2 and f ∈C1 \C2 there exists a C3 ∈C with
f ∈C3 ⊆(C1 ∪C2) \ {e}.
Proof:
By deﬁnition, the family of circuits of any independence system satisﬁes
(C1) and (C2). If C satisﬁes (C1), then (E, F) is an independence system. If C
also satisﬁes (C2), it is the set of circuits of this independence system.
(a)⇒(C3′): Let C be the family of circuits of a matroid, and let C1, C2 ∈C,
e ∈C1 ∩C2 and f ∈C1 \ C2. By applying (R3) twice we have
|C1| −1 + r((C1 ∪C2) \ {e, f }) + |C2| −1
=
r(C1) + r((C1 ∪C2) \ {e, f }) + r(C2)
≥
r(C1) + r((C1 ∪C2) \ { f }) + r(C2 \ {e})
≥
r(C1 \ { f }) + r(C1 ∪C2) + r(C2 \ {e})
=
|C1| −1 + r(C1 ∪C2) + |C2| −1.
So r((C1 ∪C2) \ {e, f }) = r(C1 ∪C2). Let B be a basis of (C1 ∪C2) \ {e, f }.
Then B ∪{ f } contains a circuit C3, with f ∈C3 ⊆(C1 ∪C2) \ {e} as required.
(C3′)⇒(C3): trivial.
(C3)⇒(b): If X ∈F and X ∪{e} contains two circuits C1, C2, (C3) implies
(C1 ∪C2) \ {e} /∈F. However, (C1 ∪C2) \ {e} is a subset of X.
(b)⇒(a): Follows from Theorem 13.8 and Proposition 13.7.
2
Especially property (b) will be used often. For X ∈F and e ∈E such that
X ∪{e} ̸∈F we write C(X, e) for the unique circuit in X ∪{e}. If X ∪{e} ∈F
we write C(X, e) := ∅.
13.3 Duality
Another basic concept in matroid theory is duality.
Deﬁnition 13.13.
Let (E, F) be an independence system. We deﬁne the dual of
(E, F) by (E, F∗), where
F∗= {F ⊆E : there is a basis B of (E, F) such that F ∩B = ∅}.
It is obvious that the dual of an independence system is again an independence
system.

300
13. Matroids
Proposition 13.14.
(E, F∗∗) = (E, F).
Proof:
F ∈F∗∗⇔there is a basis B∗of (E, F∗) such that F ∩B∗= ∅⇔
there is a basis B of (E, F) such that F ∩(E \ B) = ∅⇔F ∈F.
2
Theorem 13.15.
Let (E, F) be an independence system, (E, F∗) its dual, and let
r and r∗be the corresponding rank functions.
(a) (E, F) is a matroid if and only if (E, F∗) is a matroid. (Whitney [1935])
(b) If (E, F) is a matroid, then r∗(F) = |F| + r(E \ F) −r(E) for F ⊆E.
Proof:
Due to Proposition 13.14 we have to show only one direction of (a). So
let (E, F) be a matroid. We deﬁne q : 2E →Z+ by q(F) := |F|+r(E\F)−r(E).
We claim that q satisﬁes (R1), (R2) and (R3). By this claim and Theorem 13.10,
q is the rank function of a matroid. Since obviously q(F) = |F| if and only if
F ∈F∗, we conclude that q = r∗, and (a) and (b) are proved.
Now we prove the above claim: q satisﬁes (R1) because r satisﬁes (R2). To
check that q satisﬁes (R2), let X ⊆Y ⊆E. Since (E, F) is a matroid, (R3) holds
for r, so
r(E \ X) + 0 = r((E \ Y) ∪(Y \ X)) + r(∅) ≤r(E \ Y) + r(Y \ X).
We conclude that
r(E \ X) −r(E \ Y) ≤r(Y \ X) ≤|Y \ X| = |Y| −|X|
(note that r satisﬁes (R1)), so q(X) ≤q(Y).
It remains to show that q satisﬁes (R3). Let X, Y ⊆E. Using the fact that r
satisﬁes (R3) we have
q(X ∪Y) + q(X ∩Y)
=
|X ∪Y| + |X ∩Y| + r(E \ (X ∪Y)) + r(E \ (X ∩Y)) −2r(E)
=
|X| + |Y| + r((E \ X) ∩(E \ Y)) + r((E \ X) ∪(E \ Y)) −2r(E)
≤
|X| + |Y| + r(E \ X) + r(E \ Y) −2r(E)
=
q(X) + q(Y).
2
For any graph G we have introduced the cycle matroid M(G) which of course
has a dual. For an embedded planar graph G there is also a planar dual G∗(which
in general depends on the embedding of G). It is interesting that the two concepts
of duality coincide:
Theorem 13.16.
Let G be a connected planar graph with an arbitrary planar
embedding, and G∗the planar dual. Then
M(G∗) = (M(G))∗.

13.3 Duality
301
Proof:
For T ⊆E(G) we write T
∗:= {e∗: e ∈E(G) \ T }, where e∗is the dual
of edge e. We have to prove the following:
Claim:
T is the edge set of a spanning tree in G iff T
∗is the edge set of a
spanning tree in G∗.
Since (G∗)∗= G (by Proposition 2.42) and (T
∗)
∗
= T it sufﬁces to prove
one direction of the claim.
So let T ⊆E(G), where T
∗is the edge set of a spanning tree in G∗. (V (G), T )
must be connected, for otherwise a connected component would deﬁne a cut, the
dual of which contains a circuit in T
∗(Theorem 2.43). On the other hand, if
(V (G), T ) contains a circuit, then the dual edge set is a cut and (V (G∗), T
∗) is
disconnected. Hence (V (G), T ) is indeed a spanning tree in G.
2
This implies that if G is planar then (M(G))∗is a graphic matroid. If, for
any graph G, (M(G))∗is a graphic matroid, say (M(G))∗= M(G′), then G′
is evidently an abstract dual of G. By Exercise 34 of Chapter 2, the converse is
also true: G is planar if and only if G has an abstract dual (Whitney [1933]). This
implies that (M(G))∗is graphic if and only if G is planar.
Note that Theorem 13.16 quite directly implies Euler’s formula (Theorem
2.32): Let G be a connected planar graph with a planar embedding, and let M(G)
be the cycle matroid of G. By Theorem 13.15 (b), r(E(G))+r∗(E(G)) = |E(G)|.
Since r(E(G)) = |V (G)| −1 (the number of edges in a spanning tree) and
r∗(E(G)) = |V (G∗)| −1 (by Theorem 13.16), we obtain that the number of faces
of G is |V (G∗)| = |E(G)| −|V (G)| + 2, Euler’s formula.
Duality of independence systems has also some nice applications in polyhedral
combinatorics. A set system (E, F) is called a clutter if X ̸⊂Y for all X, Y ∈F.
If (E, F) is a clutter, then we deﬁne its blocking clutter by
BL(E, F)
:=
(E, {X ⊆E : X ∩Y ̸= ∅for all Y ∈F,
X minimal with this property}).
For an independence system (E, F) and its dual (E, F∗) let B and B∗be the
family of bases, and C and C∗the family of circuits, respectively. (Every clutter
arises in both of these ways except for F = ∅or F = {∅}.) It follows immediately
from the deﬁnitions that (E, B∗) = BL(E, C) and (E, C∗) = BL(E, B). Together
with Proposition 13.14 this implies BL(BL(E, F)) = (E, F) for every clutter
(E, F). We give some examples for clutters (E, F) and their blocking clutters
(E, F′). In each case E = E(G) for some graph G:
(1) F is the set of spanning trees, F′ is the set of minimal cuts;
(2) F is the set of arborescences rooted at r, F′ is the set of minimal r-cuts;
(3) F is the set of s-t-paths, F′ is the set of minimal cuts separating s and t (this
example works in undirected graphs and in digraphs);
(4) F is the set of circuits in an undirected graph, F′ is the set of complements
of maximal forests;
(5) F is the set of circuits in a digraph, F′ is the set of minimal feedback edge
sets;

302
13. Matroids
(6) F is the set of minimal edge sets whose contraction makes the digraph strongly
connected, F′ is the set of minimal directed cuts;
(7) F is the set of minimal T -joins, F′ is the set of minimal T -cuts.
All these blocking relations can be veriﬁed easily: (1) and (2) follow directly from
Theorems 2.4 and 2.5, (3), (4) and (5) are trivial, (6) follows from Corollary 2.7,
and (7) from Proposition 12.6.
In some cases, the blocking clutter gives a polyhedral characterization of the
Minimization Problem For Independence Systems for nonnegative cost func-
tions:
Deﬁnition 13.17.
Let (E, F) be a clutter, (E, F′) its blocking clutter and P the
convex hull of the incidence vectors of the elements of F. We say that (E, F) has
the Max-Flow-Min-Cut property if
5
x + y : x ∈P, y ∈RE
+
6
=

x ∈RE
+ :

e∈B
xe ≥1 for all B ∈F′

.
Examples are (2) and (7) of our list above (by Theorems 6.14 and 12.16),
but also (3) and (6) (see Exercise 10). The following theorem relates the above
covering-type formulation to a packing formulation of the dual problem and allows
to derive certain min-max theorems from others:
Theorem 13.18.
(Fulkerson [1971], Lehman [1979]) Let (E, F) be a clutter and
(E, F′) its blocking clutter. Then the following statements are equivalent:
(a) (E, F) has the Max-Flow-Min-Cut property;
(b) (E, F′) has the Max-Flow-Min-Cut property;
(c) min{c(A) : A ∈F} = max
5
1ly : y ∈RF′
+ , 
B∈F′:e∈B yB ≤c(e)
for all e ∈E
6
for every c : E →R+.
Proof:
Since BL(E, F′) = BL(BL(E, F)) = (E, F) it sufﬁces to prove
(a)⇒(c)⇒(b). The other implication (b)⇒(a) then follows by exchanging the roles
of F and F′.
(a)⇒(c): By Corollary 3.28 we have for every c : E →R+
min{c(A) : A ∈F} = min{cx : x ∈P} = min
5
c(x + y) : x ∈P, y ∈RE
+
6
,
where P is the convex hull of the incidence vectors of elements of F. From this,
the Max-Flow-Min-Cut property and the LP Duality Theorem 3.16 we get (c).
(c)⇒(b): Let P′ denote the convex hull of the incidence vectors of the elements
of F′. We have to show that
5
x + y : x ∈P′, y ∈RE
+
6
=

x ∈RE
+ :

e∈A
xe ≥1 for all A ∈F

.
Since “⊆” is trivial from the deﬁnition of blocking clutters we only show the other
inclusion. So let c ∈RE
+ be a vector with 
e∈A ce ≥1 for all A ∈F. By (c) we
have

13.4 The Greedy Algorithm
303
1
≤
min{c(A) : A ∈F}
=
max

1ly : y ∈RF′
+ ,

B∈F′:e∈B
yB ≤c(e) for all e ∈E

,
so let y ∈RF′
+ be a vector with 1ly = 1 and 
B∈F′:e∈B yB ≤c(e) for all e ∈E.
Then xe := 
B∈F′:e∈B yB (e ∈E) deﬁnes a vector x ∈P′ with x ≤c, proving
that c ∈
5
x + y : x ∈P′, y ∈RE
+
6
.
2
For example, this theorem implies the Max-Flow-Min-Cut Theorem 8.6 quite
directly: Let (G, u, s, t) be a network. By Exercise 1 of Chapter 7 the minimum
length of an s-t-path in (G, u) equals the maximum number of s-t-cuts such that
each edge e is contained in at most u(e) of them. Hence the clutter of s-t-paths
(example (3) in the above list) has the Max-Flow-Min-Cut Property, and so has
its blocking clutter. Now (c) applied to the clutter of minimal s-t-cuts implies the
Max-Flow-Min-Cut Theorem.
Note however that Theorem 13.18 does not guarantee an integral vector attain-
ing the maximum in (c), even if c is integral. The clutter of T -joins for G = K4
and T = V (G) shows that this does not exist in general.
13.4 The Greedy Algorithm
Again, let (E, F) be an independence system and c : E →R+. We consider the
Maximization Problem for (E, F, c) and formulate two “greedy algorithms”.
We do not have to consider negative weights since elements with negative weight
never appear in an optimum solution.
We assume that (E, F) is given by an oracle. For the ﬁrst algorithm we simply
assume an independence oracle, i.e. an oracle which, given a set F ⊆E, decides
whether F ∈F or not.
Best-In-Greedy Algorithm
Input:
An independence system (E, F), given by an independence oracle.
Weights c : E →R+.
Output:
A set F ∈F.
1⃝
Sort E = {e1, e2, . . . , en} such that c(e1) ≥c(e2) ≥· · · ≥c(en).
2⃝
Set F := ∅.
3⃝
For i := 1 to n do: If F ∪{ei} ∈F then set F := F ∪{ei}.
The second algorithm requires a more complicated oracle. Given a set F ⊆E,
this oracle decides whether F contains a basis. Let us call such an oracle a basis-
superset oracle.

304
13. Matroids
Worst-Out-Greedy Algorithm
Input:
An independence system (E, F), given by a basis-superset oracle.
Weights c : E →R+.
Output:
A basis F of (E, F).
1⃝
Sort E = {e1, e2, . . . , en} such that c(e1) ≤c(e2) ≤· · · ≤c(en).
2⃝
Set F := E.
3⃝
For i := 1 to n do: If F \ {ei} contains a basis then set F := F \ {ei}.
Before we analyse these algorithms, let us take a closer look at the oracles
required. It is an interesting questions whether such oracles are polynomially equiv-
alent, i.e. whether one can be simulated by polynomial-time oracle algorithm using
the other. The independence oracle and the basis-superset oracle do not seem to
be polynomially equivalent:
If we consider the independence system for the TSP (example (2) of the list
in Section 13.1), it is easy (and the subject of Exercise 13) to decide whether a
set of edges is independent, i.e. the subset of a Hamiltonian circuit (recall that we
are working with a complete graph). On the other hand, it is a difﬁcult problem to
decide whether a set of edges contains a Hamiltonian circuit (this is NP-complete;
cf. Theorem 15.25).
Conversely, in the independence system for the Shortest Path Problem (ex-
ample (3)), it is easy to decide whether a set of edges contains an s-t-path. Here it
is not known how to decide whether a given set is independent (i.e. subset of an
s-t-path) in polynomial time (Korte and Monma [1979] proved NP-completeness).
For matroids, both oracles are polynomially equivalent. Other equivalent ora-
cles are the rank oracle and closure oracle, which return the rank and the closure
of a given subset of E, respectively (Exercise 16).
However, even for matroids there are other natural oracles that are not polyno-
mially equivalent. For example, the oracle deciding whether a given set is a basis
is weaker than the independence oracle. The oracle which for a given F ⊆E
returns the minimum cardinality of a dependent subset of F is stronger than the
independence oracle (Hausmann and Korte [1981]).
One can analogously formulate both greedy algorithms for the Minimization
Problem. It is easy to see that the Best-In-Greedy for the Maximization Prob-
lem for (E, F, c) corresponds to the Worst-Out-Greedy for the Minimization
Problem for (E, F∗, c): adding an element to F in the Best-In-Greedy corre-
sponds to removing an element from F in the Worst-Out-Greedy. Observe that
Kruskal’s Algorithm (see Section 6.1) is a Best-In-Greedy algorithm for the
Minimization Problem in a cycle matroid.
The rest of this section contains some results concerning the quality of a
solution found by the greedy algorithms.
Theorem 13.19.
(Jenkyns [1976], Korte and Hausmann [1978]) Let (E, F) be
an independence system. For c : E →R+ we denote by G(E, F, c) the cost of

13.4 The Greedy Algorithm
305
some solution found by the Best-In-Greedy for the Maximization Problem, and
by OPT(E, F, c) the cost of an optimum solution. Then
q(E, F) ≤
G(E, F, c)
OPT(E, F, c) ≤1
for all c : E →R+. There is a cost function where the lower bound is attained.
Proof:
Let E = {e1, e2, . . . , en}, c : E →R+, and c(e1) ≥c(e2) ≥. . . ≥c(en).
Let Gn be the solution found by the Best-In-Greedy (when sorting E like this),
while On is an optimum solution. We deﬁne Ej := {e1, . . . , ej}, Gj := Gn ∩Ej
and Oj := On ∩Ej ( j = 0, . . . , n). Set dn := c(en) and dj := c(ej) −c(ej+1) for
j = 1, . . . , n −1.
Since Oj ∈F, we have |Oj| ≤r(Ej). Since Gj is a basis of Ej, we have
|Gj| ≥ρ(Ej). With these two inequalities we conclude that
c(Gn)
=
n

j=1
(|Gj| −|Gj−1|) c(ej)
=
n

j=1
|Gj| dj
≥
n

j=1
ρ(Ej) dj
≥
q(E, F)
n

j=1
r(Ej) dj
(13.1)
≥
q(E, F)
n

j=1
|Oj| dj
=
q(E, F)
n

j=1
(|Oj| −|Oj−1|) c(ej)
=
q(E, F) c(On).
Finally we show that the lower bound is sharp. Choose F ⊆E and bases
B1, B2 of F such that
|B1|
|B2| = q(E, F).
Deﬁne
c(e) :=
 1
for e ∈F
0
for e ∈E \ F
and sort e1, . . . , en such that c(e1) ≥c(e2) ≥. . . ≥c(en) and B1 = {e1, . . . , e|B1|}.
Then G(E, F, c) = |B1| and OPT(E, F, c) = |B2|, and the lower bound is at-
tained.
2
In particular we have the so-called Edmonds-Rado Theorem:

306
13. Matroids
Theorem 13.20.
(Rado [1957], Edmonds [1971])
An independence system
(E, F) is a matroid if and only if the Best-In-Greedy ﬁnds an optimum solution
for the Maximization Problem for (E, F, c) for all cost functions c : E →R+.
Proof:
By Theorem 13.19 we have q(E, F) < 1 if and only if there exists a cost
function c : E →R+ for which the Best-In-Greedy does not ﬁnd an optimum
solution. By Proposition 13.7 we have q(E, F) < 1 if and only if (E, F) is not
a matroid.
2
This is one of the rare cases where we can deﬁne a structure by its algorithmic
behaviour. We also obtain a polyhedral description:
Theorem 13.21.
(Edmonds [1970]) Let (E, F) be a matroid and r : E →Z+
its rank function. Then the matroid polytope of (E, F), i.e. the convex hull of the
incidence vectors of all elements of F, is equal to

x ∈RE : x ≥0,

e∈A
xe ≤r(A) for all A ⊆E

.
Proof:
Obviously, this polytope contains all incidence vectors of independent
sets. By Corollary 3.27 it remains to show that all vertices of this polytope are
integral. By Theorem 5.12 this is equivalent to showing that
max

cx : x ≥0,

e∈A
xe ≤r(A) for all A ⊆E

(13.2)
has an integral optimum solution for any c : E →R. W.l.o.g. c(e) ≥0 for all e,
since for e ∈E with c(e) < 0 any optimum solution x of (13.2) has xe = 0.
Let x be an optimum solution of (13.2). In (13.1) we replace |Oj| by 
e∈Ej xe
( j = 0, . . . , n). We obtain c(Gn) ≥
e∈E c(e)xe. So the Best-In-Greedy pro-
duces a solution whose incidence vector is another optimum solution of (13.2).
2
When applied to graphic matroids, this also yields Theorem 6.12. As in this
special case, we also have total dual integrality in general. A generalization of this
result will be proved in Section 14.2.
The above observation that the Best-In-Greedy for the Maximization Prob-
lem for (E, F, c) corresponds to the Worst-Out-Greedy for the Minimization
Problem for (E, F∗, c) suggests the following dual counterpart of Theorem 13.19:
Theorem 13.22.
(Korte and Monma [1979])
Let (E, F) be an independence
system. For c : E →R+ let G(E, F, c) denote a solution found by the Worst-
Out-Greedy for the Minimization Problem. Then
1 ≤
G(E, F, c)
OPT(E, F, c) ≤max
F⊆E
|F| −ρ∗(F)
|F| −r∗(F)
(13.3)
for all c : E →R+, where ρ∗and r∗are the rank functions of the dual indepen-
dence system (E, F∗). There is a cost function where the upper bound is attained.

13.4 The Greedy Algorithm
307
Proof:
We use the same notation as in the proof of Theorem 13.19. By con-
struction, Gj ∪(E \ Ej) contains a basis of E, but (Gj ∪(E \ Ej)) \ {e} does not
contain a basis of E for any e ∈Gj ( j = 1, . . . , n). In other words, Ej \ Gj is a
basis of Ej with respect to (E, F∗), so |Ej| −|Gj| ≥ρ∗(Ej).
Since On ⊆E \(Ej \ Oj) and On is a basis, Ej \ Oj is independent in (E, F∗),
so |Ej| −|Oj| ≤r∗(Ej).
We conclude that
|Gj|
≤
|Ej| −ρ∗(Ej)
and
|Oj|
≥
|Ej| −r∗(Ej).
Now the same calculation as (13.1) provides the upper bound. To see that this
bound is tight, consider
c(e) :=
 1
for e ∈F
0
for e ∈E \ F ,
where F ⊆E is a set where the maximum in (13.3) is attained. Let B1 be a basis
of F with respect to (E, F∗), with |B1| = ρ∗(F). If we sort e1, . . . , en such that
c(e1) ≥c(e2) ≥. . . ≥c(en) and B1 = {e1, . . . , e|B1|}, we have G(E, F, c) =
|F| −|B1| and OPT(E, F, c) = |F| −r∗(F).
2
1
2
M >> 2
Fig. 13.1.
If we apply the Worst-Out-Greedy to the Maximization Problem or the
Best-In-Greedy to the Minimization Problem, there is no positive lower/ﬁnite
upper bound for
G(E,F,c)
OPT(E,F,c). To see this, consider the problem of ﬁnding a minimal
vertex cover of maximum weight or a maximal stable set of minimum weight in
the simple graph shown in Figure 13.1.
However in the case of matroids, it does not matter whether we use the Best-
In-Greedy or the Worst-Out-Greedy: since all bases have the same cardinality,
the Minimization Problem for (E, F, c) is equivalent to the Maximization
Problem for (E, F, c′), where c′(e) := M−c(e) and M := 1+max{c(e) : e ∈E}.
Therefore Kruskal’s Algorithm (Section 6.1) solves the Minimum Spanning
Tree Problem optimally.
The Edmonds-Rado Theorem 13.20 also yields the following characterization
of optimum k-element solutions of the Maximization Problem.
Theorem 13.23.
Let (E, F) be a matroid, c : E →R, k ∈N and X ∈F with
|X| = k. Then c(X) = max{c(Y) : Y ∈F, |Y| = k} if and only if the following
two conditions hold:
(a) For all y ∈E \ X with X ∪{y} /∈F and all x ∈C(X, y) we have c(x) ≥c(y);

308
13. Matroids
(b) For all y ∈E \ X with X ∪{y} ∈F and all x ∈X we have c(x) ≥c(y).
Proof:
The necessity is trivial: if one of the conditions is violated for some y
and x, the k-element set X′ := (X ∪{y}) \ {x} ∈F has greater cost than X.
To see the sufﬁciency, let F′ := {F ∈F : |F| ≤k} and c′(e) := c(e) + M
for all e ∈E, where M = max{|c(e)| : e ∈E}. Sort E = {e1, . . . , en} such that
c′(e1) ≥· · · ≥c′(en) and, for any i, c′(ei) = c′(ei+1) and ei+1 ∈X imply ei ∈X
(i.e. elements of X come ﬁrst among those of equal weight).
Let X′ be the solution found by the Best-In-Greedy for the instance
(E, F′, c′) (sorted like this). Since (E, F′) is a matroid, the Edmonds-Rado The-
orem 13.20 implies:
c(X′) + kM
=
c′(X′) = max{c′(Y) : Y ∈F′}
=
max{c(Y) : Y ∈F, |Y| = k} + kM.
We conclude the proof by showing that X = X′. We know that |X| = k = |X′|. So
suppose X ̸= X′, and let ei ∈X′ \ X with i minimum. Then X ∩{e1, . . . , ei−1} =
X′ ∩{e1, . . . , ei−1}. Now if X ∪{ei} /∈F, then (a) implies C(X, ei) ⊆X′, a
contradiction. If X ∪{ei} ∈F, then (b) implies X ⊆X′ which is also impossible.
2
We shall need this theorem in Section 13.7. The special case that (E, F) is a
graphic matroid and k = r(E) is part of Theorem 6.2.
13.5 Matroid Intersection
Deﬁnition 13.24.
Given two independence systems (E, F1) and (E, F2), we de-
ﬁne their intersection by (E, F1 ∩F2).
The intersection of a ﬁnite number of independence systems is deﬁned analo-
gously. It is clear that the result is again an independence system.
Proposition 13.25.
Any independence system (E, F) is the intersection of a ﬁnite
number of matroids.
Proof:
Each circuit C of (E, F) deﬁnes a matroid (E, {F ⊆E : C \ F ̸= ∅})
by Theorem 13.12. The intersection of all these matroids is of course (E, F). 2
Since the intersection of matroids is not a matroid in general, we cannot hope
to get an optimum common independent set by a greedy algorithm. However, the
following result, together with Theorem 13.19, implies a bound for the solution
found by the Best-In-Greedy:
Proposition 13.26.
If (E, F) is the intersection of p matroids, then q(E, F) ≥1
p.

13.5 Matroid Intersection
309
Proof:
By Theorem 13.12(b), X ∪{e} contains at most p circuits for any X ∈F
and e ∈E. The statement now follows from Theorem 13.8.
2
Of particular interest are independence systems that are the intersection of two
matroids. The prime example here is the matching problem in a bipartite graph
G = (A
.
∪B, E(G)). If E = E(G) and F := {F ⊆E : F is a matching in G},
(E, F) is the intersection of two matroids. Namely, let
F1
:=
{F ⊆E : |δF(x)| ≤1 for all x ∈A}
and
F2
:=
{F ⊆E : |δF(x)| ≤1 for all x ∈B}.
(E, F1), (E, F2) are matroids by Proposition 13.4(d). Clearly, F = F1 ∩F2.
A second example is the independence system consisting of all branchings in
a digraph G (Example 8 of the list at the beginning of Section 13.1). Here one
matroid contains all sets of edges such that each vertex has at most one entering
edge (see Proposition 13.4(e)), while the second matroid is the cycle matroid
M(G) of the underlying undirected graph.
We shall now describe Edmonds’ algorithm for the following problem:
Matroid Intersection Problem
Instance:
Two matroids (E, F1), (E, F2), given by independence oracles.
Task:
Find a set F ∈F1 ∩F2 such that |F| is maximum.
We start with the following lemma. Recall that, for X ∈F and e ∈E, C(X, e)
denotes the unique circuit in X ∪{e} if X ∪{e} /∈F, and C(X, e) = ∅otherwise.
Lemma 13.27.
(Frank [1981])
Let (E, F) be a matroid and X
∈F. Let
x1, . . . , xs ∈X and y1, . . . , ys /∈X with
(a) xk ∈C(X, yk) for k = 1, . . . , s and
(b) xj /∈C(X, yk) for 1 ≤j < k ≤s.
Then (X \ {x1, . . . , xs}) ∪{y1, . . . , ys} ∈F.
Proof:
Let Xr := (X \ {x1, . . . , xr}) ∪{y1, . . . , yr}. We show that Xr ∈F for
all r by induction. For r = 0 this is trivial. Let us assume that Xr−1 ∈F for
some r ∈{1, . . . , s}. If Xr−1 ∪{yr} ∈F then we immediately have Xr ∈F.
Otherwise Xr−1 ∪{yr} contains a unique circuit C (by Theorem 13.12(b)). Since
C(X, yr) ⊆Xr−1 ∪{yr} (by (b)), we must have C = C(X, yr). But then by (a)
xr ∈C(X, yr) = C, so Xr = (Xr−1 ∪{yr}) \ {xr} ∈F.
2
The idea behind Edmonds’ Matroid Intersection Algorithm is the fol-
lowing. Starting with X = ∅, we augment X by one element in each iteration.
Since in general we cannot hope for an element e such that X ∪{e} ∈F1 ∩F2, we
shall look for “alternating paths”. To make this convenient, we deﬁne an auxiliary
graph. We apply the notion C(X, e) to (E, Fi) and write Ci(X, e) (i = 1, 2).

310
13. Matroids
X
E \ X
A(1)
X
A(2)
X
SX
TX
Fig. 13.2.
Given a set X ∈F1 ∩F2, we deﬁne a directed auxiliary graph G X by
A(1)
X
:=
{ (x, y) : y ∈E \ X, x ∈C1(X, y) \ {y} },
A(2)
X
:=
{ (y, x) : y ∈E \ X, x ∈C2(X, y) \ {y} },
G X
:=
(E, A(1)
X ∪A(2)
X ).
We set
SX
:=
{y ∈E \ X : X ∪{y} ∈F1},
TX
:=
{y ∈E \ X : X ∪{y} ∈F2}
(see Figure 13.2) and look for a shortest path from SX to TX. Such a path will
enable us to augment the set X. (If SX ∩TX ̸= ∅, we have a path of length zero
and we can augment X by any element in SX ∩TX.)
Lemma 13.28.
Let X ∈F1 ∩F2. Let y0, x1, y1, . . . , xs, ys be the vertices of a
shortest y0-ys-path in G X (in this order), with y0 ∈SX and ys ∈TX. Then
X′ := (X ∪{y0, . . . , ys}) \ {x1, . . . , xs} ∈F1 ∩F2.
Proof:
First we show that X ∪{y0}, x1, . . . , xs and y1, . . . , ys satisfy the require-
ments of Lemma 13.27 with respect to F1. Observe that X ∪{y0} ∈F1 because
y0 ∈SX. (a) is satisﬁed because (xj, yj) ∈A(1)
X
for all j, and (b) is satisﬁed
because otherwise the path could be shortcut. We conclude that X′ ∈F1.
Secondly, we show that X ∪{ys}, xs, xs−1, . . . , x1 and ys−1, . . . , y1, y0 satisfy
the requirements of Lemma 13.27 with respect to F2. Observe that X ∪{ys} ∈F2
because ys ∈TX. (a) is satisﬁed because (yj−1, xj) ∈A(2)
X
for all j, and (b) is

13.5 Matroid Intersection
311
satisﬁed because otherwise the path could be shortcut. We conclude that X′ ∈F2.
2
We shall now prove that if there exists no SX-TX-path in G X, then X is already
maximum. We need the following simple fact:
Proposition 13.29.
Let (E, F1) and (E, F2) be two matroids with rank functions
r1 and r2. Then for any F ∈F1 ∩F2 and any Q ⊆E we have
|F| ≤r1(Q) + r2(E \ Q).
Proof:
F ∩Q ∈F1 implies |F ∩Q| ≤r1(Q). Similarly F \ Q ∈F2 implies
|F \ Q| ≤r2(E \ Q). Adding the two inequalities completes the proof.
2
Lemma 13.30.
X ∈F1 ∩F2 is maximum if and only if there is no SX-TX-path in
G X.
Proof:
If there is an SX-TX-path, there is also a shortest one. We apply Lemma
13.28 and obtain a set X′ ∈F1 ∩F2 of greater cardinality.
X
E \ X
A(1)
X
A(2)
X
E \ R
R
SX
TX
Fig. 13.3.
Otherwise let R be the set of vertices reachable from SX in G X (see Figure
13.3). We have R ∩TX = ∅. Let r1 and r2 be the rank function of F1 and F2,
respectively.
We claim that r2(R) = |X ∩R|. If not, there would be a y ∈R \ X with
(X ∩R) ∪{y} ∈F2. Since X ∪{y} /∈F2 (because y /∈TX), the circuit C2(X, y)
must contain an element x ∈X \ R. But then (y, x) ∈A(2)
X means that there is an
edge leaving R. This contradicts the deﬁnition of R.

312
13. Matroids
Next we prove that r1(E \R) = |X \R|. If not, there would be a y ∈(E \R)\X
with (X \ R)∪{y} ∈F1. Since X ∪{y} /∈F1 (because y /∈SX), the circuit C1(X, y)
must contain an element x ∈X ∩R. But then (x, y) ∈A(1)
X means that there is an
edge leaving R. This contradicts the deﬁnition of R.
Altogether we have |X| = r2(R)+r1(E\R). By Proposition 13.29, this implies
optimality.
2
The last paragraph of this proof yields the following min-max-equality:
Theorem 13.31.
(Edmonds [1970])
Let (E, F1) and (E, F2) be two matroids
with rank functions r1 and r2. Then
max {|X| : X ∈F1 ∩F2} = min {r1(Q) + r2(E \ Q) : Q ⊆E} .
2
We are now ready for a detailed description of the algorithm.
Edmonds’ Matroid Intersection Algorithm
Input:
Two matroids (E, F1) and (E, F2), given by independence oracles.
Output:
A set X ∈F1 ∩F2 of maximum cardinality.
1⃝
Set X := ∅.
2⃝
For each y ∈E \ X and i ∈{1, 2} do: Compute
Ci(X, y) := {x ∈X ∪{y} : X ∪{y} /∈Fi, (X ∪{y}) \ {x} ∈Fi}.
3⃝
Compute SX, TX, and G X as deﬁned above.
4⃝
Apply BFS to ﬁnd a shortest SX-TX-path P in G X.
If none exists then stop.
5⃝
Set X := X△V (P) and go to 2⃝.
Theorem 13.32.
Edmonds’ Matroid Intersection Algorithm correctly
solves the Matroid Intersection Problem in O(|E|3θ) time, where θ is the
maximum complexity of the two independence oracles.
Proof:
The correctness follows from Lemmata 13.28 and 13.30. 2⃝and 3⃝can be
done in O(|E|2θ), 4⃝in O(|E|) time. Since there are at most |E| augmentations,
the overall complexity is O(|E|3θ).
2
Faster matroid intersection algorithms are discussed by Cunningham [1986]
and Gabow and Xu [1996]. We remark that the problem of ﬁnding a maximum
cardinality set in the intersection of three matroids is an NP-hard problem; see
Exercise 14(c) of Chapter 15.

13.6 Matroid Partitioning
313
13.6 Matroid Partitioning
Instead of the intersection of matroids we now consider their union which is
deﬁned as follows:
Deﬁnition 13.33.
Let (E, F1), . . . , (E, Fk) be k matroids. A set X ⊆E is called
partitionable if there exists a partition X = X1
.
∪· · ·
.
∪Xk with Xi ∈Fi for
i = 1, . . . , k. Let F be the family of partitionable subsets of E. Then (E, F) is
called the union or sum of (E, F1), . . . , (E, Fk).
We shall prove that the union of matroids is a matroid again. Moreover, we
solve the following problem via matroid intersection:
Matroid Partitioning Problem
Instance:
A number k ∈N, k matroids (E, F1), . . . , (E, Fk), given by inde-
pendence oracles.
Task:
Find a partitionable set X ⊆E of maximum cardinality.
The main theorem with respect to matroid partitioning is:
Theorem 13.34.
(Nash-Williams [1967]) Let (E, F1), . . . , (E, Fk) be matroids
with rank functions r1, . . . ,rk, and let (E, F) be their union. Then (E, F) is a ma-
troid, and its rank function r is given by r(X) = minA⊆X

|X \ A| + k
i=1 ri(A)

.
Proof:
(E, F) is obviously an independence system. Let X ⊆E. We ﬁrst prove
r(X) = minA⊆X

|X \ A| + k
i=1 ri(A)

.
For any Y ⊆X such that Y is partitionable, i.e. Y = Y1
.
∪· · ·
.
∪Yk with
Yi ∈Fi (i = 1, . . . , k), and any A ⊆X we have
|Y| = |Y \ A| + |Y ∩A| ≤|X \ A| +
k

i=1
|Yi ∩A| ≤|X \ A| +
k

i=1
ri(A),
so r(X) ≤minA⊆X

|X \ A| + k
i=1 ri(A)

.
On the other hand, let X′ := X × {1, . . . , k}. We deﬁne two matroids on X′.
For Q ⊆X′ and i ∈{1, . . . , k} we write Qi := {e ∈X : (e, i) ∈Q}. Let
I1 := {Q ⊆X′ : Qi ∈Fi for all i = 1, . . . , k}
and
I2 := {Q ⊆X′ : Qi ∩Qj = ∅for all i ̸= j}.
Evidently, both (X′, I1) and (X′, I2) are matroids, and their rank functions are
given by s1(Q) := k
i=1 ri(Qi) and s2(Q) :=

k
i=1 Qi
 for Q ⊆X′.

314
13. Matroids
Now the family of partitionable subsets of X can be written as
{A ⊆X : there is a function f : A →{1, . . . , k}
with {(e, f (e)) : e ∈A} ∈I1 ∩I2}.
So the maximum cardinality of a partitionable set is the maximum cardinal-
ity of a common independent set in I1 and I2. By Theorem 13.31 this maximum
cardinality equals min
5
s1(Q) + s2(X′ \ Q) : Q ⊆X′6
. If Q ⊆X′ attains this min-
imum, then for A := Q1 ∩· · · ∩Qk we have
r(X) = s1(Q)+s2(X′ \ Q) =
k

i=1
ri(Qi)+
X \
k4
i=1
Qi
 ≥
k

i=1
ri(A)+|X \ A|.
So we have found a set A ⊆X with k
i=1 ri(A) + |X \ A| ≤r(X).
Having proved the formula for the rank function r, we ﬁnally show that r is
submodular. By Theorem 13.10, this implies that (E, F) is a matroid. To show
the submodularity, let X, Y ⊆E, and let A ⊆X, B ⊆Y with r(X) = |X \ A| +
k
i=1 ri(A) and r(Y) = |Y \ B| + k
i=1 ri(B). Then
r(X) + r(Y)
=
|X \ A| + |Y \ B| +
k

i=1
(ri(A) + ri(B))
≥
|(X ∪Y) \ (A ∪B)| + |(X ∩Y) \ (A ∩B)| +
k

i=1
(ri(A ∪B) + ri(A ∩B))
≥
r(X ∪Y) + r(X ∩Y).
2
The construction in the above proof (Edmonds [1970]) reduces the Matroid
Partitioning Problem to the Matroid Intersection Problem. A reduction in
the other direction is also possible (Exercise 20), so both problems can be regarded
as equivalent.
Note that we ﬁnd a maximum independent set in the union of an arbitrary
number of matroids, while the intersection of more than two matroids is intractable.
13.7 Weighted Matroid Intersection
We now consider a generalization of the above algorithm to the weighted case.
Weighted Matroid Intersection Problem
Instance:
Two matroids (E, F1) and (E, F2), given by independence oracles.
Weights c : E →R.
Task:
Find a set X ∈F1 ∩F2 whose weight c(X) is maximum.

13.7 Weighted Matroid Intersection
315
We shall describe a primal-dual algorithm due to Frank [1981] for this problem.
It generalizes Edmonds’MatroidIntersectionAlgorithm. Again we start with
X := X0 = ∅and increase the cardinality in each iteration by one. We obtain sets
X0, . . . , Xm ∈F1 ∩F2 with |Xk| = k (k = 0, . . . , m) and m = max{|X| : X ∈
F1 ∩F2}. Each Xk will be optimum, i.e.
c(Xk) = max{c(X) : X ∈F1 ∩F2, |X| = k}.
(13.4)
Hence at the end we just choose the optimum set among X0, . . . , Xm.
The main idea is to split up the weight function. At any stage we have two
functions c1, c2 : E →R with c1(e) + c2(e) = c(e) for all e ∈E. For each k we
shall guarantee
ci(Xk) = max{ci(X) : X ∈Fi, |X| = k}
(i = 1, 2).
(13.5)
This condition obviously implies (13.4). To obtain (13.5) we use the optimality
criterion of Theorem 13.23. Instead of G X, SX and TX only a subgraph ¯G and
subsets ¯S, ¯T are considered.
Weighted Matroid Intersection Algorithm
Input:
Two matroids (E, F1) and (E, F2), given by independence oracles.
Weights c : E →R.
Output:
A set X ∈F1 ∩F2 of maximum weight.
1⃝
Set k := 0 and X0 := ∅. Set c1(e) := c(e) and c2(e) := 0 for all e ∈E.
2⃝
For each y ∈E \ Xk and i ∈{1, 2} do: Compute
Ci(Xk, y) := {x ∈Xk ∪{y} : Xk ∪{y} /∈Fi, (Xk ∪{y}) \ {x} ∈Fi}.
3⃝
Compute
A(1)
:=
{ (x, y) : y ∈E \ Xk, x ∈C1(Xk, y) \ {y} },
A(2)
:=
{ (y, x) : y ∈E \ Xk, x ∈C2(Xk, y) \ {y} },
S
:=
{ y ∈E \ Xk : Xk ∪{y} ∈F1 },
T
:=
{ y ∈E \ Xk : Xk ∪{y} ∈F2 }.
4⃝
Compute
m1
:=
max{c1(y) : y ∈S}
m2
:=
max{c2(y) : y ∈T }
¯S
:=
{ y ∈S : c1(y) = m1 }
¯T
:=
{ y ∈T : c2(y) = m2 }
¯A(1)
:=
{ (x, y) ∈A(1) : c1(x) = c1(y) },
¯A(2)
:=
{ (y, x) ∈A(2) : c2(x) = c2(y) },
¯G
:=
(E, ¯A(1) ∪¯A(2)).

316
13. Matroids
5⃝
Apply BFS to compute the set R of vertices reachable from ¯S in ¯G.
6⃝
If R ∩¯T ̸= ∅then: Find an ¯S- ¯T -path P in ¯G with a minimum number
of edges, set Xk+1 := Xk△V (P) and k := k + 1 and go to 2⃝.
7⃝
Compute
ε1
:=
min{c1(x) −c1(y) : (x, y) ∈A(1) ∩δ+(R)};
ε2
:=
min{c2(x) −c2(y) : (y, x) ∈A(2) ∩δ+(R)};
ε3
:=
min{m1 −c1(y) : y ∈S \ R};
ε4
:=
min{m2 −c2(y) : y ∈T ∩R};
ε
:=
min{ε1, ε2, ε3, ε4}
(where min ∅:= ∞).
8⃝
If ε < ∞then:
Set c1(x) := c1(x) −ε and c2(x) := c2(x) + ε for all x ∈R. Go to 4⃝.
If ε = ∞then:
Among X0, X1, . . . , Xk, let X be the one with maximum weight. Stop.
See Edmonds [1979] and Lawler [1976] for earlier versions of this algorithm.
Theorem 13.35.
(Frank [1981])
The Weighted Matroid Intersection Al-
gorithm correctly solves the Weighted Matroid Intersection Problem in
O(|E|4 + |E|3θ) time, where θ is the maximum complexity of the two indepen-
dence oracles.
Proof:
Let m be the ﬁnal value of k. The algorithm computes sets X0, X1, . . . ,
Xm. We ﬁrst prove that Xk ∈F1 ∩F2 for k = 0, . . . , m, by induction on k. This
is trivial for k = 0. If we are working with Xk ∈F1 ∩F2 for some k, ¯G is a
subgraph of (E, A(1) ∪A(2)) = G Xk. So if a path P is found in 5⃝, Lemma 13.28
ensures that Xk+1 ∈F1 ∩F2.
When the algorithm stops, we have ε1 = ε2 = ε3 = ε4 = ∞, so T is not
reachable from S in G Xm. Then by Lemma 13.30 m = |Xm| = max{|X| : X ∈
F1 ∩F2}.
To prove correctness, we show that for k = 0, . . . , m, c(Xk) = max{c(X) :
X ∈F1 ∩F2, |X| = k}. Since we always have c = c1 +c2, it sufﬁces to prove that
at any stage of the algorithm (13.5) holds. This is clearly true when the algorithm
starts (for k = 0); we show that (13.5) is never violated. We use Theorem 13.23.
When we set Xk+1 := Xk△V (P) in 6⃝we have to check that (13.5) holds.
Let P be an s-t-path, s ∈¯S, t ∈¯T . By deﬁnition of ¯G we have c1(Xk+1) =
c1(Xk)+c1(s) and c2(Xk+1) = c2(Xk)+c2(t). Since Xk satisﬁes (13.5), conditions
(a) and (b) of Theorem 13.23 must hold with respect to Xk and each of F1 and
F2.
By deﬁnition of ¯S both conditions continue to hold for Xk ∪{s} and F1.
Therefore c1(Xk+1) = c1(Xk∪{s}) = max{c1(Y) : Y ∈F1, |Y| = k+1}. Moreover,
by deﬁnition of ¯T , (a) and (b) of Theorem 13.23 continue to hold for Xk ∪{t}

13.7 Weighted Matroid Intersection
317
and F2, implying c2(Xk+1) = c2(Xk ∪{t}) = max{c2(Y) : Y ∈F2, |Y| = k + 1}.
In other words, (13.5) indeed holds for Xk+1.
Now suppose we change c1 and c2 in
8⃝. We ﬁrst show that ε > 0. By
(13.5) and Theorem 13.23 we have c1(x) ≥c1(y) for all y ∈E \ Xk and x ∈
C1(Xk, y) \ {y}. So for any (x, y) ∈A(1) we have c1(x) ≥c1(y). Moreover, by
the deﬁnition of R no edge (x, y) ∈δ+(R) belongs to ¯A(1). This implies ε1 > 0.
ε2 > 0 is proved analogously. m1 ≥c1(y) holds for all y ∈S. If in addition
y /∈R then y /∈¯S, so m1 > c1(y). Therefore ε3 > 0. Similarly, ε4 > 0 (using
¯T ∩R = ∅). We conclude that ε > 0.
We can now prove that 8⃝preserves (13.5). Let c′
1 be the modiﬁed c1, i.e.
c′
1(x) :=

c1(x) −ε
if x ∈R
c1(x)
if x /∈R . We prove that Xk and c′
1 satisfy the conditions
of Theorem 13.23 with respect to F1.
To prove (a), let y ∈E \ Xk and x ∈C1(Xk, y) \ {y}. Suppose c′
1(x) <
c′
1(y). Since c1(x) ≥c1(y) and ε > 0, we must have x ∈R and y /∈R. Since
also (x, y) ∈A(1), we have ε ≤ε1 ≤c1(x) −c1(y) = (c′
1(x) + ε) −c′
1(y), a
contradiction.
To prove (b), let x ∈Xk and y ∈E \ Xk with Xk ∪{y} ∈F1. Now suppose
c′
1(y) > c′
1(x). Since c1(y) ≤m1 ≤c1(x), we must have x ∈R and y /∈R. Since
y ∈S we have ε ≤ε3 ≤m1 −c1(y) ≤c1(x) −c1(y) = (c′
1(x) + ε) −c′
1(y), a
contradiction.
Let c′
2 be the modiﬁed c2, i.e. c′
2(x) :=

c2(x) + ε
if x ∈R
c2(x)
if x /∈R . We show that
Xk and c′
2 satisfy the conditions of Theorem 13.23 with respect to F2.
To prove (a), let y ∈E \ Xk and x ∈C2(Xk, y) \ {y}. Suppose c′
2(x) < c′
2(y).
Since c2(x) ≥c2(y), we must have y ∈R and x /∈R. Since also (y, x) ∈A(2),
we have ε ≤ε2 ≤c2(x) −c2(y) = c′
2(x) −(c′
2(y) −ε), a contradiction.
To prove (b), let x ∈Xk and y ∈E \ Xk with Xk ∪{y} ∈F2. Now suppose
c′
2(y) > c′
2(x). Since c2(y) ≤m2 ≤c2(x), we must have y ∈R and x /∈R. Since
y ∈T we have ε ≤ε4 ≤m2 −c2(y) ≤c2(x) −c2(y) = c′
2(x) −(c′
2(y) −ε), a
contradiction.
So we have proved that (13.5) is not violated during 8⃝, and thus the algorithm
works correctly.
We now consider the running time. Observe that after 8⃝, the new sets ¯S, ¯T ,
and R, as computed subsequently in 4⃝and 5⃝, are supersets of the old ¯S, ¯T ,
and R, respectively. If ε = ε4 < ∞, an augmentation (increase of k) follows.
Otherwise the cardinality of R increases immediately (in 5⃝) by at least one. So
4⃝– 8⃝are repeated less than |E| times between two augmentations.
Since the running time of 4⃝– 8⃝is O(|E|2), the total running time between
two augmentations is O(|E|3) plus O(|E|2) oracle calls (in 2⃝). Since there are
m ≤|E| augmentations, the stated overall running time follows.
2
The running time can easily be improved to O(|E|3θ) (Exercise 22).

318
13. Matroids
Exercises
1. Prove that all the independence systems apart from (5) and (6) in the list at
the beginning of Section 13.1 are – in general – not matroids.
2. Show that the uniform matroid with four elements and rank 2 is not a graphic
matroid.
3. Prove that every graphic matroid is representable over every ﬁeld.
4. Let G be an undirected graph, K ∈N, and let F contain those subsets of
E(G) that are the union of K forests. Prove that (E(G), F) is a matroid.
5. Compute a tight lower bound for the rank quotients of the independence
systems listed at the beginning of Section 13.1.
6. Let S be a family of sets. A set T is a transversal of S if there is a bijection
	 : T →S with t ∈	(t) for all t ∈T . (For a necessary and sufﬁcient
condition for the existence of a transversal, see Exercise 6 of Chapter 10.)
Assume that S has a transversal. Prove that the family of transversals of S is
the family of bases of a matroid.
7. Let E be a ﬁnite set and B ⊆2E. Show that B is the set of bases of some
matroid (E, F) if and only if the following holds:
(B1) B ̸= ∅;
(B2) For any B1, B2 ∈B and y ∈B2 \ B1 there exists an x ∈B1 \ B2 with
(B1 \ {x}) ∪{y} ∈B.
8. Let G be a graph. Let F be the family of sets X ⊆V (G), for which a
maximum matching exists that covers no vertex in X. Prove that (V (G), F)
is a matroid. What is the dual matroid?
9. Show that M(G∗)
=
(M(G))∗also holds for disconnected graphs G,
extending Theorem 13.16.
Hint: Use Exercise 31(a) of Chapter 2.
10. Show that the clutters in (3) and (6) in the list of Section 13.3 have the Max-
Flow-Min-Cut property. (Use Theorem 19.10.) Show that the clutters in (1),
(4) and (5) do not have the Max-Flow-Min-Cut property.
11.
∗
A clutter (E, F) is called binary if for all X1, . . . , Xk ∈F with k odd there
exists a Y ∈F with Y ⊆X1△· · · △Xk. Prove that the clutter of minimal
T -joins and the clutter of minimal T -cuts (example (7) of the list in Section
13.3) are binary. Prove that a clutter is binary if and only if |A ∩B| is odd for
all A ∈F and all B ∈F∗, where (E, F∗) is the blocking clutter. Conclude
that a clutter is binary if and only if its blocking clutter is binary.
Note: Seymour [1977] classiﬁed the binary clutters with the Max-Flow-Min-
Cut property.
12.
∗
Let P be a polyhedron of blocking type, i.e. we have x + y ∈P for all x ∈P
and y ≥0. The blocking polyhedron of P is deﬁned to be B(P) := {z : z⊤x ≥
1 for all x ∈P}. Prove that B(P) is again a polyhedron of blocking type and
that B(B(P)) = P.
Note: Compare this with Theorem 4.22.
13. How can one check (in polynomial time) whether a given set of edges of a
complete graph G is a subset of some Hamiltonian circuit in G?

Exercises
319
14. Prove that if (E, F) is a matroid, then the Best-In-Greedy maximizes any
bottleneck function c(F) = min{ce : e ∈F} over the bases.
15. Let (E, F) be a matroid and c : E →R such that c(e) ̸= c(e′) for all
e ̸= e′ and c(e) ̸= 0 for all e. Prove that both the Maximization and the
Minimization Problem for (E, F, c) have a unique optimum solution.
16.
∗
Prove that for matroids the independence, basis-superset, closure and rank
oracles are polynomially equivalent.
Hint: To show that the rank oracle reduces to the independence oracle, use
the Best-In-Greedy. To show that the independence oracle reduces to the
basis-superset oracle, use the Worst-Out-Greedy.
(Hausmann and Korte [1981])
17. Given an undirected graph G, we wish to colour the edges with a minimum
number of colours such that for any circuit C of G, the edges of C do not
all have the same colour. Show that there is a polynomial-time algorithm for
this problem.
18. Let (E, F1), . . . , (E, Fk) be matroids with rank functions r1, . . . ,rk. Prove
that a set X ⊆E is partitionable if and only if |A| ≤k
i=1 ri(A) for all
A ⊆X. Show that Theorem 6.19 is a special case.
(Edmonds and Fulkerson [1965])
19. Let (E, F) be a matroid with rank function r. Prove (using Theorem 13.34):
(a) (E, F) has k pairwise disjoint bases if and only if kr(A)+|E\A| ≥kr(E)
for all A ⊆E.
(b) (E, F) has k independent sets whose union is E if and only if kr(A) ≥|A|
for all A ⊆E.
Show that Theorem 6.19 and Theorem 6.16 are special cases.
20. Let (E, F1) and (E, F2) be two matroids. Let X be a maximal partitionable
subset with respect to (E, F1) and (E, F∗
2 ): X = X1
.
∪X2 with X1 ∈F1
and X2 ∈F∗
2 . Let B2 ⊇X2 be a basis of F∗
2 . Prove that then X \ B2 is a
maximum-cardinality set in F1 ∩F2.
(Edmonds [1970])
21. Let (E, S) be a set system, and let (E, F) be a matroid with rank function
r. Show that S has a transversal that is independent in (E, F) if and only if
r

B∈B B

≥|B| for all B ⊆S.
Hint: First describe the rank function of the matroid whose independent sets
are all transversals (Exercise 6), using Theorem 13.34. Then apply Theorem
13.31.
(Rado [1942])
22. Show that the running time of the Weighted Matroid Intersection Algo-
rithm (cf. Theorem 13.35) can be improved to O(|E|3θ).
23. Let (E, F1) and (E, F2) be two matroids, and c : E →R. Let X0, . . . , Xm ∈
F1 ∩F2 with |Xk| = k and c(Xk) = max{c(X) : X ∈F1 ∩F2, |X| = k} for
all k. Prove that for k = 1, . . . , m −2
c(Xk+1) −c(Xk) ≤c(Xk) −c(Xk−1).
(Krogdahl [unpublished])

320
13. Matroids
24. Consider the following problem. Given a digraph G with edge weights, a
vertex s ∈V (G), and a number K, ﬁnd a minimum weight subgraph H of G
containing K edge-disjoint paths from s to each other vertex. Show that this
reduces to the Weighted Matroid Intersection Problem.
Hint: See Exercise 18 of Chapter 6 and Exercise 4 of this chapter.
(Edmonds [1970]; Frank and Tardos [1989]; Gabow [1995])
25. Let A and B be two ﬁnite sets of cardinality n ∈N, ¯a ∈A, and c : {{a, b} :
a ∈A, b ∈B} →R a cost function. Let T be the family of edge sets of all
trees T with V (T ) = A
.
∪B and |δT (a)| = 2 for all a ∈A \ {¯a}. Show that
a minimum cost element of T can be computed in O(n7) time. How many
edges will be incident to ¯a?
References
General Literature:
Bixby, R.E., and Cunningham, W.H. [1995]: Matroid optimization and algorithms. In:
Handbook of Combinatorics; Vol. 1 (R.L. Graham, M. Gr¨otschel, L. Lov´asz, eds.), El-
sevier, Amsterdam, 1995
Cook, W.J., Cunningham, W.H., Pulleyblank, W.R., and Schrijver, A. [1998]: Combinato-
rial Optimization. Wiley, New York 1998, Chapter 8
Faigle, U. [1987]: Matroids in combinatorial optimization. In: Combinatorial Geometries
(N. White, ed.), Cambridge University Press, 1987
Gondran, M., and Minoux, M. [1984]: Graphs and Algorithms. Wiley, Chichester 1984,
Chapter 9
Lawler, E.L. [1976]: Combinatorial Optimization; Networks and Matroids. Holt, Rinehart
and Winston, New York 1976, Chapters 7 and 8
Oxley, J.G. [1992]: Matroid Theory. Oxford University Press, Oxford 1992
von Randow, R. [1975]: Introduction to the Theory of Matroids. Springer, Berlin 1975
Recski, A. [1989]: Matroid Theory and its Applications. Springer, Berlin, 1989
Schrijver, A. [2003]: Combinatorial Optimization: Polyhedra and Efﬁciency. Springer,
Berlin 2003, Chapters 39–42
Welsh, D.J.A. [1976]: Matroid Theory. Academic Press, London 1976
Cited References:
Cunningham, W.H. [1986] : Improved bounds for matroid partition and intersection algo-
rithms. SIAM Journal on Computing 15 (1986), 948–957
Edmonds, J. [1970]: Submodular functions, matroids and certain polyhedra. In: Combina-
torial Structures and Their Applications; Proceedings of the Calgary International Con-
ference on Combinatorial Structures and Their Applications 1969 (R. Guy, H. Hanani,
N. Sauer, J. Schonheim, eds.), Gordon and Breach, New York 1970, pp. 69–87
Edmonds, J. [1971]: Matroids and the greedy algorithm. Mathematical Programming 1
(1971), 127–136
Edmonds, J. [1979]: Matroid intersection. In: Discrete Optimization I; Annals of Discrete
Mathematics 4 (P.L. Hammer, E.L. Johnson, B.H. Korte, eds.), North-Holland, Amster-
dam 1979, pp. 39–49
Edmonds, J., and Fulkerson, D.R. [1965]: Transversals and matroid partition. Journal of
Research of the National Bureau of Standards B 69 (1965), 67–72

References
321
Frank, A. [1981]: A weighted matroid intersection algorithm. Journal of Algorithms 2
(1981), 328–336
Frank, A., and Tardos, ´E. [1989]: An application of submodular ﬂows. Linear Algebra and
Its Applications 114/115 (1989), 329–348
Fulkerson, D.R. [1971]: Blocking and anti-blocking pairs of polyhedra. Mathematical Pro-
gramming 1 (1971), 168–194
Gabow, H.N. [1995]: A matroid approach to ﬁnding edge connectivity and packing arbores-
cences. Journal of Computer and System Sciences 50 (1995), 259–273
Gabow, H.N., and Xu, Y. [1996]: Efﬁcient theoretic and practical algorithms for linear
matroid intersection problems. Journal of Computer and System Sciences 53 (1996),
129–147
Hausmann, D., Jenkyns, T.A., and Korte, B. [1980]: Worst case analysis of greedy type
algorithms for independence systems. Mathematical Programming Study 12 (1980), 120–
131
Hausmann, D., and Korte, B. [1981]: Algorithmic versus axiomatic deﬁnitions of matroids.
Mathematical Programming Study 14 (1981), 98–111
Jenkyns, T.A. [1976]: The efﬁciency of the greedy algorithm. Proceedings of the 7th S-
E Conference on Combinatorics, Graph Theory, and Computing, Utilitas Mathematica,
Winnipeg 1976, pp. 341–350
Korte, B., and Hausmann, D. [1978]: An analysis of the greedy algorithm for independence
systems. In: Algorithmic Aspects of Combinatorics; Annals of Discrete Mathematics 2
(B. Alspach, P. Hell, D.J. Miller, eds.), North-Holland, Amsterdam 1978, pp. 65–74
Korte, B., and Monma, C.L. [1979]: Some remarks on a classiﬁcation of oracle-type algo-
rithms. In: Numerische Methoden bei graphentheoretischen und kombinatorischen Prob-
lemen; Band 2 (L. Collatz, G. Meinardus, W. Wetterling, eds.), Birkh¨auser, Basel 1979,
pp. 195–215
Lehman, A. [1979]: On the width-length inequality. Mathematical Programming 17 (1979),
403–417
Nash-Williams, C.S.J.A. [1967]: An application of matroids to graph theory. In: Theory of
Graphs; Proceedings of an International Symposium in Rome 1966 (P. Rosenstiehl, ed.),
Gordon and Breach, New York, 1967, pp. 263–265
Rado, R. [1942]: A theorem on independence relations. Quarterly Journal of Math. Oxford
13 (1942), 83–89
Rado, R. [1957]: Note on independence functions. Proceedings of the London Mathematical
Society 7 (1957), 300–320
Seymour, P.D. [1977]: The matroids with the Max-Flow Min-Cut property. Journal of
Combinatorial Theory B 23 (1977), 189–222
Whitney, H. [1933]: Planar graphs. Fundamenta Mathematicae 21 (1933), 73–84
Whitney, H. [1935]: On the abstract properties of linear dependence. American Journal of
Mathematics 57 (1935), 509–533

14. Generalizations of Matroids
There are several interesting generalizations of matroids. We have already seen in-
dependence systems in Section 13.1, which arose from dropping the axiom (M3).
In Section 14.1 we consider greedoids, arising by dropping (M2) instead. More-
over, certain polytopes related to matroids and to submodular functions, called
polymatroids, lead to strong generalizations of important theorems; we shall dis-
cuss them in Section 14.2. In Sections 14.3 and 14.4 we consider two approaches
to the problem of minimizing an arbitrary submodular function: one using the
Ellipsoid Method, and one with a combinatorial algorithm. For the important
special case of symmetric submodular functions we mention a simpler algorithm
in Section 14.5.
14.1 Greedoids
By deﬁnition, set systems (E, F) are matroids if and only if they satisfy
(M1) ∅∈F;
(M2) If X ⊆Y ∈F then X ∈F;
(M3) If X, Y ∈F and |X| > |Y|, then there is an x ∈X \ Y with Y ∪{x} ∈F.
If we drop (M3), we obtain independence systems, discussed in Sections 13.1 and
13.4. Now we drop (M2) instead:
Deﬁnition 14.1.
A greedoid is a set system (E, F) satisfying (M1) and (M3).
Instead of the subclusiveness (M2) we have accessibility: We call a set system
(E, F) accessible if ∅∈F and for any X ∈F \ {∅} there exists an x ∈X with
X \ {x} ∈F. Greedoids are accessible (accessibility follows directly from (M1)
and (M3)). Though more general than matroids, they comprise a rich structure
and, on the other hand, generalize many different, seemingly unrelated concepts.
We start with the following result:
Theorem 14.2.
Let (E, F) be an accessible set system. The following statements
are equivalent:
(a) For any X ⊆Y ⊂E and z ∈E \ Y with X ∪{z} ∈F and Y ∈F we have
Y ∪{z} ∈F;
(b) F is closed under union.

324
14. Generalizations of Matroids
Proof:
(a) ⇒(b): Let X, Y ∈F; we show that X ∪Y ∈F. Let Z be a maximal
set with Z ∈F and X ⊆Z ⊆X ∪Y. Suppose Y \ Z ̸= ∅. By repeatedly applying
accessibility to Y we get a set Y ′ ∈F with Y ′ ⊆Z and an element y ∈Y \ Z with
Y ′ ∪{y} ∈F. We apply (a) to Z, Y ′ and y and obtain Z ∪{y} ∈F, contradicting
the choice of Z.
(b) ⇒(a) is trivial.
2
If the conditions in Theorem 14.2 hold, then (E, F) is called an antimatroid.
Proposition 14.3.
Every antimatroid is a greedoid.
Proof:
Let (E, F) be an antimatroid, i.e. accessible and closed under union. To
prove (M3), let X, Y ∈F with |X| > |Y|. Since (E, F) is accessible there is an
order X = {x1, . . . , xn} with {x1, . . . , xi} ∈F for i = 0, . . . , n. Let i ∈{1, . . . , n}
be the minimum index with xi /∈Y; then Y ∪{xi} = Y ∪{x1, . . . , xi} ∈F (since
F is closed under union).
2
Another equivalent deﬁnition of antimatroids is by a closure operator:
Proposition 14.4.
Let (E, F) be a set system such that F is closed under union
and ∅∈F. Deﬁne
τ(A) :=
4
{X ⊆E : A ⊆X, E \ X ∈F}
Then τ is a closure operator, i.e. satisﬁes (S1)–(S3) of Theorem 13.11.
Proof:
Let X ⊆Y ⊆E. X ⊆τ(X) ⊆τ(Y) is trivial. To prove (S3), assume
that there exists a y ∈τ(τ(X)) \ τ(X). Then y ∈Y for all Y ⊆E with τ(X) ⊆Y
and E \ Y ∈F, but there exists a Z ⊆E \ {y} with X ⊆Z and E \ Z ∈F. This
implies τ(X) ̸⊆Z, a contradiction.
2
Theorem 14.5.
Let (E, F) be a set system such that F is closed under union
and ∅∈F. Then (E, F) is accessible if and only if the closure operator τ of
Proposition 14.4 satisﬁes the anti-exchange property: if X ⊆E, y, z ∈E \ τ(X),
y ̸= z and z ∈τ(X ∪{y}), then y /∈τ(X ∪{z}).
Proof:
If ((E, F) is accessible, then (M3) holds by Proposition 14.3. To show
the anti-exchange property, let X ⊆E, B := E \ τ(X), and y, z ∈B with
z /∈A := E \ τ(X ∪{y}). Observe that A ∈F, B ∈F and A ⊆B \ {y, z}.
By applying (M3) to A and B we get an element b ∈B \ A ⊆E \ (X ∪A)
with A ∪{b} ∈F. A ∪{b} cannot be a subset of E \ (X ∪{y}) (otherwise
τ(X ∪{y}) ⊆E \ (A ∪{b}), contradicting τ(X ∪{y}) = E \ A). Hence b = y.
So we have A ∪{y} ∈F and thus τ(X ∪{z}) ⊆E \ (A ∪{y}). We have proved
y /∈τ(X ∪{z}).
To show the converse, let A ∈F \{∅} and let X := E \ A. We have τ(X) = X.
Let a ∈A such that |τ(X ∪{a})| is minimum. We claim that τ(X ∪{a}) = X ∪{a},
i.e. A \ {a} ∈F.
Suppose, on the contrary, that b ∈τ(X ∪{a}) \ (X ∪{a}). By (c) we have
a /∈τ(X ∪{b}). Moreover,

14.1 Greedoids
325
τ(X ∪{b}) ⊆τ(τ(X ∪{a}) ∪{b}) = τ(τ(X ∪{a})) = τ(X ∪{a}).
Hence τ(X ∪{b}) is a proper subset of τ(X ∪{a}), contradicting the choice of a.
2
The anti-exchange property of Theorem 14.5 is different from (S4). While (S4)
of Theorem 13.11 is a property of linear hulls in Rn, this is a property of convex
hulls in Rn: if y ̸= z, z ̸∈conv(X) and z ∈conv(X ∪{y}), then clearly y /∈
conv(X ∪{z}). So for any ﬁnite set E ⊂Rn, (E, {X ⊆E : X ∩conv(E \ X) = ∅})
is an antimatroid.
Greedoids generalize matroids and antimatroids, but they also contain other
interesting structures. One example is the blossom structure we used in Edmonds’
Cardinality Matching Algorithm (Exercise 1). Another basic example is:
Proposition 14.6.
Let G be a graph (directed or undirected) and r ∈V (G). Let
F be the family of all edge sets of arborescences in G rooted at r, or trees in G
containing r (not necessarily spanning). Then (E(G), F) is a greedoid.
Proof:
(M1) is trivial. We prove (M3) for the directed case; the same argument
applies to the undirected case. Let (X1, F1) and (X2, F2) be two arborescences in
G rooted at r with |F1| > |F2|. Then |X1| = |F1| + 1 > |F2| + 1 = |X2|, so let
x ∈X1 \ X2. The r-x-path in (X1, F1) contains an edge (v, w) with v ∈X2 and
w /∈X2. This edge can be added to (X2, F2), proving that F2 ∪{(v, w)} ∈F. 2
This greedoid is called the directed (undirected) branching greedoid of G.
The problem of ﬁnding a maximum weight spanning tree in a connected graph
G with nonnegative weights is the Maximization Problem for the cycle matroid
M(G). The Best-In-Greedy Algorithm is in this case nothing but Kruskal’s
Algorithm. Now we have a second formulation of the same problem: we are
looking for a maximum weight set F with F ∈F, where (E(G), F) is the
undirected branching greedoid of G.
We now formulate a general greedy algorithm for greedoids. In the special case
of matroids, it is exactly the Best-In-Greedy Algorithm discussed in Section
13.4. If we have an undirected branching greedoid with a modular cost function
c, it is Prim’s Algorithm:
Greedy Algorithm For Greedoids
Input:
A greedoid (E, F) and a function c : 2E →R, given by an oracle
which for any given X ⊆E says whether X ∈F and returns c(X).
Output:
A set F ∈F.
1⃝
Set F := ∅.
2⃝
Let e ∈E \ F such that F ∪{e} ∈F and c(F ∪{e}) is maximum;
if no such e exists then stop.
3⃝
Set F := F ∪{e} and go to 2⃝.

326
14. Generalizations of Matroids
Even for modular cost functions c this algorithm does not always provide an
optimal solution. At least we can characterize those greedoids where it works:
Theorem 14.7.
Let (E, F) be a greedoid. The Greedy Algorithm For Gree-
doids ﬁnds a set F ∈F of maximum weight for each modular weight function
c : 2E →R+ if and only if (E, F) has the so-called strong exchange property:
For all A ∈F, B maximal in F, A ⊆B and x ∈E \ B with A ∪{x} ∈F there
exists a y ∈B \ A such that A ∪{y} ∈F and (B \ y) ∪{x} ∈F.
Proof:
Suppose (E, F) is a greedoid with the strong exchange property. Let
c : E →R+, and let A = {a1, . . . , al} be the solution found by the Greedy Al-
gorithm For Greedoids, where the elements are chosen in the order a1, . . . , al.
Let B = {a1, . . . , ak}
.
∪B′ be an optimum solution such that k is max-
imum, and suppose that k < l. Then we apply the strong exchange property
to {a1, . . . , ak}, B and ak+1. We conclude that there exists a y ∈B′ with
{a1, . . . , ak, y} ∈F and (B \ y) ∪{ak+1} ∈F. By the choice of ak+1 in
2⃝
of the Greedy Algorithm For Greedoids we have c(ak+1) ≥c(y) and thus
c((B \ y) ∪{ak+1}) ≥c(B), contradicting the choice of B.
Conversely, let (E, F) be a greedoid that does not have the strong exchange
property. Let A ∈F, B maximal in F, A ⊆B and x ∈E \ B with A ∪{x} ∈F
such that for all y ∈B \ A with A ∪{y} ∈F we have (B \ y) ∪{x} /∈F.
Let Y := {y ∈B \ A : A ∪{y} ∈F}. We set c(e) := 2 for e ∈B \ Y, and
c(e) := 1 for e ∈Y ∪{x} and c(e) := 0 for e ∈E \ (B ∪{x}). Then the Greedy
Algorithm For Greedoids might choose the elements of A ﬁrst (they have
weight 2) and then might choose x. It will eventually end up with a set F ∈F
that cannot be optimal, since c(F) ≤c(B ∪{x}) −2 < c(B ∪{x}) −1 = c(B) and
B ∈F.
2
Indeed, optimizing modular functions over general greedoids is NP-hard. This
follows from the following observation (together with Corollary 15.24):
Proposition 14.8.
The problem of deciding, given an undirected graph G and k ∈
N, whether G has a vertex cover of cardinality k, linearly reduces to the following
problem: Given a greedoid (E, F) (by a membership oracle) and a function c :
E →R+, ﬁnd an F ∈F with c(F) maximum.
Proof:
Let G be any undirected graph and k ∈N. Let D := V (G)
.
∪E(G) and
F := {X ⊆D : for all e = {v, w} ∈E(G) ∩X we have v ∈X or w ∈X}.
(D, F) is an antimatroid: it is accessible and closed under union. In particular, by
Proposition 14.3, it is a greedoid.
Now consider F′ := {X ∈F : |X| ≤|E(G)| + k}. Since (M1) and (M3) are
preserved, (D, F′) is also a greedoid. Set c(e) := 1 for e ∈E(G) and c(v) := 0
for v ∈V (G). Then there exists a set F ∈F′ with c(F) = |E(G)| if and only if
G contains a vertex cover of size k.
2

14.2 Polymatroids
327
On the other hand, there are interesting functions that can be maximized over
arbitrary greedoids, for example bottleneck functions c(F) := min{c′(e) : e ∈F}
for some c′ : E →R+ (Exercise 2). See (Korte, Lov´asz and Schrader [1991]) for
more results in this area.
14.2 Polymatroids
From Theorem 13.10 we know the tight connection between matroids and sub-
modular functions. Submodular functions deﬁne the following interesting class of
polyhedra:
Deﬁnition 14.9.
A polymatroid is a polytope of type
P( f ) :=

x ∈RE : x ≥0,

e∈A
xe ≤f (A) for all A ⊆E

where E is a ﬁnite set and f : 2E →R+ is a submodular function.
It is not hard to see that for any polymatroid f can be chosen such that
f (∅) = 0 and f is monotone (Exercise 5; a function f : 2E →R is called
monotone if f (X) ≤f (Y) for X ⊆Y ⊆E). Edmonds’ original deﬁnition was
different; see Exercise 6. Moreover, we mention that the term polymatroid is
sometimes not used for the polytope but for the pair (E, f ).
If f is the rank function of a matroid, P( f ) is the convex hull of the incidence
vectors of the independent sets of this matroid (Theorem 13.21). We know that
the Best-In-Greedy optimizes any linear function over a matroid polytope. A
similar greedy algorithm also works for general polymatroids. We assume that f
is monotone:
Polymatroid Greedy Algorithm
Input:
A ﬁnite set E and a submodular, monotone function f : 2E →R+
(given by an oracle). A vector c ∈RE.
Output:
A vector x ∈P( f ) with cx maximum.
1⃝
Sort E = {e1, . . . , en} such that c(e1) ≥· · · ≥c(ek) > 0 ≥c(ek+1) ≥
· · · ≥c(en).
2⃝
If k ≥1 then set x(e1) := f ({e1}).
Set x(ei) := f ({e1, . . . , ei}) −f ({e1, . . . , ei−1}) for i = 2, . . . , k.
Set x(ei) := 0 for i = k + 1, . . . , n.
Proposition 14.10.
Let E = {e1, . . . , en} and f : 2E →R be a submodular
function with f (∅) ≥0. Let b : E →R with b(e1) ≤f ({e1}) and b(ei) ≤
f ({e1, . . . , ei}) −f ({e1, . . . , ei−1}) for i = 2, . . . , n. Then 
a∈A b(a) ≤f (A) for
all A ⊆E.

328
14. Generalizations of Matroids
Proof:
Induction on i = max{ j : ej ∈A}. The assertion is trivial for A = ∅
and A = {e1}. If i ≥2, then 
a∈A b(a) = 
a∈A\{ei} b(a) + b(ei) ≤
f (A \
{ei}) + b(ei) ≤f (A \ {ei}) + f ({e1, . . . , ei}) −f ({e1, . . . , ei−1}) ≤f (A), where
the ﬁrst inequality follows from the induction hypothesis and the third one from
submodularity.
2
Theorem 14.11.
The Polymatroid Greedy Algorithm correctly ﬁnds an x ∈
P( f ) with cx maximum. If f is integral, then x is also integral.
Proof:
Let x ∈RE be the output of the Polymatroid Greedy Algorithm for
E, f and c. By deﬁnition, if f is integral, then x is also integral. We have x ≥0
since f is monotone, and thus x ∈P( f ) by Proposition 14.10.
Now let y ∈RE
+ with cy > cx. Similarly as in the proof of Theorem 13.19
we set dj := c(ej) −c(ej+1) ( j = 1, . . . , k −1) and dk := c(ek), and we have
k

j=1
dj
j

i=1
x(ei) = cx < cy ≤
k

j=1
c(ej)y(ej) =
k

j=1
dj
j

i=1
y(ei).
Since dj ≥0 for all j there is an index j ∈{1, . . . , k} with  j
i=1 y(ei) >
 j
i=1 x(ei); however, since  j
i=1 x(ei) = f ({e1, . . . , ej}) this means that y /∈
P( f ).
2
As with matroids, we can also handle the intersection of two polymatroids.
The following polymatroid intersection theorem has many implications:
Theorem 14.12.
(Edmonds [1970,1979])
Let E be a ﬁnite set, and let f, g :
2E →R+ be submodular functions. Then the system
x
≥
0

e∈A
xe
≤
f (A)
(A ⊆E)

e∈A
xe
≤
g(A)
(A ⊆E)
is TDI.
Proof:
Consider the primal-dual pair of LPs
max

cx :

e∈A
xe ≤f (A) and

e∈A
xe ≤g(A) for all A ⊆E, x ≥0

and
min

A⊆E
( f (A)yA + g(A)z A) :

A⊆E, e∈A
(yA + z A) ≥ce for all e ∈E, y, z ≥0

.
To show total dual integrality, we use Lemma 5.22.

14.2 Polymatroids
329
Let c : E(G) →Z, and let y, z be an optimum dual solution for which

A⊆E
(yA + z A)|A||E \ A|
(14.1)
is as small as possible. We claim that F := {A ⊆E : yA > 0} is a chain, i.e. for
any A, B ∈F either A ⊆B or B ⊆A.
To see this, suppose A, B ∈F with A ∩B ̸= A and A ∩B ̸= B. Let ϵ :=
min{yA, yB}. Set y′
A := yA−ϵ, y′
B := yB −ϵ, y′
A∩B := yA∩B +ϵ, y′
A∪B := yA∪B +ϵ,
and y′(S) := y(S) for all other S ⊆E. Since y′, z is a feasible dual solution, it is
also optimum ( f is submodular) and contradicts the choice of y, because (14.1)
is smaller for y′, z.
By the same argument, F′ := {A ⊆E : z A > 0} is a chain. Now let M and
M′ be the matrices whose columns are indexed with the elements of E and whose
rows are the incidence vectors of the elements F and F′, respectively. By Lemma
5.22, it sufﬁces to show that

M
M′

is totally unimodular.
Here we use Ghouila-Houri’s Theorem 5.23. Let R be a set of rows, say
R = {A1, . . . , Ap, B1, . . . , Bq} with A1 ⊇· · · ⊇Ap and B1 ⊇· · · ⊇Bq. Let
R1 := {Ai : i odd} ∪{Bi : i even} and R2 := R \ R1. Since for any e ∈E we
have {R ∈R : e ∈R} = {A1, . . . , Ape} ∪{B1, . . . , Bqe} for some pe ∈{0, . . . , p}
and qe ∈{0, . . . , q}, the sum of the rows in R1 minus the sum of the rows in R2
is a vector with entries −1, 0, 1 only. So the criterion of Theorem 5.23 is satisﬁed.
2
One can optimize linear functions over the intersection of two polymatroids.
However, this is not as easy as with a single polymatroid. But we can use the El-
lipsoid Method if we can solve the Separation Problem for each polymatroid.
We return to this question in Section 14.3.
Corollary 14.13.
(Edmonds [1970]) Let (E, M1) and (E, M2) be two matroids
with rank functions r1 and r2. Then the convex hull of the incidence vectors of the
elements of M1 ∩M2 is the polytope

x ∈RE
+ :

e∈A
xe ≤min{r1(A),r2(A)} for all A ⊆E

.
Proof:
As r1 and r2 are nonnegative and submodular (by Theorem 13.10), the
above inequality system is TDI (by Theorem 14.12). Since r1 and r2 are integral,
the polytope is integral (by Corollary 5.14). Since r1(A) ≤|A| for all A ⊆E,
the vertices (the convex hull of which the polytope is by Corollary 3.27) are 0-
1-vectors, and thus incidence vectors of common independent sets (elements of
M1∩M2). On the other hand, each such incidence vector satisﬁes the inequalities
(by deﬁnition of the rank function).
2
Of course, the description of the matroid polytope (Theorem 13.21) follows
from this by setting M1 = M2. Theorem 14.12 has some further consequences:

330
14. Generalizations of Matroids
Corollary 14.14.
(Edmonds [1970]) Let E be a ﬁnite set, and let f, g : 2E →
R+ be submodular and monotone functions. Then
max{1lx : x ∈P( f ) ∩P(g)} = min
A⊆E( f (A) + g(E \ A)).
Moreover, if f and g are integral, there exists an integral x attaining the maximum.
Proof:
By Theorem 14.12, the dual to
max{1lx : x ∈P( f ) ∩P(g)},
which is
min

A⊆E
( f (A)yA + g(A)z A) :

A⊆E, e∈A
(yA + z A) ≥1 for all e ∈E, y, z ≥0

,
has an integral optimum solution y, z. Let B := 
A:yA≥1 A and C := 
A:z A≥1 A.
Let y′
B := 1, z′
C := 1 and let all other components of y′ and z′ be zero. We have
B ∪C = E and y′, z′ is a feasible dual solution. Since f and g are submodular
and nonnegative,

A⊆E
( f (A)yA + g(A)z A) ≥
f (B) + g(C).
Since E \ B ⊆C and g is monotone, this is at least f (B) + g(E \ B), proving
“≥”.
The other inequality “≤” is trivial, because for any A ⊆E we obtain a feasible
dual solution y, z by setting yA := 1, zE\A := 1 and all other components to zero.
The integrality follows directly from Theorem 14.12 and Corollary 5.14.
2
Theorem 13.31 is a special case. Moreover we obtain:
Corollary 14.15.
(Frank [1982]) Let E be a ﬁnite set and f, g : 2E →R such
that f is supermodular, g is submodular and f ≤g. Then there exists a modular
function h : 2E →R with f ≤h ≤g. If f and g are integral, h can be chosen
integral.
Proof:
Let M := 2 max{| f (A)| + |g(A)| : A ⊆E}. Let f ′(A) := g(E) −f (E \
A) + M|A| and g′(A) := g(A) −f (∅) + M|A| for all A ⊆E. f ′ and g′ are
nonnegative, submodular and monotone. An application of Corollary 14.14 yields
max{1lx : x ∈P( f ′) ∩P(g′)}
=
min
A⊆E( f ′(A) + g′(E \ A))
=
min
A⊆E(g(E) −f (E \ A) + M|A| + g(E \ A) −f (∅) + M|E \ A|)
≥
g(E) −f (∅) + M|E|.
So let x ∈P( f ′) ∩P(g′) with 1lx = g(E) −f (∅) + M|E|. If f and g are
integral, x can be chosen integral. Let h′(A) := 
e∈A xe and h(A) := h′(A) +

14.3 Minimizing Submodular Functions
331
f (∅) −M|A| for all A ⊆E. h is modular. Moreover, for all A ⊆E we have
h(A) ≤g′(A)+ f (∅)−M|A| = g(A) and h(A) = 1lx −h′(E \ A)+ f (∅)−M|A| ≥
g(E) + M|E| −M|A| −f ′(E \ A) = f (A).
2
The analogy to convex and concave functions is obvious; see also Exercise 9.
14.3 Minimizing Submodular Functions
The Separation Problem for a polymatroid P( f ) and a vector x asks for a set A
with f (A) < 
e∈A x(e). So this problem reduces to ﬁnding a set A minimizing
g(A), where g(A) := f (A) −
e∈A x(e). Note that if f is submodular, then g is
also submodular. Therefore it is an interesting problem to minimize submodular
functions.
Another motivation might be that submodular functions can be regarded as
the discrete analogue of convex functions (Corollary 14.15 and Exercise 9). We
have already solved a special case in Section 8.7: ﬁnding the minimum cut in an
undirected graph can be regarded as minimizing a certain symmetric submodular
function f : 2U →R+, over 2U \ {∅, U}. Before returning to this special case
we ﬁrst show how to minimize general submodular functions. We assume that we
are given an upper bound on size( f (S)). For simplicity we restrict ourselves to
integer-valued submodular functions:
Submodular Function Minimization Problem
Instance:
A ﬁnite set U. A submodular function f : 2U →Z (given by an
oracle).
Task:
Find a subset X ⊆U with f (X) minimum.
Gr¨otschel, Lov´asz and Schrijver [1981] showed how this problem can be solved
with the help of the Ellipsoid Method. The idea is to determine the minimum
by binary search; this will reduce the problem to the Separation Problem for a
polymatroid. Using the equivalence of separation and optimization (Section 4.6),
it thus sufﬁces to optimize linear functions over polymatroids. However, this can
be easily done by the Polymatroid Greedy Algorithm. We ﬁrst need an upper
bound on | f (S)| for S ⊆U:
Proposition 14.16.
For any submodular function f : 2U →Z and any S ⊆U
we have
f (U)−

u∈U
max{0, f ({u})−f (∅)} ≤f (S) ≤f (∅)+

u∈U
max{0, f ({u})−f (∅)}.
In particular, a number B with | f (S)| ≤B for all S ⊆U can be computed in
linear time, with |U| + 2 oracle calls to f .
Proof:
By repeated application of submodularity we get for ∅̸= S ⊆U (let
x ∈S):

332
14. Generalizations of Matroids
f (S) ≤−f (∅)+ f (S \{x})+ f ({x}) ≤· · · ≤−|S| f (∅)+ f (∅)+

x∈S
f ({x}),
and for S ⊂U (let y ∈U \ S):
f (S)
≥
−f ({y}) + f (S ∪{y}) + f (∅) ≥· · ·
≥
−

y∈U\S
f ({y}) + f (U) + |U \ S| f (∅).
2
Proposition 14.17.
The following problem can be solved in polynomial time:
Given a ﬁnite set U, a submodular and monotone function f : 2U →Z+ (by
an oracle) with f (S) > 0 for S ̸= ∅, a number B ∈N with f (S) ≤B for all
S ⊆U, and a vector x ∈ZU
+, decide if x ∈P( f ) and otherwise return a set
S ⊆U with 
v∈S x(v) > f (S).
Proof:
This is the Separation Problem for the polymatroid P( f ). We will
use Theorem 4.23, because we have already solved the optimization problem for
P( f ): the Polymatroid Greedy Algorithm maximizes any linear function over
P( f ) (Theorem 14.11).
We have to check the prerequisites of Theorem 4.23. Since the zero vector and
the unit vectors are all in P( f ), we can take x0 := ϵ1l as a point in the interior,
where ϵ =
1
|U|+1. We have size(x0) = O(|U| log |U|)). Moreover, each vertex of
P( f ) is produced by the Polymatroid Greedy Algorithm (for some objective
function; cf. Theorem 14.11) and thus has size O(|U|(2 + log B)). We conclude
that the Separation Problem can be solved in polynomial time. By Theorem
4.23, we get a facet-deﬁning inequality of P( f ) violated by x if x /∈P( f ). This
corresponds to a set S ⊆U with 
v∈S x(v) > f (S).
2
Since we do not require that f is monotone, we cannot apply this result
directly. Instead we consider a different function:
Proposition 14.18.
Let f : 2U →R be a submodular function and β ∈R. Then
g : 2U →R, deﬁned by
g(X) := f (X) −β +

e∈X
( f (U \ {e}) −f (U)),
is submodular and monotone.
Proof:
The submodularity of g follows directly from the submodularity of f . To
show that g is monotone, let X ⊂U and e ∈U \ X. We have g(X ∪{e})−g(X) =
f (X ∪{e}) −f (X) + f (U \ {e}) −f (U) ≥0 since f is submodular.
2
Theorem 14.19.
The Submodular Function Minimization Problem can be
solved in time polynomial in |U| + log max{| f (S)| : S ⊆U}.
Proof:
Let U be a ﬁnite set; suppose we are given f by an oracle. First compute
a number B ∈N with | f (S)| ≤B for all S ⊆U (cf. Proposition 14.16). Since f
is submodular, we have for each e ∈U and for each X ⊆U \ {e}:

14.4 Schrijver’s Algorithm
333
f ({e}) −f (∅) ≥
f (X ∪{e}) −f (X) ≥
f (U) −f (U \ {e}).
(14.2)
If, for some e ∈U, f ({e}) −f (∅) ≤0, then by (14.2) there is an optimum
set S containing e. In this case we consider the instance (U ′, B, f ′) deﬁned by
U ′ := U \ {e} and f ′(X) := f (X ∪{e}) for X ⊆U \ {e}, ﬁnd a set S′ ⊆U ′ with
f ′(S′) minimum and output S := S′ ∪{e}.
Similarly, if f (U) −f (U \ {e}) ≥0, then by (14.2) there is an optimum set S
not containing e. In this case we simply minimize f restricted to U \ {e}. In both
cases we have reduced the size of the ground set.
So we may assume that f ({e}) −f (∅) > 0 and f (U \ {e}) −f (U) > 0
for all e ∈U. Let x(e) := f (U \ {e}) −f (U). For each integer β with −B ≤
β ≤f (∅) we deﬁne g(X) := f (X) −β + 
e∈X x(e). By Proposition 14.18, g
is submodular and monotone. Furthermore we have g(∅) = f (∅) −β ≥0 and
g({e}) = f ({e}) −β + x(e) > 0 for all e ∈U, and thus g(X) > 0 for all
∅̸= X ⊆U. Now we apply Proposition 14.17 and check if x ∈P(g). If yes, we
have f (X) ≥β for all X ⊆U and we are done. Otherwise we get a set S with
f (S) < β.
Now we apply binary search: By choosing β appropriately each time, we ﬁnd
after O(log(2B)) iterations the number β∗∈{−B, −B + 1, . . . , f (∅)} for which
f (X) ≥β∗for all X ⊆U but f (S) < β∗+ 1 for some S ⊆U. This set S
minimizes f .
2
The ﬁrst strongly polynomial-time algorithm has been designed by Gr¨otschel,
Lov´asz and Schrijver [1988], also based on the ellipsoid method. Combinato-
rial algorithms to solve the Submodular Function Minimization Problem in
strongly polynomial time have been found by Schrijver [2000] and independently
by Iwata, Fleischer and Fujishige [2001]. In the next section we describe Schri-
jver’s algorithm.
14.4 Schrijver’s Algorithm
For a ﬁnite set U and a submodular function f : 2U →R, assume w.l.o.g. that
U = {1, . . . , n} and f (∅) = 0. Schrijver’s [2000] algorithm has, at any stage, a
point x in the so-called base polyhedron of f , deﬁned by

x ∈RU :

u∈A
x(u) ≤f (A) for all A ⊆U,

u∈U
x(u) = f (U)

.
We mention that the set of vertices of this base polyhedron is precisely the set of
vectors b≺for all total orders ≺of U, where we deﬁne
b≺(u) := f ({v ∈U : v ⪯u}) −f ({v ∈U : v ≺u})
(u ∈U). This fact, which we will not need here, can be proved similar to Theorem
14.11 (Exercise 13).

334
14. Generalizations of Matroids
The point x is always written as an explicit convex combination x = λ1b≺1 +
· · · + λkb≺k of these vertices. Initially, one can choose k = 1 and any total order.
Schrijver’s Algorithm
Input:
A ﬁnite set U = {1, . . . , n}. A submodular function f : 2U →Z
with f (∅) = 0 (given by an oracle).
Output:
A subset X ⊆U with f (X) minimum.
1⃝
Set k := 1, let ≺1 be any total order on U, and set x := b≺1.
2⃝
Set D := (U, A), where A = {(u, v) : u ≺i v for some i ∈{1, . . . , k}}.
3⃝
Let P := {v ∈U : x(v) > 0} and N := {v ∈U : x(v) < 0}, and let X be
the set of vertices not reachable from P in D.
If N ⊆X, then stop. Otherwise let d(v) denote the distance from P to v
in D.
4⃝
Choose the vertex t ∈N reachable from P with (d(t), t) lexicographically
maximum, and then the vertex s with (s, t) ∈A, d(s) = d(t) −1, and s
maximum.
Let i ∈{1, . . . , k} such that α := |{v : s ≺i v ⪯i t}| is maximum (the
number of indices attaining this maximum will be denoted by β).
5⃝
Let ≺s,u
i
result from ≺i by moving u just before s in the total order, and
let χu denote the incidence vector of u (u ∈U).
Compute a number ϵ with 0 ≤ϵ ≤−x(t) and write x′ := x + ϵ(χt −χs)
as an explicit convex combination of at most n vectors, chosen among
b≺1, . . . , b≺k and b≺s,u
i
for s ≺i u ⪯i t, with the additional property that b≺i
does not occur if x′(t) < 0.
6⃝
Set x := x′, rename the vectors in the convex combination of x as
b≺1, . . . , b≺k, and go to 2⃝.
Theorem 14.20.
(Schrijver [2000]) Schrijver’s Algorithm works correctly.
Proof:
The algorithm terminates if D contains no path from P to N and out-
puts the set X of vertices not reachable from P. Clearly N ⊆X ⊆U \ P, so

u∈X x(u) ≤
u∈W x(u) for each W ⊆U. Moreover, no edge enters X, so either
X = ∅or for each j ∈{1, . . . , k} there exists a v ∈X with X = {u ∈U : u ⪯j v}.
Hence, by deﬁnition, 
u∈X b≺j(u) = f (X) for all j ∈{1, . . . , k}. Moreover, by
Proposition 14.10, 
u∈W b≺j(u) ≤f (W) for all W ⊆U and j ∈{1, . . . , k}.
Therefore, for each W ⊆U,
f (W)
≥
k

j=1
λj

u∈W
b≺j(u) =

u∈W
k

j=1
λjb≺j(u) =

u∈W
x(u)
≥

u∈X
x(u) =

u∈X
k

j=1
λjb≺j(u) =
k

j=1
λj

u∈X
b≺j(u) = f (X),

14.4 Schrijver’s Algorithm
335
proving that X is an optimum solution.
2
Lemma 14.21.
(Schrijver [2000]) Each iteration can be performed in O(n3 +
γ n2) time, where γ is the time for an oracle call.
Proof:
It sufﬁces to show that 5⃝can be done in O(n3 + γ n2) time. Let x =
λ1b≺1 + · · · + λkb≺k and s ≺i t. We ﬁrst show:
Claim:
δ(χt −χs), for some δ ≥0, can be written as convex combination of
the vectors b≺s,v
i
−b≺i for s ≺i v ⪯i t in O(γ n2) time.
To prove this, we need some preliminaries. Let s ≺i v ⪯i t. By deﬁnition,
b≺s,v
i (u) = b≺i(u) for u ≺i s or u ≻i v As f is submodular, we have for s ⪯i
u ≺i v:
b≺s,v
i (u)
=
f ({w ∈U : w ⪯s,v
i
u}) −f ({w ∈U : w ≺s,v
i
u})
≤
f ({w ∈U : w ⪯i u}) −f ({w ∈U : w ≺i u}) = b≺i(u).
Moreover, for u = v we have:
b≺s,v
i (v)
=
f ({w ∈U : w ⪯s,v
i
v}) −f ({w ∈U : w ≺s,v
i
v})
=
f ({w ∈U : w ≺i s} ∪{v}) −f ({w ∈U : w ≺i s})
≥
f ({w ∈U : w ⪯i v}) −f ({w ∈U : w ≺i v})
=
b≺i(v).
Finally, observe that 
u∈U b≺s,v
i (u) = f (U) = 
u∈U b≺i(u).
As the claim is trivial if b≺s,v
i
= b≺i for some s ≺i v ⪯i t, we may assume
b≺s,v
i (v) > b≺i(v) for all s ≺i v ⪯i t. We recursively set
κv :=
χt
v −
v≺iw⪯it κw(b≺s,w
i (v) −b≺i(v))
b≺s,v
i (v) −b≺i(v)
≥0
for s ≺i v ⪯i t, and obtain 
s≺iv⪯it κv(b≺s,v
i
−b≺i) = χt −χs, because

s≺iv⪯it κv(b≺s,v
i (u) −b≺i(u)) = 
u⪯iv⪯it κv(b≺s,v
i (u) −b≺i(u)) = χt
u for all
s ≺i u ⪯i t, and the sum over all components is zero.
By letting δ :=
1

s≺i v⪯i t κv and multiplying each κu by δ, the claim follows.
Now consider ϵ := min{λiδ, −x(t)} and x′ := x + ϵ(χt −χs). If ϵ = λiδ ≤
−x(t), then we have x′ = k
j=1 λjb≺j + λi

s≺iv⪯it κv(b≺s,v
i
−b≺i), i.e. we have
written x′ as a convex combination of b≺j ( j ∈{1, . . . , k} \ {i}) and b≺s,v
i
(s ≺i
v ⪯i t). If ϵ = −x(t), we may additionally use b≺i in the convex combination.
We ﬁnally reduce this convex combination to at most n vectors in O(n3) time,
as shown in Exercise 5 of Chapter 4.
2
Lemma 14.22.
(Vygen [2003]) Schrijver’s Algorithm terminates after O(n5)
iterations.

336
14. Generalizations of Matroids
Proof:
If an edge (v, w) is introduced after a new vector b≺s,v
i
was added in 5⃝
of an iteration, then s ⪯i w ≺i v ⪯i t in this iteration. Thus d(w) ≤d(s) + 1 =
d(t) ≤d(v)+1 in this iteration, and the introduction of the new edge cannot make
the distance from P to any v ∈U smaller. As 5⃝makes sure that no element is
ever added to P, the distance d(v) never decreases for any v ∈U.
Call a block a sequence of iterations where the pair (t, s) remains constant.
Note that each block has O(n2) iterations, because (α, β) decreases lexicographi-
cally in each iteration within each block. It remains to prove that there are O(n3)
blocks.
A block can end only because of at least one of the following reasons (by the
choice of t and s, since an iteration with t = t∗does not add any edge whose head
is t∗, and since a vertex v can enter N only if v = s and hence d(v) < d(t)):
(a) the distance d(v) increases for some v ∈U.
(b) t is removed from N.
(c) (s, t) is removed from A.
We now count the number of blocks of these three types. Clearly there are
O(n2) blocks of type (a).
Now consider type (b). We claim that for each t∗∈U there are O(n2) iterations
with t = t∗and x′(t) = 0. This is easy to see: between every two such iterations,
d(v) must change for some v ∈U, and this can happen O(n2) times as d-values
can only increase. Thus there are O(n3) phases of type (b).
We ﬁnally show that there are O(n3) blocks of type (c). It sufﬁces to show
that d(t) will change before the next such block with the pair (s, t).
For s, t ∈U, we call s to be t-boring if (s, t) /∈A or d(t) ≤d(s). Let
s∗, t∗∈U, and consider the time period after a block with s = s∗and t = t∗
ending because (s∗, t∗) is removed from A, until the subsequent change of d(t∗).
We prove that each v ∈{s∗, . . . , n} is t∗-boring throughout this period. Applying
this for v = s∗concludes the proof.
At the beginning of the period, each v ∈{s∗+ 1, . . . , n} is t∗-boring due to
the choice of s = s∗in the iteration immediately preceding the period. s∗is also
t∗-boring as (s∗, t∗) is removed from A. As d(t∗) remains constant within the
considered time period and d(v) never decreases for any v, we only have to check
the introduction of new edges.
Suppose that, for some v ∈{s∗, . . . , n}, the edge (v, t∗) is added to A after
an iteration that chooses the pair (s, t). Then, by the initial remarks of this proof,
s ⪯i t∗≺i v ⪯i t in this iteration, and thus d(t∗) ≤d(s) + 1 = d(t) ≤d(v) + 1.
Now we distinguish two cases: If s > v, then we have d(t∗) ≤d(s): either because
t∗= s, or as s was t∗-boring and (s, t∗) ∈A. If s < v, then we have d(t) ≤d(v):
either because t = v, or by the choice of s and since (v, t) ∈A. In both cases we
conclude that d(t∗) ≤d(v), and v remains t∗-boring.
2
Theorem 14.20, Lemma 14.21 and Lemma 14.22 imply:
Theorem 14.23.
The Submodular Function Minimization Problem can be
solved in O(n8 + γ n7), where γ is the time for an oracle call.
2

14.5 Symmetric Submodular Functions
337
Iwata [2002] described a fully combinatorial algorithm (using only additions,
subtractions, comparisons and oracle calls, but no multiplication or division). He
also improved the running time (Iwata [2003]).
14.5 Symmetric Submodular Functions
A submodular function f : 2U →R is called symmetric if f (A) = f (U \ A)
for all A ⊆U. In this special case the Submodular Function Minimization
Problem is trivial, since 2 f (∅) = f (∅) + f (U) ≤f (A) + f (U \ A) = 2 f (A)
for all A ⊆U, implying that the empty set is optimal. Hence the problem is
interesting only if this trivial case is excluded: one looks for a nonempty proper
subset A of U such that f (A) is minimum.
Generalizing the algorithm of Section 8.7, Queyranne [1998] has found a rel-
atively simple combinatorial algorithm for this problem using only O(n3) oracle
calls. The following lemma is a generalization of Lemma 8.38 (Exercise 14):
Lemma 14.24.
Given a symmetric submodular function f : 2U →R with n :=
|U| ≥2, we can ﬁnd two elements x, y ∈U with x ̸= y and f ({x}) = min{ f (X) :
x ∈X ⊆U \ {y}} in O(n2θ) time, where θ is the time bound of the oracle for f .
Proof:
We construct an order U = {u1, . . . , un} by doing the following for
k = 1, . . . , n −1. Suppose that u1, . . . , uk−1 are already constructed; let Uk−1 :=
{u1, . . . , uk−1}. For C ⊆U we deﬁne
wk(C) :=
f (C) −1
2( f (C \ Uk−1) + f (C ∪Uk−1) −f (Uk−1)).
Note that wk is also symmetric. Let uk be an element of U \ Uk−1 that maximizes
wk({uk}).
Finally, let un be the only element in U \ {u1, . . . , un−1}. Obviously the con-
struction of the order u1, . . . , un can be done in O(n2θ) time.
Claim:
For all k = 1, . . . , n −1 and all x, y ∈U \ Uk−1 with x ̸= y and
wk({x}) ≤wk({y}) we have
wk({x}) = min{wk(C) : x ∈C ⊆U \ {y}}.
We prove the claim by induction on k. For k = 1 the assertion is trivial since
w1(C) = 1
2 f (∅) for all C ⊆U.
Let now k > 1 and x, y ∈U \ Uk−1 with x ̸= y and wk({x}) ≤wk({y}).
Moreover, let Z ⊆U with uk−1 /∈Z, and let z ∈Z \ Uk−1. By the choice of
uk−1 we have wk−1({z}) ≤wk−1({uk−1}); thus by the induction hypothesis we get
wk−1({z}) ≤wk−1(Z). Furthermore, the submodularity of f implies

338
14. Generalizations of Matroids
(wk(Z) −wk−1(Z)) −(wk({z}) −wk−1({z}))
=
1
2 ( f (Z ∪Uk−2) −f (Z ∪Uk−1) −f (Uk−2) + f (Uk−1))
−1
2 ( f ({z} ∪Uk−2) −f ({z} ∪Uk−1) −f (Uk−2) + f (Uk−1))
=
1
2( f (Z ∪Uk−2) + f ({z} ∪Uk−1) −f (Z ∪Uk−1) −f ({z} ∪Uk−2))
≥
0.
Hence wk(Z) −wk({z}) ≥wk−1(Z) −wk−1({z}) ≥0.
To conclude the proof of the claim, let C ⊆U with x ∈C and y /∈C. There
are two cases:
Case 1:
uk−1 /∈C. Then the above result for Z = C and z = x yields wk(C) ≥
wk({x}) as required.
Case 2:
uk−1 ∈C. Then we apply the above to Z = U \ C and z = y and get
wk(C) = wk(U \ C) ≥wk({y}) ≥wk({x}).
This completes the proof of the claim. Applying it to k = n −1, x = un and
y = un−1 we get
wn−1({un}) = min{wn−1(C) : un ∈C ⊆U \ {un−1}}.
Since wn−1(C) = f (C) −1
2( f ({un}) + f (U \ {un−1}) −f (Un−2)) for all C ⊆U
with un ∈C and un−1 /∈C, the lemma follows (set x := un and y := un−1).
2
The above proof is due to Fujishige [1998]. Now we can proceed analogously
to the proof of Theorem 8.39:
Theorem 14.25.
(Queyranne [1998])
Given a symmetric submodular function
f : 2U →R, a nonempty proper subset A of U such that f (A) is minimum can be
found in O(n3θ) time where θ is the time bound of the oracle for f .
Proof:
If |U| = 1, the problem is trivial. Otherwise we apply Lemma 14.24
and ﬁnd two elements x, y ∈U with f ({x}) = min{ f (X) : x ∈X ⊆U \ {y}}
in O(n2θ) time. Next we recursively ﬁnd a nonempty proper subset of U \ {x}
minimizing the function f ′ : 2U\{x} →R, deﬁned by f ′(X) := f (X) if y /∈X
and f ′(X) := f (X ∪{x}) if y ∈X. One readily observes that f ′ is symmetric
and submodular.
Let ∅̸= Y ⊂U \ {x} be a set minimizing f ′; w.l.o.g. y ∈Y (as f ′ is
symmetric). We claim that either {x} or Y ∪{x} minimizes f (over all nonempty
proper subsets of U). To see this, consider any C ⊂U with x ∈C. If y /∈C,
then we have f ({x}) ≤f (C) by the choice of x and y. If y ∈C, then f (C) =
f ′(C \ {x}) ≥f ′(Y) = f (Y ∪{x}). Hence f (C) ≥min{ f ({x}), f (Y ∪{x})} for
all nonempty proper subsets C of U.
To achieve the asserted running time we of course cannot compute f ′ explicitly.
Rather we store a partition of U, initially consisting of the singletons. At each step
of the recursion we build the union of those two sets of the partition that contain
x and y. In this way f ′ can be computed efﬁciently (using the oracle for f ).
2

Exercises
339
This result has been further generalized by Nagamochi and Ibaraki [1998] and
by Rizzi [2000].
Exercises
1. Let G be an undirected graph and M a maximum matching in G. Let F be the
family of those subsets X ⊆E(G) for which there exists a special blossom
forest F with respect to M with E(F) \ M = X. Prove that (E(G) \ M, F)
is a greedoid.
Hint: Use Exercise 23 of Chapter 10.
2. Let (E, F) be a greedoid and c′ : E →R+. We consider the bottleneck
function c(F) := min{c′(e) : e ∈F} for F ⊆E. Show that the Greedy
Algorithm For Greedoids, when applied to (E, F) and c, ﬁnds an F ∈F
with c(F) maximum.
3. This exercise shows that greedoids can also be deﬁned as languages (cf. Deﬁ-
nition 15.1). Let E be a ﬁnite set. A language L over the alphabet E is called
a greedoid language if
(a) L contains the empty string;
(b) xi ̸= xj for all (x1, . . . , xn) ∈L and 1 ≤i < j ≤n;
(c) (x1, . . . , xn−1) ∈L for all (x1, . . . , xn) ∈L;
(d) If (x1, . . . , xn), (y1, . . . , ym) ∈L with m < n, then there exists an i ∈
{1, . . . , n} such that (y1, . . . , ym, xi) ∈L.
L is called an antimatroid language if it satisﬁes (a), (b), (c) and
(d′) If (x1, . . . , xn), (y1, . . . , ym) ∈L with {x1, . . . , xn} ̸⊆{y1, . . . , ym}, then
there exists an i ∈{1, . . . , n} such that (y1, . . . , ym, xi) ∈L.
Prove: A language L over the alphabet E is a greedoid language (an antima-
troid language) if and only if the set system (E, F) is a greedoid (antimatroid),
where F := {{x1, . . . , xn} : (x1, . . . , xn) ∈L}.
4. Let U be a ﬁnite set and f : 2U →R. Prove that f is submodular if and
only if f (X ∪{y, z}) −f (X ∪{y}) ≤f (X ∪{z} −f (X) for all X ⊆U and
y, z ∈U.
5. Let P be a nonempty polymatroid. Show that then there is a submodular and
monotone function f with f (∅) = 0 and P = P( f ). ( f : 2E →R is called
monotone if f (A) ≤f (B) for all A ⊆B ⊆E).
6.
∗
Prove that a nonempty compact set P ⊆Rn
+ is a polymatroid if and only if
(a) For all 0 ≤x ≤y ∈P we have x ∈P.
(b) For all x ∈Rn
+ and all y, z ≤x with y, z ∈P that are maximal with this
property (i.e. y ≤w ≤x and w ∈P implies w = y, and z ≤w ≤x and
w ∈P implies w = z) we have 1ly = 1lz.
Note: This is the original deﬁnition of Edmonds [1970].
7. Prove that the Polymatroid Greedy Algorithm, when applied to a vector
c ∈RE and a function f : 2E →R that is submodular but not necessarily
monotone, ﬁnds

340
14. Generalizations of Matroids
max{cx :

e∈A
xe ≤f (A) for all A ⊆E}.
8. Prove Theorem 14.12 for the special case that f and g are rank functions of
matroids by constructing an integral optimum dual solution from c1 and c2 as
generated by the Weighted Matroid Intersection Algorithm.
(Frank [1981])
9.
∗
Let S be a ﬁnite set and f : 2S →R. Deﬁne f ′ : RS
+ →R as follows. For
any x ∈RS
+ there are unique k ∈Z+, λ1, . . . , λk > 0 and ∅⊂T1 ⊂T2 ⊂
· · · ⊂Tk ⊆S such that x = k
i=1 λiχ Ti, where χ Ti is the incidence vector of
Ti. Then f ′(x) := k
i=1 λi f (Ti).
Prove that f is submodular if and only if f ′ is convex.
(Lov´asz [1983])
10. Let E be a ﬁnite set and f : 2E →R+ a submodular function with f ({e}) ≤2
for all e ∈E. (The pair (E, f ) is sometimes called a 2-polymatroid.) The
Polymatroid Matching Problem asks for a maximum cardinality set X ⊆E
with f (X) = 2|X|. ( f is of course given by an oracle.)
Let E1, . . . , Ek be pairwise disjoint unordered pairs and let (E, F) be a ma-
troid (given by an independence oracle), where E = E1 ∪· · · ∪Ek. The
Matroid Parity Problem asks for a maximum cardinality set I ⊆{1, . . . , k}
with 
i∈I Ei ∈F.
(a) Show that the Matroid Parity Problem polynomially reduces to the
Polymatroid Matching Problem.
(b)
∗
Show that the Polymatroid Matching Problem polynomially reduces
to the Matroid Parity Problem.
Hint: Use an algorithm for the Submodular Function Minimization
Problem.
(c)
∗
Show that there is no algorithm for the Polymatroid Matching Problem
whose running time is polynomial in |E|.
(Jensen and Korte [1982], Lov´asz [1981])
(A problem polynomially reduces to another one if the former can be solved
with a polynomial-time oracle algorithm using an oracle for the latter; see
Chapter 15.)
Note: A polynomial-time algorithm for an important special case was given
by Lov´asz [1980,1981].
11. A function f : 2S →R∪{∞} is called crossing submodular if f (X)+ f (Y) ≥
f (X∪Y)+ f (X∩Y) for any two sets X, Y ⊆S with X∩Y ̸= ∅and X∪Y ̸= S.
The Submodular Flow Problem is as follows: Given a digraph G, functions
l : E(G) →R∪{−∞}, u : E(G) →R∪{∞}, c : E(G) →R, and a crossing
submodular function b : 2V (G) →R ∪{∞}. Then a feasible submodular ﬂow
is a function f : E(G) →R with l(e) ≤f (e) ≤u(e) for all e ∈E(G) and

e∈δ−(X)
f (e) −

e∈δ+(X)
f (e) ≤b(X)

References
341
for all X ⊆V (G). The task is to decide whether a feasible ﬂow exists and, if
yes, to ﬁnd one whose cost 
e∈E(G) c(e) f (e) is minimum possible.
Show that this problem generalizes the Minimum Cost Flow Problem and
the problem of optimizing a linear function over the intersection of two poly-
matroids.
Note: The Submodular Flow Problem, introduced by Edmonds and Giles
[1977], can be solved in strongly polynomial time; see Fujishige, R¨ock and
Zimmermann [1989]. See also Fleischer and Iwata [2000].
12.
∗
Show that the inequality system describing a feasible submodular ﬂow (Exer-
cise 11) is TDI. Show that this implies Theorems 14.12 and 19.10.
(Edmonds and Giles [1977])
13. Prove that the set of vertices of the base polyhedron is precisely the set of
vectors b≺for all total orders ≺of U, where
b≺(u) := f ({v ∈U : v ⪯u}) −f ({v ∈U : v ≺u})
(u ∈U).
Hint: See the proof of Theorem 14.11.
14. Show that Lemma 8.38 is a special case of Lemma 14.24.
References
General Literature:
Bixby, R.E., and Cunningham, W.H. [1995]: Matroid optimization and algorithms. In:
Handbook of Combinatorics; Vol. 1 (R.L. Graham, M. Gr¨otschel, L. Lov´asz, eds.), El-
sevier, Amsterdam, 1995
Bj¨orner, A., and Ziegler, G.M. [1992]: Introduction to greedoids. In: Matroid Applications
(N. White, ed.), Cambridge University Press, Cambridge 1992
Fujishige, S. [1991]: Submodular Functions and Optimization. North-Holland, Amsterdam
1991
Korte, B., Lov´asz, L., and Schrader, R. [1991]: Greedoids. Springer, Berlin 1991
McCormick, S.T. [2004]: Submodular function minimization. In: Handbook on Discrete
Optimization (K. Aardal, G. Nemhauser, R. Weismantel, eds.), Elsevier, Berlin (forth-
coming)
Schrijver, A. [2003]: Combinatorial Optimization: Polyhedra and Efﬁciency. Springer,
Berlin 2003, Chapters 44–49
Cited References:
Edmonds, J. [1970]: Submodular functions, matroids and certain polyhedra. In: Combina-
torial Structures and Their Applications; Proceedings of the Calgary International Con-
ference on Combinatorial Structures and Their Applications 1969 (R. Guy, H. Hanani,
N. Sauer, J. Schonheim, eds.), Gordon and Breach, New York 1970, pp. 69–87
Edmonds, J. [1979]: Matroid intersection. In: Discrete Optimization I; Annals of Discrete
Mathematics 4 (P.L. Hammer, E.L. Johnson, B.H. Korte, eds.), North-Holland, Amster-
dam 1979, pp. 39–49
Edmonds, J., and Giles, R. [1977]: A min-max relation for submodular functions on graphs.
In: Studies in Integer Programming; Annals of Discrete Mathematics 1 (P.L. Hammer,

342
14. Generalizations of Matroids
E.L. Johnson, B.H. Korte, G.L. Nemhauser, eds.), North-Holland, Amsterdam 1977, pp.
185–204
Fleischer, L., and Iwata, S. [2000]: Improved algorithms for submodular function mini-
mization and submodular ﬂow. Proceedings of the 32nd Annual ACM Symposium on
the Theory of Computing (2000), 107–116
Frank, A. [1981]: A weighted matroid intersection algorithm. Journal of Algorithms 2
(1981), 328–336
Frank, A. [1982]: An algorithm for submodular functions on graphs. In: Bonn Workshop
on Combinatorial Optimization; Annals of Discrete Mathematics 16 (A. Bachem, M.
Gr¨otschel, B. Korte, eds.), North-Holland, Amsterdam 1982, pp. 97–120
Fujishige, S. [1998]: Another simple proof of the validity of Nagamochi and Ibaraki’s
min-cut algorithm and Queyranne’s extension to symmetric submodular function mini-
mization. Journal of the Operations Research Society of Japan 41 (1998), 626–628
Fujishige, S., R¨ock, H., and Zimmermann, U. [1989]: A strongly polynomial algorithm
for minimum cost submodular ﬂow problems. Mathematics of Operations Research 14
(1989), 60–69
Gr¨otschel, M., Lov´asz, L., and Schrijver, A. [1981]: The ellipsoid method and its conse-
quences in combinatorial optimization. Combinatorica 1 (1981), 169–197
Gr¨otschel, M., Lov´asz, L., and Schrijver, A. [1988]: Geometric Algorithms and Combina-
torial Optimization. Springer, Berlin 1988
Iwata, S. [2002]: A fully combinatorial algorithm for submodular function minimization.
Journal of Combinatorial Theory B 84 (2002), 203–212
Iwata, S. [2003]: A faster scaling algorithm for minimizing submodular functions. SIAM
Journal on Computing 32 (2003), 833–840
Iwata, S., Fleischer, L., L., and Fujishige, S. [2001]: A combinatorial, strongly polynomial-
time algorithm for minimizing submodular functions. Journal of the ACM 48 (2001),
761–777
Jensen, P.M., and Korte, B. [1982]: Complexity of matroid property algorithms. SIAM
Journal on Computing 11 (1982), 184–190
Lov´asz, L. [1980]: Matroid matching and some applications. Journal of Combinatorial
Theory B 28 (1980), 208–236
Lov´asz, L. [1981]: The matroid matching problem. In: Algebraic Methods in Graph Theory;
Vol. II (L. Lov´asz, V.T. S´os, eds.), North-Holland, Amsterdam 1981, 495–517
Lov´asz, L. [1983]: Submodular functions and convexity. In: Mathematical Programming:
The State of the Art – Bonn 1982 (A. Bachem, M. Gr¨otschel, B. Korte, eds.), Springer,
Berlin 1983
Nagamochi, H., and Ibaraki, T. [1998]: A note on minimizing submodular functions. In-
formation Processing Letters 67 (1998), 239–244
Queyranne, M. [1998]: Minimizing symmetric submodular functions. Mathematical Pro-
gramming B 82 (1998), 3–12
Rizzi, R. [2000]: On minimizing symmetric set functions. Combinatorica 20 (2000), 445–
450
Schrijver, A. [2000]: A combinatorial algorithm minimizing submodular functions in
strongly polynomial time. Journal of Combinatorial Theory B 80 (2000), 346–355
Vygen, J. [2003]: A note on Schrijver’s submodular function minimization algorithm. Jour-
nal of Combinatorial Theory B 88 (2003), 399–402

15. NP-Completeness
For many combinatorial optimization problems a polynomial-time algorithm is
known; the most important ones are presented in this book. However, there are
also many important problems for which no polynomial-time algorithm is known.
Although we cannot prove that none exists we can show that a polynomial-
time algorithm for one “hard” (more precisely: NP-hard) problem would imply
a polynomial-time algorithm for almost all problems discussed in this book (more
precisely: all NP-easy problems).
To formalize this concept and prove the above statement we need a machine
model, i.e. a precise deﬁnition of a polynomial-time algorithm. Therefore we dis-
cuss Turing machines in Section 15.1. This theoretical model is not suitable to
describe more complicated algorithms. However we shall argue that it is equivalent
to our informal notion of algorithms: every algorithm in this book can, theoreti-
cally, be written as a Turing machine, with a loss in efﬁciency that is polynomially
bounded. We indicate this in Section 15.2.
In Section 15.3 we introduce decision problems, and in particular the classes
P and NP. While NP contains most decision problems appearing in this book, P
contains only those for which there are polynomial-time algorithms. It is an open
question whether P = NP. Although we shall discuss many problems in NP for
which no polynomial-time algorithm is known, nobody can (so far) prove that
none exists. We specify what it means that one problem reduces to another, or
that one problem is at least as hard as another one. In this notion, the hardest
problems in NP are the NP-complete problems; they can be solved in polynomial
time if and only if P = NP.
In Section 15.4 we exhibit the ﬁrst NP-complete problem, Satisfiability. In
Section 15.5 some more decision problems, more closely related to combinatorial
optimization, are proved to be NP-complete. In Sections 15.6 and 15.7 we shall
discuss related concepts, also extending to optimization problems.
15.1 Turing Machines
In this section we present a very simple model for computation: the Turing ma-
chine. It can be regarded as a sequence of simple instructions working on a string.
The input and the output will be a binary string:

344
15. NP-Completeness
Deﬁnition 15.1.
An alphabet is a ﬁnite set with at least two elements, not con-
taining the special symbol ⊔(which we shall use for blanks). For an alphabet A
we denote by A∗:= 
n∈Z+ An the set of all (ﬁnite) strings whose symbols are ele-
ments of A. We use the convention that A0 contains exactly one element, the empty
string. A language over A is a subset of A∗. The elements of a language are often
called words. If x ∈An we write size(x) := n for the length of the string.
We shall often work with the alphabet A = {0, 1} and the set {0, 1}∗of all
0-1-strings (or binary strings). The components of a 0-1-string are sometimes
called its bits. So there is exactly one 0-1-string of zero length, the empty string.
A language over {0, 1} is a subset of {0, 1}∗.
A Turing machine gets as input a string x ∈A∗for some ﬁxed alphabet A. The
input is completed by blank symbols (denoted by ⊔) to a two-way inﬁnite string
s ∈(A ∪{⊔})Z. This string s can be regarded as a tape with a read-write head;
only a single position can be read and modiﬁed at each step, and the read-write
head can be moved by one position in each step.
A Turing machine consists of a set of N + 1 statements numbered 0, . . . , N.
In the beginning statement 0 is executed and the current position of the string is
position 1. Now each statement is of the following type: Read the bit at the current
position, and depending on its value do the following: Overwrite the current bit
by some element of A ∪{⊔}, possibly move the current position by one to the left
or to the right, and go to a statement which will be executed next.
There is a special statement denoted by −1 which marks the end of the compu-
tation. The components of our inﬁnite string s indexed by 1, 2, 3, . . . up to the ﬁrst
⊔then yield the output string. Formally we deﬁne a Turing machine as follows:
Deﬁnition 15.2.
(Turing [1936])
Let A be an alphabet and ¯A := A ∪{⊔}. A
Turing machine (with alphabet A) is deﬁned by a function
	 : {0, . . . , N} × ¯A →{−1, . . . , N} × ¯A × {−1, 0, 1}
for some N ∈Z+. The computation of 	 on input x, where x ∈A∗, is the ﬁnite or
inﬁnite sequence of triples (n(i), s(i), π(i)) with n(i) ∈{−1, . . . , N}, s(i) ∈¯AZ and
π(i) ∈Z (i = 0, 1, 2, . . .) deﬁned recursively as follows (n(i) denotes the current
statement, s(i) represents the string, and π(i) is the current position):
n(0) := 0. s(0)
j
:= xj for 1 ≤j ≤size(x), and s(0)
j
:= ⊔for all j ≤0 and
j > size(x). π(0) := 1.
If (n(i), s(i), π(i)) is already deﬁned, we distinguish two cases. If n(i) ̸= −1,
then let (m, σ, δ) := 	

n(i), s(i)
π(i)

and set n(i+1) := m, s(i+1)
π(i)
:= σ, s(i+1)
j
:= s(i)
j
for j ∈Z \ {π(i)}, and π(i+1) := π(i) + δ.
If n(i) = −1, then this is the end of the sequence. We then deﬁne time(	, x) := i
and output(	, x) ∈Ak, where k := min
5
j ∈N : s(i)
j
= ⊔
6
−1, by output(	, x)j
:= s(i)
j
for j = 1, . . . , k.
If this sequence is inﬁnite (i.e. n(i) ̸= −1 for all i), then we set time(	, x) := ∞.
In this case output(	, x) is undeﬁned.

15.2 Church’s Thesis
345
Of course we are interested mostly in Turing machines whose computation is
ﬁnite or even polynomially bounded:
Deﬁnition 15.3.
Let A be an alphabet, S, T ⊆A∗two languages, and f : S →T
a function. Let 	 be a Turing machine with alphabet A such that time(	, s) < ∞
and output(	, s) = f (s) for each s ∈S. Then we say that 	 computes f . If there
exists a polynomial p such that for all s ∈S we have time(	, s) ≤p(size(s)),
then 	 is a polynomial-time Turing machine.
In the case S = A∗and T = {0, 1} we say that 	 decides the language L :=
{s ∈S : f (s) = 1}. If there exists some polynomial-time Turing machine computing
a function f (or deciding a language L), then we say that f is computable in
polynomial time (or L is decidable in polynomial time, respectively).
To make these deﬁnitions clear we give an example. The following Turing
machine 	 : {0, . . . , 4}×{0, 1, ⊔} →{−1, . . . , 4}×{0, 1, ⊔}×{−1, 0, 1} computes
the successor function f (n) = n + 1 (n ∈N), where the numbers are coded by
their usual binary representation.
	(0, 0)
=
(0, 0, 1)
0⃝
While sπ ̸= ⊔do π := π + 1.
	(0, 1)
=
(0, 1, 1)
	(0, ⊔)
=
(1, ⊔, −1)
Set π := π −1.
	(1, 1)
=
(1, 0, −1)
1⃝
While sπ = 1 do sπ := 0 and π := π −1.
	(1, 0)
=
(−1, 1, 0)
If sπ = 0 then sπ := 1 and stop.
	(1, ⊔)
=
(2, ⊔, 1)
Set π := π + 1.
	(2, 0)
=
(2, 0, 1)
2⃝
While sπ = 0 do π := π + 1.
	(2, ⊔)
=
(3, 0, −1)
Set sπ := 0 and π := π −1.
	(3, 0)
=
(3, 0, −1)
3⃝
While sπ = 0 do π := π −1.
	(3, ⊔)
=
(4, ⊔, 1)
Set π := π + 1.
	(4, 0)
=
(−1, 1, 0)
4⃝
Set sπ := 1 and stop.
Note that several values of 	 are not speciﬁed as they are never used in
any computation. The comments on the right-hand side illustrate the computation.
Statements
2⃝,
3⃝and
4⃝are used only if the input consists of 1’s only, i.e.
n = 2k −1 for some k ∈Z+. We have time(	, s) ≤4 size(s) + 5 for all inputs s,
so 	 is a polynomial-time Turing machine.
In the next section we shall show that the above deﬁnition is consistent
with our informal deﬁnition of a polynomial-time algorithm in Section 1.2: each
polynomial-time algorithm in this book can be simulated by a polynomial-time
Turing machine.
15.2 Church’s Thesis
The Turing machine is the most customary theoretical model for algorithms. Al-
though it seems to be very restricted, it is as powerful as any other reasonable

346
15. NP-Completeness
model: the set of computable functions (sometimes also called recursive func-
tions) is always the same. This statement, known as Church’s thesis, is of course
too imprecise to be proved. However, there are strong results supporting this claim.
For example, each program in a common programming language like C can be
modelled by a Turing machine. In particular, all algorithms in this book can be
rewritten as Turing machines. This is usually very inconvenient (thus we shall
never do it), but theoretically it is possible. Moreover, any function computable
in polynomial time by a C program is also computable in polynomial time by a
Turing machine.
Since it is not a trivial task to implement more complicated programs on a
Turing machine we consider as an intermediate step a Turing machine with two
tapes and two independent read-write heads, one for each tape:
Deﬁnition 15.4.
Let A be an alphabet and ¯A := A ∪{⊔}. A two-tape Turing
machine is deﬁned by a function
	 : {0, . . . , N} × ¯A2 →{−1, . . . , N} × ¯A2 × {−1, 0, 1}2
for some N ∈Z+. The computation of 	 on input x, where x ∈A∗, is the ﬁnite
or inﬁnite sequence of 5-tuples (n(i), s(i), t(i), π(i), ρ(i)) with n(i) ∈{−1, . . . , N},
s(i), t(i) ∈¯AZ and π(i), ρ(i) ∈Z (i = 0, 1, 2, . . .) deﬁned recursively as follows:
n(0) := 0. s(0)
j
:= xj for 1 ≤j ≤size(x), and s(0)
j
:= ⊔for all j ≤0 and
j > size(x). t(0)
j
:= ⊔for all j ∈Z. π(0) := 1 and ρ(0) := 1.
If (n(i), s(i), t(i), π(i), ρ(i)) is already deﬁned, we distinguish two cases. If n(i) ̸=
−1, then let (m, σ, τ, δ, ϵ) := 	

n(i), s(i)
π(i), t(i)
ρ(i)

and set n(i+1) := m, s(i+1)
π(i)
:= σ,
s(i+1)
j
:= s(i)
j
for j ∈Z \ {π(i)}, t(i+1)
ρ(i)
:= τ, t(i+1)
j
:= t(i)
j
for j ∈Z \ {ρ(i)},
π(i+1) := π(i) + δ, and ρ(i+1) := ρ(i) + ϵ.
If n(i) = −1, then this is the end of the sequence. time(	, x) and output(	, x)
are deﬁned as with the one-tape Turing machine.
Turing machines with more than two tapes can be deﬁned analogously, but we
shall not need them. Before we show how to perform standard operations with
a two-tape Turing machine, let us note that a two-tape Turing machine can be
simulated by an ordinary (one-tape) Turing machine.
Theorem 15.5.
Let A be an alphabet, and let
	 : {0, . . . , N} × (A ∪{⊔})2 →{−1, . . . , N} × (A ∪{⊔})2 × {−1, 0, 1}2
be a two-tape Turing machine. Then there exists an alphabet B ⊇A and a (one-
tape) Turing machine
	′ : {0, . . . , N ′} × (B ∪{⊔}) →{−1, . . . , N ′} × (B ∪{⊔}) × {−1, 0, 1}
such that output(	′, x) = output(	, x) and time(	′, x) = O(time(	, x))2 for
x ∈A∗.

15.2 Church’s Thesis
347
Proof:
We use the letters s and t for the two strings of 	, and denote by π and
ρ the positions of the read-write heads, as in Deﬁnition 15.4. The string of 	′ will
be denoted by u and its read-write head position by ψ.
We have to encode both strings s, t and both read-write head positions π, ρ in
one string u. To make this possible each symbol uj of u is a 4-tuple (sj, pj, tj,rj),
where sj and tj are the corresponding symbols of s and t, and pj,rj ∈{0, 1}
indicate whether the read-write heads of the ﬁrst and second string currently scans
position j; i.e. we have pj = 1 iff π = j, and rj = 1 iff ρ = j.
So we deﬁne ¯B := ( ¯A × {0, 1} × ¯A × {0, 1}); then we identify a ∈¯A with
(a, 0, ⊔, 0) to allow inputs from A∗. The ﬁrst step of 	′ consists in initializing the
marks p1 and r1 to 1:
	′(0, (., 0, ., 0))
=
(1, (., 1, ., 1)), 0)
0⃝
Set π := ψ and ρ := ψ.
Here a dot stands for an arbitrary value (which is not modiﬁed).
Now we show how to implement a general statement 	(m, σ, τ) = (m′, σ ′, τ ′,
δ, ϵ). We ﬁrst have to ﬁnd the positions π and ρ. It is convenient to assume that
our single read-write head ψ is already at the leftmost of the two positions π and
ρ; i.e. ψ = min{π, ρ}. We have to ﬁnd the other position by scanning the string
u to the right, we have to check whether sπ = σ and tρ = τ and, if so, perform
the operation required (write new symbols to s and t, move π and ρ, jump to the
next statement).
The following block implements one statement 	(m, σ, τ) = (m′, σ ′, τ ′, δ, ϵ)
for m = 0; for each m we have | ¯A|
2 such blocks, one for choice of σ and τ.
The second block for m = 0 starts with 13
⃝, the ﬁrst block for m′ with M⃝, where
M := 12| ¯A|
2m′ + 1. All in all we get N ′ = 12(N + 1)| ¯A|
2.
A dot again stands for an arbitrary value which is not modiﬁed. Similarly,
ζ and ξ stand for an arbitrary element of ¯A \ {σ} and ¯A \ {τ}, respectively. We
assume that ψ = min{π, ρ} initially; note that 10
⃝, 11
⃝and 12
⃝guarantee that this
property also holds at the end.
	′(1, (ζ, 1, ., .))
= (13, (ζ, 1, ., .), 0)
1⃝If ψ = π and sψ ̸= σ then go to 13
⃝.
	′(1, (., ., ξ, 1))
= (13, (., ., ξ, 1), 0)
If ψ = ρ and tψ ̸= τ then go to 13
⃝.
	′(1, (σ, 1, τ, 1)) = (2, (σ, 1, τ, 1), 0)
If ψ = π then go to 2⃝.
	′(1, (σ, 1, ., 0)) = (2, (σ, 1, ., 0), 0)
	′(1, (., 0, τ, 1)) = (6, (., 0, τ, 1), 0)
If ψ = ρ then go to 6⃝.
	′(2, (., ., ., 0))
= (2, (., ., ., 0), 1)
2⃝While ψ ̸= ρ do ψ := ψ + 1.
	′(2, (., ., ξ, 1))
= (12, (., ., ξ, 1), −1)
If tψ ̸= τ then set ψ := ψ −1
and go to 12
⃝.
	′(2, (., ., τ, 1))
= (3, (., ., τ ′, 0), ϵ)
Set tψ := τ ′ and ψ := ψ + ϵ.
	′(3, (., ., ., 0))
= (4, (., ., ., 1), 1)
3⃝Set ρ := ψ and ψ := ψ + 1.
	′(4, (., 0, ., .))
= (4, (., 0, ., .), −1)
4⃝While ψ ̸= π do ψ := ψ −1.
	′(4, (σ, 1, ., .))
= (5, (σ ′, 0, ., .), δ)
Set sψ := σ ′ and ψ := ψ + δ.

348
15. NP-Completeness
	′(5, (., 0, ., .))
= (10, (., 1, ., .), −1)
5⃝Set π := ψ and ψ := ψ −1.
Go to 10
⃝.
	′(6, (., 0, ., .))
= (6, (., 0, ., .), 1)
6⃝While ψ ̸= π do ψ := ψ + 1.
	′(6, (ζ, 1, ., .))
= (12, (ζ, 1, ., .), −1)
If sψ ̸= σ then set ψ := ψ −1
and go to 12
⃝.
	′(6, (σ, 1, ., ., )) = (7, (σ ′, 0, ., .), δ)
Set sψ := σ ′ and ψ := ψ + δ.
	′(7, (., 0, ., .))
= (8, (., 1, ., .), 1)
7⃝Set π := ψ and ψ := ψ + 1.
	′(8, (., ., ., 0))
= (8, (., ., ., 0), −1)
8⃝While ψ ̸= ρ do ψ := ψ −1.
	′(8, (., ., τ, 1))
= (9, (., ., τ ′, 0), ϵ)
Set tψ := τ ′ and ψ := ψ + ϵ.
	′(9, (., ., ., 0))
= (10, (., ., ., 1), −1)
9⃝Set ρ := ψ and ψ := ψ −1.
	′(10, (., ., ., .))
= (11, (., ., ., .), −1)
10
⃝Set ψ := ψ −1.
	′(11, (., 0, ., 0)) = (11, (., 0, ., 0), 1)
11
⃝While ψ ̸∈{π, ρ} do ψ := ψ + 1.
	′(11, (., 1, ., .)) = (M, (., 1, ., .), 0)
Go to M⃝.
	′(11, (., 0, ., 1)) = (M, (., 0, ., 1), 0)
	′(12, (., 0, ., 0)) = (12, (., 0, ., 0), −1)
12
⃝While ψ ̸∈{π, ρ} do ψ := ψ −1.
	′(12, (., 1, ., .)) = (13, (., 1, ., .), 0)
	′(12, (., ., ., 1)) = (13, (., ., ., 1), 0)
Any computation of 	′ passes through at most | ¯A|
2 blocks like the above for
each computation step of 	. The number of computation steps within each block
is at most 2|π −ρ| + 10. Since | ¯A| is a constant and |π −ρ| is bounded by
time(	, x) we conclude that the whole computation of 	 is simulated by 	′ with
O

(time(	, x))2
steps.
Finally we have to clean up the output: replace each symbol (σ, ., ., .) by
(σ, 0, ⊔, 0). Obviously this at most doubles the total number of steps.
2
With a two-tape Turing machine it is not too difﬁcult to implement more
complicated statements, and thus arbitrary algorithms:
We use the alphabet A = {0, 1, #} and model an arbitrary number of variables
by the string
x0##1#x1##10#x2##11#x3##100#x4##101#x5## . . .
(15.1)
which we store on the ﬁrst tape. Each group contains a binary representation of
the index i followed by the value of xi, which we assume to be a binary string.
The ﬁrst variable x0 and the second tape are used only as registers for intermediate
results of computation steps.
Random access to variables is not possible in constant time with a Turing
machine, no matter how many tapes we have. If we simulate an arbitrary algorithm
by a two-tape Turing machine, we will have to scan the ﬁrst tape quite often.
Moreover, if the length of the string in one variable changes, the substring to the
right has to be shifted. Nevertheless each standard operation (i.e. each elementary

15.2 Church’s Thesis
349
step of an algorithm) can be simulated with O(l2) computation steps of a two-tape
Turing machine, where l is the current length of the string (15.1).
We try to make this clearer with a concrete example. Consider the following
statement: Add to x5 the value of the variable whose index is given by x2.
To get the value of x5 we scan the ﬁrst tape for the substring ##101#. We copy
the substring following this up to #, exclusively, to the second tape. This is easy
since we have two separate read-write heads. Then we copy the string from the
second tape to x0. If the new value of x0 is shorter or longer than the old one, we
have to shift the rest of the string (15.1) to the left or to the right appropriately.
Next we have to search for the variable index that is given by x2. To do this,
we ﬁrst copy x2 to the second tape. Then we scan the ﬁrst tape, checking each
variable index (comparing it with the string on the second tape bitwise). When we
have found the correct variable index, we copy the value of this variable to the
second tape.
Now we add the number stored in x0 to that on the second tape. A Turing
machine for this task, using the standard method, is not hard to design. We can
overwrite the number on the second tape by the result while computing it. Finally
we have the result on the second string and copy it back to x5. If necessary we
shift the substring to the right of x5 appropriately.
All the above can be done by a two-tape Turing machine in O(l2) computation
steps (in fact all but shifting the string (15.1) can be done in O(l) steps). It
should be clear that the same holds for all other standard operations, including
multiplication and division.
By Deﬁnition 1.4 an algorithm is said to run in polynomial time if there is a
k ∈N such that the number of elementary steps is bounded by O(nk) and any
number in intermediate computation can be stored with O(nk) bits, where n is
the input size. Moreover, we store at most O(nk) numbers at any time. Hence
we can bound the length of each of the two strings in a two-tape Turing machine
simulating such an algorithm by l = O(nk · nk) = O(n2k), and hence its running
time by O(nk(n2k)2) = O(n5k). This is still polynomial in the input size.
Recalling Theorem 15.5 we may conclude that for any string function f there
is a polynomial-time algorithm computing f if and only if there is a polynomial-
time Turing machine computing f .
Hopcroft and Ullman [1979], Lewis and Papadimitriou [1981], and van Emde
Boas [1990] provide more details about the equivalence of different machine mod-
els. Another common model (which is close to our informal model of Section 1.2)
is the RAM machine (cf. Exercise 3) which allows arithmetic operations on inte-
gers in constant time. Other models allow only operations on bits (or integers of
ﬁxed length) which is more realistic when dealing with large numbers. Obviously,
addition and comparison of natural numbers with n bits can be done with O(n)
bit operations. For multiplication (and division) the obvious method takes O(n2),
but the fastest known algorithm for multiplying two n-bit integers needs only
O(n log n log log n) bit operations steps (Sch¨onhage and Strassen [1971]). This of
course implies algorithms for the addition and comparison of rational numbers

350
15. NP-Completeness
within the same time complexity. As far as polynomial-time computability is con-
cerned all models are equivalent, but of course the running time measures are
quite different.
The model of encoding all the input by 0-1-strings (or strings over any ﬁxed
alphabet) does not in principle exclude certain types of real numbers, e.g. algebraic
numbers (if x ∈R is the k-th smallest root of a polynomial p, then x can be
coded by listing k and the degree and the coefﬁcients of p). However, there is no
way of representing arbitrary real numbers in a digital computer since there are
uncountably many real numbers but only countably many 0-1-strings. We take the
classical approach and restrict ourselves to rational input in this chapter.
We close this section by giving a formal deﬁnition of oracle algorithms, based
on two-tape Turing machines. We may call an oracle at any stage of the computa-
tion; we use the second tape for writing the oracle’s input and reading its output.
We introduce a special statement −2 for oracle calls:
Deﬁnition 15.6.
Let A be an alphabet and ¯A := A ∪{⊔}. Let X ⊆A∗, and let
f (x) ⊆A∗be a nonempty language for each x ∈X. An oracle Turing machine
using f is a mapping
	 : {0, . . . , N} × ¯A2 →{−2, . . . , N} × ¯A2 × {−1, 0, 1}2
for some N ∈Z+; its computation is deﬁned as for a two-tape Turing machine,
but with one difference: If, for some computation step i, 	

n(i), s(i)
π(i), t(i)
ρ(i)

=
(−2, σ, τ, δ, ϵ) for some σ, τ, δ, ϵ, then consider the string on the second tape
x ∈Ak, k := min

j ∈N : t(i)
j
= ⊔

−1, given by xj := t(i)
j
for j = 1, . . . , k. If
x ∈X, then the second tape is overwritten by t(i+1)
j
= yj for j = 1, . . . , size(y) and
t(i+1)
size(y)+1 = ⊔for some y ∈f (x). The rest remains unchanged, and the computation
continues with n(i+1) := n(i) + 1 (and stops if n(i) = −1).
All deﬁnitions with respect to Turing machines can be extended to oracle
Turing machines. The output of an oracle is not necessarily unique; hence there can
be several possible computations for the same input. When proving the correctness
or estimating the running time of an oracle algorithm we have to consider all
possible computations, i.e. all choices of the oracle.
By the results of this section the existence of a polynomial-time (oracle) algo-
rithm is equivalent to the existence of a polynomial-time (oracle) Turing machine.
15.3 P and NP
Most of complexity theory is based on decision problems. Any language L ⊆
{0, 1}∗can be interpreted as decision problem: given a 0-1-string, decide whether
it belongs to L. However, we are more interested in problems like the following:

15.3 P and NP
351
Hamiltonian Circuit
Instance:
An undirected graph G.
Question:
Has G a Hamiltonian circuit?
We will always assume a ﬁxed efﬁcient encoding of the input as a binary
string; occasionally we extend our alphabet by other symbols. For example we
assume that a graph is given by an adjacency list, and such a list can easily be
coded as a binary string of length O(n + m log n), where n and m denote the
number of vertices and edges. We always assume an efﬁcient encoding, i.e. one
whose length is polynomially bounded by the minimum possible encoding length.
Not all binary strings are instances of Hamiltonian Circuit but only those
representing an undirected graph. For most interesting decision problems the in-
stances are a proper subset of the 0-1-strings. We require that we can decide in
polynomial time whether an arbitrary string is an instance or not:
Deﬁnition 15.7.
A decision problem is a pair P = (X, Y), where X is a language
decidable in polynomial time and Y ⊆X. The elements of X are called instances
of P; the elements of Y are yes-instances, those of X \ Y are no-instances.
An algorithm for a decision problem (X, Y) is an algorithm computing the
function f : X →{0, 1}, deﬁned by f (x) = 1 for x ∈Y and f (x) = 0 for
x ∈X \ Y.
We give two more examples, the decision problems corresponding to Linear
Programming and Integer Programming:
Linear Inequalities
Instance:
A matrix A ∈Zm×n and a vector b ∈Zm.
Question:
Is there a vector x ∈Qn such that Ax ≤b?
Integer Linear Inequalities
Instance:
A matrix A ∈Zm×n and a vector b ∈Zm.
Question:
Is there a vector x ∈Zn such that Ax ≤b?
Deﬁnition 15.8.
The class of all decision problems for which there is a polynom-
ial-time algorithm is denoted by P.
In other words, a member of P is a pair (X, Y) with Y ⊆X ⊆{0, 1}∗
where both X and Y are languages decidable in polynomial time. To prove that a
problem is in P one usually describes a polynomial-time algorithm. By the results
of Section 15.2 there is a polynomial-time Turing machine for each problem in P.
By Khachiyan’s Theorem 4.18, LinearInequalities belongs to P. It is not known
whether Integer Linear Inequalities or Hamiltonian Circuit belong to P.
We shall now introduce another class called NP which contains these problems,
and in fact most decision problems discussed in this book.

352
15. NP-Completeness
We do not insist on a polynomial-time algorithm, but we require that for
each yes-instance there is a certiﬁcate which can be checked in polynomial time.
For example, for the Hamiltonian Circuit problem such a certiﬁcate is simply
a Hamiltonian circuit. It is easy to check whether a given string is the binary
encoding of a Hamiltonian circuit. Note that we do not require a certiﬁcate for
no-instances. Formally we deﬁne:
Deﬁnition 15.9.
A decision problem P = (X, Y) belongs to NP if there is a
polynomial p and a decision problem P′ = (X′, Y ′) in P, where
X′ :=
5
x#c : x ∈X, c ∈{0, 1}⌊p(size(x))⌋6
,
such that
Y =
5
y ∈X : There exists a string c ∈{0, 1}⌊p(size(y))⌋with y#c ∈Y ′6
.
Here x#c denotes the concatenation of the string x, the symbol # and the string c.
A string c with y#c ∈Y ′ is called a certiﬁcate for y (since c proves that y ∈Y).
An algorithm for P′ is called a certiﬁcate-checking algorithm.
Proposition 15.10.
P ⊆NP.
Proof:
One can choose p to be identically zero. An algorithm for P′ just deletes
the last symbol of the input “x#” and then applies an algorithm for P.
2
It is not known whether P = NP. In fact, this is the most important open
problem in complexity theory. As an example for problems in NP that are not
known to be in P we have:
Proposition 15.11.
Hamiltonian Circuit belongs to NP.
Proof:
For each yes-instance G we take any Hamiltonian circuit of G as a
certiﬁcate. To check whether a given edge set is in fact a Hamiltonian circuit of
a given graph is obviously possible in polynomial time.
2
Proposition 15.12.
Integer Linear Inequalities belongs to NP.
Proof:
As a certiﬁcate we just take a solution vector. If there exists a solution,
there exists one of polynomial size by Corollary 5.6.
2
The name NP stands for “nondeterministic polynomial”. To explain this we
have to deﬁne what a nondeterministic algorithm is. This is a good opportunity
to deﬁne randomized algorithms in general, a concept which has already been
mentioned before. The common feature of randomized algorithms is that their
computation does not only depend on the input but also on some random bits.
Deﬁnition 15.13.
A randomized algorithm for computing a function f : S →T
can be deﬁned as an algorithm computing a function g : {s#r : s ∈S, r ∈
{0, 1}k(s)} →T . So for each instance s ∈S the algorithm uses k(s) ∈Z+ random

15.3 P and NP
353
bits. We measure the running time dependency on size(s) only; randomized algo-
rithms running in polynomial time can read only a polynomial number of random
bits.
Naturally we are interested in such a randomized algorithm only if f and g
are related. In the ideal case, if g(s#r) = f (s) for all s ∈S and all r ∈{0, 1}k(s),
we speak of a Las Vegas algorithm. A Las Vegas algorithm always computes the
correct result, only the running time may vary. Sometimes even less deterministic
algorithms are interesting: If there is at least a positive probability p of a correct
answer, independent of the instance, i.e.
p := inf
s∈S
|{r ∈{0, 1}k(s) : g(s#r) = f (s)}|
2k(s)
> 0,
then we have a Monte Carlo algorithm.
If T = {0, 1}, and for each s ∈S with f (s) = 0 we have g(s#r) = 0 for all
r ∈{0, 1}k(s), then we have a randomized algorithm with one-sided error. If in
addition for each s ∈S with f (s) = 1 there is at least one r ∈{0, 1}k(s) with
g(s#r) = 1, then the algorithm is called a nondeterministic algorithm.
Alternatively a randomized algorithm can be regarded as an oracle algorithm
where the oracle produces a random bit (0 or 1) whenever called. A nondeter-
ministic algorithm for a decision problem always answers “no” for a no-instance,
and for each yes-instance there is a chance that it answers “yes”. The following
observation is easy:
Proposition 15.14.
A decision problem belongs to NP if and only if it has a
polynomial-time nondeterministic algorithm.
Proof:
Let P = (X, Y) be a decision problem in NP, and let P′ = (X′, Y ′) be
deﬁned as in Deﬁnition 15.9. Then a polynomial-time algorithm for P′ is in fact
also a nondeterministic algorithm for P: the unknown certiﬁcate is simply replaced
by random bits. Since the number of random bits is bounded by a polynomial in
size(x), x ∈X, so is the running time of the algorithm.
Conversely, if P = (X, Y) has a polynomial-time nondeterministic algorithm
using k(x) random bits for instance x, then there is a polynomial p such that k(x) ≤
p(size(x)) for each instance x. We deﬁne X′ :=
5
x#c : x ∈X, c ∈{0, 1}⌊p(size(x))⌋6
and Y ′ :=
5
x#c ∈X′ : g(x#r) = 1, r consists of the ﬁrst k(x) bits of c
6
.
Then by the deﬁnition of nondeterministic algorithms we have (X′, Y ′) ∈P
and
Y =
5
y ∈X : There exists a string c ∈{0, 1}⌊p(size(x))⌋with y#c ∈Y ′6
.
2
Most decision problems encountered in combinatorial optimization belong to
NP. For many of them it is not known whether they have a polynomial-time
algorithm. However, one can say that certain problems are not easier than others.
To make this precise we introduce the important concept of polynomial reductions.

354
15. NP-Completeness
Deﬁnition 15.15.
Let P1 and P2 = (X, Y) be decision problems. Let f : X →
{0, 1} with f (x) = 1 for x ∈Y and f (x) = 0 for x ∈X \ Y. We say that P1
polynomially reduces to P2 if there exists a polynomial-time oracle algorithm for
P1 using f .
The following observation is the main reason for this concept:
Proposition 15.16.
If P1 polynomially reduces to P2 and there is a polynomial-
time algorithm for P2, then there is a polynomial-time algorithm for P1.
Proof:
Let A2 be an algorithm for P2 with time(A2, y) ≤p2(size(y)) for all
instances y of P2, and let f (x) := output(A2, x). Let A1 be an oracle algorithm
for P1 using f with time(A1, x) ≤p1(size(x)) for all instances x of P1. Then
replacing the oracle calls in A1 by subroutines equivalent to A2 yields an algorithm
A3 for P1. For any instance x of P1 with size(x) = n we have time(A3, x) ≤
p1(n) · p2(p1(n)): there can be at most p1(n) oracle calls in A1, and none of the
instances of P2 produced by A1 can be longer than p1(n). Since we can choose
p1 and p2 to be polynomials we conclude that A3 is a polynomial-time algorithm.
2
The theory of NP-completeness is based on a special kind of polynomial-time
reduction:
Deﬁnition 15.17.
Let P1 = (X1, Y1) and P2 = (X2, Y2) be decision problems.
We say that P1 polynomially transforms to P2 if there is a function f : X1 →X2
computable in polynomial time such that f (x1) ∈Y2 for all x1 ∈Y1 and f (x1) ∈
X2 \ Y2 for all x1 ∈X1 \ Y1.
In other words, yes-instances are transformed to yes-instances, and no-instances
are transformed to no-instances. Obviously, if a problem P1 polynomially trans-
forms to P2, then P1 also polynomially reduces to P2. Polynomial transformations
are sometimes called Karp reductions, while general polynomial reductions are also
known as Turing reductions. Both are easily seen to be transitive.
Deﬁnition 15.18.
A decision problem P ∈NP is called NP-complete if all other
problems in NP polynomially transform to P.
By Proposition 15.16 we know that if there is a polynomial-time algorithm for
any NP-complete problem, then P = NP.
Of course, the above deﬁnition would be meaningless if no NP-complete prob-
lems existed. The next section consists of a proof that there is an NP-complete
problem.
15.4 Cook’s Theorem
In his pioneering work, Cook [1971] proved that a certain decision problem, called
Satisfiability, is in fact NP-complete. We need some deﬁnitions:

15.4 Cook’s Theorem
355
Deﬁnition 15.19.
Assume X = {x1, . . . , xk} is a set of Boolean variables. A
truth assignment for X is a function T : X →{true, false}. We extend T to the
set L := X
.
∪{x : x ∈X} by setting T (x) := true if T (x) := false and vice versa
(x can be regarded as the negation of x). The elements of L are called the literals
over X.
A clause over X is a set of literals over X. A clause represents the disjunction
of those literals and is satisﬁed by a truth assignment iff at least one of its members
is true. A family Z of clauses over X is satisﬁable iff there is some truth assignment
simultaneously satisfying all of its clauses.
Since we consider the conjunction of disjunctions of literals, we also speak of
Boolean formulas in conjunctive normal form. For example, the family {{x1, x2},
{x2, x3}, {x1, x2, x3}, {x1, x3}} corresponds to the Boolean formula (x1 ∨x2) ∧
(x2 ∨x3) ∧(x1 ∨x2 ∨x3) ∧(x1 ∨x3). It is satisﬁable as the truth assignment
T (x1) := true, T (x2) := false and T (x3) := true shows. We are now ready to
specify the satisﬁability problem:
Satisfiability
Instance:
A set X of variables and a family Z of clauses over X.
Question:
Is Z satisﬁable?
Theorem 15.20.
(Cook [1971]) Satisfiability is NP-complete.
Proof:
Satisfiability belongs to NP because a satisfying truth assignment serves
as a certiﬁcate for any yes-instance, which of course can be checked in polynomial
time.
Let now P = (X, Y) be any other problem in NP. We have to show that P
polynomially transforms to Satisfiability.
By Deﬁnition 15.9 there is a polynomial p and a decision problem P′ =
(X′, Y ′) in P, where X′ :=
5
x#c : x ∈X, c ∈{0, 1}⌊p(size(x))⌋6
and
Y =
5
y ∈X : There exists a string c ∈{0, 1}⌊p(size(x))⌋with y#c ∈Y ′6
.
Let
	 : {0, . . . , N} × ¯A →{−1, . . . , N} × ¯A × {−1, 0, 1}
be a polynomial-time Turing machine for P′ with alphabet A; let ¯A := A ∪{⊔}.
Let q be a polynomial such that time(	, x#c) ≤q(size(x#c)) for all instances
x#c ∈X′. Note that size(x#c) = size(x) + 1 + ⌊p(size(x))⌋.
We will now construct a collection Z(x) of clauses over some set V (x) of
Boolean variables for each x ∈X, such that Z(x) is satisﬁable if and only if
x ∈Y.
We abbreviate Q := q(size(x) + 1 + ⌊p(size(x))⌋). Q is an upper bound on
the length of any computation of 	 on input x#c, for any c ∈{0, 1}⌊p(size(x))⌋.
V (x) contains the following Boolean variables:
– a variable vi jσ for all 0 ≤i ≤Q, −Q ≤j ≤Q and σ ∈¯A;

356
15. NP-Completeness
– a variable wi jn for all 0 ≤i ≤Q, −Q ≤j ≤Q and −1 ≤n ≤N.
The intended meaning is: vi jσ indicates whether at time i (i.e. after i steps
of the computation) the j-th position of the string contains the symbol σ. wi jn
indicates whether at time i the j-th position of the string is scanned and the n-th
instruction is executed.
So if (n(i), s(i), π(i))i=0,1,... is a computation of 	 then we intend to set vi jσ to
true iff s(i)
j
= σ and wi jn to true iff π(i) = j and n(i) = n.
The collection Z(x) of clauses to be constructed will be satisﬁable if and only
if there is a string c with output(	, x#c) = 1.
Z(x) contains the following clauses to model the following conditions:
At any time each position of the string contains a unique symbol:
– {vi jσ : σ ∈¯A}
for 0 ≤i ≤Q and −Q ≤j ≤Q;
– {vi jσ, vi jτ}
for 0 ≤i ≤Q, −Q ≤j ≤Q and σ, τ ∈¯A with σ ̸= τ.
At any time a unique position of the string is scanned and a single instruction
is executed:
– {wi jn : −Q ≤j ≤Q, −1 ≤n ≤N}
for 0 ≤i ≤Q;
– {wi jn, wi j′n′}
for 0 ≤i ≤Q, −Q ≤j, j′ ≤Q and −1 ≤n, n′ ≤N with
( j, n) ̸= ( j′, n′).
The algorithm starts correctly with input x#c for some c ∈{0, 1}⌊p(size(x))⌋:
– {v0, j,xj}
for 1 ≤j ≤size(x);
– {v0,size(x)+1,#};
– {v0,size(x)+1+ j,0, v0,size(x)+1+ j,1}
for 1 ≤j ≤⌊p(size(x))⌋;
– {v0, j,⊔}
for −Q ≤j ≤0 and size(x) + 2 + ⌊p(size(x))⌋≤j ≤Q;
– {w010}.
The algorithm works correctly:
– {vi jσ, wi jn, vi+1, j,τ}, {vi jσ, wi jn, wi+1, j+δ,m}
for 0 ≤i < Q,
−Q ≤j ≤Q, σ ∈¯A and 0 ≤n ≤N, where 	(n, σ) = (m, τ, δ).
When the algorithm reaches statement −1, it stops:
– {wi, j,−1, wi+1, j,−1}, {wi, j,−1, vi, j,σ, vi+1, j,σ}
for 0 ≤i < Q, −Q ≤j ≤Q and σ ∈¯A.
Positions not being scanned remain unchanged:
– {vi jσ, wi j′n, vi+1, j,σ}
for 0 ≤i ≤Q, σ ∈¯A, −1 ≤n ≤N and
−Q ≤j, j′ ≤Q with j ̸= j′.
The output of the algorithm is 1:
– {vQ,1,1}, {vQ,2,⊔}.

15.4 Cook’s Theorem
357
The encoding length of Z(x) is O(Q3 log Q): There are O(Q3) occurrences
of literals, whose indices require O(log Q) space. Since Q depends polynomially
on size(x) we conclude that there is a polynomial-time algorithm which, given x,
constructs Z(x). Note that p, 	 and q are ﬁxed and not part of the input of this
algorithm.
It remains to show that Z(x) is satisﬁable if and only if x ∈Y.
If Z(x) is satisﬁable, consider a truth assignment T satisfying all clauses. Let
c ∈{0, 1}⌊p(size(x))⌋with cj = 1 for all j with T (v0,size(x)+1+ j,1) = true and cj = 0
otherwise. By the above construction the variables reﬂect the computation of 	
on input x#c. Hence we may conclude that output(	, x#c) = 1. Since 	 is a
certiﬁcate-checking algorithm, this implies that x is a yes-instance.
Conversely, if x ∈Y, let c be any certiﬁcate for x. Let (n(i), s(i), π(i))i=0,1,...,m
be the computation of 	 on input x#c. Then we deﬁne T (vi, j,σ) := true iff s(i)
j
= σ
and T (wi, j,n) = true iff π(i) = j and n(i) = n. For i := m + 1, . . . , Q we set
T (vi, j,σ) := T (vi−1, j,σ) and T (wi, j,n) := T (wi−1, j,n) for all j, n and σ. Then T
is a truth assignment satisfying Z(x), completing the proof.
2
Satisfiability is not the only NP-complete problem; we will encounter many
others in this book. Now that we already have one NP-complete problem at hand,
it is much easier to prove NP-completeness for another problem. To show that
a certain decision problem P is NP-complete, we shall just prove that P ∈NP
and that Satisfiability (or any other problem which we know already to be
NP-complete) polynomially transforms to P. Since polynomial transformability is
transitive, this will be sufﬁcient.
The following restriction of Satisfiability will prove very useful for several
NP-completeness proofs:
3Sat
Instance:
A set X of variables and a collection Z of clauses over X, each
containing exactly three literals.
Question:
Is Z satisﬁable?
To show NP-completeness of 3Sat we observe that any clause can be replaced
equivalently by a set of 3Sat-clauses:
Proposition 15.21.
Let X be a set of variables and Z a clause over X with k
literals. Then there is a set Y of at most max{k −3, 2} new variables and a family
Z′ of at most max{k −2, 4} clauses over X
.
∪Y such that each element of Z′
has exactly three literals, and for each family W of clauses over X we have that
W ∪{Z} is satisﬁable if and only if W ∪Z′ is satisﬁable. Moreover, such a family
Z′ can be computed in O(k) time.
Proof:
If Z has three literals, we set Z′ := {Z}. If Z has more than three literals,
say Z = {λ1, . . . , λk}, we choose a set Y = {y1, . . . , yk−3} of k −3 new variables
and set

358
15. NP-Completeness
Z′ :=
5
{λ1, λ2, y1}{y1, λ3, y2}, {y2, λ4, y3}, . . . ,
{yk−4, λk−2, yk−3}, {yk−3, λk−1, λk}
6
.
If Z = {λ1, λ2}, we choose a new variable y1 (Y := {y1}) and set
Z′ := {{λ1, λ2, y1}, {λ1, λ2, y1}} .
If Z = {λ1}, we choose a set Y = {y1, y2} of two new variables and set
Z′ := {{λ1, y1, y2}, {λ1, y1, y2}, {λ1, y1, y2}, {λ1, y1, y2}}.
Observe that in each case Z can be equivalently replaced by Z′ in any instance
of Satisfiability.
2
Theorem 15.22.
(Cook [1971]) 3Sat is NP-complete.
Proof:
As a restriction of Satisfiability, 3Sat is certainly in NP. We now show
that Satisfiability polynomially transforms to 3Sat. Consider any collection Z
of clauses Z1, . . . , Zm. We shall construct a new collection Z′ of clauses with
three literals per clause such that Z is satisﬁable if and only if Z′ is satisﬁable.
To do this, we replace each clause Zi by an equivalent set of clauses, each
with three literals. This is possible in linear time by Proposition 15.21.
2
If we restrict each clause to consist of just two literals, the problem (called
2Sat) can be solved in linear time (Exercise 7).
15.5 Some Basic NP-Complete Problems
Karp discovered the wealth of consequences of Cook’s work for combinatorial
optimization problems. As a start, we consider the following problem:
Stable Set
Instance:
A graph G and an integer k.
Question:
Is there a stable set of k vertices?
Theorem 15.23.
(Karp [1972]) Stable Set is NP-complete.
Proof:
Obviously, Stable Set ∈NP. We show that Satisfiability polynomially
transforms to Stable Set.
Let Z be a collection of clauses Z1, . . . , Zm with Zi = {λi1, . . . , λiki} (i =
1, . . . , m), where the λi j are literals over some set X of variables.
We shall construct a graph G such that G has a stable set of size m if and
only if there is a truth assignment satisfying all m clauses.
For each clause Zi, we introduce a clique of ki vertices according to the literals
in this clause. Vertices corresponding to different clauses are connected by an edge

15.5 Some Basic NP-Complete Problems
359
x1
x1
x2
x2
x3
x3
x3
x3
x2
x1
Fig. 15.1.
if and only if the literals contradict each other. Formally, let V (G) := {vi j : 1 ≤
i ≤m, 1 ≤j ≤ki} and
E(G) :=
5
{vi j, vkl} : (i = k and j ̸= l)
or (λi j = x and λkl = x for some x ∈X)
6
.
See Figure 15.1 for an example (m = 4, Z1 = {x1, x2, x3}, Z2 = {x1, x3}, Z3 =
{x2, x3} and Z4 = {x1, x2, x3}).
Suppose G has a stable set of size m. Then its vertices specify pairwise compat-
ible literals belonging to different clauses. Setting each of these literals to be true
(and setting variables not occurring there arbitrarily) we obtain a truth assignment
satisfying all m clauses.
Conversely, if some truth assignment satisﬁes all m clauses, then we choose
a literal which is true out of each clause. The set of corresponding vertices then
deﬁnes a stable set of size m in G.
2
It is essential that k is part of the input: for each ﬁxed k it can be decided in
O(nk) time whether a given graph with n vertices has a stable set of size k (simply
by testing all vertex sets with k elements). Two interesting related problems are
the following:
Vertex Cover
Instance:
A graph G and an integer k.
Question:
Is there a vertex cover of cardinality k?

360
15. NP-Completeness
Clique
Instance:
A graph G and an integer k.
Question:
Has G a clique of cardinality k?
Corollary 15.24.
(Karp [1972]) Vertex Cover and Clique are NP-complete.
Proof:
By Proposition 2.2, Stable Set polynomially transforms to both Vertex
Cover and Clique.
2
We now turn to the famous Hamiltonian circuit problem (already deﬁned in
Section 15.3).
Theorem 15.25.
(Karp [1972]) Hamiltonian Circuit is NP-complete.
Proof:
Membership in NP is obvious. We prove that 3Sat polynomially trans-
forms to Hamiltonian Circuit. Given a collection Z of clauses Z1, . . . , Zm over
X = {x1, . . . , xn}, each clause containing three literals, we shall construct a graph
G such that G is Hamiltonian iff Z is satisﬁable.
(a)
(b)
u
u′
v
v′
u
u′
v
v′
A
Fig. 15.2.
(a)
(b)
u
u′
v
v′
u
u′
v
v′
Fig. 15.3.

15.5 Some Basic NP-Complete Problems
361
We ﬁrst deﬁne two gadgets which will appear several times in G. Consider the
graph shown in Figure 15.2(a), which we call A. We assume that it is a subgraph
of G and no vertex of A except u, u′, v, v′ is incident to any other edge of G.
Then any Hamiltonian circuit of G must traverse A in one of the ways shown
in Figure 15.3(a) and (b). So we can replace A by two edges with the additional
restriction that any Hamiltonian circuit of G must contain exactly one of them
(Figure 15.2(b)).
u
u′
e1
e2
e3
u
u′
B
(a)
(b)
Fig. 15.4.
Now consider the graph B shown in Figure 15.4(a). We assume that it is a
subgraph of G, and no vertex of B except u and u′ is incident to any other edge
of G. Then no Hamiltonian circuit of G traverses all of e1, e2, e3. Moreover, one
easily checks that for any S ⊂{e1, e2, e3} there is a Hamiltonian path from u to
u′ in B that contains S but none of {e1, e2, e3} \ S. We represent B by the symbol
shown in Figure 15.4(b).
We are now able to construct G. For each clause, we introduce a copy of
B, joined one after another. Between the ﬁrst and the last copy of B, we insert
two vertices for each variable, all joined one after another. We then double the
edges between the two vertices of each variable x; these two edges will cor-
respond to x and x, respectively. The edges e1, e2, e3 in each copy of B are
now connected via a copy of A to the ﬁrst, second, third literal of the corre-
sponding clause. This construction is illustrated by Figure 15.5 with the example
{{x1, x2, x3}, {x1, x2, x3}, {x1, x2, x3}}. Note that an edge representing a literal can
take part in more than one copy of A; these are then arranged in series.
Now we claim that G is Hamiltonian if and only if Z is satisﬁable. Let C be
a Hamiltonian circuit. We deﬁne a truth assignment by setting a literal true iff C
contains the corresponding edge. By the properties of the gadgets A and B each
clause contains a literal that is true.

362
15. NP-Completeness
A
A
A
A
A
A
A
A
A
B
B
B
Fig. 15.5.
Conversely, any satisfying truth assignment deﬁnes a set of edges correspond-
ing to literals that are true. Since each clause contains a literal that is true this set
of edges can be completed to a tour in G.
2
This proof is essentially due to Papadimitriou and Steiglitz [1982]. The problem
of deciding whether a given graph contains a Hamiltonian path is also NP-complete
(Exercise 14(a)). Moreover, one can easily transform the undirected versions to
the directed Hamiltonian circuit or Hamiltonian path problem by replacing each
undirected edge by a pair of oppositely directed edges. Thus the directed versions
are also NP-complete.
There is another fundamental NP-complete problem:

15.5 Some Basic NP-Complete Problems
363
3-Dimensional Matching (3DM)
Instance:
Disjoint sets U, V, W of equal cardinality and T ⊆U × V × W.
Question:
Is there a subset M of T with |M| = |U| such that for distinct
(u, v, w), (u′, v′, w′) ∈M one has u ̸= u′, v ̸= v′ and w ̸= w′?
Theorem 15.26.
(Karp [1972]) 3DM is NP-complete.
Proof:
Membership in NP is obvious. We shall polynomially transform Sat-
isfiability to 3DM. Given a collection Z of clauses Z1, . . . , Zm over X =
{x1, . . . , xn}, we construct an instance (U, V, W, T ) of 3DM which is a yes-
instance if and only if Z is satisﬁable.
x1
1
x1
2
x2
1
x2
2
x2
1
x1
1
x2
2
x1
2
a2
1
b1
1
a2
2
b1
2
b2
1
a1
1
b2
2
a1
2
w1
v1
v2
w2
Fig. 15.6.
We deﬁne:
U
:=
{x j
i , xi
j : i = 1, . . . , n; j = 1, . . . , m}
V
:=
{a j
i : i = 1, . . . , n; j = 1, . . . , m} ∪{v j : j = 1, . . . , m}
∪{c j
k : k = 1, . . . , n −1; j = 1, . . . , m}
W
:=
{b j
i : i = 1, . . . , n; j = 1, . . . , m} ∪{w j : j = 1, . . . , m}
∪{d j
k : k = 1, . . . , n −1; j = 1, . . . , m}
T1
:=
{(x j
i , a j
i , b j
i ), (xi
j, a j+1
i
, b j
i ) : i = 1, . . . , n; j = 1, . . . , m},
where am+1
i
:= a1
i
T2
:=
{(x j
i , v j, w j) : i = 1, . . . , n; j = 1, . . . , m; xi ∈Zj}

364
15. NP-Completeness
∪{(xi
j, v j, w j) : i = 1, . . . , n; j = 1, . . . , m; xi ∈Zj}
T3
:=
{(x j
i , c j
k, d j
k ), (xi
j, c j
k, d j
k ) : i =1, . . . , n; j =1, . . . , m; k =1, . . . , n−1}
T
:=
T1 ∪T2 ∪T3.
For an illustration of this construction, see Figure 15.6. Here m = 2, Z1 =
{x1, x2}, Z2 = {x1, x2}. Each triangle corresponds to an element of T1 ∪T2. The
elements c j
k, d j
k and the triples in T3 are not shown.
Suppose (U, V, W, T ) is a yes-instance, so let M ⊆T be a solution. Since
the a j
i ’s and b j
i appear only in elements T1, for each i we have either M ∩T1 ⊇
{(x j
i , a j
i , b j
i ) : j = 1, . . . , m} or M ∩T1 ⊇{(xi
j, a j+1
i
, b j
i ) : j = 1, . . . , m}. In the
ﬁrst case we set xi to false, in the second case to true.
Furthermore, for each clause Zj we have (λ j, v j, w j) ∈M for some literal
λ ∈Zj. Since λ j does not appear in any element of M ∩T1 this literal is true;
hence we have a satisfying truth assignment.
Conversely, a satisfying truth assignment suggests a set M1 ⊆T1 of cardinality
nm and a set M2 ⊆T2 of cardinality m such that for distinct (u, v, w), (u′, v′, w′) ∈
M1 ∪M2 we have u ̸= u′, v ̸= v′ and w ̸= w′. It is easy to complete M1 ∪M2 by
(n −1)m elements of T3 to a solution of the 3DM instance.
2
A problem which looks simple but is not known to be solvable in polynomial
time is the following:
Subset-Sum
Instance:
Natural numbers c1, . . . , cn, K.
Question:
Is there a subset S ⊆{1, . . . , n} such that 
j∈S cj = K ?
Corollary 15.27.
(Karp [1972]) Subset-Sum is NP-complete.
Proof:
It is obvious that Subset-Sum is in NP. We prove that 3DM polynomially
transforms to Subset-Sum. So let (U, V, W, T ) be an instance of 3DM. W.l.o.g.
let U ∪V ∪W = {u1, . . . , u3m}. We write S := {{a, b, c} : (a, b, c) ∈T } and
S = {s1, . . . , sn}.
Deﬁne
cj :=

ui∈sj
(n + 1)i−1
( j = 1, . . . , n)
and
K :=
3m

i=1
(n + 1)i−1.
Written in (n + 1)-ary form, the number cj can be regarded as the incidence
vector of sj ( j = 1, . . . , n), and K consists of 1’s only. Therefore each solution
to the 3DM instance corresponds to a subset R of S such that 
sj∈R cj = K, and
vice versa. Moreover, size(cj) ≤size(K) = O(m log n), so the above is indeed a
polynomial transformation.
2

15.6 The Class coNP
365
An important special case is the following problem:
Partition
Instance:
Natural numbers c1, . . . , cn.
Question:
Is there a subset S ⊆{1, . . . , n} such that 
j∈S cj = 
j /∈S cj ?
Corollary 15.28.
(Karp [1972]) Partition is NP-complete.
Proof:
We show that Subset-Sum polynomially transforms to Partition. So
let c1, . . . , cn, K be an instance of Subset-Sum. We add an element cn+1 :=
n
i=1 ci −2K
 (unless this number is zero) and have an instance c1, . . . , cn+1 of
Partition.
Case 1:
2K ≤n
i=1 ci. Then for any I ⊆{1, . . . , n} we have

i∈I
ci = K
if and only if

i∈I∪{n+1}
ci =

i∈{1,...,n}\I
ci.
Case 2:
2K > n
i=1 ci. Then for any I ⊆{1, . . . , n} we have

i∈I
ci = K
if and only if

i∈I
ci =

i∈{1,...,n+1}\I
ci.
In both cases we have constructed a yes-instance of Partition if and only if
the original instance of Subset-Sum is a yes-instance.
2
We ﬁnally note:
Theorem 15.29.
Integer Linear Inequalities is NP-complete.
Proof:
We already mentioned the membership in NP in Proposition 15.12. Any
of the above problems can easily be formulated as an instance of Integer Linear
Inequalities. For example a Partition instance c1, . . . , cn is a yes-instance if
and only if {x ∈Zn : 0 ≤x ≤1l, 2c⊤x = c⊤1l} is nonempty.
2
15.6 The Class coNP
The deﬁnition of NP is not symmetric with respect to yes-instances and no-
instances. For example, it is an open question whether the following problem
belongs to NP: given a graph G, is it true that G is not Hamiltonian? We intro-
duce the following deﬁnitions:
Deﬁnition 15.30.
For a decision problem P = (X, Y) we deﬁne its complement
to be the decision problem (X, X \ Y). The class coNP consists of all problems
whose complements are in NP. A decision problem P ∈coNP is called coNP-
complete if all other problems in coNP polynomially transform to P.

366
15. NP-Completeness
Trivially, the complement of a problem in P is also in P. On the other hand,
NP ̸= coNP is commonly conjectured (though not proved). When considering this
conjecture, the NP-complete problems play a special role:
Theorem 15.31.
A decision problem is coNP-complete if and only if its comple-
ment is NP-complete. Unless NP = coNP, no coNP-complete problem is in NP.
Proof:
The ﬁrst statement follows directly from the deﬁnition.
Suppose P = (X, Y) ∈NP is a coNP-complete problem. Let Q = (V, W) be
an arbitrary problem in coNP. We show that Q ∈NP.
Since P is coNP-complete, Q polynomially transforms to P. So there is a
polynomial-time algorithm which transforms any instance v of Q to an instance
x = f (v) of P such that x ∈Y if and only if v ∈W. Note that size(x) ≤
p(size(v)) for some ﬁxed polynomial p.
Since P ∈NP, there exists a polynomial q and a decision problem P′ =
(X′, Y ′) in P, where X′ :=
5
x#c : x ∈X, c ∈{0, 1}⌊q(size(x))⌋6
, such that
Y =
5
y ∈X : There exists a string c ∈{0, 1}⌊q(size(y))⌋with y#c ∈Y ′6
(cf. Deﬁnition 15.9). We deﬁne a decision problem (V ′, W ′) by V ′ :=
5
v#c : v ∈
V, c
∈
{0, 1}⌊q(p(size(v)))⌋6
, and v#c ∈W ′ if and only if f (v)#c′ ∈Y ′ where c′ consists of
the ﬁrst ⌊q(size( f (v)))⌋components of c.
Observe that (V ′, W ′) ∈P. Therefore, by deﬁnition, Q ∈NP. We conclude
coNP ⊆NP and hence, by symmetry, NP = coNP.
2
If one can show that a problem is in NP ∩coNP, we say that the problem has
a good characterization (Edmonds [1965]). This means that for yes-instances as
well as for no-instances there are certiﬁcates that can be checked in polynomial
time. Theorem 15.31 indicates that a problem with a good characterization is
probably not NP-complete.
To give examples, Proposition 2.9, Theorem 2.24, and Proposition 2.27 provide
good characterizations for the problems of deciding whether a given graph is
acyclic, whether it has an Eulerian walk, and whether it is bipartite, respectively.
Of course, this is not very interesting since all these problems can be solved easily
in polynomial time. But consider the decision version of Linear Programming:
Theorem 15.32.
Linear Inequalities is in NP ∩coNP.
Proof:
This immediately follows from Theorem 4.4 and Corollary 3.19.
2
Of course, this theorem also follows from any polynomial-time algorithm
for Linear Programming, e.g. Theorem 4.18. However, before the Ellipsoid
Method had been discovered, Theorem 15.32 was the only theoretical evidence
that Linear Inequalities is probably not NP-complete. This gave hope to ﬁnd a
polynomial-time algorithm for Linear Programming (which can be reduced to
Linear Inequalities by Proposition 4.16); a justiﬁed hope as we know today.

15.7 NP-Hard Problems
367
The following famous problem has a similar history:
Prime
Instance:
A number n ∈N (in its binary representation).
Question:
Is n a prime?
It is obvious that Prime belongs to coNP. Pratt [1975] proved that Prime also
belongs to NP. Finally, Agrawal, Kayal and Saxena [2004] proved that Prime
∈P by ﬁnding a surprisingly simple O(log7.5+ϵ n)-algorithm (for any ϵ > 0).
Before, the best known deterministic algorithm for Prime was due to Adleman,
Pomerance and Rumely [1983], running in O

(log n)c log log log n
time for some
constant c. Since the input size is O(log n), this is not polynomial.
NP-complete
coNP-complete
NP ∩coNP
P
NP
coNP
Fig. 15.7.
We close this section by sketching the inclusions of NP and coNP (Figure
15.7). Ladner [1975] showed that, unless P = NP, there are problems in NP \ P
that are not NP-complete. However, until the P ̸= NP conjecture is resolved, it is
still possible that all regions drawn in Figure 15.7 collapse to one.
15.7 NP-Hard Problems
Now we extend our results to optimization problems. We start by formally deﬁning
the type of optimization problems we are interested in:
Deﬁnition 15.33.
A (discrete) optimization problem is a quadruple P = (X,
(Sx)x∈X, c, goal), where
–
X is a language over {0, 1} decidable in polynomial time;

368
15. NP-Completeness
–
Sx is a subset of {0, 1}∗for each x ∈X; there exists a polynomial p with
size(y) ≤p(size(x)) for all y ∈Sx and all x ∈X, and the languages {(x, y) :
x ∈X, y ∈Sx} and {x ∈X : Sx = ∅} are decidable in polynomial time;
–
c : {(x, y) : x ∈X, y ∈Sx} →Q is a function computable in polynomial time;
and
–
goal ∈{max, min}.
The elements of X are called instances of P. For each instance x, the elements
of Sx are called feasible solutions of x. We write OPT(x) := goal{c(x, y) : y ∈Sx}.
An optimum solution of x is a feasible solution y of x with c(x, y) = OPT(x).
An algorithm for an optimization problem (X, (Sx)x∈X, c, goal) is an algorithm
A which computes for each input x ∈X with Sx ̸= ∅a feasible solution y ∈Sx. We
sometimes write A(x) := c(x, y). If A(x) = OPT(x) for all x ∈X with Sx ̸= ∅,
then A is an exact algorithm.
Depending on the context, c(x, y) is often called the cost, the weight, the proﬁt
or the length of y. If c is nonnegative, then we say that the optimization problem
has nonnegative weights. The values of c are rational numbers; we assume an
encoding into binary strings as usual.
The concept of polynomial reductions easily extends to optimization problems:
a problem polynomially reduces to an optimization problem P = (X, (Sx)x∈X, c,
goal) if it has an exact polynomial-time oracle algorithm using any function f
with f (x) ∈{y ∈Sx : c(x, y) = OPT(x)} for all x ∈X with Sx ̸= ∅. Now we
can deﬁne:
Deﬁnition 15.34.
An optimization problem or decision problem P is called NP-
hard if all problems in NP polynomially reduce to P.
Note that the deﬁnition is symmetric: a decision problem is NP-hard if and
only if its complement is. NP-hard problems are at least as hard as the hardest
problems in NP. But some may be harder than any problem in NP. A problem
which polynomially reduces to some problem in NP is called NP-easy. A problem
which is both NP-hard and NP-easy is NP-equivalent. In other words, a problem is
NP-equivalent if and only if it is polynomially equivalent to Satisfiability, where
two problems P and Q are called polynomially equivalent if P polynomially
reduces to Q, and Q polynomially reduces to P. We note:
Proposition 15.35.
Let P be an NP-equivalent problem. Then P has an exact
polynomial-time algorithm if and only if P = NP.
2
Of course, all NP-complete problems and all coNP-complete problems are
NP-equivalent. Almost all problems discussed in this book are NP-easy since they
polynomially reduce to IntegerProgramming; this is usually a trivial observation
which we do not even mention. On the other hand, most problems we discuss
from now on are also NP-hard, and we shall usually prove this by describing a
polynomial reduction from an NP-complete problem.

15.7 NP-Hard Problems
369
It is an open question whether each NP-hard decision problem P ∈NP is
NP-complete (recall the difference between polynomial reduction and polynomial
transformation; Deﬁnitions 15.15 and 15.17). Exercises 17 and 18 discuss two
NP-hard decision problems that appear not to be in NP.
Unless P = NP there is no exact polynomial-time algorithm for any NP-hard
problem. There might, however, be a pseudopolynomial algorithm:
Deﬁnition 15.36.
Let P be a decision problem or an optimization problem such
that each instance x consists of a list of integers. We denote by largest(x) the largest
of these integers. An algorithm for P is called pseudopolynomial if its running
time is bounded by a polynomial in size(x) and largest(x).
For example there is a trivial pseudopolynomial algorithm for Prime which
divides the natural number n to be tested for primality by each integer from 2 to
⌊√n⌋. Another example is:
Theorem 15.37.
There is a pseudopolynomial algorithm for Subset-Sum.
Proof:
Given an instance c1, . . . , cn, K of Subset-Sum, we construct a di-
graph G with vertex set {0, . . . , n} × {0, 1, 2, . . . , K}. For each j ∈{1, . . . , n}
we add edges (( j −1, i), ( j, i)) (i = 0, 1, . . . , K) and (( j −1, i), ( j, i + cj))
(i = 0, 1, . . . , K −cj).
Observe that any path from (0, 0) to ( j, i) corresponds to a subset S ⊆
{1, . . . , j} with 
k∈S ck = i, and vice versa. Therefore we can solve our Subset-
Sum instance by checking whether G contains a path from (0, 0) to (n, K). With
the Graph Scanning Algorithm this can be done in O(nK) time, so we have
a pseudopolynomial algorithm.
2
The above is also a pseudopolynomial algorithm for Partition because
1
2
n
i=1ci ≤n
2 largest(c1, . . . , cn). We shall discuss an extension of this algorithm
in Section 17.2. If the numbers are not too large, a pseudopolynomial algorithm
can be quite efﬁcient. Therefore the following deﬁnition is useful:
Deﬁnition 15.38.
For a decision problem P = (X, Y) or an optimization prob-
lem P = (X, (Sx)x∈X, c, goal), and a subset X′ ⊆X of instances we deﬁne the
restriction of P to X′ by P′ = (X′, X′ ∩Y) or P′ = (X′, (Sx)x∈X′, c, goal), re-
spectively.
Let P be a decision or optimization problem such that each instance consists of
a list of integers. For a polynomial p let Pp be the restriction of P to instances x
with largest(x) ≤p(size(x)). P is called strongly NP-hard if there is a polynomial
p such that Pp is NP-hard. P is called strongly NP-complete if P ∈NP and there
is a polynomial p such that Pp is NP-complete.
Proposition 15.39.
Unless P = NP there is no exact pseudopolynomial algorithm
for any strongly NP-hard problem.
2

370
15. NP-Completeness
We give some famous examples:
Theorem 15.40.
Integer Programming is strongly NP-hard.
Proof:
For an undirected graph G the integer program max{1lx : x ∈ZV (G), 0 ≤
x ≤1l, xv + xw ≤1 for {v, w} ∈E(G)} has optimum value at least k if and only
if G contains a stable set of cardinality k. Since k ≤|V (G)| for all nontrivial
instances (G, k) of Stable Set, the result follows from Theorem 15.23.
2
Traveling Salesman Problem (TSP)
Instance:
A complete graph Kn (n ≥3) and weights c : E(Kn) →Q+.
Task:
Find a Hamiltonian circuit T whose weight 
e∈E(T ) c(e) is mini-
mum.
The vertices of a TSP-instance are often called cities, the weights are also
referred to as distances.
Theorem 15.41.
The TSP is strongly NP-hard.
Proof:
We show that the TSP is NP-hard even when restricted to instances
where all distances are 1 or 2. We describe a polynomial transformation from the
Hamiltonian Circuit problem. Given a graph G on n vertices, we construct
the following instance of TSP: Take one city for each vertex of G, and let the
distances be 1 whenever the edge is in E(G) and 2 otherwise. It is then obvious
that G is Hamiltonian if and only if the optimum TSP tour has length n.
2
The proof also shows that the following decision problem is not easier than
the TSP itself: Given an instance of the TSP and an integer k, is there a tour of
length k or less? A similar statement is true for a large class of discrete optimization
problems:
Proposition 15.42.
Let F and F′ be (inﬁnite) families of ﬁnite sets, and let P be
the following optimization problem: Given a set E ∈F and a function c : E →Z,
ﬁnd a set F ⊆E with F ∈F′ and c(F) minimum (or decide that no such F exists).
Then P can be solved in polynomial time if and only if the following decision
problem can be solved in polynomial time: Given an instance (E, c) of P and an
integer k, is OPT((E, c)) ≤k? If the optimization problem is NP-hard, then so is
this decision problem.
Proof:
It sufﬁces to show that there is an oracle algorithm for the optimization
problem using the decision problem (the converse is trivial). Let (E, c) be an
instance of P. We ﬁrst determine OPT((E, c)) by binary search. Since there are
at most 1 + 
e∈E |c(e)| ≤2size(c) possible values we can do this with O(size(c))
iterations, each including one oracle call.
Then we successively check for each element of E whether there exists an
optimum solution without this element. This can be done by increasing its weight

Exercises
371
(say by one) and check whether this also increases the value of an optimum
solution. If so, we keep the old weight, otherwise we indeed increase the weight.
After checking all elements of E, those elements whose weight we did not change
constitute an optimum solution.
2
Examples where this result applies are the TSP, the MaximumWeightClique
Problem, the Shortest Path Problem with nonnegative weights, the Knapsack
Problem, and many others.
Exercises
1. Observe that there are more languages than Turing machines. Conclude that
there are languages that cannot be decided by a Turing machine.
Turing machines can also be encoded by binary strings. Consider the famous
Halting Problem: Given two binary strings x and y, where x encodes a
Turing machine 	, is time(	, y) < ∞?
Prove that the Halting Problem is undecidable (i.e. there is no algorithm
for it).
Hint: Assuming that there is such an algorithm A, construct a Turing machine
which, on input x, ﬁrst runs the algorithm A on input (x, x) and then terminates
if and only if output(A, (x, x)) = 0.
2. Describe a Turing machine which compares two strings: it should accept as
input a string a#b with a, b ∈{0, 1}∗and output 1 if a = b and 0 if a ̸= b.
3. A well-known machine model is the RAM machine: It works with an inﬁnite
sequence of registers x1, x2, . . . and one special register, the accumulator Acc.
Each register can store an arbitrary large integer, possibly negative. A RAM
program is a sequence of instructions. There are ten types of instructions (the
meaning is illustrated on the right-hand side):
WRITE
k
Acc := k.
LOAD
k
Acc := xk.
LOADI
k
Acc := xxk.
STORE
k
xk := Acc.
STOREI
k
xxk := Acc.
ADD
k
Acc := Acc + xk.
SUBTR
k
Acc := Acc −xk.
HALF
k
Acc := ⌊Acc/2⌋.
IFPOS
i
If Acc > 0 then go to i⃝.
HALT
Stop.
A RAM program is a sequence of m instructions; each is one of the above,
where k ∈Z and i ∈{1, . . . , m}. The computation starts with instruction 1; it
then proceeds as one would expect; we do not give a formal deﬁnition.

372
15. NP-Completeness
The above list of instructions may be extended. We say that a command can
be simulated by a RAM program in time n if it can be substituted by RAM
commands so that the total number of steps in any computation increases by
at most a factor of n.
(a) Show that the following commands can be simulated by small RAM
programs in constant time:
IFNEG
k
If Acc < 0 then go to k⃝.
IFZERO
k
If Acc = 0 then go to k⃝.
(b)
∗
Show that the SUBTR and HALF commands can be simulated by RAM
programs using only the other eight commands in O(size(xk)) time and
O(size(Acc)) time, respectively.
(c)
∗
Show that the following commands can be simulated by RAM programs
in O(n) time, where n = max{size(xk), size(Acc)}:
MULT
k
Acc := Acc · xk.
DIV
k
Acc := ⌊Acc/xk⌋.
MOD
k
Acc := Acc mod xk.
4.
∗
Let f : {0, 1}∗→{0, 1}∗be a mapping. Show that if there is a Turing machine
	 computing f , then there is a RAM program (cf. Exercise 3) such that the
computation on input x (in Acc) terminates after O(size(x)+time(	, x)) steps
with Acc = f (x).
Show that if there is a RAM machine which, given x in Acc, computes f (x)
in Acc in at most g(size(x)) steps, then there is a Turing machine computing
f with time(	, x) = O(g(size(x))3).
5. Prove that the following two decision problems are in NP:
(a) Given two graphs G and H, is G isomorphic to a subgraph of H?
(b) Given a natural number n (in binary encoding), is there a prime number
p with n = p p?
6. Prove: If P ∈NP, then there exists a polynomial p such that P can be solved
by a (deterministic) algorithm having time complexity O

2p(n)
.
7. Let Z be a 2Sat instance, i.e. a collection of clauses over X with two literals
each. Consider a digraph G(Z) as follows: V (G) is the set of literals over X.
There is an edge (λ1, λ2) ∈E(G) iff the clause
5
λ1, λ2
6
is a member of Z.
(a) Show that if, for some variable x, x and x are in the same strongly
connected component of G(Z), then Z is not satisﬁable.
(b) Show the converse of (a).
(c) Give a linear-time algorithm for 2Sat.
8. Describe a linear-time algorithm which for any instance of Satisfiability
ﬁnds a truth assignment satisfying at least half of the clauses.
9. Consider 3-Occurrence Sat, which is Satisfiability restricted to instances
where each clause contains at most three literals and each variable occurs in
at most three clauses. Prove that even this restricted version is NP-complete.

Exercises
373
10. Let κ : {0, 1}m →{0, 1}m be a (not necessarily bijective) mapping, m ≥2.
For x = x1 ×· · ·× xn ∈{0, 1}m ×· · ·×{0, 1}m = {0, 1}nm let κ(x) := κ(x1)×
· · · × κ(xn), and for a decision problem P = (X, Y) with X ⊆
n∈Z+{0, 1}nm
let κ(P) := ({κ(x) : x ∈X}, {κ(x) : x ∈Y}). Prove:
(a) For all codings κ and all P ∈NP we have also κ(P) ∈NP.
(b) If κ(P) ∈P for all codings κ and all P ∈P, then P = NP.
(Papadimitriou [1994])
11. Prove that Stable Set is NP-complete even if restricted to graphs whose
maximum degree is 4.
Hint: Use Exercise 9.
12. Prove that the following problem, sometimes called Dominating Set, is NP-
complete: Given an undirected graph G and a number k ∈N, is there a set
X ⊆V (G) with |X| ≤k such that X ∪(X) = V (G) ?
Hint: Transformation from Vertex Cover.
13. The decision problem Clique is NP-complete. Is it still NP-complete (pro-
vided that P ̸= NP) if restricted to
(a) bipartite graphs,
(b) planar graphs,
(c) 2-connected graphs?
14. Prove that the following problems are NP-complete:
(a) Hamiltonian Path and Directed Hamiltonian Path
Given a graph G (directed or undirected), does G contain a Hamiltonian
path?
(b) Shortest Path
Given a graph G, weights c : E(G) →Z, two vertices s, t ∈V (G), and
an integer k. Is there an s-t-path of weight at most k?
(c) 3-Matroid Intersection
Given three matroids (E, F1), (E, F2), (E, F3) (by independence oracles)
and a number k ∈N, decide whether there is a set F ∈F1 ∩F2 ∩F3 with
|F| ≥k.
(d) Chinese Postman Problem
Given graphs G and H with V (G) = V (H), weights c : E(H) →Z+
and an integer k. Is there a subset F ⊆E(H) with c(F) ≤k such that
(V (G), E(G)
.
∪F) is connected and Eulerian?
15. Either ﬁnd a polynomial-time algorithm or prove NP-completeness for the
following decision problems:
(a) Given an undirected graph G and some T ⊆V (G), is there a spanning
tree in G such that all vertices in T are leaves?
(b) Given an undirected graph G and some T ⊆V (G), is there a spanning
tree in G such that all leaves are elements of T ?
(c) Given a digraph G, weights c : E(G) →R, a set T ⊆V (G) and a
number k, is there a branching B with |δ+
B (x)| ≤1 for all x ∈T and
c(B) ≥k?

374
15. NP-Completeness
16. Prove that the following decision problem belongs to coNP: Given a matrix
A ∈Qm×n and a vector b ∈Qn, is the polyhedron {x : Ax ≤b} integral?
Hint: Use Proposition 3.8, Lemma 5.10, and Theorem 5.12.
Note: The problem is not known to be in NP.
17. Show that the following problem is NP-hard (it is not known to be in NP):
Given an instance of Satisfiability, does the majority of all truth assignments
satisfy all the clauses?
18. Show that Partition polynomially transforms to the following problem (which
is thus NP-hard; it is not known to be in NP):
K-th Heaviest Subset
Instance:
Integers c1, . . . , cn, K, L.
Question:
Are there K distinct subsets S1, . . . , SK ⊆{1, . . . , n} such that

j∈Si cj ≥L for i = 1, . . . , K ?
Hint: Deﬁne K and L appropriately.
19. Prove that the following problem is NP-hard:
Maximum Weight Cut Problem
Instance:
An undirected graph G and weights c : E(G) →Z+.
Task:
Find a cut in G with maximum total weight.
Hint: Transformation from Partition.
Note: The problem is in fact strongly NP-hard; see Exercise 3 of Chapter 16.
(Karp [1972])
References
General Literature:
Aho, A.V., Hopcroft, J.E., and Ullman, J.D. [1974]: The Design and Analysis of Computer
Algorithms. Addison-Wesley, Reading 1974
Ausiello, G., Crescenzi, P., Gambosi, G., Kann, V., Marchetti-Spaccamela, A., and Protasi,
M. [1999]: Complexity and Approximation: Combinatorial Optimization Problems and
Their Approximability Properties. Springer, Berlin 1999
Bovet, D.B., and Crescenzi, P. [1994]: Introduction to the Theory of Complexity. Prentice-
Hall, New York 1994
Garey, M.R., and Johnson, D.S. [1979]: Computers and Intractability: A Guide to the
Theory of NP-Completeness. Freeman, San Francisco 1979, Chapters 1–3, 5, and 7
Horowitz, E., and Sahni, S. [1978]: Fundamentals of Computer Algorithms. Computer
Science Press, Potomac 1978, Chapter 11
Johnson, D.S. [1981]: The NP-completeness column: an ongoing guide. Journal of Algo-
rithms starting with Vol. 4 (1981)
Karp, R.M. [1975]: On the complexity of combinatorial problems. Networks 5 (1975),
45–68
Papadimitriou, C.H. [1994]: Computational Complexity. Addison-Wesley, Reading 1994

References
375
Papadimitriou, C.H., and Steiglitz, K. [1982]: Combinatorial Optimization: Algorithms and
Complexity. Prentice-Hall, Englewood Cliffs 1982, Chapters 15 and 16
Wegener, I. [2005]: Complexity Theory: Exploring the Limits of Efﬁcient Algorithms.
Springer, Berlin 2005
Cited References:
Adleman, L.M., Pomerance, C., and Rumely, R.S. [1983]: On distinguishing prime numbers
from composite numbers. Annals of Mathematics 117 (1983), 173–206
Agrawal, M., Kayal, N., and Saxena, N. [2004]: PRIMES is in P. Annals of Mathematics
160 (2004), 781–793
Cook, S.A. [1971]: The complexity of theorem proving procedures. Proceedings of the 3rd
Annual ACM Symposium on the Theory of Computing (1971), 151–158
Edmonds, J. [1965]: Minimum partition of a matroid into independent subsets. Journal of
Research of the National Bureau of Standards B 69 (1965), 67–72
van Emde Boas, P. [1990]: Machine models and simulations. In: Handbook of Theoreti-
cal Computer Science; Volume A; Algorithms and Complexity (J. van Leeuwen, ed.),
Elsevier, Amsterdam 1990, pp. 1–66
Hopcroft, J.E., and Ullman, J.D. [1979]: Introduction to Automata Theory, Languages, and
Computation. Addison-Wesley, Reading 1979
Karp, R.M. [1972]: Reducibility among combinatorial problems. In: Complexity of Com-
puter Computations (R.E. Miller, J.W. Thatcher, eds.), Plenum Press, New York 1972,
pp. 85–103
Ladner, R.E. [1975]: On the structure of polynomial time reducibility. Journal of the ACM
22 (1975), 155–171
Lewis, H.R., and Papadimitriou, C.H. [1981]: Elements of the Theory of Computation.
Prentice-Hall, Englewood Cliffs 1981
Pratt, V. [1975]: Every prime has a succinct certiﬁcate. SIAM Journal on Computing 4
(1975), 214–220
Sch¨onhage, A., and Strassen, V. [1971]: Schnelle Multiplikation großer Zahlen. Computing
7 (1971), 281–292
Turing, A.M. [1936]: On computable numbers, with an application to the Entschei-
dungsproblem. Proceedings of the London Mathematical Society (2) 42 (1936), 230–265
and 43 (1937), 544–546

16. Approximation Algorithms
In this chapter we introduce the important concept of approximation algorithms. So
far we have dealt mostly with polynomially solvable problems. In the remaining
chapters we shall indicate some strategies to cope with NP-hard combinatorial
optimization problems. Here approximation algorithms must be mentioned in the
ﬁrst place.
The ideal case is when the solution is guaranteed to differ from the optimum
solution by a constant only:
Deﬁnition 16.1.
An absolute approximation algorithm for an optimization prob-
lem P is a polynomial-time algorithm A for P for which there exists a constant k
such that
|A(I) −OPT(I)| ≤k
for all instances I of P.
Unfortunately, an absolute approximation algorithm is known for very few
classical NP-hard optimization problems. We shall discuss two major examples,
the Edge-Colouring Problem and the Vertex-Colouring Problem in planar
graphs in Section 16.2.
In most cases we must be satisﬁed with relative performance guarantees. Here
we have to restrict ourselves to problems with nonnegative weights.
Deﬁnition 16.2.
Let P be an optimization problem with nonnegative weights and
k ≥1. A k-factor approximation algorithm for P is a polynomial-time algorithm
A for P such that
1
k OPT(I) ≤A(I) ≤k OPT(I)
for all instances I of P. We also say that A has performance ratio k.
The ﬁrst inequality applies to maximization problems, the second one to min-
imization problems. Note that for instances I with OPT(I) = 0 we require an
exact solution. The 1-factor approximation algorithms are precisely the exact
polynomial-time algorithms.
In Section 13.4 we saw that the Best-In-Greedy Algorithm for the Max-
imization Problem for an independence system (E, F) has performance ratio
1
q(E,F) (Theorem 13.19). In the following sections and chapters we shall illus-
trate the above deﬁnitions and analyse the approximability of various NP-hard
problems. We start with covering problems.

378
16. Approximation Algorithms
16.1 Set Covering
In this section we focus on the following quite general problem:
Minimum Weight Set Cover Problem
Instance:
A set system (U, S) with 
S∈S S = U, weights c : S →R+.
Task:
Find a minimum weight set cover of (U, S), i.e. a subfamily R ⊆S
such that 
R∈R R = U.
If |{S ∈S : x ∈S}| = 2 for all x ∈U, we get the Minimum Weight Vertex
Cover Problem, which is a special case: given a graph G and c : V (G) →R+,
the corresponding set covering instance is deﬁned by U := E(G), S := {δ(v) :
v ∈V (G)} and c(δ(v)) := c(v) for all v ∈V (G). As the Minimum Weight
Vertex Cover Problem is NP-hard even for unit weights (Theorem 15.24), so
is the Minimum Set Cover Problem.
Johnson [1974] and Lov´asz [1975] proposed a simple greedy algorithm for
the Minimum Set Cover Problem: in each iteration, pick a set which covers a
maximum number of elements not already covered. Chv´atal [1979] generalized
this algorithm to the weighted case:
Greedy Algorithm For Set Cover
Input:
A set system (U, S) with 
S∈S S = U, weights c : S →R+.
Output:
A set cover R of (U, S).
1⃝
Set R := ∅and W := ∅.
2⃝
While W ̸= U do:
Choose a set R ∈S \ R for which
c(R)
|R\W| is minimum.
Set R := R ∪{R} and W := W ∪R.
The running time is obviously O(|U||S|). The following performance guaran-
tee can be proved:
Theorem 16.3.
(Chv´atal [1979])
For any instance (U, S, c) of the Minimum
Weight Set Cover Problem, the Greedy Algorithm For Set Cover ﬁnds a
set cover whose weight is at most H(r) OPT(U, S, c), where r := maxS∈S |S| and
H(r) = 1 + 1
2 + · · · + 1
r .
Proof:
Let (U, S, c) be an instance of the Minimum Weight Set Cover Prob-
lem, and let R = {R1, . . . , Rk} be the solution found by the above algorithm,
where Ri is the set chosen in the i-th iteration. For j = 0, . . . , k let Wj :=  j
i=1 Ri.
For each e ∈U let j(e) := min{ j ∈{1, . . . , k} : e ∈Rj} be the iteration
where e is covered. Let
y(e) :=
c(Rj(e))
|Rj(e) \ Wj(e)−1|.

16.1 Set Covering
379
Let S ∈S be ﬁxed, and let k′ := max{ j(e) : e ∈S}. We have

e∈S
y(e)
=
k′

i=1

e∈S: j(e)=i
y(e)
=
k′

i=1
c(Ri)
|Ri \ Wi−1||S ∩(Wi \ Wi−1)|
=
k′

i=1
c(Ri)
|Ri \ Wi−1|(|S \ Wi−1| −|S \ Wi|)
≤
k′

i=1
c(S)
|S \ Wi−1|(|S \ Wi−1| −|S \ Wi|)
by the choice of the Ri in 2⃝(observe that S \ Wi−1 ̸= ∅for i = 1, . . . , k′). By
writing si := |S \ Wi−1| we get

e∈S
y(e)
≤
c(S)
k′

i=1
si −si+1
si
≤
c(S)
k′

i=1
 1
si
+
1
si −1 + · · · +
1
si+1 + 1

=
c(S)
k′

i=1
(H(si) −H(si+1))
=
c(S)(H(s1) −H(sk′+1))
≤
c(S)H(s1).
Since s1 = |S| ≤r, we conclude that

e∈S
y(e) ≤c(S)H(r).
We sum over all S ∈O for an optimum set cover O and obtain
c(O)H(r)
≥

S∈O

e∈S
y(e)
≥

e∈U
y(e)
=
k

i=1

e∈U: j(e)=i
y(e)
=
k

i=1
c(Ri) = c(R).
2

380
16. Approximation Algorithms
For a slightly tighter analysis of the non-weighted case, see Slav´ık [1997].
Raz and Safra [1997] discovered that there exists a constant c > 0 such that,
unless P = NP, no approximation ratio of c ln |U| can be achieved. Indeed, an
approximation ratio of c ln |U| cannot be achieved for any c < 1 unless each
problem in NP can be solved in O

nO(log log n)
time (Feige [1998]).
The Minimum Weight Edge Cover Problem is obviously a special case of
the Minimum Weight Set Cover Problem. Here we have r = 2 in Theorem 16.3,
hence the above algorithm is a 3
2-factor approximation algorithm in this special
case. However, the problem can also be solved optimally in polynomial time; cf.
Exercise 11 of Chapter 11.
For the Minimum Vertex Cover Problem, the above algorithm reads as
follows:
Greedy Algorithm For Vertex Cover
Input:
A graph G.
Output:
A vertex cover R of G.
1⃝
Set R := ∅.
2⃝
While E(G) ̸= ∅do:
Choose a vertex v ∈V (G) \ R with maximum degree.
Set R := R ∪{v} and delete all edges incident to v.
This algorithm looks reasonable, so one might ask for which k it is a k-factor
approximation algorithm. It may be surprising that there is no such k. Indeed, the
bound given in Theorem 16.3 is almost best possible:
Theorem 16.4.
(Johnson [1974], Papadimitriou and Steiglitz [1982])
For all
n ≥3 there is an instance G of the Minimum Vertex Cover Problem such that
nH(n −1) + 2 ≤|V (G)| ≤nH(n −1) + n, the maximum degree of G is n −1,
OPT(G) = n, and the above algorithm can ﬁnd a vertex cover containing all but
n vertices.
Proof:
For each n ≥3 and i ≤n we deﬁne Ai
n := i
j=2

n
j

and
V (Gn)
:=
5
a1, . . . , aAn−1
n , b1, . . . , bn, c1, . . . , cn
6
.
E(Gn)
:=
{{bi, ci} : i = 1, . . . , n} ∪
n−1

i=2
Ai
n

j=Ai−1
n
+1
5
{aj, bk} : ( j −Ai−1
n
−1)i + 1 ≤k ≤( j −Ai−1
n
)i
6
.
Observe that |V (Gn)| = 2n + An−1
n
, An−1
n
≤nH(n −1) −n and An−1
n
≥nH(n −
1) −n −(n −2). Figure 16.1 shows G6.
If we apply our algorithm to Gn, it may ﬁrst choose vertex aAn−1
n
(because
it has maximum degree), and subsequently the vertices aAn−1
n
−1, aAn−1
n
−2, . . . , a1.
After this there are n disjoint edges left, so n more vertices are needed. Hence the

16.1 Set Covering
381
c1
c2
c3
c4
c5
c6
b1
b2
b3
b4
b5
b6
a1
a2
a3
a4
a5
a6
a7
Fig. 16.1.
constructed vertex cover consists of An−1
n
+ n vertices, while the optimum vertex
cover {b1, . . . , bn} has size n.
2
There are, however, 2-factor approximation algorithms for the Minimum Ver-
tex Cover Problem. The simplest one is due to Gavril (see Garey and Johnson
[1979]): just ﬁnd any maximal matching M and take the ends of all edges in M.
This is obviously a vertex cover and contains 2|M| vertices. Since any vertex
cover must contain |M| vertices (no vertex covers two edges of M), this is a
2-factor approximation algorithm.
This performance guarantee is tight: simply think of a graph consisting of
many disjoint edges. It may be surprising that the above is the best known ap-
proximation algorithm for the Minimum Vertex Cover Problem. Later we shall
show that – unless P = NP – there is a number k > 1 such that no k-factor
approximation algorithm exists unless P = NP (Theorem 16.39). Indeed, a 1.36-
factor approximation algorithm does not exist unless P = NP (Dinur and Safra
[2002]).
At least Gavril’s algorithm can be extended to the weighted case. We present
the algorithm of Bar-Yehuda and Even [1981], which is applicable to the general
Minimum Weight Set Cover Problem:
Bar-Yehuda-Even Algorithm
Input:
A set system (U, S) with 
S∈S S = U, weights c : S →R+.
Output:
A set cover R of (U, S).
1⃝
Set R := ∅and W := ∅. Set y(e) := 0 for all e ∈U.
Set c′(S) := c(S) for all S ∈S.

382
16. Approximation Algorithms
2⃝
While W ̸= U do:
Choose an element e ∈U \ W.
Let R ∈S with e ∈R and c′(R) minimum. Set y(e) := c′(R).
Set c′(S) := c′(S) −y(e) for all S ∈S with e ∈S.
Set R := R ∪{R} and W := W ∪R.
Theorem 16.5.
(Bar-Yehuda and Even [1981])
For any instance (U, S, c) of
the Minimum Weight Set Cover Problem, the Bar-Yehuda-Even Algorithm
ﬁnds a set cover whose weight is at most p OPT(U, S, c), where p := maxe∈U |{S ∈
S : e ∈S}|.
Proof:
The Minimum Weight Set Cover Problem can be written as the integer
linear program
min
5
cx : Ax ≥1l, x ∈{0, 1}S6
,
where A is the matrix whose rows correspond to the elements of U and whose
columns are the incidence vectors of the sets in S. The optimum of the LP relax-
ation
min {cx : Ax ≥1l, x ≥0}
is a lower bound for OPT(U, S, c) (the omission of the constraints x ≤1l does not
change the optimum value of this LP). Hence, by Proposition 3.12, the optimum
of the dual LP
max{y1l : y A ≤c, y ≥0}
is also a lower bound for OPT(U, S, c).
Now observe that c′(S) ≥0 for all S ∈S at any stage of the algorithm. Hence
y ≥0 and 
e∈S y(e) ≤c(S) for all S ∈S, i.e. y is a feasible solution of the dual
LP and
y1l ≤max{y1l : y A ≤c, y ≥0} ≤OPT(U, S, c).
Finally observe that
c(R)
=

R∈R
c(R)
=

R∈R

e∈R
y(e)
≤

e∈U
py(e)
=
py1l
≤
p OPT(U, S, c).
2
Since we have p = 2 in the vertex cover case, this is a 2-factor approximation
algorithm for the Minimum Weight Vertex Cover Problem. The ﬁrst 2-factor
approximation algorithm was due to Hochbaum [1982]. She proposed ﬁnding an
optimum solution y of the dual LP in the above proof and taking all sets S with

16.2 Colouring
383

e∈S y(e) = c(S). The advantage of the Bar-Yehuda-Even Algorithm is that
it does not explicitly use linear programming. In fact it can easily be implemented
with O

S∈S |S|

time.
16.2 Colouring
In this section we brieﬂy discuss two more well-known special cases of the Min-
imum Set Cover Problem: We want to partition the vertex set of a graph into
stable sets, or the edge set of a graph into matchings:
Deﬁnition 16.6.
Let G be an undirected graph. A vertex-colouring of G is a
mapping f : V (G) →N with f (v) ̸= f (w) for all {v, w} ∈E(G). An edge-
colouring of G is a mapping f : E(G) →N with f (e) ̸= f (e′) for all e, e′ ∈E(G)
with e ̸= e′ and e ∩e′ ̸= ∅.
The number f (v) or f (e) is called the colour of v or e. In other words, the
set of vertices or edges with the same colour ( f -value) must be a stable set, or
a matching, respectively. Of course we are interested in using as few colours as
possible:
Vertex-Colouring Problem
Instance:
An undirected graph G.
Task:
Find a vertex-colouring f : V (G) →{1, . . . , k} of G with minimum
k.
Edge-Colouring Problem
Instance:
An undirected graph G.
Task:
Find an edge-colouring f : E(G) →{1, . . . , k} of G with minimum
k.
Reducing these problems to the Minimum Set Cover Problem is not very
useful: for the Vertex-Colouring Problem we would have to list the maximal
stable sets (an NP-hard problem), while for the Edge-Colouring Problem we
would have to reckon with exponentially many maximal matchings.
The optimum value of the Vertex-Colouring Problem (i.e. the minimum
number of colours) is called the chromatic number of the graph. The optimum
value of the Edge-Colouring Problem is called the edge-chromatic number
or sometimes the chromatic index. Both colouring problems are NP-hard:
Theorem 16.7.
The following decision problems are NP-complete:
(a) (Holyer [1981])
Decide whether a given simple graph has edge-chromatic
number 3.

384
16. Approximation Algorithms
(b) (Stockmeyer [1973])
Decide whether a given planar graph has chromatic
number 3.
The problems remain NP-hard even when the graph has maximum degree three
in (a), and maximum degree four in (b).
Proposition 16.8.
For any given graph we can decide in linear time whether the
chromatic number, or the edge-chromatic number, is less than 3, and if so, ﬁnd an
optimum colouring.
Proof:
A graph has chromatic number 1 iff it has no edges. By deﬁnition, the
graphs with chromatic number at most 2 are precisely the bipartite graphs. By
Proposition 2.27 we can check in linear time whether a graph is bipartite and in
the positive case ﬁnd a bipartition, i.e. a vertex-colouring with two colours.
To check whether the edge-chromatic number of a graph G is less than 3
(and, if so, ﬁnd an optimum edge-colouring) we simply consider the Vertex-
Colouring Problem in the line graph of G. This is obviously equivalent.
2
For bipartite graphs, the Edge-Colouring Problem can be solved, too:
Theorem 16.9.
(K¨onig [1916]) The edge-chromatic number of a bipartite graph
G equals the maximum degree of a vertex in G.
Proof:
By induction on |E(G)|. Let G be a graph with maximum degree k,
and let e = {v, w} be an edge. By the induction hypothesis, G −e has an edge-
colouring f with k colours. There are colours i, j ∈{1, . . . , k} such that f (e′) ̸= i
for all e′ ∈δ(v) and f (e′) ̸= j for all e′ ∈δ(w). If i = j, we are done since we
can extend f to G by giving e colour i.
The graph H = (V (G), {e′ ∈E(G) \ e : f (e′) ∈{i, j}}) has maximum degree
2, and v has degree at most 1 in H. Consider the maximal path P in H with
endpoint v. The colours alternate on P; hence the other endpoint of P cannot be
w. Exchange the colours i and j on P and extend the edge-colouring to G by
giving e colour j.
2
The maximum degree of a vertex is an obvious lower bound on the edge-
chromatic number of any graph. It is not always attained as the triangle K3 shows.
The following theorem shows how to ﬁnd an edge-colouring of a given simple
graph which needs at most one more colour than necessary:
Theorem 16.10.
(Vizing [1964]) Let G be an undirected simple graph with max-
imum degree k. Then G has an edge-colouring with at most k +1 colours, and such
a colouring can be found in polynomial time.
Proof:
By induction on |E(G)|. If G has no edges, the assertion is trivial. Oth-
erwise let e = {x, y0} be any edge; by the induction hypothesis there exists an
edge-colouring f of G −e with k + 1 colours. For each vertex v choose a colour
n(v) ∈{1, . . . , k + 1} \ { f (w) : w ∈δG−e(v)} missing at v.

16.2 Colouring
385
Starting from y0, construct a maximal sequence y0, y1, . . . , yt of distinct neigh-
bours of x such that n(yi−1) = f ({x, yi}) for i = 1, . . . , t.
If no edge incident to x is coloured n(yt), then we construct an edge-colouring
f ′ of G from f by setting f ′({x, yi−1}) :=
f ({x, yi}) (i = 1, . . . , t) and
f ′({x, yt}) := n(yt). So we assume that there is an edge incident to x with
colour n(yt); by the maximality of t we have f ({x, ys}) = n(yt) for some
s ∈{1, . . . , t −1}.
Consider the maximum path P starting at yt in the graph (V (G), {e′ ∈E(G −
e) : f (e′) ∈{n(x), n(yt)}}) (this graph has maximum degree 2). We distinguish
two cases.
If P does not end in ys−1, then we can construct an edge-colouring f ′ of G
from f as follows: exchange colours n(x) and n(yt) on P, set f ′({x, yi−1}) :=
f ({x, yi}) (i = 1, . . . , t) and f ′({x, yt}) := n(x).
If P ends in ys−1, then the last edge of P has colour n(x), since colour n(yt) =
f ({x, ys}) = n(ys−1) is missing at ys−1. We construct an edge-colouring f ′ of G
from f as follows: exchange colours n(x) and n(yt) on P, set f ′({x, yi−1}) :=
f ({x, yi}) (i = 1, . . . , s −1) and f ′({x, ys−1}) := n(x).
2
Vizing’s Theorem implies an absolute approximation algorithm for the Edge-
Colouring Problem in simple graphs. If we allow parallel edges the statement
is no longer true: by replacing each edge of the triangle K3 by r parallel edges
we obtain a 2r-regular graph with edge-chromatic number 3r.
We now turn to the Vertex-Colouring Problem. The maximum degree also
gives an upper bound on the chromatic number:
Theorem 16.11.
Let G be an undirected graph with maximum degree k. Then G
has an vertex-colouring with at most k + 1 colours, and such a colouring can be
found in linear time.
Proof:
The following Greedy Colouring Algorithm obviously ﬁnds such a
colouring.
2
Greedy Colouring Algorithm
Input:
An undirected graph G.
Output:
A vertex-colouring of G.
1⃝
Let V (G) = {v1, . . . , vn}.
2⃝
For i := 1 to n do:
Set f (vi) := min{k ∈N : k ̸= f (vj) for all j < i with vj ∈(vi)}.
For complete graphs and for odd circuits one evidently needs k + 1 colours,
where k is the maximum degree. For all other connected graphs k colours sufﬁce,
as Brooks [1941] showed. However, the maximum degree is not a lower bound on
the chromatic number: any star K1,n (n ∈N) has chromatic number 2. Therefore

386
16. Approximation Algorithms
these results do not lead to an approximation algorithm. In fact, no algorithms for
the Vertex-Colouring Problem with a reasonable performance guarantee for
general graphs are known; see Khanna, Linial and Safra [2000].
Since the maximum degree is not a lower bound for the chromatic number
one can consider the maximum size of a clique. Obviously, if a graph G contains
a clique of size k, then the chromatic number of G is at least k. As the pentagon
(circuit of length ﬁve) shows, the chromatic number can exceed the maximum
clique size. Indeed, there are graphs with arbitrary large chromatic number that
contain no K3. This motivates the following deﬁnition, which is due to Berge
[1961,1962]:
Deﬁnition 16.12.
A graph G is perfect if χ(H) = ω(H) for every induced sub-
graph H of G, where χ(H) is the chromatic number and ω(H) is the maximum
cardinality of a clique in H.
It follows immediately that the decision problem whether a given perfect graph
has chromatic number k has a good characterization (belongs to NP ∩coNP).
Some examples of perfect graphs can be found in Exercise 11. A polynomial-time
algorithm for recognizing perfect graphs has been found by Chudnovsky et al.
[2005].
Berge [1961] conjectured that a graph is perfect if and only if it contains
neither an odd circuit of length at least ﬁve nor the complement of such a circuit
as an induced subgraph. This so-called strong perfect graph theorem has been
proved by Chudnovsky et al. [2002]. Thirty years before, Lov´asz [1972] proved
the weaker assertion that a graph is perfect iff its complement is perfect. This is
known as the weak perfect graph theorem; to prove it we need a lemma:
Lemma 16.13.
Let G be a perfect graph and x ∈V (G). Then the graph G′ :=
(V (G)
.
∪{y}, E(G)
.
∪{{y, v} : v ∈{x} ∪(x)}), resulting from G by adding a
new vertex y which is joined to x and to all neighbours of x, is perfect.
Proof:
By induction on |V (G)|. The case |V (G)| = 1 is trivial since K2 is
perfect. Now let G be a perfect graph with at least two vertices. Let x ∈V (G),
and let G′ arise by adding a new vertex y adjacent to x and all its neighbours.
It sufﬁces to prove that ω(G′) = χ(G′), since for proper subgraphs H of G′
this follows from the induction hypothesis: either H is a subgraph of G and thus
perfect, or it arises from a proper subgraph of G by adding a vertex y as above.
Since we can colour G′ with χ(G) + 1 colours easily, we may assume that
ω(G′) = ω(G). Then x is not contained in any maximum clique of G. Let f be
a vertex-colouring of G with χ(G) colours, and let X := {v ∈V (G) : f (v) =
f (x)}. We have ω(G −X) = χ(G −X) = χ(G) −1 = ω(G) −1 and thus
ω(G −(X \ {x})) = ω(G) −1 (as x does not belong to any maximum clique of
G). Since (X \ {x}) ∪{y} = V (G′) \ V (G −(X \ {x})) is a stable set, we have
χ(G′) = χ(G −(X \ {x})) + 1 = ω(G −(X \ {x})) + 1 = ω(G) = ω(G′). 2

16.2 Colouring
387
Theorem 16.14.
(Lov´asz [1972], Fulkerson [1972], Chv´atal [1975]) For a sim-
ple graph G the following statements are equivalent:
(a) G is perfect.
(b) The complement of G is perfect.
(c) The stable set polytope, i.e. the convex hull of the incidence vectors of the
stable sets of G, is given by:

x ∈RV (G)
+
:

v∈S
xv ≤1 for all cliques S in G

.
(16.1)
Proof:
We prove (a)⇒(c)⇒(b). This sufﬁces, since applying (a)⇒(b) to the
complement of G yields (b)⇒(a).
(a)⇒(c): Evidently the stable set polytope is contained in (16.1). To prove the
other inclusion, let x be a rational vector in the polytope (16.1); we may write
xv = pv
q , where q ∈N and pv ∈Z+ for v ∈V (G). Replace each vertex v by a
clique of size pv; i.e. consider G′ deﬁned by
V (G′)
:=
{(v, i) : v ∈V (G), 1 ≤i ≤pv},
E(G′)
:=
{{(v, i), (v, j)} : v ∈V (G), 1 ≤i < j ≤pv} ∪
{{(v, i), (w, j)} : {v, w} ∈E(G), 1 ≤i ≤pv, 1 ≤j ≤pw}.
Lemma 16.13 implies that G′ is perfect. For an arbitrary clique X′ in G′ let
X := {v ∈V (G) : (v, i) ∈X′ for some i} be its projection to G (also a clique);
we have
|X′| ≤

v∈X
pv = q

v∈X
xv ≤q.
So ω(G′) ≤q. Since G′ is perfect, it thus has a vertex-colouring f with at most
q colours. For v ∈V (G) and i = 1, . . . , q let ai,v := 1 if f ((v, j)) = i for some
j and ai,v := 0 otherwise. Then q
i=1 ai,v = pv for all v ∈V (G) and hence
x =
 pv
q

v∈V (G)
= 1
q
q

i=1
ai
is a convex combination of incidence vectors of stable sets, where ai = (ai,v)v∈V (G).
(c)⇒(b): We show by induction on |V (G)| that if (16.1) is integral then the
complement of G is perfect. Since graphs with less than three vertices are perfect,
let G be a graph with |V (G)| ≥3 where (16.1) is integral.
We have to show that the vertex set of any induced subgraph H of G can be
partitioned into α(H) cliques, where α(H) is the size of a maximum stable set in
H. For proper subgraphs H this follows from the induction hypothesis, since (by
Theorem 5.12) every face of the integral polytope (16.1) is integral, in particular
the face deﬁned by the supporting hyperplanes xv = 0 (v ∈V (G) \ V (H)).
So it remains to prove that V (G) can be partitioned into α(G) cliques. The
equation 1lx = α(G) deﬁnes a supporting hyperplane of (16.1), so

388
16. Approximation Algorithms
⎧
⎨
⎩x ∈RV (G)
+
:

v∈S
xv ≤1 for all cliques S in G,

v∈V (G)
xv = α(G)
⎫
⎬
⎭
(16.2)
is a face of (16.1). This face is contained in some facets, which cannot all be of
the form {x ∈(16.1) : xv = 0} for some v (otherwise the origin would belong
to the intersection). Hence there is some clique S in G such that 
v∈S xv = 1
for all x in (16.2). Hence this clique S intersects each maximum stable set of G.
Now by the induction hypothesis, the vertex set of G −S can partitioned into
α(G −S) = α(G) −1 cliques. Adding S concludes the proof.
2
This proof is due to Lov´asz [1979]. Indeed, the inequality system deﬁning
(16.1) is TDI for perfect graphs (Exercise 12). With some more work one can prove
that for perfect graphs the Vertex-Colouring Problem, the Maximum Weight
Stable Set Problem and the Maximum Weight Clique Problem can be solved
in strongly polynomial time. Although these problems are all NP-hard for general
graphs (Theorem 15.23, Corollary 15.24, Theorem 16.7(b)), there is a number (the
so-called theta-function of the complement graph, introduced by Lov´asz [1979])
which is always between the maximum clique size and the chromatic number,
and which can be computed in polynomial time for general graphs using the
Ellipsoid Method. The details are a bit involved; see Gr¨otschel, Lov´asz and
Schrijver [1988].
One of the best known problems in graph theory has been the four colour
problem: is it true that every planar map can be coloured with four colours such
that no two countries with a common border have the same colour? If we consider
the countries as regions and switch to the planar dual graph, this is equivalent to
asking whether every planar graph has a vertex-colouring with four colours. Appel
and Haken [1977] and Appel, Haken and Koch [1977] proved that this is indeed
true: every planar graph has chromatic number at most 4. For a simpler proof
of the Four Colour Theorem (which nevertheless is based on a case checking by
a computer) see Robertson et al. [1997]. We prove the following weaker result,
known as the Five Colour Theorem:
Theorem 16.15.
(Heawood [1890])
Any planar graph has a vertex-colouring
with at most ﬁve colours, and such a colouring can be found in polynomial time.
Proof:
By induction on |V (G)|. We may assume that G is simple, and we ﬁx
an arbitrary planar embedding 	 =

ψ, (Je)e∈E(G)

of G. By Corollary 2.33, G
has a vertex v of degree ﬁve or less. By the induction hypothesis, G −v has
a vertex-colouring f with at most 5 colours. We may assume that v has degree
5 and all neighbours have different colours; otherwise we can easily extend the
colouring to G.
Let w1, w2, w3, w4, w5 be the neighbours of v in the cyclic order in which the
polygonal arcs J{v,wi} leave v.
We ﬁrst claim that there are no vertex-disjoint paths P from w1 to w3 and
Q from w2 to w4 in G −v. To prove this, let P be a w1-w3-path, and let C be

16.2 Colouring
389
the circuit in G consisting of P and the edges {v, w1}, {v, w3}. By Theorem 2.30
R2 \ 
e∈E(C) Je splits into two connected regions, and v is on the boundary of
both regions. Hence w2 and w4 belong to different regions of that set, implying
that every w2-w4-path in G must contain a vertex of C.
Let X be the connected component of the graph G[{x ∈V (G) \ {v} : f (x) ∈
{ f (w1), f (w3)}}] which contains w1. If X does not contain w3, we can exchange
the colours in X and afterwards extend the colouring to G by colouring v with
the old colour of w1. So we may assume that there is a w1-w3-path P containing
only vertices coloured with f (w1) or f (w3).
Analogously, we are done if there is no w2-w4-path Q containing only vertices
coloured with f (w2) or f (w4). But the contrary assumption means that there are
vertex-disjoint paths P from w1 to w3 and Q from w2 to w4, a contradiction. 2
Hence this is a second NP-hard problem which has an absolute approximation
algorithm. Indeed, the Four Colour Theorem implies that the chromatic number
of a non-bipartite planar graph can only be 3 or 4. Using the polynomial-time
algorithm of Robertson et al. [1996], which colours any given planar graph with
four colours, one obtains an absolute approximation algorithm which uses at most
one colour more than necessary.
F¨urer and Raghavachari [1994] detected a third natural problem which can
be approximated up to an absolute constant of one: Given an undirected graph,
they look for a spanning tree whose maximum degree is minimum among all the
spanning trees (the problem is a generalization of the HamiltonianPathProblem
and thus NP-hard). Their algorithm also extends to a general case corresponding
to the Steiner Tree Problem: Given a set T ⊆V (G), ﬁnd a tree S in G with
V (T ) ⊆V (S) such that the maximum degree of S is minimum.
On the other hand, the following theorem tells that many problems do not have
absolute approximation algorithms unless P = NP:
Proposition 16.16.
Let F and F′ be (inﬁnite) families of ﬁnite sets, and let P be
the following optimization problem: Given a set E ∈F and a function c : E →Z,
ﬁnd a set F ⊆E with F ∈F′ and c(F) minimum (or decide that no such F exists).
Then P has an absolute approximation algorithm if and only if P can be solved
in polynomial time.
Proof:
Suppose there is a polynomial-time algorithm A and an integer k such
that
|A((E, c)) −OPT((E, c))| ≤k
for all instances (E, c) of P. We show how to solve P exactly in polynomial time.
Given an instance (E, c) of P, we construct a new instance (E, c′), where
c′(e) := (k + 1)c(e) for all e ∈E. Obviously the optimum solutions remain the
same. But if we now apply A to the new instance,
|A((E, c′)) −OPT((E, c′))| ≤k
and thus A((E, c′)) = OPT((E, c′)).
2

390
16. Approximation Algorithms
Examples are the Minimization Problem For Independence Systems and
the Maximization Problem For Independence Systems (multiply c by −1),
and thus all problems in the list of Section 13.1.
16.3 Approximation Schemes
Recall the absolute approximation algorithm for the Edge-Colouring Problem
discussed in the previous section. This also implies a relative performance guaran-
tee: Since one can easily decide if the edge-chromatic number is 1 or 2 (Proposition
16.8), Vizing’s Theorem yields a 4
3-factor approximation algorithm. On the other
hand, Theorem 16.7(a) implies that no k-factor approximation algorithm exists for
any k < 4
3 (unless P = NP).
Hence the existence of an absolute approximation algorithm does not imply
the existence of a k-factor approximation algorithm for all k > 1. We shall meet a
similar situation with the Bin-Packing Problem in Chapter 18. This consideration
suggests the following deﬁnition:
Deﬁnition 16.17.
Let P be an optimization problem with nonnegative weights.
An asymptotic k-factor approximation algorithm for P is a polynomial-time
algorithm A for P for which there exists a constant c such that
1
k OPT(I) −c ≤A(I) ≤k OPT(I) + c
for all instances I of P. We also say that A has asymptotic performance ratio k.
The (asymptotic) approximation ratio of an optimization problem P with
nonnegative weights is deﬁned to be the inﬁmum of all numbers k for which there
exists an (asymptotic) k-factor approximation algorithm for P, or ∞if there is no
(asymptotic) approximation algorithm at all.
For example, the above-mentioned Edge-Colouring Problem has approxi-
mation ratio 4
3 (unless P = NP), but asymptotic approximation ratio 1 (not only
in simple graphs; see Sanders and Steurer [2005]). Optimization problems with
(asymptotic) approximation ratio 1 are of particular interest. For these problems
we introduce the following notion:
Deﬁnition 16.18.
Let P be an optimization problem with nonnegative weights.
An approximation scheme for P is an algorithm A accepting as input an instance
I of P and an ϵ > 0 such that, for each ﬁxed ϵ, A is a (1+ϵ)-factor approximation
algorithm for P.
An asymptotic approximation scheme for P is a pair of algorithms (A, A′)
with the following properties: A′ is a polynomial-time algorithm accepting a num-
ber ϵ > 0 as input and computing a number cϵ. A accepts an instance I of P and
an ϵ > 0 as input, and its output consists of a feasible solution for I satisfying
1
1 + ϵ OPT(I) −cϵ ≤A(I, ϵ) ≤(1 + ϵ) OPT(I) + cϵ.

16.3 Approximation Schemes
391
For each ﬁxed ϵ, the running time of A is polynomially bounded in size(I).
An (asymptotic) approximation scheme is called a fully polynomial (asymp-
totic) approximation scheme if the running time as well as the maximum size
of any number occurring in the computation is bounded by a polynomial in
size(I) + size(ϵ) + 1
ϵ .
In some other texts one ﬁnds the abbreviations PTAS for (polynomial-time)
approximation scheme and FPAS for fully polynomial approximation scheme.
Apart from absolute approximation algorithms, a fully polynomial approxima-
tion scheme can be considered the best we may hope for when faced with an
NP-hard optimization problem, at least if the cost of any feasible solution is a
nonnegative integer (which can be assumed in many cases without loss of gener-
ality):
Proposition 16.19.
Let P = (X, (Sx)x∈X, c, goal) be an optimization problem
where the values of c are nonnegative integers. Let A be an algorithm which, given
an instance I of P and a number ϵ > 0, computes a feasible solution of I with
1
1 + ϵ OPT(I) ≤A(I, ϵ) ≤(1 + ϵ) OPT(I)
and whose running time is bounded by a polynomial in size(I) + size(ϵ). Then P
can be solved exactly in polynomial time.
Proof:
Given an instance I, we ﬁrst run A on (I, 1). We set ϵ :=
1
1+2A(I,1) and
observe that ϵ OPT(I) < 1. Now we run A on (I, ϵ). Since size(ϵ) is polynomially
bounded in size(I), this procedure constitutes a polynomial-time algorithm. If P
is a minimization problem, we have
A(I, ϵ) ≤(1 + ϵ) OPT(I) < OPT(I) + 1,
which, since c is integral, implies optimality. Similarly, if P is a maximization
problem, we have
A(I, ϵ) ≥
1
1 + ϵ OPT(I) > (1 −ϵ) OPT(I) > OPT(I) −1.
2
Unfortunately, a fully polynomial approximation scheme exists only for very
few problems (see Theorem 17.11). Moreover we note that even the existence of a
fully polynomial approximation scheme does not imply an absolute approximation
algorithm; the Knapsack Problem is an example.
In Chapters 17 and 18 we shall discuss two problems (Knapsack and Bin-
Packing) which have a fully polynomial approximation scheme and a fully poly-
nomial asymptotic approximation scheme, respectively. For many problems the
two types of approximation schemes coincide:

392
16. Approximation Algorithms
Theorem 16.20.
(Papadimitriou and Yannakakis [1993]) Let P be an optimiza-
tion problem with nonnegative weights. Suppose that for each constant k there is
a polynomial-time algorithm which decides whether a given instance has optimum
value at most k, and, if so, ﬁnds an optimum solution.
Then P has an approximation scheme if and only if P has an asymptotic ap-
proximation scheme.
Proof:
The only-if-part is trivial, so suppose that P has an asymptotic approxi-
mation scheme (A, A′). We describe an approximation scheme for P.
Let a ﬁxed ϵ > 0 be given; we may assume ϵ < 1. We set ϵ′ :=
ϵ−ϵ2
2+ϵ+ϵ2 < ϵ
2
and ﬁrst run A′ on the input ϵ′, yielding a constant cϵ′.
For a given instance I we next test whether OPT(I) is at most 2cϵ′
ϵ . This is a
constant for each ﬁxed ϵ, so we can decide this in polynomial time and ﬁnd an
optimum solution if OPT(I) ≤2cϵ′
ϵ .
Otherwise we apply A to I and ϵ′ and obtain a solution of value V , with
1
1 + ϵ′ OPT(I) −cϵ′ ≤V ≤(1 + ϵ′) OPT(I) + cϵ′.
We claim that this solution is good enough. Indeed, we have cϵ′ < ϵ
2 OPT(I)
which implies
V ≤(1+ϵ′) OPT(I)+cϵ′ <

1 + ϵ
2

OPT(I)+ ϵ
2 OPT(I) = (1+ϵ) OPT(I)
and
V
≥
1
(1 + ϵ′) OPT(I) −ϵ
2 OPT(I)
=
2 + ϵ + ϵ2
2 + 2ϵ
OPT(I) −ϵ
2 OPT(I)
=

1
1 + ϵ + ϵ
2

OPT(I) −ϵ
2 OPT(I)
=
1
1 + ϵ OPT(I).
2
So the deﬁnition of an asymptotic approximation scheme is meaningful only
for problems (such as bin-packing or colouring problems) whose restriction to a
constant optimum value is still difﬁcult. For many problems this restriction can
be solved in polynomial time by some kind of complete enumeration.
16.4 Maximum Satisﬁability
The Satisfiability Problem was our ﬁrst NP-complete problem. In this section
we analyse the corresponding optimization problem:

16.4 Maximum Satisﬁability
393
Maximum Satisfiability (Max-Sat)
Instance:
A set X of variables, a family Z of clauses over X, and a weight
function c : Z →R+.
Task:
Find a truth assignment T of X such that the total weight of the
clauses in Z that are satisﬁed by T is maximum.
As we shall see, approximating Max-Sat is a nice example (and historically
one of the ﬁrst) for the algorithmic use of the probabilistic method.
Let us ﬁrst consider the following trivial randomized algorithm: set each vari-
able independently true with probability 1
2. Obviously this algorithm satisﬁes each
clause Z with probability 1 −2−|Z|.
Let us write r for the random variable which is true with probability 1
2 and false
otherwise, and let R = (r,r, . . . ,r) be the random variable uniformly distributed
over all truth assignments. If we write c(T ) for the total weight of the clauses
satisﬁed by the truth assignment T , the expected total weight of the clauses satisﬁed
by R is
Exp (c(R))
=

Z∈Z
c(Z) Prob(R satisﬁes Z)
=

Z∈Z
c(Z)

1 −2−|Z|
(16.3)
≥

1 −2−p 
Z∈Z
c(Z),
where p := minZ∈Z |Z|; Exp and Prob denote expectation and probability.
Since the optimum cannot exceed 
Z∈Z c(Z), R is expected to yield a solu-
tion within a factor
1
1−2−p of the optimum. But what we would really like to have is
a deterministic approximation algorithm. In fact, we can turn our (trivial) random-
ized algorithm into a deterministic algorithm while preserving the performance
guarantee. This step is often called derandomization.
Let us ﬁx the truth assignment step by step. Suppose X = {x1, . . . , xn}, and we
have already ﬁxed a truth assignment T for x1, . . . , xk (0 ≤k < n). If we now set
xk+1, . . . , xn randomly, setting each variable independently true with probability 1
2,
we will satisfy clauses of expected total weight e0 = c(T (x1), . . . , T (xk),r, . . . ,r).
If we set xk+1 true (false), and then set xk+2, . . . , xn randomly, the satisﬁed clauses
will have some expected total weight e1 (e2, respectively). e1 and e2 can be thought
of as conditional expectations. Trivially e0 = e1+e2
2
, so at least one of e1, e2 must
be at least e0. We set xk+1 to be true if e1 ≥e2 and false otherwise. This is
sometimes called the method of conditional probabilities.
Johnson’s Algorithm For Max-Sat
Input:
A set X = {x1, . . . , xn} of variables, a family Z of clauses over X,
and a weight function c : Z →R+.
Output:
A truth assignment T : X →{true, false}.

394
16. Approximation Algorithms
1⃝
For k := 1 to n do:
If Exp(c(T (x1), . . . , T (xk−1), true,r, . . . ,r))
≥Exp(c(T (x1), . . . , T (xk−1), false,r, . . . ,r))
then set T (xk) := true
else set T (xk) := false.
The expectations can be easily computed with (16.3).
Theorem 16.21.
(Johnson [1974]) Johnson’s Algorithm For Max-Sat is a
1
1−2−p -factor approximation algorithm for Max-Sat, where p is the minimum car-
dinality of a clause.
Proof:
Let us deﬁne the conditional expectation
sk := Exp(c(T (x1), . . . , T (xk),r, . . . ,r))
for k = 0, . . . , n. Observe that sn = c(T ) is the total weight of the clauses satisﬁed
by our algorithm, while s0 = Exp(c(R)) ≥

1 −2−p 
Z∈Z c(Z) by (16.3).
Furthermore, si ≥si−1 by the choice of T (xi) in 1⃝(for i = 1, . . . , n). So
sn ≥s0 ≥

1 −2−p 
Z∈Z c(Z). Since the optimum is at most 
Z∈Z c(Z), the
proof is complete.
2
Since p ≥1, we have a 2-factor approximation algorithm. However, this is not
too interesting as there is a much simpler 2-factor approximation algorithm: either
set all variables true or all false, whichever is better. However, Chen, Friesen
and Zheng [1999] showed that Johnson’s Algorithm For Max-Sat is indeed a
3
2-factor approximation algorithm.
If there are no one-element clauses (p ≥2), it is a 4
3-factor approximation
algorithm (by Theorem 16.21), for p ≥3 it is a 8
7-factor approximation algorithm.
Yannakakis [1994] found a 4
3-factor approximation algorithm for the general
case using network ﬂow techniques. We shall describe a more recent and simpler
4
3-factor approximation algorithm due to Goemans and Williamson [1994].
It is straightforward to translate Max-Sat into an integer linear program: If
we have variables X = {x1, . . . , xn}, clauses Z = {Z1, . . . , Zm}, and weights
c1, . . . , cm, we can write
max
m

j=1
cjzj
s.t.
zj
≤

i:xi∈Zj
yi +

i:xi∈Zj
(1 −yi)
( j = 1, . . . , m)
yi, zj
∈
{0, 1}
(i = 1, . . . , n, j = 1, . . . , m).
Here yi = 1 means that variable xi is true, and zj = 1 means that clause Zj is
satisﬁed. Now consider the LP relaxation:

16.4 Maximum Satisﬁability
395
max
m

j=1
cjzj
s.t.
zj
≤

i:xi∈Zj
yi +

i:xi∈Zj
(1 −yi)
( j = 1, . . . , m)
yi
≤
1
(i = 1, . . . , n)
yi
≥
0
(i = 1, . . . , n)
zj
≤
1
( j = 1, . . . , m)
zj
≥
0
( j = 1, . . . , m).
(16.4)
Let (y∗, z∗) be an optimum solution of (16.4). Now independently set each vari-
able xi true with probability y∗
i . This step is known as randomized rounding, a
technique which has been introduced by Raghavan and Thompson [1987]. The
above method constitutes another randomized algorithm for Max-Sat, which can
be derandomized as above. Let rp be the random variable which is true with
probability p and false otherwise.
Goemans-Williamson Algorithm For Max-Sat
Input:
A set X = {x1, . . . , xn} of variables, a family Z of clauses over X,
and a weight function c : Z →R+.
Output:
A truth assignment T : X →{true, false}.
1⃝
Solve the linear program (16.4); let (y∗, z∗) be an optimum solution.
2⃝
For k := 1 to n do:
If Exp(c(T (x1), . . . , T (xk−1), true,ry∗
k+1, . . . ,ry∗n )
≥Exp(c(T (x1), . . . , T (xk−1), false,ry∗
k+1, . . . ,ry∗n )
then set T (xk) := true
else set T (xk) := false.
Theorem 16.22.
(Goemans and Williamson [1994])
The Goemans-William-
son Algorithm For Max-Sat is a
1
1−

1−1
q
q -factor approximation algorithm,
where q is the maximum cardinality of a clause.
Proof:
Let us write
sk := Exp(c(T (x1), . . . , T (xk),ry∗
k+1, . . . ,ry∗n ))
for k = 0, . . . , n. We again have si ≥si−1 for i = 1, . . . , n and sn = c(T ) is
the total weight of clauses satisﬁed by our algorithm. So it remains to estimate
s0 = Exp(c(Ry∗)), where Ry∗= (ry∗
1 , . . . ,ry∗n ).
For j = 1, . . . , m, the probability that the clause Zj is satisﬁed by Ry∗is
1 −
⎛
⎝'
i:xi∈Zj
(1 −y∗
i )
⎞
⎠·
⎛
⎝'
i:xi∈Zj
y∗
i
⎞
⎠.

396
16. Approximation Algorithms
Since the geometrical mean is always less than or equal to the arithmetical
mean, this probability is at least
1 −
⎛
⎝1
|Zj|
⎛
⎝
i:xi∈Zj
(1 −y∗
i ) +

i:xi∈Zj
y∗
i
⎞
⎠
⎞
⎠
|Zj|
=
1 −
⎛
⎝1 −
1
|Zj|
⎛
⎝
i:xi∈Zj
y∗
i +

i:xi∈Zj
(1 −y∗
i )
⎞
⎠
⎞
⎠
|Zj|
≥
1 −

1 −
z∗
j
|Zj|
|Zj|
≥

1 −

1 −
1
|Zj|
|Zj|
z∗
j .
To prove the last inequality, observe that for any 0 ≤a ≤1 and any k ∈N
1 −

1 −a
k
k
≥a

1 −

1 −1
k
k
holds: both sides of the inequality are equal for a ∈{0, 1}, and the left-hand side
(as a function of a) is concave, while the right-hand side is linear.
So we have
s0 = Exp(c(Ry∗))
=
m

j=1
cj Prob(Ry∗satisﬁes Zj)
≥
m

j=1
cj

1 −

1 −
1
|Zj|
|Zj|
z∗
j
≥

1 −

1 −1
q
q
m

j=1
cjz∗
j
(observe that the sequence

1 −1
k
k
k∈N is monotonously increasing and con-
verges to 1
e). Since the optimum is less than or equal to m
j=1 z∗
j cj, the optimum
value of the LP relaxation, the proof is complete.
2
Since

1 −1
q
q
< 1
e, we have an
e
e−1-factor approximation algorithm (
e
e−1 is
about 1.582).
We now have two similar algorithms that behave differently: the ﬁrst one is
better for long clauses, while the second is better for short clauses. Hence it is
natural to combine them:

16.5 The PCP Theorem
397
Theorem 16.23.
(Goemans and Williamson [1994]) The following is a 4
3-factor
approximation algorithm for Max-Sat: run both Johnson’s Algorithm For
Max-Sat and the Goemans-Williamson Algorithm For Max-Sat and choose
the better of the two solutions.
Proof:
We use the notation of the above proofs. The algorithm returns a truth
assignment satisfying clauses of total weight at least
max{Exp(c(R)), Exp(c(Ry∗))}
≥
1
2

Exp(c(R)) + Exp(c(Ry∗))

≥
1
2
m

j=1


1 −2−|Zj|
cj +

1 −

1 −
1
|Zj|
|Zj|
z∗
j cj

≥
1
2
m

j=1

2 −2−|Zj| −

1 −
1
|Zj|
|Zj|
z∗
j cj
≥
3
4
m

j=1
z∗
j cj.
For the last inequality observe that 2 −2−k −

1 −1
k
k ≥3
2 for all k ∈N: for
k ∈{1, 2} we have equality; for k ≥3 we have 2−2−k −

1 −1
k
k ≥2−1
8 −1
e > 3
2.
Since the optimum is at least m
j=1 z∗
j cj, the theorem is proved.
2
Slightly better approximation algorithms for Max-Sat (using semideﬁnite pro-
gramming) have been found; see Goemans and Williamson [1995], Mahajan and
Ramesh [1999], and Feige and Goemans [1995]. The currently best known algo-
rithm achieves an approximation ratio of 1.275 (Asano and Williamson [2002]).
Indeed, Bellare and Sudan [1994] showed that approximating Max-Sat to
within a factor of 74
73 is NP-hard. Even for Max-3Sat (which is Max-Sat restricted
to instances where each clause has exactly three literals) no approximation scheme
exists (unless P = NP), as we shall show in the next section.
16.5 The PCP Theorem
Many non-approximability results are based on a deep theorem which gives a
new characterization of the class NP. Recall that a decision problem belongs to
NP if and only if there is a polynomial-time certiﬁcate-checking algorithm. Now
we consider randomized certiﬁcate-checking algorithms that read the complete
instance but only a small part of the certiﬁcate to be checked. They always accept
yes-instances with correct certiﬁcates but sometimes also accept no-instances.
Which bits of the certiﬁcate are read is decided randomly in advance; more
precisely this decision depends on the instance x and on O(log(size(x))) random
bits.

398
16. Approximation Algorithms
We now formalize this concept. If s is a string and t ∈Nk, then st denotes the
string of length k whose i-th component is the ti-th component of s (i = 1, . . . , k).
Deﬁnition 16.24.
A
decision
problem
(X, Y)
belongs
to
the
class
PCP(log n,1) if there is a polynomial p and a constant k ∈N, a function
f :
5
(x,r) : x ∈X, r ∈{0, 1}⌊log(p(size(x)))⌋6
→Nk
computable in polynomial time, with f (x,r) ∈{1, . . . , ⌊p(size(x))⌋}k for all x
and r, and a decision problem (X′, Y ′) in P, where X′ := {(x, π, γ ) : x ∈X, π ∈
{1, . . . , ⌊p(size(x))⌋}k, γ ∈{0, 1}k}, such that for any instance x ∈X:
If x ∈Y then there exists a c ∈{0, 1}⌊p(size(x))⌋with Prob

(x, f (x,r), cf (x,r)) ∈Y ′
= 1. If x /∈Y then Prob

(x, f (x,r), cf (x,r)) ∈Y ′
< 1
2 for all c ∈{0, 1}⌊p(size(x))⌋.
Here the probability is taken over the uniform distribution of random strings
r ∈{0, 1}⌊log(p(size(x)))⌋.
The letters “PCP” stand for “probabilistically checkable proof”. The parame-
ters log n and 1 reﬂect that, for an instance of size n, O(log n) random bits are
used and O(1) bits of the certiﬁcate are read.
For any yes-instance there is a certiﬁcate which is always accepted; while for
no-instances there is no string which is accepted as a certiﬁcate with probability
1
2 or more. Note that this error probability 1
2 can be replaced equivalently by any
number between zero and one (Exercise 15).
Proposition 16.25.
PCP(log n, 1) ⊆NP.
Proof:
Let (X, Y) ∈PCP(log n, 1), and let p, k, f, (X′, Y ′) be given as in Def-
inition 16.24. Let X′′ :=
5
(x, c) : x ∈X, c ∈{0, 1}⌊p(size(x))⌋6
, and let
Y ′′ :=
5
(x, c) ∈X′′ : Prob

(x, f (x,r), cf (x,r)) ∈Y ′
= 1
6
.
To show that (X, Y) ∈NP it sufﬁces to show that (X′′, Y ′′) ∈P. But since there are
only 2⌊log(p(size(x)))⌋, i.e. at most p(size(x)) many strings r ∈{0, 1}⌊log(p(size(x)))⌋, we
can try them all. For each one we compute f (x,r) and test whether (x, f (x,r),
cf (x,r)) ∈Y ′ (we use that (X′, Y ′) ∈P). The overall running time is polynomial
in size(x).
2
Now the surprising result is that these randomized veriﬁers, which read only
a constant number of bits of the certiﬁcate, are as powerful as the standard (de-
terministic) certiﬁcate-checking algorithms which have the full information. This
is the so-called PCP Theorem:
Theorem 16.26.
(Arora et al. [1998])
NP = PCP(log n, 1).

16.5 The PCP Theorem
399
The proof of NP ⊆PCP(log n, 1) is very difﬁcult and beyond the scope of this
book. It is based on earlier (and weaker) results of Feige et al. [1996] and Arora
and Safra [1998]. For a self-contained proof of the PCP Theorem 16.26, see also
(Arora [1994]), (Hougardy, Pr¨omel and Steger [1994]) or (Ausiello et al. [1999]).
Stronger results were found subsequently by Bellare, Goldreich and Sudan [1998]
and H˚astad [2001]. For example, the number k in Deﬁnition 16.24 can be chosen
to be 9.
We show some of its consequences for the non-approximability of combina-
torial optimization problems. We start with the Maximum Clique Problem and
the Maximum Stable Set Problem: given an undirected graph G, ﬁnd a clique,
or a stable set, of maximum cardinality in G.
Recall Proposition 2.2 (and Corollary 15.24): The problems of ﬁnding a maxi-
mum clique, a maximum stable set, or a minimum vertex cover are all equivalent.
However, the 2-factor approximation algorithm for the Minimum Vertex Cover
Problem (Section 16.1) does not imply an approximation algorithm for the Max-
imum Stable Set Problem or the Maximum Clique Problem.
Namely, it can happen that the algorithm returns a vertex cover C of size n−2,
while the optimum is n
2 −1 (where n = |V (G)|). The complement V (G)\C is then
a stable set of cardinality 2, but the maximum stable set has cardinality n
2 +1. This
example shows that transferring an algorithm to another problem via a polynomial
transformation does not in general preserve its performance guarantee. We shall
consider a restricted type of transformation in the next section. Here we deduce
a non-approximability result for the Maximum Clique Problem from the PCP
Theorem:
Theorem 16.27.
(Arora and Safra [1998]) Unless P = NP there is no 2-factor
approximation algorithm for the Maximum Clique Problem.
Proof:
Let P = (X, Y) be some NP-complete problem. By the PCP Theorem
16.26, P ∈PCP(log n, 1), so let p, k, f , P′ := (X′, Y ′) be as in Deﬁnition 16.24.
For any given x ∈X we construct a graph Gx as follows. Let
V (Gx) :=
5
(r, a) : r ∈{0, 1}⌊log(p(size(x)))⌋, a ∈{0, 1}k, (x, f (x,r), a) ∈Y ′6
(representing all “accepting runs” of the randomized certiﬁcate checking algo-
rithm). Two vertices (r, a) and (r′, a′) are joined by an edge if ai = a′
j whenever
the i-th component of f (x,r) equals the j-th component of f (x,r′). Since P′ ∈P
and there are only a polynomial number of random strings, Gx can be computed
in polynomial time (and has polynomial size).
If x ∈Y then by deﬁnition there exists a certiﬁcate c ∈{0, 1}⌊p(size(x))⌋such
that (x, f (x,r), cf (x,r)) ∈Y ′ for all r ∈{0, 1}⌊log(p(size(x)))⌋. Hence there is a clique
of size 2⌊log(p(size(x)))⌋in Gx.
On the other hand, if x /∈Y then there is no clique of size 1
22⌊log(p(size(x)))⌋in Gx:
Suppose (r(1), a(1)), . . . , (r(t), a(t)) are the vertices of a clique. Then r(1), . . . ,r(t)
are pairwise different. We set ci := a( j)
k
whenever the k-th component of f (x,r( j))
equals i, and set the remaining components of c (if any) arbitrarily. This way we

400
16. Approximation Algorithms
obtain a certiﬁcate c with (x, f (x,r(i)), cf (x,r(i))) ∈Y ′ for all i = 1, . . . , t. If x /∈Y
we have t < 1
22⌊log(p(size(x)))⌋.
So any 2-factor approximation algorithm for the Maximum Clique Problem
is able to decide if x ∈Y, i.e. to solve P. Since P is NP-complete, this is possible
only if P = NP.
2
The reduction in the above proof is due to Feige et al. [1996]. Since the error
probability 1
2 in Deﬁnition 16.24 can be replaced by any number between 0 and
1 (Exercise 15), we get that there is no ρ-factor approximation algorithm for the
Maximum Clique Problem for any ρ ≥1 (unless P = NP).
Indeed, with some more effort one can show that, unless P = NP, there exists
a constant ϵ > 0 such that no polynomial-time algorithm can guarantee to ﬁnd
a clique of size
k
nϵ in a given graph with n vertices which contains a clique of
size k (Feige et al. [1996]; see also H˚astad [1999]). The best known algorithm
guarantees to ﬁnd a clique of size
k log3 n
n(log log n)2 in this case (Feige [2004]). Of course,
all this also holds for the Maximum Stable Set Problem (by considering the
complement of the given graph).
Now we turn to the following restriction of Max-Sat:
Max-3Sat
Instance:
A set X of variables and a family Z of clauses over X, each with
exactly three literals.
Task:
Find a truth assignment T of X such that the number of clauses in
Z that are satisﬁed by T is maximum.
In Section 16.4 we had a simple 8
7-factor approximation algorithm for Max-
3Sat, even for the weighted form (Theorem 16.21). H˚astad [2001] showed that
this is best possible: no ρ-factor approximation algorithm for Max-3Sat can exist
for any ρ < 8
7 unless P = NP. Here we prove the following weaker result:
Theorem 16.28.
(Arora et al. [1998]) Unless P = NP there is no approximation
scheme for Max-3Sat.
Proof:
Let P = (X, Y) be some NP-complete problem. By the PCP Theorem
16.26, P ∈PCP(log n, 1), so let p, k, f , P′ := (X′, Y ′) be as in Deﬁnition 16.24.
For any given x ∈X we construct a 3Sat-instance Jx. Namely, for each
random string r ∈{0, 1}⌊log(p(size(x)))⌋we deﬁne a family Zr of 3Sat-clauses (the
union of all these clauses will be Jx). We ﬁrst construct a family Z′
r of clauses
with an arbitrary number of literals and then apply Proposition 15.21.
So let r ∈{0, 1}⌊log(p(size(x)))⌋and f (x,r) = (t1, . . . , tk). Let {a(1), . . . , a(sr)}
be the set of strings a ∈{0, 1}k for which (x, f (x,r), a) ∈Y ′. If sr = 0 then we
simply set Z′ := {y, ¯y}, where y is some variable not used anywhere else.
Otherwise let c ∈{0, 1}⌊p(size(x))⌋. We have that (x, f (x,r), cf (x,r)) ∈Y ′ if and
only if

16.6 L-Reductions
401
sr@
j=1
 kA
i=1

cti = a( j)
i

.
This is equivalent to
A
(i1,...,isr )∈{1,...,k}sr
⎛
⎝
sr@
j=1

ctij = a( j)
i

⎞
⎠.
This conjunction of clauses can be constructed in polynomial time because
P′ ∈P and k is a constant. By introducing Boolean variables π1, . . . , π⌊p(size(x))⌋
representing the bits c1, . . . , c⌊p(size(x))⌋we obtain a family Z′
r of ksr clauses (each
with sr literals) such that Z′
r is satisﬁed if and only if (x, f (x,r), cf (x,r)) ∈Y ′.
By Proposition 15.21, we can rewrite each Z′
r equivalently as a conjunction
of 3Sat-clauses, where the number of clauses increases by at most a factor of
max{sr −2, 4}. Let this family of clauses be Zr. Since sr ≤2k, each Zr consists
of at most l := k2k max{2k −2, 4} 3Sat-clauses.
Our 3Sat-instance Jx is the union of all the families Zr for all r. Jx can be
computed in polynomial time.
Now if x is a yes-instance, then there exists a certiﬁcate c as in Deﬁnition
16.24. This c immediately deﬁnes a truth assignment satisfying Jx.
On the other hand, if x is a no-instance, then only 1
2 of the formulas Zr are
simultaneously satisﬁable. So in this case any truth assignment leaves at least a
fraction of
1
2l of the clauses unsatisﬁed.
So any k-factor approximation algorithm for Max-3Sat with k <
2l
2l−1 satisﬁes
more than a fraction of 2l−1
2l
= 1 −1
2l of the clauses of any satisﬁable instance.
Hence such an algorithm can decide whether x ∈Y or not. Since P is NP-
complete, such an algorithm cannot exist unless P = NP.
2
16.6 L-Reductions
Our goal is to show, for other problems than Max-3Sat, that they have no ap-
proximation scheme unless P = NP. As with the NP-completeness proofs (Section
15.5), it is not necessary to have a direct proof using the deﬁnition of PCP(log n, 1)
for each problem. Rather we use a certain type of reduction which preserves ap-
proximability (general polynomial transformations do not):
Deﬁnition 16.29.
Let P = (X, (Sx)x∈X, c, goal) and P′ = (X′, (S′
x)x∈X′, c′, goal′)
be two optimization problems with nonnegative weights. An L-reduction from P
to P′ is a pair of functions f and g, both computable in polynomial time, and two
constants α, β > 0 such that for any instance x of P:
(a)
f (x) is an instance of P′ with OPT( f (x)) ≤α OPT(x);
(b) For any feasible solution y′ of f (x), g(x, y′) is a feasible solution of x such
that |c(x, g(x, y′)) −OPT(x)| ≤β|c′( f (x), y′) −OPT( f (x))|.

402
16. Approximation Algorithms
We say that P is L-reducible to P′ if there is an L-reduction from P to P′.
The letter “L” in the term L-reduction stands for “linear”. L-reductions were
introduced by Papadimitriou and Yannakakis [1991]. The deﬁnition immediately
implies that L-reductions can be composed:
Proposition 16.30.
Let P, P′, P′′ be optimization problems with nonnegative
weights. If ( f, g, α, β) is an L-reduction from P to P′ and ( f ′, g′, α′, β′) is an L-
reduction from P′ to P′′, then their composition ( f ′′, g′′, αα′, ββ′) is an L-reduction
from P to P′′, where f ′′(x) = f ′( f (x)) and g′′(x, y′′) = g(x, g′(x′, y′′)).
2
The decisive property of L-reductions is that they preserve approximability:
Theorem 16.31.
(Papadimitriou and Yannakakis [1991]) Let P and P′ be two
optimization problems with nonnegative weights. Let ( f, g, α, β) be an L-reduction
from P to P′. If there is an approximation scheme for P′, then there is an approx-
imation scheme for P.
Proof:
Given an instance x of P and a number 0 < ϵ < 1, we apply the
approximation scheme for P′ to f (x) and ϵ′ :=
ϵ
2αβ . We obtain a feasible solution
y′ of f (x) and ﬁnally return y := g(x, y′), a feasible solution of x. Since
|c(x, y) −OPT(x)|
≤
β|c′( f (x), y′) −OPT( f (x))|
≤
β max

(1 + ϵ′) OPT( f (x)) −OPT( f (x)),
OPT( f (x)) −
1
1 + ϵ′ OPT( f (x))

≤
βϵ′ OPT( f (x))
≤
αβϵ′ OPT(x)
=
ϵ
2 OPT(x)
we get
c(x, y) ≤OPT(x) + |c(x, y) −OPT(x)| ≤

1 + ϵ
2

OPT(x)
and
c(x, y) ≥OPT(x)−| OPT(x)−c(x, y)| ≥

1 −ϵ
2

OPT(x) >
1
1 + ϵ OPT(x),
so this constitutes an approximation scheme for P.
2
This theorem together with Theorem 16.28 motivates the following deﬁnition:
Deﬁnition 16.32.
An optimization problem P with nonnegative weights is called
MAXSNP-hard if Max-3Sat is L-reducible to P.

16.6 L-Reductions
403
The name MAXSNP refers to a class of optimization problems introduced by
Papadimitriou and Yannakakis [1991]. Here we do not need this class, so we omit
its (nontrivial) deﬁnition.
Corollary 16.33.
Unless P = NP there is no approximation scheme for any
MAXSNP-hard problem.
Proof:
Directly from Theorems 16.28 and 16.31.
2
We shall show MAXSNP-hardness for several problems by describing L-
reductions. We start with a restricted version of Max-3Sat:
3-Occurrence Max-Sat Problem
Instance:
A set X of variables and a family Z of clauses over X, each with at
most three literals, such that no variable occurs in more than three
clauses.
Task:
Find a truth assignment T of X such that the number of clauses in
Z that are satisﬁed by T is maximum.
That this problem is NP-hard can be proved by a simple transformation from
3Sat (or Max-3Sat), cf. Exercise 9 of Chapter 15. Since this transformation is not
an L-reduction, it does not imply MAXSNP-hardness. We need a more complicated
construction, using so-called expander graphs:
Deﬁnition 16.34.
Let G be an undirected graph and γ > 0 a constant. G is a
γ-expander if for each A ⊆V (G) with |A| ≤|V (G)|
2
we have |(A)| ≥γ |A|.
For example, a complete graph is a 1-expander. However, one is interested in
expanders with a small number of edges. We cite the following theorem without
its quite complicated proof:
Theorem 16.35.
(Ajtai [1994]) There exists a positive constant γ such that for
any given even number n ≥4, a 3-regular γ -expander with n vertices can be
constructed in O(n3 log3 n) time.
The following corollary was mentioned (and used) by Papadimitriou [1994],
and a correct proof was given by Fern´andez-Baca and Lagergren [1998]:
Corollary 16.36.
For any given number n ≥3, a digraph G with O(n) vertices
and a set S ⊆V (G) of cardinality n with the following properties can be con-
structed in O(n3 log3 n) time:
|δ−(v)| + |δ+(v)| ≤3 for each v ∈V (G);
|δ−(v)| + |δ+(v)| = 2 for each v ∈S; and
|δ+(A)| ≥min{|S ∩A|, |S \ A|} for each A ⊆V (G).
Proof:
Let γ > 0 be the constant of Theorem 16.35, and let k :=
	
1
γ

. We
ﬁrst construct a 3-regular γ -expander H with n or n + 1 vertices, using Theorem
16.35.

404
16. Approximation Algorithms
We replace each edge {v, w} by k parallel edges (v, w) and k parallel edges
(w, v). Let the resulting digraph be H ′. Note that for any A ⊆V (H ′) with
|A| ≤|V (H ′)|
2
we have
|δ+
H ′(A)| = k|δH(A)| ≥k|H(A)| ≥kγ |A| ≥|A|.
Similarly we have for any A ⊆V (H ′) with |A| > |V (H ′)|
2
:
|δ+
H ′(A)| = k|δH(V (H ′) \ A)|
≥
k|H(V (H ′) \ A)|
≥
kγ |V (H ′) \ A| ≥|V (H ′) \ A|.
So in both cases we have |δ+
H ′(A)| ≥min{|A|, |V (H ′) \ A|}.
Now we split up each vertex v ∈V (H ′) into 6k+1 vertices xv,i, i = 0, . . . , 6k,
such that each vertex except xv,0 has degree 1. For each vertex xv,i we now add
vertices wv,i, j and yv,i, j ( j = 0, . . . , 6k) connected by a path of length 12k + 2
with vertices wv,i,0, wv,i,1, . . . , wv,i,6k, xv,i, yv,i,0, . . . , yv,i,6k in this order. Finally
we add edges (yv,i, j, wv, j,i) for all v ∈V (H ′), all i ∈{0, . . . , 6k} and all j ∈
{0, . . . , 6k} \ {i}.
Altogether we have a vertex set Zv of cardinality (6k + 1)(12k + 3) for each
v ∈V (H ′). The overall resulting graph G has |V (H ′)|(6k + 1)(12k + 3) =
O(n) vertices, each of degree two or three. By the construction, G[Zv] contains
min{|X1|, |X2|} vertex-disjoint paths from X1 to X2 for any pair of disjoint subsets
X1, X2 of {xv,i : i = 0, . . . , 6k}.
We choose S to be an n-element subset of {xv,0 : v ∈V (H ′)}; note that each
of these vertices has one entering and one leaving edge.
It remains to prove that |δ+(A)| ≥min{|S ∩A|, |S \ A|} for each A ⊆V (G).
We prove this by induction on |{v ∈V (H ′) : ∅̸= A ∩Zv ̸= Zv}|. If this number
is zero, i.e. A = 
v∈B Zv for some B ⊆V (H ′), then we have
|δ+
G(A)| = |δ+
H ′(B)| ≥min{|B|, |V (H ′) \ B|} ≥min{|S ∩A|, |S \ A|}.
Otherwise let v ∈V (H ′) with ∅̸= A ∩Zv ̸= Zv. Let P := {xv,i : i =
0, . . . , 6k} ∩A and Q := {xv,i : i = 0, . . . , 6k} \ A. If |P| ≤3k, then by the
property of G[Zv] we have
|E+
G(Zv ∩A, Zv \ A)|
≥
|P| = |P \ S| + |P ∩S|
≥
|E+
G(A \ Zv, A ∩Zv)| + |P ∩S|.
By applying the induction hypothesis to A \ Zv we therefore get
|δ+
G(A)|
≥
|δ+
G(A \ Zv)| + |P ∩S|
≥
min{|S ∩(A \ Zv)|, |S \ (A \ Zv)|} + |P ∩S|
≥
min{|S ∩A|, |S \ A|}.
Similarly, if |P| ≥3k + 1, then |Q| ≤3k and by the property of G[Zv] we
have

16.6 L-Reductions
405
|E+
G(Zv ∩A, Zv \ A)|
≥
|Q| = |Q \ S| + |Q ∩S|
≥
|E+
G(Zv \ A, V (G) \ (A ∪Zv))| + |Q ∩S|.
By applying the induction hypothesis to A ∪Zv we therefore get
|δ+
G(A)|
≥
|δ+
G(A ∪Zv)| + |Q ∩S|
≥
min{|S ∩(A ∪Zv)|, |S \ (A ∪Zv)|} + |Q ∩S|
≥
min{|S ∩A|, |S \ A|}.
2
Now we can prove:
Theorem 16.37.
(Papadimitriou and Yannakakis [1991], Papadimitriou [1994],
Fern´andez-Baca and Lagergren [1998]) The 3-Occurrence Max-Sat Problem
is MAXSNP-hard.
Proof:
We describe an L-reduction ( f, g, α, β) from Max-3Sat. To deﬁne f ,
let (X, Z) be an instance of Max-3Sat. For each variable x ∈X which occurs
in more than three, say in k clauses, we modify the instance as follows. We
replace x by a new different variable in each clause. This way we introduce new
variables x1, . . . , xk. We introduce additional constraints (and further variables)
which ensure, roughly spoken, that it is favourable to assign the same truth value
to all the variables x1, . . . , xk.
We construct G and S as in Corollary 16.36 and rename the vertices such that
S = {1, . . . , k}. Now for each vertex v ∈V (G) \ S we introduce a new variable
xv, and for each edge (v, w) ∈E(G) we introduce a clause {xv, xw}. In total we
have added at most
3
2(k + 1)

6
0 1
γ
1
+ 1
 
12
0 1
γ
1
+ 3

≤315
0 1
γ
12
k
new clauses, where γ is again the constant of Theorem 16.35.
Applying the above substitution for each variable we obtain an instance
(X′, Z′) = f (X, Z) of the 3-Occurrence Max-Sat Problem with
|Z′| ≤|Z| + 315
0 1
γ
12
3|Z| ≤946
0 1
γ
1
|Z|.
Hence
OPT(X′, Z′) ≤|Z′| ≤946
0 1
γ
12
|Z| ≤1892
0 1
γ
12
OPT(X, Z),
because at least half of the clauses of a Max-Sat-instance can be satisﬁed (either
by setting all variables true or all false). So we can set α := 1892
	
1
γ

2
.
To describe g, let T ′ be a truth assignment of X′. We ﬁrst construct a truth
assignment T ′′ of X′ satisfying at least as many clauses of Z′ as T ′, and satisfying
all new clauses (corresponding to edges of the graphs G above). Namely, for

406
16. Approximation Algorithms
any variable x occurring more than three times in (X, Z), let G be the graph
constructed above, and let A := {v ∈V (G) : T ′(xv) = true}. If |S ∩A| ≥|S \ A|
then we set T ′′(xv) := true for all v ∈V (G), otherwise we set T ′′(xv) := false
for all v ∈V (G). It is clear that all new clauses (corresponding to edges) are
satisﬁed.
There are at most min{|S ∩A|, |S \ A|} old clauses satisﬁed by T ′ but not
by T ′′. On the other hand, T ′ does not satisfy any of the clauses {xv, xw} for
(v, w) ∈δ+
G(A). By the properties of G, the number of these clauses is at least
min{|S ∩A|, |S \ A|}.
Now T ′′ yields a truth assignment T = g(X, Z, T ′) of X in the obvious way:
Set T (x) := T ′′(x) = T ′(x) for x ∈X ∩X′ and T (x) := T ′′(xi) if xi is any
variable replacing x in the construction from (X, Z) to (X′, Z′).
T violates as many clauses as T ′′. So if c(X, Z, T ) and c′(X′, Z′, T ′) denote
the number of satisﬁed clauses, we conclude
|Z| −c(X, Z, T ) = |Z′| −c′(X′, Z′, T ′′) ≤|Z′| −c′(X′, Z′, T ′)
(16.5)
On the other hand, any truth assignment T of X leads to a truth assignment T ′
of X′ violating the same number of clauses (by setting the variables xv (v ∈V (G))
uniformly to T (x) for each variable x and corresponding graph G in the above
construction). Hence
|Z| −OPT(X, Z) ≥|Z′| −OPT(X′, Z′).
(16.6)
Combining (16.5) and (16.6) we get
| OPT(X, Z) −c(X, Z, T )|
≤
(|Z| −c(X, Z, T )) −(|Z| −OPT(X, Z))
≤
OPT(X′, Z′) −c′(X′, Z′, T ′)
≤
| OPT(X′, Z′) −c′(X′, Z′, T ′)|,
where T = g(X, Z, T ′). So ( f, g, α, 1) is indeed an L-reduction.
2
This result is the starting point of several MAXSNP-hardness proofs. For ex-
ample:
Corollary 16.38.
(Papadimitriou and Yannakakis [1991])
The Maximum Sta-
ble Set Problem restricted to graphs with maximum degree 4 is MAXSNP-hard.
Proof:
The construction of the proof of Theorem 15.23 deﬁnes an L-reduction
from the 3-Occurrence Max-Sat Problem to the Maximum Stable Set Prob-
lem restricted to graphs with maximum degree 4: for each instance (X, Z) a graph
G is constructed such that each from truth assignment satisfying k clauses one
easily obtains a stable set of cardinality k, and vice versa.
2
Indeed, the Maximum Stable Set Problem is MAXSNP-hard even when re-
stricted to 3-regular graphs (Berman and Fujito [1999]). On the other hand, a
simple greedy algorithm, which in each step chooses a vertex v of minimum

Exercises
407
degree and deletes v and all its neighbours, is a (k+2)
3
-factor approximation algo-
rithm for the Maximum Stable Set Problem in graphs with maximum degree k
(Halld´orsson and Radhakrishnan [1997]). For k = 4 this gives an approximation
ratio of 2 which is better than the ratio 8 we get from the following proof (using
the 2-factor approximation algorithm for the Minimum Vertex Cover Problem).
Theorem 16.39.
(Papadimitriou and Yannakakis [1991]) The Minimum Vertex
Cover Problem restricted to graphs with maximum degree 4 is MAXSNP-hard.
Proof:
Consider the trivial transformation from the Maximum Stable Set Prob-
lem (Proposition 2.2) with f (G) := G and g(G, X) := V (G) \ X for all graphs
G and all X ⊆V (G). Although this is not an L-reduction in general, it is an
L-reduction if restricted to graphs with maximum degree 4, as we shall show.
If G has maximum degree 4, there exists a stable set of cardinality at least
|V (G)|
5
. So if we denote by α(G) the maximum cardinality of a stable set and by
τ(G) the minimum cardinality of a vertex cover we have
α(G) ≥1
4(|V (G)| −α(G)) = 1
4τ(G)
and α(G) −|X| = |V (G) \ X| −τ(G) for any stable set X ⊆V (G). Hence
( f, g, 4, 1) is an L-reduction.
2
See Clementi and Trevisan [1999] for a stronger statement. In particular, there
is no approximation scheme for the Minimum Vertex Cover Problem (unless
P = NP). We shall prove MAXSNP-hardness of other problems in later chapters;
see also Exercise 18.
Exercises
1. Formulate a 2-factor approximation algorithm for the following problem.
Given a digraph with edge weights, ﬁnd a directed acyclic subgraph of max-
imum weight.
Note: No k-factor approximation algorithm for this problem is known for
k < 2.
2. The k-Center Problem is deﬁned as follows: given an undirected graph G,
weights c : E(G) →R+, and a number k ∈N, ﬁnd a set X ⊆V (G) of
cardinality k such that
max
v∈V (G) min
x∈X dist(v, x)
is minimum. As usual we denote the optimum value by OPT(G, c, k).
(a) Let S be a maximal stable set in (V (G), {{v, w} : dist(v, w) ≤2R}).
Show that then OPT(G, c, |S|) ≥R.
(b) Use (a) to describe a 2-factor approximation algorithm for the k-Center
Problem.
(Hochbaum and Shmoys [1985])

408
16. Approximation Algorithms
(c)
∗
Show that there is no r-factor approximation algorithm for the k-Center
Problem for any r < 2.
Hint: Use Exercise 12 of Chapter 15.
(Hsu and Nemhauser [1979])
3.
∗
Show that even Max-2Sat is NP-hard (Hint: Reduction from 3Sat). Deduce
from this that the Maximum Cut Problem is also NP-hard. (The Maximum
Cut Problem consists of ﬁnding a maximum cardinality cut in a given undi-
rected graph.)
Note: This is a generalization of Exercise 19 of Chapter 15.
(Garey, Johnson and Stockmeyer [1976])
4. Consider the following local search algorithm for the Maximum Cut Problem
(cf. Exercise 3). Start with any partition (S, V (G) \ S). Now check iteratively
if some vertex can be added to S or deleted from S such that the result-
ing partition deﬁnes a cut with more edges. Stop if no such improvement is
possible.
(a) Prove that the above is a 2-factor approximation algorithm. (Recall Exer-
cise 10 of Chapter 2.)
(b) Can the algorithm be extended to the Maximum Weight Cut Problem,
where we have nonnegative edge weights?
(c) Does the above algorithm always ﬁnd the optimum solution for planar
graphs, or for bipartite graphs? For both classes there is a polynomial-
time algorithm (Exercise 7 of Chapter 12 and Proposition 2.27).
Note: There exists a 1.139-factor approximation algorithm for the Maxi-
mum Weight Cut Problem (Goemans and Williamson [1995]; Mahajan and
Ramesh [1999]). But there is no 1.062-factor approximation algorithm unless
P = NP (H˚astad [2001], Papadimitriou and Yannakakis [1991]).
5. In the Directed Maximum Weight Cut Problem we are given a digraph
G with weights c : E(G) →R+, and we look for a set X ⊆V (G) such
that 
e∈δ+(X) c(e) is maximum. Show that there is a 4-factor approximation
algorithm for this problem.
Hint: Use Exercise 4.
Note: There is a 1.165-factor but no 1.09-factor approximation algorithm un-
less P = NP (Feige and Goemans [1995], H˚astad [2001]).
6. Show that the performance guarantee in Theorem 16.5 is tight.
7. Can one ﬁnd a minimum vertex cover (or a maximum stable set) in a bipartite
graph in polynomial time?
8. Show that the LP relaxation min{cx : M ⊤x ≥1l, x ≥0} of the Minimum
Weight Vertex Cover Problem, where M is the incidence matrix of an
undirected graph and c ∈RV (G)
+
, always has a half-integral optimum solution
(i.e. one with entries 0, 1
2, 1 only). Derive another 2-factor approximation
algorithm from this fact.
9.
∗
Consider the Minimum Weight Feedback Vertex Set Problem: Given an
undirected graph G and weights c : V (G) →R+, ﬁnd a vertex set X ⊆V (G)

Exercises
409
of minimum weight such that G −X is a forest. Consider the following
recursive algorithm A:
If E(G) = ∅, then return A(G, c) := ∅. If |δG(x)| ≤1 for some x ∈V (G),
then return A(G, c) := A(G −x, c). If c(x) = 0 for some x ∈V (G), then
return A(G, c) := {x} ∪A(G −x, c). Otherwise let
ϵ :=
min
x∈V (G)
c(v)
|δ(v)|
and c′(v) := c(v) −ϵ|δ(v)| (v ∈V (G)). Let X := A(G, c′). For each x ∈X
do: If G −(X \ {x}) is a forest, then set X := X \ {x}. Return A(G, c) := x.
Prove that this a 2-factor approximation algorithm for the Minimum Weight
Feedback Vertex Set Problem.
(Becker and Geiger [1996])
10. Show that for each n ∈N there is a bipartite graph on 2n vertices for which
the Greedy Colouring Algorithm needs n colours. So the algorithm may
give arbitrarily bad results. However, show that there always exists an order
of the vertices for which the algorithm ﬁnds an optimum colouring.
11. Show that the following classes of graphs are perfect:
(a) bipartite graphs;
(b) interval graphs: ({v1, . . . , vn}, {{vi, vj} : i ̸= j, [ai, bi] ∩[aj, bj] ̸= ∅}),
where [a1, b1], . . . , [an, bn] is a set of closed intervals;
(c) chordal graphs (see Exercise 28 of Chapter 8).
12.
∗
Let G be an undirected graph. Prove that the following statements are equiv-
alent:
(a) G is perfect.
(b) For any weight function c : V (G) →Z+ the maximum weight of a clique
in G equals the minimum number of stable sets such that each vertex v
is contained in c(v) of them.
(c) For any weight function c : V (G) →Z+ the maximum weight of a stable
set in G equals the minimum number of cliques such that each vertex v
is contained in c(v) of them.
(d) The inequality system deﬁning (16.1) is TDI.
(e) The clique polytope of G, i.e. the convex hull of the incidence vectors of
all cliques in G, is given by

x ∈RV (G)
+
:

v∈S
xv ≤1 for all stable sets S in G

.
(16.7)
(f) The inequality system deﬁning (16.7) is TDI.
Note: The polytope (16.7) is called the antiblocker of the polytope (16.1).
13. An instance of Max-Sat is called k-satisﬁable if any k of its clauses can
be simultaneously satisﬁed. Let rk be the fraction of clauses one can always
satisfy in any k-satisﬁable instance.
(a) Prove that r1 = 1
2. (Hint: Theorem 16.21.)

410
16. Approximation Algorithms
(b) Prove that r2 =
√
5−1
2
. (Hint: Some variables occur in one-element clauses
(w.l.o.g. all one-element clauses are positive), set them true with prob-
ability a (for some 1
2 < a < 1), and set the other variables true with
probability 1
2. Apply the derandomization technique and choose a appro-
priately.)
(c) Prove that r3 ≥2
3.
(Lieberherr and Specker [1981])
14. Erd˝os [1967] showed the following: For each constant k ∈N, the (asymp-
totically) the best fraction of the edges that we can guarantee to be in the
maximum cut is 1
2, even if we restrict attention to graphs without odd circuits
of length k or less. (Compare Exercise 4(a).)
(a) What about k = ∞?
(b) Show how the Maximum Cut Problem can be reduced to Max-Sat.
Hint: Use a variable for each vertex and two clauses {x, y}, {¯x, ¯y} for
each edge {x, y}.
(c) Use (b) and Erd˝os’ Theorem in order to prove that rk ≤3
4 for all k. (For
a deﬁnition of rk, see Exercise 13.)
Note: Trevisan [2004] proved that limk→∞rk = 3
4.
15. Prove that the error probability 1
2 in Deﬁnition 16.24 can be replaced equiv-
alently by any number between 0 and 1. Deduce from this (and the proof
of Theorem 16.27) that there is no ρ-factor approximation algorithm for the
Maximum Clique Problem for any ρ ≥1 (unless P = NP).
16. Prove that the Maximum Clique Problem is L-reducible to the Set Packing
Problem: Given a set system (U, S), ﬁnd a maximum cardinality subfamily
R ⊆S whose elements are pairwise disjoint.
17. Prove that the Minimum Vertex Cover Problem has no absolute approxi-
mation algorithm (unless P = NP).
18. Prove that Max-2Sat is MAXSNP-hard.
Hint: Use Corollary 16.38.
(Papadimitriou and Yannakakis [1991])
References
General Literature:
Asano, T., Iwama, K., Takada, H., and Yamashita, Y. [2000]: Designing high-quality ap-
proximation algorithms for combinatorial optimization problems. IEICE Transactions on
Communications/Electronics/Information and Systems E83-D (2000), 462–478
Ausiello, G., Crescenzi, P., Gambosi, G., Kann, V., Marchetti-Spaccamela, A., and Protasi,
M. [1999]: Complexity and Approximation: Combinatorial Optimization Problems and
Their Approximability Properties. Springer, Berlin 1999
Garey, M.R., and Johnson, D.S. [1979]: Computers and Intractability; A Guide to the
Theory of NP-Completeness. Freeman, San Francisco 1979, Chapter 4
Hochbaum, D.S. [1996]: Approximation Algorithms for NP-Hard Problems. PWS, Boston,
1996

References
411
Horowitz, E., and Sahni, S. [1978]: Fundamentals of Computer Algorithms. Computer
Science Press, Potomac 1978, Chapter 12
Shmoys, D.B. [1995]: Computing near-optimal solutions to combinatorial optimization
problems. In: Combinatorial Optimization; DIMACS Series in Discrete Mathematics
and Theoretical Computer Science 20 (W. Cook, L. Lov´asz, P. Seymour, eds.), AMS,
Providence 1995
Papadimitriou, C.H. [1994]: Computational Complexity, Addison-Wesley, Reading 1994,
Chapter 13
Vazirani, V.V. [2001]: Approximation Algorithms. Springer, Berlin, 2001
Cited References:
Ajtai, M. [1994]: Recursive construction for 3-regular expanders. Combinatorica 14 (1994),
379–416
Appel, K., and Haken, W. [1977]: Every planar map is four colorable; Part I; Discharging.
Illinois Journal of Mathematics 21 (1977), 429–490
Appel, K., Haken, W., and Koch, J. [1977]: Every planar map is four colorable; Part II;
Reducibility. Illinois Journal of Mathematics 21 (1977), 491–567
Arora, S. [1994]: Probabilistic checking of proofs and the hardness of approximation prob-
lems, Ph.D. thesis, U.C. Berkeley, 1994
Arora, S., Lund, C., Motwani, R., Sudan, M., and Szegedy, M. [1998]: Proof veriﬁcation
and hardness of approximation problems. Journal of the ACM 45 (1998), 501–555
Arora, S., and Safra, S. [1998]: Probabilistic checking of proofs. Journal of the ACM 45
(1998), 70–122
Asano, T., and Williamson, D.P. [2002]: Improved approximation algorithms for MAX
SAT. Journal of Algorithms 42 (2002), 173–202
Bar-Yehuda, R., and Even, S. [1981]: A linear-time approximation algorithm for the
weighted vertex cover problem. Journal of Algorithms 2 (1981), 198–203
Becker, A., and Geiger, D. [1996]: Optimization of Pearl’s method of conditioning and
greedy-like approximation algorithms for the vertex feedback set problem. Artiﬁcial
Intelligence Journal 83 (1996), 1–22
Bellare, M., and Sudan, M. [1994]: Improved non-approximability results. Proceedings of
the 26th Annual ACM Symposium on the Theory of Computing (1994), 184–193
Bellare, M., Goldreich, O., and Sudan, M. [1998]: Free bits, PCPs and nonapproximability
– towards tight results. SIAM Journal on Computing 27 (1998), 804–915
Berge, C. [1961]: F¨arbung von Graphen, deren s¨amtliche bzw. deren ungerade Kreise
starr sind. Wissenschaftliche Zeitschrift, Martin Luther Universit¨at Halle-Wittenberg,
Mathematisch-Naturwissenschaftliche Reihe (1961), 114–115
Berge, C. [1962]: Sur une conjecture relative au probl`eme des codes optimaux. Communi-
cation, 13`eme assembl´ee g´en´erale de l’URSI, Tokyo 1962
Berman, P., and Fujito, T. [1999]: On approximation properties of the independent set
problem for low degree graphs. Theory of Computing Systems 32 (1999), 115–132
Brooks, R.L. [1941]: On colouring the nodes of a network. Proceedings of the Cambridge
Philosophical Society 37 (1941), 194–197
Chen, J., Friesen, D.K., and Zheng, H. [1999]: Tight bound on Johnson’s algorithm for
maximum satisﬁability. Journal of Computer and System Sciences 58 (1999), 622–640
Chudnovsky, M., Cornu´ejols, G., Liu, X., Seymour, P., and Vu˘skovi´c, K. [2005]: Recog-
nizing Berge graphs. Combinatorica 25 (2005), 143–186
Chudnovsky, M., Robertson, N., Seymour, P., and Thomas, R. [2002]: The strong perfect
graph theorem. Manuscript, 2002. To appear in Annals of Mathematics
Chv´atal, V. [1975]: On certain polytopes associated with graphs. Journal of Combinatorial
Theory B 18 (1975), 138–154
Chv´atal, V. [1979]: A greedy heuristic for the set cover problem. Mathematics of Operations
Research 4 (1979), 233–235

412
16. Approximation Algorithms
Clementi, A.E.F., and Trevisan, L. [1999]: Improved non-approximability results for min-
imum vertex cover with density constraints. Theoretical Computer Science 225 (1999),
113–128
Dinur, I., and Safra, S. [2002]: The importance of being biased. Proceedings of the 34th
Annual ACM Symposium on the Theory of Computing (2002), 33–42; to appear in
Annals of Mathematics 162 (2005)
Erd˝os, P. [1967]: On bipartite subgraphs of graphs. Mat. Lapok. 18 (1967), 283–288
Feige, U. [1998]: A threshold of ln n for the approximating set cover. Journal of the ACM
45 (1998), 634–652
Feige, U. [2004]: Approximating maximum clique by removing subgraphs. SIAM Journal
on Discrete Mathematics 18 (2004), 219–225
Feige, U., and Goemans, M.X. [1995]: Approximating the value of two prover proof sys-
tems, with applications to MAX 2SAT and MAX DICUT. Proceedings of the 3rd Israel
Symposium on Theory of Computing and Systems (1995), 182–189
Feige, U., Goldwasser, S., Lov´asz, L., Safra, S., and Szegedy, M. [1996]: Interactive proofs
and the hardness of approximating cliques. Journal of the ACM 43 (1996), 268–292
Fern´andez-Baca, D., and Lagergren, J. [1998]: On the approximability of the Steiner tree
problem in phylogeny. Discrete Applied Mathematics 88 (1998), 129–145
Fulkerson, D.R. [1972]: Anti-blocking polyhedra. Journal of Combinatorial Theory B 12
(1972), 50–71
F¨urer, M., and Raghavachari, B. [1994]: Approximating the minimum-degree Steiner tree
to within one of optimal. Journal of Algorithms 17 (1994), 409–423
Garey, M.R., and Johnson, D.S. [1976]: The complexity of near-optimal graph coloring.
Journal of the ACM 23 (1976), 43–49
Garey, M.R., Johnson, D.S., and Stockmeyer, L. [1976]: Some simpliﬁed NP-complete
graph problems. Theoretical Computer Science 1 (1976), 237–267
Goemans, M.X., and Williamson, D.P. [1994]: New 3/4-approximation algorithms for the
maximum satisﬁability problem. SIAM Journal on Discrete Mathematics 7 (1994), 656–
666
Goemans, M.X., and Williamson, D.P. [1995]: Improved approximation algorithms for
maximum cut and satisﬁability problems using semideﬁnite programming Journal of the
ACM 42 (1995), 1115–1145
Gr¨otschel, M., Lov´asz, L., and Schrijver, A. [1988]: Geometric Algorithms and Combina-
torial Optimization. Springer, Berlin 1988
Halld´orsson, M.M., and Radhakrishnan, J. [1997]: Greed is good: approximating indepen-
dent sets in sparse and bounded degree graphs. Algorithmica 18 (1997), 145–163
H˚astad, J. [1999]: Clique is hard to approximate within n1−ϵ. Acta Mathematica 182 (1999),
105–142
H˚astad, J. [2001]: Some optimal inapproximability results. Journal of the ACM 48 (2001),
798–859
Heawood, P.J. [1890]: Map colour theorem. Quarterly Journal of Pure Mathematics 24
(1890), 332–338
Hochbaum, D.S. [1982]: Approximation algorithms for the set covering and vertex cover
problems. SIAM Journal on Computing 11 (1982), 555–556
Hochbaum, D.S., and Shmoys, D.B. [1985]: A best possible heuristic for the k-center
problem. Mathematics of Operations Research 10 (1985), 180–184
Holyer, I. [1981]: The NP-completeness of edge-coloring. SIAM Journal on Computing 10
(1981), 718–720
Hougardy, S., Pr¨omel, H.J., and Steger, A. [1994]: Probabilistically checkable proofs and
their consequences for approximation algorithms. Discrete Mathematics 136 (1994), 175–
223
Hsu, W.L., and Nemhauser, G.L. [1979]: Easy and hard bottleneck location problems.
Discrete Applied Mathematics 1 (1979), 209–216

References
413
Johnson, D.S. [1974]: Approximation algorithms for combinatorial problems. Journal of
Computer and System Sciences 9 (1974), 256–278
Khanna, S., Linial, N., and Safra, S. [2000]: On the hardness of approximating the chromatic
number. Combinatorica 20 (2000), 393–415
K¨onig, D. [1916]: ¨Uber Graphen und ihre Anwendung auf Determinantentheorie und Men-
genlehre. Mathematische Annalen 77 (1916), 453–465
Lieberherr, K., and Specker, E. [1981]: Complexity of partial satisfaction. Journal of the
ACM 28 (1981), 411–421
Lov´asz, L. [1972]: Normal hypergraphs and the perfect graph conjecture. Discrete Mathe-
matics 2 (1972), 253–267
Lov´asz, L. [1975]: On the ratio of optimal integral and fractional covers. Discrete Mathe-
matics 13 (1975), 383–390
Lov´asz, L. [1979]: On the Shannon capacity of a graph. IEEE Transactions on Information
Theory 25 (1979), 1–7
Lov´asz, L. [1979]: Graph theory and integer programming. In: Discrete Optimization I;
Annals of Discrete Mathematics 4 (P.L. Hammer, E.L. Johnson, B.H. Korte, eds.), North-
Holland, Amsterdam 1979, pp. 141–158
Mahajan, S., and Ramesh, H. [1999]: Derandomizing approximation algorithms based on
semideﬁnite programming. SIAM Journal on Computing 28 (1999), 1641–1663
Papadimitriou, C.H., and Steiglitz, K. [1982]: Combinatorial Optimization; Algorithms and
Complexity. Prentice-Hall, Englewood Cliffs 1982, pp. 406–408
Papadimitriou, C.H., and Yannakakis, M. [1991]: Optimization, approximation, and com-
plexity classes. Journal of Computer and System Sciences 43 (1991), 425–440
Papadimitriou, C.H., and Yannakakis, M. [1993]: The traveling salesman problem with
distances one and two. Mathematics of Operations Research 18 (1993), 1–12
Raghavan, P., and Thompson, C.D. [1987]: Randomized rounding: a technique for provably
good algorithms and algorithmic proofs. Combinatorica 7 (1987), 365–374
Raz, R., and Safra, S. [1997]: A sub constant error probability low degree test, and a sub
constant error probability PCP characterization of NP. Proceedings of the 29th Annual
ACM Symposium on the Theory of Computing (1997), 475–484
Robertson, N., Sanders, D.P., Seymour, P., and Thomas, R. [1997]: The four colour theorem.
Journal of Combinatorial Theory B 70 (1997), 2–44
Robertson, N., Sanders, D.P., Seymour, P., and Thomas, R. [1996]: Efﬁciently four-coloring
planar graphs. Proceedings of the 28th Annual ACM Symposium on the Theory of
Computing (1996), 571–575
Sanders, P., and Steurer, D. [2005]: An asymptotic approximation scheme for multigraph
edge coloring. Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete
Algorithms (2005), 897–906
Slav´ık, P. [1997]: A tight analysis of the greedy algorithm for set cover. Journal of Algo-
rithms 25 (1997), 237–254
Stockmeyer, L.J. [1973]: Planar 3-colorability is polynomial complete. ACM SIGACT
News 5 (1973), 19–25
Trevisan, L. [2004]: On local versus global satisﬁability. SIAM Journal on Discrete Math-
ematics 17 (2004), 541–547
Vizing, V.G. [1964]: On an estimate of the chromatic class of a p-graph. Diskret. Analiz 3
(1964), 23–30 [in Russian]
Yannakakis, M. [1994]: On the approximation of maximum satisﬁability. Journal of Algo-
rithms 17 (1994), 475–502

17. The Knapsack Problem
The MinimumWeightPerfectMatchingProblem and the WeightedMatroid
Intersection Problem discussed in earlier chapters are among the “hardest”
problems for which a polynomial-time algorithm is known. In this chapter we
deal with the following problem which turns out to be, in a sense, the “easiest”
NP-hard problem:
Knapsack Problem
Instance:
Nonnegative integers n, c1, . . . , cn, w1, . . . , wn and W.
Task:
Find a subset S ⊆{1, . . . , n} such that 
j∈S wj ≤W and 
j∈S cj
is maximum.
Applications arise whenever we want to select an optimum subset of bounded
weight from a set of elements each of which has a weight and a proﬁt.
We start by considering the fractional version in Section 17.1, which turns
out to be solvable in linear time. The integral knapsack problem is NP-hard as
shown in Section 17.2, but a pseudopolynomial algorithm solves it optimally.
Combined with a rounding technique this can be used to design a fully polynomial
approximation scheme, which is the subject of Section 17.3.
17.1 Fractional Knapsack and Weighted Median Problem
We consider the following problem:
Fractional Knapsack Problem
Instance:
Nonnegative integers n, c1, . . . , cn, w1, . . . , wn and W.
Task:
Find numbers x1, . . . , xn ∈[0, 1] such that n
j=1 xjwj ≤W and
n
j=1 xjcj is maximum.
The following observation suggests a simple algorithm which requires sorting
the elements appropriately:
Proposition 17.1.
(Dantzig [1957]) Let c1, . . . , cn, w1, . . . , wn and W be non-
negative integers with n
i=1 wi > W and

416
17. The Knapsack Problem
c1
w1
≥
c2
w2
≥· · · ≥
cn
wn
,
and let
k := min

j ∈{1, . . . , n} :
j

i=1
wi > W

.
Then an optimum solution of the given instance of the Fractional Knapsack
Problem is deﬁned by
xj
:=
1
for j = 1, . . . , k −1,
xk
:=
W −k−1
j=1 wj
wk
,
xj
:=
0
for j = k + 1, . . . , n.
2
Sorting the elements takes O(n log n) time (Theorem 1.5), and computing k
can be done in O(n) time by simple linear scanning. Although this algorithm is
quite fast, one can do even better. Observe that the problem reduces to a weighted
median search:
Deﬁnition 17.2.
Let n ∈N, z1, . . . , zn ∈R, w1, . . . , wn ∈R+ and W ∈R with
0 < W ≤n
i=1 wi. Then the (w1, . . . , wn; W)-weighted median with respect to
(z1, . . . , zn) is deﬁned to be the unique number z∗for which

i:zi<z∗
wi < W ≤

i:zi≤z∗
wi.
So we have to solve the following problem:
Weighted Median Problem
Instance:
An integer n, numbers z1, . . . , zn ∈R, w1, . . . , wn ∈R+ and a
number W with 0 < W ≤n
i=1 wi.
Task:
Find
the
(w1, . . . , wn; W)-weighted
median
with
respect
to
(z1, . . . , zn).
An important special case is the following:
Selection Problem
Instance:
An integer n, numbers z1, . . . , zn ∈R, and an integer k ∈{1, . . . , n}.
Task:
Find the k-smallest number among z1, . . . , zn.
The weighted median can be determined in O(n) time: the following algorithm
is a weighted version of the one by Blum et al. [1973]; see also Vygen [1997].

17.1 Fractional Knapsack and Weighted Median Problem
417
Weighted Median Algorithm
Input:
An integer n, numbers z1, . . . , zn ∈R, w1, . . . , wn ∈R+ and a
number W with 0 < W ≤n
i=1 wi.
Output:
The (w1, . . . , wn; W)-weighted median with respect to (z1, . . . , zn).
1⃝
Partition the list z1, . . . , zn into blocks of ﬁve elements each. Find the
(non-weighted) median of each block. Let M be the list of these
B n
5
C
median elements.
2⃝
Find (recursively) the non-weighted median of M, let it be zm.
3⃝
Compare each element with zm. W.l.o.g. let zi < zm for i = 1, . . . , k,
zi = zm for i = k + 1, . . . ,l and zi > zm for i = l + 1, . . . , n.
4⃝
If
k

i=1
wi < W ≤
l
i=1
wi then stop (z∗:= zm).
If
l
i=1
wi < W then ﬁnd recursively the

wl+1, . . . , wn; W −
l
i=1
wi

-
weighted median with respect to (zl+1, . . . , zn). Stop.
If
k

i=1
wi ≥W then ﬁnd recursively the (w1, . . . , wk; W)-weighted
median with respect to (z1, . . . , zk). Stop.
Theorem 17.3.
The Weighted Median Algorithm works correctly and takes
O(n) time only.
Proof:
The correctness is easily checked. Let us denote the worst-case running
time for n elements by f (n). We obtain
f (n) = O(n) + f
	n
5


+ O(n) + f
1
2
	n
5

5 + 1
2
	n
5

2

,
because the recursive call in 4⃝misses at least three elements out of at least half
of the ﬁve-element blocks. The above recursion formula yields f (n) = O(n): as
B n
5
C
≤
9
41n for all n ≥37, one obtains f (n) ≤cn + f
 9
41n

+ f
 7
2
9
41n

for a
suitable c and n ≥37. Given this, f (n) ≤(82c + f (36))n can be veriﬁed easily
by induction. So indeed the overall running time is linear.
2
We immediately obtain the following corollaries:
Corollary 17.4.
(Blum et al. [1973]) The Selection Problem can be solved in
O(n) time.
Proof:
Set wi := 1 for i = 1, . . . , n and W := k and apply Theorem 17.3.
2
Corollary 17.5.
The Fractional Knapsack Problem can be solved in linear
time.

418
17. The Knapsack Problem
Proof:
As remarked at the beginning of this section, setting zi :=
ci
wi (i =
1, . . . , n) reduces the Fractional Knapsack Problem to the Weighted Median
Problem.
2
17.2 A Pseudopolynomial Algorithm
We now turn to the (integral) Knapsack Problem. The techniques of the previous
section are also of some use here:
Proposition 17.6.
Let c1, . . . , cn, w1, . . . , wn and W be nonnegative integers with
wj ≤W for j = 1, . . . , n, n
i=1 wi > W, and
c1
w1
≥
c2
w2
≥· · · ≥
cn
wn
.
Let
k := min

j ∈{1, . . . , n} :
j

i=1
wi > W

.
Then choosing the better of the two feasible solutions {1, . . . , k −1} and {k} consti-
tutes a 2-factor approximation algorithm for the Knapsack Problem with running
time O(n).
Proof:
Given any instance of the Knapsack Problem, elements i ∈{1, . . . , n}
with wi > W are of no use and can be deleted beforehand. Now if n
i=1 wi ≤W,
then {1, . . . , n} is an optimum solution. Otherwise we compute the number k in
O(n) time without sorting: this is just a Weighted Median Problem as above
(Theorem 17.3).
By Proposition 17.1, k
i=1 ci is an upper bound on the optimum value of the
Fractional Knapsack Problem, hence also for the integral Knapsack Problem.
Therefore the better of the two feasible solutions {1, . . . , k −1} and {k} achieves
at least half the optimum value.
2
But we are more interested in an exact solution of the Knapsack Problem.
However, we have to make the following observation:
Theorem 17.7.
The Knapsack Problem is NP-hard.
Proof:
We prove that the related decision problem deﬁned as follows is NP-
complete: given nonnegative integers n, c1, . . . , cn, w1, . . . , wn, W and K, is
there a subset S ⊆{1, . . . , n} such that 
j∈S wj ≤W and 
j∈S cj ≥K ?
This decision problem obviously belongs to NP. To show that it is NP-
complete, we transform Subset-Sum (see Corollary 15.27) to it. Given an instance
c1, . . . , cn, K of Subset-Sum, deﬁne wj := cj ( j = 1, . . . , n) and W := K. Ob-
viously this yields an equivalent instance of the above decision problem.
2

17.2 A Pseudopolynomial Algorithm
419
Since we have not shown the Knapsack Problem to be strongly NP-hard
there is hope for a pseudopolynomial algorithm. Indeed, the algorithm given in
the proof of Theorem 15.37 can easily be generalized by introducing weights on
the edges and solving a shortest path problem. This leads to an algorithm with
running time O(nW) (Exercise 3).
By a similar trick we can also get an algorithm with an O(nC) running time,
where C := n
j=1 cj. We describe this algorithm in a direct way, without con-
structing a graph and referring to shortest paths. Since the correctness of the algo-
rithm is based on simple recursion formulas we speak of a dynamic programming
algorithm. It is basically due to Bellman [1956,1957] and Dantzig [1957].
Dynamic Programming Knapsack Algorithm
Input:
Nonnegative integers n, c1, . . . , cn, w1, . . . , wn and W.
Output:
A subset S ⊆{1, . . . , n} such that 
j∈S wj ≤W and 
j∈S cj is
maximum.
1⃝
Let C be any upper bound on the value of the optimum solution, e.g.
C :=
n

j=1
cj.
2⃝
Set x(0, 0) := 0 and x(0, k) := ∞for k = 1, . . . , C.
3⃝
For j := 1 to n do:
For k := 0 to C do:
Set s( j, k) := 0 and x( j, k) := x( j −1, k).
For k := cj to C do:
If x( j −1, k −cj) + wj ≤min{W, x( j, k)} then:
Set x( j, k) := x( j −1, k −cj) + wj and s( j, k) := 1.
4⃝
Let k = max{i ∈{0, . . . , C} : x(n, i) < ∞}. Set S := ∅.
For j := n down to 1 do:
If s( j, k) = 1 then set S := S ∪{ j} and k := k −cj.
Theorem 17.8.
The Dynamic Programming Knapsack Algorithm ﬁnds an
optimum solution in O(nC) time.
Proof:
The running time is obvious.
The variable x( j, k) denotes the minimum total weight of a subset S ⊆
{1, . . . , j} with 
i∈S wi ≤W and 
i∈S ci = k. The algorithm correctly com-
putes these values using the recursion formulas
x( j, k) =
 x( j −1, k−cj) + wj
if cj ≤k and
x( j −1, k−cj) + wj ≤min{W, x( j −1, k)}
x( j −1, k)
otherwise
for j = 1, . . . , n and k = 0, . . . , C. The variables s( j, k) indicate which of these
two cases applies. So the algorithm enumerates all subsets S ⊆{1, . . . , n} except

420
17. The Knapsack Problem
those that are infeasible or those that are dominated by others: S is said to be
dominated by S′ if 
j∈S cj = 
j∈S′ cj and 
j∈S wj ≥
j∈S′ wj. In 4⃝the best
feasible subset is chosen.
2
Of course it is desirable to have a better upper bound C than n
i=1 ci. For
example, the 2-factor approximation algorithm of Proposition 17.6 can be run;
multiplying the value of the returned solution by 2 yields an upper bound on the
optimum value. We shall use this idea later.
The O(nC)-bound is not polynomial in the size of the input, because the input
size can only be bounded by O(n log C +n log W) (we may assume that wj ≤W
for all j). But we have a pseudopolynomial algorithm which can be quite effective
if the numbers involved are not too large. If both the weights w1, . . . , wn and the
proﬁts c1, . . . , cn are small, the O(ncmaxwmax)-algorithm of Pisinger [1999] is the
fastest one (cmax := max{c1, . . . , cn}, wmax := max{w1, . . . , wn}).
17.3 A Fully Polynomial Approximation Scheme
In this section we investigate the existence of approximation algorithms of the
Knapsack Problem. By Proposition 16.16, the Knapsack Problem has no ab-
solute approximation algorithm unless P = NP.
However, we shall prove that the Knapsack Problem has a fully polynomial
approximation scheme. The ﬁrst such algorithm was found by Ibarra and Kim
[1975].
Since the running time of the Dynamic Programming Knapsack Algorithm
depends on C, it is a natural idea to divide all numbers c1, . . . , cn by 2 and round
them down. This will reduce the running time, but may lead to inaccurate solutions.
More generally, setting
¯cj :=
cj
t

( j = 1, . . . , n)
will reduce the running time by a factor t. Trading accuracy for running time is
typical for approximation schemes. For S ⊆{1, . . . , n} we write c(S) := 
i∈S ci.
Knapsack Approximation Scheme
Input:
Nonnegative integers n, c1, . . . , cn, w1, . . . , wn and W. A number
ϵ > 0.
Output:
A subset S ⊆{1, . . . , n} such that 
j∈S wj ≤W and 
j∈S cj ≥
1
1+ϵ

j∈S′ cj for all S′ ⊆{1, . . . , n} with 
j∈S′ wj ≤W.
1⃝
Run the 2-factor approximation algorithm of Proposition 17.6. Let S1
be the solution obtained. If c(S1) = 0 then set S := S1 and stop.
2⃝
Set t := max

1, ϵc(S1)
n

.
Set ¯cj :=
 cj
t

for j = 1, . . . , n.

17.3 A Fully Polynomial Approximation Scheme
421
3⃝
Apply the Dynamic Programming Knapsack Algorithm to the
instance (n, ¯c1, . . . , ¯cn, w1, . . . , wn, W); set C := 2c(S1)
t
. Let S2 be
the solution obtained.
4⃝
If c(S1) > c(S2) then set S := S1, else set S := S2.
Theorem 17.9.
(Ibarra and Kim [1975], Sahni [1976], Gens and Levner [1979])
The Knapsack Approximation Scheme is a fully polynomial approximation
scheme for the Knapsack Problem; its running time is O

n2 · 1
ϵ

.
Proof:
If the algorithm stops in 1⃝then S1 is optimal by Proposition 17.6. So we
now assume c(S1) > 0. Let S∗be an optimum solution of the original instance.
Since 2c(S1) ≥c(S∗) by Proposition 17.6, C in 3⃝is a correct upper bound on
the value of the optimum solution of the rounded instance. So by Theorem 17.8,
S2 is an optimum solution of the rounded instance. Hence we have:

j∈S2
cj ≥

j∈S2
t ¯cj = t

j∈S2
¯cj ≥t

j∈S∗
¯cj =

j∈S∗
t ¯cj >

j∈S∗
(cj −t) ≥c(S∗)−nt.
If t = 1, then S2 is optimal by Theorem 17.8. Otherwise the above inequality
implies c(S2) ≥c(S∗) −ϵc(S1), and we conclude that
(1 + ϵ)c(S) ≥c(S2) + ϵc(S1) ≥c(S∗).
So we have a (1 + ϵ)-factor approximation algorithm for any ﬁxed ϵ > 0. By
Theorem 17.8 the running time of 3⃝can be bounded by
O(nC) = O
nc(S1)
t

= O

n2 · 1
ϵ

.
The other steps can easily be done in O(n) time.
2
Lawler [1979] found a similar fully polynomial approximation scheme whose
running time is O

n log
 1
ϵ

+ 1
ϵ4

. This was improved by Kellerer and Pferschy
[2004].
Unfortunately there are not many problems that have a fully polynomial ap-
proximation scheme. To state this more precisely, we consider the Maximization
Problem For Independence Systems.
What we have used in our construction of the Dynamic Programming Knap-
sack Algorithm and the Knapsack Approximation Scheme is a certain domi-
nance relation. We generalize this concept as follows:
Deﬁnition 17.10.
Given an independence system (E, F), a cost function c : E →
Z+, subsets S1, S2 ⊆E, and ϵ > 0. S1 ϵ-dominates S2 if
1
1 + ϵ c(S1) ≤c(S2) ≤(1 + ϵ) c(S1)
and there is a basis B1 with S1 ⊆B1 such that for each basis B2 with S2 ⊆B2 we
have
(1 + ϵ) c(B1) ≥c(B2).

422
17. The Knapsack Problem
ϵ-Dominance Problem
Instance:
An independence system (E, F), a cost function c : E →Z+, a
number ϵ > 0, and two subsets S1, S2 ⊆E.
Question:
Does S1 ϵ-dominate S2 ?
Of course the independence system is given by some oracle, e.g. an indepen-
dence oracle. The Dynamic Programming Knapsack Algorithm made frequent
use of 0-dominance. It turns out that the existence of an efﬁcient algorithm for the
ϵ-Dominance Problem is essential for a fully polynomial approximation scheme.
Theorem 17.11.
(Korte and Schrader [1981]) Let I be a family of independence
systems. Let I′ be the family of instances (E, F, c) of the Maximization Problem
For Independence Systems with (E, F) ∈I and c : E →Z+, and let I′′
be the family of instances (E, F, c, ϵ, S1, S2) of the ϵ-Dominance Problem with
(E, F) ∈I.
Then there exists a fully polynomial approximation scheme for the Maximiza-
tion Problem For Independence Systems restricted to I′ if and only if there
exists an algorithm for the ϵ-Dominance Problem restricted to I′′ whose running
time is bounded by a polynomial in the length of the input and 1
ϵ .
While the sufﬁciency is proved by generalizing the Knapsack Approxima-
tion Scheme (Exercise 10), the proof of the necessity is rather involved and not
presented here. The conclusion is that if a fully polynomial approximation scheme
exists at all, then a modiﬁcation of the Knapsack Approximation Scheme does
the job. See also Woeginger [2000] for a similar result.
To prove that for a certain optimization problem there is no fully polynomial
approximation scheme, the following theorem is often more useful:
Theorem 17.12.
(Garey and Johnson [1978]) A strongly NP-hard optimization
problem satisfying
OPT(I) ≤p (size(I), largest(I))
for some polynomial p and all instances I has a fully polynomial approximation
scheme only if P = NP.
Proof:
Suppose it has a fully polynomial approximation scheme. Then we apply
it with
ϵ =
1
p(size(I), largest(I)) + 1
and obtain an exact pseudopolynomial algorithm. By Proposition 15.39 this is
impossible unless P = NP.
2
Exercises
1. Consider the fractional multi-knapsack problem deﬁned as follows. An in-
stance consists of nonnegative integers m and n, numbers wj, ci j and Wi

References
423
(1 ≤i ≤m, 1 ≤j ≤n). The task is to ﬁnd numbers xi j ∈[0, 1]
with m
i=1 xi j = 1 for all j and n
j=1 xi jwj ≤Wi for all i such that
m
i=1
n
j=1 xi jci j is minimum.
Can one ﬁnd a combinatorial polynomial-time algorithm for this problem
(without using Linear Programming)?
Hint: Reduction to a Minimum Cost Flow Problem.
2. Consider the following greedy algorithm for the Knapsack Problem (similar
to the one in Proposition 17.6). Sort the indices such that c1
w1 ≥· · · ≥cn
wn . Set
S := ∅. For i := 1 to n do: If 
j∈S∪{i} wj ≤W then set S := S ∪{i}. Show
that this is not a k-factor approximation algorithm for any k.
3. Find an exact O(nW)-algorithm for the Knapsack Problem.
4. Consider the following problem: Given nonnegative integers n, c1, . . . , cn,
w1, . . . , wn and W, ﬁnd a subset S ⊆{1, . . . , n} such that 
j∈S wj ≥W and

j∈S cj is minimum. How can this problem be solved by a pseudopolynomial
algorithm?
5.
∗
Can one solve the integral multi-knapsack problem (see Exercise 1) in pseu-
dopolynomial time if m is ﬁxed?
6. Let c ∈{0, . . . , k}m and s ∈[0, 1]m. How can one decide in O(mk) time
whether max
5
cx : x ∈Zm
+, sx ≤1
6
≤k?
7. Consider the two Lagrangean relaxations of Exercise 20 of Chapter 5. Show
that one of them can be solved in linear time while the other one reduces to
m instances of the Knapsack Problem.
8. Let m ∈N be a constant. Consider the following scheduling problem: Given
n jobs and m machines, costs ci j ∈Z+ (i = 1, . . . , n, j = 1, . . . , m), and
capacities Tj ∈Z+ ( j = 1, . . . , m), ﬁnd an assignment f : {1, . . . , n} →
{1, . . . , m} such that |{i ∈{1, . . . , n} : f (i) = j}| ≤Tj for j = 1, . . . , m, and
the total cost n
i=1 ci f (i) is minimum.
Show that this problem has a fully polynomial approximation scheme.
9. Give a polynomial-time algorithm for the ϵ-Dominance Problem restricted
to matroids.
10.
∗
Prove the if-part of Theorem 17.11.
References
General Literature:
Garey, M.R., and Johnson, D.S. [1979]: Computers and Intractability; A Guide to the
Theory of NP-Completeness. Freeman, San Francisco 1979, Chapter 4
Martello, S., and Toth, P. [1990]: Knapsack Problems; Algorithms and Computer Imple-
mentations. Wiley, Chichester 1990
Papadimitriou, C.H., and Steiglitz, K. [1982]: Combinatorial Optimization; Algorithms and
Complexity. Prentice-Hall, Englewood Cliffs 1982, Sections 16.2, 17.3, and 17.4

424
17. The Knapsack Problem
Cited References:
Bellman, R. [1956]: Notes on the theory of dynamic programming IV – maximization over
discrete sets. Naval Research Logistics Quarterly 3 (1956), 67–70
Bellman, R. [1957]: Comment on Dantzig’s paper on discrete variable extremum problems.
Operations Research 5 (1957), 723–724
Blum, M., Floyd, R.W., Pratt, V., Rivest, R.L., and Tarjan, R.E. [1973]: Time bounds for
selection. Journal of Computer and System Sciences 7 (1973), 448–461
Dantzig, G.B. [1957]: Discrete variable extremum problems. Operations Research 5 (1957),
266–277
Garey, M.R., and Johnson, D.S. [1978]: Strong NP-completeness results: motivation, ex-
amples, and implications. Journal of the ACM 25 (1978), 499–508
Gens, G.V., and Levner, E.V. [1979]: Computational complexity of approximation algo-
rithms for combinatorial problems. In: Mathematical Foundations of Computer Science;
LNCS 74 (J. Becvar, ed.), Springer, Berlin 1979, pp. 292–300
Ibarra, O.H., and Kim, C.E. [1975]: Fast approximation algorithms for the knapsack and
sum of subset problem. Journal of the ACM 22 (1975), 463–468
Kellerer, H., and Pferschy, U. [2004]: Improved dynamic programming in connection with
an FPTAS for the knapsack problem. Journal on Combinatorial Optimization 8 (2004),
5–11
Korte, B., and Schrader, R. [1981]: On the existence of fast approximation schemes. In:
Nonlinear Programming; Vol. 4 (O. Mangaserian, R.R. Meyer, S.M. Robinson, eds.),
Academic Press, New York 1981, pp. 415–437
Lawler, E.L. [1979]: Fast approximation algorithms for knapsack problems. Mathematics
of Operations Research 4 (1979), 339–356
Pisinger, D. [1999]: Linear time algorithms for knapsack problems with bounded weights.
Journal of Algorithms 33 (1999), 1–14
Sahni, S. [1976]: Algorithms for scheduling independent tasks. Journal of the ACM 23
(1976), 114–127
Vygen, J. [1997]: The two-dimensional weighted median problem. Zeitschrift f¨ur Ange-
wandte Mathematik und Mechanik 77 (1997), Supplement, S433–S436
Woeginger, G.J. [2000]: When does a dynamic programming formulation guarantee the
existence of a fully polynomial time approximation scheme (FPTAS)? INFORMS Journal
on Computing 12 (2000), 57–74

18. Bin-Packing
Suppose we have n objects, each of a given size, and some bins of equal capacity.
We want to assign the objects to the bins, using as few bins as possible. Of course
the total size of the objects assigned to one bin should not exceed its capacity.
Without loss of generality, the capacity of the bins is 1. Then the problem can
be formulated as follows:
Bin-Packing Problem
Instance:
A list of nonnegative numbers a1, . . . , an ≤1.
Task:
Find a k ∈N and an assignment f : {1, . . . , n} →{1, . . . , k} with

i: f (i)= j ai ≤1 for all j ∈{1, . . . , k} such that k is minimum.
There are not many combinatorial optimization problems whose practical rel-
evance is more obvious. For example, the simplest version of the cutting stock
problem is equivalent: We are given many beams of equal length (say 1 meter)
and numbers a1, . . . , an. We want to cut as few of the beams as possible into
pieces such that at the end we have beams of lengths a1, . . . , an.
Although an instance I is some ordered list where numbers may appear more
than once, we write x ∈I for some element in the list I which is equal to x. By |I|
we mean the number of elements in the list I. We shall also use the abbreviation
SUM(a1, . . . , an) := n
i=1 ai. This is an obvious lower bound: ⌈SUM(I)⌉≤
OPT(I) holds for any instance I.
In Section 18.1 we prove that the Bin-Packing Problem is strongly NP-hard
and discuss some simple approximation algorithms. We shall see that no algorithm
can achieve a performance ratio better than 3
2 (unless P = NP). However, one can
achieve an arbitrary good performance ratio asymptotically: in Sections 18.2 and
18.3 we describe a fully polynomial asymptotic approximation scheme. This uses
the Ellipsoid Method and results of Chapter 17.
18.1 Greedy Heuristics
In this section we shall analyse some greedy heuristics for the Bin-Packing Prob-
lem. There is no hope for an exact polynomial-time algorithm as the problem is
NP-hard:

426
18. Bin-Packing
Theorem 18.1.
The following problem is NP-complete: given an instance I of the
Bin-Packing Problem, decide whether I has a solution with two bins.
Proof:
Membership in NP is trivial. We transform the Partition problem (which
is NP-complete: Corollary 15.28) to the above decision problem. Given an instance
c1, . . . , cn of Partition, consider the instance a1, . . . , an of the Bin-Packing
Problem, where
ai =
2ci
n
j=1 cj
.
Obviously two bins sufﬁce if and only if there is a subset S ⊆{1, . . . , n} such
that 
j∈S cj = 
j /∈S cj.
2
Corollary 18.2.
Unless P = NP, there is no ρ-factor approximation algorithm
for the Bin-Packing Problem for any ρ < 3
2.
2
For any ﬁxed k, there is a pseudopolynomial algorithm which decides for a
given instance I whether k bins sufﬁce (Exercise 1). However, in general this
problem is strongly NP-complete:
Theorem 18.3.
(Garey and Johnson [1975]) The following problem is strongly
NP-complete: given an instance I of the Bin-Packing Problem and a number B,
decide whether I can be solved with B bins.
Proof:
Transformation from 3-Dimensional Matching (Theorem 15.26).
Given an instance U, V, W, T of 3DM, we construct a bin-packing instance
I with 4|T | items. Namely, the set of items is
S :=

t=(u,v,w)∈T
{t, (u, t), (v, t), (w, t)}.
Let U = {u1, . . . , un}, V = {v1, . . . , vn} and W = {w1, . . . , wn}. For each x ∈
U
.
∪V
.
∪W we choose some tx ∈T such that (x, tx) ∈S. For each t =
(ui, vj, wk) ∈T , the sizes of the items are now deﬁned as follows:
t
has size 1
C (10N 4 + 8 −i N −j N 2 −kN 3)
(ui, t)
has size
 1
C (10N 4 + i N + 1)
if t = tui
1
C (11N 4 + i N + 1)
if t ̸= tui
(vj, t)
has size
 1
C (10N 4 + j N 2 + 2)
if t = tvj
1
C (11N 4 + j N 2 + 2)
if t ̸= tvj
(wk, t)
has size
 1
C (10N 4 + kN 3 + 4)
if t = twk
1
C (8N 4 + kN 3 + 4)
if t ̸= twk
where N
:= 100n and C
:= 40N 4 + 15. This deﬁnes an instance I
=
(a1, . . . , a4|T |) of the Bin-Packing Problem. We set B := |T | and claim that

18.1 Greedy Heuristics
427
I has a solution with at most B bins if and only if the initial 3DM instance is
a yes-instance, i.e. there is a subset M of T with |M| = n such that for distinct
(u, v, w), (u′, v′, w′) ∈M one has u ̸= u′, v ̸= v′ and w ̸= w′.
First assume that there is such a solution M of the 3DM instance. Since the
solvability of I with B bins is independent of the choice of the tx (x ∈U ∪V ∪W),
we may redeﬁne them such that tx ∈M for all x. Now for each t = (u, v, w) ∈T
we pack t, (u, t), (v, t), (w, t) into one bin. This yields a solution with |T | bins.
Conversely, let f be a solution of I with B = |T | bins. Since SUM(I) = |T |,
each bin must be completely full. Since all the item sizes are strictly between 1
5
and 1
3, each bin must contain four items.
Consider one bin k ∈{1, . . . , B}. Since C 
i: f (i)=k ai = C ≡15 (mod N),
the bin must contain one t = (u, v, w) ∈T , one (u′, t′) ∈U × T , one (v′, t′′) ∈
V × T , and one (w′, t′′′) ∈W × T . Since C 
i: f (i)=k ai = C ≡15 (mod N 2), we
have u = u′. Similarly, by considering the sum modulo N 3 and modulo N 4, we
obtain v = v′ and w = w′. Furthermore, either t′ = tu and t′′ = tv and t′′′ = tw
(case 1) or t′ ̸= tu and t′′ ̸= tv and t′′′ ̸= tw (case 2).
We deﬁne M to consist of those t ∈T for which t is assigned to a bin where
case 1 holds. Obviously M is a solution to the 3DM instance.
Note that all the numbers in the constructed bin-packing instance I are polyno-
mially large, more precisely O(n4). Since 3DM is strongly NP-complete (Theorem
15.26, there are no numbers in a 3DM instance), the theorem is proved.
2
This proof is due to Papadimitriou [1994]. Even with the assumption P ̸= NP
the above result does not exclude the possibility of an absolute approximation
algorithm, for example one which needs at most one more bin than the optimum
solution. Whether such an algorithm exists is an open question.
The ﬁrst algorithm one thinks of could be the following:
Next-Fit Algorithm (NF)
Input:
An instance a1, . . . , an of the Bin-Packing Problem.
Output:
A solution (k, f ).
1⃝
Set k := 1 and S := 0.
2⃝
For i := 1 to n do:
If S + ai > 1 then set k := k + 1 and S := 0.
Set f (i) := k and S := S + ai.
Let us denote by N F(I) the number k of bins this algorithm uses for instance I.
Theorem 18.4.
The Next-Fit Algorithm runs in O(n) time. For any instance
I = a1, . . . , an we have
N F(I) ≤2⌈SUM(I)⌉−1 ≤2 OPT(I) −1.
Proof:
The time bound is obvious. Let k := N F(I), and let f be the assignment
found by the Next-Fit Algorithm. For j = 1, . . . ,
 k
2

we have

428
18. Bin-Packing

i: f (i)∈{2 j−1,2 j}
ai > 1.
Adding these inequalities we get
2k
2
3
< SUM(I).
Since the left-hand side is an integer, we conclude that
k −1
2
≤
2k
2
3
≤⌈SUM(I)⌉−1.
This proves k ≤2⌈SUM(I)⌉−1. The second inequality is trivial.
2
The instances 2ϵ, 1 −ϵ, 2ϵ, 1 −ϵ, . . . , 2ϵ for very small ϵ > 0 show that this
bound is best possible. So the Next-Fit Algorithm is a 2-factor approximation
algorithm. Naturally the performance ratio becomes better if the numbers involved
are small:
Proposition 18.5.
Let 0 < γ < 1. For any instance I = a1, . . . , an with ai < γ
for all i ∈{1, . . . , n} we have
N F(I) ≤
0SUM(I)
1 −γ
1
.
Proof:
We have 
i: f (i)= j ai > 1 −γ for j = 1, . . . , N F(I) −1. By adding
these inequalities we get (N F(I) −1)(1 −γ ) < SUM(I) and thus
N F(I) −1 ≤
0SUM(I)
1 −γ
1
−1.
2
A second approach in designing an efﬁcient approximation algorithm could be
the following:
First-Fit Algorithm (FF)
Input:
An instance a1, . . . , an of the Bin-Packing Problem.
Output:
A solution (k, f ).
1⃝
For i := 1 to n do:
Set f (i) := min
⎧
⎨
⎩j ∈N :

h<i: f (h)= j
ah + ai ≤1
⎫
⎬
⎭.
2⃝
Set k :=
max
i∈{1,...,n} f (i).
Of course the First-Fit Algorithm cannot be worse than Next-Fit. So First-
Fit is another 2-factor approximation algorithm. Indeed, it is better:

18.1 Greedy Heuristics
429
Theorem 18.6.
(Johnson et al. [1974], Garey et al. [1976]) For all instances I
of the Bin-Packing Problem,
F F(I) ≤
017
10 OPT(I)
1
.
Furthermore, there exist instances I with OPT(I) arbitrarily large and
F F(I) ≥17
10(OPT(I) −1).
We omit the complicated proof. For some small values of OPT(I), the bound
F F(I) ≤7
4 OPT(I) by Simchi-Levi [1994] is better.
Proposition 18.5 shows that the Next-Fit (and thus the First-Fit) Algorithm
behaves well if the pieces are small. So it is natural to treat the large pieces ﬁrst.
The following modiﬁcation of the First-Fit Algorithm scans the n numbers in
decreasing order:
First-Fit-Decreasing Algorithm (FFD)
Input:
An instance a1, . . . , an of the Bin-Packing Problem.
Output:
A solution (k, f ).
1⃝
Sort the numbers such that a1 ≥a2 ≥. . . ≥an.
2⃝
Apply the First-Fit Algorithm.
Theorem 18.7.
(Simchi-Levi [1994])
The First-Fit-Decreasing Algorithm
is a 3
2-factor approximation algorithm for the Bin-Packing Problem.
Proof:
Let I be an instance and k := FFD(I). Consider the j-th bin for j :=
⌈2
3k⌉. If it contains an item of size > 1
2, then each bin with smaller index did not
have space for this item, thus has been assigned an item before. As the items are
considered in nonincreasing order, there are at least j items of size > 1
2. Thus
OPT(I) ≥j ≥2
3k.
Otherwise the j-th bin, and thus each bin with greater index, contains no item
of size > 1
2. Hence the bins j, j +1, . . . , k contain at least 2(k−j)+1 items, none
of which ﬁts into bins 1, . . . , j −1. Thus SUM(I) > min{ j −1, 2(k −j) + 1} ≥
min{⌈2
3k⌉−1, 2(k−( 2
3k+ 2
3))+1} = ⌈2
3k⌉−1 and OPT(I) ≥SUM(I) > ⌈2
3k⌉−1,
i.e. OPT(I) ≥⌈2
3k⌉.
2
By Corollary 18.2 this is best possible (indeed, consider the instance 0.4,
0.4, 0.3, 0.3, 0.3, 0.3). However, the asymptotic performance guarantee is better:
Johnson [1973] proved that FFD(I) ≤11
9 OPT(I) + 4 for all instances I (see
also Johnson [1974]). Baker [1985] gave a simpler proof showing FFD(I) ≤
11
9 OPT(I) + 3. The strongest result known is the following:

430
18. Bin-Packing
Theorem 18.8.
(Yue [1990]) For all instances I of the Bin-Packing Problem,
FFD(I) ≤11
9 OPT(I) + 1.
Yue’s proof is shorter than the earlier ones, but still too involved to be presented
here. However, we present a class of instances I with OPT(I) arbitrarily large and
FFD(I) = 11
9 OPT(I). (This example is taken from Garey and Johnson [1979].)
Namely, let ϵ > 0 be small enough and I = {a1, . . . , a30m} with
ai =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
1
2 + ϵ
if 1 ≤i ≤6m,
1
4 + 2ϵ
if 6m < i ≤12m,
1
4 + ϵ
if 12m < i ≤18m,
1
4 −2ϵ
if 18m < i ≤30m.
The optimum solution consists of
6m bins containing
1
2 + ϵ, 1
4 + ϵ, 1
4 −2ϵ,
3m bins containing
1
4 + 2ϵ, 1
4 + 2ϵ, 1
4 −2ϵ, 1
4 −2ϵ.
The FFD-solution consists of
6m bins containing
1
2 + ϵ, 1
4 + 2ϵ,
2m bins containing
1
4 + ϵ, 1
4 + ϵ, 1
4 + ϵ,
3m bins containing
1
4 −2ϵ, 1
4 −2ϵ, 1
4 −2ϵ, 1
4 −2ϵ.
So OPT(I) = 9m and FFD(I) = 11m.
There are several other algorithms for the Bin-Packing Problem, some of
them having a better asymptotic performance ratio than 11
9 . In the next section we
show that an asymptotic performance ratio arbitrarily close to 1 can be achieved.
In some applications one has to pack the items in the order they arrive without
knowing the subsequent items. Algorithms that do not use any information about
the subsequent items are called online algorithms. For example, Next-Fit and
First-Fit are online algorithms, but the First-Fit-Decreasing Algorithm is
not an online algorithm. The best known online algorithm for the Bin-Packing
Problem has an asymptotic performance ratio of 1.59 (Seiden [2002]). On the
other hand, van Vliet [1992] proved that there is no online asymptotic 1.54-factor
approximation algorithm for the Bin-Packing Problem. A weaker lower bound
is the subject of Exercise 5.

18.2 An Asymptotic Approximation Scheme
431
18.2 An Asymptotic Approximation Scheme
In this section we show that for any ϵ > 0 there is a linear-time algorithm which
guarantees to ﬁnd a solution with at most (1 + ϵ) OPT(I) + 1
ϵ2 bins.
We start by considering instances with not too many different numbers. We
denote the different numbers in our instance I by s1, . . . , sm. Let I contain exactly
bi copies of si (i = 1, . . . , m).
Let T1, . . . , TN be all the possibilities of how a single bin can be packed:
{T1, . . . , TN} :=

(k1, . . . , km) ∈Zm
+ :
m

i=1
kisi ≤1

We write Tj = (tj1, . . . , tjm). Then our Bin-Packing Problem is equivalent
to the following integer programming formulation (due to Eisemann [1957]):
min
N

j=1
xj
s.t.
N

j=1
tjixj
≥
bi
(i = 1, . . . , m)
xj
∈
Z+
( j = 1, . . . , N).
(18.1)
We actually want N
j=1 tjixj = bi, but relaxing this constraint makes no dif-
ference. The LP relaxation of (18.1) is:
min
N

j=1
xj
s.t.
N

j=1
tjixj
≥
bi
(i = 1, . . . , m)
xj
≥
0
( j = 1, . . . , N).
(18.2)
The following theorem says that by rounding a solution of the LP relaxation
(18.2) one obtains a solution of (18.1), i.e. of the Bin-Packing Problem, which
is not much worse:
Theorem 18.9.
(Fernandez de la Vega and Lueker [1981]) Let I be an instance
of the Bin-Packing Problem with only m different numbers. Let x be a feasible
(not necessarily optimum) solution of (18.2) with at most m nonzero components.
Then a solution of the Bin-Packing Problem with at most N
j=1 xj + m+1
2
bins
can be found in O(|I|) time.

432
18. Bin-Packing
Proof:
Consider ⌊x⌋, which results from x by rounding down each component.
⌊x⌋does not in general pack I completely (it might pack some numbers more
often than necessary, but this does not matter). The remaining pieces form an
instance I ′. Observe that
SUM(I ′) =
N

j=1
(xj −⌊xj⌋)
m

i=1
tjisi ≤
N

j=1
xj −
N

j=1
⌊xj⌋.
So it is sufﬁcient to pack I ′ into at most SUM(I ′) + m+1
2
bins, because then the
total number of bins used is no more than
N

j=1
⌊xj⌋+ SUM(I ′) + m + 1
2
≤
N

j=1
xj + m + 1
2
.
We consider two packing methods for I ′. Firstly, the vector ⌈x⌉−⌊x⌋certainly
packs at least the elements of I ′. The number of bins used is at most m since x
has at most m nonzero components. Secondly, we can obtain a packing of I ′
using at most 2⌈SUM(I ′)⌉−1 ≤2 SUM(I ′) + 1 bins by applying the Next-Fit
Algorithm (Theorem 18.4). Both packings can be obtained in linear time.
The better of these two packings uses at most min{m, 2 SUM(I ′) + 1} ≤
SUM(I ′) + m+1
2
bins. The theorem is proved.
2
Corollary 18.10.
(Fernandez de la Vega and Lueker [1981]) Let m and γ > 0
be ﬁxed constants. Let I be an instance of the Bin-Packing Problem with only m
different numbers, none of which is less than γ . Then we can ﬁnd a solution with
at most OPT(I) + m+1
2
bins in O(|I|) time.
Proof:
By the Simplex Algorithm (Theorem 3.13) we can ﬁnd an optimum
basic solution x∗of (18.2), i.e. a vertex of the polyhedron. Since any vertex
satisﬁes N of the constraints with equality (Proposition 3.8), x∗has at most m
nonzero components.
The time needed to determine x∗depends on m and N only. Observe that
N ≤(m + 1)
1
γ , because there can be at most 1
γ elements in each bin. So x∗can
be found in constant time.
Since N
j=1 x∗
j ≤OPT(I), an application of Theorem 18.9 completes the proof.
2
Using the Ellipsoid Method (Theorem 4.18) leads to the same result. This
is not best possible: one can even determine the exact optimum in polynomial
time for ﬁxed m and γ , since Integer Programming with a constant number of
variables can be solved in polynomial time (Lenstra [1983]). However, this would
not help us substantially. We shall apply Theorem 18.9 again in the next section
and obtain the same performance guarantee in polynomial time even if m and γ
are not ﬁxed (in the proof of Theorem 18.14).
We are now able to formulate the algorithm of Fernandez de la Vega and
Lueker [1981]. Roughly it proceeds as follows. First we distribute the n numbers

18.2 An Asymptotic Approximation Scheme
433
into m + 2 groups according to their size. We pack the group with the largest
ones using one bin for each number. Then we pack the m middle groups by ﬁrst
rounding the size of each number to the largest number in its group and then
applying Corollary 18.10. Finally we pack the group with the smallest numbers.
Fernandez-de-la-Vega-Lueker Algorithm
Input:
An instance I = a1, . . . , an of the Bin-Packing Problem. A number
ϵ > 0.
Output:
A solution (k, f ) for I.
1⃝
Set γ :=
ϵ
ϵ+1 and h := ⌈ϵ SUM(I)⌉.
2⃝
Let I1 = L, M, R be a rearrangement of the list I, where
M = K0, y1, K1, y2, . . . , Km−1, ym and L, K0, K1, . . . , Km−1 and R are
again lists, such that the following properties hold:
(a)
For all x ∈L: x < γ .
(b)
For all x ∈K0: γ ≤x ≤y1.
(c)
For all x ∈Ki: yi ≤x ≤yi+1
(i = 1, . . . , m −1).
(d)
For all x ∈R: ym ≤x.
(e)
|K1| = · · · = |Km−1| = |R| = h −1 and |K0| ≤h −1.
(k, f ) is now determined by the following three packing steps:
3⃝
Find a packing SR of R using |R| bins.
4⃝
Consider the instance Q consisting of the numbers y1, y2, . . . , ym, each
appearing h times. Find a packing SQ of Q using at most m
2 + 1
more bins than necessary (using Corollary 18.10). Transform SQ into a
packing SM of M.
5⃝
As long as a bin of SR or SM has room amounting to at least γ , ﬁll it
with elements of L. Finally, ﬁnd a packing of the rest of L using the
Next-Fit Algorithm.
In
4⃝we used a slightly weaker bound than the one obtained in Corollary
18.10. This does not hurt here, and we shall need the above form in Section 18.3.
The above algorithm is an asymptotic approximation scheme. More precisely:
Theorem 18.11.
(Fernandez de la Vega and Lueker [1981]) For each 0 < ϵ ≤1
2
and each instance I of the Bin-Packing Problem, the Fernandez-de-la-Vega-
Lueker Algorithm returns a solution using at most (1 + ϵ) OPT(I) + 1
ϵ2 bins.
The running time is O(n 1
ϵ2 ) plus the time needed to solve (18.2). For ﬁxed ϵ, the
running time is O(n).
Proof:
In 2⃝, we ﬁrst determine L in O(n) time. Then we set m :=

|I|−|L|
h

.
Since γ (|I| −|L|) ≤SUM(I), we have
m ≤|I| −|L|
h
≤
|I| −|L|
ϵ SUM(I) ≤
1
γ ϵ = ϵ + 1
ϵ2
.

434
18. Bin-Packing
We know that yi must be the (|I| + 1 −(m −i + 1)h)-th smallest element
(i = 1, . . . , m). So by Corollary 17.4 we can ﬁnd each yi in O(n) time. We ﬁnally
determine K0, K1, . . . , Km−1, R, each in O(n) time. So 2⃝can be done in O(mn)
time. Note that m = O( 1
ϵ2 ).
Steps 3⃝, 4⃝and 5⃝– except the solution of (18.2) – can easily be implemented
to run in O(n) time. For ﬁxed ϵ, (18.2) can also be solved optimally in O(n) time
(Corollary 18.10).
We now prove the performance guarantee. Let k be the number of bins that
the algorithm uses. We write |SR| and |SM| for the number of bins used in the
packing of R and M, respectively.
We have
|SR| ≤|R| = h −1 < ϵ SUM(I) ≤ϵ OPT(I).
Secondly, observe that OPT(Q) ≤OPT(I): the i-th largest element of I is
greater than or equal to the i-th largest element of Q for all i = 1, . . . , hm. Hence
by 4⃝(Corollary 18.10) we have
|SM| = |SQ| ≤OPT(Q) + m
2 + 1 ≤OPT(I) + m
2 + 1.
In 5⃝we can pack some elements of L into bins of SR and SM. Let L′ be the
list of the remaining elements in L.
Case 1:
L′ is nonempty. Then the total size of the elements in each bin, except
possibly for the last one, exceeds 1 −γ , so we have (1 −γ )(k −1) < SUM(I) ≤
OPT(I). We conclude that
k ≤
1
1 −γ OPT(I) + 1 = (1 + ϵ) OPT(I) + 1.
Case 2:
L′ is empty. Then
k
≤
|SR| + |SM|
<
ϵ OPT(I) + OPT(I) + m
2 + 1
≤
(1 + ϵ) OPT(I) + 2ϵ2 + ϵ + 1
2ϵ2
≤
(1 + ϵ) OPT(I) + 1
ϵ2 ,
because ϵ ≤1
2.
2
Of course the running time grows exponentially in 1
ϵ . However, Karmarkar and
Karp showed how to obtain a fully polynomial asymptotic approximation scheme.
This is the subject of the next section.

18.3 The Karmarkar-Karp Algorithm
435
18.3 The Karmarkar-Karp Algorithm
The algorithm of Karmarkar and Karp [1982] works just as the algorithm in the
preceding section, but instead of solving the LP relaxation (18.2) optimally as in
Corollary 18.10, it is solved with a constant absolute error.
The fact that the number of variables grows exponentially in
1
ϵ might not
prevent us from solving the LP: Gilmore and Gomory [1961] developped the
column generation technique and obtained a variant of the Simplex Algorithm
which solves (18.2) quite efﬁciently in practice. Similar ideas lead to a theoretically
efﬁcient algorithm if one uses the Gro¨tschel-Lov´asz-Schrijver Algorithm
instead.
In both above-mentioned approaches the dual LP plays a major role. The dual
of (18.2) is:
max
yb
s.t.
m

i=1
tji yi
≤
1
( j = 1, . . . , N)
yi
≥
0
(i = 1, . . . , m).
(18.3)
It has only m variables, but an exponential number of constraints. However,
the number of constraints does not matter as long as we can solve the Separation
Problem in polynomial time. It will turn out that the Separation Problem is
equivalent to a Knapsack Problem. Since we can solve Knapsack Problems
with an arbitrarily small error, we can also solve the Weak Separation Problem
in polynomial time. This idea enables us to prove:
Lemma 18.12.
(Karmarkar and Karp [1982]) Let I be an instance of the Bin-
Packing Problem with only m different numbers, none of which is less than γ . Let
δ > 0. Then a feasible solution y∗of the dual LP (18.3) differing from the optimum
by at most δ can be found in O

m6 log2 mn
γ δ + m5n
δ log mn
γ δ

time.
Proof:
We may assume that δ = 1
p for some natural number p. We apply the
Gro¨tschel-Lov´asz-Schrijver Algorithm (Theorem 4.19). Let D be the poly-
hedron of (18.3). We have
B

x0, γ
2

⊆[0, γ ]m ⊆D ⊆[0, 1]m ⊆B(x0, √m),
where x0 is the vector all of whose components are γ
2 .
We shall prove that we can solve the Weak Separation Problem for (18.3),
i.e. D and b, and δ
2 in O
 nm
δ

time, independently of the size of the input vector
y. By Theorem 4.19, this implies that the Weak Optimization Problem can be
solved in O

m6 log2 m||b||
γ δ
+ m5n
δ log m||b||
γ δ

time, proving the lemma since ||b|| ≤
n.

436
18. Bin-Packing
To show how to solve the Weak Separation Problem, let y ∈Qm be given.
We may assume 0 ≤y ≤1 since otherwise the task is trivial. Now observe that
y is feasible if and only if
max{yx : x ∈Zm
+, xs ≤1} ≤1,
(18.4)
where s = (s1, . . . , sm) is the vector of the item sizes.
(18.4) is a kind of Knapsack Problem, so we cannot hope to solve it exactly.
But this is not necessary, as the Weak Separation Problem only calls for an
approximate solution.
Write y′ := ⌊2n
δ y⌋(the rounding is done componentwise). The problem
max{y′x : x ∈Zm
+, xs ≤1}
(18.5)
can be solved optimally by dynamic programming, very similarly to the Dynamic
Programming Knapsack Algorithm in Section 17.2 (see Exercise 6 of Chapter
17): Let F(0) := 0 and
F(k) := min{F(k −y′
i) + si : i ∈{1, . . . , m}, y′
i ≤k}
for k = 1, . . . , 4n
δ . F(k) is the minimum size of a set of items with total cost k
(with respect to y′).
Now the maximum in (18.5) is less than or equal to 2n
δ if and only if F(k) > 1
for all k ∈{ 2n
δ + 1, . . . , 4n
δ }. The total time needed to decide this is O
 mn
δ

. There
are two cases:
Case 1:
The maximum in (18.5) is less than or equal to 2n
δ . Then
δ
2n y′ is a
feasible solution of (18.3). Furthermore, by −b δ
2n y′ ≤b δ
2n 1l = δ
2. The task of the
Weak Separation Problem is done.
Case 2:
There exists an x ∈Zm
+ with xs ≤1 and y′x > 2n
δ . Such an x can easily
be computed from the numbers F(k) in O
 mn
δ

time. We have yx ≥
δ
2n y′x > 1.
Thus x corresponds to a bin conﬁguration that proves that y is infeasible. Since
we have zx ≤1 for all z ∈D, this is a separating hyperplane, and thus we are
done.
2
Lemma 18.13.
(Karmarkar and Karp [1982]) Let I be an instance of the Bin-
Packing Problem with only m different numbers, none of which is less than γ .
Let δ > 0. Then a feasible solution x of the primal LP (18.2) differing from the
optimum by at most δ and having at most m nonzero components can be found in
time polynomial in n, m, 1
δ and 1
γ .
Proof:
We ﬁrst solve the dual LP (18.3) approximately, using Lemma 18.12. We
obtain a vector y∗with y∗b ≥OPT(18.3) −δ. Now let Tk1, . . . , TkN′ be those bin
conﬁgurations that appeared as a separating hyperplane in Case 2 of the previous
proof, plus the unit vectors (the bin conﬁgurations containing just one element).
Note that N ′ is bounded by the number of iterations in the Gro¨tschel-Lov´asz-
Schrijver Algorithm (Theorem 4.19), so N ′ = O

m2 log mn
γ δ

.

18.3 The Karmarkar-Karp Algorithm
437
Consider the LP
max
yb
s.t.
m

i=1
tkji yi
≤
1
( j = 1, . . . , N ′)
yi
≥
0
(i = 1, . . . , m).
(18.6)
Observe that the above procedure for (18.3) (in the proof of Lemma 18.12)
is also a valid application of the Gro¨tschel-Lov´asz-Schrijver Algorithm for
(18.6): the oracle for the Weak Separation Problem can always give the same
answer as above. Therefore we have y∗b ≥OPT(18.6) −δ. Consider
min
N ′

j=1
xkj
s.t.
N ′

j=1
tkjixkj
≥
bi
(i = 1, . . . , m)
xkj
≥
0
( j = 1, . . . , N ′)
(18.7)
which is the dual of (18.6). The LP (18.7) arises from (18.2) by eliminating the
variables xj for j ∈{1, . . . , N} \ {k1, . . . , kN ′} (forcing them to be zero). In other
words, only N ′ of the N bin conﬁgurations can be used.
We have
OPT(18.7) −δ = OPT(18.6) −δ ≤y∗b ≤OPT(18.3) = OPT(18.2).
So it is sufﬁcient to solve (18.7). But (18.7) is an LP of polynomial size: it
has N ′ variables and m constraints; none of the entries of the matrix is larger
than
1
γ , and none of the entries of the right-hand side is larger than n. So by
Khachiyan’s Theorem 4.18, it can be solved in polynomial time. We obtain an
optimum basic solution x (x is a vertex of the polyhedron, so x has at most m
nonzero components).
2
Now we apply the Fernandez-de-la-Vega-Lueker Algorithm with just
one modiﬁcation: we replace the exact solution of (18.2) by an application of
Lemma 18.13. We summarize:
Theorem 18.14.
(Karmarkar and Karp [1982])
There is a fully polynomial
asymptotic approximation scheme for the Bin-Packing Problem.
Proof:
We apply Lemma 18.13 with δ = 1
2, obtaining an optimum solution x of
(18.7) with at most m nonzero components. We have 1lx ≤OPT(18.2) + 1
2. An
application of Theorem 18.9 yields an integral solution using at most OPT(18.2)+
1
2 + m+1
2
bins, as required in 4⃝of the Fernandez-de-la-Vega-Lueker Algo-
rithm.

438
18. Bin-Packing
So the statement of Theorem 18.11 remains valid. Since m ≤
2
ϵ2 and 1
γ ≤2
ϵ
(we may assume ϵ ≤1), the running time for ﬁnding x is polynomial in n and 1
ϵ .
2
The running time obtained this way is worse than O

ϵ−40
and completely out
of the question for practical purposes. Karmarkar and Karp [1982] showed how to
reduce the number of variables in (18.7) to m (while changing the optimum value
only slightly) and thereby improve the running time (see Exercise 9). Plotkin,
Shmoys and Tardos [1995] achieved a running time of O(n log ϵ−1 +ϵ−6 log ϵ−1).
Many generalizations have been considered. The two-dimensional bin packing
problem, asking for packing a given set of axis-parallel rectangles into a minimum
number of unit squares without rotation, does not have an asymptotic approxima-
tion scheme unless P = NP (Bansal and Sviridenko [2004]). See Caprara [2002]
and the references therein for related results.
Exercises
1. Let k be ﬁxed. Describe a pseudopolynomial algorithm which – given an
instance I of the Bin-Packing Problem – ﬁnds a solution for this instance
using no more than k bins or decides that no such solution exists.
2. Suppose that in an instance a1, . . . , an of the Bin-Packing Problem we have
ai > 1
3 for each i. Reduce the problem to the Cardinality Matching Prob-
lem. Then show how to solve it in linear time.
3. Find an instance I of the Bin-Packing Problem, where F F(I) = 17 while
OPT(I) = 10.
4. Implement the First-Fit Algorithm and the First-Fit-Decreasing Algo-
rithm to run in O(n log n) time.
5. Show that there is no online 4
3-factor approximation algorithm for the Bin-
Packing Problem unless P = NP.
Hint: Consider the list consisting of n elements of size 1
2 −ϵ followed by n
elements of size 1
2 + ϵ.
6. Show that 2⃝of the Fernandez-de-la-Vega-Lueker Algorithm can be
implemented to run in O

n log 1
ϵ

time.
7.
∗
Prove that for any ϵ > 0 there exists a polynomial-time algorithm which for
any instance I = (a1, . . . , an) of the Bin-Packing Problem ﬁnds a packing
using the optimum number of bins but may violate the capacity constraints
by ϵ, i.e. an f : {1, . . . , n} →{1, . . . , OPT(I)} with 
f (i)= j ai ≤1 + ϵ for
all j ∈{1, . . . , k}.
Hint: Use ideas of Section 18.2.
(Hochbaum and Shmoys [1987])
8. Consider the following Multiprocessor Scheduling Problem: Given a ﬁ-
nite set A of tasks, a positive number t(a) for each a ∈A (the processing time),
and a number m of processors. Find a partition A = A1
.
∪A2
.
∪· · ·
.
∪Am of
A into m disjoint sets such that maxm
i=1

a∈Ai t(a) is minimum.

References
439
(a) Show that this problem is strongly NP-hard.
(b) Show that a greedy algorithm which successively assigns jobs (in arbitrary
order) to the currently least used machine is a 2-factor approximation
algorithm.
(c) Show that for each ﬁxed m the problem has a fully polynomial approxi-
mation scheme.
(Horowitz and Sahni [1976])
(d)
∗
Use Exercise 7 to show that the Multiprocessor Scheduling Problem
has an approximation scheme.
(Hochbaum and Shmoys [1987])
Note: This problem has been the subject of the ﬁrst paper on approximation
algorithms (Graham [1966]). Many variations of scheduling problems have
been studied; see e.g. (Graham et al. [1979]) or (Lawler et al. [1993]).
9.
∗
Consider the LP (18.6) in the proof of Lemma 18.13. All but m constraints
can be thrown away without changing its optimum value. We are not able to
ﬁnd these m constraints in polynomial time, but we can ﬁnd m constraints
such that deleting all the others does not increase the optimum value too much
(say not more than by one). How?
Hint: Let D(0) be the LP (18.6) and iteratively construct LPs D(1), D(2), . . .
by deleting more and more constraints. At each iteration, a solution y(i) of
D(i) is given with by(i) ≥OPT

D(i)
−δ. The set of constraints is partitioned
into m + 1 sets of approximately equal size, and for each of the sets we
test whether the set can be deleted. This test is performed by considering the
LP after deletion, say D, and applying the Gro¨tschel-Lov´asz-Schrijver
Algorithm. Let y be a solution of D with by ≥OPT

D

−δ. If by ≤
by(i) + δ, the test is successful, and we set D(i+1) := D and y(i+1) := y.
Choose δ appropriately.
(Karmarkar and Karp [1982])
10.
∗
Find an appropriate choice of ϵ as a function of SUM(I), such that the re-
sulting modiﬁcation of the Karmarkar-Karp Algorithm is a polynomial-
time algorithm which guarantees to ﬁnd a solution with at most OPT(I) +
O

OPT(I) log log OPT(I)
log OPT(I)

= OPT(I) + o(OPT(I)) bins.
(Johnson [1982])
References
General Literature:
Coffman, E.G., Garey, M.R., and Johnson, D.S. [1996]: Approximation algorithms for
bin-packing; a survey. In: Approximation Algorithms for NP-Hard Problems (D.S.
Hochbaum, ed.), PWS, Boston, 1996
Cited References:
Baker, B.S. [1985]: A new proof for the First-Fit Decreasing bin-packing algorithm. Journal
of Algorithms 6 (1985), 49–70

440
18. Bin-Packing
Bansal, N., and Sviridenko, M. [2004]: New approximability and inapproximability results
for 2-dimensional bin packing. Proceedings of the 15th Annual ACM-SIAM Symposium
on Discrete Algorithms (2004), 196–203
Caprara, A. [2002]: Packing 2-dimensional bins in harmony. Proceedings of the 43rd Annual
IEEE Symposium on Foundations of Computer Science (2002), 490–499
Eisemann, K. [1957]: The trim problem. Management Science 3 (1957), 279–284
Fernandez de la Vega, W., and Lueker, G.S. [1981]: Bin packing can be solved within 1+ϵ
in linear time. Combinatorica 1 (1981), 349–355
Garey, M.R., Graham, R.L., Johnson, D.S., and Yao, A.C. [1976]: Resource constrained
scheduling as generalized bin packing. Journal of Combinatorial Theory A 21 (1976),
257–298
Garey, M.R., and Johnson, D.S. [1975]: Complexity results for multiprocessor scheduling
under resource constraints. SIAM Journal on Computing 4 (1975), 397–411
Garey, M.R., and Johnson, D.S. [1979]: Computers and Intractability; A Guide to the
Theory of NP-Completeness. Freeman, San Francisco 1979, p. 127
Gilmore, P.C., and Gomory, R.E. [1961]: A linear programming approach to the cutting-
stock problem. Operations Research 9 (1961), 849–859
Graham, R.L. [1966]: Bounds for certain multiprocessing anomalies. Bell Systems Technical
Journal 45 (1966), 1563–1581
Graham, R.L., Lawler, E.L., Lenstra, J.K., and Rinnooy Kan, A.H.G. [1979]: Optimization
and approximation in deterministic sequencing and scheduling: a survey. In: Discrete
Optimization II; Annals of Discrete Mathematics 5 (P.L. Hammer, E.L. Johnson, B.H.
Korte, eds.), North-Holland, Amsterdam 1979, pp. 287–326
Hochbaum, D.S., and Shmoys, D.B. [1987]: Using dual approximation algorithms for
scheduling problems: theoretical and practical results. Journal of the ACM 34 (1987),
144–162
Horowitz, E., and Sahni, S.K. [1976]: Exact and approximate algorithms for scheduling
nonidentical processors. Journal of the ACM 23 (1976), 317–327
Johnson, D.S. [1973]: Near-Optimal Bin Packing Algorithms. Doctoral Thesis, Dept. of
Mathematics, MIT, Cambridge, MA, 1973
Johnson, D.S. [1974]: Fast algorithms for bin-packing. Journal of Computer and System
Sciences 8 (1974), 272–314
Johnson, D.S. [1982]: The NP-completeness column; an ongoing guide. Journal of Algo-
rithms 3 (1982), 288–300, Section 3
Johnson, D.S., Demers, A., Ullman, J.D., Garey, M.R., and Graham, R.L. [1974]: Worst-
case performance bounds for simple one-dimensional packing algorithms. SIAM Journal
on Computing 3 (1974), 299–325
Karmarkar, N., and Karp, R.M. [1982]: An efﬁcient approximation scheme for the one-
dimensional bin-packing problem. Proceedings of the 23rd Annual IEEE Symposium on
Foundations of Computer Science (1982), 312–320
Lawler, E.L., Lenstra, J.K., Rinnooy Kan, A.H.G., and Shmoys, D.B. [1993]: Sequencing
and scheduling: algorithms and complexity. In: Handbooks in Operations Research and
Management Science; Vol. 4 (S.C. Graves, A.H.G. Rinnooy Kan, P.H. Zipkin, eds.),
Elsevier, Amsterdam 1993
Lenstra, H.W. [1983]: Integer Programming with a ﬁxed number of variables. Mathematics
of Operations Research 8 (1983), 538–548
Papadimitriou, C.H. [1994]: Computational Complexity. Addison-Wesley, Reading 1994,
pp. 204–205
Plotkin, S.A., Shmoys, D.B., and Tardos, ´E. [1995] Fast approximation algorithms for frac-
tional packing and covering problems. Mathematics of Operations Research 20 (1995),
257–301
Seiden, S.S. [2002]: On the online bin packing problem. Journal of the ACM 49 (2002),
640–671

References
441
Simchi-Levi, D. [1994]: New worst-case results for the bin-packing problem. Naval Re-
search Logistics 41 (1994), 579–585
van Vliet, A. [1992]: An improved lower bound for on-line bin packing algorithms. Infor-
mation Processing Letters 43 (1992), 277–284
Yue, M. [1990]: A simple proof of the inequality FFD(L) ≤11
9 OPT(L)+1, ∀L for the FFD
bin-packing algorithm. Report No. 90665, Research Institute for Discrete Mathematics,
University of Bonn, 1990

19. Multicommodity Flows and Edge-Disjoint Paths
The Multicommodity Flow Problem is a generalization of the Maximum Flow
Problem. Given a digraph G with capacities u, we now ask for an s-t-ﬂow for
several pairs (s, t) (we speak of several commodities), such that the total ﬂow
through any edge does not exceed the capacity. We model the pairs (s, t) by a
second digraph; for technical reasons we have an edge from t to s when we ask
for an s-t-ﬂow. Formally we have:
Directed Multicommodity Flow Problem
Instance:
A pair (G, H) of digraphs on the same vertices.
Capacities u : E(G) →R+ and demands b : E(H) →R+.
Task:
Find a family (x f )f ∈E(H), where x f is an s-t-ﬂow of value b( f ) in
G for each f = (t, s) ∈E(H), and

f ∈E(H)
x f (e) ≤u(e)
for all e ∈E(G).
There is also an undirected version which we shall discuss later. Again, the
edges of G are called supply edges, the edges of H demand edges. If u ≡1,
b ≡1 and x is forced to be integral, we have the the Edge-Disjoint Paths
Problem. Sometimes one also has edge weights and asks for a minimum cost
multicommodity ﬂow. But here we are only interested in feasible solutions.
Of course, the problem can be solved in polynomial time by means of Linear
Programming (cf. Theorem 4.18). However the LP formulations are quite large,
so it is also interesting that we have a combinatorial algorithm for solving the
problem approximately; see Section 19.2. This algorithm uses an LP formulation
as a motivation. Moreover, LP duality yields a useful good characterization of
our problem as shown in Section 19.1. This leads to necessary (but in general not
sufﬁcient) conditions for the Edge-Disjoint Paths Problem.
In many applications one is interested in integral ﬂows, or paths, and the
Edge-Disjoint Paths Problem is the proper formulation.
We have considered a special case of this problem in Section 8.2, where we had
a necessary and sufﬁcient condition for the existence of k edge-disjoint (or vertex-
disjoint) paths from s to t for two given vertices s and t (Menger’s Theorems
8.9 and 8.10). We shall prove that the general Edge-Disjoint Paths Problem

444
19. Multicommodity Flows and Edge-Disjoint Paths
problem is NP-hard, both in the directed and undirected case. Nevertheless there
are some interesting special cases that can be solved in polynomial time, as we
shall see in Sections 19.3 and 19.4.
19.1 Multicommodity Flows
We concentrate on the Directed Multicommodity Flow Problem but mention
that all results of this section also hold for the undirected version:
Undirected Multicommodity Flow Problem
Instance:
A pair (G, H) of undirected graphs on the same vertices.
Capacities u : E(G) →R+ and demands b : E(H) →R+.
Task:
Find a family (x f )f ∈E(H), where x f is an s-t-ﬂow of value b( f )
in (V (G), {(v, w), (w, v) : {v, w} ∈E(G)}) for each f = {t, s} ∈
E(H), and

f ∈E(H)

x f ((v, w)) + x f ((w, v))

≤u(e)
for all e = {v, w} ∈E(G).
Both versions of the Multicommodity Flow Problem have a natural for-
mulation as an LP (cf. the LP formulation of the Maximum Flow Problem in
Section 8.1). Hence they can be solved in polynomial time (Theorem 4.18). To-
day polynomial-time algorithms which do not use Linear Programming are known
only for some special cases.
We shall now mention a different LP formulation of the Multicommodity
Flow Problem which will prove useful:
Lemma 19.1.
Let (G, H, u, b) be an instance of the (Directed or Undirected)
MulticommodityFlowProblem. Let C be the set of circuits of G+H that contain
exactly one demand edge. Let M be a 0-1-matrix whose columns correspond to the
elements of C and whose rows correspond to the edges of G, where Me,C = 1 iff
e ∈C. Similarly, let N be a 0-1-matrix whose columns correspond to the elements
of C and whose rows correspond to the edges of H, where Nf,C = 1 iff f ∈C.
Then each solution of the Multicommodity Flow Problem corresponds to at
least one point in the polytope
5
y ∈RC : y ≥0, My ≤u, Ny = b
6
,
(19.1)
and each point in this polytope corresponds to a unique solution of the Multicom-
modity Flow Problem.
Proof:
To simplify our notation we consider the directed case only; the undi-
rected case follows by substituting each undirected edge by the subgraph shown
in Figure 8.2.

19.1 Multicommodity Flows
445
Let (x f )f ∈E(H) be a solution of the Multicommodity Flow Problem. For
each f = (t, s) ∈E(H) the s-t-ﬂow x f can be decomposed into a set P of
s-t-paths and a set Q of circuits (Theorem 8.8): for each demand edge f we can
write
x f (e) =

P∈P∪Q: e∈E(P)
w(P)
for e ∈E(G), where w : P ∪Q →R+. We set yP+ f := w(P) for P ∈P and
yC := 0 for f ∈C ∈C with C −f ̸∈P. This obviously yields a vector y ≥0
with My ≤u and Ny = b.
Conversely, let y ≥0 with My ≤u and Ny = b. Setting
x f (e) :=

C∈C: e, f ∈E(C)
yC
yields a solution of the Multicommodity Flow Problem.
2
With the help of LP duality we can now derive a necessary and sufﬁcient
condition for the solvability of the Multicommodity Flow Problem. We shall
also mention the connection to the Edge-Disjoint Paths Problem.
Deﬁnition 19.2.
An instance (G, H) of the (Directed or Undirected) Edge-
Disjoint Paths Problem satisﬁes the distance criterion if for each z : E(G) →
R+

f =(t,s)∈E(H)
dist(G,z)(s, t) ≤

e∈E(G)
z(e).
(19.2)
An instance (G, H, u, b) of the Multicommodity Flow Problem satisﬁes the
distance criterion if for each z : E(G) →R+

f =(t,s)∈E(H)
b( f ) dist(G,z)(s, t) ≤

e∈E(G)
u(e)z(e).
(In the undirected case, (t, s) must be replaced by {t, s}.)
The left-hand side of the distance criterion can be interpreted as a lower bound
on the cost of a solution (with respect to edge costs z), while the right-hand side
is an upper bound on the maximum possible cost.
Theorem 19.3.
The distance criterion is necessary and sufﬁcient for the solvabil-
ity of the Multicommodity Flow Problem (in both the directed and the undi-
rected case).
Proof:
We again consider only the directed case, the undirected case follows
via the substitution of Figure 8.2. By Lemma 19.1, the Multicommodity Flow
Problem has a solution if and only if the polyhedron
5
y ∈RC
+ : My ≤u, Ny = b
6
is nonempty. By Corollary 3.20, this polyhedron is empty if and only if there are
vectors z, w with z ≥0, zM + wN ≥0 and zu + wb < 0. (M and N are deﬁned
as above.)

446
19. Multicommodity Flows and Edge-Disjoint Paths
The inequality zM + wN ≥0 implies
−wf
≤

e∈P
ze
for each demand edge f
= (t, s) and each s-t-path P in G, so −wf
≤
dist(G,z)(s, t). Hence there exist vectors z, w with z ≥0, zM + wN ≥0 and
zu + wb < 0 if and only if there exists a vector z ≥0 with
zu −

f =(t,s)∈E(H)
dist(G,z)(s, t) b( f ) < 0.
This completes the proof.
2
In Section 19.2 we shall show how the LP description of Lemma 19.1 and its
dual can be used to design an algorithm for the Multicommodity Flow Problem.
Theorem 19.3 implies that the distance criterion is necessary for the solvability
of the Edge-Disjoint Paths Problem, since this can be considered as a Multi-
commodity Flow Problem with b ≡1, u ≡1 and with integrality constraints.
Another important necessary condition is the following:
Deﬁnition 19.4.
An instance (G, H) of the (Directed or Undirected) Edge-
Disjoint Paths Problem satisﬁes the cut criterion if for each X ⊆V (G)
–
|δ+
G(X)| ≥|δ−
H(X)|
in the directed case, or
–
|δG(X)| ≥|δH(X)|
in the undirected case.
Corollary 19.5.
For an instance (G, H) of the (Directed or Undirected)
Edge-Disjoint Paths Problem, the following implications hold: (G, H) has a
solution ⇒(G, H) satisﬁes the distance criterion ⇒(G, H) satisﬁes the cut cri-
terion.
Proof:
The ﬁrst implication follows from Theorem 19.3. For the second impli-
cation observe that the cut criterion is just a special case of the distance criterion,
where weight functions of the type
z(e) :=
 1
if e ∈δ+(X) (directed case) or e ∈δ(X) (undirected case)
0
otherwise
for X ⊆V (G) are considered.
2
None of the implications can be reversed in general. Figure 19.1 shows exam-
ples where there is no (integral) solution but there is a fractional solution, i.e. a
solution of the multicommodity ﬂow relaxation. So here the distance criterion is
satisﬁed. In the ﬁgures of this section demand edges are indicated by equal num-
bers at their endpoints. In the directed case, one should orient the demand edges
so that they are realizable. (A demand edge (t, s) or {t, s} is called realizable if
t is reachable from s in the supply graph.)
The two examples shown in Figure 19.2 satisfy the cut criterion (this is easily
checked), but not the distance criterion: in the undirected example choose z(e) = 1
for all e ∈E(G), in the directed example choose z(e) = 1 for the bold edges and
z(e) = 0 otherwise.

19.2 Algorithms for Multicommodity Flows
447
1
2
2
1
2
1
1
2
(a)
(b)
Fig. 19.1.
2, 4
3, 4
3
2
1
1
1, 3
4
5
1
2
4
2, 3
5
(a)
(b)
Fig. 19.2.
19.2 Algorithms for Multicommodity Flows
The deﬁnition of the MulticommodityFlowProblem directly gives rise to an LP
formulation of polynomial size. Although this yields a polynomial-time algorithm
it cannot be used for solving large instances: the number of variables is enormous.
The LP description (19.1) given by Lemma 19.1 looks even worse since it has an
exponential number of variables. Nevertheless this description proves much more
useful in practice. We shall explain this now.
Since we are interested in a feasible solution only, we consider the LP
max{0y : y ≥0, My ≤u, Ny = b}
and its dual min{zu + wb : z ≥0, zM + wN ≥0} which we can rewrite as
min{zu + wb : z ≥0, dist(G,z)(s, t) ≥−w( f ) for all f = (t, s) ∈E(H)}.
(In the undirected case replace (t, s) by {t, s}.) This dual LP has only |E(G)| +
|E(H)| variables but an exponential number of constraints. However, this is not
important since the Separation Problem can be solved by |E(H)| shortest path
computations; as only nonnegative vectors z have to be considered, we can use
Dijkstra’s Algorithm here. If the dual LP is unbounded, then this proves infea-
sibility of the primal LP. Otherwise we can solve the dual LP, but this does not
provide a primal solution in general.

448
19. Multicommodity Flows and Edge-Disjoint Paths
Ford and Fulkerson [1958] suggested to use the above consideration to solve
the primal LP directly, in combination with the Simplex Algorithm. Since most
variables are zero at each iteration of the Simplex Algorithm, one only keeps
track of those variables for which the nonnegativity constraint yC ≥0 does not
belong to the current set J of active rows. The other variables are not stored
explicitly but “generated” when they are needed (when the nonnegativity constraint
becomes inactive). The decision of which variable has to be generated in each
step is equivalent to the Separation Problem for the dual LP, so in our case it
reduces to a Shortest Path Problem. This column generation technique can be
quite effective in practice.
Even with these techniques there are many practical instances that cannot be
solved optimally. However, the above scheme also gives rise to an approximation
algorithm. Let us ﬁrst formulate our problem as an optimization problem:
Maximum Multicommodity Flow Problem
Instance:
A pair (G, H) of digraphs on the same vertices.
Capacities u : E(G) →R+.
Task:
Find a family (x f )f ∈E(H), where x f is an s-t-ﬂow in G for each
f = (t, s) ∈E(H), 
f ∈E(H) x f (e) ≤u(e) for all e ∈E(G), and
the total ﬂow value 
f ∈E(H) value (x f ) is maximum.
There are other interesting formulations. For example one can look for ﬂows
satisfying the greatest possible fraction of given demands, or for ﬂows satisfying
given demands but violating capacities as slightly as possible. Moreover one can
consider costs on edges. We consider only the Maximum Multicommodity Flow
Problem; other problems can be attacked with similar techniques.
We again consider our LP formulation
max
⎧
⎨
⎩

P∈P
y(P) : y ≥0,

P∈P:e∈E(P)
y(P) ≤u(e) for all e ∈E(G)
⎫
⎬
⎭,
where P is the family of the s-t-paths in G for all (t, s) ∈E(H), and its dual
min
⎧
⎨
⎩zu : z ≥0,

e∈E(P)
z(e) ≥1 for all P ∈P
⎫
⎬
⎭.
We shall describe a primal-dual algorithm based on these formulations which
turns out to be a fully polynomial approximation scheme. This algorithm always
has a primal vector y ≥0 that is not necessarily a feasible primal solution since
capacity constraints might be violated. Initially y = 0. At the end we shall multiply
y by a constant in order to meet all constraints. To store y efﬁciently we keep
track of the family P′ ⊆P of those paths P with y(P) > 0; in contrast to P the
cardinality of P′ will be polynomially bounded.
The algorithm also has a dual vector z ≥0. Initially, z(e) = δ for all e ∈E(G),
where δ depends on n and ϵ. In each iteration, it ﬁnds a maximally violated dual

19.2 Algorithms for Multicommodity Flows
449
constraint (corresponding to a shortest s-t-path for (t, s) ∈E(H), with respect to
edge lengths z) and increases z and y along this path:
Multicommodity Flow Approximation Scheme
Input:
A pair (G, H) of digraphs on the same vertices.
Capacities u : E(G) →R+ \ {0}. A number ϵ with 0 < ϵ ≤1
2.
Output:
Numbers y : P →R+ with 
P∈P:e∈E(P) y(P) ≤u(e) for all e ∈
E(G).
1⃝
Set y(P) := 0 for all P ∈P.
Set δ := (n(1 + ϵ))−⌈5
ϵ ⌉(1 + ϵ) and z(e) := δ for all e ∈E(G).
2⃝
Let P ∈P such that z(E(P)) is minimum.
If z(E(P)) ≥1, then go to 4⃝.
3⃝
Let γ := min
e∈E(P) u(e).
Set y(P) := y(P) + γ .
Set z(e) := z(e)

1 + ϵγ
u(e)

for all e ∈E(P).
Go to 2⃝.
4⃝
Let ξ := max
e∈E(G)
1
u(e)

P∈P:e∈E(P)
y(P).
Set y(P) := y(P)
ξ
for all P ∈P.
This algorithm is due to Young [1995] and Garg and K¨onemann [1998], based
on earlier work of Shahrokhi and Matula [1990], Shmoys [1996], and others.
Theorem 19.6.
(Garg and K¨onemann [1998])
The Multicommodity Flow
Approximation Scheme produces a feasible solution with total ﬂow value at
least
1
1+ϵ OPT(G, H, u). Its running time is O
 1
ϵ2 km(m + n log n) log n

, where
k = |E(H)|, n = |V (G)| and m = |E(G)|, so it is a fully polynomial approxima-
tion scheme.
Proof:
In each iteration the value z(e) increases by a factor 1+ϵ for at least one
edge e (the bottleneck edge). Since an edge e with z(e) ≥1 is never used anymore
in any path, the total number of iterations is t ≤m⌈log1+ϵ( 1
δ )⌉. In each iteration
we have to solve k instances of the Shortest Path Problem with nonnegative
weights to determine P. Using Dijkstra’s Algorithm (Theorem 7.4) we get an
overall running time of O(tk(m+n log n)) = O

km(m + n log n) log1+ϵ( 1
δ )

. The
stated running time now follows from observing that, for 0 < ϵ ≤1,
log1+ϵ
1
δ

=
log( 1
δ )
log(1 + ϵ) ≤
B 5
ϵ
C
log(2n)
ϵ
2
= O
log n
ϵ2

;
here we used log(1 + ϵ) ≥ϵ
2 for 0 < ϵ ≤1.
We also have to check that the maximum number of bits needed to store
any number occurring in the computation is bounded by a polynomial in log n +

450
19. Multicommodity Flows and Edge-Disjoint Paths
size(ϵ) + 1
ϵ . This is clear for the y-variables. The number δ can be stored with
O( 1
ϵ size(n(1 + ϵ)) + size(ϵ)) = O( 1
ϵ (log n + size(ϵ))) bits. To deal with the
z-variables we assume that u is integral; otherwise we multiply all capacities
by the product of the denominators in the beginning (cf. Proposition 4.1). Then
the denominator of the z-variables is bounded at any time by the product of all
capacities and the denominator of δ. Since the numerator is at most twice the
denominator we have shown that the size of all numbers is indeed polynomial in
the input size and 1
ϵ .
The feasibility of the solution is guaranteed by 4⃝.
Note that every time we add γ units of ﬂow on edge e we increase the weight
z(e) by a factor

1 + ϵγ
u(e)

. This value is at least (1+ϵ)
γ
u(e) because 1+ϵa ≥(1+ϵ)a
holds for 0 ≤a ≤1 (both sides of this inequality are equal for a ∈{0, 1}, and the
left-hand side is linear in a while the right-hand side is convex). Since e is not
used once z(e) ≥1, we cannot add more than u(e)(1 + log1+ϵ( 1
δ )) units of ﬂow
on edge e. Hence
ξ ≤1 + log1+ϵ
1
δ

= log1+ϵ
1 + ϵ
δ

.
(19.3)
Let z(i) denote the vector z after iteration i, and let Pi and γi be the path P
and the number γ in iteration i. We have z(i)u = z(i−1)u + ϵγi

e∈E(Pi) z(i−1)(e),
so (z(i) −z(0))u = ϵ i
j=1 γjα(z( j−1)), where α(z) := minP∈P z(E(P)). Let us
write β := min

zu
α(z) : z ∈RE(G)
+

. Then β ≤(z(i)−z(0))u
α(z(i)−z(0)) and thus (α(z(i))−δn)β ≤
α(z(i) −z(0))β ≤(z(i) −z(0))u. We obtain
α(z(i)) ≤δn + ϵ
β
i
j=1
γjα(z( j−1)).
(19.4)
We now prove
δn + ϵ
β
i
j=1
γjα(z( j−1)) ≤δne

ϵ
β
i
j=1 γj

.
(19.5)
by induction on i (here e denotes the base of the natural logarithm). The case
i = 0 is trivial. For i > 0 we have
δn + ϵ
β
i
j=1
γjα(z( j−1))
=
δn + ϵ
β
i−1

j=1
γjα(z( j−1)) + ϵ
β γiα(z(i−1))
≤

1 + ϵ
β γi

δne

ϵ
β
i−1
j=1 γj

,
using (19.4) and the induction hypothesis. Using 1 + x < ex for all x > 0 the
proof of (19.5) is complete.

19.3 Directed Edge-Disjoint Paths Problem
451
In particular we conclude from (19.4), (19.5) and the stopping criterion that
1 ≤α(z(t)) ≤δne

ϵ
β
t
j=1 γj

,
hence t
j=1 γj ≥β
ϵ ln
 1
δn

. Now observe that the total ﬂow value that the algo-
rithm computes is 
P∈P y(P) = 1
ξ
t
j=1 γj. By the above and (19.3) this is at
least
β ln
 1
δn

ϵ log1+ϵ( 1+ϵ
δ )
=
β ln(1 + ϵ)
ϵ
· ln
 1
δn

ln( 1+ϵ
δ )
=
β ln(1 + ϵ)
ϵ
· (⌈5
ϵ ⌉−1) ln(n(1 + ϵ))
⌈5
ϵ ⌉ln(n(1 + ϵ))
≥
β(1 −ϵ
5) ln(1 + ϵ)
ϵ
by the choice of δ. Now observe that β is the optimum value of the dual LP, and
hence, by the LP Duality Theorem 3.16, the optimum value of a primal solution.
Moreover, ln(1+ϵ) ≥ϵ −ϵ2
2 (this inequality is trivial for ϵ = 0 and the derivative
of the left-hand side is greater than that of the right-hand side for every ϵ > 0).
Hence
(1 −ϵ
5) ln(1 + ϵ)
ϵ
≥

1 −ϵ
5
 
1 −ϵ
2

= 1 + 3
10ϵ −6
10ϵ2 + 1
10ϵ3
1 + ϵ
≥
1
1 + ϵ
for ϵ ≤1
2. We conclude that the algorithm ﬁnds a solution whose total ﬂow value
is at least
1
1+ϵ OPT(G, H, u).
2
A different algorithm which gives the same result (by a more complicated
analysis) has been given before by Grigoriadis and Khachiyan [1996]. Fleischer
[2000] improved the running time of the above algorithm by a factor of k. She
observed that it is sufﬁcient to compute an approximate shortest path in 2⃝, and
used this fact to show that it is not necessary to do a shortest path computation for
each (t, s) ∈E(H) in each iteration. See also Karakostas [2002], Vygen [2004],
Bienstock and Iyengar [2004], and Chudak and Eleut´erio [2005].
19.3 Directed Edge-Disjoint Paths Problem
We start by noting that the problem is NP-hard already in a quite restricted version:
Theorem 19.7.
(Even, Itai and Shamir [1976]) The Directed Edge-Disjoint
Paths Problem is NP-hard even if G is acyclic and H just consists of two sets of
parallel edges.

452
19. Multicommodity Flows and Edge-Disjoint Paths
s
t
v1
v2
vn+1
Z1
Z2
Zm
x1
1
x2
1
x2m-1
1
x2m
1
x1
2
x2m
n
¯x1
1
¯x2
1
¯x2m-1
1
¯x2m
1
¯x1
2
¯x2m
n
Fig. 19.3.
Proof:
We polynomially transform Satisfiability to our problem. Given a fam-
ily Z = {Z1, . . . , Zm} of clauses over X = {x1, . . . , xn}, we construct an instance
(G, H) of the Directed Edge-Disjoint Paths Problem such that G is acyclic,
H just consists of two sets of parallel edges and (G, H) has a solution if and only
if Z is satisﬁable.
G contains 2m vertices λ1, . . . , λ2m for each literal and additional vertices s
and t, v1, . . . , vn+1 and Z1, . . . , Zm. There are edges (vi, x1
i ), (vi, xi
1), (x2m
i
, vi+1),
(xi
2m, vi+1), (x j
i , x j+1
i
) and (xi
j, xi
j+1) for i = 1, . . . , n and j = 1, . . . , 2m −1.
Next, there are edges (s, x2 j−1
i
) and (s, xi
2 j−1) for i = 1, . . . , n and j = 1, . . . , m.
Moreover, there are edges (Zj, t) and (λ2 j, Zj) for j = 1, . . . , m and all literals
λ of the clause Zj. See Figure 19.3 for an illustration.
Let H consist of an edge (vn+1, v1) and m parallel edges (t, s).
We show that any solution of (G, H) corresponds to a truth assignment sat-
isfying all clauses (and vice versa). Namely, the v1-vn+1-path must pass through
either all x j
i (meaning xi is false) or all ¯x j
i (meaning xi is true) for each i. One
s-t-path must pass through each Zj. This is possible if and only if the above
deﬁned truth assignment satisﬁes Zj.
2
Fortune, Hopcroft and Wyllie [1980] showed that the Directed Edge-
Disjoint Paths Problem can be solved in polynomial time if G is acyclic and

19.3 Directed Edge-Disjoint Paths Problem
453
|E(H)| = k for some ﬁxed k. If G is not acyclic, they proved that problem is
NP-hard already for |E(H)| = 2. On the other hand we have:
Theorem 19.8.
(Nash-Williams [1969])
Let (G, H) be an instance of the Di-
rected Edge-Disjoint Paths Problem, where G + H is Eulerian and H just
consists of two sets of parallel edges. Then (G, H) has a solution if and only if the
cut criterion holds.
Proof:
We ﬁrst ﬁnd a set of paths realizing the ﬁrst set of parallel edges in H by
Menger’s Theorem 8.9. After deleting these paths (and the corresponding demand
edges), the remaining instance satisﬁes the prerequisites of Proposition 8.12 and
thus has a solution.
2
If G+H is Eulerian and |E(H)| = 3, there is also a polynomial-time algorithm
(Ibaraki and Poljak [1991]); On the other hand there is the following negative
result:
Theorem 19.9.
(Vygen [1995]) The Directed Edge-Disjoint Paths Problem
is NP-hard even if G is acyclic, G + H is Eulerian, and H consists just of three
sets of parallel edges.
Proof:
We reduce the problem of Theorem 19.7 to this one. So let (G, H) be an
instance of the Directed Edge-Disjoint Paths Problem, where G is acyclic,
and H consists just of two sets of parallel edges.
For each v ∈V (G) we deﬁne
α(v)
:=
max(0, |δ+
G+H(v)| −|δ−
G+H(v)|) and
β(v)
:=
max(0, |δ−
G+H(v)| −|δ+
G+H(v)|).
We have

v∈V (G)
(α(v) −β(v)) =

v∈V (G)

|δ+
G+H(v)| −|δ−
G+H(v)|

= 0,
implying

v∈V (G)
α(v) =

v∈V (G)
β(v) =: q.
We now construct an instance (G′, H ′) of the Directed Edge-Disjoint Paths
Problem. G′ results from G by adding two vertices s and t as well as α(v) parallel
edges (s, v) and β(v) parallel edges (v, t) for each vertex v. H ′ consists of all
edges of H and q parallel edges (t, s).
This construction can obviously be done in polynomial time. In particular, the
number of edges in G + H at most quadruples. Furthermore, G′ is acyclic, G′+ H ′
is Eulerian, and H ′ just consists of three sets of parallel edges. Thus it remains to
show that (G, H) has a solution if and only if (G′, H ′) has a solution.
Each solution of (G′, H ′) implies a solution of (G, H) simply by omitting the
s-t-paths. So let P be a solution of (G, H). Let G′′ be the graph resulting from

454
19. Multicommodity Flows and Edge-Disjoint Paths
G′ by deleting all edges used by P. Let H ′′ be the subgraph of H ′ just consisting
of the q edges from t to s. (G′′, H ′′) satisﬁes the prerequisites of Proposition 8.12
and thus has a solution. Combining P with a solution of (G′′, H ′′) produces a
solution of (G′, H ′).
2
Since a solution to an instance of the Directed Edge-Disjoint Paths Prob-
lem consists of edge-disjoint circuits, it is natural to ask how many edge-disjoint
circuits a digraph has. At least for planar digraphs we have a good characterization.
Namely, we consider the planar dual graph and ask for the maximum number of
edge-disjoint directed cuts. We have the following well-known min-max theorem
(which we prove very similarly to Theorem 6.13):
Theorem 19.10.
(Lucchesi and Younger [1978]) The maximum number of edge-
disjoint directed cuts in a digraph equals the minimum cardinality of an edge set
that contains at least one element of each directed cut.
Proof:
Let A be the matrix whose columns are indexed by the edges and whose
rows are the incidence vectors of all directed cuts. Consider the LP
min{1lx : Ax ≥1l, x ≥0},
and its dual
max{1ly : y A ≤1l, y ≥0}.
Then we have to prove that both the primal and the dual LP have integral optimum
solutions. By Corollary 5.14 it sufﬁces to show that the system Ax ≥1l, x ≥0 is
TDI. We use Lemma 5.22.
Let c : E(G) →Z+, and let y be an optimum solution of max{1ly : y A ≤
c, y ≥0} for which

X
yδ+(X)|X|2
(19.6)
is as large as possible, where the sum is over all rows of A. We claim that the
set system (V (G), F) with F := {X : yδ+(X) > 0} is cross-free. To see this,
suppose X, Y ∈F with X ∩Y ̸= ∅, X \ Y ̸= ∅, Y \ X ̸= ∅and X ∪Y ̸= V (G).
Then δ+(X ∩Y) and δ+(X ∪Y) are also directed cuts (by Lemma 2.1(b)). Let
ϵ := min{yδ+(X), yδ+(Y)}. Set y′
δ+(X) := yδ+(X) −ϵ, y′
δ+(Y) := yδ+(Y) −ϵ, y′
δ+(X∩Y) :=
yδ+(X∩Y) + ϵ, y′
δ+(X∪Y) := yδ+(X∪Y) + ϵ, and y′(S) := y(S) for all other directed
cuts S. Since y′ is a feasible dual solution, it is also optimum and contradicts the
choice of y, because (19.6) is larger for y′.
Now let A′ be the submatrix of A consisting of the rows corresponding to the
elements of F. A′ is the two-way cut-incidence matrix of a cross-free family. So
by Theorem 5.27 A′ is totally unimodular, as required.
2
For a combinatorial proof, see Lov´asz [1976]. Frank [1981] gives an algorith-
mic proof.
Note that the sets of edges meeting all directed cuts are precisely the sets of
edges whose contraction makes the graph strongly connected. In the planar dual

19.4 Undirected Edge-Disjoint Paths Problem
455
graph, these sets correspond to the sets of edges meeting all directed circuits. Such
sets are known as feedback edge sets, the minimum cardinality of a feedback edge
set is the feedback number of the graph. The problem to determine the feedback
number is NP-hard in general (Karp [1972]) but polynomially solvable for planar
graphs.
Corollary 19.11.
In a planar digraph the maximum number of edge-disjoint cir-
cuits equals the minimum number of edges meeting all circuits.
Proof:
Let G be a digraph which, without loss of generality, is connected and
contains no articulation vertex. Consider the planar dual of G and Corollary 2.44,
and apply the Lucchesi-Younger Theorem 19.10.
2
A polynomial-time algorithm for determining the feedback number for pla-
nar graphs can be composed of the planarity algorithm (Theorem 2.40), the
Gro¨tschel-Lov´asz-Schrijver Algorithm (Theorem 4.21) and an algorithm for
the Maximum Flow Problem to solve the Separation Problem (Exercise 4).
An application to the Edge-Disjoint Paths Problem is the following:
Corollary 19.12.
Let (G, H) be an instance of the Directed Edge-Disjoint
Paths Problem, where G is acyclic and G + H is planar. Then (G, H) has a
solution if and only if deleting any |E(H)| −1 edges of G + H does not make
G + H acyclic.
2
In particular, the distance criterion is necessary and sufﬁcient in this case, and
the problem can be solved in polynomial time.
19.4 Undirected Edge-Disjoint Paths Problem
The following lemma establishes a connection between directed and undirected
problems.
Lemma 19.13.
Let (G, H) be an instance of the Directed Edge-Disjoint
Paths Problem, where G is acyclic and G + H is Eulerian. Consider the in-
stance (G′, H ′) of the Undirected Edge-Disjoint Paths Problem which results
from neglecting the orientations. Then each solution of (G′, H ′) is also a solution
of (G, H), and vice versa.
Proof:
It is trivial that each solution of (G, H) is also a solution of (G′, H ′).
We prove the other direction by induction on |E(G)|. If G has no edges, we are
done.
Now let P be a solution of (G′, H ′). Since G is acyclic, G must contain a
vertex v for which δ−
G(v) = ∅. Since G + H is Eulerian, we have |δ−
H(v)| =
|δ+
G(v)| + |δ+
H(v)|.
For each demand edge incident to v there must be an undirected path in P
starting at v. Thus |δ+
G(v)| ≥|δ−
H(v)| + |δ+
H(v)|. This implies |δ+
H(v)| = 0 and

456
19. Multicommodity Flows and Edge-Disjoint Paths
|δ+
G(v)| = |δ−
H(v)|. Therefore each edge incident to v must be used by P with the
correct orientation.
Now let G1 be the graph which results from G by deleting the edges incident
to v. Let H1 result from H by replacing each edge f = (t, v) incident to v by
(t, w), where w is the ﬁrst inner vertex of the path in P which realizes f .
Obviously G1 is acyclic and G1 + H1 is Eulerian. Let P1 arise from P by
deleting all the edges incident to v. P1 is a solution of (G′
1, H ′
1), the undirected
problem corresponding to (G1, H1).
By the induction hypothesis, P1 is a solution of (G1, H1) as well. So by adding
the initial edges we obtain that P is a solution of (G, H).
2
We conclude:
Theorem 19.14.
(Vygen [1995]) The UndirectedEdge-DisjointPathsProb-
lem is NP-hard even if G + H is Eulerian and H just consists of three sets of
parallel edges.
Proof:
We reduce the problem of Theorem 19.9 to the undirected case by ap-
plying Lemma 19.13.
2
Another special case in which the Undirected Edge-Disjoint Paths Prob-
lem is NP-hard is when G + H is planar (Middendorf and Pfeiffer [1993]). How-
ever, if G + H is known to be planar and Eulerian, then the problem becomes
tractable:
Theorem 19.15.
(Seymour [1981])
Let (G, H) be an instance of the Undi-
rected Edge-Disjoint Paths Problem, where G + H is planar and Eulerian.
Then (G, H) has a solution if and only if the cut criterion holds.
Proof:
We only have to prove the sufﬁciency of the cut criterion. We may assume
that G + H is connected. Let D be the planar dual of G + H. Let F ⊆E(D) be
the set of dual edges corresponding to the demand edges. Then the cut criterion,
together with Theorem 2.43, implies that |F ∩E(C)| ≤|E(C)\ F| for each circuit
C in D. So by Proposition 12.7, F is a minimum T -join, where T := {x ∈V (D) :
|F ∩δ(x)| is odd}.
Since G + H is Eulerian, by Corollary 2.45 D is bipartite, so by Theorem
12.15 there are |F| edge-disjoint T -cuts C1, . . . , C|F|. Since by Proposition 12.14
each T -cut intersects F, each of C1, . . . C|F| must contain exactly one edge of F.
Back in G + H, the duals of C1, . . . , C|F| are edge-disjoint circuits, each
containing exactly one demand edge. But this means that we have a solution of
the Edge-Disjoint Paths Problem.
2
This theorem also implies a polynomial-time algorithm (Exercise 7). In fact,
Matsumoto, Nishizeki and Saito [1986] proved that the Undirected Edge-
Disjoint Paths Problem with G + H planar and Eulerian can be solved in
O

n
5
2 log n

time.
On the other hand, Robertson and Seymour have found a polynomial-time
algorithm for a ﬁxed number of demand edges:

19.4 Undirected Edge-Disjoint Paths Problem
457
Theorem 19.16.
(Robertson and Seymour [1995]) For ﬁxed k, there is a poly-
nomial-time algorithm for the Undirected Vertex-Disjoint or Edge-Disjoint
Paths Problem restricted to instances where |E(H)| ≤k.
Note that the Undirected Vertex-Disjoint Paths Problem is also NP-hard;
see Exercise 10. Theorem 19.16 is part of Robertson’s and Seymour’s important
series of papers on graph minors which is far beyond the scope of this book. The
theorem was proved for the vertex-disjoint case; here Robertson and Seymour
proved that there either exists an irrelevant vertex (which can be deleted without
affecting solvability) or the graph has a tree-decomposition of small width (in
which case there is a simple polynomial-time algorithm; see Exercise 9). The
edge-disjoint case then follows easily; see Exercise 10. Although the running time
is O(n2m), the constant depending on k grows extremely fast and is beyond
practical use already for k = 3.
The rest of this section is devoted to the proof of two further important results.
The ﬁrst one is the well-known Okamura-Seymour Theorem:
Theorem 19.17.
(Okamura and Seymour [1981])
Let (G, H) be an instance of
the Undirected Edge-Disjoint Paths Problem, where G + H is Eulerian, G is
planar, and all terminals lie on the outer face. Then (G, H) has a solution if and
only if the cut criterion holds.
Proof:
We show the sufﬁciency of the cut criterion by induction on |V (G)| +
|E(G)|. If |V (G)| ≤2, this is obvious.
We may assume that G is 2-connected, for otherwise we may apply the in-
duction hypothesis to the blocks of G (splitting up demand edges joining different
blocks at articulation vertices). We ﬁx some planar embedding of G; by Proposi-
tion 2.31 the outer face is bounded by some circuit C.
If there is no set X ⊂V (G) with ∅̸= X ∩V (C) ̸= V (C) and |δG(X)| =
|δH(X)|, then for any edge e ∈E(C) the instance (G −e, H + e) satisﬁes the cut
criterion. This is because |δG(X)| −|δH(X)| is even for all X ⊆V (G) (as G + H
is Eulerian). By the induction hypothesis, (G −e, H + e) has a solution which
immediately implies a solution for (G, H).
So suppose there is a set X ⊂V (G) with ∅̸= X ∩V (C) ̸= V (C) and
|δG(X)| = |δH(X)|. Choose X such that the total number of connected components
in G[X] and G[V (G) \ X] is minimum. Then it is easy to see that indeed G[X]
and G[V (G)\ X] are both connected: Suppose not, say G[X] is disconnected (the
other case is symmetric). Then |δG(Xi)| = |δH(Xi)| for each connected component
Xi of G[X], and replacing X by Xi (for some i such that Xi ∩V (C) ̸= ∅) reduces
the number of connected components in G[X] without increasing the number of
connected components in G[V (G) \ X]. This contradicts the choice of X.
Since G is planar, a set X ⊂V (G) with ∅̸= X ∩V (C) ̸= V (C) such that
G[X] and G[V (G) \ X] are both connected has the property that C[X] is a path.
So let ∅̸= X ⊆V (G) with |δG(X)| = |δH(X)| such that C[X] is a path of
minimum length. Let the vertices of C be numbered v1, . . . , vl cyclically, where
V (C) ∩X = {v1, . . . , vj}. Let e := {vl, v1}.

458
19. Multicommodity Flows and Edge-Disjoint Paths
f
v1
v2
vi
vj
vk
vl−1
vl
e
C
X
Fig. 19.4.
Choose f = {vi, vk} ∈E(H) such that 1 ≤i ≤j < k ≤l (i.e. vi ∈X,
vk /∈X) and k is as large as possible (see Figure 19.4). Now consider G′ := G −e
and H ′ := (V (H), (E(H) \ { f }) ∪{{vi, v1}, {vl, vk}}). (The cases i = 1 or k = l
are not excluded, in this case no loops should be added.)
We claim that (G′, H ′) satisﬁes the cut criterion. Then by induction, (G′, H ′)
has a solution, and this can easily be transformed to a solution of (G, H).
Suppose, then, that (G′, H ′) does not satisfy the cut criterion, i.e. |δG′(Y)| <
|δH ′(Y)| for some Y ⊆V (G). As above we may assume that G[Y] and G[V (G)\
Y] are both connected. By possibly interchanging Y and V (G) \ Y we may also
assume vi /∈Y. Since Y ∩V (C) is a path and |δH ′(Y)| −|δG′(Y)| > |δH(Y)| −
|δG(Y)|, there are three cases:
(a) v1 ∈Y, vi, vk, vl /∈Y;
(b) v1, vl ∈Y, vi, vk /∈Y;
(c) vl ∈Y, v1, vi, vk /∈Y.
In each case we have Y ∩V (C) ⊆{vk+1, . . . , vi−1}, so by the choice of f we
have EH(X, Y) = ∅. Furthermore, |δG(Y)| = |δH(Y)|. By applying Lemma 2.1(c)
twice, we have
|δH(X)| + |δH(Y)|
=
|δG(X)| + |δG(Y)|
=
|δG(X ∩Y)| + |δG(X ∪Y)| + 2|EG(X, Y)|
≥
|δH(X ∩Y)| + |δH(X ∪Y)| + 2|EG(X, Y)|
=
|δH(X)| + |δH(Y)| −2|EH(X, Y)| + 2|EG(X, Y)|
=
|δH(X)| + |δH(Y)| + 2|EG(X, Y)|
≥
|δH(X)| + |δH(Y)| .
So equality must hold throughout. This implies |δG(X ∩Y)| = |δH(X ∩Y)| and
EG(X, Y) = ∅.

19.4 Undirected Edge-Disjoint Paths Problem
459
So case (c) is impossible (because here e ∈EG(X, Y)); i.e. v1 ∈Y. Therefore
X ∩Y is nonempty and C[X ∩Y] is a shorter path than C[X], contradicting the
choice of X.
2
This proof yields a polynomial-time algorithm (Exercise 11) for the Undi-
rected Edge-Disjoint Paths Problem in this special case. It can be imple-
mented in O(n2) time (Becker and Mehlhorn [1986]) and indeed in linear time
(Wagner and Weihe [1995]).
We prepare the second main result of this section by a theorem concerning
orientations of mixed graphs, i.e. graphs with directed and undirected edges. Given
a mixed graph G, can we orient its undirected edges such that the resulting digraph
is Eulerian? The following theorem answers this question:
Theorem 19.18.
(Ford and Fulkerson [1962])
Let G be a digraph and H an
undirected graph with V (G) = V (H). Then H has an orientation H ′ such that the
digraph G + H ′ is Eulerian if and only if
–
|δ+
G(v)| + |δ−
G(v)| + |δH(v)| is even for all v ∈V (G), and
–
|δ+
G(X)| −|δ−
G(X)| ≤|δH(X)| for all X ⊆V (G).
Proof:
The necessity of the conditions is obvious. We prove the sufﬁciency by
induction on |E(H)|. If E(H) = ∅, the statement is trivial.
We call a set X critical if |δ+
G(X)| −|δ−
G(X)| = |δH(X)| > 0. Let X be any
critical set. (If there is no critical set, we orient any undirected edge arbitrarily
and apply induction). We choose an undirected edge e ∈δH(X) and orient it such
that e enters X; we claim that the conditions continue to hold.
Suppose, indirectly, that there is a Y ⊆V (G) with |δ+
G(Y)| −|δ−
G(Y)| >
|δH(Y)|. Since every degree is even, |δ+
G(Y)| −|δ−
G(Y)| −|δH(Y)| must be even.
This implies |δ+
G(Y)| −|δ−
G(Y)| ≥|δH(Y)| + 2. Therefore Y was critical before
orienting e, and e now leaves Y.
Applying Lemma 2.1(a) and (b) for |δ+
G| and |δ−
G| and Lemma 2.1(c) for |δH|
we have (before orienting e):
0 + 0
=
|δ+
G(X)| −|δ−
G(X)| −|δH(X)| + |δ+
G(Y)| −|δ−
G(Y)| −|δH(Y)|
=
|δ+
G(X ∩Y)| −|δ−
G(X ∩Y)| −|δH(X ∩Y)|
+|δ+
G(X ∪Y)| −|δ−
G(X ∪Y)| −|δH(X ∪Y)| −2|EH(X, Y)|
≤
0 + 0 −2|EH(X, Y)| ≤0.
So we have equality throughout and conclude that EH(X, Y) = ∅, contradicting
the existence of e.
2
Corollary 19.19.
An undirected Eulerian graph can be oriented such that a di-
rected Eulerian graph arises.
2
Of course this corollary can be proved more easily by orienting the edges
according to their occurrence in an Eulerian walk.
We now return to the Edge-Disjoint Paths Problem.

460
19. Multicommodity Flows and Edge-Disjoint Paths
Theorem 19.20.
(Rothschild and Whinston [1966]) Let (G, H) be an instance
of the Undirected Edge-Disjoint Paths Problem, where G + H is Eulerian
and H is the union of two stars (i.e. two vertices meet all the demand edges). Then
(G, H) has a solution if and only if the cut criterion holds.
Proof:
We show that the cut criterion is sufﬁcient. Let t1, t2 be two vertices
meeting all the demand edges. We ﬁrst introduce two new vertices s′
1 and s′
2. We
replace each demand edge {t1, si} by a new demand edge {t1, s′
1} and a new supply
edge {s′
1, si}. Likewise, we replace each demand edge {t2, si} by a new demand
edge {t2, s′
2} and a new supply edge {s′
2, si}.
The resulting instance (G′, H ′) is certainly equivalent to (G, H), and H ′ just
consists of two sets of parallel edges. It is easy to see that the cut criterion continues
to hold. Moreover, G′ + H ′ is Eulerian.
Now we orient the edges of H ′ arbitrarily such that parallel edges have the
same orientation (and call the result H ′′). The two graphs H ′′ and G′ satisfy
the prerequisites of Theorem 19.18 because the cut criterion implies |δ+
H ′′(X)| −
|δ−
H ′′(X)| ≤|δG′(X)| for all X ⊆V (G). Therefore we can orient the edges of G′
in order to get a digraph G′′ such that G′′ + H ′′ is Eulerian.
We regard (G′′, H ′′) as an instance of the Directed Edge-Disjoint Paths
Problem. (G′′, H ′′) satisﬁes the (directed) cut criterion. But now Theorem 19.8
guarantees a solution which – by neglecting the orientations – is also a solution
for (G′, H ′).
2
The same theorem holds if H (neglecting parallel edges) is K4 or C5 (the
circuit of length 5) (Lomonosov [1979], Seymour [1980]). In the K5 case, at least
the distance criterion is sufﬁcient (Karzanov [1987]). However, if H is allowed to
have three sets of parallel edges, the problem becomes NP-hard, as we have seen
in Theorem 19.14.
Exercises
1. Let (G, H) be an instance of the Edge-Disjoint Paths Problem, directed or
undirected, violating the distance criterion (19.2) for some z : E(G) →R+.
Prove that then there is also some z : E(G) →Z+ violating (19.2). Moreover,
give examples where there is no z : E(G) →{0, 1} violating (19.2).
2.
∗
For an instance (G, H) of the Edge-Disjoint Paths Problem we consider
the multicommodity ﬂow relaxation and solve
min {λ : λ ∈R, y ≥0, My ≤λ1l, Ny = 1l} ,
where M and N are deﬁned as in Lemma 19.1. Let (y∗, λ∗) be an optimum
solution. Now we are looking for an integral solution, i.e. an s-t-path Pf for
each demand edge f = {t, s} ∈E(H), such that the maximum load on a
supply edge is minimum (by the load of an edge we mean the number of

Exercises
461
paths using it). We do this by randomized rounding: independently for each
demand edge we choose a path P with probability yP.
Let 0 < ϵ ≤1, and suppose that λ∗≥3 ln |E(G)|
ϵ
. Prove that then with
probability at least 1 −ϵ the above randomized rounding yields an integral
solution with maximum load at most λ∗+
9
3λ∗ln |E(G)|
ϵ
.
Hint: Use the following facts from probability theory: if B(m, N, p) is the
probability of at least m successes in N independent Bernoulli trials, each
with success probability p, then
B((1 + β)Np, N, p) < e−1
3 β2Np
for all 0 < β ≤1. Moreover, the probability of at least m successes in N
independent Bernoulli trials with success probabilities p1, . . . , pN is at most
B

m, N, 1
N (p1 + · · · + pN)

.
(Raghavan and Thompson [1987])
3. Prove that there is a polynomial-time algorithm for the (Directed or Undi-
rected) Edge-Disjoint Paths Problem where G + H is Eulerian and where
H just consists of two sets of parallel edges.
4. Show that in a given digraph a minimum set of edges meeting all directed cuts
can be found in polynomial time. Show that for planar graphs the feedback
number can be determined in polynomial time.
5. Show that in a digraph a minimum set of edges whose contraction makes the
graph strongly connected can be found in polynomial time.
6. Show that the statement of Corollary 19.12 becomes false if the condition “G
is acyclic” is omitted.
Note: In this case the Directed Edge-Disjoint Paths Problem is NP-hard
(Vygen [1995]).
7. Prove that the Undirected Edge-Disjoint Paths Problem can be solved
in polynomial time if G + H is planar and Eulerian.
8.
∗
In this exercise we consider instances (G, H) of the Undirected Vertex-
Disjoint Paths Problem where G is planar and all terminals are distinct (i.e.
e ∩f = ∅for any two demand edges e and f ) and lie on the outer face. Let
(G, H) be such an instance, where G is 2-connected; so let C be the circuit
bounding the outer face (cf. Proposition 2.31).
Prove that (G, H) has a solution if and only if the following conditions hold:
a) G + H is planar;
b) no set X ⊆V (G) separates more than |X| demand edges (we say that
X separates {v, w} if {v, w} ∩X ̸= ∅or if w is not reachable from v in
G −X).
Conclude that the Undirected Vertex-Disjoint Paths Problem in planar
graphs with distinct terminals on the outer face can be solved in polynomial
time.
Hint: To prove the sufﬁciency of (a) and (b), consider the following inductive
step: Let f = {v, w} be a demand edge such that at least one of the two

462
19. Multicommodity Flows and Edge-Disjoint Paths
v-w-paths on C does not contain any other terminal. Realize f by this path
and delete it.
Note: Robertson and Seymour [1986] extended this to a necessary and sufﬁ-
cient condition for the solvability of the Undirected Vertex-Disjoint Paths
Problem with two demand edges.
9.
∗
Let k ∈N be ﬁxed. Prove that there is a polynomial-time algorithm for the
Vertex-Disjoint Paths Problem restricted to graphs of tree-width at most
k (cf. Exercise 22 of Chapter 2).
Note: Schefﬂer [1994] proved that there is in fact a linear-time algorithm.
In contrast to that, the Edge-Disjoint Paths Problem is NP-hard even for
graphs with tree-width 2 (Nishizeki, Vygen and Zhou [2001]).
10. Prove that the Directed Vertex-Disjoint Paths Problem and the Undi-
rected Vertex-Disjoint Paths Problem are NP-hard. Prove that the vertex-
disjoint part of Theorem 19.16 implies its edge-disjoint part.
11. Show that the proof of the Okamura-Seymour Theorem leads to a polynomial-
time algorithm.
12. Let (G, H) be an instance of the Undirected Edge-Disjoint Paths Prob-
lem. Suppose that G is planar, all terminals lie on the outer face, and each
vertex not on the outer face has even degree. Furthermore, assume that
|δG(X)| > |δH(X)|
for all X ⊆V (G).
Prove that (G, H) has a solution.
Hint: Use the Okamura-Seymour Theorem.
13. Generalizing Robbins’ Theorem (Exercise 17(c) of Chapter 2), formulate and
prove a necessary and sufﬁcient condition for the existence of an orientation
of the undirected edges of a mixed graph such that the resulting digraph is
strongly connected.
(Boesch and Tindell [1980])
14. Let (G, H) be an instance of the Directed Edge-Disjoint Paths Problem
where G + H is Eulerian, G is planar and acyclic, and all terminals lie on the
outer face. Prove that (G, H) has a solution if and only if the cut criterion
holds.
Hint: Use Lemma 19.13 and the Okamura-Seymour Theorem 19.17.
15. Prove Theorem 19.18 using network ﬂow techniques.
16. Prove Nash-Williams’ [1969] orientation theorem, which is a generalization
of Robbins’ Theorem (Exercise 17(c) of Chapter 2):
An undirected graph G can be oriented to be strongly k-edge-connected (i.e.
there are k edge-disjoint s-t-paths for any pair (s, t) ∈V (G) × V (G)) if and
only if G is 2k-edge-connected.
Hint: To prove the sufﬁciency, let G′ be any orientation of G. Prove that the
system
xe
≤
1
(e ∈E(G′)),
xe
≥
0
(e ∈E(G′)),

e∈δ−(X)
xe −

e∈δ+
G′(X)
xe
≤
|δ−
G′(X)| −k
(∅̸= X ⊂V (G′))

References
463
is TDI, as in the proof of the Lucchesi-Younger Theorem 19.10.
(Frank [1980]), (Frank and Tardos [1984])
17. Prove Hu’s Two-Commodity Flow Theorem: an instance (G, H, u, b) of the
Undirected Multicommodity Flow Problem with |E(H)| = 2 has a so-
lution if and only if 
e∈δG(X) u(e) ≥
f ∈δH(X) b( f ) for all X ⊆V (G), i.e. if
and only if the cut condition holds.
Hint: Use Theorem 19.20.
(Hu [1963])
References
General Literature:
Frank, A. [1990]: Packing paths, circuits and cuts – a survey. In: Paths, Flows, and VLSI-
Layout (B. Korte, L. Lov´asz, H.J. Pr¨omel, A. Schrijver, eds.), Springer, Berlin 1990, pp.
47–100
Ripphausen-Lipa, H., Wagner, D., and Weihe, K. [1995]: Efﬁcient algorithms for disjoint
paths in planar graphs. In: Combinatorial Optimization; DIMACS Series in Discrete
Mathematics and Theoretical Computer Science 20 (W. Cook, L. Lov´asz, P. Seymour,
eds.), AMS, Providence 1995
Schrijver, A. [2003]: Combinatorial Optimization: Polyhedra and Efﬁciency. Springer,
Berlin 2003, Chapters 70–76
Vygen, J. [1994]: Disjoint Paths. Report No. 94816, Research Institute for Discrete Math-
ematics, University of Bonn, 1994
Cited References:
Becker, M., and Mehlhorn, K. [1986]: Algorithms for routing in planar graphs. Acta Infor-
matica 23 (1986), 163–176
Bienstock, D., and Iyengar, G. [2004]: Solving fractional packing problems in O∗( 1
ϵ ) iter-
ations. Proceedings of the 36th Annual ACM Symposium on the Theory of Computing
(2004), 146–155
Boesch, F., and Tindell, R. [1980]: Robbins’s theorem for mixed multigraphs. American
Mathematical Monthly 87 (1980), 716–719
Chudak, F.A., and Eleut´erio, V. [2005]: Improved approximation schemes for linear pro-
gramming relaxations of combinatorial optimization problems. In: Integer Programming
and Combinatorial Optimization; Proceedings of the 11th International IPCO Conference;
LNCS 3509 (M. J¨unger, V. Kaibel, eds.), Springer, Berlin 2005, pp. 81–96
Even, S., Itai, A., and Shamir, A. [1976]: On the complexity of timetable and multicom-
modity ﬂow problems. SIAM Journal on Computing 5 (1976), 691–703
Fleischer, L.K. [2000]: Approximating fractional multicommodity ﬂow independent of the
number of commodities. SIAM Journal on Discrete Mathematics 13 (2000), 505–520
Ford, L.R., and Fulkerson, D.R. [1958]: A suggested computation for maximal multicom-
modity network ﬂows. Management Science 5 (1958), 97–101
Ford, L.R., and Fulkerson, D.R. [1962]: Flows in Networks. Princeton University Press,
Princeton 1962
Fortune, S., Hopcroft, J., and Wyllie, J. [1980]: The directed subgraph homeomorphism
problem. Theoretical Computer Science 10 (1980), 111–121
Frank, A. [1980]: On the orientation of graphs. Journal of Combinatorial Theory B 28
(1980), 251–261

464
19. Multicommodity Flows and Edge-Disjoint Paths
Frank, A. [1981]: How to make a digraph strongly connected. Combinatorica 1 (1981),
145–153
Frank, A., and Tardos, ´E. [1984]: Matroids from crossing families. In: Finite and Inﬁnite
Sets; Vol. I (A. Hajnal, L. Lov´asz, and V.T. S´os, eds.), North-Holland, Amsterdam, 1984,
pp. 295–304
Garg, N., and K¨onemann, J. [1998]: Faster and simpler algorithms for multicommodity ﬂow
and other fractional packing problems. Proceedings of the 39th Annual IEEE Symposium
on Foundations of Computer Science (1998), 300–309
Grigoriadis, M.D., and Khachiyan, L.G. [1996]: Coordination complexity of parallel price-
directive decomposition. Mathematics of Operations Research 21 (1996), 321–340
Hu, T.C. [1963]: Multi-commodity network ﬂows. Operations Research 11 (1963), 344–360
Ibaraki, T., and Poljak, S. [1991]: Weak three-linking in Eulerian digraphs. SIAM Journal
on Discrete Mathematics 4 (1991), 84–98
Karakostas, G. [2002]: Faster approximation schemes for fractional multicommodity ﬂow
problems. Proceedings of the 13th Annual ACM-SIAM Symposium on Discrete Algo-
rithms (2002), 166–173
Karp, R.M. [1972]: Reducibility among combinatorial problems. In: Complexity of Com-
puter Computations (R.E. Miller, J.W. Thatcher, eds.), Plenum Press, New York 1972,
pp. 85–103
Karzanov, A.V. [1987]: Half-integral ﬁve-terminus-ﬂows. Discrete Applied Mathematics
18 (1987) 263–278
Lomonosov, M. [1979]: Multiﬂow feasibility depending on cuts. Graph Theory Newsletter
9 (1979), 4
Lov´asz, L. [1976]: On two minimax theorems in graph. Journal of Combinatorial Theory
B 21 (1976), 96–103
Lucchesi, C.L., and Younger, D.H. [1978]: A minimax relation for directed graphs. Journal
of the London Mathematical Society II 17 (1978), 369–374
Matsumoto, K., Nishizeki, T., and Saito, N. [1986]: Planar multicommodity ﬂows, maxi-
mum matchings and negative cycles. SIAM Journal on Computing 15 (1986), 495–510
Middendorf, M., and Pfeiffer, F. [1993]: On the complexity of the disjoint path problem.
Combinatorica 13 (1993), 97–107
Nash-Williams, C.S.J.A. [1969]: Well-balanced orientations of ﬁnite graphs and unobtrusive
odd-vertex-pairings. In: Recent Progress in Combinatorics (W. Tutte, ed.), Academic
Press, New York 1969, pp. 133–149
Nishizeki, T., Vygen, J., and Zhou, X. [2001]: The edge-disjoint paths problem is NP-
complete for series-parallel graphs. Discrete Applied Mathematics 115 (2001), 177–186
Okamura, H., and Seymour, P.D. [1981]: Multicommodity ﬂows in planar graphs. Journal
of Combinatorial Theory B 31 (1981), 75–81
Raghavan, P., and Thompson, C.D. [1987]: Randomized rounding: a technique for provably
good algorithms and algorithmic proofs. Combinatorica 7 (1987), 365–374
Robertson, N., and Seymour, P.D. [1986]: Graph minors VI; Disjoint paths across a disc.
Journal of Combinatorial Theory B 41 (1986), 115–138
Robertson, N., and Seymour, P.D. [1995]: Graph minors XIII; The disjoint paths problem.
Journal of Combinatorial Theory B 63 (1995), 65–110
Rothschild, B., and Whinston, A. [1966]: Feasibility of two-commodity network ﬂows.
Operations Research 14 (1966), 1121–1129
Schefﬂer, P. [1994]: A practical linear time algorithm for disjoint paths in graphs with
bounded tree-width. Technical Report No. 396/1994, FU Berlin, Fachbereich 3 Mathe-
matik
Seymour, P.D. [1981]: On odd cuts and multicommodity ﬂows. Proceedings of the London
Mathematical Society (3) 42 (1981), 178–192
Shahrokhi, F., and Matula, D.W. [1990]: The maximum concurrent ﬂow problem. Journal
of the ACM 37 (1990), 318–334

References
465
Shmoys, D.B. [1996]: Cut problems and their application to divide-and-conquer. In: Ap-
proximation Algorithms for NP-Hard Problems (D.S. Hochbaum, ed.), PWS, Boston,
1996
Vygen, J. [1995]: NP-completeness of some edge-disjoint paths problems. Discrete Applied
Mathematics 61 (1995), 83–90
Vygen, J. [2004]: Near-optimum global routing with coupling, delay bounds, and power
consumption. In: Integer Programming and Combinatorial Optimization; Proceedings of
the 10th International IPCO Conference; LNCS 3064 (G. Nemhauser, D. Bienstock,
eds.), Springer, Berlin 2004, pp. 308–324
Wagner, D., and Weihe, K. [1995]: A linear-time algorithm for edge-disjoint paths in planar
graphs. Combinatorica 15 (1995), 135–150
Young, N. [1995]: Randomized rounding without solving the linear program. Proceedings
of the 6th Annual ACM-SIAM Symposium on Discrete Algorithms (1995), 170–178

20. Network Design Problems
Connectivity is a very important concept in combinatorial optimization. In Chapter
8 we showed how to compute the connectivity between each pair of vertices
of an undirected graph. Now we are looking for subgraphs that satisfy certain
connectivity requirements. The general problem is:
Survivable Network Design Problem
Instance:
An undirected graph G with weights c : E(G) →R+, and a con-
nectivity requirement rxy ∈Z+ for each (unordered) pair of vertices
x, y.
Task:
Find a minimum weight spanning subgraph H of G such that for
each x, y there are at least rxy edge-disjoint paths from x to y in
H.
Practical applications arise for example in the design of telecommunication
networks which can “survive” certain edge failures.
A related problem allows edges to be picked arbitrarily often (see Goemans
and Bertsimas [1993], Bertsimas and Teo [1997]). However, this can be regarded
as a special case since G can have many parallel edges.
In Sections 20.1 and 20.2 we ﬁrst consider the Steiner Tree Problem, which
is a well-known special case. Here we have a set T ⊆V (G) of so-called terminals
such that rxy = 1 if x, y ∈T and rxy = 0 otherwise. We look for a shortest
network connecting all terminals; such a network is called a connector, and a
minimal connector is a Steiner tree:
Deﬁnition 20.1.
Let G be an undirected graph and T ⊆V (G). A connector for
T is a connected graph Y with T ⊆V (Y). A Steiner tree for T in G is a tree
S with T ⊆V (S) ⊆V (G) and E(S) ⊆E(G). The elements of T are called
terminals, those of V (S) \ T are the Steiner points of S.
Sometimes it is also required that all leaves of a Steiner tree are terminals;
evidently this can always be achieved by deleting edges.
In Section 20.3 we turn to the general Survivable Network Design Prob-
lem, and we give two approximation algorithms in Sections 20.4 and 20.5. While
the ﬁrst one is faster, the second one can always guarantee a performance ratio of
2 in polynomial time.

468
20. Network Design Problems
20.1 Steiner Trees
In this section we consider the following problem:
Steiner Tree Problem
Instance:
An undirected graph G, weights c : E(G) →R+, and a set T ⊆
V (G).
Task:
Find a Steiner tree S for T in G whose weight c(E(S)) is minimum.
We have already dealt with the special cases T = V (G) (spanning tree) and
|T | = 2 (shortest path) in Chapters 6 and 7. While we had a polynomial-time
algorithm in both of these cases, the general problem is NP-hard.
Theorem 20.2.
(Karp [1972]) The Steiner Tree Problem is NP-hard, even for
unit weights.
Proof:
We describe a transformation from Vertex Cover which is NP-complete
by Corollary 15.24. Given a graph G, we consider the graph H with vertices
V (H) := V (G)
.
∪E(G) and edges {v, e} for v ∈e ∈E(G) and {v, w} for
v, w ∈V (G), v ̸= w. See Figure 20.1 for an illustration. We set c(e) := 1 for all
e ∈E(H) and T := E(G).
Fig. 20.1.
Given a vertex cover X ⊆V (G) of G, we can connect X in H by a tree of
|X|−1 edges and join each of the vertices in T by an edge. We obtain a Steiner tree
with |X|−1+|E(G)| edges. On the other hand, let (T ∪X, F) be a Steiner tree for
T in H. Then X is a vertex cover in G and |F| = |T ∪X|−1 = |X|+|E(G)|−1.
Hence G has a vertex cover of cardinality k if and only if H has a Steiner
tree for T with k + |E(G)| −1 edges.
2
This transformation yields also the following stronger result:
Theorem 20.3.
(Bern and Plassmann [1989])
The Steiner Tree Problem is
MAXSNP-hard, even for unit weights.

20.1 Steiner Trees
469
Proof:
The transformation in the above proof is not an L-reduction in general, but
we claim that it is one if G has bounded degree. By Theorem 16.39 the Minimum
Vertex Cover Problem for graphs with maximum degree 4 is MAXSNP-hard.
For each Steiner tree (T ∪X, F) in H and the corresponding vertex cover X
in G we have
|X| −OPT(G)
=
(|F| −|E(G)| + 1) −(OPT(H, T ) −|E(G)| + 1)
=
|F| −OPT(H, T ).
Moreover, OPT(H, T ) ≤2|T | −1 = 2|E(G)| −1 and OPT(G) ≥|E(G)|
4
if G has
maximum degree 4. Hence OPT(H, T ) < 8 OPT(G), and we conclude that the
transformation is indeed an L-reduction.
2
Two variants of the Steiner Tree Problem in graphs are also NP-hard: the
Euclidean Steiner Tree Problem (Garey, Graham and Johnson [1977]) and the
Manhattan Steiner Tree Problem (Garey and Johnson [1977]). Both ask for
a network (set of straight line segments) of minimum total length which connects
a given set of points in the plane. The difference between these two problems
is that only horizontal and vertical line segments are allowed in the Manhattan
Steiner Tree Problem. In contrast to the MAXSNP-hard Steiner Tree Problem
in graphs both geometric versions have an approximation scheme. A variant of
this algorithm (which is due to Arora [1998]) also solves the Euclidean TSP
(and some other geometric problems) and will be presented in Section 21.2.
Hanan [1966] showed that the Manhattan Steiner Tree Problem can be
reduced to the Steiner Tree Problem in ﬁnite grid graphs: there always exists an
optimum solution where all line segments lie on the grid induced by the coordinates
of the terminals. The Manhattan Steiner Tree Problem is important in VLSI-
design where electrical components must be connected with horizontal and vertical
wires; see Korte, Pr¨omel and Steger [1990], Martin [1992] and Hetzel [1995]. Here
one looks for many disjoint Steiner trees. This is a generalization of the Disjoint
Paths Problem discussed in Chapter 19.
We shall now describe a dynamic programming algorithm due to Dreyfus and
Wagner [1972]. This algorithm solves the Steiner Tree Problem exactly but has
in general exponential running time.
The Dreyfus-Wagner Algorithm computes the optimum Steiner tree for all
subsets of T , starting with the two-element sets. It uses the following recursion
formulas:
Lemma 20.4.
Let (G, c, T ) be an instance of the Steiner Tree Problem. For
each U ⊆T and x ∈V (G) \ U we deﬁne
p(U)
:=
min{c(E(S)) : S is a Steiner tree for U in G};
q(U ∪{x}, x)
:=
min{c(E(S)) : S is a Steiner tree for U ∪{x} in G
whose leaves are elements of U}.
Then we have for all U ⊆V (G), |U| ≥2 and x ∈V (G) \ U:

470
20. Network Design Problems
(a) q(U ∪{x}, x) = min∅̸=U ′⊂U

p(U ′ ∪{x}) + p((U \ U ′) ∪{x})

,
(b) p(U ∪{x}) = min

miny∈U

p(U) + dist(G,c)(x, y)

,
miny∈V (G)\U

q(U ∪{y}, y) + dist(G,c)(x, y)
 
.
Proof:
(a): Every Steiner tree S for U ∪{x} whose leaves are elements of U is
the disjoint union of two trees, each containing x and at least one element of U.
Equation (a) follows.
(b): The inequality “≤” is obvious. Consider an optimum Steiner tree S for
U ∪{x}. If |δS(x)| ≥2, then
p(U ∪{x}) = c(E(S)) = q(U ∪{x}, x) = q(U ∪{x}, x) + dist(G,c)(x, x).
If |δS(x)| = 1, then let y be the nearest vertex from x in S that belongs to U or
has |δS(y)| ≥3. We distinguish two cases: If y ∈U, then
p(U ∪{x}) = c(E(S)) ≥p(U) + dist(G,c)(x, y),
otherwise
p(U ∪{x}) = c(E(S)) ≥
min
y∈V (G)\U

q(U ∪{y}, y) + dist(G,c)(x, y)

.
In (b), the minimum over these three formulas is computed.
2
These recursion formulas immediately suggest the following dynamic program-
ming algorithm:
Dreyfus-Wagner Algorithm
Input:
An undirected graph G, weights c : E(G) →R+, and a set T ⊆
V (G).
Output:
The length p(T ) of an optimum Steiner tree for T in G.
1⃝
If |T | ≤1 then set p(T ) := 0 and stop.
Compute dist(G,c)(x, y) for all x, y ∈V (G).
Set p({x, y}) := dist(G,c)(x, y) for all x, y ∈V (G).
2⃝
For k := 2 to |T | −1 do:
For all U ⊆T with |U| = k and all x ∈V (G) \ U do:
Set q(U ∪{x}, x) :=
min
∅̸=U ′⊂U

p(U ′ ∪{x}) + p((U \ U ′) ∪{x})

.
For all U ⊆T with |U| = k and all x ∈V (G) \ U do:
Set p(U ∪{x}) := min

min
y∈U

p(U) + dist(G,c)(x, y)

,
min
y∈V (G)\U

q(U ∪{y}, y) + dist(G,c)(x, y)


.

20.1 Steiner Trees
471
Theorem 20.5.
(Dreyfus and Wagner [1972])
The Dreyfus-Wagner Algo-
rithm correctly determines the length of an optimum Steiner tree in O

3tn + 2tn2
+n3
time, where n = |V (G)| and t = |T |.
Proof:
The correctness follows from Lemma 20.4. 1⃝consists of solving an All
Pairs Shortest Paths Problem which can be done in O(n3) time by Theorem
7.9.
The ﬁrst recursion in 2⃝requires O

3tn

time since there are 3t possibilities
to partition T into U ′, U \ U ′, and T \ U. The second recursion in 2⃝obviously
requires O

2tn2
time.
2
In the present form the Dreyfus-Wagner Algorithm computes the length
of an optimum Steiner tree, but not the Steiner tree itself. However, this can easily
be achieved by storing some additional information and backtracking. We have
already discussed this in detail with respect to Dijkstra’s Algorithm (Section
7.1).
Note that the algorithm in general requires exponential time and exponential
space. For a bounded number of terminals it is an O(n3)-algorithm. There is
another interesting special case where it runs in polynomial time (and space): if G
is a planar graph and all terminals lie on the outer face, then the Dreyfus-Wagner
Algorithm can be modiﬁed to run in O

n3t2
time (Exercise 3).
Since we cannot hope for an exact polynomial-time algorithm for the general
Steiner Tree Problem, approximation algorithms are valuable. One idea under-
lying some of these algorithms is to approximate the optimum Steiner tree for T
in G by a minimum weight spanning tree in the subgraph of the metric closure of
G induced by T .
Theorem 20.6.
Let G be a connected graph with weights c : E(G) →R+, and
let ( ¯G, ¯c) be the metric closure. Moreover, let T ⊆V (G). If S is an optimum
Steiner tree for T in G, and M is a minimum weight spanning tree in ¯G[T ] (with
respect to ¯c), then ¯c(E(M)) ≤2c(E(S)).
Proof:
Consider the graph H containing two copies of each edge of S. H is
Eulerian, so by Theorem 2.24 there exists an Eulerian walk W in H. The ﬁrst
appearance of the elements of T in W deﬁnes a tour W ′ in ¯G[T ]. Since ¯c satisﬁes
the triangle inequality (¯c({x, z}) ≤¯c({x, y}) + ¯c({y, z}) ≤c({x, y}) + c({y, z}) for
all x, y, z),
¯c(W ′) ≤c(W) = c(E(H)) = 2c(E(S)).
Since W ′ contains a spanning tree of ¯G[T ] (just delete one edge) the theorem is
proved.
2
This theorem was published by Gilbert and Pollak [1968] (referring to E.F.
Moore), Choukhmane [1978], Kou, Markowsky and Berman [1981], and Takahashi
and Matsuyama [1980]. It immediately suggests the following 2-factor approxi-
mation algorithm:

472
20. Network Design Problems
Kou-Markowsky-Berman Algorithm
Input:
A connected undirected graph G, weights c : E(G) →R+, and a set
T ⊆V (G).
Output:
A Steiner tree for T in G.
1⃝
Compute the metric closure ( ¯G, ¯c) and a shortest path Pst for all s, t ∈T .
2⃝
Find a minimum weight spanning tree M in ¯G[T ] (with respect to ¯c).
Set E(S) :=

{x,y}∈E(M)
E(Pxy) and V (S) :=

e∈E(S)
e.
3⃝
Output a minimal connected subgraph of S.
Theorem 20.7.
(Kou, Markowsky and Berman [1981]) The Kou-Markowsky-
Berman Algorithm is a 2-factor approximation algorithm for the Steiner Tree
Problem and runs in O

n3
time, where n = |V (G)|.
Proof:
The correctness and the performance guarantee follow directly from The-
orem 20.6. 1⃝consists of the solution of an All Pairs Shortest Paths Problem,
which can be done in O

n3
time (Theorem 7.9, Corollary 7.11). 2⃝can be done
in O

n2
time using Prim’s Algorithm (Theorem 6.5). 3⃝can be implemented
with BFS with O(n2) time.
2
Mehlhorn [1988] and Kou [1990] proposed an O

n2
-implementation of this
algorithm. The idea is to compute, instead of ¯G[T ], a similar graph whose mini-
mum weight spanning trees are also minimum weight spanning trees in ¯G[T ].
The minimum weight spanning tree itself yields a 2-factor approximation for
any metric instance of the Steiner Tree Problem. For the Euclidean Steiner
Tree Problem and the Manhattan Steiner Tree Problem the so-called Steiner
ratio, i.e. the ratio of minimum weight spanning tree to optimum Steiner tree, is
even better, namely
2
√
3 (Du and Hwang [1992]) and 3
2 (Hwang [1976]), respec-
tively.
An algorithm with a better performance ratio than the optimum spanning tree
was not known until Zelikovsky [1993] came up with an 11
6 -factor approxima-
tion algorithm for the Steiner Tree Problem. The approximation ratio has sub-
sequently been improved to 1.75 by Berman and Ramaiyer [1994], to 1.65 by
Karpinski and Zelikovsky [1997], to 1.60 by Hougardy and Pr¨omel [1999] and to
1 + ln 3
2
≈1.55 by Robins and Zelikovsky [2000]. This currently best algorithm
will be presented in the next section. On the other hand, by Theorem 20.3 and
Corollary 16.33, an approximation scheme cannot exist unless P = NP. Indeed,
Clementi and Trevisan [1999] showed that, unless P = NP, there is no 1.0006-
factor approximation algorithm for the Steiner Tree Problem. See also Thimm
[2003].

20.2 The Robins-Zelikovsky Algorithm
473
An algorithm which computes optimum Steiner trees and is quite efﬁcient,
especially in the Manhattan Steiner Tree Problem, was developped by Warme,
Winter and Zachariasen [2000].
20.2 The Robins-Zelikovsky Algorithm
Deﬁnition 20.8.
A full Steiner tree for a terminal set T in a graph G is a tree Y in
G where T is the set of leaves of Y. Every Steiner tree for T can be decomposed into
full Steiner trees for subsets of T , its full components. Unions of full components
each of which has at most k terminals are called k-restricted (with respect to
the given terminal set). More precisely, a graph Y is called k-restricted (in G with
respect to T ) if there are full Steiner trees Yi for T ∩V (Yi) in G with |T ∩V (Yi)| ≤k
(i = 1, . . . , t), such that V (Y) = t
i=1 V (Yi), and E(Y) is the disjoint union of
the sets E(Yi). Note that parallel edges may arise.
We deﬁne the k-Steiner ratio as
ρk := sup
(G,c,T )
min{c(E(Y)) : Y k-restricted connector for T }
min{c(E(Y)) : Y Steiner tree for T }

,
where the supremum is taken over all instances of the Steiner Tree Problem.
For example, 2-restricted connectors are composed of paths between terminals.
So optimum 2-restricted connectors for T in (G, c) correspond to minimum weight
spanning trees in ( ¯G[T ], ¯c); thus ρ2 ≤2 by Theorem 20.6. The stars with unit
weights show that in fact ρ2 = 2 (and in general ρk ≥
k
k−1).
Theorem 20.9.
(Du, Zhang and Feng [1991]) ρ2s ≤s+1
s .
Proof:
Let (G, c, T ) be an instance and Y an optimum Steiner tree. W.l.o.g. Y
is a full Steiner tree (otherwise handle full components separately). Moreover, by
duplicating vertices and adding edges of length zero, we may assume Y to be a
full binary tree whose leaves are the terminals. One vertex, the root, has degree
2, and all other Steiner points have degree 3. We say that a vertex v ∈V (Y) is at
level i if its distance from the root is i. All terminals are at the same level h (the
height of the binary tree).
We deﬁne s 2s-restricted connectors for T , which have total length at most
(s + 1)c(E(Y)). For v ∈V (Y), let P(v) be a path in Y from v to some leaf, such
that all these paths are edge-disjoint (for example, descend once to the left, and
then always to the right).
For i = 1, . . . , s, let Yi be the union of the following full components:
– the subtree of Y induced by the vertices up to level i, plus P(v) for every
vertex v on level i;
– for each vertex u on level ks + i: the subtree induced by the successors of u
up to level (k + 1)s + i, plus P(v) for every vertex v in the subtree at level
(k + 1)s + i (k = 0, . . . , ⌊h−1−i
s
⌋−1);

474
20. Network Design Problems
– for each vertex u on level ⌊h−1−i
s
⌋s + i: the subtree induced by all successors
of u.
Clearly, each of these trees is 2s-restricted, and the union of the trees in Yi is Y,
i.e. is a connector for T . Moreover, each edge of Y is contained once in each set
Yi, not counting the appearence in a path P(v). Moreover, each P(v) is used in
only one Yi. Thus each edge appears at most s + 1 times.
2
In particular, ρk →1 as k →∞. Thus we cannot expect to ﬁnd the optimum
k-restricted connector in polynomial time for ﬁxed k. Indeed, this problem is NP-
hard for every ﬁxed k ≥4 (Garey and Johnson [1977]). The bound of Theorem
20.9 is sharp: Borchers and Du [1997] proved that ρk = (s+1)2s+t
s2s+t
for all k ≥2,
where k = 2s + t and 0 ≤t < 2s.
We will present an algorithm that starts with a minimum spanning tree in the
subgraph of the metric closure induced by T , and tries to improve it by using
k-restricted full Steiner trees. However, the algorithm only decides for including
at most half of such a Steiner tree, its so-called loss. For each Steiner tree Y we
deﬁne a loss of Y to be a minimum cost edge set F connecting each Steiner point
of degree at least 3 to a terminal. See Figure 20.2 for an example of a full Steiner
tree and its loss (bold edges), assuming that the cost of an edge is proportional to
its length.
Fig. 20.2.
Proposition 20.10.
Let Y be a full Steiner tree for T , c : E(Y) →R+, and let F
be a loss of Y. Then c(F) ≤1
2c(E(Y)).
Proof:
Let r ∈V (Y) arbitrarily. For each Steiner point v ∈V (Y) \ T of degree
at least 3, let P(v) be a minimum cost one among the (at least two) maximal paths

20.2 The Robins-Zelikovsky Algorithm
475
from v to a vertex w such that w has larger distance from r than v and all inner
vertices have degree 2 in Y. The union of the edge sets of these paths is a loss of
Y and has at most half the total cost.
2
Instead of explicitly contracting losses of k-restricted full Steiner trees, we
adjust the costs as follows:
Proposition 20.11.
Let G be a complete graph, T ⊆V (G), c : E(G) →R+
and k ≥2. Let S ⊆T with |S| ≤k, let Y be a Steiner tree for S in G and L
a loss of Y. Let c′(e) := 0 for e ∈L and c′(e) := c(e) otherwise. We deﬁne
c/(Y, L) : E(G) →R+ by c/(Y, L)({v, w}) := min{c({v, w}), dist(Y,c′)(v, w)} for
v, w ∈S, v ̸= w, and c/(Y, L)(e) := c(e) for all other edges.
Then there exists a spanning tree M in G[S] with c/(Y, L)(E(M)) + c(L) ≤
c(E(Y)).
Moreover, for each k-restricted connector H ′ of T in G there is a k-restricted
connector H of T in G with c(E(H)) ≤c/(Y, L)(E(H ′)) + c(L).
Proof:
The ﬁrst statement is proved by induction on |E(Y)|. We may assume that
Y is a full Steiner tree (otherwise consider full components separately) and |S| > 2.
Then L ̸= ∅, and there exists a terminal v incident to an edge e = {v, w} ∈L.
Applying the induction hypothesis to Y ′ := Y −e and (S \ {v}) ∪{w} yields a
spanning tree M′ with c/(Y ′, L \ {e})(M′) ≤c(E(Y ′)) −c(L \ {e}) = c(E(Y)) −
c(L). Replacing w by v in M does not change the cost as c′(e) = 0.
For the second statement, let H ′ be a k-restricted connector of G. Replace
each edge e = {v, w} ∈E(H ′) with c/(Y, L)(e) < c(e) by a shortest v-w-
path in (Y, c′), and eliminate parallel edges. Then the resulting graph H is a
k-restricted connector of T and satisﬁes c(E(H)) = c′(E(H)) + c(E(H) ∩L) ≤
c/(Y, L)(E(H ′)) + c(L).
2
We will repeatedly modify the cost function by adding edges corresponding to
full components. The following observation says that the reduction of the cost of
a minimum spanning tree does not increase when other edges have been inserted
before:
Lemma 20.12.
(Zelikovsky [1993], Berman and Ramaiyer [1994]) Let G be a
graph, c : E(G) →R+, T ⊆V (G), (T, U) another graph, c′ : U →R+. Let m :
2U →R+, where m(X) is the cost of a minimum spanning tree in (T, E(G[T ])∪X).
Then m is supermodular.
Proof:
Let A ⊆U and f ∈U. We run Kruskal’s Algorithm in parallel
on G[T ] and on G[T ] + f , examining edges of G[T ] in the same order (with
nondecreasing cost). Both version run exactly the same way, except that the ﬁrst
one does not choose f , while the second one does not choose the ﬁrst edge that
closes a circuit in G + f containing f . Thus the minimum costs of the spanning
trees in the two graphs differ by min{γ : G[T ] + f contains a circuit containing
f whose edges have cost at most γ } −c′( f ). Clearly, this difference can only

476
20. Network Design Problems
decrease if we consider G[T ] + A and (G[T ] + A) + f instead of G[T ] and
G[T ] + f . Hence
m(A) −m(A ∪{ f }) ≤m(∅) −m({ f }).
Now let X, Y ⊆U, Y \ X = {y1, . . . , yk}, and write mi(A) := m((X ∩Y) ∪
{y1, . . . , yi−1} ∪A) for i = 1, . . . , k. By applying the above to mi we get
m(X) −m(X ∪Y)
=
k

i=1
(mi(X \ Y) −mi((X \ Y) ∪{yi}))
≤
k

i=1
(mi(∅) −mi({yi}))
=
m(X ∩Y) −m(Y),
i.e. supermodularity.
2
We now describe the algorithm. Denote by mst(c) the minimum cost of a
spanning tree in the subgraph of ( ¯G, c) induced by T .
Robins-Zelikovsky Algorithm
Input:
An undirected graph G, weights c : E(G) →R+, and a set T ⊆
V (G) of terminals. A number k ≥2.
Output:
A Steiner tree for T in G.
1⃝
Compute the metric closure ( ¯G, ¯c) of (G, c).
2⃝
Choose a subset S of at most k terminals and a pair K = (Y, L), where Y is
an optimum Steiner tree for S and L is a loss of Y, such that mst(¯c)−mst(¯c/K)
¯c(L)
is maximum and at least 1.
If such a choice is impossible, then go to 4⃝.
3⃝
Set ¯c := ¯c/K.
Go to 2⃝.
4⃝
Let H be a minimum spanning tree in the subgraph of ( ¯G, ¯c) induced by
T . Replace all edges by shortest paths in (G, c′), where c′(e) := 0 if e ∈L
for some L chosen in the algorithm and c′(e) = c(e) otherwise. Finally
compute a minimal connected subgraph spanning T .
Suppose that the algorithm stops in iteration t + 1, and let Ki := (Yi, Li) be
the Steiner tree and its loss chosen in the i-th iteration (i = 1, . . . , t). Let c0 be
the cost function ¯c after 1⃝, and let ci := ci−1/Ki be the cost function ¯c after
i iterations (i = 1, . . . , t). Then by Proposition 20.11 the algorithm computes a
solution of total cost at most mst(ct) + t
i=1 c(Li).
Let Y ∗be an optimum k-restricted connector for T , let Y ∗
1 , . . . , Y ∗
t∗be k-
restricted full Steiner trees whose union is Y ∗, let L∗
j a loss of Y ∗
j and K ∗
j =

20.2 The Robins-Zelikovsky Algorithm
477
(Y ∗
j , L∗
j ) ( j = 1, . . . , t∗), and let L∗:= L∗
1 ∪· · · ∪L∗
t∗. We write c/K ∗instead of
c/K ∗
1/ · · · /K ∗
t∗From Proposition 20.11 we obtain:
Lemma 20.13.
The algorithm computes a Steiner tree for T of weight at most
mst(ct) + t
i=1 c(Li). Moreover, c(E(Y ∗)) = mst(c/K ∗) + c(L∗).
2
Lemma 20.14.
mst(ct) ≤c(E(Y ∗)) ≤mst(c0).
Proof:
c(E(Y ∗
k )) ≤mst(c0) is trivial. When the algorithm terminates, mst(ct)−
mst(ct/K ∗
j ) ≤c(L∗
j ) for j = 1, . . . , t∗. Hence, using Lemma 20.12,
mst(ct) −mst(c/K ∗)
≤
mst(ct) −mst(ct/K ∗)
=
t∗

j=1
(mst(ct/K ∗
1/ · · · /K ∗
j−1) −mst(ct/K ∗
1/ · · · /K ∗
j ))
≤
t∗

j=1
(mst(ct) −mst(ct/K ∗
j ))
≤
t∗

j=1
c(L∗
j ),
implying mst(ct) ≤mst(c/K ∗) + c(L∗).
2
Lemma 20.15.
mst(ct) + t
i=1 c(Li) ≤c(E(Y ∗))(1 + ln 3
2 ).
Proof:
Let i ∈{1, . . . , t}. By the choice of Li in iteration i of the algorithm,
mst(ci−1) −mst(ci)
c(Li)
≥
t∗
max
j=1
mst(ci−1) −mst(ci−1/K ∗
j )
c(L∗
j )
≥
t∗
j=1(mst(ci−1) −mst(ci−1/K ∗
j ))
t∗
j=1 c(L∗
j )
≥
t∗
j=1(mst(ci−1/K ∗
1/ · · · /K ∗
j−1) −mst(ci−1/K ∗
1/ · · · /K ∗
j ))
c(L∗)
=
mst(ci−1) −mst(ci−1/K ∗)
c(L∗)
≥
mst(ci−1) −mst(c/K ∗)
c(L∗)
(we used Lemma 20.12 in the third and monotonicity in the last inequality). More-
over, the left-hand side is at least 1. Hence
t
i=1
c(Li)
≤
t
i=1
(mst(ci−1) −mst(ci))
c(L∗)
max{c(L∗), mst(ci−1) −mst(c/K ∗)}
≤
7 mst(c0)
mst(ct)
c(L∗)
max{c(L∗), x −mst(c/K ∗)}dx.

478
20. Network Design Problems
As c(E(Y ∗)) = mst(c/K ∗) + c(L∗) by Lemma 20.13 and mst(ct) ≤c(E(Y ∗)) ≤
mst(c0) by Lemma 20.14, we compute
t
i=1
c(Li)
≤
7 c(E(Y ∗))
mst(ct)
1 dx +
7 mst(c0)−mst(c/K ∗)
c(L∗)
c(L∗)
x
dx
=
c(E(Y ∗)) −mst(ct) + c(L∗) ln mst(c0) −mst(c/K ∗)
c(L∗)
.
As mst(c0) ≤2 OPT(G, c, T ) ≤2c(E(Y ∗)) = c(E(Y ∗)) + mst(c/K ∗) + c(L∗),
we obtain
mst(ct) +
t
i=1
c(Li) ≤c(E(Y ∗))

1 +
c(L∗)
c(E(Y ∗)) ln

1 + c(E(Y ∗))
c(L∗)

.
As 0 ≤c(L∗) ≤1
2c(E(Y ∗)) (by Proposition 20.10) and max{x ln(1 + 1
x ) : 0 <
x ≤1
2} is attained for x = 1
2 (as the derivative ln(1+ 1
x )−
1
x+1 is always positive),
we conclude that mst(ct) + t
i=1 c(Li) ≤c(E(Y ∗))(1 + ln 3
2 ).
2
This proof is essentially due to Gr¨opl et al. [2001]. We conclude:
Theorem 20.16.
(Robins and Zelikovsky [2000]) The Robins-Zelikovsky Algo-
rithm has a performance guarantee of ρk(1 + ln 3
2 ) and runs in polynomial time for
each ﬁxed k. For k sufﬁciently large, the performance guarantee is less than 1.55.
Proof:
By Lemma 20.13, the algorithm outputs a Steiner tree of cost at most
mst(ct) + t
i=1 c(Li). By Lemma 20.15, this is at most ρk(1 + ln 3
2 ). Choosing
k = min{|V (G)|, 22233} and applying Theorem 20.9, we get a performance ratio
of ρ2233(1 + ln 3
2 ) ≤2234
2233(1 + ln 3
2 ) < 1.55.
There are at most nk possible subsets S, and for each one there are at most nk−2
choices for the (at most k −2) Steiner points of degree at least 3 in an optimum
Steiner tree Y for S. Then, given Y, there are at most (2k −3)k−2 choices for a
loss (up to inclusion of edges with zero cost). Hence each iteration takes O(n2k)
time (for ﬁxed k), and there are at most n2k−2 iterations.
2
20.3 Survivable Network Design
Before turning to the general Survivable Network Design Problem we mention
two more special cases. If all connectivity requirements rxy are 0 or 1, the problem
is called the Generalized Steiner Tree Problem (of course the Steiner Tree
Problem is a special case). The ﬁrst approximation algorithm for the General-
ized Steiner Tree Problem was found by Agrawal, Klein and Ravi [1995].
Another interesting special case is the problem of ﬁnding a minimum weight
k-edge-connected subgraph (Here rxy = k for all x, y). See Exercise 6 for a com-
binatorial 2-factor approximation algorithm for this special case and for references
related to this problem.

20.3 Survivable Network Design
479
When considering the general Survivable Network Design Problem, given
connectivity requirements rxy for all x, y ∈V (G), it is useful to deﬁne a function
f : 2V (G) →Z+ by f (∅) := f (V (G)) := 0 and f (S) := maxx∈S, y∈V (G)\S rxy for
∅̸= S ⊂V (G). Then our problem can be formulated as the following integer
linear program:
min

e∈E(G)
c(e)xe
s.t.

e∈δ(S)
xe
≥
f (S)
(S ⊆V (G))
xe
∈
{0, 1}
(e ∈E(G)).
(20.1)
We shall not deal with this integer program in the general form but rather
make use of an important property of f :
Deﬁnition 20.17.
A function f : 2U →Z+ is called proper if it satisﬁes the
following three conditions:
–
f (S) = f (U \ S) for all S ⊆U;
–
If A ∩B = ∅, then f (A ∪B) ≤max{ f (A), f (B)};
–
f (∅) = 0.
It is obvious that f as constructed above is proper. Proper functions were
introduced by Goemans and Williamson [1995] who gave a 2-factor approxima-
tion algorithm for proper functions f with f (S) ∈{0, 1} for all S. For proper
functions f with f (S) ∈{0, 2} for all S, Klein and Ravi [1993] gave a 3-factor
approximation algorithm.
The following property of proper functions is essential:
Proposition 20.18.
A proper function f : 2U →Z+ is weakly supermodular,
i.e. at least one of the following conditions hold for all A, B ⊆U:
–
f (A) + f (B) ≤f (A ∪B) + f (A ∩B).
–
f (A) + f (B) ≤f (A \ B) + f (B \ A).
Proof:
By deﬁnition we have
f (A)
≤
max{ f (A \ B), f (A ∩B)};
(20.2)
f (B)
≤
max{ f (B \ A), f (A ∩B)};
(20.3)
f (A)
=
f (U \ A) ≤max{ f (B \ A), f (U \ (A ∪B))}
(20.4)
=
max{ f (B \ A), f (A ∪B)};
f (B)
=
f (U \ B) ≤max{ f (A \ B), f (U \ (A ∪B))}
(20.5)
=
max{ f (A \ B), f (A ∪B)}.
Now we distinguish four cases, depending on which of the four numbers f (A \
B), f (B \ A), f (A ∩B), f (A ∪B) is the smallest. If f (A \ B) is the smallest,
we add (20.2) and (20.5). If f (B \ A) is the smallest, we add (20.3) and (20.4). If

480
20. Network Design Problems
f (A ∩B) is the smallest, we add (20.2) and (20.3). If f (A ∪B) is the smallest,
we add (20.4) and (20.5).
2
In the rest of this section we show how to solve the LP relaxation of (20.1):
min

e∈E(G)
c(e)xe
s.t.

e∈δ(S)
xe
≥
f (S)
(S ⊆V (G))
xe
≥
0
(e ∈E(G))
xe
≤
1
(e ∈E(G)).
(20.6)
We do not know how to solve this LP in polynomial time for arbitrary functions
f , not even for arbitrary weakly supermodular functions. Therefore we restrict
ourselves to the case when f is proper. By Theorem 4.21 it sufﬁces to solve the
Separation Problem. We use a Gomory-Hu tree:
Lemma 20.19.
Let G be an undirected graph with capacities u ∈RE(G)
+
, and let
f : 2V (G) →Z+ be a proper function. Let H be a Gomory-Hu tree for (G, u).
Then for each ∅̸= S ⊂V (G) we have:
(a) 
e∈δG(S) u(e) ≥maxe∈δH(S)

e∈δG(Ce) u(e);
(b) f (S) ≤maxe∈δH(S) f (Ce);
where Ce and V (H) \ Ce are the two connected components of H −e.
Proof:
(a): By deﬁnition of the Gomory-Hu tree, δG(Ce) is a minimum capacity
x-y-cut for each e = {x, y} ∈E(H), and for {x, y} ∈δH(S) the left-hand side of
(a) is the capacity of some x-y-cut.
To prove (b), let X1, . . . , Xl be the connected components of H −S. Since
H[Xi] is connected and H is a tree we have for each i ∈{1, . . . ,l}:
V (H) \ Xi =

e∈δH(Xi)
Ce
(if necessary, replace Ce by V (H) \ Ce). Since f is proper we have
f (Xi) = f (V (G)\Xi) = f (V (H)\Xi) = f
⎛
⎝

e∈δH(Xi)
Ce
⎞
⎠≤
max
e∈δH(Xi) f (Ce).
Since {e ∈δH(Xi)} ⊆{e ∈δH(S)}, we conclude that
f (S) =
f (V (G) \ S) =
f
 l
i=1
Xi

≤
max
i∈{1,...,l} f (Xi) ≤
max
e∈δH(S) f (Ce). 2
Now we can show how to solve the Separation Problem for (20.6) by con-
sidering the fundamental cuts of a Gomory-Hu tree. Note that storing the proper
function f explicitly would require exponential space, so we assume that f is
given by an oracle.

20.4 A Primal-Dual Approximation Algorithm
481
Theorem 20.20.
Let G be an undirected graph, x ∈RE(G)
+
, and let f : 2V (G) →
Z+ be a proper function (given by an oracle). One can ﬁnd a set S ⊆V (G)
with 
e∈δG(S) xe < f (S) or decide that none exists in O

n4 + nθ

time. Here
n = |V (G)| and θ is the time required by the oracle for f .
Proof:
We ﬁrst compute a Gomory-Hu tree H for G, where the capacities are
given by x. H can be computed in O(n4) time by Theorem 8.35.
By Lemma 20.19(b) we have that for each ∅̸= S ⊂V (G) there exists an
e ∈δH(S) with f (S) ≤f (Ce). From Lemma 20.19(a) we get f (S)−
e∈δG(S) xe ≤
f (Ce) −
e∈δG(Ce) xe. We conclude
max
∅̸=S⊂V (G)
⎛
⎝f (S) −

e∈δG(S)
xe
⎞
⎠=
max
e∈E(H)
⎛
⎝f (Ce) −

e∈δG(Ce)
xe
⎞
⎠.
(20.7)
Hence the Separation Problem for (20.6) can be solved by checking only n −1
cuts.
2
It is worthwhile to compare (20.7) with Theorem 12.17.
In contrast to the LP relaxation (20.6) we cannot hope to ﬁnd an optimum
integral solution in polynomial time: by Theorem 20.2 this would imply P = NP.
So we consider approximation algorithms for (20.1).
In the following section we describe a primal-dual approximation algorithm
which subsequently adds edges in most violated cuts. This combinatorial algorithm
performs well if the maximum connectivity requirement k := maxS⊆V (G) f (S) is
not too large. In particular it is a 2-factor approximation algorithm for the case
k = 1, which includes the Generalized Steiner Tree Problem. In Section 20.5
we describe a 2-factor approximation algorithm for the general case. However,
this algorithm has the drawback that it uses the above solution of LP relaxation
which has a polynomial running time but is too inefﬁcient for practical purposes.
20.4 A Primal-Dual Approximation Algorithm
The algorithm which we present in this section was developped in the papers of
Williamson et al. [1995], Gabow, Goemans and Williamson [1998], and Goemans
et al. [1994], in this order.
Suppose an undirected graph G with weights c : E(G) →R+, and a proper
function f are given. We are looking for an edge set F whose incidence vector
satisﬁes (20.1).
The algorithm proceeds in k := maxS⊆V (G) f (S) phases. Since f is proper we
have k = maxv∈V (G) f ({v}), so k can be computed easily. In phase p (1 ≤p ≤k)
the proper function fp is considered, where fp(S) := max{ f (S) + p −k, 0}. It
will be guaranteed that after phase p the current edge set F (or, more precisely,
its characteristic vector) satisﬁes (20.1) with respect to fp. Let us start with some
deﬁnitions.

482
20. Network Design Problems
Deﬁnition 20.21.
Given some proper function g, some F ⊆E(G) and X ⊆
V (G), we say that X is violated with respect to (g, F) if |δF(X)| < g(X). The
minimal violated sets with respect to (g, F) are the active sets with respect to
(g, F). F ⊆E(G) satisﬁes g if no set is violated with respect to (g, F). We say
that F almost satisﬁes g if |δF(X)| ≥g(X) −1 for all X ⊆V (G).
Throughout the algorithm, the current function fp will be almost satisﬁed by
the current set F. The active sets will play a central role. A key observation is the
following:
Lemma 20.22.
Given some proper function g, some F ⊆E(G) almost satisfying
g, and two violated sets A and B. Then either A \ B and B \ A are both violated
or A ∩B and A ∪B are both violated. In particular, the active sets with respect
to (g, F) are pairwise disjoint.
Proof:
Directly from Proposition 20.18 and Lemma 2.1(c).
2
This lemma shows in particular that there can be at most n = |V (G)| active
sets. We now show how to compute the active sets; similarly to the proof of
Theorem 20.20 we use a Gomory-Hu tree.
Theorem 20.23.
(Gabow, Goemans and Williamson [1998])
Given a proper
function g (by an oracle) and a set F ⊆E(G) almost satisfying g. Then the
active sets with respect to (g, F) can be computed in O

n4 + n2θ

time. Here
n = |V (G)| and θ is the time required by the oracle for g.
Proof:
We ﬁrst compute a Gomory-Hu tree H for (V (G), F) (and unit capaci-
ties). H can be computed in O(n4) time by Theorem 8.35. By Lemma 20.19 we
have for each ∅̸= S ⊂V (G):
|δF(S)| ≥
max
e∈δH(S) |δF(Ce)|
(20.8)
and
g(S) ≤
max
e∈δH(S) g(Ce),
(20.9)
where Ce and V (H) \ Ce are the two connected components of H −e.
Let A be an active set. By (20.9), there exists an edge e = {s, t} ∈δH(A) with
g(A) ≤g(Ce). By (20.8), |δF(A)| ≥|δF(Ce)|. So we have
1 = g(A) −|δF(A)| ≤g(Ce) −|δF(Ce)| ≤1,
because F almost satisﬁes g. We must have equality throughout, in particular
|δF(A)| = |δF(Ce)|. So δF(A) is a minimum s-t-cut in (V (G), F). Let us assume
w.l.o.g. that A contains t but not s.
Let G′ be the digraph (V (G), {(v, w), (w, v) : {v, w} ∈F}). Consider a max-
imum s-t-ﬂow f in G′ and the residual graph G′
f . Form an acyclic digraph G′′
from G′
f by contracting the set S of vertices reachable from s to a vertex vS,
contracting the set T of vertices from which t is reachable to a vertex vT , and

20.4 A Primal-Dual Approximation Algorithm
483
contracting each strongly connected component X of G′
f −(S ∪T ) to a vertex vX.
There is a one-to-one correspondence between the minimum s-t-cuts in G′ and the
directed vT -vS-cuts in G′′ (cf. Exercise 5 of Chapter 8; this follows easily from the
Max-Flow-Min-Cut Theorem 8.6 and Lemma 8.3). In particular, A is the union
of sets X with vX ∈V (G′′). Since g(A) > |δF(A)| = |δ−
G′(A)| = value ( f ) and g
is proper, there exists a vertex vX ∈V (G′′) with X ⊆A and g(X) > value ( f ).
We now show how to ﬁnd A. If g(T ) > value ( f ), then set Z := T , else let vZ
be any vertex of G′′ with g(Z) > value ( f ) and g(Y) ≤value ( f ) for all vertices
vY ∈V (G′′) \ {vZ} from which vZ is reachable. Let
B := T ∪

{Y : vZ is reachable from vY in G′′}.
Since
value ( f ) < g(Z) = g(V (G) \ Z)
≤
max{g(V (G) \ B), g(B \ Z)}
=
max{g(B), g(B \ Z)}
and
g(B \ Z) ≤max{g(Y) : vY ∈V (G′′) \ {vZ}, Y ⊆B} ≤value ( f )
we have g(B) > value ( f ) = δ−
G′(B) = δF(B), so B is violated with respect to
(g, F). Since B is not a proper subset of A (as A is active) and both A and B
contain T , we conclude from Lemma 20.22 that A ⊆B. But then Z = X, as vZ
is the only vertex with Z ⊆B and g(Z) > value ( f ), and A contains all sets Y
for which vZ is reachable from vY (as δ−
G′
f (A) = ∅). Hence A = B.
For a given pair (s, t) a set B as above (if existent) can be found in linear time
by constructing G′′ (using the Strongly Connected Component Algorithm)
and then ﬁnding a topological order of G′′ (cf. Theorem 2.20), starting with vT .
We repeat the above procedure for all ordered pairs (s, t) such that {s, t} ∈E(H).
In this way we obtain a list of at most 2n −2 candidates for active sets. The
running time is evidently dominated by ﬁnding O(n) times a maximum ﬂow in G′
plus asking O(n2) times the oracle for g. Finally we can eliminate those violated
sets among the candidates that are not minimal in O(n2) time.
2
The running time can be improved if maxS⊆V (G) g(S) is small (see Exercise
8). We now turn to the description of the algorithm.
Primal-Dual Algorithm For Network Design
Input:
An undirected graph G, weights c : E(G) →R+, and an oracle for
a proper function f : 2V (G) →Z+.
Output:
A set F ⊆E(G) satisfying f .
1⃝
If E(G) does not satisfy f , then stop (the problem is infeasible).
2⃝
Set F := ∅, k := max
v∈V (G) f ({v}), and p := 1.

484
20. Network Design Problems
3⃝
Set i := 0.
Set π(v) := 0 for all v ∈V (G).
Let A be the family of active sets with respect to (F, fp), where fp is
deﬁned by fp(S) := max{ f (S) + p −k, 0} for all S ⊆V (G).
4⃝
While A ̸= ∅do:
Set i := i + 1.
Set
ϵ := min

c(e) −π(v) −π(w)
|{A ∈A : e ∈δG(A)}| : e = {v, w} ∈

A∈A
δG(A) \ F

,
and let ei be some edge attaining this minimum.
Increase π(v) by ϵ for all v ∈

A∈A
A.
Set F := F ∪{ei}.
Update A.
5⃝
For j := i down to 1 do:
If F \ {ej} satisﬁes fp then set F := F \ {ej}.
6⃝
If p = k then stop, else set p := p + 1 and go to 3⃝.
The feasibility check in
1⃝can be done in O

n4 + nθ

time by Theorem
20.20. Before we discuss how to implement 3⃝and 4⃝, let us show that the output
F is indeed feasible with respect to f . Let us denote by Fp the set F at the end
of phase p (and F0 := ∅).
Lemma 20.24.
At each stage of phase p the set F almost satisﬁes fp and F\Fp−1
is a forest. At the end of the phase p, Fp satisﬁes fp.
Proof:
Since f1(S) = max{0, f (S)+1−k} ≤max{0, maxv∈S f ({v})+1−k} ≤1
(as f is proper), the empty set almost satisﬁes f1.
After
4⃝there are no active sets, so F satisﬁes fp. In
5⃝, this property is
explicitly maintained. Hence each Fp satisﬁes fp and thus almost satisﬁes fp+1
(p = 0, . . . , k −1). To see that F \ Fp−1 is a forest, observe that each edge added
to F belongs to δ(A) for some active set A and must be the ﬁrst edge of δ(A)
added to F in this phase (as |δFp−1(A)| = fp−1(A)). Hence no edge creates a
circuit in F \ Fp−1.
2
So Theorem 20.23 can be applied to determine A. The number of iterations
within each phase is at most n −1. The only remaining implementation issue we
have to discuss is how to determine ϵ and ei in 4⃝.
Lemma 20.25.
Determining ϵ and ei in 4⃝of the algorithm can be done in O(mn)
time per phase.
Proof:
At each iteration of a phase we do the following. First we assign a number
to each vertex according to which active set it belongs to (or zero if none). This
can be done in O(n) time (note that the active sets are disjoint by Lemma 20.22).

20.4 A Primal-Dual Approximation Algorithm
485
For each edge e the number of active sets intersecting e can now be determined
in O(1) time. So ϵ and ei can be determined in O(m) time. There are at most
n −1 iterations per phase, so the time bound is proved.
2
We remark that a sophisticated implementation (Gabow, Goemans and William-
son [1998]) improves this bound to O

n2√log log n

.
Theorem 20.26.
(Goemans et al. [1994]) The Primal-Dual Algorithm For
Network Design returns a set F satisfying f in O

kn5 + kn3θ

time, where
k = maxS⊆V (G) f (S), n = |V (G)| and θ is the time required by the oracle for f .
Proof:
The feasibility of F is guaranteed by Lemma 20.24 since fk = f .
An oracle for each fp of course uses the oracle for f and thus takes time
θ + O(1). Computing the active sets takes O

n4 + n2θ

time (Theorem 20.23),
and this is done O(nk) times. Determining ϵ and ei can be done in O(n3) time
per phase (Lemma 20.25). Everything else can easily be done in O(kn2) time. 2
Exercise 8 shows how to improve the running time to O

k3n3 + kn3θ

. It
can be improved to O

k2n3 + kn2θ

by using a different clean-up step ( 5⃝of
the algorithm) and a more sophisticated implementation (Gabow, Goemans and
Williamson [1998]). For ﬁxed k and θ = O(n) this means that we have an O

n3
-
algorithm. For the special case of the Survivable Network Design Problem ( f
is determined by connectivity requirements rxy) the running time can be improved
to O

k2n2√log log n

.
Now we analyze the performance guarantee of the algorithm and justify that
we have called it a primal-dual algorithm. The dual of (20.6) is:
max

S⊆V (G)
f (S) yS −

e∈E(G)
ze
s.t.

S:e∈δ(S)
yS
≤
c(e) + ze
(e ∈E(G))
yS
≥
0
(S ⊆V (G))
ze
≥
0
(e ∈E(G)).
(20.10)
This dual LP is essential for the analysis of the algorithm.
We show how the algorithm in each phase p implicitly constructs a feasible
dual solution. Starting with y(p) = 0, in each iteration (of phase p) y(p)
A
is increased
by ϵ for each A ∈A. Moreover we set
z(p)
e
:=
⎧
⎨
⎩

S: e∈δ(S)
y(p)
S
if e ∈Fp−1
0
otherwise
.
There is no point in constructing this dual solution explicitly in the algorithm. The
variables π(v) = 
S:v∈S yS (v ∈V (G)) contain all information that is needed.

486
20. Network Design Problems
Lemma 20.27.
(Williamson et al. [1995])
For each p, (y(p), z(p)) as deﬁned
above is a feasible solution of (20.10).
Proof:
The nonnegativity constraints are obviously satisﬁed. The constraints for
e ∈Fp−1 are satisﬁed by deﬁnition of z(p)
e .
Moreover, by 4⃝of the algorithm we have

S:e∈δ(S)
y(p)
S
≤c(e)
for each e ∈E(G) \ Fp−1,
since e is added to F when equality is reached and after that sets S with e ∈δ(S)
are no longer violated with respect to (F, fp) (recall that F \ {e} satisﬁes fp−1 by
Lemma 20.24).
2
Let us denote by OPT(G, c, f ) the optimum value of the integer linear program
(20.1). Next we show:
Lemma 20.28.
(Goemans et al. [1994]) For each p ∈{1, . . . , k} we have

S⊆V (G)
y(p)
S
≤
1
k −p + 1 OPT(G, c, f ).
Proof:
OPT(G, c, f ) is greater than or equal to the optimum value of the LP
relaxation (20.6), and this is bounded from below by the objective value of any
feasible dual solution (by the Duality Theorem 3.16). Since (y(p), z(p)) is feasible
for the dual LP (20.10) by Lemma 20.27 we conclude that
OPT(G, c, f ) ≥

S⊆V (G)
f (S) y(p)
S
−

e∈E(G)
z(p)
e .
Now observe that, for each S ⊆V (G), yS can only become positive if S is
violated with respect to ( fp, Fp−1). So we may conclude that
y(p)
S
> 0 ⇒|δFp−1(S)| ≤f (S) + p −k −1.
We thus obtain:
OPT(G, c, f )
≥

S⊆V (G)
f (S) y(p)
S
−

e∈E(G)
z(p)
e
=

S⊆V (G)
f (S) y(p)
S
−

e∈Fp−1
⎛
⎝
S:e∈δ(S)
y(p)
S
⎞
⎠
=

S⊆V (G)
f (S) y(p)
S
−

S⊆V (G)
|δFp−1(S)| y(p)
S
=

S⊆V (G)
( f (S) −|δFp−1(S)|) y(p)
S
≥

S⊆V (G)
(k −p + 1) y(p)
S .
2

20.4 A Primal-Dual Approximation Algorithm
487
Lemma 20.29.
(Williamson et al. [1995]) At each iteration of any phase p we
have

A∈A
|δFp\Fp−1(A)| ≤2 |A|.
Proof:
We consider some particular iteration of phase p, which we call the
current iteration. Let A denote the family of active sets at the beginning of this
iteration, and let
H := (Fp \ Fp−1) ∩

A∈A
δ(A).
Note that all the edges of H must have been added during or after the current
iteration.
Let e ∈H. Fp \ {e} does not satisfy fp, because otherwise e would have been
deleted in the clean-up step 5⃝of phase p. So let Xe be some minimal violated
set with respect to ( fp, Fp \ {e}). Since Fp \ {e} ⊇Fp−1 almost satisﬁes fp we
have δFp\Fp−1(Xe) = {e}.
We claim that the family X := {Xe : e ∈H} is laminar. For suppose that there
are two edges e, e′ ∈H (say e was added before e′) for which Xe \ Xe′, Xe′ \ Xe,
and Xe ∩Xe′ are all nonempty. Since Xe and Xe′ are violated at the beginning of
the current iteration, either Xe ∪Xe′ and Xe ∩Xe′ are both violated or Xe \ Xe′
and Xe′ \ Xe are both violated (by Lemma 20.22). In the ﬁrst case we have
1 + 1
≤
|δFp\Fp−1(Xe ∪Xe′)| + |δFp\Fp−1(Xe ∩Xe′)|
≤
|δFp\Fp−1(Xe)| + |δFp\Fp−1(Xe′)| = 1 + 1
by the submodularity of |δFp\Fp−1| (Lemma 2.1(c)). We conclude |δFp\Fp−1(Xe ∪
Xe′)| = |δFp\Fp−1(Xe ∩Xe′)| = 1, contradicting the minimal choice of Xe or of
Xe′ because Xe ∩Xe′ could have been chosen instead. The second case is treated
analogously.
Now consider a tree-representation (T, ϕ) of X, where T is an arborescence
(cf. Proposition 2.14). For each e ∈H, Xe is violated at the beginning of the
current iteration because e has not been added yet at that time. So by Lemma
20.22 we have A ⊆Xe or A ∩Xe = ∅for all A ∈A. Hence {ϕ(a) : a ∈A}
contains only one element, denoted by ϕ(A), for each A ∈A. We call a vertex
v ∈V (T ) occupied if v = ϕ(A) for some A ∈A.
We assert that all vertices of T with out-degree 0 are occupied. Namely, for
such a vertex v, ϕ−1(v) is a minimal element of X. A minimal element of X is
violated at the beginning of the current iteration, so it contains an active set and
must thus be occupied. Hence the average out-degree of the occupied vertices is
at most 1.
Observe that there is a one-to-one correspondence between H, X, and E(T )
(see Figure 20.3; (a) shows H, the elements of A (squares) and those of X (circles);
(b) shows T ). We conclude that for each v ∈V (T )
|δT (v)| = |δH({x ∈V (G) : ϕ(x) = v})| ≥

A∈A:ϕ(A)=v
|δFp\Fp−1(A)|.

488
20. Network Design Problems
(a)
(b)
Fig. 20.3.
By summing over all occupied vertices S we obtain:

A∈A
|δFp\Fp−1(A)|
≤

v∈V (T ) occupied
|δT (v)|
≤
2 |{v ∈V (T ) : v occupied}|
≤
2 |A|.
2
The proof of the next lemma shows the role of the complementary slackness
conditions:
Lemma 20.30.
(Williamson et al. [1995]) For each p ∈{1, . . . , k} we have

e∈Fp\Fp−1
c(e) ≤2

S⊆V (G)
y(p)
S .
Proof:
In each phase p the algorithm maintains the primal complementary slack-
ness conditions
e ∈F \ Fp−1 ⇒

S:e∈δ(S)
y(p)
S
= c(e).
So we have

e∈Fp\Fp−1
c(e) =

e∈Fp\Fp−1
⎛
⎝
S:e∈δ(S)
y(p)
S
⎞
⎠=

S⊆V (G)
y(p)
S
|δFp\Fp−1(S)|.
Thus it remains to show that

S⊆V (G)
y(p)
S
|δFp\Fp−1(S)| ≤2

S⊆V (G)
y(p)
S .
(20.11)
At the beginning of phase p we have y(p) = 0, so (20.11) holds. In each
iteration, the left-hand side increases by 
A∈A ϵ|δFp\Fp−1(A)|, while the right-
hand side increases by 2ϵ|A|. So Lemma 20.29 shows that (20.11) is not violated.
2

20.5 Jain’s Algorithm
489
In (20.11) the dual complementary slackness conditions
y(p)
S
> 0 ⇒|δFp(S)| = fp(S)
appear. |δFp(S)| ≥fp(S) holds throughout, while (20.11) roughly means that
|δFp(S)| ≤2 fp(S) is satisﬁed on the average. As we shall see, this implies a
performance ratio of 2 in the case k = 1.
Theorem 20.31.
(Goemans et al. [1994]) The Primal-Dual Algorithm For
Network Design returns a set F which satisﬁes f and whose weight is at
most 2H(k) OPT(G, c, f ) in O

kn5 + kn3θ

time, where n = |V (G)|, k =
maxS⊆V (G) f (S), H(k) = 1 + 1
2 + · · · + 1
k , and θ is the time required by the
oracle for f .
Proof:
The correctness and the running time have been proved in Theorem 20.26.
The weight of F is

e∈F
c(e)
=
k

p=1
⎛
⎝

e∈Fp\Fp−1
c(e)
⎞
⎠
≤
k

p=1
⎛
⎝2

S⊆V (G)
y(p)
S
⎞
⎠
≤
2
k

p=1
1
k −p + 1 OPT(G, c, f )
=
2H(k) OPT(G, c, f )
due to Lemma 20.30 and Lemma 20.28.
2
The primal-dual approximation algorithm presented in this section has been
put into a more general framework by Bertsimas and Teo [1995]. A related, but
apparently more difﬁcult problem arises by considering vertex-connectivity instead
of edge-connectivity (one looks for a subgraph containing at least a speciﬁed
number ri j of vertex-disjoint i- j-paths for each i and j). See the remarks at the
end of the next section.
20.5 Jain’s Algorithm
In this section we present Jain’s [2001] 2-factor approximation algorithm for the
Survivable Network Design Problem. Although it has a much better perfor-
mance guarantee than the Primal-Dual Algorithm For Network Design it is
of less practical value since it is based on the equivalence of optimization and
separation (cf. Section 4.6).

490
20. Network Design Problems
The algorithm starts by solving the LP relaxation (20.6). In fact, it causes no
difﬁculty to have integral capacities u : E(G) →N on the edges, i.e. we are
allowed to pick some edges more than once:
min

e∈E(G)
c(e)xe
s.t.

e∈δ(S)
xe
≥
f (S)
(S ⊆V (G))
xe
≥
0
(e ∈E(G))
xe
≤
u(e)
(e ∈E(G)).
(20.12)
Of course we are eventually looking for an integral solution. By solving the LP
relaxation of an integer program and rounding up one gets a 2-factor approximation
algorithm if the LP relaxation always has a half-integral optimum solution (see
Exercise 8 of Chapter 16 for an example).
However, (20.12) does not have this property. To see this, consider the Petersen
graph (Figure 20.4) with u(e) = c(e) = 1 for all edges e and f (S) = 1 for all
∅̸= S ⊂V (G). Here the optimum value of the LP (20.12) is 5 (xe = 1
3 for all
e is an optimum solution), and every solution of value 5 satisﬁes 
e∈δ(v) xe = 1
for all v ∈V (G). Thus an optimum half-integral solution must have xe = 1
2 for
the edges e of a Hamiltonian circuit and xe = 0 otherwise. However, the Petersen
graph is not Hamiltonian.
Nevertheless the solution of the LP relaxation (20.12) gives rise to a 2-factor
approximation algorithm. The key observation is that for every optimum basic
solution x there is an edge e with xe ≥1
2 (Theorem 20.33). The algorithm will
then round up and ﬁx only these components and consider the remaining problem
which has at least one edge less.
We need some preparation. For a set S ⊆V (G) we denote by χS the incidence
vector of δG(S) (with respect to E(G)). For any feasible solution x of (20.12) we
call a set S ⊆V (G) tight if χSx = f (S).
Fig. 20.4.

20.5 Jain’s Algorithm
491
Lemma 20.32.
(Jain [2001]) Let G be a graph, m := |E(G)|, and f : 2V (G) →
Z+ a weakly supermodular function. Let x be a basic solution of the LP (20.12),
and suppose that 0 < xe < 1 for each e ∈E(G). Then there exists a laminar
family B of m tight subsets of V (G) such that the vectors χB, B ∈B, are linearly
independent (in RE(G)).
Proof:
Let B be a laminar family of tight subsets of V (G) such that the vectors
χB, B ∈B, are linearly independent. Suppose that |B| < m; we show how to
extend B.
Since x is a basic solution of (20.12), i.e. a vertex of the polytope, there are
m linearly independent inequality constraints satisﬁed with equality (Proposition
3.8). Since 0 < xe < 1 for each e ∈E(G) these constraints correspond to a family
S (not necessarily laminar) of m tight subsets of V (G) such that the vectors χS
(S ∈S) are linearly independent. Since |B| < m, there exists a tight set S ⊆V (G)
such that the vectors χB, B ∈B ∪{S} are linearly independent. Choose S such
that
γ (S) := |{B ∈B : B crosses S}|
is minimal, where we say that B crosses S if B ∩S ̸= ∅and B \ S ̸= ∅and
S \ B ̸= ∅.
If γ (S) = 0, then we can add S to B and we are done. So assume that
γ (S) > 0, and let B ∈B be a set crossing S. Since f is weakly supermodular we
have
f (S \ B) + f (B \ S)
≥
f (S) + f (B)
=

e∈δG(S)
xe +

e∈δG(B)
xe
=

e∈δG(S\B)
xe +

e∈δG(B\S)
xe + 2

e∈EG(S∩B,V (G)\(S∪B))
xe
or
f (S ∩B) + f (S ∪B)
≥
f (S) + f (B)
=

e∈δG(S)
xe +

e∈δG(B)
xe
=

e∈δG(S∩B)
xe +

e∈δG(S∪B)
xe + 2

e∈EG(S\B,B\S)
xe
In the ﬁrst case, S\ B and B \ S are both tight and EG(S∩B, V (G)\(S∪B)) = ∅,
implying χS\B + χB\S = χS + χB. In the second case, S ∩B and S ∪B are both
tight and EG(S \ B, B \ S) = ∅, implying χS∩B + χS∪B = χS + χB.
Hence there is at least one set T among S \ B, B \ S, S ∩B and S ∪B
that is tight and has the property that the vectors χB, B ∈B ∪{T }, are linearly
independent. We ﬁnally show that γ (T ) < γ (S); this yields a contradiction to the
choice of S.

492
20. Network Design Problems
Since B crosses S but not T , it sufﬁces to show that there is no C ∈B which
crosses T but not S. Indeed, since T is one of the sets T among S \ B, B \ S,
S ∩B and S ∪B, a set C crossing T but not S must cross B. Since B is laminar
and B ∈B this implies C /∈B.
2
Now we can prove the crucial theorem for Jain’s Algorithm:
Theorem 20.33.
(Jain [2001]) Let G be a graph and f : 2V (G) →Z+ a weakly
supermodular function that is not identically zero. Let x be a basic solution of the
LP (20.12). Then there exists an edge e ∈E(G) with xe ≥1
2.
Proof:
We may assume xe > 0 for each edge e, since otherwise we can delete e.
In fact we assume 0 < xe < 1
2 for all e ∈E(G) and will deduce a contradiction.
By Lemma 20.32 there exists a laminar family B of m := |E(G)| tight subsets
of V (G) such that the vectors χB, B ∈B, are linearly independent. The linear
independence implies in particular that none of the χB is the zero vector, hence
0 < χBx = f (B) and thus f (B) ≥1 for all B ∈B. Moreover, 
B∈B δG(B) =
E(G). By the assumption that xe < 1
2 for each e ∈E(G) we have |δG(B)| ≥
2 f (B) + 1 ≥3 for all B ∈B.
Let (T, ϕ) be a tree-representation of B. For each vertex t of the arborescence T
we denote by Tt the maximal subgraph of T which is an arborescence rooted at t (Tt
contains t and all its successors). Moreover, let Bt := {v ∈V (G) : ϕ(v) ∈V (Tt)}.
By deﬁnition of the tree-representation we have Br = V (G) for the root r of T
and B = {Bt : t ∈V (T ) \ {r}}.
Claim:
For each t ∈V (T ) we have 
v∈Bt |δG(v)|
≥
2|V (Tt)| + 1, with
equality only if |δG(Bt)| = 2 f (Bt) + 1.
We prove the claim by induction on |V (Tt)|. If δ+
T (t) = ∅(i.e. V (Tt) = {t}),
then Bt is a minimal element of B and thus 
v∈Bt |δG(v)| = |δG(Bt)| ≥3 =
2|V (Tt)| + 1, with equality only if |δG(Bt)| = 3 (implying f (Bt) = 1).
For the induction step let t ∈V (T ) with δ+
T (t) ̸= ∅, say δ+
T (t) = {(t, s1), . . .,
(t, sk)}, where k is the number of direct successors of t. Let E1 := k
i=1 δG(Bsi)\
δG(Bt) and E2 := δG

Bt \ k
i=1 Bsi

(see Figure 20.5 for an illustration).
Note that E1 ∪E2 ̸= ∅, since otherwise χBt = k
i=1 χBsi , contradicting the
assumption that the vectors χB, B ∈B, are linearly independent (note that either
Bt ∈B or t = r and then χBt = 0). Moreover, we have
|δG(Bt)| + 2|E1| =
k

i=1
|δG(Bsi)| + |E2|
(20.13)
and, since Bs1, . . . , Bsk and Bt are tight,
f (Bt) + 2

e∈E1
xe =
k

i=1
f (Bsi) +

e∈E2
xe.
(20.14)

20.5 Jain’s Algorithm
493
∈E2
∈E1
∈E1 ∩E2
Bs1
Bs2
Bt
Fig. 20.5.
Furthermore, by the induction hypothesis,

v∈Bt
|δG(v)|
≥
k

i=1

v∈Bsi
|δG(v)| + |E2|
≥
k

i=1
(2|V (Tsi)| + 1) + |E2|
(20.15)
=
2|V (Tt)| −2 + k + |E2|.
Now we distinguish three cases.
Case 1:
k + |E2| ≥3. Then by (20.15)

v∈Bt
|δG(v)| ≥2|V (Tt)| + 1,
with equality only if k + |E2| = 3 and |δG(Bsi)| = 2 f (Bsi) + 1 for i = 1, . . . , k.
We have to show that then |δG(Bt)| = 2 f (Bt) + 1.
By (20.13) we have
|δG(Bt)| + 2|E1| =
k

i=1
|δG(Bsi)| + |E2|
=
2
k

i=1
f (Bsi) + k + |E2|
=
2
k

i=1
f (Bsi) + 3,
hence |δG(Bt)| is odd. Moreover with (20.14) we conclude that
|δG(Bt)| + 2|E1| = 2
k

i=1
f (Bsi) + 3
=
2 f (Bt) + 4

e∈E1
xe −2

e∈E2
xe + 3
<
2 f (Bt) + 2|E1| + 3,

494
20. Network Design Problems
because E1 ∪E2 ̸= ∅. We have |δG(Bt)| = 2 f (Bt) + 1, as required.
Case 2:
k = 2 and E2 = ∅. Then E1 ̸= ∅, and by (20.14) 2 
e∈E1 xe is an
integer, hence 2 
e∈E1 xe ≤|E1| −1. Note that E1 ̸= δG(Bs1) since otherwise
χBs2 = χBs1 + χBt, contradicting the assumption that the vectors χB, B ∈B, are
linearly independent. Analogously, E1 ̸= δG(Bs2). For i = 1, 2 we get
2 f (Bsi) = 2

e∈δ(Bsi )\E1
xe +2

e∈E1
xe < |δG(Bsi)\ E1|+|E1|−1 = |δG(Bsi)|−1.
By the induction hypothesis this implies 
v∈Bsi |δG(v)| > 2|V (Tsi)| + 1, and as
in (20.15) we get

v∈Bt
|δG(v)| ≥
2

i=1

v∈Bsi
|δG(v)|
≥
2

i=1
(2|V (Tsi)| + 2)
=
2|V (Tt)| + 2.
Case 3:
k = 1 and |E2| ≤1. Note that k = 1 implies E1 ⊆E2, hence |E2| = 1.
By (20.14) we have

e∈E2\E1
xe −

e∈E1
xe =

e∈E2
xe −2

e∈E1
xe =
f (Bt) −f (Bs1).
This is a contradiction since the right-hand side is an integer but the left-hand side
is not; hence Case 3 cannot occur.
The claim is proved. For t = r we get 
v∈V (G) |δG(v)| ≥2|V (T )| + 1, i.e.
2|E(G)| > 2|V (T )|. Since on the other hand |V (T )|−1 = |E(T )| = |B| = |E(G)|
we have a contradiction.
2
With this theorem the following algorithm is natural:
Jain’s Algorithm
Input:
An undirected graph G, weights c : E(G) →R+, capacities u :
E(G) →N and a proper function f : 2V (G) →Z+ (given by an
oracle).
Output:
A function x : E(G) →Z+ with 
e∈δG(S) xe ≥f (S) for all S ⊆
V (G).
1⃝
Set xe := 0 if c(e) > 0 and xe := u(e) if c(e) = 0 for all e ∈E(G).
2⃝
Find an optimum basic solution y to the LP (20.12) with respect to c, u′
and f ′, where u′(e) := u(e) −xe for all e ∈E(G) and
f ′(S) := f (S) −

e∈δG(S)
xe for all S ⊆V (G).
If ye = 0 for all e ∈E(G), then stop.
3⃝
Set xe := xe + ⌈ye⌉for all e ∈E(G) with ye ≥1
2.
Go to 2⃝.

Exercises
495
Theorem 20.34.
(Jain [2001]) Jain’s Algorithm ﬁnds an integral solution to
the LP (20.12) whose cost is at most twice the optimal value of the LP. It can
be implemented to run in polynomial time. Hence it is a 2-factor approximation
algorithm for the Survivable Network Design Problem.
Proof:
After the ﬁrst iteration we have f ′(S) ≤
e∈δG(S)
1
2 ≤
|E(G)|
2
for all
S ⊆V (G). By Theorem 20.33 each subsequent iteration increases at least one xe
by at least 1 (note that f is proper, hence weakly supermodular by Proposition
20.18). Since each xe is increased by at most |E(G)|
2
after the ﬁrst iteration, the
total number of iterations is bounded by |E(G)|2
2
.
The only implementation problem is 2⃝. By Theorem 4.21 it sufﬁces to solve
the Separation Problem. For a given vector y ∈RE(G)
+
we have to decide whether

e∈δG(S) ye ≥f ′(S) = f (S) −
e∈δG(S) xe for all S ⊆V (G), and if not, ﬁnd a
violated cut. Since f is proper this can be done in O(n4 + nθ) time by Theorem
20.20, where n = |V (G)| and θ is the time bound of the oracle for f .
We ﬁnally prove the performance ratio of 2, by induction on the number of
iterations. If the algorithm terminates within the ﬁrst iteration, then the cost of the
solution is zero and thus optimal.
Otherwise let x(1) and y(1) be the vectors x and y after the ﬁrst iteration, and
let x(t) be the vector x at termination. Let ze := y(1)
e
if y(1)
e
<
1
2 and ze = 0
otherwise. We have cx(1) ≤2c

y(1) −z

. Let f (1) be the residual function deﬁned
by f (1)(S) := f (S) −
e∈δG(S) x(1)
e . Since z is a feasible solution for f (1), we
know from the induction hypothesis that c

x(t) −x(1)
≤2cz. We conclude:
cx(t) ≤cx(1) + c

x(t) −x(1)
≤2c

y(1) −z

+ 2cz = 2cy(1).
Since cy(1) is a lower bound of the cost of an optimum solution we are done. 2
Melkonian and Tardos [2004] extended Jain’s technique to a directed network
design problem. Fleischer, Jain and Williamson [2001] and Cheriyan and Vetta
[2005] showed how some vertex-connectivity constraints can be taken into ac-
count. However, results by Kortsarz, Krauthgamer and Lee [2002] indicate that
the vertex-connectivity version of the Survivable Network Design Problem is
hard to approximate.
Exercises
1. Let (G, c, T ) be an instance of the Steiner Tree Problem where G is a
complete graph and c : E(G) →R+ satisﬁes the triangle inequality. Prove
that there exists an optimum Steiner tree for T with at most |T | −2 Steiner
points.
2. Prove that the Steiner Tree Problem is MAXSNP-hard even for complete
graphs with edge weights 1 and 2 only.
Hint: Modify the proof of Theorem 20.3. What if G is disconnected?
(Bern, Plassmann [1989])

496
20. Network Design Problems
3. Formulate an O(n3t2) algorithm for the Steiner Tree Problem in planar
graphs with all terminals lying on the outer face, and prove its correctness.
Hint: Show that in the Dreyfus-Wagner Algorithm it sufﬁces to consider
sets U ⊆T that are consecutive, i.e. there is a path P whose vertices all lie
on the outer face such that V (P) ∩T = U (we assume w.l.o.g. that G is
2-connected).
(Erickson, Monma and Veinott [1987])
4. Describe an algorithm for the Steiner Tree Problem which runs in O(n3)
time for instances (G, c, T ) with |V (G) \ T | ≤k, where k is some constant.
5. Prove the following strengthening of Theorem 20.6: if (G, c, T ) is an instance
of the SteinerTreeProblem, ( ¯G, ¯c) the metric closure, S an optimum Steiner
tree for T in G, and M a minimum weight spanning tree in ¯G[T ] with respect
to ¯c, then
¯c(M) ≤2(1 −1
b)c(S),
where b is the number of leaves (vertices of degree 1) of S. Show that this
bound is sharp.
6. Find a combinatorial 2-factor approximation algorithm for the Survivable
Network Design Problem with ri j = k for all i, j (i.e. the Minimum
Weight k-Edge-Connected Subgraph Problem).
Hint: Replace each edge by a pair of oppositely directed edges (with the same
weight) and apply either Exercise 24 of Chapter 13 or Theorem 6.17 directly.
(Khuller and Vishkin [1994])
Note: For more results for similar problems, see Khuller and Raghavachari
[1996], Gabow [2003], Jothi, Raghavachari and Varadarajan [2003], and
Gabow et al. [2005].
7. Show that in the special case of the Survivable Network Design Prob-
lem the fractional relaxation (20.6) can be formulated as a linear program of
polynomial size.
8. Prove the following strengthening of Theorem 20.23: Given a proper function
g (by an oracle) and a set F ⊆E(G) almost satisfying g, the active sets with
respect to (g, F) can be computed in O

k2n2 + n2θ

time, where n = |V (G)|,
k = maxS⊆V (G) g(S), and θ is the time required by the oracle for g.
Hint: The idea is to stop the ﬂow computations whenever the value of the
maximum ﬂow is at least k, because cuts with k or more edges are not relevant
here.
The Gomory-Hu Algorithm (see Section 8.6) is modiﬁed as follows. At
each step, each vertex of the tree T is a forest (instead of a subset of ver-
tices). The edges of the forests correspond to maximum ﬂow problems whose
value is at least k. At each iteration of the modiﬁed Gomory-Hu Algorithm,
we pick two vertices s and t of different connected components of the forest
corresponding to one vertex of T . If the value of the maximum ﬂow is at least
k, we insert an edge {s, t} into the forest. Otherwise we split the vertex as in
the original Gomory-Hu procedure. We stop when all vertices of T are trees.
We ﬁnally replace each vertex in T by its tree.

Exercises
497
It is clear that the modiﬁed Gomory-Hu tree also satisﬁes the properties (20.8)
and (20.9). If the ﬂow computations are done by the Ford-Fulkerson Al-
gorithm, stopping after the k-th augmenting path, then the O(k2n2) bound
can be achieved.
Note: This leads to an overall running time of O

k3n3 + kn3θ

of the Primal-
Dual Algorithm For Network Design.
(Gabow, Goemans and Williamson [1998])
9.
∗
Consider the Survivable Network Design Problem, which we have seen
to be a special case of (20.1).
(a) Consider a maximum spanning tree T in the complete graph having cost
ri j on edge {i, j}. Show that if a set of edges satisﬁes the connectivity
requirements of the edges of T , then it satisﬁes all connectivity require-
ments.
(b) When determining the active sets at the beginning of phase p, we only
need to look for one augmenting i- j-path for each {i, j} ∈E(T ) (we
can use the i- j-ﬂow of the preceding phase). If there is no augmenting
i- j-path, then there are at most two candidates for active sets. Among
those O(n) candidates we can ﬁnd the active sets in O(n2) time.
(c) Show that updating these data structures can be done in O(kn2) total time
per phase.
(d) Conclude that the active sets can be computed in an overall running time
of O(k2n2).
(Gabow, Goemans and Williamson [1998])
10. Show that the clean-up step 5⃝of the Primal-Dual Algorithm For Net-
work Design is crucial: without 5⃝, the algorithm does not even achieve any
ﬁnite performance ratio for k = 1.
11. No algorithm for the Minimum Weight T -Join Problem with a better worst-
case complexity than O(n3) for dense graphs (cf. Corollary 12.11) is known.
Let G be an undirected graph, c : E(G) →R+ and T ⊆V (G) with |T | even.
Consider the integer linear program (20.1), where we set f (S) := 1 if |S ∩T |
is odd and f (S) := 0 otherwise.
(a) Prove that our primal-dual algorithm applied to (20.1) returns a forest in
which each connected component contains an even number of elements
of T .
(b) Prove that any optimum solution to (20.1) is a minimum weight T -join
plus possibly some zero weight edges.
(c) The primal-dual algorithm can be implemented in O(n2 log n) time if
f (S) ∈{0, 1} for all S. Show that this implies a 2-factor approximation
algorithm for the Minimum Weight T -Join Problem with nonnegative
weights, with the same running time.
Hint: By (a), the algorithm returns a forest F. For each connected compo-
nent C of F consider ¯G[V (C)∩T ] and ﬁnd a tour whose weight is at most
twice the weight of C (cf. the proof of Theorem 20.6). Now take every

498
20. Network Design Problems
second edge of the tour. (A similar idea is the basis of Christofides’
Algorithm, see Section 21.1.)
(Goemans and Williamson [1995])
12. Find an optimum basic solution x for (20.12), where G is the Petersen graph
(Figure 20.4) and f (S) = 1 for all 0 ̸= S ⊂V (G). Find a maximal laminar
family B of tight sets with respect to x such that the vectors χB, B ∈B, are
linearly independent (cf. Lemma 20.32).
13. Prove that the optimum value of (20.12) can be arbitrarily close to half the
value of an optimum integral solution.
Note: By Jain’s Algorithm (cf. the proof of Theorem 20.34) it cannot be
less than half.
References
General Literature:
Cheng, X., and Du, D.-Z. [2001]: Steiner Trees in Industry. Kluwer, Dordrecht 2001
Du, D.-Z., Smith, J.M., and Rubinstein, J.H. [2000]: Advances in Steiner Trees. Kluwer,
Boston 2000
Hwang, F.K., Richards, D.S., and Winter, P. [1992]: The Steiner Tree Problem; Annals of
Discrete Mathematics 53. North-Holland, Amsterdam 1992
Goemans, M.X., and Williamson, D.P. [1996]: The primal-dual method for approximation
algorithms and its application to network design problems. In: Approximation Algorithms
for NP-Hard Problems. (D.S. Hochbaum, ed.), PWS, Boston, 1996
Gr¨otschel, M., Monma, C.L., and Stoer, M. [1995]: Design of survivable networks. In:
Handbooks in Operations Research and Management Science; Volume 7; Network Mod-
els (M.O. Ball, T.L. Magnanti, C.L. Monma, G.L. Nemhauser, eds.), Elsevier, Amsterdam
1995
Pr¨omel, H.J., and Steger, A. [2002]: The Steiner Tree Problem. Vieweg, Braunschweig
2002
Stoer, M. [1992]: Design of Survivable Networks. Springer, Berlin 1992
Vazirani, V.V. [2001]: Approximation Algorithms. Springer, Berlin 2001, Chapters 22 and
23
Cited References:
Agrawal, A., Klein, P., and Ravi, R. [1995]: When trees collide: an approximation algorithm
for the generalized Steiner tree problem in networks. SIAM Journal on Computing 24
(1995), 440–456
Arora, S. [1998]: Polynomial time approximation schemes for Euclidean traveling salesman
and other geometric problems. Journal of the ACM 45 (1998), 753–782
Berman, P., and Ramaiyer, V. [1994]: Improved approximations for the Steiner tree prob-
lem. Journal of Algorithms 17 (1994), 381–408
Bern, M., and Plassmann, P. [1989]: The Steiner problem with edge lengths 1 and 2.
Information Processing Letters 32 (1989), 171–176
Bertsimas, D., and Teo, C. [1995]: From valid inequalities to heuristics: a uniﬁed view
of primal-dual approximation algorithms in covering problems. Operations Research 46
(1998), 503–514
Bertsimas, D., and Teo, C. [1997]: The parsimonious property of cut covering problems
and its applications. Operations Research Letters 21 (1997), 123–132

References
499
Borchers, A., and Du, D.-Z. [1997]: The k-Steiner ratio in graphs. SIAM Journal on Com-
puting 26 (1997), 857–869
Cheriyan, J., and Vetta, A. [2005]: Approximation algorithms for network design with metric
costs. Proceedings of the 37th Annual ACM Symposium on Theory of Computing (2005),
167–175
Choukhmane, E. [1978]: Une heuristique pour le probl`eme de l’arbre de Steiner. RAIRO
Recherche Op´erationnelle 12 (1978), 207–212
Clementi, A.E.F., and Trevisan, L. [1999]: Improved non-approximability results for min-
imum vertex cover with density constraints. Theoretical Computer Science 225 (1999),
113–128
Dreyfus, S.E., and Wagner, R.A. [1972]: The Steiner problem in graphs. Networks 1 (1972),
195–207
Du, D.-Z., and Hwang, F.K. [1992]: A proof of the Gilbert-Pollak conjecture on the Steiner
ratio. Algorithmica 7 (1992), 121–135
Du, D.-Z., Zhang, Y., and Feng, Q. [1991]: On better heuristic for Euclidean Steiner mini-
mum trees. Proceedings of the 32nd Annual Symposium on the Foundations of Computer
Science (1991), 431–439
Erickson, R.E., Monma, C.L., and Veinott, A.F., Jr. [1987]: Send-and-split method for
minimum concave-cost network ﬂows. Mathematics of Operations Research 12 (1987),
634–664
Fleischer, L., Jain, K., and Williamson, D.P. [2001]: An iterative rounding 2-approximation
algorithm for the element connectivity problem. Proceedings of the 42nd Annual Sym-
posium on the Foundations of Computer Science (2001), 339–347
Gabow, H.N. [2003]: Better performance bounds for ﬁnding the smallest k-edge connected
spanning subgraph of a multigraph. Proceedings of the 14th Annual ACM-SIAM Sym-
posium on Discrete Algorithms (2003), 460–469
Gabow, H.N., Goemans, M.X., and Williamson, D.P. [1998]: An efﬁcient approximation
algorithm for the survivable network design problem. Mathematical Programming B 82
(1998), 13–40
Gabow, H.N., Goemans, M.X., Tardos, ´E., and Williamson, D.P. [2005]: Approximating
the smallest k-edge connected spanning subgraph by LP-rounding. Proceedings of the
16th Annual ACM-SIAM Symposium on Discrete Algorithms (2005), 562–571
Garey, M.R., Graham, R.L., and Johnson, D.S. [1977]: The complexity of computing Steiner
minimal trees. SIAM Journal of Applied Mathematics 32 (1977), 835–859
Garey, M.R., and Johnson, D.S. [1977]: The rectilinear Steiner tree problem is NP-complete.
SIAM Journal on Applied Mathematics 32 (1977), 826–834
Gilbert, E.N., and Pollak, H.O. [1968]: Steiner minimal trees. SIAM Journal on Applied
Mathematics 16 (1968), 1–29
Goemans, M.X., and Bertsimas, D.J. [1993]: Survivable networks, linear programming and
the parsimonious property, Mathematical Programming 60 (1993), 145–166
Goemans, M.X., Goldberg, A.V., Plotkin, S., Shmoys, D.B., Tardos, ´E., and Williamson,
D.P. [1994]: Improved approximation algorithms for network design problems. Proceed-
ings of the 5th Annual ACM-SIAM Symposium on Discrete Algorithms (1994), 223–232
Goemans, M.X., and Williamson, D.P. [1995]: A general approximation technique for con-
strained forest problems. SIAM Journal on Computing 24 (1995), 296–317
Gr¨opl, C., Hougardy, S., Nierhoff, T., and Pr¨omel, H.J. [2001]: Approximation algorithms
for the Steiner tree problem in graphs. In: Cheng and Du [2001], pp. 235–279
Hanan, M. [1966]: On Steiner’s problem with rectilinear distance. SIAM Journal on Applied
Mathematics 14 (1966), 255–265
Hetzel, A. [1995]: Verdrahtung im VLSI-Design: Spezielle Teilprobleme und ein sequen-
tielles L¨osungsverfahren. Ph.D. thesis, University of Bonn, 1995

500
20. Network Design Problems
Hougardy, S., and Pr¨omel, H.J. [1999]: A 1.598 approximation algorithm for the Steiner
tree problem in graphs. Proceedings of the 10th Annual ACM-SIAM Symposium on
Discrete Algorithms (1999), 448–453
Hwang, F.K. [1976]: On Steiner minimal trees with rectilinear distance. SIAM Journal on
Applied Mathematics 30 (1976), 104–114
Jain, K. [2001]: A factor 2 approximation algorithm for the generalized Steiner network
problem. Combinatorica 21 (2001), 39–60
Jothi, R., Raghavachari, B., and Varadarajan, S. [2003]: A 5/4-approximation algorithm for
minimum 2-edge-connectivity. Proceedings of the 14th Annual ACM-SIAM Symposium
on Discrete Algorithms (2003), 725–734
Karp, R.M. [1972]: Reducibility among combinatorial problems. In: Complexity of Com-
puter Computations (R.E. Miller, J.W. Thatcher, eds.), Plenum Press, New York 1972,
pp. 85–103
Karpinski, M., and Zelikovsky, A. [1997]: New approximation algorithms for Steiner tree
problems. Journal of Combinatorial Optimization 1 (1997), 47–65
Khuller, S., and Raghavachari, B. [1996]: Improved approximation algorithms for uniform
connectivity problems. Journal of Algorithms 21 (1996), 434–450
Khuller, S., and Vishkin, U. [1994]: Biconnectivity augmentations and graph carvings.
Journal of the ACM 41 (1994), 214–235
Klein, P., and Ravi, R. [1993]: When cycles collapse: a general approximation technique
for constrained two-connectivity problems. Proceedings of the 3rd MPS Conference on
Integer Programming and Combinatorial Optimization (1993), 39–55
Korte, B., Pr¨omel, H.J., and Steger, A. [1990]: Steiner trees in VLSI-layout. In: Paths,
Flows, and VLSI-Layout (B. Korte, L. Lov´asz, H.J. Pr¨omel, A. Schrijver, eds.), Springer,
Berlin 1990, pp. 185–214
Kortsarz, G., Krauthgamer, R., and Lee, J.R. [2002]: Hardness of approximation for vertex-
connectivity network design problems. SIAM Journal on Computing 33 (2004), 704–720
Kou, L. [1990]: A faster approximation algorithm for the Steiner problem in graphs. Acta
Informatica 27 (1990), 369–380
Kou, L., Markowsky, G., and Berman, L. [1981]: A fast algorithm for Steiner trees. Acta
Informatica 15 (1981), 141–145
Martin, A. [1992]: Packen von Steinerb¨aumen: Polyedrische Studien und Anwendung. Ph.D.
thesis, Technical University of Berlin 1992 [in German]
Mehlhorn, K. [1988]: A faster approximation algorithm for the Steiner problem in graphs.
Information Processing Letters 27 (1988), 125–128
Melkonian, V., and Tardos, ´E. [2004]: Algorithms for a network design problem with
crossing supermodular demands. Networks 43 (2004), 256–265
Robins, G., and Zelikovsky, A. [2000]: Improved Steiner tree approximation in graphs.
Proceedings of the 11th Annual ACM-SIAM Symposium on Discrete Algorithms (2000),
770–779
Takahashi, M., and Matsuyama, A. [1980]: An approximate solution for the Steiner problem
in graphs. Mathematica Japonica 24 (1980), 573–577
Thimm, M. [2003]: On the approximability of the Steiner tree problem. Theoretical Com-
puter Science 295 (2003), 387–402
Warme, D.M., Winter, P., and Zachariasen, M. [2000]: Exact algorithms for plane Steiner
tree problems: a computational study. In: Advances in Steiner trees (D.-Z. Du, J.M.
Smith, J.H. Rubinstein, eds.), Kluwer Academic Publishers, Boston, 2000, pp. 81–116
Williamson, D.P., Goemans, M.X., Mihail, M., and Vazirani, V.V. [1995]: A primal-dual
approximation algorithm for generalized Steiner network problems. Combinatorica 15
(1995), 435–454
Zelikovsky, A.Z. [1993]: An 11/6-approximation algorithm for the network Steiner problem.
Algorithmica 9 (1993), 463–470

21. The Traveling Salesman Problem
In Chapter 15 we introduced the Traveling Salesman Problem (TSP) and
showed that it is NP-hard (Theorem 15.41). The TSP is perhaps the best stud-
ied NP-hard combinatorial optimization problem, and there are many techniques
which have been applied. We start by discussing approximation algorithms in Sec-
tions 21.1 and 21.2. In practice, so-called local search algorithms (discussed in
Section 21.3) ﬁnd better solutions for large instances although they do not have a
ﬁnite performance ratio.
We study the traveling salesman polytope (the convex hull of all tours in Kn)
in Section 21.4. Using a cutting plane approach (cf. Section 5.5) combined with
a branch-and-bound scheme one can solve TSP instances with several thousand
cities optimally. We shall discuss this in Section 21.6 after we have shown how to
obtain good lower bounds in Section 21.5. Note that all these ideas and techniques
can also be applied to other combinatorial optimization problems. We present them
with the TSP since this is a problem where these techniques have proved to be
most effective.
We consider the symmetric TSP only, although the asymmetric traveling sales-
man problem (where the distance from i to j can be different to the distance from
j to i) is also interesting.
21.1 Approximation Algorithms for the TSP
In this and the next section we investigate the approximability of the TSP. We
start with the following negative result:
Theorem 21.1.
(Sahni and Gonzalez [1976]) Unless P = NP there is no k-factor
approximation algorithm for the TSP for any k ≥1.
Proof:
Suppose there is a k-factor approximation algorithm A for the TSP. Then
we prove that there is a polynomial-time algorithm for the Hamiltonian Circuit
problem. Since the latter is NP-complete by Theorem 15.25, this implies P = NP.
Given a graph G, we construct an instance of the TSP with n = |V (G)| cities:
the distances are deﬁned as c : E(Kn) →Z+,
c({i, j}) :=
 1
if {i, j} ∈E(G)
2 + (k −1)n
if {i, j} /∈E(G).

502
21. The Traveling Salesman Problem
Now we apply A to this instance. If the returned tour has length n, then this
tour is a Hamiltonian circuit in G. Otherwise the returned tour has length at least
n + 1 + (k −1)n = kn + 1, where n := |V (G)|. If OPT(Kn, c) is the length of the
optimum tour, then
kn+1
OPT(Kn,c) ≤k since A is a k-factor approximation algorithm.
Hence OPT(Kn, c) > n, showing that G has no Hamiltonian circuit.
2
In most practical applications the distances of the TSP instances satisfy the
triangle inequality:
Metric TSP
Instance:
A complete graph Kn with weights c : E(Kn) →R+ such that
c({x, y}) + c({y, z}) ≥c({x, z}) for all x, y, z ∈V (Kn).
Task:
Find a Hamiltonian circuit in Kn of minimum weight.
In other words, (Kn, c) is its own metric closure.
Theorem 21.2.
The Metric TSP is NP-hard.
Proof:
Transformation from Hamiltonian Circuit as in the proof of Theorem
15.41.
2
One can immediately think of several heuristics to generate reasonably good
solutions. One of the simplest is the so-called nearest neighbour heuristic: Given an
instance (Kn, c) of the TSP, choose v1 ∈V (Kn) arbitrarily. Then for i = 2, . . . , n
choose vi among V (Kn) \ {v1, . . . , vi−1} such that c({vi−1, vi}) is minimum. In
other words, at each step the nearest unvisited city is chosen.
The nearest neighbour heuristic is not an approximation algorithm for the
Metric TSP. For inﬁnitely many n there are instances (Kn, c) for which the near-
est neighbour heuristic returns a tour of length 1
3 OPT(Kn, c) log n (Rosenkrantz,
Stearns and Lewis [1977]). See also Hurkens and Woeginger [2004].
The rest of this section is devoted to approximation algorithms for the Metric
TSP. These algorithms ﬁrst construct a closed walk containing all vertices (but
some vertices may be repeated). As the following lemma shows, this is sufﬁcient
if the triangle inequality holds.
Lemma 21.3.
Given an instance (Kn, c) of the Metric TSP and a connected
Eulerian graph G spanning V (Kn), possibly with parallel edges. Then we can
construct a tour of weight at most c(E(G)) in linear time.
Proof:
By Theorem 2.25 we can ﬁnd an Eulerian walk in G in linear time.
The order in which the vertices appear in this walk (we ignore all but the ﬁrst
occurrence of a vertex) deﬁnes a tour. The triangle inequality immediately implies
that this tour is no longer than c(E(G)).
2
We have already encountered this idea when approximating the Steiner Tree
Problem (Theorem 20.6).

21.1 Approximation Algorithms for the TSP
503
Double-Tree Algorithm
Input:
An instance (Kn, c) of the Metric TSP.
Output:
A tour.
1⃝
Find a minimum weight spanning tree T in Kn with respect to c.
2⃝
Let G be the graph containing two copies of each edge of T . G satisﬁes
the prerequisites of Lemma 21.3.
Construct a tour as in the proof of Lemma 21.3.
Theorem 21.4.
The Double-Tree Algorithm is a 2-factor approximation al-
gorithm for the Metric TSP. Its running time is O(n2).
Proof:
The running time follows from Theorem 6.5. We have c(E(T )) ≤
OPT(Kn, c) since by deleting one edge of any tour we get a spanning tree.
Therefore c(E(G)) = 2c(E(T )) ≤2 OPT(Kn, c). The theorem now follows from
Lemma 21.3.
2
For Euclidean instances one can ﬁnd an optimum tour in the graph G in 2⃝in
O(n3) time instead of applying Lemma 21.3 (Burkard, De˘ıneko and Woeginger
[1998]). The performance guarantee of the Double-Tree Algorithm is tight
(Exercise 5). The best known approximation algorithm for the Metric TSP is due
to Christoﬁdes [1976]:
Christofides’ Algorithm
Input:
An instance (Kn, c) of the Metric TSP.
Output:
A tour.
1⃝
Find a minimum weight spanning tree T in Kn with respect to c.
2⃝
Let W be the set of vertices having odd degree in T .
Find a minimum weight W-join J in Kn with respect to c.
3⃝
Let G := (V (Kn), E(T )∪M). G satisﬁes the prerequisites of Lemma 21.3.
Construct a tour as in the proof of Lemma 21.3.
Because of the triangle inequality a minimum weight perfect matching in
Kn[W] can be taken as J in 2⃝.
Theorem 21.5.
(Christoﬁdes [1976]) Christofides’ Algorithm is a 3
2-factor
approximation algorithm for the Metric TSP. Its running time is O(n3).
Proof:
The time bound is a consequence of Theorem 12.9. As in the proof of
Theorem 21.4 we have c(E(T )) ≤OPT(Kn, c). Since each tour is the union
of two W-joins we also have c(J) ≤
1
2 OPT(Kn, c). We conclude c(E(G)) =
c(E(T )) + c(J) ≤3
2 OPT(Kn, c), and the result follows from Lemma 21.3.
2

504
21. The Traveling Salesman Problem
It is not known whether there is an approximation algorithm with a better
performance ratio. On the other hand, there is the following negative result:
Theorem 21.6.
(Papadimitriou and Yannakakis [1993])
The Metric TSP is
MAXSNP-hard.
Proof:
We describe an L-reduction from the Minimum Vertex Cover Problem
for graphs with maximum degree 4 (which is MAXSNP-hard by Theorem 16.39)
to the Metric TSP.
Given an undirected graph G with maximum degree 4, we construct an instance
(H, c) of the Metric TSP as follows:
(e, v, 1)
(e, w, 1)
(e, v, 2)
(e, w, 2)
Fig. 21.1.
For each e = {v, w} ∈E(G) we introduce a subgraph He of twelve vertices
and 14 edges as shown in Figure 21.1. Four vertices of He denoted by (e, v, 1),
(e, v, 2), (e, w, 1) and (e, w, 2) have a special meaning. The graph He has the
property that it has a Hamiltonian path from (e, v, 1) to (e, v, 2) and another one
from (e, w, 1) to (e, w, 2), but it has no Hamiltonian path from (e, v, i) to (e, w, j)
for any i, j ∈{1, 2}.
Now H is the complete graph on the vertex set V (H) := 
e∈E(G) V (He). For
{x, y} ∈E(H) we set
c({x, y}) :=
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
1
if {x, y} ∈E(He) for some e ∈E(G);
distHe(x, y)
if x, y ∈V (He) for some e ∈E(G)
but {x, y} /∈E(He);
4
if x = (e, v, i) and y = ( f, v, j) with e ̸= f ;
5
otherwise.
This construction is illustrated by Figure 21.2 (only edges of length 1 or 4 are
shown).
(H, c) is an instance of the Metric TSP. We claim that it has the following
properties:
(a) For each vertex cover X of G there is a tour of length 15|E(G)| + |X|.
(b) Given any tour T , one can construct another tour T ′ in polynomial time
which is at most as long and contains a Hamiltonian path of each subgraph
He (e ∈E(G)).

21.1 Approximation Algorithms for the TSP
505
Fig. 21.2.
(c) Given a tour of length 15|E(G)| + k, we can construct a vertex cover of
cardinality k in G in polynomial time.
(a) and (c) imply that we have an L-reduction, because the optimum tour length
is 15|E(G)| + τ(G) ≤15(4τ(G)) + τ(G) as G has maximum degree 4.
To prove (a), let X be a vertex cover of G, and let (Ex)x∈X be a partition of
E(G) with Ex ⊆δ(x) (x ∈X). Then for each x ∈X the subgraph induced by

e∈Ex V (He) obviously contains a Hamiltonian path with 11|Ex| edges of length 1
and |Ex|−1 edges of length 4. Adding |X| edges to the union of these Hamiltonian
paths yields a tour with only |X| length 5 edges, |E(G)| −|X| length 4 edges and
11|E(G)| length 1 edges.
To prove (b), let T be any tour and e ∈E(G) such that T does not contain a
Hamiltonian path in He. Let {x, y} ∈E(T ), x /∈V (He), y ∈V (He), and let z be
the ﬁrst vertex outside V (He) when traversing T from y without passing x. Then
we delete the piece of the tour between x and z and replace it by {x, (e, v, 1)},
a Hamiltonian path in He from (e, v, 1) to (e, v, 2), and the edge {(e, v, 2), z}
(where v ∈e is chosen arbitrarily). At all other places where T contains vertices
of He we shortcut T . We claim that the resulting tour T ′ is not longer than T .
First suppose that k := |δT (V (He))| ≥4. Then the total weight of the edges
incident to V (He) in T is at least 4k + (12 −k
2). In T ′ the total weight of the
edges incident to V (He) is at most 5 + 5 + 11, and we have added another k
2 −1
edges by shortcutting. Since 5 + 5 + 11 + 5( k
2 −1) ≤4k + (12 −k
2), the tour has
not become longer.
Now suppose that |δT (V (He))| = 2 but T contains an edge {x, y} with x, y ∈
V (He) but {x, y} /∈E(He). Then the total length of the edges of T incident to
V (He) is at least 21, as can be checked by inspection. Since in T ′ the total length
of the edges incident to V (He) is at most 5+5+11 = 21, the tour has not become
longer.
We ﬁnally prove (c). Let T be a tour of length 15|E(G)| + k, for some k. By
(b) we may assume that T contains a Hamiltonian path of each He (e ∈E(G)), say
from (e, v, 1) to (e, v, 2); here we set v(e) := v. Then X := {v(e) : e ∈E(G)}
is a vertex cover of G. Since T contains exactly 11|E(G)| edges of length 1,
|E(G)| edges of length 4 or 5, and at least |X| edges of length 5, we conclude
that |X| ≤k.
2

506
21. The Traveling Salesman Problem
So by Corollary 16.33 an approximation scheme cannot exist unless P = NP.
Papadimitriou and Vempala [2000] showed that even the existence of a 129
128-factor
approximation algorithm would imply P = NP.
Papadimitriou and Yannakakis [1993] proved that the problem remains
MAXSNP-hard even if all weights are 1 or 2. For this special case they give
a 7
6-factor approximation algorithm.
21.2 Euclidean TSPs
In this section we consider the TSP for Euclidean instances.
Euclidean TSP
Instance:
A ﬁnite set V ⊆R2, |V | ≥3.
Task:
Find a Hamiltonian circuit T in the complete graph on V such that
the total length 
{v,w}∈E(T ) ||v −w||2 is minimum.
Here ||v−w||2 denotes the Euclidean distance between v and w. We often iden-
tify an edge with the straight line segment joining its endpoints. Every optimum
tour can thus be regarded as a polygon (it cannot cross itself).
The Euclidean TSP is evidently a special case of the Metric TSP, and it is
also strongly NP-hard (Garey, Graham and Johnson [1976], Papadimitriou [1977]).
However, one can make use of the geometric nature as follows:
Suppose we have a set of n points in the unit square, partition it by a regular
grid such that each region contains few points, ﬁnd an optimum tour within each
region and then patch the subtours together. This method has been proposed by
Karp [1977] who showed that it leads to (1 + ϵ)-approximate solutions on almost
all randomly generated instances in the unit square. Arora [1998] developed this
further and found an approximation scheme for the Euclidean TSP, which we
present in this section. A similar approximation scheme has been proposed by
Mitchell [1999].
Let 0 < ϵ < 1 be ﬁxed throughout this section. We show how to ﬁnd in
polynomial time a tour whose length exceeds the length of an optimal tour by a
factor of at most 1 + ϵ. We begin by rounding the coordinates:
Deﬁnition 21.7.
An instance V ⊆R2 of the Euclidean TSP is called well-
rounded if the following conditions hold:
(a) for all (vx, vy) ∈V , vx and vy are odd integers;
(b) maxv,w∈V ||v −w||2 ≤64n
ϵ + 16, where n := |V |;
(c) minv,w∈V ||v −w||2 ≥8.
The following result says that it is sufﬁcient to deal with well-rounded in-
stances:

21.2 Euclidean TSPs
507
Proposition 21.8.
Suppose there is an approximation scheme for the Euclidean
TSP restricted to well-rounded instances. Then there is an approximation scheme
for the general Euclidean TSP.
Proof:
Let V ⊆R2 be a ﬁnite set and n := |V | ≥3. Deﬁne L := maxv,w∈V ||v−
w||2, and let
V ′ :=

1 + 8
2 8n
ϵL vx
3
, 1 + 8
2 8n
ϵL vy
3
: (vx, vy) ∈V

.
V ′ may contain fewer elements than V . Since the maximum distance within V ′ is
at most 64n
ϵ + 16, V ′ is well-rounded. We run the approximation scheme (which
we assume to exist) for V ′ and ϵ
2 and obtain a tour whose length l′ is at most
(1 + ϵ
2) OPT(V ′).
From this tour we construct a tour for the original instance V in a straightfor-
ward way. The length l of this tour is no more than

l′
8 + 2n

ϵL
8n . Furthermore,
OPT(V ′) ≤8
 8n
ϵL OPT(V ) + 2n

.
Altogether we have
l ≤ϵL
8n

2n +

1 + ϵ
2
  8n
ϵL OPT(V ) + 2n

=

1 + ϵ
2

OPT(V ) + ϵL
2 + ϵ2L
8 .
Since OPT(V ) ≥2L, we conclude that l ≤(1 + ϵ) OPT(V ).
2
So from now on we shall deal with well-rounded instances only. W.l.o.g. let
all coordinates be within the square [0, 2N]×[0, 2N], where N := ⌈log L⌉+1 and
L = maxv,w∈V ||v −w||2. Now we partition the square successively by a regular
grid: for i = 1, . . . , N −1 let Gi := Xi ∪Yi, where
Xi
:=
5D
0, k2N−i
,

2N, k2N−iE
: k = 0, . . . , 2i −1
6
,
Yi
:=
5D
j2N−i, 0

,

j2N−i, 2NE
: j = 0, . . . , 2i −1
6
.
(The notation [(x, y), (x′, y′)] denotes the line segment between (x, y) and
(x′, y′).)
More precisely, we consider shifted grids: Let a, b ∈{0, 2, . . . , 2N −2} be
even integers. For i = 1, . . . , N −1 let G(a,b)
i
:= X(b)
i
∪Y (a)
i
, where
X(b)
i
:=
D
0, (b + k2N−i) mod 2N
,

2N, (b + k2N−i) mod 2NE
:
k = 0, . . . , 2i −1

,
Y (a)
i
:=
D
(a + j2N−i) mod 2N, 0

,

(a + j2N−i) mod 2N, 2NE
:
j = 0, . . . , 2i −1

.

508
21. The Traveling Salesman Problem
a
b
Fig. 21.3.
(x mod y denotes the unique number z with 0 ≤z < y and x−z
y
∈Z.) Note that
G N−1 = G(a,b)
N−1 does not depend on a or b.
A line l is said to be at level 1 if l ∈G(a,b)
1
, and at level i if l ∈G(a,b)
i
\ G(a,b)
i−1
(i = 2, . . . , N −1). See Figure 21.3, where thicker lines are at smaller levels. The
regions of the grid G(a,b)
i
are the sets

(x, y) ∈[0, 2N) × [0, 2N)
:
(x −a −j2N−i) mod 2N < 2N−i,
(y −b −k2N−i) mod 2N < 2N−i
for j, k ∈{0, . . . , 2i −1}. For i < N −1, some of the regions may be disconnected
and consist of two or four rectangles. Since all lines are deﬁned by even coor-
dinates, no line contains a point of our well-rounded Euclidean TSP instance.
Furthermore, each region of G N−1 contains at most one point, for any a, b.
For a polygon T and a line l of G N−1 we shall again denote by cr(T,l) the
number of times T crosses l. The following proposition will prove useful:
Proposition 21.9.
For an optimum tour T of a well-rounded instance V of the
Euclidean TSP, 
l∈G N−1 cr(T,l) ≤OPT(V ).
Proof:
Consider an edge of T of length s, with horizontal part x and vertical part
y. The edge crosses lines of G N−1 at most x
2 +1+ y
2 +1 times. Since x + y ≤
√
2s
and s ≥8 (the instance is well-rounded), the edge crosses lines of G N−1 at most
√
2
2 s + 2 ≤s times. Summing over all edges of T yields the stated inequality. 2
Set C := 7 +
B 36
ϵ
C
and P := N
B 6
ϵ
C
. For each line we now deﬁne portals: if
l =
D
0, (b + k2N−i) mod 2N
,

2N, (b + k2N−i) mod 2NE
is a horizontal line at
level i, we deﬁne the set of its portals to be

21.2 Euclidean TSPs
509

a + h
P 2N−i, (b + k2N−i) mod 2N

: h = 0, . . . , P2i

.
Similarly for vertical lines.
Deﬁnition 21.10.
Let V ⊆[0, 2N] × [0, 2N] be a well-rounded instance of the
Euclidean TSP. Let a, b ∈{0, 2, . . . , 2N −2} be given, and let the shifted grids,
C, P, and the portals be deﬁned as above. A Steiner tour is a closed walk of
straight line segments containing V such that its intersection with each line of the
grids is a subset of portals. A Steiner tour is light if for each i and for each region
of G(a,b)
i
, the tour crosses each edge of the region at most C times.
Note that Steiner tours are not necessarily polygons; they may cross them-
selves. To make a Steiner tour light we shall make frequent use of the following
Patching Lemma:
Lemma 21.11.
Let V ⊂R2 be an Euclidean TSP instance and T a tour for V .
Let l be a segment of length s of a line not containing any point in V . Then there
exists a tour for V whose length exceeds the length of T by at most 6s and which
crosses l at most twice.
Proof:
For clearer exposition, assume l to be a vertical line segment. Suppose
T crosses l exactly k times, say with edges e1, . . . , ek. Let k ≥3; otherwise the
assertion is trivial. We subdivide each of e1, . . . , ek by two new vertices without
increasing the tour length. In other words, we replace ei by a path of length 3
with two new inner vertices pi, qi ∈R2 very close to l, where pi is to the left of
l and qi is to the right of l (i = 1, . . . , k). Let the resulting tour be T ′.
Let t := ⌊k−1
2 ⌋(then k −2 ≤2t ≤k −1), and let T ′′ result from T ′ by deleting
the edges {p1, q1}, . . . , {p2t, q2t}.
Let P consist of a shortest tour through p1, . . . , pk plus a minimum cost
perfect matching of p1, . . . , p2t. Analogously, let Q consist of a shortest tour
through q1, . . . , qk plus a minimum cost perfect matching of q1, . . . , q2t. The total
length of the edges is at most 3s in each of P and Q.
Then T ′′ + P + Q crosses l at most k −2t ≤2 times, and is connected
and Eulerian. We now proceed as in Lemma 21.3. By Euler’s Theorem 2.24 there
exists an Eulerian walk in T ′′+ P + Q. By shortcutting paths this can be converted
to a tour for V , without increasing the length or the number of crossings with l.
2
The following theorem is the main basis of the algorithm:
Theorem 21.12.
(Arora [1998]) Let V ⊆[0, 2N]×[0, 2N] be a well-rounded in-
stance of the EuclideanTSP. If a and b are randomly chosen out of {0, 2, . . . , 2N−
2}, then with probability at least 1
2 a light Steiner tour exists whose length is at
most (1 + ϵ) OPT(V ).
Proof:
Let T be an optimum tour for V . We introduce Steiner points whenever
the tour crosses a line.

510
21. The Traveling Salesman Problem
Now all the Steiner points are moved to portals. The nearest portal from a
Steiner point on a line on level i can be as far away as 2N−i−1
P
. Since a line l is
at level i with probability p(l, i) :=

2i−N
if i > 1
22−N
if i = 1 , the expected total tour
length increase by moving all Steiner points at l to portals is at most
N−1

i=1
p(l, i)cr(T,l)22N−i−1
P
= N cr(T,l)
P
.
Now we modify the Steiner tour so that it becomes light. Consider the follow-
ing procedure:
For i := N −1 down to 1 do:
Apply the Patching Lemma 21.11 to each segment of a horizontal
line of G(a,b)
i
, i.e. for j, k ∈{0, . . . , 2i −1} each line between

(a + j2N−i) mod 2N, (b + k2N−i) mod 2N
and

(a + ( j + 1)2N−i) mod 2N, (b + k2N−i) mod 2N
,
which is crossed more than C −4 times.
Apply the Patching Lemma 21.11 to each segment of a vertical
line of G(a,b)
i
, i.e. for j, k ∈{0, . . . , 2i −1} each line between

(a + j2N−i) mod 2N, (b + k2N−i) mod 2N
and

(a + j2N−i) mod 2N, (b + (k + 1)2N−i) mod 2N
,
which is crossed more than C −4 times.
Two remarks must be made. A segment of a horizontal or vertical line can
consist of two separate parts. In this case the Patching Lemma is applied to each
part, so the total number of crossings afterwards may be 4.
Furthermore, observe that the application of the Patching Lemma to a vertical
line segment l in iteration i may introduce new crossings (Steiner points) on a
horizontal line segment which has one endpoint on l. These new crossings are at
portals and will not be considered anymore in subsequent iterations of the above
procedure, because they are on lines of higher level.
For each line l, the number of applications of the Patching Lemma to l is at
most cr(T,l)
C−7 , since each time the number of crossings decreases by at least C −7
(at least C −3 crossings are replaced by at most 4). For a line l, let c(l, i, a, b)
be the total number of times the Patching Lemma is applied to l at iteration i of
the above procedure. Note that c(l, i, a, b) is independent of the level of l as long
as it is at most i.
Then the total increase in tour length due to applications of the Patching Lemma
to line l is 
i≥level(l) c(l, i, a, b) · 6 · 2N−i. Furthermore, 
i≥level(l) c(l, i, a, b) ≤
cr(T,l)
C−7 .
Since l is at level j with probability p(l, j), the expected total increase in tour
length by the above procedure is at most
N−1

j=1
p(l, j)

i≥j
c(l, i, a, b) · 6 · 2N−i
=
6
N−1

i=1
c(l, i, a, b)2N−i
i
j=1
p(l, j)

21.2 Euclidean TSPs
511
≤
12cr(T,l)
C −7
.
After the above procedure, each line segment (and therefore each edge of a
region) is crossed by the tour at most C −4 times, not counting the new crossings
introduced by the procedure (see the remark above). These additional crossings
are all at one of the endpoints of the line segments. But if a tour crosses three or
more times through the same point, two of the crossings can be removed without
increasing the tour length or introducing any additional crossings. (Removing two
out of three parallel edges of a connected Eulerian graph results in a connected
Eulerian graph.) So we have at most four additional crossings for each edge of
each region (at most two for each endpoint), and the tour is indeed light.
So – using Proposition 21.9 – the expectation of the total tour length increase
is at most

l∈G N−1
N cr(T,l)
P
+

l∈G N−1
12cr(T,l)
C −7
≤OPT(V )
 N
P +
12
C −7

≤OPT(V )ϵ
2.
Hence the probability that the tour length increase is at most OPT(V )ϵ must be
at least 1
2.
2
With this theorem we can ﬁnally describe Arora’s Algorithm. The idea is
to enumerate all light Steiner tours, using dynamic programming. A subproblem
consists of a region r of a grid G(a,b)
i
with 1 ≤i ≤N −1, a set A of even
cardinality, each element of which is assigned to a portal on one of the edges of
r (such that no more than C elements are assigned to each edge), and a perfect
matching M of the complete graph on A. So for each region, we have less than
(P + 2)4C(4C)! subproblems (up to renaming the elements of A). A solution to
such a subproblem is a set of |M| paths {Pe : e ∈M} which form the intersection
of some light Steiner tour for V with r, such that P{v,w} has the endpoints v and
w, and each point of V ∩r belongs to exactly one of the paths. A solution is
optimum if the total length of the paths is shortest possible.
Arora’s Algorithm
Input:
A well-rounded instance V ⊆[0, 2N] × [0, 2N] of the Euclidean
TSP. A number 0 < ϵ < 1.
Output:
A tour which is optimal up to a factor of (1 + ϵ).
1⃝
Choose a and b randomly out of {0, 2, . . . , 2N −2}.
Set R0 :=
5
[0, 2N] × [0, 2N], V
6
.
2⃝
For i := 1 to N −1 do:
Construct G(a,b)
i
. Set Ri := ∅.
For each (r, Vr) ∈Ri−1 for which |Vr| ≥2 do:
Construct the four regions r1,r2,r3,r4 of G(a,b)
i
with r1 ∪r2 ∪r3 ∪r4 = r
and add (rj, Vr ∩rj) to Ri ( j = 1, 2, 3, 4).

512
21. The Traveling Salesman Problem
3⃝
For i := N −1 down to 1 do:
For each region r ∈Ri do: Solve all its subproblems optimally.
If |Vr| ≤1 then this is done directly,
else the already computed optimum solutions of the
subproblems for the four subregions are used.
4⃝
Compute an optimum light Steiner tour for V using the optimum solutions
of the subproblems for the four subregions.
Remove the Steiner points to obtain a tour which is no longer.
Theorem 21.13.
Arora’s Algorithm ﬁnds a tour which is, with probability at
least 1
2, no longer than (1 + ϵ) OPT(V ). The running time is O(n(log n)c) for a
constant c (depending linearly on 1
ϵ ).
Proof:
The algorithm chooses a and b randomly and then computes an optimum
light Steiner tour. By Theorem 21.12, this is no longer than (1 + ϵ) OPT(V ) with
probability at least 1
2. The ﬁnal removal of the Steiner points can only make the
tour shorter.
To estimate the running time, consider the tree of regions: the root is the region
in R0, and each region r ∈Ri has 0 or 4 children (subregions in Ri+1). Let S be
the set of vertices in this tree that have 4 children which are all leaves. Since the
interiors of these regions are disjoint and each contains at least two points of V ,
we have |S| ≤n
2. Since each vertex of the tree is either a leaf or a predecessor of
at least one vertex in S, we have at most N n
2 vertices that are not leaves and thus
at most 5
2 Nn vertices altogether.
For each region, at most (P +2)4C(4C)! subproblems arise. Subproblems cor-
responding to regions with at most one point can be solved directly in O(C) time.
For other subproblems, all possible multisets of portals on the four edges between
the subregions and all possible orders in which the portals can be traversed, are
considered. All these at most (P + 2)4C(8C)! possibilities can then be evaluated
in constant time using the stored solutions of the subproblems.
So for all subproblems of one region, the running time is O

(P + 2)8C(4C)!
(8C)!

. Observe that this also holds for disconnected regions: since the tour may
not go from one connected component of a region to another, the problem can
only become easier.
Since at most 5
2 Nn regions are considered, N = O

log n
ϵ

(the instance is
well-rounded), C = O
 1
ϵ

and P = O
 N
ϵ

, we obtain an overall running time of
O

n log n
ϵ (P + 2)8C(8C)12C
= O

n(log n)O( 1
ϵ)O
1
ϵ
O( 1
ϵ)
.
2
Of course, Arora’s Algorithm can easily be derandomized by trying all
possible values of a and b. This adds a factor of O

n2
ϵ2

to the running time. We
conclude:

21.3 Local Search
513
Corollary 21.14.
There is an approximation scheme for the Euclidean TSP.
For each ﬁxed ϵ > 0, a (1 + ϵ)-approximate solution can be determined in
O

n3(log n)c
time for some constant c.
2
Rao and Smith [1998] improved the running time to O(n log n) for each ﬁxed
ϵ > 0. However, the constants involved are still quite large for reasonable values
of ϵ, and thus the practical value seems to be limited.
21.3 Local Search
In general, the most successful technique for obtaining good solutions for TSP
instances in practice is local search. The idea is as follows. We start with any tour
found by some heuristic. Then we try to improve our solution by certain “local”
modiﬁcations. For example, we could cut our tour into two pieces by deleting two
edges and then join the pieces to a different tour.
Local search is an algorithmic principle rather than an algorithm. In particular,
two decisions must be made in advance:
–
Which are the modiﬁcations allowed, i.e. how is the neighbourhood of a solu-
tion deﬁned?
–
When do we actually modify our solution? (One possibility here is to allow
improvements only.)
To give a concrete example, we describe the well-known k-Opt Algorithm for
the TSP. Let k ≥2 be a ﬁxed integer.
k-Opt Algorithm
Input:
An instance (Kn, c) of the TSP.
Output:
A tour.
1⃝
Let T be any tour.
2⃝
Let S be the family of k-element subsets of E(T ).
3⃝
For all S ∈S and all tours T ′ with E(T ′) ⊇E(T ) \ S do:
If c(E(T ′)) < c(E(T )) then set T := T ′ and go to 2⃝.
4⃝
Stop.
A tour is called k-opt if it cannot be improved by the k-Opt Algorithm.
For any ﬁxed k there are TSP instances and k-opt tours that are not (k + 1)-opt.
For example, the tour shown in Figure 21.4 is 2-opt but not 3-opt (with respect
to Euclidean distances). It can be improved by exchanging three edges (the tour
(a, b, e, c, d, a) is optimum).
The tour shown on the right-hand side of Figure 21.5 is 3-opt with respect to
the weights shown on the left-hand side. Edges not shown have weight 4. However,

514
21. The Traveling Salesman Problem
1
3
4
4
a
b
c
d
e
Fig. 21.4.
2
3
3
2
2
3
3
2
2
2
2
2
Fig. 21.5.
a 4-exchange immediately produces the optimum solution. Note that the triangle
inequality holds.
Indeed, the situation is much worse: a tour produced by the k-Opt Algorithm
for an n-city instance can be longer than the optimum tour by a factor of 1
4n
1
2k
(for all k and inﬁnitely many n). On the other hand a 2-opt tour is never worse
than 4√n times the optimum. However, the worst-case running time of the k-Opt
Algorithm is exponential for all k. All these results are due to Chandra, Karloff
and Tovey [1999].
Another question is how to choose k in advance. Of course, instances (Kn, c)
are solved optimally by the k-Opt Algorithm with k = n, but the running time
grows exponentially in k. Often k = 3 is a good choice. Lin and Kernighan [1973]
proposed an effective heuristic where k is not ﬁxed but rather determined by the
algorithm. Their idea is based on the following concept:
Deﬁnition 21.15.
Given an instance (Kn, c) of the TSP and a tour T . An al-
ternating walk is a sequence of vertices (cities) P = (x0, x1, . . . , x2m) such that
{xi, xi+1} ̸= {xj, xj+1} for all 0 ≤i < j < 2m, and for i = 0, . . . , 2m −1 we have
{xi, xi+1} ∈E(T ) if and only if i is even. P is closed if in addition x0 = x2m.
The gain of P is deﬁned by

21.3 Local Search
515
g(P) :=
m−1

i=0
(c({x2i, x2i+1}) −c({x2i+1, x2i+2})).
P is called proper if g((x0, . . . , x2i)) > 0 for all i ∈{1, . . . , m}. We use the
abbreviation E(P) = {{xi, xi+1} : i = 0, . . . , 2m −1}.
Note that vertices may occur more than once in an alternating walk. In the
example shown in Figure 21.4, (a, e, b, c, e, d, a) is a proper closed alternating
walk. Given a tour T , we are of course interested in those closed alternating walks
P for which E(T )△E(P) again deﬁnes a tour.
Lemma 21.16.
(Lin and Kernighan [1973]) If there is a closed alternating walk
P with g(P) > 0, then
(a) c(E(T )△E(P)) = c(E(T )) −g(P);
(b) there is a proper closed alternating walk Q with E(Q) = E(P).
Proof:
Part (a) follows from the deﬁnition. To see (b), let P = (x0, x1, . . . , x2m),
and let k be the largest index for which g((x0, . . . , x2k)) is minimum. We claim
that Q := (x2k, x2k+1, . . . , x2m−1, x0, x1, . . . , x2k) is proper. For i = k + 1, . . . , m
we have
g((x2k, x2k+1, . . . , x2i)) = g((x0, x1, . . . , x2i)) −g((x0, x1, . . . , x2k)) > 0
by deﬁnition of k. For i = 1, . . . , k we have
g((x2k, x2k+1, . . . , x2m−1, x0, x1, . . . , x2i))
=
g((x2k, x2k+1, . . . , x2m)) + g((x0, x1, . . . , x2i))
≥
g((x2k, x2k+1, . . . , x2m)) + g((x0, x1, . . . , x2k))
=
g(P) > 0,
again by deﬁnition of k. So Q is indeed proper.
2
We now go ahead with the description of the algorithm. Given any tour
T , it looks for a proper closed alternating walk P and then iterates with
(V (T ), E(T )△E(P)). At each iteration it exhaustively checks all possibilities un-
til some proper closed alternating walk is found, or until one of the two parameters
p1 and p2 prevent it from doing so. See also Figure 21.6 for an illustration.
Lin-Kernighan Algorithm
Input:
An instance (Kn, c) of the TSP. Two parameters p1 ∈N (backtrack-
ing depth) and p2 ∈N (infeasibility depth).
Output:
A tour T .
1⃝
Let T be any tour.
2⃝
Set X0 := V (Kn), i := 0 and g∗:= 0.

516
21. The Traveling Salesman Problem
x0
x1
x2
x3
x4
x5
T
Fig. 21.6.
3⃝
If Xi = ∅and g∗> 0 then:
Set T := (V (T ), E(T )△E(P∗)) and go to 2⃝.
If Xi = ∅and g∗= 0 then:
Set i := min(i −1, p1). If i < 0 then stop, else go to 3⃝.
4⃝
Choose xi ∈Xi and set Xi := Xi \ {xi}.
If i is odd, i ≥3, (V (T ), E(T )△E((x0, x1, . . . , xi−1, xi, x0))) is a tour
and g((x0, x1, . . . , xi−1, xi, x0)) > g∗then:
Set P∗:= (x0, x1, . . . , xi−1, xi, x0) and g∗:= g(P∗).
5⃝
If i is odd then:
Set
Xi+1
:=
{x
∈
V (Kn) \ {x0, xi}
:
{xi, x}
/∈
E(T ) ∪
E((x0, x1, . . . , xi−1)),
g((x0, x1, . . . , xi−1, xi, x)) > g∗}.
If i is even and i ≤p2 then:
Set Xi+1 := {x ∈V (Kn) : {xi, x} ∈E(T ) \ E((x0, x1, . . . , xi))}.
If i is even and i > p2 then:
Set Xi+1 := {x ∈V (Kn) : {xi, x} ∈E(T ) \ E((x0, x1, . . . , xi)),
{x, x0} /∈E(T ) ∪E((x0, x1, . . . , xi)),
(V (T ), E(T )△E((x0, x1, . . . , xi, x, x0))) is a tour}.
Set i := i + 1. Go to 3⃝.
Lin and Kernighan have proposed the parameters p1 = 5 and p2 = 2. These
are the smallest values which guarantee that the algorithm ﬁnds a favourable 3-
exchange:
Theorem 21.17.
(Lin and Kernighan [1973]) The Lin-KernighanAlgorithm
(a) for p1 = ∞and p2 = ∞ﬁnds a proper closed alternating walk P such that
(V (T ), E(T )△E(P)) is a tour, if one exists.
(b) for p1 = 5 and p2 = 2 returns a tour which is 3-opt.

21.3 Local Search
517
Proof:
Let T be the tour the algorithm ends with. Then g∗must have been zero
all the time since the last tour change. This implies that, in the case p1 = p2 = ∞,
the algorithm has completely enumerated all proper alternating walks. In particular,
(a) holds.
In the case p1 = 5 and p2 = 2, the algorithm has at least enumerated all
proper closed alternating walks of length 4 or 6. Suppose there exists a favourable
2-exchange or 3-exchange resulting in a tour T ′. Then the edges E(T )△E(T ′)
form a closed alternating walk P with at most six edges and g(P) > 0. By
Lemma 21.16, P is w.l.o.g. proper and the algorithm would have found it. This
proves (b).
2
We should remark that this procedure has no chance of ﬁnding a “non-
sequential” exchange such as the 4-exchange shown in Figure 21.5. In this ex-
ample the tour cannot be improved by the Lin-Kernighan Algorithm, but a
(non-sequential) 4-exchange would provide the optimum solution.
So a reﬁnement of the Lin-Kernighan Algorithm could be as follows. If the
algorithm stops, we try (by some heuristics) to ﬁnd a favourable non-sequential
4-exchange. If we are successful, we continue with the new tour, otherwise we
give up.
The Lin-Kernighan Algorithm is far more effective than e.g. the 3-Opt
Algorithm. While being at least as good (and usually much better), the expected
running time (with p1 = 5 and p2 = 2) also compares favourably: Lin and
Kernighan report an empirical running time of about O(n2.2). However, it seems
unlikely that the worst-case running time is polynomial; for a precise formulation
of this statement (and a proof), see Exercise 11 (Papadimitriou [1992]).
Almost all local search heuristics used for the TSP in practice are based on
this algorithm. Although the worst-case behaviour is worse than Christofides’
Algorithm, the Lin-Kernighan Algorithm typically produces much better so-
lutions, usually within a few percent of the optimum. For a very efﬁcient variant,
see Applegate, Cook and Rohe [2003].
By Exercise 12 of Chapter 9 there is no local search algorithm for the TSP
which has polynomial-time complexity per iteration and always ﬁnds an optimum
solution, unless P = NP (here one iteration is taken to be the time between two
changes of the current tour). We now show that one cannot even decide whether
a given tour is optimum. To do this we ﬁrst consider the following restriction of
the Hamiltonian Circuit problem:
Restricted Hamiltonian Circuit
Instance:
An undirected graph G and some Hamiltonian path in G.
Question:
Does G contain a Hamiltonian circuit?
Lemma 21.18.
Restricted Hamiltonian Circuit is NP-complete.

518
21. The Traveling Salesman Problem
Proof:
Given an instance G of the Hamiltonian Circuit Problem (which is
NP-complete, see Theorem 15.25), we construct an equivalent instance of Re-
stricted Hamiltonian Circuit.
Assume V (G) = {1, . . . , n}. We take n copies of the “diamond graph” shown
in Figure 21.7 and join them vertically with edges {Si, Ni+1} (i = 1, . . . , n −1).
Ni
Si
Wi
Ei
Fig. 21.7.
It is clear that the resulting graph contains a Hamiltonian path from N1 to Sn.
We now add edges {Wi, Ej} and {Wj, Ei} for any edge {i, j} ∈E(G). Let us call
the resulting graph H. It is obvious that any Hamiltonian circuit in G induces a
Hamiltonian circuit in H.
Furthermore, a Hamiltonian circuit in H must traverse all the diamond sub-
graphs in the same way: either all from Ei to Wi or all from Si to Ni. But the
latter is impossible, so H is Hamiltonian if and only if G is.
2
Theorem 21.19.
(Papadimitriou and Steiglitz [1977])
The problem of decid-
ing whether a given tour is optimum for a given Metric TSP instance is coNP-
complete.
Proof:
Membership in coNP is clear, since an optimum tour serves as a certiﬁcate
for suboptimality.
We shall now transform RestrictedHamiltonianCircuit to the complement
of our problem. Namely, given a graph G and a Hamiltonian path P in G, we
ﬁrst check whether the ends of P are connected by an edge. If so, we are done.
Otherwise we deﬁne
ci j :=
 1
if {i, j} ∈E(G)
2
if {i, j} /∈E(G).
The triangle inequality is of course satisﬁed. Moreover, P deﬁnes a tour of cost
n + 1, which is optimum if and only if there is no Hamiltonian circuit in G.
2
Corollary 21.20.
Unless P = NP, no local search algorithm for the TSP having
polynomial-time complexity per iteration can be exact.

21.4 The Traveling Salesman Polytope
519
Proof:
An exact local search algorithm includes the decision whether the initial
tour is optimum.
2
Local search of course also applies to many other combinatorial optimization
problems. The Simplex Algorithm can also be regarded as a local search al-
gorithm. Although local search algorithms have proved to be very successful in
practice, almost no theoretical evidence for their efﬁciency is known, except in
few special cases (see, e.g., Exercise 4 in Chapter 16 and Sections 22.6 and 22.8).
For many NP-hard problems and interesting neighbourhoods (including the ones
discussed in this section) it is not even known whether a local optimum can be
computed in polynomial time; see Exercise 11. The book edited by Aarts and
Lenstra [1997] contains more examples of local search heuristics.
21.4 The Traveling Salesman Polytope
Dantzig, Fulkerson and Johnson [1954] were the ﬁrst to solve a TSP instance of
non-trivial size optimally. They started by solving an LP relaxation of a suitable in-
teger linear programming formulation, and then successively added cutting planes.
This was the starting point of the analysis of the traveling salesman polytope:
Deﬁnition 21.21.
For n ≥3 we denote by Q(n) the traveling salesman poly-
tope, i.e. the convex hull of the incidence vectors of tours in the complete graph
Kn.
Although no complete description of the traveling salesman polytope is known,
there are several interesting results, some of which are also relevant for practical
computations. Since 
e∈δ(v) xe = 2 for all v ∈V (Kn) and all x ∈Q(n), the
dimension of Q(n) is at most |E(Kn)| −|V (Kn)| =
n
2

−n = n(n−3)
2
. In order to
prove that in fact dim (Q(n)) = n(n−3)
2
, we need the following graph-theoretical
lemma:
Lemma 21.22.
For any k ≥1:
(a) The edge set of K2k+1 can be partitioned into k tours.
(b) The edge set of K2k can be partitioned into k−1 tours and one perfect matching.
Proof:
(a): Suppose the vertices are numbered 0, . . . , 2k −1, x. Consider the
tours
Ti
=
(x, i, i + 1, i −1, i + 2, i −2, i + 3, . . . ,
i −k + 2, i + k −1, i −k + 1, i + k, x)
for i = 0, . . . , k −1 (everything is meant modulo 2k). See Figure 21.8 for an
illustration. Since |E(K2k+1)| = k(2k + 1) it sufﬁces to show that these tours are
edge-disjoint. This is clear with respect to the edges incident to x. Moreover, for
{a, b} ∈E(Ti) with a, b ̸= x we have a + b ∈{2i, 2i + 1}, as is easily seen.

520
21. The Traveling Salesman Problem
i
i −1
i + 1
i + 2
i + 3
i + k
i −k + 1
i −k + 2
x
Fig. 21.8.
(b): Suppose the vertices are numbered 0, . . . , 2k −2, x. Consider the tours
Ti
=
(x, i, i + 1, i −1, i + 2, i −2, i + 3, . . . ,
i + k −2, i −k + 2, i + k −1, i −k + 1, x)
for i = 0, . . . , k −2 (everything is meant modulo 2k −1). The same argument as
above shows that these tours are edge-disjoint. After deleting them, the remaining
graph is 1-regular and thus provides a perfect matching.
2
Theorem 21.23.
(Gr¨otschel and Padberg [1979])
dim (Q(n)) = n(n −3)
2
.
Proof:
For n = 3 the statement is trivial. Let n ≥4, and let v ∈V (Kn) be an
arbitrary vertex.
Case 1:
n is even, say n = 2k + 2 for some integer k ≥1. By Lemma 21.22(a)
Kn −v is the union of k edge-disjoint tours T0, . . . , Tk−1. Now let Ti j arise from
Ti by replacing the j-th edge {a, b} by two edges {a, v}, {v, b} (i = 0, . . . , k −1;
j = 1, . . . , n −1). Consider the matrix whose rows are the incidence vectors of
these k(n −1) tours. Then the columns corresponding to edges not incident with
v form a square matrix

21.4 The Traveling Salesman Polytope
521
⎛
⎜⎜⎜⎜⎜⎜⎝
A
0
0
· · ·
0
0
A
0
· · ·
0
0
0
A
· · ·
0
· · ·
· · ·
· · ·
· · ·
· · ·
0
0
0
· · ·
A
⎞
⎟⎟⎟⎟⎟⎟⎠
,
where A =
⎛
⎜⎜⎜⎜⎜⎜⎝
0
1
1
· · ·
1
1
0
1
· · ·
1
1
1
0
· · ·
1
· · ·
· · ·
· · ·
· · ·
· · ·
1
1
1
· · ·
0
⎞
⎟⎟⎟⎟⎟⎟⎠
.
Since this matrix is nonsingular, the incidence vectors of the k(n −1) tours are
linearly independent, implying dim (Q(n)) ≥k(n −1) −1 = n(n−3)
2
.
Case 2:
n is odd, so n = 2k + 3 with k ≥1 integer. By Lemma 21.22(b)
Kn −v is the union of k tours and one perfect matching M. From the tours, we
construct k(n −1) tours in Kn as in (a). Now we complete the perfect matching
M arbitrarily to a tour T in Kn−1. For each edge e = {a, b} of M, we replace e in
T by the two edges {a, v} and {v, b}. In this way we obtain another k + 1 tours.
Similarly as above, the incidence vectors of all the k(n −1) + k + 1 = kn + 1
tours are linearly independent, proving dim (Q(n)) ≥kn + 1 −1 = n(n−3)
2
.
2
The integral points of Q(n), i.e. the tours, can be described nicely:
Proposition 21.24.
The incidence vectors of the tours in Kn are exactly the in-
tegral vectors x satisfying
0 ≤xe ≤1
(e ∈E(Kn));
(21.1)

e∈δ(v)
xe = 2
(v ∈V (Kn));
(21.2)

e∈E(Kn[X])
xe ≤|X| −1
(∅̸= X ⊂V (Kn)).
(21.3)
Proof:
Obviously the incidence vector of any tour satisﬁes these constraints.
Any integral vector satisfying (21.1) and (21.2) is the incidence vector of a per-
fect simple 2-matching, i.e. the union of vertex-disjoint circuits covering all the
vertices. The constraints (21.3) prevent circuits with less than n edges.
2
The constraints (21.3) are usually called subtour inequalities, and the polytope
deﬁned by (21.1), (21.2), (21.3) is called the subtour polytope. In general the
subtour polytope is not integral, as the instance in Figure 21.9 shows (edges not
shown have weight 3): the shortest tour has length 10, but the optimum fractional
solution (xe = 1 if e has weight 1, and xe = 1
2 if e has weight 2) has total weight 9.
The following equivalent descriptions of the subtour polytope will be useful:
Proposition 21.25.
Let V (Kn) = {1, . . . , n}. Let x ∈[0, 1]E(Kn) with 
e∈δ(v) xe
= 2 for all v ∈V (Kn). Then the following statements are equivalent:

e∈E(Kn[X])
xe ≤|X| −1
(∅̸= X ⊂V (Kn));
(21.3)

e∈E(Kn[X])
xe ≤|X| −1
(∅̸= X ⊆V (Kn) \ {1});
(21.4)

522
21. The Traveling Salesman Problem
1
1
1
2
2
2
2
2
2
Fig. 21.9.

e∈δ(X)
xe ≥2
(∅̸= X ⊂V (Kn)).
(21.5)
Proof:
For any ∅̸= X ⊂V (Kn) we have

e∈δ(V (Kn)\X)
xe =

e∈δ(X)
xe
=

v∈X

e∈δ(v)
xe −2

e∈E(Kn[X])
xe
=
2|X| −2

e∈E(Kn[X])
xe,
which implies the equivalence of (21.3), (21.4) and (21.5).
2
Corollary 21.26.
The Separation Problem for subtour inequalities can be
solved in polynomial time.
Proof:
Using (21.5) and regarding x as edge capacities we have to decide
whether there is a cut in (Kn, x) with capacity less than 2. Therefore the Separa-
tion Problem reduces to the problem of ﬁnding a minimum cut in an undirected
graph with nonnegative capacities. By Theorem 8.39 this problem can be solved
in O(n3) time.
2
Since any tour is a perfect simple 2-matching, the convex hull of all perfect
simple 2-matchings contains the traveling salesman polytope. So by Theorem 12.3
we have:
Proposition 21.27.
Any x ∈Q(n) satisﬁes the inequalities

e∈E(Kn[X])∪F
xe ≤|X| + |F| −1
2
for X ⊆V (Kn), F ⊆δ(X) with |F| odd.
(21.6)
The constraints (21.6) are called 2-matching inequalities. It is sufﬁcient to
consider inequalities (21.6) for the case when F is a matching; the other 2-
matching inequalities are implied by these (Exercise 13). For the 2-matching
inequalities, the Separation Problem can be solved in polynomial time by The-
orem 12.18. So by the Ellipsoid Method (Theorem 4.21) we can optimize a
linear function over the polytope deﬁned by (21.1), (21.2), (21.3), and (21.6) in

21.4 The Traveling Salesman Polytope
523
polynomial time (Exercise 12). The 2-matching inequalities are generalized by the
so-called comb inequalities, illustrated in Figure 21.10:
T1
T2
T3
Fig. 21.10.
Proposition 21.28.
(Chv´atal [1973], Gr¨otschel and Padberg [1979]) Let T1, . . . ,
Ts ⊆V (Kn) be s pairwise disjoint sets, s ≥3 odd, and H ⊆V (Kn) with Ti ∩H
̸= ∅and Ti \ H ̸= ∅for i = 1, . . . , s. Then any x ∈Q(n) satisﬁes

e∈δ(H)
xe +
s

i=1

e∈δ(Ti)
xe ≥3s + 1.
(21.7)
Proof:
Let x be the incidence vector of any tour. For any i ∈{1, . . . , s} we have

e∈δ(Ti)
xe +

e∈δ(H)∩E(Kn[Ti])
xe ≥3,
9
11
10
12
7
8
2
1
6
4
3
5
xe = 1
xe = 1/2
Fig. 21.11.

524
21. The Traveling Salesman Problem
since the tour must enter and leave Ti \ H as well as Ti ∩H. Summing these s
inequalities we get

e∈δ(H)
xe +
s

i=1

e∈δ(Ti)
xe ≥3s.
Since the left-hand side is an even integer, the theorem follows.
2
The fractional solution x shown in Figure 21.11 (edges e with xe = 0 are
omitted) is an example of a violated comb inequality in K12: consider H =
{1, 2, 3, 4, 5, 6}, T1 = {1, 11}, T2 = {2, 12} and T3 = {5, 6, 7, 8}. It is easy to
check that the corresponding comb inequality is violated. Note that the inequalities
(21.1), (21.2), (21.3), (21.6) are satisﬁed, and x is optimum with respect to weights
c(e) := 1 −xe (total weight 3), while the optimum tour has weight 7
2.
Let us mention just one further class: the clique tree inequalities:
Theorem 21.29.
(Gr¨otschel and Pulleyblank [1986])
Let H1, . . . , Hr be pair-
wise disjoint subsets of V (G) (the handles), and let T1, . . . , Ts (s ≥1) be pairwise
disjoint nonempty proper subsets of V (G) (the teeth) such that
–
for each handle, the number of teeth it intersects is odd and at least three;
–
each tooth T contains at least one vertex not belonging to any handle;
–
G := Kn[H1 ∪· · · ∪Hr ∪T1 ∪· · · ∪Ts] is connected, but G −(Ti ∩Hj) is
disconnected whenever Ti ∩Hj ̸= ∅.
Let tj denote the number of handles intersecting Tj ( j = 1, . . . , s). Then any
x ∈Q(n) satisﬁes
r

i=1

e∈E(Kn[Hi])
xe +
s

j=1

e∈E(Kn[Tj])
xe ≤
r

i=1
|Hi| +
s

j=1
(|Tj| −tj) −s + 1
2
. (21.8)
We omit the proof which is technically not so easy. Clique tree inequalities
include (21.3) and (21.6) (Exercise 14). They have been generalized further, e.g. to
bipartition inequalities by Boyd and Cunningham [1991]. There is a polynomial-
time algorithm for the Separation Problem for the clique tree inequalities (21.8)
with a ﬁxed number of handles and teeth (Carr [1997]), but none is known for
general clique tree inequalities. Even for the Separation Problem for comb in-
equalities no polynomial-time algorithm is known.
All the inequalities (21.1), (21.4) (restricted to the case where 3 ≤|X| ≤n−3)
and (21.6) (restricted to the case where F is a matching) deﬁne distinct facets of
Q(n) (for n ≥6). The proof that the trivial inequalities (21.1) deﬁne facets consists
of ﬁnding dim (Q(n)) linearly independent tours with xe = 1 (and the same for
xe = 0) for some ﬁxed edge e. The proof is similar to that of Theorem 21.23,
we refer to Gr¨otschel and Padberg [1979]. Even all the inequalities (21.8) deﬁne
facets of Q(n) (n ≥6). The proof is quite involved, see Gr¨otschel and Padberg
[1979], or Gr¨otschel and Pulleyblank [1986].

21.5 Lower Bounds
525
The number of facets of Q(n) grows fast: already Q(10) has more than 50
billion facets. No complete description of Q(n) is known, and it appears very
unlikely that one exists. Consider the following problem:
TSP Facets
Instance:
An integer n and an integral inequality ax ≤β.
Question:
Does the given inequality deﬁne a facet of Q(n)?
The following result shows that a complete description of the traveling sales-
man polytope is unlikely:
Theorem 21.30.
(Karp and Papadimitriou [1982]) If TSP Facets is in NP, then
NP = coNP.
Moreover, it is NP-complete to decide whether two given vertices of Q(n) are
adjacent (i.e. belong to a common face of dimension one) (Papadimitriou [1978]).
21.5 Lower Bounds
Suppose we have found some tour heuristically, e.g. by the Lin-Kernighan Al-
gorithm. We do not know in advance whether this tour is optimum or at least
close to the optimum. Is there any way to guarantee that our tour is no more than
x percent away from the optimum? In other words, is there a lower bound for the
optimum?
Lower bounds can be found by considering any LP relaxation of an integer
programming formulation of the TSP, e.g. by taking the inequalities (21.1), (21.2),
(21.3), (21.6). However, this LP is not easy to solve (though there is a polynomial-
time algorithm via the Ellipsoid Method). A more reasonable lower bound is
obtained by taking just (21.1), (21.2), (21.6), i.e. ﬁnding the minimum weight
perfect simple 2-matching (cf. Exercise 1 of Chapter 12).
However, the most efﬁcient method known is the use of Lagrangean relaxation
(cf. Section 5.6). Lagrangean relaxation was ﬁrst applied to the TSP by Held and
Karp [1970,1971]. Their method is based on the following notion:
Deﬁnition 21.31.
Given a complete graph Kn with V (Kn) = {1, . . . , n}, a 1-tree
is a graph consisting of a spanning tree on the vertices {2, . . . , n} and two edges
incident to vertex 1.
The tours are exactly the 1-trees T with |δT (i)| = 2 for i = 1, . . . , n. We know
spanning trees well, and 1-trees are not much different. For example we have:
Proposition 21.32.
The convex hull of the incidence vectors of all 1-trees is the
set of vectors x ∈[0, 1]E(Kn) with

e∈E(Kn)
xe = n,

e∈δ(1)
xe = 2,

e∈E(Kn[X])
xe ≤|X| −1 (∅̸= X ⊆{2, . . . , n}).

526
21. The Traveling Salesman Problem
Proof:
This follows directly from Theorem 6.12.
2
Observe that any linear objective function can easily be optimized over the set
of 1-trees: just ﬁnd a minimum weight spanning tree on {2, . . . , n} (cf. Section
6.1) and add the two cheapest edges incident to the vertex 1. Now Lagrangean
relaxation yields the following lower bound:
Proposition 21.33.
(Held and Karp [1970])
Given an instance (Kn, c) of the
TSP with V (Kn) = {1, . . . , n}, and λ = (λ2, . . . , λn) ∈Rn−1. Then
L R(Kn, c, λ) := min

c(E(T )) +
n

i=2
(|δT (i)| −2) λi : T is a 1-tree

is a lower bound for the length of an optimum tour, which can be computed in the
time needed to solve a Minimum Spanning Tree Problem on n −1 vertices.
Proof:
An optimum tour T is a 1-tree with |δT (i)| = 2 for all i, proving that
L R(Kn, c, λ) is a lower bound. Given λ = (λ2, . . . , λn), we choose λ1 arbitrarily
and replace the weights c by c′({i, j}) := c({i, j}) + λi + λj (1 ≤i < j ≤n).
Then all we have to do is to ﬁnd a minimum weight 1-tree with respect to c′. 2
Note that the Lagrange multipliers λi (i = 2, . . . , n) are not restricted to the
nonnegative numbers because the additional constraints |δT (i)| = 2 are equalities.
The λi can be determined by some subgradient optimization procedure; cf. Section
5.6. The maximum possible value
H K(Kn, c) := max{L R(Kn, c, λ) : λ ∈Rn−1}
(the Lagrangean dual) is called the Held-Karp bound for (Kn, c). We have:
Theorem 21.34.
(Held and Karp [1970]) For any instance (Kn, c) of the TSP
with V (Kn) = {1, . . . , n}:
H K(Kn, c)
=
min

cx : 0 ≤xe ≤1 (e ∈E(Kn)),

e∈δ(v)
xe = 2 (v ∈V (Kn)),

e⊆I
xe ≤|I| −1 for all ∅̸= I ⊆{2, . . . , n}

.
Proof:
This follows directly from Theorem 5.35 and Proposition 21.32.
2
In other words, the Held-Karp bound equals the optimum LP value of the
subtour polytope (cf. Proposition 21.25). This helps us to estimate the quality of
the Held-Karp bound for the Metric TSP. We also use the idea of Christofides’
Algorithm again:
Theorem 21.35.
(Wolsey [1980]) For any MetricTSP instance, the Held-Karp
bound is at least 2
3 of the length of an optimum tour.

21.6 Branch-and-Bound
527
Proof:
Let (Kn, c) be a Metric TSP instance, and let T be a minimum weight
1-tree in (Kn, c). We have
c(E(T )) = L R(Kn, c, 0) ≤H K(Kn, c).
Let W ⊆V (Kn) consist of the vertices having odd degree in T . Since each
vector x in the subtour polytope of (Kn, c) satisﬁes 
e∈δ(X) xe ≥2 for all ∅̸=
X ⊂V (Kn), the polyhedron
⎧
⎨
⎩x : xe ≥0 for all e ∈E(Kn),

e∈δ(X)
xe ≥2 for all X with |X ∩W| odd
⎫
⎬
⎭
contains the subtour polytope. Therefore, by Theorem 21.34,
min
⎧
⎨
⎩cx : xe ≥0 for all e ∈E(Kn),

e∈δ(X)
xe ≥1 for all X with |X ∩W| odd
⎫
⎬
⎭
≤1
2 H K(Kn, c).
But now observe that by Theorem 12.16, the left-hand side is the minimum weight
of a W-join J in (Kn, c). So c(E(T )) + c(J) ≤3
2 H K(Kn, c). Since the graph
G := (V (Kn), E(T ) ∪J) is connected and Eulerian, it is an upper bound on the
length of an optimum tour (by Lemma 21.3).
2
A different proof is due to Shmoys and Williamson [1990]. It is not known
whether this result is tight. The instance of Figure 21.9 on page 522 (edges not
shown have weight 3) is an example where the Held-Karp bound (9) is strictly
less than the length of the optimum tour (which is 10). There are instances of
the Metric TSP where H K(Kn,c)
OPT(Kn,c) is arbitrarily close to 3
4 (Exercise 15). However,
these can be considered as exceptions: in practice the Held-Karp bound is usually
much better; see e.g. Johnson, McGeoch and Rothberg [1996].
21.6 Branch-and-Bound
Branch-and-bound is a technique for the complete enumeration of all possible
solutions without having to consider them one by one. For many NP-hard com-
binatorial optimization problems it is the best known framework for obtaining an
optimum solution. It has been proposed by Land and Doig [1960] and applied to
the TSP ﬁrst by Little, Murty, Sweeny and Karel [1963].
To apply the Branch-And-Bound Method to a combinatorial optimization
(say minimization) problem, we need two steps:
–
“branch”: a given subset of the possible solutions (tours for the TSP) can be
partitioned into at least two nonempty subsets.

528
21. The Traveling Salesman Problem
–
“bound”: for a subset obtained by branching iteratively, a lower bound on the
cost of any solution within this subset can be computed.
The general procedure then is as follows:
Branch-And-Bound Method
Input:
An instance of a problem.
Output:
An optimum solution S∗.
1⃝
Set the initial tree T := ({S}, ∅), where S is the set of all feasible solutions.
Mark S active.
Set the upper bound U := ∞(or apply a heuristic in order to get a better
upper bound).
2⃝
Choose an active vertex X of the tree T (if there is none, stop).
Mark X non-active.
(“branch”) Find a partition X = X1
.
∪. . .
.
∪Xt.
3⃝
For each i = 1, . . . , t do:
(“bound”) Find a lower bound L on the cost of any solution in Xi.
If |Xi| = 1 (say Xi = {S}) and cost(S) < U then:
Set U := cost(S) and S∗:= S.
If |Xi| > 1 and L < U then:
Set T := (V (T ) ∪{Xi}, E(T ) ∪{{X, Xi}}) and mark Xi active.
4⃝
Go to 2⃝.
It should be clear that the above method always ﬁnds an optimum solution. The
implementation (and the efﬁciency) of course depends very much on the actual
problem. We shall discuss a possible implementation for the TSP.
The easiest way to perform the branching is to choose an edge e and write
X = Xe ∪Ye, where Xe (Ye) consists of those solutions in X that contain (do not
contain) edge e. Then we can write any vertex X of the tree as
SA,B = {S ∈S : A ⊆S, B ∩S = ∅}
for some A, B ⊆E(G).
For these X = SA,B, the TSP with the additional constraint that all edges of
A, but none of B, belong to the tour, can be written as an unconstrained TSP by
modifying the weights c accordingly: namely we set
c′
e :=
 ce
if e ∈A
ce + C
if e /∈A ∪B
ce + 2C
if e ∈B
with C := 
e∈E(G) ce + 1. Then the tours in SA,B are exactly the tours whose
modiﬁed weight is less than (n+1−|A|)C. Furthermore, the new and the modiﬁed
weight of any tour in SA,B differ by exactly (n −|A|)C.
Then the Held-Karp bound (cf. Section 21.5) can be used to implement the
“bound”-step.

21.6 Branch-and-Bound
529
The above Branch-And-Bound Method for the TSP has been used to solve
fairly large instances of the TSP (up to about 100 cities).
Branch-And-Bound is also often used to solve integer programs, especially
when the variables are binary (restricted to be 0 or 1). Here the most natural
branching step consists of taking one variable and trying both possible values
for it. A lower bound can easily be determined by solving the corresponding LP
relaxation.
In the worst case, Branch-And-Bound is no better than the complete enu-
meration of all possible solutions. In practice, the efﬁciency depends not only on
how the “branch” and “bound” steps are implemented. It is also important to have
a good strategy for choosing the active vertex X in 2⃝of the algorithm. Further-
more, a good heuristic at the beginning (and thus a good upper bound to start
with) can help to keep the branch-and-bound tree T small.
Branch-And-Bound is often combined with a cutting plane method (see
Section 5.5), based on the results of Section 21.4. One proceeds as follows. Since
we have an exponential number of constraints (which do not even describe Q(n)
completely), we start by solving the LP
min
⎧
⎨
⎩cx : 0 ≤xe ≤1 (e ∈E(Kn)),

e∈δ(v)
xe = 2 (v ∈V (Kn))
⎫
⎬
⎭,
i.e. with constraints (21.1) and (21.2). This polyhedron contains the perfect simple
2-matchings as integral vectors. Suppose we have a solution x∗of the above LP.
There are three cases:
(a) x∗is the incidence vector of a tour;
(b) We ﬁnd some violated subtour inequality (21.3), 2-matching inequality (21.6),
comb inequality (21.7), or clique tree inequality (21.8).
(c) No violated inequality can be found (in particular no subtour inequality is
violated), but x∗is not integral.
If x∗is integral but not the incidence vector of a tour, some subtour inequality
must be violated by Proposition 21.24.
In case (a) we are done. In case (b) we simply add the violated inequality (or
possibly several violated inequalities) to our LP and solve the new LP. In case (c),
all we have is a (usually very good) lower bound for the length of a tour. Using
this bound (and the fractional solution), we may start a Branch-And-Bound
procedure. Because of the tight lower bound we hopefully can ﬁx many variables
in advance and thus considerably reduce the branching steps necessary to obtain
an optimum solution. Moreover, at each node of the branch-and-bound tree, we
can again look for violated inequalities.
This method – called branch-and-cut – has been used to solve TSP instances
with more than 10000 cities up to optimality. Of course, many sophisticated ideas
not described here are necessary to obtain an efﬁcient implementation. In particu-
lar, good heuristics to detect violated inequalities are essential. See (Applegate et

530
21. The Traveling Salesman Problem
al. [1998,2003]) and J¨unger and Naddef [2001] for more information and further
references.
These successes in solving large instances optimally are in contrast to poor
worst-case running times. Woeginger [2002] gives a survey on subexponential
exact algorithms for NP-hard problems; see also Exercise 1.
Exercises
1. Describe an exact algorithm for the TSP by means of dynamic programming. If
the vertices (cities) are numbered 1, . . . , n, we denote by γ (A, x) the minimum
cost of a 1-x-path P with V (P) = A∪{1}, for all A ⊆{2, 3, . . . , n} and x ∈A.
The idea is now to compute all these numbers γ (A, x). Compare the running
time of this algorithm with the naive enumeration of all tours.
(Held and Karp [1962])
Note: This is the exact TSP algorithm with the best known worst-case running
time. For the Euclidean TSP, Hwang, Chang and Lee [1993] described an
exact algorithm using planar separators with a subexponential running time
O(c
√n log n).
2. Suppose the n cities of a TSP instance are partitioned into m clusters such
that the distance between two cities is zero if and only if they belong to the
same cluster.
(a) Prove that there exists an optimum tour with at most m(m −1) edges of
positive weight.
(b) Prove that such a TSP can be solved in polynomial time if m is ﬁxed.
(Triesch, Nolles and Vygen [1994])
3. Consider the following problem. A truck starting at some depot d1 must visit
certain customers c1, . . . , cn and ﬁnally return to d1. Between visiting two
customers it must visit one of the depots d1, . . . , dk. Given nonnegative sym-
metric distances between the customers and depots, we look for the shortest
possible tour.
(a) Show that this problem is NP-complete.
(b) Show that it can be solved in polynomial time if k is ﬁxed. (Hint: Use
Exercise 2.)
(Triesch, Nolles and Vygen [1994])
4. Consider the Asymmetric TSP with triangle inequality: given a number n ∈N
and distances c((i, j)) ∈R+ for i, j ∈{1, . . . , n}, i ̸= j, satisfying the triangle
inequality c((i, j) + c(( j, k)) ≥c((i, k)) for all distinct i, j, k ∈{1, . . . , n},
ﬁnd a permutation π : {1, . . . , n} →{1, . . . , n} such that n−1
i=1 c((π(i), π(i +
1))) + c((π(n), π(1))) is minimum.
Describe an algorithm that always ﬁnds a solution whose cost is at most log n
times the optimum.
Hint: First ﬁnd a digraph H with V (H) = {1, . . . , n}, |δ−
H(v)| = |δ+
H(v)| = 1
for all v ∈V (H) and minimum cost c(E(H)). Contract the circuits of H and
iterate.

Exercises
531
(Frieze, Galbiati and Mafﬁoli [1982])
Note: The currently best algorithm, a (0.842 log n)-factor approximation, is
due to Kaplan et al. [2003].
5.
∗
Find instances of the Euclidean TSP for which the Double-Tree Algo-
rithm returns a tour whose length is arbitrarily close to twice the optimum.
6. Let G be a complete bipartite graph with bipartition V (G) = A
.
∪B, where
|A| = |B|. Let c : E(G) →R+ be a cost function with c((a, b))+c((b, a′))+
c((a′, b′)) ≥c((a, b′)) for all a, a′ ∈A and b, b′ ∈B. Now the task is to
ﬁnd a Hamiltonian circuit in G of minimum cost. This problem is called the
Metric Bipartite TSP.
(a) Prove that, for any k, if there is a k-factor approximation algorithm for
the Metric Bipartite TSP then there is also a k-factor approximation
algorithm for the Metric TSP.
(b) Find a 2-factor approximation algorithm for the Metric Bipartite TSP.
(Hint: Combine Exercise 25 of Chapter 13 with the idea of the Double-
Tree Algorithm.)
(Frank et al. [1998], Chalasani, Motwani and Rao [1996])
7.
∗
Find instances of the Metric TSP for which Christofides’ Algorithm re-
turns a tour whose length is arbitrarily close to 3
2 times the optimum.
8. Show that the results of Section 21.2 extend to the Euclidean Steiner Tree
Problem. Describe an approximation scheme for this problem.
9. Prove that in the Lin-Kernighan Algorithm a set Xi contains never more
than one element for any odd i with i > p2 + 1.
10. Consider the following decision problem:
Another Hamiltonian Circuit
Instance:
A graph G and a Hamiltonian circuit C in G.
Task:
Is there another Hamiltonian circuit in G?
(a) Show that this problem is NP-complete. (Hint: Recall the proof of Lemma
21.18.)
(b)
∗
Prove that for a 3-regular graph G and e ∈E(G), the number of Hamil-
tonian circuits containing e is even.
(c) Show that Another Hamiltonian Circuit for 3-regular graphs is in P.
(Nevertheless no polynomial-time algorithm is known for ﬁnding another
Hamiltonian circuit, given a 3-regular graph G and a Hamiltonian circuit
C in G.)
11. Let (X, (Sx)x∈X, x, goal) be a discrete optimization problem with neighbour-
hoods Nx(y) ⊆Sx for y ∈Sx and x ∈X. Suppose that for each x ∈X, an
element of Sx can be found in polynomial time, and for each y ∈Sx we ﬁnd
an element y′ ∈Nx(y) with better cost or decide that none exists. Then the
problem with these neighbourhoods is said to belong to the class PLS (for
polynomial local search). Prove that if a problem in PLS exists for which it is
NP-hard to compute a local optimum for a given instance, then NP = coNP.

532
21. The Traveling Salesman Problem
Hint: Design a nondeterministic algorithm for any coNP-complete problem.
(Johnson, Papadimitriou and Yannakakis [1988])
Note: The TSP with the k-opt and the Lin-Kernighan neighbourhood are PLS-
complete (Krentel [1989], Papadimitriou [1992]), i.e. if one can ﬁnd a local
optimum in polynomial time, one can do so for every problem and neighbour-
hood in PLS (and this would imply another proof of Theorem 4.18 due to the
correctness of the Simplex Algorithm).
12. Show that one can optimize any linear function over the polytope deﬁned by
(21.1), (21.2), (21.3), (21.6).
Hint: Use Theorem 21.23 to reduce the dimension in order to obtain a full-
dimensional polytope. Find a point in the interior and apply Theorem 4.21.
13. Consider the 2-matching inequalities (21.6) in Proposition 21.27. Show that it
is irrelevant whether one requires additionally that F is a matching.
14. Show that the subtour inequalities (21.3), the 2-matching inequalities (21.6)
and the comb inequalities (21.7) are special cases of the clique tree inequalities
(21.8).
15. Prove that there are instances (Kn, c) of the Metric TSP where
H K(Kn,c)
OPT(Kn,c) is
arbitrarily close to 3
4.
Hint: Replace the edges of weight 1 in Figure 21.9 by long paths and consider
the metric closure.
16. Consider the TSP on n cities. For any weight function w : E(Kn) →R+ let
c∗
w be the length of an optimum tour with respect to w. Prove: if L1 ≤c∗
w1 and
L2 ≤c∗
w2 for two weight functions w1 and w2, then also L1 + L2 ≤c∗
w1+w2,
where the sum of the two weight functions is taken componentwise.
17. Let c0 be the cost of the optimum tour for an n-city instance of the Metric
TSP, and let c1 be the cost of the second best tour. Show that
c1 −c0
c0
≤2
n .
(Papadimitriou and Steiglitz [1978])
References
General Literature:
Cook, W.J., Cunningham, W.H., Pulleyblank, W.R., and Schrijver, A. [1998]: Combinato-
rial Optimization. Wiley, New York 1998, Chapter 7
Gutin, G., and Punnen, A.P. [2002]: The Traveling Salesman Problem and Its Variations.
Kluwer, Dordrecht 2002
Jungnickel, D. [1999]: Graphs, Networks and Algorithms. Springer, Berlin 1999, Chapter
14
Lawler, E.L., Lenstra J.K., Rinnooy Kan, A.H.G., and Shmoys, D.B. [1985]: The Traveling
Salesman Problem. Wiley, Chichester 1985
J¨unger, M., Reinelt, G., and Rinaldi, G. [1995]: The traveling salesman problem. In: Hand-
books in Operations Research and Management Science; Volume 7; Network Models

References
533
(M.O. Ball, T.L. Magnanti, C.L. Monma, G.L. Nemhauser, eds.), Elsevier, Amsterdam
1995
Papadimitriou, C.H., and Steiglitz, K. [1982]: Combinatorial Optimization; Algorithms and
Complexity. Prentice-Hall, Englewood Cliffs 1982, Section 17.2, Chapters 18 and 19
Reinelt, G. [1994]: The Traveling Salesman; Computational Solutions for TSP Applications.
Springer, Berlin 1994
Cited References:
Aarts, E., and Lenstra, J.K. [1997]: Local Search in Combinatorial Optimization. Wiley,
Chichester 1997
Applegate, D., Bixby, R., Chv´atal, V., and Cook, W. [1998]: On the solution of traveling
salesman problems. Documenta Mathematica; extra volume ICM 1998; III, 645–656
Applegate, D., Bixby, R., Chv´atal, V., and Cook, W. [2003]: Implementing the Dantzig-
Fulkerson-Johnson algorithm for large traveling salesman problems. Mathematical Pro-
gramming B 97 (2003), 91–153
Applegate, D., Cook, W., and Rohe, A. [2003]: Chained Lin-Kernighan for large traveling
salesman problems. INFORMS Journal on Computing 15 (2003), 82–92
Arora, S. [1998]: Polynomial time approximation schemes for Euclidean traveling salesman
and other geometric problems. Journal of the ACM 45 (1998), 753–782
Boyd, S.C., and Cunningham, W.H. [1991]: Small traveling salesman polytopes. Mathe-
matics of Operations Research 16 (1991), 259–271
Burkard, R.E., De˘ıneko, V.G., and Woeginger, G.J. [1998]: The travelling salesman and
the PQ-tree. Mathematics of Operations Research 23 (1998), 613–623
Carr, R. [1997]: Separating clique trees and bipartition inequalities having a ﬁxed number
of handles and teeth in polynomial time. Mathematics of Operations Research 22 (1997),
257–265
Chalasani, P., Motwani, R., and Rao, A. [1996]: Algorithms for robot grasp and delivery.
Proceedings of the 2nd International Workshop on Algorithmic Foundations of Robotics
(1996), 347–362
Chandra, B., Karloff, H., and Tovey, C. [1999]: New results on the old k-opt algorithm for
the traveling salesman problem. SIAM Journal on Computing 28 (1999), 1998–2029
Christoﬁdes, N. [1976]: Worst-case analysis of a new heuristic for the traveling salesman
problem. Technical Report 388, Graduate School of Industrial Administration, Carnegie-
Mellon University, Pittsburgh 1976
Chv´atal, V. [1973]: Edmonds’ polytopes and weakly hamiltonian graphs. Mathematical
Programming 5 (1973), 29–40
Dantzig, G., Fulkerson, R., and Johnson, S. [1954]: Solution of a large-scale traveling-
salesman problem. Operations Research 2 (1954), 393–410
Frank, A., Triesch, E., Korte, B., and Vygen, J. [1998]: On the bipartite travelling salesman
problem. Report No. 98866, Research Institute for Discrete Mathematics, University of
Bonn, 1998
Frieze, A., Galbiati, G., and Mafﬁoli, F. [1982]: On the worst-case performance of some
algorithms for the asymmetric traveling salesman problem. Networks 12 (1982), 23–39
Garey, M.R., Graham, R.L., and Johnson, D.S. [1976]: Some NP-complete geometric prob-
lems. Proceedings of the 8th Annual ACM Symposium on the Theory of Computing
(1976), 10–22
Gr¨otschel, M., and Padberg, M.W. [1979]: On the symmetric travelling salesman problem.
Mathematical Programming 16 (1979), 265–302
Gr¨otschel, M., and Pulleyblank, W.R. [1986]: Clique tree inequalities and the symmetric
travelling salesman problem. Mathematics of Operations Research 11 (1986), 537–569
Held, M., and Karp, R.M. [1962]: A dynamic programming approach to sequencing prob-
lems. Journal of SIAM 10 (1962), 196–210

534
21. The Traveling Salesman Problem
Held M., and Karp, R.M. [1970]: The traveling-salesman problem and minimum spanning
trees. Operations Research 18 (1970), 1138–1162
Held, M., and Karp, R.M. [1971]: The traveling-salesman problem and minimum spanning
trees; part II. Mathematical Programming 1 (1971), 6–25
Hurkens, C.A.J., and Woeginger, G.J. [2004]: On the nearest neighbour rule for the traveling
salesman problem. Operations Research Letters 32 (2004), 1–4
Hwang, R.Z., Chang, R.C., and Lee, R.C.T. [1993]: The searching over separators strategy
to solve some NP-hard problems in subexponential time. Algorithmica 9 (1993), 398–423
Johnson, D.S., McGeoch, L.A., and Rothberg, E.E. [1996]: Asymptotic experimental anal-
ysis for the Held-Karp traveling salesman bound. Proceedings of the 7th Annual ACM-
SIAM Symposium on Discrete Algorithms (1996), 341–350
item Johnson, D.S., Papadimitriou, C.H., and Yannakakis, M. [1988]: How easy is local
search? Journal of Computer and System Sciences 37 (1988), 79–100
Kaplan, H., Lewenstein, M., Shafrir, N., and Sviridenko, M. [2003]: Approximation algo-
rithms for the asymmetric TSP by decomposing directed regular multigraphs. Proceed-
ings of the 44th Annual IEEE Symposium on Foundations of Computer Science (2003),
56–65. To appear in the Journal of the ACM
Karp, R.M. [1977]: Probabilistic analysis of partitioning algorithms for the TSP in the
plane. Mathematics of Operations Research 2 (1977), 209–224
Karp, R.M., and Papadimitriou, C.H. [1982]: On linear characterizations of combinatorial
optimization problems. SIAM Journal on Computing 11 (1982), 620–632
Krentel, M.W. [1989]: Structure in locally optimal solutions. Proceedings of the 30th Annual
IEEE Symposium on Foundations of Computer Science (1989), 216–221
Land, A.H., and Doig, A.G. [1960]: An automatic method of solving discrete programming
problems. Econometrica 28 (1960), 497–520
Lin, S., and Kernighan, B.W. [1973]: An effective heuristic algorithm for the traveling-
salesman problem. Operations Research 21 (1973), 498–516
Little, J.D.C., Murty, K.G., Sweeny, D.W., and Karel, C. [1963]: An algorithm for the
traveling salesman problem. Operations Research 11 (1963), 972–989
Mitchell, J. [1999]: Guillotine subdivisions approximate polygonal subdivisions: a simple
polynomial-time approximation scheme for geometric TSP, k-MST, and related prob-
lems. SIAM Journal on Computing 28 (1999), 1298–1309
Papadimitriou, C.H. [1977]: The Euclidean traveling salesman problem is NP-complete.
Theoretical Computer Science 4 (1977), 237–244
Papadimitriou, C.H. [1978]: The adjacency relation on the travelling salesman polytope is
NP-complete. Mathematical Programming 14 (1978), 312–324
Papadimitriou, C.H. [1992]: The complexity of the Lin-Kernighan heuristic for the traveling
salesman problem. SIAM Journal on Computing 21 (1992), 450–465
Papadimitriou, C.H., and Steiglitz, K. [1977]: On the complexity of local search for the
traveling salesman problem. SIAM Journal on Computing 6 (1), 1977, 76–83
Papadimitriou, C.H., and Steiglitz, K. [1978]: Some examples of difﬁcult traveling salesman
problems. Operations Research 26 (1978), 434–443
Papadimitriou, C.H., and Vempala, S. [2000]: On the approximability of the traveling sales-
man problem. Proceedings of the 32nd Annual ACM Symposium on the Theory of
Computing (2000), 126–133; to appear on Combinatorica
Papadimitriou, C.H., and Yannakakis, M. [1993]: The traveling salesman problem with
distances one and two. Mathematics of Operations Research 18 (1993), 1–12
Rao, S.B., and Smith, W.D. [1998]: Approximating geometric graphs via “spanners” and
“banyans”. Proceedings of the 30th Annual ACM Symposium on the Theory of Com-
puting (1998), 540–550
Rosenkrantz, D.J. Stearns, R.E., and Lewis, P.M. [1977]: An analysis of several heuristics
for the traveling salesman problem. SIAM Journal on Computing 6 (1977), 563–581

References
535
Sahni, S., and Gonzalez, T. [1976]: P-complete approximation problems. Journal of the
ACM 23 (1976), 555–565
Shmoys, D.B., and Williamson, D.P. [1990]: Analyzing the Held-Karp TSP bound: a mono-
tonicity property with application. Information Processing Letters 35 (1990), 281–285
Triesch, E., Nolles, W., and Vygen, J. [1994]: Die Einsatzplanung von Zementmischern
und ein Traveling Salesman Problem In: Operations Research; Reﬂexionen aus Theorie
und Praxis (B. Werners, R. Gabriel, eds.), Springer, Berlin 1994 [in German]
Woeginger, G.J. [2002]: Exact algorithms for NP-hard problems. OPTIMA 68 (2002), 2–8
Wolsey, L.A. [1980]: Heuristic analysis, linear programming and branch and bound. Math-
ematical Programming Study 13 (1980), 121–134

22. Facility Location
Many economic decisions involve selecting and/or placing certain facilities to
serve given demands efﬁciently. Examples include manufacturing plants, storage
facilities, depots, warehouses, libraries, ﬁre stations, hospitals, base stations for
wireless services (like TV broadcasting or mobile phone service), etc. The prob-
lems have in common that a set of facilities, each with a certain position, has to
be chosen, and the objective is to meet the demand (of customers, users etc.) best.
Facility location problems, which occur also in less obvious contexts, indeed have
numerous applications.
The most widely studied model in discrete facility location is the so-called Un-
capacitated Facility Location Problem, also known as plant location problem
or warehouse location problem. It is introduced in Section 22.1. Although it has
been intensively studied since the 1960s (see, e.g., Stollsteimer [1963], Balinski
and Wolfe [1963], Kuehn and Hamburger [1963], Manne [1964]), no approxima-
tion algorithm was known for this problem until 1997. Since then several quite
different approaches have been used to prove an approximation guarantee. We will
present them in this chapter, and also consider extensions to more general prob-
lems, such as capacitated variants, the k-Median Problem, and the Universal
Facility Location Problem.
22.1 The Uncapacitated Facility Location Problem
The most basic problem, for which we shall present many results, is the Unca-
pacitated Facility Location Problem. It is deﬁned as follows.

538
22. Facility Location
Uncapacitated Facility Location Problem
Instance:
A ﬁnite set D of customers (clients), a ﬁnite set F of (potential)
facilities, a ﬁxed cost fi ∈R+ for opening each facility i ∈F, and
a service cost ci j ∈R+ for each i ∈F and j ∈D.
Task:
Find a subset X of facilities (called open) and an assignment σ :
D →X of the customers to open facilities, such that the sum of
facility costs and service costs

i∈X
fi +

j∈D
cσ( j) j
is minimum.
In many practical applications, service costs come from a metric c on D ∪F
(e.g., when they are proportional to geometric distances or travel times). In this
case we have
ci j + ci′ j + ci′ j′ ≥ci j′
for all i, i′ ∈F and j, j′ ∈D.
(22.1)
Conversely, if this condition holds, we can deﬁne cii := 0 and cii′ := minj∈D(ci j +
ci′ j) for i, i′ ∈F, cj j := 0 and cj j′ := mini∈F(ci j+ci j′) for j, j′ ∈D, and cji := ci j
for j ∈D and i ∈F, and obtain a metric c on D ∪F. Therefore we speak of
metric service costs if (22.1) is satisﬁed. The above problem restricted to instances
with metric service costs is called the Metric Uncapacitated Facility Location
Problem.
Proposition 22.1.
The Metric Uncapacitated Facility Location Problem is
strongly NP-hard.
Proof:
We consider the Minimum Weight Set Cover Problem with unit
weights (which is strongly NP-hard as a consequence of Corollary 15.24). Any
instance (U, S) can be transformed to an instance of the Metric Uncapacitated
Facility Location Problem as follows: let D := U, F := S, fi := 1 for i ∈S,
ci j := 1 for j ∈i ∈S and ci j := 3 for j ∈U \ {i}, i ∈S. Then, for k ≤|S|, the
resulting instance has a solution of cost |D| + k if and only if (U, S) has a set
cover of cardinality k.
2
The number 3 in the above proof can be replaced by any number greater than
1 but not greater than 3 (otherwise (22.1) would be violated). Indeed, a similar
construction shows that metric service costs are necessary to obtain approximation
algorithms: if we set ci j := ∞for j ∈U \ {i} and i ∈S in the above proof, we
see that any approximation algorithm for the Uncapacitated Facility Location
Problem would imply an approximation algorithm for set covering with the same
performance ratio (and there is no constant-factor approximation for set cover-
ing unless P = NP; see Section 16.1). Guha and Khuller [1999] and Sviridenko
[unpublished] extended the above construction to show that a 1.463-factor approx-
imation algorithm for the Metric Uncapacitated Facility Location Problem

22.2 Rounding Linear Programming Solutions
539
(even with service costs 1 and 3 only) would imply P = NP (see Vygen [2005a]
for details).
Conversely, let an instance of the Uncapacitated Facility Location Prob-
lem be given. Setting U := D, S = 2D, and c(D) := mini∈F( fi + 
j∈D ci j) for
D ⊆D yields an equivalent instance of the Minimum Weight Set Cover Prob-
lem. Although this instance has exponential size, we can run the Greedy Algo-
rithm For Set Cover and obtain a solution of cost at most (1+ 1
2 +· · ·+ 1
|D|) times
the optimum in polynomial time (cf. Theorem 16.3), as proposed by Hochbaum
[1982]:
Namely, in each step, we have to ﬁnd a pair (D, i) ∈2D × F with minimum
fi+
j∈D ci j
|D|
, open i, assign all customers in D to i and disregard them henceforth.
Although there are exponentially many choices, it is easy to ﬁnd a best one as it
sufﬁces to consider pairs (Di
k, i) for i ∈F and k ∈{1, . . . , |D|}, where Di
k is the
set of the ﬁrst k customers in a linear order with nondecreasing ci j. Clearly, other
pairs cannot be more effective.
Jain et al. [2003] showed that the performance guarantee of this greedy al-
gorithm is (log n/ log log n) even for metric instances, where n = |D|. Indeed,
before the paper of Shmoys, Tardos and Aardal [1997] no constant-factor approx-
imation algorithm was known even for metric service costs. Since then, this has
changed dramatically. The following sections show different techniques for ob-
taining constant-factor approximations for the Metric Uncapacitated Facility
Location Problem.
An even more restricted problem is given in the special case when facilities and
customers are points in the plane and service costs are geometric distances. Here
Arora, Raghavan and Rao [1998] showed that the problem has an approximation
scheme, i.e. a k-factor approximation algorithm for any k > 1, similarly to the
algorithm in Section 21.2. This result was improved by Kolliopoulos and Rao
[1999], but the algorithm seems to be still too slow for practical purposes.
In the rest of this chapter we assume general metric service costs. For a given
instance D, F, fi, ci j and a given nonempty subset X of facilities, a best assign-
ment σ : D →X satisfying cσ( j) j = mini∈X ci j can be computed easily. Therefore
we will often call a nonempty set X ⊆F a feasible solution, with facility cost
cF(X) := 
i∈X fi and service cost cS(X) := 
j∈D mini∈X ci j. The task is to ﬁnd
a nonempty subset X ⊆F such that cF(X) + cS(X) is minimum.
22.2 Rounding Linear Programming Solutions
The Uncapacitated Facility Location Problem can be formulated as an integer
linear program as follows:

540
22. Facility Location
min

i∈F
fi yi +

i∈F

j∈D
ci jxi j
s.t.
xi j
≤
yi
(i ∈F, j ∈D)

i∈F
xi j
=
1
( j ∈D)
xi j
∈
{0, 1}
(i ∈F, j ∈D)
yi
∈
{0, 1}
(i ∈F)
By relaxing the integrality constraints we get the linear program:
min

i∈F
fi yi +

i∈F

j∈D
ci jxi j
s.t.
xi j
≤
yi
(i ∈F, j ∈D)

i∈F
xi j
=
1
( j ∈D)
xi j
≥
0
(i ∈F, j ∈D)
yi
≥
0
(i ∈F)
(22.2)
This was ﬁrst formulated by Balinski [1965]. The dual of this LP is:
max

j∈D
vj
s.t.
vj −wi j
≤
ci j
(i ∈F, j ∈D)

j∈D
wi j
≤
fi
(i ∈F)
wi j
≥
0
(i ∈F, j ∈D)
(22.3)
LP rounding algorithms solve these linear programs (cf. Theorem 4.18) and round
the fractional solution of the primal LP suitably. Shmoys, Tardos and Aardal [1997]
obtained the ﬁrst constant-factor approximation by this technique:
Shmoys-Tardos-Aardal Algorithm
Input:
An instance (D, F, ( fi)i∈F, (ci j)i∈F, j∈D) of the Uncapacitated Fa-
cility Location Problem.
Output:
A solution X ⊆F and σ : D →X.

22.3 Primal-Dual Algorithms
541
1⃝
Compute an optimum solution (x∗, y∗) to (22.2) and an optimum solution
(v∗, w∗) to (22.3).
2⃝
Let k := 1, X := ∅, and U := D.
3⃝
Let jk ∈U such that v∗
jk is minimum.
Let ik ∈F with x∗
ik jk > 0 and fik minimum. Set X := X ∪{ik}.
Let Nk := { j ∈U : ∃i ∈F : x∗
i jk > 0, x∗
i j > 0}.
Set σ( j) := ik for all j ∈Nk.
Set U := U \ Nk.
4⃝
Set k := k + 1.
If U ̸= ∅then go to 3⃝.
Theorem 22.2.
(Shmoys, Tardos and Aardal [1997]) The above is a 4-factor ap-
proximation algorithm for the Metric Uncapacitated Facility Location Prob-
lem.
Proof:
By complementary slackness (Corollary 3.18), x∗
i j > 0 implies v∗
j −w∗
i j =
ci j, and thus ci j ≤v∗
j . Hence the service cost for customer j ∈Nk is at most
cik j ≤ci j + ci jk + cik jk ≤v∗
j + 2v∗
jk ≤3v∗
j ,
where i is a facility with x∗
i j > 0 and x∗
i jk > 0.
The facility cost fik can be bounded by
fik ≤

i∈F
x∗
i jk fi =

i∈F:x∗
i jk >0
x∗
i jk fi ≤

i∈F:x∗
i jk >0
y∗
i fi.
As x∗
i jk > 0 implies x∗
i jk′ = 0 for k ̸= k′, the total facility cost is at most 
i∈F y∗
i fi.
Summing up, the total cost is 3 
j∈D v∗
j + 
i∈F y∗
i fi, which is at most four
times the LP value, and hence at most four times the optimum.
2
The performance ratio was improved to 1.736 by Chudak and Shmoys [1998]
and to 1.582 by Sviridenko [2002]. Meanwhile, better performance guarantees
have been obtained with simpler and faster algorithms, which do not use a linear
programming algorithm as a subroutine. These will be presented in the next section.
22.3 Primal-Dual Algorithms
Jain and Vazirani [2001] proposed a different approximation algorithm. It is a
primal-dual algorithm in the classical sense: it computes feasible primal and dual
solutions (to the LPs presented in Section 22.2) simultaneously. The primal so-
lution is integral, and the approximation guarantee will follow from approximate
complementary slackness conditions.
One can view the algorithm as continuously raising all dual variables (starting
with zero) and freezing vj when j ∈D is tentatively connected. At any stage,
let wi j := max{0, vj −ci j}. Initially all facilities are closed. We tentatively open
facilities and connect customers when the following two events occur:

542
22. Facility Location
– vj = ci j for some tentatively open facility i and unconnected customer j.
Then set σ( j) := i and freeze vj.
– 
j∈D wi j = fi for some facility i which is not (yet) tentatively open.
Then tentatively open i. For all unconnected customers j ∈D with vj ≥ci j:
set σ( j) := i and freeze vj.
Several events can occur at the same time and are then processed in arbitrary
order. This continues until all customers are connected.
Now let V be the set of facilities that are tentatively open, and let E be the set
of pairs {i, i′} of distinct tentatively open facilities such that there is a customer
j with wi j > 0 and wi′ j > 0. Choose a maximal stable set X in the graph
(V, E). Open the facilities in X. For j ∈D with σ( j) /∈X, reset σ( j) to an open
neighbour of σ( j) in (V, E).
Actually, X can be chosen greedily while tentatively opening facilities. Then
the algorithm can be described more formally as follows. Here Y is the set of
facilities that are not (yet) tentatively open.
Jain-Vazirani Algorithm
Input:
An instance (D, F, ( fi)i∈F, (ci j)i∈F, j∈D) of the Uncapacitated Fa-
cility Location Problem.
Output:
A solution X ⊆F and σ : D →X.
1⃝
Set X := ∅, Y := F and U := D.
2⃝
Set t1 := min{ci j : i ∈F \ Y, j ∈U}.
Set t2 := min{τ : ∃i ∈Y : 
j∈U max{0, τ −ci j} + 
j∈D\U max{0, vj −
ci j} = fi}.
Set t := min{t1, t2}.
3⃝
For i ∈F \ Y and j ∈U with ci j = t do:
Set σ( j) := ϕ(i), vj := t and U := U \ { j}.
4⃝
For i ∈Y with 
j∈U max{0, t −ci j} + 
j∈D\U max{0, vj −ci j} = fi do:
Set Y := Y \ {i}.
If there are i′ ∈X and j ∈D \ U with vj > ci j and vj > ci′ j
then set ϕ(i) := i′
else set ϕ(i) := i and X := X ∪{i}.
For j ∈U with ci j ≤t do: Set σ( j) := ϕ(i), vj := t and U := U \{ j}.
5⃝
If U ̸= ∅then go to 2⃝.
Theorem 22.3.
(Jain and Vazirani [2001])
For metric instances I, the Jain-
Vazirani Algorithm opens a set X of facilities with 3cF(X)+cS(X) ≤3 OPT(I).
In particular, it is a 3-factor approximation algorithm for the Metric Uncapaci-
tated Facility Location Problem. It can be implemented to run in O(m log m)
time, where m = |F||D|.
Proof:
First observe that t is non-decreasing in the course of the algorithm.

22.3 Primal-Dual Algorithms
543
The algorithm computes a primal solution X and σ, and numbers vj, j ∈D,
which together with wi j := max{0, vj −ci j}, i ∈F, j ∈D, constitute a feasible
solution to the dual LP (22.3). Hence 
j∈D vj ≤OPT(I). For each open facility
i, all customers j with wi j > 0 are connected to i, and fi = 
j∈D wi j. Moreover,
we claim that the service cost for each customer j is at most 3(vj −wσ( j) j).
We distinguish two cases. If cσ( j) j = vj −wσ( j) j, this is clear. Otherwise
cσ( j) j > vj and wσ( j) j = 0. This means that ϕ(i) ̸= i when j is connected in 3⃝
or 4⃝, so there is a (closed) facility i ∈F \ (Y ∪X) with ci j ≤vj and a customer
j′ with wi j′ > 0 and wσ( j) j′ > 0, and hence ci j′ = vj′ −wi j′ < vj′ and cσ( j) j′ =
vj′ −wσ( j) j′ < vj′. Note that vj′ ≤vj, because j′ is connected to σ( j) before j.
We conclude that cσ( j) j ≤cσ( j) j′ +ci j′ +ci j < vj′ +vj′ +vj ≤3vj = 3(vj −wσ( j) j).
For the running time we observe that the number of iterations is at most
|D| + 1 as at least one customer is removed from U in each iteration, maybe
except the ﬁrst one if fi = 0 for some i ∈F. The total time for computing t1 in
2⃝, and for 3⃝, is O(m log m) if we sort all ci j once in advance. Next, note that
t2 = min{ ti
2
|Ui| : i ∈Y}, where
ti
2 = fi +

j∈D\U:vj>ci j
(ci j −vj) +

j∈Ui
ci j
and Ui is the set of unconnected customers whose service cost to i is at most the
new value of t. As this number is in fact what we want to compute, we proceed
as follows.
We maintain t2, ti
2 and |Ui| (i ∈Y) throughout; initially t2 = ∞, ti
2 = fi
and |Ui| = 0 for all i. When a new customer j is connected and vj > ci j for
some i ∈Y, then ti
2 is reduced by vj and |Ui| is reduced by one, which may
also imply a change of t2. However, we also have to increase |Ui| by one and
increase ti
2 by ci j (and possibly change t2) when the budget of j reaches ci j for
some i ∈Y and j ∈U. This can be done by changing the deﬁnition of t1 in 2⃝
to t1 := min{ci j : i ∈F, j ∈U} and performing these updates before 5⃝for all
i ∈Y and j ∈U with ci j = t. Note that there are O(m) such updates overall,
each of which takes constant time.
The if-statement in 4⃝can be implemented in O(|D|) time as i′ ∈X, j ∈D\U
and vj > ci′ j implies σ( j) = i′.
2
A better primal-dual algorithm has been proposed by Jain et al. [2003]. One
idea is to relax the feasibility of the dual variables. We interpret the dual variables
as the customers’ budgets, which they use to pay their service costs and contribute
to facility opening costs. We open a facility when the offered contributions sufﬁce
to pay the opening cost. Connected customers do not increase their budget any-
more, but they can still offer a certain amount to other facilities if these are closer
and re-connecting would save service cost. The algorithm proceeds as follows.

544
22. Facility Location
Dual Fitting Algorithm
Input:
An instance (D, F, ( fi)i∈F, (ci j)i∈F, j∈D) of the Uncapacitated Fa-
cility Location Problem.
Output:
A solution X ⊆F and σ : D →X.
1⃝
Let X := ∅and U := D.
2⃝
Set t1 := min{ci j : i ∈X, j ∈U}.
Set t2
:=
min{τ
:
∃i
∈
F \ X
:

j∈U max{0, τ −ci j} +

j∈D\U max{0, cσ( j) j −ci j} = fi}.
Set t := min{t1, t2}.
3⃝
For i ∈F \X with 
j∈U max{0, t−ci j}+
j∈D\U max{0, cσ( j) j −ci j} = fi
do:
Set X := X ∪{i}.
For j ∈D \ U with ci j < cσ( j) j do: Set σ( j) := i.
4⃝
For i ∈X and j ∈U with ci j ≤t do:
Set σ( j) := i, vj := t and U := U \ { j}.
5⃝
If U ̸= ∅then go to 2⃝.
Theorem 22.4.
The above algorithm computes numbers vj, j ∈D, and a feasible
solution X, σ of cost at most 
j∈D vj. It can be implemented to run in O(|F|2|D|)
time.
Proof:
The ﬁrst statement is evident. The running time can be obtained as for
the Jain-Vazirani Algorithm. However, we have to update all ti
2 whenever a
customer is re-connected, i.e. whenever a new facility is opened.
2
We will ﬁnd a number γ such that 
j∈D vj ≤γ ( fi + 
j∈D ci j) for all pairs
(i, D) ∈F × 2D (i.e. ( vj
γ )j∈D is a feasible solution to the dual LP in Exercise 3).
This will imply the performance ratio γ . Of course, we have to assume service
costs to be metric.
Consider i ∈F and D ⊆D with |D| = d. Number the customers in D in the
order in which they are removed from U in the algorithm; w.l.o.g. D = {1, . . . , d}.
We have v1 ≤v2 ≤· · · ≤vd.
Let k ∈D. Note that k is connected at time t = vk in the algorithm, and
consider the time when t is set to vk in 2⃝. For j = 1, . . . , k −1 let
rj,k :=
 ci( j,k) j
if j is connected to i( j, k) ∈F at this time
vk
otherwise, i.e. if vj = vk
.
We now write down valid inequalities for these variables. First, for j =
1, . . . , d −2,
rj, j+1 ≥rj, j+2 ≥· · · ≥rj,d
(22.4)
because the service cost decreases if customers are re-connected. Next, for k =
1, . . . , d,

22.3 Primal-Dual Algorithms
545
k−1

j=1
max{0,rj,k −ci j} +
d

l=k
max{0, vk −cil} ≤fi.
(22.5)
To see this, we consider two cases. If i ∈F \ X at the considered time, (22.5)
holds by the choice of t in 2⃝. Otherwise i was inserted into X before, and at
that time 
j∈U max{0, vj −ci j} + 
j∈D\U max{0, cσ( j) j −ci j} = fi. Later the
left-hand side can only become smaller.
Finally, for 1 ≤j < k ≤d,
vk ≤rj,k + ci j + cik,
(22.6)
which is trivial if rj,k = vk, and otherwise follows from the choice of t1 in 2⃝by
observing that the right-hand side of (22.6) is at least ci( j,k)k due to metric service
costs, and that facility i( j, k) is open at the considered time.
To prove a performance ratio, we consider the following optimization problem
for γF ≥1 and d ∈N. As we want to make a statement for all instances, we
consider fi, ci j and vj ( j = 1, . . . , d) and rj,k (1 ≤j < k ≤d) as variables:
maximize
d
j=1 vj −γF fi
d
j=1 ci j
subject to
vj
≤
vj+1
(1 ≤j < d)
rj,k
≥
rj,k+1
(1 ≤j < k < d)
rj,k + ci j + cik
≥
vk
(1 ≤j < k ≤d)
k−1

j=1
max{0,rj,k −ci j} +
d

l=k
max{0, vk −cil}
≤
fi
(1 ≤k ≤d)
d
j=1 ci j
>
0
fi
≥
0
vj, ci j
≥
0
(1 ≤j ≤d)
rj,k
≥
0
(1 ≤j < k ≤d)
(22.7)
Note that this optimization problem can be easily re-formulated as a linear
program; it is often referred to as the factor-revealing LP. Its optimum values
imply performance guarantees for the Dual Fitting Algorithm:
Theorem 22.5.
Let γF ≥1, and let γS be the supremum of the optimum values
of the factor-revealing LP (22.7) over all d ∈N. Let an instance of the Metric
Uncapacitated Facility Location Problem be given, and let X∗⊆F be any
solution. Then the cost of the solution produced by the Dual Fitting Algorithm
on this instance is at most γFcF(X∗) + γScS(X∗).

546
22. Facility Location
Proof:
The algorithm produces numbers vj and, implicitly, rj,k for all j, k ∈D
with vj ≤vk and j ̸= k. For each pair (i, D) ∈F ×2D, the numbers fi, ci j, vj,rj,k
satisfy conditions (22.4), (22.5) and (22.6) and thus constitute a feasible solution
to (22.7) unless d
j=1 ci j = 0. Hence d
j=1 vj −γF fi ≤γS
d
j=1 ci j (this follows
directly from (22.5) and (22.6) if ci j = 0 for all j ∈D). Choosing σ ∗: D →X∗
such that cσ ∗( j) j = mini∈X∗ci j, and summing over all pairs (i, { j ∈D : σ ∗( j) = i})
(i ∈X∗), we get

j∈D
vj ≤γF

i∈X∗
fi + γS

j∈D
cσ ∗( j) j = γFcF(X∗) + γScS(X∗).
As the solution computed by the algorithm has total cost at most 
j∈D vj, this
proves the theorem.
2
To apply this, we observe:
Lemma 22.6.
Consider the factor-revealing LP (22.7) for some d ∈N.
(a)
For γF = 1, the optimum is at most 2.
(b)
(Jain et al. [2003]) For γF = 1.61, the optimum is at most 1.61.
(c)
(Mahdian, Ye and Zhang [2002]) For γF = 1.11, the optimum is at most 1.78.
Proof:
Here we only prove (a). For a feasible solution we have
d
⎛
⎝fi +
d

j=1
ci j
⎞
⎠
≥
d

k=1
⎛
⎝
k−1

j=1
rj,k +
d

l=k
vk
⎞
⎠
≥
d

k=1
dvk −(d −1)
d

j=1
ci j,
(22.8)
implying that d d
j=1 vj ≤d fi +(2d −1) d
j=1 ci j, i.e. d
j=1 vj ≤fi +2 d
j=1 ci j.
2
The proofs of (b) and (c) are quite long and technical. (a) directly implies
that ( vj
2 )j∈D is a feasible dual solution, and the Dual Fitting Algorithm is a
2-factor approximation algorithm. (b) implies a performance ratio of 1.61. Even
better results can be obtained by combining the Dual Fitting Algorithm with
scaling and greedy augmentation, techniques presented in the next section. For
later use we summarize what follows from Theorem 22.5 and Lemma 22.6:
Corollary 22.7.
Let (γF, γS) ∈{(1, 2), (1.61, 1.61), (1.11, 1.78)}. Let an in-
stance of the Metric Uncapacitated Facility Location Problem be given, and
let ∅̸= X∗⊆F be any solution. Then the cost of the solution produced by the
Dual Fitting Algorithm on this instance is at most γFcF(X∗) + γScS(X∗).
2

22.4 Scaling and Greedy Augmentation
547
22.4 Scaling and Greedy Augmentation
Many approximation results are asymmetric in terms of facility cost and service
cost. Often the service cost can be reduced by opening additional facilities. Indeed,
this can be exploited to improve several performance guarantees.
Proposition 22.8.
Let ∅̸= X, X∗⊆F. Then 
i∈X∗(cS(X) −cS(X ∪{i})) ≥
cS(X) −cS(X∗).
In particular, there exists an i ∈X∗with cS(X)−cS(X∪{i})
fi
≥cS(X)−cS(X∗)
cF(X∗)
.
Proof:
For j ∈D let σ( j) ∈X such that cσ( j) j = mini∈X ci j, and let σ ∗( j) ∈X∗
such that cσ ∗( j) j = mini∈X∗ci j. Then cS(X) −cS(X ∪{i}) ≥
j∈D:σ ∗( j)=i(cσ( j) j −
ci j) for all i ∈X∗. Summation yields the lemma.
2
By greedy augmentation of a set X we mean iteratively picking an element
i ∈F maximizing cS(X)−cS(X∪{i})
fi
and adding it to X until cS(X)−cS(X ∪{i}) ≤fi
for all i ∈F. We need the following lemma:
Lemma 22.9.
(Charikar and Guha [1999]) Let ∅̸= X, X∗⊆F. Apply greedy
augmentation to X, obtaining a set Y ⊇X. Then
cF(Y) + cS(Y) ≤
cF(X) + cF(X∗) ln

max

1, cS(X) −cS(X∗)
cF(X∗)

+ cF(X∗) + cS(X∗).
Proof:
If cS(X) ≤cF(X∗) + cS(X∗), the above inequality evidently holds even
with X in place of Y. Greedy augmentation never increases the cost.
Otherwise, let X = X0, X1, . . . , Xk be the sequence of augmented sets, such
that k is the ﬁrst index for which cS(Xk) ≤cF(X∗) + cS(X∗). By renumbering
facilities we may assume Xi \ Xi−1 = {i} (i = 1, . . . , k). By Proposition 22.8,
cS(Xi−1) −cS(Xi)
fi
≥cS(Xi−1) −cS(X∗)
cF(X∗)
for i = 1, . . . , k. Hence fi ≤cF(X∗) cS(Xi−1)−cS(Xi)
cS(Xi−1)−cS(X∗) (note that cS(Xi−1) > cS(X∗)),
and
cF(Xk) + cS(Xk)
≤
cF(X) + cF(X∗)
k

i=1
cS(Xi−1) −cS(Xi)
cS(Xi−1) −cS(X∗) + cS(Xk).
As the right-hand side increases with increasing cS(Xk) (the derivative is 1 −
cF(X∗)
cS(Xk−1)−cS(X∗) > 0), we do not make the right-hand side smaller if we replace
cS(Xk) by cF(X∗) + cS(X∗). Using x −1 ≥ln x for x > 0, we get

548
22. Facility Location
cF(Xk) + cS(Xk)
≤
cF(X) + cF(X∗)
k

i=1

1 −cS(Xi) −cS(X∗)
cS(Xi−1) −cS(X∗)

+ cS(Xk)
≤
cF(X) −cF(X∗)
k

i=1
ln cS(Xi) −cS(X∗)
cS(Xi−1) −cS(X∗) + cS(Xk)
=
cF(X) −cF(X∗) ln cS(Xk) −cS(X∗)
cS(X) −cS(X∗) + cS(Xk)
=
cF(X) + cF(X∗) ln cS(X) −cS(X∗)
cF(X∗)
+ cF(X∗) + cS(X∗).
2
This can be used to improve several of the previous performance guarantees.
Sometimes it is good to combine greedy augmentation with scaling. We get the
following general result:
Theorem 22.10.
Suppose there are positive constants β, γS, γF and an algorithm
A which, for every instance, computes a solution X such that βcF(X) + cS(X) ≤
γFcF(X∗) + γScS(X∗) for each ∅̸= X∗⊆F. Let δ ≥1
β .
Then scaling facility costs by δ, applying A to the modiﬁed instance, and apply-
ing greedy augmentation to the result with respect to the original instance yields a
solution of cost at most max{ γF
β + ln(βδ), 1 + γS−1
βδ } times the optimum.
Proof:
Let X∗be the set of open facilities of an optimum solution to the
original instance. We have βδcF(X) + cS(X) ≤γFδcF(X∗) + γScS(X∗). If
cS(X) ≤cS(X∗) + cF(X∗), then we have βδ(cF(X) + cS(X)) ≤γFδcF(X∗) +
γScS(X∗) + (βδ −1)(cS(X∗) + cF(X∗)), so X is a solution that costs at most
max{1 + γFδ−1
βδ , 1 + γS−1
βδ } times the optimum. Note that 1 + γFδ−1
βδ
≤γF
β + ln(βδ)
as 1 −1
x ≤ln x for all x > 0.
Otherwise we apply greedy augmentation to X and get a solution of cost at
most
cF(X) + cF(X∗) ln cS(X) −cS(X∗)
cF(X∗)
+ cF(X∗) + cS(X∗)
≤
cF(X) + cF(X∗) ln (γS −1)cS(X∗) + γFδcF(X∗) −βδcF(X)
cF(X∗)
+ cF(X∗) + cS(X∗).
The derivative of this expression with respect to cF(X) is
1 −
βδcF(X∗)
(γS −1)cS(X∗) + γFδcF(X∗) −βδcF(X) ,
which is zero for cF(X) = γF−β
β
cF(X∗) + γS−1
βδ cS(X∗). Hence we get a solution
of cost at most

22.4 Scaling and Greedy Augmentation
549
γF
β + ln(βδ)

cF(X∗) +

1 + γS −1
βδ

cS(X∗).
2
With Corollary 22.7 we can apply this result to the Dual Fitting Algorithm
with β = γF = 1 and γS = 2: by setting δ = 1.76 we obtain an approximation
guarantee of 1.57. With β = 1, γF = 1.11 and γS = 1.78 (cf. Corollary 22.7) we
can do even better:
Corollary 22.11.
(Mahdian, Ye and Zhang [2002]) Multiply all facility costs by
δ = 1.504, apply the Dual Fitting Algorithm, scale back the facility costs, and
apply greedy augmentation. Then this algorithm has an approximation guarantee
of 1.52.
2
This is the best performance ratio that is currently known for the Metric
Uncapacitated Facility Location Problem.
For the special case when all service costs are between 1 and 3, greedy aug-
mentation yields an even better performance ratio. Let α be the solution of the
equation α + 1 = ln 2
α; we have 0.463 ≤α ≤0.4631. A simple calculation shows
that α =
α
α+1 ln 2
α = max{
ξ
ξ+1 ln 2
ξ : ξ > 0}.
Theorem 22.12.
(Guha and Khuller [1999]) Consider the Uncapacitated Fa-
cility Location Problem restricted to instances where all service costs are within
the interval [1, 3]. This problem has a (1 + α + ϵ)-factor approximation algorithm
for every ϵ > 0.
Proof:
Let ϵ > 0, and let k := ⌈1
ϵ ⌉. Enumerate all solutions X ⊆F with
|X| ≤k.
We compute another solution as follows. We ﬁrst open one facility i with
minimum opening cost fi, and then apply greedy augmentation to obtain a solution
Y. We claim that the best solution costs at most 1 + α + ϵ times the optimum.
Let X∗be an optimum solution and ξ = cF(X∗)
cS(X∗) . We may assume that |X∗| > k,
as otherwise we have found X∗above. Then cF({i}) ≤1
k cF(X∗). Moreover, as
the service costs are between 1 and 3, cS({i}) ≤3|D| ≤3cS(X∗).
By Lemma 22.9, the cost of Y is at most
1
k cF(X∗) + cF(X∗) ln

max

1, 2cS(X∗)
cF(X∗)

+ cF(X∗) + cS(X∗)
=
cS(X∗)
ξ
k + ξ ln

max

1, 2
ξ

+ ξ + 1

≤
cS(X∗)(1 + ξ)

1 + ϵ +
ξ
ξ + 1 ln

max

1, 2
ξ

≤
(1 + α + ϵ)(1 + ξ)cS(X∗)
=
(1 + α + ϵ)(cF(X∗) + cS(X∗)).
2

550
22. Facility Location
This performance guarantee seems to be best possible in view of the following:
Theorem 22.13.
If there is an ϵ > 0 and a (1+α−ϵ)-factor approximation algo-
rithm for the Uncapacitated Facility Location Problem restricted to instances
with service costs 1 and 3 only, then P = NP.
This has been shown by Sviridenko [unpublished] (based on results by Feige
[1998] and Guha and Khuller [1999]) and can be found in the survey by Vygen
[2005a].
22.5 Bounding the Number of Facilities
The k-Facility Location Problem is the Uncapacitated Facility Location
Problem with the additional constraint that no more than k facilities may be
opened, where k is a natural number that is part of the instance. A special case,
where facility opening costs are zero, is the well-known k-Median Problem. In
this section we describe an approximation algorithm for the Metric k-Facility
Location Problem.
For problems which become much easier if a certain type of constraints is
omitted, Lagrangean relaxation is a common technique. Here we relax the bound
on the number of open facilities and add a constant λ to each facility opening
cost.
Theorem 22.14.
(Jain and Vazirani [2001])
If there is a constant γS and a
polynomial-time algorithm A, such that for every instance of the Metric Un-
capacitated Facility Location Problem A computes a solution X such that
cF(X) + cS(X) ≤cF(X∗) + γScS(X∗) for every ∅̸= X∗⊆F, then there is a
(2γS)-factor approximation algorithm for the Metric k-Facility Location Prob-
lem with integral data.
Proof:
Let an instance of the Metric k-Facility Location Problem be given.
We assume that service costs are integers within {0, 1, . . . , cmax} and facility open-
ing costs are integers within {0, 1, . . . , fmax}.
First we check if there is a solution with zero cost, and ﬁnd one if it exists.
This is easy; see the proof of Lemma 22.15. Hence we assume that the cost of any
solution is at least 1. Let X∗be an optimum solution (we will use it for analysis
only).
Let A(λ) ⊆F be the solution computed by A for the instance where all facility
opening costs are increased by λ but the constraint on the number of facilities is
omitted. We have cF(A(λ)) + |A(λ)|λ + cS(A(λ)) ≤cF(X∗) + |X∗|λ + γScS(X∗),
and hence
cF(A(λ)) + cS(A(λ)) ≤cF(X∗) + γScS(X∗) + (k −|A(λ)|)λ
(22.9)
for all λ ≥0. If |A(0)| ≤k, then A(0) is a feasible solution costing at most γS
times the optimum, and we are done.

22.5 Bounding the Number of Facilities
551
Otherwise |A(0)| > k, and note that |A( fmax + γS|D|cmax + 1)| = 1 ≤k. Set
λ′ := 0 and λ′′ := fmax + γS|D|cmax + 1, and apply binary search, maintaining
|A(λ′′)| ≤k < |A(λ′)|. After O(log |D| + log fmax + log cmax) iterations, in each
of which we set one of λ′, λ′′ to their arithmetic mean depending on whether
|A( λ′+λ′′
2
)| ≤k or not, we have λ′′ −λ′ ≤
1
|D|2 . (Note that this binary search works
although λ →|A(λ)| is not monotonic in general.)
If |A(λ′′)| = k, then (22.9) implies that A(λ′′) is a feasible solution costing
at most γS times the optimum, and we are done. However, we will not always
encounter such a λ′′, because λ →|A(λ)| is not always monotonic and can jump
by more than 1 (Archer, Rajagopalan and Shmoys [2003] showed how to ﬁx this
by perturbing costs, but were unable to do it in polynomial time).
Thus we consider X := A(λ′) and Y := A(λ′′) and assume henceforth |X| >
k > |Y|. Let α :=
k−|Y|
|X|−|Y| and β :=
|X|−k
|X|−|Y|.
Choose a subset X′ of X with |X′| = |Y| such that mini∈X′ cii′ = mini∈X cii′
for each i′ ∈Y, where we write cii′ := minj∈D(ci j + ci′ j).
We open either all elements of X′ (with probability α) or all elements of Y
(with probability β = 1−α). In addition, we open a set of k−|Y| facilities of X\X′,
chosen uniformly at random. Then the expected facility cost is αcF(X)+βcF(Y).
(Note that X and Y are not necessarily disjoint, and so we may even pay twice
for opening some facilities. Thus αcF(X) + βcF(Y) is in fact an upper bound on
the expected facility cost.)
Let j ∈D, and let i′ be a closest facility in X, and let i′′ be a closest facility
in Y. Connect j to i′ if it is open, else to i′′ if it is open. If neither i′ nor i′′ is
open, connect j to a facility i′′′ ∈X′ minimizing ci′′i′′′.
This yields an expected service cost αci′ j + βci′′ j if i′ ∈X′ and at most
αci′ j + (1 −α)βci′′ j + (1 −α)(1 −β)ci′′′ j
≤
αci′ j + β2ci′′ j + αβ

ci′′ j + min
j′∈D(ci′′ j′ + ci′′′ j′)

≤
αci′ j + β2ci′′ j + αβ(ci′′ j + ci′′ j + ci′ j)
=
α(1 + β)ci′ j + β(1 + α)ci′′ j
if i′ ∈X \ X′.
Thus the total expected service cost is at most
(1 + max{α, β})(αcS(X) + βcS(Y)) ≤

2 −1
|D|

(αcS(X) + βcS(Y)).
Overall, using (22.9), we get an expected cost of at most

2 −1
|D|
 
α(cF(X) + cS(X)) + β(cF(Y) + cS(Y))

≤

2 −1
|D|
 
cF(X∗) + γScS(X∗) + (λ′′ −λ′)(|X| −k)(k −|Y|)
|X| −|Y|


552
22. Facility Location
≤

2 −1
|D|
 
cF(X∗) + γScS(X∗) + (λ′′ −λ′)|X| −|Y|
4

≤

2 −1
|D|
 
cF(X∗) + γScS(X∗) +
1
4|D|

≤

2 −1
|D|
 
1 +
1
4|D|
 
cF(X∗) + γScS(X∗)

≤

2 −
1
2|D|
 
cF(X∗) + γScS(X∗)

and thus at most 2γS(cF(X∗) + cS(X∗)).
Note that the expected cost is easy to compute even under the condition that
a subset Z is opened with probability 1 and k −|Z| randomly chosen facilities
of some other set are opened. Hence one can derandomize this algorithm by the
method of conditional probabilities: ﬁrst open X′ or Y depending on where the
bound on the expected cost is at most (2 −
1
|D|)(α(cF(X) + cS(X)) + β(cF(Y) +
cS(Y))), and then successively open facilities of X \ X′ such that this bound
continues to hold.
2
In particular, by the Dual Fitting Algorithm (Corollary 22.7), we obtain
a 4-factor approximation algorithm for the Metric k-Facility Location Prob-
lem with integral data. The ﬁrst constant-factor approximation algorithm for the
Metric k-Facility Location Problem was due to Charikar et al. [2002].
The running time of the binary search is weakly polynomial and works for
integral data only. However we can make it strongly polynomial by discretizing
the input data:
Lemma 22.15.
For any instance I of the Metric k-Facility Location Problem,
γmax ≥1 and 0 < ϵ ≤1, we can decide whether OPT(I) = 0 and otherwise
generate another instance I ′ in O(|F||D| log(|F||D|)) time, such that all service
and facility costs are integers in {0, 1, . . . , ⌈2γmax(k+|D|)3
ϵ
⌉}, and for each 1 ≤γ ≤
γmax, each solution to I ′ with cost at most γ OPT(I ′) is a solution to I with cost at
most γ (1 + ϵ) OPT(I).
Proof:
Let n := k + |D|. Given an instance I, we ﬁrst compute an upper bound
and a lower bound on OPT(I) differing by a factor of at most 2n2 −1 as follows.
For each B ∈{ fi : i ∈F} ∪{ci j : i ∈F, j ∈D} we consider the bipartite graph
G B := (D ∪F, {{i, j} : i ∈F, j ∈D, fi ≤B, ci j ≤B}).
The smallest B for which the elements of D belong to at most k different con-
nected components of G B, each of which contains at least one facility, is a lower
bound on OPT(I). This number B can be found in O(|F||D| log(|F||D|)) time by
a straightforward variant of Kruskal’s Algorithm for minimum spanning trees.
Moreover, for this B we can choose an arbitrary facility in each connected
component of G B that contains an element of D, and connect each customer with
service cost at most (2|D| −1)B (using the assumption that service costs are

22.6 Local Search
553
metric). Thus OPT(I) ≤kB + (2|D| −1)|D|B < (2n2 −1)B unless B = 0, in
which case we are done.
Thus we can ignore facility and service costs exceeding B′ := 2γmaxn2B. We
obtain I ′ from I by setting each ci j to ⌈min{B′,ci j}
δ
⌉and each fi to ⌈min{B′, fi}
δ
⌉, where
δ = ϵB
n . Now all input numbers are integers in {0, 1, . . . , ⌈2γmaxn3
ϵ
⌉}.
We have
OPT(I ′) ≤OPT(I)
δ
+n = OPT(I) + ϵB
δ
< (2n2 −1)B + ϵB
δ
≤2n2B
δ
=
B′
γmaxδ ,
and thus a solution to I ′ of cost at most γ OPT(I ′) contains no element of cost
⌈B′
δ ⌉, and hence is a solution to I of cost at most
δγ OPT(I ′) ≤γ (OPT(I) + ϵB) ≤γ (1 + ϵ) OPT(I).
2
Corollary 22.16.
There is a strongly polynomial 4-factor approximation algo-
rithm for the Metric k-Facility Location Problem.
Proof:
Apply Lemma 22.15 with γmax = 4 and ϵ =
1
4|D|, and apply Theorem
22.14 with the Dual Fitting Algorithm to the resulting instance. We have
γS = 2 by Corollary 22.7 and get a solution of total cost at most

2 −
1
2|D|
 
1 +
1
4|D|
 
cF(X∗) + γScS(X∗)

≤4

cF(X∗) + cS(X∗)

for any ∅̸= X∗⊆F.
2
22.6 Local Search
As discussed in Section 21.3, local search is a technique that is often applied
successfully in practice, although usually no good approximation guarantees can
be shown. It was therefore a surprise to learn that facility location problems can be
approximated well by local search. This was ﬁrst explored by Korupolu, Plaxton
and Rajaraman [2000] and led to several strong results subsequently. We shall
present some of them in this and the next section.
For the Metric k-Median Problem, local search yields the best known per-
formance ratio. Before presenting this result, we consider the simplest possible
local search algorithm: we start with an arbitrary feasible solution (i.e., a set of
k facilities) and improve it by single swaps. Note that we have to consider only
the service cost, as facility costs are zero in the k-Median Problem. Moreover,
it causes no loss of generality to assume that a solution must contain exactly k
facilities.
Theorem 22.17.
(Arya et al. [2004])
Consider an instance of the Metric k-
Median Problem. Let X be a feasible solution and X∗an optimum solution. If
cS((X \ {x}) ∪{y}) ≥cS(X) for all x ∈X and y ∈X∗, then cS(X) ≤5cS(X∗).

554
22. Facility Location
Proof:
Let us consider optimum assignments σ and σ ∗of the customers to the
k facilities in X and X∗, respectively. We say that x ∈X captures y ∈X∗if
|{ j ∈D : σ( j) = x, σ ∗( j) = y}| > 1
2|{ j ∈D : σ ∗( j) = y}|. Each y ∈X∗is
captured by at most one x ∈X.
Let π : D →D be a bijection such that for all j ∈D:
– σ ∗(π( j)) = σ ∗( j); and
– if σ(π( j)) = σ( j) then σ( j) captures σ ∗( j).
Such a mapping π can be obtained easily by ordering, for each y ∈X∗, the
elements of { j ∈D : σ ∗( j) = y} = { j0, . . . , jt−1} such that customers j with
identical σ( j) are consecutive, and setting π( jk) := jk′, where k′ = (k +⌊t
2⌋) mod
t.
Let us deﬁne a swap to be an element of X × X∗. For a swap (x, y) we call
x the source and y the target. We will deﬁne k swaps such that each y ∈X∗is
the target of exactly one of them.
If an x ∈X captures only one facility y ∈X∗, we consider a swap (x, y). If
there are l such swaps, then there are k −l elements left in X and in X∗. Some
of the remaining elements of X (at most k−l
2 ) may capture at least two facilities
of X∗; we will not consider these. For each remaining facility y ∈X∗we choose
an x ∈X such that x does not capture any facility, and such that each x ∈X is
source of at most two such swaps.
We now analyze the swaps one by one. Consider the swap (x, y), and let
X′ := (X \ {x}) ∪{y}. Then cS(X′) ≥cS(X). Transform σ : D →X to a new
assignment σ ′ : D →X′ by reassigning customers as follows:
Customers j ∈D with σ ∗( j) = y are assigned to y. Customers j ∈D
with σ( j) = x and σ ∗( j) = y′ ∈X∗\ {y} are assigned to σ(π( j)); note that
σ(π( j)) ̸= x as x does not capture y′. For all other customers, the assignment
does not change.
We have
0
≤
cS(X′) −cS(X)
≤

j∈D:σ ∗( j)=y
(cσ ∗( j) j −cσ( j) j) +

j∈D:σ( j)=x,σ ∗( j)̸=y
(cσ(π( j)) j −cσ( j) j)
≤

j∈D:σ ∗( j)=y
(cσ ∗( j) j −cσ( j) j) +

j∈D:σ( j)=x
(cσ(π( j)) j −cσ( j) j)
as cσ(π( j)) j ≥mini∈X ci j = cσ( j) j by deﬁnition of σ.
We now sum over all swaps. Note that each facility of X∗is the target of
exactly one swap, thus the sum of the ﬁrst terms is cS(X∗) −cS(X). Moreover,
each x ∈X is the source of at most two swaps. Hence
0
≤

j∈D
(cσ ∗( j) j −cσ( j) j) + 2

j∈D
(cσ(π( j)) j −cσ( j) j)
≤
cS(X∗) −cS(X) + 2

j∈D
(cσ ∗( j) j + cσ ∗( j)π( j) + cσ(π( j))π( j) −cσ( j) j)

22.6 Local Search
555
=
cS(X∗) −cS(X) + 2

j∈D
(cσ ∗( j) j + cσ ∗(π( j))π( j))
=
cS(X∗) −cS(X) + 4cS(X∗),
because π is a bijection.
2
Thus a local optimum is a 5-approximation. However, this does not make any
statement about the running time to achieve a local optimum; conceivably, the
number of steps to reach a local optimum could be exponential. However, by
discretizing costs we obtain a strongly polynomial running time:
Corollary 22.18.
Let 0 < ϵ ≤1. Then the following is a strongly polynomial (5+
ϵ)-factor approximation algorithm for the Metric k-Median Problem: transform
the instance according to Lemma 22.15 with γmax = 5 and ϵ
5 in place of ϵ, start
with any set of k facilities, and apply swaps decreasing the service cost as long as
possible.
Proof:
As each service cost of the new instance is an integer in {0, 1, . . .,
⌈50(k+|D|)3
ϵ
⌉}, we can apply at most |D|⌈50(k+|D|)3
ϵ
⌉successive swaps each of which
reduces the total service cost.
2
Using multiswaps the approximation guarantee can be improved signiﬁcantly:
Theorem 22.19.
(Arya et al. [2004])
Consider an instance of the Metric k-
Median Problem, and let p ∈N. Let X be a feasible solution and X∗an optimum
solution. If cS((X\A)∪B) ≥cS(X) for all A ⊆X and B ⊆X∗with |A| = |B| ≤p,
then cS(X) ≤(3 + 2
p)cS(X∗).
Proof:
Let σ and σ ∗again be optimum assignments of the customers to the k
facilities in X and X∗, respectively. For each A ⊆X, let C(A) be the set of
facilities in X∗that are captured by A, i.e.
C(A) :=

y ∈X∗: |{ j ∈D : σ( j) ∈A, σ ∗( j) = y}| > 1
2|{ j ∈D : σ ∗( j) = y}|

.
We partition X = A1
.
∪· · ·
.
∪Ar and X∗= B1
.
∪· · ·
.
∪Br as follows:
Let {x ∈X : C({x}) ̸= ∅} =: {x1, . . . , xs} =: ¯X.
Set r := max{s, 1}.
For i = 1 to r −1 do:
Set Ai := {xi}.
While |Ai| < |C(Ai)| do:
Add an element x ∈X \ (A1 ∪· · · ∪Ai ∪¯X) to Ai.
Set Bi := C(Ai).
Set Ar := X \ (A1 ∪· · · ∪Ar−1) and Br := X∗\ (B1 ∪· · · ∪Br−1).
It is clear that this algorithm guarantees |Ai| = |Bi| ≥1 for i = 1, . . . ,r, and
that the sets A1, . . . , Ar are pairwise disjoint and B1, . . . , Br are pairwise disjoint.
Note that adding an element is always possible if |Ai| < |C(Ai)|, because then

556
22. Facility Location
|X \ (A1 ∪· · · ∪Ai ∪¯X)|
=
|X| −|A1| −· · · −|Ai| −|{xi+1, . . . , xr}|
>
|X∗| −|C(A1)| −· · · −|C(Ai)| −|C({xi+1})| −· · · −|C({xr})|
=
|X∗\ (C(A1) ∪· · · ∪C(Ai) ∪C({xi+1}) ∪· · · ∪C({xr}))|
≥
0.
Let π : D →D be a bijection such that for all j ∈D:
– σ ∗(π( j)) = σ ∗( j);
– if σ(π( j)) = σ( j) then σ( j) captures σ ∗( j); and
– if σ( j) ∈Ai and σ(π( j)) ∈Ai for some i ∈{1, . . . ,r}, then Ai captures
σ ∗( j).
Such a mapping π can be obtained almost identically as in the proof of Theorem
22.17.
We now deﬁne a set of swaps (A, B) with |A| = |B| ≤p, A ⊆X and B ⊆X∗.
Each swap will be associated with a positive weight. The swap (A, B) means that
X is replaced by X′ := (X \ A) ∪B; we say that A is the source set and B is the
target set.
For each i ∈{1, . . . ,r} with |Ai| ≤p, we consider the swap (Ai, Bi) with
weight 1. For each i ∈{1, . . . ,r} with |Ai| = q > p, we consider the swap
({x}, {y}) with weight
1
q−1 for each x ∈Ai \ {xi} and y ∈Bi. Each y ∈X∗
appears in the target set of swaps of total weight 1, and each x ∈X appears in
the source set of swaps of total weight at most p+1
p .
We reassign customers as in the case of single swaps. More precisely, for a
swap (A, B) we reassign all j ∈D with σ ∗( j) ∈B to σ ∗( j) and all j ∈D with
σ ∗( j) /∈B and σ( j) ∈A to σ(π( j)). Note that we have B ⊇C(A) for each of
the considered swaps (A, B). Thus, for all j ∈D with σ( j) ∈A and σ ∗( j) /∈B
we have σ(π( j)) /∈A. Therefore we can bound the increase in cost due to the
swap as follows:
0
≤
cS(X′) −cS(X)
≤

j∈D:σ ∗( j)∈B
(cσ ∗( j) j −cσ( j) j) +

j∈D:σ( j)∈A,σ ∗( j)/∈B
(cσ(π( j)) j −cσ( j) j)
≤

j∈D:σ ∗( j)∈B
(cσ ∗( j) j −cσ( j) j) +

j∈D:σ( j)∈A
(cσ(π( j)) j −cσ( j) j)
as cσ(π( j)) j ≥cσ( j) j by deﬁnition of σ. Hence taking the weighted sum over all
swaps yields
0
≤

j∈D
(cσ ∗( j) j −cσ( j) j) + p + 1
p

j∈D
(cσ(π( j)) j −cσ( j) j)
≤
cS(X∗) −cS(X) + p + 1
p

j∈D
(cσ ∗( j) j + cσ ∗( j)π( j) + cσ(π( j))π( j) −cσ( j) j)

22.6 Local Search
557
=
cS(X∗) −cS(X) + p + 1
p

j∈D
(cσ ∗( j) j + cσ ∗(π( j))π( j))
=
cS(X∗) −cS(X) + 2 p + 1
p
cS(X∗),
because π is a bijection.
2
Arya et al. [2004] also showed that this performance guarantee is tight. Like
Corollary 22.18, Lemma 22.15 and Theorem 22.19 imply a (3 + ϵ)-factor approx-
imation algorithm for any ϵ > 0. This is the currently best known approximation
guarantee for the Metric k-Median Problem.
We can apply similar techniques to the Metric Uncapacitated Facility Lo-
cation Problem and obtain a simple approximation algorithm based on local
search:
Theorem 22.20.
(Arya et al. [2004]) Consider an instance of the Metric Unca-
pacitated Facility Location Problem. Let X and X∗be any feasible solutions.
If neither X \ {x} nor X ∪{y} nor (X \ {x}) ∪{y} is better than X for any x ∈X
and y ∈F \ X, then cS(X) ≤cF(X∗) + cS(X∗) and cF(X) ≤cF(X∗) + 2cS(X∗).
Proof:
We use the same notation as in the previous proofs. In particular, let σ
and σ ∗be optimum assignments of the customers to X and X∗, respectively.
The ﬁrst inequality is easily proved by considering, for each y ∈X∗,
the operation of adding y to X, which increases the cost by at most fy +

j∈D:σ ∗( j)=y(cσ ∗( j) j −cσ( j) j). Summing these values up yields that cF(X∗) +
cS(X∗) −cS(X) is nonnegative.
Let again π : D →D be a bijection such that for all j ∈D:
– σ ∗(π( j)) = σ ∗( j);
– if σ(π( j)) = σ( j) then σ( j) captures σ ∗( j) and π( j) = j.
Such a mapping π can be obtained as in the proof of Theorem 22.17 after ﬁxing
π( j) := j for |{ j ∈D : σ ∗( j) = y, σ( j) = x}|−|{ j ∈D : σ ∗( j) = y, σ( j) ̸= x}|
elements j ∈D with σ ∗( j) = y and σ( j) = x for any pair x ∈X, y ∈X∗where
x captures y.
To bound the facility cost of X, let x ∈X, and let Dx := { j ∈D : σ( j) = x}.
If x does not capture any y ∈X∗, we consider dropping x and reassigning each
j ∈Dx to σ(π( j)) ∈X \ {x}. Hence
0 ≤−fx +

j∈Dx
(cσ(π( j)) j −cx j).
(22.10)
If the set C({x}) of facilities captured by x is nonempty, let y ∈C({x}) be
a nearest facility in C({x}) (i.e. minj∈D(cx j + cyj) is minimum). We consider the
addition of each facility y′ ∈C({x}) \ {y}, which increases the cost by at least
zero and at most
fy′ +

j∈Dx:σ ∗( j)=y′,π( j)= j
(cσ ∗( j) j −cx j).
(22.11)

558
22. Facility Location
Moreover, we consider the swap ({x}, {y}). For j ∈Dx we reassign j to
σ(π( j)) if π( j) ̸= j, and to y otherwise.
The new service cost for j ∈Dx is at most cσ(π( j)) j in the ﬁrst case, cσ ∗( j) j if
π( j) = j and σ ∗( j) = y, and
cyj ≤cx j + min
k∈D(cxk + cyk) ≤cx j + min
k∈D(cxk + cσ ∗( j)k) ≤2cx j + cσ ∗( j) j
otherwise, where the second inequality holds because x captures σ ∗( j) if π( j) =
j.
Altogether, the swap from x to y increases the cost by at least zero and at
most
fy −fx −

j∈Dx
cx j +

j∈Dx:π( j)̸= j
cσ(π( j)) j
+

j∈Dx:π( j)= j,σ ∗( j)=y
cσ ∗( j) j +

j∈Dx:π( j)= j,σ ∗( j)̸=y
(2cx j + cσ ∗( j) j).
(22.12)
Adding the nonnegative terms (22.11) and (22.12) yields
0
≤

y′∈C({x})
fy′ −fx +

j∈Dx:π( j)̸= j
(cσ(π( j)) j −cx j)
+

j∈Dx:π( j)= j,σ ∗( j)=y
(cσ ∗( j) j −cx j) +

j∈Dx:π( j)= j,σ ∗( j)̸=y
2cσ ∗( j) j
≤

y′∈C({x})
fy′ −fx +

j∈Dx:π( j)̸= j
(cσ(π( j)) j −cx j) + 2

j∈Dx:π( j)= j
cσ ∗( j) j.
(22.13)
Summing (22.10) and (22.13), respectively, over all x ∈X yields
0
≤

x∈X

y′∈C({x})
fy′ −cF(X) +

j∈D:π( j)̸= j
(cσ(π( j)) j −cσ( j) j) + 2

j∈D:π( j)= j
cσ ∗( j) j
≤
cF(X∗) −cF(X) +

j∈D:π( j)̸= j
(cσ ∗( j) j + cσ ∗( j)π( j) + cσ(π( j))π( j) −cσ( j) j)
+2

j∈D:π( j)= j
cσ ∗( j) j
=
cF(X∗) −cF(X) + 2cS(X∗).
2
With Lemma 22.15 this implies a (3 + ϵ)-factor approximation algorithm for
any ϵ > 0. Combining this with Theorem 22.10, we get a 2.375-approximation
algorithm (Exercise 11). Charikar and Guha [1999] proved the same approximation
guarantee for a very similar local search algorithm.

22.7 Capacitated Facility Location Problems
559
22.7 Capacitated Facility Location Problems
A main advantage of local search algorithms is their ﬂexibility; they can be applied
to arbitrary cost functions and even in the presence of complicated additional
constraints. For facility location problems with hard capacities, local search is the
only technique that is currently known to yield an approximation guarantee.
There are several capacitated facility location problems. Mahdian and P´al
[2003] deﬁned the following general problem, which contains several important
special cases:
Universal Facility Location Problem
Instance:
Finite sets D of customers and F of potential facilities; a metric c on
V := D ∪F, i.e. distances ci j ≥0 (i, j ∈V ) with cii = 0, ci j = cji
and ci j + cjk ≥cik for all i, j, k ∈V ; a demand dj ≥0 for each
j ∈D; and for each i ∈F a cost function fi : R+ →R+ ∪{∞}
which is left-continuous and non-decreasing.
Task:
Find xi j ∈R+ for i ∈F and j ∈D, with 
i∈F xi j = dj for all
j ∈D, such that c(x) := cF(x) + cS(x) is minimum, where
cF(x) :=

i∈F
fi
 
j∈D
xi j

and
cS(x) :=

i∈F

j∈D
ci jxi j.
fi(z) can be interpreted as the cost of installing capacity z at facility i. We
have to specify how the functions fi are given. We assume an oracle that, for
each i ∈F, u, c ∈R+ and t ∈R, computes fi(u) and max{δ ∈R : u + δ ≥
0, fi(u + δ) −fi(u) + c|δ| ≤t}. This is a natural assumption as this oracle can
be implemented trivially for the most important special cases of the Universal
Facility Location Problem. These are:
– the Metric Uncapacitated Facility Location Problem. Here dj = 1 ( j ∈
D), and fi(0) = 0 and fi(z) = ti for some ti ∈R+ and all z > 0 (i ∈F).
– the Metric Capacitated Facility Location Problem. Here fi(0) = 0,
fi(z) = ti for 0 < z ≤ui and fi(z) = ∞for z > ui, where ui, ti ∈R+
(i ∈F).
– the Metric Soft-Capacitated Facility Location Problem. Here dj = 1
( j ∈D), and fi(z) = ⌈z
ui ⌉ti for some ui ∈N, ti ∈R+ and all z ≥0 (i ∈F).
Note that in the ﬁrst and third case there is always an optimum integral solution.
While this is trivial in the ﬁrst case, it follows easily in the third case by taking
an arbitrary optimum solution y and applying the following observation to dj = 1
for j ∈D and zi = max{z : fi(z) ≤fi(
j∈D yi j)} ∈Z+ for i ∈F:
Proposition 22.21.
Let D and F be ﬁnite sets, dj ≥0 ( j ∈D), zi ≥0 (i ∈F)
and ci j ≥0 (i ∈F, j ∈D) such that 
j∈D dj ≤
i∈F zi. Then an optimum
solution to

560
22. Facility Location
min
⎧
⎨
⎩

i∈F, j∈D
ci jxi j : x ≥0,

i∈F
xi j = dj ( j ∈D),

j∈D
xi j ≤zi (i ∈F)
⎫
⎬
⎭
(22.14)
can be found in O(n3 log n) time, where n = |D|+|F|. If all dj and zi are integral,
then there exists an integral optimum solution.
Proof:
(22.14) is equivalent to the instance (G, b, c) of the HitchcockProblem,
deﬁned by G := (A
.
∪B, A × B), A := {vj : j ∈D}
.
∪{0}, B := {wi : i ∈F},
b(vj) := dj for j ∈D, b(wi) = −zi for i ∈F, b(0) := 
i∈F zi −
j∈D dj,
c(vj, wi) := ci j and c(0, wi) := 0 for i ∈F and j ∈D. Thus (22.14) can be
solved in O(n3 log n) time by Theorem 9.16. If b is integral, the Minimum Mean
Cycle-Cancelling Algorithm and the Successive Shortest Path Algorithm
compute integral optimum solutions.
2
The soft-capacitated version can be reduced quite easily to the uncapacitated
one, by a technique that has been proposed originally by Jain and Vazirani [2001]:
Theorem 22.22.
(Mahdian, Ye and Zhang [2002]) Let γF and γS be constants
and A a polynomial-time algorithm such that, for every instance of the Metric
Uncapacitated Facility Location Problem, A computes a solution X with
cF(X) + cS(X) ≤γFcF(X∗) + γScS(X∗) for each ∅̸= X∗⊆F. Then there is
a (γF + γS)-factor approximation algorithm for the Metric Soft-Capacitated
Facility Location Problem.
Proof:
Consider an instance I = (F, D, (ci j)i∈F, j∈D, ( fi)i∈F) of the Metric
Soft-Capacitated Facility Location Problem, where fi(z) = ⌈z
ui ⌉ti for i ∈F
and z ∈R+. We transform it to the instance I ′ = (F, D, ( f ′
i )i∈F, (c′
i j)i∈F, j∈D) of
the Metric Uncapacitated Facility Location Problem by setting f ′
i := ti and
c′
i j := ci j + ti
ui for i ∈F and j ∈D. (Note that c′ is metric whenever c is metric.)
We apply A to I ′ and ﬁnd a solution X ∈F and an assignment σ : D →X.
Set xi j := 1 if σ( j) = i and xi j := 0 otherwise. If σ ∗: D →F is an optimum
solution to I, where X∗:= {i ∈F : ∃j ∈D : σ ∗( j) = i} is the set of facilities
opened at least once,
cF(x) + cS(x)
=

i∈X
0|{ j ∈D : σ( j) = i}|
ui
1
ti +

j∈D
cσ( j) j
≤

i∈X
ti +

j∈D
c′
σ( j) j
≤
γF

i∈X∗
ti + γS

j∈D
c′
σ ∗( j) j
≤
(γF + γS)

i∈X∗
0|{ j ∈D : σ ∗( j) = i}|
ui
1
ti + γS

j∈D
cσ ∗( j) j.
2

22.8 Universal Facility Location
561
Corollary 22.23.
The Metric Soft-Capacitated Facility Location Problem
has a 2.89-factor approximation algorithm.
Proof:
Apply Theorem 22.22 to the Dual Fitting Algorithm (Corollary
22.7(c)); here γF = 1.11 and γS = 1.78.
2
See Exercise 10 for a better approximation guarantee.
When dealing with hard capacities, we have to allow the demand of customers
to be split, i.e. assigned to multiple open facilities: if we do not allow splitting,
we cannot expect any result as even deciding whether a feasible solution exists at
all is NP-complete (this contains the Partition problem; cf. Corollary 15.28).
The ﬁrst approximation algorithm for the Metric Capacitated Facility Lo-
cation Problem was due to P´al, Tardos and Wexler [2001], extending an earlier
result for a special case by Korupolu, Plaxton and Rajaraman [2000]. The ap-
proximation guarantee was then improved to 5.83 by Zhang, Chen and Ye [2004].
For the special case of uniform facility opening costs, Levi, Shmoys and Swamy
[2004] obtained a 5-factor approximation algorithm by rounding an LP relaxation.
The work by P´al, Tardos and Wexler [2001] has been generalized to the so-
called Universal Facility Location Problem by Mahdian and P´al [2003]. They
obtained a 7.88-factor approximation algorithm. In the next section we present
a local search algorithm that yields a performance guarantee of 6.702 for the
Universal Facility Location Problem. But let us ﬁrst note the following.
Lemma 22.24.
(Mahdian and P´al [2003]) Every instance of the Universal Fa-
cility Location Problem has an optimum solution.
Proof:
If there is no solution with ﬁnite cost, any solution is optimum. Other-
wise let (xi)i∈N be a sequence of solutions whose costs approach the inﬁmum
c∗∈R+ of the set of costs of feasible solutions. As this sequence is bounded,
there is a subsequence (xij)j∈N converging to some x∗. x∗is feasible. As all
fi are left-continuous and non-decreasing, we have c(x∗) = c(limj→∞xij) ≤
limj→∞c(xij) = c∗, i.e. x∗is optimum.
2
22.8 Universal Facility Location
In this section, based on Vygen [2005b], we present a local search algorithm for the
Universal Facility Location Problem. It uses two operations. First, for t ∈F
and δ ∈R+ we consider the operation Add(t, δ), which consists in replacing the
current feasible solution x by an optimum solution y to the following problem:
min

cS(y)
:
yi j ≥0 (i ∈F, j ∈D),

i∈F
yi j = dj ( j ∈D),

j∈D
yi j ≤

j∈D
xi j (i ∈F \ {t}),

j∈D
yt j ≤

j∈D
xt j + δ

.
(22.15)

562
22. Facility Location
We denote by cx(t, δ) := cS(y) −cS(x) + ft(
j∈D xt j + δ) −ft(
j∈D xt j) the
estimated cost of this operation; this is an upper bound on c(y) −c(x).
Lemma 22.25.
(Mahdian and P´al [2003]) Let ϵ > 0. Let x be a feasible solution
to a given instance, and let t ∈F. Then there is an algorithm with running time
O(|V |3 log |V |ϵ−1) that ﬁnds a δ ∈R+ with cx(t, δ) ≤−ϵc(x) or decides that no
δ ∈R+ exists for which cx(t, δ) ≤−2ϵc(x).
Proof:
We may assume that c(x) > 0. Let C := {νϵc(x) : ν ∈Z+, ν ≤⌈1
ϵ ⌉}.
For each γ ∈C let δγ be the maximum δ ∈R+ for which ft(
j∈D xt j + δ) −
ft(
j∈D xt j) ≤γ . We compute cx(t, δγ ) for all γ ∈C.
Suppose there is a δ ∈R+ with cx(t, δ) ≤−2ϵc(x). Then consider
γ := ϵc(x)
⎡
⎢⎢⎢
1
ϵc(x)
⎛
⎝ft
⎛
⎝
j∈D
xt j + δ
⎞
⎠−ft
⎛
⎝
j∈D
xt j
⎞
⎠
⎞
⎠
⎤
⎥⎥⎥
∈C.
Note that δγ ≥δ and hence cx(t, δγ ) < cx(t, δ) + ϵc(x) ≤−ϵc(x).
The running time is dominated by solving |C| problems of the type (22.15).
Hence the running time follows from Proposition 22.21.
2
If there is no sufﬁciently proﬁtable Add operation, the service cost can be
bounded. The following result is essentially due to P´al, Tardos and Wexler [2001]:
Lemma 22.26.
Let ϵ > 0, and let x, x∗be feasible solutions to a given instance,
and let cx(t, δ) ≥−ϵ
|F|c(x) for all t ∈F and δ ∈R+. Then cS(x) ≤cF(x∗) +
cS(x∗) + ϵc(x).
Proof:
Consider the (complete bipartite) digraph G = (D
.
∪F, (D × F) ∪(F ×
D)) with edge weights c(( j, i)) := ci j and c((i, j)) := −ci j for i ∈F and
j ∈D. Let b(i) := 
j∈D(xi j −x∗
i j) for i ∈F, S := {i ∈F : b(i) > 0} and
T := {i ∈F : b(i) < 0}.
Deﬁne a b-ﬂow g : E(G) →R+ by g(i, j) := max{0, xi j −x∗
i j} and g( j, i) :=
max{0, x∗
i j −xi j} for i ∈F, j ∈D.
Write g as the sum of bt-ﬂows gt for t ∈T , where bt(t) = b(t), bt(v) = 0
for v ∈T \ {t} and 0 ≤bt(v) ≤b(v) for v ∈V (G) \ T . (This can be done by
standard ﬂow decomposition techniques.)
For each t ∈T , gt deﬁnes a feasible way to reassign customers to t, i.e. a new
solution xt deﬁned by xt
i j := xi j + gt( j, i) −gt(i, j) for i ∈F, j ∈D. We have
cS(xt) = cS(x) + 
e∈E(G) c(e)gt(e) and hence
cx(t, −b(t)) ≤

e∈E(G)
c(e)gt(e) + ft
 
j∈D
x∗
t j

−ft
 
j∈D
xt j

.
If the left-hand side is at least −ϵ
|F|c(x) for each t ∈T , summation yields

22.8 Universal Facility Location
563
−ϵc(x)
≤

e∈E(G)
c(e)g(e) +

t∈T
ft
 
j∈D
x∗
t j

≤

e∈E(G)
c(e)g(e) + cF(x∗)
=
cS(x∗) −cS(x) + cF(x∗).
2
We will now describe the second type of operation. Let x be a feasible solution
for a given instance of the Universal Facility Location Problem. Let A be
an arborescence with V (A) ⊆F and δ ∈x
A := {δ ∈RV (A) : 
j∈D xi j + δi ≥
0 for all i ∈V (A), 
i∈V (A) δi = 0}.
Then we consider the operation Pivot(A, δ), which consists in replacing x by
a solution x′ with 
j∈D x′
i j = 
j∈D xi j + δi for i ∈V (A), 
j∈D x′
i j = 
j∈D xi j
for i ∈F \ V (A) and c(x′) ≤c(x) + cx(A, δ), where cx(A, δ) := 
i∈V (A) cx
A,i(δ)
and
cx
A,i(δ) := fi
 
j∈D
xi j + δi

−fi
 
j∈D
xi j

+


l∈A+
i
δl
cip(i)
for i ∈V (A). Here A+
i
denotes the set of vertices reachable from i in A, and
p(i) is the predecessor of i in A (and arbitrary if i is the root). Such an x′ can be
constructed easily by moving demand along the edges in A in reverse topological
order. Note that the orientation of A is irrelevant for cx(A, δ) and used only to
simplify notation.
The operation will be performed if its estimated cost cx(A, δ) is sufﬁciently
negative. This guarantees that the resulting local search algorithm stops after a
polynomial number of improvement steps. We call 
i∈V (A)


l∈A+
i δl
cip(i) the
estimated routing cost of Pivot(A, δ).
We now show how to ﬁnd an improving Pivot operation unless we are at an
approximate local optimum:
Lemma 22.27.
(Vygen [2005b]) Let ϵ > 0 and A an arborescence with V (A) ⊆
F. Let x be a feasible solution. Then there is an algorithm with running time
O(|F|4ϵ−3) that ﬁnds a δ ∈x
A with cx(A, δ) ≤−ϵc(x) or decides that no δ ∈x
A
exists for which cx(A, δ) ≤−2ϵc(x).
Proof:
Number V (A) = {1, . . . , n} in reverse topological order, i.e. for all
(i, j) ∈E(A) we have i > j. For k ∈V (A) with (p(k), k) ∈E(A) let
B(k) := {i < k : (p(k), i) ∈E(A)} be the set of smaller siblings of k, and let
B(k) := ∅if k is the root of A. Let Ik := 
l∈B(k)∪{k} A+
l , b(k) := max({0}∪B(k))
and s(k) := max({0} ∪(A+
k \ {k})).
Let C := {ν ϵ
n c(x) : ν ∈Z, −⌈n
ϵ ⌉−n ≤ν ≤⌈n
ϵ ⌉+ n}. We compute the table
(T x
A(k, γ ))k∈{0,...,n},γ ∈C, deﬁned as follows. Let T x
A(0, 0) := 0, T x
A(0, γ ) := ∅for
all γ ∈C \ {0}, and for k = 1, . . . , n let T x
A(k, γ ) be an optimum solution δ ∈RIk
of

564
22. Facility Location
max
 
i∈Ik
δi
:
γ ′ ∈C, T x
A(b(k), γ ′) ̸= ∅, δi = (T x
A(b(k), γ ′))i for i ∈

l∈B(k)
A+
l ,
γ ′′ ∈C, T x
A(s(k), γ ′′) ̸= ∅, δi = (T x
A(s(k), γ ′′))i for i ∈A+
k \ {k},

j∈D
xkj + δk ≥0, γ ′ + γ ′′ + cx
A,k(δ) ≤γ

if the set over which the maximum is taken is nonempty, and T x
A(k, γ ) := ∅
otherwise.
Roughly, −
i∈Ik(T x
A(k, γ ))i is the minimum excess we get at the predecessor
p(k) of k when moving demand from each vertex in Ik to its respective predecessor
or vice versa, at a total rounded estimated cost of at most γ .
Note that T x
A(k, 0) ̸= ∅for k = 0, . . . , n. Thus we can choose the minimum
γ ∈C such that T x
A(n, γ ) ̸= ∅and n
i=1(T x
A(n, γ ))i ≥0. Then we choose
δ ∈x
A such that δi = (T x
A(n, γ ))i or 0 ≤δi ≤(T x
A(n, γ ))i for all i = 1, . . . , n
and | 
l∈A+
i δl| ≤| 
l∈A+
i (T x
A(n, γ ))l| for all i = 1, . . . , n. This can be done by
setting δ := T x
A(n, γ ) and repeatedly decreasing δi for the maximum i for which
δi > 0 and 
l∈A+
k δl > 0 for all vertices k on the path from n to i in A. Note
that the property cx(A, δ) ≤γ is maintained. It remains to show that γ is small
enough.
Suppose there exists an operation Pivot(A, δ) with cx(A, δ) ≤−2ϵc(x). As
cx
A,i(δ) ≥−fi(
j∈D xi j) ≥−c(x) for all i ∈V (A), this also implies cx
A,i(δ) <
cF(x) ≤c(x). Hence γi := ⌈cx
A,i(δ)
n
ϵc(x)⌉ϵc(x)
n
∈C for i = 1, . . . , n, and 
i∈I γi ∈
C for all I ⊆{1, . . . , n}. Then an easy induction shows 
i∈Ik(T x
A(k, 
l∈Ik γl))i ≥

i∈Ik δi for k = 1, . . . , n. Hence we ﬁnd a pivot operation with estimated cost at
most n
i=1 γi < cx(A, δ) + ϵc(x) ≤−ϵc(x).
The running time can be estimated as follows. We have to compute n|C|
table entries, and for each entry T x
A(k, γ ) we try all values of γ ′, γ ′′ ∈C. This
yields values δi for i ∈Ik \ {k}, and the main step is to compute the maximum
δk for which γ ′ + γ ′′ + cx
A,k(δ) ≤γ . This can be done directly with the oracle
that we assumed for the functions fi, i ∈F. The ﬁnal computation of δ from
T x
A(n, γ ), γ ∈C, is easily done in linear time. Hence the overall running time is
O(n|C|3) = O(|F|4ϵ−3).
2
We consider Pivot(A, δ) for special arborescences: stars and comets. A is
called the star centered at v if A = (F, {(v, w) : w ∈F \{v}}) and the comet with
center v and tail (t, s) if A = (F, {(t, s)} ∪{(v, w) : w ∈F \ {v, s}}) and v, t, s
are distinct elements of F. Note that there are less than |F|3 stars and comets.
We will now show that an (approximate) local optimum has low facility cost.
Lemma 22.28.
Let x, x∗be feasible solutions to a given instance, and let cx(A, δ)
≥−ϵ
|F|c(x) for all stars and comets A and δ ∈x
A. Then cF(x) ≤4cF(x∗) +
2cS(x∗) + 2cS(x) + ϵc(x).
Proof:
We use the notation of Lemma 22.26 and consider the following instance
of the Hitchcock Problem:

22.8 Universal Facility Location
565
minimize

s∈S,t∈T
cst y(s, t)
subject to

t∈T
y(s, t) = b(s)
for all s ∈S,

s∈S
y(s, t) = −b(t)
for all t ∈T,
y(s, t) ≥0
for all s ∈S, t ∈T.
(22.16)
It is well-known from min-cost ﬂow theory that there exists an optimum so-
lution y : S × T →R+ of (22.16) such that F := (S ∪T, {{s, t} : y(s, t) > 0}) is
a forest (this is proved just like Theorem 9.6; cf. Exercise 13 of Chapter 9).
As (bt(s))s∈S,t∈T is a feasible solution of (22.16), we have

s∈S,t∈T
cst y(s, t)
≤

s∈S,t∈T
cstbt(s)
=

s∈S,t∈T
cst(gt(δ+(s)) −gt(δ−(s)))
≤

e∈E(G)
|c(e)|g(e)
≤
cS(x∗) + cS(x).
(22.17)
We will now deﬁne at most |F| Pivot operations. We say that an operation
Pivot(A, δ) closes s ∈S (with respect to x and x∗) if 
j∈D xsj > 
j∈D xsj +δs =

j∈D x∗
sj. We say that it opens t ∈T if 
j∈D xt j < 
j∈D xt j + δt ≤
j∈D x∗
t j.
Over all operations that we are going to deﬁne, each s ∈S will be closed once,
and each t ∈T will be opened at most four times. Moreover, the total estimated
routing cost will be at most 2 
s∈S,t∈T cst y(s, t). Thus the total estimated cost of
the operations will be at most 4cF(x∗)+2cS(x∗)+2cS(x)−cF(x). This will prove
the lemma.
To deﬁne the operations, orient F as a branching B each of whose components
is rooted at an element of T . Write y(e) := y(s, t) if e ∈E(B) has endpoints
s ∈S and t ∈T . A vertex v ∈V (B) is called weak if y(δ+
B (v)) > y(δ−
B (v)) and
strong otherwise. We denote by +
s (v), +
w(v) and +(v) the set of strong, weak,
and all children of v ∈V (B) in B, respectively.
Let t ∈T , and let +
w(t) = {w1, . . . , wk} be the weak children of t ordered such
that r(w1) ≤· · · ≤r(wk), where r(wi) := max

0, y(wi, t) −
t′∈+
w(wi) y(wi, t′)

.
Moreover, order +
s (t) = {s1, . . . , sl} such that y(s1, t) ≥· · · ≥y(sl, t).
Let us ﬁrst assume k > 0. For i = 1, . . . , k −1 consider a Pivot with the star
centered at wi, routing
– at most 2y(wi, t′) units of demand from wi to each weak child t′ of wi,
– y(wi, t′) units from wi to each strong child t′ of wi, and
– r(wi) units from wi to +
s (wi+1),
closing wi and opening a subset of +(wi) ∪+
s (wi+1). The estimated routing
cost is at most

566
22. Facility Location

t′∈+
w(wi)
cwit′2y(wi, t′) +

t′∈+
s (wi)
cwit′ y(wi, t′) + ctwir(wi)
+ ctwi+1r(wi+1) +

t′∈+
s (wi+1)
cwi+1t′ y(wi+1, t′),
as r(wi) ≤r(wi+1) ≤
t′∈+
s (wi+1) y(wi+1, t′).
To deﬁne more Pivot operations related to t, we distinguish three cases.
Case 1: t is strong or l = 0. Then consider:
• a Pivot with the star centered at wk, routing
– y(wk, t′) units of demand from wk to each child t′ of wk, and
– y(wk, t) units from wk to t,
closing wk and opening t and the children of wk, and
• a Pivot with the star centered at t, routing
– at most 2y(s, t) units from each strong child s of t to t,
closing the strong children of t and opening t. (In the case l = 0 the second Pivot
can be omitted.)
Case 2: t is weak, l ≥1, and y(wk, t) + y(s1, t) ≥l
i=2 y(si, t). Then consider:
• a Pivot operation with the star centered at wk, routing
– y(wk, t′) units of demand from wk to each child t′ of wk, and
– y(wk, t) units from wk to t,
closing wk, opening the children of wk, and opening t,
• a Pivot operation with the star centered at s1, routing
– y(s1, t′) units from s1 to each child t′ of s1, and
– y(s1, t) units from s1 to t,
closing s1, opening the children of s1, and opening t, and
• a Pivot operation with the star centered at t, routing
– at most 2y(si, t) units from si to t for i = 2, . . . ,l,
closing s2, . . . , sl and opening t.
Case 3: t is weak, l ≥1, and y(wk, t) + y(s1, t) < l
i=2 y(si, t). Then consider:
• a Pivot operation with the comet with center wk and tail (t, s1), routing
– y(wk, t′) units of demand from wk to each child t′ of wk,
– y(wk, t) units from wk to t, and
– at most 2y(s1, t) units from s1 to t,
closing wk and s1 and opening t and the children of wk,
• a Pivot operation with the star centered at t, routing
– at most 2y(si, t) units from si to t for each odd element i of {2, . . . ,l},

22.8 Universal Facility Location
567
closing the odd elements of {s2, . . . , sl} and opening t, and
• a Pivot operation with the star centered at t, routing
– at most 2y(si, t) units from si to t for each even element i of {2, . . . ,l},
closing the even elements of {s2, . . . , sl} and opening t.
In the case k = 0 we consider the same Pivot operations, except that the ﬁrst
one is omitted in Case 1 and 2 (where y(w0, t) := 0) and replaced by a Pivot
with the star centered at t in Case 3, routing at most 2y(s1, t) units from s1 to t,
closing s1 and opening t.
We collect all these Pivot operations for all t ∈T . Then, altogether, we
have closed each s ∈S once and opened each t ∈T at most four times, with
a total estimated routing cost of at most 2 
{s,t}∈E(F) cst y(s, t), which is at most
2cS(x∗) + 2cS(x) by (22.17). If none of the operations has an estimated cost of
less than −ϵ
|F|c(x), we have −ϵc(x) ≤−cF(x) + 4cF(x∗) + 2cS(x∗) + 2cS(x), as
required.
2
From the previous results we can conclude:
Theorem 22.29.
Let 0 < ϵ ≤1, and let x, x∗be feasible solutions to a given
instance, and let cx(t, δ) > −
ϵ
8|F|c(x) for t ∈F and δ ∈R+ and cx(A, δ) >
−
ϵ
8|F|c(x) for all stars and comets A and δ ∈x
A. Then c(x) ≤(1 + ϵ)(7cF(x∗) +
5cS(x∗)).
Proof:
By Lemma 22.26 we have cS(x) ≤cF(x∗) + cS(x∗) + ϵ
8c(x), and by
Lemma 22.28 we have cF(x) ≤4cF(x∗) + 2cS(x∗) + 2cS(x) + ϵ
8c(x). Hence
c(x) = cF(x) + cS(x) ≤7cF(x∗) + 5cS(x∗) + ϵ
2c(x), implying c(x) ≤(1 +
ϵ)(7cF(x∗) + 5cS(x∗)).
2
We ﬁnally apply a standard scaling technique and obtain the main result of
this section:
Theorem 22.30.
(Vygen [2005b])
For every ϵ > 0 there is a polynomial-time
(
√
41+7
2
+ϵ)-approximation algorithm for the Universal Facility Location Prob-
lem.
Proof:
We may assume ϵ ≤1
3. Let β :=
√
41−5
2
≈0.7016. Set f ′
i (z) := β fi(z)
for all z ∈R+ and i ∈F, and consider the modiﬁed instance.
Let x be any initial feasible solution. Apply the algorithms of Lemma 22.25
and Lemma 22.27 with
ϵ
16|F| in place of ϵ. They either ﬁnd an Add or Pivot
operation that reduces the cost of the current solution x by at least
ϵ
16|F|c(x), or
they conclude that the prerequisites of Theorem 22.29 are fulﬁlled.
If x is the resulting solution, c′
F and cF denote the facility cost of the modiﬁed
and original instance, respectively, and x∗is any feasible solution, then cF(x) +
cS(x) = 1
β c′
F(x) + cS(x) ≤1
β (6c′
F(x∗) + 4cS(x∗) + 3ϵ
8 c(x)) + c′
F(x∗) + cS(x∗) +
ϵ
8c(x) ≤(6 + β)cF(x∗) + (1 + 4
β )cS(x∗) + 3ϵ
4 c(x) = (6 + β)(cF(x∗) + cS(x∗)) +
3ϵ
4 c(x). Hence c(x) ≤(1 + ϵ)(6 + β)c(x∗).

568
22. Facility Location
Each iteration reduces the cost by a factor of at least
1
1−
ϵ
16|F| , hence after
1
−log(1−
ϵ
16|F| ) < 16|F|
ϵ
iterations the cost reduces at least by a factor of 2 (note that
log x < x −1 for 0 < x < 1). This implies a weakly polynomial running time. 2
In particular, as
√
41+7
2
< 6.702, we have a 6.702-factor approximation algo-
rithm. This is the best approximation guarantee known today.
Exercises
1. Show that the k-Median Problem (without requiring metric service costs)
has no constant-factor approximation algorithm unless P = NP.
2. Consider an instance of the Uncapacitated Facility Location Problem.
Prove that cS
: 2F
→R+ ∪{∞} is supermodular, where cS(X) :=

j∈D mini∈X ci j.
3. Consider a different integer programming formulation of the Uncapacitated
Facility Location Problem with a 0/1-variable zS for each pair S ∈F ×2D:
minimize

S=(i,D)∈F×2D
⎛
⎝fi +

j∈D
ci j
⎞
⎠zS
subject to

S=(i,D)∈F×2D: j∈D
zS
≥
1
( j ∈D)
zS
∈
{0, 1}
(S ∈F × 2D)
Consider the natural LP relaxation and its dual. Show how to solve them in
polynomial time (despite their exponential size). Show that the optimum LP
value is the same as that of (22.2) and (22.3).
4. Consider the LP relaxation of a simple special case of the Metric Capaci-
tated Facility Location Problem, in which each facility can serve up to u
customers (u ∈N): This LP is obtained by extending (22.2) by the constraints
yi ≤1 and xi j ≤uyi for i ∈F and j ∈D.
Show that this class of LPs has an unbounded integrality gap, i.e. the ratio of
the cost of an optimum integral solution over the optimum LP value can be
arbitrary large.
(Shmoys, Tardos and Aardal [1997])
5. Consider the Uncapacitated Facility Location Problem with the property
that each customer j ∈D is associated with a demand dj > 0 and service
costs per unit demand are metric, i.e. ci j
dj +
ci′ j
dj +
ci′ j′
dj′ ≥
ci j′
dj′ for i, i′ ∈F and
j, j′ ∈D. Modify the approximation algorithms for the case of unit demands
and show that the same performance guarantees can be obtained in this more
general case.

Exercises
569
6. Consider the factor-revealing LP (22.7) for γF = 1. Show that the supremum
of the optima for all d ∈N is 2.
(Jain et al. [2003])
7. Consider an instance of the MetricUncapacitatedFacilityLocationProb-
lem. Now the task is to ﬁnd a set X ⊆F such that 
j∈D mini∈X c2
i j is min-
imum. Find a constant-factor approximation algorithm for this problem. Try
to achieve a performance ratio less than 3.
8. Combine Theorem 22.3 and Theorem 22.10 to show that the Jain-Vazirani
Algorithm combined with scaling and greedy augmentation yields an ap-
proximation guarantee of 1.853.
9.
∗
The Max-k-Cover Problem is deﬁned as follows. Given a set system (U, F)
and a natural number k, ﬁnd a subset S ⊆F with |S| = k and |  S|
maximum. Prove that the natural greedy algorithm (iteratively picking a set
covering as many new elements as possible) is an (
e
e−1)-factor approximation
algorithm for the Max-k-Cover Problem.
10. Show that there is a 2-factor approximation algorithm for the Metric Soft-
Capacitated Facility Location Problem.
Hint: Combine the proof of Theorem 22.22 with the analysis of the Dual
Fitting Algorithm; here (22.6) can be strengthened.
(Mahdian, Ye and Zhang [2003])
11. Combine local search (Theorem 22.20) with discretizing costs (Lemma 22.15)
and scaling and greedy augmentation (Theorem 22.10) to obtain a 2.375-factor
approximation algorithm for the Metric Uncapacitated Facility Location
Problem.
12. Consider the special case of the Universal Facility Location Problem
where the cost functions fi are linear for all i ∈F. Describe a 3-factor
approximation algorithm for this case.
13. Let α0, α1, . . . , αr ∈R+ with α1 = maxr
i=1 αi and S := r
i=0 αi. Show that
there exists a partition {2, . . . ,r} = I0
.
∪I1 with αk + 
i∈Ik 2αi ≤S for
k = 0, 1.
Hint: Sort the list and take every second element.
14.
∗
Consider a local search algorithm for the Metric Capacitated Facility Lo-
cation Problem which, in addition to the algorithm in Section 22.8, has an
additional operation, namely a Pivot on forests that are the disjoint union of
two stars. It can be proved that this operation can be implemented in poly-
nomial time in this special case. Show that with this additional operation one
can obtain a performance ratio of 5.83.
Hint: Modify the proof of Lemma 22.28 using this new operation. Use Exer-
cise 13.
(Zhang, Chen and Ye [2004])

570
22. Facility Location
References
General Literature:
Cornu´ejols, G., Nemhauser, G.L., and Wolsey, L.A. [1990]: The uncapacitated facility
location problem. In: Discrete Location Theory (P. Mirchandani, R. Francis, eds.), Wiley,
New York 1990, pp. 119–171
Shmoys, D.B. [2000]: Approximation algorithms for facility location problems. Proceed-
ings of the 3rd International Workshop on Approximation Algorithms for Combinatorial
Optimization; LNCS 1913 (K. Jansen, S. Khuller, eds.) Springer, Berlin 2000, pp. 27–33
Vygen, J. [2005a]: Approximation algorithms for facility location problems (lecture notes).
Report No. 05950-OR, Research Institute for Discrete Mathematics, University of Bonn,
2005
Cited References:
Archer, A., Rajagopalan, R., and Shmoys, D.B. [2003]: Lagrangian relaxation for the k-
median problem: new insights and continuity properties. Algorithms – Proceedings of
the 11th Annual European Symposium on Algorithms, Springer, Berlin 2003, pp. 31-42.
Arora, S., Raghavan, P., and Rao, S. [1998]: Approximation schemes for Euclidean k-
medians and related problems. Proceedings of the 30th Annual ACM Symposium on
Theory of Computing (1998), 106–113
Arya, V., Garg, N., Khandekar, R., Meyerson, A., Munagala, K., and Pandit, V. [2004]:
Local search heuristics for k-median and facility location problems. SIAM Journal on
Computing 33 (2004), 544–562
Balinski, M.L. [1965]: Integer programming: methods, uses, computation. Management
Science 12 (1965), 253–313
Balinski, M.L., and Wolfe, P. [1963]: On Benders decomposition and a plant location
problem. Working paper ARO-27. Mathematica, Princeton 1963
Charikar, M., Guha, S., Tardos, ´E., and Shmoys, D.B. [2002]: A constant-factor approxi-
mation algorithm for the k-median problem. Journal of Computer and System Sciences
65 (2002), 129–149
Charikar, M., and Guha, S. [1999]: Improved combinatorial algorithms for the facility
location and k-median problems. Proceedings of the 40th Annual IEEE Conference on
Foundations of Computer Science (1999), 378–388
Chudak, F.A., and Shmoys, D.B. [1998]: Improved approximation algorithms for unca-
pacitated facility location. In: Integer Programming and Combinatorial Optimization;
Proceedings of the 6th International IPCO Conference; LNCS 1412 (R.E. Bixby, E.A.
Boyd, R.Z. Rios-Mercado, eds.) Springer, Berlin 1998, pp. 180-194; to appear in SIAM
Journal on Computing
Feige, U. [1998]: A threshold of ln n for the approximating set cover. Journal of the ACM
45 (1998), 634–652
Guha, S., and Khuller, S. [1999]: Greedy strikes back: improved facility location algorithms.
Journal of Algorithms 31 (1999), 228–248
Hochbaum, D.S. [1982]: Heuristics for the ﬁxed cost median problem. Mathematical Pro-
gramming 22 (1982), 148–162
Jain, K., Mahdian, M., Markakis, E., Saberi, A., and Vazirani, V.V. [2003]: Greedy facility
location algorithms analyzed using dual ﬁtting with factor-revealing LP. Journal of the
ACM 50 (2003), 795–824
Jain, K., and Vazirani, V.V. [2001]: Approximation algorithms for metric facility location
and k-median problems using the primal-dual schema and Lagrangian relaxation. Journal
of the ACM 48 (2001), 274–296

References
571
Kolliopoulos, S.G., and Rao, S. [1999]: A nearly linear-time approximation scheme for the
Euclidean k-median problem. Algorithms – Proceedings of the 7th European Symposium
on Algorithms (ESA); LNCS 1643 (J. Neˇsetˇril, ed.), Springer, Berlin 1999, pp. 378–389
Korupolu, M., Plaxton, C., and Rajaraman, R. [2000]: Analysis of a local search heuristic
for facility location problems. Journal of Algorithms 37 (2000), 146–188
Kuehn, A.A., and Hamburger, M.J. [1963]: A heuristic program for locating warehouses.
Management Science 9 (1963), 643–666
Levi, R., Shmoys, D.B., and Swamy, C. [2004]: LP-based approximation algorithms for
capacitated facility location. In: Integer Programming and Combinatorial Optimization;
Proceedings of the 10th International IPCO Conference; LNCS 3064 (G. Nemhauser, D.
Bienstock, eds.), Springer, Berlin 2004, pp. 206–218
Mahdian, M., and P´al, M. [2003]: Universal facility location. In: Algorithms – Proceedings
of the 11th European Symposium on Algorithms (ESA); LNCS 2832 (G. di Battista, U.
Zwick, eds.), Springer, Berlin 2003, pp. 409–421
Mahdian, M., Ye, Y., and Zhang, J. [2002]: Improved approximation algorithms for metric
facility location problems. Proceedings of the 5th International Workshop on Approxi-
mation Algorithms for Combinatorial Optimization; LNCS 2462 (K. Jansen, S. Leonardi,
V. Vazirani, eds.) Springer, Berlin 2002, pp. 229–242
Mahdian, M., Ye, Y., and Zhang, J. [2003]: A 2-approximation algorithm for the soft-
capacitated facility location problem. Approximation, Randomization, and Combinatorial
Optimization: Algorithms and Techniques; LNCS 2764 (S. Arora, K. Jansen, J.D.P.
Rolim, A. Sahai, eds.), Springer, Berlin 2003, pp. 129–140
Manne, A.S. [1964]: Plant location under economies-of-scale-decentralization and compu-
tation. Management Science 11 (1964), 213–235
P´al, M., Tardos, ´E., and Wexler, T. [2001]: Facility location with hard capacities. Proceed-
ings of the 42nd Annual IEEE Symposium on the Foundations of Computer Science
(2001), 329–338
Shmoys, D.B., Tardos, ´E., and Aardal, K. [1997]: Approximation algorithms for facility
location problems. Proceedings of the 29th Annual ACM Symposium on the Theory of
Computing (1997), 265–274
Stollsteimer, J.F. [1963]: A working model for plant numbers and locations. Journal of
Farm Economics 45 (1963), 631–645
Sviridenko, M. [2002]: An improved approximation algorithm for the metric uncapacitated
facility location problem. In: Integer Programming and Combinatorial Optimization; Pro-
ceedings of the 10th International IPCO Conference; LNCS 2337 (W. Cook, A. Schulz,
eds.), Springer, Berlin 2002, pp. 240–257
Vygen, J. [2005b]: From stars to comets: improved local search for universal facility loca-
tion. Report No. 05947-OR, Research Institute for Discrete Mathematics, University of
Bonn, 2005
Zhang, J., Chen, B., and Ye, Y. [2004]: Multi-exchange local search algorithm for the
capacitated facility location problem. In: Integer Programming and Combinatorial Op-
timization; Proceedings of the 10th International IPCO Conference; LNCS 3064 (G.
Nemhauser, D. Bienstock, eds.), Springer, Berlin 2004, pp. 219–233

Notation Index
N
set of natural numbers {1, 2, 3, . . .}
Z (Z+)
set of (nonnegative) integers
Q (Q+)
set of (nonnegative) rationals
R (R+)
set of (nonnegative) real numbers
⊂
proper subset
⊆
subset or equal
.
∪
disjoint union
X△Y
symmetric difference of sets X and Y
||x||2
Euclidean norm of a vector x
||x||∞
inﬁnity-norm of a vector x
x mod y
the unique number z with 0 ≤z < y and x−z
y
∈Z
x ⊤, A⊤
transpose of vector x and matrix A
⌈x⌉
smallest integer greater than or equal to x
⌊x⌋
greatest integer less than or equal to x
f = O(g)
O-notation
4
f = (g)
-notation
4
size(x)
encoding length of x; length of the binary string x
6, 65, 344
log x
logarithm of x with basis 2
6
V (G)
vertex set of graph G
13
E(G)
edge set of graph G
13
G[X]
subgraph of G induced by X ⊆V (G)
14
G −v
subgraph of G induced by V (G) \ {v}
14
G −e
graph obtained by deleting edge e from G
14
G + e
graph obtained by adding edge e to G
14
G + H
sum of graphs G and H
14
G/X
the graph resulting from G by contracting the subset X
of vertices
14
E(X, Y)
set of edges between X \ Y and Y \ X
14
E+(X, Y)
set of directed edges from X \ Y to Y \ X
14
δ(X), δ(v)
E(X, V (G) \ X), E({v}, V (G) \ {v})
14
(X), (v)
set of neighbours of vertex set X, of vertex v
14
δ+(X), δ+(v) set of edges leaving vertex set X, vertex v
14
δ−(X), δ−(v) set of edges entering vertex set X, vertex v
14
2S
power set of S
15

574
Notation Index
Kn
complete graph on n vertices
15
P[x,y]
x-y-subpath of P
16
dist(v, w)
length of the shortest x-y-path
16
c(F)

e∈F
c(e) (assuming that c : E →R and F ⊆E)
16
Kn,m
complete bipartite graph on n and m vertices
32
cr(J,l)
number of times the polygon J crosses line l
34, 508
G∗
planar dual of G
40
e∗
an edge of G∗; the dual of e
40
x ⊤y, xy
scalar product of the vectors x and y
49
x ≤y
for vectors x and y: inequality holds in each component
49
rank(A)
the rank of matrix A
50
dim X
dimension of a nonempty set X ⊆Rn
50
I
identity matrix
52
ej
j-th unit vector
52
AJ
submatrix of A consisting of the rows in J only
53
bJ
subvector of b consisting of the components with
indices in J
53
1l
vector whose components are all one
56
conv(X)
convex hull of all vectors in X
60
det A
determinant of a matrix A
66
sgn(π)
signum of permutation π
66
E(A, x)
ellipsoid
74
B(x,r)
Euclidean ball with center x and radius r
74
volume (X)
volume of the non-empty set X ⊆Rn
75
||A||
norm of matrix A
76
X◦
polar set of X
87
PI
integer hull of polyhedron P
91
(A)
maximum absolute value of the subdeterminants
of matrix A
92
P′, P(i)
ﬁrst and i-th Gomory-Chv´atal truncation of P
107
L R(λ)
Lagrangean relaxation
110
δ(X1, . . . , X p) multicut
133
cπ((x, y))
reduced cost of edge (x, y) with respect to π
147
( ¯G, ¯c)
metric closure of (G, c)
149
exf (v)
difference between incoming and outgoing ﬂow of v
157
value ( f )
value of an s-t-ﬂow f
157
↔
G
digraph resulting from G by adding the reverse edges
159
←e
reverse edge of directed edge e
159
u f (e)
residual capacity of edge e with respect to ﬂow f
159
G f
residual graph with respect to ﬂow f
159
GL
f
level graph of G f
166
λst
minimum capacity of a cut separating s and t
173
λ(G)
minimum capacity of a cut in G (edge-connectivity)
180

Notation Index
575
ν(G)
maximum cardinality of a matching in G
216
τ(G)
minimum cardinality of a vertex cover in G
216
TG(x)
Tutte matrix of G, depending on vector x
218
qG(X)
number of odd connected components in G −X
220
α(G)
maximum cardinality of a stable set in G
238
ζ(G)
minimum cardinality of an edge cover in G
238
r(X)
rank of X in an independence system
291
σ(X)
closure of X in an independence system
291
M(G)
cycle matroid of an undirected graph G
294
ρ(X)
lower rank of X in an independence system
294
q(E, F)
rank quotient of an independence system (E, F)
294
C(X, e)
for X ∈F: the unique circuit in X ∪{e}
(or ∅if X ∪{e} ∈F)
299
(E, F∗)
dual of an independence system (E, F)
299
P( f )
polymatroid for a submodular function f
327
⊔
blank symbol
344
{0, 1}∗
set of all binary strings
344
P
class of polynomially solvable decision problems
351
NP
class of decision problems with certiﬁcates for
yes-instances
352
x
negation of the literal x
355
coNP
class of complements of problems in NP
365
OPT(x)
value of an optimum solution for instance x
368
A(x)
value of the output of algorithm A for an optimization
problem on input x
368
largest(x)
largest integer appearing in instance x
369
H(n)
1 + 1
2 + 1
3 + · · · + 1
n
378
χ(G)
chromatic number of G
386
ω(G)
maximum cardinality of a clique in G
386
Exp(X)
expectation of the random variable X
393
Prob(X)
probability of an event X
393
SUM(I)
sum over all elements in I
425
N F(I)
output of the Next-Fit Algorithm for instance I
427
F F(I)
output of the First-Fit Algorithm for instance I
427
FFD(I)
output of the First-Fit-Decreasing Algorithm for I
429
G(a,b)
i
shifted grid
507
Q(n)
convex hull of the incidence vectors of the tours in Kn
519
H K(Kn, c)
Held-Karp bound for the TSP instance (Kn, c)
526
cF(X), cF(x) facility cost of a solution
539, 559
cS(X), cS(x)
service cost of a solution
539, 559

Author Index
Aardal, K.
341, 539–541, 568, 571
Aarts, E.
519, 533
Ackermann, W.
122, 125
Adleman, L.M.
367, 375
Agrawal, A.
367, 375, 478, 498
Aho, A.V.
7, 12, 374
Ahuja, R.K.
139, 146, 155, 168, 186, 212
Ajtai, M.
403, 411
Albrecht, C.
154, 155
Alspach, B.
321
Alt, H.
217, 242
Anderson, I.
222, 241, 242
Anstee, R.P.
285, 289
Aoshima, K.
31, 46
Appel, K.
388, 411
Applegate, D.
517, 529, 533
Archer, A.
551, 570
Armstrong, R.D.
211, 212
Arora, S.
398–400, 411, 469, 498, 506,
509, 511, 512, 533, 539, 570, 571
Arya, V.
553, 555, 557, 570
Asano, T.
397, 410, 411
Ausiello, G.
374, 399, 410
Avi-Itzak, B.
140
Avis, D.
55, 64
Bachem, A.
270, 288, 342
Baker, B.S.
429, 439
Balakrishnan, V.K.
139
Balinski, M.L.
268, 269, 537, 540, 570
Ball, M.O.
140, 242, 263, 268, 269, 288,
498, 533
Bansal, N.
438, 440
Bar-Hillel, Y.
12
Bar-Yehuda, R.
381–383, 411
Becker, A.
409, 411
Becker, M.
459, 463
Becvar, J.
424
Bellare, M.
397, 399, 411
Bellman, R.E.
144, 146, 148–150, 155,
187, 200, 213, 419, 424
Benders, J.F.
570
Berge, C.
20, 46, 218, 222, 229, 230, 232,
239, 242, 267, 269, 386, 411
Berman, L.
471, 472, 500
Berman, P.
406, 411, 472, 475, 498
Bern, M.
468, 498
Bertsimas, D.J.
115, 467, 489, 498, 499
Bienstock, D.
289, 451, 463, 465, 571
Birkhoff, G.
247, 268, 269
Bixby, R.E.
320, 341, 533, 570
Bj¨orner, A.
341
Bland, R.G.
54, 55, 64, 74, 90
Blum, M.
416, 417, 424
Blum, N.
242
Bock, F.C.
128, 140
Boesch, F.
462, 463
Bollob´as, B.
46
Bondy, J.A.
46
Borchers, A.
474, 499
Borgwardt, K.-H.
56, 64
Bor˚uvka, O.
119, 140, 141
Bovet, D.P.
374
Boyd, E.A.
110, 116, 570
Boyd, S.C.
524, 533
Br`egman, L.M.
241, 242
Brooks, R.L.
385, 411
Budach, L.
243
Burkard, R.E.
187, 503, 533
Busacker, R.G.
199, 212
Camion, P.
43, 44, 46
Caprara, A.
285, 289, 438, 440
Carath´eodory, C.
63, 64, 112, 116
Carr, R.
524, 533
Cayley, A.
119, 136, 140
Chalasani, P.
531, 533
Chandra, B.
514, 533
Chang, R.C.
530, 534
Chao, K.-M.
140
Charikar, M.
547, 552, 558, 570
Chazelle, B.
125, 140

578
Author Index
Chen, B.
561, 569, 571
Chen, J.
394, 411
Cheng, X.
498, 499
Cheriton, D.
125, 140
Cheriyan, J.
172, 186, 495, 499
Cherkassky, B.V.
148, 155, 166, 187
Choukhmane, E.
471, 499
Christoﬁdes, N.
498, 503, 517, 526, 531,
533
Chu, Y.
128, 140
Chudak, F.A.
451, 463, 541, 570
Chudnovsky, M.
386, 411
Church, A.
345, 346
Chv´atal, V.
55, 63, 64, 99, 107, 110, 116,
269, 289, 378, 387, 411, 523, 533
Clementi, A.E.F.
407, 412, 472, 499
Cobham, A.
6, 12
Coffman, E.G.
439
Collatz, L.
321
Cook, S.A.
354, 355, 358, 375
Cook, W.
93, 94, 100, 112, 113, 115, 116,
186, 212, 263, 269, 288, 320, 411, 463,
517, 532, 533, 571
Cormen, T.H.
139, 155, 186
Cornu´ejols, G.
187, 411, 570
Crescenzi, P.
374, 410
Crestin, J.P.
43
Cunningham, W.H.
115, 186, 187, 212,
263, 264, 266, 270, 288, 312, 320, 341,
524, 532, 533
Dahlhaus, E.
186, 187
Dantzig, G.B.
53, 54, 64, 102, 106, 116,
117, 161, 187, 415, 419, 424, 519, 533
De˘ıneko, V.G.
503, 533
Delaunay, B.
137, 141
Demers, A.
440
Derigs, U.
263, 268, 269
Dessouky, M.I.
182, 188
Deza, M.
117
di Battista, G.
571
Dial, R.B.
153, 155
Diestel, R.
46, 132, 140
Dijkstra, E.W.
26, 122, 140, 145, 146,
149, 153, 155, 201, 206, 211, 447, 449,
471
Dilworth, R.P.
239, 242, 243
Dinic, E.A.
166, 167, 183, 187
Dinur, I.
381, 412
Dirac, G.A.
42, 46
Dixon, B.
125, 140
Doig, A.G.
527, 534
Dreyfus, S.E.
155, 469–471, 496, 499
Du, D.-Z.
472–474, 498–500
Dulmage, A.L.
239, 243
Edmonds, J.
6, 12, 23, 46, 73, 89, 90,
98, 99, 116, 128, 129, 131, 132, 134,
137, 139–141, 147, 155, 164–166, 184,
187, 200–202, 212, 215, 217, 220, 229,
231, 234–238, 241, 242, 245, 247–250,
252, 263–265, 270, 272, 273, 277, 280,
285–287, 289, 305–309, 312, 314–316,
319, 320, 327–330, 339, 341, 366, 375,
533
Egerv´ary, E.
267, 270
Egoryˇcev, G.P.
240, 242
Eisemann, K.
431, 440
Eisenbrand, F.
114, 116
Eleut´erio, V.
451, 463
Elias, P.
161, 187
Erd˝os, P.
242, 243, 288, 410, 412
Erickson, R.E.
496, 499
Euler, L.
13, 30, 31, 35, 36, 40, 41, 45, 46,
275, 301, 509
Even, S.
381–383, 411, 451, 463
Faigle, U.
320
Fakcharoenphol, J.
147, 155
Falikman, D.I.
240, 243
Farkas, G.
59, 60, 64, 81, 97, 100
Feder, T.
217, 243
Feige, U.
380, 397, 399, 400, 408, 412,
550, 570
Feinstein, A.
161, 187
Feng, Q.
473, 499
Fern´andez-Baca, D.
403, 405, 412
Fernandez de la Vega, W.
431–433, 437,
438, 440
Fischetti, M.
285, 289
Fleischer, L.K.
208, 212, 333, 341, 342,
451, 463, 495, 499
Fleischer, R.
156
Floyd, R.W.
149, 150, 154, 155, 424
Fonlupt, J.
112, 116
Ford, L.R.
146, 148–150, 155, 159–161,
164, 181, 183, 186, 187, 193, 194, 200,
207, 209, 212, 217, 448, 459, 463, 497
Fortune, S.
137, 140, 452, 463
Fourier, J.B.J.
63, 64
Francis, R.
570
Frank, A.
82, 90, 135, 139, 140, 179,
185–187, 288, 309, 315, 316, 320, 321,
330, 340, 342, 454, 463, 464, 531, 533
Fredman, M.L.
123, 125, 140, 146, 155
Fremuth-Paeger, C.
237, 243

Author Index
579
Friesen, D.K.
394, 411
Frieze, A.
531, 533
Frobenius, G.
217, 222, 243
Fujishige, S.
166–168, 183, 187, 333, 338,
341, 342
Fujito, T.
406, 411
Fulkerson, D.R.
99, 106, 116, 131, 141,
159–161, 164, 181, 183, 186, 187, 193,
194, 200, 207, 209, 212, 217, 239, 243,
302, 319–321, 387, 412, 448, 459, 463,
497, 519, 533
F¨urer, M.
389, 412
Gabow, H.N.
125, 129, 139, 141, 179,
185, 187, 263, 270, 312, 320, 321, 481,
482, 485, 496, 497, 499
Gabriel, R.
535
G´acs, P.
81, 90
Galbiati, G.
531, 533
Gale, D.
57, 64, 208, 212
Galil, Z.
125, 141, 166, 187
Gallai, T.
161, 187, 237, 238, 242, 243,
249, 252
Gallo, G.
155
Gambosi, G.
374, 410
Garcia-Diaz, A.
186
Garey, M.R.
374, 381, 408, 410, 412,
422–424, 426, 429, 430, 439, 440, 469,
474, 499, 506, 533
Garg, N.
449, 464, 570
Gauss, C.F.
70
Gavril, F.
381
Geelen, J.F.
220, 243
Geiger, D.
409, 411
Gens, G.V.
421, 424
Geoffrion, A.M.
111, 116
Gerards, A.M.H.
116, 242, 269, 285, 288
Ghouila-Houri, A.
104, 116, 329
Gilbert, E.N.
471, 499
Giles, R.
23, 46, 98, 99, 116, 341
Gilmore, P.C.
435, 440
Goemans, M.X.
88, 115, 116, 394, 395,
397, 408, 412, 467, 479, 481, 482, 485,
486, 489, 497–500
Gofﬁn, J.L.
111, 116
Goldberg, A.V.
147, 148, 155, 168, 172,
186, 187, 197, 211, 212, 237, 243, 499
Goldfarb, D.
74, 90
Goldreich, O.
399, 411
Goldwasser, S.
412
Gomory, R.E.
99, 106–108, 116, 172–
175, 178, 180, 184, 187, 269, 281, 282,
285, 288, 289, 435, 440, 480–482, 496, 497
Gondran, M.
139, 155, 186, 212, 320
Gonzalez, T.
501, 535
Gowen, P.J.
199, 212
Graham, R.L.
46, 186, 212, 242, 269, 320,
341, 439, 440, 469, 499, 506, 533
Graver, J.E.
94, 116
Graves, R.L.
116
Graves, S.C.
440
Grigoriadis, M.D.
451, 464
Gr¨opl, C.
478, 499
Gr¨otschel, M.
46, 70, 74, 76, 79, 82, 84,
86, 89, 90, 186, 211, 212, 242, 269, 270,
288, 320, 331, 333, 341, 342, 388, 412,
498, 520, 523, 524, 533
Guan, M.
275, 289
Guha, S.
538, 547, 549, 550, 558, 570
Gusﬁeld, D.
178, 187
Gutin, G.
532
Guy, R.
140, 289, 320, 341
Hadlock, F.
46, 286, 289
Hajnal, A.
464
Haken, W.
388, 411
Hall, M.
187, 213
Hall, P.
216, 217, 239, 243
Halld´orsson, M.M.
407, 412
Halmos, P.R.
216, 243
Hamburger, M.J.
537, 571
Hammer, P.L.
46, 116, 270, 320, 341,
413, 440
Han, Y.
11, 12
Hanan, M.
469, 499
Hanani, H.
140, 289, 320, 341
Hao, J.
179, 187
Harvey, W.
110, 116
Hassin, R.
210, 212
H˚astad, J.
399, 400, 408, 412
Hausmann, D.
295, 304, 319, 321
Heawood, P.J.
388, 412
Held, M.
525–528, 530, 533–535
Hell, P.
321
Henzinger, M.R.
146, 155, 185, 187
Hetzel, A.
469, 499
Hierholzer, C.
30, 46
Hitchcock, F.L.
187, 192, 212
Hochbaum, D.S.
382, 407, 410, 412,
438–440, 465, 498, 539, 570
Hoey, D.
137, 141
Hoffman, A.J.
52, 64, 98, 101, 103, 116,
182, 187, 209, 213, 247
Holyer, I.
383, 412
Hopcroft, J.E.
7, 12, 40, 46, 217, 237, 240,
243, 349, 374, 375, 452, 463

580
Author Index
Hoppe, B.
208, 213
Horowitz, E.
374, 411, 439, 440
Hougardy, S.
399, 412, 472, 499, 500
Hsu, W.L.
408, 412
Hu, T.C.
172–175, 178, 180, 183, 184,
187, 281, 282, 285, 288, 463, 464,
480–482, 496, 497
Hurkens, C.A.J.
502, 534
Hwang, F.K.
472, 498, 499, 500
Hwang, R.Z.
530, 534
Ibaraki, T.
179, 180, 186–188, 339, 342,
453, 464
Ibarra, O.H.
420, 421, 424
Iri, M.
31, 46, 199, 213
Itai, A.
451, 463
Iudin, D.B.
74, 90
Iwama, K.
410
Iwata, S.
220, 243, 333, 337, 341, 342
Iyengar, G.
451, 463
Jain, K.
489, 491, 492, 494, 495, 498–500,
539, 541–544, 546, 550, 560, 569, 570
Jansen, K.
570, 571
Jarn´ık, V.
122, 141
Jenkyns, T.A.
295, 304, 321
Jensen, P.M.
340, 342
Jewell, W.S.
199, 213
Jin, Z.
211, 212
John, F.
75
Johnson, D.B.
146, 156
Johnson, D.S.
187, 374, 378, 380, 381,
393, 394, 397, 408, 410–413, 422–424,
426, 429, 430, 439, 440, 469, 474, 499,
506, 527, 532–534
Johnson, E.L.
46, 116, 270, 273, 277, 280,
286, 287, 289, 320, 341, 342, 413, 440
Johnson, S.
106, 116, 519, 533
Jothi, R.
496, 500
J¨unger, M.
463, 530, 532
Jungnickel, D.
186, 212, 237, 243, 532
Kahn, A.B.
29, 46
Kaibel, V.
463
Kann, V.
374, 410
Kaplan, H.
531, 534
Karakostas, G.
451, 464
Karel, C.
527, 534
Karger, D.R.
125, 141, 179, 185, 188
Karloff, H.
514, 533
Karmarkar, N.
82, 90, 434–440
Karp, R.M.
82, 90, 127, 129, 141, 147,
151, 152, 155, 156, 164–166, 184, 187,
200–202, 212, 217, 237, 240, 243, 354,
358, 360, 363–365, 374, 375, 434–440,
455, 464, 468, 500, 506, 525–528, 530,
533–535
Karpinski, M.
472, 500
Karzanov, A.V.
166, 188, 237, 243, 460,
464
Kayal, N.
367, 375
Kellerer, H.
421, 424
Kernighan, B.W.
514–517, 525, 531–534
Khachiyan, L.G.
74, 80, 81, 90, 351, 437,
451, 464
Khandekar, R.
570
Khanna, S.
386, 413
Khintchine, A.
70, 90
Khuller, S.
496, 500, 538, 549, 550, 570
Kim, C.E.
420, 421, 424
King, V.
125, 141, 168, 188
Klee, V.
55, 64
Klein, M.
194, 195, 213
Klein, P.
125, 141, 155, 478, 479, 498,
500
Klinz, B.
208, 213
Knuth, D.E.
12, 29, 47, 137, 141
Koch, J.
388, 411
Kolliopoulos, S.G.
539, 571
K¨onemann, J.
449, 464
K¨onig, D.
26, 32, 47, 113, 216, 217, 238,
239, 243, 244, 267, 384, 413
Koopmans, T.C.
64
Korte, B.
46, 116, 122, 141, 155, 186,
212, 270, 288, 295, 304, 306, 319–321,
327, 340–342, 413, 422, 424, 440, 463,
469, 500, 533
Kortsarz, G.
495, 500
Korupolu, M.
553, 561, 571
Kou, L.
471, 472, 500
Krauthgamer, R.
495, 500
Krentel, M.W.
532, 534
Krogdahl, S.
319
Kruskal, J.B.
52, 64, 101, 103, 116, 121,
122, 129, 131, 137, 141, 247, 304, 307,
325
Kuehn, A.A.
537, 571
Kuhn, H.W.
57, 63, 64, 116, 187, 217,
243, 246, 268, 270
Kuich, W.
188
Kumar, M.P.
166, 188
Kuratowski, K.
36–38, 40, 45, 47
Ladner, R.E.
367, 375
Lagergren, J.
403, 405, 412
Land, A.H.
527, 534

Author Index
581
Lasserre, J.B.
96, 116
Lawler, E.L.
155, 212, 242, 263, 269, 316,
320, 421, 424, 439, 440, 532
Lee, J.R.
495, 500
Lee, R.C.T.
530, 534
Legendre, A.M.
35, 47
Lehman, A.
302, 321
Leiserson, C.E.
139, 155, 186
Lenstra, H.W.
432, 440
Lenstra, J.K.
440, 519, 532, 533
Leonardi, S.
571
Letchford, A.N.
285, 289
Levi, R.
561, 571
Levine, M.S.
179, 188
Levner, E.V.
421, 424
Lewenstein, M.
534
Lewis, H.R.
349, 375
Lewis, P.M.
502, 534
Lieberherr, K.
410, 413
Lin, S.
514–517, 525, 531–534
Linial, N.
386, 413
Lipton, R.J.
263, 270
Little, J.D.C.
527, 534
Liu, T.
128, 140
Liu, X.
411
Lomonosov, M.V.
460, 464
Lov´asz, L.
46, 70, 74, 76, 79, 81, 82–84,
86, 89, 90, 135, 140, 141, 186, 211, 212,
219, 220, 223, 224, 232, 242, 243, 267–
270, 288, 320, 327, 331, 333, 340–342,
378, 386–388, 411–413, 435–437, 439,
454, 455, 463, 464, 500
L¨owner, K.
75
Lucchesi, C.L.
454, 455, 463, 464
Lueker, G.S.
431–433, 437, 438, 440
Lund, C.
411
Mader, W.
132, 180, 183, 188
Mafﬁoli, F.
531, 533
Magnanti, T.L.
138–140, 155, 186, 212,
242, 269, 288, 498, 533
Mahajan, S.
397, 408, 413
Mahdian, M.
546, 549, 559–562, 569–571
Maheshwari, S.N.
166, 172, 186, 188
Malhotra, V.M.
166, 188
Mangaserian, O.
424
Manne, A.S.
537, 571
Manu, K.S.
139, 141
Marchetti-Spaccamela, A.
374, 410
Markakis, E.
570
Markowsky, G.
471, 472, 500
Marsh, A.B.
263, 264, 266, 270, 272, 285,
289
Martello, S.
423
Martin, A.
469, 500
Matsumoto, K.
456, 464
Matsuyama, A.
471, 500
Matula, D.W.
449, 464
McCormick, S.T.
341
McGeoch, L.A.
527, 534
Megiddo, N.
153, 156
Mehlhorn, K.
155, 172, 186, 242, 263,
270, 459, 463, 472, 500
Meinardus, G.
321
Melkonian, V.
495, 500
Mendelsohn, N.S.
239, 243
Menger, K.
158, 162–164, 183, 185, 188,
216, 287, 443, 453
Meyer, R.R.
92, 116, 424
Meyerson, A.
570
Micali, S.
237, 243
Middendorf, M.
456, 464
Mihail, M.
500
Mikl´os, D.
288
Milkov´a, E.
119, 141
Miller, D.J.
321
Miller, R.E.
375, 464, 500
Minkowski, H.
52, 60, 61, 64
Minoux, M.
139, 155, 186, 212, 320
Minty, G.J.
19, 47, 55, 64
Mirchandani, P.
570
Mitchell, J.
506, 534
Monma, C.L.
140, 242, 269, 288, 304,
306, 321, 496, 498, 499, 533
Moore, E.F.
26, 47, 146, 148–150, 156,
200, 471
Motwani, R.
217, 243, 411, 531, 533
Motzkin, T.S.
63, 64
Mucha, M.
220, 243
Mulmuley, K.
220, 244
Munagala, K.
570
Munkres, J.
246, 270
Murty, K.G.
527, 534
Murty, U.S.R.
46
Naddef, D.
530
Nagamochi, H.
179, 180, 186–188, 339,
342
Namaad, A.
166, 187
Nash-Williams, C.S.J.A.
132, 133, 135,
141, 313, 321, 453, 462, 464
Nemhauser, G.L.
46, 113, 115, 116, 140,
242, 269, 288, 289, 341, 342, 408, 412,
465, 498, 533, 570, 571
Nemirovskii, A.S.
74, 90
Neˇsetˇril, J.
119, 122, 141, 571

582
Author Index
Neˇsetˇrilov´a, H.
119, 141
Nicholls, W.
137, 141
Nierhoff, T.
499
Nishizeki, T.
456, 462, 464
Nolles, W.
530, 535
Okamura, H.
457, 462, 464
Orden, A.
54, 64, 192, 213
Ore, O.
209, 213
Orlin, J.B.
139, 154–156, 168, 179, 186,
187, 203–206, 211–213
Oxley, J.G.
320
Padberg, M.W.
63, 82, 90, 282, 285, 289,
520, 523, 524, 533
P´al, M.
559, 561, 562, 571
Pallottino, S.
155
Pandit, V.
570
Papadimitriou, C.H.
82, 90, 187, 242, 269,
349, 362, 373–375, 380, 392, 402, 403,
405–408, 410, 411, 413, 423, 427, 440,
504, 506, 518, 525, 532–534
Pardalos, P.M.
156
Paul, M.
242
Petersen, J.
241, 244, 490, 498
Pettie, S.
146, 149, 156
Pfeiffer, F.
456, 464
Pferschy, U.
421, 424
Phillips, D.T.
186
Phillips, S.
182, 188
Picard, J.
182, 188
Pisinger, D.
420, 424
Plassmann, P.
468, 498
Plaxton, C.
553, 561, 571
Plotkin, S.A.
204, 205, 211, 213, 438,
440, 499
Plummer, M.D.
224, 232, 242, 288
Poljak, S.
453, 464
Pollak, H.O.
471, 499
Polyak, B.T.
111, 117
Pomerance, C.
367, 375
Pratt, V.
367, 375, 424
Prim, R.C.
122, 123, 125, 137, 141, 145,
325, 472
Pr¨omel, H.J.
186, 212, 399, 412, 463, 469,
472, 498–500
Protasi, M.
374, 410
Pr¨ufer, H.
136, 141
Pulleyblank, W.R.
99, 115, 116, 186, 212,
242, 269, 270, 273, 288, 289, 320, 524,
532, 533
Punnen, A.P.
532
Queyranne, M.
181, 182, 188, 337, 338,
342
Rabin, M.O.
220, 244
Radhakrishnan, J.
407, 412
Rado, R.
305–308, 319, 321
Radzik, T.
153, 156
Raghavachari, B.
389, 412, 496, 500
Raghavan, P.
395, 413, 461, 464, 539, 570
Rajagopalan, R.
551, 570
Rajaraman, R.
553, 561, 571
Ramachandran, V.
146, 156
Ramaiyer, V.
472, 475, 498
Raman, R.
146, 156
Ramesh, H.
397, 408, 413
Rao, A.
531, 533
Rao, M.R.
82, 90, 282, 285, 289
Rao, S.
147, 155, 168, 185, 187, 188, 513,
534, 539, 570, 571
Rauch, M.
125, 140
Ravi, R.
478, 479, 498, 500
Raz, R.
380, 413
Recski, A.
320
R´edei, L.
44, 47
Reinelt, G.
285, 289, 532, 533
Richards, D.S.
498
Rinaldi, G.
532
Rinnooy Kan, A.H.G.
440, 532
Rios-Mercado, R.Z.
570
Ripphausen-Lipa, H.
463
Rivest, R.L.
139, 155, 186, 424
Rizzi, R.
238, 244, 288, 289, 339, 342
Robbins, H.E.
44, 47, 462, 463
Robertson, N.
44, 45, 47, 388, 389, 413,
456, 457, 462, 464
Robins, G.
472, 473, 476, 478, 500
Robinson, S.M.
424
R¨ock, H.
341, 342
Rohe, A.
263, 269, 517, 533
Rolim, J.D.P.
571
Rose, D.J.
185, 188
Rosenberg, I.G.
117
Rosenkrantz, D.J.
502, 534
Rosenstiehl, P.
321
Rothberg, E.E.
527, 534
Rothschild, B.
460, 464
Rubinstein, J.H.
498, 500
Ruhe, G.
186, 212
Rumely, R.S.
367, 375
Rustin, R.
140
Saberi, A.
570
Safra, S.
380, 381, 386, 399, 411–413

Author Index
583
Sahai, A.
571
Sahni, S.
374, 411, 421, 424, 439, 440,
501, 535
Saito, N.
456, 464
Sanders, D.P.
413
Sanders, P.
390, 413
Sankowski, P.
220, 243
Sauer, N.
140, 289, 320, 341
Saxena, N.
367, 375
Sch¨afer, G.
263, 270
Schefﬂer, P.
462, 464
Schietke, J.
155
Sch¨onhage, A.
349, 375
Schonheim, J.
140, 289, 320, 341
Schrader, R.
327, 341, 422, 424
Schrijver, A.
61, 63, 70, 74, 76, 79, 82–84,
86, 89, 90, 100, 106, 108, 109, 112,
115–117, 140, 155, 186, 212, 213, 241,
242, 244, 267, 270, 286, 288, 320, 331,
333–335, 341, 342, 388, 412, 435–437,
439, 455, 463, 500, 532
Schulz, A.S.
114, 116, 211, 213, 571
Seb˝o, A.
280, 289
Seiden, S.S.
430, 440
Seymour, P.D.
44, 45, 47, 106, 117, 187,
279, 289, 318, 321, 411, 413, 456, 457,
460, 462–464
Shafrir, N.
534
Shahrokhi, F.
449, 464
Shamir, A.
451, 463
Shamos, M.I.
137, 141
Shannon, C.E.
161, 187
Shenoy, N.
137, 141
Shiloach, Y.
166, 183, 188
Shioura, A.
168, 188
Shisha, O.
64
Shmoys, D.B.
407, 411, 412, 438–440,
449, 465, 499, 527, 532, 535, 539–541,
551, 561, 568, 570, 571
Shor, N.Z.
74, 90
Simchi-Levi, D.
429, 441
Skutella, M.
115, 116, 208, 212
Slav´ık, P.
380, 413
Sleator, D.D.
166, 168, 188
Smith, J.M.
498, 500
Smith, W.D.
513, 534
S´os, V.T.
140, 288, 342, 464
Specker, E.
410, 413
Spencer, T.
125, 141
Sperner, E.
239, 244
Spirakis, P.
213
Stearns, R.E.
502, 534
Steger, A.
399, 412, 469, 498, 500
Steiglitz, K.
242, 269, 362, 375, 380, 413,
423, 518, 532–534
Stein, C.
185, 188
Steinitz, E.
61, 64, 90
Steurer, D.
390, 413
Stirling, J.
3, 12
Stockmeyer, L.
384, 408, 412, 413
Stoer, M.
179, 180, 188, 498
Stollsteimer, J.F.
537, 571
Strassen, V.
349, 375
Su, X.Y.
183, 188
Subramanian, S.
155
Sudan, M.
397, 399, 411
Sviridenko, M.
438, 440, 534, 538, 541,
550, 571
Swamy, C.
561, 571
Swamy, M.N.S.
186
Sweeny, D.W.
527, 534
Szegedy, C.
241, 244
Szegedy, M.
411, 412
Szigeti, Z.
241, 244
Sz˝onyi, T.
288
Takada, H.
410
Takahashi, M.
471, 500
Tang, L.
186, 187
Tardos, ´E.
82, 90, 116, 186, 197, 204, 205,
208, 211–213, 320, 321, 438, 440, 463,
464, 495, 499, 500, 539–541, 561, 562,
568, 570, 571
Tarjan, R.E.
29, 40, 46, 47, 122, 123, 125,
137, 140, 141, 146, 155, 166–168, 172,
186–188, 197, 211, 212, 242, 263, 270,
424
Tarry, G.
26
Taub, A.H.
64
Teo, C.
467, 489, 498
Thatcher, J.W.
375, 464, 500
Theis, D.O.
285, 289
Thimm, M.
472, 500
Thomas, R.
411, 413
Thomassen, C.
33, 37, 38, 47
Thompson, C.D.
395, 413, 461, 464
Thorup, M.
146, 156
Thulasiraman, K.
186
Tindell, R.
462, 463
Todd, M.J.
74, 90
Tolsto˘ı, A.N.
194, 213
Tomizawa, N.
200, 213
Toth, P.
423
Tovey, C.
514, 533
Tr´emaux,
26
Trevisan, L.
407, 410, 412, 413, 472, 499

584
Author Index
Triesch, E.
530, 533, 535
Trippen, G.
156
Tucker, A.W.
57, 64, 116, 187
Tunc¸el, L.
172, 188
Turing, A.M.
343–346, 348–351, 354,
355, 371, 372, 375
Tutte, W.T.
35–37, 47, 132, 133, 141,
218–222, 232, 241, 244, 267, 269, 275,
286, 289, 464
Ullman, J.D.
7, 12, 349, 374, 375, 440
Vaidya, P.M.
88, 90
van der Waerden, B.L.
241–243
van Emde Boas, P.
349, 375
van Leeuwen, J.
375
van Vliet, A.
430, 441
Varadarajan, K.R.
263, 270
Varadarajan, S.
496, 500
Vaughan, H.E.
216, 243
Vazirani, U.V.
220, 244
Vazirani, V.V.
185, 188, 220, 237, 243,
244, 411, 498, 500, 541, 542, 544, 550,
560, 569–571
Veinott, A.F.
102, 117, 496, 499
Vempala, S.
506, 534
Vetta, A.
495, 499
Vishkin, U.
496, 500
Vizing, V.G.
384, 385, 390, 413
von Neumann, J.
57, 64, 247, 268, 270
von Randow, R.
320
Vorono¨ı, G.
137
Vu˘skovi´c, K.
411
Vygen, J.
155, 183, 188, 206, 213, 335,
342, 416, 424, 451, 453, 456, 461–465,
530, 533, 535, 539, 550, 561, 563, 567,
570, 571
Wagner, D.
459, 463, 465
Wagner, F.
179, 180, 188
Wagner, H.M.
192, 213
Wagner, K.
37, 40, 47
Wagner, R.A.
469–471, 496, 499
Warme, D.M.
473, 500
Warshall, S.
149, 150, 154, 156
Weber, G.M.
263, 270
Wegener, I.
375
Weihe, K.
183, 189, 459, 463, 465
Weismantel, R.
115, 211, 213, 341
Welsh, D.J.A.
320
Werners, B.
535
Wetterling, W.
321
Wexler, T.
561, 562, 571
Weyl, H.
60, 61, 64
Whinston, A.
460, 464
White, N.
320, 341
Whitney, H.
29, 42, 46, 47, 163, 189, 300,
301, 321
Willard, D.E.
125, 140, 146, 155
Williamson, D.P.
394, 395, 397, 408,
411, 412, 479, 481, 482, 485–488, 495,
497–500, 527, 535
Wilson, R.J.
46
Winter, P.
473, 498, 500
Woeginger, G.J.
187, 208, 213, 422, 424,
502, 503, 530, 533–535
Wolfe, P.
54, 64, 116, 537, 570
Wolsey, L.A.
94, 113, 115, 117, 138, 140,
526, 535, 570
Wu, B.Y.
140
Wyllie, J.
452, 463
Xu, Y.
312, 321
Yamashita, Y.
410
Yannakakis, M.
185, 187, 188, 392, 394,
402, 403, 405–408, 410, 413, 504, 506,
532, 534
Yao, A.C.
440
Ye, Y.
546, 549, 560, 561, 569, 571
Young, N.
449, 465
Younger, D.H.
454, 455, 463, 464
Yue, M.
430, 441
Zachariasen, M.
473, 500
Zelikovsky, A.Z.
472, 473, 475, 476, 478,
500
Zhang, J.
546, 549, 560, 561, 569, 571
Zhang, Y.
473, 499
Zheng, H.
394, 411
Zhou, H.
137, 141
Zhou, X.
462, 464
Ziegler, G.M.
211, 213, 341
Zimmermann, U.
341, 342
Zipkin, P.H.
440
Zwick, U.
149, 156, 571

Subject Index
2Sat
358, 372
3-Dimensional Matching (3DM)
363
3-Matroid Intersection
373
3-Occurrence Max-Sat Problem
403,
405
3-Occurrence Sat
372
3-Opt Algorithm
517
3DM
363
3Sat
357, 358
ϵ-Dominance Problem
422
ϵ-dominance
421
γ -expander
403
0-1-string
11, 344
1-tree
525
2-connected graph
29
2-edge-connected graph
44
2-matching inequalities
522, 529, 532
2-opt
513, 514
2-polymatroid
340
3-connected graph
37
3-connected planar graph
90
3-dimensional polytope
90
3-cut
133, 185
3-opt
513, 516
absolute approximation algorithm
377,
385, 389, 390, 420
abstract dual graph
42, 46, 301
accessible set system
323
Ackermann’s function
122, 125
active vertex (network ﬂows)
168
active vertex set (network design)
482,
497
acyclic digraph
19, 20, 28, 46
Add
561
adjacency list
25, 351
adjacency matrix
25
adjacent edges
13
adjacent vertices
13
admissible edge (network ﬂows)
169
afﬁnely independent
62
algebraic numbers
350
algorithm
5, 368
algorithm for a decision problem
351
All Pairs Shortest Paths Problem
148–150, 278, 472
almost satisfying edge set (network design)
482
alphabet
344
alternating ear-decomposition
224–226
alternating forest
230, 231
alternating path
217, 251, 309
alternating walk
514
– closed
514, 516
– proper
515, 516
– proper closed
515
Another Hamiltonian Circuit
531
anti-arborescence
27
antiblocker
409
antichain
239
antimatroid
324, 339
approximation algorithm
– absolute
377, 389, 390, 420
– asymptotic k-factor
390
– k-factor
377
approximation ratio
390
– asymptotic
390
approximation scheme
390, 392, 400,
402, 403, 469, 506, 513
– asymptotic
390, 392
– fully polynomial
391, 421, 422, 448,
449
– fully polynomial asymptotic
391, 434,
437
arborescence
18, 24, see Minimum
Weight Arborescence Problem,
see Minimum Weight Rooted
Arborescence Problem, 131, 134, 135,
139, 325
arborescence polytope
139

586
Subject Index
arc
13
Arora’s Algorithm
511, 512
articulation vertex
17
Assignment Problem
246, 268
associated with the ear-decomposition
224, 260
asymmetric traveling salesman problem
501
Asymmetric TSP
530
asymptotic approximation ratio
390
asymptotic approximation scheme
390,
392, 433
asymptotic k-factor approximation
algorithm
390
asymptotic performance ratio
390
augment (network ﬂows)
159
augmenting cycle
193–195
augmenting path
159, 160, 164, 217, 218,
229, 239, 252
average running time
6, 56
b-factor
271
b-ﬂow
191, 208
b-matching
271, see Maximum Weight
b-Matching Problem, 272
b-matching polytope
272, 273, 282
backtracking
3
balance
191
Bar-Yehuda-Even Algorithm
381,
382
barrier
221
base of a blossom
230
base polyhedron
333
basic solution
50, 491, 492
basis
291, 295, 318
basis-superset oracle
303, 304
Bellman’s principle of optimality
144
Berge’s Theorem
218, 229, 230, 239
Berge-Tutte formula
222, 232, 267, 269
Best-In-Greedy Algorithm
303–308,
319, 325, 377
BFS
26, 29, 144, 316, 472
BFS-tree
26
Bin-Packing Problem
425–433, 436,
437
binary clutter
318
binary representation
6
binary search
158, 331, 370
binary string
344, 351
bipartite graph
32, 42
bipartition
32, 44
bipartition inequalities
524
Birkhoff-von-Neumann Theorem
247,
268
bit
344
Bland’s pivot rule
54
block
29, 43
blocking clutter
301, 302, 318
blocking ﬂow
166, 167, 183
blocking polyhedron
318
blossom
230, 250, 251, 260
– inner
232
– out-of-forest
250
– outer
232
blossom forest
– general
232, 250, 260
– special
232, 339
Blossom Shrinking Lemma
230, 232
BlossomPath
251
Boolean formula
355
Boolean variable
355
bottleneck edge
160
bottleneck function
319, 327, 339
bottleneck matching problem
268
bounded LP
92
bounded polyhedron
81
Branch-And-Bound Method
527–529
branch-and-bound tree
529
branch-and-cut
529
branching
18, see Maximum Weight
Branching Problem, 127, 139, 373
branching greedoid
325
branching polytope
138
Breadth-First Search
see BFS
bridge
17, 42
capacity
157, 159
– residual
159
Capacity Scaling Algorithm
201, 202
Carath´eodory’s theorem
63
Cardinality Matching Problem
215,
217, 219, 220, 237, 240
Cayley’s theorem
136
certiﬁcate
352, 366
certiﬁcate-checking algorithm
352, 397,
398
chain
239
Chinese Postman Problem
275, 373
– Directed
210
chordal graph
185, 409
Christofides’ Algorithm
498, 503,
526, 531
chromatic index
383
chromatic number
383, 384
– edge–
383, 384

Subject Index
587
Church’s thesis
346
Chv´atal rank
110
circuit
16, 41, 291, 298
– Hamiltonian
16, 318
– undirected
18–20
circulation
157, 193, 194
city
370
clause
355
Clique
360, 373
clique
15, 16, see Maximum Weight
Clique Problem, see Maximum
Clique Problem
clique polytope
409
clique tree inequalities
524, 529, 532
closed alternating walk
514, 516
closed Jordan curve
33
closed walk
16
closure
291, 297
closure operator
324
closure oracle
304
clutter
301, 302, 318
cocycle basis
20, 21
cocycle space
20
colour
383
colouring
– edge–
383, 384
– vertex–
383, 385, 388
column generation technique
435, 448
comb inequalities
523, 524, 529
comet
564
commodity
443
complement of a decision problem
365
complement of a graph
15
complementary slackness
58
complementary slackness conditions
488
complete graph
15
component
– connected
17, 25, 122
– strongly connected
19, 27–29
computable in polynomial time
6, 345
computation
344, 346
compute a function
6, 345
cone
52
– ﬁnitely generated
52, 53, 60
– polyhedral
52, 53, 60, 92, 93, 112
conjunctive normal form
355
connected component
17, 25, 122
connected digraph
18
connected region
33
connected undirected graph
17
connected vertex set
17
connectivity
– edge–
29, 173, 179, 184
– vertex–
29, 181, 185
connectivity requirements
479
connector
467
coNP
365
coNP-complete
365, 366
conservative weights
143, 147, 276
containing a subgraph
14
Continued Fraction Expansion
68–70, 89
contraction
14
convex combination
60
convex function
340
convex hull
60, 61
convex set
60
cost
368
– reduced
147
covered vertex
215
critical path method (CPM)
182
cross-free family
21–23, 105
crossing submodular function
340
customer (facility location
538
cut
18, 41, see Minimum Capacity Cut
Problem, 180, see Maximum Weight
Cut Problem
– directed
18, 184, 454
– r-
18
– s-t-
18, 159, 161, 182, 483
– T -
279, 288
– undirected
18–20
cut cancelling algorithm
210
cut condition
463
cut criterion
446, 453, 456, 457, 460, 462
cut-incidence matrix
– one-way
105, 106, 113
– two-way
105, 106, 113
cutting plane method
106, 110, 529
cutting stock problem
425
cycle
16
cycle basis
20, 21, 35, 44
cycle matroid
294, 300, 304
cycle space
20
cycle time
154
decidable in polynomial time
345
decidable language
345
decision problem
350, 351
decomposition theorem for polyhedra
63
degree
14
Delaunay triangulation
137
demand
191
demand edge
164, 443
dense graph
25
dependent set
291

588
Subject Index
Depth-First Search
see DFS
derandomization
393, 512
determinant
66, 71
DFS
26, 27, 29, 240
DFS-forest
27
DFS-tree
26
digraph
13
Dijkstra’s Algorithm
26, 145, 146,
149, 153, 201, 447
Dilworth’s Theorem
239
dimension
50
Dinic’s Algorithm
166, 167
direct predecessor
43
direct successor
43
Directed Chinese Postman Problem
210
directed cut
18, 184, 454
Directed Edge-Disjoint Paths Problem
164, 183
directed graph
see digraph
Directed Hamiltonian Path
373
Directed Maximum Weight Cut
Problem
408
Directed Multicommodity Flow
Problem
443, 444
disconnected
17
discrete optimization problem
367
Disjoint Paths Problem
– Directed Edge-
163, 451–453, 455,
461, 462
– Directed Vertex-
163, 462
– Undirected Edge-
163, 455–457,
459, 460, 462
– Undirected Vertex-
163, 457, 461,
462
distance
16, 370
distance criterion
445, 446, 455
distance labeling
169
divide and conquer
9, 263
dominance relation
421
Dominating Set
373
Double-Tree Algorithm
503, 531
doubly stochastic matrix
240, 247
Dreyfus-Wagner Algorithm
470, 471,
496
Drilling Problem
1
dual complementary slackness conditions
58
Dual Fitting Algorithm
544, 546, 549
dual graph
– abstract
42, 46, 301
– planar
40–42, 286, 300
dual independence system
299, 300
dual LP
57, 89
Duality Theorem
57, 59
dynamic ﬂows
206
dynamic programming
144, 151, 419,
470, 530
Dynamic Programming Knapsack
Algorithm
419–421, 436
dynamic tree
167, 168
ear
29
ear-decomposition
29, 44
– alternating
224–226
– associated with the
224, 260
– M-alternating
224–226
– odd
223, 241
– proper
29
Ear-Decomposition Algorithm
224,
225
edge
13
edge–chromatic number
383, 384
edge–colouring
383, 384
Edge-Colouring Problem
383–385,
390
edge–connectivity
29, 173, 179, 184
edge cover
15, 238, 241, see Minimum
Weight Edge Cover Problem
Edge-Disjoint Paths Problem
445,
446, 461, 462
– Directed
163, 451–453, 455, 461, 462
– Undirected
163, 455–457, 459, 460,
462
edge progression
16, 147, 151
Edmonds’ Branching Algorithm
128,
129
Edmonds’ Cardinality Matching
Algorithm
231, 234–238, 249
Edmonds’ Matroid Intersection
Algorithm
309, 312
Edmonds-Karp Algorithm
164–166,
217
Edmonds-Rado Theorem
305, 307, 308
efﬁcient algorithm
6
elementary step
5, 349
ellipsoid
74, 75, 89
Ellipsoid Method
65, 74–76, 81, 83,
84, 331, 388, 432, 522
empty graph
15
empty string
344
endpoints of a path
16
endpoints of an edge
13
enumeration
2, 527
equivalent problems
120
Euclidean Algorithm
68, 69, 73, 97

Subject Index
589
Euclidean ball
74
Euclidean norm
76
Euclidean Steiner Tree Problem
469,
472, 531
Euclidean Traveling Salesman
Problem
506
Euclidean TSP
506, 507, 509, 511, 513,
530
Euler’s Algorithm
30, 31
Euler’s formula
35, 36, 301
Eulerian digraph
459
Eulerian graph
30, 42, 276, 459, 502
Eulerian walk
30, 275, 471, 502, 509
exact algorithm
368
expander graph
403
extreme point
60, 63
f -augmenting cycle
193, 194
f -augmenting path
159, 160
face of a polyhedron
50, 51
face of an embedded graph
33–35
facet
51, 62
facet-deﬁning inequality
51, 524
facility
538
facility cost
539
facility location
114, 537, see (Metric)
Uncapacitated Facility Location
Problem, see Universal Facility
Location Problem, see Metric (Soft-)
Capacitated Facility Location
Problem
factor-critical graph
221, 223–226
Farkas’ Lemma
59
fast matrix multiplication
149
feasible potential
147, 148, 194
feasible solution of an LP
49
feasible solution of an optimization
problem
50, 368
feedback edge set
455
feedback number
455, 461
feedback vertex set
see Minimum
Weight Feedback Vertex Set
Problem
Fernandez-de-la-Vega-Lueker
Algorithm
433, 437
FF
see First-Fit Algorithm
FFD
see First-Fit-Decreasing
Algorithm
Fibonacci heap
123–125, 129, 145, 146,
179
Fibonacci number
89
ﬁnite basis theorem for polytopes
61
ﬁnitely generated cone
52, 53, 60
First-Fit Algorithm
428, 429, 438
First-Fit-Decreasing Algorithm
429,
430, 438
Five Colour Theorem
388
ﬂow
157, see Maximum Flow Problem,
see Minimum Cost Flow Problem, see
Maximum Flow Over Time Problem
– b-
191, 208
– blocking
166, 167, 183
– s-t-
157, 160, 161
ﬂow conservation rule
157
Flow Decomposition Theorem
161
ﬂow over time
– s-t-
207
ﬂows over time
206
Floyd-Warshall Algorithm
149, 150,
154
forbidden minor
45
Ford-Fulkerson Algorithm
160, 161,
164, 181, 217, 497
forest
17, see Maximum Weight Forest
Problem, 135
forest polytope
138
four colour problem
388
Four Colour Theorem
388, 389
Fourier-Motzkin elimination
63
FPAS
see fully polynomial approximation
scheme
fractional b-matching problem
210
Fractional Knapsack Problem
415–417
fractional matching polytope
247
fractional perfect matching polytope
247,
268, 269
Fujishige’s Algorithm
167, 183
full component of a Steiner tree
473
full Steiner tree
473
full-dimensional polyhedron
50, 81
fully polynomial approximation scheme
391, 421, 422, 448, 449
fully polynomial asymptotic approximation
scheme
391, 434, 437
fundamental circuit
20, 21, 43
fundamental cut
21, 282, 480
gain of an alternating walk
514
Gallai-Edmonds decomposition
238, 249,
252
Gallai-Edmonds Structure Theorem
237
Gaussian Elimination
54, 70–74, 80, 89
general blossom forest
232, 250, 260
Generalized Steiner Tree Problem
478, 481

590
Subject Index
girth
35, 278
Goemans-Williamson Algorithm For
Max-Sat
395, 397
Gomory’s cutting plane method
108
Gomory-Chv´atal-truncation
107, 269
Gomory-Hu Algorithm
175, 178, 496
Gomory-Hu tree
173, 175, 178, 281, 282,
285, 480–482, 497
good algorithm
6
good characterization
222, 366
graph
9, 13
– directed
see digraph
– mixed
459, 462
– simple
13
– undirected
13
Graph Scanning Algorithm
24, 25
graphic matroid
294, 301, 306, 318
greatest common divisor
68
greedoid
323–327, 339
greedy algorithm
121, 303, 378
Greedy Algorithm For Greedoids
325, 326, 339
Greedy Algorithm For Set Cover
378
Greedy Algorithm For Vertex Cover
380
greedy augmentation
547, 548, 569
Greedy Colouring Algorithm
385,
409
grid graphs
469
Gr¨otschel-Lov´asz-Schrijver Algo-
rithm
83, 84, 86, 435, 436, 455
half-ellipsoid
75, 78
Hall condition
216
Hall’s Theorem
216, 217
Halting Problem
371
Hamiltonian Circuit
351, 352, 360
Hamiltonian circuit
16, 304, 318
Hamiltonian graph
16, 42
Hamiltonian Path
373
Hamiltonian path
16
handle
524
heap
123
heap order
124
Held-Karp bound
526–528
hereditary graph property
45
Hermite normal form
97
Hilbert basis
93, 112
Hitchcock Problem
192
Hoffman’s circulation theorem
182
Hoffman-Kruskal Theorem
101, 103, 247
Hopcroft-Karp Algorithm
217, 240
Hungarian Method
268
hypergraph
21
in-degree
14
incidence matrix
24, 105
incidence vector
61, 62
incident
13
independence oracle
303, 304, 316
independence system
291, see Maxi-
mization Problem For Independence
Systems, see Minimization Problem
For Independence Systems
– dual
299, 300
independent set
291
induced subgraph
14
infeasible LP
49, 58, 59
inner blossom
232
inner vertex
230, 232
input size
6
instance
351, 368
integer hull
91
Integer Linear Inequalities
351, 352,
365
Integer Programming
91, 92, 351, 370,
432
Integral Flow Theorem
161
integral polyhedron
97–99, 101, 374
integrality constraints
91
interior point algorithms
65, 82
intersection of independence systems
308
intersection of matroids
308
interval graph
409
interval matrix
114
interval packing
114
inverse of a matrix
71
isolated vertex
14
isomorphic graphs
14
Jain’s Algorithm
494, 495
Jain-Vazirani Algorithm
542
Job Assignment Problem
2, 8, 112,
157, 191, 215
Johnson’s Algorithm For Max-Sat
393, 394, 397
Jordan curve theorem
33
k-Center Problem
407, 408
k-connected graph
29, 163
k-edge-connected graph
29, 163
– strongly
183, 462
k-edge-connected subgraph
478, see
Minimum Weight k-Edge-Connected
Subgraph Problem
k-Facility Location Problem
550

Subject Index
591
k-factor approximation algorithm
377
– asymptotic
390
k-Median Problem
550
k-Opt Algorithm
513, 514
k-opt tour
513, 532
k-regular graph
14
k-restricted Steiner tree
473
K-th Heaviest Subset
374
K3,3
36, 40
K5
36, 40
Karmarkar-Karp Algorithm
435–439
Karp reduction
354
key
123
Khachiyan’s theorem
80, 81
Knapsack Approximation Scheme
420,
421
Knapsack Problem
292, 415, 418–421,
423, 435, 436
K¨onigsberg
30
K¨onig’s Theorem
113, 216, 239, 267
Kou-Markowsky-Berman Algorithm
472
Kruskal’s Algorithm
307
Kruskal’s Algorithm
121, 122, 129,
131, 137, 304, 325
Kuratowski’s Theorem
36, 38, 40, 45
L-reducible
402
L-reduction
401, 402, 469, 504
L1-distance
1
L∞-distance
1
Lagrange multipliers
111, 526
Lagrangean dual
111, 114, 526
Lagrangean relaxation
110–112, 114, 423,
525, 526, 550
laminar family
21–23, 105, 250, 260, 491
language
344, 350
Las Vegas algorithm
125, 353
leaf
17, 18
length (of a path or circuit)
16
length (of a string)
344
level (Euclidean TSP)
508
level graph
166
lexicographic rule
54
lexicographical order
3, 12
light Steiner tour
509
Lin-Kernighan Algorithm
515–517,
531
line
13
line graph
16
Linear Inequalities
351, 366
linear inequality system
59, 62, 80
linear program
see LP
Linear Programming
49, 52, 53, 65,
80–82, 351, 366
linear reduction
120
linear time
4
linear-time algorithm
6
linear-time graph algorithm
25
literal
355
local edge-connectivity
173, 184
local optimum
531
local search
513, 518, 519, 531, 553, 559
loop
14, 40, 42
loss of a Steiner tree
474
lower rank function
294
L¨owner-John ellipsoid
75
LP
8, 49
– dual
57, 89
– primal
57
LP Duality Theorem
see Duality
Theorem
Lucchesi-Younger Theorem
454, 455
M-alternating ear-decomposition
224–
226
M-alternating path
217
M-augmenting path
217, 218, 239
MA order
179, 184, 185
Manhattan Steiner Tree Problem
469, 472, 473
Marriage Theorem
217
matching
9, 15, see Cardinality
Matching Problem, 216, see
Maximum Weight Matching
Problem, 265
– b-
271, 272
– perfect
215, 264
matching polytope
265
matrix norm
76
matroid
293, 295–297, 299, 306
matroid intersection
308, see Weighted
Matroid Intersection Problem
Matroid Intersection Problem
309,
312, 314
Matroid Parity Problem
340
Matroid Partitioning Problem
313,
314
matroid polytope
131, 306, 327, 329
Max-3Sat
397, 400, 402, 403
Max-2Sat
408, 410
Max-Flow-Min-Cut property
302, 318
Max-Flow-Min-Cut Theorem
161, 303,
483
Max-Sat
see Maximum Satisfiability,
394, 395, 397

592
Subject Index
maximal
16
Maximization Problem
303–307
Maximization Problem For In-
dependence Systems
292, 421,
422
maximum
16
Maximum Clique Problem
399, 400,
410
Maximum Cut Problem
408, 410
Maximum Flow Over Time Problem
207
Maximum Flow Problem
157–160,
164–167, 169, 172, 455
Maximum Matching Problem
236
Maximum Multicommodity Flow
Problem
448
Maximum Satisfiability (Max-Sat)
393
Maximum Stable Set Problem
399,
400, 406, 407
Maximum Weight b-Matching Problem
271, 273, 285, 288
Maximum Weight Branching Problem
125, 126, 128, 293
Maximum Weight Clique Problem
388
Maximum Weight Cut Problem
286,
374, 408
Maximum Weight Forest Problem
120, 292
Maximum Weight Matching Problem
245, 293
Maximum Weight Stable Set Problem
292, 388
MAXSNP
403
MAXSNP-hard
402, 403, 405–407, 410,
468, 495, 504, 506
median
see Weighted Median Problem
– weighted
416
Menger’s Theorem
162–164, 183, 216,
287
Merge-Sort Algorithm
10, 11
method of conditional probabilities
393
Metric Bipartite TSP
531
Metric Capacitated Facility Location
Problem
559, 561, 568, 569
metric closure
149, 154, 471
Metric k-Facility Location Problem
550, 553
Metric k-Median Problem
553, 555
Metric Soft-Capacitated Facility Location
Problem
569
Metric Soft-Capacitated Facility
Location Problem
559–561
Metric TSP
502–504, 506, 518, 526
Metric Uncapacitated Facility
Location Problem
538, 541, 542,
546, 549, 557, 559
minimal
16
minimal face
52
Minimization Problem
304, 306, 307
Minimization Problem For Indepen-
dence Systems
292, 302
minimum
16
Minimum Capacity Cut Problem
172,
173
Minimum Capacity T -Cut Problem
281, 282, 285, 288
Minimum Cost Flow Problem
192,
195–197, 199, 202, 204–206, 209–211,
341
Minimum Mean Cycle Algorithm
152,
287
Minimum Mean Cycle Problem
151,
152, 154
Minimum Mean Cycle-Cancelling
Algorithm
196, 197
Minimum Set Cover Problem
378, 383
Minimum Spanning Tree Problem
120–122, 125, 129, 137, 292, 471, 472,
503, 526
Minimum Vertex Cover Problem
380,
381, 399, 407, 410, 504
Minimum Weight Arborescence
Problem
126
Minimum Weight Edge Cover Problem
269, 380
Minimum Weight Feedback Vertex Set
Problem
408, 409
Minimum Weight k-Edge-Connected
Subgraph Problem
496
Minimum Weight Perfect Matching
Problem
245–247, 254, 262, 263, 276,
277
Minimum Weight Rooted Arborescence
Problem
126, 131, 132, 137, 183
Minimum Weight Set Cover Problem
378, 381, 382
Minimum Weight T -Join Problem
276–278, 280, 281, 286, 497
Minimum Weight Vertex Cover
Problem
378, 382, 408
minor
36, 45
mixed graph
459, 462
mixed integer program
91
modular function
15, 16, 291, 326, 330
monotone set function
327

Subject Index
593
Monte Carlo algorithm
185, 353
Moore-Bellman-Ford Algorithm
146, 148, 200
Multicommodity Flow Approximation
Scheme
449
Multicommodity Flow Problem
444,
445, 447
multicut
133, 138
multigraph
14
multiplication
349
Multiprocessor Scheduling Problem
438, 439
near-perfect matching
221, 224
nearest neighbour heuristic
502
negative circuit
148, 150
neighbour
13, 14
nested family
21
network
157
network matrix
106, 113
network simplex method
211
Next-Fit Algorithm
427, 428
NF
see Next-Fit Algorithm
no-instance
351
node
13
nondeterministic algorithm
353
nonnegative weights
368
nonsaturating push
171
NP
352, 353, 372, 398
NP-complete
354, 355, 357
– strongly
369
NP-easy
368
NP-equivalent
368
NP-hard
368
– strongly
369, 422
O-notation
4
odd circuit
32, 44
odd cover
31
odd cycle cover
286
odd ear-decomposition
223, 241
odd join
31
Okamura-Seymour Theorem
457, 462
one-sided error
353
one-way cut-incidence matrix
105, 106,
113
online algorithms
430
optimization problem
367
– discrete
367
optimum basic solution
80, 81
optimum solution of an LP
49
optimum solution of an optimization
problem
368
oracle
83, 292, 303, 319, 331, 334, 481,
483
oracle algorithm
83, 354, 368
oracle Turing machine
350
orientation
14, 459, 462
Orlin’s Algorithm
203–206
out-degree
14
out-of-forest blossom
250
outer blossom
232
outer face
34, 45, 457, 471, 496
outer vertex
230, 232
P
351
Padberg-Rao Theorem
282
parallel edges
13
partially ordered set
238
Partition
365, 369
partitionable
313, 319
Patching Lemma
509, 510
path
16
– undirected
18
Path Enumeration Algorithm
3, 5
PCP Theorem
398, 399
PCP(log n, 1)
398
perfect b-matching
271, 275
perfect graph
386, 387, 409
perfect matching
215, 217, 218, 221,
222, see Minimum Weight Perfect
Matching Problem, 264
perfect matching polytope
264, 267, 269,
281
perfect simple 2-matching
271, 285, 522,
529
performance ratio
377
permanent of a matrix
240
permutation
1, 3, 66
permutation matrix
240, 247
Petersen graph
490
Pivot
563
pivot rule
54
planar dual graph
40–42, 286, 300
planar embedding
33, 40, 45
planar graph
33, 40, 46, 300, 301
plant location problem
537
Platonic graphs
45
Platonic solids
45
PLS
531
point
13
pointed polyhedron
52
polar
87, 90
polygon
33, 506
polygonal arc
33
polyhedral combinatorics
61

594
Subject Index
polyhedral cone
52, 53, 60, 92, 93, 112
polyhedron
8, 50
– bounded
81
– full-dimensional
50, 81
– integral
97–99, 101, 374
– rational
50
polyhedron of blocking type
318
polymatroid
327, 331, 332, 339
Polymatroid Greedy Algorithm
327,
328, 331, 332, 339
polymatroid intersection theorem
328
Polymatroid Matching Problem
340
polynomial reduction
354, 368
polynomial transformation
354
polynomial-time algorithm
6, 345, 349
polynomial-time Turing machine
345,
349
polynomially equivalent oracles
304, 319
polynomially equivalent problems
368
polytope
50, 60, 61
portal (Euclidean TSP)
508
poset
238
power set
15
predecessor
43
– direct
43
preﬂow
184
– s-t-
168
Prim’s Algorithm
122, 123, 125, 137,
325, 472
primal complementary slackness conditions
58
primal-dual algorithm
248, 315, 448, 485
Primal-Dual Algorithm For Network
Design
483, 485, 489, 497
primal LP
57
primal-dual algorithm
541
Prime
367
printed circuit boards
1
priority queue
123
probabilistic method
393
probabilistically checkable proof (PCP)
398
problem
– decision
350, 351
– optimization
367
program evaluation and review technique
(PERT)
182
proper alternating walk
515, 516
proper closed alternating walk
515
proper ear-decomposition
29
proper function
479–481
pseudopolynomial algorithm
369, 419,
420, 426, 438
PTAS
see approximation scheme
Push
169, 171
push
– nonsaturating
171
– saturating
171
Push-Relabel Algorithm
169, 171,
172, 184, 211
quickest transshipment problem
208
r-cut
18
radix sorting
12
RAM machine
349, 371
randomized algorithm
125, 185, 219, 352,
353, 393, 395
randomized rounding
395, 461
rank function
291, 296
– lower
294
rank of a matrix
50, 71
rank oracle
304
rank quotient
294, 295, 318
rate of ﬂow
207
rate of growth
4
rational polyhedron
50
reachable
16
realizable demand edge
446
realizing path
164
recursive algorithms
9
recursive function
346
reduced cost
147
region (Euclidean TSP)
508
regular expression
7
Relabel
169, 170
relative performance guarantees
377
representable matroid
294, 318
residual capacity
159
residual graph
159
Restricted Hamiltonian Circuit
517
restriction of a problem
369
reverse edge
159
Robins-Zelikovsky Algorithm
476,
478
root
18, 43, 230
running time
6
running time of graph algorithms
25
s-t-cut
18, 159, 161, 182, 483
s-t-ﬂow
157, 160, 161
s-t-ﬂow over time
207
s-t-path
304
s-t-preﬂow
168
Satisfiability
355
satisﬁable
355

Subject Index
595
satisﬁed clause
355
satisfying edge set (network design)
482
saturating push
171
scalar product
49
scaling technique
168, 201, 210
scheduling
423
scheduling problem
439
Schrijver’s Algorithm
334, 335
Selection Problem
416, 417
semideﬁnite programming
65
separating edge set
18
separating hyperplane
60, 82
separation oracle
83, 87
Separation Problem
83, 86, 88, 281,
282, 331, 332, 435, 447, 448, 455, 480,
481, 495, 522, 524
separator
263
series-parallel graphs
44
service cost
539
set cover
378, see Minimum Set Cover
Problem, see Minimum Weight Set
Cover Problem
Set Packing Problem
410
set system
21
shifted grid (Euclidean TSP)
507
Shmoys-Tardos-Aardal Algorithm
540
Shortest Path
373
shortest path
26
Shortest Path Problem
143, 145, 146,
276, 278, 292, 448
shrinking
14
sign of a permutation
66
simple b-matching
271
simple graph
13
simple Jordan curve
33
Simplex Algorithm
53–57, 65, 71, 82,
432, 448, 519
simplicial order
185
singleton
14
sink
157, 191
skew-symmetric
218
solution of an optimization problem
– feasible
50, 368
– optimum
368
sorting
9, 11, 12
source
157, 191
spanning subgraph
14
spanning tree
18, 43, 119, see Minimum
Spanning Tree Problem, 133, 136, 301,
373, 525
spanning tree polytope
129, 131, 138
spanning tree solution
211
sparse graph
25
special blossom forest
232, 233, 339
Sperner’s Lemma
239
Stable Set
358, 373
stable set
15, 16, 238, see Maximum
Weight Stable Set Problem, see
Maximum Stable Set Problem
stable set polytope
387
standard embedding
40
star
17, 564
Steiner points
467, 495
Steiner ratio
472, 473
Steiner tour
509
– light
509
Steiner tree
see Steiner Tree Problem,
467, 471, 472
Steiner Tree Problem
293, 468–472,
476, 478, 495
Stirling’s formula
3
strong perfect graph theorem
386
strongly connected component
19, 27–29
Strongly Connected Component
Algorithm
27–29, 483
strongly connected digraph
19, 44, 46,
461, 462
strongly connected graph
454
strongly k-edge-connected graph
183,
462
strongly NP-complete
369
strongly NP-hard
369, 422
strongly polynomial-time algorithm
6, 82
subdeterminant
92
subdivision
44, 45
subgradient optimization
111, 526
subgraph
14
– induced
14
– k-edge-connected
478
– spanning
14
subgraph degree polytope
286
submodular ﬂow
340, 341
Submodular Flow Problem
340, 341
submodular function
15, 181, 296, 327,
328, 330–332, 340
Submodular Function Minimization
Problem
331–334, 336
Subset-Sum
364, 369
subtour inequalities
521, 522, 529
subtour polytope
521, 526
Successive Shortest Path Algorithm
199, 200, 246
successor
43
– direct
43
sum of matroids
313

596
Subject Index
supermodular function
15, 330
– weakly
479, 491, 492
supply
191
supply edge
164, 443
supporting hyperplane
50
Survivable Network Design Problem
467, 479, 485, 489, 495–497
symmetric submodular function
337, 338
system of distinct representatives
239
system of linear equations
71
T -cut
279, see Minimum Capacity
T -Cut Problem, 288
T -join
276, see Minimum Weight
T -Join Problem, 279, 287, 303
T -join polyhedron
280
TDI-system
99–101, 103, 113, 131, 266,
267, 328, 341, 409, 463
terminal (Disjoint Paths Problem)
164
terminal (Steiner tree)
467
test set
94
theta-function
388
-notation
4
tight edge (weighted matching)
248, 260
tight set (network design)
490
time complexity
6
time-expanded network
212
tooth
524
topological order
20, 28, 29, 483
total dual integrality
306
totally dual integral system
see
TDI-system
totally unimodular matrix
101–106, 161
tour
16, 521
tournament
44
transportation problem
192
transshipment problem
192
transversal
318, 319
traveling salesman polytope
519, 525
Traveling Salesman Problem (TSP)
292, 370, 501, 513, 515, 517, 518, 526,
528, 529
tree
17, 24, 325
tree-decomposition
44, 457
tree-representation
22, 105, 250, 487, 492
tree-width
44, 462
TreePath
252, 260
triangle inequality
154, 471, 495, 502, 530
truth assignment
355
TSP
see Traveling Salesman Problem
TSP Facets
525
Turing machine
343–346
Turing reduction
354
Tutte condition
221, 222
Tutte matrix
218
Tutte set
222
Tutte’s Theorem
221, 222, 241
Two-Commodity Flow Theorem
463
two-tape Turing machine
346, 348
two-way cut-incidence matrix
105, 106,
113
unbounded face
34
unbounded LP
49, 58, 59
Uncapacitated Facility Location
Problem
115, 538–540, 542, 544, 550
undecidable problem
371
underlying undirected graph
14
Undirected Chinese Postman Problem
276, 286
undirected circuit
18–20
undirected cut
18–20
undirected graph
13
Undirected Minimum Mean Cycle
Problem
287
Undirected Multicommodity Flow
Problem
444, 463
undirected path
18
uniform matroid
294, 318
unimodular matrix
96, 113
unimodular transformation
96, 113
union of matroids
313
Universal Facility Location Problem
559, 567
Update
254
value of an s-t-ﬂow
157
vector matroid
294
vertex
13
vertex–colouring
383, 385, 388
Vertex-Colouring Problem
383, 385,
386, 388
vertex–connectivity
29, 181, 185
Vertex Cover
359, 360
vertex cover
15, 216, 326, see Minimum
Weight Vertex Cover Problem, see
Minimum Vertex Cover Problem
vertex-disjoint paths
162
Vertex-Disjoint Paths Problem
462
– Directed
163, 462
– Undirected
163, 457, 461, 462
vertex of a polyhedron
50, 52, 56, 61, 63
violated vertex set (network design)
482
Vizing’s Theorem
384, 385, 390
VLSI-design
154, 469
Vorono¨ı diagram
137

Subject Index
597
walk
16
– closed
16
warehouse location problem
537
weak duality
54
Weak Optimization Problem
83, 84,
86, 435
weak perfect graph theorem
386
weak separation oracle
83
Weak Separation Problem
83, 84, 435,
436
weakly supermodular function
479, 491,
492
weight
368
Weighted Matching Algorithm
254,
260, 262–265, 268, 275, 278, 279, 285
Weighted Matroid Intersection
Algorithm
315, 316, 319, 340
Weighted Matroid Intersection
Problem
314–316
weighted median
416
Weighted Median Algorithm
417
Weighted Median Problem
416
well-rounded Euclidean TSP instance
506, 507
word
344
worst-case running time
6
Worst-Out-Greedy Algorithm
136,
304, 306, 307
yes-instance
351


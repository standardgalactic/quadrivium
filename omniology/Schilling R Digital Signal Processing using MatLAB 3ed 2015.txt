
Digital Signal Processing
Using MATLAB®
Third Edition
Robert J. Schilling and Sandra L. Harris
Clarkson University
Potsdam, NY
Australia ● Brazil ● Mexico ● Singapore ● United Kingdom ● United States
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

This is an electronic version of the print textbook. Due to electronic rights restrictions, some third party content may be suppressed. Editorial 
review has deemed that any suppressed content does not materially affect the overall learning experience. The publisher reserves the right to 
remove content from this title at any time if subsequent rights restrictions require it. For valuable information on pricing, previous
editions, changes to current editions, and alternate formats, please visit www.cengage.com/highered to search by
ISBN#, author, title, or keyword for materials in your areas of interest.
Important Notice: Media content referenced within the product description or the product text may not be available in the eBook version.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

© 2017, 2012 Cengage Learning®
WCN: 02-200-203
ALL RIGHTS RESERVED. No part of this work covered by the copyright 
herein may be reproduced, transmitted, stored, or used in any form 
or by any means graphic, electronic, or mechanical, including but not 
limited to photocopying, recording, scanning, digitizing, taping, Web 
distribution, information networks, or information storage and retrieval 
systems, except as permitted under Section 107 or 108 of the 1976 
United States Copyright Act, without the prior written permission of 
the publisher.
Library of Congress Control Number: 2015949697
ISBN:  978-1-305-63519-7
Cengage Learning 
20 Channel Center Street 
Boston, MA 02210 
USA
Cengage Learning is a leading provider of customized learning solutions 
with employees residing in nearly 40 different countries and sales in more 
than 125 countries around the world. Find your local representative at 
www.cengage.com
Cengage Learning products are represented in Canada by  
Nelson  Education, Ltd.
To learn more about Cengage Learning Solutions, visit  
www.cengage.com/engineering
Purchase any of our products at your local college store or at our pre-
ferred online store www.cengagebrain.com
MATLAB is a registered trademark of The MathWorks, 3 Apple Hill Drive, 
Natick MA 01760. 
Unless otherwise noted, all items © Cengage Learning.
Digital Signal Processing Using MATLAB®, 
Third Edition
Robert J. Schilling and Sandra L. Harris
Product Director, Global Engineering:  
Timothy L. Anderson
Senior Content Developer: Mona Zeftel
Associate Media Content Developer:  
Ashley Kaupert
Product Assistant: Teresa Versaggi
Marketing Manager: Kristin Stine
Director, Content and Media Production: 
Sharon L. Smith
Content Project Manager: Nadia Saloom
Production Service:  RPK Editorial Services, Inc.
Copyeditor: Harlan James
Proofreader: Martha McMaster
Indexer: Rose Kernan
Compositor: MPS Limited
Senior Art Director: Michelle Kunkler
Internal Designer: Carmela Pereira
Cover Designer: Tin Box Studio
Cover Image: agsandrew/Shutterstock.com
Intellectual Property
 
Analyst: Christine Myaskovsky
 
Project Manager: Sarah Shainwald
Text and Image Permissions Researcher: 
Kristiina Paul
Senior Manufacturing Planner: Doug Wilke
For product information and technology assistance, contact us at 
Cengage Learning Customer & Sales Support, 1-800-354-9706.
For permission to use material from this text or product,  
submit all requests online at www.cengage.com/permissions  
Further permissions questions can be emailed to 
permissionrequest@cengage.com
Printed in the United States of America 
Print Number: 01  Print Year: 2015
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

To our mothers 
for all they have done for us
Bette Rose Schilling
and
Florence E. Harris
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Digital signal processing, more commonly known as DSP, is a 
field of study with increasingly widespread applications in our 
technological world. This book focuses on the development, im-
plementation, and application of modern DSP techniques. The 
textbook structure consists of three major parts as summarized 
in Table 1.
Audience and Prerequisites
This book is targeted primarily toward second-semester juniors, 
seniors, and beginning graduate students in electrical and com-
puter engineering and related fields that rely on digital signal 
processing. It is assumed that the students have taken a circuits 
course, or a signals and systems course, or a mathematics course 
that includes an introduction to both the Laplace transform and 
the Fourier transform. There is enough material, and sufficient 
flexibility in the way it can be covered, to provide for courses 
of different lengths without adding supplementary material. 
Exposure to MATLAB programming is useful, but it is not  
Preface
v
Table 1: Textbook 
Structure
Part 
Name and Chapters  
I 
Signal and System Analysis 
1. Signal Processing 
2. Discrete-time Systems in the Time Domain 
3. Discrete-time Systems in the Frequency Domain 
4. Fourier Transforms and Signal Spectra 
II
Filter Design
5. Filter Types and Characteristics 
6. FIR Filter Design 
7. IIR Filter Design 
III 
Advanced Signal Processing 
8. Multirate Signal Processing 
9. Adaptive Signal Processing 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

vi    Preface
essential. Graphical user interface (GUI) modules are included at the end of each chapter 
that allow students to interactively explore signal processing concepts and techniques  
without any need for programming. MATLAB computation problems are supplied for 
those users who are familiar with MATLAB and are interested in developing their own 
DSP programs.
This book is written in an engaging informal style that endeavors to provide motiva-
tion for each new topic and features a careful transition between topics. Significant terms 
are set apart for convenient reference using Margin Notes and Definitions. Important 
results are stated as Propositions in order to highlight their significance, and Algorithms 
are included to summarize the steps used to implement design procedures. In order to 
motivate students with examples that are of direct interest, many of the examples feature 
the processing of speech and music. This theme is also a focus of the DSP Companion 
course software, which includes a facility for recording and playing back speech and  
sound. This way, students can experience directly the effects of various signal processing 
techniques.
Chapter Structure
All of the chapters follow the template shown in Figure 1. Each chapter starts with a 
brief list of the topics covered. This is followed by a motivation section that introduces 
one or more examples of practical problems that can be solved using techniques covered 
in the chapter. The main body of each chapter is used to introduce a series of analysis 
tools and signal processing techniques. Within these sections, the analysis methods and 
processing techniques evolve from simple to more complex. Sections near the end of the 
chapter marked with a * denote more advanced or more specialized material that can be 
Figure 1: Chapter 
Structure
Problems
Chapter summary
GUI software,
case studies
Concepts,
techniques,
examples
Motivation
Topics
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Preface    vii
skipped without loss of continuity. Numerous examples are used throughout to illustrate 
the principles involved.
Near the end of each chapter is a GUI software and Case Studies section that intro-
duces GUI Modules designed to allow the student to interactively explore the chapter 
concepts and techniques without any need for programming. The GUI Modules fea-
ture a common user interface that is simple to use and easy to learn. Results exported 
from one module can be imported into other modules. This section also includes Case 
Study examples that present complete solutions to practical problems in the form of  
MATLAB programs. The Chapter Summary concisely reviews important concepts and 
includes a table of student learning outcomes for each section. The chapter concludes 
with an extensive set of homework problems separated into three categories and cross- 
referenced to the sections. The Analysis and Design problems can be done by hand or 
with a calculator. They are designed to test (and in some cases extend) student under-
standing of the chapter material. The GUI Simulation problems allow the student to 
interactively explore processing and design techniques using the chapter GUI modules. 
No programming is required for these problems. MATLAB Computation problems are 
provided that require students to write programs that apply the signal processing tech-
niques covered in the chapter. Complete solutions to selected problems, marked with the 
 symbol, are available using the DSP Companion software.
DSP Companion Software
One of the unique features of this textbook is a highly integrated collection of course 
software called the DSP Companion. It is available on the publisher’s companion web 
site, and it features a menu-based graphical user interface driver program called g_dsp. 
The DSP Companion runs under MATLAB and features supplementary course material 
that can be used both inside the classroom by the instructor and outside the classroom 
by the student. The DSP Companion provides direct access to the textbook material as 
well as additional features that allow for class demonstrations and interactive student 
exploration of analysis and design concepts. The DSP Companion is self-contained in 
the sense that only MATLAB itself is required; there is no need for access to optional 
MATLAB toolboxes.
The menu options of the DSP Companion are listed in Table 2. The Settings 
option allows the user to configure the DSP Companion by selecting operating  
modes and default folders for exporting, importing, and printing results. The GUI 
Modules option is used to run the chapter graphical user interface modules. In the 
Examples option, MATLAB code for all of the examples appearing in the text can 
be viewed and executed. The Figures and the Tables options are used to display pdf 
files of all of the figures and tables that appear in the text. Similarly, the Definitions 
option displays definitions, propositions, and algorithms from the text. The next two 
menu options are only available with the Instructor version of DSP Companion. The 
Presentations option displays PowerPoint lectures, with each presentation covering a 
section of a chapter, while the Solutions option displays solutions to all of the end 
of chapter problems. For the Student version of DSP Companion, there is a Marked 
Problems option that display solutions to selected end of chapter problems. The Doc-
umentation option provides user help for the DSP Companion functions and the 
GUI modules. Finally, the Web option allows the user to download the latest version 
of the DSP Companion from the publisher web site.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

viii    Preface
Acknowledgments
This project has been several years in the making, and many individuals have contributed 
to its completion. The reviewers commissioned by Brooks/Cole and Cengage Learning 
made numerous thoughtful and insightful suggestions that were incorporated into the 
final draft. Thanks to graduate students Joe Tari, Rui Guo, and Lingyun Bai for helping 
review the initial DSP Companion software. Special thanks to Bill Stenquist of Brooks/
Cole who worked closely with us to bring the first edition to completion. We thank the re-
viewers for this edition for their helpful comments: Selim Awad, University of Michigan- 
Dearborn; Frederick Harris, San Diego State University; Thomas Hebert, University of 
Houston; Vishal Monga, Penn State; and Tokunbo Ogunfunmi, Santa Clara University. 
The latest edition has a number of important improvements.
We wish to acknowledge and thank our Global Engineering team at Cengage 
Learning for their dedication to this new edition: Timothy Anderson, Product Director; 
Mona Zeftel, Senior Content Developer; Jana Lewis, Content Project Manager; Kristin 
Stine, Marketing Manager; Elizabeth Murphy, Engagement Specialist; Ashley Kaupert,  
Associate Media Content Developer; Teresa Versaggi and Alexander Sham, Product  
Assistants; and Rose Kernan of RPK Editorial Services, Inc. They have skillfully guided 
every aspect of this text’s development and production to successful completion.
Robert J. Schilling 
Sandra L. Harris 
Potsdam, NY 
Option 
Description 
Type 
Links 
Settings 
Adjust default settings 
 
 
GUI Modules 
Graphical user interface modules 
.m, .mat 
11 
Examples 
View and run MATLAB examples 
.m, .mat 
120 
Figures 
View all figures 
.pdf 
431 
Tables 
View all tables 
.pdf 
75 
Definitions 
View definitions, propositions, algorithms 
.pdf 
58 
Presentations 
Display PowerPoint lectures (instructor) 
.pptx 
91 
Solutions 
Solutions to all problems (instructor) 
.pdf 
487 
Marked Problems 
Solutions to selected problems (student) 
.pdf 
54 
Documentation 
Help for DSP Companion functions 
.m 
124 
Web 
Software updates 
url 
6 
Exit 
Exit DSP Companion 
 
 
Table 2: DSP 
Companion Menu 
Options
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Margin Contents xix
Part I: Signal and System Analysis 1
1. Signal Processing 3
 
1.1 
Motivation 3
 
1.1.1 
Digital and Analog Processing 4
 
1.1.2 
Total Harmonic Distortion (THD) 6
 
1.1.3 
A Notch Filter 7
 
1.1.4 
Active Noise Control 7
 
1.1.5 
Video Aliasing 9
 
1.2 
Signals and Systems 11
 
1.2.1 
Signal Classification 11
 
1.2.2 
System Classification 16
 
1.3 
Sampling of Continuous-time Signals 21
 
1.3.1 
Sampling as Modulation 21
 
1.3.2 
Aliasing 23
 
1.4 
Reconstruction of Continuous-time Signals 27
 
1.4.1 
Reconstruction Formula 27
 
1.4.2 
Zero-order Hold 29
 
1.4.3 
Delayed First-order Hold 32
 
1.5 
Prefilters and Postfilters 35
 
1.5.1 
Anti-aliasing Filter 35
 
1.5.2 
Anti-imaging Filter 39
 
*1.6 
DAC and ADC Circuits 41
 
1.6.1 
Digital-to-analog Converter (DAC) 41
 
1.6.2 
Analog-to-digital Converter (ADC) 43
Contents
ix
*Sections marked with a * contain more advanced or specialized material that can be skipped without  
loss of continuity.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

x    Contents
 
1.7 
DSP Companion 48
 
1.7.1 
Installation 48
 
1.7.2 
Menu Options 48
 
1.7.3 
GUI Modules 50
 
1.7.4 
Functions 50
 
1.8 
GUI Modules and Case Studies 51
 
1.9 
Chapter Summary 59
 
1.10 
Problems 61
 
1.10.1 Analysis and Design 61
 
1.10.2 GUI Simulation 66
 
1.10.3 MATLAB Computation 68
2. Discrete-time Systems in the Time Domain 70
 
2.1 
Motivation 70
 
2.1.1 
Home Mortgage 71
 
2.1.2 
Range Measurement with Radar 72
 
2.2 
Discrete-time Signals 74
 
2.2.1 
Signal Classification 74
 
2.2.2 
Common Signals 79
 
2.3 
Discrete-time Systems 82
 
2.4 
Difference Equations 86
 
2.4.1 
Zero-input Response 87
 
2.4.2 
Zero-state Response 90
 
2.5 
Block Diagrams 95
 
2.6 
The Impulse Response 97
 
2.6.1 
FIR Systems 97
 
2.6.2 
IIR Systems 99
 
2.7 
Convolution 102
 
2.7.1 
Linear Convolution 102
 
2.7.2 
Circular Convolution 105
 
2.7.3 
Zero Padding 107
 
2.7.4 
Deconvolution 110
 
2.7.5 
Polynomial Arithmetic 111
 
2.8 
Correlation 113
 
2.8.1 
Linear Cross-correlation 113
 
2.8.2 
Circular Cross-correlation 116
 
2.9 
Stability in the Time Domain 119
 
2.10 
GUI Modules and Case Studies 121
 
2.11 
Chapter Summary 130
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Contents    xi
 
2.12 
Problems 133
 
2.12.1 Analysis and Design 134
 
2.12.2 GUI Simulation 141
 
2.12.3 MATLAB Computation 143
3. Discrete-time Systems in the Frequency Domain 147
 
3.1 
Motivation 147
 
3.1.1 
Satellite Attitude Control 148
 
3.1.2 
Modeling the Vocal Tract 149
 
3.2 
Z-transform Pairs 151
 
3.2.1 
Region of Convergence 152
 
3.2.2 
Common Z-transform Pairs 154
 
3.3 
Z-transform Properties 159
 
3.3.1 
General Properties 159
 
3.3.2 
Causal Properties 164
 
3.4 
Inverse Z-transform 166
 
3.4.1 
Noncausal Signals 166
 
3.4.2 
Synthetic Division 167
 
3.4.3 
Partial Fractions 169
 
3.4.4 
Residue Method 174
 
3.5 
Transfer Functions 177
 
3.5.1 
The Transfer Function 177
 
3.5.2 
Zero-state Response 179
 
3.5.3 
Poles, Zeros, and Modes 181
 
3.5.4 
DC Gain 183
 
3.6 
Signal Flow Graphs 185
 
3.7 
Stability in the Frequency Domain 188
 
3.7.1 
Input-output Representations 188
 
3.7.2 
BIBO Stability 189
 
3.7.3 
The Jury Test 192
 
3.8 
Frequency Response 195
 
3.8.1 
Frequency Response 195
 
3.8.2 
Sinusoidal Inputs 197
 
3.8.3 
Periodic Inputs 200
 
*3.9 
System Identification 203
 
3.9.1 
Least-squares Fit 204
 
3.9.2 
Persistently Exciting Inputs 207
 
3.10 
GUI Modules and Case Studies 209
 
3.11 
Chapter Summary 217
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

xii    Contents
 
3.12 
Problems 220
 
3.12.1 Analysis and Design 220
 
3.12.2 GUI Simulation 231
 
3.12.3 MATLAB Computation 232
4. Fourier Transforms and Spectral Analysis 234
 
4.1 
Motivation 234
 
4.1.1 
Fourier Series 235
 
4.1.2 
DC Wall Transformer 236
 
4.1.3 
Frequency Response 238
 
4.2 
Discrete-time Fourier Transform (DTFT) 239
 
4.2.1 
DTFT 239
 
4.2.2 
Properties of the DTFT 243
 
4.3 
Discrete Fourier Transform (DFT) 247
 
4.3.1 
DFT 247
 
4.3.2 
Matrix Formulation 250
 
4.3.3 
Fourier Series and Discrete Spectra 252
 
4.3.4 
DFT Properties 255
 
4.4 
Fast Fourier Transform (FFT) 262
 
4.4.1 
Decimation in Time FFT 262
 
4.4.2 
FFT Computational Effort 266
 
4.4.3 
Alternative FFT Implementations 268
 
4.5 
Fast Convolution and Correlation 269
 
4.5.1 
Fast Convolution 269
 
*4.5.2 
Fast Block Convolution 273
 
4.5.3 
Fast Correlation 276
 
4.6 
White Noise 281
 
4.6.1 
Uniform White Noise 281
 
4.6.2 
Gaussian White Noise 285
 
4.7 
Auto-correlation 289
 
4.7.1 
Auto-correlation of White Noise 289
 
4.7.2 
Power Density Spectrum 291
 
4.7.3 
Extracting Periodic Signals from Noise 292
 
4.8 
Zero Padding and Spectral Resolution 298
 
4.8.1 
Frequency Response Using the DFT 298
 
4.8.2 
Zero Padding 302
 
4.8.3 
Spectral Resolution 303
 
4.9 
The Spectrogram 307
 
4.9.1 
Data Windows 307
 
4.9.2 
Spectrogram 308
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Contents    xiii
 
*4.10 
Power Density Spectrum Estimation 312
 
4.10.1 Bartlett’s Method 312
 
4.10.2 Welch’s Method 316
 
4.11 
GUI Modules and Case Studies 320
 
4.12 
Chapter Summary 326
 
4.13 
Problems 329
 
4.13.1 Analysis and Design 329
 
4.13.2 GUI Simulation 336
 
4.13.3 MATLAB Computation 338
Part II: Filter Design 343
5. Filter Types and Characteristics 345
 
5.1 
Motivation 345
 
5.1.1 
Filter Design Specifications 346
 
5.1.2 
Filter Realization Structures 348
 
5.2 
Frequency-selective Filters 351
 
5.2.1 
Linear Design Specifications 352
 
5.2.2 
Logarithmic Design Specifications (dB) 357
 
5.3 
Linear-phase and Zero-phase Filters 359
 
5.3.1 
Linear Phase 359
 
5.3.2 
Amplitude Response 360
 
5.3.3 
Linear-phase Zeros 363
 
5.3.4 
Zero-phase Filters 365
 
5.4 
Minimum-phase and Allpass Filters 368
 
5.4.1 
Minimum-phase Filters 368
 
5.4.2 
Allpass Filters 372
 
5.4.3 
Inverse Systems and Equalization 376
 
5.5 
Quadrature Filters 377
 
5.5.1 
Differentiator 377
 
5.5.2 
Hilbert Transformer 378
 
5.5.3 
Digital Oscillator 381
 
5.6 
Notch Filters and Resonators 384
 
5.6.1 
Notch Filters 384
 
5.6.2 
Resonators 387
 
5.7 
Narrowband Filters and Filter Banks 389
 
5.7.1 
Narrowband Filters 389
 
5.7.2 
Filter Banks 391
 
5.8 
Adaptive Filters 394
 
5.8.1 
Transversal Filters 394
 
5.8.2 
Pseudo-filters 396
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

xiv    Contents
 
5.9 
GUI Modules and Case Studies  397
 
5.10 
Chapter Summary 402
 
5.11 
Problems 405
 
5.11.1 Analysis and Design 405
 
5.11.2 GUI Simulation 413
 
5.11.3 MATLAB Computation 415
6. FIR Filter Design 417
 
6.1 
Motivation 417
 
6.1.1 
Numerical Differentiators 418
 
6.1.2 
Signal-to-noise Ratio 420
 
6.2 
Windowing Method 422
 
6.2.1 
Truncated Impulse Response 423
 
6.2.2 
Windowing 426
 
6.3 
Frequency-sampling Method 434
 
6.3.1 
Frequency Sampling 434
 
6.3.2 
Transition-band Optimization 437
 
6.4 
Least-squares Method 441
 
6.5 
Optimal Equiripple Filter Design 445
 
6.5.1 
Minimax Error Criterion 445
 
6.5.2 
Parks-McClellan Algorithm 447
 
6.6 
Differentiators and Hilbert Transformers 454
 
6.6.1 
Differentiator Design 454
 
6.6.2 
Hilbert Transformer Design 457
 
6.7 
Quadrature Filter Design 460
 
6.7.1 
Generation of a Quadrature Pair 460
 
6.7.2 
Quadrature Filter Design 462
 
6.7.3 
Equalizer Design 466
 
6.8 
Filter Realization Structures 470
 
6.8.1 
Direct Forms 470
 
6.8.2 
Cascade Form 472
 
6.8.3 
Lattice Form 474
 
*6.9 
Finite Word Length Effects 477
 
6.9.1 
Binary Number Representation 478
 
6.9.2 
Input Quantization Error 479
 
6.9.3 
Coefficient Quantization Error 482
 
6.9.4 
Roundoff Error, Overflow, and Scaling 486
 
6.10 
GUI Modules and Case Studies 489
 
6.11 
Chapter Summary 495
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Contents    xv
 
6.12 
Problems 499
 
6.12.1 Analysis and Design 499
 
6.12.2 GUI Simulation 504
 
6.12.3 MATLAB Computation 506
7. IIR Filter Design 511
 
7.1 
Motivation 511
 
7.1.1 
Tunable Plucked-string Filter 512
 
7.1.2 
Colored Noise 514
 
7.2 
Filter Design by Pole-zero Placement 516
 
7.2.1 
Resonator 516
 
7.2.2 
Notch Filter 519
 
7.2.3 
Comb Filters 522
 
7.3 
Filter Design Parameters 526
 
7.4 
Classical Analog Filters 528
 
7.4.1 
Butterworth Filters 529
 
7.4.2 
Chebyshev-I Filters 533
 
7.4.3 
Chebyshev-II Filters 536
 
7.4.4 
Elliptic Filters 538
 
7.5 
Bilinear Transformation Method 540
 
7.6 
Frequency Transformations 547
 
7.6.1 
Analog Frequency Transformations 547
 
7.6.2 
Digital Frequency Transformations 550
 
7.7 
Filter Realization Structures 553
 
7.7.1 
Direct Forms 553
 
7.7.2 
Parallel Form 556
 
7.7.3 
Cascade Form 559
 
*7.8 
Finite Word Length Effects 562
 
7.8.1 
Coefficient Quantization Error 563
 
7.8.2 
Roundoff Error, Overflow, and Scaling 566
 
7.8.3 
Limit Cycles 570
 
7.9 
GUI Modules and Case Studies 572
 
7.10 
Chapter Summary 579
 
7.11 
Problems 582
 
7.11.1 Analysis and Design 582
 
7.11.2 GUI Simulation 587
 
7.11.3 MATLAB Computation 590
Part III: Advanced Signal Processing 593
8. Multirate Signal Processing 595
 
8.1 
Motivation 595
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

xvi    Contents
 
8.1.1 
Narrowband Filters 596
 
8.1.2 
Intersample Delay Systems 598
 
8.2 
Integer Sampling Rate Converters 599
 
8.2.1 
Sampling Rate Decimator 599
 
8.2.2 
Sampling Rate Interpolator 600
 
8.3 
Rational Sampling Rate Converters 603
 
8.3.1 
Single-stage Converters 603
 
8.3.2 
Multistage Converters 606
 
8.4 
Polyphase Filters 608
 
8.4.1 
Polyphase Decimator 608
 
8.4.2 
Polyphase Interpolator 612
 
8.5 
Narrowband Filters 614
 
8.6 
Filter Banks 616
 
8.6.1 
Analysis and Synthesis Banks 616
 
8.6.2 
Subfilter Design 620
 
8.6.3 
Polyphase Representation 621
 
8.7 
Perfect Reconstruction Filter Banks 624
 
8.7.1 
Time-division Multiplexing 624
 
8.7.2 
Perfect Reconstruction 626
 
8.8 
Transmultiplexors 630
 
*8.9 
Oversampled A-to-D Converters 634
 
8.9.1 
Anti-aliasing Filters 634
 
8.9.2 
A Sigma-delta ADC 638
 
*8.10 
Oversampled D-to-A Converters 642
 
8.10.1 Anti-imaging Filters 642
 
8.10.2 Passband Equalization 644
 
8.11 
GUI Modules and Case Studies 646
 
8.12 
Chapter Summary 652
 
8.13 
Problems 655
 
8.13.1 Analysis and Design 655
 
8.13.2 GUI Simulation 663
 
8.13.3 MATLAB Computation 664
9. Adaptive Signal Processing 667
 
9.1 
Motivation 667
 
9.1.1 
System Identification 668
 
9.1.2 
Channel Equalization 669
 
9.1.3 
Signal Prediction 670
 
9.1.4 
Noise Cancellation 670
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Contents    xvii
 
9.2 
Mean Square Error 671
 
9.2.1 
Adaptive Transversal Filters 671
 
9.2.2 
Cross-correlation Revisited 672
 
9.2.3 
Mean Square Error 673
 
9.3 
Least Mean Square (LMS) Method 678
 
9.4 
Performance Analysis of LMS Method 682
 
9.4.1 
Step Size 682
 
9.4.2 
Convergence Rate 686
 
9.4.3 
Excess Mean Square Error 689
 
9.5 
Modified LMS Methods 692
 
9.5.1 
Normalized LMS Method 692
 
9.5.2 
Correlation LMS Method 694
 
9.5.3 
Leaky LMS Method 698
 
9.6 
Adaptive Filter Design with Pseudo-filters 701
 
9.6.1 
Pseudo-filters 701
 
9.6.2 
Adaptive Filter Design 703
 
9.6.3 
Linear-phase Adaptive Filters 705
 
9.7 
Recursive Least Squares (RLS) Method 708
 
9.7.1 
Performance Criterion 708
 
9.7.2 
Recursive Formulation 709
 
*9.8 
Active Noise Control 714
 
9.8.1 
The Filtered-x LMS Method 714
 
9.8.2 
Secondary-path Identification 716
 
9.8.3 
Signal-synthesis Method 719
 
*9.9 
Adaptive Function Approximation 723
 
9.9.1 
Nonlinear Functions 724
 
9.9.2 
Radial Basis Functions (RBFs) 726
 
9.9.3 
Raised-cosine RBF Networks 728
 
*9.10 
Nonlinear System Identification (NLMS) 734
 
9.11 
GUI Modules and Case Studies 740
 
9.12 
Chapter Summary 744
 
9.13 
Problems 748
 
9.13.1 Analysis and Design 748
 
9.13.2 GUI Simulation 753
 
9.13.3 MATLAB Computation 754
References and Further Reading 761
Appendix 1 Transform Tables 765
Appendix 2 Mathematical Identities 774
Index 777
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Margin Contents
Number 
Term 
Symbol 
Page
 1.1  
Causal signal  
xa(t) 
 14
 1.2  
Linear system  
S 
 
 16
 1.3  
Time-invariant system  
 S  
  
 17
 1.4  
Stable system  
 S  
  
 18
 1.5  
Frequency response  
 Ha(f)  
  
 19
 1.6  
Impulse response  
 ha(t)  
  
 19
 1.7  
Bandlimited signal  
 xa(t)  
  
 24
 1.8  
Transfer function  
 Ha(S)  
  
 29
 2.1  
Impulse response  
 h(k)  
  
 97
 2.2  
FIR and IIR systems  
S 
  
 98
 2.3  
Linear convolution  
 h(k) * x(k)    
103
 2.4  
Circular convolution  
 h(k) o x(k)    
106
 2.5  
Linear cross-correlation  
 ryx(k)  
  
113
 2.6  
Circular cross-correlation  
 cyz(k)  
  
116
 2.7  
BIBO stability  
 uuhuu1 , `    
120
 3.1  
Z-transform  
 X(z)  
  
151
 3.2  
Transfer function  
 H(z)  
  
178
 3.3  
Frequency response  
 H(f)  
  
195
 4.1  
Discrete-time Fourier transform (DTFT)  
 X(f)  
  
239
 4.2  
Discrete Fourier transform (DFT)  
 X(i)  
  
249
 4.3  
Expected value  
 E [fx(k)]    
282
 4.4  
Circular auto-correlation  
 cxx(k)  
  
289
 4.5  
Spectrogram  
 G(m, i)    
308
 5.1  
Group delay  
 D(f)  
  
359
 5.2  
Linear-phase filter  
 H(z)  
  
360
 5.3  
Minimum-phase filter  
 H(z)  
  
369
 5.4 
Allpass Filter 
H(z) 
 
372
 6.1  
Signal-to-noise ratio  
 SNR(y)  
  
420
 6.2  
Quantization operator  
 QN(x)  
  
479
 9.1  
Random cross-correlation  
 ryx(i)  
  
672
xix
Table I: Definitions
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Number 
Description  
  Page 
 1.1  
 Signal Sampling  
  
 
 25
 1.2  
 Signal Reconstruction  
  
 
 28
 2.1  
 Stability in the Time Domain  
  
 
120
 3.1  
 Stability in the Frequency Domain  
  
 
190
 3.2  
 Frequency Response  
  
 
198
 4.1  
 Parseval’s Identity: DTFT  
  
 
244
 4.2  
 Parseval’s Identity: DFT  
  
 
260
 5.1  
 Paley-Wiener Theorem  
  
 
352
 5.2  
 Linear-phase Filter  
  
 
362
 5.3  
 Minimum-phase Allpass Decomposition  
  
 
373
 6.1  
 Alternation Theorem  
  
 
447
 6.2  
 Flow Graph Reversal Theorem  
  
 
471
 8.1 
 Perfect Reconstruction Filter Bank 
 
 
627
 8.2 
 Perfect Reconstruction Frequency Domain 
 
 
628
 9.1  
 Least Mean Square (LMS) Convergence  
  
 
684
 9.2  
 Raised-cosine RBF Network  
  
 
731
Number   
Description  
Page
 1.1  
 Successive Approximation  
  
 
 45
 3.1  
 Residue Method  
  
 
175
 4.1  
 Bit Reversal  
  
 
265
 4.2  
 FFT  
  
 
266
 4.3  
 Problem Domain  
  
 
267
 4.4  
 IFFT  
  
 
268
 4.5  
 Fast Block Convolution  
  
 
274
 5.1  
 Zero-phase Filter  
  
 
366
 5.2  
 Minimum-phase Allpass Decomposition  
  
 
373
 6.1  
 Windowed FIR Filter  
  
 
431
 6.2  
 Ripple Size  
  
 
449
 6.3  
 Equiripple FIR Filter  
  
 
451
 6.4 
 Lattice-form Realization 
 
 
475
 7.1  
 Bilinear Transformation Method  
  
 
544
 9.1  
 RLS Method  
  
 
711
 9.2  
 Raised-cosine RBF Network Evaluation  
 
 
731
Table II: Propositions
Table III: Algorithms
xx    Margin Contents
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1
Signal and System Analysis
P A R T
1
Signal Processing
3
2
Discrete-time
Systems in the
Time Domain
Discrete-time
Systems in the
Frequency Domain
4
Fourier Transforms
and Signal Spectra
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3
Signal Processing
C H A P T E R  1
CHAPTER ToPiCS
 1.1 
Motivation 
 1.2  Signals and Systems 
 1.3  Sampling of Continuous-
time Signals 
 1.4  Reconstruction of  
Continuous-time Signals 
 1.5  Prefilters and Postfilters
*1.6  DAC and ADC Circuits 
 1.7  DSP Companion 
 1.8  GUI Modules and Case 
Studies 
 1.9  Chapter Summary 
 1.10 Problems
Motivation
A signal is a physical variable whose value varies with time or 
space. When the value of the signal is available over a continuum 
of times, it is referred to as a continuous-time signal. Continuous-
time signals whose amplitudes also vary over a continuous range 
are called analog signals. Everyday examples of analog signals 
include temperature, pressure, liquid level, chemical concentra-
tion, voltage and current, position, velocity, acceleration, force 
and torque. If the value of the signal is available only at discrete 
instants of time, it is called a discrete-time signal. Although some 
signals, for example economic data, are inherently discrete-time 
signals, a more common way to produce a discrete-time signal, 
x(k), is to take samples of an underlying analog signal, xa(t).
x(k) 5
D  
xa(kT ),   uku 5 0, 1, 2, Á
Here T denotes the sampling interval or time between samples, 
and 5
D  means equals by definition. When finite precision is used 
to represent the value of x(k), the sequence of quantized val-
ues is then called a digital signal. A system or algorithm which 
processes one digital signal x(k) as its input and produces a sec-
ond digital signal y(k) as its output is a digital signal proces-
sor. Digital signal processing (DSP) techniques have widespread 
applications, and they play an increasingly important role in the 
modern world. Application areas include speech recognition, 
detection of targets with radar and sonar, processing of music 
and video, seismic exploration for oil and gas deposits, medical 
1.1
Continuous-time signal
Analog signal
Discrete-time signal
Sampling interval
Digital signal
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4    Chapter 1  Signal Processing
signal processing including EEG, EKG, and ultrasound, communication channel equal-
ization, and satellite image processing. The focus of this book is the development, imple-
mentation, and application of modern DSP techniques.
We begin this introductory chapter with a comparison of digital and analog sig-
nal processing. Next, some practical problems are posed that can be solved using DSP 
techniques. This is followed by characterization and classification of signals. The funda-
mental notion of the spectrum of a signal is then presented including the concepts of 
bandlimited and white noise signals. This leads naturally to the sampling process which 
takes a continuous-time signal and produces a corresponding discrete-time signal. Sim-
ple conditions are presented that ensure that an analog signal can be reconstructed from 
its samples. When these conditions are violated, a phenomenon called aliasing occurs. 
The use of guard filters to reduce the effects of aliasing is discussed. Next DSP hard-
ware in the form of analog-to-digital converters (ADCs) and digital-to-analog converters 
(DACs) is examined. The hardware discussion includes ways to model the quantization 
error associated with finite precision converters. A menu-based graphical user interface 
(GUI) program called the DSP Companion is introduced that provides direct access to 
an extensive set of supplementary course materials. The DSP Companion allows the stu-
dent and the instructor to run and view chapter GUI modules, PowerPoint lecture slides, 
examples, figures, tables, definitions, propositions, algorithms, and selected problem solu-
tions that appear throughout the text. The GUI modules can be used to interactively 
explore the digital signal processing techniques covered in each chapter without any need 
for programming. For example, in this chapter GUI module g_sample allows the user to 
investigate the sampling of continuous-time signals including aliasing and quantization 
effects. The module g_reconstruct then allows the user to explore the reconstruction of 
continuous-time signals from their samples. The chapter concludes with a case study 
example, and a summary of signal sampling and reconstruction.
1.1.1 Digital and Analog Processing
For many years, almost all signal processing was done with analog circuits as shown in 
Figure 1.1. For example, operational amplifiers, resistors, and capacitors are used to real-
ize frequency-selective filters.
With the advent of specialized microprocessors with built-in data conversion circuits 
(Papamichalis, 1990), it is now commonplace to perform signal processing digitally as 
shown in Figure 1.2. Digital processing of analog signals is more complex because it typ-
ically requires the three components shown in Figure 1.2. The analog-to-digital converter 
or ADC at the front end converts the analog input xa(t) into an equivalent digital signal 
DSP Companion
GUI modules
Figure 1.1: Analog 
Signal Processing
xa(t)
Analog
processing
circuit
ya(t)
Figure 1.2: Digital 
Signal Processing
ADC
Digital
processing
program
DAC
x(k)
y(k)
xa(t)
ya(t)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.1 Motivation    5
x(k). The processing of x(k) is then achieved with an algorithm that is implemented in 
software. For a filtering operation, the DSP algorithm consists of a difference equation, 
but other types of processing are also possible and are often used. The digital output sig-
nal y(k) is then converted back to an equivalent analog signal ya(t) by the digital-to-analog  
converter or DAC.
Although the DSP approach requires more steps than analog signal processing, there 
are many important benefits to working with signals in digital form. A comparison of the 
relative advantages and disadvantages of the two approaches is summarized in Table 1.1.
The primary advantages of analog signal processing are speed and cost. Digital sig-
nal processing is not as fast due to the limits on the sampling rates of the converter 
circuits. In addition, if substantial computations are to be performed between samples, 
then the clock rate of the processor also can be a limiting factor. Speed can be an issue 
in real-time applications where the kth output sample y(k) must be computed and sent to 
the DAC as soon as possible after the kth input sample x(k) is available from the ADC. 
The delay is sometimes referred to as latency. However, there are also applications where 
the entire input signal is available ahead of time for processing off-line. For this batch 
mode type of processing, speed is less critical.
DSP hardware is often somewhat more expensive than analog hardware because 
analog hardware can consist of as little as a few discrete components on a stand-alone 
printed circuit board. The cost of DSP hardware varies depending on the performance 
characteristics required. In some cases, a PC may already be available to perform other 
functions for a given application, and in these instances the marginal expense of adding 
DSP hardware is not large.
In spite of these limitations, there are great benefits to using DSP techniques. Indeed, 
DSP is superior to analog processing with respect to virtually all of the remaining fea-
tures listed in Table 1.1. One of the most important advantages is the inherent flexibility 
available with a software implementation. Whereas an analog circuit might be tuned with 
a potentiometer to vary its performance over a limited range, a DSP algorithm can be 
completely replaced, on the fly, when circumstance warrant. DSP also offers considerably 
higher performance than analog signal processing. For example, digital filters with arbi-
trary magnitude responses and linear phase responses can be designed easily whereas this 
is not feasible with analog filters.
A common problem that plagues analog systems is the fact that the component val-
ues tend to drift with age and with changes in environmental conditions such as temper-
ature. This leads to a need for periodic calibration or tuning. With DSP there is no drift 
problem and therefore no need to manually recalibrate.
Since data are already available in digital form in a DSP system, with little or no addi-
tional expense one can log the data associated with the operation of the system so that its 
performance can be monitored, either locally or remotely over a network connection. If 
an unusual operating condition is detected, its exact time and nature can be determined, 
Real time
Latency
Table 1.1: Comparison 
of Analog and Digital 
Signal Processing
Feature
Analog Processing
Digital Processing
Speed 
Fast 
Moderate 
Cost 
Low to moderate 
Moderate 
Flexibility 
Low 
High 
Performance 
Moderate 
High 
Self-calibration 
No 
Yes 
Data logging capability 
No 
Yes 
Adaptive capability 
Limited 
Yes 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6    Chapter 1  Signal Processing
and a higher-level control system can be alerted. Although strip chart recorders can be 
added to an analog system, this substantially increases the expense thereby negating one 
of its potential advantages.
The flexibility inherent in software can be exploited by having the parameters of the 
DSP algorithm vary with time and adapt as the characteristics of the input signal or the 
processing task change. Applications, like system identification and active noise control, 
exploit adaptive signal processing, a topic that is addressed in Chapter 9.
1.1.2 Total Harmonic Distortion (THD)
With the widespread use of digital computers, DSP applications are now commonplace. 
As a simple initial example, consider the problem of designing an audio amplifier to 
boost signal strength without distorting the shape of the input signal. For the amplifier 
shown in Figure 1.3, suppose the input signal xa(t) is a pure sinusoidal tone of amplitude 
a and frequency F0 Hz.
 
xa(t) 5 a cos(2F0t) 
 (1.1.1)
An ideal amplifier will produce a desired output signal yd (t) that is a scaled and delayed 
version of the input signal. For example, if the scale factor or amplifier gain is K and the 
delay is , then the desired output is
yd (t) 5 Kxa(t 2 )
 
 5 Ka cos f2F0(t 2 )g 
 (1.1.2)
In a practical amplifier, the relationship between the input and the output is only approx-
imately linear, so some additional terms are present in the actual output ya.
ya(t) 5 Afxa(t)g
 
 < d0
2 1 o
M21
i51
di cos(2iF0t 1 i) 
 (1.1.3)
The presence of the additional harmonics indicates that there is distortion in the amplified 
signal due to nonlinearities within the amplifier. For example, if the amplifier is driven 
with an input whose amplitude a is too large, then the amplifier will saturate with the 
result that the output is a clipped sine wave that sounds distorted when played through 
a speaker. To quantify the amount of distortion, the average power contained in the ith 
harmonic is d 2
i y2 for i $ 1 and d 2
i y4 for i 5 0. Thus the average power of the signal ya(t) is
 
Py 5 d 2
0
4 1 1
2 o
M21
i51
d 2
i  
 (1.1.4)
The total harmonic distortion or THD of the output signal ya(t) is defined as the 
power in the spurious harmonic components, expressed as a percentage of the total 
power. Thus the following can be used to measure the quality of the amplifier output.
 
THD 5 
D 100(Py 2 d 2
1y2)
Py
 % 
 (1.1.5)
Gain
Average power
Total harmonic 
distortion
Figure 1.3: An 
Audio Amplifier
K
xa(t)
ya(t)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.1 Motivation    7
For an ideal amplifier di 5 0 for i Þ 1 and
 
d1 5 Ka
 
 (1.1.6a)
 
1 5 22F0 
 (1.1.6b)
Consequently, for a high-quality amplifier, the THD is small, and when no distortion 
is present THD 5 0. Suppose the amplifier output is sampled to produce the following 
digital signal of length N 5 2M.
 
y(k) 5 ya(kT ),  0 # k , N 
 (1.1.7)
If the sampling interval is set to T 5 1y(NF0), then this corresponds to one period of 
ya(t). By processing the digital signal x(k) with something called the discrete Fourier 
transform or DFT, it is possible to determine di and i for 0 # i , M. In this way the total 
harmonic distortion can be measured. The DFT is a key analytic tool that is introduced 
in Chapter 4.
1.1.3 A Notch Filter
As a second example of a DSP application, suppose one is performing sensitive acous-
tic measurements in a laboratory setting using a microphone. Here, any ambient back-
ground sounds in the range of frequencies of interest have the potential to corrupt the 
measurements with unwanted noise. Preliminary measurements reveal that the overhead 
fluorescent lights are emitting a 120 Hz hum, which corresponds to the second harmonic 
of the 60 Hz commercial AC power. The problem then is to remove the 120 Hz frequency 
component while affecting the other nearby frequency components as little as possible. 
Consequently, you want to process the acoustic data samples with a notch filter designed 
to remove the effects of the fluorescent lights. After some calculations, you arrive at the 
following digital filter to process the measurements x(k) to produce a filtered signal y(k).
y(k) 5 1.6466y(k 2 1) 2 .9805y(k 2 2) 1 .9905x(k)
 
2 1.6471x(k 2 1) 1 .9905x(k 2 2) 
 (1.1.8)
The filter in (1.1.8) is a notch filter with a bandwidth of 4 Hz, a notch frequency of 
Fn 5 120 Hz, and a sampling frequency of fs 5 1280 Hz. A plot of the frequency response 
of this filter is shown in Figure 1.4 where a sharp notch at 120 Hz is apparent. Notice 
that except for frequencies very close to Fn, all other frequency components of x(k) are 
passed through the filter without attenuation. The design of notch filters is discussed in 
Chapter 7. 
1.1.4 Active Noise Control
An application area of DSP that makes use of adaptive signal processing is active con-
trol of acoustic noise (Kuo and Morgan, 1996). Examples include industrial noise from 
rotating machines, propeller and jet engine noise, road noise in an automobile, and noise 
caused by air flow in heating, ventilation, and air conditioning systems. As an illustration 
of the latter, consider the active noise control system shown in Figure 1.5, which consists 
of an air duct with two microphones and a speaker. The basic principle of active noise 
control is to inject a secondary sound into the environment so as to cancel the primary 
sound using destructive interference.
Notch filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8    Chapter 1  Signal Processing
The purpose of the reference microphone in Figure 1.5 is to detect the primary noise 
x(k) generated by the noise source or blower. The primary noise signal is then passed 
through a digital filter of the following form.
 
y(k) 5 o
m
i50
wi (k)x(k 2 i ) 
 (1.1.9)
The output of the filter y(k) drives a speaker that creates the secondary sound, some-
times called antisound. The error microphone, located downstream of the speaker, 
detects the sum of the primary and secondary sounds and produces an error signal e(k). 
The objective of the adaptive algorithm is to take x(k) and e(k) as inputs and adjust the 
filter weights w(k) so as to drive e2(k) to zero. If zero error can be achieved, then silence 
is observed at the error microphone. In practical systems, the error or residual sound is 
significantly reduced by active noise control.
To illustrate the operation of this adaptive DSP system, suppose the blower noise is 
modeled as a periodic signal with fundamental frequency F0 and r harmonics plus some 
random white noise v(k).
 
x(k) 5 o
r
i51
ai cos(2ikF0T 1 i) 1 v(k),  0 # k , p 
 (1.1.10)
Antisound
Figure 1.4:  
Magnitude 
Response of a 
Notch Filter with 
Fn 5 120 Hz
0
100
200
300
400
500
600
700
0
0.2
0.4
0.6
0.8
1
1.2
1.4
f (Hz)
A(f)
Figure 1.5: Active 
Control of Acoustic 
Noise in an Air 
Duct
Controller
Blower
Reference
microphone
Speaker
Error
microphone
y(k)
e(k)
x(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.1 Motivation    9
For example, suppose F0 5 100 Hz and there are r 5 4 harmonics with amplitudes 
ai 5 1yi and random phase angles. Suppose the random white noise term is distributed 
uniformly over the interval f2.5, .5g. Let p 5 2048 samples, and suppose the sampling 
interval is T 5 1y1600 s and the filter order is m 5 40. The adaptive algorithm used to 
adjust the filter weights is called the FXLMS method, and it is discussed in detail in 
Chapter 9. The results of applying this algorithm are shown in Figure 1.6.
Initially the filter weights are set to w(0) 5 0, which corresponds to no noise control 
at all. The adaptive algorithm is not activated until sample k 5 512, so the first quarter 
of the plot in Figure 1.6 represents the ambient or primary noise detected at the error 
microphone. When adaptation is activated, the error begins to decrease rapidly, and after 
a short transient period it reaches a steady-state level that is more than an order of mag-
nitude quieter than the primary noise itself. We can quantify the noise reduction by using 
the following measure of overall noise cancellation.
 
E 5 10 log10 1 o
py421
i50
e2(i)2 2 10 log10 1 o
p21
i53py4
e2(i)2   dB  
 (1.1.11)
The overall noise cancellation E is the log of the ratio of the average power of the noise 
during the first quarter of the samples divided by the average power of the noise during 
the last quarter of the samples, expressed in units of decibels. Using this measure, the 
noise cancellation observed in Figure 1.6 is E 5 32.1 dB.
1.1.5 Video Aliasing
Later in Chapter 1 we focus on the problem of sampling a continuous-time signal xa(t) 
to produce the following discrete-time signal, where T .  0 is the sampling interval and 
fs 5 1yT is the sampling frequency.
 
x(k) 5 xa(kT ),  uku 5 0, 1, 2, Á  
 (1.1.12)
Figure 1.6:  
Squared Error 
Signal with Active 
Noise Control  
Activated at 
k 5 512 
0
500
1000
1500
2000
0
10
20
30
40
50
60
70
80
90
100
k
e2(k)
Noise reduction 5 32.1 dB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

10    Chapter 1  Signal Processing
An important theoretical and practical question that arises in connection with the 
sampling process is this. Under what conditions do the samples x(k) contain all the 
information needed to reconstruct the signal xa(t)? The Shannon sampling theorem 
says that if the signal xa(t) is bandlimited and the sampling rate fs is greater than twice 
the bandwidth or highest frequency present, then it is possible to interpolate between 
the x(k) to precisely reconstruct xa(t). However, if the sampling frequency is too low, 
then the samples become corrupted, a process known as aliasing. An interesting way 
to illustrate aliasing is to examine a video signal in the form of an M 3 N image Ia(t) 
that varies with time. Here Ia(t) consists of an M 3 N array of picture elements or 
pixels, where the number of rows M and columns N depends on the video format 
used. If Ia(t) is sampled with a sampling interval of T then the resulting discrete-time 
signal is
 
I(k) 5 Ia(kT ),  uku 5 0, 1, 2, Á  
 (1.1.13)
Here fs 5 1yT is the sampling rate in frames per second. Depending on the content of the 
image, the sampling rate fs may or may not be sufficiently high to avoid aliasing.
As a simple illustration, suppose the image consists of a rotating disk with a dark line 
on it to indicate orientation as shown in Figure 1.7. A casual look at the sequence of frames 
in Figure 1.7 suggests that the disk appears to be rotating counterclockwise at a rate of  
45 degrees per frame. However, this is not the only interpretation possible. For exam-
ple, an alternative explanation is that the disk is actually rotating clockwise at a rate of  
315 degrees/frame. Both interpretations are plausible. Is the motion captured by the 
snapshots a fast clockwise rotation or a slow counter clockwise rotation? If the disk 
is in fact rotating clockwise at F0 revolutions/s but the sampling rate is fs # 2F0, then 
aliasing occurs in which case the disk can appear to turn backwards at a slow rate. 
Interestingly, this manifestation of aliasing was quite common in older western films 
that featured wagon trains heading west. The spokes on the wagon wheels sometimes 
appeared to move backwards because of the slow frame rate used to shoot the film and 
display it on older TVs.
Aliasing
Pixels
Figure 1.7: Four 
Video Frames of a 
Rotating Disk. Is 
the disk rotating 
slowly counter-
clockwise at 45 
degrees per frame 
or is it rotating 
clockwise at 
315 degrees per 
frame?
25
0
5
25
0
5
0
5
0
5
k 5 2
k 5 4
k 5 3
k 5 1
25
0
5
25
25
25
25
25
0
5
0
5
0
5
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.2 Signals and Systems    11
Signals and Systems
1.2.1 Signal Classification
Recall that a signal is a physical variable whose value varies with respect to time or space. 
To simplify the notation and terminology, we will assume that, unless noted otherwise, 
the independent variable denotes time. If the value of the signal, the dependent vari-
able, is available over a continuum of times, t [ R, then the signal is referred to as a  
continuous-time signal. An example of a continuous-time signal, xa(t), is shown in Figure 1.8.
In many cases of practical interest, the value of the signal is only available at discrete 
instants of time in which case it is referred to as a discrete-time signal. That is, signals can 
be classified as continuous-time or discrete-time depending on whether the independent 
variable is continuous or discrete, respectively. Common everyday examples of discrete-
time signals include economic statistics such as the monthly unemployment rate. In DSP 
applications, a more common way to produce a discrete-time signal, x(k), is to sample an 
underlying continuous-time signal, xa(t), as follows.
 
x(k) 5 xa(kT ),  uku 5 0, 1, 2, Á  
 (1.2.1)
Here T . 0 is the time between samples or sampling interval in seconds. The sample spac-
ing also can be specified using the reciprocal of the sampling interval, which is called the 
sampling frequency, fs.
 
fs 5
D  1
T  Hz  
 (1.2.2)
Here the unit of Hz is understood to mean samples/s. Notice that the integer k in (1.2.1) 
denotes discrete time or, more specifically, the sample number. The sampling interval T is 
left implicit on the left-hand side of (1.2.1) because this simplifies subsequent notation. In 
those instances where the value of T is important, it will be stated explicitly. An example 
of a discrete-time signal generated by sampling the continuous-time signal in Figure 1.8 
using T 5 .25 seconds is shown in Figure 1.9.
1.2
Continuous-time signal
Discrete-time signal
Sampling interval
Sampling frequency
Discrete-time index
Figure 1.8: A 
Continuous-time 
Signal xa(t)
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
2.5
3
3.5
4
xa(t) 5 10t exp(2t)
t (sec)
xa(t)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

12    Chapter 1  Signal Processing
Just as the independent variable can be continuous or discrete, so can the dependent vari-
able or amplitude of the signal be continuous or discrete. If the number of bits of precision 
used to represent the value of x(k) is finite, then we say that x(k) is a quantized or discrete-am-
plitude signal. For example, if N bits are used to represent the value of x(k), then there are 
2N distinct values that x(k) can assume. Suppose the value of x(k) ranges over the interval 
fxm, xMg. Then the quantization level, or spacing between adjacent discrete values of x(k), is
 
q 5 xM 2 xm
2N
 
 (1.2.3)
The quantization process can be thought of as passing a signal through a piecewise- 
constant staircase type function. For example, if the quantization is based on rounding to the 
nearest N bits, then the process can be represented with the following quantization operator.
 
QN (x) 5
D   
q ? round 1
x
q2 
 (1.2.4)
A graph of QN(x) for x ranging over the interval f21, 1g using N 5 5 bits is shown in 
Figure 1.10. A quantized discrete-time signal is called a digital signal. That is, a digital 
signal, xq(k), is discrete in both time and amplitude with
 
xq(k) 5 QN fxa(kT )g 
 (1.2.5)
By contrast, a signal that is continuous in both time and amplitude is called an analog 
signal. An example of a digital signal obtained by quantizing the amplitude of the dis-
crete-time signal in Figure 1.9 is shown in Figure 1.11. In this case, the 5-bit quantizer 
in Figure 1.10 is used to produce xq(k). Careful inspection of Figure 1.11 reveals that at 
some of the samples there are noticeable differences between xq(k) and xa(kT). If round-
ing is used, then the magnitude of the error is, at most, qy2.
Most of the analysis in this book will be based on discrete-time signals rather than 
digital signals. That is, infinite precision is used to represent the value of the dependent 
variable. Finite precision, or finite word length effects, are examined in Chapters 6 and 7 
in the context of digital filter design. When digital filters are implemented in MATLAB 
using the default double-precision arithmetic, this corresponds to 64 bits of precision  
Quantized signal
Quantization level
Quantization operator
Digital signal
Analog signal
Figure 1.9: A  
Discrete-time 
Signal x(k) with 
T 5 .25
0
.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
2.5
3
3.5
4
x(k) 5 10kT exp(2kT)
kT (sec)
x(k)
T 5 .25
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.2 Signals and Systems    13
(16 decimal digits). In most instances this is sufficiently high precision to yield insignifi-
cant finite word length effects.
A digital signal xq(k) can be modeled as a discrete-time signal x(k) plus random  
quantization noise, v(k), as follows.
 
xq(k) 5 x(k) 1 v(k) 
 (1.2.6)
An effective way to measure the size or strength of the quantization noise is to use aver-
age power defined as the mean, or expected value, of v2(k). Typically, v(k) is modeled as 
a random variable uniformly distributed over the interval f2qy2, qy2g with probability 
Quantization noise
Expected value
Figure 1.10:  
Quantization over 
fxm, xMg 5 f21,1g 
Using N 5 5 Bits
21
20.420.2
20.6
20.8
0.6
0.8
0.4
0.2
0
1
21
20.8
20.6
20.4
20.2
0
0.2
0.4
0.6
0.8
1
x
Q(x)
q 5 .0625
Figure 1.11: A  
Digital Signal xq(k) 
0
.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
2.5
3
3.5
4
xq(k) 5 QN[xa (kT)]
kT (sec)
xq(k)
T 5 .25
q 5 .125
N 5 5 bits
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

14    Chapter 1  Signal Processing
density p(x) 5 1yq. In this case, the expected value of v2(k) is
E fv2g 5#
qy2
2qy2
p(x)x2dx
 
 5 1
q#
qy2
2qy2
x2dx
 
 (1.2.7)
Thus the average power of the quantization noise is proportional to the square of the 
quantization level with
 
E fv2g 5 q2
12  
 (1.2.8)
Quantization Noise
EXAMPLE 1.1
Suppose the value of a discrete-time signal x(k) is constrained to lie in the interval 
f210, 10g. Let xq(k) denote a digital version of x(k) using quantization level q, 
and consider the following problem. Suppose the average power of the quantiza-
tion noise, v(k), is to be less than .001. What is the minimum number of bits that 
are needed to represent the value of xq(k)? The constraint on the average power 
of the quantization noise is
E fv2g , .001
Thus from (1.2.3) and (1.2.8) we have
(xM 2 xm)2
12(2N)2
, .001
Recall that the signal range is xm 5 210 and xM 5 10. Multiplying both sides by 
12, taking the square root of both sides, and then solving for 2N yields
2N .
20
Ï.012
Finally, taking the natural log of both sides and solving for N we have
N .
 ln (182.5742)
 ln (2)
5 7.5123
Since N must be an integer, the minimum number of bits needed to ensure that 
the average power of the quantization noise is less than .001 is N 5 8 bits.
The relationship between the four basic signal types is summarized in Table 1.2.  
Signals can be further classified depending on whether or not they are nonzero for nega-
tive values of the independent variable, time.
A signal xa(t) defined for t [ R is causal if and only if it is zero for negative t. Other-
wise, the signal is noncausal.
xa(t) 5 0 for t , 0
DEFiNiTioN 
1.1 Causal Signal
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.2 Signals and Systems    15
Most of the signals that we work with will be causal signals. A simple, but important, 
example of a causal signal is the unit step, which is denoted a(t) and defined
 
a(t) 5
D  5
0, t , 0
1, t $ 0
 
 (1.2.9)
Note that any signal can be made into a causal signal by multiplying by the unit step. For 
example, xa(t) 5 exp(2ty)a(t) is a causal decaying exponential with time constant . 
Another important example of a causal signal is the unit impulse, which is denoted a(t). 
Strictly speaking, the unit impulse is not a function, because it is not defined at t 5 0. 
However, the unit impulse can be defined implicitly by the equation
 
#
t
2`
a()d 5 a(t) 
 (1.2.10)
That is, the unit impulse a(t) is a signal that, when integrated, produces the unit step a(t). 
Consequently, we can loosely think of the unit impulse as the derivative of the unit step 
function, keeping in mind that the derivative of the unit step is not defined at t 5 0. The 
two essential characteristics of the unit impulse that follow from (1.2.10) are
 
a(t) 5 0,  t Þ 0 
 (1.2.11a)
 
#
`
2`
a(t)dt 5 1
 
 (1.2.11b)
A more informal way to view the unit impulse is to consider a narrow pulse of width 
 and height 1y starting at t 5 0. The unit impulse can be thought of as the limit of a 
sequence of pulses as the pulse width  goes to zero. By convention, we graph the unit 
impulse as a vertical arrow with the height of the arrow equal to the strength, or area, of 
the impulse, as shown in Figure 1.12.
The unit impulse has an important property that is a direct consequence of (1.2.11). 
If xa(t) is a continuous function, then
#
`
2`
xa()a( 2 t0)d 5#
`
2`
xa(t0)a( 2 t0)d
 5 xa(t0)#
`
2`
a( 2 t0)d
 
 5 xa(t0)#
`
2`
a()d
 
 (1.2.12)
Since the area under the unit impulse is one, we then have the following sifting property 
of the unit impulse
 
#
`
2`
xa (t)a (t 2 t0)dt 5 xa (t0)  
 (1.2.13)
Unit step
Unit impulse
Sifting property
Continuous-time, T 5 0
Discrete-time, T . 0 
Continuous  
amplitude,  
q 5 0
analog signal, xa (t)
discrete-time signal, x(k) 
Discrete  
amplitude,  
q . 0
quantized signal, xq (t)
digital signal, xq(k)  
Table 1.2: Basic Signal 
Types
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

16    Chapter 1  Signal Processing
From (1.2.13) we see that when a continuous function of time is multiplied by an impulse 
and then integrated, the effect is to sift out or sample the value of the function at the time 
the impulse occurs.
1.2.2 System Classification
Just as signals can be classified, so can the systems that process those signals. Consider a 
system S with input x and output y as shown in Figure 1.13. In some cases, for example 
biomedical systems, the input is referred to as the stimulus, and the output is referred to 
as the response. We can think of the system in Figure 1.13 as an operator S that acts on 
the input signal x to produce the output signal y.
 
y 5 Sx 
 (1.2.14)
If the input and output are continuous-time signals, then the system S is called a continuous- 
time system. A discrete-time system is a system S that processes a discrete-time input x(k) 
to produce a discrete-time output y(k). There are also examples of systems that contain 
both continuous-time signals and discrete-time signals. These systems are referred to as 
sampled-data systems.
Almost all of the examples of systems in this book belong to an important class of 
systems called linear systems.
Let x1 and x2 be arbitrary inputs and let a and b be arbitrary scalars. A system S is 
linear if and only if the following holds; otherwise it is a nonlinear system.
S(ax1 1 bx2) 5 aSx1 1 bSx2
Continuous, discrete 
systems
DEFiNiTioN 
1.2 Linear System
Figure 1.12: Unit 
Impulse, a(t),  
and Unit Step, 
a(t)
22
21.5
21
20.5
0
0.5
1
1.5
2
20.5
0
0.5
1
1.5
t (sec)
xa(t)
a(t)
a(t)
Figure 1.13: A  
System S with 
Input x and  
Output y
x
y
S
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.2 Signals and Systems    17
Thus a linear system has two distinct characteristics. When a 5 b 5 1, we see that the 
response to a sum of inputs is just the sum of the responses to the individual inputs. Sim-
ilarly, when b 5 0, we see that the response to a scaled input is just the scaled response 
to the original input. Examples of linear discrete-time systems include the notch filter in 
(1.1.8) and the adaptive filter in (1.1.9). On the other hand, if the analog audio amplifier 
in Figure 1.3 is overdriven and its output saturates to produce harmonics as in (1.1.3), 
then this is an example of a nonlinear continuous-time system. Another important class 
of systems is time-invariant systems.
A system S with input xa(t) and output ya(t) is time-invariant if and only if whenever 
the input is translated in time by , the output is also translated in time by . Other-
wise the system is a time-varying system.
Sxa(t 2 ) 5 ya(t 2 )
For a time-invariant system, delaying or advancing the input delays or advances the 
output by the same amount, but it does not otherwise affect the shape of the output. 
Therefore the results of an input-output experiment do not depend on when the experi-
ment is performed. Time-invariant systems described by differential or difference equa-
tions have constant coefficients. More generally, physical time-invariant systems have 
constant parameters. The notch filter in (1.1.8) is an example of a discrete-time system 
that is both linear and time-invariant. On the other hand, the adaptive digital filter in 
(1.1.9) is a time-varying system, because the weights w(k) are coefficients that change 
with time as the system adapts. The following example shows that the concepts of linear-
ity and time-invariance can sometimes depend on how the system is characterized.
DEFiNiTioN 
1.3 Time-invariant System
System Classification
EXAMPLE 1.2
Consider the operational amplifier circuit shown in Figure 1.14. Here input resistor 
R1 is fixed, but feedback resistor R2 represents a sensor or transducer whose resis-
tance changes with respect to a sensed environmental variable such as temperature 
or pressure. For this inverting amplifier configuration, the output voltage ya(t) is
ya(t) 5 23
R2(t)
R1 4
 x1(t)
(Continued )
Figure 1.14: An 
Inverting Amplifier with 
a Feedback Transducer
1
2
R2(t)
2
1
x1
R1
2
1
ya
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

18    Chapter 1  Signal Processing
Another important classification of systems focuses on the question of what happens 
to the signals as time increases. We say that a signal xa(t) is bounded if and only if there 
exists a Bx . 0 called a bound such that
 
uxa(t)u # Bx  for  t [ R 
 (1.2.15)
A system S is with input xa(t) and output ya(t) is stable, in a bounded input bounded 
output (BIBO) sense, if and only if every bounded input produces a bounded output. 
Otherwise it is an unstable system.
Thus an unstable system is a system for which the magnitude of the output grows arbi-
trarily large with time for a least one bounded input.
Bounded signal
Bound
DEFiNiTioN 
1.4 Stable System
This is an example of a linear continuous-time system that is time-varying because 
parameter R2(t) varies as the temperature or pressure changes. However, another 
way to model this system is to consider the variable resistance of the sensor as a 
second input x2(t) 5 R2(t). Viewing the system in this way, the system output is
ya(t) 5 2x1(t)x2(t)
R1
This formulation of the model is a nonlinear time-invariant system, but with two 
inputs. Thus, by introducing a second input we have converted a single-input 
time-varying linear system to a two-input time-invariant nonlinear system.
Stability
EXAMPLE 1.3
As a simple example of a system that can be stable or unstable depending on its 
parameter values, consider the following first-order linear continuous-time sys-
tem where a Þ 0.
dya (t)
dt
1 aya (t) 5 xa (t)
Suppose the input is the unit step xa(t) 5 a(t) which is bounded with a bound 
of Bx 5 1. Direct substitution can be used to verify that for t $ 0, the solution is
ya(t) 5 ya(0) exp(2at) 1 1
a f1 2  exp(2at)g
If a . 0, then the exponential terms grow without bound, which means that the 
bounded input ua(t) produces an unbounded output ya(t). Thus this system is 
unstable, in a BIBO sense, when a . 0.
Just as light can be decomposed into a spectrum of colors, signals also contain energy 
that is distributed over a range of frequencies. To decompose a continuous-time signal 
xa(t) into its spectral components, we use the Fourier transform.
 
Xa( f ) 5
 FT hxa(t)j 5
D  #
`
2`
xa(t) exp(2j2ft)dt 
 (1.2.16)
Fourier transform
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.2 Signals and Systems    19
It is assumed that the reader is familiar with the basics of continuous-time trans-
forms, specifically the Laplace transform and the Fourier transform. Tables of transform 
pairs and transform properties for all of the transform used in this text can be found 
in Appendix 1. Here f [ R denotes frequency in cycles/s or Hz. In general the Fourier 
transform, Xa( f ), is complex. As such, it can be expressed in polar form in terms of its 
magnitude Aa( f ) 5 uXa( f )u and phase angle a( f  ) 5 /Xa( f  ) as follows.
 
Xa( f ) 5 Aa( f ) expf ja( f )g 
 (1.2.17)
The real-valued function Aa( f ) is called the magnitude spectrum of xa(t), while the 
real-valued function a( f ) is called the phase spectrum of xa(t). More generally, Xa( f ) 
itself is called the spectrum of xa(t). For a real xa(t), the magnitude spectrum is an even 
function of f , and the phase spectrum is an odd function of f .
When a signal passes through a linear system, the shape of its spectrum changes. 
Systems designed to reshape the spectrum in a particular way are called filters. The effect 
that a linear system has on the spectrum of the input signal can be characterized by the 
frequency response.
Let S be a stable linear time-invariant continuous-time system with input xa(t) and out-
put ya(t). Then the frequency response of the system S is denoted Ha( f ) and defined as
Ha( f ) 5
D  Ya( f )
Xa( f )
Thus the frequency response of a linear system is just the Fourier transform of the output 
divided by the Fourier transform of the input. Since Ha( f ) is complex, it can be represented 
by its magnitude Aa( f ) 5 uHa( f )u and its phase angle a( f ) 5 /Ha( f ) as follows: 
 
Ha( f ) 5 Aa( f ) expf ja( f )g 
 (1.2.18)
The function Aa( f ) is called the magnitude response of the system, while a( f ) is called 
the phase response of the system. The magnitude response indicates how much each fre-
quency component of xa(t) is scaled as it passes through the system. That is, Aa( f ) is the 
gain of the system at frequency f . Similarly, the phase response indicates how much each 
frequency component of xa(t) gets advanced in phase by the system. That is, a( f ) is the 
phase shift of the system at frequency f . Therefore, if the input to the stable system is a 
pure sinusoidal tone xa(t) 5 sin(2F0t), the steady-state output of the stable system is
 
ya(t) 5 Aa(F0) sinf2F0t 1 a(F0)g 
 (1.2.19)
The magnitude response of a real system is an even function of f , while the phase 
response is an odd function of f . This is similar to the magnitude and phase spectra of 
a real signal. Indeed, there is a simple relationship between the frequency response of a 
system and the spectrum of a signal. To see this, consider the impulse response.
Suppose the initial condition of a continuous-time system S is zero. Then the output 
of the system corresponding to the unit impulse input is denoted ha(t) and is called the 
system impulse response.
ha(t) 5 Sa(t)
Using the sifting property of the unit impulse in (1.2.13), one can show that the Fou-
rier transform of the unit impulse is simply Da( f ) 5 1. It then follows from Definition 1.5  
that when the input is the unit impulse, the Fourier transform of the system output is 
Polar form
Magnitude, phase 
spectra
Filter
DEFiNiTioN 
1.5 Frequency Response
Magnitude, phase 
responses
Gain
Phase shift
DEFiNiTioN 
1.6 impulse Response
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

20    Chapter 1  Signal Processing
Ya( f ) 5 Ha( f ). That is, an alternative way to represent the frequency response is as the 
Fourier transform of the impulse response.
 
Ha( f ) 5
 FT hha(t)j 
 (1.2.20)
In view of (1.2.17), the magnitude response of a system is just the magnitude spectrum of 
the impulse response, and the phase response is just the phase spectrum of the impulse response. 
It is for this reason that the same symbol, Aa( f ), is used to denote both the magnitude spectrum 
of a signal and the magnitude response of a system. A similar remark holds for a( f ), which is 
used to denote both the phase spectrum of a signal and the phase response of a system.
Figure 1.15:  
Frequency 
Response of Ideal 
Lowpass Filter
ƒ
Ha(ƒ)
−B
0
B
1
ideal Lowpass Filter
EXAMPLE 1.4
An important example of a continuous-time system is the ideal lowpass filter. 
An ideal lowpass filter with cutoff frequency B Hz is a system whose frequency 
response is the following pulse of height one and radius B centered at f 5 0.
B( f ) 5
D  5
1, u  f u # B
0, u  f u . B
A plot of the ideal lowpass frequency response is shown in Figure 1.15. Recall from 
Definition 1.5 that Ya( f ) 5 Ha( f )Xa( f ). Consequently, the filter in Figure 1.15  
passes the frequency components of xa(t) in the range f2B, Bg through the filter 
without any distortion whatsoever, not even any phase shift. Furthermore, the 
remaining frequency components of xa(t) outside the range f2B, Bg are com-
pletely eliminated by the filter. The idealized nature of the filter becomes appar-
ent when we look at the impulse response of the filter. To compute the impulse 
response from the frequency response, we must apply the inverse Fourier trans-
form. Using the of Fourier transform pairs in Appendix 1, this yields
ha(t) 5 2B ? sinc(2Bt)
Here the normalized sinc function is defined as follows.
 sinc (x) 5
D  
 sin(x)
x
Ideal lowpass filter
Sinc function
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.3 Sampling of Continuous-time Signals    21
Notice that the sinc function, and therefore the impulse response, is not a causal 
signal. But ha(t) is the filter output when a unit impulse input is applied at time t 5 0. 
Consequently, for the ideal filter we have a causal input producing a noncausal output. 
This is not possible for a physical system. Therefore the frequency response in Figure 1.15  
cannot be realized with physical hardware. In Section 1.4, we examine some lowpass 
filters that are physically realizable and can be used to approximate the ideal frequency 
response characteristic.
Sampling of Continuous-time Signals
1.3.1 Sampling as Modulation
The process of sampling a continuous-time signal xa(t) to produce a discrete-time signal 
x(k) can be viewed as a form of amplitude modulation. To see this, let T (t) denote a 
periodic impulse train of period T.
 
T (t) 5 
D o
`
k52`
a(t 2 kT ) 
 (1.3.1)
1.3
Periodic impulse train
The sinc function is a two-sided decaying sinusoid that is confined to the envelope 
1y(x). Here  sinc (k) 5 0 for k Þ 0. The value of sinc(x) at x 5 0 is determined by 
applying L’Hospital’s rule which yields sinc(0) 5 1. Some authors define the sinc 
function as  sinc (x) 5 sin(x)yx. The impulse response of the ideal lowpass filter is 
 sinc (2BT) scaled by 2B. A plot of the impulse response for the case B 5 100 Hz 
is shown in Figure 1.16.
Notation!
Figure 1.16: Impulse Response of Ideal Lowpass Filter when B 5 100 Hz
20.04
20.02
0
0.02
0.04
250
0
50
100
150
200
250
t (sec)
ha(t)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

22    Chapter 1  Signal Processing
Thus T(t) consists of unit impulses at integer multiples of the sampling interval T. The 
sampled signal is denoted x ⁄
a(t), and is defined as the following product.
 
x⁄
a(t) 5
D  
xa (t) T (t) 
 (1.3.2)
Since x⁄
a(t) is obtained from xa(t) by multiplication by a periodic signal, this process is 
a form of amplitude modulation of T (t). In this case T (t) plays a role similar to the high-
frequency carrier wave in AM radio, and xa(t) represents the low-frequency information 
signal. A block diagram of the impulse model of sampling is shown in Figure 1.17.
Using the basic properties of the unit impulse in (1.2.11), the sampled version of 
xa(t) can be written as follows.
x⁄
a(t) 5 xa(t)T (t)
 5 xa(t) o
`
k52`
a(t 2 kT )
 5 o
`
k52`
xa(t)a(t 2 kT )
 
 5 o
`
k52`
xa(kT)a(t 2 kT ) 
 (1.3.3)
Thus the sampled version of xa(t) is the following amplitude-modulated impulse train.
 
x⁄
a(t) 5 o
`
k52`
x(k)a(t 2 kT ) 
 (1.3.4)
Whereas T (t) is a constant-amplitude or uniform train of impulses, x⁄
a(t) is a nonuniform 
impulse train with the strength of the kth impulse equal to sample x(k). A graph illustrat-
ing the relationship between T (t) and x⁄
a(t) for the case xa(t) 5 10t exp(2t)ua(t) is shown 
in Figure 1.18.
It is useful to note from (1.3.4) that x⁄
a(t) is actually a continuous-time signal, rather 
than a discrete-time signal. However, it is a very special continuous-time signal in that it is 
zero everywhere except at the samples where it has impulses whose areas correspond to the 
sample values. Consequently, there is a simple one-to-one relationship between the con-
tinuous-time signal x⁄
a(t) and the discrete-time signal x(k). If x⁄
a(t) is a causal continuous- 
time signal, we can apply the Laplace transform to it. The Laplace transform of a causal 
continuous-time signal xa(t) is denoted Xa(s) and is defined
 
Xa(s) 5 Lhxa(t)j 5
D  #
`
0
xa(t) exp(2st)dt 
 (1.3.5)
It is assumed that the reader is familiar with the basics of the Laplace transform. 
Tables of common Laplace transform pairs and Laplace transform properties can 
be found in Appendix 1. Comparing (1.3.5) with (1.2.16) it is clear that for causal 
signals, the Fourier transform is just the Laplace transform but with the complex 
variable s replaced by j2f . Consequently, the spectrum of a causal signal can be 
Sampled signal
Amplitude modulation
Laplace transform
Figure 1.17:  
Sampling as 
Amplitude 
Modulation of an 
Impulse Train
3
xa(t)
a(t)
xa(t)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.3 Sampling of Continuous-time Signals    23
obtained from its Laplace transform as follows.
 
Xa( f ) 5 Xa(s)us 5 j2f 
 (1.3.6)
At this point a brief comment about notation is in order. Note that the same base sym-
bol, Xa, is being used to denote both the Laplace transform, Xa(s), in (1.3.5), and the Fou-
rier transform, Xa( f ), in (1.3.6). Clearly, an alternative approach would be to introduce 
distinct symbols for each. However, the need for additional symbols will arise repeat-
edly in subsequent chapters, so using separate symbols in each case quickly leads to a 
proliferation of symbols that can be confusing in its own right. Instead, the notational 
convention adopted here is to rely on the argument type, a complex s or a real f , to dis-
tinguish between the two cases and dictate the meaning of Xa. The subscript a denotes a  
continuous-time or analog quantity. The less cumbersome X, without a subscript, is 
reserved for discrete-time quantities introduced later.
If the periodic impulse train T (t) is expanded into a complex Fourier series, the result 
can be substituted into the definition of x⁄
a(t) in (1.3.2). Taking the Laplace transform of 
x⁄
a(t) and converting the result using (1.3.6), we then arrive at the following expression for 
the spectrum of the sampled version of xa(t).
 
X
⁄
a( f  ) 5 1
T o
`
i52`
Xa( f 2 ifs)  
 (1.3.7)
1.3.2 Aliasing
The representation of the spectrum of the sampled version of xa(t) depicted in (1.3.7) 
is called the aliasing formula. The aliasing formula holds the key to determining condi-
tions under which the samples x(k) contain all the information necessary to completely 
Notation!
Aliasing formula
Figure 1.18:  
Periodic Impulse 
Train in (a) and 
Sampled Version 
of xa(t) in (b) 
Using Impulse 
Sampling
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
(a)
t (sec)
T(t)
0
0.5
1
1.5
2
2.5
3
3.5
4
0
1
2
3
4
(b)
t (sec)
xa(t)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

24    Chapter 1  Signal Processing
reconstruct or recover xa(t) from the samples. To see this, we first consider the notion of 
a bandlimited signal.
A continuous-time signal xa(t) is bandlimited to bandwidth B if and only if its magni-
tude spectrum satisfies
uXa( f  )u 5 0 for u f u . B
Typically B is chosen to be as small as possible. Thus if xa(t) is bandlimited to B, then 
the highest frequency component present in xa(t) is B Hz. It should be noted that some 
authors use a slightly different definition of the term bandlimited by replacing the strict 
inequality in Definition 1.7 with u f u $ B.
The aliasing formula in (1.3.7) is quite revealing when it is applied to bandlimited sig-
nals. Notice that the aliasing formula says that the spectrum of the sampled version of a 
signal is just a sum of scaled and shifted spectra of the original signal with the replicated 
versions of Xa( f ) centered at integer multiples of the sampling frequency fs. This is a 
characteristic of amplitude modulation in general, where the unshifted spectrum (i 5 0) 
is called the base band and the shifted spectra (i Þ 0) are called side bands. An illustration 
comparing the magnitude spectra of xa(t) and x⁄
a(t) is shown in Figure 1.19.
The case shown in Figure 1.19 corresponds to fs 5 3By2 and is referred to as  
under-sampling because fs # 2B. The details of the shape of the even function uXa( f )u within 
f2B, Bg are not important, so for convenience a triangular spectrum is used. Note how the 
sidebands in Figure 1.19b overlap with each other and with the baseband. This overlap is an 
indication of an undesirable phenomenon called aliasing. As a consequence of the overlap, the 
shape of the spectrum of x⁄
a(t) in f2B, Bg has been altered and is different from the shape of 
DEFiNiTioN 
1.7 Bandlimited Signal
Base, side bands
Undersampling
Aliasing
Figure 1.19:  
Magnitude Spectra 
of xa(t) in (a) and 
x⁄
a(t) in (b) when 
B 5 100, fs 5 3By2 
(a)
2300
2200
2100
0
100
200
300
0
0.5
1
1.5
2
fd
f (Hz)
fs
2fs
|xa(f )|
(b)
2300
2200
2100
0
100
200
300
0
50
100
150
200
f (Hz)
fs
2fs
fd
|xa(f )|
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.3 Sampling of Continuous-time Signals    25
the spectrum of xa(t) in Figure 1.19a. The end result is that no amount of signal-independent  
filtering of x⁄
a(t) will allow us to recover the spectrum of xa(t) from the spectrum of x⁄
a(t). 
That is, the overlap or aliasing has caused the samples to be corrupted to the point that the 
original signal xa(t) can no longer be recovered from the samples x(k). Since xa(t) is band-
limited, it is evident that there will be no aliasing if the sampling rate is sufficiently high. This 
fundamental result is summarized in the Shannon sampling theorem.
Suppose a continuous-time signal xa(t) is bandlimited to B Hz. Let x⁄
a(t) denote the 
sampled version of xa(t) using impulse sampling with a sampling frequency of fs. 
Then the samples x(k) contain all the information necessary to recover the original 
signal xa(t) if
fs . 2B
In view of the sampling theorem, it should be possible to reconstruct a continu-
ous-time signal from its samples if the signal is bandlimited and the sampling frequency 
exceeds twice the bandwidth. When fs . 2B, the sidebands of X
⁄
a( f ) do not overlap with 
each other or the baseband. By properly filtering X
⁄
a( f ) it should be possible to recover 
the baseband and rescale it to produce Xa( f ). Before we consider how to do this, it is of 
interest to see what happens in the time domain when aliasing occurs due to an inade-
quate sampling rate. If aliasing occurs, it means that there is another lower-frequency 
signal that will produce identical samples. Among all signals that generate a given set of 
samples, there is only one signal that is bandlimited to less than half the sampling rate. 
All other signals that generate the same samples are high-frequency impostors or aliases. 
The following example illustrates this point.
PRoPoSiTioN
1.1 Signal Sampling
Impostors, aliases
Aliasing
EXAMPLE 1.5
The simplest example of a bandlimited signal is a pure sinusoidal tone that has all 
its power concentrated at a single frequency F0. For example, consider the follow-
ing signal where F0 5 90 Hz.
xa(t) 5 sin(180t)
From the Fourier transform pair table in Appendix 1, the spectrum of xa(t) is
Xa(  f  ) 5 j  f(  f 1 90) 2 (  f 2 90)g
2
Thus xa(t) is a bandlimited signal with bandwidth B 5 90 Hz. From the sampling 
theorem, we need fs . 180 Hz to avoid aliasing. Suppose xa(t) is sampled at the 
rate fs 5 100 Hz. In this case T 5 .01 seconds, and the samples are
x(k) 5  xa(kT )
 5  sin(180kT )
 5  sin(1.8k)
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

26    Chapter 1  Signal Processing
The existence of a lower-frequency signal associated with the samples, x(k), can be 
predicted directly from the aliasing formula. Indeed, a simple way to interpret the aliasing 
formula in (1.3.7) is to introduce something called the folding frequency.
 
fd 5
D
 fs
2  
 (1.3.8)
Thus the folding frequency is simply one half of the sampling frequency. If xa(t) has any 
frequency components outside of fd, then in x⁄
a(t) these frequencies get reflected about fd 
and folded back into the range f2fd, fdg. For the case in Example 1.5, fd 5 50 Hz. Thus 
the original frequency component at F0 5 90 Hz gets reflected about fd to produce a fre-
quency component at 10 Hz. Notice that in Figure 1.19 the folding frequency is at the 
center of the first region of overlap. The part of the spectrum of xa(t) that lies outside of 
the folding frequency gets aliased back into the range f2fd,  fdg as a result of the overlap.
Folding frequency
Figure 1.20: Common Samples of Two Bandlimited Signals
0
0.02
0.04
0.06
0.08
0.1
22
21.5
21
20.5
0
0.5
1
1.5
2
t (sec)
x(t)
xa
xb
 5  sin(2k 2 .2k)
 5  sin(2k) cos(.2k) 2  cos(2k) sin(.2k)
 5 2sin(.2k)
 5 2sin(20kT )
Thus the samples of the 90 Hz signal xa(t) 5  sin(180t) are identical to the sam-
ples of the following lower-frequency signal that has its power concentrated at  
10 Hz.
xb(t) 5 2 sin(20t)
A plot comparing the two signals xa(t) and xb(t) and their shared samples is shown 
in Figure 1.20.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.4 Reconstruction of Continuous-time Signals    27
Reconstruction of Continuous-time Signals
1.4.1 Reconstruction Formula
When the signal xa(t) is bandlimited and the sampling rate fs is higher than twice the 
bandwidth, the samples x(k) contain all the information needed to reconstruct xa(t). To 
illustrate the reconstruction process, consider the bandlimited signal whose magnitude 
spectrum is shown in Figure 1.21a. In this case we have oversampled by selecting a sam-
pling frequency fs 5 300 Hz that is three times the bandwidth, B 5 100 Hz. The magni-
tude spectrum of x⁄
a(t) is shown in Figure 1.21b. Note how the increase in fs beyond 2B 
has caused the sidebands to spread out so they no longer overlap with each other or the 
baseband. In this case there are no spectral components of xa(t) beyond the folding fre-
quency fd 5 fsy2 to be aliased back into the range f2fd, fdg.
The problem of reconstructing the signal xa(t) from x⁄
a(t) reduces to one of recover-
ing the spectrum Xa( f ) from the spectrum X 
⁄
a( f ). This can be achieved by passing x⁄
a(t) 
through an ideal lowpass reconstruction filter Hideal( f ) that removes the side bands and 
rescales the base band. The required frequency response of the reconstruction filter is 
shown in Figure 1.22. To remove the side bands, the cutoff frequency of the filter should 
be set to the folding frequency. From the aliasing formula in (1.3.7), the gain of the filter 
needed to rescale the baseband is T. Thus the required ideal lowpass frequency response is
 
Hideal( f ) 5
D  5
T, u f  u # fd
0, u f  u . fd
 
 (1.4.1)
1.4
Oversampled
Figure 1.21:  
Magnitude Spectra 
of xa(t) in (a) and 
x⁄
a(t) in (b) when 
B 5 100, fs 5 3B 
(a)
2600
2400
2200
0
200
400
600
0
0.5
1
1.5
2
fd
fs
2fs
B
2B
f (Hz)
|xa(f)|
(b)
|xa(f )|
2600
2400
2200
0
200
400
600
0
100
200
300
400
f (Hz)
fs
2fs
fd
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

28    Chapter 1  Signal Processing
From the aliasing formula in (1.3.7), one can recover the spectrum of xa(t) as follows.
 
Xa( f ) 5 Hideal( f )X 
⁄
a( f ) 
 (1.4.2)
Using (1.2.20) and the table of Fourier transform pairs in Appendix 1, the impulse 
response of the ideal reconstruction filter is
hideal(t) 5
 IFT hHideal( f  )j
 5 2T fd  sinc (2fdt)
 
 5
 sinc ( fst)
 
 (1.4.3)
Next, we take the inverse Fourier transform of both sides of (1.4.2). Using (1.3.4), the 
sifting property of the unit impulse, and the convolution property of the Fourier trans-
form (Appendix 1), we have
xa(t) 5
 IFT hHideal( f  )X 
⁄
a( f  )j
 5#
`
2`
hideal(t 2 )x ⁄
a()d
 5#
`
2`
hideal(t 2 ) o
`
k52`
x(k)a( 2 kT )d
 5 o
`
k52`
x(k)#
`
2`
hideal(t 2 )a( 2 kT )d
 
 5 o
`
k52`
x(k)hideal(t 2 kT )
 
 (1.4.4)
Finally, substituting (1.4.3) into (1.4.4) yields the Shannon reconstruction formula.
Suppose a continuous-time signal xa(t) is bandlimited to B Hz. Let x(k) 5 xa(kT ) be 
the kth sample of xa(t) using a sampling frequency of fs 5 1yT. If fs . 2B, then xa(t) 
can be reconstructed from x(k) as follows.
xa(t) 5 o
`
k52`
x(k) sinc f fs(t 2 kT )g
The Shannon reconstruction formula is an elegant result that is valid as long as xa(t) 
is bandlimited to B and fs . 2B. Note that the sinc function is used to interpolate between 
the samples. The importance of the reconstruction formula is that it demonstrates that 
all the essential information about xa(t) is contained in the samples x(k) as long as xa(t) is 
bandlimited and the sampling rate exceeds twice the bandwidth.
PRoPoSiTioN
1.2 Signal Reconstruction
Figure 1.22:  
Frequency 
Response of  
Ideal Lowpass 
Reconstruction 
Filter
ƒ
Hideal(ƒ)
2ƒd
0
ƒd
T
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.4 Reconstruction of Continuous-time Signals    29
1.4.2 Zero-order Hold
Exact reconstruction of xa(t) from its samples requires an ideal filter. One can approxi-
mately reconstruct xa(t) using a practical filter. We begin by noting that an efficient way 
to characterize a linear time-invariant continuous-time system in general is in term of its 
transfer function.
Let xa(t) be a causal nonzero input to a continuous-time linear system, and let ya(t) be 
the corresponding output assuming zero initial conditions. Then the transfer function 
of the system is defined
Ha(s) 5
D  Ya(s)
Xa(s)
Thus the transfer function is just the Laplace transform of the output divided by the 
Laplace transform of the input assuming zero initial conditions. A table of common 
Laplace transform pairs can be found in Appendix 1. From the sifting property of the 
unit impulse in (1.2.13), the Laplace transform of the unit impulse is Xa(s) 5 1. In view 
of Definition 1.8, this means that an alternative way to characterize the transfer function 
is to say that Ha(s) is the Laplace transform of the impulse response, ha(t).
 
Ha(s) 5 Lhha(t)j 
 (1.4.5)
Again note that the same base symbol, Ha, is being used to denote both the transfer 
function, Ha(s), in Definition 1.8, and the frequency response, Ha(f), in Definition 1.5. 
The notational convention adopted here and throughout the text is to rely on the argu-
ment type, a complex s or a real f , to distinguish between the two cases and dictate the 
meaning of Ha.
DEFiNiTioN
1.8 Transfer Function
Notation!
Signal Reconstruction
EXAMPLE 1.6
Consider the following signal. For what range of values of the sampling interval 
T can this signal be reconstructed from its samples?
xa(t) 5 sin(5t) cos(3t)
The signal xa(t) does not appear in the table of Fourier transform pairs in Appen-
dix 1. However, using the trigonometric identities in Appendix 2 we have
xa(t) 5
 sin(8t) 1  sin(2t)
2
Since xa(t) is the sum of two sinusoids and the Fourier transform is linear, it fol-
lows that xa(t) is bandlimited with a bandwidth of B 5 4 Hz. From the sampling 
theorem we can reconstruct xa(t) from its samples if fs . 2B or 1yT . 8. Hence 
the range of sampling intervals over which xa(t) can be reconstructed from x(k) is
0 , T , .125 sec
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

30    Chapter 1  Signal Processing
As an illustration of a continuous-time system and its transfer function, consider 
a system which delays the input by  seconds.
ya(t) 5 xa(t 2 )
This type of system might be used, for example, to model a transportation lag in 
a process control system or a signal propagation delay in a telecommunication 
system. Using the definition of the Laplace transform in (1.3.5) and the fact that 
xa(t) is causal, we have
Ya(s) 5 Lhxa(t 2 )j
 5#
`
0
xa(t 2 ) exp(2st)dt
 5#
`
2
xa() expf2s( 1 )gd j  5 t 2 
 5  exp(2s)#
`
0
xa() exp(2s)d
 5  exp(2s)Xa(s)
It then follows from Definition 1.8 that the transfer function of a transporta-
tion lag with delay  is
Ha(s) 5 exp(2s)
A block diagram of the transportation lag is shown in Figure 1.23.
The reconstruction of xa(t) in Proposition 1.2 interpolates between the samples using 
the sinc function. A simpler form of interpolation is to use a low degree polynomial fitted 
to the samples. To that end, consider the following linear system with a delay called a 
zero-order hold (ZOH).
 
ya(t) 5#
t
0
fxa() 2 xa( 2 T)gd 
 (1.4.6)
Recalling that the integral of the unit impulse is the unit step, we find that the impulse 
response of this system is
h0(t) 5#
t
0
fa() 2 a( 2 T)gd
 5#
t
0
a()d 2#
t
0
a( 2 T)d
 
 5 a(t) 2 a(t 2 T )
 
 (1.4.7)
Zero-order hold
Figure 1.23: Transfer Function of Transportation Lag with Delay 
exp(2s)
xa(t)
xa(t 2 )
Transportation Lag
EXAMPLE 1.7
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.4 Reconstruction of Continuous-time Signals    31
Thus the impulse response of the zero-order hold filter is a pulse of unit height and 
width T starting at t 5 0, as shown in Figure 1.24.
Since the zero-order hold is linear and time-invariant, the response to an impulse of 
strength x(k) at time t 5 kT will then be a pulse of height x(k) and width T starting at 
t 5 kT. When the input is x⁄
a(t), we simply add up all the responses to the scaled and shifted 
impulses to get a piecewise-constant approximation to xa(t) as shown in Figure 1.25. Notice 
that this is equivalent to interpolating between the samples with a polynomial of degree 
zero. It is for this reason that the system in (1.4.6) is called a zero-order hold. It holds onto 
the most recent sample and extrapolates to the next one using a polynomial of degree zero.
Figure 1.24:  
Impulse Response 
of Zero-order Hold 
Filter
21 20.5
0.5
1
1.5
2
2.5
3
3.5
0
4
20.5
0
0.5
1
1.5
t/T
h0(t)
Figure 1.25:  
Reconstruction  
of xa(t) with a 
Zero-order Hold
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
2.5
3
3.5
4
t (sec)
y0(t)
y0
xa
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

32    Chapter 1  Signal Processing
The zero-order hold filter also can be described in terms of its transfer function. 
Recall from (1.4.5) that the transfer function is just the Laplace transform of the impulse 
response in (1.4.7). Using the linearity of the Laplace transform and the results of Exam-
ple 1.7, we have
H0(s) 5 Lhh0(t)j
 5 Lha(t)j 2 Lha(t 2 T)j
 
 5 f1 2  exp(2Ts)gLha(t)j 
 (1.4.8)
Finally, from the table of Laplace transform pairs in Appendix 1, the transfer function 
for a zero-order hold filter is
 
H0(s) 5 1 2  exp(2Ts)
s
 
 (1.4.9)
A typical DSP system was described in Figure 1.2 as an analog-to-digital conver-
ter (ADC), followed by a digital signal processing algorithm, followed by a digital- 
to-analog converter (DAC). We now have mathematical models available for the two  
converter blocks. The ADC can be modeled by an impulse sampler, while the DAC can 
be modeled with a zero-order hold filter as shown in Figure 1.26, which is an updated 
version of Figure 1.2. Note that the impulse sampler is represented symbolically with a 
switch that opens and closes every T seconds.
It should be emphasized that the formulation depicted in Figure 1.26 uses mathematical  
models of signal sampling and signal reconstruction. With the impulse model of sam-
pling we are able to determine constraints on Xa(f) and T that ensure that all the essential 
information about xa(t) is contained in the samples x(k). For signal reconstruction, the 
piecewise-constant output from the zero-order hold filter is an effective model for the 
DAC output signal. To complement the mathematical models in Figure 1.26, physical 
circuit models of the ADC and the DAC blocks are investigated in Section 1.6.
1.4.3 Delayed First-order Hold
The piecewise-constant interpolation between samples generated by a zero-order hold filter 
can be refined by using a first-order hold (FOH) filter. The objective here is to construct a 
filter whose output generates a piecewise-linear interpolation between samples. As a starting 
point, consider the following triangle function of unit height and unit radius centered at zero.
 
 Tri (t) 5
D  5
1 2 utu, utu # 1
0,
utu . 1
 
 (1.4.10)
Suppose the impulse response of a reconstruction filter is h(t) 5 (1yT ) Tri (tyT ). The 
response to the amplitude modulated impulse train, x⁄
a(t), in (1.3.4) is then
 
y(t) 5 o
`
k52`
x(k) Tri 1
t 2 kT
T 2 
 (1.4.11)
The output y(t) represents a piecewise-linear interpolation between the samples. Unfortu-
nately, from (1.4.10) it is clear that the impulse response h(t) is not a causal signal, because 
Mathematical models
Triangle function
Figure 1.26: Mathematical Model of DSP System
T
ADC
DSP
algorithm
1 2 exp(2Ts)
s
DAC
xa
yb
xa
ya
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.4 Reconstruction of Continuous-time Signals    33
the triangle is centered at t 5 0. As a consequence, a filter with impulse response h(t) cannot 
be implemented in real time. However, if we delay the impulse response by one sample, then 
it does become a causal signal. This is referred to as a delayed first-order hold.
 
h1(t) 5 1
T
  Tri  1
t 2 T
T 2 
 (1.4.12)
A plot of the delayed zero-order hold impulse response is shown in Figure 1.27, where 
it is evident that h1(t) is causal. Figure 1.28 shows the reconstruction of xa(t) using the 
delayed first-order hold filter. Clearly, piecewise-linear interpolation is a better representa-
tion of the shape of the underlying analog signal than the piecewise-constant interpolation 
shown previously in Figure 1.25, but it is shifted to the right by one sample due to the delay. 
As the sampling frequency fs increases, the practical effect of the delay is reduced.
Figure 1.28:  
Reconstruction 
of xa(t) with a 
Delayed First-order 
Hold
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
2.5
3
3.5
4
t (sec)
y1(t)
y1
xa
Figure 1.27:  
Impulse Response 
of Delayed First-
order Hold Filter
21 20.5
0.5
1
1.5
0
2
2.5
3
3.5
4
20.5
0
0.5
1
1.5
t/T
h1(t)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

34    Chapter 1  Signal Processing
The delayed first-order hold filter can be represented in the frequency domain using a 
transfer function. First note that the triangular impulse response can be written as a sum 
of delayed ramp functions as follows.
 
h1(t) 5 1
T 31
t
T2
 a(t) 2 21
t 2 T
T 2
 a(t 2 T ) 11
t 2 2T
T 2
 a(t 2 2T )4   (1.4.13)
Using the Laplace transform table and the time shift property in Appendix 1 then yields
 
H1(s) 5 1
T 2 3
1
s2 2 2 exp(2Ts)
s2
1
 exp(22Ts)
s2
4 
 (1.4.14)
Thus the transfer function of the delayed first-order hold is expressed simply as
 
H1(s) 53
1 2  exp(2Ts)
Ts
4
2
 
 (1.4.15)
Notice the similarity with the zero-order hold transfer function in (1.4.9). It indicates 
that if the output of a zero-order hold is scaled by 1yT 2 and then sent through a second 
zero-order hold, the end result is a delayed first-order hold. An alternative approach to 
reconstructing xa(t) is to extrapolate from the last two samples using a predictive first-
order hold filter (Ingle and Proakis, 2000).
Delayed first-order 
hold
The DSP Companion is discussed in detail in Section 1.7. It includes the following 
function for evaluating the frequency response of a linear continuous-time system.
% F_FREQS: Compute frequency response of continuous-time system
%
% 
b(1)s^m + b(2)s^(m-1) + ... + b(m+1)
% 
 H(s) = ------------------------------------
% 
a(1)s^n + a(2)s^(n-2) + ... + a(n+1)
%
% Usage:
% 
[H,f] = f_freqs (b,a,N,fmax);
% Pre:
%  
b = vector of length m+1 containing numerator coefficients
%  
a = vector of length n+1 containing denominator coefficients
%  
N = number of discrete frequencies
% 
fmax = maximum frequency (0 <= f <= fmax)
% Post:
%  
H = 1 by N complex vector containing the frequency response
%  
f = 1 by N vector the containing frequencies at which H is
% 
evaluated
% Notes:
% 
H(s) must be stable.
To plot the magnitude and phase responses on a single screen, one can use the 
following built-in MATLAB functions.
A = abs(H);  
% magnitude response
phi = angle(H);  
% phase response
subplot(2,1,1)  
% top half of screen
plot (f,A)  
% magnitude response plot
subplot(2,1,2)  
% bottom half of screen
plot(f,phi)  
% phase response plot
DSP Companion
DSP Companion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.5 Prefilters and Postfilters    35
Prefilters and Postfilters
The sampling theorem in Proposition 1.1 tells us that to avoid aliasing during the 
sampling process, the signal xa(t) must be bandlimited and we must sample at a rate 
that is greater than twice the bandwidth. Unfortunately, a quick glance at the Fourier 
transform pair table in Appendix 1 reveals that many of those signals are, in fact, not 
bandlimited. The exceptions are sines and cosines that have all their power concen-
trated at a single frequency. Of course, a general periodic signal can be expressed as a 
Fourier series, and as long as the series is truncated to a finite number of harmonics, 
the resulting signal will be bandlimited. For a general nonperiodic signal, if we are to 
avoid aliasing we must bandlimit the signal explicitly by passing it through an analog 
lowpass filter as in Figure 1.29.
1.5.1 Anti-aliasing Filter
The prefilter in Figure 1.29 is called a anti-aliasing filter or a guard filter. Its function 
is to remove all frequency components outside the range f2Fc, Fcg where Fc , fd, so 
that aliasing does not occur during sampling. Here fd 5 fsy2 is the folding frequency. 
The optimal choice for an anti-aliasing filter is an ideal lowpass filter. Since this filter 
is not physically realizable, we instead approximate the ideal lowpass characteristic. 
For example, a widely used family of lowpass filters is the set of Butterworth filters 
(Ludeman, 1986). A lowpass Butterworth filter of order n has the following magnitude 
response.
 
uHa( f )u 5
1
Ï1 1 ( fyFc)2n,  n $ 1 
 (1.5.1)
Notice that uHa(Fc)u 5 1yÏ2 where Fc is called the 3 dB cutoff frequency of the filter. The 
term arises from the fact that when the filter gain is expressed in units of decibels or dB 
we have
 
20 log10{|Ha(Fc)|} < 23 dB 
 (1.5.2)
The magnitude responses of several Butterworth filters are shown in Figure 1.30. 
Note that as the order n increases, the magnitude response approaches the ideal lowpass 
characteristic, which is shown for comparison. However, unlike the ideal lowpass filter, 
the Butterworth filters introduce phase shift as well.
The transfer function of a lowpass Butterworth filter of order n with radian cutoff 
frequency Vc 5 2Fc can be expressed as follows.
 
Ha(s) 5
Vn
c
sn 1 Vca1sn 2 1 1 V2
ca2sn 2 2 Á 1 Vn
c
 
 (1.5.3)
1.5
Anti-aliasing filter
Butterworth filters
Cutoff frequency
Figure 1.29: DSP System with Analog Prefilter and Postfilter
Anti-
aliasing
ﬁlter
ADC
DSP
algorithm
DAC
Anti-
imaging
ﬁlter
xb
xa
yb
yc
xa
ya
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

36    Chapter 1  Signal Processing
When Vc 5 1 rad/s or Fc 5 .5y Hz, this corresponds to a normalized Butterworth filter. 
A list of the coefficients for the first few normalized Butterworth filters is summarized in 
Table 1.3. Notice that the denominator polynomials in Table 1.3 are factored into qua-
dratic factors when n is even and into quadratic and linear factors when n is odd. This 
is done because the preferred way to realize Ha(s) with a circuit is as a series or cascade 
configuration of first- and second-order blocks. That way, the overall transfer function is 
less sensitive to the precision of the circuit elements.
Normalized  
Butterworth filter 
order  
a(s)
1 
(s 1 1)
2 
(s2 1 1.5142s 1 1) 
3 
(s 1 1)(s2 1 s 1 1) 
4 
(s2 1 1.8478s 1 1)(s2 1 0.7654s 1 1) 
5 
(s 1 1)(s2 1 1.5180s 1 1)(s2 1 0.6180s 1 1) 
6 
(s2 1 1.9318s 1 1)(s2 1 1.5142s 1 1)(s2 1 0.5176s 1 1) 
7 
(s 1 1)(s2 1 1.8022s 1 1)(s2 1 1.2456s 1 1)(s2 1 0.4450s 1 1) 
8 
 (s2 1 1.9622s 1 1)(s2 1 1.5630s 1 1)(s2 1 1.1110s 1 1)(s2 1 0.3986s 1 1) 
Table 1.3: Second-
order Factors 
of Normalized 
Butterworth Lowpass 
Filters 
Figure 1.30:  
Magnitude 
Responses 
of Lowpass 
Butterworth Filters 
with Fc 5 1 
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.2
0.4
0.6
0.8
1
1.2
f (Hz)
|Ha(f)|
2
4
8
16
Ideal filter
First-order Filter
EXAMPLE 1.8
Consider the problem of realizing a lowpass Butterworth filter of order n 5 1 
with a circuit. From Table 1.3, we have a1 5 1. Thus from (1.5.3) the first-order 
transfer function is
H1(s) 5
Vc
s 1 Vc
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.5 Prefilters and Postfilters    37
Since all of the first-order filter sections in Table 1.3 have a1 5 1, the filter in  
Figure 1.31 can be used for a general first-order filter section. An nth order Butterworth 
filter also contains second-order filter sections.
This transfer function can be realized with a simple RC circuit with RC 5 1yVc. 
However, a passive circuit realization of H1(s) can experience electrical loading 
effects when it is connected in series with other blocks to form a more general fil-
ter. Therefore, consider instead the active circuit realization shown in Figure 1.31,  
which uses three operational amplifiers (op amps). This circuit has a high input 
impedance and a low output impedance, which means it will not introduce signifi-
cant loading effects when connected to other circuits. The transfer function of the 
circuit (Dorf and Svoboda, 2000) is
H1(s) 5
1y(RC)
s 1 1y(RC)
Thus we require 1y(RC) 5 Vc. Typically, C is chosen to be a convenient value, 
and then R is computed using
R 5
1
VcC
As an illustration, suppose a cutoff frequency of Fc 5 1000 Hz is desired. Then 
Vc 5 2000. If C 5 0.01 F, which is a common value, then the required value 
for R is
R 5 15.915  k V
Integrated circuits typically contain up to four op amps. Therefore, the first-
order filter section can be realized with a single integrated circuit and seven dis-
crete components. The resistors in Figure 1.31 all have the same resistance R, so 
isolated resistor network chips can be used.
Figure 1.31: Active Circuit Realization of First-order Lowpass Butterworth  
Filter Block
R
R
1
1
2
2
R
C
2
1
xa
2
1
ya
1
2
R
R
R
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

38    Chapter 1  Signal Processing
Figure 1.32: Active Circuit Realization of Second-order Lowpass Butterworth 
Filter Block
R1
R1
R1
R1
R1
R2
R1
R1
1
1
1
1
2
2
2
2
C
C
1
2
1
2
xa
ya
Second-order Filter
EXAMPLE 1.9
Consider the problem of realizing a lowpass Butterworth filter of order n 5 2 with a 
circuit. From (1.5.3) the general form of the second-order transfer function is
H2(s) 5
V2
c
s2 1 Vca1 1 V2
c
Using a state-space formulation of H2(s), this second-order block can be realized 
with the active circuit shown in Figure 1.32, which uses four op amps. The trans-
fer function of the circuit (Dorf and Svoboda, 2000) is
H2(s) 5
1y(R1C)2
s2 1 sy(R2C) 1 1y(R1C)2
From the last term of the denominator, we require 1y(R1C)2 5 V2
c. Again, typi-
cally C is chosen to be a convenient value, and then R1 is computed using
R1 5
1
VcC
Next, from the linear term of the denominator we have 1y(R2C) 5 Vca1. Solving 
for R2 and expressing the final result in terms of R1 yields
R2 5 R1
a1
As an illustration, suppose a cutoff frequency of Fc 5 5000 Hz is desired. Then 
Vc 5 104, and from Table 1.3 we have a1 5 1.5142. If we pick C 5 0.01 F, then 
the two resistors are
R1 < 3.183  k V
R2 < 2.251  k V
This filter also can be realized with a single integrated circuit plus 10 discrete 
components.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.5 Prefilters and Postfilters    39
A general lowpass Butterworth filter can be realized by using a cascade connection of 
first- and second-order blocks where the output of one block is used as the input to the 
next block. In this way an anti-aliasing filter of order n can be constructed.
The Butterworth family of lowpass filters is one of several families of classical ana-
log filters (Lam, 1979). Other classical analog filters that could be used for anti-aliasing 
filters include Chebyshev filters and elliptic filters. Classical analog lowpass filters are 
considered in detail in Chapter 7, where we investigate a digital design technique that 
converts an analog filter into an equivalent digital filter.
Classical analog filters are sufficiently popular that they have been implemented as 
integrated circuits using switched capacitor technology (Jameco, 2014). For example, the 
National Semiconductor LMF6-100 is a sixth-order lowpass Butterworth filter whose 
cutoff frequency is tunable with an external clock signal of frequency fclock 5 100Fc. 
The switched capacitor technology involves internal sampling or switching at the rate 
fclock. Therefore, if the signal xa(t) does not have any significant spectral content beyond 
50 times the desired sampling rate fs, then the switch-capacitor filter can be used as an 
anti-aliasing filter to remove the frequency content in the range ffsy2, 50fsg. For example, 
with a desired sampling rate of 2 kHz, a switched-capacitor filter can remove (i.e., signifi-
cantly reduce) frequencies in the range from 1 kHz to 100 kHz.
1.5.2 Anti-imaging Filter
The postfilter in Figure 1.29 is called an anti-imaging filter or smoothing filter. The pur-
pose of this filter is to remove the residual high-frequency components of yb(t). The 
zero-order hold transfer function of the DAC tends to reduce the size of the sidebands 
of y⁄
a(t) which can be thought of as images of the baseband spectrum. However, it does 
not eliminate them completely. The magnitude response of the zero-order hold, shown 
in Figure 1.33, reveals a lowpass type of characteristic that is different from the ideal 
reconstruction filter due to the presence of a series of lobes. For the lowpass filter in  
Figure 1.33, the output is the piecewise constant signal yb(t) in Figure 1.26, rather than 
the ideally reconstructed signal ya(t).
Cascade connection
Anti-imaging filter
Figure 1.33:  
Magnitude 
Response of  
Zero-order Hold
23
22
21
0
1
2
3
0
0.2
0.4
0.6
0.8
1
1.2
f/fs
|H0(f )|
DAC
Ideal reconstruction
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

40    Chapter 1  Signal Processing
Note from Figure 1.33 that the zero-order hold has zero gain at multiples of the sampling 
frequency where the images of the baseband spectrum are centered. The effect of this lowpass 
type of characteristics is to reduce the size of the sidebands as can be seen Figure 1.34 which 
shows a discrete-time DAC input with a triangular spectrum in Figure 1.34a and the corre-
sponding spectrum of the piecewise-constant DAC output in Figure 1.34b.
The purpose of the anti-imaging filter is to further reduce the residual images of 
the baseband spectrum centered at multiples of the sampling frequency. A lowpass filter 
similar to the anti-aliasing filter can be used for this purpose. A careful inspection of the 
baseband spectrum of yb(t) in Figure 1.34 reveals that it is slightly distorted due to the 
nonflat passband characteristic of the zero-order hold shown in Figure 1.33. Interestingly 
enough, this can be compensated for with a digital filter as part of the DSP algorithm by 
preprocessing y⁄
a(t) before it is sent into the DAC.
A simple way to reduce the need for an anti-imaging filter is to increase the sampling 
rate fs beyond the minimum needed to avoid aliasing. As we shall see in Chapter 8, the DAC 
sampling rate can be increased as part of the DSP algorithm by inserting samples between 
those coming from the ADC, a process known as interpolation. The effect of oversampling is 
to spread out the images along the frequency axis, which allows the zero-order hold to more 
effectively attenuate them. This can be seen in Figure 1.35, which is identical to Figure 1.34  
except that the signals have been oversampled by a factor of two. Notice that oversampling 
also has the beneficial effect of reducing the distortion of the baseband spectrum that is 
caused by the nonideal passband characteristic of the zero-order hold.
Finally, it is worth noting that there are applications where an anti-imaging filter  
may not be needed at all. For example, in a digital control application the output of the 
DAC might be used to drive a relatively slow electro-mechanical device such as a motor. 
These devices already have a lowpass frequency response characteristic, so it is not nec-
essary to filter the DAC output. For some DSP applications, the desired output may be 
Figure 1.34:  
Magnitude Spectra 
of DAC Input in  
(a) and Output in 
(b) when fs 5 2B 
(a)
2400
2200
0
200
400
0
100
200
300
f (Hz)
|ya(f)|
2400
2200
0
200
400
0
0.5
1
1.5
f (Hz)
|yb(f)|
(b)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 1.6 DAC and ADC Circuits     41
information that can be extracted directly from the discrete-time signal y⁄
a in which case 
there is no need to convert from digital back to analog.
DAC and ADC Circuits 
This optional material presents circuit realizations of digital-to-analog and analog-to-digital 
converters. This material is included for those readers specifically interested in hardware. This 
section, and others like it marked with *, can be skipped without loss of continuity.
1.6.1 Digital-to-analog Converter (DAC)
Recall that a digital-to-analog converter or DAC can be modeled mathematically as a 
zero-order hold as in (1.4.9). A physical model of a DAC is entirely different because 
the input is an N-bit binary number, b 5 bN21bN22 Á b1b0, rather than an amplitude- 
modulated impulse train. A DAC is designed to produce an analog output ya that is pro-
portional to the decimal equivalent of the binary input b. DAC circuits can be classified 
as unipolar if ya $ 0 or bipolar if ya is both positive and negative. For the simpler case of 
a unipolar DAC, the decimal equivalent of the binary input b is
 
x 5 o
N21
k50
bk2k 
 (1.6.1)
The binary input of a bipolar DAC represents negative numbers as well using two’s com-
plement, offset binary, or a sign-magnitude format (Grover and Deller, 1999). The most 
common type of DAC is the R-2R ladder circuit shown in Figure 1.36 for the case N 5 4. 
The configuration of resistors across the top is the R-2R ladder. On each rung of the 
 1.6
Optional material
DAC
Unipolar, bipolar
Figure 1.35:  
Magnitude Spectra 
of DAC Input in  
(a) and Output in 
(b) when fs 5 4B 
(a)
2800
2400
0
400
800
0
200
400
600
f (Hz)
|ya(f)|
(b)
2800 
2400 
0
400
 800 
0
0.5
1
1.5
f (Hz)
|yb(f)|
*
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

42    Chapter 1  Signal Processing
ladder is a digitally controlled single pole double throw (SPDT) switch that sends current 
into either the inverting input (when bk 5 1) or the noninverting input (when bk 5 0) of 
the operational amplifier or op amp. 
To analyze the operation of the circuit in Figure 1.36, we begin at the end of the ladder 
and work backwards. First note that for an ideal op amp, the voltage at the inverting input 
is the same as that at the noninverting input, namely V 5 0. Consequently, the current Ik 
through the kth switch does not depend on the switch position. The equivalent resistance 
looking into node 0 from the left is therefore R0 5 R because it consists of two resistors of 
resistance 2R in parallel. Thus the current entering node 0 from the left is split in half. Next 
consider node 1. The equivalent resistance to the right of node 1 is 2R because it consists of a 
series combination of R and R0. This means that the equivalent resistance looking into node 
1 from the left is again R1 5 R because it consists of two resistors of resistance 2R in parallel. 
This again means that the current entering node 1 from the left is split in half. This process 
can be repeated as many times as needed until we conclude that the equivalent resistance 
looking into node N 2 1 from the left is R. Consequently, the current drawn by the R-2R 
ladder, regardless of the switch positions, is as follows where Vr is the reference voltage.
 
Ir 5 2Vr
R  
 (1.6.2)
Since the current is split in half each time it enters another node of the ladder, the 
current shunted through the kth switch is Ik 5 Iry2N 2 k. Consequently, using (1.6.1) the 
total current entering the op amp section is
I 5 o
N21
k50
bkIk
 5 o
N21
k50
bkIr2k 2 N
 51
Ir
2N2o
N21
k50
bk2k
 
 51
Ir
2N2x
 
 (1.6.3)
Reference voltage
Figure 1.36: A 4-bit Unipolar DAC Using an R-2R Ladder
GND
2
1
R
I
V
b3
b2
b1
b0
I3
2R
R
3
2
1
0
R
R
1
0
1
0
1
0
1
0
I2
2R
I1
2R
I0
2R
2R
2Vr
ya
Ir
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 1.6 DAC and ADC Circuits     43
Finally, an ideal op amp has infinite input impedance, which means that the current drawn 
by the inverting input is zero. Therefore, using (1.6.2) and (1.6.3), the op amp output is
ya 5 2RI
 51
2RIr
2N 2x
 
 51
Vr
2N2x
 
 (1.6.4)
Setting the binary input in (1.6.4) to x 5 1, we see that the quantization level of the 
DAC is q 5 Vry2N. The range of output values for the DAC in Figure 1.36 is
 
0 # ya #1
2N 2 1
2N 2Vr 
 (1.6.5)
In view of (1.6.5), the DAC in Figure 1.36 is a unipolar DAC. It can be converted to a 
bipolar DAC with outputs in the range 2Vr # ya , Vr by replacing Vr with 2Vr and add-
ing a second op amp circuit at the output that performs level shifting (see Problem 1.16).  
In this case b is interpreted as an offset binary input whose decimal equivalent is as fol-
lows, where x is as in (1.6.1).
 
xbipolar 5 x 2 2N21 
 (1.6.6)
In general, a signal conditioning circuit can be added to the DAC in Figure 1.36 to per-
form scaling, offset, and impedance matching (Dorf and Svoboda, 2000). It is useful for 
the DAC circuit to have a low output impedance so that the DAC output is capable of 
supplying adequate current to drive the desired load.
1.6.2 Analog-to-digital Converter (ADC)
An analog-to-digital converter or ADC must take an analog input 2Vr # xa , Vr and 
convert it to a binary output b whose decimal value is equivalent to xa. The input-output 
characteristic of an N-bit bipolar ADC is shown in Figure 1.37 for the case Vr 5 5 and 
Signal conditioning
ADC
Figure 1.37:  
Input-Output 
Characteristic  
of 4-bit ADC with 
Vr 5 5 
25
24
23
22
21
1
2
3
4
0
5
28
26
24
22
0
2
4
6
8
xa
x
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

44    Chapter 1  Signal Processing
N 5 4. Note that the staircase is shifted to the left by half a step so that the ADC output 
is less sensitive to low-level noise when xa 5 0. The horizontal length of the step is the 
quantization level which, for a bipolar ADC, can be expressed as:
 
q 5
Vr
2N 2 1 
 (1.6.7)
Successive-approximation Converters
The most widely used ADC consists of a comparator circuit plus a DAC in the feedback 
configuration shown in Figure 1.38. The input is the analog voltage xa to be converted, 
and the output is the equivalent N-bit binary number b.
A very simple form of the ADC can be realized by replacing the block labeled 
SAR logic with a binary counter that counts the pulses of the periodic pulse train or 
clock signal, fclock. As the counter output b increases from 0 to 2N 2 1, the DAC output 
ya ranges from 2Vr to Vr. When the value of ya becomes larger than the input xa, the 
comparator output u switches from 1 to 0, at which time the counter is disabled. The 
count b is then the digital equivalent of the analog input xa using the offset binary code 
in (1.6.6).
Although an ADC based on the use of a counter is appealing because of its concep-
tual simplicity, there is a significant practical drawback. The time required to perform 
a conversion is variable and can be quite long. For random inputs with a mean value 
of zero, it takes on the average 2N21 clock pulses to perform a conversion, and it can 
take as long as 2N clock pulses when xa 5 Vr. A much more efficient way to perform a 
conversion is to execute a binary search for the proper value of b by using a successive 
approximation register (SAR). The basic idea is to start with the most significant bit, 
bN21, and determine if it should be 0 or 1 based on the comparator output. This cuts 
the range of uncertainty for the value of xa in half and can be done in one clock pulse. 
Once bN21 is determined, the process is then repeated for bit bN22 and so on until the least 
significant bit, b0, is determined. The successive approximation technique is summarized 
in Algorithm 1.1.
Figure 1.38: An  
N-bit Successive 
Approximation ADC
xa
2
1
u
SAR
logic
ƒclock
b
N
DAC
Vr
ya
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 1.6 DAC and ADC Circuits     45
1. Set Dy 5 Vr, ya 5 2Vr 
2. For k 5 0 to N 2 1 do 
h  
(a) Set i 5 N 2 1 2 k 
(b) If ya 1 Dy . xa, set u(k) 5 0, bi 5 0; else set u(k) 5 1, bi 5 1. 
(c) Set 
ya  5 ya 1 biDy
Dy 5 Dyy2
j 
The virtue of the binary search approach is that it takes exactly N clock pulses to per-
form a conversion, independent of the value of xa. Thus the conversion time is constant 
and the process is much faster. For example, for a precision of N 5 12 bits, the conversion 
time in comparison with the counter method is reduced, on the average, by a factor of 
211y12 5 170.7 or two orders of magnitude.
ALgoRiTHM
1.1 Successive 
Approximation
Successive Approximation
EXAMPLE 1.10
As an illustration of the successive approximation conversion technique, suppose the 
reference voltage is Vr 5 5 volts, the converter precision is N 5 10 bits, and the value 
to be converted is xa 5 2.891 volts. When Algorithm 1.1 is applied, the traces of ya(k) 
and ua(k) for 0 # k , N are as shown in Figure 1.39. Note how the DAC output 
quickly adjusts in Figure 1.39a to the value of xa by cutting the interval of uncertainty 
in half with each clock pulse. From (1.6.7), the quantization level in this case is
q 5
Vr
2N 2 1
 5 5
512
 5 9.8  mV 
(a)
0
1
2
3
4
5
6
7
8
25
22.5
0
2.5
5
k
ya
xa
Figure 1.39: DAC Output. (a) and SAR Input
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

46    Chapter 1  Signal Processing
Flash Converters
There is another type of ADC, called the flash converter, that is used in applications 
where very high-speed conversion is essential, such as in a digital oscilloscope. A simple 
2-bit flash converter is shown in Figure 1.40. It consists of a linear resistor array, an array 
of comparator circuits, and an encoder circuit.
The resistors in the resistor array are selected such that the voltage drop between 
successive inverting inputs to the comparators is the quantization level q in (1.6.7). The 
fractional resistances at the ends of the array cause the input-output characteristic to 
shift to the left by qy2 as shown previously in Figure 1.37. Therefore, the voltage at the 
inverting input of the kth comparator is
 
ck 5 2Vr 11k 1 1
22q,  0 # k , N 2 1 
 (1.6.8)
The analog input xa is compared to each of the threshold voltages ck. For those k for 
which xa . ck, the comparator outputs will be dk 5 1, while the remaining comparators 
will have outputs dk 5 0. Thus the comparator outputs d can be thought of as a bar graph 
code where all the bits to one side of a certain bit are turned on. The encoder circuit takes 
Flash converter
Figure 1.40: A 
2-bit Flash 
Converter
c0
c1
c2
R
R
xa
3R/2
R/2
Vr
2Vr
3 to 2
Encoder
b0
b1
2
1
d2
2
1
d1
2
1
d0
(b)
0
1
2
3
4
5
6
7
8
21
0
1
2
k
u(k)
Figure 1.39: (Continued) (b) during Successive Approximation Steps
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 1.6 DAC and ADC Circuits     47
Table 1.4: Inputs and 
Outputs of Encoder 
Circuit when N 5 2 
input Range 
 d 5 d2d1d0 
 b 5 b1b0
21 # xayVr , 2.75 
 000 
 00 
2.75 # xayVr , 2.25 
 001 
 01 
2.25 # xayVr , .25 
 011 
 10 
.25 # xayVr # 1 
 111 
 11 
The DSP Companion that accompanies this text is discussed in Section 1.7. It con-
tains the following functions for performing analog-to-digital and digital-to-analog 
conversions.
% F_ADC: Perform N-bit analog-to-digital conversion
% F_DAC: Perform N-bit digital to analog conversion
%
% Usage:
% 
[b,d,y] = f_adc (x,N,Vr);
% 
y 
= f_dac (c,N,Vr);
% Pre:
% 
x = analog input
% 
N = number of bits
% 
Vr = reference voltage (-Vr <= x < Vr)
% 
c = string containing N-bit binary input
% Post:
% 
b = 1 by N vector containing binary output
% 
d = decimal output (offset binary)
% 
y = quantized analog output
DSP Companion
DSP Companion
the 2N 2 1 comparator outputs d and converts them to an N-bit binary output b. It does 
so by setting the decimal equivalent of b equal to i where i is the largest subscript such 
that di 5 1. For example, for the 2-bit converter in Figure 1.40, the four possible inputs 
and outputs are summarized in Table 1.4.
The beauty of the flash converter is that the entire conversion can be done in a single 
clock pulse. More specifically, the conversion time is limited only by the settling time of the 
comparator circuits and the propagation delay of the encoder circuit. Unfortunately, the 
extremely fast conversion time is achieved at a price. The converter shown in Figure 1.40  
has a precision of only 2 bits. In general, for an N-bit converter there will be a total 
of 2N 2 1 comparator circuits required. Furthermore, the encoder circuit will require 
2N 2 1 inputs. Consequently, as N increases, the flash converter becomes very hardware 
intensive. As a result, practical high-speed flash converters are typically lower precision  
(6 to 8 bits) in comparison with the medium-speed successive approximation converters 
(8 to 16 bits). There are a number of other types of ADCs as well including the slower, 
but very high precision, sigma delta converters (see Chapter 8) and dual integrating con-
verters (Grover and Deller, 1999).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

48    Chapter 1  Signal Processing
DSP Companion 
Software that accompanies this text is available on the publisher’s companion web site. It 
features a menu-based graphical user interface (GUI) program called the DSP Companion.  
The DSP Companion runs under MATLAB and contains extensive supplementary 
course material that can be used both inside the classroom by the instructor and outside 
the classroom by the student. The DSP Companion provides direct access to most of the 
textbook material as well as additional features that allow for class demonstrations and 
interactive student exploration of analysis and design concepts.
1.7.1 installation
The DSP Companion is installed using MATLAB itself (Marwan, 2003). When the zip 
file is downloaded from the companion web site and unzipped, it places a several startup 
files in the default folder at c:\dspczip. Next, MATLAB must be run, and within MAT-
LAB, the Default Folder must be set to the unzip folder by either browsing to it or enter-
ing a cd (change directory) command. The DSP Companion is then installed by entering 
the setup command.
>> cd c:\dspczip
>> setup 
All of the course software can be conveniently accessed through the driver program, 
g_dsp. The DSP Companion driver program is launched by entering the following com-
mand from the MATLAB command prompt:
>> g_dsp 
The startup screen for g_dsp is shown in Figure 1.41.
1.7.2 Menu options
The menu options of the DSP Companion are listed in Table 1.5. Most of the options on 
the menu toolbar produce submenus of selections. The Settings option allows the user 
to configure the DSP Companion by selecting operating modes and default folders for 
exporting, importing, and printing results. The GUI Modules option is used to run the 
chapter graphical user interface modules described in more detail below. In the Examples 
option, the code for all of the MATLAB examples appearing in the text can be viewed 
and executed. The Figures and the Tables options are used to display pdf files of all of 
the figures and tables that appear in the text. Similarly, the Definitions option displays 
definitions, propositions, and algorithms from the text. The next two menu options are 
only available with the Instructor version of DSP Companion. The Presentations option 
displays PowerPoint lectures, with each presentation covering a section of a chapter, 
while the Solutions option displays solutions to all of the end of chapter problems. For 
the Student version of DSP Companion, there is a Marked Problems option that display 
solutions to selected end of chapter problems. The Documentation option provides user 
help for the DSP Companion functions and the GUI modules. Finally, the Web option 
allows the user to download the latest version of the DSP Companion from the publisher 
web site.
1.7
DSP Companion
Driver program
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.7 DSP Companion     49
Figure 1.41: DSP Companion Driver Program g_dsp
DSP Companion
Student Version: 3.0
option 
Description 
Type 
Links 
Settings 
Adjust default settings 
 
 
GUI Modules 
Graphical user interface modules 
.m, .mat 
11 
Examples 
View and run MATLAB examples 
.m, .mat 
120 
Figures 
View all figures 
.pdf 
431 
Tables 
View all tables 
.pdf 
75 
Definitions 
View definitions, propositions, algorithms 
.pdf 
61
Presentations 
Display PowerPoint lectures (instructor) 
.pptx 
91 
Solutions 
Solutions to all problems (instructor) 
.pdf 
487 
Marked Problems 
Solutions to selected problems (student) 
.pdf 
54 
Documentation 
Help for DSP Companion functions 
.m 
124 
Web 
Software updates 
url 
6 
Exit 
Exit DSP Companion 
 
 
Table 1.5: DSP 
Companion Menu 
Options
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

50    Chapter 1  Signal Processing
1.7.3 gUi Modules 
The GUI modules implement the signal processing algorithms developed throughout the 
text. They feature a standard interface that is simple to learn and easy to use. Results 
exported from one module can be imported into other modules. The GUI modules allow 
the user to interactively explore the signal processing techniques covered in each chapter 
without any need for programming. When the GUI Modules option is selected, the user 
is provided with the submenu of chapter GUI modules listed in Table 1.6. Each of the 
GUI modules is described in detail near the end of the corresponding chapter.
1.7.4 Functions
Algorithms developed in the text are implemented as a library of DSP Companion 
functions, supplemented by functions that are already available as part of the standard 
MATLAB interpreter. These functions are described in detail in the chapters. Custom 
functions are developed in those instances where corresponding MATLAB functions 
are not available. In order to minimize the expense to the student and maximize acces-
sibility, it is assumed that no additional MATLAB toolboxes are available. Instead, the 
necessary functions are provided in the DSP Companion. However, for students who 
do have access to additional MATLAB toolboxes, those toolboxes can be used without 
conflict, because the DSP Companion functions all follow the f_xxx naming convention. 
User documentation for all of the DSP Companion functions can be accessed from the 
Documentation menu or from the MATLAB command prompt using the help or doc 
commands.
 
doc dspc 
% Help for all DSP Companion functions
Once the name of the function is known, a help window for that function can be obtained 
directly by using the function name as the command-line argument.
 
doc f_xxx  
% Help for DSP Companion function f_xxx
 
doc g_xxx  
% Help for DSP Companion GUI module g_xxx
The MATLAB lookfor command also can be used to locate the names of candidate 
functions by searching for key words in the first comment line of each function.
GUI modules
Naming convention
Table 1.6: DSP 
Companion GUI 
Modules  
Module  
Description  
Chapter  
g_sample 
Sample a continuous-time signal 
1 
g_reconstruct 
Reconstruct a continuous-time signal 
1 
g_systime 
Discrete-time systems in the time domain 
2 
g_correlate 
Cross-correlate a pair of signals 
2 
g_sysfreq 
Discrete-time systems in the frequency domain 
3 
g_spectra 
Signal spectral analysis 
4 
g_filter 
Filter types and characteristics 
5 
g_fir 
FIR filter design 
6 
g_iir 
IIR filter design 
7 
g_multirate 
Multirate signal processing 
8 
g_adapt 
Adaptive signal processing 
9 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.8 GUI Modules and Case Studies    51
EXAMPLE 1.11
To illustrate the type of information provided by the doc command or the Docu-
mentation menu option, consider the following example, which elicits help docu-
mentation for a DSP Companion function appearing in Chapter 4.
 
doc f_corr 
The resulting display, shown below, follows a standard format for all DSP Com-
panion functions. It includes a description of what the function does and how the 
function is called including definitions of all input and output arguments.
F_CORR: Fast cross-correlation of two discrete-time signals
Usage: 
r = f_corr (y,x,circ,norm) 
Pre: 
y = vector of length L containing first signal 
x = vector of length M <= L containing second signal
circ = optional correlation type code (default 0): 
0 = linear correlation 
1 = circular correlation 
norm = optional normalization code (default 0): 
0 = no normalization 
1 = normalized cross-correlation 
Post: 
r =  vector of length L contained selected cross-  
correlation of y with x. 
Notes: 
To compute auto-correlation use x = y. 
See also: f_corrcoef, f_conv, f_blockconv, conv, corrcoef 
The standard MATLAB convention for optional calling arguments is used for all of the 
DSP Companion functions. If the optional argument appears at the end of the list, it can 
simply be left off. If it appears in the middle of the list, it can be replaced by the empty matrix, 
[ ]. For example, the following calls of function f_corr from Example 1.11 are equivalent.
r = f_corr (x,y);  
% default values for circ, norm
r = f_corr (x,y,0);
r = f_corr (x,y,[],0); 
% [] for default value of circ
r = f_corr (x,y,0,0);
gUi Modules and Case Studies
This section focuses on sampling and reconstruction of continuous-time signals. Two 
GUI modules are presented, one for signal sampling and the other for signal reconstruc-
tion. Both allow the user to interactively explore the concepts covered in this chapter 
without any need for programming. Case study programming examples using the DSP 
Companion functions are then presented.
Optional calling 
arguments
1.8
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

52    Chapter 1  Signal Processing
g_sample: Continuous-time Signal Sampling 
The DSP Companion includes a graphical user interface module called g_sample that 
allows the user to interactively investigate the sampling process. The GUI module g_sample  
features a display screen with tiled windows as shown in Figure 1.42. The design features 
of g_sample are summarized in Table 1.7.
The Block diagram window in the upper-left corner shows a block diagram that 
includes two signal processing blocks, an anti-aliasing filter and an ADC. Below each 
block is pair of Edit boxes that allow the user to change the parameters of the signal 
processing blocks. Parameter changes are activated with the Enter key. The anti-aliasing 
filter is a lowpass Butterworth filter with user-selectable filter order, n, and cutoff fre-
quency, Fc. Selecting n 5 0 removes the filtering operation completely, so xb 5 xa. The 
user-selectable parameters for the ADC are the number of bits of precision, N, and the 
reference voltage, Vr. Input signals larger in magnitude than Vr get clipped, and when N 
is small the effects of quantization error become apparent.
The Type and View windows in the upper-right corner of the screen allow the user to 
select both the type of input signal xa and the viewing mode. The inputs include several 
common signals plus an imported user-defined input. For the latter selection, the user 
must provide the file name (without the .m extension) of an M-file function that returns 
the input vector xa evaluated at the time vector t. For example, if the following file is 
saved under the name u_sample.m, then the signal that it generates can be sampled and 
analyzed with the GUI module g_sample.
 
function xa = u_sample(t)  
% Example user function
 
xa = t .* exp(-t);  
% t can be a vector
The View window options include the color-coded time signals (xa, xb, x), and their 
magnitude spectra. Other viewing options portray the characteristics of the two signal 
processing blocks. They include the magnitude response of the anti-aliasing filter, and 
the input-output characteristic of the ADC. The Plot window along the bottom half of 
the screen shows the selected view. Below the Type and View windows is a Slider bar that 
controls the sampling frequency fs.
The Menu bar at the top of the screen includes several menu options. The Export 
option is used to save a, b, x, y, fs to a MAT file. Here fs 5 fs and a and b are the coeffi-
cient vectors of a digital version of the analog nth order Butterworth anti-aliasing filter. 
The vector x contains the digital output samples from the ADC, and y is a copy of x. All 
GUI modules export the same set of five variables. This is done for compatibility so that 
other GUI modules can import an appropriate subset of these variables. That way results 
gUi Module
item 
Variables 
Block diagram 
xa(t), xb(t), x(k) 
Edit parameters 
n, Fc, N, Vr 
Input type 
constant, damped exponential, cosine, square wave, import 
Plot view 
time signals, magnitude spectra, magnitude response, ADC 
characteristic 
Slider 
sampling frequency fs 
Menu buttons 
export, caliper, print, help, exit 
Import 
desired analog input m-file 
Export 
a, b, x, y, fs
Table 1.7: Features of 
GUI Module g_sample
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.8 GUI Modules and Case Studies    53
Figure 1.42: Display Screen for GUI Module g_sample
Select type
Select view
Slider bar
Edit parameters
0
0.5
1
1.5
2
2.5
3
3.5
4
−1.5
−1
−0.5
0
0.5
1
1.5
Time signals, square wave input: n 5 4, Fc 5 4, N 5 8, Vr 5 1, fs 5 20 
t (sec)
x(t)
 
 
xa(t)
xb(t)
x(k)
0
0.2
0.4
0.6
0.8
1
0
0.5
1
g_sample
Anti−
aliasing
filter
xa
xb
ADC
x
Slider bar
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

54    Chapter 1  Signal Processing
exported from one GUI module can be imported into other GUI modules for additional 
analysis. The only thing that can be imported into g_sample is an m-file function for com-
puting the analog input. The Caliper option allows the user to measure any point on the 
current plot by moving the mouse cross hairs to that point and clicking. The Print option 
sends the GUI window to a printer or a file. Finally, the Help option provides the user 
with some helpful suggestions on how to effectively use module g_sample.
g_reconstruct: Continuous-time Signal Reconstruction
The DSP Companion includes a graphical user interface module called g_reconstruct that 
allows the user to interactively investigate the signal reconstruction process. GUI mod-
ule g_reconstruct features a display screen with tiled windows as shown in Figure 1.43.  
The design features of g_reconstruct are summarized in Table 1.8.
The Block diagram window in the upper-left corner shows a block diagram that 
includes two signal processing blocks, a DAC, and an anti-imaging filter. Below each 
block is pair of edit boxes that allow the user to change the parameters of the signal pro-
cessing blocks. Parameter changes are activated with the Enter key. For the DAC, the user 
can select the number of bits of precision, N, and the reference voltage, Vr. When N is 
small, the effects of quantization error become apparent. The user-selectable parameters 
for the anti-imaging Butterworth filter are the filter order, n, and the cutoff frequency, Fc. 
Selecting n 5 0 removes the filtering operation completely so ya 5 yb.
The Type and View windows in the upper-right corner of the screen allow the user 
to select both the type of input signal y, and the viewing mode. The inputs include sev-
eral common signals plus an imported user-defined input where the user supplies the file 
name of an M-file function as was described for GUI module g_sample. For example,
 
function ya = u_reconstruct(t)  
% Example user function
 
ya = 1 ./ (1 + t.^2)  
% t can be a vector
The viewing options include the color-coded time signals (yb, ya, y) and their mag-
nitude spectra. Another viewing option shows the magnitude responses of the DAC and 
the anti-imaging filter. The Plot window along the bottom half of the screen shows the 
selected view.
The Menu bar at the top of the screen includes several menu options. The Export 
option is used to save a, b, x, y, fs to a MAT file. Here fs 5 fs and a and b are the coeffi-
cient vectors of a digital version of the analog nth order Butterworth anti-imaging filter. 
The vector y contains the digital input samples to the DAC and x is a copy of y. For 
compatibility reasons, all GUI modules export the same set of five variables. This way 
results exported from one module can be imported into other GUI modules for further 
processing. The only thing that can be imported into g_reconstruct is an m-file function 
gUi Module
item 
Variables 
Block diagram 
y(k), yb(t), ya(t) 
Edit parameters 
N, Vr n, Fc
Input type 
constant, damped exponential, cosine, square wave, import 
Plot view 
time signals, magnitude spectra, magnitude response
Slider 
sampling frequency fs 
Menu buttons 
export, caliper, print, help, exit 
Import 
desired analog input m-file 
Export 
a, b, x, y, fs 
Table 1.8: Features 
of GUI Module 
g_reconstruct
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.8 GUI Modules and Case Studies    55
Figure 1.43: Display Screen for GUI Module g_reconstruct
Select type
Select view
Slider bar
Edit parameters
0
0.5
1
1.5
2
2.5
3
3.5
4
−1 
−0.5
0
0.5
1
1.5
Time signals, cosine input: N 5 8, Vr 5 1, n 5 4, Fc 5 4, fs 5 20
t (sec)
y(t)
 
yb(t)
ya(t)
y
0
0.2
0.4
0.6
0.8
1
0
0.5
1
g_reconstruct
y
DAC
yb
ya 
Anti−
imaging
filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

56    Chapter 1  Signal Processing
for computing the samples of the analog output. The Caliper option allows the user to 
measure any point on the current plot by moving the mouse cross hairs to that point and 
clicking. The Print option prints the contents of the plot window to a printer or a file. 
Finally, the Help option provides the user with some helpful suggestions on how to effec-
tively use module g_reconstruct.
Anti-aliasing Filter Design
An anti-aliasing filter, or guard filter, is an analog lowpass filter that is placed in front 
of the ADC to reduce the effects of aliasing when the signal to be sampled is not band-
limited. Consider the configuration shown in Figure 1.44, which features an nth-order 
lowpass Butterworth filter followed by an ADC. Suppose the Butterworth filter has a 
cutoff frequency of Fc and the ADC is a bipolar N-bit analog-to-digital converter with 
a reference voltage of Vr. Since the Butterworth filter is not an ideal lowpass filter, we 
oversample by a factor of  . 1. That is,
 
fs 5 2Fc,   . 1 
 (1.8.1)
The design task is then as follows. Find the minimum filter order, n, that will ensure 
that the magnitude of the aliasing error is no larger than the quantization error of 
the ADC.
Given the monotonically decreasing nature of the magnitude response of the Butter-
worth filter (see Figure 1.30), the maximum aliasing error occurs at the folding frequency 
fd 5 fsy2. The largest signal that the ADC can process has magnitude Vr. Thus from 
(1.5.1) and (1.8.1), the maximum aliasing error is
Ea 5 VruHa(fsy2)u
 5 VruHa(Fc)u
 
 5
Vr
Ï1 1 2n  
 (1.8.2)
Next, if the bipolar ADC input-output characteristic is offset by qy2 as in Figure 1.37,  
then the size of the quantization error is uequ # qy2, where q is the quantization level. Thus 
from (1.6.7), the maximum quantization error is
Eq 5 q
2
 
 5 Vr
2N 
 (1.8.3)
Case Study 1.1
Oversampling
Figure 1.44:  
Preprocessing with 
an Anti-aliasing 
Filter
xa(t)
Anti-
aliasing
ﬁlter
xb(t)
ADC
x(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.8 GUI Modules and Case Studies    57
Setting E2
a 5 E2
q, we observe that the reference voltage Vr drops out. Taking reciprocals 
then yields
 
1 1 2n 5 22N 
 (1.8.4)
Finally, solving for n, the required order of the anti-aliasing filter must satisfy
 
n $
 ln (22N 2 1)
2 ln ()
 
 (1.8.5)
Of course the filter order n must be an integer. Consequently, the required order of n is as 
follows, where ceil rounds up to the nearest integer.
 
n 5
 ceil  3
 ln (4N 2 1)
2 ln () 4 
 (1.8.6)
MATLAB function case1_1 computes n using (1.8.6) for oversampling rates in the 
range 2 #  # 4 and ADC precisions in the range 10 # N # 16 bits. It can be executed 
directly from the DSP Companion driver program g_dsp.
When case1_1 is run, it produces the plot shown in Figure 1.45, which displays the 
required anti-aliasing filter order versus the ADC precision for three different values of 
oversampling. As expected, as the oversampling factor  increases, the required filter 
order decreases.
8
8.5
9
9.5
10
10.5
11
11.5
12
4
5
6
7
8
9
10
11
12
 5 .5fs/Fc 
N (bits)
n (filter order)
  5  4
  5  3
  5  2
Figure 1.45:  
Minimum Order of 
Anti-aliasing Filter 
Needed to Ensure 
That Magnitude 
of Aliasing Error 
Matches ADC 
Quantization Error 
for Different Levels 
of Oversampling  
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

58    Chapter 1  Signal Processing
Video Aliasing 
Recall from Section 1.1 that a video signal can be represented by an M 3 N image Ia(t) 
that varies with time. Here Ia(t) consists of an array of picture elements or pixels where 
the number of rows, M, and columns, N, depends on the video format used. Each pixel 
is a solid color. For example, if RGB true color format is used, then the color is specified 
with 24 bits, with 8 for red, 8 for green, and 8 for blue. Consequently each pixel can take 
on an integer color value in the large, but finite, range 0 # c , 224. This makes Ia(t) a 
quantized signal. If Ia(t) is sampled with a sampling interval of T, the resulting signal is 
discrete in both time and amplitude, in which case it is a digital signal.
 
I(k) 5 Ia(kT ),  k $ 0 
 (1.8.7)
To illustrate the sampling process and the phenomenon of aliasing, suppose the 
image Ia(t) features a rotating disk with a dark radial line to indicate orientation as shown 
previously in Figure 1.7. Suppose the disk is rotating clockwise at a rate of F0 revolu-
tions/s. If the line on the disk starts out horizontal and to the right, this corresponds to 
an initial angle of (0) 5 0. For a clockwise rotation, the angle at time t is
 
a(t) 5 22F0t 
 (1.8.8)
Next, suppose the image Ia(t) is sampled at a rate of fs frames/s. The angle of the line, as 
seen by the viewer of the kth frame, is
 
(k) 5 22F0k
fs
 
 (1.8.9)
Since the disk is rotating at a constant rate of F0 Hz, a reference point at the end of a line 
segment of radius r can be thought of as a two-dimensional signal xa(t) [ R2 with the 
following rectangular coordinates.
 
xa(t) 53
r cosfa(t)g
r sinfa(t)g4 
 (1.8.10)
It follows that the signal xa(t) is bandlimited to F0 Hz. From the sampling theorem in 
Proposition 1.1, aliasing will be avoided if fs . 2F0. Note from (1.8.9) that this corre-
sponds to the line rotating less than  radians. Thus when fs . 2F0, the sequence of 
images will show a disk that appears to be rotating clockwise which, in fact, it is. How-
ever, for fs # 2F0, aliasing will occur. For the limiting case, fs 5 2F0, the disk rotates 
exactly half a turn in each image, so it is not possible to tell which direction it is turning. 
Indeed, when fs 5 F0, the disk does not appear to be rotating at all. Since the sampling 
frequency must satisfy fs . 2F0 to avoid aliasing, it is convenient to represent the sample 
rate as a fraction of 2F0.
 
fs 5 2F0 
 (1.8.11)
Here  is the oversampling factor, where  . 1 corresponds to oversampling and  # 1 
represents undersampling. When  . 1 the disk appears to turn clockwise, and when 
 5 .5 it appears to stop. For values of  near .5, the perceived direction and speed of 
rotation vary. The following function can be used to interactively view the spinning disk 
at different sampling rates to see the effects of aliasing first hand. Like all examples in this 
text, case1_2 can be executed from the driver program g_dsp. Give it a try!
Case Study 1.2
Oversampling factor
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.9 Chapter Summary     59
Chapter Summary 
Learning outcomes
This chapter was designed to provide the student with an opportunity to achieve the 
learning outcomes summarized in Table 1.9.
Chapter 1 focused on signals, systems, and the process of sampling and reconstruc-
tion. A signal is a physical variable whose value changes with time or space. Although 
our focus is on time signals, the DSP techniques introduced also can be applied to spatial 
signals such as images.
Signals and Systems
A continuous-time signal, xa(t), is a signal whose independent variable, t, takes on values 
over a continuum. A discrete-time signal, x(k), is a signal whose independent variable is 
available only at discrete instants of time t 5 kT where T is the sampling interval. If x(k) 
is the sampled version of xa(t), then
 
x(k) 5 xa(kT ),  uku 5 0, 1, 2, Á  
 (1.9.1)
Just as the independent variable can be continuous or discrete, so can the dependent 
variable or amplitude of the signal. When a discrete-time signal is represented with finite 
precision, the signal is said to be quantized because it can only take on discrete values. A 
quantized discrete-time signal is called a digital signal. That is, a digital signal is discrete 
both in time and in amplitude. The spacing between adjacent discrete values is called the 
quantization level q. For an N-bit signal with values ranging over the interval fxm, xMg, 
the quantization level is
 
q 5 xM 2 xm
2N
 
 (1.9.2)
1.9
Continuous-time,  
discrete-time signals
Quantization level
Num.
Learning outcome  
Sec. 
1 
Understand the advantages and disadvantages of digital signal 
processing 
1.1 
2 
Know how to classify signals in terms of their independent and depen-
dent variables 
 1.2 
 
3 
Know how to model quantization error and understand the source 
 1.2 
4 
Understand what it means for a signal to be bandlimited and how to 
bandlimit a signal 
 1.5 
 
5 
Understand the significance of the sampling theorem and how to apply 
it to avoid aliasing 
 1.5 
 
6 
Know how to reconstruct a signal from its samples 
 1.6 
7 
Understand how to specify and use anti-aliasing and anti- 
imaging filters 
 1.7 
8 
Understand the operation and limitations of ADC and DAC converters 
 1.8 
9 
Know how to use the DSP Companion to access supplementary course 
materials 
 1.9 
 
10 
Be able to use the GUI modules g_sample and g_reconstruct  
to investigate continuous-time signal sampling and reconstruction 
 1.9 
 
Table 1.9: Learning 
Outcomes for  
Chapter 1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

60    Chapter 1  Signal Processing
Just as light can be decomposed into a spectrum of colors, a signal xa(t) contains energy 
that is distributed over a range of frequencies. These spectral components are obtained by 
applying the Fourier transform to produce a complex-valued frequency domain represen-
tation of the signal, Xa(f), called the signal spectrum. The magnitude of Xa(f) is called 
the magnitude spectrum, and the phase angle of Xa(f) is called the phase spectrum. The  
frequency response of a linear continuous-time system with input xa(t) and output ya(t) is 
the spectrum of the output signal divided by the spectrum of the input signal.
 
Ha(f) 5 Ya( f  )
Xa( f ) 
 (1.9.3)
The frequency response is complex-valued, with the magnitude, Aa( f ) 5 uHa( f )u, 
called the magnitude response of the system, and the phase angle, a( f ) 5 /Ha( f ), 
called the phase response of the system. A linear system that is designed to reshape the 
spectrum of the input signal in some desired way is called a frequency-selective filter.
Continuous-time Signal Sampling
The process of creating a discrete-time signal by sampling a continuous-time signal xa(t) 
can be modeled mathematically as amplitude modulation of a uniform periodic impulse 
train, T (t).
 
x⁄
a(t) 5 xa(t)T (t) 
 (1.9.4)
The effect of sampling is to scale the spectrum of xa(t) by 1yT, where T is the sampling 
interval, and to replicate the spectrum of xa(t) at integer multiples of the sampling fre-
quency fs 5 1yT. A signal xa(t) is bandlimited to B Hz if and only if the spectrum is zero 
for u f u . B. A bandlimited signal of bandwidth B can be reconstructed from its samples 
by passing it through an ideal lowpass filter with gain T and a cutoff frequency at the 
folding frequency fd 5 fsy2 as long as the sampling frequency satisfies:
 
fs . 2B 
 (1.9.5)
Consequently, a signal can be reconstructed from its samples if the signal is bandlimited 
and the sampling frequency is greater than twice the bandwidth. This fundamental result 
is known as the Shannon sampling theorem. When the signal xa(t) not bandlimited or the 
sampling rate does not exceed twice the bandwidth, the replicated spectral components 
of x⁄
a(t) overlap, and it is not possible recover xa(t) from its samples. The phenomenon of 
spectral overlap is known as aliasing.
Most signals of interest are not bandlimited. To minimize the effects of aliasing the 
input signal is preprocessed with a lowpass filter analog filter called an anti-aliasing filter 
before it is sampled with an analog-to-digital converter (ADC). A practical lowpass filter, 
such as a Butterworth filter, does not completely remove spectral components above the 
cutoff frequency Fc. However, the residual aliasing can be further reduced by oversampling  
at a rate fs 5 2Fc where  . 1 is the oversampling factor.
Continuous-time Signal Reconstruction
Once the digital input signal x(k) from the ADC is processed with a DSP algorithm, the 
resulting output y(k) is typically converted back to an analog signal using a digital-to- 
analog converter (DAC). Whereas an ADC can be modeled mathematically with an 
impulse sampler, a DAC is modeled as a zero-order hold filter with transfer function
 
H0(s) 5 1 2  exp(2Ts)
s
 
 (1.9.6)
Magnitude, phase 
spectra
Frequency response
Magnitude, phase 
response
Bandlimited signal
Folding frequency
Sampling theorem
Anti-aliasing filter
Zero-order hold
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.10 Problems     61
The transfer function of the DAC is the Laplace transform of the output divided by the 
Laplace transform of the input assuming zero initial conditions. The piecewise-constant 
output of the DAC contains high-frequency spectral components called images centered 
at integer multiples of the sampling frequency. These can be reduced by postprocessing 
with a second lowpass analog filter called an anti-imaging filter.
A circuit realization of a DAC was presented that featured an R-2R resistor ladder net-
work, an operational amplifier, and a bank of digitally-controlled analog switches. Circuit 
realizations of ADCs that were considered included the successive approximation ADC 
and the flash ADC. The successive approximation ADC is a widely used, high-precision, 
medium-speed converter that requires N clock pulses to perform an N-bit conversion. By 
contrast, the flash ADC is a very fast, hardware-intensive, medium-precision converter 
that requires 2N 2 1 comparator circuits, but performs a conversion in a single clock pulse.
DSP Companion
The final topic covered in this chapter was the DSP Companion, a menu-based GUI 
program that provides direct access to an extensive set of supplementary course materi-
als. The DSP Companion software is available on the publisher’s companion web site, 
and it is accessed through the driver program, g_dsp. The driver program includes menu 
options for changing settings, running chapter GUI modules, viewing PowerPoint lec-
tures, and running MATLAB examples. It also can be used to view figures, tables, defi-
nitions, propositions, algorithms, and solutions to selected end of chapter problems that 
appear throughout the text. The instructor version provides access to solutions to all 
of the end of chapter problems. The DSP Companion provides user documentation for 
the DSP Companion functions that implement algorithms developed in each chapter. 
There is also a menu option that allows the user to upload the latest version of the DSP  
Companion software.
Problems 
The problems are divided into Analysis and Design problems that can be solved by hand 
or with a calculator, GUI Simulation problems that are solved using the GUI modules  
g_sample and g_reconstruct, and MATLAB Computation problems that require user pro-
grams. Solutions to selected problems can be accessed with the DSP Companion driver 
program, g_dsp. Students are encouraged to use these problems, which are identified with 
a 
, as a check on their understanding of the material.
1.10.1 Analysis and Design
Section 1.2: Signals and Systems
1.1 
Suppose the input to an amplifier is xa(t) 5 sin(2F0t) and the steady-state output is
ya(t) 5 100 sin(2F0t 1 1) 2 2 sin(4F0t 1 2) 1 cos(6F0t 1 3)
(a) Is the amplifier a linear system or is it a nonlinear system?
(b) What is the gain of the amplifier?
(c) Find the average power of the output signal.
(d) What is the total harmonic distortion of the amplifier?
Anti-imaging filter
1.10
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

62    Chapter 1  Signal Processing
1.2 
Consider the following signum function that returns the sign of its argument.
sgn(t) 5
D  5
   1, t . 0
   0, t 5 0
21, t , 0
(a) Using Appendix 1, find the magnitude spectrum.
(b) Find the phase spectrum.
1.3 
Parseval’s identity states that a signal and its spectrum are related in the following way.
#
`
2`
uxa(t)u2dt 5#
`
2`
uXa( f )u2df
 
Use Parseval’s identity to compute the following integral.
J 5#
`
2`
 sinc 2(2Bt)dt
1.4 
Consider the causal exponential signal
xa(t) 5  exp(2ct)a(t)
(a) Using Appendix 1, find the magnitude spectrum.
(b) Find the phase spectrum.
(c) Sketch the magnitude and phase spectra when c 5 1.
1.5 
If an analog signal xa(t) is square integrable, then the energy that the signal contains 
within the frequency band fF0, F1g where F1 $ F0 $ 0 can be computed as follows.
E(F0, F1) 5 2#
F1
F0
uXa( f  )u2df
 
Consider the following two-sided exponential signal with c . 0.
xa(t) 5 exp(2cut u)
(a) Find the total energy, E(0, `).
(b) Find the percentage of the total energy that lies in the frequency range  
f0, 2g Hz.
1.6 
Let xa(t) be a periodic signal with period T0. The average power of xa(t) can be 
defined as follows.
Px 5 1
T0#
T0
0
uxa(t)u2dt
 
Find the average power of the following periodic continuous-time signals.
(a) xa(t) 5 cos(2F0t)
(b) xa(t) 5 c
(c) A periodic train of pulses of amplitude a, duration T, and period T0
1.7 
Consider the following discrete-time signal where the samples are represented using 
N bits.
x(k) 5 exp(2ckT)(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.10 Problems     63
(a) How many bits are needed to ensure that the quantization level is less than .001?
(b) Suppose N 5 8 bits. What is the average power of the quantization noise?
1.8 
Show that the spectrum of a causal signal xa(t) can be obtained from the Laplace 
transform Xa(s) by replacing s by j2f . Is this also true for noncausal signals?
Section 1.3: Sampling of Continuous-time Signals
1.9 
Consider the following periodic signal.
xa(t) 5 1 1  cos(10t)
(a) Compute the magnitude spectrum of xa(t).
(b) Suppose xa(t) is sampled with a sampling frequency of fs 5 8 Hz. Sketch the 
magnitude spectra of xa(t) and the sampled signal, x⁄
a(t).
(c) Does aliasing occur when xa(t) is sampled at the rate fs 5 8 Hz? What is the 
folding frequency in this case?
(d) Find a range of values for the sampling interval T that ensures that aliasing 
does not occur.
(e) Assuming fs 5 8 Hz, find an alternative lower-frequency signal, xb(t), that has 
the same set of samples as xa(t). 
1.10 Consider the following bandlimited signal.
xa(t) 5 sin(4t)f1 1  cos2(2t)g
(a) Using the trigonometric identities in Appendix 2, find the maximum frequency 
present in xa(t).
(b) For what range of values for the sampling interval T can this signal be recon-
structed from its samples?
Section 1.4: Reconstruction of Continuous-time Signals
1.11 It is not uncommon for students to casually restate the sampling theorem in Prop-
osition 1.1 in the following way. “A signal must be sampled at twice the highest fre-
quency present to avoid aliasing.” Interestingly enough, this informal formulation 
is not quite correct. To verify this, consider the following simple signal.
xa(t) 5  sin(2t)
(a) Find the magnitude spectrum of xa(t), and verify that the highest frequency 
present is F0 5 1 Hz.
(b) Suppose xa(t) is sampled at the rate fs 5 2 Hz. Sketch the magnitude spectra 
of xa(t) and the sampled signal, x⁄
a(t). Do the replicated spectra overlap?
(c) Compute the samples x(k) 5 xa(kT) using the sampling rate fs 5 2 Hz. Is it 
possible to reconstruct xa(t) from x(k) using the reconstruction formula in 
Proposition 1.2 in this instance?
(d) Restate the sampling theorem in terms of the highest frequency present, but 
this time correctly.
1.12 Why is it not possible to physically construct an ideal lowpass filter? Use the impulse 
response, ha(t), to explain your answer.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

64    Chapter 1  Signal Processing
1.13 There are special circumstances where it is possible to reconstruct a signal from its sam-
ples even when the sampling rate is less than twice the bandwidth. To see this, consider 
a signal xa(t) whose spectrum Xa( f ) has a hole in it as shown in Figure 1.46.
(a) What is the bandwidth of the signal xa(t) whose spectrum is shown in Figure 1.46?  
The pulses are of radius 100 Hz.
(b) Suppose the sampling rate is fs 5 750 Hz. Sketch the spectrum of the sampled 
signal x⁄
a(t).
(c) Show that xa(t) can be reconstructed from x⁄
a(t) by finding an idealized recon-
struction filter with input x⁄
a(t) and output xa(t). Sketch the magnitude response 
of the reconstruction filter.
(d) For what range of sampling frequencies below 2fs can the signal be recon-
structed from the samples using the type of reconstruction filter from part (c)?
Section 1.5: Prefilters and Postfilters
1.14 Consider the problem of using an anti-aliasing filter as shown in Figure 1.47. Sup-
pose the anti-aliasing filter is a lowpass Butterworth filter of order n 5 4 with cut-
off frequency Fc 5 2 kHz.
(a) Find a lower bound fL on the sampling frequency that ensures that the aliasing 
error is reduced by a factor of at least .005.
(b) The lower bound fL represents oversampling by what factor?
1.15 Show that the transfer function of a linear continuous-time system is the Laplace 
transform of the impulse response.
21000
2600
2200
0
200
600
1000
0
0.5
1
1.5
f (Hz)
|xa (f)|
Figure 1.46: A 
Signal Whose 
Spectrum Has a 
Hole in It
xa(t)
Anti-
aliasing
ﬁlter
xb(t)
ADC
x(k)
Figure 1.47:  
Preprocessing with 
an Anti-aliasing 
Filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.10 Problems     65
Section 1.6: DAC and ADC Circuits
1.16 A bipolar DAC can be constructed from a unipolar DAC by inserting an operational 
amplifier at the output as shown in Figure 1.48. Note that the unipolar N-bit DAC 
uses a reference voltage of 2VR, rather than 2Vr as in Figure 1.36. This means that the 
unipolar DAC output is 22ya where ya is given in (1.6.4). Analysis of the operational 
amplifier section of the circuit reveals that the bipolar DAC output is then
za 5 2ya 2 Vr
(a) Find the range of values for za.
(b) Suppose the binary input is b 5 bN21bN22 Á b0. For what value of b is za 5 0?
(c) What is the quantization level of this bipolar DAC?
1.17 Suppose a bipolar ADC is used with a precision of N 5 12 bits and a reference 
voltage of Vr 5 10 volts. 
(a) What is the quantization level q?
(b) What is the maximum value of the magnitude of the quantization noise, assuming 
the ADC input-output characteristics are offset by qy2 as in Figure 1.37?
(c) What is the average power of the quantization noise?
1.18 Suppose an 8-bit bipolar successive approximation ADC has reference voltage 
Vr 5 10 volts.
(a) If the analog input is xa 5 23.941 volts, find the successive approximations by 
filling in the entries in Table 1.10.
(b) If the clock rate is fclock 5 200 kHz, what is the sampling rate of this ADC?
(c) Find the quantization level of this ADC.
(d) Find the average power of the quantization noise.
b
Unipolar
DAC
2Vr
22ya
R
R
Vr
R
+
−
GND
za
2
1
Figure 1.48: A 
Bipolar N-bit DAC
Table 1.10: Successive 
Approximations
k
b72k 
u(k) 
ya
0
 
 
   
1 
 
 
 
2 
 
 
 
3 
 
 
 
4 
 
 
 
5 
 
 
 
6 
 
 
 
7 
 
 
 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

66    Chapter 1  Signal Processing
1.19 An alternative to the R-2R ladder DAC is the weighted-resistor DAC shown in  
Figure 1.49 for the case N 5 4. Here the switch controlled by bit bk is open when bk 5 0 
and closed when bk 5 1. Recall that the decimal equivalent of the binary input b is as 
follows.
x 5 o
N21
k50
bk2k
(a) Show that the current through the kth branch of an N-bit weighted-resistor 
DAC is
Ik 5 2Vrbk
2N 2 kR, 0 # k , N
(b) Show that the DAC output voltage is
ya 51
Vr
2N2x
(c) Find the range of output values for this DAC.
(d) Is this DAC unipolar, or is it bipolar?
(e) Find the quantization level of this DAC.
1.10.2 gUi Simulation
Section 1.3: Sampling of Continuous-time Signals
1.20 Use GUI module g_sample to plot the time signals and magnitude spectra of the 
square wave using fs 5 10 Hz. On the magnitude spectra plot, use the Caliper 
option to display the amplitude and frequency of the third harmonic of x(k). Are 
there even harmonics present in the square wave?
1.21 Use GUI module g_sample to Import the signal in the file u_sample and plot its magni-
tude spectrum. Do the following two cases. For which ones is there noticeable aliasing?
(a) fs 5 2 Hz
(b) fs 5 10 Hz
Figure 1.49: A 
4-bit Weighted-
resistor DAC
GND
ya
1
2
R
I
V
2Vr
b3
I3
2R
b2
I2
4R
b1
I1
8R
b0
I0
16R
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.10 Problems     67
1.22 Consider the following exponentially damped sine wave with c 5 1 and F0 5 1.
xa(t) 5 exp(2ct) sin(2F0t)a(t)
(a) Write a MATLAB function called u_sample2 that returns the value xa(t).
(b) Use the User-Defined option in GUI module g_sample to sample this signal at 
fs 5 12 Hz. Plot the time signals.
(c) Adjust the sampling rate to fs 5 6 Hz and set the cutoff frequency to Fc 5 2 Hz. 
Plot the magnitude spectra.
Section 1.4: Reconstruction of Continuous-time Signals
1.23 Use GUI module g_reconstruct to Import the signal in the file u_reconstruct1. 
Adjust fs to 12 Hz and set Vr 5 4.
(a) Plot the time signals, and use the Caliper option to identify the amplitude and 
time of the peak output.
(b) Plot the magnitude spectra.
1.24 Consider the exponentially damped sine wave in problem 1.22.
(a) Write a MATLAB function that returns the value xa(t).
(b) Use the User-Defined option in GUI module g_reconstruct to sample this sig-
nal at fs 5 8 Hz. Plot the time signals.
(c) Adjust the sampling rate to fs 5 4 Hz and set Fc 5 2 Hz. Plot the magnitude 
spectra.
Section 1.5: Prefilters and Postfilters
1.25 Use GUI module g_sample to plot the magnitude responses of the following 
anti-aliasing filters. What is the oversampling factor, , in each case?
(a) n 5 2, Fc 5 1, fs 5 2
(b) n 5 6, Fc 5 2, fs 5 12
1.26 Use GUI module g_reconstruct to plot the magnitude responses of the following 
anti-imaging filters. What is the oversampling factor in each case?
(a) n 5 1, Fc 5 2, fs 5 5
(b) n 5 8, Fc 5 4, fs 5 16
1.27 Use the GUI module g_reconstruct to plot the magnitude responses of a 12-bit 
DAC with reference voltage Vr 5 10 volts and a 6th order Butterworth anti-imaging  
filter with cutoff frequency Fc 5 2 Hz. Use oversampling by a factor of two.
Section 1.6: DAC and ADC Circuits
1.28 Use GUI module g_sample with the damped exponential input to plot the time 
signals using the following ADCs. For what cases does the ADC output saturate? 
Write down the quantization level on each time plot.
(a) N 5 4, Vr 5 1
(b) N 5 8, Vr 5 .5
(c) N 5 8, Vr 5 1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

68    Chapter 1  Signal Processing
1.29 Use GUI module g_reconstruct with the damped exponential input to plot the time 
signals using the following DACs. What is the quantization level in each case?
(a) N 5 4, Vr 5 .5
(b) N 5 12, Vr 5 2
1.10.3 MATLAB Computation
Section 1.4: Reconstruction of Continuous-time Signals
1.30 Write a MATLAB function called u_sinc that returns the value of the normalized 
sinc function
 sinc (x) 5
 sin(x)
x
 
Note that, by L’Hospital’s rule,  sinc (0) 5 1. Make sure your function works prop-
erly when x 5 0. Plot  sinc (2t) for 21 # t # 1 using N 5 401 samples.
1.31 The purpose of this problem is to numerically verify the signal reconstruction for-
mula in Proposition 1.2. Consider the following bandlimited periodic signal, which 
can be thought of as a truncated Fourier series.
xa(t) 5 1 2 2 sin(t) 1  cos(2t) 1 3 cos(3t)
 
Write a MATLAB program that uses the function u_sinc from Problem 1.30 to 
approximately reconstruct xa(t) as follows.
xp(t) 5 o
p
k52p
xa(kT ) sinc f fs(t 2 kT )g
 
Use a sampling rate of fs 5 6 Hz. Plot xa(t) and xp(t) on the same graph using 101 
points equally spaced over the interval f22, 2g. Using f_prompt, prompt for the 
number p and do the following three cases.
(a) p 5 5
(b) p 5 10
(c) p 5 20
1.32 Consider the zero-order hold and delayed first-order hold filters used to recon-
struct continuous-time signals from their samples. Since both filters are causal, the 
frequency responses can be obtained from the transfer functions by replacing s by 
j2f  in (1.4.9) and (1.4.15), respectively. Write a MATLAB script that computes 
and plots the magnitude response of each filter. Use fs 5 1 Hz and plot both mag-
nitude responses over f0, 4g Hz on the same graph using a legend.
Section 1.5: Prefilters and Postfilters
1.33 The Butterworth filter is optimal in the sense that, for a given filter order, the mag-
nitude response is as flat as possible in the passband. If ripples are allowed in the 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.10 Problems     69
passband, then an analog filter with a sharper cutoff can be achieved. Consider the 
following Chebyshev-I lowpass filter from Chapter 7.
Ha(s) 5
1263.7
s5 1 6.1s4 1 67.8s3 1 251.5s2 1 934.3s 1 1263.7
 
Write a MATLAB program that uses the DSP Companion function f_  freqs to 
compute the magnitude response of this filter. Plot it over the range f0, 3g Hz. This 
filter is optimal in the sense that the passband ripples are all of the same size.
1.34 Consider the following Chebyshev-II lowpass filter from Chapter 7.
Ha(s) 5
3s4 1 499s2 1 15747
s5 1 20s4 1 203s3 1 1341s2 1 5150s 1 15747
 
Write a MATLAB program that uses the DSP Companion function f_  freqs to 
compute the magnitude response of this filter. Plot it over the range f0, 3g Hz. This 
filter is optimal in the sense that the stopband ripples are all of the same size.
1.35 Consider the following elliptic lowpass filter from Chapter 7.
Ha(s) 5
2.0484s2 1 171.6597
s3 1 6.2717s2 1 50.0487s 1 171.6597
 
Write a MATLAB program that uses the DSP Companion function f_  freqs to 
compute the magnitude response of this filter. Plot it over the range [0, 3] Hz. This 
filter has the sharpest cutoff and is optimal in the sense that the passband ripples 
and the stopband ripples are all of the same size.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

70
Discrete-time Systems  
in the Time Domain
C H A P T E R  2
Motivation
A discrete-time system is an entity that processes a discrete-time 
input signal, x(k), to produce a discrete-time output signal, y(k). 
Recall from Chapter 1 that if the signals x(k) and y(k) are discrete 
in amplitude, as well as in time, then they are digital signals, and 
the associated system is a digital signal processor. In this chapter 
we focus our attention on analyzing the input-output behavior of 
linear time-invariant (LTI) discrete-time systems. This material 
lays a mathematical foundation for subsequent chapters where 
we design digital filters and develop digital signal processing 
(DSP) algorithms. A finite-dimensional LTI discrete-time system 
can be represented in the time domain by a constant-coefficient 
difference equation.
y(k) 1 o
n
i51
aiy(k 2 i) 5 o
m
i50
bix(k 2 i)
There are a number of important DSP applications that 
require the processing of pairs of signals. For example, if h(k) 
is the response of a linear discrete-time system to a unit impulse 
input, the response to an arbitrary input x(k) when the initial 
condition is zero is the convolution of the impulse response with 
the input.
y(k) 5 o
k
i50
h(i)x(k 2 i), k $ 0
There is a second operation involving a pair of signals that 
closely resembles convolution. To measure the degree to which 
2.1
Difference equation
Convolution
 CHAPTER ToPiCS
2.1 
Motivation 
2.2 
Discrete-time Signals 
2.3 
Discrete-time Systems 
2.4 
Difference Equations 
2.5 
Block Diagrams 
2.6 
The Impulse Response 
2.7 
Convolution 
2.8 
Correlation 
2.9 
Stability in the Time 
Domain 
2.10 GUI Modules and Case 
Studies 
2.11 Chapter Summary 
2.12 Problems
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.1 Motivation    71
an L-point signal h(k) is similar to an M-point signal x(k) where M # L, one can com-
pute the cross-correlation of h with x.
rhx(k) 5 1
Lo
L21
i50
h(i)x(i 2 k), 0 #  k , L
Comparing the cross-correlation with the convolution, we see that, apart from scaling, 
the essential difference is in the sign of the independent variable of the second signal. 
Cross-correlation has a number of important applications. For example, in radar or 
sonar processing cross-correlation can be used to determine if a received signal contains 
an echo of the transmitted signal.
We begin this chapter by examining a number of practical problems that can be 
modeled as discrete-time systems using difference equations. Techniques for solving dif-
ference equations in the time domain are then presented. The complete solution of a 
linear difference equation can be decomposed into the sum of two parts. The zero-input 
response is the part of the solution that arises from nonzero initial conditions, and the 
zero-state response is the part of the solution that is associated with a nonzero input. 
The interconnection of simple discrete-time systems to produce more complex systems 
is illustrated using block diagrams. The zero-state response of an LTI system is com-
pletely characterized by the response to a single input, the unit impulse. Discrete-time 
systems are classified into two basic groups, those with a finite duration impulse response 
(FIR) and those with an infinite duration impulse response (IIR). Linear and circular 
convolution and correlation are then considered. The relationship between them is inves-
tigated, and implementations based on matrix multiplication are presented. One of the 
most important qualitative characteristics of discrete-time systems is stability, because 
almost all practical systems are stable. The stability of a LTI discrete-time system in the 
time domain can be determined directly from its impulse response. Finally, GUI modules 
called g_systime and g_correlate are presented that allow the user to interactively explore 
discrete-time systems and perform convolution and correlation operations without any 
need for programming. The chapter concludes with some case study examples and a sum-
mary of discrete-time systems in the time domain.
2.1.1 Home Mortgage
As a simple illustration of a discrete-time system that affects many families, consider the 
problem of purchasing a home with a mortgage loan. Suppose a fixed-rate mortgage is 
taken out at an annual interest rate of r compounded monthly, where r is expressed as a 
fraction rather than a percent. Let y(k) denote the balance owed to the lending agency 
at the end of month k, and let x(k) denote the monthly mortgage payment. The balance 
owed at the end of month k is the balance owed at the end of month k 2 1, plus the 
monthly interest on that balance, minus the monthly payment.
 
y(k) 5 y(k 2 1) 11
r
122
 y(k 2 1) 2 x(k) 
 (2.1.1)
Here the initial condition, y(21) 5 y0, is the size of the mortgage. Given the difference- 
equation representation in (2.1.1), there are a number of practical questions that a poten-
tial home buyer might want to ask. For example, if the size of the mortgage is D dollars 
and the duration is N months, what is the required monthly mortgage payment? Another 
question is how many months does it take before the majority of the monthly payment 
goes toward reducing principal rather than paying interest? These questions can be easily 
Cross-correlation
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

72    Chapter 2  Discrete-time Systems in the Time Domain
answered once we develop the tools for solving this system. As we shall see, the mortgage 
loan system is an example of an unstable discrete-time system.
2.1.2 Range Measurement with Radar
An application where correlation arises is in the measurement of range to a target using 
radar. Consider the radar installation shown in Figure 2.1.
Here the radar antenna transmits an electromagnetic wave x(k) into space. When a 
target is illuminated by the radar, some of the signal energy is reflected back and returns 
to the radar receiver. The received signal y(k) can be modeled using the following differ-
ence equation.
 
y(k) 5 (k 2 d) 1 v(k) 
 (2.1.2)
The first term in (2.1.2) represents the echo of the transmitted signal. Typically the echo 
is very faint (0 ,  V 1) because only a small fraction of the original signal is reflected 
back with most of the signal energy dissipated through dispersion into space. The delay 
of d samples accounts for the propagation time required for the signal to travel from the 
transmitter to the target and back again. Thus d is proportional to the time of flight. The 
second term in (2.1.2) accounts for random atmospheric measurement noise, v(k), that is 
picked up and amplified by the receiver. In effect, (2.1.2) is a model of the communication 
channel from the transmitter to the receiver when an echo is received.
The objective in processing the received signal y(k) is to determine whether or not 
an echo is present. If an echo is detected, then the distance to the target can be obtained 
from the delay d. If T is the sampling interval, then the time of flight in seconds is  5 dT. 
Suppose c denotes the signal propagation speed. For radar this is the speed of light. Then 
the distance to the target is computed as follows, where the factor of two arises because 
the signal must make a round trip.
 
r 5 cdT
2  
 (2.1.3)
To detect if an echo is present in the noise-corrupted received signal, y(k), we com-
pute the normalized cross-correlation of y(k) with x(k). If the measurement noise v(k) is 
not present, the normalized cross-correlation will have a spike of unit amplitude at i 5 d. 
When v(k) Þ 0, the height of the spike will be reduced.
As an illustration, suppose the transmitted signal x(k) consists of M 5 512 points 
of white noise uniformly distributed over f21, 1g. Other broadband signals, for example 
Figure 2.1:  
Radar Illuminating 
a Target 
y(k)
x(k)
Target
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.1 Motivation    73
a multifrequency chirp, might also be used. Next let the received signal y(k) consist of 
N 5 2048 points. Suppose the attenuation factor is  5 .05. Let v(k) consist of zero-
mean Gaussian white noise with a standard deviation of  5 .05. A plot of the received 
signal is shown in Figure 2.2.
Note, from a direct inspection of Figure 2.2, that it is not at all apparent whether y(k) 
contains a delayed and attenuated echo of x(k), much less where it is located. However, 
if we compute the normalized cross-correlation of y(k) with x(k), then the presence and 
location of an echo are evident, as can be seen in Figure 2.3. Using the MATLAB max 
function, we find that the time of flight in this case is d 5 939 samples. The range to the 
target then can be found using (2.1.3).
Figure 2.2: Signal 
Received at Radar 
Station 
k
0
400
800
1200
1600
2000
 y(k)
20.5
20.4
20.3
20.2
20.1
0
0.1
0.2
0.3
0.4
0.5
k
0
400
800
1200
1600
2000
yx(k)
20.1
20.05
0
0.05
0.1
0.15
0.2
0.25
0.3
Figure 2.3:  
Normalized Cross- 
correlation of 
Transmitted Signal 
with Received 
Signal 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

74    Chapter 2  Discrete-time Systems in the Time Domain
Discrete-time Signals 
The process of sampling a continuous-time signal xa(t) to produce a discrete-time sig-
nal x(k) 5 xa(kT) was introduced in Chapter 1. This is the most common way for dis-
crete-time signals to arise in practice because physical variables exist in continuous time.  
Discrete-time signals can be categorized in a number of useful ways.
2.2.1 Signal Classification 
Finite and infinite Signals 
One of the most fundamental ways to characterize a discrete-time signal is to count the 
number of samples. A signal x(k) is a finite duration signal, or a finite signal, if and only 
if x(k) is defined over a finite interval N1 # k # N2 where N2 $ N1. Alternatively, one 
can think of x(k) as being defined for all integers k but equal to zero outside the interval 
fN1, N2g.
 
x(k) 5 0, k Ó fN1, N2g 
 (2.2.1)
Thus a finite signal exists, or can be nonzero, for a finite number of samples N. Other-
wise, it is an infinite duration signal or an infinite signal. When practical computational 
algorithms, such as the fast Fourier transform (FFT), are implemented in software, they 
are applied to finite signals.
Finite signals are often used to approximate infinite signals. For example, suppose a 
signal x(k) is defined for 0 # k , ` but approaches zero asymptotically as k approaches 
infinity.
 
ux(k)u S 0  as  k S ` 
 (2.2.2)
This signal can be approximated by a finite signal over the range f2N, Ng by using a suf-
ficiently large N and truncating or removing the “tail’’ of the infinite signal.
2.2
Finite signal
Infinite signal
There are two built-in MATLAB functions for creating white noise signals. The 
following code segment creates an N-point white noise signal v that is uniformly 
distributed between a and b.
 
N = 100;  
% number of points
 
a = 21;  
% lower limit
 
b = 1;  
% upper limit
 
v = a 1 (b2a)*rand(N,1)  
% uniform white noise in [a,b]
A similar technique can be used to generate normal or Gaussian white noise by 
replacing rand with randn. In this case a will be the mean, and b 2 a will be the stan-
dard deviation. White noise is discussed in detail in Chapter 4.
MATLAB Functions
MATLAB Functions
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.2 Discrete-time Signals     75
Causal and Noncausal Signals
When working with discrete-time systems, it is often convenient to consider input signals 
whose samples are zero for negative time. The contribution from the part of the input 
signal that would have come from negative time can be accounted for using initial con-
ditions. A signal x(k) whose samples do not exist or are zero for k , 0 is referred to as a 
causal signal.
 
x(k) 5 0, k , 0 
 (2.2.3)
Thus a causal signal x(k) is a signal that is zero for negative time. Otherwise, the signal 
is a noncausal signal. Note that a signal that is defined over an interval f0, N21g is both 
finite and causal.
Periodic and Aperiodic Signals
Among signals x(k) that are defined for all k, there is a subset of signals that are periodic. 
A signal x(k) is a periodic signal if and only if there exists an integer N . 0 called the 
period such that
 
x(k 1 N) ; x(k) 
 (2.2.4)
Here the notation ;, which is pronounced identically equal, indicates that the two sides 
are equal for all values of the independent variable k. A casual observer might think that 
if a continuous-time signal xa(t) is periodic, then the discrete-time signal generated by 
sampling xa(t) must also be periodic. This is not necessarily the case, as can be seen from 
the following example.
Causal signal
Noncausal signal
Periodic signal
Identically equal
Periodic Signals
EXAMPLE 2.1
Suppose a discrete-time signal x(k) is constructed by sampling a continuous-time 
signal xa(t) using a sampling rate of fs 5 1yT Hz. For example, consider the fol-
lowing periodic continuous-time signal and its samples.
xa(t) 5 cos(2F0t)
x(k) 5 cos(2F0kT )
Here xa(t) is periodic with a period of T0 5 1yF0 sec. However, for x(k) to be peri-
odic there must be an integer N . 0 such that x(k 1 N) ; x(k). This will occur if 
there is an integer number of samples per period of xa(t). That is,
fs 5 NF0, N $ 3
Recall from the sampling theorem that to avoid aliasing, it is necessary that the band-
limited signal xa(t) be sampled at a rate of fs . 2F0. Thus the minimum sampling 
rate that will yield a periodic alias-free discrete-time signal is fs 5 3F0. Here the dis-
crete-time period in seconds is  5 NT, and the continuous-time period is T0 5 1yF0.
Interestingly enough, it is possible to produce a periodic signal x(k) whose 
period is longer than the period of xa(t). Consider the case when the sampling rate 
fs is related to the fundamental frequency F0 by a rational number LyM where L 
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

76    Chapter 2  Discrete-time Systems in the Time Domain
Bounded and Unbounded Signals
Later in the chapter we consider the important concept of stability. Although several 
different types of stability can be defined, one of the most useful ones is bounded-input 
bounded-output (BIBO) stability. A signal x(k) is a bounded signal if and only if there 
exists a constant Bx . 0 called a bound such that for all k
 
ux(k)u # Bx 
 (2.2.5)
Otherwise x(k) is an unbounded signal. Thus, the graph of a bounded signal lies within 
a horizontal strip of radius Bx about the time axis, where Bx is the bound as shown in 
Figure 2.4. Unbounded signals grow arbitrarily large. For example, the periodic signal 
x(k) 5  sin (2F0kT) is bounded with bound Bx 5 1, but the undamped exponential sig-
nal x(k) 5 2k is unbounded.
For signals x(k) that are defined for all k, it is possible to think of these signals as 
vectors in a vector space of sequences. One way to measure the length or norm of such a 
vector is using the /1 norm.
 
uuxuu1 5
D  o
`
k52`
ux(k)u 
 (2.2.6)
If uuxuu1 , `, then we says that the signal x(k) is absolutely summable. Another way to 
measure the length of x is by using the /2 norm.
 
uuxuu2 5
D  3 o
`
k52`
ux(k)u24
1y2
 
 (2.2.7)
Bounded signal
Unbounded signal
Norm
Absolutely summable
and M are positive integers.
fs 5 LF0
M , L . 2M
In this case Mfs 5 LF0 or LT 5 MT0. Thus the time associated with L samples 
corresponds to M periods of xa(t). In this instance x(k) is periodic with period L. 
Of course, it is possible for the ratio fsyF0 to be irrational. When fs and F0 are 
related by an irrational number such as fs 5 Ï5F0, then x(k) is not periodic.
k
x(k)
Bx
2Bx
Figure 2.4: A 
Bounded Signal 
x(k) 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.2 Discrete-time Signals     77
If uuxuu2 , `, then x(k) is said to be square summable. Note that (uau 1 ubu)2 $ uau2 1 ubu2 
due to the presences of cross terms. As a consequence, the absolutely summable signals 
include the square summable signals as a special case.
 
uuxuu1 , ` 1  uuxuu2 , `  
 (2.2.8)
Energy and Power Signals
Closely related to square summable signals are the concepts of energy and power. Suppose 
x(k) represents the voltage across a one ohm resistor at time k as shown in Figure 2.5. Then 
p(k) 5 x2(k) can be interpreted as the instantaneous power dissipated by the resistor at time k. 
For continuous-time signals, the integral of power over time is the total energy. For discrete-
time signals, we define the energy of a signal x(k) as follows.
 
Ex 5
D  o
`
k52`
 ux(k)u2  
 (2.2.9)
Comparing (2.2.9) with (2.2.7), we see that Ex 5 uuxuu2
2. Therefore the energy Ex is finite if 
and only if x(k) is square summable. A signal for which Ex , ` is called an energy signal. 
It is clear that not all signals are energy signals. For example, the constant signal x(k) 5 c 
for c Þ 0 has infinite energy and therefore is not an energy signal. For signals that do not 
go to zero fast enough to be square summable, it is useful to compute the average of the 
instantaneous power, p(x) 5 ux(k)u2. The average power of a signal x(k) is denoted Px and 
defined as follows.
 
Px 5
D  lim
NS` 
1
2N 1 1 o
N
k52N
 ux(k)u2  
 (2.2.10)
A signal for which Px is nonzero and finite is called a power signal. If x(k) is not an energy 
signal because its energy is infinite, it may qualify as a power signal. For example, the 
constant nonzero signal x(k) 5 c has infinite energy but average power Px 5 ucu2.
The computation of average power simplifies when x(k) is a causal signal, a periodic 
signal, or a finite signal. For a causal signal, the average value of the power is
 
Px 5 lim
N S ` 
1
N 1 1 o
N
k50
ux(k)u2 
 (2.2.11)
Similarly, if x(k) ; x(k 1 N) is periodic with a period of N, then to compute the average 
power it is only necessary to use one period.
 
Px 5 1
No
N21
k50
ux(k)u2 
 (2.2.12)
Square summable
Energy
Energy signal
Average power
Power signal
x(k)
R
6
Figure 2.5:  
Instantaneous 
Signal Power, 
p(k) 5 x2(k)yR 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

78    Chapter 2  Discrete-time Systems in the Time Domain
Finally, if x(k) is a causal finite signal with N nonzero samples, (2.2.12) can be used to 
compute its average power. Note that every finite signal x(k) has a periodic extension, 
xp(k), that is defined for all k where x(k) for 0 # k , N represents one period. The expres-
sion in (2.2.12) then represents the average power of one period of xp(k).
There is a simple relationship between energy and power. For the finite causal signal 
x 5 fx(0), . . . , x(N 2 1)gT, the energy and the average power are both finite and related 
as follows.
  
Ex 5 NPx 
 (2.2.13)
This relationship between energy and average power also holds in the limit as N S ` in 
the following sense. For infinite signals, if x(k) is an energy signal, then its average power 
is Px 5 0, and if x(k) is a power signal, then its energy is infinite. Finite nonzero signals 
are both energy signals and power signals.
Geometric Series
There is a well-known result from mathematics that is helpful in determining the energy 
of an important class of signals. Let z be real or complex scalar. The following power 
series converges if and only if uzu , 1 (see Problem 3.9).
 
o
`
i50
zi 5
1
1 2 z, uzu , 1  
 (2.2.14)
The power series in (2.2.14) is called the geometric series. It is clear that the geometric 
series will not converge for uzu $ 1, since there are an infinite number of terms and each 
one is at least as large as one. Many of the discrete-time signals that appear in practice 
consist of a sum of terms, each of which can be analyzed with the geometric series.
Periodic extension
Geometric series
Geometric Series
EXAMPLE 2.2
Consider the following causal signal that includes a gain A and an exponential 
factor c.
x(k) 5 Ack, k $ 0
First consider under what conditions x(k) is absolutely summable. Using (2.2.13)
uuxuu1 5 o
`
k50
uAc ku
 5 o
`
k50
uAu ? ucku
 5 uAuo
`
k50
ucuk
 5
uAu
1 2 ucu, ucu , 1
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.2 Discrete-time Signals     79
2.2.2 Common Signals
Unit impulse
There are a few common signals that arise repeatedly in applications and examples. Per-
haps the simplest of these is the unit impulse signal,which is nonzero at a single sample.
 
(k) 5
D  5
1, k 5 0
0, k Þ 0
 
 (2.2.15)
The unit impulse (k) plays a role in discrete time that is analogous to the role played by 
the unit impulse or Dirac delta a(t) in continuous time. Note that the unit impulse is both 
finite and causal. A graph of the unit impulse is shown in Figure 2.6a.
Unit Step
Another very common signal that is related to the unit impulse in a simple way is the unit 
step signal, which is causal but not finite.
 
(k) 5 
D 5
1, k $ 0
0, k , 0
 
 (2.2.16)
Unit impulse
Unit step
(a)
(b)
k
210
25
(k)
20.5
0
0.5
1
1.5
k
210
25
10
0
5
10
0
5
(k)
20.5
0
0.5
1
1.5
Figure 2.6: (a) Unit 
Impulse Signal,  
(b) Unit Step 
Signal 
Thus x(k) is absolutely summable if and only if ucu , 1. From (2.2.8) every signal 
that is absolutely summable is also square summable. Therefore, x(k) is an energy 
signal. The energy of x(k) is
Ex 5 o
`
k50
uAcku2
 5 o
`
k50
uAu2 ? uc ku2
 5 uAu2o
`
k50
(ucu2) k
 5
uAu2
1 2 ucu2, ucu , 1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

80    Chapter 2  Discrete-time Systems in the Time Domain
A plot of (k) is shown in Figure 2.6(b). In continuous time, the unit step a(t) is the 
integral of the unit impulse a(t). This is also true in discrete time where the integral is 
replaced by a sum.
 
(k) 5 o
k
i52`
(i) 
 (2.2.17)
Note that if x(k) starts out as a general signal defined for all k, the signal x(k)(k) is a 
causal signal, because multiplication by (k) has the effect of zeroing the samples asso-
ciated with negative time.
Causal Exponential
Perhaps the most common discrete-time signal that appears in several different forms and 
many applications is the causal exponential signal where c is a scalar that may be positive 
or negative, real or complex.
 
x(k) 5 ck(k) 
 (2.2.18)
A causal exponential is bounded for ucu # 1 and unbounded otherwise. When c , 0, the 
value of x(k) oscillates between positive and negative, as can be seen in the plots of x(k) 
in Figure 2.7. From Example 2.2, the causal exponential is absolutely summable and an 
energy signal if and only if ucu , 1. Note that x(k) in (2.2.18) includes the exponential 
form x(k) 5  exp(2kTy) by setting c 5  exp(2Ty).
Causal exponential
Figure 2.7: Causal 
Exponentials 
x(k) 5 ck(k)  
(a) Bounded c . 0, 
(b) Unbounded 
c . 0, (c) Bounded  
c , 0, (d) Unbounded 
c , 0 
25
0
5
10
15
21
20.5
0
0.5
1
(a) c 5 .8
k
x(k)
25
0
5
10
15
21
20.5
0
0.5
1
(c) c 5 2.8
k
x(k)
25
0
5
10
15
25
0
5
(b) c 5 1.05
k
x(k)
25
0
5
10
15
25
0
5
(d) c 5 21.05
k
x(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.2 Discrete-time Signals     81
Power Signals
The most common examples of power signals include basic periodic signals such as the 
cosine and sine.
 
x1(k) 5 cos(2F0kT ) 
 (2.2.19a)
 
x2(k) 5 sin(2F0kT ) 
 (2.2.19b)
A general sinusoid can be written as a linear combination of x1(k) and x2(k). Inter-
estingly enough, the cosine and sine signals also can be written in terms of exponentials. 
Suppose c 5 exp( j2F0T) where j 5 Ï21, and c* denotes the complex conjugate of  
c. Then using Euler’s identity from Appendix 2, the cosine in (2.2.19a) can be written as 
x1(k) 5 (ck 1 c*k)y2. Similarly the sine in (2.2.19b) can be written as x2(k) 5 (ck 2 c*k)y(j2). 
Plots of the discrete-time cosine and sine are shown in Figure 2.8.
The common signals introduced in this section all can be classified according 
to the criteria introduced in Section 2.2.1. Their characteristics are summarized in  
Table 2.1.
Power signals
Figure 2.8:  
Periodic Power  
Signals with 
fs 5 20F0,  
(a) x1(k) 5  
cos(2F0kT)  
(b) x2(k) 5
sin(2F0kT)
220
215
210
25
0
5
10
15
20
21
20.5
0
0.5
1
(a)
k
x1(k)
x2(k)
220
215
210
25
0
5
10
15
20
21
20.5
0
0.5
1
(b)
k
Table 2.1:  
Characteristics of 
Common Discrete-time 
Signals 
x(k) 
Finite 
Causal 
Periodic 
Bound Bx
Energy Ex
Power Px
(k) 
yes 
yes 
no 
1 
1
1
(k) 
no 
yes 
no 
1 
`
1
ck(k), ucu , 1 
no 
yes 
no 
ucu 
1
1 2 ucu2
0 
ck(k), ucu . 1 
no 
yes 
no 
` 
` 
` 
cos(2kyN) 
no 
no 
yes 
1 
`
.5 
sin(2kyN) 
no 
no 
yes 
1 
` 
.5 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

82    Chapter 2  Discrete-time Systems in the Time Domain
Discrete-time Systems 
A discrete-time system S processes a discrete-time input signal x(k) to produce a discrete-
time output signal y(k) as shown in Figure 2.9. Discrete-time systems can be categorized 
in a number of useful ways.
Linear and Nonlinear Systems
Just as with continuous-time systems, discrete-time systems can be linear or nonlinear. 
Suppose yi(k) is the output or response of the system S to the input xi (k) for 1 # i # 2. If 
a and b are arbitrary scalars, then S is a linear system if and only if
 
x(k) 5 ax1(k) 1 bx2(k) 1 y(k) 5 ay1(k) 1 by2(k) 
 (2.3.1)
Otherwise the system S is nonlinear. A block diagram of a linear discrete-time system is 
shown in Figure 2.10. A system that follows (2.3.1) is said to obey the principle of super-
position. There are two important special cases. If b 5 0 in (2.3.1), the input x(k) 5 ax1(k) 
produces the output y(k) 5 ay1(k). Thus when S is linear, scaling the input by a has the 
effect of scaling the output by the same constant a. For example, when a 5 0 this says 
that a zero input generates a zero output. Similarly, if a 5 b 5 1 in (2.3.1), the input 
x(k) 5 x1(k) 1 x2(k) produces the output y(k) 5 y1(k) 1 y2(k). Thus when S is linear, the 
response to the sum of two inputs is the sum of the responses to the individual inputs. 
Almost all of the discrete-time systems we consider will be linear.
2.3
Linear system
Nonlinear
Principle of 
superposition
In plotting discrete-time signals as in Figures 2.6–2.8, it is often convenient to use the 
MATLAB function stem, which emphasizes that the signal is defined only at integer 
multiples of the sampling interval. The following code fragment plots one cycle of a 
sine wave.
 
fs = 10;  
% sampling frequency
 
F0 = 1; 
% frequency of sine
 
N = fs/F0; 
% samples per cycle
 
k = 0 : N-1; 
% sample numbers
 
x = sin(2*pi*F0*k/fs) 
% samples (T = 1/fs)
 
stem(k,x,’.’,’LineWidth’,1.5);  % plot symbol ’.’, wide line
Stem
MATLAB Functions
MATLAB Functions
Figure 2.9: A  
Discrete-time  
System S with 
Input x(k) and 
Output y(k) 
x(k)
y(k)
S
Figure 2.10: A 
Linear Discrete-
time System 
ax1(k) + bx2(k)
ay1(k) + by2(k)
S
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.3 Discrete-time Systems     83
Time-invariant and Time-varying Systems
Another important classification has to do with whether or not the input-output behav-
ior of the system S changes with time. Suppose y(k) is the output corresponding to the 
input x(k). A discrete-time system S is time-invariant if and only if for an arbitrary inte-
ger m the output produced by the shifted input x(k 2 m) is y(k 2 m). Otherwise S is a 
time-varying system.
For a time-invariant system, shifting the input in time shifts the output by the same 
amount. It does not otherwise change the output. A block diagram of a time-invariant sys-
tem is shown in Figure 2.11. Time-invariant systems have parameters that are constant. For 
example, for a discrete-time system that is implemented as a difference equation, the sys-
tem is time-invariant if and only if the coefficients of the difference equation are constant. 
Adaptive filters, which are introduced in Chapter 9, are examples of time-varying systems.
Causal and Noncausal Systems
Physical systems operating in real time have the fundamental property that the output 
at the present time k cannot depend on the future input x(i) for i . k because that input 
has not yet occurred. Such a system is said to be causal. More specifically, suppose that 
yi(k) is the output produced by input xi(k) for 1 # i # 2. A discrete-time system is causal 
if and only if for each time k
 
x1(i) 5 x2(i) for i # k 
 y1(k) 5 y2(k) 
 (2.3.2)
For a causal system S, if two inputs are identical up to time k, then the correspond-
ing outputs also must be identical at time k. Otherwise S is a noncausal system. This 
implies that for a causal system, the present output, y(k), cannot depend on the future 
input x(i) for i . k.
There are instances where signal processing is not performed online in real time. In these 
cases, noncausal systems can be used to process the data offline in batch mode where “future’’ 
samples of the input are available. The following example illustrates the two types of systems.
Time-invariant system
Time-varying system
Causal system
Noncausal system
Offline processing
Figure 2.11: A 
Time-invariant  
Discrete-time 
System 
x(k 2 m)
y(k 2 m)
S
Causal and Noncausal Systems
EXAMPLE 2.3
One of the problems that arises in practice is the need to develop a numerical 
estimate of the derivative of a continuous-time signal xa(t) from the samples of 
the signal x(k) 5 xa(kT). For example, one might want to estimate velocity from 
position samples. The simplest technique that can be implemented in real time is 
to approximate dxa(t)ydt using the slope of the line connecting the present sample 
x(k) with the previous sample x(k 2 1).
y1(k) 5 x(k) 2 x(k 2 1)
T
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

84    Chapter 2  Discrete-time Systems in the Time Domain
This is referred to as a first-order backward difference approximation to the deriv-
ative, also called the backward Euler approximation. It is first-order because the 
maximum distance between the samples used is one. It is backward because it 
goes backward in time to get the samples used. The backward difference differ-
entiator is a causal system because the present output y1(k) does not depend on 
future values of the input x(k).
Although the backward difference approximation is simple and can be imple-
mented in real time, it has the disadvantage that the estimate of the derivative 
ends up being centered between the samples x(k) and x(k 2 1). Thus it produces 
an estimate that is delayed by half a sample or Ty2 seconds. This problem can 
be rectified by using the following second-order central difference approximation 
that uses the slope of the line connecting samples x(k 1 1) and x(k 2 1).
y2(k) 5 x(k 1 1) 2 x(k 2 1)
2T
This is called a central difference because it is centered at sample x(k). Thus there 
is no delay in this estimate of the derivative. However, the price paid is that this 
numerical differentiator can not be implemented in real time because it is a non-
causal system. Notice that the present output y2(k) depends on the future input 
x(k 1 1). Consequently, a central difference approximation can only be imple-
mented offline after samples of x(k) have been recorded and saved. As an illus-
tration of the numerical differentiation, consider the following continuous-time 
input sampled at a rate of fs 5 10 Hz.
xa(t) 5  sin(t)
Here dxa(t)ydt 5  cos(t) A plot of dxa(t)ydt, y1(k) and y2(k) is shown in  
Figure 2.12. Observe that the backward difference y1(k) is delayed by half a sample,  
while y2(k) is not. The central difference y2(k) is also somewhat more accurate 
Backward Euler 
approximation
Figure 2.12 Causal and Noncausal Numerical Estimates of the Derivative of 
xa(t) 5 sin(t) 
0
0.5
1
1.5
2
2.5
24
23
22
21
0
1
2
3
4
kT
y(k)
dxa(t)/dt
Backward Euler
Central difference
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.3 Discrete-time Systems     85
Stable and Unstable Systems
Another fundamental classification of discrete-time systems is based on the notion of 
stability. Recall that an input x(k) is bounded if and only if there exists a Bx . 0 called 
a bound such that ux(k)u # Bx for all k. We say that a discrete-time system S is a bound-
ed-input bounded-output (BIBO) stable system if and only if every bounded input x(k) 
produces a bounded output y(k).
 
ux(k)u # Bx 1 uy(k)u # By 
 (2.3.3)
If at least one bounded input can be found that generates an unbounded output, then the 
system S is BIBO unstable.
Stable system
Unstable system
Stable and Unstable Systems
EXAMPLE 2.4
As a simple illustration of a discrete-time system that is BIBO stable, consider a 
running average filter of order M 2 1.
y(k) 5 1
M o
M21
i50
x(k 2 i)
Suppose the input x(k) is bounded with bound Bx. Then
uy(k)u 5 u
1
M o
M21
i50
x(k 2 i)u
 # 1
M o
M21
i50
ux(k 2 i)u
 
 5 Bx
Consequently the output y(k) is bounded with bound By 5 Bx. By contrast, con-
sider the home mortgage system introduced in Section 2.1.
y(k) 5 y(k 2 1) 11
r
122 y (k 2 1) 2 x(k)
 
511 1 r
122 y (k 2 1) 2 x(k)
Recall that x(k) is the monthly payment, y(k) is the balance owed at the end of 
month k, and r . 0 is the annual interest rate expressed as a fraction. This is an 
example of a system that is not BIBO stable. Later in this chapter and also in 
Chapter 3, we develop direct tests for stability. For now, we can use the definition 
because it is a second-order approximation. Finally, it should be noted that the 
process of numerical differentiation is inherently sensitive to the presence of 
noise, an issue that is of concern for practical signals. Even though the noise 
itself may be small in amplitude, the slope of the noise signal can still be large. 
Consequently, differentiation tends to amplify the high frequency component of 
the noise. The issue of designing digital filters that approximate differentiators is 
discussed in more detail in Chapter 6.
(Continued    )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

86    Chapter 2  Discrete-time Systems in the Time Domain
to check stability. Recall that the initial condition y(21) represents the amount 
of the loan. If the first payment is due at k 5 0, then the balance owed would 
have grown by ry(21)y12 over the course of the first month. If the payment x(0) 
is less than the accumulated interest, then the balance owed will increase with 
y(0) . y(21). Thus, the following bounded input produces an unbounded output.
ux(k)u , ry(21)
12
 1 uy(k)u S ` as  k S `
It follows that the home mortgage system is a BIBO unstable system. Indeed, if 
the bounded input x(k) 5 0 is used, then no monthly payments are made, and the 
balance owed grows without bound due to the accumulated interest.
Passive and Active Systems
The final classification of systems that we consider has to do with the energy of the input 
and output signals. Suppose the input is square summable and is therefore an energy 
signal with energy Ex as in (2.2.9). Similarly, suppose the resulting output y(k) is also an 
energy signal with energy Ey. The discrete-time system S is a passive system if and only if 
the energy does not increase.
 
Ey # Ex 
 (2.3.4)
Consequently, a passive system does not add energy to the input signal as it propagates 
through the system to produce the output signal. Otherwise, S is referred to as an active 
system. An active physical system requires a power source, whereas a passive system does 
not. A special case of a passive system is a lossless system, which is a discrete-time system 
S for which the energy stays the same.
 
Ey 5 Ex 
 (2.3.5)
Lossless physical systems contain energy storage elements such as capacitors, inductors, 
springs, and masses. However, they do not contain energy dissipative elements such as 
resistors or dampers.
Difference Equations
Every finite dimensional linear time-invariant (LTI) system can be represented in the time 
domain by a constant-coefficient difference equation with input x(k) and output y(k).
 
y(k) 1 o
n
i51
aiy(k 2 i) 5 o
m
i50
bix(k 2 i)  
 (2.4.1)
Here M 5  max hn, mj is the dimension of the system. For convenience, we will refer to the 
LTI system in (2.4.1) as the system S. Note that the coefficient of the present output y(k) 
has been normalized to a0 5 1. This can always be done by first dividing both sides of 
(2.4.1) by a0 if needed. The output of the system S at time k can be expressed as follows.
 
y(k) 5 o
m
i50
bix(k 2 i) 2 o
n
i51
aiy(k 2 i) 
 (2.4.2)
Passive system
Active system
Lossless system
2.4
LTI system
System dimension
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.4 Difference Equations    87
The system S, in addition to being linear and time-invariant, is also causal because the 
present output y(k) does not depend on the future input x(i) for i . k.
When considering inputs to discrete-time systems, it is convenient to focus on causal 
signals where x(k) 5 0 for k , 0. When causal inputs are used, the output or response of 
a discrete-time system will depend on both the input x(k) and the initial condition, which 
is represented by a vector y0 [ Rn of past outputs.
 
y0 5
D  f
 y(21), y(22), Á , y(2n)gT 
 (2.4.3)
In general the complete response y(k) will depend on both y0 and the input x(i) for 
0 # i # k, as shown in Figure 2.13. For the system S, the contributions to the output 
from initial condition y0 and the input x(k) can be considered separately. Because the 
system is linear, they can be added to produce the complete response, y(k).
2.4.1 Zero-input Response
The output of the discrete-time system S when the input is x(k) 5 0 is denoted yzi(k) and 
is referred to as the zero-input response of the system. Thus the zero-input response is the 
solution of the simplified system.
 
y(k) 1 o
n
i51
aiy (k 2 i) 5 0, y0 Þ 0 
 (2.4.4)
The zero-input response, yzi(k), is the part of the overall response that arises from the 
initial condition y0. To find the zero-input response, consider a generic solution of the 
form y(k) 5 zk where the complex scalar z is yet to be determined. Substituting y(k) 5 zk 
into (2.4.4) and multiplying both sides by zn2k yields
 
a(z) 5 zn 1 a1zn21 1 . . . 1 an 5 0  
 (2.4.5)
The polynomial a(z) 5 zn 1 a1zn21 1 . . . 1 an is called the characteristic polynomial of the 
system. If we factor a(z) into its n roots p1, p2, . . . , pn, the factored form of the character-
istic polynomial of S is
 
a(z) 5 (z 2 p1)(z 2 p2) Á (z 2 pn) 
 (2.4.6)
Assuming that an Þ 0 in (2.4.4), it follows that z is not a factor of a(z) so the roots of 
a(z) are nonzero. The characteristic polynomial is the key to determining the zero-input 
response. The signal y(k) 5 czk is a solution of (2.4.4) for an arbitrary scalar c if and only 
if z is a root of the characteristic polynomial. The simplest case occurs when there are n 
Initial condition
Zero-input response
Characteristic 
polynomial
Figure 2.13:  
The Complete 
Response of the 
System S Depends 
on the Initial 
Condition y0 and 
the Input x(k) 
x(k)
y0
y(k)
S
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

88    Chapter 2  Discrete-time Systems in the Time Domain
distinct roots, in which case we say that the roots are simple. For n simple roots, individual 
solutions of the form cipk
i  can be added to produce the zero-input response
 
yzi (k) 5 o
n
i51
ci  pk
i , k $ 2n 
 (2.4.7)
The terms in (2.4.7) of the form ci pk
i  are called the natural modes of the system. Each 
simple root of the characteristic polynomial a(z) generates a simple natural mode term in 
the zero-input response.
 
 simple natural mode 5 c0 pk  
 (2.4.8)
The vector of weighting coefficients c 5 fc1, c2, . . . , cngT in (2.4.7) depends on the initial 
conditions. In particular, setting yzi(k) 5 y(k) for 2n # k # 21 yields n equations in the 
n unknown elements of the coefficient vector c [ Rn.
Simple roots
Natural mode
Zero-input Response: Simple Roots
EXAMPLE 2.5
To illustrate the process of finding a zero-input response, consider the following 
two-dimensional discrete-time system.
y(k) 2 .6y(k 2 1) 1 .05y(k 2 2) 5 2x(k) 1 x(k 2 1)
Suppose the initial condition is y(21) 5 3 and y(22) 5 2, in which case 
y0 5 f3, 2gT. By inspection, the characteristic polynomial of this system is
a(z) 5 z2 2 .6z 1 .05
 5 (z 2 .5)(z 2 .1)
Thus the vector of simple roots is p 5 f.5, .1gT, and the form of the zero-input 
response is
yzi(k) 5 c1(.5)k 1 c2(.1)k
To find the coefficient vector c [ R2, we apply the initial condition, yzi (21) 5 3 
and yzi (22) 5 2. In matrix form, these two equations can be written as
3
2
10
4
1004 c 53
3
24
Subtracting twice the first equation from the second yields 80c2 5 24 or 
c2 5 21y20. Similarly, subtracting ten times the first equation from the second 
yields 216c1 5 228 or c2 5 7y4. Thus the zero-input response associated with 
this initial condition is
yzi(k) 5 1.75(.5)k 2 .05(.1)k
An alternative way to compute the coefficient vector c using MATLAB is
A = [2 10 ; 4 100]; 
y0 = [3 ; 2];
c = A \ y0
MATLAB 
Functions
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.4 Difference Equations    89
The roots of the characteristic polynomial can be real or complex. Since the coeffi-
cients of a(z) are real, complex roots always occur in complex conjugate pairs. The two 
natural mode terms associated with a complex conjugate pair of roots can be combined 
by using Euler’s identity.
 
exp(6j) 5 cos() 6 j sin() 
 (2.4.10)
Note that the left side of Euler’s identity is the polar form of a complex variable whose 
magnitude is r 5 1, and the right side is the rectangular form. When complex roots occur 
in conjugate pairs, their coefficients also form complex conjugate pairs. Suppose a pair of 
Complex conjugate 
roots
Euler’s identity
The natural modes in (2.4.7) correspond to the simplest and most common special 
case. A more involved case arises when one of the roots in (2.4.6) is repeated. For exam-
ple, suppose root p occurs r times as a root of a(z). In this case p is referred to as a root 
of multiplicity r, and it generates a multiple natural mode term of the following form.
 
 multiple natural mode 5 (c1 1 c2k 1 . . . 1 cr k r21)pk  
 (2.4.9)
Observe that the coefficient of a root of multiplicity r is a polynomial, c(k), of degree r 2 1. 
Again the r coefficients of c(k) are determined from the initial conditions. For the special 
case when r 5 1, the coefficient polynomial c(k) reduces to a polynomial of degree zero, that 
is, a constant as in (2.4.8). The expression in (2.4.9) represents a general natural mode of 
order r, whereas for simple roots the natural modes are all modes of order one as in (2.4.8).
Multiple roots
Zero-input Response: Multiple Roots
EXAMPLE 2.6
As an illustration of what happens when the characteristic polynomial has multi-
ple roots, consider the following two-dimensional discrete-time system. 
y(k) 1 y(k 2 1) 1 .25y(k 2 2) 5 3x(k)
Suppose the initial condition is y(21) 5 21 and y(22) 5 6 or y0 5 f21, 6gT. The 
characteristic polynomial of this system is
a(z) 5 z2 1 z 1 .25
5 (z 1 .5)2
Thus the vector of roots is p 5 f2.5, 2.5gT, which means that p 5 2.5 is a root 
of multiplicity 2. Thus the form of the zero-input response is
yzi(k) 5 (c1 1 c2k)(2.5)k
To find the coefficient vector c [ R2, we apply the initial condition, yzi (21) 5 21 
and yzi (22) 5 6. In matrix form, these two equations can be written as
3
22
2
4
284 c 53
21
64
Adding twice the first equation to the second yields 24c2 5 4 or c2 5 21. Adding 
four times the first equation to the second yields 24c1 5 2 or c1 5 2.5. Thus the 
zero-input response associated with this initial condition is
yzi(k) 5 2(.5 1 k)(2.5)k
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

90    Chapter 2  Discrete-time Systems in the Time Domain
Zero-input Response: Complex Roots
EXAMPLE 2.7
As an example of a discrete-time system whose characteristic polynomial has 
complex roots, consider the following two-dimensional system. 
y(k) 1 .49y(k 2 2) 5 3x(k)
Suppose the initial condition is y(21) 5 4 and y(22) 5 22 or y0 5 f4, 22gT. The 
characteristic polynomial of this system is
a(z) 5 z2 1 1
 5 (z 2 j.7)(z 1 j.7)
Thus the roots are p1,2 5 6j.7. Expressing the roots in polar form, the magnitude 
is r 5 .7, and the phase angle is  5 y2. That is, p1,2 5 .7 exp (6jy2). From 
(2.4.11), the form of the zero-input response is
yzi (k) 5 .7k fc1 cos (ky2) 1 c2 sin (ky2)g
Next, applying the initial conditions y(21) 5 4 and y(22) 5 22 yields
2c2y.7 5 4
2c1y.49 5 22
Thus c1 5 .98, c2 5 22.8, and the zero-input response is
yzi (k) 5 .7k f.98 cos (ky2) 2 2.8 sin (ky2)g
complex conjugate roots is expressed in polar form as p1,2 5 r exp(6j). By using Euler’s 
identity it is possible to show that the pair of natural mode terms can be combined to 
form the following damped sinusoid called a complex mode.
 
 complex mode 5 rk fc1 cos(k) 1 c2 sin(k)g  
 (2.4.11)
In the event that some of the roots are simple while others are multiple, the zero- 
input response consists of a combination of first order and higher order modes which can 
be real or complex. Together, these modes will include n unknown coefficients that are 
solved for by applying the n initial conditions.
2.4.2 Zero-state Response
In general, the output of a LTI discrete-time system S also contains a component that 
is due to the presence of the input x(k). The output of S corresponding to an arbitrary 
input x(k), when the initial condition vector is zero, is denoted yzs(k) and is referred to as 
the zero-state response.
 
y(k) 1 o
n
i51
ai y(k 2 i) 5 o
m
i50
bi x(k 2 i), y0 5 0 
 (2.4.12)
Zero-state response
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.4 Difference Equations    91
The computation of the zero-state response is more involved than the computation 
of the zero-input response because there are infinitely many inputs that might be con-
sidered. In Section 2.7, a systematic procedure for finding the zero-state response to any 
input is presented. For now, we illustrate the process of finding the zero-state response by 
considering the following important class of inputs.
 
x(k) 5 Apk
0(k) 
 (2.4.13)
Here x(k) is a causal exponential with amplitude A and exponential factor p0. To simplify 
the final result we assume that the roots of the characteristic polynomial a(z) are distinct 
from one another and from p0. Just as the characteristic polynomial a(z) can be obtained 
from inspection of the coefficients of the output in (2.4.12), a second polynomial b(z) 
called the input polynomial can be generated from the coefficients of the input. For the 
case when m # n, this polynomial is
 
b(z) 5 b0z n 1 b1z n21 1 . . . 1 bmz n2m 
 (2.4.14)
Given an input as in (2.4.13) and n 1 1 distinct roots, the zero-state response has a form 
generally similar to the zero-input response.
 
yzs (k) 5 o
n
i50
di pk
i (k) 
 (2.4.15)
The coefficient vector d [ Rn11 can be computed directly from the polynomials a(z) and 
b(z) as follows.
 
di 5 A(z 2 pi)b(z)
(z 2 p0)a(z) u z 5 pi, 0 # i # n 
 (2.4.16)
Note that in general di Þ 0 because pi is root of the denominator in (2.4.16) for 0 # i # n.
Input polynomial
Zero-state Response
EXAMPLE 2.8
Consider the two-dimensional discrete-time system from Example 2.5 with the 
following input x(k).
y(k) 5 .6y(k 2 1) 2 .05y(k 2 2) 1 2x(k) 1 x(k 2 1)
x(k) 5 .8k 1 1(k)
The characteristic polynomial of this system is
a(z) 5 z2 2 .6z 1 .05
 5 (z 2 .5)(z 2 .1)
From inspection of the input terms, the input polynomial b(z) is
b(z) 5 2z2 1 z
 5 2z(z 1 .5)
In this case we have A 5 .8 and
Ab(z)
(z 2 p0)a(z) 5
1.6z(z 1 .5)
(z 2 .8)(z 2 .5)(z 2 .1)
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

92    Chapter 2  Discrete-time Systems in the Time Domain
Because the system S in (2.4.1) is linear, the complete response associated with both 
a nonzero initial condition y0 and a nonzero input x(k) is just the sum of the zero-input 
response and the zero-state response.
 
y(k) 5 yzi(k) 1 yzs(k)  
 (2.4.17)
Most of the discrete-time systems we investigate later have the property that they are 
BIBO stable. For a stable system, the zero-input response will go to zero as k approaches 
infinity.
 
uyzi(k)u S 0  as  k S ` 
 (2.4.18)
If the input to a stable system is a power signal, then the complete response will be dom-
inated by the zero-state response. Consequently, we will focus most of our attention on 
the zero-state response. However, when the transient behavior associated with the initial 
condition is of interest, then the zero-input response must be computed as well.
Complete response
Stable system
From (2.4.16), the coefficient vector d [ R3 is
d0 5 1.6(.8)(1.3)
.3(.7)
5 7.92
d1 5 1.6(.5)(1.0)
2.3(.4)
5 26.67
d2 5 1.6(.1)(.6)
2.7(2.4) 5 .343
Applying (2.4.15), the zero-state response associated with the input x(k) is then
yzs(k) 5 f7.92(.8)k 2 6.67(.5)k 1 .343(.1)kg(k)
Complete Response
EXAMPLE 2.9
Again consider the two-dimensional discrete-time system from Example 2.5 and 
Example 2.8. Suppose the initial condition is y0 5 f3, 2gT and the input is
x(k) 5 .8k 1 1(k)
The zero-input response associated with the initial condition was computed 
in Example 2.5, and the zero-state response associated with the input was 
computed in Example 2.8. Thus from (2.4.17), the complete response of this 
system is
y(k) 5 yzi(k) 1 yzs(k)
 
 5 1.75(.5)k 2 .05(.1)k 1 f7.92(.8)k 2 6.67(.5)k 1 .343(.1)kg(k)
To verify that y(k) satisfies the initial condition y0 5 f3, 2gT, we have
y(21) 5 1.75(.5)21 2 .05(.1)21 5 3
y(22) 5 1.75(.5)22 2 .05(.1)22 5 2
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.4 Difference Equations    93
A convenient way to find the complete responses is to compute it numerically using 
the DSP Companion function f_filter0.
% F_FILTER0 Compute the complete response of a discrete-time system
% FILTER: 
Compute zero-state output of a linear discrete-time  
% 
 
system
%
% Usage:
% 
y = f_filter0 (a,b,x,y0);
% 
yz = filter (b,a,x);
% Pre:
% 
b = vector of length m+1 containing input coefficients
% 
a = vector of length n+1 containing output coefficients
% 
x = vector of N input samples x = [x(0),...,x(N-1)]
% 
y0 = optional vector of n past outputs y0 = [y(-1),...,y(-n)]
DSP Companion
DSP Companion
25
0
5
10
15
20
0
2
4
(a)
k
yzi(k)
yzs(k)
y(k)
25
0
5
10
15
20
0
2
4
(b)
k
25
0
5
10
15
20
0
2
4
(c)
k
Figure 2.14: The Output of the System in Example 2.9: (a) Zero-input 
Response with y0 5 f3, 2gT, (b) Zero-state Response with x(k) 5 10(.8)k(k), 
(c) Complete response. 
Plots of the zero-input response, the zero-state response, and the complete 
response are shown in Figure 2.14. Notice that for k W 1, the complete response 
is dominated by the zero-state response because the zero-input response of this 
stable system quickly dies out.
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

94    Chapter 2  Discrete-time Systems in the Time Domain
% Post
% 
y = vector of length p containing the output samples. If
% 
 
 y0 is present, then y = [y(-n),y(-n+1),...,y(N-1)],
% 
 
 otherwise y = [y(0),y(1),...,y(N-1)].
% 
yz = vector of length N containing the zero-state output
% 
 
 samples. yz = [y(0),y(1),...,y(N-1)].
The function filter is a standard MATLAB function. Calling filter is equivalent to 
calling f_filter without the optional initial condition argument y0.
The advantage of the numerical technique is that it works for any input x(k). 
Furthermore, the filter and f_   filter0 functions are valid for any n $ 0 and any 
m $ 0. Thus the numerical approach is very general. However, it does not produce 
a closed-form expression for the system output. Instead it yields a fixed number of 
output samples.
Numerical Zero-state Response
EXAMPLE 2.10
Consider the following four-dimensional discrete-time system driven by input 
x(k).
y(k) 5 3y(k 2 1) 2 3.49y(k 2 2) 1 1.908y(k 2 3) 2 .4212y(k 2 4) 1 x(k)22x(k23)
x(k) 5 2k(.7)k sin(2ky10)(k)
The two coefficient vectors are
a 5 f1, 3, 23.49, 1.908, 2.4212gT
b 5 f1, 0, 0, 22gT
Using the MATLAB function p 5 roots(a), the roots of the characteristic poly-
nomial are
p 53
.9
.9
.6 1 j.4
.6 2 j.44
Thus there is a double root at p1,2 5 .9 and a complex conjugate pair of roots 
at p3,4 5 .6 6 j.4. Plots of the first N 5 50 points of the input and zero-state 
response are shown in Figure 2.15, which was generated by running exam2_10 
from the driver program g_dsp.
MATLAB 
Functions
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.5 Block Diagrams    95
Block Diagrams
Discrete-time systems can be displayed graphically in the form of block diagrams. Using 
block diagrams, the flow of information and the interconnection between subsystems can 
be visualized. A block diagram is a set of blocks that represent processing units intercon-
nected by directed line segments that represent signals. There are four basic components 
as shown in Figure 2.16. The gain block scales the signal by the constant specified in the 
2.5
0
50
22
21
0
1
2
(a)
k
x(k)
0
5
10
15
20
25
30
35
40
45
50
2100
250
0
50
(b)
k
5
10
15
20
25
30
35
40
45
yzs(k)
Figure 2.15: Numerical Solution of the Zero-state Response for the Four- 
dimensional System in Example 2.10: (a) Input x(k) 5 2k(.7)k 
sin(2ky10)(k), (b) Zero-state Response 
Figure 2.16: Basic 
Elements of Block 
Diagrams 
2
Summer
x(k)
x(k) 2 y(k)
y(k)
Break Out
x(k)
x(k)
x(k)
x(k)
Ax(k)
A
Gain
Delay
x(k)
x(k 2 1)
z21
1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

96    Chapter 2  Discrete-time Systems in the Time Domain
Figure 2.17: Block 
Diagram of a 
Moving Average 
(MA) System of 
Order m 5 3 
z21
z21
1
1
1
x(k)
y(k)
b3
b2
z21
b1
b0
block label. The delay block, which is labeled z21, delays the signal by one sample. The 
summer or summing junction block adds or subtracts two or more signals as indicated by 
the sign labels. Finally, the break out point extracts a copy of the signal.
To develop a block diagram for a linear discrete-time system, first consider the case 
when the output is a weighted sum of the past inputs.
 
y(k) 5 o
m
i50
bix(k 2 i) 
 (2.5.1)
This is sometimes referred to as a moving average or MA model because, when 
bi 5 1y(m 1 1), the output is a moving average of the past m 1 1 samples of the input. 
More generally, it is a weighted average with the coefficient vector b specifying the weights. 
A MA model has a simple block diagram as can be seen in Figure 2.17, which illustrates 
the case m 5 3. Since each z21 block delays the input by one sample, the structure in  
Figure 2.17 is sometimes referred to as a tapped delay line.
Next consider a block diagram for a more general LTI system where the current out-
put y(k) depends on both the past inputs and the past outputs.
 
y(k) 5 o
m
i50
bix(k 2 i) 2 o
n
i51
aiy(k 2 i) 
 (2.5.2)
Recall that M 5  max hm,nj is the dimension of the system. Suppose m # n. In this case 
we can pad the coefficient vector b [ Rm11 with n 2 m zeros so that b [ Rn11. Alter-
natively, if m . n we can pad the coefficient vector a [ Rn11 with m 2 n zeros so that 
a [ Rm11. In either case, the zero-padded coefficient vectors a and b will both be of length 
M 1 1 where M 5 max hn, mj is the system dimension. The difference equation in (2.5.2) 
then can be expressed using a single sum as follows.
 
y(k) 5 b0x(k) 1 o
M
i51
bix(k 2 i) 2 aiy(k 2 i) 
 (2.5.3)
Using this formulation, the system S can be represented in block diagram form as shown 
in Figure 2.18, which illustrates the two-dimensional case, M 5 2. Notice that in general 
the intermediate signals ui(k) and the output y(k) can be defined recursively as follows.
 uM(k) 5 bMx(k) 2 aMy(k)
 
 uM21(k) 5 bM21x(k) 2 aM21y(k) 1 uM(k 2 1) 
 (2.5.4)
 
o
 u1(k) 5 b1x(k) 2 a1y(k) 1 u2(k 2 1)
 
 y(k) 5 b0x(k) 1 u1(k 2 1)
 
 (2.5.5)
Delay block
MA model
Tapped delay line
Dimension
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.6 The Impulse Response     97
The vector u(21) [ RM can be thought of as a generalized initial condition vector for the 
system S. The block diagram shown in Figure 2.18 is called a transformed direct form 
II realization of the system S. Later, in Chapter 6 and Chapter 7, we examine a number 
of alternative realizations, each with its own advantages and disadvantages. One of the 
useful features of the block diagram in Figure 2.18 is that it requires a minimum number 
of delay elements. Consequently, it is optimal in terms of memory usage. The MATLAB 
function filter is based on the realization in Figure 2.18.
The impulse Response 
The simplest nonzero input that can be applied to a discrete-time system is the unit 
impulse input (k). When the initial condition of the system S is zero, the effect of apply-
ing an impulse input is to excite the natural modes of the system. The resulting output is 
called the impulse response.
The impulse response of a linear time invariant system S is the zero-state response h(k) 
produced by the unit impulse input.
x(k) 5 (k) 1 h(k) 5 yzs(k)
Note that if the system S is a causal system, then its impulse response h(k) will be 
a causal signal. As we shall see, the impulse response h(k) is the key to computing the 
zero-state response to any input x(k). Before examining how this is accomplished, it is 
useful to investigate the impulse response of two important classes of systems.
2.6.1 FiR Systems
Consider a discrete-time system S described by difference equation (2.4.1) but where the 
output coefficient vector is simply a 5 0.
 
y(k) 5 o
m
i50
bi x(k 2 i) 
 (2.6.1)
2.6
DEFiNiTioN
2.1  impulse Response
Figure 2.18:  
A Block Diagram 
of the System 
in (2.5.3) when 
M 5 2
z21
z21
1
1
1
u2(k)
2
2
u1(k)
x(k)
y(k)
b2
b1
a1
a2
b0
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

98    Chapter 2  Discrete-time Systems in the Time Domain
The impulse response of this type of system is can be computed directly from Defini-
tion 2.1, which yields
 
h(k) 5 o
m
i50
bi (k 2 i) 
 (2.6.2)
Recall that (k 2 i) 5 0 for k Þ i and (0) 5 1. Consequently, h(k) in (2.6.2) can be nonzero 
only for 0 # k # m. More specifically, the impulse response of the FIR system in (2.6.1) is
 
h 5 hb0, b1, . . . , bm, 0, 0, . . . j  
 (2.6.3)
The first m samples of the impulse response are just the input coefficients bi, and the 
remaining samples are all zero. A system whose impulse response contains a finite num-
ber of nonzero samples is called a finite impulse response system.
A linear system S is a finite impulse response or FIR system if and only if the impulse 
response h(k) has a finite number of nonzero samples. Otherwise, S is an infinite 
impulse response or IIR system.
It follows that the system in (2.6.1) is an FIR system with an impulse response of dura-
tion m. FIR systems have a number of useful characteristics that make them good candi-
dates for digital filters. The design of digital FIR filters is presented in Chapter 6, and the 
design of digital IIR filters is presented in Chapter 7.
FIR impulse response
DEFiNiTioN
2.2  FiR and iiR Systems
impulse Response: FiR
EXAMPLE 2.11
As an illustration of an FIR system, consider the running average filter introduced 
in Example 2.4. Here the filter output at time k is the average of the last M samples 
of the input.
y(k) 5 1
M o
M21
i50
x(k 2 i)
A running average filter tends to smooth out the variation in the input signal, 
depending on the value of M. From (2.6.3), the impulse response of the running 
average filter is
h(k) 55
1
M ,
0 # k , M
0,
M # k , `
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.6 The Impulse Response     99
2.6.2 iiR Systems
Next consider a linear time-invariant system S as in (2.4.1) where the number of output 
terms is n $ 1.
 
y(k) 5 o
m
i50
bix(k 2 i) 2 o
n
i51
aiy(k 2 i) 
 (2.6.4)
A systematic technique for finding the impulse response for this more general type 
of system is introduced in Chapter 3 using the Z-transform. For now, to illustrate the 
computation of h(k) suppose that m # n and the roots of the characteristic polynomial 
a(z) are simple and nonzero. The factored form of a(z) is
 
a(z) 5 (z 2 p1)(z 2 p2) Á (z 2 pn) 
 (2.6.5)
25
0
5
10
15
20
25
0
0.5
1
(a)
k
x(k)
h(k)
25
0
5
10
15
20
25
20.1
0
0.1
0.2
(b)
k
Figure 2.19: Impulse Response of Running Average Filter Using M 5 10 
Samples: (a) Input, (b) Output 
Thus the effect of the running average filter is to take an impulse input of 
height one and width one sample and spread it out to form a pulse of height 
1yM and width M samples. In both cases the height times the width (the 
area) is one. The running average impulse response when M 5 10 is shown in  
Figure 2.19.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

100    Chapter 2  Discrete-time Systems in the Time Domain
impulse Response: iiR
EXAMPLE 2.12
As an example of an IIR system, consider the following two-dimensional dis-
crete-time system S.
 
y(k) 5 2x(k) 2 3x(k 2 1) 1 4x(k 2 2) 1 .2y(k 2 1) 1 .8y(k 2 2)
The characteristic polynomial of S is
a(z) 5 z2 2 .2z 2 .8
 5 (z 2 1)(z 1 .8)
Thus S has simple nonzero roots at p 5 f1, 2.8gT. From (2.6.6), the form of the 
impulse response is
h(k) 5 d0(k) 1 fd1 1 d2(2.8)kg(k)
By inspection, the polynomial associated with the input terms is
b(z) 5 2z2 2 3z 1 4
Applying (2.6.7) with p0 5 0, the coefficient vector d [ R3 is
d0 5
4
(21)(.8) 5 25
d1 5 2 2 3 1 4
1.8
5 1.67
d2 5 2(.64) 2 3(2.8) 1 4
(2.8)(21.8)
5 5.33
Finally, the impulse response of the system S is
h(k) 5 25(k) 1 f1.67 2 5.33(2.5)kg(k)
A plot of h(k) is shown in Figure 2.20. Clearly, h(k) has infinite duration and S is 
an IIR system.
When m # n and the characteristic polynomial has simple nonzero roots, the form of the 
impulse response is
 
h(k) 5 d0(k) 1 o
n
i51
dipk
i(k) 
 (2.6.6)
From (2.4.16), the coefficient vector d [ Rn11 is computed as follows, where b(z) is the 
input polynomial defined in (2.4.14) and p0 5 0.
 
di 5 (z 2 pi)b(z)
za(z)
u z5pi
, 0 # i # n 
 (2.6.7)
If di Þ 0 for some i . 0, then the duration of the impulse response h(k) is infinite, in 
which case S is an IIR system.
IIR impulse response
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.6 The Impulse Response     101
22
21
21.5
20.5
0
1
1.5
0.5
2
22
21.5
21
20.5
0
0.5
1
1.5
2
X
X
O
O
Re(z)
Im(z)
Figure 2.21: Poles 
and Zeros of the 
IIR System in 
Example 2.12 
Figure 2.20: Impulse Response of the IIR System in Example 2.12:  
(a) Input, (b) Output 
25
0
5
10
15
20
25
30
0
0.5
1
(a)
k
x(k)
h(k)
25
0
5
10
15
20
25
30
24
22
0
2
4
6
(b)
k
The roots of b(z) and a(z) are referred to as the poles and zeros, respectively, of the 
discrete-time system. A plot of the poles and zeros of the IIR system in Example 2.12 is 
shown in Figure 2.21. By convention, an X is used to mark a pole and an O is used to mark 
a zero.
Poles, zeros
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

102    Chapter 2  Discrete-time Systems in the Time Domain
The DSP Companion contains the following functions for computing the impulse 
response of a linear discrete-time system and for plotting the pole-zero pattern of a 
linear discrete-time system. If the MATLAB control system toolbox is available, then 
the function pzmap also can be used.
% F_IMPULSE: Compute impulse response
% F_PZPLOT: Plot transfer function poles and zeros
%
% Usage:
% 
[h,k] = f_impulse (b,a,N);
% 
f_pzplot (b,a,caption,fsize,lsize,psize,caption);
% Pre:
% 
b = vector of length m+1 containing input coefficients
% 
a = vector of length n+1 containing output coefficients
% 
N = number of samples
% 
caption = optional plot title
% 
fsize 
= optional font size. Default: 10
% 
lsize 
= optional line size for poles and
% 
 
 zeros. Default: 1.5
% 
psize 
= optional maximum range for the plot
% 
 
 Default: 3
% Post:
% 
h 
= vector of length N containing the impulse  
% 
 
 response
% 
k 
=  vector of length N containing the discrete 
% 
 
 solution times
%
% See also: F_PZSURF
DSP Companion
DSP Companion
Convolution
2.7.1 Linear Convolution
Once the impulse response is known, the zero-state response to an arbitrary input x(k) 
can be computed from it. To see this, first note that a causal signal x(k) can be written as a 
weighted sum of impulses as follows. This is called the sifting property of the unit impulse.
 
x(k) 5 o
k
i50
x(i)(k 2 i) 
 (2.7.1)
Here the ith term is an impulse of amplitude x(i) occurring at time k 5 i. Recall that 
the zero-state response to an impulse of unit amplitude occurring at k 5 0 is h(k). If the 
system is linear and time-invariant, scaling the input by x(i) and delaying it by i samples 
produces an output that is also scaled by x(i) and delayed by i samples. That is, the output 
corresponding to the ith term in (2.7.1) is simply x(i)h(k 2 i). Furthermore, since S is a 
linear system, the response to the sum of inputs in (2.7.1) is just the sum of the responses 
2.7
Sifting property
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.7 Convolution    103
to the individual inputs. Thus the zero-state output produced by x(k) in (2.7.1) can be 
written as
 
y(k) 5 o
k
i50
h(k 2 i)x(i)  
 (2.7.2)
The formulation on the right-hand side of (2.7.2) is referred to as the linear convolution 
of the signal h(k) with the signal x(k). Note that if x(k) is not causal, then the lower limit 
of the sum becomes i 5 2`, and if h(k) is not causal, the upper limit of the sum becomes 
i 5 `.
Let h(k) and x(k) be causal discrete-time signals. The linear convolution of h(k) with 
x(k) is denoted h(k) w x(k) and defined
h(k) w x(k) 5
D  o
k
i50
h(k 2 i)x(i), k $ 0
The operator w is called the convolution operator. Comparing Definition 2.3 with (2.7.2), 
we see that the zero-state response of a discrete-time system can be expressed in terms of 
convolution as
 
yzs(k) 5 h(k) w x(k) 
 (2.7.3)
That is, the zero-state response is the linear convolution of the impulse response with the 
input. The convolution operator has a number of useful properties. For example, starting 
with Definition 2.3 and using a change of variable
 
h(k) w x(k) 5 o
k
i50
h(k 2 i)x(i)
 5 o
0
m5k
h(m)x(k 2 m), m 5 k 2 i
 5 o
k
m50
h(m)x(k 2 m)
 
 5 x(k) w h(k) 
 (2.7.4)
Thus the convolution operator is commutative; the two arguments can be interchanged with-
out changing the result. Consequently, the zero-state response also can be written as
 
y(k) 5 o
k
i50
h(i)x(k 2 i)  
 (2.7.5)
When the system S is the FIR system in (2.6.1) with an impulse response of duration m, 
the upper limit in (2.7.5) can be replaced by the constant m, and the impulse response can 
be replaced with the input coefficients h(k) 5 bk.
In addition to being commutative, the convolution operator is also associative and 
distributive as summarized in Table 2.2. The other two properties can be established 
using Definition 2.3 and are left as exercises (Problems 2.28, 2.29). 
Zero-state output
Linear convolution
definition
2.3 Linear Convolution
Convolution operator
Commutative operetor
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

104    Chapter 2  Discrete-time Systems in the Time Domain
Name 
Property 
Commutative 
 f w g 
=   g w f 
Associative 
 f w (g w h) 
= 
 f w (g w h) 
Distributive 
 f w (g 1 h) 
=  f w g 1 f w h 
Table 2.2: Properties 
of the Linear 
Convolution Operator 
MATLAB Functions 
The linear convolution of two finite discrete-time signals h and x can be computed 
using the MATLAB function conv as follows.
% CONV: Perform linear convolution
%
% Usage:
% 
y = conv (h,x);
% Pre:
% 
h = vector of length L
% 
x = vector of length M
% Post:
% 
y =  vector of length L+M-1 containing convolution of h 
% 
with x
MATLAB Functions
Linear Convolution
EXAMPLE 2.13
Suppose the input to the system in Example 2.12 is the following causal sine.
x(k) 5 10 sin(.1k)(k)
If the initial condition is zero, the output of this system using the convolution 
representation is
 y(k) 5 o
k
i50
h(i)x(k 2 i)
 5 10o
k
i50
{25(i) 1 f1.67 2 5.33(2.5)ig}  sin f.1(k 2 i)g, k $ 0
A plot of the zero-state response, obtained by running exam2_13, is shown in 
Figure 2.22. Here the discrete-time signal was computed directly using filter, and 
the interpolated version was computed with convolution using conv.
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.7 Convolution    105
Figure 2.22: Zero-state Response for Example 2.13 
0
5
10
15
20
25
30
35
40
0
20
40
60
80
100
120
k
yzs(k)
Direct
Convolution
2.7.2 Circular Convolution
For a numerical implementation of the convolution operation to be practical, the sig-
nals h(k) and x(k) must be finite. Suppose h(k) is defined for all k but nonzero only for 
0 # k , L. Similarly, let x(k) be a signal that is nonzero for 0 # k , M. Then the linear 
convolution in (2.7.5) can be expressed as
 
y(k) 5 o
L21
i50
h(i)x(k 2 i), 0 # k , L 1 M 2 1  
 (2.7.6)
The upper limit on the summation can be changed from k to L 2 1 because x(k) is 
causal and h(i) 5 0 for i $ L. Observe that when i 5 L 2 1, we have x(k 2 i) Þ 0 for 
L 2 1 # k , L 1 M. Consequently, the linear convolution of an L-point signal with an 
M-point signal is a signal of length L 1 M 2 1.
There is an alternative way to define convolution where the length of the result is 
the same as the lengths of the two operands. To define this form of convolution, we first 
introduce the notion of a periodic extension of a finite signal x(k). The periodic extension 
of an N-point signal x(k) is a signal xp(k) defined for all integers k as follows.
 
xp(k) 5
D  
x f mod (k, N)g  
 (2.7.7)
Here the MATLAB function mod(k,N) is read as k modulo N. For a fixed integer N, the 
function mod (k, N) is a periodic ramp of period N where mod(k, N) 5 k for 0 # k , N. 
Consequently, the periodic extension xp(k) 5 x(k) for 0 # k , N and xp(k) extends x(k) 
periodically in both positive and negative directions. Using xp(k) leads to the following 
alternative form of convolution called circular convolution.
Periodic extension
Periodic ramp
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

106    Chapter 2  Discrete-time Systems in the Time Domain
Let h(k) and x(k) be N-point signals, and let xp(k) be the periodic extension of x(k). Then 
the circular convolution of h(k) with x(k) is denoted yc(k) 5 h(k) + x(k) and defined as
h(k) + x(k) 5
D  o
N21
i50
h(i)xp(k 2 i), 0 # k , N
Evaluating the periodic extension, xp(k 2 i), is equivalent to a counterclockwise cir-
cular shift of x(2i) by k samples as illustrated in Figure 2.23. Note how h(i) is distributed 
counterclockwise around the outer ring, while x(2i) is distributed clockwise around the 
inner ring. For different values of k, signal x(2i) gets rotated k samples counterclock-
wise. The circular convolution is just the sum of the products of the N points distributed 
around the circle.
It is possible to represent circular convolution using matrix multiplication. Let h 
and yc be N 3 1 column vectors containing the samples of h(k) and yc(k) 5 h(k) + x(k)
, respectively.
 
h  5 fh(0), h(1), . . . , h(N 2 1)gT
 
 (2.7.8a)
 
yc 5 fyc(0), yc(1), . . . , yc(N 2 1)gT 
 (2.7.8b)
Since circular convolution is a linear transformation from h to yc, it can be represented 
by an N 3 N matrix C(x). Consider, in particular, the following matrix that corresponds 
to the case N 5 5.
 
C(x) 53
x(0)
x(4)
x(3)
x(2)
x(1)
x(1)
x(0)
x(4)
x(3)
x(2)
x(2)
x(1)
x(0)
x(4)
x(3)
x(3)
x(2)
x(1)
x(0)
x(4)
x(4)
x(3)
x(2)
x(1)
x(0)4
 
 (2.7.9)
DEFiNiTioN
2.4 Circular Convolution
h(2)
h(1)
x(1)
x(2)
x(3)
x(4)
x(5)
x(6)
x(7)
x(0)
h(0)
h(7)
h(6)
h(5)
h(4)
h(3)
Figure 2.23:  
Counterclockwise 
Circular Shift of 
x(2i) by k 5 2 
with N 5 8 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.7 Convolution    107
Observe that the columns of the circular convolution matrix C(x) are just downward rota-
tions of x(i). Using (2.7.8) and (2.7.9), the circular convolution in Definition 2.4 can be 
expressed in vector form as
 
yc 5 C(x)h 
 (2.7.10)
Note from (2.7.9) that if the input x(k) is selected with some care, the circular convo-
lution matrix C(x) will be nonsingular, in which case h can be recovered from yc using 
h 5 C21(x)yc.
Circular convolution 
matrix
Circular Convolution
EXAMPLE 2.14
To illustrate how to compute circular convolution using the matrix formulation, 
consider the following two finite signals of length N 5 3.
h 5 f2, 21, 6gT
x 5 f5, 3, 24gT
From (2.7.9), the 3 3 3 circular convolution matrix C(x) is
C(x) 53
5
24
3
3
5
24
24
3
54
Using (2.7.10), if yc(k) 5 h(k) + x(k), then
yc 5 C(x)h
 53
5
24
3
3
5
24
24
3
543
2
21
64
 5 f32, 223, 19gT
2.7.3 Zero Padding
A high-speed version of circular convolution will be developed in Chapter 4 using the 
fast Fourier transform (FFT). However, without some preprocessing a direct application 
of circular convolution will not produce the zero-state response of a linear discrete-time 
system, even when it is an FIR system with a finite input. To see this, note from the Defi-
nition 2.4 that if we evaluate yc(k) for k $ N we find that circular convolution is periodic 
with a period of N.
 
yc(k 1 N) 5 yc(k) 
 (2.7.11)
Since the zero-state response is not, in general, periodic it follows that circular convolu-
tion produces a different response than linear convolution. Fortunately, there is a simple 
preprocessing step that can be performed that allows us to achieve linear convolution 
using circular convolution. To keep the formulation general, let h(k) be an L-point signal, 
and let x(k) be an M-point signal. Suppose we construct two new signals, each of length 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

108    Chapter 2  Discrete-time Systems in the Time Domain
N 5 L 1 M 2 1, by using zero padding. In particular, we can pad h(k) with M 2 1 zeros 
and x(k) with L 2 1 zeros.
 
hz 5 fh(0), h(1), . . . , h(L 2 1),  0, . . . , 0]T 
 (2.7.12a)
 
xz 5 fx(0), x(1), . . . , x(M 2 1), 0, . . . , 0]T 
 (2.7.12b)
Thus hz and xz are zero-padded vectors of length N 5 L 1 M 2 1. Next, consider the 
circular convolution of hz(k) with xz(k). If xzp(k) is the periodic extension of xz(k), then
 yc(k) 5 hz(k) + xz(k)
 5 o
N21
i50
hz(i)xzp(k 2 i)
 
 5 o
L21
i50
hz(i)xzp(k 2 i), 0 # k , N 
 (2.7.13)
Since 0 # i , L and 0 # k , N, the minimum value for k 2 i at k 5 0 and i 5 L 2 1 is 
2(L 2 1). But xz(i) has L 2 1 zeros padded to the end of it. Therefore, xzp(2i) 5 0 for 
0 # i , L. This means that xzp(k 2 i) in (2.7.13) can be replaced by xz(k 2 i). The result 
is then the linear convolution of h(k) with x(k) for 0 # k , N.
 
hz(k) + xz(k) 5 h(k) w x(k), 0 # k , N  
 (2.7.14)
In summary, linear convolution can be achieved by computing the circular convolu-
tion of zero-padded versions of the two signals. Consequently, the zero-state response 
can be computed using circular convolution. The following example illustrates this 
technique.
Zero padding
¸˝˛
M 2 1
¯˘˙
L 2 1
Zero Padding
EXAMPLE 2.15
Consider the signals from Example 2.14. In this case L 5 3, M 5 3, and 
N 5 L 1 M 2 1 5 5. The zero-padded signals are
 
hz 5 f2, 21, 6, 0, 0gT
 
xz 5 f5, 3, 24, 0, 0gT
From (2.7.9), the 5 3 5 circular convolution matrix C(x) is
 C(x) 53
5
0
0
24
3
3
5
0
0
24
24
3
5
0
0
0
24
3
5
0
0
0
24
3
54
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.7 Convolution    109
The function f_conv implements a fast form of convolution based on the FFT as 
discussed in Chapter 4. The relationship between linear and circular convolution is sum-
marized in Table 2.3.
The linear and the circular convolutions of finite discrete-time signals h and x can be 
computed using the DSP Companion function f_conv as follows.
% F_CONV: Fast linear or circular convolution
%
% Usage:
% 
y 
= f_conv (h,x,circ)
% Pre:
% 
h 
= vector of length L
% 
x 
= vector of length M
% 
circ = optional convolution type code (default: 0)
%
% 
0 = linear convolution
% 
1 = circular convolution (requires M = L)
% Post:
% 
y =  vector of length L+M-1 containing the
% 
convolution of h with x. If circ=1, x and
% 
y are of length L.
% Note:
% 
If h is the impulse response of a discrete-time
% 
linear system and x is the input, then y is the
% 
zero-state response when circ = 0.
DSP Companion
DSP Companion
Property 
Equation 
Linear convolution 
h(k) w x(k) 5o
k
i 5 0
 
h(i)x(k 2 i) 
Circular convolution 
h(k) + x(k) 5o
N21
i 5 0
  
h(i)xp(k 2 i) 
Zero padding 
h(k) w x(k) 5 hz(k) + xz(i) 
Table 2.3: Linear and 
Circular Convolution 
Thus from (2.7.10) and (2.7.14), the linear convolution of h(k) with x(k) is
 y 5 C(xz)hz
 53
5
0
0
24
3
3
5
0
0
24
24
3
5
0
0
0
24
3
5
0
0
0
24
3
543
2
21
6
0
04
 5 f10, 1, 19, 22, 224gT
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

110    Chapter 2  Discrete-time Systems in the Time Domain
2.7.4 Deconvolution
There are applications where one knows the impulse response h(k) and the output y(k), and 
the objective is to reconstruct the input x(k). The process of finding the input x(k) given the 
impulse response h(k) and the output y(k) is referred to as deconvolution. Deconvolution 
also includes finding the impulse response h(k), given the input x(k) and the output y(k). 
This is a special case of a more general problem called system identification, a topic that is 
discussed in Chapter 3 and Chapter 9. When h(k) and x(k) are both causal noise-free signals, 
recovery of h(k) is relatively simple. Suppose the input x(k) is chosen such that x(0) Þ 0. 
Evaluating (2.7.5) at k 5 0 then yields y(0) 5 h(0)x(0) or
 
h(0) 5 y(0)
x(0) 
 (2.7.15)
Once h(0) is known, the remaining samples of h(k) can be obtained recursively. For exam-
ple, evaluating (2.7.5) at k 5 1 yields
 
y(1) 5 h(0)x(1) 1 h(1)x(0) 
 (2.7.16)
Solving (2.7.16) for h(1), we then have
 
h(1) 5 y(1) 2 h(0)x(1)
x(0)
 
 (2.7.17)
This process can be repeated for 2 # k , N. The expression for the general case is obtained 
by decomposing the sum in (2.7.5) into the i , k terms and the i 5 k term. Solving for h(k), 
we then arrive at the following recursive formulation of the impulse response.
 
h(k) 5
1
x(0) 3y(k) 2 o
k21
i50
h(i)x(k 2 i)4, k $ 1  
 (2.7.18)
With the initialization in (2.7.15), the formulation in (2.7.18) solves the deconvolution 
problem, thereby reconstructing the impulse response from the input and the output. The 
following example illustrates this technique.
Deconvolution
Deconvolution
EXAMPLE 2.16
Consider the signals from Example 2.14. In this case both h(k) and x(k) are of 
length N 5 3 with
 
h 5 f2, 21, 6gT
 
x 5 f5, 3, 24gT
Let y(k) 5 h(k) w x(k). Circular convolution with zero padding was used in 
Example 2.15 to find the following zero-state response.
 
y 5 f10, 1, 19, 22, 224gT
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.7 Convolution    111
2.7.5 Polynomial Arithmetic
Linear convolution has a simple and useful interpretation in terms of polynomial arith-
metic. Suppose a(z) and b(z) are polynomials of degree L and M, respectively.
 
a(z) 5 a0 zL 1 a1zL 2 1 1. . .1 aL  
(2.7.19a)
 
b(z) 5 b0zM 1 b1zM 2 1 1. . .1 bM 
 (2.7.19b)
Thus the coefficient vectors a and b are of length L 1 1 and M 1 1. Next let c(z) be the 
following product polynomial.
 
c(z) 5 a(z)b(z) 
 (2.7.20)
In this case c(z) is of degree N 5 L 1 M, and its coefficient vector c is of length L 1 M 1 1. 
That is,
 
c(z) 5 c0zL 1 M 1 c1zL 1 M 2 1 1. . .1 cL 1 M 
 (2.7.21)
The coefficient vector of c(z) can be obtained directly from the coefficient vectors of 
a(z) and b(z) using linear convolution as follows.
 
c(k) 5 a(k) w b(k), 0 # k , L 1 M 1 2 
 (2.7.22)
Thus linear convolution is equivalent to polynomial multiplication. Since deconvolution 
allows us to recover a(k) from b(k) and c(k), deconvolution is equivalent to polynomial 
division.
Polynomial  
multiplication, division
To recover h(k) from x(k) and y(k) using deconvolution, we start with (2.7.15). The 
first sample of the impulse response is h(0) 5 y(0)yx(0) 5 2. Next, applying (2.7.18) 
with k 5 1 yields
 h(1) 5 y(1) 2 h(0)x(1)
x(0)
 5 1 2 2(3)
5
5 21
Finally, applying (2.7.18) with k 5 2 then yields
 h(2) 5 y(2) 2 h(0)x(2) 2 h(1)x(1)
x(0)
 5 19 2 2(24) 2 (21)3
5
 5 6
Thus the impulse response vector is
h 5 f2, 21, 6gT ✓
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

112    Chapter 2  Discrete-time Systems in the Time Domain
Polynomial Multiplication
EXAMPLE 2.17
To illustrate the relationship between linear convolution and polynomial arithme-
tic, consider the following two polynomials.
 
a(z) 5 2z2 2 z 1 6
 
b(z) 5 5z2 1 3z 2 4
In this case the coefficient vectors are a 5 f2, 21, 6gT and b 5 f5, 3, 24gT. Sup-
pose c(k) 5 a(k) w b(k). Then from Example 2.15
c 5 f10, 1, 19, 22, 224gT
Thus the product of a(z) with b(z) is the polynomial of degree four whose coeffi-
cient vector is given by c. That is,
c(z) 5 a(z)b(z)
 5 10z4 1 z3 1 19z2 1 22z 2 24
Using direct multiplication of a(z) times b(z) one can verify this result.
 c(z) 55
10z4
25z3
130z2
6z3
23z2
118z
28z2
14z
224
10z4
1z3
119z2
122z
2246
MATLAB Functions 
MATLAB functions
The deconvolution of two finite discrete-time signals h and x can be computed using 
the MATLAB function deconv as follows.
% DECONV: Perform linear deconvolution
%
% Usage:
% 
[h,r] = deconv (y,x);
% Pre:
% 
y = vector of length N containing output
% 
x = vector of length M < N containing input
% Post:
% 
 h =  quotient vector of length N containing impulse 
% 
 
response
% 
r = remainder vector of length N-1
% Note:
% 
 If y and x represent the coefficients of polynomials, 
% 
then h contains the coefficients of the quotient
% 
 polynomial and r contains the coefficients of the 
% 
remainder polynomial.
% 
y(z) = h(z)x(z) + r(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.8 Correlation    113
Correlation
2.8.1 Linear Cross-correlation
Next we turn our attention to correlation, an operation closely related to convolution.
Let y(k) be an L-point signal and let x(k) be an M-point signal where M # L. Then the 
linear cross-correlation of y(k) with x(k) is denoted ryx(i) and defined
ryx(k) 5 
D 1
Lo
L21
i50
y(i)x(i 2 k), 0 # k , L
Since x(k) is causal, the lower limit of the sum in Definition 2.5 can be set to i 5 k. 
The variable k is sometimes called the lag variable because it represents the number 
of samples that x(i) is shifted to the right, or delayed, before the sum of products is 
computed.
Definition 2.5 indicates how to compute the cross-correlation of two determinis-
tic discrete-time signals. Linear cross-correlation is sometimes defined without the scale 
factor, 1yL. A scale factor is included in Definition 2.5 because this way Definition 2.5 
is consistent with an alternative statistical definition of the cross-correlation of two ran-
dom signals. The cross-correlation of a pair of random signals is introduced in Chapter 
9 using the expected value operator. The formulation in Definition 2.5 applies to finite 
causal signals. For practical computations, this is the most important special case. How-
ever, it is possible to extend the definition of correlation to noncausal signals and to 
infinite signals (power signals) by extending the lower and upper limits, respectively, of 
the summation in Definition 2.5.
Just as was the case with convolution, there is a matrix formulation of cross- 
correlation. Let y and ryx be L 3 1 column vectors containing the samples of y(k) and 
ryx(k), respectively.
 
y  5 fy(0), y(1), . . . , y(L 2 1)gT
 
 (2.8.1a)
 
ryx 5 fryx(0), ryx(1), . . . , ryx(L 2 1)gT 
 (2.8.1b)
Cross-correlation is a linear transformation from y to ryx. Consequently, it can be rep-
resented by an L 3 L matrix D(x). Consider, in particular, the following matrix which 
corresponds to the case where L 5 5 and M 5 3.
 
D(x) 5 1
5 3
x(0)
x(1)
x(2)
0
0
0
x(0)
x(1)
x(2)
0
0
0
x(0)
x(1)
x(2)
0
0
0
x(0)
x(1)
0
0
0
0
x(0)4
 
 (2.8.2)
Note how the rows of D(x) are constructed by shifting x(k) to the right. However, unlike 
the circular convolution matrix in (2.7.9), when the samples of x get shifted off the right 
end of D(x), they do not wrap around and reappear on the left end. Using (2.8.1), (2.8.2), 
2.8
DEFiNiTioN
2.5 Linear 
Cross-correlation
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

114    Chapter 2  Discrete-time Systems in the Time Domain
and Definition 2.5, the linear cross-correlation of y(k) with x(k) can be expressed in vec-
tor form as
 
ryx 5 D(x)y 
 (2.8.3)
Observe from (2.8.2) that if x(0) Þ 0, then the cross-correlation matrix D(x) is non-
singular, which means signal y(k) can be recovered from the cross-correlation using 
y 5 D21(x)ryx.
Linear cross-correlation can be used to measure the degree to which the shape of 
one signal is similar to the shape of another signal. The following example illustrates this 
point.
Signal shape
Linear Cross-correlation
EXAMPLE 2.18
To demonstrate how linear cross-correlation can be computed using the matrix 
formulation, consider the following pair of discrete-time signals.
x 5 f0, 2, 1, 2, 0gT
y 5 f4, 21, 22, 0, 4, 2, 4, 0, 22, 2gT
Here L 5 10 and M 5 5. A plot of the two signals y(k) and x(k) is shown in Figure 2.24.  
Note how the graph of x(k) is a flattened “M” shaped signal. Furthermore, the 
longer signal y(k) contains a scaled and translated version of x(k) with a larger 
“M” starting at k 5 3. To verify that y(k) contains a scaled and shifted version of 
Figure 2.24: Two Finite Discrete-time Signals 
0
1
2
3
4
5
6
7
8
9
23
22
21
0
1
2
3
4
5
k
Signals
x
y
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.8 Correlation    115
x(k), we compute the linear cross-correlation. From (2.8.2) and (2.8.3), the linear 
cross-correlation of y(k) with x(k) is
ryx 5 D(x)y
 5 1
10 3
0
2
1
2
0
0
0
0
0
0
0
0
2
1
2
0
0
0
0
0
0
0
0
2
1
2
0
0
0
0
0
0
0
0
2
1
2
0
0
0
0
0
0
0
0
2
1
2
0
0
0
0
0
0
0
0
2
1
2
0
0
0
0
0
0
0
0
2
1
2
0
0
0
0
0
0
0
0
2
1
0
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
0
043
4
21
22
0
4
2
4
0
22
24
 5 f20.4, 0.4, 0.8, 1.8, 0.8, 0.4, 0.2, 20.2, 0.4, 0.0gT
In this case ryx(k) reaches its maximum value at a lag of k 5 3. This is evident 
from the plot of ryx(k) shown in Figure 2.25. The fact that ryx(k) has a clear peak 
at k 5 3 indicates there is a strong positive correlation between y(k) and x(k 2 3). 
Indeed, except for scaling, the two signals exactly match in this case.
y(k) 5 2x(k 2 3), 3 # k , 3 1 M
Thus a dominant peak in ryx(k) at k 5 p is an indication that a signal similar to 
x(k) is present in signal y(k) starting at k 5 p.
Figure 2.25: Linear Cross-correlation of the Signals in Figure 2.24
0
1
2
3
4
5
6
7
8
9
20.5
0
0.5
1
1.5
2
i
ryx(i)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

116    Chapter 2  Discrete-time Systems in the Time Domain
Although the cross-correlation in Definition 2.5 can be used to detect the presence of 
one signal in another signal, it does suffer from a practical drawback. When y(k) contains 
a scaled and shifted version of x(k), the cross-correlation will contain a distinct peak. 
However, the height of the peak will depend on the data in y(k) and x(k). For example, 
scaling y(k) or x(k) by a will scale the height of the peak by a. Consequently, one is left 
with the question, how high does the peak have to be for a significant correlation to 
exist? A simple way to solve this problem is to develop a normalized version of cross- 
correlation. It can be shown (Proakis and Manolakis, 1988), that the square of the cross- 
correlation is bounded from above in the following way.
 
r2
yx(k) #1
M
L2 rxx(0)ryy(0), 0 # k , L 
 (2.8.4)
By making use of (2.8.4), we can introduce the following scaled version of cross-correlation  
called the normalized linear cross-correlation of y(k) with x(k).
 
yx(k) 5
D  
ryx(k)
Ï(MyL)rxx(0)ryy(0), 0 # k , L  
 (2.8.5)
By construction, the magnitude of the normalized cross-correlation is bounded by B 5 1. 
That is, the normalized cross-correlation is guaranteed to lie within the following interval.
 
21 # yx(k) # 1, 0 # k , L 
 (2.8.6)
If the normalized cross-correlation is used, then any peak that begins to approach 
the maximum value of one indicates a very strong positive correlation between y(k) and 
x(k), regardless of whether one of the signals, is very small or very large in comparison 
with the other. For the cross-correlation example shown in Figure 2.25, the peak of the 
normalized cross-correlation is yx(3) 5 0.744, indicating there is a strong positive cor-
relation between y(k) and x(k 2 3). It is also possible to compute the cross-correlation of 
a signal x(k) with itself. This is referred to as the auto-correlation, rxx(k).
2.8.2 Circular Cross-correlation
Practical cross-correlations often involve long signals, so it is helpful to develop a numer-
ical implementation of linear cross-correlation that is more efficient than the direct 
method in Definition 2.5. Recall that linear convolution can be achieved by using circular 
convolution with zero padding. A similar approach can be used with cross-correlation.
Let y(k) and x(k) be N-point signals, and let xp(k) be the periodic extension of x(k). 
The circular cross-correlation of y(k) with x(k) is denoted cyx(k) and defined
cyx(k) 5 
D 1
No
N21
i50
y(i)xp(i 2 k), 0 # k , N
Circular cross-correlation operates on two signals of the same length. Compar-
ing Definition 2.6 with Definition 2.5, we see that for circular cross-correlation x(k) is 
replaced by its periodic extension xp(k). The effect of this change is to replace the lin-
ear shift of x(k) with a circular shift or rotation of x(k), hence the name circular cross- 
correlation. A diagram illustrating circular cross-correlation for the case N 5 8 and k 5 2 
is shown in Figure 2.26. Note that evaluating xp(i) at i 2 k is equivalent to a clockwise 
Normalized linear 
cross-correlation
Auto-correlation
DEFiNiTioN
2.6 Circular 
Cross-correlation
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.8 Correlation    117
circular shift of x(i) by k samples as shown in Figure 2.25. Observe how y(i) is distrib-
uted counterclockwise around the outer ring, while x(i) is distributed counterclockwise 
around the inner ring. For different values of the lag k, the signal xp(i 2 k) gets rotated k 
samples clockwise. The circular cross-correlation is just the sum of the products of the N 
points distributed around the circle.
Just as was the case with linear cross-correlation, we can scale cyx(k) to produce the 
following normalized circular cross-correlation whose value is restricted to the interval 
f21, 1g (see Problem 2.40).
 
yx(k) 5
cyx(k)
Ïcxx(0)cyy(0)  
 (2.8.7)
There are a number of useful properties of circular cross-correlation. For example, 
consider the effect of interchanging the roles of y(k) and x(k). Note from Definition 2.6 
that y(k) can be replaced by its periodic extension yp(k) without affecting the result. Con-
sequently, using the change of variable q 5 i 2 k we have
cxy(k) 5 1
No
N21
i50
x(i)yp(i 2 k)
 5 1
No
N21
i50
xp(i)yp(i 2 k)
 5 1
N o
N212k
q52k
xp(q 1 k)yp(q) j q 5 i 2 k
 5 1
No
N21
q50
yp(q)xp(q 1 k)
 
 5 1
No
N21
q50
y(q)xp(q 1 k)
 
 (2.8.8)
Normalized circular 
cross-correlation
y(2)
y(1)
x(3)
x(2)
x(1)
x(0)
x(7)
x(6)
x(5)
x(4)
y(0)
y(7)
y(6)
y(5)
y(4)
y(3)
Figure 2.26:  
Clockwise Circular 
Shift of x(i) by 
k 5 2 with N 5 8 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

118    Chapter 2  Discrete-time Systems in the Time Domain
Observe that the summations in (2.8.8) all extend over one period. Consequently, the 
starting sample of the sum can be changed from q 5 2k to q 5 0 without affecting the 
result. From Definition 2.6, the last line of (2.8.8) is cyx(2k). Thus we have the following 
symmetry property of circular cross-correlation which says that changing the order of y 
and x is equivalent to changing the sign of the lag variable.
 
cxy(k) 5 cyx(2k), 0 # k , N 
 (2.8.9)
There is a simple and elegant relationship between circular cross-correlation and cir-
cular convolution. Comparing the expression in Definition 2.6 with that in Definition 2.5, 
observe that the circular cross-correlation of y(k) with x(k) is just a scaled version of the 
circular convolution of y(k) with x(2k). That is,
 
cyx(k) 5 y(k) + x(2k)
N
, 0 # k , N  
 (2.8.10)
With the use of zero padding, linear cross-correlation can be achieved using circular 
cross-correlation. Suppose y(k) is an L-point signal and x(k) is an M-point signal with 
M # L. Let yz(k) be a zero-padded version of y(k) with M 1 p zeros appended where 
p $ 21. Similarly, let xz(k) be a zero-padded version of x(k) with L 1 p zeros. Therefore 
yz and xz are both signals of length N 5 L 1 M 1 p.
 
xz 5 fx(0), . . . , x(M 2 1), 0, . . . , 0]T 
 (2.8.11a)
 
yz 5 f y(0), . . . , y(L 2 1), 0, . . . , 0]T 
 (2.8.11b)
Next, let xzp be the periodic extension of xz(k) as in (2.7.7), and consider the circular 
cross-correlation of yz(k) with xz(k).
 
cyzxz(k) 5 1
No
N21
i50
yz(i)xzp(i 2 k), 0 # k , N 
 (2.8.12)
If we restrict cyzxz(k) to 0 # k , L, it can be shown to be proportional to the linear cross- 
correlation in Definition 2.5. In particular, recalling that y(k) is an L-point signal, we have
 
cyzxz(k) 5 1
No
L21
i50
yz(i)xzp(i 2 k), 0 # k , L 
 (2.8.13)
Since 0 # k , L, the minimum value for i 2 k is 2 (L 2 1). But xz(k) has L 1 p zeros 
padded to the end of it. Therefore xzp(2k) 5 0 for 0 # k # L 1 p. It follows that xzp(i 2 k) 
in (2.8.13) can be replaced by xz(i 2 k) as long as p $ 2 1. The result is then the linear 
cross-correlation of yz(k) with xz(k). But for 0 # k , L, the linear cross-correlation of 
yz(k) with xz(k) is identical to the linear cross-correlation of y(k) with x(k), except for a 
scale factor of LyN. Consequently,
 
ryx(k) 51
N
L2cyzxz(k), 0 # k , L  
 (2.8.14)
The relationship between linear and circular cross-correlation and the properties of 
cross-correlation are summarized in Table 2.4.
Symmetry property
¸˝˛
L 1 p
¯˘˙
M 1 p
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.9 Stability in the Time Domain    119
Stability in the Time Domain
Practical discrete-time systems, particularly digital filters, tend to have one qualitative 
feature in common: they are stable. Recall from Section 2.2 that a signal x(k) is bounded 
if and only if ux(k)u # Bx for some finite bound Bx . 0.
2.9
Property 
Equation 
Linear cross-correlation 
ryx(k) 5 1
L o
L21
i 5 0
y(i)x(i 2 k) 
Circular cross-correlation 
cyx(k) 5 1
N o
N21
i50
  
y(i)xp(i 2 k) 
Time reversal 
cyx(2k) 5 cxy(k) 
Circular convolution 
cyx(k) 5 y(k) + x(2k)
N
 
Zero padding 
ryx(k) 51
N
L2cyzxz(k) 
Table 2.4:  
Linear and Circular 
Cross-correlation 
If the MATLAB signal processing toolbox is available, then the function xcorr can 
be used to perform linear cross-correlation. Alternatively, the DSP Companion con-
tains the following implementation of both linear and circular cross-correlation.
% F_CORR: Fast cross-correlation of two discrete-time signals
%
% Usage:
% 
r = f_corr (y,x,circ,norm)
% Pre:
% 
y 
= vector of length L containing first signal
% 
x 
= vector of length M <= L containing second signal
% 
circ = optional correlation type code (default 0):
%
% 
0 = linear correlation
% 
1 = circular correlation
%
% 
norm = optional normalization code (default 0):
%
% 
0 = no normalization
% 
1 = normalized cross-correlation
% Post:
% 
r = vector of length L contained selected cross-
% 
correlation of y with x.
% Notes:
% 
To compute auto-correlation use x = y.
DSP Companion
DSP Companion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

120    Chapter 2  Discrete-time Systems in the Time Domain
A discrete-time system is bounded-input bounded-output (BIBO) stable if and only if 
every bounded input produces a bounded output. Otherwise the system is unstable.
The stability of a discrete-time system can be determined directly from the impulse 
response h(k). Suppose the input x(k) is bounded by a bound Bx. Recall that the output 
is the convolution of the input with the impulse response. Then from (2.7.5) and the 
inequalities in Appendix 2, we have
uy(k)u 5 uo
k
i50
h(i)x(k 2 i)u
 # o
k
i50
uh(i)x(k2i)u
 
 5 o
k
i50
uh(i)u ? ux(k 2 i)u
 # Bxo
k
i50
uh(i)u
 
 # Bx o
`
i52`
uh(i)u
 
 (2.9.1)
It follows from (2.9.1) that the system is BIBO stable if the infinite series on the right-
hand side converges to a finite value. This condition is not only sufficient for BIBO sta-
bility, it is also necessary (see Problem 2.43). This leads to the following fundamental 
stability result.
A linear time-invariant discrete-time system is BIBO stable if and only if the impulse 
response h(k) is absolutely summable.
uuhuu1 5 o
`
k52`
uh(k)u , `
There is an important class of discrete-time systems that is always stable. Consider 
an FIR system.
 
y(k) 5 o
m
i50
bix(k 2 i)  
 (2.9.2)
Recall from (2.6.3) that the impulse response is h 5 hb0, b1, . . . , bm, 0, 0, . . . j. It follows 
that for an FIR system, the impulse response is absolutely summable.
 
uuhuu1 5 o
m
i50
ubiu 
(2.9.3)
Therefore FIR systems are always BIBO stable. This is one of the features that make FIR 
systems useful candidates for digital filters (Chapter 6), and particularly for adaptive digi-
tal filters (Chapter 9). The more general IIR systems, by contrast, can be stable or unstable.
DEFiNiTioN
2.7 BiBo Stable
PRoPoSiTioN
2.1 Stability in the Time 
Domain
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.10 GUI Modules and Case Studies     121
GUi Modules and Case Studies 
This section focuses on applications of discrete-time systems. Graphical user interface 
modules called g_systime and g_correlate are introduced that allow user to explore the 
input-output behavior of linear discrete-time systems in the time domain and perform 
cross-correlations and convolutions without any need for programming. Case study pro-
gramming examples using the DSP Companion functions are then presented.
g_systime: Discrete-time System Analysis in the Time Domain 
The DSP Companion includes a GUI module called g_systime that allows the user to 
explore the input-output behavior of linear discrete-time systems in the time domain. GUI 
module g_systime features a display screen with tiled windows as shown in Figure 2.27.  
The design features of g_systime are summarized in Table 2.5.
2.10
GUi module
BiBo Stability: Time Domain
EXAMPLE 2.19
Consider the home mortgage system introduced in Section 2.1, where k denotes 
the month and the fraction r . 0 represents the annual interest rate.
y(k) 511 1 r
122 y(k 2 1) 2 x(k)
First we compute the impulse response using (2.6.6). By inspection, the charac-
teristic polynomial is
a(z) 5 z 2 p1
 p1 5 1 1 r
12
Hence the form of the impulse response is
 h(k) 5 d0(k) 1 d1pk
1(k)
The polynomial associated with the input terms is b(z) 5 2z. Using (2.6.7) with 
p0 5 0, the coefficient vector d [ R2 is
d0 5 0
 d1 5 21
p1
Hence the impulse response of the home mortgage system is
h(k) 5 2(1 1 ry12)k 2 1(k)
Since r . 0, the samples of h(k) grow with time, so h(k) is clearly not absolutely 
summable. It follows that the home mortgage system is BIBO unstable.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

122    Chapter 2  Discrete-time Systems in the Time Domain
Select Type
Select View
Slider Bar
22
21
0
1
2
22
21
0
1
2
X
X
O
O
Polynomial Roots: ‘x’ 5 a(z), ‘o’ 5 b(z)
Re(z)
Im(z)
22
21
0
1
2
22
0
2
240
220
0
20
Re(z)
Magnitude of b(z)/a(z)
Im(z)
|b(z)/a(z)| (dB)
Edit Parameters
0
0.2
0.4
0.6
0.8
1
0
0.5
1
g_systime
x(k)
y(k)
S
Figure 2.27: Display Screen for GUI Module g_systime
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.10 GUI Modules and Case Studies     123
Table 2.5: Features of 
GUI Module g_systime
item 
Variables 
Block diagram 
x(k), y(k) 
Edit parameters 
a, b, y0, fs, c, F0 
Input type 
constant, unit impulse, unit step, damped cosine, white noise, record 
x, import 
Plot view 
time signals, polynomial roots, convolution, deconvolution 
Slider 
number of samples N 
Push buttons 
play x, play y 
Check boxes 
line/stem plot, linear/dB scale 
Menu buttons 
export, caliper, print, help, exit 
Import 
a, b, x, fs 
Export 
a, b, x, y, fs 
The Block diagram window in the upper-left corner of the screen contains a color-
coded block diagram of the system under investigation. Below the block diagram are a 
number of edit boxes whose contents can be modified by the user. The edit boxes for a 
and b allow the user to select the coefficients of the characteristic polynomial, a(z), and 
input polynomial, b(z), of the following difference equation where a0 5 1.
 
y(k) 5 o
m
i50
bix(k 2 i) 2 o
n
i51
aiy(k 2 i), y0 [ Rn 
 (2.10.1)
The initial condition vector y0 5 f y( 2 1), Á , y( 2 n)gT, and the coefficient vectors, a 
and b, can be edited directly by clicking on the shaded area and entering in new values. 
Any MATLAB statement defining a, b, or y0 can be entered in the box. For example, the 
edit box can be cleared and then the MATLAB function poly can be used to compute a 
coefficient vector from a vector of desired roots as follows.
a = poly ([.7*j, -.7*j, .9, -.4]); 
The Enter key is used to activate a change to a parameter. Additional scalar parameters 
that appear in edit boxes are associated with the causal exponential and damped causal 
cosine inputs.
 
x(k) 5 ck(k) 
 (2.10.2)
 
x(k) 5 ck cos (2F0kT)(k) 
 (2.10.3)
They include the sampling frequency fs, the exponential damping factor c, which is con-
strained to lie the interval f21, 1g, and the input frequency 0 # F0 # fsy2. The Parameters 
window also contains two push-button controls. The push-button controls play the sig-
nals x(k) and y(k) on the PC speaker using the current sampling rate. This option allows 
the user to hear the filtering effects of the system S on various types of inputs.
The Type and View windows in the upper-right corner of the screen allow the user to 
select both the type of input signal and the viewing mode. The inputs include the zero input, 
the unit impulse, the unit step, a damped cosine input, white noise uniformly distributed over 
f21, 1g, recorded sounds from a PC microphone, and user-defined inputs imported from 
MAT files. The Recorded sound option can be used to record up to one second of sound at a 
sampling rate of 8192 Hz. For the Import option, a MAT file containing the input vector x, 
the sampling frequency fs, and the coefficient vectors a and b must be supplied by the user.
View options include time plots of the input x(k) and output y(k), the roots of the char-
acteristic and input polynomials a(z) and b(z), polynomial multiplication using convolution, 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

124    Chapter 2  Discrete-time Systems in the Time Domain
and polynomial division using deconvolution. The time plots use continuous time or discrete 
time depending on the status of the Stem plot check-box control. The sketch of the roots of 
a(z) and b(z) also includes a surface plot of ub(z)ya(z)u which can be linear or in decibels (dB) 
as 20 log10(ub(z)ya(z)u). The Plot window on the bottom half of the screen shows the selected 
view. The curves are color-coded to match the block diagram labels. The slider bar below the 
Type and View window allows the user to change the number of samples N.
The Menu bar at the top of the screen includes several menu options. The Export 
option is used to save a, b, x, y, fs to a MAT file. Results exported from one GUI module 
can be imported into other GUI modules for additional processing. The Caliper option 
allows the user to measure any point on the current plot by moving the mouse cross hairs 
to that point and clicking. The Print option sends the GUI window to a printer or a file. 
Finally, the Help option provides the user with some helpful suggestions on how to effec-
tively use module g_systime.
g_correlate: Correlation and Convolution 
The DSP Companion includes a GUI module called g_correlate that allows the user to 
explore cross-correlations and convolutions of pairs of signals and auto-correlations of 
individual signals. GUI module g_correlate features a display screen with tiled windows 
as shown in Figure 2.28. The design features of g_correlate are summarized in Table 2.6.
The upper-left corner of the screen contains a block diagram that specifies the oper-
ation being performed. The signals are color-coded, and the labels change depending on 
the signal processing operation selected.
The Parameters window below the block diagram contains edit boxes. Parameters L, 
M, c, and fs can be edited directly by the user. Parameters L and M are the lengths of 
the input signals y(k) and x(k), respectively, with M , L. Parameter c represents a scale 
factor that is used to add a scaled and shifted version of x(k) to y(k) so that its presence 
can be detected using cross-correlation. Parameter fs is the sampling rate. Changes to 
parameter values are activated with the Enter key. The Parameters window also includes 
push button and check box controls. The push button controls play the signals x(k) and 
y(k) on the PC speaker using a using the current sampling rate.
The Type and View windows in the upper-right corner of the screen allow the user 
to select both the type of input signal, and the viewing mode. The inputs include white 
noise inputs, periodic inputs, a periodic y with a synchronized impulse train for x, inputs 
recorded from a PC microphone, and user-defined inputs imported from a MAT file. For 
GUi Module
Table 2.6: Features of 
GUI Module g_correlate 
item 
Variables 
Block diagram 
x(k), y(k), yx(k) 
Edit parameters 
c, fs, L, M 
Input type 
white noise, periodic, impulse train record x and y, import 
Plot view 
inputs x and y, convolution cross-correlation, auto-correlation 
Slider 
number of samples of delay d 
Push buttons 
play x, play y 
Check boxes 
linear/circular, regular/normalized 
Menu buttons 
export, caliper, print, help, exit 
Import 
x, y, fs 
Export 
a, b, x, y, fs
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.10 GUI Modules and Case Studies     125
Select Type
Select View
Slider Bar
Edit Parameters
0
500
1000
1500
2000
2500
3000
3500
4000
4500
20.1
0
0.1
0.2
0.3
0.4
Normalized Cross-correlation: White Noise Input
k
0
0.2
0.4
0.6
0.8
1
0
0.5
1
x(k)
y(k)
Linear
cross−
correlation
yx(k)
yx(k)
Figure 2.28: Display Screen for GUI Module g_correlate
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

126    Chapter 2  Discrete-time Systems in the Time Domain
the recorded inputs, up to .5 second for x and 2 seconds for y can be recorded at 8192 Hz. 
The imported inputs are specified in a user-supplied MAT file containing fs and the 
vectors x and y. For the white noise inputs, the signal y(k) contains a scaled and delayed 
version of the signal x(k). That is, y(k) is computed as follows where xz(k) is a zero-ex-
tended version of x(k).
 
y(k) 5 xz(k) 1 v(k), 0 # k , L 
 (2.10.4)
The scale factor c in the Parameters window can be modified by the user. The delay d can 
be set anywhere between 0 and L using the horizontal slider bar that appears below the 
Type and View windows.
The View options include plots of x(k), y(k), the convolution of x(k) with y(k), the 
cross-correlation of y(k) with x(k), and the auto-correlation of y(k). There are two check 
boxes. The first check box allows the user to toggle between linear correlation or convo-
lution and circular correlation or convolution. The second check box allows the user to 
choose between regular and normalized cross-correlations and auto-correlations. The 
Plot window on the bottom half of the screen shows the selected view. The curves are 
color-coded to match the block diagram labels.
The Menu bar at the top of the screen includes several menu options. The Export 
option is used to save a, b, x, y, fs to a MAT file. Here a and b are the coefficient vectors 
of a running average filter. They are included for compatibility with other GUI modules 
so results exported from one GUI module can be imported into other GUI modules for 
additional processing. The Caliper option allows the user to measure any point on the 
current plot by moving the mouse crosshairs to that point and clicking. The Print option 
sends the Plot window to a printer or a file. Finally, the Help option provides the user 
with some helpful suggestions on how to effectively use module g_correlate.
Home Mortgage 
Recall from Section 2.1 that the following discrete-time system can be used to model a 
home mortgage loan.
y(k) 5 y(k 2 1) 11
r
122 y(k 2 1) 2 x(k)
Here y(k) is the balance owed at the end of month k, r is the annual interest rate expressed 
as a fraction, and x(k) is the monthly mortgage payment. One of the questions posed in Sec-
tion 2.1 was the following. If the size of the mortgage is y0 dollars and the duration of the 
mortgage is N months, what is the required monthly payment? Now that we have the neces-
sary tools in place, we can answer this and related questions. To streamline the notation, let
p1 5 1 1 r
12
Then the difference equation can be simplified to
y(k) 5 2p1y(k 2 1) 2 x(k)
The size of the mortgage, y0, enters as an initial condition, y(21) 5 y0. The charac-
teristic and input polynomials of this system are
a(z) 5 z 2 p1
b(z) 5 2z
Thus the form of the zero-input part of the response is
yzi(k) 5 c1 pk
1
Case Study 2.1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.10 GUI Modules and Case Studies     127
Setting y(21) 5 y0 yields c1yp1 5 y0. Thus the zero-input response arising from the initial 
condition is
yzi(k) 5 y0 pk11
1
, k $  21
Next we examine the zero-state response. Let A be the monthly payment. Then the 
input x(k) is a step of amplitude A. Note that this is a special case of the causal exponen-
tial input in (2.4.13) where the exponential factor is p0 5 1. Thus from (2.4.15) the form 
of the zero-state response is
yzs (k) 5 fd0 1 d1p k
1g(k)
Applying (2.4.16) with p0 5 1 the coefficient vector d [ R2 is
d0 5 A(21)
1 2 p1
d1 5 A(2p1)
p1 2 1
Thus the zero-state response produced by the input x(k) 5 A(k) is
 yzs(k) 5 A(1 2 pk11
1
)(k)
p1 2 1
Finally, the zero-input and zero-state responses can be combined to produce the complete response.
y(k)  5  yzi(k) 1 yzs(k)
 5  y0 pk11
1
1 A(1 2 pk11
1
)(k)
p1 2 1
, k $  21
The length of the mortgage is N months. Setting y(N) 5 0 and solving for A yields the 
required monthly payment.
A 5 y0(1 2 p1)pN11
1
1 2 pN11
1
The first part of case2_1 prompts the user for the interest rate and then computes 
the required monthly payment for loans of different sizes and durations. The results are 
shown in Figure 2.29.
0
50
100
150
200
250
300
0
500
1000
1500
2000
2500
3000
Size of Mortgage ($1000)
Monthly Payment ($)
15 year
20 year
30 year
Figure 2.29:  
Monthly Mortgage 
Payments for  
6% Interest
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

128    Chapter 2  Discrete-time Systems in the Time Domain
0
50
100
150
200
250
300
350
400
0
20
40
60
80
100
120
140
160
180
200
Month
Balance ($1000)
6A/r
Balance due
Figure 2.30:  
Balance Owed over 
the Duration of 
the Mortgage 
Another question that was asked in Section 2.1 was: how many months does it take  
before more than half of the monthly payment goes toward reducing the principal, rather than  
paying interest? The monthly interest is (ry12)y(k). Thus we require (ry12)y(k) , Ay2 or
y(k) , 6A
r
The smallest k that satisfies this inequality is the crossover month. The second half of 
case2_1 computes the zero-input response directly and uses the MATLAB function filter 
to compute the zero-state response corresponding to a $200,000 mortgage over 30 years. 
The resulting plot of the balance owed is shown in Figure 2.30. Note that the crossover 
month does not occur until month 250, well beyond the midpoint of the 30-year loan.
Echo Detection 
Recall from Section 2.1 that one of the applications of cross-correlation is for radar 
processing as shown previously in Figure 2.1. Here x(k) is the transmitted signal and 
y(k) is the received signal. First consider the transmitted signal. Suppose the sampling 
frequency is fs 5 1 MHz and the number of transmitted samples is M 5 512. One possi-
ble choice for the transmitted signal is a uniformly distributed white noise signal like the 
one used in Section 2.1. Another possibility a multi-frequency chirp, a sinusoidal  signal 
whose frequency varies with time. For example, let T 5 1yfs, and consider the following 
chirp signal with variable frequency f(k) for 0 # k , M.
f(k) 5
kfs
2(M 2 1)
x(k) 5 sinf2f (k)kTg
The received signal y(k) includes a scaled and delayed version of the transmitted 
signal plus measurement noise. Suppose the received signal consists of L 5 2048 samples. 
Crossover
Case Study 2.2
Chirp
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.10 GUI Modules and Case Studies     129
If xz(k) denotes the transmitted signal, zero-extended to L points, then the received signal 
can be expressed as follows.
y(k) 55
v(k),
0 # k , d
x(k 2 d) 1 v(k),
d # k , d 1 M
v(k),
d 1 M # k , d 1 L
The first term in y(k) is the echo of the transmitted signal that is reflected back from the 
illuminated target. Typically, the echo will be attenuated due the dispersion, so  V 1. In 
addition, the echo will be delayed by d samples due to the time it takes for the transmitted 
signal to travel to the target, bounce off, and return. The second term in y(k) is random 
atmospheric measurement noise picked up by the receiver. For example, suppose v(k) is 
white noise uniformly distributed over the interval f20.1, 0.1g.
To determine the range to the target, let c be the propagation speed of the transmitted 
signal. For radar applications, this corresponds to the speed of light or c 5 1.86282 3 105 
miles/s. The time of flight of the signal is then  5 dT sec. Multiplying  by the signal 
propagation speed and dividing by two for the round trip, we then arrive at the following 
expression for the range to the target.
r 5 cdT
2
Thus the key to finding the distance to the target is to detect the presence, and location, 
of the echo of the transmitted signal x(k) in the received signal y(k). This can be achieved 
by running case2_2 from the DSP Companion driver program.
When case2_2 is run it first constructs the chirp signal x(k). The resulting plot is shown 
in Figure 2.31. Note how the frequency changes with time. Function case2_2 then con-
structs the received signal y(k) and computes the normalized linear cross-correlation of y(k) 
with x(k). The echo of x(k) buried in y(k) becomes evident when we examine the normal-
ized cross-correlation plot shown in Figure 2.32. For this example,  5 0.02 and d 5 1304. 
Although the peak at yx(d) 5 .147 is not large due to the noise, it is distinct. Using the 
MATLAB function max to locate the peak, the value reported for the range to the target is
r 5 12.15 miles 
0
100
200
300
400
500
600
21.5
21
20.5
0
0.5
1
1.5
k
x(k)
Figure 2.31:  
The Transmitted 
Multi-frequency 
Chirp Signal x(k) 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

130    Chapter 2  Discrete-time Systems in the Time Domain
0
500
1000
1500
2000
2500
20.1
20.05
0
0.05
0.1
0.15
k
yx(k)
Figure 2.32:  
Normalized Linear 
Cross-correlation 
of Received 
Signal y(k) with 
Transmitted Signal 
x(k) 
The precision with which the range can be measured depends on both the sampling fre-
quency fs and the propagation speed c. The smallest increment for r corresponds to a 
delay of d 5 1 sample, which yields
Dr 5 0.0093 miles 
Chapter Summary 
Learning outcomes
This chapter was designed to provide the student with an opportunity to achieve the 
learning outcomes summarized in Table 2.7.
Signals and Systems
Chapter 2 focused on linear time-invariant discrete-time systems in the time domain.  
Discrete-time signals can be classified into a number of useful categories including finite 
and infinite signals, causal and noncausal signals, periodic and aperiodic signals, bounded 
and unbounded signals, and energy and power signals. Some common signals include the 
unit impulse (k), the unit step (k), causal exponentials, and periodic power signals. A 
discrete-time system S processes a discrete-time input signal x(k) to produce a discrete-
time output signal y(k). Discrete-time systems can be classified into linear and nonlinear 
systems, time-invariant and time-varying systems, causal and noncausal systems, stable 
and unstable systems, and passive and active systems.
Difference Equations
A finite-dimensional linear time-invariant (LTI) discrete-time system S can be repre-
sented in the time domain by a constant-coefficient difference equation.
 
y(k) 5 o
m
i50
bix(k 2 i) 2 o
n
i11
aiy(k 2 i), y0 [ Rn 
 (2.11.1)
2.11
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.11 Chapter Summary     131
The output of the system depends on the causal input x(k) and the initial condition, 
y0 5 f y(21), y(22), . . . , y(2n)gT. The output or response of S can be written as the sum 
of the zero-input response plus the zero-state response.
 
y(k) 5 yzi(k) 1 yzs(k) 
 (2.11.2)
The zero-input response, yzi(k), is the part of the response that comes from the initial 
condition, y0, when the input is x(k) 5 0. The zero-state response, yzs(k), is the part of the 
response that comes from the input, x(k), when the initial condition is y0 5 0. The key to 
the zero-input response is the characteristic polynomial of S.
 
a(z) 5 zn 1 a1zn21 1. . .1 an 
 (2.11.3)
Each root pi of the characteristic polynomial is a system pole that generates a natural 
mode term of the form ci(k)pk
i  in the zero-input response. If the root pi is a simple root 
that only occurs once, then the coefficient ci(k) is a constant. Otherwise ci(k) is a poly-
nomial in k of degree one less than the multiplicity of the root. The coefficients of the 
natural mode terms are computed by solving the equations that arise from applying the 
initial condition yzi(2i) 5 y(2i) for 1 # i # n. The zero-state response will contain nat-
ural mode terms that are excited by the input x(k), plus additional terms whose form 
depends on the type of input. The zero-state response to any input can be computed from 
the impulse response using convolution.
The impulse Response and Convolution
The impulse response is the zero-state response of the system to the unit impulse input, 
(k). If a system has an impulse response that is zero after a finite number of samples, 
then it is called an FIR system. Otherwise, it is an IIR system. The impulse response of 
an FIR system can be obtained directly from inspection of the input coefficients of the 
Initial condition
Characteristic 
polynomial
Natural mode
Impulse response
Num.
Learning outcome
Sec. 
1 
Be able to classify discrete time signals and systems into a variety of useful 
categories
2.2 
2 
Become familiar with common discrete-time signals and their characteristics
2.2 
3 
Know how to find the zero-input response of a difference equation for an 
arbitrary initial condition
2.3 
4 
Know how to find the zero-state response of a difference equation for a non-
zero input
2.3 
5 
Know how to find the complete response of a difference equation both analyti-
cally and numerically
2.3 
6 
Be able to represent discrete-time linear time-invariant systems graphically 
using block diagrams
2.4 
7 
Know how to compute the impulse response and classify systems based on 
the duration of the impulse response
2.5 
8 
Know how to perform linear and circular convolutions and compute the  
zero-state response using convolution 
2.6 
9 
Understand what it means for a system to be stable and be able to determine 
stability from the impulse response
2.7 
10 
Know how to use the GUI modules g_systime and g_correlate to interactively 
explore the input-output behavior of discrete-time systems
2.8 
Table 2.7: Learning 
Outcomes for  
Chapter 2 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

132    Chapter 2  Discrete-time Systems in the Time Domain
difference equation. A system is BIBO stable if and only if every bounded input is guar-
anteed to produce a bounded output. Otherwise, the system is unstable. The system S is 
stable if and only if the impulse response h(k) is absolutely summable. All FIR systems 
are stable, but IIR systems may or may not be stable.
Given the impulse response, h(k), the zero-state response of the system S to any input 
can be obtained from it using linear convolution. If h(k) and x(k) are two causal signals, 
the linear convolution of h(k) with x(k) is defined as
 
h(k) w x(k) 5 o
k
i50
h(i)x(k 2 i), k $ 0 
 (2.11.4)
When h(k) is the impulse response of a linear discrete-time system and x(k) is the system 
input, the zero-state response of the system is the linear convolution of h(k) with x(k).
 
yzs(k) 5 h(k) w x(k) 
 (2.11.5)
The convolution operation is commutative, so h(k) w x(k) 5 x(k) w h(k). In addition, 
if x(0) Þ 0, then h(k) can be recovered from x(k) and y(k) using a process known as 
deconvolution. Setting h(0) 5 y(0)yx(0), the remaining samples of the impulse response 
are obtained recursively using
 
h(k) 5
1
x(0) 5y(k) 2 o
k21
i50
h(i)x(k 2 i)6, k $ 1 
 (2.11.6)
In terms of computation, convolution corresponds to the process of polynomial 
multiplication, while deconvolution corresponds to polynomial division. If h(k) and x(k) 
are both signals of length N and x(k) is replaced by its periodic extension, xp(k), this 
results in an operation called the circular convolution of h(k) with x(k).
 
h(k) + x(k) 5 o
N21
i50
h(i)xp(k 2 i), 0 # k , N 
 (2.11.7)
Circular convolution is a more efficient form of convolution that can be represented with 
matrix multiplication. Linear convolution of two finite signals can be implemented with 
circular convolution by zero-padding the signals.
  
h(k) w x(k) 5 hz(k) + xz(k) 
 (2.11.8)
Correlation
An operation that is closely related to convolution is correlation. The linear cross- 
correlation of an L-point signal y(k) with an M-point signal x(k) is defined as follows 
where it is assumed that M # L.
  
ryx(k) 5 1
Lo
L21
i50
y(i)x(i 2 k), 0 # k , L 
 (2.11.9)
Here k is referred to as the lag variable because it represents the amount by which the 
second signal is delayed before the sum of products is computed. Linear cross-correlation 
BIBO stable
Linear convolution
Deconvolution
Circular convolution
Linear cross- 
correlation
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.12 Problems     133
can be used to measure the degree to which the shape of signal x(k) is similar to the 
shape of signal y(k). In particular, if y(k) contains a scaled version of x(k) delayed by d 
samples, then ryx(k) will exhibit a peak at k 5 d. The linear cross-correlation of a signal 
x(k) with itself is called the auto-correlation, rxx(k). The following normalized version of 
linear cross-correlation takes on values in the interval f21, 1g.
  
yx (k) 5
ryx(k)
Ï(MyL)rxx(0)ryy(0), 0 # k , L 
 (2.11.10)
Just as there is a circular version of convolution, there is also a circular version of 
cross-correlation. Suppose x(k) and y(k) are both of length N. If x(k) is replaced by its 
periodic extension xp(k), then this results in the following formulation of cross-correlation 
called circular cross-correlation.
  
cyx (k) 5 1
No
N21
i50
y(i)xp(i 2 k), 0 # k , N 
 (2.11.11)
The normalized version of circular cross-correlation takes on values in the interval f21, 1g.
  
yx(k) 5
cyx(k)
Ïcxx(0)cyy(0), 0 # k , N 
 (2.11.12)
There is a simple relationship between circular correlation and circular convolution. 
Circular cross-correlation of y(k) with x(k) is just a scaled version of circular convolution 
of y(k) with x(2k).
 
cyx(k) 5 y(k) + x(2k)
N
, 0 # k , N 
 (2.11.13)
GUi Modules
The DSP Companion includes GUI modules called g_systime and g_correlate that allow 
the user to interactively explore the input-output behavior of a discrete-time system 
in the time domain, and perform convolutions and correlations without any need for 
programming.
Problems 
The problems are divided into Analysis and Design problems that can be solved by hand 
or with a calculator, GUI Simulation problems that are solved using GUI modules g_sys-
time and g_correlate, and MATLAB Computation problems that require a user program. 
Solutions to selected problems can be accessed with the DSP Companion driver pro-
gram, g_dsp. Students are encouraged to use these problems, which are identified with a 
, as a check on their understanding of the material.
Auto-correlation
Circular 
cross-correlation
2.12
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

134    Chapter 2  Discrete-time Systems in the Time Domain
2.12.1 Analysis and Design
Section 2.2: Discrete-time Signals
2.1 
Classify each of the following signals as finite or infinite. For the finite signals, find 
the smallest integer N such that x(k) 5 0 for uku . N.
(a) x(k) 5 (k 1 5) 2 (k 2 5) 
(b) x(k) 5 sin(.2k)(k) 
(c) x(k) 5 min(k2 2 9,0)(k) 
(d) x(k) 5 (k)(2k)y(1 1 k2) 
(e) x(k) 5 tan(Ï2k)f(k) 2 (k 2 100)g 
(f) x(k) 5 (k) 1  cos (k) 2 (21)k 
(g) x(k) 5 k2k sin (.5k) 
2.2 
Classify each of the following signals as causal or noncausal.
(a) x(k) 5 maxhk, 0j 
(b) x(k) 5 sin(.2k)(2k) 
(c) x(k) 5 1 2  exp (2k) 
(d) x(k) 5 mod (k, 10) 
(e) x(k) 5 tan(Ï2k)f(k) 1 (k 2 100)g 
(f) x(k) 5 cos(k) 1 (21)k 
(g) x(k) 5 sin(.5k)y(1 1 k2) 
2.3 
Classify each of the following signals as periodic or aperiodic. For the periodic 
signals, find the period, M.
(a) x(k) 5 cos(.02k) 
(b) x(k) 5 sin(.1k) cos(.2k) 
(c) x(k) 5 cos(Ï3k) 
(d) x(k) 5 exp(jy8) 
(e) x(k) 5 mod (k, 10) 
(f) x(k) 5 sin2(.1k)(k) 
(g) x(k) 5 j 2k 
2.4 
Classify each of the following signals as bounded or unbounded.
(a) x(k) 5 k cos (.1k)y(1 1 k2) 
(b) x(k) 5 sin(.1k) cos(.2k)(k 2 3) 
(c) x(k) 5 cos(k2) 
(d) x(k) 5 tan(.1k)f(k) 2 (k 2 10)g 
(e) x(k) 5 k2y(1 1 k2) 
(f) x(k) 5 k exp (2k)(k) 
2.5 
For each of the following signals, determine whether or not it is bounded. For the 
bounded signals, find a bound, Bx.
(a) x(k) 5 f1 1  sin (5k)g(k) 
(b) x(k) 5 k(.5)k(k) 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.12 Problems     135
(c) x(k) 53
(1 1 k) sin (10k)
1 1 (.5)k 4(k) 
(d) x(k) 5 f1 1 (21)kg cos (10k)(k) 
2.6 
Consider the following sum of causal exponentials.
x(k) 5 (c1pk
1 1 c2pk
2)(k)
(a) Using the inequalities in Appendix 2, show that
ux(k)u # uc1u ? up1uk 1 uc2u ? up2uk
(b) Show that x(k) is absolutely summable if up1u , 1 and up2u , 1. Find an upper 
bound on uuxuu1.
(c) Suppose up1u , 1 and up2u , 1. Find an upper bound on the energy Ex. 
2.7 
Find the average power of the following signals.
(a) x(k) 5 10 
(b) x(k) 5 20(k) 
(c) x(k) 5
 mod (k, 5) 
(d) x(k) 5 a cos(ky8) 1 b sin(ky8) 
(e) x(k) 5 100fu(k 1 10) 2 u(k 2 10)g 
(f) x(k) 5 j k 
Section 2.3: Discrete-time Systems
2.8 
Classify each of the following systems as linear or nonlinear.
(a) y(k) 5 4fy(k 2 1) 1 1gx(k) 
(b) y(k) 5 6kx(k) 
(c) y(k) 5 2y(k 2 2) 1 10x(k 1 3) 
(d) y(k) 5 .5y(k) 2 2y(k 2 1) 
(e) y(k) 5 .2y(k 2 1) 1 x2(k) 
(f) y(k) 5 2y(k 2 1)x(k 2 1)y10 
2.9 
Classify each of the following systems as time-invariant or time-varying.
(a) y(k) 5 fx(k) 2 2y(k 2 1)g2 
(b) y(k) 5  sinfy(k 2 1)g 1 3x(k 2 2) 
(c) y(k) 5 (k 1 1)y(k 2 1) 1  cosf.1x(k)g 
(d) y(k) 5 .5y(k 2 1) 1  exp ( 2 ky5)(k) 
(e) y(k) 5  logf1 1 x2(k 2 2)g 
(f) y(k) 5 kx(k 2 1) 
2.10 Classify each of the following systems as causal or noncausal.
(a) y(k) 5 f3x(k) 2 y(k 2 1)g3 
(b) y(k) 5  sinfy(k 2 1)g 1 3x(k 1 1) 
(c) y(k) 5 (k 1 1)y(k 2 1) 1  cosf.1x(k2)g 
(d) y(k) 5 .5y(k 2 1) 1  exp(2ky5)(k) 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

136    Chapter 2  Discrete-time Systems in the Time Domain
(e) y(k) 5  logf1 1 y2(k 2 1)x2(k 1 2)g 
(f) h(k) 5 (k 1 3) 2 (k 2 3)
2.11 Consider the following system that consists of a gain of A and a delay of d samples.
y(k) 5 Ax(k 2 d)
(a) Find the impulse response h(k) of this system. 
(b) Classify this system as FIR or IIR. 
(c) Is this system BIBO stable? If so, find uuhuu1. 
(d) For what values of A and d is this a passive system? 
(e) For what values of A and d is this an active system? 
(f) For what values of A and d is this a lossless system? 
Section 2.4: Difference Equations
2.12 Consider the following linear time-invariant discrete-time system S.
y(k) 2 y(k 2 2) 5 2x(k)
(a) Find the characteristic polynomial of S and express it in factored form. 
(b) Write down the general form of the zero-input response, yzi(k). 
(c) Find the zero-input response when y(21) 5 4 and y(22) 5 21. 
2.13 Consider the following linear time-invariant discrete-time system S.
y(k) 5 1.8y(k 2 1) 2 .81y(k 2 2) 2 3x(k 2 1)
(a) Find the characteristic polynomial a(z) and express it in factored form. 
(b) Write down the general form of the zero-input response, yzi(k). 
(c) Find the zero-input response when y(21) 5 2 and y(22) 5 2. 
2.14 Consider the following linear time-invariant discrete-time system S.
y(k) 5 2.64y(k 2 2) 1 x(k) 2 x(k 2 2)
(a) Find the characteristic polynomial a(z) and express it in factored form. 
(b) Write down the general form of the zero-input response, yzi(k), expressing it 
as a real signal. 
(c) Find the zero-input response when y(21) 5 3 and y(22) 5 1. 
2.15 Consider the following linear time-invariant discrete-time system S.
y(k) 2 2y(k 2 1) 1 1.48y(k 2 2) 2 .416y(k 2 3) 5 5x(k)
(a) Find the characteristic polynomial a(z). Using the MATLAB function roots, 
express it in factored form. 
(b) Write down the general form of the zero-input response, yzi(k). 
(c) Write the equations for the unknown coefficient vector c [ R3 as Ac 5 y0, 
where y0 5 f y(21), y(22), y(23)gT is the initial condition vector. 
Roots
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.12 Problems     137
2.16 Consider the following linear time-invariant discrete-time system S.
y(k) 2 .9y(k 2 1) 5 2x(k) 1 x(k 2 1)
(a) Find the characteristic polynomial a(z) and the input polynomial b(z). 
(b) Write down the general form of the zero-state response, yzs(k), when the input 
is x(k) 5 3(.4)k(k). 
(c) Find the zero-state response. 
2.17 Consider the following linear time-invariant discrete-time system S.
y(k) 5 y(k 2 1) 2 .24y(k 2 2) 1 3x(k) 2 2x(k 2 1)
(a) Find the characteristic polynomial a(z) and the input polynomial b(z). 
(b) Suppose the input is the unit step, x(k) 5 (k). Write down the general form 
of the zero-state response, yzs(k). 
(c) Find the zero-state response to the unit step input. 
2.18 Consider the following linear time-invariant discrete-time system S.
y(k) 5 y(k 2 1) 2 .21y(k 2 2) 1 3x(k) 1 2x(k 2 2)
(a) Find the characteristic polynomial a(z) and the input polynomial b(z). Express 
a(z) in factored form. 
(b) Write down the general form of the zero-input response, yzi(k). 
(c) Find the zero-input response when the initial condition is y(21) 5 1 and 
y(22) 5 21.
(d) Write down the general form of the zero-state response when the input is 
x(k) 5 2(.5)k21(k). 
(e) Find the zero-state response using the input in (d). 
(f) Find the complete response using the initial condition in (c) and the input  
in (d). 
Section 2.5: Block Diagrams
2.19 Consider the following linear time-invariant discrete-time system S. Sketch a block 
diagram of this IIR system.
y(k) 5 3y(k 2 1) 2 2y(k 2 2) 1 4x(k) 1 5x(k 2 1)
2.20 Consider the following linear time-invariant discrete-time system S. Sketch a block 
diagram of this FIR system.
 y(k) 5 x(k) 2 2x(k 2 1) 1 3x(k 2 2) 2 4x(k 2 4)
2.21 Consider the following linear time-invariant discrete-time system S called an 
auto-regressive system. Sketch a block diagram of this system.
 y(k) 5 x(k) 2 .8y(k 2 1) 1 .6y(k 2 2) 2 .4y(k 2 3)
2.22 Consider the block diagram shown in Figure 2.33.
(a) Write a single difference equation description of this system.
(b) Write a system of difference equations for this system for ui(k) for 1 # i # k 
and y(k).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

138    Chapter 2  Discrete-time Systems in the Time Domain
Section 2.6: The impulse Response
2.23 Consider the following linear time-invariant discrete-time system S.
 y(k) 5 .6y(k 2 1) 1 x(k) 2 .7x(k 2 1)
(a) Find the characteristic polynomial and the input polynomial. 
(b) Write down the form of the impulse response, h(k). 
(c) Find the impulse response. 
2.24 Consider the following linear time-invariant discrete-time system S.
y(k) 5 2.25y(k 2 2) 1 x(k 2 1)
(a) Find the characteristic polynomial and the input polynomial. 
(b) Write down the form of the impulse response, h(k). 
(c) Find the impulse response. Use the identities in Appendix 2 to express h(k) in 
real form. 
2.25 Consider the following linear time-invariant discrete-time system S. Suppose 
0 , m # n and the characteristic polynomial a(z) has simple nonzero roots.
 y(k) 5 o
m
i50
bi x(k 2 i) 2 o
n
i51
ai y(k 2 i)
(a) Find the characteristic polynomial a(z) and the input polynomial b(z). 
(b) Find a constraint on b(z) that ensures that the impulse response h(k) does not 
contain an impulse term. 
2.26 Consider the following linear time-invariant discrete-time system S. Compute and 
sketch the impulse response of this FIR system.
 y(k) 5 u(k 2 1) 1 2u(k 2 2) 1 3u(k 2 3) 1 2u(k 2 4) 1 u(k 2 5)
Section 2.7: Convolution
2.27 Using the definition of linear convolution, show that for any signal h(k)
h(k) w (k) 5 h(k)
Figure 2.33:  
A Block Diagram 
of the System in 
Problem 2.22 
z21
1
1
1
u2(k)
2
2
u1(k)
x(k)
y(k)
1.8
.9
2.4
21.5
2.1
z21
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.12 Problems     139
2.28 Use Definition 2.3 and the commutative property to show that the linear convolu-
tion operator is associative.
 f (k) w fg(k) w h(k)g 5 f  f(k) w g(k)g w h(k)
2.29 Use Definition 2.3 to show that the linear convolution operator is distributive.
 f(k) w fg(k) 1 h(k)g 5 f(k) w g(k) 1 f(k) w h(k)
2.30 Suppose h(k) and x(k) are defined as follows.
 h 5 f2, 21, 0, 4gT
x 5 f5, 3, 27, 6gT
(a) Let yc(k) 5 h(k) + x(k). Find the circular convolution matrix C(x) such that 
yc 5 C(x)h. 
(b) Use C(x) to find yc(k).
2.31 Suppose h(k) and x(k) are the following signals of length L and M, respectively.
h 5 f3, 6, 21gT
x 5 f2, 0, 24, 5gT
(a) Let hz and xz be zero-padded versions of h(k) and x(k) of length N 5 L 1 M 2 1. 
Construct hz and xz. 
(b) Let yc(k) 5 hz(k) + xz(k). Find the circular convolution matrix C(xz) such that 
yc 5 C(xz)hz. 
(c) Use C(xz) to find yc(k). 
(d) Use yc(k) to find the linear convolution y(k) 5 h(k) w x(k) for 0 # k , N.
2.32 Consider a linear discrete-time system S with input x and output y. Suppose S 
is driven by an input x(k) for 0 # k , L to produce a zero-state output y(k). Use 
deconvolution to find the impulse response h(k) for 0 # k , L if x(k) and y(k) are 
as follows.
x 5 f2, 0, 21, 4gT
y 5 f6, 1, 24, 3gT
2.33 Suppose x(k) and y(k) are the following finite signals.
 x 5 f5, 0, 24gT
 y 5 f10, 25, 7, 4, 212gT
(a) Write the polynomials x(z) and y(z) whose coefficient vectors are x and y, 
respectively. The leading coefficient corresponds to the highest power of z. 
(b) Using long division, compute the quotient polynomial q(z) 5 y(z)yx(z). 
(c) Deconvolve y(k) 5 h(k) w x(k) to find h(k), using (2.7.15) and (2.7.18). Com-
pare the result with q(z) from part (b).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

140    Chapter 2  Discrete-time Systems in the Time Domain
Section 2.8: Correlation
2.34 Some books use the following alternative way to define the linear cross-correlation 
of an L point signal y(k) with an M-point signal x(k). Using a change of variable, 
show that this is equivalent to Definition 2.5.
 ryx(k) 5 1
L o
L212k
n50
y(n 1 k)x(n)
2.35 Suppose x(k) and y(k) are defined as follows.
x 5 f5, 0, 210gT
y 5 f1, 0, 22, 4, 3gT
(a) Find the linear cross-correlation matrix D(x) such that ryx 5 D(x)y.
(b) Use D(x) to find the linear cross-correlation ryx(k).
(c) Find the normalized linear cross-correlation yx(k).
2.36 Suppose y(k) is as follows.
y 5 f5, 7, 22, 4, 8, 6, 1gT
(a) Construct a 3-point signal x(k) such that ryx(k) reaches its peak positive value 
at k 5 3 and ux(0)u 5 1.
(b) Construct a 4-point signal x(k) such that ryx(k) reaches its peak negative value 
at k 5 2 and ux(0)u 5 1.
2.37 Suppose x(k) and y(k) are defined as follows.
 x 5 f4, 0, 212, 8gT
y 5 f2, 3, 1, 21gT
(a) Find the circular cross-correlation matrix E(x) such that cyx 5 E(x)y.
(b) Use E(x) to find the circular cross-correlation cyx(k).
(c) Find the normalized circular cross-correlation yx(k).
2.38 Suppose y(k) is as follows.
y 5 f8, 2, 23, 4, 5, 7gT
(a) Construct a 6-point signal x(k) such that yx(2) 5 1 and ux(0)u 5 6.
(b) Construct a 6-point signal x(k) such that yx(3) 5 21 and ux(0)u 5 12.
2.39 Let x(k) be an N-point signal with average power Px.
(a) Show that rxx(0) 5 cxx(0) 5 Px.
(b) Show that xx(0) 5 xx(0) 5 1.
2.40 This problem establishes the normalized circular cross-correlation inequality, 
uyx(k)u # 1. Let x(k) and y(k) be sequences of length N where xp(k) is the periodic 
extension of x(k).
(a) Consider the signal u(i, k) 5 ay(i) 1 xp(i 2 k) where a is arbitrary. Show that
1
No
N21
i50
fay(i) 1 xp(i 2 k)g2 5 a2cyy(0) 1 2acyx(k) 1 cxx(0) $ 0
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.12 Problems     141
(b) Show that the inequality in part (a) can be written in matrix form as
fa, 1g3
cyy(0)
cyx(k)
cyx(k)
cxx(0)43
a
14 $ 0
(c) Since the inequality in part (b) holds for any a, the 2 3 2 coefficient matrix C(k) is 
positive semi-definite, which means that  det fC(k)g $ 0. Use this fact to show that
 c2
yx(k) # cxx(0)cyy(0), 0 # k , N
(d) Use the results from part (c) and the definition of normalized cross-correlation  
to show that
21 # yx(k) # 1, 0 # k , N
Section 2.9: BiBo Stability
2.41 Consider the following FIR system.
 y(k) 5 o
5
i50
(1 1 i)2x(k 2 i)
Let x(k) be a bounded input with bound Bx. Show that y(k) is bounded with bound 
By 5 cBx. Find the minimum scale factor, c.
2.42 Consider a linear time-invariant discrete-time system S with the following impulse 
response. Find conditions on A and p that guarantee that S is BIBO stable.
 h(k) 5 Apk(k)
2.43 From Proposition 2.1, a linear time-invariant discrete-time system S is BIBO stable 
if and only if the impulse response h(k) is absolutely summable, that is, uuhuu1 , `. 
Show that uuhuu1 , ` is necessary for stability. That is, suppose that S is stable but 
h(k) is not absolutely summable. Consider the following input, where h*(k) denotes 
the complex conjugate of h(k) (Proakis and Manolakis,1992).
x(k) 55
h*(k)
uh(k)u, h(k) Þ 0
0,
h(k) 5 0
(a) Show that x(k) is bounded by finding a bound Bx.
(b) Show that S is not is BIBO stable by showing that y(k) is unbounded at k 5 0. 
2.12.2 GUi Simulation 
Section 2.4: Difference Equations 
2.44 Consider the following discrete-time system. Use GUI module g_systime to sim-
ulate this system. Hint: You can enter the b vector in the edit box by using two 
statements on one line: i = 0:8; b = cos(pi*i/4)
y(k) 5 o
8
i50
 cos(iy4)x(k 2 i) 
(a) Plot the polynomial roots.
(b) Plot and the impulse response using N 5 40.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

142    Chapter 2  Discrete-time Systems in the Time Domain
2.45 Consider a discrete-time system with the following characteristic and input polyno-
mials. Use GUI module g_systime to plot the step response using N 5 100 points. 
The MATLAB poly function can be used to specify coefficient vectors a and b in 
terms of their roots, as discussed in Section 2.9.
a(z) 5 (z 1 .5 6 j.6)(z 2 .9)(z 1 .75)
 b(z) 5 3z2(z 2 .5)2
 
2.46 Consider the following linear discrete-time system.
y(k) 5 1.7y(k 2 2) 2 .72y(k 2 4) 1 5x(k 2 2) 1 4.5x(k 2 4) 
Use GUI module g_systime to plot the following damped cosine input and the 
zero-state response to it using N 5 30. To determine F0, set 2F0kT 5 .3k and 
solve for F0yfs where T 5 1yfs.
x(k) 5 .97k cos(.3k) 
2.47 Consider the following linear discrete-time system.
 y(k) 5 2.4y(k 2 1) 1 .19y(k 2 2) 2 .104y(k 2 3) 1 6x(k) 2 7.7x(k 2 1) 1 2.5x(k 2 2) 
Create a MAT-file called prob2_47 that contains fs 5 100, the appropriate coef-
ficient vectors a and b, and the following input samples, where v(k) is white noise 
uniformly distributed over f2.2, .2g. Uniform white noise can be generated with the 
MATLAB function rand.
x(k) 5 k exp(2ky50) 1 v(k), 0 # k , 500 
(a) Print the MATLAB program used to create prob2_47.mat. 
(b) Use GUI module g_systime and the Import option to plot the roots of the 
characteristic polynomial and the input polynomial. 
(c) Plot the zero-state response on the input x(k). 
2.48 Consider the following discrete-time system, which is a narrow band resonator  
filter with sampling frequency of fs 5 800 Hz.
y(k) 5 .704y(k 2 1) 2 .723y(k 2 2) 1 .141x(k) 2 .141x(k 2 2) 
Use GUI module g_systime to find the zero-input response for the following initial 
conditions. In each chase plot N 5 50 points.
(a) y0 5 f10, 23gT
(b) y0 5 f25, 28gT
2.49  Consider the following discrete-time system, which is a notch filter with sampling 
interval T 5 1y360 sec.
y(k) 5 .956y(k 2 1) 2 .914y(k 2 2) 1 x(k) 2 x(k 2 1) 1 x(k 2 2) 
Use GUI module g_systime to find the output corresponding to the sinusoidal 
input x(k) 5 cos(2F0kT)(k). Do the following cases. Use the caliper option to 
estimate the steady state amplitude in each case.
(a) Plot the output when F0 5 10 Hz. 
(b) Plot the output when F0 5 60 Hz.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.12 Problems     143
Section 2.7: Convolution
2.50 Consider the following two polynomials. Use g_systime to compute, plot, and save 
in a data file the coefficients of the product polynomial c(z) 5 a(z)b(z). Then Import 
the saved file and display the coefficients of the product polynomial.
a(z) 5 z2 2 2z 1 3
b(z) 5 4z3 1 5z2 2 6z 1 7 
2.51 Consider the following two polynomials. Use g_systime to compute, plot, and save 
in a data file the coefficients of the quotient polynomial q(z) and the remainder 
polynomial r(z) where b(z) 5 q(z)a(z) 1 r(z). Then Import the saved file and display 
the coefficients of the quotient and remainder polynomials.
a(z) 5 z2 1 3z 2 4
b(z) 5 4z4 2 z2 2 8
Section 2.8: Correlation 
2.52 Use the GUI module g_correlate to record the sequence of vowels “A’’, “E’’, “I’’, 
“O’’, “U’’ in y. Play y to make sure you have a good recording of all five vowels. 
Then record the vowel “O’’ in x. Play x back to make sure you have a good record-
ing of “O’’ that sounds similar to the “O’’ in y. Export the results to a MAT-file 
named my_vowels.mat.
(a) Plot the inputs x and y showing the vowels.
(b) Plot the normalized cross-correlation of y with x using the Caliper option to 
mark the peak which should show the location of x in y.
(c) Based on the plots in (a), estimate the lag d1 that would be required to get the 
“O’’ in x to align with the “O’’ in y. Compare this with the peak location d2 in 
(b). Find the percent error relative to the estimated lag d1. There will be some 
error due to the overlap of x with adjacent vowels and co-articulation effects 
in creating y.
2.53 The file prob2_53.mat contains two signals, x and y, and their sampling frequency, 
fs. Use the GUI module g_correlate to Import x, y, and fs.
(a) Plot x(k) and y(k).
(b) Plot the normalized linear cross-correlation yx(k). Does y(k) contain any 
scaled and shifted versions of x(k)? Determine how many, and use the Caliper 
option to estimate the locations of x(k) within y(k).
2.12.3 MATLAB Computation 
Section 2.4 Difference Equations
2.54 Consider the following discrete-time system.
y(k) 5 .95y(k 2 1) 1 .035y(k 2 2) 2 .462y(k 2 3) 1 .351y(k 2 4)
1 .5x(k) 2 .75x(k 2 1) 2 1.2x(k 2 2) 1 .4x(k 2 3) 2 1.2x(k 2 4)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

144    Chapter 2  Discrete-time Systems in the Time Domain
Write a MATLAB program that uses filter and plot to compute and plot the zero-
state response of this system to the following input. Plot both the input and the 
output on the same graph.
x(k) 5 (k 1 1)2(.8)k(k), 0 # k # 100
2.55 Consider the following discrete-time system.
 a(z) 5 z4 2 .3z3 2 .57z2 1 .115z 1 .0168
b(z) 5 10(z 1 .5)3
This system has four simple nonzero roots. Therefore the zero-input response 
consists of a sum of the following four natural mode terms.
yzi(k) 5 c1pk
1 1 c2pk
2 1 c3pk
3 1 c4pk
4 
The coefficients can be determined from the initial condition
y0 5 f y(21), y(22), y(23), y(24)gT 
Setting yzi(2k) 5 y(2k) for 1 # k # 4 yields the following linear algebraic system 
in the coefficient vector c 5 fc1, c2, c3, c4gT.
3
p21
1
p21
2
p21
3
p21
4
p22
1
p22
2
p22
3
p22
4
p23
1
p23
2
p23
3
p23
4
p24
1
p24
2
p24
3
p24
443
c1
c2
c3
c44
5 y0 
Write a MATLAB program that uses roots to find the roots of the characteristic 
polynomial and then solves this linear algebraic system for the coefficient vec-
tor c using the MATLAB left division or \ operator when the initial condition 
is y0. Print the roots and the coefficient vector c. Use stem to plot the zero-input 
response yzi(k) for 0 # k # 40.
2.56 Consider the discrete-time system in Problem 2.55. Write a MATLAB program that 
uses the DSP Companion function f_filter0 to compute the zero-input response to 
the following initial condition. Use stem to plot the zero-input response yzi(k) for 
2 4 # k # 40.
y0 5 f y(21), y(22), y(23), y(24)gT 
Section 2.7 Convolution
2.57 Consider the following running average filter. 
y(k) 5 1
10o
9
i50
x(k 2 i), 0 # k # 100 
Write a MATLAB program that performs the following tasks.
(a) Use filter and plot to compute and plot the zero-state response to the following 
input, where v(k) is a random white noise uniformly distributed over f2.1, .1g. 
x(k) 5  exp(2ky20) cos(ky10)(k) 1 v(k) 
Plot x(k) and y(k) below one another. Uniform white noise can be generated 
using the MATLAB function rand.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.12 Problems     145
(b) Add a third curve to the graph in part (a) by computing and plotting the zero-
state response using conv to perform convolution. 
2.58 Consider the following FIR filter. Write a MATLAB program that performs the 
following tasks.
y(k) 5 o
20
i50
(21)ix(k 2 i)
10 1 i2
 
(a) Use the function filter to compute and plot the impulse response h(k) for 
0 # k , N where N 5 50. 
(b) Compute and plot the following periodic input.
x(k) 5  sin(.1k) 2 2 cos(.2k) 1 3 sin(.3k), 0 # k , N 
(c) Use conv to compute the zero-state response to the input x(k) using convo-
lution. Also compute the zero-state response to x(k) using filter. Plot both 
responses on the same graph using a legend.
2.59 Consider the following pair of signals.
h 5 f1, 2, 3, 4, 5, 4, 3, 2, 1gT
x 5 f2, 21, 3, 4, 25, 0, 7, 9, 26gT 
Verify that linear convolution and circular convolution produce different results 
by writing a MATLAB program that uses the DSP Companion function f_conv 
to compute the linear convolution y(k) 5 h(k) w x(k) and the circular convolution 
yc(k) 5 h(k) + x(k). Plot y(k) and yc(k) below one another on the same screen.
2.60 Consider the following pair of signals.
h 5 f1, 2, 4, 8, 16, 8, 4, 2, 1gT
 
x 5 f2, 21, 24, 24, 21, 2gT  
Verify that linear convolution can be achieved by zero padding and circular convo-
lution by writing a MATLAB program that pads these signals with an appropriate 
number of zeros and uses the DSP Companion function f_conv to compare the linear 
convolution y(k) 5 h(k) w x(k) with the circular convolution yzc(k) 5 hz(k) + xz(k). 
Plot the following.
(a) The zero-padded signals hz(k) and xz(k) on the same graph using a legend
(b) The linear convolution y(k) 5 h(k) w x(k)
(c) The zero-padded circular convolution yzc(k) 5 hz(k) + xz(k)
2.61  Consider the following polynomials.
a(z) 5 z4 1 4z3 1 2z2 2 z 1 3
b(z) 5 z3 2 3z2 1 4z 2 1
c(z) 5 a(z)b(z) 
Let a [ R5, b [ R4, and c [ R8 be the coefficient vectors of a(z), b(z), and c(z), 
respectively.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

146    Chapter 2  Discrete-time Systems in the Time Domain
(a) Find the coefficient vector of c(z) by direct multiplication by hand. 
(b) Write a MATLAB program that uses conv to find the coefficient vector of c(z) 
by computing c as the linear convolution of a with b. 
(c) In the program, show that a can be recovered from b and c by using the 
MATLAB function deconv to perform deconvolution.
Section 2.8: Correlation
2.62 Consider the following pair of signals.
 
x = [2, 24, 3, 7, 6, 1, 9, 4, 23, 2, 7, 8]T
y = [3, 2, 1, 0, 21, 22, 23,−2, 21, 0, 1, 2]T
Verify that linear cross-correlation and circular cross-correlation produce different 
results by writing a MATLAB program that uses the DSP Companion function 
f_corr to compute the linear cross-correlation, ryx(k), and the circular cross-correla-
tion, cyx(k). Plot ryx(k) and cyx(k) below one another on the same screen.
2.63 Consider the following pair of signals.
y = [1, 8, 23, 2, 7, 25, 21, 4]T
x = [2, 23, 4, 0, 5]T
Verify that linear cross-correlation can be achieved by zero-padding and circular 
cross-correlation by writing a MATLAB program that pads these signals with an 
appropriate number of zeros and uses the DSP Companion function f_corr to com-
pute the linear cross-correlation ryx(k) and the circular cross-correlation cyzxz(k). Plot 
the following.
(a) The zero-padded signals xz(k) and yz(k) on the same graph using a legend.
(b) The linear cross-correlation ryx(k) and the scaled zero-padded circular cross- 
correlation (NyL)cyzxz(k) on the same graph using a legend.
2.64 Consider the following pair of signals of length N 5 8.
x 5 f2, 24, 7, 3, 8, 26, 5, 1gT
y 5 f3, 1, 25, 2, 4, 9, 7, 0gT
 
Write a MATLAB program that performs the following tasks.
(a) Use the DSP Companion function f_corr to compute the circular cross- 
correlation, cyx(k).
(b) Compute and print u(k) 5 x(2k) using the periodic extension, xp(k).
(c) Verify that cyx(k) 5 f y(k) + x(2k)gyN by using the DSP Companion func-
tion f_conv to compute and plot the scaled circular convolution, w(k) = 
fu(k) + x(k)gyN. Plot cyx(k) and w(k) below one another on the same screen.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

C H A P T E R  3
Motivation
Recall that a discrete-time system is a system that processes a  
discrete-time input signal, x(k), to produce a discrete-time 
output signal, y(k). If the signals x(k) and y(k) are discrete 
in amplitude as well as in time, then they are digital signals, 
and the associated system is a digital signal processor. In 
this chapter we focus our attention on analyzing the input- 
output behavior of linear time-invariant discrete-time systems 
in the frequency domain. Together with Chapter 2, this material 
lays a mathematical foundation for subsequent chapters where 
we design digital filters and develop digital signal processing 
(DSP) algorithms.
An essential tool for the analysis of discrete-time systems 
is the Z-transform, a transformation that maps or transforms  
a discrete-time signal x(k) into a function X(z) of a complex  
variable z.
X(z) 5 Zhx(k)j
With the help of the Z-transform, the difference-equation 
description of a discrete-time system in Chapter 2 can be con-
verted to a simple algebraic equation which is readily solved 
for the Z-transform of the output, Y(z). Applying the inverse 
Z-transform to Y(z) then produces the zero-state response, yzs(k). 
Important qualitative features of discrete-time systems also can 
be obtained with the help of the Z-transform. Recall that a dis-
crete-time system is stable if and only if every bounded input sig-
nal is guaranteed to produce a bounded output signal. Stability is  
3.1
147
CHAPTER ToPiCs
 3.1 
Motivation
 3.2 
Z-transform Pairs
 3.3 
Z-transform Properties
 3.4 
Inverse Z-transform
 3.5 
Transfer Functions
 3.6 
Signal Flow Graphs
 3.7 
Stability in the Frequency 
Domain 
 3.8 
Frequency Response 
*3.9 
System Identification 
3.10 GUI Modules  
and Case Studies 
3.11 Chapter Summary
3.12 Problems
Discrete-time systems  
in the Frequency Domain 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

148    Chapter 3  Discrete-time Systems in the Frequency Domain
an essential characteristic of practical digital filters, and the easiest way to establish sta-
bility is with the Z-transform.
We begin this chapter by examining a number of practical problems that can be mod-
eled using discrete-time systems and solved using the Z-transform. Next, the Z-transform 
is defined, its region of convergence is analyzed, and a basic table of Z-transform pairs 
is developed. The size of this table is then expanded by introducing a number of impor-
tant Z-transform properties. The problem of inverting the Z-transform is then addressed 
using the synthetic division, partial fraction expansion, and residue methods. Next, two 
new representations of a discrete-time system are introduced. The first is the transfer 
function representation, which includes a discussion of poles, zeros, and modes. The sec-
ond is a compact graphical representation called a signal flow graph. A simple frequency 
domain criterion is presented that determines when a system is BIBO stable, and the Jury 
stability test is introduced. The frequency response of a discrete-time systems is then 
introduced, and interpretations of it are provided in terms of the transfer function and 
the steady-state response to periodic inputs. Least-squares linear system identification 
using ARMA models is then discussed. Finally, a GUI module called g_sysfreq is intro-
duced that allows the user the explore the input-output behavior of discrete-time systems 
in the frequency domain without any need for programming. The chapter concludes with 
some case study examples and a summary of discrete-time system analysis techniques.
3.1.1 satellite Attitude Control
As an example of a discrete-time system, consider the problem of finding a discrete- 
equivalent of a sampled-data system. Recall from Chapter 1 that a sampled-data system 
is system that contains both continuous-time signals and discrete-time signals. Conse-
quently, any system that has a digital-to-analog converter (DAC) or an analog-to-digital 
converter (ADC) is a sampled-data system. For example, consider the feedback control 
system shown in Figure 3.1 that contains both.
The task of this feedback system is to control the angular position of a satellite that is 
spinning about one axis in space, as shown in Figure 3.2. Here r(k) is the desired angular 
position of the satellite at discrete time k, and y(k) is the actual angular position. Since there is 
no friction, the motion of the satellite can be modeled using Newton’s second law as follows.
 
Jd 2ya(t)
dt2
5 xa(t) 
 (3.1.1)
From Figure 3.1, the input xa(t) is the torque generated by the thrusters, and J is the 
moment of inertia about the axis of rotation. The digital controller acts on the error  
signal input, e(k) 5 r(k) 2 y(k) to produce a control signal x(k). For example, consider 
the following difference equation used to implement a simple controller.
 
x(k) 5 cfe(k) 2 e(k 2 1)g 
 (3.1.2)
Sampled-data system
Error signal
Figure 3.1: Single-axis Satellite Attitude Control System
r
e
Controller
x
DAC
Satellite
ADC
2
1
a
ya
y
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.1 Motivation    149
Here the controller gain, c, is an engineering design parameter whose value can be chosen 
to satisfy some performance specification. The controller in (3.1.2) is an example of a 
finite impulse response or FIR discrete-time system.
Next suppose the DAC is modeled as a zero-order hold as in Chapter 1. Then 
the discrete-equivalent of the DAC and the satellite can be modeled with the following 
hold-equivalent discrete-time system (Franklin et al., 1990) where y(k) 5 ya(kT ).
 
y(k) 5 2y(k 2 1) 2 y(k 2 2) 11
T 2
2J2
 fx(k 2 1) 1 x(k 2 2)g 
 (3.1.3)
Substituting (3.1.2) into (3.1.3), and recalling that e(k) 5 r(k) 2 y(k), the entire 
closed-loop system then can be modeled by the following discrete-equivalent system 
whose input is the desired angle r(k) and output is the actual angle y(k).
 
y(k) 5 (d 2 1)y(k 2 1) 1 dy(k 2 2) 1 d fr(k 2 1) 1 r(k 2 2)g 
 (3.1.4a)
 
d 5 cT 2
2J  
 (3.1.4b) 
Thus the sampled-data control system consists of three separate discrete-time systems: 
the controller in (3.1.2), the hold-equivalent of the DAC and the satellite in (3.1.3), and 
the overall closed-loop discrete-equivalent system in (3.1.4). Later in this chapter we will 
see that this control system is stable, and therefore operates successfully, for the following 
stable range of values for the controller gain.
 
0 , c , 2J
T 2 
 (3.1.5)
3.1.2 Modeling the Vocal Tract
One of the exciting application areas of DSP is the analysis and synthesis of human 
speech. Both speech recognition and speech synthesis are becoming increasingly com-
monplace as a user interface between humans and machines. An effective technique for 
generating synthetic speech is to use a discrete-time signal to represent the output from 
Satellite model
Closed-loop system
Stable range
Figure 3.2: A 
Spinning Satellite
ya
J
Solar panel
Solar panel
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

150    Chapter 3  Discrete-time Systems in the Frequency Domain
the vocal chords and a slowly varying digital filter to model the vocal tract as illustrated 
by the block diagram in Figure 3.3 (Rabiner and Schafer, 1978; Markel and Gray, 1976).
Speech sounds can be decomposed into fundamental units called phonemes that are 
either voiced or unvoiced. Unvoiced phonemes are associated with turbulence in the vocal 
tract and are therefore modeled using filtered white noise. Unvoiced phonemes include the 
fricatives such as the s, sh, and f sounds, and the terminal sounds p, t, and k. Voiced pho-
nemes are associated with periodic excitation of the vocal chords. They include the vowels, 
nasal sounds, and transient terminal sounds such as b, d and g. Voiced phonemes can be 
modeled as the response of a digital filter to a periodic impulse train with period M.
 
x(k) 5 o
`
i50
(k 2 iM) 
(3.1.6)
If T is the sampling interval, then the period of the impulse train in seconds is T0 5 MT. 
The fundamental frequency or pitch of the speaker is then
 
F0 5
1
MT 
(3.1.7)
Speaker pitch typically ranges from about 50 Hz to 400 Hz, with male speakers  
having a lower pitch, on average, than female speakers. An illustration of a short segment 
of the vowel “O” is shown in Figure 3.4.
To estimate the pitch of this speaker, we examine the distance between peaks in  
Figure 3.4. Using the DSP Companion function f_caliper, the period shown with plus 
marks in Figure 3.4 is about T0 5 6.9 ms, which corresponds to a pitch of
 
F0 < 145  Hz  
 (3.1.8)
The linear system S in Figure 3.3 models the vocal tract cavity including the throat, 
mouth, and lips. An effective model for most sounds is an nth order auto-regressive system, 
where the coefficient vector of the input polynomial is simply b 5 1.
 
y(k) 5 x(k) 2 o
n
i51
ai  y(k 2 i) 
 (3.1.9)
Finding a suitable parameter vector a [ Rn for the vocal tract model given the input x(k) and 
output y(k) is an example of system identification, a topic that is examined in Section 3.9.
Phoneme
Pitch
Auto-regressive system
System identification
Figure 3.3: Digital 
Speech Synthesis
Unvoiced
Voiced
Pitch
x
Volume
Linear
system
S
Vocal tract
y
Speech
F0
Impulse
train
A
White
noise
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Z-transform Pairs
The Z-transform is a powerful tool that is useful for analyzing and solving linear discrete-
time systems.
The Z-transform of a discrete-time signal x(k) is a function X(z) of a complex  
variable z defined
X(z) 5
D  o
`
k52`
x(k)z2k
From Definition 3.1 we see that the Z-transform is a power series in the variable z21. 
The operation of performing a Z-transform can also be represented with the Z-transform  
operator, Z, as follows
 
Zhx(k)j 5 X(z) 
 (3.2.1)
Note that, by convention, the corresponding uppercase letter is used to denote the  
Z-transform of a time signal. For most practical signals, the Z-transform can be expressed 
in factored form as a ratio of two polynomials.
 
X(z) 5
b0(z 2 z1)(z 2 z2) Á (z 2 zm)
(z 2 p1)(z 2 p2) Á (z 2 pn)  
 (3.2.2)
Here the roots of the numerator polynomial are called the zeros of X(z), and the roots of 
the denominator polynomial are called the poles of X(z). To compute the Z-transform 
directly from Definition 3.1, the following generalization of the geometric series is helpful 
where z is any real or complex-valued quantity (see Problem 3.9).
 
o
`
k5m
zk 5
zm
1 2 z, m $ 0 and uzu , 1  
 (3.2.3)
3.2
DEFiniTion
3.1 Z-transform
notation!
Zeros, poles
Generalized  
geometric series
0
0.5
0.4
0.3
0.2
0.1
0
20.1
20.2
20.3
20.4
20.5
10
20
30
40
50
60
kT (msec)
y(k)
Figure 3.4: 
Segment of 
Recorded  
Vowel “O”
3.2 Z-transform Pairs    151
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

152    Chapter 3  Discrete-time Systems in the Frequency Domain
3.2.1 Region of Convergence
For a given value of z, the power series in Definition 3.1 may or may not converge. To 
explore the region of convergence in the complex plane C, first note that every signal can 
be decomposed into the sum of two parts.
 
x(k) 5 xc(k) 1 xa(k) 
 (3.2.4)
Here the causal part, xc(k), and the anti-causal part, xa(k), are
 
xc(k)  5 
 
D x(k)(k) 
 (3.2.5)
 
xa(k)  5 
D x(k)(2k 2 1) 
 (3.2.6)
The causal part is zero for k , 0, and the anti-causal part is zero for k $ 0.
 
x 5 {…, x(23), x(22), x(21) x(0), x(1), x(2), …} 
 (3.2.7)
For a general signal x(k), the causal part xc(k) will have one region of convergence, and 
the anti-causal part xa(k) will have another region of convergence. The overall region of 
convergence, VROC, will include the intersection of the two regions.
Causal, anti-causal 
parts
anti-causal part
(+++)+++*
causal part
(++)++*
EXAMPLE 3.1
To illustrate a Z-transform and its region of convergence, consider the following 
two-sided exponential signal.
x(k) 55 ak,
k $ 0
bk,
k , 0
Note that this can be written as a sum of causal and anti-causal parts as follows.
x(k) 5 ak(k) 1 bk(2k 2 1)
Since the Z-transform is a linear operation, the transforms for the two parts 
can be computed separately and then added. Using the geometric series in (3.2.3),
Xc(z) 5 o
`
k50
akz2k
 5 o
`
k50
(ayz)k
 5
1
1 2 ayz, uayzu , 1
 5
z
z 2 a, uzu . uau
xc(k)
"
(')'*
xa(k)
Region of Convergence
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.2 Z-transform Pairs    153
Thus the region of convergence of the causal part is outside the circle of radius uau 
centered at the origin. The anti-causal Z-transform can be computed in a similar 
manner using a change of variable.
Xa(z) 5 o
21
k52`
bkz2k
5 o
1
i5`
b2izi, i 5 2k
5 o
`
i51
(zyb)i
5
zyb
1 2 zyb, uzybu , 1
5 2z
z 2 b, uzu , ubu
In this case the region of convergence of the anti-causal part is inside the circle of 
radius ubu centered at the origin. The complete Z-transform is then
X(z) 5 Xc(z) 1 Xa(z)
5
z
z 2 a 2
z
z 2 b
5 z(z 2 b) 2 z(z 2 a)
(z 2 a)(z 2 b)
5
(a 2 b)z
(z 2 a)(z 2 b), uau , uzu , ubu
Hence the region of convergence of X(z) is an annular ring with inner radius uau 
and outer radius ubu centered at the origin of the complex plane. Of course if 
uau $ ubu, then the ring evaporates and the Z-transform does not exist because the 
region of convergence is the empty set.
For this example the region of convergence of X(z) is equal to the intersection 
of the regions of convergence of the causal part Xc(z) and the anti-causal part 
Xa(z). In some instances, the region of convergence of X(z) can be larger. This can 
occur, for example, when Xc(z) 1 Xa(z) experiences pole-zero cancellation.
The results of Example 3.1 can be generalized in the following way. Suppose h p1, . . . , pnj 
are the poles of the causal part Xc(z), and hq1, . . . , qrj are the poles of the anti-causal part 
Xa(z) that survive any pole-zero cancellation in Xc(z) 1 Xa(z). Define the radius of the 
innermost anti-causal pole and the outermost causal pole as follows.
 
Rm 5 
D min
r
i51 {uqiu} 
 (3.2.8)
 
Rm 5 
D max
n
i51 {upiu} 
 (3.2.9)
The region of convergence of the causal part is uzu . RM, and the region of conver-
gence of the anti-causal part is uzu , Rm. That is, the causal part converges outside its 
outermost pole, and the anti-causal part converges inside its innermost pole. If x(k) is 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

154    Chapter 3  Discrete-time Systems in the Frequency Domain
Table 3.1: Regions  
of Convergence
signal 
Length 
VROC 
Causal
Infinite 
uzu . RM
Anti-causal
Infinite 
uzu , Rm
General
Infinite 
RM , uzu , Rm
causal, then the region of convergence is uzu . RM, and if x(k) is anti-causal, the region of 
convergence is uzu , Rm. For general signals, the region of convergence is
 
VROC 5 hz [ C : RM , uzu , Rmj  
 (3.2.10)
Plots of showing the regions of convergence for two important cases are shown in 
Figure 3.5. An important special case occurs when x(k) is a finite signal. For a finite 
causal signal of length m, the Z-transform is
X(z) 5 x(0) 1 x(1)z21 1 Á 1 x(m 2 1)z12m
5 x(0)zm21 1 x(1)zm22 1 Á 1 x(m 2 1)
zm21
 
 (3.2.11)
Thus a finite causal signal of length m . 1 has m 2 1 poles at z 5 0, which means its 
region of convergence is uzu . 0. If m 5 1 as in x(k) 5 (k), then the region of conver-
gence is the entire complex plane. The infinite cases are summarized in Table 3.1. Most of 
the signals that come up in practice are causal, either finite or infinite. For these signals 
the region of convergence is simply
 
VROC 5 hz [ C : uzu . RMj  
 (3.2.12)
General signals
Causal signals
RM
(b)
Re(z)
Rm
RM
(a)
Re(z)
Im(z)
Im(z)
Figure 3.5: Region 
of Convergence  
Is Shaded for  
(a) General Signals, 
(b) Causal Signals. 
Here Rm is the 
radius of the 
innermost pole 
of the anti-causal 
part, and RM is 
the radius of the 
outermost pole of 
the causal part
3.2.2 Common Z-transform Pairs
The following examples illustrate the use of the geometric series to compute the  
Z-transform of the common discrete-time signals introduced in Section 2.2.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

EXAMPLE 3.2
Recall that the unit impulse, denoted (k), is a signal that is zero everywhere except at 
k 5 0 where it takes on the value one. Using Definition 3.1, we find that the only non-
zero term in the series is the constant term whose coefficient is unity. Consequently,
Zh(k)j 5 1
Since the Z-transform of a unit impulse is constant, it has no poles, which means 
that its region of convergence is the entire complex plane, z [ C.
Unit impulse
Next recall that the when the unit impulse (k) is summed from 2` to k this pro-
duces the unit step (k).
EXAMPLE 3.3
The unit step, denoted (k), is a causal infinite signal that takes on the value one 
for k $ 0. Applying Definition 3.1 and using the geometric series in (3.2.3), we have
U(z) 5 o
`
k50
z2k
5 o
`
k50
(1yz)k
5
1
1 2 1yz, u1yzu , 1
Negative powers of z can be cleared by multiplying the numerator and the denom-
inator by z, which then yields
Zh(k)j 5
z
z 2 1, uzu . 1
Note that the Z-transform has a zero at z 5 0 and a pole at z 5 1. Plots of the 
signal (k) and the pole-zero pattern of its Z-transform are shown in Figure 3.6.
Unit step
0
5
10
15
20.5
0
0.5
1
1.5
k
x(k)
22
21
0
1
2
22
21
0
1
2
Re(z)
Im(z)
X
O
Figure 3.6: Time Plot and Pole-zero Pattern of a Unit Step Signal (k) for Example 3.2
3.2 Z-transform Pairs    155
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

156    Chapter 3  Discrete-time Systems in the Frequency Domain
0
5
10
15
0
0.5
1
k
x(k)
22
0
2
22
21
0
1
2
Re(z)
X
O
0
5
10
15
0
1
2
3
4
5
k
x(k)
22
0
2
22
21
0
1
2
Re(z)
Im(z)
Im(z)
X
O
Figure 3.7: Time Plots and Pole-zero Patterns of Causal Exponentials for 
Example 3.4 with c 5 0.8 (Bounded) and c 5 1.1 (Unbounded)
The unit step function (k) is useful because if we multiply another signal by (k) 
the result is a causal signal. By generalizing the unit step function slightly, we generate the 
most important Z-transform pair.
EXAMPLE 3.4
Let c be real or complex, and consider the following causal exponential signal 
generated by taking powers of c.
x(k) 5 ck(k)
Using Definition 3.1 and the geometric series in (3.2.3) we have
X(z) 5 o
`
k50
ckz2k
5 o
`
k50
(cyz)k
5
1
1 2 cyz, ucyzu , 1
Causal Exponential
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The causal exponentials in Figure 3.7 have a single real pole. Another important case 
corresponds to a complex conjugate pair of poles. The associated signal is referred to as 
an exponentially damped sine.
Clearing 1yz by multiplying the numerator and denominator by z, the Z- 
transform of the causal exponential is
Zhck(k)j 5
z
z 2 c, uzu . ucu
Plots of the causal exponential signal and its pole-zero pattern are shown in  
Figure 3.7. The first case, c 5 0.8, corresponds to a damped exponential, and the 
second, c 5 1.1, to a growing exponential. If the pole at z 5 c is negative, then 
x(k) oscillates between positive and negative values as it decays or grows. When 
c 5 1, the causal exponential reduces to the unit step in Example 3.3.
EXAMPLE 3.5
Let c and d be real with c . 0, and consider the following causal exponentially 
damped sine wave.
x(k) 5 ck sin(dk)(k)
Using Euler’s identity in Appendix 2, we can express  sin (dk) as
sin(dk) 5
 exp(jdk) 2  exp(2jdk)
j2
Thus the signal x(k) can be represented as
 x(k) 5 ck fexp(jdk) 2 exp(2jdk)g(k)
j2
 5 fc exp( jd)gk(k) 2 fc exp(2jd)gk(k)
j2
It is clear from Definition 3.1 that the Z-transform operator is a linear opera-
tor. That is, the transform of the sum of two signals is the sum of the transforms, 
and the transform of a scaled signal is just the scaled transform. Using this prop-
erty, Euler’s identity, and the results of Example 3.4 we have
X(z) 5  Z5
fc exp(jd)gk(k) 2 fc exp(2jd)gk(k)
j2
6
Exponentially Damped sine
(Continued)
3.2 Z-transform Pairs    157
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

158    Chapter 3  Discrete-time Systems in the Frequency Domain
Figure 3.8: Time Plot and Pole-zero Pattern of Exponentially Damped Sine 
Wave for Example 3.5 with c 5 .8 and d 5 .6
5  Zh fc exp( jd )gk(k)j
j2
2 Zh fc exp(2jd)gk(k)j
j2
5  1
j25
z
z 2 c exp(jd) 2
z
z 2 c exp(2jd)6, uzu . c
5  
czfexp( jd) 2 exp(2jd)g
j 2fz 2 c exp( jd)gfz 2 c exp(2jd)g, uzu . c
5  
c sin(d)z
z2 2 cfexp( jd) 1 exp(2jd)g 1 c2, uzu . c
Note that X(z) has poles with a magnitude of c and a phase angle of 6d. Apply-
ing Euler’s identity to the denominator, the Z-transform of the causal exponen-
tially damped sine wave is
Zhck sin(dk)(k)j 5
c sin(d)z
z2 2 2c cos(d)z 1 c2, uzu . c
Plots of the exponentially damped sine wave and its pole-zero pattern are shown in  
Figure 3.8. Note that the magnitude of the poles, c, determines whether, and how 
fast, the signal decays to zero, while the angle of the poles, d, determines the  
frequency of the oscillation.
0
5
10
15
20.5
0
0.5
1
k
x(k)
22
21
0
1
2
22
21
0
1
2
X
X
O
Re(z)
Im(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A brief summary of the most important Z-transform pairs is shown in Table 3.2. 
The unit step function is a special case of the causal exponential function with c 5 1. 
Similarly, transforms of the sine and cosine are obtained from the damped sine and 
cosine by setting the damping factor to c 5 1. A more complete table of Z-transform 
pairs can be found in Appendix 1.
Z-transform Properties
3.3.1 General Properties
The effective size of Table 3.2 can be increased significantly by judiciously applying a 
number of the properties of the Z-transform.
Linearity Property
The most fundamental property of the Z-transform is the notion that the Z-transform 
operator is a linear operator. In particular, if x(k) and y(k) are two signals and a and b 
are two arbitrary constants, then from Definition 3.1 we have
Zhax(k) 1 by(k)j 5 o
`
k52`
fax(k) 1 by(k)gz2k
 
5 a o
`
k52`
x(k)z2k 1 b o
`
k52`
y(k)z2k 
 (3.3.1)
3.3
signal
Z-transform 
Poles
VROC
(k)
1 
none
z [ C
(k)
z
z 2 1 
z 5 1
uzu . 1
k(k)
z
(z 2 1)2 
z 5 1
uzu . 1
ck(k)
z
z 2 c
z 5 c
uzu . ucu
k(c)k(k)
cz
(z 2 c)2
z 5 c
uzu . ucu
sin(dk)(k)
 sin(d)z
z2 2 2 cos(d)z 1 1
z 5 exp(6jd)
uzu . 1
cos(dk)(k)
fz 2 cos(d)gz
z2 2 2 cos(d)z 1 1
z 5 exp(6jd)
uzu . 1
ck sin(dk)(k)
c sin(d)z
z2 2 2c cos(d)z 1 c2
z 5 c exp(6jd)
uzu . c
ck cos(dk)(k)
fz 2 c cos(d)gz
z2 2 2c cos(d)z 1 c2
z 5 c exp(6jd)
uzu . c
Table 3.2: Basic 
Z-transform Pairs
3.3 Z-transform Properties    159
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

160    Chapter 3  Discrete-time Systems in the Frequency Domain
Figure 3.9: The  
r-sample Delay 
Operator 
x(k 2 r)
z2r
x(k)
Thus the Z-transform of the sum of two signals is just the sum of the Z-transforms of 
the signals. Similarly, the Z-transform of a scaled signal is just the scaled Z-transform of 
the signal.
 
Zhax(k) 1 by(k)j 5 aX(z) 1 bY(z)  
 (3.3.2)
Recall that Example 3.5 was an example that made use of the linearity property 
to compute a Z-transform. The region of convergence of the Z-transform of 
ax(k) 1 by(k) includes the intersection of the regions of convergence of X(z) and 
Y(z).
Delay Property
Perhaps the most widely used property, particularly for the analysis of linear difference 
equations, is the time shift or delay property. Let r $ 0 denote the number of samples by 
which a causal discrete-time signal x(k) is delayed. Applying Definition 3.1 and using the 
change of variable i 5 k 2 r, we have
Zhx(k 2 r)j 5 o
`
k50
x(k 2 r)z2k
5 o
`
i52r
x(i)z2(i1r)
 , i 5 k 2 r
 
5 z2ro
`
i50
x(i)z2i 
 (3.3.3)
Consequently, delaying a signal by r samples is equivalent to multiplying its Z-transform 
by z2r.
 
Zhx(k 2 r)j 5 z2rX(z)  
 (3.3.4)
In view of the delay property, we can regard z21 as a unit delay operator and z2r as a 
delay of r samples as shown in Figure 3.9.
Linearity property
Delay property
Unit delay
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Z-scale Property
Another property that can be used to produce new Z-transform pairs is obtained by 
multiplying a discrete-time signal by an exponential. In particular, let c be constant and 
consider the signal c kx(k).
Zhckx(k)j 5 o
`
k52`
ckx(k)z2k
 
5 o
`
k52`
x(k)1
z
c2
2k
 
 (3.3.5)
Consequently, multiplying a time signal by c k is equivalent to scaling the Z-transform 
variable z by 1yc.
 
Zhckx(k)j 5 X1
z
c2  
 (3.3.6)
A simple illustration of the z-scale property can be found in Table 3.2. Notice 
that the Z-transform of the causal exponential signal can be obtained from the  
Z-transform of the unit step by simply replacing z with zyc. To determine the region 
of convergence of the Z-transform of c kx(k), we replace z by zyc in the region of 
convergence of X(z).
Z-scale property
EXAMPLE 3.6
As a simple illustration of the linearity and delay properties, consider the problem 
of finding the Z-transform of a pulse of height b and duration M samples starting 
at k 5 0. This signal can be written in terms of steps as a step up of amplitude b at 
time k 5 0, followed by a step down of amplitude b at time k 5 M.
x(k) 5 bf(k) 2 (k 2 M)g
Applying the linearity and the delay properties and using Table 3.2, we have
X(z) 5  b(1 2 z2M)Zh (k)j
5  b(1 2 z2M)z
z 2 1
5  b(zM 2 1)
zM 2 1(z 2 1), uzu . 1
Note that when b 5 1 and M 5 1 this reduces to X(z) 5 1, which is the  
Z-transform of the unit impulse.
Pulse
3.3 Z-transform Properties    161
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

162    Chapter 3  Discrete-time Systems in the Frequency Domain
Time Multiplication Property
We can add an important family of entries to Table 3.2 by considering what happens 
when we take the derivative of the Z-transform of a signal.
dX(z)
dz
5 d
dz
 o
`
k52`
x(k)z2k
5 2 o
`
k52`
kx(k)z2(k 1 1)
 
5 2z21 o
`
k52`
kx(k)z2k 
 (3.3.7)
Since the sum in (3.3.7) is just the Z-transform of kx(k), this yields the time multiplication 
property
 
Zhkx(k)j 5 2zdX(z)
dz
 
 (3.3.8)
Thus multiplying a discrete-time signal by the time variable k is equivalent to taking the 
derivative of the Z-transform and scaling by 2z.
Time multiplication 
property
EXAMPLE 3.7
As an illustration of the time multiplication and Z-scale properties, consider the 
following signal, which is referred to as a unit ramp.
x(k) 5
D   
k(k)
Applying the time multiplication property we have
X(z) 5 2z d
dzZh(k)j
5 2z d
dz5
z
z 2 16
5
z
(z 2 1)2
Thus the Z-transform of a unit ramp is
Zhk(k)j 5
z
(z 2 1)2, uzu . 1
The time multiplication property can be applied repeatedly to obtain the Z- 
transform of additional signals from the family km(k) for m $ 0. The results for 
0 # m # 2 are summarized in Appendix 1.
Given the Z-transform of the unit ramp, the Z-scale property can be applied 
to generalize the signal further. Consider the following causal exponential with a 
linear coefficient.
x(k) 5 kck(k)
Unit Ramp
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Convolution Property
Perhaps the most important property of the Z-transform, when it comes to solving  
difference equations, is the convolution property. Suppose h(k) and x(k) are zero extended, 
as needed, so they are defined for all k. By interchanging the order of the summations and 
using a change of variable, one can show that the Z-transform of the convolution of two 
signals is just the product of the Z-transforms of the signals. Using Definitions 2.3 and 3.1
Zhh(k) w x(k)j 5 o
`
k52`
fh(k) w x(k)gz2k
5 o
`
k52`3 o
`
i52`
h(i)x(k 2 i)4z2k
5 o
`
i52`
h(i) o
`
k52`
x(k 2 i)z2k
5 o
`
i52`
h(i) o
`
m52`
x(m)z2(m1i), m 5 k 2 i
 
5 o
`
i52`
h(i)z2i o
`
m52`
x(m)z2m 
 (3.3.9)
Thus convolution in the time domain maps into multiplication in the Z-transform domain, 
also called the complex frequency domain.
 
Zhh(k) w x(k)j 5 H(z)X(z)  
 (3.3.10)
Time Reversal Property
Another property that effectively expands the table of Z-transform pairs arises when we 
reverse time by replacing x(k) by x(2k).
Zhx(2k)j 5 o
`
k52`
x(2k)z2k
Convolution
Using the Z-scale property and the Z-transform of the unit ramp,
X(z) 5 Zhk(k)juz5zyc
5
z
(z 2 1)2uz5zyc
5
zyc
(zyc 2 1)2
5
cz
(z 2 c)2
Thus the Z-transform of a causal exponential with a linear coefficient is
Zhkck(k)j 5
cz
(z 2 c)2, uzu . ucu
Note that when c 5 1, this reduces to the Z-transform of the unit ramp.
3.3 Z-transform Properties    163
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

164    Chapter 3  Discrete-time Systems in the Frequency Domain
 5 o
2`
i5`
x(i)zi, i 5 2k
 
 5 o
`
i52`
x(i)(1yz)2i
 
 (3.3.11)
Thus reversing time is equivalent to taking the reciprocal of z.
 
Zhx(2k)j 5 X(1yz)  
 (3.3.12)
The time reversal property can be used, for example, to convert the Z-transform of a 
causal signal into the Z-transform of a noncausal signal. Note that to determine the 
region of convergence of the Z-transform of x(2k) we replace z by 1yz in the region of 
convergence of X(z).
Correlation Property
Convolution in the time domain maps into multiplication in the Z-transform or frequency 
domain. Since cross-correlation and convolution are generally similar operations, it is 
not surprising that the correlation operation simplifies in the frequency domain as well. 
To see this, let two finite signals y(k) and x(k) of length L and M, respectively, be zero- 
extended as needed so they are defined for all k. Recalling Definition 2.5, the cross- 
correlation of y(k) with x(k) can be expressed as follows.
ryx(k) 5 1
L o
`
i52`
y(i)x(i 2 k)
5 1
L o
`
i52`
y(k)xf2(k 2 i)g
 
5 y(k) w x(2k)
L
 
 (3.3.13)
Thus cross-correlation of y(k) with x(k) is just a scaled version of convolution of y(k) 
with x(2k). Using the convolution property, we then have
 
Zhryz(k)j 5 Y(z)Zhx(2k)j
L
 
 (3.3.14)
But x(2k) is the time-reversed version of x(k). Applying the time reversal property then 
yields the correlation property.
 
Zhryz(k)j 5 Y(z)X(1yz)
L
 
 (3.3.15)
Note that this is essentially the convolution property, but with scaling by 1yL and X(z) 
replaced by X(1yz).
3.3.2 Causal Properties
initial Value Theorem
The Z-transform properties considered thus far all expand the effective size of the table 
of Z-transform pairs. There are also some properties that can serve as partial checks on 
the validity of a computed Z-transform pair. Suppose the signal x(k) is causal. It then fol-
lows from Definition 3.1 that as z S ` all of the terms go to zero except for the constant 
Time reversal property
Correlation property
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

term that represents the initial value of a causal signal. This is referred to as the initial 
value theorem.
 
x(0) 5 lim 
zS` X(z)  
 (3.3.16)
Final Value Theorem
The initial value x(0) can be thought of as the value of a causal signal x(k) at one end 
of the time range. The value of x(k) at the other end of the range as k S ` also can be 
obtained from X(z), assuming it is finite. Consider the following function of z.
 
X1(z) 5 (z 2 1)X(z) 
 (3.3.17)
Suppose that the poles of X1(z) are all strictly inside the unit circle. Therefore X(z) has 
poles strictly inside the unit circle except for the possibility of a simple pole at z 5 1. The 
terms in x(k) associated with the poles inside the unit circle, even if they are multiple 
poles, will all decay to zero as k S `. This will be shown in Section 3.4. Thus the steady-
state solution as k S ` consists of either zero or a term associated with a pole at z 5 1. 
In Section 3.4 it is shown that the part of x(k) associated with a pole at z 5 1 is simply 
X1(1). Consequently, if (z 2 1)X(z) has all its poles strictly inside the unit circle, then from 
the final value theorem
 
x(`) 5 lim 
zS1 (z 2 1)X(z)  
 (3.3.18)
It is important to emphasize that the final value theorem is applicable only if the region 
of convergence of (z 2 1)X(z) includes the unit circle.
Initial value theorem
Final value theorem
EXAMPLE 3.8
Consider the Z-transform of a pulse of amplitude a and duration M that was 
previously computed in Example 3.6.
x(k) 5 bf(k) 2 (k 2 M)g
X(z) 5
b(zM 2 1)
zM 2 1(z 2 1)
Using (3.3.11), the initial value of x(k) is
x(0) 5 lim 
zS ` X(z)
5 lim
zS ` b(z
M 2 1)
zM21(z 2 1)
5 b
initial and Final Value Theorems
(Continued)
3.3 Z-transform Properties    165
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

166    Chapter 3  Discrete-time Systems in the Frequency Domain
Property
Description
Linearity
Zhax(k) 1 by(k)j 5 aX(z) 1 bY(z) 
Delay
Zhx(k 2 r)j 5 z2rX(z) 
Time multiplication
Zhkx(k)j 5 2zdX(z)
dz  
Time reversal 
Zhx(2k)j 5 X(1yz) 
Z-scale 
Zhakx(k)j 5 X(zya) 
Complex conjugate 
Zhx*(k)j 5 X*(z*)
Convolution 
Zhh(k) w x(k)j 5 H(z)X(z) 
Correlation 
Zhryx(k)j 5 Y(z)X(1yz)
L
 
Initial value 
x(0) 5 lim
zS`X(z) 
Final value 
x(`) 5 lim
zS1 (z 2 1)X(z),  stable  
Table 3.3: Z-transform 
Properties 
A summary of the basic properties of the Z-transform can be found in Table 3.3. 
Verification of the complex conjugate property is left as an exercise (see Problem 3.14).
inverse Z-transform
There are a number of application areas where it is relatively easy to find the Z-transform 
of the solution to a problem. To find the actual solution, x(k), we must return to the time 
domain by computing the inverse Z-transform of X(z).
 
x(k) 5 Z21hX(z)j 
 (3.4.1)
3.4.1 noncausal signals
Z-transforms that are associated with linear time-invariant systems take the form of a 
ratio of two polynomials in z. That is, they are rational polynomials in z. By convention, 
the denominator polynomial, a(z), is always normalized to make its leading coefficient one.
 
X(z) 5 b0zm 1 b1zm21 1 Á 1 bm
zn 1 a1zn21 1 Á 1 an
5 b(z)
a(z) 
 (3.4.2)
3.4
Inverse Z-transform
Normalized 
denominator
Similarly, using (3.3.13) and noting that all of the poles of (z 2 1)X(z) are inside 
the unit circle, the final value of x(k) is
x(`) 5 lim
zS1 (z 2 1)X(z)
 5 lim
zS1 b(z
M 2 1)
z
M21
 5 0
These values are consistent with x(k) being a pulse of amplitude b and duration M.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

If x(k) is a causal signal, then X(z) will be a proper rational polynomial where m # n. That 
is, the number of zeros of X(z) will be less than or equal to the number of poles. If m $ n, 
X(z) can be preprocessed by dividing the numerator polynomial b(z) by the denominator 
polynomial a(z) using long division. Recall from Section 2.7 that this produces a quotient 
polynomial Q(z) and a remainder polynomial R(z).
 
X(z) 5 Q(z) 1 R(z)
a(z)  
 (3.4.3)
The quotient polynomial will be of degree m 2 n and can be expressed
 
Q(z) 5 o
m2n
i50
qizi 
 (3.4.4)
For the special case when m 5 n, the quotient polynomial is simply the constant, b0. 
Comparing (3.4.4) with the power series in Definition 3.1, the inverse Z-transform of the 
quotient polynomial is
 
q(k) 5 o
m2n
i50
qi(k 1 i) 
 (3.4.5)
Thus Q(z) represents the anti-causal part of the signal, plus the constant term. The 
inverse Z-transform of the remaining part of X(z) in (3.4.3), which represents the part 
of the signal for k . 0, can be obtained using techniques presented in the remainder of 
Section 3.4. For the remainder of this section, we assume that X(z) represents a causal 
signal, or the causal part of a signal, which means that X(z) is given in (3.4.2) with m # n.
3.4.2 synthetic Division
synthetic Division Method
If only a finite number of samples of x(k) are required, then the synthetic division method 
can be used to invert X(z). First we rewrite (3.4.2) in terms of negative powers of z. 
Recalling that m # n, we can multiply the numerator and denominator by z2n, which yields
 
X(z) 5 z2r(b0 1 b1z21 1 Á bmz2m)
1 1 a1z21 1 Á 1 anz2n
 
 (3.4.6)
In this case r 5 n 2 m is the difference between the number of poles and the number of 
zeros. The basic idea behind the synthetic division method is to perform long division 
of the numerator polynomial by the denominator polynomial in (3.4.6) to produce an 
infinitely long quotient polynomial. Then X(z) can be written as
 
X(z) 5  z2r fq0 1 q1z21 1 q2z22 1 Á g 
 (3.4.7)
If q(k) is the signal whose kth sample is qk, then comparing (3.4.7) with Definition 3.1 it 
is clear that x(k) is q(k) delayed by r samples. Therefore the inverse Z-transform of X(z) 
using synthetic division is
 
x(k) 5 q(k 2 r)(k 2 r)  
 (3.4.8)
Proper rational 
polynomial
3.4 Inverse Z-transform    167
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

168    Chapter 3  Discrete-time Systems in the Frequency Domain
N 5 100; 
% number of points
delta 5 [1 ; zeros(N-1,1)]; 
% unit impulse input
x 5 filter(b,a,delta); 
% impulse response
stem(x) 
% plot x in discrete time
MATLAB Functions
MATLAB Function
impulse Response Method
In Section 3.7 it will be shown that there is a very easy numerical way to find a finite 
number of samples of the inverse Z-transform of X(z). It is based on an interpretation 
of x(k) as the impulse response of a discrete-time system with characteristic polynomial 
a(z) and input polynomial b(z). The impulse response is computed using the MATLAB 
function filter as follows.
EXAMPLE 3.9
As an illustration of the synthetic division method, consider the following 
Z-transform.
X(z) 5
z 1 1
z2 2 2z 1 3
There are n 5 2 poles and m 5 1 zeros. Multiplying the top and bottom by z22 
yields
X(z) 5
z21(1 1 z21)
1 2 2z21 1 3z22
Here r 5 n 2 m 5 1. Performing long division yields
1 1 3z21 1 3z22 2 3z23 2 15z24 1 Á
  1 2 2z21 1 3z22   1 1 z21
1 2 2z21 1 3z22
3z21 2 3z22
3z21 2 6z22 1 9z23
3z22 2 9z23
3z22 2 6z23 1 9z24
23z23 2 9z24
23z23 1 6z24 2 9z25
215z24 1 9z25
Applying (3.4.8) with r 5 1, the inverse Z-transform of X(z) is
x(k) 5  f0, 1, 3, 3, 23, 215, Á g, 0 # k # 5
synthetic Division
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.4.3 Partial Fractions
The synthetic division and impulse response methods are useful, but they have the draw-
back that they do not produce a closed-form expression for x(k) as a function of k. 
Instead, they generate numerical values for a finite number of samples of x(k). If it is 
important to find a closed-form expression for the inverse Z-transform of X(z), then the 
method of partial fraction expansion can be used.
simple Poles
The easiest way to find the inverse Z-transform of X(z) is to locate X(z) in a table of 
Z-transform pairs and read across the row to find the corresponding x(k). Unfortunately, 
for most problems of interest X(z) does not appear in the table. The basic idea behind 
partial fractions is to write X(z) as a sum of terms, each of which is in a Z-transform 
table. First consider the case when the n poles of X(z) are nonzero and simple. Then X(z) 
in (3.4.2) can be written in factored form as
 
X(z) 5
b(z)
(z 2 p1)(z 2 p2) Á (z 2 pn) 
 (3.4.9)
Next express X(z)yz using partial fractions.
 
X(z)
z
5 o
n
i50
Ri
z 2 pi
 
 (3.4.10)
In this case X(z)yz has n 1 1 simple poles. Coefficient Ri is Residue called the residue 
of X(z)yz at pole pi where p0 5 0 is the pole added by dividing X(z) by z. A general 
approach to find the residues is to put the n 1 1 partial fraction terms over a common 
denominator similar to (3.4.9) and then equate numerators. By equating the coefficients 
of like powers of z this results in n 1 1 equations in the n 1 1 unknown residues.
Although this general approach will work, it is usually simpler to compute each  
residue directly. Multiplying both sides of (3.4.10) by (z 2 pk) and evaluating the result 
at z 5 pk, we find that
 
Rk 5 (z 2 pk)X(z)
z
uz5pk, 0 # k # n  
 (3.4.11)
The term “residue” comes from the fact that Ri is what remain of X(z)yz at z 5 pi, 
after the pole at z 5 pi has been removed. Once the partial fraction representation in 
(3.4.10) is obtained, it is a simple matter to find the inverse Z-transform. First multiply 
both sides of (3.4.10) by z.
 
X(z) 5 R0 1 o
n
i51
Riz
z 2 pi
 
 (3.4.12)
Using the linearity property of the Z-transform and entries from Table 3.2, the inverse 
Z-transform of X(z) is
 
x(k) 5 R0(k) 13o
n
i51
Ri(pi)k4(k)  
 (3.4.13)
Simple poles
Residue
3.4 Inverse Z-transform    169
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

170    Chapter 3  Discrete-time Systems in the Frequency Domain
Multiple Poles
When the poles of X(z)yz include multiple poles, poles that occur more than once, the 
partial fraction expansion takes on a different form. Suppose p is a pole of multiplic-
ity m $ 1. Recall from Chapter 2 that this would generate a natural mode term in the 
zero-input response of the form c(k)pk(k) where c(k) is a polynomial of degree m 2 1. 
This type of term also appears in x(k). For example, suppose X(z) consists of a single 
nonzero pole at z 5 p1 repeated n times. Then the partial fraction expansion is
 
X(z)
z
5
R0
z 2 p0
1
c1
z 2 p1
1
c2
(z 2 p1)2 1 Á 1
cn
(z 2 p1)n 
 (3.4.14)
Residue R0 is associated with the pole at p0 5 0 and it can be found as before using 
(3.4.11). To find the coefficient vector c [ Rn, one can put the terms in (3.4.14) over a 
common denominator and equate numerators. By equating the coefficients of like powers 
of z, this results in n equations in the n unknowns c [ Rn.
Multiple poles
EXAMPLE 3.10
As an illustration of the partial fraction expansion method with simple poles, 
consider the following Z-transform.
X(z)
z
5
10(z2 1 4)
z(z2 2 2z 2 3)
5
10(z2 1 4)
z(z 1 1)(z 2 3)
Here X(z)yz has simple real poles at p0 5 0, p1 5 21, and p2 5 3. Using (3.4.11), 
the residues are
R0 5
10(z2 1 4)
(z 1 1)(z 2 3)u
z50
5 40
23
R1 5 10(z2 1 4)
z(z 2 3) u
z521
5 50
4
R2 5 10(z2 1 4)
z(z 1 1) u
z53
5 130
12
It then follows from (3.4.13) that a closed-form expression for the inverse Z-transform  
of X(z) is
x(k) 5 240
3
  (k) 13
25(21)k
2
1 65(3)k
6 4(k)
simple Poles: Partial Fractions
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

EXAMPLE 3.11
As an illustration of the partial fraction expansion method with multiple poles, 
consider the following Z-transform.
X(z)
z
5
2(z 1 3)
z(z2 2 4z 1 4)
5 2(z 1 3)
z(z 2 2)2
Here X(z)yz has a simple pole at p0 5 0 and a multiple pole of multiplicity m1 5 2 
at p1 5 2. From (3.4.14), the form of the partial fraction expansion is
X(z)
z
5
R0
z 1
c1
z 2 2 1
c2
(z 2 2)2
Using (3.4.11), the residue of the pole at p0 5 0 is
R0 5 2(z 1 3)
(z 2 2)2u
z 5 0
5 6
4 5 1.5
Next, substituting R0 5 1.5 and putting the terms of the partial fraction expan-
sion over a common denominator yields
X(z)
z
5
1.5(z 2 2)2 1 c1z(z 2 2) 1 c2z
z(z 2 2)2
5
1.5(z2 2 4z 1 4) 1 c1(z2 2 2z) 1 c2z
z(z 2 2)2
5
(1.5 1 c1)z2 1 (c2 2 2c1 2 6)z 1 6
z(z 2 2)2
The numerator of X(z)yz is b(z) 5 2z 1 6. Equating coefficients of like powers of 
z yields 6 5 6 and
1.5 1 c1 5 0
c2 2 2c1 1 6 5 2
From the first equation c1 5 21.5. It then follows from the second equation that 
c2 5 27. Multiplying both sides X(z)yz by z, the partial fraction expansion in this 
case is
X(z) 5 1.5 1 21.5z
z 2 2 1
27z
(z 2 2)2
5 1.5 2 1.5z
z 2 2 2 (3.5)2z
(z 2 2)2
Multiple Poles: Partial Fractions
(Continued)
3.4 Inverse Z-transform    171
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

172    Chapter 3  Discrete-time Systems in the Frequency Domain
In general X(z) may have both simple poles and multiple poles. In this case the  
partial fraction expansion of X(z)yz will be a combination of terms like those in (3.4.10) 
for the simple poles and those in (3.4.14) for the multiple poles with n 1 1 total terms. 
The residues of the simple poles can be computed directly using (3.4.11). To compute the 
remaining coefficients, one can put all the terms over a common denominator and then 
equate numerators.
Complex Poles
The poles of X(z) can be real or complex. If the signal x(k) is real, then complex poles 
appear in conjugate pairs. The techniques for simple and multiple poles can be applied 
to complex poles. When the poles appear as complex conjugate pairs, their residues 
will also be complex conjugates. The pair of terms in x(k) associated with a complex 
conjugate pair of poles can be combined using Euler’s identity, and the result will be 
a real damped sinusoidal term. Although this approach is certainly feasible, there is 
an alternative that avoids the use of complex arithmetic. When complex poles occur, 
it is often easier to work directly with real second-order terms. To illustrate, suppose 
X(z) consists of the following strictly proper (m , n) rational polynomial with poles at 
z 5 c exp(6jd).
 
X(z) 5
b0z 1 b1
fz 2 c exp( jd)gfz 2 c exp(2jd)g 
 (3.4.15)
Note that if X(z) starts out with a numerator of degree m 5 2, one can always perform  
a long division step to produce a constant quotient plus a remainder term of the form  
of (3.4.15).
The basic idea is to write X(z) as a linear combination of the damped sine and cosine 
terms from Table 3.2. Since the numerators of these terms do not contain any constant 
terms, this form can be achieved by multiplying and dividing X(z) by z.
 
X(z) 5 1
z
 5
b0z2 1 b1z
fz 2 c exp(jd)gfz 2 c exp(2jd)g6 
 (3.4.16)
Note that the 1yz factor can be regarded as a delay of one sample. Next consider the 
numerator in (3.4.16). To make the numerator of X(z) equal to a linear combination of 
the numerators of the damped sine and cosine terms in Table 3.2, we need to find f1 and f2 
such that
b0z2 1 b1z 5 f1fc sin(d)zg 1 f2fz 2 c cos(d)gz
 
5 f2z2 1 cf  f1 sin(d) 2 f2 cos(d)gz 
 (3.4.17)
Complex conjugate 
pair
From Table 3.2, the inverse Z-transform of X(z) is then
x(k) 5 1.5(k) 2 f1.5(2)k 1 3.5k(2)kg(k)
5 1.5(k) 2 (1.5 1 3.5k)2k(k)
Note that in this case the polynomial coefficient of the double pole at z 5 2 is 
c(z) 5 2(1.5 1 3.5z).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Equating coefficients of like powers of z yields f2 5 b0 and f1 5 fb1yc 1 f2 cos (d)gy
sin (d). Thus the weighting coefficients for the linear combination of sine and cosine  
terms are
 
f1 5 b1 1 cb0 cos(d)
 sin(d)
 
 (3.4.18a)
 
f2 5 b0 
 (3.4.18b)
Recalling the delay in (3.4.16) and using Table 3.2, the inverse Z-transform of the  
quadratic term is
 
x(k) 5 ck21h f1sinfd(k 2 1)g 1 f2cosfd(k 2 1)gj(k 2 1) 
 (3.4.19)
Notice from (3.4.19) that x(0) 5 0. This is consistent with the result obtained by applying 
the initial value theorem to X(z) in (3.4.15).
EXAMPLE 3.12
To illustrate the case of complex poles, consider the following Z-transform.
X(z) 5
3z 1 5
z2 2 4z 1 13
The poles of X(z) are at
p1,2 5 4 6 Ï16 2 4(13)
2
5 2 6 j3
5 c exp(6jd )
In polar coordinates, the magnitude c and phase angle d are
c 5 Ï4 1 9 5 3.61
d 5 arctan(3/2) 5 .983
Using (3.4.18), the weighting coefficients for the damped sine and cosine terms 
are
f1 5 5 1 3.61(3) cos(.983)
 sin(.983)
5 13.2
f2 5 3
Finally, from (3.4.19) the inverse Z-transform is
x(k) 5 3.61k21h13.2 sinf.983(k 2 1)g 1 3 cosf.983(k 2 1)gj(k 2 1)
Complex Poles: Table
3.4 Inverse Z-transform    173
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

174    Chapter 3  Discrete-time Systems in the Frequency Domain
3.4.4 Residue Method
The partial fraction method is effective for simple poles, but it can become cumbersome 
for multiple poles and furthermore it requires the use of a Z-transform table. There is an 
alternative method that does not require a table and can be less work for multiple poles. It 
is based on the following elegant formulation of the inverse Z-transform from the theory 
of functions of a complex variable.
 
x(k) 5 Z21hX(z)j 5 1
j2 $
C
X(z)zk 2 1dz  
 (3.4.20)
Here (3.4.20) is a contour integral where C is a counter-clockwise contour in the region 
of convergence of X(z) that encloses all of the poles. The inverse Z-transform formula-
tion in (3.4.20) is based on the Cauchy integral theorem (see Problem 3.28). Practical 
evaluation of x(k) in (3.4.20) is achieved by using the Cauchy residue theorem.
 
x(k) 5 o
q
i51
Res(pi, k) 
 (3.4.21)
Here pi for 1 # i # q are the q distinct poles of X(z)zk 2 1. The notation Res(pi, k) denotes 
the residue of X(z)zk 2 1 at the pole z 5 pi. To compute the residues it is helpful to first 
factor the denominator polynomial as follows.
 
X(z) 5
b(z)
(z 2 p1)m1(z 2 p2)m2 Á (z 2 pq)mq 
 (3.4.22)
Here pi is a pole of multiplicity mi for 1 # i # q. There are two cases. If pi is a simple pole 
with multiplicity mi 5 1, then the residue is what remains of X(z)zk 2 1 at the pole after the 
pole has been removed.
 
 Res (pi, k) 5 (z 2 pi)X(z)zk 2 1uz5pi   if   mi 5 1  
 (3.4.23)
Comparing (3.4.23) with (3.4.11), it is clear that for simple poles the residue method 
is the same amount of work as the partial fraction method where Ri 5
 Res (pi, 1). 
However there is no need to use a table in this case because Res(pi, k) includes the 
dependence on k.
The second case is for multiple poles. When pi is a pole of multiplicity mi . 1, the 
expression for the residue in (3.4.23) has to be generalized as follows.
 
 Res (pi, k) 5
1
(mi 2 1)! d mi21
dzmi21 h(z 2 pi)miX(z)zk 2 1juz5pi
   for   mi . 1   (3.4.24)
Thus the pole is again removed, but before the result is evaluated at the pole it is differen-
tiated mi 2 1 times and scaled by 1y(mi 2 1)!. Note that the expression for a multiple-pole 
residue in (3.4.24) reduces to the simpler expression for a simple-pole residue in (3.4.23) 
when mi 5 1 because 0! 5
D  
1.
The residue method requires the same amount of computational effort as the partial 
fraction method for the case of simple poles, but it requires less computational effort for 
multiple poles because there is only a single term associated with a multiple pole, not mi 
terms. In addition, the residue method does not require the use of a table.
The residue method does suffer from a drawback. Because X(z)zk 2 1 depends on k, there 
can be some values of k that cause a pole at z 5 0 to appear, and when this happens its resi-
due must be included in (3.4.21) as well. For example, if X(0) Þ 0, then X(z)zk 2 1 will have a 
Inverse Z-transform
Cauchy residue 
theorem
Multiplicity
Simple pole residue
Multiple-pole residue
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

pole at z 5 0 when k 5 0, but the pole disappears for k . 0. Fortunately, this problem can be 
easily resolved by separating x(k) into two cases, k 5 0 and k . 0. The initial value theorem 
can be used to compute x(0). Letting z S ` and recalling that m # n, this yields
 
x(0) 55
b0, m 5 n
0,
m , n
 
 (3.4.25)
Note that the initial value can be written compactly as x(0) 5 b0(n 2 m). The residue 
method is summarized in Algorithm 3.1.
1. Factor the denominator polynomial of X(z) as in (3.4.22).
2. Set x(0) 5 b0(n 2 m).
3. For i 5 1 to q do
h
 
If mi 5 1 then pi is a simple pole and
 Res (pi, k) 5 (z 2 pi)X(z)zk 2 1uz5pi
 
else pi is a multiple pole and
 Res (pi, k) 5
1
(mi 2 1)! dmi 2 1
dzmi 2 1h(z 2 pi)miX(z)zk 2 1juz5pi
j
4. Set
x(k) 5 x(0)(k) 13o
q
i51
 Res ( pi, k)4(k 2 1)
Initial value
ALGoRiTHM 
3.1 Residue Method
EXAMPLE 3.13
As a simple initial example, consider a Z-transform with two simple nonzero poles.
X(z) 5
z2
(z 2 a)(z 2 b)
The initial value of x(k) is x(0) 5 1. The two residues are
 Res (a, k) 5 z k 1 1
z 2 bu
z5a
5 a k 1 1
a 2 b
 Res (b, k) 5 z k11
z 2 au
z5b
5 b k11
b 2 a
Thus,
x(k) 5 x(0)(k) 1 fRes (a, k) 1
 Res (b, k)g(k 2 1)
5 (k) 11
ak 1 1 2 bk 1 1
a 2 b 2(k 2 1)
51
ak 1 1 2 bk 1 1
a 2 b 2(k)
simple Poles: Residue Method
3.4 Inverse Z-transform    175
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

176    Chapter 3  Discrete-time Systems in the Frequency Domain
There is a built-in MATLAB function available called residue that can be used to 
compute the residue terms for partial fraction expansions.
% RESIDUE: Compute residues and poles
%
% Usage:
%     [r,p,q] 5 residue (b,a);
% Pre:
%     b 5 vector of length m11 containing numerator  
% 
coefficients (m <5 n)
MATLAB Functions
MATLAB Function
EXAMPLE 3.14
Next consider a mixed case that has both simple and multiple poles.
X(z) 5
1
(z 2 a)2(z 2 b)
The initial value of x(k) is x(0) 5 0. The residue of the multiple pole at z 5 a is
 Res (a, k) 5 d
dz 5
zk 2 1
z 2 b6u z5a
5 (z 2 b)(k 2 1)zk22 2 zk21
(z 2 b)2
u
z5a
5 (a 2 b)(k 2 1)ak22 2 ak21
(a 2 b)2
5 f(a 2 b)(k 2 1) 2 agak22
(a 2 b)2
The residue of the simple pole at z 5 b is
 Res (b, k) 5
z k 2 1
(z 2 a)2u
z5b
5
bk 2 1
(b 2 a)2
Thus the inverse Z-transform of X(z) is
x(k) 5 x(0)(k) 1 f Res (a, k) 1 Res (b, k)g(k 2 1)
53
f(a 2 b)(k 2 1) 2 agak22
(a 2 b)2
1
bk21
(b 2 a)24(k 2 1)
53
f(a 2 b)(k 2 1) 2 agak22 2 bk21
(a 2 b)2
4(k 2 1)
Mixed Poles: Residue Method
(Continued    )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.5 Transfer Functions    177
Transfer Functions
3.5.1 The Transfer Function
The examples of linear discrete-time systems introduced thus far are all special cases of 
the following generic linear time-invariant difference equation, which we refer to as the 
system S.
 
y(k) 1 o
n
i51
ai y(k 2 i) 5 o
m
i50
bi x(k 2 i) 
 (3.5.1)
By convention, the coefficient of the current output, y(k), has been normalized to one by 
dividing both sides of the equation, if needed, by a0. Recall from Chapter 2 that the com-
plete solution to (3.5.1) depends on both the causal input x(k) and the initial condition, 
y0 5 f y(21), y(22), . . . , y(2n)gT. That is, in general, the output y(k) can be decomposed 
into the sum of two parts.
 
y(k) 5 yzi (k) 1 yzs (k) 
 (3.5.2)
The first term, yzi(k), is the zero-input response. It is the part of the output that is gen-
erated by the initial condition. When the input is zero, y(k) 5 ysi(k). For a system with 
stable natural modes
 
yzi (k) S 0 as k S ` 
 (3.5.3)
The term yzs(k) is the zero-state response. The zero-state response is the part of the output 
that is generated by the input x(k). When the initial condition is zero, y(k) 5 yzs(k). In view 
of (3.5.3), one can measure the zero-state response of a stable system by first waiting for 
yzi(k) to die out, and then exciting the system with the input and measuring the output.
The difference equation in (3.5.1) is merely one way to represent a discrete-time  
system. The following alternative representation, based on the Z-transform, is a more 
compact algebraic characterization.
3.5
Zero-input response
Zero-state response
%     a 5 vector of length n11 containing denominator  
% 
coefficients
% Post:
%     r 5 vector of length n containing the residues  
% 
R_i in (3.4.13)
%     p 5 vector of length n containing the poles
%     q 5 residue R_0 in (3.4.11).
% Note:
%     If X(z) contains multiple poles, then the  
%     corresponding elements of r are the coefficients
%     c_i in (3.4.14)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

178    Chapter 3  Discrete-time Systems in the Frequency Domain
Figure 3.10: 
Transfer Function 
Representation
H(z)
y(k)
x(k)
Let x(k) be a nonzero input to the system S, and let y(k) be the output assuming  
the initial condition is zero. Then the transfer function of the system is defined
H(z) 5
D  Y(z)
X(z)
The transfer function is simply the Z-transform of the zero-state response divided by the 
Z-transform of the input. The transfer function, H(z), is a concise algebraic representa-
tion of the system. By multiplying both sides of the expression for H(z) by X(z) we get the 
frequency-domain input-output representation
 
Y(z) 5 H(z)X(z)  
 (3.5.4)
A block diagram which shows the relationship between the input, the output, and the 
transfer function of a discrete-time system is shown in Figure 3.10.
The transfer function of the system S can be determined using the time shift property 
of the Z-transform. Taking the Z-transform of both sides of (3.5.1) yields
 
Y(z) 1 o
n
i51
aiz2iY(z) 5 o
m
i50
biz2iX(i) 
 (3.5.5)
Factoring Y(z) from the left-hand side and X(z) from the right-hand side then yields
 
11 1 o
n
i51
aiz2i2Y(z) 51o
m
i50
biz2i2X(z) 
 (3.5.6)
Finally, solving for Y(z)yX(z) produces the following transfer function for the discrete-
time system S.
 
H(z) 5 b0 1 b1z21 1 Á 1 bmz2m
1 1 a1z21 1 Á 1 anz2n
 
 (3.5.7)
Comparing (3.5.7) with (3.5.1), it is apparent that the transfer function can be obtained 
directly from inspection of the difference equation. The transfer function representation 
in (3.5.7) is in terms of z21. To convert it to positive powers of z, we multiply the top and 
bottom by z n.
 
H(z) 5 z n 2 m(b0z m 1 b1z m21 1 Á 1 bm)
z n 1 a1z n21 1 Á 1 an
 
 (3.5.8)
Note that when m # n, the transfer function H(z) can have n 2 m zeros at z 5 0 and that 
when m . n, it can have m 2 n poles at z 5 0.
DEFiniTion
3.2 Transfer Function
Frequency-domain 
representation
Transfer function
Inspection
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.5 Transfer Functions    179
3.5.2 Zero-state Response
One of the useful features of the transfer function representation is that it provides us 
with a simple means of computing the zero-state response of a discrete-time system to 
an arbitrary input x(k). Taking the inverse Z-transform of both sides of (3.5.4) yields the 
following formulation of the zero-state response.
 
yzs(k) 5 Z21hH(z)X(z)j  
 (3.5.9)
Consequently, if the initial condition is zero, the output of the system is just the inverse 
Z-transform of the product of the transfer function and the Z-transform of the input. 
The following example illustrates this technique for computing the output.
EXAMPLE 3.15
EXAMPLE 3.16
As a simple illustration of computing the transfer function, consider the following  
discrete-time system.
y(k) 5 1.2y(k 2 1) 2 .32y(k 2 2) 1 10x(k 2 1) 1 6x(k 2 2)
Using (3.5.7), we see from inspection that
H(z) 5
10z21 1 6z22
1 2 1.2z21 1 .32z22
Note that when the terms corresponding to delayed outputs are on the right-hand 
side of the difference equation, the signs of the coefficients must be reversed in 
the denominator of H(z). To reformulate H(z) in terms of positive powers of z, 
we multiply the numerator and the denominator by z2, which yields the positive- 
power form of the transfer function.
H(z) 5
10z 1 6
z2 2 1.2z 1 .32
Consider the discrete-time system from Example 3.15. Suppose the input is the 
unit step x(k) 5 (k) and the initial condition is zero. Then the Z-transform of 
the output is
Y(z) 5 H(z)X(z)
51
10z 1 6
z2 2 1.2z 1 .322
 
z
z 2 1
5
(10z 1 6)z
(z2 2 1.2z 1 .32)(z 2 1)
Transfer Function
Zero-state Response
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

180    Chapter 3  Discrete-time Systems in the Frequency Domain
0
5
10
15
20
25
30
35
40
45
0
20
40
60
80
100
120
140
160
180
200
k
y(k)
Figure 3.11: Zero-state Output for Example 3.16
To find y(k) we invert Y(z), using the residue method in Algorithm 3.1. The 
denominator of Y(z) is already partially factored. Applying the quadratic for-
mula, the remaining two roots are
p1,2 5 1.2 6 Ï1.44 2 1.28
2
5 1.2 6 .4
2
5 h.8, .4j
The initial value of the output is
y(0) 5 b0(m 2 n) 5 0
The residues of the three poles are
 Res (.8, k) 5
(10z 1 6)zk
(z 2 .4)(z 2 1)u
z5.8
5 14(.8)k
.4(2.2) 5 2175(.8)k
 Res (.4, k) 5
(10z 1 6)zk
(z 2 .8)(z 2 1)u
z5.4
5
10(.4)k
2.4(2.6) 5 41.7(.4)k
 Res (1, k) 5
(10z 1 6)zk
(z 2 .8)(z 2 .4)u
z51
5
16
.2(.6) 5 133.3
Finally, the zero-state response is
y(k) 5 y(0)(k) 1 f Res (.8, k) 1
 Res (.6, k) 1
 Res (1, k)g (k 2 1)
 5 f133.3 2 175(.8)k 1 41.7(.4)kg (k 2 1)
 5 f133.3 2 175(.8)k 1 41.7(.4)kg (k)
A plot of the step response, generated by running exam3_16, is shown in Figure 3.11.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.5 Transfer Functions    181
3.5.3 Poles, Zeros, and Modes
Recall that the poles and zeros of a signal were defined earlier in Section 3.2. This con-
cept also can be applied to discrete-time systems using the transfer function. Using the  
positive-power version of H(z) in (3.5.8) and factoring the numerator and denominator, 
this results in the following factored form of the transfer function.
 
H(z) 5 b0zn2m(z 2 z1)(z 2 z2) Á (z 2 zm)
(z 2 p1)(z 2 p2) Á (z 2 pn)
 
 (3.5.10)
The roots of the numerator polynomial are called the zeros of the discrete-time sys-
tem, and the roots of the denominator polynomial are called the poles of the system. 
Recall that the multiplicity of a pole or zero corresponds to the number of times it 
appears.
Factored form
Zeros, poles
EXAMPLE 3.17
Consider the home mortgage system discussed in Case Study 2.2. The difference 
equation for this system is
y(k) 5 y(k 2 1) 11
r
122y(k 2 1) 2 x(k)
Thus the transfer function of this system is
H(z) 5
21
1 2 (1 1 ry12)z21 5
2z
z 2 (1 1 ry12)
The home mortgage system has one zero at z 5 0 and one pole at z 5 1 1 ry12 
where r is the annual interest rate, expressed as a fraction.
Poles and Zeros
Poles and zeros have simple interpretations in the time domain as well. Suppose 
Y(z) 5 H(z)X(z). Then the output y(k) can be decomposed into the sum of two types of 
terms called modes.
 
y(k) 5
 natural modes 1
 forced modes  
 (3.5.11)
Each natural mode term is generated by a pole of H(z), while each forced mode term  
is generated by a pole of the input or forcing function X(z). For a simple pole at p, the 
corresponding mode is of the form cpk for some constant c. More generally, if p is a pole 
of multiplicity r, then the mode is of the form
 
multiple mode 5 (c0 1 c1k 1 Á 1 cr21kr21)pk(k), r $ 1 
 (3.5.12)
Thus a multiple pole is similar to a simple pole, except that the coefficient is a polynomial 
in k of degree r 2 1. For a simple pole, r 5 1, and the coefficient reduces to a polynomial 
of degree zero, a constant. Interpretation of poles of Y(z) as modes of y(k) is useful 
because it allows us to write down the form of y(k), showing the dependence on k, directly 
from inspection of the factored form of Y(z). For example, if H(z) has poles at pi for 
Multiple mode
Natural mode
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

182    Chapter 3  Discrete-time Systems in the Frequency Domain
1 # i # n and X(z) has poles at qi for 1 # i # r, and if all the poles are simple, then the 
form of y(k) is
 
y(k) 5 o
n
i51
cipk
i (k) 1 o
r
i51
diqk
i (k) 
 (3.5.13)
The first sum in (3.5.13) represents the natural modes, and the second sum represents the 
forced modes. Of course if there are multiple poles, including those caused by poles of 
X(z) matching those of H(z), then there will be fewer terms in (3.5.13), but some of the 
terms will have polynomial coefficients.
The zeros of H(z) and X(z) also have simple interpretations in terms of y(k). Since 
Y(z) 5 H(z)X(z), there is a possibility of pole-zero cancellation between H(z) and X(z). 
For example, if H(z) has a zero at z 5 q and X(z) has a pole at z 5 q, then the pole 
at z 5 q will not appear in Y(z), which means that there is no corresponding forced-
mode term in y(k). That is, the zeros of H(z) can suppress certain forced-mode terms and  
prevent them from appearing in y(k).
Similarly, with a judicious choice for x(k) one can suppress natural-mode terms in 
y(k). In particular, if H(z) has a pole at z 5 p and X(z) has a zero at z 5 p, then the  
natural-mode term generated by the pole of H(z) will not appear in the output y(k). 
Finally, if a pole of X(z) matches a pole of H(z), then this effectively increases the multi-
plicity of the pole in Y(z), a phenomenon known as harmonic forcing. We illustrate these 
ideas with the following example.
poles of H(z)
5
poles of X(z)
5
Pole-zero cancellation
Harmonic forcing
EXAMPLE 3.18
Consider the discrete-time system discussed in Example 3.16. The factored form 
of its transfer function is
H(z) 5
10(z 1 .6)
(z 2 .8)(z 2 .4)
Thus there will be up to two natural mode terms in y(k). Next consider the fol-
lowing input signal:
x(k) 5 10(2.6)k(k) 2 4(2.6)k21(k 2 1)
Using the properties of the Z-transform and Table 3.2, the Z-transform is
X(z) 5 101
z
z 1 .62 2 4z211
z
z 1 .62
5 10z 2 4
z 1 .6
5 10(z 2 .4)
z 1 .6
Cancelled Modes
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.5 Transfer Functions    183
stable Modes
It is instructive to look at what happens to a multiple mode term in the limit as k S `. 
A multiple mode term can be written as c(k)pk(k), where c(k) is a polynomial of degree 
r 2 1 with r being the multiplicity of the pole. For r . 1, the polynomial factor will  
satisfy uc(k)u S ` as k S `. However, if upu , 1, the exponential factor will satisfy 
upku S 0 as k S `. To determine the value of the product c(k)pk in the limit as k S `, 
first note that
 lim
kS ` uc(k)pku 5 lim
kS ` uc(k)u
upu2k
 
 5 lim
k S ` 
uc(k)u
 expf2k log (upu)g
 
 (3.5.14)
To evaluate the limit, L’Hospital’s rule must be applied. The (r 2 1)th derivative of the 
polynomial c(k) in (3.5.12) is the constant (r 2 1)! cr 2 1. The (r 2 1)th derivative of 
the exponential is f2log (u pu)g r21 expf2k log (u pu)g. Since  log (upu) , 0, by L’Hospital’s 
rule the limit is
 
lim
k S ` c(k)p
k 5 0 3 upu , 1 
 (3.5.15)
Consequently, the exponential factor, p k, goes to zero faster than the polynomial factor 
c(z) goes to infinity if and only if upu , 1. Note that (3.5.15) is the justification for the 
claim about yzi(k) in (3.5.3). In summary, a stable mode is a mode that is associated with 
a pole that lies inside the unit circle.
3.5.4 DC Gain
When the poles of the system all lie strictly inside the unit circle, it is clear from (3.5.15) 
that all of the natural mode terms decay to zero with increasing time. In this case, it is 
possible to determine the steady-state response of the system to a step input directly from 
Stable mode
Consequently, there is potentially one forced-mode term in y(k). Using (3.5.9), 
the zero-state response is
y(k) 5 Z21hH(z)X(z)j
5 Z215
100
z 2 .86
5 y(0)(k) 1
 Res (.8, k)(k 2 1)
5 100(.8)k21(k 2 1)
For this particular input, neither the forced mode generated by the pole of X(z) at 
z 5 2.6 nor the natural mode generated by the pole of H(z) at z 5 .4 appears in 
the zero-state output y(k) due to pole-zero cancellation.
Note that if we applied an input of the form x(k) 5 .8k(k), then this would 
create a double pole in Y(z) at z 5 .8, which is an instance of harmonic forcing. 
Inputs which cause harmonic forcing can sometimes generate large responses. 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

184    Chapter 3  Discrete-time Systems in the Frequency Domain
H(z). Suppose the input is a step of amplitude c. Then using (3.5.4) and the final value 
theorem
lim
kS` y(k) 5 lim
zS1 (z 2 1)Y(z)
5 lim
zS1 (z 2 1)H(z)Zhc(k)j
5 lim
zS1  (z 2 1)H(z)1
cz
z 2 12
 
5 H(1)c 
 (3.5.16)
Thus the steady-state response to a step of amplitude c is the constant H(1)c. A unit step 
input can be regarded as a cosine input x(k) 5 cos(2F0k)(k) whose frequency happens 
to be F0 5 0 Hz. That is, a step or constant input is actually a DC input. The amount by 
which the amplitude of the DC input is scaled as it passes through the system to produce 
the steady-state output is called the DC gain of the system. From (3.5.16), it is evident 
that
 
 DC gain 5 H(1)  
 (3.5.17)
Consequently, if the poles of the transfer function H(z) are all strictly inside the unit 
circle, the DC gain of the system is simply H(1). In Section 3.8, we will see that the gain 
of the system at other frequencies also can be obtained by evaluating H(z) at appropriate 
values of z.
DC gain
EXAMPLE 3.19
As an illustration of poles, zeros, and DC gain, consider the following transfer 
function where 0 , r , 1.
H(z) 5
zn
zn 2 rn
This is the transfer function of a comb filter, a system that is discussed in detail in  
Chapter 7. Note that H(z) has n poles and n zeros. The zeros are all at the origin, 
while the n poles are spaced equally around a circle of radius r. A plot of uH(z)u 
for the case n 5 6 and r 5 0.9 is shown in Figure 3.12. Note the placement of the 
six poles (the plot is clipped at uH(z)u # 2) and the multiple zero at the origin. The 
DC gain of the comb filter is
 DC gain 5 H(1) 5
1
1 2 r n 5 2.134
Comb Filter
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.6 Signal Flow Graphs    185
signal Flow Graphs
Engineers often use diagrams to visualize relationships between variables and sub-
systems. In Chapter 2 this type of visualization was represented using block diagrams.  
Discrete-time systems also can be represented efficiently using a special type of diagram 
called a signal flow graph. A signal flow graph is a collection of nodes interconnected by 
arcs. Each node is represented with a dot, and each arc is represented with a directed line 
segment. An arc is an input arc if it enters a node and an output arc if it leaves a node. 
Each arc transmits or carries a signal in the direction indicated. When two or more input 
arcs enter a node, their signals are added together. That is, a node can be thought of as a 
summing junction with respect to its inputs. An output arc carries the value of the signal 
leaving the node. When an arc is labeled, the labeling indicates what type of operation is 
performed on the signal as it traverses the arc. For example, a signal might be scaled by a 
constant or, more generally, acted on by a transfer function. As a simple illustration, con-
sider the signal flow graph shown in Figure 3.13 that consists of four nodes and three arcs.
Observe that the node labeled x has two input arcs and one output arc. Thus the value 
of the signal leaving the node is x 5 au 1 bv. When an arc is not labeled, the default gain 
is assumed to be one. Therefore, the value of the signal at the output node is
y 5 x
 
5 au 1 bv 
 (3.6.1)
3.6
Node, arc
Label
22
21
0
1
2
22
21
0
1
2
0
0.5
1
1.5
2
Re(z)
Im(z)
|b(z)/a(z)|
Figure 3.12: Magnitude of Comb Filter Transfer Function with n 5 6 and r 5 0.9
Figure 3.13: A 
Simple Signal  
Flow Graph
u
x
v
y
a
b
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

186    Chapter 3  Discrete-time Systems in the Frequency Domain
Figure 3.15: Signal 
Flow Graph of  
a Discrete-time  
System, 
m 5 n 5 3
x
r
y
2a1
b1
b0
z21
z21
z21
2a3
b2
2a2
b3
In order to develop a signal flow graph for the discrete-time system S, we first factor 
the transfer function H(z) into a product of two transfer functions as follows.
H(z) 5 b0 1 b1z21 1 Á bmz2m
1 1 a1z21 1 Á 1 anz2n
 51
b0 1 b1z21 1 Á bmz2m
1
21
1
1 1 a1z21 1 Á 1 anz2n2
 
 5 Hb(z)Ha(z)
 
 (3.6.2)
Here H(z) consists of two subsystems in series, one associated with the numerator poly-
nomial and the other with the denominator polynomial as shown in Figure 3.14. The 
intermediate variable, R(z) 5 Ha(z)X(z), is the output from the first subsystem and the 
input to the second subsystem.
Subsystem Ha(z) processes input x(k) to produce an intermediate variable r(k), and 
then subsystem Hb(z) acts on r(k) to produce the output y(k). The decomposition in  
Figure 3.14 can be represented by the following pair of difference equations.
 
r(k) 5 x(k) 2 o
n
i51
air(k 2 i) 
 (3.6.3a)
 
y(k) 5 o
m
i50
bi r(k 2 i) 
 (3.6.3b)
Given the decomposition in (3.6.3), the entire system can be represented with a signal flow 
graph as shown in Figure 3.15, which illustrates the case m 5 n 5 3. The output of the top 
center node is r(k), and the nodes below it produce delayed versions of r(k). The left-hand 
side of the signal flow graph ladder is a feedback system that implements (3.6.3a) to pro-
duce r(k), while the right-hand side is a feed forward system that implements (3.6.3b) to pro-
duce y(k). In the general case, the order of the signal flow graph will be M 5  max hm, nj. 
If m Þ n, then some of the arcs will be missing or labeled with gains of zero.
Figure 3.14: Decomposition of Transfer Function H(z)
x(k)
1
1 1 a1z21 1 · · · 1 anz2n
r(k)
y(k)
b0 1 b1z21 1 · · · 1 bmz2m
1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.6 Signal Flow Graphs    187
Figure 3.16: Signal Flow Graph of System in Example 3.20
x
r
y
1.8
2.9
0
2.4
1.6
z21
z21
EXAMPLE 3.20
Consider a discrete-time system with the following transfer function.
H(z) 5
2.4z21 1 1.6z22
1 2 1.8z21 1 .9z22
By inspection of Figure 3.15, it is evident that the signal flow graph of this system 
is as shown in Figure 3.16. Note how the gain of one of the arcs on the right-hand 
side is zero due to a missing term in the numerator. Given H(z), the difference 
equation of this system is
y(k) 5 2.4x(k 2 1) 1 1.6x(k 2 2) 1 1.8y(k 2 1) 2 .9y(k 2 2)
signal Flow Graph
Note that we now have four distinct ways to represent a system. In the time domain 
there are the difference equation and the impulse response, in the frequency domain there 
is the transfer function, and in the graphical domain there is the signal flow graph, as 
shown in Figure 3.17. With some care, it should be possible to go from any one of these 
representations to another, usually by inspection.
Transfer
function
Difference
equation
Signal
ﬂow
graph
Figure 3.17:  
Alternative Ways 
to Represent a  
Discrete-time 
System
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

188    Chapter 3  Discrete-time Systems in the Frequency Domain
ARMA Models
The transfer function of the first subsystem in Figure 3.14 is a special case of the follow-
ing structure.
 
HAR(z) 5
b0
1 1 a1z21 1 Á 1 anz2n  
 (3.6.4)
The system HAR(z), whose numerator is constant, is called an auto-regressive or AR 
model. Similarly, the second subsystem in Figure 3.14 has the following structure.
 
HMA(z) 5 b0 1 b1z21 1 Á 1 bmz2m  
 (3.6.5)
System HMA(z), whose denominator is one, is called a moving average or MA model. The 
name “moving average” arises from the fact that if bi 5 1y(m 1 1), then the output is a 
running average of the last m 1 1 samples. More generally, if the bi are not equal, then 
HMA(z) represents a weighted moving average of the last m 1 1 samples.
The general case of a transfer function H(z) for the system S in (3.6.2) is called an  
auto-regressive moving average or ARMA model. From Figure 3.14 a general ARMA 
model can always be factored into a product of an AR model and a MA model.
 
HARMA(z) 5 HAR(z)HMA(z) 
 (3.6.6)
For the signal flow graph representation in Figure 3.15, the left side of the ladder is the 
AR part, and the right side of the ladder is the MA part. Note that the MA model in 
(3.6.5) is the FIR system discussed in Chapter 2. Both the AR model in (3.6.4) and the 
ARMA model in (3.6.2) are examples of IIR systems, with the AR model being a special 
case. An application where an AR model arises is the model of the human vocal tract that 
was introduced in Section 3.1.2.
stability in the Frequency Domain
3.7.1 input-output Representations
Recall from Chapter 2 that the zero-state response of a system can be represented in the 
time domain as the convolution of the impulse response h(k) with the input x(k).
 
y(k) 5 h(k) w x(k) 
 (3.7.1)
This is sometimes referred to as the input-output representation of the system S in the 
time domain. An important property of the Z-transform is that convolution in the time 
domain maps into multiplication in the frequency domain. Taking the Z-transform of 
both sides of (3.7.1) leads to the input-output representation in the frequency domain.
 
Y(z) 5 H(z)X(z) 
 (3.7.2)
Suppose the input is the unit impulse x(k) 5 (k). Then in the time domain the result-
ing output is the impulse response y(k) 5 h(k). But in the frequency domain X(z) 5 1, so 
AR model
MA model
ARMA model
3.7
Input-output 
representation
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.7 Stability in the Frequency Domain    189
the resulting output in the frequency domain is Y(z) 5 H(z). It follows that the transfer 
function is just the Z-transform of the impulse response or, putting it another way, the 
impulse response is
 
h(k) 5 Z21hH(z)j  
 (3.7.3)
That is, the Z-transform of the impulse response is the transfer function, and the 
inverse Z-transform of the transfer function is the impulse response. In effect, h(k) and 
H(z) are equivalent representations of the system S, one in the time domain and the  
other the frequency domain. The relationship between h(k) and H(z) is summarized in 
Figure 3.18.
In view of (3.7.3), one way to find the inverse Z-transform of, say, X(z) is to regard 
X(z) as a transfer function of a linear time-invariant system. The inverse transform x(k) 
is then just the impulse response of this system. The impulse response can be computed 
numerically using the MATLAB filter function. This is the basis of the impulse response 
method for numerically inverting the Z-transform presented in Section 3.4.
3.7.2 BiBo stability
Practical discrete-time systems, particularly digital filters, tend to have one qualitative fea-
ture in common: they are stable. Recall from Chapter 2 that a system S is BIBO stable if and 
only if every bounded input is guaranteed to produce a bounded output. Proposition 2.1 
provided a simple time-domain criterion for stability. A system is BIBO stable if and only 
if the impulse response is absolutely summable. There is an equivalent and even simpler test 
for stability in the frequency domain. Consider an input signal x(k) and its Z-transform.
 
X(z) 5
d(z)
(z 2 q1)n1(z 2 q2)n2 Á (z 2 qr)nr 
 (3.7.4)
To determine the conditions under which this signal is bounded, first note that each pole 
qi of X(z) generates a mode or term in x(k) of form ci (k)q k
i , where ci(k) is a polynomial 
whose degree is one less than the multiplicity ni of the pole. From (3.5.15) these modes 
goes to zero as k S ` if and only if the poles are strictly inside the unit circle. Therefore 
the terms of x(k) associated with poles inside the unit circle are bounded. Any pole strictly 
outside the unit circle generates a mode that grows and therefore an unbounded x(k).
Impulse response
Impulse response 
method
Figure 3.18: Two 
Input-output  
Representations  
of the Linear  
System S
x(k)
h(k)
Convolution
y(k)
Z
X(z)
Y(z)
H(z)
Z21
Z21
Multiplication
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

190    Chapter 3  Discrete-time Systems in the Frequency Domain
21.5
21
20.5
0
0.5
1
1.5
21.5
21
20.5
0
0.5
1
1.5
Re(z)
Im(z)
Figure 3.19: Region 
of Stable Poles
Next consider poles on the unit circle. For simple poles on the unit circle the coeffi-
cient polynomial ci(k) is constant, and the magnitude of the pole is uqiu 5 1, so the modes 
associated with these poles are also bounded although they do not go to zero. Finally, 
multiple poles on the unit circle have polynomial coefficients that grow with time. Since 
uqk
iu 5 1, these modes generate an unbounded x(k). In summary, a signal is bounded if 
and only if its Z-transform has poles inside or on the unit circle with the poles on the unit 
circle being simple poles.
Next consider the output of the system. If the transfer function H(z) has poles at pi 
for 1 # i # n, then Y(z) can be expressed
 
Y(z) 5
b(z)d(z)
(z 2 p1)m1 Á (z 2 pu)mu(z 2 q1)n1 Á (z 2 qr)nr 
 (3.7.5)
Suppose x(k) is bounded. Then y(k) will be bounded if the poles of H(z) are all 
strictly inside the unit circle. However, y(k) will not be bounded for an arbitrary bounded 
x(k) if H(z) has a pole outside the unit circle or on the unit circle. The latter case can 
cause an unbounded y(k) because even a simple pole of H(z) on the unit circle could be 
matched by a simple pole of X(z) at the same location (harmonic forcing), thus creating a 
double pole and an unstable mode. These observations lead to the following fundamental 
frequency-domain stability criterion.
A linear time-invariant system S with transfer function H(z) is BIBO stable if and only 
if the poles of H(z) satisfy
upi u , 1, 1 # i # n
The poles of a stable system, whether simple or multiple, must all lie strictly inside the 
unit circle as shown in Figure 3.19. Recall from (3.5.15) that this is equivalent to saying 
that all of the natural modes of the system must decay to zero.
PRoPosiTion
3.1 stability in the  
Frequency Domain
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.7 Stability in the Frequency Domain    191
FiR systems
From the time-domain analysis in Chapter 2 it was determined that there was one class 
of systems that was always stable: FIR systems.
 
y(k) 5 o
m
i50
bix(k 2 i) 
 (3.7.6)
When the unit impulse input is applied, this results in the following FIR impulse response.
 
h(k) 5 o
m
i50
bi(k 2 i) 
 (3.7.7)
From Proposition 2.1, a system is BIBO stable if and only if the impulse response h(k) is 
absolutely summable. Since the impulse response of an FIR system is, by definition, finite, it 
is always absolutely summable. Therefore all FIR systems are BIBO stable. Applying the Z- 
transform to both sides of (3.7.7) and using the time shift property, the FIR transfer function is
H(z) 5 o
m
i50
bi z2i
 
5 1
zmo
m
i50
bi zm 2 i 
 (3.7.8)
Here all the poles of H(z) are well within the unit circle, namely at z 5 0. It follows from 
Proposition 3.1 that FIR systems are always BIBO stable.
iiR systems
IIR systems may or may not be stable, depending on the coefficients of the denominator 
polynomial.
FIR transfer function
Unstable Transfer Function
EXAMPLE 3.21
Consider a discrete-time system with the following transfer function.
H(z) 5
10
z 1 1
This system has a single pole on the unit circle at z 5 21. Since it is not strictly 
inside, from Proposition 3.1 this is an example of an unstable system. When a 
discrete-time system has one or more simple poles on the unit circle and the rest 
of its poles are inside the unit circle, it is sometimes referred to as a marginally 
unstable system. For a marginally unstable system there are natural modes that 
neither grow without bound nor decay to zero. From Algorithm 3.1, the impulse 
response of the system H(z) is
h(k) 5 h(0)(k) 1
 Res (21, k)(k 2 1)
 510(21)k21(k 2 1)
Thus the pole at z 5 21 produces a term or mode that oscillates between 61. 
Since this system is unstable, there must be at least one bounded input that produces 
an unbounded output. Consider the following input that is bounded by Bx 5 1.
x(k) 5 (21)k(k)
Marginally unstable
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

192    Chapter 3  Discrete-time Systems in the Frequency Domain
The Z-transform of the zero-state response is
Y(z) 5 H(z)X(z)
5
10z
(z 1 1)2
Applying Algorithm 3.1, the zero-state response is
y(k) 5  Z21hY(z)j
5  y(0)(k) 1
 Res (21, k)(k 2 1)
5  10k(21)k21(k 2 1)
5  10k(21)k21(k)
Clearly, y(k) is not bounded. Note that the forced mode of X(z) at z 5 21 exactly 
matches the marginally unstable natural mode of H(z) at z 5 21. This causes Y(z) 
to have a double pole at z 5 21 thereby generating a mode that grows. Again, 
this phenomenon of driving a system with one of its natural modes is known as  
harmonic forcing. It tends to elicit a large response because the input reinforces 
the natural motion of the system.
Harmonic forcing
3.7.3 The Jury Test
The stability criterion in Proposition 3.1 provides us with a simple and easy way to test 
for stability. In particular, if a 5 f1, a1, a2, Á , angT denotes the (n 1 1) 3 1  coefficient  
vector of the denominator polynomial of H(z), then the following MATLAB command 
returns the radius of the largest pole.
r 5 max(abs(roots(a))); 
% radius of largest pole
In this case the system is stable if and only if r , 1, that is, if and only if the region of conver-
gence of H(z) includes the unit circle. In some instances, a system has one or more unspeci-
fied design parameters, and we want to determine a range of values for the parameters over 
which the system is stable. To see how this can be done, let a(z) denote the denominator 
polynomial of the transfer function H(z) where it is assumed, for convenience, that a0 . 0.
 
a(z) 5 a0zn 1 a1zn 2 1 1 Á 1 an 
 (3.7.9)
The objective is to determine if a(z) is a stable polynomial, a polynomial whose roots  
lie strictly inside the unit circle. There are two simple necessary conditions that a stable 
polynomial must satisfy, namely
 
a(1) . 0 
 (3.7.10a)
 
(21)na(21) . 0 
 (3.7.10b)
If the polynomial in question violates either (3.7.10a) or (3.7.10b), then there is no need 
to test further because it is known to be unstable.
Suppose a(z) passes the necessary conditions in (3.7.10), which means that it is a 
viable candidate for a stable polynomial. We then construct a table from the coefficients 
of a(z) known as a Jury table. The case n 5 3 is shown in Table 3.4.
Observe that the first two rows of the Jury table are obtained directly from inspection 
of the coefficients of a(z). The rows appear in pairs, with the even rows being reversed 
MATLAB 
Functions
Stable polynomial
Necessary conditions
Jury table
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.7 Stability in the Frequency Domain    193
versions of the odd rows immediately above them. Starting with row three, the odd rows 
are constructed using 2 3 2 determinants as follows.
 
b0 5 1
a0u
a0
a3
a3
a0u, b1 5 1
a0u
a0
a2
a3
a1u, b2 5 1
a0u
a0
a1
a3
a2u 
 (3.7.11a)
 
c0 5 1
b0u
b0
b2
b2
b0u, c1 5 1
b0u
b0
b1
b2
b1u 
 (3.7.11b)
 
d0 5 1
c0u
c0
c1
c1
c0u 
 (3.7.11c)
As a matter of convenience, any odd row can be scaled by a positive value. Once the Jury 
table is constructed, it is a simple matter to determine if the polynomial is stable. The 
polynomial a(z) is stable if the first entry in each odd row of the Jury table is positive.
 
a0 . 0, b0 . 0, c0 . 0, Á  
 (3.7.12)
Table 3.4: The Jury 
Table, n 5 3 
Row 
Coefficients 
1
a0
 a1 
 a2 
 a3 
2 
a3 
 a2 
 a1 
 a0 
3 
b0 
 b1 
 b2 
 
4 
b2 
 b1 
 b0 
 
5 
c0 
 c1 
 
 
6 
c1 
 c0 
 
 
7 
d0 
 
 
 
8 
d0 
 
 
 
Jury Test
EXAMPLE 3.22
Consider the following general second-order discrete-time system with design param-
eters a1 and a2. Suppose there is no pole-zero cancellation between a(z) and b(z).
H(z) 5
b(z)
z2 1 a1z 1 a2
For this system
a(z) 5 z2 1 a1z 1 a2
The first two rows of the Jury table are
J2 53
1
a1
a2
a2
a1
14
Applying (3.7.11), the elements in the third row are
b0 5 u
1
a2
a2
1u 5 1 2 a2
2
b1 5 u
1
a1
a2
a1u 5 a1(1 2 a2)
From the stability condition b0 . 0 we have
ua2u , 1
(Continued   )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

194    Chapter 3  Discrete-time Systems in the Frequency Domain
Given b0 and b1, the first four rows of the Jury table are
J4 53
1
a1
a2
a2
a1
1
1 2 a2
2
a1(1 2 a2)
a1(1 2 a2)
a 2 a2
2 4
From (3.7.11), the element in the fifth row of the Jury table is
c0 5
1
1 2 a2
2u
1 2 a2
2
a1(1 2 a2)
a1(1 2 a2)
1 2 a2
2 u
5
(1 2 a2
2)2 2 a2
1(1 2 a2)2
1 2 a2
2
5
(1 2 a2)2f(1 1 a2)2 2 a2
1g
1 2 a2
2
Since ua2u , 1, the denominator and the first factor in the numerator are both  
positive. Thus the condition c0 . 0 reduces to 
(1 1 a2)2 . a2
1
We can break this inequality into two cases. If a1 $ 0, then (1 1 a2) . a1 or
a2 . a1 2 1, a1 $ 0
In the a2 versus a1 plane, a2 must be above a line of slope 1 and intercept 21 when 
a1 $ 0. If a1 , 0, then 1 1 a2 . 2a1 or
a2 . 2a121, a1 , 0
Here a2 must be above a line of slope 21 and intercept 21 when a1 , 0. Adding the 
constraint, ua2u , 1, this generates the stable region in parameter space shown in  
Figure 3.20. As long as the two coefficients lie in the shaded region known as the 
stability triangle, the second-order system will be BIBO stable.
Stability triangle
22.5 22 21.5 21 20.5
0
0.5
1
1.5
2
2.5
21.5
21
20.5
0
0.5
1
1.5
a1
a2
Real poles
Complex poles
Figure 3.20: Stable Parameter Region for a Second-order System
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.8 Frequency Response     195
The parabolic line in Figure 3.20 that separates real poles from complex poles is 
obtained by factoring the second-order denominator polynomial a(z) 5 z2 1 a1z 1 a2, 
which yields poles at
 
p1,2 5
2a1 6 Ïa2
1 2 4a2
2
 
 (3.7.13)
When 4a2 . a2
1 the poles go from real to complex. Thus the parabola a2 5 a2
1y4 separates 
the two regions in Figure 3.20. Recall that stable real poles generate natural modes in 
the form of damped exponentials. A stable complex conjugate pair of poles generates a 
natural mode that is an exponentially damped sinusoid. Although the stability result in 
Figure 3.20 only applies to a second-order systems, it is quite useful because a practical 
way to implement a higher-order filter with transfer function H(z) is as a product of  
second-order factors Hi(z) and, if needed, one first-order factor.
 
H(z) 5 H1(z)H2(z) Á Hr(z) 
 (3.7.14)
This is called the cascade form, and it is discussed in Chapter 7. This type of imple-
mentation has practical value because as the filter order increases the locations of the 
poles can become highly sensitive to small errors in the polynomial coefficients. As a 
result, a direct form implementation of a high-order IIR filter can sometimes become 
unstable due to finite precision effects, whereas the cascade form implementation may 
remain stable.
Frequency Response 
3.8.1 Frequency Response
The spectral characteristics or frequency content of a signal x(k) can be modified in a 
desired manner by passing x(k) through a linear discrete-time system to produce a second 
signal y(k), as shown in Figure 3.21. In this case we refer to the system that processes x(k) 
to produce y(k) as a digital filter.
Typically signal x(k) will contain significant power at certain frequencies and less 
power or no power at other frequencies. The distribution of power across frequencies is 
called the power density spectrum. A digital filter is designed to reshape the spectrum of 
the input by removing certain spectral components and enhancing others. The manner in 
which the spectrum of x(k) is reshaped is specified by the frequency response of the filter.
Let H(z) be the transfer function of a stable linear system S, and let T be the sampling 
interval. The frequency response of the system is denoted H(f) and defined
H( f  ) 5
D  H(z)uz5exp ( j2f T ), 0 # u f u # fsy2
Cascade form
3.8
DEFiniTion
3.3  Frequency Response
Figure 3.21: Digital 
Filter with Transfer 
Function H(z)
Digital
ﬁlter
x(k)
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

196    Chapter 3  Discrete-time Systems in the Frequency Domain
Figure 3.22:  
Evaluation of 
the Frequency 
Response along 
uzu 5 1. (The  
exterior labels 
denote fractions  
of fyfs)
Re(z)
Im(z)
1
21
1
0
23/8
21/4
21/8
1/8
3/8
1/4
61/2
21
As the frequency f  ranges over the interval f2fsy2, fsy2g, the argument  
z 5  exp( j2fT ) traces out the unit circle in a counterclockwise sense as shown in  
Figure 3.22. In other words, the frequency response is just the transfer function H(z) 
evaluated along the unit circle. Recall that the upper frequency limit, fsy2, represents an 
upper bound on frequencies that can be present in xa(t) without aliasing. In Chapter 1 
we referred to fd 5 fsy2 as the folding frequency. For H( f  ) to be well defined, the region 
of convergence of H(z) must include the unit circle. This will be the case as long as the 
system S is BIBO stable.
A comment about notation is in order. Note that the same base symbol, H, is being 
used to denote both the transfer function, H(z), and the frequency response, H( f  ), two 
quantities that are distinct but related. An alternative approach would be to introduce 
separate symbols for each. However, the need for separate symbols will arise repeatedly, 
and using distinct symbols in each instance quickly leads to a proliferation of symbols 
that can become confusing in its own right. Instead, we will adopt the convention that the 
argument type, a complex z or a real f , will be used to dictate the meaning of H. Some 
authors use the notation H(e j) for the frequency response, where  5 2f  and T 5 1.
The frequency response is defined over the range f2fsy2,  fsy2g, where negative fre-
quencies correspond to the bottom half of the unit circle and positive frequencies the top 
half. However, if H(z) is generated by a difference equation with real coefficients, then all 
of the information about H(f) is contained in the positive frequency range f0, fsy2g. More 
specifically, if the coefficients of H(z) are real, then the frequency response satisfies the 
following symmetry property where H *( f  ) denotes the complex-conjugate of H( f  ).
 
H(2f  ) 5 H *(f  ), 0 # u f u # fsy2  
 (3.8.1)
Since H( f  ) is complex, it can be written in polar form as H( f  ) 5 A( f  ) expf( f  )g. 
The magnitude of H( f  ) is called the magnitude response of the filter.
 
A( f  ) 5
D  Ï Re 2fH( f  )g 1
 Im 2 fH( f  )g, 0 # u f u # fsy2 
 (3.8.2)
notation!
Symmetry property
Magnitude response
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.8 Frequency Response     197
Applying the symmetry property in (3.8.1) yields
A(2f ) 5 uH(2f )g
5 uH*( f )u
5 uH( f )u
 
5 A( f ) 
 (3.8.3)
Consequently, the magnitude response of a real filter is an even function of f  with 
A(2f ) 5 A( f ). The phase angle ( f ) is called the phase response of the filter.
 
( f ) 5
D  tan215
 Im fH( f )g
 Re fH( f )g6, 0 # u f  u # fsy2 
 (3.8.4)
In this case applying the symmetry property in (3.8.1) yields
(2f ) 5 /H(2f )
5 /H*( f )
5 2/H( f )
 
5 2( f ) 
 (3.8.5)
Consequently, the phase response of a real filter is an odd function of f  with 
(2f ) 5 2( f ).
3.8.2 sinusoidal inputs
There is a simple physical interpretation of the frequency response in terms of inputs and 
outputs. To see this, it is helpful to first examine the steady-state response of the system 
S to the following complex sinusoidal input.
x(k) 5 fcos(2f kT ) 1 j sin(2kf T )g(k)
 5 exp( j2ktT )(k)
 5 fexp( j2f T gk(k)
 
 5 pk(k)
 
 (3.8.6)
Thus the complex sinusoid is really a causal exponential input with a complex exponen-
tial factor p 5 exp(  j2f T ). If the system S is stable, then all of the natural modes will 
go to zero as k S `. Hence the steady-state response will consist of the forced mode 
associated with the pole of X(z) at p 5 exp(  j2f T ). Applying Algorithm 3.1, the steady-
state response is
 
yss(k) 5
 Res (p, k) 
 (3.8.7)
The Z-transform of x(k) is X(z) 5 zy(z 2 p). Since H(z) is stable and upu 5 1, the pole at 
z 5 p is a simple pole, and the residue is
 Res (p, k) 5 (z 2 p)Y(z)zk21uz5p
5 (z 2 p)H(z)X(z)zk21uz5p
5 H(z)zkuz5p
 
5 H(p)pk 
 (3.8.8)
Phase response
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

198    Chapter 3  Discrete-time Systems in the Frequency Domain
Substituting (3.8.8) into (3.8.7) with p 5 exp(2f T ) and using the polar form of H( f ), 
we have
 yss(k) 5 H(p)pk
 5 H( f ) exp(  j2kf T )
 5 A( f ) expf j( f )g exp(  j2kf T )
 
 5 A( f ) expf  j2kf T 1 ( f )g
 
 (3.8.9)
Consequently, when a complex sinusoidal input of frequency f  is applied to a stable 
system S, the steady-state response is also a complex sinusoidal signal of frequency f , 
but its amplitude has been scaled by A( f ) and its phase angle has been shifted by 
( f ). The steady-state response to a real sinusoidal input behaves in a similar manner. 
Suppose
x(k)  5  cos(2kf T )
 
 5 .5fexp(  j2kf T ) 1  exp(2j2f T )g 
 (3.8.10)
Since the system S is linear, using the identities in Appendix 2 the steady-state response 
to x(k) is
yss(k) 5  .5fH( f ) exp(  j2kf T ) 1 H(2f ) exp(2j 2kf T )g
5  .5hH( f ) exp(  j2kf T ) 1 fH( f ) exp(  j2f T )g*j
5  
 Re hH( f ) exp(  j2kf T )j
5  
 Re hA( f ) expf  j2kf T 1 ( f )gj
 
5  A( f ) cosf2kfT 1 ( f )g
 
 (3.8.11)
Thus a real sinusoidal input behaves in the same way as a complex sinusoidal input 
when it is processed by the system S.
Let H( f ) 5 A( f ) expf j( f )g be the frequency response of a stable linear system S, 
and let x(k) 5 cos(2kf T ) be a sinusoidal input where T is the sampling interval and 
0 # u f u # fsy2. The steady-state response to x(k) is
yss(k) 5 A( f ) cosf2f kT 1 ( f )g
In view of Proposition 3.2, the magnitude response and the phase response have 
simple interpretations which allow them to be measured directly. The magnitude response 
A( f ) specifies the gain of the system at frequency f , the amount by which the sinusoidal 
input x(k) is amplified or attenuated. The phase response ( f ) specifies the phase shift of 
the system at frequency f , the number of radians by which the sinusoidal input x(k) is 
advanced if positive or delayed if negative.
By designing a filter with a prescribed magnitude response, certain frequencies  
can be removed, and others enhanced. The design of digital FIR filters is the focus of 
Chapter 6, and the design of digital IIR filters is the focus of Chapter 7.
PRoPosiTion
3.2  Frequency Response
Gain
Phase shift
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.8 Frequency Response     199
Frequency Response
EXAMPLE 3.23
As an example of computing the frequency response of a discrete-time system, 
consider a second-order digital filter with the following transfer function.
H(z) 5
z 1 1
z2 1 .64
Let  5 2f T. Then using Definition 3.3 and Euler’s identity, the frequency 
response is
H( f ) 51
z 1 1
z2 2 .642u
z5  exp (j)
5
 exp(j) 1 1
 exp(j2) 2 .64
5
 cos() 1 1 1 j sin()
 cos(2) 2 .64 1 j sin(2)
The magnitude response of the filter is
A( f ) 5 uH( f )u
 5 Ïfcos(2f T ) 1 1g2 1 sin2(2f T )
Ïfcos(4f T ) 2 .64g2 1 sin2(4f T )
The phase response of the filter is
( f ) 5 /H( f )
 5  tan213
 sin(2fT )
 cos(2fT ) 1 14 2  tan213
 sin(4f T )
 cos(4f T ) 2 .644
Plots of the magnitude response and the phase response for the case fs 5 2000 Hz 
are shown in Figure 3.23. Only the positive frequencies are plotted because S is 
a real system. Notice that frequencies near f 5 500 Hz are enhanced by the filter, 
whereas the frequency component at f 5 1000 Hz is eliminated.
0
200
400
600
800
1000
0
1
2
3
4
f (Hz)
A(f)
Figure 3.23: Frequency Response of Filter in Example 3.24
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

200    Chapter 3  Discrete-time Systems in the Frequency Domain
3.8.3 Periodic inputs
The expression for the steady-state output in Proposition 3.2 can be generalized to  
periodic inputs. Suppose a continuous-time signal xa(t) is periodic with period T0 and 
fundamental frequency F0 5 1yT0. The signal xa(t) can be approximated with a truncated 
Fourier series as follows.
 
xa(t) < d0
2 1 o
M
i51
di cos(2iF0t 1 i) 
 (3.8.12)
Let ci denote the ith complex Fourier coefficient of xa(t). That is,
 
ci 5 1
T0#
T0y2
2T0y2
xa(t) exp(2j2iF0t)dt, i $ 0 
 (3.8.13)
From Appendix 1, the magnitude di and phase angle i of the ith harmonic of xa(t) 
can be obtained from ci as di 5 2uciu and i 5 /ci, respectively. Next let the samples  
x(k) 5 xa(kT) be a discrete-time input to a stable linear system with transfer function H(z).
 
x(k) 5 d0
2 1 o
M
i51
di cos(2iF0kT 1 i)  
 (3.8.14)
Since the system H(z) is linear, it follows that the steady-state response to a sum of inputs 
is just the sum of the steady-state responses to the individual inputs. Setting f 5 iF0 in 
Proposition 3.2, we find that the steady-state response to  cos(2iF0 kT 1 i) is scaled 
by A(iF0) and shifted in phase by (iF0). Thus the steady-state response to the sampled 
periodic input x(k) is
 
yss(k) 5 A(0)d0
2
1 o
M
i51
A(iF0)di cosf2iF0kT 1 i 1 (iF0)g  
 (3.8.15)
It should be pointed out that there is a practical upper limit on the number of  
harmonics M. The sampling process will introduce aliasing if there are harmonics located 
at or above the folding frequency, fsy2. Therefore, for (3.8.15) to be valid, the number of 
harmonics must satisfy
 
M , fs
2F0
 
 (3.8.16)
0
200
400
600
800
1000
24
22
0
2
4
f (Hz)
(f)
Figure 3.23: (Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.8 Frequency Response     201
steady-state Response
EXAMPLE 3.24
As an illustration of using (3.8.15) to compute the steady-state response, consider 
the following stable first-order filter.
H(z) 5
0.2z
z 2 0.8
Let  5 2fT. Then from Definition 3.3 and Euler’s identity, the frequency 
response of this filter is
 H( f ) 5
0.2z
z 2 0.8u z5 exp(j)
 5
0.2 exp(  j)
 exp(  j) 2 0.8
 5
0.2 exp(  j)
 cos() 2 0.8 1 j sin()
The filter magnitude response is
A( f ) 5 uH( f )u
 5
0.2
Ïfcos(2f T ) 2 0.8g2 1 sin2(2f T )
The filter phase response is
( f ) 5 /H( f )
 5 2f T 2  tan213
 sin(2f T )
 cos(2f T ) 2 0.84
Next suppose the input xa(t) is an even periodic pulse train of period T0 where 
the pulses are of unit amplitude and radius 0 # a # T0y2. From the Fourier series 
table in Appendix 1, the truncated Fourier series of xa(t) is
xa(t) < 2a
T0
1 4a
T0o
M
i51
 sinc (2iF0a) cos(2iF0t)
Recall from Chapter 1 that 
 sinc (x) 5 sin(x)y(x). Suppose the period is 
T0 5 .01  s and a 5 T0y4, which corresponds to a square wave with fundamental 
frequency F0 5 100 Hz. If we sample at fs 5 2000 Hz, then to avoid aliasing the 
M harmonics must be below 1000 Hz. Thus from (3.8.16), the maximum number 
of harmonics used to approximate xa(t) should be M 5 9. The sampled version 
of xa(t) is then
x(k) 5 1
2 1 o
9
i51
 sinc 1
i
22 cos(.1ik)
(Continued   )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

202    Chapter 3  Discrete-time Systems in the Frequency Domain
Finally, from (3.8.15) the steady-state response to the periodic input x(k) is
yss(k) 5 1
2 1 o
9
i51
A(100i) sinc 1
i
22 cosf.1ik 1 (100i)g
Plots of x(k) and yss(k), obtained by running exam3_25 from the driver program  
g_dsp, are shown in Figure 3.24. The oscillation or ringing in the approximation 
to the square wave is caused by the fact that only a finite number of harmonics are 
used (Gibb’s phenomenon). The steady-state output is much smoother than x(k) 
due to the low-pass nature of the filter H(z).
0
5
10
15
20
25
30
35
40
45
20.5
0
0.5
1
1.5
k
x(k)
0
5
10
15
20
25
30
35
40
45
20.5
0
0.5
1
1.5
k
yss(k)
Figure 3.24: Steady-state Response to Periodic Pulse Train
The DSP Companion contains the following function for finding the frequency 
response of a stable linear discrete-time system.
% F_FREQZ: Compute the frequency response of a discrete-time  
% 
 system
%
% Usage:
% 
[H,f] 5 f_freqz (b,a,N,fs);
% Pre:
DSP Companion
DsP Companion
(Continued   )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 3.9 System Identification    203
system identification
Sometimes the desired input-output behavior of a system is known, but the parameters 
of the transfer function, difference equation, or signal flow graph are not. For example, 
a segment of speech can be measured as in Section 3.1.2, but the model of the system 
that produced that speech segment is unknown. The problem of identifying a transfer 
function H(z) from input and output measurements is referred to as system identification. 
Here the system S can be thought of as a black box as shown in Figure 3.25. The box is 
black in the sense that it is not possible to see, or know with certainty, the details of what 
is inside.
An effective way to identify a suitable transfer function is to first assume a particular 
form or structure for the black-box model. For example, one could use a MA model, an 
AR model, or an ARMA model. Once a model is selected, a suitable order for the model 
must be determined. If the selected structure is sufficiently general for the problem at 
 3.9
Optional material
System identification
*
% 
b  
5 numerator polynomial coefficient vector
%    a  
5 denominator polynomial coefficient vector
%    N  
5 frequency precision factor: df 5 fs/N
%    fs  
5 optional sampling frequency (default 5 1)
% Post:
%    H 5 1 by N11 complex vector containing discrete
%  
frequency response
%    f 5 1 by N11 vector containing discrete
%  
frequencies at which H is evaluated
% Notes:
%    1. The frequency response is evaluated along the
%  
top half of the unit circle. Thus f ranges
%  
from 0 to fs/2.
%
%    2. H(z) must be stable. Thus the roots of a(z) must
%  
lie inside the unit circle.
Note that f_freqz is the discrete-time version of the continuous-time function f_freqs 
introduced in Chapter 1. To compute and plot the magnitude response and phase 
response, the following standard MATLAB functions can be used.
A 5 abs(H);  
% magnitude response
subplot(2,1,1)  
% place above
plot (f,A)  
% magnitude response plot
phi 5 angle(H);  
% phase response
subplot(2,1,2)  
% place below
plot (f,phi)   
% phase response plot
Figure 3.25: A 
Black-box System 
to Be Identified
Black
box
x(k)
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

204    Chapter 3  Discrete-time Systems in the Frequency Domain
hand and the model order is sufficiently large, then it should be possible to closely fit the 
model to the data.
3.9.1 Least-squares Fit
For this introduction to system identification, we employ an ARMA model structure 
because this is sufficiently general to include both the auto-regressive (AR) model and 
the moving average (MA) model as special cases. In Chapter 9, the question of system 
identification is revisited, this time in the context of adaptive MA models. Recall that an 
ARMA model has a transfer function that can be expressed as follows.
 
H(z) 5 b0 1 b1z21 1 Á 1 bmz2m
1 1 a1z21 1 Á 1 anz2n  
 (3.9.1)
System identification is based on a set of input-output measurements. Usually these 
are real measurements, but in some cases they can represent the desired behavior of a 
fictitious system. Suppose the following input-output data are available.
 
D 5 hfx(k), y(k)g [ R2 u 0 # k , Nj 
 (3.9.2)
The difference equation associated with H(z) is
 
y(k) 5 o
m
i50
bi x(k 2 i) 2 o
n
i51
ai y(k 2 i) 
 (3.9.3)
To develop a compact solution for the unknown parameter vectors a [ Rn and b [ Rm11, 
it is helpful to reformulate the expression for y(k). To that end, let p 5 n 1 m 1 1 be the 
number of unknown parameters. Define a state vector q(k) [ Rp and a composite param-
eter vector c [ Rp as follows.
 
q(k) 5
D   
f2y(k 2 1), . . . , 2y(k 2 n), x(k), . . . , x(k 2 m)gT, 0 # k , N  (3.9.4)
 
c 5
D  
fa1, . . . , an, b0, . . . , bmgT 
 (3.9.5)
It then follows from (3.9.4) and (3.9.5) that the output y(k) in (3.9.3) can be written com-
pactly as
 
y(k) 5 cTq(k), 0 # k , N 
 (3.9.6)
The N equations in (3.9.6) can be recast as a single vector equation. In partic-
ular, define an N 3 p coefficient matrix Q and an N 3 1 right-hand side vector y as 
follows.
 
Q 5
D  3
q1(0)
q2(0)
Á
qp(0)
q1(1)
q2(1)
Á
qp(1)
o
o
∞
o
q1(N)
q2(N)
Á
qp(N)4
, y 5
D  3
y(0)
y(1)
o
y(N)4
 
 (3.9.7)
Note that the kth row of the matrix Q is qT(k), where the x(k) and y(k) in q(k) are taken 
from the input-output measurements in D. Similarly, y is the sequence of measured output 
State, parameter 
vectors
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 3.9 System Identification    205
samples from D. The scalar cTq(k) in (3.9.6) can be replaced by its transpose, qT(k)c. 
It then follows from (3.9.7) that the N equations in (3.9.6) can be expressed in vector 
form as
 
Qc 5 y 
 (3.9.8)
When the number of measurements N matches the number of parameters p, the 
matrix Q is square. If the square matrix Q is nonsingular, then the parameter vector c is 
unique and can be solved for as c 5 Q21y. Although this will cause the ARMA model to 
fit the input-output data exactly, it is typically not an effective practical solution, because 
it is valid only for the N samples in D. Experience shows that it is unlikely that the fit will 
be acceptable for a new input x(k).
A better approach is to use a long recording of input-output data (N W p) that is 
representative of the overall behavior of the unknown system. When N . p, the system 
of N equations in the p unknowns becomes an over-determined linear algebraic system, a 
system that may not have a solution because there are more constraints than unknowns. In 
this case, there will be a nonzero residual error vector that represents the difference between  
Qc and y.
 
r(c) 5
D  Qc 2 y 
 (3.9.9)
Since it is typically the case that the norm uur(c)uu . 0, we seek to find a c [ Rp that 
will minimize uur(c)uu. Using the square of the Euclidean norm, this leads to the least-
squares error criterion.
 
EN(c) 5
D  
uur(c)uu2 5 o
N21
i50
r2
i (c) 
 (3.9.10)
To minimize the scalar EN(c), consider the p 3 N matrix QT, which denotes the  
transpose of Q. Multiplying both sides of (3.9.8) on the left by QT yields
 
QTQc 5 QTy 
 (3.9.11)
Note that QTQ is a square p 3 p matrix. Suppose the input and output samples in D 
are such that the p columns of the coefficient matrix Q are linearly independent, which 
means that Q is of full rank. Then the square matrix QTQ will be nonsingular. Multiplying 
both sides of (3.9.11) on the left by the inverse then produces a least-squares parameter  
vector, c.
 
c 5 (QTQ)21QTy  
 (3.9.12)
The matrix Q1 5 (QTQ)21QT is called the pseudo-inverse or Moore-Penrose inverse of 
Q (Noble, 1969). From linear algebra, the parameter vector c obtained by multiplying 
by the pseudo-inverse is a least-squares solution of (3.9.8). More specifically, among all 
vectors c [ Rp that minimize EN(c), the vector c in (3.9.12) is the one whose norm uucuu is 
smallest. It should be pointed out that if the input-output data D are generated by a highly 
underdamped system and n W 1, then some of the poles of H(z) will be close to the unit 
circle and they may migrate over into the unstable region as a result of finite precision 
computations.
Residual error
Least squares
Full rank
Pseudo-inverse
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

206    Chapter 3  Discrete-time Systems in the Frequency Domain
Figure 3.26: Least-squares Error EN(c), for the System in Example 2.26 Using 
N 5 100 Points and n 5 m
0
0.5
1
1.5
2
2.5
3
3.5
4
0
5
10
15
20
25
30
35
40
45
50
n, m
EN(c)
N 5 100  
system identification
EXAMPLE 3.25
As an illustration of least-squares system identification of an ARMA model, sup-
pose the input-output data D are generated by using N 5 100 samples from the 
following black-box system.
H
 data (z) 5
2.8z 1 .64
z2 2 1.2728z 1 .81
Let the input be white noise uniformly distributed over the interval f21, 1g. Since the 
order of the underlying black-box system is normally not known, it can be useful to 
perform a system identification using different values for the ARMA model order n, 
say with m 5 n. A plot of the error EN for 0 # n # 4 obtained by running exam3_26 
is shown in Figure 3.26.
Although (3.9.12) is a convenient way to express the optimal parameter vector, to 
actually compute c it is not necessary to form QTQ, compute the inverse, and multi-
ply. Instead, the following MATLAB command using the left division or backslash 
operator can be used to find the least-squares solution to an over-determined system.
c 5 Q\y;   
% Least-squares solution of Qc 5 y
MATLAB Functions
MATLAB Functions
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 3.9 System Identification    207
3.9.2 Persistently Exciting inputs
When first encountered, the adjectives “persistent” and “exciting” might seem a bit 
strange when applied to an input signal, but both have important practical meanings. 
They are associated with the idea that the input signal generate outputs that are suffi-
ciently “rich” to cause the coefficient matrix Q in (3.9.7) to have full rank. To see how a 
problem might arise, suppose the input is constant with x(k) 5 d for 0 # k , N. Then 
from (3.9.4), the last m 1 1 columns of Q will be linearly dependent, in which case Q is 
not of full rank. More generally, suppose the input x(k) has the following Z-transform.
 
X(z) 5
d(z)
(z 2 r1)(z 2 r2) Á (z 2 rn) 
 (3.9.13)
If the poles of X(z) are all strictly inside the unit circle then, since S is stable, both the 
input x(k) and th output y(k) will decay to zero.
 
ux(k)u 1 uy(k)u S 0 as  k S ` 
 (3.9.14)
Figure 3.27: Comparison of the Magnitude and Phase Responses of the 
Black-box System of Example 3.26 and the ARMA Model of Order n 5 m 5 2
0
0.1
0.2
0.3
0.4
0.5
0
2
4
6
A(f)
Black-box system
ARMA model
0
0.1
0.2
0.3
0.4
0.5
21
0
1
2
f/fs
(f)
Black-box system
ARMA model
f/fs
Notice how the error decreases with increasing n, showing an improved fit 
between the ARMA model and the black-box system. When n 5 m $ 2, the error 
goes to zero as expected. This is also evident from a comparison of the two fre-
quency responses when n 5 m 5 2 which is shown in Figure 3.27.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

208    Chapter 3  Discrete-time Systems in the Frequency Domain
The DSP Companion contains a function called f_idarma for performing system 
identification using an ARMA model. When n 5 0 or m 5 0, it can be used to  
identify an AR model or an MA model, respectively.
% F_IDARMA: Identify ARMA model using least squares fit
%
%  
Y(z)    b(1) 1 b(2)z^(-1) 1 ... 1 b(m+1)z^(2(m11))
%  
–––– 5 –––––––––––––––––––––––––––––––––––––––––––
%  
X(z)    a(1) 1 a(2)z^(-1) 1 ... 1 a(n+1)z^(2(n11))
%
% Usage:
%  
[b,a,e] 5 f_idarma (u,y,m,n)
% Pre:
%  
x 5 p by 1 vector containing input samples
%  
y 5 p by 1 vector containing output samples
%  
m 5 number of zeros (m >5 0)
%  
n 5 number of poles (n >5 0)
% Post:
%  
b 5 1 by (m+1) vector of numerator coefficients
DSP Companion
DsP Companion
In this case the bottom rows of Q begin to approach rows of zeros, so increasing the num-
ber of samples N does not contribute to improving the estimate of c. This is because the 
forced modes in y(k) generated by the transient input x(k) do not persist in time, but they 
die out. To generate forced modes that persist but do not grow, S must be stable, and the 
poles of X(z) must be simple poles on the unit circle that produce sinusoidal terms in y(k). 
For example, poles at r1, 2 5  exp(6  j2fT) produce a steady-state sinusoidal term in y(k) 
of frequency f . Notice that this suggests that the input x(k) should be a power signal 
rather than an energy signal.
In order to get an accurate value for c, the other thing that is needed is to excite all of 
the natural modes of the system Exciting with the input. This can be achieved by driving 
the system with an input that contains many frequencies spread over the range f0, fsy2g. 
Although this can be done with an input consisting of several sinusoids, a particularly 
effective input is a white noise input. As we shall see in Chapter 4, a white noise input is 
a broadband signal whose power is spread evenly over all frequencies. This is why a white 
noise input was used in Example 3.26.
When an expensive industrial system needs to be identified, for example one that is 
being used in a manufacturing process, it may not be feasible to take the system offline to 
generate the input-output data D. In instances like these, the system can remain in oper-
ation and be identified online by superimposing a small white noise signal on top of the 
nominal input signal. Thus if u(k) is the input required for normal operation and v(k) is 
a small white noise signal, then
 
x(k) 5 u(k) 1 v(k) 
 (3.9.15)
The discussion of persistently exciting inputs presented here is a brief informal intro-
duction intended to make the reader aware that care must be taken in selecting the input. 
A more formal and detailed presentation of persistent excitation can be found, for exam-
ple, in (Haykin, 2002).
Persist
Excite
Online identification
(Continued   )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.10 GUI Modules and Case Studies    209
GUi Modules and Case studies
This section focuses on applications of discrete-time systems. A graphical user interface 
module called g_sysfreq is introduced that allows user to explore the input-output behav-
ior of linear discrete-time systems in the frequency domain without any need for pro-
gramming. Case study programming examples using the DSP Companion functions are 
then presented.
g_sysfreq: Discrete-time system Analysis in the Frequency Domain
The DSP Companion includes a GUI module called g_sysfreq that allows the user to 
investigate the input-output behavior of linear discrete-time systems in the frequency 
domain. GUI module g_sysfreq features a display screen with tiled windows as shown in 
Figure 3.28. The design features of g_sysfreq are summarized in Table 3.5.
The Block Diagram window in the upper-left corner of the screen contains a color-
coded block diagram of the system under investigation. Below the block diagram are a 
number of Edit boxes whose contents can be modified by the user. The edit boxes for 
a and b allow the user to select the coefficients of the numerator polynomial and the 
denominator polynomial of the following transfer function.
 
H(z) 5 b0 1 b1z21 1 Á 1 bmz2m
1 1 a1z21 1 Á 1 a2n
n
 
 (3.10.1)
The numerator and denominator coefficient vectors can be edited directly by clicking 
on the shaded area and entering in new values. Any MATLAB statement or statements 
defining a and b can be entered. The Enter key is used to activate a change to a parameter. 
3.10
GUi Module
item 
Variables 
Block diagram 
x(k), y(k) 
Edit parameters 
a, b, fs, c, F0 
Input type 
white noise, unit impulse, unit step, damped cosine, record x, import 
Plot view 
time signals, magnitude response, phase response, pole-zero plot 
Slider 
number of samples N 
Push buttons 
play x, play y 
Check boxes 
line/stem plot, linear/dB scale 
Menu buttons 
export, caliper, print, help, exit 
Import 
a, b, x, fs 
Export 
a, b, x, y, fs 
Table 3.5: Features of 
GUI Module g_sysfreq 
%  
a 5 1 by (n+1) vector of denominator coefficients
% 
with a(1) = 1.
%  
e 5 least squares error
% Note:
%  
The input u must be rich in frequency content:
%  
the magnitude spectrum must be nonzero at at
%  
least p frequencies
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

210    Chapter 3  Discrete-time Systems in the Frequency Domain
Figure 3.28: Display Screen for GUI Module g_sysfreq
Select Type
Select View
Slider Bar
22
21
0
1
2
22
21
0
1
1
0.5
0
2
Pole-zero Plot
Re(z)
Im(z)
22
21
0
1
2
22
0
2
0
5
10
Re(z)
Magnitude of Transfer Function
Im(z)
|b(z)/a(z)|
Edit Parameters
g_sysfreq
x
H(z)
y
0
0.2
0.4
0.6
0.8
1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.10 GUI Modules and Case Studies    211
Additional scalar parameters that appear in edit boxes are associated with the damped 
cosine input.
 
x(k) 5 ck cos(2F0kT)(k) 
 (3.10.2)
They include the input frequency 0 # F0 # fsy2, the exponential damping factor c, 
which is constrained to lie the interval f21, 1g, and the sampling frequency fs. The 
Parameters window also contains two push-button controls. The push-button controls 
play the signals x(k) and y(k) on the PC speaker using the current sampling rate. This 
option is active on any PC with a sound card. It allows the user to hear the filtering effects 
of H(z) on various types of inputs.
The Type and View windows in the upper-right corner of the screen allow the user to 
select both the type of input signal and the viewing mode. The inputs include white noise 
uniformly distributed over f21, 1g, a unit impulse input, a unit step input, the damped 
cosine input in (3.10.2), recorded sounds from a PC microphone, and user-defined inputs 
imported from a MAT file. The Recorded sound option can be used to record up to one 
second of sound at a sampling rate of 8192 Hz. For the Import option, a MAT file con-
taining the input vector x, the sampling frequency fs, and the coefficient vectors a and b 
must be supplied by the user.
The View options include plots of the input x(k), the output y(k), the magnitude 
response A( f ), the phase response ( f ), and a pole-zero sketch. The magnitude response 
is either linear or logarithmic, depending on the status of the dB check box control.  
Similarly, the plots of the input and the output use interpolated continuous time or  
discrete time, depending on the status of the stem plot check box control. The pole-
zero sketch also includes a clipped plot of the transfer function surface uH(z)u. The Plot 
window on the bottom half of the screen shows the selected view. All curves are color-
coded to match the block diagram labels. The slider bar below the Type and View window 
allows the user to change the number of samples N.
The Menu bar at the top of the screen includes several menu options. The Export 
option is used to save a, b, x, y, fs to a MAT file. Results exported from one GUI module 
can be imported into other GUI modules for additional processing. The Caliper option 
allows the user to measure any point on the current plot by moving the mouse cross hairs 
to that point and clicking. The Print option sends the GUI window to a printer or a  
file. Finally, the Help option provides the user with some helpful suggestions on how to 
effectively use module g_sysfreq.
satellite Attitude Control
In Section 3.1, the following discrete-time model was introduced for a single-axis satellite 
attitude control system.
y(k) 5 (1 2 d)y(k 2 1) 2 dy(k 2 2) 1 d fr(k 2 1) 1 r(k 2 2)g
d 5 cT 2
2J
Here r(k) is the desired angular position of the satellite at the kth sampling time, and y(k) 
is the actual angular position. The constants c, T, and J denote the controller gain, the 
Case Study 3.1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

212    Chapter 3  Discrete-time Systems in the Frequency Domain
sampling interval, and the satellite moment of inertia, respectively. By inspection of the 
difference equation, the transfer function of this control system is
H(z) 5 Y(z)
R(z)
5
d(z21 2 z22)
1 2 (1 2 d)z21 1 dz22
5
d(z 1 1)
z2 1 (d 2 1)z 1 d
The controller gain c appearing in the expression for d is an engineering design 
parameter that must be chosen to satisfy some performance specification. The most fun-
damental performance constraint is that the control system be stable. The denominator 
polynomial of the transfer function is
a(z) 5 z2 1 (d 2 1)z 1 d
We can apply the Jury test to determine a stable range for c. The first two rows of the 
Jury table are
J2 53
1
d 2 1
d
d
d 2 1
14
Using (3.7.12), the elements of the third row are
 b0 5 u
1
d 
d
1u 5 1 2 d 2
 b1 5 u
1
d 2 1
d
d 2 1u 5 2(1 2 d )2
From the stability condition b0 . 0 we have
udu , 1
The first four rows of the Jury table are
 J4 53
1
d 2 1
d
d
d 2 1
1
1 2 d 2
2 (1 2 d )2
2(1 2 d )2
1 2 d 2 4
Using (3.7.12), the element in the fifth row of the Jury table is
 c0 5
1
1 2 d 2 u
1 2 d 2
2(1 2 d )2
2(1 2 d )2
1 2 d 2 u
 5 (1 2 d 2)2 2 (1 2 d )4
1 2 d 2
 5
(1 2 d )2f(1 1 d )2 2 (1 2 d )2g
1 2 d 2
 5 4d(1 2 d )2
1 2 d 2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.10 GUI Modules and Case Studies    213
Since udu , 1, the denominator and the second factor in the numerator are positive. Thus 
the stability condition c0 . 0 reduces to d . 0. Together with udu , 1, this yields 0 , d , 1. 
From the definition of d in the original difference equation, we conclude that the control 
system is BIBO stable for the following range of controller gains.
0 , c , 2J
T 2
To test the effectiveness of the control system, suppose T 5 .1 s and J 5 5 N-m-sec2. 
Then 0 , c , 1000 is the stable range. Suppose a ground station issues a command to 
rotate the satellite one quarter of a turn. Then the desired angular position signal is
r(k) 51

22(k)
MATLAB function case3_1 computes the resulting zero-state response for three  
different values of the controller gain c. It can be executed directly from the DSP Com-
panion driver program g_dsp.
When case3_1 is executed, it produces the plot shown in Figure 3.29. Note that for all 
three controller gains, the satellite turns to the desired 90 degree orientation. The control 
system pole locations for the three controller gains are summarized in Table 3.6. When 
c 5 100 there are two distinct real poles, and a sluggish overdamped response results. 
When c 5 171.6 there is a double real pole, which results in a critically damped response. 
This is the fastest possible step response among those that do not overshoot the final 
position. When c 5 500 there is a pair of complex conjugate poles which generate an 
oscillatory underdamped response.
Figure 3.29: 
Response of  
Satellite to a  
90 Degree Step 
Input Using  
Different  
Controller Gains
0
8
6
2
4
10
12
14
16
18
8
6
2
4
10
12
14
16
18
8
6
2
4
10
12
14
16
18
20
0
50
100
150
Overdamped
0
20
0
50
100
150
Critically damped
0
20
0
50
100
150
k
y(k) (deg)
Underdamped
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

214    Chapter 3  Discrete-time Systems in the Frequency Domain
c
Poles 
Case 
100 
p1,2 5 .770, .130
Overdamped 
171.6 
p1,2 5 .414, .414
Critically damped 
500 
p1,2 5 .25 6 .661j
Underdamped 
Table 3.6: Control 
System Pole Locations 
for Different Controller 
Gains 
speech Compression
Recall from Section 3.1 that speech production can be modeled using an AR discrete-
time system as shown in Figure 3.3. Fundamental speech components or phonemes are 
either voiced or unvoiced sounds. For the unvoiced phonemes such as the fricatives s, 
sh, and f and the plosives p, t, and k, the AR model is driven by a random white noise 
input. For voiced sounds, which include the vowels, nasal sounds, and transient termi-
nal sounds such as b, d and g, the input to the AR model is a periodic impulse train 
with period M.
x(k) 5 o
`
i50
(k 2 iM)
If T is the sampling interval, then the period of the impulse train in seconds is T0 5 MT. 
The fundamental frequency or pitch of the speaker is then
F0 5
1
MT 
Speaker pitch typically ranges from about 50 Hz to 400 Hz. An illustration of a 
short segment of the vowel “O” was shown previously in Figure 3.4, where the pitch was 
estimated to be F0 < 145 Hz. For the linear system S in Figure 3.3 that models air flow 
through the vocal tract including the throat, mouth, and lips, the following AR model 
can be used.
y(k) 5 b0x(k) 2 o
n
i51
aiy(k 2 i)
Finding appropriate values for b0 and a from the recorded y(k) is a system identification 
problem that can be solved using the techniques discussed in Section 3.9. For an AR 
model to be valid, only a short segment of speech can be used. For example, by using 
segments of length  5 20 ms, the statistical characteristics of the speech remain effec-
tively constant. That is, the speech signal can be assumed to be a stationary signal over an 
interval of this length. Typically speech is sampled at a sampling rate of fs 5 8000 Hz. 
Hence a segment of duration  consists of N samples where
N 5 fs
5 160 
A direct brute-force way to transmit speech over a communication channel is to send 
the samples themselves, Y 5 hy(k) u 0 # k , Nj. The utility of using an AR model is 
that the coefficients of the model can be sent instead, and then the speech can be recon-
structed at the receiver (Rabiner and Schafer, 1978). Typically, a frame of speech trans-
mitted in this manner includes the following information.
 frame 5 f  fs, F0, v, b0, a1, . . . , ang
Case Study 3.2
Phonemes
Pitch
Stationary signal
Frame
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.10 GUI Modules and Case Studies    215
Here fs is the sampling frequency, F0 is the pitch, v is a voiced/unvoiced switch, b0 is the 
volume, and a [ Rn specifies the remaining coefficients of the AR filter. Thus the total 
length of the frame is p 5 n 1 4. If an effective model can be achieved for p , N, then 
fewer bits of data have to be sent over the communication channel. Consequently, a less 
expensive lower-bandwidth channel can be used. Typically an AR filter of order n 5 10 
is sufficient. In this case a savings of 146y160 or 91.3 percent can be achieved using this 
data compression technique.
MATLAB function case3_2 tests the idea of sending the model rather than the mea-
sured data. The user can try out different order AR models, listen to the original and 
the reconstructed sound segments, and view the compression achieved. Note that an AR 
model is found by calling f_idarma from Section 3.9 with m 5 0.
When case3_2 is executed, the user is prompted for the AR model order n and the 
pitch period M. Suggested default values are provided as a starting point, but the user is 
encouraged to see what happens as other values are used. One of the problems that can 
occur with an AR filter is that it can become unstable. This becomes more likely as the 
filter order increases and the pitch period varies.
Fibonacci sequence and the Golden Ratio
There is a simple discrete-time system that can be used to produce a well-known sequence 
of numbers called the Fibonacci sequence.
y(k) 5 y(k 2 1) 1 y(k 2 2) 1 x(k)
The impulse response of this system is the Fibonacci sequence. Note that with x(k) 5 (k) 
and a zero initial condition, we have y(0) 5 1. For k . 0, the next number in the sequence 
is just the sum of the previous two. This yields the following impulse response.
h(k) 5 f1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, Á g
Fibonacci introduced this model in 1202 to describe how fast rabbits could breed 
under ideal circumstances (Cook, 1979). He starts with one male-female pair and assumes 
that at the end of each month they breed. One month later the female produces another 
male-female pair and the process continues. The number of pairs at the end of each 
month then follows the Fibonacci pattern. The Fibonacci numbers occur in a surprising 
number of places in nature. For example, the number of petals in flowers is often a Fibon-
acci number, as can be seen from Table 3.7.
The system used to generate the Fibonacci sequence is an unstable system with h(k) 
growing without bound as k S `. However, it is of interest to investigate the ratio of 
Case Study 3.3
Fibonacci sequence
Flower 
number of Petals 
Iris 
 3
Wild rose 
 5
Delphinium 
 8
Corn marigold 
13
Aster 
21
Pyrethrum 
34
Michelmas daisy 
55
Table 3.7: Fibonacci 
Numbers and Flowers 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

216    Chapter 3  Discrete-time Systems in the Frequency Domain
successive samples of the impulse response. This ratio converges to a special number 
called the golden ratio.
 5
D  lim
kS` 5
h(k)
h(k 2 1)6 < 1.618
The golden ratio is noteworthy in that it has been used in Greek architecture as far back 
as 430 BC in the construction of the Parthenon, a temple to the goddess Athena. For 
example, the ratio of the width to the height of the front of the temple is .
Consider the problem of finding the exact value of the golden ratio. From the differ-
ence equation, the transfer function of the Fibonacci system is
H(z) 5
1
1 2 z21 2 z22
5
z2
z2 2 z 2 1
Factoring the denominator, this system has poles at
p1,2 5 1 6 Ï5
2
Using Algorithm 3.1, the initial value is h(0) 5 1, and the residues at the two poles are
 Res ( p1, k) 5 pk 1 1
1
p1 2 p2
 Res (p2, k) 5 pk 1 1
2
p2 2 p1
Thus the impulse response is
h(k) 5 h(0)(k) 1 f Res (p1, k) 1
 Res (p2, k)g(k 2 1)
5 (k) 11
pk 1 1
1
2 pk 1 1
2
p1 2 p2 2 (k 2 1)
51
pk 1 1
1
2 pk 1 1
2
p1 2 p2 2 (k)
Note that up1u . 1 and up2u , 1. Therefore, pk 1 1
2
S 0 as k S `. Thus we have
 5 lim
kS`5
pk11
1
pk
1 6 5 p1
Consequently, the golden ratio is
 5 1 1 Ï5
2
5 1.6180339 Á
Golden ratio
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.11 Chapter Summary    217
MATLAB function case3_3 computes the impulse response of the Fibonacci 
system. It also computes the golden ratio, both directly and as a limit of the ratio 
g(k) 5 h(k)yh(k 2 1).
When case3_3 is executed, it produces the plot shown in Figure 3.30, which graphs 
g(k) 5 h(k)yh(k 2 1). It is evident that g(k) rapidly converges to the golden ratio, .
Chapter summary
Learning outcomes
This chapter was designed to provide the student with an opportunity to achieve the 
learning outcomes summarized in Table 3.8.
Z-transform
Chapter 3 focused on the analysis of linear time-invariant discrete-time systems in the 
frequency domain using the Z-transform. The Z-transform is an essential analytical tool 
for digital signal processing. It is a transformation that maps a discrete-time signal x(k) 
into an algebraic function X(z) of a complex variable z.
 
X(z) 5 o
`
k52`
x(k)z2k, z [ VROC 
 (3.11.1)
Typically X(z) is a ratio of two polynomials in z. The roots of the numerator polyno-
mial are called the zeros of X(z), and the roots of the denominator polynomial are called 
the poles of X(z). For general noncausal signals, the region of convergence, VROC, is an 
annular region centered at the origin of the complex plane. For anti-causal signals that 
3.11
Zeros, poles
Region of convergence
0
6
8
2
4
10
12
14
16
18
20
0
0.5
1
1.5
2
2.5
3
k
h(k)/h(k 2 1)
 5 1.618034
Figure 3.30: 
Numerical  
Approximation  
of the Golden 
Ratio
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

218    Chapter 3  Discrete-time Systems in the Frequency Domain
num.  
Learning outcome
sec. 
1 
Know how to compute the Z-transform using the geometric series 
3.2 
2 
Understand how to use the Z-transform properties to expand the size of a 
table of Z-transform pairs
3.3 
3 
Be able to invert the Z-transform using synthetic division, partial  
fractions, and the residue method
3.4 
4 
Be able to go back and forth between difference equations, transfer functions, 
impulse responses, and signal flow graphs
3.5–3.7 
5 
Understand the relationship between poles, zeros, natural modes and forced 
modes of a linear system
3.5 
6 
Know how to find the impulse response and use it to compute the zero-state 
response to any input
3.7 
7 
Understand the differences between FIR systems and IIR systems, and know 
which system is always stable
3.7 
8 
Appreciate the significance of the unit circle in the Z-plane in terms of stabil-
ity, and know how to evaluate stability using the Jury test
3.8 
9 
Be able to compute the frequency response from the transfer function
3.9 
10 
Know how to compute the steady-state output of a stable discrete-time  
system corresponding to a periodic input
3.9 
11 
Know how to use the GUI module g_system to investigate the input-output 
behavior of a discrete-time system
3.10 
Table 3.8: Learning 
Outcomes for  
Chapter 3 
are nonzero for k , 0, X(z) converges inside the innermost pole, and for causal signals 
that are nonzero for k $ 0, X(z) converges outside the outermost pole.
The Z-transform is usually found by consulting a table of Z-transform pairs. The 
size of the table is effectively enlarged by using the Z-transform properties. The initial 
and final value theorems allow us to recover the initial and final values of the signal 
x(k) directly from its Z-transform. More generally, a finite number of samples of x(k) 
can be obtained by inverting the Z-transform using the synthetic division method or the 
impulse response method. If a closed-form expression for x(k) is desired, then the inverse 
Z-transform should be computed using either the partial fraction method with a table or 
the residue method.
 
x(k) 5 x(0)(k) 13o
q
i51
 Res (pi, k)4(k 2 1) 
 (3.11.2)
Cauchy’s residue method is the method of choice for most cases (real poles, single or 
multiple) because for multiple poles it requires less computational effort than the partial 
fraction method. Furthermore, unlike the partial fraction method, the residue method 
does not require the use of a table of Z-transform pairs.
Transfer Function
The transfer function of a discrete-time system is a compact algebraic representation of 
the system defined as the Z-transform of the output divided by the Z-transform of a 
nonzero input, assuming the initial condition is zero.
 
H(z) 5 Y(z)
X(z) 
 (3.11.3)
Inverse Z-transform
Transfer function
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.11 Chapter Summary    219
Using the linearity and time shift properties, the transfer function can be obtained directly 
from inspection of the difference equation representation of a discrete-time system. A 
third representation of a discrete-time system is the signal flow graph, which is a concise 
graphical description. It it possible to go directly from any of the three representations to 
another by inspection.
BiBo stability
The time-domain equivalent of the transfer function is the impulse response. The impulse 
response is the zero-state response of the system when the input is the unit impulse (k). 
The easiest way to compute the impulse response h(k) is to take the inverse Z-transform 
of the transfer function H(z).
 
h(k) 5 Z21hH(z)j 
 (3.11.4)
The zero-state response of the system to any input can be obtained from the impulse 
response using convolution. Convolution, in the time domain, maps into multiplication 
in the Z-transform domain. This leads to the following input-output representation in the 
frequency domain.
 
Y(z) 5 H(z)X(z) 
 (3.11.5)
A system is BIBO stable if and only if every bounded input is guaranteed to pro-
duce a bounded output. Otherwise, the system is unstable. Each pole of H(z) generates a  
natural mode term in y(k). For stable systems, the natural modes all decay to zero. A sys-
tem is BIBO stable if and only if all of the poles of H(z) lie strictly inside the unit circle 
of the complex plane. The Jury test is a tabular stability test that can be used to determine 
ranges for the system parameters over which a system is stable. All FIR systems are stable 
because their poles are all at the origin, but IIR systems may be stable or unstable.
Frequency Response
The frequency response of a stable discrete-time system is the transfer function evaluated 
along the unit circle. If fs is the sampling frequency and T 5 1yfs is the sampling interval, 
then the frequency response is
 
H( f ) 5 H(z)uz5exp ( j2f T ), 0 # u f u # fsy2 
 (3.11.6)
A digital filter is a discrete-time system that is designed to have a prescribed frequency 
response. The magnitude A( f ) 5 uH( f )u is called the magnitude response of the filter, and 
the phase angle ( f ) 5 /H( f ) is called the  phase response of the filter. When a stable 
discrete-time system is driven by a cosine input with frequency 0 # f # fsy2, the steady-
state output is a sinusoid of frequency f  whose amplitude is scaled by A( f ) and whose 
phase is shifted by ( f ).
 
yss(k) 5 A( f ) cosf2fkT 1 ( f )g 
 (3.11.7)
By designing a digital filter with a prescribed magnitude response, certain frequencies can 
be removed from the input signal, and other frequencies can be enhanced.
*system identification
System identification is the process of finding the parameters of a discrete-time model of 
a system using only input and output measurements. Different structures that can be used 
Impulse response
Input-output 
representation
BIBO stable
Stability test
Magnitude, phase 
responses
Optional material
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

220    Chapter 3  Discrete-time Systems in the Frequency Domain
for the model include an auto-regressive (AR) model, a moving average (MA) model, and 
an auto-regressive moving average (ARMA) model.
 
H(z) 5 b0 1 b1z21 1 Á 1 bmz2m
1 1 a1z21 1 Á 1 anz2n  
 (3.11.8)
A least-squares error criterion can be used to find optimal parameter values as long as 
the number of input-output samples is at least as large as the number of parameters. For 
a good least-squares fit, the input should be persistently exciting (e.g., white noise), and 
the number of samples should satisfy N W
 max (m, n).
GUi Module
The DSP Companion includes a GUI module called g_sysfreq that allows the user 
to interactively investigate the input-output behavior of a discrete-time system in the  
frequency domain without any need for programming.
Problems
The problems are divided into Analysis and Design problems that can be solved by hand or 
with a calculator, GUI Simulation problems that are solved using GUI module g_sysfreq,  
and MATLAB Computation problems that require a user program. Solutions to selected 
problems can be accessed with the DSP Companion driver program, g_dsp. Students 
are encouraged to use these problems, which are identified with a 
, as a check on their 
understanding of the material.
3.12.1 Analysis and Design
section 3.2: Z-transform Pairs
3.1 Consider the following finite causal signal where x(0) 5 8.
x 5 f8, 26, 4, 22, 0, 0, Á g
(a) Find the Z-transform X(z), and express it as a ratio of two polynomials in z.
(b) What is the region of convergence of X(z)?
3.2 Consider the following finite anti-causal signal where x(21) 5 4.
x 5 f Á , 0, 0, 3, 27, 2, 9, 4g
(a) Find the Z-transform X(z), and express it as a ratio of two polynomials in z.
(b) What is the region of convergence of X(z)?
3.3 Consider the following finite noncausal signal where x(0) 5 3.
x 5 f Á , 0, 0, 1, 2, 3, 2, 1, 0, 0, Á g
(a) Find the Z-transform X(z), and express it as a ratio of two polynomials in z.
(b) What is the region of convergence of X(z)?
3.12
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.12 Problems    221
3.4 Consider the following causal signal.
x(k) 5 2(.8)k21(k)
(a) Find the Z-transform X(z), and express it as a ratio of two polynomials in z.
(b) What is the region of convergence of X(z)?
3.5 Consider the following anti-causal signal.
x(k) 5 5(2.7)k11(2k 2 1)
(a) Find the Z-transform X(z), and express it as a ratio of two polynomials in z.
(b) What is the region of convergence of X(z)?
3.6 Consider the following noncausal signal.
x(k) 5 10(.6)k(k 1 2)
(a) Find the Z-transform X(z), and express it as a ratio of two polynomials in z.
(b) What is the region of convergence of X(z)?
3.7 Consider the following noncausal signal. Show that X(z) does not exist for any 
scalar c. That is, show that the region of convergence of X(z) is the empty set.
x(k) 5 ck
3.8 Consider the following discrete-time signal.
x(k) 5 ak sin(bk 1 )(k)
(a) Use Table 3.2 and the trigonometric identities in Appendix 2 to find X(z).
(b) Verify that X(z) reduces to an entry in Table 3.2 when  5 0. Which one?
(c) Verify that X(z) reduces to another entry in Table 3.2 when  5 y2. 
Which one?
3.9 The basic geometric series in (2.2.14) is often used to compute Z-transforms. It can 
be generalized in a number of ways.
(a) Prove that the geometric series in (2.2.14) converges to 1y(1 2 z) for uzu , 1 by 
showing that
 lim 
NS` (1 2 z)o
N
k50
z
k
5 1 3  uzu , 1
(b) Use (2.2.14) to establish (3.2.3). That is, show that
o
`
k5m
zk 5
zm
1 2 z, m $ 0, uzu , 1
(c) Use the results of part (b) to show the following. Hint: Write the sum as a 
difference of two infinite series.
o
n
k5m
zk 5 zm 2 zn11
1 2 z
, n $ m $ 0, uzu , 1
Geometric series
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

222    Chapter 3  Discrete-time Systems in the Frequency Domain
(d) Show that the result in part (c) holds for all complex z by multiplying both 
sides by 1 2 z and simplifying the left-hand side.
section 3.3: Z-transform Properties
3.10 Suppose X(z) converges on Vx 5 hz [ Cu uzu . Rxj and Y(z) converges on 
Vy 5 hz [ Cu uzu , Ryj.
(a) Classify x(k) and y(k) as to their type: causal, anti-causal, noncausal. 
(b) Find a subset of the region of convergence of ax(k) 1 by(k). 
(c) Find the region of convergence of ckx(k). 
(d) Find the region of convergence of y(2k). 
3.11 Consider the following signal.
x(k) 55
10,
0 # k , 4
22, 4 # k , `
(a) Write x(k) as a difference of two step signals.
(b) Use the time shift property to find X(z). Express your final answer as a ratio 
of two polynomials in z.
(c) Find the region of convergence of X(z).
3.12 Consider the following signal.
x(k) 55
2k, 0 # k , 9
18,
9 # k , `
(a) Write x(k) as a difference of two ramp signals.
(b) Use the time shift property to find X(z). Express your final answer as a ratio 
of two polynomials in z.
(c) Find the region of convergence of X(z).
3.13 Use Appendix 1 and the properties of the Z-transform to find the Z-transform  
of the following cubic exponential signal. Simplify your final answer as much as 
you can.
x(k) 5 k3ck(k)
3.14 Let x*(k) denote the complex conjugate of x(k). Show that the Z-transform of 
x*(k) can be expressed in terms of the Z-transform of x(k) as follows. This is called 
the complex conjugate property.
Zhx*(k)j 5 X*(z*)
3.15 Let h(k) and x(k) be the following pair of signals.
h(k) 5 f1 2 (.9)kg(k)
x(k) 5 (21)k(k)
(a) Find H(z) as a ratio of polynomials in z and its region of convergence. 
Complex conjugate 
property
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.12 Problems    223
(b) Find X(z) as a ratio of polynomials in z and its region of convergence. 
(c) Use the convolution property to find the Z-transform of h(k) w x(k) as a ratio 
of polynomials in z and its region of convergence. 
3.16 In Problem 3.15 the region of convergence of the Z-transform of h(k) w x(k) is 
VROC 5 VH > VX, where VH is the region of convergence of H(z) and VX is the 
region of convergence of X(z). Is this true in general? If not, find an example of an 
H(z) and an X(z) where VROC is larger than VH > VX.
3.17 Consider the following noncausal signal.
x(k) 5 ck(2k)
(a) Using Definition 3.1 and the geometric series, find X(z) as a ratio of two  
polynomials in z and its region of convergence. 
(b) Verify the results of part (a) by instead finding X(z) using Table 3.2 and the 
time reversal property. 
3.18 Consider the following pair of finite causal signals, each starting with sample k 5 0.
x(k) 5 f1, 2, 3g
y(k) 5 f7, 2, 4, 6, 1g
(a) Find X(z) as a ratio of polynomials in z, and find the region of convergence. 
(b) Find Y(z) as a ratio of polynomials in z, and find the region of convergence. 
(c) Consider the cross-correlation of y(k) with x(k).
ryx(k) 5 1
5o
4
i50
y(i)x(i 2 k), 0 # k , 5
Using the correlation property, find the Z-transform of the cross- 
correlation ryx(k) as a ratio of polynomials in z, and find the region of 
convergence. 
3.19 Consider the following Z-transform.
X(z) 5
10(z 2 2)2(z 1 1)3
(z 2 .8)2(z 2 1)(z 2 .2)2
(a) Find x(0) without inverting X(z).
(b) Find x(`) without inverting X(z).
(c) Write down the form of x(k) from inspection of X(z). You can leave the coef-
ficients of each term of X(z) unspecified.
3.20 A student attempts to apply the final value theorem to the following Z-transform 
and gets the steady-state value x(`) 5 25. Is this correct? If not, what is the value 
of x(k) as k S `? Explain your answer.
X(z) 5
10z3
(z2 2 z 2 2)(z 2 1), uzu . 2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

224    Chapter 3  Discrete-time Systems in the Frequency Domain
section 3.4: inverse Z-transform
3.21 Consider the following Z-transform.
X(z) 5
z4 1 1
z2 2 3z 1 2, uzu . 2
(a) Find the causal part of x(k).
(b) Find the anti-casual part of x(k).
3.22 Consider the following Z-transform.
X(z) 5 z4 1 2z3 1 3z2 1 2z 1 1
z4
, uzu . 0
(a) Rewrite X(z) in terms negative powers of z.
(b) Use Definition 3.1 to find x(k).
(c) Verify that x(k) is consistent with the initial value theorem.
(d) Verify that x(k) is consistent with the final value theorem.
3.23 Consider the following Z-transform.
X(z) 5
2z
z2 2 1, uzu . 1
(a) Find x(k) for 0 # k # 5 using the synthetic division method.
(b) Find x(k) using the partial fraction method.
(c) Find x(k) using the residue method.
3.24 Consider the following Z-transform. Find x(k) using the time shift property and 
the residue method.
X(z) 5
100
z2(z 2 .5)3, uzu . .5
3.25 Consider the following Z-transform. Use Algorithm 3.1 to find x(k). Express your 
final answer as a real signal.
X(z) 5
1
z2 1 1, uzu . 1
3.26 Repeat Problem 3.25, but use Table 3.2 and the Z-transform properties.
3.27 Consider the following Z-transform. Find x(k).
X(z) 5
5z3
(z2 2 z 1 .25)(z 1 1), uzu . 1
3.28 The formulation of the inverse Z-transform using the contour integral in (3.4.20) 
is based on the Cauchy integral theorem. This theorem states if C is any counter-
clockwise contour that encircles the origin then
1
j2 $
C
zk 2 1 2 idz 55
1, i 5 k
0, i Þ k
Cauchy integral 
theorem
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.12 Problems    225
Use Definition 3.1 and the Cauchy integral theorem to show that the Z-transform 
can be inverted as in (3.4.20). That is, show that
x(k) 5 1
j2
  $CX(z)zk21dz
3.29 Consider the following Z-transform.
X(z) 5
z
z 2 1
(a) Find x(k) if the region of convergence is uzu . 1. 
(b) Find x(k) if the region of convergence is uzu , 1. 
3.30 When two signals are multiplied, this corresponds to one signal modulating the 
amplitude of the other signal. The following property of the Z-transform is called 
the modulation property.
Zhh(k)x(k)j 5 1
j2 $
C
H(u)X1
z
u2u21du
Use the Cauchy integral representation of a time signal in Problem 3.28 to verify 
the modulation property.
section 3.5: Transfer Function
3.31 Consider a running average filter of order M 2 1.
y(k) 5 1
M o
M21
i50
x(k 2 i)
(a) Find the transfer function H(z). Express it as a ratio of two polynomials in z.
(b) Use the geometric series in (3.2.3) to show that an alternative form of the 
transfer function is as follows. Hint: Express y(k) as a difference of two sums.
H(z) 5
zM 2 1
M(z 2 1)zM21
(c) Convert the transfer function in part (b) to a difference equation. 
3.32 Consider a discrete-time system described by the following difference equation.
y(k) 5 y(k 2 1) 2 .24y(k 2 2) 1 2x(k 2 1) 2 1.6x(k 2 2)
(a) Find the transfer function H(z).
(b) Write down the form of the natural mode terms of this system.
(c) Find the zero-state response to the step input x(k) 5 10(k).
(d) Find the zero-state response to the causal exponential input x(k) 5 .8k(k). 
Does a forced mode term appear in y(k)? If not, why not?
(e) Find the zero-state response to the causal exponential input x(k) 5 .4k(k). Is 
this an example of harmonic forcing? Why or why not?
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

226    Chapter 3  Discrete-time Systems in the Frequency Domain
Figure 3.31: Signal  
Flow Graph of  
a Cascade 
Configuration
H1(z)
H2(z)
v
y
x
Figure 3.32: Signal  
Flow Graph of  
a Parallel 
Configuration
H1(z)
H2(z)
y
x
Figure 3.33: Signal  
Flow Graph of 
a Feedback 
Configuration
H1(z)
H2(z)
y
x
3.33 Consider a discrete-time system described by the following transfer function.
H(z) 5 z 1 .5
z 2 .7
(a) Find an input x(k) that creates a forced mode of the form c(.3)k and causes the 
natural mode term to disappear in the zero-state response.
(b) Find an input x(k) that has no zeros and creates a forced mode of the form 
(c1k 1 c2)(.7)k in the zero-state response.
3.34 Consider a discrete-time system described by the following transfer function.
H(z) 5 3(z 2 .4)
z 1 .8
(a) Suppose the zero-state response to an input x(k) is y(k) 5 (k). Find X(z). 
(b) Find x(k). 
section 3.6: signal Flow Graphs
3.35 Find the transfer function H(z) 5 Y(z)yX(z) of the system whose signal flow graph 
is shown in Figure 3.31. This is called a cascade configuration of H1(z) and H2(z).
3.36 Find the overall transfer function H(z) 5 Y(z)yX(z) of the system whose signal 
flow graph is shown in Figure 3.32. This is called a parallel configuration of H1(z) 
and H2(z).
3.37 Find the overall transfer function H(z) 5 Y(z)yX(z) of the system whose signal 
flow graph is shown in Figure 3.33. This is called a feedback configuration of H1(z) 
and H2(z).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.12 Problems    227
3.38 Consider a discrete-time system described by the following difference equation.
y(k) 5 .6y(k 2 1) 1 .16y(k 2 2) 1 10x(k 2 1) 1 5x(k 2 2)
(a) Find the transfer function H(z).
(b) Find the impulse response h(k).
(c) Sketch the signal flow graph.
3.39 Consider a discrete-time system described by the following transfer function.
H(z) 5
4z2 1 1
z2 2 1.8z 1 .81
(a) Find the difference equation.
(b) Find the impulse response h(k).
(c) Sketch the signal flow graph.
3.40 Consider a discrete-time system described by the following impulse response.
h(k) 5 f2 2 .5k 1 .2k21g(k)
(a) Find the transfer function H(z).
(b) Find the difference equation.
(c) Sketch the signal flow graph.
3.41 Consider a discrete-time system described by the signal flow graph shown in  
Figure 3.34.
(a) Find the transfer function H(z).
(b) Find the impulse response h(k).
(c) Find the difference equation.
3.42 A discrete-time system has poles at z 5 6.5 and zeros at z 5 6j2. The system has a 
DC gain of 20.
(a) Find the transfer function H(z).
(b) Find the impulse response h(k).
(c) Find the difference equation.
(d) Sketch the signal flow graph.
Figure 3.34: Signal 
Flow Graph  
of System in  
Problem 3.41
2.49
x
u
y
21.4
4
28
3
z21
z21
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

228    Chapter 3  Discrete-time Systems in the Frequency Domain
Figure 3.35: Signal Flow Graph of System in Problem 3.42
x
u
y
0
2.49
1
2
1
26
21.2
1.8
.36
.32
z21
z21
z21
z21
3.43 Consider a discrete-time system described by the signal flow graph shown in Figure 3.35.
(a) Find the transfer function H(z).
(b) Write the difference equations as a system of two equations.
(c) Write the difference equation as a single equation.
section 3.7: stability in the Frequency Domain
3.44 Consider a system with the following impulse response.
h(k) 5 (21)k(k)
(a) Find the transfer function, H(z).
(b) Find a bounded input x(k) that produces an unbounded zero-state output y(k).
(c) Find a bound for x(k).
(d) Show that the zero-state output y(k) is unbounded.
3.45 Consider a discrete-time system described by the following transfer function.
H(z) 5
1
z2 1 1
(a) Show that this system is BIBO unstable. 
(b) Find a bounded x(k) with bound Bx 5 1 that produces an unbounded zero-
state output. 
(c) Find the Z-transform of the zero-state output when the input in part (b) is 
applied. 
3.46 Is the following system BIBO stable? Show your work.
H(z) 5
5z2(z 1 1)
(z 2 .8)(z2 1 .2z 2 .8)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.12 Problems    229
3.47 Consider the following transfer function with parameter .
H(z) 5
z2
(z 2 .8)(z2 2 z 1 )
(a) Sketch the stability triangle from Figure 3.20. Use the sketch to find the max-
imum range of values for  over which H(z) is stable.
(b) Find the poles of H(z) corresponding to the two stability limits in part (a).
3.48 Consider the following transfer function with parameter .
H(z) 5
z2
(z 1 .7)(z2 1 z 1 .5)
(a) Sketch the stability triangle from Figure 3.20. Use the sketch to find the max-
imum range of values for  over which H(z) is stable.
(b) Find the poles of H(z) corresponding to the two stability limits in part (a).
3.49 Consider the following discrete-time system.
H(z) 5
10z
z2 2 1.5z 1 .5
(a) Find the poles and zeros of H(z).
(b) Show that this system is BIBO unstable.
(c) Find a bounded input x(k) that produces and unbounded output. Show that 
x(k) is bounded. Hint: Use harmonic forcing.
(d) Find the zero-state response produced by the input in part (c) and show that 
it is unbounded.
section 3.8: Frequency Response
3.50 Consider the following system that consists of a gain of A and a delay of d samples.
y(k) 5 Ax(k 2 d)
(a) Find the transfer function, the poles, the zeros, and the DC gain.
(b) Is this system BIBO stable? Why or why not?
(c) Find the impulse response of this system.
(d) Find the frequency response of this system.
(e) Find the magnitude response.
(f) Find the phase response.
3.51 Consider the following first-order IIR system.
H(z) 5 z 1 .5
z 2 .5
(a) Find the frequency response H( f ).
(b) Find and sketch the magnitude response A( f ).
(c) Find and sketch the phase response ( f ).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

230    Chapter 3  Discrete-time Systems in the Frequency Domain
3.52 Consider the following first-order FIR system that is called a backwards Euler 
differentiator.
H(z) 5 z 2 1
Tz
(a) Find the frequency response H( f ).
(b) Find and sketch the magnitude response A( f ).
(c) Find and sketch the phase response ( f ).
(d) Find the steady-state response to the following periodic input.
x(k) 5 2 cos(.8k) 2  sin(.5k)
3.53 Consider the following second-order system.
H(z) 5 3(z 1 1)
z2 2 .81
(a) Find the frequency response H( f ).
(b) Find and sketch the magnitude response A( f ).
(c) Find and sketch the phase response ( f ).
(d) Find the steady-state response to the following periodic input.
x(k) 5 10 cos(.6k)
3.54 Consider a system with the following impulse response.
h(k) 5 10(.5)k(k)
(a) Find the transfer function. 
(b) Find the magnitude and phase responses. 
(c) Find the fundamental frequency F0, expressed as a fraction of fs, of the  
following periodic input.
x(k) 5 o
9
i50
1
1 1 i cos(.1ik)
(d) Find the steady-state response yss(k) to the periodic input in part (c). Express 
your final answer in terms of F0.
3.55 For the system in Problem 3.54, consider the following following complex sinusoi-
dal input.
x(k) 5  cos(ky3) 1 j sin(ky3)
(a) Find the frequency F0 of x(k), expressed as a fraction of fs. 
(b) Find the steady-state output yss(k). 
section 3.9: system identification
3.56 An alternative to using an ARMA model for system identification is to use a MA 
model. One important advantage of a MA model is that it is always stable.
H(z) 5 o
m
i50
biz2i
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.12 Problems    231
(a) Let D be the input-output data in (3.9.2). Let y 5 f y(0), Á , y(N 2 1)gT, and 
let b [ Rm11 be the parameter vector. Find a N 3 (m 1 1) coefficient matrix 
U, analogous to Q in (3.9.7), such that the MA model agrees with the data D 
when N 5 m 1 1 and
Ub 5 y
(b) Assuming U is of full rank, find an expression for an optimal least-squares b 
when N . m 1 1.
3.12.2 GUi simulation
section 3.5: Transfer Function
3.57 Consider the system in Problem 3.53. Use GUI module g_sysfreq to perform the 
following tasks.
(a) Plot the pole-zero pattern. Is this system BIBO stable?
(b) Plot the response to white noise. Use Caliper to mark the minimum point.
3.58 Consider the system in Problem 3.53. Use GUI module g_sysfreq to plot the  step 
response. Estimate the DC gain from the step response using the Caliper option.
3.59 Consider the following linear discrete-time system.
H(z) 5
5z22 1 4.5z24
1 2 1.8z22 1 .81z24
Use GUI module g_sysfreq to plot the following damped cosine input and  
the zero-state response to it.
x(k) 5 .96k cos(.4k)
3.60 Consider the following linear discrete-time system.
H(z) 5
6 2 7.7z21 1 2.5z22
1 2 1.7z21 1 .8z22 2 .1z23
Create a MAT-file called prob3_59.mat that contains fs 5 100, the appropriate 
coefficient vectors a and b, and the following input samples where v(k) is white 
noise uniformly distributed over f2.5, .5g.
x(k) 5 k exp(2ky50) 1 v(k), 0 # k , 500
Use GUI module g_sysfreq and the Import option to plot this input and the zero-
state response to this input.
section 3.8: Frequency Response
3.61 Consider the following linear discrete-time system. Suppose the sampling frequency 
is fs 5 1000 Hz. Use GUI module g_sysfreq to plot the magnitude response using 
the linear scale and the phase response.
H(z) 5
10(z2 1 .8)
(z2 1 .9)(z2 1 .7)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

232    Chapter 3  Discrete-time Systems in the Frequency Domain
3.62 Consider the following linear discrete-time system. Use GUI module g_sysfreq to 
plot the magnitude response and the phase response. Use fs 5 100 Hz, and use the 
dB scale for the magnitude response.
H(z) 5 5(z2 1 .9)
(z2 2 .9)2
3.63 Consider the running average filter in Problem 3.31. Suppose M = 10. Use GUI 
module g_sysfreq to perform the following tasks.
(a) Plot the impulse response using N 5 100 and stem plots.
(b) Plot the magnitude response using the linear scale.
(c) Plot the magnitude response using the dB scale.
(d) Plot the phase response.
3.12.3 MATLAB Computation
section 3.5: Transfer Function
3.64 Consider the following discrete-time system.
H(z) 5
1.5z4 2 .4z3 2 .8z2 1 1.1z 2 .9
z4 2 .95z3 2 .035z2 1 .462z 2 .351
Write a MATLAB program that uses filter and plot to compute and plot the zero-
state response of this system to the following input. Plot both the input and the 
output on the same graph.
x(k) 5 (k 1 1)(.9)k(k), 0 # k # 100
3.65 Consider the following discrete-time system. 
H(z) 5
2z5 1 .25z4 2 .8z3 2 1.4z2 1 .6z 2 .9
z5 1 .055z4 2 .85z3 2 .04z2 1 .49z 2 .32
Write a MATLAB program that performs the following tasks.
(a) Compute and display the poles, zeros, and DC gain. Is this system stable?
(b) Plot the poles and zeros using the DSP Companion function f_pzplot.
(c) Plot the transfer function surface using f_pzsurf.
section 3.8: Frequency Response
3.66 Consider the following discrete-time system.
H(z) 5
10z3
z4 2 .81
Write a MATLAB program that performs the following tasks.
(a) Use f_ freqz to compute the magnitude response and the phase response at 
M 5 500 points, assuming fs 5 200 Hz. Plot them as a 2 by 1 array of plots.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.12 Problems    233
(b) Use filter to compute the zero-state response to the following periodic input 
with F0 5 10 Hz.
x(k) 5 3 cos(2F0kT)(k), 0 # k # 100
Compute the steady-state response yss(k) to x(k) using the magnitude and phase 
responses evaluated at f 5 F0. Plot the zero-state response and the steady-state 
response on the same graph using a legend.
section 3.9: system identification
3.67 The MAT file prob3_67 contains an input signal x, an output signal y, and a sam-
pling frequency fs. Write a MATLAB program that performs system identification 
with this data by performing the following tasks.
(a) Load x, y, and fs from  prob3_67 and use f_idarma to compute an AR model 
of order n 5 8. Print the coefficient vector a.
(b) Plot the first 100 samples of the data y(k), and the AR model output, yAR(k), 
on the same graph using a legend.
3.68 System identification can be performed using a MA model instead of the AR model 
discussed in Section 3.9. Recall that this was the focus of Problem 3.56, where an 
expression for the optimal m 3 1 coefficient vector b was found. 
(a) Using the results from Problem 3.56, write a function called f_idma, similar to 
the DSP Companion function f_idarma, that performs system identification 
using a MA model. Do not simply call f_idarma with n 5 0. Instead, imple-
ment the formula found in Problem 3.56. The calling sequence for f_idma 
should be as follows.
% F_IDMA: MA system identification
%
% Usage:
%  
[b,E] 5 f_idma (x,y,m);
% Pre:
%  
x 5 vector of length N containing the input 
% 
samples
%  
y 5 vector of length N containing the output 
% 
 
samples
%  
m 5 the order of the MA model (m < N)
% Post:
%  
b 5 vector of length m+1 containing the 
% 
 
least-squares coefficients
%
%  
E 5 least squares error
(b) Test your f_idma function by solving Problem 3.67, but using a MA model of 
order m 5 20.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

234
Fourier Transforms  
and Spectral Analysis
Motivation
Recall that the Z-transform starts with a discrete-time signal 
x(k) and transforms it into a function X(z) of a complex vari-
able z. In this chapter we focus on important special cases of the 
Z-transform that apply when the region of convergence includes 
the unit circle. The first case involves evaluating the Z-transform 
along the unit circle itself. This leads to the discrete-time Fourier 
transform or DTFT.
X(f ) 5 o
`
k52`
x(k) exp(2j2kfT), u f u # fsy2
The DTFT, denoted X(f ) 5
 DTFT hx(k)j, is referred to as the 
spectrum of the signal x(k). The spectrum reveals how the aver-
age power of x(k) is distributed over the frequencies in the inter-
val f2fsy2, fsy2g. The spectrum of the impulse response of a 
linear system is the frequency response of the system.
The second important special case involves applying the 
Z-transform to an N-point signal, and evaluating it at N points 
uniformly spaced around the unit circle. This leads to the dis-
crete Fourier transform or DFT.
X(i) 5 o
N21
i50
x(k) exp(j2ikT), 0 # i , N
The DFT, denoted X(i) 5
 DFT hx(k)j, is a sampled version of 
the DTFT. The DFT provides the same type of spectral infor-
mation as the DTFT, but with lower resolution. In particular, 
uX(i)u2yN is the average power of x(k) at frequency fi 5 ifsyN 
4.1
C H A P T E R  4
CHAPTER ToPiCS
4.1 
Motivation 
4.2 
Discrete-time Fourier 
Transform (DTFT) 
4.3 
Discrete Fourier Transform 
(DFT) 
4.4 
Fast Fourier Transform 
(FFT) 
4.5 
Fast Convolution and 
Correlation 
4.6 
White Noise 
4.7 
Auto-correlation 
4.8 
Zero-padding and Spectral 
Resolution 
4.9 
The Spectrogram 
4.10 Power Density Spectrum 
Estimation 
4.11 GUI Modules and Case 
Studies 
4.12 Chapter Summary 
4.13 Problems
*
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.1 Motivation    235
for 0 # i , N. For computational purposes, the DFT is more efficient than the DTFT 
because it consists of only N terms, and it only needs to be evaluated at N discrete fre-
quencies. There is a highly efficient implementation of the DFT called the fast Fourier 
transform or FFT. The computational effort of the DFT grows as the square of the 
length of the signal N, whereas the computational effort of the FFT grows at the much 
slow rate of N log2(N). The FFT can be used to develop fast versions of both convolution 
and correlation.
We begin this chapter by introducing a number of examples where spectral anal-
ysis can be put to use. Next the DTFT and its inverse are introduced. The DTFT is 
used to compute the spectra of discrete-time signals of infinite duration. Many of the 
properties of the DTFT are inherited directly from the Z-transform. The DFT and its 
inverse are then defined, and some useful properties of the DFT based on symmetry 
are developed. A simple graphical relationship among the Z-transform, the DTFT, and 
the DFT is presented. The highly efficient FFT implementation of the DFT is then 
derived using the decimation in time approach. This is followed by a comparison of the 
relative computational effort required by the FFT and the DFT in terms of the number 
of floating point operations or FLOPs. The use of the DFT to compute the magnitude, 
phase, and power density spectra of finite signals is then presented. This is followed by 
an introduction to white noise. White noise is an important type of random signal that 
is useful for signal modeling and for system identification. It can be characterized in 
an elegant way, in terms of its auto-correlation and its spectrum. The use of the DFT 
to approximate the frequency response of a discrete-time system is examined, as is the 
use of zero padding to interpolate between discrete frequencies. Next the spectrogram 
is introduced to characterize signals whose spectral characteristics evolve with time. 
This is followed by an examination of techniques that are used to numerically esti-
mate a continuous power density spectrum. Finally a GUI module called g_spectra 
is introduced that allows the user to interactively explore the spectral characteristics 
of a variety of discrete-time signals without any need for programming. The chapter 
concludes with some case study examples, and a summary of Fourier transforms and 
spectral analysis.
4.1.1 Fourier Series 
Consider a periodic continuous-time signal xa(t) with period T0. For example, xa(t) might 
represent the hum or whine of a rotating machine where the fundamental frequency, 
F0 5 1yT0, changes with the speed of rotation. Since xa(t) is periodic, it can be approxi-
mated by a truncated Fourier series using M harmonics.
 
xa(t) <
o
M21
i52(M21)
ci exp(ji2F0t) 
 (4.1.1)
This is the complex form of the Fourier series with the ith Fourier coefficient being
 
ci 5 1
T0#
T0
0
xa(t) exp(2ji2F0t)dt 
 (4.1.2)
Fourier series coefficients of common periodic waveforms can be found in Appendix 1.  
Next suppose xa(t) is converted to a discrete-time signal x(k) by sampling it at N 5 2M 
points using a sampling frequency of fs 5 NF0. Thus the sampling interval is T 5 T0yN 
and
 
x(k) 5 xa(kT), 0 # k , N 
 (4.1.3)
FFT
Truncated Fourier 
series
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

236    Chapter 4  Fourier Transforms and Spectral Analysis
Note that the N samples of xa(t) span one period. To compute the ith Fourier coefficient 
of xa(t) we approximate the integral in (4.1.2) with a sum. Recalling that F0 5 fsyN and 
T 5 T0yN this yields
ci 5 1
T0
 #
T0
0
xa(t) exp(2ji2F0t)
 < 1
T0
 o
N21
k50
xa(kT ) exp(2ji2F0kT )T, N W 1
 5 T
T0
 o
N21
k50
x(k) exp(2ji2fskTyN)
 
 5 1
N o
N21
k50
x(k) exp(2jik2yN ), 0 , uiu , Ny2 
 (4.1.4)
Thus the Fourier coefficients of the periodic signal xa(t) can be approximated using 
the samples x(k). Interestingly enough, the summation part of (4.1.4) is the discrete Fou-
rier transform or DFT of the samples. Let X(i) 5
 DFT hx(k)j. Then the Fourier coeffi-
cients of xa(t) can be approximated as follows.
 
ci < X(i)
N , 0 # i , Ny2 
 (4.1.5)
The DFT produces M 5 Ny2 complex Fourier coefficients hc0, c1, Á , cM21j. To 
obtain the remaining coefficients in (4.1.1), observe from (4.1.2) that for real xa(t) the 
coefficients corresponding to negative values of i are the complex conjugates of the coef-
ficients corresponding to positive values of i. Thus
 
c2i < X *(i)
N , 0 # i , Ny2 
 (4.1.6)
4.1.2 DC Wall Transformer 
Many electronic items receive power from batteries or from a DC wall transformer. A DC 
wall transformer is an inexpensive power supply that approximates the constant voltage 
produced by a battery. A block diagram of a typical DC wall transformer is shown in 
Figure 4.1.
The transformer block steps the 120 volt 60 Hz sinusoidal AC input signal, xa, down to 
a lower voltage AC signal, ua. The full-wave bridge rectifier consists of four diodes arranged 
in a configuration that takes the absolute value of the AC signal, ua. Thus the signals xa(t), 
ua(t), and va(t) can be modeled as follows where 0 ,  , 1 depends on the desired DC 
voltage.
 
xa(t) 5 120Ï2 sin(120t) 
 (4.1.7a)
Figure 4.1:  
DC Wall 
Transformer
Bridge
rectifier
xa
ua
va
ya
Lowpass
filter
Transformer
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.1 Motivation    237
 
ua(t) 5 xa(t) 
 (4.1.7b)
 
va(t) 5 uua(t)u 
 (4.1.7c)
The full-wave bridge rectifier output va has a DC component or average value, d0y2, 
plus a periodic component that has a fundamental frequency of F0 5 120 Hz. To produce 
a pure DC output similar to that of a battery, the nonconstant part of va must be filtered 
out with the lowpass RC filter section whose transfer function is
 
Ha(s) 5
1
RCs 1 1 
 (4.1.8)
Of course the lowpass filter is not an ideal filter, so some of the nonconstant part 
of va survives in the wall transformer output ya in the form of a small AC ripple voltage. 
The end result is that a DC wall transformer output can be modeled as a DC com-
ponent, d0y2, plus a small periodic ripple component with a fundamental frequency  
of 120 Hz.
 
ya(t) 5 d0
2 1 o
M21
i51
di cos(240it 1 i) 
 (4.1.9)
For an ideal wall transformer, there is no AC ripple so di 5 0 for i . 0. Thus we can 
measure the quality of the wall transformer output (or any other DC power supply out-
put) by using the total harmonic distortion, THD, as follows.
 
Py 5 d 2
0
4 1 1
2 o
M21
i51
d 2
i  
 (4.1.10a)
 
THD 5
100(Py 2 d 2
0y4)
Py
 % 
 (4.1.10b)
Notice that this definition of total harmonic distortion is similar to that used in 
Chapter 1 for an ideal amplifier, except that in (4.1.10b) the term d 2
0y4 is removed from 
the numerator, whereas in (1.1.5) the term d 2
1y2 was removed from the numerator. This 
is because, for an ideal DC power supply, the output should be the zeroth harmonic, 
whereas for an ideal amplifier the output should be the first harmonic. The term d 2
0y4 
represents the power of the DC term or zeroth harmonic, while d 2
i y2 represents the power 
of the ith harmonic for i . 0.
From Appendix 1, the coefficients di and i of the cosine series in (4.1.9) can be 
obtained from the complex Fourier series coefficients in (4.1.2) as follows.
 
di 5 2uciu, 0 # i , M
 
 (4.1.11a)
 
i 5  tan211
2Imhcij
Rehcij 2, 0 # i , M 
 (4.1.11b)
To measure the harmonic distortion THD, we sample the output at N 5 2M points 
using a sampling frequency of fs 5 NF0. If Y(i) 5
 DFT hya(kT )j, then from (4.1.5) and 
(4.1.11) we have
 
di 5 2uY(i)u
N
, 0 # i , M
 
 (4.1.12a)
 
i 5  tan21S
2ImhY(i)j
RehY(i)j 2, 0 # i , M 
 (4.1.12b)
Ripple voltage
Total harmonic 
distortion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

238    Chapter 4  Fourier Transforms and Spectral Analysis
4.1.3 Frequency Response 
The DFT also can be used to characterize a linear discrete-time system or digital fil-
ter. Consider the discrete-time system shown in Figure 4.2. Recall from Chapter 3 that 
the system transfer function is the Z-transform of the zero-state output divided by the 
Z-transform of a nonzero input.
 
H(z) 5 Y(z)
X(z) 
 (4.1.13)
Also recall from Chapter 3 that if the system is stable, and we evaluate the transfer func-
tion along the unit circle using z 5 exp(  j2f T ), the resulting function of f  is called 
the frequency response. The samples of the frequency response can be easily approxi-
mated using the DFT. In particular, if X(i) 5
 DFT hx(k)j and Y(i) 5
 DFT hy(k)j, then 
the approximate frequency response evaluated at discrete frequency fi 5 ifsyN Hz is as 
follows, where the accuracy of the approximation improves as N increases.
 
H(i) 5 Y(i)
X(i), 0 # i , N 
 (4.1.14)
The frequency response specifies how much each sinusoidal input gets scaled in 
magnitude and shifted in phase as it passes through the system. For the DFT method 
of evaluating the frequency response in (4.1.14), the input signal x(k) should be cho-
sen such that it has power at all frequencies of interest so as to avoid division by zero. 
For example, we can let x(k) be a unit impulse or a random white noise signal. Since 
H(k) is complex, it can be expressed in polar form in terms of its magnitude and phase 
angle.
 
H(i) 5 A(i) expf j(i)g, 0 # i , N 
 (4.1.15)
Once H(i) is known, the steady-state response of the system to a sinusoidal 
input at a discrete frequency can be obtained from inspection. For example, suppose 
x(k) 5 c sin(2fnkT ) for some 0 # n , N. Then the steady-state output generated by this 
input is scaled in amplitude by A(n) and shifted in phase by (n).
 
yss(k) 5 A(n)c sinf2fnkT 1 (n)g 
 (4.1.16)
As an illustration, consider a stable second-order discrete-time system characterized 
by the following transfer function.
 
H(z) 5 10 1 20z21 2 5z22
1 2 .2z21 2 .63z22 
 (4.1.17)
This system is stable with poles at z 5 .5 and z 5 2.9. Suppose the sampling frequency 
is fs 5 100 Hz and the DFT is evaluated using N 5 256 points. The resulting magnitude 
response A(i) and phase response (i) for 0 # i # Ny2 are shown in Figure 4.3. Note 
Frequency response
Figure 4.2: A Linear 
Discrete-time 
System with Transfer 
Function H(z)
H(z)
x(k)
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.2 Discrete-time Fourier Transform (DTFT)     239
that only the positive frequencies are plotted. Values of i in the range Ny2 , i , N cor-
respond to the values of z 5 exp( j2fiT ) that are in the lower half of the unit circle and 
therefore represent negative frequencies.
Discrete-time Fourier Transform (DTFT) 
In Chapters 2 and 3 we focused on digital signal processing from the point of view of 
the system that acts on the input x(k) to produce the output y(k). In this chapter we look 
more carefully at the characteristics of the signals themselves.
4.2.1 DTFT 
Each discrete-time signal can be characterized in terms of how its average power is dis-
tributed over frequencies in the interval f2fsy2,  fsy2g. A particularly useful tool for this 
purpose is the discrete-time Fourier transform or DTFT. It is a transformation the maps 
a discrete time signal into a continuous frequency signal as follows.
The discrete-time Fourier transform or DTFT of x(k) is denoted X(f ) 5 DTFThx(k)j 
and defined
X( f ) 5
D o
`
k52`
x(k) exp(2jk2f T ), 0 # u f u # fsy2
4.2
DEFiniTion
4.1 DTFT
0
50
0
50
100
150
f (Hz)
f (Hz)
A(f)
0
10
15
5
20
25
30
35
40
45
10
15
5
20
25
30
35
40
45
50
24
22
0
2
4
(f)
Figure 4.3:  
Magnitude 
Response and 
Phase Response of 
System in (4.1.17) 
Using fs 5 100 Hz 
and N 5 256 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

240    Chapter 4  Fourier Transforms and Spectral Analysis
Note that the DTFT is really just the Z-transform X(z) evaluated along the unit circle 
using z 5 exp(j2f T ). That is,
 
X( f ) 5 X(z)uz5  exp( j2f T ), 0 # u f u # fsy2  
 (4.2.1)
As a consequence, X( f ) is well defined if and only if the region of convergence of X(z) 
includes the unit circle. Given the analysis in Chapters 2 and 3, this means that for a 
causal signal the DTFT X( f ) exists if x(k) is absolutely summable or if the poles of X(z) 
all lie strictly inside the unit circle.
The DTFT of x(k) is referred to as the spectrum of the signal x(k). Since the spec-
trum X( f ) is complex, it can be represented in polar form as X( f ) 5 Ax( f ) expfjx( f )g 
where
 
Ax( f ) 5 uX( f )u  
 (4.2.2a)
 
x( f ) 5 /X( f ) 
 (4.2.2b)
In this case Ax( f ) is called the magnitude spectrum of x(k), and x( f ) is called the phase 
spectrum of x(k).
The time signal x(k) can be recovered from its spectrum X( f ) using the inverse 
transform. To isolate sample x(i), multiply X( f ) by the complex conjugate exponential, 
exp(ji2f T ), and integrate over one period.
#
fsy2
2fsy2
X( f ) exp( ji2f T )df 5#
fsy2
2fsy2 o
`
k52`
x(k) exp(2jk2f T ) exp( ji2f T )df
 5 o
`
k52`
x(k)#
fsy2
2fsy2
 expfj(i 2 k)2fTgdf
 5 o
`
k52`
x(k)fs(i 2 k)
 
5 x(i)fs 
 (4.2.3)
Here we have used the fact that the complex exponential is periodic with period fs when 
i Þ k. It is valid to interchange the order of the integral and the sum because x(k) is abso-
lutely summable. Solving (4.2.3) for x(i) then yields the inverse DTFT or IDTFT.
 
 IDTFT hX( f )j 5 1
fs#
fsy2
2fsy2
X( f ) exp(jk2f T )df, uku $ 0  
 (4.2.4)
The DTFT in Definition 4.1 is sometimes called the analysis equation because it decom-
poses a signal into its spectral components. The inverse DTFT in (4.2.4) is then called 
the synthesis equation because it reconstructs or synthesizes the signal from its spectral 
components.
From Euler’s identity, exp( j2f T ) is periodic with period fs. It then follows from 
Definition 4.1 that the spectrum of x(k) is also periodic with period fs.
 
X(f 1 fs) 5 X(f ) 
 (4.2.5)
The periodic nature of X(f ) also follows from the observation that X(f ) is X(z) evalu-
ated along the unit circle with each period corresponding to one trip around the circle. 
Since X(f ) is periodic with period fs, the frequency is typically restricted to the interval 
f2fsy2, fsy2g.
Spectrum
Magnitude, phase 
spectra
IDTFT
Periodic property
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.2 Discrete-time Fourier Transform (DTFT)     241
If the time signal x(k) is real, then the frequency interval can be restricted still fur-
ther. This is a consequence of the symmetry property of the DTFT. When x(k) is real, it 
follows from Definition 4.1 that
 
X(2f ) 5 X *(f ) 
 (4.2.6)
Thus the spectrum of x(k) along negative frequencies is just the complex conjugate of the 
spectrum of x(k) at positive frequencies. Just as was the case with the frequency response 
in Chapter 3, the symmetry property can be used to show that the magnitude spectrum 
of a real signal is an even function of f , and the phase spectrum of a real signal is an odd 
function of f .
 
Ax(2f ) 5 Ax( f )
 
 (4.2.7a)
 
x(2f ) 5 2x( f ) 
 (4.2.7b)
Consequently, for real signals all of the essential information about the spectrum is con-
tained in the nonnegative frequency range f0, fsy2g. A summary of the symmetry proper-
ties of the DTFT is shown in Table 4.1
Symmetry property
Real signals
Property 
Equation 
x(k)
Periodic 
X(f 1 fs) 5 X(f)
General 
Symmetry 
X(2f) 5 X*(f)
Real 
Even Magnitude 
Ax(2f) 5 Ax(f)
Real 
Odd Phase 
x(2f) 5 2x(f)
Real 
Table 4.1:  
Symmetry Properties 
of the DTFT 
Spectrum of Causal Exponential
EXAMPLE 4.1
As an illustration of using the DTFT to analyze a discrete-time signal, consider the  
following causal exponential.
x(k) 5 ck(k)
From Table 3.1 the Z-transform of this signal is
X(z) 5
z
z 2 c
 5
1
1 2 cz21
Here X(z) has a pole at z 5 c. Thus for the DTFT to converge it is necessary that 
ucu , 1. From (4.2.1) the spectrum of x(k) is
X( f ) 5
1
1 2 c exp(2j2f T )
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

242    Chapter 4  Fourier Transforms and Spectral Analysis
20.5
0
0.5
0
2
4
6
20.5 20.4 20.3 20.2 20.1
0.1
0.2
0.3
0.4
20.4 20.3 20.2 20.1
0.1
0.2
0.3
0.4
0
0.5
21
20.5
0
0.5
1
f/fs
f/fs
Ax(f)
x(f)
Figure 4.4: Magnitude and Phase Spectra of Causal Exponential with c 5 0.8 
Using Euler’s identity, the magnitude spectrum of the causal exponential is
Ax( f ) 5 uX( f )u
5
1
Ïf1 2 c cos(2f T )g2 1 c2 sin2(2f T )
5
1
Ï1 2 2c cos(2f T ) 1 c2
Similarly, the phase spectrum is
x( f ) 5 /X( f )
 5 2 tan213
c sin(2f T )
1 2 c cos(2f T )4
Plots of the magnitude spectra and the phase spectra, obtained by running 
exam4_1, are shown in Figure 4.4. Note how the magnitude spectrum is even 
and the phase spectrum is odd. Also observe that normalized frequency is plotted 
along the abscissa because x(k) may or may not have been obtained by sampling 
an underlying continuous-time signal. Thus the independent variable is
f
 norm  5
D  f
fs
Using normalized frequency, f
 norm , is equivalent to setting the sampling interval to 
T 5 1 sec.
Normalized frequency
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.2 Discrete-time Fourier Transform (DTFT)     243
Signals can be categorized or classified based on the part of the overall spectrum 
or frequency range that they occupy (Proakis and Manolakis, 1992). Table 4.2 summa-
rizes some practical signals that include examples taken from biomedical, geological, and 
communications applications. Note the impressive range of frequencies (26 orders of 
magnitude!) going from circadian rhythms that oscillate at approximately one cycle per 
day to lethal gamma rays that are faster than a billion billion Hz.
4.2.2 Properties of the DTFT 
The DTFT has several properties that it inherits directly from the Z-transform. There are 
a number of additional properties that are specific to the DTFT itself.
Time Shift Property 
An example of a property that comes directly from the Z-transform is the time shift 
property. Recall that multiplying the Z-transform by z2r is equivalent to delaying the 
signal x(k) by r samples. Since X( f ) is X(z) evaluated along the unit circle, the factor z2r 
is exp(2j2rf T ). Thus the DTFT version of the time shift property is
 
 DTFT hx(k 2 r)j 5 exp(2j2rf T )X( f )  
 (4.2.8)
Frequency Shift Property 
There is a dual to the time shift property called the frequency shift property. Using the 
inverse DTFT in (4.2.4) and a change of variable,
 IDTFT  hX(f 2 F0)j 5 1
fs#
fsy2
2fsy2
X(f 2 F0) exp(  jk2fT )df
Time shift property
Frequency shift 
property
Table 4.2: Frequency 
Ranges of Some 
Practical Signals 
Signal Type 
Frequency Range (Hz) 
Circadian rhythm 
1.1 3 1025 2 1.2 3 1025 
Earthquake 
1022 2 101 
Electrocardiogram (ECG) 
0 2 102 
Electroencephalogram (EEG) 
0 2 102 
AC power 
5 3 102 2 6 3 102 
Wind 
102 2 103 
Speech 
102 2 4 3 103 
Audio 
2 3 101 2 2 3 104 
AM radio 
5.4 3 105 2 1.6 3 106 
FM radio 
8.8 3 107 2 1.08 3 108 
Cell phone 
8.1 3 108 2 9 3 108 
TV 
3 3 108 2 9.7 3 108 
GPS 
1.52 3 109 2 1.66 3 109 
Shortwave radio 
3 3 106 2 3 3 109 
Radar, microwave 
3 3 109 2 3 3 1012 
Infrared light 
3 3 1012 2 4.3 3 1014 
Visible light 
4.3 3 1014 2 7.5 3 1014 
Ultraviolet light 
7.5 3 1014 2 3 3 1017 
X-ray 
3 3 1017 2 3 3 1019 
Gamma ray 
5 3 1019 2 1021 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

244    Chapter 4  Fourier Transforms and Spectral Analysis
 5 1
fs#
fsy21F0
2fsy21F0
X(F ) expf jk2(F 1 F0)T gdF, F 5 f 2 F0
 5
 exp(jk2F0T)
fs
#
fsy2
2fsy2
X(F) exp(jk2FT)dF
 
5 exp(jk2F0T)x(k) 
 (4.2.9)
Thus the frequency shift property of the DTFT is
 
 DTFT h exp(jk2F0T)x(k)j 5 X(f 2 F0)  
 (4.2.10)
Parseval’s identity 
Recall from Chapter 2 that absolutely summable signals are also square summable and 
therefore energy signals. For energy signals, there is a simple relationship between the 
time signal and its spectrum. Using Definition 4.1, we have
 
#
fsy2
2fsy2
X( f )Y*( f )df 5#
fsy2
2fsy2 o
`
k52`
x(k) exp(2j2kfT) o
`
i52`
y*(i) exp(j2ifT)df
 5 o
`
k52` o
`
i52`
x(k)y*(i)#
fsy2
2fsy2
 exp[2j 2 (k2i)f T ]df
 5 o
`
k52` o
`
i52`
x(k)y*(i)fs(k 2 i)
 
 5 fs o
`
k52`
x(k)y*(k) 
 (4.2.11)
This leads to the DTFT version of the following result known as Parseval’s identity.
Let x(k) and y(k) be absolutely summable with discrete-time Fourier transforms X( f ) 
and Y( f ), respectively. Then
o
`
k52`
x(k)y*(k) 5 1
fs#
fsy2
2fsy2
X( f )Y*( f )df
Note that when y(k) 5 x(k) in Proposition 4.1, the left-hand side reduces to the energy 
of the signal x(k).
 
o
`
k52`
ux(k)u2 5 1
fs#
fsy2
2fsy2
uX( f )u2df  
 (4.2.12)
Thus Parseval’s identity provides us with a different way to compute the energy Ex using 
the spectrum X( f ). With this in mind, the energy density of x(k) is defined as follows.
 
Sx( f ) 5
D  
uX( f )u2  
 (4.2.13)
PRoPoSiTion 
4.1 Parseval’s identity: 
DTFT
Energy density
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.2 Discrete-time Fourier Transform (DTFT)     245
It follows that the total energy of a signal is proportional to the integral of the energy 
density. More generally, for a real signal where the energy density is an even function, the 
amount of energy in the nonnegative frequency band ff1,  f2g is
 
E(f1, f2) 5 2
fs#
f2
f1
Sx( f )df  
 (4.2.14)
The factor two in (4.2.14) accounts for the negative frequencies. The total energy is 
Ex 5 E(0, fsy2). Recall that the auto-correlation of x(k) evaluated at a lag of k 5 0 is 
another way to express the total energy, Ex 5 rxx(0).
Wiener-Khintchine Theorem
One of the properties that is inherited from the Z-transform is the correlation property. Recall 
that when z 5  exp( j2f T ), replacing z by 1yz is equivalent to replacing f by 2f. Therefore 
from Table 3.3, the DTFT of the cross-correlation, ryx(k), of finite signals of length L is
 
Ryx( f ) 5 Y( f )X(2f  )
L
 
 (4.2.15)
Some authors define cross-correlation using infinite signals and therefore do not divide 
by the signal length L. The definition of finite cross-correlation introduced in Chapter 2 
is used here because it is more consistent with the generalization of cross-correlation to 
random signals that is introduced in Chapter 9.
It is of interest to consider the case of auto-correlation when y(k) 5 x(k). In this case 
X( f )X(2f) 5 uX( f )u2 5 Sx( f ). This leads to the Wiener-Khintchine theorem, which says 
that the DTFT of the auto-correlation of x(k) is a scaled version of the energy density 
spectrum of x(k).
 
Rxx( f ) 5 Sx( f )
L
 
 (4.2.16)
The properties of the DTFT are summarized in Table 4.3. Most of these properties 
have direct analogs with the Z-transform properties in Table 3.3 and are obtained by 
substituting z 5 exp(  j2fT ).
Wiener-Khintchine 
theorem
Property 
Time Signal 
DTFT 
Linearity 
ax(k) 1 by(k) 
aX( f ) 1 bY( f ) 
Time shift 
x(k 2 r) 
exp(2j2rfT)X( f ) 
Frequency shift 
exp(jk2F0T)x(k) 
X(f 2 F0) 
Time reversal 
x(2k) 
X(2f) 
Complex conjugate 
x*(k) 
X*(2f) 
Convolution 
h(k) w x(k) 
H( f )X( f ) 
Correlation 
ryx(k) 
Y( f )X(2f)
L
 
Wiener-Khintchine 
rxx(k) 
Sx( f )
L
 
 o
`
k52` x(k)y*(k) 
1
fs#
fsy2
2fsy2
X( f )Y*( f )df 
Parseval
  o
`
k52`
ux(k)u2 
1
fs#
fsy2
2fsy2
uX( f )u2df 
Table 4.3: DTFT 
Properties 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

246    Chapter 4  Fourier Transforms and Spectral Analysis
inverse DTFT of ideal  
Lowpass Characteristic
EXAMPLE 4.2
An ideal lowpass filter with a cutoff frequency of 0 , Fc , fsy2 has a phase 
response of ( f ) 5 0 and a magnitude response consisting of a rectangular win-
dow of radius Fc centered at f 5 0.
Hlow( f ) 5 ( f 1 Fc) 2 ( f 2 Fc), 0 # u f u # fsy2
Here the rectangle window is expressed using a step up at f 5 2Fc followed by 
a step down at f 5 Fc. Consider the problem of finding the impulse response, 
hlow(k), using the expression for the IDTFT in (4.2.4) and the identities from 
Appendix 2.
hlow(k) 5 1
fs#
fsy2
2fsy2
Hlow( f ) exp(j2kfT)df
 5 1
fs#
Fc
2Fc
 exp(j2kfT)df
 5 1
fs
 
 exp( j2kf T )
j2kT
u
Fc
2Fc
 5
 exp(j2kFcT) 2  exp(2j2kFcT)
j2k
 5
 sin(2kFcT)
k
 5 2FcT sin(2kFcT)
2kFcT
Recall from Section 1.2 that 
 sinc (x) 5
D  sin(x)y(x). Thus the ideal lowpass 
impulse response can be written in terms of the sinc function as
hlow(k) 5 2FcT sinc (2kFcT )
Plots of hlow(k) and Hlow( f ) for the case when Fc 5 fsy4 are shown in Figure 4.5. 
Notice that the impulse response is noncausal. Therefore an ideal lowpass character-
istic cannot be achieved in real time with a physically realizable filter. In Chapters 6 
and 7 we examine a variety of ways to approximate the ideal lowpass characteristic 
with FIR and IIR digital filters.
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3 Discrete Fourier Transform (DFT)     247
Discrete Fourier Transform (DFT) 
4.3.1 DFT 
In this section we focus on causal finite signals. The discrete Fourier transform, or DFT, 
can be regarded as a special case of the discrete-time Fourier transform. Recall from 
Definition 4.1 that the DTFT of a causal signal x(k) is
 
X( f ) 5 o
`
k50
x(k) exp(2jk2f T ), 0 # u f  u # fsy2 
 (4.3.1)
4.3
215
210
25
0
5
10
15
20.2
0
0.2
0.4
0.6
k
f/fs
(a)
20.5
0
0.5
20.5
0
0.5
1
1.5
(b)
hlow(k)
Hlow(f)
20.4 20.3 20.2 20.1
0.1
0.2
0.3
0.4
Figure 4.5:  Impulse Response (a) and Frequency Response (b) of an Ideal 
Lowpass Filter with Cutoff Frequency F0 5 fsy4
Table 4.4:  
Basic DTFT Pairs 
x(k)
X( f )
Parameters 
(k) 
1 
—
ck(k) 
 exp( j2fT)
 exp( j2fT) 2 c 
ucu , 1 
kck(k) 
c exp( j2fT)
fexp( j2fT) 2 cg2 
 ucu , 1
2FcT  sinc (2kFcT) 
(f 1 Fc) 2 (f 2 Fc) 
0 , Fc , fsy2 
(k 1 r) 2 (k 2 r 2 1) 
 sinf(2r 1 1)fg
 sin(f)
—
A table of basic DTFT pairs is shown in Table 4.4. This was constructed by starting 
with the stable Z-transform pairs in Table 3.2 and adding some additional pairs.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

248    Chapter 4  Fourier Transforms and Spectral Analysis
Although the DTFT is an important tool for analyzing discrete-time signals, it suffers 
from certain practical limitations when used as a computational tool. One drawback is 
that a direct evaluation of X( f ) using the definition requires an infinite number of floating 
point operations, or FLOPs. This is compounded by a second computational drawback, 
namely that the transform itself must be evaluated at an infinite number of frequencies f . 
The first limitation can be removed by focusing on signals of finite duration. Since X( f ) 
is just X(z) evaluated along the unit circle, X( f ) converges for signals that are absolutely 
summable. But if x(k) is absolutely summable, then ux(k)u S 0 as k S `. Consequently, 
for sufficiently large values of N, X( f ) can be approximated by the following finite sum.
 
X( f ) < o
N21
k50
x(k) exp(2jk2f T ) 
 (4.3.2)
To address the second limitation, we sample X( f ) by evaluating it at N equally spaced 
values of f . In particular, consider the following discrete frequencies equally spaced over 
one period of X( f ).
 
fi 5 ifs
N, 0 # i , N 
 (4.3.3)
The discrete frequencies, fi, are sometimes referred to as bin frequencies. Let zi be the 
point in the complex plane corresponding to frequency fi.
 
zi 5  exp(ji2yN) 
 (4.3.4)
Notice that uziu 5 1. Thus the N evaluation points in (4.3.4) are equally spaced around the 
unit circle as shown in Figure 4.6 for the case N 5 8. Observe that the unit circle is tra-
versed in the counter-clockwise direction with z0 5 1, zNy4 5 j, zNy2 5 21, and z3Ny4 5 2j. 
The formulation of the discrete Fourier transform can be simplified by introducing the 
following factor which corresponds to zi for i 5 21.
 
WN 5
D   exp(2j2yN )  
 (4.3.5)
Bin frequencies
Figure 4.6:  
Evaluation Points 
of DFT 
Re(z)
Im(z)
1
21
21
1
z2
z3
z4
z5
z6
z7
z1
z0
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3 Discrete Fourier Transform (DFT)     249
Using Euler’s identity it is clear that W N
N 5 1. Consequently, the factor WN can be 
thought of as the Nth root of unity. Note that W ik
N can be regarded as a function of i or k. 
This function has a number of interesting symmetry properties, including the following 
orthogonal property whose proof is left as an exercise (see Problem 4.15).
 
o
N21
i50
W ik
N 5 N(k), 0 # k , N 
 (4.3.6)
The discrete values of z in (4.3.4) can be reformulated in terms of the Nth root of 
unity as zi 5 W 2i
N . When this value for zi is substituted into the truncated expression for 
the DTFT in (4.3.2), the resulting transformation from discrete-time x(k) to discrete- 
frequency X(i) is called the discrete Fourier transform or DFT.
Let x(k) be a causal N-point signal, and let WN 5 exp(2j2yN ). The discrete Fourier 
transform or DFT of x(k), denoted X(i) 5
 DFT hx(k)j, is defined
X(i) 5
D  o
N21
k50
x(k)W ik
N, 0 # i , N
In terms of notation, it should pointed out that the same base symbol is being used to 
denote the Z-transform, X(z); the DTFT, X( f ); and the DFT, X(i). This is consistent 
with the convention adopted earlier where the argument type, a complex z, a real f , or 
an integer i, is used to distinguish between the different cases and dictate the meaning 
of X. This approach is used in order to avoid a proliferation of different, but related, 
symbols.
The continuous-time Fourier transform, Xa( f ) 5 FThxa(t)j, has an inverse whose 
form is almost identical to the original transform. The same is true of the discrete Fourier 
transform. The inverse DFT or IDFT is denoted x(k) 5
 IDFT hX(i)j, and it is computed 
as follows.
 
x(k) 5  1
No
N21
i50
X(i)W2ki
N , 0 # k , N  
 (4.3.7)
To verify that (4.3.7) does indeed represent the IDFT, we use (4.3.6) and Definition 4.2.
o
N21
i50
X(i)W 2ki
N
5 o
N21
i503o
N21
m50
x(m)W im
N4W 2ki
N
 
5 o
N21
m50
x(m)o
N21
i50
W(m2k)i
N
 
5 o
N21
m50
x(m)N(m 2 k)
 
5 Nx(k) 
 (4.3.8)
Nth root of unity
Orthogonal property
DEFiniTion
4.2 DFT
notation!
IDFT
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

250    Chapter 4  Fourier Transforms and Spectral Analysis
Thus the IDFT is identical to the DFT, except that WN has been replaced by its com-
plex conjugate W *
N 5 W 21
N , and the final result is normalized by N. In practical terms, 
this means that any algorithm devised to compute the DFT can be used, with minor 
modification, to compute the IDFT as well.
4.3.2 Matrix Formulation 
The DFT can be interpreted as a transformation or mapping from a vector of input sam-
ples x to a vector of output samples X.
 
x 5 fx(0), x(1), Á , x(N 2 1)gT 
 (4.3.9a)
 
X 5 fX(0), X(1), Á , X(N 2 1)gT 
 (4.3.9b)
Here x and X are N 3 1 column vectors, written as transposed rows to conserve space. 
Since the DFT is a linear transformation from x to X, it can be represented by an N 3 N 
matrix. Consider, in particular, the matrix Wik 5 W ik
N where the row index i and column 
index k are assumed to start from zero. For example, for the case N 5 5
 
W 53
W 0
N
W 0
N
W 0
N
W 0
N
W 0
N
W 0
N
W 1
N
W 2
N
W 3
N
W 4
N
W 0
N
W 2
N
W 4
N
W 6
N
W 8
N
W 0
N
W 3
N
W 6
N
W 9
N
W 12
N
W 0
N
W 4
N
W 8
N
W 12
N
W 16
N4
 
 (4.3.10)
Note that W is symmetric. Comparison of (4.3.9) and (4.3.10) with Definition 4.2 
reveals that the DFT can be expressed in vector form as
 
X 5 Wx  
 (4.3.11)
For small values of N, (4.3.11) can be used to compute the DFT. As we shall see, for 
moderate to large values of N, a much more efficient FFT implementation of the DFT 
is the method of choice.
The vector form of the DFT also can be used to compute the IDFT. Multiplying 
both sides of (4.3.11) on the left by W 21 yields x 5 W 21X. If we compare this equa-
tion with (4.3.7), we find that there is no need to explicitly invert the matrix W because 
W 21 5 W *yN. Thus the matrix form for the IDFT is
 
x 5 W *X
N
 
 (4.3.12)
The following examples illustrate computation of the DFT and the IDFT for small val-
ues of N.
Matrix DFT
Matrix IDFT
DFT
EXAMPLE 4.3
As an example of computing a DFT using the vector form, suppose the input 
samples are as follows.
x 5 f3, 21, 0, 2gT
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3 Discrete Fourier Transform (DFT)     251
Thus N 5 4 and from (4.3.5)
W4 5 cos1
2
4 2 2 j sin1
2
4 2
 5 2j
Next from (4.3.10) and (4.3.11) the DFT of x is
X 5 Wx
 53
W 0
4
W 0
4
W 0
4
W 0
4
W 0
4
W 1
4
W 2
4
W 3
4
W 0
4
W 2
4
W 4
4
W 6
4
W 0
4
W 3
4
W 6
4
W 9
44
 x
 53
1
1
1
1
1
2 j
2 1
j
1
2 1
1
2 1
1
j
2 1
2 j43
3
21
0
24
 53
4
3 1 j3
2
3 2 j34
Note that even though the signal x(k) is real, its DFT X(i) is complex.
iDFT
EXAMPLE 4.4
As a numerical check, suppose we compute the IDFT of the result from Example 4.3.
X 5 f4, 3 1 j 3, 2, 3 2 j 3gT
Using N 5 4, the matrix W from Example 4.3, and (4.3.12) we have
 x 5 W *X
N
 5 1
4 3
1
1
1
1
1
2 j
2 1
j
1
2 1
1
2 1
1
j
2 1
2 j4
*
3
4
3 1 j3
2
3 2 j34
 5 1
4 3
1
1
1
1
1
j
2 1
2 j
1
2 1
1
2 1
1
2 j
2 1
j43
4
3 1 j3
2
3 2 j34
 5 1
43
12
24
0
84
53
3
21
0
24
 ✓
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

252    Chapter 4  Fourier Transforms and Spectral Analysis
4.3.3 Fourier Series and Discrete Spectra
Periodic signals have a special form of spectrum called a discrete spectrum. Suppose xa(t) 
is a periodic continuous-time signal with period T0 and fundamental frequency F0 5 1yT0. 
Then from Appendix 1, xa(t) can be expanded into a complex Fourier series
 
xa(t) 5 o
`
i52`
ci exp(ji2F0t) 
 (4.3.13)
where the ith complex Fourier coefficient is
 
ci 5 1
T0#
T0
0
xa(t) exp(2ji2F0t)dt, 0 # uiu , ` 
 (4.3.14)
Let x(k) 5 xa(kT) be the kth sample of xa(t) using a sampling interval of T. Since x(k) 
is a power signal, to examine the spectrum of x(k), we need to generalize X( f ) to include 
the possibility of impulse terms such as a(f 2 F0). By starting with X( f ) 5 a(f 2 F0), 
applying the inverse DTFT, and using the sifting property of the unit impulse, one can 
show that the DTFT of a complex exponential in the time domain is a shifted impulse in 
the frequency domain.
 
 DTFT h fs exp( j2F0kT)j 5 a( f 2 F0) 
 (4.3.15)
Using (4.3.15) and the linearity of the DTFT, the spectrum of the periodic signal x(k) 
is then
 
X( f ) 5 1
fs o
`
i52`
ci a( f 2 iF0)  
 (4.3.16)
Notice that X( f ) in (4.3.16) is zero everywhere except at the harmonic frequencies, iF0, 
where it contains impulses of strength ciyfs. Since X( f ) is zero except at integer multiples 
of F0, it is referred to as a discrete-frequency spectrum or simply a discrete spectrum. For 
a periodic signal with a discrete spectrum, all of the power is concentrated at the fun-
damental frequency F0 and its harmonics. This includes the zeroth harmonic if the DC 
component or average value of x(k) is nonzero.
Fourier Coefficients 
The periodic continuous-time signal xa(t) in (4.3.13) can be approximated by truncating 
the Fourier series to M harmonics.
 
xa(t) <
o
M21
i52(M21)
ci exp(ji2F0t) 
 (4.3.17)
Suppose that xa(t) is sampled at N 5 2M points using a sampling rate of fs 5 NF0. In this 
case the N samples cover one period of xa(t) with T0 5 NT. If the number of samples per 
period is sufficiently large, then over the ith sampling interval the integrand in (4.3.14) 
can be approximated by its initial value. This leads to the following approximation for the 
ith Fourier coefficient.
ci <
1
NTo
N21
k50
xa(kT) exp(2ji2F0kT)T
 5 1
No
N21
k50
x(k) exp(2jik2yN)
Power signal
Discrete spectrum
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3 Discrete Fourier Transform (DFT)     253
 5 1
No
N21
k50
x(k)W ik
N
 
 5 X(i)
N , 0 # i , M
 
 (4.3.18)
Hence the Fourier coefficients of xa(t) can be obtained from the DFT of the samples of 
xa(t). For a real signal xa(t), it follows from (4.3.14) that c2i 5 c*i . Thus the complete set 
of Fourier coefficients is
 
ci <5
X(i)
N ,
0 # i , M
X *(i)
N , 2M , i , 0
 
 (4.3.19)
The Fourier series is often expressed in real form using either sines and cosines or 
cosines with phase shift.
 
xa(t) 5 d0
2 1 o
`
i51
di cos(2iF0t 1 i) 
 (4.3.20)
The magnitude di and phase angle i of the ith harmonic also can be obtained using the 
DFT. From Appendix 1 and (4.3.19) we have
 
di < 2uX(i)u
N
 
 (4.3.21a)
 
i < /X *(i) 
 (4.3.21b)
In general, DFT sample X(i) specifies the magnitude and phase angle of the ith spec-
tral component of x(k) where x(k) can be thought of as one cycle of a longer periodic 
signal, xp(k). The DFT can be written in polar form as X(i) 5 Ax(i) expfjx(i)g. In this 
case the magnitude spectrum, Ax(i), and phase spectrum, x(i), are defined as follows for 
0 # i , N.
 
Ax(i) 5
D  
uX(i)u 
 (4.3.22a)
 
x(i) 5
D  
/X(i) 
 (4.3.22b)
The average power of x(k) at discrete frequency fi 5 ifsyN can be determined from 
the power density spectrum, which is just the square of the magnitude spectrum, normal-
ized by N.
 
Sx(i) 5
D  uX(i)u2
N
, 0 # i , N  
 (4.3.23)
Fourier coefficients
Magnitude, phase 
spectra
Power density 
spectrum
Spectra
EXAMPLE 4.5
As a very simple example of a discrete time signal and its spectra, consider the 
unit impulse x(k) 5 (k). Using Definition 4.2
X(i) 5 o
N21
k50
x(k)W ik
N
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

254    Chapter 4  Fourier Transforms and Spectral Analysis
For continuous-time signals, the Fourier transform Xa( f ) 5 FThxa(t)j is used to 
compute the spectrum. Together with the DTFT and the DFT this makes three Fourier 
transforms for computing signal spectra. A comparison of the three Fourier transforms 
is listed in Table 4.5. They differ from one another in the types of independent variables, 
continuous or discrete.
The DSP Companion contains the following function for evaluating the mag-
nitude, phase, and power density spectra of a finite discrete-time signal using  
the DFT.
% F_SPEC: Compute magnitude, phase, and power density spectra
%
% Usage:
% 
[A,phi,S,f] = f_spec (x,N,fs);
% Pre:
% 
x 
= vector of length M containing signal samples
% 
N 
= optional integer specifying number of points
% 
 
 in spectra (default M). If N > M, then N-M
% 
 
 zeros are padded to x.
% 
fs = optional sampling frequency in Hz (default 1)
% Post:
% 
A 
= vector of length N containing magnitude spectrum
% 
phi = vector of length N containing phase spectrum (radians)
% 
S 
= vector of length N containing power density spectrum
% 
f 
= vector of length N containing the discrete evaluation
% 
 
 frequencies: 0 <= f(i) <= (N-1)*fs/N.
%
DSP Companion
DSP Companion
 5 o
N21
k50
(k)W ik
N
5 1, 0 # i , N
It then follows from (4.3.22) and (4.3.23) that the magnitude, phase, and power 
density spectra of the unit impulse for 0 # i , N are
Ax(i) 5 1
x(i) 5 0
Sx(i) 5 1
N
The fact that Sx(i) is a constant nonzero value for all i means that the unit impulse 
has its power distributed evenly over all N discrete frequencies.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3 Discrete Fourier Transform (DFT)     255
The DTFT is a special case of the Z-transform evaluated along the unit circle. For 
causal finite signals, the DFT is a sampled version of the DTFT. For the DTFT and the 
DFT to exist, the region of convergence of the Z-transform must include the unit circle. 
If x(k) is causal, then x(k) must be absolutely summable or, equivalently, the poles of 
X(z) must lie strictly inside the unit circle. The relationships between the domains of X(z), 
X( f ), and X(i) are summarized in Figure 4.7.
4.3.4 DFT Properties 
Like the Z-transform and the DTFT, the DFT has a number of important properties. 
First consider the Nth root of unity, WN, which satisfies some useful symmetry properties. 
For convenient reference, the symmetry properties of WN are summarized in Table 4.6.  
Each entry can be verified using (4.3.5). Note that the first four properties verify that W k
N 
moves clockwise around the unit circle as k ranges from 0 to N. The remaining symmetry 
properties are useful for developing important properties of the DFT and the fast Fourier 
transform, or FFT.
Periodic Property 
Note from Definition 4.2 that the dependence of X(i) on i occurs only in the exponent of 
WN. As a result, X(i) can be regarded as a function that is defined for all integer values of 
i, not just for 0 # i , N. It is of interest to examine what happens when we go outside the 
range 0 # i , N. First note from entry four of Table 4.6 that W Nk
N 5 1 for every integer k. 
Figure 4.7:  
Relationships 
among the 
Domains of the 
Z-transform, the 
DTFT, and the DFT 
of Causal Signals 
Re(z)
Im(z)
1
1
21
21
VROC
X(z) 5 Z{x(k)}
X(f) 5 DTFT{x(k)}
X(i) 5 DFT{x(k)}
Transform 
Symbol
Time 
Frequency 
Fourier transform 
Xa( f ) 5 FThxa(t)j 
Continuous 
Continuous 
Discrete-time Fourier transform 
X( f ) 5 DTFThx(k)j 
Discrete 
Continuous 
Discrete Fourier transform 
X(i) 5 DFThx(k)j 
Discrete 
Discrete 
Table 4.5: Comparison 
of Fourier Transforms 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

256    Chapter 4  Fourier Transforms and Spectral Analysis
We use this result to demonstrate that X(i) is periodic.
X(i 1 N) 5 o
N21
k50
x(k)W (i 1 N)k
N
 5 o
N21
k50
x(k)W ik
N
 
 5 X(i)
 
 (4.3.24)
Thus the DFT is periodic with a period of N. This is analogous to X( f ) being peri-
odic with period fs.
Midpoint Symmetry Property 
If the signal x(k) is real, then all of the information needed to reconstruct the N real 
points of x(k) is contained in the first Ny2 complex points of X(i). To see this, we compute 
the DFT starting at the end and working backwards. In particular from Definition 4.2,  
Table 4.6, and (4.3.24), we have
 X(N 2 i) 5 o
N21
k50
x(k)W  (N 2 i)k
N
 5 o
N21
k50
x(k)W 2ik
N W Nk
N
 5 o
N21
k50
x(k)W 2ik
N
 
 5 X *(i)
 
 (4.3.25)
The most important consequence of (4.3.25) lies in the observation that for a real 
signal the DFT contains redundant information. Recall that X(i) can be expressed in 
polar form as X(i) 5 Ax(i) expf  jx(i)g where Ax(i) is the magnitude spectrum and x(i) is 
the phase spectrum. In many cases of practical interest, N is a power of two and there-
fore even. When N is even, it follows from (4.3.25) that the magnitude and phase spectra 
exhibit the following symmetry about the midpoint, X(Ny2).
 
Ax(Ny2 1 i) 5 Ax(Ny2 2 i), 0 # i , Ny2  
 (4.3.26a)
 
x(Ny2 1 i) 5 2x(Ny2 2 i), 0 # i , Ny2 
 (4.3.26b)
Thus the magnitude spectrum Ax(i) exhibits even symmetry about the midpoint, while the 
phase spectrum x(i) exhibits odd symmetry about the midpoint. The following numeri-
cal example illustrates this important observation.
Periodic property
Midpoint symmetry 
property
Property 
Description 
Property 
 Description 
1 
W Ny4
N
5 2j 
5 
W (i1N)k
N
5 W ik
N 
2 
W Ny2
N
5 21 
6 
W i1Ny2
N
5 2W i
N 
3 
W 3Ny4
N
5 j 
7 
W 2i
N 5 W i
Ny2 
4 
W N
N 5 1 
8 
W *N 5 W 21
N  
Table 4.6: Symmetry 
Properties of WN
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3 Discrete Fourier Transform (DFT)     257
Figure 4.8: Midpoint Symmetry of Magnitude and Phase Spectra of Signal in 
Example 4.6 
0
64
128
192
256
0
2
4
6
8
10
i
0
64
128
192
256
21
20.5
0
0.5
1
i
Ax(i)
x(i)
DFT Midpoint Symmetry
EXAMPLE 4.6
Consider the following real signal of length N 5 256.
x(k) 5 f.8k 2 (2.9)kg(k), 0 # k , 256
The DFT of this signal can be found by running exam4_6. Plots of the resulting 
magnitude spectrum Ax(i) and phase spectrum x(i) for 0 # i , N are shown in 
Figure 4.8. Observe the even symmetry of Ax(i) about sample Ny2 5 128 and the 
odd symmetry of x(i) about Ny2. In this case all of the information about the 
256 real points in x(k) is encoded in the first 128 complex points of X(i).
Property 
Equation 
x(k)
Periodic 
X(i 1 N) 5 X(i) 
General 
Symmetry 
X*(i) 5 X(N 2 i) 
Real 
Even magnitude 
Ax(Ny2 1 i) 5 Ax(Ny2 2 i) 
Real, N even 
Odd phase 
x(Ny2 1 i) 5 2x(Ny2 2 i) 
Real, N even 
Table 4.7:  
Symmetry Properties 
of the DFT 
A summary of the symmetry properties of the DFT is shown in Table 4.7. These are 
analogous to the symmetry properties of the infinite-dimensional DTFT in Table 4.3.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

258    Chapter 4  Fourier Transforms and Spectral Analysis
Linearity Property 
The DFT is a linear transformation from one N-point sequence into another. That is, if 
x(k) and y(k) are signals and a and b are constants, then from Definition 4.2
 DFT hax(k) 1 by(k)j 5 o
N21
k50
fax(k) 1 by(k)gWik
N
 5 ao
N21
k50
x(k)Wik
N 1 bo
N21
k50
y(k)Wik
N
 
5 aX(i) 1 bY(i) 
 (4.3.27)
Thus the DFT of the sum of two signals is just the sum of the DFTs of the signals. Simi-
larly, the DFT of a scaled signal is just the scaled DFT of the signal.
Time Reversal Property 
The next property requires that we work with the periodic extension of x(k). Recall 
from the discussion of circular convolution in Section 2.7 that the periodic exten-
sion of x(k) is denoted xp(k) and defined in terms of the MATLAB mod function as 
follows.
 
xp(k) 5
D  xf mod (k, N)g  
 (4.3.28)
Therefore, xp(k) 5 x(k) for 0 # k , N and xp(k) extends x(k) periodically in both positive 
and negative directions. Given that xp(k) is defined for all k, consider the replacement of 
k by 2 k. This is called time reversal or reflection. Using xp(k 1 N) 5 xp(k), Table 4.6, 
and a change of variable
 DFT hxp(2k)j 5 o
N21
k50
xp(2k)W ik
N
 5 o
N21
k50
xp(N 2 k)W ik
N, m 5 N 2 k
 5 o
1
m5N
xp(m)W i(N2m)
N
 
 5 o
N
m51
xp(m)W 2im
N
 
 (4.3.29)
Here we have made use of the identity W N
N 5 1. Next note that xp(m) 5 x(m) for 1 # m , N. 
Furthermore, xp(N) 5 x(0) and W iN
N 5 W i0
N. Thus for real x(k) the DFT can be rewritten as
 DFT hxp(2k)j 5 o
N21
m50
x(m)W 2im
N
 
5 X *(i) 
 (4.3.30)
Linearity property
Periodic extension
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3 Discrete Fourier Transform (DFT)     259
Consequently, for real signals, reversing time is equivalent to taking the complex conju-
gate of the DFT.
Circular Shift Property 
Another important property that makes use of the periodic extension xp(k) is the circular 
shift property. Suppose we shift xp(k) by r samples. Then, using a change of variables, we have
 DFT hxp(k 2 r)j  5 o
N21
k50
xp(k 2 r)W ik
N
 5 o
N212r
q52r
xp(q)W  i(q1r)
N
, q 5 k 2 r
 
 5 W ir
N o
N212r
q52r
xp(q)W  iq
N 
 (4.3.31)
Just as xp(q) is a periodic function of q with period N, from Table 4.6 the factor W iq
N is 
also a periodic function of q of period N. Consequently, the term xp(q)W iq
N in (4.3.26) 
is periodic with period N, which means that the summation over one period can start at 
q 5 0 rather than q 5 2r without affecting the result. Thus
 DFT hxp(k 2 r)j 5 W ir
No
N21
q50
xp(q)W  iq
N
 
5 W ir
NX(i) 
 (4.3.32)
The time shift by r samples is referred to as a circular shift because xp(k) is a periodic 
extension of x(k). One can think of the N samples of x(k) as being wrapped around the 
unit circle of the complex plane shown previously in Figure 4.6. The signal xp(k 2 r) then 
represents a counter-clockwise shift of x(k) by r samples.
Circular Convolution Property 
Recall from Definition 2.4 in Section 2.7 that the circular convolution of h(k) with x(k) is 
defined in terms of the periodic extension xp(k) as follows.
 
h(k) + x(k) 5 o
N21
i50
h(i)xp(k 2 i) 
 (4.3.33)
Just as convolution maps into multiplication under both the Z-transform and the 
DTFT, the same is true of the DFT in terms of circular convolution. Using the circular 
shift property and Definition 4.2
  DFT  hh(k) + x(k)j 5 DFT 3o
N21
m50
h(m)xp(k 2 m)4
 5 o
N21
m50
h(m) DFT hxp(k 2 i)j
 5 o
N21
m50
h(m)W im
N X(i)
 
 5 H(i)X(i)
 
 (4.3.34)
Time reversal property
Circular shift property
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

260    Chapter 4  Fourier Transforms and Spectral Analysis
Thus circular convolution maps into multiplication under the DFT. This is analogous 
to the linear convolution property of the Z-transform and the DTFT.
Circular Correlation Property 
Closely related to the operation of circular convolution is circular correlation. Recall 
from Definition 2.6 in Section 2.8 that the circular convolution of y(k) with x(k) is defined 
in terms of the periodic extension xp(k) as follows.
 
cyx(k) 5 1
No
N21
i50
y(i)xp(i 2 k) 
 (4.3.35)
One of the properties of circular cross-correlation from Table 2.4 in Chapter 2 was 
that circular cross-correlation can be computed using circular convolution as follows.
 
cyx(k) 5 y(k) + x(2k)
N
 
 (4.3.36)
Applying the circular convolution property and the time reversal property to real 
signals then leads to the circular correlation property.
 
 DFT hcyx(k)j 5 X(i)Y*(i)
N
 
 (4.3.37)
Parseval’s identity 
Parseval’s identity is a simple and elegant relationship between a time signal and its  
transform. There are several versions of Parseval’s identity depending on the independent 
variable (continuous or discrete) and the signal duration (finite or infinite). For example, 
the DTFT version of Parseval’s identity is given in Proposition 4.1. The DFT version of 
Parseval’s identity is very similar. Using (4.3.6)
o
N21
i50
X(i)Y*(i) 5 o
N21
i503o
N21
k50
x(k)W ki
No
N21
m50
y*(m)W 2mi
N 4
 5 o
N21
k50
 o
N21
m50
x(k)y*(m)o
N21
i50
W (k 2 m)i
N
 5 o
N21
k50
 o
N21
m50
x(k)y*(m)N(k 2 m)
 
 5 No
N21
k50
x(k)y*(k)
 
 (4.3.38)
The end result is the DFT version of Parseval’s identity.
Let x(k) and y(k) be two N-point time signals with discrete Fourier transforms X(i) 
and Y(i), respectively. Then
o
N21
k50
x(k)y* (k) 5 1
N o
N21
i50
X(i)Y* (i)
Circular convolution 
property
Circular correlation 
property
PRoPoSiTion
4.2 Parseval’s identity: 
DFT
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3 Discrete Fourier Transform (DFT)     261
Note that when y(k) 5 x(k) in Proposition 4.2 the left-hand side reduces to the energy of 
the signal x(k).
 
o
N21
k50
ux(k)u2 5 1
No
N21
i50
uX(i)u2  
 (4.3.39)
As an illustration of how we can apply Parseval’s identity, recall that the average power 
of an N-point signal is defined
 
Px 5
D  1
N o
N21
k50
ux(k)u2 
 (4.3.40)
Comparing (4.3.40) with (4.3.39), we see that Parseval’s identity provides us with an 
alternative frequency-domain version of average power. In particular, recalling that 
Sx(i) 5 uX(i)u2yN is the power density spectrum, it follows that
 
Px 5 1
No
N21
i50
Sx(i) 
 (4.3.41)
Thus the average power is just the average of the power density spectrum of x(k), hence 
the name power density spectrum.
Parseval’s identity
Power density 
spectrum
Parseval’s identity
EXAMPLE 4.7
To verify Parseval’s identity, consider the signal x(k) from Example 4.3.
x 5 f3, 21, 0, 2gT
In this case N 5 4. A direct time-domain computation of the average power using 
(4.3.40) yields
Px 5 1
No
N21
k50
ux(k)u2
 5 .25(9 1 1 1 0 1 4)
 5 3.5
From Example 4.3, the DFT of x(k) is
X 5 f4, 3 1 j3, 2, 3 2 j3gT
Thus from (4.3.23), the power density spectrum is
Sx 5 .25f16, 18, 4, 18gT
5 f4, 4.5, 1, 4.5gT
Finally, from (4.3.41) the frequency-domain method of determining the average  
power is
Px 5 1
No
N21
i50
Sx(i)
 5 .25(4 1 4.5 1 1 1 4.5)
= 3.5 ✓
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

262    Chapter 4  Fourier Transforms and Spectral Analysis
A summary of the properties of the DFT can be found in Table 4.8. The last prop-
erty, called the Wiener-Khintchine theorem, is derived in Section 4.7.
Fast Fourier Transform (FFT)
The discrete Fourier transform or DFT is an important computational tool that is widely 
used in DSP. As with any computational method, it can be rated in terms of how the 
computational effort grows as the size of the problem increases. Recall that the DFT of 
an N-point signal x(k) is
 
X(i) 5 o
N21
k50
x(k)W ik
N, 0 # i , N 
 (4.4.1)
The N 2 values of W ik
N do not depend on x(k), so they can be precomputed once and 
stored in an N 3 N matrix W. Each point X(i) then requires N complex multiplications. 
Since there are N values of i, the total number of complex floating point operations, or 
FLOPs, required to compute the entire DFT is
 
nDFT 5 N 2   FLOPs  
 (4.4.2)
Thus the computational effort, measured in complex multiplications, grows as the square 
of the size of the problem, N. In this case we say that the DFT computation is of order 
O(N 2). More generally, a computational algorithm is of order O(N p) if and only if for 
some constant c, the number of computations n satisfies
 
n < cN p, N W 1  
 (4.4.3)
4.4.1 Decimation in Time FFT 
The popularity of the DFT arises, in part, from the fact that there is an implementation of 
it that dramatically decreases the computational time, particularly for large values of N. 
To see how the improvement in speed is achieved, suppose that the number of points N is 
a power of two. That is, N 5 2r for some integer r where
 
r 5 log2(N) 
 (4.4.4)
4.4
Complex FLOP
Algorithm order
Property 
Time Signal 
DFT 
Comments 
Linearity 
ax(k) 1 by(k) 
aX(i) 1 bY(i) 
General 
Time Reversal 
xp(2k) 
X*(i) 
Real x 
Circular shift 
xp(k 2 r) 
W ir
N X(i) 
General 
Circular convolution 
x(k) + y(k) 
X(i)Y(i) 
General 
Circular correlation 
cyx(k) 
Y(i)X*(i)
N
 
Real x 
Wiener-Khintchine 
cxx(k) 
Sx(i) 
General 
o
N21
k50
x(k)y*(k)
1
N o
N21
i50
X(i)Y*(k)
two signals
Parseval
o
N21
 
k50
ux(k)u2
1
No
N21
i50
uX(i)u2 
one signal 
Table 4.8:  
DFT Properties 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.4 Fast Fourier Transform (FFT)    263
Next suppose we decimate the N-point signal x(k) into two Ny2-point signals xe(k) and 
xo(k) corresponding to the even and odd indices or subscripts of x, respectively.
 
xe 5
D  
fx(0), x(2), Á , x(N 2 2)gT 
 (4.4.5a)
 
xo 5
D  fx(1), x(3), Á , x(N 2 1)gT 
 (4.4.5b)
The DFT in (4.4.1) then can be recast as two sums, one corresponding to the even values 
of k, and the other corresponding to the odd values of k. Recalling from Table 4.6 that 
W 2k
N 5 W k
Ny2 we have
X(i) 5 o
Ny221
k50
x(2k)W 2ki
N 1 o
Ny221
k50
x(2k 1 1)W (2k11)i
N
 5 o
Ny221
k50
xe(k)W 2ki
N 1 W i
N o
Ny221
k50
xo(k)W 2ki
N
 5 o
Ny221
k50
xe(k)W ki
Ny2 1 W i
N o
Ny221
k50
xo(k)W ki
Ny2
 
 5 Xe(i) 1 W  i
NXo(i), 0 # i , N
 
 (4.4.6)
Here Xe(i) 5
 DFT hxe(k)j and Xo(i) 5
 DFT hxo(k)j are Ny2-point transforms of the even 
and odd parts of x(k), respectively. Thus (4.4.6) decomposes the original N-point prob-
lem into two Ny2-point subproblems with N complex multiplications and N complex 
additions required to merge the solutions. Each Ny2-point DFT requires N2y4 FLOPs. 
Hence for large values of N, the number of floating point multiplications required to 
implement the even-odd decomposition in (4.4.6) is
 
neo < N2
2    FLOPs , N W 1 
 (4.4.7)
Comparing (4.4.7) with (4.4.2), we see that the computational effort has been reduced 
by a factor of two for large values of N. It is helpful to break the merging formula in 
(4.4.6) into two cases where 0 # i , Ny2.
 
X(i) 5 Xe(i) 1 W i
NXo(i) 
 (4.4.8a)
 
X(i 1 Ny2) 5 Xe(i 1 Ny2) 1 W i1Ny2
N
Xo(i 1 Ny2) 
 (4.4.8b)
Since Xe(i) and Xo(i) are Ny2-point transforms, we know from the periodic property 
in Table 4.7 that Xe(i 1 Ny2) 5 Xe(i) and similarly for Xo(i). Furthermore, from Table 4.6 
we have W i1Ny2
N
5 2W i
N. Thus (4.4.8) can be simplified as follows where 0 # i , Ny2.
 
Y(i) 5 W i
NXo(i) 
 (4.4.9a)
 
X(i) 5 Xe(i) 1 Y(i) 
 (4.4.9b)
 
X(i 1 Ny2) 5 Xe(i) 2 Y(i) 
 (4.4.9c)
The merging formula is written as three equations using a temporary variable Y(i). 
This increases storage by one complex scalar, but it reduces the number of complex 
multiplications from two to one. The computations in (4.4.9) are referred to as an ith 
order butterfly. The name arises from the flow graph representation of (4.4.9) shown in  
Figure 4.9. With a little imagination, the “wings’’ of the butterfly are apparent.
The even-odd decomposition of the DFT in (4.4.9) consists of two Ny2-point DFTs 
plus Ny2 interleaved butterfly computations. A block diagram that illustrates the case 
N 5 8 is shown in Figure 4.10.
Time decimation
Computational 
butterfly
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

264    Chapter 4  Fourier Transforms and Spectral Analysis
The beauty of the even-odd decomposition technique in Figure 4.10 is that there is 
nothing to prevent us from applying it again! For example, the even samples xe(k) and the 
odd samples xo(k) can each be decimated into even and odd parts. In this way we can decom-
pose each Ny2-point transform into a pair of Ny4-point transforms. Since N 5 2r this pro-
cess can be continued a total of r times. In the end we are left with a collection of elementary 
two-point DFTs. Interestingly enough, the two-point DFT is simply a butterfly of order 
i 5 0. The resulting algorithm is called the decimation in time fast Fourier transform or FFT 
(Cooley and Tukey, 1965). The signal flow graph for the case N 5 8 is shown in Figure 4.11. 
Note how there are r iterations, and each iteration consists of Ny2 butterfly computations.
Observe from Figure 4.11 that the input to the first iteration has been scrambled due 
to repeated decimation into even and odd subsequences. As it turns out, there is a simple 
numerical relationship between the original DFT order and the scrambled FFT order. 
This becomes apparent when we look at the binary representations of the indices as 
shown in Table 4.9 for the case N 5 8. Note how the scrambled FFT indices are obtained 
by taking the normal DFT indices, converting to binary, reversing the bits, and then con-
verting back to decimal.
FFT
Figure 4.9: Signal 
Flow Graph of ith 
Order Butterfly 
Computation 
Xe(i)
X(i)
21
X(i 1 N/2)
Xo(i)
W i
N
Figure 4.10:  
Even-Odd  
Decomposition of 
DFT with N 5 8 
Odd
DFT
Even
DFT
x(0)
Xe(0)
Xe(1)
Xe(2)
Xe(3)
Xo(0)
Xo(1)
Xo(2)
Xo(3)
x(2)
x(4)
x(6)
x(1)
x(3)
x(5)
x(7)
X(0)
X(1)
X(2)
X(3)
X(4)
X(5)
X(6)
X(7)
8
W 0
8
W 1
8
W 2
8
W 3
21
21
21
21
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.4 Fast Fourier Transform (FFT)    265
The following MATLAB-based algorithm is designed to interchange the elements of 
the input vector x using the bit reversal process shown in Table 4.9.
1. b = dec2bin(x,N); 
% convert to binary string 
2. c = b(N:-1:1); 
% reverse bits 
3. x = bin2dec(c); 
% convert back to decimal value 
The MATLAB function dec2bin converts the decimal value x into a binary string of 
zeros and ones. Step 2 reverses the string elements, and then bin2dec in step 3 converts the 
bit-reversed string back to a decimal value. If Algorithm 4.1 is called twice, it restores the 
elements of x to the original order. That is, Algorithm 4.1 is its own inverse.
The overall decimation in time FFT is summarized in Algorithm 4.2. The input vector is 
scrambled in step 1 with a call to Algorithm 4.1. The rNy2 butterfly computations are per-
formed in step 2, and the final result is copied from x to X in step 3. To analyze step 2, it is 
ALgoRiTHM
4.1 Bit Reversal
DFT  
order 
Forward Binary 
Reverse Binary 
FFT  
order 
0 
000 
000 
0 
1 
001 
100 
4 
2 
010 
010 
2 
3 
011 
110 
6 
4 
100 
001 
1 
5 
101 
101 
5 
6 
110 
011 
3 
7 
111 
111 
7 
Table 4.9:  
Scrambled FFT Order 
Using Bit Reversal, 
N 5 8
Figure 4.11: Signal Flow Graph of Decimation in Time FFT with N 5 8 
Iteration 1
Iteration 2
Iteration 3
x(0)
x(4)
x(2)
x(6)
x(1)
x(5)
x(3)
x(7)
X(0)
X(1)
X(2)
X(3)
X(4)
X(5)
X(6)
X(7)
21
21
21
21
21
21
21
21
21
21
21
21
8
W 0
8
W 1
8
W 2
8
W 0
8
W 2
8
W 0
8
W 2
8
W 3
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

266    Chapter 4  Fourier Transforms and Spectral Analysis
helpful to refer to Figure 4.11. Step 2 consists of three loops. The outer loop goes through 
the r iterations, moving from left to right in Figure 4.11. Notice that for each iteration there 
are groups of butterflies. In step 2, s is the spacing between groups, g is the number of 
groups, and b is the number of butterflies per group. The parameter b is also the wingspan 
of the butterfly. The second loop goes through the g groups associated with the current iter-
ation, and the third and innermost loop goes through the b butterflies of each group. The 
first three equations in the innermost loop compute the weight w and the butterfly location 
n, while the last three equations compute the butterfly outputs as in (4.4.9).
1. Call Algorithm 4.1 to scramble the input vector x.
2. For i 5 1 to r do 
 
 
 
% iteration
{ 
(a) Compute 
 
s 5 2i 
% group spacing 
 
g 5 Nys  
% number of groups 
 
b 5 sy2 
% butterfliesygroup 
(b) For k 5 0 to g 2 1 do
 
{ 
 
For m 5 0 to b 2 1 do
 
{ 
  5 22mgyN
w 5  cos() 1 j sin()
n 5 ks 1 m
y 5 wx(n 1 b)
x(n) 5 x(n) 1 y
x(n 1 b) 5 x(n) 2 2y
 
} 
  } 
}
3. For k 5 0 to N 2 1 set X(k) 5 x(k). 
4.4.2 FFT Computational Effort 
Although the derivation of the FFT requires some attention to detail, the payoff is 
worthwhile because the end result is an algorithm that is dramatically faster than the 
DFT for practical values of N. To see how much faster, note from Figure 4.11 that there 
are r iterations and Ny2 butterflies per iteration. Given the weight W i
N, the butterfly 
computation in (4.4.9) requires one complex multiplication. Thus there are rNy2 complex 
multiplications required for the FFT. It then follows from (4.4.4) that the computational 
effort of the FFT is
 
nFFT 5 N log2(N )
2
   FLOPs  
 (4.4.10)
ALgoRiTHM
4.2 FFT
FFT speed
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.4 Fast Fourier Transform (FFT)    267
Thus the FFT algorithm is of order OfN log2(N )g while the DFT is of order O(N2). 
A graphical comparison of the number of FLOPs required by the DFT and the FFT is 
shown in Figure 4.12 for 1 # N # 256. Even over this modest range of values for N, the 
improvement in the speed of the FFT in comparison with the DFT is quite dramatic. For 
many practical problems, larger values of N in the range 1024 to 8192 are often used. For 
the case N 5 1024, the DFT requires 1.049 3 106 FLOPs, while the FFT requires only 
5.12 3 103 FLOPs. In this instance a speed improvement by a factor of 204.8, or more 
than two orders of magnitude, is achieved.
Given the highly efficient FFT, there are a number of DSP applications where the 
best way to solve a time domain problem is to use the following steps. 
1. Transform to the frequency domain using the FFT. 
2. Solve the problem in the frequency domain. 
3. Transform back to the time domain using the IFFT. 
Recall that the formulation for the inverse discrete Fourier transform or IDFT is very 
similar to that of the DFT, namely
 
x(k) 5 1
N o
N21
i50
X(i)W 2ki
N  
 (4.4.11)
One way to devise an algorithm for the IDFT is to modify Algorithm 4.2 by adding an 
input parameter that specifies which direction we want to transform, forward or reverse. 
Still another approach is to use Algorithm 4.2 itself, without modification. Recall from 
Table 4.6 that the complex conjugate of WN is just W *
N 5 W 21
N . This being the case, we 
can reformulate (4.4.11) in terms of the FFT as follows.
x(k) 5 1
N o
N21
i50
X(i)W 2ki
N
ALgoRiTHM
4.3 Problem domain
Figure 4.12:  
Computational 
Effort of DFT 
and FFT for 
1 # N # 256
0
50
100
150
200
250
300
0
10
20
30
40
50
60
70
N
FLOPs/1000
DFT
FFT
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

268    Chapter 4  Fourier Transforms and Spectral Analysis
 5 1
No
N21
i50
X(i)(W ki
N)*
 5 1
N1o
N21
i50
X * (i)W ki
N2
*
 
 51
1
N2
 FFT *hX *(i)j 
 (4.4.12)
Thus we can implement an IFFT of X(i) by taking the complex conjugate of the FFT 
of X *(i) and then normalizing the result by N. These steps are summarized in the follow-
ing algorithm. Note that this approach takes advantage of the fact that the FFT can be 
applied to either a real or a complex signal.
1. For k 5 0 to N 2 1 set x(k) 5 X *(k).
2. Call Algorithm 4.2 to compute X(i) 5
 FFT hx(k)j.
3. For k 5 0 to N 2 1 set x(k) 5 X *(k)yN. 
4.4.3 Alternative FFT implementations 
There is one drawback to the FFT, as implemented, that is evident from Figure 4.11. For 
the FFT the number of points N must be a power of two, while the DFT is defined for all 
N $ 1. Often this is not a serious limitation because the user may be at liberty to choose 
a value for N, say in collecting data samples for an experiment. In still other cases, it may 
be possible to pad the signal x(k) with a sufficient number of zeros to make N a power of 
two. Because N is a power of two, the formulation of the FFT in Algorithm 4.2 is called 
a radix-two version of the FFT. It is also possible to factor N in other ways and achieve 
alternative versions of the FFT (Ingle and Proakis, 2000). In general the alternative ver-
sions will have computational speeds that lie between the DFT and the radix-two FFT 
shown in Figure 4.11.
An alternative to the decimation in time method summarized in Algorithm 4.2 
is something called the decimation in frequency method (see, e.g., Schilling and  
Harris, 2000). The decimation in frequency method starts by decomposing x(k) into 
a first half 0 # k , Ny2, and a second half Ny2 # k , N. The merging formula for 
the decimation in frequency method is X(2i) 5
 DFT ha(k)j and X(2i 1 1) 5
 DFT hb(k)j 
where
 
a(k) 5 x(k) 1 x(k 1 Ny2) 
 (4.4.13a)
 
b(k) 5 fx(k) 2 x(k 1 Ny2)gW  k
N 
 (4.4.13b)
The process proceeds in a manner generally similar to the decimation in time 
approach and again leads to rNy2 butterfly computations. In this case it is the 
output vector X(i) that ends up being scrambled, hence the name decimation in 
frequency.
ALgoRiTHM 
4.4 iFFT
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.5 Fast Convolution and Correlation     269
MATLAB Functions
There are a number of MATLAB functions for computing the FFT and signal spec-
tra that are very simple to use.
% FFT: Compute a fast Fourier transform
%
% Usage:
% 
X = fft(x,N);
% Pre:
% 
x = vector of length M containing samples to be transformed
% 
N = optional number of samples to transform. If N > M, x is
% 
 
zero-padded with N-M samples. The spacing between
% 
 
discrete frequencies is Delta_F = f_s/N.
% Post:
% 
X = complex vector containing the DFT of x. 
There is also a function x 5 ifft(X) that is used to perform the inverse FFT as in 
Algorithm 4.4. Once the FFT is obtained, the magnitude, phase, and power density 
spectra can be easily obtained. Recall that the DSP Companion function f_spec also 
can be used.
X 
= fft(x,N); 
% Compute N-point FFT
A 
= abs(X); 
% Magnitude spectrum
phi = angle(X); 
% Phase spectrum
S 
= A.^2/N; 
% Power density spectrum 
MATLAB Functions
Fast Convolution and Correlation 
In this section we again focus on causal finite signals. The FFT provides us with a very 
efficient way to implement two important operations, convolution and correlation. The 
basic idea is to use Algorithm 4.3 to first transform the problem into the frequency 
domain with the FFT, then perform the operation in the frequency domain, and finally 
transform back to the time domain with the IFFT.
4.5.1 Fast Convolution 
Suppose h(k) is of length L and x(k) is of length M. From (2.7.6), the linear convolution 
of h(k) with x(k) is
 
h(k) w x(k) 5 o
L
i50
h(i)x(k 2 i), 0 # k , L 1 M 2 1 
 (4.5.1)
For computational purposes, the most important result from Chapter 2 was that the 
linear convolution of two finite signals can be implemented using circular convolution 
with zero padding.
 
h(k) w x(k) 5 hz(k) + xz(k) 
 (4.5.2)
4.5
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

270    Chapter 4  Fourier Transforms and Spectral Analysis
Here hz(k) is the zero-padded version of h(k) obtained by appending M 1 p zeros, and 
xz(k) is the zero-padded version of x(k) using L 1 p zeros where p $ 21. Thus the com-
mon length of hz(k) and xz(k) is N 5 L 1 M 1 p. Next, recall from from the properties 
of the DFT in Table 4.8 that
 
 DFT hhz(k) + xz(k)j 5 Hz(i)Xz(i) 
 (4.5.3)
That is, circular convolution in the time domain maps into multiplication in the frequency 
domain using the DFT. Consequently, an effective way to perform circular convolution is
 
hz(k) + xz(k) 5
 IDFT hHz(i)Xz(i)j, 0 # k , N 
 (4.5.4)
In view of (4.5.4) we now have in place all of the tools needed to perform a practical, 
highly efficient, linear convolution of two finite signals. All that is needed is to make the 
signal length N 5 L 1 M 1 p be a power of two. Since (4.5.2) holds for any p $ 2 1, 
consider the following value for N.
 
N 5
 nextpow2 (L 1 M 2 1)  
 (4.5.5)
Here the MATLAB function nextpow2 finds the smallest integer power of two that is 
greater than or equal to its calling argument. For this value of N, the highly efficient radix- 
two FFT can be used to compute the DFTs of hz(k) and xz(k) and the IDFT of Hz(i)Xz(i). 
This results in the following version of linear convolution called fast convolution.
 
h(k) w x(k) 5
 IFFT hHz(i)Xz(i)j, 0 # k , L 1 M 2 1  
 (4.5.6)
A block diagram of the fast convolution operation is shown in Figure 4.13. Even 
though fast convolution involves several steps, there is a value for N beyond which fast 
convolution is more efficient than the direct computation of linear convolution.
Computational Effort 
To simplify the analysis of the computational effort, suppose the signals h(k) and 
x(k) are both of length L where L is a power of two. In this case zero padding to 
Zero padding
Fast convolution
Figure 4.13: Fast Linear Convolution 
Zero
padding
Zero
padding
FFT
FFT
3
IFFT
h(k)
x(k)
y(k)
Hz(i)
hz(k)
xz(k)
Xz(i)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.5 Fast Convolution and Correlation     271
length N 5 2L is sufficient. From (4.4.10) the two FFTs in Figure 4.13 each require 
(Ny2) log2(N) complex floating point operations, or FLOPs, while the inverse FFT 
requires (Ny2) log2(N) 1 1 FLOPs. The multiplication of Hz(i) times Xz(i) for 0 # i , N 
requires an additional N FLOPs. Hence the total number of complex FLOPs is 
(3Ny2) log2(N) 1 N 1 1. The direct computation of linear convolution in (4.5.1) does 
not involve any complex arithmetic, so to provide a fair comparison we should count 
the number of real multiplications. The product of two complex numbers can be 
expressed as follows.
 
(a 1 jb)(c 1 jd) 5 ac 2 bd 1 j(ad 1 bc) 
 (4.5.7)
Consequently each complex multiplication requires four real multiplications. Recalling 
that N 5 2L, the total number of real FLOPs required to perform a fast linear convolu-
tion of two L-point signals is then
 
nfast 5 12L log2(2L) 1 8L 1 4   FLOPs  
 (4.5.8)
Next consider the number of real multiplications required to implement linear convo-
lution directly. Setting M 5 L in (4.5.1), the number of real multiplications or FLOPs is
 
ndir 5 2L2   FLOPs  
 (4.5.9)
Comparing (4.5.9) with (4.5.8), we see that for small values of L a direct computation of 
linear convolution will be faster. However, the L2 term grows faster than the L log2(2L) 
term, so eventually the fast convolution will outperform direct convolution. A plot of 
the number of real FLOPs required by the two methods for signal lengths in the range 
2 # L # 1024 is shown in Figure 4.14. The two methods require roughly the same number 
of FLOPs for L # 32. However, fast linear convolution is superior to direct linear convo-
lution for signal lengths in the range L $ 64, and as L increases it becomes significantly 
faster.
Real FLOPs
Figure 4.14:  
Comparison of 
Computational 
Effort Required for 
Linear Convolution 
of Two L-point 
Signals 
0
200
400
600
800
1000
1200
0
500
1000
1500
2000
2500
L
FLOPs/1000
Direct
Fast
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

272    Chapter 4  Fourier Transforms and Spectral Analysis
Figure 4.15: Zero-state Response of x(k) Using Fast Linear Convolution
0
200
400
600
800
1000
1200
26
24
22
0
2
4
6 x 104
k
Signals
Input
Zero-state output
Fast Convolution
EXAMPLE 4.8
To illustrate the use of fast convolution, consider a linear discrete-time system 
with the following transfer function.
H(z) 5
.98 sin(y24)z
z2 2 1.96 cos(y24)z 1 .9604
Using the Z-transform pairs in Table 3.2, the impulse response of this system is
h(k) 5 .98k sin(ky24)(k)
Next suppose this system is driven with the following exponentially damped sinu-
soidal input.
x(k) 5 k2(.99)k cos(ky48)(k)
Since both h(k) and x(k) decay to zero, we can approximate them as L-point sig-
nals when L is sufficiently large. The zero-state response of the system can be 
obtained by running exam4_8. It computes the linear convolution of h(k) with 
x(k) using fast convolution with L 5 512. Plots of the input x(k) and zero-state 
output y(k) are shown in Figure 4.15. In this case the total number of real FLOPs 
was nfast 5 4.67 3 104. This is in contrast to the direct method, which would 
require ndir 5 5.24 3 105 FLOPs. This corresponds to a savings of 91.1%.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.5 Fast Convolution and Correlation     273
*4.5.2 Fast Block Convolution 
The efficient formulation of convolution based on the FFT assumes that the two sig-
nals are of finite duration. There are some applications where the input x(k) is available 
continuously and is of indefinite duration. For example, the input might represent a 
long speech signal obtained from a microphone. In cases like these, the number of input 
samples M will be very large, and computation of an FFT of length N 5 L 1 M 1 p 
may not be practical. Another potential drawback of batch processing is that none 
of the samples of the filtered output are available until the entire N points have been 
processed.
These difficulties associated with very long inputs can be addressed by using a tech-
nique known as block convolution. Suppose the impulse response consists of L samples 
and the input contains M samples where M W L. The basic idea is to break up the input 
signal into blocks or sections of length L. Each of these blocks is convolved with the  
L-point impulse response h(k). If the results are combined in the proper way, the original 
(L 1 M 2 1)-point convolution can be recovered.
To see how this is achieved, first note that the x(k) can be padded with up to L 2 1 
zeros, if needed, such that the length of the zero-padded input, xz(k), is QL for some 
integer Q . 1. The zero-padded input then can be expressed as a sum of Q blocks of 
length L as follows.
 
xz(k) 5 o
Q21
i50
xi(k 2 iL), 0 # k , M 
 (4.5.10)
Here the Q blocks, or subsignals of length L, are extracted from the original signal x(k) 
using a window of length L as follows.
 
xi(k) 5
D  5
x(k 1 iL),
0 # k , L
0
           
,  
              
otherwise
 
 (4.5.11)
Subsignal xi(k) is the segment of x(k) starting at k 5 Li, but it has been shifted so that it 
starts at k 5 0. Using (4.5.10) and (4.5.1), the linear convolution of h(k) with x(k) is then
 h(k) w x(k) 5 o
Q21
i50
h(k) w xi(k 2 iL)
 
5 o
Q21
i50
yi(k 2 iL) 
 (4.5.12)
Here yi(k) is the convolution of h(k) with the ith subsignal xi(k). That is,
 
yi(k) 5 h(k) w xi(i), 0 # i , Q 
 (4.5.13)
The block convolutions in (4.5.13) are between two L-point signals. If these signals are 
padded with L 1 p zeros where p $ 21 and 2L 1 p is a power of two, then an FFT 
can be used. The results must then be shifted and added as in (4.5.12). The resulting 
procedure, known as the overlap-add method of block convolution, is summarized in the 
following algorithm.
optional material
Overlap-add method
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

274    Chapter 4  Fourier Transforms and Spectral Analysis
1. Compute 
Msave 5 M
r 5 L 2 mod(M, L)
M 5 M 1 r
xz 5 fx(0), Á , x(Msave 2 1), 0, Á , 0gT [ RM
Q 5 M
L
N 5 2ceilflog2(2L21)g
hz 5 fh(0), Á , h(L 2 1), 0, Á , 0gT [ RN
Hz 5 FFThhz(k)j
y0 5 f0, Á , 0gT [ RL(Q21) 1 N
2. For i 5 0 to Q 2 1 compute 
xi(k) 5 xz(k 1 iL), 0 # k , L
xiz(k) 5 fxi(0), Á , xi(L 2 1), 0, Á , 0gT [ RN
Xiz(i) 5 FFThxiz(k)j
yi(k) 5 IFFThHz(i)Xiz(i)j
y0(k) 5 y0(k) 1 yi(k 2 Li), Li # k , Li 1 2N 2 1
3. Set 
y(k) 5 y0(k), 0 # k , L 1 Msave 2 1
In step 1 of Algorithm 4.5., the original number of input samples is saved in Msave. 
If mod(M, L) . 0, then M is not an integer multiple of L. In this case, r zeros are padded 
to the end of x so that the length of xz(k) is M 5 QL where Q is an integer representing 
the number of blocks. Next N is computed so that the zero-padded length of h satisfies 
N $ 2L 2 1 and is a power of two. Since Hz only has to be computed once, it is also 
computed in step 1. The Q block convolutions are then performed in step 2, and the 
results are overlapped and added using y0 for storage. Finally, the relevant part of y0(k) 
is extracted in step 3. There is a closely related alternative to the overlap-add method in 
Algorithm 4.5 called the overlap-save method (see, e.g., Oppenheim et al., 1999).
ALgoRiTHM
4.5 Fast Block Convolution
Fast Block Convolution
EXAMPLE 4.9
As an illustration of the fast block convolution technique, suppose the impulse 
response is
h(k) 5 .8k sin 1
k
4 2, 0 # k , 12
Thus L 5 12. Next, let the input consist of white noise uniformly distributed over 
f21, 1g.
x(k) 5 v(k), 0 # k , 70
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.5 Fast Convolution and Correlation     275
Figure 4.16: Block Convolution of h(k) with x(k) where L 5 12, M 5 32, Q 5 3,  
and N 5 32
0
10
20
30
40
50
60
70
80
22
0
2
k
y(k)
0
10
20
30
40
50
60
70
80
21
0
1
k
h(k)
0
10
20
30
40
50
60
70
80
22
0
2
k
x(k)
In this case Msave 5 70. The number of samples in the zero-padded version of x is
M 5 M 1 fL 2 mod(M, L)g
5 70 1 12 2 mod(70, 12)
5 82 2 10
5 72
Thus Q 5 72y12 and there are exactly Q 5 6 blocks of length 12 in xz(k). Since 
both h and xi are of length L, the minimum length of the zero-padded versions of 
h and xi will be L 1 L 1 p for p $ 21. We can choose N 5 2L 1 p to be a power 
of two as follows.
q 5
 nextpow2 (2L 2 1)
5 32
Plots of the impulse response h(k), the zero-padded input xz(k), and the out-
put y(k) are shown in Figure 4.16. These we generated by running exam4_9. The 
zero-padded version of the input is sectioned into Q blocks, each of length L. 
Script exam4_9 also computes the convolution in the direct manner. Both out-
puts are plotted in Figure 4.16, where it can be seen that they are identical. For 
this example, a modest value for M was used so that the results are easier to 
visualize.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

276    Chapter 4  Fourier Transforms and Spectral Analysis
4.5.3 Fast Correlation 
Practical cross-correlations often involve long signals, so it is useful to develop a 
numerical implementation of linear cross-correlation that is more efficient than 
the direct method. Recall from Definition 2.5 that if y(k) is a signal of length L and 
x(k) is a signal of length M # L, then the linear cross-correlation of y(k) with  
x(k) is
 
ryx(k) 5 1
Lo
L21
i50
y(i)x(i 2 k), 0 # k , L 
 (4.5.14)
Just as was the case with convolution, linear cross-correlation can be achieved using 
circular cross-correlation with zero padding. In particular from Table 2.4
 
ryx(k) 51
N
L2cyzxz(k) 
 (4.5.15)
Here yz(k) is a zero-padded version y(k) with M 1 p zeros appended. Similarly, xz(k) is 
a zero-padded version of x(k) with L 1 p zeros where p $ 21. Therefore xz and yz are 
The DSP Companion contains the following functions for performing fast convolu-
tions and fast block convolutions.
% F_CONV: 
Fast linear or circular convolution
% F_BLOCKCONV: Fast linear block convolution
%
% Usage:
% 
y 
= f_conv (h,x,circ);
% 
y 
= f_blockconv (h,x);
% Pre:
% 
h 
= vector of length L containing pulse
% 
 
 response signal
% 
x 
= vector of length M containing input signal
% 
circ = optional convolution type code (default: 0)
%
% 
 
 0 = linear convolution
% 
 
 1 = circular convolution (requires M = L)
% Post:
% 
y = vector of length L+M-1 containing the
% 
 
convolution of h with x. If circ = 1,
% 
 
y is of length L.
% Note:
% 
If h is the impulse response of a discrete-time
% 
linear system and x is the input, then y is the
% 
zero-state response when circ = 0.
DSP Companion
DSP Companion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.5 Fast Convolution and Correlation     277
both signals of length N 5 L 1 M 1 p. Next recall from the properties of the DFT in 
Table 4.8 that
 
 DFT hcyzxz(k)j 5 Yz(i)X*z(i)
N
 
 (4.5.16)
Therefore, an effective way to perform circular cross-correlation is
 
cyzxz(k) 5
 IDFT hYz(i)X *z(i)j
N
 
 (4.5.17)
To convert (4.5.17) to a more efficient implementation based on the FFT, we need to 
make the signal length N 5 L 1 M 1 p be a power of two. Since (4.5.15) holds for any 
p $21, consider the following value for N.
 
N 5
 nextpow2 (L 1 M 2 1) 
 (4.5.18)
For this value of N a radix-two FFT can be used in place of the DFT. Using (4.5.15) and 
(4.5.17) we then arrive at the following highly efficient version of linear cross-correlation 
called fast cross-correlation.
 
ryx(k) 5 IFFThYz(i)X*
z(i)j
L
, 0 # k , L  
 (4.5.19)
Note the strong similarity between fast convolution in (4.5.6) and the fast correlation 
in (4.5.19). The only differences are that Yz(i) is replaced by its complex conjugate, Y *z(i), 
the final result is scaled by 1yL, and the result is evaluated only for 0 # k , L. A block 
diagram of the fast correlation operation is shown in Figure 4.17. Just as with fast convo-
lution, there is a value for L beyond which fast correlation is more efficient than the direct 
computation of cross-correlation using (4.5.14).
Fast cross-correlation
Figure 4.17: Fast Linear Cross-correlation 
Zero
padding
Zero
padding
FFT
FFT*
×
y(k)
x(k)
r{yz}(k)
Xz*(i)
Yz(i)
xz(k)
yz(k)
IFFT
L
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

278    Chapter 4  Fourier Transforms and Spectral Analysis
0
500
1000
1500
2000
2500
0
500
1000
1500
2000
2500
L
FLOPs/1000
Direct
Fast
Figure 4.18:  
Comparison of 
Computational 
Effort for Linear 
Cross-correlation 
of Two L-Point 
Signals
Computational Effort 
The analysis of the computational effort for fast cross-correlation is similar to 
that for fast convolution. For simplicity, suppose the signals x(k) and y(k) are 
both of length L where L is a power of two. Then from (4.5.18) zero padding to 
length N 5 2L is sufficient. Proceeding as was done with convolution, the number 
of real FLOPs required to perform a fast linear cross-correlation of two L-point 
signals is
 
nfast 5 12L log2(2L) 1 8L 1 6   FLOPs  
 (4.5.20)
Next consider the number of real multiplications required to implement linear 
cross-correlation directly. A direct application of (4.5.14) yields L2 1 L multiplica-
tions. However, the lower limit on the sum in (4.5.14) can be replaced by i 5 k because 
x(k) is causal. This reduces the number of real multiplications by approximately a 
factor of two.
 
ndir 5 L2
2 1 L   FLOPs  
 (4.5.21)
Comparing (4.5.21) with (4.5.20) we again see that for small values of L, a direct compu-
tation of linear cross-correlation will be faster. However, the L2 term grows faster than 
the L log2(2L) term, so eventually fast cross-correlation will outperform direct cross- 
correlation. A plot of the number of real FLOPs required by the two methods for signal 
lengths in the range 2 # L # 2048 is shown in Figure 4.18. The two methods require 
roughly the same number of FLOPs for L 5 256. However, fast linear cross-correlation 
is superior to direct linear cross-correlation for signal lengths in the range L $ 512, and 
as L increases it becomes significantly faster.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.5 Fast Convolution and Correlation     279
0
200
400
600
800
1000
1200
21.5
21
20.5
0
0.5
1
1.5
2
2.5
k
Signals
y
x 1 2
Figure 4.19: A Pair of Discrete-time Signals
Fast Linear Correlation
EXAMPLE 4.10
To illustrate the use of fast correlation, let L 5 1024 and M 5 512, and consider 
the following pair of signals where v(k) is white noise uniformly distributed over 
the interval f21, 1g.
x(k) 5 3k
M exp1
24k
M 2 sin1
5k2
M 2, 0 # k , M
y(k) 5 xz(k 2 p) 1 v(k), 0 # k , L
We refer to x(k) as a multi-frequency chirp signal because it contains a range of 
frequencies due to the k2 factor in the sin. Here xz(k) denotes the zero-padded 
extension of x(k). Thus xz(k 2 p) is just x(k) shifted to the right by p samples. For 
this example p 5 279. A plot of these two signals, obtained by running exam4_10, 
is shown in Figure 4.19. Also computed is the normalized linear cross-correla-
tion of y(k) with x(k) as shown in Figure 4.20. Observe that the peak correlation 
occurs at yx(279) 5 .173, as expected. This indicates that cross-correlation has 
succeeded in detecting and identifying the location of the chirp x(k) within y(k). 
The number of real FLOPs required by fast cross-correlation in this case was 
nfast 5 1.02 3 105. This is in contrast to the direct computation using (4.5.14), 
which requires ndir 5 5.25 3 105 real FLOPs. This corresponds to a savings  
of 80.6%.
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

280    Chapter 4  Fourier Transforms and Spectral Analysis
If the MATLAB signal processing toolbox is available, there is a function 
called xcorr for performing cross-correlations. It computes ryx(k) for both positive 
an negative lags, k, and it also provides for different types of normalization.
The DSP Companion contains the following function for computing fast linear and 
circular cross-correlations.
% F_CORR: Fast cross-correlation of two discrete-time signals
%
% Usage:
% 
r = f_corr (y,x,circ,norm)
% Pre:
% 
y 
= vector of length L containing first signal
% 
x 
= vector of length M <= L containing second signal
% 
circ = optional correlation type code (default 0):
%
% 
 
0 = linear correlation
% 
 
1 = circular correlation
%
% 
norm = optional normalization code (default 0):
%
% 
 
0 = no normalization
% 
 
1 = normalized cross-correlation
% Post:
% 
r = vector of length L contained selected cross-
% 
 
correlation of y with x.
% Notes:
% 
To compute auto-correlation use x = y. 
DSP Companion
DSP Companion
Figure 4.20: Normalized Cross-correlation of the Signals in Figure 4.19 
0
200
400
600
800
1000
1200
20.1
20.05
0
0.05
0.1
0.15
0.2
0.25
k
yx(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.6 White Noise     281
White noise 
In this section we examine the spectral characteristics of an important type of random signal 
called white noise. White noise is more formally referred to as a random process (Grimmett, 
2001). It is useful because it provides an effective way to model physical signals, signals that 
are typically corrupted with noise. For example, the quantization error associated with an 
analog-to-digital converter (ADC) can be modeled with white noise. Another important 
application area is in the identification of linear discrete-time systems. White noise input 
signals are particularly suitable for system identification because they contain power at all 
frequencies and therefore excite all of the natural modes of the system under investigation.
4.6.1 Uniform White noise 
Let x be a random variable that takes on values in the interval fa, bg. If each value of 
x is equally likely to occur, then we say that x is uniformly distributed over the interval 
fa, bg. A uniformly distributed random variable can be characterized by the following 
probability density function.
 
p(x) 55
1
b 2 a , a # x # b
0
         
,
           otherwise 
 
 (4.6.1)
A graph of the uniform probability density function is shown in Figure 4.21 for the case 
fa, bg 5 f23, 7g. Note that the area under a probability density curve is always one. For 
any probability density function, the probability that a random variable x lies in an inter-
val fc, dg can be computed as
 
Pfc, d g 5#
d
c
p(x)dx 
 (4.6.2)
4.6
Uniform probability 
density
25
0
5
10
0
0.05
0.1
0.15
x
p(x)
Figure 4.21:  
Probability 
Density Function 
Corresponding 
to a Uniform 
Distribution over 
fa, bg where 
a 5 23 and b 5 7 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

282    Chapter 4  Fourier Transforms and Spectral Analysis
Random variables are characterized by their statistical properties. To define a variety 
of statistical characteristics, the following operator is useful.
Let x be a random variable with probability density p(x). The expected value of f(x) is 
denoted Eff(x)g and defined
E f f (x)g 5
D  #
`
2`
f(x)p(x)dx
Note that for the case of uniform weighting, the expected value of f(x) can be inter-
preted as the average value of f(x) over the interval fa, bg. More generally, the expected 
value represents a weighted average with weighting p(x). Given the expected value, the  
ith moment of x is defined as E fxi g for i $ 1. Thus the ith moment is just the expected 
value of the polynomial xi. The most fundamental moment is the first moment which is 
called the mean of x.
 
 5
D  
E fxg  
 (4.6.3)
The mean  is the average value about which a random variable is distributed. For ran-
dom variables uniformly distributed over the interval fa, bg, a direct application of Defi-
nition 4.3 reveals that the mean is  5 (a 1 b)y2.
Once the mean is determined, a second set of moments called the central moments 
can be computed. The ith central moment is defined as E f(x 2 )ig. The central moments 
specify the distribution of x about the mean. The first nonzero central moment is the 
second central moment which is called the variance of x.
 
 2 5
D  
E f(x 2 )2g  
 (4.6.4)
The variance  2 is a measure of the spread of the random variable about the mean. The 
square root of the variance, , is called the standard deviation of x.
DEFiniTion 
4.3 Expected Value
ith Moment
Mean
ith Central moment
Variance
MATLAB, like most programming languages, provides a facility for generating uni-
formly distributed random numbers. To create an array of N random numbers v 
uniformly distributed over fa, bg, the following code fragment can be used.
v = a + (b-a)*rand(N,1); 
% Uniform random numbers
MATLAB Functions
MATLAB Functions
In general, rand(N, M) returns an N 3 M matrix of random numbers uniformly dis-
tributed over the interval f0, 1g. The offset by a and scaling by b 2 a, then maps f0, 1g 
into fa, bg.
The signal v is referred to as uniform white noise. More specifically, it is white noise 
that is uniformly distributed over the interval fa, bg. The reason for the term white arises 
from the fact that, just as white light contains all colors, a white noise signal contains 
power at all frequencies.
The average power of a random variable x is the second moment Efx2g. For the 
uniformly distributed random variable, v, the average power can be computed using 
Uniform white noise
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.6 White Noise     283
Definition 4.3 and the probability density function in (4.6.1) as follows.
Pv 5 E fv2g
5#
`
2`
v2p(v)dv
5
1
b 2 a#
b
a
v2dv
 
5
x3
3(b 2 a)u
b
a 
 (4.6.5)
Consequently, the average power of white noise uniformly distributed over the interval fa, bg is
 
Pv 5 b3 2 a3
3(b 2 a)  
 (4.6.6)
For the special case fa, bg 5 f2c, cg, this reduces to Pv 5 c2y3. These results are sum-
marized in Appendix 2 for convenient reference. Recall from (4.3.18) that the power den-
sity spectrum, Sx(i) 5 uX(i)u2yN, specifies the amount of power at frequency fi 5 ifsyN. 
For a white noise signal, the power density spectrum is flat as can be seen from the fol-
lowing numerical example.
Average power
Uniform White noise
EXAMPLE 4.11
Suppose a 5 25, b 5 5, and N 5 512. Then from (4.6.6) the average power of the 
white noise signal v(k) is
Pv 5 53 2 (25)3
3(5 2 (25)
5 250
30
5 8.333
Plots of a uniform white noise signal and its power density spectrum can be 
obtained by running exam4_11. The time signal v(k) for 0 # k , N is shown in 
Figure 4.22, and its power density spectrum Sv(i) for 0 # i , Ny2 is shown in 
Figure 4.23. Note how the power density spectrum is, at least roughly, flat and 
nonzero indicating that there is power at all Ny2 discrete frequencies. The uneven 
nature of the computed power density spectrum in Figure 4.22 is addressed later 
in the chapter where techniques for estimating a smoother power density spec-
trum are introduced.
The horizontal line in Figure 4.23 specifies the theoretical average power Pv 
from (4.6.6). The actual average power of v(k) can be computed directly for 
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

284    Chapter 4  Fourier Transforms and Spectral Analysis
0
100
50
200 250
150
300 350 400 450 500
210
28
26
24
22
0
2
4
6
8
10
k
v(k)
Figure 4.22: White Noise Uniformly Distributed over f25, 5g with N 5 512 
0
50
100
150
200
250
0
10
20
30
40
50
60
70
80
Pv 5 8.333
i
Sv(i)
Figure 4.23: Power Density Spectrum of Uniformly Distributed White Noise 
comparison. Applying the time-domain formula in (4.3.40) yields
Pv 5  1
No
N21
i50
v2(k)
5 8.781
As the signal length N increases, the difference between the actual average power, 
Pv, and the theoretical average power, Pv, decreases.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.6 White Noise     285
4.6.2 gaussian White noise 
Uniformly distributed white noise is appropriate in those instances where the value of the 
signal is constrained to lie within a fixed interval. For example, the quantization noise of an 
n-bit bipolar ADC with reference voltage Vr and rounding lies in the interval f2qy2, qy2g 
where q 5 Vry2 n21. density However, there are other cases where it is more natural to model 
the noise using a normal or Gaussian probability density function as follows.
 
p(x) 5
1
Ï2
 exp 3
2(x 2 )2
22 4  
 (4.6.7)
Here the two parameters are the mean  and the standard deviation . The mean specifies 
where the random values are centered, while the standard deviation specifies the spread 
of the random values about the mean. A plot of the bell-shaped Gaussian probability 
density function is shown in Figure 4.24 for the case where  5 0 and  5 1.
Gaussian probability 
density
In general, randn(N, M) returns an N 3 M matrix of random numbers with a normal or 
Gaussian distribution with a mean of zero and a standard deviation of one. The offset 
by mu and scaling by sigma then maps this into a distribution with the desired mean and 
standard deviation.
Random numbers with a normal or Gaussian distribution can be generated using the 
MATLAB function randn. To create an array of N random numbers v with a Gaussian 
distribution of mean mu and standard deviation sigma the following code fragment 
can be used.
v = mu + sigma*randn(N,1); 
% Gaussian random numbers 
MATLAB Functions
MATLAB Functions
Figure 4.24:  
Gaussian 
Probability Density  
Function with 
 5 0 and  5 1
24
23
22
21
0
1
2
3
4
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
x
p(x)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

286    Chapter 4  Fourier Transforms and Spectral Analysis
The random signal v is referred to as Gaussian white noise with mean  and standard 
deviation . Again, it is called white noise because v(k) contains power at all frequencies. 
Later, we will see how to filter it to produce colored noise.
For a Gaussian random variable v, the average power can be computed as E fv2g using 
the probability density function in (4.6.7). To simplify the final result, note that Gaussian 
noise with mean  and standard deviation  can be represented as v(k) 5  1 w(k) where 
w(k) is zero-mean noise with standard deviation . Thus
Pv 5 E fv2g
5 E f( 1 w)2g
5 E f2 1 2w 1 w2g
5 2 1 2E fwg 1 E fw2g
 
5 2 1 2E fwg 1  2 
 (4.6.8)
Since w(k) has zero mean, the average power of Gaussian white noise with mean  and 
standard deviation  is
 
Pv 5 2 1 
 2  
 (4.6.9)
The decomposition of average power in (4.6.9) applies to uniform white noise as well (see 
Problem 4.31). Recall that the average power of noise uniformly distributed over fa, bg 
is Pv 5 (b3 2 a3)yf3(b 2 a)g. Using (4.6.9), the average power of noise uniformly distrib-
uted over fa, bg also can be expressed as
 
pv 5 a 1 b
2
1 (b 2 a)2
12
 
 (4.6.10)
Gaussian white noise
Average power
gaussian White noise
EXAMPLE 4.12
Consider the following continuous-time periodic signal produced by an AM 
mixer with frequencies F1 and F2.
xa(t) 5  sin(2F1t) cos(2F2t)
From the trigonometric identities in Appendix 2, the product of two sinusoids 
produces sum and difference frequencies.
xa(t) 5
 sinf2(F1 1 F2)tg 1  sinf2(F1 2 F2)tg
2
Suppose F1 5 300 Hz, F2 5 100 Hz, and xa(t) is corrupted with Gaussian noise, 
v(k), with zero mean and standard deviation  5 .8. If the result is then sampled 
at fs 5 1 kHz using N 5 1024 samples, the corresponding discrete-time signal is
x(k) 5  sin(.3k) cos(.1k) 1 v(k), 0 # k , N
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.6 White Noise     287
From (4.6.9), the average power of the noise term is
Pv 5 .64
The signal x(k) and its power density spectrum Sx(i) can be obtained by run-
ning exam4_12. The first quarter of the noise-corrupted time-signal is shown 
in Figure 4.25, and its power density spectrum is shown in Figure 4.26. Note 
0
50
100
150
200
250
25
24
23
22
21
0
1
2
3
4
5
k
x(k)
Figure 4.25: Periodic Signal Corrupted with Zero-mean Gaussian White Noise 
0
100 150
50
200 250 300 350 400 450 500
0
10
20
30
40
50
60
70
80
f (Hz)
Sx(f)
Figure 4.26: Power Density Spectrum of Noise-corrupted Periodic Signal 
(Continued   )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

288    Chapter 4  Fourier Transforms and Spectral Analysis
As a convenience to the user, the DSP Companion contains the following functions 
for generating uniform and Gaussian white noise signals.
% F_RANDINIT: Initialize the random number generator
% F_RANDU: 
Generate uniform random noise
% F_RANDG: 
Generate Gaussian random noise
%
% Usage:
% 
 
 f_randinit (seed)
%  
A 
= f_randu (m,n,a,b);
% 
A 
= f_randg (m,n,mu,sigma);
% Pre:
% 
seed = nonnegative integer. Each seed produces a
% 
 
 different pseudo-random sequence.
% 
m 
= number of rows
% 
n 
= number of columns
% 
a 
= lower limit for the uniform distribution
% 
b 
= upper limit for the uniform distribution
% 
mu 
= mean of the Gaussian distribution
% 
 sigma = standard deviation of the Gaussian
% 
 
 distribution
% Post:
% 
A = m by n matrix of random numbers
DSP Companion
DSP Companion
The function f_randinit uses the MATLAB object RandStream to control the 
sequence of random numbers generated by successive calls to f_randu and f_randg. 
Each distinct seed produces a different pseudo-random sequence. In this way the 
same random sequence can be reproduced, if needed. If it is important to produce a 
different random sequence each time a program is run, then seed can be replaced by 
sum(100*clock).
how it is difficult to tell from the time plot in Figure 4.25 that x(k) contains a 
periodic component given the presence of the noise. However, the two spec-
tral components at the sum frequency F1 1 F2 5 400 Hz and the difference fre-
quency F1 2 F2 5 200 Hz are evident from the power density spectrum plot in 
Figure 4.26 where two distinct spikes are present. The Gaussian white noise 
also is apparent in Figure 4.26 as low level power that is distributed over all 
frequencies.
The power density spectrum plot in Figure 4.26 uses the independent variable 
f 5 ifsyN, rather than i, in order to facilitate interpretation of the frequency. Thus 
it is a plot of Sx( f ) versus f  rather than Sx(i) versus i.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.7 Auto-correlation     289
Auto-correlation 
In this section we use correlation techniques to process noise-corrupted signals. Recall 
that the cross-correlation of a signal with itself is called auto-correlation. Consider the 
case of circular auto-correlation.
Let x(k) be an N-point signal and let xp(k) be its periodic extension. The circular 
auto-correlation of x(k) is denoted cxx(k) and defined
cxx(k) 5
D  1
No
N21
i5k
x(i)xp(i 2 k), 0 # k , N
As the notation suggests, auto-correlation is a special case of cross-correlation with y 5 x. 
Since xp(0) 5 x(0), it follows from Definition 4.4 that circular auto-correlation at a lag 
of k 5 0 is simply the average power of x(k). Thus the average power can be expressed in 
terms of circular auto-correlation as
 
Px 5 cxx(0) 
 (4.7.1)
Auto-correlation can be normalized just as cross-correlation. Using (4.7.1), the normal-
ized circular auto-correlation, denoted xx(k), is
 
xx(k) 5 cxx(k)
Px
, 0 # k , N 
 (4.7.2)
In view of (4.7.1), it follows that xx(0) 5 1. Since uxx(k)u # 1, this means that the nor-
malized circular auto-correlation always reaches its peak value of one at a lag of k 5 0.
4.7.1 Auto-correlation of White noise 
White noise has a particularly simple auto-correlation. To see this let v(k) be a random 
white noise signal of length N with a mean of  5 0. Recall that the mean of a random 
signal is the expected value, E fv(k)g. If a random signal has the property that it is ergodic, 
then the expected value of f hv(k)j for a function f  can be approximated by replacing the 
ensemble average in Definition 4.3 with the simpler time average.
 
E f f hv(k)jg < 1
No
N21
k50
f hv(k)j  
 (4.7.3)
Using Definition 4.4, the circular auto-correlation of v(k) can be expressed in terms of 
expected values as follows.
cvv(k) 5 1
No
N21
i50
v(i)vp(i 2 k)
 
< E fv(i)vp(i 2 k)g 
 (4.7.4)
The samples of a white noise signal are statistically independent of one another. For 
statistically independent random variables, the expected value of the product is equal 
4.7
DEFiniTion
4.4 Circular 
Auto-correlation
Normalized circular 
auto-correlation
Ergodic signal
Statistically 
independent
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

290    Chapter 4  Fourier Transforms and Spectral Analysis
to the product of the expected values. Since the signal v(i) has zero mean, it follows 
that
cvv(k) < E fv(i)vp(i 2 k)g
 5 E fv(i)gE fvp(i 2 k)g
 
 5 0, k Þ 0
 
 (4.7.5)
When two signals are statistically independent, and one or both have zero mean, the expected 
value of the product is zero. In this case we say that the two signals are uncorrelated.
For the case k 5 0, cvv(0) 5 Pv where Pv is the average power of v(k). Thus from 
(4.7.5), the circular auto-correlation of a zero-mean white noise signal with average power 
Pv can be expressed as
 
cvv(k) < Pv (k), 0 # k , N  
 (4.7.6)
That is, the circular auto-correlation of zero-mean white noise is simply an impulse of 
strength Pv at k 5 0. As the value of N increases, the approximation in (4.7.6) becomes 
more accurate. The same analysis that was used to develop the approximation in (4.7.6) 
can be applied to linear auto-correlation as well, and the result is identical. Thus for 
N W 1, the linear auto-correlation of zero-mean white noise with average power Pv can 
be approximated as
 
rvv(k) < Pv (k), 0 # k , N  
 (4.7.7)
To numerically verify (4.7.7), suppose v(k) is Gaussian white noise with mean  5 0 
and standard deviation  5 1. The normalized linear auto-correlation for the case 
N 5 1024 is shown in Figure 4.27. Since the auto-correlation is normalized, the theo-
retical result should be vv(k) 5 (k). When circular auto-correlation is used, the only 
difference is that there is no narrowing of the tail of vv(k) for large values of k.
Uncorrelated signals
Zero-mean white noise
Figure 4.27:  
Normalized Linear 
Auto-correlation 
of Zero-mean 
Gaussian  
White Noise 
0
200
400
600
800
1000
20.2
0
0.2
0.4
0.6
0.8
1
1.2
k
vv(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.7 Auto-correlation     291
4.7.2 Power Density Spectrum 
There is a simple and elegant relationship between the power density spectrum and cir-
cular auto-correlation. Recall that the power density spectrum specifies the distribution 
of power over the discrete frequencies. For an N-point signal x(k), the power density 
spectrum is as follows where X(i) is the DFT of x(k).
 
Sx(i) 5
D  uX(i)u2
N
, 0 # i , N 
 (4.7.8)
To determine the relationship between Sx(i) and cxx(k), we use the cross-correlation 
property of the DFT from Table 4.8 with y 5 x. Applying the DFT to cxx(k) yields
Cxx(i) 5 DFThcxx(k)j
 5 X(i)X*(i)
N
 
 5 uX(i)u2
N
, 0 # i , N 
 (4.7.9)
Combining (4.7.8) and (4.7.9) then leads to the following alternative formulation of the 
power density spectrum.
 
Sx(i) 5 Cxx(i), 0 # i , N  
 (4.7.10)
Thus the DFT of the circular auto-correlation of x(k) is the power density spectrum. 
This is the DFT version of the Wiener-Khintchine theorem, and it is listed as one of the 
DFT properties in Table 4.8. A block diagram illustrating the Wiener-Khintchine theo-
rem is shown in Figure 4.28.
It is instructive to apply (4.7.10) to white noise. Suppose v(k) is zero-mean white noise 
with average power Pv. Using (4.7.6) and (4.7.10) we have
Sv(i) 5 Cvv(i)
 <
 DFT hPv(k)j
 
 5 Pv
 
 (4.7.11)
Hence zero-mean white noise with average power Pv has a power density spectrum that 
is flat and equal to Pv. It is for this reason that we refer to the noise as white because it 
contains power at all frequencies just as white light contains all colors.
Power density 
spectrum
White noise
Figure 4.28:  
Power Density  
Spectrum 
Using Circular 
Auto-correlation 
x(k)
Circular
auto-
correlation
cxx(k)
DFT
Sx(i)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

292    Chapter 4  Fourier Transforms and Spectral Analysis
Power Density Spectrum
EXAMPLE 4.13
To illustrate the application of Figure 4.28, let N 5 512 and consider a signal x(k) 
that consists of a double pulse of radius M 5 8 centered at k 5 Ny2.
x(k) 55
0,
0
# k , Ny2 2 M
1,
Ny2 2 M # k ,
Ny2
21,
Ny2
# k , Ny2 1 M
0, Ny2 1 M # k ,
N
When exam4_13 is run, it computes the power density spectrum Sx(i) by finding 
the DFT of the circular auto-correlation as in Figure 4.28. The resulting plots of 
x(k) and its power density spectrum, Sx(i), are shown in Figure 4.29.
0
100
150
50
200
250
300
350
400
450
500
21
0
1
k
x(k)
0
50
100
150
200
250
300
20.1
0
0.1
0.2
0.3
i
Sx(i)
Figure 4.29: Power Density Spectrum of Double Pulse of Width M 5 8 Using 
Circular Auto-correlation 
4.7.3 Extracting Periodic Signals from noise
Practical signals are often corrupted with noise. Suppose x(k) is a periodic signal with 
period M. We can model a noisy version of x(k) as follows.
 
y(k) 5  x(k) 1 v(k), 0 # k , N 
 (4.7.12)
Here v(k) represents additive zero-mean white noise that may arise, for example, from the 
measurement process or perhaps because x(k) is transmitted over a noisy communication 
channel.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.7 Auto-correlation     293
Period Estimation 
Our initial objective is to estimate the period of x(k) using y(k). Since v(k) contains power 
at all frequencies, completely removing v(k) with a filtering operation is not feasible. 
Instead, we can use correlation techniques. Consider the circular auto-correlation of the 
noisy signal y(k). Let yp(k) and vp(k) be the periodic extensions of the N-point signals 
y(k) and v(k), respectively. Using (4.7.4), Definition 4.4, and the fact that the expected 
value operator is linear, we have
cyy(k) < E fy(i)yp(i 2 k)g
5 E fhx(i) 1 v(i)jhxp(i 2 k) 1 vp(i 2 k)jg
5 E fx(i)xp(i 2 k) 1 x(i)vp(i 2 k) 1 v(i)xp(i 2 k) 1 v(i)vp(i 2 k)g
5 E fx(i)xp(i 2 k)g 1 E fx(i)vp(i 2 k)g 1 E fv(i)xp(i 2 k)g 1 E fv(i)vp(i 2 k)g
 
< cxx(k) 1 cxv(k) 1 cvx(k) 1 cvv(k) 
 (4.7.13)
Typically, the noise v(k) is statistically independent of the signal x(k). Since E fv(k)g 5 0, 
this means that the circular cross-correlation terms, cxv(k) and cvx(k), are both zero. Then 
using (4.7.6), the circular auto-correlation of the noisy signal y(k) simplifies to
 
cyy(k) < cxx(k) 1 Pv (k) 
 (4.7.14)
Consequently, cyy(0) 5 Px 1 Pv where Px is the average power of the signal, and Pv is 
the average power of the noise. For k . 0 we have cyy(k) < cxx(k). That is, the effect of 
auto-correlation is to average out or reduce the noise, so the circular auto-correlation of 
y(k) is less noisy than y(k) itself. Not only does the circular auto-correlation reduce noise, 
but it is also periodic with the same period as x(k). In particular, using x(k 1 M) 5 x(k) 
we have
cxx(k 1 M) 5 1
No
N21
i50
x(i)xp(i 2 k 2 M)
5 1
No
N21
i50
x(i)xp(i 2 k)
 
5 cxx(k) 
 (4.7.15)
Thus the circular auto-correlation of a periodic signal is itself periodic with the same 
period, but it is less noisy.
 
cxx(k 1 M) 5 cxx(k), 0 # k , N 2 M 
 (4.7.16)
Since cxx(k) is periodic with period M, and cyy(k) < cxx(k) for k . 0, it follows that 
the auto-correlation of y(k) will be periodic with period M. We can estimate the period 
of x(k) using a distinctive reference point such as a peak in cyy(k).
Periodic signal
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

294    Chapter 4  Fourier Transforms and Spectral Analysis
Period Estimation
EXAMPLE 4.14
Suppose N 5 256. Consider the following periodic signal that includes two sinu-
soidal components.
x(k) 5 cos1
32k
N 2 1 sin1
48k
N 2
Note that the cosine term has period Ny16 and the sine term has period Ny24. 
Thus the period of x(k) is the common period
M 5 N
8
 5 32
Suppose y(k) is a noisy version of x(k) as in (4.7.12) where v(k) is white noise uni-
formly distributed over the interval f2.5, .5g. A plot of y(k), obtained by running 
exam4_14, is shown in Figure 4.30. Note that the periodic nature of the underly-
ing signal x(k) is apparent, but it is difficult to precisely estimate the period due 
the presence of the noise. By contrast, the circular auto-correlation of y(k) is 
much less noisy, as can be seen from the plot in Figure 4.31. Using the DSP Com-
panion function f_caliper and rounding, the period of cyy(k) is M 5 32.
Figure 4.30: A Noisy Periodic Signal 
0
50
100
150
200
250
300
22.5
22
21.5
21
20.5
0
0.5
1
1.5
2
2.5
k
y(k)
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.7 Auto-correlation     295
Signal Estimation 
Once the period of a noise-corrupted periodic signal has been determined, we can use this 
information to extract the signal itself from the noise. Suppose x(k) is an N-point signal 
that is periodic with period M V N. Then the number of complete cycles of x(k) in y(k) is
 
L 5
 floor 1
N
M2 
 (4.7.17)
Next let M(k) be an N-point periodic impulse train with period M. One can represent 
M(k) as follows.
 
M(k) 5 o
L21
i50
(k 2 iM), 0 # k , N 
 (4.7.18)
Suppose y(k) is a noisy version of x(k) that has been corrupted by zero-mean white noise 
v(k) as in (4.7.12). The underlying periodic signal x(k) can be extracted from the noisy 
signal y(k) by cross-correlating with M(k). To see this, let yp(k) be the periodic extension 
of y(k). Recalling the time-reversal property of circular cross-correlation in Table 2.4 and 
using Definition 4.4, the circular cross-correlation of y(k) with M(k) is
cyM(k) 5 cM y(2k)
5 1
No
N21
i50
M(i)yp(i 1 k)
5 1
No
N21
i503o
L21
q50
(i 2 qM)4 yp(i 1 k)
 
5 1
No
L21
q50
yp(qM 1 k) 
 (4.7.19)
Figure 4.31: Circular Auto-correlation of Noisy Periodic Signal in Figure 4.30 
0
50
100
150
200
250
300
21
20.5
0
0.5
1
1.5
k
cyy(k)
1
1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

296    Chapter 4  Fourier Transforms and Spectral Analysis
Next the expression for y(k) in (4.7.12) can be substituted in (4.7.19). Recalling that x(k) 
is periodic with period M, we then have
cyM(k) 5 1
No
L21
q50
xp(qM 1 k) 1 1
No
L21
q50
vp(qM 1 k)
 5 1
No
L21
q50
xp(k) 1 1
No
L21
q50
vp(qM 1 k)
 
 5 Lx(k)
N
1 1
No
L21
q50
vp(qM 1 k), 0 # k , N 
 (4.7.20)
For each k, the last term in (4.7.20) represents a sum of L statistically independent noise 
terms. Since v(k) is zero-mean white noise, this means that for L W 1 the last term is 
approximately zero. Thus for N W M we have the following method of extracting a peri-
odic signal from noise. The underlying periodic signal x(k) can be approximated using 
x⁄(k) where
 
x⁄(k) 51
N
L2cyM(k), 0 # k , N  
 (4.7.21)
A block diagram summarizing the steps required to extract a periodic signal from noise 
using circular cross-correlation is shown in Figure 4.32.
Periodic signal 
extraction
Extracting a Periodic Signal  
from noise
EXAMPLE 4.15
To illustrate the application of Figure 4.32 to extract a periodic signal from noise, 
let N 5 256 and consider the following noisy periodic signal.
x(k) 5 cos1
32k
N 2 1  sin1
48k
N 2
y(k) 5 x(k) 1 v(k)
Suppose v(k) is white noise uniformly distributed over f2.5, .5g. The signal y(k) 
was considered previously in Example 4.14 and is shown in Figure 4.30. The anal-
ysis of the auto-correlation of y(k) in Figure 4.31 revealed the period of x(k) to be 
M 5 32. Therefore the number of complete cycles of x(k) in y(k) is
L 5 floor 1
N
M2
 5 8
Figure 4.32:  
Extracting a 
Periodic Signal 
of Period M 
from Noise 
Using Circular 
Cross-correlation
x(k)
1
y(k)
cyM(k)
M(k)
Circular
cross-
correlation
x(k)
(k)
N
L
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.7 Auto-correlation     297
For the periodic signal extraction in Example 4.15, the period of the periodic compo-
nent of y(k) consists of an integer number of samples M. It is also possible for the peri-
odic component of y(k) to have a period T0 that is not an integer multiple of T. For this 
more general case, a modified version of the technique in Figure 4.32 should be applied. 
When T0 Þ MT for an integer M, then the periodic impulse train M(k) should be gen-
eralized in such a way that the lone unit pulse at multiples of M samples is replaced by a 
pair of adjacent smaller pulses whose amplitudes sum to unity. In particular, let
 
M 5
 floor 1
T0
T2 
 (4.7.22)
Then a fraction  of the unit pulse will occur at multiples of sample M, and the remain-
ing fraction, 1 2 , will occur at one plus multiples of sample M where
 
 5 1 2  T0
T 1 M 
 (4.7.23)
The extraction of x(k) from the noise-corrupted y(k) using the technique in  
Figure 4.32 is performed by running exam4_15. The estimate of x(k) using circu-
lar cross-correlation is
x⁄(k) 51
N
L2cyM(k)
A plot comparing the first two periods of x(k) with the estimate, x⁄(k), is shown in  
Figure 4.33. The reconstruction, in this case, is quite reasonable, but it is not exact 
given that the noise term in (4.7.20) is only approximately zero. Note that the 
estimate, x⁄(k), should improve at NyM increases.
An alternative approach to extracting a periodic signal from noise is to use a sharp 
comb filter. First autocorrelation is used to estimate the the frequency of the funda-
mental harmonic, F0 5 fsyM. The design of comb filters is discussed in Chapter 7.
Figure 4.33: Comparison of Periodic Signal x(k) and Estimate x⁄(k) Extracted 
from y(k) Using Circular Cross-correlation 
0
10
20
30
40
50
60
70
22.5
22
21.5
21
20.5
0
0.5
1
1.5
2
2.5
k
Signals
Signal
Estimate
Noisy signal
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

298    Chapter 4  Fourier Transforms and Spectral Analysis
Notice that if T0 5 MT, then  5 1, and all of the unit pulse is applied at multiples of 
sample M. Furthermore, as T0 S (M 1 1)T,  S 0 and (1 2 ) S 1.
Zero Padding and Spectral Resolution 
4.8.1 Frequency Response Using the DFT 
Recall that the frequency response H( f ) of a stable linear system was defined as the 
transfer function H(z) evaluated along the unit circle. Alternatively, H( f ) is the DTFT of 
the impulse response h(k). For a causal system,
 
H( f ) 5 o
`
k50
h(k) exp(2jk2f T ) 
 (4.8.1)
Put another way, the frequency response is the spectrum of the impulse response. 
The magnitude spectrum, A( f ) 5 uH( f )u, is the magnitude response of the system, and 
the phase spectrum, ( f ) 5 /H( f ), is the phase response of the system. There is a sim-
ple way to approximate the frequency response using the DFT, and in certain instances 
this approximation is exact. Let H(i) denote the DFT of the first N samples of h(k).
 
H(i) 5
 DFT hh(k)j, 0 # i , N 
 (4.8.2)
Next suppose we evaluate the frequency response at the ith discrete frequency fi 5 ifsyN. 
Recalling that WN 5  exp(2j2yN) and using (4.8.1),
H(fi) 5 o
`
k50
h(k) exp(2jki2yN)
 5 o
`
k50
h(k)W  ik
N
 5 o
N21
k50
h(k)W  ik
N 1 o
`
k5N
h(k)W  ik
N
 
 5 H(i) 1 o
`
k5N
h(k)W ik
N
 
 (4.8.3)
Thus the difference between H(fi) and H(i) is represented by the tail of the DTFT of the 
impulse response h(k). The magnitude of this difference can be bounded in the following way.
 uH( fi) 2 H(i)u 5 uo
`
k5N
h(k)W  ik
Nu
 # o
`
k5N
uh(k)W  ik
Nu
 5 o
`
k5N
uh(k)u ? uW ik
Nu
 
 5 o
`
k5N
uh(k)u
 
 (4.8.4)
Notice that the upper bound in (4.8.4) no longer depends on i. For a stable filter, the impulse 
response is absolutely summable, so the right-hand side of (4.8.4) is finite and goes to zero 
as N S `. Consequently, the DFT of the impulse response can be used to approximate the 
discrete-time frequency response as long as the number of samples N is sufficiently large.
 
H(i) < H( fi ), 0 # i # N
2  
 (4.8.5)
4.8
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.8 Zero Padding and Spectral Resolution     299
In (4.8.5) we restrict i to the interval 0 # i # Ny2 because, for a real system, H(i) exhibits 
symmetry about the midpoint i 5 Ny2 as in Table 4.7. In effect, all of the essential informa-
tion is contained in the subinterval 0 # i # Ny2 which corresponds to positive frequencies.
For an important class of digital filters, the approximation in (4.8.5) is exact. To see 
this recall that if H(z) is an FIR filter of order m, then h(k) 5 0 for k . m. It then follows 
from (4.8.4) that the upper bound on the error is zero for N . m. That is, if H(z) is an 
FIR filter of order m and N . m, then H(i) is an exact representation of the N samples 
of the frequency response. The frequency response, H(i), can be expressed in polar form as 
H(i) 5 A(i) expf  j(i)g where the A(i) is the magnitude response, and (i) is the phase response.
 
A(i) 5
D  
uH(i)u 
 (4.8.6a)
 
(i) 5
D  
/H(i) 
 (4.8.6b)
Discrete-time Frequency Response
EXAMPLE 4.16
As an example of using (4.8.2) to find the frequency response, consider a running 
average filter of order M 2 1.
y(k) 5 1
M o
M21
i50
x(k 2 i)
The impulse response of this FIR filter is a pulse of amplitude 1yM and duration 
M samples starting at k 5 0.
h(k) 5 1
M o
M21
i50
(k 2 i)
The frequency response H(i) 5
 DFT hh(k)j for the case M 5 8, N 5 1024, and 
fs 5 200 Hz can be obtained by running exam4_16. Note that since N 5 210, the 
FFT implementation can be used. The resulting magnitude response A( f ) and 
phase response ( f ) are shown in Figure 4.34. There are My2 lobes in the mag-
nitude response, and the phase response has My2 jump discontinuities between 
the lobes but is otherwise linear. The independent variable used in Figure 4.34 is 
f 5 ifsyN.
The relatively large side lobes in the magnitude response reveal that a run-
ning average is not particularly effective as a lowpass filter because the stopband 
gain outside the main lobe is relatively large. If a weighted average of the last M 
samples is used instead, the size of the side lobes can be reduced. For example, 
consider the following weighted average that uses weighting based on something 
called the Hanning window.
y(k) 5 1
M o
M21
i50
 31 2  cos1
2k
M 2 124
 x(k 2 i)
The magnitude response for this weighted average filter is shown in Figure 4.35. 
Note how the amplitudes of the side lobes have been significantly reduced as a 
result of the weighting, although the main lobe is now wider.
Running average filter
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

300    Chapter 4  Fourier Transforms and Spectral Analysis
Decibel Scale (dB) 
The plot of the magnitude response in Figure 4.34 is referred to as a linear plot because 
both axes use linear scales. If there is a large frequency range or a large range in the val-
ues of the magnitude response, then logarithmic units are often used. When plotting the 
0
100
0
0.5
1
f (Hz)
f (Hz)
A(f)
0
20
30
10
40
50
60
70
80
90
20
30
10
40
50
60
70
80
90
100
24
22
0
2
(f)
Figure 4.34: Frequency Response of Running Average Filter with M 5 8, 
N 5 1024, and fs 5 200 Hz 
Figure 4.35: Magnitude Response of Weighted Average Filter with Hanning 
Window Weighting, m 5 8, N 5 1024, and fs 5 200 Hz 
0
20
30
10
40
50
60
70
80
90
100
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
f (Hz)
A(f)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.8 Zero Padding and Spectral Resolution     301
magnitude response of a filter, or the magnitude spectrum of a signal, the logarithmic 
unit that is typically used is the decibel, which is abbreviated dB. A decibel is defined as 
10 times the logarithm (base 10) of the signal power. Since power corresponds to uH(i)u2, 
this translates to 20 times  log10 of uH(i)u.
 
A(i) 5
D  
20 log10(uH(i)u)   dB  
 (4.8.7)
One of the useful characteristics of the dB scale is that it allows us to better quantify 
how close to zero the magnitude response gets in the stop band. Notice from (4.8.7) that a 
gain of unity corresponds to zero dB and that every reduction in uH(i)u by a factor of 10 cor-
responds to a change of 220 dB. For an ideal filter, the signal attenuation in the stopband is 
complete, so the magnitude response is zero. However, in practice Wiener and Paley (1934) 
showed that the magnitude response of a causal system cannot be identically zero over a 
continuum of frequencies, it can only be zero at isolated frequencies. By using decibels, we 
can see how close to zero the magnitude response gets. However, there is one problem that 
arises with the use of decibels. If the magnitude response does go to zero at certain isolated 
frequencies as in Figure 4.34, then from (4.8.7) this corresponds to a decibel value of minus 
infinity. To accommodate this situation we usually replace uH(i)u in (4.8.7) as follows.
 
A(i) 5 20 log10( max huH(i)u, j)   dB  
 (4.8.8)
Here  . 0 is taken to be a very small number. For example, the single precision machine 
epsilon, M 5 1.19 3 1027, might be used. A plot of the magnitude response of the filter 
in Example 4.16 using  5 M and the logarithmic dB scale is shown in Figure 4.36.
Decibel
Figure 4.36:  
Magnitude 
Response of  
Running Average 
Filter in Example 
4.16 Using the  
Logarithmic  
Decibel Scale 
0
10
20
30
40
50
60
70
80
90
2150
2100
250
0
50
f (Hz)
A«(f) (dB)
Recall from Section 3.8 that the DSP Companion function f_freqz can be used 
to compute the discrete-time frequency response. Function f_freqz is the discrete 
equivalent of the continuous-time frequency response function f_freqs introduced in 
Chapter 1. If the MATLAB signal processing toolbox is available, there is a function 
called freqz that is available for computing the discrete frequency response.
DSP Companion
DSP Companion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

302    Chapter 4  Fourier Transforms and Spectral Analysis
4.8.2 Zero Padding
There is a simple way to interpolate between the points of signal spectra without increas-
ing the sampling rate. The length of the signal can be increased from N to M by padding 
x(k) with M 2 N zeros as follows.
 
xz(k) 5
D  5
x(k)
 
, 0 # k , N
0,
N # k , M
 
 (4.8.9)
Here xz(k) denotes the zero-padded version for x(k). Suppose Sz(i) is the power density 
spectrum of xz(k), but scaled by MyN because no new power was added by the zero 
padding. That is,
 
Sz(i) 5
D  uXz(i)u2
N
, 0 # i , M 
 (4.8.10)
where Xz(i) 5
 DFT hxz(k)j. Then the frequency precision, or space between adjacent dis-
crete frequencies, of Sz(i) is
 
Dfz 5 fs
M  
 (4.8.11)
The effect of zero padding is to interpolate between the original points of the spec-
trum. In particular, if M 5 qN for some integer q, there will be q 2 1 new points between 
each of the original N points. The original points remain unchanged.
 
Xz(qi) 5 X(i), 0 # i , N 
 (4.8.12)
By decreasing Dfz through zero padding, the location of an individual sinusoidal 
spectral component can be more precisely determined as can be seen from the following 
example.
Zero-padded
Frequency precision
Zero Padding
EXAMPLE 4.17
As an illustration of how zero padding can be put to effective use, suppose 
fs 5 1024 Hz and N 5 256. Consider a noise-free signal with a single sinusoidal 
spectral component at F0 5 330.5 Hz.
x(k) 5 cos(2F0kT), 0 # k , N
If we zero-pad x(k) by a factor of eight, then M 5 8N 5 2048. Thus the fre-
quency precision before and after zero padding is
Dfx 5 fs
N 5 4  Hz 
Dfz 5 fs
M 5 .5  Hz 
The corresponding sinusoidal frequency indices for the two cases are
ix 5 F0
Dfx
5 82.625
iz 5 F0
Dfz
5 661
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.8 Zero Padding and Spectral Resolution     303
4.8.3 Spectral Resolution 
Although zero padding allows us to identify the precise location of an isolated sinusoidal 
frequency component, it is less helpful when it comes to trying to resolve the difference 
between two closely spaced sinusoidal components. When sinusoidal components are too 
close together, their peaks in the power density spectrum tend to merge into a single broader 
Notice that ix is not an integer so the rounded value, 83, must be used. It follows 
that the peaks in the power density spectra will occur at
fx 5 83Dfx 5 332  Hz 
fz 5 661Dfz 5 330.5  Hz 
Consequently, with a frequency precision of Dfz 5 .5 Hz, the zero-padded power 
density spectrum should be able to locate the sinusoidal component exactly. The 
two power density spectra can be computed by running exam4_17, and the result-
ing plot is shown in Figure 4.37. To clarify the display, only the frequency range 
300 # f # 360 Hz is shown. The low-precision power density spectrum, Sx( f ), 
is plotted with isolated points, while the high-precision power density spectrum, 
Sz( f ), is plotted as points connected by straight lines. Note how Sz( f ) interpo-
lates between the points of Sx( f ). In this case MyN 5 8, so there are seven points 
of Sz( f ) between each pair of points in Sx( f ).
The ringing in Sz( f ) arises because we are multiplying an infinitely long x(k) by 
a rectangular window to obtain xz(k). As we shall see later in this chapter, we can 
reduced the side lobes, at the expense of broadening the main lobe, by multiplying 
by a different window (see also Example 4.16).
Ringing
Figure 4.37: Power Density Spectra Using Zero Padding of N 5 256 Samples 
of x(k) to Create M 5 2048 Samples of xz(k) 
300
310
320
330
340
350
360
0
10
20
30
40
50
60
70
80
f (Hz)
Sz(f)
Sx(f)
Sz(f)
Exact F0
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

304    Chapter 4  Fourier Transforms and Spectral Analysis
peak. The basic problem with zero padding is that it does not add any new information to 
the signal. The frequency resolution, or smallest frequency difference that one can detect, 
is limited by the sampling frequency and the number of nonzero samples, N. The following 
ratio, called the Rayleigh limit, represents the value of the frequency resolution.
 
DF 5 fs
N  
 (4.8.13)
Thus there is a difference between the frequency precision, which is the spacing between 
discrete frequencies of the DFT, and the frequency resolution, which is the smallest differ-
ence in frequencies that can be reliably detected. Zero padding sometimes can be helpful 
in distinguishing between two spectral components, but we cannot go below the Rayleigh 
limit in (4.8.13). Observe that since fs 5 1yT, the Rayleigh limit is just the reciprocal 
of the length of the original signal,  5 NT. Thus for a given sampling frequency, to 
improve the frequency resolution we must add more samples.
Rayleigh limit
Frequency resolution
Frequency Resolution
EXAMPLE 4.18
As an illustration of the detection of two closely spaced sinusoidal components, 
suppose fs 5 1024 Hz and N 5 1024. Consider the following noise-free signal 
with spectral components at F0 5 330 Hz, and F1 5 331 Hz.
x(k) 5 sin(2F0kT ) 1  cos(2F1kT ), 0 # k , N
If we zero-pad x(k) by a factor of two, then M 5 2N 5 2048. Thus the frequency 
precision before and after zero padding is
Dfx 5 fs
N 5 1  Hz 
Dfz 5 fs
M 5 .5  Hz 
The two power density spectra can be computed by running exam4_18. The 
resulting plots are shown in Figures 4.38 and 4.39, respectively. To clarify the 
display, only the frequency range of 320 # f # 340 Hz is shown in each case. The 
low-precision power density spectrum in Figure 4.38 shows a single broad peak 
centered at the midpoint,
Fm 5 F0 1 F1
2
5 330.5  Hz 
By zero padding we can decrease the frequency increment from 1 Hz to .5 Hz. 
This results in the plot shown in Figure 4.39, where a pair of peaks begins to 
emerge but with some overlap. The power density spectrum between the peaks 
decreases sufficiently to make the two peaks discernible, but it does not go to zero 
between the peaks. From (4.8.13), the Rayleigh limit is
DF 5 fs
N 5 1  Hz 
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.8 Zero Padding and Spectral Resolution     305
320 322 324 326 328 330 332 334 336 338 340
0
50
100
150
200
250
300
f (Hz)
Sz(f)
Sz(f)
Exact F0, F1
Figure 4.39: Spectral Resolution of Two Closely Spaced Sinusoidal 
Components Using Zero Padding with fs 5 1024 Hz, N 5 1024, and M 5 2048 
Figure 4.38: Power Density Spectrum of Two Closely Spaced Sinusoidal 
Components with fs 5 1024 Hz and N 5 1024 
320
324 326 328
322
330
334 336 338
332
340
0
50
100
150
200
250
300
f (Hz)
Sx(f)
Sx(f)
Exact F0, F1
Interestingly enough, increasing M beyond 2048 does not make the two peaks 
more discernible because the separation between F0 and F1 is the Rayleigh limit.
uF1 2 F0u 5 DF
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

306    Chapter 4  Fourier Transforms and Spectral Analysis
320
324 326 328
322
330
334 336 338
332
340
0
100
200
300
400
500
600
f (Hz)
Sx(f)
Sx(f)
Exact F0, F1
Figure 4.40: Spectral Resolution of Two Closely Spaced Sinusoidal Components 
Using Improved Frequency Resolution with fs 5 1024 Hz and N 5 2048 
Of course we can decrease the Rayleigh limit by adding more samples. If the num-
ber of samples is doubled to N 5 2048, then the resulting power density spectrum, 
without any zero padding, is shown in Figure 4.40. Here it is clear that the two 
peaks are now separated.
The concept of zero padding is useful even when the motivation is not to improve the 
frequency precision. If the number of samples N is not a power of two, then a radix-two  
FFT cannot be used. Instead a DFT or perhaps a less efficient alternative form of 
the FFT is needed. This problem can be circumvented by simply padding x(k) with 
enough zeros to make the new length M the next power of two. The following code 
fragment can be used.
M = nextpow2(length(x)); 
% M = next power of 2
X = fft(x,M); 
% Zero-pad to M samples
For example, if N 5 1000 then padding x(k) with 24 zeros will increase the num-
ber of samples to M 5 210 5 1024. This increases storage requirements by 2.4 per-
cent, but in exchange it significantly decreases the computational time. Recall that 
a 1000-point DFT requires N2 5 106 complex FLOPs. However, a 1024-point FFT 
requires only (My2) log2(M) 5 5120 complex FLOPs. Thus at a cost of 2.4 percent 
in storage space, we have purchased an increase in speed by a factor of 99.5, a real 
bargain!
MATLAB Functions
MATLAB Functions
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.9 The Spectrogram    307
x(L/2)
x(0)
x(L)
x2(k)
x4(k)
x6(k)
x8(k)
x1(k)
x3(k)
x5(k)
x7(k)
x9(k)
x(N 2 1 2 L/2)
x(N 2 1)
...
...
Figure 4.41:  
Decomposition of 
x(k) into 2M 2 1 
Overlapping 
Subsignals of 
Length L where 
M 5 5 
The Spectrogram
4.9.1 Data Windows 
Many signals of practical interest are sufficiently long that their spectral characteristics can 
be thought of as changing with time. For example, a segment of recorded speech can be bro-
ken down into more basic units such as words, syllables, or phonemes. Each basic unit will 
have its own distinct spectral characteristics. One way to capture the fact that the spectral 
characteristics are changing with time is to compute a sequence of short overlapping DFTs, 
sometimes called a short term Fourier transform or STFT. For example, suppose x(k) is a 
long N-point signal where N 5 LM for integers L and M. Then x(k) can be decomposed 
into 2M 2 1 overlapping subsignals of length L as shown in Figure 4.41 for the case M 5 5.
If the subsignals are denoted xm(k) for 0 # m , 2M 2 1, then the mth subsignal can 
be extracted from the original signal x(k) as
 
xm(k) 5
D  
x(mLy2 1 k), 0 # m , 2M 2 1, 0 # k , L 
 (4.9.1)
A useful way to look at subsignal xm(k) is as a product of the original signal x(k) times a 
window function wR(k).
 
xm(k) 5 wR(k 2 mLy2)x(k) 
 (4.9.2)
Comparing (4.9.2) with (4.9.1), the window function is a rectangular window of unit 
amplitude that is zero for all k except 0 # k , L. In terms of step functions
 
wR(k) 5
D  (k) 2 (k 2 L)  
 (4.9.3)
When a signal is multiplied by a rectangular window in the time domain, this creates 
something called spectral leakage in the frequency domain. An example of this phenom-
enon can be seen in the frequency response of the running average filter in Example 4.16. 
There the impulse response was a rectangular pulse of height 1yM and width M. The 
magnitude response in Figure 4.34 included several side lobes in addition to the larger 
main lobe. These side lobes are a manifestation of spectral leakage that occurs when there 
is an abrupt transition or vertical edge in the time domain.
The side lobes can be reduced, at the expense of making the main lobe wider, by using a 
“softer’’ or less abrupt window, as was done in Figure 4.35 using the Hanning window. Non-
rectangular data windows go to zero more gradually than the rectangular window, and for 
these windows the frequency domain leakage outside the main lobe is reduced. Some popu-
lar window functions for generating data windows of width L are summarized in Table 4.10.
Plots of the rectangular, Hanning, Hamming, and Blackman windows in Table 4.10 
for the case L 5 256 are shown in Figure 4.42. Note how all of the windows except the 
rectangular window, wR(k), decrease gradually. With the exception of the Hamming win-
dow, they all taper to zero by the time the edge of the window is reached. The spectral 
characteristics of data windows are examined in more detail in Chapter 6 where they are 
used in the design of digital FIR filters.
4.9
STFT
Subsignal
Rectangular window
Spectral leakage
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

308    Chapter 4  Fourier Transforms and Spectral Analysis
4.9.2 Spectrogram
The 2M 2 1 subsignals of length L in (4.9.1) overlap one another by Ly2 samples. Sup-
pose we take the DFTs of windowed versions of xm(k) for 0 # m , 2M 2 1 and arrange 
them as the rows of a matrix. This matrix, called a spectrogram, reveals how the spectral 
characteristics of the signal x(k) change with time.
Let x(k) be an N-point signal decomposed into 2M 2 1 overlapping subsignals xm(k) 
of length L as in (4.9,1), and let w(k) be a window function from Table 4.10. The spec-
trogram of x(k) is a (2M 2 1) 3 L matrix G defined
G(m, i) 5
D  
u DFT hw(k)xm(k)ju, 0 # m , 2M 2 1, 0 # i , L
The spectrogram G(m, i) is a collection of short DFTs parameterized by the starting 
time. The first independent variable m specifies the starting time, in increments of Ly2, 
while the second variable i specifies the frequency, in increments of fsyL. The purpose of 
DEFiniTion 
4.5 Spectrogram
0
50
100
150
200
250
20.2
0
0.2
0.4
0.6
0.8
1
k
w(k)
0
1
2
3
Figure 4.42:  
Window Functions 
for L 5 250:  
0 = Rectangular,  
1 = Hanning,  
2 = Hamming,  
and 3 = Blackman 
number 
name 
w(k) 
0 
Rectangular 
wR(k) 
1 
Hanning 
30.5 2 0.5 cos 1
2k
L 24wR(k) 
2 
Hamming 
30.54 2 0.46 cos 1
2k
L 24wR(k) 
3 
Blackman 
30.42 2 0.5 cos 1
2k
L 2 1 0.08 sin 1
4k
L 24wR(k) 
Table 4.10: Window 
Functions for Data 
Windows of Width L 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.9 The Spectrogram    309
the window function w(k) is to reduce frequency domain leakage. When the basic rectan-
gular window is used, the values of G(m, i) tend to get smeared or spread out along the 
frequency dimension due to leakage.
The spectrogram is typically displayed as a two-dimensional contour plot. Note that 
G(m, i) $ 0. The two-dimensional display uses different colors or shades of a color to 
denote the values of G(m, i) within certain bands or levels. Thus a spectrogram is a con-
tour plot of slices or level surfaces of the windowed magnitude spectra. Note that if x(k) 
is real, then G(m, i) will exhibit even symmetry about the line i 5 Ly2. Consequently, 
only the (2M 2 1) 3 Ly2 submatrix on the left needs to be plotted for real x(k).
Spectrogram
EXAMPLE 4.19
Consider the signal x(k) shown in Figure 4.43, which consists of a four-second 
recording of the vowels, {A, E, I, O, U}, spoken in succession. In this case, the 
sampling rate was fs 5 8000 Hz, and the number of samples was N 5 32000. If 
we choose L 5 256, then M 5 125 and G is a 250 3 256 matrix.
When exam4_19 is run it computes G(m, i). The resulting spectrogram 
(just the first half) using p 5 12 levels and a rectangular window is shown in  
Figure 4.44. Note how each vowel has its own distinct set of contour lines. For 
example, the vowel “I” has significant power between zero and 1500 Hz, while 
the vowel “A” has power between zero and about 600 Hz, with some additional 
power centered around 2500 Hz. The use of a rectangular window (no tapering) 
tends to smear the spectrogram features parallel to the frequency axis due to 
the leakage phenomenon. A somewhat cleaner spectrogram can be obtained by 
using the Hamming window as shown in Figure 4.45. Here the islands associ-
ated with peaks in the magnitude response are more isolated from one another.
Figure 4.43: Recording of Vowels at fs 5 8000 Hz 
0
0.5
1
1.5
2
2.5
3
3.5
4
21
20.8
20.6
20.4
20.2
0
0.2
0.4
0.6
0.8
1
kT (sec)
x(k)
A
E
I
O
U
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

310    Chapter 4  Fourier Transforms and Spectral Analysis
Figure 4.44: Spectrogram of Vowels Using Rectangular Window and 12 Levels 
f (Hz)
t (sec)
A
E
I
O
U
0
500
1000 1500 2000 2500 3000 3500 4000
0
0.5
1
1.5
2
2.5
3
3.5
4
Figure 4.45: Spectrogram of Vowels Using Hamming Window and 12 Levels 
f (Hz)
t (sec)
A
E
I
O
U
0
500
1000 1500 2000 2500 3000 3500 4000
0
0.5
1
1.5
2
2.5
3
3.5
4
The spectrogram plots time on one axis and frequency on the other axis. Along the 
time axis, it is clear from Figure 4.41 that a DFT is computed every Ly2 samples where L 
is the subsignal length. Therefore the precision along the vertical time axis is
 
Dt 5 LT
2  
 (4.9.4)
Recall that N 5 LM where N is the total number of samples, and M is the number of 
overlapping subsignals. Since the subsignals overlap, the number of subsignals must 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.9 The Spectrogram    311
satisfy 2 # M # Ny2. It then follows that L 5 NyM must lie in the range 2 # L # Ny2. 
Thus the achievable values for the time precision Dt fall in the range
 
T # Dt # NT
4  
 (4.9.5)
Next consider the frequency axis. Since each subsignal is of length L, the L-point 
DFT has a frequency increment or precision of
 
Df 5  fs
L 
 (4.9.6)
This results in the following range of values for the frequency precision Df  along the 
horizontal axis.
 
2fs
N # Df # fs
2 
 (4.9.7)
Note that the best time precision is then Dt 5 T, and the best frequency precision 
is Df 5 2FsyN. Unfortunately, these minimal values for Dt and Df  cannot be achieved 
simultaneously. Indeed, if one multiplies the expression for Dt in (4.9.4) times the expres-
sion for Df  in (4.9.6) and simplifies, the final result is constant.
 
DtDf 5 1
2  
 (4.9.8)
Thus, choosing L to minimize Dt will maximize Df , and conversely the inverse rela-
tionship between Dt and Df  shows the inherent trade-off that one must make in selecting 
the subsignal length L.
It is also possible to recursively decompose the signal x(k) into low frequency and 
high frequency parts using something called the discrete wavelet transform or DWT. A 
treatment of signal representation using wavelets is outside the scope of this book. For 
a discussion of wavelets and their use in time-frequency analysis, the interested reader is 
referred, for example, to (Burrus and Guo, 1997).
DWT
The DSP Companion contains the following function for computing the spectro-
gram of a discrete-time signal.
% F_SPECGRAM: Compute spectrogram of a signal
%
% Usage:
% 
[G,f,t] = f_specgram (x,L,fs,win)
% Pre:
% 
x 
= vector of length N samples
% 
L 
= subsignal length
% 
fs = sampling frequency
% 
win = window type
%
% 
0 = rectangular
% 
1 = Hanning
DSP Companion
DSP Companion
(Continued   )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

312    Chapter 4  Fourier Transforms and Spectral Analysis
% 
2 = Hamming
% 
3 = Blackman
%
% Post:
% 
G = (2M-1) by L matrix containing spectrogram where M = N/L.
% 
f = vector of length L/2 containing frequency values
% 
t = vector of length 2M-1 containing time values
Power Density Spectrum Estimation
In this section we develop techniques for estimating the underlying continuous-time 
power density spectrum (PDS) of a signal when the length of the data sequence N is 
large. In addition, we focus on the practical problem of detecting the presence and loca-
tion of one or more sinusoidal components buried in a signal that is corrupted with noise. 
The problem is made more challenging when the frequencies of the unknown sinusoidal 
components do not correspond to any of the discrete frequencies fi 5 ifsyN, which are 
sometimes called the bin frequencies.
The topic of power density spectrum estimation is one that has been studied in some 
depth (Proakis and Manolakis, 1992; Ifeachor and Jervis, 2002; Prat, 1997). The treat-
ment presented here is a presentation of the classical nonparametric estimation methods. 
Recall that the power density spectrum of an N-point signal is given by
 
Sx(i) 5  uX(i)u2
N
, 0 # i , N 
 (4.10.1)
Here Sx(i) specifies the average power contained in the ith harmonic of the periodic 
extension, xp(k), of the N-point signal x(k). The average power of x(k) is the average of 
the power density spectrum.
 
Px 5 1
No
N21
i50
Sx(i)  
 (4.10.2)
4.10.1 Bartlett’s Method
The formulation of the power density spectrum in (4.10.1) is sometimes called a  
periodogram. If the sequence x(k) is long, then a more reliable way to estimate the power 
density spectrum is to partition x(k) into a number of subsignals. Suppose N 5 LM for a 
pair of positive integers L and M. Then x(k) can be partitioned into M subsignals, each 
of length L, as shown in Figure 4.46.
If the subsignals are denoted xm(k) for 0 # m , M, then the mth subsignal can be 
extracted from the original signal x(k) as
 
xm(k) 5
D  
x(mL 1 k), 0 # k , L 
 (4.10.3)
 4.10
Optional material
Bin frequency
Periodogram
mth subsignal
*
Figure 4.46:  
Partition of x(k) 
into M Subsignals 
of Length L 
x(0)
x(L)
x(2L)
x1(k)
x2(k)
xm(k)
x(N 2 1)
...
...
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 4.10 Power Density Spectrum Estimation    313
Next let Xm(i) 5
 DFT hxm(k)j for 0 # m , M. The estimated power density spectrum of 
x(k) is obtained by taking the average of the M individual power density spectra.
 
SB(i) 5
1
LM o
M21
m50
uXm(i)u2, 0 # i , L  
 (4.10.4)
The power density spectrum estimate SB(i) is called an average periodogram or  
Bartlett’s method (Bartlett, 1948). Note that one of the consequences of Bartlett’s method 
is that it changes the frequency precision of the power density spectrum. If fs is the sam-
pling frequency, the frequency precision, or space between adjacent discrete frequencies, is
 
Df 5  fs
L 
 (4.10.5)
This is in contrast to the periodogram method in (4.10.1) where the frequency precision 
is Df 5 fsyN. Since N 5 LM, the frequency precision of Bartlett’s method is larger by a 
factor of M. This loss of precision is offset by the observation that the variance in the 
estimated power density spectrum is reduced by the same factor. We can approximate the 
variance of the average periodogram as follows.
 
2
B 5 1
Lo
L21
i50
fSB(i) 2 Pxg2 
 (4.10.6)
Ideally, the power density spectrum of white noise should be a flat line at Sx(i) 5 Px as 
in (4.7.11). By reducing the variance of the computed power density spectrum, Bartlett’s 
method comes closer to this ideal, as can be seen in the following example.
Bartlett’s method
Bartlett’s Method: White noise
EXAMPLE 4.20
As an illustration of how the average periodogram can improve the estimate of 
the power density spectrum, let x(k) be white noise uniformly distributed over the 
interval f25, 5g. Then from (4.4.6) the average power of the white noise signal is
Pv 5 53 2 (25)3
3f5 2 (25)g
 5 250
30
 5 8.333
Suppose the length of x(k) is N 5 2048. One way to factor N is L 5 256 and 
M 5 NyL 5 8. In this case, we compute eight periodograms, each of length 256. 
The resulting average periodogram can be obtained by running exam4_20. A plot 
of the estimated power density spectrum, SB(i), is shown in Figure 4.47. This uni-
form white noise signal was previously analyzed in Example 4.11. Comparing Figure 
4.47(a) with Figure 4.23, we see that the average periodogram estimate is noticeably 
flatter and closer to the ideal characteristic, Pv 5 8.333. The actual average power 
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

314    Chapter 4  Fourier Transforms and Spectral Analysis
Figure 4.47: Periodograms of White Noise Uniformly Distributed over f25, 5g 
with N 5 4096; (a) Average Periodogram with L 5 512, M 5 8,  2
B 5 9.940; 
(b) Single Periodogram with L 5 4096, M 5 1,  2
x 5 76.225 
0
20
40
60
80
100
120
0
50
100
(a)
i
SB(i)
Sx(i)
0
200
300
100
400
500
600
700
800
900 1000
0
50
100
(b)
i
is also closer to the theoretical value of Pv as a result of using the longer signal of 
length N 5 2048. In Figure 4.33 Px 5 8.781 whereas in this case we have
Px 5 1
No
N21
k50
x2(k)
 5 8.581
The reduction in the variance of the estimate is a consequence of the averaging 
of M periodograms. Using (4.10.6), the variance of the estimate in this case is
 2
B 5 1
L o
L21
i50
fSB(i) 2 Pxg2
 5 8.979
The curious reader might wonder if perhaps the same improvement in the esti-
mate of the power density spectrum might be achieved with a single periodogram 
by simply increasing N from 512 in Example 4.11 to the 2048 samples used in 
Example 4.20. This is an intuitively appealing conjecture, but unfortunately it 
does not hold, as can be seen in Figure 4.47(b), which uses L 5 2048 and M 5 1. 
Although the average power Px 5 8.581 does come closer to Pv as N increases, 
it is apparent that the variance in the power density spectrum curve does not 
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 4.10 Power Density Spectrum Estimation    315
decrease in this case. In particular, by using (4.10.6) the variance using a single 
periodogram of length N 5 2048 is
 2
x 5 73.621
If we take the ratio of the two variances we find that 2
xy2
B 5 8.200. Thus the 
improvement in the variance achieved by Bartlett’s average periodogram method 
was by a factor of approximately M where M 5 8 is the number of subsignals.
Next consider the practical problem of detecting the presence, and precise location, 
of unknown sinusoidal components in a signal x(k).
Bartlett’s Method: Periodic input
EXAMPLE 4.21
Suppose the sampling rate is fs 5 1024 Hz, and the number of samples is N 5 512. 
If Bartlett’s method is used with L 5 128 and M 5 4, then from (4.10.5) the fre-
quency precision is
Df  5  fs
L
 5  8  Hz 
Consider a signal x(k) that contains two sinusoidal components: one at frequency 
F0 5 200 Hz, and the other at frequency F1 5 331 Hz.
x(k) 5  sin(2F0kT ) 2 Ï2 cos(2F1kT )
Observe that F0 is an integer multiple of Df , so F0 corresponds to discrete fre-
quency i0 with
i0 5 F0
Df
 5 25
However, F1 is not an integer multiple of the frequency precision Df . Therefore, 
F1 falls between two discrete frequencies at
i1 5 F1
Df
 5 41.375
The average periodogram estimate of the power density spectrum of x(k) can be 
obtained by running exam4_21. The resulting plot of SB( f ) is shown in Figure 4.48.  
Here we have used the independent variable f 5 ifsyL to facilitate interpretation 
of the frequencies. Note that there are two spikes indicating the presence of two 
sinusoidal components.
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

316    Chapter 4  Fourier Transforms and Spectral Analysis
The first spike is centered at F0 5 200 Hz as expected. This spike is symmetric 
about f 5 F0, and it is relatively narrow with a width of 68 Hz which corresponds 
to the frequency precision Df . Next consider the spectral spike associated with F1. 
Unfortunately, this spike is not as narrow as the low-frequency spike, particularly 
near the bottom. Furthermore, the high-frequency spike is also not centered at 
F1 5 331 Hz. Instead the peak of the second spike occurs below F1 at discrete 
frequency
f41 5 41fs
L
 5 328  Hz 
The estimate of the location of the second spectral component is low by 0.91 
percent. Thus the estimated power density spectrum does an effective job in detect-
ing sinusoidal components located at the discrete frequencies, but the perfor-
mance is less impressive for frequencies located between the discrete frequencies.
0
100 150
50
200 250 300 350 400 450 500
0
5
10
15
20
25
30
35
40
f (Hz) 
SB(f)
Exact F0, F1
SB(f)
Figure 4.48: Average Periodogram of Signal in Example 4.21 with N 5 512, 
L 5 128, and M 5 4
4.10.2 Welch’s Method
The broad spike in Figure 4.48, associated with a spectral component that is not aligned 
with one of the discrete frequencies, fi 5 ifsyL, is a manifestation of the leakage phenom-
enon. Basically, the spectral power that should be concentrated at a single frequency   has 
spread out or leaked into adjacent frequencies. To reduce the leakage, Welch (1967) pro-
posed modifying the average periodogram approach in two ways. Rather than partition 
the signal into M distinct subsignals as in Figure 4.46, we instead decompose x(k) into 
a number of overlapping subsignals. If N 5 LM, then a total of 2M 2 1 subsignals of 
Leakage
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 4.10 Power Density Spectrum Estimation    317
length L can be constructed using an overlap of Ly2 as shown previously in Figure 4.41.  
If the subsignals are denoted xm(k) for 0 # m , 2M 2 1, then mth subsignal can be 
extracted from the original signal x(k) as
 
xm(k) 5
D  
x(mLy2 1 k), 0 # k , L 
 (4.10.7)
Recall that the extraction of x0(k) from x(k) can be thought of as multiplication 
of x(k) by the following rectangular window of width L. The other subsignals can be 
extracted in a similar manner with a shifted rectangular window.
 
wR(k) 5 (k) 2 (k 2 L) 
 (4.10.8)
When the DFT of subsignal xm(k) is computed, it can be thought of as finding the 
Fourier coefficients of the periodic extension of xm(k). However, if subsignal xm(k) does 
not represent an integer number of cycles of a given sinusoidal component, then the 
periodic extension of xm(k) will have a jump discontinuity. This discontinuity leads to the 
well known Gibb’s phenomenon of the Fourier series, a phenomenon that causes ringing 
or oscillations near the discontinuity when the signal is reconstructed. This ringing man-
ifests itself in the frequency domain as the leakage observed in the second spectral spike 
in Figure 4.49.
If T is the sampling interval, then the length of subsignal xm(k) is  5 LT. Thus for a 
sinusoidal component at frequency F0, the number of cycles per subsignal is
i0 5 F0
 
 5 LF0
fs
 
 (4.10.9)
Since fi 5 ifsyL, it follows that discrete frequency fi contains exactly i cycles per subsignal. 
For the first spectral component in Example 4.21 at F0 5 200 Hz, the number of cycles per 
subsignal was i0 5 25. However, for the troublesome spectral component at F1 5 331 Hz, 
the number of cycles per subsignal was i1 5 F1 5 41.375. Thus there was a jump discon-
tinuity associated with the second spectral component.
In order to reduce the effects of jump discontinuities in the periodic extensions of 
the subsignals, Welch proposed that the subsignals be multiplied by a window other than 
the rectangular window. If the window goes to zero gradually, rather than abruptly as in 
(4.10.8), then the jump discontinuities in the periodic extensions of the subsignals can be 
eliminated. A number of popular candidates for data window functions were summarized 
in Table 4.10, and plots of these window functions were shown previously in Figure 4.42.  
Given a window w(k) of width L, we can compute the DFT of the mth windowed subse-
quence of x(k). An estimate of the power density spectrum is then obtained by averaging 
the 2M 2 1 overlapping windowed periodograms.
 
SW (i) 5
1
L(2M 2 1) o
2M22
m50
u DFT hw(k)xm(k)ju2, 0 # i , L  
 (4.10.10)
Power density spectrum estimate SW (i) is called a modified average periodogram or 
Welch’s method. The overlap between subsignals could be increased or decreased. How-
ever, the 50% overlap has been shown to improve certain statistical properties of the 
estimate (Welch, 1967). In the case of the Hanning window, the 50% overlap means that 
each sample is equally weighted (see Problem 4.43).
Rectangular window
Welch’s method
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

318    Chapter 4  Fourier Transforms and Spectral Analysis
Welch’s Method: noisy Periodic input
EXAMPLE 4.22
As an illustration of Welch’s method, consider a noise corrupted version of the 
signal from Example 4.21. That is, suppose F0 5 200 Hz, F1 5 331 Hz, and v(k) is 
white noise uniformly distributed over the interval f23, 3g.
x(k) 5  sin(2F0kT ) 2 Ï2 cos(2F1kT ) 1 v(k), 0 # k , N
Suppose N 5 1024, and we factor N as L 5 256, M 5 NyL 5 4. Thus Welch’s 
method uses 2M 2 1 5 7 subsignals, each of length L 5 256. From (4.10.5), the 
frequency precision is
Df 5 fs
L
5 4
 Hz 
In this case the discrete frequency indices of F0 and F1 are
i0 5 F0
Df 5 50
i1 5 F1
Df 5 82.75
An estimate of the power density spectrum of x(k) using Welch’s method with 
the Hanning window can be obtained by running exam4_22. The resulting plot 
of SW( f ) is shown in Figure 4.49. Here we have used the independent variable 
f 5 ifsyL to facilitate interpretation of the frequencies. Note that there are two 
spikes indicating the presence of two sinusoidal components. In addition, there is 
power uniformly distributed over all frequencies which represents the white noise 
Figure 4.49: Modified Average Periodogram of Noisy Periodic Signal 
0
100 150
50
200 250 300 350 400 450 500
0
20
40
60
80
100
120
f (Hz)
SW(f)
Exact F0, F1
SW(f)
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 4.10 Power Density Spectrum Estimation    319
term v(k). The average power of the white noise is Pv 5 3, which is indicated by 
the horizontal line. The two peaks occur at
f50 5 50Df 5 200  Hz 
f83 5 83Df 5 332  Hz 
In this case, there is improvement in the estimate of F1 in comparison with Example 
4.21, but only because L has been doubled, which improves the frequency precision 
from 8 Hz to 4 Hz. The important difference between Figure 4.49 and Figure 4.48 
lies in the observation that the width of the second peak is now comparable to the 
width of the first peak. This reduction in width is a result of the windowing, which 
reduces the leakage into nearby frequencies. The peaks in Figure 4.49 are actually 
somewhat wider than the frequency precision of 64 Hz. This is the price that is 
paid for the leakage reduction through windowing. The spectral characteristics of 
data windows are investigated in more detail in Chapter 6 where they are used to 
improve the performance of digital FIR filters.
The DSP Companion contains the following function for estimating the power den-
sity spectrum using both Bartlett’s method and Welch’s method.
% F_PDS: Compute estimated power density spectrum
%
% Usage:
% 
[S,f,Px] = f_pds (x,N,L,fs,win,meth);
% Pre::
% 
x 
= vector of length n containing input samples
% 
N 
= total number of samples. If N > n, then x is  
% 
 
 padded
% 
 
 with N - n zeros.
% 
L 
= length of subsequence to use. L must be an integer  
% 
 
 factor
% 
 
 of N. That is N = LM for a pair of integers L and M.
% 
fs = sampling frequency
% 
win = window type to be used
%
% 
0 = rectangular
% 
1 = Hanning
% 
2 = Hamming
% 
3 = Blackman
%
% 
meth = an integer method selector.
%
% 
0 = Bartlett’s average periodogram
% 
1 = Welch’s modified average periodogram
%
% Post:
% 
S = 1 by L vector containing estimate of power density  
% 
 
 spectrum
% 
f = 1 by L vector containing frequencies at which S is
% 
 
 evaluated (0 to (L-1)fs/L).
% 
Px = average power of x
DSP Companion
DSP Companion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

320    Chapter 4  Fourier Transforms and Spectral Analysis
If the MATLAB signal processing toolbox is available, the object spectrum and the 
method psd can be used to compute the power density spectrum.
gUi Modules and Case Studies 
This section focuses on applications of Fourier transforms and the spectral analysis of 
discrete-time signals. A graphical user interface module called g_spectra is introduced 
that allows user to interactively explore the spectral characteristics of discrete-time sig-
nals without any need for programming. Case study programming examples using the 
DSP Companion functions are then presented.
g_spectra: Spectral Analysis of Discrete-time Signals
The DSP Companion includes a GUI module called g_spectra that allows the user to 
investigate the spectral characteristics of a variety of discrete-time signals. GUI mod-
ule g_spectra features a display screen with tiled windows as shown in Figure 4.50. The 
design features of GUI module g_spectra are summarized in Table 4.11.
The  Block Diagram window in the upper-left corner of the screen contains a color-
coded block diagram of the operation being performed. This module computes the DFT 
of an N-point signal x(k).
 
X(i) 5
 DFT hx(k)j, 0 # i , N 
 (4.11.1)
Below the block diagram are a number of edit boxes containing parameters that can 
be modified by the user. The parameters include the sampling frequency fs, the frequency 
of a cosine input F0, a bound b on zero-mean uniform white noise, and a clipping thresh-
old c. Changes to the parameters are activated with the Enter key. To the right of the edit 
boxes are pushbutton and check box controls. The Play x as sound pushbutton plays the 
signal x(k) on the PC speaker. The dB Display check box control toggles the graphical 
display between a linear scale and a logarithmic dB scale. The Add noise check box adds 
white noise uniformly distributed over f2b, bg to the input x(k). Finally, the Clip check 
box activates clipping of the input x(k) to the interval f2c, cg.
The Type and  View windows in the upper-right corner of the screen allow the user 
to select both the type of input signal x(k) and the viewing mode. The inputs include sev-
eral common signals that can optionally include white noise depending on the status of 
the Add noise check box. There are two inputs that the user can customize. The Record 
4.11
gUi Module
item
Variables 
Block diagram 
x(k), y(k) 
Edit parameters 
fs, F0, C, d 
Input type 
impulse, cosine, damped exponential record x, import
Plot view 
time signal, magnitude spectrum, phase spectrum, power den-
sity spectrum, window, spectrogram
Slider 
number of samples N 
Push buttons 
play x as sound 
Check boxes 
linear/dB scale, add uniform noise f2c, cg, clip to f2d, dg
Menu buttons 
window, export, caliper, print, help, exit 
Import 
x, fs 
Export 
a, b, x, y, fs 
Table 4.11: Features  
of GUI Module 
g_spectra 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.11 GUI Modules and Case Studies     321
Select Type
Select View
Slider Bar
Edit Parameters
0
100
0
0.2
0.4
0.6
0.8
1
200
300
400
500
600
700
800
900
1000
0
100
200
300
400
0
0.5
1
Magnitude Spectrum: Cosine Input
f (Hz)
A(f)
g_spectra
x(k)
DFT
y(k)
Slider Bar
Edit Parameters
0.2
0.4
0.6
0.8
1
x(k)
DFT
y(k)
Figure 4.50: Display Screen for GUI Module g_spectra
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

322    Chapter 4  Fourier Transforms and Spectral Analysis
sound input allows the user to record one second of sound from the PC microphone at 
a sampling rate of fs 5 8192 Hz. Proper recording can be verified by using the Play x as 
sound pushbutton. The Import option prompts for the name of a user-supplied MAT file 
that contains up to 8192 samples of x(k) plus the sampling frequency fs. If no fs is pres-
ent in the MAT file, then the current value for the sampling frequency is used. Near the 
bottom of the Type and View windows is a slider bar that allows the user to control the 
number of samples N. When N is changed, the currently selected input is recomputed.
The viewing options include the time signal x(k), the magnitude spectrum Ax( f ), the 
phase spectrum x( f ), and the estimated power density spectrum SW( f ) where
 
fi 5 ifs
N, 0 # i # Ny2 
 (4.11.2)
The power density spectrum estimate uses Welch’s modified average periodogram method 
with L 5 Ny4. There are also viewing options for displaying the data window and the 
spectrogram of x using overlapping subsignals of length L 5 Ny8. The Plot window on 
the bottom half of the screen shows the selected view. The curves are color-coded to 
match the block diagram in the upper left corner of the screen. The magnitude spectrum 
and power density spectrum plots can be displayed using either the linear scale or the 
logarithmic dB scale depending on the status of the dB Display check box.
The Menu bar at the top of the screen includes several options. The Window option 
allows the user to choose the data window to be used for the power density spectrum estimate 
and the spectrogram. The Export option is used to save a, b, x, y, fs to a MAT file. Here a and 
b are the coefficient vectors of a running average filter. They are included for compatibility 
with other GUI modules so results exported from one GUI module can be imported into 
other GUI modules for additional processing. The  Caliper option allows the user to measure 
any point on the current plot by moving the mouse crosshairs to that point and clicking. The  
Print option sends the GUI window to a printer or a file. Finally, the Help option provides the 
user with some helpful suggestions on how to effectively use module  g_spectra.
Signal Detection
One of the application areas of spectral analysis is the detection of signals buried in noise. 
Suppose the sampling rate is fs, and the signal to be analyzed includes M sinusoidal com-
ponents with frequencies F1, F2, Á , FM where M and 0 # Fi , fsy2 are unknown and 
must be determined from a spectral analysis of the following signal.
x(k) 5 o
M
i51
 sin(2FikT ) 1 v(k), 0 # k , N
Here v(k) is additive white noise uniformly distributed over the interval f2b, bg. For 
example, v(k) might represent measurement noise or noise picked up in a communication 
channel over which x(k) is transmitted.
The unknown frequencies can be detected and identified by running case4_1, which 
uses N 5 1024, fs 5 2000 Hz, and b 5 2.
The first part of the  case4_1 prompts the user for a seed for the random number gen-
erator and for the number of unknown frequencies, M. Each seed generates a different 
set of M random frequencies. A plot of the first Ny8 samples of the signal x(k) generated 
using the default responses is shown in Figure 4.51. Note that because of the additive 
white noise, it is not clear from direct inspection of x(k) whether it has any sinusoidal 
components, much less how many and their locations on the frequency spectrum. How-
ever, the existence of sinusoidal spectral components is apparent when we examine the 
power density spectrum plot, Sx( f ), shown in Figure 4.52.
Case Study 4.1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.11 GUI Modules and Case Studies     323
In this case there are three sinusoidal components corresponding to the three distinct 
peaks. To determine the locations of these peaks the user is prompted for a threshold 
value, s, so that all f  for which Sx( f ) . s can be displayed. Using the default threshold 
value, the three unknown frequencies are identified to be
F1 5 422  Hz 
F2 5 539  Hz 
F3 5 920  Hz 
Figure 4.51:  
A Noise-
corrupted Signal 
with Unknown 
Sinusoidal 
Components, 
N 5 1024 and 
fs 5 2000 Hz 
0
0.01
0.02
0.03
0.04
0.05
0.06
25
24
23
22
21
0
1
2
3
4
5
t (sec)
x(t)
0
100 200 300 400 500 600 700 800 900 1000
0
50
100
150
200
250
f (Hz)
Sx(f)
Figure 4.52: Power 
Density Spectrum 
of the Signal in 
Figure 4.51 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

324    Chapter 4  Fourier Transforms and Spectral Analysis
Distortion Due to Clipping
Many a parent has had the experience of having a child turn the music up so loud that 
the sound begins to distort. The distortion is caused by the fact that the amplifier or the 
speakers are being overdriven to the point that they are no longer operating in their linear 
range. This type of distortion can be modeled with a saturation nonlinearity where the 
output is clipped at lower and upper limits.
y 5
 clip (x, a, b) 5
D  5
a, 2` , x , a
x,
a # x # b
b,
b , x , `
When the input x is in the interval fa, bg, there is a simple linear relationship, y 5 x. Out-
side this range, y saturates to a for x , a or b for x . b. A graph of a clipped cosine for 
the case fa, bg 5 f2.7, .7g is shown in Figure 4.53.
In a sound system, clipping typically occurs because the magnitude of the ampli-
fied output signal exceeds the DC power supply levels of the amplifier. To illustrate 
this phenomenon, consider a cosine input of unit amplitude and frequency F0 5 625 
Hz. Using a sampling frequency of fs 5 20 kHz, and N 5 32 points yields one period  
of x(k).
x(k) 5  cos(1300kT ), 0 # k , N
Suppose this signal is sent through a saturation nonlinearity with a clipping interval, f2c, cg,
y(k) 5
 clip fx(k), 2c, cg, 0 # k , N
A plot of the resulting input and output for the case c 5 0.7 is shown in Figure 4.53. 
We can quantify the amount of distortion caused by clipping by computing the total 
harmonic distortion, THD. Output y(k) is a sampled version of an underlying periodic 
Case Study 4.2
Figure 4.53:  
Saturation of x 
Due to Clipping 
to the Interval 
f2.7, .7g 
0
5
10
15
20
25
30
21.5
21
20.5
0
0.5
1
1.5
k
y(k)
Cosine
Clipped Cosine
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.11 GUI Modules and Case Studies     325
signal ya(t) that can be approximated by the following truncated Fourier series.
ya(t) 5 d0
2 1 o
Ny221
i51
di cos (2F0t 1 i)
Recall from (4.1.12) that the Fourier series coefficients can be obtained directly from the 
DFT of y(k). In particular, if Ay(i) is the magnitude spectrum of y(k), then
di 5
2Ay(i)
N
, 0 # i , Ny2
Each term of the Fourier series has an average power associated with it. From (4.1.10a), 
the total average power of ya(t) is
Py 5 d 2
0
4 1 1
2 o
Ny2
i51
d 2
i
The total harmonic distortion, THD, is the average power of the unwanted harmonics 
expressed as a percentage of the total average power. Thus from (4.1.10b) we have
THD 5
100(Py 2 d 2
1y2)
Py
 %
The total harmonic distortion can be obtained by running case4_2.
When  case_2 is run, it generates the clipping plot in Figure 4.53 and the magnitude 
spectrum plot shown in Figure 4.54. Note the presence of power at the third, fifth, and 
other odd harmonics due to the clipping operation. Using a clipping threshold of c 5 0.7, 
the total harmonic distortion in this case is found to be
THD 5 1.93%
For audio signals the THD of a pure tone is one measure of the quality of the sound.
Figure 4.54:  
Magnitude 
Spectrum of 
Clipped Cosine, 
THD 5 1.93%
0
2
4
6
8
10
12
14
16
0
2
4
6
8
10
12
14
i
A(i)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

326    Chapter 4  Fourier Transforms and Spectral Analysis
Chapter Summary 
Learning outcomes
This chapter was designed to provide the student with an opportunity to achieve the 
learning outcomes summarized in Table 4.12.
Discrete-time Fourier Transform (DTFT)
Chapter 4 focused on Fourier transforms and the spectral analysis of discrete-time sig-
nals. For absolutely summable signals, the region of convergence of the Z-transform 
includes the unit circle. The discrete-time Fourier transform is obtained by evaluating the 
Z-transform X(z) along the unit circle using z 5  exp( j2f T ). This produces a mapping 
from discrete time to continuous frequency called the DTFT.
 
X( f ) 5 o
`
k52`
x(k) exp( j2kf T ), u f u # fsy2 
 (4.12.1)
The function of X( f ) is called the spectrum of x(k). The magnitude Ax( f ) 5 uX( f )u is 
the magnitude spectrum, and the phase angle x( f ) 5 /X( f ) is the phase spectrum. 
The spectrum X( f ) is periodic with period fs. For real signals, the spectrum satisfies the 
symmetry condition
 
X(2f  ) 5 X *( f ) 
 (4.12.2)
This implies that real signals have an even magnitude spectrum and an odd phase 
spectrum. For real signals, all of the essential information is contained in the range 
0 # f # fsy2 that corresponds to the positive frequencies along the top half of the unit 
circle. The DTFT inherits properties from the Z-transform, and it has additional proper-
ties including Parseval’s identity.
4.12
DTFT
Spectrum
num.  
Learning outcome  
Sec. 
1 
Know how to compute the spectra of discrete-time signals using the 
discrete-time Fourier transform (DTFT) 
4.2  
2 
Know how to use the discrete Fourier transform (DFT) to find the magni-
tude, phase, and power density spectra of finite signals 
4.3  
3 
Know how to apply and use the properties of DFT 
4.3–4.4 
4 
Know how to compute the fast Fourier transform (FFT) using  
decimation in time 
4.5  
5 
Be able to compare the computational complexity of the DFT  
and the FFT in terms of the number of FLOPs 
4.5  
6 
Understand how to characterize white noise, and why this  
random signal is useful for signal modeling and system testing 
4.6  
7 
Know how to compute the frequency response of a stable linear discrete-
time system using the DFT 
4.7  
8 
Understand how zero padding can be used to interpolate between dis-
crete frequencies and improve the frequency precision 
4.8  
9 
Understand what a spectrogram is and how it is used to characterize 
signals whose spectral characteristics change with time 
4.10 
10 
Understand how to estimate the power density spectrum of a signal 
using Bartlett’s and Welch’s methods 
4.9
11 
Know how to use GUI module g_spectra to perform spectral analysis of 
discrete-time signals 
4.11
Table 4.12:  
Learning Outcomes  
for Chapter 4 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.12 Chapter Summary     327
Discrete Fourier Transform (DFT)
When the DTFT is applied to an N-point signal, and X( f ) is evaluated at N discrete 
frequencies equally spaced around the unit circle, the resulting transform is called the  
discrete-Fourier transform or DFT. Let WN denote the Nth root of unity.
 
WN 5  exp(2j2yN) 
 (4.12.3)
Then the DFT, which is denoted X(i) 5
 DFT hx(k)j, is defined in terms of WN as
 
X(i) 5 o
N21
k50
x(k)W ik
N, 0 # i , N 
 (4.12.4)
Since WN is complex, X(i) will be complex. We can express the DFT in polar form as 
X(i) 5 Ax(i) expf jx(i)g where
 
Ax(i) 5 uX(i)u  
 (4.12.5a)
 
x(i) 5 /X(i) 
 (4.12.5b)
The magnitude Ax(i) is called the magnitude spectrum of x(k), and the phase angle x(i) 
is called the phase spectrum of x(k). The power density spectrum of x(k) is defined
 
Sx(i) 5  uX(i)u2
N
 
 (4.12.6)
Here Sx(i) can be interpreted as the average power of the ith harmonic of the periodic 
extension of x(k). For a signal x(k) of length N, the DFT is periodic with period N. In 
addition, if x(k) is real, the spectrum satisfies a midpoint symmetry condition
 
X *(i) 5 X(N 2 i) 
 (4.12.7)
The magnitude and power density spectra exhibit even symmetry about the midpoint 
i 5 Ny2, and the phase spectrum exhibits odd symmetry about the midpoint. Consequently, 
for real signals, all of the essential information is contained in the range 0 # i # Ny2 that 
corresponds to the positive discrete frequencies also called bin frequencies.
 
fi 5 ifs
N, 0 # i # N
2  
 (4.12.8)
Here fs 5 1yT is the sampling frequency, and the highest discrete frequency, fNy2, is the 
folding frequency fd 5 fsy2.
Just as light can be decomposed into different colors, signals have power that is dis-
tributed over different frequencies. If xp(k) denotes the periodic extension of x(k), then 
the average power of xp(k) at discrete frequency fi is given by Pi 5 Sx(i). The total average 
power of x(k) is the average of the power density spectrum.
 
Px 5 1
No
N21
i50
Sx(i) 
 (4.12.9)
White noise v(k) with average power Pv is a random signal whose power density 
spectrum is flat and equal to Pv. From the Wiener-Khintchine theorem, the DFT of the 
circular auto-correlation of a signal is equal to the power density spectrum of the signal. 
The auto-correlation of white noise is
 
cvv(k) 5 Pv(k) 
 (4.12.10)
DFT
Power density 
spectrum
Bin frequencies
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

328    Chapter 4  Fourier Transforms and Spectral Analysis
Fast Fourier Transform (FFT)
When the number of samples N is an integer power of two, a highly efficient implemen-
tation of the DFT called the radix two fast Fourier transform or FFT is available. For 
large values of N, the number of complex floating point operations or FLOPs required 
to perform a DFT is approximately N2, while the number of complex FLOPs required 
for an FFT is only (Ny2) log2(N). Thus the FFT is P times faster than the DFT where
 
P 5
N
2 log2(N) 
 (4.12.11)
For N 5 1024 the FFT is about 200 times faster, and for N 5 8192 it is more than 1200 
times faster than the DFT.
Spectral Analysis of Signals
The frequency response of a stable linear discrete-time system with transfer function H(z) 
can be approximated at discrete frequencies fi 5 ifsyN using H(i) 5
 DFT hh(k)j where 
h(k) is the truncated impulse response. For an IIR system the approximation becomes 
increasingly accurate as the number of samples N increases, and for an FIR system of 
order m, the approximation is exact for N . m.
The spacing between discrete frequencies, Df 5 fsyN, is called the frequency precision. 
The frequency precision of an N-point signal x(k) can be improved by appending zeros to 
the end of x(k). If M 2 N zeros are added, this results in an M-point zero-padded signal, 
xz(k), whose spectrum is the same as x(k), but with a finer frequency precision of
 
Df 5 fs
M 
 (4.12.12)
Using zero padding, isolated sinusoidal spectral components of x(k) can be more 
accurately detected and located. Power density spectrum peaks of closely spaced sinusoids 
tend to merge into one broad peak due to spectral leakage. The smallest frequency differ-
ence that can be reliably detected is called the frequency resolution. The frequency resolu-
tion, DF, is the reciprocal of the duration of x(k) and is referred to as the Rayleigh limit.
 
DF 5 fs
N 
 (4.12.13)
The Spectrogram
Some practical signals, such as speech and music, are sufficiently long that their spectral 
characteristics can be thought of as changing with time. A long signal x(k) can be parti-
tioned into 2M 2 1 overlapping subsignals of length L and these subsignals can be win-
dowed and then transformed with L-point DFTs. When the resulting magnitude spectra 
are arranged as the rows of a (2M 2 1) 3 L matrix, it is called the spectrogram of x(k).
 
G(m, i) 5 u DFT hw(k)x(mLy2 1 k)ju 
 (4.12.14)
The spectrogram shows how the spectrum changes with time. The first independent vari-
able m specifies the starting time in increments of Ly2 samples, while the second inde-
pendent variable i specifies the discrete frequency in increments of fsyL. In choosing L 
and M, there is a tradeoff in the time precision Dt and the frequency precision Df  of the 
spectrogram that is governed by the constant product
 
DtDf 5  1
2 
 (4.12.15)
FLOPs
FFT
Frequency precision
zero-padded
Rayleigh limit
Spectrogram
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.13 Problems    329
*Power Density Spectrum Estimation
A number of techniques have been proposed for obtaining improved estimates of the 
underlying continuous-time power density spectrum of a signal. The basic definition of 
Sx(i) in (4.12.6) is called a periodogram. Improved estimates with a smaller variance can 
be obtained using Bartlett’s average periodogram method, and Welch’s modified average 
periodogram method. Average periodograms are computed by partitioning a long signal 
x(k) into subsignals of length L. For Bartlett’s method, the subsignals do not overlap, 
while for Welch’s method they overlap by Ly2 samples. Welch’s method also multiplies 
the subsignals by data windows that taper gradually to zero at each end. The use of data 
windows tends to reduce the effects of the spectral leakage, a computational artifact that 
leads to overly broad spikes in the estimated power density spectrum.
gUi Module
The DSP Companion includes a GUI module called g_spectra that allows the user to 
perform spectral analysis of discrete-time signals without any need for programming.
Problems
The problems are divided into Analysis and Design problems that can be solved by 
hand or with a calculator, GUI Simulation problems that are solved using GUI modules  
g_correlate and g_spectra, and MATLAB Computation problems that require a user pro-
gram. Solutions to selected problems can be accessed with the DSP Companion driver 
program, g_dsp. Students are encouraged to use these problems, which are identified with 
a 
, as a check on their understanding of the material.
4.13.1 Analysis and Design
Section 4.2: Discrete-time Fourier Transform (DTFT)
4.1 
Find the DTFT of the following signals where ucu , 1.
(a) x(k) 5 ck cos(2F0kT )(k) 
(b) x(k) 5 ck sin(2F0kT )(k) 
4.2 
Consider the following signal where ucu , 1.
x(k) 5 k2ck(k)
(a) Using Appendix 1, find the spectrum X( f ). 
(b) Find the magnitude spectrum, Ax( f ). 
(c) Find the phase spectrum, x( f ). 
4.3 
Consider the following causal finite signal with x(0) 5 1.
x(k) 5 f1, 2, 1gT
(a) Find the spectrum X( f ). 
(b) Find the magnitude spectrum, Ax( f ). 
(c) Find the phase spectrum, x( f ). 
Optional material
Average periodogram
Data windows
Spectral leakage
4.13
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

330    Chapter 4  Fourier Transforms and Spectral Analysis
4.4 
Let xa(t) be periodic with period T0, and let x(k) be a sampled version of xa(t) using 
sampling interval T.
(a) For what values of T is x(k) periodic? Provide an example. 
(b) For what values of T is x(k) not periodic? Provide an example.
4.5 
If one allows for the possibility that X( f ) can contain impulses of the form a( f ), 
then the table of DTFT pairs can be expanded. Using the inverse DTFT of an 
impulse, find the DTFT of x(k) where c is an arbitrary constant.
x(k) 5 c
4.6 
Using Euler’s identity, find the inverse DTFT of the following signals.
(a) X1( f ) 5 a(f 2 F0) 1 a(f 1 F0)
2
 
(b) X2( f ) 5 a(f 2 F0) 2 (f 1 F0)
j2
 
4.7 
Consider the following discrete-time signal.
x(k) 5 c cos(2F0kT 1 )
(a) Find a and b such that x(k) 5 a cos(2F0kT ) 1 b sin(2F0kT ). 
(b) Use part (a) and Problem 4.6 to find X( f ). 
4.8 
Suppose a signal x(k) has the following magnitude spectrum.
Ax( f ) 5  cos(f T ), 0 # u f u # fsy2
(a) Find the energy density spectrum, Sx( f ). 
(b) Find the total energy, Ex. 
(c) Find the energy in the frequency range 0 # u f u # fs where 0 #  # .5. 
4.9 
Show that the DTFT satisfies the following property called the frequency differen-
tiation property.
 DTFT hkTx(k)j 51
j
22
dX( f )
df
4.10 Recall from Problem 3.30 that the Z-transform satisfies the following modulation 
property.
Zhh(k)x(k)j 5 1
j2 $
C
H(u)X 1
z
u2u21du
Use this result and the relationship between the Z-transform and the DTFT to 
show an equivalent modulation property of the DTFT. Here multiplication in the 
time domain maps into convolution in the frequency domain.
 
 DTFT hh(k)x(k)j 5 1
fs#
fsy2
2fsy2
H()X(f 2 )d 
 (4.13.1)
Section 4.3: Discrete Fourier Transform (DFT)
4.11 The following scalar, c, is real. Find its value.  Hint: Use Euler’s identity.
c 5 j j
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.13 Problems    331
4.12 Consider the following discrete-time signal.
x 5 f2, 21, 3gT
(a) Find the third root of unity, W3.
(b) Find the 3 3 3 DFT transformation matrix W.
(c) Use W to find the DFT of x.
(d) Find the inverse DFT transformation matrix W 21.
(e) Find the discrete-time signal x whose DFT is given by
X 5 f3, 2j, jgT
4.13 Verify the following values of W k
N 5 exp(2j2kyN ) appearing in Table 4.6.
W k
N 55
2j, k 5 Ny4
21, k 5 Ny2
j,
k 5 3Ny4
1,
k 5 N
4.14 Using the results of Problem 4.13, verify the following properties of 
WN 5  exp(2j2yN) appearing in Table 4.6.
(a) W (i 1 N)k
N
5 W ik
N
(b) W i 1 Ny2
N
5 2W i
N
(c) W 2i
N 5 W i
Ny2
(d) W *N 5 W 21
N
4.15 The following orthogonal property of WN was used to derive the IDFT.
o
N21
i50
W ik
N 5 N(k), 0 # k , N
 
The finite geometric series in Problem 3.9c is valid for any complex z. Use this to 
verify the orthogonality property of WN.
4.16 Complete the following DFT pairs for N-point signals.
(a) If x(k) 5 (k), find X(i).
(b) If X(i) 5 (i), find x(k).
4.17 Consider the following discrete-time signal.
x 5 f1, 2, 1, 0gT
(a) Find X(i) 5
 DFT hx(k)j.
(b) Compute the magnitude spectrum Ax(i).
(c) Compute the phase spectrum x(i).
(d) Compute the power density spectrum Sx(i).
4.18 Let x(k) be an N-point signal. Starting with the definition of average power in 
(4.3.40), use Parseval’s identity to show that the average power is the average of the 
power density spectrum.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

332    Chapter 4  Fourier Transforms and Spectral Analysis
4.19 Consider the following discrete-time signal.
x 5 f21, 2, 2, 1gT
(a) Find the average power Px.
(b) Find the DFT of x.
(c) Verify Parseval’s identity this case.
4.20 Consider the following discrete-time signal where ucu , 1.
x(k) 5 ck, 0 # k , N
(a) Find X(i).
(b) Use the geometric series to simplify X(i) as much as possible.
4.21 Suppose x(k) is a real N-point signal. Show that the spectrum of x(k) satisfies the 
following symmetry properties.
(a) 
 Re hX(i)j 5
 Re hX(N 2 i)j 
(b) 
 Im hX(i)j 5 2 Im hX(N 2 i)j
4.22 Suppose x(k) is a real with X(i) 5
 DFT hx(k)j.
(a) Show that X(0) is real.
(b) Show that when N is even, X(Ny2) is real.
Section 4.4: Fast Fourier Transform (FFT)
4.23 Consider an N-point signal x(k). Find the smallest integer N such that a radix-two 
FFT of x(k) is at least 100 times as fast as the DFT of x(k) when speed is measured 
in complex FLOPs.
4.24 Recall that the DFT of an N-point signal is periodic with period N. One of the 
properties of the DFT is the conjugate property
 DFT hx*(k)j 5 X *(2i)
 
This property can be used to compute two real DFTs of length N using a single 
complex DFT of length N. Let a(k) and b(k) be real and consider the complex 
signal
c(k) 5 a(k) 1 jb(k), 0 # k , N
 
Using the identities in Appendix 2 and the conjugate property, show that
A(i) 5 C(i) 1 C*(2i)
2
B(i) 5 C(i) 2 C*(2i)
j2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.13 Problems    333
Section 4.5: Fast Convolution and Correlation
4.25 Suppose h(k) and x(k) are both of length L 5 2048.
(a) Find the number of real FLOPs for a fast linear convolution of h(k) with x(k).
(b) Find the number of real FLOPs for a direct linear convolution of h(k) with x(k).
(c) Express the answer to (a) as a percentage of the answer to (b).
4.26 Suppose h(k) is of length L, and x(k) is of length M. Let L and M be powers of two 
with M $ L.
(a) Find the number of real FLOPs for a fast linear convolution of h(k) with x(k). 
Does your answer agree with (4.5.8) when M 5 L?
(b) Find the number of real FLOPs for a direct linear convolution of h(k) with 
x(k). Does your answer agree with (4.5.9) when M 5 L?
4.27 Suppose L is a power of two and M 5 QL for some positive integer Q. Let nblock 
be the number of real FLOPs needed to compute a fast block convolution of an  
L-point signal h(k) with an M-point signal x(k). Find nblock.
4.28 Use the DFT to solve the following.
(a) Recover x(k) from cyx(k) and y(k).
(b) Recover y(k) from cyx(k) and x(k).
4.29 Suppose x(k) and y(k) are both of length L 5 4096.
(a) Find the number of real FLOPs for a fast linear cross-correlation of y(k) with 
x(k).
(b) Find the number of real FLOPs for a direct linear cross-correlation of y(k) 
with x(k).
(c) Express the answer to (a) as a percentage of the answer to (b).
4.30 Suppose y(k) is of length L and x(k) is of length M # L.
(a) Find the number of real FLOPs for a fast linear cross-correlation of y(k) with 
x(k). Does your answer agree with (4.5.20) when M 5 L?
(b) Find the number of real FLOPs for a direct linear cross-correlation of y(k) 
with x(k). Does your answer agree with (4.5.21) when M 5 L?
Section 4.6: White noise
4.31 Let v(k) be an N-point white noise signal with mean v and variance  2
v. Show that 
the average power, the mean, and the variance are related as follows.
Pv 5 2
v 1 2
v
4.32 Let v(k) be an N-point white noise signal with mean v and variance  2
v. Show that 
the circular auto-correlation of v(k) is
cvv(k) < 2
v 1  2
v(k)
4.33 Let v(k) be an N-point white noise signal with mean v and variance  2
v. Using the 
results of Problem 4.32, show that the power density spectrum of v(k) is
Sv(i) <  2
v 1 N2
v(i)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

334    Chapter 4  Fourier Transforms and Spectral Analysis
4.34 Let v be a random variable that is uniformly distributed over the interval fa, bg.
(a) Find the mth statistical moment, E fv mg, for m $ 0.
(b) Verify that E fvmg 5 Pv in (4.6.6) when m 5 2.
4.35 Let x be a random variable whose probability density function is shown in Figure 4.55.
(a) What is the probability that 2.5 # x # .5?
(b) Find E fx2g.
Section 4.7: Auto-correlation
4.36 Consider the following discrete-time signal.
x 5 f10, 25, 20, 0, 15gT
(a) Using (2.8.2), find a linear auto-correlation matrix D(x) such that rxx 5 D(x)x.
(b) Use D(x) to find the linear auto-correlation rxx(k).
(c) Using Definition 2.5, find the normalized linear auto-correlation xx(k).
(d) Find the average power Px.
4.37 Consider the following discrete-time signal.
x 5 f12, 4, 28, 16gT
(a) Starting with (2.8.2), but replacing x with xp, find the circular auto-correlation 
matrix E(x) such that cxx 5 E(x)x.
(b) Use E(x) to find the circular auto-correlation cxx(k).
(c) Find the normalized circular auto-correlation xx(k).
4.38 A white noise signal v(k) is uniformly distributed over the interval f2a, ag. Suppose 
v(k) has the following circular auto-correlation.
cvv(k) 5 8(k), 0 # k , 1024
(a) Find the interval radius a.
(b) Sketch the power density spectrum of v(k).
Figure 4.55:  
Probability Density 
Function for  
Problem 4.35 
22
21.5
21
20.5
0
0.5
1
1.5
2
20.5
0
0.5
1
1.5
2
x
p(x)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.13 Problems    335
Section 4.8: Zero Padding and Spectral Resolution
4.39 Consider the following digital filter where uau , 1.
H(z) 5
1
1 2 az21
(a) Find the impulse response h(k).
(b) Find the frequency response H( f ).
(c) Let H(i) be the N-point DFT of h(k), and let fi 5 ifsyN. Given an arbitrary 
 . 0, use (4.8.4) to find a lower bound n such that for N $ n,
uH(i) 2 H( fi)u #   for  0 # i , N
4.40 A signal xa(t) is sampled at N 5 300 points using a sampling rate of fs 5 1600 Hz. 
Let xz(k) be a zero-padded version of x(k) using M 2 N zeros. Suppose a radix-two 
FFT is used to find Xz(i).
(a) Find a lower bound on M that ensures that the frequency precision of Xz(i) is 
no larger than 2 Hz.
(b) How much faster or slower is the FFT of xz(k) in comparison with the DFT of 
x(k)? Express your answer as a ratio of the computational effort of the FFT to 
the computational effort of the DFT.
Section 4.9: Spectrogram
4.41 Consider the spectrogram in Definition 4.5. Suppose the data x(k) is real.
(a) Find the number of complex FLOPs needed if the DFT is used. 
(b) Find the number of complex FLOPs needed if the FFT is used. 
4.42 Consider the spectrogram in Definition 4.5.
(a) Modify the spectrogram definition using zero padding so the frequency preci-
sion is improved by a factor of two.
(b) Compute the percent increase in computational effort for the modified spec-
trogram in comparison with the original spectrogram assuming the FFT is 
used. Use complex FLOPs to measure the computational effort and assume 
x(k) is real.
(c) Does the modified spectrogram have improved frequency resolution? If not, 
how can the frequency resolution be improved and what is the tradeoff?
Section 4.10: Power Density Spectrum Estimation
4.43 One of the problems with using data windows to reduce the Gibb’s phenomenon in 
the periodic extension of an N-point signal x(k) is that the samples are no longer 
weighted equally when computing an estimate of the power density spectrum. This 
is particularly the case when no overlap of subsignals is used.
(a) Use the trigonometric identities in Appendix 2 to show that the Hanning win-
dow in Table 4.10 can be expressed as
w(k) 5 .5 1 .5 cos3
2(k 2 Ly2)
L
4, 0 # k , L
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

336    Chapter 4  Fourier Transforms and Spectral Analysis
(b) If a 50% overlap of subsignals is used for the power density spectrum estimate, 
then each overlapped sample gets counted twice, once with weight w(k) and once 
with weight w(k 1 Ly2). Show that if the Hanning window is used, the overlapped 
samples are weighted equally. Find the total weight for each overlapped sample.
(c) Are there any other windows in Table 4.10 for which the total weighting of the 
overlapped samples is uniform when a 50% overlap is used? If so, which ones?
4.13.2 gUi Simulation
Section 4.7: Auto-correlation
4.44 Using the GUI module g_correlate, select the periodic input.
(a) Plot x(k) and y(k).
(b) Plot the normalized circular auto-correlation, yy(k). Notice how the noise has 
been reduced.
(c) Estimate the period of y(k) in seconds by estimating the period of yy.
4.45 Using the GUI module g_correlate, select the white noise input. Set the scale factor 
to c 5 0.
(a) Plot x(k) and y(k). What is the range of values over which the uniform white 
noise is distributed?
(b) Verify that ryy(k) < Py(k) by plotting the auto-correlation of y(k).
(c) Use the Caliper option to estimate Py.
(d) Verify that this estimate of Py is consistent with the theoretical value in (4.6.6).
4.46 Using the GUI module  g_correlate, select the impulse train input. This sets y(k) to 
a periodic input, and x(k) to an impulse train whose period matches the period of 
y(k). Set L 5 4096 and M 5 4096.
(a) Plot the noise-corrupted periodic input y(k) and the periodic impulse train x(k).
(b) Plot the normalized circular auto-correlation of y(k).
(c) Plot the normalized circular cross-correlation yx(k). This should be propor-
tional to y(k) but with the noise reduced.
Section 4.9: Spectrogram
4.47 Use the GUI module  g_spectra to plot the spectrogram of the following signals. 
Use fs 5 3000 Hz and N 5 2048 samples for each.
(a) Cosine of unit amplitude and frequency F0 5 400 Hz
(b) Cosine of unit amplitude and frequency F0 5 400 Hz, clipped to f2.5, .5g
(c) Cosine of unit amplitude and frequency F0 5 400 Hz, plus white noise uni-
formly distributed over f21.5, 1.5g
4.48 Using the GUI module  g_spectra record the word HELLO. Play it back to make 
sure it is recorded properly. Export it to a MAT-file called hello. Then use the 
Import option to load it. Plot the following spectral characteristics.
(a) Magnitude spectrum
(b) Power density spectrum (Hamming window)
(c) Spectrogram
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.13 Problems    337
4.49 Consider the signal shown in Figure 4.56, which contains one or more sinusoidal 
components corrupted with white noise. The complete signal x(k) and the sampling 
frequency fs are stored in the file  prob4_49.mat. Use the GUI module g_spectra 
and Import it. Then plot the following spectral characteristics.
(a) The power density spectrum (Hamming window). Use the Caliper option to 
estimate the frequencies of the sinusoidal components.
(b) The spectrogram (Hamming window).
Section 4.10: Power Density Spectrum Estimation
4.50 Use the GUI module  g_spectra to plot the power density spectrum of a noise-free 
cosine input using the default parameter values. Use the dB scale and do the follow-
ing cases.
(a) Rectangular window
(b) Hanning window
(c) Hamming window
(d) Blackman window
4.51 Use the GUI module g_spectra to plot the following characteristics of a noise- 
corrupted damped exponential input using the default parameter values. Use the 
linear scale. 
(a) Time signal
(b) Magnitude spectrum
(c) Power density spectrum (Blackman window)
(d) Blackman window
0
20
40
60
80
100
120
23
22
21
0
1
2
3
k
x(k)
Figure 4.56:  
Noise-corrupted 
Signal with 
Unknown 
Sinusoidal 
Components 
(Samples 0 to Ny8) 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

338    Chapter 4  Fourier Transforms and Spectral Analysis
4.52 Consider the following noise-corrupted periodic signal with a sampling frequency 
of fs 5 1600 Hz and N 5 1024. Here v(k) is white noise uniformly distributed over 
f21, 1g.
x(k) 5  sin(600kT ) cos2(200kT ) 1 v(k), 0 # k , N
 
Create a MAT-file called  prob4_52 containing x and fs. Then use  g_spectra to 
Import it and plot the following.
(a) Magnitude spectrum
(b) Power density spectrum using Welch’s method (rectangular window)
(c) Power density spectrum using Welch’s method (Blackman window)
4.53 Use the GUI module g_spectra to perform the following analysis of the vowels. 
Play back the sound in each case to make sure you have a good recording.
(a) Record one second of the vowel “A,” export it, and plot the time signal.
(b) Record one second of the vowel “E,” export it, and plot the time signal.
(c) Record one second of the vowel “I,” export it, and plot the time signal.
(d) Record one second of the vowel “O,” export it, and plot the time signal.
(e) Record one second of the vowel “U,” export it, and plot the time signal.
4.54 A signal stored in  prob4_54.mat contains white noise plus a single sinusoidal com-
ponent whose frequency does not correspond to any of the discrete frequencies. 
Use GUI module  g_spectra to plot the following spectral characteristics.
(a) The magnitude spectrum of x(k) using the linear scale.
(b) The power density spectrum of x(k) using the Blackman window. Use the  
Caliper option to estimate the frequency of the sinusoidal component.
4.13.3 MATLAB Computation
Section 4.3: Discrete Fourier Transform: DFT
4.55 Let xa(t) be a periodic pulse train of period T0. Suppose the pulse amplitude is 
a 5 10, and the pulse duration is  5 T0y5 as shown in Figure 4.57 for the case 
T0 5 1. This signal can be represented by the following cosine form Fourier series.
xa(t) 5 d0
2 1 o
`
i51
di cos1
2it
T0
1 i2
 
Write a MATLAB program that uses the DFT to compute coefficients d0 and 
(di, i) for 1 # i , 16. Plot di and i using a 2 3 1 array of plots and the MATLAB 
function stem.
4.56 In addition to saturation due to clipping, another common type of nonlinearity is 
the dead-zone nonlinearity shown in Figure 4.58. The algebraic representation of a 
dead zone of radius a is as follows.
F(x, a) 5
D  5
0, 0 # uxu # a
x , a , uxu , `
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.13 Problems    339
 
Suppose fs 5 2000 Hz, and N 5 100. Consider the following input signal where 
0 # k , N corresponds to one cycle.
x(k) 5  cos(40kT ), 0 # k , N
 
Let the dead-zone radius be a 5 .25. Write a MATLAB program that does the 
following.
(a) Compute and plot y(k) 5 F fx(k), ag versus k.
(b) Compute and plot the magnitude spectrum of y(k).
0
0.4
0.6
0.8
0.2
1
1.4
1.6
1.8
2
0
5
10
15
t
xa(t)
1.2
Figure 4.57:  
Periodic Pulse 
Train with a 5 10 
and T0 5 1
Figure 4.58:  
Dead-zone 
Nonlinearity of 
Radius a 
21
20.6 20.4 20.2
20.8
0
0.2
0.4
0.6
0.8
1
21
20.8
20.6
20.4
20.2
0
0.2
0.4
0.6
0.8
1
x
y
 a
−a
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

340    Chapter 4  Fourier Transforms and Spectral Analysis
(c) Using the DFT, compute and print the total harmonic distortion of y(k) 
caused by the dead zone. Here, if di and i for 0 # i , M are the cosine form 
Fourier coefficients of y(k) with M 5 Ny2, then
THD 5
100(Py 2 d 2
1y2)
Py
 %
4.57 Repeat Problem 4.56, but using fs 5 1000 Hz, N 5 50 samples, and the cubic 
nonlinearity
F(x) 5 x3
Section 4.5: Fast Convolution and Correlation
4.58 Let h(k) and x(k) be two N-point white noise signals uniformly distributed 
over f21, 1g. Recall that the MATLAB function conv can be used to compute 
linear convolution. Write a MATLAB program that uses tic and toc to com-
pute the computational time, tdir, of conv and the computational time, tfast, of 
the DSP Companion function f_conv for the cases N 5 4096, N 5 8192, and 
N 5 16384.
(a) Print the two computational times tdir and tfast for N 5 4096, 8192, and 16384.
(b) Plot tdir versus Ny1024 and ttast versus Ny1024 on the same graph and include 
a legend.
4.59 Consider the following linear discrete-time system. Write a MATLAB program 
that performs the following tasks.
H(z) 5
z
z2 2 1.4z 1 .98
(a) Compute and plot the impulse response h(k) for 0 # k , L 2 1 where L 5 500.
(b) Construct an M-point white noise input x(k) that is distributed uniformly over 
f25, 5g where M 5 10000. Use the DSP Companion function f_blockconv to 
compute the zero-state response y(k) to the input x(k) using block convolu-
tion. Plot y(k) for 9500 # k , 10000.
(c) Print the number of FFTs and the lengths of the FFTs used to perform the 
block convolution.
Section 4.6: White noise
4.60 Consider the following noise-corrupted periodic signal with a sampling frequency 
of fs 5 1600 Hz and N 5 1024.
x(k) 5  sin2(400kT ) cos2(300kT ) 1 v(k), 0 # k , N
 
Here v(k) is zero-mean Gaussian white noise with a standard deviation of  5 1yÏ2. 
Write a program that performs the following tasks.
(a) Compute and plot the power density spectrum Sx( f ) for 0 # f # fsy2.
(b) Compute and print the average power of x(k) and the average power of v(k).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.13 Problems    341
4.61 Write a program that creates a 1 3 2048 vector x of white noise uniformly 
distributed over f2.5, .5g. The program should then compute and display the 
following.
(a) The average power Px, the predicted average power Pv, and the percent error 
in Px.
(b) Plot the estimated power density spectrum using Bartlett’s method with 
L 5 512. Use a y-axis range of f0, 1g. In the plot title, print L and the esti-
mated variance  2
B of the power density spectrum.
(c) Repeat part (b), but use L 5 32.
Section 4.7: Auto-correlation
4.62 Let x(k) be an N-point white noise signal uniformly distributed over f21, 1g where 
N 5 4096. Write a program that performs the following tasks.
(a) Create x(k) and then compute and plot the normalized circular auto- 
correlation, xx(k).
(b) Compute cxx(k) and use the result to compute and plot the power density spec-
trum of x(k).
(c) Compute and print the average power Px.
4.63 Consider the following N-point periodic signal of period M. Suppose M 5 128 
and N 5 1024.
x(k) 5 1 1 3 cos1
2k
M 2 2 2 sin1
4k
M 2, 0 # k , N
 
Let y(k) be a noise-corrupted version of x(k) where v(k) is white noise uniformly 
distributed over f21, 1g.
y(k) 5 x(k) 1 v(k), 0 # k , N
 
The objective of this problem is to study how sensitive the periodic signal extrac-
tion technique in (4.7.21) is to the estimate of the period M.
x⁄
m(k) 51
N
L2cym(k)
 
Write a program which performs the following tasks.
(a) Compute and plot the noise-corrupted periodic signal y(k).
(b) Compute and plot on the same graph x(k) and x⁄
m(k) for m 5 M 2 5 using a 
legend.
(c) Compute and plot on the same graph x(k) and x⁄
m(k) for m 5 M using a legend.
(d) Compute and plot on the same graph x(k) and x⁄
m(k) for m 5 M 1 5 using a 
legend.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

342    Chapter 4  Fourier Transforms and Spectral Analysis
Section 4.8: Zero Padding and Spectral Resolution
4.64 Consider the following digital filter of order m 5 2p where p 5 20.
H(z) 5 o
2p
i50
biz2i
bp 5 .5
bi 5 f.54 2 .46 cos (iyp)gh sinf.75(i 2 p)g2sinf.25(i 2 p)gj
(i 2 p)
, i Þ p
 
Suppose fs 5 200 Hz. Write a program that uses filter to do the following.
(a) Compute and plot the impulse response h(k) for 0 # k , N where N 5 64.
(b) Compute and plot the magnitude response A( f ) for 0 # f # fsy2.
(c) What type of filter is this, FIR or IIR? What range of frequencies gets passed 
by this filter?
4.65 Consider the following digital filter of order n where n 5 11 and r 5 .98.
H(z) 5 (1 1 rn)(1 2 z2n)
2(1 2 rnz2n)
Suppose fs 5 2200 Hz. Write a program that uses  filter to do the following.
(a) Compute and plot the impulse response h(k) for 0 # k , N where N 5 1001.
(b) Compute and plot the magnitude response A( f ) for 0 # f # fsy2.
(c) What type of filter is this, FIR or IIR? Which frequencies get rejected by this 
filter?
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5
Filter Types
and Characteristics
7
6
FIR Filter Design
IIR Filter Design
2
Filter Design
P A R T
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

345
Filter Types and 
Characteristics
C H A P T E R  5
CHAPTER ToPiCs
5.1 
Motivation
5.2 
Frequency-selective Filters 
5.3 
Linear-phase and  
Zero-phase Filters 
5.4 
Minimum-phase and  
Allpass Filters 
5.5 
Quadrature Filters 
5.6 
Notch Filters and 
Resonators
5.7 
Narrowband Filters and 
Filter Banks
5.8 
Adaptive Filters
5.9 
GUI Modules and  
Case Studies
5.10 Chapter Summary
5.11 Problems
Motivation
Part 2 of this book focuses on the design and application of digital 
filters. The objective of this chapter is to provide the reader with an 
overview of the wide variety of digital filters that appear in appli-
cations. While this chapter introduces a variety of filter types, the 
design details for FIR filters, IIR filters, multirate filters, and adap-
tive filters are covered in Chapters 6 through 9, respectively. Certain 
fundamental filter characteristics, such as design specifications and 
phase response characteristics of FIR and IIR filters, are covered in 
this chapter. The different filter types and the chapters where their 
design techniques are covered are summarized in Table 5.1.
To lay a foundation for filter design, it is helpful to first 
consider certain fundamental characteristics that filters have in 
common. One characteristic of frequency-selective filters is that 
they are constructed to meet certain design specifications. For 
example, the design specifications dictate which frequencies or 
spectral components of the input are passed by the filter, which 
are rejected, and the degree to which rejected frequencies are 
blocked by the filter. These characteristics are specified using a 
desired magnitude response A( f ).
H( f ) 5 A( f ) expf j ( f )g
Sometimes the phase response  ( f ) is left unspecified, but in cer-
tain cases the design specifications call for a certain type of phase 
response as well such as a linear-phase response,  ( f ) 5 22f , 
that corresponds to a constant delay , or a zero-phase response, 
 ( f ) 5 0, that only can be realized with a noncausal filter.
5.1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

346    Chapter 5  Filter Types and Characteristics
Table 5.1: Design of 
Digital Filters
Filter Type 
Chapter 
Filter Type
Chapter 
FIR
6
Chebyshev II
7
Linear-phase
6
Elliptic
7
Zero-phase
6
Decimator
8
Equiripple
6
Interpolator
8
Differentiator
6
Rational converter
8
Hilbert transformer
6
Narrowband
8
Quadrature
6
Analysis bank
8
Equalizer
6
Synthesis bank
8
IIR
7
Transmultiplexor
8
Notch
7
Adaptive LMS
9
Resonator
7
Adaptive RLS
9
Comb
7
Adaptive FXLMS
9
Butterworth
7
Adaptive NLMS
9
Chebyshev I
7
 
Each filter can be implemented physically in hardware, or mathematically in soft-
ware, using any one of several filter structures. For example, both FIR and IIR transfer 
functions can be realized with the following cascade structure where the L blocks are 
second-order subsystems.
H(z) 5 b0H1(z)H2(z) Á HL(z)
When infinite-precision arithmetic is used, all the different filter structures are equivalent 
in terms of their input-output characteristics. However, when finite-precision arithmetic 
is used to implement the filter, some of the structures are superior to others in terms of 
their sensitivity to detrimental finite word length effects.
We begin this chapter by introducing examples of filter specifications and structures. 
The frequency-selective filter design problem is then formulated by presenting a set 
of filter design specifications, both linear and logarithmic, for the desired magnitude 
response A( f ). Next the notion of a linear-phase filter is introduced, and four types of 
FIR linear-phase filters are presented. This is followed by a decomposition of a general 
IIR filter into a minimum-phase part whose phase lag is as small as possible and an 
allpass part whose magnitude response is constant. Next a Hilbert transformer filter is 
introduced that produces a sinusoidal output that is delayed by a quarter of a cycle, 
so that the input and output are in phase quadrature. This is followed by a discussion 
of notch filters and resonators, filters that reject or pass isolated frequencies. The use 
of multirate techniques to implement narrowband filters and filter banks for frequency-
division multiplexing is then presented. This is followed by an introduction to adaptive 
transversal filters and their use in solving problems whose characteristics evolve with 
time. Finally, a GUI module called g_filters is introduced that allows the user to construct 
specific FIR and IIR filters from design specifications and examine finite precision 
effects, such as coefficient quantization, without any need for programming. The chapter 
concludes with a case study example and a summary of filter types and characteristics.
5.1.1 Filter Design specifications
Perhaps the most common type of digital filter is a lowpass filter. A digital lowpass filter 
is a filter that removes the higher frequencies but passes the lower frequencies. An exam-
ple of a magnitude response of a digital lowpass filter is shown in Figure 5.1. This is a 
Lowpass filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.1 Motivation    347
0
0
1
1.2
f (Hz)
A(f)
1 2 p
1 1 p
s
Fp
Fs
fs/2
Figure 5.1:  
Magnitude 
Response of 
a Lowpass 
Chebyshev-I  
Filter of Order 
n 5 4
fourth-order Chebyshev-I filter. The design of FIR filters is discussed in Chapter 6, and 
the design of IIR filters, including Chebyshev filters, is discussed in Chapter 7.
Passband
The shaded areas in Figure 5.1 represent the design specifications. For a lowpass filter, 
the shaded region in the upper-left corner represents the filter passband, and the shaded 
region in the lower-right corner represents the filter stopband. Notice that the passband 
has width Fp and radius p. That is, the desired magnitude response must meet, or exceed, 
the following passband specification.
 
uA( f ) 2 1u # p, 0 # f # Fp 
(5.1.1)
Here 0 , Fp , fsy2 is the passband cutoff frequency, and p . 0 is the passband ripple. 
The passband ripple can be made small but must be positive for a physically realizable fil-
ter. It is called a ripple factor because the magnitude response sometimes oscillates within 
the passband as shown in Figure 5.1. However, for some filters such as Butterworth  
filters and Chebyshev-II filters, the magnitude response decreases monotonically within 
the passband. All of the classical IIR filters discussed in Chapter 7 have a passband 
ripple that satisfies A( f ) # 1 as shown in Figure 5.1. This is not the case for FIR filters, 
discussed in Chapter 6. For the IIR filter in Figure 5.1, the passband cutoff frequency is 
Fpyfs 5 .15, and the passband ripple is p 5 .08.
stopband
Similar to the passband, the shaded stopband region in the lower-right corner of Figure 5.1  
has width fsy2 2 Fs and height s. Thus the desired magnitude response must meet, or 
exceed, the following stopband specification.
 
0 # A( f ) # s, Fs # f # fsy2 
(5.1.2)
Passband
Passband specification
Stopband
Stopband specification
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

348    Chapter 5  Filter Types and Characteristics
It is evident from Figure 5.1 that the passband specification is met exactly, whereas the 
stopband specification is exceeded in this case. Here Fp , Fs , fsy2 is the stopband cutoff 
frequency, and s . 0 is the stopband attenuation. Again the stopband attenuation can be 
made small, but must be positive for a physically realizable filter. For the filter in Figure 5.1,  
the stopband cutoff frequency is Fsyfs 5 .25 and the stopband attenuation is s 5 .08.
Transition Band
Notice that there is a significant part of the spectrum that is left unspecified. The frequency 
band, fFp, Fsg, between the passband and the stopband is called the transition band. The 
width of the transition band can be made small but it must be positive for a physically 
realizable filter. Indeed, as the passband ripple, the stopband attenuation, and the transition 
bandwidth all approach zero, the required order of the filter approaches infinity. The 
limiting special case of the filter with p 5 0, s 5 0, and Fs 5 Fp is an ideal lowpass filter.
5.1.2 Filter Realization structures
Each digital filter has a number of alternative realizations depending on which filter struc-
ture is used. Filter realization structures are considered in detail at the end of Chapter 6 
(for FIR filters) and Chapter 7 (for IIR filters). The different filter realizations are equiva-
lent to one another as long as infinite-precision arithmetic is used. To illustrate some filter 
realization structures, consider the fourth-order lowpass filter in Figure 5.1. Using design 
techniques covered in Chapter 7, the transfer function of this Chebyshev-I lowpass filter is
 
H(z) 5 .0095 1 .0379z21 1 .0569z22 1 .0379z23 1 .0095z24
1 2 2.2870z21 1 2.5479z22 2 1.4656z23 1 .3696z24  
(5.1.3)
Direct Form ii
Filter realization structures can be represented graphically using the signal flow graphs 
introduced in Section 3.6. For example, a direct form II realization of H(z) is shown 
in Figure 5.2. Recall that the nodes are summing nodes, and arcs without labels have a 
default gain of unity. Notice that the gains of the branches correspond directly to the 
coefficients of the numerator and denominator polynomials of H(z). This is a character-
istic of direct form realizations that sets them apart from the indirect forms.
Filter realizations also can be represented mathematically using the difference equa-
tions associated with the signal flow graph. The direct form II filter realization shown in 
Figure 5.2 can be implemented with the following pair of difference equations where u(k) 
is an intermediate variable.
u(k) 5 x(k) 1 2.2870x(k 2 1) 2 2.5479x(k 2 2)
1 1.4656x(k 2 3) 2 .3696x(k 2 4)
 
(5.1.4a)
y(k) 5 .0095u(k) 1 .0379u(k 2 1) 1 .0569u(k 2 2)
 
1 .0379u(k 2 3) 1 .0095u(k 2 4)
 
(5.1.4b)
Cascade Form
To develop an alternative to the direct form II realization in Figure 5.2, we first recast the 
transfer function in (5.1.3) in terms of positive powers of z, which yields
 
H(z) 5 .0095z4 1 .0379z3 1 .0569z2 1 .0379z 1 .0095
z4 2 2.2870z3 1 2.5479z2 2 1.4656z 1 .3696  
(5.1.5)
Transition band
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.1 Motivation    349
The .0095 can be factored from the numerator. The resulting numerator polynomial and 
denominator polynomial then can be factored into zeros and poles. Suppose complex- 
conjugate pairs of zeros are grouped together and similarly for complex-conjugate pairs 
of poles. The transfer function then can be written as a product of two second-order 
transfer functions, each with real coefficients. This is called a cascade form realization.
 
H(z) 5 .0095H1(z)H2(z) 
(5.1.6)
There are several possible formulations of the two second-order blocks depending on how 
the zeroes and poles are ordered and grouped together. One such ordering from Chapter 7 is
 
H1(z) 5
1 1 2z21 1 z22
1 2 1.0328z21 1 .7766z22 
(5.1.7)
 
H2(z) 5
1 1 2z21 1 z22
1 2 1.2542z21 1 .4759z22 
(5.1.8)
Each of the second-order blocks can be realized using one of the direct forms. For 
example, a signal flow graph which uses direct form II realizations from Section 3.6 for 
the two blocks is shown in Figure 5.3.
As with the higher-order direct-form realization, the signal flow graph of the cascade 
realization can be implemented with a system of difference equations as follows.
 
u0(k) 5 .0095x(k)
 
(5.1.9a)
u1(k) 5 u0(k) 1 2u0(k 2 1) 2 u0(k 2 2)
 
1 1.0328u1(k 2 1) 2 .7766u1(k 2 2) 
(5.1.9b)
u2(k) 5 u1(k) 1 2u1(k 2 1) 2 u1(k 2 2)
 
1 1.2542u2(k 2 1) 2 .4749u2(k 2 2) 
(5.1.9c)
y(k) 5 u2(k) 
(5.1.9d)
Cascade form 
realization
Figure 5.2:  
Signal Flow Graph 
of a Direct Form 
II Realization of 
the Fourth-order 
Chebyshev-I Filter
x
u
y
.0095
.0569
.0379
.0379
22.5479
1.4656
2.2870
z21
z21
z21
.0095
2.3696
z21
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

350    Chapter 5  Filter Types and Characteristics
Quantization Error
When a filter is implemented in software using MATLAB, double-precision floating point 
arithmetic is used for all calculations. Double precision uses 64 bits, which corresponds to 
about 16 decimal digits for the mantissa or fractional part, and the remaining bits used 
to represent the sign and exponent. For convenience of display, only four decimal places 
are shown in Figures 5.2 and 5.3. In most instances, double-precision arithmetic is a 
good approximation to infinite-precision arithmetic, so no significant finite word length 
effects are apparent. However, if a filter is implemented on specialized DSP hardware, or 
if storage space or speed requirements dictate the need to use single-precision floating 
point arithmetic or integer fixed-point arithmetic, then finite word length effects begin to 
manifest themselves.
To illustrate the detrimental effects that limited precision can have, suppose the 
coefficients of the fourth-order Chebyshev-I lowpass filter in Figure 5.1 are represented 
using N bits. The resulting magnitude responses for three cases are shown in Figure 5.4. 
0
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45 0.5
0
0.2
0.4
0.6
0.8
1
f/fs
A(f)
N 5 6
 N 5 9
N 5 12
Figure 5.4:  
Magnitude 
Responses of 
Fourth-order 
Chebyshev-I 
Lowpass Filter 
Using N Bits 
of Precision to 
Represent the 
Coefficients
Figure 5.3: Signal Flow Graph of a Cascade-form Realization of the Fourth-order 
Chebyshev-I Filter Using Direct Form II Realizations for the Second-order Blocks
x
u1
u2
u0
y
1.0328
.0095
2.7766
2
21
1.2542
2
21
2.4759
z21
z21
z21
z21
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.2 Frequency-selective Filters    351
Comparing Figure 5.4 with Figure 5.1, we see that the case using N 5 12 bits is almost 
correct, but the lower-precision cases, N 5 6 and N 5 9, have magnitude responses that 
differ significantly from the double-precision version shown in Figure 5.1. Interestingly 
enough, if the precision is lowered still further to N 5 4 bits, the coefficient quantization 
error becomes so large that the poles of the filter migrate outside the unit circle at which 
point the implementation becomes unstable! For the unstable case, there is no frequency 
response.
Frequency-selective Filters
A digital filter is a discrete-time system that reshapes the spectrum of the input signal to 
produce desired spectral characteristics in the output signal. Recall from Definition 3.3  
that a stable system with transfer function H(z) has the following frequency response 
where fs is the sampling rate.
 
H( f ) 5
D  H(z)uz5  exp(  j2f T ), 0 # u f u #  fsy2 
(5.2.1)
Thus the frequency response is just the transfer function evaluated along the 
unit circle. The complex-valued function H( f ) can be expressed in polar form as 
H( f ) 5 A( f ) exp f  j ( f )g where A( f ) denotes the magnitude response and  ( f ) denotes 
the phase response of the filter.
 
A( f )  5
D   uH( f )u, 0 # u f u # fsy2 
(5.2.2a)
 
 ( f ) 5
D  /H( f ), 0 # u f u # fsy2 
(5.2.2b)
In Proposition 3.2, it was shown that for a stable system H(z) the steady-state response 
to the sinusoidal input x(k) 5 sin(2F0kT) is
 
y(k) 5 A(F0) sinf2F0kT 1  (F0)g 
(5.2.3)
Thus the magnitude response A(F0) can be interpreted as the gain of the filter at fre-
quency F0. It specifies the amount by which a sinusoidal signal of frequency F0 is scaled 
as it passes through the filter. Similarly, the phase response  (F0) can be interpreted as 
the phase shift of the filter at frequency F0. It specifies the number of radians by which a 
sinusoidal signal of frequency F0 gets advanced as it passes through the filter.
By designing a filter with a specified A( f ) or  ( f ) we can control the spectral 
characteristics of the output signal y(k). Most digital filters are designed to produce a 
desired magnitude response A( f ). However, there are specialized filters, such as allpass 
filters, that are designed to produce a desired phase response. A particularly useful phase 
response is a linear phase response of the form
 
 ( f ) 5 22f  
(5.2.4)
Linear-phase filters have the property that each spectral component gets delayed by the 
same amount, namely  seconds. Consequently, the spectral components of the input 
signal that survive at the filter output are not otherwise distorted. Although it is possible 
to approximate linear-phase filters in the passband with IIR filters (e.g., with Bessel fil-
ters), it turns out that it is much simpler to use FIR filters to design linear-phase filters. 
Linear-phase FIR filters are discussed in detail in Section 5.3.
5.2
Frequency response
Magnitude, phase 
responses
Gain
Phase shift
Linear phase response
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

352    Chapter 5  Filter Types and Characteristics
5.2.1 Linear Design specifications
There are many specialized frequency-selective filters that one can consider. However, 
the most common filters fall into four basic categories: lowpass, highpass, bandpass, and 
bandstop. The magnitude responses of ideal versions of the four basic filter types are 
shown in Figure 5.5.
Note that in each case the upper frequency limit is the folding frequency, fsy2, because this 
is the highest frequency that the digital filter can process. Recall from Chapter 1 that analog 
signals at higher frequencies get aliased back into the range f0, fsy2g during the sampling 
process. The range of frequencies over which A( f ) 5 1 is called the passband, and the range 
of frequencies over which A( f ) 5 0 is called the stopband. One of the advantages of digital 
filters is that the passband gain can be set to a value greater than one, if desired, in which 
case the signal is amplified in the passband. Analog filters also can have passbands with gains 
greater than one, but they must be implemented as active filters, rather than passive filters.
There is a fundamental result that limits what kind of filters can be used to achieve 
the idealized frequency response characteristics in Figure 5.5. It is called the Paley-Wiener  
theorem (1934).
Let H( f ) be the frequency response of a stable causal filter with A( f ) 5 uH( f )u. Then
#
fsy2
2fsy2
u log fA( f )gudf , `
Passband
Stopband
PRoPosiTion
5.1  Paley-Wiener Theorem
Figure 5.5: Ideal 
Magnitude 
Responses of the 
Four Basic Filter 
Types
0
0
1
1.6
Fp
Lowpass
0
0
1
1.6
Fs
Highpass
0
0
1
1.6
Bandpass
0
0
1
1.6
f (Hz)
Bandstop
Bandstop
fs/2
fs/2
fs/2
Fp2
Fp1
Fs1
Fs2
fs/2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.2 Frequency-selective Filters    353
All of the ideal frequency-selective filters in Figure 5.5 have the property that they 
exhibit complete attenuation of the signal (s = 0), over a stopband of nonzero length. 
Since log(0) 5 2`, it follows from the Paley-Wiener theorem that none of the ideal fil-
ters can be causal. That is, the magnitude response of a causal filter can only go to zero at 
isolated frequencies such as the My2 frequencies of the running average filter in Example 
4.16, not over a nonzero range of frequencies. Note that this is consistent with the anal-
ysis of an ideal low pass filter found in Example 4.2. There the impulse response for a 
cutoff frequency of Fc was found to be
 
hlow(k) 5 2FcT   sinc (2kFcT ) 
(5.2.5)
Since hlow(k) Þ 0 for k , 0, this filter is not causal and therefore does not have a real-time 
physical realization. In spite of Proposition 5.1, causal filters can be designed that closely 
approximate ideal frequency response characteristics. For example, the impulse response 
in (5.2.5) can be multiplied by a window of radius m centered at k 5 0 and then delayed 
by m samples to make it causal. This general approach is the basis for one of the FIR 
filter design techniques presented in Chapter 6.
In addition to the constraint on the stopband, a practical filter must also contain 
a transition band separating the passband from the stopband. A more realistic design 
specification for the magnitude response of a lowpass filter is shown in Figure 5.6. There 
are two differences worth noting. First, both the passband and the stopband are specified 
by a range of acceptable values for the desired magnitude response. The parameter p 
is called the passband ripple because the magnitude response often oscillates within the 
passband. For FIR filters A( f ) typically oscillates within f1 2 p, 1 1 pg, but for IIR 
filters it remains within f1 2 p, 1g.
The parameter s is the stopband attenuation. The passband ripple and stopband 
attenuation can be made small, but not zero. The second difference is that there is a 
transition band of width Fs 2 Fp between the passband and the stopband. Again, the width 
of the transition band can be made small (at the expense of the filter order), but not zero.
The desired magnitude response must fall within the shaded area in Figure 5.6. 
Note how the ideal cutoff Fp in Figure 5.5 has been split into two cutoff frequencies, Fp 
Passband ripple
Stopband attenuation
Figure 5.6: Linear 
Magnitude 
Response 
Specifications for  
a Lowpass Filter
1
s
0
0
Passband
Stopband
f (Hz)
A(f)
Fp
Fs
fs/2
1 2 p
1 1 p
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

354    Chapter 5  Filter Types and Characteristics
Figure 5.7: Linear 
Magnitude 
Response 
Specifications for  
a Highpass Filter
00
Stopband
Passband
1
s
f (Hz)
A(f)
Fs
Fp
fs/2
1 2 p
1 1 p
Figure 5.8: Linear 
Magnitude 
Response 
Specifications for  
a Bandpass Filter
1 2 p
1 1 p
1
s
f (Hz)
A(f)
Fp2
Fs2
Fs1
Fp1
fs/2
0
0
Stopband
Stopband
Passband
and Fs, to create a transition band in Figure 5.6. A practical design specification for the 
magnitude response of a highpass filter is shown in Figure 5.7. Again, a transition band 
has been created by splitting the ideal cutoff frequency Fs into two cutoff frequencies,  
Fs and Fp.
The design specification for the magnitude response of a bandpass filter is somewhat 
more involved because there are two transition bands bracketing the passband, as can 
be seen in Figure 5.8. Thus there are four cutoff frequencies plus a passband ripple p 
and a stopband attenuation s. The design specification for the magnitude response of a 
bandstop filter also has two transition bands, this time bracketing the stopband, as can 
be seen in Figure 5.9.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.2 Frequency-selective Filters    355
Figure 5.9: Linear 
Magnitude 
Response 
Specifications for  
a Bandstop Filter
1 2 p
1 1 p
1
s
f (Hz)
A(f)
Fs2
Fp2
Fp1
Fs1
fs/2
00
Passband
Passband
Stopband
Linear Design specifications
EXAMPLE 5.1
As a simple illustration of design specifications for a desired magnitude response, 
consider the following first-order IIR filter.
H(z) 5 .5(1 2 c)(1 1 z21)
1 2 cz21
Recasting H(z) in terms of positive powers of z we have
H(z) 5 .5(1 2 c)(z 1 1)
z 2 c
Thus H(z) has a zero at z 5 21 and a pole at z 5 c. For the filter to be stable it is 
necessary that the pole satisfy ucu , 1. Before computing the complete frequency 
response, we evaluate the filter gain at the two ends of the spectrum. Setting 
f 5 0 in z 5 exp(2f T ) yields z 5 1. Thus the low-frequency or DC gain of the 
filter is
A(0) 5  uH(z)uz51
5  .5(1 2 c)2
1 2 c
5 1
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

356    Chapter 5  Filter Types and Characteristics
Next, setting f 5 fsy2 in z 5 exp(2f T ) yields z 5 21. Thus the high-frequency 
gain of the filter is
A(fsy2) 5  uH(z)uz521
5  .5(1 2 c)0
21 2 c
5  0
Thus H(z) is a lowpass filter with a passband gain of one. To make the example 
specific, suppose c 5 .5. Then from (5.2.1) the frequency response of this IIR 
filter is
H( f ) 5  H(z)uz5  exp( j2f T )
5  .25fexp(  j2f T ) 1 1g
 exp(  j2f T ) 2 .5
5  .25f(cos(2f T ) 1 1) 1 j sin(2f T )g
fcos(2f T ) 2 .5g 1 j sin(2f T )
The magnitude response of the lowpass filter is then
A( f ) 5  uH( f )u
5  .25Ïfcos(2f T ) 1 1g2 1 sin2(2f T )
Ïfcos(2f T ) 2 .5g2 1  sin2(2f T )
For this simple first-order filter, suppose the cutoff frequencies for the transi-
tion band are taken to be
Fp 5 .1 fs
Fs 5 .4  fs
The magnitude response decreases monotonically in this case. Consequently, the 
passband ripple satisfies 1 2 p 5 A(Fp) or
p 5  1 2 A(Fp)
5  1 2 .25Ïfcos(.2) 1 1g2 1 sin2(.2)
Ïfcos(.2) 2 .5g2 1 sin2(.2)
5  .2839
Similarly, from Figure 5.6, the stopband attenuation satisfies
s 5  A(Fs)
5  .25Ïfcos(.8) 1 1g2 1 sin2(.8)
Ïfcos(.8) 2 .5g2 1 sin2(.8)
5  .1077
A plot of the magnitude response of the first-order filter is shown in Figure 5.10.  
For convenience, normalized frequency, f
 norm 5 fyfs, is used as the independent 
variable in this case. For this filter the passband ripple, the stopband atten-
uation, and the transition bandwidth are all relatively large because this is the  
lowest-order IIR filter possible.
Normalized frequency
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.2 Frequency-selective Filters    357
Figure 5.10: Magnitude Response of First-order IIR Lowpass Filter
0
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45 0.5
0
0.2
0.4
0.6
0.8
1
1.2
Passband
Stopband
Transition band
A(f)
f/fs
5.2.2 Logarithmic Design specifications (dB)
The filter specifications in Figures 5.6 through 5.9 are referred to as linear specifications 
because they are applied to the actual value of A( f ). It is also common to use a logarithmic 
specification that represents the value of the magnitude response using the decibel or dB scale.
 
A( f ) 5
D  10 log10huHs f du2j dB  
(5.2.6)
A logarithmic design specification for the magnitude response of a lowpass filter is 
shown in Figure 5.11. Note that the passband ripple in dB is Ap, and stopband attenu-
ation in dB is As. The dB scale is useful to show the degree of attenuation in the stop-
band. The lowpass specifications in Figure 5.6 and Figure 5.11 are equivalent. Using 
(5.2.6), we find the logarithmic specifications can be expressed in terms of the linear  
specifications as
 
Ap 5  220 log10(1 2 p)  dB  
(5.2.7a)
 
As 5  220 log10(s)  dB  
(5.2.7b)
Similarly, solving (5.2.7) for p and s, the linear specifications can be expressed in terms 
of the logarithmic specifications as
 
p 5  1 2 102Apy20 
(5.2.8a)
 
s 5  102Asy20
 
(5.2.8b)
Decibel
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

358    Chapter 5  Filter Types and Characteristics
Figure 5.11:  
Logarithmic 
Magnitude 
Response 
Specifications for  
a Lowpass Filter
f (Hz)
Fp
Fs
fs/2
0
Ap
2As
2Ap
0
Passband
Stopband
A(f) dB
Logarithmic Design specifications
EXAMPLE 5.2
To facilitate a comparison of the two types of filter design specifications, consider 
the following first-order IIR system.
H(z) 5 .25(1 1 z21)
1 2 .5z21
This filter, which was considered in Example 5.1, has a passband cutoff frequency 
of Fp 5 .1fs and a stopband cutoff frequency of Fs 5 .4fs. Using (5.2.7a), and the 
linear passband ripple from Example 5.1, we arrive at the following equivalent pass-
band ripple in dB.
Ap 5  220 log10(1 2 .2839)
5  2.901  dB 
Next, using (5.2.7b) and the linear stopband attenuation from Example 5.1, the 
equivalent logarithmic stopband attenuation is
As 5  220 log10(.1077)
5  19.356  dB 
A plot of the magnitude response in dB can be obtained by running exam5_2 with 
the results shown in Figure 5.12. Note that the display range had to be clipped  
(at 240 dB), because A(fsy2) 5 0 which means that A(fsy2) 5 2` dB.
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.3 Linear-phase and Zero-phase Filters     359
Linear-phase and Zero-phase Filters 
5.3.1 Linear Phase
The filter design specifications discussed thus far are specifications of the desired mag-
nitude response of the filter. It is also possible to design filters with prescribed phase 
responses. To illustrate the type of phase responses that are desirable and achievable, 
consider the following analog system.
 
Ha(s) 5 exp(2s) 
(5.3.1)
Recall from the properties of the Laplace transform in Appendix 1 that Ha(s) represents 
an ideal delay line of delay . Thus the input is delayed by  but is not otherwise dis-
torted by this system. The frequency response of the delay line is Ha( f ) 5 exp(2j2f  ). 
Consequently, if we regard the delay line as a filter, it is an allpass filter with A( f ) 5 1 
and with a phase response of
 
a( f ) 5 22f  
(5.3.2)
The linear phase response in (5.3.2) represents a pure delay. To interpret the meaning 
of a nonlinear phase response, it is helpful to introduce the concept of group delay.
Let  ( f ) be the phase response of a linear system. The group delay of the system is 
denoted, D( f ), and defined
D( f )  5
D   1
21
22 d  ( f )
df
5.3
Delay line
DEFiniTion
5.1  Group Delay
Figure 5.12: Magnitude Response of a First-order IIR Lowpass Filter Using the dB Scale
0
0.1
0.05
0.2 0.25
0.15
0.3 0.35 0.4 0.45 0.5
240
235
230
225
220
215
210
25
0
5
10
Passband
Stopband
A(f) dB
f/fs
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

360    Chapter 5  Filter Types and Characteristics
The group delay is the negative of the slope of the phase response, scaled by 1y(2). 
Observe from (5.3.2), that the group delay of a delay line is simply D( f ) 5 . That is, 
for a delay line the group delay specifies the amount by which the signal is delayed as it 
is processed by the filter. For most filters, the group delay D( f ) is not constant. In these 
cases, we can interpret D( f ) as the amount by which the spectral component at frequency 
f  gets delayed as it is processed by the filter. Given the notion of group delay, we are now 
in a position to define what is meant by a generalized linear-phase digital filter.
Let Fz denote the set of frequencies at which the magnitude response is A( f ) 5 0. A 
digital filter H(z) is a linear-phase filter if and only if there exists a constant  such that
D ( f ) 5 , f Ó Fz
A digital filter is a linear-phase filter if the group delay is constant, except possibly at 
frequencies at which the magnitude response is zero. These spectral components do not 
appear in the filter output, so the group delay is not meaningful for f [ Fz. Typically Fz is 
a finite isolated set of frequencies and often Fz is the empty set. Note that Definition 5.1 
and Definition 5.2 imply the following general form for a linear phase response.
 
 ( f ) 5  1 ( f ) 2 2f  
(5.3.3)
Here  is a constant and ( f ) is piecewise constant with jump discontinuities between 0 
and , permitted at the frequencies, Fz, at which A( f ) 5 0.
5.3.2 Amplitude Response
An alternative way to characterize a linear-phase filter is in terms of its frequency 
response, which can be written in the following general form.
 
H( f ) 5 Ar( f ) expf  j( 2 2f  )g  
(5.3.4)
Here the factor Ar( f ) is real, but it can be both positive and negative. It is referred to as the 
amplitude response of H(z). This is to distinguish it from the magnitude response, A( f ), 
which is never negative. Since the polar form of the frequency response is 
H( f ) 5 A( f ) expf  j ( f )g, it follows from (5.3.3) and (5.3.4) that the amplitude response is
 
Ar( f ) 5 A( f ) expf  j( f )g 
(5.3.5)
Therefore the uAr( f )u 5 A( f ) and /Ar ( f ) 5 ( f ). The points at which Ar ( f ) 5 0 are 
the points, Fz, where the phase can abruptly change by  as Ar( f ) changes sign. Thus 
the piecewise-constant function ( f ) in (5.3.3) jumps between zero and  each time the 
amplitude response, Ar( f ), changes sign.
A linear phase characteristic in the passband can be achieved in the analog domain by 
an IIR Bessel filter. However, the linear-phase feature does not survive the analog-to-digital 
transformation. Consequently, it is better to start with a digital FIR filter as follows.
 
H(z) 5 o
m
i50
biz2i 
(5.3.6)
For an FIR filter, there is a simple symmetry condition on the coefficients that guarantees 
a linear phase response. First we illustrate this with a special case.
DEFiniTion
5.2  Linear-phase Filter
Amplitude response
Bessel filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.3 Linear-phase and Zero-phase Filters     361
The even symmetry of h(k) about the midpoint k 5 my2 is one way to obtain a  
linear-phase filter. Another approach is to use odd symmetry of h(k) about k 5 my2, as 
can be seen from the following example.
Even symmetry
EXAMPLE 5.3
Consider an FIR filter of order m 5 4 having the following symmetric transfer 
function.
H(z) 5 c0 1 c1z 21 1 c2z 22 1 c1z 23 1 c0z 24
Recall that for an FIR filter, h(k) 5 bk for 0 # k # m. Thus the impulse response, 
h 5 fc0, c1, c2, c1, c0gT, exhibits even symmetry about the midpoint k 5 m y2. The 
frequency response of this filter, in terms of  5 2 f T, is
H(  f  ) 5 H(z)uz5  exp(  j)
 5 c0 1 c1 exp(2j) 1 c2 exp(2j2) 1 c1 exp(2j3) 1 c0 exp(2j4)
 5 exp(2j2)fc0 exp(  j2) 1 c1 exp(  j) 1 c2 1 c1 exp(2j) 1 c0 exp(22j)g
Combining terms with identical coefficients, and using Euler’s identity, we have
H( f  ) 5   exp(2j2)hc0fexp( j2) 1  exp(2j2)g 1 c1fexp( j) 1  exp(2j)g 1 c2j
5   exp(2j2)f2c0 cos(2) 1 2c1 cos() 1 c2g
5   exp(2j4f T )Ar( f )
Comparing with (5.3.4), we see that this is a linear-phase system with phase offset 
 5 0 and delay  5 2T. In this case, the amplitude response, Ar( f  ), is the follow-
ing even function that may be positive or negative.
Ar( f ) 5 2c0 cos(4f T ) 1 2c1 cos(2f T ) 1 c2
odd symmetry
EXAMPLE 5.4
Consider an FIR filter of order m 5 4 having the following skew-symmetric transfer 
function.
H(z) 5 c0 1 c1z21 2 c1z23 2 c0z24
Recalling that h(k) 5 bk, we see that for this filter the impulse response, 
h 5 fc0, c1, 0, 2c1, 2c0gT, exhibits odd symmetry about the midpoint k 5 my2. 
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

362    Chapter 5  Filter Types and Characteristics
Type
Midpoint 
symmetry 
h(k)
Filter order 
m
Phase  
offset 
Amplitude 
Response 
Ar(f)
Zeros
Frequency 
Band
1
Even
Even
0
Even
None
All
2
Even
Odd
0
Even
H(21) 5 0
Lowpass
3
Odd
Even
y2
Odd
H(61) 5 0
Bandpass
4
Odd
Odd
y2
Odd
H(1) 5 0
Highpass
Table 5.2:  
Linear-phase FIR  
Filters, D(f) 5 mTy2
The results of Examples 5.1 and 5.2 can be generalized. In particular, it is possible to 
show by examining additional cases (see Problems 5.16–5.17) that an FIR filter of order 
m is a linear-phase filter when it satisfies the following symmetry condition.
Let H(z) be an FIR filter of order m. Then H(z) is a linear-phase filter with group delay 
D( f ) 5 mTy2 if and only if the impulse response h(k) satisfies the symmetry condition
h(k) 5 6h(m 2 k), 0 # k # m
The symmetry constraint in Proposition 5.2 says that the impulse response must 
exhibit even symmetry about the midpoint, k 5 my2, when the plus sign is used, or odd 
symmetry about the midpoint when the minus sign is used. We can further decompose the 
symmetry condition into cases where the filter order m is even or odd, and this results in 
a total of four linear-phase filter types as summarized in Table 5.2.
Observe that when the filter order m is even, there is a middle sample, h(my2), 
about which the impulse response exhibits either even or odd symmetry. When m is odd, 
the symmetry is about a point that is midway between a pair of samples. The impulse 
responses of the four linear-phase filter types are shown in Figure 5.13 for the case 
h(k) 5 k2 for k # floor (my2). Note that the middle sample of the type 3 filter must sat-
isfy h(my2) 5 2h(my2), which means that h(my2) 5 0 in this case. For filters with even 
symmetry, the phase offset is  5 0, and the amplitude response in (5.3.4) is an even 
function. For filters with odd symmetry,  5 y2, and the amplitude response is an odd 
function. For type 1 and type 2 filters, the impulse responses are the numerical equivalent 
of palindromes, words that are the same whether they are spelled forwards or backwards.
PRoPosiTion
5.2  Linear-phase FiR Filter
Even symmetry
Odd symmetry
Phase offset
The frequency response of this filter, in terms of  5 2f T, is
H( f ) 5  H(z)uz5  exp(j)
5  c0 1 c1 exp(2j) 2 c1 exp(2j3) 2 c0 exp(2j4)
5   exp(2j 2)fc0 exp( j 2) 1 c1 exp( j) 2 c1 exp(2j) 2 c0 exp(22j)g
Combining terms with identical coefficients, and using Euler’s identity, we have
H( f ) 5   exp(2j2) hc0fexp(  j2) 2 exp(2j2)g 1 c1fexp(  j) 2 exp(2j)gj
5  j exp(2j2)f2c0 sin(2) 1 2c1 sin()g
5   expf  j(y2 2 4f T )gAr( f )
Comparing with (5.3.4), this is a linear-phase system with phase offset  5 y2 
and delay  5 2T. In this case, the amplitude response, Ar( f ), is the following odd 
function that may be positive or negative.
Ar( f ) 5 2c0 sin(4f T ) 1 2c1 sin(2f T )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.3 Linear-phase and Zero-phase Filters     363
Figure 5.13:  
Impulse 
Responses of 
Four Types of 
Generalized  
Linear-phase 
Filters
0
5
10
220
210
0
10
20
m 5 10 
k
0
5
10
220
210
0
10
20
m 5 9
k
0
5
10
220
210
0
10
20
m 5 10
k
0
5
10
220
210
0
10
20
m 5 9
k
h1(k)
h2(k)
h3(k)
h4(k)
5.3.3 Linear-phase Zeros
The symmetry condition that guarantees a linear phase response, also imposes certain 
constraints on the zeros of an FIR filter. To see this we start with (5.3.6) and apply Prop-
osition 5.2 with a change of variable.
H(z) 5  o
m
i50
h(i)z2i
5  6o
m
i50
h(m 2 i)z2i
5  6o
0
k5m
h(k)z2(m2k), k 5 m 2 i
 
5  6z2mo
m
k50
h(k)zk
 
(5.3.7)
The final summation on the right-hand side of (5.3.7) is simply H(z21). Thus the linear- 
phase symmetry condition, expressed in terms of the transfer function, is
 
H(z) 5 6z2mH(z21) 
(5.3.8)
Here the plus sign in (5.3.8) applies to filter types 1 and 2 that exhibit even symmetry 
about the midpoint my2, and the minus sign applies to filter types 3 and 4 that exhibit 
Symmetry condition
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

364    Chapter 5  Filter Types and Characteristics
odd symmetry. The frequency domain symmetry condition (5.3.8) places constraints 
on the locations of the zeros of H(z). For example, consider a type 2 filter with even 
symmetry and odd order. Using the plus sign and evaluating (5.3.8) at z 5 21 yields 
H(21) 5 2H(21). Therefore every type 2 linear-phase filter has a zero at z 5 21. Con-
sequently, a type 2 filter is lowpass or possibly bandpass.
Next consider a type 3 filter with odd symmetry and even order. Using the minus 
sign and evaluating (5.3.8) at z 5 21 yields H(21) 5 2H(21), which means that z 5 21 
is also a zero of the type 3 filter. Evaluating (5.3.8) with the minus sign at z 5 1 yields 
H(1) 5 2H(1). Hence a type 3 filter has zeros at z 5 61. It follows that a type 3 filter is 
bandpass.
Finally, consider a type 4 filter where both the symmetry and the order are odd. 
Using the minus sign and evaluating (5.3.8) at z 5 1 yields H(1) 5 2H(1), which means 
that z 5 1 is a zero of H(z). Thus a type 4 filter is highpass or possibly bandpass. The 
zeros and passbands of the four linear-phase filter types are summarized in the last two 
columns of Table 5.2. Note that the type 1 filter (even symmetry, even order) is the most 
general in that it does not contain zeros at either end of the frequency range.
In addition to the zeros at the two ends of the frequency range, the constraint on 
H(z) in (5.3.8) also implies that complex zeros must appear in certain patterns. Let 
z 5 r exp(  j ) be a complex zero with r . 0. Then from (5.3.8), z21 5 r21 exp(2j  ) must 
also be a zero of H(z). Furthermore, if the coefficient vector b is real, then zeros must 
occur in complex-conjugate pairs. Hence for r Þ 0, the linear-phase zeros will appear in 
groups of four and satisfy the following reciprocal symmetry.
 
Q 5 h r exp(6 j  ), r21 exp(7j  ) j 
(5.3.9)
When the zeros are real, this yields  5 0 or  5  and the set Q in (5.3.9) reduces to a 
pair of reciprocal real zeros. A pole-zero plot for a type 1 FIR linear-phase filter showing 
a typical arrangement of both real and complex zeros is shown in Figure 5.14. Being an 
FIR filter, the poles are all located at the origin.
Linear-phase zeros
Figure 5.14:  
Poles and Zeros 
of the Type 1 FIR 
Linear-phase Filter 
of Order m 5 6
21.5
21
20.5
0
0.5
1
1.5
21.5
21
20.5
0
0.5
1
1.5
X
O
O
O
O
O
O
Re(z)
Im(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.3 Linear-phase and Zero-phase Filters     365
The importance of linear-phase filters is that they do not distort the signals that are 
processed by the filter. This is significant in applications, such as speech or music pro-
cessing, where the effects of phase distortion can be noticeable. To verify the effect of 
a linear-phase filter on an individual spectral component, suppose x(k) 5 cos ( 2  f k T ). 
For a filter with group delay D( f ) 5 mTy2, the phase response is  ( f ) 5 2mTf . Thus 
from Proposition 3.2, the steady-state output is
yss (k) 5  A( f ) cosf2  f k T 1  ( f )g
5  A( f ) cos( 2  f k T 2  m T f)
5  A( f ) cosf 2  f (k 2 my2)Tg
 
5  A( f )x(k 2 my2)
 
(5.3.10)
Consequently, the spectral component at frequency f  is scaled by A( f ) and delayed by 
D( f ) 5 mTy2 but is not otherwise affected by the filter. A summary of the steady-state 
input-output characteristics of linear-phase filters is shown in Figure 5.15.
5.3.4 Zero-phase Filters
Suppose the frequency response H( f ) is real and non-negative. Since the imaginary part 
is identically zero, this means that H(z) is a zero-phase filter.
 
 ( f ) 5 0, 0 # u f u # fsy2 
(5.3.11)
A zero-phase filter is a linear-phase filter with a group delay of zero. Just as with an 
ideal frequency-selective filter, a zero-phase filter cannot be realized with a causal system. 
However, if we drop the need for a real time implementation and focus in input signals 
of finite length, then there is a relatively simple procedure for implementing a zero-phase 
filter. The key is the time-reversal property of the DFT. Recall from Table 4.8 that for a 
real N-point signal x(k) with periodic extension xp(k),
 
 DFT  hxp(2k)j 5 X *(i) 
(5.3.12)
For 0 # k , N, the time-reversed periodic extension of x(k) is just x(N 2 k) where 
x(N) 5 x(0). Suppose C(z) is a linear-phase FIR filter of order m. Then C(z) will have 
frequency response of C(i) and a group delay of D( f ) 5 mTy2. Let the input to the 
filter be x(k) and the output be q1(k) as shown in Figure 5.16. Then in the frequency  
domain
 
Q1(i) 5 C(i)X(i) 
(5.3.13)
Zero-phase filter
Time-reversal property
Figure 5.15:  
Steady-state 
Output of a Linear-
phase FIR Filter 
with Group Delay 
D(f) 5 mTy2 when 
x(k) 5 cos( 2  f k T )
A(ƒ) exp(2jmTƒ)
x(k)
A(ƒ)x(k 2 m/2)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

366    Chapter 5  Filter Types and Characteristics
Next suppose the filter output q1(k) is time reversed to produce a new signal 
q2(k) 5 q1(N 2 k). Using (5.3.13) and the time-reversal property in (5.3.12)
 
Q2(i) 5 C*(i)X*(i) 
(5.3.14)
The time-reversed output is then used as in input to a second copy of C(z) as shown in 
Figure 5.16. Using (5.3.14), the resulting output is
 
Q3(i) 5 C(i)C*(i)X*(i) 
(5.3.15)
Note that the first use of C(z) caused a delay mTy2 in q1(k). By reversing q1(k) to pro-
duce q2(k) and then processing q2(k) with C(z) again, this has the effect of causing a time 
advance of mTy2, thus cancelling the time delay. The resulting output, q3(k), is then 
time-reversed again using y(k) 5 x3(N 2 k) to cancel the original time reversal. Using 
(5.3.15) and the time-reversal property in (5.3.12) this yields
 
Y(i) 5 C*(i)C(i)X(i) 
(5.3.16)
Finally, C *(i)C(i) 5 uC(i)u2. Thus the noncausal system in Figure 5.16 has the following 
real non-negative frequency response whose phase response is  (i) 5 0.
 
H(i) 5 uC(i)u2 
(5.3.17)
Since the FIR filter C(z) processes the signal twice, the noncausal filter H(z) is of order 
2m. To design and implement a noncausal FIR filter with zero phase and a prescribed 
magnitude response, the following algorithm can be used.
1. Pick a desired magnitude response Ad (i) $ 0 for 0 # i , N.
2.  Using techniques from Chapter 6, design a linear-phase filter C(z) of order m with 
coefficient vector c [ R m11 and magnitude response A(i) 5 ÏAd (i).
3.  Implement the noncausal filter H(z) as follows for 0 # k , N where qi (N ) 5 qi (0) 
for 1 # i # 3.
q1(k) 5  o
m
i50
cix(k 2 i)
q2(k) 5  q1(N 2 k)
q3(k) 5  o
m
i50
ciq2(k 2 i)
y(k) 5  q3(N 2 k)
Noncausal system
ALGoRiTHM
5.1  Zero-phase filter
Figure 5.16: Noncausal Zero-phase FIR Filter of Order 2m
Time
reversal
Time
reversal
x(k)
y(k)
F(z)
F(z)
q1(k)
q2(k)
q3(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.3 Linear-phase and Zero-phase Filters     367
0
10
20
30
40
50
60
70
80
90
25
0
5
(a)
k
0
10
20
30
40
50
60
70
80
90
24
22
0
2
4
6
(b)
k
x(k)
y(k)
y(k)
x1(k)
Figure 5.17: Input and Output of a Noncausal Zero-phase FIR Filter of Order 
2m 5 60 from Example 5.5
Zero-phase Filter
EXAMPLE 5.5
To illustrate a filter with zero phase shift, suppose fs 5 200 Hz and consider an input 
that includes two sinusoidal components with frequencies F0 5 20 Hz and F1 5 60 Hz.
x1(k) 5  2 cos(2F0kT )
x2(k) 5  23 sin(2F1kT )
x(k) 5  x1(k) 1 x2(k)
Using design techniques presented in Chapter 6, suppose C(z) consists of a low-
pass FIR filter of order m 5 30 with cutoff frequency
Fc 5 F0 1 F1
2
The noncausal filter H(z) should then block component x2(k) while it passes 
component x1(k) undistorted and with no phase shift. Plots of x(k) and the filter 
output y(k) generated by exam5_5 are shown in Figure 5.17.
Notice that after a brief startup transient the output y(k) closely matches compo-
nent x1(k) as expected. However, in addition to the startup transient there is also a 
transient of length approximately my2 at the end of the time signal. This is a charac-
teristic of zero-phase filters. The ending transient is actually the startup transient of the 
second stage in Figure 5.16 where the signal is run backwards through C(z) to cancel 
the time delay. By selecting a suitable initial condition for stage 2, the second transient 
can be reduced. Alternatively, if the signal length N is large in comparison with the filter 
order m, then the transients will only occupy a small fraction of the filter output signal.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

368    Chapter 5  Filter Types and Characteristics
Minimum-phase and Allpass Filters
Every digital filter with a rational transfer function can be expressed as a product of two 
specialized filters. The first one is called a minimum-phase filter, and the second is an 
allpass filter.
5.4.1 Minimum-phase Filters
The magnitude response, by itself, does not provide enough information to completely 
specify a filter. Indeed, among IIR filters having m zeros, there are up to 2m distinct filters, 
each having an identical magnitude response A( f ). To see this, recall that a rational IIR 
transfer function can be written as a ratio of two polynomials in z.
 
H(z) 5 b(z)
a(z) 
(5.4.1)
Since the polynomial b(z) has real coefficients, the complex conjugate b*(z) can be  
obtained by replacing z by z*. On the unit circle z 5  exp(j2fT), which means that z* 5 z21. 
Thus the square of the magnitude response can be expressed as follows.
A2( f ) 5  ub(z)u2
ua(z)u2u
z5  exp(  j2f T )
5  b(z)b*(z)
ua(z)u2 u
z5  exp(  j2f T )
 
5  b(z)b(z21)
ua(z)u2 u
z5  exp(  j2f T )
 
(5.4.2)
Next suppose H(z) has a zero at z 5 c with c Þ 0. Then b(z) and b(z21) can be written in 
partially factored form as
 
b(z) 5  (z 2 c)b0(z)
 
(5.4.3a)
 
b(z21) 5  (z21 2 c)b0(z21) 
(5.4.3b)
5.4
The DSP Companion contains the following function that implements Algorithm 5.1  
to perform zero-phase filtering. If the MATLAB signal processing toolbox is avail-
able, then the function filtfilt can be used to perform zero-phase filtering.
% F_ZEROPHASE Compute the output of noncausal zero-phase filter
%
% Usage:
%  
y = f_zerophase(b,x);
% Pre:
%  
b =  array of length m+1 containing the FIR filter
% 
coefficients
%  
x = array of length N containing the input samples
% Post:
%  
y = array of length N containing the output samples
DSP Companion
DsP Companion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.4 Minimum-phase and Allpass Filters    369
If the factor, z 2 c, in b(z) is interchanged with the corresponding factor, z21 2 c, in 
b(z21), the product b(z)b(z21) does not change, which means A2( f ) in (5.4.2) does not 
change. Replacing the factor, z 2 c, with the factor, z21 2 c, is equivalent to replacing the 
zero at z 5 c with a zero at its reciprocal z 5 c21 and scaling by a constant. To determine 
the constant first note that
z21 2 c 5  z21(1 2 cz)
 
5  2cz21(z 2 c21) 
(5.4.4)
When the magnitude response of H(z) is computed, we evaluate H(z) along the unit circle 
which means uz21u 5 1. Thus a new numerator polynomial which does not change the 
magnitude response of H(z) is as follows.
 
B(z) 5 2c(z 2 c21)b(z)
(z 2 c)
 
(5.4.5)
Since this can be done with any of the m zeros of b(z), there are up to 2m distinct com-
binations of zeros of H(z) that all yield filters with identical magnitude responses. The 
differences between these filters lie in their phase responses  ( f ).
A digital filter H(z) is a minimum-phase filter if and only if all of its zeros lie inside or 
on the unit circle. Otherwise it is a nonminimum-phase filter.
Every IIR filter H(z) can be converted to a minimum-phase filter with the same mag-
nitude response by replacing the zeros outside the unit circle with their reciprocals. The 
term minimum phase arises from the fact that the net phase change of a minimum-phase 
filter, over the frequency range f0, fsy2g, is
 
 (fsy2) 2  (0) 5 0 
(5.4.6)
Nonminimum-phase filters have at least one zero outside the unit circle. It can be shown 
(Proakis and Manolakis, 1988) that if H(z) has p zeros outside the unit circle, then the net 
phase change is (fsy2) 2 (0) 5 2p. Consequently, among filters that have the same 
magnitude response, the minimum-phase filter is the filter that has the smallest amount of 
phase lag.
DEFiniTion
5.3  Minimum-phase Filter
Minimum phase
Minimum-Phase Filter
EXAMPLE 5.6
To illustrate how different filters can have identical magnitude responses, consider 
the following second-order IIR filter.
H00(z) 5 2(z 1 .8)(z 2 .9)
(z 2 .5)2 1 .25
This is a stable IIR filter with poles at z 5 .5 6 j.5. It is also a minimum-phase 
filter because the zeros at z 5 2.8 and z 5 .9 are both inside the unit circle. 
There are three other IIR filters that have an identical magnitude response, which  
we obtain by using the reciprocal of the first zero, the second zero, or both and 
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

370    Chapter 5  Filter Types and Characteristics
Figure 5.18: Pole-zero Plots of Four Filters Having Same A(f)
21.5
0
1.5
21.5
0
1.5
X
X
O
O
21.5
0
1.5
21.5
0
1.5
X
X
O
O
21.5
0
1.5
21.5
0
1.5
X
X
O
O
21.5
0
1.5
21.5
0
1.5
X
X
O
O
Re(z)
Re(z)
H01(z)
H11(z)
H00(z)
H10(z)
Re(z)
Re(z)
Im(z)
Im(z)
Im(z)
Im(z)
multiplying by the negative of the original zero as in (5.4.5). Thus the transfer 
functions of the other three filters are
H10(z) 5  1.6(z 1 1.25)(z 2 .9)
(z 2 .5)2 1 .25
H01(z) 5  21.8(z 1 .8)(z 2 1.11)
(z 2 .5)2 1 .25
H11(z) 5  21.44(z 1 1.25)(z 2 1.11)
(z 2 .5)2 1 .25
Pole-zero plots of the four equivalent filters are shown in Figure 5.18. Only 
filter H00(z) is a minimum-phase filter. Since filter H11(z) has all of its zeros out-
side the unit circle, it is called a maximum-phase filter, while H10(z) and H01(z) 
are called mixed-phase filters.
Plots of the four magnitude responses are shown in Figure 5.19 where it is 
evident that they are all identical. However, the four phase responses are distinct 
from one another as can be seen in the plot in Figure 5.20. Note that the net 
phase change for the minimum-phase filter is zero. For the two mixed-phase fil-
ters, H10(z) and H01(z), the net phase change is 2. Finally, the maximum-phase 
filter, H11(z), has a net phase change of 22.
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.4 Minimum-phase and Allpass Filters    371
0
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45 0.5
0
1
2
3
4
5
6
f/fs
A(f )
Figure 5.19: Identical Magnitude Responses of the Four Filters in Example 5.6
Figure 5.20: The Phase Responses of the Four Filters in Example 5.6
0
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45 0.5
24
23
22
21
0
1
2
3
4
f/fs
01
11
10
00
(f )
Every rational IIR filter can be converted to minimum-phase form by replacing each 
zero outside the unit circle by its reciprocal and scaling by the negative of the zero. If the 
original filter has a pair of complex conjugate zeros at z 5 r exp(6j) with r . 1, then 
both zeros must be replaced by zeros at z 5 r21 exp(7j) in order for the coefficients of 
the new filter to remain real.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

372    Chapter 5  Filter Types and Characteristics
5.4.2 Allpass Filters
Another important class of IIR filters that can be used to provide phase compensation 
is the allpass filter. As the name implies, an allpass filter is a filter that passes all spectral 
components equally because it has a flat magnitude response.
A digital filter H(z) is an allpass filter if and only if it has the following magnitude response.
A( f ) 5 1, 0 # u f u # fsy2
There are no special constraints on the phase response of an allpass filter. Allpass filters 
have transfer functions coefficients with the following reflective structure.
 
Hall(z) 5 an 1 an 2 1z21 1 Á 1 z2n
1 1 a1z21 1 Á 1 anz2n 5 z2na(z)
a(z21)  
(5.4.7)
Notice that the numerator polynomial is just the denominator polynomial, a(z21), 
but with the coefficients reversed. To see how this gives a flat magnitude response, let 
A( f ) denote the magnitude response of the FIR filter H(z) 5 a(z21) corresponding to 
the denominator in (5.4.7). Since the magnitude response is even and uzu 5 1 on the unit 
circle, the magnitude response of the filter in (5.4.7) is
Aall( f ) 5  uHall(z)uz5  exp(j2fT)
5  u
z2na(z)
a(z21) u z5  exp( j2f T )
5  uz2nu ? ua(z)u
ua(z21)u u
z5  exp( j2f T )
5  A(2f  )
A( f )
 
5  1
 
(5.4.8)
The process of converting a filter H(z) to minimum-phase form can be thought of as 
multiplication of H(z) by a transfer function F(z). To illustrate, suppose H(z) has a sin-
gle zero at z 5 c where c lies outside the unit circle. Then replacing this zero with one at 
z 5 c21 and multiplying by 2c is equivalent to multiplying H(z) by
 
F(z) 5 2c(z 2 c21)
z 2 c
 
(5.4.9)
If z 5 c is the only zero of H(z) outside the unit circle, then the minimum-phase version 
of H(z) can be expressed as
 
Hmin(z) 5 F(z)H(z) 
(5.4.10)
Next consider the characteristics of the transfer function F(z) used to convert H(z) to 
minimum-phase form. Note from (5.4.9) that
F(z) 5  2c(z 2 c21)
z 2 c
5  2cz 1 1
z 2 c
 
5  2c 1 z21
1 2 cz21
 
(5.4.11)
DEFiniTion
5.4  Allpass Filter
Reflective structure
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.4 Minimum-phase and Allpass Filters    373
Comparing (5.4.11) with (5.4.7), we see that F(z) is an allpass filter with a 5 f1, 2cgT. Although 
F(z) was developed using only one zero outside the unit circle, the process can be repeated for 
any number of zeros with the resulting allpass filter having factors similar to (5.4.9).
The magnitude response of the inverse of a filter is just the inverse of the magnitude 
response of the filter. Hence if H
 all (z) 5 F 21(z), then H
 all (z) is an allpass filter. Further-
more, multiplying both sides of (5.3.10) on the left by H
 all (z), we conclude that the every 
rational IIR transfer function H(z) can be decomposed into the product of an allpass 
filter F 21(z) times a minimum-phase filter H
 min (z) 5 F(z)H(z).
Let H(z) be a rational IIR transfer function, and let H
 min (z) be the minimum-phase 
form of H(z). Then there exists a stable allpass filter H
 all  such that
H(z) 5 H
 all (z)H
 min (z)
A block diagram of the decomposition into allpass and minimum-phase parts is shown 
in Figure 5.21. The minimum-phase part is the minimum-phase form of H(z). Therefore 
the magnitude response of H
 min (z) is identical to the magnitude response of H(z). The 
allpass part is the inverse of the system that transforms H(z) into its minimum-phase 
form. Consequently, the allpass part H
 all (z) is always stable.
Another way to characterize an allpass filter is in terms of its poles and zeros. For each 
pole of H
 all (z) at z 5 c, there is a matching zero at its reciprocal, z 5 c21. Thus allpass 
filters always have the same number of poles and zeros, with the poles and zeros forming 
reciprocal pairs. The following algorithm summarizes the steps needed to decompose a 
general IIR filter into its minimum-phase and allpass parts.
1. Set H
 min (z) 5 H(z), and H
 all (z) 5 1. Factor the numerator polynomial of H(z) as follows.
b(z) 5 b0(z 2 z1)(z 2 z2) Á (z 2 zm)
2. For i 5 1 to m do
h
 
If uziu . 1 then compute 
F(z) 5  2ziz 1 1
z 2 zi
H
 min (z) 5  F(z)H
 min (z)
H
 all (z) 5  F21(z)H
 all (z)
j
PRoPosiTion
5.3 Minimum-phase Allpass  
Decomposition 
ALGoRiTHM
5.2 Minimum-phase  
Allpass Decomposition 
Figure 5.21:  
Decomposition 
of IIR Filter H(z) 
into Allpass and 
Minimum-phase 
Parts
y(k)
Hall(z)
Hmin(z)
H(z)
x(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

374    Chapter 5  Filter Types and Characteristics
22
21 20.5
21.5
0
1
1.5
0.5
2
22
21.5
21
20.5
0
0.5
1
1.5
2
X
X
O
O
Re(z)
Im(z)
Figure 5.22: Pole-zero Plot of System in Example 5.7
Minimum-phase Allpass 
Decomposition
EXAMPLE 5.7
As an illustration of the decomposition of an IIR transfer function into allpass 
and minimum-phase parts, consider the following digital filter.
H(z) 5 .2f(z 1 .5)2 1 1.52g
z2 2 .64
This is a stable IIR filter with real poles at p1,2 5 6.8 and complex-conjugate 
zeros at z1,2 5 2.5 6 j1.5 as shown in Figure 5.22. Since the zeros are both out-
side the unit circle, this is a maximum-phase filter. The minimum-phase form is 
obtained by replacing the zeros by their reciprocals and multiplying by the nega-
tives of the zeros. The new zeros are
z3,4 5  1
z1,2
5  
1
2.5 6 j1.5
5  2.5 7 j1.5
.25 1 2.25
5  2.2 7 j.6
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.4 Minimum-phase and Allpass Filters    375
f/fs
f/fs
0
0
1
2
3
A(f)
 A
Aall
all
min
Amin
0
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
0.5
24
22
0
2
4

(f)
Figure 5.23: Magnitude and Phase Plots of H(z), Hall(z), and Hmin(z)
The product z1z2 5 uz1u2. Thus the minimum-phase form of H(z) is
H
 min (z) 5  uz1u2.2(z 2 z3)(z 2 z4)
z2 2 .64
5  (.25 1 2.25).2f(z 1 .2)2 1 .62g
z2 2 .64
5  .5f(z 1 .2)2 1 .62g
z2 2 .64
Since both zeros were replaced, the allpass part just the original numerator 
divided by the numerator of H
 min (z).
H
 all (z) 5  .2f(z 1 .5)2 1 1.52g
.5f(z 1 .2)2 1 .62g
5  .4f(z 1 .5)2 1 1.52g
(z 1 .2)2 1 .62
A plot of the original and decomposed magnitude responses and the orig-
inal and decomposed phase responses is shown in Figure 5.23. Note that 
A
 min ( f ) 5 A( f ) and A
 all ( f ) 5 1 as expected. It is also evident that 
 min ( f ) has 
less phase lag than  ( f ).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

376    Chapter 5  Filter Types and Characteristics
5.4.3 inverse systems and Equalization
One of the application areas of minimum-phase systems is the construction of inverse 
systems. Suppose H(z) 5 b(z)ya(z) is a stable system with frequency response H( f ). The 
inverse of H(z) is the system whose transfer function is the reciprocal, H21(z) 5 a(z)yb(z). 
If H21(z) is stable, then it has a frequency response H21( f ) 5 1yH( f ). Consequently, 
the frequency response of a cascade or series configuration of the two systems is 
H21( f )H( f ) 5 1. In this case we say that H21(z) is an equalizer for the system H(z) in the 
sense that it cancels out the effects of H(z) with
 
H21(z)H(z) 5 1 
(5.4.12)
The problem arises when H21(z) is not stable. This will occur if H(z) has zeros on or 
outside the unit circle because they become poles of the reciprocal system. Suppose the 
design objective for an equalizer system is more limited. If the objective is to cancel only 
the magnitude response characteristics of H(z), then the minimum-phase form of H(z) 
can be used. In particular, suppose A( f ) . 0, which means that H(z) has no zeros on 
the unit circle. Then H
 min (z) has no zeros on or outside the unit circle, which means that 
H21
 min (z) is stable. Consider the following equalized system.
 
H
 equal (z) 5 H21
 min (z)H(z) 
(5.4.13)
To examine the magnitude response of the equalized system, first note from the minimum- 
phase allpass decomposition in (5.4.12) that
 
H
 equal (z) 5 H
 all (z) 
(5.4.14)
Recall from Algorithm 5.2 that the poles of H
 all (z) are the reciprocals of the zeros of H(z) 
that lie outside the unit circle. Therefore H
 all (z) is stable. Since H
 all (z) is an allpass filter, 
it follows that H21
 min (z) serves as a magnitude equalizer with the magnitude response of the 
equalized system being
 
A
 equal ( f ) 5 1 
(5.4.15)
Therefore H21
min(z) cancels or equalizes the magnitude response of H(z), but not the phase 
response. In Chapter 6, a two-stage quadrature filter design technique is presented that 
allows us to design a filter that equalizes both the magnitude and the phase response 
of a stable H(z), but it introduces a delay in H
 equal (z). Equalizers with delay also can be 
designed using adaptive filters as discussed in Chapter 9.
Equalizer
Magnitude equalizer
The DSP Companion contains the following function for decomposing a transfer 
function into minimum-phase and allpass parts.
% F_MINALL: Factor a filter into minimum-phase and allpass parts
%
% Usage:
%  
[B_min,A_min,B_all,A_all] = f_minall (b,a)
% Pre:
%  
b = vector of length m+1 containing coefficients
%  
of numerator polynomial.
%  
a = vector of length n+1 containing coefficients
%  
of denominator polynomial (n >= m).
% Post:
%  
B_min = (q+1) by 1 vector containing numerator
%  
coefficients of minimum-phase part
DSP Companion
DsP Companion
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.5 Quadrature Filters     377
Quadrature Filters 
5.5.1 Differentiator
A pair of periodic signals is said to be in phase quadrature if one signal lags the other 
by a quarter of a cycle. A common example of a quadrature pair is the sine and cosine. 
Certain specialized filters have steady-state outputs that are in phase quadrature with 
the input. Perhaps the simplest example is a differentiator. A continuous-time differen-
tiator has transfer function Ha(s) 5 s and frequency response Ha( f ) 5 j2f . In order 
to approximate a differentiator with a causal mth order linear-phase filter, consider a  
discrete-time frequency response with a delay of mTy2.
 
H( f ) 5 j2f exp(2jmf T ) 
(5.5.1)
In real-time applications such as digital feedback control, a PID controller is imple-
mented by approximating the derivative term with a first-order backward Euler difference 
as follows.
 
y(k) < x(k) 2 x(k 2 1)
T
 
(5.5.2)
This numerical approximation replaces dx(t)ydt at t 5 kT with the slope of the line seg-
ment connecting sample x(k 2 1) to sample x(k). The FIR transfer function of the back-
ward Euler differentiator is
H(z) 5  1 2 z21
T
 
5  z 2 1
Tz  
(5.5.3)
Thus H(z) has a pole at z 5 0 and a zero at z 5 1. Using Euler’s identity, the frequency 
response in this case is
H( f ) 5  1 2  exp(2j2f T )
T
5  j2 exp(2jf T )3
 exp(  jf T ) 2  exp(2jf T )
j2T
4
 
5  3
j2 sin(f T )
T
4 exp(2jf T )
 
(5.5.4)
A plot of the magnitude response of the backward Euler differentiator is shown in  
Figure 5.24 along with the ideal differentiator magnitude response. For normalized fre-
quencies in the range f0, .1g the magnitude response approximation is close, but then it 
begins to diverge for higher frequencies.
5.5
Phase quadrature
Backward Euler 
difference
%  
A_min = (r+1) by 1 vector containing denominator
%  
coefficients of minimum-phase part
%  
B_all = (s+1) by 1 vector containing numerator
%  
coefficients of allpass part
%  
A_all = (s+1) by 1 vector containing denominator
%  
coefficients of allpass part
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

378    Chapter 5  Filter Types and Characteristics
Since the backward Euler approximation is a linear-phase FIR filter of order m 5 1, 
it includes a delay of Ty2 or half a sample. Ignoring the effects of this delay, the remain-
ing phase response matches that of the differentiator exactly. This is a consequence of the 
fact that the backward Euler impulse response is
 
h 5 1
T
  f1, 21g 
(5.5.5)
Note that h(k) exhibits odd symmetry about the midpoint k 5 my2. Since H(z) is of odd 
order, it follows from Table 5.2 that the backward Euler differentiator is a type 4 linear-phase 
FIR filter with a phase offset of  5 y2, which is exactly what is needed for a differentiator.
To obtain an approximation that has a more accurate magnitude response, we can 
employ a higher-order type 4 linear-phase FIR filter. Using design techniques presented 
in Chapter 6, the following FIR filter of order m 5 5 is obtained.
y(k) 5 .0509x(k) 2 .1415x(k 2 1) 1 1.2732x(k 2 2)
 
21.2732x(k 2 3) 1 .1415x(k 2 4) 2 .0509x(k 2 5) 
(5.5.6)
Notice the odd symmetry in the coefficient vector b [ R6, which is the impulse response. 
A plot of the magnitude response of this fifth-order FIR filter is shown in Figure 5.25. 
Although it is not exact, it does show a marked improvement over the backward Euler 
differentiator, and a more accurate approximation can be obtained by increasing m. Tak-
ing the delay of 2.5 samples into account, the phase response with an phase offset of 
 5 y2 is exact.
5.5.2 Hilbert Transformer
Another filter that produces a steady state output that is in phase quadrature with a 
sinusoidal input is called a phase shifter or a Hilbert transformer. It is a filter that has the 
following desired frequency response.
 
Hd ( f ) 5 2j sgn ( f ), 0 # u f u # fsy2 
(5.5.7)
Hilbert transformer
0
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45 0.5
0
0.5
1
1.5
2
2.5
3
3.5
A(f)
Backward Euler
Ideal
f/fs
Figure 5.24:  
Magnitude 
Response of the 
Backward Euler 
Approximation to 
a Differentiator
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.5 Quadrature Filters     379
Here sgn denotes the sign function defined  sgn ( f ) 5 fyu f u for f Þ 0 and  sgn (0) 5 0. 
Thus
 
 sgn ( f ) 5
D  5
1,
f . 0
0,
f 5 0
21,
f , 0
 
(5.5.8)
Notice that there is a constant phase shift of /Hd( f ) 5 2y2 for 0 ,  f ,  fsy2. Like 
the ideal frequency selective filters, the Hilbert transformer cannot be realized with a 
causal system. This can be seen by applying the inverse DTFT (see Problem 5.27), which 
yields the following impulse response.
 
hd (k) 55
1 2  cos(k)
k
,
k Þ 0
0,
k 5 0
 
(5.5.9)
To obtain a causal linear-phase approximation using an FIR filter of order m, we 
include a delay of mTy2 in the desired frequency response.
 
Hh( f ) 5 2j sgn ( f ) exp(2jmf T ), 0 # u f ug , fsy2 
(5.5.10)
Since the factor j in (5.5.10) inserts a phase shift of 2y2, this suggests the use of a 
linear-phase filter from Table 5.2 that has a phase offset of  5 y2 and an amplitude 
response of Ar( f ) 5 21. Either a type 3 or a type 4 linear-phase filter could be used. As 
an illustration, suppose a type 3 linear-phase filter of order m 5 40 is designed using 
techniques from Chapter 6. The resulting magnitude response is shown in Figure 5.26. 
It is evident that this approximation to the Hilbert transformer is actually a wide band-
pass filter. This is because a type 3 linear-phase FIR filter has zeros at H(61) 5 0 which 
correspond to the two ends of the frequency range. As the filter order m increases, the 
passband can be made wider, but the gain will always be zero at DC and at fsy2.
In order to produce a pair of signals x1(k) and x2(k) that are in phase quadrature, 
suppose the input consists of a cosine of frequency f .
 
x(k) 5 cos(2f kT ) 
(5.5.11)
Sign function
0
0.5
0
0.5
1
1.5
2
2.5
3
3.5
A(f)
Ideal
FIR filter
f/fs
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45
Figure 5.25:  
Magnitude 
Response 
of a Type 4 
Linear-phase 
Approximation to 
a Differentiator 
Using a Filter of 
Order m 5 5
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

380    Chapter 5  Filter Types and Characteristics
The Hilbert transformer in (5.5.10) will shift the phase of x(k) by 2y2, but it will also 
delay the signal by mTy2. Thus the total phase shift of the causal Hilbert transformer is
 
 ( f ) 5 2
2 2  m f T 
(5.5.12)
Suppose x2(k) is produced by passing x(k) through a Hilbert transformer, Hh(z), as in 
Figure 5.27. To ensure that x1(k) and x2(k) are exactly y2 radians apart, x1(k) must be 
the output of an allpass filter with an identical delay of mTy2 so that the delays of x1(k) 
and x2(k) match. Recall that m is even, so this can be achieved with an integer delay of 
my2 samples using a filter with transfer function
 
D(z) 5 z2my2 
(5.5.13)
For the cosine input in (5.5.11), the steady-state values of x1(k) and x2(k) in Figure 5.27 
will then be delayed versions of a cosine and a sine as follows.
 
x1(k) 5 cosf2f(k 2 my2)Tg
 
(5.5.14a)
 
x2(k) 5 Ah( f ) sinf2f(k 2 my2)Tg 
(5.5.14b)
Figure 5.27:  
Generation of 
Phase-Quadrature 
Signals and a 
Half-band Output 
Using a Hilbert 
Transformer
x(k)
x1(k)
x2(k)
y(k)
z2m/2
Hilbert
transformer
j
1
Figure 5.26:  
Magnitude 
Response of a  
Type 3 Linear-phase 
Approximation to a 
Hilbert Transformer 
Using a Filter of 
Order m 5 40
f/fs
0
0.5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
A(f)
Ideal
FIR filter
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.5 Quadrature Filters     381
Note that the Hilbert transformer output x2(k) has a frequency-dependent amplitude, 
Ah( f ). Thus x2(k) is a phase-shifted version of x1(k) so long as Ah( f ) < 1.
Hilbert transformers are used in a number of applications in communications and 
speech processing. For example, the Hilbert transformer can be used as shown in the 
second half of Figure 5.27 to create a complex signal of the following form.
 
y(k) 5 x1(k) 1 jx2(k) 
(5.5.15)
Recall that if a signal is real, then its magnitude spectrum is an even function, and 
its phase spectrum is an odd function. When we generalize to complex signals as in 
(5.5.15), this symmetry constraint no longer applies. Indeed, when the Hilbert trans-
former amplitude response is ideal with Ah( f ) 5 1, the spectrum of the complex out-
put signal y(k) is
Y( f ) 5  X1( f ) 1 jX2( f )
 
5  fexp(2jmf T ) 1 jHh( f )gX( f ) 
(5.5.16)
From (5.5.10), jHh( f ) 5 sgn ( f ) exp(2jmf T ) where  sgn ( f ) 5 61. Thus the spectrum 
of y(k) is
 
Y( f ) 55
2 exp(2jmf T )X( f ) ,
0 , f , fsy2
X(0)  ,
f 5 0
0 ,
2 fsy2 , f , 0
 
(5.5.17)
Notice that the spectrum of the complex output y(k) is zero over the negative frequency 
range. Put another way, Y(z) 5 0 along the bottom half of the unit circle. For this reason, 
y(k) in Figure 5.27 is referred to as a half-band signal.
The complex signal y(k) is the discrete-equivalent of an analytic signal because its 
spectrum is zero for 2 fsy2 , f , 0. From (5.5.17) it is evident that y(k) contains all the 
information needed to reconstruct the original signal x(k), but it occupies only half of the 
bandwidth. As a result, y(k) can be transmitted more efficiently than x(k). Recall from 
the frequency shift property of the DTFT in Table 4.3 that if a signal is modulated by a 
complex exponential, this shifts its spectrum.
 
q(k) 5   exp( j2F0kT )y(k) 
(5.5.18)
 
Q( f ) 5  Y(f 2 F0)
 
(5.5.19)
Using this technique, several half-band signals can be translated to different regions 
of the spectrum and then transmitted simultaneously. This technique, which is referred to 
as frequency-division multiplexing, is discussed in Chapter 8.
5.5.3 Digital oscillator
A discrete-time system that generates a quadrature pair at a fixed frequency F0 is called 
a digital oscillator. A sinusoidal oscillator of frequency F0 will produce the following 
quadrature pair.
 
x1(k) 5 cos(2F0kT ) 
(5.5.20a)
 
x2(k) 5 sin(2F0kT ) 
(5.5.20b)
Complex signal
Half-band signal
Frequency-division 
multiplexing
Digital oscillator
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

382    Chapter 5  Filter Types and Characteristics
To construct a linear system whose output is x1(k), we express x1(k) as a linear combina-
tion of x1(k 2 1) and x2(k 2 1). For convenience, let  5 2F0T. Using the cosine of the 
sum trigonometric identity from Appendix 2,
x1(k) 5  cosf2F0(k 2 1)T 1 g
5  cosf2F0(k 2 1)Tg cos() 2  sinf2F0(k 2 1)Tg sin()
 
5  cos()x1(k 2 1) 2  sin()x2(k 2 1)
 
(5.5.21)
Applying a similar analysis to x2(k) using the sine of the sum trigonometric identity yields
 
x2(k) 5  sin()x1(k 2 1) 1  cos()x2(k 2 1) 
(5.5.22)
The quadrature pair in (5.5.20) satisfies the following vector difference equation.
 
3
x1(k)
x2(k)4 53
cos()
2sin()
sin()
cos() 4  3
x1(k 2 1)
x2(k 2 2)4 
(5.5.23)
 
8 
C(F0)
Note that the two-dimensional system in (5.5.23) has no input. Instead, it is a non-
zero initial condition, X(0) [ R2, that starts the oscillation. Evaluating (5.5.20) at k 5 0, 
the required initial condition is x1(0) 5 1 and x2(0) 5 0. Let X(k) [ R2 be a 2 3 1 column 
vector with elements x1(k) and x2(k). Then (5.5.23) and the initial condition can be writ-
ten in vector form as
 
X(k) 5 C(F0)X(k 2 1), X(0) 5 f1, 0gT 
(5.5.24)
The 2 3 2 coefficient matrix C(F0) is a rotation matrix. When the vector X(k 2 1) is 
multiplied by C(F0), it rotates X(k 2 1) counter-clockwise about the origin by the angle 
 5 2F0T to produce X(k). In this way, the solution to (5.5.24) starts at X(0) 5 f1, 0gT 
and traverses the unit circle counter-clockwise in the x2 versus x1 plane.
Once a quadrature pair is generated by a digital oscillator, it can be used to synthesize 
a more general periodic signal by post processing x1(k) and x2(k) to generate harmonics. 
To see how, consider the following classical family of orthogonal polynomials called the 
Chebyshev polynomials of the first kind. The first two Chebyshev polynomials of the first 
kind are T0(x) 5 1 and T1(k) 5 x. Subsequent polynomials are generated recursively as 
follows.
 
Ti(x) 5 2xTi21(x) 2 Ti 2 2(x), i $ 2 
(5.5.25)
The Chebyshev polynomials have many interesting properties including the following 
harmonic generation property.
 
Ti fcos()g 5 cos(i), i $ 0 
(5.5.26)
Since x1(k) is a cosine, this means that any even periodic function can be generated 
from x1(k) using a suitable linear combination of the Chebyshev polynomials of the first 
kind.
To generate an odd periodic function, we can use the Chebyshev polynomials of the 
second kind. The first two Chebyshev polynomials of the second kind are U0(x) 5 1 and 
U1(x) 5 2x. The recursive relationship for generating the remaining Chebyshev polyno-
mials is similar to (5.5.25), namely
 
Ui(x) 5 2xUi21(x) 2 Ui22(x), i $ 0 
(5.5.27)
Rotation matrix
Chebyshev 
polynomials, first kind
Chebyshev 
polynomials, second 
kind
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.5 Quadrature Filters     383
To generate odd harmonics using the Chebyshev polynomials of the second kind, both 
the cosine and the sine must be available as a quadrature pair.
 
sin()Ui21fcos()g 5 sin(i), i $ 1 
(5.5.28)
Suppose the frequency F0 V fs. To avoid aliasing, the ith harmonic must be below 
the folding frequency fd 5 fsy2. Thus the number of harmonics that can be generated 
without aliasing is
 
r , fs
2F0
 
(5.5.29)
A general periodic signal y(k) with period T0 5 1yF0 can be approximated with r har-
monics using the following truncated Fourier series.
 
y(k) 5 a0 1 o
r
i51
ai cos(2iF0T ) 1 bi sin(2iF0T ) 
(5.5.30)
Given the quadrature pair in (5.5.20) and the harmonic generation properties in 
(5.5.26) and (5.5.28), the periodic signal y(k) can be expressed as follows where fe(x) and 
fo(x) are polynomials.
 
y(k) 5 fefx1(k)g 1 x2(k)fofx1(k)g 
(5.5.31)
A block diagram that shows the digital oscillator and the post processing needed to form 
a general periodic output y(k) is shown in Figure 5.28. The polynomials fe(x) and fo(x) 
represent the even and odd parts of y(k), respectively. Using (5.5.30), they are formed 
from the Chebyshev polynomials as follows.
 
fe(x) 5  o
r
i50
aiTi(x)
 
(5.5.32)
 
fo(x) 5  o
r
i51
biUi 2 1(x) 
(5.5.33)
As an illustration of the use of a digital oscillator to generate an arbitrary peri-
odic waveform, suppose fs 5 1000 Hz and F0 5 25 Hz. Up to r 5 19 harmonics can be 
generated without aliasing. Suppose the coefficient vectors are a 5 f0, 1, .5, .25gT and 
b 5 f1, 2.5, .25gT, and N 5 200 points are computed. A plot of the resulting output y(k) 
versus the first quadrature signal x1(k) is shown in Figure 5.29. Also plotted is x2(k) ver-
sus x1(k), which generates a circle as expected. The closed curve corresponding to y(k) 
versus x1(k) shows that y(k) is indeed periodic with period F0 5 25 Hz.
In addition to the specialized quadrature filters presented here, a general two-stage 
quadrature filter of order 2m can be developed that meets both magnitude-response and 
phase-response specifications. The details of the design of quadrature filters are pre-
sented in Chapter 6.
Figure 5.28:  
Generation of a 
Periodic Output 
from a Quadrature 
Pair
x1(k)
x2(k)
x(0)
Digital
oscillator
3
y(k)
fe
fo
1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

384    Chapter 5  Filter Types and Characteristics
notch Filters and Resonators
5.6.1 notch Filters
The frequency selective filters in Section 5.2 all have the property that the stopbands are 
of nonzero length. There is a class of filters called notch filters where the stopband con-
sists of a single frequency. To define a design specification for a notch filter it is helpful 
to use the following continuous-frequency version of the unit impulse called a unit pulse.
 
1( f ) 5
D  5
1,
f 5 0
0,
f Þ 0
 
(5.6.1)
Note that 1( f ) is similar to the unit impulse, a( f ), except that it takes on a value of one 
at f 5 0. Given the unit pulse function in (5.6.1), a notch filter with a notch at F0 is a filter 
whose magnitude response is
 
Anotch( f ) 5
D  1 2 1(u f u 2 F0), 0 # u f u # fsy2 
(5.6.2)
Hence a notch filter is a filter designed to pass all frequencies except F0 with F0 
completely blocked by the filter. Note that no explicit constraint is placed on the phase 
response. Notch filters can be approximated with low-order IIR filters using techniques 
discussed in Chapter 7. Perhaps the simplest example of a notch filter is a filter designed 
to block DC, that is, a filter with a notch at F0 5 0 Hz. If a noncausal filter is permitted, 
then an exact DC-blocking filter is easily implemented. All one has to do is set the output 
y(k) equal to the input x(k) minus its mean.
For a causal realization of a DC notch filter, consider a first order IIR filter with the 
following structure.
 
HDC (z) 5 c(z 2 1)
z 2 r , 0 V  r , 1 
(5.6.3)
5.6
Notch filter
Unit pulse
Figure 5.29:  
A Numerical  
Example of the 
Quadrature Pair 
and a Periodic 
Output Generated 
from Them
22
21 20.5
21.5
0
1
0.5
1.5
2
22
21.5
21
20.5
0
0.5
1
1.5
2
x1
y(k)
y(k)
x2(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.6 Notch Filters and Resonators    385
Recall that along the unit circle, z 5 exp( j2f T ), so DC corresponds to z 5 1. Thus the 
zero of HDC(z) at z 5 1 ensures that the DC component of x(k) will be blocked. To ensure 
a passband gain of one, we can set HDC(21) 5 1 where z 5 21 corresponds to the fold-
ing frequency, fsy2. Setting HDC(21) 5 1 and solving for c yields
 
c 5 1 1 r
2
 
(5.6.4)
Notice that 0 V r , 1 implies c < 1. For an effective notch filter, it is also required that 
Anotch( f ) < 1 for f Þ F0. This can be achieved by placing the pole at z 5 r very close to 
the zero at z 5 1. Let Fc be the 3 dB cutoff frequency of the notch filter. That is, Fc is the 
frequency at which
 
A2
notch(Fc) 5 .5 
(5.6.5)
For a DC notch filter, Fc V fsy2. Consequently, cos(2FcT) < 1 and sin(2FcT) <  
2FcT. Recall also that r < 1. Using these approximations and (5.6.3), the square of the 
magnitude response at f 5 Fc is
A2
notch(Fc) 5  uc fexp(  j2FcT ) 2 1gu2
uexp(  j2FcT ) 2 ru
5  
c2hfcos(2FcT ) 2 1]2 1 sin2(2FcT )j
fcos(2FcT ) 2 r]2 1 sin2(2FcT )
 
<  
(2FcT)2
(1 2 r)2 1 (2Fc)2
 
(5.6.6)
Setting A2
notch(Fc) 5 .5 as in (5.6.5) then yields
 
(2FcT)2 < (1 2 r)2 
(5.6.7)
The bandwidth of the stopband or notch is DF 5 2Fc. Solving (5.6.7) for r and expressing 
the final result in terms of the notch bandwidth yields the following design formula for r.
 
r < 1 2 DF
fs
 
(5.6.8)
Thus to design a simple IIR filter that blocks DC with a notch of width DF, we use 
(5.6.8) to determine the pole radius r, and then (5.6.4) to determine the gain c.
Bandwidth
DC notch Filter
EXAMPLE 5.8
As an illustration of a notch filter that blocks DC, suppose fs 5 100 Hz, and con-
sider the sequence of three filters with notch bandwidths of DF 5 f.1, .2, .4g Hz. 
From (5.6.8) and (5.6.4),the corresponding poles and gains are
r 5 f.9969, .9937, .9874g
c 5 f.9984, .9969, .9937g
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

386    Chapter 5  Filter Types and Characteristics
22
21.5
21
20.5
0
0.5
1
1.5
2
0
0.2
0.4
0.6
0.8
1
1.2
F 5 .1 Hz
A(f)
f (Hz)
For most notch filters, the notch frequency is not F0 5 0 but is instead somewhere 
in the range 0 , F0 # fsy2. For a notch at frequency F0, there must be a zero on the unit 
circle at z 5  exp(j2F0T). Hence the radius of the zero is one and the angle is
 
0 5 2F0T  
(5.6.9)
Since z 5 exp(  j0) is complex, there will be a complex-conjugate pair of zeros at 
z 5 exp(6j0) and a matching complex-conjugate pair of poles at z 5 r exp(6j0) where 
0 V r , 1. Thus the general form of a notch filter is
 
Hnotch(z) 5 cfz 2  exp(j0)gfz 2  exp(2j0)g
fz 2 r exp(j0)gfz 2 r exp(2j0)g 
(5.6.10)
The poles and zeros occur in conjugate pairs, so the coefficients of Hnotch are real. Tech-
niques to determine c and r are presented in Chapter 7 along with design examples. Given 
the notch filter magnitude response specification in (5.6.2), notch filters with more than 
one notch frequency can realized by using a series configuration of individual notch 
filters, also called a cascade configuration. An implementation of a double-notch filter 
with notches at F0 and F1 using a series configuration of two notch filters is shown in  
Figure 5.31. If multiple notch frequencies are equally spaced to include a fundamental, 
F0, and its harmonics, then a special structure called an inverse comb filter can be used. 
The design of inverse comb filters is discussed in Chapter 7.
Inverse comb filter
Figure 5.30: Magnitude Responses of Notch Filters Blocking DC with Notches 
DF 5 h.4, .2, 1.j
Plots of the magnitude response are shown in Figure 5.30. Note that only 4% of the 
frequency range is plotted in order to more clearly see the notch. Although the notch 
filter was not designed to satisfy a phase response specification, because the pole and 
zero are so close together their phase contributions almost cancel except near DC. 
Consequently, the phase response of HDC(z) will be close to zero within the passband.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.6 Notch Filters and Resonators    387
5.6.2 Resonators
Resonators and notch filers form a complementary pair. Whereas an ideal notch filter 
is designed to stop a single frequency and pass all others, an ideal resonator is designed 
to pass a single frequency and stop all others. That is, it is designed to “resonate’’ at a 
specific frequency. The magnitude response specification of a resonator with resonant 
frequency F0 is
 
Ares( f ) 5
D  1(u f u 2 F0), 0 # u f u # fsy2 
(5.6.11)
If Anotch( f ) is the magnitude response of a notch filter with notch frequency F0, and 
Ares( f ) is the magnitude response of a similarly designed resonator with resonant fre-
quency F0, then
 
A2
notch( f ) 1 A2
res( f ) 5 1, 0 # u f u # fsy2 
(5.6.12)
In this case we say that Hnotch(z) and Hres(z) form a power-complementary pair. The com-
plementary pair relationship in (5.6.12) suggests that one way to design a resonator is
 
Hres(z) 5 1 2 Hnotch(z) 
(5.6.13)
If this approach is applied to the DC notch filter in (5.6.3), then after simplification using 
(5.6.4) this results in the following DC resonator.
 
Hdc(z) 5 .5(1 2 r)(z 1 1)
z 2 r
 
(5.6.14)
Notice that the coefficients of the numerator can be very small when r < 1. Direct substi-
tution reveals that Hdc(1) 5 1 and Hdc(21) 5 0.
An alternative approach to a general resonator with resonant frequency 0 , F0 # fsy2 
is to put poles at z 5 r exp(60) to create the resonance and zeros at z 5 61 to ensure a 
bandpass characteristic.
 
Hres(z) 5
c(z2 2 1)
fz 2 r exp(j0)gfz 2 r exp(2j0)g 
(5.6.15)
Techniques to determine c and r are presented in Chapter 7 together with examples. 
Given the resonator magnitude response specification in (5.6.11), resonators with more 
than one resonant frequency can realized by using a parallel configuration of individ-
ual resonators. An implementation of a double-resonator with resonance frequencies 
at F0 and F1 using a parallel configuration of two resonators is shown in Figure 5.32. 
If multiple resonator frequencies are equally spaced to include a fundamental, F0, and 
its harmonics, then a special structure called a comb filter can be used as described in 
Chapter 7.
Power-complementary 
pair
Comb filter
Figure 5.31:  
A Double-notch 
Filter with Notches 
at F0 and F1 
Using a Series 
Configuration of 
Two Notch Filters
x(k)
Notch
filter
F0
Notch
filter
F1
u(k)
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

388    Chapter 5  Filter Types and Characteristics
Figure 5.32:  
A Double-resonator 
with Resonance 
Frequencies 
at F0 and F1 
Using a Parallel 
Configuration of 
Two Resonators
x(k)
u1(k)
u2(k)
y(k)
Resonator
F1
Resonator
F0
1
Figure 5.33: Frequency Responses of a Power-Complementary Pair, a DC 
Resonator, and a DC Notch Filter with DF 5 fsy20
20.5
0
0.5
0
0.5
1
Resonator
Notch filter
A2sum
20.5
0
0.1
0.2
0.3
0.4
20.4 20.3 20.2 20.1
0.1
0.2
0.3
0.4
20.4 20.3 20.2 20.1
0.5
22
0
2
Resonator
Notch filter
A2(f)
f/fs
f/fs
(f)
Power-complementary Pair
EXAMPLE 5.9
As an illustration of a power-complementary pair, consider a DC resonator 
designed using (5.6.14). To more clearly see the shape of the resonance, suppose 
a fairly wide resonance bandwidth of DF 5 fsy20 is used, which represents 5% of 
the frequency range. From (5.6.8), this results in a pole radius of
r 5 .8429
Plots of the magnitude response and the phase response of the resulting DC 
resonator, Hdc(z), are shown in Figure 5.33. Also shown are the magnitude and 
phase responses of the complementary DC notch filter, HDC(z). Note how the 
magnitude responses form a power-complementary pair even though these filters 
are not ideal. It is also clear that the phase responses are flat for frequencies well 
away from the resonance and the notch.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.7 Narrowband Filters and Filter Banks    389
narrowband Filters and Filter Banks
5.7.1 narrowband Filters
Suppose a lowpass filter has a cutoff frequency of Fc. As the sampling frequency fs is 
increased, the normalized cutoff frequency, Fcyfs, will approach zero. Filters with very 
small normalized passbands are called narrowband filters. Narrowband filters have prac-
tical applications, but they can be challenging to design. An effective way to implement 
a narrowband filter is to convert it into a filter whose normalized cutoff frequency lies 
somewhere near the middle of the Nyquist range with Fcyfs < .25. Since the cutoff fre-
quency, Fc, is typically fixed by the application, this necessitates changing the sampling 
frequency fs. Suppose, however, that the sampled input x(k) is already available and must 
be used.
A brute-force analog approach to lowering the sampling rate is summarized in 
Figure 5.34. Here the input x(k) is first converted back to analog form, xa(t), using a 
DAC. Next xa(t) is resampled with an ADC at a lower sampling rate, Fs. This results in a 
new discrete-time signal, x0(k), that can be filtered by H0(z) to produce y0(k). The process 
is then reversed to restore the original sampling rate. First y0(k) is converted to ya(t) using 
a DAC, and then y(k) is obtained from ya(t) by sampling at the original sampling rate, fs, 
using an ADC. This analog approach does have the advantage that the new sampling rate 
Fs, seen by H0(z), can be arbitrary. However, it is an expensive, hardware-intensive, multi-
stage approach that requires four converters, and each converter introduces quantization 
noise.
Interestingly enough, the sampling rate fs can be changed with a purely discrete-
time system, a system that is linear but time-varying. When the sampling rate is reduced 
by an integer factor M, this is called to as a decimator. If the sampling rate is increased 
by an integer factor L, this is called an interpolator. More generally, the sampling rate 
can be changed by a factor LyM which is called a rational sampling rate converter. The 
design and application of sampling rate converters are presented in Chapter 8. There are 
a number of applications that arise when the sampling rate is allowed to change. This 
area of investigation, which is the focus of Chapter 8, is referred to as multirate signal 
processing.
The multirate application of interest here is the design of a narrowband filter. 
Suppose an integer decimator is used to reduce the sampling rate to Fs 5 fsyM. In order 
to move the cutoff frequency Fc to the middle of the Nyquist range, it is necessary that 
Fcyfs < .25. Thus the sampling rate reduction factor M can be computed as follows.
 
M 5 round1
fs
4Fc2 
(5.7.1)
5.7
Narrowband filter
Decimator
Interpolator
Multirate signal 
processing
Figure 5.34: Analog Multirate Design of a Narrowband Filter
ADC
ADC
DAC
DAC
x
xa
ya
y
x0
H0(z)
y0
Fs
ƒs
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

390    Chapter 5  Filter Types and Characteristics
The multirate narrowband filter design technique is summarized in Figure 5.35. First 
the input x(k) is down-sampled by a factor of M with a decimator. Next the down- 
sampled signal x0(k) is filtered by a filter H0(z) with cutoff frequency Fc < Fsy4, which is 
half the folding frequency. Finally the original sampling rate is restored by up-sampling 
y0(k) by the factor M using an interpolator to produce y(k).
Figure 5.35: Digital Multirate Design of a Narrowband Filter Using a Decimator and an 
Interpolator
x(k)
x0(k)
y0(k)
H0(z)
y(k)
M
M
Figure 5.36: Magnitude Response of a Multirate Narrowband Lowpass Filter 
of order m 5 80 Using Rate Conversion Factor M 5 10
0
0.5
0
0.2
0.4
0.6
0.8
1
1.2
A(f)
Ideal
Multirate
f/fs
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45
narrowband Filter
EXAMPLE 5.10
As an illustration of the design of a narrowband filter, suppose the original sam-
pling rate is fs 5 1000 Hz, and it is desired to design a lowpass filter with a cutoff 
frequency of Fc 5 25 Hz, which is 1/20 the folding frequency. From (5.7.1), the 
rate conversion factor for the decimator and the interpolator is
M 5 round3
1000
4(25)4 5 10
Using techniques presented in Chapter 6, a linear-phase windowed FIR filter 
H0(z) of order m 5 80 can be designed using the Hamming window. The resulting 
magnitude response of the multirate narrowband lowpass filter is shown in Figure 
5.36. It is evident that a close approximation to the desired response is obtained.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.7 Narrowband Filters and Filter Banks    391
In Chapter 8 it is shown that a sampling rate converter with a rate conversion factor of 
M requires a lowpass digital anti-aliasing or anti-imaging filter with a cutoff frequency of 
fsy(2M). Since this is itself a narrowband filter when M W 1, it would seem that we are still 
confronted with the problem of implementing a narrowband filter. However, when M W 1, 
the overall rate conversion factor M can be factored into a product of smaller integer 
factors.
 
M 5 M1 ? M2 Á Mp 
(5.7.2)
The sampling rate converter then can be implemented using a cascade configuration of p 
smaller sampling rate converters with rate conversion factors M1, M2, . . . Mp. In this way 
a narrowband filter with M W 1 can be realized using a series of less narrow filters. This 
multistage approach can be used, for example, when M . 10.
5.7.2 Filter Banks
In addition to narrowband filters with small cutoff frequencies, there are applications 
that make use of bandpass filters with narrow passbands. A filter bank is a parallel con-
nection of filters with non-overlapping passbands where the overall passband occupies 
the entire Nyquist range.
 
o
N21
i50
Ai( f ) 5 1, 0 # u f u # fsy2 
(5.7.3)
As an illustration, the magnitude responses of a filter bank consisting of four filters 
are shown in Figure 5.37. A parallel configuration of N filters in a filter bank can have 
either a common input or a common output. Consider the case when the N filters are 
driven by the same input x(k) as shown in Figure 5.38. Note that output yi(k) represents 
the part of the input x(k) whose spectrum lies in the ith subband. The configuration in 
Filter bank
Figure 5.37:  
Magnitude 
Responses of a 
Filter Bank
0
0.2
0.3
0.1
0.4
0.5
0.6
0.7
0.8
0.9
1
20.5
0
0.5
1
1.5
A(f)
A0
A1
A2
A3
A0
f/fs
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

392    Chapter 5  Filter Types and Characteristics
Figure 5.38 is referred to as an analysis filter bank because it decomposes the input into 
subsignals.
An important application of analysis filter banks arises when one attempts to send 
several low bandwidth signals simultaneously over a single high bandwidth communica-
tion channel. Suppose xi(k) is a bandlimited signal of bandwidth B that represents infor-
mation to be transmitted. Subsignal xi(k) occupies the part of the spectrum represented 
by 0 # u f u # B where it is assumed that B V  fs. Next suppose F0 $ 2B. Consider a new 
complex subsignal ui(k) that is constructed by modulating xi(k) with a complex exponen-
tial of frequency iF0.
 
ui(k) 5 exp(j2iF0kT )xi(k), 0 # i , N 
(5.7.4)
Recall from the frequency shift property of the DTFT that if xi(k) is modulated by 
the complex exponential, this shifts the spectrum of xi(k) to the right by iF0 Hz.
 
Ui( f ) 5 Xi(f 2 iF0), 0 # i , N 
(5.7.5)
Since F0 $ 2B, the shifted magnitude spectra, Ai 5 uUiu, of the subsignals will not overlap. 
In particular, the spectrum of Ui( f ) will be centered at iF0 with a radius of B as shown in 
Figure 5.39 for the case N 5 4. A composite input x(k) is then constructed by summing 
the modulated subsignals as follows.
 
x(k) 5 o
N21
i50
ui(k) 
(5.7.6)
Suppose the ith filter in an analysis filter bank has a magnitude response centered at iF0 
for 0 # i , N. When this filter bank is driven with the composite input x(k), the output 
yi(k) will be xi(k), but with its spectrum shifted to the right by iF0. Consequently, the orig-
inal subsignal xi(k) then can be recovered from yi(k) by modulating it with the complex 
conjugate of the complex exponential.
 
xi(k) < exp(2j2iF0kT )yi(k), 0 # i , N 
(5.7.7)
The extraction of xi(k) with an analysis filter bank using (5.7.6) is only approximate 
because the filters in the filter bank are not ideal. The technique of simultaneously trans-
mitting several low-bandwidth subsignals over a single high-bandwidth channel, using 
Analysis filter bank
Figure 5.38: A Four-
filter Analysis Filter 
Bank
y0(k)
y1(k)
y2(k)
x(k)
y3(k)
F0(z)
F1(z)
F2(z)
F3(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.7 Narrowband Filters and Filter Banks    393
different parts of the spectrum for each subsignal is called frequency-division multiplexing. 
It is also possible to interleave the time samples of subsignals when they are transmitted, 
and this is referred to as time-division multiplexing.
Another way to configure the parallel filters in a filter bank is with a common 
summed output as shown in Figure 5.40. Since the subsignals are combined to form a 
single output y(k), this is called a synthesis filter bank. Applications which employ both 
analysis and synthesis filter banks are discussed in Chapter 8.
Frequency-, time- 
division multiplexing
Synthesis filter bank
0
1
20.5
0
0.5
1
1.5
A(f)
f/fs
0.2
0.3
0.1
0.4
0.5
0.6
0.7
0.8
0.9
A0
A1
A2
A3
A0
Figure 5.39:  
Subsignal 
Magnitude 
Spectra, Ai 5 uUiu,  
Occupying 
Different Parts 
of the Nyquist 
Spectrum
Figure 5.40: A Five- 
filter Synthesis 
Filter Bank
x0(k)
x1(k)
x2(k)
x3(k)
x4(k)
1
1
1
y(k)
G0(z)
G1(z)
G2(z)
G3(z)
G4(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

394    Chapter 5  Filter Types and Characteristics
Adaptive Filters 
The final type of digital filter that we consider is an adaptive filter. Filter design specifi-
cations for adaptive filters are distinct from the others because they are not formulated in 
the frequency domain using magnitude and phase responses. Instead, the design specifi-
cations of an adaptive filter are expressed in the time domain.
5.8.1 Transversal Filters
The most common structure for an adaptive filter is a time-varying FIR filter, also called 
a transversal filter.
 
y(k) 5 o
m
i50
w(i)x(k 2 i) 
(5.8.1)
The (m 1 1) 3 1 coefficient vector w(k) is called the weight vector. Although the trans-
versal FIR structure is not as general as an IIR structure, it does have one important 
redeeming feature. As long as w(k) remains bounded, the transversal filter will be bounded- 
input bounded-output (BIBO) stable.
The adaptive filter design problem is formulated by provided the designer with a 
pair of time signals, the input x(k) and a desired output, d(k). There are several appli-
cation areas for adaptive filters, including system identification, channel equalization, 
signal prediction, and noise cancellation, all described in Chapter 9. They differ from 
one another mainly in the way the input and the desired output are formulated. A block 
diagram of an adaptive filter is shown in Figure 5.41.
The unusual notation with a diagonal arrow through the transversal filter block 
is symbolic of a needle on a dial, where turning the dial corresponds to adjusting 
the parameters of the transversal filter. The output of a transversal filter can be for-
mulated in a compact way by introducing the following vector of past inputs called 
a state vector.
 
u(k) 5
D  fx(k), x(k 2 1), Á , x(k 2 m)gT 
(5.8.2)
Given the (m 1 1) 3 1 weight vector w(k) and state vector u(k), the filter output is simply 
the dot product of w(k) with u(k).
 
y(k) 5 wT(k)u(k) 
(5.8.3)
5.8
Transversal filter
State vector 
Figure 5.41:  
An Adaptive Filter
x(k)
d(k)
y(k)
w(k)
e(k)
2
1
Update
algorithm
Transversal
filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.8 Adaptive Filters     395
To adjust the parameter vector w(k), an optimization procedure is used. Note from  
Figure 5.41 that the error signal, e(k), is the difference between the desired output d(k) 
and the filter output y(k).
 
e(k) 5 d(k) 2 y(k) 
(5.8.4)
Since the objective is to make the filter output closely follow the desired output, this is 
equivalent to minimizing the size of the error. In particular, the time-domain design spec-
ification for an adaptive filter is to minimize the expected value of the square of the error, 
also called the mean square error.
 
(w) 5
D  E fe2(k)g 
(5.8.5)
To find a weight vector that minimizes the mean square error, (w), an iterative 
optimization procedure is used. Suppose w(0) [ Rm11 is an initial guess. The method of 
steepest descent is used to search for an optimal w.
 
w(k 1 1) 5 w(k) 2 =fw(k)g, k $ 1 
(5.8.6)
Here  . 0 is the step size which must be sufficiently small to ensure convergence. The 
gradient vector =(w) 5 −(w)y−w represents the direction of increasing mean square 
error. For the purposes of computing the gradient, the instantaneous value of the squared 
error, e2(k), can be used. This method simplification leads to the following efficient update 
formula for the weights called the least mean square or LMS method (Widrow and  
Sterns, 1985).
 
w(k 1 1) 5 w(k) 1 2e(k)u(k), k $ 0 
(5.8.7)
A detailed treatment of the LMS method including a convergence analysis can be 
found in Chapter 9. It will be shown that there is a tradeoff in selecting  between the 
speed of convergence and the amount of residual noise in the steady-state solution. Tech-
niques where  is allowed to vary with k are also considered.
Error signal
Mean square error
LMS method
Adaptive Filter
EXAMPLE 5.11
To illustrate the use of an adaptive filter, consider the problem of identifying a 
linear discrete-time system from input-output measurements. Suppose the input 
x(k) consists of white noise uniformly distributed over the interval f21, 1g, and 
the desired output d(k) is produced by applying the input to the following IIR 
system with poles at z 5 .4 6 j.6, zeros at z 5 6.9, and a gain factor of two.
y(k) 5 2x(k) 2 1.62x(k 2 2) 1 .8y(k 2 1) 2 .52y(k 2 2)
Next suppose an adaptive transversal filter of order m 5 12 is used. If the step size 
is  5 .01 and N 5 1000 iterations are performed starting with an initial guess of 
w(0) 5 0, this results in an FIR model whose magnitude and phase responses 
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

396    Chapter 5  Filter Types and Characteristics
Figure 5.42: Magnitude and Phase Responses of the Adaptive Filter of Order  
m 5 12
A(f)
0
0.5
0
2
4
6
8
(a)
Desired Response
Adaptive Model
0
0.5
22
21
0
1
2
(b)
Desired Response
Adaptive Model
(f)
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
f/fs
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
f/fs
5.8.2 Pseudo-filters
Although the design specifications of an adaptive filter are specified in the time domain 
in the form of minimizing the mean square error, they can be recast in a more familiar 
frequency-domain form by using the notion of a pseudo-filter. Let F 5 h f0, f1, Á , fN 2 1j 
be a set of distinct frequencies in the range f0, fsy2g. Consider a composite input x(k) that 
consists of sinusoids at these frequencies.
 
x(k) 5 o
N21
i50
 cos(2fikT ) 
(5.8.8)
Next suppose that at frequency fi the desired gain is Ai, and the desired phase shift is i 
for 0 # i , N. Then the desired steady-state response to the input x(k) is
 
d(k) 5 o
N21
i50
CiAi cos(2fikT 1 i) 
(5.8.9)
are shown in Figure 5.42. Note that although there is some error, the fit is quite 
close for both the magnitude and the phase, in spite of the fact that the desired 
response was generated by a more general IIR system. The fit can be improved by 
increasing m , but at the expense of using a higher order model.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.9 GUI Modules and Case Studies     397
Here the constants Ci . 0 are relative weights which can be used by the designer to indi-
cate that one frequency is more important than another. Often uniform weighting is used 
with Ci 5 1 for 0 # i , N. However, if the fit to a desired magnitude or phase response 
at say fj is difficult to achieve, then frequency fj can be given increased weight in the opti-
mization procedure by increasing Cj. Since the filter that produced the desired response 
in (5.8.9) corresponds to a fictitious filter, it is referred to as a pseudo-filter. For causal 
filters with real coefficients, the desired magnitude and phase responses have to satisfy 
certain constraints if a close fit is to be achieved. Pseudo-filter constraints are discussed 
in Chapter 9.
GUi Modules and Case studies 
This section focuses on the characteristics of various types of digital filters. A graphical 
user interface module called g_  filters is introduced that allows users to interactively 
explore types and characteristics of digital filters without any need for programming. 
A case study programming example using the DSP Companion functions is then 
presented.
g_filters: Evaluation of Digital Filter Characteristics 
The DSP Companion includes a GUI module called g_filters that allows the user to 
construct filters from design specifications and also evaluate filter realization structures 
and coefficient quantization effects, all without any need for programming. GUI module 
g_filters features a display screen with tiled windows as shown in Figure 5.43. The design 
features of g_filters are summarized in Table 5.3.
The Block diagram window in the upper left-hand corner contains a block diagram 
of the filter under investigation which can be an FIR filter, an IIR filter, or a user-de-
fined filter. The FIR filters are designed with the windowing method discussed in Chapter 
6, while the IIR filters are Butterworth filters created with the bilinear transformation 
method discussed in Chapter 7. The following transfer function is used where ai 5 0 for 
the FIR filters.
 
H(z) 5 b0 1 b1z21 1 Á bmz2m
1 1 a1z21 1 Á 1 anz2n 
(5.9.1)
The Parameters window below the block diagram displays edit boxes containing the 
filter parameters. The contents of each edit box can be directly modified by the user 
with the changes activated with the Enter key. Parameters F0, F1, B, and fs are the lower 
cutoff frequency, upper cutoff frequency, transition bandwidth, and sampling frequency, 
respectively. The lowpass filter uses cutoff frequency F0, the highpass filter uses cutoff 
frequency F1, and the bandpass and bandstop filters use both F0 and F1. The parame-
ters deltap 5 p and deltas 5 s specify the passband ripple and stopband attenuation, 
respectively.
The Type and View windows in the upper-right corner of the screen allow the user 
to select both the type of filter and the viewing mode. There are two categories of filter 
types. First the user must select either an FIR filter or an IIR filter. Within each of these 
types, the user can then select the basic frequency-selective filter type: lowpass, highpass, 
Pseudo-filter
5.9
GUi Module
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

398    Chapter 5  Filter Types and Characteristics
Select Type
Select View
Slider Bar
Edit Parameters
0
100
0
0.2
0.4
0.6
0.8
1
200
300
400
500
600
700
800
900
1000
0
0.5
1
1.5
1
0.5
0
Direct Butterworth IIR Filter: n 5 18, m 5 18, c 5 32, q 5 0.000488281, emax 5 0.10466
f (Hz)
A(f)
Unquantized
Quantized, N 5 17
Specifications
g_filters
x(k)
H(z)
y(k)
Figure 5.43: Display Screen for GUI Module g_filters
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.9 GUI Modules and Case Studies     399
bandpass, or bandstop. There is also a user-defined filter whose parameters are imported 
from a MAT file that contains a, b, and the sampling frequency fs. The View options 
include magnitude responses, phase responses, pole-zero plots, impulse responses, and 
the coefficient quantizer input-output characteristic. The dB check box toggles between 
logarithmic (dB) and linear displays of the magnitude response. When dB is checked, the 
passband ripple and stopband attenuation factors in the Parameter window also change 
to their logarithmic equivalents, Ap and As.
Just below the Type and View windows is a horizontal slider bar that controls N, the 
number of bits of precision used for coefficient quantization. To determine the quantiza-
tion level, first cmax is computed such that
 
cmax 5  max hua1u, Á , uanu, ub0u, Á , ubmuj 
(5.9.2)
The scale factor c is then set to the next higher power of two using the MATLAB  
nextpow2 function.
 
c 5
 nextpow2 (cmax) 
(5.9.3)
A fixed-point representation of the filter coefficients a and b uses M 5 log2(c) bits for 
the integer part, and N 2 M bits for the fraction part. This corresponds to the following 
coefficient quantization level.
 
q 5
c
2N 2 1 
(5.9.4)
As the number of bits of precision used to represent a and b decreases, the filter perfor-
mance begins to deteriorate. Finite precision effects for FIR filters and IIR filters are 
discussed in detail at the ends of Chapters 6 and 7, respectively.
All of the view options, except the quantizer characteristic, display two cases for 
comparison. The first is a double precision floating point filter that approximates the 
unquantized case, and the second is a fixed-point N-bit quantized filter using coefficient 
quantization. Higher order IIR filters can become unstable for small values of N. When 
this happens, the migration of the quantized poles outside the unit circle can be viewed 
directly using the pole-zero plot option. The Plot window along the bottom half of the 
screen shows the selected view.
The Menu bar at the top of the screen includes several menu options. The Export 
option is used to save a, b, x, y, fs to a MAT file. Results exported from one GUI module 
Coefficient 
quantization level
Table 5.3: Features of 
GUI Module g_filters
item 
Variables 
Block diagram
x(k), y(k)
Edit parameters 
F0, F1, B, fs, p, s
Filter type
lowpass, highpass, bandpass, bandstop, import
Plot view
magnitude response, phase response, pole-zero plot, impulse response, 
quantization characteristic
Slider
bits for coefficient quantization N
Check boxes
FIR/IIR filter, linear/dB scale
Menu buttons
realization, export, caliper, print, help, exit
Import
a, b, fs
Export
a, b, x, y, fs
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

400    Chapter 5  Filter Types and Characteristics
can be imported into other GUI modules for additional processing. The Caliper option 
allows the user to measure any point on the current plot by moving the mouse crosshairs 
to that point and clicking. The Realization option allows the user to choose between 
direct and cascade form realization structures. The cascade realization implements the n
th order filter using a series of L 5
 ceil hNy2j second-order filters.
 
H(z) 5 b0H1(z) Á HL(z) 
(5.9.5)
The cascade realization has superior numerical properties compared to the direct reali-
zation, and this can be verified by comparing the two while when the number of bits of 
precision is allowed to vary. The Print option sends the GUI screen to a printer or a file. 
Finally, the Help option provides the user with some helpful suggestions on how to use 
module g_filters effectively.
Highpass Elliptic Filter
To illustrate the use of filter design specifications and the effects of coefficient quantiza-
tion, suppose fs 5 200 Hz, and consider the problem of constructing a highpass digital 
filter to meet the following design specifications.
 
Fs 5  40 Hz 
(5.9.6a)
 
Fp 5  42 Hz 
(5.9.6b)
 
p 5  0.05
 
(5.9.6c)
 
s 5  0.05
 
(5.9.6d)
There are many filters that can meet or exceed these specifications. Design techniques 
for FIR filters are discussed in Chapter 6 and for IIR filters are discussed in Chapter 7. 
Notice that the width of the transition band is relatively small at
B 5  uFp 2 Fsu
 
5 2 Hz
 
(5.9.7)
As B, p, and s are decreased, the required filter order increases. As we shall see in Chap-
ter 7, the filter with the smallest order that still meets the specifications is an elliptic filter. 
An elliptic filter is an IIR filter that is optimal in the sense that the magnitude response 
contains ripples of equal amplitude in both the passband and the stopband. The coef-
ficients of an elliptic filter can be obtained by using the DSP Companion function  
f_ellipticz presented in Chapter 7.
When case5_1 is run, it produces an elliptic filter of order n 5 6. It then computes and 
plots two magnitude responses as shown in Figure 5.44. The second magnitude response 
is that of a quantized elliptic highpass filter using coefficient quantization with a scale 
factor of c 5 4 and N 5 10 bits. Note that whereas the unquantized (double precision 
floating point) filter meets the design specifications, the quantized filter clearly does not.
The loss of fidelity of the magnitude response can be attributed to the fact that the 
poles and zeros of the quantized filter tend to move from their optimal positions as the 
quantization level in f_quant increases. Function case5_1 generates four plots of the poles 
and zeros corresponding to different levels of quantization as shown in Figure 5.45. 
Inspection reveals that both the poles and the zeros move significantly as the number of 
bits for coefficient quantization decreases from N 5 10 to N 5 6. This can be attributed 
to the fact that the roots of a polynomial are very sensitive to the values of the polyno-
mial coefficients. For the final case of N 5 6, the poles have migrated outside the unit 
circle, which means that this implementation does not even have a frequency response 
because the filter has become unstable.
Case Study 5.1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.9 GUI Modules and Case Studies     401
Figure 5.44:  
Magnitude 
Responses of 
Unquantized and 
Quantized Elliptic 
Highpass Filters of 
Order n 5 6
0
20
30
10
40
50
60
70
80
90
100
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
f (Hz)
A(f)
Unquantized
Quantized, N 5 10
Figure 5.45:  
Pole-zero Plots of 
Elliptic Highpass 
Filters of Order 
n 5 6 Using 
Quantized and 
Unquantized 
Coefficients
21.5
0
1.5
21.5
0
1.5
X
X
X
X
X
X
O
O
O
O
O
O
21.5
0
1.5
21.5
0
1.5
X
X
X
X
X
X
O
O
O
O
O
O
N 5 10
N 5 6
N 5 8
21.5
0
1.5
21.5
0
1.5
X
X
X
X
X
X
O
O
O
O
O
O
21.5
0
1.5
21.5
0
1.5
X
X
X
X
X
X
O
O
O
O
O
O
Re(z)
Re(z)
Re(z)
Re(z)
Im(z)
Im(z)
Im(z)
Im(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

402    Chapter 5  Filter Types and Characteristics
Chapter summary
Learning outcomes
This chapter was designed to provide the student with an opportunity to achieve the 
learning outcomes summarized in Table 5.4.
Magnitude Response specifications
Chapter 5 focused on filter types and characteristics. Both FIR and IIR filters were 
examined. The following IIR transfer function reduces to an FIR transfer function when 
ai 5 0 for 1 # i # n.
 
H(z) 5 b0 1 b1z21 1 Á 1 bmz2m
1 1 a1z21 1 Á 1 anz2n  
(5.10.1)
Filter design specifications are usually formulated in terms of a desired magnitude 
response, A( f ) 5 uH( f )u. The basic types of frequency-selective filters are lowpass, high-
pass, bandpass, and bandstop filters. For a lowpass filter, the design specifications are as 
follows.
 
1 2 p #  A( f ) # 1 1 p, 0 # f # Fp 
(5.10.2a)
 
0 #  A( f ) # s, Fs # f # fsy2
 
(5.10.2b)
Here (5.10.2a) is the passband specification, and (5.10.2b) is the stopband specification. 
For the passband, 0 , Fp , fsy2 is the passband cutoff frequency and p . 0 is the pass-
band ripple factor. For the stopband, Fp , Fs , fsy2 is the stopband cutoff frequency, 
and s . 0 is the stopband attenuation factor. Left unspecified is the band of frequencies 
5.10
Lowpass specification
Passband ripple
Stopband attenuation
num.
Learning outcome 
sec.
1
Know how to specify the design characteristics of the four basic frequency- 
selective filters
5.2
2
Be able to go back and forth between linear and logarithmic design 
specifications 
5.2
3
Understand what a linear-phase filter is and what kind of FIR impulse 
response is required 
5.3
4
Know how to implement a noncausal zero-phase filter 
5.3
5
Know how to decompose a general transfer function into its minimum-phase 
and all pass factors 
5.4
6
Understand what it means for signals to be in phase quadrature and how to 
generate them with a digital oscillator 
5.5
7
Know what functions notch filters, resonators, and comb filters are designed 
to perform 
5.6
8
Understand what narrowband filters are and how sampling rate converters are 
used to design them 
5.7
9
Know how analysis and synthesis filter banks are constructed and applied 
5.7
10
Understand the differences between adaptive and fixed filters 
5.8 
11 
Be able to use the GUI module g_filters to explore filter design specifications 
and characteristics including finite precision effects 
5.9
Table 5.4: Learning 
Outcomes for  
Chapter 5
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.10 Chapter Summary    403
between the passband and the stopband which is called the transition band. The width of 
the transition band is
 
B 5 uFs 2 Fpu 
(5.10.3)
As the transition bandwidth B, passband ripple p, and stopband attenuation s approach 
zero, the order of the filter required to meet the specifications approaches infinity. The 
limiting special case of B 5 0, p 5 0, and s 5 0 is an ideal lowpass filter. The second 
basic type is a highpass filter. Finally, bandpass filters have two stopbands bracketing a 
passband, and bandstop filters have two passbands bracketing a stopband. The filter mag-
nitude response is often represented using a logarithmic scale of decibels (dB) as follows.
 
A( f ) 5 20 log10 {uH( f )u} dB 
(5.10.4)
The ripple and attenuation factors, p and s, have logarithmic equivalents, Ap and As, 
that are expressed in units of dB. The logarithmic scale can be useful for showing the 
degree of attenuation in the stopband.
Phase Response specifications
Often the desired phase response of a filter is left unspecified. One noteworthy excep-
tion is the design of linear-phase filters. Linear-phase filters with a phase response of 
 ( f ) 5 22f  delay all spectral components of the input by the constant , and there-
fore do not distort the spectral components that pass through the filter. Although a linear 
phase characteristic can be approximated in the passband with an IIR Bessel filter, the 
simplest way to design an exact linear-phase filter is to use an mth order FIR filter with 
an impulse response that satisfies the following linear-phase symmetry constraint.
 
h(k) 5 6h(m 2 k), 0 # k # m 
(5.10.5)
The symmetry constraint in (5.10.5) yields a filter with a constant group delay of  
 5 mTy2. It is a direct impulse response constraint on the FIR coefficients because 
for an FIR filter the nonzero part of the impulse response is specified by the numerator 
coefficients.
 
h 5 hb0, b1, Á , bm, 0, 0, Á j 
(5.10.6)
If the plus sign is used in (5.10.6), the impulse response h(k) is a palindrome that 
exhibits even symmetry about the midpoint k 5 my2, otherwise it exhibits odd symmetry. 
There are four types of linear-phase FIR filters, depending on whether the symmetry is 
even or odd and whether the filter order m is even or odd. The most general linear-phase 
filter is a type 1 filter with even symmetry and even order. The other three filter types have 
zeros at one or both ends of the frequency range and are sometimes used for specialized 
applications. The zeros of a linear-phase FIR filter that are not on the unit circle occur 
in groups of four because for every zero at z 5 r exp(j), there is a reciprocal zero at 
z 5 r21 exp( j). A special case of a linear-phase filter is a zero-phase filter, which is a 
noncausal filter that must be implemented offline.
Every IIR transfer function H(z) has a minimum-phase form, Hmin(z), whose magni-
tude response is the same as that of H(z), but whose phase response has the least amount 
of phase lag possible. The minimum-phase form of H(z) can be obtained as follows.
 
Hmin(z) 5 H21
all (z)H(z) 
(5.10.7)
Here Hall(z) is a allpass filter that is constructed from the zeros of H(z) that lie outside 
the unit circle. An allpass filter passes all spectral components equally with a magnitude 
response of Aall( f ) 5 1.
Transition band
Logarithmic scale
Linear-phase filters
Symmetry constraint
FIR
Impulse response
Minimum-phase form
Allpass filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

404    Chapter 5  Filter Types and Characteristics
Filter Types
In addition to the four basic frequency-selective filters, there are a wide variety of special-
ized filters. Filters that have a steady-state output that is shifted by a quarter of a cycle 
when a sinusoidal input is applied are quadrature filters. For example, a differentiator is 
a quadrature filter. Another important example of a quadrature filter is a Hilbert trans-
former whose frequency response is
 
Hh( f ) 5 2j sgn ( f ), 0 # u f u # fsy2 
(5.10.8)
With the help of a Hilbert transformer, a complex signal whose spectrum occupies only 
half the bandwidth of the original signal can be generated.
Another useful class of filters includes notch filters and resonators. A notch filter is a 
filter that is designed to block a single isolated frequency F0 and pass all others.
 
Hnotch( f ) 5 1 2 1(f 2 F0) 
(5.10.9)
A resonator, by contrast, is the dual of a notch filter. It passes a single frequency F0 
and blocks all others.
 
Hres( f ) 5 1(f 2 F0) 
(5.10.10)
Notch filters and resonators can be implemented using low-order IIR filters. Notch filters 
with multiple notches can be realized using a cascade configuration of individual notch 
filters. Likewise, resonators with multiple resonant frequency can be realized using a par-
allel configuration of individual resonators. Resonators and notch filters with a complete 
set of equally spaced frequencies are called comb filters.
A narrowband filter is a filter whose normalized passband satisfies DF V fs. Narrow-
band filters can be effectively implemented by using sampling rate converters that lower 
the sampling rate to Fs 5 fsyM where FcyFs < .25. A sampling rate converter that lowers 
the sampling rate is a decimator, and one that raises the sampling rate is an interpolator.
Bandpass filters with narrow passbands are used in parallel filter banks where the 
entire Nyquist spectrum is partitioned into a set of non-overlapping magnitude responses.
 
o
N21
i50
Ai( f ) 5 1 
(5.10.11)
Filter banks with a common input are called analysis banks because they decompose a 
signal into its parts, and filter banks that produce a common summed output are called 
synthesis banks because they reconstruct a signal from it parts. By modulating bandlim-
ited subsignals with complex exponentials and using analysis filter banks, several low 
bandwidth signals can be transmitted simultaneously over a single high bandwidth chan-
nel. This process is known as frequency-division multiplexing.
Adaptive filters are distinct from the other filters in a number of ways. They are 
time-varying discrete-time systems, and the design specifications for an adaptive filter are 
expressed in the time domain rather than the frequency domain. An adaptive transversal 
filter is an FIR filter whose weights are updated iteratively to minimize the mean square 
error between the filter output y(k) and a desired output d(k). Adaptive filters find wide-
spread application in areas such as system identification, inverse filtering or equalization, 
signal prediction, and noise cancellation.
Design techniques and applications for the many different filter types are considered 
in detail in the remaining chapters of this text.
Quadrature filter
Hilbert transformer
Notch filter
Resonator
Narrowband filter
Decimator
Interpolator
Filter bank
Frequency-division 
multiplexing
Adaptive filters 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.11 Problems    405
GUi Module
The DSP Companion includes a GUI module called g_filters that allows the user to con-
struct filters from design specifications and examine the detrimental effects of coefficient 
quantization without any need for programming.
Problems
The problems are divided into Analysis and Design problems that can be solved by 
hand or with a calculator, GUI Simulation problems that are solved using GUI module 
g_filters, and MATLAB Computation problems that require a user program. Solutions 
to selected problems can be accessed with the DSP Companion driver program, g_dsp. 
Students are encouraged to use these problems, which are identified with a 
, as a check 
on their understanding of the material.
5.11.1 Analysis and Design
section 5.2: Frequency-selective Filters
5.1 
Consider the following first order IIR filter.
H(z) 5 .4(1 2 z21)
1 1 .2z21
(a) Compute and sketch the magnitude response A( f ).
(b) What type of filter is this (lowpass, highpass, bandpass, bandstop)?
(c) Suppose Fp 5 .4fs. Find the passband ripple p.
(d) Suppose Fs 5 .2fs. Find the stopband attenuation s.
5.2 
A bandpass filter has a sampling frequency of fs 5 2000 Hz and satisfies the fol-
lowing design specifications.
fFs1, Fp1, Fp2, Fs2, p, sg 5 f200, 300, 600, 700, .15, .05g
(a) Find the logarithmic passband ripple, Ap.
(b) Find the logarithmic stopband attenuation, As.
(c) Using a logarithmic scale, sketch the shaded passband and stopband regions 
that A( f ) must lie within.
5.3 
A bandstop filter has a sampling frequency of fs 5 200 Hz and satisfies the follow-
ing design specifications.
fFp1, Fs1, Fs2, Fp2, Ap, Asg 5 f30, 40, 60, 80, 2, 30g
(a) Find the linear passband ripple, p.
(b) Find the linear stopband attenuation, s.
(c) Using a linear scale, sketch the shaded passband and stopband regions that 
A( f ) must lie within.
5.4 
Suppose H(z) is a stable filter with A( f ) 5 0 for .1 # ufyfsu # .2. Show that H(z) is 
not causal.
5.11
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

406    Chapter 5  Filter Types and Characteristics
section 5.3: Linear-phase and Zero-phase Filters
5.5 
Consider the following FIR filter of order M 2 1 known as a running average filter.
H(z) 5 1 1 z21 1 Á 1 z2(M 2 1)
M
(a) Find the impulse response of this filter.
(b) Is this a linear-phase filter? If so, what type?
(c) Find the group delay of this filter.
5.6 
A linear-phase FIR filter H(z) of order m 5 8 has zeros at z 5 6j.5 and z 5 6.8.
(a) Find the remaining zeros of H(z) and sketch the poles and zeros in the com-
plex plane.
(b) The DC gain of the filter is 2. Find the filter transfer function H(z).
(c) Suppose the input signal gets delayed by 20 ms as it passes through this filter. 
What is the sampling frequency, fs?
5.7 
Consider a type 1 FIR linear-phase filter of order m 5 2 with coefficient vector 
b 5 f1, 1, 1gT.
(a) Find the transfer function, H(z).
(b) Find the amplitude response, Ar( f ).
(c) Find the zeros of H(z).
5.8 
Consider a type 2 FIR linear-phase filter of order m 5 1 with coefficient vector 
b 5 f1, 1gT.
(a) Find the transfer function, H(z).
(b) Find the amplitude response, Ar( f ).
(c) Find the zeros of H(z).
5.9 
Consider a type 3 FIR linear-phase filter of order m 5 2 with coefficient vector 
b 5 f1, 0, 21gT.
(a) Find the transfer function, H(z).
(b) Find the amplitude response, Ar( f ).
(c) Find the zeros of H(z).
5.10 Consider a type 4 FIR linear-phase filter of order m 5 1 with coefficient vector 
b 5 f1, 21gT.
(a) Find the transfer function, H(z).
(b) Find the amplitude response, Ar( f ).
(c) Find the zeros of H(z).
5.11 Consider the following FIR filter.
H(z) 5 1 1 2z21 1 3z22 2 3z23 2 2z24 2 z25
(a) Is this a linear-phase filter? If so, what is the type?
(b) Sketch a signal flow graph showing a direct-form II realization of H(z) as in 
Section 5.1.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.11 Problems    407
5.12 Consider the following FIR filter.
H(z) 5 1 1 z21 2 5z22 1 z23 2 6z24
(a) Is this a linear-phase filter? If so, what is the type?
(b) Sketch a signal flow graph showing a direct-form II realization of H(z) as in 
Section 5.1.
(c) Using the MATLAB function roots find the zeros of H(z). Then sketch a sig-
nal flow graph showing a cascade form realization of H(z).
5.13 Let H(z) be an arbitrary FIR transfer function of order m. Show that H(z) can be 
written as a sum of two linear-phase transfer functions He(z) and Ho(z) where he(k) 
exhibits even symmetry about k 5 my2 and ho(k) exhibits odd symmetry about 
k 5 my2. Hint: Add and subtract h(m 2 k).
H(z) 5 He(z) 1 Ho(z)
5.14 Recall from Table 5.1 that linear-phase FIR filters of types 2 through 4 have zeros 
at z 5 21 or z 5 1 or both. A type 1 linear-phase FIR filter is more general.
(a) Show that for a type 1 linear-phase FIR filter, symmetry constraint (5.3.8) 
does not imply that H(z) has a zero at z 5 21.
(b) Show that for a type 1 linear-phase FIR filter, symmetry constraint (5.3.8) 
does not imply that H(z) has a zero at z 5 1.
5.15 This question focuses on the concept of the amplitude response of a filter.
(a) Show how to compute the magnitude response from the amplitude response.
(b) Suppose the magnitude response equals the amplitude response for 0 # f # F0, 
but for f .  F0 they differ. What happens to the phase response at f 5 F0?
5.16 Suppose H(z) is a type 2 linear-phase FIR filter.
H(z) 5 c0 1 c1z21 1 c1z22 1 c0z23
(a) Find the amplitude response of this filter.
(b) Find the phase offset, , and group delay, D( f ), of this filter.
5.17 Suppose H(z) is a type 4 linear-phase FIR filter.
H(z) 5 c0 1 c1z21 2 c1z22 2 c0z23
(a) Find the amplitude response of this filter.
(b) Find the phase offset, , and group delay, D( f ), of this filter.
5.18 Suppose the impulse response of an FIR filter of order m 5 5 is as follows where 
the X terms are to be determined.
h 5 f2, 4, 3, X, X, Xg
(a) Assuming H(z) is a linear-phase filter, find the complete impulse response. If 
there are multiple solutions, find each of them.
(b) For each solution in part (a), indicate the linear-phase FIR filter type.
(c) For each solution in part (a), find the phase offset, , and the group delay, 
D( f ).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

408    Chapter 5  Filter Types and Characteristics
5.19 Consider the following running average filter.
H(z) 5 1
10o
9
i50
z2i
(a) Write down the difference equation for this filter.
(b) Convert this filter to a noncausal zero-phase filter. That is, write down the 
difference equations for the zero-phase version of the running average filter.  
You can use coefficients fi 5 Ï1y10 in Algorithm 5.1.
section 5.4: Minimum-phase and Allpass Filters
5.20 Consider the following IIR filter.
H(z) 5 2(z 1 1.25)(z2 1 .25)
z(z2 2 .81)
(a) Find the minimum-phase version of this system, and sketch its poles and zeros.
(b) Find the maximum-phase version of this system, and sketch its poles and 
zeros.
(c) How many transfer functions with real coefficients have the same magnitude 
response as H(z)?
5.21 The following IIR filter has two parameters  and . For what values of these 
parameters is this an allpass filter?
H(z) 5 1 1 3z21 1 ( 1 )z22 1 2z23
2 1 ( 2 )z21 1 3z22 1 z23
5.22 Consider the following IIR filter.
H(z) 5 10(z2 2 4)(z2 1 .25)
(z2 1 .64)(z2 2 .16)
(a) Find Hmin(z), the minimum-phase version of H(z).
(b) Sketch the poles and zeros of Hmin(z).
(c) Find an allpass filter Hall(z) such that H(z) 5 Hall(z)Hmin(z).
(d) Sketch the poles and zeros of Hall(z).
5.23 Let H(z) be a nonzero linear-phase FIR filter of order m 5 2.
(a) Is it possible for H(z) to be a minimum-phase filter? If so, construct an exam-
ple. If not, why not?
(b) Is it possible for H(z) to be an allpass filter? If so, construct an example. If 
not, why not?
5.24 Suppose H(z) is a filter with input x(k) and output y(k) whose magnitude response 
satisfies the following constraint.
A( f ) # 1, u f u # fsy2
(a) Show that uY( f )u # uX( f )u.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.11 Problems    409
(b) Use Parseval’s identity to show that H(z) is a passive system. That is, show that 
the energy of y(k) is less than or equal to the energy of x(k).
o
`
k52`
uy(k)u2 # o
`
k52`
ux(k)u2
5.25 Suppose H(z) is an allpass filter with input x(k) and output y(k) whose magnitude 
response satisfies the following constraint.
A( f ) 5 1, u f u # fsy2
(a) Show that uY( f )u 5 uX( f )u.
(b) Use Parseval’s identity to show that H(z) is a lossless system. That is, show that 
the energy of y(k) is equal to the energy of x(k).
o
`
k52`
uy(k)u2 5 o
`
k52`
ux(k)u2
5.26 Consider the following IIR filter.
H(z) 5 2z2 1 5z 1 2
z2 2 1
(a) Find the minimum-phase form of H(z).
(b) Find a magnitude equalizer G(z) such that G(z)H(z) is an allpass filter with 
magnitude response A( f ) 5 1.
section 5.5: Quadrature Filters 
5.27 An ideal Hilbert transformer has the following frequency response.
Hd ( f ) 5 2j sgn ( f ), 0 # u f u , fsy2
Using the inverse DTFT, show that the impulse response of an ideal Hilbert trans-
former is
  
hd (k) 55
1 2  cos(k)
k
,
k Þ 0
0,
k 5 0
5.28 Let X(k) 5 fx1(k), x2(k)gT. A digital oscillator that produces two sinusoidal outputs 
x1(k) and x2(k) that are in phase quadrature can be obtained using a first-order 
two-dimensional system of the following form.
X(k) 5 AX(k 2 1), X(0) 5 c
(a) Find a coefficient matrix A that produces an oscillator with frequency F0 5 .3fs.
(b) Find an initial condition vector c that produces the solution
X(k) 53
 cos(.6k)
 sin(.6k)4
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

410    Chapter 5  Filter Types and Characteristics
(c) Find a coefficient matrix A and an initial condition vector c that produces the 
solution
X(k) 53
d cos(2F0kT 1 )
d sin(2F0kT 1 )4
5.29 Suppose the following quadrature pair of sinusoidal signals with frequency F0 and 
unit amplitude is available.
X(k) 53
 cos(2F0kT )
 sin(2F0kT )4
(a) Find the Chebyshev polynomials of the first kind Ti(x) for 0 # i # 3.
(b) Find the Chebyshev polynomials of the second kind Ui(x) for 0 # i # 3.
(c) Let X(k) 5 fx1(k), x2(k)gT. Find polynomials f  and g such that
f  fx1(k)g 1 x2(k)gfx1(k)g 5  cos3(2F0kT ) 1 3 sin2(2F0kT )
section 5.6: notch Filters and Resonators 
5.30 The general form for a notch filter with a notch at F0 Þ 0 is given in (5.6.10) where 
0 5 2F0T.
Hnotch(z) 5 cfz 2  exp(j0)gfz 2  exp(2j0)g
fz 2 r exp(j0))gfz 2 r exp(2j0)g
(a) Rewrite Hnotch(z) as a ratio of two polynomials with real coefficients.
(b) Find an expression for the gain factor c such that Hnotch( f ) 5 1 at f 5 0.
5.31 Using the results from Problem 5.30 and (5.6.8), design a notch filter Hnotch(z) that 
has a notch at F0 5 .1fs and a notch bandwidth of DF 5 .01fs.
5.32 Suppose the following two filters are notch filters with notches at F0 and F1, respec-
tively. Using {a, b, A, B}, write the difference equations of a double-notch filter 
with notches at F0 and F1.
H0(z) 5 b0z2 1 b1z 1 b2
z2 1 a1z 1 a2
H1(z) 5 B0z2 1 B1z 1 B2
z2 1 A1z 1 A2
5.33 Consider the DC notch filter in (5.6.3).
HDC(z) 5 .5(1 1 r)(z 2 1)
z 2 r
(a) Find the impulse response h(k).
(b) Find the difference equation.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.11 Problems    411
5.34 The general form for a resonator with a resonant frequency of F0 is given in (5.6.15) 
where 0 5 2F0T.
Hres(z) 5
c(z2 2 1)
fz 2 r exp(j0)gfz 2 r exp(2j0)g
(a) Rewrite Hres(z) as a ratio of two polynomials with real coefficients.
(b) Find an expression for the gain factor c such that uHres( f )u 5 1 at f 5 F0.
5.35 Using the results from Problem 5.34 and (5.6.8), design a resonator Hres(z) that has 
a resonant frequency at F0 5 .4fs and a bandwidth of DF 5 .02fs.
5.36 Suppose the following two filters are resonators with resonant frequencies at F0 
and F1, respectively. Using {a, b, A, B}, write the difference equations of a double-
resonator with resonant frequencies at F0 and F1.
H0(z) 5 b0z2 1 b1z 1 b2
z2 1 a1z 1 a2
H1(z) 5 B0z2 1 B1z 1 B2
z2 1 A1z 1 A2
5.37 Consider the DC resonator in (5.6.14).
Hdc(z) 5 .5(1 2 r)(z 1 1)
z 2 r
(a) Find the impulse response h(k).
(b) Find the difference equation.
section 5.7: narrowband Filters and Filter Banks 
5.38 Consider the problem of designing a lowpass narrowband filter. Suppose the sam-
pling frequency is fs 5 20 kHz.
(a) The desired lowpass cutoff frequency is Fc 5 50 Hz. Find the sampling rate 
reduction factor M such that if Fs 5 fsyM, then the new normalized cutoff 
frequency will be Fc 5 .25Fs.
(b) A cascade configuration of three rate converters with rate conversion factors 
M1, M2, M3 can be used to implement a multistage sampling rate converter. 
Factor M from part (a) as follows, where the maximum of hM1, M2, M3j is as 
small as possible.
M 5 M1 M2 M3
5.39 Consider the filter bank with m 5 2 filters shown in Figure 5.46.
(a) Find an expression for the magnitude response A0( f ) in the transition band 
f.2fs, .3fsg.
(b) Find the 3 dB cutoff frequency F0 of the first filter.
(c) Find an expression for the magnitude response A1( f ) in the transition band 
f.2fs, .3fsg.
(d) Find the 3 dB cutoff frequency F1 of the second filter.
(e) Show that the two filters form a magnitude-complementary pair with
 
A0( f ) 1 A1( f ) 5 1 
(5.11.1)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

412    Chapter 5  Filter Types and Characteristics
5.40 Consider the filter bank with m 5 2 filters shown in Figure 5.47.
(a) Find an expression for the magnitude response A0( f ) in the transition band 
f.22fs, .28fsg.
(b) Find the 3 dB cutoff frequency F0 of the first filter.
(c) Find an expression for the magnitude response A1( f ) in the transition band 
f.22fs, .28fsg.
(d) Find the 3 dB cutoff frequency F1 of the second filter.
(e) Show that the two filters form a power-complementary pair with
 
A2
0( f ) 1 A2
1( f ) 5 1 
(5.11.2)
Figure 5.47:  
A Power-
complementary 
Pair of Filters 
A2(f)
0
0.5
20.5
0
0.5
1
1.5
A2
0
A2
1
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45
f/fs
0
0.5
20.5
0
0.5
1
1.5
A(f)
A1
A0
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45
f/fs
Figure 5.46:  
A Magnitude-
complementary 
Pair of Filters
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.11 Problems    413
section 5.8: Adaptive Filters
5.41 Consider the mean square error performance criterion used for the adaptive filter 
in Figure 5.41.
(w) 5 E fe2(k)g
(a) Suppose the mean square error is approximated as (w) < e2(k). Find the gra-
dient vector =(w) 5 −(w)y−w. Express your final answer in terms of the state 
vector of past inputs, u, in (5.8.2).
(b) Using your expression for =(w) from part (a) and the step size , show that 
the following steepest descent method for approximating w in (5.8.6) reduces 
to the LMS method in (5.8.7).
w(k 1 1) 5 w(k) 2 =fw(k)g
5.42 Consider the adaptive filter shown in Figure 5.48. This configuration can be used to 
design an equalizer with delay. Here G(z) is a stable IIR filter. Suppose the adaptive 
filter converges to an FIR filter H(z) with error e(k) 5 0. Let
Gequal(z) 5 G(z)H(z)
(a) Show that Gequal(z) is an allpass filter with Aequal( f ) 5 1.
(b) Show that Gequal(z) is a linear-phase filter with equal( f ) 5 22MTf.
5.11.2 GUi simulation
section 5.2: Frequency-selective Filters
5.43 Use the GUI module g_filters and select an IIR bandpass filter. Reduce the number of 
bits of precision N until the quantized filter first goes unstable. Then increase N by one.
(a) Plot the magnitude response.
(b) Plot the pole-zero pattern.
5.44 Use the GUI module g_filters and select an IIR highpass filter. Adjust the number of 
bits of precision N to highest value that still makes the quantized filter go unstable. 
(a) Plot the unstable pole-zero plot.
(b) Increase N by one so the quantized filter becomes stable. Then plot the impulse 
response.
Figure 5.48:  
Equalizer Design 
Using an Adaptive 
Filter
z2M
G(z)
e(k)
d(k)
2
1
Adaptive 
filter
x(k)
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

414    Chapter 5  Filter Types and Characteristics
5.45 Use the GUI module g_    filters and import the filter in MAT-file prob5_45.mat. Set 
the number of bits for coefficient quantization to N 5 10.
(a) Plot the magnitude response using a direct form realization.
(b) Plot the phase response using a direct form realization.
(c) Plot the magnitude response using a cascade form realization.
(d) Plot the phase response using a cascade form realization.
5.46 Use the GUI module g_filters and select an FIR bandstop filter. Adjust the number 
of bits of precision N until the quantization level q is larger than .005.
(a) Plot the magnitude response.
(b) Plot the pole-zero plot.
5.47 Use the GUI module g_filters and select an FIR lowpass filter. Adjust the parame-
ter values to fs 5 100 Hz, F0 5 30 Hz, and B 5 10 Hz.
(a) Plot the magnitude response using the dB scale.
(b) Plot the phase response.
(c) Plot the impulse response. Is this a linear-phase filter? If so, what type?
section 5.3: Linear-phase and Zero-phase Filters
5.48 Consider the following running average filter. Create a MAT-file called prob5_48.mat 
that contains fs 5 300, a, and b for this filter.
y(k) 5 1
10o
9
i50
x(k 2 i)
Use the GUI module g_filters and import this filter.
(a) Plot the magnitude response.
(b) Plot the phase response.
(c) Plot the pole-zero plot.
(d) Plot the impulse response. Is this a linear-phase filter? If so, what type?
5.49 The derivative of an analog signal xa(t) can be approximated numerically by tak-
ing differences between the samples of the signal using the following first-order  
backward Euler differentiator.
y(k) 5 x(k) 2 x(k 2 1)
T
Create a MAT-file called prob5_49.mat that contains fs 5 10, a and b for this filter. 
Then use GUI module g_     filters and import this filter.
(a) Plot the magnitude response.
(b) Plot the phase response.
(c) Plot the impulse response. Is this a linear-phase filter? If so, what type?
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.11 Problems    415
section 5.6: notch Filters and Resonators
5.50 A notch filter is a filter that is designed to remove a single frequency. Consider the 
following transfer function for a notch filter.
H(z) 5
.9766(1 1 z21 1 z22)
1 1 .9764z21 1 .9534z22
Create a MAT-file called prob5_50.mat that contains fs 5 1000 and the a and b for 
this filter. Then use GUI module g_filters to import this filter. Set N 5 6 bits.
(a) Plot the magnitude response. Use the Caliper option to estimate the notch 
frequency.
(b) Plot the phase response.
(c) Plot the pole-zero pattern.
5.11.3 MATLAB Computation
section 5.4: Minimum-phase and Allpass Filters
5.51 Consider the following IIR filter.
H(z) 5 1 1 1.75z22 2 .5z24
1 1 .4096z24
(a) Write a MATLAB program that uses f_minall to compute and print the coef-
ficients of the minimum-phase and allpass parts of H(z).
(b) Use the MATLAB subplot command to plot the magnitude responses A( f ), 
Amin( f ), and Aall( f ) on a single screen using three separate plots.
(c) Repeat part (b), but for the phase responses.
(d) Use f_pzplot to plot the poles and zeros of H(z), Hmin(z), and Hall(z) on one 
screen using three separate square plots.
section 5.6: notch Filters and Resonators
5.52 A comb filter is a filter that extracts a set of isolated equally spaced frequencies 
from a signal. Consider the following comb filter that has n teeth.
H(z) 5
b0
1 2 rnz2n
Here the filter gain is b0 5 1 2 rn. Suppose n 5 10, r 5 .98, and fs 5 300 Hz. Write 
a MATLAB program that uses f_freqz to compute the frequency response. Com-
pute both the unquantized frequency response (set bits = 64) and the frequency 
response with coefficient quantization using f_quant with N 5 4 bits. Plot both 
magnitude responses on a single plot using the linear scale and a legend.
5.53 An inverse comb filter is a filter that eliminates a set of isolated equally spaced 
frequencies from a signal. Consider the following inverse comb filter that has n 
teeth.
H(z) 5 b0(1 2 z2n)
1 2 rnz2n
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

416    Chapter 5  Filter Types and Characteristics
Here the filter gain is b0 5 (1 1 rn)y2. Suppose n 5 10, r 5 .96, and fs 5 200 
Hz. Write a MATLAB program that uses f_freqz to compute the frequency 
response. Compute both the unquantized frequency response (set bits = 64), 
and the frequency response with coefficient quantization using f_quant with 
N 5 4 bits. Plot both magnitude responses on a single plot using the linear scale 
and a legend.
section 5.8: Adaptive Filters
5.54 Consider the following FIR system.
G(z) 5 3 2 4z21 1 2z22 1 7z23 1 4z24 1 9z25
Suppose G(z) is driven by N 5 500 samples of white noise x(k) uniformly distrib-
uted over f210, 10g. Let D(z) 5 G(z)X(z) represent the desired output. Write a 
MATLAB program that performs the following tasks.
(a) Compute the optimal weight w for an adaptive transversal filter of order m 
using the LMS method. Start from an initial guess of w(0) 5 0 and choose a 
step size  that ensures convergence. Compute and display the final w for three 
cases: m 5 3, m 5 5, and m 5 7. Also display the coefficient vector b of G(z).
(b) Let H(z) be the transfer function of the transversal filter using the final weights 
when m 5 7. Create a 2 3 1 array of plots. Plot the magnitude responses of 
G(z) and H(z) on the first plot with a legend. Plot the phase responses of G(z) 
and H(z) on the second plot with a legend.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

417
FIR Filter Design
C H A P T E R  6
CHAPTER ToPICs
 6.1 
Motivation
 6.2 
Windowing Method
 6.3 
 Frequency-sampling 
Method
 6.4 
Least-squares Method
 6.5 
 Optimal Equiripple  
Filter Design
 6.6 
 Differentiators and  
Hilbert Transformers
 6.7 
Quadrature Filter Design
 6.8 
 Filter Realization 
Structures
*6.9 
 Finite Word Length 
Effects
 6.10  GUI Modules and  
Case Studies
 6.11 Chapter Summary
 6.12 Problems
Motivation
A digital filter is a discrete-time system that is designed to 
reshape the spectrum of the input signal in order to produce 
desired spectral characteristics in the output signal. In this chap-
ter we focus on a specific type of digital filter, the finite impulse 
response or FIR filter. An mth order FIR filter is a discrete-time 
system with the following generic transfer function.
H(z) 5 b0 1 b1z21 1 Á 1 bmz2m
FIR filters offer a number of important advantages in 
comparison with IIR filters. The nonzero part of the impulse 
response of an FIR filter is simply h(k) 5 bk for 0 # k # m. 
Consequently, an FIR impulse response can be obtained directly 
from inspection of the transfer function, the difference equation, 
or the signal flow graph. Since the poles of an FIR filter are all 
located at the origin, FIR filters are always stable. FIR filters 
tend to be less sensitive to finite word length effects. Unlike IIR 
filters, quantized FIR filters cannot become unstable or exhibit 
limit cycle oscillations. FIR filters can be designed to closely 
approximate arbitrary magnitude responses if the order of the 
filter is allowed to be sufficiently large. Furthermore, by using 
coefficient symmetry, the phase response of an FIR filter can be 
made to be linear. This is an important characteristic because it 
means that different spectral components of the input signal are 
delayed by the same amount as they are processed by the filter. A 
linear-phase filter does not distort a signal within the passband; 
6.1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

418    Chapter 6  FIR Filter Design
it only delays it. Using a quadrature filter approach, FIR filters can be designed to meet 
both magnitude and phase response specifications as long as sufficient delay is included.
Although impressive control of the frequency response can be achieved with FIR 
filters, these filters do suffer from some limitations in comparison with IIR filters. One 
fundamental drawback occurs when we attempt to design a sharp frequency-selective fil-
ter with a narrow transition band, similar to an IIR elliptic filter. To meet the same design 
specifications with an FIR filter, a much higher-order filter is required. The increased 
order means a longer delay, larger storage requirements, and a longer computational 
time. Computational time is important in real-time applications where inter-sample sig-
nal processing must be performed before each new ADC sample arrives.
We begin this chapter by introducing an example of an application of FIR filters. 
Next, several methods for FIR filter design are presented. Most of the design methods 
produce linear-phase FIR filters with a propagation delay of  5 mTy2, where m is the 
filter order and T is the sampling interval. The first design method is the windowing 
method, a simple and effective technique that is based on a soft or gradual truncation 
of the delayed impulse response. Next, a frequency-sampling method is presented that 
uses the inverse DFT to compute the filter coefficients. The frequency-sampling method 
can be optimized with the inclusion of transition-band samples. This is followed by the 
least-squares method. This is an optimization method that uses an arbitrary set of dis-
crete frequencies and a user-selectable weighting function. Next, optimization based on 
the Parks-McClellan algorithm is used to design a sharp equiripple frequency-selective 
FIR filter that is analogous to the IIR elliptic filter. All of these methods can be used to 
construct a filter with a prescribed magnitude response and a linear phase response. Next 
the focus turns to specialized linear-phase FIR filters such as Hilbert transformers that 
produce signals that are in phase quadrature. These signals form the basis for a general 
two-stage quadrature filter that can be designed to meet both magnitude response and 
phase response specifications as long as sufficient delay is included. FIR filter realization 
structures are examined next, including both direct and indirect forms. They differ from 
one another in computational time, storage requirements, and sensitivity to finite word 
length effects. Sources of error due to finite word length are investigated. Finally, a GUI 
module called g_ fir is introduced that allows the user design and evaluate a variety of 
FIR filters without any need for programming. The chapter concludes with a case study 
example and a summary of FIR filter design techniques.
6.1.1 Numerical Differentiators
In engineering applications there are instances where it is useful to obtain the derivative 
of an analog signal from the samples of the signal. For example, an estimate of velocity 
might be obtained from position measurements, or an acceleration estimate might be 
obtained from velocity measurements. Numerical differentiation is a challenging practi-
cal problem, because the process is highly sensitive to the presence of noise. As an intro-
duction to the topic, consider the use of simple low-order FIR filters to numerically 
approximate the differentiation process. The objective is to design a digital equivalent to 
the following analog system.
 
Ha(s) 5 s 
 (6.1.1)
Suppose the analog signal to be differentiated, xa(t), is approximated with a linear poly-
nomial in the neighborhood of t 5 kT.
 
xa(t) < x(k) 1 c(t 2 kT) 
 (6.1.2)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.1 Motivation    419
Recall that x(k) 5 xa(kT ). Let x?
a(t) 5 dxa(t)ydt denote the derivative of xa(t). Then from 
(6.1.2) we have x?
a(kT ) 5 c. Evaluating (6.1.2) at t 5 (k 2 1)T and solving for c yields the 
first-order approximation for the derivative, y1(k) < x?
a(kT ).
 
y1(k) 5 x(k) 2 x(k 2 1)
T
 
 (6.1.3)
The formulation in (6.1.3) is called a backward Euler numerical approximation to the deriv-
ative. Note that it is an FIR filter of order m 5 1 with coefficient vector b 5 f1yT, 21yT g. 
The transfer function of the first-order backward Euler differentiator is shown in  
Figure 6.1.
The FIR filter model of the differentiation process can be improved if we approxi-
mate xa(t) with a quadratic polynomial as follows.
 
xa(t) < x(k) 1 c(t 2 kT ) 1 d(t 2 kT )2 
 (6.1.4)
Again x?
a(kT ) 5 c. To determine the parameter c, we evaluate (6.1.4) at t 5 (k 2 1)T and 
at t 5 (k 2 2)T, which yields
 
x(k 2 1) 5 x(k) 2 Tc 1 T 2d
 
 (6.1.5a)
 
x(k 2 2) 5 x(k) 2 2Tc 1 4T 2d 
 (6.1.5b)
If (6.1.5b) is subtracted from four times (6.1.5a), the dependence on d drops out. The 
resulting equation then can be solved for c, which yields the second-order differentiator, 
y2(k) < x?
a(kT).
 
y2(k) 5 3x(k) 2 4x(k 2 1) 1 x(k 2 2)
2T
 
 (6.1.6)
This FIR filter is called a second-order backward differentiator. It is backward, because it 
makes use of current and past samples, not future samples. This is important if the dif-
ferentiator is to be used in real-time applications such as feedback control. The transfer 
function of the second-order backward differentiator is shown in Figure 6.2.
It would be tempting to continue this process using higher-order polynomial approx-
imations of xa(t). Unfortunately, one quickly reaches a point of diminishing return in 
terms of accuracy. This is because higher-degree polynomials are not effective in model-
ing the underling trend of signals. Even though they can be made to go through all of the 
samples, they tend to do so by oscillating wildly between the samples as the polynomial 
degree increases. Later, we will examine a different design approach that can be used to 
approximate a delayed version of a differentiator. To examine the effectiveness of the 
differentiator in Figure 6.2, consider the following noise-free input signal.
 
xa(t) 5 sin(t) 
 (6.1.7)
Backward Euler 
differentiator
Second-order 
backward 
differentiator
Figure 6.2: 
Block Diagram 
of a Second-
order Backward 
Differentiator 
Figure 6.1: Block 
Diagram of a First-
order Backward 
Euler Differentiator 
x
y
1 2 z21
T
x
y
3 2 4z21 1 z22
2T
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

420    Chapter 6  FIR Filter Design
Plots of xa(t), x?
a(t) and y2(k) are shown in Figure 6.3. Here a sampling frequency of  
fs 5 20 Hz is used. After a short start up transient, the approximation is effective in this case.
6.1.2 signal-to-noise Ratio
Although the approximation to the derivative in Figure 6.3 is effective for the noise-free 
signal in (6.1.7), when noise is added to xa(t) the numerical approximations to the deriva-
tive rapidly deteriorate. To illustrate, consider the noise-corrupted signal
 
y(k) 5 x(k) 1 v(k) 
 (6.1.8)
Here v(k) is white noise uniformly distributed over the interval f2c, cg. To specify the size 
of the noise relative to the size of the signal, the following concept be used.
Let y(k) 5 x(k) 1 v(k) represent a signal x(k) that is corrupted with noise v(k). If Px 
is the average power of the signal and Pv is the average power of the noise, then the  
signal-to-noise ratio is defined as
SNR(y)  5
D   
10 log10 1
Px
Pv2 dB
Recall that the average power is the mean or expected value of the square of the signal, 
Px 5 E fx2(k)g. A signal that is all noise has a signal-to-noise ratio of minus infinity, while 
a noise-free signal has a signal-to-noise ratio of plus infinity. When the signal-to-noise 
ratio is zero dB, the power of the signal is equal to the power of the noise.
Using the trigonometric identities from Appendix 2, one can show that the average 
power of a sinusoid of amplitude A is A2y2. From (4.6.6) or Appendix 2, the average 
power of white noise uniformly distributed over f2c, cg is Pv 5 c2y3. Suppose c 5 .01, 
DEFINITIoN 
6.1 signal-to-noise Ratio
0
0.5
1
1.5
2
2.5
3
3.5
4
24
23
22
21
0
1
2
3
4
5
t (sec)
Signals
xa
dxa/dt
y2
xa
dxa/dt
y2
Figure 6.3:  
Numerical  
Approximation to 
the Derivative of 
a Sinusoidal Input 
Using an FIR Filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.1 Motivation    421
which is only a modest amount of noise. Then from Definition 6.1, the signal-to-noise 
ratio of the noise corrupted sinusoid in (6.1.8) is
 
SNR(y) 5 10 log103
1y2
(.01)2y34
 
 5 10 log101
104
6 2
 
 5 32.22 dB
 
 (6.1.9)
When the noise-corrupted sinusoid in (6.1.8) is processed with the second-order differ-
entiator, the results are as shown in Figure 6.4. Note that xa(t) itself is not significantly 
distorted because the signal-to-noise ratio is positive and relatively large. However, the 
numerical approximation to x?
a(t) does not fare well in comparison with the noise-free 
case in Figure 6.3. This is because the differentiation process amplifies the high-frequency 
part of the noise. In this particular instance, most of the noise could have been removed 
by first preprocessing y(k) with a narrowband resonator filter with resonant frequency 
F0 5 1 Hz. Resonator filter design is discussed in Chapter 7. However, for a more general 
broadband signal, xa(t), this is not a viable option because the filtering would remove 
important spectral components of xa(t) itself.
Typically, additive white noise v(k) has zero mean and is statistically independent 
of the signal x(k), which means that E fx(k)v(k)g 5 E fx(k)gE fv(k)g. In this case there is 
a simple relationship between the average power of the noise-corrupted signal and the 
average power of the noise-free signal. Here
 
Py 5 Efy2(k)g
 
5 Efhx(k) 1 v(k)j2g
 
5 Efx2(k) 1 2x(k)v(k) 1 v2(k)g
 
5 Efx2(k)g 1 Ef2x(k)v(k)g 1 Efv2(k)g
 
5 Px 1 2Efx(k)gEfv(k)g 1 Pv 
 (6.1.10)
Statistically  
independent signals
0
0.5
1
1.5
2
2.5
3
3.5
4
24
23
22
21
0
1
2
3
4
5
t (sec)
Signals
xa 1 noise
xa
dxa/dt
dxa/dt
y2
y2
Figure 6.4: 
Numerical 
Approximations to 
the Derivative of 
a Noisy Sinusoidal 
Input Using a 
Second-order FIR 
Filter 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

422    Chapter 6  FIR Filter Design
For zero-mean noise, Efv(k)g 5 0, which means (6.1.10) reduces to
 
Py 5 Px 1 Pv  
 (6.1.11)
Thus for a signal x(k) that is corrupted with zero-mean white noise v(k), the average power 
of the noise-corrupted signal y(k) is just the sum of the average power of the signal plus 
the average power of the noise. Typically y(k) is known or can be measured. If the average 
power of either the signal x(k) or the noise v(k) is known or can be computed, then the 
average power of the other can be determined using (6.1.11). The signal-to-noise ratio of 
y(k) then can be determined using Definition 6.1.
Windowing Method
In this section we examine a simple technique for designing a linear-phase FIR filter with 
a prescribed magnitude response. Recall from Section 5.3 that, for mth order linear-phase 
FIR filters, the design specifications are formulated in terms of the desired amplitude 
response, Ar( f ), as follows. 
 
H( f ) 5 Ar( f ) expf  j( 2 mf T )g 
 (6.2.1)
Here the amplitude response is real, but unlike the magnitude response, it can be 
positive or negative. A FIR lowpass design specification based on the desired amplitude 
response is shown in Figure 6.5. The stopband attenuation, s, represents the radius of a 
region centered about Ar( f ) 5 0 within which the amplitude response must lie. Note that 
Figure 6.5 is equivalent to Figure 5.6, but applied to the amplitude response rather than 
the magnitude response.
Zero-mean white noise
6.2
Amplitude response
0
0
Passband
Stopband
 f
Ar(f)
1
1 2 p
1 1 p
Fp
Fs
fs/2
s
2s
Figure 6.5: 
Amplitude 
Response 
Specification for 
an FIR Lowpass 
Filter with 
A(f) 5 uAr(f)u 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.2 Windowing Method    423
6.2.1 Truncated Impulse Response
The basic idea behind the windowing method is to first truncate the desired impulse 
response to a finite number of samples and then delay the impulse response, as needed, 
so that it is causal. A filter is then designed with an impulse response that matches the 
delayed truncated impulse response. Except for the zero-phase filters, the filters that we 
have considered thus far all have been causal filters with h(k) 5 0 for k , 0. In order 
to develop a general design technique based on idealized filters, we consider noncausal 
filters as well. When h(k) is noncausal, the DTFT of h(k) gives rise to the following 
frequency response.
 
H( f ) 5 o
`
k52`
h(k) exp(2j2kf T ) 
 (6.2.2)
Using Euler’s identity in (6.2.2), we see that H( f ) is periodic with period fs. Consequently, 
we can interpret (6.2.2) as a complex Fourier series of the periodic function H( f ). The k
th coefficient of the Fourier series is
 
h(k) 5 1
fs#
fsy2
2fsy2
H( f ) exp( j2kf T )df, 2` , k , ` 
 (6.2.3)
Using (6.2.3), we can recover the impulse response h(k) from a desired frequency response 
H( f ). Since we are interested in designing a linear-phase filter that does not distort the 
input signal, the frequency response must satisfy the linear-phase symmetry constraint 
introduced in Section 5.3.
Type 1 and Type 2 Filters
Let m be the filter order, and suppose h(k) exhibits even symmetry about k 5 my2. Then 
from (6.2.1) and Table 5.1, the desired frequency response for a type 1 or a type 2 causal 
linear-phase filter with a group delay of  5 mTy2 is
 
H( f ) 5 Ar( f ) exp(2jmf T )  j types 1, 2 
 (6.2.4)
For a type 1 or type 2 linear-phase filter, the amplitude response, Ar( f ), is a real even 
function that is specified by the filter designer. The desired magnitude response is 
A( f ) 5 uAr( f )u. Using (6.2.4), Euler’s identity, and the fact that Ar( f ) is even, the expres-
sion for h(k) in (6.2.3) becomes
h(k) 5 1
fs
 #
fsy2
2fsy2
Ar( f ) exp(2jmf T ) expf j2kf T gdf
5 T #
fsy2
2fsy2
Ar( f ) expf  j2(k 2 .5m) f T gdf
5 T #
fsy2
2fsy2
Ar( f )h cosf2(k 2 .5m) f T g 1 j sinf2(k 2 .5m) f T gjdf
 
5 T #
fsy2
2fsy2
Ar( f ) cosf2(k 2 .5m)f T gdf  
 (6.2.5)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

424    Chapter 6  FIR Filter Design
Since the integrand in (6.2.5) is even, the integral can be performed over the positive fre-
quencies and doubled. This results in the following impulse response for a type 1 or type 2  
linear-phase filter of order m.
 
h(k) 5 2T #
fsy2
0
Ar( f ) cosf2(k 2 .5m) f T gd f, 0 # k # m  
 (6.2.6)
Type 3 and Type 4 Filters
Next suppose the impulse response exhibits odd symmetry about k 5 my2. Then from 
(6.2.1) and Table 5.1, the desired frequency response for a causal type 3 or a type 4 filter 
with a group delay of  5 mTy2 is
 
H( f ) 5 jAr( f ) exp(2jmf T )  j types 3, 4 
 (6.2.7)
Note that we have used the fact that exp( jy2) 5 j. For a type 3 or type 4 linear-phase 
filter the amplitude response, Ar( f ), is a real odd function that is specified by the designer 
to get the desired magnitude response, A( f ) 5 uAr( f )u. Using (6.2.7), Euler’s identity, and 
the fact that Ar( f ) is odd, the expression for the impulse response in (6.2.3) becomes
 
h(k) 5 j
fs#
fsy2
2fsy2
Ar( f ) expf j2(k 2 .5m)f Tgdf
 
5 jT#
fsy2
2fsy2
Ar( f )h cosf2(k 2 .5m)f T g 1 j sinf2(k 2 .5m)f T gjdf
 
5 2T#
fsy2
2fsy2
Ar( f ) sinf2(k 2 .5m)f T gdf  
 (6.2.8)
Here we have used the fact that exp(jy2) 5 j. Again the integrand in (6.2.8) is even, so 
the integral can be performed over the positive frequencies and doubled. This yields the 
following impulse response for a type 3 or type 4 linear-phase filter or order m.
 
h(k) 5 22T #
fsy2
0
Ar( f ) sinf2(k 2 .5m)f T gdf, 0 # k # m  
 (6.2.9)
Recall that the type 1 linear-phase filter (even symmetry, even order) is the most 
general in the sense that there are no zeros at f 5 0 or f 5 fsy2. Using (6.2.6) with order 
m 5 2p, the impulse responses for the four ideal type 1 frequency-selective filters are sum-
marized in Table 6.1.
Types 1, 2
Types 3, 4
Filter  
h(p) 
h(k), 0 # k # m, k Þ p
Lowpass  
 2F0T  
 
 sinf2(k 2 p)F0Tg
(k 2 p)
 
Highpass  
 1 2 2F0T  
 2 sinf2(k 2 p)F0Tg
(k 2 p)
 
Bandpass  
 2(F1 2 F0)T  
 
 sinf2(k 2 p)F1Tg 2 sinf2(k 2 p)F0Tg
(k 2 p)
 
Bandstop  
 1 2 2(F1 2 F0)T  
 
 sinf2(k 2 p)F0Tg 2  sinf2(k 2 p)F1Tg
(k 2 p)
Table 6.1:  
Impulse Responses  
of Ideal Frequency- 
selective Linear-phase 
Type 1 FIR Filters of 
Order m 5 2p
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.2 Windowing Method    425
Truncated Impulse Response Filter
EXAMPLE 6.1
As an illustration of an FIR filter designed by the truncated impulse response 
approach, suppose fs 5 100 Hz, and consider the problem of designing a lowpass 
filter with cutoff frequency F0 5 fsy4. Suppose m 5 40 is used to approximate 
H( f ). Then p 5 20 and from Table 6.1, the filter coefficients are h(p) 5 .5 and
h(k) 5
 sinf.5(k 2 p)g
(k 2 p)
 
5 .5  sinc f.5(k 2 p)g, 0 # k # m
The delay in this case is
 5 pT
5 .2 sec
The impulse response of this filter can be obtained by running exam6_1 using  
g_dsp. From the resulting plot, shown in Figure 6.6, we see that this is indeed a type 1  
linear-phase FIR filter with a palindrome-type impulse response, h(k), centered 
about k 5 20. Notice that the ideal sinc function impulse response is truncated 
to a radius of p 5 my2 samples and then delayed by p samples to make it causal.
The magnitude response and phase response generated by exam6_1 are shown 
in Figure 6.7. Although the magnitude response is an effective approximation 
25
0
5
10
15
20
25
30
35
40
45
20.2
20.1
0
0.1
0.2
0.3
0.4
0.5
0.6
k
h(k)
Figure 6.6: Palindrome Impulse Response of Type 1 Linear-phase FIR Filter 
with m 5 40
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

426    Chapter 6  FIR Filter Design
0
50
0
0.5
1
f (Hz)
A(f)
0
10
15
5
20
25
30
35
40
45
10
15
5
20
25
30
35
40
45
50
24
22
0
2
4
f (Hz)
(f)
2

Figure 6.7: Frequency Response of Truncated Impulse Response FIR Filter 
with m 5 40
to the ideal lowpass characteristic, it is evident that there is  ringing or oscilla-
tion, particularly in the neighborhood of the cutoff frequency, F0 5 25 Hz. The 
approximation to an ideal lowpass characteristic can be improved by increasing 
the filter order. However, even for large values of p, significant ringing persists 
near f 5 F0. This is an inherent characteristic of the Fourier series representa-
tion that is present whenever there is a jump discontinuity in the function being 
approximated. The oscillation near a jump discontinuity is referred to as Gibb’s 
phenomenon.
It is of interest to observe the phase response carefully. The jump discontinui-
ties in the passband occurring at  5 2 are an artifact of the fact that the phase 
is computed modulo 2, so at 2 it wraps around to . However, the jump dis-
continuities of amplitude  in the stopband are discontinuities that occur because 
of a sign change in the amplitude response, Ar( f ). Notice that all of these jumps 
occur at points in the set Fz where A( f ) 5 0.
Gibb’s phenomenon
6.2.2 Windowing
The beauty of the truncated impulse response method is that it allows us to design a 
magnitude response of prescribed shape. However, if the desired magnitude response 
contains one or more jump discontinuities, then the oscillations caused by Gibb’s 
phenomenon effectively prohibit the design of filters having a very small passband 
ripple or stopband attenuation. Fortunately, there is a tradeoff we can make that 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.2 Windowing Method    427
reduces the ripples at the expense of increasing the width of the transition band. To 
see how this can be done, first notice that a filter transfer function can be rewritten 
as follows.
 
H(z) 5 o
`
i52`
wR(i)h(i)z2i 
 (6.2.10)
This corresponds to a causal filter of order m if wR(i) is the following rectangular window 
of order m.
 
wR(i) 5
D  5
1,
0 # i # m
0,
otherwise
 
 (6.2.11)
Thus truncation of the delayed impulse response to 0 # i # m is equivalent to multi-
plication of the impulse response by a rectangular window. It is the abrupt truncation 
of the impulse response that causes the oscillations associated with Gibb’s phenom-
enon. The amplitude of the oscillations can be decreased by tapering the impulse 
response to zero more gradually. Recall that for an FIR filter the numerator coefficients 
are bi 5 h(i). Therefore if w(i) represents a tapered window of width m, then the  
windowed coefficients are
 
bi 5 w(i)h(i), 0 # i # m  
 (6.2.12)
There are many windows that have been proposed. Some of the more popular 
fixed windows are summarized in Table 6.2. They include the rectangular window 
(also called the boxcar window), the Hanning window, the Hamming window, and the 
Blackman window. Recall from Chapter 4 that these are the same data windows that 
were used with the spectrogram and with Welch’s method of estimating the power den-
sity spectrum of a signal.
Plots of the windows are shown in Figure 6.8 for the case m 5 60. Notice that they 
all are symmetric about i 5 my2 and they attain a peak value of w(i) 5 1 at the midpoint. 
With the exception of the rectangular window, they all gradually taper to zero at the end 
points i 5 0 and i 5 m, except for the Hamming window, which is near zero. Multiplica-
tion by a tapered window can be thought of as a form of soft truncation as opposed to 
the hard or abrupt truncation of the rectangular window. Since the windows are all palin-
dromes with w(i) 5 w(m 2 i), filters using the windowed coefficients in (6.2.12) continue 
to be linear-phase filters.
Rectangular window
Windowed coefficients
Table 6.2:  
Windows of Order m
 Type  
Name 
w(i), o ≤ i ≤ m
 0 
Rectangular  
1
 1 
Hanning  
.5 2 .5 cos1
i
.5m2
 2 
Hamming  
 .54 2 .46 cos1
i
.5m2
 3 
Blackman  
 .42 2 .5 cos1
i
.5m2 1 .08 cos1
2i
.5m2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

428    Chapter 6  FIR Filter Design
0
10
20
30
40
50
60
0
0.2
0.4
0.6
0.8
1
i
w(i)
0
1
2
3
Figure 6.8:  
Windows Used to 
Taper Truncated 
Impulse Response: 
0 = rectangular,  
1 = Hanning,  
2 = Hamming, and  
3 = Blackman
Windowed Lowpass Filter
EXAMPLE 6.2
To illustrate the effects of the different windows, consider the design of a lowpass 
filter with cutoff frequency F0 5 fsy4. Suppose m 5 40 and p 5 my2. Then from 
Table 6.1, Table 6.2, and (6.2.12), the filter coefficients using the rectangular win-
dow are
bi 5 .5  sinc f.5(i 2 p)g, 0 # i # m
A plot of the rectangular magnitude response obtained by running exam6_2, 
using normalized frequency, f
 norm 5 fyfs, is shown in Figure 6.9. Here the log-
arithmic dB scale is used for the magnitude response because it better illustrates 
the amount of attenuation in the stopband, which is 21 dB or roughly a factor of 
10 in this case.
Next consider the same filter, but using the Hanning window. From Table 6.2, 
the filter coefficients are
bi 5 .25f1 1  cos(iyp)g  sinc f.5(i 2 p)g, 0 # i # m
A plot of the Hanning magnitude response is shown in Figure 6.10. Notice that 
the attenuation in the stopband is now 44 dB, and furthermore the response is 
also more flat in the passband in comparison with the rectangular window in Fig-
ure 6.9. However, this improvement in passband ripple and stopband attenuation 
is achieved at the expense of a what is clearly a wider transition band.
Still better stopband attenuation can be obtained using the Hamming win-
dow. From Table 6.2, the filter coefficients are
bi 5 .5f.54 1 .46 cos(iyp)g  sinc f.5(i 2 p)g, 0 # i # m
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.2 Windowing Method    429
0
0.5
As 5 20.9 dB
2120
2100
280
260
240
220
0
20
f/fs
A(f) (dB)
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45
Figure 6.9: Lowpass Magnitude Response Using a Rectangular Window, m 5 40 
0
0.5
2120
2100
280
260
240
220
0
20
f/fs
A(f) (dB)
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45
As 5 43.9 dB
Figure 6.10: Lowpass Magnitude Response Using a Hanning Window, m 5 40  
A plot of the Hamming magnitude response is shown in Figure 6.11. In this case 
the stopband attenuation is 53 dB, and the lobes are roughly constant throughout 
most of the stopband.
Finally, the maximum stopband attenuation can be obtained using the Black-
man window. From Table 6.2, the filter coefficients are
bi 5 .5f.42 1 .5 cos(iyp) 1 .08 cos(2iyp)g  sinc f.5(i 2 p)g, 0 # i # m
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

430    Chapter 6  FIR Filter Design
0
0.5
As 5 75.1 dB
2120
2100
280
260
240
220
0
20
f/fs
A(f) (dB)
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45
Figure 6.12: Lowpass Magnitude Response Using a Blackman Window, m 5 40  
0
0.5
As 5 54.5 dB
2120
2100
280
260
240
220
0
20
f/fs
A(f) (dB)
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45
Figure 6.11: Lowpass Magnitude Response Using a Hamming Window, m 5 40  
A plot of the Blackman magnitude response is shown in Figure 6.12. Here the 
stop band attenuation is 75 dB, which is 54 dB better than with the rectangular 
window. However, the choice of the Blackman window is not clear-cut, because 
the transition band is largest in this case.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.2 Windowing Method    431
Window Type 
Bnorm 5 uFS 2 FpuyFs 
dp 
ds 
Ap (dB) 
As (dB) 
Rectangular  
 .9
m  
 .0819  
 .0819  
 .742  
 21 
Hanning   
 3.1
m   
 .0063  
 .0063  
 .055  
 44 
Hamming  
 3.3
m   
 .0022  
 .0022  
 .019  
 53 
Blackman  
 5.5
m   
 .00017  
 .00017  
 .0015  
 75.4 
Table 6.3:  
Design Characteristics 
of Windows 
The improvements in passband ripple and stopband attenuation achieved by 
using windowing come at the cost of a wider transition band. However, the width of 
the transition band for each window can be controlled by the filter order m. A summary 
of the filter design characteristics using the different windows can be found in Table 6.3.
Kaiser Windows
Additional windows have been proposed, including the Bartlett, Lanczos, Tukey, Dolph- 
Chebyshev, and Kaiser windows. The Kaiser window, wK(i), is a near-optimal window 
based on the use of zeroth-order modified Bessel functions of the first kind (Kaiser, 1966).
 
wK(i) 5 I0{(1 2 f(i 2 p)ypg2)1y2}
I0()
, 0 # i # m 
 (6.2.13)
Here I0 is a zeroth-order modified Bessel function of the first kind. It is obtained by 
solving a certain family of differential equations. To evaluate I0(x), the MATLAB func-
tion besseli (0, x) can be used. Unlike the fixed windows in Table 6.2, the Kaiser window 
has an adjustable shape parameter,  $ 0, that allows the user to control the tradeoff 
between the main lobe width and the side lobe amplitudes. When  5 0, the Kaiser win-
dow reduces to the rectangular window. For a given window size m, increasing  leads to 
an increase in the stopband attenuation As. However, this also causes the main lobe to get 
wider, thereby increasing the width of the transition band. The width of the main lobe 
can be decreased by increasing m. Kaiser (1974) developed the following approximations 
for determining suitable values for  and m, given a desired stopband attenuation As and 
a desired normalized transition band B
 norm 5 uFs 2 Fpuyfs.
 
 <5
.1102(As 2 8.7),
As . 50
.5842(As 2 21).4 1 .07886(As 2 21), 21 # As # 50
0,
As , 21
 
 (6.2.14)
 
m <
As 2 8
4.568 Bnorm
 
 (6.2.15)
Recall that As 5 21 dB corresponds to a rectangular window. We summarize the steps of 
the windowed FIR filter design procedure with the following algorithm.
1.  Pick m . 0 and a window w.
2.  For i 5 0 to m compute 
bi 5 w(i)2T #
fsy2
0
Ar( f ) cosf2(i 2 .5m)f T gdf
3.  Set 
H(z) 5 o
m
i50
biz2i
ALgoRITHM 
6.1 Windowed FIR Filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

432    Chapter 6  FIR Filter Design
The FIR filter produced by Algorithm 6.1 is a type 1 or a type 2 linear-phase filter 
with delay  5 mTy2 and an even amplitude response, Ar( f ). To design a type 3 or type 4  
filter, the desired amplitude response should be odd, and the expression for bi in step 2 
should be based on (6.2.9) instead of (6.2.6).
0
0.5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Windowed filter
Ideal filter
f/fs
A(f)
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45
Figure 6.13: Magnitude Response of Bandpass Filter Using a Hamming  
Window, m 5 80
Windowed Bandpass Filter
EXAMPLE 6.3
Consider the problem of designing a bandpass filter with cutoff frequencies F0 5 fsy8 
and F1 5 3fsy8. Suppose the Blackman window is used. Using Table 6.1, Table 6.2, and  
Algorithm 6.1, the filter coefficients are bp 5 .5 and for 0 # i # m, i Þ p.
bi 5 f.42 1 .5 cos(iyp) 1 .08 cos(2iyp)ghsinf.75(i 2 p)g 2 sinf.25(i 2 p)gj
(i 2 p)
Suppose m 5 80. A plot of the magnitude response, obtained by running 
exam6_3, is shown in Figure 6.13. It is clear that the passband and stopband rip-
ples have been effectively reduced in comparison with Figure 6.7. From Table 6.3, 
the normalized width of the transition band in this case is
 B
 norm 5 DF
fs
 5 5.5
80
 5 .069
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.2 Windowing Method    433
The DSP Companion contains two functions for designing a linear-phase FIR filter 
using the windowing method. The first function, f_  firideal, uses design specifications 
of ideal frequency-selective filters.
%  F_FIRIDEAL Design an ideal linear-phase frequency-selective 
% windowed FIR filter
%
% Usage: 
%        b = f_firideal (f_type,F,m,fs,win)
% Pre: 
%        f_type =  integer selecting the frequency-selective filter 
% 
type
%
%                 0 = lowpass
%                 1 = highpass
%                 2 = bandpass
%                 3 = bandstop
%
%        F      =  scalar or vector of length two containing the 
%                 cutoff frequency or frequencies.
%        m      = filter order (even)
%        fs     = sampling frequency
%        win    = the window type to be used:
%
%                 0 = rectangular
%                 1 = Hanning
%                 2 = Hamming
%                 3 = Blackman
% Post: 
%        b   = 1 by m+1 vector of filter coefficients. 
A general linear-phase FIR filter with an arbitrary amplitude response can be 
designed with the windowing method using the function f_ firwin. The first calling 
argument, fun, is the name of a user-supplied function that specifies the desired ampli-
tude response.
% F_FIRWIN: Design a general windowed FIR filter
%
% Usage: 
%        b = f_firwin (@fun,m,fs,win,sym,p)
% Pre: 
%        fun = name of user-supplied function that 
%              specifies the desired amplitude 
%              response of the filter. Usage:
%
%              [Ar,theta] = fun(f,fs,p)
%
%              Here f is the frequency, fs is the sampling 
%              frequency, and p is an optional parameter 
%              vector containing things like cutoff 
%              frequencies,etc. Output Ar is the desired 
DSP Companion
DsP Companion
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

434    Chapter 6  FIR Filter Design
%              amplitude response. Optional output theta is
%              included for compatibility with f_firquad.
%              Set theta = zeros(size(f)) for a linear-
%              phase filter.
%
%        m   = the filter order
%        fs  = sampling frequency
%        win = the window type to be used:
%
%              0 = rectangular
%              1 = Hanning
%              2 = Hamming
%              3 = Blackman
% 
%        sym = symmetry of impulse response.  
%
%              0 = even symmetry of h(k) about k = m/2
%              1 = odd symmetry of h(k) about k = m/2
%
%        p   = an optional vector of length contained 
%              design parameters to be passed to fun.  
%              For example p might contain cutoff 
%              frequencies or gains.
% Post: 
%        b = 1 by m+1 vector of filter coefficients.  
Frequency-sampling Method
An alternative technique for designing a linear-phase FIR filter with a prescribed mag-
nitude response is the frequency-sampling method. As the name implies, this method is 
based on using samples of the desired frequency response.
6.3.1 Frequency sampling
Suppose there are N frequency samples equally spaced over the range 0 # f , fs with the 
ith discrete frequency being
 
fi 5  ifs
N
, 0 # i , N 
 (6.3.1)
Recall from Section 4.8 that the samples of the frequency response of an FIR filter can be 
obtained directly from the DFT of the impulse response. In particular, for an FIR filter 
of order m 5 N 2 1 we have H(fi) 5 H(i) for 0 # i , N where H(i) 5 DFThh(k)j. Taking 
the inverse DFT, we then arrive at the following expression for the impulse response of 
the desired FIR filter.
 
h(k) 5 IDFThH(fi)j, 0 # k , N  
 (6.3.2)
6.3
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.3 Frequency-sampling Method    435
The filter in (6.3.2) has a frequency response H( f ) that interpolates, or passes through, 
the N samples. Next consider the problem of placing constraints on H( f ) that ensure that 
the filter is a linear-phase filter. Suppose h(k) is a linear-phase impulse response exhibiting 
even symmetry about the midpoint k 5 my2. Then from (6.2.4) the frequency response of 
this type 1 or type 2 filter is as follows.
 
H( f ) 5 Ar( f ) exp(2jmf T ) 
 (6.3.3)
Here the amplitude response, Ar( f ), is a real even function that specifies the desired 
magnitude response, A( f ) 5 uAr( f )u. Recalling the expression for the IDFT in (4.3.7) 
and using (6.3.1) through (6.3.3), the kth sample of the impulse response can be 
written as
 
h(k) 5 1
No
N21
i50
H(fi) exp( j2ikyN)
 
5 1
No
N21
i50
Ar(fi) exp(2jmfiT) exp(j2ikyN)
 
5 1
No
N21
i50
Ar(fi) exp(2jmiyN) exp(j2ikyN)
 
5 1
No
N21
i50
Ar(fi) expf j2i(k 2 .5m)yNg
 
5 1
No
N21
i50
Ar(fi)h cosf2i(k 2 .5m)yNg 1 j sinf2i(k 2 .5m)yNgj     (6.3.4)
For a real h(k) the sine terms in (6.3.4) cancel one another. The i 5 0 term can be treated 
separately, in which case the expression for h(k) in (6.3.4) reduces to
 
h(k) 5 Ar(f0)
N
1 1
No
N21
i51
Ar(fi) cosf2i(k 2 .5m)yNg 
 (6.3.5)
Using the symmetry properties of the DFT in Table 4.7, one can show that the con-
tributions of the i term and the N 2 i term are identical, which means they can be 
combined. Recalling that bk 5 h(k) for an FIR filter, we arrive at the following expres-
sion for the coefficients of a linear-phase frequency-sampled filter of order m, where 
m 5 N 2 1.
 
bk 5 Ar(0)
m 1 1 1
2
m 1 1 o
floor(.5m)
i51
Ar(  fi) cos3
2i(k 2 .5m)
m 1 1 4, 0  #  k # m   (6.3.6)
Note that for a type 1 filter the order m is even, in which case floor(my2) 5 my2. For a 
type 2 filter m is odd.
Interpolated response
Frequency-sampled  
filter, types 1, 2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

436    Chapter 6  FIR Filter Design
0
0.5
0
0.5
1
1.5
Filter
Samples
Ideal
0
0.5
2100
250
0
As 5 15.6 dB
f/fs
A(f) (dB)
A(f)
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
f/fs
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
Figure 6.14: Magnitude Response of a Frequency-sampled Lowpass Filter 
with m 5 20
Frequency-sampled Lowpass Filter
EXAMPLE 6.4
To illustrate the frequency-sampling method, consider the problem of designing a 
lowpass filter with cutoff frequency F0 5 fsy4. Suppose a filter of order m 5 20 is 
used. In this case the samples of the desired amplitude response are
Ar( fi) 55
1, 0 # i # 5
0, 6 # i # 10
Next from (6.3.6) the filter coefficients are
bk 5 1
21 1 2
21o
5
i51
 cos3
2i(k 2 10)
21
4, 0 # k # 20
A plot of the magnitude response, obtained by running exam6_4, is shown in 
Figure 6.14. Note the significant ripples in the magnitude response between the 
samples. The stopband attenuation is more easily seen in the logarithmic plot, 
which reveals that As 5 15.6 dB.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.3 Frequency-sampling Method    437
6.3.2 Transition-band optimization
The ringing or ripple in the magnitude response evident in Figure 6.14 is caused by the 
abrupt transition from passband to stopband in the desired magnitude response. One way 
to reduce these oscillations is to taper the filter coefficients using a data window. Recall 
that the tradeoff involved in using a window to reduce ripple is a wider transition band. 
With the frequency-sampling method one can forgo the use of a window and instead 
explicitly specify A( f ) in the transition band by including one or more transition-band 
samples. This increase in the width of the transition band has the effect of improving the 
passband ripple and stopband attenuation, as can be seen from the following example.
Transition-band 
samples
Filter with Transition-band sample
EXAMPLE 6.5
Again consider the problem of designing a lowpass filter with cutoff frequency 
F0 5 fsy4. Suppose a filter of order m 5 20 is used as in Example 6.4. However, in 
this case we insert a single transition-band sample as follows.
Ar(fi) 55
1,
0 # i # 5
.5,
i 5 6
0,
7 # i # 10
As 5 29.5 dB
0
0.5
0
0.5
1
1.5
Filter
Samples
Ideal
0
0.5
2100
0
250
f/fs
A(f) (dB)
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
A(f)
f/fs
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
Figure 6.15: Magnitude Response of a Lowpass Filter with m 5 20 and One  
Transition-band Sample, Ar(f6) 5 .5 
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

438    Chapter 6  FIR Filter Design
Filter with optimal Transition-band 
sample
EXAMPLE 6.6
Again consider the problem of designing a lowpass filter of order m 5 20 with 
cutoff frequency F0 5 fsy4. In this case we insert a general transition-band sample 
as follows.
Ar(fi) 55
1,
0 # i # 5
u,
i 5 6
0,
7 # i # 10
Using (6.3.6), the filter coefficients are
bk(u) 5 1
21 1 2
21 5o
5
i51
 cos3
2i(k 2 10)
21
4 1 u cos3
26(k 2 10)
21
46, 0 # k # 20
The problem is to find a value for u that maximizes the stopband attenuation As. 
A simple way to achieve this is to compute the stopband attenuation, As(u), for 
three distinct values of u in the range 0 , u , 1. For example, suppose the val-
ues u 5 f.25, .5, .75gT are used. Consider a quadratic polynomial passed through 
these three data points.
As(u) 5 c1 1 c2u 1 c3u2
The transition-band sample that was inserted in the desired magnitude response 
in Example 6.5 was Ar(f6) 5 .5, which corresponds to a straight line interpolation 
between the end of the ideal passband and the start of the ideal stopband. Clearly, 
other choices for the value of the transition-band sample are also possible. One can 
use this extra degree of freedom to shape the magnitude response in the transition 
band in order to maximize the stopband attenuation, as can be seen in the following 
example.
Using (6.3.6), the filter coefficients are
bk 5 1
21 1 2
21 5o
5
i51
 cos3
2i(k 2 10)
21
4 1 .5 cos3
26(k 2 10)
21
46, 0 # k # 20
A plot of the logarithmic magnitude response, obtained by running exam6_5, is 
shown in Figure 6.15 on the previous page. Comparing the results with Figure 
6.14, it is clear that the stopband attenuation has increased from 15.6 dB to 29.5 
dB. The passband ripple has also been reduced, but at the expense of a wider 
transition band.
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.3 Frequency-sampling Method    439
0
0.5
0
0.5
1
1.5
Filter
Samples
Ideal
0
0.5
2100
0
As 5 39.9 dB
250
f/fs
A(f) (dB)
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
A(f)
f/fs
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
Figure 6.16: Magnitude Responses of a Lowpass Filter with m 5 20 and an 
Optimal Transition-band Sample, Ar(f6) 5 .388
The coefficient vector, c 5 fc1, c2, c3gT, of the polynomial that interpolates the 
data points must satisfy the following system of three linear algebraic equations.
3
1
u1
u2
1
1
u2
u2
2
1
u3
u2
343
c1
c2
c34 53
As(u1)
As(u2)
As(u3)4
Solving this system for c then yields a polynomial model of the stopband atten-
uation as a function of the transition-band sample. The stopband attenuation is 
maximized by differentiating As(u), setting the result to zero, and solving for u. 
This yields the following optimal transition sample value.
umax 5 2c2
2c3
This optimization procedure is implemented in  exam6_6. Running exam6_6 from 
g_dsp produces a coefficient vector of c 5 f18.35, 62.74, 280.83gT. Thus the opti-
mal transition-band sample is
Ar(f6) 5 umax
5
262.74
2(280.83)
5 .388
A plot of the optimum magnitude response is shown in Figure 6.16. By using 
an optimal value for the transition sample, the stopband attenuation has 
increased to As 5 39.9 dB.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

440    Chapter 6  FIR Filter Design
The notion of using samples in the transition band can be extended to more than 
one sample with a corresponding improvement in the stopband attenuation. Rabiner  
et al. (1970) have developed tables of optimal transition-band samples for FIR filters 
of different lengths and different numbers of transition-band samples. For example, by 
using a filter of order m 5 15 with five passband samples and two transition-band sam-
ples, a stopband attenuation in excess of 100 dB can be achieved.
The frequency-sampling method also can be used to design linear-phase FIR filters 
whose impulse responses exhibit odd symmetry about k 5 my2. From (6.2.7) this corre-
sponds to a filter with the following type of frequency response.
 
H( f ) 5 jAr( f ) exp(2jmf T ) 
 (6.3.7)
Using (6.3.1) through (6.3.3), we can write the kth sample of the impulse response as 
follows where the number of samples is N 5 m 1 1.
 
h(k) 5 1
No
N21
i50
H(  fi) exp (  j2ikyN)
 
5 j
No
N21
i50
Ar(  fi) exp(2jmfiT ) exph  j2i(kyNgj
 
5 j
No
N21
i50
Ar(  fi) exp h  j f2i(k 2 .5m)yNgj
 
5 j
No
N21
i50
Ar(  fi)hcosf2i(k 2 .5m)yNg 1 j sinf2i(k 2 .5m)yNgj     (6.3.8)
For a real h(k) the cosine terms in (6.3.8) cancel one another and we have
 
h(k) 5 21
N o
N21
i50
Ar(fi) sinf2i(k 2 .5m)yNg 
 (6.3.9)
Since Ar( f ) is an odd function, the i 5 0 terms drops out because Ar(0) 5 0. Using 
the symmetry properties of the DFT in Table 4.7, we can show that the contributions 
of the i term and the N 2 i term are identical. Thus we can sum half of the terms and 
double the result. Recalling that bk 5 h(k) for an FIR filter, we arrive at the following 
expression for the coefficients of a linear-phase frequency-sampled filter of order m where 
m 5 N 2 1.
 
bk 5
22
m 1 1 o
floor(.5m)
i51
Ar(fi) sin 3
2i(k 2 .5m)
m 1 1 4, 0 # k # m  
 (6.3.10)
Note that for a type 3 filter the order m is even, in which case floor(my2) 5 my2. For a 
type 4 filter m is odd.
Frequency-sampled 
filter, types 3, 4
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.4 Least-squares Method     441
The DSP Companion contains the following function for designing a linear-phase 
FIR filter using the frequency-sampling method.
% F_FIRSAMP: Design a frequency-sampled FIR filter
%
% Usage:  
%        b = f_firsamp (Ar,m,fs,sym)
% Pre: 
%        Ar  = 1 by floor(m/2)+1 array containing the 
%              samples of the desired amplitude response.
%
%              |Ar(i)| = |H(f_i)|  
%
%              Here the ith discrete frequency is
%
%              f_i = (i-1)fs/(m+1)
%
%              where fs is the sampling frequency.
%        m   = order of filter
%        fs  = the sampling frequency in Hz
%        sym = symmetry of impulse response.  
%
%              0 = even symmetry of h(k) about k = m/2
%              1 = odd symmetry of h(k) about k = m/2
% Post: 
%        b   = 1 by m+1 vector of filter coefficients. 
DSP Companion
DsP Companion
Least-squares Method 
The frequency response of a digital filter is periodic with period fs. Because the window-
ing method uses a truncated Fourier series expansion of the desired amplitude response, 
Ad( f ), this method produces a filter that is optimal in the sense that it minimizes the 
following objective.
 
J 5#
fsy2
0
fAr( f ) 2 Ad( f )g2df  
 (6.4.1)
The actual amplitude response Ar( f ) is real but can be positive and negative. For a type 1  
or a type 2 linear-phase filter, Ar( f ) is even, and for a type 3 or a type 4 linear-phase 
filter it is odd. An alternative approach to filter design is to use a discrete version of 
the objective J. Let hF0, F1, Á , Fpj be a set of p 1 1 distinct frequencies with F0 5 0, 
Fp 5 fsy2, and
 
F0 , F1 , Á , Fp 
 (6.4.2)
The spacing between the discrete frequencies is often uniform, as in Fi 5 ifsy(2p), but this 
is not required. Next, let w(i) . 0 be a weighting function where w(i) specifies the relative 
6.4
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

442    Chapter 6  FIR Filter Design
importance of discrete frequency Fi. The special case, w(i) 5 1 for 0 # i # p is referred to 
as uniform weighting. A weighted discrete version of the objective function in (6.4.1) can 
be formulated as follows.
 
Jp 5 o
p
i50
{w(i)fAr(Fi) 2 Ad (Fi)g}2 
 (6.4.3)
A filter design technique that minimizes Jp is called a least-squares method. The filter 
amplitude response depends on the filter coefficient vector b, but the exact form of this 
dependence is determined by the type of linear-phase filter used. To illustrate the least-
squares method, consider the most general linear-phase filter, a type 1 filter of order m 
where m # 2p.
 
H(z) 5 o
m
i50
biz2i 
 (6.4.4)
Recall from Table 5.1 that for a type 1 filter, m is even and the impulse response satisfies 
the even symmetry condition h(m 2 k) 5 h(k). For FIR filters in general bi 5 h(i), which 
means bm2i 5 bi for 0 # i # m. For convenience, let  5 2fT and let m 5 2r. We then 
can write the frequency response of H(z) as follows.
H( f ) 5 o
m
i50
bi exp(2ji)
 
5  exp(2jr)o
m
i50
bi expf2j(i 2 r)g 
 (6.4.5)
Since m is even, the middle or rth term can be separated out from the sum. The remaining 
terms then can be combined in pairs using the symmetry condition, bm2i 5 bi, and Euler’s 
identity.
H( f ) 5  exp(2jr)hbr 1 o
r21
i50
bi expf2j(i 2 r)g 1 bm2i expf2j(m 2 i 2 r)gj
5  exp(2jr)hbr 1 o
r21
i50
bif expf2j(i 2 r)g 1  expf j(i 2 r 2 m 1 2r)gj
 
5  exp(2jr)hbr 1 o
r21
i50
bif expf2j(i 2 r)g 1  expfj(i 2 r)gj
 
5  exp(2jr)hbr 1 2o
r21
i50
bi cosf(i 2 r)gj
 
5 Ar( f ) exp(2jr) 
 (6.4.6)
For convenience, define ci 5 bi for i Þ r and cr 5 bry2. Recalling that  5 2f T, we 
then arrive at the following amplitude response for a type 1 linear-phase FIR filter 
of order 2r.
 
Ar( f ) 5 2o
r
i50
ci cosf2(i 2 r)f T g  
 (6.4.7)
Least-squares method
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.4 Least-squares Method     443
Now that we have an expression which shows the dependence of Ar( f ) on c, we 
can proceed to find an optimal value for c and therefore b. From (6.4.3), the objective 
function is
 
Jp(c) 5 o
p
i505w(i)32o
r
k50
ck cosf2(k 2 r)FiTg 2 Ad(Fi)46
2
 
5 o
p
i505o
r
k50
2w(i) cosf2(k 2 r)FiT gck 2 w(i)Ad(Fi)6
2
 
 (6.4.8)
To find a vector c that minimizes Jp(c), it is helpful to reformulate (6.4.8) in terms of 
vector notation. Let the (p 1 1) 3 (r 1 1) coefficient matrix G and the (p 1 1) 3 1 right-
hand side vector d be defined as follows.
 
Gik 5
D  
2w(i) cosf2(k 2 r)FiTg, 0 # i # p, 0 # k # r 
 (6.4.9a)
 
di 5
D  
w(i)Ad(Fi), 
0 # i # p 
 (6.4.9b)
If c 5 fc0, c1, Á , crgT is the unknown coefficient vector, then the objective function in 
(6.4.8) can be written in compact vector form as
 
Jp(c) 5 o
p
i505o
r
k50
Gikck 2 di6
2
5 o
p
i50
(Gc 2 d)2
i
 
5 uuGc 2 duu2 
 (6.4.10)
Since p $ r, the linear algebraic system Gc 5 d is overdetermined with more equations 
than unknowns, so that in general there is no c such that Gc 5 d. To find the coefficient 
vector c that minimizes Jp(c), multiply Gc 5 d on the left by GT. This yields the normal 
equations.
 
GTGc 5 GTd  
 (6.4.11)
The solution to (6.4.11) is the coefficient vector of the least-squares filter, the filter that 
minimizes Jp(c). Note that GTG is a square (r 1 1) 3 (r 1 1) matrix of full rank. Conse-
quently, the optimal coefficient vector can be expressed as
 
c 5 (GTG)21GTd 
 (6.4.12)
Recall from Section 3.9 that the matrix G1 5 (GTG)21GT is the pseudo-inverse of G. Nor-
mally, one does not solve for c using the pseudo-inverse. Instead, the normal equations 
in (6.6.11) are solved directly because this takes only about one third as many FLOPs for 
large r. The normal equations can become ill-condition for large values of r. Once the 
(r 1 1) 3 1 vector c is determined, the original (m 1 1) 3 1 coefficient vector b is then 
obtained as follows.
 
bi 55
ci,
0 # i , r
2cr,
i 5 r
c2r 2 i,
r , i # 2r
 
 (6.4.13)
Normal 
equations
Pseudo-inverse
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

444    Chapter 6  FIR Filter Design
0
0.5
0
0.5
1
1.5
Ideal
LS
0
0.5
0
0.5
1
1.5
Ideal
Weighted LS
f/fs
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
A(f)
A(f)
f/fs
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
Figure 6.17: Magnitude Responses of a Least-squares Bandpass Filter of 
Order m 5 64 Using p 1 1 5 129 Discrete Frequencies and Both Uniform 
Weighting and Increased Passband Weighting
Least-squares Bandpass Filter
EXAMPLE 6.7
To illustrate the least-squares method, consider the problem of designing a band-
pass filter with a piecewise-linear magnitude response that features a passband of 
3fsy16 # u f u # 5fsy16 and transition bands of width fsy32. Suppose p 5 40 and 
uniformly spaced discrete frequencies are used.
Fi 5 ifs
2p , 0 # i # p
Two cases are considered. The first one uses uniform weighting, and the sec-
ond weights the passband samples by w(i) 5 10. In both cases, the order of the 
FIR filter is m 5 40. The results, obtained by running  exam6_6, are shown 
in Figure 6.17. Note how by weighting the passband samples more heavily, 
the passband ripple is reduced, but at the expense of less attenuation in the 
stopband.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.5 Optimal Equiripple Filter Design     445
optimal Equiripple Filter Design 
The FIR filter design methods discussed thus far all involve optimization. The window-
ing method (with a rectangular window) minimizes the mean-squared error in (6.4.1). 
The frequency sampling method uses optimal placement of transition band samples to 
maximize the stopband attenuation. The least-squares method minimizes a weighted sum 
of squares of errors at discrete frequencies as in (6.4.3). In this section we examine an 
optimization technique that is based on minimizing the maximum of the absolute value 
of the error within the passband and the stopband.
6.5.1 Minimax Error Criterion
To simplify the development, we focus our attention on the most general type of  
linear-phase FIR filter, a type 1 filter. Consequently, the filter order is even with m 5 2p, 
and the impulse response h(k) exhibits even symmetry about the midpoint k 5 p. From 
(6.2.4), this means that the frequency response can be represented as follows.
 
H( f ) 5 Ar( f ) exp(2j2pf T ) 
 (6.5.1)
Here the filter amplitude response, Ar( f ), is a real even function of f . Like the fre-
quency response, the amplitude response is periodic with period fs. Therefore, the 
6.5
The DSP Companion contains the following function for designing a type 1 or  
type 2 linear-phase FIR filter using the least-squares method.
% F_FIRLS: Design a least-squares linear-phase FIR filter
%
% Usage: 
%        b  = f_firls (F,Ar,m,fs,w)
% Pre: 
%        F  = 1 by (p+1) vector containing discrete 
%             frequencies with F(1) = 0 and F(p+1)=fs/2.
%        Ar = 1 by (p+1) vector containing samples of
%             desired amplitude response at the discrete
%             frequencies.
% 
%             |Ar(i)| = |H_d(F_i)|  
%
%        m  = order of filter (1 <= m <= 2*p)
%        fs = the sampling frequency in Hz
%        w  = 1 by (p+1) vector containing weighting 
%             factors.  If w is not present, then 
%             uniform weighting is used.
% Post: 
%        b  = 1 by m+1 vector of filter coefficients. 
DSP Companion
DsP Companion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

446    Chapter 6  FIR Filter Design
amplitude response can be approximated with the following truncated trigonometric 
Fourier series.
 
Ar( f ) 5 o
p
i50
di cos(2if T ) 
 (6.5.2)
Notice that there are no sine terms because Ar( f ) is an even function. For a windowed 
filter with the rectangular window, the coefficients di are the Fourier coefficients. In 
this case the resulting filter minimizes the mean square error in (6.4.1). However, there 
is another way to compute the di that minimizes an alternative objective. To see this it is 
helpful to reformulate the truncated cosine series in (6.5.2) using Chebyshev polynomials. 
Recall from Section 5.5 that the first two Chebyshev polynomials of the first kind are 
T0(x) 5 1 and T1(x) 5 x, and the remaining Chebyshev polynomials are defined recur-
sively as follows.
 
Tk(x) 5 2xTk21(x) 2 Tk22(x), k $ 2 
 (6.5.3)
Thus Tk(x) is a polynomial of degree k. The Chebyshev polynomials are a classic family 
of orthogonal polynomials that have many useful properties (Schilling and Lee, 1988). In 
particular, recall from (5.5.26) that
 
Tkfcos()g 5  cos(k), k $ 0 
 (6.5.4)
In view of this harmonic generating property, the truncated cosine series in (6.5.2) can be 
recast in terms of Chebyshev polynomials as
 
Ar( f ) 5 o
p
k50
dkTk fcos(2f T )g 
 (6.5.5)
Thus Ar( f ) can be thought of as a trigonometric polynomial, a polynomial in 
x 5 cos(2f T ). To formulate an alternative error criterion, let Ad( f ) be the desired 
amplitude response, and let w( f ) . 0 be a weighting function. Then the weighted error 
at frequency f  is defined as
 
E( f ) 5
D  
w( f )fAd ( f ) 2 Ar( f )g 
 (6.5.6)
A logical way to select the weighting function is to set w( f ) 5 1yp in the passband 
and w( f ) 5 1ys in the stopband. This way, if one of the specifications is more stringent 
than the other, it will be given a higher weight because it is more difficult to satisfy. The 
weighting function in (6.5.6) can be scaled by a positive constant without changing the 
nature of the optimization problem. This leads to the following normalized weighting 
function.
 
w( f ) 55
syp,
f [ passband
1,
f [ stopband 
 (6.5.7)
Frequency-selective filter specifications place constraints on the amplitude response in 
the passband and the stopband, but not in the transition band. Let Fc denote the set of 
frequencies over which the amplitude response is specified. Then Fc is the following sub-
set of f0, fsy2g whereødenotes a union of disjoint sets.
 
Fc 5
D  
passbandsøstopbands 
 (6.5.8)
The frequency bands for the four basic frequency-selective filter types are summarized in 
Table 6.4.
Chebyshev polynomials
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.5 Optimal Equiripple Filter Design     447
Given the filter specification frequencies and the weighting function, the design 
objective is then to find a Chebyshev coefficient vector d [ Rp11 that solves the following 
optimization problem.
 
 min 
d[R p11 fmax 
f[Fc  huE( f )ujg 
 (6.5.9)
The performance criterion in (6.5.9) is called the minimax criterion because it minimizes 
the maximum of the absolute value of the error over the passband and the stopband.
6.5.2 Parks-McClellan Algorithm
The formulation of the amplitude response Ar( f ) as a polynomial in x 5 cos(2f T ) 
effectively transforms the filter design problem into a polynomial approximation prob-
lem over the set Fc. Let  denote the optimal value of the minimax performance criterion.
 
 5  min 
d[Rp11 fmax 
f[Fc  huE( f )ujg 
 (6.5.10)
Parks and McClellan (1972a,b) applied the alternation theorem from the theory of poly-
nomial approximation to solve for d. This theorem is due to Remez (1957).
The function Ar( f ) in (6.5.2) solves the minimax optimization problem in (6.5.15) if 
and only if there exists at least p 1 2 extremal frequencies F0 , F1 , Á , Fp11 in Fc 
such that E(Fi11) 5 2E(Fi) and
uE(Fi)u 5 , 0 # i , p 1 2
Extremal frequencies are frequencies at which the magnitude of the error achieves its 
extreme or maximum value within the passband or the stopband. Extremal frequencies 
include local minima, local maxima, and band edge frequencies. The name, alternation 
theorem, arises from the fact that the sign of the error alternates as one traverses the 
extremal frequencies. An example of an optimal amplitude response for a lowpass filter 
is shown in Figure 6.18. Notice that the local extrema are associated with ripples in the 
amplitude response and, within each band, these ripples are of the same size. It is for this 
reason that an optimal minimax filter is called an equiripple filter.
For the equiripple filter shown in Figure 6.18 the passband ripple is p 5 0.06, the 
stopband attenuation is s 5 0.04, and the filter order is m 5 12. Observe that there 
are four extrema frequencies in the passband and another four in the stopband. Thus  
the number of extrema frequencies is p 1 2 5 8, so from Proposition 6.1 the amplitude 
Minimax criterion
PRoPosITIoN
6.1 Alternation Theorem
Equiripple filter
Filter Type 
Fc
Lowpass  
 f0, FpgøfFs, fsy2g
Highpass  
 f0, FsgøfFp, fsy2g
Bandpass  
 f0, Fs1gøfFp1, Fp2gøfFs2, fsy2g
Bandstop  
 f0, Fp1gøfFs1, Fs2gøfFp2, fsy2g
Table 6.4:  
Frequency Bands 
for the Four Basic 
Frequency-selective 
Filters 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

448    Chapter 6  FIR Filter Design
response in Figure 6.18 is optimal. The need for at least p 1 2 extremal frequencies arises 
from the following observations. Since Ar( f ) is a polynomial of degree p, there can be up 
to p 2 1 local minima and local maxima where the slope of Ar( f ) is zero. For a lowpass 
or highpass filter, the optimal amplitude response also goes through the interior pass-
band and stopband edge points.
 
Ar(Fp) 5 1 2 p 
 (6.5.11a)
 
Ar(Fs) 5 s
 
 (6.5.11b)
That makes p 1 1 extremal frequencies. In addition at least one, and perhaps both, of the 
endpoint frequencies, F 5 0 and F 5 fsy2, are also extremal frequencies. Therefore, for 
a lowpass or a highpass filter the number of extremal frequencies in the optimal ampli-
tude response will be either p 1 2 or p 1 3. Bandpass and bandstop filters can have up 
to p 1 5 extremal frequencies because there are two additional band edge frequencies. 
From Proposition 6.1, an optimal equiripple amplitude response must have at least p 1 2 
extremal frequencies.
In order to determine the optimal Chebyshev coefficient vector d [ Rp11, we start 
with the alternation theorem. Using the definition of E( f ) in (6.5.6), we have
 
w(Fi)fAd(Fi) 2 Ar(Fi)g 5 (21)i, 0 # i # p 1 1 
 (6.5.12)
These equations can be recast as
 
Ar(Fi) 1 (21)i
w(Fi) 5 Ad(Fi), 0 # i # p 1 1 
 (6.5.13)
Let i 5 2FiT for 0 # i # p 1 1. Then substituting for Ar(Fi) using (6.5.2) yields
 
o
p
k50
dk cos(ki) 1 (21)i
w(Fi) 5 Ad (Fi), 0 # i # p 1 1 
 (6.5.14)
0
0.5
20.2
0
0.2
0.4
0.6
0.8
1
1.2
f/fs
Ar(f)
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45
Figure 6.18:  
Optimal Amplitude 
Response Using 
the Minimax  
Optimization Crite-
rion with m 5 12 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.5 Optimal Equiripple Filter Design     449
Next let fd0, Á , dp, gT represent a vector of unknown parameters. Then the p 1 2 equa-
tions in (6.5.14) can be rewritten as the following vector equation.
3
1
 cos(0)
Á
 cos(p0)
1yw(F0)
1
 cos(1)
Á
 cos(p1)
21yw(F1)
o
1
 cos(p)
Á
 cos(pp)
(21)pyw(Fp)
1
 cos(p11)
Á
 cos(pp11)
(21)p11yw(Fp11)43
d0
d1
o
dp
4
53
Ad (F0)
Ad (F1)
o
Ad (Fp)
Ad (Fp11)4
    (6.5.15)
The linear algebraic system in (6.5.15) can be solved for d and , but the real problem 
is to find appropriate values for the extremal frequencies Fi. This can be done using an 
iterative process (Remez, 1957). One starts with an initial guess such as p 1 2 frequencies 
equally spaced over the set Fc. Once (6.5.15) is solved for d, the error function E( f ) in 
(6.5.6) can be computed. Typically E( f ) is evaluated on a dense grid of at least M 5 16p 
points in Fc. This discrete grid of frequencies should include the edge points of the bands 
in Fc. If uE(Fi)u ,  1  for some small tolerance , then convergence has been achieved. 
Otherwise, a new set of extremal frequencies is determined and the process is repeated.
Equation (6.5.15) can be solved at each iteration, but when m is large this step is slow 
and as m increases the solution becomes increasingly sensitive to finite word length effects 
due to accumulated roundoff error. A more efficient and accurate alternative is to first 
find the parameter . The following algorithm can be used to estimate the ripple size  
associated with a given set of extremal frequencies (Janovetz and Hanson, 1995).
1.  Set q 5
 floor (py15 1 1),  5 105, and xi 5 cos(2FiT ) for 0 # i # p 1 1.
2.  For i 5 0 to p 1 1 Do
     { 
        (a) Set c 5 1
        (b) For j 5 0 to q 2 1 Do
              { 
               (i)  Set k 5 j
               (ii) While (k # p 1 1) Do
                   { 
                       If k Þ i set c 5 2(xi 2 xk)c
                       Set k 5 k 1 q
                   } 
             }
        (c) If (ucu , ) set c 5 .
        (d) Set ai 5 1yc
     }
3.  Set  5 0,  5 0, s 5 1;
4.  For i 5 0 to p 1 1 Do
     { 
 5  1 aiAd(Fi)
 5  1 saiyW(Fi)
s 5 2s
     }
5.  Set  5 y
ALgoRITHM
6.2 Ripple size
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

450    Chapter 6  FIR Filter Design
Once  is known, the terms in (6.5.13) involving  can be brought over to the right-
hand side. The new augmented right-hand side vector is then
 
yi 5 Ad(Fi) 2 (21)i
w(Fi) , 0 # i # p 
 (6.5.16)
It then follows that (6.5.13) can be rewritten as
 
Ar(Fi) 5 yi, 0 # i # p 
 (6.5.17)
Thus the value of Ar( f ) is known at p 1 1 frequencies. Since Ar( f ) is a polynomial of 
degree p in x 5 2 f T, there is no need to solve (6.5.15) for the coefficient vector d at 
each iteration. Instead, the p 1 1 points in (6.5.17) can be used to construct Lagrange 
interpolating polynomials (Parks and McClellan, 1972a,b). Using xf 5 cos(2 f T ), and 
xi 5 cos(2FiT ) for 0 # i # p, the ith Lagrange interpolating polynomial is as follows 
where Π denotes the product.
 
Li( f ) 5 P
p
j50,jÞi
xf 2 xj
xi 2 xj
, 0 # i # p 
 (6.5.18)
Note that Li(Fk) 5 (i 2 k) where (i) is the unit impulse. Using this orthogonality prop-
erty and (6.5.17), the amplitude response can be represented as follows.
 
Ar( f ) 5 o
p
i50
yi Li( f ) 
 (6.5.19)
Given the expression for Ar( f ) in (6.5.19), the error E( f ) can be evaluated on the dense 
grid of points in Fc and then a new set of extremal points Fi can be computed. Candidates 
for the new frequencies are generated from local maxima and local minima on the dense 
grid including the two end points, 0 and fsy2. For a local maximum to be included, the 
error must be positive, and for a local minimum it must be negative.
If more than p 1 2 candidate frequencies are identified, the excess frequencies must 
be removed. The heuristic used to eliminate excess frequencies is to delete adjacent non- 
alternating extremal frequencies first, in each case removing the one with the smallest 
absolute error (Janovetz and Hanson, 1995). Once all of the adjacent non-alternating 
extremal frequencies have been deleted, the frequencies with the smallest absolute error 
are removed until there is only one excess frequency left. When only one excess frequency 
remains, either the first or the last frequency is deleted depending on which has the 
smaller absolute error.
The exchange algorithm converges when the percent difference between the largest 
extremal error and the smallest extremal error becomes sufficiently small, or a maximum 
number of iterations has been reached.
Once convergence has taken place, one must identify the filter coefficients. This is 
done by applying the frequency sampling method to Ar( f ) as discussed in Section 6.3. 
For a type 1 linear-phase FIR filter, it follows from (6.3.6) that the filter coefficients are
 
fi 5
ifs
m 1 1, 
0 # i # m 
 (6.5.20a)
 
bk 5 Ar(f0)
m 1 1 1
2
m 1 1 o
p
i51
Ar(fi) cos3
2i(k 2 p)
m 1 1 4, 0 # k # m 
 (6.5.20b)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.5 Optimal Equiripple Filter Design     451
The Remez exhange algorithm for finding the optimal extremal frequencies for an 
equiripple FIR filter is summarized as follows.
1.  Pick m 5 2p, N . 0, and  . 0. Set k 5 0.
2.   Construct a dense grid on the set Fc consisting of M 5 16p points fi including 
the edge points of the bands in Fc.
3.   Construct an initial guess for the extremal frequencies Fi for 0 # i # p 1 1 using 
p 1 2 of the grid points uniformly spaced over Fc.
4.  Do
     {
        (a) Apply Algorithm 6.2 to compute the ripple size .
        (b)  Compute the amplitude response Ar(fi) and the error E(fi) over the dense 
grid 0 # i # M.
        (c)  Using E(fi) for 0 # i # M, search for a new set of extremal frequencies Fi 
for 0 # i # p 1 1.
        (d)  Let Em and EM be the extremal errors whose magnitudes are smallest and 
largest, respectively.
        (e) Set k 5 k 1 1.
     }
5.  While (EM 2 Em)yEM $ ) and (k , N)
6.  Compute the coefficient vector b [ Rm 1 1 using (6.5.20).
The minimax ripple, , may or may not satisfy the stopband specification  # s 
depending on the filter order m. If the ripples in the resulting amplitude response exceed 
the filter specifications, then the filter order m should be increased. Kaiser has proposed 
the following rough estimate for the equiripple filter order needed to meet a given design 
specification (Rabiner et al., 1975).
 
m < ceil 5
2f10 log10 (ps) 1 13g
14.6B
 norm 
1 16 
 (6.5.21)
Here B
 norm 5 uFs 2 Fpuyfs is the normalized width of the transition band. Equation 
(6.5.21) can be used as a starting point for choosing m. If the filter specifications are not 
met, then m can be gradually increased until they are met or exceeded.
ALgoRITHM
6.3 Equiripple FIR Filter
Equiripple filter order
Equiripple Filter
EXAMPLE 6.8
As an illustration of the equiripple filter design technique, consider the prob-
lem of designing a bandpass filter with fs 5 200 Hz and the following design 
specifications.
(Fs1, Fp1, Fp2, Fs2) 5 (36, 40, 60, 64) Hz
(p, s) 5 (0.02, 0.02)
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

452    Chapter 6  FIR Filter Design
0
0.5
0
0.2
0.4
0.6
0.8
1
f/fs
A(f)
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45
Figure 6.19: Magnitude Response of the Optimal Equiripple Bandpass  
Filter, m 5 84
The transition band is fairly narrow in this case with a normalized width of
B
 norm 5
Fp1 2 Fs1
fs
5 0.04
From (6.5.21), an initial estimate of the filter order required to meet these 
specifications is
m < ceil 5
2f10 log10(0.0004) 1 13g
14.6(0.04)
1 16
 
5 73
The coefficients of the equiripple filter can be computed by running script 
exam6_13. The estimate of m < 73 is a bit low, and the resulting filter does not 
quite meet the specifications. The magnitude response for the case m 5 84 is 
shown in Figure 6.19, where the equiripple nature of the magnitude response is 
apparent.
It is somewhat difficult to visualize the degree to which the specifications are 
satisfied while viewing Figure 6.19. Instead, the stopband attenuation is easier to 
see when the magnitude response is plotted using the logarithmic dB scale. Using 
(5.2.7), the passband ripple and stopband attenuation in dB are
Ap 5 0.1755 dB
 
As 5 33.98 dB
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.5 Optimal Equiripple Filter Design     453
The DSP Companion contains the following function for designing an optimal 
equiripple FIR filter using the Parks-McLellan algorithm.
% F_FIRPARKS: Design a Parks-McClellan equiripple FIR filter
%
% Usage:
%        [b,n]  = f_firparks (m,Fp,Fs,deltap,deltas,ftype,fs)
% Pre:
%        m      = filter order (m >= 2)
%        Fp     = passband cutoff frequency or frequencies
%        Fs     = stopband cutoff frequency or frequencies
%        deltap = passband ripple
%        deltas = stopband attenuation
%        ftype  = filter type
%
%                 0 = lowpass
%                 1 = highpass
%                 2 = bandpass (Fp and Fs are vectors)
%                 3 = bandstop (Fp and Fs are vectors)
%
DSP Companion
DsP Companion
0
0.5
250
245
240
235
230
225
220
215
210
25
0
5
f/fs
A(f) (dB)
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45
Figure 6.20: Magnitude Response of the Optimal Equiripple Bandpass Filter 
in dB, m 5 84  
A plot of the magnitude response in units of dB is shown in Figure 6.20. Here 
it is clear that for a filter of order m 5 84, the specified stopband attenuation of 
As 5 33.98 dB is exceeded. Therefore, the filter order might be reduced somewhat 
and still meet the specification.
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

454    Chapter 6  FIR Filter Design
Differentiators and Hilbert Transformers 
In this section we investigate some specialized linear-phase FIR filters that have impulse 
responses and amplitude responses that exhibit odd symmetry. Filters with odd symme-
try have a phase offset of  5 y2. Consequently, after the effects of the group delay 
have been removed, the steady-state output of the filter is in phase quadrature with a 
sinusoidal input.
6.6.1 Differentiator Design
Recall that in Section 6.1 we considered the problem of designing low-order differentia-
tors based on numerical approximations of the slope of the input signal. An alternative 
approach is to design a filter whose frequency response corresponds to that of a differ-
entiator. Recall that an analog differentiator is a system with the following frequency  
response
 
Ha( f ) 5 j2f  
 (6.6.1)
Thus a differentiator has a constant phase response of a( f ) 5 y2 and a magnitude 
response that increases linearly with f . If a causal linear-phase FIR filter is used to 
approximate a differentiator, then we must allow for a group delay of  5 mTy2. Thus 
the problem is to design a digital filter with the following frequency response.
 
Hd ( f ) 5 j2f T exp(2jmf T ) 
 (6.6.2)
Note that a T is included in the amplitude response, Ar( f ) 5 2f T, because a digital 
differentiator only processes frequencies in the range 0 # f # fsy2. The group delay in 
this case is  5 mTy2.
In order to choose between a type 3 linear-phase FIR filter and a type 4 filter, it is 
helpful to look at the zeros of H( f ). Recall from Table 5.1 that a type 3 filter with even 
order m has a zero at f 5 0 and a zero at f 5 fsy2. This is in contrast to a type 4 filter 
with an odd order m that only has a zero at f 5 0. If Ar( f ) 5 2f T, then Ar(0) 5 0 and 
Ar(fsy2) 5 . Thus the type 4 linear-phase filter appears to be the filter of choice. The 
effectiveness of the type 4 filter in comparison with the type 3 filter is illustrated with the 
following example.
6.6
Phase quadrature
Differentiator
%        fs = sampling frequency in Hz
% Post:
%        b = 1 by (m+1) coefficient vector of numerator
%            polynomial
%        n = optional estimate of filter order needed to
%            meet specs.
%
% Note: This function uses an equiripple FIR filter
%       implementation based on a free C version developed
%       by Jake Janovetz (janovetz@uiuc.edu).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.6 Differentiators and Hilbert Transformers     455
Differentiator
EXAMPLE 6.9
Consider a linear-phase FIR filter of order m with an impulse response h(k) that 
exhibits odd symmetry about the midpoint k 5 my2. Suppose the windowing 
method is used to design Hd(z). Since H( f ) does not contain any jump disconti-
nuities, a rectangular window should suffice. From (6.2.9) the filter coefficients are
bi 5 22T#
fsy2
0
Ar( f ) sinf2(i 2 .5m)f T gdf
5 22T#
fsy2
0
2f T sinf2(i 2 .5m)f T gdf
To simplify the integral, let  5 2fT. Then the expression for bi becomes
bi 5 21
 #

0
 sinf(i 2 .5m)gd
5 21

  5
 sinf(i 2 .5m)g
(i 2 .5m)2
2  cosf(i 2 .5m)g
i 2 .5m
6u

0
Thus the filter coefficients for the differentiator are
bi 5
 cosf(i 2 .5m)g
i 2 .5m
2
 sinf(i 2 .5m)g
(i 2 .5m)2
, 0 # i # m
Note that for a type 3 filter m is even and (i 2 .5m) is a multiple of . In this case 
when i Þ .5m, the sine terms drop out. Using L’Hospital’s rule and putting both 
terms over a common denominator, the midpoint case i 5 my2 yields bmy2 5 0. 
Hence for a type 3 filter
bi 5
 cosf(i 2 .5m)g
i 2 .5m
, 0 # i # m, i Þ .5m
For a type 4 filter, m is odd, and the factor i 2 .5m never goes to zero. In this case 
(i 2 .5m) is an odd multiple of y2, and the cosine terms drop out. Hence for 
a type 4 filter
bi 5 2
 sinf(i 2 .5m)g
(i 2 .5m)2
, 0 # i # m
Plots of the magnitude and impulse responses for a type 3 filter with m 5 12 are 
shown in Figure 6.21. These were generated by running exam6_9. Notice that the 
zero at f 5 fsy2 causes significant ringing in the magnitude response, particularly 
at higher frequencies. In this cases the impulse response goes to zero relatively 
slowly, suggesting a poor fit.
When a type 4 filter is used, the results are more effective. Plots of the mag-
nitude and impulse responses for the case m 5 11 are shown in Figure 6.22. Even 
though this is a lower-order filter than the type 3 filter, it is clearly a better fit with 
some error evident at higher frequencies. Notice how the impulse response goes 
to zero relatively quickly.
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

456    Chapter 6  FIR Filter Design
0
0.5
0
1
2
3
4
Ideal
FIR filter
22
0
2
4
6
8
10
12
14
21
0
1
k
f/fs
A(f)
h(k)
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
Figure 6.21: A Differentiator Using a Type 3 Linear-phase FIR Filter of Order 
m 5 12 and a Rectangular Window 
0
0.5
0
1
2
3
4
Ideal
FIR filter
22
0
2
4
6
8
10
12
21.5
21
20.5
0
0.5
1
1.5
k
f/fs
A(f)
h(k)
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
Figure 6.22: A Differentiator Using a Type 4 Linear-phase FIR Filter of Order 
m 5 11 and a Rectangular Window
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.6 Differentiators and Hilbert Transformers     457
Noncausal Differentiator
It should be pointed out that the phase response of the FIR differentiator is ( f ) 5 y2 2 mfT
, which corresponds to (0) 5 y2 and a group delay of  5 mTy2. A constant phase response 
of ( f ) 5 y2, corresponding to the pure differentiator in (6.6.1), can be achieved if a non-
causal implementation is used. A noncausal differentiator is obtained by multiplying Hd(z) by 
zp where p 5 floor(my2). Thus the transfer function of the noncausal filter is
 
HD(z) 5 o
m
i50
bizp 2 i 
 (6.6.3)
A noncausal filter can be used when offline batch processing is used with the entire input 
signal available ahead of time. If the differentiator requires a real-time implementation, 
then a causal filter must be used.
6.6.2 Hilbert Transformer Design
An important example of a filter that can be realized with a type 3 or type 4 linear-phase 
FIR filter is the Hilbert transformer. The analog version of a Hilbert transformer is a 
system with the following frequency response.
 
Ha( f ) 5 2j sgn ( f ) 
 (6.6.4)
Recall from (5.5.8) that sgn denotes the signum or sign function defined  sgn ( f ) 5 fyu f u for 
f Þ 0 and  sgn (0) 5 0. Thus a Hilbert transformer imparts a constant phase shift of 2y2 
for f . 0 and y2 for f , 0. Hilbert transformers are used in a number of applications in 
communications and speech processing. Recall from Section 5.5 that Hilbert transformers 
can be used to generate complex half-band signals whose spectrum is zero for f , 0.
To implement a Hilbert transformer with a causal linear-phase FIR filter, we insert 
a delay of  5 mTy2 where m is the filter order. Using (6.6.4), this yields the following 
frequency response for a digital Hilbert transformer.
 
Hh( f ) 5 2j sgn ( f ) exp(2jmf T ), 0 , u f u , fsy2 
 (6.6.5)
From Table 5.1, this corresponds to the frequency response of a linear-phase FIR filter 
with a phase offset of  5 y2 and an amplitude response of Ar( f ) 5 2 sgn ( f ). Thus a 
type 3 or a type 4 filter can be used.
Noncausal 
differentiator
Hilbert transformer
Hilbert Transformer
EXAMPLE 6.10
Consider a linear-phase FIR filter of order m with an impulse response h(k) that 
exhibits odd symmetry about k 5 my2. Suppose the windowing method is used to 
design H(z). From (6.2.9) the filter coefficients, using bi 5 h(i), are
bi 5 22T#
fsy2
0
Ar( f ) sinf2(i 2 .5m)f Tgdf
 
5 2T#
fsy2
0
 sinf2(i 2 .5m)f Tgdf
 
55
22T cosf2(i 2 .5m)f Tg
2(i 2 .5m)T
6u
fsy2
0
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

458    Chapter 6  FIR Filter Design
A(f)
h(k)
0
0.5
0
0.5
1
1.5
Ideal
FIR filter
25
0
5
10
15
20
25
30
35
21
0
1
k
f/fs
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
Figure 6.23: A Hilbert Transformer Using a Type 3 Linear-phase FIR Filter of 
Order m 5 30 and a Rectangular Window
Thus the filter coefficients for the Hilbert transformer using a rectangular win-
dow are
bi 5 1 2  cosf(i 2 .5m)g
(i 2 .5m)
, 0 # i # m
When m is odd as in a type 4 filter, the factor i 2 .5m never goes to zero. In this 
case (i 2 .5m) is an odd multiple of y2 and the cosine term drops out. Thus for 
a type 4 filter
bi 5
1
(i 2 .5m), 0 # i # m
For a type 3 filter with m even, L’Hospital’s rule applied to the case i 5 my2 yields 
bmy2 5 0. Plots of the magnitude and impulse responses for the type 3 case with 
m 5 30 are shown in Figure 6.23, where a rectangular window is used. These 
were generated by running  exam6_10. Notice that this approximates a Hilbert 
transformer, but only over a limited range of frequencies. Often this is sufficient 
depending on the application. The ripples can be reduced by tapering the coef-
ficients with a data window. Plots of the magnitude and impulse responses for a 
type 3 filter with m 5 30 and the Hamming window are shown in Figure 6.24. It 
is evident that the magnitude response is smoother in the passband, but the width 
of the passband has been reduced somewhat in this case.
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.6 Differentiators and Hilbert Transformers     459
The DSP Companion contains the following functions for designing a differentiator 
filter and a Hilbert transformer filter, respectively.
% F_FIRDIFF: Design a differentiator linear-phase FIR filter
% F_HILBERT: Design a Hilbert transformer linear-phase FIR filter
%  
% Usage:
%        b = f_firdiff(m,win);
%        b = f_hilbert(m,win);
% Pre: 
%        m   = the filter order 
%        win = the window type to be used:
%  
%              0 = rectangular
%              1 = Hanning
%              2 = Hamming
%              3 = Blackman
% Post:
%        b = 1 by (m+1) array of FIR filter coefficients
DSP Companion
DsP Companion
0
0.5
0
0.5
1
1.5
Ideal
FIR filter
25
0
5
10
15
20
25
30
35
21
0
1
k
f/fs
A(f)
h(k)
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
Figure 6.24: A Hilbert Transformer Using a Type 3 Linear-phase FIR Filter of 
Order m 5 30 and a Hamming Window
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

460    Chapter 6  FIR Filter Design
Quadrature Filter Design
In this section a general two-stage quadrature filter is developed that is designed to meet 
both amplitude response and phase response specifications, but with a group delay. Stage 
one makes use of a Hilbert transformer to produce a pair of signals in phase quadrature. 
Stage two applies linear-phase FIR filters to these signals to generate the real and imagi-
nary parts of the desired frequency response.
6.7.1 generation of a Quadrature Pair
The overall structure of the two-stage quadrature filter is shown in Figure 6.25. It features 
two parallel branches, with each branch consisting of a cascaded pair of linear-phase 
FIR filters.
The purpose of stage one is to generate a quadrature pair that is synchronized with 
a sinusoidal input. Suppose the input is a cosine of frequency f .
 
x(k) 5 cos(2f kT ) 
 (6.7.1)
The FIR filter in the upper-left corner of Figure 6.25, H1(z) 5 z2my2, is a pure delay of my2 
samples. It is assumed that m is even, so my2 is an integer. Thus the magnitude response 
is A1( f ) 5 1, and the phase response is 1( f ) 5 2mfT. It follows that the steady-state 
output of the upper branch of stage one is simply x(k) delayed by my2 samples.
 
x1(k) 5 cosf2f(k 2 .5m)Tg 
 (6.7.2)
The FIR filter in the lower-left corner of Figure 6.25, H2(z), is a Hilbert transformer. 
If the order of the FIR filter is m, then this corresponds to a type 3 linear-phase filter 
with frequency response
 
H2( f ) 5 A2( f ) expf2j(mf T 1 y2)g, 0 ,  f ,  fsy2 
 (6.7.3)
For m sufficiently large, the magnitude response is A2( f ) < 1. Note that the phase is lin-
ear with a group delay of mTy2 and an offset of 2 y2. Thus the steady-state output of 
the lower branch of stage one is
x2(k) 5 A2( f ) cosf2f (k 2 .5m)T 2 y2g
 
5 A2( f ) sinf2f (k 2 .5m)T g
 
< sinf2f (k 2 .5m)T g, 0 , f ,  fsy2 
 (6.7.4)
6.7
Figure 6.25:  
A Two-stage  
Quadrature Filter 
H1(z)
H2(z)
G(z)
F(z)
y1(k)
y2(k)
y(k)
x1(k)
Stage 1
Stage 2
x2(k)
x(k)
1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.7 Quadrature Filter Design    461
It follows from (6.7.2) and (6.7.4) that the stage one outputs, x1(k) and x2(k), form a 
quadrature pair. The relative phase relationship of y2 is exact, and the amplitude of 
x1(k) is exact with A1( f ) 5 1. However, the amplitude of x2(k) is A2( f ) < 1 over an 
interval within the range 0 , f , fsy2. Since H2(z) is a type 3 FIR linear-phase filter, 
A2(0) 5 A2(fsy2) 5 0.
21
20.5
0
0.5
1
21
20.8
20.6
20.4
20.2
0
0.2
0.4
0.6
0.8
1
x1(k)
x2(k)
Figure 6.26: Phase Plane Plots Corresponding to Different Frequencies 
Generated by Stage One Using a Hilbert Transformer of Order m 5 60 with a 
Hamming Window 
Quadrature Pair
EXAMPLE 6.11
To illustrate the effectiveness of stage one in generating a quadrature pair, con-
sider the case of a linear-phase filter of order m 5 30 with a Hamming window. 
The magnitude response of this Hilbert transformer was shown previously in 
Figure 6.25. To directly view the amplitude and phase relationships between 
x1(k) and x2(k), suppose the filter H2(z) is driven by the cosine input in (6.7.1). 
Steady-state plots of x2(k) versus x1(k) for N 5 100 values of f  are shown in 
Figure 6.26, where
fi 5 ifs
2N, 0 # i , N
The elliptical patterns correspond to frequencies near the two ends of the Nyquist 
range. The darker circle of unit radius corresponds to the range of frequencies 
over which the quadrature signals both have unit amplitude.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

462    Chapter 6  FIR Filter Design
6.7.2 Quadrature Filter Design
Next consider the problem of designing an FIR filter with a desired magnitude response 
Ad( f ) and a desired phase response d( f ), subject to the constraint that the total phase 
response d( f ) also includes a linear-phase term representing a constant group delay of 
q 5 mT. Thus the total phase is
 
d( f ) 5 d( f ) 2 2mfT  
 (6.7.5)
Here d( f ) will be referred to as the residual phase, the phase that remains after the effects 
of the group delay have been removed. Suppose the FIR filters F(z) and G(z) in stage 
two of Figure 6.25 are type 1 linear-phase filters of order m. Let Af( f ) and Ag( f ) denote 
the amplitude responses of F(z) and G(z), respectively, and let input x(k) be a cosine of 
frequency f  as in (6.7.1). Since F(z) is linear-phase, from (6.7.2) the steady-state output 
of F(z) is
 
y1(k) 5 Af ( f ) cosf2f(k 2 m)Tg 
 (6.7.6)
Next, suppose the Hilbert transformer bandwidth is sufficiently large that A2( f ) < 1 for 
the frequency of interest. Since G(z) is linear-phase, from (6.7.4) the steady-state output 
of G(z) is
 
y2(k) < Ag( f ) sinf2f (k 2 m)Tg 
 (6.7.7)
Making use of the cosine of the sum trigonometric identity from Appendix 2, the 
desired steady-state output of the quadrature filter with magnitude response Ad( f ) and 
phase response d( f ) 5 d( f ) 2 2mfT is
yd (k) 5 Ad ( f ) cosf2f (k 2 m)T 1 d ( f )g
 
5 Ad ( f ) cosfd ( f )g cosf2f (k 2 m)Tg
 
2Ad ( f ) sinfd ( f )g sinf2f (k 2 m)T g 
 (6.7.8)
From (6.7.6), (6.7.7) and Figure 6.24, the quadrature filter output is
y(k) 5 y1(k) 1 y2(k)
 
< Af( f ) cosf2f(k 2 m)T g 1 Ag( f ) sinf2f(k 2 m)T g 
 (6.7.9)
Setting yd(k) 5 y(k) from (6.7.8) and (6.7.9), the desired amplitude responses for the stage 
two linear-phase filters are
 
Af ( f ) 5 Ad ( f ) cosfd ( f )g 
 (6.7.10)
 
Ag( f ) 5 2Ad ( f ) sinfd ( f )g 
 (6.7.11)
Here F(z) and G(z) are designed to approximate Af( f ) and Ag( f ), respectively. The 
quadrature filter transfer function from Figure 6.25 is
 
Hq(z) 5 H1(z)F(z) 1 H2(z)G(z)  
 (6.7.12)
Since H1(z), H2(z), F(z), and G(z) are all linear-phase FIR filters of order m, the two-
stage quadrature filter Hq(z) is an FIR filter of order 2m. It will have a group delay of 
q 5 mT, a magnitude response that approximates Ad( f ), and a residual phase response 
that approximates d( f ).
Residual phase
Amplitude responses
Quadrature filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.7 Quadrature Filter Design    463
To interpret the contributions of F(z) and G(z), note that the desired frequency 
response Hd( f ), minus the delay, can be written in terms of its real and imaginary parts 
as follows.
 
H
⁄
d ( f ) 5 Ad ( f ) cosfd ( f )g 1 jAd ( f ) sinfd ( f )g 
 (6.7.13)
Comparing (6.7.13) with (6.7.10) and (6.7.11), we see that filter F(z) is used to design the 
real part of the desired frequency response, and filter G(z) is used to design the imaginary 
part. In summary, the four linear-phase FIR filters that make up the quadrature filter in 
Figure 6.25 perform the following functions. The FIR filter H1(z) in the upper left pro-
vides a delay of mTy2 for synchronization, Hilbert transformer H2(z) provides a phase 
shift of y2 to produce a quadrature pair, F(z) approximates the real part of the desired 
frequency response, and G(z) approximates the imaginary part.
Since the Hilbert transformer is a type 3 linear-phase filter, its frequency response 
must satisfy the end-point zero constraints, A2(0) 5 A2(fsy2) 5 0. From Figure 6.25 this 
means that in the steady state y2(k) 5 0 when f 5 0 and when f 5 fsy2. This is not as 
serious of a limitation as, at first, it might appear. Recall that the two frequency limits 
f 5 0 and f 5 fsy2 correspond to z 5 61. However, for a real system with desired trans-
fer function Hd(z), the points Hd( 6 1) will be real. Thus for a real system, the desired 
phase response will satisfying
 
 mod fd ( f ), g 5 0, f [ h0, fsy2j 
 (6.7.14)
Since the desired phase angle for a real filter is a multiple of  at the two end points, it 
follows that the imaginary part of the desired frequency response will be zero at f 5 0 
and f 5 fsy2.
Quadrature Filter
EXAMPLE 6.12
As an illustration of a quadrature filter, suppose the desired magnitude 
response consists of three sections as follows, where fT represents normalized 
frequency.
Ad( f ) 55
.5(1 1 8f T),
0 # f T , .125
64(f T 2 .125)2,
.125 # f T # .375
1 1 .5 sinf16(f T 2 .375)g, .375 , f T # .5
Next let the residual phase response be the following.
d( f ) 5 (1 2 .25fT) 1 2 sin(8f T )
Suppose the linear-phase filters are of order m 5 160 with a Hamming win-
dow. This produces an overall group delay of  5 160T. Plots of the magnitude 
response and the residual phase response of the quadrature filter, obtained by 
running exam6_12, are shown in Figure 6.27. Although the fit is not exact, it is 
an effective fit of both the desired magnitude response and the desired residual 
phase response.
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

464    Chapter 6  FIR Filter Design
Quadrature filters include linear-phase filters as a special case. Observe that when the 
residual phase response is set to d( f ) 5 0 this corresponds to linear phase. From (6.7.1), this 
implies Ag( f ) 5 0 and Af( f ) 5 Ad( f ) which means that F(z) is an mth order linear-phase 
filter and G(z) 5 0. Given the delay of my2 samples in stage one, the quadrature filter will 
have the accuracy of an mth order linear-phase filter, but with a group delay of q 5 mT.
The desired magnitude response in Figure 6.27 only goes to zero at a single point, 
f 5 .25fs. Recall from the Paley-Wiener theorem in Proposition 5.1 that if the magnitude 
response is identically zero over an interval of nonzero length, then an exact realization is 
not possible with a causal filter. Examples include the ideal lowpass, highpass, bandpass 
and bandstop filters. It is of interest to see what happens when the quadrature design 
method is applied to such a filter. Using (6.7.10) and (6.7.11), the magnitude response 
and the residual phase response of the quadrature filter can be expressed in terms of the 
amplitude responses of the second stage filters as follows.
 
Aq( f ) 5 ÏA2
f( f ) 1 A2
g( f ) 
 (6.7.15)
 
q( f ) 5 arctan 3
2Ag( f )
Af ( f ) 4 
 (6.7.16)
The expression for Aq( f ) is well behaved and should yield a good approximation except 
possibly near the ends of the Nyquist range where the magnitude response of the Hilbert 
transformer begins to decrease. However, the expression for q( f ) is not well behaved when 
Ad( f ) 5 0. Indeed if Ad( f ) 5 0 over a stopband interval, then from (6.7.10) and (6.7.11) 
it follows that both Af  ( f ) 5 0 and Ag( f ) 5 0. But from (6.7.15), this implies that the 
residual phase response over the idealized stopband is arctan(0y0) which is numerically 
0
0.1
0.05
0.15
0.25
0.35
0.45
0.05
0.15
0.25
0.35
0.45
0.2
0.3
0.4
0.5
20.5
0
0.5
1
1.5
 f/fs
 f/fs
A(f)
Ideal
Quadrature Filter
0
0.1
0.2
0.3
0.4
0.5
25
0
5
(f)
Ideal
Quadrature Filter
Figure 6.27: Magnitude and Residual Phase Responses of the Two-Stage 
Quadrature Filter from Example 6.12 with m 5 120 and a Hamming Window 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.7 Quadrature Filter Design    465
not well defined. Put another way, the phase angle is not meaningful when the magnitude 
is zero. As a simple numerical remedy, let s . 0 be a small stopband attenuation factor. 
One can then replace the desired magnitude response in (6.7.10) and (6.7.11) as follows, 
which effectively bounds Ad ( f ) away from zero.
 
Af  ( f ) 5 maxhAd ( f ), sj cosfd ( f )g 
 (6.7.17)
 
Ag( f ) 5 2 maxhAd ( f ), sj sinfd ( f )g 
 (6.7.18)
0
0.1
0.2
0.3
0.4
0.5
20.5
0
0.5
1
1.5
2
f/fs
A(f)
(f)
Ideal
Quadrature Filter
0
0.1
0.2
0.3
0.4
0.5
24
22
0
2
4
f/fs
Ideal
Quadrature Filter
0.05
0.15
0.25
0.35
0.45
0.05
0.15
0.25
0.35
0.45
Figure 6.28: Magnitude and Residual Phase Responses of the Two-Stage Quadra-
ture Filter from Example 6.13 with m 5 150, a Blackman Window, and s 5 .01 
Frequency-selective Filter
EXAMPLE 6.13
To illustrate the application of the quadrature method to an idealized frequency- 
selective filter, consider the following highpass filter where a is the unit step.
A( f ) 5 a(fT 2 .25)
As a desired residual phase response, consider the following squared sinusoid.
( f ) 5  sinf6(f T )2g
Suppose the linear-phase filters are of order m 5 150 with a Blackman window. 
Let the stopband attenuation factor be s 5 .01 or 2 40 dB. Plots of the resulting 
magnitude response and residual phase response, obtained by running exam6_12, 
are shown in Figure 6.28. Again, the fit is not exact, but it is a reasonable fit of 
both the desired magnitude response and the desired residual phase response, 
including over the stopband.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

466    Chapter 6  FIR Filter Design
An alternative way to design a filter with both a prescribed magnitude response and a 
prescribed residual phase response is to generalize the least-squares design method in Sec-
tion 6.4. In particular, the integrand of the objective function J in (6.4.1) can be generalized 
by including terms for the error in both the real part and the imaginary part of the fre-
quency response. This technique is implemented in the DSP Companion function f_  firls2.
6.7.3 Equalizer Design
A particularly nice application of quadrature filters arises in the design of equalizer fil-
ters. Recall from Proposition 5.3 that every rational transfer function H(z) can be decom-
posed into the product of an allpass part, Hall(z), and a minimum-phase part, Hmin(z). 
In Section 5.4.3 it was shown that the inverse of the minimum-phase part serves as a 
magnitude equalizer for the original system.
 
Hequal(z) 5 H21
min(z)H(z) 
 (6.7.19)
From Proposition 5.3, Hequal(z) 5 Hall(z), which means that Aequal( f ) 5 1. That is, post 
processing by the inverse of the minimum-phase part of H(z) eliminates distortion in the 
magnitude response, but not the phase response.
optimal Delay
A more complete equalizer is one that eliminates both magnitude distortion and phase 
distortion. To achieve this more complete form of equalization, suppose the original sys-
tem has a magnitude response of Ad( f ) and a phase response of d( f ).
 
H( f ) 5 Ad ( f ) expf jd ( f )g 
 (6.7.20)
First we decompose the total phase response d( f ) into a linear-phase part, 22fd, 
and a residual phase part, d( f ). To find an optimal value for the linear-phase delay d, 
consider N 1 1 values of f  equally spaced over the Nyquist range.
 
fi 5 ifs
2N, 0 # i # N 
 (6.7.21)
Let f [ RN11 be a column vector whose ith element is fi, and let  [ RN11 be the column 
vector of corresponding values of the phase.
 
i 5 d(fi), 0 # i # N 
 (6.7.22)
Suppose a delay  is chosen to minimize a sum of the squares of the difference between 
d( f ) and 22f .
 
E() 5 o
N
i50
(i 1 2fi)2 
 (6.7.23)
Setting dE()yd 5 0 and solving for  yields the following least-squares value for the 
delay  where the sums are expressed as dot products of the column vectors f  and .
 
 51
21
22
Tf
fTf
 
 (6.7.24)
If ( f ) decreases with f , which is typical of a many physical systems, then Tf , 0 
which means that the optimal delay  will be positive. The  in (6.7.24) ensures that the  
linear-phase term, 22f, accounts for as much of the total phase d( f ) as possible in 
the sense of minimizing E(). The value of  in general will not be an integer multiple of 
T. Let d be the nearest integer multiple of the sampling interval T. That is,
Generalized 
least-squares 
filters  
Magnitude 
equalization
Optimal delay
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.7 Quadrature Filter Design    467
 
M 5 round 1

T2 
 (6.7.25)
 
d 5 MT 
 (6.7.26)
The residual phase d( f ) is then the phase remaining after the effects of a delay of d have 
been removed.
 
d( f ) 5 d( f ) 1 2fd 
 (6.7.27)
The optimal delay term extracted from d( f ) to produce a residual phase term d( f ) 
is useful for quadrature filter design in general, because it tends to minimize the magni-
tude of the residual phase term. When ud( f )u # , there will be no jump discontinuities 
in d( f ), and this makes d( f ) easier to approximate with the mth order stage two filters, 
F(z) and G(z).
Equalizer Design
To design an equalizer using a quadrature filter, it is necessary to choose a desired magni-
tude response AD( f ) and residual phase response D( f ). Suppose the original magnitude 
response Ad( f ) is positive. To invert Ad( f ) and d( f ) we use
 
AD( f ) 5
1
Ad( f ) 
 (6.7.28)
 
D( f ) 5 2d( f ) 
 (6.7.29)
This should produce in a quadrature filter Hq(z) that can be used to equalize the original 
system H(z).
 
Hequal(z) 5 Hq(z)H(z) 
 (6.7.30)
To the extent that a quadrature filter of order 2m does achieve a magnitude response that 
matches AD( f ) and a residual phase response that matches D( f ), the equalized system 
will be an allpass system with unit magnitude response, zero residual phase response, and 
a group delay of q 5 (M 1 m)T. Hence if x(k) is the input to the equalized system, the 
steady-state equalized output will be
 
y(k) < xfk 2 (M 1 m)g 
 (6.7.31)
In this way, the amplitude and phase distortion created by passing x(k) through H(z) can 
be cancelled. The original signal x(k) is restored, but with a delay of M 1 m samples.
Equalized system
Equalized output
Equalizer Filter
EXAMPLE 6.14
As an illustration of the use of a quadrature filter to design an equalizer or mag-
nitude and phase compensator, consider a system with the following magnitude 
and phase responses.
Ad( f ) 5
1.1
.1 1 ( fT 2 .2)2y.09
d( f ) 5 220f T 1  sin2(2kT)
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

468    Chapter 6  FIR Filter Design
0
10
15
25
35
45
5
15
25
35
45
5
15
25
35
45
5
20
30
40
50
0
1
2
(a)
Ad(f)
0
10
20
30
40
50
0
5
10
(b)
Aq(f)
0
10
20
30
40
50
0.5
1
1.5
(c)
f/fs
Aequal(f)
Figure 6.29: Magnitude Responses of (a) System H(z) in Example 6.14,  
(b) Equalizer Filter Hq(z), and (c) Equalized System Hequal(z) 5 Hq(z)H(z) 
Suppose the sampling frequency is fs 5 100 Hz. Using (6.7.24) with N 5 300, 
the optimal least-squares delay associated with the phase response is
 5 .0925
From (6.7.25), a delay of M 5 9 samples is used to compute the desired resid-
ual phase response d( f ) in (6.7.27). Suppose a quadrature filter of order 2m 
with m 5 100 and a Hamming window is used. Running exam6_14 results 
in the three magnitude response plots shown in Figure 6.29. Here (a) shows 
the original magnitude response Ad( f ), (b) shows the magnitude response 
Aq( f ) of the equalizer, and (c) shows the equalized magnitude response, 
which is close to unity except near the ends of the Nyquist range where 
the Hilbert transformer approximation is less accurate. The three residual 
phase responses are shown in Figure 6.30. Again (a) shows the original 
residual phase response d( f ), (b) shows the residual phase response q( f ) 
of the equalizer, and (c) shows the equalized residual phase response which 
is close to zero throughout, indicating effective cancellation of the phase  
distortion.
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.7 Quadrature Filter Design    469
0
10
15
5
20
25
35
45
30
40
50
25
0
5
(a) M 5 9
d(f)
0
10
20
30
40
50
25
0
5
(b)
q(f)
0
10
20
30
40
50
22
0
2
(c)
f/fs
equal(f)
15
5
25
35
45
15
5
25
35
45
Figure 6.30: Residual Phase Responses of (a) System H(z) in Example 6.14, 
(b) Equalizer Filter Hq(z), and (c) Equalized System Hequal(z) 5 Hq(z)H(z) 
The DSP Companion contains the following function for designing a quadrature filter.
% F_FIRQUAD: Design a general two-stage quadrature filter 
%
% Usage: 
%        b = f_firquad (fsys,m,fs,win,deltas,q)
% Pre: 
%        fsys,  = handle of function that specifies the desired
%                  magnitude response and residual phase response:
%
%                 [Ar,theta] = fsys(f,fs,q);
%
%                 Here f is the frequency, fs is the sampling 
%                 frequency, and q is an optional parameter 
%                 vector containing things like cutoff 
%                 frequencies,etc. Output Ar is a the desired
%                 desired magnitude response and output theta
%                 is the desired residual phase response at f. 
DSP Companion
DsP Companion
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

470    Chapter 6  FIR Filter Design
Filter Realization structures
In this section we investigate alternative configurations that can be used to realize or imple-
ment FIR filters with signal flow graphs. These filter realization structures differ from one 
another with respect to memory requirements, computational time, and sensitivity to finite 
word length effects. An mth-order FIR filter has the following transfer function.
 
H(z) 5 b0 1 b1z21 1 Á 1 bmz2m 
 (6.8.1)
6.8.1 Direct Forms
Tapped Delay Line
By taking the inverse Z-transform of Y(z) 5 H(z)X(z) using (6.8.1) and the delay prop-
erty, this results in the following time-domain representation of an mth order FIR filter.
 
y(k) 5 o
m
i50
bix(k 2 i) 
 (6.8.2)
The representation in (6.8.2) is a direct representation because the coefficients of the dif-
ference equations are obtained directly from inspection of the transfer function. A signal 
flow graph of a direct form realization, for the case m 5 3, is shown in Figure 6.31. This 
structure is called a tapped delay line.
Transposed Tapped Delay Line
Every signal flow graph satisfies something called the flow graph reversal theorem.
6.8
%                 Since there is a group delay of tau = mT where 
%                 T = 1/fs, the total phase response will be:
%
%                 phi(f) = theta(f) - 2*pi*f*m*T
%
%        m      = order each subfilter (must be even)
%        fs     = sampling frequency
%        win    = the window type to be used:
%
%                 0 = rectangular
%                 1 = Hanning
%                 2 = Hamming
%                 3 = Blackman
%
%        deltas = stopband attenuation factor (0 < deltas << 1)
%        q      = an optional vector of length contained 
%                 design parameters to be passed to fun.  
%                 For example q might contain cutoff 
%                  frequencies or gains.
% Post: 
%        b = 1 by 2m+1 array of filter coefficients.  
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.8 Filter Realization Structures    471
If each arc of a signal flow graph is reversed and the input and output labels are 
interchanged, then the resulting signal flow graph is equivalent.
Applying the flow graph reversal theorem to the direct form graph in Figure 6.31 and 
drawing the final result with the input on the left rather than the right, we arrive at the 
FIR filter realization shown in Figure 6.32, which is called a transposed tapped delay line 
filter realization.
Linear-phase Form
The tapped delay line realizations in Figures 6.31 and 6.32 are general realizations that 
are valid for any FIR filter. However, many FIR filters are designed to be linear-phase 
FIR filters. Recall that bi 5 h(i) for an FIR filter. If the linear-phase symmetry constraint 
in Proposition 5.2 is recast in terms of the FIR filter coefficients, this yields
 
bk 5 6bm2k, 0 # k # m 
 (6.8.3)
By exploiting the symmetry constraint, we can develop a filter realization that requires 
only about half as many floating-point multiplications (FLOPs). To illustrate the method, 
consider the most general linear-phase filter, the type 1 filter with even symmetry and 
even order. Letting p 5 my2, it is possible to rewrite (6.8.2) as
 
y(k) 5 bpx(k 2 p) 1 o
p21
i50
bifx(k 2 i) 1 x(k 2 m 1 i)g  
 (6.8.4)
Thus the number of multiplications has been reduced from m to my2 1 1. Similar expres-
sions for y(k) can be developed for the other three types of linear-phase FIR filters (see 
Problem 6.20). A signal flow graph realization of a type 1 linear-phase FIR filter is shown 
in Figure 6.33 for the case m 5 6.
Observe from Figure 6.33 that there are my2 1 1 floating-point multiplications, one 
for each distinct coefficient. However, the number of delay elements is m, so there is 
no reduction in the memory requirements. A comparison of the direct form FIR filter 
realizations is summarized in Table 6.5, where it can be seen that the three realizations 
are identical in terms of storage requirements. In each case the number of floating-point 
PRoPosITIoN
6.2 Flow graph Reversal 
Theorem 
Transposed tapped  
delay line
Linear-phase filter
Figure 6.31:  
Tapped-delay-line 
Realization of FIR 
Filter, m 5 3
z21
x
b1
b2
b3
b0
z21
z21
y
Figure 6.32:  
Transposed 
Tapped-delay-line 
Realization of FIR 
Filter, m 5 3
x
b2
b1
b0
b3
z21
z21
z21
y
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

472    Chapter 6  FIR Filter Design
operations or FLOPs grows linearly with the order of the filter. For large values of m, the 
linear-phase form has approximately half as many multiplications, but about one and a 
half times as many additions.
6.8.2 Cascade Form
The direct forms have the virtue that they can be obtained directly from inspection of the 
transfer function. However, the direct forms also suffer from a practical drawback. As 
the order of the filter, m, increases, the direct form filters become increasingly sensitive to 
the finite word length effects that are discussed in Section 6.9. For example, the roots of 
a polynomial can be very sensitive to small changes in the coefficients of the polynomial, 
particularly as the degree of the polynomial increases. To develop a realization that is less 
sensitive to the effects of finite precision, it is helpful to first recast the transfer function 
in (6.8.1) in terms of positive powers of z. If the numerator is then factored, this yields 
the factored form
 
H(z) 5 b0(z 2 z1)(z 2 z2) Á (z 2 zm)
zm
 
 (6.8.5)
Since H(z) has m poles at z 5 0, an FIR filter is always stable. The coefficients of H(z) 
are assumed to be real, so complex zeros occur in conjugate pairs. The representation 
in (6.8.5) can be recast as a product of M second-order subsystems as follows where 
M 5 floorf(m 1 1)y2g.
 
H(z) 5 b0H1(z) Á HM(z)  
 (6.8.6)
Figure 6.33:  
Direct-form 
Realization of a 
Type 1 Linear-
phase FIR Filter, 
m 5 6
z21
b2
b1
b0
b3
z21
z21
z21
z21
z21
x
y
 Direct Form 
storage Elements 
Additions 
Multiplications
 Tapped delay line  
m 
m 1 1  
m 1 1 
 Transposed tapped delay line  
m 
m 1 1  
m 1 1 
 Linear phase  
m 
3ny2 1 1  
my2 1 1
Table 6.5:  
Comparison of Direct-
form Realizations of 
FIR Filter of Order m 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.8 Filter Realization Structures    473
This is called a cascade-form realization and a block diagram for the case M 5 2 is shown 
in Figure 6.34. Second-order block Hi(z) is constructed from two poles at z 5 0 and either 
two real zeros or a complex conjugate pair of zeros. This way, the coefficients of Hi(z) are 
guaranteed to be real.
 
Hi(z) 5 1 1 bi1z21 1 bi2z22, 1 # i # M 
 (6.8.7)
If Hi(z) is constructed from zeros zi and zj, then the coefficients can be computed 
using sums and products as follows for 1 # i # M.
 
bi1 5 2(zi 1 zj) 
 (6.8.8a)
 
bi2 5 zizj 
 (6.8.8b)
Let ui denote the output of the ith second-order block. Then from (6.8.6) and (6.8.7), a 
cascade-form realization is characterized by the following time domain equations.
 
u0(k) 5 b0x(k) 
 (6.8.9a)
 
ui(k) 5 ui21(k) 1 bi1ui21(k 2 1) 1 bi2ui21(k 2 2), 1 # i # M 
 (6.8.9b)
 
y(k) 5 uM(k) 
 (6.8.9c)
If m is even, there will be M subsystems, each of order two, and if m is odd, there will 
be M 2 1 second-order subsystems plus one first-order subsystem. The coefficients of a 
first-order subsystem are obtained from (6.8.8) by setting zj 5 0.
Either of the tapped-delay-line forms can be used to realize the second-order blocks 
in (6.8.7). A block diagram of the overall structure of a cascade-form realization, for the 
case M 5 3, is shown in Figure 6.35. Since the cascade-form coefficients must be com-
puted using (6.8.8), rather than obtained directly from inspection of H(z), the cascade-
form realization is example of an indirect form.
Cascade-form
Figure 6.34:  
Cascade-form 
Block Diagram, 
M 5 2
b0
H1(z)
H2(z)
u1(k)
u0(k)
y(k)
x(k)
Figure 6.35: Cascade-form Realization of FIR Filter, M 5 3
x
z21
z21
z21
z21
z21
z21
b11
b12
b21
b22
b31
b32
b0
u1
u0
u2
u3
y
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

474    Chapter 6  FIR Filter Design
6.8.3 Lattice Form
Another indirect form that finds applications in speech processing and adaptive systems 
is the lattice-form realization shown in Figure 6.37 for the case m 5 2. The time domain 
equations for a lattice-form realization of order m are expressed in terms of the interme-
diate variables ui and vi. From Figure 6.37, we have
 
u0(k) 5 b0x(k) 
 (6.8.10a)
 
v0(k) 5 u0(k) 
 (6.8.10b)
Figure 6.36: Cascade-form Realization of Filter in Example 6.15 
x
z21
z21
z21
z21
z21
2.3
2.8
.9
.41
2.18
3
u1
u0
u2
u3
y
FIR Cascade Form
EXAMPLE 6.15
As an illustration of a cascade-form realization of an FIR filter, consider the fol-
lowing fifth-order transfer function.
H(z) 5 3(z 2 .6)(z 1 .3)f(z 2 .4)2 1 .25g(z 1 .9)
z5
Inspection of H(z) reveals that the zeros are
z1 5 .6
z2 5 2.3
z3,4 5 .4 6 j.5
z5 5 2.9
Suppose H1(z) is a block associated with the real zeros, z 5 .6 and z 5 2.3, H2(z) 
is associated with the complex conjugate pair of zeros, and H3(z) is a first-order 
block associated with the zero, z 5 2.9. Running exam6_14, we get the following 
subsystems.
b0 5 3
H1(z) 5 1 2 .3z21 2 .18z22
H2(z) 5 1 2 .8z21 1 .41z22
H3(z) 5 1 1 .9z21
A signal flow graph of the resulting cascade-form realization of the fifth-order 
FIR filter is as shown in Figure 6.36.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.8 Filter Realization Structures    475
 
ui(k) 5 ui21(k) 1 Kivi21(k 2 1), 1 # i # m 
 (6.8.10c)
 
vi(k) 5 Kiui21(k) 1 vi21(k 2 1), 1 # i # m  
 (6.8.10d)
 
y(k) 5 um(k)
 
 (6.8.10e)
Thus an mth order lattice-form realization has m stages. Coefficient Ki of the ith stage 
is called a reflection coefficient. To determine the vector of m reflection coefficients, it is 
useful to introduce the following operation that is applicable to an mth order FIR filter.
 
z2mH(z21) 5 o
m
i50
bm 2 iz2i 
 (6.8.11)
Comparing (6.8.11) with (6.8.1), we see that z2mH(z21) is simply the polynomial obtained 
by reversing the coefficients of H(z). The following algorithm can be used to compute the 
reflection coefficients.
1.  Factor H(z) into H(z) 5 b0Am(z) and compute
Bm(z) 5 z2mAm(z21)
Km 5 lim
zS`Bm(z)
2.  For i 5 m down to 2 compute
     { 
Ai21(z) 5 Ai(z) 2 KiBi(z)
1 2 K2
i
Bi21(z) 5 z2(i 2 1)Ai21(z21)
Ki21 5 lim
zS`  Bi21(z)
     }
Algorithm 6.4 produces b0 and an m 3 1 vector of reflection coefficients, K, as long as 
uKiu Þ 1 for 1 # i # m. If uKiu 5 1, then Ai 2 i(z) has a zero on the unit circle. In this case, 
this zero can be factored out and the algorithm applied to the reduced-order polynomial.
Reflection coefficient
ALgoRITHM
6.4 Lattice-form 
Realization
Figure 6.37:  
Lattice-form 
Realization of FIR 
Filter, M 5 2
x
z21
z21
u1
1
0
b0
2
K1
K1
K2
K2
u0
u2
y
FIR Lattice Form
EXAMPLE 6.16
As an illustration of a lattice-form realization of an FIR filter, consider the fol-
lowing second-order transfer function.
H(z) 5 2 1 6z21 2 4z22
(Continued  )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

476    Chapter 6  FIR Filter Design
There are additional indirect forms that have been proposed and are used. Included 
among these are the frequency-sampling realization and the parallel form realizations 
(see, e.g., Proakis and Manolakis, 1992).
Applying step 1 of Algorithm 6.4 yields b0 5 2 and
A2(z) 5 1 1 3z21 2 2z22
B2(z) 5 22 1 3z21 1 z22
K2 5 22
Next, applying step 2 with i 5 2
A1(z) 5 1 1 3z21 2 2z22 1 2( 2 2 1 3z21 1 z22)
1 2 4
5 23 1 9z21
23
5 1 2 3z21
B1(z) 5 23 1 z21
K1 5 23
Thus b0 5 2, and the reflection coefficient vector is K 5 f 2 3,22gT. A signal 
flow graph of the lattice-form realization is shown in Figure 6.38.
Figure 6.38: Lattice-form Realization of FIR Filter in Example 6.16
x
z21
z21
u1
1
0
2
2
23
23
22
22
u0
u2
y
The DSP Companion contains the following functions for computing indirect form 
realizations of a FIR transfer functions.
% F_CASCADE: Find cascade-form digital filter realization
% F_LATTICE: Find lattice-form FIR filter realization 
%
% Usage:
%        [B,A,b_0] = f_cascade (b)
%        [K,b_0]   = f_lattice (b)
% Pre: 
%        b = vector of length m+1 containing coefficients 
%            of numerator polynomial.
DSP Companion
DsP Companion
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 6.9 Finite Word Length Effects     477
% Post: 
%        B   = N by 3 matrix containing coefficients of 
%              numerators of second-order blocks.
%        A   = N by 3 matrix containing coefficients of 
%              denominators of second-order blocks.
%        b_0 = numerator gain
%        K   = 1 by m vector containing reflection 
%              coefficients
% Notes: 
%        1. It is required that b(1)<>0.  Otherwise factor 
%           out a z^-1 and then find the cascade form
%        2. This algorithm assumes that $|K(i)| ~= 1$ 
%           for 1 <= i <= m.
To evaluate the cascade- and lattice-form filters, the outputs from the calls to f_cascade 
and f_lattice are used as inputs to the following filter evaluation functions.
% F_FILTCAS: Compute output of cascade-form filter realization
% F_FILTLAT: Compute output of lattice-form filter realization
%
% Usage: 
%        y = f_filtcas (B,A,b_0,x)
%        y = f_filtlat (K,b_0,x)
% Pre: 
%        B   = N by 2 matrix containing numerator 
%              coefficients of second-order blocks. 
%        A   = N by 3 matrix containing denominator 
%              coefficients of second-order blocks.
%        b_0 = numerator gain factor
%        x   = vector of length p containing samples of
%              input signal. 
%        K   = 1 by m vector containing reflection 
%              coefficients
% Post:: 
%        y = vector of length p containing samples of 
%            output signal assuming zero initial 
%            conditions.
Finite Word Length Effects 
When a filter is implemented, finite precision must be used to represent the values of the 
signals and the filter coefficients. The resulting reductions in filter performance caused by 
going from infinite precision to finite precision are called finite word length effects. If a 
filter is implemented in software using MATLAB, then double-precision floating-point 
arithmetic is used. This corresponds to 64 bits or a precision of about 16 significant 
decimal digits. In most instances, this is a sufficiently good approximation to infinite-
precision arithmetic that no significant finite word length effects are apparent. However,  
if a filter is implemented on specialized DSP hardware (Kuo and Gan, 2005) or if storage 
or speed requirements dictate the need to use single-precision floating-point arithmetic 
or fixed-point arithmetic, then finite word length effects begin to manifest themselves.
 6.9
Optional material
*
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

478    Chapter 6  FIR Filter Design
6.9.1 Binary Number Representation
Finite word length effects depend on the method used to represent numbers. If a filter is 
implemented in software on a PC, then typically a binary floating-point representation 
of numbers is used where some of the bits are used to represent the mantissa, or frac-
tional part, and the remaining bits are reserved to represent the exponent. For example, 
MATLAB uses 64 bits with N 5 53 bits reserved for the mantissa and 11 bits for the 
exponent. Floating-point representations have the advantage that they can represent very 
large and very small numbers, hence issues of overflow and scaling typically do not come 
into play. However, the spacing between adjacent floating-point values is not constant; 
instead it is proportional to the magnitude of the number.
An alternative to the floating-point representation is the fixed-point representation 
that does not have a field of bits reserved to represent the exponent. Both floating-point 
and fixed-point representations can be found on specialized DSP hardware. The fixed-
point representation is faster and more efficient than the floating-point representation. 
Furthermore, the precision, or spacing between adjacent values, is uniform throughout 
the range of numerical values represented. Unfortunately, this range typically is much 
smaller than for floating-point numbers, so one has to be concerned with scaling and 
the possibility of overflow. Although finite word length effects appear in both floating- 
point and fixed-point arithmetic, they are less severe for floating-point representations, 
particularly double-precision representations such as those used with MATLAB and 
C++.
In this section we will focus our attention on the use of fixed-point numerical repre-
sentations. An N-bit binary fixed-point number b, consisting of N binary digits, or bits, 
can be expressed as follows.
 
b 5 b0 b1 Á bN21, 0 # bi # 1 
 (6.9.1)
Typically the number b is normalized with the binary point appearing between bits b0 and 
b1. In this case, the decimal equivalent of a positive number is
 
x 5 o
N21
i51
bi22i 
 (6.9.2)
Bit b0 is reserved to hold the sign of x, with b0 5 0 indicating a positive number 
and b0 5 1 a negative number. There are several schemes for encoding negative num-
bers, including sign magnitude, one’s complement, offset binary, and two’s complement 
(Gerald and Wheatley, 1989). The most popular method is the two’s complement rep-
resentation. The two’s complement representation of a negative number is obtained by 
complementing each bit and then adding one to the least significant bit. Any carry past 
the sign bit is ignored. One of the virtues of two’s complement arithmetic occurs when 
several numbers are added. If the sum fits within N bits, then the total will be correct even 
if the intermediate results or partial sums overflow!
The range of values that can be represented in (6.9.2) is 21 # x , 1. Of course many 
values of interest may fall outside this normalized range. Larger values can be accommo-
dated by using a fixed scale factor c. Notice from (6.9.2) that scaling by c 5 2M effectively 
moves the binary point M places to the right. This increases the range to 2c # x , c, but 
causes a loss of precision. The precision, or spacing between adjacent values, is constant 
and is called the quantization level.
 
q 5
c
2N21  
 (6.9.3)
Bits
Quantization level
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 6.9 Finite Word Length Effects     479
When a scale factor of c 5 2M is used, this corresponds to reserving M 1 1 bits for the 
integer part, including the sign, and the remaining L 5 N 2 (M 1 1) bits for the frac-
tional part as shown in Figure 6.39.
6.9.2 Input Quantization Error
Recall from Chapter 1 that the value of the input signal x(k) is quantized to a finite num-
ber of bits as a result of analog-to-digital (ADC) conversion. To model quantization, it 
is helpful to introduce the following operator.
Let N be the number of bits used to represent a real value x. The quantized version of 
x is denoted QN(x) and defined as
QN(x) 5
D  
q floor1
x 1 qy2
q 2
The form of quantization in Definition 6.2 uses rounding, which can be seen from 
the qy2 term in the numerator. If this term is removed, then the resulting quantization 
operation uses truncation. For convenience, we will assume that quantization by round-
ing to N bits is used. A graph of the nonlinear input-output characteristic of the quanti-
zation operator for the case N 5 4 is shown in Figure 6.40.
If an ADC has a precision of N bits, then the quantized output from the ADC, 
denoted xq(k), is computed as follows.
 
xq(k) 5 QNfx(k)g 
 (6.9.4)
DEFINITIoN
6.2 Quantization operator
Figure 6.39:  
Fixed-point 
Representation 
of N-bit Number 
Using Scale Factor 
c 5 2M  
Integer part
M 1 1
N 2 (M 1 1)
Fraction part
21
20.620.420.2
20.8
0.4
0.6
0.8
0.2
0
1
21
20.8
20.6
20.4
20.2
0
0.2
0.4
0.6
0.8
1
x
QN(x)
q 5 0.125
Figure 6.40:  
Input-output 
Characteristic 
of Quantization 
Operator for N 5 4 
Using Rounding 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

480    Chapter 6  FIR Filter Design
Although the deterministic nonlinear model in (6.9.4) is accurate, it is more useful 
for analysis purposes to replace it by an equivalent linear statistical model. The quantized 
signal can be regarded as an infinite-precision signal x(k) plus a quantization error term, 
Dx(k), as shown in Figure 6.41. Thus the quantized signal is
 
xq(k) 5 x(k) 1 Dx(k) 
 (6.9.5)
Recall from Chapter 1 that if uxa(t)u # c, then uDx(k)u # qy2 where q in (6.9.3) is the ADC 
quantization level. Consequently, when rounding is used the quantization error can be 
modeled as white noise that is uniformly distributed over the interval f2qy2, qy2g. The 
probability density of ADC quantization noise is as shown in Figure 6.42.
A convenient measure of the size of the quantization noise is the average power. For 
zero-mean noise, the average power is the variance 2
x 5 EfDx2g. Using Definition 4.3 and 
the probability density in Figure 6.42, the average power of the ADC quantization noise is
2
x 5#
`
2`
x2p(x)dx
5 1
q#
qy2
2qy2
x2dx
 
5 q2
12 
 (6.9.6)
Quantization error
x
p(x)
q/2
2q/2
1/q
Figure 6.42:  
Probability Density 
of ADC Quantiza-
tion Noise Using 
Rounding 
Figure 6.41:  
Linear Statistical 
Model of Input 
Quantization 
Dx(k)
xq(k)
xa(t)
x(k)
T
1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 6.9 Finite Word Length Effects     481
The quantization noise associated with the input signal is filtered by the system H(z) 
and appears in the system output as
 
yq(k) 5 y(k) 1 Dy(k) 
 (6.9.7)
To determine the average power of the output noise, first note that for a linear system 
the response to the input noise Dx(k) is the output noise Dy(k). That is, if h(k) is the impulse 
response of the system, then subtracting the noise-free output from the complete out-
put yields
 
Dy(k) 5 o
m
i50
h(i)Dx(k 2 i) 
 (6.9.8)
The input quantization noise Dx(k) is zero-mean white noise. Consequently, the average 
power of the output noise is
E fDy2(k)g 5 E 3o
m
i50
h(i)Dx(k 2 i)o
m
p50
h(p)Dx(k 2 p)4
 
5 o
m
i50o
m
p50
h(i)h(p)EfDx(k 2 i)Dx(k 2 p)g
 
5 o
m
i50
h2(i)EfDx2(k 2 i)g
 
53o
m
i50
h2(i)4E fDx2(k)] 
 (6.9.9)
Thus the average power of the output noise is proportional to the average power of the 
input quantization noise as follows.
 
2
y 5 G2
x 
 (6.9.10)
The constant of proportionality G is called the power gain. From (6.9.9), the power gain 
is computed from the impulse response h(k) as follows.
 
G 5
D
 o
m
k50
h2(k)  
 (6.9.11)
Power gain
Input Quantization Noise
EXAMPLE 6.17
As an illustration of ADC quantization effects, suppose an ADC with a precision 
of N 5 8 bits is used to sample an input signal with range uxa(t)u # 10. From 
(6.9.3), the ADC quantization level is
q 5 10
27
5 .0781
It then follows from (6.9.6) that the average power of the ADC quantization noise 
at the input is
2
x 5 .07812
12
5 5.0863 3 1024
(Continued  )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

482    Chapter 6  FIR Filter Design
6.9.3 Coefficient Quantization Error
FIR filter coefficients are also quantized when they are stored in fixed-length memory 
locations. Let the unquantized or infinite-precision version of the transfer function be
 
H(z) 5 o
m
i50
biz2i 
 (6.9.12)
Suppose the elements of the coefficient vector are quantized to N bits to yield bq 5 QN(b), 
where QN is the N-bit quantization operator introduced in Definition 6.2. The quantized 
coefficient vector bq can be expressed as follows.
 
bq 5 b 1 Db 
 (6.9.13)
Here Db is the coefficient quantization error. If ubiu # c, then the elements of the vector Db 
can be modeled as random numbers uniformly distributed over f2qy2, qy2g, where q is 
the quantization level given in (6.9.3). For an FIR filter the effects of coefficient quanti-
zation are relatively easy to model. Let Hq(z) denote the transfer function using quantized 
coefficients. From (6.9.12) and (6.9.13)
Hq(z) 5 o
m
i50
(bi 1 Dbi)z2i
5 o
m
i50
biz2i 1 o
m
i50
Dbiz2i
 
5 H(z) 1 DH(z) 
 (6.9.14)
Coefficient  
quantization error
Next, suppose the quantization noise is passed through the following FIR filter.
H(z) 5 10o
30
i50
.9iz2i
The impulse response of this FIR filter is
h(k) 5 10(.9)kf(k) 2 (k 2 31)g
Using the geometric series and (6.9.11), the power gain of the filter is
G 5 o
30
k50
f10(.9)k g
 2
5 100 3o
`
k50
.81k 2 o
`
k531
.81k4
5 100 3
1
1 2 .81 2
.8131
1 2 .814
5 525.37
Finally, using (6.9.10) the average power of the ADC quantization noise appear-
ing at the filter output is
 2
y 5 G2
x
5 526.37(5.0863 3 1024)
5 .2672
Observe that even though the noise power at the input is quite small, the noise 
power at the output can be large.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 6.9 Finite Word Length Effects     483
Here the subsystem DH(z) is the coefficient quantization error transfer function
 
DH(z) 5 o
m
i50
Dbiz2i 
 (6.9.15)
From (6.9.14) it is clear that quantization of the FIR coefficients is equivalent to introduc-
ing the error system DH(z) in parallel with the unquantized system as shown in Figure 6.43
One can place a simple upper bound on the effects of coefficient quantization on the 
frequency response H( f ). Using uDbiu # qy2, the error in the system magnitude response 
can be bounded as follows.
DA( f ) 5 uo
m
i50
Dbi exp(2ji2f T )u
# o
m
i50
uDbi exp(2ji2f T )u
5 o
m
i50
uDbiu
 
# (m 1 1)q
2
 
 (6.9.16)
It then follows from (6.9.3) that for an mth order FIR filter with coefficients ubiu # c that are 
quantized to N bits, the error in the magnitude of the frequency response satisfies
 
DA( f ) # (m 1 1)c
2N
 
 (6.9.17)
The upper bound in (6.9.17) is a conservative one that assumes a worst case in which each 
of the coefficient quantization errors has the same sign and is the maximum value possible.
Magnitude error
Figure 6.43:  
The Effects of 
Coefficient Quan-
tization on an FIR 
Filter H(z)
x
y
H(z)
DH(z)
1
FIR Coefficient Quantization Error
EXAMPLE 6.18
As an illustration of FIR coefficient quantization noise, consider a bandpass fil-
ter of order m 5 64 designed using the least-squares method. Suppose the coef-
ficients are quantized using N 5 8 bits. Evaluation of the (m 1 1) coefficients 
reveals that they lie in the range ubiu # c where c 5 1. Using (6.9.17), the error in 
the magnitude response is bounded as follows.
DA( f ) # 64 1 1
28
5 .2539
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

484    Chapter 6  FIR Filter Design
Unit Circle Zeros
Another way to evaluate the effects of coefficient quantization is to look at the loca-
tions of the zeros. The roots of a polynomial can be very sensitive to small changes 
in the coefficients of the polynomial, particularly for higher-degree polynomials. As a 
consequence, higher-order direct form realizations of H(z) can be sensitive to coeffi-
cient quantization error. For FIR filters the most important case corresponds to zeros 
on the unit circle. Zeros on the unit circle are qualitatively important because they 
produce complete attenuation of the input signal at specific frequencies. For example, 
suppose it is desired to remove the frequency F0. This is achieved by placing zeros at 
When exam6_18 is run, it produces the two magnitude response plots shown 
in Figure 6.44. The first plot approximates the unquantized case using double- 
precision floating-point arithmetic. The second plot uses a tapped delay line 
direct form realization with quantized coefficients. It is apparent from inspection 
that quantizing to N 5 8 bits introduces error, particularly in the stopband. The 
signal attenuation for the quantized case is not nearly as good as it is for the 
unquantized case.
A(f) (dB)
A(f) (dB)
0
0.5
2100
250
0
(a)
0
0.5
2100
250
0
(b)
f/fs
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
f/fs
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
Figure 6.44: Magnitude Responses of Least-squares Bandpass Filter of Order 
m 5 64 for (a) Unquantized Coefficients and (b) Coefficients Quantized to 
N 5 8 Bits 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 6.9 Finite Word Length Effects     485
z 5 6 exp( j0) where 0 5 2F0T. A second-order FIR filter that completely attenuates 
frequency F0 is then
H(z) 5 fz 2  exp(j0)gfz 2  exp( 2 j0)g
z2
 
5 z2 2 f exp(  j0) 1  exp(2j0)gz 1 1
z2
 
5 z2 2 2 cos(0)z 1 1
z2
 
5 1 2 2 cos(0)z21 1 z22 
 (6.9.18)
Note that H(z) has a coefficient vector of b 5 f1, 22 cos(0), 1gT. If b is quantized, the 
small change in b1 will effectively change 0, but there will be no change in b0 or b2 because 
they can be represented exactly. Consequently, the quantized zero will remain on the unit 
circle, although the angle (i.e. the frequency) will change. It follows that if H(z) is realized 
using a cascade of quantized second-order blocks, then zeros on the unit circle will be  
preserved although their frequencies will change.
Linear-phase Block
Most FIR filters are linear-phase filters, so it is useful to examine what effect coefficient 
quantization has on this important property. Recall that the most general type 1 linear- 
phase FIR filter of order m can be realized with the direct form shown in Figure 6.33.  
From (6.8.4), the difference equation of this direct form linear-phase realization is as 
follows where p 5 my2.
 
y(k) 5 bpx(k 2 p) 1 o
p21
i50
bifx(k 2 i) 1 x(k 2 m 1 i)g 
 (6.9.19)
To preserve the linear phase response of a type 1 filter, it is necessary that bi 5 bm 2 i for 
0 # i # m. But from the structure in (6.9.19) it is apparent that bi and bm 2 i are imple-
mented with the same coefficient. Consequently, if the p 1 1 coefficients in (6.9.19) are 
quantized, the resulting quantized filter will still be a linear-phase filter; only its magni-
tude response will be affected. That is, the linear-phase feature of the direct form realiza-
tion in (6.9.19) is unaffected by coefficient quantization.
As it turns out, a linear phase response can be preserved even when a cascade-form 
realization is used. Recall that linear-phase filters have the property that if z 5 r exp( j) 
is a zero with r Þ 1, then so is its reciprocal, z 5 r21 exp(2j). For a filter with real coef-
ficients, complex zeros appear in conjugate pairs. Consequently, complex zeros that are 
not on the unit circle appear in groups of four. To preserve this grouping one can use the 
following fourth-order block in a cascade-form realization.
H(z) 5 fz 2 r exp(2j)gfz 2 r exp(2j)gfz 2 r21 exp(2j)gfz 2 r21 exp( j)g
z4
 
5 fz2 2 2r cos()z 1 r2gfz2 2 2r21 cos()z 1 r22g
z4
 
5 fz2 2 2r cos()z 1 r2gfr2z2 2 2r cos()z 1 1g
r2z4
 
5 c0(1 1 c1z21 1 c2)(c2 1 c1z21 1 1) 
 (6.9.20)
Linear phase
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

486    Chapter 6  FIR Filter Design
Note that this factored fourth-order linear-phase block can be realized as a cascade of 
two second-order blocks where the distinct coefficients are c 5 fr22, 22r cos(), r2gT. The 
coefficient order is reversed for the two blocks. If c is quantized, this reciprocal zero 
relationship is preserved. Hence the quantized fourth-order block is still a linear-phase 
system. A block diagram of a cascade-form realization of a fourth-order linear-phase 
block is shown in Figure 6.45.
6.9.4 Roundoff Error, overflow, and scaling
The arithmetic used to compute the filter output must also be performed using finite pre-
cision. The time domain representation of an FIR filter using a tapped delay line direct 
form realization is
 
y(k) 5 o
m
i50
bix(k 2 i) 
 (6.9.21)
If the coefficients are quantized to N bits and the signals are quantized to N bits, then 
the product terms in (6.9.21) will each be of length 2N bits. When the products are then 
rounded to N bits, the resulting error is called roundoff error. It can be modeled as white 
noise with a separate white noise source for each product. Assuming the roundoff noise 
sources are statistically independent of one another, the noise sources associated with the 
products can be combined into a single error term as follows where QN denotes the N-bit 
quantization operator.
 
e(k) 5 o
m
i50
QNfbix(k 2 i)g 2 bix(k 2 i) 
 (6.9.22)
For a tapped delay line direct form realization, this results in the equivalent linear model 
of roundoff error shown in Figure 6.46 for the case m 5 2.
Suppose both the coefficient bi and the input x(k 2 i) lie in the range f2c, cg. Then 
the quantization level is q as given in (6.9.3). Each roundoff error noise is uniformly dis-
tributed over f2qy2, qy2g and has average power  2
x 5 q2y12. Since the m 1 1 sources are 
assumed to be statistically independent, their contributions can be added, which means 
that the average power of the roundoff noise appearing at the filter output is
 
 2
y 5 (m 1 1)q 2
12
 
 (6.9.23)
The roundoff noise can be reduced further if a special hardware architecture is used. 
Some DSP processors use a 2N-bit double length accumulator to store the results of 
the multiplications in (6.9.21). When this hardware configuration is used, it is only the 
Roundoff error
Roundoff noise
Figure 6.45:  
Cascade-form 
Realization of 
a Fourth-order 
Linear-phase Block
x
y
c1
c0
c2
c2
z21
z21
c1
z21
z21
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 6.9 Finite Word Length Effects     487
Figure 6.46:  
Linear Model of 
Product Roundoff 
Error in an FIR 
Filter, m 5 2
x
y
e
b1
b0
b2
z21
z21
final sum, y(k), that is quantized to N bits. As a consequence, the noise term in (6.9.22) 
simplifies to
 
e(k) 5 QN 3o
n
i50
bix(k 2 i)4 2 y(k) 
 (6.9.24)
In this case there is only one roundoff noise source instead of m 1 1 as in (6.9.22). The 
end result is that the average power of the roundoff error output noise in (6.9.23) is 
reduced by a factor of m 1 1. This makes the use of a double-length accumulator an 
attractive hardware option for implementing a direct form FIR filter.
overflow
Another source of error occurs as a result of the summing operation in (6.9.21). The sum of 
several N-bit numbers will not necessarily fit within N bits. When the sum is too large to fit, 
this results in overflow error. Overflow errors can cause a significant change in the filter out-
put. This type of error can eliminated, or significantly reduced, by scaling either the input or 
the filter coefficients. If ux(k)u # c, the FIR filter output in (6.9.21) is also bounded as follows.
uy(k)u 5 uo
m
i50
bix(k 2 i)u
 
# o
m
i50
ubix(k 2 i)u
 
5 o
m
i50
ubiu ? ux(k 2 i)u
 
# co
m
i50
ubiu 
 (6.9.25)
Thus uy(k)u # cuubuu1 where uubuu1 is the L1 norm of the coefficient vector b. That is,
 
uubuu1 5
D  o
m
i50
ubiu 
 (6.9.26)
From (6.9.25) it is apparent that addition overflow at the output is eliminated  
(i.e., uy(k)u # c) when the input signal x(k) is scaled by s1 using scale factor s1 5 1yuubuu1. 
Overflow error
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

488    Chapter 6  FIR Filter Design
A signal flow graph of a third-order direct form FIR filter realization that uses scaling to 
prevent overflow is shown in Figure 6.47.
Scaling using the L1 norm is effective in preventing overflow, but it does suffer from 
a practical drawback. Roundoff noise and ADC quantization noise are not affected sig-
nificantly by scaling. As a result, when the input is scaled by s1, the resulting reduction 
in signal strength can cause a corresponding reduction in the signal-to-noise ratio. Less 
severe forms of scaling can be used that eliminate most, but not all, overflow. For exam-
ple, if the input signal is a pure sinusoid, then overflow from this type of periodic input 
can be eliminated by using scaling that is based on the filter magnitude response.
 
uuHuu` 5 
D
max
0#f#fsy2huH( f )u} 
 (6.9.27)
Overflow from a pure sinusoidal input is prevented if the input signal x(k) is scaled by 
s` 5 1yuuHuu`. Another commonly used form of scaling uses the L2 or Euclidean norm.
 
uubuu2  5 
D  1o
m
i50
ubiu22
1y2
 
 (6.9.28)
Again the scale factor is s2 5 1yuubuu2. One advantage of the L2 norm is that, like the 
L1 norm, it is easy to compute. The three norms can be shown to satisfy the following 
relationship.
 
uubuu2 # uuHuu` # uubuu1 
 (6.9.29)
Figure 6.47:  
Scaling to Prevent 
Addition Overflow 
in an FIR Filter,  
m 5 3
x
y
1/s1
b1
b0
s1
b2
b3
z21
z21
z21
FIR overflow and scaling
EXAMPLE 6.19
As an illustration of the prevention of overflow by scaling, suppose ux(k)u # 5, 
and consider the following FIR filter.
H(z) 5 o
20
i50
z2i
1 1 i
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.10 GUI Modules and Case Studies     489
Figure 6.48 Magnitude Response of FIR Filter in Example 6.19 
0
0.5
0.5
1
1.5
2
2.5
3
3.5
4
f/fs
A(f)
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45
Here c 5 5. Running exam6_19 produces the following values for the scale factors 
for this filter of order m 5 20.
s1 5 3.6454
s2 5 1.2643
s` 5 3.6454
In this instance it turns out that the L1 and L` scale factors are identical. For this 
FIR system s` 5 A(0) because the magnitude response achieves its peak at f 5 0, as 
can be seen from the plot shown in Figure 6.48.
gUI Modules and Case studies 
This section focuses on the design and realization of FIR filters. A graphical user inter-
face module called g_  fir is introduced that allows the user to design and implement FIR 
filters, all without any need for programming. A case study programming example using 
the DSP Companion functions is then presented.
g_fir: Design and implement FIR filters
The DSP Companion includes a GUI module called g_  fir that allows the user to design 
a variety of FIR filters. GUI module g_  fir features a display screen with tiled windows as 
shown in Figure 6.49. The design features of g_fir are summarized in Table 6.6.
6.10
gUI Module
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

490    Chapter 6  FIR Filter Design
Select Type
Select View
Slider Bar
Edit Parameters
0
100
200
300
400
500
600
700
800
900
1000
0
0.2
0.4
0.6
0.8
1
1
0.5
00
0.2
0.4
0.6
0.8
1
Bandpass Least-squares Filter: m 5 40
f (Hz)
A(f)
FIR filter
Specifications
g_fir
x(k)
H(z)
y(k)
Figure 6.49: Display Screen for GUI Module g _fir
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.10 GUI Modules and Case Studies     491
The upper left-hand Block diagram window contains a block diagram of the FIR 
filter under investigation. It is an mth order filter with the following transfer function.
 
H(z) 5 o
m
i50
biz2i 
 (6.10.1)
The Parameters window below the block diagram displays edit boxes containing the 
filter parameters. The contents of each edit box can be directly modified by the user with 
the Enter key used to activate changes. The parameters F0, F1, B, and fs are the lower 
cutoff frequency, upper cutoff frequency, transition bandwidth, and sampling frequency, 
respectively. The parameters deltap 5 p and deltas 5 s are the passband ripple factor 
and the stopband attenuation factor, respectively. For a lowpass filter the passband cutoff 
is F0, and for a highpass filter the passband cutoff is F1. Bandpass filters have a passband 
of fF0, F1g, and bandstop filters have a stopband of fF0, F1g. In all cases B is the width of 
the transition band.
The Type and View windows in the upper-right corner of the screen allow the user to 
select both the type of filter and the viewing mode. The filter types include lowpass, high-
pass, bandpass, and bandstop filters. Also included is a user-defined filter. The desired 
amplitude response and residual phase response are imported from a used-defined M-file 
that has the following calling sequence where u_fir is a user-supplied name.
[Ar,theta] = u_fir (f,fs); 
% Frequency response
When u_  fir is called with frequency f  and sampling frequency fs, it must evaluate the 
desired amplitude response and residual phase response at the vector f , and return 
the results in the vectors Ar and theta, respectively. Output theta is only needed for 
the quadrature filter type. For all other filters types, the residual phase response is 
theta = zeros(size( f )).
The View options include the magnitude response, the phase response, the impulse 
response, a pole-zero plot, and the window used with the windowed design method. 
The dB check box toggles the magnitude response display between linear and logarith-
mic scales. When it is checked, the passband ripple and stopband attenuation in the 
Parameters window change to their logarithmic equivalents, Ap and As, respectively. 
The Plot window along the bottom half of the screen shows the selected view. Below 
the Type and View windows is a horizontal slider bar that allows the user to directly 
control the filter order m.
The Menu bar at the top of the screen includes several menu options. The Export option 
is used to save a, b, x, y, fs to a MAT file. Results exported from one GUI module can be 
imported into other GUI modules for additional processing. The Caliper option allows the 
Table 6.6:  
Features of  
GUI Module g_fir
Item  
Variables 
Block diagram  
x(k), y(k) 
Edit parameters  
F0, F1, B, fs, p, s 
Filter type  
lowpass, highpass, bandpass, bandstop, import 
Plot view  
 magnitude response, phase response, impulse response,  
pole-zero plot, window 
Slider  
filter order m 
Check boxes  
linear/dB scale 
Menu buttons  
method, export, caliper, print, help, exit 
Import  
desired frequency response m-file 
Export  
a, b, x, y, fs
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

492    Chapter 6  FIR Filter Design
user to measure any point on the current plot by moving the mouse cross hairs to that point 
and clicking. The Method option allows the user to select the filter design method from 
the windowed, frequency-sampled, least-squares, equiripple, and quadrature methods. The 
Print option sends the GUI window to a printer or a file. Finally, the  Help option provides 
the user with some helpful suggestions on how to effectively use module g_  fir.
Recall that GUI module g_  filters was discussed previously in Section 5.9. Using  
g_  filters, different filter realization structures can be investigated, and the effects of coef-
ficient quantization error can be explored. To analyze an FIR filter designed with g_  fir in 
this manner, use the Export option from g_  fir and the Import option in g_  filters.
Bandstop Filter Design: A Comparison
In order to illustrate the different design methods for constructing an FIR filter, suppose 
fs 5 2000 Hz, and consider the problem of designing a bandstop filter to meet the fol-
lowing specifications.
 
(Fp1, Fs1, Fs2, Fp2) 5 (200, 300, 700, 800) Hz 
 (6.10.2a)
 
(p, s) 5 (0.04, 0.02) 
 (6.10.2b)
Using (5.2.7), the corresponding passband ripple and stopband attenuation in dB are
 
Ap 5 0.36 dB 
 (6.10.3a)
 
As 5 33.98 dB 
 (6.10.3b)
To facilitate a fair comparison of the five design methods covered in this chapter, a com-
mon filter order of m 5 80 is used for all cases. Plots of the resulting magnitude  responses 
can be obtained by running case6_1 from the driver program g_dsp.
The first plot generated by case6_1 corresponds to a windowed filter and is shown in  
Figure 6.50. In this case a Hanning window was used. Since this is a stopband filter and 
the dB scale is used, we focus on the stopband rather than the passband. It is apparent 
that the windowed filter does not meet the stopband specification because the transition 
band is too wide. The stopband attenuation in this case is Astop 5 22.7 dB.
Case Study 6.1
0
1000
280
270
260
250
240
230
220
210
0
10
20
m 5 80, As 5 22.6 dB
200 300
100
400 500 600 700 800 900
f (Hz)
A(f) (dB)
Figure 6.50:  
Magnitude 
Response of 
a Windowed 
Bandstop Filter 
Using a Hanning 
Window, m 5 80 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.10 GUI Modules and Case Studies     493
The second plot generated by case6_1, which corresponds to the frequency-sampled 
method, is shown in Figure 6.51. Here the spacing between the frequency samples is
DF 5 fs
m
 
5 25 Hz 
 (6.10.4)
From (6.10.2), the width of the transition band is B 5 100 Hz. Consequently, there are 
three samples in the interior of each transition band. On the surface it appears that the 
frequency-sampled magnitude response may have met the stopband specification. How-
ever, a close inspection of Figure 6.51 reveals that near f 5 Fs2, the magnitude response 
is larger than the As specification, so the stopband attenuation ends up being only 
Astop 5 24.5 dB in this case.
The third plot generated by case6_1, corresponding to the least-squares method, is as 
shown in Figure 6.52. To facilitate comparison with the equiripple method, the weighting 
vector for the least-squares method was constructed using (6.7.7). From (6.10.2) we have 
syp 5 .5, so this resulted in the following weighting vector.
 
w(i) 55
.5, fi [ passband
1,
fi [ stopband 
 (6.10.5)
The least-squares method also requires that the weights be specified in the transition 
band, and in this case they were set to the passband value. The resulting magnitude 
response in Figure 6.52 does meet the design specifications using a filter of order m 5 80 
with a stopband attenuation of Astop 5 34.5 dB in comparison with the specification of 
As 5 33.98 dB.
The fourth plot generated by case6_1 corresponds to the equiripple design method and 
is as shown in Figure 6.53. It is apparent from Figure 6.53 that the equiripple filter easily 
meets, and in fact exceeds, the stopband specification. The stopband attenuation achieved in 
this case is Astop 5 72.8 dB. Recall that each 20 dB corresponds to a reduction in gain by a 
factor of 10. Therefore the stopband gain is somewhere between 1024 and 1023 for this filter.
Figure 6.51:  
Magnitude 
Response of a  
Frequency- 
sampled Bandstop 
Filter, m 5 80
0
1000
280
270
260
250
240
230
220
210
0
10
20
m 5 80, As 5 24.5 dB
200 300
100
400 500 600 700 800 900
f (Hz)
A(f) (dB)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

494    Chapter 6  FIR Filter Design
The last plot generated by case6_1, corresponding to the quadrature method, is 
shown in Figure 6.54. The quadrature method is more general than the other methods 
because the residual phase response can be specified. To facilitate a comparison, the 
residual phase response was set to d( f ) 5 0. The resulting magnitude response is shown 
in Figure 6.54. The filter order was m 5 80 with a Blackman window. To demonstrate 
that small values of stopband attenuation can be used, it was set to s 5 .0001. Note that 
the stopband attenuation appears to closely track the specification throughout most of 
the stopband. However, it does not satisfy it at the edges of the stopband where the stop-
band attenuation is Astop 5 20.7 dB.
Figure 6.53:  
Magnitude 
Response of 
an Equiripple 
Bandstop Filter, 
m 5 80
0
1000
280
270
260
250
240
230
220
210
0
10
20
200 300
100
400 500 600 700 800 900
f (Hz)
A(f) (dB)
m 5 80, As 5 72.8 dB
Figure 6.52:  
Magnitude 
Response of a 
Least-squares 
Bandstop Filter, 
m 5 80
m 5 80, As 5 34.5 dB
0
1000
280
270
260
250
240
230
220
210
0
10
20
200 300
100
400 500 600 700 800 900
f (Hz)
A(f) (dB)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.11 Chapter Summary     495
Figure 6.54:  
Magnitude 
Response of 
a Quadrature 
Bandstop Filter 
with a Blackman 
Window and 
s 5 .0001, 
m 5 80
0
200 300
100
400 500 600 700 800 900 1000
280
270
260
250
240
230
220
210
0
10
20
m 5 80, As 5 20.7 dB
f (Hz)
A(f) (dB)
Chapter summary 
Learning outcomes
This chapter was designed to provide the student with an opportunity to achieve the 
learning outcomes summarized in Table 6.7.
6.11
Table 6.7:  
Learning Outcomes  
for Chapter 6 
Num.   
Learning outcome 
sec. 
 1  
Understand the relative advantages and disadvantages of FIR  
 
 
filters in comparison with IIR filters  
6.1
 2  
Know how to measure the signal-to-noise ratio  
6.1 
 3  
Be able to design a linear-phase FIR filter with a prescribed  
6.2 
 
  
magnitude response using the windowing method  
 
 4  
Understand why windows are used and the tradeoff involved  
6.2 
 5  
Be able to design a linear-phase FIR filter with a prescribed  
 
 
  
magnitude response using the frequency-sampling method  
6.3
 6  
Know how to insert frequency samples in the transition band to  
 
 
  
control filter performance  
6.3 
 7  
Be able to design a linear-phase FIR filter with a prescribed  
 
 
  
magnitude response using the least-squares method  
6.4 
 8  
Be able to design a linear-phase equiripple frequency-selective  
 
 
 
filter using the Parks-McClellan algorithm  
6.5 
 9  
Know how to design FIR differentiators and Hilbert transformers  
6.6 
 10  
Be able to design a FIR filters with prescribed magnitude and  
 
 
  
residual phase responses using the quadrature method  
6.7 
 11  
Understand the benefits of different filter realization structures  
6.8 
 12  
Be aware of detrimental finite word length effects and know  
 
 
  
how to minimize them  
6.9 
 13  
Know how to use the GUI module g_ fir to design and  
 
 
  
analyze digital FIR filters without any programming 
6.10
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

496    Chapter 6  FIR Filter Design
Finite Impulse Response Filters  
Chapter 6 focused on the design of finite impulse response or FIR digital filters having 
the following transfer function.
 
H(z) 5 o
m
i50
biz2i 
 (6.11.1)
FIR filters offer a number of important advantages in comparison with IIR filters. FIR 
filters are always stable, regardless of the values of the filter coefficients. FIR filters are 
also less sensitive to finite word length effects. The impulse response of an FIR filter can 
be obtained directly from inspection of the transfer function as follows.
 
h 5 hb0, b1 , Á , bm, 0, 0, Á j 
 (6.11.2)
Techniques are available for designing FIR filters that closely approximate arbitrary mag-
nitude responses if the order of the filter is allowed to be sufficiently large. One drawback 
of FIR filters is that they require higher-order filters than IIR filters that satisfy the same 
design specifications. This implies a longer delay, larger storage requirements, and longer 
computational times: a consideration that may be important for real-time signal process-
ing applications.
Linear-phase Filters
The phase response of FIR filters can be made to be linear. A linear phase response is an 
important characteristic because it means that different spectral components of the input 
signal are delayed by the same amount as they are processed by the filter. A linear-phase fil-
ter does not distort a signal within the passband, it only delays it by  5 mTy2. The symme-
try constraints on the impulse response that ensures that an FIR filter has linear phase is
 
h(k) 5 6h(m 2 k), 0 # k # m 
 (6.11.3)
If the plus sign is used, the impulse response h(k) is a palindrome that exhibits even 
symmetry about the midpoint k 5 my2; otherwise it exhibits odd symmetry. There are 
four types of linear-phase FIR filters, depending on whether the symmetry is even or odd 
and the filter order is even or odd. The most general linear-phase filter is a type 1 filter 
with even symmetry and even order. The other three filter types have zeros at one or both 
ends of the frequency range, and they are sometimes used for specialized applications, 
such as the design of differentiators and Hilbert transformers.
Filter Design Methods
Four techniques were presented for designing an mth order linear-phase FIR filter, and 
one for a 2m-th order FIR filter. The first four all introduce a constant group delay cor-
responding to half the filter length to make the impulse response causal. The windowing 
method is a truncated impulse response technique that tapers the coefficients with a data 
window to reduce ringing in the amplitude response caused by the Gibb’s phenomenon. 
There is a tradeoff between the reduction in ringing and the width of the transition band. 
Popular windows include the rectangular, Hanning, Hamming, and Blackman windows. 
When the rectangular window is used, the resulting filter minimizes the following mean 
square error where Ad(r) is the desired amplitude response and Ar( f ) is the actual ampli-
tude response.
 
J 5#
fsy2
0
uAr( f ) 2 Ad( f )u2df  
 (6.11.4)
FIR filter advantages
Symmetry constraints
Linear-phase filter 
types
Windowing method
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.11 Chapter Summary     497
The frequency-sampling method uses m equally spaced frequencies and the IDFT 
to compute the filter coefficients. Oscillations in the resulting magnitude response 
can be reduced by including one or more frequency samples in a transition band. The 
values of the transition-band samples can be optimized to maximize the stopband 
attenuation.
The least-squares method is a direct optimization method that uses an arbitrary 
strictly increasing set of discrete frequencies. It minimizes a weighted sum of squares 
of the error between the desired and the actual amplitude responses. Finding the coef-
ficients requires solving a linear algebraic system of order p 1 1 where p 5 my2. If 
significant error exists at certain frequencies, these frequencies can be given additional 
weight to redistribute the error. The windowed, frequency-sampled, and least-squares 
methods can be used to design general linear-phase FIR filters with prescribed ampli-
tude responses.
The fourth method is the optimal equiripple method, a technique that minimizes the 
maximum of the absolute value of the error in the passband and the stopband. Equirip-
ple filters have amplitude responses that have ripples of equal magnitude in the passband 
and the in the stopband. For a given set of frequency-selective filter design specifica-
tions, equiripple filters tend to be of lower order than filters designed with the windowed,  
frequency-sampled, and least-squares methods. To minimize the maximum of the abso-
lute value of the error in the passband and the stopband, the amplitude response of an 
optimal equiripple filter must satisfy the following equations.
 
Ar(Fi) 5 Ad(Fi) 2 (21)i
w(Fi) , 0 # i , p 1 2 
 (6.11.5)
Here w( f ) . 0 is a weighting function, and the Fi are extremal frequencies in the pass-
band and the stopband where the magnitude of the error achieves its maximum value of 
. The FIR amplitude response Ar( f ) can be shown to be a polynomial in x 5 cos(2f T ) 
of degree p 5 floor(my2). The following normalized weighting function is used to design 
an equiripple frequency-selective filter.
 
w( f ) 55
syp,
f [ passband
1,
f [ stopband 
 (6.11.6)
The last filter design method is a quadrature filter of order 2m. Quadrature fil-
ters differ from the others in that they are designed to meet both magnitude response 
and residual phase response specifications. The residual phase response is the phase 
response remaining after the phase associated with a constant group delay has been 
removed. A quadrature filter of order 2m has the following transfer function where 
p 5 my2.
 
H(z) 5 H1(z)F(z) 1 H2(z)G(z) 
 (6.11.7)
Quadrature filters use a delay, H1(z), of my2 samples and a Hilbert transformer, H2(z), 
in the first stage to create a pair of signals that are in phase quadrature. Linear-phase 
FIR filters, F(z) and G(z), are then applied to these signals to approximate the real and 
imaginary parts of the desired frequency response, respectively. The desired magnitude 
response Ad( f ) must have a stopband attenuation, s . 0, for the phase response to be 
well defined.
Frequency-sampled 
filter
Least-squares filter
Optimal equiripple 
filter
Quadrature filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

498    Chapter 6  FIR Filter Design
Filter Realization structures
There are a number of alternative signal flow graph realizations of FIR filters. Direct- 
form realizations have the property that the gains in the signal flow graphs are obtained 
directly from inspection of the transfer function. For FIR filters these include the 
tapped delay line, the transposed tapped delay line, and a direct form realization for 
linear-phase filters that requires only about half as many floating-point multiplications. 
There are also a number of indirect realizations whose parameters must be computed 
from the original transfer function. The indirect forms decompose the original transfer 
function into lower-order blocks by combining complex conjugate pairs of zeros. For 
example, FIR filters can be realized with the following cascade-form realization which 
is based on factoring H(z).
 
H(z) 5 b0H1(z) Á HM(z) 
 (6.11.8)
Here M 5 floorf(m 1 1)y2g and the Hi(z) are second-order blocks with real coefficients 
except for HM(z), which is a first-order block when the filter order m is odd. Another FIR 
filter realization is the lattice-form realization that consists of m blocks and a signal flow 
graph that resembles a lattice ladder structure. All of the filter realizations are equivalent 
to one another in terms of their overall input-output behavior when infinite precision 
arithmetic is used.
*Finite Word Length Effects
Finite word length effects arise when a filter is implemented in either hardware or soft-
ware. They are caused by the fact that both the filter parameters and the filter signals 
must be represented using a finite number of bits of precision. Both floating-point and 
fixed-point numerical representations can be used. MATLAB uses a 64-bit double-precision  
floating-point representation that minimizes finite word length effects. When an N-bit fixed-
point representation is used for values in the range f2c, cg, the quantization level, or spacing 
between adjacent values, is
 
q 5
c
2N 2 1 
 (6.11.9)
Typically, the scale factor is c 5 2M for some integer M $ 0. This way, M 1 1 bits are 
used to represent the integer part including the sign, and the remaining N 2 (M 1 1) bits 
are used for the fraction part.
Quantization error can arise from ADC quantization, input quantization, coef-
ficient quantization, and product roundoff quantization. It is modeled as additive 
white noise uniformly distributed over f2qy2, qy2g. Another source of error is over-
flow error that can occur when several finite precision numbers are added. Overflow 
error can be eliminated by proper scaling of the input. The roots of a polynomial are 
very sensitive to the changes in the polynomial coefficients, particularly for higher- 
degree polynomials. Quantized FIR filters do not become unstable because all of 
the poles are at the origin. More generally, FIR filters are less sensitive to finite word 
length effects than IIR filters. Indirect form realizations tend to be less sensitive to 
finite word length effects because the block transfer functions are only of second 
order.
Direct-form 
realizations
Cascade-form 
realization
Optional material
Quantization level
Quantization error
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.12 Problems    499
gUI Modules
The DSP Companion includes a GUI module called g_  fir that allows the user to design 
and implement FIR filters without any need for programming. The GUI module g_  filters  
described in Section 5.9 allows the user to investigate different filter realization structures 
and the effects of coefficient quantization error. Filters designed with g_  fir can be evalu-
ated with g_  filters by exporting and importing the filter parameters.
Problems
The problems are divided into Analysis and Design problems that can be solved by hand 
or with a calculator, GUI Simulation problems that are solved using GUI module g_  fir, 
and MATLAB Computation problems that require a user program. Solutions to selected 
problems can be accessed with the DSP Companion driver program, g_ dsp. Students 
are encouraged to use these problems, which are identified with a 
, as a check on their 
understanding of the material.
6.12.1 Analysis and Design
section 6.1: Motivation
6.1 
Consider the following noise-corrupted periodic signal. Here v(k) is white noise 
uniformly distributed over f2.5, .5g.
x(k) 5 3 1 2 cos(.2k)
y(k) 5 x(k) 1 v(k)
(a) Find the average power of the noise-free signal, x(k).
(b) Find the signal-to-noise ratio of y(k).
(c) Suppose y(k) is sent through an ideal lowpass filter with cutoff frequency 
F0 5 .15fs to produce z(k). Is the signal x(k) affected by this filter? Find the 
signal-to-noise ratio of z(k).
section 6.2: Windowing Method
6.2 
Consider the problem of designing an mth order type 3 linear-phase FIR filter hav-
ing the following amplitude response.
Ar( f ) 5 sin(2f T ), 0 # f # fsy2
(a) Assuming m 5 2p for some integer p, find the coefficients using the windowing 
method with the rectangular window.
(b) Find the filter coefficients using the windowing method with the Hamming 
window.
6.3 
Suppose a lowpass filter of order m 5 10 is designed using the windowing method 
with the Hanning window and fs 5 2000 Hz.
(a) Estimate the width of the transition band.
(b) Estimate the linear passband ripple and stopband attenuation.
(c) Estimate the logarithmic passband ripple and stopband attenuation.
6.12
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

500    Chapter 6  FIR Filter Design
6.4 
Consider the problem of using the windowing method to design a lowpass filter to 
meet the following specifications.
(fs, Fp, Fs) 5 (200, 30, 50) Hz
(Ap, As) 5 (.02, 50) dB
(a) Which types of windows can be used to satisfy these design specifications?
(b) For each of the windows in part (a), find the minimum order of filter m that 
will satisfy the design specifications.
(c) Assuming an ideal piecewise-constant amplitude response is used, find an 
appropriate value for the cutoff frequency Fc.
6.5 
Suppose the windowing method is used to design an mth order lowpass FIR filter. 
The candidate windows include rectangular, Hanning, Hamming, and Blackman.
(a) Which window has the smallest transition band?
(b) Which window has the smallest passband ripple, Ap?
(c) Which window has the largest stopband attenuation, As?
6.6 
A linear-phase FIR filter is designed with the windowing method using the  
Hanning window. The filter meets its transition bandwidth specification of 200 Hz 
exactly with a filter of order m 5 30.
(a) What is the sampling rate, fs?
(b) Find the filter order needed to achieve the same transition bandwidth using 
the Hamming window.
(c) Find the filter order needed to achieve the same transition bandwidth using 
the Blackman window.
6.7 
Consider the problem of designing an ideal linear-phase bandstop FIR filter with 
the windowing method using the Blackman window. Find the coefficients of a filter 
of order m 5 40 using the following cutoff frequencies.
(fs, Fs1, Fs2) 5 (10, 2, 4) kHz
6.8 
Consider the problem of designing a type 1 linear-phase windowed FIR filter with 
the following desired amplitude response.
Ar( f ) 5 cos(f T ), 0 # u f u # fsy2
Suppose the filter order is even with m 5 2p. Find the impulse response h(k) using a 
rectangular window. Simplify the expression for h(k) as much as possible.
section 6.3: Frequency-sampling Method
6.9 
Consider the problem of designing a type 1 linear-phase bandpass FIR filter 
using the frequency-sampling method. Suppose the filter order is m 5 60. Find 
a simplified expression for the filter coefficients using the following ideal design 
specifications.
(fs, Fp1, Fp2) 5 (1000, 100, 300) Hz
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.12 Problems    501
section 6.4: Least-squares Method
6.10 Consider a type 3 linear-phase FIR filter of order m 5 2p. Find a simplified expres-
sion for the amplitude response Ar( f ) similar to (6.4.7), but for a type 3 linear-phase 
FIR filter.
6.11 Use the results of Problem 6.10 to derive the normal equations for the coefficients 
of a least-squares type 3 linear-phase filter. Specifically, find expressions for the 
coefficient matrix G and the right-hand side vector d, and show how to obtain the 
filter coefficients from the solution of the normal equations.
section 6.5: Equiripple Filter Design
6.12 Suppose the equiripple design method is used to construct a highpass filter to meet 
the following specifications. Estimate the required filter order.
(fs, Fs, Fp) 5 (100, 20, 30) kHz
(Ap, As) 5 (.2, 32) dB
6.13 Consider the problem of constructing an equiripple bandstop filter of order m 5 40. 
Suppose the design specifications are as follows.
(fs, Fp1, Fs1, Fs2, Fp2) 5 (200, 20, 30, 50, 60) Hz
(p, s) 5 (.05, .03)
(a) Let r be the number of extremal frequencies in the optimal amplitude response. 
Find a range for r.
(b) Find the set of specification frequencies, F.
(c) Find the weighting function w( f ).
(d) Find the desired amplitude response Ad( f ).
(e) The amplitude response Ar( f ) is a polynomial in x. Find x in terms of f , and 
find the polynomial degree.
6.14 Consider the problem of constructing an equiripple lowpass filter of order m 5 4 
satisfying the following design specifications.
(fs, Fp, Fs) 5 (10, 2, 3) Hz
(p, s) 5 (.05, .1)
Suppose the initial guess for the extremal frequencies is as follows.
(F0, F1, F2, F3) 5 (0, Fp, Fs, fsy2)
(a) Find the weights w(Fi) for 0 # i # 3.
(b) Find the desired amplitude response values Ad(Fi) for 0 # i # 3.
(c) Find the extremal angles i 5 2FiT for 0 # i # 3.
(d) Write down the vector equation that must be solved to find the Chebyshev 
coefficient vector d and the parameter . You do not have to solve the equa-
tion, just formulate it.
section 6.6: Hilbert Transformer Design
6.15 Consider the problem of designing a filter to approximate a differentiator. Use 
the frequency-sampling method to design a type 3 linear-phase filter of order 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

502    Chapter 6  FIR Filter Design
m 5 40 that approximates a differentiator, but with a delay my2 samples. That is, 
find simplified expressions for the coefficients of a filter with the following desired 
amplitude response.
Ar( f ) 5 2fT
section 6.7: Quadrature Filter Design
6.16 Consider the problem of designing a quadrature filter with the following frequency 
response. To simplify the final answer, you can assume that the Hilbert transformer 
component of the quadrature filter is ideal.
H( f ) 55
5j exp(2j20f T ),
0 , f , fsy2
0,
f 5 0, 6fsy2
25j exp(2j20f T ),
2 fsy2 , f , 0
(a) Find the magnitude response A( f ) and the residual phase response ( f ). 
(b) Suppose windowed filters with a Hamming window are used. Find F(z) and G(z).
6.17 Suppose F(z) and G(z) are the following FIR filters.
F(z) 5 1 1 2z21 1 z22
G(z) 5 2 1 z21 1 2z22
(a) Show that F(z) and G(z) are type 1 linear-phase FIR filters.
(b) Find the amplitude responses Af( f ) and Ag( f ).
(c) Assuming F(z) and G(z) are used to construct a quadrature filter using an ideal 
Hilbert transformer, find the magnitude response Aq( f ) and the residual phase 
response q( f ).
section 6.8: Filter Realization structures
6.18 Consider the following FIR filter. Find a cascade-form realization of this filter and 
sketch the signal flow graph.
H(z) 5 10(z2 2 .6z 2 .16)f(z 2 .4)2 1 .25g
z4
6.19 Consider the following FIR filter. Find a lattice-form realization of this filter and 
sketch the signal flow graph.
H(z) 5 1 1 2z21 1 3z22 1 4z23
6.20 Find an efficient direct form realization for a linear-phase filter of order m 5 2p 
similar to (6.8.4), but applicable to a type 3 filter. Sketch the signal flow graph for 
the case m 5 4.
section 6.9: Finite Word Length Effects
6.21 Suppose a 12-bit fixed-point representation is used to represent values in the  
range 210 # x , 10.
(a) How many distinct values of x can be represented?
(b) What is the quantization level, or spacing between adjacent values?
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.12 Problems    503
6.22 Consider the system shown in Figure 6.55. The ADC has a precision of 10 bits and 
an input range of uxa(t)u # 10. The transfer function of the digital filter is
H(z) 5
3z2 2 2z
z2 2 1.2z 1 .32
(a) Find the quantization level of the ADC.
(b) Find the average power of the quantization noise at the input x.
(c) Find the power gain of H(z).
(d) Find the average power of the quantization noise at the output y.
6.23 Suppose a 16-bit fixed-point representation is used for values in the range 
uxu # 8.
(a) How many distinct values of x can be represented?
(b) What is the quantization level, or spacing between adjacent values?
(c) How many bits are used to represent the integer part (including the sign)?
(d) How many bits are used to represent the fraction part?
6.24 Suppose the coefficients of an FIR filter of order m 5 30 all lie within the range 
ubiu # 4. Assuming they are quantized to N 5 12 bits, find an upper bound on the error 
in the magnitude of in the frequency response caused by coefficient quantization.
6.25 A high-order FIR filter is realized as a cascade of second-order blocks.
(a) Suppose the filter has a sampling rate of fs 5 300 Hz and a zero at 
z0 5 exp( jy3). Find a nonzero periodic signal x(k) that gets completely atten-
uated by the filter.
(b) If a zero of a second-order block starts out on the unit circle, will the radius 
of the zero change as a result of the coefficient quantization? That is, will the 
zero still be on the unit circle?
(c) If a zero of a second-order block starts out on the unit circle, will the angle 
of the zero change as a result of the coefficient quantization? That is, will the 
frequency of the zero change?
6.26 Consider the following FIR filter.
H(z) 5 (z2 1 25)(z2 1 .04)
z4
(a) Show that this is a type 1 linear-phase filter.
(b) Sketch a signal flow graph realization of H(z) that is still a linear-phase system 
even when the coefficients are quantized.
6.27 Consider the following FIR filter.
H(z) 5 3 1 4z21 1 6z22 1 4z23 1 3z24
Figure 6.55: ADC 
Quantization Noise 
xa
H(z)
x
ADC
y
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

504    Chapter 6  FIR Filter Design
Suppose the input signal lies in the range ux(k)u # 10. Find scale factor for the input 
that ensures that the filter output will not overflow the range uy(k)u # 10.
6.12.2 gUI simulation 
section 6.2: Windowing Method
6.28 Use the GUI module g_  fir to design a windowed lowpass filter. Set the width of 
the transition band to B 5 150 Hz. For each of the following cases, find the lowest 
value for the filter order m that meets the specifications. Plot the linear magnitude 
response in each case.
(a) Rectangular window
(b) Hanning window
(c) Hamming window
(d) Blackman window
6.29 Use the GUI module g_  fir to construct a windowed highpass filter using the  
Hamming window.
(a) Plot the linear magnitude response and use the Caliper option to measure the 
actual width of the transition band.
(b) Plot the phase response.
(c) Plot the impulse response.
6.30 Use the GUI module g_  fir to design a windowed bandstop filter with the Hanning 
window to meet the following specifications. Adjust the filter order to the lowest 
value that meets the design specifications.
(fs, Fp1, Fs1, Fs2, Fp2) 5 (100, 20, 25, 35, 40) Hz
(p, s) 5 (.05, .05) 
(a) Plot the magnitude response using the linear scale.
(b) Export filter parameters a, b, and fs to prob6_30.mat. Then use GUI module 
g_  filters to import this filter. Adjust the number of bits used for coefficient 
quantization to N 5 6. Plot the linear magnitude responses.
section 6.3: Frequency-sampling Method
6.31 Use the GUI module g_  fir to design a frequency-sampled bandstop filter to meet 
the following specifications. Adjust the filter order to the lowest value that meets 
the design specifications.
(fs, Fp1, Fs1, Fs2, Fp2) 5 (100, 20, 25, 35, 40) Hz
(p, s) 5 (.05, .05) 
(a) Plot the magnitude response using the linear scale.
(b) Export filter parameters a, b, and fs to prob6_31.mat. Then use GUI module 
g_  filters to import this filter. Adjust the number of bits used for coefficient 
quantization to N 5 6. Plot the linear magnitude responses.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.12 Problems    505
6.32 Write an amplitude response function called prob6_32.m for the following user- 
defined filter (see u_  fir1.m for an example).
Ar( f ) 5
 cos(f  2y100)
1 1 f 2
, 0 # f # 10 Hz
Using GUI module g_  fir, set fs 5 20 Hz and select a frequency-sampled filter. 
Then use the Import option to load this filter. Plot the following cases.
(a) Magnitude response, m 5 10
(b) Magnitude response, m 5 20
(c) Magnitude response, m 5 40
(d) Impulse response, m 5 40
section 6.4: Least-squares Method
6.33 Use the GUI module g_  fir to design a least-squares bandpass filter to meet the 
following specifications. Adjust the filter order to the lowest value that meets the 
design specifications.
(fs, Fs1, Fp1, Fp2, Fs2) 5 (2000, 300, 400, 600, 700) Hz
(Ap, As) 5 (.4, 30) dB
(a) Plot the magnitude response using the dB scale.
(b) Export filter parameters a, b, and fs to prob6_33.mat. Then use GUI module 
g_  filters to import this filter. Adjust the number of bits used for coefficient 
quantization to N 5 6. Plot the linear magnitude responses.
6.34 Use the GUI module g_  fir to import the filter in file u_  fir1. Adjust the filter order 
to m 5 90. Plot the linear magnitude response for each of the following cases.
(a) Windowed filter with Blackman window
(b) Least-squares filter
6.35 Write an amplitude response function called prob6_35.m for the following user- 
defined filter (see u_  fir1 for an example).
Ar( f ) 5 2ucos1
2f
fs
 2u
Then use the Import option of GUI module g_fir to load this filter. Select a least-
squares filter. Plot the linear magnitude response for the following three cases.
(a) m 5 10
(b) m 5 20
(c) m 5 40
section 6.5: Equiripple Filter Design
6.36 Use the GUI module g_  fir to design an equiripple bandpass filter to meet the 
following specifications. Adjust the filter order to the lowest value that meets the 
design specifications.
(fs, Fs1, Fp1, Fp2, Fs2) 5 (2000, 300, 400, 600, 700) Hz
(Ap, As) 5 (.4, 30) dB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

506    Chapter 6  FIR Filter Design
(a) Plot the magnitude response using the dB scale.
(b) Export filter parameters to prob6_36. Then use GUI module g_  filters to 
import this filter. Adjust the number of bits used for coefficient quantization 
to N 5 6. Plot the linear magnitude responses.
section 6.7: Quadrature Filter Design
6.37 Write an amplitude response and residual phase response function called 
prob6_37.m for the following user-defined filter (see u_  fir1 for an example).
Ar( f ) 5 10fyfs
( f ) 5  sin(20fyfs)
Set the filter order to m 5 150, and select the quadrature filter. Use the Import 
option of GUI module g_  fir to load this filter. 
(a) Print your amplitude response and residual phase response functions.
(b) Plot the linear magnitude response.
(c) Plot the phase response.
6.38 Write an amplitude response and residual phase response function called 
prob6_38.m for the following user-defined filter (see u_  fir1 for an example).
Ar( f ) 5 .5h1 1
 sgn fsin(8f T )gj
( f ) 5 0
Set s 5 .001 and m 5 120. Then use the Import option of GUI module g_  fir to 
load this filter. Plot the following.
(a) The linear magnitude response of a least-squares filter
(b) The pole-zero pattern of a least-squares filter
(c) The linear magnitude response of a quadrature filter
(d) The pole-zero pattern of a quadrature filter
6.12.3 MATLAB Computation
section 6.1: Motivation
6.39 Write a MATLAB program that constructs the following signal where fs 5 200 
Hz. Here v(k) is white noise uniformly distributed over f21, 1g, F1 5 10 Hz, 
F2 5 30 Hz, and N 5 4096. Use a random number generator seed of 100 to pro-
duce v(k).
x(k) 5 4 sin(2F1kT) cos(2F2kT), 0 # k , N
 
y(k) 5 x(k) 1 v(k), 
0 # k , N
(a) Compute Px and Pv directly from the samples. Use Definition 6.1 to compute 
and print the signal-to-noise ratio (SNR) of y(k).
(b) Compute Py directly from the samples. Use Pv, (6.1.1) and Definition 6.1 to 
compute and print the SNR of y(k).
(c) Compute and print the percent error of the estimate of the SNR found  
in part (b) relative to the SNR found in part (a).
(d) Plot the magnitude spectrum of y(k) showing the signal and the noise.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.12 Problems    507
section 6.2: Windowing Method
6.40 Write a MATLAB program that uses f_  firideal to design a linear-phase lowpass 
FIR filter of order m 5 40 with passband cutoff frequency Fp 5 fsy5 and stopband 
cutoff frequency Fs 5 fsy4 where the sampling frequency is fs 5 100 Hz. Use a rect-
angular window, and set the ideal cutoff frequency to the middle of the transition 
band. Use f_  freqz to compute and plot the magnitude response using the linear 
scale. Then use Table 6.3, the hold on command, and the fill function to add the 
following items to your magnitude response plot.
(a) A shaded area showing the passband ripple, p
(b) A shaded area showing the stopband attenuation, s
6.41 Write a MATLAB program that uses f_  firideal to design a linear-phase high-
pass FIR filter of order m 5 30 with stopband cutoff frequency Fs 5 20 Hz, 
passband cutoff frequency Fp 5 30 and sampling frequency fs 5 100 Hz. Use a 
Hanning window, and set the ideal cutoff frequency to the middle of the tran-
sition band.
(a) Use f_  freqz to compute and plot the magnitude response using the dB scale.
(b) Use Table 6.3, the hold on command, and the fill function to add a shaded area 
showing the predicted stopband attenuation, As.
6.42 Write a MATLAB program that uses f_  firideal to design a linear-phase highpass 
FIR filter of order m 5 40 with stopband cutoff frequency Fs 5 20 Hz, passband 
cutoff frequency Fp 5 30, and sampling frequency fs 5 100 Hz. Use a Hamming 
window, and set the ideal cutoff frequency to the middle of the transition band.
(a) Use f_  freqz to compute and plot the magnitude response using the dB scale.
(b) Use Table 6.3, the hold on command, and the fill function to add a shaded area 
showing the predicted stopband attenuation, As.
6.43 Write a MATLAB program that uses f_  firwin to design a linear-phase highpass 
FIR filter of order m 5 60 with stopband cutoff frequency Fs 5 20 Hz, passband 
cutoff frequency Fp 5 30, and sampling frequency fs 5 100 Hz. Use a Blackman 
window, and make the desired amplitude response piecewise-constant with cutoff 
Fc 5 (Fs 1 Fp)y2.
(a) Use f_freqz to compute and plot the magnitude response using the dB scale.
(b) Use Table 6.3, the hold on command, and the fill function to add a shaded area 
showing the predicted stopband attenuation, As.
6.44 Write a MATLAB program that uses f_  firwin to design a type 1 linear-phase FIR 
filter of order m 5 80 using fs 5 1000 Hz and the Hamming window to approximate 
the following amplitude response. Use f_  freqz to compute the magnitude response.
Ar( f ) 55
1
f
2502
2
,
0 # u f u , 250
.5 cos3
(f 2 250)
500 4, 250 # u f u , 500
(a) Plot the linear magnitude response.
(b) On the same graph, add the desired magnitude response and a legend.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

508    Chapter 6  FIR Filter Design
section 6.3: Frequency-sampling Method
6.45 Write a MATLAB program that uses function f_  firsamp to design a linear-phase 
bandpass FIR filter of order m 5 40 using the frequency-sampling method. Use 
a sampling frequency of fs 5 200 Hz and a passband of Fp 5 f20, 60g Hz. Use 
f_  freqz to compute and plot the linear magnitude response. Add the frequency 
samples using a separate plot symbol and a legend. Do the following cases.
(a) No transition-band samples (ideal amplitude response)
(b) One transition-band sample of amplitude .5 on each side of the passband
6.46 Write a MATLAB program that uses function f_  firsamp to design a linear-phase 
bandstop FIR filter of order m 5 60 using the frequency-sampling method. Use a 
sampling frequency of fs 5 20 kHz and a stopband of Fs 5 f3, 8g kHz. Use f_  freqz 
to compute and plot the linear magnitude response. Add the frequency samples 
using a separate plot symbol and a legend. Do the following cases.
(a) No transition-band samples (ideal amplitude response)
(b) One transition-band sample of amplitude .5 on each side of the stopband
section 6.4: Least-squares Method
6.47 Write a MATLAB program that uses function f_  firls to design a least-squares lin-
ear-phase FIR filter of order m 5 30 with sampling frequency fs 5 400 and the 
following amplitude response.
Ar( f ) 55
f
100,
0 # u f u , 100
200 2 f
100
,
100 # u f u # 200
Select 2m equally spaced discrete frequencies, and use uniform weighting. Use 
f_  freqz to compute and plot both magnitude responses (ideal and actual) on 
the same graph.
section 6.5: Equiripple Filter Design
6.48 The Chebyshev polynomials have several interesting properties. Write a MATLAB 
program that uses the DSP Companion function f_  chebpoly and the subplot com-
mand to construct a 2 3 2 array of plots of the Chebyshev polynomials, Tk(x) for 
1 # k # 4. Use the plot range, 21 # x # 1. Using induction and your observations 
of the plots, list as many general properties of Tk(x) as you can. Use the help com-
mand for instructions on how to use f_  chebpoly.
6.49 Write a MATLAB function called u_  firorder that estimates the order of an equirip-
ple filter required to meet given design specifications using (6.5.21). The calling 
sequence for u_  firorder should be as follows.
% U_FIRORDER: Estimate required order for FIR equiripple filter
%
% Usage:
%        m = u_firorder (deltap,deltas,Bnorm);
(Continued )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.12 Problems    509
% Pre:
%        deltap = passband ripple
%        deltas = stopband attenuation
%        Bnorm  = normalized transition bandwidth
% Post:
%        m = estimated FIR equiripple order
Test your function by plotting a family of curves on one graph. For the kth curve, 
use deltap 5 deltas 5 , where  5 .03k for 1 # k # 3. Plot m versus Bnorm 
for .01 # Bnorm # .1 and include a legend.
6.50 Write a MATLAB program that uses the function f_  firparks to design an equirip-
ple lowpass filter to meet the following design specifications where fs 5 4000 Hz. 
Find the lowest order filter that meets the specifications.
(Fp, Fs) 5 (1200, 1400) Hz
(p, s) 5 (.03, .04)
(a) Print the minimum filter order and the estimated order based on (6.5.21).
(b) Plot the linear magnitude response.
(c) Use fill to add shaded areas to the plot showing the design specifications.
6.51 Write a MATLAB program that uses the function f_firparks to design an equiripple 
highpass filter to meet the following design specifications where fs 5 300 Hz. Find 
the lowest-order filter that meets the specifications.
(Fs, Fp) 5 (90, 110) Hz
(p, s) 5 (.02, .03)
(a) Print the minimum filter order and the estimated order based on (6.5.21).
(b) Plot the linear magnitude response.
(c) Use fill to add shaded areas to the plot showing the design specifications.
section 6.6: Hilbert Transformer Design
6.52 Write a MATLAB program that uses the function f_hilbert to compute a Hilbert 
transformer filter using a Blackman window. Do the following cases.
(a) Use f_  freqz to compute and plot the magnitude responses for m 5 40 and 
m 5 80 on the same graph. Also show the ideal magnitude response and add a 
legend. Specify the filter type in the title.
(b) Use f_  freqz to compute and plot the magnitude responses for m 5 41 and 
m 5 81 on the same graph. Also show the ideal magnitude response and add a 
legend. Specify the filter type in the title.
section 6.7: Quadrature Filter Design
6.53 Using function f_firquad and Example 6.14 as a starting point, write a MATLAB 
program that designs an equalizer for a system H(z) with the following magnitude 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

510    Chapter 6  FIR Filter Design
and phase responses. Use filters of order m 5 160, s 5 .001, and the Hamming 
window.
Ad ( f ) 5 expf2(f T 2 .25)2y.01g
d ( f ) 5 210(f T )2 1  sin(5f T )
(a) Print the optimal delay  and the total delay q of the equalizer.
(b) Print a 3 3 1 array of plots showing the magnitude responses of the original 
system, the equalizer, and the equalized system similar to Figure 6.29.
(c) Print a 3 3 1 array of plots showing the residual phase responses of the origi-
nal system, the equalizer, and the equalized system similar to Figure 6.30.
(d) Plot the impulse response of the equalizer filter.
section 6.8: Filter Realization structures
6.54 Consider the following FIR transfer function.
H(z) 5 o
20
i50
z2i
1 1 i
(a) Write a MATLAB program that uses f_ lattice to compute a lattice-form reali-
zation of this filter. Print the gain and the reflection coefficients of the blocks.
(b) Suppose the sampling frequency is fs 5 600 Hz. Use f_  freqz to compute 
the frequency response using a lattice-form realization. Compute both the 
unquantized frequency response (e.g., 64 bits), and the frequency response 
with coefficient quantization using N 5 8 bits. Plot both magnitude responses 
on a single plot using the dB scale and a legend.
6.55 Consider the following FIR impulse response. Suppose the filter order is m 5 30.
h(k) 5 k 1 1
m
, 0 # k # m
(a) Write a MATLAB program that uses f_ cascade to compute a cascade-form 
realization of this filter. Print the gain b0 and the block coefficients, B and A.
(b) Suppose the sampling frequency is fs 5 400 Hz. Use f_  freqz to compute 
the frequency response using a cascade-form realization. Compute both the 
unquantized frequency response (set bits = 64), and the frequency response 
with coefficient quantization using 8 bits. Plot both magnitude responses on a 
single plot using the dB scale and a legend.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

511
IIR Filter Design
C H A P T E R  7
CHAPTER ToPICs
 7.1 
Motivation 
 7.2 
Filter Design by Pole-zero 
Placement 
 7.3 
Filter Design Parameters 
 7.4 
Classical Analog Filters 
 7.5 
Bilinear Transformation 
Method 
 7.6 
Frequency Transformations
 7.7 
Filter Realization 
Structures 
*7.8 
Finite Word Length 
Effects
 7.9 
GUI Modules and Case 
Studies 
 7.10 Chapter Summary 
 7.11 Problems
Motivation
Just as mechanical filters are used in pipes to block the flow of 
certain particles, discrete-time systems can be used as digital  
filters to block the flow of certain signals. A digital filter is a  
discrete-time system that is designed to reshape the spectrum 
of the input signal in order to produce desired spectral charac-
teristics in the output signal. Thus a digital filter is a frequency- 
selective filter that modifies the magnitude spectrum and the 
phase spectrum by selecting certain spectral components and 
inhibiting others. In this chapter we investigate the design of dig-
ital infinite impulse response (IIR) filters having the following 
generic transfer function.
H(z) 5 b0 1 b1z21 1 Á 1 bmz2m
1 1 a1z21 1 Á 1 anz2n
Recall that H(z) is an IIR filter if ai Þ 0 for some i $ 1. Other-
wise, H(z) is an FIR filter. Simple specialized IIR filters can be 
designed directly using a careful placement of poles and zeros. 
A more general technique is to start with a normalized lowpass 
analog filter and put it through a series of transformations to 
convert it to a desired digital filter. First a frequency transfor-
mation is used to map a normalized lowpass analog filter into 
a specified frequency-selective analog filter. This is followed by 
a bilinear transformation that converts the analog filter into an 
equivalent digital filter.
7.1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

512    Chapter 7  IIR Filter Design
We begin this chapter by introducing some practical examples of applications of 
IIR filters. Next, simple techniques based on pole-zero placement and gain matching 
are introduced for direct design of specialized IIR filters such as resonators, notch fil-
ters, and comb filters. This is followed by a presentation of the classical analog lowpass 
filters including Butterworth, Chebyshev, and elliptic filters. An analog-to-digital filter 
transformation technique called the bilinear transformation method is then presented. 
To generalize to other filters, frequency transformations are introduced that map nor-
malized lowpass filters into lowpass, highpass, bandpass, and bandstop filters. IIR fil-
ter realization structures are then examined, including both direct and indirect forms. 
They are compared in terms of computational time, storage requirements, and sensi-
tivity to finite word length effects. Sources of error associated with finite word length 
are investigated. Finally a GUI module called g_iir is introduced that allows the user to 
design and evaluate a variety of digital IIR filters without any need for programming. 
The chapter concludes with a case study example and a summary of IIR filter design 
techniques.
7.1.1 Tunable Plucked-string Filter
Computer-generated music is a natural application area for IIR filter design (Steiglitz, 
1996). A simple, yet highly effective, building block for the synthesis of musical sounds is 
the tuned plucked-string filter shown in Figure 7.1. The output from this type of filter can 
be used, for example, to synthesize the sound from a stringed instrument such as a guitar.
The design parameters of the plucked-string filter in Figure 7.1 are the sampling fre-
quency fs, the pitch parameter 0 , c , 1, the feedback delay L, and the feedback attenu-
ation factor 0 , r , 1. Later the components of this type of filter are examined in detail. 
For now, consider the question of developing an expression for the overall transfer func-
tion H(z) 5 Y(z)yX(z) of the plucked-string filter. The block with input e(k) and output 
w(k) is a first-order lowpass filter with transfer function
F(z) 5
D  W(z)
E(z)
 
5 1 1 z21
2
 
 (7.1.1)
Recall from (5.4.7) that the block with input w(k) and output y(k) is a first-order all-
pass filter. Allpass filters pass all frequencies equally because they have flat magnitude 
responses. The purpose of an allpass filter is to change the phase of the input and thereby 
Figure 7.1: A  
Tunable Plucked-
string Filter
1
c 1 z21
1 1 cz21
rLz2L
1 1 z21
2

y
e
x
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.1 Motivation    513
introduce some delay. The transfer function of the allpass filter in Figure 7.1 is as follows, 
where 0 , c , 1 is the pitch parameter.
G(z) 5
D  Y(z)
W(z)
 
5 c 1 z21
1 1 cz21 
 (7.1.2)
The key to finding the overall transfer function is to compute the Z-transform of the 
summing junction output, E(z). From (7.1.1), (7.1.2), and Figure 7.1
 E(z) 5 X(z) 1 rLz2LY(z)
 5 X(z) 1 rLz2LG(z)W(z)
 
 5 X(z) 1 rLz2LG(z)F(z)E(z) 
 (7.1.3)
Solving (7.1.3) for E(z) yields
 
E(z) 5
X(z)
1 2 rLz2LG(z)F(z) 
 (7.1.4)
From (7.1.1) and (7.1.2), the Z-transform of the output can be expressed as
Y(z) 5 G(z)W(z)
5 G(z)F(z)E(z)
 
5
G(z)F(z)X(z)
1 2 rLz2LG(z)F(z) 
 (7.1.5)
It follows that the overall transfer function of the tunable plucked-string filter is
H(z) 5 Y(z)
X(z)
 
 5
G(z)F(z)
1 2 rLz2LG(z)F(z) 
 (7.1.6)
To verify that this is an IIR filter, substitute the expressions for F(z) and G(z), from (7.1.1) 
and (7.1.2), respectively, into (7.1.6), which yields
 
H(z) 5
1
c 1 z21
1 1 cz2121
1 1 z21
2 2
1 2 rLz2L1
c 1 z21
1 1 cz2121
1 1 z21
2 2
 
 (7.1.7)
Multiplying the top and bottom of (7.1.7) by (1 1 cz21), and combining terms then leads 
to the following simplified transfer function for the tunable plucked-string filter.
 
H(z) 5
.5fc 1 (1 1 c)z21 1 z22g
1 1 cz21 2 .5rLfcz2L 1 (1 1 c)z2(L11) 1 z2(L12)g  
 (7.1.8)
Since the denominator polynomial satisfies a(z) Þ 1, this is indeed an IIR filter. The 
plucked-string sound is generated by the filter output when the input is an impulse or a 
short burst of white noise.
Plucked-string filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

514    Chapter 7  IIR Filter Design
The frequency response of the plucked-string filter consists of a series of peaks or 
resonances that decay gradually depending on the value of the feedback attenuation 
parameter r. To tune the first resonant frequency, the parameters L and c are used. Sup-
pose the sampling frequency is fs and the desired value for the first resonance, or pitch, is 
F0. Then L and c can be computed as follows (Jaffe and Smith, 1983).
 
L 5
 floor 1
fs 2 .5F0
F0 2 
 (7.1.9a)
 
  5 fs 2 (L 1 .5)F0
F0
 
 (7.1.9b)
 
c  5 1 2 
1 1 
 
 (7.1.9c)
To make the discussion specific, suppose the sampling frequency is fs 5 44.1 kHz, 
a value commonly used in digital recording. Next, suppose the desired location of the 
first resonance is F0 5 740 Hz. Applying (7.1.9) then yields L 5 59 and c 5 .8272. If the 
feedback attenuation factor is set to r 5 .999, then this results in the plucked-string filter 
magnitude response shown in Figure 7.2. Interestingly enough, the resonant frequencies 
are almost, but not quite, harmonically related (see Steiglitz, 1996). When Figure 7.2 
is generated by running fig7_2 from g_dsp, the sound made by the filter output is also 
played on the PC speakers. Give it a try, and let your ears be the judge!
7.1.2 Colored Noise
As a second example of an application of IIR filters, consider the problem of creating 
a test signal with desired spectral characteristics. One of the most popular test signals is 
white noise because it contains power at all frequencies. In particular, an N-point white 
noise signal v(k) with average power Pv has the following power-density spectrum.
 
Sv( f ) < Pv, 0 # u f  u ,  fs
2 
 (7.1.10)
Pitch
Figure 7.2:  
Magnitude 
Response of 
Plucked-string 
Filter: L 5 59, 
c 5 .8272, r 5 .999 
0
5
10
15
20
25
0
2
4
6
8
10
12
14
16
18
f (kHz)
A(f)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.1 Motivation    515
The term “white” arises from the fact x(k) contains power at all frequencies just as 
white light is composed of all colors. If the natural frequencies of a linear system are 
confined to an interval fF0, F1g, then it is more efficient to excite these natural modes with 
a signal that has its power restricted to the frequency range fF0, F1g. Since only a subset 
of the entire range of frequencies is represented, this type of signal is sometimes referred 
to as colored noise. For example, low-frequency noise might be thought of as red and 
high-frequency noise as blue. The desired power density spectrum for colored noise in the 
interval fF0, F1g is
 
Sx( f ) 5 Pxfua_u f  u 2 F0+ 2 a_u f  u 2 F1+g 
 (7.1.11)
A simple scheme for generating colored noise with a desired power density spectrum 
is shown in Figure 7.3. The basic approach is to start with white noise, which is easily 
constructed, and then pass it through a digital filter that removes the undesired spec-
tral components. The digital filter can be either an IIR filter or an FIR filter. However, 
because a linear phase response is not crucial, the colored noise can be generated more 
efficiently using an IIR filter.
The ideal filter is a bandpass filter with a low-frequency cutoff of F0, a high- 
frequency cutoff of F1, and a passband gain of one as shown in Figure 7.4. The removal 
of signal power below F0 Hz and above F1 Hz means that the average power of the col-
ored noise x(k) will be the following fraction of the average power of the white noise v(k).
 
Px 5 2(F1 2 F0)Pv
fs
 
 (7.1.12)
Colored noise
Figure 7.4:  
Magnitude 
Response of an 
Ideal Bandpass 
Filter
0
0
1
2
f (Hz)
A(f)
F0
F1
fs/2 
Figure 7.3:  
Generation of 
Colored Noise x(k) 
from White Noise 
v(k)
x
y
IIR
filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

516    Chapter 7  IIR Filter Design
To make the example specific, suppose the sampling frequency is fs 5 800 Hz, and 
the desired frequency band for the colored noise is fF0, F1g 5 f150, 300g Hz. Let v(k) 
consist of N 5 2048 samples of white noise uniformly distributed over f21, 1g. Using a 
design technique covered later in the chapter, a tenth-order elliptic bandpass filter can be 
constructed. When the white noise is passed through this IIR filter, this results in colored 
noise with the power-density spectrum shown in Figure 7.5. It is evident that there is still 
a small amount of power outside the desired range f150, 300g Hz. This is due to the fact 
that practical filters, unlike ideal filters, have a transition band between the passband and 
the stopband. The width of the transition band can be reduced by going to a higher-order 
filter, but it cannot be eliminated completely.
Filter Design by Pole-zero Placement
In addition to the basic frequency-selective filters, there are a number of specialized filters 
that arise in applications. A direct design procedure is available for these filters based on 
gain matching and a careful placement of poles and zeros.
7.2.1 Resonator
Recall that a bandpass filter is a filter that passes signals whose frequencies lie within an 
interval fF0, F1g. When the width of the passband is small in comparison with fs, we say 
that the filter is a narrowband filter. An important limiting special case of a narrowband 
filter is a filter designed to pass a single frequency 0 , F0 , fsy2. Such a filter is called 
a resonator with a resonant frequency of F0. Thus the frequency response of an ideal 
resonator is
 
Hres( f ) 5 1( f 2 F0), 0 # f # fsy2 
 (7.2.1)
7.2
Resonator
Figure 7.5: Power-
density Spectrum 
of Colored Noise 
Created by 
Sending White 
Noise through a 
Bandpass Filter 
with fF0, F1g 5
f150, 300g Hz
0
50
100
150
200
250
300
350
400
0
0.1
0.2
0.3
0.4
0.5
0.6
0.8
0.9
0.7
1
f (Hz)
Sx(f)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.2 Filter Design by Pole-zero Placement    517
Here 1( f ) denotes the unit pulse (k) but with the integer argument k replaced by a real 
argument f . That is,
 
1( f ) 5
D  5
1,
f 5 0
0, f Þ 0
 
 (7.2.2)
A resonator can be used to extract a single frequency component, or a very narrow 
range of frequencies, from a signal. A simple way to design a resonator is to place a pole 
near the point on the unit circle that corresponds to the resonant frequency F0. Recall 
that as the angles of the points along the top half of the unit circle go from 0 to , the 
frequency f  ranges from 0 to fsy2. Thus the angle corresponding to frequency F0 is
 
0 5 2F0
fs
 
 (7.2.3)
The radius of the pole must be less than one for the filter transfer function, Hres(z), to be 
stable. Furthermore, if the coefficients of the denominator of Hres(z) are to be real, then 
complex poles must occur in conjugate pairs. One can ensure that the resonator com-
pletely attenuates the two end frequencies, f 5 0 and f 5 fsy2, by placing zeros at z 5 1 
and z 5 21, respectively. These constraints yield a resonator transfer function with the 
following factored form.
 
Hres(z) 5
b0(z 2 1)(z 1 1)
fz 2 r exp(j0)gfz 2 r exp(2j0)g 
 (7.2.4)
Using Euler’s identity from Appendix 2, the resonator transfer function can be simplified 
and expressed as a ratio of two polynomials.
Hres(z) 5
b0(z2 2 1)
z2 2 rfexp( j0) 1 exp(2j0)gz 1 r2
 5
b0(z2 2 1)
z2 2 r2 Re hexp( j0)jz 1 r2
 
 5
b0(z2 2 1)
z2 2 2r cos(0)z 1 r2
 
 (7.2.5)
There are two design parameters that remain to be determined, the pole radius r and 
the gain factor b0. To achieve a sharp filter that is highly selective one needs r < 1, but for 
stability it is essential that r , 1. Suppose DF denotes the radius of the 3 dB passband of 
the filter. Thus uHres( f )u $ 1yÏ2 for f  in the range fF0 2 DF2, F0 1 DF2g. For a narrow-
band filter DF V fs. In this case the following approximation, derived in Section 5.6, can 
be used to estimate the pole radius.
 
r < 1 2 DF
fs
 
 (7.2.6)
The gain factor b0 in the numerator of Hres(z) is inserted to ensure that the pass-
band gain is one. The value of z corresponding to the nominal center of the passband 
is z0 5 exp(  j2F0T ). Setting uH(z0)u 5 1 in (7.2.5) and solving for b0 yields the following 
expression for the gain factor.
 
b0 5 uexp( j20) 2 2r cos(0) exp( j0) 1 r2u
uexp( j20) 2 1u
 
 (7.2.7)
Angle of F0
Pole radius
Gain factor
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

518    Chapter 7  IIR Filter Design
The resonator transfer function, in terms of negative powers of z, is then
 
Hres(z) 5
b0(1 2 z22)
1 2 2r cos(0)z21 1 r2z22  
 (7.2.8)
Resonator Filter
EXAMPLE 7.1
Suppose the sampling frequency is fs 5 1200 Hz, and it is desired to design a 
resonator to meet the following specifications.
F0  5 200  Hz 
DF 5 6  Hz 
From (7.2.3), the pole angle is
0 5 2(200)
1200
 5 
3
Clearly DF V fs in this case. Thus, from (7.2.6) the pole radius is
r 5 1 2 6
1200
 5 .9843
Next, from (7.2.7) the scalar multiplier b0 required for a passband gain of one is
b0 5 uexp( j2y3) 2 2(.9843) cos(y3) exp( jy3) 1 (.9843)2u
uexp( j2y3) 2 1u
 5 .0156
Finally, from (7.2.8) the transfer function of the resonator is
Hres(z) 5
.0156(1 2 z22)
1 2 2(.9843) cos(y3)z21 1 (.9843)2z22
 5
.0156(1 2 z22)
1 2 .9843z21 1 .9688z22
A plot of the poles and zeros of the resonator, obtained by running exam7_1, 
is shown in Figure 7.6. The complex-conjugate pair of poles just inside the unit 
circle causes the magnitude response of the resonator to peak near f 5 F0. This is 
evident from the plot of the resonator magnitude response shown in Figure 7.7.
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.2 Filter Design by Pole-zero Placement    519
Figure 7.6: Poles and Zeros of a Resonator 
22
21 20.5
21.5
0
1
0.5
1.5
2
22
21.5
21
20.5
0
0.5
1
1.5
2
X
Re(z)
Im(z)
X
Figure 7.7: Magnitude Response of a Resonator with F0 5 200 Hz
0
100
200
300
400
500
600
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
f (Hz)
A(f)
7.2.2 Notch Filter
Another specialized filter that occurs in applications is the notch filter. A notch filter can 
be thought of as a limiting special case of a bandstop filter where the width of the stop-
band goes to zero. That is, a notch filter is a filter designed to remove a single frequency 
F0 called the notch frequency. The frequency response of an ideal notch filter is
 
Hnotch( f ) 5 1 2 1( f 2 F0), 0 # f # fsy2 
 (7.2.9)
Notch filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

520    Chapter 7  IIR Filter Design
To design a notch filter, we start by placing a zero at the point on the unit circle that 
corresponds to the notch frequency F0. The angle, 0, associated with frequency F0 was 
given previously in (7.2.3). Placing a zero at z0 5 exp( j2F0T ) ensures that Hnotch(F0) 5 0 
as desired. However, it does not leave us with a design parameter to control the 3-dB 
bandwidth of the stopband. This is achieved by placing a pole at the same angle 0 but 
just inside the unit circle with r , 1. Since the poles and zeros must occur in complex- 
conjugate pairs (for real coefficients) this yields a transfer function for a notch filter with 
the following factored form.
 
Hnotch(z) 5 b0fz 2 exp( j0)gfz 2 exp(2j0)g
fz 2 r exp( j0)gfz 2 r exp(2j0)g 
 (7.2.10)
Using Euler’s identity, the transfer function in (7.2.10) again can be simplified and 
expressed as a ratio of two polynomials with the final result being
 
Hnotch(z) 5 b0(z2 2 2 cos(0)z 1 1)
z2 2 2r cos(0)z 1 r2  
 (7.2.11)
There are two design parameters to be specified, the pole radius r, and the gain factor 
b0. Just as with the resonator, to achieve a sharp filter that is highly selective requires r < 1, 
but for stability and to avoid canceling the zero it essential that r , 1. If DF denotes the 
radius of the 3-dB stopband of the filter, and if DF V fs, then the approximation for r in 
(7.2.6) can be used.
The gain factor b0 in the numerator of Hnotch(z) is inserted to ensure that the pass-
band gain is one. The passband includes both f 5 0 and f 5 fsy2. To set the DC gain to 
one, set uHnotch(1)u 5 1 in (7.2.11) and solve for b0, which yields
 
b0 5 u1 2 2r cos(0) 1 r2u
2u1 2  cos(0)u
 
 (7.2.12)
Alternatively, the high-frequency gain can be set to one using uHnotch(21)u 5 1. The notch 
filter transfer function, in terms of negative powers of z, is then
 
Hnotch(z) 5 b0f1 2 2 cos(0)z21 1 z22g
1 2 2r cos(0)z21 1 r2z22  
 (7.2.13)
Notch Filter
EXAMPLE 7.2
Suppose the sampling frequency is fs 5 2400 Hz, and it is desired to design a 
notch filter to meet the following specifications.
F0  5 800  Hz 
DF 5 18  Hz 
From (7.2.3), the angle of the zero is
0 5 2(800)
2400
 5 2
3
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.2 Filter Design by Pole-zero Placement    521
22
21 20.5
21.5
0
1
0.5
1.5
2
22
21.5
21
20.5
0
0.5
1
1.5
2
XO
XO
Re(z)
Im(z)
Figure 7.8: Poles and Zeros of a Notch Filter
In this case DF V fs. Thus from (7.2.6) the pole radius is
r 5 1 2 18
2400
 5 .9764
Next, from (7.2.12) the scalar multiplier b0 required for a DC passband gain of 
one is
b0 5 u1 2 2(.9764) cos(2y3) 1 (.9764)2u
2u1 2  cos(2y3)u
 5 .9766
Finally, from (7.2.13) the transfer function of the notch filter is
Hnotch(z) 5
.9766f1 2 2 cos(2y3)z21 1 z22g
1 2 2(.9764) cos(2y3)z21 1 (.9764)2z22
 5
.9766(1 1 z21 1 z22)
1 1 .9764z21 1 .9534z22
A plot of the poles and zeros of the notch filter, obtained by running exam7_2, is 
shown in Figure 7.8. Note how the poles “almost’’ cancel the zeros. The complex- 
conjugate pair of zeros on the unit circle cause the magnitude response of the 
notch filter to go to zero at f 5 F0. This is apparent from the plot of the notch 
filter magnitude response shown in Figure 7.9.
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

522    Chapter 7  IIR Filter Design
7.2.3 Comb Filters
Another specialized filter, one that includes a resonator as a special case, is the comb filter. 
A comb filter is a narrowband filter that has several equally spaced passbands starting 
at f 5 0. In the limit as the widths of the passbands go to zero, the comb filter is a filter 
that passes DC, a fundamental frequency F0, and several of its harmonics. Thus an ideal 
comb filter of order n has the following frequency response where F0 5 fsyn.
 
Hcomb( f ) 5 o
  floor (ny2)
i50
1( f 2 iF0), 0 # f # fsy2 
 (7.2.14)
Note that if n is even, then floor(ny2) 5 ny2. Consequently, for a comb filter of even 
order there are ny2 1 1 resonant frequencies in the range f0, fsy2g, and for a comb filter 
of odd order, there are only (n 2 1)y2 1 1 resonant frequencies. Odd order comb filters 
do not have a resonant frequency at f 5 fsy2. Since the resonant frequencies are equally 
spaced, a comb filter of order n has a very simple transfer function, namely,
 
Hcomb(z) 5
b0
1 2 rnz2n  
 (7.2.15)
Thus Hcomb(z) has n zeros at the origin, and the poles of Hcomb(z) correspond to the n 
roots of rn. That is, the poles are equally spaced around a circle of radius r , 1. The dis-
tribution of poles for the even case n 5 10 and r 5 .9843 is shown in Figure 7.10.
To achieve a highly selective comb filter one needs r < 1 using (7.2.6), but for stabil-
ity it is necessary that r , 1. The gain factor can be selected such that the passband gain 
at DC is one. Setting uHcomb(1)u 5 1 in (7.2.15) and solving for b0 yields
 
b0 5 1 2 rn  
 (7.2.16)
Comb filter
Figure 7.9: Magnitude Response of a Notch Filter with F0 5 800 Hz
0
200
400
600
800
1000
1200
0
0.2
0.4
0.6
0.8
1
1.2
1.4
f (Hz)
A(f)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.2 Filter Design by Pole-zero Placement    523
Figure 7.10: Poles 
and Zeros of a 
Comb Filter of 
Order n 5 10
22
21 20.5
21.5
0
1
1.5
0.5
2
22
21.5
21
20.5
0
0.5
1
1.5
2
X
X
X
X
X
X
X
X
X
X
O
Re(z)
Im(z)
Figure 7.11:  
Magnitude 
Response of a 
Comb Filter with 
n 5 10, fs 5 200 Hz,  
and DF 5 1 Hz
0
20
30
10
40
50
60
70
80
90
100
0
0.2
0.4
0.6
0.8
1
f (Hz)
A(f)
A plot of the magnitude response of the comb filter in Figure 7.10, corresponding 
to the case n 5 10, is shown in Figure 7.11. Here the sampling frequency was selected to 
be fs 5 200 Hz, and the 3 dB radius was DF 5 1 Hz. A comb filter might be used, for 
example, to extract a periodic signal from noise. It provides an alternative approach to 
the correlation method discussed in Example 4.15.
Just as the comb filter is a generalization of the resonator, there is a generalization 
of the notch filter called an inverse comb filter that eliminates DC, the fundamental notch 
frequency F0, and several of its harmonics. An ideal inverse comb filter of order n has the 
following frequency response where F0 5 fsyn.
 
Hinv( f ) 5 1 2 o
  floor (ny2)
i50
1( f 2 iF0), 0 # f # fsy2 
 (7.2.17)
Inverse comb filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

524    Chapter 7  IIR Filter Design
In addition to having zeros equally spaced around the unit circle, the inverse comb filter 
also has equally spaced poles just inside the unit circle. Thus the transfer function of an 
inverse comb filter of order n has the following form.
 
Hinv(z) 5 b0(1 2 z2n)
1 2 rnz2n
 
 (7.2.18)
The distribution of poles and zeros for the odd-order case, n 5 11 and r 5 .9857, is shown 
in Figure 7.12. Note how there are no poles or zeros at z 5 21 because n is odd.
To pass the frequencies between the harmonics of F0 one needs r < 1 using (7.2.6), 
but for stability and to avoid pole-zero cancellation, it is necessary that r , 1. The gain 
factor can be selected such that the passband gain at f 5 F0y2 is one where F0 5 fsyn. 
The point on the unit circle corresponding to the middle of the first passband is 
z1 5 exp( jyn). Setting uHinv(z1)u 5 1 in (7.2.18) and solving for b0 yields
 
b0 5 1 1 rn
2
 
 (7.2.19)
A plot of the magnitude response of the inverse comb filter in Figure 7.12, corre-
sponding to the case n 5 11, is shown in Figure 7.13. Here the sampling frequency was 
selected to be fs 5 2200, and the 3-dB radius was DF 5 10 Hz.
There are a number of applications of comb filters and inverse comb filters. For 
example, suppose the input signal is a noise-corrupted periodic signal with a known fun-
damental frequency of F0. Let Hcomb(z) be a comb filter of order n, and suppose the 
sampling frequency satisfies
 
fs 5 nF0 
 (7.2.20)
The first resonant frequency of the comb filter is then F0. Consequently, the comb filter 
can be used to extract the first ny2 harmonics of the periodic component of the input 
signal in this case.
Figure 7.12: Poles 
and Zeros of an 
Inverse Comb 
Filter of Order 
n 5 11
22
21 20.5
21.5
0
1
1.5
0.5
2
22
21.5
21
20.5
0
0.5
1
1.5
2
XO
XO
XO
XO
XO
XO
XO
XO
XO
XO
XO
Re(z)
Im(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.2 Filter Design by Pole-zero Placement    525
Another application arises when an input signal includes a periodic noise component 
that needs to be removed. For example, a sensitive acoustic or a biomedical measurement 
may be corrupted by the “hum’’ of 60 Hz AC overhead fluorescent lights. In this case an 
inverse comb filter can be used to remove this periodic noise.
Figure 7.13:  
Magnitude 
Response of an 
Inverse Comb 
Filter with n 5 11,  
fs 5 2200 Hz, and 
DF 5 10 Hz
0
200 300
100
400 500 600 700 800 900 10001100
0
0.2
0.4
0.6
0.8
1
f (Hz)
A(f)
The DSP Companion contains the following functions for designing IIR filters by 
gain matching and pole-zero placement.
% F_IIRRES: 
Design an IIR resonator filter
% F_IIRNOTCH: Design an IIR notch filter
% F_IIRCOMB: 
Design an IIR comb filter
% F_IIRINV: 
Design an IIR inverse comb filter
%
% Usage:
% 
[b,a] = f_iirres 
(F0,DeltaF,fs)
% 
[b,a] = f_iirnotch (F0,DeltaF,fs)
% 
[b,a] = f_iircomb (n,DeltaF,fs)
% 
[b,a] = f_iirinv 
(n,DeltaF,fs)
% Pre:
% 
F0 
= resonant or notch frequency
% 
DeltaF = 3dB radius
% 
fs 
= sampling frequency
% 
n 
= filter order
% Post:
% 
b = 1 by (n+1) numerator coefficient vector
% 
a = 1 by (n+1) denominator coefficient vector
DSP Companion
DsP Companion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

526    Chapter 7  IIR Filter Design
Filter Design Parameters 
The most widely used design procedure for digital IIR filters starts with a normalized 
lowpass analog filter called a prototype filter. One then transforms the prototype filter 
into a desired frequency-selective digital filter. There are four classical families of analog 
lowpass filters that are typically used as prototype filters, and each family is optimal in 
some sense. Before examining them, it is helpful to introduce two filter design parameters 
that are derived from the filter design specifications shown in Figure 7.14. Note that for 
IIR filters the upper bound on the magnitude response in the passband is one rather 
than 1 1 p. Recall that Fp is the passband cutoff frequency, Fs is the stopband cutoff 
frequency, p is the passband ripple, and s is the stopband attenuation. Left unspecified 
is the magnitude response in the transition band, whose width is B 5 Fs 2 Fp. Let Aa( f ) 
denote the desired analog magnitude response. The passband and stopband specifica-
tions for an analog filter can be represented by separate inequalities as follows.
 
1 2 p # Aa( f ) # 1, 0 # u f u # Fp 
 (7.3.1a)
 
0 # Aa( f ) # s, Fs # u f u , ` 
 (7.3.1b)
The design specifications in (7.3.1) are linear design specifications. In order to better 
reveal the amount of attenuation in the stopband, the magnitude response is sometimes 
plotted in units of dB using the logarithmic scale, A( f ) 5 20 log10{uH( f )u}. The logarith-
mic equivalents of the passband ripple and stopband attenuation are
 
Ap 5 220 log10(1 2 p) 
 (7.3.2a)
 
As 5 220 log10(s)
 
 (7.3.2b)
For example, a stop attenuation of s 5 .01 corresponds to As 5 40 dB, and each reduc-
tion by a factor of ten generates an increase of 20 dB. The design procedures for the 
classical analog filters are based on linear specifications. However, if the user instead 
7.3
Prototype filter
Figure 7.14:  
Design 
Specifications of 
an IIR Lowpass 
Filter
1
 1 2 dp
ds
0
0
Fp
Fs
fs/2
Passband
Stopband
f (Hz)
A(f)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.3 Filter Design Parameters     527
starts out with logarithmic specifications, they can be converted to equivalent linear spec-
ifications as follows.
 
p 5 1 2 102Apy20 
 (7.3.3a)
 
s 5 102Asy20
 
 (7.3.3b)
The development of design formulas for the classical analog filters can be stream-
lined by introducing the following two filter design parameters that are obtained from the 
filter design specifications (Porat, 1997).
 
r 5
D  
Fp
Fs
    
 (7.3.4a)
 
d 5
D  3
(1 2 p)22 2 1
22
s
2 1 4
1y2
 
 (7.3.4b)
The first parameter 0 , r , 1 is called the selectivity factor. Note that for an ideal filter, 
there is no transition band so Fs 5 Fp. Consequently, for an ideal filter, the selectivity 
factor is r 5 1, whereas for a practical filter, r , 1.
The second parameter, d . 0, is called the discrimination factor. Observe that when 
p 5 0, the numerator in (7.4.4b) goes to zero, so d 5 0. Similarly when s 5 0, the denom-
inator in (7.4.4b) goes to infinity so again d 5 0. Hence for an ideal filter, the discrimina-
tion factor is d 5 0, whereas as for a practical filter d . 0. See Table 7.1 for a summary of 
the filter design parameter characteristics.
Selectivity factor
Discrimination factor
Filter Type 
selectivity Factor
Discrimination Factor 
Ideal
r 5 1 
d 5 0 
Practical 
r , 1
d . 0 
Table 7.1: Filter 
Design Parameters
Filter Design Parameters
EXAMPLE 7.3
As a simple illustration of filter design parameters, consider the problem of design-
ing a lowpass analog filter to meet the following logarithmic design specifications.
(Fp, Fs)  5 (400, 500) Hz
(Ap, As) 5 (.5, 35) dB
First convert from logarithmic to linear specifications. From (7.3.3a), the required 
passband ripple is
p 5 1 2 102.5y20
 5 .0559
Similarly, from (7.3.3b), the required stopband attenuation is
s 5 10235y20
 5 .0178
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

528    Chapter 7  IIR Filter Design
Classical analog filters can be designed by starting with a desired magnitude response 
and working backwards to determine the poles, zeros, and gain. To apply this reverse pro-
cedure, it is necessary to first develop a relationship between an analog transfer function 
Ha(s) and the square of its magnitude response. Recall that the frequency response of an 
analog filter is defined as follows.
 
Ha( f ) 5 Ha(s)us5j2f 
 (7.3.5)
The frequency response Ha( f ) expressed in polar form is Ha( f ) 5 Aa( f ) expf ja( f )g 
where Aa( f ) is the magnitude response, and a( f ) is the phase response. Since the coeffi-
cients of Ha(s) are real, the square of the magnitude response can be expressed as follows 
where H*a(s) denotes the complex conjugate of Ha(s).
A2
a( f ) 5 uHa( f )u2
 5 uHa(s)u2
s5j2f
 5 hHa(s)H *a(s)jus5j2f
 
 5 hHa(s)Ha(2s)jus5j2f 
 (7.3.6)
Instead of replacing s by j2f  on the right-hand side of (7.3.6), one can replace f  by 
sy( j2) on the left-hand side. This yields the following fundamental relationship between 
the transfer function and its squared magnitude response.
 
Ha(s)Ha(2s) 5 A2
a1
s
j22  
 (7.3.7)
Each of the classical analog filters can be characterized by its squared magnitude response. 
The relationship in (7.3.7) then can be employed to synthesize the filter transfer function.
Classical Analog Filters 
The most popular design procedure for digital IIR filters is to start with a normalized 
lowpass analog filter and then transform it into an equivalent frequency-selective digital 
filter.
Squared magnitude 
response
7.4
For this filter, the width of the transition band is B 5 100 Hz. Thus the selectivity 
factor r is less than one, and from (7.4.4a) we have
r 5 .8
Finally, both the passband ripple and the stopband attenuation are positive. Thus 
the discrimination factor d will also be positive. From (7.4.4b)
d 53
(1 2 .0559)22 2 1
(.0178)22 2 1 4
1y2
 5 .006213
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.4 Classical Analog Filters     529
7.4.1 Butterworth Filters
The first family of analog filters that we consider are the Butterworth filters. A Butter-
worth filter of order n is a lowpass analog filter with the following squared magnitude 
response.
 
A2
a( f ) 5
1
1 1 ( fyFc)2n  
 (7.4.1)
Notice from (7.4.1) that A2
a(Fc) 5 .5. Frequency Fc is called the 3-dB cutoff frequency 
because
 
20 log10{Aa(Fc)} < 23  dB  
 (7.4.2)
A plot of the squared magnitude response for a Butterworth filter of order n 5 4 with a 
3-dB cutoff frequency of Fc 5 1 Hz is shown in Figure 7.15.
The poles of Ha(s) can be recovered from the squared magnitude response. Using 
(7.4.1) and the relationship in (7.3.7),
Ha(s)Ha(2s) 5 A2
a 1
s
 j22
 5
1
1 1 fsy( j2Fc)g2n
 5
( j2Fc)2n
s2n 1 ( j2Fc)2n
 
 5
(21)n(2Fc)2n
s2n 1 (21)n(2Fc)2n 
 (7.4.3)
3-dB cutoff
Figure 7.15:  
Squared 
Magnitude 
Response of 
a Lowpass 
Butterworth Filter 
of Order n 5 4 
with Fc 5 1 Hz. 
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
f (Hz)
A2(f)
a
(1 2 p)2
Fp
Fs
2s
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

530    Chapter 7  IIR Filter Design
Thus the poles, pk, of Ha(s)Ha(2s) lie on a circle of radius 2Fc at angles k where
 
k 5 (2k 1 1 1 n)
2n
, 0 # k , 2n 
 (7.4.4a)
 
pk 5 2Fc exp( jk),   0 # k , 2n 
 (7.4.4b)
A normalized lowpass filter is a lowpass filter whose cutoff frequency is Fc 5 1y(2) Hz, 
which corresponds to a radian cutoff frequency of Vc 5 1 rad/s. For a normalized lowpass 
Butterworth filter, the poles are equally spaced around the unit circle with a separation 
of yn radians. Two cases are illustrated in Figure 7.16, corresponding to an odd order 
(n 5 5) and an even order (n 5 6). Note that in either case the first n poles all lie in the 
left half of the complex plane. One associates the left-half plane poles hp0, p1, Á , pn21j 
with Ha(s) in (7.4.3) and the right-half plane poles hpn, pn 1 1, Á , p2n21j with Ha(2s). 
This way, filter Ha(s) is guaranteed to be stable. The transfer function of an nth order 
lowpass Butterworth filter with cutoff frequency Fc is then
 
Ha(s) 5
(2Fc)n
(s 2 p0)(s 2 p1) Á (s 2 pn21)
 
 (7.4.5)
Butterworth filters have a number of useful qualitative properties. For example, notice 
that the magnitude response decreases monotonically starting from Aa(0) 5 1. For high 
frequencies, the asymptotic attenuation of an nth order filter is 20n dB per decade. That is,
 
20 log10{Aa(10f  )} < 20 log10{Aa( f )} 2 20n dB , f W Fc 
 (7.4.6)
Perhaps the most noteworthy property of Butterworth filters is that the first 2n 2 1 deriv-
atives of the squared magnitude response are zero at f 5 0. Consequently, among filters 
of order n, the flat Butterworth filter magnitude response is as flat as possible at f 5 0. 
For this reason, Butterworth filters are called maximally flat filters.
The two design parameters available with Butterworth filters are the filter order n 
and the cutoff frequency Fc. Suppose it is desired to develop a lowpass Butterworth filter 
satisfying the linear design specification in Figure 7.15. Then from (7.4.1), the passband 
and stopband specification constraints are
 
1
1 1 (FpyFc)2n 5 (1 2 p)2 
 (7.4.7a)
 
1
1 1 (FsyFc)2n 5 2
s
 
 (7.4.7b)
Normalized lowpass 
filter
Maximally flat
Figure 7.16: Poles 
of Normalized 
Lowpass 
Butterworth Filters
21
0
1
21
0
1
n 5 5
Re(s)
Im(s)
X
X
X
X
X
21
0
1
21
0
1
n 5 6
Re(s)
Im(s)
X
X
X
X
X
X
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.4 Classical Analog Filters     531
The passband constraint in (7.4.7a) and the stopband constraint in (7.4.7b) can each be 
solved for F 2n
c . By setting these two expressions for F 2n
c  equal, one can eliminate the cutoff 
frequency parameter, Fc. Solving the resulting equation for the filter order n and using the 
design parameters in (7.3.4) then yields
 
n 5
 ceil 3
 ln (d)
 ln (r)4  
 (7.4.8)
Thus the required filter order can be expressed directly in terms of the discrimination 
factor d and the selectivity factor r defined in (7.3.4). The function ceil in (7.4.8) is used 
because the expression in the square brackets may not be an integer. The ceil function 
rounds up to the next integer value. Since n is rounded up, typically this means that the 
passband and the stopband specifications will be exceeded (more than met). To meet the 
passband constraint exactly, one can solve (7.4.7a) for Fc, which yields
 
Fcp 5
Fp
f(1 2 p)22 2 1g1y(2n) 
 (7.4.9)
In this case, the stopband constraint is exceeded. Similarly, to meet the stopband con-
straint exactly, one solves (7.4.7b) for Fc, which yields the slightly simpler expression
 
Fcs 5
Fs
(22
s
2 1)1y(2n) 
 (7.4.10)
In this case the passband constraint is exceeded. Finally, both constraints can be exceeded 
(assuming the expression for n is not already an integer) when the cutoff frequency is set 
to the average.
 
Fc 5
Fcp 1 Fcs
2
 
 (7.4.11)
The design formulas in (7.4.8) through (7.4.11) are all based on the linear design 
specifications in Figure 7.15. If the logarithmic design specifications are used instead, 
then (7.3.3) should be applied first to convert Ap and As into p and s, respectively.
Butterworth Filter
EXAMPLE 7.4
As an illustration of the use of the design formulas, consider the problem of 
designing a lowpass Butterworth filter to meet the following linear design 
specifications.
Fp 5 1000  Hz 
Fs 5 2000  Hz 
p  5 .05
s  5 .05
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

532    Chapter 7  IIR Filter Design
Butterworth filter transfer functions can be designed directly using (7.4.4) and (7.4.5). 
There is also an alternative table-based approach that works well for low-order filters. It 
starts with a normalized lowpass Butterworth filter and then makes use of a simple fre-
quency transformation. Let H
 norm  denote the transfer function of a normalized nth-order 
Butterworth lowpass filter, a filter with a 3 dB radian cutoff frequency of Vc 5 1 rad/s.
 
H
 norm (s) 5
an
sn 1 a1sn 2 1 1 Á 1 an
 
 (7.4.12)
The coefficients of the denominator polynomials for the first few normalized Butter-
worth lowpass filters are summarized in Table 7.2.
Next let Fc denote the desired 3-dB cutoff frequency in Hz. The transfer function 
Ha(s) can be obtained by replacing s in (7.4.12) with syVc where Vc 5 2Fc. Thus if a(s) 
is as given in Table 7.2, then an nth-order lowpass Butterworth filter with radian cutoff 
frequency Vc has the transfer function Ha(s) 5 H
 norm (syVc) or
 
Ha(s) 5
Vn
can
sn 1 Vca1sn21 1 Á 1 Vn
can
 
 (7.4.13)
Lowpass Butterworth
From (7.3.4), the selectivity and discrimination factors are
r 5 .5
d 51
.9522 2 1
.0522 2 12
1y2
 5 .0165
Thus from (7.4.8) the minimum filter order is
n 5
 ceil 3
 ln (.0165)
 ln (.5) 4
 5
 ceil (5.9253)
 5 6
Next, from (7.4.9), the cutoff frequency for which the passband specification is 
met exactly is
Fcp 5
1000
(.9522 2 1)1y12
 5 1203.8  Hz 
Similarly, from (7.4.10), the cutoff frequency for which the stopband specification 
is met exactly is
Fcs 5
2000
(.0522 2 1)1y12
 5 1209.0  Hz 
Any cutoff frequency in the range Fcp # Fc # Fcs will meet or exceed both specifica-
tions. For example, Fc 5 1206 Hz will suffice. Using (7.3.2), the equivalent logarith-
mic passband and stopband specifications are Ap 5 .4455 dB and As 5 26.02 dB.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.4 Classical Analog Filters     533
The replacement of s with syVc is an example of a frequency transformation that 
maps a normalized lowpass filter into a general lowpass filter. In Section 7.6, other exam-
ples of frequency transformations are presented that convert normalized lowpass filters 
into highpass, bandpass, and bandstop filters.
Table 7.2:  
Denominators of 
Normalized Lowpass 
Butterworth Filters 
with a0 5 1
n
a1
a2
a3
a4
a5
a6
a7
a8
1
1
0
0
0
0
0
0
0
2
1.414214
1
0
0
0
0
0
0
3
2
2
1
0
0
0
0
0
4
2.613126
3.414214
2.613126
1
0
0
0
0
5
3.236068
5.236068
5.236068
3.236068
1
0
0
0
6
3.863703
7.464102
9.14162
7.464102
3.863703
1
0
0
7
4.493959
10.09783
14.59179
14.59179
10.09783
4.493959
1
0
8
5.125831
13.13707
21.84615
25.68836
21.84615
13.13707
5.125831
1
Butterworth Transfer Function
EXAMPLE 7.5
As an illustration of the frequency-transformation method using Table 7.2, con-
sider the problem of designing a transfer function for a third-order lowpass But-
terworth filter with a cutoff frequency of Fc 5 10 Hz. In this case Vc 5 20, and 
from Table 7.2
Ha(s) 5
2.481 3 105
s3 1 125.7s2 1 7896s 1 1.481 3 105
7.4.2 Chebyshev-I Filters
The magnitude responses of Butterworth filters are smooth and flat because of the max-
imally flat property. However, a drawback of the maximally flat property is that the tran-
sition band of a Butterworth filter is not as narrow as it could be. An effective way to 
decrease the width of the transition band is to allow ripples or oscillations in the pass-
band or the stopband. The following Chebyshev-I filter of order n is designed to allow n 
ripples within the passband.
 
A2
a( f ) 5
1
1 1 2T 2
n( fyFp)  
 (7.4.14)
Here n is the filter order, Fp is the passband frequency,  . 0 is a ripple factor parameter, 
and Tn(x) is a polynomial of degree n called a Chebyshev polynomial of the first kind. 
Recall from Section 5.5 that the Chebyshev polynomials can be generated recursively. The 
Ripple factor
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

534    Chapter 7  IIR Filter Design
first two Chebyshev polynomials are T0(x) 5 1 and T1(x) 5 x, and the remaining poly-
nomials are then computed from the previous two according to the recurrence relation
 
Tk11(x) 5 2xTk(x) 1 Tk21(x), k $ 1 
 (7.4.15)
Therefore T2(x) 5 2x2 2 1 and so on. The first few Chebyshev polynomials are summa-
rized in Table 7.3.
The Chebyshev polynomials have many interesting properties. For example, Tn(x) 
is an odd function when n is odd and an even function when n is even. Furthermore, 
Tn(1) 5 1 for all n. For the purpose of filter design, the most important property is that 
Tn(x) oscillates in the interval f21, 1g when uxu # 1, and Tn(x) is monotonic when uxu . 1. 
This oscillation causes the square of the magnitude response of a Chebyshev-I filter to 
have ripples of equal size in the passband and be monotonically decreasing outside of 
the passband. A plot of the squared magnitude response is shown in Figure 7.17 for the 
case n 5 4 with Fp 5 1 Hz. Note that because Tn(1) 5 1, it follows from (7.4.14) that, at 
the edge of the passband,
 
A2
a(Fp) 5
1
1 1 2 
 (7.4.16)
Table 7.3: Chebyshev 
Polynomials of the 
First Kind
n
Tn(x)
0 
1 
1
x 
2
2x2 2 1 
3
4x3 2 3x 
4
8x4 2 8x2 1 1 
5
16x5 2 20x3 1 5x 
6
32x6 2 48x4 1 18x2 2 1 
7
64x7 2 112x5 1 56x3 2 7x 
8
128x8 2 256x6 1 160x4 2 32x2 1 1 
Figure 7.17:  
Squared 
Magnitude 
Responses of 
a Chebyshev-I 
Lowpass Filter of 
Order n 5 4 with 
Fp 5 1 Hz
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
f (Hz)
Fs
2s
1/(1 1 2)
A2(f)
a
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.4 Classical Analog Filters     535
Therefore the ripple factor parameter, , controls the size of the passband ripple of the 
filter. By setting 1y(1 1 2) 5 (1 2 p)2 and solving for , a desired passband ripple, p, 
is achieved by setting the ripple factor parameter to
 
 5 f(1 2 p)22 2 1g1y2  
 (7.4.17)
Notice from Figure 7.17 that not only are the n ripples in A2
a( f ) confined to the pass-
band, but they are all of the same amplitude, p. Because of this characteristic, Cheby-
shev filters are called equiripple filters. More specifically, Chebyshev-I filters are optimal 
in the sense that they are equiripple in the passband. At the start of the passband the 
squared magnitude response is either one or 1y(1 1 2), depending on whether n is odd 
or even, respectively.
 
A2
a(0) 55
1,
n  odd
 
1
1 1 2 ,
n  even 
 
 (7.4.18)
Unlike the poles of a Butterworth filter that are on a circle, the poles of a Chebyshev-I  
filter are on an ellipse. The minor and major axes of the ellipse are computed as follows 
where F0 5 Fp.
 
 5 21 1 Ï22 1 1 
 (7.4.19a)
 
r1 5 F0(1yn 2 21yn) 
 (7.4.19b)
 
r2 5 F0(1yn 1 21yn) 
 (7.4.19c)
The angles at which the poles are located are the same as those for a Butterworth filter, namely
 
k 5 (2k 1 1 1 n)
2n
, 0 # k , n 
 (7.4.20)
If the poles are expressed in rectangular form as pk 5 k 1 jk, then the real and imagi-
nary parts of the poles are
 
k 5 r1 cos(k), 0 # k , n 
 (7.4.21a)
 
k 5 r2 sin(k),  0 # k , n 
 (7.4.21b)
The DC gain of the Chebyshev-I filter is Aa(0) as given in (7.4.18). Let 
 5 (21)np0 p1 Á pn21. Then the transfer function of an nth order Chebyshev-I filter is 
as follows.
 
Ha(s) 5
Aa(0)
(s 2 p0)(s 2 p1) Á (s 2 pn21)  
 (7.4.22)
The only design parameter that remains to be determined for a Chebyshev-I filter 
is the filter order n. The minimal filter order depends on the filter design specifications. 
Using the selectivity and discrimination factors in (7.3.4)
 
n 5
 ceil 3
 ln (d21 1 Ïd22 2 1 )
 ln (r21 1 Ïr22 2 1 )4  
 (7.4.23)
Unlike a Butterworth filter, a Chebyshev-I filter always meets the passband specification 
exactly as long as the ripple factor  is chosen as in (7.4.17). The stopband specification 
will be exceeded when the expression inside the square brackets in (7.4.23) is less than the 
integer filter order n.
Ripple factor
Equiripple filters
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

536    Chapter 7  IIR Filter Design
7.4.3 Chebyshev-II Filters
The use of the term “Chebyshev-I filter” suggests that there also may be a Chebyshev-II 
filter, and this is indeed the case. A Chebyshev-II filter is an equiripple filter that has the 
ripples in the stopband rather than the passband. This is achieved by having the following 
squared magnitude response.
 
A2
a( f ) 5
2T 2
n(Fsyf  )
1 1 2T 2
n(Fsyf  )  
 (7.4.24)
The design parameter, n, for the Chebyshev-II filter is the same as that for the Chebyshev-I  
filter. However, in this case the magnitude response oscillates in the stopband and is mono-
tonically decreasing outside the stopband. A plot of the squared magnitude response is 
shown in Figure 7.18 for the case n 5 4 with Fs 5 1 Hz. Recalling that Tn(1) 5 1, it fol-
lows from (7.4.24) that at the edge of the stopband
 
A2
a(Fs) 5
2
1 1 2 
 (7.4.25)
In this case the ripple factor parameter, , controls the size of the stopband attenuation of 
the filter. By setting 2y(1 1 2) 5 2
s and solving for , it follows that a desired stopband 
attenuation, s, is achieved by setting the ripple factor parameter to
 
 5 s(1 2 2
s)21y2  
 (7.4.26)
Again notice from Figure 7.18 that there are n ripples in A2
a( f ) confined to the stop-
band, and they are all of the same amplitude, s. Consequently, Chebyshev-II filters are 
Ripple factor
Chebyshev-I Filter
EXAMPLE 7.6
As an illustration of the use of the Chebyshev design formulas, consider the prob-
lem of designing a lowpass Chebyshev-I filter to meet the same design specifi-
cations as in Example 7.4. From Example 7.4 the selectivity and discrimination 
factors are
r 5 .5
d 5 .0165
Then from (7.4.23) the minimum filter order is
n 5
 ceil 3
 log f(.0165)21 1 Ï(.0165)22 2 1g
 log f(.5)21 1 Ï(.5)22 2 1g 4
 5
 ceil (3.6449)
5 4
Note that by permitting ripples in the passband, the design specification can be 
met with a Chebyshev-I filter of order n 5 4. This is in contrast to the maximally 
flat Butterworth filter response in Example 7.4 that required a filter of order n 5 6 
for the same specifications.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.4 Classical Analog Filters     537
optimal in the sense that they are equiripple in the stopband. At the end of the stopband 
the squared magnitude response is either zero or 2y(1 1 2) depending on whether n is 
odd or even, respectively. That is,
 
lim
fS` A2
a( f ) 55
0,
n  odd 
2
1 1 2 ,
n  even 
 
 (7.4.27)
Because fyFp in the Chebyshev-I magnitude response has been replaced by Fsyf  in 
the Chebyshev-II magnitude response, the poles of the Chebyshev-II filter are located 
at the reciprocals of the poles of the Chebyshev-I filter. That is, if pk 5 k 1 jk are the 
poles defined in (7.4.21), but with F0 5 Fs, then the Chebyshev-II poles are
 
qk 5 (2Fs)2
pk
, 0 # k , n 
 (7.4.28)
Note from (7.4.24) that the numerator of A2
a ( f ) is not constant. This means that a 
Chebyshev-II filter also has either n or n 2 1 finite zeros. They are located along the 
imaginary axis at
 
rk 5 j2Fs
 sin(k), 0 # k , n 
 (7.4.29)
When n is even, there are n finite zeros as indicated in (7.4.29). However, when n is 
odd there are only n 2 1 finite zeros. Indeed, when n is odd, observe from (7.4.20) that 
(n21)y2 5 . Thus r(n21)y2 can be thought of as an infinite zero in this case.
For every Chebyshev-II filter the DC gain is Aa(0) 5 1. Let  5 q0q1 Á qn21y 
(r0r1 Á rn21) where r(n21)y2 is left out if n is odd. Then the transfer function of an  
nth-order Chebyshev-II filter is
 
Ha(s) 5 (s 2 r0)(s 2 r1) Á (s 2 rn 2 1)
(s 2 q0)(s 2 q1) Á (s 2 qn 2 1)  
 (7.4.30)
Figure 7.18:  
Squared 
Magnitude 
Responses of 
a Chebyshev-II 
Lowpass Filter of 
Order n 5 4 with 
Fs 5 1 Hz
0
0.5
1
1.5
2
0
0.2
0.4
0.6
0.8
1
f (Hz)
Fp
(1 2 p)2
2/(1 1 2)
A2(f)
a
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

538    Chapter 7  IIR Filter Design
Again, when n is odd, the numerator factor (s 2 r(n21)y2) is left out of (7.4.30). The mini-
mum order for a Chebyshev-II filter is the same as the minimum order for a Chebyshev-I 
filter and is given in (7.4.23). Thus the Chebyshev-II filter has a smaller transition band 
than the Butterworth filter, but like the Butterworth filter it is monotonic in the passband. 
A Chebyshev-II filter will always meet the stopband specification exactly as long as the 
ripple factor  is chosen as in (7.4.26). The passband specification will be exceeded when 
the expression inside the square brackets in (7.4.23) is less than the integer filter order n.
7.4.4 Elliptic Filters
The last classical lowpass analog filter that we consider is the elliptic or Cauer filter. Ellip-
tic filters are filters that are equiripple in both the passband and the stopband. In that sense 
elliptic IIR filters are analogous to the equiripple FIR filters constructed in Chapter 6  
using the Parks-McLellan algorithm. The squared magnitude response of an nth order 
elliptic filter is as follows.
 
A2
a( f ) 5
1
1 1 2U2
n( fyFp)  
 (7.4.31)
Here Un is an nth order Jacobian elliptic function, also called a Chebyshev rational func-
tion (Porat, 1997). By permitting ripples in both the passband and the stopband, elliptic 
filters achieve very narrow transition bands. A plot of the squared magnitude response is 
shown in Figure 7.19 for the case n 5 4 with Fp 5 1 Hz.
The design parameter, , for an elliptic filter is identical to that of the Chebyshev-I 
filter. Since Un(1) 5 1 for all n, it follows from (7.4.31) that (7.4.16) holds. This in turn 
means that the passband specification can be met exactly if the ripple factor  is set 
to satisfy (7.4.17). Elliptic filters are considerably more complex to analyze and design 
than Butterworth and Chebyshev filters. Finding the zeros and poles of an elliptic filter 
involves the iterative solution of nonlinear algebraic equations, equations whose terms 
include integrals (Parks and Burrus, 1987). Let us focus, instead, on the remaining design 
Figure 7.19:  
Squared 
Magnitude 
Responses of an 
Elliptic Lowpass 
Filter of Order 
n 5 4 with  
Fp 5 1 Hz
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
f (Hz)
Fs
2
s
A2(f)
a
(1 2 p)2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.4 Classical Analog Filters     539
parameter, the minimal filter order. Let g(x) denote the following function which is called 
a complete elliptic integral of the first kind.
 
g(x) 5#
y2
0
d
Ï1 2 x2 sin2() 
 (7.4.32)
Recalling the definitions of the selectivity and discrimination factors in (7.3.4), the 
required order for an elliptic filter to meet the design specifications is
 
n 5
 ceil 3
g(r2)g(Ï1 2 d 2)
g(Ï1 2 r2)g(d 2)4  
 (7.4.33)
Elliptic filters always meet the passband specification exactly as long as the ripple factor  
is chosen as in (7.4.17). The stopband specification will be exceeded when the expression 
in the square brackets in (7.4.33) is smaller than the filter order n.
Complete elliptic 
integral 
Elliptic Filter
EXAMPLE 7.7
For comparison, consider an elliptic lowpass filter that meets the same specifica-
tions that were used in Examples 7.4 and 7.6. From Example 7.4, the selectivity 
and discrimination factors are
r 5 .5
d 5 .0165
The elliptic integral function in (7.4.32) can be evaluated numerically using the 
MATLAB function ellipke. Running exam7_7, the filter order is
n 5
 ceil 3
g(.25)g(Ï1 2 (.0165)2)
g(.75)gf(.0165)2g 4
 5
 ceil (2.9061)
 5 3
In this case, note that by permitting ripples in both the passband and the stop-
band, the design specification can be met with an elliptic filter of order n 5 3. 
This is in contrast to the Chebyshev filters that required n 5 4 and the Butter-
worth filter that required n 5 6.
Although the elliptic filter is the filter of choice if the only criteria is to minimize the 
order, the other classical analog filters are often used as well because they tend to have 
better (more linear) phase response characteristics. A summary of the essential charac-
teristics of the classical analog filters is shown in Table 7.4.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

540    Chapter 7  IIR Filter Design
Bilinear Transformation Method
Now that a collection of analog prototype filters is in place, the next task is to transform 
an analog filter into an equivalent digital filter. Although a number of approaches are 
available, they all must satisfy the fundamental qualitative constraint that a stable ana-
log filter Ha(s) transforms into a stable digital filter H(z). The classical analog nth-order 
7.5
The DSP Companion contains four functions for computing the coefficients of the 
classical lowpass analog filters and a function for computing the analog frequency 
response. Recall from Chapter 1 that the DSP Companion function f_      freqs can be 
used to compute the frequency response of an analog filter.
% F_BUTTERS: 
Design Butterworth lowpass analog filter
% F_CHEBY1S: 
Design Chebyshev-I lowpass analog filter
% F_CHEBY2S: 
Design a Chebyshev-II lowpass analog filter
% F_ELLIPTICS: Design elliptic lowpass analog filter
%
% Usage:
% 
[b,a] = f_butters (Fp,Fs,deltap,deltas,n)
% 
[b,a] = f_cheby1s (Fp,Fs,deltap,deltas,n)
% 
[b,a] = f_cheby2s (Fp,Fs,deltap,deltas,n)
% 
[b,a] = f_elliptics (Fp,Fs,deltap,deltas,n)
% Pre:
% 
Fp 
= passband cutoff frequency in Hz
% 
Fs 
= stopband cutoff frequency in Hz
% 
 
 (Fs > Fp)
% 
deltap 
= passband ripple
% 
deltas 
= stopband attenuation
% 
n 
= an optional integer specifying the filter
% 
 
order. If n is not present, the smallest
% 
 
order which meets the specifications is
% 
 
used.
% Post:
% 
b = coefficient vector of numerator polynomial
% 
a = 1 by (n+1) coefficient vector of denominator
% 
polynomial
DSP Companion
DsP Companion
Table 7.4: Summary 
of Classical Analog 
Filters
Analog Filter
Passband
stopband
Transition Band
Exact 
specification
Butterworth
Monotonic
Monotonic
Broad
Either
Chebyshev-I
Equiripple
Monotonic
Narrow
Passband
Chebyshev-II
Monotonic
Equiripple
Narrow
Stopband
Elliptic
Equiripple
Equiripple
Very narrow
Passband
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.5 Bilinear Transformation Method    541
filters discussed in Section 7.4 each have n distinct poles pk. Therefore, Ha(s), can be writ-
ten in partially factored form as
 
Ha(s) 5
b(s)
(s 2 p0)(s 2 p1) Á (s 2 pn21) 
 (7.5.1)
A design technique that is highly effective is based on replacing integration with 
a discrete-time numerical approximation. Recall that an integrator has the continuous-
time transfer function H0(s) 5 1ys. The time-domain input-output representation of an 
integrator is
 
ya(t) 5#
t
0
xa()d 
 (7.5.2)
To approximate the area under the curve xa(t) numerically, let x(k) 5 xa(kT ) where T is 
the sampling interval. Consider the trapezoids formed by connecting the samples with 
line segments as shown in Figure 7.20. This is equivalent to using a piecewise-linear 
approximation to xa(t). Suppose y(k) denotes the approximation to the integral at time 
t 5 kT. The approximation at time kT is the approximation at time (k 2 1)T plus the 
area of the kth trapezoid. From Figure 7.20, the kth trapezoid has width T and average 
height fx(k 2 1) 1 x(k)gy2. Thus,
 
y(k) 5 y(k 2 1) 11
T
22 fx(k 2 1) 1 x(k)g 
 (7.5.3)
The approximation in (7.5.3) is called a trapezoid rule integrator. Taking the Z- 
transform of both sides of (7.5.3) and using the delay property yields (1 2 z21)Y(z) 5
(Ty2)(1 1 z21)X(z). Thus the transfer function of a trapezoid rule integrator is
 
H0(z) 5 T
2 1
1 1 z21
1 2 z212 
 (7.5.4)
Trapezoid rule 
integrator
Figure 7.20:  
Piecewise-linear 
Approximation of 
Integration Using 
Trapezoids
0
2
3
1
4
5
6
7
8
9
10
0
0.5
1
1.5
2
2.5
3
t/T
xa(t)
>
^
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

542    Chapter 7  IIR Filter Design
Note that replacing 1ys by H0(z) is equivalent to replacing s by 1yH0(z). Therefore one 
can approximate the integration process with a trapezoid rule integrator by making the 
following substitution for s in the analog filter transfer function Ha(s).
 
H(z) 5 Ha(s)us5g(z) 
 (7.5.5)
Here g(z) 5 1yH0(z). That is, the substitution s 5 g(z) in (7.5.5) uses
 
g(z) 5 2
T 1
z 2 1
z 1 12  
 (7.5.6)
The transformation from Ha(s) to H(z) in (7.5.5) is called a bilinear transformation, 
and filter designs based on it use the bilinear transformation method. Before designing a 
filter using the bilinear transformation, it is helpful to examine the relationship between 
z and s 5 g(z) in more detail. First note that the transformation can be inverted. That is, 
one can solve (7.5.6) for z, which yields
 
z 5 2 1 sT
2 2 sT 
 (7.5.7)
Next suppose s is expressed in rectangular form as s 5  1 j. Substituting this into 
(7.5.7) and taking the magnitude of both sides yields
 
uzu 5 Ï(2 1 T)2 1 (T)2
Ï(2 2 T)2 1 (T)2 
 (7.5.8)
Notice that if  5 0, then uzu 5 1. Thus the bilinear transformation maps the imaginary 
axis of the s plane into the unit circle of the z plane. Furthermore, if  , 0, then uzu , 1, 
which means that the left half of the s plane is mapped into the interior of the unit cir-
cle in the z plane. Similarly, when  . 0 the right-half of the s plane is mapped into the 
exterior of the unit circle in the z plane. Therefore the bilinear transformation satisfies the 
fundamental property that it is guaranteed to transform a stable analog filter Ha(s) into 
a stable digital filter H(z). A graphical summary of the bilinear transformation is shown 
in Figure 7.21.
The bilinear transformation in Figure 7.21 maps the entire imaginary axis of the 
s plane onto the unit circle of the z plane as in (7.5.7). In so doing, the infinite analog 
frequency range, 0 # F , `, gets compressed or warped into the finite digital frequency 
range, 0 # f , fsy2. To develop the relationship between F and f , let s 5 j2F denote 
Bilinear transformation
Figure 7.21:  
Bilinear 
Transformation 
from the s Plane 
onto the z Plane
22
21
0
1
2
22
21
0
1
2
Re(s)
T Im(s)
22
21
0
1
2
22
21
0
1
2
Re(z)
Im(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.5 Bilinear Transformation Method    543
a point on the imaginary axis of the s plane, and let z 5 exp( j2f T ) denote the cor-
responding point on the unit circle of the z plane. Setting s 5 g(z) in (7.5.6) and using 
Euler’s identity yields
j2F 5 2
T 3
 exp( j2f T ) 2 1
 exp( j2f T ) 1 14
 5 2
T 1
 exp( jf T )fexp( jf T ) 2 exp( jf T )g
 exp( jf T )fexp(jf T ) 1 exp(2jf T )g2
 5 2
T 3
j2 sin(f T )
2 cos(f T )4
 
 5 j2 tan(f T )
T
 
 (7.5.9)
Solving (7.5.9) for F then results in the following relationship between the digital filter 
frequency f  and the analog filter frequency F.
 
F 5
 tan(f T )
T
 
 (7.5.10)
The transformation from f  to F in (7.5.10) is called frequency warping because it rep-
resents an expansion of the finite digital frequency range 0 # f , fsy2 into the infinite 
analog frequency range 0 # F , `. The nonlinear frequency warping curve is shown in 
Figure 7.22. Note that there is an asymptote at the folding frequency fd 5 fsy2.
The mapping from digital frequency f  to analog frequency F in (7.5.10) can be 
inverted by solving (7.5.10) for f . Multiplying (7.5.10) by T, taking the arctangent of 
both sides, and dividing by T yields
 
f 5
 tan 21(FT )
T
 
 (7.5.11)
Frequency warping
Figure 7.22:  
Frequency 
Warping Caused 
by the Bilinear 
Transformation
0
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45 0.5
0
1
2
3
4
5
6
f/fs
f (Hz)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

544    Chapter 7  IIR Filter Design
When the bilinear transformation from s to z in (7.5.7) is performed, the analog 
frequencies in the range 0 # F , ` get compressed into digital frequencies in the range 
0 # f , fsy2 as indicated in (7.5.11). One can take this nonlinear compression into 
account in filter design by first prewarping each desired digital cutoff frequency fc into 
a corresponding analog cutoff frequency Fc using (7.5.10). When the bilinear transfor-
mation is then performed, these prewarped cutoff frequencies get warped back into the 
original desired digital cutoff frequencies as in (7.5.11). The overall design procedure for 
the bilinear-transformation method is summarized in the following algorithm, where it is 
assumed that m # n.
1.  Prewarp all digital cutoff frequencies, fi, into corresponding analog cutoff frequen-
cies, Fi, using (7.5.10).
2. Construct an analog prototype filter Ha(s) using the prewarped cutoff frequencies.
3.  If Ha(s) is low order, compute H(z) 5 Hafg(z)g using (7.5.6). For a higher-order 
Ha(s), the following steps can be used.
(a) Factor the numerator and denominator of Ha(s) as follows.
Ha(s) 5 0(s 2 u1) Á (s 2 um)
(s 2 v1) Á (s 2 vn)
(b) Compute the digital zeros and poles as follows using (7.5.7).
zi 5 2 1 uiT
2 2 uiT,  1 # i # m
pi 5 2 1 viT
2 2 viT, 1 # i # n
(c) Compute the digital filter gain using
b0 5 0T n2m(2 2 u1T) Á (2 2 umT)
(2 2 v1T) Á (2 2 vnT)
(d) Construct the factored form of the digital filter as follows.
H(z) 5 b0(z 1 1)n2m(z 2 z1) Á (z 2 zm)
(z 2 p1) Á (z 2 pn)
4. Express H(z) as a ratio of two polynomials in z21.
Algorithm 7.1 assumes that Ha(s) is a proper rational polynomial, which means that 
m # n. If m , n, then Ha(s) will have n 2 m zeros at s 5 `. Note from step 3d that these 
n 2 m high-frequency zeros get mapped into zeros at z 5 21, the highest digital fre-
quency that H(z) can process.
AlgoRITHM
7.1 Bilinear Transforma-
tion Method
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.5 Bilinear Transformation Method    545
Bilinear Transformation Method
EXAMPLE 7.8
As an illustration of using Algorithm 7.1 to design a digital lowpass filter, sup-
pose the sampling frequency is fs 5 20 Hz, and consider the following lowpass 
design specifications.
( f0, f1) 5 (2.5, 7.5)   Hz 
(p, s) 5 (.1, .1)
Here f0 and f1 denote the desired passband and stopband frequencies, respec-
tively. From step 1 of Algorithm 7.5.1, the prewarped passband and stopband 
frequencies are
F0 5
 tan(2.5y20)
y20
 5 2.637  Hz 
F1 5
 tan(7.5y20)
y20
 5 15.37  Hz 
Suppose the analog prototype filter used is a lowpass Butterworth filter. From 
(7.4.8), the minimum order for the filter is
n 5
 ceil 5
 log 3
.922 2 1
.122 2 14
2 log 1
2.637
15.3726
 5
 ceil (1.715)
5 2
Next, suppose the cutoff frequency, Fc, is selected to meet the passband specifica-
tion exactly. Then from (7.4.9), the required cutoff frequency is
Fc 5
2.637
(.922 2 1)1y4
 5 3.789  Hz 
Thus the radian cutoff frequency is Vc 5 2Fc 5 23.81 rad/s. From Table 7.2, 
and (7.4.13), the transfer function of a prewarped Butterworth filter of order 
n 5 2 is
Ha(s) 5
V2
c
s2 1 Ï2Vcs 1 V2
c
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

546    Chapter 7  IIR Filter Design
Figure 7.23: Magnitude Response of a Digital IIR Filter Obtained by a Bilinear 
Transformation of the Prewarped Analog Butterworth Filter: n 5 2 and 
Fc 5 3.789 Hz
0
2
3
1
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1
f (Hz)
A(f)
Fp
Fs
s
1 2 p
Since Ha(s) is a low-order filter, one can apply step 3 of Algorithm 7.5.1 using 
direct substitution. Thus from (7.5.6), the discrete-equivalent transfer function 
H(z) is
H(z) 5 Hafg(z)g
 5
V2
c
g2(z) 1 Ï2Vcg(z) 1 V2
c
 5
V2
c
3
2(z 2 1)
T(z 1 1)4
2
1 Ï2Vc3
2(z 2 1)
T(z 1 1)4 1 V2
c
 5
(TVc)2(z 1 1)2
4(z 2 1)2 1 2Ï2TVc(z 2 1)(z 1 1) 1 (TVc)2(z 1 1)2
 5
(TVc)2(z 1 1)2
4(z2 2 2z 1 1) 1 2Ï2TVc(z2 2 1) 1 (TVc)2(z2 1 2z 1 1)
Next the terms in the denominator are combined, the denominator is normalized, 
and the numerator and denominator are multiplied by z22. The final result after 
substitution of T 5 1y20 and Vc 5 23.81 is then
H(z) 5
.1613(1 1 2z21 1 z22)
1 2 .5881z21 1 .2334z22
A plot of the digital filter magnitude response is shown in Figure 7.23. Note how 
the passband specification, 1 2 p # A( f ) # 1 for 0 # f # 2.5 Hz is met exactly, 
and the stopband specification 0 # A( f ) # s for 7.5 # f # 10 Hz is exceeded.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.6 Frequency Transformations    547
Frequency Transformations
At this point, the tools are in place to design digital lowpass filters. One starts with a 
normalized classical analog lowpass filter Ha(s). Next the cutoff frequency is prewarped 
using (7.5.10). Finally, the bilinear transformation s 5 g(z) in (7.5.6) is applied to pro-
duce a digital equivalent filter H(z). In this section we use frequency transformations to 
extend this technique, so it is also applicable to the other frequency-selective filters such 
as highpass, bandpass, and bandstop filters.
7.6.1 Analog Frequency Transformations
Recall that one way to design a lowpass Butterworth filter with a radian cutoff frequency 
of V0 is to start with a normalized lowpass Butterworth filter and replace s with syV0. This 
is an example of a frequency transformation. Using this same basic approach, one can 
transform a normalized lowpass filter into other types of frequency-selective filters, such as 
highpass, bandpass, and bandstop filters. To illustrate the procedure, consider the following 
normalized lowpass Butterworth filter of order n 5 1 taken from (7.4.12) and Table 7.2.
 
Hnorm(s) 5
1
s 1 1 
 (7.6.1)
7.6
The DSP Companion contains the following function for performing a digital to 
analog filter transformation using the bilinear transformation method.
% F_BILIN: Bilinear analog to digital filter transformation
%
% Usage:
%  
[B,A] = f_bilin (b,a,fs)
% Pre:
% 
b 
= vector of length m+1 containing coefficients
% 
 
 of analog numerator polynomial.
% 
a 
= vector of length n+1 containing coefficients
% 
 
 of analog denominator polynomial (n >= m).
% 
fs = sampling frequency in Hz
% Post:
% 
B 
= (n+1) by 1 vector containing coefficients of
% 
 
 digital numerator polynomial.
% 
A 
= (n+1) by 1 vector containing coefficients of
% 
 
 digital denominator polynomial.
% Notes:
% 
The critical frequencies of H(s) must first be
% 
prewarped using:
%
% 
  F = tan(pi*f*T)/(pi*T)
DSP Companion
DsP Companion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

548    Chapter 7  IIR Filter Design
Suppose s is replaced, not by syV0, but by V0ys. The resulting transfer function is then
Ha(s) 5 Hnorm(V0ys)
 5
1
V0ys 1 1
 
 5
s
s 1 V0
 
 (7.6.2)
Notice that Aa(0) 5 0 and Aa(`) 5 1, which means that Ha(s) is a highpass filter. Further-
more, Aa(V0) 5 1yÏ2. Thus the frequency transformation D(s) 5 V0ys maps a normal-
ized lowpass filter into a highpass filter with a 3-dB cutoff frequency of V0 rad/s.
It is also possible to transform a normalized lowpass filter into a bandpass filter. 
Recall that a bandpass filter has a low-frequency cutoff, V0, and a high-frequency cutoff, 
V1. Since a bandpass filter has two cutoff frequencies, the complex frequency variable 
s must be replaced by a quadratic polynomial of s in order to double the order of the 
transfer function. In particular, if s is replaced by D(s) 5 (s2 1 V0V1)yf(V1 2 V0)sg, then 
the resulting filter is a bandpass filter with the desired cutoff frequencies.
Just as the highpass transformation is the reciprocal of the lowpass transformation, 
the bandstop transformation is the reciprocal of the bandpass transformation. A sum-
mary of the four basic frequency transformations can be found in Table 7.5. Using these 
transformations, a normalized lowpass transfer function Hnorm(s) with a cutoff frequency 
of Vc 5 1 rad/s can be converted into an arbitrary lowpass, highpass, bandpass, or band-
stop transfer function Ha(s).
Table 7.5: Analog 
Frequency 
Transformations, 
Ha(s) 5 HnormfD(s)g 
Ha(s) 
D(s) 
Lowpass with cutoff V0 
s
V0
 
Highpass with cutoff V0 
V0
s  
Bandpass with cutoffs V0,V1 
s2 1 V0V1
(V1 2 V0)s 
Bandstop with cutoffs V0,V1 
(V1 2 V0)s
s2 1 V0V1
 
lowpass to Bandpass
EXAMPLE 7.9
As an illustration of the frequency-transformation method, consider the problem 
of designing an analog bandpass filter. Suppose the desired cutoff frequencies are 
F0 5 5 Hz and F1 5 15 Hz. Thus the corresponding radian cutoff frequencies are 
V0 5 10 rad/s and V1 5 30 rad/s. As a starting point, consider the first-order 
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.6 Frequency Transformations    549
Given the classical analog lowpass filters in Section 7.4, and the frequency transfor-
mations in Table 7.5, it is possible to design a variety of analog frequency-selective filters. 
The bilinear analog-to-digital filter transformation in Section 7.5 then can be applied to 
convert these analog filters to equivalent digital filters. Before the bilinear transformation 
is applied, all cutoff frequencies must be prewarped using (7.5.10). The following example 
illustrates the use of the bilinear transformation method to construct a bandpass filter.
lowpass Butterworth filter in (7.6.1). Using the third entry of Table 7.5, the band-
pass filter transfer function is
Ha(s) 5 H normfD(s)g
 5
1
s2 1 V0V1
(V1 2 V0)s 1 1
 5
(V1 2 V0)s
s2 1 (V1 2 V0)s 1 V0V1
 5
20s
s2 1 20s 1 3002
Digital Bandpass Filter
EXAMPLE 7.10
Consider the second-order analog bandpass filter from Example 7.9. That is, 
suppose the desired cutoff frequencies are F0 5 5 Hz, and F1 5 15 Hz, and the 
sampling rate is fs 5 50 Hz. Applying (7.5.10), the prewarped radian cutoff fre-
quencies are
V0 5 2 tan(F0T )
T
 5 100 tan(.2)
 5 32.49
V1 5 2 tan(F1T )
T
 5 100 tan(.6)
 5 137.64
From Example 7.9, the transfer function of a second-order Butterworth band-
pass filter with cutoff frequencies of V0 and V1 is
Ha(s) 5
(V1 2 V0)s
s2 1 (V1 2 V0)s 1 V0V1
 5
105.15s
s2 1 105.15s 1 4472.1
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

550    Chapter 7  IIR Filter Design
Next apply the bilinear transformation to convert Ha(s) into an equivalent digital 
filter. Using (7.5.6) this yields
H(z) 5 Hafg(z)g
 5
105.15g(z)
g2(z) 1 105.15g(z) 1 4472.1
 5
105.15 3
2(z 2 1)
T(z 1 1)4
3
2(z 2 1)
T(z 1 1)4
2
1 105.15 3
2(z 2 1)
T(z 1 1)4 1 4472.1
 5
1.0515(z 2 1)(z 1 1)
(z 2 1)2 1 1.0515(z 2 1)(z 1 1) 1 .44721(z 1 1)2
 5
1.0515(z2 2 1)
(z2 2 2z 1 1) 1 1.0515(z2 2 1) 1 .44721(z2 1 2z 1 1)
Combining like terms, normalizing the denominator, and multiplying top and 
bottom by z22 then results in the following digital bandpass filter using the  
bilinear transformation method
H(z) 5
.4208(1 2 z22)
1 2 .4425z21 1 .1584z22
A plot of the magnitude response of H(z), obtained by running exam7_10, is 
shown in Figure 7.24. Although this filter is far from ideal because of its low 
order, it does have the correct 3-dB cutoff frequencies.
Figure 7.24: Frequency Response of a Second-order Digital Bandpass Filter 
from Example 7.6.2
0
5
10
15
20
25
0
0.2
0.4
0.6
0.8
1
f (Hz)
A(f)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.6 Frequency Transformations    551
Figure 7.25:  
Digital Filter 
Design Using an 
Analog Frequency 
Transformation
Normalized
lowpass
filter
Frequency
transformation
Table 7.5
Bilinear
transformation
Algorithm 7.1
Analog
Analog
Digital
The design technique based on an analog frequency transformation of a lowpass 
prototype filter is summarized in Figure 7.25.
7.6.2 Digital Frequency Transformations
Frequency transformations from lowpass filters to other frequency-selective filters also 
can be done in the discrete-time or digital domain. In this case, the bilinear analog-to- 
digital filter transformation is applied to a normalized lowpass prototype filter. The 
resulting lowpass digital filter is then transformed to a lowpass, highpass, bandpass, or 
bandstop filter using a digital frequency transformation.
Let Hlow(z) be a digital lowpass filter with a cutoff frequency of Fc. This is converted 
to another frequency-selective filter by replacing z with a frequency transformation D(z) 
as follows.
 
H(z) 5 HlowfD(z)g 
 (7.6.3)
The transformation D(z) must satisfy certain qualitative properties. First, it must map 
a rational polynomial, Hlow(z), into a rational polynomial H(z). This means that D(z) 
itself must be a ratio of polynomials. Because D(z) is a frequency response transfor-
mation, it should map the unit circle into the unit circle. Evaluating D(z) along the 
unit circle yields the frequency response D( f ). Thus the magnitude response must 
satisfy
 
uD( f )u 5 1, 0 # u f u , fsy2 
 (7.6.4)
Notice that the magnitude response constraint in (7.6.4) is an allpass characteristic. 
Hence D(z) must be an allpass filter as in (5.4.7). To maintain stability, the transforma-
tion D(z) must also map the interior of the unit circle into the interior of the unit circle. 
Constantinides (1970) has developed four basic digital frequency transformations that 
are summarized in Table 7.6. The source filter is a lowpass filter with a cutoff frequency 
of Fc, and the destination filter is the filter listed in column one.
The design technique based on a digital frequency transformation of a lowpass filter 
is summarized in Figure 7.26.
Figure 7.26: Digital 
Filter Design Using 
a Digital Frequency 
Transformation
Normalized
lowpass
filter
Frequency
transformation
Table 7.6
Bilinear
transformation
Algorithm 7.1
Analog
Digital
Digital
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

552    Chapter 7  IIR Filter Design
Table 7.6: Digital 
Frequency 
Transformations, 
H(z) 5 HlowfD(z)g
H(z)
D(z)
Coefficients 
Lowpass with cutoff F0 
2(z 2 a0)
a0z 2 1  
a0 5
 sinf(Fc 2 F0)g
 sinf(Fc 1 F0)g 
Highpass with cutoff F0 
z 2 a0
a0z 2 1 
a0 5
 cosf(Fc 1 F0)g
 cosf(Fc 2 F0)g 
Bandpass with cutoffs F0, F1
2(z2 1 a0z 1 a1)
a1z2 1 a0z 1 1
 5
 cosf(F1 1 F0)g
 cosf(F1 2 F0)g 
 
 5 tan(Fc) cotf(F1 2 F0)g 
a0 5 22
 1 1 
 
a1 5  2 1
 1 1 
Bandstop with cutoffs F0, F1
z2 1 a0z 1 a1
a1z2 1 a0z 1 1
 5
 cosf(F1 1 F0)g
 cosf(F1 2 F0)g 
 
 5 tan(Fc) tanf(F1 2 F0)g 
a0 5 22
 1 1 
 
a1 5 1 2 
1 1  
The DSP Companion contains the following functions for designing classical  
frequency-selective IIR digital filters using the method summarized in Figure 7.25. 
Recall that the function f_      freqz can be used to compute the frequency response of 
a digital filter.
% F_BUTTERZ: 
Design a Butterworth IIR digital filter
% F_CHEBY1Z: 
Design a Chebyshev-I IIR digital filter
% F_CHEBY2Z: 
Design a Chebyshev-II IIR digital filter
% F_ELLIPTICZ: Design elliptic IIR digital filter
%
% Usage:
% 
[b,a] = f_butterz (Fp,Fs,deltap,deltas,ftype,fs,n)
% 
[b,a] = f_cheby1z (Fp,Fs,deltap,deltas,ftype,fs,n)
% 
[b,a] = f_cheby2z (Fp,Fs,deltap,deltas,ftype,fs,n)
% 
[b,a] = f_ellipticz (Fp,Fs,deltap,deltas,ftype,fs,n)
% Pre:
% 
Fp  
= passband cutoff frequency or frequencies
% 
Fs  
= stopband cutoff frequency or frequencies
% 
deltap  = passband ripple
% 
deltas  = stopband attenuation
% 
ftype  = filter type
DSP Companion
DsP Companion
(Continued    )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.7 Filter Realization Structures    553
Filter Realization structures
In this section we investigate alternative configurations that can be used to realize IIR fil-
ters with signal flow graphs. These filter realizations differ from one another with respect 
to memory requirements, computational time, and sensitivity to finite word length effects.
7.7.1 Direct Forms
An nth-order IIR filter has a transfer function H(z) that can be expressed as a ratio of 
two polynomials.
 
H(z) 5 b0 1 b1z21 1 Á 1 bnz2n
1 1 a1z21 1 Á 1 anz2n  
 (7.7.1)
For convenience, we have assumed that the degree of the numerator is equal to the degree 
of the denominator because this simplifies and streamlines the treatment. This is not a 
limiting assumption because one can always pad the numerator or denominator coeffi-
cient vector with zeros, as needed, to make them the same length.
Direct Form I
The simplest realization of H(z) is based on factoring the transfer function into its autore-
gressive and moving average parts as follows.
 
H(z) 51
1
1 1 a1z21 1 Á 1 anz2n 21
b0 1 b1z21 1 Á1 bnz2n
1
2 
 (7.7.2)
 
Har(z) 
Hma(z)
7.7
(++++)++++* (++++)++++*
%
% 
 
0 = lowpass
% 
 
1 = highpass
% 
 
2 = bandpass (Fp and Fs are vectors)
% 
 
3 = bandstop (Fp and Fs are vectors)
%
% 
fs 
= 
sampling frequency in Hz
% 
n 
= 
an optional integer specifying the
% 
 
filter order. If n is not present, an
% 
 
estimate of order required to meet the
% 
 
specifications is used.
% Post:
% 
b = 1 by (n+1) coefficient vector of numerator
% 
  polynomial
% 
a  = 1 by (n+1) coefficient vector of denominator
% 
  polynomial
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

554    Chapter 7  IIR Filter Design
If U(z) denotes the output of the moving average subsystem, Hma(z), then the input-out-
put description of the IIR filter can be written in terms of the intermediate variable U(z) 
as follows.
 
U(z) 5 Hma(z)X(z) 
 (7.7.3a)
 
Y(z) 5 Har(z)U(z) 
 (7.7.3b)
Taking the inverse Z-transform of (7.7.3), using (7.7.2) and the delay property, an 
IIR filter can be represented in the time domain by the following pair of difference 
equations.
 
u(k) 5 o
n
i50
bix(k 2 i)
 
 (7.7.4a)
 
y(k) 5 u(k) 2 o
n
i51
aiy(k 2 i) 
 (7.7.4b)
The representation in (7.7.4) is called a direct form I realization. It is a direct repre-
sentation because the coefficients of the difference equations can be obtained directly 
from inspection of the transfer function. A signal flow graph of a direct form I realiza-
tion, for the case n 5 3, is shown in Figure 7.27. Note how the left side of the signal flow 
graph implements the moving average part in (7.7.4a), and the right side implements the 
autoregressive part in (7.7.4b).
Direct Form II
There is a very simple change that can be made to (7.7.2) to generate an alternative direct 
form realization that has some advantages over the direct form I structure. Suppose the 
order of the autoregressive and moving average subsystems is interchanged. Clearly, this 
does not affect the overall transfer function.
 
H(z) 51
b0 1 b1z21 1 Á 1 bnz2n
1
21
1
1 1 a1z21 1 Á 1 anz2n2 
 (7.7.5)
 
Hma(z) 
Har(z)
Direct form I
(++++)++++*(++++)++++*
Figure 7.27: Direct 
Form I Realization, 
n 5 3
2a3
2a2
2a1
z21
z21
z21
z21
z21
z21
u
y
x
b0
b1
b2
b3
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.7 Filter Realization Structures    555
Next let U(z) denote the output of the autoregressive subsystem, Har(z). Then the input- 
output description of the overall filter, in terms of the intermediate variable U(z), is as follows.
 
U(z) 5 Har(z)X(z)  
 (7.7.6a)
 
Y(z) 5 Hma(z)U(z) 
 (7.7.6b)
Taking the inverse Z-transform of (7.7.6), using (7.7.5) and the delay property, an IIR 
filter can be represented in the time domain by the following difference equations.
 
u(k) 5 x(k) 2 o
n
i51
aix(k 2 i) 
 (7.7.7a)
 
y(k) 5 o
n
i50
biu(k 2 i)
 
 (7.7.7b)
This alternative representation of an IIR filter is called a direct form II realiza-
tion. A signal flow graph of a direct form II realization, for the case n 5 3, is shown in  
Figure 7.28. In this case, the left side of the signal flow graph implements the autoregres-
sive part in (7.7.7a), and the right side implements the moving average part in (7.7.7b).
It is of interest to compare the signal flow graphs in Figure 7.27 and Figure 7.28. Each 
arc associated with a delay element requires one memory or storage element to implement. 
Consequently, direct form I requires a total of 2n storage elements, whereas direct form II 
requires only n storage elements. Since the minimum number of storage elements required 
for an nth order filter is n, direct form II is an example of a canonic representation.
Transposed Direct Form II
Just as was the case with FIR filters, one can apply the flow graph reversal theorem in 
Proposition 6.2 to generate a transposed direct form realization by reversing the direc-
tions of all arcs and interchanging the labels of the input and output. After redrawing 
the signal flow graph so the input is on the left, this yields the transposed direct form II 
realization shown in Figure 7.29 for the case m 5 3.
The difference equations describing the transposed direct form II realization can be 
obtained directly from inspection of Figure 7.29. Here a vector of intermediate variables 
u 5 fu1, u2, Á , ungT is used. The intermediate variables are defined recursively, starting at 
Direct form II
Canonic
Transposed direct 
form II
Figure 7.28: Direct 
Form II Realization, 
n 5 3
2a3
2a2
2a1
z21
z21
z21
u
y
x
b0
b1
b2
b3
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

556    Chapter 7  IIR Filter Design
the bottom of the signal flow graph and moving up the center. The last equation is then 
the output equation.
 
u1(k) 5 bnx(k) 2 any(k)
 
 (7.7.8a)
 
ui(k) 5 bix(k) 2 aiy(k) 1 ui21(k), 2 # i # n 
 (7.7.8b)
 
y(k) 5 b0x(k) 1 un(k 2 1) 
 (7.7.8c)
The direct form realizations also can be compared in terms of the required compu-
tational effort. Each summing junction node with m inputs requires m 2 1 floating-point 
additions, and each arc with a constant nonunity gain requires one floating-point multi-
plication. The results of the comparison are summarized in Table 7.7, where it can be seen 
that the three direct form realizations are identical in terms of the computational time, 
measured in floating-point operations or FLOPs. In each case, the number of FLOPs 
grows linearly with the order of the filter. However, the direct form II realizations have 
the advantage that they require only half as much memory.
7.7.2 Parallel Form
Just as with FIR filters, IIR filters have a number of indirect forms whose coefficients are 
derived from the original coefficient vectors, a and b. To develop the indirect form reali-
zation structures, consider the following partially factored form of the transfer function.
 
H(z) 5
b(z)
(z 2 p1)(z 2 p2) Á (z 2 pn) 
 (7.7.9)
Here pk is the kth pole of the filter. If H(z) has poles at z 5 0, these poles represent 
pure delays that can be removed and treated separately. Consequently, it is assumed that 
pk Þ 0 for 1 # k # n. For most practical filters, the nonzero poles are distinct from one 
Figure 7.29:  
Transposed Direct 
Form II Realization, 
n 5 3
2a3
2a2
2a1
z21
z21
z21
u1
u2
u3
y
x
b0
b1
b2
b3
Table 7.7: Comparison 
of Direct Form 
Realizations of IIR 
Filter
Direct Form
storage Elements
Additions
Multiplications
I
2n 
2n
2n 2 1 
II
n 
2n
2n 2 1 
Transposed II 
n 
2n
2n 2 1 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.7 Filter Realization Structures    557
another. Suppose H(z)yz is expressed in partial fraction form. Multiplying both sides by 
z then results in the following representation of H(z), assuming the poles are distinct.
 
H(z) 5 R0 1 o
n
i51
Riz
z 2 pi
 
 (7.7.10)
Here Ri is the residue of H(z)yz at the ith pole with p0 5 0. From (7.7.10)
 
R0 5 H(0)
 
 (7.7.11a)
 
Ri  5 (z 2 pi)H(z)
z
u z 5 pi
, 1 # i # n 
 (7.7.11b)
The problem with using (7.7.10) directly for a signal flow graph realization is that 
the poles and residues are often complex. If H(z) has real coefficients, then the com-
plex poles and residues will appear in conjugate pairs. Consequently, one can rewrite 
H(z) as a sum of N second-order subsystems with real coefficients as follows, where 
N 5
 floor h(n 1 1)y2j.
 
H(z) 5 R0 1 o
N
i51
Hi(z)  
 (7.7.12)
This is called a parallel form realization. The ith second-order subsystem is constructed 
by combining pairs of terms in (7.7.10) associated with either real poles or complex- 
conjugate pairs of poles. This way, the second-order coefficients will be real. Combining 
the terms associated with poles pi and pj, simplifying, and expressing the final result in 
terms of negative powers of z yields
 
Hi(z) 5
bi0 1 bi1z21
1 1 ai1z21 1 ai2z22, 1 # i # N 
 (7.7.13)
Note that bi2 5 0. The real coefficients of the second-order block can be expressed in 
terms of the poles and residues as follows for 1 # i # N.
 
bi0 5 Ri 1 Rj
 
 (7.7.14a)
 
bi1 5 2(Ri pj 1 Rj pi) 
 (7.7.14b)
 
ai1 5 2(pi 1 pj)
 
 (7.7.14c)
 
ai2 5 pi pj
 
 (7.7.14d)
Let ui denote the output of the ith second-order block. Then from (7.7.12) and (7.7.13), a par-
allel form realization is characterized in the time domain by the following difference equations.
ui(k) 5 bi0x(k) 1 bi1x(k 2 1) 2 ai1ui(k 2 1)
 
 2 ai2ui(k 2 2), 1 # i # N
 
 (7.7.15a)
 
y(k) 5 R0x(k) 1 o
N
i51
ui(k)
 
 (7.7.15b)
If n is even, then there will be N subsystems, each of order two, whereas if n is odd, there 
will be N 2 1 second-order subsystems and one first-order subsystem. The coefficients of 
the first-order subsystem are obtained from (7.7.14) by setting Rj 5 0 and pj 5 0.
Any of the direct forms can be used to realize the second-order blocks in (7.7.13). A 
block diagram showing the overall structure of a parallel form realization, for the case 
N 5 2, is shown in Figure 7.30. Since the parallel form coefficients must be computed 
using (7.7.14), rather than obtained directly from inspection of H(z), the parallel form 
realization is an example of an indirect form.
Parallel form 
realization
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

558    Chapter 7  IIR Filter Design
Figure 7.30:  
Parallel Form Block 
Diagram, N 5 2
1
1
x(k)
y(k)
u1
u2
H2(z)
H1(z)
R0
IIR Parallel Form
EXAMPLE 7.11
As an illustration of a parallel form realization of an IIR filter, consider the fol-
lowing fourth-order transfer function.
H(z) 5
2z(z3 1 1)
f(z 1 .3)2 1 .16g(z 2 .8)(z 1 .7)
Inspection of H(z) reveals that the poles are
p1, 2 5 2.3 6 j.4
p3 5 .8
p4 5 2.7
Suppose H1(z) is a block associated with the complex-conjugate pair of poles, and 
H2(z) is associated with the real poles. Running exam7_11 produces the following 
three subsystems.
R0 5 0
H1(z) 5 3.266 2 2.134z21
1 1 .6z21 1 .25z22
H2(z) 5 21.266 1 3.220z21
1 2 .1z21 2 .56z22
Suppose a direct form II realization is used for the second-order blocks. The 
resulting signal flow graph of a parallel form realization of the fourth-order filter 
is as shown in Figure 7.31.
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.7 Filter Realization Structures    559
z21
2.6
u1
u2
y
x
2.25
.1
.56
22.134
3.266
21.266
3.220
z21
z21
z21
7.7.3 Cascade Form
An even simpler way to decompose H(z) into lower-order subsystems is to factor both the 
numerator and the denominator of H(z) as follows.
 
H(z) 5 b0(z 2 z1)(z 2 z2) Á (z 2 zn)
(z 2 p1)(z 2 p2) Á (z 2 pn)  
 (7.7.16)
Note that if the degree of the numerator in (7.7.1) is m , n, then the factored representation in 
(7.7.16) will have n 2 m zeros at zk 5 0. Since the coefficients of H(z) are assumed to be real, 
complex poles and zeros will occur in conjugate pairs. The representation in (7.7.16) can be  
recast as a product of N second-order subsystems as follows where N 5
 floor f(n 1 1)y2g.
 
H(z) 5 b0H1(z) Á HN(z)  
 (7.7.17)
This is called a cascade form realization. Second-order block Hi(z) is constructed from 
either two real zeros or a complex-conjugate pair of zeros (and similarly for the poles). 
This way, the coefficients of Hi(z) are guaranteed to be real.
 
Hi(z) 5 1 1 bi1z21 1 bi2z22
1 1 ai1z21 1 ai2z22, 1 # i # N 
 (7.7.18)
If Hi(z) is constructed from zeros zi and zj and poles pq and pr, then the four coeffi-
cients can be computed using sums and products as follows for 1 # i # N.
 
bi1 5 2(zi 1 zj)  
 (7.7.19a)
 
bi2 5 zizj
 
 (7.7.19b)
 
ai1 5 2(pq 1 pr) 
 (7.7.19c)
 
ai2 5 pq pr
 
 (7.7.19d)
Cascade form 
realization
Figure 7.31: Parallel Form Realization from Example 7.11
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

560    Chapter 7  IIR Filter Design
Let ui denote the output of the ith second-order block. Then from (7.7.17) and (7.7.18), a 
cascade form realization is characterized by the following time domain equations.
 
u0(k) 5 b0x(k)
 
 (7.7.20a)
ui(k) 5 ui21(k) 1 bi1ui 2 1(k 2 1) 1 bi2ui21(k 2 2)
 
 2 ai1ui(k 2 1) 2 ai2ui(k 2 2), 1 # i # N  
 (7.7.20b)
 
y(k)  5 uN (k)
 
 (7.7.20c)
Just as with the parallel form, if n is even, there will be N subsystems, each of order two, 
while if n is odd, there will be N 2 1 second-order subsystems plus one first-order sub-
system. The coefficients of a first-order subsystem are obtained from (7.7.19) by setting 
zj 5 0 and pr 5 0.
Any of the direct forms can be used to realize the second-order blocks in (7.7.18). A 
block diagram of the overall structure of a cascade form realization, for the case N 5 2, 
is shown in Figure 7.32. Since the cascade form coefficients must be computed using 
(7.7.19), rather than obtained directly from inspection of H(z), the cascade form realiza-
tion is another example of an indirect form.
Figure 7.32:  
Cascade Form 
Block Diagram, 
N 5 2
H2(z)
u1(k)
u0(k)
H1(z)
x(k)
y(k)
b0
IIR Cascade Form
EXAMPLE 7.12
To compare the cascade form realization with the parallel form realization, con-
sider the fourth-order transfer function introduced earlier in Example 7.11.
H(z) 5
2z(z3 1 1)
f(z 1 .3)2 1 .16g(z 2 .8)(z 1 .7)
The poles are listed in Example 7.11. There is a single zero at z 5 0, and the remain-
ing zeros are the three roots of 21, which are equally spaced around the unit circle.
 z1 5 21
z2, 3 5  cos(y3) 6 j sin(y3)
 z4 5 0
Suppose H1(z) is a block associated with the complex-conjugate pairs of zeros 
hz2, z3j and poles hp1, p2j, and H2(z) is associated with the real zeros and poles. 
Running exam7_12 produces the following three subsystems.
 b0 5 2
H1(z) 5
1 2 z21 1 z22
1 1 .6z21 1 .25z22
H2(z) 5
1 1 z21
1 2 .1z21 2 .56z22
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.7 Filter Realization Structures    561
The DSP Companion contains the following functions for computing indirect form 
realizations of an IIR transfer function.
% F_PARALLEL: Find parallel form filter realization
% F_CASCADE:  Find cascade form digital filter realization
%
% Usage:
% 
[B,A,R0] = f_parallel (b,a)
% 
[B,A,b0] = f_cascade (b,a)
% Pre:
% 
b = vector of length n+1 containing coefficients
% 
 
of numerator polynomial.
% 
a = vector of length n+1 containing coefficients
% 
 
of denominator polynomial.
DSP Companion
DsP Companion
Figure 7.33: Cascade Form Realization from Example 7.12
2
2.6
2.25
21
.1
.56
0
u0
u1
u2
z21
z21
z21
z21
y
x
If a direct form II realization is used for the second-order blocks, then the result-
ing signal flow graph of a cascade form realization of the fourth-order filter is as 
shown in Figure 7.33.
One advantage that the cascade form has over the parallel form is that there is con-
siderable flexibility in the way the zeros and poles can be grouped together to form the 
second-order blocks. Let bi(z) be the numerator associated with the ith pair of zeros, and 
let aj(z) be the denominator associated with the jth pair of poles. Since i and j range from 
1 to N, there are a total of N! possible orderings of the numerators (and similarly for the 
denominators). Hence the total number of ways the numerators and denominators can 
be combined to form second-order blocks is P = (N!)2. All of the orderings are equiva-
lent if infinite precision arithmetic is used. For finite-precision filters, it is recommended 
that pairs of zeros and poles that are close to one another be grouped together in order 
to reduce the occurrence of block outputs that are very large or very small. First the pole 
closest to the unit circle is paired with the nearest zero. This process is repeated until all 
of the poles and zeros are paired. Finally, it is recommended that the blocks be ordered 
either in terms of increasing pole distance from the unit circle or in terms of decreasing 
pole distance (Jackson, 1986).
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

562    Chapter 7  IIR Filter Design
Finite Word length Effects
IIR filters are more general than FIR filters and, because of this, there are certain addi-
tional finite word length effects that are peculiar to IIR filters. We begin by examining 
finite word length effects that the two types of filters have in common. For example, 
input quantization error for an IIR filter is the same as that for an FIR filter. The only 
difference is that when the power gain G in (6.9.11) is used to compute the power of the 
quantization noise at the output, there are an infinite number of terms to sum for an IIR 
filter instead of the finite number for an FIR filter.
 7.8
Optional material
% Post:
% 
B = N by 3 matrix containing coefficients of
% 
 
numerators of second-order blocks.
% 
A = N by 3 matrix containing coefficients of
% 
 
denominators of second-order blocks.
% 
R0 = constant term of parallel form realization.
% 
b0 = numerator gain
% Notes:
% 
1. It is required that length(b) = length(a). If
% 
 
needed, pad b with trailing zeros.
% 
2. It is required that b(1)<>0. Otherwise factor
% 
 
out a z^-1 and then find the parallel or
% 
 
cascade form
Once the parameters of the indirect forms are obtained by calls to f_parallel or 
f_cascade, the indirect forms can be evaluated using the following DSP Companion 
functions. 
% F_FILTPAR: Compute output of parallel form filter realization
% F_FILTCAS: Compute output of cascade form filter realization
%
% Usage:
% 
y = f_filtpar (B,A,R0,x)
% 
y = f_filtcas (B,A,b0,x)
% Pre:
% 
B = N by 2 matrix containing numerator
% 
 
 coefficients of second-order blocks.
% 
A = N by 3 matrix containing denominator
% 
 
 coefficients of second-order blocks.
% 
R0 = direct term of parallel form realization
% 
x = vector of length p containing samples
% 
 
 of input signal.
% 
b0 = numerator gain factor
% Post:
% 
y = vector of length p containing samples of
% 
 
 output signal assuming zero initial
% 
 
 conditions.
%
% Note: The arguments B, A, 0, and b0 are obtained by
% 
calling f_parallel or f_cascade.
*
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 7.8 Finite Word Length Effects    563
7.8.1 Coefficient Quantization Error
The effects of coefficient quantization error are somewhat more involved for an IIR filter 
because one has to consider both the poles and the zeros, not just the zeros as with an 
FIR filter. Recall that if the coefficients range over the interval f2c, cg and N bits are used 
to represent the coefficients, then the quantization level is
 
q 5
c
2N21 
 (7.8.1)
If c 5 2M, then for a fixed-point representation, M 1 1 bits are used for the integer part 
(including the sign), and L 5 N 2 (M 1 1) bits are used for the fractional part. Consider 
an IIR filter with the following transfer function.
 
H(z) 5 b0 1 b1z21 1 Á1 bmz2m
1 1 a1z21 1Á1 anz2n  
 (7.8.2)
The filter parameters, a and b, must be quantized because they are stored in fixed length 
memory locations. Assuming the coefficients of H(z) are quantized to N bits, this results 
in the following quantized transfer function, where QN(x) is the staircase-like quantiza-
tion operator in Definition 6.2.
 
Hq(z) 5 QN(b0) 1 QN(b1)z21 1 Á 1 QN(bm)z2m
1 1 QN(a1)z21 1 Á 1 QN(an)z2n
 
 (7.8.3)
Pole locations
One way to evaluate the effects of coefficient quantization is to look at the locations of 
the poles and zeros. Recall that the roots of a polynomial can be very sensitive to small 
changes in the coefficients of the polynomial, particularly for higher degree polynomi-
als. As a consequence, high-order direct form realizations of H(z) can be very sensitive 
to coefficient quantization error. For example, if H(z) is a narrowband filter with poles 
clustered just inside the unit circle, then some of those poles may migrate across the unit 
circle and render a direct form realization unstable. Even if the poles do not cross the unit 
circle, movement of a pole or a zero near the unit circle can cause a significant change in 
the magnitude response.
Given the sensitivity of the poles and zeros to coefficient quantization, the preferred 
realizations are the indirect cascade and parallel forms based on second-order blocks. For 
both of these realizations, the poles are decoupled from one another with each pair of 
poles associated with its own second-degree polynomial. For the cascade realization, this 
is also true for the zeros. However, for the parallel form realization, the zeros are more 
sensitive to coefficient quantization because the residues in (7.7.11) depend on all of the 
coefficients of H(z).
It is of interest to examine a typical second-order block in more detail. Suppose a 
complex-conjugate pair of poles is located at p 5 r exp(6j). Then the transfer function 
of this block can be written as follows.
H(z) 5
b(z)
fz 2 r exp( j)gfz 2 r exp(2j)g
 5
b(z)
z2 2 rfexp( j) 1 exp(2j)gz 1 r2
 
 5
b(z)
z2 2 2r cos()z 1 r2
 
 (7.8.4)
Quantization level
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

564    Chapter 7  IIR Filter Design
The coefficient vector of the denominator of a second-order block is a 5 f1,22r cos (), 
r2gT. The denominator coefficient vector can be recast in terms of the pole p as follows.
 
a 5 f1, 22Re(p), upu2gT 
 (7.8.5)
Note that the real part of the pole is proportional to a1, but the radius of the pole is pro-
portional to Ïa2. This nonlinear dependence of the pole radius on coefficient a2 means 
that achievable pole locations using quantized versions of a will not be equally spaced. 
Using the stability triangle in Figure 3.20 for stable poles, coefficient a1 must be in the 
range (2c, c) where c 5 2. The distribution of possible pole locations of stable poles for 
the case N 5 5 is shown in Figure 7.34. Note that not only is the grid of possible pole 
locations not uniform, it is also sparse in the vicinity of the real axis.
The problem of nonuniform placement of poles can be circumvented by using a 
coupled-form realization of the denominators of the second-order blocks (Rabiner et al.,  
1970). This realization features parameters that correspond directly to the real and imag-
inary parts of the poles. As a result, coefficient quantization produces a uniform grid of 
realizable pole locations. A disadvantage of the coupled-form realization is that the num-
ber of multiplications is increased in comparison with a direct form realization.
Zero Placement
The placement of zeros of a second-order block is also constrained, as in Figure 7.34. 
For some filters of interest, the zeros are on the unit circle. In these cases, r 5 1 and the 
second-order block transfer function simplifies to
 
H(z) 5 b0(z2 2 2 cos()z 1 1)
a(z)
 
 (7.8.6)
Since b2 5 b0, the zeros of the quantized transfer function remain on the unit circle, only 
their angles (frequencies) change. Thus zeros of cascade form filters that are on the unit 
circle are relatively insensitive to coefficient quantization. Examples of a filters with zeros 
on the unit circle include notch filters and inverse comb filters.
Figure 7.34:  
Realizable Stable 
Pole Locations 
for a Quantizing 
Second-order 
Block with c 5 2 
and N 5 5 
21
20.6
20.2 0
0.6
0.2
1
20.4
20.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Re(z)
Im(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 7.8 Finite Word Length Effects    565
Figure 7.35: Magnitude Responses of a Comb Filter Using a Double-precision 
Floating-point Implementation and a Fixed-point Representation with c 5 2 
and N 5 4
0
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45 0.5
225
220
215
210
25
0
5
f/fs
A(f) (dB)
Unquantized
Quantized
IIR Coefficient Quantization
EXAMPLE 7.13
As an illustration of the detrimental effects of coefficient quantization error, con-
sider a comb filter designed to extract a finite number of isolated equally spaced 
frequencies. From (7.2.15) and (7.2.16), for a filter of order n 5 9 with a pole 
radius of r 5 .98, the comb filter transfer function is
H(z) 5
.1663
1 2 .8337z29
This is a relatively benign example because all but two of the coefficients can 
be represented exactly, only b0 and a9 have quantization error. Suppose, c 5 2 
and N 5 4 bits are used to quantize the coefficients. Thus the coefficient values 
are in the range f22, 2g, and the quantization level is q 5 .25. A comparison of 
the magnitude responses for the double precision floating-point case (64 bits) 
and the N-bit fixed point case is shown in Figure 7.35, where it is evident that 
there is a difference in the magnitude responses, as would be expected given the 
low precision. The attenuation between the frequencies to be extracted is clearly 
inferior for the quantized filter. When N $ 12, the two magnitude responses are 
more or less indistinguishable. For N , 4, the quantized system Hq(z) becomes 
unstable.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

566    Chapter 7  IIR Filter Design
7.8.2 Roundoff Error, overflow, and scaling
As with FIR filters, the arithmetic used to compute an IIR filter output must be per-
formed with finite precision. For example, the output of a general IIR filter can be com-
puted as follows using a direct form II realization.
 
u(k) 5 o
n
i51
aiu(k 2 i) 1 x(k) 
 (7.8.7a)
 
y(k) 5 o
m
i50
biu(k 2 i)
 
 (7.8.7b)
If the coefficients are quantized to N bits, and the signals are quantized to N bits, then 
the product terms will each be of length 2N bits. When the products are rounded to N 
bits, the resulting roundoff error can be modeled as uniformly distributed white noise. In 
this instance there are several sources of noise. Assuming the roundoff noise sources are 
statistically independent of one another, the noise sources associated with the input terms 
can be combined, and similarly for the noise sources associated with the output terms.
 
ea(k) 5
D  o
n
i51
QN faiu(k 2 i)g 2 aiu(k 2 i) 
 (7.8.8a)
 
eb(k) 5
D  o
n
i50
QNfbiu(k 2 i)g 2 biu(k 2 i) 
 (7.8.8b)
Using a second-order direct form II realization, this results in the equivalent linear model 
of roundoff error displayed in Figure 7.36 for the case m 5 n 5 2.
For an IIR filter, the roundoff noise appearing at the output will depend on the 
power gain G. If h(k) is the impulse response of a stable IIR filter, then the power gain is
 
G 5 
D o
`
k50
h2(k) 
 (7.8.9)
It can be shown (e.g. Oppenheim et al, 1999) that the average power of the roundoff 
noise appearing at the filter output is as follows where q is the signal quantization level in 
(7.8.1), and G is the power gain.
 
2
y 5 (Gn 1 m 1 1)q2
12
 
 (7.8.10)
Roundoff error
Power gain
Roundoff noise
Figure 7.36: Linear 
Model of Product 
Roundoff Error in 
a Direct Form II 
Realization
2a1
2a2
u
b0
eb
ea
b1
b2
z21
z21
x
y
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 7.8 Finite Word Length Effects    567
overflow
Another source of error occurs as a result of the summing operations in (7.8.7). The sum 
of several N-bit numbers will not always fit within N bits. When the sum is too large to 
fit, this results in overflow error. A single overflow error can cause a significant change in 
filter performance. This is because when the two’s-complement representation overflows, 
even by a small amount, it goes from a large positive number to a large negative number 
or conversely. This can be seen from the overflow characteristic shown in Figure 7.37, 
where it is assumed that the numbers are fractions in the interval 21 # x , 1.
There are a number of approaches to compensating for overflow. One way is to detect 
overflow and then clip the magnitude of the summing junction output to the maximum 
value that can be represented. This clipping or saturation characteristic is shown with the 
dashed line in Figure 7.37. Clipping tends to reduce, but not eliminate, the detrimental 
effects of overflow error.
scaling
Another approach is to eliminate overflow from occurring at all by the use of scaling. Let 
hi(k) be the impulse response measured at the output of the ith summing node. For the 
second-order block shown in Figure 7.36, there are two summing nodes with
 
h1(k) 5 Z215
1
a(z)6 
 (7.8.11a)
 
h2(k) 5 Z215
b(z)
a(z)6 
 (7.8.11b)
Notice that h1(k) is the impulse response of the auto-regressive or all-pole part, and h2(k) 
is the complete impulse response of H(z). If yi(k) is the output of the ith summing node, 
and ux(k)u # c, then
uyi(k)u 5 uo
`
p50
hi (p)x(k 2 p)u
 # o
`
p50
uhi(p)x(k 2 p)u
 5 o
`
p50
uhi(p)u ? ux(k 2 p)u
 
 # co
`
p50
uhi(p)u
 
 (7.8.12)
Overflow error
Figure 7.37:  
Overflow 
Characteristic of 
Two’s Complement 
Addition
clipping
clipping
21
21
1
1
2
22
ƒ(u)
u
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

568    Chapter 7  IIR Filter Design
Thus uyi(k)u # cuuhiuu1 where uuhiuu is the L1 norm of hi. That is,
 
uuhiuu1 5
D  o
`
k50
uhi(k)u 
 (7.8.13)
The L1 norm of hi in (7.8.13) is an infinite series version of the L1 norm of the coefficient 
vector b in (6.9.26). Recall from Section 2.9 that if the system Hi(z) is BIBO stable, then 
the impulse response hi(k) is absolutely summable. Consequently, for stable filters the 
infinite series in (7.8.13) will converge.
Suppose there are a total of r summing nodes in the signal flow graph. Then addition 
overflow can be prevented if the input signal x(k) is scaled by s1 where scale factor sp is 
defined as follows.
 
sp 5
1
maxi51
r {uuhiuup}, 1 # p # `  
 (7.8.14)
A signal flow graph of a general second-order direct form II realization that uses 
scaling to prevent overflow is shown in Figure 7.38.
Although scaling using the L1 norm is effective in preventing overflow, it does suffer 
from a practical drawback. Roundoff noise and input quantization noise are not significantly 
affected by scaling. As a consequence, when the input is scaled by s1, the resulting reduction in 
signal strength can cause a corresponding reduction in the signal-to-noise ratio. A less severe 
form of scaling can be used that eliminates most, but not all, overflow. In those instances 
where overflow does occur, it can be compensated for by using clipping. If the input signal 
is a pure sinusoid, then overflow from this type of periodic input can be eliminated by using 
scaling that is based on the filter magnitude response. Here the L` norm of Hi is used where
 
uuHiuu` 5
D   max 
0#f#fsy2{uHi( f )} 
 (7.8.15)
Addition overflow from a pure sinusoidal input is prevented if the input signal x(k) is 
scaled by s` where s` is computed as in (7.8.14) with hi replaced by Hi. The most common 
form of scaling uses the L2 or energy norm, which is defined as follows.
 
uuhiuu2 5
D  1o
`
k50
uhi(k)u22
1y2
 
 (7.8.16)
Again the scale factor s2 is computed using (7.8.14). One advantage of the L2 norm is that 
it is relatively easy to compute. In fact, for certain realizations, closed-form expressions 
for s2 can be computed in terms of the filter coefficients (Ifeachor and Jervis, 2002). The 
L2 norm of hi in (7.8.16) is a generalization of the Euclidean norm of the coefficient vec-
tor b in (6.9.28). Like their finite-dimensional counterparts, the L1, L2, and L` norms can 
be shown to satisfy the following relationship.
 
uuhuu2 # uuH uu` # uuhuu1 
 (7.8.17)
Scale factor
Figure 7.38:  
Scaling to Prevent 
Addition Overflow 
in a Second-order 
Direct Form II 
Block
2a1
s1
1/s1
2a2
u
b0
b1
b2
z21
z21
x
y
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 7.8 Finite Word Length Effects    569
IIR overflow and scaling
EXAMPLE 7.14
As an illustration of the prevention of overflow by scaling, suppose ux(k)u # 5, 
and consider the following IIR filter.
H(z) 5
4z21
1 2 .64z22
Here c 5 5, b 5 f0, 4, 0gT, and a 5 f1, 0, 2.64gT. Expressing H(z) in terms of pos-
itive powers of z and factoring the denominator yields
H(z) 5
4z
(z 2 .8)(z 1 .8)
From (7.8.11a), the impulse response of the auto-regressive part of H(z) is
h1(k) 5 Z215
1
(z 2 .8)(z 1 .8)6
 5 .625f(.8)k21 2 (2.8)k21g(k 2 1)
 5 .78125f(.8)k 2 (2.8)kg(k)
Similarly, from (7.8.11b), the impulse response of H(z) is
h2(k) 5 Z215
4z
(z 2 .8)(z 1 .8)6
 5 2.5f(.8)k 2 (2.8)kg(k)
Notice that h2(k) $ h1(k) for k $ 0. Therefore it is sufficient to compute the norm 
of h(k) 5 h2(k). From (7.8.13), the L1 norm is
uuhuu1 5 2.5o
`
k50
u(.8)k 2 (2.8)ku
 5 5o
`
i50
(.8)2i
 5 5o
`
i50
(.64)i
 5
5
1 2 .64
 5 13.89
Thus a scale factor which will eliminate fixed-point overflow is s1 5 1y13.89 or
s1 5 .072
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

570    Chapter 7  IIR Filter Design
7.8.3 limit Cycles
For IIR filters, there is an unusual finite word length effect that is sometimes observed 
when the input goes to zero. Recall that if a filter is stable and the input goes to zero, then 
the output should approach zero as the natural mode terms die out. However, for finite-
precision IIR filters the output sometimes approaches a nonzero constant, or it oscillates. 
These zero-input oscillations are a nonlinear phenomenon called limit cycles. There are 
two types of limit cycles. The first is a limit cycle that is caused by overflow error. Over-
flow limit cycles can be quite large in amplitude, but they can be eliminated if the outputs 
of the summing junctions are clipped. Of course, scaling by s1 will also eliminate this type 
of limit cycle. The second type of limit cycle is a small limit cycle of amplitude q that can 
occur as a result of product roundoff error.
Limit cycles
Figure 7.39: Scaling to Prevent Addition Overflow for the System in 
Example 7.14
.072
13.89
u
0
0
.64
4
0
z21
z21
x
y
A signal flow graph of a direct form II realization with scaling to avoid overflow 
is shown in Figure 7.39.
limit Cycle
EXAMPLE 7.15
As an illustration of a limit cycle caused by product roundoff error, consider the 
following quantized first-order IIR system.
y(k) 5 QN f2.7y(k 2 1)g 1 3x(k)
Suppose N 5 4 bits are used with a scale factor of c 5 4. In this case, the quan-
tization level is
q 5 4
23 5 .5
Next consider the impulse response of the quantized system, hq(k). The result, 
computed using exam7_15, is shown in Figure 7.40. Note that even though H(z) 
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 7.8 Finite Word Length Effects    571
In IIR filters, limit cycle solutions can be small limit cycles associated with roundoff 
error as in Figure 7.40, or large amplitude limit cycles associated with overflow. For FIR 
filters, limit cycles are not possible because there are no feedback paths to sustain an 
oscillation. Indeed, in comparison with IIR filters, FIR filters are less sensitive to finite 
word length effects in general. Along with their guaranteed stability, this is one of the 
advantages of FIR filters that accounts for their popularity.
Figure 7.40: Limit Cycle in Impulse Response Caused by Product Roundoff 
Error Using N 5 4 Bits and a Scale Factor of c 5 4 
0
5
10
15
20
25
30
22
0
2
k
hq(k)
0
5
10
15
20
25
30
22
0
2
k
h(k)
clearly is stable with a pole at p 5 2.7, the steady-state response does not go to 
zero. Instead it oscillates with period two and amplitude q because of the product 
roundoff error. For comparison, the unquantized impulse response, h(k), is also 
displayed.
(Continued)
DSP Companion
DsP Companion
% F_FILTER1: Compute the quantized zero-state response of an IIR filter
% F_IMPULSE: Compute the quantized impulse response of an IIR filter
% F_FREQZ: Compute frequency response of an IIR filter using the DFT
%
% Usage:
% 
y 
= f_filter1 (b,a,x,bits,realize);
% 
[h,k] = f_impulse (b,a,N,bits,realize);
% 
[H,f] = f_freqz (b,a,N,fs,bits,realize);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

572    Chapter 7  IIR Filter Design
% Pre:
% 
b  
 = coefficient vector of numerator polynomial
% 
a  
=  coefficient vector of denominator polynomial
% 
x  
= a vector of length N containing the input
% 
bits  
= optional integer specifying the number of
% 
 
 fixed-point bits used for coefficient quantization.
% 
 
 The default is double precision floating-point
% 
realize = optional integer specifying the realization
% 
 
 structure to use. The default is to use the
% 
 
 direct form of MATLAB function filter.
%
% 
0 = direct form
% 
1 = cascade form
% 
2 = lattice form (FIR) or parallel form (IIR)
%
% 
N 
= number of samples
% 
fs 
= sampling frequency (default = 1)
% Post:
% 
y = N by 1 vector containing the zero-state response
% 
h = N by 1 vector containing the impulse response
% 
k = N by 1 vector containing discrete times
% 
H = 1 by N+1 complex vector containing the frequency response
% 
f = 1 by N+1 vector containing discrete frequencies (0 to fs/2)
% Note:
% 
For the parallel form, the poles of H(z) must be distinct.
gUI Modules and Case studies 
This section focuses on the design and realization of IIR filters. A graphical user interface 
module called g_iir is introduced that allows the user to design and implement IIR filters, 
without any need for programming. A case study programming example using the DSP 
Companion functions is then presented.
g_iir: Design and implement IIR filters 
The DSP Companion includes a GUI module called g_iir that allows the user to design 
a variety of IIR filters. GUI module g_iir features a display screen with tiled windows as 
shown in Figure 7.41. The design features of g_iir are summarized in Table 7.8.
The upper left-hand Block diagram window contains a block diagram of the IIR 
filter under investigation. It is an nth order filter with the following transfer function.
 
H(z) 5 b0 1 b1z21 1 Á 1 bnz2m
1 1 a1z21 1 Á 1 anz2n  
 (7.9.1)
The Parameters window below the block diagram displays edit boxes containing the 
filter parameters. The contents of each edit box can be directly modified by the user 
with the changes activated with the Enter key. Parameters F0, F1, B, and fs are the lower 
cutoff frequency, upper cutoff frequency, transition bandwidth, and sampling frequency, 
respectively. The lowpass filter uses cutoff frequency F0, the highpass filter uses cutoff fre-
quency F1, and the bandpass and bandstop filters use both F0 and F1. For resonators and 
notch filters, F0 is used. The parameters deltap 5 p and deltas 5 s specify the passband 
ripple and stopband attenuation, respectively.
7.9
gUI Module
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.9 GUI Modules and Case Studies     573
Select Type
Select View
Slider Bar
Edit Parameters
0
100
200
300
400
500
600
700
800
900
1000
0
0.2
0
0.2
0.4
0.6
0.8
1
0
0.5
1
0.4
0.6
0.8
1
Chebyshev{I filter, n 5 8
f (Hz)
A(f)
FIR filter
Specifications
g_iir
x(k)
H(z)
y(k)
Figure 7.41: Display Screen for GUI Module g_iir
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

574    Chapter 7  IIR Filter Design
The Type and View windows in the upper-right corner of the screen allow the user 
to select both the type of frequency-selective filter and the viewing mode. The filter types 
include a resonator filter, a notch filter, lowpass, highpass, bandpass, and bandstop filters, 
and a user-defined filter whose coefficients are imported from a MAT file that contains a, 
b, and the sampling rate fs. The View options include the magnitude response, the phase 
response, and a pole-zero plot that also contains the impulse response. The Plot window 
along the bottom half of the screen shows the selected view.
Just below the view options is a checkbox control that toggles the magnitude response 
display between linear and logarithmic scales. When it is checked, the passband ripple 
and stopband attenuation in the Parameters window change to their logarithmic equiva-
lents, Ap and As, respectively. Below the Type and View windows is a horizontal slider bar 
that allows the user to directly control the filter order n. Note that the filter order may or 
may not meet all of the design specifications depending on the value of n. Furthermore, 
for some filters, when n is set too high the filter implementation can become unstable due 
to finite word length effects.
The Menu bar at the top of the screen includes several menu options. The Export 
option is used to save a, b, x, y, fs to a MAT file. Results exported from one GUI module 
can be imported into other GUI modules for additional processing. The Caliper option 
allows the user to measure any point on the current plot by moving the mouse crosshairs 
to that point and clicking. The Prototype option allows the user to select the analog pro-
totype filter (Butterworth, Chebyshev-I, Chebyshev-II, or elliptic) for use with the basic 
frequency-selective filter types. The Print option sends the GUI window to a printer or a 
file. Finally, the Help option provides the user with some helpful suggestions on how to 
effectively use module g_iir.
Reverb Filter
Music generated in a concert hall sounds rich and full because it arrives at the listener 
along multiple paths, both direct and through a series of reflections. This reverberation 
effect can be simulated by processing the sound signal with a reverb filter (Steiglitz, 1996). 
The speed of sound in air at room temperature is about v 5 345 m/s. Suppose the dis-
tance from the music source to the listener is d m. If fs denotes the sampling rate, then 
the distance in samples is
 
L 5 floor1
fsd
v 2 
 (7.9.2)
Case Study 7.1
Item 
Variables 
Block diagram 
x(k), y(k) 
Edit parameters 
F0, F1, B, fs, p, s 
Filter type 
resonator, notch, lowpass, highpass, bandpass, bandstop, import 
Plot view 
magnitude response, phase response, pole-zero plot, impulse response 
Slider 
filter order n 
Check boxes 
linear/dB scale 
Menu buttons 
prototype, export, caliper, print, help, exit 
Import 
a, b, fs 
Export 
a, b, x, y, fs 
Table 7.8: Features of 
GUI Module g_iir
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.9 GUI Modules and Case Studies     575
Sound is attenuated as it travels through air and becomes dispersed. Let 0 , r , 1 be 
the factor by which sound is attenuated as it propagates from the source to the listener. 
To roughly approximate a reverberation effect, suppose the echoes from one or more 
reflections arrive at multiples of L samples and are attenuated by powers of r. If there are 
n echoes, or paths of increasing length, then this effect can be modeled by the following 
difference equation.
 
y(k) 5 o
n
i51
rix(k 2 Li) 
 (7.9.3)
In the limit as the number of echoes, n, approaches infinity, we arrive at the following 
geometric series transfer function for multiple echoes.
F(z) 5 o
`
i51
riz2Li
 
 5
rz2L
1 2 rz2L 
 (7.9.4)
Observe that F(z) is essentially a comb filter with poles equally spaced around a circle of 
radius r. A block diagram of the comb filter that explicitly shows the feedback path taken 
by the echos is shown in Figure 7.42.
To develop a more refined model of the reverberation effect, one should take into 
consideration the observation that high-frequency sounds tend to get absorbed more 
than low-frequency sounds. This effect can be included by inserting a first-order lowpass 
filter, D(z), into the feedback path of the basic comb filter (Moorer, 1979).
 
D(z) 5
1 2 g
1 2 gz21 
 (7.9.5)
Here the real pole 0 , g , 1 controls the cutoff frequency of the lowpass characteristic. 
A block diagram showing this more refined lowpass comb filter is shown in Figure 7.43. 
The transfer function of the lowpass comb filter can be obtained from Figure 7.43 by 
solving for the summing junction output signal U(z). Working backwards around the 
loop we have
U(z) 5 X(z) 1 D(z)Y(z)
 
 5 X(z) 1 D(z)rz2LU(z) 
 (7.9.6)
Solving (7.9.6) for U(z) then yields
 
U(z) 5
X(z)
1 2 rz2LD(z) 
 (7.9.7)
Figure 7.42: Basic 
Comb Filter used 
to Model Multiple 
Echoes
1
Attenuated
delay
Echo
x
y
rz2L
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

576    Chapter 7  IIR Filter Design
Finally from Figure 7.43, (7.9.7), and (7.9.5), the output of the lowpass comb filter is
Y(z) 5 rz2LU(z)
 5
rz2LX(z)
1 2 rz2LD(z)
 5
rz2LX(z)
1 2 rz2L(1 2 g)y(1 2 gz21)
 
 5
rz2L(1 2 gz21)X(z)
1 2 gz21 2 r(1 2 g)z2L
 
 (7.9.8)
Multiplying both sides of (7.9.8) by zL11, the overall transfer function of the lowpass 
comb filter, in terms of positive powers of z, is
 
C(z) 5
r(z 2 g)
zfzL 2 gzL21 2 r(1 2 g)g  
 (7.9.9)
The reverberation effect can be made much fuller and richer when multiple lowpass 
comb filters with different delays and cutoff frequencies are used. Moorer (1979) has 
proposed using multiple lowpass comb filters in parallel followed by an allpass filter. The 
allpass filter inserts a frequency-dependent phase shift, or delay, but does not change the 
magnitude response. Recall from (5.4.7) that an allpass filter is a filter whose numerator 
and denominator exhibit reverse symmetry. For example, the following Mth order allpass 
filter can delay signals up to M samples.
 
G(z) 5 c 1 z2M
1 1 cz2M  
 (7.9.10)
The overall configuration for a reverb filter, featuring P lowpass comb sections, is 
shown in Figure 7.44. The composite transfer function of the reverb filter is as follows.
 
H(z) 5 G(z)o
P
i51
Ci(z) 
 (7.9.11)
Figure 7.43:  
Lowpass Comb 
Filter that 
Includes Effects 
of Frequency-
dependent Sound 
Absorption
1
x
Lowpass
u
y
rz2L
12 gz21
12 g
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.9 GUI Modules and Case Studies     577
To implement the reverb filter in Figure 7.44, the following parameters can be used 
(Moorer, 1979). For the allpass filter set
 
 c 5 .7
 
 (7.9.12a)
 
M 5 floor(.006fs) 
 (7.9.12b)
This corresponds to a delay of .006 sec. Next, use P 5 6 lowpass comb filters with the 
following attenuation factor, delays, and poles.
 
r  5 .83
 
 (7.9.13a)
 
L 5 floorhf.050, .056, .061, .068, .072, .078g fsj 
 (7.9.13b)
 
g  5 f.24, .26, .28, .29, .30, .32g
 
 (7.9.13c)
The DSP Companion contains a function called f_reverb that computes the reverb filter 
output using the parameters in (7.9.12) and (7.9.13). The reverb filter can be tested by 
running case7_1 from g_dsp.
When case7_1 is run, it first computes the impulse response shown in Figure 7.45. 
Unlike many of the discrete-time systems we have investigated, the impulse response of 
this system takes a very long time to die out because of the significant delays representing 
echos in the system.
The second part of case7_1 computes the magnitude response shown in Figure 7.46. 
The interactions of the multiple lowpass comb filters provide a magnitude response that 
is broadband, yet exhibits detailed variation. This is due, in part, to the very high order 
of the reverb filter. From (7.9.9), (7.9.10), and Figure 7.44, the total filter order is as 
follows.
 
n 5 M 1 P 1 L1 1 Á 1 LP 
 (7.9.14)
Figure 7.44: A 
Reverb Filter
1
x
y
Cp(z)
C2(z)
G(z)
C1(z)
???
???
???
Lowpass
comb
Allpass
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

578    Chapter 7  IIR Filter Design
Recall from (7.9.12) and (7.9.13) that the allpass order M and the delays, Li, are propor-
tional to the sampling frequency, which is set to fs 5 8000 Hz in case7_1. Using (7.9.12) 
through (7.9.14) this results in an IIR reverb filter order n where
n 5 floor(.006fs) 1 6 1 floorhf.050 1 .056 1 .061 1 .068 1 .072 1 .078gfsj
 
 5 3134
 
 (7.9.15)
Since the reverb filter is stable, this means that all 3134 poles must be inside the unit 
circle! The final segment of case7_1 displays a menu that allows the user to record up to 
four seconds of sound from the PC microphone, and then play it back on the PC speaker 
both with and without reverb filtering. The differences in the sound are distinct and quite 
striking. Give it a try!
Figure 7.45:  
Impulse Response 
of a Reverb Filter
0
1000 2000 3000 4000 5000 6000 7000 8000 9000
20.4
20.3
20.2
20.1
0
0.1
0.2
0.3
0.4
0.5
0.6
k
 h(k)
Figure 7.46:  
Magnitude 
Response of a 
Reverb Filter
0
500
1000 1500 2000 2500 3000 3500 4000
0
5
10
15
20
25
30
f (Hz)
A(f)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.10 Chapter Summary     579
Chapter summary 
learning outcomes
This chapter was designed to provide the student with an opportunity to achieve the 
learning outcomes summarized in Table 7.9.
Infinite Impulse Response Filters
Chapter 7 focused on the design of infinite impulse response, or IIR, digital filters with 
the following transfer function.
 
H(z) 5 b0 1 b1z21 1 Á1 bmz2m
1 1 a1z21 1 Á 1 anz2n  
 (7.10.1)
The four basic frequency-selective filter types are lowpass, highpass, bandpass, and band-
stop filters. For an ideal filter, the passband gain is A( f ) 5 1, the stopband is A( f ) 5 0, 
and there is no transition band. However, for a practical filter of finite order there must 
be a transition band separating the passband and stopband. Furthermore, for a physi-
cally realizable filter the passband gain is not constant but lies instead within an interval 
f1 2 p,1g where p . 0 is the passband ripple. Similarly, the stopband gain lies within an 
interval f0, sg where s . 0 is the stopband attenuation. The filter magnitude response is 
often represented using a logarithmic scale of decibels (dB) as follows.
 
A( f ) 5 20 log10{uH( f )u} dB 
 (7.10.2)
The ripple and attenuation specifications, p and s, have logarithmic equivalents, Ap and 
As, that are expressed in units of dB. The logarithmic scale is useful for showing the 
amount of attenuation in the stopband.
There are a number of specialized IIR filters that can be designed using pole-zero 
placement and gain matching. These include a resonator, which is designed to pass a single 
frequency, and a notch filter, which is designed to reject a single frequency. Generaliza-
tions of these two basic filters include the comb filter which is designed to pass several 
7.10
Passband ripple
Stopband attenuation
Resonator
Notch filter
Table 7.9: Learning  
Outcomes for  
Chapter 7
Num.
learning outcome
sec. 
1 
Know how to design a tunable plucked string filter for music synthesis 
7.1 
2 
Be able to design resonators, notch filters, and comb filters using  
pole-zero placement with gain matching 
7.2
3 
Understand the characteristics and relative advantages of classical  
lowpass analog Butterworth, Chebyshev, and elliptic filters
7.4
4 
Know how to convert an analog filter into an equivalent digital filter using the 
bilinear transformation method
7.5 
5 
Be able to convert a normalized lowpass filter into lowpass, highpass, bandpass, 
or bandstop filters using frequency transformations
7.6 
6 
Understand the benefits of different filter realization structures 
7.7 
7 
Be aware of detrimental finite word length effects and know how to  
minimize them
7.8 
8 
Know how to use the GUI module g_iir to design and analyze digital IIR filters 
without any need for programming
7.9 
9 
Know how to add special effects to music and speech using reverb filters
7.9 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

580    Chapter 7  IIR Filter Design
harmonically related frequencies, and the inverse comb filter which is designed to reject 
several harmonically related frequencies. Comb filters and inverse comb filters can be used 
to pass or reject a periodic input that is corrupted with noise if the first resonant frequency 
is set to match the fundamental frequency of the periodic input.
Classical Analog Prototype Filters
A highly effective and widely used approach for designing frequency-selective IIR filters 
starts with an analog prototype filter and then transforms it to an equivalent digital filter. 
There are four classical families of analog filters that are used for prototype filters, and 
each is optimal in some sense. Butterworth filters have the property that their magnitude 
responses are as flat as possible in the passband. Butterworth filters are easy to design, 
but they have a relatively wide transition band. The transition band can be made more 
narrow by allowing ripples in the magnitude response. Chebyshev-I filters have ripples 
of equal amplitude in the passband and meet the passband specifications exactly, while 
Chebyshev-II filters have ripples of equal amplitude in the stopband and meet the stop-
band specification exactly. Elliptic filters have a narrow transition band that is achieved 
by allowing ripples of equal size in both the passband and the stopband. Thus elliptic 
IIR filters are analogous to the equiripple FIR filters designed with the Parks-McLellan 
algorithm.
For the classical filters, the filter order required to meet a given design specification 
can be computed using two design parameters called the selectivity factor, r, and the dis-
crimination factor, d. For an ideal filter r 5 1 and d 5 0, whereas for a practical filter 
r , 1 and d . 0. The classical analog lowpass filters Ha(s) can be designed by starting 
with the magnitude response Aa( f ) and working backwards to determine the poles, zeros, 
and gain using the following fundamental relationship.
 
Ha(s)Ha(2s) 5 A2
a 1
s
 j22 
 (7.10.3)
Bilinear Analog-to-digital Filter Transformation
Analog frequency-selective prototype filters of various types can be obtained by apply-
ing frequency transformations to a normalized lowpass filter, a filter whose radian cutoff 
frequency is V0 5 1 rad/s. For bandpass and bandstop filters, these frequency transfor-
mations double the order of the filter. It is also possible to perform frequency trans-
formations on digital lowpass filters. The most commonly used analog-to-digital filter 
transformation technique is the bilinear transformation method. This technique maps a 
stable analog filter Ha(s) into a stable digital filter H(z) using the following change of 
variables in Ha(s).
 
s 5 2
T 1
z 2 1
z 1 12 
 (7.10.4)
The bilinear transformation method maps the imaginary axis of the s plane onto the unit 
circle of the z plane. The resulting compression of frequencies is called frequency warping, 
and it must be taken into account when specifying the critical frequencies of the filter. 
That is, in constructing the analog prototype filter each of the desired cutoff frequencies 
should first be prewarped using
 
F 5
 tan(f T )
T
 
 (7.10.5)
Prototype filter
Design parameters
Normalized filter
Bilinear transformation
Frequency warping
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.10 Chapter Summary     581
IIR filters can be used for music synthesis and to create special sound effects. For 
example, the impulse response of the tunable plucked string filter can be adjusted to 
emulate the sound produced by a variety of stringed instruments. The rich, full sound of 
concert halls can be reproduced by introducing special sound effects using a reverb filter. 
A reverb filter is a high-order IIR filter that is implemented as a parallel configuration of 
comb filters, each having a lowpass filter in their feedback path, followed by an allpass 
filter.
Filter Realization structures
There are a number of alternative signal flow graph realizations of IIR filters. Direct 
form realizations have the property that the gains in the signal flow graphs are obtained 
directly from inspection of the transfer function. For IIR filters, direct form realizations 
include the direct form I, direct form II, and transposed direct form II structures. The 
direct form II realizations are canonic in the sense that they require the minimum number 
of memory locations to store past signal samples.
There are also a number of indirect realizations whose parameters must be computed 
from the original transfer function. The indirect forms decompose the original transfer 
function into lower-order blocks by combining complex-conjugate pairs of poles and 
zeros. For example, IIR filters can be realized with the following cascade form realization 
that is based on factoring H(z).
 
H(z) 5 b0H1(z) Á HN(z) 
 (7.10.6)
Here N 5 floorf(n 1 1)y2g and the Hi(z) are second-order blocks with real coefficients 
except for HN(z), which is a first-order block when the filter order n is odd. For IIR filters, 
an additional indirect filter structure is the parallel form realization that is based on a 
partial fraction expansion of H(z).
 
H(z) 5 R0 1 o
N
i51
Hi(z) 
 (7.10.7)
Again, N 5 floorf(n 1 1)y2g and each Hi(z) is a second-order block with real coefficients 
except for HN(z), which is a first-order block when n is odd. All of the filter realizations 
are equivalent to one another in terms of their overall input-output behavior if infinite 
precision arithmetic is used.
*Finite Word length Effects
Finite word length effects arise when a filter is implemented in either hardware or soft-
ware. They are caused by the fact that both the filter parameters and the filter signals must 
be represented using a finite number of bits of precision. Both floating-point and fixed-
point numerical representations can be used. MATLAB uses a double-precision 64-bit 
floating-point representation that tends to minimize finite word length effects. When an 
N-bit fixed point representation is used for values in the range f2c, cg, the quantization 
level, or spacing between adjacent values, is
 
q 5
c
2N21 
 (7.10.8)
Typically, the scale factor is c 5 2M for some integer M $ 0. This way, M 1 1 bits are 
used to represent the integer part including the sign, and the remaining L 5 N 2 (M 1 1) 
bits are used for the fraction part.
Music synthesis
Direct forms
Cascade form
Parallel form
Optional material
Quantization level
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

582    Chapter 7  IIR Filter Design
Quantization error can arise from input or ADC quantization, coefficient quantiza-
tion, and product roundoff quantization. It is modeled using additive white noise uni-
formly distributed over f2qy2, qy2g. Another source of error is overflow error, which can 
occur when several finite precision numbers are added. Overflow error can be reduced by 
clipping or eliminated by proper scaling of the input. The roots of a polynomial are very 
sensitive to the changes in the polynomial coefficients, particularly for high-degree poly-
nomials. It is for this reason that IIR filter implementations can become unstable when 
their poles migrate across the unit circle as a result of quantization error. In addition, 
overflow error and roundoff error can cause steady-state oscillations in an IIR filter out-
put after the input goes to zero. These oscillations are called limit cycles. For IIR filters, 
the indirect form realizations tend to be less sensitive to finite word length effects because 
the block transfer functions are only of second order. IIR filters are more sensitive to 
finite word length effects than FIR filters.
gUI Module
The DSP Companion includes a GUI module called g_iir that allows the user to design 
and compare IIR filters without any need for programming. The DSP Companion also 
includes GUI module g_filters, described in Section 5.9, that allows the user to import 
filters from g_iir. In this way, one can compare different realization structures and explore 
the effects of coefficient quantization error.
Problems
The problems are divided into Analysis and Design problems that can be solved by hand 
or with a calculator, GUI Simulation problems that are solved using GUI module g_iir, 
and MATLAB Computation problems that require a user program. Solutions to selected 
problems can be accessed with the DSP Companion driver program, g_dsp. Students 
are encouraged to use these problems, which are identified with a 
, as a check on their 
understanding of the material.
7.11.1 Analysis and Design
section 7.1: Motivation
7.1 
Consider the problem of designing a filter whose impulse response emulates the 
sound from a stringed musical instrument. Suppose the sampling frequency is 
fs 5 44.1 kHz and the desired resonant frequency or pitch is F0 5 480 Hz.
(a) Find the feedback parameter L and the pitch parameter c in Figure 7.1.
(b) Suppose the attenuation factor is r 5 .998. Find the tunable plucked-string 
filter transfer function H(z).
section 7.2: Filter Design by Pole-zero Placement
7.2 
Consider the problem of designing a resonator that extracts the frequency F0 5 100 Hz.
(a) Find a sampling frequency fs that places the resonator pole at an angle of 
0 5 y2.
Limit cycles
7.11
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.11 Problems    583
(b) Design a resonator Hres(z) that has a 3-dB passband radius of DF 5 2 Hz.
(c) Sketch a signal flow graph using a direct form II realization.
7.3 
Consider the problem of designing a resonator that has two resonant frequencies. 
Suppose the sampling frequency is fs 5 360 Hz.
(a) Design a resonator H0(z) that has a resonant frequency at F0 5 90 Hz and a 
3-dB passband radius of 3 Hz.
(b) Design a resonator H1(z) that has a resonant frequency of F1 5 120 Hz and a 
3-dB passband radius of 4 Hz.
(c) Combine H0(z) and H1(z) to produce a resonator H(z) that has resonant 
frequencies at F0 5 90 Hz and F1 5 120 Hz. Hint: Use one of the indirect  
forms.
(d) Sketch the signal flow graph of H(z) using direct form II realizations for the 
blocks H0(z) and H1(z).
7.4 
Consider the problem of designing a notch filter that eliminates the frequency 
F0 5 60 Hz.
(a) Suppose the notch filter pole is at the angle 0 5 y3. Find the sampling fre-
quency fs.
(b) Design a notch filter Hnotch(z) that has a 3-dB stopband radius of DF 5 1 Hz.
(c) Sketch the signal flow graph using a transposed direct form II realization.
7.5 
Consider the problem of designing a notch filter that has two notch frequencies. 
Suppose the sampling frequency is fs 5 360 Hz.
(a) Design a notch filter H0(z) that has a notch frequency at F0 5 60 Hz and a 
3-dB stopband radius of 2 Hz.
(b) Design a notch filter H1(z) that has a notch frequency at F0 5 90 Hz and a 
3-dB stopband radius of 2 Hz.
(c) Combine H0(z) and H1(z) to produce a notch filter H(z) that has notches at 
F0 5 60 Hz and F1 5 90 Hz. Hint: Use one of the indirect forms.
(d) Sketch the signal flow graph of H(z) using direct form II realizations for the 
blocks H0(z) and H1(z).
7.6 
Consider an input signal y(k) that consists of a periodic component x(k) plus a 
random white noise component v(k).
y(k) 5 x(k) 1 v(k), 0 # k , 256
Suppose the sampling rate is fs, and this results in a signal x(k) that is periodic 
with a period of L 5 16. Design a comb filter Hcomb(z) that passes harmonics zero 
through Ly2 of x(k). Use a 3-dB passband radius of DF 5 fsy100.
7.7 
Consider an input y(k) that consists of a signal of interest, x(k), plus a disturbance, 
d(k).
y(k) 5 x(k) 1 d(k), 0 # k , N
Suppose that when the sampling rate is fs, the disturbance d(k) is periodic with a 
period of L 5 12. Design an inverse comb filter Hinv(z) that removes harmonics 
zero through Ly2 of d(k) from y(k). Use a 3-dB passband radius of DF 5 fsy200.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

584    Chapter 7  IIR Filter Design
section 7.3: Filter Design Parameters
7.8 
Consider the problem of designing a lowpass analog filter Ha(s) to meet the follow-
ing specifications.
fFp, Fs, p, sg 5 f1000, 1200, .05, .02g
(a) Find the passband ripple and stopband attenuation in units of dB.
(b) Find the selectivity factor, r.
(c) Find the discrimination factor, d.
7.9 
Consider the following design specifications for a lowpass analog filter.
fFp, Fs, p, sg 5 f50, 60, .05, .02g
Find the minimum-order filter needed to meet these specifications using the follow-
ing classical analog filters.
(a) Butterworth filter
(b) Chebyshev-I filter
(c) Chebyshev-II filter
section 7.4: Classical Analog Filters
7.10 Consider the problem of designing a lowpass analog Butterworth filter to meet the 
following specifications.
fFp, Fs, p, sg 5 f300, 500, .1, .05g
(a) Find the minimum filter order n.
(b) For what cutoff frequency Fc is the passband specification exactly met?
(c) For what cutoff frequency Fc is the stopband specification exactly met?
(d) Find a cutoff frequency Fc for which Ha(s) exceeds both the passband and the 
stopband specification.
7.11 Find the transfer function H(s) of a third-order analog lowpass Butterworth filter 
that has a 3-dB cutoff frequency of Fc 5 4 Hz.
7.12 Sketch the poles and zeros of an analog lowpass Butterworth filter of order n 5 8 
that has a 3-dB cutoff frequency of Fc 5 1y Hz.
7.13 Consider the problem of designing an analog lowpass Chebyshev-I filter to meet 
the following design specifications. Find the minimum order of the filter.
fFp, Fs, p, sg 5 f100, 200, .03, .05g
7.14 Design a second-order analog lowpass Chebyshev-I filter, Ha(s), using Fp 5 10 Hz 
and p 5 .1.
7.15 Find the minimum order n of an analog elliptic filter that will meet the following 
design specifications. You can use the MATLAB function ellipke to evaluate an 
elliptic integral of the first kind.
fFp, Fs, p, sg 5 f100, 200, .03, .05g
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.11 Problems    585
section 7.5: Bilinear Transformation Method
7.16 Consider the following first-order analog filter.
Ha(s) 5
s
s 1 4
(a) What type of frequency-selective filter is this (lowpass, highpass, bandpass, or 
bandstop)?
(b) What is the 3-dB cutoff frequency f0 of this filter?
(c) Suppose fs 5 10 Hz. Find the prewarped cutoff frequency F0.
(d) Design a digital equivalent filter H(z) using the bilinear transformation 
method.
7.17 The simplest digital equivalent filter is one that preserves the impulse response of 
Ha(s). Let ha(t) denote the desired impulse response.
ha(t) 5 L21{Ha(s)}
Next let T be the sampling interval. The objective is to design a digital filter H(z) 
whose impulse response h(k) satisfies
h(k) 5 ha(kT ), k $ 0
Thus the impulse response of H(z) consists of samples of the impulse response of 
Ha(s). This design technique, which preserves the impulse response, is called the 
impulse-invariant method. Suppose Ha(s) is a stable, strictly proper, rational poly-
nomial with n distinct poles hp1, p2, Á , pnj.
(a) Expand Ha(s)ys into partial fractions.
(b) Find the impulse response ha(t).
(c) Sample ha(t) to find the impulse response h(k).
(d) Find the transfer function H(z).
7.18 Consider the following analog prototype filter of order n 5 2.
Ha(s) 5
6
s2 1 5s 1 6
(a) Find the poles of Ha(s)ys.
(b) Find the residues of Ha(s)ys at each pole.
(c) Find a digital equivalent transfer function using the impulse-invariant method 
in Problem 7.17. You can assume the sampling interval is T 5 .5 sec.
7.19 Consider the following analog filter that has n poles and m zeros with m # n.
Ha(s) 5 (s 2 z1)(s 2 z2) Á (s 2 zm)
(s 2 p1)(s 2 p2) Á (s 2 pn)
An alternative way to convert an analog filter into a digital filter is to map each pole 
and zero of Ha(s) into a corresponding pole and zero of H(z) using z 5 exp(sT). 
This yields:
H(z) 5 b0(z 1 1)n2mfz 2 exp(z1T )gfz 2 exp(z2T )g Á fz 2 exp(zmT )g
fz 2 exp( p1T )gfz 2 exp( p2T )g Á fz 2 exp(pnT )g
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

586    Chapter 7  IIR Filter Design
Note that if n . m, then Ha(s) has n 2 m zeros at s 5 `. These zeros are mapped 
into the highest digital frequency, z 5 21. The gain factor b0 is selected such 
that the two filters have the same passband gain. For example if Ha(s) is a low-
pass filter, then Ha(0) 5 H(1). This method, which is analogous to Algorithm 7.1  
but using a different transformation, is called the matched Z-transform method. 
Use the matched Z-transform method to find a digital equivalent of the following 
analog filter. You can assume T 5 .2. Match the gains at DC.
Ha(s) 5
10s 1 1
s2 1 3s 1 2
section 7.6: Frequency Transformations
7.20 Find the transfer function H(s) of a second-order highpass Butterworth filter that 
has a 3-dB cutoff frequency of Fc 5 5 Hz.
7.21 Find the transfer function H(s) of a fourth-order bandpass Butterworth filter that 
has 3-dB cutoff frequencies of F0 5 2 Hz and F1 5 4 Hz.
section 7.7: Filter Realization structures
7.22 Sketch a direct form I signal flow graph realization of the following IIR transfer 
function.
H(z) 5
.8 2 1.2z21 1 .4z23
1 2 .9z21 1 .6z22 1 .3z23
7.23 Sketch a direct form II signal flow graph realization of the following difference 
equation.
y(k) 5 10x(k) 1 2x(k 2 1) 2 4x(k 2 2) 1 5x(k 2 3) 2 .7y(k 2 2) 1 .4y(k 2 3)
7.24 Sketch a transposed direct form II signal flow graph realization of the following 
transfer function.
H(z) 5 1 2 2z21 1 3z22 2 4z23
1 1 .8z21 1 .6z22 1 .4z23
7.25 Consider the following IIR system.
H(z) 5
z3
(z 2 .8)(z2 2 z 1 .24)
(a) Expand H(z) into partial fractions.
(b) Sketch a parallel form signal flow graph realization by combining the two 
poles that are closest to the unit circle into a second-order block. Use a direct 
form II realization for each block.
7.26 Consider the following IIR system.
H(z) 5 2(z2 1 .64)(z2 2 z 1 .24)
(z2 1 1.2z 1 .27)(z2 1 .81)
(a) Sketch the poles and zeros of H(z).
(b) Sketch a cascade form signal flow graph realization by grouping the complex 
zeros with the complex poles. Use a direct form II realization for each block.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.11 Problems    587
section 7.8: Finite Word length Effects
7.27 Consider the following IIR filter. Suppose 8-bit fixed-point arithmetic is used to 
implement this filter using a scale factor of c 5 4.
H(z) 5
2z
z 1 .7
(a) Find the quantization level q.
(b) Find the power gain of this filter.
(c) Find the average power of the product round-off error.
7.28 Consider the following IIR filter.
H(z) 5
.5
z 1 .9
(a) Sketch a direct form II signal flow graph of H(z).
(b) Suppose all filter variables are represented as fixed-point numbers, and the 
input is constrained to ux(k)u # c where c 5 2. Find a scale factor, s1, that elim-
inates summing junction overflow error.
(c) Sketch a modified direct form II signal flow graph of H(z) that implements 
scaling to eliminate summing junction overflow.
7.29 For the system in Problem 7.28, find a scale factor s` that will eliminate summing 
junction overflow when the input is a pure sinusoid of amplitude c # 5.
7.30 Let fclip(x) be the following unit clipping nonlinearity.
fclip(x) 5
D  5
21,
2 ` , x , 21
x,
2 1 # x # 1
1,
1 , x , `
Show how fclip can be used to eliminate limit cycles due to overflow error by sketch-
ing a modified direct form II signal flow graph of a second-order IIR block. You 
can assume all values are represented as fractions in the interval [21, 1].
7.11.2 gUI simulation
section 7.2: Filter Design by Pole-zero Placement
7.31 Use the GUI module g_iir to design a resonator filter with resonant frequency 
F0 5 100 Hz.
(a) Plot the linear magnitude response. Use the Caliper option to mark the peak.
(b) Plot the phase response. Is this a linear-phase filter?
(c) Plot the pole-zero plot.
7.32 Use the GUI module g_iir to design a notch filter with a notch frequency of 
F0 5 200 Hz and a sampling frequency of fs 5 1200 Hz.
(a) Plot the magnitude response.
(b) Plot the phase response. Is this a linear-phase filter?
(c) Plot the impulse response.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

588    Chapter 7  IIR Filter Design
7.33 Create a MAT-file called prob7_33.mat that contains b, a, and fs for an inverse 
comb filter of order n 5 12 using fs 5 1000 Hz and a 3-dB radius of DF 5 2 Hz. 
Then use the GUI module g_iir and the Import option to load this filter.
(a) Plot the linear magnitude response.
(b) Plot the phase response
(c) Plot the pole-zero pattern.
section 7.4: Classical Analog Filters
7.34 Use the GUI module g_iir to construct a Chebyshev-I lowpass filter. Plot the linear 
magnitude response for the following cases.
(a) Adjust the filter order n to the highest value that does not meet the specifications.
(b) Adjust the filter order n to the lowest value that meets or exceeds the specifications.
7.35 Use the GUI module g_iir to design a lowpass Butterworth filter. Adjust the filter 
order to the lowest value that meets or exceeds the specifications. Plot the following.
(a) The linear magnitude response
(b) The phase response. Is this a linear-phase filter?
(c) The pole-zero plot
7.36 Use the GUI module g_iir to design a highpass Chebyshev-I filter. Adjust the filter 
order to the lowest value that meets or exceeds the specifications. Plot the following.
(a) The linear magnitude response
(b) The phase response. Is this a linear-phase filter?
(c) The pole-zero plot
7.37 Use the GUI module g_iir to design a bandpass Chebyshev-II filter. Adjust the filter 
order to the lowest value that meets or exceeds the specifications. Plot the following.
(a) The linear magnitude response
(b) The phase response. Is this a linear-phase filter?
(c) The pole-zero plot
7.38 Use the GUI module g_iir to design a bandstop elliptic filter. Adjust the filter order 
to the lowest value that meets or exceeds the specifications. Plot the following.
(a) The linear magnitude response
(b) The phase response. Is this a linear-phase filter?
(c) The pole-zero plot
7.39 Use the GUI module g_iir to design a Butterworth bandpass filter. Find the small-
est order filter that meets or exceeds the following design specifications.
ffs, Fs1, Fp1, Fp2, Fs2g 5 f2000, 300, 400, 600, 700g Hz
fAp, Asg 5 f.6, 30g dB
(a) Plot the magnitude response using the dB scale.
(b) Plot the pole-zero pattern.
(c) Export a, b, and fs to a MAT-file named prob7_39.mat. Then use GUI module 
g_filters to import this filter. Adjust the number of bits used for coefficient 
quantization to N 5 12. Plot the linear magnitude responses.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.11 Problems    589
7.40 Use the GUI module g_iir to design a Chebyshev-I bandpass filter. Find the small-
est order filter that meets or exceeds the following design specifications.
ffs, Fs1, Fp1, Fp2, Fs2g 5 f2000, 300, 400, 600, 700g Hz
fp, sg 5 f.05, .03g
(a) Plot the linear magnitude response.
(b) Plot the pole-zero pattern.
(c) Export a, b, and fs to a MAT-file named prob7_40.mat. Then use GUI mod-
ule g_filters to import this filter. Adjust the number of bits used for coefficient 
quantization to N 5 10. Plot the linear magnitude responses.
7.41 Use the GUI module g_iir to design an elliptic bandpass filter. Find the smallest 
order filter that meets or exceeds the following design specifications.
ffs, Fs1, Fp1, Fp2, Fs2g 5 f2000, 350, 400, 600, 650g Hz
fp, sg 5 f.04, .02g
(a) Plot the linear magnitude response.
(b) Plot the pole-zero pattern.
(c) Export a, b, and fs to a MAT-file named prob7_41.mat. Then use GUI mod-
ule g_filters to import this filter. Adjust the number of bits used for coefficient 
quantization to N 5 9. Plot the linear magnitude responses.
7.42 Use the GUI module g_iir to design a Butterworth bandpass filter. Find the small-
est order filter that meets or exceeds the following design specifications.
ffs, Fp1, Fs1, Fs2, Fp2g 5 f100, 20, 25, 35, 40g Hz
fp, sg 5 f.05, .02g 
(a) Plot the magnitude response using the dB scale.
(b) Plot the pole-zero pattern.
(c) Export a, b, and fs to a MAT-file named prob7_42.mat. Then use GUI mod-
ule g_filters to import this filter. Adjust the number of bits used for coefficient 
quantization to N 5 16. Plot the linear magnitude responses.
7.43 Use the GUI module g_iir to design a Chebyshev-II bandstop filter. Find the small-
est order filter that meets or exceeds the following design specifications.
ffs, Fp1, Fs1, Fs2, Fp2g 5 f20000, 2500, 3000, 4000, 4500g Hz
fp, sg 5 f.04, .03g
(a) Plot the linear magnitude response.
(b) Plot the pole-zero pattern.
(c) Export a, b, and fs to a MAT-file named prob7_43.mat. Then use GUI mod-
ule g_filters to import this filter. Adjust the number of bits used for coefficient 
quantization to N 5 17. Plot the linear magnitude responses.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

590    Chapter 7  IIR Filter Design
7.44 Use the GUI module g_iir to design an elliptic bandstop filter. Find the smallest 
order filter that meets or exceeds the following design specifications.
ffs, Fp1, Fs1, Fs2, Fp2g 5 f20, 6.5, 7, 8, 8.5g Hz
fp, sg 5 f.02, .015g
(a) Plot the linear magnitude response.
(b) Plot the pole-zero pattern.
(c) Export a, b, and fs to a MAT-file named prob7_44.mat. Then use GUI mod-
ule g_filters to import this filter. Adjust the number of bits used for coefficient 
quantization to N 5 14. Plot the linear magnitude responses.
7.45 Use the GUI module g_iir and the Import option to load the filter in MAT-file 
u_iir1.
(a) Plot the linear magnitude response. What type of filter is this?
(b) Plot the phase response
(c) Plot the impulse response.
7.11.3 MATlAB Computation
section 7.4: Classical Analog Filters
7.46 Write a MATLAB program that uses f_butters to design an analog Butterworth 
lowpass filter to meet the following design specifications.
fFp, Fs, p, sg 5 f10, 20, .04, .02g
(a) Print the filter order.
(b) Use f_freqs to compute and plot the magnitude response for 0 # f # 2Fs.
(c) Use fill to add shaded areas showing the design specifications on the magni-
tude response plot.
7.47 Write a MATLAB program that uses f_cheby1s to design an analog Chebyshev-I 
lowpass filter to meet the following design specifications.
fFp, Fs, p, sg 5 f10, 20, .04, .02g
(a) Print the filter order.
(b) Use f_freqs to compute and plot the magnitude response for 0 # f # 2Fs.
(c) Use fill to add shaded areas showing the design specifications on the magni-
tude response plot.
7.48 Write a MATLAB program that uses f_cheby2s to design an analog Chebyshev-II 
lowpass filter to meet the following design specifications.
fFp, Fs, p, sg 5 f10, 20, .04, .02g
(a) Print the filter order.
(b) Use f_freqs to compute and plot the magnitude response for 0 # f # 2Fs.
(c) Use fill to add shaded areas showing the design specifications on the magni-
tude response plot.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.11 Problems    591
7.49 Write a MATLAB program that uses f_elliptics to design an analog elliptic lowpass 
filter to meet the following design specifications.
fFp, Fs, p, sg 5 f10, 20, .04, .02g
(a) Print the filter order.
(b) Use f_freqs to compute and plot the magnitude response for 0 # f # 2Fs.
(c) Use fill to add shaded areas showing the design specifications on the magni-
tude response plot.
section 7.5: Bilinear Transformation Method
7.50 Write a MATLAB program that uses f_butters and f_bilin to find the digital equiv-
alent, H(z), of a sixth-order lowpass Butterworth filter using the bilinear transfor-
mation method. Suppose the sampling frequency is fs 5 10 Hz. Prewarp the analog 
cutoff frequency so that the digital cutoff frequency comes out to be Fc 5 1 Hz.
(a) Plot the impulse response, h(k).
(b) Use f_pzplot to plot the poles and zeros of H(z).
(c) Use f_freqz to compute and plot the magnitude response, A( f ). Add the ideal 
magnitude response and a plot legend.
7.51 Write a MATLAB program that uses f_butterz to design a digital Butterworth 
bandstop filter that meets the following design specifications.
ffs, Fp1, Fs1, Fs2, Fp2, p, sg 5 f2000, 200, 300, 600, 700, .05, .03g
(a) Find the smallest filter order that meets the specifications. Print the order.
(b) Use f_freqz to compute and plot the magnitude response.
(c) Use fill to add shaded areas showing the design specifications.
7.52 Write a MATLAB program that uses f_cheby1z to design a digital Chebyshev-I 
bandstop filter that meets the following design specifications.
ffs, Fp1, Fs1, Fs2, Fp2, p, sg 5 f2000, 200, 300, 600, 700, .05, .03g
(a) Find the smallest filter order that meets the specifications. Print the order.
(b) Use f_freqz to compute and plot the magnitude response.
(c) Use fill to add shaded areas showing the design specifications.
7.53 Write a MATLAB program that uses f_cheby2z to design a digital Chebyshev-II 
bandpass filter that meets the following design specifications.
ffs, Fs1, Fp1, Fp2, Fs2, p, sg 5 f1600, 250, 350, 550, 650, .06, .04g
(a) Find the smallest filter order that meets the specifications. Print the order.
(b) Use f_    freqz to compute and plot the magnitude response.
(c) Use fill to add shaded areas showing the design specifications.
7.54 Write a MATLAB program that uses f_ellipticz to design a digital elliptic bandpass 
filter that meets the following design specifications.
ffs, Fs1, Fp1, Fp2, Fs2, p, sg 5 f1600, 250, 350, 550, 650, .06, .04g
(a) Find the smallest filter order that meets the specifications. Print the order.
(b) Use f_    freqz to compute and plot the magnitude response.
(c) Use fill to add shaded areas showing the design specifications.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

592    Chapter 7  IIR Filter Design
section 7.6: Frequency Transformations
7.55 Write a MATLAB program that uses f_butters and f_low2highs to design an analog 
Butterworth highpass filter to meet the following design specifications.
fFs, Fp, Ap, Asg 5 f4, 6, .5, 24g
(a) Print the filter order, p and s.
(b) Use f_      freqs to compute and plot the magnitude response for 0 # f # 2Fp 
using the linear scale.
(c) Use fill to add shaded areas showing the design specifications on the magni-
tude response plot.
7.56 Write a MATLAB program that uses f_cheby1s and f_low2bps to design an analog 
Chebyshev-I bandpass filter to meet the following design specifications.
fFs1, Fp1, Fp2, Fs2, Ap, Asg 5 f35, 45, 60, 70, .4, 28g
(a) Print the filter order, p and s.
(b) Use f_        freqs to compute and plot the magnitude response for 0 # f # 2Fs2 
using the linear scale.
(c) Use fill to add shaded areas showing the design specifications on the magni-
tude response plot.
7.57 Write a MATLAB function called f_        filtnorm that returns the Lp norm, uuhuup, of a 
digital filter. The function f_        filtnorm should use the following calling sequence.
% F_FILTNORM: Return L_p norm of filter H(z) = b(z)/a(z)
%
% Usage:
% 
d = f_filtnorm (b,a,p)
% Pre:
% 
b = vector of length m+1 containing coefficients of
% 
 
numerator polynomial.
% 
a = vector of length n+1 containing coefficients of
% 
 
denominator polynomial.
% 
p = integer specifying norm type. Use p = Inf for
% 
 
the infinity norm
% Post:
% 
d = the L_p norm, ||h||_p
Test f_filtnorm by writing a MATLAB program that computes and prints the L1, L2, 
and L` norms of a comb filter with n 5 10 and r 5 .98. Verify that (7.8.17) holds 
in this case.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3
Advanced Signal Processing
P A R T
9
8
Multirate
Signal Processing
Adaptive
Signal Processing
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

595
C H A P T E R  8
Multirate Signal Processing
CHAPTER ToPiCS
 8.1 
Motivation 
 8.2 
Integer Sampling Rate 
Converters 
 8.3 
Rational Sampling Rate 
Converters 
 8.4 
Polyphase Filters 
 8.5 
Narrowband Filters 
 8.6 
Filter Banks 
 8.7 
Perfect Reconstruction 
Filter Banks 
 8.8 
Transmultiplexors 
*8.9 
Oversampled A-to-D 
Converters 
*8.10 Oversampled D-to-A 
Converters 
 8.11 GUI Modules and Case 
Studies 
 8.12 Chapter Summary 
 8.13 Problems
Motivation 
All of the discrete-time systems encountered thus far have sig-
nals that are sampled at a single fixed sampling rate, fs. If this 
assumption is relaxed to allow some of the signals to be sampled 
at one rate while others are sampled at another rate, this leads 
to a multirate system. Multirate systems can offer important 
advantages over fixed-rate systems in terms of overall perfor-
mance. One of the simplest examples of a multirate system is 
a sampling rate decimator that decreases the sampling rate of a 
discrete-time signal by an integer factor M.
y(k) 5 o
m
i50
bix(Mk 2 i)
Here output y(k) is a filtered version of the input x(k), but the 
input is evaluated only at every Mth sample. Extracting every  
Mth sample effectively reduces the sampling rate by M. The low-
pass filtering operation is needed in order to preserve the spectral 
characteristics of the down-sampled signal. It is also possible 
to increase the sampling rate by an integer factor L using an 
interpolator. More generally, sampling rate converters can be 
designed where the ratio of the output sampling frequency to the 
input sampling frequency is an arbitrary rational number, LyM.
Modern high-performance DSP systems exploit the ben-
efits of multirate systems. For example, filter banks can be 
designed to simultaneously transmit several subsignals over 
a single high-bandwidth communications channel, a process 
8.1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

596    Chapter 8  Multirate Signal Processing
known as frequency-division multiplexing. Other applications include data compres-
sion, narrowband filter design, and the design of high-performance analog-to-digital  
and digital-to-analog converters that use oversampling to achieve improved noise 
immunity.
We begin this chapter by introducing some examples of applications of multi-
rate systems. Next the design of integer sampling rate decimators and interpolators 
is presented. These rate converter building blocks are then used to construct rational 
sampling rate converters, both single-stage and multistage. This is followed by an inves-
tigation of efficient realization structures for rate converters using polyphase filters. 
Next the discussion turns to multirate system applications, starting with the design 
of narrowband filters and filter banks. The filter bank discussion includes an analysis 
of perfect reconstruction DFT filter banks and transmultiplexor systems. Next the 
improved performance characteristics of oversampled sigma-delta analog-to-digital 
converters are presented. This is followed by an analogous presentation applied to 
oversampled digital-to-analog converters that includes the notion of passband equal-
ization. Finally, a GUI module, called g_multirate, is introduced that allows the user 
to design and evaluate multirate DSP systems without any need for programming. 
The chapter concludes with a case study example and a summary of multirate signal 
processing techniques.
8.1.1 Narrowband Filters
Several signals can be transmitted simultaneously over a single communication channel 
by allocating a separate band of frequencies for each signal. This technique, known as 
frequency-division multiplexing or subband processing, requires the use of a bank of 
filters. In this way each filter can be used to extract a different signal. The magnitude 
response of a bank of N 5 4 filters is shown in Figure 8.1. Note that the transition bands 
overlap with one another in such away that the overall magnitude response is that of an 
allpass filter.
Figure 8.1:  
Magnitude 
Responses of a 
Bank of Filters
0
0.2
0.3
0.1
0.4
0.5
0.6
0.7
0.8
0.9
1
20.5
0
0.5
1
1.5
A(f)
f/fs
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.1 Motivation     597
A filter is referred to as a narrowband filter when the width of the passband is very 
small in comparison with the sampling frequency fs. For example, let Bp 5 Fp2 2 Fp1 
denote the width of the passband of a bandpass filter. This filter is a narrowband filter if
 
Bp V  fs 
 (8.1.1)
Narrowband lowpass and highpass filters can be defined in an analogous way. The chal-
lenge in designing narrowband filters arises when one considers the required width of the 
transition band. Consider a bank of N filters. Since the discrete-time frequency response 
is periodic with period fs, one can take the frequency range to be f0, fsg as in Figure 8.1. 
Here the ith filter is centered at Fi 5 ifsyN for 0 # i , N and it has a maximum passband 
width of
 
Bp < fs
N 
 (8.1.2)
In order to maximize the use of the spectrum, the width of the transition band 
should be small in comparison with the width of the passband. Consequently for a nar-
rowband filter, the width of the transition band is very small in comparison with fs. As 
an illustration, suppose a filter bank of N 5 10 filters is to be designed, and suppose 
the width of the transition band is set to Bt 5 Bpy10. Then the normalized width of the 
transition band is
B 5 Bt
fs
5
Bp
10fs
 
5 .01 
 (8.1.3)
This design requirement is quite severe. To see what it implies, suppose the passband  
ripple and stopband attenuation are as follows for each filter in the bank.
 
(p, s) 5 (.01, .01) 
 (8.1.4)
If the equiripple FIR filter design method is used to design the filters, then from (6.5.21) 
the estimated order of the filter required to meet the design specification is
m < ceil5
2f10 log10(ps) 1 13g
14.6B
1 16
 5 ceil5
2f10 log10(.0001) 1 13g
14.6(.01)
1 16
 
 5 186
 
 (8.1.5)
Clearly, a very high-order filter is needed to meet the narrowband design specifi-
cation in this case. If an alternative FIR design method is used such as a windowed,  
frequency-sampled, or least-squares filter, the required filter order will be even higher. 
Implementing a filter of such a high order brings with it a host of problems, including 
significant memory requirements, lengthy signal delay, and potentially debilitating finite 
word length effects. Fortunately, by using a multirate design with a polyphase realiza-
tion, these difficulties can be reduced significantly and the performance of the narrow-
band filter can be improved.
Narrowband filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

598    Chapter 8  Multirate Signal Processing
8.1.2 intersample Delay Systems
A design task that occurs repeatedly in different applications is the problem of delaying 
a discrete-time signal without otherwise distorting it. If the desired delay is an integer 
multiple of the sampling interval T, then this is achieved easily. For example, in software 
one can allocate a memory buffer in the form of a shift register of length L as shown in 
Figure 8.2. Here the signal appearing at the output end will be a delayed version of the 
input with a delay of L samples or  5 LT.
It is more challenging to design a system where the delay is not an integer multiple of 
the sampling interval, but instead involves an intersample delay. In effect, what is required 
is an allpass filter with a phase response of
 
( f ) 5 22f  
 (8.1.6)
Recall that linear-phase FIR filters of order m can be designed with a delay of  5 mTy2. 
By using multirate techniques an intersample delay can be achieved. The basic idea is to 
first increase the sampling rate by a factor M. One then delays the up-sampled signal by 
0 , L , M samples using a shift register. This is followed by decreasing the sampling 
rate by a factor M to restore the original sampling frequency. The processing steps are 
summarized in Figure 8.3.
The factor of M interpolator in Figure 8.3 increases the sampling rate by M so 
that the intermediate signal r(k) is sampled at the rate fS 5 Mfs. Once x(k) has been up- 
sampled to produce r(k), this intermediate signal is then delayed by an integer number of 
samples L using the shift register in Figure 8.2. If the length of the shift register is in the 
range 0 , L , M, this produces a fractional delay when viewed in terms of the original 
sampling rate fs. The factor of M decimator down-samples the delayed signal r(k 2 L) 
by M, thereby restoring the original sampling frequency. The overall delay introduced by 
the three blocks in Figure 8.3 is
 
 51m 1 L
M2T 
 (8.1.7)
The delay of m samples in (8.1.7) arises because the interpolator and decimator blocks in 
Figure 8.3 each include a linear-phase FIR lowpass filter of order m.
Intersample delay
Figure 8.2: Delay of Discrete-time Signal Using an L-sample Shift Register
x(k)
x(k 2 1)
x(k 2 L)
y(k)
x(k)
Figure 8.3: Intersample Delay of Discrete-time Signal Using a Multirate System
x(k)
r(k)
y(k)
Factor
of M
Decimator
L-Sample
Shift
Register
Factor
of M
Interpolator
r(k 2 L)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.2 Integer Sampling Rate Converters    599
integer Sampling Rate Converters
Modern high-performance DSP systems often make use of multirate systems, systems 
where some of the signals are sampled at one frequency and others are sampled at 
another frequency. For example, the need for a sharp high-order analog anti-aliasing 
prefilter can be avoided if oversampling is used and the sampling rate is later reduced to 
the desired value.
A conceptually simple way to change the sampling rate is to convert a discrete-time 
signal from digital to analog with a DAC, and then resample the analog signal with an 
ADC using the desired sampling frequency. This brute force approach to sampling rate 
conversion has the advantage that the new sampling rate can be any value achievable by 
the ADC. However, a drawback is that the DAC and ADC introduce additional quanti-
zation noise and aliasing error. In this section, techniques are introduced that avoid these 
drawbacks by implementing sampling rate converters entirely in the discrete-time domain.
8.2.1 Sampling Rate Decimator
Let us begin with a relatively simple problem, namely, a reduction in the sampling 
rate by an integer factor M. A sampling rate converter that reduces the sampling rate 
is called a decimator because it removes samples. Let x(k) be the discrete-time signal 
obtained by sampling an analog signal xa(t) at the rate fs. If T 5 1yfs is the sampling 
interval, then
 
x(k) 5 xa(kT) 
 (8.2.1)
The objective is to start with x(k) and synthesize a new discrete-time signal y(k) that cor-
responds to sampling xa(t) at the reduced rate, fsyM, where M is a positive integer. Since 
M is an integer, it would appear that this can be accomplished by simply extracting every 
Mth sample of x(k) as follows.
 
xM(k) 5 x(Mk) 
 (8.2.2)
The problem with this basic approach is that it does not take into account the fre-
quency content of the two signals. If the original signal x(k) is sampled in a manner 
that avoids aliasing, then from the sampling theorem the analog signal xa(t) must be 
bandlimited to less than fsy2 Hz. However, to avoid aliasing with the reduced-rate signal, 
xM(k), the analog signal must be bandlimited to less than fsy(2M) Hz. Thus to eliminate 
aliasing in xM(k), one must first pass x(k) through a lowpass filter with a cutoff frequency 
of FM 5 fsy(2M).
 
HM ( f ) 5
D  5
1,
0 # u f u # FM
0,
FM , u f u # fsy2 
 (8.2.3)
Unlike an analog anti-aliasing filter associated with an ADC, the filter in (8.2.3) is a digital 
anti-aliasing filter. Sampling rate decimation by an integer factor, M, is summarized in the 
block diagram shown in Figure 8.4. Note that it is standard practice to denote sampling 
rate reduction in (8.2.2), also called down-sampling, with the down-arrow notation T.
8.2
Multirate systems
Decimator
Digital anti-aliasing 
filter
Down-sampling
Figure 8.4:  
Sampling Rate 
Decimation by an 
Integer Factor, M
x
r
HM(z)
y
M
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

600    Chapter 8  Multirate Signal Processing
Since the anti-aliasing filter in Figure 8.4 is a digital filter, any of the linear-phase 
FIR filter design techniques discussed in Chapter 6 can be applied to design this lowpass 
filter. If nonlinear phase distortion is not of concern, then an IIR filter can be used for 
HM(z). When HM(z) is implemented with an FIR filter of order m, the output of the sam-
pling rate decimator can be expressed in the time domain as follows.
 
y(k) 5 o
m
i50
bix(Mk 2 i)  
 (8.2.4)
The replacement of k on the right-hand side of (8.2.4) by Mk sets the down-sampler 
apart from a normal linear time-invariant FIR filter. The down-sampling operation in 
(8.2.4) continues to be a linear operation. However, if the input is delayed by n samples, 
the output will not be delayed by n samples except when n is a multiple of M. Conse-
quently, a decimator is a linear time-varying system.
Figure 8.5: Sampling Rate Decimation by an Integer Factor M 5 2 Using  
an FIR Filter of Order m 5 20 with a Hamming Window
0
10
20
30
40
50
60
70
80
22
21
0
1
2
k
x(k)
integer Decimator
EXAMPLE 8.1
As an illustration of a sampling rate decimator, consider the following analog  
input signal.
xa(t) 5 sin(2t) 2 .5 cos(4t)
Let the sampling frequency be fs 5 40 Hz. Suppose the objective is to decimate 
the samples x(k) by a factor of M 5 2. From (8.2.3) the required lowpass filter 
has a gain of HM(0) 5 1 and a cutoff frequency of FM 5 10 Hz. Suppose a win-
dowed linear-phase filter of order m 5 20 with a Hanning window is used. Since 
the original signal was oversampled and is bandlimited to 2 Hz, the FIR filter 
does not have any appreciable effect in this instance. The decimator output is 
obtained by running exam8_1. Plots of the original samples and the decimated 
samples are shown in Figure 8.5. It is apparent from inspection that, after an 
initial start-up transient, the decimated samples faithfully reproduce the original 
signal. Note that there is a delay of my2 5 10 of the original samples caused by 
the linear-phase filter HL(z).
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.2 Integer Sampling Rate Converters    601
8.2.2 Sampling Rate interpolator
Next, consider the dual problem of designing a converter that increases the sampling rate 
by an integer factor L. A sampling rate converter that increases the sampling rate is called 
a interpolator because it inserts new samples that interpolate between the original samples. 
Here the objective is to synthesize a discrete-time signal y(k) that corresponds to sampling 
xa(t) at the increased rate of Lfs, where L is a positive integer. Since L is an integer, every 
Lth sample of the new signal xL(k) will correspond to a sample of the original signal x(k). 
There are potentially many ways to interpolate between the original samples. The easiest 
is to simply insert L 2 1 zero samples between each of the original samples as follows.
 
xL(k) 55
x(kyL),
uku 5 0, L, 2L, Á
0,
otherwise
 
 (8.2.5)
A helpful way to view xL(k) is in terms of the following periodic impulse train with period L.
 
L(k) 5
D o
`
i52`
(k 2 Li) 
 (8.2.6)
The signal xL(k) is the periodic impulse train, L(k), amplitude modulated by x(kyL). 
That is,
 
xL(k) 5 x(kyL)L(k) 
 (8.2.7)
Note that x(kyL) is not defined except when k is an integer multiple of L. However, the 
product in (8.2.7) is well defined for all k because L(k) 5 0 when k is not a multiple of L. 
One can always replace kyL in (8.2.7) by floor(kyL) without changing the result.
The effect of using zero samples for interpolation can be seen by looking at the 
Z-transform of the interpolated signal. Using the change of variable i 5 kyL
XL(z) 5 o
`
k50
x(kyL)L(k)z2k
5 o
`
i50
x(i)z2Li
 
5 o
`
i50
x(i)(zL)2i 
 (8.2.8)
Interpolator
Figure 8.5: (Continued)
0
5
10
15
20
25
30
35
40
22
21
0
1
2
k
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

602    Chapter 8  Multirate Signal Processing
The Z-transform of the up-sampled signal can be expressed in terms of the Z-transform 
of the input as
 
XL(z) 5 X(zL) 
 (8.2.9)
Recall that the spectrum of a discrete-time signal can be obtained from the Z-transform 
by evaluating the Z-transform along the unit circle. Replacing z in (8.2.9) by exp(  j 2f T), 
the spectrum of the interpolated signal is as follows.
 
XL( f ) 5 X(Lf ), 0 # u f u # fsy2 
 (8.2.10)
Thus the spectrum of the interpolated signal, xL(k), is an L-fold replication of the spec-
trum of the original signal, x(k), with each copy centered at a multiple of fsyL. These 
L 2 1 images of the original spectrum must be removed by passing xL(k) through a low-
pass anti-imaging filter with a cutoff frequency of FL 5 fsy(2L).
 
HL( f ) 5
D  5
L,
0 # u f u # FL
0,
FL , u f u # fsy2 
 (8.2.11)
Note that the passband gain of the anti-imaging filter has been set to HL(0) 5 L. This is 
done to compensate for the fact that the average value of xL(k) is 1yL times the average 
value of x(k), due to the presence of the zero samples. Unlike an analog anti-imaging 
filter associated with a DAC, the filter in (8.2.11) is a digital anti-imaging filter. Sampling 
rate interpolation by an integer factor of L is summarized in the block diagram shown in 
Figure 8.6. Again it is standard practice to denote a sampling rate increase in (8.2.5), also 
called up-sampling, with the up-arrow notation c.
Since the anti-imaging filter in Figure 8.6 is a digital filter, any of the linear-phase 
FIR filter design techniques introduced in Chapter 6 can be used to design this lowpass 
filter. Alternatively, an IIR filter can be used if nonlinear phase distortion is not a con-
cern. When HL(z) is implemented as an FIR filter of order m, the output of the sampling 
rate interpolator can be expressed in the time domain as follows.
 
y(k) 5 o
m
i50
bix 1
k 2 i
L 2 L(k 2 i)  
 (8.2.12)
Digital anti-imaging 
filter
Up-sampling
Figure 8.6:  
Sampling Rate 
Interpolation by an 
Integer Factor, L 
x
xL
HL(z)
y
L
integer interpolator
EXAMPLE 8.2
As an illustration of a sampling rate interpolator, consider the same analog input 
signal used in Example 8.1.
xa(t) 5 sin(2t) 2 .5 cos(4t)
Suppose the sampling frequency is fs 5 20 Hz. Consider the problem of increas-
ing the sampling rate by a factor of L 5 3. Using (8.2.11), the required lowpass 
filter has a gain of HL(0) 5 3 and a cutoff frequency of FL 5 10y3 Hz. Suppose 
a windowed linear-phase filter of order m 5 20 with a Hanning window is used. 
The insertion of two zero samples between each of the original samples causes 
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.3 Rational Sampling Rate Converters     603
Rational Sampling Rate Converters 
8.3.1 Single-stage Converters
Sampling rate conversion by integer factors is useful, but it can be too restrictive in some 
practical applications. For example, digital audio tape (DAT) used in sound recording 
studios has a sampling rate of fs 5 48 kHz, while a compact disc (CD) is recorded at a 
sampling rate of fs 5 44.1 kHz. In order to convert music from one format to the other, a 
noninteger change in the sampling rate is required. Fortunately, all the tools are in place 
to realize a much larger set of sampling rate converters. The basic approach is to first 
8.3
Figure 8.7: Sampling Rate Interpolation by an Integer Factor L 5 3, Using an 
FIR Filter of Order m 5 20 with a Hanning Window
0
5
10
15
20
25
30
35
40
22
21
0
1
2
k
x(k)
0
20
40
60
80
100
120
22
21
0
1
2
k
y(k)
high-frequency images of the original spectrum to appear that must be removed 
by the anti-imaging filter. The interpolator output is obtained by running 
exam8_2. Plots of the original samples and the interpolated samples are shown in  
Figure 8.7. It is apparent from inspection that the interpolated samples have filled 
in between the original samples and preserved the wave shape. The output has 
been delayed by my2 5 10 samples due to HL(z). It may seem counter-intuitive 
that inserting a run of zero samples can interpolate between existing samples. It 
is the inclusion of the lowpass filter that effectively recovers the underlying band-
limited analog signal.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

604    Chapter 8  Multirate Signal Processing
interpolate the signal by a factor of L, and then decimate the result by a factor of M. The 
net effect of this cascade configuration of an interpolator followed by a decimator is to 
change the sampling rate by a rational factor LyM. That is,
 
fS 51
L
M2 fs 
 (8.3.1)
A block diagram of a rational sampling rate converter is shown in Figure 8.8. If LyM , 1, 
then the system in Figure 8.8 is a rational decimator, and if LyM . 1, it is a rational 
interpolator.
The interpolation in Figure 8.8 is done first in order to work at the higher sampling 
rate, thereby preserving the original spectral characteristics of x(k). Moreover, this order-
ing has an added benefit because the cascade configuration of the two lowpass filters 
can be combined into a single equivalent lowpass filter with a frequency response of 
H0( f ) 5 HL( f )HM( f ). The simplified configuration is shown in Figure 8.9. The pass-
band gain of this composite anti-aliasing and anti-imaging filter is H0(0) 5 L, and the 
cutoff frequency is F0 where
 
F0 5  min 5
fs
2L, fs
2M6 
 (8.3.2)
Thus the frequency response of the composite digital anti-aliasing and anti-imaging filter is
 
H0( f ) 55
L,
   
0 # u f u # F0
0,
F0 , u f u # fsy2 
 (8.3.3)
If the composite filter is a linear-phase FIR filter of order m, then the output of a rational 
sampling rate converter can be expressed as follows in the time domain.
 
y(k) 5 o
m
i50
bi L(Mk 2 i)x 1
Mk 2 i
L 2  
 (8.3.4)
As a partial check, observe that (8.3.4) reduces to the decimator special case in (8.2.4) 
when L 5 1 because L(k) ; 1 when L 5 1. Similarly, (8.3.4) reduces to the interpolator 
special case in (8.2.12) when M 5 1.
Figure 8.8:  
Rational Sampling 
Rate Converter 
with a Conversion 
Factor, LyM 
x
Interpolator
Decimator
HL(z)
y
L
HM(z)
M
Figure 8.9: Simplified Rational Sampling Rate Converter with a Composite  
Anti-aliasing and Anti-imaging Filter and a Conversion Factor, LyM 
x
H0(z)
y
L
M
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.3 Rational Sampling Rate Converters     605
Rational Sampling Rate Converter
EXAMPLE 8.3
As an illustration of a rational sampling rate converter, consider the following 
analog input signal. 
xa(t) 5 cos(2t) 1 .8 sin(4t)
Suppose the sampling frequency is fs 5 20 Hz, and consider the problem of 
changing the sampling rate of x(k) by a factor of LyM 5 3y2. In this case, the 
required lowpass filter has a gain of H0(0) 5 3 and a cutoff frequency of
F0 5  min 5
20
6 , 20
46
5 10
3  Hz
Suppose a windowed linear-phase filter of order m 5 20 with a Hamming win-
dow is used. The converter output is obtained by running exam8_3. Plots of the 
original samples and the rate-converted samples are shown in Figure 8.10. It is 
apparent from inspection that the interpolated samples have filled in between the 
original samples with three new samples for each pair of original samples. The 
output is delayed by my2 5 10 samples due to H0(z).
0
5
10
15
20
25
30
35
40
22
21
0
1
2
k
0
10
20
30
40
50
60
22
21
0
1
2
k
x(k)
y(k)
Figure 8.10: Sampling Rate Conversion by a Rational Factor LyM 5 3y2 Using 
an FIR Filter of Order m 5 20 with a Hamming Window
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

606    Chapter 8  Multirate Signal Processing
8.3.2 Multistage Converters
In some practical applications, the values for L or M can be relatively large. This presents 
some special challenges when it comes to implementation. If either L or M is large, the 
composite anti-aliasing and anti-imaging filter, H0(z), will be a narrowband lowpass filter 
with a cutoff of F0 V fs. Narrowband linear-phase filter specifications are difficult to 
meet and can require very high-order FIR filters. This in turn can mean large computa-
tional times and detrimental finite word length effects. The latter drawback can be miti-
gated by using a multistage sampling rate converter. The basic idea is to factor the desired 
conversion ratio into a product of ratios, each of which uses smaller values for L and M.
 
L
M 51
L1
M12 1
L2
M22 Á 1
Lr
Mr2 
 (8.3.5)
One can then implement r lower-order stages separately and configure them in a cascade, 
as shown in Figure 8.11 for the case r 5 2. The optimal number of stages and the optimal 
factoring of LyM can be determined based on minimizing the computational time and 
the storage requirements (Crochiere and Rabiner; 1975, 1976).
Multistage converter
Figure 8.11: A Multistage Sampling Rate Converter with r 5 2 Stages 
x
Stage 1
Stage 2
H1(z)
H2(z)
y
L1
L2
M2
M1
DAT to CD
EXAMPLE 8.4
Consider the problem of designing a sampling rate converter that will transform a 
signal x(k) that was sampled using the standard digital audio tape (DAT) format 
to a signal y(k) that is suitable playing on a compact disc (CD) drive. Since the CD 
sampling rate of 44.1 kHz is smaller than the DAT sampling rate of 48 kHz, this 
requires a rational decimator. The required frequency conversion ratio is
L
M 5 44.1
48
5 441
480
5 147
160
Consequently, this application requires a rational decimator with L 5 147 and 
M 5 160. From (8.3.2) and (8.3.3) a single-stage composite anti-aliasing and 
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.3 Rational Sampling Rate Converters     607
The DSP Companion contains the following functions for performing integer and 
rational sampling rate conversion. If the MATLAB signal processing toolbox is 
available, then the functions decimate, interp, and resample can be used to change 
the sampling rate.
% F_DECIMATE: Reduce sampling rate by factor M.
% F_INTERPOL: Increase sampling rate by factor L.
% F_RATECONV: Convert sampling rate by rational factor L/M.
%
% Usage:
% 
[y,b] = f_decimate (x,fs,M,m,f_type,alpha)
% 
[y,b] = f_interpol (x,fs,L,m,f_type,alpha)
% 
[y,b] = f_rateconv (x,fs,L,M,m,f_type,alpha)
% Pre:
% 
x 
= a vector of length P containing the input
% 
 
 samples
% 
fs 
= sampling frequency of x
% 
M 
= an integer specifying the conversion
% 
 
 factor (M >= 1)
% 
L 
= an integer specifying the conversion
% 
 
 factor (L >= 1).
% 
m 
= the order of the lowpass FIR anti-
% 
 
 aliasing anti-imaging filter.
DSP Companion
DSP Companion
anti-imaging filter H0( f ) must have a passband gain of H0(0) 5 147 and a cutoff 
frequency of
F0 5 min 5
24
147, 24
1606 kHz
 5 150 Hz
Thus the ideal frequency response for the composite anti-aliasing and anti-imaging 
filter is
H0( f ) 55
147,
0 # u f u , 150
0,
150 # u f u , 24000
This is clearly a narrowband lowpass filter with a normalized cutoff frequency of 
F0yfs 5 .003125. A direct single-stage implementation would require a very high-
order linear-phase FIR filter. This can be avoided if a multistage implementation 
is used. For example, the following three conversion ratios all have single-digit 
integer factors.
147
160 51
7
82 1
7
52 1
3
42
Using this multistage approach, a DAT-to-CD converter can be implemented 
using two decimators and one interpolator. To convert from CD to DAT format, 
the reciprocals (two interpolators and a decimator) can be used. A detailed design 
of a CD-to-DAT sampling rate converter is presented in Section 8.11.
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

608    Chapter 8  Multirate Signal Processing
Polyphase Filters 
Sampling rate converters have a considerable amount of built-in redundancy in terms 
of the required computational effort. In the case of a decimator with M W 1, all of 
the input samples are processed by the lowpass filter, but only every Mth sample of the 
filter output is used. An analogous observation holds for an interpolator with L W 1. 
Here most of the samples that are being processed by the lowpass filter are zero samples 
inserted between the original samples. Consequently, many of the floating-point opera-
tions are multiplications by zero.
8.4.1 Polyphase Decimator
To develop efficient realization structures for rate converters, we begin with a decimator. 
Recall from Figure 8.4 that an integer factor of M decimator consists of a lowpass filter 
with cutoff frequency FM 5 fsy(2M) followed by a down-sampler, TM. Suppose the low-
pass anti-aliasing filter HM(z) is an FIR filter.
 
HM(z) 5 o
m
i50
hM(i)z2i 
 (8.4.1)
8.4
% 
f_type = the FIR filer type to be used:
%
% 
 
0 = windowed (rectangular)
% 
 
1 = windowed (Hanning)
% 
 
2 = windowed (Hamming)
% 
 
3 = windowed (Blackman)
% 
 
4 = frequency-sampled
% 
 
5 = least-squares
% 
 
6 = equiripple
%
% 
alpha  = an optional scaling factor for the
% 
 
 cutoff frequency of the FIR filter.
% 
 
 Default: alpha = 1. If present, the
% 
 
 cutoff frequency used for the anti-
% 
 
 aliasing filter H_0(z) is
%
% 
 
    F_c = alpha*F_0
% Post:
% 
y = a 1 by N vector containing the output
% 
 
samples. Here N = floor(P/M).
% 
b = a 1 by (m+1) vector containing FIR filter
% 
 
coefficients
% Notes:
% 
If L or M are relatively large (e.g. greater
% 
than 10), then it is the responsibility of the user
% 
to perform the rate conversion in stages using
% 
multiple calls. Otherwise, the required value
% 
for m can be very large.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.4 Polyphase Filters     609
Let p 5 floor(myM) be the number of complete segments of length M in the impulse 
response hM. For convenience, pad hM with M 2 1 zeros so that hM(i) is defined for 
0 # i , m 1 M 2 1. One can then define the following sequence of subfilters where Ek(z) 
uses every Mth sample of hM(i) starting with sample k (Bellanger et al., 1976).
 
Ek(z) 5
D  o
p
i50
hM(Mi 1 k)z2i, 0 # k , M 
 (8.4.2)
Here z2kEk(zM) processes every Mth sample of its input starting with sample k for 
0 # k , M. Consequently, all of the samples can be processed using the following 
polyphase representation of HM(z).
 
HM(z) 5 o
M21
k50
z2kEk(zM)  
 (8.4.3)
This is called an M-channel polyphase decomposition of HM(z). It is a parallel form real-
ization where the kth branch operates on the kth phase of the input
 
xk 5 hx(k), x(M 1 k), x(2M 1 k), Á j 
 (8.4.4)
A block diagram of a factor of M decimator using a polyphase decomposition of the 
anti-aliasing filter is shown in Figure 8.12 for the case M 5 4.
The polyphase implementation in Figure 8.12 can be made more efficient by rearrang-
ing the order of the blocks. In general, a filter and a down-sampler can not be interchanged 
without affecting the overall input-output behavior. However, there is an important spe-
cial case called the noble identity when an interchange can be achieved (Cristi, 2004). 
The noble identity for a decimator is shown in block diagram form in Figure 8.13.  
Since the output is identically zero, the upper branch in Figure 8.13 is equivalent to the 
lower branch. To verify this, suppose E(z) in Figure 8.13 is an mth order FIR filter.
 
E(z) 5 o
m
i50
biz2i 
 (8.4.5)
Polyphase 
representation
Phase
Figure 8.12:  
A Factor of M 
Decimator Using 
a Polyphase 
Decomposition of 
the Anti-aliasing 
Filter HM(z) with 
M 5 4 
z–1
z–1
z–1
x0
x1
x2
x3
x(k)
y(k)
1
1
1
E0(z4)
E1(z4)
E2(z4)
E3(z4)
4
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

610    Chapter 8  Multirate Signal Processing
From (8.4.5) the output of the block, E(zM), in the upper branch of Figure 8.13 is
 
x1(k) 5 o
m
i50
bix(k 2 Mi) 
 (8.4.6)
The down-sampler block extracts every Mth sample of x1(k), which yields
 
y1(k) 5 o
m
i50
bix(Mk 2 Mi) 
 (8.4.7)
Next consider the lower branch of Figure 8.13. The output of the down-sampler 
block is simply
 
x2(k) 5 x(Mk) 
 (8.4.8)
Next, the filter E(z) is applied to x2(k), which yields
y2(k) 5 o
m
i50
bix2(k 2 i)
 
5 o
m
i50
bix(Mk 2 Mi) 
 (8.4.9)
From (8.4.9) and (8.4.7), y2(k) ; y1(k), which establishes the noble identity for decima-
tors. In words, if a down-sampler TM is pushed backwards through a filter E(zM), then to 
maintain equivalence the argument, zM must be replaced by z.
Observe from Figure 8.12 that the sampling rate of the input signals processed by the 
subfilters is the original sampling rate fs because the down-sampling occurs as the last 
operation. The down-sampler can be pushed backwards through the summing junctions, 
in which case a copy of it appears in each of the parallel branches following the subfil-
ters. The order of the subfilter and down-sampler blocks then can be interchanged using 
the noble identity in Figure 8.13. The resulting equivalent realization of the decimator is 
shown in Figure 8.14.
Although Figure 8.14 has more blocks than Figure 8.12 due to the replication of 
the down-samplers, the realization in Figure 8.14 is actually more efficient. Note that 
by replacing zM by z in Ek(z) this effectively reduces the length of the subfilter by a fac-
tor of M. Furthermore, the input signals driving the subfilters in Figure 8.14 have been 
down-sampled by a factor of M so the samples are arriving at a slower rate. For conve-
nience, suppose the floating-point operations or FLOPs are measured using multipli-
cations. The number of FLOPs/s that are needed to compute phase xk in Figure 8.14 is 
k 5 f(m 1 1)yMgfsyM. Since there are M phases in Figure 8.14, the required computa-
tional rate for the polyphase output is
 
M 5 (m 1 1)fs
M  FLOPs/s 
 (8.4.10)
Figure 8.13: The 
Decimator Noble 
Identity where  
the Upper Branch 
Is Equivalent to 
the Lower Branch
x(k)
x1
x2
y1
y2
y(k) ≡ 0
1
2
E(zM)
E(z)
M
M
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.4 Polyphase Filters     611
Suppose the filter to be converted to polyphase form has the following transfer 
function.
H(z) 5 1 1 2z21 1 4z22 1 6z23 1 8z24 1 10z25 1 12z26 1 14z27
Suppose M 5 2 polyphase filters are to be used. Then from (8.4.2), H(z) can 
be represented as follows.
H(z) 5 (1 1 4z22 1 8z24 1 12z26) 1 z21(2 1 6z22 1 10z24 1 14z26)
Observe how every M 5 2 samples are processed by z2iEi(z2) starting with sample 
i for 0 # i , 2. Another polyphase representation of H(z) can be obtained using 
M 5 4 polyphase filters.
H(z) 5 (1 1 8z24) 1 z21(2 1 10z24) 1 z22(4 1 12z24) 1 z23(6 1 14z24)
8
3
3
3
3
Figure 8.14:  
A More Efficient 
Realization of 
a Factor of M 
Decimator Using 
a Polyphase 
Decomposition of 
the Anti-aliasing 
Filter with M 5 4 
z21
z21
z21
x0
x1
x2
x3
x(k)
y(k)
1
1
1
E0(z)
E1(z)
E2(z)
E3(z)
4
4
4
4
Polyphase Filters
EXAMPLE 8.5
This is in contrast to a direct realization of HM(z) in (8.4.1) that requires (m 1 1)fs 
FLOPs/s. Thus the polyphase decimator realization is more computationally efficient by 
a factor of M.
E0(z2)
E1(z2)
E0(z4)
E1(z4)
E2(z4)
E3(z4)
8
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

612    Chapter 8  Multirate Signal Processing
8.4.2 Polyphase interpolator
Next consider a polyphase realization of an interpolator. Recall from Figure 8.6 that an 
integer factor of L interpolator consists of an up-sampler cL followed by a lowpass filter 
with cutoff frequency FL 5 fsy(2L). Suppose the lowpass anti-imaging filter HL(z) is an 
FIR filter.
 
HL(z) 5 o
m
i50
hL(i)z2i 
 (8.4.11)
As with the decimator, let p 5 floor(myL) be the number of complete segments of length 
L in the impulse response hL. For convenience, pad h with L 2 1 zeros so that h(i) is 
defined for 0 # i , m 1 L 2 1. The sequence of subfilters is then defined as before but in 
this case there are L phases of the signal and L subfilters to process them.
 
Fk(z) 5
D  o
p
i50
hL(Li 1 k)z2i, 0 # k , L 
 (8.4.12)
Here z2kFk(zL) processes every Lth sample of its input starting with sample k for  
0 # k , L. Hence all of the samples can be processed using the following polyphase rep-
resentation of HL(z).
 
HL(z) 5 o
L21
k50
z2kFk(zL)  
 (8.4.13)
This is an L-channel polyphase decomposition of HL(z). It is a parallel form realization 
where the kth branch operates on the kth phase of the input
 
xk 5 hx(k), x(L 1 k), x(2L 1 k), Á j 
 (8.4.14)
A block diagram of a factor of L interpolator using a polyphase decomposition of the 
anti-imaging filter HL(z) is shown in Figure 8.15 for the case L 5 3.
Just as there is a decimator version of the noble identity, there is also an interpolator 
version and it is shown in Figure 8.16, where F(z) is an mth order FIR filter similar to E(z) 
in (8.4.5). To verify that the upper branch in Figure 8.16 is equivalent to the lower branch, 
Polyphase 
representation
Phase
Figure 8.15: A 
Factor of L 
Interpolator Using 
a Polyphase 
Decomposition of 
the Anti-imaging 
Filter HL(z) with 
L 5 3
x0
x1
x2
x(k)
y(k)
1
1
F0(z3)
F1(z3)
F2(z3)
3
z21
z21
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.4 Polyphase Filters     613
recall from (8.2.6) and (8.2.7) that the output of the first block in the upper branch of 
Figure 8.16 is
 
x1(k) 5 x(kyL)L(k) 
 (8.4.15)
The filter F(zL) is then applied to x1(k), which yields
 
y1(k) 5 o
m
i50
bix 1
k 2 Li
L 2 L(k 2 Li) 
 (8.4.16)
Next consider the lower branch of Figure 8.16. The output of the filter F(z) is simply
 
x2(k) 5 o
m
i50
bix(k 2 i) 
 (8.4.17)
The up-sampler cL is then applied to x2(k). From (8.2.6) and (8.2.7) this yields
y2(k) 5 x2(kyL)L(k)
53o
m
i50
bix1
k
L 2 i24 L(k)
5 o
m
i50
bix 1
k 2 Li
L 2 L(k)
 
5 o
m
i50
bix 1
k 2 Li
L 2 L(k 2 Li) 
 (8.4.18)
The last line in (8.4.18) follows from the fact that L(k) in (8.2.6) is periodic with period L; 
hence delaying it by Li does not change the result. From (8.4.18) and (8.4.16), y2(k) ; y1(k), 
which establishes the noble identity for interpolators. In words, if an up-sampler cL is 
pushed forwards through a filter F(zL), then to maintain equivalence the argument zL must 
be replaced by z.
Observe from Figure 8.15 that the sampling rate of the input signals processed by the 
subfilters is Lfs because the up-sampling occurs as the first operation. The up-sampler can 
be pushed forwards through the pickoff points, in which case a copy of it appears in each 
parallel branch preceding the subfilters. The order of the subfilter and up-sampler blocks 
then can be interchanged using the noble identity in Figure 8.16. The resulting equivalent 
realization of the interpolator is shown in Figure 8.17.
Again, although Figure 8.17 has more blocks than Figure 8.15 due to the replication 
of the up-samplers, the realization in Figure 8.17 is more efficient. Note that by replacing 
zL by z in Fi(z) this effectively reduces the length of the subfilter by a factor of L. Further-
more, the input signals that are now driving the subfilters in Figure 8.15 have not been 
up-sampled by a factor of L so the samples are arriving at a slower rate. The number of 
Figure 8.16:  
The Interpolator 
Noble Identity 
where the 
Upper Branch is 
Equivalent to the 
Lower Branch
1
2
x1
y1
y2
x2
L
L
F(zL)
F(z)
x(k)
y(k) ≡ 0
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

614    Chapter 8  Multirate Signal Processing
FLOPs∙s that are needed to compute phase xk in Figure 8.17 is k 5 f(m 1 1)yLgfsyL. 
Since there are L phases, the required computational rate for the polyphase output is
 
L 5 (m 1 1)fs
L  FLOPs/s 
 (8.4.19)
This is in contrast to a direct realization of HL(z) in (8.4.11) that requires (m 1 1)fs 
FLOPs/s. Thus the polyphase interpolator realization is more computationally efficient 
by a factor of L.
Additional savings in computational effort can be achieved when linear-phase FIR filters 
are used. Recall that linear-phase filters satisfy the symmetry constraint h(m 2 k) 5 6h(k) 
for 0 # k # m. This redundancy in the filter coefficients can be exploited using an approach 
analogous to Figure 6.33 but in the context of the polyphase structure. The same observa-
tion applies to polyphase decimators.
Narrowband Filters
Now that we have a means of changing the sampling rate of a discrete-time signal, this 
technique can be put to work in a number of practical ways. One important application 
is the design of narrowband filters. A narrowband filter is a sharp filter whose passband 
is small in comparison with the sampling frequency. To implement a narrowband filter, 
an IIR filter such as an elliptic filter might be used, but this introduces nonlinear phase 
distortion, and furthermore high-order IIR filters can exhibit detrimental finite word 
length effects. Implementations of linear-phase narrowband filters typically require very 
high-order FIR filters. This implies larger memory requirements, longer computational 
times, increased signal delay, and more significant finite word length effects. The latter 
problem can be reduced by using a multirate design of a narrowband filter. Suppose the 
ideal filter specification is to pass frequencies in the range 0 # u f u # F0 where F0 V fs.
 
H( f ) 55
1,
0 # u f u # F0
0,
F0 , u f u # fsy2
 
 (8.5.1)
The first step of the multirate method is to reduce the sampling rate by an integer 
factor M. This has the effect of increasing the relative width of the passband by a 
8.5
Figure 8.17:  
A More Efficient 
Realization of  
a Factor of L  
Interpolator  
Using a Polyphase 
Decomposition of 
the Anti-imaging 
Filter with L 5 3
1
1
x(k)
y(k)
x0
x1
x2
F0(z)
F1(z)
F2(z)
z21
z21
3
3
3
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.5 Narrowband Filters    615
factor of M. Setting MF0 # fsy4 yields the following upper bound on the decimation 
factor M.
 
M # fs
4F0
 
 (8.5.2)
For the maximum value of M, the new cutoff frequency is MF0 5 fsy4. Consequently, a 
reduction in the sampling rate transforms a narrowband filter, H(z), into a regular filter, 
G(z), with a cutoff frequency that is approximately one fourth of the sampling rate.
 
G( f ) 55
1, 0 # u f u # MF0
0, MF0 , u f u # fsy2
 
 (8.5.3)
A regular filter G(z) is easier to implement than a narrowband filter. To complete the pro-
cess, the original sampling frequency must be restored using sampling rate interpolation 
by a factor of M. The resulting overall implementation of a multirate narrowband filter is 
shown in the block diagram in Figure 8.18. The following example compares a multirate 
narrowband design with a conventional fixed-rate design.
Decimation factor
Figure 8.18: A Multirate Narrowband Filter Using a Rate Conversion Factor M
HM(z)
HM(z)
G(z)
x(k)
y(k)
M
M
Multirate Narrowband Filter
EXAMPLE 8.6
To illustrate the multirate design technique, consider the problem of designing 
an ideal lowpass filter with a cutoff frequency of F0 5 fsy32. Using (8.5.2), the 
decimation factor must satisfy
M # fs
4F0
5 8
Suppose M 5 8, and the windowing method is used to design both HM(z) and G(z). 
When exam8_5 is run, it generates the magnitude responses shown in Figure 8.19. 
To clarify the display, only the first quarter of the frequency range, 0 # f # fsy8, is 
shown. The fixed-rate magnitude response corresponds to a windowed FIR filter 
of order m 5 240 using the Blackman window. For comparison, the multirate 
magnitude response uses a windowed FIR filter of order m 5 80 with the Black-
man window. The anti-aliasing and anti-imaging filters HM(z) are also FIR filters 
of order m 5 80. Thus the two approaches are roughly comparable in terms of 
memory requirements, computational time, and signal delay. However, the multi-
rate design is less sensitive to finite word length effects because it is a cascade of 
three filters of order m 5 80, instead of one filter of order m 5 240. It is evident 
from inspection of Figure 8.19 that the multirate design is superior to the fixed-
rate design in terms of the width of the transition band. The passband ripple of 
the fixed-rate design can be reduced by decreasing m, but this is achieved at the 
expense of further increases in the width of the transition band.
(Continued   )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

616    Chapter 8  Multirate Signal Processing
Filter Banks 
8.6.1 Analysis and Synthesis Banks  
Our investigation of filters thus far has focused on individual filters with a single input 
and a single output. With multirate systems it is often desirable to decompose the overall 
spectrum into N separate frequency bands. For example, using a combination of lowpass, 
bandpass, and highpass filters, the entire digital spectrum can be covered with a bank of 
N subband filters. The magnitude response for a filter bank is shown in Figure 8.20. Since 
the discrete-time frequency response is periodic with period fs, the spectrum in Figure 8.20  
is plotted over the positive frequencies f0, fsg rather than f2fsy2, fsy2g. Notice that the 
subband magnitude responses are not ideal. In particular, the transition bands have non-
zero widths, and adjacent transition bands overlap. This way the entire spectrum is used 
and the overall filter bank is an allpass filter. Decomposing the overall spectrum into N 
separate frequency bands and processing independent subsignals within each band is 
called frequency-division multiplexing.
Filter banks are implemented as parallel configurations of filters, as shown in  
Figure 8.21. The parallel configuration on the left side of Figure 8.19 is called an analysis 
bank because it decomposes the input x(k) into N subsignals. Thus an analysis bank is a 
single-input N-output system. Depending on the application, there may also be a second 
parallel configuration of N filters as shown on the right side of Figure 8.21. This is called 
a synthesis bank because it combines the subsignals into a single composite signal y(k). 
Consequently, a synthesis bank is an N-input single-output system.
For the bank of N filters shown in Figure 8.21, the width of each subband is 1yN 
times the width of the overall spectrum, f0,  fsg. Since each subband is of width fsyN, the 
8.6
Frequency bands
Frequency-division 
multiplexing
Analysis bank
Synthesis bank
Figure 8.19: Magnitude Responses of Narrowband Lowpass Filters Using a 
Fixed-rate Design with m 5 240 and a Multirate Design with m 5 80 Using  
a Rate Conversion Factor M 5 8
0
0.02
0.04
0.06
0.08
0.1
0.12
0
0.2
0.4
0.6
0.8
1
1.2
f/fs
A(f)
Ideal
Fixed-Rate
Multirate
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.6 Filter Banks     617
bandlimited subsignals can be down-sampled or decimated by a factor of N. This makes 
processing of the separate channels more efficient. In general the decimation factor can be 
less than or equal to the number of subbands. When it is equal, as shown in Figure 8.22,  
this is referred to as a fully decimated filter bank.
Following the subband processing, the subsignals are up-sampled or interpolated by 
a factor of N to restore the original sampling rate. Finally, the up-sampled signals are 
recombined into a single signal in the synthesis filter bank on the right.
It is sometimes the case that an analysis filter bank is used by itself without a match-
ing synthesis bank. For example, each subfilter in Figure 8.22 extracts its own frequency 
band, and together these subbands cover the entire spectrum f0, fsg. The subband outputs 
therefore can be used for real-time spectral analysis of the input signal. In this way one 
can see how the spectral content of x(k) changes with time. Another important appli-
cation for subband processing is data compression as can be seen from the following 
example.
Fully decimated bank
Spectral analysis
Figure 8.20:  
Magnitude 
Responses of a 
Bank of N 5 3 
Subband Filters 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
20.5
0
0.5
1
1.5
f/fs
A(f)
A0
A1
A2
A0
Figure 8.21: Analysis and Synthesis Filter Banks
x(k)
y(k)
1
Subband
Processing
H0(z)
H1(z)
Analysis
Synthesis
HN 2 1(z)
GN 2 1(z)
G1(z)
G0(z)
...
...
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

618    Chapter 8  Multirate Signal Processing
Figure 8.22: Fully Decimated and Interpolated Filter Banks
x
y
1
Subband
Processing
H0
H1
HN 2 1
GN 2 1
G1
G0
N
N
N
N
N
N
...
...
...
...
MP3 Files
EXAMPLE 8.7
Subsignals created by an analysis filter bank are sometimes encoded in order 
to achieve data compression. Data compression results in practical savings in 
storage requirements and transmission times. For example, suppose x(k) rep-
resents an audio recording. A popular convention for compressing, saving, and 
transmitting music is the MPEG-1 Audio Layer III format, more commonly 
known as an MP3 file, as shown in Figure 8.23. The size of a data file used to 
save one second of music depends on both the sampling rate (the total number 
of samples) and the sample precision (number of bits per sample). To mea-
sure the compression factor, the bit rate, expressed in units of kbit/s, is used. 
For example, uncompressed audio CD format music uses a sampling rate of 
fs 5 44.1 kHz. If 32 bits per sample are used, then the uncompressed bit rate 
for an audio CD is
f
 CD 5 44.1(32) 5 1411.2  kbitys 
The MP3 format uses a lossy compression algorithm in order to reduce the 
file size for saving music. The basic idea is to abandon the objective of faithfully 
representing every detail of the music waveform. Instead, it exploits the charac-
teristics of human hearing and perception to perform perceptual coding based 
on psychoacoustic models (Jayant et al., 1993). The subsignals xi in Figure 8.22 
are associated with frequency bands. Episodes of silence and frequency bands of 
sounds that are masked by other sounds are grouped together into frames. Other 
frames contain clearly perceived high-priority sounds. Depending on the relative 
importance of frames to human perception, they are assigned a number of bits in 
such a way as to achieve a desired overall bit rate. The bit allocation information 
and frame size are stored in a header followed by the compressed frame data. The 
MP3 file then consists of a sequence of header-data pairs. Fourteen different bit 
rates, ranging from 32 kbit/s to 320 kbit/s, are possible as part of the MPEG-1 
MP3 standard as shown in Table 8.1. The choice of a bit rate represents a tradeoff 
MP3 file
Bit rate
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.6 Filter Banks     619
Figure 8.23: Using an MP3 Encoder to Create Compressed Music Files
x
MP3
Encoder
.mp3 file
GN − 1
G1
G0
N
N
N
...
...
between the overall quality of the compressed recording and the amount of com-
pression (file size) achieved. A commonly used bit rate is 128 kbit/s resulting in a 
compression ratio of 11.0. It should be pointed out that the compression ratios 
and the file sizes do not include the overhead associated with the header informa-
tion for each frame.
A plot showing how the compression ratios and the file sizes (Mbyte/min) vary 
with the MP3 bit rates is shown in Figure 8.24, where the common case of a 128 
kbit/s is highlighted with a solid plot symbol. The problem of implementing an 
MP3 decoder or player is comparatively simple. The header information is used 
to convert each frame back into subband signals yi. These subband signals are 
inputs to a synthesis filter bank that reconstructs a time signal y(k) that can be 
played on an audio device. The signal y(k) no longer represents every detail of 
the uncompressed waveform x(k), but the aspects of it that are most important to 
human hearing and perception have been preserved.
Bit Rate
Compression
File Size
(kbit/s)
Ratio 
(Mbyte/min)
 32
44.1
0.240
 40
35.3
0.300
 48
29.4
0.360
 56
25.2
0.420
 64
22.1
0.480
 80
17.6
0.600
 96
14.7
0.720
112
12.6
0.840
128
11.0
0.960
160
8.8
1.200
192
7.4
1.440
224
6.3
1.680
256
5.5
1.920
320
4.4
2.400
Table 8.1: MP3 Bit Rates 
and Compression Ratios 
for Audio CD Data
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

620    Chapter 8  Multirate Signal Processing
8.6.2 Subfilter Design
Subfilter Gi(z) in Figure 8.22 is designed to pass frequencies within a band of width fsyN 
centered at Fi and similarly for subfilter Hi(z). Here
 
Fi 5 ifs
N , 0 # i , N 
 (8.6.1)
Each of the subfilters can be generated from a single prototype lowpass filter by shifting 
the frequency of the prototype filter by Fi. Let G(z) and H(z) be lowpass prototype filters 
with cutoff frequencies of fsy(2N). The overall development can be simplified by letting 
G(z) be a noncausal lowpass prototype filter and H(z) be a causal lowpass prototype filter 
(Chrisi, 2004).
 
G(z) 5 o
m
k50
g(2k)zk 
 (8.6.2)
 
H(z) 5 o
m
k50
h(k)z2k 
 (8.6.3)
Whereas z2k can be thought of as representing a delay of k samples, zk represents an 
advance of k samples. A noncausal prototype filter, G(z), can be designed using the lin-
ear-phase FIR filter design techniques presented in Chapter 6. One simply designs a 
causal linear-phase FIR filter of order m. The transfer function is then multiplied by 
zm to insert a time advance of m samples. This does not change the magnitude response 
because uzu 5 1 on the unit circle. The sign of the phase response changes which then gives 
a constant group delay of 2my2 samples.
Prototype filter
Figure 8.24: MP3 Compression Ratios and File Sizes 
0
50
100
150
200
250
300
350
0
20
40
60
Bit Rate (kb/s)
Compression Ratio
0
50
100
150
200
250
300
350
0
1
2
3
Bit Rate (kb/s)
File Size (MB/min)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.6 Filter Banks     621
Let G( f ) and H( f ) denote the frequency responses of the low pass prototype filters. 
Recall from the frequency shift property of the DTFT in Table 4.3 that modulation by a 
complex exponential shifts the frequency response.
 
 DTFT hexp( jk2FiT )g(k)j 5 G (  f 2 Fi) 
 (8.6.4)
 
 DTFT hexp( jk2FiT )h(k)j 5 H(  f 2 Fi) 
 (8.6.5)
The DFT defined in Section 4.3 makes use of the constant WN 5 exp(2j2yN). Using 
(8.6.1), the complex exponential coefficient used to shift the frequency can be expressed 
as follows.
exp( jk2FiT ) 5 exp( jki2yN)
 
5 W2ki
N  
 (8.6.6)
Consequently, if the kth coefficient of the prototype lowpass filter is multiplied by W2ki
N , 
the resulting filter is a bandpass filter centered at Fi. That is,
 
Gi( f ) 5 G(f 2 Fi) 
 (8.6.7)
 
Hi( f ) 5 H(f 2 Fi) 
 (8.6.8)
The transfer functions of the subband filters can be expressed in terms of the low-
pass prototype transfer functions. Recall from the z-scale property of the Z-transform in  
Table 3.3 that
 
Zhakx(k)j 5 X 1
z
a2 
 (8.6.9)
Since g(k) is multiplied by (W2i
N )k and similarly for h(k), this yields the following 
subband filter transfer functions for 0 # i , N. Here the two-sided Z-transform is used 
because g(k) is noncausal.
 
Gi(z) 5 ZhW2ki
N g(k)j 5 G(W i
Nz) 
 (8.6.10)
 
Hi(z) 5 ZhW2ki
N h(k)j 5 H(W i
Nz) 
 (8.6.11)
8.6.3 Polyphase Representation
An efficient implementation of filter banks can be obtained if the subband filters are repre-
sented using polyphase filters. Polyphase filters Ek(z) and Fk(z) were defined in Section 8.4  
for a factor of M decimator and a factor of L interpolator, respectively. Since Figure 8.22  
uses factor of N decimators and interpolators, we replace M and L by N and use 
p 5
 floor (myN).
 
Ek(z) 5 o
p
i50
g(Ni 1 k)z2i 
 (8.6.12)
 
Fk(z) 5 o
p
i50
h(Ni 1 k)z2i 
 (8.6.13)
Both the noncausal prototype filter G(z) and the causal prototype filter H(z) can be 
represented using the polyphase filters.
 
G(z) 5 o
N
k50
zkE2k(zN) 
 (8.6.14)
 
H(z) 5 o
N
k50
z2kFk(zN) 
 (8.6.15)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

622    Chapter 8  Multirate Signal Processing
For the noncausal filter, the term zkE2k(zN) processes every Nth sample of the input start-
ing with sample 2k. For the causal filter, the term z2kFk(zN) processes every Nth sample 
of input starting with sample k for 0 # k , N.
The last step is to show that the polyphase representations can be implemented 
directly using the DFT. First consider the analysis bank, which uses Gi(z). Let Xk denote 
the kth term in (8.6.14). That is,
 
Xk 5 zkE2k(zN), 0 # k , N 
 (8.6.16)
Using (8.6.10), (8.6.14), and the fact that WN
N 5 1, we have the following representa-
tion for subfilter Gi(z).
Gi(z) 5 G(W i
Nz)
 5 o
N21
k50
(W i
Nz)kE2kf(W i
Nz)Ng
 5 o
N21
k50
W ik
Nz kE2k(W Ni
N zN)
 5 o
N21
k50
W ik
Nz kE2k(zN)
 
 5 o
N21
k50
XkW ik
N
 
 (8.6.17)
But (8.6.17) is the ith component of the DFT of Xk. That is, the vector G(z) 5 
fG0(z), G1(z), Á , GN21(z)g is the DFT of the vector X 5 fX0, X1, Á , XN21g. A block 
diagram of the polyphase implementation of a DFT analysis filter bank is shown in  
Figure 8.25. Here we have used the noble identity of decimators to push the down- 
samplers in Figure 8.22 backward through the polyphase filters. Of course if N is selected 
to be a power of two, then the implementation in Figure 8.25 can be made even more 
computationally efficient by using the FFT.
Figure 8.25:  
Polyphase 
Implementation 
of a DFT Analysis 
Bank
z
z
DFT
x0(k)
x(k)
x1(k)
E0(z)
E21(z)
E1 2 N(z)
xN 2 1(k)
N
N
N
???
???
???
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.6 Filter Banks     623
Next consider the synthesis bank which uses subfilters Hi (z). Let Yk denote the kth 
term in (8.6.15). That is,
 
Yk 5 z2kFk(zN), 0 # k , N 
 (8.6.18)
Using (8.6.11), (8.6.15), and the fact that WN
N 5 1, we have the following representa-
tion for subfilter Hi(z).
Hi(z) 5 H(Wi
Nz)
5 o
N21
k50
(W i
Nz)2kFkf(W i
Nz)Ng
5 o
N21
k50
W 2ik
N z2kFk(W Ni
N zN)
5 o
N21
k50
W2ik
N z2kFk(zN)
 
5 o
N21
k50
YkW2ik
N  
 (8.6.19)
Here it is clear from (8.6.19) that Hi(z) is N times the ith component of the IDFT of Yk. That 
is, apart from scaling by N, the vector H(z) 5 fH0(z), H1(z), Á , HN21(z)g is the IDFT of 
the vector Y 5 fY0, Y1, Á , YN21g. A block diagram of the polyphase implementation of a 
DFT synthesis filter bank is shown in Figure 8.26. Again, the noble identity for interpolators 
has been used in Figure 8.22 to push the up-samplers forward through the polyphase filters.
Interestingly enough, the two-stage quadrature filter developed in Section 6.7 and 
pictured previously in Figure 6.25 also can be regarded as a filter bank. The first stage 
is an analysis bank that decomposes the input x(k) into quadrature components in the 
form of delayed cosine and sine terms. Stage 2 is then a synthesis bank that reconstructs 
the output y(k). The stage two filters realize the real and imaginary parts of the desired 
frequency response, respectively.
Figure 8.26:  
Polyphase 
Implementation 
of a DFT Synthesis 
Bank 
IDFT
y0(k)
y(k)
y1(k)
yN 2 1(k)
NF0(z)
NF1(z)
NFN 2 1(z)
N
N
N
???
???
???
1
1
z21
z21
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

624    Chapter 8  Multirate Signal Processing
Perfect Reconstruction Filter Banks
The DFT analysis and synthesis filter banks in Figure 8.25 and Figure 8.26 can be 
simplified by developing models of the down-sampling and up-sampling sections using 
the notion of time-division multiplexing. This leads to a simple criterion, under which 
a signal decomposed by an analysis bank can be perfectly reconstructed by a synthesis 
bank.
8.7.1 Time-division Multiplexing 
Time-division multiplexing is a technique where several time signals, ui(k) for 0 # i , N, 
are combined into a single composite signal x(k) that is transmitted. At the receiver end, 
the subsignals are extracted from the composite signal using a process called time-division 
8.7
The DSP Companion contains the following functions for implementing analysis 
and synthesis DFT filter banks. The lowpass prototype filters with impulse responses 
g and h are both causal filters. When they are FIR linear-phase filters of order m, the 
bank outputs are delayed by my2 samples.
% F_SYNBANK:   Compute the composite output of a DFT synthesis
% 
filter bank
% F_ANALBANK:  Compute the subsignal outputs of a DFT analysis
% 
filter bank
%
% Usage:
% 
x = f_synbank (X,h);
% 
X = f_analbank(x,g,N);
% Pre:
% 
x =  complex vector of length r = Nq containing the
% 
samples of a composite signal containing N interlaced 
% 
subsignals. If fs is the sampling rate of the 
% 
subsignals, then N*fs is the sampling rate of x.
% 
X = p by N matrix containing subsignal i in column i
% 
h =  impulse response of a causal lowpass prototype
% 
filter for f_synbank
% 
g =  impulse response of a causal lowpass prototype
% 
filter for f_analbank
% 
N =  number of subsignals in x
% Post:
% 
X and x
% Notes:
% 
 When g and h are mth order FIR linear-phase lowpass
% 
filters,
% 
 f_analbank and f_synbank delay the output by m/2
% 
samples.
DSP Companion
DSP Companion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.7 Perfect Reconstruction Filter Banks    625
demultiplexing. For example, suppose ui(k) appears as every Nth sample of x(k) starting 
with sample i. Then to extract ui(k) from x(k) we use
 
ui(k) 5 x(Nk 1 i), 0 # i , N 
 (8.7.1)
Consequently ui is simply the ith phase of the signal x(k). That is,
 
ui 5 hx(i), x(N 1 i), x(2N 1 i), Á j 
 (8.7.2)
To extract ui from x in a block diagram, we shift x by i samples using i time advance 
blocks with transfer function z, and then down-sample the shifted signal by N. The 
resulting system is called a time-division demultiplexor (DMUX). A 1 to N demultiplexor 
is shown in Figure 8.27. It is a single-input N-output system with input x(k) and output 
ui(k) for 0 # i , N.
The task of synthesizing a single composite signal x(k) from a set of N subsignals 
ui(k) for 0 # k , N is called time-division multiplexing. The subsignals are interlaced 
into every Nth sample of the composite signal x(k) as follows.
 
x 5 hu0(0), u1(0), Á , uN21(0), u0(1), u1(1), Á , uN21(1), u0(2), Á j  
 (8.7.3)
Again the samples of phase ui appear as every Nth sample of x starting with sample i. 
To construct x(k) with a block diagram we first up-sample ui(k) by N. Recall from (8.2.7) 
that this inserts N 2 1 zeros between the original samples of ui(k). The up-sampled signal 
must then be delayed by i samples by passing it through i time delay blocks with transfer 
function z21. The delayed signals are then combined. The resulting system is called a  
time-division multiplexor (MUX). An N to 1 multiplexor is shown in Figure 8.28. It is a  
N-input single-output system with inputs ui(k) for 0 # i , N and output x(k).
The multiplexor and demultiplexor systems are inverses of one another. If a 1 by N 
demultiplexor is followed by an N by 1 multiplexor, then the single output matches the 
single input. Similarly, if a N by 1 multiplexor is followed by a 1 by N demultiplexor, then 
the N outputs match the N inputs.
Time-division 
demultiplexor
Time-division 
multiplexor
Figure 8.27:  
A 1 to N  
Time-division 
Demultiplexor 
(DMUX) 
z
z
1 to N
DMUX
u0(k)
x(k)
u1(k)
x(k)
uN 2 1(k)
u0(k)
u1(k)
uN 2 1(k)
???
???
N
N
N
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

626    Chapter 8  Multirate Signal Processing
8.7.2 Perfect Reconstruction
A time-division demultiplexor appears at the front end of the DFT analysis bank in Fig-
ure 8.25. Here the demultiplexor serves to extract the N phases of the input signal x(k). 
Similarly, a time-division multiplexor appears at the back end of the DFT synthesis bank 
shown in Figure 8.26. Here its function is to combine the N phases into a composite out-
put signal y(k). A simplified block diagram of an analysis bank followed by a synthesis 
bank using the demultiplexor and multiplexor blocks is shown in Figure 8.29. Note that 
the DFT and IDFT blocks have dropped out due to cancellation.
Figure 8.28: An N to 1 Time-division Multiplexor (MUX)
N to 1
MUX
u0(k)
u1(k)
uN 2 1(k)
u0(k)
u1(k)
uN 2 1(k)
x(k)
???
???
y(k)
1
1
z21
z21
N
N
N
Figure 8.29: Simplified Analysis and Synthesis DFT Filter Banks
x(k)
y(k)
1 to N
DMUX
N to 1
MUX
...
...
NF0(z)
E0(z)
E21(z)
NF1(z)
NFN 2 1(z)
E1 2 N(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.7 Perfect Reconstruction Filter Banks    627
A combined analysis and synthesis filter bank is referred to as a perfect reconstruction 
filter bank if the output from the synthesis bank exactly matches the input to the analysis 
bank. That is, the filter bank in Figure 8.29 is a perfect reconstruction filter bank if
 
y(k) 5 x(k), uku $ 0 
 (8.7.4)
When FIR filters are used for the prototype lowpass filters, the frequency responses of 
the subband filters overlap. In spite of this coupling, it is still possible to achieve perfect 
reconstruction.
The filter bank in Figure 8.29 exhibits perfect reconstruction with y(k) 5 x(k) for 
uku $ 0 if the lowpass prototype filters G(z) and H(z) are selected such that
NE2k(z)Fk(z) 5 1, 0 # k , N
Prop. 8.1 is evident from inspection of Figure 8.29 because when NE2k(z)Fk(z) 5 1 
for 0 # k , N, Figure 8.29 collapses to a cascade connection of a demultiplexor followed 
by its inverse, a multiplexor. The polyphase filters in Prop. 8.1 depend on the prototype 
lowpass filters G(z) and H(z) that are designed by the user. The following example illus-
trates how the prototype filters might be selected.
Perfect reconstruction 
filter bank
PRoPoSiTioN
8.1 Perfect Reconstruction 
Filter Bank 
Perfect Reconstruction: FiR Prototype
EXAMPLE 8.8
Suppose the prototype filters in (8.6.2) and (8.6.3), respectively, are both of order 
m 5 N 2 1. Since p 5
 floor (myN) 5 0, it then follows from (8.6.12) and (8.6.13) 
that the polyphase filters are simply constants with
E2k(z) 5 g(2k)
 Fk(z) 5 h(k)
This leads to many solutions that satisfy Prop. 8.1. For example, if H(z) is designed 
with h(k) Þ 0 for 0 # k , N, then g(2k) 5 1yfNh(k)g. A simple special case is 
h(k) 5 1yN for 0 # k , N which is a moving average filter. Then g(2k) 5 1 for 
0 # k , N. In this case the prototype filters are
G(z) 5 1 1 z 1 Á 1 zN 2 1
H(z) 5 1 1 z21 1 Á 1 z2(N21)
N
When these prototype filters are used, the analysis and synthesis filter banks 
in Figure 8.25 and Figure 8.26 take on a particularly simple form as shown in  
Figure 8.30.
A frequency domain version of the criterion in Prop. 8.1 is obtained by evaluating 
the criterion along the unit circle using z 5 exp( j2f T).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

628    Chapter 8  Multirate Signal Processing
The filter bank in Figure 8.29 exhibits perfect reconstruction with y(k) 5 x(k) for  
uku $ 0 if the lowpass prototype filters G(z) and H(z) are selected such that for u f u # fsy2
NE2k( f )Fk( f ) 5 1, 0 # k , N
This formulation can be used to construct an alternative set of lowpass prototype 
filters for perfect reconstruction based on ideal filters.
PRoPoSiTioN
8.2 Perfect Reconstruction: 
Frequency Domain
Figure 8.30: Analysis and Synthesis Filter Banks Using the Prototype Filters in Example 8.8
IDFT
1 to N
DMUX
N to 1
MUX
DFT
Analysis Bank
Synthesis Bank
x
yN 2 1
y0
y1
xN 2 1
x0
x1
y
???
???
Perfect Reconstruction:  
ideal Lowpass Prototype
EXAMPLE 8.9
Suppose the prototype filters are ideal lowpass filters with identical frequency 
responses G( f ) 5 A( f ) and H( f ) 5 A( f ). Let the passband gain be ÏN and the 
cutoff frequency be 1y(2N).
A( f ) 55
ÏN,
0 # u f u , fs
2N
0,
 
otherwise 
From (8.4.5), the frequency response of E2i(z) is the DTFT of the impulse 
response g(Nk 2 i). Note that g(k) is bandlimited to B 5 fsy(2N). Using the time 
shift and time scale properties of the DTFT from Appendix 1.5, the frequency 
response E2k( f ) is
E2i( f ) 5
 DTFT hg(Nk 2 i)j
5
 DTFT hg(Nk)j exp(2ji2f T )
5 1
NG 1
f
N2 exp(2ji2f T )
5 1
N A 1
f
N2 exp(2ji2f T )
51
1
ÏN2 exp(2ji2f T ), 2 fsy2 # f , fsy2
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.7 Perfect Reconstruction Filter Banks    629
Approximate Reconstruction
The ideal lowpass prototype filters with impulse responses g(k) and h(k) can be approx-
imated with causal mth order FIR lowpass linear-phase filters. When G(z) in (8.6.2) is 
replaced with a causal filter similar to H(z) in (8.6.3), the coefficients of the polyphase 
filter Ek(z) in (8.6.12) should be changed to g(Ni 1 N 2 k), and E2k(z) in Figure 8.25 
should be replaced by Ek(z) for 0 # k , N. Once this is done, the analysis bank and  
the synthesis bank each introduce a delay of my2 samples. In this case, the output of an 
analysis bank followed by a synthesis bank satisfies
 
y(k) < x(k 2 m) 
 (8.7.5)
This is a more general criterion for a perfect reconstruction filter bank where the shape of 
the signal is preserved but the signal is delayed, and possibly scaled by , depending on 
the passband gains of the prototype filters.
The DSP Companion contains the following functions for performing time-division 
multiplexing and demultiplexing. These functions perform inverse operations.
% F_TIMEMUX:  Construct x using time-division multiplexing
% F_TIMEDMUX: Decompose X using time-division demultiplexing
%
% Usage:
% 
x = f_timemux (X);
% 
X = f_timedmux (x,N);
% Pre:
% 
X = p by N matrix containing subsignal i in column i
% 
N = number of subsignals in x
% 
x =  complex vector of length r = Nq containing the 
% 
 
samples of a composite signal containing N 
% 
 
interlaced subsignals. If fs is the sampling rate 
% 
 
of the subsignals, then N*fs is the sampling rate of x.
% Post:
% 
x and X
DSP Companion
DSP Companion
Similarly from (8.4.12), the frequency response of Fi(z) is the DTFT of the 
impulse response h(Nk 1 i).
Fi( f ) 5
 DTFT hh(Nk 1 i)j
5
 DTFT hh(Nk)j exp( ji2f T)
5 1
NH 1
f
N2 exp( ji2f T)
5 1
NA1
f
N2 exp( ji2f T)
51
1
ÏN2 exp( ji2f T ), 2fsy2 # f , fsy2
It then follows that NE2i( f )Fi( f ) 5 1 for 0 # i , N, which implies perfect 
reconstruction.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

630    Chapter 8  Multirate Signal Processing
Transmultiplexors
An important new category of applications for filter banks arises if the order of the two 
banks is interchanged as shown in Figure 8.31. When a synthesis bank is followed by an 
analysis bank, the resulting N-input N-output system is referred to as a transmultiplexor 
system. Transmultiplexors are used in communications to simultaneously transmit several 
subsignals over a single channel using frequency-division multiplexing. Here C(z) rep-
resents the transfer function of a communication channel over which the composite signal 
x(k) is transmitted. When present, D(z) represents a channel equalizer that is designed to 
restore a signal that has been degraded as a result of transmission over the channel.
First consider the case when the channel is ideal with C(z) 5 1. Here there is no need 
for an equalizer, so D(z) 5 1. Replacing the synthesis and analysis banks by the DFT 
banks shown in Figure 8.26 and Figure 8.25, respectively, we arrive at the simplified 
transmultiplexor system shown in Figure 8.32. Note that the multiplexor and demulti-
plexor blocks have dropped out due to cancellation.
By comparing Figure 8.32 with Figure 8.29, it is clear that the two configurations are 
functionally similar. In particular, the criteria needed to achieve perfect reconstruction 
summarized in Propositions 8.1 and 8.2 apply to transmultiplexors as well. Consequently, 
the prototype filters designed in Examples 8.7 and 8.8 also can be used for transmulti-
plexor systems.
Next suppose the communications channel is not ideal. For example, in addition to 
additive noise, the channel may delay and attenuate the signal, and this can be represented 
by the transfer function C(z). For nonideal communications channels, the criterion for 
perfect reconstruction needs to be relaxed somewhat to include a delay of r samples for 
some r $ 0 and possibly scaling by .
 
y(k) 5 x(k 2 r) 
 (8.8.1)
When C(z) Þ 1, an equalizer system with transfer function D(z) can be included in 
front of the analysis bank as shown in Figure 8.31. The purpose of the equalizer is to 
restore the signal to its original form. Equalizers were discussed in Chapters 5 and 6. 
Using Algorithm 5.2, any IIR transfer function C(z) can be decomposed into the product 
of an allpass part and a minimum-phase part as follows.
 
C(z) 5 C
 all (z)C
 min (z) 
 (8.8.2)
The allpass part, as the name implies, is an allpass filter with uC
 all ( f )u 5 1. The  
minimum-phase part has no zeros outside the unit circle, and from (8.8.2) it has a 
8.8
Transmultiplexor 
system
Figure 8.31: A Transmultiplexor System
xN 2 1
x0
x1
???
yN 2 1
y0
y1
x
y
Equalizer
Channel
Synthesis
Bank
Analysis
Bank
???
C(z)
D(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.8 Transmultiplexors    631
magnitude response that is identical to C(z). The equalizer is constructed by using the 
inverse of the minimum-phase part of C(z).
 
D(z) 5 C 21
 min (z) 
 (8.8.3)
The cascade combination, C(z)D(z), is then an allpass filter
 
C(z)D(z) 5 C
 all (z) 
 (8.8.4)
Thus the equalizer system D(z) restores the channel magnitude response to match 
that of an ideal channel. However, it may or may not restore the phase response as well 
because the allpass filter C
 all (z) is not, in general, a linear-phase filter. The one impor-
tant exception is when the channel transfer function C(z) does not contain any zeros 
outside the unit circle. In this case, C(z) is itself a minimum-phase system in which case 
C
 all (z) 5 1.
If C(z) is not a minimum-phase system, or an explicit expression for the transfer 
function C(z) is not available, then two other approaches to designing an equalizer might 
be used. The first is to measure the channel frequency response, C( f ), and then design 
a quadrature filter that cancels both the amplitude response and the residual phase 
response as discussed in Section 6.7. The second approach is to work in the time domain 
and design an equalizer using an adaptive filter D(z). Here a cascade combination of C(z) 
and the adaptive filter D(z) is placed in parallel with the desired transfer function z2r as 
shown in Figure 8.33. The error e(k) is fed back to D(z) in order to adjust its coefficients 
so as to minimize the squared error e2(k) using the LMS method. Design techniques for 
adaptive filters, including equalizers or inverse systems, are treated in detail in Chapter 9.
Adaptive filter
Figure 8.32: Transmultiplexor with Ideal Communication Channel
yN 2 1
E1 2 N(z)
NFN 2 1(z)
E21(z)
E0(z)
NF0(z)
NF1(z)
y0
DFT
IDFT
y1
xN 2 1
x0
x1
???
???
???
???
Figure 8.33:  
Design of an 
Equalizer for C(z) 
Using an Adaptive 
FIR System
1 2
C(z)
D(z)
y(k)
x(k)
e(k)
z2r
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

632    Chapter 8  Multirate Signal Processing
Figure 8.34: Spectra of Four Bandlimited Subsignals with a Bandwidth  
of B 5 fsy4
25
0
5
20.5
0
0.5
1
1.5
f (Hz)
A(f)
u2
u3
u1
u0
EXAMPLE 8.10
To illustrate the process of signal synthesis, let the number of filters in the bank be 
N 5 4, and let the sampling frequency be fs 5 10 Hz. Suppose the subsignals are 
of length p 5 64, and suppose all subsignals are bandlimited to u f u # F0 where 
F0 5 fsy4. For convenience, the subsignals are defined in terms of their spectra as 
follows, where fi 5 ifsyp for uiu # py4.
U0(i) 5 cos1
fi
2F02
U1(i) 5 0.75(1 2 ufiuyF0)
U2(i) 5 usin1
fi
F02u
U3(i) 5 0.5
Plots of the magnitude spectra are shown in Figure 8.34. After interchanging the 
two halves of the spectra using the MATLAB function fftshift, the time signals 
are then recovered from the spectra as follows using the inverse DFT.
uq(k) 5 IDFThUq(i)j, 0 # q , 4
Signal Synthesis
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.8 Transmultiplexors    633
Next the uq(k) are up-sampled by a factor of N interpolator with anti-imaging 
filter HN(z) to produce xq(k) for 0 # q , N. Here HL(z) is a windowed lowpass 
Blackman filter of order m 5 120 with cutoff frequency fsy(4N).
xq(k) 5 o
m
i50
bi uq 1
k 2 i
L 2 L(k 2 i), 0 # q , 4
The complex signal x(k) is constructed by first multiplying xq(k) by W2qk
N  to 
shift the frequency to Fq 5 q fsyN. The composite signal x(k) is then the sum of 
the modulated subsignals.
x(k) 5 o
3
q50
xq(k)W2qk
4
Plots of the real and imaginary parts of the composite signal x(k), obtained by 
running exam8_9, are shown in Figure 8.35. Note that due to the up-sampling, 
x(k) is now of length Np 5 256.
Next consider transmission over a communication channel. Suppose the 
communication channel is modeled as a first-order lowpass filter with delay
C(z) 5 (z 1 1)z210
z 2 .9
Since C(z) has no zeros outside the unit circle, it is a minimum-phase system. 
Factoring out the delay of r 5 10 samples, the inverse of the remaining system 
can be used as an equalizer
D(z) 5 z 2 .9
z 1 1
Figure 8.35: Real and Imaginary Parts of the Composite High-bandwidth 
Signal x(k) Containing Four Subsignals Using Frequency-division Multiplexing 
0
50
100
150
200
250
300
20.2
0
0.2
0.4
0.6
k
Real{x(k)}
0
50
100
150
200
250
300
20.04
20.02
0
0.02
0.04
k
Imag{x(k)}
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

634    Chapter 8  Multirate Signal Processing
oversampled A-to-D Converters
8.9.1 Anti-aliasing Filters
One of the practical difficulties associated with analog-to-digital conversion is the need 
for a lowpass analog anti-aliasing prefilter to bandlimit the signal to less than half of 
the sampling rate. Sharp high-order analog filters have characteristics that can drift with 
time and even cause them to become unstable. Using multirate techniques, some of the 
anti-aliasing task can be transferred to the digital domain and thereby allow for a simpler 
low-order analog filter.
 8.9
Optional material
Figure 8.36: Magnitude Spectrum of the Composite High-bandwidth Signal 
Showing the Spectra of Subsignals in Each of Four Bands Using a Windowed 
Blackman Anti-imaging Filter 
0
5
10
15
20
25
30
35
40
20.5
0
0.5
1
1.5
2
f (Hz)
A(f)
A0
A0
A1
A2
A3
The cascade combination then behaves as a pure delay of r 5 10 samples.
C(z)D(z) 5 z210
The input to the analysis bank at the receiver end is
y(k) 5 x(k 2 10)
Next, the magnitude spectrum of y(k) is computed using
A(i) 5 uDFThy(k)ju, 0 # i , Np
The resulting magnitude spectrum of y(k) is shown in Figure 8.36. It is clear that 
the spectra of xq(k) have been shifted and centered at Fq 5 qfsyN for 0 # q , N.
*
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 8.9 Oversampled A-to-D Converters    635
Suppose the range of frequencies of interest for an analog signal xa(t) is 0 # u F u # Fa. 
Normally, to avoid aliasing one must bandlimit xa(t) to Fa Hz with a sharp analog low-
pass filter, and then sample at a rate fs that is greater than twice the bandwidth. Instead 
consider oversampling xa(t) using the following increased sampling rate.
 
fs 5 2MFa 
 (8.9.1)
Here M is an integer greater than one. This corresponds to oversampling by a factor of 
fsy(2Fa) 5 M. Oversampling by a factor of M significantly reduces the requirements for 
the anti-aliasing filter. The analog anti-aliasing filter now has to satisfy the following 
simpler frequency response specification.
 
Ha( f ) 55
1,
0 # u f u # Fa
0,
MFa # u f u , ` 
 (8.9.2)
Even though Ha( f ) is still an ideal filter with no passband ripple and complete stopband 
attenuation, the width of the transition band Df 5 (M 2 1)Fa is no longer zero. Given 
a large transition band, Ha(s) can be approximated with a simple inexpensive low-order 
filter such as a first- or second-order Butterworth filter, realizable by a single integrated 
circuit as discussed in Section 1.5. Recall from (7.4.1) that the magnitude response of an 
nth order analog Butterworth lowpass filter with a cutoff frequency of Fa is
 
An( f ) 5
1
Ï1 1 (fyFa)2n 
 (8.9.3)
The tradeoff of an increased sampling rate for a simpler analog anti-aliasing filter 
leads to a discrete-time signal that is sampled at a rate that is significantly higher than 
twice the maximum frequency of interest. Following the sampling operation, one can 
reduce the sampling rate to the minimum value needed using a decimator. The resulting 
structure, called an oversampled ADC, is shown in the block diagram in Figure 8.37. Note 
that it has two anti-aliasing filters: a low-order analog filter Ha(s) with cutoff frequency Fa, 
and a higher-order digital filter HM(z), also with cutoff frequency Fa.
The use of a simpler analog anti-aliasing filter is an important benefit of the over-
sampled ADC, but it is not the only one. Another advantage becomes apparent upon 
examination of the ADC quantization noise. Recall from the discussion of quantization 
in (6.9.3) that if uxa(t)u # c, then the quantization level or precision of an N-bit ADC is
 
q 5
c
2N 2 1 
 (8.9.4)
The quantized output of the ADC can be represented as follows, where x(k) is the exact 
value, and v(k) is the quantization error.
 
xq(k) 5 x(k) 1 v(k) 
 (8.9.5)
Assuming rounding is used, the quantization error, v(k), can be modeled as white noise 
uniformly distributed over the interval f2qy2, qy2g. It was shown in (6.9.6) that the aver-
age power of the quantization noise, 2
v 5 E fv(k)2g, can be expressed in terms of q as
 
2
v 5 q2
12 
 (8.9.6)
Oversampling
Butterworth lowpass 
filter
Oversampled ADC
ADC quantization level
Quantization noise 
power
Figure 8.37:  
Oversampled ADC 
Using Sampling 
Rate Decimation 
by an Integer 
Factor M 
ADC
HM(z)
Ha(s)
xa
xq
y
M
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

636    Chapter 8  Multirate Signal Processing
In Figure 8.37, the quantized signal, xq(k), is processed by a digital anti-aliasing fil-
ter, HM(z), and then down-sampled. The down-sampler block does not affect the average 
power of xq(k). This is because even though only every Mth sample is extracted, thereby 
lowering the total energy, the resulting signal is also shorter by a factor of M. To examine 
the effects of the filter HM(z) on the quantization noise, recall from (6.9.10) that the aver-
age power of the noise at the filter output is
 
2
y 5 G2
v 
 (8.9.7)
Here G is the power gain of HM(z). If the digital anti-aliasing filter HM(z) is an FIR filter 
of order m, then from (6.9.11) the power gain can be expressed in terms of the impulse 
response hM(k) as
 
G 5 o
m
k50
h2
M(k) 
 (8.9.8)
The expression for the power gain can be simplified further. Using Parseval’s identity in 
Proposition 4.3, the power gain can be recast in terms of HM( f ) 5 DTFThhM(k)j as
G 5 1
fs#
.5fs
2.5fs
uHM( f )u2df
5 1
fs#
.5fsyM
2.5fsyM
df
 
5 1
M 
 (8.9.9)
Combining (8.9.7) and (8.9.9), the average power of the quantization noise appearing at 
the output of the oversampled ADC is
 
2
y 5 2
v
M
 
 (8.9.10)
Consequently, oversampling by a factor of M has the beneficial effect of reducing the 
quantization noise power by a factor of M. This is achieved because oversampling by M 
effectively spreads the noise power out over the frequency range f2fsy2, fsy2g. The digital 
anti-aliasing filter HM(z) with cutoff frequency F0 5 fsy(2M) then removes most of the 
quantization noise.
From (8.9.5), the reduction in the quantization noise power in (8.9.10) can be inter-
preted as an increase in the number of bits of precision. For example, let BM be the num-
ber of bits of precision using oversampling by a factor of M, and let N be the number of 
bits of precision without oversampling. Using (8.9.4), (8.9.6), and (8.9.10) and equating 
the quantization noise power for the two cases yields
 
c2
12M f22(BM21)g 5
c2
12f22(N21)g 
 (8.9.11)
Canceling the common terms, taking reciprocals, and then taking the base-2 logarithm 
of each side, one arrives at the following expression for the corresponding precision when 
oversampling is used.
 
BM 5 N 2 log2(M)y2  bits  
 (8.9.12)
Power gain
Quantization noise
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 8.9 Oversampled A-to-D Converters    637
Note that oversampling by a factor of M 5 4 decreases the required precision of the 
ADC by 1 bit. That is, the signal-to-quantization noise ratio of an N-bit ADC without 
oversampling is the same as the signal-to-quantization noise ratio of an (N 2 1)-bit ADC 
with oversampling by a factor of M 5 4. Both systems have the same quantization error 
and therefore the same precision. For the more general case, oversampling by M 5 4r 
reduces the required ADC precision by r bits.
ADC effective precision
oversampled ADC
EXAMPLE 8.11
To illustrate the effectiveness of oversampling in the analog-to-digital conversion 
process, let the analog anti-aliasing filter be an nth-order Butterworth filter with 
the magnitude response in (8.9.3). The objective of this example is to use over-
sampling to ensure that the maximum aliasing the error is sufficiently small. The 
maximum of the magnitude of the aliasing error occurs at the folding frequency, 
fd 5 fsy2. At this frequency the aliasing error scale factor is
n 5
D  
An( fd)
If oversampling by a factor of M is used, then fs 5 2MFa, which means fd 5 MFa. 
Using (8.9.3), to achieve an aliasing error scale factor of  requires
1
Ï1 1 M2n # 
Taking reciprocals and squaring both sides then yields
1 1 M2n $ 22
Solving for M
M $ (22 2 1).5yn
For example, if a second-order Butterworth filter is used, then n 5 2. Suppose a 
desired value for the aliasing error scale factor is  5 .01. In this case, the required 
sampling rate conversion factor is
 M 5 ceilh(104 2 1)1y4j
 5 ceil(9.9997)
 5 10
The calculation of M can be verified graphically using Figure 8.38, which 
was generated by running exam8_10. The curves display the aliasing error scale 
factor n versus the oversampling factor M for the first four Butterworth filters, 
1 # n # 4. For clarity of display, the ordinate is the aliasing error scale factor 
is in units of dB. It is evident that a very low aliasing error can be achieved by 
using a combination of oversampling and a sufficiently high-order Butterworth 
anti-aliasing filter.
Aliasing error scale 
factor
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

638    Chapter 8  Multirate Signal Processing
8.9.2 A Sigma-delta ADC
The power density spectrum of the quantization noise, v(k), is flat over the frequency 
range 0 # u f u # fsy2 and is equal to 2
v in (8.9.6).
 
Pv( f ) 5 2
v 
 (8.9.13)
When the lowpass filter HM(z) in Figure 8.37 is applied to v(k), it removes the fraction of 
the noise that is outside the passband thereby decreasing the average power of the noise 
by a factor of M. The improvement in the signal-to-quantization noise ratio would be 
even more dramatic if the power density spectrum of the quantization noise were not flat 
but instead had a higher fraction of its power in the stopband of HM(z).
By using a different quantization scheme called sigma-delta modulation, the spectrum 
of the quantization noise can be reshaped such that even more of the power lies outside 
the passband of HM(z) (Candy and Temes, 1992). A block diagram of a sigma-delta 
ADC is shown in Figure 8.39.
To analyze the input-output behavior of the sigma-delta ADC, it is helpful to first 
convert it to a discrete-equivalent form. First consider the integrator block with input 
ea(t) and output ua(t). If a backward Euler approximation with a normalized sampling 
interval of T 5 1 is used to represent integration, then the discrete-equivalent transfer 
function HI(z) 5 U(z)yE(z) of the integrator block is
 
HI(z) 5
1
1 2 z21 
 (8.9.14)
Next the ADC can be modeled as an N-bit quantizer, uq(k) 5 QNfu(k)g. To develop a 
linear model of the quantization process, the quantized ADC output uq(k) is represented 
as the unquantized signal u(k) plus quantization error v(k).
 
uq(k) 5 u(k) 1 v(k) 
 (8.9.15)
Here the quantization error v(k) is white noise uniformly distributed over f2qy2, qy2g with 
variance 2
v 5 q2y12, as in (8.9.6). Sigma-delta ADCs typically use a very high sampling 
Sigma-delta 
modulation
Figure 8.38: Aliasing-error Scale Factor n in dB versus the Oversampling 
Factor M Using Butterworth Anti-aliasing Filters of Order n
0
5
10
15
20
25
30
35
40
2140
2120
2100
280
260
240
220
0
M
20 log10(n) (dB)
 n 5 1
 n 5 2
 n 5 3
 n 5 4
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 8.9 Oversampled A-to-D Converters    639
rate fs and a very low precision N. In fact, they often use the lowest number of bits pos-
sible, N 5 1. In this case, the quantizer uq(k) 5 Q1fu(k)g reduces to a simple comparator. 
Thus a 1-bit ADC has outputs of 6c, depending on whether its input is positive or neg-
ative, respectively. The signal uq(k) can be thought of as a representation of xa(t) using 
pulse count modulation. The 1-bit DAC is modeled as z21, which accounts for a process-
ing delay of one sample. Substituting these models for the subsystems into Figure 8.39, 
one arrives at the discrete-equivalent linear model of the sigma-delta ADC in Figure 8.40.
Figure 8.39: A Sigma-delta ADC Using Oversampling by a Factor of M
1
2
s
1
HM(z)
uq(k)
y(k)
ua(t)
ea(t)
xa(t)
yq(k)
M
ADC
DAC
Figure 8.40: Linear Discrete-equivalent of a 1-bit Sigma-delta ADC 
1
2
1 2 z21
1
z21
HM(z)
uq(k)
y(k)
u(k)
e(k)
x(k)
y(k)
1
M
Sigma-delta Quantization
EXAMPLE 8.12
As an illustration of the operation of the sigma-delta ADC in Figure 8.39, sup-
pose the ADC input range is c 5 10. Consider the following analog input.
xa(t) 5 8 cos(2Fat)
Suppose Fa 5 1y128, and an oversampling factor of M 5 64 is used. This results in 
a sampling rate of fs 5 2MFa 5 1 Hz. Let the total number of samples be p 5 8192, 
which corresponds to M cycles of xa(t). When exam8_11 is run, it produces the plots 
in Figure 8.41. Figure 8.41a shows one cycle of the analog input. In Figure 8.41b, it is 
apparent that the positive pulses are denser for more positive values of ua(t), and the 
negative pulses are denser for more negative values of xa(t). The magnitude spectrum 
of uq(k) is shown in Figure 8.41c. There is a large spike at f = Fa, but there are many 
other discrete spectra at higher frequencies as well. Finally, the output y(k) in Figure 
8.41d is obtained by removing the spectral power of Xq( f ) outside of FM 5 fsy(2M) 
and then reconstructing y(k) using the inverse DFT. It is clear that y(k) is a reason-
able reproduction of xa(kT), even though a very crude 1-bit quantization is used.
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

640    Chapter 8  Multirate Signal Processing
The linear model of a sigma-delta ADC in Figure 8.40 is a two-input system with 
inputs x(k) and v(k). To find the Z-transform of the quantized output uq(k), note from 
Figure 8.40 that
Uq(z) 5 U(z) 1 V(z)
5 HI(z)E(z) 1 V(z)
 
5 HI(z)fX(z) 2 z21Uq(z)g 1 V(z) 
 (8.9.16)
Solving (8.9.16) for Uq(z) and then substituting for HI(z) using (8.9.14) yields
Uq(z) 53
1
1 1 z21HI(z)4 fHI(z)X(z) 1 V(z)g
5 (1 2 z21)fHI(z)X(z) 1 V(z)g
 
 5 X(z) 1 (1 2 z21)V(z) 
 (8.9.17)
The quantization noise appearing at the output will be a lowpass filtered version 
of R(z) 5 (1 2 z21)V(z). It is the factor 1 2 z21 that changes or reshapes the spectral 
characteristics of the noise. In particular, using Euler’s identity, the power density 
spectrum is
Pr( f ) 5 u1 2 exp(2j2f T )u22
v
5 uexp(2jf T )(exp( jf T ) 2 exp(2jf T )u22
v
5 uexp(2jf T )u2u
  j 2 sin(f T )u22
v
 
5 4 sin2(f T )2
v 
 (8.9.18)
Figure 8.41: Operation of the 1-bit Sigma-delta ADC of Example 8.12 
0
20
40
60
80
100
120
140
210
0
10
k
xa(kT)
0
20
40
60
80
100
120
140
210
0
10
k
uq(k)
0
0.1
0.15
0.05
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0
2
4
3104
f/fs
|Uq(f)|
0
20
40
60
80
100
120
140
210
0
10
k
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 8.9 Oversampled A-to-D Converters    641
The lowpass filter has a gain of one and a cutoff frequency of fsy(2M). Since the 
down-sampler does not change the average power, the average power of the quantization 
noise appearing at the output of the sigma-delta ADC is
Py 5 1
fs#
.5fsyM
2.5fsyM
Pr( f )df
 
5 4T2
v#
.5fsyM
2.5fsyM
 sin2(f T )df  
 (8.9.19)
Because a sigma-delta ADC typically uses only a 1-bit quantizer, it compensates for 
this by oversampling by a factor M W 1. For u f u # fsy(2M),  sin (f T ) < f T . Using 
this approximation, the average power of the quantization noise at the output simplifies to
Py < 4T2
v#
.5fsyM
2.5fsyM
(Tf  )2df
5 (42T 32
v)1
f 3
32u
.5fsyM
2.5fsyM
5 (42T 32
v)1
f 3
s
12M32
 
51
2
3M32 2
v 
 (8.9.20)
If a sigma-delta ADC uses a quantizer with B bits, then the quantization level from 
(8.9.4) is q 5 cy2B21. From (8.9.6) and (8.9.20), the average power of the sigma-delta 
ADC quantization noise in terms of the input amplitude c is
 
Py <
2c2
36f22(B21)gM3 
 (8.9.21)
Due to the presence of the M3 factor in the denominator, the quantization noise power 
decreases rapidly with M. As before, the reduction in the quantization noise power in 
(8.9.21) can be interpreted as an increase in the number of bits of precision. Let N 
be the number of bits of precision of a regular ADC without oversampling. Using 
(8.9.4), (8.9.6), and (8.9.21), and equating the quantization noise power for the two 
cases yields
 
2c2
36f22(B21)gM3 <
c2
12f22(N21)g 
 (8.9.22)
Canceling the common terms, taking reciprocals, taking the base-2 logarithm of each 
side, and simplifying the result, one arrives at the following expression for the corre-
sponding precision when a sigma-delta ADC is used.
 
B < N 2  log2(3y2)y2 2 3 log2(M)y2  bits  
 (8.9.23)
From (8.9.23) it is apparent that each increase in M by a factor of four leads to a 
decrease of three bits in the required precision for the ADC. A comparison in the reduc-
tions of the number of bits required to achieve a given quantization noise power Py for 
oversampling with direct quantization and oversampling with sigma-delta quantization 
is shown in Table 8.2.
ADC effective precision
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

642    Chapter 8  Multirate Signal Processing
To interpret Table 8.2, suppose an N 5 12 bit ADC without oversampling is used. 
For oversampling by a factor of M 5 64, an oversampled ADC with equivalent precision 
requires only BM 5 9 bits, and a sigma-delta ADC requires only B 5 3.9 bits. With still 
more oversampling by a factor of M 5 256, the equivalent oversampled ADC requires 
BM 5 8 bits, and the sigma-delta ADC requires only B 5 0.9 bits. Thus a 1-bit sigma- 
delta ADC can be used.
It should be pointed out that oversampling by a large factor M means that the 
anti-aliasing filter HM(z) with cutoff frequency FM 5 fsy(2M) is a narrowband filter 
that can be a challenge to implement. For example, for M 5 64, the cutoff frequency is 
fM 5 .0078fs. Increased savings in the number of bits can be achieved by higher-order 
sigma-delta ADCs, but the higher-order systems tend to be less stable (Oppenheim  
et al., 1999).
oversampled D-to-A Converters
8.10.1 Anti-imaging Filters
Just as oversampling can be used to ease the requirements on the analog anti-aliasing 
prefilter of an ADC, it can also be used to ease the requirements on the analog anti- 
imaging postfilter of a DAC. Recall that a DAC can be modeled as a zero-order hold with 
transfer function
 
H0(s) 5 1 2 exp(2Ts)
2
 
 (8.10.1)
The DAC output is a piecewise-constant signal containing spectral images centered at 
multiples of the sampling frequency. Suppose the range of frequencies of interest for 
the analog output signal ya(t) is 0 # u f u # Fa where fs 5 2Fa. Normally, the piecewise- 
constant DAC output is passed through a sharp analog lowpass filter with a cutoff fre-
quency of fsy2 to remove the spectral images that are centered at multiples of fs. Instead, 
consider an increase in the sampling rate to fs 5 2LFa, where L is an integer greater than 
one, before performing the digital-to-analog conversion. This oversampling by a factor 
of L spreads the spectral images in the DAC output out so that now they are centered 
at multiples of Lfs, thus making them easier to filter out. In particular, the analog anti- 
imaging filter now has to satisfy the following simpler frequency response specification.
 
Ha( f ) 55
1,
0 # u f u # Fa
0,
LFa # u f u , ` 
 (8.10.2)
  8.10
Optional material
Table 8.2: Equivalent 
Precision of 
Oversampled ADCs 
when the Regular 
ADC Is N bits, and the 
Oversampling Factor 
Is M
M 
oversampled ADC, BM
Sigma-delta ADC, B
  2 
N 2 0.5 
N 2 0.6 
  4 
N 2 1.0 
N 2 2.1 
  8 
N 2 1.5 
N 2 3.6 
 16 
N 2 2.0 
N 2 5.1 
 32 
N 2 2.5 
N 2 6.6 
 64 
N 2 3.0 
N 2 8.1 
128 
N 2 3.5 
N 2 9.6 
256 
N 2 4.0 
N 2 11.1
*
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

  8.10 Oversampled D-to-A Converters    643
Even though Ha( f ) is still an ideal filter with no passband ripple and complete stopband 
attenuation, the width of the transition band Df 5 (L 2 1)Fa is no longer zero. Given a 
large transition band, Ha(s) can be approximated with a simple, inexpensive low-order 
filter, such as a first- or second-order analog Butterworth filter.
The two-step process of sampling rate interpolation followed by a DAC is called 
oversampled digital-to-analog conversion. The overall structure of an oversampled DAC 
is shown in the block diagram in Figure 8.42. Note that it has two anti-imaging filters: a 
higher-order digital filter HL(z) with cutoff frequency Fa, and a low-order analog filter 
Ha(s), also with cutoff frequency Fa.
Just as with an ADC, the digital anti-imaging filter, HL(z), effectively reduces the 
quantization noise power because it removes frequencies in the expanded stopband, 
Fa # u f u # Lfsy2. As a result, the average power of the quantization noise at the DAC 
output yq(k) is as in (8.7.10) but with M replaced by L. That is,
 
2
y 5 2
x
L  
 (8.10.3)
As with the ADC, the reduction in the quantization noise power in (8.10.3) can be 
interpreted as an increase in the number of bits of precision. Let BL be the number of 
bits of precision using oversampling by a factor of L, and let N be the number of bits 
of precision without oversampling. Using (8.9.4), (8.9.6), and (8.10.3) and equating the 
quantization noise power for the two cases yields (8.9.11) but with M replaced by L. Solv-
ing the revised version of (8.9.11) for BL we then arrive at the following expression for the 
corresponding DAC precision when oversampling by a factor of L is used.
 
BL 5 N 2  log2(L)y2  bits  
 (8.10.4)
A comparison of the reductions of the number of bits required to achieve a given 
quantization noise power for regular and oversampled DACs is shown in Table 8.3. For 
example, when L 5 16 an oversampled DAC requires two fewer bits than a regular DAC 
to achieve the same level of quantization noise.
Oversampled DAC
Quantization noise
Figure 8.42: Oversampled DAC Using Sampling Rate Interpolation and Passband 
Equalization 
DAC
r
HL(z)
Ha(s)
x
ya
yq
L
L
oversampled DAC, BL
  2
N 2 0.5
  4 
N 2 1.0
  8 
N 2 1.5
 16 
N 2 2.0
 32 
N 2 2.5
 64 
N 2 3.0
128 
N 2 3.5
256 
N 2 4.0
Table 8.3: Equivalent 
Precision of 
Oversampled DACS 
when the Regular 
DAC Is N bits and the 
Oversampling Factor 
Is M
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

644    Chapter 8  Multirate Signal Processing
8.10.2 Passband Equalization
The use of a sampling rate interpolator opens up additional opportunities in terms of 
improving system performance in the passband, 0 # u f u # Fa. From (8.10.1) and Euler’s 
identity, the magnitude response of the zero-order hold model of the DAC using a sam-
pling frequency of Lfs is
A0( f ) 5 uH0(s)us5j2f
 5 u
1 2  exp(2j2f T yL)
j2f
u
 5 u
 exp( jf T yL)fexp( jf T yL) 2 exp(2jf T yL)g
j2f
u
 5 u
 exp(2jf T yL) sin(f T yL)
f
u
 5 u
 sin(f T yL)
f
u
 
 5 T usinc(f T yL)u
L
 
 (8.10.5)
A plot of the magnitude response of the DAC is shown in Figure 8.43. The side lobes are 
what cause the images of the baseband spectrum to appear at the output.
Within the passband, the DAC shapes the magnitude spectrum of the signal using 
the scale factor A0( f ). The effects of the DAC within the passband can be compensated 
for by using a more general version of the digital anti-imaging filter HL(z). In particular, 
one can equalize the effects of the DAC, within the frequency band 0 # u f u # Fa, by 
using a digital anti-imaging filter with the following frequency response.
 
HL( f ) 55
L
T usinc(f T yL)u ,
0 # u f u # Fa
0,
Fa , u f u , fsy2 
 (8.10.6)
DAC magnitude 
response
Figure 8.43:  
Magnitude 
Response of a 
Zero-order Hold 
Model of a DAC
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
1.2
(L/T)A0(f)
f/(Lfs)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

  8.10 Oversampled D-to-A Converters    645
When L W 1, the spectral distortion in the passband due to the DAC is small because 
sinc(f T yL) < 1. However, if a low-order analog anti-imaging filter is used, then there 
can be a fairly significant ripple within the passband. This ripple can be compensated 
for as well with the digital filter HL(z). Suppose an nth-order Butterworth filter is used 
for the analog anti-imaging filter. Then from (8.9.3) and (8.10.6), the effects of both the 
DAC and the analog postfilter can be equalized, within the passband, by using a digital 
anti-imaging filter with the following frequency response. This way, the overall magnitude 
response is flat within the passband.
 
HL( f ) 55
LÏ1 1 ( fyFa)2n
T usinc(f T yL)u , 0 # u f u # Fa
0,
Fa , u f u , fsy2  
 (8.10.7)
DAC magnitude 
equalizer
oversampled DAC
EXAMPLE 8.13
To illustrate the effectiveness of oversampling in the digital-to-analog conversion 
process, let the analog anti-imaging filter be an nth-order Butterworth filter with 
the magnitude response in (8.9.3). The objective of this example is to use over-
sampling to ensure that the magnitude spectra of the images are sufficiently small. 
Since the Butterworth magnitude response decreases monotonically, the spectral 
images are all scaled by a factor of at least An(fd) where fd 5 fsy2 is the folding 
frequency. Thus the aliasing error scale factor for the spectral images is
n 5
D  An( fd)
If oversampling by a factor of L is used, then fs 5 2LFa, which means fd 5 LFa. 
Using (8.9.3) to achieve a spectral image scaling factor of  requires
1
Ï1 1 L2n # 
Taking reciprocals and squaring both sides then yields
1 1 L2n $ 22
Solving for L
L $ (22 2 1).5yn
For example, if a first-order Butterworth filter is used, then n 5 1. Suppose the 
desired spectral image scaling factor value is  5 .05. In this case the sampling 
rate conversion factor required is
 L 5 ceilh(400 2 1)1y2j
 5 ceil(19.975)
 5 20
Aliasing error scale 
factor
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

646    Chapter 8  Multirate Signal Processing
GUi Modules and Case Studies 
This section focuses on the design and realization of multirate systems. A graphical user 
interface module called g_multirate is introduced that allows the user to design rate con-
verters and evaluate multirate systems, without any need for programming. A case study 
programming example using the DSP Companion functions is then presented.
g_multirate: Design and evaluate multirate systems 
The DSP Companion includes a GUI module called g_multirate that allows the user to 
perform rational rate conversion and evaluate multirate systems. GUI module g_multirate 
features a display screen with tiled windows as shown in Figure 8.45. The design features 
of g_multirate are summarized in Table 8.4.
8.11
GUi Module
Figure 8.44: Magnitude Response of Equalized Digital Anti-imaging Filter 
HL(z), Compensating for the Effects of the DAC and the Analog Anti-imaging 
Filter when n 5 1 and L 5 20
0
0.01
0.02
0.03
0.04
0.05
0
0.5
1
1.5
f/fs
(T/L)AL(f)
The magnitude response of the equalized digital anti-imaging filter HL(z) is 
shown in Figure 8.44. It was generated by running exam8_12. For clarity of dis-
play, only the range 0 # u f u # fsyL is shown. Since L 5 20 in this case, almost all 
of the passband compensation is included to cancel the effects of the first-order 
Butterworth filter. The DAC (zero-order hold) magnitude response is essentially 
flat in the passband because L W 1.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.11 GUI Modules and Case Studies     647
Figure 8.45: Display Screen for GUI Module g_multirate
Select Type
Select View
Slider Bar
Edit Parameters
0
500
0
0.2
0.4
0.6
0.8
1
1000
1500
2000
22
0
2
1
0.5
0
Frequency-modulated Input, L/M 5 1.5, Least-squares Filter, m 5 60
x(k)
0
500
1000
1500
2000
22
0
2
k
y(k)
g_multirate
x(k)
L
H0(z)
M
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

648    Chapter 8  Multirate Signal Processing
The upper left-hand Block diagram window contains a block diagram of the multi-
rate system under investigation. It is a single-stage rational sampling rate converter with 
rate a conversion factor LyM described by the following time domain equation.
 
y(k) 5 o
m
i50
bi L(Mk 2 i)x 1
Mk 2 i
L 2 
 (8.11.1)
The Parameters window below the block diagram displays edit boxes containing sim-
ulation parameters. The contents of each edit box can be directly modified by the user 
with the Enter key used to activate the changes. Parameter fs is the sampling frequency, 
L is the interpolation factor, M is the decimation factor, and c is a damping factor used 
with the damped-cosine input. There are also two pushbutton controls that play the input 
signal x(k) and the output signal y(k), respectively, on the PC speaker.
The Type and View windows in the upper-right corner of the screen allow the user 
to select both the type of input and the viewing mode. The input types include uniformly 
distributed white noise, a damped cosine with a frequency of fsy10 and damping factor ck, 
an amplitude-modulated (AM) sine wave, and a frequency-modulated (FM) sine wave. 
The Record sound option prompts the user to record one second of audio data using the 
PC microphone. Finally, the Import option prompts the user for the name of a MAT file 
containing a vector of samples x and the sampling frequency fs.
The View options include the time signals x(k) and y(k) and their magnitude spectra. 
Additional viewing options include the magnitude response, phase response, and impulse 
response of the combined anti-aliasing and anti-imaging filter.
 
H0(z) 5 o
m
i50
biz2i 
 (8.11.2)
The impulse response plot also plots the poles and zeros of the filter. There is also a 
dB check box control that toggles the magnitude spectra and magnitude response plots 
between linear and logarithmic scales. The filter order, m, is directly controllable with 
the horizontal slider bar below the Type and View windows. The Plot window along the 
bottom half of the screen shows the selected view.
The Menu bar at the top of the screen includes several menu options. The Export 
option is used to save a, b, x, y, fs to a MAT file. Results exported from one GUI module 
can be imported into other GUI modules for additional processing. The Caliper option 
allows the user to measure any point on the current plot by moving the mouse crosshairs 
item 
Variables 
Block diagram 
x(k), y(k) 
Edit parameters 
fs, L, M, c 
Input type 
white noise, damped cosine, AM input, 
FM input, record x, import 
Plot view 
magnitude response, phase response, 
pole-zero plot, impulse response 
Slider 
filter order m 
Push buttons 
play x, play y 
Check boxes 
linear/dB scale 
Menu buttons 
filter, export, caliper, print, help, exit 
Import 
x, fs
Export 
a, b, x, y, fs
Table 8.4:  
Features of GUI  
Module g_multirate 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.11 GUI Modules and Case Studies     649
to that point and clicking. The Filter option allows the user to select the filter type for 
the combined anti-aliasing and anti-imaging filter, H0(z). The choices include windowed 
(rectangular, Hanning, Hamming, or Blackman), frequency-sampled, least-squares, and 
equiripple filters. The Print option sends the GUI window to a printer or a file. Finally, 
the Help option provides the user with some helpful suggestions on how to effectively use 
module g_multirate.
Sampling Rate Converter (CD to DAT)
A multistage sampling rate converter is a good vehicle for illustrating the topics covered 
in this chapter. Consider the problem of converting music from compact disc (CD) for-
mat to digital audio tape (DAT) format. A CD is sampled at a rate of 44.1 kHz, while 
a DAT is sampled at a somewhat higher rate of 48 kHz. Thus the required frequency 
conversion ratio to go from CD format to DAT format is
L
M 5 48
44.1
5 480
441
 
5 160
147 
 (8.11.3)
First suppose a single-stage sampling rate converter is to be used. From (8.3.2), the com-
bined anti-aliasing and anti-imaging filter H0( f ) must have a cutoff frequency of
F0 5  min 5
44100
2(147), 44100
2(160)6
 
5 138.9 Hz 
 (8.11.4)
The required passband gain is H0(0) 5 160. Hence from (8.3.3) the desired frequency 
response for the ideal lowpass filter is
 
H0( f ) 55
160,
0 # u f u , 138.9
0,
138.9 # u f u , 22500 
 (8.11.5)
Filter H0(z) is a narrowband filter with normalized cutoff frequency of F0yfs 5 .003025. 
Since a direct single-stage implementation would require a very high-order linear-phase 
FIR filter, instead consider a multi-stage implementation. Example 8.4 examined the case 
of converting from DAT to CD. Using the reciprocals of those conversion factors, this 
results in the following three-stage implementation.
 
160
147 51
8
72 1
5
72 1
4
32 
 (8.11.6)
Thus, a CD-to-DAT converter can be implemented using two rational interpolators and 
one rational decimator. A block diagram of the three-stage sampling rate converter is 
Case Study 8.1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

650    Chapter 8  Multirate Signal Processing
shown in Figure 8.46. Here r1(k) and r2(k) are intermediate output signals from stages one 
and two, respectively. The sampling rates of r1(k), r2(k), and y(k) are
 
f1 51
8
72 44.1 5 50.4 kHz 
 (8.11.7a)
 
f2 51
5
72 50.4 5 36.0 kHz 
 (8.11.7b)
 
f3 51
4
32 36.0 5 48.0 kHz 
 (8.11.7c)
Using (8.3.2) and (8.11.6), the three cutoff frequencies of the combined anti-aliasing 
and anti-imaging filters are as follows.
 
F1 5  min 5
44100
2(8) , 44100
2(7) 6 5 2756.3 Hz 
 (8.11.8a)
 
F2 5  min 5
50400
2(5) , 50400
2(7) 6 5 3600 Hz 
 (8.11.8b)
 
F3 5  min 5
436000
2(4) , 36000
2(3) 6 5 4500 Hz 
 (8.11.8c)
It then follows from (8.3.3) that the ideal frequency responses of the three-stage  
filters are
 
H1( f ) 55
8,
0 # u f u , 2756.3
0,
2756.3 # u f u , 22500 
 (8.11.9a)
 
H2( f ) 55
5, 0 # u f u , 3600
0, 3600 # u f u , 25200 
 (8.11.9b)
 
H3( f ) 55
4,
0 # u f u , 4500
0,
4500 # u f u , 18000 
 (8.11.9c)
Suppose the stage filters are each implemented using a windowed linear-phase FIR 
filter of order m 5 60 with the Hamming window. Plots of the magnitude responses of 
the three filters are shown in Figure 8.47.
Recall that the DSP Companion contains a function called f_rateconv for performing 
a single-stage rational sampling rate conversion by LyM. The three-stage conversion is 
implemented with multiple calls to f_rateconv by running the case8_1 from g_dsp.
When case8_1 is run, it first computes the magnitude responses of the three-stage  
filters, as shown in Figure 8.47. It then prompts the user to speak into the microphone 
and records the response at the CD rate. The recorded speech is then played back at 
both the CD rate and the higher DAT rate for comparison by the ear. Next, the recorded 
speech is displayed in a plot, and crosshairs appear. The user should use the mouse to 
select the start of a speech segment. The selected segment is then converted from the 
Figure 8.46: A Three-stage CD-to-DAT Sampling Rate Converter
H1
H2
r2
r1
H3
7
3
7
8
4
5
x
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.11 GUI Modules and Case Studies     651
CD rate to the DAT rate in three stages, with the results shown in Figure 8.48. Note that 
although all the segments appear to be the same shape, a close inspection of the scales 
along the horizontal axes shows that each is at a different sampling rate.
Figure 8.47:  
Magnitude 
Responses of the 
Three-stage Filters 
0
0.5
1
1.5
2
2.5
3
3104
0
1
2
3
4
5
6
7
8
9
f (Hz)
A(f)
A1
A2
A3
Figure 8.48:  
Rate-converted 
Segments of 
Recorded Speech
0
50
100
150
200
250
300
350
400
20.5
0
0.5
 x(k)
0
100
150
50
200
250
300
350
400
450
500
20.5
0
0.5
r1(k)
r2(k)
0
50
100
150
200
250
300
350
20.5
0
0.5
0
50
100
150
200
250
300
350
400
450
20.5
0
0.5
k
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

652    Chapter 8  Multirate Signal Processing
Num. 
Learning outcome 
Sec. 
 1 
Know how to design and implement integer decimators and interpolators
8.2 
 2 
Know how to design and implement rational sampling rate converters, both  
single-stage and multistage
8.3 
 3 
Be able to efficiently implement sampling rate converters using polyphase filters
8.4 
 4 
Know how to apply multirate techniques to design intersample delay systems
8.5 
 5 
Be able to use multirate techniques to design narrowband filters
8.5 
 6 
Know how to design analysis and synthesis DFT filter banks 
8.6 
 7 
Understand how to design lowpass prototype filters for perfect reconstruction 
filter banks
8.7 
 8 
Know how to design channel equalizers for transmultiplexor systems
8.8 
 9 
Understand how to design an oversampled ADC and know what benefits are 
achieved
8.9 
10 
Understand how to design an oversampled DAC and know what benefits are 
achieved
8.10 
11 
Know how to use the GUI module g_multirate to design and evaluate multirate 
DSP systems
8.11 
Table 8.5:  
Learning Outcomes  
for Chapter 8 
Chapter Summary 
Learning outcomes
This chapter was designed to provide the student with an opportunity to achieve the 
learning outcomes summarized in Table 8.5.
Rate Converters
Chapter 8 focused on multirate signal processing techniques. Every multirate system con-
tains at least one rate converter. A sampling rate converter is a linear time-varying system 
that changes the sampling rate of a discrete-time signal without converting the signal 
back to analog form. Instead, the resampling is done in the digital domain. The simplest 
sampling rate converter decreases the sampling frequency by an integer factor M. This is 
called a decimator, and it can be implemented with the following time-domain equation.
 
y(k) 5 o
m
i50
bix(Mk 2 i) 
 (8.12.1)
The FIR filter with impulse response hM(k) 5 bk for 0 # k # m is a lowpass filter with a 
passband gain of one and a cutoff frequency of FM 5 fsy(2M). This filter is included to 
preserve the spectral characteristics of x(k). Decreasing the sampling rate by a factor of 
M is called down-sampling, and it is represented in block diagrams with the symbol TM. 
The effect of down-sampling is to spread the spectrum out by a factor of M. Hence to 
avoid aliasing, HM(z) 5 ZhhM(k)j is inserted as a digital anti-aliasing filter.
It is also possible to increase the sampling rate by an integer factor L. This is called 
an interpolator, and it can be implemented with the following time-domain equation.
 
y(k) 5 o
m
i50
biL(k 2 i)x 1
k 2 i
L 2 
 (8.12.2)
8.12
Decimator
Interpolator
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.12 Chapter Summary     653
Here L(k) is a periodic train of impulses of period L starting at k 5 0. The FIR fil-
ter with impulse response hL(k) 5 bk for 0 # k # m is a lowpass filter with a passband 
gain of L and a cutoff frequency of FL 5 fsy(2L). Again, this filter is included to pre-
serve the spectral characteristics of x(k). Increasing the sampling rate by a factor of L 
is called up-sampling, and it is represented in block diagrams with the symbol cL. The 
effect of up-sampling is to compress the spectrum by a factor of L. Because the spec-
trum is periodic, this generates images of the original spectrum that must be removed by 
HL(z) 5 ZhhL(k)j, which is a digital anti-imaging filter.
More general rate conversion by a rational factor LyM can be achieved by using a 
cascade configuration of an interpolator with rate conversion factor L, followed by a 
decimator with rate conversion factor M. This is called a rational rate converter, and it 
can be implemented using the following time-domain equation.
 
y(k) 5 o
m
i50
biL(Mk 2 i)x 1
Mk 2 i
L 2 
 (8.12.3)
Because the interpolator is followed by the decimator, the anti-imaging post-filter of the 
interpolator can be combined with the anti-aliasing pre-filter of the decimator. The cut-
off frequency of the cascade combination of the two filters is F0 5  min hFL, FMj and 
the passband gain is L. Thus, the desired frequency response of the ideal rational rate 
converter filter is
 
H0( f ) 55
L,
0 # u f u # F0
0,
F0 , u f u , fsy2 
 (8.12.4)
Typically, anti-aliasing and anti-imaging filters are implemented with FIR filters because 
linear-phase characteristics can be easily implemented that delay the signal, but do not 
otherwise distort it. IIR filters, by contrast, can achieve sharper transition bands than FIR 
filters of the same order, but they do so at the expense of introducing nonlinear phase 
distortion.
To simplify the filter design, rational sampling rate converters with large values for 
L or M are implemented as a cascade configuration of lower-order rate converters. This 
is called a multistage rate converter, and it is based on the following factorization of the 
rate conversion factor.
 
L
M 51
L1
M12 1
L2
M22 Á 1
Lp
Mp2 
 (8.12.5)
Both interpolators and decimators can be implemented efficiently using polyphase 
filters. If the rate conversion factor is N, then the ith phase of the input signal includes 
every Nth sample of x(k) starting with sample i. A parallel combination of N polyphase 
filters is used, one for each phase. The output samples are obtained by summing the out-
puts of the parallel branches. Polyphase rate converter realizations reduce the number of 
floating-point multiplications or FLOPs per output sample by a factor of N.
Narrowband Filters
There are many applications of multirate systems in modern DSP systems. For example, 
if it is necessary to delay a discrete-time signal by a fraction of a sample, the signal can be 
up-sampled by M, delayed by 0 , L , M using a shift register, and then down-sampled 
Rational rate converter
Rational rate converter 
filter
Multistage rate 
converter
Polyphase filters
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

654    Chapter 8  Multirate Signal Processing
by M to restore the sampling rate. Using linear-phase FIR filters of order m, this achieves 
an intersample delay of
 
 51m 1 L
M2T 
 (8.12.6)
Another example is the design of a narrowband filter, a filter whose passband is very 
small in comparison with fs. Here the signal is down-sampled to spread out the spectrum, 
filtered with a regular filter with cutoff frequency Fc < fsy4, and then up-sampled to 
restore the original sampling rate.
Filter Banks and Transmultiplexors
An allpass filter that covers the entire spectrum f0, fsg can be decomposed into a par-
allel connection of subband filters called a filter bank. There a two basic types of filter 
banks, analysis banks and synthesis banks. An analysis bank decomposes a signal x(k) 
into subsignals, xi(k), occupying subbands of f0, fsg. The subsignals are down-sampled 
and processed separately. Examples of subband processing include spectrum analyzers 
and data compression algorithms, such as MP3 encoding of music files. A synthesis bank 
takes the processed subband signals, yi(k), up-samples them, and combines them into a 
single broadband signal, y(k). A typical single-input single-output filter bank includes an 
analysis bank followed by a synthesis bank. When the output of the synthesis bank exactly 
matches the input to the analysis bank, this is called a perfect reconstruction filter bank. 
The N subfilters can be implemented efficiently using noncausal and causal polyphase 
filters, E2k(z) and Fk(z). Perfect reconstruction is achieved if the lowpass prototype filters 
G(z) and H(z) are used to construct the polyphase filters are designed such that
 
NE2k(z)Fk(z) 5 1, 0 # k , N 
 (8.12.7)
When a synthesis bank is followed by an analysis banks, this multi-input multi- 
output system is referred to as a transmultiplexor system. Transmultiplexors are used in 
communications systems where several lower-frequency subsignals are simultaneously 
transmitted over a single high-bandwidth channel, a process known as frequency-division 
multiplexing. When several time signals are interlaced into a single composite time signal, 
this is called time-division multiplexing.
*oversampled ADCs and DACs
Another class of applications of multirate systems arises in the implementation of 
high-performance analog-to-digital and digital-to-analog converters. An oversampled 
ADC uses oversampling by a factor of M followed by a sampling rate decimator. This 
technique reduces the requirements on the analog anti-aliasing filter by inserting a tran-
sition band of width B 5 (M 2 1)Fa where Fa is the bandwidth of the analog signal. This 
means that a high-order analog anti-aliasing filter with a sharp cutoff can be replaced 
with a less-expensive low-order analog anti-aliasing filter. The oversampled ADC has the 
added benefit that the average power of the quantization noise appearing at the output 
is reduced by a factor of M. The quantization noise level can be reduced still further by 
modifying the spectrum of the quantization error using the sigma-delta quantization 
method. Sigma-delta ADCs based on 1-bit quantization have a quantization error, and 
therefore an accuracy, that is superior to regular ADCs, as long as the oversampling fac-
tor M is made sufficiently large.
Intersample delay
Narrowband filter
Filter bank
Perfect reconstruction 
filter bank
Transmultiplexor
Optional material
Oversampled ADC
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.13 Problems     655
Oversampling also can be used to implement a high-performance DAC. An oversampled 
DAC consists of a sampling rate interpolator with a conversion factor of L followed by a 
DAC. The effect of the up-sampling is to reduce the requirements on the analog anti-imag-
ing filter by inserting a transition band of width B 5 (L 2 1)Fa where Fa is the bandwidth 
of the signal. Again this means that a high-order analog anti-imaging filter with a sharp 
cutoff can be replaced with a less expensive, low-order analog anti-imaging filter. Like the 
oversampled ADC, the oversampled DAC has the added benefit that the average power of 
the quantization noise appearing at the output is reduced by a factor of L.
For an oversampled DAC, the effects of both the zero-order hold and the analog 
anti-imaging filter Ha(s) can be equalized, within the passband, by using a more gen-
eral digital anti-imaging filter for the interpolator. For example, suppose the analog anti- 
imaging filter is an nth-order Butterworth filter. Then the overall magnitude response 
of the oversampled DAC will be flat within the passband if the following digital anti- 
imaging filter is used.
 
HL( f ) 55
LÏ1 1 (fyFa)2n
T usinc(f T yL)u ,
0 # u f u # Fa
0,
Fa , u f u , fsy2
 
 (8.12.8)
GUi Module
The DSP Companion includes a GUI module called g_multirate that allows the user to 
design and evaluate rational sampling rate converters without any need for programming.
Problems 
The problems are divided into Analysis and Design problems that can be solved by hand 
or with a calculator, GUI Simulation problems that are solved using GUI module g_mul-
tirate, and MATLAB Computation problems that require a user program. Solutions to 
selected problems can be accessed with the DSP Companion driver program, g_dsp. Stu-
dents are encouraged to use these problems, which are identified with a 
, as a check on 
their understanding of the material.
8.13.1 Analysis and Design
Section 8.1: Motivation
8.1 
The multirate narrowband filter that was designed in Figure 8.19 has a passband 
cutoff of Fp 5 0.03fs. The stopband for the multirate case appears to have a cutoff 
of approximately Fs 5 0.035fs. Suppose the phase response does not have to be lin-
ear, so an IIR Butterworth filter from Chapter 7 can be used. Recall that IIR filters 
can become numerically unstable when the order or number of poles n is made too 
large.
(a) Assuming p 5 .01 and s 5 .01, use (7.3.4) to find the selectivity factor r and 
the discrimination factor d of this narrowband filter.
(b) Using (7.4.8), find the order n needed for a Butterworth filter.
Oversampled DAC
Passband equalization
8.13
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

656    Chapter 8  Multirate Signal Processing
8.2 
Suppose a signal is sampled at a rate of fs 5 10 Hz. Consider the problem of using 
the variable delay system in Figure 8.49 to implement an overall delay of  5 4.38 sec.
(a) Find the smallest interpolation factor M that is needed.
(b) Suppose the linear-phase FIR filters are of order m 5 40. How much delay is 
introduced by the two lowpass filters?
(c) What length of shift register, L, is needed to achieve the overall delay?
Section 8.2: integer Sampling Rate Converters
8.3 
Consider the problem of designing a sampling rate decimator with a down- 
sampling factor of M 5 8.
(a) Sketch a block diagram of the sampling rate decimator.
(b) Find the required frequency response of the ideal anti-aliasing digital filter, 
assuming fs is the sampling rate of x(k).
(c) Using Tables 6.1 and 6.2, design an anti-aliasing filter of order m 5 40 using 
the windowing method with a Hanning window.
(d) Find the difference equation for the sampling rate decimator.
8.4 
Consider the problem of designing a sampling rate interpolator with an up- 
sampling factor of L 5 10.
(a) Sketch a block diagram of the sampling rate interpolator.
(b) Find the required frequency response of the ideal anti-imaging digital filter 
assuming fs is the sampling rate of x(k).
(c) Using Tables 6.1 and 6.2, design an anti-imaging filter of order m 5 30 using 
the windowing method with a Hamming window.
(d) Find the difference equation for the sampling rate interpolator.
8.5 
Consider the sampling rate interpolator shown previously in Figure 8.6. The input 
x(k) is sampled at rate fs and has a triangular magnitude spectrum Ax( f ), as shown 
in Figure 8.50. Suppose the up-sampling factor is L 5 3.
(a) Sketch the spectrum of the zero-interpolated signal xL(k) defined in (8.2.5) for 
0 # u f u # fsy2.
(b) Sketch the magnitude response of the ideal anti-imaging filter HL(z).
(c) Sketch the magnitude spectrum of y(k) for 0 # u f u # fsy2.
Section 8.3: Rational Sampling Rate Converters
8.6 
Consider the problem of designing a rational sampling rate converter with a fre-
quency conversion factor of LyM 5 2y3.
(a) Sketch a block diagram of the sampling rate converter.
(b) Find the required frequency response of the ideal anti-aliasing and anti- 
imaging digital filter, assuming fs is the sampling rate of x(k).
Figure 8.49: An Intersample Delay System
HM(z)
HM(z)
Shift L
x(k)
y(k)
M
M
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.13 Problems     657
(c) Use Tables 6.1 and 6.2 to design an anti-aliasing and anti-imaging filter of 
order m 5 50 using the windowing method with the Blackman window.
(d) Find the difference equation for the sampling rate converter.
8.7 
Consider the problem of designing a rational sampling rate converter with a fre-
quency conversion factor of LyM 5 5y4.
(a) Sketch a block diagram of the sampling rate converter.
(b) Find the required frequency response of the ideal anti-aliasing and anti- 
imaging digital filter assuming fs is the sampling rate of x(k).
(c) Use Tables 6.1 and 6.2 to design an anti-aliasing and anti-imaging filter of 
order m 5 50 using the windowing method with the Blackman window.
(d) Find the difference equation for the sampling rate converter.
8.8 
Suppose a multirate signal processing application requires a sampling rate conver-
sion factor of LyM 5 .525.
(a) Find the required frequency response of the ideal anti-aliasing and anti- 
imaging digital filter assuming a single-stage converter is used.
(b) Factor LyM into a product of two rational numbers whose numerators and 
denominators are less than 10.
(c) Sketch a block diagram of a multi-stage sampling rate converter based on your 
factoring of LyM from part (b).
(d) Find the required frequency responses of the ideal combined anti-aliasing and 
anti-imaging digital filters for each of the stages in part (c).
8.9 
Suppose a multirate signal processing application requires a sampling rate conver-
sion factor of LyM 5 3.15.
(a) Find the required frequency response of the ideal anti-aliasing and anti- 
imaging digital filter assuming a single-stage converter is used.
(b) Factor LyM into a product of two rational numbers whose numerators and 
denominators are less than 10.
Figure 8.50:  
Magnitude 
Spectrum of x(k) 
in Problem 8.5 
20.5
0
0.5
0
0.5
1
1.5
f/fs
Ax(f)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

658    Chapter 8  Multirate Signal Processing
(c) Sketch a block diagram of a multi-stage sampling rate converter based on your 
factoring of LyM from part (b).
(d) Find the required frequency responses of the ideal combined anti-aliasing and 
anti-imaging digital filters for each of the stages in part (c).
Section 8.4: Polyphase Filters
8.10 Consider an integer decimator with down-sampling factor M and a linear-phase 
FIR anti-aliasing filter of order m.
(a) Find nM, the number of floating-point multiplications (FLOPs) needed to 
compute each sample of the output using a direct realization.
(b) Suppose a polyphase filter realization is used to implement the decimator. Find 
NM, the number of FLOPs needed to compute each sample of the output.
(c) Express NM as a percentage of nM.
8.11 Consider the problem of designing a decimator with fs 5 60 Hz and a down-sampling 
factor of M 5 3.
(a) What is the sampling rate of the output signal?
(b) Sketch the desired magnitude response of the ideal anti-aliasing filter 
HM(z).
(c) Suppose the anti-aliasing filter is a windowed filter of order m 5 32 using the 
Hamming window. Use Tables 6.1 and 6.2 to find the impulse response, hM(k).
(d) Suppose a polyphase realization is used. Find the transfer functions Ei(z) of 
the polyphase filters.
(e) Sketch a block diagram of a polyphase filter realization of the decimator.
8.12 Consider an integer interpolator with an up-sampling factor of L and a linear-phase 
FIR anti-imaging filter of order m.
(a) Find nL, the number of floating-point multiplications (FLOPs) needed to 
compute each sample of the output using a direct realization.
(b) Suppose a polyphase filter realization is used to implement the interpolator. 
Find NL, the number of FLOPs needed to compute each sample of the output.
(c) Express NL as a percentage of nL.
8.13 Consider the problem of designing an interpolator with fs 5 12 Hz and an up- 
sampling factor L 5 4.
(a) What is the sampling rate of the output signal?
(b) Sketch the desired magnitude response of the ideal anti-imaging filter HL(z).
(c) Suppose the anti-imaging filter is a windowed filter of order m 5 20 using the 
Hanning window. Use Tables 6.1 and 6.2 to find the impulse response, hL(k).
(d) Suppose a polyphase realization is used. Find the transfer functions Fi(z) of 
the polyphase filters.
(e) Sketch a block diagram of a polyphase filter realization of the interpolator.
8.14 Consider a polyphase filter realization of a rational rate converter with rate conver-
sion factor LyM 5 2y3.
(a) Suppose the following FIR filter is used for the anti-aliasing filter of the deci-
mator part. Find the filters Ei(z) for a polyphase realization of HM(z).
HM(z) 5 o
30
i50
biz2i
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.13 Problems     659
(b) Suppose the following FIR filter is used for the anti-imaging filter of the inter-
polator part. Find the filters Fi(z) for a polyphase realization of HL(z).
HL(z) 5 o
30
i50
ciz2i
(c) Sketch a block diagram of a polyphase realization of the rational rate conver-
ter using a cascade configuration of an interpolator followed by a decimator.
8.15 Consider a polyphase filter realization of a rational rate converter with rate conver-
sion factor LyM 5 4y3.
(a) Suppose the following FIR filter is used for the anti-aliasing filter of the deci-
mator part. Find the filters Ei(z) for a polyphase realization of HM(z).
HM(z) 5 o
40
i50
biz2i
(b) Suppose the following FIR filter is used for the anti-imaging filter of the inter-
polator part. Find the filters Fi(z) for a polyphase realization of HL(z).
HL(z) 5 o
40
i50
ciz2i
(c) Sketch a block diagram of a polyphase realization of the rational rate conver-
ter using a cascade configuration of an interpolator followed by a decimator.
8.16 Consider the following FIR filter.
H(z) 5 1 1 3z21 1 5z22 1 Á 1 23z211
(a) Find polyphase filters Ei(z) such that
H(z) 5 o
1
i50
z2iEi(z2)
(b) Find polyphase filters Ei(z) such that
H(z) 5 o
5
i50
z2iEi(z6)
(c) Which of the two polyphase realizations of H(z) is faster in terms of the num-
ber of floating-point multiplications per output sample? How many times 
faster is it than a direct implementation of H(z)?
8.17 Consider the following FIR filter.
H(z) 5 2 1 4z21 1 6z22 1 Á 1 24211
(a) Find polyphase filters Ei(z) such that
H(z) 5 o
3
i50
z2iEi(z4)
(b) Find polyphase filters Ei(z) such that
H(z) 5 o
2
i50
z2iEi(z3)
(c) Which of the two polyphase realizations of H(z) is faster in terms of the num-
ber of floating-point multiplications per output sample? How many times 
faster is it than a direct implementation of H(z)?
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

660    Chapter 8  Multirate Signal Processing
Section 8.5: Narrowband Filters 
8.18 Consider the problem of designing a multirate narrowband lowpass FIR filter as 
shown in Figure 8.51. Suppose the sampling frequency is fs 5 8000 Hz and the 
cutoff frequency is F0 5 200 Hz.
(a) Find the largest integer frequency conversion factor M that can be used.
(b) Using (6.3.6), design an anti-aliasing filter HM(z) of order m 5 32 using the 
frequency-sampled method. Do not use any transition band samples.
Section 8.6: Filter Banks 
8.19 Consider the problem of designing a complex passband filter with the following 
ideal magnitude response.
A( f ) 55
0,
0 # f , F0
1,
F0 # f # F1
0,
F1 , f , fs
(a) Let B 5 F1 2 F0 be the width of the passband, and consider the problem of 
designing a lowpass filter G(z) with cutoff frequency Fc 5 By2. Using Tables 
6.1 and 6.2, find the impulse response g(k) for a filter of order m 5 60 using 
the windowing method with the Hamming window.
(b) Using the frequency shift property of the DTFT in (8.6.4) and g(k), find the 
impulse response h(k) of the complex passband filter with cutoff frequencies 
F0 and F1.
(c) Is the magnitude response of H(z) an even function of f ? Why or why not?
(d) Is the magnitude response of H(z) a periodic function of f ? If so, what is the 
period?
8.20 Consider the problem of designing a complex highpass filter with the following 
ideal magnitude response.
A( f ) 55
0, 0 # f , F0
1,
F0 # f , fs
(a) Let B 5 fs 2 F0 be the width of the passband, and consider the problem 
of designing a lowpass filter G(z) with cutoff frequency Fc 5 By2. Using  
Tables 6.1 and 6.2, find the impulse response g(k) for a filter of order m 5 50 
using the windowing method with the Blackman window.
(b) Using the frequency shift property in (8.6.4) and g(k), find the impulse response 
h(k) of the complex highpass filter with a cutoff frequency of F0.
(c) Is the magnitude response of H(z) an even function of f ? Why or why not?
(d) Is the magnitude response of H(z) a periodic function of f ? If so, what is the 
period?
Figure 8.51: A Multirate Narrowband FIR Filter
HM(z)
HM(z)
G(z)
x
y
M
M
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.13 Problems     661
8.21 Consider the problem of designing a complex two-band filter with the magnitude 
response shown in Figure 8.52.
(a) Let B 5 .1fs be the width of each passband, and consider the problem of 
designing a lowpass filter G(z) with cutoff frequency Fc 5 By2. Using Tables 
6.1 and 6.2, find the filter impulse response g(k) for a filter of order m 5 80 
using the windowing method with the Hanning window.
(b) Using the frequency shift property in (8.6.4) and g(k), find the impulse response 
h1(k) of the complex passband filter with cutoff frequencies .1fs and .2fs.
(c) Using the frequency shift property in (8.6.4) and g(k), find the impulse response 
h2(k) of the complex passband filter with cutoff frequencies .3fs and .4fs.
(d) Using h1(k) and h2(k), find the impulse response h(k) of a filter whose magni-
tude response approximates A( f ) in Figure 8.52.
(e) Sketch a block diagram of H(z) using blocks H1(z) and H2(z).
Section 8.7: Perfect Reconstruction Filter Banks 
8.22 Consider the problem of designing prototype filters for a perfect reconstruction 
filter bank with N 5 9 subfilters.
(a) Let h(k) be the impulse response of the lowpass prototype filter for the synthe-
sis bank. Design a windowed lowpass linear-phase FIR filter of order m 5 8 
with a cutoff frequency of F0 5 fsy(2N) and a Hamming window. Use a type 1 
linear-phase filter. Find h(k) for 0 # k # 8.
(b) Using (8.6.12) and (8.6.13), find simplified expressions for the polyphase filters 
E2k(z) and Fk(z) in terms of g and h.
(c) Use Proposition 8.1 to find a noncausal filter g for the analysis bank that will 
produce perfect reconstruction. Find g( 2 k) for 0 # k # 8.
Figure 8.52:  
Two-band 
Magnitude 
Response of 
Problem 8.21
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
f/fs
A(f)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

662    Chapter 8  Multirate Signal Processing
Section 8.8: Transmultiplexors 
8.23 Consider a transmultiplexor system as shown in Figure 8.30. Suppose the commu-
nication channel has the following transfer function.
C(z) 5
0.4(z2 1 .64)(z2 2 .81)
z12(z2 1 .3z 2 .4)(z2 2 .25)
(a) Find the poles and zeros of C(z).
(b) Is C(z) stable?
(c) Is C(z) a minimum-phase system?
(d) Find a proper equalizer system D(z) for this channel.
(e) Find the transfer function of the equalized channel.
Section 8.9: oversampled A-to-D Converters
8.24 Consider the two-band filter whose magnitude response was shown previously in 
Figure 8.52. Find the power gain G of this filter.
8.25 Consider an ideal lowpass filter with a passband gain of A $ 1 and a cutoff fre-
quency of Fc , fsy2. For what value of Fc is the power gain equal to one?
8.26 Consider the 10-bit oversampled ADC shown in Figure 8.53 with analog inputs in 
the range uxa(t)u # 5.
(a) Find the average power of the quantization noise of the quantized input, xq(k).
(b) Suppose a second-order Butterworth filter is used for the analog anti-aliasing 
prefilter. The objective is to reduce the aliasing error by a factor of  5 .005. 
Find the minimum required oversampling factor M.
(c) Find the average power of the quantization noise at the output, y(k), of the 
oversampling ADC.
(d) Suppose fs 5 1000 Hz. Sketch the ideal magnitude response of the digital 
anti-aliasing filter HM( f ).
(e) Using Tables 6.1 and 6.2, design a linear-phase FIR filter of order m 5 80 
whose frequency response approximates HM( f ) using the windowing method 
with a Hanning window.
8.27 A 12-bit oversampled ADC oversamples by a factor of M 5 64. To achieve the 
same average power of the quantization noise at the output, but without using 
oversampling, how many bits are required?
8.28 Suppose an analog signal in the range uxa(t)u # 5 is sampled with a 10-bit over-
sampled ADC with an oversampling factor of M 5 16. The output of the ADC is 
passed through an FIR filter H(z), as shown in Figure 8.54 where
H(z) 5 1 2 2z21 1 3z22 2 2z23 1 z24
(a) Find the quantization level q.
(b) Find the power gain of the filter H(z).
Figure 8.53:  
An Oversampled 
ADC with an 
Oversampling 
Factor M 
ADC
HM(z)
Ha(s)
xa
xq
y
M
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.13 Problems     663
(c) Find the average power of the quantization noise at the system output, y(k).
(d) To get the same quantization noise power, but without using oversampling, 
how many bits are required?
Section 8.10: oversampled D-to-A Converters
8.29 Consider the 10-bit oversampling DAC shown in Figure 8.55 with analog outputs 
in the range uya(t)u # 10.
(a) Suppose a first-order Butterworth filter is used for the analog anti-imaging 
postfilter. The objective is to reduce the imaging error by a factor of  5 .05. 
Find the minimum required oversampling factor L.
(b) Find the average power of the quantization noise at the output of the DAC.
(c) Suppose fs 5 2000 Hz. Find the ideal frequency response of the digital 
anti-imaging filter HL( f ). Include passband equalizer compensation for both 
the analog anti-imaging filter and the zero-order hold.
8.13.2 GUi Simulation
Section 8.2: integer Sampling Rate Converters
8.30 Using the GUI module g_multirate, select the amplitude-modulated (AM) input. 
Reduce the sampling rate of the input using an integer decimator with a down- 
sampling factor of M 5 2. Use a windowed filter with the Hanning window, and 
plot the following.
(a) The time signals
(b) Their magnitude spectra
(c) The filter magnitude response
(d) The filter impulse response
8.31 Using the GUI module g_multirate, select the frequency-modulated (FM) input. 
Increase the sampling frequency of the input using an interpolator with an up- 
sampling factor of L 5 3. Use a windowed filter with the Hamming window, and 
plot the following.
(a) The time signals
(b) Their magnitude spectra
(c) The filter magnitude response
(d) The filter phase response
Figure 8.54:  
A Discrete-time 
Multirate System 
xa(t)
xq(k)
H(z)
Oversampled
ADC
y(k)
Figure 8.55: An 
Oversampling 
DAC with an 
Oversampling 
Factor L
DAC
HL(z)
Ha(s)
x
ya
L
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

664    Chapter 8  Multirate Signal Processing
8.32 Using the GUI module g_multirate, print the magnitude responses of the following 
anti-aliasing and anti-imaging filters using the linear scale.
(a) Windowed filter with the Blackman window
(b) Frequency-sampled filter
(c) Least-squares filter
8.33 Using the GUI module g_multirate, adjust the filter order to m 5 80. Print the magni-
tude responses of the following anti-aliasing and anti-imaging filters using the dB scale.
(a) Windowed filter with the Hanning window
(b) Windowed filter with the Hamming window
(c) Equiripple filter
Section 8.3: Rational Sampling Rate Converters
8.34 Using the GUI module g_multirate, select the damped cosine input. Set the damp-
ing factor to c 5 .995, the up-sampling factor to L 5 2, and the down-sampling 
factor to M 5 3. Plot the following.
(a) The time signals
(b) The magnitude spectra
8.35 Using the GUI module g_multirate, record the word hello in x. Play it back to make 
sure it is a good recording. Save the recording in a MAT-file named prob8_35.mat 
using the Export option. Then reload it using the Import option. Play it back with 
and without rate conversion to hear the difference. Plot the following.
(a) The time signals
(b) Their magnitude spectra
(c) The filter impulse response
8.36 Use the GUI module g_multirate and the Import option to load the MAT-file  
u_multirate1. Convert the sampling rate using L 5 4 and M 5 3 and a frequency- 
sampled filter. Plot the following.
(a) The time signals. What word is recorded?
(b) Their magnitude spectra
(c) The filter impulse response
8.13.3 MATLAB Computation
Section 8.2: integer Sampling Rate Converters
8.37 Consider the following periodic analog signal with three harmonics.
xa(t) 5 cos(2t) 2 .8 sin(4t) 1 .6 cos(6t)
 
Suppose this signal is sampled at fs 5 64 Hz using N 5 120 samples to produce 
a discrete-time signal x(k) 5 xa(kT) for 0 # k , N. Write a MATLAB program 
that uses f_decimate to decimate this signal by converting it to a sampling rate of 
Fs 5 32 Hz. For the anti-aliasing filter, use a windowed filter of order m 5 40 with 
the Hamming window. Use the subplot command and the stem function to plot the 
following discrete-time signals on one screen.
(a) The original signal x(k)
(b) The resampled signal y(k) below it using a different color
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.13 Problems     665
8.38 Consider the following periodic analog signal with three harmonics.
xa(t) 5 sin(2t) 2 3 cos(4t) 1 2 sin(6t)
 
Suppose this signal is sampled at fs 5 24 Hz using N 5 50 samples to produce 
a discrete-time signal x(k) 5 xa(kT) for 0 # k , N. Write a MATLAB program 
that uses f_interpol to interpolate this signal by converting it to a sampling rate of 
Fs 5 72 Hz. For the anti-imaging filter, use a least-squares filter of order m 5 50. 
Use the subplot command and the stem function to plot the following discrete-time 
signals on the same screen.
(a) The original signal x(k)
(b) The resampled signal y(k) below it using a different color
Section 8.3: Rational Sampling Rate Converters
8.39 Consider the following periodic analog signal with three harmonics.
xa(t) 5 2 cos(2t) 1 3 sin(4t) 2 3 sin(6t)
 
Suppose this signal is sampled at fs 5 30 Hz using N 5 50 samples to produce a 
discrete-time signal x(k) 5 xa(kT) for 0 # k , N. Write a MATLAB program that 
uses f_rateconv to convert it to a sampling rate of Fs 5 50 Hz. For the anti-aliasing 
and anti-imaging filter use a frequency-sampled filter of order m 5 60. Use the 
subplot command and the stem function to plot the following discrete-time signals 
on the same screen.
(a) The original signal x(k)
(b) The resampled signal y(k) below it using a different color
Section 8.5: Narrowband Filters 
8.40 Write a MATLAB function called u_narrowband that uses the DSP Companion 
functions f_firideal and f_rateconv to compute the zero-state response of the mul-
tirate narrowband lowpass filter shown in Figure 8.51. The calling sequence for 
u_narrowband is as follows.
% U_NARROWBAND:  Compute output of multirate narrowband lowpass 
% 
 
filter
%
% Usage:
% 
[y,M] = u_narrowband (x,F0,win,fs,m);
% Pre:
% 
x 
= array of length N containing input samples
% 
F0 = lowpass cutoff frequency (F0 <= fs/4)
% 
win = window type
%
% 
 
0 = rectangular
% 
 
1 = Hanning
% 
 
2 = Hamming
% 
 
3 = Blackman
%
% 
fs = sampling frequency
% 
m 
= filter order (even)
% Post:
% 
y 
= array of length N containing output samples
% 
M 
= frequency conversion factor used
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

666    Chapter 8  Multirate Signal Processing
 
Use the maximum frequency conversion factor possible. Test function u_narrow-
band by writing a program that uses it to design a lowpass filter with a cutoff fre-
quency of F0 5 10 Hz, a sampling frequency of fs 5 400 Hz, and a filter order of 
m 5 50. Plot the following.
(a) The narrowband filter impulse response
(b) The narrowband filter magnitude response and the ideal magnitude response 
on the same graph with a legend
Section 8.6: Filter Banks 
8.41 The data file prob8_41.mat contains a composite signal x, the number of subsignals 
it contains N, and the sampling rate fs of the subsignals. Write a MATLAB func-
tion called prob8_41.m that uses f_analbank to decompose the composite signal x 
into its N subsignals X. Your function should perform the following tasks.
(a) Load x, N, and fs from prob8_41.mat.
(b) Compute and plot the magnitude spectrum of x(k).
(c) Using the windowing method in Tables 6.1 and 6.2, compute the impulse 
response g of a causal FIR lowpass prototype filter of order m 5 80 with cut-
off frequency F0 5 fsy(4N) and a Blackman window. Use f_freqz to compute 
and plot the magnitude response of the prototype filter.
(d) Use f_analbank with the filter g to decompose the composite signal x(k) into 
its subsignals xi(k) for 0 # i , N.
(e) Using a 2 by 2 array of subplots, plot the magnitude spectra of the subsignals. 
Note: Use fftshift to interchange the first and second halves of the magnitude 
spectra.
Section 8.7: Perfect Reconstruction Filter Banks 
8.42 The data file prob8_41.mat contains a composite signal x, the number of subsignals 
it contains N and the sampling rate fs of the subsignals. Write a MATLAB function 
called prob8_42.m that uses f_analbank to decompose the composite signal into its 
subsignals X, and then use f_synbank to reconstruct y from the subsignals. Your 
function should perform the following tasks.
(a) Load x, N, and fs from prob8_41.mat.
(b) Using a 2 by 1 array of subplots, plot the real and imaginary parts of the 
composite signal x(k).
(c) Compute and plot the magnitude spectrum of the composite signal x(k).
(d) Using the windowing method in Tables 6.1 and 6.2, compute the impulse 
response g of a causal FIR lowpass prototype filter of order m 5 80 with cut-
off frequency F0 5 fsy(4N) and a Blackman window.
(e) Use f_analbank and g to decompose x into N subsignals X. Then use f_synbank 
with h 5 g to reconstruct y from the N subsignals X.
(f) Using a 2 by 1 array of subplots, plot the real and imaginary parts of the 
reconstructed composite signal y(k).
(g) Compute and plot the magnitude spectrum of the reconstructed composite 
signal y(k).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

667
Adaptive Signal Processing
C H A P T E R  9
CHAPTER ToPiCS
 9.1 
Motivation 
 9.2 
Mean Square Error 
 9.3 
Least Mean Square (LMS) 
Method 
 9.4 
Performance Analysis of 
LMS Method 
 9.5 
Modified LMS Methods 
 9.6 
Adaptive Filter Design 
with Pseudo-filters
 9.7 
Recursive Least Squares 
(RLS) Method 
*9.8 
Active Noise Control 
*9.9 
Adaptive Function 
Approximation 
*9.10 Nonlinear System  
Identification (NLMS) 
 9.11 GUI Modules and Case 
Studies 
 9.12 Chapter Summary 
 9.13 Problems
Motivation
The digital filters investigated in previous chapters have one fun-
damental characteristic in common. The coefficients of these 
filters are fixed; they do not evolve with time. When the filter 
parameters are allowed to vary with time, this leads to a power-
ful new family of digital filters called adaptive filters. This chap-
ter focuses on an adaptive FIR type of filter called a transversal 
filter that has the following generic form.
y(k) 5 o
m
i50
wi (k)x(k 2 i)
Notice that the constant FIR coefficient vector b has been 
replaced by a time-varying vector w(k) of length m 1 1 called 
the weight vector. The adaptive filter design problem consists of 
developing an algorithm for updating the weight vector, w(k), to 
ensure that the filter satisfies some design criterion. For example, 
the objective might be to get the filter output y(k) to track a 
desired output d(k) as time increases. The transversal filter struc-
ture has an important qualitative advantage over an adaptive 
IIR filter structure. Once the weight vector has converged, the 
resulting filter is guaranteed to be stable.
We begin this chapter by introducing some examples of appli-
cations of adaptive filters. Four broad classes of applications are 
examined, including system identification, channel equalization, 
signal prediction, and noise cancellation. Next the mean square 
error design criterion is formulated, and a closed-form filter 
9.1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

668    Chapter 9  Adaptive Signal Processing
solution called the Weiner filter is developed. A simple and elegant method for updating 
the filter weights called the least mean square or LMS method is then developed. The per-
formance characteristics of the LMS method are investigated, including bounds on the 
step size that ensure convergence, estimates of the convergence rate, and estimates of the 
steady-state error. A number of modifications to the basic LMS method are then presented, 
including the normalized LMS method, the correlation LMS method, and the leaky LMS 
method. An adaptive design technique for FIR filters using pseudo-filters is then intro-
duced. Next a fast recursive adaptive filter technique called the recursive least squares or 
RLS method is presented. A more general LMS technique called the filtered-x or FXLMS 
method is then developed along with a signal-synthesis method. Both are applied to the 
problem of active control of acoustic noise. Attention then turns to the problem of using 
adaptive techniques to approximate nonlinear functions of several variables with raised- 
cosine radial basis function (RBF) networks. These networks are then applied to the prob-
lem of identifying nonlinear discrete-time systems using a nonlinear least mean square 
(NLMS) method. Finally, a GUI module called g_adapt is introduced that allows the user 
to perform system identification without any need for programming. The chapter concludes 
with an application example, and a summary of adaptive signal processing techniques.
There are a variety of applications of adaptive filters that arise in fields ranging from 
equalization of telephone channels to geophysical exploration for oil and gas deposits. In 
this section some general classes of applications of adaptive signal processing are intro-
duced. A brief history of adaptive signal processing and its applications can be found in 
Haykin (2002).
9.1.1 System identification
The success enjoyed by engineers in applying analysis and design techniques to practical 
problems often can be traced to the effective use of mathematical models of physical 
phenomena. In many instances, a mathematical model can be developed by applying 
underlying physical principles to each component. However, there are other instances 
where this bottom-up approach is less effective because the physical system or phenom-
enon is too complex and is not well understood. In these cases it is often useful to think 
of the unknown system as a black box, where measurements can be taken of the input 
and output but little is known about the details of what is inside the box (hence the term 
“black”). It is assumed that the unknown system can be modeled as a linear discrete-time 
system. The problem of obtaining a model of the system from input and output measure-
ments is called the system identification problem. Adaptive filters are highly effective for 
performing system identification using the configuration shown in Figure 9.1.
It is standard practice to represent an adaptive filter in a block diagram using a diag-
onal arrow through the block. The arrow can be thought of as a needle on a dial that is 
adjusted as the parameters of the adaptive filter are changed. The system identification 
Black box
System identification
Figure 9.1: System 
Identification
e(k)
d(k)
2 1
Adaptive 
filter
Black
box
x(k)
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.1 Motivation    669
configuration in Figure 9.1 shows the adaptive filter in parallel with the unknown black-
box system. Both systems are driven by the same test input x(k). The objective is to adjust 
the parameters or coefficients of the adaptive filter so that its output mimics the response 
of the unknown system. Thus the desired output, d(k), is the output of the unknown 
system, and the difference between the desired output and the adaptive filter output, y(k), 
is the error signal e(k).
 
e(k) 5
D  d(k) 2 y(k) 
 (9.1.1)
The algorithm for updating the parameters of the adaptive filter uses the error e(k) 
and the input x(k) to adjust the weights so as to reduce the square of the error. Later 
the adaptive filter block will be explored in more detail. For now, notice that if the error 
signal can be made to go to zero, then the adaptive filter output is an exact reproduction 
of the unknown system output. In this case the adaptive filter becomes an exact model of 
the unknown black-box system. This model can be used in simulation studies, and it also 
can be used to predict the response of the black box to new inputs.
9.1.2 Channel Equalization
Another important class of applications of adaptive filters can be found in the communi-
cation industry. Consider the problem of transmitting information over a communication 
channel. At the receiving end, the signal will be distorted due to the effects of the channel 
itself. For example, the channel invariably will exhibit some type of frequency response 
characteristic, with some spectral components of the input attenuated more than others. 
In addition, there will be phase distortion and delay, and the signal may be corrupted 
with additive noise. To remove, or at least minimize, the detrimental effects of the com-
munication channel, one must pass the received signal through a filter that approximates 
the inverse of the channel so that the cascade or series connection of the two systems 
restores the original signal. The technique of inserting an inverse system in series with an 
original unknown system is called equalization because it results in an overall system with 
a unity transfer function with delay. Equalization, or inverse modeling, can be achieved 
with an adaptive filter using the configuration shown in Figure 9.2.
Here the black-box system that represents the unknown communication channel is in 
series with the adaptive filter. This series combination is in parallel with a delay element 
corresponding to a delay of M samples. Thus the desired output in this case is simply a 
delayed version of the transmitted signal.
 
d(k) 5 x(k 2 M) 
 (9.1.2)
The reason for inserting a delay is that the black-box system typically imparts some delay 
to the signal x(k) as it is processed by the system. Therefore an exact inverse system 
would have to include a corresponding time advance, something that is not feasible for 
Error signal
Equalization
Figure 9.2:  
Channel 
Equalization
z2M
Black
box
e(k)
d(k)
2 1
Adaptive 
filter
x(k)
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

670    Chapter 9  Adaptive Signal Processing
a causal filter. Furthermore, if the unknown black-box system does represent a commu-
nication channel, then delaying the signal by M samples will not distort the information 
that arrives at the receiver. Recall that a constant group delay can be achieved by using a 
linear-phase FIR filter.
9.1.3 Signal Prediction
As an illustration of another class of applications, consider the problem of encoding 
speech for transmission or storage. The direct technique is to encode the speech samples 
themselves. An efficient alternative is to use the past samples of the speech to predict 
the values of future samples. Typically, the error in the prediction has a smaller variance 
than the original speech signal itself. Consequently, the prediction error can be encoded 
using a smaller number of bits than a direct encoding of the speech itself. In this way an 
efficient encoding system can be implemented. An adaptive filter can be used to predict 
future samples of speech, or other signals, by using the configuration shown in Figure 9.3.
In this case the desired output is the input itself. Since the adaptive filter processes 
a delayed version of the input, the only way the error can be made to go to zero is if the 
adaptive filter successfully predicts the value of the input M samples into the future. Of 
course an exact prediction of a completely random input is not possible with a causal 
system. Typically, the input consists of an underlying deterministic component plus addi-
tive noise. In these cases, information from the past samples can be used to minimize the 
square of the prediction error.
9.1.4 Noise Cancellation
Still another broad class of applications of adaptive filters focuses on the problem 
of interference or noise cancellation. As an illustration, suppose the driver of a car 
places a call using a cell phone. The cell phone microphone will pick up both the 
driver’s voice plus ambient road noise that varies with the car’s speed and the driving 
conditions. To make the speaker’s voice more intelligible at the receiving end, a second 
reference microphone can be placed in the car to measure the ambient road noise. An 
adaptive filter then can be used to process this reference signal and subtract the result 
from the signal detected by the primary microphone, using the configuration shown 
in Figure 9.4.
Note that the desired output d(k) 5 x(k) 1 v(k) consists of speech plus noise. The 
reference signal r(k) is a filtered version of the noise. The presence of an unknown black-
box system takes into account the fact that the primary microphone and the reference 
microphone are placed at different locations and therefore the reference signal r(k) is 
different from, but correlated to, the noise v(k) appearing at primary microphone. The 
error in this case is
 
e(k) 5 x(k) 1 v(k) 2 y(k) 
 (9.1.3)
Figure 9.3:  
Signal Prediction
z2M
e(k)
d(k)
2 1
Adaptive 
filter
x(k)
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.2 Mean Square Error    671
If the speech, x(k), and the additive road noise, v(k), are uncorrelated with one another, 
then the minimum possible value for e2(k) occurs when y(k) 5 v(k), which corresponds to 
the road noise being removed completely from the transmitted speech signal.
Mean Square Error
9.2.1 Adaptive Transversal Filters
An mth-order adaptive transversal filter is a linear time-varying discrete-time system than 
can be represented by the following difference equation.
 
y(k) 5 o
m
i50
wi(k)x(k 2 i) 
 (9.2.1)
Note that the filter output is a time-varying linear combination of the past inputs. A sig-
nal flow graph of an adaptive transversal filter is shown in Figure 9.5 for the case m 5 4. 
Given the structure shown in Figure 9.5, a transversal filter is sometimes referred to as a 
tapped delay line.
A compact input-output formulation of a transversal filter can be obtained by intro-
ducing the following pair of (m 1 1) 3 1 column vectors.
 
u(k) 5
D  fx(k), x(k 2 1), Á , x(k 2 m)gT 
 (9.2.2a)
 
w(k) 5
D  fw0(k), w1(k), Á , wm(k)gT 
 (9.2.2b)
Here u(k) is a vector of past inputs called the state vector, and w(k) is the current value 
of the weight vector. Combining (9.2.1) and (9.2.2), the adaptive filter output can be 
expressed as a dot product of the two vectors.
 
y(k) 5 wT(k)u(k), k $ 0  
 (9.2.3)
9.2
Tapped delay line
State, weight vectors
Figure 9.4:  
Noise cancellation 
e(k)
d(k)
r(k)
2
1
1
Black
box
Adaptive
filter
x(k)
y(k)
(k)
Figure 9.5:  
Signal Flow Graph 
of an Adaptive 
Transversal Filter
x(k)
y(k)
w0(k)
w1(k)
w2(k)
w3(k)
w4(k)
z21
z21
z21
z21
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

672    Chapter 9  Adaptive Signal Processing
In view of (9.2.3), an adaptive filter can be thought of has having two inputs, the 
time-varying weight vector w(k) and the vector of past inputs u(k). The vector w(k) is 
itself the output of a weight-update algorithm as shown in Figure 9.6. Recall that d(k) 
in Figure 9.6 represents the desired output, and the difference between d(k) and the filter 
output y(k) is the error, e(k). That is,
 
e(k) 5 d(k) 2 y(k) 
 (9.2.4)
The details of how the desired output d(k) and the filter input x(k) are generated depend 
on the type of adaptive filter application. Examples of different classes of applications 
were presented in Section 9.1.
9.2.2 Cross-correlation Revisited
Typically, the filter input x(k) and the desired output d(k) are modeled as random signals 
or, more formally, as random processes (Haykin, 2002). This makes y(k) and e(k) random 
as well. For the purpose of this analysis, let us assume that x(k) and d(k) are stationary 
random signals, meaning that their statistical properties do not change with time. The 
notion of cross-correlation, first introduced in Section 2.8, can be extended to random 
signals using the expected value operator.
Let y(k) be an L-point random signal and let x(k) be an M-point random signal where 
M # L. Then the cross-correlation of y(k) with x(k) is denoted as ryx(i) and defined as
ryx(i) 5
D  E fy(k)x(k 2 i)g, 0 # i , L
Recall from (4.7.3) that if a random signal has the property that it is ergodic, then the 
expected value operation can be approximated using a time average. Consequently, for a 
signal x(k) the expected, or mean, value can be approximated as
 
E fx(k)g < 1
No
N21
i50
x(k 2 i), N W 1 
 (9.2.5)
If the signal x(k) is periodic, then the expected value can be determined exactly by com-
puting the average value of over one period.
Desired output
Random processes
DEFiNiTioN 
9.1 Random 
Cross-correlation 
Ergodic
Figure 9.6:  
Adaptive Filter 
Block Showing the 
Weight-update 
Algorithm
x(k)
d(k)
y(k)
w(k)
e(k)
2
1
Update
algorithm
Transversal
filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.2 Mean Square Error    673
When the approximation in (9.2.5) is used to evaluate the cross-correlation of two ran-
dom signals, Definition 9.1 reduces to the deterministic definition of linear cross-correlation 
introduced previously in Definition 2.5. It is in this sense that Definition 9.1 is a generaliza-
tion of the notion of cross-correlation to random signals.
There are two properties of cross-correlation that are helpful in the analysis of adap-
tive filters. Since x(k) is assumed to be stationary, the statistical properties of x(k) do not 
change with time. Consequently, if x(k) is translated in time its expected or mean value 
does not change.
 
E fx(k 2 i)g 5 E fx(k)g, i $ 0 
 (9.2.6)
Another fundamental property concerns the expected value of the product of two 
signals. If two random signals x(k) and y(k) are statistically independent of one another, 
then the expected value of their product is equal to the product of their expected values.
 
E fx(k)y(k)g 5 E fx(k)g E fy(k)g 
 (9.2.7)
Note that if x(k) and y(k) are statistically independent and either x(k) or y(k) has zero 
mean, then the expected value of their product is zero. Zero-mean signals for which 
E fx(k)y(k)g 5 0 are said to be uncorrelated.
Suppose v(k) denotes white noise uniformly distributed over fa, bg. This particular 
broadband signal turns out to be an excellent input signal for system identification pur-
poses due to its flat power density spectrum. White noise was examined in detail in Sec-
tion 4.6. For convenient reference, the following expression denotes the average power of 
uniform white noise as developed in Chapter 4 and summarized in Appendix 2.
 
Pv 5 b3 2 a3
3(b 2 a) 
 (9.2.8)
9.2.3 Mean Square Error
The average of the square of the error of the system in Figure 9.6 is referred to as the 
mean square error of the system. The mean square error, (w), can be expressed in terms 
of the expected value operator as follows.
 
(w) 5
D  E fe2(k)g 
 (9.2.9)
To see how the mean square error (MSE) is affected by the filter weights, consider the 
case when the weight vector w is held constant. Using (9.2.3) and (9.2.4), the square of 
the error can be expressed as follows.
e2(k) 5 fd(k) 2 wTu(k)g2
5 d 2(k) 2 2d(k)wTu(k) 1 fwTu(k)g2
 
5 d 2(k) 2 2wTd(k)u(k) 1 wTu(k)uT(k)w 
 (9.2.10)
Since the expected value operation is linear, the expected value of the sum is the sum 
of the expected values, and scaling a variable simply scales its expected value. Taking the 
expected value of both sides of (9.2.10) then yields the following expression for the mean 
square error.
 
(w) 5 E fd 2(k)g 2 2wTE fd(k)u(k)g 1 wTE fu(k)uT(k)gw 
 (9.2.11)
Stationary signals
Statistically  
independent signals
Uncorrelated signals
Mean square error
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

674    Chapter 9  Adaptive Signal Processing
To develop a compact formulation of (w), consider an (m 1 1) 3 1 column vector p 
and an (m 1 1) 3 (m 1 1) matrix R defined as follows.
 
p 5
D  E fd(k)u(k)g 
 (9.2.12a)
 
R 5
D  E fu(k)uT(k)g 
 (9.2.12b)
The vector p is referred to as the cross-correlation vector of the desired output d(k) with 
the vector of past inputs u(k). From (9.2.2a), pi 5 E fd(k)x(k 2 i)g. It then follows from 
Definition 9.1 that
 
pi 5 rdx(i ), 0 # i # m  
 (9.2.13)
The square matrix R, obtained by taking the expected value of the outer product of 
u(k) with itself, is referred to as the auto-correlation matrix of the past inputs. Again note 
from (9.2.2a) that
 
Rij 5 E fx(k 2 i)x(k 2 j )g, 0 # i, j # m 
 (9.2.14)
Since x(k) is assumed to be stationary, the signal x(k 2 i)x(k 2 j) can be trans-
lated in time without changing its expected value. Replacing k with k 1 i yields 
Rij 5 E fx(k)x(k 1 i 2 j)g. Thus from Definition 9.1
 
Rij 5 rxx( j 2 i ), 0 # i, j # m  
 (9.2.15)
The auto-correlation matrix has a number of interesting and useful properties. 
First notice from (9.2.14) that since x(k 2 i)x(k 2 j) 5 x(k 2 j)x(k 2 i), it follows that 
R is symmetric. Next observe from (9.2.15) that j 5 i yields Rii 5 rxx(0). But the auto- 
correlation evaluated at a lag of zero is just the average power. That is,
Rii 5 rxx(0)
 5 E fx2(k)g
 
 5 Px, 0 # i # m 
 (9.2.16)
Consequently, when x(k) is stationary, the diagonal elements of R are all identical 
and equal to the average power of the input. More generally, it is clear from (9.2.15) 
that the symmetric auto-correlation matrix R has diagonal bands, or stripes of equal 
elements, above and below the diagonal as can be seen from the case m 5 4 shown 
in (9.2.17). A matrix with this diagonal striped structure is referred to as a Toeplitz 
matrix.
 
R 53
rxx(0)
rxx(1)
rxx(2)
rxx(3)
rxx(4)
rxx(1)
rxx(0)
rxx(1)
rxx(2)
rxx(3)
rxx(2)
rxx(1)
rxx(0)
rxx(1)
rxx(2)
rxx(3)
rxx(2)
rxx(1)
rxx(0)
rxx(1)
rxx(4)
rxx(3)
rxx(2)
rxx(1)
rxx(0)4
 
 (9.2.17)
Using the definitions of the cross-correlation vector p and the input-correlation 
matrix R in (9.2.12), the expression for the mean square error performance function in 
(9.2.11) simplifies to
 
(w) 5 Pd 2 2wTp 1 wTRw  
 (9.2.18)
Cross-correlation 
vector
Auto-correlation 
matrix
Average power
Toeplitz matrix
Mean square error
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.2 Mean Square Error    675
Here Pd 5 E fd 2(k)g is the average power of the desired output. It is clear from (9.2.18) 
that the mean square error is a quadratic function of the weight vector w. Note that when 
m 5 1, the mean square error can be thought of as a surface over the w plane. The objec-
tive is to locate the lowest point on this error surface.
To find an optimal value for w, one that minimizes the mean square error, consider 
the gradient vector =(w) of partial derivatives of (w) with respect to the elements of w. 
Taking the derivative of (w) in (9.2.18) with respect to wi and combining the results for 
0 # i # m, it is possible to show that
 
=(w) 5 2(Rw 2 p) 
 (9.2.19)
Consider the case when the input-correlation matrix R is invertible. Setting =(w) 5 0 
in (9.2.19) and solving for w, one arrives at the following optimal value for the weight 
vector.
 
w* 5 R21p  
 (9.2.20)
The optimal weight vector w* in (9.2.20) is referred to as the Wiener solution (Levinson, 
1947).
Wiener solution
optimal Weight Vector
EXAMPLE 9.1
As an illustration of the mean square error and the optimal weight vector, con-
sider the following example adapted from Widrow and Sterns (1985). Suppose 
m 5 1 and the input and desired output are the following periodic functions of 
period N, where N . 2.
 x(k) 5 2 cos1
2k
N 2
 d(k) 5 sin1
2k
N 2
Thus the adaptive filter must perform a scaling and a phase shifting operation on 
the input. First consider the cross-correlation vector p. Since d(k) and x(k) are 
periodic, their expected values can be computed by averaging over one period. 
For convenience, let  5 2yN. Using (9.2.13) and the trigonometric identities in 
Appendix 2 yields
pi 5 E fd(k)x(k 2 i)g
 5 E f2 sin(k) cosh(k 2 i)jg
 5 2E fsin(k)hcos(k) cos(i) 1 sin(k) sin(i)jg
 5 2 cos(i)E fsin(k) cos(k)g 1 2 sin(i)E fsin2(k)g
 5 cos(i)E f sin(2k)g 1 sin(i)E f1 2  cos(2k)g
 5 sin(i), 0 # i # 1
(Continued     )
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

676    Chapter 9  Adaptive Signal Processing
Thus the cross-correlation between the desired output and the vector of past 
inputs is
p 5 f0, sin()gT
Next consider the auto-correlation matrix of the past inputs. Here
E fx(k)x(k 2 i)g 5 E f4 cos(k) cosh(k 2 i)jg
 5 4E fcos(k)hcoshk) cos(i) 1 sin(k) sin(i)jg
 5 4 cos(i)E fcos2(k)g 1 4 sin(i)E fcos(k) sin(k)g
 5 2 cos(i)E f1 1 cos(2k)g 1 2 sin(i)E fsin(2k)g
 5 2 cos(i)
Thus from (9.2.17), the auto-correlation matrix is
R 5 23
1
 cos()
 cos()
1 4
Since  5 2yN and N . 2, it is evident that R is symmetric, banded, and nonsin-
gular with
 det (R) 5 4f1 2  cos2()g
 5 4 sin2()
From (9.2.20), the optimal value for the weight vector in this case is
 w* 53
2
2 cos()
2 cos()
2 4
213
0
 sin()4
 5
1
4 sin2() 3
2
22 cos()
22 cos()
2 4 3
0
 sin()4
 5
1
2 sin2() 3
2 cos() sin()
 sin() 4
 5 .5f2 cot(),
 
cos()gT
To make the problem more specific, suppose N 5 4, in which case  5 y2. A plot 
of the resulting mean square error surface is shown in Figure 9.7. In this case the 
optimal weight vector that minimizes the mean square error is
w* 5 f0, .5gT
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.2 Mean Square Error    677
When adaptive filters are used for system identification, the user often has direct 
control of the input signal x(k). To get reliable results, the spectral content of the input 
should be sufficiently rich that it excites all of the natural modes of the system being 
identified. One input that is particularly rich in frequency content is a random white noise 
input, a signal whose power density spectrum is flat.
Figure 9.7: Mean Square Error Surface for Example 9.1
24
22
0
2
4
24
22
0
2
4
0
20
40
60
80
w1
w0
(w)
White Noise input
EXAMPLE 9.2
Suppose x(k) is zero-mean white noise with an average power of
Px 5 E fx2(k)g
To determine the input-correlation matrix, first note that for white noise the sig-
nals x(k) and x(k 2 i) are statistically independent for i Þ 0. Thus the expected 
value of the product is equal to the product of the expected values. Since x(k) is 
zero-mean white noise, E fx(k)g 5 0. Hence for i Þ 0
 E fx(k)x(k 2 i)g 5 E fx(k)gE fx(k 2 i)g
 5 0
It then follows from (9.2.16) that for zero-mean white noise with average power  
Px, the input-correlation matrix is simply
R 5 PxI
Consequently, zero-mean white noise produces a nonsingular, diagonal input- 
correlation matrix with the average power of x(k) along the diagonal. It follows 
from (9.2.20) that the optimal weight in this case is simply
w* 5 p
Px
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

678    Chapter 9  Adaptive Signal Processing
Least Mean Square (LMS) Method
The optimal weight vector found in Section 9.2 was based on the assumption that the 
input x(k) and the desired output d(k) are stationary random signals. This assumption is 
useful for analysis purposes, because it allows one to determine, for example, the charac-
teristics of the input that are required to ensure that an optimal weight vector exists and 
is unique. However, in actual applications of adaptive filters the input and desired output 
are often not stationary; instead their statistical properties evolve with time. When the 
input and desired output are not stationary; it is useful to iteratively update an estimate 
of the optimal weight vector using a numerical search technique.
Suppose w(0) is an initial guess for the optimal weight vector. For example, in the 
absence of any specialized knowledge about the application, one might simply take 
w(0) 5 0. At subsequent time steps, the new weight is set to the old weight plus a correc-
tion term as follows.
 
w(k 1 1) 5 w(k) 1 Dw(k), k $ 0 
 (9.3.1)
A simple way to compute a correction term, Dw(k), is to use the gradient vector of partial 
derivatives of the mean square error (w) with respect to the elements of w.
 
=i(w) 5
D  −(w)
−wi
, 0 # i # m 
 (9.3.2)
The gradient vector, =(w), points in the direction of maximum increase of (w). For 
example, when m 5 1, the mean square error is a surface and =(w) is a 2 3 1 vector that 
points in the steepest uphill direction, the direction of steepest ascent. Since the objec-
tive is to find the minimum point on this surface, consider a step of length  . 0 in the 
opposite direction of the gradient. That is, set Dw(k) 5 2=fw(k)g, in which case the 
weight-update algorithm becomes
 
w(k 1 1) 5 w(k) 2 =fw(k)g, k $ 0 
 (9.3.3)
The weight-update formula in (9.3.3) is called the method of steepest descent. Note 
that the step size, , must be kept small because as one departs from the point w(k), the 
direction of steepest descent changes. The main computational difficulty of the steepest- 
descent method is the need to compute the gradient vector, =(w), at each discrete time. 
Since the ith element of the gradient vector represents the slope of (w) along the i
th dimension, the gradient vector can be approximated numerically using differences. 
For example, let i j denote the jth column of the (m 1 1) 3 (m 1 1) identity matrix 
I 5 fi1, i2, Á , i m11g. If  . 0 is small, then the jth element of the gradient vector can be 
approximated with the following forward difference.
 
=j (w) < (w 1 i j11) 2 (w)

, 0 # j # m 
 (9.3.4)
Observe that in the limit as  approaches zero, the expression in (9.3.4) is the partial 
derivative of (w) with respect to wj. The approximation of the gradient in (9.3.4) requires 
m 1 2 evaluations of the mean square error. Suppose the mean square error is itself 
approximated by using a time average of N samples of the square of the error. From 
(9.2.3), each sample of e2(k) requires m 1 1 floating-point multiplications or FLOPs. 
Thus the total number of FLOPs required to numerically estimate the gradient of the 
mean square error is
 
r 5 N(m 1 1)(m 1 2) FLOPs 
 (9.3.5)
9.3
Steepest descent 
method
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.3 Least Mean Square (LMS) Method    679
For a large filter m W 1, and for an accurate estimate N W 1. Consequently, implement-
ing the steepest-descent method using a numerical estimate of the gradient vector can be 
computationally expensive.
There is an alternative approach to estimating the gradient vector that is much more 
cost effective (Widrow and Sterns, 1985). Suppose that, for the purpose of computing 
the gradient, the mean square error is approximated using the instantaneous value of the 
square of the error. That is, for the purpose of computing =(w), the following approxi-
mation is used for the mean square error.
 
(w) < e2(k) 
 (9.3.6)
This is clearly a rough approximation, because it is equivalent to using a single sample to 
estimate the mean. The approximation in (9.3.6) leads to a substantial simplification in 
the expression for the gradient. Let =
⁄(w) be the estimate of the gradient using (9.3.6). 
Then from (9.2.3) and (9.2.4)
=
⁄(w) 5 2e(k) −e(k)
−w
 
5 22e(k)u(k) 
 (9.3.7)
Using this estimate of the gradient, the steepest-descent method in (9.3.3) then reduces to 
the following simplified weight-update algorithm.
 
w(k 1 1) 5 w(k) 1 2e(k)u(k), k $ 0  
 (9.3.8)
The weight-update formula in (9.3.8) is called the least mean square or LMS method 
(Widrow and Hoff, 1960). Note that unlike the traditional steepest-descent method, the 
LMS method requires only m 1 1 FLOPs to estimate the gradient at each time step. The 
LMS is a highly efficient way to update the weight vector. For example, when N 5 10 and 
m 5 10, the LMS method is more than two orders of magnitude faster than the numeri-
cal steepest-descent method.
Although the approximation used to estimate the gradient vector in (9.3.6) may 
appear to be rather crude, experience has shown that the LMS algorithm for updating the 
weights is quite robust. Indeed, Hassibi et al. (1996) have shown that the LMS algorithm 
is optimal when a minimax error criterion is used.
The estimate of the gradient in (9.3.7) is itself a random signal. It is instructive to 
examine the mean or expected value of this random signal. Suppose the weight w has 
converged to its steady-state value and is constant. Starting from (9.3.7) and using the 
definitions of p and R in (9.2.12),
E f=
⁄(w)g 5 22E fe(k)u(k)g
5 22E fd(k)u(k) 2 y(k)u(k)g
5 22E fd(k)u(k) 2 u(k)huT(k)wjg
5 22hE fd(k)u(k)g 2 E fu(k)uT(k)gwj
 
5 2(Rw 2 p) 
 (9.3.9)
LMS method
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

680    Chapter 9  Adaptive Signal Processing
Figure 9.8: System Identification 
e(k)
d(k)
H(z)
2
1
Adaptive 
filter
x(k)
y(k)
System identification
EXAMPLE 9.3
To illustrate the LMS method, consider the system identification problem shown 
in Figure 9.8. To make the example specific, suppose the system to be identified 
has the following transfer function.
H(z) 5
2 2 3z21 2 z22 1 4z24 1 5z25 2 8z26
1 2 1.6z21 1 1.75z22 2 1.436z23 1 .6814z24 2 .1134z25 2 .0648z26
Next suppose the input x(k) consists of N 5 1000 samples of white noise uni-
formly distributed over f21, 1g. Let the order of the adaptive transversal filter be 
m 5 50, and suppose the step size is  5 .01. A plot of the first 500 samples of 
the square of the error, obtained by running exam9_3, is shown in Figure 9.9. It 
is clear that the square of the error converges close to zero after approximately 
400 samples.
One way to assess the effectiveness of the adaptive filter is to compare the 
magnitude response of the system H(z) with the magnitude response of the adap-
tive filter, W(z), using the final steady-state weight, w(N 2 1). The two magni-
tude responses are plotted in Figure 9.10, where it is evident that they are nearly 
identical. Note that this is true in spite of the fact that H(z) is a an IIR filter with 
six poles and six zeros, while the steady-state adaptive filter W(z) is an FIR filter 
of order m 5 50. By making the order of the adaptive filter sufficiently large, it 
is apparent that it can model an IIR filter as well. Of course, if the system to be 
identified is an FIR filter of order p, then an exact fit should be obtained using 
any adaptive filter of order m $ p assuming infinite precision arithmetic is used.
But from (9.2.19), the exact value of the gradient of the mean square error is 
=(w) 5 2(Rw 2 p). Hence
 
E f=
⁄(w)g 5 =(w) 
 (9.3.10)
That is, the expected value of the estimate of the gradient of the mean square error is equal 
to the gradient of the mean square. Therefore, =
⁄(w) is an unbiased estimate of =(w).
Unbiased estimate
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.3 Least Mean Square (LMS) Method    681
Figure 9.9: First 500 Samples of Squared Error during System Identification 
Using the LMS Method with m 5 50 and  5 .01
0
100 150
50
200 250 300 350 400 450 500
0
100
200
300
400
500
600
700
k
e2(k)
Figure 9.10: Magnitude Responses of the Original IIR System and the Iden-
tified System Using the LMS Method with m 5 50,  5 .01, and N 5 1000 
Samples
0
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45 0.5
0
10
20
30
40
50
60
70
Adaptive
Desired
A(f)
f/fs
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

682    Chapter 9  Adaptive Signal Processing
Performance Analysis of LMS Method
Although the LMS method is very simple to implement, there remains the question of 
determining an effective value for the step size, . The step size should be small enough 
to guarantee convergence to an acceptable steady-state error, yet large enough to ensure 
that the speed of convergence is reasonable.
9.4.1 Step Size
Recall that the LMS method uses the error e(k) and the vector of past inputs u(k) to 
update the weight estimate as follows.
 
w(k 1 1) 5 w(k) 1 2e(k)u(k), k $ 0 
 (9.4.1)
Since w(k) is a random signal, it is helpful to consider what happens to its mean or expected 
value as k increases. Taking the expected value of both sides of (9.4.1) and noting that 
e(k) 5 d(k) 2 uT(k)w(k), this yields
E fw(k 1 1)g 5 E fw(k)g 1 2E fe(k)u(k)g
5 E fw(k)g 1 2E fhd(k) 2 uT(k)w(k)ju(k)g
5 E fw(k)g 1 2E fd(k)u(k) 2 u(k)huT(k)w(k)jg
 
 5 E fw(k)g 1 2E fd(k)u(k)g 2 2E fu(k)uT(k)w(k)g 
 (9.4.2)
9.4
The DSP Companion contains the following function that implements the LMS 
method.
% F_LMS: System identification using least mean square (LMS) method
%
% Usage:
% 
[w,e]  = f_lms (x,d,m,mu,w)
% Pre:
% 
x  = N by 1 vector containing input samples
% 
d  = N by 1 vector containing desired output
% 
 
 samples
% 
m  = order of transversal filter (m >= 0)
% 
mu  = step size to use for updating w
% 
w  = an optional (m+1) by 1 vector containing
% 
 
 the initial values of the weights.
% 
 
 Default: w = 0
% Post:
% 
w  = (m+1) by 1 weight vector of filter
% 
 
 coefficients
% 
e  = an optional N by 1 vector of errors where
% 
 
 e(k) = d(k)-y(k)
% Notes:
% 
Typically mu << 1/[(m+1)*P_x] where P_x is the
% 
average power of input x.
DSP Companion
DSP Companion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.4 Performance Analysis of LMS Method    683
Recall that if two random signals are statistically independent of one another, the 
expected value of the product is the product of the expected values. For moderate con-
vergence rates, the past inputs u(k) and the weights w(k) can be assumed to be statistically 
independent. Using the definitions of R and p from (9.2.12), one can rewrite (9.4.2) as
E fw(k 1 1)g 5 E fw(k)g 1 2E fd(k)u(k)g 2 2E fu(k)uT(k)gE fw(k)g
5 E fw(k)g 1 2p 2 2RE fw(k)g
 
5 (I 2 2R)E fw(k)g 1 2p 
 (9.4.3)
To further simplify the expression for the expected value of the weight vector, it is 
helpful to introduce a signal that represents the variation of the weight from its optimal 
value. The weight variation vector is denoted as w(k) and defined as
 
w(k) 5
D  w(k) 2 w* 
 (9.4.4)
To reformulate (9.4.3) in terms of the weight variation, one can substitute w(k) =
w(k) 1 w* and use E fw*g 5 w* and Rw* 5 p. This yields
E fw(k 1 1)g 1 w* 5 (I 2 2R)hE fw(k)g 1 w*j 1 2p
5 (I 2 2R)E fw(k)g 1 (I 2 2R)w* 1 2p
 
5 (I 2 2R)E fw(k)g 1 w* 
 (9.4.5)
Thus the expected value of the weight variation at iteration k 1 1 is simply
 
E fw(k 1 1)g 5 (I 2 2R)E fw(k)g 
 (9.4.6)
The virtue of the formulation in (9.4.6) is that it can be solved directly for E fw(k)g 
using induction. Note that E fw(0)g 5 w(0), in which case E fw(1)g 5 (I 2 2R)w(0). 
More generally,
 
E fw(k)g 5 (I 2 2R)kw(0), k $ 0 
 (9.4.7)
The solution in (9.4.7) can be expressed in terms of the original weight vector w(k) by 
simply replacing w(k) with w(k) 2 w*. This yields the following closed-form solution for 
the expected value of the weight vector at step k.
 
E fw(k)g 5 w* 1 (I 2 2R)k fw(0) 2 w*g, k $ 0  
 (9.4.8)
It is clear from (9.4.8) that E fw(k)g will converge to the optimal weight w* starting from 
an arbitrary initial guess if and only if (I 2 2R)k converges to the zero matrix as k 
approaches infinity. At this point, it is helpful to make use of a result from linear algebra. 
If A is a square matrix, then
 
Ak S 0 as k S ` 
 (9.4.9)
if and only if the eigenvalues of A all lie strictly inside the unit circle of the complex 
plane (Noble, 1969). It can be shown by direct substitution that the ith eigenvalue of 
A 5 I 2 2R is ri 5 1 2 2i where i is the ith eigenvalue of R. Thus E fw(k)g in (9.4.8) 
converges to w* as k S ` if and only if
 
u1 2 2iu , 1 for 1 # i # m 1 1 
 (9.4.10)
Since R is symmetric and positive-definite, its eigenvalues i are real and positive. 
Consequently, (9.4.10) can be rewritten as 21 , 1 2 2i , 1. Subtracting one from 
Weight variation
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

684    Chapter 9  Adaptive Signal Processing
each term and dividing each term by 22i then yields the inequality 1yi .  . 0. This 
must hold for all m 1 1 eigenvalues of R. Let
 
max 5
D  maxh1, 2, Á , m11j 
 (9.4.11)
It then follows that the range of step sizes over which E fw(k 1 1)g converges, starting 
from an arbitrary w(0), is 0 ,  , 1ymax.
Let w(0) [ Rm11 be arbitrary and let max be the largest eigenvalue of the auto-cor-
relation matrix R. The LMS method converges in the following statistical sense if and 
only if 0 ,  , 1ymax.
E fw(k)g S w* as k S `
PRoPoSiTioN 
9.1 LMS Convergence 
Step Size
EXAMPLE 9.4
As a simple illustration of how to find a range of step sizes for the LMS method, 
consider an adaptive filter of order m 5 1. Suppose N $ 4 and the input and 
desired output are as follows.
 x(k) 5 2 cos1
2k
N 2
 d(k) 5 sin1
2k
N 2
For convenience, let  5 2yN. This two-dimensional adaptive filter was consid-
ered previously in Example 9.1, where it was determined that
R 5 2 3
1
 
cos()
 cos()
1 4
Thus the characteristic polynomial of the auto-correlation matrix is
D() 5  det hI 2 Rj
 5  det 53
 2 2
22 cos()
22 cos()
 2 2 46
 5 ( 2 2)2 2 4 cos2()
 5 2 2 4 1 4 2 4 cos2()
 5 2 2 4 1 4 sin2()
Using the quadratic formula, the eigenvalues of R are
 1,2 5 4 6 Ï16 2 16 sin2()
2
 5 2 6 2Ï1 2  sin2()
 5 2 6 2 cos()
 5 2f1 6 cos()g
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.4 Performance Analysis of LMS Method    685
A key advantage of the LMS method in (9.4.1) is that it is very simple to implement. 
For example, there is no need to compute the auto-correlation matrix R or the cross-cor-
relation vector p. Unfortunately, the upper bound on the step size in Proposition 9.1 is not 
nearly as easy to determine. Not only must the auto-correlation matrix R be constructed, 
but its eigenvalues must be computed as well. By making use of another result from linear 
algebra, an alternative step size bound can be developed, one that is more conservative 
but much easier to compute. Recall that the trace of a square matrix is just the sum of the 
diagonal elements. The trace is also equal to the sum of the eigenvalues. That is,
 
trace(R) 5 o
m11
i51
i 
 (9.4.12)
Since R is symmetric and positive-definite, i . 0. It then follows from (9.4.12) that
 
max , trace(R) 
 (9.4.13)
Applying (9.4.13) to Proposition 9.1, a more conservative range for the step size is 
0 ,  , 1ytrace(R). This effectively eliminates the need to compute eigenvalues. The 
requirement to find R itself also can be eliminated by exploiting the special banded 
structure of R. Recall from (9.2.16) that the diagonal elements of R are all equal to one 
another with Rii 5 Px where Px 5 E fx2(k)g is the average power of the input. Since the 
trace is just the sum of the m 1 1 diagonal elements, trace(R) 5 (m 1 1)Px. This leads 
to the following smaller, but much simpler, range of values for the step size over which 
convergence of the LMS method is assured.
 
0 ,  ,
1
(m 1 1)Px
 
 (9.4.14)
Note how the upper bound on  decreases with the order of the filter and the power 
of the input signal. In practical applications, it is not uncommon to choose a value for 
the step size in the range .01 , (m 1 1)Px , .1, well below the upper limit (Kuo and 
Morgan, 1996).
Trace
LMS step size
Note that the eigenvalues of R are real and positive. Since N $ 4, it follows that 
0 ,  # y2 and the largest eigenvalue is max 5 2f1 1  cos()g. Thus from Propo-
sition 9.1, the range of step sizes over which the LMS method converges is
0 ,  ,
.5
1 1  cos()
Revised Step Size
EXAMPLE 9.5
Suppose the input for an mth-order adaptive filter consists of zero-mean white 
noise uniformly distributed over an interval f2c, cg. From (9.2.8), the average 
power of x(k) is
Px 5 c2
3
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

686    Chapter 9  Adaptive Signal Processing
9.4.2 Convergence Rate
The step size determines not only whether the LMS method converges, but it also deter-
mines how fast it converges. One way to view convergence is to examine what happens to 
w(k) in (9.4.8). An equivalent way to view convergence is to examine the learning curve, 
which is a plot of the mean square error as a function of the iteration number. Recall 
from (9.2.18) that the mean square error can be expressed as follows.
 
fw(k)g 5 Pd 2 2wT(k)p 1 wT(k)Rw(k) 
 (9.4.15)
To see how fast the mean square error converges, it is helpful to develop an alter-
native representation for the auto-correlation matrix R. Let i denote the ith eigenvalue 
of R, and let qi be its associated eigenvector. That is,
 
Rqi 5 i qi, 1 # i # m 1 1 
 (9.4.16)
Suppose the m 1 1 eigenvectors are arranged as columns of an (m 1 1) 3 (m 1 1) matrix 
Q 5 fq1, q2, Á , qm11g. Next, let L be the diagonal matrix of order m 1 1, with the eigen-
values along the diagonal.
 
L 5
D  3
1
0
Á
0
0
2
Á
0
o
o
∞
o
0
0
Á
m114
 
 (9.4.17)
Using Q and L, the m 1 1 eigenvector equations in (9.4.16) can be rewritten as a single 
matrix equation as follows.
 
RQ 5 LQ 
 (9.4.18)
Note that the right-hand side of (9.4.18) can be replaced with QL because L is diagonal. 
Since the auto-correlation matrix R is symmetric, the eigenvectors of R form a linearly 
independent set, which means the eigenvector matrix Q is invertible. First commute the 
matrices on the right-hand side of (9.4.18), and then post-multiply both sides of (9.4.18) 
by Q21. This yields the following alternative representation of the auto-correlation matrix 
in terms of its eigenvectors and eigenvalues.
 
R 5 QLQ21  
 (9.4.19)
Learning curve
Eigenvector
Applying (9.4.14), this yields the following range of step sizes for the LMS 
method.
0 ,  ,
3
(m 1 1)c2
For example, for the system identification application in Example 9.3, the magni-
tude of the white noise input was c 5 1, and the filter order was m 5 50. Thus the 
range of step sizes for Example 9.3 is
0 ,  , .0588
The step size used in Example 9.3,  5 .01, was well within this range.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.4 Performance Analysis of LMS Method    687
The representation of R in (9.4.19) has a number of useful properties. For example, 
a direct calculation reveals that R2 5 QL2Q21. Using induction, it is not difficult to 
show that
 
Rk 5 QLkQ21, k $ 0 
 (9.4.20)
This property can be used to evaluate the rate of convergence. Substituting the expression 
for R from (9.4.19) into (9.4.8) and using (9.4.18), we have
E fw(k)g 5 w* 1 (I 2 2QLQ21)k fw(0) 2 w*g
5 w* 1 (QIQ21 2 2QLQ21)k fw(0) 2 w*g
5 w* 1 hQ(I 2 2L)Q21jk fw(0) 2 w*g
 
5 w* 1 Q(I 2 2L)kQ21fw(0) 2 w*g 
 (9.4.21)
The factor (I 2 2L)k in (9.4.21) consists of the following diagonal matrix where 
ri 5 1 2 2i.
 
(I 2 2L)k 53
rk
1
0
Á
0
0
rk
2
Á
0
o
o
∞
o
0
0
Á
rk
m114
 
 (9.4.22)
Note that this confirms that the LMS method converges if and only if u1 2 2iu , 1 for 
1 # i # m 1 1. The speed of convergence is dominated by the factor ri, whose magnitude 
is largest because this corresponds to the slowest mode. Let
 
min 5
D  minh1, 2, Á , m11j 
 (9.4.23)
Suppose the step size is constrained to be at most half of its maximum value,  
 # .5ymax. Then 0 # ri , 1, and the radius of the slowest or dominant mode is
 
rmax 5 1 2 2min 
 (9.4.24)
The rate of convergence of the LMS method can be characterized by an exponential 
time constant, mse. Since the mean square error is a quadratic function of the weights, 
the mean square error converges at a rate of r2k
max. Suppose the input and desired output 
are obtained by sampling with a sampling interval of T. Then the exponential rate of 
convergence is  exp(2kTymse). Using (9.4.24), this results in the following equation for 
the mean square error time constant.
 
 exp(2kTymse) 5 (1 2 2min)2k 
 (9.4.25)
Taking the log of both sides of (9.4.25) and solving the resulting equation for mse yields
 
mse 5
2T
2 ln(1 2 2min) 
 (9.4.26)
If  is sufficiently small, one can use the approximation ln (1 1 x) < x. This results in 
the following simplified approximation for the mean square error time constant of the 
LMS method.
 
mse <
T
4min s  
 (9.4.27)
LMS time constant
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

688    Chapter 9  Adaptive Signal Processing
Note that the time constant can be expressed in units of iterations, rather than seconds, 
by setting T 5 1. Furthermore, observe that in order to speed up convergence one must 
increase the step size. However, if the step size is made too large, then from Proposition 
9.1 the LMS will not converge at all.
Time Constant
EXAMPLE 9.6
Consider the system identification example presented in Example 9.3. There the 
filter order was m 5 50, and the input consisted of N 5 1000 samples of white 
noise uniformly distributed over f21, 1g. From (9.2.8), the average power of 
the input is Px 5 1y3. Recall that for a zero-mean white noise input the auto- 
correlation matrix is very easy to compute. In particular, from Example 9.2,
R < PxI
 51
1
32I
Since R is diagonal, it has a single eigenvalue,  5 1y3, repeated m 1 1 5 51 
times. Thus the minimum eigenvalue is
min 5 1
3
The step size used in Example 9.3 was  5 .01. Applying (9.4.27), this results in 
the following time constant estimate for the system identification example.
mse <
1
4min
5 3
.04
5 75
Here T 5 1, which yields the time constant in iterations. Since  exp(25) 5 .007, 
the mean square error should be reduced to less than one percent of its peak 
value after five time constants or M 5 375 iterations. Inspection of the plot of 
e2(k) versus k in Figure 9.9 confirms that this is the case, at least approximately. 
It should be pointed out that the plot of the squared error in Figure 9.9 is a 
rough approximation to the learning curve. Recall that the learning curve is a 
plot of the mean square error, fw(k)g, and to obtain a better approximation 
one would have to perform the system identification many times with different 
white noise inputs and then average the squares of the errors for each run (see  
Problem 9.34).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.4 Performance Analysis of LMS Method    689
9.4.3 Excess Mean Square Error
The previous analysis of the LMS method suggests that one should choose a step size 
that is as large as possible, consistent with convergence, in order to reduce the mean 
square error time constant. As it turns out, there is one more factor called the excess 
mean square error that grows with the step size. Once the excess mean square error is 
taken into account, one finds that in selecting  there is a tradeoff between convergence 
speed and steady-state accuracy.
Recall that the essential assumption of the LMS method is the approximation of the 
MSE mean square error (MSE), with the squared error for the purpose of estimating the 
gradient vector. This leads to the gradient approximation, =
⁄(w) 5 22e(k)u(k), found in 
(9.3.7). This estimate differs from the exact value, =(w) 5 2(Rw 2 p), in (9.2.19). The 
error in the estimate of the gradient of the mean square error can be modeled as an addi-
tive noise term as follows.
 
=fw(k)g 5 =
⁄fw(k)g 1 v(k) 
 (9.4.28)
The noise term, v(k), causes the steady-state value of the mean square error to be larger 
than the theoretical minimum value, and the difference is referred to as the excess mean 
square error.
 
excess(k) 5
D  fw(k)g 2 min 
 (9.4.29)
To determine an expression for the minimum mean square error, it is useful to refor-
mulate the mean square error in terms of the weight variation, w(k) 5 w* 1 w(k). Using 
(9.4.15) and substituting w(k) 5 w(k) 1 w*,
(w) 5 Pd 2 2pT(w* 1 w) 1 (w* 1 w)TR(w* 1 w)
5 Pd 2 2pTw* 2 2pTw 1 (w*)TRw* 1 (w*)TRw 1 wTRw* 1 wTRw
5 Pd 2 2pTw* 2 2pTw 1 (w*)Tp 1 (R21p)TRw 1 wTp 1 wTRw
5 Pd 2 2pTw* 2 2pTw 1 pTw* 1 pT(R21)TRw 1 pTw 1 wTRw
 
5 Pd 2 pTw* 1 wTRw 
 (9.4.30)
Here use was made of the following observations: RT 5 R, the transpose of the inverse is 
the inverse of the transpose, the transpose of the product is the product of the transposes 
in reverse order, and the transpose of a scalar is the scalar. From (9.4.30), it is clear that 
the mean square error or MSE achieves its minimum value at w 5 0. Using w* 5 R21p 
in (9.4.30), one arrives at the following expression for the minimum mean square error.
 
min 5 Pd 2 pTR21p 
 (9.4.31)
Combining (9.4.29) through (9.4.31) then results in the following formulation of the 
excess mean square error in terms of the weight variation.
 
excess(k) 5 wT(k)Rw(k) 
 (9.4.32)
By examining the statistical properties of the gradient noise term in (9.4.28), it is 
possible to develop the following approximation for the excess MSE (Widrow and  
Sterns, 1985).
 

 excess (k) < 
 min (m 1 1)Px 
 (9.4.33)
Minimum MSE
Excess MSE
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

690    Chapter 9  Adaptive Signal Processing
It is apparent from (9.4.31) that substantial computational effort is required to determine 
the minimum mean squared error. For this reason, a normalized version of the excess 
mean square error is often used. The misadjustment factor of the LMS method is denoted 
Mf and defined
 
Mf 5
D  excess
min
 
 (9.4.34)
From (9.4.33) the misadjustment factor of the LMS method is simply
 
Mf < (m 1 1)Px  
 (9.4.35)
Note that the normalized excess mean square error increases with the step size, the filter 
order, and the average power of the input. The dependence on the step size means that 
in order to reduce Mf one must decrease , but to reduce mse one must increase . Thus 
there is a tradeoff between the convergence speed and the steady-state accuracy of the 
LMS method.
Misadjustment factor
Excess Mean Square Error
EXAMPLE 9.7
To illustrate the relationship between excess mean square error and step size, 
consider an adaptive filter of order m 5 1 with the following input and desired 
output.
x(k) 5 2 cos(.5k) 1 v(k)
d(k) 5 sin(.5k)
Here v(k) represents white noise uniformly distributed over the interval f2.5, .5g 
that is statistically independent of 2 cos(.5k). Thus the average power of the 
input is
Px 5 E fx2(k)g
 5 E f4 cos2(.5k) 1 4v(k) cos(.5k) 1 v2(k)g
 5 4E fcos2(.5k)g 1 4E fv(k) cos(.5k)g 1 E fv2(k)g
 5 2E fcos(k) 1 1g 1 4E fv(k)gE fcos(.5k)g 1 Pv
 5 2 1 Pv
From (9.2.8), the average power of the white noise uniformly distributed over 
f2.5, .5g is Pv 5 (.5)2y3. Thus the average power of the input is
Px 5 25
12
Using (9.4.14) with m 5 1, the range of step sizes needed for convergence of the 
LMS method is
0 ,  , .24
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.4 Performance Analysis of LMS Method    691
The effects of step size, filter order, and input power on the performance characteris-
tics of the LMS method are summarized in Table 9.1.
Figure 9.11: Excess Mean Square Error of an Adaptive Filter of Order m 5 1 
Using Two Step Sizes: (a)  5 .1, (b)  5 .01 
0
50
100
150
200
250
0
0.5
1
k
0
50
100
(b)  5 .01
(a)  5 .1
150
200
250
0
0.5
1
k
e2(k)
e2(k)
Property 
Value  
Convergence range 
0 ,  ,
1
(m 1 1)Px
 
Learning-curve time constant 
mse <
1
4min
 
Misadjustment factor 
Mf < (m 1 1)Px 
Table 9.1:  
Performance 
Characteristics of an 
Adaptive Transversal 
Filter of Order m with 
Step Size  and Input 
Power Px 5 E fx2(k)g
Next from (9.4.35), the normalized excess mean square error or misadjustment 
factor, Mf, is
Mf < 25
6
To see the effects of , suppose w(0) 5 0 and consider two special cases corre-
sponding to  5 .1 and  5 .01. Plots of the squared error can be obtained by 
running exam9_7. Observe from Figure 9.11a that when  5 .1 the squared error 
converges very rapidly due to the relatively large step size. However, it is apparent 
that the steady-state error is also relatively large. When  5 .01 in Figure 9.11b, 
the squared error takes longer to converge, but one is rewarded with a steady-
state excess mean square error that is clearly smaller. This illustrates the tradeoff 
between convergence speed and steady-state accuracy.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

692    Chapter 9  Adaptive Signal Processing
Modified LMS Methods
There are a number of useful modifications that can be made to the LMS method to 
enhance performance. In this section three variants of the basic LMS method are exam-
ined (Kuo and Morgan, 1996).
9.5.1 Normalized LMS Method
Recall from Table 9.1 that the upper bound on  needed to ensure convergence depends 
on the filter order m and the input power Px. A similar observation holds for the step size 
needed to achieve a given misadjustment factor or excess mean square error. To develop 
a version of the LMS method that has a step size, , that does not depend on the input 
power or the filter order, the following normalized LMS method has been proposed.
 
w(k 1 1) 5 w(k) 1 2(k)e(k)u(k), k $ 0  
 (9.5.1)
Note how the normalized LMS method differs from the basic LMS method in that the 
step size, (k), is no longer constant. Instead, it varies with time as follows.
 
(k) 5

(m 1 1)P
⁄
x(k)
 
 (9.5.2)
Here P
⁄
x(k) is a running estimate of the average power of the input. Observe that if P
⁄
x(k) 
is replaced with the exact average power, Px, then from Table 9.1 the range of constant 
step sizes needed to ensure convergence is simply
 
0 ,  , 1 
 (9.5.3)
It is in this sense that the step size has been normalized. The beauty of the normalized 
approach is that a single value can be used for , independent of the filter size and the 
input power.
The simplest way to estimate the average power of the input is to use a rectangular 
window or running-average filter. The following is an Nth-order running-average filter 
with input x2(k).
 
P
⁄
x(k) 5
1
N 1 1o
N
i50
x2(k 2 i) 
 (9.5.4)
One of the key features of the LMS method is its highly efficient implementation. In order 
to preserve this feature, care must be taken to minimize the number of floating-point oper-
ations required at each iteration. The number of multiplications and divisions needed to 
compute P
⁄
x in (9.5.4) is N 1 2. This can be reduced with a recursive formulation of the 
running-average filter. Using the change of variable, j 5 i 2 1, (9.5.4) can be rewritten as
P
⁄
x(k) 5
1
N 1 1o
N21
j521
x2(k 2 1 2 j)
5
1
N 1 1 3o
N
j50
x2(k 2 1 2 j)4 1 x2(k) 2 x2(k 2 N)
N 1 1
 
5 P
⁄
x(k 2 1) 1 x2(k) 2 x2(k 2 N)
N 1 1
 
 (9.5.5)
The recursive formulation in (9.5.5) reduces the number of FLOPs per iteration from 
N 1 2 to three. Although the implementation in (9.5.5) is faster than the one in (9.5.4), 
9.5
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.5 Modified LMS Methods    693
it is not more efficient in terms of memory requirements because N 1 1 samples of the 
input still have to be stored. Recall from (9.5.1) that m 1 1 samples of the input are 
already being stored in the form of the vector u. If N 5 m, then the following dot product 
can be used instead to estimate the average power of the input.
 
P
⁄
x(k) 5 uT(k)u(k)
m 1 1  
 (9.5.6)
This approach has the advantage that no additional values of x(k) need to be stored. 
Substitution of (9.5.6) into (9.5.2) results in a further simplification because the m 1 1 
factors cancel. This yields the following simplified expression for the time-varying step 
size (Slock, 1993).
 
(k) 5

uT(k)u(k) 
 (9.5.7)
There is one additional practical difficulty that can arise when a nonstationary input 
is used. If the input x(k) is zero for m 1 1 consecutive samples, then u(k) 5 0 and the step 
size in (9.5.7) becomes unbounded. This problem also occurs when the algorithm starts 
up if u(0) 5 0. To avoid this numerical difficulty, let  be a small positive value. Then the 
step size will never be larger than y if the following modified step size is used for the 
normalized LMS method.
 
(k) 5

 1 uT(k)u(k)  
 (9.5.8)
Normalized LMS
Normalized LMS Method
EXAMPLE 9.8
To illustrate how the step size changes with the normalized LMS method, sup-
pose the input is the following amplitude-modulated sine wave.
x(k) 5 cos1
k
1002 sin1
k
5 2
Next suppose the desired output d(k) is produced by applying x(k) to the follow-
ing second-order IIR resonator filter.
H(z) 5
1 2 z22
1 1 z21 1 .9z22
Let the order of the adaptive filter be m 5 5, and suppose the normalized step size 
is  5 .5. If  5 .05, then the maximum step size is y 5 10. Plots of the input 
x(k), the squared error, e2(k), and the variable step size (k), obtained by running 
exam9_8, are shown in Figure 9.12. Notice that the amplitude-modulated input 
signal reaches its minimum value at k 5 50 in Figure 9.12a where the cosine fac-
tor is zero. The step size hits its peak value in Figure 9.12b somewhat later due to 
the delay in the running-average estimate of the input power. At k 5 0 the step 
size saturates at (0) 5 y. Observe that in spite of the increase in step size due 
to a loss of input signal power, the squared error in Figure 9.12c converges and 
remains small even when the step size increases.
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

694    Chapter 9  Adaptive Signal Processing
0
20
30
10
40
50
60
70
80
90
100
21
0
1
(a)
k
x(k)
(k)
0
100
0
5
10
(b)
k
k
0
100
0
0.5
1
(c)
e2(k)
20
30
10
40
50
60
70
80
90
20
30
10
40
50
60
70
80
90
9.5.2 Correlation LMS Method
Recall from Table 9.1 that the learning-curve time constant is inversely proportional to 
the step size. Consequently, the step size  should be relatively large to ensure rapid con-
vergence. However, the excess mean square error following convergence is directly pro-
portional to , which means that  should be relatively small to improve steady-state 
accuracy. To avoid this tradeoff, one might use a large step size while convergence is occur-
ring, and then a small step size once convergence has been achieved. The essential task, 
then, is to detect when convergence has taken place. It is not realistic to use w* or min to 
detect convergence because of the computational burden involved. Instead, a less direct 
means must be employed. Suppose w(k) has converged to w*, and consider the product of 
the error e(k) with the vector of past inputs u(k). Recalling that uT(k)w is a scalar,
e(k)u(k) 5 fd(k) 2 y(k)gu(k)
5 fd(k) 2 uT(k)wgu(k)
 
5 d(k)u(k) 2 u(k)uT(k)w 
 (9.5.9)
Taking the expected value of both sides of (9.5.9) and evaluating the result at the optimal 
weight, w* 5 R21p, then yields
E fe(k)u(k)g 5 E fd(k)u(k)g 2 E fu(k)uT(k)gw*
5 p 2 Rw*
 
5 0 
 (9.5.10)
Figure 9.12: Normalized LMS Method with m 5 5,  5 .5, and  5 .05: (a) Input, 
(b) Step Size, (c) Squared Error 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.5 Modified LMS Methods    695
Given the definition of u(k) in (9.2.2), the expected value of e(k)u(k) can be 
interpreted as a cross-correlation of the error with the input. In particular, using  
Definition 9.1 and (9.2.2), one can rewrite (9.5.10) as
 
rex(i) 5 0, 0 # i # m 
 (9.5.11)
From (9.5.11), it is evident that when the weight vector is optimal, the error is uncor-
related with the input. This is an instance of a more general principle that says when an 
optimal solution is found, the error in the solution is orthogonal to the data on which the 
solution is based. From the special case i 5 0 in (9.5.11), one can obtain the following 
scaler relationship, which holds when the LMS method has converged.
 
E fe(k)x(k)g 5 0 
 (9.5.12)
The basic idea behind the correlation LMS method (Shan and Kailath, 1988) is to 
choose a step size that is directly proportional to the magnitude of E fe(k)x(k)g. This way, 
the step size will become small when the LMS method has converged but will be larger 
during the convergence process. One way to estimate the expected value in (9.5.12) is to 
use a running-average filter with input e(k)x(k). However, this would mean increased 
storage requirements because the samples of e(k) are not already stored. A less expensive 
alternative approach to approximating E fe(k)x(k)g is to use a first-order lowpass IIR 
filter with the following transfer function.
 
H(z) 5 (1 2 )z
z 2   
 (9.5.13)
The scalar 0 ,  , 1 is called a smoothing parameter, and typically  < 1. The filter 
output will be an estimate of E fe(k)x(k)g using an exponentially weighted average. The 
equivalent width of the exponential window is N 5 1y(1 2 ) samples. If r(k) is the filter 
output, and e(k)x(k) is the filter input, then
 
r(k 1 1) 5 r(k) 1 (1 2 )e(k)x(k)  
 (9.5.14)
Since r(k) becomes small once convergence has taken place, the step size is made propor-
tional to ur(k)u using a proportionality constant or relative step size of  . 0. This results 
in the following time-varying step size for the correlation LMS method.
 
(k) 5 ur(k)u  
 (9.5.15)
The correlation LMS method can be interpreted as having two modes of opera-
tion. When convergence has been achieved, the step size becomes small and the algo-
rithm is in the sleep mode. Because (k) is small, the excess mean square error is also 
small. Furthermore, if y(k) contains measurement noise that is uncorrelated with x(k), 
this noise will not cause an increase in (k). However, if d(k) or x(k) change signifi-
cantly, this causes ur(k)u to increase, and the algorithm then enters the active or track-
ing mode characterized by an increased step size. Once the algorithm converges to the 
new optimal weight, the step size decreases again and the algorithm reenters the sleep 
mode.
Smoothing parameter
Correlation LMS
Sleep mode
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

696    Chapter 9  Adaptive Signal Processing
Figure 9.13: A Time-varying Feedback System where the Switch Opens at 
k 5 Ny2
d(k)
q(k)
2
1
G(z)
k 5 N/2
x(k)
Correlation LMS Method
EXAMPLE 9.9
To illustrate how the correlation method detects convergence and changes modes 
of operation, let the input be N samples of white noise uniformly distributed over 
the interval f21, 1g. Consider the feedback system shown in Figure 9.13. Suppose 
the open-loop transfer function (when the switch is open) is
 Hopen(z) 5 D(z)
X(z)
 5 G(z)
 5
1.28
z2 2 .64
The system in Figure 9.13 is a time-varying linear system because the switch in the 
feedback path starts out closed but opens starting at sample k 5 Ny2. This might 
correspond, for example, to a feedback sensor malfunctioning. When the switch 
is closed, the Z-transform of the intermediate signal q(k) is
Q(z) 5 X(z) 2 D(z)
 5 X(z) 2 G(z)Q(z)
Solving for Q(z) yields
Q(z) 5
X(z)
1 1 G(z)
It then follows that, when the switch is closed, the output is
D(z) 5 G(z)Q(z)
 5 G(z)X(z)
1 1 G(z)
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.5 Modified LMS Methods    697
Figure 9.14: Identification of a Time-varying Feedback System Using the 
Correlation LMS Method with m 5 25,  5 .5, and  5 .95: (a) Squared 
Error, (b) Step Size
0
100
200
300
400
500
600
700
800
0
2
4
6
(a)
k
0
100
200
300
400
500
600
700
800
0
0.05
0.1
0.15
0.2
(b)
k
(k)
e2(k)
Thus the closed-loop transfer function of the system in Figure 9.13, with the 
switch closed, is
 Hclosed(z) 5 D(z)
X(z)
 5
G(z)
1 1 G(z)
 5
1.28y(z2 2 .64)
1 1 1.28y(z2 2 .64)
 5
1.28
z2 1 .64
Consequently, when the switch is closed for samples 0 # k , Ny2, the system 
has an imaginary pair of poles at z 5 6j0.8. At time k 5 Ny2, the switch opens, 
and the transfer function reduces to G(z) with real poles z 5 60.8. Suppose this 
time-varying system is identified with an adaptive filter of order m 5 25 using 
the correlation LMS method with N 5 800. Let the relative step size be  5 .5, 
and the smoothing factor be  5 .95. A plot of the squared error and the step 
size, obtained by running exam9_9, is shown in Figure 9.14. Observe how the 
error converges in about 150 samples in Figure 9.14a and enters the sleep mode 
around 250 samples in Figure 9.14b with a small step size. At sample Ny2 5 400, 
the desired output abruptly changes and the step size increases, indicating that 
the active or tracking mode has been entered. The error reconverges around 550 
samples and then reenters the sleep mode around 650 samples.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

698    Chapter 9  Adaptive Signal Processing
9.5.3 Leaky LMS Method
White noise is a highly effective input for system identification because it has a flat power 
density spectrum and therefore excites all of the natural modes of the system being iden-
tified. When an input with poor spectral content is used, the auto-correlation matrix R 
can become singular, and the LMS method can diverge with one or more elements of the 
weight vector growing without bound. An elegant way to guard against this possibility is 
to introduce a second term in the mean square error objective function. Let  . 0, and 
consider the following augmented MSE.
 
 fw(k)g 5
D  E fe2(k)g 1 wT(k)w(k) 
 (9.5.16)
The last term in (9.5.16) is called a penalty function term because the minimization pro-
cess tends to penalize any selection of w for which wTw is large. In this way, the search for 
a minimum automatically avoids solutions for which uuwuu is large. The parameter  . 0 
controls how severe the penalty is, and when  5 0 the objective function in (9.5.16) 
reduces to the original mean square error, fw(k)g.
To see what effect the penalty term has on the LMS algorithm, consider the gradient 
vector of partial derivatives of  with respect to the elements of w. Using the assumption 
E fe2(k)g < e2(k) from (9.3.6) to compute an estimate of the gradient yields
=
⁄(w) 5 2e −e
−w 1 2w
5 22e −y
−w 1 2w
 
5 22eu 1 2w 
 (9.5.17)
Substituting this estimate for the gradient into the steepest-descent method in (9.3.3) then 
yields the following weight-update formula.
w(k 1 1) 5 w(k) 2 f2w(k) 2 2e(k)u(k)g
 
5 (1 2 2)w(k) 1 2e(k)u(k) 
 (9.5.18)
Finally, define  5
D  1 2 2. Substituting  into (9.5.18) then results in the following sim-
plified formulation called the leaky LMS method.
 
w(k 1 1) 5 w(k) 1 2e(k)u(k), i $ 0  
 (9.5.19)
The composite parameter  is called the leakage factor. Note that when  5 1, 
the leaky LMS method reduces to the basic LMS method. If u(k) 5 0, then w(k) 
“leaks’’ to zero at the rate w(k) 5 kw(0). Typically the leakage factor is in the range 
0 ,  , 1 with  < 1. It can be shown that including a leakage factor has the same 
effect as adding low-level white noise to the input (Gitlin et al., 1982). This makes the 
algorithm more stable for a variety of inputs. However, it also means that there is a cor-
responding increase in the excess mean square error due to the presence of the penalty 
Augmented MSE
Penalty function
Leaky LMS
Leakage factor
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.5 Modified LMS Methods    699
Figure 9.15: Prediction of the Input Signal
z2M
e(k)
d(k)
2 1
Adaptive 
filter
x(k)
y(k)
Leaky LMS Method
EXAMPLE 9.10
To illustrate the use of the leaky LMS method, consider the problem of using an 
adaptive filter to predict the value the input signal as shown in Figure 9.15. Sup-
pose the input consists of two sinusoids plus noise.
x(k) 5 2 sin1
k
122 1 3 cos1
k
4 2 1 v(k)
Here v(k) is white noise uniformly distributed over the interval f2.2, .2g. Next, 
suppose it is desired to predict the value of the input M 5 10 samples into 
the future. Let the order of the adaptive filter be m 5 20, and the step size be  
 5 .002. To keep the excess mean square error associated with leakage small, 
one needs a leakage factor of  5 1 2 2 where  V .5. Setting  5 .05 
yields  5 .9998. Running exam9_10 from g_dsp produces the plots shown in  
Figure 9.16. Observe from Figure 9.16c that after about 40 samples, the algorithm 
has converged. The filter output y(k) in Figure 9.16b is an effective approximation 
of the input x(k) in Figure 9.16a, but advanced by M 5 10 samples.
term. Bellanger (1987) has shown that the excess mean square error is proportional to 
(1 2 )2y2, which means that this ratio must be kept small. Since  5 1 2 2, this is 
equivalent to 42 V 1 or
 
 5 1 2 2 
 (9.5.20a)
 
 V .5 
 (9.5.20b)
It is of interest to note that, because y(k) 5 wT(k)u(k), limiting wT(k)w(k) has the effect 
of limiting the magnitude of the output. This can be useful in applications such as active 
noise control, where a large y(k) can overdrive a speaker and distort the sound (Elliott  
et al., 1987).
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

700    Chapter 9  Adaptive Signal Processing
The normalized, correlation, and leaky LMS methods are three popular variants of 
the basic LMS method. There are other modifications that have been proposed (Kuo and 
Morgan, 1996). For example, the step size  can be replaced by a diagonal time-varying 
step size matrix M(k) where the ith diagonal element is i(k). This modification, with 
each dimension having its own step size, is called the variable step size LMS method. 
Other modifications, intended to increase speed at the expense of accuracy, include the 
signed LMS methods which replace e(k) with sgnfe(k)g or u(k) withsgn [u(k)] where sgn is 
the sign or signum function.
Figure 9.16: Prediction M 5 10 Samples Ahead Using the Leaky LMS Method 
with m 5 20,  5 .002, and  5 .9998: (a) Input, (b) Output, (c) Squared Error
20
30
10
40
50
60
70
80
90
20
30
10
40
50
60
70
80
90
0
100
25
0
5
(a)
k
0
100
25
0
5
(b)
k
0
10
20
30
40
50
60
70
80
90
0
20
40
(c)
k
x(k)
y(k)
e2(k)
The DSP Companion contains the following functions which correspond to modi-
fied versions of the basic LMS method.
% F_LMSNORM: System identification using normalized LMS method
% F_LMSCORR: System identification using correlation LMS method.
% F_LMSLEAK: System identification using leaky LMS method
%
% Usage:
% 
[w,e,mu] = f_lmsnorm (x,d,m,alpha,delta,w);
% 
[w,e,mu] = f_lmscorr (x,d,m,alpha,beta,w);
% 
[w,e]  
= f_lmsleak (x,d,m,mu,nu,w);
DSP Companion
DSP Companion
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.6 Adaptive Filter Design with Pseudo-filters     701
Adaptive Filter Design with Pseudo-filters 
9.6.1 Pseudo-filters
Recall from Chapter 6 that most FIR filters are designed to have prescribed magnitude 
responses and linear phase responses. Filters with prescribed nonlinear phase characteris-
tics can be designed using the quadrature method in Chapter 6. As an alternative, the LMS 
method can be used to compute filter coefficients. The basic idea behind this approach is to 
use a synthetic pseudo-filter to generate the desired output as shown in Figure 9.17.
As the name implies, a pseudo-filter is a fictional linear discrete-time system that 
may or may not have a physical realization. A pseudo-filter is characterized implicitly by 
a desired relationship between a periodic input and a steady-state output. Let T be the 
sampling interval, and suppose the input consists of a sum of N sinusoids as follows.
 
x(k) 5 o
N21
i50
Ci cos(2fi kT) 
 (9.6.1)
9.6
% Pre:
% 
x  
= N by 1 vector containing input samples
% 
d  
= N by 1 vector containing desired output
% 
 
 samples
% 
m  
= order of transversal filter (m >= 0)
% 
alpha  = normalized step size (0 to 1)
% 
delta  = an optional positive scalar controlling
% 
 
 the maximum step size which is mu =
% 
 
 alpha/delta. Default: alpha/100
% 
w  
= an optional (m+1) by 1 vector containing
% 
 
 the initial values of the weights.
% 
 
 Default: w = 0
% 
beta  = an scalar containing the smoothing
% 
 
 parameter. beta is approximately one
% 
 
 with 0 < beta < 1. Default: 1 - 0.5/(m+1)
% 
mu  
= step size to use for updating w
% 
nu  
= an optional leakage factor in the range 0 to 1.
% 
 
 Pick nu = 1 - 2*mu*gamma where gamma << 0.5.
% 
 
 (default: 1 - 0.1*mu).
% Post:
% 
w  = (m+1) by 1 weight vector of filter
% 
 
 
coefficients
% 
e  = an optional N by 1 vector of errors where
% 
 
 
e(k) = d(k)-y(k)
% 
mu = an optional N by 1 vector of step sizes
% Notes:
% 
1.  When nu = 1, the leaky LMS method reduces to the basic
% 
 
LMS method.
% 
2. Typically mu << 1/[(m+1)*P_x] where P_x is the
% 
 
average power of input x.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

702    Chapter 9  Adaptive Signal Processing
The only constraints on the discrete frequencies hf0, f1, Á , fN21j are that they be distinct 
from one another and that they lie within the Nyquist range 0 # fi , fsy2. For example, 
the discrete frequencies are often taken to be the uniformly spaced DFT frequencies.
 
fi 5 ifs
2N, 0 # i , N 
 (9.6.2)
The relative weights, Ci . 0, in (9.6.1) are selected based on the importance of each fre-
quency in the overall design specification. For example, if Ck . Ci, then frequency fk 
will be given more emphasis than frequency fi. As a starting point, one can use uniform 
weighting with
 
Ci 5 1, 0 # i , N 
 (9.6.3)
Once the input is selected, the desired steady-state output is then specified. Since the 
artificial pseudo-filter is assumed to be linear, the steady-state output will be periodic with 
the same period as the input.
 
d(k) 5 o
N21
i50
AiCi cos (2fikT 1 i) 
 (9.6.4)
Here Ai and i denote the desired gain and phase shift, respectively, at frequency fi. Sup-
pose that the adaptive transversal filter in Figure 9.17 is of order m. It is convenient to 
represent the total phase shift in terms of its variation from a linear-phase filter.
 
( f ) 5 ( f ) 2 mf T 
 (9.6.5)
Here the linear phase term, 2mf T, represents a delay of my2 samples. The term ( f ) is 
called the residual phase shift because it represents the phase shift remaining after the lin-
ear phase shift associated with a delay of  5 mTy2 has been removed. When the residual 
phase shift is  5 0, this corresponds to a linear-phase FIR filter of order m.
The design specifications consist of a set of four N 3 1 vectors, h  f, C, A, j. Here f  
is the frequency vector, C is the relative weight vector, A is the magnitude vector, and  is 
the residual phase vector. In this way, N samples of the desired frequency response, both 
magnitude and phase, can be specified.
Frequency Response Constraints
It should be emphasized that the implicit relationship between x(k) and d(k) represents a 
pseudo-filter because the magnitude Ai 5 A(fi) and phase i 5 (fi) of a causal discrete-
time system are not independent of one another. For a causal filter, the real and imagi-
nary parts of the frequency response are interdependent, and so are the magnitude and 
phase (Proakis and Manolakis, 1992). Consequently, one cannot independently specify 
both the magnitude and the phase and expect to obtain an exact fit using a causal linear 
Uniform relative 
weights
Pseudo-filter output
Residual phase shift
Design specifications
Figure 9.17:  
Adaptive Filter 
Design Using a 
Pseudo-filter 
e(k)
d(k)
2 1
Adaptive 
filter
Pseudo-filter
x(k)
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.6 Adaptive Filter Design with Pseudo-filters     703
filter. Instead, an optimal approximation to the pseudo-filter specifications using a causal 
adaptive transversal filter of order m is obtained.
There are also constraints on the desired magnitude and phase characteristics that 
arise from the fact that the weighting coefficients w(k) are real. Suppose w(k) has con-
verged after M iterations to an FIR filter with real coefficient vector b 5 w(M 2 1). Thus 
the transfer function is
 
W(z) 5 o
m
i50
biz2i 
 (9.6.6)
Recall that the frequency response W( f ) is just the transfer function W(z) evaluated 
along the unit circle using z 5 exp( j2f T). At two points along the unit circle z, and 
therefore W( f ), are real, so the imaginary part is zero.
 
A( f ) sinf( f )g 5 0, f [ h0, fsy2j 
 (9.6.7)
Consider the two cases separately. When f 5 0, it follows from (9.6.5) that (0) 5 (0). 
Thus
 
A(0) sinf(0)g 5 0 
 (9.6.8)
Consequently, in selecting the filter design specifications, for a real filter either the DC 
gain should be A(0) 5 0, or the DC residual phase shift (0) should be an integer multiple 
of . The case when f 5 fsy2 depends on the filter order m. Suppose m is even. Note from 
(9.6.5) that
 
 sinf(fsy2)g 5  sinf(fsy2) 2 my2)g 
 (9.6.9)
Using the sine of the sum trigonometric identity from Appendix 2, if m is even then 
sinf(fsy2)g 5 6 sinf(fsy2)g, so the constraints on A( f ) and ( f ) at f 5 fsy2 are similar 
to those in (9.6.8).
 
A(fsy2) sinf(fsy2)g 5 0 
 (9.6.10)
Therefore, in selecting the filter design specifications for a real filter of even order, either 
the gain at the Nyquist frequency should be A(fsy2) 5 0, or the residual phase shift (fsy2) 
should be an integer multiple of . It is possible to show that when m is odd, (fsy2) 
should be an odd multiple of y2 or A(fsy2) should be zero.
9.6.2 Adaptive Filter Design
When the order of the adaptive filter is relatively small, the (m 1 1) 3 1 weight vector 
w can be computed offline by solving Rw 5 p. Given x(k) and d(k), closed form expres-
sions for the input auto-correlation matrix R and the cross-correlation vector p can 
be obtained (Problems 9.15, 9.16). This direct approach can become computationally  
expensive (and sensitive to roundoff error) as m becomes large. In these instances it makes 
more sense to numerically search for an optimal w using the LMS method.
 
w(k 1 1) 5 w(k) 1 2e(k)u(k) 
 (9.6.11)
Since the objective is to find a fixed FIR filter of order m that best fits the design specifi-
cations, the adaptive filter is allowed to run for M W 1 iterations until the squared error 
has converged to its steady-state value. The coefficient vector of the fixed FIR filter, W(z), 
is then set to the final steady-state value of the weights.
 
 b 5 w(M 2 1) 
 (9.6.12a)
 
 W(z) 5 o
m
i50
biz2i
 
 (9.6.12b)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

704    Chapter 9  Adaptive Signal Processing
The frequency response of the FIR filter is then computed and compared with the pseudo-
filter specifications. If the fit is acceptable, then the process terminates. Otherwise the order 
m can be increased, or the relative weights C can be changed at those frequencies where the 
error is largest. The overall design process is summarized in the flowchart in Figure 9.18.
Figure 9.18:  
Adaptive FIR Filter 
Design Process
Acceptable
fit?
No
Yes
Pick N, M, ƒ, A, 
Pick m, C
Apply LMS
Compare H(ƒ)
b 5 w(M 2 1)
y 5 bTu
Adaptive FiR Filter Design
EXAMPLE 9.11
To illustrate the use of a pseudo-filter, consider the problem of designing an FIR 
filter with the following desired magnitude response and residual phase response.
 A( f ) 55
1.5 2 6f
fs
,
0 # fs , fs
6
.5,
fs
6 # f , fs
3
.5 1 .5 sin1
6f 
fs 2,
fs
3 # f , fs
2
 ( f ) 5 2f T sin(4f T )
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.6 Adaptive Filter Design with Pseudo-filters     705
Figure 9.19: Frequency Responses of a Pseudo-filter and an FIR Filter of 
Order m 5 30: (a) Magnitude Responses, (b) Residual Phase Responses
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
2
A(f)
(a)
Pseudo-filter
FIR filter
0
0.1
0.15
0.15
0.05
0.05
0.2
0.25
0.25
0.3
0.35
0.35
0.4
0.45
0.45
0.5
21
20.5
0
0.5
f/fs
f/fs
(f) 
(b)
Pseudo-filter
FIR filter
Suppose there are N 5 60 discrete frequencies uniformly distributed as in (9.6.2). 
Let the relative weights also be uniform as in (9.6.3). Suppose the order of the 
adaptive FIR filter is m 5 30 and the step size is  5 .0001. Let the LMS method 
run for M 5 2000 iterations, starting from an initial guess of w(0) 5 0. A com-
parison of the desired and actual frequency responses can be obtained by running 
exam9_11 with the results shown in Figure 9.19. Notice that there is a good fit of 
both the magnitude response in Figure 9.19a and the residual phase response in 
Figure 9.19b.
9.6.3 Linear-phase Adaptive Filters
The special case of linear-phase filters or ( f ) 5 0 is important because it corresponds to 
each spectral component of x(k) being delayed by the same amount as it is processed by the 
filter. Hence there is no phase distortion of the input, only a delay. Recall from Table 5.1 
that an FIR filter of order m with coefficient vector b is a type-1 linear-phase filter if m is 
even and the filter coefficients satisfy the following even symmetry condition.
 
bi 5 bm2i, 0 # i # m 
 (9.6.13)
A signal flow graph of a transversal filter satisfying this linear-phase symmetry condition 
is shown in Figure 9.20 for the case m 5 4. The signal flow graph in Figure 9.20 can be 
made more efficient by combining branches with identical weights. This results in the 
equivalent signal flow graph shown in Figure 9.21, which features fewer floating-point 
Linear-phase symmetry 
condition
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

706    Chapter 9  Adaptive Signal Processing
multiplications. To develop a concise formulation of the filter output, consider the  
following pair of (my2 1 1) 3 1 vectors.
w⁄(k) 5
D  fw0(k), w1(k), Á , wmy2(k)gT 
 (9.6.14a)
u⁄(k) 5
D  fx(k)1x(k 2m), Á, x(k2my211) 1 x(k2my221), x(k2my2)gT 
 (9.6.14b)
Here w⁄(k) consists of the first my2 1 1 weights, while u⁄(k) is constructed from pairs of 
the past inputs. Applying (9.6.9) to Figure 9.21, the linear-phase transversal filter has the 
following compact representation using the dot product.
 
y(k) 5 w⁄(k)Tu⁄(k), k $ 0 
 (9.6.15)
Note that for large values of m, the linear-phase formulation in (9.6.10) requires approxi-
mately half as many floating-point multiplications or FLOPs as the standard representa-
tion in (9.2.3). When the LMS method is applied to the linear-phase transversal structure, 
there is a similar savings in computational effort using
 
w⁄(k 1 1) 5 w⁄(k) 1 2e(k)u⁄(k), k $ 0 
 (9.6.16)
Figure 9.20:  
A Type-1 Linear-
phase Transversal 
Filter of Order 
m 5 4 
x(k)
y(k)
w2(k)
w1(k)
w0(k)
w1(k)
w2(k)
z21
z21
z21
z21
Figure 9.21:  
Equivalent  
Linear-phase 
Transversal Filter 
of Order m 5 4 
x(k)
y(k)
w1(k)
w0(k)
w2(k)
z21
z21
z21
z21
Adaptive Linear-phase FiR Filter
EXAMPLE 9.12
To illustrate the design of an adaptive linear-phase FIR filter, consider a pseudo-
filter with the following piecewise-constant magnitude response and linear phase 
response specifications.
A( f ) 5 a(f 1 fsy4) 2 a(f 2 fsy4)
 ( f ) 5 2mf T
Suppose there are N 5 60 discrete frequencies uniformly distributed as in (9.6.2). 
Let the order of the FIR filter be m 5 120, and the step size be  5 .00005. 
Suppose the LMS method runs for M 5 1500 iterations starting from an initial 
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.6 Adaptive Filter Design with Pseudo-filters     707
Figure 9.22: Learning Curve with Step Size  5 0.00005
0
500
1000
1500
0
100
200
300
400
500
600
700
800
900
k
e2(k)
Figure 9.23: Magnitude Responses of a Pseudo-filter and a Linear-phase FIR 
Filter of Order m 5 120
0
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45 0.5
20.5
0
0.5
1
1.5
f/fs
A(f)
Pseudo-filter
FIR filter
guess of w(0) 5 0. The results obtained from running exam9_12 are shown in 
Figures 9.22 and 9.23. The learning curve for the adaptive filter is shown in  
Figure 9.22. The convergence is somewhat unusual in that it exhibits a peri-
odic sequence of spikes of decreasing amplitude. A comparison of magnitude 
responses is shown in Figure 9.23. Observe that the two responses are quite close 
when using a high-order filter and a small step size. Careful inspection shows that 
the stopband attenuation only approximates a gain of zero. When A( f ) is plotted 
using a log scale, the stopband attenuation is approximately 240 dB.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

708    Chapter 9  Adaptive Signal Processing
Recursive Least Squares (RLS) Method
There is a popular alternative to the LMS method called the recursive least squares or 
RLS method (Treichler et al., 2001; Haykin, 2002). The RLS method typically con-
verges much faster than the LMS method, but at a cost of more computational effort 
per iteration.
9.7.1 Performance Criterion
To formulate the RLS method, consider a modification of the least squares performance 
criterion introduced in Section 9.2. Suppose the following more general time-varying 
performance criterion is used.
 
k(w) 5 o
k
i51
k 2 ie2(i) 1 kwTw, k $ 1 
 (9.7.1)
Here the exponential weighting factor, 0 ,  # 1, is also called the forgetting factor 
because when  , 1 it has the effect of reducing the contributions from errors in the 
remote past. The second term in (9.7.1) is a regularization term with  . 0 called the  
regularization parameter. Note that the second term is similar to the penalty function 
term in the leaky LMS method in that it tends to prevent solutions for which wTw grows 
arbitrarily large. Thus it has the effect of making the RLS method more stable. When 
 5 1 (no exponential weighting) and  5 0 (no regularization), the performance crite-
rion in (9.7.1) is proportional to the mean square error at time k.
To determine a weight vector w that minimizes k(w), substitute e(i) 5 d(i) 2 wTu(i) 
into (9.7.1), where d(i) is the desired output, and u(i) is the vector of past inputs. This 
results in the following more detailed expression for the performance index.
 k(w) 5 o
k
i51
k2ifd(i) 2 wTu(i)g2 1 kwTw
 5 o
k
i51
 
k2i hd 2(i) 2 2d(i)wTu(i) 1 fwTu(i)g2 j 1 kwTw
 5 o
k
i51
k2ifd 2(i) 2 2wTd(i)u(i)g 1 o
k
i51
k2iwTu(i)fuT(i)wg 1 kwTw
 5 o
k
i51
k2ifd 2(i) 2 2wTd(i)u(i)g 1 wT1o
k
i51
k 2 iu(i)uT(i) 1 kI2w 
 (9.7.2)
The expression for k(w) can be made more concise by introducing the following gener-
alized versions of the auto-correlation matrix and cross-correlation vector, respectively, 
at time k.
 
R(k) 5
D  o
k
i51
k 2 iu(i)uT(i) 1 kI 
 (9.7.3a)
 
p(k) 5
D  o
k
i51
k 2 id(i)u(i) 
 (9.7.3b)
9.7
Forgetting factor
Regularization 
parameter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.7 Recursive Least Squares (RLS) Method    709
Substituting (9.7.3) into (9.7.2), the RLS performance criterion can be expressed as the 
following quadratic function of the weight vector.
 
k(w) 5 o
k
i51
k2id 2(i) 2 2wTp(k) 1 wTR(k)w 
 (9.7.4)
Following the same procedure that was used in Section 9.2, the gradient vector 
of partial derivatives of k(w) with respect to the elements of w can be shown to be 
=k(w) 5 2fR(k)w 2 p(k)g. Setting =k(w) 5 0 and solving for w, one arrives at the fol-
lowing expression for the optimal weight at time k.
 
w(k) 5 R21(k)p(k) 
 (9.7.5)
Unlike the LMS method, which asymptotically approaches the optimal weight vector 
using a gradient-based search, the RLS method attempts to find the optimal weight at 
each iteration.
9.7.2 Recursive Formulation
Although the weight vector in (9.7.5) is optimal in terms of minimizing k(w), it is appar-
ent that the computational effort required to find w(k) is large, and it grows more burden-
some as k increases. Fortunately, there is a way to reformulate the required computations 
to make them more economical. The basic idea is to start with the solution at iteration 
k 2 1 and add a correction term to obtain the solution at iteration k. In this way, the 
required quantities can be computed recursively. For example, using (9.7.3a) one can 
recast the expression for R(k) as follows.
R(k) 5  1o
k
i51
k2i21u(i)uT(i) 1 k21I2
 
5  1o
k21
i51
k2i21u(i)uT(i) 1 k21I2 1 u(k)uT(k) 
 (9.7.6)
Observe from (9.7.3a) that the coefficient of  in (9.7.6) is just R(k 2 1). Consequently, 
the exponentially weighted and regularized auto-correlation matrix can be computed 
recursively as follows.
 
R(k) 5 R(k 2 1) 1 u(k)uT(k), k $ 1 
 (9.7.7)
An initial value for R(k) is required to start the recursion process. Using (9.7.3a) and 
assuming that the input x(k) is causal, one can set R(0) 5 I.
A similar procedure can be used to obtain a recursive formulation for the general-
ized cross-correlation vector in (9.7.3b) by factoring out a  and separating the i 5 k 
term. This yields the following recursive formulation for p(k) that can be initialized with 
p(0) 5 0.
 
p(k) 5 p(k 2 1) 1 d(k)u(k), k $ 1 
 (9.7.8)
Although the recursive computations of R(k) and p(k) greatly simplify the computa-
tional effort for these quantities, there remains the problem of inverting R(k) in (9.7.5) to 
find w(k). It is this step that dominates the computational effort because the number of 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

710    Chapter 9  Adaptive Signal Processing
floating-point operations or FLOPs needed to solve R(k)w 5 p(k) is proportional to m3, 
where m is the order of the transversal filter. As it turns out, it is also possible to compute 
R21(k) recursively. To achieve this, one needs to make use of a result from linear algebra. Let 
A and C be square nonsingular matrices, and let B and D be matrices of appropriate dimen-
sions. Then the matrix inversion lemma can be stated as follows (Woodbury, 1950; Kailath, 
1960).
 
(A 1 BCD)21 5 A21 2 A21B(DA21B 1 C21)21DA21  
 (9.7.9)
This result is precisely what is needed to express R21(k) in terms of R21(k 2 1). Recalling 
(9.7.7), let A 5 R(k 2 1), B 5 u(k), C 5 1, and D 5 uT(k). Then applying the matrix 
inversion lemma in (9.7.9)
 
R21(k) 5 1
 3R21(k 2 1) 2 R21(k 2 1)u(k)uT(k)R21(k 2 1)
 1 uT(k)R21(k 2 1)u(k) 4 
 (9.7.10)
To simplify the final formulation, it is helpful to introduce the following notational 
quantities.
 
r(k) 5
D  R21(k 2 1)u(k) 
 (9.7.11a)
 
c(k) 5
D   1 uT(k)r(k) 
 (9.7.11b)
From the expression for R(k) in (9.7.3a), it is evident that R(k) is a symmetric matrix. 
Since the transpose of the inverse equals the inverse of the transpose, this means that 
rT(k) 5 uT(k)R21(k 2 1). Substituting (9.7.11) into (9.7.10), the inverse of the generalized 
auto-correlation matrix can be expressed recursively as follows.
 
R21(k) 5 1
 3R21(k 2 1) 2 r(k)rT(k)
c(k) 4, k $ 1 
 (9.7.12)
The beauty of the formulation in (9.7.12) is that no explicit matrix inversions are required. 
Instead, the expression for the inverse is updated at each step by using dot products and 
scalar multiplications. To start the process, an initial value for the inverse of the auto- 
correlation matrix is required. Assuming x(k) is causal in (9.7.3a), let
 
R21(0) 5 21I 
 (9.7.13)
Although this initial estimate of the inverse of the auto-correlation matrix is not likely to 
be accurate (except perhaps for a white noise input), the exponential weighting associated 
with  , 1 tends to minimize the effects of any initial error in the estimate after a suffi-
cient number of iterations. The effective window length associated with the exponential 
weighting is
 
M 5
1
1 2  
 (9.7.14)
The steps required to compute the optimal weight at each step using the RLS method 
are summarized in Algorithm 9.1. To emphasize the fact that no inverses are explicitly 
computed, the notation Q is used for R21.
Matrix inversion 
lemma
Effective window 
length
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.7 Recursive Least Squares (RLS) Method    711
1. Pick 0 ,  # 1,  . 0, m $ 0, N $ 1.
2.  Set w 5 0, p 5 0, and Q 5 21I. Here w and p are (m 1 1) 3 1 and Q is 
(m 1 1) 3 (m 1 1).
3. For k 5 1 to N compute
{ 
u 5 fx(k), x(k 2 1), Á , x(k 2 m)gT
r 5 Qu
c 5  1 uTr
p 5 p 1 d(k)u
Q 5 1
 3Q 2 rrT
c 4
w 5 Qp
}
The RLS method in Algorithm 9.1 typically converges much faster than the LMS 
method. However, the computational effort per iteration is larger, even with the efficient 
recursive formulation. For moderate to large values of the transversal filter order m, the com-
putational effort in Algorithm 9.1 is dominated by the computation of r, w, and Q in step 
3. The number of FLOPs required to compute r, w, and the symmetric Q is approximately 
3(m 1 1)2. Thus the computational effort is proportional to m2 for large values of m. This 
makes the RLS method an algorithm of order O(m2). This is in contrast to the much simpler 
LMS method that is an algorithm of order O(m), with the computational effort proportional 
to m. There is a faster implementation of the RLS method called the fast transversal filter 
(FTF) method that is linear in m (Cioffi and Kailath, 1984; Slock and Kailath, 1991).
The design parameters associated with the RLS method are the forgetting factor 
0 ,  # 1, the regularization parameter  . 0, and the transversal filter order m $ 0. 
The required filter order depends on the application and is often found empirically. 
Haykin (2002) has shown that the parameter  5 1 2  plays a role similar to the step 
size in the LMS method. Therefore,  should be close to unity to keep  small. If a given 
effective window length for the exponential weighting is desired, then the forgetting fac-
tor can be computed using (9.7.14). The choice for the regularization parameter depends 
on the signal-to-noise ratio (SNR) of the input x(k). Moustakides (1997), has shown that 
when the SNR is high (e.g., 30 dB or higher), the RLS method exhibits fast convergence 
using the following value for the regularization parameter.
 
 5 Px 
 (9.7.15)
Here Px 5 E fx2(k)g is the average power of x(k), which is assumed to have zero mean; 
otherwise the variance of x(k) should be used. As the SNR of x(k) decreases, the value 
of  should be increased.
ALgoRiTHM 
9.1 RLS Method
RLS Method
EXAMPLE 9.13
To compare the performance characteristics of the RLS and LMS methods, again 
consider the system identification problem posed in Example 9.3. Here the system 
to be identified was a sixth-order IIR system with the following transfer function.
H(z) 5
2 2 3z21 2 z22 1 4z24 1 5z25 2 8z26
1 2 1.6z21 1 1.75z22 2 1.436z23 1 .6814z24 2 .1134z25 2 .0648z26
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

712    Chapter 9  Adaptive Signal Processing
Suppose the input x(k) consists of N 5 1000 samples of white noise uni-
formly distributed over f21, 1g. Let the order of the adaptive transversal filter be  
m 5 50, and suppose a forgetting factor of  5 .98 is used. The regularization 
parameter  is set to the average power of the input as in (9.7.15). A plot of the 
first 200 samples of the square of the error, obtained by running exam9_13, is 
shown in Figure 9.24. In this case the square of the error converges close to zero 
after approximately 30 samples. This is in contrast to the LMS method, previously 
shown in Figure 9.9, which took approximately 400 samples to converge. Thus, 
when measured in iterations, the RLS method is faster than the LMS method by 
an order of magnitude. However, it should be kept in mind that the LMS method 
requires about m 5 50 FLOPs per iteration, whereas the RLS method requires 
about 3m2 5 7500 FLOPs per iteration. In terms of FLOPs the two methods 
appear to be roughly equivalent in this case.
Based on (9.7.5), one might expect the RLS method to converge even faster, say 
in one iteration. The reason it took approximately 30 iterations to converge was due 
to the transients associated with the startup of the algorithm. Since x(k) is a causal 
signal, the vector of past inputs u continues to be populated with zero samples for 
the first m 5 50 iterations. The RLS method converges in less than m samples in this 
instance due to the presence of the forgetting factor  5 .98, which tends to reduce 
the influence of samples in the remote past.
An FIR model, W(z), identified with the RLS method is obtained by using 
the final steady-state estimate for the weight vector, w(N 2 1). The magni-
tude responses of this FIR system and the original system H(z) are plotted in  
Figure 9.25, where it is evident that they are nearly identical.
Figure 9.24: First 200 Samples of Squared Error during System Identification 
Using the RLS Method with m 5 50,  5 .99, and  5 Px
0
40
60
80
20
100 120
160 180
140
200
0
0.5
1
1.5
2
2.5
3
k
e2(k)
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.7 Recursive Least Squares (RLS) Method    713
0
0.1 0.15
0.05
0.2 0.25 0.3 0.35 0.4 0.45 0.5
0
10
20
30
40
50
60
70
f/fs
A(f)
H(z)
W(z)
Figure 9.25: Magnitude Responses of the System, H(z), and the Identified 
Adaptive Model, W(z), Using the RLS Method with m 5 50,  5 .99, and 
 5 Px
The DSP Companion contains the following function which implements the RLS 
method in Algorithm 9.1.
% F_RLS: System identification using the RLS method
%
% Usage:
%  
[w,e] 
= f_rls (x,d,m,gamma,delta,w)
% Pre:
% 
x 
= N by 1 vector containing input samples
%  
d  
= N by 1 vector containing desired output samples
%  
m  
= order of transversal filter (m >= 0)
%  
gamma  = forgetting factor (0 to 1)
% 
delta  = optional regularization parameter
%  
 
 (delta > 0). Default P_x
%  
w  
= optional initial values of the weights.
%  
 
 Default: w = 0
% Post:
%  
w  = (m+1) by 1 weight vector of filter
%  
 
 
coefficients
%  
e  =  an optional N by 1 vector of errors where
%  
 
 
e(k) = d(k)-y(k)
% Note:
%  
As the SNR of x decreases, delta should be increased.
DSP Companion
DSP Companion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

714    Chapter 9  Adaptive Signal Processing
Active Noise Control
One of the emerging application areas of adaptive signal processing is active noise control. 
The basic idea behind the active control of acoustic noise is to inject a secondary sound into 
an environment so as to cancel the primary sound using destructive interference. Appli-
cation areas include jet engine noise, road noise in automobiles, blower noise in air ducts, 
transformer noise, and industrial noise from rotating machines (Kuo and Morgan, 1996). 
Active noise control requires an adaptive filter configuration as shown in Figure 9.26.
Here the input or reference signal, x(k), denotes samples of acoustic noise obtained 
from a microphone or a non-acoustic sensor. The transfer function G(z) represents the 
physical characteristics of the air channel over which the noise travels. The primary noise, 
d(k), is combined with secondary noise, y(k) produced by the adaptive filter. The secondary 
noise, also called anti-noise, is designed to destructively interfere with the primary noise 
so as to produce silence at the error microphone, e(k). The feature that makes Figure 9.26  
different from a standard system identification configuration is the appearance of a 
secondary-path system with transfer function F(z). The secondary system represents the 
physical system used to generate and measure the secondary sound. It includes such things 
as a power amplifier, a speaker, the secondary air channel, the error microphone, and a 
preamp. The system F(z) can be modeled offline using system identification techniques.
9.8.1 The Filtered-x LMS Method
The basic LMS method needs to be modified to take into account the presence of the 
secondary-path transfer function F(z) in Figure 9.26. To this end, suppose the adaptive 
transversal filter in Figure 9.26 has converged to an FIR system with a constant weight 
vector b. The transfer function of the resulting FIR filter is then
 
W(z) 5 o
m
i50
biz2i 
 (9.8.1)
Replacing the adaptive filter in Figure 9.26 with W(z), the Z-transform of the steady-
state error signal is
E(z) 5 D(z) 2 Y
⁄(z)
5 G(z)X(z) 2 F(z)W(z)X(z)
 
5 fG(z) 2 F(z)W(z)gX(z) 
 (9.8.2)
For the error to be zero for all inputs x(k), it is required that G(z) 2 F(z)W(z) 5 0 or
 
W(z) 5 F21(z)G(z) 
 (9.8.3)
 9.8
Optional material
Anti-noise
Figure 9.26: Active 
Control of Acoustic 
Noise Using an 
Adaptive Filter 
e(k)
G(z)
F(z)
d(k)
2
1
Adaptive 
filter
y(k)
x(k)
y(k)
*
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 9.8 Active Noise Control    715
On the surface, the expression for W(z) in (9.8.3) would appear to provide a simple 
solution to the problem of finding an optimal active noise control filter. Unfortunately, 
this solution is not a practical one. Recall that the secondary-path transfer function F(z) 
includes the air channel over which the sound travels from the speaker to the error micro-
phone. The propagation of sound through air introduces a delay that is proportional to 
the path length. Given the presence of a delay in F(z), the inverse of F(z) must have a cor-
responding advance, which means that F21(z) is not causal and therefore not physically 
realizable in real time.
To obtain a causal approximation to W(z), consider the error signal in the time 
domain. Recall that multiplication of Z-transforms in the frequency domain corresponds 
to convolution of the corresponding signals in the time domain. Consequently, from 
Figure 9.26
e(k) 5 d(k) 2 y⁄(k)
 5 d(k) 2 f(k) w y(k)
 
 5 d(k) 2 f(k) w fwT(k)u(k)g 
 (9.8.4)
Here f(k) is the impulse response of the secondary system F(z), and w denotes the linear 
convolution operation. Recall that u(k) is the (m 1 1) 3 1 vector of past inputs at time 
k, and w(k) is the (m 1 1) 3 1 weight vector at time k. The mean square error objective 
function is
 
(w) 5 E fe2(k)g 
 (9.8.5)
For the purpose of estimating the gradient vector of partial derivatives of (w) with 
respect to the elements of w, the LMS approximation, E fe2(k)g < e2(k), can be used. 
Combining this with (9.8.4) yields
=(w) < 2e(k)=e(k)
 
 5 22e(k)ff(k) w u(k)g 
 (9.8.6)
To simplify the final result, let x⁄(k) denote a filtered version of the input using the filter 
F(z). That is, X
⁄(z) 5 F(z)X(z) or
 
x⁄(k) 5 f(k) w x(k) 
 (9.8.7)
Similarly, let u⁄(k) denote the (m 1 1) 3 1 vector of filtered past inputs. That is,
 
u⁄(k) 5 fx⁄(k), x⁄(k 2 1), Á , x⁄ (k 2 m)gT 
 (9.8.8)
Combining (9.8.6) through (9.8.8), it then follows that the gradient of the mean square 
error can be expressed in terms of the filtered input as
 
=(w) < 22e(k)u⁄(k) 
 (9.8.9)
The LMS method uses the steepest descent method as a starting point. If  . 0 
denotes the step length, then the steepest descent method for updating the weights is
 
w(k 1 1) 5 w(k) 2 =fw(k)g 
 (9.8.10)
Substituting the approximation for the gradient from (9.8.9) into (9.8.10), this results in 
the following weight update algorithm called the filtered-x LMS method or simply the 
FXLMS method.
 
w(k 1 1) 5 w(k) 1 2e(k)u⁄(k), k $ 0  
 (9.8.11)
Filtered past inputs
FXLMS method
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

716    Chapter 9  Adaptive Signal Processing
Note that the only difference between the FXLMS method and the LMS method is 
that the vector of past inputs is first filtered by the secondary-path transfer function, hence 
the name filtered-x LMS method. In practice, an approximation to the secondary system 
F
⁄(z) < F(z) is used for the prefiltering of x(k) because an exact model of the secondary 
path is not available. A block diagram of the FXLMS method is shown in Figure 9.27.
9.8.2 Secondary-path identification
The key to implementing the FXLMS method in Figure 9.27 is the identification of a 
model for the secondary path F(z). To illustrate a secondary path, consider the air duct 
active noise control system shown in Figure 9.28. The secondary system represents the 
path traveled by the noise-canceling sound, including the loud speaker and the error 
microphone. It should be pointed out that there may be some feedback from the speaker 
to the reference microphone. For the purpose of this analysis, it is assumed that the feed-
back is negligible. One way to reduce the effects of feedback is to use a directional micro-
phone. Another way to eliminate feedback completely is to use a non-acoustic sensor 
in place of the reference microphone to produce a signal x(k) that is correlated with the 
primary noise. Alternatively, the effects of feedback can be taken into account using a 
feedback neutralization scheme (Warnaka et al., 1984).
The block diagram in Figure 9.28 is a high-level diagram that leaves many of the 
details implicit. A more detailed representation that shows the measurement scheme used 
to identify a model for the secondary path is shown in Figure 9.29.
Figure 9.27:  
The Filtered-x LMS 
Method 
1
2
x(k)
e(k)
G(z)
(k)
y(k)
d(k)
y(k)
W(z)
F(z)
LMS
F(z)
x(k)
Figure 9.28:  
Active Noise  
Control in an  
Air Duct
Blower
Reference
microphone
Error
microphone
Speaker
Controller
e(k)
y(k)
x(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 9.8 Active Noise Control    717
Active control of random broadband noise is a challenging problem. Fortunately, 
the noise that appears in practice often has a significant narrowband or periodic compo-
nent. For example, rotating machines generate harmonics whose fundamental frequency 
varies with the speed of rotation. Similarly, electrical transformers and overhead fluores-
cent lights emit harmonics with a fundamental frequency of F0 5 60 Hz. Let fs 5 1yT 
denote the sampling frequency. Then the primary noise can be modeled as follows.
 
x(k) 5 o
r
i50
ci cos(2iF0kT ) 1 bi sin(2iF0kT ) 1 v(k), 0 # k , N    (9.8.12)
Here r is the number of harmonics, and F0 is the frequency of the fundamental harmonic. 
Since fsy2 is the highest frequency that can be represented without aliasing, the number 
of harmonics is limited to r , fsy(2F0). The broadband term v(k) is additive white noise.
To determine the amount of noise cancellation achieved by active noise control, suppose 
the controller is not activated for the first Ny4 samples. The average power over the first Ny4 
samples of e(k) provides a base line relative to which noise cancellation can be measured.
 
Pu 5 4
N o
Ny421
k50
e2(k) 
 (9.8.13)
If the controller is activated at sample k 5 Ny4 with the weights updated as in (9.8.11), 
then the system will undergo a transient segment with the weights converging to their 
optimal values assuming the step size  is sufficiently small. If the adaptive filter has 
reached the steady state by sample 3Ny4, then the average power of the error achieved 
by the controller is
 
Pc 5 4
N o
N21
k53Ny4
e2(k) 
 (9.8.14)
Figure 9.29:  
Identification of 
Secondary-path 
Model, F
⁄(z)
x(k)
1
2
Adaptive 
filter
DAC
ADC
Anti-
imaging
Anti-
aliasing
Power
amplifier
Pre-
amplifier
Microphone
Speaker
e(k)
d(k)
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

718    Chapter 9  Adaptive Signal Processing
The noise reduction achieved by active noise control can be expressed in decibels as follows.
 
Eanc 5 10 log10 1
Pc
Pu2 dB  
 (9.8.15)
Noise reduction
Figure 9.30: Active Noise Control Using the FXLMS Method
0
500
1000
1500
2000
2500
0
10
20
30
40
50
60
70
80
90
k
e2(k)
Noise reduction 5 32.3 dB
FXLMS Method
EXAMPLE 9.14
To illustrate the use of the FXLMS to achieve active noise control, consider the air 
duct system shown in Figure 9.28. Suppose the primary path G(z) and secondary- 
path F(z) are each modeled using FIR filters of order n 5 20. For the purpose 
of this simulation, let the coefficients of the filters consist of random numbers 
uniformly distributed over the interval f21, 1g. Suppose the noise-corrupted peri-
odic input in (9.8.12) is used. Let the sampling frequency be fs 5 2000 Hz, and 
the fundamental frequency be F0 5 100 Hz. Suppose the number of harmonics 
is r 5 5, and the additive white noise v(k) is uniformly distributed over f2.5, .5g. 
Finally, suppose the coefficients for each harmonic are generated randomly in 
the interval f21, 1g thereby producing random amplitudes and phases for the r 
harmonics. The FXLMS method in Figure 9.29 can be applied to this system by 
running exam9_14. Here N 5 2400 samples are used with an adaptive filter of 
order m 5 40 and a step size of  5 .0001. A plot of the resulting squared error 
is shown in Figure 9.30. Note that the active noise control is activated at sample 
k 5 600. Once the transients have decayed to zero, it is evident that a signifi-
cant amount of noise reduction is achieved. The noise reduction measured using 
(9.8.15) is Eanc 5 32.3 dB.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 9.8 Active Noise Control    719
9.8.3 Signal-synthesis Method
An alternative approach to active noise control that is applicable to narrowband noise is 
the signal-synthesis method. Suppose the input or reference signal is a noise-corrupted 
periodic signal as in (9.8.12). If the frequency of the fundamental harmonic, F0, is known 
or can be measured, then a more direct signal-synthesis approach can be used as shown 
in Figure 9.31. For notational convenience, define
 
0 5
D  2F0T 
 (9.8.16)
Since the primary- and secondary-path models are assumed to be linear, the form of 
the control signal required to cancel the periodic component of the noise is
 
y(k) 5 o
r
i51
pi(k) cos(ik0) 1 qi(k) sin(ik0) 
 (9.8.17)
Next suppose the secondary path in Figure 9.27 is modeled with an FIR filter of order m 
with coefficient vector f . That is,
 
F(z) 5 o
m
i50
fiz2i 
 (9.8.18)
To obtain a concise formulation of the secondary noise signal, let g(k) denote the vector 
of past control signals y(k).
 
g(k) 5
D  fy(k), y(k 2 1), Á , y(k 2 m)gT 
 (9.8.19)
It then follows that the secondary noise signal at time k can be expressed with a dot 
product as
 
y⁄(k) 5 f Tg(k) 
 (9.8.20)
The objective is to choose values for the coefficients p(k) and q(k) in (9.8.17) so 
as to minimize the mean square error (p, q) 5 E fe2(k)g. As with the LMS method, for 
the purpose of estimating the gradient one can approximate the mean square error with 
E fe2(k)g < e2(k). Using (9.8.17) through (9.8.20), the partial derivative of the mean 
square error with respect to the ith element of the coefficient vector p is
−(p, q)
−pi
< 2e(k)−e(k)
−pi
5 22e(k)−f Tg(k)
−pi
5 22e(k)o
m
j51
fj
−y(k 2 j)
−pi
 
5 22e(k)o
m
j51
fj cosfi(k 2 j)0g 
 (9.8.21)
Figure 9.31: Active Noise Control by the Signal-synthesis Method
e(k)
G(z)
F(z)
F0
d(k)
2
1
y(k)
x(k)
Signal
analysis
Signal
synthesis
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

720    Chapter 9  Adaptive Signal Processing
Similarly, the partial derivative of the mean square error with respect to the ith element 
of the coefficient vector q is
 
−(p, q)
−qi
< 22e(k)o
m
j51
fj sin fi(k 2 j)0g 
 (9.8.22)
To simplify the final result, let P(k) and Q(k) be r 3 1 vectors of intermediate variables 
defined as follows.
 
 Pi(k) 5
D  o
m
j50
fj cosfi(k 2 j)0g 
 (9.8.23a)
 
 Qi(k) 5
D  o
m
j50
fj sinfi(k 2 j)0g  
 (9.8.23b)
It then follows that the partial derivatives of the mean square error with respect the ele-
ments of p and q are
 
−(p, q)
−pi
< 22e(k)Pi(k) 
 (9.8.24a)
 
−(p, q)
−qi
< 22e(k)Qi(k) 
 (9.8.24b)
One can now use the steepest descent method to update the coefficients of the  
secondary-sound control signal. Let  . 0 denote the step size. Using (9.8.24), this results 
in the following signal-synthesis active noise control method.
 
3
p(k 1 1)
q(k 1 1)4 53
p(k)
q(k)4 1 2e(k)3
P(k)
Q(k)4, k $ 0  
 (9.8.25)
One feature that sets the signal-synthesis method apart from the FXLMS method is 
that it is typically a lower dimensional method. The signal-synthesis method has a weight 
vector of dimension n 5 2r where r is the number of harmonics in the periodic compo-
nent of the primary noise. Since the maximum number of harmonics that can be accom-
modated with a sampling frequency of fs is r , fsy(2F0), this means that the maximum 
dimension of the signal-synthesis method is n , fsyF0, which is often relatively small.
Although the dimension of the signal synthesis method is small, it is apparent from 
(9.8.23) that there is considerable computational effort required to compute the inter-
mediate variables P(k) and Q(k). At each time step, a total of 2r(m 1 1) floating-point 
multiplications or FLOPs are required. Note that the trigonometric function evaluations 
can be precomputed and stored in a look-up table. Since the 2r(m 1 1) FLOPs must be 
performed in less than T seconds, this could limit the sampling frequency fs. Further-
more, for large values of m the computations in (9.8.23) could exhibit significant accu-
mulated round-off error. Fortunately, these limitations can be minimized by developing 
a recursive formulation for P(k) and Q(k). Using the cosine of the sum trigonometric 
identity from Appendix 2,
 Pi(k) 5 o
m
j50
fj cosfi(k 2 j 2 1 1 1)0g
 5 o
m
j50
fj cosfi(k 2 1 2 j)0g cos(i0) 2  sinfi(k 2 1 2 j)0g sin(i0)
 
 5  cos(i0)Pi(k 2 1) 2  sin(i0)Qi(k 2 1)
  (9.8.26)
Signal-synthesis 
method
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 9.8 Active Noise Control    721
Similarly, using the sine of the sum trigonometric identity from Appendix 2, one can 
show that Q(k) can be expressed recursively as
 
Qi(k) 5 sin(i0)Pi(k 2 1) 1  cos(i0)Qi(k 2 1) 
 (9.8.27)
The recursive update formulas for P(k) and Q(k) can be expressed compactly in vector 
form as follows.
 
3
Pi(k)
Qi(k)4 53
 cos(i0)
2  sin(i0)
 sin(i0)
 cos(i0) 43
Pi(k 2 1)
Qi(k 2 1)4, 1 # i # r  
 (9.8.28)
Note that the number of FLOPs per iteration has been reduced from 2r(m 1 1) to 4r. 
Thus the computational effort of the recursive implementation is independent of the size 
of the filter used to model F(z). To start the recursive computation of P(k) and Q(k), 
initial values must be used. From (9.8.23), the appropriate starting values are
 
 Pi(0) 5 o
m
j50
fj cos(ij0)  
 (9.8.29a)
 
 Qi(0) 5 2o
m
j50
fj sin(ij0) 
 (9.8.29b)
The signal-synthesis method is based on the key assumption that the fundamental 
frequency of the periodic component of the primary noise is known or can be measured. 
Examples of a applications where F0 is known include the noise from electrical trans-
formers and from overhead fluorescent lights, where F0 5 60 Hz is a parameter that is 
regulated carefully by the power industry. In those applications where F0 is not known, it 
might be measured as shown by the signal analysis block in Figure 9.30. For example, if 
the primary noise is produced by a rotating machine or motor, then a tachometer might 
be used to measure F0. Another approach is to lock onto the periodic component of 
the noise using a phase-locked loop (Schilling et al., 1998). The virtue of measuring the 
fundamental frequency is that the signal-synthesis method then can track changes in the 
period of the periodic component of the primary noise.
Signal-synthesis Method
EXAMPLE 9.15
As an illustration of the signal-synthesis method, consider the air duct system 
shown in Figure 9.28. To facilitate comparison with the FXLMS method, sup-
pose the input and system parameters are the same as in Example 9.14. The sig-
nal-synthesis method in Figure 9.31 can be applied to this system by running 
exam9_15. Again N 5 2400 samples are used, but this time the selected step size is 
 5 .001. A plot of the resulting squared error is shown in Figure 9.32. Note that 
the active noise control is activated at sample k 5 600. Once the transients have 
decayed to zero, it is apparent a significant amount of noise reduction is achieved. 
The noise reduction measured using (9.8.15) is Eanc 5 32.6 dB. In this case the 
final estimates for the control signal coefficients were as follows.
p 5 f2.0892, 2.1159, 1.0692, 2.9898, 2.1797gT
q 5 f2.1065, .1826, 1.3938, 21.6894, 2.3873gT
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

722    Chapter 9  Adaptive Signal Processing
0
200
300
300
100
100
400
500
500
600
700
700
800
900
900
1000
0
500
1000
1500
(a)
f (Hz)
0
200
400
600
800
1000
0
500
1000
1500
(b)
f (Hz)
Ac(f)
Au(f)
Figure 9.33: Magnitude Spectra of Error (a) without Active Noise Control and 
(b) with Active Noise Control Using the Signal-synthesis Method 
Figure 9.32: Active Noise Control Using the Signal-synthesis Method
0
500
1000
1500
2000
2500
0
10
20
30
40
50
60
70
80
90
k
e2(k)
Noise reduction 5 32.6 dB
Another way to look at the performance of an active noise control system is to exam-
ine the magnitude spectrum of the error signal with and without the noise control 
activated. The results are shown in Figure 9.33. The five harmonics are clearly evident 
in Figure 9.33a, which corresponds to the first Ny4 samples before the active noise con-
trol is activated. The harmonics effectively are eliminated in Figure 9.33b, which cor-
responds to the last Ny4 samples after the noise controller has reached a steady state.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 9.9 Adaptive Function Approximation     723
Adaptive Function Approximation 
The LMS method can be used to identify linear discrete-time systems from input- 
output measurements. Nonlinear discrete-time systems have a present output y(k) that is 
a nonlinear function of past inputs and outputs. In order to generalize the LMS approach 
to the identification of nonlinear discrete-time systems, we first focus on the problem of 
approximating a nonlinear function of several variables using adaptive techniques.
 9.9
Optional material
The DSP Companion contains the following functions for implementing active noise 
control.
% F_FXLMS:  Active noise control using the filtered-x LMS method
% F_SIGSYN: Active noise control using the signal-synthesis method
%
% Usage:
%  
[w,e]  = f_fxlms  (x,g,f,m,mu,w)
%  
[p,q,e]  = f_sigsyn (x,g,f,f_0,f_s,r,mu)
% Pre:
%  
x  
= N by 1 vector containing input samples
%  
g  
= n by 1 vector containing coefficients of
%  
 
 the primary system. The desired output
%  
 
 is D(z) = G(z)X(z)
%  
f  
= n by 1 vector containing coefficients of
%  
 
 the secondary system.
%  
m  
= order of transversal filter (m >= 0)
%  
mu  = step size to use for updating w
%  
w  
= an optional (m+1) by 1 vector containing
%  
 
 the initial values of the weights. Default:
%  
 
 w = 0
%  
f_0  = fundamental frequency of periodic component
%  
 
 of the input in Hz
%  
f_s  = sampling frequency in Hz
%  
r  
= number of harmonics of x(k) it is desired
%  
 
 to cancel (1 to f_s/(2*f_0))
%  
mu  = step size to use for updating p and q
% Post:
%  
w  
= (m+1) by 1 weight vector of filter
%  
 
 coefficients
%  
e  
= an optional N by 1 vector of errors
%  
p  
= r by 1 vector of cosine coefficients
%  
q  
= r by 1 vector of sine coefficients
% Notes:
%  
Typically mu << 1/[(m+1)*P_x’] where P_x’ is the
%  
average power of filtered input X’(z) = F(z)X(z).
DSP Companion
DSP Companion
*
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

724    Chapter 9  Adaptive Signal Processing
9.9.1 Nonlinear Functions
Suppose f  is a real-valued function of p variables u1, u2, Á , up. Let f :R p S R be defined 
on a domain U , Rp.
 
y 5 f(u), u [ U 
 (9.9.1)
We assume that f  is sufficiently smooth to be continuously differentiable. Suppose the 
domain U is the following compact subset of Rp where ui is constrained to lie in the inter-
val fai, big for 1 # i # p.
 
U 5 fa1, b1g 3 fa2, b2g 3 Á 3 fap, bpg 
 (9.9.2)
One approach to approximating the function f :U S R is to overlay the domain U 
with a uniform grid of points and then develop a local representation of f  valid over each 
grid element. To that end, suppose there are d grid points equally spaced along each of 
the p dimensions of U. Then the total number of grid points is
 
r 5 d p 
 (9.9.3)
Let Dui denote the grid-point spacing along dimensions i. Then
 
Dui 5
D  bi 2 ai
d 2 1 , 1 # i # p 
 (9.9.4)
As an illustration, suppose p 5 2 and the grid point precision is d 5 6. Then the grid 
consists of r 5 36 grid points, as shown in Figure 9.34.
It is clear from (9.9.3) that as the number of dimensions, p, and the number of grid 
points per dimension, d, grow the total number of grid points, r, can be become very 
large. There are two distinct ways to order this large number of grid points. First con-
sider the use of a vector q [ f0, d 2 1gp. Here q is a vector whose elements are integers in 
the range 0 to d 2 1. Element qi is the integer coordinate of grid point u(q) along the ith 
dimension. That is, if u(q) denotes the qth grid point, then
 
u(q) 53
a1 1 q1Du1
a2 1 q2Du2
o
ap 1 qpDup4
 
 (9.9.5)
Grid-point spacing
Figure 9.34:  
Grid Points over 
the Domain U with 
p 5 2 and d 5 6
0
b2
b1a1
a2
Du2
Du1
u1
u2
1
2
3
4
5
0
1
2
3
4
5
u
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 9.9 Adaptive Function Approximation     725
Here q can be thought of as vector subscript whose integer elements specify the normal-
ized displacement of the point u(q) along each of the p dimensions. The virtue of the this 
vector subscript approach is that for an arbitrary u [ U, it is easy to identify the vertices 
of the grid element that contains u. For example, the vector subscript of the base vertex 
of the grid element containing point u is
 
 q(u) 5
 floor 53
(u1 2 a1)yDu1
(u2 2 a2)yDu2
o
(up 2 ap)yDup46
 
 (9.9.6a)
 
q(u) 5
 clip fq(u), 0, d 2 2g
 
 (9.9.6b)
The clipping of the vector subscript to the interval f0, d 2 2g in (9.9.6b) is included in 
order to account for the possibility that u may lie on or outside the boundary of U. When 
clipping occurs, the base vertex of the grid element closest to u is obtained.
Once the subscript of the base vertex is found, the subscripts of the other vertices 
are easily determined by adding combinations of 0 and 1 to the elements of q(u). For 
example, let bk [ Rp denote a binary vector whose elements represent the decimal value 
k. Then the vector subscript of the kth vertex of the grid element containing the point u 
is computed as follows.
 
qk 5 q(u) 1 bk, 0 # k , 2p 
 (9.9.7)
The following example illustrates how to find the vertices of a grid element contain-
ing an arbitrary vector u.
Vector subscript
Grid element vertices
grid Element
EXAMPLE 9.16
Consider the case p 5 2. Suppose the domain U is as follows.
U 5 f210, 10g 3 f25, 5g
Let the grid-point precision be d 5 6. From (9.9.4), the grid-point spacings are
Du 53
b1 2 a1
d 2 1
b2 2 a2
d 2 14
53
4
24
Suppose u 5 f3.2, 2.4gT represents an arbitrary point in the domain U. Then 
from (9.9.6) the vector subscript of the base (lower-left) vertex of the grid element 
containing u is
q(u) 5
 floor 53
(u1 2 a1)yDu1
(u2 2 a2)yDu246 5
 floor 53
13.2y4
4.6y246 53
3
24
3
3
(Continued)
u1
u2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

726    Chapter 9  Adaptive Signal Processing
9.9.2 Radial Basis Functions (RBFs)
Our overall objective is to approximate the function f  on the domain U. To do this, it 
is helpful to consider a second way to order the r grid points, this time using a scalar 
subscript. For grid point u(q), the scalar subscript i can be computed from the vector 
subscript q as follows.
 
i 5 q1 1 q2d 1Á1 qpd p21 
 (9.9.8)
As the integer elements of q range from 0 to d 2 1, the value of i ranges from 0 to r 2 1, 
where r is as in (9.9.3). The computation in (9.9.8) can be seen to be a base d-to-decimal 
conversion of q. Consequently, a decimal-to-base d conversion of i can be used to recover 
q from i. MATLAB functions base2dec and dec2base are available to perform these con-
versions. The relationship between the vector subscript q and the scalar subscript i of the 
grid points is summarized in Figure 9.35.
Using the scalar subscript i, the r grid points ui have a natural ordering from 0 to 
r 2 1. Given this one-dimensional ordering of the grid points, consider the following 
structure for approximating the function f .
 
f0(u) 5
D  wTg(u) 
 (9.9.9)
Here w 5 fw0, Á , wr21gT is an r 3 1 weight vector. The function g:Rp S Rr represents an 
r 3 1 vector of functions called radial basis functions with the ith radial basis function 
being centered at grid point ui. A radial basis function, or RBF, is a continuous function 
gi:Rp S R such that
 
gi(ui) 5 1 
 (9.9.10a)
 
ugi(u)u S 0 as uuu 2 uiuu S ` 
 (9.9.10b)
Scalar subscript
Radial basis function
Figure 9.35:  
Transformations 
between Vector 
and Scalar 
Subscripts of Grid 
Points
Base-d to decimal
Decimal to base-d
Vector subscript
q e Rp
Scalar subscript
i e R
There are a total of 2p 5 4 vertices per grid element. From (9.9.7), the subscripts 
of the vertices of the grid element containing u are
q0 5 q(u) 1 f0, 0gT 5 f3, 2gT
q1 5 q(u) 1 f0, 1gT 5 f3, 3gT
q2 5 q(u) 1 f1, 0gT 5 f4, 2gT
q3 5 q(u) 1 f1, 1gT 5 f4, 3gT
The point u is shown in Figure 9.34. Inspection confirms that hq0, q1, q2, q3j do 
indeed specify the vertices of the grid element containing u.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 9.9 Adaptive Function Approximation     727
Thus an RBF is one at its center point, and its value approaches zero as the distance 
from its center point increases. A popular example of a radial basis function is the 
Gaussian RBF.
 
gi(u) 5  exp3
(u 2 ui)T(u 2 ui)
22
4 
 (9.9.11)
Here the variance, 2, controls the rate at which the RBF goes to zero as the distance 
from the center point increases. The Gaussian RBF has a number of useful properties, 
not the least of which is the observation that it has an infinite number of derivatives, all 
of which are continuous. However, the Gaussian RBF also has some drawbacks. Since 
gi(u) . 0 for all u, it follows that if  is too large, there will be many terms in (9.9.9) that 
contribute to f0(u). This defeats the local nature of the representation which, ideally, has 
only a few terms contributing to f0(u). On the other hand, if  is too small then the value 
of gi(u) will be near zero midway between the grid points, which reduces the effectiveness 
of gi in approximating f .
There are a number of potential candidates for radial basis functions (Webb and 
Shannon, 1998). As an alternative to the Gaussian RBF, consider the following one- 
dimensional raised-cosine RBF centered about z 5 0 (Schilling et al., 2001).
 
(z) 55
1 1  cos (z)
2
,
uzu # 1
0,
uzu . 1
 
 (9.9.12)
Note that (z) satisfies the two basic properties in (9.9.10). A plot comparing the 
one-dimensional Gaussian RBF and the one-dimensional raised-cosine RBF is shown 
in Figure 9.36 for the case  5 1. Observe that the raised-cosine RBF is a continuously 
Gaussian RBF
Raised-cosine RBF
Figure 9.36:  
Comparison of 
One-dimensional 
Gaussian and 
Raised-cosine RBFs 
22
21.5
21
20.5
0
0.5
1
1.5
2
20.2
0
0.2
0.4
0.6
0.8
1.2
1
z
G(z)
Compact support
Gaussian
Raised-cosine
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

728    Chapter 9  Adaptive Signal Processing
differentiable function. Furthermore, the set of z over which the raised-cosine RBF 
is nonzero is contained in the compact (closed and bounded) set S 5 f21, 1g. That is, 
the raised-cosine RBF has compact support. This is in contrast to the Gaussian RBF, 
which does not have compact support. The compact support property ensures that a 
representation based on a sum of RBFs will be a local representation with only a few 
of the terms contributing to f0(u).
The raised-cosine RBF can be generalized from one dimension to p dimensions by 
forming a product of scalar RBFs, one for each dimension. The scale factors Dui in (9.9.4) 
can be used to convert the normalized scaler RBF in (9.9.12) into a scalar RBF that 
takes into account the grid-point spacing. This results in the following p-dimensional 
raised-cosine RBF centered at ui. Here the notation Π denotes the product.
 
gi(u) 5P
p
k51
 1
uk 2 ui
k
Duk 2  
 (9.9.13)
9.9.3 Raised-cosine RBF Networks
When the raised-cosine radial basis functions in (9.9.13) are used in the approximation 
to f  in (9.9.9), the resulting representation will have output y0 and will be referred to as 
a raised-cosine RBF network.
 
y0 5 wTg(u)  
 (9.9.14)
Raised-cosine RBF networks are analogous to artificial neural networks in the sense 
that they both can be used to approximate general nonlinear functions. For neural net-
works, the nonlinearity is the activation function (Zurada, 1992).
Compact support
Raised-cosine RBF 
network
Raised-cosine RBF
EXAMPLE 9.17
Suppose p 5 2 and the domain U over which f  is defined is
U 5 f24, 4g 3 f22, 2g
If the grid point precision is d 5 3, then the grid-point spacing is Du 5 f4, 2gT. 
There are a total of r 5 9 grid points with an RBF centered about each one. A 
plot of the RBF centered at the origin can be obtained by running exam9_17 with 
the result shown in Figure 9.37. The continuous differentiability and compact 
support characteristics of this two-dimensional raised-cosine RBF are evident 
from inspection.
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 9.9 Adaptive Function Approximation     729
Raised-cosine RBFs have a number of interesting and useful properties. To start with, 
they are continuously differentiable functions of u which means that the approximation 
f0(u) in (9.9.9) is continuously differentiable. The compact support characteristic, evident 
in Figures 9.36 and 9.37, means that the RBFs do not interact with one another at the grid 
points. This leads to the following important orthogonality property on the grid points.
 
gk(ui) 55
1,
i 5 k
0,
i Þ k 
 (9.9.15)
The orthogonality property suggests an easy way to select the r 3 1 vector of weights 
w. Suppose wk 5 f(uk) for 0 # k , r. Using the orthogonality property, all of the terms in 
f0(ui) are zero except the ith term, which is wi. Therefore the approximation f0(u) in (9.9.9) 
is exact at the r grid points.
 
f0(ui) 5 f (ui), 0 # i , r 
 (9.9.16)
If the grid point precision, d, is sufficiently large, then the weights wi 5 f(ui) will produce 
an effective approximation of the function f .
The orthogonality property is based on a lack of interaction between RBFs at the grid 
points. Another noteworthy and curious property of the raised-cosine RBF occurs between 
the grid points. Suppose the weights in (9.9.14) are all set to unity. It can be shown that the 
resulting raised-cosine RBF network has the following property (Schilling et al., 2001).
 
o
r21
i50
gi(u) 5 1, u [ U 
 (9.9.17)
For convenience, (9.9.17) will be referred to as the constant interpolation property. It says 
that if the RBFs are equally weighted, then the surface that they produce when their 
contributions are combined is perfectly flat over the domain U. This property, which 
is not shared by Gaussian RBFs, is useful in those instances where the function being 
approximated is flat over part of its domain. This might occur, for example, if f  includes 
saturation or dead-zone effects.
Orthogonal property
Constant interpolation 
property
Figure 9.37: A Two-dimensional Raised-cosine RBF Centered at u 5 0 
26
24
22
0
2
4
6
22
0
2
0
0.2
0.4
0.6
0.8
1
u1
u2
g(u)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

730    Chapter 9  Adaptive Signal Processing
Perhaps the most fundamental property of raised-cosine RBF network lies in its abil-
ity to accurately approximate the function f . Recall that both f  and f0 are continuous, 
and the domain U over which they are defined is compact. As a consequence, one can 
define the error in the approximation as follows.
 
E0(d) 5
D   max 
u [U hu f0(u) 2 f(u)uj 
 (9.9.18)
Using the fact that f  is assumed to be continuously differentiable, one can show  
(Problem 9.25) that E0(d) S 0 as d S `. That is, the approximation f0(u) converges  
uniformly to f(u) on U. If f  is only continuous, but not continuously differentiable, then 
convergence still occurs, but it is pointwise rather than uniform over U.
Although the raised-cosine RBF network approximation is accurate and not difficult 
to implement, it does suffer from the drawback that the number of terms r can become 
very large. Fortunately, for each u [ U almost all of these terms are zero. Only the local 
terms in the neighborhood of u contribute to f0(u). Indeed, because each raised-cosine 
Approximation error
Figure 9.38: Constant Interpolation Produced by Four Equally Weighted 
Raised-cosine Radial Basis Functions
24
22
0
2
4
24
22
0
2
4
0
0.5
1
1.5
u1
u2
f0(u)
Constant interpolation Property
EXAMPLE 9.18
As an illustration of the constant interpolation property, suppose p 5 2 and the 
domain U over which f  is defined is
U 5 f21, 1g 3 f21, 1g
Let the grid-point precision be d 5 2. Then the grid-point spacing is Du 5 f2, 2gT. 
In this case there are r 5 4 grid points located at the corners of the 2 3 2 square 
U. Suppose the weight vector is w 5 f1, 1, 1, 1gT. A plot of the resulting interpo-
lated surface f0(u), obtained by running exam9_18, is shown in Figure 9.38. It is 
apparent that on the domain U the interpolated surface is flat.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 9.9 Adaptive Function Approximation     731
RBF has compact support and goes to zero at the adjacent grid points, the only terms 
contributing to f0(u) are the terms corresponding to the vertices of the grid element con-
taining u. Consequently, for each u, the number of nonzero terms in f0(u) is at most
 
K 5 2p 
 (9.9.19)
The number of nonzero terms, K, is typically much smaller than r. Equally important, 
it is independent of the grid-point precision d. Recall that the vertices of the local grid 
element containing u can be found using (9.9.6) and (9.9.7). This is the basis for the fol-
lowing highly efficient algorithm for evaluating the RBF network output.
1. Set y0 5 0. Compute q(u) using (9.9.6).
2. For k 5 0 to K 2 1 do
{
(a) Compute qk using (9.9.7) 
(b) Convert qk to i using (9.9.8). 
(c) Compute grid point ui 5 u(qk) using (9.9.5). 
(d) Compute y0 5 y0 1 wigi(u) using (9.9.13) 
}
The difference in speed between Algorithm 9.2 and a brute-force evaluation of 
(9.9.14) can be dramatic. For example, suppose p 5 6 and d 5 10. This yields a total of 
r 5 106 potential terms to evaluate. However, using Algorithm 9.2 requires the evaluation 
of only K 5 64 terms. Thus Algorithm 9.2 is faster than a direct evaluation by a factor 
of ryM 5 15625, or more than four orders of magnitude! Note that the execution speed 
stays the same, regardless of the grid point precision d.
We summarize the essential characteristics of the raised-cosine RBF network as 
follows.
The raised-cosine RBF network approximation in (9.9.9) has the following properties.
(a) The function f0(u) is continuously differentiable.
(b) If wi 5 f(ui) for 0 # i , r, then the approximation f0(u) is exact at the r grid 
points.
(c) The storage requirements for f0(u) are r 5 d p memory locations.
(d) For each u [ U, at most K 5 2p terms are nonzero. The evaluation speed of 
f0(u) is independent of the grid point precision d.
(e)  The approximation f0(u) converges uniformly to f(u) on the domain U. That 
is, E0(d) S 0 as d S `.
Next consider the problem of updating the weights of the raised-cosine RBF net-
work in order to adaptively approximate f(u). Let u(k) for 0 # k , M be a set of random 
vectors uniformly distributed over the domain U.
 
U
 train 5 hu(0), u(1), Á , u(M 2 1)j 
 (9.9.20)
The set U
 train , U is called a training set. The training set is used to adjust the weights 
w in order to train f0(u) to approximate f(u). At iteration k, the error between the output 
of the function f  in (9.9.1) and the output of the function f0 in (9.9.14) is
 
e(k) 5 y(k) 2 y0(k) 
 (9.9.21)
Independent
ALgoRiTHM
9.2 Raised-cosine RBF 
Network Evaluation
PRoPoSiTioN
9.2 Raised-cosine RBF 
Network 
Training set
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

732    Chapter 9  Adaptive Signal Processing
The objective is to minimize the mean square error (w) 5 E fe2(k)g. For the purpose 
of estimating the gradient, the fundamental LMS assumption, E fe2(k)g < e2(k), is used. 
Then from (9.9.21) the partial derivative of (w) with respect to the ith element of w is
−(w)
−wi
< 2e(k)−e(k)
−wi
5 22e(k)−wTgfu(k)g
−wi
 
5 22e(k)gifu(k)g, 0 # i , r 
 (9.9.22)
Thus the gradient vector is =(w) < 22e(k)gfu(k)g. Using this gradient estimate in the 
steepest-descent method then results in the following update formula for the raised- 
cosine RBF network weights called a nonlinear least mean square (NLMS) method. Here 
 . 0 is the step size.
 
w(k 1 1) 5 w(k) 1 2e(k)gfu(k)g, k $ 0  
 (9.9.23)
Observe that the weight update algorithm for the NLMS method differs from the linear 
LMS method only in that u(k) has been replaced by the nonlinear function gfu(k)g.
To test the accuracy of the approximation f0(u), it is important to validate it using an 
independent test set distinct from the training set used in (9.9.23). For example, suppose 
a total of L vectors v(k) [ U are generated for this purpose. For example, the v(k) might 
be random vectors uniformly distributed over U.
 
U
 test 5 hv(0), v(1), Á , v(L 2 1)j 
 (9.9.24)
If e(k) 5 y(k) 2 y0(k) denotes the error for test sample v(k), then the percent error of the 
approximation is
 
E(L) 5
D  1001
iei
iyi2 % 
 (9.9.25)
Note that when the raised-cosine RBF network output is y0(k) 5 0, the percent error is 
E(L) 5 100%. Values of E(L) V 100% represent a good fit between the system f(u) and 
f0(u), with E(L) 5 0% being a perfect fit.
NLMS method
Test set
Percent error
Adaptive Function Approximation
EXAMPLE 9.19
To illustrate adaptive function approximation with a raised-cosine RBF network, 
we again use p 5 2 because this makes the results easier to visualize. A challeng-
ing example of a nonlinear function of two variables is the MATLAB “peaks” 
function displayed by entering the peaks command.
 f(u) 5 3(1 2 u1)2 exp(2u2
1) 2 (u2 1 1)2
210(u1y5 2 u3
1 2 u5
2) exp(2u2
1 2 u2
2)
2(1y3) exp(2(u1 1 1)2 2 u2
2)
 U 5 f23, 3g 3 f23, 3g
Let the grid-point precision be d 5 41, which yields r 5 1681 terms in the 
RBF approximation. Suppose the step size is  5 .05 and the initial guess for the 
weights is w 5 0. Finally, let the number of training set samples be M 5 105 and 
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 9.9 Adaptive Function Approximation     733
E(1000) 5 2.41% 
23
22
21
0
1
2
3
22
0
2
26
24
22
0
2
4
6
8
u1
u2
f0(u)
Figure 9.39: Approximation of the MATLAB peaks Function Using a Raised-cosine 
RBF Network with d 5 41
0
2
3
1
4
5
6
7
8
9
10
3104
0
10
20
30
40
50
60
70
p 5 2, d 5 41, r 5 1681, E(1000) 5 2.41%
k
e2(k)
Figure 9.40: Learning Curve for Example 9_19 Using a Step Size of  5 .05 
the number of test set samples be L 5 103. The results of running exam9_19 are 
shown in Figure 9.39. Note that error is E(L) 5 2.41%, indicating a reasonable fit. 
The corresponding learning curve is shown in Figure 9.40, where it is evident that 
the approximation f0 has converged. Interestingly enough, if the weights are set 
to wi(0) 5 f(ui) for 0 # i , r as in Proposition 9.2, the resulting error starts out at 
E(L) 5 2.59%, and after training it becomes E(L) 5 2.31%.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

734    Chapter 9  Adaptive Signal Processing
The DSP Companion contains the following functions for implementing adaptive 
function approximation using raised-cosine RBF networks.
% F_RBFW:  Approximate a nonlinear function with a raised-cosine
% 
RBF network
% F_RBFY0: Compute output of an RBF network
%
% Usage:
% 
[w,e] = f_rbfw (@f,U,a,b,d,mu,ic,w);
% 
y0  
= f_rbfy0 (x,w,a,b,d,u);
% Pre:
% 
 f  
=  name of user-supplied function that returns the 
% 
 
 value of the nonlinear function to be approxi-
% 
 
 mated. % Here u is a p by 1 vector and
%
% 
 
 y = f(u)
%
% 
 U  
=  p by M matrix containing the M training samples
% 
 
 u(k) in the columns of U.
% 
a  
= p by 1 vector of lower bounds on u
% 
b  
= p by 1 vector of upper bounds on u
% 
d  
= number of grid points per dimension (d >= 2)
% 
mu  
= step length for gradient search
% 
ic  
= an initial condition code. If ic <> 0, compute the
% 
 
  initial weights to ensure that the network is
% 
 
 exact at the grid points.
% 
 w  
=  r by 1 vector containing the initial value 
% 
 
 (f_rbfw) or the value (f_rbf0) of the weights.
% 
 
 Default: w = 0. Here r  = d^p.
% 
x  
= N by 1 vector of inputs
% 
u  
= p by 1 initial state vector
% Post:
% 
w  
= r by 1 weight vector
% 
y0  
= output of RBF network, y0 = w’g(u)
DSP Companion
DSP Companion
Nonlinear System identification (NLMS)
The ability to approximate a nonlinear function of several variables using a parame-
terized RBF structure has many potential applications in science and engineering. In 
this section we apply adaptive RBF networks to identity nonlinear discrete-time systems 
using the nonlinear least mean square (NLMS) method from (9.9.23). A linear discrete-
time ARMA system can be generalized to a nonlinear ARMA system in the following 
way.
 
y(k) 5 f fx(k), Á , x(k 2 m), y(k 2 1), Á , y(k 2 n)g, k $ 0  
 (9.10.1)
 9.10
Optional material
Nonlinear ARMA 
system
*
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 9.10 Nonlinear System Identification (NLMS)    735
Here f  is a real-valued function that is assumed to be continuous, x(k) is the system input 
at time k, and y(k) is the system output at time k. Thus the present output, y(k), depends 
on the past inputs and the past outputs in some nonlinear fashion. For convenience, the 
nonlinear discrete-time system in (9.10.1) will be referred to as the system Sf. A block 
diagram of the system Sf is shown in Figure 9.41.
A more compact formulation of Sf can be obtained by introducing the following 
generalization of the vector of past inputs called the state vector.
 
u(k) 5
D  fx(k), Á , x(k 2 m), y(k 2 1), Á , y(k 2 n)gT 
 (9.10.2)
Thus the state vector is a vector containing the past inputs and the past outputs. The 
number of elements, p, in the state vector is called the dimension of the system.
 
p 5 m 1 n 1 1 
 (9.10.3)
Given the p 3 1 state vector u(k), the output of the nonlinear system Sf at time k is simply
 
y(k) 5 f fu(k)g, k $ 0 
 (9.10.4)
Suppose the nonlinear system Sf is BIBO stable. Recall from Chapter 2 that a system 
is BIBO stable if and only if every bounded input x(k) produces a bounded output y(k). 
Let xm and xM be lower and upper bounds, respectively, on the input x(k).
 
xm # x(k) # xM 
 (9.10.5)
Typically, the input bounds are selected by the user, and then the output bounds are 
estimated experimentally from measurements. Suppose x is an L 3 1 white noise input 
uniformly distributed over fxm, xMg where L W 1. Let y(k) be the resulting output of Sf. 
Then output bounds can be set as follows, where y $ 0.
 
 ym 5 min 
L21
k50 y(k) 2 y  
 (9.10.6a)
 
 yM 5 max 
L21
k50 y(k) 1 y 
 (9.10.6b)
Here the safety margin y $ 0 is used because this takes into account the fact that x(k) is 
of finite length. Furthermore, it is prudent to use y . 0 because the RBF model output, 
State vector
Dimension
Nonlinear system Sf
Figure 9.41:  
Block Diagram of a 
Nonlinear System 
Sf of Dimension 
p 5 m 1 n 1 1
x(k)
y(k)
ƒ
???
???
z21
z21
z21
z21
y(k 2 1)
x(k 2 1)
y(k 2 n)
x(k 2 m)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

736    Chapter 9  Adaptive Signal Processing
y1(k), may range outside of the interval f ym, yMg given that y1(k) is only an approximation 
to y(k). Of course making y too large effectively reduces the precision of the RBF model 
by making the grid-point spacing Dy large.
If the input is constrained as in (9.10.5), then the domain of the continuous function 
f  is restricted to the following compact subset of Rp.
 
U 5 fxm, xMgm11 3 fym, yMgn 
 (9.10.7)
Suppose the raised-cosine RBF network, f0(u), discussed in Section 9.9 is used to 
approximate f(u). The resulting nonlinear discrete-time system will be referred to as the 
RBF model S1 with output y1(k).
 
y1(k) 5 f0fu(k)g, k $ 0 
 (9.10.8)
For the RBF model, the vectors of lower bounds a [ Rp and upper bounds b [ Rp 
on u(k) are as follows.
 
ai 55
xm,
1 # i # m 1 1
ym,
m 1 2 # i # p 
 (9.10.9)
 
bi 55
xM,
1 # i # m 1 1
yM,
m 1 2 # i # p 
 (9.10.10)
If d $ 2 is the grid-point precision, then the grid-point spacings for the input x(k) 
and the output y(k) are
 
Dx 5 xM 2 xm
d 2 1  
 (9.10.11a)
 
Dy 5 yM 2 ym
d 2 1  
 (9.10.11b)
Before considering an example of an RBF model of a nonlinear discrete-time system, 
it is useful to briefly consider some practical issues, starting with the initial conditions. 
One must take some care in selecting the initial state, u(0), of the RBF model S1. Recall 
that the raised-cosine RBF network approximation, f0(u), in (9.9.9) has compact sup-
port. As a consequence, the RBF model output goes to y1(k) 5 0 when u(k) Ó V where
 
V 5 fxm 2 Dx, xM 1 Dxgm11 3 fym 2 Dy, yM 1 Dygn 
 (9.10.12)
A simple example of this can be seen in Figure 9.38, which shows a four-point RBF net-
work where U 5 f21, 1g2 is the flat surface on the top and the network goes to zero outside 
of V 5 f23, 3g2. Since the network output is zero outside of V, this means that the RBF 
model is a BIBO stable system. However, it does place a constraint on the choice of ini-
tial state, u(0) [ Rp. For example, consider the case when the lower bound ym 2 Dy . 0. 
If y(k) 5 0 for some 2n # k # 21, then y(k) 5 0 for k $ 0. This can be remedied by 
selecting the following initial state for the RBF model. This places u(0) at the center of U.
 
u(0) 5 a 1 b
2
 
 (9.10.13)
A similar problem can arise when the RBF model is being trained if u(k) wanders 
outside of V. This can be addressed by training the RBF model using M separate sin-
gle-step simulations rather than one M-step simulation. For example, let U
 train  be a p 3 M 
random matrix whose columns are uniformly distributed over the domain U. Then in 
(9.9.23) the weight w(k 1 1) can be updated using the kth column of U
 train  for u(k).
Domain of Sf
RBF model S1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 9.10 Nonlinear System Identification (NLMS)    737
The final practical issue that needs to be addressed is the question of how to deter-
mine whether or not the RBF model S1 represents a good fit to the system Sf. Let 
y [ RL and e [ RL be column vectors containing samples of the output y(k) and error 
e(k) 5 y(k) 2 y1(k) associated with a test input. Then the percent error, E(L), defined 
previously in (9.9.25) can be used.
In MATLAB, the system Sf and the RBF model S1 can be tested for a specific input 
x(k) by initializing the state vector as in (9.10.13) and then updating the state vector 
at each iteration as follows.
if (k == 1) 
% update state vector
 
u = [x(k) ; u(1:p-1)];
else
 
u = [x(k) ; u(1:m) ; y(k-1) ; u(m+2:p-1)];
end
MATLAB Functions
MATLAB Functions
Nonlinear System identification
EXAMPLE 9.20
As a practical example of a nonlinear discrete-time system, consider a continu-
ously stirred tank chemical reactor. Suppose x(k) is the reactant concentration at 
time k, and y(k) is the product concentration at time k. If the input is scaled such 
that 0 # x(k) # 1, then the Van de Vusse reaction can be characterized by the fol-
lowing nonlinear discrete-time system (Hernandez and Arkun, 1996).
y(k) 5 c1 1 c2x(k 2 1) 1 c3y(k 2 1) 1 c4x3(k 2 1) 1 c5y(k 2 2)x(k 2 1)x(k 2 2)
The presence of the fourth and fifth terms makes this system nonlinear. If the 
sampling interval is T 5 .04 hours, then the parameters of the system can be 
taken to be
c 5 f.558, .538, .116, 2.127, 2.034gT
For this system m 5 2 and n 5 2. Thus the system dimension is p 5 5. Since the 
input is non-negative and has been normalized,
fxm, xMg 5 f0, 1g
To determine the output bounds, consider a test input consisting of 
L 5 1000 points and a safety margin of y 5 .1, as in (9.10.6). This results in 
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

738    Chapter 9  Adaptive Signal Processing
Figure 9.42: Comparison of System Sf and Raised-Cosine RBF Model with 
m 5 2, n 5 2, d 5 5
0
10
20
30
40
50
60
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
k
Outputs
y(k)
y1(k)
fym, yMg 5 f.4580, 1.1647g, which means the domain of the function f  in this  
case is
U 5 f0, 1g3 3 f.4580, 1.1647g2
Suppose the initial conditions are w(0) 5 0 and u(0) 5 (a 1 b)y2, as in (9.10.13). 
Let the number of grid points per dimension be d 5 5. From (9.9.3) this produces 
an RBF model with r 5 3125 terms. However, from (9.9.19) only K 5 32 of these 
terms are nonzero for each u. Using (9.10.11) the grid-point spacings in the x and 
y directions are 
Dx 5 .25
 Dy 5 .1767
Suppose the elements of the weight vector are computed using a training set with 
M 5 5 3 104 random state vectors uniformly distributed over U. Finally, the sys-
tem is tested with an input x(k) of L 5 103 samples of white noise uniformly 
distributed over fxm, xMg. Given the higher-dimensional search space (p 5 5), 
a relatively large step size of  5 1 is used. A comparison of the two outputs, 
obtained by running exam9_20, is shown in Figure 9.42. The solid horizontal 
lines indicate the boundaries of the domain U in the y direction, and the dotted 
lines show where the grid points are located. For convenience in viewing the dis-
play, only the first 60 samples are plotted. Careful inspection reveals that there 
are only minor differences between the two outputs. Using (9.9.25), the percent 
error in this case was E(L) 5 1.06%. The corresponding learning curve is shown 
in Figure 9.43.
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

 9.10 Nonlinear System Identification (NLMS)    739
The DSP Companion contains the following functions which implement nonlin-
ear system identification using a raised-cosine RBF network. The DSP Companion 
function  f_rbfw described at the end of Section 9.9 can be used to train the weights 
for a nonlinear discrete-time system. The following two functions evaluate the raised- 
cosine RBF model and the nonlinear system.
% F_RBFY1:  Compute output of a RBF nonlinear system model
% F_NONSYS:  Compute the output of a nonlinear discrete-time system
%
% Usage:
%  
y1 = f_rbfy1 (x,w,a,b,d,m,n,u);
%  
y  = f_nonsys (@f,x,m,n,u);
% Pre:
%  
x  = N by 1 vector of inputs
%  
w  = r by 1 weight vector where r = d^p
%  
a  = p by 1 vector of lower bounds on u
%  
b  = p by 1 vector of upper bounds on u
%  
d  = number of grid points per dimension (d >= 2)
%  
m  = number of past inputs
%  
n  = number of past outputs
%  
u  = p by 1 initial state vector
%  
f  = name of user-supplied function that returns values of 
% 
 
 the nonlinear function to approximated. Here u is t p by 1
%  
 
 vector u = [x(k),...,x(k-m),y(k-1),...,y(k-n] and f
%  
 
 specifies the right-hand side of the nonlinear 
% 
 
 discrete-time system.
DSP Companion
DSP Companion
Figure 9.43: Learning Curve for the Nonlinear System in Example 9.19 with 
 5 1
0
1
1.5
0.5
2
2.5
3
3.5
4
4.5
5
3104
0
0.2
0.4
0.6
0.8
1
1.2
1.4
k
e2(k)
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

740    Chapter 9  Adaptive Signal Processing
gUi Modules and Case Studies
This section focuses on system identification using adaptive signal processing techniques. 
A graphical user interface module called g_adapt is introduced that allows the user to 
perform system identification without any need for programming. A case study program-
ming example using the DSP Companion functions is then presented.
g_adapt: Perform Adaptive Signal Processing
The DSP Companion includes a GUI module called g_adapt that allows the user to 
perform system identification using a variety of adaptive techniques without any need 
for programming. GUI module g_adapt features the display screen with tiled windows 
shown in Figure 9.44. The design features of g_adapt are summarized in Table 9.2.
The upper left-hand Block diagram window contains a block diagram of the adaptive 
system under investigation. It features an mth order transversal filter characterized by the 
following time-varying difference equation.
 
y(k) 5 o
m
i50
wi(k)x(k 2 i), 0 # k , N 
 (9.11.1)
The Parameters window below the block diagram displays edit boxes containing sim-
ulation parameters. The contents of each edit box can be directly modified by the user, 
with the Enter key used to activate the changes. The parameters a and b are the coefficient 
vectors of the black-box system to be identified.
 
o
n
i50
aid(k 2 i) 5 o
p
i50
bix(k 2 i), 0 # k , N 
 (9.11.2)
It is important to select a such that the resulting black-box system is BIBO stable. For 
example, a can be specified in terms of its roots using the function poly. The parameter 
fs is the sampling frequency, in Hz, and the parameter m $ 0 is the order of the adap-
tive filter. The remaining parameters are all real scalars that control the behavior of the 
adaptive algorithm. Parameter mu is the step length, and it should be chosen to satisfy the 
following bound, where Px is the average power of the input x.
 
0 ,  ,
1
(m 1 1)Px
 
 (9.11.3)
9.11
gUi Module
%
%  
y  = f(u)
%
% Post:
%  
y1 = N by 1 vector of outputs of RBF model
%  
y  = N by 1 vector of output of the nonlinear discrete-
% 
 
 time system
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.11 GUI Modules and Case Studies    741
Select Type
Select View
Slider Bar
Edit Parameters
0
50
100
150
200
250
210
25
0
5
10
k
Outputs
0
0.2
0.4
0.6
0.8
1
0
0.5
1
g_adapt
x(k)
Adaptive filter
Black box
d(k)
1
y(k)
e(k)
LMS Method, Outputs: m 5 40, mu 5 0.0200, E 5 0.002193
d(k)
y(k)
Figure 9.44: Display Screen for GUI Module g_adapt
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

742    Chapter 9  Adaptive Signal Processing
Parameter nu is the leakage factor for the leaky LMS method. Typically, nu , 1 with 
nu < 1. When nu 5 1, the leaky LMS method reduces to the LMS method. Parameter 
alpha is the normalized step size used in the normalized LMS method and the relative 
step size of the correlation LMS method. The correlation LMS method also uses the 
smoothing parameter beta where 0 , beta , 1 with beta < 1. Inappropriate values for 
the scalar control parameters can cause some of the methods to diverge.
The Type and View windows in the upper-right corner of the screen allow the user to 
select both the type of adaptive algorithm and the viewing mode. The algorithm options 
include the LMS method, the normalized LMS method, the correlation LMS method, 
the leaky LMS method, and the RLS method. The View options include the input, a 
comparison of outputs, a comparison of magnitude responses, the learning curve, the 
step sizes used during learning, and the final weights found. The Plot window along the 
bottom half of the screen shows the selected view.
There are two checkbox controls. The dB checkbox toggles the magnitude 
response display between linear and logarithmic scales. The Import checkbox toggles 
the source of the input and desired output data. When it is checked, the user is 
prompted for the name of a MAT file that contains the vector of input samples x, 
the vector of desired output samples y, and the sampling frequency fs. Then d is 
set equal to y. This is done to make the import option compatible with the export 
options of the other GUI modules that do not export d. Using the Import option 
and a user-created MAT file, systems with input-output data generated offline from 
another source (e.g. from measurements) can be identified. When the Import data 
checkbox is not checked, the input consists of white noise uniformly distributed over 
f21, 1g, and the desired output is computed using the coefficients a and b. The num-
ber of samples, N, is controlled with the horizontal slider bar appearing below the 
Type and View windows. This control is active only when the Import checkbox is not 
checked.
The Menu bar at the top of the screen includes several menu options. The Export 
option is used to save a, b, x, y, fs to a MAT file. Here y is set to d before the data is saved. 
This is done to maintain compatibility with other GUI modules. Results exported from 
one GUI module can be imported into other GUI modules for additional processing. 
The Caliper option allows the user to measure any point on the current plot by moving 
the mouse crosshairs to that point and clicking. The Print option sends the GUI window 
to a printer or a file. Finally, the Help option provides the user with some helpful sugges-
tions on how to effectively use module g_adapt.
item 
Variables 
Block diagram 
x(k), y(k), d(k), e(k) 
Edit parameters 
a, b, fs, N, , , ,  
Filter type 
LMS, normalized LMS, correlation LMS, leaky LMS, RLS
Plot view 
input, outputs, magnitude response, learning curve, step sizes, final 
weights
Slider 
filter order m 
Check boxes 
import/no import, linear/dB scale 
Menu buttons 
export, caliper, print, help, exit 
Import 
x, y, fs 
Export 
a, b, x, y, fs 
Table 9.2:  
Features of GUI  
Module g_adapt
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.11 GUI Modules and Case Studies    743
identification of a Chemical Process
In the chemical process-control industry it is common to have dynamic systems with 
time delays due to transportation lags. This leads to process models that are described 
by both differential and difference equations. As an illustration, consider the following 
first-order with dead time system that can be used to model, for example, a heated stirred 
tank (Bequette, 2003).
 
dya(t)
dt
1 pya(t) 5 cxa(t 2 ) 
 (9.11.4)
Here  is the dead time or transportation lag in the input, and p and c are system parame-
ters. Let T denote the sampling interval. The differential-difference system in (9.11.4) can 
be converted to a discrete-time system by approximating the derivative using a backward 
difference as follows.
 
dya(t)
dt
< y(k) 2 y(k 2 1)
T
 
 (9.11.5)
Here it is understood that y(k) 5 ya(kT). The input delay can be modeled exactly in dis-
crete time if some care is taken is choosing the sampling interval. Suppose T 5 yM for 
some integer M $ 1. Then xa(t 2 ) can be replaced by x(k 2 M), where x(k) 5 xa(kT). 
These substitutions yield the following equivalent discrete-time model.
 
y(k) 2 y(k 2 1)
T
1 py(k) 5 cx(k 2 M) 
 (9.11.6)
Thus the differential-difference system in (9.11.4) can be approximated by an IIR filter if 
the sampling interval is chosen to be an integer submultiple of the delay . To investigate 
how well the IIR model can be approximated by an adaptive transversal filter, suppose 
the delay is  5 5 sec and the sampling interval is T 5 .5 sec. This yields an input delay of 
M 5 10 samples. Let the remaining system parameters be p 5 .2 and c 5 4. The normal-
ized LMS method can be used to identify this system using case9_1.
When case9_1 is run with an adaptive filter of order m 5 60 using N 5 1000 samples, w(0) = 
0, and a normalized step size of  5 .15, it produces the learning curve shown in Figure 9.45.  
Case Study 9.1
Case study 9.1
Figure 9.45:  
Learning Curve 
Using the 
Normalized LMS 
Method with 
m 5 60 and 
 5 .15
0
200 300
100
400 500 600 700 800 900 1000
0
5
10
15
20
25
k
e2(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

744    Chapter 9  Adaptive Signal Processing
The normalized LMS method appears to converge in approximately 700 samples. Next the 
final values for the weights are used, and a new white noise test input of length L 5 500 is 
generated to compare the responses of the two systems. The results are shown in Figure 9.46 
where, for convenience of display, only the first 100 samples are shown. There is a good fit 
with the stirred tank model with a percent error from (9.9.25) of E(L) 5 1.26%.
Chapter Summary
Learning outcomes
This chapter was designed to provide the student with an opportunity to achieve the 
learning outcomes summarized in Table 9.3.
Least Mean Square Techniques
Chapter 9 focused on linear and nonlinear adaptive signal processing techniques and 
their applications. The adaptive filter structure used was a linear mth-order transversal 
filter of the following form.
 
y(k) 5 o
m
i50
wi(k)x(k 2 i), k $ 0 
 (9.12.1)
One important qualitative feature of this structure is that once the weights converge to 
their steady-state values, the resulting FIR filter is guaranteed to be BIBO stable. Adap-
tive systems can be configured in a number of ways, depending on the application. Exam-
ples include system identification, channel equalization, signal prediction, and noise 
cancellation. Consider the following (m 1 1) 3 1 state vector of past inputs.
 
u(k) 5 fx(k), x(k 2 1), Á , x(k 2 m)gT 
 (9.12.2)
9.12
Transversal filter
State vector
Figure 9.46:  
Comparison of 
Desired and Actual 
Outputs Using the 
Final Values for 
the Weights
0
20
30
10
40
50
60
70
80
90
100
27
26
25
24
23
22
21
0
1
2
3
k
Outputs
E(500) = 1.26%
d(k)
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.12 Chapter Summary    745
A compact expression for the transversal-filter output can be obtained in terms of u(k) 
using the following dot product formulation.
 
y(k) 5 wT(k)u(k), k $ 0 
 (9.12.3)
The (m 1 1) 3 1 weight vector w(k) is adjusted so as to minimize the mean square 
error (w) 5 E fe2(k)g, where E is the expected value operator. Here the system error is 
e(k) 5 d(k) 2 y(k), where d(k) is the desired output. For example, in the system iden-
tification application d(k) is the output of the system to be identified. When the input 
x(k) is sufficiently rich in frequency content, the mean square error can be shown to be 
a positive-definite quadratic function of the weight vector w, which means that a unique 
optimal weight vector exists. The weights can be adjusted by searching the mean square 
error function using a steepest-descent search method. For the purpose of computing 
the gradient of (w), one can use the simplifying assumption E fe2(k)g < e2(k). This leads 
to the following popular weight-update algorithm called the least mean square or LMS 
method.
 
w(k 1 1) 5 w(k) 1 2e(k)u(k), k $ 0 
 (9.12.4)
The scalar parameter  is the step size. If Px 5 E fx2(k)g denotes the average power of the 
input, then the LMS method will converge for step lengths in the following range.
 
0 ,  ,
1
(m 1 1)Px
 
 (9.12.5)
Since the time constant of the LMS method is inversely proportional to the step size, 
larger step sizes are preferable for faster convergence. However, once convergence has 
been achieved, excess mean square error is encountered as a result of the approxima-
tion used for the gradient of the mean square error. Because excess mean square error is 
Weight vector
LMS method
Convergence
Num.  Learning outcome  
Sec. 
 1 
Understand how to use adaptive filters to perform system identification, channel 
equalization, signal prediction, and noise cancellation
9.1 
 2 
Know how to compute the mean square error and how to find an optimal weight 
vector that minimizes the mean square error
9.2 
 3 
Understand how to implement the least mean square (LMS) method for updating 
the weight vector
9.3 
 4 
Know how to find bounds on the step size that ensure steady-state convergence 
of the LMS method
9.4 
 5 
Know how to estimate the rate of convergence and the steady-state error of the 
LMS method
9.4 
 6 
Understand how to modify the basic LMS method to enhance performance using 
the normalized, correlation, and leaky LMS methods
9.5 
 7 
Be able to design adaptive FIR filters using pseudo-filters
9.6 
 8 
Know how to apply the recursive least squares (RLS) method
9.7 
 9
Know how to apply the filtered-x LMS and signal-synthesis adaptive methods to 
achieve active control of acoustic noise
9.8 
10 
Know how to use a raised-cosine RBF network to approximate a nonlinear  
function f(u) of p variables
9.9 
11 
Be able to identify nonlinear discrete-time systems using an using raised-cosine 
RBF networks
9.10 
12 
Know how to use the GUI module  g_adapt to perform system identification
9.11 
Table 9.3:  
Learning Outcomes  
for Chapter 9
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

746    Chapter 9  Adaptive Signal Processing
proportional to the step size, there is a tradeoff in choosing  between convergence speed 
and steady-state accuracy. In many applications, it is common to choose values for  that 
are about an order of magnitude less than the upper bound in (9.12.5).
The basic LMS method in (9.12.4) can be modified in a number of ways to enhance 
performance. These include the normalized LMS method, the correlation LMS method, 
and the leaky LMS method. The normalized and correlation methods feature step sizes 
that vary with time. For example, for the normalized LMS method the step size is as fol-
lows, where 0 ,  , 1 is the normalized step size and  . 0 is included to ensure that the 
step size never grows beyond y.
 
(k) 5

 1 uT(k)u(k) 
 (9.12.6)
The leaky LMS method tends to make the LMS method more stable by preventing the 
weights from becoming too large. It is useful for narrowband inputs because it has the 
effect of adding white noise to the input. However, this tends to increase the excess mean 
square error once convergence has taken place.
Adaptive signal processing can be used to design FIR filters with prescribed magni-
tude and phase response characteristics. This is done by applying system identification to 
a synthetic pseudo-filter. The desired phase response for an mth order FIR filter is decom-
posed into a linear part, 2mf T, and a nonlinear part, ( f ), called the residual phase.
 
( f ) 5 ( f ) 2 mfT 
 (9.12.7)
The magnitude and phase responses of a real physical systems are not completely inde-
pendent of one another. For example, for a real system the following end-point con-
straints apply.
 
A( f ) sinf( f )g 5 0, f [ h0, fsy2j 
 (9.12.8)
An important alternative to the LMS family of methods for updating the weights is 
the recursive least squares or RLS method. The RLS method attempts to find the optimal 
weight at each time step, unlike the LMS method, which approaches the optimal weights 
gradually using a steepest-descent search. As a consequence, the RLS method typically 
converges much faster that the LMS method, but there is more computational effort per 
iteration. The computational effort of the RLS method is of order O(m2), whereas the 
computational effort for the LMS method is of order O(m) where m is the filter order.
*Active Noise Control
A practical application area of adaptive signal processing is the active control of acoustic 
noise. The basic premise is to inject a secondary sound into an environment so as to can-
cel the primary sound using destructive interference. This application requires adaptive 
techniques, because the nature of the unwanted sound and the characteristics of the 
environment typically are unknown and change with time. The secondary sound must be 
generated by a speaker, transmitted over an air channel, and detected by a microphone. 
Consequently, active noise control requires a modification of the LMS method called the 
filtered-x or FXLMS method.
 
w(k 1 1) 5 w(k) 1 2e(k)u⁄(k), k $ 0 
 (9.12.9)
The FXLMS differs from the LMS method in that the state vector of past inputs first 
must be filtered by a model of the path traveled by the secondary sound. This model 
Normalized step size
Pseudo-filters
Residual phase
RLS method
Optional material
FXLMS method
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.12 Chapter Summary    747
typically is obtained offline using standard system identification techniques. Often the 
primary noise consists of a narrowband periodic component plus white noise. When the 
fundamental frequency of the periodic component of the noise is known, a more direct 
signal-synthesis approach can be used to cancel the periodic component of the primary 
noise.
*Adaptive Function Approximation
Another class of applications for adaptive signal processing is the approximation of non-
linear functions of several variables. Let f:U S R be continuously differentiable on a 
compact domain U , Rp.
 
y 5 f(u), u [ U 
 (9.12.10)
The function f(u) can be approximated with a function f0(u) by constructing a uniform 
grid of points on U and developing a local representation valid on each grid element. If p 
is the number of dimensions and d is the number of grid points per dimension, then there 
are a total of r 5 d p grid points. The function f  can be approximated using
 
f0(u) 5 wTg(u) 
 (9.12.11)
Here w [ Rp is an adjustable weight vector, and g(u) is a p 3 1 vector of radial basis 
functions (RBF), with function gi(u) centered at grid point u i [ U. Radial basis functions 
evaluate to one at the center point and approach zero as the radial distance from the cen-
ter point increases. Using a raised-cosine type of RBF, the approximation f0(u) converges 
uniformly to f(u) on U as the grid-point precision d approaches infinity. Starting from an 
initial guess w(0) [ U, the weight can be updated using the following generalization of 
the LMS method called a nonlinear least mean square or NLMS method. Here  . 0 is 
the step size, y0 5 f0(u), and e(k) 5 y(k) 2 y0(k) is the error at iteration k.
 
w(k 1 1) 5 w(k) 1 2e(k)gfu(k)g, k $ 0 
 (9.12.12)
*Nonlinear System identification
A practical application of nonlinear function approximation is the identification of non-
linear discrete-time systems. Here state vector u(k) is generalized as follows, where x(k) is 
the system input and y(k) is the system output.
 
u(k) 5 fx(k), x(k 2 1), Á , x(k 2 m), y(k 2 1), Á , y(k 2 n)g  
 (9.12.13)
Given the state vector u(k) of past inputs and outputs, the present output y(k) of a  
discrete-time nonlinear system Sf can be expressed as
 
y(k) 5 f fu(k)g, k $ 0 
 (9.12.14)
The function f(u) is assumed to be a continuous nonlinear function of p 5 m 1 n 1 1 
variables. If the nonlinear system in (9.12.14) is BIBO stable and the input is bounded by 
fxm, xMg, then the output will be bounded by some fym, yMg. In this case the domain of 
the function f(u) is
 
U 5 fxm, xMgm11 3 fym, yMgn 
 (9.12.15)
The raised-cosine RBF network used to approximate nonlinear functions then can 
be applied to (9.12.14) to identify the nonlinear system Sf.
Optional material
Radial basis functions
NLMS method
Optional material
State vector
Nonlinear system
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

748    Chapter 9  Adaptive Signal Processing
gUi Module
The DSP Companion includes a GUI module called g_adapt that allows the user to eval-
uate and compare several adaptive system identification techniques without any need for 
programming.
Problems
The problems are divided into Analysis and Design problems that can be solved by 
hand or with a calculator, GUI Simulation problems that are solved using GUI module  
g_adapt, and MATLAB Computation problems that require a user program. Solutions 
to selected problems can be accessed with the DSP Companion driver program, g_dsp. 
Students are encouraged to use these problems, which are identified with a 
, as a check 
on their understanding of the material.
9.13.1 Analysis and Design
Section 9.2: Mean Square Error
9.1 
The transversal filter structure used in this chapter is a time-varying FIR filter. One 
can generalize it by using the following time-varying IIR filter.
y(k) 5 o
m
i50
bi(k)x(k 2 i) 2 o
n
i51
ai(k)y(k 2 i)
(a) Find suitable definitions for the state vector u(k) and the weight vector w(k) 
such that the output of the time-varying IIR filter can be expressed as a dot 
product as in (9.2.3). That is,
y(k) 5 wT(k)u(k), k $ 0
(b) Suppose the weight vector w(k) converges to a constant. Is the resulting filter 
guaranteed to be BIBO stable? Why or why not?
9.2 
Suppose a transversal adaptive filter is of order m 5 2. Find the input auto- 
correlation matrix R for the following cases.
(a) The input x(k) consists of white noise uniformly distributed over the interval fa, bg.
(b) The input x(k) consists of Gaussian white noise with mean x and variance 2
x.
9.3 
Find the constant term, Pd 5 E fd 2(k)g, of the mean square error when the desired 
output is the following signal.
d(k) 5 b 1 sin1
2k
N 2 2 cos1
2k
N 2
9.4 
Consider a transversal filter of order m 5 1. Suppose the input and desired output 
are as follows.
x(k) 5 2 1  sin(ky2)
 d(k) 5 1 2 3 cos(ky2)
(a) Find the cross-correlation vector p.
(b) Find the input auto-correlation matrix R.
(c) Find the optimal weight vector w*.
9.13
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.13 Problems    749
9.5 
Suppose the first row of an auto-correlation matrix R is r 5 f9, 7, 5, 3, 1g.
(a) Find R.
(b) What is the average power of the input?
(c) Suppose x(k) is white noise uniformly distributed over the interval f0, cg. Find c.
9.6 
Suppose v(k) is white noise uniformly distributed over f2c, cg. Consider the follow-
ing input.
x(k) 5 2 1 sin(ky2) 1 v(k)
 
Find the input auto-correlation matrix R. Does your answer reduce to that of  
Problem 9.4 when c 5 0?
9.7 
Suppose an input x(k) and a desired output d(k) have the following auto-correla-
tion matrix and cross-correlation vector. Find the optimal weight vector w*.
R 53
5
1
1
54, p 53
3
224
Section 9.3: Least Mean Square (LMS) Method
9.8 
Suppose the mean square error is approximated using a running-average filter of 
order M 2 1 as follows.
(w) < 1
M o
M21
i50
e2(k 2 i)
(a) Find an expression for the gradient vector =(w) using this approximation for 
the mean square error.
(b) Using the steepest-descent method and the results from part (a), find a 
weight-update formula.
(c) How many floating-point multiplications (FLOPs) are required per iteration 
to update the weight vector? You can assume that 2 is computed ahead of 
time.
(d) Verify that when M 5 1 the weight-update formula reduces to the LMS 
method.
9.9 
There is an offline or batch procedure for computing the optimal weight vector 
called the least-squares method (see Problem 9.36). For large values of m, the least-
squares method requires approximately 4(m 1 1)3y3 FLOPs to find w. How many 
iterations are required before the computational effort of the LMS method equals 
or exceeds the computational effort of the least-squares method?
Section 9.4: Performance Analysis of LMS Method
9.10 Suppose an input x(k) has the following auto-correlation matrix.
R 53
2
1
1
24
(a) Using the eigenvalues of R, find a range of step sizes that ensures convergence 
of the LMS method.
(b) Using the average power of the input, find a more conservative range of step 
sizes that ensures convergence of the LMS method.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

750    Chapter 9  Adaptive Signal Processing
(c) Suppose the step size is one tenth the maximum in part (b). Find the time con-
stant of the mean square error in units of iterations.
(d) Using the same step size as in part (c), find the misadjustment factor Mf.
9.11 Suppose the LMS learning curve converges to within one percent of its final steady-
state value in 200 iterations.
(a) Find the learning-curve time constant, mse, in units of iterations.
(b) If the minimum eigenvalue of R is min 5 .1, what is the step size?
(c) If the step size is  5 .02, what is the minimum eigenvalue of R?
9.12 Suppose the misadjustment factor for the LMS method is Mf 5 .4 when the input 
is white noise uniformly distributed over f22, 2g.
(a) Find the average power of the input.
(b) If the step size is  5 .01, what is the filter order?
(c) If the filter order is m 5 9, what is the step size?
Section 9.5: Modified LMS Methods
9.13 Financial considerations dictate that a production system must remain in operation 
while the system is being identified. During normal operation of the linear system, 
the input x(k) has relatively poor spectral content.
(a) Which of the modified LMS methods would appear to be an appropriate 
choice? Why?
(b) How might the input be modified slightly to improve identification without 
significantly affecting the normal operation of the system?
9.14 Consider the normalized LMS method.
(a) What is the maximum value of the step size?
(b) Describe an initial condition for the past inputs that will cause the step size to 
saturate to its maximum value.
Section 9.6: Adaptive FiR Filter Design
9.15 Consider the following periodic input that is used as part of the input-output spec-
ification for a pseudo-filter. Suppose fi 5 ifsy(2N) for 0 # i , N. Find the auto- 
correlation matrix R for this input.
x(k) 5 o
N21
i50
Ci cos(2fikT )
9.16 Consider the following periodic input and desired output that form the input- 
output specification for a pseudo-filter. Suppose fi 5 ifsy(2N) for 0 # i , N. Find 
the cross-correlation vector p for this input and desired output.
x(k) 5 o
N21
i50
Ci cos(2fikT )
d(k) 5 o
N21
i50
AiCi cos(2fikT 1 i)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.13 Problems    751
Section 9.7: Recursive Least Squares (RLS) Method
9.17 Consider the following expression for the generalized cross-correlation vector used 
by the RLS method.
p(k) 5 o
k
i51
k 2 id(i)u(i)
 
Show that p(k) can be expressed recursively in terms of p(k 2 1) by deriving the 
expression for p(k) in (9.7.8).
Section 9.8: Active Noise Control
9.18 Consider the active noise control system shown in Figure 9.47. Suppose the second-
ary path is modeled as a delay with attenuation. That is, for some delay  . 0 and 
some attenuation 0 ,  , 1,
y⁄(t) 5 y(t 2 )
(a) Let the sampling interval be T 5 yM. Find the transfer function, F(z).
(b) Suppose the primary path G(z) is modeled as follows. Find W(z) using (9.8.3).
G(z) 5 o
m
i50
z2i
1 1 i
(c) Is the controller W(z) physically realizable? Why or why not?
Section 9.9: Adaptive Function Approximation
9.19 Consider the following candidate for a scalar radial basis function.
Gi(z) 55
 cos2i1
z
2 2, uzu # 1
0,
uzu . 1
(a) Show that Gi(z) qualifies as an RBF for i $ 1.
(b) Does Gi(z) have compact support?
(c) Show that Gi(z) reduces to the raised-cosine RBF when i 5 1.
9.20 Consider a raised-cosine RBF network with p 5 1, d 5 2, and U 5 f0, 1g. Using 
the trigonometric identities from Appendix 2, show that the constant interpolation 
property holds in this case. That is, show that
g0(u) 1 g1(u) 5 1, a1 # u # b1
Figure 9.47:  
Active Control of 
Acoustic Noise
e(k)
G(z)
F(z)
W(z)
d(k)
2
1
y(k)
x(k)
y(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

752    Chapter 9  Adaptive Signal Processing
9.21 Suppose the nonlinear function in (9.9.1) is f(u) 5 c for some constant c. Let di 5 2 
for 1 # i # p and wi 5 c for 0 # i , r. Show that the raised-cosine RBF network, 
f0, is exact. That is, show that if f0(u) 5 wTg(u), then
f0(u) 5 c for u [ U
9.22 Consider a one-dimensional raised-cosine RBF network f0(u). Let =f0(u) denote 
the gradient vector of partial derivatives of f0(u) with respect to the elements of u. 
Show that =f0(u) 5 0 at the r grid points. That is, show that
=f0(ui) 5 0, 0 # i , r
9.23 Suppose the nonlinear function f  in (9.9.1) is continuously differentiable. Let 
f0(u) 5 wTg(u), where wi 5 f(ui) for 0 # i , r. Consider the following metric for the 
error between the function f(u) and the raised-cosine RBF network approximation 
f0(u).
E0(d ) 5
D  max 
u[U hu f(u) 2 f0(u)uj
 
Show that the raised-cosine RBF network converges uniformly to f(u) on the 
domain U as d approaches infinity. That is, show that
E0(d) S 0 as d S `
Section 9.10: Nonlinear System identification 
9.24 Consider the problem of identifying the nonlinear discrete-time system in (9.9.4) 
using a raised-cosine RBF network. Let the number of past inputs be m 5 1 and 
the number of past outputs be n 5 1. Suppose the range of values for the inputs 
is fxm, xMg 5 f22, 2g, and the range of values for the outputs is fym, yMg 5 f23, 3g. 
Let the number of grid points per dimension be d 5 4.
(a) Find the domain U of the function f .
(b) What is the total number of grid points?
(c) What is the grid-point spacing in the x direction and in the y direction?
(d) For each u, what is the maximum number of nonzero terms in the RBF  
network output?
(e) Consider the following state vector. Find the vector subscripts of the vertices 
of the grid element containing u.
u 5 f.3, 21.7, 1.1gT.
(f) Find the scalar subscripts of the vertices of the grid element containing the u 
in part (e).
9.25 Consider a raised-cosine RBF network with m 5 2 past inputs and n 5 2 past out-
puts. Suppose the range of values for the inputs is fxm, xMg 5 f0, 5g, and the range 
of values for the outputs is fym, yMg 5 f22, 8g. Let the number of grid points per 
dimension be d 5 6.
(a) Find the compact support V of the overall network. That is, find the smallest 
closed, bounded region V , Rp such that
u Ó V 1 f0(u) 5 0
(b) Show that, in general, V S U as d S `, where U [ Rp is the domain of f .
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.13 Problems    753
9.13.2 gUi Simulation
Section 9.3: Least Mean Square (LMS) Method
9.26 Using the GUI module g_adapt, identify the black-box system using the LMS 
method. Set the step size to  5 .03, and then plot the following.
(a) The outputs
(b) The magnitude responses
(c) The learning curve
(d) The final weights
9.27 Consider the following FIR black-box system. Use the GUI module g_adapt to 
identify this system using the LMS method.
H(z) 5 1 2 2z21 1 7z22 1 4z24 2 3z25
 
Export the results to a MAT-file named prob7_27.mat, and then reload it using the 
Import option.
(a) Plot the learning curve when m 5 3.
(b) Plot the learning curve when m 5 5.
(c) Plot the learning curve when m 5 7.
(d) Plot the final weights m 5 7.
Section 9.5: Modified LMS Methods
9.28 Use the GUI module g_adapt to identify the following black-box system using the 
normalized LMS method.
H(z) 5
3
1 2 .7z24
(a) Plot the magnitude responses.
(b) Plot the learning curve.
(c) Plot the step sizes.
9.29 Use the GUI module g_adapt to identify the following black-box system using the 
correlation LMS method with a filter of order m 5 50.
H(z) 5
2
1 1 .8z24
(a) Plot the magnitude responses.
(b) Plot the learning curve.
(c) Plot the step sizes.
9.30 Using the GUI module g_adapt, identify the black-box system using the leaky 
LMS method. Adjust the number of samples to N 5 500 and the leakage factor to 
mu 5 .999. Plot the following.
(a) The outputs
(b) The magnitude responses
(c) The learning curve
9.31 Using the GUI module g_adapt, with the default parameter values, identify the 
black-box system using the leaky LMS method. Plot the learning curve for the 
following cases corresponding to different values of the leakage factor.
(a) nu 5 .999
(b) nu 5 .995
(c) nu 5 .990
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

754    Chapter 9  Adaptive Signal Processing
9.32 Using the GUI module g_adapt and the Import option, load the input and desired 
output from the MAT-file u_adapt1. Then identify the system that produced this 
input-output data using the normalized LMS method. Plot the following.
(a) The learning curve
(b) The magnitude responses using the dB scale
(c) The step sizes. Use the Caliper option to mark the largest step size.
Section 9.7: Recursive Least Squares (RLS) Method
9.33 Using the GUI module g_adapt, identify the black-box system using the following 
two methods. Plot the learning curve for each case. Observe the scale of the depen-
dent variable.
(a) The LMS method
(b) The RLS method
9.13.3 MATLAB Computation
Section 9.1: Motivation
9.34 Consider the problem of designing an equalizer as shown in Figure 9.48. Suppose 
that the delay is M 5 15 and H(z) represents a communication channel with the 
following transfer function.
H(z) 5
1 1 .5z21
1 1 .4z21 2 .32z22
 
Write a MATLAB program that uses the DSP Companion function f_lms to con-
struct an equalizer of order m 5 30 for H(z). Suppose x(k) consists of N 5 1000 
samples of white noise uniformly distributed over f23, 3g. Use a step size of 
 5 .002.
(a) Plot the learning curve.
(b) Using the final weights, compute y(k) using input r(k). Then plot d(k) and y(k) 
for 0 # k # Ny10 on the same graph with a legend.
(c) Using the final weights, plot the magnitude responses of H(z), W(z), and 
F(z) 5 H(z)W(z) on the same graph using a legend. For the abscissa, use  
normalized frequency, fyfs.
9.35 Consider the problem of designing an adaptive noise-cancellation system as shown 
in Figure 9.49. Suppose the additive noise v(k) is white noise uniformly distributed 
over f22, 2g. Let the primary microphone signal be as follows.
x(k) 5  cos1
k
102 2 .5 sin1
k
202 1 .25 cos1
k
302
Figure 9.48:  
Equalization of a 
Communication 
Channel, H(z) 
z2M
e(k)
d(k)
2 1
x(k)
y(k)
r(k)
H(z)
W(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.13 Problems    755
 
Suppose the path for detecting the noise signal has the following transfer function.
H(z) 5
.5
1 1 .25z22
 
Write a MATLAB program that uses the DSP Companion function f_lms to cancel 
the noise v(k) corrupting the signal d(k). Use an adaptive filter of order m 5 30, 
N 5 3000 samples, and a step size of  5 .003.
(a) Plot the learning curve.
(b) Using the final weights, compute y(k) using input r(k). Then plot x(k), d(k), 
and e(k) for 0 # k # Ny10 on the same graph with a legend.
Section 9.3: Least Mean Square (LMS) Method
9.36 There is an offline alternative to the LMS method called the least-squares method 
that is available when the entire input signal and desired output signal are available 
ahead of time. Suppose the weight vector w is constant. Taking the transpose of 
(9.2.3) and replacing the actual output by the desired output yields
uT(k)w 5 d(k), 0 # k , N
 
Let d 5 fd(0), d(1), Á , d(N 2 1)gT, and let X be an N 3 (m 1 1) past input matrix 
whose ith row is uT(i) for 0 # i , N. Then the N equations can be recast as the fol-
lowing vector equation.
Xw 5 d
 
When N . (m 1 1), this constitutes an over-determined linear algebraic system of 
equations. A weight vector that minimizes the squared error E 5 (Xw 2 d)T(Xw 2 d) 
is obtained by premultiplying both sides by XT. This yields the normal equations
XTXw 5 XTd
 
The coefficient matrix XTX is (m 1 1) 3 (m 1 1). If x(k) has adequate spectral 
content, XTX will be nonsingular. In this case the optimal weight vector in a least-
squares sense can be obtained by premultiplying by the inverse of XTX, which yields
w 5 (XTX )21 XTd
 
Write a MATLAB function called f_lsfit that computes the optimal least-squares 
FIR filter weight vector, b 5 w, by solving the normal equations using the  
MATLAB left division operator, \. The calling sequence should be as follows.
% F_LSFIT:  FIR system identification using offline least-squares 
fit method
Figure 9.49:  
Noise Cancellation
e(k)
d(k)
r(k)
H(z)
W(z)
2
1
1
x(k)
y(k)
v(k)
(Continued)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

756    Chapter 9  Adaptive Signal Processing
%
% Usage:
% 
w = f_lsfir (x,d,m)
% Pre:
% 
x = N by 1 vector containing input samples
% 
d = N by 1 vector containing desired output samples
% 
m = order of transversal filter (m < N)
% Post:
% 
b = (m+1) by 1 least-squares FIR filter coefficient vector
In constructing X, you can assume that x(k) is causal. Test f_lsfit by using N = 250 
and m = 30. Let x be white noise uniformly distributed over [−1, 1], and let d be a filtered 
version of x using the following IIR filter.
H(z) 5
1 1 z22
1 2 .1z21 2 .72z22
(a) Use stem to plot the least-squares weight vector b.
(b) Compute y(k) using the weight vector b. Then plot d(k) and y(k) for 0 # k # 50 
on the same graph using a legend.
Section 9.4: Performance Analysis of LMS Method
9.37 A plot of the squared error is only a rough approximation to the learning curve 
in the sense that E fe2(k)g < e2(k). Write a MATLAB program that uses the DSP 
Companion function f_lms to identify the following system. For the input use 
N 5 500 samples of white noise uniformly distributed over f21, 1g, and for the 
filter order use m 5 30.
H(z) 5
z
z3 1 .7z2 2 .8z 2 .56
(a) Use a step size  that corresponds to .1 of the upper bound in (9.4.16). Print 
the step size used.
(b) Compute and print the mean square error time constant in (9.4.29), but in 
units of iterations.
(c) Construct and plot a learning curve by performing the system identification 
M 5 50 times with a different white noise input used each time. Plot the aver-
age of the M e2(k) versus k curves and draw vertical lines at integer multiples 
of the time constant.
Section 9.5: Modified LMS Methods
9.38 Consider the problem of performing system identification as shown in Figure 9.50. 
Suppose the system to be identified is the following auto-regressive or all-pole filter.
H(z) 5
1
z4 2 .1z2 2 .72
 
Write a MATLAB program which uses the DSP Companion function f_lmsnorm 
to identify a model of order m 5 60 for this system. Use an input consisting of 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.13 Problems    757
N 5 1200 samples of white noise uniformly distributed over f21, 1g, a constant 
step size of  5 .1, and a maximum step size of max 5 5.
(a) Plot the learning curve.
(b) Plot the step sizes.
(c) Plot the magnitude response of H(z) and W(z) on the same graph using a 
legend where W(z) is the adaptive filter using the final values for the weights.
9.39 Consider the problem of performing system identification as shown previously in 
Figure 9.50. Suppose the system to be identified is the following IIR filter.
H(z) 5
z2
z3 1 .8z2 1 .25z 1 .2
 
Write a MATLAB program that uses the DSP Companion function f_lmscorr 
to identify a model of order m 5 50 for this system. Use an input consisting of 
N 5 2000 samples of white noise uniformly distributed over f21, 1g, a relative step 
size of  5 1, and the default smoothing parameter .
(a) Plot the learning curve.
(b) Plot the step sizes.
(c) Plot the magnitude response of H(z) and W(z) on the same graph using a 
legend where W(z) is the adaptive filter using the final values for the weights.
9.40 Consider the following IIR filter.
H(z) 5 10(z2 1 z 1 1)
z4 1 .2z2 2 .48
 
Write a MATLAB program that uses the DSP Companion function f_lmsleak 
to identify a model of order m 5 30 for this system. Use an input consisting of 
N 5 120 samples, a step size of  5 .005, and the following periodic input.
x(k) 5 cos1
k
5 2 1  sin1
k
102
(a) Plot the learning curve for  5 .99.
(b) Plot the learning curve for  5 .98.
(c) Plot the learning curve for  5 .96.
(d) Using  5 .995 and the final value for the weights, plot d(k) and y(k) on the 
same graph with a legend.
Figure 9.50:  
Identification of 
Linear Discrete-
time System, H(z) 
e(k)
d(k)
2 1
x(k)
y(k)
H(z)
W(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

758    Chapter 9  Adaptive Signal Processing
Section 9.6: Adaptive FiR Filter Design
9.41 Use the DSP Companion functions to write a MATLAB program that designs an 
FIR filter to meet the following pseudo-filter design specifications.
 A( f ) 5H
2,
0 # f , fs
6
3,
fs
6 # f , fs
3
3 2 241f 2 fs
32,
fs
3 # f , 5fs
12
1,
5fs
12 # f # fs
2
( f ) 5 ( f ) 2 30fyfs
( f ) 5 0
 
Suppose there are N 5 80 discrete frequencies equally spaced over 0 # f , fsy2, as 
in (9.6.2). Use f_lms with a step size of  5 .0001 and M 5 2000 iterations.
(a) Choose an order for the adaptive filter that best fits the phase specification. 
Print the order m.
(b) Plot the magnitude response of the filter obtained using the final weights. On 
the same graph, plot the desired magnitude response with isolated plot sym-
bols at each of the N discrete frequencies and a plot legend.
(c) Plot the phase response of the filter obtained using the final weights. On the 
same graph plot the desired phase response with isolated plot symbols at each 
of the N discrete frequencies and a plot legend.
Section 9.7: Recursive Least Squares (RLS) Method
9.42 Consider the problem of designing a signal predictor as shown in Figure 9.51. Sup-
pose the signal whose value is to be predicted is as follows.
x(k) 5 sin1
k
5 2 cos1
k
102 1 v(k), 0 # k , N
 
Here N 5 200, and v(k) is white noise uniformly distributed over f2.05, .05g. Write 
a MATLAB program that uses the DSP Companion function f_rls to predict the 
Figure 9.51:  
Signal Prediction
z2M
e(k)
d(k)
2 1
x(k)
y(k)
W(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.13 Problems    759
value of this signal M 5 20 samples into the future. Use a filter of order m 5 40 
and a forgetting factor of  5 .9.
(a) Plot the learning curve.
(b) Using the final weights, compute the output y(k) corresponding to the input 
x(k). Then plot x(k) and y(k) on separate graphs above one another, using the 
subplot command. Use the fill function to shade a section of x(k) of length M 
starting at k 5 160. Then shade the corresponding predicted section of y(k) 
starting at k 5 140.
Section 9.8: Active Noise Control
9.43 Consider the active noise control system shown previously in Figure 9.47. Suppose 
the secondary path is modeled by the following transfer function, which takes into 
account the delay and attenuation of sound as it travels through air, and the char-
acteristics of the microphones, speaker, amplifiers, and DAC.
F(z) 5
.2z23
1 2 1.4z21 1 .48z22
 
Suppose the sampling frequency is fs 5 2000 Hz. Write a MATLAB program that 
uses the DSP Companion function f_lms to identify an FIR model of the second-
ary path F(z) using an adaptive filter of order m 5 25. Choose an input and a step 
size that causes the algorithm to converge.
(a) Plot the learning curve to verify convergence.
(b) Plot the magnitude responses of F(z) and the model, F
⁄(z), on the same graph 
using a legend.
(c) Plot the phase responses of F(z) and the model, F
⁄(z), on the same graph using 
a legend.
9.44 Consider the active noise control system shown previously in Figure 9.47. Suppose 
the primary noise x(k) consists of the following noise-corrupted periodic signal.
x(k) 5 2o
5
i51
 sin(2F0ikT )
1 1 i
1 v(k)
 
Here the fundamental frequency is F0 5 100 Hz and fs 5 2000 Hz. The additive 
noise term, v(k), is white noise uniformly distributed over f2.2, .2g. Coefficient 
vectors for FIR models of the secondary path F(z) and the primary path G(z) are 
contained in MAT-file prob9_44.mat. The coefficient vectors are f  and g. Write 
a MATLAB program that loads f  and g and uses the DSP Companion function 
f_fxlms to apply active noise control with the filtered-x LMS method starting at 
sample Ny4 where N 5 2000. Use a noise controller of order m 5 30 and a step 
size of  5 .002. Plot the learning curve including a title that displays the amount 
of noise cancellation in dB using (9.8.15).
9.45 Consider the active noise control system shown previously in Figure 9.47. Suppose 
the primary noise x(k) consists of the following noise-corrupted periodic signal.
x(k) 5 2o
5
i51
 sin(2F0ikT )
1 1 i
1 v(k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

760    Chapter 9  Adaptive Signal Processing
 
Here the fundamental frequency is F0 5 100 Hz, and fs 5 2000 Hz. The additive 
noise term, v(k), is white noise uniformly distributed over f2.2, .2g. Coefficient 
vectors for FIR models of the secondary path F(z) and the primary path G(z) are 
contained in MAT file prob9_44.mat. The coefficient vectors are f  and g. Write 
a MATLAB program that loads f  and g and uses the DSP Companion function 
f_sigsyn to apply active noise control with the signal-synthesis method starting at 
sample Ny4 where N 5 2000. Use a step size of  5 .04.
(a) Plot the learning curve. Add a title which displays the amount of noise cancel-
lation in dB using (9.8.15).
(b) Plot the magnitude spectra of the noise without cancellation.
(c) Plot the magnitude spectra of the noise with cancellation.
Section 9.10: Nonlinear System identification
9.46 Consider the following nonlinear discrete-time system which has m 5 0 past inputs 
and n 5 1 past outputs.
y(k) 5 .8y(k 2 1) 1 .3fx(k) 2 y(k 2 1)g3
 
Suppose the input x(k) consists of N 5 1000 samples of white noise uniformly dis-
tributed over f21, 1g. Let the number of grid point per dimension be d 5 8. Write 
a MATLAB program that performs the following tasks.
(a) Initialize the random number generator using f_randinit(1000). Create x(k), 
and use the DSP Companion function f_nonsys to compute a set of output 
bounds ym and yM such that ym # y(k) # yM. Use a safety margin of y 5 .1, 
as in (9.10.6). Then compute Dx and Dy. Print a, b, Dx, Dy, and the total num-
ber of grid points r.
(b) Plot the output y(k) corresponding to the white noise input x(k). Include 
dashed lines showing the grid values along the y dimension.
(c) Let f(u) denote the right-hand side of the nonlinear difference equation where 
u(k) 5 fx(k), y(k 2 1)gT. Plot the surface f(u) over the domain fa1, b1g 3 fa2, b2g.
9.47 Consider the following nonlinear discrete-time system which has m 5 0 past inputs 
and n 5 1 past outputs.
y(k) 5 .8y(k1) 1 .3fx(k) 2 y(k 2 1)g3
 
Let the range of inputs be 21 # x(k) # 1 and the number of grid points per dimen-
sion be d 5 10. Write a MATLAB program that does the following.
(a) Initialize the random number generator using f_randinit(1000). Create x(k), 
and use f_nonsys to compute a set of output bounds ym and yM such that 
ym # y(k) # yM. Use N 5 1000 points of white noise uniformly distributed over 
f21, 1g for the input and a safety margin of y 5 .1, as in (9.10.6). Print a, b, 
and the total number of grid points r.
(b) Use DSP Companion function f_rbfw with ic 5 1 to compute a weight vector 
w that makes the RBF network exact at the r grid points. Then use f_rbfy1 to 
compute the output y1(k) to a white noise test input with L 5 100 points uni-
formly distributed over f21, 1g. Compute the nonlinear system response y(k) 
to the same input. Plot the two outputs on one graph using a legend. Compute 
the percent error E and add this to the graph title.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

References and Further 
Reading
1. 
Ahmed, N. and Natarajan, T., Discrete-Time Signals and Systems, Reston: Reston, 
VA, 1983.
2. 
Bartlett, M. S., “Smoothing periodograms from time series with continuous  
spectra,” Nature, Vol. 161, pp. 686–687, May, 1948.
3. 
Bellanger, M., Adaptive Digital Filters and Signal Analysis, Marcel Dekker: New 
York, 1987.
4. 
Bellanger, M., Bonnerot, G., and Coudreuse, M., “Digital filtering by polyphase 
network: Application to sample rate alteration and filter banks,” IEEE Trans. 
Acoustics, Speech, and Signal Processing, Vol. 24, pp. 109–114, 1976.
5. 
Bequette, B. W., Process Control: Modeling, Design, and Simulation, Prentice Hall: 
Upper Saddle River, NJ, 2003.
6. 
Burrrus, C. S. and Guo, H., Introduction to Wavelets and Wavelet Transforms:  
A Primer, Prentice Hall: Upper Saddle River, NJ, 1997.
7. 
Candy, J. C. and Temes, G. C., Oversampling Delta-Sigma Data Converters,  
IEEE Press: New York, NY, 1992.
8. 
Chapman, S. J., MATLAB Programming for Engineers, Second Edition,  
Brooks/Cole: Pacific Grove, CA, 2002.
9. 
Chatfield, C., The Analysis of Time Series, Chapman and Hall: London, 1980.
10. 
Constantinides, A. G., “Spectral transformations for digital filters,” Proc. Inst. 
Elec. Engr., Vol. 117, pp. 1585–1590, 1970.
11. 
Cook, T. A., The Curves of Life, Dover, 1979.
12. 
Cooley, J. W. and Tukey, R. W., “An algorithm for machine computation of com-
plex Fourier series,” Mathematics of Computation, Vol. 19, pp. 297–301, 1965.
13. 
Cristi, R., Modern Digital Signal Processing, Brooks-Cole-Thomson Learning: 
Pacific Grove, CA, 2004.
14. 
Crochiere, R. E. and Rabiner, L. R., “Further considerations in the design of dec-
imators and interpolators,” IEEE Trans. Acoustics, Speech, and Signal Processing, 
Vol. 24, pp. 296–311, 1976.
15. 
Crochiere, R. E. and Rabiner, L. R., “Optimum FIR digital filter implementations 
for decimation, interpolation, and narrow-band filtering,” IEEE Trans. Acoustics, 
Speech, and Signal Processing, Vol. 23, No. 5, pp. 444–456, 1975.
761
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

762    References and Further Reading
16. 
Dorf, R. C. and Svoboda, J. A., Introduction to Electric Circuits, Wiley: New York, 2000.
17. 
Durbin, J., “Efficient estimation of parameters in moving average model,” 
Biometrika, Vol. 46, pp. 306–316, 1959.
18. 
Dwight, H. B., Tables of Integrals and Other Mathematical Data, Fourth Edition, 
Macmillan: New York, 1961.
19. 
Elliott, S. J., Stothers, I. M., and Nelson, P. A., “A multiple error LMS algorithm 
and its application to the active control of sound and vibration,” IEEE Trans. 
Acoustics, Speech and Signal Processing, Vol. ASSP–35, pp. 1423–1434, 1987.
20. 
Franklin, G. E., Powell, J. D., and Workman, M. L., Digital Control of Dynamic 
Systems, Second Edition, Addison-Wesley: Reading, MA, 1990.
21. 
Gerald, C. F. and Wheatley, P. O., Applied Numerical Analysis, Fourth Edition, 
Addison-Wesley: Reading, MA, 1989.
22. 
Gitlin, R. D., Meadors, H. C., and Weinstein, S. B., “The tap-leakage algorithm: 
An algorithm for the stable operation of a digitally implemented, fractional adap-
tive spaced equalizer,” Bell System Tech. J., Vol. 61, pp. 1817–1839, 1982.
23. 
Grimmett, G. R., Probability and Random Processes, Third Edition, Oxford 
University Press: Oxford, UK, 2001.
24. 
Grover, D. and Deller J. R., Digital Signal Processing and the Microcontroller, 
Prentice Hall, Upper Saddle River, NJ, 1999.
25. 
Hanselman, D. and Littlefield, B., Mastering MATLAB 6, Prentice Hall: Upper 
Saddle River, NJ, 2001.
26. 
Hassibi, B. A., Sayed, H., and Kailath, T., “H` optimality of the LMS algorithm,” 
IEEE Trans. on Signal Processing, Vol. 44, pp. 267–280, 1996.
27. 
Haykin, S., Adaptive Filter Theory, Fourth Edition, Prentice Hall: Upper Saddle 
River, NJ, 2002.
28. 
Hernandez, E. and Arkun, Y., “Stability of nonlinear polynomial ARMA models 
and their inverse,” Int. J. Control, Vol. 63, No. 5, pp. 885–906, 1996.
29. 
Ifeachor, E. C. and Jervis, B. W., Digital Signal Processing: A Practical Approach, 
Second Edition, Prentice-Hall: Harlow, UK, 2002.
30. 
Ingle, V. K. and Proakis, J. G., Digital Signal Processing Using MATLAB, Brooks/
Cole: Pacific Grove, CA, 2000.
31. 
Jackson, L. B., Digital Filters and Signal Processing, Third Edition, Kluwer 
Academic Publishers: Boston, 1996.
32. 
Jaffe, D. A. and Smith, J. O., “Extensions of the Karplus-Strong plucked-string 
algorithm,” Computer Music Journal, Vol. 7, No. 2, pp. 56–69, 1983.
33. 
Jameco Electronics Catalog, 1355 Shoreway Road, Belmont, CA, 94002–4100, 2014.
34. 
Janovetz, J. and Hanson, C., “Parks-McClellan algorithm for FIR filter design,” 
janovetz@coewl.cen.uiuc.edu, 1995.
35. 
Jansson, P. A., Deconvolution, Academic Press: Waltham, MA, 1997.
36. 
Jayant, N., Johnston, J., and Safranek, R., “Signal compression based on models 
of human perception,” Proc. IEEE, Vol. 81, No. 10, pp. 1385–1422, 1993.
37. 
Kailath, T., Estimating Filters for Linear Time-Invariant Channels, Quarterly 
Progress Rep., 58, MIT Research Laboratory for Electronics: Cambridge, MA,  
pp. 185–197, 1960.
38. 
Kaiser, J. F., “Digital Filters,” Chap. 7 of System Analysis by Digital Computer,  
F. F. Kuo and J. F. Kaiser, Eds., Wiley: New York, 1966.
39. 
Kaiser, J. F., “Nonrecursive digital filter design using the I0-sinh window function,” 
Proc. 1974 IEEE Int. Symp. on Circuits and Systems, San Francisco, CA, pp. 20–23, 
April, 1974.
40. 
Kuo, S. M. and Gan, W.-S., Digital Signal Processing: Architectures, Implementations, 
and Applications, Pearson-Prentice Hall: Upper Saddle River, NJ, 2005.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

References and Further Reading    763
41. 
Kuo, S. M. and Morgan, D. R., Active Noise Control Systems: Algorithms and DSP 
Implementations, Wiley: New York, 1996.
42. 
Lam, H. Y.-F., Analog and Digital Filters, Prentice-Hall: Englewood Cliffs,  
NJ, 1979.
43. 
Levinson, N., “The Wiener RMS criterion in filter design and prediction,”  
J. Math. Phys., Vol. 25, pp. 261–278, 1947.
44. 
Ljung, L. Morf, M., and Falconer, D., “Fast calculation of gain matrices for recur-
sive estimation schemes,” Int. J. Control, Vol. 27, pp 1–19, 1978.
45. 
Ludeman, L. C., Fundamentals of Digital Signal Processing, Harper and Row: New 
York, 1986.
46. 
Markel, J. D. and Gray, A. H., Jr., Linear Prediction of Speech, Springer-Verlag: 
New York, 1976.
47. 
Marwan, N., “Make Install Tool for MATLAB,” www.agnld.uni-potsdam.de 
/marwan/6.download/ whitepaper_makeinstall.html, Potsdam, Germany, 2003.
48. 
McGillem, C. D. and Cooper, G. R., Continuous and Discrete Signal and System 
Analysis, Holt, Rhinehart and Winston: New York, 1974.
49. 
Mitra, S. K., Digital Signal Processing: A Computer-Based Approach, Second 
Edition, McGraw-Hill Irwin: Boston, 2001.
50. 
Moorer, J. A., “About the reverberation business,” Computer Music Journal,  
Vol. 3, No. 2, pp. 13–28, 1979.
51. 
Moustakides, G. V., “Study of the transient phase of the forgetting factor RLS,” 
IEEE Trans. Signal Processing, Vol. 45, pp. 2468–2476, 1997.
52. 
Nilsson, J. W., Electric Circuits, Addison–Wesley: Reading, MA, 1982.
53. 
Noble, B., Applied Linear Algebra, Prentice Hall: Englewood Cliffs, NJ, 1969.
54. 
Oppenheim, A. V., Schafer, R. W., and Buck, J. R., Discrete-Time Signal Processing, 
Prentice Hall: Upper Saddle River, NJ, 1999.
55. 
Papamichalis, P., Digital Signal Processing Applications with the TMS320 Family. 
Theory, Algorithms, and Implementations, Vol. 3, Texas Instruments: Dallas TX, 
1990.
56. 
Park, S. K. and Miller, K. W., “Random number generators: good ones are hard to 
find,” Communications of the ACM, Vol. 31, pp. 1192–1201, 1988.
57. 
Parks, T. W. and Burrus, C. S., Digital Filter Design, Wiley: New York, 1987.
58. 
Parks, T. W. and McClellan, J. H., “Chebyshev approximation for nonrecur-
sive digital filters with linear phase,” IEEE Trans. Circuit Theory, Vol. CT-19,  
pp. 189–194, Mar., 1972.
59. 
Parks, T. W. and McClellan, J. H., “A program for the design of linear phase finite 
impulse response filters,” IEEE Trans. Audio Electroacoustics, Vol. AU-20, No. 3, 
pp. 195–199, Aug., 1972.
60. 
Porat, B., A Course in Digital Signal Processing, Wiley: New York, 1997.
61. 
Proakis, J. G. and Manolakis, D. G., Digital Signal Processing: Principles, 
Algorithms, and Applications, Second Edition, Macmillan Publishing: New York, 
1992.
62. 
Rabiner, L. R. and Crochiere, R. E., “A novel implementation for narrow-band 
FIR digital filters,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. 23, 
No. 5, pp. 457–464, 1975.
63. 
Rabiner, L. R., Gold, B., and McGonegal, C. A., “An approach to the approxima-
tion problem for nonrecursive digital filters,” IEEE Trans. Audio and Electroacoustic, 
Vol. AU-18, pp. 83–106, June, 1970.
64. 
Rabiner, L. R., McClellan, J. H., and Parks, T. W., “FIR digital filter design 
techniques using weighted Chebyshev approximation,” Proc. IEEE, Vol. 63,  
pp. 595–610, 1975.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

764    References and Further Reading
65. 
Rabiner, L. R. and Schafer, R. W., Digital Processing of Speech Signals, Prentice-
Hall: Englewood Cliffs, NJ, 1978.
66. 
Remez, E. Y., “General computational methods of Chebyshev approximations,” 
Atomic Energy Translation 4491, Vol. 198, p. 2063, 1934.
67. 
Roads, C, Pope, S. T., Piccialli, A., and DePolki, G., Editors, Musical Signal 
Processing, Swets & Zeitlinger: Lisse, Netherlands, 1997.
68. 
Schilling, R. J., Al-Ajlouni, A., Carroll, J. J., and Harris, S. L., “Active control of 
narrowband acoustic noise of unknown frequency using a phase-locked loop,” Int. 
J. Systems Science, Vol. 29, No. 3, pp. 287–295, 1998.
69. 
Schilling, R. J., Carroll, J. J., and Al-Ajlouni, A., “Approximation of nonlinear 
systems with radial basis function neural networks,” IEEE Trans. Neural Networks, 
Vol. 12, No. 1, pp. 1–15, 2001.
70. 
Schilling, R. J. and Harris, S. L., Applied Numerical Methods for Engineers Using 
MATLAB and C, Brooks-Cole: Pacific Grove, CA, 2000.
71. 
Schilling, R. J. and Lee, H., Engineering Analysis: A Vector Space Approach, Wiley: 
New York, 1988.
72. 
Shan, T. J. and Kailath, T., “Adaptive algorithms with an automatic gain control 
feature,” IEEE Trans. Circuits and Systems, Vol. CAS-35, pp. 122–127, 1988.
73. 
Shannon, C. E., “Communication in the process of noise,” Proc. IRE, Jan.,  
pp. 10–21, 1949.
74. 
Slock, T. T. M., “On the convergence behavior of the LMS and the normalized 
LMS algorithms,” IEEE Trans. Signal Processing, Vol. 41, pp. 2811–2825, 1993.
75. 
Steiglitz, K., A Digital Signal Processing Primer with Applications to Digital Audio 
and Computer Music, Addison-Wesley: Menlo Park, CA, 1996.
76. 
Strum, R. E. and Kirk, D. E., First Principles of Discrete Systems and Digital 
Signal Processing, Addison-Wesley: Reading, MA, 1988.
77. 
Treichler, J. R., Johnson, C. R. Jr., and Larimoore, M. G., Theory and Design of 
Adaptive Filters, Prentice-Hall: Upper Saddle River, NJ, 2001.
78. 
Tretter, S. A., Introduction to Discrete-Time Signal Processing, Wiley: New York, 1976.
79. 
Warnaka, G. E., Poole, L. A., and Tichy, J., “Active acoustic attenuators,” U.S. 
Patent 4,473906, Sept. 25, 1984.
80. 
Wasserman, P. D., Neural Computing: Theory and Practice, Van Nostrand 
Reinhold: New York, 1989.
81. 
Webb, A. and Shannon, S, “Shape-adaptive radial basis functions,” IEEE Trans. 
Neural Networks, Vol. 9, Nov. 1998.
82. 
Weiner, N. and Paley, R. E. A. C., Fourier Transforms in the Complex Domain, 
American Mathematical Society: Providence, RI, 1934.
83. 
Welch, P. D., “The use of fast fourier transform for the estimation of power spectra: 
A method based on time averaging over short modified periodograms,” IEEE Trans. 
Audio and Electroacoustics, Vol, AU-15, pp. 70–73, June, 1967.
84. 
Widrow, B. and Hoff, M. E. Jr., “Adaptive Switching Circuits,” IRE WESCON 
Conv. Rec., Part 4, pp. 96–104, 1960.
85. 
Widrow, B. and Stearns, S. D., Adaptive Signal Processing, Prentice Hall: 
Englewood Cliffs, NJ, 1985.
86. 
Wilkinson, J. H., Rounding Error in Algebraic Processes, Prentice Hall: Englewood 
Cliffs, NJ, 1963.
87. 
Woodbury, M., Inverting Modified Matrices, Mem. Rep. 42, Statistical Research 
Group: Princeton University, Princeton, NJ, 1950.
88. 
Zurada, J. M., Artificial Neural Systems, West Publishing: St. Paul, 1992.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Appendix 1
Transform Tables
xa(t 1 T ) 5 xa(t)
 xa(t) 5 o
`
k52`
ck exp1
j2kt
T 2
 ck 5 1
T#
T
xa(t) exp1
2j2kt
T 2
Trigonometric Form:
 xa(t) 5 a0
2 1 o
`
k51
ak cos1
2kt
T 2 1 bk sin1
2kt
T 2
 ak 5 2
T#
T
xa(t) cos1
2kt
T 2dt 5 2 Rehckj
 bk 5 2
T#
T
xa(t) sin1
2kt
T 2dt 5 22 Imhckj
Cosine Form:
 xa(t) 5 d0
2 1 o
`
k51
dk cos1
2kt
T
1 k2
 dk 5 Ïa2
k 1 b2
k 5 2ucku
 k 5  tan211
2bk
 ak2 5  tan211
2Imhckj
 Rehckj2
Fourier Series
765
1.1  
Complex Form:
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

766    Appendix 1  Transform Tables
Description
xa(t)
Fourier Series 
Odd square wave 
 sgn3sin1
2t
T 24 
4
 o
`
k51
 1
2k 2 1 sin3
2(2k 2 1)t
T
4 
Even square wave 
 sgn3cos1
2t
T 24 
 4
 o
`
k51
(21)k21
2k 2 1   cos3
2(2k 2 1)t
T
4
Impulse train 
 T(t) 
 1
T 1 2
T o
`
k51
 
cos1
2kt
T 2 
Even pulse train 
 a3cos1
2t
T 2 2 cos1
2 
T 24 
 2
T 1 4
T  o
`
k51
 
sinc1
2k
T 2 cos1
2kt
T 2 
Rectified sine wave 
usin1
2t
T 2u 
2
 2 4
 o
`
k51
 1
4k2 2 1  cos1
4kt
T 2
Sawtooth wave 
mod(t, T) 
1
2 2 1
 o
`
k51
 
1
k  sin1
2kt
T 2
Fourier Transform
Fourier transform (FT):
Xa(f)5
D#
`
2`
xa(t) exp(2j2ft)dt, f [ R
Inverse Fourier transform (IFT):
xa(t) 5#
`
2`
Xa(f) exp(  j2ft)df, t [ R
1.2
Table A1: Fourier 
Series Pairs
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.2 Fourier Transform    767
Entry 
xa(t)
Xa(f)
Description 
1 
 exp(2ct)a(t) 
 
1
c 1 j2f 
Causal exponential 
2 
 exp(2cutu) 
 
2c
c2 1 42f2 
Double exponential 
3 
 expf2(ct)2g 
 Ï expf2(fyc)2g
c
 
Gaussian 
4 
 exp( j2F0t) 
 a(f 2 F0) 
Complex exponential 
5 
 t exp(2ct)a(t) 
 
1
(c 1 j2f)2  
Damped polynomial 
6 
 exp(2ct) cos(2F0t)a(t) 
 
c 1 j2f
(c 1 j2f)2 1 (2F0)2 
Damped cosine 
7 
 exp(2ct) sin(2F0t)a(t) 
 
2F0
(c 1 j2f)2 1 (2F0)2 
Damped sine 
8 
 a(t) 
 1 
Unit impulse 
9 
 a(t) 
 a(f)
2
1
1
j2f 
Unit step 
10 
 1 
 a(f) 
Constant 
11 
 a(t 1 B) 2 a(t 2 B) 
 2B sinc (2Bf) 
Pulse 
12 
 2B sinc (2Bt) 
 a(f 1 B) 2 a(f 2 B) 
Sinc function 
13 
 sgn(t) 
 1
jf 
Signum function 
14 
 cos(2F0t) 
 a(f 1 F0) 1 a(f 2 F0)
2
 
Cosine 
15 
 sin(2F0t) 
 jfa(f 1 F0) 2 a(f 2 F0)g
2
 
Sine
Table A2: Fourier 
Transform Pairs, 
c . 0
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

768    Appendix 1  Transform Tables
Property 
 xa(t)  
 Xa(f) 
Symmetry 
 Real 
 Xa(f) 5 X*(2f) 
Even magnitude 
 Real 
 uXa(2f)u 5 uXa(f)u 
Odd phase 
 Real 
 /Xa(2f) 5 2/Xa(f) 
Linearity 
 ax1(t) 1 bx2(t) 
 aX1(f) 1 bX2(f) 
Time scale 
 xa(at) 
 1
uauXa1
f
a2 
Reflection 
 xa(2t) 
 Xa(2f) 
Duality 
 Xa(t) 
 xa(2f) 
Complex conjugate 
 x*a(t) 
 X*
a(2f) 
Time shift 
 xa(t 2 T) 
 exp(2j2f T )Xa(f ) 
Frequency shift 
 exp( j2F0t)xa(t) 
 Xa(f 2 F0) 
Time differentiation 
 dkxa(t)
dtk  
 (j2f)kXa(f) 
Frequency differentiation 
 tkxa(t) 
 1
1
22
k
 dkXa(f)
df k  
Time convolution 
 #
`
2`
x1()x2(t 2 )d 
 X1(f)X2(f) 
Frequency convolution 
 x1(t)x2(t) 
 #
`
2`
X1()X2(f 2 )d 
Cross-correlation 
 #
`
2`
x1()x*2(t 1 )d 
 X1(f)X*
2(f) 
Parseval 
 #
`
2`
xa(t)y*a(t)dt 
 #
`
2`
Xa(f)Y*
a(f)df 
 #
`
2`
uxa(t)u2dt 
 #
`
2`
uXa(f)u2df
Laplace Transform
Laplace transform (LT):
Xa(s) 5
D  #
`
0
xa(t) exp(2st)dt, Re(s) . c
Inverse Laplace transform (ILT):
xa(t) 5 1
j2#
c1j`
c2j`
Xa(s) exp(st)ds, t $ 0
1.3
Table A3: Fourier 
Transform Properties
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.3 Laplace Transform    769
Entry 
 xa(t)  
 Xa(s) 
 Description 
1 
a(t) 
 1 
 Unit impulse 
2 
a(t) 
 1
s 
 Unit step 
3 
tma(t)
 m!
sm11 
 Polynomial 
4 
exp(2ct)a(t) 
 
1
s 1 c 
 Exponential 
5 
 exp(2ct)tma(t) 
 
m!
(s 1 c)m11 
 Damped polynomial 
6 
 sin(2F0t)a(t) 
 
2F0
s2 1 (2F0)2 
 Sine 
7 
 cos(2F0t)a(t) 
 
s
s2 1 (2F0)2 
 Cosine 
8 
 exp(2ct) sin(2F0t)a(t) 
 
2F0
(s 1 c)2 1 (2F0)2 
 Damped sine 
9 
 exp(2ct) cos(2F0t)a(t) 
 
s 1 c
(s 1 c)2 1 (2F0)2 
 Damped cosine 
10 
 t sin(2F0t)a(t) 
 
4F0
fs2 1 (2F0)2g2 
 Polynomial sine 
11 
 t cos(2F0t)a(t) 
 s2 2 (2F0)2
fs2 1 (2F0)2g2 
 Polynomial cosine
Property
xa(t)
Xa(f)
Linearity 
 ax1(t) 1 bx2(t) 
 aX1(s) 1 bX2(s) 
Complex conjugate 
 x*(t) 
 X*(s*) 
Time scale 
 xa(at), a . 0 
 1
aXa1
s
a2 
Time multiplication
 txa(t) 
 2 dXa(s)
ds  
Time division 
 xa(t)
t  
 #
`
s
Xa()d 
Time shift 
 xa(t 2 T)a(t 2 T) 
 exp(2sT)Xa(s) 
Frequency shift 
 exp(2at)xa(t) 
 Xa(s 1 a) 
Derivative 
 dxa(t)
dt  
 sXa(s) 2 xa(01) 
Integral 
 #
t
0
xa()d 
 Xa(s)
s  
Differentiation 
 dkxa(t)
dtk  
skXa(s) 2o
k21
i50
 sk2i21 dixa(01)
dti
 
Convolution 
 #
t
0
xa()ya(t 2 )d 
 Xa(s)Ya(s) 
Initial value 
 xa(01) 
 lim
sS`
 
sXa(s) 
Final value 
 lim
tS`
 
xa(t) 
 lim
sS0
 
sXa(s),  stable 
Table A4: Laplace 
Transform Pairs
Table A5: Laplace 
Transform Properties
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

770    Appendix 1  Transform Tables
Z-transform
Z-transform (ZT):
X(z) 5
D o
`
k52`
x(k)z2k, r , uzu , R
Inverse Z-transform (IZT):
x(k) 5 1
j2 #
C X(z)zk21dz, uku 5 0, 1, Á
Entry 
 x(k)  
 X(z) 
 Description 
1 
 (k) 
 1 
 Unit impulse 
2 
 (k) 
 
z
z 2 1 
 Unit step 
3 
 k(k) 
 
z
(z 2 1)2 
 Unit ramp 
4 
 k2(k) 
 z(z 1 1)
(z 2 1)3 
 Unit parabola 
5 
 ak(k) 
 
z
z 2 a 
 Exponential 
6 
 kak(k) 
 
az
(z 2 a)2 
 Linear exponential 
7 
 k2ak(k) 
 az(z 1 a)
(z 2 a)3  
 Quadratic exponential 
8 
 sin(bk)(k) 
 
z sin(b)
z2 2 2z cos(b) 1 1 
 Sine 
9 
 cos(bk)(k) 
 
zfz 2  cos(b)g
z2 2 2z cos(b) 1 1 
 Cosine 
10 
 ak sin(bk)(k) 
 
az sin(b)
z2 2 2az cos(b) 1 a2 
 Damped sine 
11 
 ak cos(bk)(k) 
 
zfz 2 a cos(b)g
z2 2 2az cos(b) 1 a2 
 Damped cosine
1.4
Table A6: Z-transform 
Pairs
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.5 Discrete-time Fourier Transform    771
Property 
 x(k)
 X(z) 
Linearity 
 ax(k) 1 by(k) 
 aX(z) 1 bY(z) 
Complex conjugate 
 x*(k) 
 X*(z*) 
Time reversal 
 x(2k) 
 X(1yz) 
Time shift 
 x(k 2 r) 
 z2rX(z) 
Time multiplication 
 kx(k) 
 2zdX(z)
dz  
Z-scale 
 akx(k) 
 X(zya) 
Convolution 
 h(k) w x(k) 
 H(z)X(z) 
Correlation 
 ryx(k) 
 Y(z)X(1yz)
L
 
Initial value 
 x(0) 
 lim 
zS`
 
X(z) 
Final value 
 x(`) 
 lim
zS1
 
(z 2 1)X(z),  stable 
Discrete-time Fourier Transform
Discrete-time Fourier transform (DTFT):
X( f  ) 5
D o
`
k52`
x(k) exp(2jk2f T ), f [ R
Inverse discrete-time Fourier transform (IDTFT):
x(k) 5 1
fs#
fsy2
2fsy2
X(  f ) exp( jk2f T )df, uku 5 0, 1, 2, Á
Entry  
x(k) 
X(f) 
Description 
1 
(k) 
1 
Unit impulse 
2 
ak(k), uau , 1 
 exp( j2fT)
 exp( j2fT) 2 a
Exponential 
3 
k(a)k(k), uau , 1 
a exp( j2fT)
fexp( j2fT) 2 ag2
Linear exponential 
4 
2F0T  sinc (2kF0T) 
(f 1 F0) 2 (f 2 F0) 
Sinc function 
5 
(k 1 r) 2 (k 2 r 2 1) 
 sinf(2r 1 1)fg
 sin(f)
 
Pulse function
1.5
Table A7: Z-transform 
Properties
Table A8: Discrete-
time Fourier Transform 
Pairs 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

772    Appendix 1  Transform Tables
Property 
Time Signal 
DTFT
Periodic 
 General 
 X(f 1 fs) 5 X(f) 
Symmetry 
 Real 
 X(2f) 5 X*(f) 
Even Magnitude 
 Real 
 Ax(2f) 5 Ax(f) 
Odd Phase 
 Real 
 x(2f) 5 2x(f) 
Linearity 
 ax(k) 1 by(k) 
 aX(f) 1 bY(f) 
Complex conjugate 
 x*(k) 
 X*(2f) 
Time reversal 
 x(2k) 
 X(2f) 
Time shift 
 x(k 2 r) 
 exp(2j2rfT)X(f) 
Frequency shift 
exp( jk2F0T)x(k) 
 X(f 2 F0) 
Time scale 
 x(Nk), B 5 fs
2N 
 1
N X1
f
N2 
Multiplication 
 x(k)y(k) 
 #
fsy2
2fsy2
X()Y(f 2 )d 
Convolution 
 h(k) w x(k) 
 H(f)X(f) 
Correlation 
 ryx(k) 
 Y(f)X(2f)
L
 
Wiener-Khintchine 
 rxx(k) 
 Sx(f)
L  
Parseval
 o
`
52`  x(k)y*(k)  
 1
fs#
fsy2
2fsy2
X(f)Y*(f)df 
o
`
52`  ux(k)u2  
 1
fs#
fsy2
2fsy2
uX(f)u2df
Discrete Fourier Transform (DFT) 
Discrete Fourier transform (DFT):
X(i) 5
D  o
N21
k50
x(k) exp1
2jki2
N 2, 0 # i , N
Inverse discrete Fourier transform (IDFT):
x(k) 5 1
N o
N21
i50
X(i) exp1
jki2
N 2, 0 # k , N
1.6
Table A9: Discrete-
time Fourier Transform 
Properties
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.6 Discrete Fourier Transform (DFT)     773
Property 
Time Signal 
DFT 
Comments 
Periodic 
 General 
 X(i 1 N) 5 X(i) 
 
Symmetry 
 Real 
 X*(i) 5 X(N 2 i) 
 
Even magnitude 
 Real 
 Ax(Ny2 1 i) 5 Ax(Ny2 2 i) 
 N even 
Odd phase 
 Real 
 x(Ny2 1 i) 5 2x(Ny2 2 i) 
 N even 
Linearity 
 ax(k) 1 by(k) 
 aX(i) 1 bY(i) 
 
Time reversal 
 xp(2k) 
 X*(i) 
 Real x 
Circular shift 
 xp(k 2 r) 
 exp1
2j2ir
N 2X(i) 
 
Circular convolution 
 x(k) + y(k) 
 X(i)Y(i) 
 
Circular correlation 
 cyx(k) 
 Y(i)X*(i)
N
 
 Real x 
Wiener-Khintchine 
 cxx(k) 
 Sx(i) 
 
Parseval
 o
N21
k50
 x(k)y*(k) 
 1
N o
N21
i50
 
X(i)Y*(k) 
 
 o
N21
k50
 
ux(k)u2 
 1
N o
N21
i50
 
uX(i)u2 
Table A10: Discrete 
Fourier Transform 
Properties
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Appendix 2
Mathematical Identities
 j 5 Ï21
 z 5 x 1 jy
 z* 5 x 2 jy
 z 1 z* 5 2 Re(z) 5 2x
 z 2 z* 5 j2 Im(z) 5 j2y
 zz* 5 uzu2 5 x2 1 y2
Polar form:
 z 5 A exp( j)
 A 5 Ïx2 1 y2
  5  tan211
y
x2
 x 5 A cos()
 y 5 A sin()
Complex Numbers
774
2.1  
Rectangular form:
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Euler’s Identity
 exp(6 j) 5 cos() 6 j sin()
 cos() 5
 exp( j) 1 exp(2j)
2
 sin() 5
 exp( j) 2 exp(2j)
j2
 exp(6 jy2) 5 6j
Trigonometric Identities 
Analysis:
 cos2(a) 1 sin2(a) 5 1
 cos(a 1
2 b) 5 cos(a) cos(b) 1
2 sin(a) sin(b)
 sin(a 1
2 b) 5 sin(a) cos(b) 1
2 cos(a) sin(b)
 cos(2a) 5 cos2(a) 2 sin2(a))
 sin(2a) 5 2 sin(a) cos(a)
Synthesis:
 cos2(a) 5 1 1  cos(2a)
2
 sin2(a) 5 1 2  cos(2a)
2
 cos(a) cos(b) 5
 cos(a 1 b) 1  cos(a 2 b)
2
 sin(a) sin(b) 5
 cos(a 2 b) 2 cos(a 1 b)
2
 sin(a) cos(b) 5
 sin(a 1 b) 1 sin(a 2 b)
2
2.2
2.3
2.3 Trigonometric Identities    775
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

776    Appendix 2  Mathematical Identities
Inequalities 
Scalar:
 uabu 5 uau ? ubu
ua 1 bu # uau 1 ubu
Vector:
 uuxuu2 5 o
n
i51
x2
i
 uuAuu 5 max
n
i51 {uiu}
 det(I 2 A) 5P
n
i51
( 2 i)
 uux 1 yuu # uuxuu 1 uuyuu
 uxTyu # uuxuu ? uuyuu
 uuAxuu # uuAuu ? uuxuu
Uniform White Noise 
 Pv 5 E  fv2(k)g < 1
No
N21
i50
v2(k)
 Pv 5 b3 2 a3
3(b 2 a)  when  a # v # b
 Pv 5 c2
3  when 2c # v # c
2.4
2.5
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

777
Index
radial basis functions (RBFs) 726–733
scalar subscript 726
test set 732
vector subscript 725
Adaptive RBF networks 726–733
approximation error 730
constant interpolation property 729–730
example of 730
Gaussian RBF 727
orthoganal property 729
raised-cosine RBF 728–733
RBF network, defined 728
RBF network evaluation algorithm 731
RBF network training 731–732
Adaptive signal processing 667–740
active noise control 714–723
adaptive filter design 701–707
adaptive FIR filter design 703–707
adaptive function approximation 
723–734
adaptive transversal filters 394–396
channel equalization 669–670
cross-correlation 672–673
error signal 669
filtered-x LMS (FXLMS) method 
714–718
grid points 
leaky LMS method 698–700
least mean square (LMS) method 678–682
mean square error 671–677
modified LMS methods 692–701
motivation for 667–671
nonlinear discrete-time systems 82
A
Active noise control 7–9, 714–723
anti-noise 714
antisound 8
error signal 8
filtered past inputs 715
filtered-x LMS (FXLMS) method 714–718
introduction to 7–9
noise cancellation 9
noise reduction 718
primary noise 8
secondary-path identification 716–718
signal-synthesis method 719–722
signal-synthesis method, example of 
721–722
Adaptive filter 394–397, 404
defined 394
error signal 395
example of 395–396 
Adaptive FIR filter design 701–707
examples of 704–705, 706–707
linear-phase adaptive-filters 705–706
linear-phase symmetry condition 705
pseudo-filter output 702
pseudo-filters 701–703
residual phase shift 702
uniform relative weights 702
Adaptive function approximation 723–734
examples of 725–726, 732–733
grid element vertices 725
grid-point spacing 724
nonlinear functions 724–726
percent error 732
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

778    Index
Adaptive signal processing (Continued)
nonlinear system identification 734–739
normalized LMS method 692–694
radial basis functions (RBF) 726–733
recursive least squares (RLS) method 
708–713
signal prediction 670–671
system identification 668–669
variable step size LMS method 
Adaptive transversal filters 394–396, 
671–672
defined 394
mean square error 395, 671–672
signal flow graph 671
state vector 671
tapped delay line 671
weight vector 671
ADCs (see Analog-to-digital converters 
(ADCs))
Algorithms 
bilinear-transformation method 544
bit reversal 265
equiripple FIR filter 451
fast block convolution 274
FFT, decimation time 
FFT, input vector scrambling 266
FFT, problem domain 267
IFFT 268
lattice-form realization 475
minimum-phase allpass decomposition 
373
Parks-McClellan 447–451
raised-cosine RBF network evaluation 731
recursive least squares (RLS) 711
residue method 175
ripple size 449–450
successive approximation 45
windowed FIR filter design 431
zero-phase filter 366
Aliasing 10, 23–26
example of 25–26
Aliasing formula 23
Allpass filters 372–375, 403
allpass structure 372
definition of 372
minimum-phase decomposition 373–375
signal delay 
Amplitude modulation 22
Amplitude response 360–363, 422
Analog filters, classic 39, 528–540, 580
Butterworth filters 529–533
Chebyshev-I filters 533–536
Chebyshev-II filters 536–538
elliptic filters 538–539
summary of 540
Analog frequency transformations 547–551
digital bandpass filter, example of 549–550
discussion of 547–548
lowpass to bandpass, example of 548–549
table of 548
Analog processing 4–6
disadvantages of 6
Analog signal, defined 3, 12, 15
Analog-to-digital converters (ADCs) 4, 
43–47
circuits 43–47
defined 4
flash 46–47
multistage 
successive-approximation 44–46
Anti-aliasing filter 56–57, 60, 634–638
design of 56–57
examples of 
functions of 
order 57
Antisound, defined 7
AR (see Auto-regressive (AR) model)
Arcs 185
defined 185
label 185
ARMA (see Auto-regressive moving  
average (ARMA))
Auto-correlation 116, 289–298
average power 
circular, definition of 289
ergodic signal 289
example of 292
normalized 290
periodic signal 293, 296–297
power density spectrum 291–292
power density spectrum, example of 292
random variables statistically indepen-
dent 289
signal estimation 295–298
uncorrelated signals 290
white noise, of 289–291
zero-mean white noise 290
Auto-regressive moving average (ARMA) 
188, 204
Average power 6, 77, 283, 286
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index    779
B
Backward Euler approximation 84
Backward Euler difference 377–378
Bandlimited signal, definition of 24
Bartlett’s method 312–316
average periodogram 313
defined 313
periodic input, example of 315–316
white noise, example of 313–315
Base band, defined 24
BIBO (see Bounded-input bounded- 
output (BIBO))
Bilinear-transformation method 540–546, 
580
algorithm 544
analog frequencies 544
digital frequency range 544
discussion of 540–546
example of 545–546
frequency warping 543
prewarping 544
trapezoid rule integrator 541
Bilinear-transformation, defined 542
Bin frequencies, defined 248
Binary number representation 478–479
fixed-point representation 478–479
floating-point representation 478
Bipolar, defined 478
Black box, understanding of 203, 668
Block diagrams 95–97
delay block 96
dimension 96
moving average (MA) model
tapped delay line 96
Bounded, definition of 18
Bounded-input bounded-output (BIBO)  
120, 219
Bounded signal 18, 76
Butterworth filters 
3-B cutoff 529
ciel function 531
denominators of normalized 533
design of 529–533
design, example of 531–532
first-order, example of 36–37
frequency transformation 
introduction to 529
lowpass, defined 532, 635
magnitude response 
maximally flat 530
normalized, defined 36
normalized lowpass 530
second-order, example of 38
transfer function, example of 35, 533
C
Cascade form, FIR filters 472–474
example of 474
Cascade form, IIR filters 558–561
block diagram 560
example of 560–561
realization 559
Cascade-form realization 348–350, 
472–474, 559
Case studies
anti-aliasing filter design 56–57
bandstop filter design 492–495
distortion due to clipping 324–325
echo detection 128–130
Fibonacci sequence and golden ratio 
215–217
high-pass elliptic filter 400–401
home mortgage 126–128
identification of a chemical process 
743–744
reverb filter 574–578
sampling rate converter 649–651
satellite altitude control 148–149, 211–214
signal detection 322–323
speech compression 214–215
video aliasing 58
Causal exponential, example of  80, 241–242
Causal signal, definition of 14
Channel equalization, adaptive filters
Chebyshev-I filters 
design of 533–534
example of 536
magnitude response 534
polynomials 534
ripple factor 533, 535
Chebyshev polynomials 382
Chebyshev-II filters 
defined 536
design of 536–538
magnitude response 537
ripple factor 536
Characteristic polynomial 87–90, 131
complex conjugate roots 89
multiple roots 89
simple roots 88
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

780    Index
Chirp, multi-frequency, defined 128
ciel function 531
Circular convolution 105–107, 259–260
definition of 106
example of 107
matrix 107 
periodic extension 105
periodic ramp 105 
Circular cross-correlation 116–118, 133
circular convolution, relationship to 
118–119
definition of 116
normalized 117
Coefficient quantization error, IIR filters 
563–565
Colored noise 514–516
defined 515
Comb filters 184–185, 522–525
design, IIR 522–525
gain 184, 524
inverse 523
transfer function, example of 184–185
Complex poles, example of  173
Continuous-time signal
defined 9, 11, 15, 59
reconstruction of 27–34, 60–61
sampling of  21–26, 60
Continuous-time system, defined 16
Controller gain, defined 
Conversion circuits 41–47
analog-to-digital 43–47
digital-to-analog 41–43
Converters 41–47
analog-to-digital (ADCs) 4, 43–47
digital-to-analog (DACs) 4, 41–43
flash 36–47
successive-approximation  
44–46
Convolution 70, 102–112, 130–131
circular 105–107, 132, 259–260
commutative operator 103
cross-correlation 71
deconvolution 110–111, 132
example of 104–105, 107
linear 102–104, 132
operator 103
polynomial arithmetic 111–112
table of equations 109
zero padding 107–109
zero-state output 103 
Correlation 113–119, 277–280, 672–73 
(see also Auto cross-correlation; 
Cross-correlation; Fast convolution; 
Periodic Signals)
auto 133
circular cross-correlation 116–118
cross-correlation 113–118
defined 113, 116
fast linear cross-correlation 277–280
fast 276–280
linear cross-correlation 113–116, 132–133
periodic signals from noise, extracting 
292–298
radar, range measurement with 
signal shape 114
Correlation LMS method 694–697
discussion of 694–695
estimate 695
example of 696–697
sleep mode 697
smoothing parameter 697
step size 694
Cross-correlation 113–119
auto-correlation 116, 133
circular 116–118, 133
definition of 113, 116
discussion of 
example of 279–280
fast linear 277–280
fast linear, computational effort 278
linear, example of 114–115
normalized linear 116
D
DACs (see Digital-to-analog converters (DACs))
Data windows 307–310
rectangular 307
spectral leakage 307
table of functions 308
DC wall transformer 236–237
Decibel (dB), defined 301
Decibel (dB) scale 300–301
Decimator 389, 404, 599
defined 389, 599
Deconvolution 110–111
discussion of 110
example of 110–111
Design, FIR filters 417–499, 667–671
adaptive 667–671
advantages 496
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index    781
differentiators 377–378, 454–457
equalizer 466–469
finite word length effects 477–489
first-order differentiator 
frequency-sampling method 434–441, 497
Hilbert transformers 378–381, 404, 457–459
lattice form 474–476
least-squares method 441–45, 466, 497
linear-phase form 471–472, 496
motivation for 417–422
numerical differentiatiors 418–420
optimal equiripple 445–454, 497
quadrature design 460–468
realization structures 470–476
second-order differentiator 
signal-to-noise ratio 420–422
windowed filter algorithm 431
windowing method 422–434, 496
Design, IIR filters 
analog filters, classic 528–540
Bessel filter 360
bilinear-transformation method 540–546
Butterworth filters 529–533
Chebyshev-I filters 533–536
Chebyshev-II filters 536–538
colored noise 514–516
comb filters 522–525
digital frequency transformation 551–552
elliptic filters 538–539
finite word length effects 562–572
FIR filters, comparison to 
frequency transformations 547–552
motivation for 511–516
notch filter 519–522
parameters 526–528
pole-zero placement, by 516–525
power gain 566
realization structures 553–561
roundoff noise 566
tunable plucked-string filter 512–515
zero placement 564
Design parameters, IIR filters 
discrimination factor 527
discussion of 526–528
example of 527–528
prototype 526
selectivity factor 527
squared magnitude response 28
table of 527 
DFT (see Discrete Fourier transform (DFT))
Difference equation 70, 86–95
Differentiators 377–378, 418–420, 454–457
backward Euler 419
discussion of  377–378, 418–420
design 454–456
example of 455–456
noncausal 457
numerical 418–420
second-order backward 419
Digital frequency transformations 551–552
Digital oscillator 381–384
generation of periodic output 384
Digital signal, defined 3, 12
Digital signal processing (DSP) 4–6
advantages of 5
Digital-to-analog converters (DACs) 4, 41–43
bipolar 41
circuits 41–43
defined 4
reference voltage 42
signal conditioning circuit 43
unipolar 41
Direct form I realization 553–554
Direct forms, FIR filters 
linear-phase form 359–360, 403
tapped-delay line 470
transposed tapped delay line 470–471
Direct forms, IIR filters 553–556
canonic representation 555
direct form I 553–554
direct form II 554–555
transposed direct form II 555–556
Discrete Fourier transform (DFT) 7, 
238–239, 247–262, 327
Bartlett’s method 312–316
bin frequencies 327
correlation with 260
definition of 249
discrete spectra 252–254
discrete-time frequency response 298–301
fast Fourier transform (FFT) 328
Fourier series 252–255
frequency response 238, 298–301
inverse discrete Fourier-transform 
(IDFT) 249
matrix formulation 250
orthogonal properties 249
Parseval’s theorem 244–245
power density spectrum (PDS)  
evaluation 261, 327
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

782    Index
Discrete Fourier transform (Continued)
power signal 252 
properties 255–262
roots of unity  249
time-reversal property 365
Discrete Fourier transform (DFT)  
properties 241, 243–247
circular convolution 259–260
circular correlation 260
circular shift 259
example of 250–251
frequency shift 243
linearity 258
magnitude spectrum 253
midpoint symmetry 256–267
Parseval’s identity 244, 260–261
periodic 255–256
periodic extension 258
phase spectrum 253
symmetry 241
table of 245, 257, 262
time reversal 258–259
time shift 243
Wiener-Khintchine theorem 245
Discrete-time Fourier transform (DTFT) 
239–247, 326
basic pairs 247
definition of 239
frequency shift property 243–244 
inverse DTFT 246
Parseval’s identity 244
properties of 243–247
time shift property 243
Wiener-Khintchine theorem 245
Discrete-time signal 3, 11, 15, 59,  
74–82, 130
absolutely summable 76
bounded 76
causal 75
classification 81
defined 3, 11, 15, 59
energy 77–78
finite 74
infinite 74
noncausal 75
periodic 75
power 77–78
spectrum, example of 253–254
square summable 77
unbounded 76
Discrete-time system analysis 70–121, 
147–209
alternative presentations of 187
block diagrams 95–97
convolution 70, 102–112
correlation 113–119
difference equations 86–95, 130–131
home mortgage as example of 71–73, 
126–128
finite signal 74, 77
frequency response 195–203
impulse response 97–102, 131–132
infinite signal 74
inverse Z-transform 166–177, 218
motivation for 70–72, 147–151
range measurement with radar as  
example of 72–73
signal flow graphs 185–188
signals 74–82 
software applications 209–217
stability 188–195
transfer functions 177–185
Z-transform pairs 151–159
Z-transform properties 159–166
Discrete-time systems 16, 70, 82–86
active 86
causal 83–85
defined 16, 70
example of 83–85, 85–86, 148–151, 
192–192
linear 82
linear time-invariant 86
lossless 86
marginally unstable 191
noncausal 83–85
nonlinear 82
passive 86
principle of superposition 82
stable 85–86
unstable 85–86
unstable transfer function, example of 
191–192
DSP (see Digital signal processing  
(DSP))
DSP Companion,  34–35, 47, 48–56, 
93–94, 102, 109
documentation 50–51
driver program 48–49
installation of 48
menu options 48–49
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index    783
DSP Companion functions 50–51
f_adc 47
f_analbank 624
f_bilin 547
f_blockconv 276
f_butters 540
f_butterz 552–553
f_cascade 476–477, 561
f_cheby1s 540
f_cheby1z 552–553
f_cheby2s 540
f_cheby2z 552–553
f_conv 109, 276
f_corr 119, 280
f_dac 47
f_decimate 607–608
f_elliptics 540
f_ellipticz 552–553
f_filtcas 477, 562
f_filter0 93–94
f_filter1 571–572
f_filtlat 477
f_filtpar 562
f_firdiff 459
f_firideal 433
f_firls 445
f_firparks 453–454
f_firquad 469–470
f_firsamp 441
f_firwin 433–434
f_freqs 34, 301
f_freqz 202–203, 301, 571–572
f_fxlms 723
f_hilbert 459
f_idarma 208–209
f_iircomb 525
f_iirinv 525
f_iirnotch 525
f_iirres 525
f_impulse 102, 571–572
f_interpol 607–608
f_lattice 476–477
f_lms 682
f_minall 376–377
f_nonsys 739–740
f_parallel 561, 
f_pds 319
f_randg 288
f_randinit 288
f_randu 288
f_rateconv 607–608
f_rbfw 734
f_rbfy0 734
f_rby1 739–740
f_reverb 577
f_rls 713
f_spec 254
f_specgram 311–312
f_sigsyn 723
f_synbank 624
f_timedemux 629
f_timemux 629
f_zerophase 368
pzplot 102
DSP Companion GUI modules 
g_adapt 740–742
g_correlate 124–126
g_filters 397–400
g_fir 489–492
g_iir 572–574
g_multirate 646–649
g_reconstruct 54–56
g_sample 52–54
g_spectra 320–322
g_sysfreq 209–211
g_systime 121–124
DTFT (see Discrete-time Fourier  
transform (DTFT))
E
Echo detection, example of 128–130
Eigenvector 686
Elliptic filters 538–539
complete elliptic integral 539
design of 538–539
example of 539
magnitude response 538
order 539
Energy density 244
Energy signal, definition of 77
Equalizer 466–469
design 467
equalized output 467
equalized system 467
example of 467–469
magnitude equalization 466
optimal delay 466–467
Equalization, defined 376
Equiripple filter 447–454
alternation theorem 447
Chebyshev filters as 535
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

784    Index
Equiripple filter (Continued)
defined 447
equiripple FIR filter algorithm 451
example of 451–453
optimal, FIR 445–454
order 451
Ergodic property 672
Euler’s identity 89
Even symmetry, example of 361
Expected value, definition of 3
F
Fast block convolution 273–275
algorithm 274
overlap-add method 273
example of 274–275
Fast convolution 269–276
block 273–275
computational effort 270–271
discussion of 269–271
example of 272, 274–275
linear 272
zero padding 270
Fast Fourier transform (FFT) 235, 262–269
algorithm, IFFT 268
alternative implementations 268
bit reversal algorithm 265
complex FLOPs 262
computational butterfly 263
computational effort 262, 266–268
decimation time in 262–266
even-odd decomposition 264
input vector scrambling 266
scrambled order 265
signal flow graphs 264–265
FFT (see Fast Fourier transform (FFT))
Fibonacci sequence 215–216
Filter banks 391–393, 404, 616–624
analysis 392, 616–618
fully decimated 617
MP3 files, example of 618–620
narrowband 391–393
perfect reconstruction 624–629
polyphase representation 621–623
subfilter design 620–621
synthesis 393, 616–618
uniform DFT 
Filter specifications 
design parameters 527
linear design 352–357
linear-phase 471–472
logarithmic design 357–359
lowpass 402, 526
passband 347
stopband 347
transition band 348
Filter structures 348–351, 470–476, 581
cascade-form realization 348–350, 472–474
direct form II realization 348 
direct form 581
finite word length effects 477–489, 
572–572
quantization error 351
realization structures 470–476, 553–561
Filtered-x LMS (FXLMS) method 714–718
discussion of 714–718
example of 718
filtered past inputs 715
noise reduction 718
secondary-path identification 716–718
Filters 19, 345–401, 417–489, 511–572 
(see also Design; Filter specifications; 
Filter structures; Finite word length 
effects; Linear-design specifications; 
Realization structures)
adaptive FIR 
allpass 372–375, 403
analog, classic 39
anti-aliasing 35–39
anti-imaging 39–41, 602
Butterworth 35,
Chebyshev-I 533–536
Chebyshev-II 536–538
comb 522–525
cutoff frequency 35
design, digital 346
digital anti-aliasing 599
digital anti-imaging 602
elliptic 538–539
equiripple 447–454
FIR finite word length effects 477–489
FIR, realization structures 470–476
first-order, example of 36–37
frequency-sampled 434–436
frequency-selective 351–359
gain 351
highpass elliptic 400–401
IIR, decomposition of 373
IIR, finite word length effects 562–572
IIR, realization structures 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index    785
linear-phase 359–360, 403
linear-phase FIR 362
linear-phase response 351
lowpass 346
minimum-phase 368–371
minimum-phase 368–371
notch 6-7, 384–387, 404
optimal equiripple FIR filters 445–454
pseudo-filters 396–397
quadrature 377–384, 404
resonators 387–388
second-order, example of 38
smoothing 39
specifications 346–348
tunable plucked-string 512–514
sero-phase 365–368 
Finite word length effects, FIR 477–487, 497
binary number representation 478–479
discussion of 477–489
linear-phase block 485–486
overflow 486–489
power of roundoff noise
quantization error 479–482, 497
quantization level 478, 497
quantization operator 479
roundoff error 486–487
scaling 486–489
unit circle zeros 484–485
Finite word length effects, IIR 563–565, 
570–572
coefficient quantization error 563–565
discussion of 572–572
limit cycles 570–572
overflow 567
roundoff error 566
roundoff noise 566
scaling 567–568
Finite-impulse response (FIR) 97–99, 191, 
403
filter design 417–499
filter realization structures 470–476
IRR filters, comparison to 191–192
transfer function 191 
FIR (see Finite-impulse response (FIR))
Flash converters 46–47
Folding frequency 26
Fourier series 235–236, 252–255
coefficients 252–253
introduction to 235–236
truncated form 235
Fourier transform 18, 234–280
introduction to 234–239
polar form 19 
short-term 307
table of 255 
Frequency-division multiplexing, defined 
381, 393, 404, 616
Frequency ranges, table of 243
Frequency resolution 304–306, 328
defined 304
example of 304–306
Rayleigh limit 304, 328
Frequency response 195–203, 351
continuous-time system, definition of 
decibel scale (dB) 300–301
definition of 19, 60
discrete Fourier transform (DFT) 298–301
discrete-time 219
discrete-time, example of 399–300
discrete-time system, definition of 195
example of 199–200, 299–300
gain 198
magnitude response 19, 60, 196, 219, 351
periodic inputs 200–202
phase response 19, 60, 197, 219, 351
proposition 198
running average filter 299–300
sinusoidal inputs 197–200
steady-state response 201–202
Frequency sampling 434–436
Frequency transformations 547–552 
analog 547–551
digital 551–552
Frequency-sampled filter 434–436
type 1 435
type 2 435
type 3 440
type 4 440 
Frequency sampling 434–436
Frequency-sampling method 434–441
frequency-sampled filter, even 
frequency-sampled filter, odd 
lowpass filter, example of 436
transition-band optimization 437–441
FXLMS (see Filtered-x LMS (FXLMS) 
method)
G
Gaussian white noise 285–288
definition of 286
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

786    Index
Gaussian white noise (Continued)
example of 286–288
power of zero-mean 290
probability density 285 
Geometric series 78–79
defined 78
example of 78–79
Gibb’s phenomenon 426
Golden ratio 216–217
Graphical user interface (GUI), defined 2
GUI (see Graphical user interface (GUI))
H
Hamming window 307, 308, 309, 310, 312, 
319, 390
Hanning window 307, 308, 311, 317, 318, 319
Harmonic forcing 192
Hilbert transformers 378–381, 404, 457–459
complex signal 380
design 457–459
discussion of 378–381
example of 457–459
half-band signal 380
magnitude response graph 380
phase-quadrature signals 380
sign function 379 
I
Ideal lowpass filter, example of 20–21
L’Hospital’s rule 21
sinc function 20, 21
IDFT (see Inverse discrete Fourier- 
transform (IDFT))
IIR (see Infinite-impulse response (IIR))
IIR coefficient quantization error, exam-
ple of 565
Impulse response 19, 97–102, 131–132, 
403, 579
continuous-time system, definition of 19
discrete-time system, definition of 97, 189
discrete-time system, example of 98–99, 
100–101
example of, FIR system 98–99
example of, IIR system 100–101
finite, definition of 98
FIR systems 97–99
IIR systems 99–101
linear time-invariant system, definition 
of 97
method 189
Infinite-impulse response (IIR) 191–192, 
511–572
filter, discussion of 511–572
filter realization structures 553–561, 581
finite word length effects 562–572
FIR filters, comparison to 
minimum-phase form 403
system, discussion of 191–192
Initial condition 87, 131
Initial value theorem 164–165, 175
Input quantization error, FIR filters 479–482
Input quantization noise, example of 
481–482
Integer decimator 600–601
defined 600
example of 600–601
Integer interpolator 601–603
defined 601
example of 602–603
Interpolator 389, 404, 601–602, 612–614
defined 389, 601
integer 601
sampling rate 601–602
polyphase interpolator 612–614
Inverse discrete Fourier-transform (IDFT) 
240–251
defined 240, 249
example of 246, 251
matrix formulation of 250
periodic property 240 
Inverse systems 376
Inverse Z-transform 166–177, 218
impluse response method 168
noncausal signals 166–167
partial fraction method 169–173
residue method 174–177
synthetic division method 167–168
J
Jury table 192–193
Jury test 192–195, 219
example of 193–194
K
Kaiser windows 431–432
L
Laplace transform 22–23, 61
definition of 22
disussion of 22–23
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index    787
Latency, defined 5
Lattice form, FIR filters 474–476
discussion of 474–475
example of 475–476
realization algorithm 475
reflection coefficient 475
Leaky LMS method 698–700
augmented mean square error 698
discussion of 698–699
example of 699–700
leakage factor 698
penalty function 698
Least mean square (LMS) method 
678–682 (see also Filtered-x LMS 
(FXLMS) method; Leaky LMS 
method; Mean square error;  
Normalized LMS method)
filtered-x LMS (FXLMS) method 
convergence 684 
correlation method 694–697
defined 679
excess mean square error 689–691
introduction to 678–681
leaky method 698–700
learning curve 686
misadjustment factor 690
nonlinear LMS method 732
normalized method 692–694
normalized method, example of 693–694
performance analysis of 682–691
recursive LMS method 708–713
revised step size, example of 685–686
sleep mode 695
smoothing parameter 695
steepest descent, method of 678
step size 682–685
step size, example of 684–685
system identification, example of 680–681
time constant 687–688
unbiased estimate 680
weight variation vector 683
Least-squares method 204–205, 395, 441–445
bandpass filter, example of 444
discussion of 204–205
error criterion 205
normal equations 443
parameter vector 204
pseudo-inverse 443
residual error 205
state vector 204, 394
uniform weighting 442
weighting function 441
Limit cycles, IIR filters 570–571
discussion of  570–571
example of 570–571
Linear auto-correlation, definition of 116
Linear convolution 102–104
definition of 103
example of 104–105
operator properties 104
Linear cross-correlation 113–116
example of, 114–115
normalized 116
Linear design specifications, filters 352–357
example of 355–357
ideal magnitude responses 352
logarithmic terms, in 357–359
normalized frequency 356
Paley-Wiener theorem 352 
Linear-phase block, FIR 485–486
Linear-phase filters 359–360, 403, 471–472, 496
delay line 359
definition of 360, 471
group delay 359–360
impulse responses of 424
symmetry condition 363, 403, 496
types of 362–363, 423–424, 496
zeros 363–365
Linear system, definition of 16
LMS (see Least mean square (LMS) method)
Logarithmic design specifications, filters 
357–359
decibel (dB) scale 357, 403
example of 358–359
linear specifications 352–356
M
MA (see Moving-average (MA) model)
Magnitude equalizer 376
Magnitude response 196, 402
discrete-time system 196
Mathematical identities 774–776
complex numbers 774
Euler’s 775
inequalities 776
trigonometric 775
uniform white noise 776
MATLAB, elements of 12, 48–51, 88, 123, 
168, 206, 209, 258, 320, 581, 737
DSP Companion and 48–51
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

788    Index
MATLAB, functions
abs 34
base2dec 726
besseli 431
case1_1 57
case3_1 213
case3_2 215
case3_3 217
conv 104
decimate 607
deconv 112
dec2base 726
ellipke 539
fft 269, 306
fftshift 632
filter 94, 97, 128, 168, 189, 572
freqz 301
interp 607
max 73, 129, 192
mod 105, 258
nextpow2 270, 306, 399
peaks 732, 733
phi 203
plot 203
poly 123
pzmap 102
rand 74, 282
randn 74, 285
resample 607
residue 176–177
roots 94
stem 82, 168
subplot 203
xcorr 119, 280
Matrix formulation 250
DFT, of 250
IDFT, of 250
Mean, defined 282
Mean square error 689–691
adaptive-transversal filters  
395, 691
auto-correlation matrix 674
average power 674
cross-correlation vector 674
cross-correlation 672–673
defined 673, 674
desired output 672
ergodic signal 672
excess, example of 690–691
minimum 689
optimal weight vector, example of 675–677 
random cross-correlation 672
stationary signals 673
statistically independent signals 673
Toeplitz matrix 674
uncorrelated signals 673
white noise, example of 677 
Wiener solution 675
Minimum-phase decomposition 373–375
algorithm 373
discussion of 373
example of 374–375
Minimum-phase filters 368–371
decomposition 
definition of 369
discussion of 368–369
example of 369–371
maximum-phase filter 
mixed-phase filter 
Moving-average (MA) model 96
MP3 file, example of 618–620
Multirate signal processing 389, 595–652
anti-aliasing filters 634–638
anti-imaging filters 642–643
decimator factor 615
down-sampling 599
filter banks 616–624
frequency bands 616 
integer sampling rate converters 599–603
intersample delay systems 598
motivation for 595–598
narrowband filter 596–598, 614–616
narrowband filter bank 596
narrowband filter, example of 615–616
oversampled A-to-D converters 634–642
oversampled D-to-A converters 642–646
passband equalization 644–646
perfect reconstruction filter banks 
624–629
polyphase decimator 608–611
polyphase filters 608–614 
polyphase interpolator 
polyphase representation 621–623
rational sampling rate converters 
603–607
sampling rate decimator 599–601
sampling rate interpolator 601–603
sigma-delta ADC638–642
transmultiplexors 630–634
up-sampling 602 
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index    789
N
Narrowband filter 387–393, 404, 596–598, 
614–616
banks 391–393, 596
definition 597
example of 390, 615–616
Noise 7–9, 514–516
active control 7–9, 
colored noise 514–516
periodic signals from, extracting 292–298
Noise power 
oversampling ADC 635
roundoff error, IIR filters 566
Nonlinear system 734–739
defined 16
dimension 735
domain 736
example of 737–739
identification 734–739
nonlinear ARMA system 734–735
radial basis function (RBF) model 736
state vector 735
Normalized LMS method 692–694
defined 693
discussion of 692–693
example of 693–694
step size 692
Normalized lowpass filter, defined 
Normalized mean square error 
Notch filter 7, 384–387, 519–522, 579
bandwidth 385
defined 7, 519
design of 519–520
DSP application, as 6–7
example of 385–386, 520–22
frequency 386, 519–520
gain 520
inverse comb386
O
Odd symmetry, example of  361–362
Offline processing 83
Optimal equiripple FIR filters 445–454
alternation theorem 447
Chebyshev polynomials 446
equiripple filter, defined 447
equiripple FIR filter algorithm 
minimax error criterion 445–447
minimax optimization criterion 448
optimal amplitude response 448
Parks-McClellan algorithm 447–453
ripple size algorithm 449–450
Overflow 487–489, 569–570
error 487
example of 488–489, 569–570
FIR filters 487–489
IIR filters 567
Oversampling, defined 27, 56, 58
Oversampled ADC 634–638
aliasing error scale factor 637
discussion of 634–637
effective precision 637
example of 637–638
noise power 635
power gain 636
quantization level 635
quantization noise 636
Oversampled DAC 642–646
aliasing error scale factor 645
anti-aliasing filters 642–643
defined, 643
discussion of 642–645
equivalent precision 643
example of 645–646
magnitude equalizer 645
magnitude response 644
passband equalization 644–646
quantization noise 643
P
Parallel form, IIR filters 556–558
discussion of 556–558
example of 558
indirect form 
realization 557
Parks-McClellan algorithm 447–453
Parseval’s theorem 244–245, 260–261
Partial fraction method 169–173
complex conjugate pairs 172
complex poles 172–173
example of 170, 171–172, 173
multiple poles 170–172
residue 169
simple poles 169–170
Passband, defined 347, 351
Passband ripple, defined 353, 402, 579
PDS (see Power density spectrum (PDS))
Perfect reconstruction filter banks 624–629
approximate reconstruction 629
defined 627
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

790    Index
Perfect reconstruction filter banks (Continued)
FIR prototype, example of 627
ideal lowpass prototype, example of 
628–629
time-division demultiplexor 625
time-division multiplexing 624–626
time-division multiplexor 625
Perfect reconstruction, frequency domain, 
defined 628
Periodic inputs 200–202
Periodic property, DFT 
Periodic signal estimate 
Periodic signals 75–76, 292–298
estimating the period of noisy 293
estimation 295–298
example of 75–76, 296–297
extracting from noise 292–298
period estimation 294–295
periodic circular auto-correlation 293
Periodic impulse train 21
Periodogram, defined 312–313
Phase offset 361
Phase response, discrete-time system 197
Phase shift 19, 198
Phase spectrum defined,
Phonemes, defined 150, 214
Piecewise constant, defined 
Pitch, defined 150, 214, 514
Pixels 10, 
Pole locations, IIR filters 563–564
Pole-zero placement, filter design by 516–525
comb filters 522–525
notch filter 519–522
pole radius 517
resonator 516–519
resonator filter 516
Poles, discrete-time system 101, 151, 217
Polynomial arithmetic 111–112
division 111
example of 112
multiplication 111
Polynomials, Chebyshev 
Polyphase decimator 608–611
discussion of 608–611
filters, example of 611
polyphase decomposition 609
polyphase representation 609
Polyphase interpolator 612–614
discussion of 612–614
polyphase decomposition 612
polyphase representation 612
Power density spectrum (PDS) 261, 292
discussion of 261
example of 292
Power density spectrum (PDS) estimation 
312–319, 329
average periodogram 313
Bartlett’s method 312–316
bin frequencies 312
leakage phenomenon 316
modified average periodogram 317
mth subsignal 312
periodogram 312–313
Welch’s method 316–319
window functions 
Power signal, definition of 77, 81
Prefilters and postfilters 35–41
anti-aliasing filter 35–39, 634–638
anti-imaging filter 39–41, 61
Butterworth filter 35, 36
cascade connection 39
cutoff frequency, defined 35
first-order filter, example of 36–37
normalized Butterworth filter 36
second-order filter, example of 38
Principle of superposition 82
Prototype filter, defined 526, 579, 620
Pseudo-filters 396–397
defined 397
design of 
examples of 
linear-phase 
Q
Quadrature filters 377–384, 460–468, 497
amplitude response 462
design 462–466
equalizer 466–469
example of 463–464
frequency-selective filter, example of 465
generalized least-squares 466
phase quadrature 377
quadrature pair 460
quadrature pair, example of 461
residual phase 462
transfer function 462
Quantization 12–14, 59
error 
level 12, 59
noise power 14
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index    791
noise, defined 13 
noise, example of 14
Quantization error 479–486, 564
coefficient, FIR filters 482–485
coefficient, FIR filters, example 483–484
coefficient, IIR filters 
discussion of 479–484
input, FIR filters 479–482
input quantization noise 480–481
linear-phase block, FIR filters 485–486
magnitude-response error 483
pole locations, IIR filters 
system power gain 481
unit circle zeros, FIR filters 484–485
zero placement, IIR filters 564
Quantization level, defined 12, 399, 563
Quantization operator, definition of 12, 479
Quantized signal 12
R
Radar, range measurement with 72–73
Radial basis functions (RBF) 726–739
adaptive networks 731–733
approximation error 730
compact support 728
example of 730, 723–733
Gaussian 727
Gaussian interpolation property 729
nonlinear least mean square (NLMS) 
method 732
nonlinear system identification 734–739
orthogonal property 729
raised-cosine 727, 728–733
scalar subscript 726
Random cross-correlation, definition of 672
Range measurement with radar 72–73
Rational sampling rate converter 603–607
example of 605
Rayleigh limit 304, 328
RBF (see Radial basis functions (RBF))
Real signal 241
Real time 5
Realization structures 470–476, 553–556
discussion of 470–476
FIR filters 470–476
IIR filters 553–556
Realization structures, FIR filters  
470–476, 497
cascade form 472–474, 497
direct forms 470–471, 497
lattice form 474–476
linear-phase form 471–472
tapped-delay line 470
transposed tapped-delay line 470–471
Realization structures, IIR filters 553–561
cascade form 558–561
direct forms 553–556
parallel form 556–558
Reconstruction formula 27–29
Recursive least squares (RLS) method 
708–713
algorithm 711
effective window length 710
example of 711–712
forgetting factor 708
matrix inversion lemma 710
performance criterion 708–709
recursive formulation 709–711
regularization parameter 708
Reference voltage 42
Region of convergence (ROC) 152–154, 217
anti-causal part 152
causal part 152
causal signals 154
defined 152
example of 152–153
general signals 154
Residue method 174–177
algorithm for 175
Cauchy residue theorem 174
examples of poles 175
initial value theorem 175
mixed poles 176
multiple poles 174
simple poles 174
Resonator 387–388, 404, 516–19, 579
angle 517
discussion of 387–388, 516–519
filter, defined 516
filter, example of 388, 518–519
filter gain 517 
pole radius 517
power-complementary pair 387 
Reverb filter 574–578
Ripple factor 535, 536
Chebyshev-I 535
Chebyshev-II 536
Ripple voltage 237 
RLS (see Recursive least squares (RLS) 
method)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

792    Index
ROC (see Region of convergence (ROC))
Roots of unity, DFT 
Roundoff error 486–487, 566
FIR filters 486–487
IIR filters 566
noise power, IIR filters 566
Roundoff noise 486, 566
S
Sampling frequency 11
Sampling interval, defined 3, 10, 11
Sampling rate 603–607
DAT-to-CD converter, example of 
606–607
multistage converters 606–607
rational converters 603–607
Sampled signal, definition of 22
Sampling theorem 60
Satellite attitude control, example of 
148–149, 211–214
as closed-loop system 149
error signal 148
model of 149
stable range 149
Scaling 487–488, 568–570
example of 488–489, 569–570
FIR filters 486–489
IIR filters 567–568
overflow, to avoid 487–488, 568
scale factor 568
Shannon reconstruction formula 28, 60
Side bands, defined 24
Sifting property, definition of 15, 
Sigma-delta ADC 638–642
ADC effective precision 641
oversampled ADC, equivalent precision 
642
sigma-delta modulation 638
sigma-delta quantization, example of 
639–640
Signal classification 9, 11–16, 102
sifting property 102
Signal detection, example of 322–323
Signal flow graphs 185–188
auto-regressive (AR) model 188
auto-regressive moving average 
(ARMA) 188
defined 185
example of 187
moving-average (MA) model 188
Signal processing (see also Adaptive 
signal processing; Multirate signal 
processing)
active noise control 7–9
digital and analog processing 3–4
motivation for 2–9
notch filter 6–7
signals and systems 9
total harmonic distortion (THD) 5–6
Signal reconstruction 27–34
delayed first-order hold 32–34
delayed zero-order hold, plot of 33
example of 29
reconstruction formula 27–29
reconstruction theorem 28
transfer function 29, 30, 34
transportation lag, example of 30
triangle function 32
zero-order hold 30, 31, 32, 60
Signal sampling 25
aliases 25
aliasing 25–26
bandlimited signal 60
folding frequency 60
imposters 25
impulse sampling 
Laplace transform 
modulation, as 
sampling theorem 25
Signal-to-noise ratio 420–422
definition of 420
statistically independent signals 421
zero-mean white noise 422
Sinusoidal inputs 197–200
Software applications 
anti-aliasing filter design, example of 56–57
bandstop filter design: a comparison, 
example of 492–495
DFT and spectral analysis 
discrete-time system analysis 
distortion due to clipping, example of 
324–325
echo detection, example of 128–130
Fibonacci sequence and the golden 
ratio, example of  215–217
g_adapt 740–742
g_correlate 124–126
g_filters 397–400
g_fir 489–492
g_iir 571–574
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index    793
g_multirate 646–649
g_reconstruct 54–56
g_sample 52–54
g_spectra 320–322
g_sysfreq 209–211
g_systime 121–124 
highpass elliptic filter, example of 400–401
home mortgage, example of 71–73, 
126–128
identification of a chemical process,  
example of 743–744
reverb filter, example of 574–578
sampling rate converter (CD to DAT), 
example of 649–651
satellite attitude control, example of 
148–149, 211–214
signal detection, example of 322–323
speech compression 214–215 
video aliasing 58
Spectral analysis 234–320, 328, 617
Bartlett’s method 312–316
decibel scale (dB) 300–301
discrete Fourier transform (DFT) 247–262
discrete-time Fourier transform 
(DTFT) 239–247
discrete-time frequency response 298–301
fast Fourier transform (FFT) 235, 262–269
inverse discrete Fourier-transform 
(IDFT) 
motivation for 234–239
Parseval’s theorem 244–245, 
power density spectrum (PDS) 
evaluation 
resolution 303–306
signal spectra 
software applications 
spectrogram 307–312
spectrum of causal exponential,  
example of 241–242
Welch’s method 
white noise 
zero padding 302–303
Spectral resolution 303–306
Spectrogram 307–312, 328
definition of 308
example of 309–310
Spectrum 19, 240
magnitude 19, 60, 240, 326
phase 19, 60, 240, 326
system classification of 
Speech analysis and pitch, example of 214–215
frame 214
stationary signal 214
Stability, continuous-time system 18
definition of 18
example of 18
Stability, discrete-time system 119–121, 
189–195, 219
bounded-input bounded-output (BIBO) 
120–121, 189–192, 194, 219
example of 121
frequency domain 188–195
input-output representation 188–189, 219
Jury test 192–195, 219
time-domain stability constraint 120
triangle 194
unstable transfer function, example of 
191–192
Stable polynomial, necessary conditions 192
Stable system, definition of 92
Stopband, defined 347, 351
Stopband attenuation 352, 402, 579
Symmetry property, DFT 196
Synthesis filter bank, defined 393
Synthetic division method 
System classification 16–21 (see also 
Fourier transform, Discrete Fourier 
transform (DFT), Discrete-time Fou-
rier transform (DTFT)) 
example of 17–18
stability, example of 18
time-invariant 17
System identification 150, 203–208, 219–220
adaptive filters 
example of 206–207
least-squares fit 204–205
nonlinear 
online identification 208
persistently exciting inputs 207–208
System power gain, defined 19
T
Tapped-delay line 470
THD (see Total harmonic distortion 
(THD))
Time constant 687–688
mean square error 687–688
example of 688
Time-division multiplexing, defined 393, 
624–625
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

794    Index
Time-invariant system, definition of 17, 83
Time-varying system, definition of 83
Toeplitz matrix 674
Total harmonic distortion (THD) 6–7, 237 
determination of 6, 237
discussion of 6–7
Trace, square matrix 685
Transfer function, definition of  218
Transfer functions 177–185
cancelled modes 182–183
DC gain 183–184
definition of 178
difference equation 
example of 179–180, 181
factored form 181
frequency-domain representation 178
harmonic forcing 182
modes 181–184
multiple mode 181
natural mode 181
pole-zero cancellation 182
poles and zeros 181–184
stable mode 183
zero-input response 177
zero-state response 177, 179–180
Transform tables 765–773
discrete Fourier (DFT) 772–773
discrete-time Fourier (DTFT) 771–772
Fourier series 765
Fourier transform 766–768
Laplace 768–769
Z 770–771
Transition band 403, 437
defined 403
samples 437
Transition-band optimization 437–441
discussion of 437–440
filter with transition-band sample,  
example of 437–438
filter with optimal transition-band sam-
ple, example of 438–439
frequency-sampled filter, odd 
Transmultiplexors 630–634
adaptive filter 631
defined 630
equalizer design 631
ideal communication channel 631
signal synthesis, example of 632–634
Trigonometric identities 775
Truncated impulse response 423–426
filter, example of 425–426
type 1 filter 423–424
type 2 filter 423–424
type 3 filter 424
type 4 filter 424
Tunable plucked-string filter 512–514
U
Undersampling, defined 24
Uniform white noise 281–284
average power of 283
defined 282
example of 283–284
Unit circle zeros, FIR 484–485
Unit impulse 15–16, 79
defined 15
example of 16
Unit ramp, example of 
Unit step 15–16, 79
defined 15
example of 16
Unstable system, definition of 18
V
Video aliasing 9–10 
Vector norm 76
Vocal tract, modeling of 149–151
as auto-regressive system 150
W
Welch’s method 316–319
leakage phenomenon 316
modified average periodogram 317
noisy periodic input, example of  
318–319
rectangular window 317
White noise 281–288
auto-correlation of 
example of 283–284
expected value 282
Gaussian 285–288
ith moment 282
ith central moment 282
standard deviation 282
uniform 281–284
variance 282
Windowed bandpass filter, example of 432
Windowed lowpass filter, example of 
428–430
Windowed filter coefficients 427
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index    795
Windowing method 422–434
amplitude response 422
Kaiser windows 431–432
rectangular window order 
truncated impulse response 423–426
types of 
windowed bandpass filter 
windowed filter coefficients 
windowed lowpass filter 
Windows 426–434
algorithm for FIR filter design 
design characteristics 431
functions of 
Hamming 310, 428
Hanning 428
Kaiser 431–432
types of 427–428
rectangular 307, 427
Z
Z-scale property 161
Z-transform 151–167, 217
definition of 151
introduction to 151–152
inverse 166–177
pairs, discussion of 151–159
pairs, table of 159
properties, discussion of 159–166
properties, table of 166
Z-transform pairs 151–159
causal exponential 156–157
common 154–159 
exponentially damped sine 157–158
generalized geometric series 151
table of important 159
unit impulse 155
unit step 155
Z-transform properties 159–166
causal 164–166
delay 160
final value theorem 165–166
initial value theorem 164–165, 175
linearity 159–160
table of important 166
time multiplication 162
unit delay operator 160
unit ramp 162–163
Z-scale 161
Zero-input response 87–90
characteristic polynomial 87
complex conjugate roots 89
example of, complex roots 90
example of, multiple roots 89
example of, simple roots 88
natural mode 88, 131
Zero-order hold 29–32, 40, 60
Zero padding 107–109, 302–303
convolution 108
example of 108–109, 302–303
FFT, and 302–303
frequency resolution 
Rayleigh limit 304, 328
Zero-phase filters 365–368
algorithm 366
example of 367
noncausal system 366
Zero-state response 90–93
complete response, example of 92–93
example of 91–92
numerical, example of 94–95
input polynomial 91
Zeros 151, 217
discrete-time system 101
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). 
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.


Ergebnisse der Mathematik und ihrer Grenzgebiete, 3. Folge
A Series of Modern Surveys in Mathematics 64
Nihat Ay
Jürgen Jost
Hông Vân Lê
Lorenz Schwachhöfer
Information 
Geometry

Ergebnisse der Mathematik und
Volume 64
ihrer Grenzgebiete
3. Folge
A Series of Modern Surveys
in Mathematics
Editorial Board
L. Ambrosio, Pisa
V. Baladi, Paris
G.-M. Greuel, Kaiserslautern
M. Gromov, Bures-sur-Yvette
G. Huisken, Tübingen
J. Jost, Leipzig
J. Kollár, Princeton
G. Laumon, Orsay
U. Tillmann, Oxford
J. Tits, Paris
D.B. Zagier, Bonn
For further volumes:
www.springer.com/series/728

Nihat Ay r Jürgen Jost r Hông Vân Lê r
Lorenz Schwachhöfer
Information Geometry

Nihat Ay
Information Theory of Cognitive Systems
MPI for Mathematics in the Sciences
Leipzig, Germany
and
Santa Fe Institute, Santa Fe, NM, USA
Jürgen Jost
Geometric Methods and Complex Systems
MPI for Mathematics in the Sciences
Leipzig, Germany
and
Santa Fe Institute, Santa Fe, NM, USA
Hông Vân Lê
Mathematical Institute of ASCR
Czech Academy of Sciences
Praha 1, Czech Republic
Lorenz Schwachhöfer
Department of Mathematics
TU Dortmund University
Dortmund, Germany
ISSN 0071-1136
ISSN 2197-5655 (electronic)
Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge / A Series of Modern Surveys
in Mathematics
ISBN 978-3-319-56477-7
ISBN 978-3-319-56478-4 (eBook)
DOI 10.1007/978-3-319-56478-4
Library of Congress Control Number: 2017951855
Mathematics Subject Classiﬁcation: 60A10, 62B05, 62B10, 62G05, 53B21, 53B05, 46B20, 94A15,
94A17, 94B27
© Springer International Publishing AG 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
Information geometry is the differential geometric treatment of statistical models.
It thereby provides the mathematical foundation of statistics. Information geometry
therefore is of interest both for its beautiful mathematical structure and for the in-
sight it provides into statistics and its applications. Information geometry currently
is a very active ﬁeld. For instance, Springer will soon launch a new topical jour-
nal “Information Geometry”. We therefore think that the time is appropriate for
a monograph on information geometry that develops the underlying mathematical
theory in full generality and rigor, that explores the connections to other mathemat-
ical disciplines, and that proves abstract and general versions of the classical results
of statistics, like the Cramér–Rao inequality or Chentsov’s theorem. These, then,
are the purposes of the present book, and we hope that it will become the standard
reference for the ﬁeld.
Parametric statistics as introduced by R. Fisher considers parametrized families
of probability measures on some ﬁnite or inﬁnite sample space Ω. Typically, one
wishes to identify a parameter so that the resulting probability measure best ﬁts
the observation among the measures in the family. This naturally leads to quanti-
tative questions, in particular, how sensitively the measures in the family depend
on the parameter. For this, a geometric perspective is expedient. There is a natural
metric, the Fisher metric introduced by Rao, on the space of probability measures
on Ω. This metric is simply the projective or spherical metric obtained when one
considers a probability measure as a non-negative measure with a scaling factor to
render its total mass equal to unity. The Fisher metric thus is a Riemannian metric
that induces a corresponding structure on parametrized families of probability mea-
sures as above. Furthermore, moving from one reference measure to another yields
an afﬁne structure as discovered by S.I. Amari and N.N. Chentsov. The investiga-
tion of these metric and afﬁne structures is therefore called information geometry.
Information-theoretical quantities like relative entropies (Kullback–Leibler diver-
gences) then ﬁnd a natural geometric interpretation.
Information geometry thus provides a way of understanding information-
theoretic quantities, statistical models, and corresponding statistical inference meth-
ods in geometric terms. In particular, the Fisher metric and the Amari–Chentsov
v

vi
Preface
structure are characterized by their invariance under sufﬁcient statistics. Several ge-
ometric formalisms have been identiﬁed as powerful tools to this end and emphasize
respective geometric aspects of probability theory. In this book, we move beyond
the applications in statistics and develop both a functional analytic and a geometric
theory that are of mathematical interest in their own right. In particular, the theory
of dually afﬁne structures turns out to be an analogue of Kähler geometry in a real
as opposed to a complex setting.
Also, as the concept of Shannon information can be related to the entropy con-
cepts of Boltzmann and Gibbs, there is also a natural connection between informa-
tion geometry and statistical mechanics. Finally, information geometry can also be
used as a foundation of important parts of mathematical biology, like the theory of
replicator equations and mathematical population genetics.
Sample spaces could be ﬁnite, but more often than not, they are inﬁnite, for in-
stance, subsets of some (ﬁnite- or even inﬁnite-dimensional) Euclidean space. The
spaces of measures on such spaces therefore are inﬁnite-dimensional Banach spaces.
Consequently, the differential geometric approach needs to be supplemented by
functional analytic considerations. One of the purposes of this book therefore is
to provide a general framework that integrates the differential geometry into the
functional analysis.

Acknowledgements
We would like to thank Shun-ichi Amari for many fruitful discussions. This work
was mainly carried out at the Max Planck Institute for Mathematics in the Sciences
in Leipzig. It has also been supported by the BSI at RIKEN in Tokyo, the ASSMS,
GCU in Lahore-Pakistan, the VNU for Sciences in Hanoi, the Mathematical Insti-
tute of the Academy of Sciences of the Czech Republic in Prague, and the Santa Fe
Institute. We are grateful for the excellent working conditions and ﬁnancial support
of these institutions during extended visits of some of us. In particular, we should
like to thank Antje Vandenberg for her outstanding logistic support.
The research of J.J. leading to his book contribution has received funding from
the European Research Council under the European Union’s Seventh Framework
Programme (FP7/2007-2013)/ERC grant agreement no. 267087. The research of
H.V.L. is supported by RVO: 67985840.
vii

Contents
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
A Brief Synopsis . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
An Informal Description . . . . . . . . . . . . . . . . . . . . . .
6
1.2.1
The Fisher Metric and the Amari–Chentsov Structure
for Finite Sample Spaces
. . . . . . . . . . . . . . . . .
7
1.2.2
Inﬁnite Sample Spaces and Functional Analysis
. . . . .
8
1.2.3
Parametric Statistics . . . . . . . . . . . . . . . . . . . .
10
1.2.4
Exponential and Mixture Families from the Perspective
of Differential Geometry
. . . . . . . . . . . . . . . . .
14
1.2.5
Information Geometry and Information Theory . . . . . .
15
1.3
Historical Remarks . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.4
Organization of this Book . . . . . . . . . . . . . . . . . . . . .
20
2
Finite Information Geometry . . . . . . . . . . . . . . . . . . . . .
25
2.1
Manifolds of Finite Measures . . . . . . . . . . . . . . . . . . .
25
2.2
The Fisher Metric . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.3
Gradient Fields . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
2.4
The m- and e-Connections . . . . . . . . . . . . . . . . . . . . .
42
2.5
The Amari–Chentsov Tensor and the α-Connections . . . . . . .
47
2.5.1
The Amari–Chentsov Tensor
. . . . . . . . . . . . . . .
47
2.5.2
The α-Connections
. . . . . . . . . . . . . . . . . . . .
50
2.6
Congruent Families of Tensors
. . . . . . . . . . . . . . . . . .
52
2.7
Divergences
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
2.7.1
Gradient-Based Approach . . . . . . . . . . . . . . . . .
68
2.7.2
The Relative Entropy
. . . . . . . . . . . . . . . . . . .
70
2.7.3
The α-Divergence . . . . . . . . . . . . . . . . . . . . .
73
2.7.4
The f -Divergence . . . . . . . . . . . . . . . . . . . . .
76
2.7.5
The q-Generalization of the Relative Entropy . . . . . . .
78
ix

x
Contents
2.8
Exponential Families
. . . . . . . . . . . . . . . . . . . . . . .
79
2.8.1
Exponential Families as Afﬁne Spaces . . . . . . . . . .
79
2.8.2
Implicit Description of Exponential Families . . . . . . .
84
2.8.3
Information Projections . . . . . . . . . . . . . . . . . .
91
2.9
Hierarchical and Graphical Models . . . . . . . . . . . . . . . .
100
2.9.1
Interaction Spaces . . . . . . . . . . . . . . . . . . . . .
101
2.9.2
Hierarchical Models . . . . . . . . . . . . . . . . . . . .
108
2.9.3
Graphical Models . . . . . . . . . . . . . . . . . . . . .
112
3
Parametrized Measure Models
. . . . . . . . . . . . . . . . . . . .
121
3.1
The Space of Probability Measures and the Fisher Metric
. . . .
121
3.2
Parametrized Measure Models . . . . . . . . . . . . . . . . . . .
135
3.2.1
The Structure of the Space of Measures . . . . . . . . . .
139
3.2.2
Tangent Fibration of Subsets of Banach Manifolds . . . .
140
3.2.3
Powers of Measures . . . . . . . . . . . . . . . . . . . .
143
3.2.4
Parametrized Measure Models and k-Integrability . . . .
150
3.2.5
Canonical n-Tensors of an n-Integrable Model . . . . . .
164
3.2.6
Signed Parametrized Measure Models
. . . . . . . . . .
168
3.3
The Pistone–Sempi Structure . . . . . . . . . . . . . . . . . . .
170
3.3.1
e-Convergence . . . . . . . . . . . . . . . . . . . . . . .
170
3.3.2
Orlicz Spaces
. . . . . . . . . . . . . . . . . . . . . . .
172
3.3.3
Exponential Tangent Spaces . . . . . . . . . . . . . . . .
176
4
The Intrinsic Geometry of Statistical Models
. . . . . . . . . . . .
185
4.1
Extrinsic Versus Intrinsic Geometric Structures . . . . . . . . . .
185
4.2
Connections and the Amari–Chentsov Structure
. . . . . . . . .
189
4.3
The Duality Between Exponential and Mixture Families . . . . .
201
4.4
Canonical Divergences . . . . . . . . . . . . . . . . . . . . . . .
210
4.4.1
Dual Structures via Divergences . . . . . . . . . . . . . .
210
4.4.2
A General Canonical Divergence . . . . . . . . . . . . .
213
4.4.3
Recovering the Canonical Divergence of a Dually Flat
Structure . . . . . . . . . . . . . . . . . . . . . . . . . .
215
4.4.4
Consistency with the Underlying Dualistic Structure . . .
217
4.5
Statistical Manifolds and Statistical Models . . . . . . . . . . . .
219
4.5.1
Statistical Manifolds and Isostatistical Immersions . . . .
220
4.5.2
Monotone Invariants of Statistical Manifolds . . . . . . .
223
4.5.3
Immersion of Compact Statistical Manifolds into Linear
Statistical Manifolds . . . . . . . . . . . . . . . . . . . .
226
4.5.4
Proof of the Existence of Isostatistical Immersions . . . .
228
4.5.5
Existence of Statistical Embeddings . . . . . . . . . . . .
238

Contents
xi
5
Information Geometry and Statistics . . . . . . . . . . . . . . . . .
241
5.1
Congruent Embeddings and Sufﬁcient Statistics
. . . . . . . . .
241
5.1.1
Statistics and Congruent Embeddings . . . . . . . . . . .
244
5.1.2
Markov Kernels and Congruent Markov Embeddings
. .
253
5.1.3
Fisher–Neyman Sufﬁcient Statistics . . . . . . . . . . . .
261
5.1.4
Information Loss and Monotonicity . . . . . . . . . . . .
263
5.1.5
Chentsov’s Theorem and Its Generalization . . . . . . . .
268
5.2
Estimators and the Cramér–Rao Inequality . . . . . . . . . . . .
277
5.2.1
Estimators and Their Bias, Mean Square Error, Variance .
277
5.2.2
A General Cramér–Rao Inequality
. . . . . . . . . . . .
281
5.2.3
Classical Cramér–Rao Inequalities
. . . . . . . . . . . .
286
5.2.4
Efﬁcient Estimators and Consistent Estimators . . . . . .
287
6
Fields of Application of Information Geometry
. . . . . . . . . . .
295
6.1
Complexity of Composite Systems . . . . . . . . . . . . . . . .
295
6.1.1
A Geometric Approach to Complexity
. . . . . . . . . .
296
6.1.2
The Information Distance from Hierarchical Models . . .
298
6.1.3
The Weighted Information Distance . . . . . . . . . . . .
307
6.1.4
Complexity of Stochastic Processes . . . . . . . . . . . .
317
6.2
Evolutionary Dynamics . . . . . . . . . . . . . . . . . . . . . .
327
6.2.1
Natural Selection and Replicator Equations . . . . . . . .
328
6.2.2
Continuous Time Limits . . . . . . . . . . . . . . . . . .
333
6.2.3
Population Genetics . . . . . . . . . . . . . . . . . . . .
336
6.3
Monte Carlo Methods . . . . . . . . . . . . . . . . . . . . . . .
348
6.3.1
Langevin Monte Carlo . . . . . . . . . . . . . . . . . . .
350
6.3.2
Hamiltonian Monte Carlo . . . . . . . . . . . . . . . . .
351
6.4
Inﬁnite-Dimensional Gibbs Families
. . . . . . . . . . . . . . .
354
Appendix A
Measure Theory . . . . . . . . . . . . . . . . . . . . . . .
361
Appendix B
Riemannian Geometry
. . . . . . . . . . . . . . . . . . .
367
Appendix C
Banach Manifolds . . . . . . . . . . . . . . . . . . . . . .
381
References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
387
Index
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
397
Nomenclature
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
403

Chapter 1
Introduction
1.1 A Brief Synopsis
Parametric statistics is concerned with families
p : M →P(Ω)
(1.1)
of probability measures on some sample space Ω. That is, for each ξ in the parame-
ter space M, we have a probability measure p(·;ξ) on Ω. And typically, one wishes
to estimate the parameter ξ based on random samples drawn from some unknown
probability distribution on Ω, so as to identify a particular p(·;ξ0) that best ﬁts that
sampling distribution. Information geometry provides geometric tools to analyze
such families. In particular, a basic question is how sensitively p(x;ξ) depends on
the sample x. It turns out that this sensitivity can be quantiﬁed by a Riemannian
metric, the Fisher metric originally introduced by Rao. Therefore, it is natural to
bring in tools from differential geometry. That metric on the parameter space M
is obtained by pulling back some universal structure from P(Ω) via (1.1). When
Ω is inﬁnite, which is not an untypical situation in statistics, however, P(Ω) is
inﬁnite-dimensional, and therefore functional analytical problems arise. One of the
main features of this book consists in a general, and as we think, most satisfactory,
approach to these issues.
From a geometric perspective, it is natural to look at invariances. On the one
hand, we can consider mappings
κ : Ω →Ω′
(1.2)
into some other space Ω′. Such a κ is called a statistic. In some cases, Ω′ might even
be ﬁnite even if Ω itself is inﬁnite. For instance, Ω′ could simply be the index set
of some ﬁnite partition of Ω, and κ(x) then would simply tell us in which member
of that partition the point x is found. In other cases, κ might stand for a speciﬁc
observable on Ω. A natural question then concerns the possible loss of information
about the parameter ξ ∈M from the family (1.1) when we only observe κ(x) instead
of x itself. The statistic κ is called sufﬁcient for the family (1.1) when no information
© Springer International Publishing AG 2017
N. Ay et al., Information Geometry, Ergebnisse der Mathematik und ihrer Grenzgebiete.
3. Folge / A Series of Modern Surveys in Mathematics 64,
DOI 10.1007/978-3-319-56478-4_1
1

2
1
Introduction
is lost at all. It turns out that the information loss can be quantiﬁed by the difference
of the Fisher metrics of the originally family p and the induced family κ∗p. We shall
also show that the information loss can be quantiﬁed by tensors of higher order. In
fact, one of the results that we shall prove in this book is that the Fisher metric is
uniquely characterized (up to a constant factor, of course) by being invariant under
all sufﬁcient statistics.
Another invariance concerns reparametrizations of the parameter space M. Of
course, as a Riemannian metric, the Fisher metric transforms appropriately under
such reparametrizations. However, there are particular families p with particular
parametrizations. These naturally play an important role. In order to see how they
arise, we need to look at the structure of the space P(Ω) of probability measures
more carefully (see Fig. 1.1). Every probability measure is a measure tout court, that
is, there is an embedding
ı : P(Ω) →S(Ω)
(1.3)
into the space S(Ω) of all ﬁnite signed measures on Ω. As a technical point, for a
probability measure, that is, an element of P(Ω), we require it to be nonnegative,
but for a general (ﬁnite) measure, an element of S(Ω), we do not impose this restric-
tion. The latter space is a linear space. p ∈P(Ω) then is simply characterized by

Ω dp(x) = 1, and so, P(Ω) becomes a convex subset (because of the nonnegativ-
ity constraint) of an afﬁne subspace (characterized by the condition

Ω dμ(x) = 1)
of the linear space S(Ω). On the other hand, there is also a projection
π : M(Ω) →P(Ω)
(1.4)
of the space of nonnegative measures by assigning to each m ∈M(Ω) the relative
measure of subsets. For any measurable subsets A,B⊆Ω with m(B) > 0, π(m)
looks at the quotients m(A)
m(B), that is, the relative measures of those subsets. That is,
a probability measure is now considered as an equivalence class of measures up
to a scaling factor. Of course, as such, a probability measure and such an equiva-
lence class is not quite the same, and therefore the target of π in (1.4) is not really
P(Ω), but P(Ω) can be easily identiﬁed with π(M(Ω)) (modulo certain technical
points that we suppress in this synopsis), by simply normalizing a measure by m(Ω)
(assuming that the latter is ﬁnite). From the perspective of such relative measures,
π(M(Ω)), that is by what we have just said, P(Ω), can be seen as the positive
part of a projective space of the linear space S(Ω), that is, as the positive orthant or
sector of the unit sphere in S(Ω). When Ω is ﬁnite, the linear space S(Ω) is ﬁnite-
dimensional, and therefore, it can be naturally equipped with a Euclidean metric.
This metric then also induces a metric on the unit sphere, or in the terminology de-
veloped here, the projection map π from (1.4) then induces a metric on P(Ω). This
is the Fisher metric, a fundamental object of our study. When Ω is inﬁnite, then the
space S(Ω) is inﬁnite-dimensional, but it does not carry the structure of a Hilbert
space. Nevertheless, by considering variations of class L2(Ω,μ), we still obtain
an L2-scalar product, and that will again be the Fisher metric. (The space within
which we vary our measure—L1,L∞or L2—will be an important technical issue

1.1
A Brief Synopsis
3
for our functional analytical considerations. In fact, L1 will be the natural choice,
as it behaves naturally under a change of base measure.)
It turns out that the structure induced on P(Ω) by (1.4) in a certain sense is dual
to the afﬁne structure induced on this space by the embedding (1.3). In fact, this
dual structure is afﬁne itself, and it can be described in terms of an exponential map.
In order to better understand this, let us discuss the two possible ways in which
a measure can be normalized to become a probability measure. We start with a
probability measure μ and want to move to another probability measure ν. We can
write additively
ν = μ + (ν −μ)
(1.5)
and connect ν with μ by the straight line
μ + t(ν −μ),
with t ∈[0,1].
(1.6)
When we consider an arbitrary variation
μ + tξ,
(1.7)
when we want to stay within the class of probability measures, we need to subtract
ξ0 := ξ(Ω), that is, consider the measure ξ −ξ0 deﬁned by (ξ −ξ0)(A) := ξ(A) −
ξ(Ω). Thus, we get the variation
μ + t(ξ −ξ0).
(1.8)
Here, we see the problem that even if μ is nonnegative, as it should be as a probabil-
ity measure, and if μ+tξ is nonnegative as well for t ∈[0,1], μ+t(ξ −ξ0) need not
always be nonnegative. Expressing this geometrically, the geodesic μ + t(ξ −ξ0)
(which is meaningful for all t ∈R) with respect to the afﬁne structure on the sim-
plex may leave the simplex of probability measures. Thus, this afﬁne structure is not
complete. Alternatively, we can consider a multiplicative variation and write
ν = exp
 dν
dμ

μ
(1.9)
where dν
dμ is the Radon–Nikodym derivative of ν w.r.t. μ. A general variation would
then be of the form
exp(tf )μ,
with t ∈[0,1],
(1.10)
where we require that the function expf be in L1(Ω,μ). Here, we choose the ex-
ponential for two reasons. First, this ensures that the measure exp(tf )μ is nonneg-
ative if μ is. Thus, we do not run into the problem of noncompleteness as for the
additive variation. Secondly, we can consider a linear space of functions f here.
There is an important technical problem, though. exp(t1f ) ∈L1 does not imply
that exp(t2f ) ∈L1 for t2 > t1; we shall return to this problem. For the moment,
it can be circumvented by requiring that f ∈L∞(Ω,μ) because that implies that

4
1
Introduction
Fig. 1.1 Natural inclusion
and projection
exp(tf ) ∈L∞as well for all t. Again, for a general function f , we need to impose
a normalization in order to stay in the class of probability distributions. This leads
to the variation
exp(tf )
Z(t) μ
with Z(t) :=

Ω
exp(tf )dμ.
(1.11)
This multiplicative normalization is, of course, in line with our view of the prob-
ability measures as equivalence classes of measures up to a factor. Moreover, we
can consider the family (1.11) as a geodesic for an afﬁne structure, as we shall now
explain. First, although the normalization factor Z(t) depends on the measure μ,
this does not matter as we are considering elements of a projective space which
does not see a global factor. Secondly, when we have two probability measures
μ,μ1 with μ1 = φμ for some positive function φ with φ ∈L1(Ω,μ) and hence
φ−1 ∈L1(Ω,μ1), then the variations exp(tf )μ of μ correspond to the variations
exp(tf )
φ
μ1 of μ1. At the level of the linear spaces, the correspondence would be be-
tween f and f −logφ (we might wish to require here that φ,φ−1 ∈L∞according
to the previous discussion, but let us ignore this technical point for the moment).
The important point here is that we can identify the variations at μ and μ1 here in
a manner that does not depend on the individual f , because the shift by logφ is the
same for all f . Moreover, when we have μ2 = ψμ1, then μ2 = ψφμ, and the shift
is by log(ψφ) = logψ +logφ. But this is precisely what an afﬁne structure amounts
to. Thus, we have identiﬁed the second afﬁne structure on the space of probability
measures. It possesses a natural exponential map f →expf , is naturally adapted
to our description of probability measures as equivalence classes of measures, and
is complete in contrast to the ﬁrst afﬁne structure. As we shall explore in more de-
tail in the geometric part of this book, these two structures are naturally dual to each
other. They are related by a Legendre transform that generalizes the duality between
entropy and free energy that is at the heart of statistical mechanics.

1.1
A Brief Synopsis
5
This pair of dual afﬁne structures was discovered by Amari and Chentsov, and
the tensor describing it is therefore called the Amari–Chentsov tensor. The Amari–
Chentsov tensor encodes the difference between the two afﬁne connections, and
they can be recovered from the Fisher metric and this tensor. Like the Fisher met-
ric, the Amari–Chentsov tensor is invariant under sufﬁcient statistics, and uniquely
characterized by this fact, as we shall also show in this book. Spaces with such a
pair of dual afﬁne structures turn out to have a richer geometry than simple afﬁne
spaces. In particular, such afﬁne structures can be derived from potential functions.
In particularly important special cases, these potential functions are the entropy and
the free energy as known from statistical mechanics.
Thus, there is a natural connection between information geometry and statistical
mechanics. Of course, there is also a natural connection between statistical mechan-
ics and information theory, through the analogy between Boltzmann–Gibbs entropy
and Shannon information. In many interesting cases within statistical mechanics,
the interaction of physical elements can be described in terms of a graph or, more
generally, in terms of a hypergraph. This leads to families of Boltzmann–Gibbs dis-
tributions that are known as hierarchical or graphical models.
In fact, information geometry also directly leads to geometric descriptions of in-
formation theoretical concepts, and this is another topic that we shall systematically
explore in this book. In particular, we shall treat conditional and relative entropies
from a geometric perspective, analyze exponential families, including interaction
spaces and hierarchical and graphical models, and describe applications like repli-
cator equations in mathematical biology and population game theory. Since many
of those geometric properties and applications show themselves already in the case
where the sample space Ω is ﬁnite and hence the spaces of measures on it are ﬁnite-
dimensional, we shall start with a chapter on that case.
We consider families of measures p(ξ) on a sample space Ω parametrized by
ξ from our parameter space M. For different ξ, the resulting measures might be
quite different. In particular, they may have rather different null sets. Nevertheless,
in many cases, for instance, if M is a ﬁnite-dimensional manifold, we may write
such a family as
p(ξ) = p(·;ξ)μ0,
(1.12)
for some base measure μ0 that does not depend on ξ. p : Ω × M →R is the den-
sity function of p w.r.t. μ0, and we then need that p(·;ξ) ∈L1(Ω,μ0) for all ξ.
This looks convenient, after all such a μ0 is an auxiliary object, and it is a general
mathematical principle that structures should not depend on such auxiliary objects.
Implementing this principle systematically will, in fact, give us the crucial lever-
age needed to develop the general theory. Let us be more precise. As already ob-
served above, when we have another probability measure μ1 with μ1 = φμ0 for
some positive function φ with φ ∈L1(Ω,μ0) and hence φ−1 ∈L1(Ω,μ1), then
ψ ∈L1(Ω,μ1) precisely if ψφ ∈L1(Ω,μ0). Thus, the L1-spaces naturally corre-
spond to each other, and it does not matter which base measure we choose, as long
as the different base measures are related by L1-functions.

6
1
Introduction
Second, the differential of p in some direction V is then given by
dξp(V ) = ∂V p(·;ξ)μ0 ∈L1(Ω,μ0),
(1.13)
assuming that this quantity exists. According to what we have just said, however,
what we should consider is not ∂V p(·;ξ)μ0, which measures the change of measure
w.r.t. the background measure μ0, but rather the rate of change of p(ξ) relative to the
measure p(ξ) itself, that is, the Radon–Nikodym derivative of dξp(V ) w.r.t. p(ξ),
that is, the logarithmic derivative
∂V logp(·;ξ) = d{dξp(V )}
dp(ξ)
.
(1.14)
(Note that this is not a second derivative, as the outer d stands for the Radon–
Nikodym derivative, that is, essentially a quotient of measures. The slightly con-
fusing notation ultimately results from writing integration with respect to p(ξ) as

dp(ξ).)
This then leads to the Fisher metric
gξ(V,W) =

Ω
∂V logp(·;ξ) ∂W logp(·;ξ) dp(ξ).
(1.15)
One may worry here about what happens when the density p is not positive almost
everywhere. In order to see that this is not really a problem, we introduce the formal
square roots

p(ξ) :=

p(·;ξ)√μ0,
(1.16)
and use the formal computation
dξ
√p(V ) = 1
2∂V logp(·;ξ)

p(ξ)
(1.17)
to rewrite (1.15) as
gξ(V,W) = 4

Ω
d

dξ
√p(V ) · dξ
√p(W)

.
(1.18)
Also, in a sense to be made precise, an L1-condition on p(ξ) becomes an L2-
condition on √p(ξ) in (1.16), and an L2-condition is precisely what we need in
(1.18) for the derivatives. According to (1.17), this means that we should now im-
pose an L2-condition on ∂V logp(·;ξ). Again, all this is naturally compatible with
a change of base measure.
1.2 An Informal Description
Let us now informally describe some of the main points of information geometry
as treated in this book, and thereby perhaps already give away some of our secrets.

1.2
An Informal Description
7
“Informally” here is meant seriously, indeed. That is, we shall suppress certain tech-
nical points that will, of course, be clariﬁed in the main text.
1.2.1 The Fisher Metric and the Amari–Chentsov Structure
for Finite Sample Spaces
Let ﬁrst I = {1,...,n}, n ∈N = {1,2,...}, be a ﬁnite sample space, and consider
the set of nonnegative measures M(I) = {(m1,...,mn) : mi ≥0,	
j mj > 0} on it.
A probability measure is then either a tuple (p1,...,pn) ∈M(I) with 	
j pj = 1,
or a measure up to scaling. The latter means that we do not consider the measure
mi of an i ∈I, or more generally, of a subset of I, but rather only quotients mi
mj
whenever mj > 0. In other words, we look at relative instead of absolute mea-
sures. Clearly, M(I) can be identiﬁed with the positive sector Rn
+ of Rn. The ﬁrst
perspective would then identify the set of probability measures with the simplex
Σn−1 = {(y1,...,yn) : yi ≥0,	
j yj = 1}, whereas the latter would rather iden-
tify it with the positive part of the projective space Pn−1, that is, with the posi-
tive orthant or sector of the unit sphere Sn−1 in Rn, Sn−1
+
= {(q1,...,qn) : qi ≥0,
	
j(qj)2 = 1}. Of course, Σn−1 and Sn−1
+
are homeomorphic, but otherwise, their
geometry is different. We shall utilize both of them. Foremost, the sphere Sn−1
carries its natural metric induced from the Euclidean metric on Rn. Therefore, we
obtain a Riemannian metric on the set of probability measures. This is the Fisher
metric. Next, let us take a measure μ0 = (m1,...,mn) with mi > 0 for all i. We
call a measure φμ0 = (φ1m1,...,φnmn) compatible with μ0 if φi > 0 for all i.
Let us call the space of these measures M+(I,μ0). Of course, this space does
not really depend on μ0; the only relevant aspect is that all entries of μ0 be pos-
itive. Nevertheless, it will be instructive to look at the dependence on μ0 more
carefully. M+(I,μ0) forms a group under pointwise multiplication. Equally im-
portantly, we get an afﬁne structure. Considering Σn−1, this is obvious, as the sim-
plex is a convex subset of an (n −1)-dimensional afﬁne subspace of Rn. Perhaps
somewhat more surprisingly, there is another afﬁne structure which we shall now
describe. Let μ1 = φ1μ0 ∈M+(I,μ0) and μ2 = φ2μ1 ∈M+(I,μ1). Thus, we
also have μ2 = φ2φ1μ0 ∈M+(I,μ0). In particular, whenever μ = φμ0 is compat-
ible with μ0, we have a canonical identiﬁcation of M+(I,μ) with M+(I,μ0) via
multiplication by φ. Of course, these spaces are not linear, due to the positivity con-
straints. We can, however, consider the linear space T ∗
μ0 ∼= Rn of (f1,...,fn) with
fi ∈R. This space is bijective to M+(I,μ0) via φi = efi. This is an exponential
map, as familiar from Riemannian geometry or Lie group theory. (As will be ex-
plained in Chap. 2, this space can be naturally considered as the cotangent space
of the space of measures at the point μ0. For the purposes of this introduction, the
difference between tangent and cotangent spaces is not so important, however, and
in any case, as soon as we have a metric, there is a natural identiﬁcation between
tangent and cotangent spaces.) Now, there is a natural identiﬁcation between T ∗
μ

8
1
Introduction
and T ∗
μ0; in fact, if μ = φμ0, this is achieved by the correspondence gi = fi −logφi,
because then egμ = ef μ0. Since this identiﬁcation is independent of f and g, and
furthermore is compatible with the above product structure, we have a natural cor-
respondence between the (co)tangent spaces at different points of M+, that is, an
afﬁne structure. We thus have not one, but two afﬁne structures. These two struc-
tures are indeed different, but there is a natural duality between them. This is the
Amari–Chentsov structure which we shall describe in Sect. 1.1.
1.2.2 Inﬁnite Sample Spaces and Functional Analysis
So far, the sample space has been ﬁnite. Let us now consider a general sample space,
that is, some set Ω together with a σ-algebra, so that we can consider the space of
measures on Ω. We shall assume for the rest of this discussion that Ω is inﬁnite, as
we have already described the ﬁnite case, and we want to see which aspects natu-
rally extend. Again, in this introduction, we restrict ourselves to the positive mea-
sures. Probability measures then can again either be considered as measures μ with
μ(Ω) = 1, or as relative measures, that is, considering only quotients μ(A)
μ(B) whenever
μ(B) > 0. In the ﬁrst case, we would deal with an inﬁnite dimensional simplex, in
the second one with the positive orthant or sector of an inﬁnite-dimensional sphere.
Now, given again some base measure μ0, the space of compatible measures would
be M+(Ω,μ0) = {φμ0 : φ ∈L1(Ω,μ0),φ > 0 almost everywhere}. Some part of
the preceding naturally generalizes. In particular, when μ1 = φ1μ0 ∈M+(Ω,μ0)
and μ2 = φ2μ1 ∈M+(Ω,μ1), then μ2 = φ2φ1μ0 ∈M+(Ω,μ0). And this is pre-
cisely the property that we shall need. However, we no longer have a multiplica-
tive structure, because if φ,ψ ∈L1(Ω,μ0), then their product φψ need not be
in L1(Ω,μ0) itself. Moreover, the exponential map f →ef (deﬁned in a point-
wise manner, i.e., ef (x) = ef (x)) is no longer deﬁned for all f . In fact, the natural
linear space would be L2(Ω,μ0), but if f ∈L2(Ω,μ0), then ef need not be in
L1(Ω,μ0). But nevertheless, wherever deﬁned, we have the above afﬁne corre-
spondence. So, at least formally, we again have two afﬁne structures, one from the
inﬁnite-dimensional simplex, and the other as just described. Also, from the posi-
tive sector of the inﬁnite dimensional sphere, we again get (an inﬁnite-dimensional
version of) a Riemannian metric. Now, developing the functional analysis required
to make this really work is one of the major achievements of this book, see Chap. 3.
Our approach is different from and more general than the earlier ones of Amari–
Nagaoka [16] and Pistone–Sempi [216].
For our treatment, another simple observation will be important. There is a natu-
ral duality between functions f and measures φμ,
(f,φμ) =

Ω
f φ dμ,
(1.19)
whenever f and φ satisfy appropriate integrability conditions. From the perspective
of the duality between functions and measures, we might require that φ be in L1

1.2
An Informal Description
9
and f be in L∞. We can turn (1.19) into a symmetric pairing by rewriting it as

f (μ)1/2,φ(μ)1/2
=

Ω
f (dμ)1/2φ(dμ)1/2.
(1.20)
Since this is symmetric, we would now require that both factors be in L2. The ob-
jects involved in (1.20), that is, those that transform like (dμ)1/2, that is, with the
square root of the Jacobian of a coordinate transformation, are called half-densities.
In particular, the group of diffeomorphisms of Ω (assuming that Ω carries a differ-
entiable structure) operates by isometries on the space of half-densities of class L2.
Therefore, this is a good space to work with. In fact, for the Amari–Chentsov struc-
ture, we also have to consider (1/3)-densities, that is, objects that transform like a
cubic root of a measure.
In order to make this precise, in our approach, we deﬁne the Banach spaces of
formal rth powers of (signed) measures, denoted by Sr(Ω), where 0 < r ≤1. For
instance, S1(Ω) = S(Ω) is the Banach space of ﬁnite signed measures on Ω with
the total variation as the Banach norm. The space S1/2(Ω) is the space of signed
half-densities which is a Hilbert space in a natural way (the concept of a half-density
will be discussed in more detail after (1.20)). Just as we may regard the set of prob-
ability measures and (positive) ﬁnite measures as subsets P(Ω)⊆M(Ω)⊆S(Ω),
there are analogous inclusions Pr(Ω)⊆Mr(Ω)⊆Sr(Ω) of rth powers of prob-
ability measures (ﬁnite measures, respectively). In particular, we also get a rig-
orous deﬁnition of the (formal) tangent bundle T Pr(Ω) and T Mr(Ω), where
TμMr(Ω) = Lk(Ω,μ) for k = 1/r ≥1, so this is precisely the tangent space which
was relevant in our previous discussion.
We also deﬁne the signed kth power ˜πk : Sr(Ω) →S(Ω) for k := 1/r ≥1
which is a differentiable homeomorphism between these sets and can hence be re-
garded as a coordinate map on S(Ω) changing the differentiable structure. It maps
Pr(Ω) to P(Ω) and Mr(Ω) to M(Ω), respectively. A similar approach was used
by Amari [8] who introduced the concept of α-representations, expressing a statisti-
cal model in different coordinates by taking powers of the model. The advantage of
our approach is that the deﬁnition of the parametrization ˜πk is universally deﬁned
on Sr(Ω) and does not depend on a particular parametrized measure model.
Given a statistical model (M,Ω,p), we interpret it as a differentiable map from
M to P(Ω)⊆S(Ω). Then the notion of k-integrability of the model from [25] can
be interpreted in this setting as the condition that for r = 1/k, the rth power pr map-
ping M to Pr(Ω)⊆Sr(Ω) is continuously differentiable. Note that in the deﬁnition
of the model (M,Ω,p), we do not assume the existence of a measure dominating
all measures p(ξ), nor do we assume that all measures p(ξ) have the same null sets.
With this, our approach is indeed more general than the notions of differentiable
families of measures deﬁned, e.g., in [9, 16, 25, 216, 219].
For each n, we can deﬁne a canonical n-tensor on Sr(Ω) for 0 < r ≤1/n, which
can be pulled back to M via pr. In the cases n = 2 and n = 3, this produces the
Fisher metric and the Amari–Chentsov tensor of the model, respectively. We shall
show in Chap. 5 that the canonical n-tensors are invariant under sufﬁcient statis-

10
1
Introduction
tics and, moreover, the set of tensors invariant under sufﬁcient statistics are alge-
braically generated by the canonical n-tensors. This is a generalization of the results
of Chentsov and Campbell to the case of tensors of arbitrary degree and arbitrary
measure spaces Ω.
Let us also mention here that when Ω is a manifold and the model p(ξ) consists
of smooth densities, the Fisher metric can already be characterized by invariance
under diffeomorphisms, as has been shown by Bauer, Bruveris and Michor [44].
Thus, in the more restricted smooth setting, a weaker invariance property already
sufﬁces to determine the Fisher metric. For the purposes of this book, in particular
the mathematical foundation of parametric statistics, however, the general measure
theoretical setting that we have developed is essential.
There is another measure theoretical structure which was earlier introduced by
Pistone–Sempi [216]; we shall discuss that structure in detail in Sect. 3.3. In fact, to
appreciate the latter, the following observation is a key. Whenever ef ∈L1(Ω,μ0),
then for t < 1, etf = (ef )t ∈Lp(Ω,μ0) for p = 1/t > 1. Thus, the set of f with
ef ∈L1 is not only starshaped w.r.t. the origin, but whenever we scale by a factor
t < 1, the integrability even improves. This, however, substantially differs from our
approach. In fact, in the Pistone–Sempi structure the topology used (e-convergence)
is very strong and, as we shall see, it decomposes the space M+(Ω;μ0) of measures
compatible with μ0 into connected components, each of which is an open convex set
in a Banach space. Thus, M+(Ω;μ0) becomes a Banach manifold with an afﬁne
structure under the e-topology. In contrast, the topology that we use on Pr(Ω) is
essentially the Lk-topology on Lk(Ω,μ) for k = 1/r, which is much weaker. This
implies that, on the one hand, Pr(Ω) is not a Banach manifold but merely a closed
subset of the Banach space Sr(Ω), so it carries far less structure than M+(Ω;μ0)
with the Pistone–Sempi topology. On the other hand, our structure is applicable
to many statistical models which are not continuous in the e-topology of Pistone–
Sempi.
1.2.3 Parametric Statistics
We now return to the setting of parametric statistics, because that is a key applica-
tion of our theory. In parametric statistics, one considers only parametrized families
of measures on the sample space Ω, rather than the space P(Ω) of all probability
measures on Ω. These families are typically ﬁnite-dimensional (although our ap-
proach can also naturally handle inﬁnite-dimensional families). We consider such a
family as a mapping p : M →P(Ω) from the parameter space M into P(Ω), and
p needs to satisfy appropriate continuity and differentiability properties related to
the L1-topology on P(Ω). In fact, some of the most difﬁcult technical points of our
book are concerned with getting these properties right, so that the abstract Fisher
and Amari–Chentsov structures on P(Ω) can be pulled back to such an M via p.
This makes these structures functorial in a natural way.
The task or purpose of parametric statistics then is to identify an element of such
a family M that best describes the statistics obtained from sampling Ω. A map that

1.2
An Informal Description
11
converts the samples from Ω into estimates for a parameter ξ ∈M is called an
estimator. The Fisher metric quantiﬁes the sensitivity of the dependence of the pa-
rameter ξ on the samples in Ω, and this leads to the Cramér–Rao inequality which
constrains any estimator. Moreover, instead of sampling from Ω, we could consider
a map κ : Ω →Ω′ to some possibly much smaller space. In general, sampling from
a smaller space loses some information about the parameter ξ, and consequently, the
Fisher metric decreases. In fact, we shall show such a monotonicity result under very
general assumptions. In informal terms, such a map κ is called a sufﬁcient statistic
for a family p : M →P(Ω) if sampling from Ω′ is as good for identifying the pa-
rameter ξ as sampling from Ω itself. In that case, the parameter sensitivity should
be the same in either case, and according to the interpretation of the Fisher met-
ric just given, it should be invariant under sufﬁcient statistics. A remarkable result
of Chentsov says that, conversely, the Fisher metric and the Amari–Chentsov ten-
sor are uniquely determined by their invariance under sufﬁcient statistics. Chentsov
proved this result in the ﬁnite case only. Building upon our work in [25], we present
a proof of this unique characterization of the Fisher and Amari–Chentsov structure
in the general situation of an arbitrary sample space Ω. This is one of the main
results derived in this book. In fact, we shall prove a very general result that classi-
ﬁes all tensors that are invariant under congruent Markov kernels. These statistical
aspects of information geometry are taken up in Chap. 5 for the general case of an
arbitrary Ω, with reference to the case of a ﬁnite Ω already treated in Chap. 2. We
shall now describe this in some more detail.
Let κ : Ω →Ω′ be a statistic (see (1.2)). Such a κ then induces a map κ∗on
signed measures via
κ∗μ(A) := μ

ω ∈Ω : κ(ω) ∈A

= μ

κ−1A

,
and thus a statistical model (M,Ω,p) on Ω gets transformed into one on Ω′,
(M,Ω′,p′). In general, it will be more difﬁcult to recover the parameter ξ ∈M
from κ⋆p(·;ξ) by observations on ω′ ∈Ω′ than from the original p(·;ξ) through
observations of x ∈Ω, because κ might map several ω into the same ω′. In fact, if
we put
g′
kl(ξ) =

p′
ω′;ξ
∂logp′(ω′;ξ)
∂ξk
∂logp′(ω′;ξ)
∂ξl
dμ′
ω′
(1.21)
then
(gkl) ≥

g′
kl

(1.22)
in the sense of tensors, that is, the difference is a nonnegative deﬁnite tensor. When
no information is lost, the statistic is called sufﬁcient, and we have equality in (1.22).
(There are various characterizations of sufﬁcient statistics, and we shall show their
equivalence under our general conditions. Informally, a statistic is sufﬁcient for the
parameter ξ if having observed ω′, no further information about ξ can be obtained
from knowing which of the possible ω with κ(ω) = ω′ had occurred.)

12
1
Introduction
More generally, we consider a Markov kernel, that is
K : Ω →P

Ω′
.
(1.23)
For instance, we can consider conditional probability distributions p(ω′|ω) for
ω′ ∈Ω′. Of course, a statistic κ induces the Markov kernel Kκ where
Kκ(ω) = δκ(ω),
(1.24)
the Dirac measure at κ(ω). A Markov kernel K induces the Markov morphism
K∗: S(Ω) −→S

Ω′
,
K∗μ

A′
:=

Ω
K

ω;A′
dμ(ω),
(1.25)
that is, we simply integrate the kernel with respect to a measure on Ω to get a
measure on Ω′. In particular, a statistic κ then induces the Markov morphism Kκ
∗. It
turns out that it is expedient to consider invariance properties with respect to Markov
morphisms. While this will be technically important, in this Introduction, we shall
simply consider statistics.
When κ : Ω →Ω′ is a statistic, a Markov kernel L : Ω′ →P(Ω), i.e., going in
the opposite direction now, is called κ-congruent if
κ⋆

L

ω′
= δω′
for all ω′ ∈Ω′.
(1.26)
In order to assess the information loss caused by going from Ω to P(Ω′) via a
Markov kernel, there are two aspects
1. Several ω ∈Ω might get mapped to the same ω′ ∈Ω′. This clearly represents a
loss of information, because then we can no longer recover ω from observing ω′.
And if this distinction between the different ω causing the same ω′ is relevant
for estimating the parameter ξ, then lumping several ω into the same ω′ loses
information about ξ.
2. An ω ∈Ω gets diluted, that is, we have a distribution p(·|ω) in place of a single
value. By itself, this does not need to cause a loss of information. For instance,
for different values of ω, the corresponding distributions could have disjoint sup-
ports.
In fact, any Markov kernel can be decomposed into a statistic and a congruent
Markov kernel. That is, there is a Markov kernel Kcong : Ω →P( ˆΩ) which is con-
gruent w.r.t. some statistic κ1 : ˆΩ →Ω, and a statistic κ2 : ˆΩ →Ω′ such that
K = κ2∗Kcong.
(1.27)
Moreover, we have the general monotonicity theorem
Theorem 1.1
Let (M,Ω,p) be a statistical model on Ω as before, let K : Ω →
P(Ω′) be a Markov kernel, inducing the family p′(·;ξ) = K∗(p(·;ξ)). Moreover,
let gM and g′
M denote the corresponding Fisher metrics. Then
gM(V,V ) ≥g′
M(V,V )
for all V ∈TξM and ξ ∈M.
(1.28)

1.2
An Informal Description
13
If K = Kκ is the Markov kernel induced by a statistic κ as in (1.24), and if
(M,Ω,p) has a positive regular density function, equality here holds for all ξ and
all V if and only if the statistic κ is sufﬁcient.
When K = Kκ, the difference gM(V,V ) −g′
M(V,V ) can then be taken as the
information loss caused by the statistic κ.
Conversely, as already mentioned several times, we have
Theorem 1.2
The Fisher metric is the unique metric, and the Amari–Chentsov
tensor is the only 3-tensor (up to a constant scaling factor) that are invariant under
sufﬁcient statistics.
The Fisher metric also enters into the Cramér–Rao inequality of statistics. The
task of parametric statistics is to ﬁnd an element in the parameter space Ξ that
is most appropriate for describing the observations made in Ω. In this sense, one
deﬁnes an estimator as a map
ˆξ : Ω →Ξ
that associates to every observed datum x in Ω a probability distribution from the
class Ξ. As Ξ can also be considered as a family of product measures on ΩN
(N ∈N), we can also associate to every tuple (x1,...,xN) of observations an ele-
ment of Ξ. The most important example is the maximum likelihood estimator that
selects that element of Ξ which assigns the highest weight to the observation x
among all elements in Ξ.
Let ϑ : Ξ →Rd be coordinates on Ξ. We can then write the family Ξ as p(·;ϑ)
in terms of those coordinates ϑ. For simplicity, we assume that d = 1, that is, we
have only a single scalar parameter. The general case can be easily reduced to this
one.
We deﬁne the bias of an estimator ˆξ as
bˆξ(ϑ) := Eϑ ˆξ −ϑ,
(1.29)
where Eϑ stands for the expectation w.r.t. p(·;ϑ). The Cramér–Rao inequality then
says
Theorem 1.3 Any estimator ˆξ satisﬁes
Eϑ

(ˆξ −ϑ)2
≥
(1 + b′
ˆξ(ϑ))2
g(ϑ)
+ bˆξ(ϑ)2,
(1.30)
where ′ stands for a derivative w.r.t. ϑ.
In particular, when the estimator is unbiased, that is, bˆξ = 0, we have
Eϑ
ˆξ −Eϑ(ˆξ)
2
= Eϑ

(ˆξ −ϑ)2
≥
1
g(ϑ),
(1.31)
that is, the variance of ˆξ is bounded from below by the inverse of the Fisher metric.

14
1
Introduction
Here, g(ϑ) is an abbreviation for g(ϑ)( ∂
∂ϑ , ∂
∂ϑ ).
Thus, we see that the Fisher metric g(ϑ) measures how sensitively the probability
density p(ω;ϑ) depends on the parameter ϑ. When this is small, that is, when vary-
ing ϑ does not change p(ω;ϑ) much, then it is difﬁcult to estimate the parameter ϑ
from the data, and the variance of an estimator consequently has to be large.
All these statistical results will be shown in the general framework developed in
Chap. 3, that is, in much greater generality than previously known.
1.2.4 Exponential and Mixture Families from the Perspective
of Differential Geometry
Before those statistical applications, however, in Chap. 4, we return to the differ-
ential geometric aspects already introduced in Chap. 2. Recall that there are two
different afﬁne structures on our spaces of probability measures, one coming from
the simplex, the other from the exponential maps. Consequently, for each of these
structures, we have a notion of a linear family. For the ﬁrst structure, these are the
so-called mixture families
p(x;η) = c(x) +
d

i=1
gi(x)ηi,
depending on functions gi and c (which has to be adjusted to make p(·;η) into a
probability measure), where η1,...,ηn are the parameters. For the second structure,
we have the exponential families
p(x;ϑ) = exp

γ (x) + fi(x)ϑi −ψ(ϑ)

,
(1.32)
depending on functions fi (observables in a statistical mechanics interpretation)
with parameters ϑi and
ψ(ϑ) = log

exp

γ (x) + fi(x)ϑi
dx
(1.33)
being the normalization required to make p(·;ϑ) a probability distribution. In fact,
in statistical mechanics, ψ is known as the free energy. Of course, we can try to
write one and the same family in either the η or the ϑ parameters. Of course, the
relationship between them will be nonlinear. Remarkably, when working with the ϑ
parameters, we can obtain the Fisher metric from
gij = ∂i∂jψ(ϑ),
and the Amari–Chentsov tensor from
Tijk = ∂i∂j∂kψ(ϑ),

1.2
An Informal Description
15
where ∂i is the derivative w.r.t. ϑi. In particular, ψ(ϑ) is a strictly convex function
because its second derivatives are given by the Fisher metric, hence are positive
deﬁnite. It is important to point out at this stage that convexity here is meant in
the sense of afﬁne geometry, and not in the sense of Riemannian geometry. Con-
vexity here simply means that the matrix of ordinary second derivatives is positive
semideﬁnite, and this property is invariant under afﬁne coordinate transformations
only. In Riemannian geometry, one would rather require that the matrix of second
covariant derivatives be positive semideﬁnite, and this property is invariant under
arbitrary coordinate transformations because the transformation rules for covariant
derivatives involve a metric dependent term that compensates the possible nonlin-
earities of coordinate transformations. Thus, even though the second derivatives of
our function yield the metric tensor, the convexity involved here is an afﬁne notion.
(This is somewhat similar to Kähler geometry where a Kähler metric is a Hermitian
metric that is locally given by the complex Hessian of some potential function. Here,
the allowed transformations are the holomorphic ones, as opposed again to general
coordinate transformations. In fact, it turns out that those afﬁne structures that we
are considering here, that is, those that are locally derived from some strictly convex
potential function, can be seen as real analogues of Kähler structures.) In any case,
since we are dealing with a convex function, we can therefore pass to its Legendre
transform ϕ. This also induces a change of parameters, and remarkably, this is pre-
cisely the transition from the ϑ to the η parameters. With respect to the latter, ϕ is
nothing but the negative of the entropy of the probability distribution p, that is,
ϕ =

p(x;ϑ)logp(x;ϑ)dx.
(1.34)
This naturally yields the relations for the inverse metric tensor
gij = ∂i∂jϕ(η),
(1.35)
where now ∂i is a derivative w.r.t. ηi. Moreover, we have the duality relations
ηi = ∂iψ(ϑ),
ϑi = ∂iϕ(η).
These things will be explained and explored within the formalism of differential
geometry.
1.2.5 Information Geometry and Information Theory
The entropy occurring in (1.34) is also known as the Shannon information, and
it is the basic quantity of information theory. This naturally leads to the question
of the relations between the Fisher information, the basic quantity of information
geometry, and the Shannon information. The Fisher information is an inﬁnitesimal
quantity, whereas the Shannon information is a global quantity. One such relation

16
1
Introduction
is given in (1.35): The inverse of the Fisher metric is obtained from the second
derivatives of the Shannon information. But there is more. Given two probability
distributions, we have their Kullback–Leibler divergence
DKL(q∥p) :=
 
logq(x) −logp(x)

q(x)dx.
(1.36)
Here, we shall take as our base measure simply dx.
This quantity is nonnegative (DKL(q∥p) ≥0, with equality only for p = q), but
not symmetric in q and p (DKL(q∥p) ̸= DKL(p∥q) in general), and so, we cannot
take it as the square of a distance function. It turns out that the Fisher metric can
be obtained by taking second derivatives of DKL(q∥p) w.r.t. p at q = p, whereas
taking second derivatives there in the dual coordinates w.r.t. q yields the inverse of
the Fisher metric. In fact, this non-symmetry makes the relation between Shannon
entropy and information geometry more subtle and more interesting, as we shall
now brieﬂy explain.
Henceforth, for simplicity, we shall put γ (x) = 0 in (1.32), as γ will play no
essential role for the moment. As in (1.32), we assume that some functions (observ-
ables) fi, i = 1,...,n, are given, and that w.r.t. q, they have certain expectation
values,
Eq(fi) = ¯fi,
i = 1,...,n.
(1.37)
For any 0 ≤m ≤n, we then look for the probability distribution p(m) that has the
same expectation values for the functions fj, j = 1,...,m,
Ep(m)(fj) = ¯fj,
j = 1,...,m,
(1.38)
and that maximizes the entropy
H

p(m)
= −

logp(m)(x)p(m)(x)dx
(1.39)
among all distributions satisfying (1.38). An easy calculation shows that such a p(m)
is necessarily of the form (1.32) on the support of p(m), that is,
p(m)(x) = exp
 m

j=1
fj(x)ϑj −ψ(ϑ)

=: p(m)(x;ϑ)
(1.40)
for suitable coefﬁcients ϑj which are determined by the requirement (1.38). This
means that among all distributions with the same expectation values (1.38), the
exponential distribution (1.40) has the largest entropy. For this p(m)(x;ϑ), the
Kullback–Leibler distance from q becomes
DKL

q
p(m)
= −H(q) + H

p(m)
.
(1.41)

1.3
Historical Remarks
17
Since, as noted, p(m) maximizes the entropy among all distributions with the same
expectation values for the fj, j = 1,...,m, as q, the Kullback–Leibler divergence
in (1.41) is nonnegative, as it should be. Moreover, among all exponential distribu-
tions p(x;θ), we have
DKL

q
p(m)(·;ϑ)

= inf
θ DKL

q
p(m)(·;θ)

(1.42)
when the coefﬁcients ϑj are chosen to satisfy (1.38), as we are assuming. That is,
among all such exponential distributions that with the same expectation values as q
for the functions fj, j = 1,...,m, minimizes the Kullback–Leibler divergence. We
may consider this as the projection of the distribution q onto the family of expo-
nential distributions p(m)(x;θ) = exp(	m
j=1 fj(x)θj −ψ(θ)). Since, however, the
Kullback–Leibler divergence is not symmetric, this is not obtained by the geodesic
projection w.r.t. the Fisher metric; rather, two afﬁne ﬂat connections enter which are
dual w.r.t. the Fisher metric. These afﬁne ﬂat connections come from the two afﬁne
ﬂat structures described above.
The procedure can be iterated w.r.t. m, by projecting p(m)(·;ϑ) onto the exponen-
tial family of distributions p(m−1)(·;θ). As deﬁned above in (1.38), these families
are obtained by ﬁxing the expectation values of more and more observables. For
m = 0, we simply obtain the uniform distribution p0, and we have
DKL

q
p(k)
= DKL

q
p(n)
+ DKL

p(n)p(n−1)
+ ··· + DKL

p(k+1)p(k)
(1.43)
for k = 0,...,n, as will be shown in Sect. 4.3. This decomposition will be system-
atically explored in Sect. 6.1.
Other applications of information geometry presented in Chap. 6 will include
Monte Carlo methods, inﬁnite-dimensional Gibbs families, and evolutionary dy-
namics. The latter concerns the dynamics of biological populations subjected to the
effects of selection, mutation, and random sampling. Those structures can be natu-
rally interpreted in terms of information geometric concepts.
1.3 Historical Remarks
In 1945, in his fundamental paper [219] (see also [220]), Rao used Fisher infor-
mation to deﬁne a Riemannian metric on a space of probability distributions and,
equipped with this tool, to derive the Cramér–Rao inequality. Differential geom-
etry was not well-known at that time, and only 30 years later, in [89], Efron ex-
tended Rao’s ideas to the higher-order asymptotic theory of statistical inference.1
He deﬁned smooth subfamilies of larger exponential families and their statistical
1In [11, p. 67] Amari uncovered a less known work of Harold Hotelling on the Fisher information
metric submitted to the American Mathematical Society Meeting in 1929. We refer the reader to
[11] for details.

18
1
Introduction
curvature, which, in the language of Riemannian geometry, is the second funda-
mental form of the subfamilies regarded as Riemannian submanifolds in the Rie-
mannian manifold of the underlying exponential family provided with the Fisher
metric. (In [16, p. 23] statistical curvature is also called embedding curvature or e-
curvature and totally geodesic submanifolds are called autoparallel submanifolds.)
Efron named those smooth subfamilies “curved exponential families.” In 1946–
1948, the geophysicist and Bayesian statistician Jeffreys introduced what we today
call the Kullback–Leibler divergence, and discovered that for two distributions that
are inﬁnitely close we can write their Kullback–Leibler divergence as a quadratic
form whose coefﬁcients are given by the elements of the Fisher information matrix
[131, 132]. He interpreted this quadratic form as the length element of a Riemannian
manifold, with the Fisher information playing the role of the Riemannian metric.
From this geometrization of the statistical model, he derived his prior distributions
as the measures naturally induced by the Riemannian metric.
In 1955, in his lectures at the H. Poincaré Institute, Kolmogorov discussed the
problem of the existence of natural differentiable structures on ensembles of prob-
ability distribution. Following a suggestion by Morozova, see [188], Chentsov de-
ﬁned an afﬁne ﬂat connection (the e-connection) on the set P+(Ω,μ) [60]. Further,
analyzing the “naturality” condition for differentiable structures, Chentsov invented
the category of mathematical statistics [61]. This category was introduced indepen-
dently and almost at the same time by Morse and Sacksteder [189], using foun-
dational ideas of Wald [252] and Blackwell [49] in the statistical decision theory
and under the inﬂuence of the categorical approach in algebraic topology that was
very fashionable at that time. The morphisms in the Chentsov category of mathe-
matical statistics are Markov morphisms and geometric notions on probability dis-
tribution ensembles are required to be invariant under Markov morphisms [65]. In
his inﬂuential book [65] Chentsov considered only geometry on probability distri-
bution spaces P+(Ω,μ) for ﬁnite sample spaces Ω, referring to technical difﬁcul-
ties of treating inﬁnite-dimensional differentiable manifolds. The only exceptions
are curved exponential families—subfamilies of the canonical exponential families
deﬁned by the e-connection in [60]. Using the categorical approach, in particular
Markov morphisms and a related notion of congruent embedding, see Deﬁnition 5.1,
Chentsov discovered the Amari–Chentsov connections and proved the uniqueness
of the Amari–Chentsov structure by their invariance under sufﬁcient statistics [65].
Independently, inspired by the Efron paper and Dawid’s discussion on it [89],
Amari deﬁned in [6, 7] the notion of α-connections and showed its usefulness in the
asymptotic theory of statistical estimation. In particular, using geometric methods,
Amari achieved Fisher’s life-long dream of showing that the maximal likelihood
estimator is optimal [7, 8, 16], see also Sect. 5.2 below. Shortly after this, Amari and
Nagaoka introduced the notion of dual connections, developed the general theory of
dually ﬂat spaces, and applied it to the geometry of α-connections [194].
These events prepared the birth of information geometry, whose name appeared
for the ﬁrst time (in English) in [15], which was known before also as the differen-
tial geometrical theory of statistics. (Certain aspects of information geometry, e.g.,

1.3
Historical Remarks
19
results due to Morozova and Chentsov concerning invariants of pairs of probabil-
ity distributions, belong to Markovian categorial geometry [63, 188], which is not
necessarily a part of differential geometry. Thus the name “information geometry” is
more suitable, and it also sounds more attractive.) The ﬁeld of information geometry
developed in particular thanks to the work of Amari and his school. Further devel-
opments of information geometry were mainly focused on divergence functions and
their generalizations, and devoted to applications in statistical inference and infor-
mation theory, especially in higher-order asymptotic theory of statistical inference.
Here we would like to mention the papers by Csiszár [72–74] on divergence func-
tions, especially f -divergences and their invariant properties, and by Eguchi on the
dualistic geometry of general divergences (contrast functions) [91–93], and the pa-
pers on the geometry of manifolds with dually ﬂat connections [13, 34, 59, 236].
We recommend [11, 56] for a survey and bibliography on divergence geometry
and [9, 11, 16, 148, 188] for a survey of applications of information geometry in
the early period. Later applications of information geometry include neural net-
works, machine learning, evolutionary biology, etc. [11, 16]; see also Chap. 6 in our
book. Regarding Markovian categorial geometry, in addition to the aforementioned
papers by Chentsov and Morozova–Chentsov, we also would like to mention the
paper by Campbell [57] on an extension of the Chentsov theorem for ﬁnite sam-
ple spaces, which will be signiﬁcantly generalized in Chaps. 2 and 5. The papers
[186, 187] (see also [188, §6]) by Morozova and Chentsov on the geometry of α-
connections, in particular, giving an explicit description of totally geodesic subman-
ifolds of the manifold M+(I) provided with an α-connection, belong to the inter-
section of Markovian categorial geometry and divergence geometry. We note that
the dualistic geometries considered in the early period of information geometry, in
particular the geometry of curved exponential families, are not necessarily related to
ﬁnite sample spaces but they are supposed to be ﬁnite-dimensional [39, 41, 62, 90].
Amari [9], Lauritzen [160], Murray–Rice [192] have proposed general concepts of
ﬁnite-dimensional statistical models. Among further advancements in information
geometry are Lauritzen’s introduction of the notion of statistical manifolds and Lê’s
immersion theorem, which we shall discuss in Chap. 4. The inﬁnite-dimensional
information geometry and, in particular, inﬁnite-dimensional families of probability
distributions were ﬁrst considered in Pistone–Sempi’s work [216] in 1995, see also
our discussion in Chap. 3, and later in subsequent papers by Pistone and coauthors;
see, e.g., [106], and recently in [25], which combines the approach of Markovian
categorial geometry with functional analytical techniques. As an application we
have proved a version of the Chentsov theorem which will be generalized further
in this book. We would also like to mention [164] for another view on Chentsov’s
theorem and its generalizations.
Finally, we note that information geometry has been generalized for quantum
systems, but the related circle of questions lies outside the scope of our book, and
we refer the interested reader to excellent reviews in [16, 188, 214].

20
1
Introduction
1.4 Organization of this Book
Let us summarize the organization of our book. After this introduction, in Chap. 2,
we shall explain the basic constructions in the ﬁnite-dimensional case, that is, when
the underlying space Ω on which we study probability distributions has only ﬁnitely
many elements. This chapter also provides the natural context for a formal treatment
of interaction spaces and of hierarchical models, emphasizing the important special
class of graphical models. The space of probability distributions on such a ﬁnite
space as treated in this chapter is ﬁnite-dimensional. In the next Chap. 3, we consider
a general space Ω. Consequently, we shall have to deal with inﬁnite-dimensional
spaces of probability measures, and technical complications emerge. We are able,
however, to develop a functional analytic framework within which these compli-
cations can be overcome. We shall introduce and develop the important notion of
parametrized measure models and deﬁne suitable integrability properties, in order
to obtain the analogue of the structures considered in Chap. 2. These structures will
not depend on the choice of a base measure because we shall set up the framework in
such a way that all objects transform appropriately under a change of base measure.
We shall also discuss the structure of Pistone and Sempi. The following Chap. 4
will develop the differential geometry of statistical models. This includes dualis-
tic structures, consisting of a Riemannian metric and two connections that are dual
to each other with respect to that metric. When these connections are torsion-free,
such a structure can more compactly be described as a statistical model, given by
a metric, that is, a symmetric positive deﬁnite 2-tensor, and a symmetric 3-tensor.
These are abstract versions of the Fisher metric and the Amari–Chentsov tensor.
Alternatively, it can be described through a divergence. Any statistical model can
be isostatistically immersed into a standard model deﬁned by an Amari–Chentsov
structure, and this then provides the link between the abstract differential geometry
of Chap. 4 and the general functional analysis of Chap. 3. When these connections
are even ﬂat, the structure can be locally obtained from potential functions, that is,
convex functions whose second derivatives yield the metric and whose third deriva-
tives yield the 3-tensor. Here, convexity is considered as an afﬁnely invariant notion,
and consequently, we need to discuss the underlying afﬁne structure. This also gives
rise to a dual structure via the Legendre transform of the convex function. That
is, we shall ﬁnd a pair of dual afﬁne structures, and this is the geometry discov-
ered by Amari and Chentsov. Chapter 5 will turn to the statistical aspects. We shall
present one of the main results of information geometry, that the Fisher metric and
the Amari–Chentsov tensor are characterized by their invariance under sufﬁcient
statistics. Our treatment of sufﬁcient statistics here is more general than what can
be found in statistics texts. Also, we shall discuss estimators and derive a general
version of the Cramér–Rao inequality within our framework. In the last chapter, we
shall connect our treatment of information geometry with various applications and
other ﬁelds. Building upon the treatment of interaction spaces and of hierarchical
models in Chap. 2, we shall describe information theoretical complexity measures
and applications of information geometry to Markov chains. This yields an approach
to the analysis of systems of interacting units. Moreover, we shall discuss how infor-
mation geometry provides a natural setting for the basic structures of mathematical

1.4
Organization of this Book
21
biology, like mathematical population genetics, referring for a more detailed pre-
sentation to [124], however. We shall also brieﬂy sketch a formal relationship of
information geometry with functional integrals, the Gibbs families of statistical me-
chanics. In fact, these connections with statistical physics already shine through in
Sect. 4.3. Here, however, we only offer an informal way of thinking without techni-
cal rigor. We hope that this will be helpful for a better appreciation of the meaning
of various concepts that we have treated elsewhere in this book. In an appendix, we
provide a systematic, but brief, overview of the basic concepts and results from mea-
sure theory, Riemannian geometry and Banach manifolds on which we shall freely
draw in the main text.
The standard reference for the development based on differential geometry is
Amari’s book [8] (see also the more recent treatment [16], and also [192]). In fact,
from statistical mechanics, Balian et al. [38] arrived at similar constructions. The
development of information geometry without the requirement of a differentiable
structure is based on Csiszár’s work [75] and has been extended in [76] and [188]. In
identifying unique mathematical structures of information geometry, invariance as-
sumptions due to Chentsov [65] turn out to be fundamental. A systematic approach
to the theory has been developed in [25]. The geometric background material can
be found in [137].
When we speak about geometry in this book, we mean differential geometry. In
fact, however, differential geometry is not the only branch of geometry that is useful
for statistics. Algebraic geometry is also important, and the corresponding approach
is called algebraic statistics. Algebraic statistics treats statistical models for discrete
data whose probabilities are solution sets of polynomial equations of the param-
eters. It uses tools of computational commutative algebra to determine maximum
likelihood estimators. Algebraic statistics also works with mixture and exponential
families; they are called linear and toric models, respectively. While information ge-
ometry is concerned with the explicit representation of models through parametriza-
tions, algebraic statistics highlights the fact that implicit representations (through
polynomial equations) provide additional important information about models. It
utilizes tools from algebraic geometry in order study the interplay between explicit
and implicit representations of models. It turns out that this study is particularly
important for understanding closures of models. In this regard, we will present im-
plicit descriptions of exponential families and their closures in Sect. 2.8.2. In the
context of graphical models and their closures [103], this leads to a generalization
of the Hammersley–Clifford theorem. We shall prove the original version of this
theorem in Sect. 2.9.3, following Lauritzen’s presentation [161]. In this book, we
do not address further aspects of algebraic statistics and refer to the monographs
[84, 107, 209, 215] and the seminal work of Diaconis and Sturmfels [82].
When we speak about information, we mean classical information theory à la
Shannon. Nowadays, there also exists the active ﬁeld of quantum information the-
ory. The geometric aspects of quantum information theory are explained in the
monograph [45].
We do not assume that the reader possesses a background in statistics. We rather
want to show how statistical concepts can be developed in a manner that is both

22
1
Introduction
rigorous and general with tools from information theory, differential geometry, and
functional analysis.
On the Notation and Some Conventions
We have two kinds of spaces, a measure space Ω on which our measures live,
and a parameter space M that parametrizes a family of measures. The elements
of Ω will be denoted by either x or ω. Depending on the circumstances, Ω may
carry some further structure, like a topology or a differentiable structure. When the
measure space is ﬁnite, we denote it by I instead, and its elements by i, to conform
to standard conventions. In the ﬁnite case, these elements i will usually be written
as indices.
The parameters in M will usually be denoted by ξ. When we have particular
parametrized families of measures, we use other Greek minuscules, more precisely
ϑ for the parameters of exponential families, and η for those of mixture families.
An estimator for the parameter ξ is written as ˆξ, as usual in the statistics literature.
We use the letter μ, ν, or m to indicate a general ﬁnite measure, and p to stand
for a probability measure. Calligraphic letters will stand for spaces of measures, and
so, M(Ω) and P(Ω) will denote spaces of general or probability measures on Ω.
Since our (probability) measures will live on Ω, but depend on a parameter ξ ∈M,
we write p(x;ξ) to indicate these dependencies. When the element of Ω plays no
role, we may also simply write p(ξ). Thus, in the context of ξ-dependence, p(ξ) is
a general ﬁnite measure, not necessarily a probability measure.
We shall often need to use some base measure on Ω, which will be denoted by μ
or μ0. The integration of an integrable (w.r.t. μ) function f , that is, f ∈L1(Ω,μ),
will be written as

f (x)dμ(x); thus, when we carry out an integration, we shall
write dμ in place of μ. Also, the pairing between a function f ∈L∞(Ω,μ) and
a measure φμ with φ ∈L1(Ω,μ) will be written as (f,φ) =

f (x)φ(x)dμ(x)
whereas an L2-product will be denoted by ⟨h,k⟩=

h(x)k(x)dμ(x).
In the ﬁnite case, we shall use somewhat different conventions. Of course, we
shall then use sums in place of integrals. Here, Σn−1 = {(p1,...,pn) : pi ≥0,
	
j pj = 1} is the unit simplex of probability measures on a space I of n elements.
As will become clear in our main text, instead of Σn−1, it is often more natural to
consider the positive sector of the sphere; for a somewhat annoying technical reason,
it is better to take the sphere of radius 2 instead of the unit sphere. Therefore, we
shall work with Sn−1
2,+ = {(q1,...,qn) : qi ≥0,	
j(qj)2 = 4}. Σn−1 and Sn−1
2,+ are
homeomorphic, of course, but often, the latter is better suited for our purposes than
the former.
Ep will mean the expectation value w.r.t. the (probability) measure p.
We shall often use the normal or Gaussian distribution on Rd with center or mean
x and covariance matrix Λ = (λij),
N(y;x,Λ) =
1

2πd|Λ|
exp

−λij(xi −yi)(xj −yj)
2

,
(1.44)

1.4
Organization of this Book
23
putting |Λ| := det(λij) and denoting the inverse of Λ by Λ−1 = (λij), and where
the standard summation convention is used (see Appendix B). We shall also simply
write N(x,Λ) for N(·;x,Λ).
Our key geometric objects are the Fisher metric and the Amari–Chentsov tensor.
Therefore, they are denoted by single letters, g for the Fisher metric, and T for the
Amari–Chentsov tensor. Consequently, for instance, other metrics will be denoted
differently, by g or by other letters, like h.
We shall always write log for the logarithm, and in fact, this will always mean
the natural logarithm, that is, with basis e.

Chapter 2
Finite Information Geometry
The considerations of this chapter are restricted to the situation of probability distri-
butions on a ﬁnite number of symbols, and are hence of a more elementary nature.
We pay particular attention to this case for two reasons. On the one hand, many ap-
plications of information geometry are based on statistical models associated with
ﬁnite sets, and, on the other hand, the ﬁnite case will guide our intuition within the
study of the inﬁnite-dimensional setting. Some of the deﬁnitions in this chapter can
and will be directly extended to more general settings.
2.1 Manifolds of Finite Measures
Basic Geometric Objects
We consider a non-empty and ﬁnite set I.1 The real
algebra of functions I →R is denoted by F(I), and its unity 1I or simply 1 is
given by 1(i) = 1, i ∈I. This vector spans the space R · 1 := {c · 1 ∈F(I) : c ∈R}
of constant functions which we also abbreviate by R. Given a function g : R →R
and an f ∈F(I), by g(f ) we denote the composition i →g(f )(i) := g(f (i)).
The space F(I) has the canonical basis ei, i ∈I, with
ei(j) =
1,
if i = j,
0,
otherwise,
and every function f ∈F(I) can be written as
f =

i∈I
f i ei,
where the coordinates f i are given by the corresponding values f (i). We natu-
rally interpret linear forms σ : F(I) →R as signed measures on I and denote the
1This set I will play the role of the no longer necessarily ﬁnite space Ω in Chap. 3.
© Springer International Publishing AG 2017
N. Ay et al., Information Geometry, Ergebnisse der Mathematik und ihrer Grenzgebiete.
3. Folge / A Series of Modern Surveys in Mathematics 64,
DOI 10.1007/978-3-319-56478-4_2
25

26
2
Finite Information Geometry
corresponding dual space F(I)∗, the space of R-valued linear forms on F(I), by
S(I). In a more general context, this interpretation is justiﬁed by the Riesz repre-
sentation theorem. Here, it allows us to highlight a particular geometric perspective,
which makes it easier to introduce natural information-geometric objects. Later in
the book, we will treat general signed measures, and thereby have to carefully dis-
tinguish between various function spaces and their dual spaces.
The space S(I) has the dual basis δi, i ∈I, deﬁned by
δi(ej) :=
1,
if i = j,
0,
otherwise.
Each element δi of the dual basis corresponds, interpreted as a measure, to the Dirac
measure concentrated in i. A linear form μ ∈S(I), with μi := μ(ei), then has the
representation
μ =

i∈I
μi δi
with respect to the dual basis. This representation highlights the fact that μ can be
interpreted as a signed measure, given by a linear combination of Dirac measures.
The basis ei, i ∈I, allows us to consider the natural isomorphism between F(I)
and S(I) deﬁned by ei →δi. Note that this isomorphism is based on the existence
of a distinguished basis of F(I). Information geometry, on the other hand, aims at
identifying structures that are independent of such a particular choice of a basis.
Therefore, the canonical basis will be used only for convenience, and all relevant
information-geometric structures will be independent of this choice.
In what follows, we introduce several submanifolds of S(I) which play an im-
portant role in this chapter and which will be generalized and studied later in the
book:
Sa(I) :=

i∈I
μi δi :

i∈I
μi = a

(for a ∈R),
M(I) :=

μ ∈S(I) : μi ≥0 for all i ∈I

,
M+(I) :=

μ ∈M(I) : μi > 0 for all i ∈I

,
P(I) :=

μ ∈M(I) : μi ≥0 for all i ∈I, and

i∈I
μi = 1

,
P+(I) :=

μ ∈P(I) : μi > 0 for all i ∈I, and

i∈I
μi = 1

.
(2.1)
Obviously, we have the following inclusion chain of submanifolds of S(I):
P+(I) ⊆M+(I) ⊆S(I).

2.1
Manifolds of Finite Measures
27
In Sect. 3.1, we shall also alternatively interpret P+(I) as the set of measures in
M+(I) that are deﬁned up to a scaling factor, that is, as the projective space asso-
ciated with M+(I). From that point of view, P+(I) is a positive spherical sector
rather than a simplex.
Tangent and Cotangent Bundles
We start with the vector space S(I). Given a
point μ ∈S(I), clearly the tangent space is given by
TμS(I) = {μ} × S(I).
(2.2)
Consider the natural identiﬁcation of S(I)∗= F(I)∗∗with F(I):
F(I) −→S(I)∗,
f −→

S(I) →R,μ →μ(f )

.
(2.3)
With this identiﬁcation, the cotangent space of S(I) in μ is given by
T ∗
μS(I) = {μ} × F(I).
(2.4)
As an open submanifold of S(I), M+(I) has the same tangent and cotangent space
at a point μ ∈M+(I):
TμM+(I) = {μ} × S(I),
T ∗
μM+(I) = {μ} × F(I).
(2.5)
Finally, we consider the manifold P+(I). Obviously, for μ ∈P+(I) we have
TμP+(I) = {μ} × S0(I).
(2.6)
In order to specify the cotangent space, we consider the natural identiﬁcation map
(2.3). In terms of this identiﬁcation, each f ∈F(I) deﬁnes a linear form on S(I),
which we now restrict to S0(I). We obtain the map ρ : F(I) →S0(I)∗that as-
signs to each f the linear form S0(I) →R, μ →μ(f ). Obviously, the kernel of ρ
consists of the space of constant functions, and we obtain the natural isomorphism
ρ : F(I)/R →S0(I)∗, f + R →ρ(f + R) := ρ(f ). It will be useful to express
the inverse ρ−1 in terms of the basis δi, i ∈I, of S(I). In order to do so, assume
f ∈S0(I)∗, and consider an extension 
f ∈S(I)∗. One can easily see that, with
f i := 
f (δi), i ∈I, the following holds:
ρ−1(f ) =

i∈I
f i ei

+ R.
(2.7)
Summarizing, we obtain
T ∗
μP+(I) = {μ} ×

F(I)/R

(2.8)
as the cotangent space of P+(I) at μ.

28
2
Finite Information Geometry
Fig. 2.1 Illustration of the
chart ϕ for n = 2, with the
two coordinate vectors
δ1 −δ3 and δ2 −δ3
After having speciﬁed tangent and cotangent spaces at individual points μ, we
ﬁnally list the corresponding tangent and cotangent bundles:
T S(I) = S(I) × S(I),
T ∗S(I) = S(I) × F(I),
T M+(I) = M+(I) × S(I),
T ∗M+(I) = M+(I) × F(I),
T P+(I) = P+(I) × S0(I),
T ∗P+(I) = P+(I) ×

F(I)/R

.
(2.9)
Example 2.1 (Local coordinates) In this example we consider a natural coordinate
system of P+(I) which is quite useful (see Fig. 2.1). We assume I = [n + 1] =
{1,...,n,n + 1}. With the open set
U :=

x = (x1,...,xn) ∈Rn : xi > 0 for all i, and
n

i=1
xi < 1

,
we consider the map
ϕ : P+(I) →U,
μ =
n+1

i=1
μi δi →ϕ(μ) := (μ1,...,μn)
and its inverse
ϕ−1 : U →P+(I),
(x1,...,xn) →
n

i=1
xi δi +

1 −
n

i=1
xi

δn+1.
We have the coordinate vectors
∂
∂xi

μ
= ∂ϕ−1
∂xi

ϕ(μ)
= δi −δn+1,
i = 1,...,n,

2.2
The Fisher Metric
29
which form a basis of S0(I). Formula (2.7) allows us to identify its dual basis with
the following basis of F(I)/R:
dxi := ei + R,
i = 1,...,n.
Each vector f + R in F(I)/R can be expressed with respect to dxi, i = 1,...,n:
f + R =
 n+1

i=1
f i ei

+ R
=
n+1

i=1
f i (ei + R)
=
n

i=1
f i (ei + R) + f n+1(en+1 + R)
=
n

i=1
f i (ei + R) + f n+1

1 −
n

i=1
ei

+ R

=
n

i=1
f i(ei + R) −
n

i=1
f n+1(ei + R)
=
n

i=1

f i −f n+1
(ei + R).
The coordinate system of this example will be useful for explicit calculations later
on.
2.2 The Fisher Metric
The Deﬁnition
Given a measure μ ∈M+(I), we have the following natural L2-
product on F(I):
⟨f,g⟩μ = μ(f · g),
f,g ∈F(I).
(2.10)
This product allows us to consider the vector space isomorphism
F(I) −→S(I),
f −→f μ := ⟨f,·⟩μ.
(2.11)
The notation f μ emphasizes that, via this isomorphism, functions deﬁne linear
forms on F(I) in terms of densities with respect to μ. The inverse, which we de-
note by φμ, maps linear forms to functions and represents a simple version of the

30
2
Finite Information Geometry
Radon–Nikodym derivative with respect to μ:
φμ : S(I) −→F(I) = S(I)∗,
a =

i
ai δi −→da
dμ :=

i
ai
μi
ei.
(2.12)
This gives rise to the formulation of (2.10) on the dual space of F(I):
⟨a,b⟩μ = μ
 da
dμ · db
dμ

=

i
1
μi
aibi,
a,b ∈S(I).
(2.13)
With this metric, the orthogonal complement of S0(I) in S(I) is given by R · μ =
{λ · μ : λ ∈R}, and we have the orthogonal decomposition a = Π⊤
μ a + Π⊥
μ a of
vectors a ∈S(I), where
Π⊤
μ (a) =

i∈I

ai −μi

j∈I
aj

δi,
Π⊥
μ (a) =

i∈I

μi

j∈I
aj

δi.
(2.14)
If we restrict this metric to S0(I) ⊆S(I), then we obtain the following identiﬁcation
of S0(I) with the dual space:
φμ : S0(I) −→F(I)/R = S0(I)∗,
a −→da
dμ + R.
(2.15)
With the natural inclusion map ı : S0(I) →S(I), and ıμ := φμ ◦ı ◦φμ−1, the fol-
lowing diagram commutes:
S(I)
φμ
S(I)∗
S0(I)
ı
φμ
S0(I)∗
ıμ
(2.16)
This diagram deﬁnes linear maps between tangent and cotangent spaces in the in-
dividual points of M+(I) and P+(I). Collecting all these maps to corresponding
bundle maps, we obtain a commutative diagram between the tangent and cotangent
bundles:
T M+(I)
φ
T ∗M+(I)
T P+(I)
ı
φ
T ∗P+(I)
ı
(2.17)

2.2
The Fisher Metric
31
The inner product (2.13) deﬁnes a Riemannian metric on M+(I), on which the
maps φ and φ are based.
Deﬁnition 2.1 (Fisher metric) Given two vectors A = (μ,a) and B = (μ,b) of the
tangent space TμM+(I), we consider
gμ(A,B) := ⟨a,b⟩μ.
(2.18)
This Riemannian metric g on M+(I) is called the Fisher metric.
The Fisher metric was introduced as a Riemannian metric by Rao [219]. It is
relevant for estimation theory within statistics and also appears in mathematical
population genetics where it is known as the Shahshahani metric [123, 124]. We
shall outline the biological perspective of this metric in Sect. 6.2.
We now express the Fisher metric with respect to the coordinates of Example 2.1,
where we concentrate on the restriction of the Fisher metric to P+(I). With respect
to the chart ϕ of Example 2.1, the ﬁrst fundamental form of the Fisher metric is
given as
gij(μ) =
n

k=1
1
μk
δki δkj +
1
μn+1
=
 1
μi +
1
μn+1 ,
if i = j,
1
μn+1 ,
otherwise.
(2.19)
The inverse of this matrix is given as
gij(μ) =
μi (1 −μi),
if i = j,
−μi μj,
otherwise.
(2.20)
Written as matrices, we have
G(μ) := (gij)(μ) =
1
μn+1
⎛
⎜⎜⎜⎝
μn+1
μ1 + 1
1
···
1
1
μn+1
μ2 + 1
···
1
...
...
...
...
1
1
···
μn+1
μn + 1
⎞
⎟⎟⎟⎠,
(2.21)
G−1(μ) =

gij
(μ) =
⎛
⎜⎜⎜⎝
μ1 (1 −μ1)
−μ1 μ2
···
−μ1 μn
−μ2 μ1
μ2 (1 −μ2)
···
−μ2 μn
...
...
...
...
−μn μ1
−μn μ2
···
μn (1 −μn)
⎞
⎟⎟⎟⎠.
(2.22)
This is nothing but the covariance matrix of the probability distribution μ, in the
following sense. We draw the element i ∈{1,...,n} with probability μi, and we
put Xi = 1 and Xj = 0 for j ̸= i when i happens to be drawn. We then have the
expectation values
μi = E(Xi) = E

X2
i

,
(2.23)

32
2
Finite Information Geometry
and hence, the variances and covariances are
Var(Xi) = μi(1 −μi),
Cov(XiXj) = −μiμj
for j ̸= i,
(2.24)
that is, (2.22). In fact, this is the statistical origin of the Fisher metric as a covariance
matrix [219].
The Fisher metric is an example of a covariant 2-tensor on M, in the sense of the
following deﬁnition (see also (B.16) and (B.17) in Appendix B).
Deﬁnition 2.2
A covariant n-tensor Θ on a manifold M is a collection of n-
multilinear maps
Θξ :
n×TξM −→R,
(V1,...,Vn) −→Θξ(V1,...,Vn)
which is continuous in the sense that for continuous vector ﬁelds Vi the function
ξ →Θξ(V1,...,Vn) is continuous.
If f : M1 →M2 is a differentiable map between the manifolds M1 and M2, then
it can be used to pull back covariant n-tensors. That is, if Θ is a covariant n-tensor
on M2, then its pullback to M1 by f is deﬁned to be the tensor f ∗(Θ) on M1 given
as
f ∗(Θ)ξ(V1,...,Vn) := Θf (ξ)
 ∂f
∂V1
,..., ∂f
∂Vn

.
(2.25)
Information geometry deals with statistical models, that is, submanifolds of
P+(I). Instead of considering submanifolds directly, we take a slightly different
perspective here. We consider statistical models as a manifold together with an
embedding into P+(I), or more generally, into M+(I). To be more precise, let
be M an n-dimensional (differentiable) manifold and M →M+(I), ξ →p(ξ) =
	
i∈I pi(ξ)δi, an embedding. The pullback (2.25) of the Fisher metric g deﬁnes a
metric on M. More precisely, for A,B ∈TξM we set
gξ(A,B) := p∗(g)ξ(A,B)
(2.25)
= gp(ξ)
 ∂p
∂A, ∂p
∂B

=

i
1
pi(ξ)
∂pi
∂A (ξ)∂pi
∂B (ξ)
=

i
pi(ξ)∂logpi
∂A
(ξ)∂logpi
∂B
(ξ).
(2.26)
This representation of the Fisher metric is more familiar within the standard
information-geometric treatment.

2.2
The Fisher Metric
33
Fig. 2.2 Mapping from simplex to sphere
Extending the Fisher Metric to the Boundary
As is obvious from (2.13) and
also from the ﬁrst fundamental form (2.19), the Fisher metric is not deﬁned at the
boundary of the simplex. It is, however, possible to extend the Fisher metric to
the boundary by identifying the simplex with a sector of a sphere in RI = F(I).
In order to be more precise, we consider the following sector of the sphere with
radius 2 (or, equivalently (up to the factor 2, of course), the positive part of the
projective space, according to the interpretation of the set of probability measures
as a projective version of the space of positive measures alluded to above and taken
up in Sect. 3.1):
S2,+(I) :=

f ∈F(I) : f (i) > 0 for all i ∈I, and

i
f 2(i) = 4

.
As a submanifold of F(I) it carries the induced standard metric ⟨·,·⟩of F(I). We
consider the following diffeomorphism (see Fig. 2.2):
π1/2 : P+(I) →S2,+(I),
μ =

i
μi δi →2

i
√μiei.
Note that ∥π1/2(μ)∥=
	
i(2√μi)2 = 2
	
i μi = 2.
Proposition 2.1 The map π1/2 is an isometry between P+(I) with the Fisher met-
ric g and S2,+(I) with the induced canonical scalar product of F(I):
∂π1/2
∂A (μ), ∂π1/2
∂B
(μ)
 
= gμ(A,B),
A,B ∈TμP+(I).
That is, the Fisher metric coincides with the pullback of the standard metric on F(I)
by the map π1/2. In particular, the extension of the standard metric on S2,+(I) to
the boundary can be considered as an extension of the Fisher metric.

34
2
Finite Information Geometry
Proof With a,b ∈S0(I) such that A = (μ,a) and B = (μ,b), we have
∂π1/2
∂A (μ), ∂π1/2
∂B
(μ)
 
=
 d
dt π1/2(μ + ta)

t=0
, d
dt π1/2(μ + tb)

t=0
 
=

i
1
√μi
ai ·
1
√μi
bi
= gμ(A,B).
□
Fisher and Hellinger Distance
Proposition 2.1 allows us to give an explicit for-
mula for the Riemannian distance between two points μ,ν ∈P+(I) which is de-
ﬁned as
dF (μ,ν) :=
inf
γ :[r,s]→P+(I)
γ (r)=μ,γ (s)=ν
L(γ ),
where L(γ ) denotes the length of a curve γ : [r,s] →P+(I) given by
L(γ ) =
 s
r
 ˙γ (t)

γ (t) dt =
 s
r

gγ (t)

˙γ (t), ˙γ (t)

dt.
We refer to dF as the Fisher distance. With Proposition 2.1 we directly obtain the
following corollary.
Corollary 2.1 Let d : S2,+(I) × S2,+(I) →R denote the metric that is induced
from the standard metric on F(I). Then
dF (μ,ν) = d

π1/2(μ),π1/2(ν)

= 2arccos

i
√μi νi

.
(2.27)
Proof We have
cosα =
⟨π1/2(μ),π1/2(ν)⟩
∥π1/2(μ)∥· ∥π1/2(ν)∥=
	
i(2√μi)(2√νi)
2 · 2
=

i
√μi νi.
This implies
dF (μ,ν)
2
= α = arccos

i
√μi νi

.
□
A distance measure that is closely related to the Fisher distance is the Hellinger
distance. It is not induced from F(I) onto S2,+(I) but restricted to S2,+(I):
dH(μ,ν) :=
!
i
(√μi −√νi)2.
(2.28)

2.2
The Fisher Metric
35
Fig. 2.3 Illustration of the
relation between the Fisher
distance dF (μ,ν) and the
Hellinger distance dH (μ,ν)
of two probability measures
μ and ν, see Eq. (2.29)
We have the following relation between dF and dH (see Fig. 2.3):
dH(μ,ν) =
!
i
(√μi −√νi)2
=
!
i
(μi −2√μi νi + νi)
=
"
#
#
$2

1 −

i
√μi νi

=
!
2

1 −cos
1
2 dF (μ,ν)

.
(2.29)
Chentsov’s Characterization of the Fisher Metric
In what follows, we present
a classical characterization of the Fisher metric through invariance properties. This
is due to Chentsov [64].
Consider two non-empty and ﬁnite sets I and I ′. A Markov kernel is a map
K : I →P

I ′
,
i →Ki :=

i′∈I ′
Ki
i′ δi′.
(2.30)
Particular examples of Markov kernels are given in terms of (deterministic) maps
f : I →I ′. Given such a map, we simply deﬁne Kf by i →δf (i). Each Markov
kernel K induces a corresponding map between probability distributions:
K∗: P(I) →P

I ′
,
μ =

i∈I
μi δi →

i∈I
μi Ki.
(2.31)

36
2
Finite Information Geometry
The map K∗is called the Markov morphism induced by K. Note that K∗may also
be regarded as a linear map K∗: S(I) →S(I ′). Given a map f : I →I ′, we use
f∗:= (Kf )∗as short-hand notation.
Now assume that |I| ≤|I ′|. We call a Markov kernel K congruent if there is a
partition Ai, i ∈I, of I ′, such that the following condition holds:
Ki
i′ > 0
⇔
i′ ∈Ai.
(2.32)
If K is congruent and μ ∈P+(I) then K∗(μ) ∈P+(I ′). This implies a differentiable
map
K∗: P+(I) →P+

I ′
,
and the differential in μ is given by
dμK∗: TμP+(I) →TK∗(μ)P+

I ′
,
(μ,ν −μ) →

K∗(μ),K∗(ν) −K∗(μ)

.
The following theorem has been proven by Chentsov.
Theorem 2.1 (Cf. [65, Theorem 11.1]) We assign to each non-empty and ﬁnite set
I a metric hI on P+(I). If for each congruent Markov kernel K : I →P(I ′) we
have invariance in the sense
hI
p(A,B) = hI ′
K∗(p)

dpK∗(A),dpK∗(B)

,
or for short (K∗)∗(hI ′) = hI in the notation of (2.25), then there is a constant α > 0,
such that hI = α gI for all I, where the latter is the Fisher metric on P+(I).
Proof Step 1: First we consider permutations π : I →I. The center cI :=
1
|I|
	
i∈I δi is left-invariant, that is, π∗(cI) = cI . With Ei := (cI,δi −cI) ∈
TcI P+(I), we also have
dcI π∗(Ei) = Eπ(i),
i ∈I.
For each i,j ∈I, i ̸= j, choose a transposition π of i and j, that is, π(i) = j,
π(j) = i, and π(k) = k if k /∈{i,j}. This implies
hI
ii(cI) = hI
cI (Ei,Ei) = hI
π∗(cI )

dcI π∗(Ei),dcI π∗(Ei)

= hI
cI (Eπ(i),Eπ(i))
= hI
cI (Ej,Ej) = hI
jj(cI) =: f1(n),
where we set n := |I|. In a similar way, we obtain that all hI
ij(cI) with i ̸= j coin-
cide. We denote them by f2(n). The functions f1(n) and f2(n) have to satisfy the
following:
f1(n) + (n −1)f2(n) =

j∈I
hI
ij(cI) =

j∈I
hI
cI (Ei,Ej)
= hI
cI

Ei,

j∈I
Ej

= hI
cI (Ei,0) = 0.

2.2
The Fisher Metric
37
Consider two vectors
a =

i∈I
ai δi,
b =

i∈I
bi δi.
Assuming a,b ∈S0(I), we have 	
i∈I ai = 0 and 	
i∈I bi = 0 and therefore
a =

i∈I
ai

δi −cI

,
b =

i∈I
bi

δi −cI

.
This implies for A = (cI,a) and B = (cI,b)
hI
cI (A,B) =

i,j∈I
ai bj hI
ij(cI) =

i∈I
ai bi hI
ii(cI) +

i,j∈I
i̸=j
ai bj hI
ij(cI)
= f1(n)

i∈I
ai bi + f2(n)

i,j∈I
i̸=j
ai bj
= −(n −1)f2(n)

i∈I
ai bi + f2(n)

i,j∈I
i̸=j
ai bj
= f2(n)

−n

i∈I
ai bi +

i,j∈I
ai bj

= −f2(n)

i∈I
1
1
n
ai bi
= −f2(n)gI
cI (A,B).
Step 2: In this step, we show that the function f (n) is actually independent of
n and therefore a constant. In order to do so, we divide each element i ∈I into
k elements. More precisely, we set I ′ := I × {1,...,k}. With the partition Ai :=
{(i,j) : 1 ≤j ≤k}, i ∈I, we deﬁne the Markov kernel K by
i →Ki = 1
k
k

j=1
δ(i,j).
This kernel satisﬁes K∗(cI) = cI ′, and we have
dcI K∗(Ei) = dcI K∗

cI,δi −cI

=

cI ′, 1
k
k

j=1

δ(i,j) −1
n

i′∈I
δ(i′,j)


38
2
Finite Information Geometry
=

cI ′, 1
k
k

j=1

δ(i,j) −1
nk

i′∈I
k

j′=1
δ(i′,j′)

= 1
k
k

j=1
E′
(i,j).
With r,s ∈I, r ̸= s, this implies
f2(n) = hI
cI (Er,Es) = hI ′
cI′

1
k
k

j=1
E′
r,j, 1
k
k

j=1
E′
s,j

= 1
k2
k

j1,j2=1
hI ′
cI′

E′
r,j1,E′
s,j2

= 1
k2 k2 f2(n · k) = f2(n · k).
Exchanging the role of n and k, we obtain f2(k) = f2(k · n) = f2(n) and therefore
−f2(n) is a positive constant in n, which we denote by α. In the center cI , we have
shown that
hI
cI (A,B) = α gI
cI (A,B) = 0,
A,B ∈TcI P+(I).
It remains to show that this equality also holds for all other points. This is our next
step.
Step 3: First consider a point μ ∈P+(I) that has rational coordinates, that is,
μ =

i∈I
ki
n δi,
with 	
i ki = n. We now deﬁne a set I ′ and a congruent Markov kernel K : I →
P(I ′) so that K∗(μ) = cI ′. With
I ′ :=
%
i∈I

{i} × {1,...,ki}

,
(“&” denotes the disjoint union) we consider the Markov kernel
K : i →1
ki
ki

j=1
δ(i,j).
Obviously, we have
dμK∗: A =

μ,

i∈I
ai δi

→dμK∗(A) =

cI ′,

i∈I
ki

j=1
ai
ki
δ(i,j)

.

2.3
Gradient Fields
39
This implies
hI
μ(A,B) = hI ′
K∗(μ)

dμK∗(A),dμK∗(B)

= α gI ′
cI′

dμK∗(A),dμK∗(B)

= α

i∈I
ki

j=1
1
1
n
ai
ki
bi
ki
= α

i∈I
ki
1
1
n
ai
ki
bi
ki
= α

i∈I
1
μi
ai bi
= α gI
μ(A,B).
We have this equality for all probability vectors μ with rational coordinates. As
we assume continuity with respect to the base point μ, the equality of hI
μ(A,B) =
α gI ′
μ(A,B) holds for all μ ∈P+(I).
□
2.3 Gradient Fields
In this section, we are going to study vector and covector ﬁelds on M+(I) and
P+(I). We begin with the ﬁrst case, which is the simpler one, and consider covector
ﬁelds given by a differentiable function V : M+(I) →R. The differential in μ is
deﬁned as the linear form
dμV : S(I) →R,
a →dμV (a) = ∂V
∂a (μ).
In terms of the canonical basis, we have
dμV =

i
∂iV (μ)ei ∈F(I),
(2.33)
where ∂iV (μ) := ∂V
∂μi (μ) := ∂V
∂δi (μ). This deﬁnes the covector ﬁeld
dV : M+(I) →F(I),
μ →dμV.
The Fisher metric g allows us to identify dμV with an element of TμM+(I) in
terms of the map φμ, the gradient of V in μ:
gradμV := φ−1
μ (dμV ) =

i
μi ∂iV (μ)δi.
(2.34)
Given a function f : M+(I) →F(I), μ →f (μ) = 	
i∈I f i(μ)ei, we can ask
whether there exists a differentiable function V such that f (μ) = dμV . In this case,
f is called exact. It is easy to see that f is exact if and only if the condition
∂f i
∂μj
= ∂f j
∂μi
(2.35)
holds on M+(I) for all i,j ∈I.

40
2
Finite Information Geometry
Now we come to vector and covector ﬁelds on P+(I). The commutative diagram
(2.17) allows us to relate sections to each other. Of particular interest are sections
in T ∗P+(I) = P+(I) × (F(I)/R) (covector ﬁelds) as well as sections in T P+(I)
(vector ﬁelds). As all bundles are of product form P+(I) × V, sections are given by
functions f : P+(I) →V. We assume that f is a C∞function. We will also use
C∞extensions 
f : U →V, where U is an open subset of S(I) containing P+(I),
and 
f |U = f . To simplify the notation, we will also use the same symbol f for the
extension 
f . Given a section f : P+(I) →F(I), we assign various other sections
to it:
f : P+(I) →R,
μ →f (μ) := μ(f (μ)) = 	
i μi f i(μ),
[f ] : P+(I) →(F(I)/R),
μ →f (μ) + R,
φ(f ) : P+(I) →S(I),
μ →f (μ)μ = 	
i μi f i(μ)δi,
'
f : P+(I) →S0(I),
μ →(f (μ) −f (μ))μ = 	
i μi(f i(μ) −f (μ))δi.
In what follows, we consider covector ﬁelds given by a differentiable function V :
P+(I) →R. The differential in μ is deﬁned as the linear form
dμV : TμP+(I) →R,
a →dμV (a) = ∂V
∂a (μ),
which deﬁnes a covector ﬁeld dV : μ →dμV ∈T ∗
μP+(I). In order to interpret it as
a vector in F(I)/R, consider an extension 
V : U →R of V to an open neighborhood
of P+(I). This yields a corresponding extension dμ
V : S(I) →R, and according to
(2.7) we have
dμV =

i
∂i 
V (μ)ei + R,
(2.36)
where ∂i 
V (μ) = ∂
V
∂δi (μ). The Fisher metric g allows us to identify dμV with an
element of TμP+(I) via the map φμ, the gradient of V in μ:
gradμV := φ−1
μ (dμV ).
(2.37)
(See (B.22) in Appendix B for the general construction.)
Proposition 2.2 Let V : P+(I) →R be a differentiable function, U an open subset
of S(I) that contains P+(I), and 
V : U →R a differentiable continuation of V ,
that is, 
V |P+(I) = V . Then the coordinates of gradμ V with respect to δi are given
as
(gradμ V )i = μi

∂i 
V (μ) −

j
μj ∂j 
V (μ)

,
i ∈I.

2.3
Gradient Fields
41
Proof This follows from (2.36), (2.37), and the deﬁnition of φμ. Alternatively, one
can show this directly: We have to verify
gμ(gradμ V,a) = dμV (a),
a ∈TμP+(I).
gμ(gradμ V,a) =

i
1
μi

μi

∂i 
V (μ) −

j
μj∂j 
V (μ)

ai
=

i
ai ∂i 
V (μ) −

i
ai

j
μj∂j 
V (μ)
(
)*
+
=0
= ∂
V
∂a (μ)
= lim
t→0

V (μ + ta) −
V (μ)
t
= lim
t→0
V (μ + ta) −V (μ)
t
= ∂V
∂a (μ)
= dμV (a).
□
Proposition 2.3 Consider a map f : U →F(I), μ →f (μ) = 	
i∈I f i(μ)ei, de-
ﬁned on a neighborhood of P+(I). Then the following statements are equivalent:
(1) The vector ﬁeld '
f is a Fisher gradient ﬁeld on P+(I).
(2) The covector ﬁeld [f ] : P+(I) →F(I)/R, μ →[f ](μ) := f (μ) + R, is exact,
that is, there exists a function V : P+(I) →R satisfying dμV = [f ](μ).
(3) The relation
∂f i
∂μj
+ ∂f j
∂μk
+ ∂f k
∂μi
= ∂f i
∂μk
+ ∂f k
∂μj
+ ∂f j
∂μi
(2.38)
holds on P+(I) for all i,j,k ∈I.
Proof (1) ⇔(2) This is clear.
(2) ⇔(3) The covector ﬁeld f + R is exact if and only if it is closed. The lat-
ter property is expressed in local coordinates. Without restriction of generality we
assume I = {1,...,n,n + 1} and choose the coordinate system of Example 2.1.
∂ϕ−1
∂xi

ϕ(p)
= δi −δn+1,
i = 1,...,n.
This family is a basis of S0(I). The dual basis in F(I)/R is given as
ei + R,
i = 1,...,n.

42
2
Finite Information Geometry
We now express the covector ﬁeld [f ] in these coordinates:
f (μ) + R =
 n+1

i=1
f i(p)ei

+ R
=
n

i=1

f i(μ) −f n+1(μ)

(ei + R).
The covector ﬁeld f +R is closed, if the coefﬁcients f i −f n+1 satisfy the following
integrability condition:
∂(f i −f n+1)
∂(δj −δn+1) (μ) = ∂(f j −f n+1)
∂(δi −δn+1) (μ),
i,j = 1,...,n.
This is equivalent to
∂f i
∂δj + ∂f j
∂δn+1 + ∂f n+1
∂δi
= ∂f j
∂δi +
∂f i
∂δn+1 + ∂f n+1
∂δj .
Replacing n + 1 by k yields the integrability condition (2.38).
□
2.4 The m- and e-Connections
The tangent bundle T M+(I) and the cotangent bundle T ∗M+(I) are of product
structure. Given two points μ and ν in M+(I), this allows for the following natural
identiﬁcation of TμM+(I) with TνM+(I) and T ∗
μM+(I) with T ∗
ν M+(I):

Π(m)
μ,ν : TμM+(I) −→TνM+(I),
(μ,a) −→(ν,a),
(2.39)

Π(e)
μ,ν : T ∗
μM+(I) −→T ∗
ν M+(I),
(μ,f ) −→(ν,f ).
(2.40)
Note that these identiﬁcations of ﬁbers is not a consequence of the triviality of the
vector bundles only. In general, a trivial vector bundle has no distinguished trivial-
ization. However, in our case the bundles have a natural product structure.
With the bundle isomorphism φ (see diagram (2.17)) one can interpret 
Π(e)
μ,ν as a
parallel transport in T M+(I), given by

Π(e)
μ,ν : TμM+(I) −→TνM+(I),
(μ,a) −→

ν,
φ−1
ν
◦φμ

(a)

.
Here, one has
φ−1
ν
◦φμ

(a) = da
dμ ν =

i
νi
ai
μi
δi.

2.4
The m- and e-Connections
43
One immediately observes the following duality of the two parallel transports with
respect to the Fisher metric. With A = (μ,a) and B = (ν,b):
gν
 
Π(e)
μ,νA, 
Π(m)
μ,ν B

=

i
1
νi

νi
ai
μi

bi =

i
1
μi
aibi = gμ(A,B).
(2.41)
The correspondence of tangent spaces can be encoded more effectively in terms of
an afﬁne connection, which is a differential version of the parallel transport that
speciﬁes the directional derivative of a vector ﬁeld in the direction of another vector
ﬁeld. To be more precise, let A and B be two vector ﬁelds M+(I) →T M+(I).
There exist maps a,b : M+(I) →S(I) satisfying Bμ = (μ,bμ) and Aμ = (μ,aμ).
With a curve γ : (−ϵ,ϵ) →M+(I), γ (0) = μ and ˙γ (0) = Aμ the covariant deriva-
tive of B in the direction of A can be obtained from the parallel transports as follows
(see Eq. (B.33) in Appendix B):
∇(m,e)
A
B

μ := lim
t→0
1
t
 
Π(m,e)
γ (t),μ(Bγ (t)) −Bμ

∈TμM+(I).
(2.42)
The pair (2.42) of afﬁne connections ∇(m) and ∇(e) corresponds to two kinds of
straight line, the so-called geodesic, and exponential maps which specify a natural
way of locally identifying the tangent space in μ with a neighborhood of μ (in
M+(I)).
Proposition 2.4
(1) The afﬁne connections ∇(m) and ∇(e), deﬁned by (2.42), are given by
∇(m)
A B

μ =

μ, ∂b
∂aμ
(μ)

,
∇(e)
A B

μ =

μ, ∂b
∂aμ
(μ) −
daμ
dμ · dbμ
dμ

μ

.
(2) As corresponding (maximal) m- and e-geodesic with initial point μ ∈M+(I)
and initial velocity a ∈TμM+(I) we have
γ (m) :
,
t−,t+-
→M+(I),
t →μ + ta,
with
t−:= −min
μi
ai
: i ∈I,ai > 0

,
t+ := min
 μi
|ai| : i ∈I,ai < 0

(we use the convention min∅= ∞), and
γ (e) : R →M+(I),
t →exp

t da
dμ

μ.

44
2
Finite Information Geometry
(3) As corresponding exponential maps .
exp(m) and .
exp(e), we obtain
.
exp(m) : T →M+(I),
(μ,a) →μ + a,
(2.43)
with T := {(μ,ν −μ) ∈T M+(I) : μ,ν ∈M+(I)}, and
.
exp(e) : T M+(I) →M+(I),
(μ,a) →exp
 da
dμ

μ.
(2.44)
Proof The m-connection:
∇(m)
A B

μ = lim
t→0
1
t
 
Π(m)
γ (t),μ(Bγ (t)) −Bμ

=

μ, lim
t→0
1
t (bγ (t) −bμ)

=

μ, ∂b
∂aμ
(μ)

.
In order to get the geodesic of the m-connection we consider the corresponding
equation:
¨γ = 0
with γ (0) = μ, ˙γ (0) = a.
Its solution is given by
t →μ + t a
which is deﬁned for the maximal time interval ]t−,t+[. Setting t = 1 gives us the
corresponding exponential map .
exp(m).
The e-connection: Now we consider the covariant derivative induced by the ex-
ponential parallel transport 
Π(e):
∇(e)
A B

μ := lim
t→0
1
t
 
Π(e)
γ (t),μ(Bγ (t)) −Bμ

=

μ, lim
t→0
1
t

i

μi
bγ (t),i
γi(t) −bμ,i

δi

=

μ,

i
d
dt

μi
bγ (t),i
γi(t)

t=0
δi

=

μ,

i
 ∂bi
∂aμ
(μ) −1
μi
aμ,ibμ,i

δi

.
The equation for the corresponding e-geodesic is given as
¨γ −˙γ 2
γ = 0
with γ (0) = μ, ˙γ (0) = a.
(2.45)

2.4
The m- and e-Connections
45
One can easily verify that the solution of (2.45) is given by the following curve γ :
t →

i
μi et ai
μi δi.
(2.46)
Setting t = 1 in (2.46), we obtain the corresponding exponential map .
exp(e) which
is deﬁned on the whole tangent bundle T M+(I):
(μ,a) →exp
 da
dμ

μ =

i
μi e
ai
μi δi.
□
In what follows, we restrict the m- and e-connections to the simplex P+(I).
First consider the m-connection. Given a point μ ∈P+(I) and two vector ﬁelds
A,B : P+(I) →T P+(I), we observe that the covariant derivative in μ is already
in the tangent space of P+(I) in μ, that is, ∇(m)
A B|μ ∈TμP+(I). This allows us to
deﬁne the m-connection on P+(I) simply by
∇(m)
A B

μ := ∇(m)
A B

μ.
(2.47)
The situation is different for the e-connection. There, we have in general ∇(e)
A B|μ /∈
TμP+(I). In order to obtain an e-connection on the simplex, we have to project
∇(e)
A B|μ onto TμP+(I) with respect to the Fisher metric gμ in μ, which leads to the
following covariant derivative on the simplex (see (2.14)):
∇(e)
A B

μ =

μ, ∂b
∂aμ
(μ) −
daμ
dμ · dbμ
dμ

μ + gμ(Aμ,Bμ)μ

.
(2.48)
Proposition 2.5
Consider the afﬁne connections ∇(m) and ∇(e) deﬁned by (2.47)
and (2.48), respectively. Then the following holds:
(1) The corresponding (maximal) m- and e-geodesic with initial point μ ∈P+(I)
and initial velocity a ∈TμP+(I) are given by
γ (m) :
,
t−,t+-
→P+(I),
t →μ + ta,
with
t−:= −min
μi
ai
: i ∈I,ai > 0

,
t+ := min
 μi
|ai| : i ∈I,ai < 0

,
and
γ (e) : R →P+(I),
t →
exp(t da
dμ)
μ(exp(t da
dμ))
μ.

46
2
Finite Information Geometry
(2) As corresponding exponential maps exp(m) and exp(e) we have
exp(m) : T →P+(I),
(μ,a) →μ + a,
with T := {(μ,ν −μ) ∈T P+(I) : μ,ν ∈P+(I)}, and
exp(e) : T P+(I) →P+(I),
(μ,a) →
exp( da
dμ)
μ(exp( da
dμ))
μ.
Proof Clearly, we only have to prove the statements for the e-connection. From the
deﬁnition (2.48), we obtain the equation for the corresponding e-geodesic:
¨γ −˙γ 2
γ + γ

i
˙γ 2
i
γi
= 0
with γ (0) = μ, ˙γ (0) = a.
(2.49)
The solution of (2.49) is given by the following curve γ :
t →

i
μi et ai
μi
	
j μj e
t
aj
μj
δi.
(2.50)
We now verify this: Obviously, we have γ (0) = μ. Furthermore, a straightforward
calculation gives us
˙γi(t) = γi(t)
 ai
μi
−

j
γj(t) aj
μj

and
¨γi(t) = ˙γi(t)
 ai
μi
−

j
γj(t) aj
μj

−γi(t)

j
˙γj(t) aj
μj
= γi(t)
 ai
μi
−

j
γj(t) aj
μj
2
−γi(t)

j
˙γj(t) aj
μj
.
This implies ˙γ (0) = a and
¨γi(t) −˙γi(t)2
γi
−γi(t)

j

¨γj(t) −˙γj(t)2
γj(t)

= −γi(t)

j
˙γj(t) aj
μj
+ γi(t)

j
γj(t)

k
˙γk(t) ak
μk
= 0,

2.5
The Amari–Chentsov Tensor and the α-Connections
47
Fig. 2.4 m- and e-geodesic
in P+({1,2,3}) with initial
point μ and velocity a
which proves that all conditions (2.49) are satisﬁed. Setting t = 1 in (2.50), we
obtain the corresponding exponential map exp(e) which is deﬁned on the whole
tangent bundle T P+(I):
(μ,a) →
exp( da
dμ)
μ(exp( da
dμ))
μ =

i
μie
ai
μi
	
j μje
aj
μj
δi.
□
As an illustration of the m- and e-geodesic of Proposition 2.5(1), see Fig. 2.4.
2.5 The Amari–Chentsov Tensor and the α-Connections
2.5.1 The Amari–Chentsov Tensor
We consider a covariant 3-tensor using the afﬁne connections ∇(m) and ∇(e): For
three vector ﬁelds A : μ →Aμ = (μ,aμ), B : μ →Bμ = (μ,bμ), and C : μ →
Cμ = (μ,cμ) on M+(I), we deﬁne
Tμ(Aμ,Bμ,Cμ) := gμ
∇(m)
A B

μ −∇(e)
A B

μ,Cμ

=

i∈I
μi
aμ,i
μi
bμ,i
μi
cμ,i
μi
.
(2.51)
We refer to this tensor as the Amari–Chentsov tensor. Note that for vector ﬁelds
A,B,C on P+(I) and μ ∈P+(I) we have
Tμ(Aμ,Bμ,Cμ) = gμ

∇(m)
A B

μ −∇(e)
A B

μ,Cμ

.
We have seen that the Fisher metric g on P+(I) is uniquely characterized in terms
of invariance (see Theorem 2.1). Following Chentsov, the same uniqueness property
also holds for the tensor T on P+(I), which is the content of the following theorem.

48
2
Finite Information Geometry
Theorem 2.2 We assign to each non-empty and ﬁnite set I a (non-trivial) covariant
3-tensor SI on P+(I). If for each congruent Markov kernel K : I →P(I ′) we have
invariance in the sense that
SI
μ(A,B,C) = SI ′
K∗(μ)

dμK∗(A),dμK∗(B),dμK∗(C)

then there is a constant α > 0 such that SI = α TI for all I, where TI denotes the
Amari–Chentsov tensor on P+(I).2
One can prove this theorem by following the same steps as in the proof of The-
orem 2.1. Alternatively, it immediately follows from the more general result stated
in Theorem 2.3.
By analogy, we can extend the deﬁnition (2.51) to a covariant n-tensor for all
n ≥1:
τ n
μ

V (1),V (2),...,V (n)
:=

i∈I
μi
v(1)
μ,i
μi
v(2)
μ,i
μi
···
v(n)
μ,i
μi
=

i∈I
1
μin−1 v(1)
μ,i v(2)
μ,i ··· v(n)
μ,i.
(2.52)
Obviously, we have
τ 2 = g,
and
τ 3 = T.
It is easy to extend the representation (2.26) of the Fisher metric g to the covariant n-
tensor τ n. Given a differentiable manifold M and an embedding p : M →M+(I),
one obtains as pullback of τ n the following covariant n-tensor, deﬁned on M:
τ n
ξ (V1,...,Vn) :=

i
pi(ξ)∂logpi
∂V1
(ξ)··· ∂logpi
∂Vn
(ξ).
As suggested by (2.52), the tensor τ n is closely related to the following multi-
linear form:
Ln
I : F(I) × ··· × F(I)
(
)*
+
n times
→R,
(f1,...,fn) →Ln
I(f1,...,fn) :=

i
f1i ···fni.
(2.53)
In order to see this, consider the map
π1/n : M+(I) →F(I),
μ =

i
μi δi →π1/n(μ) := n

i
μi
1
n ei.
2Note that we use the abbreviation T if corresponding statements are clear without reference to the
set I, which is usually the case throughout this book.

2.5
The Amari–Chentsov Tensor and the α-Connections
49
This implies
Ln
I
∂π1/n
∂v(1) (μ),..., ∂π1/n
∂v(n) (μ)

=

i

μ
−n−1
n
i
v(1)
i

···

μ
−n−1
n
i
v(n)
i

= τ n
μ

V (1),...,V (n)
.
This proves that the tensor τ n is nothing but the π1/n-pullback of the multi-linear
form Ln
I . In this sense, it is a very natural tensor. Furthermore, for n = 2 and n = 3,
we have seen that the restrictions of g and T to the simplex P+(I) are naturally
characterized in terms of their invariance with respect to congruent Markov em-
beddings (see Theorem 2.1 and Theorem 2.2). This raises the question whether the
tensors τ n on M+(I), or their restrictions to P+(I), are also characterized by in-
variance properties. It is easy to see that for all n, τ n is indeed invariant. However,
τ n are not the only invariant tensors. In fact, Chentsov’s results treat the only non-
trivial uniqueness cases. Already for n = 2, Campbell has shown that the metric g
is not the only one that is invariant if we consider tensors on M+(I) rather than
on P+(I) [57]. Furthermore, for higher n, there are other possible invariant tensors,
even when restricting to P+(I). For instance, for n = 4 we can consider the tensors
τ {1,2},{3,4}(V1,V2,V3,V4) := τ 2(V1,V2)τ 2(V3,V4) = g(V1,V2)g(V3,V4),
τ {1,3},{2,4}(V1,V2,V3,V4) := τ 2(V1,V3)τ 2(V2,V4) = g(V1,V3)g(V2,V4),
τ {1,4},{2,3}(V1,V2,V3,V4) := τ 2(V1,V4)τ 2(V2,V3) = g(V1,V4)g(V2,V3).
It is obvious that all of these invariant tensors are mutually different and also differ-
ent from τ 4. Similarly, for n = 5 we have, for example,
τ {1,2},{3,4,5}(V1,V2,V3,V4,V5) := τ 2(V1,V2)τ 3(V3,V4,V5)
= g(V1,V2)T(V3,V4,V5),
τ {1,4},{2,3,5}(V1,V2,V3,V4,V5) := τ 2(V1,V4)τ 3(V2,V3,V5)
= g(V1,V4)T(V2,V3,V5).
From these examples it becomes evident that for each partition
P =

i1
1,...,in1
1

,...,

i1
l ,...,inl
l

of the set {1,...,n} with n = n1 + ··· + nl one can deﬁne an invariant n-tensor
τ P(V1,...,Vn) in a corresponding fashion, see Deﬁnition 2.6 below. Our general-
ization of Chentsov’s uniqueness results, Theorem 2.3, will state that any invariant
n-tensor will be a linear combination of these, i.e., the dimension of the space of
invariant n-tensors depends on the number of partitions of the set {1,...,n}. In
fact, this result will even hold if we consider arbitrary (inﬁnite) measure spaces (see
Theorem 5.6).

50
2
Finite Information Geometry
2.5.2 The α-Connections
The Amari–Chentsov tensor T is closely related to a family of afﬁne connections,
deﬁned as a convex combination of the m- and the e-connections. As in our previous
derivations we ﬁrst consider the afﬁne connections on M+(I) and then restrict them
to P+(I). Given α ∈[−1,1], we deﬁne the following convex combination, the α-
connection:
∇(α) := 1 −α
2
∇(m) + 1 + α
2
∇(e) = ∇(m) + 1 + α
2
∇(e) −∇(m)
.
(2.54)
Obviously, for vector ﬁelds A, B, and C we have
g
∇(α)
A B,C

= g
∇(m)
A B,C

−1 + α
2
T(A,B,C).
More explicitly, we have
∇(α)
A B

μ =

μ,

i
 ∂bi
∂aμ
(μ) −1 + α
2
aμ,ibμ,i
μi

δi

.
(2.55)
This allows us to determine the geodesic and the exponential map of the α-
connection. The differential equation for the α-geodesic with initial point μ and
initial velocity a follows from (2.55):
¨γ −1 + α
2
˙γ 2
γ = 0,
γ (0) = μ, ˙γ (0) = a.
(2.56)
It is easy to verify that the following curve satisﬁes this equation:
γ (α)(t) =

μ
1−α
2 + t 1 −α
2
μ−1+α
2 a

2
1−α
.
(2.57)
By setting t = 1, we can deﬁne the corresponding exponential map:
.
exp(α) : (μ,a) →

μ
1−α
2 + 1 −α
2
μ−1+α
2 a

2
1−α
.
(2.58)
Finally, the α-geodesic with initial point μ and endpoint ν has the following more
symmetric structure:
γ (α)(t) =

(1 −t)μ
1−α
2 + t ν
1−α
2 
2
1−α .
(2.59)
Now we come to the α-connection ∇(α) on P+(I) by projection of ∇(α) (see (2.14)).
For μ ∈P+(I) and two vector ﬁelds A and B that are tangential to P+(I), we obtain

2.5
The Amari–Chentsov Tensor and the α-Connections
51
as projection
∇(α)
A B

μ =

μ,

i
 ∂bi
∂aμ
(μ) −1 + α
2
aμ,ibμ,i
μi
−μi

j
aμ,jbμ,j
μj

δi

.
(2.60)
This implies the following corresponding geodesic equation:
¨γ −1 + α
2
 ˙γ 2
γ −γ

j
˙γ 2
j
γj

= 0,
γ (0) = μ, ˙γ (0) = a.
(2.61)
It is reasonable to make a solution ansatz by normalization of the unconstrained
geodesic (2.57) and (2.59). However, it turns out that, in order to solve the geodesic
Eq. (2.61), both normalized curves have to be reparametrized. More precisely, it has
been shown in [187] (Theorems 14.1 and 15.1) that, with appropriate reparametriza-
tions τμ,a and τμ,ν, we have the following forms of the α-geodesic in the simplex
P+(I):
γ (α)(t) =

i∈I
μi(1 + τμ,a(t) 1−α
2
ai
μi )
2
1−α
	
j∈I μj(1 + τμ,a(t) 1−α
2
aj
μj )
2
1−α
δi
(2.62)
and
γ (α)(t) =

i∈I
(μ
1−α
2
i
+ τμ,ν(t)(ν
1−α
2
i
−μ
1−α
2
i
))
2
1−α
	
j∈I(μ
1−α
2
j
+ τμ,ν(t)(ν
1−α
2
i
−μ
1−α
2
i
))
2
1−α
δi.
(2.63)
An explicit expression for the reparametrizations τμ,a and τμ,ν is unknown. In gen-
eral, we have the following implications:
γ (α)(0) = μ,
dγ (α)
dt
(0) = ˙τμ,a(0)a = a
⇒
τμ,a(0) = 0,
˙τμ,a(0) = 1,
and
γ (α)(0) = μ,
γ (α)
μ,ν(1) = ν
⇒
τμ,ν(0) = 0,
τμ,ν(1) = 1.
As the two expressions (2.62) and (2.63) of the geodesic γ (α) yield the same velocity
a at t = 0, we obtain, with 	
i∈I ai = 0,
a =
1
τμ,a(1)
2
1 −α

i∈I
μi

( νi
μi )
1−α
2
	n
j=1 μj( νj
μj )
1−α
2
−1

δi
(2.64)
and
a = ˙τμ,ν(0)
2
1 −α

i∈I
μi
 νi
μi
 1−α
2
−

j∈I
μj
 νj
μj
 1−α
2 
δi.
(2.65)

52
2
Finite Information Geometry
A comparison of (2.64) and (2.65) yields
˙τμ,ν(0)

j∈I
μj
 νj
μj
 1−α
2
=
1
τμ,a(1).
(2.66)
2.6 Congruent Families of Tensors
In Theorem 2.1 we showed that the Fisher metric g on P+(I) is characterized by
the property that it is invariant under congruent Markov kernels. In this section,
we shall generalize this result and give a complete description of all families of
covariant n-tensors on P+(I) or, more general, on M+(I) with this property.
Before doing this, we need to introduce some more notation. Recall that for a
non-empty ﬁnite set I we deﬁned S(I) as the vector space of signed measures on I
on page 26, that is,
S(I) =

i∈I
aiδi : ai ∈R

,
where δi is the Dirac measure supported at i ∈I. On this space, we deﬁne the norm
∥μ∥1 := |μ|(I) =

i∈I
|ai|,
where μ =

i∈I
aiδi.
(2.67)
Remark 2.1 The norm deﬁned in (2.67) is the norm of total variation, which we
shall deﬁne for general measure spaces in (3.1).
Moreover, recall the subsets P(I)⊆M(I)⊆S(I) introduced in (2.1), where P(I)
denotes the set of probability measures and M(I) the set of ﬁnite measures on I,
respectively. By (2.67), we can also write
P(I) =

m ∈M(I) : ∥m∥1 = 1

= M(I) ∩S1(I).
By (2.1), P+(I)⊆S1(I) and M+(I)⊆S(I) are open subsets, where S1(I)⊆S(I)
is an afﬁne subspace with underlying vector spaces S0(I). Thus, the tangent bundles
of these spaces can be naturally given as in (2.9).
For each μ ∈M+(I), there is a decomposition of the tangent space
TμM+(I) = TμP+(I) ⊕Rμ = S0(I) ⊕Rμ.
(2.68)
Indeed, TμP+(I) = S0(I) has codimension one in TμM+(I) = S(I), and Rμ ∩
S0(I) = 0.
We also deﬁne the projection
πI : M+(I) −→P+(I),
πI(μ) =
1
∥μ∥1
μ,

2.6
Congruent Families of Tensors
53
which rescales an arbitrary ﬁnite measure on I to become a probability measure.
Obviously, πI(μ) = μ if and only if μ ∈P+(I). Clearly, πI is differentiable. To
calculate its differential, we let V ∈TμM+(I) = S, and use
dμπI(V ) = d
dt

t=0
πI(μ + tV ) = d
dt

t=0
1
∥μ + tV ∥1
(μ + tV ).
If V ∈S0(I)⊆S, then ∥μ + tV ∥1 = ∥μ∥1 by the deﬁnition of S0(I). On the other
hand, if V = c0μ, then πI(μ + tV ) = πI(1 + tc0)μ = πI(μ) is constant, whence
for the differential we obtain
dμπI(V ) =

1
∥μ∥1 V
for V ∈TμP+(I) = S0(I),
0
for V ∈Rμ.
(2.69)
Deﬁnition 2.3 (Covariant n-tensors on M+(I) and P+(I))
(1) A covariant n-tensor on P+(I) is a continuous map
Θn
I : P+(I) ×
n×S0(I) −→R,
(μ;V1,...,Vn) −→

Θn
I

μ(V1,...,Vn)
such that (Θn
I )μ is n-linear on×
n S0(I) for ﬁxed μ ∈P+(I).
(2) A covariant n-tensor on M+(I) is a continuous map
˜Θn
I : M+(I) ×
n×S −→R,
(μ;V1,...,Vn) −→
 ˜Θn
I

μ(V1,...,Vn)
such that ( ˜Θn
I )μ is n-linear on×
n S for ﬁxed μ ∈M+(I).
(3) Given a covariant n-tensor Θn
I on P+(I), we deﬁne the extension of Θn
I to
M+(I) to be the covariant n-tensor
 ˜Θn
I

μ(V1,...,Vn) :=

Θn
I

πI (μ)

dμπI(V1),...,dμπI(Vn)

.
(4) Given a covariant n-tensor ˜Θn
I on M+(I), we deﬁne its restriction to P+(I) to
be the tensor
 ˜Θn
I

μ(V1,...,Vn) :=
 ˜Θn
I

μ(V1,...,Vn).
Remark 2.2 By convention, a covariant 0-tensor on P+(I) and M+(I) is simply a
continuous function Θ0
I : P+(I) →R and ˜Θ0
I : M+(I) →R, respectively.
The extension of Θn
I is merely the pull-back of Θn
I under the map πI : M+(I) →
P+(I); the restriction of ˜Θn
I is the pull-back of the inclusion P+(I) →M+(I) as
deﬁned in (2.25).
In general, in order to describe a covariant n-tensor ˜Θn
I on M+(I), we deﬁne for
a multiindex ⃗i := (i1,...,in) ∈I n
θ⃗i
I;μ :=
 ˜Θn
I

μ

δi1,...,δin
=:
 ˜Θn
I

μ

δ⃗i
.
(2.70)

54
2
Finite Information Geometry
Clearly, these functions are continuous in μ ∈M+(I), and they uniquely deter-
mine ˜Θn
I , since for arbitrary vectors Vk = 	
i∈I vk;iδi ∈S the multilinearity implies
 ˜Θn
I

μ(V1,...,Vn) =

⃗i=(i1,...,in)∈I n
θ⃗i
I;μv1,i1 ···vn,in.
(2.71)
Let K : I →P(I ′) be a Markov kernel between the ﬁnite sets I and I ′, as de-
ﬁned in (2.30). Such a map induces a corresponding map between probability dis-
tributions as deﬁned in (2.31), and as was mentioned there, this formula also yields
a linear map
K∗: S(I) −→S

I ′
,
μ =

i∈I
μiδi −→

i∈I
μiKi,
where
Ki := K(i) =

i′∈I ′
Ki
i′δi′.
Clearly, K∗is a linear map between S(I) and S(I ′), and Ki
i′ ≥0, implies K∗μ ∈
M(I ′) for all μ ∈M(I). Moreover, 	
i′∈I ′ Ki
i′ = 1 implies that for all μ ∈M(I),
∥K∗μ∥1 =


i∈I,i′∈I ′
μiKi
i′δi′
1
=

i∈I,i′∈I ′
μiKi
i′ =

i∈I
μi = ∥μ∥1.
That is,
∥K∗μ∥1 = ∥μ∥1
for all μ ∈M(I).
(2.72)
This also implies that the image of P(I) under K∗is contained in P(I ′). In partic-
ular, it follows that for μ ∈M(I),
K∗(πIμ) = K∗

1
∥μ∥1
μ

=
1
∥K∗μ∥1
K∗μ = πI ′(K∗μ),
i.e.,
K∗(πIμ) = πI ′(K∗μ)
for all μ ∈M(I).
(2.73)
Deﬁnition 2.4 (Tensors invariant under congruent embeddings) A congruent fam-
ily of covariant n-tensors is a collection { ˜Θn
I : I ﬁnite}, where ˜Θn
I is a covariant n-
tensor on M+(I), which is invariant under congruent Markov kernels in the sense
that
K∗
∗˜Θn
I ′ = ˜Θn
I
(2.74)
for any congruent Markov kernel K : I →P(I ′) with the deﬁnition of the pull-back
in (2.25).

2.6
Congruent Families of Tensors
55
A restricted congruent family of covariant n-tensors is a collection {Θn
I :
I ﬁnite}, where Θn
I is a covariant n-tensor on P+(I), which is invariant under con-
gruent Markov kernels in the sense that (2.74) holds when replacing ˜Θn
I and ˜Θn
I ′ by
Θn
I and Θn
I ′, respectively.
Proposition 2.6 There is a correspondence between congruent families of covari-
ant n-tensors and restricted congruent families of covariant n-tensors in the follow-
ing sense:
(1) Let { ˜Θn
I : I ﬁnite} be a congruent family of covariant n-tensors, and let Θn
I be
the restriction of ˜Θn
I to P+(I).
Then {Θn
I : I ﬁnite} is a restricted congruent family of covariant n-tensors.
Moreover, any restricted congruent family of covariant n-tensors can be de-
scribed in this way.
(2) Let {Θn
I : I ﬁnite} be a restricted congruent family of covariant n-tensors, and
let ˜Θn
I be the extension of Θn
I to M+(I).
Then { ˜Θn
I : I ﬁnite} is a congruent family of covariant n-tensors.
Proof This follows from unwinding the deﬁnitions. Namely, if { ˜Θn
I : I ﬁnite} is
a congruent family of covariant n-tensors, then the restriction is given as Θn
I :=
˜Θn
I |P(I). Now if K : I →P(I ′) is a congruent Markov kernel, then because of
(2.72), K∗maps P(I) to P(I ′), whence if (2.74) holds, it also holds for the restric-
tion of both sides to P(I) and P(I ′), respectively, showing that {Θn
I : I ﬁnite} is a
restricted congruent family of covariant n-tensors.
For the second assertion, let {Θn
I : I ﬁnite} be a restricted congruent family of
covariant n-tensors. Then the extension of Θn
I is given as ˜Θn
I := π∗
I Θn
I , whence for
a congruent Markov kernel K : I →P(I ′) we get from (2.73)
K∗
∗˜Θn
I ′ = K∗
∗π∗
I ′Θn
I ′ = (πI ′K∗)∗Θn
I ′ = (K∗πI)∗Θn
I ′ = π∗
I K∗
∗Θn
I ′ = π∗
I Θn
I = ˜Θn
I ,
so that (2.74) holds.
□
In the following, we shall therefore mainly deal with the description of congruent
families of covariant n-tensors, since by virtue of Proposition 2.6 this immediately
yields a description of all restricted congruent families as well.
Example 2.2 (Congruent families of covariant 0-tensors) Let { ˜Θ0
I : M+(I) →R :
I ﬁnite} be a congruent family of covariant 0-tensors, i.e., of continuous functions
˜Θ0
I : M+(I) →R (cf. Remark 2.2). Let μ ∈M+(I) and ρ := πI(μ) ∈P+(I) be
the normalization of μ, so that μ = ∥μ∥1ρ. Deﬁne the congruent embedding deter-
mined by
K : S

{0}

−→S(I),
δ0 −→ρ.
Then by the congruence condition,
˜Θ0
I (μ) = ˜Θ0
I

∥μ∥1ρ

= ˜Θ0
I

K∗

∥μ∥1δ0
= ˜Θ0
{0}

∥μ∥1δ0
=: a

∥μ∥1

.

56
2
Finite Information Geometry
That is, a congruent family of covariant 0-tensors is given as
˜Θ0
I (μ) = a

∥μ∥1

(2.75)
for some continuous function a : (0,∞) →R. Conversely, every family given as in
(2.75) is congruent, since Markov morphisms preserve the total mass by (2.72).
In particular, a restricted congruent family of covariant 0-tensors is given by a
constant.
Deﬁnition 2.5
(1) Let ˜Θn
I be a covariant n-tensor on M+(I) and let σ be a permutation of
{1,...,n}. Then the permutation of ˜Θn
I by σ is deﬁned by
 ˜Θn
I
σ(V1,...,Vn) := Θn
I (Vσ1,...,Vσn).
(2) Let ˜Θn
I and ˜Ψ m
I
be covariant n- and m-tensors on M+(I), respectively. Then
the tensor product of ˜Θn
I and ˜Ψ m
I
is the covariant (n + m)-tensor on M+(I)
deﬁned by
 ˜Θn
I ⊗˜Ψ m
I

(V1,...,Vn+m) := ˜Θn
I (V1,...,Vn) · ˜Ψ m
I (Vn+1,...,Vn+m).
The permutation by σ and the tensor product of covariant tensors on P+(I) is de-
ﬁned analogously.
Observe that the tensor product includes multiplication by a continuous function,
which is regarded as a covariant 0-tensor.
By the deﬁnition of the pull-back K∗
∗in (2.25) it follows immediately that
K∗
∗

c1 ˜Θn
I + c2 ˜Ψ n
I

= c1K∗
∗
 ˜Θn
I

+ c2K∗
∗
 ˜Ψ n
I

,
K∗
∗
 ˜Θn
I
σ
=

K∗
∗
 ˜Θn
I
σ,
K∗
∗
 ˜Θn
I ⊗˜Ψ m
I

= K∗
∗
 ˜Θn
I

⊗K∗
∗
 ˜Ψ m
I

.
This implies the following statement.
Proposition 2.7
(1) Let { ˜Θn
I : I ﬁnite} and { ˜Ψ n
I : I ﬁnite} be two congruent families of covariant
n-tensors. Then any linear combination {c1 ˜Θn
I + c2 ˜Ψ n
I : I ﬁnite} is also a con-
gruent family of covariant n-tensors.
(2) Let { ˜Θn
I : I ﬁnite} be a congruent family of covariant n-tensors. Then for any
permutation σ of {1,...,n}, {( ˜Θn
I )σ : I ﬁnite} is a congruent family of covariant
n-tensors.
(3) Let { ˜Θn
I : I ﬁnite} and { ˜Ψ m
I : I ﬁnite} be two congruent families of covariant
n- and m-tensors, respectively. Then the tensor product { ˜Θn
I ⊗˜Ψ m
I : I ﬁnite} is
also a congruent family of covariant (n + m)-tensors.

2.6
Congruent Families of Tensors
57
The analogous statements hold for restricted congruent family of covariant n-
tensors.
The following introduces an important class of congruent families of covariant
n-tensors.
Proposition 2.8 For a ﬁnite set I deﬁne the canonical n-tensor τ n
I as

τ n
I

μ(V1,...,Vn) :=

i∈I
1
mn−1
i
v1;i ···vn;i,
(2.76)
where Vk = 	
i∈I vk;iδi ∈S(I) and μ = 	
i∈I miδi ∈M+(I). Then {τ n
I : I ﬁnite}
is a congruent family of covariant n-tensors.
The component functions of this tensor from (2.70) are therefore given as
θi1,...,in
I;μ
=

1
mn−1
i
if i1 = ··· = in =: i,
0
otherwise.
(2.77)
This is well deﬁned since mi > 0 for all i as μ ∈M+(I). Observe that the restriction
of τ n
I to P+(I) coincides with the deﬁnition in (2.52), so that, in particular, the
restriction of τ 1
I to P+(I) vanishes, while τ 2
I and τ 3
I are the Fisher metric and the
Amari–Chentsov tensor on P+(I), respectively.
Proof Let K : I →P(I ′) be a congruent Markov kernel with the partition (Ai)i∈I
of I ′ as deﬁned in (2.32). That is, K(i) := Ki
i′δi′ with Ki
i′ = 0 if i′ /∈Ai. If μ =
	
i∈I miδi, then
μ′ := K∗μ =

i∈I,i′∈Ai
miKi
i′δi′ =:

i′∈I ′
m′
i′δi′.
Thus,
m′
i′ = miKi
i′
for the (unique) i ∈I with i′ ∈Ai.
(2.78)
Then with the notation from before

τ n
I ′

μ′

K∗δi1,...,K∗δin
=

τ n
I ′

μ′
 
i′
1∈Ai1
Ki1
i′
1 δi′
1,...,

i′n∈Ain
Kin
i′n δi′
n

=

i′
k∈Aik
Ki1
i′
1 ···Kin
i′n θ
i′
1,...,i′
n
I ′;μ′
.
By (2.77), the only summands with θ
i′
1,...,i′
n
I ′;μ′
̸= 0 are those where i′
1 = ··· = i′
n =: i′.
If we let i ∈I be the index with i′ ∈Ai, then, as K is a congruent Markov morphism,
we have Kik
i′ = 0 unless ik = i.

58
2
Finite Information Geometry
That is, Ki1
i′
1 ···Kin
i′n θ
i′
1,...,i′
n
I ′;μ′
̸= 0 only if i′
1 = ··· = i′
n =: i′ and i1 = ··· = in =: i
with i′ ∈Ai. In particular, if not all of i1,...,in are equal (2.74) holds for Vk = δik,
since in this case,

τ n
I ′

μ′

K∗δi1,...,K∗δin
= 0 =

τ n
I

μ

δi1,...,δin
.
On the other hand, if i1 = ··· = in =: i, then the above sum reads

τ n
I ′

μ′

K∗δi,...,K∗δi
=

i′∈Ai
Ki
i′ ···Ki
i′θi′,...,i′
I ′;μ′
(2.77)
=

i′∈Ai

Ki
i′
n
1
(m′
i′)n−1
(2.78)
=

i′∈Ai

Ki
i′
n
1
(miKi
i′)n−1
=
1
mn−1
i

i′∈Ai
Ki
i′ =
1
mn−1
i

i′∈I ′
Ki
i′
( )* +
=1
=
1
mn−1
i
=

τ n
I

μ

δi,...,δi
,
so that (2.74) holds for V1 = ··· = Vn = δi as well. Thus, the n-linearity of the
tensors shows that (2.74) always holds, which shows the claim.
□
By Propositions 2.7 and 2.8, we can therefore construct further congruent fami-
lies which we shall now describe in more detail.
For n ∈N, we denote by Part(n) the collection of partitions P = {P1,...,Pr}
of {1,...,n}, that is, /
k Pk = {1,...,n}, and these sets are pairwise disjoint. We
denote the number r of sets in the partition by |P|.
Given a partition P = {P1,...,Pr} ∈Part(n), we associate to it a bijective map
πP :
%
i∈{1,...,r}

{i} × {1,...,ni}

−→{1,...,n},
(2.79)
where ni := |Pi|, such that πP({i} × {1,...,ni}) = Pi. This map is well deﬁned, up
to permutation of the elements in Pi.
Part(n) is partially ordered by the relation P ≤P′ if P is a subdivision of P′.
This ordering has the partition {{1},...,{n}} into singleton sets as its minimum and
{{1,...,n}} as its maximum.
Deﬁnition 2.6 (Canonical tensor of a partition)
Let P ∈Part(n) be a partition, and
let πP be the bijective map from (2.79). For each ﬁnite set I, the canonical n-tensor

2.6
Congruent Families of Tensors
59
of P is the covariant n-tensor deﬁned by

τ P
I

μ(V1,...,Vn) :=
r0
i=1

τ ni
I

μ(VπP(i,1),...,VπP(i,ni))
(2.80)
with the canonical tensor τ ni
I from (2.76).
Observe that this deﬁnition is independent of the choice of the bijection πP, since
τ ki
I is symmetric.
Example 2.3
(1) If P = {{1,...,n}} is the trivial partition, then
τ P
I = τ n
I .
(2) If P = {{1},...,{n}} is the partition into singletons, then
τ P
I (V1,...,Vn) = τ 1
I (V1)···τ 1
I (Vn).
(3) To give a concrete example, let n = 5 and P = {{1,3},{2,5},{4}}. Then
τ P
I (V1,...,V5) = τ 2
I (V1,V3) · τ 2
I (V2,V5) · τ 1
I (V4).
Observe that the restriction of τ P to P+(I) vanishes if P contains a singleton set,
since τ 1
I vanishes on P+(I) by (2.52). Thus, the restriction of the last two examples
to P+(I) vanishes.
Proposition 2.9
(1) Every family of covariant n-tensors given by
 ˜Θn
I

μ =

P∈Part(n)
aP

∥μ∥1

τ P
I

μ
(2.81)
with continuous functions aP : (0,∞) →R is congruent. Likewise, every family
of restricted covariant n-tensors given by
Θn
I =

P∈Part(n)
|Pi|≥2
cPτ P
I
(2.82)
with cP ∈R is congruent.
(2) The class of congruent families of (restricted) covariant tensors in (2.81) and
(2.82), respectively, is the smallest such class which is closed under taking lin-
ear combinations, permutations, and tensor products as described in Proposi-
tion 2.7, and which contains the canonical n-tensors {τ n
I } and the covariant
0-tensors from (2.75).

60
2
Finite Information Geometry
(3) For any congruent family of this class, the functions aP and the constants cP in
(2.81) and (2.82), respectively, are uniquely determined.
Proof Evidently, the class of families of (restricted) covariant tensors in (2.81) and
(2.82), respectively, is closed under taking linear combinations and permutations.
To see that it is closed under taking tensor products, note that
τ P
I ⊗τ P′
I = τ P∪P′
I
,
where P ∪P′ ∈Part(n + m) is the partition of {1,...,n + m} obtained by regarding
P ∈Part(n) and P′ ∈Part(m) as partitions of {1,...,n} and {n + 1,...,n + m},
respectively.
Moreover, if P = {P1,...,Pr} ∈Part(n), we may—after applying a permutation
of {1,...,n}—assume that
P1 = {1,...,k1},P2 = {k1 + 1,...,k1 + k2},...,Pr = {n −kr + 1,...,n},
with ki = |Pi|, and in this case, (2.80) and Deﬁnition 2.5 imply that
τ P
I =

τ k1
I

⊗

τ k2
I

⊗··· ⊗

τ kr
I

.
Therefore, all (restricted) families given in (2.81) and (2.82), respectively, are con-
gruent by Proposition 2.7, and any class containing the canonical n-tensors and
congruent 0-tensors which is closed under linear combinations, permutations and
tensor products must contain all families of the form (2.81) and (2.82), respectively.
This proves the ﬁrst two statements.
In order to prove the third part, suppose that

P∈Part(n)
aP

∥μ∥1

τ P
I

μ = 0
(2.83)
for all ﬁnite I and μ ∈M+(I), but there is a partition P0 with aP0 ̸≡0. In fact,
we pick P0 to be minimal with this property, and choose a multiindex ⃗i ∈I n with
P(⃗i) = P0. Then
0 =

P∈Part(n)
aP

∥μ∥1

τ P
I

μ

δ⃗i
=

P≤P0
aP

∥μ∥1

τ P
I

μ

δ⃗i
= aP0

∥μ∥1

τ P0
I

μ

δ⃗i
.
The ﬁrst equation follows since (τ P
I )μ(δ⃗i) ̸= 0 only if P ≤P(⃗i) = P0 by Lemma 2.1,
whereas the second follows since aP ≡0 for P < P0 by the minimality assumption
on P0.
But (τ P0
I )μ(δ⃗i) ̸= 0 again by Lemma 2.1, since P(⃗i) = P0, so that aP0(∥μ∥1) = 0
for all μ, contradicting aP0 ̸≡0.

2.6
Congruent Families of Tensors
61
Thus, (2.83) occurs only if aP ≡0 for all P, showing the uniqueness of the func-
tions aP in (2.81).
The uniqueness of the constants cP in (2.82) follows similarly, but we have to
account for the fact that δi /∈S0(I) = TμP+(I). In order to get around this, let I be
a ﬁnite set and J := {0,1,2} × I. For i ∈I, we deﬁne
Vi := 2δ(0,i) −δ(1,i) −δ(2,i) ∈S0(J),
and for a multiindex ⃗i = (i1,...,in) ∈I n we let

τ P
J

μ

V ⃗i
:=

τ P
J

μ(Vi1,...,Vin).
Multiplying this term out, we see that (τ P
J )μ(V ⃗i) is a linear combination of terms
of the form (τ P
J )μ(δ(a1,i1),...,δ(an,in)), where ai ∈{0,1,2}. Thus, from Lemma 2.1
we conclude that

τ P
J

μ

V ⃗i
̸= 0
only if P ≤P(⃗i).
(2.84)
Moreover, if P(⃗i) = {P1,...,Pr} with |Pi| = ki, then by Deﬁnition 2.6 we have

τ P(⃗i)
J

cJ

V ⃗i
=
r0
i=1

τ ki
J

cJ (Vi,...,Vi)
=
r0
i=1

2ki + 2(−1)ki
|J|ki−1 = |J|n−|P(⃗i)|
r0
i=1

2ki + 2(−1)ki
.
In particular, since 2ki + 2(−1)ki > 0 for all ki ≥2 we conclude that

τ P(⃗i)
J

cJ

V ⃗i
̸= 0,
(2.85)
as long as P(⃗i) does not contain singleton set.
With this, we can now proceed as in the previous case: assume that

P∈Part(n),|Pi|≥2
cPτ P
I = 0
when restricted to×
n S0(I),
(2.86)
for constants cP which do not all vanish, and we let P0 be minimal with cP0 ̸= 0.
Let ⃗i = (i1,...,in) ∈I n be a multiindex with P(⃗i) = P0, and let J := {0,1,2} × I
be as above. Then
0 =

P∈Part(n),|Pi|≥2
cP

τ P
J

μ

V ⃗i (2.84)
=

P≤P0,|Pi|≥2
cP

τ P
J

μ

V ⃗i
= cP0

τ P0
J

μ

V ⃗i
,

62
2
Finite Information Geometry
where the last equality follows by the assumption that P0 is minimal. But
(τ P0
J )μ(V ⃗i) ̸= 0 by (2.85), whence cP0 = 0, contradicting the choice of P0.
This shows that (2.86) can happen only if all cP = 0, and this completes the
proof.
□
In the light of Proposition 2.9, it is thus reasonable to use the following terminol-
ogy.
Deﬁnition 2.7 The class of covariant tensors given in (2.81) and (2.82), respec-
tively, is called the class of congruent families of (restricted) covariant tensors
which is algebraically generated by the canonical n-tensors {τ n
I }.
We are now ready to state the main result of this section.
Theorem 2.3 (Classiﬁcation of congruent families of covariant n-tensors)
The
class of congruent families of covariant n-tensors on ﬁnite sets is the class alge-
braically generated by the canonical n-tensors {τ n
I }. That is, any (restricted) con-
gruent family of covariant n-tensors is of the form (2.81) and (2.82), respectively.
For n = 2, there are only two partitions, {{1},{2}} and {{1,2}}. Thus, in this case
the theorem states that each (restricted) congruent family of invariant 2-tensors must
be of the form
 ˜Θ2
I

μ(V1,V2) = a

∥μ∥1

g(V1,V2) + b

∥μ∥1

τ 1(V1)τ 1(V2),

Θ2
I

μ(V1,V2) = cg(V1,V2).
Therefore, we recover the theorems of Chentsov (cf. Theorem 2.1) and Campbell
(cf. [57] or [25]).
In the case n = 3, there is no partition with |Pi| ≥2 other than {{1,2,3}}, whence
it follows that the only restricted congruent family of covariant 3-tensors is—up to
multiplication by a constant—the canonical tensor τ 3
I , which coincides with the
Amari–Chentsov tensor T (cf. Theorem 2.2, see also [25] for the non-restricted
case).
On the other hand, for n = 4, there are several partitions with |Pi| ≥2, hence a
restricted congruent family of covariant 4-forms is of the form
Θ4
I (V1,...,V4) = c0τ 4(V1,...,V4) + c1g(V1,V2)g(V3,V4)
+ c2g(V1,V3)g(V2,V4) + c3g(V1,V4)g(V2,V3)
for constants c0,...,c3, where g = τ 2
I is the Fisher metric. Thus, in this case there
are more invariant families of such tensors. Evidently, for increasing n, the dimen-
sion of the space of invariant families increases rapidly.
The rest of this section will be devoted to the proof of Theorem 2.3 and will be
split up into several lemmas.

2.6
Congruent Families of Tensors
63
A multiindex ⃗i = (i1,...,in) ∈I n induces a partition P(⃗i) of the set {1,...,n}
into the equivalence classes of the relation k ∼l ⇔ik = il. For instance, for
n = 6 and pairwise distinct elements i,j,k ∈I, the partition induced by ⃗i :=
(j,i,i,k,j,i) is
P(⃗i) =

{1,5},{2,3,6},{4}

.
Lemma 2.1
Let τ P
I be the canonical n-tensor from Deﬁnition 2.6, and deﬁne the
center
cI := 1
|I|

i∈I
δi ∈P+(I),
(2.87)
as in the proof of Theorem 2.1. Then for any λ > 0 we have

τ P
I

λcI

δ⃗i
=

( |I|
λ )n−|P|
if P ≤P(⃗i),
0
otherwise.
(2.88)
Proof Let P = {P1,...,Pr} with |Pi| = ki, and let πP be the map from (2.79). Then
by (2.80) we have

τ P
I

μ

δ⃗i
=
r0
i=1

τ ki
I

μ

δiπP(i,1),...,δiπP(i,ki)
=
r0
i=1
θ
iπP(i,1),...,iπP(i,ki)
I;μ
.
Thus, (τ P
I )μ(δ⃗i) ̸= 0 if and only if θ
iπP(i,1),...,iπP(i,ki)
I;μ
̸= 0 for all i, and by (2.77)
this is the case if and only if iπP(i,1) = ··· = iπP(i,ki) for all i. But this is equivalent
to saying that P ≤P(⃗i), showing that (τ P
I )λcI (δ⃗i) = 0 if P ≰P(⃗i).
For μ = λcI , the components mi of μ all equal mi = λ/|I|, whence in this case
we have for all multiindices ⃗i with P ≤P(⃗i),

τ P
I

λcI

δ⃗i
=
r0
i=1
θi,...,i
I;λcI
(2.77)
=
r0
i=1
|I|
λ
ki−1
=
|I|
λ
k1+···+kr−r
=
|I|
λ
n−|P|
,
showing (2.88).
□
Now let us suppose that { ˜Θn
I : Iﬁnite} is a congruent family of covariant n-
tensors, and deﬁne θ⃗i
I,μ as in (2.70) and cI ∈P+(I) as in (2.87). The following
lemma generalizes Step 1 in the proof of Theorem 2.1.
Lemma 2.2 Let { ˜Θn
I : I ﬁnite} and θ⃗i
I,μ be as before, and let λ > 0. If ⃗i, ⃗j ∈I n are
multiindices with P(⃗i) = P(⃗j), then
θ⃗i
I,λcI = θ
⃗j
I,λcI .

64
2
Finite Information Geometry
Proof If P(⃗i) = P(⃗j), then there is a permutation σ : I →I such that σ(ik) = jk for
k = 1,...,n. We deﬁne the congruent Markov kernel K : I →P(I) by Ki := δσ(i).
Then evidently, K∗cI = cI , and (2.74) implies
θ⃗i
I,λcI =
 ˜Θn
I

λcI

δi1,...,δin
=
 ˜Θn
I

K∗(λcI )

K∗δi1,...,K∗δin
=
 ˜Θn
I

λcI

δj1,...,δjn
= θ
⃗j
I,λcI ,
which shows the claim.
□
By virtue of this lemma, we may deﬁne
θP
I,λcI := θ⃗i
I,λcI ,
where ⃗i ∈I n is a multiindex with P(⃗i) = P.
The following two lemmas generalize Step 2 in the proof of Theorem 2.1.
Lemma 2.3
Let { ˜Θn
I : I ﬁnite} and θP
I,λcI be as before, and suppose that P0 ∈
Part(n) is a partition such that
θP
I,λcI = 0
for all P < P0, λ > 0 and I.
(2.89)
Then there is a continuous function fP0 : (0,∞) →R such that
θP0
I,λcI = fP0(λ)|I|n−|P0|.
(2.90)
Proof Let I,J be ﬁnite sets, and let I ′ := I × J . We deﬁne the congruent Markov
kernel
K : I −→P

I ′
,
i −→1
|J|

j∈J
δ(i,j)
with the partition ({i} × J)i∈I of I ′. Then K∗cI = cI ′ is easily veriﬁed. Moreover,
if ⃗i = (i1,...,in) ∈I n is a multiindex with P(⃗i) = P0, then
θP0
I,λcI
=
 ˜Θn
I

λcI

δi1,...,δin
(2.74)
=
 ˜Θn
I ′

K∗(λcI )

K∗δi1,...,K∗δin
=
 ˜Θn
I ′

λcI′
 1
|J|

j1∈J
δ(i1,j1),..., 1
|J|

jn∈J
δ(in,jn)

=
1
|J|n

(j1,...,jn)∈J n
θP((i1,j1),...,(in,jn))
I ′,λcI′
.

2.6
Congruent Families of Tensors
65
Observe that P((i1,j1),...,(in,jn)) ≤P(⃗i) = P0. If P((i1,j1),...,(in,jn)) < P0,
then θP((i1,j1),...,(in,jn))
I ′,λcI′
= 0 by (2.89).
Moreover,
there
are
|J||P0|
multiindices
(j1,...,jn) ∈J n
for
which
P((i1,j1),...,(in,jn)) = P0, and since for all of these θP((i1,j1),...,(in,jn))
I ′,λcI′
= θP0
I ′,λcI′ ,
we obtain
θP0
I,λcI =
1
|J|n

(j1,...,jn)∈J n
θP((i1,j1),...,(in,jn))
I ′,λcI′
= |J||P0|
|J|n θP0
I ′,λcI′,
and since |I ′| = |I||J|, it follows that
1
|I|n−|P0| θP0
I,λcI =
1
|I|n−|P0|

1
|J|n−|P0| θP0
I ′,λcI′

=
1
|I ′|n−|P0| θP0
I ′,λcI′.
Interchanging the roles of I and J in the previous arguments, we also get
1
|J|n−|P0| θP0
J,λcJ =
1
|I ′|n−|P0| θP0
I ′,λcI′ =
1
|I|n−|P0| θP0
I,λcI ,
whence fP0(λ) :=
1
|I|n−|P0| θP0
I,λcI is indeed independent of the choice of the ﬁnite
set I.
□
Lemma 2.4 Let { ˜Θn
I : I ﬁnite} and λ > 0 be as before. Then there is a congruent
family { ˜Ψ n
I : I ﬁnite} of the form (2.81) such that
 ˜Θn
I −˜Ψ n
I

λcI = 0
for all ﬁnite sets I and all λ > 0.
Proof For a congruent family of covariant n-tensors { ˜Θn
I : I ﬁnite}, we deﬁne
N
 ˜Θn
I

:=

P ∈Part(n) :
 ˜Θn
I

λcI

δ⃗i
= 0 whenever P(⃗i) ≤P

.
If N({ ˜Θn
I }) ⊊Part(n), then let
P0 = {P1,...,Pr} ∈Part(n)\N
 ˜Θn
I

be a minimal element, i.e., such that P ∈N({ ˜Θn
I }) for all P < P0. In particular, for
this partition (2.89) and hence (2.90) holds. Let
 ˜Θ′n
I

μ :=
 ˜Θn
I

μ −∥μ∥n−|P0|
1
fP0

∥μ∥1

τ P0
I

μ
(2.91)
with the function fP0 from (2.90). Then { ˜Θ′n
I : I ﬁnite} is again a covariant family
of covariant n-tensors.
Let P ∈N({ ˜Θn
I }) and ⃗i be a multiindex with P(⃗i) ≤P. If (τ P0
I )λcI (δ⃗i) ̸= 0, then
by Lemma 2.1 we would have P0 ≤P(⃗i) ≤P ∈N({ ˜Θn
I }) which would imply that
P0 ∈N({ ˜Θn
I }), contradicting the choice of P0.

66
2
Finite Information Geometry
Thus, (τ P0
I )λcI (δ⃗i) = 0 and hence ( ˜Θ′n
I)λcI (δ⃗i) = 0 whenever P(⃗i) ≤P, showing
that P ∈N({ ˜Θ′n
I}).
Thus, what we have shown is that N({ ˜Θn
I })⊆N({ ˜Θ′n
I}). On the other hand, if
P(⃗i) = P0, then again by Lemma 2.1

τ P0
I

λcI

δ⃗i
=
|I|
λ
n−|P0|
,
and since ∥λcI∥1 = λ, it follows that
 ˜Θ′n
I

λcI

δ⃗i (2.91)
=
 ˜Θn
I

λcI

δ⃗i
−λn−|P0|fP0(λ)

τ P0
I

λcI

δ⃗i
= θP0
I,λcI −λn−|P0|fP0(λ)
|I|
λ
n−|P0|
= θP0
I,λcI −fP0(λ)|I|n−|P0| (2.90)
= 0.
That is, ( ˜Θ′n
I)λcI (δ⃗i) = 0 whenever P(⃗i) = P0. If ⃗i is a multiindex with P(⃗i) < P0,
then P(⃗i) ∈N({ ˜Θ′n
I}) by the minimality of P0, so that ˜Θn
I (δ⃗i) = 0. Moreover,
(τ P0
I )λcI (δ⃗i) = 0 by Lemma 2.1, whence
 ˜Θ′n
I

λcI

δ⃗i
= 0
whenever P(⃗i) ≤P0,
showing that P0 ∈N({ ˜Θ′n
I}). Therefore,
N
 ˜Θn
I

⊊N
 ˜Θ′n
I

.
What we have shown is that given a congruent family of covariant n-tensors { ˜Θn
I }
with N({ ˜Θn
I }) ⊊Part(n), we can enlarge N({ ˜Θn
I }) by subtracting a multiple of the
canonical tensor of some partition. Repeating this ﬁnitely many times, we conclude
that for some congruent family { ˜Ψ n
I } of the form (2.81)
N
 ˜Θn
I −˜Ψ n
I

= Part(n),
and this implies by deﬁnition that ( ˜Θn
I −˜Ψ n
I )λcI = 0 for all I and all λ > 0.
□
Finally, the next lemma generalizes Step 3 in the proof of Theorem 2.1.
Lemma 2.5 Let { ˜Θn
I : I ﬁnite} be a congruent family of covariant n-tensors such
that ( ˜Θn
I )λcI = 0 for all I and λ > 0. Then ˜Θn
I = 0 for all I.
Proof The proof of Step 3 in Theorem 2.1 carries over almost literally. Namely,
consider μ ∈M+(I) such that πI(μ) = μ/∥μ∥1 ∈P+(I) has rational coefﬁcients,

2.6
Congruent Families of Tensors
67
i.e.,
μ = ∥μ∥1

i
ki
n δi
for some ki,n ∈N and 	
i∈I ki = n. Let
I ′ :=
%
i∈I

{i} × {1,...,ki}

,
so that |I ′| = n, and consider the congruent Markov kernel
K : i −→1
ki
ki

j=1
δ(i,j).
Then
K∗μ = ∥μ∥1

i
ki
n

1
ki
ki

j=1
δ(i,j)

= ∥μ∥1
1
n

i
ki

j=1
δ(i,j) = ∥μ∥1cI ′.
Thus, (2.74) implies
 ˜Θn
I

μ(V1,...,Vn) =
 ˜Θn
I ′

∥μ∥1cI′
(
)*
+
=0
(K∗V1,...,K∗Vn) = 0,
so that ( ˜Θn
I )μ = 0 whenever πI(μ) has rational coefﬁcients. But these μ form a
dense subset of M+(I), whence ( ˜Θn
I )μ = 0 for all μ ∈M+(I), which completes
the proof.
□
Proof of Theorem 2.3 Let { ˜Θn
I : I ﬁnite} be a congruent family of covariant n-
tensors. By Lemma 2.4 there is a congruent family { ˜Ψ n
I : I ﬁnite} of the form (2.81)
such that ( ˜Θn
I −˜Ψ n
I )λcI = 0 for all ﬁnite I and all λ > 0.
Since { ˜Θn
I −˜Ψ n
I : I ﬁnite} is again a congruent family, Lemma 2.5 implies that
˜Θn
I −˜Ψ n
I = 0 and hence ˜Θn
I = ˜Ψ n
I is of the form (2.81), showing the ﬁrst part of
Theorem 2.3.
For the second part, observe that by Proposition 2.6 any restricted congruent fam-
ily of covariant n-tensors is the restriction of a congruent family of n-tensors, that
is, by the ﬁrst part of the theorem, the restriction of a family of the form (2.81).
This restriction takes the form (2.82) with cP := aP(1), observing that the restric-
tion of τ P
I with a partition containing a singleton set vanishes as τ 1
I vanishes when
restricted to P+(I).
□

68
2
Finite Information Geometry
Fig. 2.5 Illustration of (A) the difference vector p −q in Rn pointing from q to p; and (B) the
difference vector X(q,p) = ˙γq,p(0) as the inverse of the exponential map in q
2.7 Divergences
In this section, we derive distance-like functions, so-called divergences, that are
naturally associated with a manifold, equipped with a Riemannian metric g and an
afﬁne connection ∇, possibly different from the Levi-Civita connection of g. In our
context, this will lead to the relative entropy and its extensions, the α-divergences,
on M+(I). These divergences are special cases of canonical divergences which will
be deﬁned in Sect. 4.3.
2.7.1 Gradient-Based Approach
We begin our motivation of a divergence in terms of a simple example where the
manifold is Rn, equipped with the standard Euclidean metric and its corresponding
connection, the Levi-Civita connection. Consider a point p ∈Rn, and the vector
ﬁeld pointing to p (see Fig. 2.5(A)):
Rn →Rn,
q →p −q.
(2.92)
Obviously, the difference ﬁeld (2.92) can be seen as the negative gradient of the
squared distance
Dp : Rn →R,
q →Dp(q) := D(p ∥q) := 1
2∥p −q∥2 = 1
2
n

i=1
(pi −qi)2,
that is,
p −q = −gradq Dp.
(2.93)
Here, the gradient gradq is taken with respect to the canonical inner product on Rn.
We shall now generalize the relation (2.93) between the squared distance Dp and
the difference of two points p and q to the more general setting of a differentiable
manifold M. Given a point p ∈M, we want to deﬁne a vector ﬁeld q →X(q,p), at
least in a neighborhood of p, that corresponds to the difference vector ﬁeld (2.92).

2.7
Divergences
69
Obviously, the problem is that the difference p −q is not naturally deﬁned for a
general manifold M. We need an afﬁne connection ∇in order to have a notion of
a difference. Given such a connection ∇, for each point q ∈M and each direction
X ∈TqM we consider the geodesic γq,X, with the initial point q and the initial
velocity X, that is, γq,X(0) = q and ˙γq,X(0) = X (see (B.38) in Appendix B). If
γq,X(t) is deﬁned for all 0 ≤t ≤1, the endpoint p = γq,X(1) is interpreted as the
result of a translation of the point q along a straight line in the direction of the
vector X. The collection of all these translations is summarized in terms of the
exponential map
expq : Uq →M,
X →γq,X(1),
(2.94)
where Uq ⊆TqM denotes the set of tangent vectors X, for which the domain of
γq,X contains the unit interval [0,1] (see (B.39) and (B.40) in Appendix B).
Given two points p and q, one can interpret any X with expq(X) = p as a dif-
ference vector X that translates q to p. For simplicity, we assume the existence and
uniqueness of such a difference vector, denoted by X(q,p) (see Fig. 2.5(B)).
This is a strong assumption, which is, however, always locally satisﬁed. On the
other hand, although being quite restrictive in general, this property will be satis-
ﬁed in our information-geometric context, where g is given by the Fisher metric
and ∇is given by the m- and e-connections and their convex combinations, the
α-connections. We shall consider these special but important cases in Sects. 2.7.2
and 2.7.3.
If we attach to each point q ∈M the difference vector X(q,p), we obtain a vector
ﬁeld that corresponds to the vector ﬁeld (2.92) in Rn. In order to interpret the vector
ﬁeld q →X(q,p) as a negative gradient ﬁeld of a (squared) distance function, and
thereby generalize (2.93), we need a Riemannian metric g on M. We search for a
function Dp satisfying
X(q,p) = −gradq Dp,
(2.95)
where the Riemannian gradient is taken with respect to g (see Appendix B). Ob-
viously, we may set Dp(p) = 0. In order to recover Dp from (2.95), we consider
any curve γ : [0,1] →M that connects q with p, that is, γ (0) = q and γ (1) = p,
and integrate the inner product of the curve velocity ˙γ (t) with the vector X(γ (t),p)
along the curve:
 1
0

X

γ (t),p

, ˙γ (t)

dt = −
 1
0

gradγ (t) Dp, ˙γ (t)

dt
= −
 1
0
(dγ (t)Dp)

˙γ (t)

dt
= −
 1
0
d Dp ◦γ
d t
(t)dt
= Dp

γ (0)

−Dp

γ (1)

= Dp(q) −Dp(p) = Dp(q).
(2.96)

70
2
Finite Information Geometry
This deﬁnes, at least locally, a function Dp that is assigned to the Riemannian metric
g and the connection ∇. In what follows, we shall mainly use the standard notation
D(p ∥q) = Dp(q) of a divergence as a function D of two arguments.
2.7.2 The Relative Entropy
Now we apply the idea of Sect. 2.7.1 in order to deﬁne divergences for the m- and
e-connections on the cone M+(I) of positive measures. We consider a measure
μ ∈M+(I) and deﬁne two vector ﬁelds on M+(I) as the inverse of the exponential
maps given by (2.43) and (2.44):
ν →
X(m)(ν,μ) :=

i∈I
νi
μi
νi
−1

δi,
ν →
X(e)(ν,μ) :=

i∈I
νi log μi
νi
δi.
(2.97)
We can easily verify that these vector ﬁelds are gradient ﬁelds: The functions
f i(ν) := μi
νi
−1
and
gi(ν) := log μi
νi
trivially satisfy the integrability condition (2.35), that is, ∂f i
∂νj = ∂f j
∂νi and ∂gi
∂νj = ∂gj
∂νi
for all i,j. Therefore, for both connections there are corresponding divergences that
satisfy Eq. (2.95).
We derive the divergence of the m-connection ﬁrst, which we denote by D(m).
We consider a curve γ : [0,1] →M+(I) connecting ν with μ, that is, γ (0) = ν and
γ (1) = μ. This implies


X(m)
γ (t),μ

, ˙γ (t)

=

i∈I
1
γi(t)

μi −γi(t)

˙γi(t)
(2.98)
and
D(m)(μ∥ν) =
 1
0


X(m)
γ (t),μ

, ˙γ (t)

dt
=

i∈I
 1
0
1
γi(t)

μi −γi(t)

˙γi(t)dt
=

i∈I
-
μi logγi(t) −γi(t)
,1
0
=

i∈I
(μi logμi −μi −μi logνi + νi)
=

i∈I

νi −μi + μi log μi
νi

.

2.7
Divergences
71
With the same calculation for the e-connection, we obtain the corresponding diver-
gence, which we denote by D(e). Again, we consider a curve γ connecting ν with μ.
This implies


X(e)
γ (t),μ

, ˙γ (t)

=

i∈I
˙γi(t) log μi
γi(t)
(2.99)
and
D(e)(μ∥ν) =
 1
0


X(e)
γ (t),μ

, ˙γ (t)

dt
=

i∈I
 1
0
˙γi(t) log μi
γi(t) dt
=

i∈I
1
γi(t)

1 + log μi
γi(t)
21
0
=

i∈I

μi −νi

1 + log μi
νi

=

i∈I

μi −νi + νi log νi
μi

= D(m)(ν ∥μ).
These calculations give rise to the following deﬁnition:
Deﬁnition 2.8 (Kullback–Leibler divergence ([155, 156]))
The function DKL :
M+(I) × M+(I) →R deﬁned by
DKL(μ∥ν) :=

i∈I
νi −

i∈I
μi +

i∈I
μi log μi
νi
(2.100)
is called the relative entropy, information divergence, or Kullback–Leibler diver-
gence (KL-divergence). Its restriction to the set of probability distributions is given
by
DKL(μ∥ν) =

i∈I
μi log μi
νi
.
(2.101)
Proposition 2.10 The following holds:

X(m)(ν,μ) = −gradν DKL(μ∥·),

X(e)(ν,μ) = −gradν DKL(·∥μ),
(2.102)

72
2
Finite Information Geometry
where DKL is given by (2.100) in Deﬁnition 2.8. Furthermore, DKL is the only func-
tion on M+(I) × M+(I) that satisﬁes the conditions (2.102) and DKL(μ∥μ) = 0
for all μ.
Proof The statement is obvious from the way we introduced DKL as a potential
function of the gradient ﬁeld ν →
X(m)(ν,μ) and ν →
X(e)(ν,μ), respectively. The
following is an alternative direct veriﬁcation. We ﬁrst compute the partial deriva-
tives:
∂DKL(μ∥·)
∂νi
(ν) = −μi
νi
+ 1,
∂DKL(·∥μ)
∂νi
(ν) = −log μi
νi
.
With the formula (2.34), we obtain

gradν DKL(μ∥·)

i = νi

−μi
νi
+ 1

= −μi + νi,

gradν DKL(·∥μ)

i = −νi log μi
νi
.
A comparison with (2.97) veriﬁes (2.102) which uniquely characterize DKL(μ∥·)
and DKL(·∥μ), up to a constant depending on μ. With the additional assumption
DKL(μ∥μ) = 0 for all μ, this constant is ﬁxed.
□
We now ask whether the restriction (2.101) of the Kullback–Leibler divergence
to the manifold P+(I) is the right divergence function in the sense that (2.102) also
holds for the exponential maps of the restricted m- and e-connections. It is easy to
verify that this is indeed the case. In order to elaborate on the geometric reason for
this, we consider a general Riemannian manifold M and a submanifold N. Given
an afﬁne connection ∇on M, we can deﬁne its restriction ∇to N. More precisely,
denoting the projection of a vector Z in TpM onto TpN by Π⊤
p (Z), we deﬁne
∇XY|p := Π⊤
p (∇XY|p), where X and Y are vector ﬁelds on N. Furthermore, we
denote the exponential map of ∇by expp and its inverse by X(p,q).
Now, given p ∈N, we consider a function 
Dp on M that satisﬁes Eq. (2.95).
With the restriction Dp of 
Dp to the submanifold N, this directly implies
Π⊤
q

X(q,p)

= −Π⊤
q (gradq 
Dp) = −gradq Dp.
However, in order to have X(q,p) = −gradq Dp, which corresponds to Eq. (2.95)
on the submanifold N, the following equality is required:
X(q,p) = Π⊤
q

X(q,p)

.
(2.103)

2.7
Divergences
73
We now verify this condition for the m- and e-connections on M+(I) and its
submanifold P+(I). One can easily show that the vector ﬁelds
ν →X(m)(ν,μ) :=

i∈I
νi
μi
νi
−1

δi = 
X(m)(ν,μ),
ν →X(e)(ν,μ) :=

i∈I
νi

log μi
νi
−

j∈I
νj log μj
νj

δi
(2.104)
satisfy
exp(m)
ν,X(m)(ν,μ)

= μ
and
exp(e)
ν,X(e)(ν,μ)

= μ,
(2.105)
respectively, where the exponential maps are given in Proposition 2.5. On the other
hand, if we project the vectors 
X(m)(ν,μ) and 
X(e)(ν,μ) onto S0(I) ∼= TνP+(I) by
using (2.14), we obtain
X(m)(ν,μ) = Π⊤
ν

X(m)(ν,μ)

(2.106)
and
X(e)(ν,μ) = Π⊤
ν

X(e)(ν,μ)

.
(2.107)
This proves that the condition (2.103) is satisﬁed, which implies
Proposition 2.11 The following holds:
X(m)(ν,μ) = −gradν DKL(μ∥·),
X(e)(ν,μ) = −gradν DKL(·∥μ),
(2.108)
where DKL is given by (2.101) in Deﬁnition 2.8. Furthermore, DKL is the only func-
tion on P+(I) × P+(I) that satisﬁes the conditions (2.108) and DKL(μ∥μ) = 0 for
all μ.
2.7.3 The α-Divergence
We now extend the derivations of Sect. 2.7.2 to the α-connections, leading to a
generalization of the relative entropy, the so-called α-divergence (Deﬁnition 2.9 be-
low). In order to do, so we deﬁne the following vector ﬁeld as the inverse of the
α-exponential map on the manifold M+(I) given by (2.58):
ν →
X(α)(ν,μ) :=
2
1 −α

i∈I
νi
μi
νi
 1−α
2
−1

δi.
(2.109)

74
2
Finite Information Geometry
Again, we can easily verify that the vector ﬁeld ν →
X(α)(ν,μ) is a gradient ﬁeld by
observing that the integrability condition (2.35) is trivially satisﬁed, that is, ∂f i
∂νj =
∂f j
∂νi for all i,j, where
f i(ν) :=
μi
νi
 1−α
2
−1.
In order to derive the divergence D(α) of the α-connection, we consider a curve
γ : [0,1] →M+(I) connecting ν with μ. We obtain


X(α)
γ (t),μ

, ˙γ (t)

=
2
1 −α

i∈I
˙γi(t)
 μi
γi(t)
 1−α
2
−1

(2.110)
and
D(α)(μ∥ν) =
 1
0


X(α)
γ (t),μ

, ˙γ (t)

dt
=

i∈I
 1
0
2
1 −α ˙γi(t)
 μi
γi(t)
 1−α
2
−1

dt
=

i∈I
1
4
1 −α2 γi(t)
1+α
2 μi
1−α
2 −
2
1 −α γi(t)
21
0
=

i∈I

2
1 + α μi −

4
1 −α2 ν
1+α
2
i
μ
1−α
2
i
−
2
1 −α νi

=

i∈I

2
1 −α νi +
2
1 + α μi −
4
1 −α2 ν
1+α
2
i
μ
1−α
2
i

.
Obviously, we have
D(−α)(μ∥ν) = D(α)(ν ∥μ).
(2.111)
These calculations give rise to the following deﬁnition:
Deﬁnition 2.9 (α-Divergence) The function D(α) : M+(I)×M+(I) →R deﬁned
by
D(α)(μ∥ν) :=
2
1 −α

i∈I
νi +
2
1 + α

i∈I
μi −
4
1 −α2

i∈I
ν
1+α
2
i
μ
1−α
2
i
(2.112)
is called the α-divergence. Its restriction to probability measures is given by
D(α)(μ∥ν) =
4
1 −α2

1 −

i∈I
ν
1+α
2
i
μ
1−α
2
i

.
(2.113)

2.7
Divergences
75
Proposition 2.12 The following holds:

X(α)(ν,μ) = −gradν D(α)(μ∥·),
(2.114)
where D(α) is given by (2.112) in Deﬁnition 2.9. Furthermore, D(α) is the only func-
tion on M+(I) × M+(I) that satisﬁes the conditions (2.114) and D(α)(μ∥μ) = 0
for all μ.
Proof The statement is obvious from the way we introduced D(α) as a potential
function of the gradient ﬁeld ν →
X(α)(ν,μ). The following is an alternative direct
veriﬁcation. We compute the partial derivative:
∂D(α)(μ∥·)
∂νi
(ν) =
2
1 −α

1 −ν
1+α
2 −1
i
μ
1−α
2
i

.
With the formula (2.34), we obtain

gradν D(α)(μ∥·)

i = νi ·
2
1 −α

1 −ν
1+α
2 −1
i
μ
1−α
2
i

=
2
1 −α

νi −ν
1+α
2
i
μ
1−α
2
i

.
A comparison with (2.109) veriﬁes (2.114) which uniquely characterizes the func-
tion D(α)(μ∥·), up to a constant depending on μ. With the additional assumption
D(α)(μ∥μ) = 0 for all μ, this constant is ﬁxed.
□
With L’Hopitâl’s rule, one can easily verify
lim
α→−1D(α)(μ∥ν) = D(m)(μ∥ν) = DKL(μ∥ν)
(2.115)
and
lim
α→1D(α)(μ∥ν) = D(e)(μ∥ν) = DKL(ν ∥μ),
(2.116)
where DKL is relative entropy deﬁned by (2.100).
In what follows, we use the notation D(α) also for α ∈{−1,1} by setting
D(−1)(μ∥ν) := DKL(μ∥ν) and D(1)(μ∥ν) := DKL(ν ∥μ). This is consistent with
the deﬁnition of the α-connection, given by (2.54), where we have the m-connection
for α = −1 and the e-connection for α = 1. Note that D(0) is closely related to the
Hellinger distance (2.28):
D(0)(μ∥ν) = 2

dH(μ,ν)
2.
(2.117)
We would like to point out that the α-divergence on the simplex P+(I), −1 < α < 1,
based on (2.95) does not coincide with the restriction of the α-divergence on
M+(I). To be more precise, we have seen that the restriction of the relative en-
tropy, deﬁned on M+(I), to the submanifold P+(I) is already the right divergence

76
2
Finite Information Geometry
for the projected m- and e-connections (see Proposition 2.11). The situation turns
out to be more complicated for general α. From Eq. (2.65) we obtain
X(α)(ν,μ) = ˙τν,μ(0)Π⊤
ν

X(α)(ν,μ)

.
This equality deviates from the condition (2.103) by the factor ˙τν,μ(0), which proves
that the restriction of the α-divergence, which is deﬁned on M+(I), to the subman-
ifold P+(I) does not coincide with the α-divergence on P+(I). As an example, we
consider the case α = 0, where the α-connection is the Levi-Civita connection of the
Fisher metric. In that case, the canonical divergence equals 1
2(dF (μ,ν))2, where dF
denotes the Fisher distance (2.27). Obviously, this divergence is different from the
divergence D(0), given by (2.117), which is based on the distance in the ambient
space M+(I), the Hellinger distance. On the other hand, 1
2(dF )2 can be written as
a monotonically increasing function of D(0):
1
2

dF (μ,ν)
2 = 2 arccos2

1 −1
4 D(0)(μ∥ν)

.
(2.118)
2.7.4 The f -Divergence
Our derivation of D(α) was based on the idea of a squared distance function associ-
ated with the α-connections in terms of the general Eq. (2.95). However, it turns out
that, although being naturally motivated, the functions D(α) do not share all prop-
erties of the square of a distance, except for α = 0. The symmetry is obviously not
satisﬁed. On the other hand, we have D(α)(μ∥ν) ≥0, and D(α)(μ∥ν) = 0 if and
only if μ = ν. One can verify this by considering D(α) as being a function of a more
general structure, which we are now going to introduce. Given a strictly convex
function f : R+ →R, we deﬁne
Df (μ∥ν) :=

i∈I
μi f
 νi
μi

.
(2.119)
This function is known as the f -divergence, and it was introduced and studied by
Csiszár [70–73]. Jensen’s inequality immediately implies
Df (μ∥ν) =

j∈I
μj

i∈I
μi
	
j∈I μj
f
 νi
μi

≥

j∈I
μj

f

i∈I
μi
	
j∈I μj
νi
μi

=

j∈I
μj

f
 	
i∈I νi
	
j∈I μj

,

2.7
Divergences
77
where the equality holds if and only if μ = ν. If f (x) is non-negative for all x, and
f (1) = 0, then we obtain
Df (μ∥ν) ≥0,
and
Df (μ∥ν) = 0 if and only if μ = ν.
(2.120)
In order to reformulate D(α) as such a function Df , we deﬁne
f (α)(x) :=
⎧
⎪⎨
⎪⎩
4
1−α2 ( 1−α
2
+ 1+α
2 x −x
1+α
2 ),
if α /∈{−1,1},
x −1 −logx,
if α = −1,
1 −x + x logx,
if α = 1.
(2.121)
With this deﬁnition, we have D(α) = Df (α). Furthermore, it is easy to verify that for
each α ∈[−1,1], the function f (α) is non-negative and vanishes if and only if its
argument is equal to one. This proves that the functions D(α) satisfy (2.120), which
is a property of a metric. In conclusion, we have seen that, although D(α) is not
symmetric and does not satisfy the triangle inequality, it still has some important
properties of a squared distance. In this sense, we have obtained a distance-like
function that is associated with the α-connection and the Fisher metric on M+(I),
coupled through Eq. (2.95). The following proposition suggests a way to recover
these two objects from the α-divergence.
Proposition 2.13 The following holds:
gμ(X,Y) = ∂2D(α)(μ∥·)
∂Y∂X
(ν)

ν=μ
,
(2.122)
Tμ(X,Y,Z) = −
2
3 −α
∂3D(α)(μ∥·)
∂Z ∂Y ∂X (ν)

ν=μ
.
(2.123)
The proof of this proposition is by simple calculation. This implies that the Fisher
metric can be recovered through the partial derivatives of second order (see (2.122)).
In particular, this determines the Levi-Civita connection ∇(0) of g, and we can use
T to derive the α-connection based on the deﬁnition (2.54):
g
∇(α)
X Y,Z

= g
∇(0)
X Y,Z

−α
2 T(X,Y,Z).
(2.124)
Thus, we can recover the Fisher metric and the α-connection from the partial deriva-
tives of the α-divergence up to the third order. We obtain the following expansion
of the α-divergence:
D(α)(μ∥ν) = 1
2

i,j
gμ(δi,δj)(μ)(νi −μi)(νj −μj)
+ 1
6
α −3
2

i,j,k
Tμ(δi,δj,δk)(νi −μi)(νj −μj)(νk −μk)
+ O

∥ν −μ∥4
.
(2.125)

78
2
Finite Information Geometry
Any function D that satisﬁes the positivity (2.120) and for which the bilinear form
gμ(X,Y) := ∂2D(μ∥·)
∂Y∂X
(ν)

ν=μ
is positive deﬁnite, is called a divergence or contrast function (see [93, 173]). In
Chap. 4, we will revisit divergence functions and related expressions between ten-
sors and afﬁne connections in terms of partial derivatives of potential functions from
a more general perspective. We highlight a few important facts already in this sec-
tion. The coupling between the divergence function D(α) and the tensors g and T
through the above expansion (2.125) is clearly not one-to-one, as the derivatives of
order greater than three are not ﬁxed. For instance, one could simply neglect the
higher-order terms in order to obtain a divergence function that has the same expan-
sion up to order three. A more interesting divergence for g and T is given in terms
of the f -divergence. One can easily prove that
Df (μ∥ν) = 1
2f ′′(1)

i,j
gμ

δi,δj
(μ)(νi −μi)(νj −μj)
+ 1
6f ′′′(1)

i,j,k
Tμ

δi,δj,δk
(νi −μi)(νj −μj)(νk −μk)
+ O

∥ν −μ∥4
.
(2.126)
If we choose a function f that satisﬁes f ′′(1) = 1 and f ′′′(1) = α−3
2 , then this
expansion coincides with (2.125) up to the third order. Clearly, f (α), as deﬁned
in (2.121), satisﬁes these two conditions. However, this is only one of inﬁnitely
many possible choices. This shows that the coupling between a divergence function
and an afﬁne connection through (2.95), which uniquely characterizes the relative
entropy and the α-divergence on M+(I), is stronger than the coupling through the
speciﬁcation of the partial derivatives up to the third order.
2.7.5 The q-Generalization of the Relative Entropy
There is a different way of relating the α-divergence to the relative entropy. Instead
of verifying the consistency of DKL and D(α) in terms of (2.115) and (2.116), one
can rewrite D(α) so that it resembles the structure of the relative entropy (2.100).
This approach is based on Tsallis’ so-called q-generalization of the entropy and
the relative entropy [248, 249]. Here, q is a parameter with values in the unit
interval ]0,1[ which directly corresponds to α = 1 −2q ∈]−1,+1[. With this
reparametrization, the α-divergence (2.112) becomes
D(1−2q)(μ∥ν) = 1
q

i
νi +
1
1 −q

i
μi −
1
q(1 −q)

i
ν1−q
i
μq
i .
(2.127)

2.8
Exponential Families
79
This can be rewritten in a way that resembles the structure of the relative entropy
DKL. In order to do so, we deﬁne the q-exponential function and its inverse, the
q-logarithmic function:
expq(x) :=

1 + (1 −q)x

1
1−q ,
logq(x) :=
1
1 −q

x1−q −1

.
(2.128)
For q →1, these deﬁnitions converge to the ordinary deﬁnitions. Now we can
rewrite (2.127) as follows:
D(1−2q)(μ∥ν) = 1
q

i
νi −

i
μi −

i
μi logq
 νi
μi

.
(2.129)
This resembles, up to the factor 1
q , the Kullback–Leibler divergence (2.100). In this
sense, the α-divergence can be considered as a q-generalization of the Kullback–
Leibler divergence. These generalizations turn out to be relevant in physics, leading
to the ﬁeld of nonextensive statistical mechanics as a generalization of Boltzmann–
Gibbs statistical mechanics [198, 249]. Information geometry contributes to a better
geometric understanding of this new ﬁeld of research [197, 204, 205]. (For a detailed
overview of related information-geometric works, see [11].)
2.8 Exponential Families
2.8.1 Exponential Families as Afﬁne Spaces
In Sects. 2.4 and 2.5 we introduced the m- and e-connections and their convex com-
binations, the α-connections. In general, the notion of an afﬁne connection extends
the notion of an afﬁne action of a vector space V on an afﬁne space E. Given a
point p ∈E and a vector v ∈V , such an action translates p along v into a new
point p + v. On the other hand, for each pair of points p,q ∈E, there is a vec-
tor v, called the difference vector between p and q, which translates p into q, that
is, q = p + v. The afﬁne space E can naturally be interpreted as a manifold with
tangent bundle E × V . The translation map E × V →E is then nothing but the
exponential map exp of the afﬁne connection given by the natural parallel transport
Πp,q : (p,v) →(q,v). Clearly, this is a very special parallel transport. It is, how-
ever, closely related to the transport maps (2.39) and (2.40), which deﬁne the m- and
e-connections. Therefore, we can ask the question whether the exponential maps of
the m- and e-connections deﬁne afﬁne actions on M+(I) and P+(I). This afﬁne
space perspective of M+(I) and P+(I), and their respective extensions to spaces of
σ-ﬁnite measures (see Remark 3.8(1)), has been particularly highlighted in [192].
Obviously, M+(I), equipped with the m-connection, is not an afﬁne space,
simply because the corresponding exponential map is not complete. For each
μ ∈M+(I) there is a vector v ∈S(I) so that μ + v /∈M+(I). Now, let us come

80
2
Finite Information Geometry
to the e-connection. The exponential map .
exp(e) is deﬁned on the tangent bundle
T M+(I) and translates each point μ along a vector Vμ ∈TμM+(I). In order to in-
terpret it as an afﬁne action, we identify the tangent spaces TμM+(I) and TνM+(I)
in two points μ and ν in terms of the corresponding parallel transport. More pre-
cisely, we introduce an equivalence relation ∼by which we identify two vectors
Aμ = (μ,a) ∈TμM+(I) and Bν = (ν,b) ∈TνM+(I) if Bν is the parallel trans-
port of Aμ from μ to ν, that is, Bν = 
Π(e)
μ,νAμ. Also, the equivalence class of a
vector (μ,a) can be identiﬁed with the density da
dμ, which is an element of F(I).
Obviously,
Aμ ∼Bν
⇔
b = da
dμ
⇔
db
dν = da
dμ.
Now we show that F(I) acts afﬁnely on M+(I). Consider a point μ and a vector f ,
interpreted as translation vector. This deﬁnes a vector (μ,f μ) ∈TμM+(I), which
is mapped via the exponential map to
.
exp(e)
μ (f μ) = ef μ.
Altogether we have deﬁned the map
M+(I) × F(I) →M+(I),
(μ,f ) →μ + f := ef μ,
(2.130)
which satisﬁes
(μ + f ) + g = ef μ + g = eg
ef μ

= ef +gμ = μ + (f + g).
Furthermore, with the vector vec(μ,ν) := log( dν
dμ) we obviously have μ +
vec(μ,ν) = ν, and this is the only vector that translates μ to ν. This veriﬁes that +
is an afﬁne action of F(I) on M+(I).
We apply the same derivation in order to deﬁne an afﬁne structure on P+(I). This
leads to the following version of the map (2.130) deﬁned for the simplex P+(I) (see
Fig. 2.6):
P+(I) ×

F(I)/R

→P+(I),
(μ,f + R) →μ + (f + R) :=
ef
μ(ef )μ.
(2.131)
This is an afﬁne action of the vector space F(I)/R on P+(I) with difference
vector
vec : P+(I) × P+(I) →F(I)/R,
(μ,ν) →vec(μ,ν) = log
 dν
dμ

+ R.
Therefore, P+(I) is an afﬁne space over F(I)/R.
The corresponding afﬁne subspaces play a central role within information geom-
etry.

2.8
Exponential Families
81
Fig. 2.6 The afﬁne action of
F(I)/R on the simplex
P+(I)
Deﬁnition 2.10 (Exponential family) An afﬁne subspace E of P+(I) with respect
to + is called an exponential family. Given a measure μ0 ∈M+(I) and a linear
subspace L of F(I), the following submanifold of P+(I) is an exponential family:
E(μ0,L) :=

ef
μ0(ef )μ0 : f ∈L

.
(2.132)
To simplify the notation, in the case where μ0 is the counting measure, that is,
μ0 = 	
i∈I δi, we simply write E(L) instead of E(μ0,L).
Clearly, all exponential families have the structure (2.132). We always assume
1 ∈L and thereby ensure uniqueness of L. Furthermore, with this assumption we
have dim(E) = dim(L) −1.
Given two points μ,ν ∈P+(I), the m- and e-connections provide two kinds of
straight lines connecting them:
γ (m)
μ,ν : [0,1] →P+(I),
t →(1 −t)μ + t ν,
γ (e)
μ,ν : [0,1] →P+(I),
t →
( dν
dμ)t
μ(( dν
dμ)t)
μ.
This allows us to consider two kinds of geodesically convex sets. A set S is said to
be m-convex if
μ,ν ∈S
⇒
γ (m)
μ,ν (t) ∈S
for all t ∈[0,1],
and e-convex if
μ,ν ∈S
⇒
γ (e)
μ,ν(t) ∈S
for all t ∈[0,1].
Exponential families are clearly e-convex. On the other hand, they can also be
m-convex. Given a partition S of I and probability measures μA with support A,
A ∈S, the following set is an m-convex exponential family:
M := M(μA : A ∈S) :=
 
A∈S
ηA μA : ηA > 0,

A∈S
ηA = 1

.
(2.133)

82
2
Finite Information Geometry
To see this, deﬁne the base measure as μ0 := 	
A∈S μA and L as the linear hull
of the vectors 1A, A ∈S. Then the elements of M(μA : A ∈S) are precisely the
elements of the exponential family E(μ0,L), via the correspondence

A∈S
ηA μA =

A∈S
ηA
	
B∈S ηB
μA
=

A∈S
elogηA
	
B∈S elogηB μA
=

A∈S
eλA
	
B∈S eλB μA
=

A∈S

i∈A
eλA
	
B∈S eλB μi δi
=

i∈I
e
	
A∈S λA1A(i)
	
j∈I e
	
A∈S λA1A(j)μj
μi δi
=
e
	
A∈S λA1A
μ0(e
	
A∈S λA1A)
μ0,
(2.134)
where λA = log(ηA), A ∈S. It turns out that M(μA : A ∈S) is not just one in-
stance of an m-convex exponential family. In fact, as we shall see in the following
theorem, which together with its proof is based on [179], (2.133) describes the gen-
eral structure of such exponential families. Note that for any set S of subsets A of
I and corresponding distributions μA with support A, the set (2.133) will be m-
convex. However, when the sets A ∈S form a partition of I, this set will also be an
exponential family.
Theorem 2.4 Let E = E(μ0,L) be an exponential family in P+(I). Then the fol-
lowing statements are equivalent:
(1) The exponential family E is m-convex.
(2) There exists a partition S ⊆2I of I and elements μA ∈P+(A), A ∈S, such
that
E = M(μA : A ∈S),
(2.135)
where the RHS of this equation is deﬁned by (2.133).
(3) The linear space L is a subalgebra of F(I), i.e., closed under (pointwise) mul-
tiplication.
The proof of this theorem is based on the following lemma.
Lemma 2.6
The smallest convex exponential family containing two probability
measures μ = 	
i∈I μi δi and ν = 	
i∈I νi δi with the supports equal to I coin-

2.8
Exponential Families
83
cides with M(μA : A ∈Sμ,ν) where Sμ,ν is the partition of I having i,j ∈I in the
same block if and only if μiνj = μjνi and μA equals the conditioning of μ to A,
that is,
μA =

i∈I
μA,i δi,
with
μA,i :=

μi
	
j∈A μj ,
if i ∈A,
0
otherwise.
Proof Let Sμ,ν have n blocks and an element iA of A be ﬁxed for each A ∈Sμ,ν.
The numbers μiA
kνiA
−k, A ∈Sμ,ν, 0 ≤k < n, are elements of a Vandermonde
matrix which has nonzero determinant because μiA/νiA, A ∈Sμ,ν, are pairwise
different. Therefore, for 0 ≤k < n the vectors (μiA
kνiA
−k)A∈Sμ,ν are linearly in-
dependent, and so are the vectors (μikνi−k)i∈I . Then the probability measures pro-
portional to (μik+1νi−k)i∈I are independent. These probability measures belong to
any exponential family containing μ and ν and, in turn, their convex hull is con-
tained in any convex exponential family containing μ and ν. In particular, it is
contained in M = M(μA : A ∈Sμ,ν) because μ and ν, the latter being equal to
	
A∈S(	
j∈A νj)μA, belong to M by construction. Since the convex hull has the
same dimension as M, any m-convex exponential family containing μ and ν in-
cludes M.
□
Proof of Theorem 2.4 (1) ⇒(2) Let S be a partition of I with the maximal number
of blocks such that E = E(μ0,L) contains M(μA : A ∈S) for some probability
measures μA. For any probability measure μ with the support equal to I and i ∈A,
j ∈B, belonging to different blocks A,B of S, denote by Hμ,i,j the hyperplane of
vectors (tC)C∈S satisfying
tA · μi μA,j −tB · μj μB,i = 0.
Since no such Hμ,i,j contains the hyperplane given by 	
A∈S tA = 1, a probability
measure ν = 	
A∈S tA μA in M exists such that all equations μiνj = μjνi with i,j
in different blocks of S are simultaneously violated. This implies that each block of
S is a union of blocks of Sμ,ν. If, additionally, μ ∈E then M(μA : A ∈Sμ,ν) is
contained in E on account of Lemma 2.6. By maximality of the number of blocks,
Sμ,ν = S. Hence, μ = 	
A∈S(	
j∈A μj)μA belongs to M, and thus E = M.
(2) ⇒(3) Given the equality (2.135), we can represent E in terms of (2.134). This
implies that L is spanned by the vectors 1A, A ∈S. The linear space L obviously
forms a subalgebra of F(I). This is because the multiplication of two indicator
functions 1A and 1B, where A,B ∈S, equals 1A if A = B, and equals the zero
function otherwise.
(3) ⇒(1) Assume μ,ν ∈E(μ0,L). This means that there exist functions
f,g ∈L with μ =
ef
μ0(ef ) μ0 and ν =
eg
μ0(eg) μ0. Now consider a convex combina-
tion (1 −t)μ + t ν, 0 ≤t ≤1. With f and g, the function h := log((1 −t)
ef
μ0(ef ) +

84
2
Finite Information Geometry
t
eg
μ0(eg)) is also an element of the algebra L. Therefore
(1 −t)μ + t ν = (1 −t)
ef
μ0(ef ) μ0 + t
eg
μ0(eg) μ0 =
eh
μ0(eh) μ0 ∈E(μ0,L). □
2.8.2 Implicit Description of Exponential Families
We have introduced exponential families as afﬁne subspaces with respect to the
translation (2.131). In many applications, however, it is important to consider not
only strictly positive probability measures but also limit points of a given exponen-
tial family. In order to incorporate such distributions, we devote this section to the
study of closures of exponential families, which turns out to be particularly conve-
nient in terms of implicit equations. Classical work on various extensions of expo-
nential families is due to Chentsov [65], Barndorff-Nielsen [39], Lauritzen [159],
and Brown [55]. The theory has been considerably further developed more recently
by Csiszár and F. Matúš [76, 77], going far beyond our context of ﬁnite state spaces.
Implicit equations play an important role within graphical model theory, where
they are related to conditional independence statements and the Hammersley–
Clifford Theorem 2.9 (see [161]). We will address graphical models and their gen-
eralizations, hierarchical models, in Sect. 2.9. The following material is based on
[222] and touches upon the seminal work of Geiger, Meek, and Sturmfels [103].
Let us start with the exponential family itself, without the boundary points. To
this end, consider a reference measure μ0 = 	
i∈I μ0,i δi and a subspace L of F(I)
with 1 ∈L. Throughout this section, we ﬁx a basis f0 := 1, f1, ..., fd, of L, where
d is the dimension of E(μ0,L). Obviously, a probability measure μ is an element
of E(μ0,L) if and only if
vec(μ0,μ) ∈L/R,
which is equivalent to
log
 dμ
dμ0

∈L,
(2.136)
or

log
 dμ
dμ0

,n
 
= 0
for all n ∈L⊥,
(2.137)
where L⊥is the orthogonal complement of L with respect to the canonical scalar
product ⟨·,·⟩on F(I).
Exponentiating both sides of (2.137) yields
0
i
 μi
μ0,i
n(i)
= 1
for all n ∈L⊥.

2.8
Exponential Families
85
Fig. 2.7 Two examples of exponential families. Reproduced from [S. Weis, A. Knauf (2012) En-
tropy distance: New quantum phenomena, Journal of Mathematical Physics 53(10) 102206], with
the permission of AIP Publishing
This is equivalent to
0
i
μin(i) =
0
i
μ0,in(i)
for all n ∈L⊥.
We deﬁne n+ := max{n(i),0} and n−:= max{−n(i),0} and reformulate this con-
dition by
0
i
μin+(i) 0
i
μ0,in−(i) =
0
i
μin−(i) 0
i
μ0,in+(i)
for all n ∈L⊥.
(2.138)
With the abbreviation μn := 7
i μin(i), (2.138) can be written as
μn+μ0n−= μn−μ0n+
for all n ∈L⊥.
(2.139)
This proves that μ ∈E(μ0,L) if and only if (2.139) is satisﬁed. Theorem 2.5 below
states that the same criterion holds also for all elements μ in the closure of E(μ0,L).
Before we come to this result, we ﬁrst have to introduce the notion of a facial set.
Non-empty facial sets are the possible support sets that distributions of a given
exponential family can have. There is an instructive way to characterize them. Given
the basis f0 := 1, f1, ..., fd of L, we consider the afﬁne map
E : P(I) →Rd+1,
μ →

1,μ(f1),...,μ(fd)

.
(2.140)
(Here, μ(fk) = Eμ(fk) denotes the expectation value of fk with respect to μ.)
Obviously, the image of this map is a polytope, the convex support cs(E) of E,
which is the convex hull of the images of the Dirac measures δi in P(I), that is,
E(δi) = (δi(f0),δi(f1),...,δi(fd)) = (1,f1(i),...,fd(i)), i ∈I. The situation is
illustrated in Fig. 2.7 for two examples of two-dimensional exponential families. In
each case, the image of the simplex under the map E, the convex support of E, is
shown as a “shadow” in the horizontal plane, a triangle in one case and a square in
the other case. We observe that the convex supports can be interpreted as “ﬂattened”

86
2
Finite Information Geometry
versions of the individual closures E, where each face of cs(E) corresponds to the
intersection of E with a face of the simplex P(I). Therefore, the faces of the convex
support determine the possible support sets, which we call facial sets. In order to
motivate their deﬁnition below, note that a set C is a face of a polytope P in Rn if
either C = P or C is the intersection of P with an afﬁne hyperplane H such that
all x ∈P , x /∈C, lie on one side of the hyperplane. Non-trivial faces of maximal
dimension are called facets. It is a fundamental result that every polytope can equiv-
alently be described as the convex hull of a ﬁnite set or as a ﬁnite intersection of
closed linear half-spaces (corresponding to its facets) (see [261]).
In particular we are interested in the face structure of cs(E). Since we assumed
that 1 ∈L, the image of E is contained in the afﬁne hyperplane x1 = 1, and we
can replace every afﬁne hyperplane H by an equivalent central hyperplane (which
passes through the origin). For the convex support cs(E), we want to know which
points from E(δi), i ∈I, lie on each face. This motivates the following deﬁnition.
Deﬁnition 2.11 A set F ⊆I is called facial if there exists a vector ϑ ∈Rd+1 such
that
d

k=0
ϑkfk(i) = 0
for all i ∈F ,
d

k=0
ϑkfk(i) ≥1
for all i ∈I \ F .
(2.141)
Lemma 2.7 Fix a subset F ⊆I. Then we have:
(1) F is facial if and only if for any u ∈L⊥:
supp

u+
⊆F
⇔
supp

u−
⊆F
(2.142)
(here, we consider u as an element of F(I), and for any f ∈F(I), supp(f ) :=
{i ∈I : f (i) ̸= 0}).
(2) If μ is a solution to (2.139), then supp(μ) is facial.
Proof One direction of the ﬁrst statement is straightforward: Let u ∈L⊥and sup-
pose that supp(u+) ⊆F . Then

i∈F
u(i)fk(i) = −

i /∈F
u(i)fk(i),
k = 0,1,...,d,
and therefore
0 =

i∈F
u(i)
d

k=0
ϑkfk(i) = −

i /∈F
u(i)
d

k=0
ϑkfk(i).
Since 	d
k=0 ϑkfk(i) > 1 and u(i) ≤0 for i /∈F it follows that u(i) = 0 for i /∈F ,
proving one direction of the ﬁrst statement.
The opposite direction is a bit more complicated. Here, we present a proof us-
ing elementary arguments from polytope theory (see, e.g., [261]). For an alter-
native proof using Farkas’ Lemma see [103]. Assume that F is not facial. Let

2.8
Exponential Families
87
F ′ be the smallest facial set containing F . Let PF and PF ′ be the convex hulls
of {(f0(i),...,fd(i)) : i ∈F} and {(f0(i),...,fd(i)) : i ∈F ′}. Then PF con-
tains a point g from the relative interior of PF ′. Therefore g can be represented
as gk = 	
i∈F α(i)fk(i) = 	
i∈F ′ β(i)fk(i), where α(i) ≥0 for all i ∈F and
β(i) > 0 for all i ∈F ′. Hence u(i) := α(i) −β(i) (where α(i) := 0 for i /∈F
and β(i) := 0 for x /∈F ′) deﬁnes a vector u ∈L⊥such that supp(u+) ⊆F and
supp(u−) ∩(I \ F) = F ′ \ F ̸= ∅.
The second statement now follows immediately: If μ satisﬁes (2.139) for some
u ∈L⊥, then the LHS of (2.139) vanishes if and only if the RHS vanishes, and by
the ﬁrst statement this implies that supp(μ) is facial.
□
Theorem 2.5 A distribution μ is an element of the closure of E(μ0,L) if and only
if it satisﬁes (2.139).
Proof The ﬁrst thing to note is that it is enough to prove the theorem when μ0,i = 1
for all i ∈I. To see this observe that μ ∈E(L) if and only if λ	
i μ0,iμi δi ∈
E(μ0,L), where λ > 0 is a normalizing constant, which does not appear in (2.139)
since they are homogeneous.
Let ZL be the set of solutions of (2.139). The derivation of Eqs. (2.139) was
based on the requirement that E(L) ⊆ZL, which also implies E(L) ⊆ZL = ZL.
It remains to prove the reversed inclusion E(L) ⊇ZL. Let μ ∈ZL \ E(L) and put
F := supp(μ). We construct a sequence μ(n) in E(L) that converges to μ as n →∞.
We claim that the system of equations
d

k=0
bkfk(i) = logμi
for all i ∈F
(2.143)
in the variables bk, k = 0,1,...,d, has a solution. Otherwise we can ﬁnd a function
v(i), i ∈I, such that 	
i∈I v(i)logμi ̸= 0 and 	
i∈I v(i)fk(i) = 0 for all k. This
leads to the contradiction μv+ ̸= μv−. Fix a vector ϑ ∈Rd+1 with property (2.141).
For any n ∈N deﬁne
μ(n) := 1
Z

i
e−n	
k ϑkfk(i)e
	
k bkfk(i) δi ∈E(L),
(2.144)
where Z is a normalization factor. By (2.141) and (2.143) it follows that
limn→∞μ(n) = μ. This proves the theorem.
□
The last statement of Lemma 2.7 can be generalized by the following explicit
description of the closure of an exponential family.
Theorem 2.6 (Closure of an exponential family)
Let L be a linear subspace of
F(I), and let S(L) denote the set of non-empty facial subsets of I (see Deﬁni-
tion 2.11, and Lemma 2.7). Deﬁne for each set F ∈S(L) the truncated exponential

88
2
Finite Information Geometry
family as
EF := EF (μ0,L) :=

1
	
j∈F μj

i∈F
μi δi : μ =

i∈I
μi δi ∈E(μ0,L)

.
(2.145)
Then the closure of the exponential family E is given by
E(μ0,L) =
8
F∈S(L)
EF .
(2.146)
Proof “⊆” Let μ be in the closure of E(μ0,L). Clearly, μ satisﬁes Eqs. (2.139)
and therefore, by Lemma 2.7, its support set F is facial. Furthermore, the same
reasoning that underlies Eq. (2.143) yields a solution of the equations
d

k=0
ϑk fk(i) = log μi
μ0,i
,
i ∈F.
Using these ϑ values for k = 1,...,d, we extend μ by
μi :=
1
Z(ϑ) exp

d

k=1
ϑk fk(i)

,
i ∈I,
to a distribution μ with full support. Obviously, μ deﬁnes μ through truncation.
“⊇” Let μ ∈EF for some non-empty facial set F . Then μ has a representation
μi :=

μ2,i exp(	d
k=0 ϑk fk(i)),
if i ∈F,
0,
otherwise.
With a vector ϑ′ = (ϑ′
0,ϑ′
1,...,ϑd) ∈Rd+1 that satisﬁes (2.141), the sequence
μ(n)
i
:= exp

d

k=0

ϑk −nϑ′
k

fk(i)

∈E(μ0,L)
converges to μ, proving μ ∈E(μ0,L).
□
Example 2.4 (Support sets of an exponential family) In this example, we apply
Theorem 2.6 to the exponential families shown in Fig. 2.7. These are families
of distributions on I = {1,2,3,4}, and we write the elements of F(I) as vectors
(x1,x2,x3,x4). In order to determine the individual facial subsets of I, we use the
criterion given in the ﬁrst part of Lemma 2.7.
(1) Let us start with the exponential family shown in Fig. 2.7(A). The space L of
this exponential family is the linear hull of the orthogonal vectors
(1,1,1,1),
(1,1,−2,0),
(1,−1,0,0).

2.8
Exponential Families
89
Its one-dimensional orthogonal complement L⊥is spanned by the vector
(1,1,1,−3). This implies the following pairs (supp(u+),supp(u−)), u = u+ −
u−∈L⊥, of disjoint support sets:
(∅,∅),

{1,2,3},{4}

,

{4},{1,2,3}

.
(2.147)
The criterion (2.142) for a subset F of I to be a facial set simply means that
for any of the support set pairs (M,N) in (2.147), either M and N are both
contained in F or neither of them is. This yields the following set of facial sets:
∅, {1}, {2}, {3}, {1,2}, {1,3}, {2,3}, {1,2,3,4}.
Obviously, these sets, except ∅, are exactly the support sets of distributions that
are in the closure of the exponential family (see Fig. 2.7(A)).
(2) Now let us come to the exponential family shown in Fig. 2.7(B). Its linear space
L is spanned by
(1,1,1,1),
(1,1,−1,−1),
(1,−1,−1,1),
with the orthogonal complement L⊥spanned by (1,−1,1,−1). As possible
support set pairs, we obtain
(∅,∅),

{1,3},{2,4}

,

{2,4},{1,3}

.
Applying criterion (2.142) ﬁnally yields the facial sets
∅, {1}, {2}, {3}, {4}, {1,2}, {1,4}, {2,3}, {3,4}, {1,2,3,4}.
Also in this example, these sets, except ∅, are the possible support sets of dis-
tributions that are in the closure of the exponential family (see Fig. 2.7(B)).
Theorem 2.5 provides an implicit description of the closure of an exponential
family E(μ0,L). Here, however, we have to test the equations with inﬁnitely many
elements n of the orthogonal complement L⊥of L. Now we ask the question
whether the test can be reduced to a ﬁnite number of vectors n ∈L⊥. For an el-
ement μ ∈E(μ0,L), clearly the orthogonality (2.137) has to be tested only for a
basis n1,...,nc, c = |I| −dim(L), of L, which is equivalent to
μn+
k μ0n−
k = μn−
k μ0n+
k
for all k = 1,...,c.
(2.148)
This criterion is sufﬁcient for the elements of E(μ0,L). It turns out, however, that it
is not sufﬁcient for describing elements in the boundary of E(μ0,L). But it is still
possible to reduce the number of equations to a ﬁnite number. In order to do so,
we have to replace the basis n1,...,nc by a so-called circuit basis, which is still a
generating system but contains in general more than c elements.

90
2
Finite Information Geometry
Deﬁnition 2.12 A circuit vector of the space L is a nonzero vector n ∈L⊥with
inclusion minimal support, i.e., if n′ ∈L⊥satisﬁes supp(n′) ⊆supp(n), then n′ =
λn for some λ ∈R. A circuit is the support set of a circuit vector. A circuit basis is
a subset of L⊥containing precisely one circuit vector for every circuit.
This deﬁnition allows us to prove the following theorem.
Theorem 2.7
Let E(μ0,L) be an exponential family. Then its closure E(μ0,L)
equals the set of all probability distributions that satisfy
μc+μ0c−= μc−μ0c+
for all c ∈C,
(2.149)
where C is a circuit basis of L.
The proof is based on the following two lemmas.
Lemma 2.8 For every vector n ∈L⊥there exists a sign-consistent circuit vector
c ∈L⊥, i.e., if c(i) ̸= 0 ̸= n(i) then signc(i) = signn(i), for all i ∈I.
Proof Let c be a vector with inclusion-minimal support that is sign-consistent with
n and satisﬁes supp(c) ⊆supp(n). If c is not a circuit vector, then there exists a
circuit vector c′ with supp(c′) ⊆supp(c). A suitable linear combination c + αc′,
α ∈R, gives a contradiction to the minimality of c.
□
Lemma 2.9 Every vector n ∈L⊥is a ﬁnite sign-consistent sum of circuit vectors
n = 	r
k=1 ck, i.e., if ck(i) ̸= 0 then signck(i) = signn(i), for all i ∈I.
Proof Use induction on the size of supp(n). In the induction step, use a sign-
consistent circuit vector, as in the last lemma, to reduce the support.
□
Proof of Theorem 2.7 Again, we can assume μ0,i = 1 for all i ∈I. By Theo-
rem 2.5 it sufﬁces to show the following: If μ satisﬁes (2.149), then it also satisﬁes
μn+ = μn−for all n ∈L⊥. Write n = 	r
k=1 ck as a sign-consistent sum of circuit
vectors ck, as in the last lemma. Without loss of generality, we can assume ck ∈C
for all k. Then n+ = 	r
k=1 c+
k and n−= 	r
k=1 c−
k . Hence μ satisﬁes
μn+ −μn−= μ
	r
k=2 c+
k 
μc+
1 −μc−
1 
+

μ
	r
k=2 c+
k −μ
	r
k=2 c−
k 
μc−
1 ,
so the theorem follows easily by induction.
□
Example 2.5 Let I := {1,2,3,4}, and consider the vector space L spanned by the
following two functions (here, we write functions on I as row vectors of length 4):
f0 = (1,1,1,1)
and
f1 = (−α,1,0,0),

2.8
Exponential Families
91
where α /∈{0,1} is arbitrary. This generates a one-dimensional exponential family
E(L). The kernel of L is then spanned by
n1 = (1,α,−1,−α)
and
n2 = (1,α,−α,−1),
but these two vectors do not form a circuit basis: They correspond to the two rela-
tions
μ1μ2α = μ3μ4α
and
μ1μ2α = μ3αμ4.
(2.150)
It follows immediately that
μ3μ4α = μ3αμ4.
(2.151)
If μ3μ4 is not zero, then we conclude that μ3 = μ4. However, on the boundary this
does not follow from Eqs. (2.150): Possible solutions to these equations are given
by
μ(a) = (0,a,0,1 −a)
for 0 ≤a < 1.
(2.152)
However, μ(a) does not lie in the closure of the exponential family E(L), since all
members of E(L) satisfy μ3 = μ4.
A circuit basis of A is given by the vectors
(0,0,1,−1),
(1,α,0,−1 −α),
and
(1,α,−1 −α,0),
which have the following corresponding equations:
μ3 = μ4,
μ1μ2α = μ41+α,
and
μ1μ2α = μ31+α.
(2.153)
By Theorem 2.7, these three equations characterize E(L).
Using arguments from matroid theory, the number of circuits can be shown to
be less than or equal to
 m
d+2

, where m = |I| is the size of the state space and d is
the dimension of E(μ0,L), see [83]. This gives an upper bound on the number of
implicit equations describing E(μ0,L). Note that
 m
d+2

is usually much larger than
the codimension m−d −1 of E(μ0,L) in the probability simplex. In contrast, if we
only want to ﬁnd an implicit description of all probability distributions of E(μ0,L),
which have full support, then m −d −1 equations are enough.
It turns out that even in the boundary the number of equations can be further re-
duced: In general we do not need all circuits for the implicit description of E(μ0,L).
For instance, in Example 2.5, the second and third equation of (2.153) are equivalent
given the ﬁrst one, i.e., we only need two of the three circuits to describe E(μ0,L).
2.8.3 Information Projections
In Sect. 2.7.2 we have introduced the relative entropy. It turns out to be the right
divergence function for projecting probability measures onto exponential families.

92
2
Finite Information Geometry
These projections, referred to as information projections, are closely related to large-
deviation theory and maximum-likelihood estimation in statistics. The foundational
work on information projections is due to Chentsov [65] and Csiszár [75] (see also
the tutorial by Csiszár and Shields [78]). Csiszár and Matúš revisited the classical
theory of information projections within a much more general setting [76]. The
differential-geometric study of these projections and their generalizations based on
dually ﬂat structures (see Sect. 4.3) is due to Amari and Nagaoka [8, 16, 194].
In order to treat the most general case, where probability distributions do not
have to be strictly positive, we have to extend the relative entropy or KL-divergence
(2.101) of Deﬁnition 2.8 so that it is deﬁned for general probability distribu-
tions μ,ν ∈P(I). It turns out that, although DKL is continuous on the prod-
uct P+(I) × P+(I), there is no continuous extension to the Cartesian product
P(I)×P(I). As DKL is used for minimization problems, it is reasonable to consider
the following lower semi-continuous extension of DKL with values in the extended
line R+ := {x ∈R : x ≥0}∪{∞} (considered as a topological space where U ⊆R+
is a neighborhood of ∞if it contains an interval (x,∞)):
DKL(μ∥ν) :=
	
i∈I μi log μi
νi ,
if supp(μ) ⊆supp(ν),
∞,
otherwise.
(2.154)
Here, we use the convention μi log μi
νi = 0 whenever μi = 0. Deﬁning μi log μi
νi to
be ∞if μi > νi = 0 allows us to rewrite (2.154) as DKL(μ∥ν) = 	
i∈I μi log μi
νi .
Whenever appropriate, we use this concise expression.
We summarize the basic properties of this (extended) relative entropy or KL-
divergence.
Proposition 2.14 The function (2.154) satisﬁes the following properties:
(1) DKL(μ∥ν) ≥0, and DKL(μ∥ν) = 0 if and only if μ = ν.
(2) The functions DKL(μ∥·) and DKL(·∥ν) are continuous for all μ,ν ∈P(I).
(3) DKL is lower semi-continuous, that is, for all (μ(k),ν(k)) →(μ,ν), we have
DKL(μ∥ν) ≤liminf
k→∞DKL

μ(k) ν(k)
.
(2.155)
(4) DKL is jointly convex, that is, for all μ(j), ν(j), λj ∈[0,1], j = 1,...,n, satis-
fying 	n
j=1 λj = 1,
DKL

n

j=1
λj μ(j)
n

j=1
λj ν(j)

≤
n

j=1
λj DKL

μ(j) ν(j)
.
(2.156)
The proof of Proposition 2.14 involves the following basic inequality.

2.8
Exponential Families
93
Lemma 2.10 (log-sum inequality)
For arbitrary non-negative real numbers
a1,...,am and b1,...,bm, we have
m

k=1
ak log ak
bk
≥
 m

k=1
ak

log
	m
k=1 ak
	m
k=1 bk
,
(2.157)
where equality holds if and only if ak
bk is independent of k. Here, a log a
b is deﬁned to
be 0 if a = 0 and ∞if a > b = 0.
Proof We set a := 	m
k=1 ak and b := 	m
k=1 bk. With the strict convexity of the
function f : [0,∞) →R, f (x) := x logx for x > 0 and f (0) = 0, we obtain
m

k=1
ak log ak
bk
=
m

k=1
bk
ak
bk
log ak
bk
= b
m

k=1
bk
b f
ak
bk

≥b f
 m

k=1
bk
b
ak
bk

= bf
a
b

= a log a
b .
□
Proof of Proposition 2.14
(1) In the case of probability measures, the log-sum inequality (2.157) implies

i∈I
μi log μi
νi
≥

i∈I
μi

log
	
i∈I μi
	
i∈I νi
= 0,
where equality holds if and only if μi = c νi for some constant c, which, in this
case, has to be equal to one.
(2) This follows directly from the continuity of the functions logx, where
log0 := −∞, and x logx, where 0log0 := 0.
(3) If νi > 0, we have
lim
k→∞μ(k)
i
log μ(k)
i
ν(k)
i
= μi log μi
νi
.
(2.158)
If νi = 0 and μi > 0,
lim
k→∞μ(k)
i
log μ(k)
i
ν(k)
i
= ∞= μi log μi
νi
.
(2.159)
Finally, if νi = 0 and μi = 0,
liminf
k→∞μ(k)
i
log μ(k)
i
ν(k)
i
≥liminf
k→∞

μ(k)
i
−ν(k)
i

= 0 = μi log μi
νi
,
(2.160)

94
2
Finite Information Geometry
since logx ≥1 −1
x for x > 0. Altogether, (2.158), (2.159), and (2.160) imply
liminf
k→∞

i∈I
μ(k)
i
log μ(k)
i
ν(k)
i
≥

i∈I
liminf
k→∞μ(k)
i
log μ(k)
i
ν(k)
i
≥

i∈I
μi log μi
νi
,
which equals ∞whenever there is at least one i ∈I satisfying μi > νi = 0.
(4) We use again the log-sum inequality (2.157):
DKL

n

j=1
λj μ(j)
n

j=1
λj ν(j)

=

i∈I

n

j=1
λj μ(j)
i

log
	n
j=1 λj μ(j)
i
	n
j=1 λj ν(j)
i
≤

i∈I
n

j=1
λj μ(j)
i
log λj μ(j)
i
λj ν(j)
i
=
n

j=1
λj

i∈I
μ(j)
i
log μ(j)
i
ν(j)
i
=
n

j=1
λj DKL

μ(j) ν(j)
.
□
We consider information projections onto exponential and corresponding mix-
ture families. They are assigned to a linear subspace L of F(I) and measures
μ1 ∈P(I), μ2 ∈P+(I). Without loss of generality, we assume 1 ∈L and choose a
basis f0 := 1,f1,...,fd of L. The mixture family through μ1 is simply the set of
distributions that have the same expectation values of the fk as the distribution μ1,
that is,
M := M(μ1,L) :=

ν ∈P(I) : ν(fk) = μ1(fk),k = 1,...,d

.
(2.161)
The corresponding exponential family E := E(μ2,L) through μ2 is given as the
image of the parametrization
ϑ = (ϑ1,...,ϑd) →
1
Z(ϑ)

i∈I
μ2,i exp

d

k=1
ϑk fk(i)

δi,
(2.162)
where
Z(ϑ) :=

j
μ2,j exp

d

k=1
ϑk fk(j)

.
Theorem 2.8 For any distribution ˆμ ∈P(I), the following statements are equiva-
lent:
(1) ˆμ ∈M ∩E.

2.8
Exponential Families
95
(2) For
all
ν1 ∈M,
ν2 ∈E:
DKL(ν1 ∥ˆμ) < ∞,
DKL(ν1 ∥ν2) < ∞
iff
DKL( ˆμ∥ν2) < ∞, and
DKL(ν1 ∥ν2) = DKL(ν1 ∥ˆμ) + DKL( ˆμ∥ν2).
(2.163)
In particular, the intersection M ∩E consists of the single point ˆμ.
(3) ˆμ ∈M, and DKL( ˆμ∥ν2) = infν∈M DKL(ν ∥ν2) for all ν2 ∈E.
(4) ˆμ ∈E, and DKL(ν1 ∥ˆμ) = infν∈E DKL(ν1 ∥ν) for all ν1 ∈M.
Furthermore, there exists a unique distribution ˆμ that satisﬁes one and therefore all
of these conditions.
Proof (1) ⇒(2) We choose ν1 ∈M and ν2 ∈E (strict positivity). As ˆμ ∈E, there
is a sequence μ(n) ∈E, μ(n) →ˆμ. This implies

i∈I
(ν1,i −ˆμi)log μ(n)
i
ν2,i
= 0.
(2.164)
This is because log dμ(n)
dν2 ∈L, and ν1, ˆμ ∈M. By continuity,

i∈I
(ν1,i −ˆμi)log ˆμi
ν2,i
= 0.
(2.165)
This equality is equivalent to
DKL(ν1 ∥ν2) = DKL(ν1 ∥ˆμ) + DKL( ˆμ∥ν2).
(2.166)
As we assumed ν2 to be strictly positive, this means that DKL(ν1 ∥ν2) and
DKL( ˆμ∥ν2) are ﬁnite, so that DKL(ν1 ∥ˆμ) has to be ﬁnite. This is only the case
if supp( ˆμ) ⊇supp(ν1) for all ν1 ∈M. By continuity, (2.166) also holds for ν2 ∈E.
Note, however, that we do not exclude the case where DKL(ν1 ∥ν2) and DKL( ˆμ∥ν2)
become inﬁnite when ν2 does not have full support. We ﬁnally prove unique-
ness: Assume ˆμ′ ∈M ∩E. Then the Pythagorean relation (2.163) implies for
ν1 = ν2 = ˆμ′
0 = DKL(ν1 ∥ν2) = DKL(ν1 ∥ˆμ) + DKL( ˆμ∥ν2) = DKL

ˆμ′  ˆμ

+ DKL

ˆμ
 ˆμ′
,
and therefore ˆμ′ = ˆμ, that is, M ∩E = { ˆμ}.
(2) ⇒(3) For all ν1 ∈M and ν2 ∈E, we obtain
DKL(ν1 ∥ν2) = DKL(ν1 ∥ˆμ) + DKL( ˆμ∥ν2) ≥DKL( ˆμ∥ν2) ≥inf
ν∈MDKL(ν ∥ν2).
This implies
inf
ν1∈MDKL(ν1 ∥ν2) ≥DKL( ˆμ∥ν2) ≥inf
ν∈MDKL(ν ∥ν2).

96
2
Finite Information Geometry
(2) ⇒(4) For all ν1 ∈M and ν2 ∈E, we obtain
DKL(ν1 ∥ν2) = DKL(ν1 ∥ˆμ) + DKL( ˆμ∥ν2) ≥DKL(ν1 ∥ˆμ) ≥inf
ν∈E
DKL(ν1 ∥ν).
This implies
inf
ν2∈E
DKL(ν1 ∥ν2) ≥DKL(ν1 ∥ˆμ) ≥inf
ν∈E
DKL(ν1 ∥ν).
(3) ⇒(1) We consider the KL-divergence for ν2 = μ2, the base measure of the
exponential family E = E(μ2,L) which we have assumed to be strictly positive:
M →R,
ν →DKL(ν ∥μ2).
(2.167)
This function is strictly convex and therefore has ˆμ ∈M as its unique minimizer. In
what follows, we prove that ˆμ is contained in the (relative) interior of M so that we
can derive a necessary condition for ˆμ using the method of Lagrange multipliers.
This necessary condition then implies ˆμ ∈E. We structure this chain of arguments
in three steps:
Step 1: Deﬁne the curve [0,1] →M, t →ν(t) := (1 −t) ˆμ + t ν, and consider
its derivative
d
dt DKL

ν(t)
μ2

t=t0
=

i∈I
(νi −ˆμi)log νi(t0)
μ2,i
(2.168)
for t0 ∈(0,1). If ˆμi = 0 for some i with νi > 0 then the derivative (2.168) converges
to −∞when t0 →0. As ˆμ is the minimizer of DKL(ν(·)∥μ2), this is ruled out,
proving
supp(ν) ⊆supp( ˆμ),
for all ν ∈M.
(2.169)
Step 2: Let us now consider the particular situation where M has a non-empty
intersection with P+(I). This is obviously the case, when we choose μ1 to be
strictly positive, as μ1 ∈M = M(μ1,L) by deﬁnition. In that case supp( ˆμ) = I, by
(2.169). We consider the restriction of the function (2.167) to M∩P+(I) and intro-
duce Lagrange multipliers ϑ0,ϑ1,...,ϑd, in order to obtain a necessary condition
for ˆμ to be its minimizer. More precisely, differentiating

i∈I
νi log νi
μ2,i
−ϑ0

1 −

i∈I
νi

−
d

k=1
ϑk

μ1(fk) −

i∈I
νifk(i)

(2.170)
with respect to νi leads to the necessary condition
logνi + 1 −logμ2,i −ϑ0 −

k
ϑkfk(i) = 0,
i ∈I,

2.8
Exponential Families
97
which is equivalent to
νi = μ2,i exp

ϑ0 −1 +
d

k=1
ϑkfk(i)

,
i ∈I.
(2.171)
As the minimizer, ˆμ has this structure and is therefore contained in E, proving ˆμ ∈
M ∩E.
Step 3: In this ﬁnal step, we drop the assumption that μ1 is strictly positive and
consider the sequence
M(n) := M

μ(n)
1 ,L

,
(2.172)
where μ(n)
1
= (1 −1
n)μ1 + 1
n μ2, n ∈N. Each of these distributions μ(n)
1
is strictly
positive so that, according to Step 2, we have a corresponding sequence ˆμ(n) of
distributions in M(n) ∩E. The limit of any convergent subsequent is an element of
M ∩E and, by uniqueness, coincides with ˆμ.
(4) ⇒(1) Deﬁne S := supp( ˆμ). Then ˆμ is contained in the family ES (see Theo-
rem 2.6), deﬁned in terms of the parametrization
νi(ϑ) := νi(ϑ1,...,ϑd) :=

1
ZS(ϑ) μ2,i exp(	d
k=1 ϑk fk(i)),
if i ∈S,
0,
otherwise,
with
ZS(ϑ) :=

j∈S
μ2,j exp

d

k=1
ϑk fk(j)

.
Note that ν( ˆϑ) = ˆμ for some ˆϑ. With this parametrization, we obtain the function
DKL

ν1
ν(ϑ)

= DKL(ν1 ∥μ2) −
d

k=1
ϑk ν1(fk) + log

ZS(ϑ)

,
and its partial derivatives
∂DKL(ν1 ∥ν(·))
∂ϑk
= ν(ϑ)(fk) −ν1(fk),
k = 1,...,d.
(2.173)
As ˆμ = ν( ˆϑ) is the minimizer, ˆϑ satisﬁes Eqs. (2.173), which implies that ˆμ is
contained in M.
Existence: We proved the equivalence of the conditions for any distribution ˆμ,
which, in particular, implies the uniqueness of a distribution that satisﬁes one and
therefore all of these conditions. To see that there exists such a distribution, consider
the function (2.167) and observe that it has a unique minimizer ˆμ ∈M ∩E (see the
proof of the implication “(3) ⇒(1)”).
□

98
2
Finite Information Geometry
Let us now use Theorem 2.8 in order to deﬁne projections onto mixture and
exponential families, referred to as the I-projection and rI-projection, respectively
(see [76, 78]).
Let us begin with the I-projection. Consider a mixture family M = M(μ1,L)
as deﬁned by (2.161). Following the criterion (3) of Theorem 2.8, we deﬁne the
distance from M by
DKL(M∥·) : P(I) →R,
μ →DKL(M∥μ) := inf
ν∈MDKL(ν ∥μ).
(2.174)
Theorem 2.8 implies that there is a unique point ˆμ ∈M that satisﬁes DKL( ˆμ∥μ) =
DKL(M∥μ). It is obtained as the intersection of M(μ1,L) with the closure of
the exponential family E(μ,L). This allows us to deﬁne the I-projection πM :
P(I) →M, μ →ˆμ.
Now let us come to the analogous deﬁnition of the rI-projection, which will play
an important role in Sect. 6.1. Consider an exponential family E = E(μ2,L), and,
following criterion (4) of Theorem 2.8, deﬁne the distance from E by
DKL(·∥E) : P(I) →R,
μ →DKL(μ∥E) := inf
ν∈E DKL(μ∥ν) = inf
ν∈E
DKL(μ∥ν),
(2.175)
where the last equality follows from the continuity of DKL(μ∥·). Theorem 2.8 im-
plies that there is a unique point ˆμ ∈E that satisﬁes DKL(μ∥ˆμ) = DKL(μ∥E). It
is obtained as the intersection of the closure of E(μ2,L) with the mixture family
M(μ,L). This allows us to deﬁne the projection πE : P(I) →E, μ →ˆμ.
Proposition 2.15
Both information distances, DKL(M∥·) and DKL(·∥E), are
continuous functions on P(I).
Proof We prove the continuity of DKL(M∥·). One can prove the continuity of
DKL(·∥E) following the same reasoning.
Let μ be a point in P(I) and μn ∈P(I), n ∈N, a sequence that converges to μ.
For all ν ∈M, we have DKL(M∥μn) ≤DKL(ν ∥μn), n ∈N, and by the continuity
of DKL(ν ∥·) we obtain
limsup
n→∞
DKL(M∥μn) ≤limsup
n→∞
DKL(ν ∥μn) = lim
n→∞DKL(ν ∥μn) = DKL(ν ∥μ).
(2.176)
From the lower semi-continuity of the KL-divergence DKL (Lemma 2.14), we ob-
tain the lower semi-continuity of the distance DKL(M∥·) (see [228]). Taking the
inﬁmum of the RHS of (2.176) then leads to
limsup
n→∞
DKL(M∥μn) ≤inf
ν∈MDKL(ν ∥μ) = DKL(M∥μ) ≤liminf
n→∞DKL(M∥μn),
(2.177)
proving limn→∞DKL(M∥μn) = DKL(M∥μ).
□
In Sects. 2.9 and 6.1, we shall study exponential families that contain the uniform
distribution, say μ0,i = 1
|I|, i ∈I. In that case, the projection ˆμ = πE(μ) coincides

2.8
Exponential Families
99
with the so-called maximum entropy estimate of μ. To be more precise, let us con-
sider the function
ν →DKL(ν ∥μ0) = log|I| −H(ν),
(2.178)
where
H(ν) := −

i∈I
νi logνi.
(2.179)
The function H(ν) is the basic quantity of Shannon’s theory of information [235],
and it is known as the Shannon entropy or simply the entropy. It is continuous and
strictly concave, because the function f : [0,∞) →R, f (x) := x logx for x > 0
and f (0) = 0, is continuous and strictly convex. Therefore, H assumes its maxi-
mal value in a unique point, subject to any linear constraint. Clearly, ν minimizes
(2.178) in a linear family M if and only if it maximizes the entropy on M. There-
fore, assuming that the uniform distribution is an element of E, the distribution ˆμ of
Theorem 2.8 is the one that maximizes the entropy, given the linear constraints of
the set M. This relates the information projection to the maximum entropy method,
which has been proposed by Jaynes [128, 129] as a general inference method, based
on statistical mechanics and information theory. In order to motivate this method,
let us brieﬂy elaborate on the information-theoretic interpretation of the entropy as
an information gain, due to Shannon [235]. We assume that the probability measure
ν represents our expectation about the outcome of a random experiment. In this in-
terpretation, the larger νi is the higher our conﬁdence that the events i will be the
outcome of the experiment. With expectations, there is always associated a surprise.
If an event i is not expected prior to the experiment, that is, if νi is small, then it
should be surprising to observe it as the outcome of the experiment. If that event,
on the other hand, is expected to occur with high probability, that is, if νi is large,
then it should not be surprising at all to observe it as the experiment’s outcome. It
turns out that the right measure of surprise is given by the function νi →−logνi.
This function quantiﬁes the extent to which one is surprised by the outcome of an
event i ∈I, if i was expected to occur with probability νi. The entropy of ν is the
expected (or mean) surprise and is therefore a measure of the subjective uncertainty
about the outcome of the experiment. The higher that uncertainty, the less informa-
tion is contained in the probability distribution about the outcome of the experiment.
As the uncertainty about this outcome is reduced to zero after having observed the
outcome, this uncertainty reduction can be interpreted as information gain through
the experiment. In his inﬂuential work [235], Shannon provided an axiomatic char-
acterization of information based on this intuition.
The interpretation of entropy as uncertainty allows us to interpret the distance be-
tween μ and its maximum entropy estimate ˆμ = πE(μ) as reduction of uncertainty.
Lemma 2.11 Let ˆμ be the maximum entropy estimate of μ. Then
DKL(μ∥E) = DKL(μ∥ˆμ) = H( ˆμ) −H(μ).
(2.180)

100
2
Finite Information Geometry
Proof It follows from (2.163) that
DKL(μ∥μ0) = DKL(μ∥ˆμ) + DKL( ˆμ∥μ0).
If we choose μ0 to be the uniform distribution, this amounts to

log|I| −H(μ)

= DKL(μ∥ˆμ) +

log|I| −H( ˆμ)

.
□
This will be further explored in Sect. 6.1.2. See also the discussion in Sect. 4.3
of the duality between exponential and mixture families.
2.9 Hierarchical and Graphical Models
We have derived and studied exponential families E(μ0,L) from a purely geometric
perspective (see Deﬁnition 2.10). On the other hand, they naturally appear in statis-
tical physics, known as families of Boltzmann–Gibbs distributions. In that context,
there is an energy function which we consider to be an element of a linear space L.
One typically considers a number of particles that interact with each other so that the
energy is decomposed into a family of interaction terms, the interaction potential.
The strength of interaction, for instance, then parametrizes the space L of energies,
and one can study how particular system properties change as result of a parame-
ter change. This mechanistic description of a system consisting of interacting units
has inspired corresponding models in many other ﬁelds, such as the ﬁeld of neural
networks, genetics, economics, etc.
It is remarkable that purely geometric information about the system can reveal
relevant features of the physical system. For instance, the Riemannian curvature
with respect to the Fisher metric can help us to detect critical parameter values
where a phase transition occurs [54, 218]. Furthermore, the Gibbs–Markov equiv-
alence in statistical physics [191], the equivalence of particular mechanistic and
phenomenological properties of a system, can be interpreted as an equivalence of
two perspectives of the same geometric object, its explicit parametrization and its
implicit description as the solution set of corresponding equations. This close con-
nection between geometry and physical systems allows us to assign to the developed
geometry a mechanistic interpretation.
In this section we want to present a more reﬁned view of exponential families that
are naturally deﬁned for systems of interacting units, so-called hierarchical mod-
els. Of particular interest are graphical models, where the interaction is compatible
with a graph. For these models, the Gibbs–Markov equivalence is then stated by the
Hammersley–Clifford theorem. The material of this section is mainly based on Lau-
ritzen’s monograph [161] on graphical models. However, we shall conﬁne ourselves
to discrete models with the counting measure as the base measure. The next section
on interaction spaces incorporates work of Darroch and Speed [80].

2.9
Hierarchical and Graphical Models
101
2.9.1 Interaction Spaces
Consider a ﬁnite set V of units or nodes. To simplify the notation we sometimes
choose V to be the set [N] = {1,...,N}. We assign to each node a correspond-
ing (non-empty and ﬁnite) set of conﬁgurations Iv. For every subset A ⊆V , the
conﬁgurations on A are given by the Cartesian product
IA :=×
v∈A
Iv.
(2.181)
Note that in the case where A is the empty set, the product space consists of the
empty sequence ϵ, that is, I∅= {ϵ}. We have the natural projections
XA : IV →IA,
(iv)v∈V →(iv)v∈A.
(2.182)
Given a distribution p ∈P(I), the XA become random variables and we denote the
XA-image of p by pA and use the shorthand notation
p(iA) := pA(iA) =

iV \A
p(iA,iV \A),
iA ∈IA.
(2.183)
Given an iA with p(iA) > 0, we deﬁne
p(iB|iA) := p(iA,iB)
p(iA)
.
(2.184)
By FA we denote the algebra of real-valued functions f ∈F(IV ) that only de-
pend on A, that is, the image of the algebra homomorphism F(IA) →F(IV ),
g →g ◦XA (see the ﬁrst paragraph of Sect. 2.1). This is called the space of A-
interactions. Clearly this space has dimension 7
v∈A |Iv|. Note that we recover the
one-dimensional space of constant functions on IV for A = ∅.
We consider the canonical scalar product on FV = F(IV ), deﬁned by ⟨f,g⟩:=
	
i f igi, which coincides with the scalar product (2.10) for the counting measure
μ = 	
i δi. With the A-marginal
f (iA) :=

i′
V \A
f

iA,i′
V \A

,
iA ∈IA,
(2.185)
of a function f ∈FV , the orthogonal projection PA onto FA with respect to ⟨·,·⟩
has the following form.
Proposition 2.16
PA(f )(iA,iV \A) = f (iA)
|IV \A|,
iA ∈IA, iV \I ∈IV \I.
(2.186)

102
2
Finite Information Geometry
(Note that, according to our convention, |I∅| = 1.)
Proof We have to show

f −PA(f ),g

= 0
for all g ∈FA.

f −PA(f ),g

= ⟨f,g⟩−

PA(f ),g

=

iA,iV \A
f (iA,iV \A)g

iA,i′
V \A

−

iA,iV \A
f (iA)
|IV \A| g

iA,i′
V \A

=

iA
g

iA,i′
V \A
 
iV \A
f (iA,iV \A)
(
)*
+
=f (iA)
−

iA
f (iA)
|IV \A|

iV \A
g

iA,i′
V \A

=

iA
g

iA,i′
V \A

f (iA) −

iA
f (iA)
|IV \A| |IV \A|g

iA,i′
V \A

= 0.
□
Note that
PAPB = PBPA = PA∩B
for all A,B ⊆V .
(2.187)
We now come to the notion of pure interactions. The vector space of pure A-
interactions is deﬁned as

FA := FA ∩
 9
B⊊A
FB⊥

.
(2.188)
Here, the orthogonal complements are taken with respect to the scalar product ⟨·,·⟩.
The space 
FA consists of functions that depend on arguments in A but not only on
arguments of a proper subset B of A. Obviously, the following holds:
f ∈
FA
⇔
f ∈FA and f (iB) = 0 for all B ⊊A and iB ∈IB,
(2.189)
where f (iB) is deﬁned by (2.185). We denote the orthogonal projection of f onto

FA by 
PA. The following holds:
PA 
PB =
 
PB,
if B ⊆A,
0,
otherwise.
(2.190)
The ﬁrst case is obvious. The second case follows from (2.189) (and B ⊈A ⇒
A ∩B ⊊B):
PA 
PB = PAPB 
PB = PA∩B 
PB = 0.

2.9
Hierarchical and Graphical Models
103
Proposition 2.17
(1) The spaces 
FA, A ⊆V , of pure interactions are mutually orthogonal.
(2) For all A ⊆V , we have

PA =

B⊆A
(−1)|A\B|PB,
PA =

B⊆A

PB.
(2.191)
(3) The space of A-interactions, A ⊆V , has the following orthogonal decomposi-
tion into spaces of pure interactions:
FA =
:
B⊆A

FB.
(2.192)
(4) For A ⊆V , the dimension of the space of pure A-interactions is given by
dim(
FA) =

B⊆A
(−1)|A\B| 0
v∈B
|Iv| =
0
v∈A

|Iv| −1

.
(2.193)
For the proof of Proposition 2.17 we need the Möbius inversion formula, which
we state and prove ﬁrst.
Lemma 2.12 (Möbius inversion)
Let Ψ and Φ be functions deﬁned on the set of
subsets of a ﬁnite set V , taking values in an Abelian group. Then the following
statements are equivalent:
(1) For all A ⊆V, Ψ (A) = 	
B⊆A Φ(B).
(2) For all A ⊆V, Φ(A) = 	
B⊆A(−1)|A\B|Ψ (B).
Proof

B⊆A
Φ(B) =

B⊆A

D⊆B
(−1)|B\D|Ψ (D)
=

D⊆A,C⊆A\D
(−1)|C|Ψ (D)
=

D⊆A
Ψ (D)

C⊆A\D
(−1)|C|
= Ψ (A).
The last equality results from the fact that the inner sum equals 1, if A \ D = ∅. In
the case A \ D ̸= ∅we set n := |A \ D| and get

C⊆A\D
(−1)|C| =
n

k=1

C ⊆A \ D : |C| = k
(−1)k
=
n

k=0
n
k

(−1)k

104
2
Finite Information Geometry
= (1 −1)n
= 0.
For the proof of the second implication, we use the same arguments:

B⊆A
(−1)|A\B|Ψ (B) =

D⊆B⊆A
(−1)|A\B|Φ(D)
=

D⊆A
Φ(D)

C⊆A\D
(−1)|C|
= Φ(A).
□
The Möbius inversion formula is of independent interest within discrete mathe-
matics and has various generalizations (see [1]). We now come to the proof of the
above proposition.
Proof of Proposition 2.17
(1) First observe that
9
B⊊A
FB⊥=
9
v∈A
FA\{v}⊥.
(2.194)
Here, the inclusion “⊆” follows from the corresponding inclusion of the index
sets: {A \ {v} : v ∈A} ⊆{B ⊆A : B ̸= A}. The opposite inclusion “⊇” follows
from the fact that any B ⊊A is contained in some A \ {v}, which implies FB ⊆
FA\{v} and therefore FB⊥⊇FA\{v}⊥.
From (2.194) we obtain

FA = FA ∩
9
B⊊A
FB⊥= FA ∩
9
v∈A
FA\{v}⊥,
and therefore

PA = PA
0
v∈A
(idFV −PA\{v}).
(2.195)
This implies that 
PA and PB commute. As a consequence, we derive

PA 
PB = PA 
PA 
PB = 
PAPA 
PB = 0
if A ̸= B.
The last equality follows from PA 
PB = 0 according to (2.190), where A ̸= B
implies A ⊈B or B ⊈A. This yields

PA 
PB = 
PB 
PA
if A ̸= B,
and therefore the spaces 
FA, A ⊆V , are mutually orthogonal.

2.9
Hierarchical and Graphical Models
105
(2) We use (2.195):

PA = PA
0
v∈A
(idFV −PA\{v})
= PA

B⊆A
(−1)|B| 0
v∈B
PA\{v}
(by direct multiplication)
= PA

B⊆A
(−1)|B|PA\B

iteration of (2.187), together with
9
v∈B

A \ {v}

= A \ B

= PA

B⊆A
(−1)|A\B|PB
(change of summation index)
=

B⊆A
(−1)|A\B|PAPB
=

B⊆A
(−1)|A\B|PB.
(FB subspace of FA)
This proves the ﬁrst part of the statement. For the second part, we use the
Möbius inversion of Lemma 2.12. It implies
PA =

B⊆A

PB,
which is the second part of the statement.
(3) The inclusion “⊇” is clear. We prove the opposite inclusion “⊆”:
f ∈FA
⇒
f = PA(f ) =

B⊆A

PB(f )
( )* +
∈
FB
∈
:
B⊆A

FB.
(4) From (2.192) we know
dim(FA) =

B⊆A
dim(
FB),
A ⊆V.
The Möbius inversion formula implies
dim(
FA) =

B⊆A
(−1)|A\B|dim(FB)
=

B⊆A
(−1)|A\B| 0
v∈B
|Iv|
=
0
v∈A

|Iv| −1

.
□

106
2
Finite Information Geometry
In the remaining part of this section, we concentrate on binary nodes v ∈V with
state spaces Iv = {0,1} for all v ∈V . In this case, by (2.193),
dim(
FA) =
0
v∈A

|Iv| −1

= 1.
We are now going to deﬁne a family of vectors eA : IV = {0,1}V →R that span the
individual spaces 
FA and thereby form an orthogonal basis of FV . In order to do so,
we interpret the symbols 0 and 1 as the elements of the group Z2, with group law
determined by 1+1 = 0. (Below, we shall interpret 0 and 1 as elements of R, which
will then lead to a different basis of FV .) The set of group homomorphisms from
the product group IV = Z2V into the unit circle of the complex plane forms a group
with respect to pointwise multiplication, called the character group. The elements
of that group, the characters of IV , can be easily speciﬁed in our setting. For each
v ∈V , we ﬁrst deﬁne the function
ξv : IV = {0,1}V →{−1,1},
ξv(i) := (−1)Xv(i) =

1,
if Xv(i) = 0,
−1,
if Xv(i) = 1.
The characters of IV are then given by the real-valued functions
eA(i) :=
0
v∈A
ξv(i),
A ⊆V.
(2.196)
With E(A,i) := |{v ∈A : Xv(i) = 1}|, we can rewrite (2.196) as
eA(i) = (−1)E(A,i),
A ⊆V.
(2.197)
These vectors are known as Walsh vectors [127, 253] and are used in various ap-
plications. In particular, they play an important role within Holland’s genetic algo-
rithms [110, 166]. If follows from general character theory (see [120, 158]) that the
eA form an orthogonal basis of the real vector space FV . We provide a more direct
derivation of this result.
Proposition 2.18 (Walsh basis) The vector eA spans the one-dimensional vector
space 
FA of pure A-interactions. In particular, the family eA, A ⊆V , of Walsh
vectors forms an orthogonal basis of FV .
Proof For A = ∅, we have e∅= 	
i 1 · ei = 1 which spans 
F∅.
Now let A ̸= ∅, and observe
f ∈
FA
⇔
f ∈FA and PB(f ) = 0 for all B ⊊A.
(2.198)
Below, we verify (2.198) by using

i∈{0,1}V
(−1)E(A,i) = 0.
(2.199)

2.9
Hierarchical and Graphical Models
107
To see this, let v be an element of A and deﬁne
I−:=

i : Xv(i) = 1

,
I+ :=

i : Xv(i) = 0

.
Obviously, E(A,i) = E(A \ {v},i) if i ∈I+. This implies (2.199):

i
(−1)E(A,i) =

i∈I+
(−1)E(A\{v},i) −

i∈I−
(−1)E(A\{v},i) = 0.
Now we verify (2.198). For eA ∈FA we have
PA(eA)(iA,iV \A) =
1
2|V \A|

i′
V \A∈IV \A
eA

iA,i′
V \A

= eA(iA,iV \A),
which follows from the fact that E(A,i) does not depend on iV \A. Furthermore,
PB(eA) = 0 for B ⊊A:
PB(eA)(iB,iV \B)
=
1
2|V \B|

i′
V \B
eA(iB,i′
V \B)
=
1
2|V \B|

i′
V \B
(−1)E(A,(iB,i′
V \B))
=
1
2|V \B|

i′
V \B
(−1){E(A,(iB,iV \B))+E(A,(iB,i′
V \B))}
=
1
2|V \B| (−1)E(B,(iB,iV \B)) · 2|V \B| ·

i′
V \B
(−1)E(A\B,(iB,i′
V \B))
(
)*
+
=0,
according to (2.199), since A \ B ̸= ∅
= 0.
□
Remark 2.3 (Characters of ﬁnite groups for the non-binary case) Assuming that
each set Iv has the structure of the group Znv = Z/nvZ, that is, Iv = {0,1,...,
nv −1}, nv = |Iv|, we denote its nv characters by
χv,iv : Iv →C,
jv →χv,iv(jv) := exp

i 2π iv jv
nv

.

108
2
Finite Information Geometry
(Here, i denotes the imaginary number in C.) We can write any function f : IV →R
on the Abelian group IV =×v∈V Znv uniquely in the form
f =

i=(iv)v∈V ∈IV
ϑi
0
v∈V
χv,iv
with complex coefﬁcients ϑi satisfying ϑ−i = ¯ϑi (the bar denotes the complex con-
jugation). This allows us to decompose f into a unique sum of l-body interactions

A⊆V,|A|=l

i=(iv)v∈V ∈IV
iv̸=0 iff v∈A
ϑi
0
v∈A
χv,iv.
In many applications, functions are decomposed as
f =

A⊆V
fA,
for suitable
fA ∈FA,A ⊆V,
(2.200)
where the family fA, A ⊆V , of functions is referred to as the interaction poten-
tial. However, it is not always natural to assume that the fA are elements of the
spaces 
FA, A ⊆V . One frequently used way of decomposing f assumes for each
node v ∈V a distinguished state ov ∈Iv, the so-called vacuum state. Having this
state, one requires that the family (fA)A⊆V is normalized in the following sense:
(i)
f∅= 0,and
(ii)
fA(i) = 0 if there is a v ∈A satisfying Xv(i) = ov.
(2.201)
With these requirements, the decomposition (2.200) is unique and can be obtained in
terms of the Möbius inversion (Lemma 2.12, for details see [258], Theorem 3.3.3).
If we work with binary values 0 and 1, interpreted as elements in R, it is natural
to set ov = 0 for all v. In that case, any function f can be uniquely decomposed as
f =

A⊆V
ϑA
0
v∈A
Xv.
(2.202)
Obviously, fA = ϑA
7
v∈A Xv ∈FA and the conditions (2.201) are satisﬁed. Note
that, despite the similarity between the monomials 7
v∈A Xv and those deﬁned
by (2.196), the decomposition (2.202) is not an orthogonal one, and, for A ̸= ∅,
7
v∈A Xv /∈
FA. Clearly, by rewriting ξv = (−1)Xv as 1 −2Xv (where the values of
Xv are now interpreted as real numbers), we can transform one representation of a
function into the other.
2.9.2 Hierarchical Models
We are now going to describe the structure of the interactions in a system. This
structure sets constraints on the interaction potentials and the corresponding Gibbs

2.9
Hierarchical and Graphical Models
109
distributions. Although pairwise interactions are most commonly used in applica-
tions, which is associated with the edges of a graph, we have to incorporate, in
particular, higher-order interactions by considering generalizations of graphs.
Deﬁnition 2.13 (Hypergraph)
Let V be a ﬁnite set, and let S be a set of subsets
of V . We call the pair (V,S) a hypergraph and the elements of S hyperedges.
(Note that, at this point, we do not exclude the cases S = ∅and S = {∅}.) When V
is ﬁxed we usually refer to the hypergraph only by S. A hypergraph S is a simplicial
complex if it satisﬁes
A ∈S,B ⊆A
⇒
B ∈S.
(2.203)
We denote the set of all inclusion maximal elements of a hypergraph S by Smax.
A simplicial complex S is determined by Smax. Finally, we can extend any hy-
pergraph S to the simplicial complex S by including any subset A of V that is
contained in a set B ∈S.
For a hypergraph S, we consider the sum
FS :=

A∈S
FA.
(2.204)
Note that for S = ∅, this space is zero-dimensional and consists of the constant
function f ≡0. (This follows from the usual convention that the empty sum equals
zero.) For S = {∅}, we have the one-dimensional space of constant functions on IV .
In order to evaluate the dimension of FS in the general case, we extend S to the
simplicial complex S and represent the vector space as the inner sum of the orthog-
onal sub-spaces 
FA (see Proposition 2.17),
FS =
:
A∈S

FA,
(2.205)
which directly implies
dim(FS) =

A∈S
0
v∈A

|Iv| −1

.
(2.206)
For the particular case of binary nodes we obtain the number |S| of hyperedges of
the simplicial complex S as the dimension of FS.
Deﬁnition 2.14 (Hierarchical model) For a hypergraph S, we deﬁne a hierarchical
model as the exponential family generated by the vector space FS (see Deﬁni-
tion 2.10):
ES := E(FS) =
 
i∈IV
ef (i)
	
i′∈IV ef (i′) δi : f ∈FS

.
(2.207)

110
2
Finite Information Geometry
Fig. 2.8 Exponential family
of product distributions
Note that for S = ∅and S = {∅}, the hierarchical model ES consists of only
one element, the uniform distribution, and is therefore zero-dimensional. In order
to remove this ambiguity, we always assume S ̸= ∅in the context of hierarchical
models (but still allow S = {∅}). With this assumption, the dimension of ES is one
less than the dimension of FS:
dim(ES) =

∅̸=A∈S
0
v∈A

|Iv| −1

.
(2.208)
Example 2.6
(1) (Independence model) A hierarchical model is particularly simple if the hy-
pergraph is a partition S = {A1,...,An} of V . The corresponding simplicial
complex is then given by
S =
n
8
k=1
2Ak.
(2.209)
The hierarchical model ES consists of all strictly positive distributions that fac-
torize according to the partition S, that is,
p(i) =
n
0
k=1
p(iAk).
(2.210)
We refer to this hierarchical model as an independence model. In the special
case of binary units, it has the dimension
dim(ES) =
n

k=1

2|Ak| −1

.
(2.211)
(2) (Interaction model of order k) In this example, we want to model interactions up
to order k. For that purpose, it is sufﬁcient to consider the hypergraph consisting

2.9
Hierarchical and Graphical Models
111
of the subsets of cardinality k:
Sk :=
V
k

.
(2.212)
The corresponding simplicial complex is given by
Sk :=

A ⊆V : 0 ≤|A| ≤k

=
k8
l=0
V
l

.
(2.213)
This deﬁnes the hierarchical model
E(k) := ESk
(2.214)
with dimension
dim

E(k)
=
k

l=1
N
l

,
(2.215)
and we obviously have
E(1) ⊆E(2) ⊆··· ⊆E(N).
(2.216)
The information geometry of this hierarchy has been developed in more detail
by Amari [10]. The exponential family E(1) consists of the product distributions
(see Fig. 2.8). The extension to E(2) incorporates pairwise interactions, and E(N)
is nothing but the whole set of strictly positive distributions. Within the ﬁeld of
artiﬁcial neural networks, E(2) is known as a Boltzmann machine [15].
In Sect. 6.1, we shall study the relative entropy distance of a distribution p from
a hierarchical model ES, that is, infq∈ES D(p ∥q). This distance can be evaluated
with the corresponding maximum entropy estimate ˆp (see Sect. 2.8.3). More pre-
cisely, consider the set of distributions q that have the same A-marginals as p for
all A ∈S:
M := M(p,S) :=

q ∈P(IV ) :

iV \A
q(iA,iV \A) =

iV \A
p(iA,iV \A)
for all A ∈S and all iA ∈IA

.
Obviously, M is a closed and convex subset of P(IV ). Therefore, the restriction of
the continuous and strictly concave Shannon entropy H(q) = −	
i q(i)logq(i) to
M attains its maximal value in a unique distribution ˆp ∈M. We refer to this distri-
bution ˆp as the maximum entropy estimate (of p) with respect to S. The following
lemma provides a sufﬁcient condition for a distribution to be the maximum entropy
estimate with respect to S.

112
2
Finite Information Geometry
Lemma 2.13 (Maximum entropy estimation for hierarchical models)
Let S be a
non-empty hypergraph and let p ∈P(IV ). If a distribution ˆp satisﬁes the following
two conditions then it is the maximum entropy estimate of p with respect to S:
(1) There exist functions φA ∈FA, A ∈Smax, satisfying
ˆp(iV ) =
0
A∈Smax
φA(iV ).
(2.217)
(2) For all A ∈Smax, the A-marginal of ˆp coincides with the A-marginal of p, that
is,

iV \A
ˆp(iA,iV \A) =

iV \A
p(iA,iV \A),
for all iA ∈IA.
(2.218)
Proof We prove that ˆp is in the closure of ES. As ˆp is also in M(p,S), the state-
ment follows from Theorem 2.8.
ˆp(iV ) =
0
A∈Smax
φA(iV ) = lim
ε→0
7
A∈Smax(φA(iV ) + ε)
	
jV
7
A∈Smax(φA(jV ) + ε) = lim
ε→0 p(ε)(iV ),
where obviously p(ε) ∈ES.
□
For a strictly positive distribution p, the conditions (2.217) and (2.218) of
Lemma 2.13 are necessary and sufﬁcient for a distribution ˆp to be the maximum
entropy estimate of p with respect to S. This directly follows from Theorem 2.8.
In general, however, Lemma 2.13 provides only a sufﬁcient condition, but not a
necessary one, as follows from the following observation. If the maximum entropy
estimate ˆp lies in the boundary of the hierarchical model ES, it does not necessarily
have to admit the product structure (2.217).
In Sect. 6.1, we shall use Lemma 2.13 for the evaluation of a number of examples.
2.9.3 Graphical Models
Graphs provide a compact way of representing a particular kind of hierarchi-
cal models, the so-called graphical models [161]. In this section we present the
Hammersley–Clifford theorem which is central within the theory of graphical mod-
els.
We consider an undirected graph G = (V,E), with node set V and edge set
E ⊆
V
2

. If {v,w} ∈E then we write v ∼w and call v and w neighbors. Given
a node v the set {w ∈V : v ∼w} of neighbors is called the boundary of v and
denoted by bd(v). For an arbitrary subset A of V , we deﬁne the boundary bd(A) :=
∪v∈A(bd(v) \ A) of A and its closure cl(A) := A ∪bd(A). Note that according to
this deﬁnition, A and bd(A) are always disjoint.

2.9
Hierarchical and Graphical Models
113
A path in V is a sequence γ = (v1,...,vn) satisfying vi ∼vi+1 for all i =
1,...,n −1. Let A, B, and S be three disjoint subsets of V . We say that S sep-
arates A from B if for every path γ = (v1,...,vn) with v1 ∈A and vn ∈B there is
a vi ∈S. Note that bd(A) separates A from V \ cl(A).
A subset C of V is called a clique (of G), if for all v,w ∈C, v ̸= w, it holds that
v ∼w. The set of cliques is a simplicial complex in the sense of Deﬁnition 2.13,
which we denote by C(G). A hierarchical model that only includes interactions of
nodes within cliques of a graph has very special properties.
Deﬁnition 2.15 Let G be a graph, and let C(G) be the simplicial complex consist-
ing of the cliques of G. Then the hierarchical model E(G) := EC(G), as deﬁned by
(2.207), is called a graphical model.
A graph encodes natural conditional independence properties, so-called Markov
properties, which are satisﬁed by all distributions of the corresponding graphical
model. In Deﬁnition 2.16 below, we shall use the notation XA ⊥⊥XB |XC for
the conditional independence statement “XA and XB are stochastically indepen-
dent given XC.” This clearly depends on the underlying distribution which is not
mentioned explicitly in this notation. Formally, this conditional independence with
respect to a distribution p ∈P(IV ) is expressed by
p(iA,iB |iC) = p(iA |iC)p(iB |iC)
whenever p(iC) > 0,
(2.219)
where we apply the deﬁnition (2.184). If we want to use marginals only, we can
rewrite this condition as
p(iA,iB,iC) = p(iA,iC)p(iB,iC)
p(iC)
whenever p(iC) > 0.
(2.220)
This is equivalent to the existence of two functions f ∈FA∪C and g ∈FB∪C such
that
p(iA,iB,iC) = f (iA,iC)g(iB,iC),
(2.221)
where we can ignore “whenever p(iC) > 0” in (2.219) and (2.220).
Deﬁnition 2.16 Let G be a graph with node set V , and let p be a distribution on IV .
Then we say that p satisﬁes the
(G) global Markov property, with respect to G, if
A,B,S ⊆V disjoint, S separates A from B
⇒
XA ⊥⊥XB |XS;
(L) local Markov property, with respect to G, if
v ∈V
⇒
Xv ⊥⊥XV \v |Xbd(v);
(P) pairwise Markov property, with respect to G, if
v,w ∈V,v ≁w
⇒
Xv ⊥⊥Xw |XV \{v,w}.

114
2
Finite Information Geometry
Proposition 2.19
Let G be a graph with node set V , and let p be a distribution
on IV . Then the following implications hold:
(G)
⇒
(L)
⇒
(P).
Proof (G) ⇒(L) This is a consequence of the fact that bd(v) separates v from
V \ cl(v), as noted above.
(L) ⇒(P) Assuming that v,w ∈V are not neighbors, one has w ∈V \ cl(w) and
therefore
bd(v) ∪

V \ cl(v)

\ {w}

= V \ {v,w}.
(2.222)
From the local Markov property (L), we know that
Xv ⊥⊥XV \cl(v) |Xbd(v).
(2.223)
We now use the following general rule for conditional independence statements:
XA ⊥⊥XB |XS,C ⊆B
⇒
XA ⊥⊥XB |XS∪C.
With (2.223) and (V \ cl(v)) \ {w} ⊆V \ cl(v), this rule implies
Xv ⊥⊥XV \cl(v) |Xbd(v)∪[(V \cl(v))\{w}].
Because of (2.222), this is equivalent to
Xv ⊥⊥XV \cl(v) |XV \{v,w}.
With w ∈V \ cl(v) we ﬁnally obtain
Xv ⊥⊥Xw |XV \{v,w},
which proves the pairwise Markov property.
□
Now we provide a criterion for a distribution p that is sufﬁcient for the global
Markov property. We say that p factorizes according to G or satisﬁes the
(F) factorization property, with respect to G, if there exist functions fC ∈FC, C a
clique, such that
p(i) =
0
C clique
fC(i).
(2.224)
Proposition 2.20
Let G be a graph with node set V , and let p be a distribution
on IV . Then the factorization property implies the global Markov property, that is,
(F)
⇒
(G).

2.9
Hierarchical and Graphical Models
115
Proof We assume that S separates A from B and have to show
XA ⊥⊥XB |XS.
The complement V \ S of S in V can be written as the union of its connected
components Vi, i = 1,...,n:
V \ S = V1 ∪··· ∪Vn.
We deﬁne

A :=
8
i∈{1,...,n}
Vi∩A̸=∅
Vi,

B :=
8
i∈{1,...,n}
Vi∩A=∅
Vi.
Obviously, A ⊆
A, and B ⊆
B (S separates A from B). Furthermore, a clique C
is contained in 
A ∪S or 
B ∪S. Therefore, we can split the factorization of p as
follows:
p(i) =
0
C clique
fC(i)
=
0
C clique
C⊆
A∪S
fC(i) ·
0
C clique
C⊈
A∪S
fC(i)
=: g(i
A,iS) · h(i
B,iS).
With (2.221), this proves X
A ⊥⊥X
B |XS. Since A ⊆
A and B ⊆
B, we ﬁnally obtain
XA ⊥⊥XB |XS.
□
Generally, there is no equivalence of the above Markov conditions. On the other
hand, if we assume strict positivity of a distribution p, that is, p ∈P+(IV ), then
we have equivalence. This is the content of the Hammersley–Clifford theorem of
graphical model theory.
Theorem 2.9 (Hammersley–Clifford theorem) Let G be a graph with node set V ,
and let p be a strictly positive distribution on IV . Then p satisﬁes the pairwise
Markov property if and only if it factorizes according to G.
Proof We only have to prove that (P) implies (F), since the opposite implication
directly follows from Propositions 2.19 and 2.20.
We assume that p satisﬁes the pairwise Markov property (P). As p is strictly
positive, we can consider logp(i). We choose one conﬁguration i∗∈IV and deﬁne
HA(i) := logp

iA,i∗
V \A

,
A ⊆V.

116
2
Finite Information Geometry
Here (iA,i∗
V \A) coincides with i on A and with i∗on V \ A. Clearly, HA does not
depend on iV \A, and therefore HA ∈FA. We deﬁne
φA(i) :=

B⊆A
(−1)|A\B|HB(i).
Also, the φA depend only on A. With the Möbius inversion formula (Lemma 2.12),
we obtain
logp(i) = HV (i) =

A⊆V
φA(i).
(2.225)
In what follows we use the pairwise Markov property of p in order to prove that
in the representation (2.225), φA = 0 whenever A is not a clique. This obviously
implies that p factorizes according to G and completes the proof.
Assume A is not a clique. Then there exist v,w ∈A, v ̸= w, v ≁w. Consider
C := A \ {v,w}. Then
φA(i) =

B⊆A
(−1)|A\B|HB(i)
=

B⊆A
v,w/∈B
(−1)|A\B|HB(i) +

B⊆A
v∈B,w/∈B
(−1)|A\B|HB(i)
+

B⊆A
v/∈B,w∈B
(−1)|A\B|HB(i) +

B⊆A
v,w∈B
(−1)|A\B|HB(i)
=

B⊆C
(−1)|C\B|+2HB∪{v,w}(i) +

B⊆C
(−1)|C\B|+1HB∪{v}(i)
+

B⊆C
(−1)|C\B|+1HB∪{w}(i) +

B⊆C
(−1)|C\B|HB(i)
=

B⊆C
(−1)|C\B|
HB(i) −HB∪{v}(i) −HB∪{w}(i) + HB∪{v,w}(i)

.
(2.226)
We now set D := V \ {v,w} and use the pairwise Markov property (P) in order to
show that (2.226) vanishes:
HB∪{v,w}(i) −HB∪{v}(i)
= log
p(iB,iv,iw,i∗
D\B)
p(iB,iv,i∗w,i∗
D\B)
= log
p(iB,iw,i∗
D\B) · p(iv |iB,iw,i∗
D\B)
p(iB,i∗w,i∗
D\B) · p(iv |iB,i∗w,i∗
D\B)

2.9
Hierarchical and Graphical Models
117
= log
p(iB,iw,i∗
D\B) · p(iv |iB,i∗
D\B)
p(iB,i∗w,i∗
D\B) · p(iv |iB,i∗
D\B)
= log
p(iB,iw,i∗
D\B) · p(i∗
v |iB,i∗
D\B)
p(iB,i∗w,i∗
D\B) · p(i∗v |iB,i∗
D\B)
= log
p(iB,iw,i∗
D\B) · p(i∗
v |iB,iw,i∗
D\B)
p(iB,i∗w,i∗
D\B) · p(i∗v |iB,i∗wi∗
D\B)
= log
p(iB,i∗
v,iw,i∗
D\B)
p(iB,i∗v,i∗w,i∗
D\B)
= HB∪{w}(i) −HB(i).
This implies φA(i) = 0 and, with the representation (2.225), we conclude that p
factorizes according to G.
□
The Hammersley–Clifford theorem implies that for strictly positive distributions,
all Markov properties of Deﬁnition 2.16 are equivalent. The set of strictly positive
distributions that satisfy one of these properties, and therefore all of them, is given
by the graphical model E(G). Its closure E(G) is sometimes referred to as the ex-
tended graphical model. We want to summarize the results of this section by an
inclusion diagram. In order to do so, for each property (prop) ∈{(F),(G),(L),(P)},
we deﬁne
M(prop) :=

p ∈P(IV ) : p satisﬁes (prop)

,
and
M(prop)
+
:= M(prop) ∩P+(IV ).
Clearly, the set of strictly positive distributions that factorize according to G, that is,
M(F)
+ , coincides with the graphical model E(G). One can easily verify that M(G),
M(L), and M(P) are closed subsets of P(IV ) that contain the extended graphical
model E(G) as a subset. However, the set M(F) is not necessarily closed: limits of
factorized distributions do not have to be factorized. Furthermore, it is contained in
E(G) (see the proof of Lemma 2.13).
These considerations, Propositions 2.19 and 2.20, and the Hammersley–Clifford
theorem (Theorem 2.9) can be summarized in terms of the following diagram.
M(F)
+
=
E(G)
=
M(G)
+
=
M(L)
+
=
M(P)
+
⊆
⊆
⊆
⊆
⊆
M(F)
⊆
E(G)
⊆
M(G)
⊆
M(L)
⊆
M(P)
(2.227)
The upper row of equalities in this diagram summarizes the content of the
Hammersley–Clifford theorem (Theorem 2.9). The lower row of inclusions in this
diagram follows from Propositions 2.19 and 2.20. Each of the horizontal inclusions

118
2
Finite Information Geometry
can be strict in the sense that there exists a graph G for which the inclusion is strict.
Corresponding examples are given in Lauritzen’s monograph [161], referring to
work by Moussouris [191], Matúš [174], and Matúš and Studený [181].
Remark 2.4
(1) Conditional independence statements give rise to a natural class of models,
referred to as conditional independence models (see the monograph of Stu-
dený [241]). This class includes graphical models as prime examples. By the
Hammersley–Clifford theorem, on the other hand, graphical models are also
special within the class of hierarchical models. A surprising and important re-
sult of Matúš highlights the uniqueness of graphical models within both classes
([178], Theorem 1). If a hierarchical model is speciﬁed in terms of conditional
independence statements, then it is already graphical. This means that one
cannot specify any other hierarchical model in terms of conditional indepen-
dence statements. Furthermore, the result of Matúš also provides a new proof
of the Hammersley–Clifford theorem ([178], Corollary 1). It is not based on the
Möbius inversion of the classical proof which we have used in our presentation.
(2) Graphical model theory has been further developed by Geiger, Meek, and
Sturmfels using tools from algebraic statistics [103]. In their work, they de-
velop a reﬁned geometric understanding of the results presented in this section.
This understanding is not restricted to graphical models but also applies to gen-
eral hierarchical models. Let us brieﬂy sketch their perspective. All conditional
independence statements that appear in Deﬁnition 2.16 can be reformulated in
terms of polynomial equations. Each Markov property can then be associated
with a corresponding set of polynomial equations, which generates an ideal I
in the polynomial ring R[x1,...,x|IV |]. (The indeterminates are the coordinates
p(i), i ∈IV , of the distributions in P(IV ).) This leads to the ideals I (G), I (L),
and I (P) that correspond to the individual Markov properties, and we obviously
have
I (P) ⊆I (L) ⊆I (G).
(2.228)
Each of these ideals fully characterizes the graphical model as its associated
variety in P+(IV ), which follows from the Hammersley–Clifford theorem. The
respective varieties M(G), M(L), and M(P) in the full simplex P(IV ) differ in
general and contain the extended graphical model E(G) as a proper subset. On
the other hand, one can fully specify E(G) in terms of polynomial equations
using Theorems 2.5 and 2.7. Denoting the corresponding ideal by I (G), we
can extend the above inclusion chain (2.228) by
I (G) ⊆I (G).
(2.229)
Stated differently, the ideal I (G) encodes all Markov properties in terms of
elements of an appropriate ideal basis. Let us now assume that we are given
an arbitrary hierarchical model ES with respect to a hypergraph S, and let us
denote by IS the ideal that is generated by the corresponding Eqs. (2.139) or,

2.9
Hierarchical and Graphical Models
119
equivalently, (2.149). Geiger, Meek, and Sturmfels interpret the elements of a
ﬁnite ideal basis of IS as generalized conditional independence statements and
prove a version of the Hammersley–Clifford theorem for hierarchical models.
Note, however, that the above mentioned result of Matúš implies that there is
a correspondence between ideal basis elements and actual conditional indepen-
dence statements only for graphical models.

Chapter 3
Parametrized Measure Models
3.1 The Space of Probability Measures and the Fisher Metric
This section has a more informal character. It introduces the basic concepts
and problems of information geometry on—typically—inﬁnite sample spaces and
thereby sets the stage for the more formal considerations in the next section. The
perspective here will be somewhat different from that developed in Chap. 2, as the
constructions for the probability simplex presented there did not have to grapple
with the measure theoretical complications that we shall encounter here. Neverthe-
less, the analogy with the ﬁnite-dimensional case will guide our intuition.
Let Ω be a set with a σ-algebra B of subsets;1 for example, Ω can be a topo-
logical space and B the σ-algebra of Borel sets, i.e., the σ-algebra generated by the
open sets. Later on, Ω will also have to carry a differentiable structure.
For a signed measure μ on Ω, we have the total variation
∥μ∥T V := sup
n

i=1
μ(Ai)
,
(3.1)
where the supremum is taken over all ﬁnite partitions Ω = A1 ⊎··· ⊎An with dis-
joint sets Ai ∈B. If ∥μ∥T V < ∞, the signed measure μ is called ﬁnite. We consider
the Banach space S(Ω) of all signed ﬁnite measures on Ω with the total variation
as Banach norm. The subsets of all ﬁnite non-negative measures and of probability
measures on Ω will be denoted by M(Ω) and P(Ω), respectively.
The null sets of a measure μ are those subsets A of Ω with μ(A) = 0. A ﬁnite
non-negative measure μ1 dominates another ﬁnite measure μ2 if every null set of
μ1 is also a null set of μ2. Two ﬁnite non-negative measures are called compatible
if they dominate each other, i.e., if they have the same null sets. Spaces of such
measures will be the basis of our subsequent constructions, and we shall therefore
1Ω will take over the role of the ﬁnite sample space I in Sect. 2.1.
© Springer International Publishing AG 2017
N. Ay et al., Information Geometry, Ergebnisse der Mathematik und ihrer Grenzgebiete.
3. Folge / A Series of Modern Surveys in Mathematics 64,
DOI 10.1007/978-3-319-56478-4_3
121

122
3
Parametrized Measure Models
now formalize these notions. We take some σ-ﬁnite non-negative measure μ0. Then
S(Ω,μ0) :=

μ = φ μ0 : φ ∈L1(Ω,μ0)

is the space of signed measures dominated by μ0. This space can be identiﬁed in
terms of the canonical map
ican : S(Ω,μ0) →L1(Ω,μ0)
μ
→dμ
dμ0
,
(3.2)
where the latter is the Radon–Nikodym derivative of μ w.r.t. μ0. Note that
∥μ∥T V = ∥φ∥L1(Ω,μ0) =

dμ
dμ0

L1(Ω,μ0)
.
As we see from this description that ican is a Banach space isomorphism, we refer
to the topology of S(Ω,μ0) also as the L1-topology.
If μ1, μ2 are compatible ﬁnite non-negative measures, they are absolutely contin-
uous with respect to each other in the sense that there exists a non-negative function
φ that is integrable with respect to either of them, such that
μ2 = φμ1,
or, equivalently,
μ1 = φ−1μ2.
(3.3)
As noted before, φ is then the Radon–Nikodym derivative of μ2 with respect to μ1.
Being integrable, φ is ﬁnite almost everywhere (with respect to both μ1 and μ2)
on Ω, and since the situation is symmetric between φ and φ−1, φ is also positive
almost everywhere. Thus, for any ﬁnite non-negative measure μ on Ω, we let
F+(Ω,μ) :=

φ ∈L1(Ω,μ), φ > 0 μ-a.e.

(3.4)
be the space of integrable functions on Ω that are positive almost everywhere with
respect to μ. (The reason for the notation will become apparent in Sect. 3.2 below.)
In fact, in later sections, we shall ﬁnd it more convenient to work with the space of
measures
M+(Ω,μ) :=

φμ : φ ∈L1(Ω,μ), φ > 0 μ-a.e.

(3.5)
than with the space F+(Ω,μ) of functions. Of course, these two spaces can easily
be identiﬁed, as they simply differ by multiplication with μ.
In particular, the topology of S(Ω,μ0) is independent of the particular choice of
the reference measure μ0 within its compatibility class, because if
φ ∈L1(Ω,μ0)
and
ψ ∈L1(Ω,φμ0),
then ψφ ∈L1(Ω,μ0).
(3.6)
Compatibility is an equivalence relation on the space of ﬁnite non-negative mea-
sures on Ω, and that space is therefore partitioned into equivalence classes. The set
of such equivalence classes is quite large. For instance, the Dirac measure at any

3.1
The Space of Probability Measures and the Fisher Metric
123
point of Ω generates its own such class. More generally, in Euclidean space, we can
consider Hausdorff measures of subsets of possibly different Hausdorff dimensions.
In the sequel, we shall usually work within a single such compatibility class with
some base measure μ0. The basic example that one may have in mind is of course
the Lebesgue measure on a Euclidean or Riemannian domain, or else the Hausdorff
measure on some ﬁxed subset.
Any other ﬁnite non-negative measure μ that is compatible with μ0 is thus of the
form
μ = φμ0
for some
φ ∈F+(Ω,μ0),
(3.7)
and therefore, by (3.3),
μ0 = φ−1μ
with
φ−1 ∈F+(Ω,μ).
(3.8)
This yields the identiﬁcations
F+(Ω,μ) = F+(Ω,μ0) = : F+,
M+(Ω,μ) = M+(Ω,μ0) = : M+,
(3.9)
where we use F+ and M+ if there is no ambiguity over which base measure μ0 is
used. Moreover, if
μ1 = φμ0,
μ2 = ψμ1,
with
φ ∈F+(Ω,μ0), ψ ∈F+(Ω,μ1)
then by (3.6)
μ2 = ψφμ0
with
ψφ ∈F+(Ω,μ0).
F+(Ω,μ), however, is not a group under pointwise multiplication since for φ1,φ2 ∈
L1(Ω,μ0), their product φ1φ2 need not be in L1(Ω,μ0).
The question arises, however, whether F+ possesses an—ideally dense—subset
that is a group, perhaps even a Lie group. That is, to what extent can we linearize
the partial multiplicative group structure via an exponential map, in the same man-
ner as z →ez maps the additive group (R,+) to the multiplicative group (R+,·)?
Formally, we might be inclined to identify the tangent space TμF+ of F+ at any
compatible μ with
Bμ :=

f : Ω →R ∪{±∞}, e±f ∈L1(Ω,μ)

.
(3.10)
We should point out that here we attempt to revert the construction of
Sects. 2.1, 2.2. However, we immediately run into the difﬁculty that the set Bμ
is in general not a vector space, but only a convex subset of L1(Ω,μ). Thus, a
reasonable deﬁnition of TμF+ would be to deﬁne it as the vector space generated
by Bμ.
We now switch to working with the space M+ of measures instead of the space
F+ of functions, where, as mentioned, φ ∈F+ corresponds to φμ ∈M+. One
approach to provide some structure on M+ has been pursued by Pistone and Sempi

124
3
Parametrized Measure Models
who constructed a Banach norm on TμM+ such that Bμ becomes a convex open
subset. The Banach norm is—up to equivalence—independent of the choice of μ so
that in this way M+ becomes a Banach manifold. The topology which is imposed
on M+ in this way is called the e-topology. We shall describe this in detail in
Sect. 3.3.
Let us return to our discussion of the analogy between the general situation and
the construction of Sects. 2.1, 2.2. There the space of functions was the cotangent
space of a measure m in the space of measures, and the tangent and the cotangent
space were then identiﬁed through a scalar product (and the scalar product chosen
then was the Fisher metric). Here, we take the duality between functions and mea-
sures
(φ,m) →

φ dm
as a starting point and vary a measure via m →Fm for some non-negative func-
tion F . This duality will then induce the scalar product
⟨f,F⟩m =

f F dm
which we shall then use to deﬁne the Fisher metric.
The construction is tied together by the exponential map
exp: TμF+⊇Bμ →F+
f →ef
(3.11)
that converts arbitrary functions into non-negative ones. In other words, we apply
the exponential function z →ez to each value f (x) of the function f .
Here, in fact, the underlying structure is even an afﬁne one, in the following
sense. When we take a measure μ′ = φμ in place of μ, then an exponential image
ef μ from TμF+ is also an exponential image egμ′ = egφμ from Tμ′F+, and the
relationship is
f = g + logφ.
(3.12)
Thus, f and g are related by adding the function logφ which is independent of both
of them.
Also, of course,
log: M+ →Bμ⊆TμM+
φμ →logφ
(3.13)
is the inverse of the exponential map.2
2For reasons of integrability, this structure need not deﬁne an afﬁne space in the sense of Sect. 2.8.1.
We only have the structure of an afﬁne manifold, in the sense of possessing afﬁne coordinate
changes. This issue will be clariﬁed in Sect. 3.2 below.

3.1
The Space of Probability Measures and the Fisher Metric
125
There is also another, likewise ultimately futile, approach to a putative tangent
space TμM+ of M+ which is based on a duality relation between M+ and those f
that are not only integrable (L1) w.r.t. μ, but even of class L∞(Ω,μ). This duality
is given by
(f,φμ) :=

f φ dμ,
(3.14)
which exists since φ ∈L1(Ω,μ).
On this space, T ∞
μ M+, we then have a natural scalar product, namely the L2-
product
⟨f,g⟩μ :=

fg dμ
for f,g ∈T ∞
μ M+.
(3.15)
However, the completion of T ∞
μ M+ with respect to the norm ∥·∥induced by ⟨·,·⟩μ
is the Hilbert space T 2
μM+ of functions f of class L2 with respect to μ, which is
larger than those for which e±f are of class L1.
Thus, by this construction, we do not quite succeed in making M+ into a Lie
group. Nevertheless, (3.15) yields a natural Riemannian metric on M+ that will
play an important role in the sequel. A Riemannian structure on a differentiable
manifold X assigns to each tangent space TpX a scalar product, and this product has
to depend differentiably on the base point. At this point, this is only formal, how-
ever. When Ω is not ﬁnite, spaces of measures or functions are inﬁnite-dimensional
in general because the value at every point is a degree of freedom. Only when Ω
is a ﬁnite set do we obtain a ﬁnite-dimensional measure or function space. Thus,
we need to deal with inﬁnite-dimensional manifolds, e.g., compatibility classes of
measures. In Appendix C, we discuss the appropriate concept, that of a Banach
manifold. As mentioned, in the present discussion, we only have a weak such struc-
ture, however, as our space is not complete w.r.t. the L2-structure of the metric. In
other words, the topology induced by the metrics on the tangent spaces is weaker
than the Banach space topology that we are working with. For the moment, we shall
therefore proceed in a formal way. Below, in Sect. 3.2, we shall provide a rigorous
construction that avoids these issues.
The natural identiﬁcation between the spaces T 2
μM+ and T 2
μ′M+ (which for
the moment will take the role of tangent spaces—and we shall therefore omit the
superscript 2) with μ′ = φμ is given by
f →
1
√φ f ;
(3.16)
we then have
 1
√φ f, 1
√φ g
 
μ′ =

1
√φ f ·
1
√φ gφ dμ =

fg dμ = ⟨f,g⟩μ.
In particular, if Ω is a differentiable manifold and
κ : Ω →Ω

126
3
Parametrized Measure Models
is a diffeomorphism, then μ is transformed into κ∗μ, called the push-forward of μ
under κ, with
κ∗μ(V ) := μ

κ−1V

for all V ∈B.
(3.17)
Since

V
1
|detdκ(x)|μ(d(κ(x))) =

κ−1V μ(dx) by the transformation formula, we
have
κ∗μ

κ(x)
detdκ(x)
 = μ(y)
for y = κ(x).
We thus have
⟨f,g⟩κ∗μ =

κ∗f,κ∗g

μ
(3.18)
with
κ∗f (x) = f

κ(x)

.
In other words, if we employ this transformation rule for tangent vectors, then the
diffeomorphism group of Ω acts by isometries on M+ with respect to the met-
ric given by (3.15). Thus, our metric is invariant under diffeomorphisms of Ω. One
might then wish to consider the quotient of M+ by the action of the diffeomorphism
group D(Ω). Of course, if Ω is ﬁnite, then D(Ω) is simply the group of permuta-
tions of the elements of Ω. This group has a ﬁxed point, namely the probability
measure with
p(xi) = 1
n
for i = 1,...,n
(assuming Ω consists of the n elements x1,...,xn), and its scalar multiples. We
therefore expect that the quotient M+/D(Ω) will have singularities.
In the inﬁnite case, the situation is somewhat different (although we still get
singularities). Namely, if Ω is a compact oriented differentiable manifold, then a
theorem of Moser [190] says that any two probability measures μ,ν that are vol-
ume forms, i.e., are smooth and positive on all open sets, (in local coordinates
(x1,...,xn) on Ω, they are thus smooth positive multiples of dx1 ∧··· ∧dxn)
are related by a diffeomorphism κ,
ν = κ∗μ,
or equivalently,
μ =

κ−1
∗ν =: κ∗ν.
Thus, the diffeomorphism group acts transitively on the space of volume forms of
total measure 1, and the quotient by the diffeomorphism group of this space is there-
fore a single point.
We recall that a ﬁnite non-negative measure μ1 dominates another ﬁnite non-
negative measure μ2 if every null set for μ1 also is a null set for μ2. Of course, two
mutually dominant ﬁnite non-negative measures are compatible.

3.1
The Space of Probability Measures and the Fisher Metric
127
In the ﬁnite case, i.e., for Ω = {x1,...,xn}, μ1 then dominates μ2 if
μ1(xi) = 0
implies μ2(xi) = 0
for any i = 1,...,n.
In particular, a measure μ with
μ(xi) > 0
for all i
dominates every other measure on Ω.
This is different in the inﬁnite case; for example, if Ω is a compact oriented
differentiable manifold, then a volume form which is positive on every open set no
longer dominates a Dirac measure supported at some point in Ω.
In information geometry, one wishes to study only probability measures, that is,
measures that satisfy the normalization
μ(Ω) =

Ω
dμ = 1.
(3.19)
It might seem straightforward to simply impose this condition upon measures and
then study the space of those measures satisfying it as a subspace of the space of all
measures. A somewhat different point of view, however, emerges from the following
consideration. The normalization (3.19) can simply be achieved by rescaling a given
measure μ, that is, by multiplying it by some appropriate λ ∈R. λ is simply obtained
as μ(Ω)−1. The freedom of rescaling a measure now expresses that we are not
interested in absolute “sizes” μ(A) of subsets of Ω, but rather only in relative ones,
like μ(A)
μ(Ω) or μ(A1)
μ(A2). Therefore, we identify the space P(Ω) of probability measures
on Ω as the projective space
P1M(Ω),
i.e., the space of all equivalence classes in M(Ω) under multiplication by positive
real numbers. Of course, elements of P(Ω) can be considered as measures satis-
fying (3.19), but more appropriately as equivalence classes of measures giving the
same relative sizes of subsets of Ω.
Our above metric then also induces a metric on P(Ω) as a quotient of M(Ω)
which is different from the one obtained by identifying P1M(Ω) with the subspace
of M(Ω) consisting of metrics satisfying (3.19). Let us recall from Proposition 2.1
in Sect. 2.2 the case where Ω is ﬁnite, Ω = {1,...,n}. In that case, the probability
measures on Ω are given by
Σn−1 :=

(p1,...,pn) : pi ≥0 for i = 1,...,n, and
n

i=1
pi = 1

.
These form an (n −1)-dimensional simplex in the positive cone Rn
+ of Rn. The
projective space
P1Rn
+,

128
3
Parametrized Measure Models
however, is naturally identiﬁed with the corresponding spherical sector
Sn−1
+
:=

(z1,...,zn) : zi ≥0 for i = 1,...,n,
n

i=1
z2
i = 1

.
There is a natural bijection
Σn−1 →Sn−1
+
(p1,...,pn) →(√p1,...,√pn).
(3.20)
Let us now try to carry this over to the inﬁnite-dimensional case, under the assump-
tion that Ω is a differentiable manifold of dimension n. In that case, we can consider
each (Radon) measure μ as a density. This means that for each x ∈Ω, if we con-
sider the space Gl(n,R) as the space of all bases of TxΩ (again, the identiﬁcation is
not canonical as we need to select one basis V = (v1,...,vn) that is identiﬁed with
id ∈Gl(n,R)),3
μ(x)(XV ) = |detX|μ(x)(V ).
Likewise, we call ρ a half-density, if we have
ρ(x)(XV ) = |detX|1/2ρ(x)(V )
for all X ∈Gl(n,R) and bases V . Below, in Deﬁnition 3.54, we shall give a precise
deﬁnition of the space of half-densities (in fact, of any rth power of a measure with
0 < r ≤1).
In this interpretation, our above L2-product on M+(Ω) becomes an L2-product
on the space of half-densities
⟨ρ,σ⟩:=

Ω
ρσ,
where we no longer need a base measure μ. And the diffeomorphism group of Ω
then acts by isometries on the Hilbert space of half-densities of class L2.
If we now have a probability measure μ, then its square root √μ is a half-density
that is contained in the unit sphere of that Hilbert space. Conversely, up to the is-
sue of regularity, the part of that unit sphere that corresponds to non-negative half-
densities can be identiﬁed with the probability measures on Ω. As mentioned, it
carries a metric that is invariant under the action of the diffeomorphism group of Ω,
i.e., under relabeling of the points of Ω.
3Again, we are employing a fundamental mathematical principle here: Instead of considering ob-
jects in isolation, we rather focus on the transformations between them. Thus, instead of an individ-
ual basis, we consider the transformation that generates it from some (arbitrarily chosen) standard
basis. This automatically gives a very powerful structure, that of a group (of transformations).

3.1
The Space of Probability Measures and the Fisher Metric
129
The duality relation (3.14) for a probability measure μ′ = φμ then becomes

f,μ′
= Eμ′(f ),
(3.21)
the expectation value of f w.r.t. the probability measure μ′. This is a linear operation
on the space of functions f and an afﬁne operation on the space of probability
measures μ′.
Let us clarify the relation with our previous construction of the metric: If f is a
tangent vector to a probability measure μ, then it generates the curve
etf μ,
t ∈R,
through μ. By taking the square root as before, we obtain the curve
e
1
2 tf √μ,
t ∈R,
in the space of half-densities that has the expansion in t
√μ + 1
2tf √μ + O

t2
.
Thus, the tangent vector that corresponds to f in the space of half-densities is
1
2f √μ.
The inner product of two such tangent vectors is
 1
2f √μ · 1
2g√μ = 1
4

fgμ.
Thus, up to the inessential factor 1
4, we regain our original Riemannian metric on
M+(Ω). In order to eliminate that factor, in Proposition 2.1, we had used the sphere
with radius 2 instead of that of radius 1. Therefore, we should also modify (3.20) in
the same manner if we want to have an isometry.
Let us now translate this into the usual statistical interpretation: We have a family
p(x;s) of probability measures depending on a parameter s, −ε < s < ε. Then the
squared norm of the tangent vector to this family at s = 0 is (up to some factor 4)
4

d
ds

p(x;s) d
ds

p(x;s)|s=0 dx
=

d
ds logp(x;s) d
ds logp(x;s)p(x;0)|s=0 dx
= Ep
 d
ds logp(x;s)
2
s=0

,
(3.22)

130
3
Parametrized Measure Models
where Ep denotes the expectation with respect to the probability measure p =
p(·;0). By polarization, if s = (s1,...,sn) is now n-dimensional, we obtain the
Fisher information metric
Ep
 ∂
∂sμ
logp(x;s) ∂
∂sν
logp(x;s)

.
(3.23)
We can also rewrite the above formula to obtain
Ep
 ∂
∂sμ
logp(x;s) ∂
∂sν
logp(x;s)

=

∂
∂sμ
logp(x;s) ∂
∂sν
logp(x;s)p(x;0)dx
= −

∂2
∂sμ∂sν
logp(x;s)p(x;0)dx
(3.24)
since

∂
∂sμ logp(x;s)p(x;0)dx= ∂
∂sμ

logp(x;s)p(x;0)dx= ∂
∂sμ

p(x;s)dx =
∂
∂sμ 1 = 0, which implies
0 = ∂
∂sν

∂
∂sμ
logp(x;s) p(x;s)dx =

∂2
∂sμ∂sν
logp(x;s) p(x;s)dx
+

∂
∂sμ
logp(x;s) ∂
∂sν
logp(x;s) p(x;s)dx.
This step also admits the following interpretation:
∂
∂sν
logp(x;s)
(3.25)
which is called the score of the family with respect to the parameter sν. Our above
computation then gives
Ep
 ∂
∂sν
logp(x;s)

= 0,
(3.26)
that is, the expectation value of the score vanishes. (This expresses the fact that the
cross-entropy
−

p(x)logq(x)dx
(3.27)
is minimal w.r.t. q precisely for q = p.)
The Fisher metric (3.22) then expresses the covariance matrix of the score.

3.1
The Space of Probability Measures and the Fisher Metric
131
Returning to (3.24), (3.26) yields the formula
Ep
 ∂
∂sμ
logp(x;s) ∂
∂sν
logp(x;s)

= −Ep

∂2
∂sμ∂sν
logp(x;s)

(3.28)
as another representation of the Fisher metric (3.23).
We can also write our metric as

1
p(x;0)
∂
∂sμ
p(x;s) ∂
∂sν
p(x;s)dx.
In the ﬁnite case, this becomes
n

i=1
1
pi
∂
∂sμ
pi
∂
∂sν
pi.
Remark 3.1 This metric is called the Shashahani metric in mathematical biology,
see Sect. 6.2.1.
As veriﬁed in Proposition 2.1, this is simply the metric obtained on the sim-
plex Σn−1 when identifying it with the spherical sector Sn−1
2,+ via the map 4p = q2,
q ∈Sn−1
2,+ . If the second derivatives
∂2
∂sμ∂sν p vanish, i.e., if p(x;s) is linear in s, then
n

i=1
1
pi
∂
∂sμ
pi
∂
∂sν
pi =
∂2
∂sμ∂sν
n

i=1
pi logpi.
As will be discussed below, this means that the negative of the entropy is a potential
for the metric. This will be applied in Theorem 6.4.
The Fisher metric then induces a metric on any smooth family of probability mea-
sures on Ω. To understand the Fisher metric, it is often useful to write a probability
distribution in exponential form,
p(x;s) = exp

−H(x,s)

,
(3.29)
where the normalization required for

p(x;s)dx = 1 is supposed to be contained
in H. The Fisher metric is then simply given by
Ep
 ∂
∂sμ
logp(x;s) ∂
∂sν
logp(x;s)

= Ep
∂H
∂sμ
∂H
∂sν

=
 ∂H
∂sμ
∂H
∂sν
p(x;s)dx.
(3.30)
Particularly important in this regard are the so-called exponential families (cf. Deﬁ-
nition 2.10).

132
3
Parametrized Measure Models
Deﬁnition 3.1 An exponential family is a family of probability distributions of the
form
p(x;ϑ) = exp

γ (x) +
n

i=1
fi(x)ϑi −ψ(ϑ)

μ(x),
(3.31)
where ϑ = (ϑ1,...,ϑn) is an n-dimensional parameter, γ (x) and f1(x),...,fn(x)
are functions and μ(x) is a measure on Ω.
(Of course, γ could be absorbed into μ, but this would be inconvenient for our
subsequent discussion of examples.) The function ψ simply serves to guarantee the
normalization

Ω
p(x;ϑ) = 1;
namely
ψ(ϑ) = log

exp
;
γ (x) +

fi(x)ϑi<
μ(dx).
Here, the family is deﬁned only for those ϑ for which

exp
;
γ (x) +

fi(x)ϑi<
μ(dx) < ∞.
The set of those ϑ for which this is satisﬁed is convex, but can otherwise be quite
complicated.
Exponential families will yield important examples of the parametrized measure
models introduced in Sect. 3.2.4.
Example 3.1 The normal distribution N(μ,σ 2) =
1
√
2πσ exp(−(x−μ)2
2σ 2 ) on R, with
Lebesgue measure dx, with parameters μ, σ can easily be written in this form by
putting
γ (x) = 0,
f1(x) = x,
f2(x) = x2,
ϑ1 = μ
σ 2 ,
ϑ2 = −1
2σ 2 ,
ψ(ϑ) = μ2
2σ 2 + log
√
2πσ = −(ϑ1)2
4ϑ2 + 1
2 log

−π
ϑ2

,
and analogously for multivariate normal distributions N(y,Λ), i.e., Gaussian dis-
tributions on Rn. See [169] for a systematic analysis.
For an exponential family, we have
∂
∂ϑi logp(x;ϑ) = fi(x) −
∂
∂ϑi ψ(ϑ)
(3.32)

3.1
The Space of Probability Measures and the Fisher Metric
133
and
∂2
∂ϑi∂ϑj logp(x;ϑ) = −
∂2
∂ϑi∂ϑj ψ(ϑ).
(3.33)
This expression no longer depends on x, but only on the parameter θ. Therefore, the
Fisher metric on such a family is given by
gij(p) = −Ep

∂2
∂ϑi∂ϑj logp(x;ϑ)

=

∂2
∂ϑi∂ϑj ψ(ϑ) p(x;ϑ)dx
=
∂2
∂ϑi∂ϑj ψ(ϑ)
since

p(x;ϑ)dx = 1.
(3.34)
For the normal distribution, we compute the metric in terms of ϑ1 and ϑ2, using
(3.33) and transform the result to the variables μ and σ with (B.16) and obtain at
μ = 0
g
 ∂
∂μ, ∂
∂μ

= 1
σ 2 ,
(3.35)
g
 ∂
∂μ, ∂
∂σ

= 0,
(3.36)
g
 ∂
∂σ , ∂
∂σ

= 2
σ 2 .
(3.37)
As the Fisher metric is invariant under diffeomorphisms of Ω = R, and since x →
x −μ is such a diffeomorphism, it sufﬁces to perform the computation at μ = 0.
The metric computed there, however, up to a simple scaling is the hyperbolic metric
of the half-plane
H :=

(μ,σ) : μ ∈R,σ > 0

,
and so, the Fisher metric on the family of normal distributions is the hyperbolic
metric.
Let us summarize some points that will be important for the sequel. We have
constructed the Fisher metric as the natural Riemannian metric in the space of lines
of (ﬁnite non-negative) measures, i.e., on a projective space over a linear space. In
the ﬁnite case, this projective space is simply a spherical sector. In particular, our
metric is then the standard metric on the sphere, and it therefore has sectional cur-
vature κ ≡1 (or, more precisely, 1
4 if we utilize the sphere of radius 2, to get the
normalizations right). This, in fact, carries over to the general case (see [99] for
an explicit computation). Therefore, the Fisher metric is not Euclidean. By way of
contrast, our space of probability measures can be viewed as a linear space in two
different manners. On the one hand, as in the ﬁnite case, it can be represented as

134
3
Parametrized Measure Models
a simplex in a vector space. Thus, any probability measure can be represented as a
convex linear combination of certain extremal measures. More precisely, when Ω
is a metrizable topological space, then the map Ω →P(Ω), x →δ(x) that assigns
to every x ∈Ω the delta measure supported at x is an embedding. If Ω is also sep-
arable, then the image is a closed subspace of P(Ω). Also, in this case, this image
contains precisely the extreme points of the convex set P(Ω). See [5], Sect. 15.2,
for details.
We shall call this representation as a convex linear combination of extremal mea-
sures a mixture representation. On the other hand, our space of probability mea-
sures can be represented as the exponential image of a linear tangent space. This
gives the so-called exponential representation. We shall see below that these two
linear structures are dual to each other, in the sense that each of them is the under-
lying afﬁne structure for some connection, and the two corresponding connections
are dual with respect to the Fisher metric. Of course, neither of these connections
can be the Levi-Civita connection of the Fisher metric as the latter does not have
vanishing curvature.
The Fisher metric also allows the following construction: If Σ is a set with a
measure σ(u) on it, and if we have a mapping
h : Σ →P(Ω),
i.e., if we have a family of measures on Ω parametrized by u ∈Σ, we may then
consider variations
h(u;s) : Σ × (−ε,ε) →P(Ω),
e.g.,
h(u;s) = exph(u;0) sϕ(u)
for some function ϕ.
If we have two such variations ϕ1, ϕ2, we can use the Fisher metric and the
measure σ to form their L2-product

Σ
Eh(u)

ϕ1,ϕ2
σ(du)
=

Σ

Ω
ϕ1(u)(x) ϕ2(u)(x) h(u)(dx)

σ(du).
In other words, we integrate the Fisher product with respect to the measure on our
family of measures. The Fisher metric can formally be considered as a special case
of this construction, namely when Σ is a singleton. The general construction allows
us to average over a family of measures. For example, if we have a Markov pro-
cess with transition probability p(·|y) for each y ∈Ω, we consider this as a family
h : Ω →P(Ω), with h(y) = p(·|y), and we may average the Fisher products taken
with respect to the measures p(·|y) with respect to some initial probability distribu-
tion p0(y) for y. Thus, if we have families of such transition probabilities p(·|y;s),

3.2
Parametrized Measure Models
135
we get the averaged Fisher product
gij(s) :=
 
∂
∂si logp(·|y;s) ∂
∂sj logp(·|y;s)dx

p0(dy).
(3.38)
Under sufﬁcient conditions, this metric is usually obtained from the distributions
p(n)(x0,x1,...,xn;s) := p0(x0;s)p(x1|x0;s)···p(xn|xn−1;s),
n = 0,1,2,...
Denoting the Fisher metric for s →p(n)(·;s) by g(n)
ij , we can consider the limit
gij(s) := lim
n→∞
1
n g(n)
ij (s),
(3.39)
if it exists. A sufﬁcient condition for the existence of this limit is given by the sta-
tionarity of the process. In that case, the metric g deﬁned by (3.39) reduces to the
metric g deﬁned by (3.38). We shall take up this issue in Sect. 6.1.
3.2 Parametrized Measure Models
In this section, we shall give the formal deﬁnitions of parametrized measure mod-
els, providing solutions to some of the issues described in Sect. 3.1, improving
upon [26]. First of all, the intuitive deﬁnition of a statistical model is to regard it as
a family p(ξ)ξ∈M of probability measures on some sample space Ω which varies in
a differentiable fashion with ξ ∈M. To make this formal, we need to provide some
kind of differentiable structure on the space P(Ω) of probability measures. This is
done by noting that P(Ω) is contained in the Banach space S(Ω) of ﬁnite signed
measures on Ω, provided with the Banach norm ∥·∥T V of total variation from (3.1).
Therefore, we shall regard a statistical model as a C1-map between Banach man-
ifolds p : M →S(Ω), as described in Appendix C, whose image is contained in
P(Ω).
Since P(Ω) may also be regarded as the projectivization of the space of ﬁnite
measures M(Ω) via rescaling, any C1-map p : M →M(Ω) induces a statistical
model p0 : M →P(Ω) by
p0(ξ) =
p(ξ)
∥p(ξ)∥T V
.
(3.40)
It is often more convenient to study C1-maps p : M →M(Ω), called parametrized
measure models, and then use (3.40) to obtain a statistical model p0. In case we
consider C1-maps p : M →S(Ω), also allowing for non-positive measures, we call
it a signed parametrized measure model.
Let us assume for simplicity that all measures p(ξ) are dominated by some ﬁxed
measure μ0, even though later we shall show that this assumption is inessential. As

136
3
Parametrized Measure Models
it turns out, a dominating measure μ0 exists if M contains a countable dense subset
which is the case, e.g., if M is a ﬁnite-dimensional manifold. In this case,
p(ξ) = p(·;ξ)μ0,
where p : Ω × M →R is called the density function of p w.r.t. μ0. Evidently,
p(·;ξ) ∈L1(Ω,μ0) for all ξ. The differential of p in the direction of a tangent
vector V ∈TξM is then given by
dξp(V ) = ∂V p(·;ξ)μ0 ∈L1(Ω,μ0),
where for simplicity we assume that the partial derivative of the density function
exists, even though we shall show that this condition may be weakened as well.
Attempting to deﬁne an inner product on TξM analogous to the Fisher metric
in (2.18), we have to regard dξp(V ) as an element of Tp(ξ)S(Ω) ∼= L1(Ω,p(ξ)),
which leads to
gξ(V,W) =

dξp(V ),dξp(W)

p(ξ)
=

Ω
d{dξp(V )}
dp(ξ)
d{dξp(W)}
dp(ξ)
dp(ξ)
(3.41)
=

Ω
∂V p(·;ξ)
p(·;ξ)
∂Wp(·;ξ)
p(·;ξ)
dp(ξ)
=

Ω
∂V logp(·;ξ) ∂W logp(·;ξ) dp(ξ).
We immediately encounter two problems. The ﬁrst one is that—unlike in the
case of a ﬁnite sample space Ω—the above integral may diverge, if we only assume
that ∂V logp(·;ξ) ∈L1(Ω,p(ξ)); rather, we should demand that ∂V logp(·;ξ) ∈
L2(Ω,p(ξ)) which in the case of an inﬁnite sample space Ω is a proper subset.
The second problem is that the functions logp(·;ξ) used in (3.41) to deﬁne the
Fisher metric are not deﬁned if we drop the assumption that p > 0, i.e., that all
measures have the same null sets as μ0. This is the reason why in most deﬁni-
tions of differentiable families of measures the equivalence of the measures p(ξ) in
the family and hence the positivity of the density function p is required; cf., e.g.,
[9, 16, 25, 219]. For instance, if Ω is a ﬁnite sample space, then the description of the
Fisher metric on M(Ω) or on P(Ω) in its canonical coordinates develops singular-
ities outside the sets M+(Ω) and P+(Ω), respectively, cf. (2.13) and (2.19). How-
ever, if we use the coordinates (√pi(ξ))i∈I instead, then this metric coincides—
up to a constant factor—with the standard inner product on Euclidean space and
hence extends to all of M(Ω) and P(Ω), respectively, cf. Proposition 2.1. That
is, the seeming degeneracy of the Fisher metric near the boundary of P+(Ω,μ0)
is only due to an inconvenient choice of coordinates, while with the right choice
(√pi(ξ))i∈I it becomes the Euclidean metric on the sphere. Formulating it slightly
differently, the point is that not all the individual factors under the integral in (3.41)
need to be well-deﬁned, but it sufﬁces that their product is.

3.2
Parametrized Measure Models
137
Generalizing this approach, let us introduce half-densities, by which we mean
formal square roots of measures. That is, we let

p(ξ) :=

p(·;ξ)√μ0,
where for the moment we regard √μ0 merely as a formal symbol. Taking derivatives
of this yields
dξ
√p(V ) = 1
2
∂V p(·;ξ)
√p(·;ξ)
√μ0 = 1
2∂V logp(·;ξ)

p(ξ),
whence
dξ
√p(V ) · dξ
√p(W) = 1
4∂V logp(·;ξ)∂W logp(·;ξ)p(·;ξ)μ0,
and so
gξ(V,W) = 4

Ω
d

dξ
√p(V ) · dξ
√p(W)

,
as in (3.22). Analogously, if we deﬁne
3√p(ξ) :=
3√p(·;ξ) 3√μ0, again regarding
3√μ0 as a formal symbol, then
dξ 3√p(V ) = 1
3
∂V p(·;ξ)
√p(·;ξ)2
3√μ0 = 1
3∂V logp(·;ξ) 3
p(ξ),
so that the Amari–Chentsov tensor T from (2.51) can be written as
Tξ(V,W,U) =

Ω
∂V logp(·;ξ)∂W logp(·;ξ)∂U logp(·;ξ) dp(ξ)
= 27

Ω
d

dξ
3
p(ξ)(V ) · dξ
3
p(ξ)(W) · dξ
3
p(ξ)(U)

. (3.42)
This suggests that we should try to make formal sense of the objects √μ0,
3√μ0
and hence of p(ξ)1/2 = √p(ξ), p(ξ)1/3 =
3√p(ξ), etc. The idea of taking rth powers
of a measure p(ξ) with 0 < r < 1 has been introduced in a less rigorous way by
Amari in [8, p. 66], where such powers are called α-representations.
We shall use a more formal approach and for 0 < r ≤1 construct the Banach
spaces Sr(Ω) in Sect. 3.2.3 by a certain direct limit construction, as well as the
subsets
Pr(Ω)⊆Mr(Ω)⊆Sr(Ω).
The elements of these sets are called rth powers of probability measures (ﬁnite
measures, signed ﬁnite measures, respectively), and they have the feature that one
can formally take the (1/r)th power of them to obtain a probability measure (ﬁnite
measure, signed ﬁnite measure, respectively), and raising to the (1/r)th power is a
C1-regular map between Banach spaces.

138
3
Parametrized Measure Models
Once we have done this, we call a parametrized measure model k-integrable if
the map
p1/k : M −→M1/k(Ω)⊆S1/k(Ω),
ξ −→p(ξ)1/k
(3.43)
is a C1-map as deﬁned in Appendix C. If p(ξ) = p(·;ξ)μ0 is dominated by μ0, then
the differential of this map is given by
∂V p1/k = dξp1/k(V ) := 1
k ∂V logp(·;ξ)p1/k(ξ),
and in order for this to be the kth root of a measure, we have to require that
∂V logp(·;ξ) ∈Lk(Ω,p(ξ)).
We call such a model weakly k-integrable if the map p1/k : M →M1/k(Ω)
is a weak C1-map, cf. Appendix C for a deﬁnition. As we shall show in Theo-
rem 3.2, the model is k-integrable if and only if the k-norm V →∥∂V p1/k∥k de-
pends continuously on V ∈T M, and it is weakly k-integrable if and only if the map
V →∂V p1/k is weakly continuous. Thus, our deﬁnition of k-integrability coincides
with that given in [25, Deﬁnition 2.4].
In general, on an n-integrable parametrized measure model we may deﬁne the
canonical n-tensor

τ n
M

ξ(V1,...,Vn) =

Ω
∂V1 logp(·;ξ)···∂Vn logp(·;ξ) dp(ξ),
(3.44)
which is a generalization of the Fisher metric g = τ 2
M (3.41) and the Amari–
Chentsov tensor T = τ 3
M (3.42). In fact, we show that τ n
M is the pullback of a
naturally deﬁned covariant n-tensor Ln
Ω on S1/n(Ω) via the map p1/n : M →
M1/n(Ω)⊆S1/n(Ω), where k ≥n. In particular, τ n
M := (p1/n)∗Ln
Ω is well deﬁned
for a k-integrable parametrized measure model with k ≥n, even if p is not a positive
function, in which case (3.44) has to be interpreted with care.
While for most applications it will sufﬁce to consider statistical models which are
dominated by some measure μ0, our development of the theory will show that this
is an inessential condition. Intuitively, it is plausible that the quantity ∂V p(·;ξ)μ0
which measures the change of measure w.r.t. the background measure μ0 is not
a signiﬁcant quantity, but rather the rate of change of p(ξ) relative to the measure
p(ξ) itself. That is, the relevant quantity to consider as a derivative is the logarithmic
derivative
∂V logp(·;ξ) = d{dξp(V )}
dp(ξ)
,
where the fraction stands for the Radon–Nikodym derivative. An important obser-
vation is that for any parametrized measure model, this Radon–Nikodym derivative
always exists, so that the logarithmic derivative ∂V logp(ξ) may be deﬁned also in
the absence of a dominating measure μ0. This is all that is needed to deﬁne the
notions of k-integrability and the canonical tensors mentioned above.

3.2
Parametrized Measure Models
139
3.2.1 The Structure of the Space of Measures
The aim of this section is to provide the formal set-up of parametrized measure
models in order to make the discussion in the preceding section rigorous. As before,
let Ω be a measurable space. We let
P(Ω) := {μ : μ a probability measure on Ω},
M(Ω) := {μ : μ a ﬁnite measure on Ω},
S(Ω) := {μ : μ a signed ﬁnite measure on Ω},
S0(Ω) :=

μ ∈S(Ω) :

Ω
dμ = 0

.
(3.45)
Clearly, P(Ω)⊆M(Ω)⊆S(Ω), and S0(Ω),S(Ω) are real vector spaces. In fact,
both S0(Ω) and S(Ω) are Banach spaces whose norm is given by the total variation
∥· ∥T V (3.1), whence any subset carries a canonical topology which is determined
by saying that a sequence (νn)n∈N in (a subset of) S(Ω) converges to ν∞if and only
if
lim
n→∞∥νn −ν∞∥T V = 0.
With respect to this topology, the subsets
P(Ω)⊆M(Ω)⊆S(Ω)
are closed.
Remark 3.2 Evidently, for the applications we have in mind, we are interested
mainly in statistical models. However, we can take the point of view that P(Ω) =
P(M(Ω)) is the projectivization of M(Ω) via rescaling. Thus, given a parametrized
measure model (M,Ω,p), normalization yields a statistical model (M,Ω,p0) de-
ﬁned by
p0(ξ) :=
p(ξ)
∥p(ξ)∥T V
,
which is again a C1-map. Indeed, the map μ →∥μ∥T V on M(Ω) is a C1-map,
being the restriction of the linear (and hence differentiable) map μ →

Ω dμ on
S(Ω).
Observe that while S(Ω) is a Banach space, the subsets M(Ω) and P(Ω) do
not carry a canonical manifold structure.
By the Jordan decomposition theorem, each measure μ ∈S(Ω) can be decom-
posed uniquely as
μ = μ+ −μ−
with μ± ∈M(Ω), μ+ ⊥μ−.
(3.46)

140
3
Parametrized Measure Models
The latter means that we have a disjoint union Ω = P ⊎N with μ+(N) =
μ−(P) = 0. Thus, if we deﬁne
|μ| := μ+ + μ−∈M(Ω),
then (3.46) implies
μ(A)
 ≤|μ|(A)
for all μ ∈S(Ω) and A ∈Σ,
(3.47)
so that
∥μ∥T V =
 |μ|

T V = |μ|(Ω).
In particular,
P(Ω) =

μ ∈M(Ω) : ∥μ∥T V = 1

.
Moreover, ﬁxing a measure μ0 ∈M(Ω), we deﬁne
M(Ω,μ0) =

μ = φ μ0 : φ ∈L1(Ω,μ0), φ ≥0

,
M+(Ω,μ0) =

μ = φ μ0 : φ ∈L1(Ω,μ0), φ > 0

,
P(Ω,μ0) =

μ ∈M(Ω,μ0) :

Ω
dμ = 1

,
P+(Ω,μ0) =

μ ∈M+(Ω,μ0) :

Ω
dμ = 1

,
S(Ω,μ0) =

μ = φ μ0 : φ ∈L1(Ω,μ0)

.
(3.48)
By the Radon–Nikodym theorem, P(Ω,μ0)⊆M(Ω,μ0)⊆S(Ω,μ0) consist of
those measures in P(Ω)⊆M(Ω)⊆S(Ω) which are dominated by μ0, and the
canonical isomorphism, ican : S(Ω,μ0) →L1(Ω,μ0) in (3.2) given by taking the
Radon–Nikodym derivative w.r.t. μ0 yields an isomorphism whose inverse is given
by
ı−1
can : L1(Ω,μ0) −→S(Ω,μ0),
φ −→φ μ0.
Observe that ıcan is an isometry of Banach spaces, since evidently
∥φ∥L1(Ω,μ0) =

Ω
|φ| dμ0 = ∥φ μ0∥T V .
3.2.2 Tangent Fibration of Subsets of Banach Manifolds
In this section, we shall use the notion of differentiable maps between Banach
spaces, as described in Appendix C. In particular, a curve in a Banach space

3.2
Parametrized Measure Models
141
(V ;∥· ∥) is a differentiable map c : I →V with an (open) interval I⊆R. That is,
for each t0 ∈I there exists the limit
d
dt

t=t0
c(t) = ˙c(t0) := lim
t→t0
c(t) −c(t0)
t −t0
.
(3.49)
Deﬁnition 3.2
Let (V ;∥· ∥) be a Banach space, X⊆V an arbitrary subset and
x0 ∈X. Then v ∈V is called a tangent vector of X at x0, if there is a curve c :
(−ε,ε) →X⊆V such that c(0) = x0 and ˙c(0) = v.
The set of all tangent vectors at x0 is called the tangent double cone of X at x0
and is denoted by Tx0X. We also deﬁne the tangent ﬁbration of X as
T X :=
%
x0∈X
Tx0X⊆X × V ⊆V × V,
equipped with the induced topology and with the canonical projection map
T X →X.
Remark 3.3 The reader should be aware that, unlike in some texts, we do not use
tangent ﬁbration as a synonym for the tangent bundle, since for general subsets
X⊆V , Tx0X⊆V may fail to be a vector subspace, and for x0 ̸= x1, the tangent
cones Tx0X and Tx1X need not be homeomorphic.
For instance, let X := {(x,y)⊤∈R2 : xy = 0}⊆R2, so that X is the union of the
two coordinate axes. Then T(x,0)X and T(0,y)X with x,y ̸= 0 are the x-axis and the
y-axis, respectively, and hence linear subspaces of V = R2, but T(0,0)X = X is not a
subspace. This example also shows that Tx0X and Tx1X need not be homeomorphic
if x0 ̸= x1, whence the projection T X →X mapping Tx0X to x0 is in general only
a topological ﬁbration, but not a vector bundle or a ﬁber bundle.
But at least, Tx0X is invariant under multiplication by (positive or negative)
scalars and hence is a double cone. This is seen by replacing the curve c in Def-
inition 3.2 by ˜c(t) := c(t0t) ∈X for some t0 ∈R, so that ˜c(0) = x0 and ˙˜c(0) = t0v ∈
Tx0X.
If X⊆V is a submanifold, however, then Tx0X and T X coincide with the standard
notion of the tangent space at x0 and the tangent bundle of X, respectively. Thus,
in this case the tangent cone is a linear subspace, and the tangent ﬁbration is the
tangent bundle of X.
For instance, if X = U⊆V is an open set, then Tx0U = V for all x0 and hence,
T U = U × V . Indeed, the curve c(t) = x0 + tv ∈U for small |t| satisﬁes the prop-
erties required in the Deﬁnition 3.2. In this case, the tangent ﬁbration U × V →U
is a (trivial) vector bundle.
If M is a Banach manifold and F : M →V is a C1-map whose image is
contained in X⊆V , then by the chain rule, for any v ∈Tx0M and any curve
c : (−ε,ε) →M with c(0) = x0, ˙c(0) = v,
d
dt

t=0
F

c(t)

= dx0F(v),

142
3
Parametrized Measure Models
and as t →F(c(t)) is a curve in X with F(c(0)) = F(x0), it follows that dx0F(v) ∈
TF(x0)X for all v ∈T M. That is, a C1-map F : M →X⊆V induces a continuous
map
dF : T M −→T X,
(x0,v) −→dx0F(v).
Theorem 3.1 Let V = S(Ω) be the Banach space of ﬁnite signed measures on Ω.
Then the tangent cones of M(Ω) and P(Ω) at μ are TμM(Ω) = S(Ω,μ) and
TμP(Ω) = S0(Ω,μ), respectively, so that
T M(Ω) =
%
μ∈M(Ω)
S(Ω,μ)⊆M(Ω) × S(Ω)
and
T P(Ω) =
%
μ∈P(Ω)
S0(Ω,μ)⊆P(Ω) × S(Ω).
Proof Let ν ∈Tμ0M(Ω) and let (μt)t∈(−ε,ε) be a curve in M(Ω) with ˙μ0 = ν. Let
A⊆Ω be such that μ0(A) = 0. Then as μt(A) ≥0, the function t →μt(A) has a
minimum at t = 0, whence
0 = d
dt

t=0
μt(A) = ˙μ0(A) = ν(A),
where the second equation is evident from (3.49). That is, ν(A) = 0 whenever
μ0(A) = 0, i.e., μ0 dominates ν, so that ν ∈S(Ω,μ0). Thus, Tμ0M(Ω)⊆S(Ω,μ0).
Conversely, given ν = φμ0 ∈S(Ω,μ0), deﬁne μt := p(ω;t)μ0 where
p(ω;t) :=

1 + tφ(ω)
if tφ(ω) ≥0,
exp(tφ(ω))
if tφ(ω) < 0.
As p(ω;t) ≤max(1 + tφ(ω),1), it follows that μt ∈M(Ω), and as the derivative
∂tp(ω;t) exists for all t and its absolute value is bounded by |φ| ∈L1(Ω,μ0),
it follows that t →μt is a C1-curve in M(Ω) with ˙μ0 = φμ0 = ν, whence ν ∈
Tμ0M(Ω) as claimed.
To show the statement for P(Ω), let (μt)t∈(−ε,ε) be a curve in P(Ω) with
˙μ0 = ν. Then as μt is a probability measure for all t, we conclude that


Ω
dν
 =


Ω
1
t d(μt −μ0 −tν)
 ≤∥μt −μ0 −tν∥T V
|t|
t→0
−−→0,
so that ν ∈S0(Ω). Since P(Ω)⊆M(Ω), it follows that Tμ0P(Ω)⊆Tμ0M(Ω) ∩
S0(Ω) = S0(Ω,μ0) for all μ0 ∈P(Ω).
Conversely, given ν = φμ0 ∈S0(Ω,μ0), deﬁne the curve λt := μt∥μt∥−1
T V ∈
P(Ω) with μt from above, which is a C1-curve in P(Ω) as ∥μt∥T V > 0, and it is
straightforward that λ0 = μ0 and ˙λ0 = φμ0 = ν.
□

3.2
Parametrized Measure Models
143
Remark 3.4
(1) Even though the tangent cones of the subsets P(Ω)⊆M(Ω)⊆S(Ω) at each
point μ are closed vector subspaces of S(Ω), these subsets are not Banach
manifolds, and hence in particular not Banach submanifolds of S(Ω). This is
due to the fact that the spaces S0(Ω,μ) need not be isomorphic to each other
for different μ.
This can already be seen for ﬁnite Ω = {ω1,...,ωk}. In this case, we may
identify S(Ω) with Rk by the map 	k
i=1 xiδωi ∼= (x1,...,xk), and with this,
T M(Ω) ∼=

(x1,...,xk;y1,...,yk) ∈Rk × Rk : xi ≥0,
xi = 0 ⇒yi = 0

,
and this is evidently not a submanifold of R2k. Indeed, in this case the dimension
of TμM(Ω) = S(Ω,μ) equals |{ω ∈Ω | μ(ω) > 0}|, which varies with μ. Of
course, this simply reﬂects the geometric stratiﬁcation of the closed probability
simplex in terms of the faces of various dimensions. Theorem 3.1 then describes
such a stratiﬁcation also in inﬁnite dimensions.
(2) Observe that the curves μt and λt, respectively, used in the proof of Theo-
rem 3.1 are contained in M+(Ω,μ0) and P+(Ω,μ0), respectively, whence it
also follows that
Tμ0M+(Ω,μ0) = S(Ω,μ0)
and
Tμ0P+(Ω,μ0) = S0(Ω,μ0).
But if μ1 ∈M+(Ω,μ0), then μ1 and μ0 are compatible measures (i.e.,
they have the same null sets), whence in this case, S(Ω,μ0) = S(Ω,μ1)
and S0(Ω,μ0) = S0(Ω,μ1). That is, the subset M+(Ω,μ0)⊆S(Ω,μ0)
has at each point all of S(Ω,μ0) as its tangent space, but in general,
M+(Ω,μ0)⊆S(Ω,μ0) is not open.4 This is a quite remarkable and unusual
phenomenon in Differential Geometry.
That is, neither on M(Ω) nor on M+(Ω,μ0) is there a canonical manifold
structure in general, and the same is true for P(Ω) and P+(Ω,μ0), respectively.
Nevertheless, Deﬁnition 3.2 and Theorem 3.1 allow us to speak of the tangent
ﬁbration of M(Ω) and P(Ω), respectively.
3.2.3 Powers of Measures
Let us now give the formal deﬁnition of roots of measures. On the set M(Ω) we
deﬁne the preordering μ1 ≤μ2 if μ2 dominates μ1. Then (M(Ω),≤) is a directed
set, meaning that for any pair μ1,μ2 ∈M(Ω) there is a μ0 ∈M(Ω) dominating
both of them (e.g., μ0 := μ1 + μ2).
4More precisely, M+(Ω,μ0) is not open unless Ω is the disjoint union of ﬁnitely many μ0-atoms,
where A⊆Ω is a μ0-atom if for each B⊆A either B or A\B is a μ0-null set.

144
3
Parametrized Measure Models
For ﬁxed r ∈(0,1] and measures μ1 ≤μ2 on Ω we deﬁne the linear embedding
ıμ1
μ2 : L1/r(Ω,μ1) −→L1/r(Ω,μ2),
φ −→φ
dμ1
dμ2
r
.
Observe that
ıμ1
μ2 (φ)

1/r =

Ω
ıμ1
μ2 (φ)
1/r dμ2
r
=

Ω
|φ|1/r dμ1
dμ2
dμ2
r
=

Ω
|φ|1/r dμ1
r
= ∥φ∥1/r,
(3.50)
so that ıμ1
μ2 is an isometry. Moreover, ıμ1
μ2 ıμ2
μ3 = ıμ1
μ3 whenever μ1 ≤μ2 ≤μ3. Then
we deﬁne the space of rth roots of measures on Ω to be the directed limit over the
directed set (M(Ω),≤)
Sr(Ω) := lim
−→L1/r(Ω,μ).
(3.51)
Let us give a more concrete deﬁnition of Sr(Ω). On the disjoint union of the
spaces L1/r(Ω,μ) for μ ∈M(Ω) we deﬁne the equivalence relation
L1/r(Ω,μ1) ∋φ ∼ψ ∈L1/r(Ω,μ2)
⇐⇒
ıμ1
μ0 (φ) = ıμ2
μ0 (ψ)
⇐⇒
φ
dμ1
dμ0
r
= ψ
dμ2
dμ0
r
for some μ0 ≥μ1,μ2. Then Sr(Ω) is the set of all equivalence classes of this
relation, cf. Fig. 3.1.
Let us denote the equivalence class of φ ∈L1/r(Ω,μ) by φμr, so that μr ∈
Sr(Ω) is the equivalence class represented by 1 ∈L1/r(Ω,μ). Then the equiva-
lence relation yields
μr
1 =
dμ1
dμ2
r
μr
2
as elements of Sr(Ω)
(3.52)
whenever μ1 ≤μ2, justifying this notation. In fact, from this description in the case
r = 1 we see that
S1(Ω) = S(Ω).
Observe that by (3.50) ∥φ∥1/r is constant on equivalence classes, whence there is a
norm on Sr(Ω), also denoted by ∥· ∥1/r, for which the inclusions
Sr(Ω,μ) →Sr(Ω)
and
L1/r(Ω,μ) −→Sr(Ω),
φ −→φμr
(3.53)
are isometries. For r = 1, we have ∥· ∥1 = ∥· ∥T V . Thus,
φμr
1/r = ∥φ∥1/r =

Ω
|φ|1/r dμ
r
for 0 < r ≤1.
(3.54)

3.2
Parametrized Measure Models
145
Fig. 3.1 Natural description of Sr(Ω) in terms of powers of the Radon–Nikodym derivative
Note that the equivalence relation also preserves non-negativity of functions,
whence we may deﬁne the subsets
Mr(Ω) :=

φμr : μ ∈M(Ω),φ ≥0

,
Pr(Ω) :=

φμr : μ ∈P(Ω),φ ≥0,∥φ∥1/r = 1

.
(3.55)
In analogy to (3.48) we deﬁne for a ﬁxed measure μ0 ∈M(Ω) and r ∈(0,1] the
spaces
Sr(Ω,μ0) :=

φ μr
0 : φ ∈L1/r(Ω,μ0)

,
Mr(Ω,μ0) :=

φ μr
0 : φ ∈L1/r(Ω,μ0),φ ≥0

,
Pr(Ω,μ0) :=

φ μr
0 : φ ∈L1/r(Ω,μ0),φ ≥0,∥φ∥1/r = 1

,
Sr
0(Ω,μ0) :=

φμr
0 : φ ∈L1/r(Ω,μ0),

Ω
φ dμ = 0

.
The elements of Pr(Ω,μ0),Mr(Ω,μ0),Sr(Ω,μ0) are said to be dominated
by μr
0.
Remark 3.5 The concept of rth roots of measures has been indicated in [200,
Ex. IV.1.4]. Moreover, if Ω is a manifold and r = 1/2, then S1/2(Ω) is even a
Hilbert space which has been considered in [192, 6.9.1]. This Hilbert space is also
related to the Hilbert manifold of ﬁnite-entropy probability measures deﬁned in
[201].

146
3
Parametrized Measure Models
The product of powers of measures can now be deﬁned for all r,s ∈(0,1) with
r + s ≤1 and for measures φμr ∈Sr(Ω,μ) and ψμs ∈Ss(Ω,μ):

φμr
·

ψμs
:= φψμr+s.
By deﬁnition φ ∈L1/r(Ω,μ) and ψ ∈L1/s(Ω,μ), whence Hölder’s inequality
implies that ∥φψ∥1/(r+s) ≤∥φ∥1/r∥ψ∥1/s < ∞, so that φψ ∈L1/(r+s)(Ω,μ) and
hence, φψμr+s ∈Sr+s(Ω,μ). Since by (3.52) this deﬁnition of the product is in-
dependent of the choice of representative μ, it follows that it induces a bilinear
product
· : Sr(Ω) × Ss(Ω) −→Sr+s(Ω),
where r,s,r + s ∈(0,1],
(3.56)
satisfying the Hölder inequality
∥νr · νs∥1/(r+s) ≤∥νr∥1/r∥νs∥1/s,
(3.57)
so that the product in (3.56) is a bounded bilinear map.
Deﬁnition 3.3 (Canonical pairing) For r ∈(0,1) we deﬁne the pairing
(·;·) : Sr(Ω) × S1−r(Ω) −→R,
(ν1;ν2) :=

Ω
d(ν1 · ν2).
(3.58)
It is straightforward to verify that this pairing is non-degenerate in the sense that
(νr;·) = 0 ⇐⇒νr = 0.
(3.59)
Lemma 3.1 (Cf. [200, Ex. IV.1.3]) Let {νn : n ∈N}⊆S(Ω) be a countable family
of (signed) measures. Then there is a measure μ0 ∈M(Ω) dominating νn for all n.
Proof We assume w.l.o.g. that νn ̸= 0 for all n and deﬁne
μ0 :=
∞

n=1
1
2n∥νn∥T V
|νn|.
Since ∥νn∥T V = |νn|(Ω), it follows that this sum converges, so that μ0 ∈M(Ω) is
well deﬁned. Moreover, if μ0(A) = 0, then |νn|(A) = 0 for all n, showing that μ0
dominates all νn as claimed.
□
From Lemma 3.1, we can now conclude the following statement:
Any sequence in Sr(Ω) is contained in Sr(Ω,μ0) for some μ0 ∈M(Ω).
In particular, any Cauchy sequence in Sr(Ω) is a Cauchy sequence in
Sr(Ω,μ0) ∼= L1/r(Ω,μ0)
for
some
μ0
and
hence
convergent.
Thus,

3.2
Parametrized Measure Models
147
(Sr(Ω),∥· ∥1/r) is a Banach space. It also follows that Sr(Ω,μ0) is a closed
subspace of S(Ω) for all μ0 ∈M(Ω).
In analogy to Theorem 3.1, we can also determine the tangent cones of the sub-
sets Pr(Ω)⊆Mr(Ω)⊆Sr(Ω).
Proposition 3.1 For each μ ∈M(Ω) (μ ∈P(Ω), respectively), the tangent cone
of Pr(Ω)⊆Mr(Ω)⊆Sr(Ω) at μr are TμrMr(Ω) = Sr(Ω,μ) and TμrPr(Ω) =
Sr
0(Ω,μ), respectively, so that the tangent ﬁbrations are given as
T Mr(Ω) =
%
μr∈Mr(Ω)
Sr(Ω,μ)⊆Mr(Ω) × Sr(Ω)
and
T Pr(Ω) =
%
μr∈Pr(Ω)
Sr
0(Ω,μ)⊆Pr(Ω) × Sr(Ω).
Proof We have to adapt the proof of Theorem 3.1. The proof of the statements
Sr(Ω,μ)⊆TμrMr(Ω) and Sr
0(Ω,μ)⊆TμrPr(Ω) is identical to that of the corre-
sponding statement in Theorem 3.1; just as in that case, one shows that for φ ∈
L1/r(Ω,μ0) the curves μr
t := p(ω;t)μr
0 with p(ω;t) := 1 + tφ(ω) if tφ(ω) ≥0
and p(ω;ξ) = exp(tφ(ω)) if tφ(ω) < 0 is a differentiable curve in Mr(Ω), and
λr
t := μr
t /∥μr
t ∥1/r is a differentiable curve in Pr(Ω), and their derivative is φμr
0 at
t = 0.
In order to show the other direction, let (μr
t )t∈(−ε,ε) be a curve in Mr(Ω). Then
Q ∩(−ε,ε) is countable, whence by Lemma 3.1 there is a measure ˆμ such that
(μr
t ) ∈Mr(Ω, ˆμ) for all t ∈Q, and since Sr(Ω, ˆμ)⊆Sr(Ω) is closed, it follows
that μr
t ∈M(Ω, ˆμ) for all t. Now we can apply the argument from Theorem 3.1 to
the curve t →(μr
t · ˆμ1−r)(A) for A⊆Ω.
□
Remark 3.6 Just as in the case of r = 1, we may take two points of view on the
relation of Mr(Ω) and Pr(Ω). The one is that Pr(Ω) may be regarded as a subset
of Mr(Ω), but also, the normalization map
Mr(Ω) −→Pr(Ω),
μr −→
μr
∥μr∥1/r
allows us to regard Pr(Ω) as the projectivization P(Mr(Ω)). It will depend on the
context which point of view is better adapted.
Besides multiplying roots of measures, we also wish to take their powers. Here,
we have two possibilities for dealing with signs. For 0 < k ≤r−1 and νr = φμr ∈
Sr(Ω) we deﬁne
|νr|k := |φ|kμrk
and
˜νk
r := sign(φ)|φ|kμrk.
(3.60)

148
3
Parametrized Measure Models
Since φ ∈L1/r(Ω,μ), it follows that |φ|k ∈L1/kr(Ω,μ), so that |νr|k, ˜νk
r ∈
Srk(Ω). By (3.52) these powers are well deﬁned, independent of the choice of the
measure μ, and, moreover,
 |νr|k 
1/(rk) =
˜νk
r

1/(rk) = ∥νr∥k
1/r.
(3.61)
Proposition 3.2 Let r ∈(0,1] and 0 < k ≤1/r, and consider the maps
πk, ˜πk : Sr(Ω) −→Srk(Ω),
πk(ν) := |ν|k,
˜πk(ν) := ˜νk.
Then πk, ˜πk are continuous maps. Moreover, for 1 < k ≤1/r they are C1-maps
between Banach spaces, and their derivatives are given as
dνr ˜πk(ρr) = k |νr|k−1 · ρr
and
dνrπk(ρr) = k ˜νk−1
r
· ρr.
(3.62)
Observe that for k = 1, π1(νr) = |νr| fails to be C1, whereas ˜π1(νr) = νr, so that
˜π1 is the identity and hence a C1-map.
Proof Let us ﬁrst assume that 0 < k ≤1. We assert that for all x,y ∈R we have the
estimates
|x + y|k −|x|k ≤|y|k
and
sign(x + y)|x + y|k −sign(x)|x|k ≤21−k|y|k.
(3.63)
For k = 1, (3.63) is obvious. If 0 < k < 1, then by homogeneity it sufﬁces to show
these for y = 1. Note that the functions
x −→|x + 1|k −|x|k
and
x −→sign(x + 1)|x + 1|k −sign(x)|x|k
are continuous and tend to 0 for x →±∞, and then (3.63) follows by elementary
calculus.
Let ν1,ν2 ∈Sr(Ω), and choose μ0 ∈M(Ω) such that ν1,ν2 ∈Sr(Ω,μ0), i.e.,
νi = φiμr
0 with φi ∈L1/r(Ω,μ0). Then
πk(ν1 + ν2) −πk(ν1)

1/rk =
|φ1 + φ2|k −|φ1|k
1/rk
≤
 |φ2|k
1/rk
by (3.63)
= ∥ν2∥k
1/r
by (3.61),
so that lim∥ν2∥1/r→0 ∥πk(ν1 + ν2) −πk(ν1)∥1/rk = 0, showing the continuity of πk
for 0 < k ≤1. The continuity of ˜πk follows analogously.
Now let us assume that 1 < k ≤1/r. In this case, the functions
x −→|x|k
and
x −→sign(x)|x|k

3.2
Parametrized Measure Models
149
with x ∈R are C1-maps with respective derivatives
x −→k sign(x)|x|k−1
and
x −→k|x|k−1.
Thus, if we pick νi = φiμr
0 as above, then by the mean value theorem we have
πk(ν1 + ν2) −πk(ν1) =

|φ1 + φ2|k −|φ1|k
μrk
0
= k sign(φ1 + ηφ2)|φ1 + ηφ2|k−1φ2μrk
0
= k sign(φ1 + ηφ2)|φ1 + ηφ2|k−1μr(k−1)
0
· ν2
for some function η : Ω →(0,1). If we let νη := ηφ2μr
0, then ∥νη∥1/r ≤∥ν2∥1/r,
and we get
πk(ν1 + ν2) −πk(ν1) = k ˜πk−1(ν1 + νη) · ν2.
With the deﬁnition of dν1 ˜πk from (3.62) we have
πk(ν1 + ν2) −πk(ν1) −dν1πk(ν2)

1/(rk)
=
k

˜πk−1(ν1 + νη) −˜πk−1(ν1)

· ν2

1/(rk)
≤k
 ˜πk−1(ν1 + νη) −˜πk−1(ν1)

1/(r(k−1))∥ν2∥1/r
and hence,
∥πk(ν1 + ν2) −πk(ν1) −dν1πk(ν2)∥1
rk
∥ν2∥1
r
≤k
 ˜πk−1(ν1 + νη) −˜πk−1(ν1)

1
r(k−1) .
Thus, the differentiability of πk will follow if
 ˜πk−1(ν1 + νη) −˜πk−1(ν1)

1/(r(k−1))
∥ν2∥1/r→0
−−−−−−→0,
and because of ∥νη∥1/r ≤∥ν2∥1/r, this is the case if ˜πk−1 is continuous.
Analogously, one shows that ˜πk is differentiable if πk−1 is continuous.
Since we already know continuity of πk and ˜πk for 0 < k ≤1, and since C1-
maps are continuous, the claim now follows by induction on ⌈k⌉.
□
Thus, (3.62) implies that the differentials of πk and ˜πk (which coincide on
Pr(Ω) and Mr(Ω)) yield continuous maps
dπk = d ˜πk :
T Pr(Ω)
−→
T Prk(Ω)
T Mr(Ω)
−→
T Mrk(Ω),
(μ,ρ) −→kμrk−r · ρ.

150
3
Parametrized Measure Models
3.2.4 Parametrized Measure Models and k-Integrability
In this section, we shall now present our notion of a parametrized measure model.
Deﬁnition 3.4 (Parametrized measure model) Let Ω be a measurable space.
(1) A parametrized measure model is a triple (M,Ω,p) where M is a (ﬁnite or
inﬁnite-dimensional) Banach manifold and p : M →M(Ω)⊆S(Ω) is a C1-
map in the sense of Deﬁnition C.3.
(2) The triple (M,Ω,p) is called a statistical model if it consists only of probability
measures, i.e., such that the image of p is contained in P(Ω).
(3) We call such a model dominated by μ0 if the image of p is contained in
M(Ω,μ0). In this case, we use the notation (M,Ω,μ0,p) for this model.
If a parametrized measure model (M,Ω,μ0,p) is dominated by μ0, then there
is a density function p : Ω × M →R such that
p(ξ) = p(·;ξ)μ0.
(3.64)
Evidently, we must have p(·;ξ) ∈L1(Ω,μ0) for all ξ. In particular, for ﬁxed ξ,
p(·;ξ) is deﬁned only up to changes on a μ0-null set. The existence of a dominating
measure μ0 is not a strong restriction, as the following shows.
Proposition 3.3 Let (M,Ω,p) be a parametrized measure model. If M contains a
countable dense subset, e.g., if M is a ﬁnite-dimensional manifold, then there is a
measure μ0 ∈M(Ω) dominating the model.
Proof Let (ξn)n∈N⊆M be a dense countable subset. By Lemma 3.1, there is a
measure μ0 dominating all measures p(ξn) for n ∈N, i.e., p(ξn) ∈M(Ω,μ0).
If ξnk →ξ, so that p(ξnk) →p(ξ), then as the inclusion S(Ω,μ0) →S(Ω)
is an isometry by (3.53), it follows that (p(ξnk))k∈N is a Cauchy sequence in
S(Ω,μ0), and as the latter is complete, it follows that p(ξ) ∈S(Ω,μ0) ∩M(Ω) =
M(Ω,μ0).
□
Deﬁnition 3.5 (Regular density function)
Let (M,Ω,μ0,p) be a parametrized
measure model dominated by μ0. We say that this model has a regular density func-
tion if the density function p : Ω × M →R satisfying (3.64) can be chosen such
that for all V ∈TξM the partial derivative ∂V p(·;ξ) exists and lies in L1(Ω,μ0).
Remark 3.7
The standard notion of a statistical model always assumes that it is
dominated by some measure and has a positive regular density function (e.g., [9,
§2 , p. 25], [16, §2.1], [219], [25, Deﬁnition 2.4]). In fact, the deﬁnition of a
parametrized measure model or statistical model in [25, Deﬁnition 2.4] is equiv-
alent to a parametrized measure model or statistical model with a positive regular
density function in the sense of Deﬁnition 3.5. In contrast, in [26], the assumption

3.2
Parametrized Measure Models
151
of regularity and, more importantly, of the positivity of the density function p is
dropped.
It is worth pointing out that the density function p of a parametrized measure
model (M,Ω,μ0,p) does not need to be regular, so that the present notion is indeed
more general. The formal deﬁnition of differentiability of p implies that for each
C1-path ξ(t) ∈M with ξ(0) = ξ, ˙ξ(0) =: V ∈TξM, the curve t →p(·;ξ(t)) ∈
L1(Ω,μ0) is differentiable. This implies that there is a dξp(V ) ∈L1(Ω,μ0) such
that

p(·;ξ(t)) −p(·;ξ)
t
−dξp(V )(·)

1
t→0
−−−−−−→0.
If this is a pointwise convergence, then dξp(V ) = ∂V p(·;ξ) is the partial derivative
and whence, ∂V p(·;ξ) lies in L1(Ω,μ0), so that the density function is regular.
However, in general convergence in L1(Ω,μ0) does not imply pointwise conver-
gence, whence there are parametrized measure models in the sense of Deﬁnition 3.4
without a regular density function, cf. Example 3.2.3 below. Nevertheless, we shall
use the following notations interchangeably,
dξp(V ) = ∂V p = ∂V p(·;ξ) μ0,
(3.65)
even if p does not have a regular density function and the derivative ∂V p(·;ξ) does
not exist.
Example 3.2
(1) The family of normal distributions on R
p(μ,σ) :=
1
√
2πσ
exp

−(x −μ)2
2σ 2

dx
is a statistical model with regular density function on the upper half-plane H =
{(μ,σ) : μ,σ ∈R,σ > 0}.
(2) To see that there are parametrized measure models without a regular density
function, consider the family (p(ξ))ξ>−1 of measures on Ω = (0,π)
p(ξ) :=

(1 + ξ (sin2(t −1/ξ))1/ξ2) dt
for ξ ̸= 0,
dt
for ξ = 0.
This model is dominated by the Lebesgue measure dt, with density function
p(t;ξ) = 1 + ξ (sin2(t −1/ξ))1/ξ2 for ξ ̸= 0, p(t;0) = 1. Thus, the partial
derivative ∂ξp does not exist at ξ = 0, whence the density function is not regu-
lar.
On the other hand, p : (−1,∞) →M(Ω,dt) is differentiable at ξ = 0 with
d0p(∂ξ) = 0, so that (M,Ω,p) is a parametrized measure model in the sense of

152
3
Parametrized Measure Models
Deﬁnition 3.4. To see this, we calculate

p(ξ) −p(0)
ξ

T V
=

sin2(t −1/ξ)
1/ξ2
dt

1
=
 π
0

sin2(t −1/ξ)
1/ξ2
dt
=
 π
0

sin2 t
1/ξ2
dt
ξ→0
−−→0,
which shows the claim. Here, we used the π-periodicity of the integrand for
ﬁxed ξ and dominated convergence in the last step.
Since for a parametrized measure model (M,Ω,p) the map p is C1, it follows
that its derivative yields a continuous map between the tangent ﬁbrations
dp : T M −→T M(Ω) =
%
μ∈M(Ω)
S(Ω,μ).
That is, for each tangent vector V ∈TξM, its differential dξp(V ) is contained in
S(Ω,p(ξ)) and hence dominated by p(ξ). Therefore, we can take the Radon–
Nikodym derivative of dξp(V ) w.r.t. p(ξ).
Deﬁnition 3.6
Let (M,Ω,p) be a parametrized measure model. Then for each
tangent vector V ∈TξM of M, we deﬁne
∂V logp(ξ) := d{dξp(V )}
dp(ξ)
∈L1
Ω,p(ξ)

(3.66)
and call this the logarithmic derivative of p at ξ in the direction V .
If such a model is dominated by μ0 and has a positive regular density function p
for which (3.64) holds, then we calculate the Radon–Nikodym derivative as
d{dξp(V )}
dp(ξ)
= d{dξp(V )}
dμ0
·
dp(ξ)
dμ0
−1
= ∂V p(·;ξ)

p(·;ξ)
−1 = ∂V logp(·;ξ).
This justiﬁes the notation in (3.66) even for models without a positive regular density
function.
For a parametrized measure model (M,Ω,p) and k > 1 we consider the map
p1/k := π1/kp : M −→M1/k(Ω)⊆S1/k(Ω),
ξ −→p(ξ)1/k.
Since π1/k is continuous by Proposition 3.2, it follows that p1/k is continuous as
well. We deﬁne the following notions of k-integrability.

3.2
Parametrized Measure Models
153
Deﬁnition 3.7 (k-Integrable parametrized measure model)
A parametrized mea-
sure model (M,Ω,p) (statistical model, respectively) is called k-integrable if the
map
p1/k : M −→M1/k(Ω)⊆S1/k(Ω)
is a C1-map in the sense of Deﬁnition C.1. It is called weakly k-integrable if this
map is a weak C1-map, again in the sense of Deﬁnition C.1.
Furthermore, we call the model (weakly) ∞-integrable if it is (weakly) k-
integrable for all k ≥1.
Evidently, every parametrized measure model is 1-integrable by deﬁnition.
Moreover, since for 1 ≤l < k we have p1/l = πk/lp1/k and πk/l is a C1-map by
Proposition 3.2, it follows that (weak) k-integrability implies (weak) l-integrability
for 1 ≤l < k.
Example 3.3 An exponential family, as deﬁned by (3.31), generalizes the family of
normal distributions and represents an extremely important statistical model. It will
play a central role throughout this book.
Adapting (3.31) to the notation in this context, we can write it for ξ =
(ξ1,...,ξn) ∈U⊆Rn as
p(ξ) = exp

γ (ω) +
n

i=1
fi(ω)ξi −ψ(ξ)

μ(ω),
(3.67)
for suitable functions fi,γ on Ω and ψ on U. Therefore, for V = (v1,...,vn),
∂V p1/k(ξ) = 1
k

n

i=1
vifi(ω) + ∂V ψ(ξ)

× exp

γ (ω)/k +
n

i=1
fi(ω)ξi/k −ψ(ξ)/k

μ(ω)1/k,
and the k-integrability of this model for any k is easily veriﬁed from there. There-
fore, exponential families provide a class of examples of ∞-integrable parametrized
measure models. See also [216, p. 1559].
Remark 3.8
(1) In Sect. 2.8.1, we have introduced exponential families for ﬁnite sets as afﬁne
spaces. Let us comment on that structure for the setting of arbitrary measurable
spaces as described in [192]. Consider the afﬁne action (μ,f ) →ef μ, deﬁned
by (2.130). Clearly, multiplication of a ﬁnite measure μ ∈M+(Ω) with ef will
in general lead to a positive measure on Ω that is not ﬁnite but σ-ﬁnite. As we
restrict attention to ﬁnite measures, and thereby obtain the Banach space struc-
ture of S(Ω), it is not possible to extend the afﬁne structure of Sect. 2.8.1 to the

154
3
Parametrized Measure Models
general setting of measurable spaces. However, we can still deﬁne exponential
families as geometric objects that correspond to a “section” of an afﬁne space,
leading to the expression (3.31).
(2) Fukumizu [101] proposed the notion of a kernel exponential family, based on
the theory of reproducing kernel Hilbert spaces. In this approach, one considers
a Hilbert space H of functions Ω →R that is deﬁned in terms of a so-called
reproducing kernel k : Ω × Ω →R. In particular, the inner product on H is
given as
⟨f,g⟩H =
 
f (ω)g

ω′
k

ω,ω′
μ0(dω)μ0

dω′
.
We will revisit this product again in Sect. 6.4 (see (6.196)). Within this
framework, the sum 	n
i=1 fi(ω)ξi in (3.67) is then replaced by an integral

k(ω,ω′)ξ(ω′)μ0(dω′), leading to the deﬁnition of a kernel exponential fam-
ily.
Proposition 3.4
Let (M,Ω,p) be a (weakly) k-integrable parametrized measure
model. Then its (weak) derivative is given as
∂V p1/k(ξ) := 1
k ∂V logp(ξ) p1/k(ξ) ∈S1/k
Ω,p(ξ)

,
(3.68)
and for any functional α ∈S1/k(Ω)′ we have
∂V α

p(ξ)1/k
= α

∂V p1/k(ξ)

.
(3.69)
Observe that if p(ξ) = p(·;ξ)μ0 with a regular density function p, the derivative
∂V p1/k(ω;ξ) is indeed the partial derivative of the function p(ω;ξ)1/k.
Proof Suppose that the model is weakly k-integrable, i.e., p1/k is weakly differen-
tiable, and let V ∈TξM and α ∈S(Ω)′. Then
α

∂V logp(ξ)p(ξ)

= α(∂V p)
(3.66)
= α

∂V

πkp1/k
(3.62)
= α

kp(ξ)1−1/k · ∂V p1/k
,
whence
α

k∂V p1/k −∂V logp(ξ)p1/k
· p(ξ)1−1/k
= 0
for all α ∈S(Ω)′. On the other hand, ∂V logp ∈Tp1/k(ξ)M1/k(Ω) = S1/k(Ω,p(ξ))
according to Proposition 3.1, and from this, (3.68) follows.
The identity (3.69) is simply the deﬁnition of ∂V p1/k(ξ) being the weak Gâteaux-
derivative of p1/k, cf. Proposition C.2.
□
The following now gives a description of (weak) integrability in terms of the
(weak) derivative.

3.2
Parametrized Measure Models
155
Theorem 3.2 Let (M,Ω,p) be a parametrized measure model. Then the following
hold:
(1) The model is k-integrable if and only if the map
V −→
∂V p1/k
k
(3.70)
deﬁned on T M is continuous.
(2) The model is weakly k-integrable if and only if the weak derivative of p1/k is
weakly continuous, i.e., if for all V0 ∈T M
∂V p1/k ⇀∂V0p1/k
as
V →V0.
Remark 3.9 In [25, Deﬁnition 2.4], k-integrability (in case of a positive regular den-
sity function) was deﬁned by the continuity of the norm function in (3.70), whence
it coincides with Deﬁnition 3.7 by Theorem 3.2.
Our motivation for also introducing the more general deﬁnition of weak k-
integrability is that it is the weakest condition that ensures that integration and dif-
ferentiation of kth roots can be interchanged, as explained in the following.
If (M,Ω,μ0,p) has the density function p : Ω × M →R given by (3.64), then
for ξ ∈M and V ∈TξM we have
p1/k(ξ) = p(·;ξ)1/kμ1/k
0
and
∂V p1/k = ∂V p(·;ξ)1/kμ1/k
0
,
where
∂V p(·;ξ)1/k := 1
k
∂V p(·;ξ)
p(·;ξ)1−1/k ∈Lk(Ω,μ0).
(3.71)
Thus, if we let α(·) := (·;μ1−1/k
0
) with the canonical pairing from (3.58), then (3.69)
takes the form
∂V

A
p(ω;ξ)1/k dμ0(ω) =

A
∂V p(ω;ξ)1/k dμ0(ω).
(3.72)
Evidently, if p is a regular density function, then the weak partial derivative is
indeed the partial derivative of p, in which case (3.72) is obvious, as integration and
differentiation may be interchanged under these regularity conditions.
Example 3.4 For arbitrary k > 1, the following is an example of a parametrized
measure model which is l-integrable for all 1 ≤l < k, weakly k-integrable, but not
k-integrable.
Let Ω = (−1,1) with the Lebesgue measure dt, and let f : [0,∞) −→R be a
smooth function such that
f (u) > 0,f ′(u) < 0
for u ∈[0,1),
f (u) ≡0
for u ≥1.

156
3
Parametrized Measure Models
For ξ ∈R, deﬁne the measure p(ξ) = p(ξ;t) dt, where
p(ξ;t) :=
⎧
⎪⎨
⎪⎩
1
if t ≤0 and ξ ∈R arbitrary,
|ξ|k−1f (t|ξ|−1)k dt
if ξ ̸= 0 and t > 0,
0
otherwise.
Since on its restrictions to (−1,0] and (0,1) the density function p as well as pr
with 0 < r < 1 are positive with bounded derivative for ξ ̸= 0, it follows that p is
∞-integrable for ξ ̸= 0. For ξ = 0, we have
p(ξ) −p(0)

1 = |ξ|k−1
 |ξ|
0
f

t|ξ|−1k dt = |ξ|k
 1
0
f (u)k du,
whence, as k > 1,
lim
ξ→0
∥p(ξ) −p(0)∥1
|ξ|
= 0,
showing that p is also a C1-map at ξ = 0 with differential ∂ξp(0) = 0. That is,
(R,Ω,p) is a parametrized measure model with d0p = 0.
For 1 ≤l ≤k and ξ ̸= 0 we calculate
∂ξp1/l = χ(0,1)(t) ∂ξ

|ξ|(k−1)/lf

t|ξ|−1k/l
dt1/l
= χ(0,1)(t)sign(ξ)|ξ|(k−l−1)/l
k −1
l
f (u)k/l −u

f k/l′(u)

u=t|ξ|−1 dt1/l.
Thus, it follows that
∂ξp1/l(ξ)
l
l = |ξ|k−l
 1
0
k −1
l
f (u)k/l −u

f k/l′(u)
l
du,
so that
limξ→0
∂ξp1/l(ξ)

l = 0 =
∂ξp1/l(0)

l
for 1 ≤l < k,
limξ→0
∂ξp1/k(ξ)

k > 0 =
∂ξp1/k(0)

k.
That is, by Theorem 3.2 the model is l-integrable for 1 ≤l < k, but it fails to be
k-integrable. On the other hand,
∂ξp1/k(ξ) · dt1−1/k
1 = |ξ|1−1/k
 1
0

1 −1
k

f (u) −uf ′(u)

du,
so that limξ→0 ∥∂ξp1/k(ξ) · dt1−1/k∥1 = 0. As we shall show in Lemma 3.3 below,
this implies that ∂ξp1/k(ξ) ⇀0 = ∂ξp1/k(0) as ξ →0. Thus, the model is weakly
k-integrable by Theorem 3.2.

3.2
Parametrized Measure Models
157
The rest of this section will be devoted to the proof of Theorem 3.2 which is
somewhat technical and therefore will be divided into several lemmas.
Before starting the proof, let us give a brief outline of its structure.
We begin by proving the second statement of Theorem 3.2. Note that for a weak
C1-map the differential is weakly continuous by deﬁnition, so one direction of the
proof is trivial. The reverse implication is the content of Lemmata 3.2 through 3.6.
We give a decomposition of the dual space S1/k(Ω)′ in Lemma 3.2 and a sufﬁ-
cient criterion for the weak convergence of sequences in S1/k(Ω,μ0) in Lemma 3.3
as well as a criterion for weak k-integrability in terms of interchanging differentia-
tion and integration along curves in M in Lemma 3.4.
Unfortunately, we are not able to verify this criterion directly for an arbitrary
model p. The technical obstacle is that the measures of the family p(ξ) need not
be equivalent. We overcome this difﬁculty by modifying the model p to a model
pε(ξ) := p(ξ)+ εμ0, where ε > 0 and μ0 ∈M(Ω) is a suitable measure, so that pε
has a positive density function. Then we show in Lemma 3.5 that the differential of
pε remains weakly continuous, and ﬁnally in Lemma 3.6 we show that pε is weakly
k-integrable as it satisﬁes the criterion given in Lemma 3.4; furthermore it is shown
that taking the limit ε →0 implies the weak k-integrability of p as well, proving the
second part of Theorem 3.2.
The ﬁrst statement of Theorem 3.2 is proven in Lemmata 3.7 and 3.8. Again, one
direction is trivial: if the model is k-integrable, then its differential is continuous by
deﬁnition, whence so is its norm (3.70). That is, we have to show the converse.
In Lemma 3.7 we show that the continuity of the map (3.70) implies the weak
continuity of the differential of p1/k. This implies, on the one hand, that the model
is weakly k-integrable by the second part of Theorem 3.2 which was already shown,
and on the other hand, the Radon–Riesz theorem (cf. Theorem C.3) implies that
the differential of p1/k is even norm-continuous. Then in Lemma 3.8 we give the
standard argument that a weak C1-map with a norm-continuous differential must be
a C1-map, and this will complete the proof.
Lemma 3.2 For each μ0 ∈M(Ω), there is an isomorphism

S1/k(Ω)
′ ∼=

S1/k(Ω,μ0)
⊥⊕Sk/(k−1)(Ω,μ0),
(3.73)
where (S1/k(Ω,μ0))⊥denotes the annihilator of S1/k(Ω,μ0)⊆S1/k(Ω) and
where Sk/(k−1)(Ω,μ0) is embedded into the dual via the canonical pairing (3.58).
That is, we can write any α ∈(S1/k(Ω))′ uniquely as
α(·) =

·;φαμ1−1/k
0

+ βμ0(·),
(3.74)
where φα ∈Lk/(k−1)(Ω,μ0) and βμ0(S1/k(Ω,μ0)) = 0.
Proof The restriction of α to S1/k(Ω,μ0) yields a functional on Lk(Ω,μ0) given
as ψ →α(ψμ1/k
0
). Since the dual of Lk(Ω,μ0) is Lk/(k−1)(Ω,μ0), there is a φα

158
3
Parametrized Measure Models
such that
α

ψμ1/k
0

=

Ω
ψφα dμ0 =

ψμ1/k
0
;φαμ1−1/k
0

,
and then (3.74) follows by letting βμ0(·) := α(·) −(·;φαμ1−1/k
0
).
□
Lemma 3.3
Let ν1/k
n
= ψnμ1/k
0
be a bounded sequence in S1/k(Ω,μ0), i.e.,
limsup∥ν1/k
n
∥k < ∞. If lim

Ω |ψn|dμ0 = 0, then
ν1/k
n
⇀0,
i.e., limα(ν1/k
n
) = 0 for all α ∈(S1/k(Ω))′.
Proof Suppose that

Ω |ψn|dμ0 →0 and let φ ∈Lk/(k−1)(Ω,μ0) and τ ∈L∞(Ω).
Then
limsup

ν1/k
n
;φμ1−1/k
0
 ≤limsup

Ω
|ψn||φ −τ|dμ0 + ∥τ∥∞

Ω
|ψn|dμ0

≤limsup∥ψn∥k∥φ −τ∥k/(k−1),
using Hölder’s inequality in the last estimate. Since ∥ν1/k
n
∥k = ∥ψn∥k and hence,
limsup∥ψn∥k < ∞, the bound on the right can be made arbitrarily small as
L∞(Ω)⊆Lk/(k−1)(Ω,μ0) is a dense subspace. Therefore, lim(ν1/k
n
;φμ1−1/k
0
) = 0
for all φ ∈Lk/(k−1), and since β(ν1/k
n
) = 0 for all β ∈(S1/k(Ω,μ0))⊥, the assertion
follows from (3.74).
□
Before we go on, let us introduce some notation. Let (M,Ω,p) be a parametrized
measure model such that ∂V logp(ξ) ∈Lk(Ω,p(ξ)) for all V ∈TξM, and let (ξt)t∈I
be a curve in M. By Proposition 3.3, there is a measure μ0 ∈M(Ω) dominating all
p(ξt). For t,t0 ∈I and 1 ≤l ≤k we deﬁne the remainder term as
rl(t,t0) := p1/l(ξt+t0) −p1/l(ξt0) −t∂ξ′t0 p1/l ∈S1/l(Ω),
(3.75)
and we deﬁne the functions pt,qt ∈L1(Ω,μ0) with pt ≥0 and qt;l,rt,t0;l ∈
Ll(Ω,μ0) such that
p(ξt) = ptμ0
and
∂ξ′t p
= qtμ0,
qt;l
:=
qt
lp1−1/l
t
so that
∂ξ′t p1/l = qt;lμ1/l
0 ,
rt,t0;l := p1/l
t+t0 −p1/l
t0
−tqt0;l
so that
rl(t,t0) = rt,t0;lμ1/l
0 .
(3.76)
Lemma 3.4
Let (M,Ω,p) be a parametrized measure model such that
∂V logp(ξ) ∈Lk(Ω,p(ξ)) for all V ∈TξM and the function V →∂V p1/k ∈
S1/k(Ω) is weakly continuous.

3.2
Parametrized Measure Models
159
Then the model is weakly k-integrable if for any curve (ξt)t∈I in M and the
measure μ0 ∈M(Ω) and the functions pt,qt,qt;l deﬁned in (3.76) and any A⊆Ω
and t0 ∈I
d
dt

t=t0

A
p1/k
t
dμ0 =

A
qt0;k dμ0
(3.77)
or, equivalently, for all a,b ∈I

A

p1/k
b
−p1/k
a

dμ0 =
 b
a

A
qt;k dμ0 dt.
(3.78)
Proof Note that the right-hand side of (3.77) can be written as

A
qt0;k dμ0 =

∂ξ′t0 p1/k;χAμ1−1/k
0

with the pairing from (3.58), whence it depends continuously on t0 by the weak
continuity of V →∂V p1/k. Thus, the equivalence of (3.77) and (3.78) follows from
the fundamental theorem of calculus.
Now (3.78) can be rewritten as

p1/k(ξb) −p1/k(ξa);φμ1−1/k
0

=
 b
a

∂ξ′t p1/k;φμ1−1/k
0

dt
(3.79)
for φ = χA, and hence, (3.79) holds whenever φ = τ is a step function. But now,
if φ ∈Lk/(k−1) is given, then there is a sequence of step functions (τn) such that
sign(φ)τn ↗|φ|, and since (3.79) holds for all step functions, it also holds for φ by
dominated convergence.
If β ∈S1/k(Ω,μ0)⊥, then clearly, β(p1/k(ξt)) = β(∂ξ′t p1/k) = 0, whence by
(3.74) we have for all α ∈(S1/k(Ω,μ0))′
α

p1/k(ξb) −p1/k(ξa)

=
 b
a
α

∂ξ′t p1/k
dt,
and since the function t →α(∂ξ′t p1/k) is continuous by the assumed weak continu-
ity of V →∂V p1/k, differentiation and the fundamental theorem of calculus yield
(3.69) for V = ξ′
t , and as the curve (ξt) was arbitrary, (3.69) holds for arbitrary
V ∈T M.
But (3.69) is equivalent to saying that ∂V p1/k(ξ) is the weak Gâteaux-differential
of p1/k (cf. Deﬁnition C.1), and since this map is assumed to be weakly continuous,
it follows that p1/k is a weak C1-map, whence (M,Ω,p) is weakly k-integrable. □
Lemma 3.5
Let (M,Ω,p) be a parametrized measure model for which the map
V →∂V p1/k is weakly continuous. Let (ξt)t∈I be a curve in M, and let μ0 ∈M(Ω)
be a measure dominating p(ξt) for all t. For ε > 0, deﬁne the parametrized measure
model pε as
pε(ξ) := p(ξ) + εμ0.
(3.80)

160
3
Parametrized Measure Models
Then the map t →∂ξ′t p1/k
ε
∈S1/k(Ω) is weakly continuous, and for all t0 ∈I,
1
t rε
k(t,t0) ⇀0
as t →0,
where rε
k(t,t0) is deﬁned analogously to (3.75).
Proof We deﬁne the functions pε
t , qε
t , qε
t;l and rε
t,t0;l satisfying (3.76) for the
parametrized measure model pε, so that pε
t = pt + ε and qε
t = qt. For t,t0 ∈I
we have
qε
t;k −qε
t0;k
 =

qt
k(pε
t )1−1/k −
qt0
k(pε
t0)1−1/k

≤
1
k(pε
t0)1−1/k |qt −qt0| +

1
k(pε
t )1−1/k −
1
k(pε
t0)1−1/k
|qt|
≤
1
k(pε
t0)1−1/k

|qt −qt0| + k

pε
t0
1−1/k −

pε
t
1−1/kqε
t;k

(3.63)
≤
1
kε1−1/k

|qt −qt0| + k|pt0 −pt|1−1/k|qt;k|

.
Thus,

Ω
qε
t;k −qε
t0;k
 dμ0 ≤
1
kε1−1/k

Ω

|qt −qt0| + k|pt0 −pt|1−1/k|qt;k|

dμ0
≤
1
kε1−1/k

∥qt −qt0∥1 + k∥pt0 −pt∥1−1/k
1
∥qt;k∥k

and since ∥qt −qt0∥1 = ∥dξ′t p −dξ′t0p∥1 and ∥pt0 −pt∥1 = ∥p(ξt) −p(t0)∥1 tend
to 0 for t →t0 as p is a C1-map by the deﬁnition of parametrized measure model,
and ∥qt;k∥k = ∥∂ξ′t p1/k∥k is bounded for t →t0 by (C.1) as ∂ξ′t p1/k ⇀∂ξ′t0 p1/k, it
follows that the integral tends to 0 and hence, Lemma 3.3 implies that ∂ξ′t p1/k
ε
⇀
∂ξ′t0 p1/k
ε
as t →t0, showing the weak continuity of t →∂ξ′t p1/k
ε
.
For the second claim, note that by the mean value theorem there is an ηt between
pε
t+t0 and pε
t0 (and hence, ηt ≥ε) for which
rε
t,t0;k
 =


pε
t+t0
1/k −

pε
t0
1/k −t
qt0
k(pε
t0)1−1/k
 =

pt+t0 −pt0
kη1−1/k
t
−t
qt0
k(pε
t0)1−1/k

≤|rt,t0;1|
kη1−1/k
t
+ |t|

qt0
kη1−1/k
t
−
qt0
k(pε
t0)1−1/k

= |rt,t0;1|
kη1−1/k
t
+
|t||qt0||(pε
t0)1−1/k −η1−1/k
t
|
k(pε
t0)1−1/kη1−1/k
t
(3.63)
≤C

|rt,t0;1| + |t||qt0;k||pt+t0 −pt0|1−1/k

3.2
Parametrized Measure Models
161
for C := 1/kε1−1/k > 0 depending only on k and ε. Thus,

Ω
rε
t,t0;k
 dμ0 ≤C

Ω
|rt,t0;1| dμ0 + |t|

Ω
|qt0;k||pt+t0 −pt0|1−1/k dμ0

≤C

∥rt,t0;1∥1 + |t|∥qt0;k∥k∥pt+t0 −pt0∥1−1/k
1

= C
r1(t,t0)

1 + |t|
∂ξ′t p1/k
k
p(t + t0) −p(t0)
1−1/k
1

,
using Hölder’s inequality in the second line. Since p is a C1-map, ∥r1(t,t0)∥1/|t|
and ∥p(t + t0) −p(t0)∥1 tend to 0, whereas ∥∂ξ′t p1/k∥k is bounded close to t0 by
(C.1) since t →∂ξ′t p1/k is weakly continuous, so that
1
|t|

Ω
rε
t,t0;k
 dμ0
t→0
−−→0,
which by Lemma 3.3 implies the second assertion.
□
Since by deﬁnition the derivative of V →∂V p1/k is weakly continuous for any
k-integrable model, the second assertion of Theorem 3.2 will follow from the fol-
lowing.
Lemma 3.6
Let (M,Ω,p) be a parametrized measure model for which the map
V →∂V p1/k is weakly continuous. Then (M,Ω,p) is weakly k-integrable.
Proof Let (ξt)t∈I be a curve in M, let μ0 ∈M(Ω) be a measure dominating p(ξt)
for all t, and deﬁne the parametrized measure model pε(ξ) and rk(t,t0) as in (3.80).
By Lemma 3.5, we have for any A⊆Ω and t0 ∈I and the pairing (·;·) from (3.58)
0 = lim
t→0
1
t

rk(t,t0);χAμ1−1/k
0

= lim
t→0
1
t

A

pε
t+t0
1/k −

pε
t0
1/k −tqε
t0;k

dμ0
= d
dt

t=t0

A

pε
t
1/k dμ0 −

A
qε
t0;k dμ0,
showing that
d
dt

t=t0

A

pε
t
1/k dμ0 =

A
qε
t0;k dμ0
(3.81)
for all t0 ∈I. As we observed in the proof of Lemma 3.4, the weak continuity of the
map t →∂ξ′t p1/k
ε
implies that the integral on the right-hand side of (3.81) depends
continuously on t0 ∈I, whence integration implies that for all a,b ∈I we have

A

pε
b
1/k −

pε
a
1/k
dμ0 =
 b
a

A
qε
t;k dμ0 dt.
(3.82)

162
3
Parametrized Measure Models
Now |qε
t;k| ≤|qt;k| and |(pε
b)1/k −(pε
a)1/k∥≤|pb −pa|1/k by (3.63), whence we
can use dominant convergence for the limit as ε ↘0 in (3.82) to conclude that (3.78)
holds, so that Lemma 3.4 implies the weak k-integrability of the model.
□
We now have to prove the ﬁrst assertion of Theorem 3.2. We begin by showing
the weak continuity of the differential of p1/k.
Lemma 3.7
Let (M,Ω,p) be a parametrized measure model with ∂V logp(ξ) ∈
Lk(Ω,p(ξ)) for all ξ, and suppose that the function (3.70) is continuous. Then the
map V →∂V p1/k is weakly continuous, so that the model is weakly k-integrable.
Proof Let (Vn)n∈N be a sequence, Vn ∈TξnM with Vn →V0 ∈Tξ0M, and let μ0 ∈
M(Ω) be a measure dominating all p(ξn). In fact, we may assume that there is a
decomposition Ω = Ω0 ⊎Ω1 such that
p(ξ0) = χΩ0μ0.
We adapt the notation from (3.76) and deﬁne pn,qn ∈L1(Ω,μ0) and qn;l ∈
Ll(Ω,μ0), replacing t ∈I by n ∈N0 in (3.76). In particular, p0 = χΩ0. Then on
Ω0 we have
|qn;k −qn;0| =

qn
kp1−1/k
n
−q0
k
 ≤1
k |qn −q0| + |qn|
k

1
p1−1/k
n
−1

=
1
k |qn −q0| + |qn;k|
1 −p1−1/k
n

(3.63)
≤
1
k |qn −q0| + |qn;k||1 −pn|1−1/k
=
1
k |qn −q0| + |qn;k||pn −p0|1−1/k.
Thus,

Ω0
|qn;k −qn;0| dμ0 ≤1
k

Ω0
|qn −q0| dμ0 +

Ω0
|qn;k||pn −p0|1−1/k dμ0
≤1
k ∥qn −q0∥1 + ∥qn;k∥k∥pn −p0∥1−1/k
1
= 1
k ∥∂Vnp −∂V0p∥1 +
∂Vnp1/k
k
p(ξn) −p(ξ0)
1−1/k
1
.
Since p is a C1-map, both ∥∂Vnp −∂V0p∥1 and ∥p(ξn) −p(ξ0)∥1 tend to 0, whereas
∥∂Vnp1/k∥k tends to ∥∂V0p1/k∥k by the continuity of (3.70). Moreover, ∂V0p is dom-
inated by p(ξ0) = χΩ0μ0, whence q0 and q0;k vanish on Ω1. Thus, we conclude
that
0 = lim

Ω0
|qn;k −qn;0| dμ0 = lim

Ω
|χΩ0qn;k −qn;0| dμ0.
(3.83)

3.2
Parametrized Measure Models
163
By Lemma 3.3, this implies that χΩ0∂Vnp1/k ⇀∂V0p1/k, whence by (C.1) we
have
∥∂V0p∥k ≤liminf∥χΩ0∂Vnp∥k ≤limsup∥χΩ0∂Vnp∥k
≤limsup∥∂Vnp∥k = ∥∂V0p∥k,
using again the continuity of (3.70). Thus, we have equality in these estimates, i.e.,
∥∂V0p∥k = lim∥χΩ0∂Vnp∥k = lim∥∂Vnp∥k,
and since ∥∂Vnp∥k
k = ∥χΩ0∂Vnp∥k
k + ∥χΩ1∂Vnp∥k
k, this implies that
lim∥χΩ1∂Vnp∥k = 0.
Thus,
lim

Ω1
|qn;k −qn;0| dμ0 = lim

Ω1
|qn;k| dμ0 = lim∥χΩ1∂Vnp∥1 = 0
as ∥χΩ1∂Vnp∥k →0, so that together with (3.83) we conclude that
lim

Ω
|qn;k −qn;0| dμ0 = 0,
and now, Lemma 3.3 implies that ∂Vnp1/k ⇀∂V0p1/k for an arbitrary convergent se-
quence (Vn) ∈T M, showing the weak continuity, and the last assertion now follows
from Lemma 3.6.
□
Lemma 3.8 The ﬁrst assertion of Theorem 3.2 holds.
Proof By the deﬁnition of k-integrability, the continuity of the map p1/k for a k-
integrable parametrized measure model is evident from the deﬁnition, so that the
continuity of (3.70) follows.
Thus, we have to show the converse and assume that the map (3.70) is continuous.
By Lemma 3.7, this implies that the map V →∂V p1/k is weakly continuous and
hence, the model is weakly k-integrable by Lemma 3.6. In particular, (3.69) holds
by Proposition 3.4.
Together with the continuity of the norm, it follows from the Radon–Riesz theo-
rem (cf. Theorem C.3) that the map V →∂V p1/k is continuous even in the norm of
S1/k(Ω).
Let (ξt)t∈I be a curve in M and let V := ξ′
0 ∈Tξ0M, and recall the deﬁnition of
the remainder term rk(t,t0) from (3.75). Thus, what we have to show is that
1
|t|
rk(t,t0)

k
t→0
−−→0.
(3.84)

164
3
Parametrized Measure Models
By the Hahn–Banach theorem (cf. Theorem C.1), we may for each pair t,t0 ∈I
choose an α ∈S1/k(Ω)′ with α(rk(t,t0)) = ∥rk(t,t0)∥k and ∥α∥k = 1. Then we
have
rk(t,t0)

k =
α

rk(t,t0)
 =
α

p1/k(ξt+t0) −p1/k(ξt0)

−t∂ξ′t0p1/k))

(3.69)
=

 t+t0
t0
α

∂ξ′sp1/k −∂ξ′t0 p1/k
ds

≤
 t+t0
t0
∥α∥k
∂ξ′sp1/k −∂ξ′t0 p1/k
k ds
≤|t| max
|s−t0|≤t
∂ξ′sp1/k −∂ξ′t0 p1/k
k
Thus,
∥rk(t,t0)∥k
|t|
≤max
|s−t0|≤t
|∂ξ′sp1/k −∂ξ′t0 p1/k
k,
and by the continuity of the map V →∂V p1/k in the norm of S1/k(Ω), the right-
hand side tends to 0 as t →0, showing (3.84) and hence the claim.
□
3.2.5 Canonical n-Tensors of an n-Integrable Model
We begin this section with the formal deﬁnition of an n-tensor on a vector space.
Deﬁnition 3.8 Let (V,∥·∥) be a normed vector space (e.g., a Banach space). A co-
variant n-tensor on V is a multilinear map Θ :×
n V →R which is continuous
w.r.t. the product topology.
We can characterize covariant n-tensors by the following proposition.
Proposition 3.5 Let (V,∥· ∥) be a Banach space and Θ :×
n V →R a be multi-
linear map. Then the following are equivalent.
(1) Θ is a covariant n-tensor on V , i.e., continuous w.r.t. the product topology.
(2) There is a C > 0 such that for V1,...,Vn ∈V
Θ(V1,...,Vn)
 ≤C∥V1∥···∥Vn∥.
(3.85)
Proof To see that the ﬁrst condition implies (3.85), we proceed by induction on n.
For n = 1 this is clear as a continuous linear map Θ : V →R is bounded. Suppose
that (3.85) holds for all n-tensors and let Θn+1 : ×
n+1 V →R be a covariant
(n+1)-tensor. For ﬁxed V1,...,Vn ∈V , the map Θn+1(·,V1,...,Vn) is continuous
and hence bounded linear.

3.2
Parametrized Measure Models
165
On the other hand, for ﬁxed V0, the map Θn+1(V0,V1,...,Vn) is a covariant
n-tensor and hence by induction hypothesis,
Θn+1(V0,V1,...,Vn)
 ≤CV0
if ∥Vi∥= 1 for all i > 0.
(3.86)
The uniform boundedness principle (cf. Theorem C.2) now shows that the constant
CV0 in (3.86) can be chosen to be C∥V0∥for some ﬁxed C ∈R, so that (3.85) holds
for Θn+1, completing the induction.
Next, to see that (3.85) implies the continuity of Θn, let (V (k)
i
)k∈N ∈V , i =
1,...,n be sequences converging to V 0
i . Then
Θ

V (k)
1 ,...,V (k)
n

−Θ

V 0
1 ,...,V 0
n
 =

n

i=1
Θ(V (k)
1
,...,V (k)
i
−V 0
i ,...,V 0
n )

(3.85)
≤
n

i=1
C
V (k)
1
···
V (k)
i
−V 0
i
···
V 0
n
,
and this tends to 0 as ∥V (k)
i
∥
k→∞
−−−→∥V 0
i ∥and ∥V (k)
i
−V 0
i ∥
k→∞
−−−→0. Thus, Θ is
continuous in the product topology.
□
Deﬁnition 3.9 (Covariant n-tensors on a manifold) Let M be a C1-manifold. A co-
variant n-tensor ﬁeld on M is a family (Θξ)ξ∈M of covariant n-tensor ﬁelds on TξM
which are weakly continuous, i.e., such that for continuous vector ﬁelds V1,...,Vn
on M the function Θ(V1,...,Vn) is continuous on M.
An important example of such a tensor is given by the following
Deﬁnition 3.10 (Canonical n-tensor) For n ∈N, the canonical n-tensor is the co-
variant n-tensor on S1/n(Ω), given by
Ln
Ω(ν1,...,νn) = nn

Ω
d(ν1 ···νn),
where νi ∈S1/n(Ω).
(3.87)
Moreover, for 0 < r ≤1/n the canonical n-tensor τ n
Ω;r on Sr(Ω) is deﬁned as

τ n
Ω;r

μr(ν1,...,νn) :=
⎧
⎨
⎩
1
rn

Ω d(ν1 ···νn · |μr|1/r−n)
if r < 1/n,
Ln
Ω(ν1,...,νn)
if r = 1/n,
(3.88)
where νi ∈Sr(Ω) = TμrSr(Ω).
Observe that for a ﬁnite set Ω = I, the deﬁnition of Ln
I coincides with the co-
variant n-tensor given in (2.53).
For n = 2, the pairing (·;·) : S1/2(Ω) × S1/2(Ω) →R from (3.58) satisﬁes
(ν1;ν2) = 1
4 L2
Ω(ν1,ν2).

166
3
Parametrized Measure Models
Since (ν;ν) = ∥ν∥2
2 by (3.54), it follows:5

S1/2(Ω), 1
4 L2
Ω

is a Hilbert space with norm ∥· ∥2.
(3.89)
For a C1-map Φ : M1 →M2 and a covariant n-tensor ﬁeld Θ on M2, the pull-
back Φ∗Θ given by
Φ∗Θ(V1,...,Vn) := Θ

dΦ(V1),...,dΦ(Vn)

(3.90)
is a covariant n-tensor ﬁeld on M1. If Φ : M1 →M2 and Ψ : M2 →M3 are differ-
entiable, then this immediately implies for the composition Ψ Φ : M1 →M3:
(Ψ Φ)∗Θ = Φ∗Ψ ∗Θ.
(3.91)
Proposition 3.6 Let n ∈N and 0 < s ≤r ≤1/n. Then
τ n
Ω;r =

˜π1/rn∗Ln
Ω
(3.92)
and

˜πr/s∗τ n
Ω;r = τ n
Ω;s,
(3.93)
with the C1-maps ˜π1/rn and ˜πr/s from Proposition 3.2, respectively.
Proof Unwinding the deﬁnition we obtain for k := 1/rn ≥1:

˜πk∗Ln
Ω

μr

ν1
r ,...,νn
r

= Ln
Ω

dμr ˜πk
ν1
r

,...,dμr ˜πk
νn
r

(3.62)
= Ln
Ω

k|μr|k−1 · ν1
r ,...,k|μr|k−1 · νn
r

= knnn

Ω
d

|μr|k−1 · ν1
r

···

|μr|k−1 · νn
r

=
1
rn

Ω
d

ν1
r ···νn
r · |μr|1/r−n
=

τ n
Ω;r

μr

ν1
r ,...,νn
r

,
showing (3.92). Thus,

˜πr/s∗τ n
Ω;r =

˜πr/s∗
˜π1/rn∗Ln
Ω =

˜π1/rn ˜πr/s∗Ln
Ω =

˜π1/sn∗Ln
Ω
= τ n
Ω;s,
showing (3.93).
□
5Observe that the factor 1/4 in (3.89) by which the canonical form differs from the Hilbert in-
ner product is responsible for having to use the sphere of radius 2 rather than the unit sphere in
Proposition 2.1.

3.2
Parametrized Measure Models
167
Remark 3.10 If Ω = I is a ﬁnite measurable space, then Mr
+(I) is an open subset
of Sr(I) and hence a manifold. Moreover, the restrictions ˜πk : Mr
+(I) →Ms
+(I)
is a C1-map even for k ≤1, so that we may use (3.92) to deﬁne τ n
Ω;r for all r ∈(0,1]
on M+(I).
That is, if μ = 	
i∈I miδi ∈M+(I) and Vk = 	
i∈I vk;iδi ∈S(I), then
τ n
I;r(V1,...,Vn) = 1
rn

I
v1;i ···vn;im1/r−n
i
δi =

i∈I
1
rnmn−1/r
i
v1;i ···vn;i.
(3.94)
For r = 1, the tensor τ n
I;1 coincides with the tensor τ n
I in (2.76).
This explains on a more conceptual level why τ n
I = τ n
I;1 cannot be extended from
M+(I) to S(I), whereas L2
I = ˜π1/2τ n
I can be extended to a tensor on S2(I), cf.
(2.52).
If (M,Ω,p) is an n-integrable parametrized measure model, then we deﬁne the
canonical n-tensor of (M,Ω,p) as
τ n
M = τ n
(M,Ω,p) :=

p1/n∗Ln
Ω,
(3.95)
so that by (3.68)
τ n
M(V1,...,Vn) := Ln
Ω

dξp1/n(V1),...,dξp1/n(Vn)

=

Ω
∂V1 logp(ξ)···∂Vn logp(ξ) dp(ξ),
(3.96)
where the second line follows immediately from (3.68) and (3.87). That is, τ n
M co-
incides with the tensor given in (3.44).
Observe that τ n
M is indeed continuous, since τ n
M(V,...,V ) = nn∥dξp1/n(V )∥n
n
is continuous for all V .
Example 3.5
(1) For n = 1, the canonical 1-form is given as
τ 1
M(V ) :=

Ω
∂V logp(ξ)dp(ξ) = ∂V
p(ξ)
.
(3.97)
Thus, it vanishes if and only if ∥p(ξ)∥is locally constant, e.g., if (M,Ω,p) is a
statistical model.
(2) For n = 2, τ 2
M coincides with the Fisher metric g from (3.41).
(3) For n = 3, τ 3
M coincides with the Amari–Chentsov 3-symmetric tensor T from
(3.42).
Remark 3.11 While the above examples show the statistical signiﬁcance of τ n
M for
n = 1,2,3, we shall show later that the tautological forms of even degree τ 2n
M can

168
3
Parametrized Measure Models
be used to measure the information loss of statistics and Markov kernels, cf. Theo-
rem 5.5. Moreover, in [160, p. 212] the question is posed if there are other signiﬁcant
tensors on statistical manifolds, and the canonical n-tensors may be considered as
natural candidates.
3.2.6 Signed Parametrized Measure Models
In this section, we wish to comment on the generalization of Deﬁnition 3.4 to fami-
lies of ﬁnite signed measures (p(ξ))ξ∈M, i.e., dropping the assumption that p(ξ) is
a non-negative measure. That is, we may simply consider C1-maps p : M →S(Ω).
However, with this approach there is no notion of k-integrability or of canoni-
cal tensors, as the term ∂V logp(ξ) from (3.66), which is necessary to deﬁne these
notions, cannot be given sense without further assumptions.
For instance, if Ω = {ω} is a singleton and p(ξ) := ξδω for ξ ∈R, then p : R →
S(Ω) is certainly a C1-map, but logp(ξ) = logξ cannot be continuously extended
at ξ = 0.
Therefore, we shall make the following deﬁnition.
Deﬁnition 3.11 Let Ω be a measurable space and M be a (Banach-)manifold.
(1) A signed parametrized measure model is a triple (M,Ω,p), where M is a (ﬁnite
or inﬁnite-dimensional) Banach manifold and p : M →S(Ω) is a C1-map in
the sense of Sect. 3.2.2.
(2) A signed parametrized measure model (M,Ω,p) is said to have a logarithmic
derivative if for each ξ ∈M and V ∈TξM the derivative dξp(V ) ∈S(Ω) is
dominated by |p(ξ)|. In this case, we deﬁne analogously to (3.66)
∂V log
p(ξ)
 := d{dξp(V )}
dp(ξ)
.
(3.98)
Here, for signed measures μ,ν ∈S(Ω) such that |μ| dominates ν, the Radon–
Nikodym derivative dν
dμ is the unique function in L1(Ω,|μ|) such that
ν = dν
dμμ.
Just as in the non-signed case, we can now consider the (1/k)th power of p.
Here, we shall use the signed power in order not to lose information on p(ξ). That
is, we consider the (continuous) map
˜p1/k : M −→S1/k(Ω),
ξ −→˜π1/kp(ξ) =: ˜p(ξ)1/k.

3.2
Parametrized Measure Models
169
Assuming that this map is differentiable and differentiating the equation p = ˜πk ˜p1/k
just as in the non-signed case, we obtain in analogy to (3.68)
dξ ˜p1/k(V ) := 1
k ∂V log
p(ξ)
 ˜p1/k(ξ) ∈S1/k
Ω,
p(ξ)

(3.99)
so that, in particular, ∂V log|p(ξ)| ∈Lk(Ω,|p(ξ)|), and in analogy to Deﬁnition 3.7
we deﬁne the following.
Deﬁnition 3.12 (k-Integrable signed parametrized measure model)
A signed
parametrized measure model (M,Ω,p) is called k-integrable for k ≥1 if it has
a logarithmic derivative and the map ˜p1/k : M →S1/k(Ω) is a C1-map. Further-
more, we call the model ∞-integrable if it is k-integrable for all k ≥1.
Just as in the non-signed case, this allows us to deﬁne the canonical n-tensor
for an n-integrable signed parametrized measure model. Namely, in analogy to
(3.95) and (3.96) we deﬁne for an n-integrable signed parametrized measure model
(M,Ω,p) the canonical n-tensor of (M,Ω,p) as
τ n
M = τ n
(M,Ω,p) :=
˜p1/n∗Ln
Ω,
(3.100)
so that by (3.99) and (3.87)
τ n
M(V1,...,Vn) := Ln
Ω

dξ ˜p1/n(V1),...,dξ ˜p1/n(Vn)

=

Ω
∂V1 log
p(ξ)
···∂Vn log
p(ξ)
 dp(ξ).
Example 3.6
(1) For n = 1, the canonical 1-form is given as
τ 1
M(V ) :=

Ω
∂V log
p(ξ)
dp(ξ) = ∂V

p(ξ)(Ω)

.
(3.101)
Thus, it vanishes if and only if p(ξ)(Ω) is locally constant, but of course, as
p(ξ) is a signed measure, in general this quantity does not equal ∥p(ξ)∥T V =
|p(ξ)|(Ω).
(2) For n = 2 and n = 3, τ 2
M and τ 3
M coincide with the Fisher metric and the Amari–
Chentsov 3-symmetric tensor T from (3.41) and (3.42), respectively. That is,
for signed parametrized measure models which are k-integrable for k ≥2 or
k ≥3, respectively, the tensors gM and T, respectively, still can be deﬁned.
Note, however, that gM may fail to be positive deﬁnite. In fact, it may even be
degenerate, so that gM does not necessarily yield a Riemannian (and possibly
not even a pseudo-Riemannian) metric on M for signed parametrized measure
models.

170
3
Parametrized Measure Models
3.3 The Pistone–Sempi Structure
The deﬁnition of (k-integrable) parametrized measure models (statistical models,
respectively) discussed in Sect. 3.2 is strongly inspired by the deﬁnition of Amari
[9]. There is, however, another beautiful concept of geometrizing the space of ﬁnite
measures (probability measures, respectively) which was ﬁrst suggested by Pistone
and Sempi in [216].
This approach is, on the one hand, more restrictive, since instead of geometrizing
all of M(Ω), it only geometrizes the subset M+ = M+(Ω,μ0) of ﬁnite measures
compatible with a ﬁxed measure μ0, where two measures are called compatible if
they have the same null sets. On the other hand, it deﬁnes on M+ the structure
of a Banach manifold, whence a much stronger geometric structure than that of
M(Ω) deﬁned in Sect. 3.2. We shall refer to this as the Pistone–Sempi structure on
M+(Ω,μ0).
The starting point is to deﬁne a topology on M+, called the topology of e-
convergence or e-topology, cf. Deﬁnition 3.13 below. For this topology, the inclu-
sion M+ = M+(Ω,μ0) →M(Ω) is continuous, where M(Ω) is equipped with
the L1-topology induced by the inclusion M(Ω,μ0) →S(Ω).
Since M+ is a Banach manifold, it is natural to consider models which are given
by C1-maps p : M →M+ from a (Banach-)manifold M to the Banach manifold
M+. As it turns out, such a map, when regarded as a parametrized measure model
p : M →M+ →M(Ω) is ∞-integrable in the sense of Deﬁnition 3.7. The con-
verse, however, is far from being true in general, see Example 3.8 below.
In fact, we shall show that in the e-topology, the set M+ decomposes into several
connected components each of which can be canonically identiﬁed with a convex
open set in a Banach space, and this induces the Banach manifold structure estab-
lished in [216].
We shall begin our discussion by introducing the notion of the e-topology, which
is deﬁned using the notion of convergence of sequences. We shall then recall some
basic facts about Orlicz spaces and show that to each μ ∈M+ there is an associated
exponential tangent space TμM+ which is an Orlicz space. It then turns out that the
image of the injective map
logμ0 : ef μ0 −→f
maps the connected component of μ in M+ into an open convex subset of TμM+.
We also refer to the results in [230] where this division of M+ into open cells is
established as well.
3.3.1 e-Convergence
Deﬁnition 3.13 (Cf. [216]) Let (gn)n∈N be a sequence of measurable functions in
the ﬁnite measure space (Ω,μ), and let g ∈L1(Ω,μ) be such that gn,g > 0 μ-
a.e. We say that (gn)n∈N is e-convergent to g if the sequences (gn/g) and (g/gn)

3.3
The Pistone–Sempi Structure
171
converge to 1 in Lp(Ω,gμ) for all p ≥1. In this case, we also say that the sequence
of measures (gnμ)n∈N is e-convergent to the measure gμ.
It is evident from this deﬁnition that the measure μ may be replaced by a compat-
ible measure μ′ ∈M+(Ω,μ) without changing the notion of e-convergence. There
are equivalent reformulations of e-convergence given as follows.
Proposition 3.7 (Cf. [216]) Let (gn)n∈N and g be as above. Then the following are
equivalent:
(1) (gn)n∈N is e-convergent to g.
(2) For all p ≥1, we have
lim
n→∞

Ω

gn
g
p
−1
g dμ = lim
n→∞

Ω

 g
gn
p
−1
g dμ = 0.
(3) The following conditions hold:
(a) (gn) converges to g in L1(Ω,μ),
(b) for all p ≥1 we have
limsup
n→∞

Ω
gn
g
p
g dμ < ∞
and
limsup
n→∞

Ω
 g
gn
p
g dμ < ∞.
For the proof, we shall use the following simple
Lemma 3.9
Let (fn)n∈N be a sequence of measurable functions in (Ω,μ) such
that
(1) limn→∞∥fn∥1 = 0,
(2) limsupn→∞∥fn∥p < ∞for all p > 1.
Then limn→∞∥fn∥p = 0 for all p ≥1, i.e., (fn) converges to 0 in Lp(Ω,μ) for
all p ≥1.
Proof For p ≥1, we have by Hölder’s inequality
∥fn∥p
p =

Ω
|fn|pdμ =

Ω
|fn|1/2|fn|p−1/2 dμ
≤
f 1/2
n

2
f p−1/2
n

2 = ∥fn∥1/2
1 ∥fn∥(2p−1)/2
2p−1
.
Since ∥fn∥1 →0 and ∥fn∥2p−1 is bounded, ∥fn∥p →0 follows.
□
Proof of Proposition 3.7 (1) ⇒(2) Note that the expressions |(gn/g)p −1| and
|(g/gn)p −1| are increasing in p, hence we may assume w.l.o.g. that p ∈N. Let
fn := gn/g −1, so that by hypothesis limn→∞

Ω |fn|pg dμ = 0 for all p ≥1.

172
3
Parametrized Measure Models
Then

Ω

gn
g
p
−1
g dμ =

Ω
(1 + fn)p −1
g dμ
≤
p

k=1
p
k

Ω
|fn|kg dμ −→0,
and the other assertion follows analogously.
(2) ⇒(3) The ﬁrst equation in (2) for p = 1 reads
0 = lim
n→∞

Ω

gn
g −1
g dμ = lim
n→∞

Ω
|gn −g|dμ,
so that (3)(a) is satisﬁed. Moreover, it is evident that the convergence conditions in
(2) imply the boundedness of (gn/g) and (g/gn) in Lp(Ω,gμ).
(3) ⇒(1) Again, let fn := gn/g −1. Then (3)(a) implies that (fn) converges
to 0 in L1(Ω,gμ), and the ﬁrst condition in (3)(b) implies that (fn) is bounded in
Lp(Ω,gμ) for all p > 1. This together with Lemma 3.9 implies that (fn) converges
to 0 and hence (gn/g) to 1 in Lp(Ω,gμ) for all p ≥1.
Note that g/gn −1 = −fn · (g/gn). By the above, (fn) tends to 0 in L2(Ω,gμ),
and (g/gn) is bounded in that space by (3)(b). Thus, by Hölder’s inequality,
(g/gn −1) = −fn · (g/gn) tends to 0 in L1(Ω,gμ) and, moreover, this sequence is
bounded in Lp(Ω,gμ) for all p ≥1 by the second condition in (3)(b). Thus, (g/gn)
tends to 1 in Lp(Ω,gμ) for all p ≥1 by Lemma 3.9.
□
3.3.2 Orlicz Spaces
In this section, we recall the theory of Orlicz spaces which is needed in Sect. 3.3.3
for the description of the geometric structure on M(Ω). Most of the results can be
found, e.g., in [153].
A function φ : R →R is called a Young function if φ(0) = 0, φ is even, con-
vex, strictly increasing on [0,∞) and limt→∞t−1φ(t) = ∞. Given a ﬁnite measure
space (Ω,μ) and a Young function φ, we deﬁne the Orlicz space
Lφ(μ) :=

f : Ω →R


Ω
φ(εf )dμ < ∞for some ε > 0

.
The elements of Lφ(μ) are called Orlicz functions of (φ,μ). Convexity of φ and
φ(0) = 0 implies
φ(cx) ≤cφ(x)
and
φ

c−1x

≥c−1φ(x)
for all c ∈(0,1), x ∈R.
(3.102)

3.3
The Pistone–Sempi Structure
173
We deﬁne the Orlicz norm on Lφ(μ) as
∥f ∥φ,μ := inf

a > 0


Ω
φ
f
a

dμ ≤1

.
If f is an Orlicz function and ε > 0 and K ≥

Ω φ( f
a )dμ ≥1, then

Ω
φ(εf )dμ ≤K ⇒

Ω
φ

K−1εf

dμ
(3.102)
≤
K−1

Ω
φ(εf )dμ = 1,
whence

Ω
φ(εf )dμ ≤K ⇒∥f ∥φ,μ ≤K
ε ,
(3.103)
as long as K ≥1. In particular, every Orlicz function has ﬁnite norm.
Observe that the inﬁmum in the deﬁnition of the norm is indeed attained, as for
a sequence (an)n∈N descending to ∥f ∥φ,μ the sequence gn := φ(f/an) is mono-
tonically increasing as φ is even and increasing on [0,∞). Thus, by the monotone
convergence theorem,

Ω
φ

f
∥f ∥φ,μ

dμ = lim
n→∞

Ω
φ
 f
an

dμ ≤1.
We assert that ∥· ∥φ,μ is indeed a norm on Lφ(μ). For the positive deﬁniteness,
suppose that ∥f ∥φ,μ = 0. Then

Ω φ(nf )dμ ≤1 for all n ∈N, and again by the
monotone convergence theorem,
1 ≥lim
n→∞

Ω
φ(nf )dμ =

Ω
lim
n→∞φ(nf )dμ,
so that, in particular, limn→∞φ(nf (ω)) < ∞for a.e. ω ∈Ω. But since
limt→∞φ(t) = ∞, this implies that f (ω) = 0 for a.e. ω ∈Ω, as asserted.
The homogeneity ∥cf ∥φ,μ = |c| ∥f ∥φ,μ is immediate from the deﬁnition. Fi-
nally, for the triangle inequality let f1,f2 ∈Lφ(Ω,μ) and let ci := ∥fi∥φ,μ. Then
the convexity of φ implies

Ω
φ
f1 + f2
c1 + c2

dμ =

Ω
φ

c1
c1 + c2
f1
c1
+
c2
c1 + c2
f2
c2

dμ
≤
c1
c1 + c2

Ω
φ
f1
c1

dμ +
c2
c1 + c2

Ω
φ
f2
c2

dμ
≤1.
Thus, ∥f1 + f2∥φ,μ ≤c1 + c2 = ∥f1∥φ,μ + ∥f2∥φ,μ so that the triangle inequality
holds.
Example 3.7 For p > 1, let φ(t) := |t|p. This is then a Young function, and
Lφ(μ) = Lp(Ω,μ) as normed spaces.

174
3
Parametrized Measure Models
In this example, we could also consider the case p = 1, even though φ(t) = |t| is
not a Young function as it fails to meet the condition limt→∞t−1φ(t) = ∞. How-
ever, this property of Young functions was not used in the veriﬁcation of the norm
properties of ∥· ∥φ,μ.
Proposition 3.8
Let (fn)n∈N be a sequence in Lφ(μ). Then the following are
equivalent:
(1) limn→∞fn = 0 in the Orlicz norm.
(2) There is a K > 0 such that for all c > 0, limsupn→∞

Ω φ(cfn)dμ ≤K.
(3) For all c > 0, limn→∞

Ω φ(cfn)dμ = 0.
Proof Suppose that (1) holds. Then for any c > 0 and ε ∈(0,1) we have
ε ≥ε limsup
n→∞

Ω
φ

cε−1fn

dμ
(
)*
+
≤1 if ∥fn∥φ,μ ≤ε/c
(3.102)
≥
limsup
n→∞

Ω
φ(cfn)dμ,
and since ε ∈(0,1) is arbitrary, (3) follows. Obviously, (3) implies (2), and if (2)
holds for some K, then assuming w.l.o.g. that K ≥1, we conclude from (3.103) that
limsupn→∞∥fn∥φ,μ ≤Kc−1 for all c > 0, whence limn→∞∥fn∥φ,μ = 0, which
shows (1).
□
Now let us investigate how the Orlicz spaces behave under a change of the Young
function φ.
Proposition 3.9 Let (Ω,μ) be a ﬁnite measure space, and let φ1,φ2 : R →R be
two Young functions. If
limsup
t→∞
φ1(t)
φ2(t) < ∞,
then Lφ2(μ)⊆Lφ1(μ), and the inclusion is continuous, i.e., ∥f ∥φ1,μ ≤c ∥f ∥φ2,μ
for some c > 0 and all f ∈Lφ2(μ). In particular, if
0 < liminf
t→∞
φ1(t)
φ2(t) ≤limsup
t→∞
φ1(t)
φ2(t) < ∞,
then Lφ1(μ) = Lφ2(μ), and the Orlicz norms ∥· ∥φ1,μ and ∥· ∥φ2,μ are equivalent.
Proof By our hypothesis, φ1(t) ≤Kφ2(t) for some K ≥1 and all t ≥t0. Let f ∈
Lφ2(μ) and a := ∥f ∥φ2,μ. Moreover, decompose
Ω := Ω1 ⊎Ω2
with
Ω1 :=

ω ∈Ω
 f (ω)
 ≥at0

.

3.3
The Pistone–Sempi Structure
175
Then
K ≥K

Ω
φ2
|f |
a

dμ ≥

Ω1
Kφ2
|f |
a

dμ
≥

Ω1
φ1
|f |
a

dμ
as |f |
a ≥t0 on Ω1
=

Ω
φ1
|f |
a

dμ −

Ω2
φ1
|f |
a

dμ
≥

Ω
φ1
|f |
a

dμ −

Ω2
φ1(t0)dμ
as |f |
a < t0 on Ω2
≥

Ω
φ1
|f |
a

dμ −φ1(t0)μ(Ω).
Thus,

Ω φ1( |f |
a ) ≤K + φ1(t0)μ(Ω) =: c, hence f ∈Lφ1(μ). As c ≥1, (3.103)
this implies that ∥f ∥φ1,μ ≤ca = c∥f ∥φ2,μ, and this proves the claim.
□
The following lemma is a straightforward consequence of the deﬁnitions and we
omit the proof.
Lemma 3.10
Let (Ω,μ) be a ﬁnite measure space, let φ : R →R be a Young
function, and let ˜φ(t) := φ(λt) for some constant λ > 0.
Then ˜φ is also a Young function. Moreover, Lφ(μ) = L ˜φ(μ) and ∥· ∥˜φ,μ =
λ∥· ∥φ,μ, so that these norms are equivalent.
Furthermore, we investigate how the Orlicz spaces relate when changing the mea-
sure μ to another measure μ′ ∈M(Ω,μ).
Proposition 3.10
Let 0 ̸= μ′ ∈M(Ω,μ) be a measure such that dμ′/dμ ∈
Lp(Ω,μ) for some p > 1, and let q > 1 be the dual index, i.e., p−1 + q−1 = 1.
Then for any Young function φ we have
Lφq(μ)⊆Lφ
μ′
,
and this embedding is continuous.
Proof Let h := dμ′/dμ ∈Lp(Ω,μ) and c := ∥h∥p > 0. If f ∈Lφq(μ) and a :=
∥f ∥φq,μ, then by Hölder’s inequality we have

Ω
φ
|f |
a

dμ′ =

Ω
φ
|f |
a

hdμ ≤c
φ
|f |
a

q
= c
φq
|f |
a

1/q
1
(
)*
+
≤1
≤c.
Thus, f ∈Lφ(μ′), and (3.103) implies that ∥f ∥φ,μ′ ≤ca = c∥f ∥φq,μ, which shows
the claim.
□

176
3
Parametrized Measure Models
Finally, we show that the Orlicz norms are complete.
Theorem 3.3 Let φ be a Young function. Then (Lφ(μ),∥· ∥φ,μ) is a Banach space.
Proof Since limt→∞φ(t)/t = 0, Proposition 3.9 implies that we have a continu-
ous inclusion (Lφ(μ),∥· ∥φ,μ) →L1(Ω,μ). In particular, any Cauchy sequence
(fn)n∈N in Lφ(μ) is a Cauchy sequence in L1(Ω,μ), and since the latter space is
complete, it L1-converges to a limit function f ∈L1(Ω,μ). Therefore, after pass-
ing to a subsequence, we may assume that fn →f pointwise almost everywhere.
Let ε > 0. Then there is an N(ε) such that for all n,m ≥N(ε) we have ∥fn −
fm∥φ,μ < ε, that is, for all n,m ≥N(ε) we have

Ω
φ

ε−1(fm −fn)

dμ ≤1.
Taking the pointwise limit n →∞, Fatou’s lemma yields

Ω
φ

ε−1(fm −f )

dμ ≤liminf
n→∞

Ω
φ

ε−1(fm −fn)

dμ ≤1,
which implies that fm −f ∈Lφ(μ) and ∥fm −f ∥φ,μ ≤ε for all m ≥N(ε). There-
fore, f ∈Lφ(μ) and limm→∞∥fm −f ∥φ,μ = 0.
□
3.3.3 Exponential Tangent Spaces
For an arbitrary μ ∈M+, we deﬁne the set
ˆBμ(Ω) :=

f : Ω →[−∞,+∞],|f | < ∞μ-a.e. : ef ∈L1(Ω,μ)

,
which by Hölder’s inequality is a convex subset of the space of measurable functions
Ω →[−∞,+∞]. For μ0, there is a bijection
logμ0 : M+ −→ˆBμ0(Ω),
φ μ0 −→log(φ).
That is, logμ0 canonically identiﬁes M+ with a convex set. Replacing μ0 by a
measure μ′
0 ∈M+, we have logμ′
0 = logμ0 −u, where u := logμ′
0 μ0. Moreover, we
let
Bμ(Ω) := ˆBμ(Ω) ∩

−ˆBμ(Ω)

=

f : Ω →[−∞,∞]
 e±f ∈L1(Ω,μ)

=

f : Ω →[−∞,∞]
 e|f | ∈L1(Ω,μ)

and
B0
μ(Ω) :=

f ∈Bμ(Ω)
 (1 + s)f ∈Bμ(Ω) for some s > 0

.
The points of B0
μ(Ω) are called inner points of Bμ(Ω).

3.3
The Pistone–Sempi Structure
177
Deﬁnition 3.14 Let μ ∈M+. Then
TμM+ :=

f : Ω →[−∞,∞]
 tf ∈Bμ(Ω) for some t ̸= 0

is called the exponential tangent space of M+ at μ.
Evidently, f ∈TμM+ iff

Ω(exp|tf | −1) dμ < ∞for some t > 0. Thus,
TμM+ = Lexp|t|−1(μ),
and hence is an Orlicz space, i.e., it has a Banach norm.6 If ∥f ∥Lexp|t|−1(μ) < 1, then

Ω
(exp|f | −1) dμ ≤1
=⇒

Ω
e|f | dμ < ∞=⇒f ∈Bμ(Ω).
That is, Bμ(Ω)⊆TμM+ contains the unit ball w.r.t. the Orlicz norm and hence is a
neighborhood of the origin. Furthermore, limt→∞tp/(exp|t|−1) = 0 for all p ≥1,
so that Proposition 3.9 implies that
L∞(Ω,μ)⊆TμM+⊆
9
p≥1
Lp(Ω,μ),
(3.104)
where all inclusions are continuous.
Observe that the inclusions in (3.104) are proper, in general. As an example, let
Ω = (0,1) be the unit interval, and let μ = dt be the Lebesgue measure.
Let f (t) := (logt)2. Since
 1
0 |logt|ndt = n! for all n ∈N, it follows that f ∈
Lp((0,1),dt) for all p. However, for x > 0 we have
 1
0
exp

xf (t)

dt =
∞

n=0
1
n!xn
 1
0
log(t)2n dt =
∞

n=0
(2n)!
n! xn.
But this power series diverges for all x ̸= 0, hence (logt)2 /∈Tμ(Ω,μ).
For the ﬁrst inclusion, observe that |logt| ∈Tμ(Ω,μ) as exp(α|logt|) = t−α,
which is integrable for α < 1. However, |logt| is unbounded and hence not in
L∞((0,1),dt).
Remark 3.12 In [106, Deﬁnition 6], TμM+ is called the Cramer class of μ. More-
over, in [106, Proposition 7] (see also [216, Deﬁnition 2.2]), the centered Cramer
class is deﬁned as the functions u ∈TμM+ with

Ω u dμ = 0. Thus, the centered
Cramer class is a closed subspace of codimension one.
6In [216] the Young function cosht −1 was used instead of exp|t| −1. However, these produce
equivalent Orlicz spaces by Proposition 3.9.

178
3
Parametrized Measure Models
In order to understand the topological structure of M+ with respect to the e-
topology, it is useful to introduce the following preorder on M+:
μ′ ⪯μ
if and only if
μ′ = φμ with φ ∈Lp(Ω,μ) for some p > 1.
(3.105)
In order to see that ⪯is indeed a preorder, we have to show transitivity, as the
reﬂexivity of ⪯is obvious. Thus, let μ′′ ⪯μ′ and μ′ ⪯μ, so that μ′ = φμ and
μ′′ = ψμ′ with φ ∈Lp(Ω,μ) and ψ ∈Lp′(Ω,μ′), then φp,ψp′φ ∈L1(Ω,μ) for
some p,p′ > 1. Let λ := (p′−1)/(p+p′−1) ∈(0,1). Then by Hölder’s inequality,
we have
L1(Ω,μ1) ∋

ψp′φ
1−λ
φpλ = ψp′(1−λ)φ1+λ(p−1) = (ψφ)p′′,
where p′′ = pp′/(p + p′ −1) > 1, so that ψφ ∈Lp′′(Ω,μ), and hence, μ′′ ⪯μ as
μ′′ = ψφμ.
From the preorder ⪯we deﬁne the equivalence relation on M+ by
μ′ ∼μ
if and only if
μ′ ⪯μ and μ ⪯μ′,
(3.106)
in which case we call μ and μ′ similar, and hence we obtain a partial ordering on
the set of equivalence classes M+/∼
-
μ′,
⪯[μ]
if and only if
μ′ ⪯μ.
Proposition 3.11
Let μ′ ⪯μ. Then TμM+⊆Tμ′M+ is continuously embedded
w.r.t. the Orlicz norms on these tangent spaces.
In particular, if μ ∼μ′, then TμM+ = Tμ′M+ with equivalent Banach norms.
If we denote the isomorphism class of these spaces as T[μ]M+, then there are con-
tinuous inclusions
T[μ]M+ →T[μ′]M+
if
-
μ′,
⪯[μ].
(3.107)
Proof Let μ′ = φμ with φ ∈Lp(Ω,μ), p > 1, and let q > 1 be the dual index, i.e.,
p−1 + q−1 = 1. Then by Hölder’s inequality,

Ω

exp|tf | −1

dμ′ =

Ω

exp|tf | −1

φ dμ
≤∥φ∥p

Ω

exp|tf | −1
q dμ
1/q
.
(3.108)
Let ψ : R →R, ψ(t) := ∥φ∥q
p(exp|t| −1)q, which is a Young function. Let f ∈
Lψ(μ) and a := ∥f ∥Lψ(μ). Then the right-hand side of (3.108) with t := a−1 is
bounded by 1, whence so is the left-hand side, so that f ∈Lexp|t|−1(μ′) = Tμ′M+
and ∥f ∥Lψ(μ) = a ≥∥f ∥Lexp|x|−1(μ′). Thus, there is a continuous inclusion
Lψ(μ) →Lexp|x|−1
μ′
= Tμ′M+(Ω).

3.3
The Pistone–Sempi Structure
179
But now, as limt→∞
ψ(t)
exp|qt|−1 = ∥φ∥q
p ∈(0,∞), Proposition 3.9 implies that
as Banach spaces, Lψ(μ) ∼= Lexp|qt|−1(μ) and furthermore, Lexp|qt|−1(μ) ∼=
Lexp|t|−1(μ) = Tμ′M+(Ω) by Lemma 3.10.
□
Proposition 3.12 The subspace in (3.107) is neither closed nor dense, unless
μ ∼μ′, in which case these subspaces are equal. In fact, f ∈T[μ′]M+ lies in the
closure of T[μ]M+ if and only if

|f | + ε log

dμ′/dμ

+ ∈T[μ]M+
for all ε > 0.
(3.109)
Here, the subscript refers to the decomposition of a function into its non-negative
and non-positive part, i.e., to the decomposition
ψ = ψ+ −ψ−
with
ψ± ≥0,ψ+ ⊥ψ−.
Proof Let p > 1 be such that φ := dμ′/dμ ∈Lp(Ω,μ), and assume w.l.o.g. that
p ≤2. Furthermore, let u := logφ. Then
K :=

Ω

exp

(p −1)|u|

−1

dμ′ = e−1

Ω
max

φp−1,φ1−p
dμ′
= e−1

Ω
max

φp,φ2−p
dμ ≤e−1

Ω
max

φp,1

dμ < ∞.
If we let ψ(t) := K−1(exp|t| −1), then ψ is a Young function, and by Proposi-
tion 3.9, Lψ(μ′) = Lexp|t|−1(μ′) = T[μ′]M+. For f ∈T[μ′]M+ we have
|f | −

|f | + εu

+
 ≤ε|u|
and therefore

Ω
ψ
||f | −(|f | + εu)+|
ε(p −1)−1

dμ′ ≤

Ω
ψ

(p −1)|u|

dμ′
= K−1

Ω

exp(p −1)|u| −1

dμ′ = 1,
so that ∥|f |−(|f |+εu)+∥Lψ(μ′) ≤ε(p −1)−1 by the deﬁnition of the Orlicz norm.
That is, limε→0(|f | + εu)+ = |f | in Lψ(μ′) = Lexp|t|−1(μ′) = T[μ′]M+.
In particular, if (3.109) holds, then |f | lies in the closure of T[μ′]M+. Observe
that (f±+εu)+ ≤(|f |+εu)+, whence (3.109) implies that (f±+εu)+ ∈T[μ′]M+,
whence f± lie in the closure of T[μ′]M+, and so does f = f+ −f−.
On the other hand, if (fn)n∈N is a sequence in T[μ]M+ converging to f , then
min(|f |,|fn|) is also in T[μ]M+ converging to |f |, whence we may assume w.l.o.g.
that 0 ≤fn ≤f . Let ε > 0 and choose n such that ∥f −fn∥< ε in T[μ′]M+. Then

180
3
Parametrized Measure Models
by the deﬁnition of the Orlicz space we have
1 ≥

Ω

exp

ε−1(f −fn)

−1

dμ′ =

Ω
exp

ε−1(f −fn + εu)

dμ −μ′(Ω)
≥

Ω
exp

ε−1(f −fn + εu)+

dμ −μ′(Ω),
so that

Ω(exp(ε−1(f −fn + εu)+) −1) dμ < ∞, whence (f −fn + εu)+ ∈
T[μ]M+. On the other hand,
(f + εu)+ ≤(f + εu −fn)+ + fn,
and since the summands on the right are contained in T[μ]M+, so is (f + εu)+ as
asserted.
Thus, we have shown that f lies in the closure of T[μ′]M+ iff (3.109) holds.
Let us assume from now on that μ′ ≁μ. It remains to show that in this case,
T[μ]M+⊆T[μ′]M+ is neither closed nor dense. In order to see this, let Ω+ := {ω ∈
Ω | u(ω) ≥0} and Ω−:= Ω\Ω+. Observe that

Ω
exp(pu+)dμ = μ(Ω−) +

Ω+
φp dμ < ∞,
as φ ∈Lp(Ω,μ), whence u+ ∈T[μ]M+.
We assert that |u|a ∈T[μ′]M+, but /∈T[μ]M+ for all a > 0. Namely, pick t > 0
such that ta < min(1,p −1) and calculate

Ω
exp

t|u|a
dμ′ =

Ω+
φta dμ′ +

Ω−
φ−ta dμ′
≤

Ω+
φ1+ta dμ +

Ω−
φ1−ta
( )* +
≤1
dμ ≤

Ω
max

φp,1

dμ < ∞.
Thus, |u|a ∈T[μ′]M+ for all a > 0. On the other hand, if t > 0, then

Ω
exp

t|u|a
dμ =

Ω+
φta dμ +

Ω−
φ−ta dμ
≥

Ω+
1dμ +

Ω−
φ−(1+ta) dμ′ ≥

Ω
φ−(1+ta) dμ′,
and the last integral is ﬁnite if and only if φ−1 ∈L1+ta(Ω,μ′) for some t > 0, if
and only if μ ⪯μ′ and hence μ ∼μ′. Since this case was excluded, it follows that

Ω exp(t|u|a)dμ = ∞for all t > 0, whence |u|a /∈T[μ]M+ as asserted.
Thus, our assertion follows if we can show that |u|a is contained in the closure
of T[μ]M+ for 0 < a < 1, but it is not in the closure of T[μ]M+ for a = 1.
For a = 1 and ε ∈(0,1), (|u| + εu)+ = (1 + ε)u+ + (1 −ε)u−= 2εu+ +
(1 −ε)|u|. Since u+ ∈T[μ]M+ and |u| /∈T[μ]M+, it follows that (|u| + εu)+ /∈

3.3
The Pistone–Sempi Structure
181
T[μ]M+, which shows that (3.109) is violated for f = |u| ∈T[μ′]M+, whence |u|
does not lie in the closure of T[μ]M+, so that the latter is not a dense subspace.
If 0 < a < 1, then (|u|a + εu)+ = ua
+ + εu+ + (ua
−−εu−)+. Now ua
+ + εu+ ≤
(a + ε)u+ + 1, whereas ua
−−εu−≥0 implies that u−≤ε1/(a−1), so that (ua
−−
εu−)+ ≤Cε, where Cε := max{ta −εt | 0 ≤t ≤ε1/(a−1)}.
Thus, (|u|a + εu)+ ≤(a + ε)u+ + 1 + Cε, and since u+ ∈T[μ]M+ this implies
that (|u|a + εu)+ ∈T[μ]M+, so that |u|a lies in the closure of T[μ]M+, but not in
T[μ]M+ for 0 < a < 1. Thus, T[μ]M+ is not a closed subspace of T[μ′]M+.
□
The following now is a reformulation of Propositions 3.4 and 3.5 in [216].
Proposition 3.13
A sequence (gnμ0)n∈N ∈M(Ω,μ0) is e-convergent to g0μ0 ∈
M(Ω,μ0) if and only if gnμ0 ∼g0μ0 for large n, and un := loggn ∈Tg0μ0M+
converges to u0 := logg0 ∈Tgμ0M+ in the Banach norm on Tgμ0M+ described
above.
Proof If (gnμ0)n∈N ∈M(Ω,μ0) e-converges to g0μ0 ∈M(Ω,μ0), then for
large n, (gn/g0) and (g0/gn) are contained in Lp(Ω,g0μ0), so that gnμ0 ∼g0μ0
and hence, un := log|gn| ∈Tg0μ0M+.
Moreover, by Proposition 3.7 (gnμ0)n∈N ∈M(Ω,μ0) e-converges to g0μ0 ∈
M(Ω,μ0) if and only if for all p ≥1
0 = lim
n→∞

Ω

gn
g0
p
−1
 +

g0
gn
p
−1


g0 dμ0
= lim
n→∞

Ω
ep(un−u0) −1
 +
ep(u0−un) −1

g0 dμ0
= lim
n→∞

Ω
2sinh

p|un −u0|

g0 dμ0.
By Proposition 3.8, this is equivalent to saying that (un)n∈N converges to u0 in the
Orlicz space Lsinh|t|(g0μ0).
However, Lsinh|t|(g0μ0) = Lexp|t|−1(g0μ0) = Tg0μ0M+ by Proposition 3.9,
since limt→∞
sinh|t|
exp|t|−1 = 1/2 ∈(0,∞).
□
By virtue of this proposition, we shall refer to the topology on TμM+ obtained
above as the topology of e-convergence or the e-topology. Note that the ﬁrst state-
ment in Proposition 3.13 implies that the equivalence classes of ∼are open and
closed in the e-topology.
Theorem 3.4 Let K⊆M+ be an equivalence class w.r.t. ∼, and let T := T[μ]M+
for μ ∈K be the common exponential tangent space, equipped with the e-topology.
Then for all μ ∈K,
Aμ := logμ(K)⊆T

182
3
Parametrized Measure Models
is open convex, and logμ : K →Aμ is a homeomorphism where K is equipped
with the e-topology. In particular, the identiﬁcation logμ : Aμ →K allows us to
canonically identify K with an open convex subset of the afﬁne space associated
to T .
Remark 3.13 This theorem shows that the equivalence classes w.r.t. ∼are the con-
nected components of the e-topology on M(Ω,μ0), and since each such component
is canonically identiﬁed as a subset of an afﬁne space whose underlying vector space
is equipped with a family of equivalent Banach norms, it follows that M(Ω,μ0) is
a Banach manifold. This is the afﬁne Banach manifold structure on M(Ω,μ0) de-
scribed in [216], therefore we refer to it as the Pistone–Sempi structure.
The subdivision of M+(Ω) into disjoint open connected subsets was also noted
in [230].
Proof of Theorem 3.4 If f ∈Aμ, then, by deﬁnition, (1 + s)f,−sf ∈ˆBμ(Ω) for
some s > 0. In particular, sf ∈Bμ(Ω), so that f ∈T and hence, Aμ⊆T . Moreover,
if f ∈Aμ then λf ∈Aμ for λ ∈[0,1].
Next, if g ∈Aμ, then μ′ := egμ ∈K. Therefore, f ∈Aμ′ if and only if K ∋
ef μ′ = ef +gμ if and only if f + g ∈Aμ, so that Aμ′ = g + Aμ for a ﬁxed g ∈T .
From this, the convexity of Aμ follows.
Therefore, in order to show that Aμ⊆T is open, it sufﬁces to show that 0 ∈Aμ′
is an inner point for all μ′ ∈K. For this, observe that for f ∈B0
μ′(Ω) we have
(1 + s)f ∈Bμ′(Ω) and hence e±(1+s)f ∈L1(Ω,μ′), so that ef ∈L1+s(Ω,μ′)
and e−f ∈L2+s(Ω,ef μ′), whence ef μ′ ∼μ′ ∼μ, so that ef μ′ ∈K and hence,
f ∈Aμ′. Thus, 0 ∈B0
μ′(Ω)⊆Aμ′, and since B0
μ′(Ω) contains the unit ball of the
Orlicz norm, the claim follows.
□
In the terminology which we developed, we can formulate the relation of the
Pistone–Sempi structure on M+ with k-integrability as follows.
Proposition 3.14
The parametrized measure model (M+(Ω,μ),Ω,i) is ∞-
integrable, where i : M+(Ω,μ) →M(Ω) is the inclusion map and M+(Ω,μ)
carries the Banach manifold structure from the e-topology.
Proof The measures in M+(Ω,μ) are dominated by μ, and for the inclusion map
we have
∂V logi

exp(f )μ

= ∂V i
and the inclusion i : TμM+(Ω) →Lk(Ω,μ) is continuous for all k ≥1 by (3.104).
Thus, (M+(Ω,μ),Ω,i) is k integrable for all such k.
□
Example 3.8
Let Ω := (0,1), and consider the 1-parameter family of ﬁnite mea-
sures
p(ξ) = p(ξ,t)dt := exp

−ξ2 (logt)2
dt ∈M+

(0,1),dt

,
x ∈R.

3.3
The Pistone–Sempi Structure
183
Since ∂ξ logp(ξ,t) = −2ξ(logt)2 and
 1
0
∂ξ logp(ξ,t)
k dp(ξ) = 2k|ξ|k
 1
0
(logt)2k exp

−ξ2 (logt)2
dt < ∞
and this expression depends continuously on ξ for all k, it follows that this
parametrized measure model is ∞-integrable.
However, p is not even continuous w.r.t. the e-topology. Namely, in this topology
p(ξ) →p(0) as ξ →0 would imply that
 1
0
p(0,t)
p(ξ,t) −1

dp(0)
ξ→0
−−→0.
Since p(0) = dt, this is equivalent to
 1
0
exp

ξ2 (logt)2
dt
ξ→0
−−→1.
However,
 1
0
exp

ξ2 (logt)2
dt =
∞

k=0
1
n!ξ2n
 1
0
(logt)2n dt =
∞

k=0
(2n)!
n! ξ2n = ∞
for all ξ ̸= 0, so that this expression does not converge.
We end this section with the following result which illustrates how the ordering
⪯provides a stratiﬁcation of ˆBμ0(Ω).
Proposition 3.15 Let μ′
0,μ′
1 ∈M+ with fi := logμ0(μ′
i) ∈ˆBμ0(Ω), and let μ′
λ :=
exp(f0 + λ(f1 −f0))μ0 for λ ∈[0,1] be the segment joining μ′
0 and μ′
1. Then the
following hold:
(1) The measures μ′
λ are similar for λ ∈(0,1).
(2) μ′
λ ⪯μ′
0 and μ′
λ ⪯μ′
1 for λ ∈(0,1).
(3) Tμ′
λM+ = Tμ′
0M+ + Tμ′
1M+ for λ ∈(0,1).
Proof Let δ := f1 −f0 and φ := exp(δ). Then for all λ1,λ2 ∈[0,1], we have
μ′
λ1 = φλ1−λ2μ′
λ2.
(3.110)
For λ1 ∈(0,1) and λ2 ∈[0,1], we pick p > 1 such that λ2 + p(λ1 −λ2) ∈(0,1).
Then by (3.110) we have
φp(λ1−λ2)μ′
λ2 = μ′
λ2+p(λ1−λ2) ∈M+,
so that φp(λ1−λ2) ∈L1(Ω,μ′
λ2) or φλ1−λ2 ∈Lp(Ω,μλ2) for small p−1 > 0. There-
fore, μ′
λ1 ⪯μ′
λ2 for all λ1 ∈(0,1) and λ2 ∈[0,1], which implies the ﬁrst and second
statement.

184
3
Parametrized Measure Models
This implies that Tμ′
iM+⊆Tμ′
λM+ = Tμ′
1/2M+ for i = 0,1 and all λ ∈(0,1)
which shows one inclusion in the third statement.
In order to complete the proof, observe that
Tμ′
1/2M+ = Tμ′
1/2M+(Ω+,μ0) ⊕Tμ′
1/2M+(Ω−,μ0),
where Ω+ := {ω ∈Ω | δ(ω) > 0} and Ω−:= {ω ∈Ω | δ(ω) ≤0}. If g ∈
Tμ′
1/2M+(Ω+,μ0), then for some t ̸= 0

Ω
exp

|tg|

dμ′
0 ≤

Ω+
exp

|tg| + 1
2δ

dμ′
0 +

Ω−
dμ′
0
=

Ω+
exp

|tg|

dμ′
1/2 +

Ω−
dμ′
0 < ∞,
so that g ∈Tμ′
0(Ω,μ0) and hence, Tμ′
1/2M+(Ω+,μ0)⊆Tμ′
0(Ω,μ0). Analogously,
one shows that Tμ′
1/2M+(Ω−,μ0)⊆Tμ′
0(Ω,μ1), which completes the proof.
□

Chapter 4
The Intrinsic Geometry of Statistical Models
4.1 Extrinsic Versus Intrinsic Geometric Structures
In geometry, an extrinsic and intrinsic perspective can be distinguished. Differen-
tial geometry started as the geometry of curves and surfaces in three-dimensional
Euclidean space. Of course, this can be generalized to higher dimensions and codi-
mensions, but Gauss had a deeper insight [102]. His Theorema Egregium says that
the curvature of a surface, now referred to as Gauss curvature, only depends on in-
trinsic measurements within the surface and does not refer to the particular way the
surface is embedded in the ambient three-dimensional Euclidean space. Although
this theorem explicitly refers to the curvature of surfaces, it highlights a more gen-
eral paradigm of geometry. In fact, Riemann developed a systematic approach to
geometry, now called Riemannian geometry, that treats geometric quantities intrin-
sically, as metric structures on manifolds without referring to any embedding into
some Euclidean space [225]. Nevertheless, geometry can also be developed extrinsi-
cally, because Nash’s embedding theorem [196] tells us that any Riemannian man-
ifold can be isometrically embedded into some Euclidean space, that is, it can be
realized as a submanifold of some Euclidean space, and its intrinsic Riemannian
metric then coincides with the restriction of the extrinsic Euclidean metric to that
submanifold. It is therefore a matter of convenience whether differential geometry
is developed intrinsically or extrinsically. In most cases, after all, the intrinsic view
is the more convenient and transparent one. (In algebraic geometry, the situation is
somewhat similar, although more complicated. A projective variety is a subvariety
of some complex projective space, and inherits the latter’s structures; in particular,
we can restrict the Fubini–Study metric to a projective variety, and when the lat-
ter is smooth, it thus becomes a Riemannian manifold. On the other hand, there is
the abstract notion of an algebraic variety. In contrast to the differential geomet-
ric situation, however, not every abstract algebraic variety can be embedded into
some complex projective space, that is, realized as a projective variety. Neverthe-
less, many algebraic varieties can be so embedded, and for those thus also both an
intrinsic and an extrinsic perspective are possible.)
© Springer International Publishing AG 2017
N. Ay et al., Information Geometry, Ergebnisse der Mathematik und ihrer Grenzgebiete.
3. Folge / A Series of Modern Surveys in Mathematics 64,
DOI 10.1007/978-3-319-56478-4_4
185

186
4
The Intrinsic Geometry of Statistical Models
Nevertheless, the two approaches are not completely equivalent. This stems from
the fact that the isometric embedding produced by Nash’s theorem is in general not
unique. Submanifolds of a Euclidean space need not be rigid, that is, there may
exist different isometric embeddings of the same manifold into the same ambient
Euclidean space that cannot be transformed into each other by a Euclidean isom-
etry of that ambient space. In many cases, submanifolds can even be continuously
deformed in the ambient space while keeping their intrinsic geometry. In general,
the rigidity or non-rigidity of submanifolds is a difﬁcult topic that depends not
only on the codimension—the higher the codimension, the more room we have
for deformations—, but also on intrinsic properties. For instance, closed surfaces
of positive curvature in 3-space are rigid, but there exist other closed surfaces that
are not rigid. We do not want to enter into this topic here, but simply point out that
the non-uniqueness of isometric embeddings means that such an embedding is an
additional datum that is not determined by the intrinsic geometry.
Thus, on the one hand, the intrinsic geometry by its very deﬁnition does not
depend on an isometric embedding. Distances and notions derived from them, like
that of a geodesic curve or a totally geodesic submanifold, are intrinsically deﬁned.
On the other hand, the shape of the embedded object in the ambient space inﬂuences
constructions like projections from the exterior onto that object.
Also, the intrinsic and the extrinsic geometry are not equivalent, in the sense
that distances measured intrinsically are generally larger than those measured ex-
trinsically, in the ambient space. The only exception occurs for convex subsets of
afﬁne linear subspaces of Euclidean spaces, but compact manifolds cannot be iso-
metrically embedded in such a manner. We should point out, however, that there
is another, stronger, notion of isometric embedding where intrinsic and extrinsic
distances are the same. For that, the target space can no longer be chosen as a ﬁnite-
dimensional Euclidean space, but it has to be some Banach space. More precisely,
every bounded metric space (X,d) can be isometrically embedded, in this stronger
sense, into an L∞-space. We simply associate to every point x ∈X the distance
function d(x,·),
X →L∞(X)
x →d(x,·).
(4.1)
Since by the triangle inequality
d(x1,·) −d(x2,·)

L∞= sup
y∈X
d(x1,y) −d(x2,y)
 = d(x1,x2),
this embedding does indeed preserve distances.
In information geometry, the situation is somewhat analogous, as we shall now
explain. In Chap. 2, we have developed an extrinsic approach, embedding our pa-
rameter spaces into spaces of measures and deﬁning the Fisher metric in terms of
such embeddings, like the restriction of the Euclidean metric to a submanifold. The
same holds for Chap. 3, where, however, we have mainly addressed the complica-
tions arising from the fact that our space of measures was of inﬁnite dimension. In

4.1
Extrinsic Versus Intrinsic Geometric Structures
187
our formal Deﬁnition 3.4 of statistical models, we explicitly include the embedding
p : M →P+(Ω) and thereby interpret the points of M as being elements of the am-
bient set of strictly positive probability measures.1 This way we can pull back the
natural geometric structures on P+(Ω), not only the Fisher metric g, but also the
Amari–Chentsov tensor T, and consider them as natural structures deﬁned on M.
The question then arises whether information geometry can also alternatively be de-
veloped in an intrinsic manner, analogous to Riemannian geometry. After all, some
important aspects of information geometry are completely captured in terms of the
structures deﬁned on M. For instance, we have shown that the m-connection and the
e-connection on P+(Ω) are dual with respect to the Fisher metric. If we equip M
with the pullback g of the Fisher metric and the pullbacks ∇and ∇∗of the m-and
the e-connection, respectively, then it is easy to see that the duality of ∇and ∇∗
with respect to g, which is an intrinsic property, is inherited from the duality of the
corresponding objects on the bigger space. This duality already captures important
aspects of information geometry, which led Amari and Nagaoka to the deﬁnition of a
dualistic structure (g,∇,∇∗) on a manifold M. It turns out that much of the theory
presented so far can indeed be derived based on a dualistic structure, without as-
suming that the metric and the dual connections are distinguished as natural objects
such as the Fisher metric and the m- and e-connections. Alternatively, Lauritzen
[160] proposed to consider as the basic structure of information geometry a mani-
fold M together with a Riemannian metric g and a 3-symmetric tensor T , where g
corresponds to the Fisher metric and T corresponds to the Amari–Chentsov tensor.
He referred to such a triple (M,g,T ) as a statistical manifold, which, compared to
a statistical model, ignores the way M is embedded in P+(Ω) and therefore only
refers to intrinsic aspects captured by g and T . Note that any torsion-free dualistic
structure in the sense of Amari and Nagaoka deﬁnes a statistical manifold in terms
of T (A,B,C) := g(∇∗
AB −∇AB,C). Importantly, we can also go back from this in-
trinsic approach to an extrinsic one, analogously to Nash’s theorem. In Sect. 4.5 we
shall derive Lê’s theorem which says that any statistical manifold that is compact,
possibly with boundary, can be embedded, in a structure preserving way, in some
P+(Ω) so that it can be interpreted as a statistical model, even with a ﬁnite set Ω of
elementary events. Although we cannot expect such an embedding to be naturally
distinguished, there is a great advantage of having an intrinsically deﬁned structure
similar to the dualistic structure of Amari and Nagaoka or the statistical manifold of
Lauritzen. Such a structure is general enough to be applicable to other contexts, not
restricted to our context of measures. For instance, quantum information geometry,
which is not a subject of this book, can be treated in terms of a dualistic structure.
It turns out that the dualistic structure also captures essential information-geometric
aspects in many other ﬁelds of application [16].
As in the Nash case, the embedding produced by Lê’s theorem is not unique
in general. Therefore, we also need to consider such a statistical embedding as an
additional datum that is not determined by the intrinsic geometry. Therefore, our
1Here, for simplicity of the discussion, we restrict attention to strictly positive probability mea-
sures, instead of ﬁnite signed measures.

188
4
The Intrinsic Geometry of Statistical Models
notion of a parametrized measure model includes some structure not contained in the
notion of a statistical manifold. The question then is to what extent this is relevant
for statistics. Clearly, different embeddings, that is, different parametrized measure
models based on the same intrinsic statistical manifold, constitute different families
of probability measures in parametric statistics. Nevertheless, certain aspects are
intrinsic. For instance, whether a submanifold is autoparallel w.r.t. a connection ∇
depends only on that connection, but not on any embedding. Intrinsically, the two
connections ∇and ∇∗play equivalent roles, but extrinsically, of course, exponential
and mixture families play different roles. By the choice of our embedding, we can
therefore let either ∇or ∇∗become the exponential connection. When, however,
one of them, say ∇, is so chosen then it becomes an intrinsic notion of what an
exponential subfamily is. Therefore, even if we embed the same statistical manifold
differently into a space of probability measures, that is, let it represent different
parametrized families in the sense of parametric statistics, the corresponding notions
of exponential subfamily coincide.
We should also point out that the embedding produced in Lê’s theorem is dif-
ferent from that on which Deﬁnition 3.4 of a signed parametrized measure model
depends, because for the latter the target space S(Ω) is inﬁnite-dimensional (un-
less Ω is ﬁnite), like the L∞-space of (4.1). The strength of Lê’s theorem derives
precisely from the fact that the target space is ﬁnite-dimensional, if the statistical
manifold is compact (possibly with boundary). In any case, in Lê’s theorem, the
structure of a statistical manifold is given and the embedding is constructed. In con-
trast, in Deﬁnition 3.4, the space P(Ω) is used to impose the structure of a statistical
model onto M. The latter situation is also different from that of (4.1), because the
embedding into P(Ω) is not extrinsically isometric in general, but rather induces a
metric (and a pair of connections) on M in the same manner as a submanifold of a
Euclidean space inherits a metric from the latter.
In this chapter, we ﬁrst approach the structures developed in our previous chap-
ters from a more differential-geometric perspective. In fact, with concepts and tools
from ﬁnite-dimensional Riemannian geometry, presented in Appendix B, we can
identify and describe relations among these structures in a very efﬁcient and trans-
parent way. In this regard, the concept of duality plays a dominant role. This is
the perspective developed by Amari and Nagaoka [16] which we shall explore in
Sects. 4.2 and 4.3. The intrinsic description of information geometry will naturally
lead us to the deﬁnition of a dualistic structure. This allows us to derive results solely
based on that structure. In fact, the situation that will be analyzed in depth is when
we not only have dual connections, but when we also have two such distinguished
connections that are ﬂat. Manifolds that possess two such dually ﬂat connections
have been called afﬁne Kähler by Cheng–Yau [59] and Hessian manifolds by Shima
[236]. They are real analogues of Kähler manifolds, and in particular, locally the
entire structure is encoded by some strictly convex function. The second derivatives
of that function yield the metric. In particular, that function then is determined up
to some afﬁne linear term. The third derivatives yield the Christoffel symbols of the
metric as well as those of the two dually ﬂat connections. In fact, for one of them,
the Christoffel symbols vanish, and it is afﬁne in the coordinates w.r.t. which we

4.2
Connections and the Amari–Chentsov Structure
189
have deﬁned and computed the convex function. By a Legendre transform, we can
pass to dual coordinates and a dual convex functions. With respect to those dual
coordinates, the Christoffel symbols, now derived from the dual convex function, of
the other ﬂat connection vanish, that is, they are afﬁne coordinates for the second
connection. Also, the second derivatives of the dual convex function yield the in-
verse of the metric. This structure can also be seen as a generalization of the basic
situation of statistical mechanics where one of our convex functions becomes the
free energy and the other the (negative of the) entropy, see [38]. We’ll brieﬂy return
to that aspect in Sect. 4.5 below.
Finally, we return to the general case of a dualistic structure that is not necessarily
ﬂat and address the question to what extent such a dualistic structure, more precisely
a statistical manifold, is different from our previous notion of a statistical model.
This question will be answered by Lê’s embedding theorem, whose proof we shall
present.
4.2 Connections and the Amari–Chentsov Structure
Let now p(ξ) be a d-dimensional smooth family of probability measures depending
on the parameter ξ. The base measure will not play an important role, and so we
shall simply write integration as

dx in this chapter. The family p(ξ) then has to
deﬁne a statistical model in the sense of Deﬁnition 3.4 with a logarithmic deriva-
tive in the sense of Deﬁnition 3.6, and it has to be 2-integrable when we compute
the Fisher metric and 3-integrable for the Amari–Chentsov tensor, in the sense of
Deﬁnition 3.7. The latter simply means that the integrals underlying the expectation
values in (4.2) and (4.3) below exist; see, for instance, (4.6). We shall henceforth
assume that.
For the Fisher metric (3.41), we then have
gij(ξ) = Ep(ξ)
 ∂
∂ξi logp(·;ξ) ∂
∂ξj logp(·;ξ)

=

Ω
∂
∂ξi logp(x;ξ) ∂
∂ξj logp(x;ξ) p(x;ξ)dx,
(4.2)
and so
∂
∂ξk gij(ξ) = Ep
 ∂
∂ξk
∂
∂ξi logp
∂
∂ξj logp

+ Ep
 ∂
∂ξi logp
∂
∂ξk
∂
∂ξj logp

+ Ep
 ∂
∂ξi logp
∂
∂ξj logp
∂
∂ξk logp

.
(4.3)

190
4
The Intrinsic Geometry of Statistical Models
Therefore, by (B.45),
Γ (0)
ijk = Ep

∂2
∂ξi∂ξj logp
∂
∂ξk logp + 1
2
∂
∂ξi logp
∂
∂ξj logp
∂
∂ξk logp

(4.4)
yields the Levi-Civita connection ∇(0) for the Fisher metric. More generally, we can
deﬁne a family ∇(α), −1 ≤α ≤1, of connections via
Γ (α)
ijk = Ep

∂2
∂ξi∂ξj logp
∂
∂ξk logp + 1 −α
2
∂
∂ξi logp
∂
∂ξj logp
∂
∂ξk logp

= Γ (0)
ijk −α
2 Ep
 ∂
∂ξi log p
∂
∂ξj logp
∂
∂ξk logp

.
(4.5)
We also recall the Amari–Chentsov tensor (3.42)
Ep
 ∂
∂ξi logp
∂
∂ξj logp
∂
∂ξk logp

=

Ω
∂
∂ξi logp(x;ξ) ∂
∂ξj logp(x;ξ) ∂
∂ξk logp(x;ξ) p(x;ξ)dx.
(4.6)
We note the analogy between the Fisher metric tensor (4.2) and the Amari–
Chentsov tensor (4.6). The family ∇(α) of connections thus is determined by a com-
bination of ﬁrst derivatives of the Fisher tensor and the Amari–Chentsov tensor.
Lemma 4.1 All the connections ∇(α) are torsion-free.
Proof A connection is torsion-free iff its Christoffel symbols Γijk are symmetric in
the indices i and j, see (B.32). Equation (4.5) exhibits that symmetry.
□
Lemma 4.2 The connections ∇(−α) and ∇(α) are dual to each other.
Proof
Γ (−α)
ijk
+ Γ (α)
ijk = 2Γ (0)
ijk
yields 1
2(∇(−α) + ∇(α)) = ∇(0). As observed in (B.50), this implies that the two
connections are dual to each other.
□
The preceding can also be developed in abstract terms. We shall proceed in sev-
eral steps in which we shall successively narrow down the setting. We start with a
Riemannian metric g. During our subsequent steps, that metric will emerge as the
abstract version of the Fisher metric. g determines a unique torsion-free connection
that respects the metric, the Levi-Civita connection ∇(0). The steps then consist in
the following:
1. We consider two further connections ∇,∇∗that are dual w.r.t. g. In particular,
∇(0) = 1
2(∇+ ∇∗).

4.2
Connections and the Amari–Chentsov Structure
191
2. We assume that both ∇and ∇∗are torsion-free. It turns out that the tensor T
deﬁned by T (X,Y,Z) = g(∇∗
XY −∇XY,Z) is symmetric in all three entries.
(While connections themselves do not deﬁne tensors, the difference of connec-
tions does, see Lemma B.1.) The structure then is compactly encoded by the
symmetric 2-tensor g and the symmetric 3-tensor T . T will emerge as an ab-
stract version of the Amari–Chentsov tensor. Alternatively, the structure can be
derived from a divergence, as we shall see in Sect. 4.4. Moreover, as we shall see
in Sect. 4.5, any such structure can be isostatistically immersed into a standard
structure as deﬁned by the Fisher metric and the Amari–Chentsov tensor.
3. We assume furthermore that ∇and ∇∗are ﬂat, that is, their curvatures vanish.
In that case, locally, there exists a convex function ψ whose second derivatives
yield g and whose third derivatives yield T . Moreover, passing from ∇to ∇∗
then is achieved by the Legendre transform of ψ. The primary examples of this
latter structure are exponential and mixture families. Their geometries will be
explored in Sect. 4.3.
The preceding structures have been given various names in the literature, and we
shall try to record them during the course of our mathematical analysis.
We shall now implement those steps. First, following Amari and Nagaoka [16],
we formulate
Deﬁnition 4.1 A triple (g,∇,∇∗) on a differentiable manifold M consisting of a
Riemannian metric g and two connections ∇,∇∗that are dual to each other with
respect to g in the sense of Lemma 4.2 is called a dualistic structure on M.
(We might then call the quadruple (M,g,∇,∇∗) a dualistic space or dualistic
manifold, but usually, the underlying manifold M is ﬁxed and therefore need not be
referred to in our terminology.)
Of particular importance are dualistic structures with two torsion-free dual con-
nections. According to the preceding, when g is the Fisher metric, then for any
−1 ≤α ≤1, (g,∇(α),∇(−α)) is such a torsion-free dualistic structure. As we shall
see, any torsion-free dualistic structure is equivalently encoded by a Riemannian
metric g and a 3-symmetric tensor T , leading to the following notion, introduced
by Lauritzen [160] and generalizing the pair consisting of the Fisher metric and the
Amari–Chentsov tensor.
Deﬁnition 4.2
A statistical structure on a manifold M consists of a Riemannian
metric g and a 3-tensor T that is symmetric in all arguments. A statistical manifold
is a manifold M equipped with a statistical structure.
We shall now develop the relation between the preceding notions.
Deﬁnition 4.3 Let ∇,∇∗be torsion-free connections that are dual w.r.t. the Rie-
mannian metric g. Then the 3-tensor
T = ∇∗−∇
(4.7)
is called the 3-symmetric tensor of the triple (g,∇,∇∗).

192
4
The Intrinsic Geometry of Statistical Models
Remark 4.1 The tensor T has been called the skewness tensor by Lauritzen [160].
When the Christoffel symbols of ∇and ∇∗are Γijk and Γ ∗
ijk, then T has com-
ponents
Tijk = Γ ∗
ijk −Γijk.
(4.8)
By Lemma B.1, T is indeed a tensor, in contrast to the Christoffel symbols, be-
cause the non-tensorial term in (B.29) drops out when we take the difference of two
Christoffel symbols. We now justify the name (see [8] and [11, Thm. 6.1]).
Theorem 4.1 The 3-symmetric tensor Tijk is symmetric in all three indices.
Proof First, Tijk is symmetric in the indices i,j, since Γijk and Γ ∗
ijk are, because
they are torsion-free.
To show symmetry w.r.t. j,k, we compare (B.43), that is,
Zg(V,W) = g(∇ZV,W) + g

V,∇∗
ZW

(4.9)
which expresses the duality of ∇and ∇∗, with
Zg(V,W) = (∇Zg)(V,W) + g(∇ZV,W) + g(V,∇ZW)
(4.10)
which follows from the product rule for the connection ∇. This yields
(∇Zg)(V,W) = g

V,

∇∗
Z −∇Z

W

,
(4.11)
and since the LHS is symmetric in V and W, so then is the RHS. Writing this out in
indices yields the required symmetry of T . Indeed, (4.11) yields
(∇∂
∂xi g)jk = g
 ∂
∂xj ,

∇∗−∇

∂
∂xi
∂
∂xk

=: g
 ∂
∂xj ,T ℓ
ij
∂
∂xℓ

,
and hence
Tijk = gkℓT ℓ
ij
(4.12)
is symmetric w.r.t. j,k.
□
Conversely, we have the result of Lauritzen [160].
Theorem 4.2
A metric g and a symmetric 3-tensor T yield a dualistic structure
with torsion-free connections.

4.2
Connections and the Amari–Chentsov Structure
193
Proof Let ∇(0) be the Levi-Civita connection for g. We deﬁne the connection ∇by
g(∇ZV,W) := g

∇(0)
Z V,W

−1
2T (Z,V,W),
(4.13)
or equivalently, with T deﬁned by
g

T (Z,V ),W

= T (Z,V,W),
(4.14)
∇ZV = ∇(0)
Z V −1
2T (Z,V ).
(4.15)
Then ∇is linear in Z and V and satisﬁes the product rule (B.26), because ∇(0) does
and T is a tensor. It is torsion free, because T is symmetric. Indeed,
∇ZV −∇V Z −[Z,V ] = ∇ZV −∇V Z −[Z,V ] −1
2

T (Z,V ) −T (V,Z)

= 0.
Moreover,
∇∗
ZV = ∇(0)
Z V + 1
2T (Z,V )
(4.16)
is a torsion free connection that is dual to ∇w.r.t. g:
g(∇ZV,W) + g

V,∇∗
ZW

= g

∇(0)
Z V,W

+ g

V,∇(0)
Z W

−1
2T (Z,V,W) + 1
2T (Z,W,V )
= g

∇(0)
Z V,W

+ g

V,∇(0)
Z W

by the symmetry of T
= Zg(V,W)
since ∇(0) is the Levi-Civita connection, see (B.46).
□
The pair (g,T ) represents the structure more compactly than the triple (g,∇,∇∗),
because in contrast to the connections, T transforms as a tensor by Lemma B.1. In
fact, from such a pair (g,T ), we can generate an entire family of torsion free con-
nections
∇(α)
Z V = ∇(0)
Z V −α
2 T (Z,V )
for −1 ≤α ≤1,
(4.17)
or written with indices
Γ (α)
ijk = Γ (0)
ijk −α
2 Tijk.
(4.18)
The connections ∇(α) and ∇(−α) are then dual to each other. And
∇(0) = 1
2

∇(α) + ∇(−α)
(4.19)
then is the Levi-Civita connection, because it is metric and torsion free.

194
4
The Intrinsic Geometry of Statistical Models
Such statistical structures will be further studied in Sects. 4.3 and 4.5.
We now return to an extrinsic setting and consider families of probability distri-
butions, equipped with their Fisher metric. We shall then see that for α = ±1, we
obtain additional properties. We begin with an exponential family (3.31),
p(x;ϑ) = exp

γ (x) + fi(x)ϑi −ψ(ϑ)

.
(4.20)
So, here we require that the function exp(γ (x) + fi(x)ϑi −ψ(ϑ)) be integrable
for all parameter values ϑ under consideration, and likewise that expressions like
fj(x)exp(γ (x) + fi(x)ϑi −ψ(ϑ)) or fj(x)fk(x)exp(γ (x) + fi(x)ϑi −ψ(ϑ)) be
integrable as well. In particular, as analyzed in detail in Sects. 3.2 and 3.3, this
is a more stringent requirement than the fi(x) simply being L1-functions. But if
the model provides a family of ﬁnite measures, i.e., if it is a parametrized measure
model, then it is always ∞-integrable, so that all canonical tensors and, in partic-
ular, the Fisher metric and the Amari–Chentsov tensor are well-deﬁned; cf. Exam-
ple 3.3. We observe that we may allow for points x with γ (x) = −∞, as long as
those integrability conditions are not affected. The reason is that exp(γ (x)) can be
incorporated into the base measure.
When we have such integrability conditions, then differentiability w.r.t. the pa-
rameters ϑj holds.
We compute
∂
∂ϑj logp(x;ϑ) =

fj(x) −Ep(fj)

p(x;ϑ).
(4.21)
In particular,
Ep
 ∂
∂ϑk logp

= 0,
(4.22)
because Ep(fj(x) −Ep(fj)) = 0 or because of the normalization

pdx = 1. We
then get
Γ (1)
ijk = Ep

∂2
∂ϑi∂ϑj logp
∂
∂ξk logp

= −
∂2
∂ϑi∂ϑj ψ(ϑ) Ep
 ∂
∂ϑk logp

= 0.
Thus, ϑ yields an afﬁne coordinate system for the connection ∇(1), and we have
Lemma 4.3 The connection ∇(1) is ﬂat.
Proof See (B.34) in the Appendix.
□
Deﬁnition 4.4 The connection ∇(1) is called the exponential connection, abbrevi-
ated as e-connection.

4.2
Connections and the Amari–Chentsov Structure
195
We now consider a family
p(x;η) = c(x) +
d

i=1
gi(x)ηi,
(4.23)
an afﬁne family of probability measures, a so-called mixture family. (We might wish
to add a term ν(η) in order to achieve the normalization

p(x;η)dx = 1, but as one
readily computes that this ν is given by ν(η) = 1 −

(c(x) + 	gi(x)ηi)dx, which
is linear in η, it can simply be incorporated by a redeﬁnition of the functions c(x)
and gi(x). Here, we require that the functions c(x) and gi(x) be integrable, and
the expressions (4.25) and (4.26) below as well. Again, differentiability w.r.t. the
parameters ηj is then obvious.
We then have

c(x)dx = 1,

gi(x)dx = 0
for all i.)
(4.24)
And then,
∂
∂ηi
logp(x;η) = gi(x)
p(x;η),
(4.25)
and
∂2
∂ηi∂ηj
logp(x;η) = −gi(x)gj(x)
p(x;η)2 .
(4.26)
Consequently,
∂2
∂ηi∂ηj
logp + ∂
∂ηi
logp
∂
∂ηj
logp = 0.
This implies
Γ (−1)
ijk
= 0.
(4.27)
In other words, now η is an afﬁne coordinate system for the connection ∇(−1), and
analogously to Lemma 4.3, (4.27) implies
Lemma 4.4 The connection ∇(−1) is ﬂat.
□
Deﬁnition 4.5 ∇(−1) is called the mixture or m-connection.
These connections have already been described in more concrete terms in
Sect. 2.4. Amari and Nagaoka [16] call a triple (g,∇,∇∗) consisting of a Rieman-
nian metric g and two connections that are dual to each other and both ﬂat a dually
ﬂat structure.
We shall now develop an intrinsic perspective, that is, no longer speak about
families of probability distributions, but simply consider a triple consisting of a

196
4
The Intrinsic Geometry of Statistical Models
Riemannian metric and two torsion-free ﬂat connections that are dual to each other.
Let ∇and ∇∗be dually ﬂat connections. We choose afﬁne coordinates ϑ1,...,ϑd,
for ∇; the vector ﬁelds ∂i :=
∂
∂ϑi are then parallel. We deﬁne vector ﬁelds ∂j via

∂i,∂j
= δj
i

=

1
for i = j
0
else

.
(4.28)
We have for any vector V
0 = V

∂i,∂j
=

∇V ∂i,∂j
+

∂i,∇∗
V ∂j
,
and since ∂i is parallel for ∇, we conclude that ∂j is parallel for ∇∗. Since ∇∗is
torsion-free, then also [∂j,∂k] = 0 for all j and k, and so, we may ﬁnd ∇∗-afﬁne
coordinates ηj with ∂j =
∂
∂ηj . Here and in the following, the position of the indices,
i.e., whether we have upper or lower indices, is important because it indicates the
transformation behavior under coordinate changes. For example, if when changing
the ϑ-coordinates ∂i transforms as a vector (contravariantly), then ∂j transforms as
a 1-form (covariantly). For changes of the η-coordinates, the rules are reversed. In
particular, we have
∂j =

∂jϑi
∂i
and
∂i = (∂iηj)∂j
(4.29)
as the transition rules between the ϑ- and η-coordinates. Writing then the metric
tensor in terms of the ϑ- and η-coordinates, resp., as
gij := ⟨∂i,∂j⟩,
gij :=

∂i,∂j
,
(4.30)
we obtain from ⟨∂i,∂j⟩= δj
i
∂ηj
∂ϑi = gij,
∂ϑi
∂ηj
= gij.
(4.31)
Theorem 4.3 There exist strictly convex potential functions ϕ(η) and ψ(ϑ) satis-
fying
ηi = ∂iψ(ϑ),
ϑi = ∂iϕ(η),
(4.32)
as well as
gij = ∂i∂jψ,
(4.33)
gij = ∂i∂jϕ.
(4.34)
Proof The ﬁrst equation of (4.32) can be solved locally iff
∂iηj = ∂jηi.
(4.35)

4.2
Connections and the Amari–Chentsov Structure
197
From the preceding equation, this is nothing but the symmetry
gij = gji,
(4.36)
and so, local solvability holds; moreover, we obtain
gij = ∂i∂jψ.
(4.37)
Thus, ψ is strictly convex. ϕ can be found by the same reasoning or, more elegantly,
by duality; in fact, we simply put
ϕ := ϑiηi −ψ
(4.38)
from which
∂iϕ = ϑi + ∂ϑj
∂ηi
ηj −∂ϑj
∂ηi
∂
∂ϑj ψ = ϑi.
□
Since ψ and ϕ are strictly convex, the relation
ϕ(η) + ψ(ϑ) = ϑiηi
(4.39)
means that they are related by Legendre transformations,
ϕ(η) = max
ϑ

ϑiηi −ψ(ϑ)

,
(4.40)
ψ(ϑ) = max
η

ϑiηi −φ(η)

.
(4.41)
Of course, all these formulae are valid locally, i.e., where ψ and ϕ are deﬁned. In
fact, the construction can be reversed, and all that is needed locally is a convex
function ψ(ϑ) of some local coordinates.
Remark 4.2
(1) Cheng–Yau [59] call an afﬁne structure that is obtained from such local convex
functions a Kähler afﬁne structure and consider it a real analogue of Kähler
geometry. The analogy consists in the fact that the Kähler form of a Kähler
manifold can be locally obtained as the complex Hessian of some function, in
the same manner that here, the metric is locally obtained from the real Hessian.
In fact, concepts that have been developed in the context of Kähler geometry,
like Chern classes, can be transferred to this afﬁne context and can then be used
to derive restrictions for a manifold to carry such a dually ﬂat structure. Shima
[236] speaks of a Hessian structure instead. For instance, Amari and Armstrong
in [13] show that the Pontryagin forms vanish on Hessian manifolds. This is a
strong local constraint.
(2) In the context of decision theory, this duality was worked out by Dawid and
Lauritzen [81]. This includes concepts like the Bregman divergences.

198
4
The Intrinsic Geometry of Statistical Models
(3) In statistics, so-called curved exponential families also play a role; see, for in-
stance, [16]. A curved exponential family is a submanifold of some exponential
family. That is, we have some mapping M′ →M,ξ →ϑ(ξ) from some param-
eter space M′ into the parameter space M of an exponential family as in (4.20)
and consider a family of the form
p(x;ξ) = exp

γ (x) + fi(x)ϑi(ξ) −ψ

ϑ(ξ)

(4.42)
parametrized by ξ ∈M′. The family is called curved because M′ does not need
to carry an afﬁne structure here, and even if it does, the mapping ξ →ϑ(ξ) does
not need to be afﬁne.
In order to see that everything can be derived from a strictly convex function
ψ(ϑ), we deﬁne the metric
gij = ∂i∂jψ
and the α-connection through
Γ (α)
ijk = Γ (0)
ijk −α
2 ∂i∂j∂kψ
where Γ (0)
ijk is the Levi-Civita connection for gij. Since
Γ (0)
ijk = 1
2(gik,j + gjk,i −gij,k) = 1
2∂i∂j∂kψ,
(4.43)
we have
Γ (α)
ijk = 1
2(1 −α)∂i∂j∂kψ,
(4.44)
and since this is symmetric in i and j, ∇(α) is torsion free. Since Γ (α)
ijk + Γ (−α)
ijk
=
2Γ (0)
ijk , ∇(α) and ∇(−α) are dual to each other. Recalling (4.18),
Tijk = ∂i∂j∂kψ
(4.45)
is the 3-symmetric tensor.
In particular, Γ (1)
ijk = 0, and so ∇(1) deﬁnes a ﬂat structure, and the coordinates ϑ
are afﬁne coordinates for ∇(1).
Remark 4.3 As an aside, we observe that in the ϑ-coordinates, the curvature of the
Levi-Civita connection becomes
Rk
lij = 1
2gkngmr

∂j∂n∂rψ ∂i∂l∂mψ −∂j∂l∂mψ ∂i∂n∂rψ
+ 1
2∂j∂l∂rψ ∂i∂m∂nψ −1
2∂j∂m∂nψ ∂i∂l∂rψ

= 1
4

T k
jrT r
ℓi −T k
irT r
ℓj


4.2
Connections and the Amari–Chentsov Structure
199
when writing it in terms of the 3-symmetric tensor. Remarkably, the curvature tensor
can be computed from the second and third derivatives of ψ; no fourth derivatives
are involved. The reason is that the derivatives of the Christoffel symbols in (B.37)
drop out by symmetry because the Christoffel symbols in turn can be computed
from derivatives of ψ, see (4.43), and those commute. The curvature tensor thus
becomes a quadratic expression of coefﬁcients of the 3-symmetric tensor.
In particular, if we subject the ϑ-coordinates to a linear transformation so that at
the point under consideration,
gij = δij,
we get
Rk
lij = 1
4(∂j∂m∂kψ ∂i∂m∂lψ −∂j∂m∂lψ ∂i∂m∂kψ)
= 1
4(TjkmTiℓm −TjℓmTikm)
(4.46)
(where we sum over the index m on the right). In a terminology developed in the
context of mirror symmetry, we thus have an afﬁne structure that in general is not a
Frobenius manifold (see [86] for this notion) because the latter condition would re-
quire that the Witten–Dijkgraaf–Verlinde–Verlinde equations hold, which are equiv-
alent to the vanishing of the curvature. Here, we have found a nice representation
of these equations in terms of the 3-symmetric tensor. While this vanishing may
well happen for the potential functions ψ for certain dually ﬂat structures, it does
not hold for the Fisher metric as we have seen already that it has constant positive
sectional curvature.
The dual connection then is ∇(−1), with Christoffel symbols
Γ (−1)
ijk
= ∂i∂j∂kψ
(4.47)
with respect to the ϑ-coordinates. The dually afﬁne coordinates η can be obtained
as before:
ηj = ∂jψ,
(4.48)
and so also
gij = ∂iηj.
(4.49)
The corresponding potential is again obtained by a Legendre transformation
ϕ(η) = max
ϑ

ϑiηi −ψ(ϑ)

,
ψ(ϑ) + ϕ(η) −ϑ · η = 0,
(4.50)
and
ϑj = ∂jϕ(η),
gij = ∂ϑj
∂ηi
= ∂i∂jϕ(η).
(4.51)

200
4
The Intrinsic Geometry of Statistical Models
We also remark that the Christoffel symbols for the Levi-Civita connection for the
metric gij with respect to the ϑ-coordinates are given by
˜Γ ijk = −Γijk = −1
2∂i∂j∂kψ,
(4.52)
and so
˜Γ (α)ijk = ˜Γ ijk −α
2 ∂i∂j∂kψ = −Γ (−α)
ijk
,
and so, with respect to the dual metric gij, α and −α reverse their rules. So,
˜Γ (1) = −Γ (−1) vanishes in the η-coordinates.
In conclusion, we have shown
Theorem 4.4
A dually ﬂat structure, i.e., a Riemannian metric g together with
two ﬂat connections ∇and ∇∗that are dual with respect to g is locally equivalent
to the datum of a single convex function ψ, where convexity here refers to local
coordinates ϑ and not to any metric.
As observed at the end of Appendix B, it sufﬁces that the connections ∇and
∇∗are dual with respect to g and torsion-free and that the curvature of one of
them vanishes, because this then implies that the other curvature also vanishes, see
Lemma B.2.
One should point out, however, in order to get a global structure from such lo-
cal data, compatibility conditions under coordinate changes need to be satisﬁed. In
general, this is a fundamental point in geometry, but for our present purposes, this is
not so relevant as the manifolds of probability distributions do not exhibit nontrivial
global phenomena. For example, in the case of a ﬁnite underlying space, the prob-
ability distributions are represented by a simplex or a spherical sector, as we have
seen above, and so, the topology is trivial.
For a dually ﬂat structure, completeness of one of the connections does not imply
completeness of the other one. In fact, in information geometry, the exponential con-
nection is complete (under appropriate assumptions) while the mixture connection
is not. (In this regard, see also the example constructed from (B.41) in Appendix B
of an incomplete connection on a complete manifold.)
Finally, we observe that in a dually ﬂat structure, the assumption that a subman-
ifold be autoparallel implies the seemingly stronger property that it is itself dually
ﬂat w.r.t. the induced structure (see [16]).
Lemma 4.5 Let (g,∇,∇∗) be a dually ﬂat structure on the manifold M, and let S
be a submanifold of M. Then, if S is autoparallel for ∇or ∇∗, then it is dually ﬂat
w.r.t. the metrics and connections induced from (g,∇,∇∗).
Proof That S is autoparallel w.r.t., say, ∇means that the restriction of ∇to vector
ﬁelds tangent to S is a connection of S, as observed after (B.42), which then is also

4.3
The Duality Between Exponential and Mixture Families
201
ﬂat. And by (B.43), the connection ∇S,∗induced by ∇∗on S satisﬁes
Z⟨V,W⟩= ⟨∇ZV,W⟩+

V,∇∗
ZW

(4.53)
for all tangent vectors Z of S and vector ﬁelds V , W that are tangent to S. Since ∇∗
is torsion-free, so then is ∇S,∗, and since the curvature of ∇S = ∇vanishes, so then
does the curvature of the dual connection ∇S,∗by Lemma B.2. Thus, both ∇S and
∇S,∗are ﬂat, and S is dually ﬂat w.r.t. the induced metric and connections.
□
4.3 The Duality Between Exponential and Mixture Families
In this section, we shall assume that the parameter space M is a (ﬁnite-dimensional)
differentiable manifold. Instead of considering ϕ and ψ as functions of the coordi-
nates η and ϑ, resp., we may consider them as functions on our manifold M, i.e.,
for p ∈M, instead of
ψ

ϑ(p)

,
we simply write
ψ(p)
by abuse of notation.
We now discuss another important concept of Amari and Nagaoka, see Sect. 3.4
in [16].
Deﬁnition 4.6 For p,q ∈M, we deﬁne the canonical divergence
D(p∥q) := ψ(p) + ϕ(q) −ϑi(p)ηi(q).
(4.54)
(Note the contrast to (4.39) where all expressions were evaluated at the same
point.) From (4.40) or (4.41), we have
D(p∥q) ≥0,
(4.55)
and
D(p∥q) = 0
⇐⇒
p = q
(4.56)
by (4.39) and strict convexity. In general, however, D(p∥q) is not symmetric in
p and q. Thus, while D(p∥q) behaves like the square of a distance function in a
certain sense, it is not a true squared distance function. Also, for a derivative with

202
4
The Intrinsic Geometry of Statistical Models
respect to the ﬁrst argument2
D

(∂i)p
q

: = ∂iD(p∥q)
= ∂iψ(p) −ηi(q)
= ηi(p) −ηi(q)
by (4.32).
(4.57)
This vanishes for all i precisely at p = q. Again, this is the same behavior as shown
locally by the square of a distance function on a Riemannian manifold which has
a global minimum at p = q. Likewise for a derivative with respect to the second
argument
D

p

∂j
q

= ∂jD(p∥q) = ∂jϕ(q) −ϑj(p).
(4.58)
Again, this vanishes for all j iff p = q.
For the second derivatives
D

(∂i∂j)p
q

|q=p = ∂i∂jD(p∥q)|q=p = gij(p)
by (4.37)
(4.59)
and
D

p

∂i∂j
q

|p=q = ∂i∂jD(p∥q)|p=q = gij(q).
(4.60)
Thus, the metric is reproduced from the distance-like 2-point function D(p∥q).
Theorem 4.5 The divergence is characterized by the relation
D(p∥q) + D(q∥r) −D(p∥r) =

ϑi(p) −ϑi(q)

ηi(r) −ηi(q)

,
(4.61)
using (4.39), i.e., ψ(q) + ϕ(q) = ϑi(q)ηi(q).
This is the product of the two tangent vectors at q. Equation (4.61) can be seen
as a generalization of the cosine formula in Hilbert spaces,
1
2∥p −q∥2 + 1
2∥q −r∥2 −1
2∥p −r∥2 = ⟨p −q,r −q⟩.
Proof To obtain (4.61), we have from (4.54)
D(p∥q) + D(q∥r) −D(p∥r)
= ψ(p) + ϕ(q) −ϑi(p)ηi(q) + ψ(q) + ϕ(r) −ϑi(q)ηi(r)
−ψ(p) −ϕ(r) + ϑi(p)ηi(r)
2The notation employed is based on the convention that a derivative involving ϑ-variables operates
only on those expressions that are naturally expressed in those variables, and not on those expressed
in the η-variables. Thus, it operates, for example, on ψ, but not on ϕ.

4.3
The Duality Between Exponential and Mixture Families
203
using (4.39), i.e., ψ(q) + ϕ(q) = ϑi(q)ηi(q). Conversely, if (4.61) holds, then
D(p∥p) = 0 and, by differentiating (4.61) w.r.t. p and then setting r = p,
D((∂i)p∥q) = ηi(p) −ηi(q). Since a solution of this differential equation is
unique, D has to be the divergence. Thus, the divergence is indeed characterized
by (4.61).
□
The following conclusions from (4.61), Corollaries 4.1, 4.2, and 4.3, can be seen
as abstract instances of Theorem 2.8 (note, however, that Theorem 2.8 also handles
situations where the image of the projection is in the boundary of the simplex; this
is not covered by the present result).
Corollary 4.1
The ∇-geodesic from q to p is given by tϑi(p) + (1 −t)ϑi(q),
since ϑi are afﬁne coordinates for ∇, and likewise, the ∇∗-geodesic from q to r is
tηi(r) + (1 −t)ηi(q). Thus, if those two geodesics are orthogonal at q, we obtain
the Pythagoras relation
D(p∥r) = D(p∥q) + D(q∥r).
(4.62)
□
In particular, such a q where these two geodesics meet orthogonally is the point
closest to p on that ∇∗-geodesic. We may therefore consider such a q as the pro-
jection of p on that latter geodesic. By the same token, we can characterize the pro-
jection of p ∈M onto an autoparallel submanifold N of M by such a relation. That
is, the unique point q on N minimizing the canonical divergence D(p∥r) among
all r ∈N is characterized by the fact that the ∇-geodesic from p to N meets N
orthogonally. This observation admits the following generalization:
Corollary 4.2
Let N be a differentiable submanifold of M. Then q ∈N is a sta-
tionary point of the function D(p∥·) : N →R,r →D(p∥r) iff the ∇-geodesic from
p to q meets N orthogonally.
Proof We differentiate (4.61) w.r.t. r ∈N and then put q = r. Recalling that (4.58)
vanishes when the two points coincide, we obtain
−D

p
∂jr

=

ϑi(p) −ϑi(q)

∂jηi(r).
Thus, when we consider a curve r(t),t ∈[0,1] with r(0) = q in N, we have
−d
dt D

p
r(t)

=

ϑi(p) −ϑi(q)
∂ηi(r)
∂ηj
dηj(t)
dt
.
For t = 0, this is the product between the tangent vector of the geodesic from p to
q and the tangent vector of the curve r(t) at q = r(0), as in (4.61). This yields the
claim.
□
We now turn to a situation where we can even ﬁnd and characterize minima for
the projection onto a submanifold.

204
4
The Intrinsic Geometry of Statistical Models
Corollary 4.3
Let N be a submanifold of M that is autoparallel for the connec-
tion ∇∗. Let p ∈M. Then q ∈N satisﬁes
q = argmin
r∈N
D(p∥r)
(4.63)
precisely if the ∇-geodesic from p to q is orthogonal to N at q.
Proof This follows directly from Corollary 4.1, see (4.61).
□
Corollary 4.4
Let N1 ⊆N2 be differentiable submanifolds of M which are au-
toparallel w.r.t. to ∇∗, and assume that N2 is complete w.r.t. ∇.3 Let qi be the pro-
jection in the above sense of some distribution p onto Ni,i = 1,2. Then q1 is also
the projection of q2 onto N1, and we have
D(p∥q1) = D(p∥q2) + D(q2∥q1).
(4.64)
Proof Since q1 ∈N1 ⊆N2, we may apply Corollary 4.1 to get the Pythagoras re-
lation (4.64). By Lemma 4.5, both N2 and N1 are dually ﬂat (although here we ac-
tually need this property only for N2). Therefore, the minimizing ∇-geodesic from
q2 to N1 (which exists as N2 is assumed to be ∇-complete) stays inside N2, and it
is orthogonal to N1 at its endpoint q∗
1 by Corollary 4.3, and by Corollary 4.1 again,
we get the Pythagoras relation
D

p
q∗
1

= D(p∥q2) + D

q2
q∗
1

.
(4.65)
Since D(p∥q∗
1) ≥D(p∥q1) and D(q2∥q∗
1) ≤D(q2∥q1) by the respective mini-
mizing properties, comparing (4.64) and (4.65) shows that we must have equal-
ity in both cases. Likewise, we may apply the Pythagoras relation in N1 to get
D(q2∥q1) = D(q2∥q∗
1) + D(q∗
1|q1) to then infer D(q∗
1∥q1) = 0 which by the prop-
erties of the divergence D (see (4.56)) implies q∗
1 = q1. This concludes the proof. □
We now return to families of probability distributions and consider the prime
example to which we want to apply the preceding theory, the exponential family
(4.20)
p(x;ϑ) = exp

γ (x) + fi(x)ϑi −ψ(ϑ)

,
(4.66)
with
ψ(ϑ) = log

exp

γ (x) + fi(x)ϑi
dx,
(4.67)
that is,
p(x;ϑ) =
1
Z(ϑ) exp

γ (x) + fi(x)ϑi
(4.68)
3We shall see in the proof why this assumption is meaningful.

4.3
The Duality Between Exponential and Mixture Families
205
with the expression
Z(ϑ) :=

exp

γ (x) + fi(x)ϑi
dx = eψ(ϑ),
(4.69)
which is called the zustandssumme or partition function in statistical mechanics.
According to the theory developed in Sect. 4.2, such an exponential family carries
a dually ﬂat structure with the Fisher metric and the exponential and the mixture
connection. We can therefore explore the implications of Theorem 4.4. Also, ex-
ponential subfamilies, that is, when we restrict the ϑ-coordinates to some linear
subspace, inherit such a dually ﬂat structure, according to Lemma 4.5.
There are two special cases of (4.66) where the subsequent formulae will simplify
somewhat. The ﬁrst case occurs when γ (x) = 0. In fact, the general case can be
reduced to this one, because exp(γ (x)) can be incorporated in the base measure
μ(x), compare (3.12). Nevertheless, the function γ will play a role in Sect. 6.2.3.
We could also introduce f0(x) = γ (x) and put the corresponding coefﬁcient ϑ0 = 1.
The other simple case occurs when there are no fi. Anticipating some of the
discussion in Sect. 6.4, we call
Γ

p(·;ϑ)

:= −

p(x;ϑ)γ (x)dx
(4.70)
the potential energy and obtain the relation
ψ = logZ(ϑ)
=

exp

γ (x) −ψ(ϑ)

ψ(ϑ)dx
since

p(x;ϑ)dx = 1
= −

p(x;ϑ)logp(x;ϑ)dx +

p(x;ϑ)γ (x)dx
= −ϕ −Γ,
(4.71)
where −ϕ is the entropy. Thus, the free energy, deﬁned as −ψ, is the difference
between the potential energy and the entropy.4
We return to the general case (4.66). We have the simple identity
∂kZ(ϑ)
∂ϑi1 ...∂ϑik =

fi1 ···fik exp

γ (x) + fi(x)ϑi
dx,
(4.72)
and hence
Ep(fi1 ···fik) =
1
Z(ϑ)
∂kZ(ϑ)
∂ϑi1 ···∂ϑik ,
(4.73)
an identity that will appear in various guises in the rest of this section. Of course, by
(4.69), we can and shall also express this identity in terms of ψ(ϑ). Thus, when we
4The various minus signs here come from the conventions of statistical physics, see Sect. 6.4.

206
4
The Intrinsic Geometry of Statistical Models
know Z, or equivalently ψ, we can compute all expectation values of the “observ-
ables” fi.
First, we put
ηi(ϑ) :=

fi(x)p(x;ϑ)dx = Ep(fi),
the expectation of the coefﬁcient of ϑi w.r.t.
the probability measure p(·;ϑ),
(4.74)
and have from (4.73)
ηi = ∂iψ,
(4.75)
as well as, recalling (3.34),
gij = ∂i∂jψ
for the Fisher information metric,
(4.76)
as computed above. For the dual potential, we ﬁnd
ϕ(η) = ϑiηi −ψ(ϑ)
=
 
logp(x;ϑ) −γ (x)

p(x;ϑ)dx,
(4.77)
involving the entropy −

logp(x;ϑ) p(x;ϑ)dx; this generalizes (4.71). For the
divergence, we get
D(p∥q) = ψ(ϑ) −ϑi

fi(x)q(x;η)dx +
 
logq(x;η) −γ (x)

q(x;η)dx
= ψ(ϑ) −
 
logp(x;ϑ) −γ (x) + ψ(ϑ)

q(x;η)dx
+
 
logq(x;η) −γ (x)

q(x;η)dx
=
 
logq(x) −logp(x)

q(x)dx,
(4.78)
the dual of the Kullback–Leibler divergence DKL introduced in (2.154).
Equation (4.74) is a linear relationship between η and p, and so, by inverting
it, we can express the p(·;ϑ) as linear combinations of the ηi. (We assume here,
as always, that the fi are independent, i.e., that the parametrization of the family
p(x;ϑ) is non-redundant.) In other words, when replacing the coordinates ϑ by η,
we obtain a mixture family
p(x;η) = c(x) + gi(x)ηi.
(Obviously, we are abusing the notation here, because the functional dependence of
p on η will be different from the one on ϑ.)

4.3
The Duality Between Exponential and Mixture Families
207
We check the consistency
ηi(ϑ) =

fi(x)p(x;ϑ)dx =

fi(x)

c(x) + gj(x)ηj

dx
from (4.74), from which we obtain

fi(x)c(x)dx = 0,

fi(x)gj(x)dx = δj
i .
(4.79)
If we consider the potential ϕ(η) given by (4.77), for computing the inverse of
the Fisher metric through its second derivatives as in (4.51), we may suppress the
term −

γ (x) p(x;η)dx as this is linear in η. Then the potential is the negative of
the entropy, and
∂2
∂ηi∂ηj

logp(x;η) p(x;η)dx
=

∂
∂ηi
logp(x;η) ∂
∂ηj
logp(x;η) p(x;η)dx
=

gi(x)gj(x)
1
c(x) + gk(x)ηk
dx
is the inverse of the Fisher metric.
Thus, we have
Theorem 4.6 With respect to the mixture coordinates, (the negative of) the entropy
is a potential for the Fisher metric.
□
It is also instructive to revert the construction and go from the ηi back to the ϑi;
namely, we have
ϑj =
∂
∂ηj
 
c(x) + gi(x)ηi

log

c(x) + gi(x)ηi

−γ (x)

dx
=

gj(x)

log

c(x) + gi(x)ηi

−γ (x) + 1

dx
=

gj(x)

logp(x;η) −γ (x) + 1

dx
=

gj(x)

logp(x;η) −γ (x)

dx
because

gj(x)dx = 0. This is a linear relationship between ϑ and logp(x;η) −
γ (x), and so, we can invert it and write
logp(x;ϑ) = γ (x) + fi(x)ϑi
(4.80)

208
4
The Intrinsic Geometry of Statistical Models
to express logp as a function of ϑ, except that then the normalization

p(x;ϑ)dx = 1 does not necessarily hold, and so, we need to subtract a term
ψ(ϑ), i.e.,
logp(x;ϑ) = γ (x) + fi(x)ϑi −ψ(ϑ).
(4.81)
The reason that ψ(ϑ) is undetermined comes from

gj(x)dx = 0; namely, we
must have the consistency
ϑj =

gj(x)

fi(x)ϑi −ψ(ϑ) + 1

dx
from the above, and this holds because of

gj(x)dx = 0 and

gj(x)fi(x)dx = δj
i .
For our exponential family
p(x;ϑ) = exp

γ (x) + fi(x)ϑi −ψ(ϑ)

,
(4.82)
with ψ(ϑ) = log

exp(γ (x) + fi(x)ϑi)dx, we also obtain a relationship between
the expectation values ηi for the functions fi and the expectation values ηij of the
products fifj (this is a special case of the general identity (4.73)):
ηij =

fi(x)fj(x)exp

γ (x) + fk(x)ϑk −ψ(ϑ)

dx
= exp

−ψ(ϑ)

∂2
∂ϑi∂ϑj

exp

γ (x) + fk(x)ϑk
dx
= exp

−ψ(ϑ)
 ∂
∂ϑi

fj(x)exp

γ (x) + fk(x)ϑk
dx
= exp

−ψ(ϑ)
 ∂
∂ϑi

exp

ψ(ϑ)

ηj

= exp

−ψ(ϑ)

ηj
∂
∂ϑi

exp

γ (x) + fk(x)ϑk
dx + ∂ηj
∂ϑi
= ηiηj + gij,
(4.83)
see (4.49). We thus have the important result
Theorem 4.7
gij = ηij −ηiηj.
(4.84)
Thus, when we consider our coordinates ϑi as the weights given to the observables
fi(x) on the basis of γ (x), our metric gij(ϑ) is then simply the covariance matrix
of those observables at the given weights or coordinates.
□
To put this another way, if our observations yield not only the average values ηi
of the functions fi, but also the averages ηij of the products fifj—which in general
are different from the products ηiηj of the average values of the fi—, and we wish

4.3
The Duality Between Exponential and Mixture Families
209
to represent those in our probability distribution, we need to introduce additional
parameters ϑij and construct
p

x;ϑi,ϑij
= exp

γ (x) + fi(x)ϑi + fi(x)fj(x)ϑij −ψ(ϑ)

(4.85)
with
ϑij =
∂
∂ηij
ϕ(ηi,ηij),
(4.86)
ϕ(ηi,ηij) =
 
logp(x;ηi,ηij) −γ (x)

p(x;ηi,ηij)dx
(4.87)
analogously to the above considerations.
Our deﬁnition (4.54) can also be interpreted in the sense that for ﬁxed ϕ, D(p∥q)
can be taken as our potential ψ(ϑ) as ϕ(η) is independent of ϑ and ϑ · η is linear in
ϑ so that neither of them enters into the second derivatives. Of course, this is (4.59).
Here, p = q then corresponds to η = 0. From (4.57) and Theorem 4.3 (see (4.32) or
(4.75)), we obtain the negative gradient ﬂow for this potential ψ(p) = D(p∥q),
˙ϑi = −gij∂jψ(ϑ) = −gijηj,
(4.88)
or, since
˙ηj = gji ˙ϑi,
(4.89)
˙ηj = −ηj.
(4.90)
This is a linear equation, and the solution moves along a straight line in the η-
coordinates, i.e., along a geodesic for the dual connection ∇∗(see Proposition 2.5),
towards the point η = 0, i.e., p = q. In particular, the gradient ﬂow for the Kullback–
Leibler divergence DKL moves on a straight line in the mixture coordinates.
A special case of an exponential family is a Gaussian one. Let A = (Aij)i,j=1,...,n
be a symmetric, positive deﬁnite n × n-matrix and let ϑ ∈Rn be a vector. The
observables here are the components x1,...,xn of x ∈Rn. We shall use the notation
ϑtx = 	n
i=1 ϑixi and so on.
The Gaussian integral is
I(A,ϑ) : =

dx1 ···dxn exp

−1
2xtAx + ϑtx

= exp
1
2ϑtA−1ϑ

exp

−1
2ytAy

dy1 ···dyn
= exp
1
2ϑtA−1ϑ
(2π)n
detA
 1
2
(4.91)

210
4
The Intrinsic Geometry of Statistical Models
(with the substitution x = A−1ϑ + y; note that the integral exists because A is pos-
itive deﬁnite). Thus, with
ψ(ϑ) := 1
2ϑtA−1ϑ + 1
2 log (2π)n
detA ,
(4.92)
we have our exponential family
p(x;ϑ) = exp

−1
2xtAx + ϑtx −ψ(ϑ)

.
(4.93)
Since ψ is a quadratic function of ϑ, all higher derivatives vanish, and in particu-
lar the connection Γ (−1) = 0 and also the curvature tensor R vanishes, see (4.47),
(4.46).
By (3.34), the metric is given by
gij =
∂2
∂ϑi∂ϑj ψ =

A−1
ij
(4.94)
and is thus independent of ϑ. This should be compared with the results around
(3.35). In contrast to those computations, here A is ﬁxed, and not variable as σ 2.
Equation (4.93) is equivalent to (3.35), noting that μ = ϑ1σ 2 there. It can also be
expressed in terms of moments

xi1 ···xim
: = Ep

xi1 ···xim
=

xi1 ···xim exp(−1
2xtAx + ϑtx)dx1 ···dxn

exp(−1
2xtAx + ϑtx)dx1 ···dxn
=
1
I(A,ϑ)
∂
∂ϑi1 ···
∂
∂ϑim I(A,ϑ).
(4.95)
In fact, we have

xixj
−

xi
xj
=

A−1
ij
(4.96)
by (4.91), in agreement with the general result of (4.84). (In the language of sta-
tistical physics, the second-order moment ⟨xixj⟩is also called a propagator.) For
ϑ = 0, the ﬁrst-order moments ⟨xi⟩vanish because the exponential is then quadratic
and therefore even.
4.4 Canonical Divergences
4.4.1 Dual Structures via Divergences
In this chapter we have studied the intrinsic geometry of statistical models, leading
to the notion of a dualistic structure (g,∇,∇∗) on M. Of particular interest are

4.4
Canonical Divergences
211
dualistic structures with torsion-free dual connections ∇and ∇∗. As we have proved
in Sect. 4.2, see Theorems 4.1 and 4.2, such a structure is equivalently given by
(g,T ), where g is a Riemannian metric and T a 3-symmetric tensor. This is an
abstract and intrinsically deﬁned version of the pair consisting of the Fisher metric
and the Amari–Chentsov tensor. A natural approach to such a structure has been
proposed by Eguchi [93] based on divergences.
Deﬁnition 4.7 Let M be a differentiable manifold. A divergence or contrast func-
tion on M is a real-valued smooth function D : M × M →R, (p,q) →D(p∥q),
satisfying
D(p∥q) ≥0,
D(p∥q) = 0 ⇔p = q,
(4.97)
and moreover,
VpVqD(p∥q)|p=q > 0
(4.98)
for any smooth vector ﬁeld V on M that is non-zero at p. Given a divergence D, its
dual
D∗: M × M →R,
D∗(p∥q) := D(q∥p)
(4.99)
also satisﬁes the conditions (4.97) and (4.98) and is therefore a divergence (contrast
function) on M.
In Sect. 2.7, we have introduced and discussed various divergences deﬁned
on M+(I) and P+(I), respectively, including the relative entropy and the α-
divergence. These divergences were tightly coupled with the Fisher metric and the
α-connection. Furthermore, we have seen the tight coupling of a dually ﬂat structure
with the canonical divergence of Deﬁnition 4.6.
In what follows, we elaborate on how divergences induce dualistic structures.
We use the following notation for a function on M deﬁned by the value of the
partial derivative in M × M with respect to the smooth vector ﬁelds V1,...,Vn, and
W1,...,Wm on M:
D(V1 ···Vn∥W1 ···Wm)(p) := (V1)p ···(Vn)p(W1)q ···(Wm)qD(p∥q)|p=q.
We note that by (4.97), cf. (4.57)
VqD(p∥q)|p=q = VpD(p∥q)|p=q = 0.
(4.100)
By (4.98) the tensor g(D)
g(D)(V,W) := −D(V ∥W)
(4.101)
is a Riemannian metric on M (compare with (4.59)). (Note that this expression is
symmetric, that is, D(V ∥W) = D(W∥V ), in contrast to D(p∥q) which in general

212
4
The Intrinsic Geometry of Statistical Models
is not symmetric.) In addition to this metric, the divergence induces a connection
∇(D), given by
g(D)
∇(D)
X Y,Z

:= −D(XY∥Z).
(4.102)
Applying (4.102) to the dual divergence D∗and noting that g(D) = g(D∗), we obtain
a connection ∇(D∗) that satisﬁes
g(D)
∇(D∗)
X
Y,Z

= −D∗(XY∥Z) = −D(Z∥XY)
(4.103)
for all smooth vector ﬁelds Z on M.
Theorem 4.8 The two connections ∇(D) and ∇(D∗) are torsion-free and dual with
respect to g(D) = g(D∗).
Proof We shall appeal to Theorem 4.2 and show that the tensor
T (D)(X,Y,Z) := g(D)
∇(D∗)
X
Y −∇(D)
X Y,Z

(4.104)
is a symmetric 3-tensor.
We ﬁrst observe that T (D) is a tensor. It is linear in all its arguments, and for any
f ∈C∞(M) we have
T (D)(f X,Y,Z) = f T (D)(X,Y,Z),
because, by Lemma B.1, the difference of two connections is a tensor. Moreover, by
the symmetry of D(X∥Y), we have
T (D)(X,Y,Z) −T (D)(Y,X,Z) = D

[X,Y]
Z

−D

Z
[X,Y]

= 0,
−T (D)(X,Z,Y) + T (D)(X,Y,Z) = D(XY∥Z) + D(Y∥XZ)
−D(XZ∥Y) −D(Z∥XY)
= X

D(Y∥Z) −D(Z∥Y)

= 0,
and hence T (D) is symmetric.
□
We say that a torsion-free dualistic structure (g,∇,∇∗) is induced by a diver-
gence D if
g = g(D),
∇= ∇(D),
and
∇∗= ∇(D∗).
(4.105)

4.4
Canonical Divergences
213
4.4.2 A General Canonical Divergence
We have the following inverse problem:
Given a torsion-free dualistic structure (g,∇,∇∗), is there always a corresponding diver-
gence D that induces that structure?
This question has been positively answered by Matumoto [173]. His result also
follows from Lê’s embedding Theorem 4.10 of Sect. 4.5 (see Corollary 4.5). On
the one hand, it is quite satisfying to know that any torsion-free dualistic structure
can be encoded by a divergence in terms of (4.105). For instance, in Sect. 2.7 we
have shown that the Fisher metric g together with the m- and e-connections can be
encoded in terms of the relative entropy. More generally, g together with the ±α-
connections can be encoded by the α-divergence (see Proposition 2.13). Clearly,
these divergences are special and have very particular meanings within information
theory and statistical physics. The relative entropy generalizes Shannon informa-
tion as reduction of uncertainty and the α-divergence is closely related to the Rényi
entropy [223] and the Tsallis entropy [248, 249]. Indeed, these quantities are cou-
pled with the underlying dualistic structure, or equivalently with g and T, in terms
of partial derivatives, as formulated in Proposition 2.13. However, the relative en-
tropy and the α-divergence are more strongly coupled with g and T than expressed
by this proposition. In general, we have many possible divergences that induce a
given torsion-free dualistic structure, and there is no way to recover the relative
entropy and the α-divergence without making stronger requirements than (4.105).
On the other hand, in the dually ﬂat case there is a distinguished divergence, the
canonical divergence as introduced in Deﬁnition 4.6, which induces the underlying
dualistic structure. This canonical divergence represents a natural choice among the
many possible divergences that satisfy (4.105). It turns out that the canonical diver-
gence recovers the Kullback–Leibler divergence in the case of a dually ﬂat statistical
model (see Eq. (4.78)), which highlights the importance of a canonical divergence.
But which divergence should we choose, if the manifold is not dually ﬂat? For in-
stance in the general Riemannian case, where ∇and ∇∗both coincide with the
Levi-Civita connection of g, we do not necessarily have dual ﬂatness. The need for
a general canonical divergence in such cases has been highlighted in [35]. We can
reformulate the above inverse problem as follows:
Given a torsion-free dualistic structure (g,∇,∇∗), is there always a corresponding “canon-
ical” divergence D that induces that structure?
We use quotation marks because it is not fully clear what we should mean by
“canonical.” Clearly, in addition to the basic requirement (4.105), such a divergence
should coincide with those divergences that we had already identiﬁed as “canonical”
in the basic cases of Sect. 2.7 and Deﬁnition 4.6. We therefore impose the following
two
Requirements:
1. In the self-dual case where ∇= ∇∗coincides with the Levi-Civita connection
of g, the canonical divergence should simply be D(p ∥q) = 1
2d2(p,q).

214
4
The Intrinsic Geometry of Statistical Models
2. We already have a canonical divergence in the dually ﬂat case. Therefore, a gen-
eralized notion of a canonical divergence, which applies to any dualistic struc-
ture, should recover the canonical divergence of Deﬁnition 4.6 if applied to a
dually ﬂat structure.
In [23], Ay and Amari propose a canonical divergence that satisﬁes these re-
quirements, following the gradient-based approach of Sect. 2.7.1 (see also the re-
lated work [14]). Assume that we have a manifold M equipped with a Riemannian
metric g and an afﬁne connection ∇. Here, we are only concerned with the lo-
cal construction of a canonical divergence and assume that for each pair of points
q,p ∈M, there is a unique ∇-geodesic γq,p : [0,1] →M satisfying γq,p(0) = q
and γq,p(1) = p. This is equivalent to saying that for each pair of points q and
p there is a unique vector X(q,p) ∈TqM satisfying expq(X(q,p)) = p, where
exp denotes the exponential map associated with ∇(see (B.39) and (B.40) in Ap-
pendix B). Given a point p, this allows us to consider the vector ﬁeld q →X(q,p),
which we interpreted as difference ﬁeld in Sect. 2.7.1 (see Fig. 2.5). Now, if this vec-
tor ﬁeld is the (negative) gradient ﬁeld of a function Dp, in the sense of Eq. (2.95),
then Dp(q) = D(p ∥q) can be written as an integral along any path from q to p
(see Eq. (2.96)). Choosing the ∇-geodesic γq,p as a particular path, we obtain
D(p ∥q) =
 1
0

X

γq,p(t),p

, ˙γq,p(t)

dt.
(4.106)
Since the geodesic connecting γq,p(t) and p is a part of the geodesic connecting q
and p, corresponding to the interval [t,1], we have
X

γq,p(t),p

= (1 −t) ˙γq,p(t).
(4.107)
Using also the reversed geodesic γp,q(t) = γq,p(1 −t), this leads to the following
representations of the integral (4.106), which we use as a
Deﬁnition 4.8 We deﬁne the canonical divergence associated with a Riemannian
metric g and an afﬁne connection ∇locally by
D(p ∥q) := D∇(p ∥q)
:=
 1
0
(1 −t)
 ˙γq,p(t)
2 dt
(4.108)
=
 1
0
t
 ˙γp,q(t)
2 dt.
(4.109)
It is obvious from this deﬁnition that D(p ∥q) ≥0, and D(p ∥q) = 0 if and
only if p = q, which implies that D is actually a divergence. In the self-dual case,
∇= ∇∗is the Levi-Civita connection with respect to g. In that case, the velocity
ﬁeld ˙γp,q is parallel along the geodesic γp,q, and therefore
 ˙γp,q(t)

γ (t) =
 ˙γp,q(0)

p =
X(p,q)

p = d(p,q),

4.4
Canonical Divergences
215
where d(p,q) denotes the Riemannian distance between p and q. This implies that
the canonical divergence has the following natural form:
D(p ∥q) = 1
2 d2(p,q).
(4.110)
This shows that the canonical divergence satisﬁes Requirement 1 above. In the gen-
eral case, where ∇is not necessarily the Levi-Civita connection, we obtain the en-
ergy of the geodesic γp,q as the symmetrized version of the canonical divergence:
1
2

D(p ∥q) + D(q ∥p)

= 1
2
 1
0
 ˙γp,q(t)
2 dt.
(4.111)
Remark 4.4
(1) We have deﬁned the canonical divergence based on the afﬁne connection ∇
of a given dualistic structure (g,∇,∇∗). We can apply the same deﬁnition to
the dual connection ∇∗instead, leading to a canonical divergence D(∇∗). Note
that, in general we do not have D(∇∗)(p ∥q) = D(∇)(q ∥p), a property that is
satisﬁed for the relative entropy and the α-divergence introduced in Sect. 2.7
(see Eq. (2.111)). In Sect. 4.4.3, we will see that this relation generally holds
for dually ﬂat structures. On the other hand, the mean
¯D(∇)(p ∥q) := 1
2

D(∇)(p ∥q) + D(∇∗)(q ∥p)

always satisﬁes ¯D(∇∗)(p ∥q) = ¯D(∇)(q ∥p), suggesting yet another deﬁnition
of a canonical divergence [11, 23]. For a comparison of various notions of di-
vergence duality, see also the work of Zhang [260].
(2) Motivated by Hooke’s law, Henmi and Kobayashi [119] propose a canonical
divergence that is similar to that of Deﬁnition 4.109.
(3) In the context of a dually ﬂat structure (g,∇,∇∗) and its canonical divergence
(4.54) of Deﬁnition 4.6, Fujiwara and Amari [100] studied gradient ﬁelds that
are closely related to those given by (2.95).
In what follows, we are going to prove that the canonical divergence also satisﬁes
Requirement 2.
4.4.3 Recovering the Canonical Divergence of a Dually Flat
Structure
In the case of a dually ﬂat structure (g,∇,∇∗), a canonical divergence is well-
known, which is given by (4.54) in Deﬁnition 4.6. This is a distinguished divergence
with many natural properties, which we have elaborated on in Sect. 4.3. We are now
going to show that the divergence given by (4.109) coincides with the canonical

216
4
The Intrinsic Geometry of Statistical Models
divergence of Deﬁnition 4.6 in the dually ﬂat case. In order to do so, we consider
∇-afﬁne coordinates ϑ = (ϑ1,...,ϑd) and ∇∗-afﬁne coordinates η = (η1,...,ηd).
In the ϑ-coordinates, the ∇-geodesic connecting p with q has the form
ϑ(t) := ϑ(p) + t

ϑ(q) −ϑ(p)

.
(4.112)
Hence, the velocity is constant
˙ϑ(t) = ϑ(q) −ϑ(p) =: z.
(4.113)
The canonical divergence of ∇is given by
D(∇)(p ∥q) =
 1
0
t gij

ϑ(t)

zizj dt.
(4.114)
Since gij(ϑ) = ∂i∂jψ(ϑ) according to (4.33), where ψ is a strictly convex potential
function, we have
D(∇)(p ∥q) =
 1
0
t ∂i∂jψ

ϑ(p) + t z

zizj dt
=
 1
0
t d2
dt2 ψ

ϑ(t)

dt
= −
 1
0
d
dt ψ

ϑ(t)

dt +
1
t d
dt ψ

ϑ(t)
21
0
= ψ

ϑ(p)

−ψ

ϑ(q)

+ ∂iψ

ϑ(q)

ϑi(q) −ϑi(p)

(4.48)
= ψ

ϑ(p)

−ψ

ϑ(q)

+ ηi(q)

ϑi(q) −ϑi(p)

(4.39)
= ψ

ϑ(p)

+ ϕ

η(q)

−ϑi(p)ηi(q).
(4.115)
Thus, we obtain exactly the deﬁnition (4.54) where ψ(ϑ(p)) is abbreviated by ψ(p)
and ϕ(η(q)) by ϕ(q).
We derived the canonical divergence D for the afﬁne connection ∇based on
(4.109). We now use the same deﬁnition, in order to derive the canonical divergence
D(∇∗) of the dual connection ∇∗. The ∇∗-geodesic connecting p with q has the
following form in the η-coordinates:
η(t) = η(p) + t

η(q) −η(p)

.
(4.116)
Hence, the velocity is constant
˙η(t) = η(q) −η(p) =: z∗.
(4.117)
The divergence D(∇∗) is given by
D(∇∗)(p ∥q) =
 1
0
t gij
η(t)

z∗
i z∗
j dt.
(4.118)

4.4
Canonical Divergences
217
Since gij(η) = ∂i∂jϕ(η), we have
D(∇∗)(p ∥q) =
 1
0
t ∂i∂jϕ

η(p) + tz∗
z∗
i z∗
j dt
=
 1
0
t d2
dt2 ϕ

η(t)

dt
= −
 1
0
d
dt ϕ

η(t)

dt +
1
t d
dt ϕ

η(t)
21
0
= ϕ

η(p)

−ϕ

η(q)

+ ∂iϕ

η(q)

ηi(q) −ηi(p)

(4.51)
= ϕ

η(p)

−ϕ

η(q)

+ ϑi(q)

ηi(q) −ηi(p)

(4.39)
= ϕ

η(p)

+ ψ

ϑ(q)

−ϑi(q)ηi(p).
A comparison with (4.115) shows
D(∇∗)(p ∥q) = D(∇)(q ∥p).
(4.119)
This proves that ∇and ∇∗give the same canonical divergence except that p and
q are interchanged because of the duality. Instances of this general relation in the
dually ﬂat case are given by the α-divergences, see (2.111).
4.4.4 Consistency with the Underlying Dualistic Structure
We have deﬁned our canonical divergence D based on a metric g and an afﬁne
connection ∇(see (4.109)). It is natural to require that the corresponding dualistic
structure (g,∇,∇∗) is encoded by this divergence in terms of (4.105), or, in local
coordinates ξ = (ξ1,...,ξn), by
gij = −D(∂i∥∂j),
Γijk = −D(∂i∂j∥∂k),
Γ ∗
ijk = −D(∂k∥∂i∂j). (4.120)
Since the geometry is determined by the derivatives of D(p ∥q) at p = q, we
consider the case where p and q are close to each other, that is
zi = ξi(q) −ξi(p)
(4.121)
is small for all i. We evaluate the divergence by Taylor expansion up to O(∥z∥3).
Note that X(p,q) is of order ∥z∥.
Proposition 4.1 When ∥z∥= ∥ξ(q) −ξ(p)∥is small, the canonical divergence
(4.109) is expanded as
D(p ∥q) = 1
2 gij(p)zizj + 1
6 Λijk(p)zizjzk + O

∥z∥4
(4.122)

218
4
The Intrinsic Geometry of Statistical Models
where
Λijk = 2∂igjk −Γijk.
(4.123)
Proof The Taylor series expansion of the local coordinates ξ(t) of the geodesic
γp,q(t) is given by
ξi(t) = ξi(p) + t Xi −t2
2 Γ i
jk XjXk + O

t3∥X∥3
,
(4.124)
where X(p,q) = Xi∂i. This follows from γp,q(0) = p, ˙γp,q(0) = X(p,q), and
∇˙γp,q ˙γp,q = 0 (see the geodesic equation (B.38) in Appendix B). When ∥z∥is small,
X is of order O(∥z∥). Hence, we regard (4.124) as a Taylor expansion with respect
to X and t ∈[0,1] when z is small. When t = 1, we have
zi = Xi −1
2 Γ i
jkXj Xk + O

∥X∥3
.
(4.125)
This in turn gives
Xi = zi + 1
2 Γ i
jk zjzk + O

∥z∥3
.
(4.126)
We calculate D(p ∥q) by using the representation (4.109). The velocity at t is given
as
˙ξi(t) = Xi −t Γ i
jk XjXk + O

t2∥X∥3
(4.127)
= zi + 1
2(1 −2t)Γ i
jk zjzk + O

t2∥z∥3
.
(4.128)
We also use
gij

γp,q(t)

= gij(p) + t ∂kgij(p)zk + O

t2∥z∥2
.
(4.129)
Collecting these terms, we obtain
t gij

γp,q(t)
 ˙ξi(t) ˙ξj(t)
= t gij(p)zizj +

t2 ∂igjk(p) + (−2t2 + t)Γijk(p)

zizjzk + O

t3∥z∥4
.
By integration, we have
D(p ∥q) =
 1
0
t gij

γp,q(t)
 ˙ξi(t) ˙ξj(t)dt
(4.130)
= 1
2 gij(p)zizj + 1
6 Λijk(p)zizjzk + O

∥z∥4
,
(4.131)
where indices of Λijk are symmetrized by means of multiplication with zizjzk. □

4.5
Statistical Manifolds and Statistical Models
219
Theorem 4.9 ([23, Theorem 1])
Let (g,∇,∇∗) be a torsion-free dualistic struc-
ture. Then the canonical divergence D(∇)(p ∥q) of Deﬁnition 4.8 is consistent with
(g,∇,∇∗) in the sense of (4.105).
Proof Without loss of generality, we restrict attention to the connection ∇and con-
sider only the canonical divergence D = D(∇). By differentiating equation (4.122)
with respect to ξ(p), we obtain
∂i D(p ∥q)
= 1
2∂igjk(p)zjzk −gij(p)zj −1
2 Λijk(p)zjzk + O

∥z∥3
,
(4.132)
∂i∂jD(p ∥q)
= 1
2 ∂i∂jgkl(p)zkzl −2∂igjk(p)zk + gij + Λijk(p)zk + O

∥z∥2
.
(4.133)
We need to symmetrize the indexed quantities of the RHS with respect to i,j. By
evaluating ∂i∂jD(p ∥q) at p = q, i.e., z = 0, we have
g(D)
ij
= −D(∂i ∥∂j) = D(∂i∂j ∥·) = gij,
(4.134)
proving that the Riemannian metric derived from D is the same as the original one.
We further differentiate (4.133) with respect to ξ(q) and evaluate it at p = q. This
yields
Γ (D)
ijk = −D(∂i∂j ∥∂k) = 2∂igjk −Λijk
= Γijk.
(4.135)
Hence, the afﬁne connection ∇(D) derived from D coincides with the original afﬁne
connection ∇, given that ∇is assumed to be torsion-free.
□
4.5 Statistical Manifolds and Statistical Models
In Sect. 4.2 we have analyzed the notion of a statistical manifold (Deﬁnition 4.2),
introduced by Lauritzen [160] as a formalization of the notion of a statistical model
and showed the equivalence between a statistical manifold and a manifold provided
with a torsion-free dualistic structure. In this section, we shall analyze the rela-
tion between the Lauritzen question, whether any statistical manifold is induced
by a statistical model, and the existence of an isostatistical immersion (Deﬁni-
tion 4.9, Lemma 4.8). The main theorem of this section asserts that any statistical
manifold is induced by a statistical model (Theorem 4.10). The proof will occupy
Sects. 4.5.3, 4.5.4. In Sect. 4.5.2 we shall study some simple obstructions to the
existence of isostatistical immersions, which are helpful for understanding the strat-
egy of the proof of Theorem 4.10. Finally, in Sect. 4.5.5 we shall strengthen our

220
4
The Intrinsic Geometry of Statistical Models
immersion theorem by showing that we can embed any compact statistical manifold
(possibly with boundary) into (P+([N]),g,T) for some ﬁnite N (Theorem 4.11).
An analogous (but weaker) embedding statement for non-compact statistical mani-
folds will also follow.
All manifolds in this section are assumed to have ﬁnite dimension.
4.5.1 Statistical Manifolds and Isostatistical Immersions
In Deﬁnition 4.2, we have introduced Lauritzen’s notions of a statistical manifold,
that is, a manifold M equipped with a Riemannian metric g and a 3-symmetric
tensor T .
Remark 4.5 As in the Riemannian case [195], we call a smooth manifold M pro-
vided with a statistical structure (g,T ) that are Ck-differentiable a Ck-statistical
manifold. Occasionally, we shall drop “Ck” before “statistical manifold” if there is
no danger of confusion.
The Riemannian metric g generalizes the notion of the Fisher metric and the
3-symmetric tensor T generalizes the notion of the Amari–Chentsov tensor. Statis-
tical manifolds also encompass the class of differentiable manifolds supplied with a
divergence as we have introduced in Deﬁnition 4.7.
As follows from Theorem 4.1, a torsion-free dualistic structure (g,∇,∇∗) de-
ﬁnes a statistical structure. Conversely, by Theorem 4.2 any statistical structure
(g,T ) on M deﬁnes a torsion-free dualistic structure (g,∇,∇∗) by (4.15) and the
duality condition ∇AB + ∇∗
AB = 2∇(0)
A B, where ∇(0) is the Levi-Civita connec-
tion, see (4.16). As Lauritzen remarked, the representation (M,g,T ) is practical for
mathematical purposes, because as a symmetric 3-tensor, T has simpler transforma-
tional properties than ∇[160].
Lauritzen raised in [160, §4, p. 179] the question of whether any statistical man-
ifold is induced by a statistical model. More precisely, he wrote after giving the def-
inition of a statistical manifold: “The above deﬁned notion could seem a bit more
general than necessary, in the sense that some Riemannian manifolds with a sym-
metric trivalent tensor T might not correspond to a particular statistical model.”
Turning this positively, the question is whether for a given (Ck)-statistical manifold
(M,g,T ) we can ﬁnd a sample space Ω and a (Ck+1-)smooth family of probability
distributions p(x;ξ) on Ω with parameter ξ ∈M such that (cf. (4.2), (4.6))
g(ξ;V1,V2) = Ep(·;ξ)
 ∂
∂V1
logp(·;ξ) ∂
∂V2
logp(·;ξ)

,
(4.136)
T (ξ;V1,V2,V3) = Ep(·;ξ)
 ∂
∂V1
logp(·;ξ)
∂
∂V2
logp(·;ξ)
∂
∂V3
logp(·;ξ)

.
(4.137)

4.5
Statistical Manifolds and Statistical Models
221
If (4.136) and (4.137) hold, we shall call the function p(x;ξ) a probability density
for g and T , see also Remark 3.7. We regard the Lauritzen question as the existence
question of a probability density for the tensors g and T on a statistical manifold
(M,g,T ).
Our approach in solving the Lauritzen question is to reduce the existence prob-
lem of probability densities on a statistical manifold to an immersion problem of
statistical manifolds.
Deﬁnition 4.9
A smooth (resp. C1) map h from a smooth (resp. C0) statistical
manifold (M1,g1,T1) to a statistical manifold (M2,g2,T2) will be called an isosta-
tistical immersion if h is an immersion of M1 into M2 such that g1 = h∗(g2), T1 =
h∗(T2).
Of course, the notion of an isostatistical immersion is an intrinsic counterpart
of that of a sufﬁcient statistic. In fact, a sufﬁcient statistic as deﬁned in (5.1) or in
Deﬁnition 5.8 is characterized by the fact that it preserves the Fisher metric and the
Amari–Chentsov tensor, see Theorems 5.5 and 5.6.
Lemma 4.6
Assume that h : (M1,g1,T1) →(M2,g2,T2) is an isostatistical im-
mersion. If there exist a measure space Ω and a function p(x;ξ2) : Ω × M2 →R
such that p is a probability density for the tensors g2 and T2 then h∗(p)(x;ξ1) :=
p(x;h(ξ1)) is a probability density for g1 and T1.
Proof Since h is an isostatistical immersion, we have
g1(ξ;V1,V2) = g2

h(ξ);h∗(V1),h∗(V2)

=

Ω
∂
∂h∗(V1) logp

x;h(ξ)

∂
∂h∗(V2) logp

x;h(ξ)

p

x;h(ξ)

dx
= Eh∗(p)
 ∂
∂V1
logh∗(p)(·;ξ) ∂
∂V2
logh∗(p)(·;ξ)

.
Thus h∗(p) is a probability density for g1. In the same way, h∗(p) is a probability
density for T1. This completes the proof of Lemma 4.6.
□
Example 4.1
(1) The statistical manifold5 (P+([n]),g,T) has a natural probability density p ∈
C∞([n] × P+([n])) deﬁned by p(x;ξ) := ξ(x).
(2) Let g0 denote the Euclidean metric on Rn as well as its restriction to the positive
sector Rn
+. Let {ei} be an orthonormal basis of Rn. Denote by {xi} the dual basis
5P+([n]) is the interior of the probability simplex Σn−1.

222
4
The Intrinsic Geometry of Statistical Models
of (Rn)∗. Set
T ∗:=
n

i=1
2dx3
i
xi
.
Then (Rn
+,g0,T ∗) is a statistical manifold. By Proposition 2.1 the embedding
π1/2 : P+

[n]

→Rn
+,
ξ =
n

i=1
p(i;ξ)δi →2
n

i=1

p(i;ξ) ei,
where δi is the Dirac measure concentrated at i ∈[n], is an isometric embed-
ding of the Riemannian manifold (P+([n]),g) into the Riemannian manifold
(Rn
+,g0).
Now let us compute (π1/2)∗(T ∗). Since xi(π1/2(ξ)) = 2√p(i;ξ), we obtain

π1/2∗
T ∗
(ξ;V1,V1,V1) =
n

i=1
2(∂V (2√p(i;ξ)))3
2√p(i;ξ)
=
n

i=1
(∂V p(i;ξ))3
p(i;ξ)2
=
n

i=1

∂V logp(i;ξ)
3p(i;ξ) = T(ξ;V1,V1,V1).
This shows that π1/2 is an isostatistical immersion of the statistical manifold
(P+([n]),g,T) into the statistical manifold (Rn
+,g0,T ∗).
Now we formulate our answer to Lauritzen’s question.
Theorem 4.10 (Existence of isostatistical immersions (cf. [162]))
Any smooth
(resp. C0) compact statistical manifold (M,g,T ) (possibly with boundary) admits
an isostatistical immersion into the statistical manifold (P+([N]),g,T) for some
ﬁnite number N. Any non-compact statistical manifold (M,g,T ) admits an immer-
sion I into the space P+(N) of all positive probability measures on the set N of
all natural numbers such that g is equal to the Fisher metric deﬁned on I(M) and
T is equal to the Amari–Chentsov tensor deﬁned on I(M). Hence any statistical
manifold is a statistical model.
This theorem then links the abstract differential geometry developed in this chap-
ter with the functional analysis established in Chap. 3.
This result will be proved in Sects. 4.5.3, 4.5.4. In the remainder of this section,
compact manifolds may have non-empty boundary.
Since the statistical structure (g,T) on P+([N]) is deﬁned by the canonical diver-
gence [16, Theorem 3.13], see also Proposition 2.13, we obtain from Theorem 4.10
the following result due to Matumoto.

4.5
Statistical Manifolds and Statistical Models
223
Corollary 4.5 (Cf. [173, Theorem 1])
For any statistical manifold (M,g,T ) we
can ﬁnd a divergence D of M which deﬁnes g and T by the formulas (4.101),
(4.103).
Proof Assume that a statistical manifold (M,g,T ) is compact. By Theorem 4.10
(M,g,T ) admits an isostatistical immersion I
into a statistical manifold
(P+([N]),g,T) for some ﬁnite number N. This statistical manifold is compatible
with the KL-divergence (compare with Proposition 2.13 for α = −1), which implies
that the contrast function M × M →R, (p,q) →DKL(I(p)∥I(q)), is compatible
with (M,g,T ).
Now assume that (M,g,T ) is a non-compact manifold. It is known that
(M,g,T ) admits a countable locally ﬁnite open cover {Ui} such that each Ui is
a subset of a compact subset in M. By the argument above, each statistical manifold
(Ui,g|Ui,T|Ui) admits a compatible divergence Di. With a partition of unity we can
glue the divergence functions Di and deﬁne a smoothly extended contrast function
on M × M, thereby following Matumoto’s ﬁnal step of his construction [173].
□
4.5.2 Monotone Invariants of Statistical Manifolds
Before going to develop a strategy for a proof of Theorem 4.10 we need to under-
stand what could be an obstruction for the existence of an isostatistical immersion
between statistical manifolds.
Deﬁnition 4.10 Let K(M,e) denote the category of statistical manifolds M with
morphisms being embeddings. A functor of this category is called a monotone in-
variant of statistical manifolds.
Remark 4.6
Since any isomorphism between statistical manifolds deﬁnes an in-
vertible isostatistical immersion, any monotone invariant is an invariant of statistical
manifolds.
In this subsection we study some simple monotone invariants of statistical mani-
folds and refer the reader to [163] for more sophisticate monotone invariants.
Let f : (M1,g1,T1) →(M2,g2,T2) be a statistical immersion. Then for any
x ∈M1 the differential df : (TxM1,g1(x),T1(x))→(Tf (x)M2,g2(f (x)),T2(f (x)))
deﬁnes an isostatistical immersion of the statistical manifold (TxM1,g1(x),T1(x))
into the statistical manifold (Tf (x)M2,g2(f (x)),T2(f (x))).
A statistical manifold (Rm,g,T ) is called a linear statistical manifold if g and
T are constant tensors.
Thus we start our study by investigating functors of the subcategory Kl(M,e) of
linear statistical manifolds M = (Rn,g,T ). Such a functor will be called a linear
monotone invariant.

224
4
The Intrinsic Geometry of Statistical Models
Given a linear statistical manifold M = (Rn,g,T ) we set
M3(T ) :=
max
|x|=1,|y|=1,|z|=1T (x,y,z),
M2(T ) :=
max
|x|=1,|y|=1T (x,y,y),
M1(T ) := max
|x|=1T (x,x,x).
Clearly, we have
0 ≤M1(T ) ≤M2(T ) ≤M3(T ).
Proposition 4.2
The comasses Mi, i ∈[1,3], are non-negative linear monotone
invariants, which vanish if and only if T = 0.
Proof Clearly Mi(T ) ≥0 for i = 1,2,3. Now we are going to show that M1 van-
ishes at T only if T = 0. Observe that M1 = 0 if and only if T (x,x,x) = 0 for
all x ∈Rn. Writing T in coordinate expression T (x,y,z) = 	aijkxiyjzk, we note
that T (x,x,x) = 0 if and only if T = 0, since T is symmetric.
Next we shall show that Mi(T ) is a linear monotone invariant for i = 1,2,3. As-
sume that e is a linear embedding (Rn,g,T ) into (Rm, ¯g, ¯T ). Then T is a restriction
of the 3-symmetric tensor ¯T . Hence we have
Mi(T ) ≤Mi( ¯T ),
for i = 1,2,3.
This implies that Mi are linear monotone invariants.
□
Thus Mi is a functor from the category Kl(M,e) of linear statistical manifolds
to the category (R,≤) of real numbers with morphism being the relation “≤”.
Using the linear statistical monotone invariant M1, we deﬁne for any statistical
manifold (M,g,T ) the following number
M1
0(T ) := sup
x∈M
M1
T (x)

.
Since the restriction of M1
0 to the subcategory Kl(M,e) is equal to M1, we shall
abbreviate M1
0 as M1, if there is no danger of confusion.
By Proposition 4.2 we obtain immediately
Proposition 4.3 The comass M1 is a non-negative monotone invariant, which van-
ishes if and only if T = 0.
Thus M1 is a functor from the category K(M,e) of statistical manifolds to the
category (R,≤) of real numbers with morphism being the relation “≤”.
In what follows we shall show two applications of the monotone invariant M1.
Proposition 4.4 below will guide our strategy of the proof of Theorem 4.10 in the

4.5
Statistical Manifolds and Statistical Models
225
later part of this section. The equality (4.138) below suggests that the statistical
manifold (P+([N]),g,T) might be a good candidate for a target of isostatistical
embeddings of statistical manifolds.
Proposition 4.4 A statistical line (R,g0,T ) can be embedded into a linear statis-
tical manifold (RN,g0,T ′) if and only if M1(T ) ≤M1(T ′).
Proof The “only” assertion of Proposition 4.4 is obvious. Now we shall show
that we can embed (R,g0,T ) into (RN,g0,T ′) if we have M1(T ) ≤M1(T ′).
We note that T ′(v,v,v) deﬁnes an anti-symmetric function on the sphere
SN−1(|v| = 1) ⊆RN. Thus there is a point v ∈SN−1 such that T ′(v,v,v) =
M1(T ). Clearly, the line {t · v|t ∈R} deﬁnes the required embedding.
□
The example of the family of normal distributions treated on page 132 yields the
normal Gaussian statistical manifold (Γ 2,g,T). Recall that Γ 2 is the upper half
of the plane R2(μ,σ), g is the Fisher metric and T is the Amari–Chentsov tensor
associated to the probability density
p(x;μ,σ) =
1
√
2π σ
exp
−(x −μ)2
2σ 2

,
where x ∈R.
Proposition 4.5 The statistical manifold (P+([N]),g,T) cannot be embedded into
the Cartesian product of m copies of the normal Gaussian statistical manifold
(Γ 2,g,T) for any N ≥4 and ﬁnite m.
Proof By Lemma 4.9 proved below, we obtain for N ≥4
M1
P+

[N]

,g,T

= ∞.
(4.138)
(In chronological order Lemma 4.9 has been invented for proving (4.138). We
decide to move Lemma 4.9 to a later subsection for a better understanding of the
proof of Theorem 4.10.)
The tensor g of the manifold (Γ 2,g,T) has been computed on p. 132. T can be
computed analogously. (The formulas are due to [160].)
g
 ∂
∂μ, ∂
∂μ

= 1
σ 2 ,
g
 ∂
∂μ, ∂
∂σ

= 0,
g
 ∂
∂σ , ∂
∂σ

= 2
σ 2 ,
T
 ∂
∂μ, ∂
∂μ, ∂
∂μ

= 0 = T
 ∂
∂μ, ∂
∂σ , ∂
∂σ

,

226
4
The Intrinsic Geometry of Statistical Models
T
 ∂
∂μ, ∂
∂μ, ∂
∂σ

= 2
σ 3 ,
T
 ∂
∂σ , ∂
∂σ , ∂
∂σ

= 8
σ 3 .
M1(R2(μ,σ)) < ∞. It follows that the norm M1 of a direct product of ﬁnite
copies of R2(μ,σ) is also ﬁnite. Since M1 is a monotone invariant, the space
P+([N]) cannot be embedded into the direct product of m copies of the normal
Gaussian statistical manifold for any N ≥4 and any m < ∞.
□
After investigating obstructions to the existence of isostatistical immersions, we
now return to the proof of Theorem 4.10.
4.5.3 Immersion of Compact Statistical Manifolds into Linear
Statistical Manifolds
We denote by T0 the “standard” 3-tensor on Rm:
T0 =
m

i=1
dx3
i .
One important class of linear statistical manifolds are those of the form
(Rm,g0,A · T0) where A ∈R.
In the ﬁrst step of our proof of Theorem 4.10 we need the following
Proposition 4.6
Let (Mm,g,T ) be a compact smooth (resp. C0) statistical
manifold. Then there exist numbers N ∈N+ and A > 0 as well as a smooth
(resp. C1) immersion f : (Mm,g,T ) →(RN,g0,A · T0) such that f ∗(g0) = g and
f ∗(A · T0) = T .
The constant A enters into Proposition 4.6 to ensure that the monotone invariants
Mi(RN,g0,A · T0) can be sufﬁciently large.
Our proof of Proposition 4.6 uses the Nash immersion theorem, the Gromov im-
mersion theorem and an algebraic trick. We also note that, unlike the Riemannian
case, Proposition 4.6 does not hold for non-compact statistical manifolds. For exam-
ple, for any n ≥4, the statistical manifold (P+([n]),g,T) does not admit an isosta-
tistical immersion to any linear statistical manifold. This follows from the theory of
monotone invariants of statistical manifolds developed in [163], where we showed
that the monotone invariant M1 of (P+([n]),g,T) is inﬁnite, and the monotone in-
variant M1 of any linear statistical manifold is ﬁnite [163, §3.6, Proposition 4.2],
or see the proof of Proposition 4.5 above.
Nash’s embedding theorem ([195, 196]) Any smooth (resp. C0) Riemannian man-
ifold (Mn,g) can be isometrically embedded into (RN(n),g0) for some N(n) de-
pending on n and on the compactness property of Mn.

4.5
Statistical Manifolds and Statistical Models
227
Remark 4.7
One important part in the proof of the Nash embedding theorem for
C0-Riemannian manifolds (Mn,g) is his immersion theorem ([195, Theorems 2, 3,
4, p. 395]), where the dimension N(n) of the target Euclidean space depends on the
dimension W(n) of Whitney’s immersion [256] of Mn into RW(n) and hence N(n)
can be chosen not greater than min(W(n) + 2,2n −1). If Mn is compact (resp.
non-compact), then Nash proved that (Mn,g) can be C1-isometrically embedded in
(R2n,g0) (resp. (R2n+1,g0)). Nash’s results on C1-embedding have been sharpened
by Kuiper in 1955 by weakening the dependency of N(n) on the dimension of the
Whitney embedding of Mn into RW ′(n) [154].
Nash proved his isometric embedding theorem for smooth (actually Ck, k ≥3)
Riemannian manifolds (Mn,g) in 1956 for N(n) = (n/2)(3n + 11) if Mn is
compact [196, Theorem 2, p. 59], and for N(n) = 3
2n3 + 7n2 + 5
2n if Mn is
non-compact [196, Theorem 3, p. 63]. The best upper bound estimate N(n) ≤
max{n(n+5)/2,n(n+3)/2+5} for the smooth (compact or non-compact) case has
been obtained by Günther in 1989, using a different proof strategy [114, pp. 1141,
1142]. Whether the isometric immersion theorem is also true for C2-Riemannian
manifolds remains unknown. The problem is that the C0-case and the case of
C2+α,α > 0 (including the smooth case) are proved by different methods. The C0-
case is proved by a limit process, where we have control on the ﬁrst derivatives but
no control on higher derivatives of an immersion. So the limit immersion may not be
smooth though the original immersion is smooth. On the other hand, the C2+α-case
is proved by the famous Nash implicit function theorem, which was developed later
by Gromov in [112, 113].
Gromov’s immersion theorem ([113, 2.4.9.3’ (p. 205), 3.1.4 (p. 234)]) Suppose
that Mm is given with a smooth (resp. C0) symmetric 3-form T . Then there exists a
smooth immersion f : Mm →RN1(m) with N1(m) = 3(n +
n+1
2

+
n+2
3

) (resp. a
C1-immersion f with N1(m) = (m + 1)(m + 2)/2 + m) such that f ∗(T0) = T .
Proof of Proposition 4.6 First we choose an immersion f1 : (Mm,g,T ) →
(RN1(m),g0,T0) such that
f ∗
1 (T0) = T.
(4.139)
The existence of f1 follows from the Gromov immersion theorem.
Then we choose a positive (large) number A such that
g −A−1 · f ∗
1 (g0) = g1
(4.140)
is a Riemannian metric on M, i.e., g1 is a positive symmetric bilinear form. Such a
number A exists, since M is compact.
Next we choose an isometric immersion
f2 :

Mm,g1

→

RN(m),g0

.
The existence of f2 follows from the Nash isometric immersion theorem.

228
4
The Intrinsic Geometry of Statistical Models
Lemma 4.7
For all N there is a linear isometric embedding LN : (RN,g0) →
(R2N,g0) such that L∗
N(T0) = 0.
Proof For x = (x1,...,xN) ∈RN we set
LN(x1,...,xN) :=

f 1(x1),...,f N(xN)

where f i embeds the statistical line (R(xi),(dxi)2,0) into the statistical plane
(R2(x2i−1,x2i),(dx2i−1)2 + (dx2i)2,(dx2i−1)3 + (dx2i)3) as follows:
f i(xi) := 1
√
2
(x2i−1 −x2i).
Since fi is an isometric embedding of (R(xi),(dxi)2) into (R2(x2i−1,x2i),
(dx2i−1)2 + (dx2i)2), LN is an isometric embedding of (RN,g0) into (R2N,g0).
Set T 2i
0 := (dx2i−1)3 + (dx2i)3. Clearly, (f i)∗T 2i
0
= 0. Since T0 = 	N
i=1 T 2i
0 , it
follows that L∗
N(T0) = 0. This completes the proof of Lemma 4.7.
□
Completion of the proof of Proposition 4.6 We choose an immersion
f3 : Mm →RN1(m) ⊕R2N(m)
as follows
f3(x) := A−1 · f1(x) ⊕(LN(m) ◦f2)(x).
Using (4.140) and the isometry property of f2, we obtain
(f3)∗(g0) = A−1 · f ∗
1 (g0|RN1(m)) + f ∗
2 (g0|R2N(m)) = (g −g1) + g1 = g,
which implies that f3 is an isometric embedding. Using (4.139) and Lemma 4.7, we
obtain
(f3)∗(A · T0) = A−1 · f ∗
1 (A · T0|RN1(m)) = f ∗
1 (T0) = T.
This implies that the immersion f3 satisﬁes the condition of Proposition 4.6.
□
4.5.4 Proof of the Existence of Isostatistical Immersions
Proposition 4.6 plays an important role in the proof of Theorem 4.10. Using it, we
deduce Theorem 4.10 from the following
Proposition 4.7 For any linear statistical manifold (Rn,g0,A · T0) there exists an
isostatistical immersion of (Rn,g0,A · T0) into (P+([4n]),g,T).

4.5
Statistical Manifolds and Statistical Models
229
Proof We shall choose a very large positive number
¯A = ¯A(n,A),
(4.141)
which will be speciﬁed later in the proof of Lemma 4.8. First, ¯A in (4.141) is re-
quired to be so large that there exists a number 1 < λ = λ( ¯A) < 2 satisfying the
following equation:
λ2 +
3n
(2 ¯A)2 = 4.
(4.142)
Equation (4.142) implies that (λ,(2 ¯A)−1,(2 ¯A)−1,(2 ¯A)−1) ∈R4 is a point in the
positive sector S3
2/√n,+.
Hence there exists a positive number r( ¯A) such that for all 0 < r ≤r( ¯A) the ball
U( ¯A,r) of radius r in the sphere S3
2/√n centered at the point

λ,(2 ¯A)−1,(2 ¯A)−1,(2 ¯A)−1
also belongs to the positive sector S3
2/√n,+. For such r the Cartesian product
×n times U( ¯A,r) is a subset in S4n−1
2,+
⊆R4n. This geometric observation helps
us to reduce the proof of Proposition 4.7 to the proof of the following simpler state-
ment.
Lemma 4.8
For given positive number A > 0 there exist a positive number ¯A,
satisfying (4.142) and depending only on n and A, a positive number r < r( ¯A) and
an isostatistical immersion h from (Rn,g0,A · T0) into (P+([4n]),g,T) such that
h(Rn,g0,A · T0) ⊆×n times U( ¯A,r).
Proof Since
(U( ¯A,r),g0|U( ¯A,r),T ∗|U( ¯A,r))
is
a
statistical
submanifold
of
(R4
+,g0,T ∗), the Cartesian product

×
n times
U( ¯A,r),
n
:
i=1
(g0)|U( ¯A,r),
n
:
i=1
T ∗
U( ¯A,r)

is a statistical submanifold of the statistical manifold (R4n
+ ,g0,T ∗). Taking into
account Example 4.1.2, we conclude that

×
n times
U( ¯A,r),
n
:
i=1
(g0)|U( ¯A,r),
n
:
i=1
T ∗
U( ¯A,r)

is a statistical submanifold of (P+([4n]),g,T). Hence, to prove Lemma 4.8, it suf-
ﬁces to show that there are positive numbers ¯A = ¯A(n,A), r < r( ¯A) and an isostatis-
tical immersion f : ([0,R],dx2,A · dx3) →(U( ¯A,r),(g0)|U( ¯A,r),T ∗|U( ¯A,r)). On

230
4
The Intrinsic Geometry of Statistical Models
U( ¯A,r), for any given ρ > 0 we consider the distribution D(ρ) ⊆T U( ¯A,r) deﬁned
by
Dx(ρ) :=

v ∈TxU( ¯A,r) : |v|g0 = 1,T ∗(v,v,v) = ρ

.
Clearly, the existence of an isostatistical immersion f : (R,dx2,A · dx3) →
(U( ¯A,r),(g0)|U( ¯A,r),T ∗|U(A,r)) is equivalent to the existence of an integral curve
of the distribution D(A) on U( ¯A,r). Intuitively, ¯A should be as large as possible
to ensure that the monotone invariant M1(U) is as large as possible for a small
neighborhood U ∋x in (P+([4n]),g,T), see Corollary 4.6 below.
We shall search for the required integral curve using the following geometric
lemma.
Lemma 4.9
There exist a positive number ¯A = ¯A(n,A) and an embedded torus
T 2 in U( ¯A,r) which is provided with a unit vector ﬁeld V on T 2 such that
T ∗(V,V,V ) = A.
Proof of Lemma 4.9 Set
x0 = x0( ¯A) :=

λ,(2 ¯A)−1,(2 ¯A)−1,(2 ¯A)−1
∈S3
2/√n,+,
where λ = λ( ¯A) is deﬁned by (4.142). The following lemma is a key step in the
proof of Lemma 4.9.
Lemma 4.10 There exists a positive number ¯A = ¯A(n,A) such that the following
assertion holds. Let H be any 2-dimensional subspace in Tx0U( ¯A,r) ⊆R4. Then
there exists a unit vector w ∈H such that T ∗(w,w,w) ≥
√
2A.
Proof of Lemma 4.10 Denote by ⃗x0 the vector in R4 with the same coordinates as
those of the point x0. For any given H as in Lemma 4.10 there exists a unit vector
⃗h in R4, which is not co-linear with ⃗x0 and which is orthogonal to H, such that a
vector w ∈R4 belongs to H if and only if w is a solution to the following two linear
equations:
⟨w, ⃗x0⟩= 0,
(4.143)
⟨w, ⃗h⟩= 0.
(4.144)
Adding a multiple of ⃗x0 to ⃗h if necessary, and taking the normalization, we can
assume that
⃗h = (0 = h1,h2,h3,h4)
and

i
h2
i = 1.
Case 1. Suppose that not all the coordinates hi of ⃗h are of the same sign. Since
the statistical manifold (Rn,g0,T ∗) as well as the positive sector S3
2/√n,+ are invari-
ant under the permutation of coordinates (x2,x3,x4), observing that the last three

4.5
Statistical Manifolds and Statistical Models
231
coordinates of x0 are equal, w.l.o.g. we assume that h2 ≤0,h3 > 0. We put
k2 :=
−h2

(h2)2 + (h3)2 ,
k3 :=
h3

(h2)2 + (h3)2 .
We shall search for the required vector w for Lemma 4.10 in the following form:
w :=

w1,w2 = (1 −ε2)k3,w3 = (1 −ε2)k2,0 = w4

∈R4.
(4.145)
Recall that w must satisfy (4.144) and (4.143). We observe that for any choice
of w1 and ε2 Eq. (4.144) for w is satisﬁed. Now we need to ﬁnd the parameters
(w1,ε2) of w in (4.145) such that w satisﬁes (4.143). For this purpose we choose
(w1,ε2) to be a solution of the following system of equations
λ · w1 + (1 −ε2) · (2 ¯A)−1 · (k2 + k3) = 0,
(4.146)
w2
1 =

2ε2 −ε2
2

.
(4.147)
Note that (4.146) is equivalent to (4.143), and (4.147) normalizes w so that |w|2 = 1.
From (4.146) we express w1 in terms of ε2 as follows:
w1 = −(1 −ε2)(k2 + k3)
λ · 2 ¯A
.
(4.148)
Substituting the value of w1 from (4.148) into (4.147), we get the following equation
for ε2:
(k2 + k3)2
(λ · 2 ¯A)2 + 1

ε2
2 −

2 + 2(k2 + k3)2
(λ · 2 ¯A)2

ε2 +
k2 + k3
λ · 2 ¯A
2
= 0,
which we simplify as follows:
ε2
2 −2ε2 +
(k2 + k3)2
(k2 + k3)2 + 4λ2 ¯A2 = 0.
(4.149)
Clearly, the following choice of ε2 is a solution to (4.149):
ε2 = 1 −
2λ ¯A

(k2 + k3)2 + 4λ2 ¯A2 .
(4.150)
By our assumption on h2 and h3, we have 0 ≤k2,k3 ≤1. Since 1 < λ < 2 by
(4.142), we conclude that when ¯A goes to inﬁnity, the value ε2 goes to zero. Hence
there exists a number N1 > 0 such that if ¯A > N1 then
ε2 > 0
and
(1 −ε2)2 ≥3
4.
(4.151)

232
4
The Intrinsic Geometry of Statistical Models
We shall show that for ε2 in (4.150) that also satisﬁes (4.151) if ¯A is sufﬁciently
large, and for w1 deﬁned by (4.148), the vector w deﬁned by (4.145) satisﬁes the re-
quired condition of Lemma 4.10. Since x0 = (λ,(2 ¯A)−1,(2 ¯A)−1,(2 ¯A)−1) we have
T ∗
x0(w,w,w) = 2w3
1
λ
+ (4 ¯A)

w3
2 + w3
3

.
(4.152)
Now assume that ¯A > N1. Noting that ε2 is positive and close to zero, and using
k2 ≥0, k3 ≥0, we obtain from (4.145)
w2 ≥0, w3 ≥0.
(4.153)
Since 0 < ε < 1, 0 < k2 + k3 < 2, and λ, ¯A are positive, we obtain from (4.148)
w1 < 0
and
|w1| < 1
λ ¯A.
(4.154)
Taking into account (4.145) and (4.151), we obtain
w2
2 + w2
3 = (1 −ε2)2 ≥3
4.
(4.155)
Using (4.154), we obtain from (4.152)
T ∗
x0(w,w,w) ≥−2
λ4 ¯A3 + (4 ¯A) ·

w3
2 + w3
3

.
(4.156)
Observing that the function x3/2 + (c −x)3/2 is convex on the interval [0,c] for any
c > 0, and therefore (w3
2 + w3
3) reaches the minimum under the constraints (4.153)
and (4.155) at w2 = w3 =
√
3/
√
2, we obtain from (4.156)
T ∗
x0(w,w,w) ≥−2
λ4 ¯A3 + (4 ¯A) · 2
√
3
√
2
3
= −2
λ4 ¯A3 + 8
=
3
2
3
¯A.
(4.157)
Increasing ¯A if necessary, noting that 1 < λ = λ(A), Eq. (4.157) implies that there
exists a large positive number ¯A(n,A) depending only on n and A such that any
subspace H deﬁned by Eqs. (4.143) and (4.144), where h is in Case 1, contains a
unit vector w that satisﬁes the condition in Lemma 4.10, i.e., the RHS of (4.157) is
larger than
√
2A.
Case 2. Without loss of generality we assume that h2 ≥h3 ≥h4 > 0 and there-
fore we have
α := h2 + h3
h4
≥2.
(4.158)
We shall search for the required vector w for Lemma 4.10 in the following form:
w :=

w1,w2 = −(1 −ε2),w3 = −(1 −ε2),w4 = α(1 −ε2)

.
(4.159)

4.5
Statistical Manifolds and Statistical Models
233
Equations (4.159) and (4.158) ensure that ⟨w, ⃗h⟩= 0 for any choice of param-
eters (w1,ε2) of w in (4.159). Next we require that the parameters (w1,ε2) of w
satisfy the following two equations:
λ · w1 + (1 −ε2)(α −2)
2 ¯A
= 0,
(4.160)
w2
1 + (1 −ε2)2
2 + α2
= 1.
(4.161)
Note that (4.160) is equivalent to (4.143) and (4.161) normalizes w. From (4.160)
we express w1 in terms of ε2 as follows:
w1 = −(1 −ε2)(α −2)
λ2 ¯A
.
(4.162)
Set
B :=

2 + α2
+ (α −2)2
4λ2 ¯A2 .
(4.163)
Plugging (4.162) into (4.161) and using (4.163), we obtain the following equation
for ε2:
(1 −ε2)2B −1 = 0,
which is equivalent to the following equation:
(1 −ε2)2 = 1
B .
(4.164)
Since α ≥2 by (4.158), from (4.163) we have B > 0. Clearly,
ε2 := 1 −
1
√
B
(4.165)
is a solution to (4.164).
Since α ≥2 and ε2 ≤1 by (4.165), we obtain from (4.162) that w1 ≤0. Tak-
ing into account 1 < λ, ¯A > 0, we derive from (4.162) and (4.165) the following
estimates:
T ∗
x0(w,w,w) = 2w3
1
λ
+ (4 ¯A)(1 −ε2)3
α3 −2

> 2w3
1 + (4 ¯A)

α3 −2

(1 −ε2)3
= −(α −2)3
4 ¯A3(
√
B)3 + 4 ¯A(α3 −2)
(
√
B)3
≥−
α3 −2
4 ¯A3(
√
B)3 + 4 ¯A(α3 −2)
(
√
B)3 (since α ≥2)
= α3 −2
(
√
B)3

−1
4 ¯A3 + 4 ¯A

.
(4.166)

234
4
The Intrinsic Geometry of Statistical Models
Lemma 4.11 There exists a large number ¯A = ¯A(n,A) depending only on n such
that for all choices of α ≥2 we have
(α3 −2)
(
√
B)3 ≥1
102 .
Proof To prove Lemma 4.11, it sufﬁces to show that for α ≥2 we have
104
α3 −2
2 ≥B3.
(4.167)
Clearly, there exists a positive number N2 such that if ¯A > N2, then by (4.163), we
have
B < 3
2

2 + α2
(4.168)
for any α ≥2. Hence (4.167) is a consequence of the following relation:
104
α3 −2
2 ≥
13
2

2 + α223
,
(4.169)
which we shall establish now. To prove (4.169), it sufﬁces to show that
103
α3 −2
2 ≥

2 + α23.
(4.170)
The inequality (4.170) is equivalent to the following:
999α6 −6α4 −4000α3 −12α2 + 3992 ≥0.
(4.171)
Since α ≥2, it follows that α3 ≥8 and hence
999α6 −4000α3 = 499α6 + 500α3
α3 −8

≥499α6.
(4.172)
Using 2α6 ≥6α4, we obtain
499α6 −6α4 ≥497α6.
(4.173)
Using a4 ≥16, we obtain
497α6 −12α2 = 496α6 + α2
α4 −12

> 496α6 > 496α6.
(4.174)
From (4.172), (4.173) and (4.174), we obtain
999α6 −6α4 −4000α3 −12α2 + 3992 ≥496α6 + 3992 > 0.
(4.175)
This proves (4.170) and hence completes the proof of Lemma 4.11.
□

4.5
Statistical Manifolds and Statistical Models
235
Lemma 4.11 implies that when ¯A = ¯A(A,n) is sufﬁciently large, the RHS of
(4.166) is larger than
√
2A. This proves the existence of ¯A, which depends only on
n and A, for Case 2.
This completes the proof of Lemma 4.10.
□
From Lemma 4.10 we immediately obtain the following.
Corollary 4.6
There exists a small neighborhood U1 ∋x0 in ¯U( ¯A,r) such that
the following statement holds. For any x ∈U1 and any two-dimensional subspace
H ⊆TxU1, we have
max

T ∗(v,v,v)
 v ∈H and |v|g0 = 1

≥5
4A.
Completion of the proof of Lemma 4.9 Let ¯A = ¯A(n,A) satisfy the condition of
Lemma 4.10. Now we choose a small embedded torus T 2 in U1 ⊆U( ¯A,r). By
Corollary 4.6, for all x ∈T 2 we have
max

T ∗(v,v,v)
 v ∈TxT 2 and |v|g0 = 1

≥5
4A.
(4.176)
Denote by T1T 2 the bundle of the unit tangent vectors of T 2. Since T 2 = R2/Z2
is parallelizable, we have T1T 2 = T 2 × S1. Thus the existence of a vector ﬁeld
V required in Lemma 4.9 is equivalent to the existence of a function T 2 →S1
satisfying the condition of Lemma 4.9. Next we claim that there exists a unit vector
ﬁeld W on T 2 such that T ∗(W,W,W) = 0. First we choose some orientation for T 2,
that induces an orientation on T1T 2 and hence on the circle S1. Take an arbitrary
unit vector ﬁeld W ′ on T 2, equivalently we pick a function W ′ : T 2 →S1. Now
we consider the ﬁber bundle F over T 2 whose ﬁber over x ∈T 2 consists of the
interval [W ′,−W ′] deﬁned by the chosen orientation on the circle of unit vectors
in TxS2. Since T ∗(W ′,W ′,W ′) = −T ∗(W,W,W), for each x ∈T 2 there exists a
value W on F(x) such that T ∗(W,W,W) = 0 and W is closest to W ′. Using W we
identify the circle S1 with the interval [0,1). The existence of W implies that the
existence of a function V : T 2 →[0,1), regarded as a unit vector ﬁeld V on T 2,
that satisﬁes the condition of Lemma 4.9 is equivalent to the existence of a function
f : T 2 →[0,1) satisfying the same condition. Now let V (x) be the smallest value
of unit vector V (x) ∈[0,1) ⊆S1(TxT 2) such that
T ∗
V (x),V (x),V (x)

= A
for each x ∈T 2. The existence of V (x) follows from (4.176). This completes the
proof of Lemma 4.9.
□
As we have noted, Lemma 4.9 implies Lemma 4.8.
□
This ﬁnishes the proof of Proposition 4.7.
□

236
4
The Intrinsic Geometry of Statistical Models
Proof of Theorem 4.10 Case I. M is a compact manifold. In this case, the ex-
istence of an isostatistical immersion of a statistical manifold (M,g,T ) into
(P+([N]),g,T) for some ﬁnite N follows from Proposition 4.6 and Proposition 4.7.
Case II. M is a non-compact manifold. We shall reduce the existence of an im-
mersion of (Mm,g,T ) into P+(N) satisfying the condition of Theorem 4.10 to
Case I, using a partition of unity and Nash’s trick. (The Nash trick is a bit more
complicated and can be used to embed a non-compact Riemannian manifold into
a ﬁnite-dimensional Euclidean manifold.) Since (Mm,g,T ) is ﬁnite-dimensional,
there exists a countable locally ﬁnite open bounded cover Ui, i = 1,∞, of Mm.
We can then ﬁnd compact submanifolds with boundary Ai ⊆Ui whose union also
covers Mm.
Let {vi} be a partition of unity subjected to the cover {Ui} such that vi is strictly
positive on Ai. Let Si be a sphere of dimension m.
The following lemma is based on a trick that is similar to Nash’s trick in [196,
part D, pp. 61–62].
Lemma 4.12 For each i there exists a smooth map φi : Ai →Si with the following
properties:
(i) φi can be extended smoothly to the whole Mm.
(ii) For each Si there exists a statistical structure (gi,Ti) on Si such that
g =

i

φi∗(gi),
(4.177)
T =

i

φi∗(Ti).
(4.178)
Proof Let φi map the boundary of Ui into the north point of the sphere Si. Further-
more, we can assume that this map φi is injective in Ai. Clearly, φi can be extended
smoothly to the whole Mn. This proves assertion (i) of Lemma 4.12.
(ii) The existence of a Riemannian metric gi on Si that satisﬁes (4.177)
g =

i

φi∗(gi)
has been proved in [196, part D, pp. 61–62]. For the reader’s convenience, and for
the proof of the last assertion of Lemma 4.12, we shall repeat Nash’s proof.
Let γi be a Riemannian metric on Si. Set
g0 =

i

φi∗(γi),
(4.179)
where by Lemma 4.12 φi is a smooth map from Mm to Si. This is a well-deﬁned
metric, since the covering {Ui} is locally ﬁnite. By rescaling the metric γi, we can
assume that g −g0 is a positive metric. Now we set
gi :=

φi∗(γi) + vi · (g −g0).
(4.180)

4.5
Statistical Manifolds and Statistical Models
237
We claim that there is a Riemannian metric ˜γi on Si such that

φi∗( ˜γi) = gi.
(4.181)
Note gi −(φi)∗(γi) has a support on Ui since supp(vi) ⊆Ui. Since φi is injective
in Ai, ((φi)−1)∗(gi −(φi)∗(γi)) is a non-negative quadratic form on Si. Hence
˜γi :=

φi−1∗
gi −

φi∗(γi)

+ γi
is a Riemannian metric on Si that satisﬁes (4.181).
Now we compute

i

φi∗( ˜γi) =

i
gi
(by (4.181))
=

i

φi∗(γi) + vi · (g −g0)
(by (4.180))
= g0 + (g −g0) = g
(by (4.179)).
This proves (4.177).
The proof of the existence of Ti that satisﬁes (4.178) follows the same scheme,
as for the proof of the existence of gi; it is even easier, since we do not have the
issue of positivity of Ti. So we leave it to the reader as an exercise.
□
Continuation of the proof of Theorem 4.10 Let a1,...,a∞be a sequence of positive
numbers with
∞

i=1
a2
i = 4.
By the proof of Theorem 4.10, there exist a large number l(m) depending only on
m and an isostatistical immersion
ψi : (Si,gi,Ti) →

S4l(m)−1
ai,+
,g0,T ∗
for any i ∈N. Here the sphere S4l(m)−1
ai
has radius smaller than 2, so we have to
adjust the number ¯A in the proof of Case 1. The main point is that the value λ = λ( ¯A)
deﬁned by a modiﬁed Eq. (4.142), where the RHS is replaced by a2
i , is bounded from
below and from above by a number that depends only on l(m) and the radius ai.
Thus the RHS of (4.157) goes to inﬁnity, when ¯A goes to inﬁnity. Similarly, the
RHS of (4.166) goes to inﬁnity when ¯A goes to inﬁnity.
Set
I i := ψi ◦φi : Mm →

S4l(m)−1
ai,+
,g0,T ∗
⊆

R4l(m),g0,T ∗
.
Clearly, the map I := (I 1 ×···×I ∞) maps Mm into the Cartesian product of the
positive sectors S4l(m)−1
ai,+
, that is, a subset of the positive sectors S∞
√
2,+ of all positive

238
4
The Intrinsic Geometry of Statistical Models
probability measures on N. Since ψi are isostatistical immersions, by (4.177) and
(4.178) the map I satisﬁes the required condition of Theorem 4.10.
□
4.5.5 Existence of Statistical Embeddings
Theorem 4.11 Any smooth (resp. C0) compact statistical manifold (Mn,g,T ) ad-
mits an isostatistical embedding into the statistical manifold (P+([N]),g,T) for
some ﬁnite number N. Any smooth (resp. C0) non-compact statistical manifold
(Mn,g,T ) admits an embedding I into the space P+(N) of all probability measures
on N such that g and T coincide with the Fisher metric and the Amari–Chentsov
tensor on I(Mn), respectively.
Proof To prove Theorem 4.11, we repeat the proof of Theorem 4.10, replacing the
Nash immersion theorem by the Nash embedding theorem. First we observe that
our immersion f3 constructed in Sect. 4.5.3 is an embedding, if f2 is an isometric
embedding. The existence of an isometric embedding f2 is ensured by the Nash the-
orem. Hence, if Mn is compact, to prove the existence of an isostatistical embedding
of (Mn,g,T ) into (P+([N]),g,T), it sufﬁces to prove the strengthened version of
Proposition 4.7, where the existence of an isostatistical immersion is replaced by
the existence of an isostatistical embedding, but we need only to embed a bounded
domain D in a statistical manifold (Rn,g0,A · T0) into (P+([N]),g,T).
As in the proof of Proposition 4.7, the proof of the new strengthened version of
Proposition 4.7 is reduced to the proof of the existence of an isostatistical immersion
of a bounded statistical interval ([0,R],dt2,A · dt3) into a torus T 2 of a small
domain in (S7
2/√n,+,g,T ∗) ⊆(R8,g0,T ∗), see the proof of Lemma 4.8.
The statistical immersion produced with the help of Lemma 4.9 will be an
embedding if not all the integral curves of the distribution D(A) on the torus
T 2 are closed curves. Now we shall search for an isostatistical embedding of
([0,R],dt2,A · dt3) into a torus T 2 × T 2 of a small domain in (S3
1/√n,+,g0,T ∗) ×
(S3
1/√n,+,g0,T ∗) ⊆(S7
2/√n,+,g,T ∗) ⊆(R8,g0,T ∗). Since T 4 is parallelizable, re-
peating the argument at the end of the proof of Lemma 4.8, we choose a distribution
D(A) ⊆T T 4 such that D(A) = T 4 × S2 and
DxA =

v ∈TxT 4  |v|g0 = 1, and T ∗(v,v,v) = A

.
Now assume that the integral curves of D(A) that lie on the ﬁrst factor T 2 × y for
all y ∈S3
1/√n,+ are closed. Since T 2 is compact, there is a positive number p1 such
that the periods of these integral curves are at least p1.
Now let us consider the following integral curve γ (t) of D(A) on T 4. The curve
γ (t) begins at a point (0,0,0,0) ∈T 4. Here we identify T 1 with [0,1]/(0 = 1).
The integral curve lies on T 2 × (0,0) until it approaches (0,0,0,0) again. Since
Dx(A) = S2, we can slightly modify the direction of γ (t) and let it leave the torus

4.5
Statistical Manifolds and Statistical Models
239
T 2 × (0,0) and after a very short time γ (t) must stay on the torus T 2 × (ε,ε)
where ε is sufﬁciently small. Without loss of generality we assume that the pe-
riod of any closed curve of the distribution D(A) ∩T (T 2 × (ε,ε)) is at least p1.
Repeating this procedure, since R and p1 are ﬁnite, we produce an embedding of
([0,R],dt2,A · dt3) into T 4 ⊆(S3
1/√n,+,g0,T ∗) × (S3
1/√n,+,g0,T ∗).
This completes the proof of Theorem 4.11 in the case when Mn is compact.
It remains to consider the case where Mn is non-compact. Using the existence
of isostatistical embeddings for the compact case we can assume that ψi is an em-
bedding for all i. Now we shall show that the map I is an embedding. Assume that
x ∈M. By the assumption, there exists an Ai such that x is an interior point of Ai.
Then for any y ∈M, I i(x) ̸= I i(y), since ψi is an embedding, φi is injective in the
interior of Ai and maps the boundary of Ai to the north pole of Si.
□
Remark 4.8
There are many open questions concerning immersions of Ck-
statistical manifolds. One important problem is to ﬁnd a class of statistical manifolds
(M,g,T) of exponential type (i.e., M are exponential families as in (3.31)) that ad-
mit an isostatistical embedding into linear statistical manifolds (Rn,g0,A · T0) or
into statistical manifolds (P+([N]),g,T). This is a difﬁcult problem, if dimM ≥2.
A version of this problem is to ﬁnd an explicit embedding of the Riemannian man-
ifold (M,g) underlying a statistical manifold (M,g,T) of exponential type into
Euclidean spaces. The problem of embedding of hyperbolic Riemannian spaces
into Euclidean spaces has been considered by many geometers. We refer the reader
to [52] for a survey.

Chapter 5
Information Geometry and Statistics
5.1 Congruent Embeddings and Sufﬁcient Statistics
In this section, we shall consider statistics and Markov kernels between measure
spaces Ω and Ω′, partially following [25, 26]. For a historical overview of the no-
tions introduced here, see also Remark 5.9 below. These establish a standard method
to obtain new statistical models out of given ones. Such transitions can be inter-
preted as data processing in statistical decision theory, which can be deterministic
(i.e., given by a measurable map, a statistic) or noisy (i.e., given by a Markov ker-
nel).
Given measure spaces Ω and Ω′, a statistic is a measurable map κ : Ω →Ω′,
i.e., κ associates to each event ω ∈Ω an event κ(ω) in Ω′.
For instance, if Ω is divided into subsets Ω = A1 ⊎··· ⊎An, then this induces a
map κ from Ω into the set {1,...,n}, associating to each element of Ai the value i,
thus lumping together each set Ai to a single point.
Given such a statistic, we can associate to each probability measure μ on Ω a
probability measure μ′ = κ∗μ on Ω′, given as
κ∗μ(A) := μ

ω ∈Ω : κ(ω) ∈A

= μ

κ−1A

.
In the above example, μ′(i) would equal μ(Ai). In this way, for a statistical model
(M,Ω,p), a statistic induces another statistical model (M,Ω′,p′) given as
p′(ξ) = κ∗p(ξ),
which we call the model induced by κ. In general, we cannot expect the induced
statistical model to contain the same amount of information, as one can see already
from the above example: Knowing p′(ξ) only allows us to recover the probability
of Ai w.r.t. p(ξ), but we no longer know the probability of subsets of Ai w.r.t. p(ξ).
Apart from statistics, we may also consider Markov kernels between measurable
spaces Ω and Ω′. A Markov kernel K associates to each ω ∈Ω a probability mea-
sure K(ω) on Ω′, and we write K(ω;A′) for the probability of A′ w.r.t. K(ω) for
© Springer International Publishing AG 2017
N. Ay et al., Information Geometry, Ergebnisse der Mathematik und ihrer Grenzgebiete.
3. Folge / A Series of Modern Surveys in Mathematics 64,
DOI 10.1007/978-3-319-56478-4_5
241

242
5
Information Geometry and Statistics
A′ ⊆Ω′. Again, this induces for each probability measure μ on Ω a probability
measure on Ω′, deﬁned as
K∗μ

A′
:=

Ω
K∗

ω;A′
dμ(ω).
The map K∗associating to a measure on Ω a measure on Ω′ is called the Markov
morphism induced by K.
The concept of Markov kernels is more general than that of statistics. Indeed,
each statistic induces a Markov kernel by setting
K(ω) := δκ(ω),
which denotes the Dirac measure supported on κ(ω). Just as in the case of a
statistic κ, a Markov kernel K induces for a statistical model (M,Ω,p) a model
(M,Ω′,p′) by
p′(ξ) := K∗p(ξ).
A particular example of a Markov kernel is given by random walks in a set Ω.
Here, to each ω ∈Ω one associates a probability measure K(ω) on Ω which gives
the probability distribution of the position of a particle located at ω after the next
move.
Again, it is evident that in general there will be some loss of information when
passing from the statistical model (M,Ω,p) to (M,Ω′,p′).
Given a statistic or, more general, a Markov kernel between Ω and Ω′, it is
now interesting to characterize statistical models (M,Ω,p) for which there is no
information loss when replacing it by the model (M,Ω′,p′).
In the case of statistics, it is plausible that this is the case if there is a density
function such that p(ξ) = p(ω;ξ)μ0 which is constant on the ﬁbers of κ, i.e.,
p(ξ) = p(·;ξ)μ0 = p′
κ(·);ξ

μ0
(5.1)
for some function p′ deﬁned for pairs (ω′;ξ) with ξ ∈M and ω′ ∈Ω′. In this case,
p′(ξ) = p′(·;ξ)μ′
0
for the induced measure μ′
0 := κ∗μ0. Thus, all information of p can be recovered
from p′ which is typically a simpler function, as it is deﬁned on the (presumably
smaller) space Ω′.
Therefore, if the model (M,Ω,μ0,p) satisﬁes (5.1), then we call κ a Fisher–
Neyman sufﬁcient statistic for the model, cf. Deﬁnition 5.8.
If the model (M,Ω,p) has a regular density function, then we also show some
equivalent characterizations of the Fisher–Neyman sufﬁciency of a statistic κ for
this model, the most important of which is the Fisher–Neyman characterization; cf.
Theorem 5.3.
The notion of Fisher–Neyman sufﬁcient statistics is closely related to that of κ-
congruent embeddings, where κ is a statistic between Ω and Ω′. Roughly speaking,

5.1
Congruent Embeddings and Sufﬁcient Statistics
243
given a ﬁnite measure μ0 on Ω and its push-forward μ′
0 := κ∗μ0, measures on Ω′
dominated by μ′
0 are mapped to measures on Ω via
Kμ0 : f ′(·)μ′
0 −→f ′
κ(·)

μ0,
(5.2)
where f ′ ∈L1(Ω′,μ′
0). The image of this map is called a κ-congruent subspace.
Note that Kμ0 maps probability measures on Ω′ to probability measures on Ω, and
the composition
f ′(·)μ′
0
Kμ0
−−→f ′
κ(·)

μ0
κ∗
−→f ′(·)μ′
0
(5.3)
is the identity.
Comparing (5.2) with (5.1), it is then evident that κ is a Fisher–Neyman suf-
ﬁcient statistic for the model (M,Ω,p) if and only if p(ξ) is contained in some
κ-congruent subspace.
We shall deﬁne a congruent embedding as a bounded linear map K∗which is
a right inverse of κ∗, i.e., κ∗K∗μ′ = μ′ for all measures μ′ in the domain of K∗,
and such that K∗is monotone, i.e., maps non-negative measures to non-negative
measures; cf. Deﬁnition 5.1. In this case, one can show that K∗is of the form Kμ0
from (5.2) for a suitable measure μ0 on Ω; cf. Proposition 5.2.
If the map K∗is induced by a Markov kernel K, then we call this Markov kernel
congruent. However, in contrast to the situation of a ﬁnite Ω, the map K∗need not
be induced by a Markov kernel in general. In fact, we show that this is the case if and
only if the measure μ0 on Ω admits transverse measures w.r.t. κ; cf. Theorem 5.1.
Such a transverse measure exists for ﬁnite measure spaces, explaining why in the
ﬁnite case every congruent embedding is induced by a Markov kernel.
Furthermore, if (M,Ω,p) is a statistical model with an inﬁnite Ω and (M,Ω′,p′)
is the statistical model induced by a Markov kernel (or a statistic) between Ω
and Ω′, then the question of k-integrability becomes an important issue since, unlike
in the case of a ﬁnite Ω, not every statistical model is k-integrable. As one of our
main results we show that Markov kernels preserve k-integrability, i.e., if (M,Ω,p)
is k-integrable, then so is (M,Ω′,p′); cf. Theorem 5.4.
In fact, in Theorem 5.4 we also show that ∥∂V logp′(ξ)∥k ≤∥∂V logp(ξ)∥k for
all V ∈TξM, as long as the models are k-integrable, and that equality holds either
for all k > 1 for which k-integrability holds, or for no such k > 1. This result allows
us to deﬁne the kth order information loss of a Markov kernel K as the difference
∂V logp′(ξ)
k
k −
∂V logp(ξ)
k
k ≥0.
In particular, if k = 2n is an even integer, then by the deﬁnition (3.95) of the canon-
ical 2n-tensors τ 2n
M of (M,Ω,p) and τ ′2n
M of (M,Ω′,p′) we immediately conclude
(cf. Theorem 5.5) that
τ 2n
M (V,...,V ) −τ ′2n
M (V,...,V ) ≥0.

244
5
Information Geometry and Statistics
If k ≥2 then, as τ 2
M and τ ′2
M coincide with the Fisher metrics gM and g′
M, respec-
tively, we obtain the monotonicity formula
gM(V,V ) −g′
M(V,V ) ≥0,
(5.4)
and this difference represents the 2nd order information loss. (The interpretation of
this difference as information loss was established by Amari in [7].)
Note that this result goes well beyond the versions of the monotonicity inequality
known in the literature (cf. [16, 25, 164]) where further assumptions on Ω or the
Markov kernel K are always made.
In the light of the deﬁnition of information loss, we say that a statistic or, more
generally, a Markov kernel is sufﬁcient for a model (M,Ω,p) if the information
loss vanishes for some (and hence for all) k > 1 for which the model is k-integrable.
As it turns out, the Fisher–Neyman sufﬁcient statistics are sufﬁcient in this sense;
in fact, under the assumption that (M,Ω,p) is given by a positive regular density
function, a statistic is sufﬁcient if and only if it is Fisher–Neyman sufﬁcient, as
we show in Proposition 5.6. However, without this positivity assumption there are
sufﬁcient statistics which are not Fisher–Neyman sufﬁcient, cf. Example 5.6.
Furthermore, we also give a classiﬁcation of covariant n-tensors on the space of
(roots of) measures which are invariant under sufﬁcient statistics or, more generally,
under congruent embeddings. This generalizes Theorem 2.3, where this was done
for ﬁnite spaces Ω. In the case of an inﬁnite Ω, some care is needed when deﬁning
these tensors. They cannot be deﬁned on the space of (probability) measures on Ω,
but on the spaces of roots of measure Sr(Ω) from Sect. 3.2.3.
Our result (Theorem 5.6) states that the space of tensors on a statistical model
which are invariant under congruent embeddings (and hence under sufﬁcient statis-
tics) is algebraically generated by the canonical n-tensors. In particular, Corollar-
ies 5.3 and 5.4 generalize the classical theorems of Chentsov and Campbell, char-
acterizing 2- and 3-tensors which are invariant under congruent embeddings, to the
case of arbitrary measure spaces Ω.
5.1.1 Statistics and Congruent Embeddings
Given two measurable spaces (Ω,B) and (Ω′,B′), a measurable map
κ : Ω −→Ω′
will be called a statistic. If μ is a (signed) measure on (Ω,B), it then induces a
(signed) measure κ∗μ on (Ω,B′), via
κ∗μ(A) := μ

x ∈Ω : κ(x) ∈A

= μ

κ−1A

,
which is called the push-forward of μ by κ (cf. 3.17). If μ ∈M(Ω) is a ﬁnite
measure, then so is κ∗μ, and
∥κ∗μ∥T V = κ∗μ

Ω′
= μ

κ−1
Ω′
= μ(Ω) = ∥μ∥T V ,

5.1
Congruent Embeddings and Sufﬁcient Statistics
245
so that
∥κ∗μ∥T V = ∥μ∥T V
for all μ ∈M(Ω).
(5.5)
In particular, κ∗maps probability measures to probability measures, i.e.,
κ∗

P(Ω)

⊆P

Ω′
.
If we consider κ∗as a map of signed measures, then
κ∗: S(Ω) −→S

Ω′
(5.6)
is a bounded linear map. In fact, using the Jordan decomposition μ = μ+ −μ−for
μ ∈S(Ω) with μ± ∈M(Ω) and μ+ ⊥μ−(3.46) then
∥κ∗μ∥T V = ∥κ∗μ+ −κ∗μ−∥T V ≤∥κ∗μ+∥T V + ∥κ∗μ−∥T V
(5.5)
= ∥μ+∥T V + ∥μ−∥T V = ∥μ∥T V ,
whence
∥κ∗μ∥T V ≤∥μ∥T V
with equality iff
κ∗(μ+) ⊥κ∗(μ−).
(5.7)
Furthermore, if μ1 dominates μ2, then κ∗(μ1) dominates κ∗(μ2) by (3.17), whence
for μ ∈M(Ω), κ∗yields a bounded linear map
κ∗: S(Ω,μ) −→S

Ω′,κ∗μ

,
(5.8)
and if we write
κ∗(φμ) = φ′κ∗(μ),
(5.9)
then φ′ ∈L1(Ω′,κ∗μ) is called the conditional expectation of φ ∈L1(Ω,μ)
given κ. This yields a bounded linear map
κμ
∗: L1(Ω,μ) −→L1
Ω′,μ′
,
φ −→φ′
(5.10)
with φ′ from (5.9). We also deﬁne the pull-back of a measurable function φ′ :
Ω′ →R as
κ∗φ′ := φ′ ◦κ.
With this, for subsets A′,B′ ⊆Ω′ and A := κ−1(A′),B := κ−1(B′) ⊆Ω we have
κ∗χA′ = χA and hence,
μ(A ∩B) =

A∩B
dμ =

B
χA dμ =

κ−1(B′)
d

κ∗χA′ μ

=

B′ d

κ∗

κ∗χA′ μ

or
μ(A ∩B) = μ

κ−1
A′ ∩B′
= κ∗μ

A′ ∩B′
=

B′ χA′ d(κ∗μ).

246
5
Information Geometry and Statistics
Thus, κ∗(κ∗χA′μ) = χA′κ∗μ. By linearity, this equation holds when replacing χA′
by a step function on Ω′, whence by the density of step functions in L1(Ω′,μ′) we
obtain for φ′ ∈L1(Ω′,κ∗μ)
κ∗

κ∗φ′ μ

= φ′ κ∗μ
and thus,
κμ
∗

κ∗φ′
= φ′.
(5.11)
That is, the conditional expectation of κ∗φ′ given κ equals φ′. While this was al-
ready stated in case when Ω is a manifold and κ a differentiable map in (3.18), we
have shown here that (5.11) holds without any restriction on κ or Ω.
Example 5.1
(1) Let Ω := A1 ⊎··· ⊎An be a partition of the Ω into measurable sets. To this
partition, we associate the statistic κ : Ω →In := {1,...,n} by setting
κ(ω) := i
if ω ∈Ai.
Thus, this statistic lumps together all events in the class Ai.
Conversely, any statistic κ : Ω →I into a ﬁnite set I yields a partition of Ω
into the disjoint measurable subsets Ai := κ−1(i), i ∈I.
(2) If κ is bijective and κ−1 is also measurable, then for any function f on Ω

Ω
f dμ(x) =

Ω′

κ−1∗f dκ∗μ,
(5.12)
so that κ∗yields a bijection of measures on Ω and on Ω′. We shall show in
Proposition 5.4 that this is essentially the only situation where κ∗is a bijection.
As it turns out, κμ
∗preserves k-integrability, as the following result shows.
Proposition 5.1
Let κ : Ω →Ω′ be a statistic and μ ∈M(Ω), μ′ := κ∗μ ∈
M(Ω′), and let κμ
∗: L1(Ω,μ) →L1(Ω′,μ′) be the map from (5.10). Then the
following hold for φ ∈L1(Ω,μ) and φ′ := κμ
∗φ ∈L1(Ω′,μ′):
(1) If φ ∈Lk(Ω,μ) for 1 ≤k ≤∞, then φ′ ∈Lk(Ω′,μ′), and
φ′
k ≤∥φ∥k.
(5.13)
(2) For 1 < k < ∞, equality in (5.13) holds iff φ = κ∗φ′.
Proof By deﬁnition, ∥φ′∥1 = ∥κ∗(φμ)∥T V and ∥φ∥1 = ∥φμ∥T V , so that (5.7) im-
plies (5.13) for k = 1.
If φ ∈L∞(Ω,μ) then |φμ| ≤∥φ∥∞μ, and by monotonicity of κ∗it follows that
φ′μ′ =
κ∗(φμ)
 ≤κ∗

|φμ|

≤
φ

∞μ′,
whence ∥φ′∥∞≤∥φ∥∞, so that (5.13) holds for k = ∞.

5.1
Congruent Embeddings and Sufﬁcient Statistics
247
For φ′ ∈Lk(Ω′,μ′), we have
κ∗φ′k
k =

Ω
κ∗φ′k dμ =

Ω′ dκ∗

κ∗φ′kμ
 (5.11)
=

Ω′
φ′kdμ′ =
φ′k
k,
which implies that κ∗φ′ ∈Lk(Ω,μ) and
κ∗φ′
k =
φ′
k
for all φ′ ∈Lk
Ω′,μ′
,1 ≤k ≤∞.
(5.14)
Suppose now that φ ∈Lk(Ω,μ) with 1 < k < ∞is such that φ′ ∈Lk(Ω′,μ′),
and assume that φ ≥0 and hence, φ′ ≥0. Then
φ′k
k =

Ω′ φ′k dμ′ (5.9)
=

Ω′ φ′k−1 dκ∗(φμ) =

Ω
κ∗
φ′k−1
φ dμ
(∗)
≤
κ∗
φ′k−1
k/(k−1)∥φ∥k
(∗∗)
=
φ′k−1
k/(k−1)∥φ∥k =
φ′k−1
k
∥φ∥k.
From this, (5.13) follows. Here we used Hölder’s inequality at (∗), and (5.14) ap-
plied to φ′k−1 ∈Lk/(k−1)(Ω′,μ′) at (∗∗). Moreover, equality in Hölder’s inequality
at (∗) holds iff φ = c κ∗φ′ for some c ∈R, and the fact that κ∗(φμ) = φ′μ′ easily
implies that c = 1, i.e., equality in (5.13) occurs iff φ = κ∗φ′.
If we drop the assumption that φ ≥0, we decompose φ = φ+ −φ−into its non-
negative and non-positive part, and let φ′
± ≥0 be such that κ∗(φ±μ) = φ′
±μ′. Al-
though in general, φ′
+ and φ′
−do not have disjoint support, the linearity of κ∗still
implies that φ′ = φ′
+ −φ′
−. Let us assume that φ′
± ∈Lk(Ω′,μ′). Then
φ′
k =
φ′
+ −φ′
−

k ≤
φ′
+

k +
φ′
−

k ≤∥φ+∥k + ∥φ−∥k = ∥φ∥k,
using (5.13) applied to φ± ≥0 in the second estimate. Equality in the second esti-
mate holds iff φ± = κ∗φ′
±, and thus, φ = φ+ −φ−= κ∗(φ′
+ −φ′
−) = κ∗φ′.
Thus, it remains to show that φ′ ∈Lk(Ω′,μ′) whenever φ ∈Lk(Ω,μ). For
this, let φ ∈Lk(Ω,μ), (φn)n∈N be a sequence in L∞(Ω,μ) converging to φ in
Lk(Ω,μ), and let φ′
n := κμ
∗(φn) ∈L∞(Ω′,μ′) ⊆Lk(Ω′,μ′). As (φ′
n −φ′
m)± ∈
L∞(Ω′,μ′) ⊆Lk(Ω′,μ′), (5.13) holds for φn −φm by the previous argument, i.e.,
φ′
n −φ′
m

k ≤∥φn −φm∥k,
which tends to 0 for n,m →∞, as (φ)n is convergent and hence a Cauchy sequence
in Lk(Ω,μ). Thus (φ′)n is also a Cauchy sequence, whence it converges to some
˜φ′ ∈Lk(Ω′,μ′). It follows that φn −κ∗φ′
n converges in Lk(Ω,μ) to φ −κ∗˜φ′, and
as κ∗((φn −κ∗φ′
n)μ) = 0 for all n, we have
0 = κ∗

φ −κ∗˜φ′
μ

= φ′μ′ −˜φ′μ′,
whence φ′ = ˜φ′ ∈Lk(Ω′,μ′).
□

248
5
Information Geometry and Statistics
Remark 5.1 The estimate (5.13) in Proposition 5.1 also follows from [199, Propo-
sition IV.3.1].
Recall that M(Ω) and S(Ω) denote the spaces of all (signed) measures on Ω,
whereas M(Ω,μ) and S(Ω,μ) denotes the subspace of the (signed) measures on
Ω which are dominated by μ.
Deﬁnition 5.1 (Congruent embedding and congruent subspace)
Let κ : Ω →Ω′
be a statistic and μ′ ∈M(Ω′). A κ-congruent embedding is a bounded linear map
K∗: S(Ω′,μ′) →S(Ω) such that
(1) K∗is monotone, i.e., it maps non-negative measures to non-negative measures,
or shortly, K∗(M(Ω′,μ′)) ⊆M(Ω).
(2) K∗is a right inverse of κ∗, i.e., κ∗(K∗(ν′)) = ν′ for all ν′ ∈S(Ω′,μ′).
Moreover, the image of a κ-congruent embedding K∗is called a κ-congruent sub-
space of S(Ω).
Example 5.2
(1) (Congruent embeddings with ﬁnite measure spaces) Consider ﬁnite sets I
and I ′. From Example 5.1 above we know that a statistic κ : I →I ′ is equivalent
to a partition (Ai′)i′∈I ′ of I, setting Ai′ := κ−1(i′). In this case, κ∗δi = δκ(i).
Suppose that K∗: S(I ′) →S(I) is a κ-congruent embedding. Then we de-
ﬁne the coefﬁcients Ki′
i by the equation
K∗δi′ =

i∈I
Ki′
i δi.
The positivity condition implies that Ki′
i ≥0. Thus, the second condition holds
if and only if we have for all i′
0 ∈I ′
δi′
0 = κ∗K∗δi′
0 = κ∗

i∈I
K
i′
0
i δi

=

i∈I
K
i′
0
i δκ(i) =

i′∈I ′
 
i∈Ai′
K
i′
0
i

δi′.
Thus, if i′ ̸= i′
0, then 	
i∈Ai′ K
i′
0
i = 0, which together with the non-negativity
of these coefﬁcients implies that K
i′
0
i
= 0 for all i /∈Ai′
0. Furthermore,
	
i∈Ai′
0
K
i′
0
i = 1, so that K∗δi′
0 = 	
i∈Ai′
0
K
i′
0
i δi is a probability measure on I
supported on Ai′
0.
Thus, for ﬁnite measure spaces, a congruent embedding is the same as a
congruent Markov morphism as deﬁned in (2.31).
(2) Let κ : Ω →Ω′ be a statistic and let μ ∈M(Ω) be a measure with μ′ := κ∗μ ∈
M(Ω′). Then the map
Kμ : S

Ω′,μ′
−→S(Ω,μ) ⊆S(Ω),
φ′μ′ −→κ∗φ′μ
(5.15)

5.1
Congruent Embeddings and Sufﬁcient Statistics
249
for all φ′ ∈L1(Ω′,μ′) is a κ-congruent embedding, since
κ∗Kμ

φ′μ′
= κ∗

κ∗φ′μ
 (5.11)
= φ′κ∗μ = φ′μ′.
Observe that the ﬁrst example is a special case of the second if we use μ :=
	
i∈I,i′∈I ′ Ki′
i δi, so that κ∗μ = 	
i′∈I ′ δi′. In fact, we shall now see that the above
examples exhaust all possibilities of congruent embeddings.
Proposition 5.2 Let κ : Ω1 →Ω2 be a statistic, let K∗: S(Ω2,μ2) →S(Ω1) for
some μ2 ∈M(Ω2) be a κ-congruent embedding, and let μ1 := K∗μ2 ∈M(Ω1).
Then K∗= Kμ1 with the map Kμ1 given in (5.15). In particular, any κ-congruent
subspace of S(Ω1) is of the form
Cκ,μ1 :=

κ∗φ2μ1 : φ2 ∈L1(Ω2,κ∗μ1)

⊆S(Ω1,μ1)
(5.16)
for some μ1 ∈M(Ω1).
Proof We have to show that K∗(φ2μ2) = κ∗φ2μ1 for all φ2 ∈L1(Ω2,μ2). By con-
tinuity, it sufﬁces to show this for step functions, as these are dense in L1(Ω2,μ2),
whence by linearity, we have to show that for all A2 ⊆Ω2, A1 := κ−1(A2) ⊆Ω1
K∗(χA2μ2) = χA1μ1.
(5.17)
Let A1
2 := A2 and A2
2 := Ω2\A2, and let Ai
1 := κ−1(Ai
2). We deﬁne the mea-
sures μi
2 := χAi
2μ2 ∈M(Ω2), and μi
1 := K∗μi
2 ∈M(Ω1). Since μ1
2 + μ2
2 = μ2, it
follows that μ1
1 + μ2
1 = μ1 by the linearity of K∗.
Taking indices mod 2, and using κ∗μi
1 = κ∗K∗μi
2 = μi
2 by the κ-congruency
of K∗, note that
μi
1

Ai+1
1

= μi
1

κ−1
Ai+1
2

= κ∗μi
1

Ai+1
2

= μi
2

Ai+1
2

= 0.
Thus, for any measurable B ⊆Ω we have
μ1
1(B) = μ1
1

B ∩A1
1

since μ1
1

B ∩A2
1

≤μ1
1

A2
1

= 0
= μ1
1

B ∩A1
1

+ μ2
1

B ∩A1
1

since μ2
1

B ∩A1
1

≤μ2
1

A1
1

= 0
= μ1

B ∩A1
1

since μ1 = μ1
1 + μ2
1
= (χA1μ1)(B)
since A1
1 = κ−1
A1
2

= κ−1(A2) = A1.
That is, χA1μ1 = μ1
1 = K∗μ1
2 = K∗(χA1
2μ2) = K∗(χA2μ2), so that (5.17) fol-
lows.
□

250
5
Information Geometry and Statistics
For further reference, we shall also introduce the following notion.
Deﬁnition 5.2 (Transverse measures) Let κ : Ω →Ω′ be a statistic. A measure μ ∈
M(Ω) is said to admit transverse measures if there are measures μ⊥
ω′ on κ−1(ω′)
such that for all φ ∈L1(Ω,μ) we have

Ω
φ dμ =

Ω′

κ−1(ω′)
φ dμ⊥
ω′

dμ′
ω′
,
(5.18)
where μ′ := κ∗μ. In particular, the function
Ω′ −→ˆR,
ω′ −→

κ−1(ω′)
φ dμ⊥
ω′
is measurable for all φ ∈L1(Ω,μ).
Observe that the choice of transverse measures μ⊥
ω′ is not unique, but rather, one
can change these measures for all ω′ in a μ′-null set.
Transverse measures exist under rather mild hypotheses, e.g., if one of Ω,Ω′ is
a ﬁnite set, or if Ω,Ω′ are differentiable manifolds equipped with a Borel measure
μ and κ is a differentiable map.
However, there are statistics and measures which do not admit transverse mea-
sures, as we shall see in Example 5.3 below.
Proposition 5.3
Let κ : Ω →Ω′ be a statistic and μ ∈M(Ω) a measure which
admits transverse measures {μ⊥
ω′ : ω′ ∈Ω′}. Then μ⊥
ω′ is a probability measure for
almost every ω′ ∈Ω′ and hence, we may assume w.l.o.g. that μ⊥
ω′ ∈P(κ−1(ω′)) for
all ω′ ∈Ω′.
Proof Let μ′ := κ∗μ. Deﬁne A′
ε := {ω′ ∈Ω′ : μ⊥
ω′(κ−1(ω′)) ≥1 + ε} for a given
ε > 0. Then for φ := χκ−1(A′ε) the two sides of Eq. (5.18) read

Ω
χκ−1(A′ε) dμ = μ

κ−1
A′
ε

= μ′
A′
ε

,

Ω′

κ−1(ω′)
χκ−1(Aε) dμ⊥
ω′

dμ′
ω′
=

A′ε
μ⊥
ω′

κ−1
ω′
dμ′
ω′
≥(1 + ε)μ′
A′
ε

.
Thus, (5.18) implies
μ′
A′
ε

≥(1 + ε)μ′
A′
ε

,
and hence, μ′(A′
ε) = 0 for all ε > 0. Thus,
μ′
ω′ ∈Ω′ : μ⊥
ω′

κ−1
ω′
> 1

= μ′
 ∞
8
n=1
A′
1/n

≤
∞

n=1
μ′
A′
1/n

= 0,

5.1
Congruent Embeddings and Sufﬁcient Statistics
251
whence {ω′ ∈Ω′ : μ⊥
ω′(κ−1(ω′)) > 1} is a μ′-null set. Analogously, {ω′ ∈Ω′ :
μ⊥
ω′(κ−1(ω′)) < 1} is a μ′-null set, that is, μ⊥
ω′ ∈P(κ−1(ω′)) and hence ∥μ⊥
ω′∥T V =1
for μ′-a.e. μ′ ∈Ω′.
Thus, if we replace μ⊥
ω′ by ˜μ⊥
ω′ := ∥μ⊥
ω′∥−1
T V μ⊥
ω′, then ˜μ⊥
ω′ ∈P(κ−1(ω′)) for all
ω′ ∈Ω′, and since ˜μ⊥
ω′ = μ⊥
ω′ for μ′-a.e. ω′ ∈Ω′, it follows that (5.18) holds when
replacing μ⊥
ω′ by ˜μ⊥
ω′.
□
The following example shows that transverse measures do not always exist.
Example 5.3 Let Ω := S1 be the unit circle, regarded as a subgroup of the complex
plane, with the 1-dimensional Borel algebra B. Let Γ := exp(2π
√
−1Q) ⊆S1 be
the subgroup of rational rotations, and let Ω′ := S1/Γ be the quotient space with the
canonical projection κ : Ω →Ω′. Let B′ := {A′ ⊆Ω′ : κ−1(A′) ∈B}, so that κ :
Ω →Ω′ is measurable. For γ ∈Γ , we let mγ : S1 →S1 denote the multiplication
by γ .
Let λ be the 1-dimensional Lebesgue measure on Ω and λ′ := κ∗λ be the induced
measure on Ω′. Suppose that λ admits κ-transverse measures (λ⊥
ω′)ω′∈Ω′. Then for
each A ∈B we have
λ(A) =

Ω′

A∩κ−1(ω′)
dλ⊥
ω′

dλ′
ω′
.
(5.19)
Since λ is invariant under rotations, we have on the other hand for γ ∈Γ
λ(A) = λ

m−1
γ A

=

Ω′

(m−1
γ A)∩κ−1(ω′)
dλ⊥
ω′

dλ′
ω′
=

Ω′

A∩κ−1(ω′)
d

(mγ )∗λ⊥
ω′

dλ′
ω′
.
(5.20)
Comparing (5.19) and (5.20) implies that ((mγ )∗λ⊥
ω′)ω′∈Ω′ is another family of κ-
transverse measures of λ which implies that (mγ )∗λ⊥
ω′ = λ⊥
ω′ for λ′-a.e. ω′ ∈Ω′,
and as Γ is countable, it follows that
(mγ )∗λ⊥
ω′ = λ⊥
ω′
for all γ ∈Γ and λ′-a.e. ω′ ∈Ω′.
Thus, for λ′-a.e. ω′ ∈Ω′ we have λ⊥
ω′({γ · x}) = λ⊥
ω′({x}), and since Γ acts transi-
tively on κ−1(ω′), it follows that singleton subsets have equal measure, i.e., there is
a constant cω′ with
λ⊥
ω′

A′
= cω′
A′
for all A′ ⊆κ−1(ω′). As κ−1(ω′) is countably inﬁnite, this implies that λ⊥
ω′ = 0 if
cω′ = 0, and λ⊥
ω′(κ−1(ω′)) = ∞if cω′ > 0. Thus, λ⊥
ω′ is not a probability measure
for λ′-a.e. ω′ ∈Ω′, contradicting Proposition 5.3. This shows that λ does not admit
κ-transverse measures.

252
5
Information Geometry and Statistics
Let us conclude this section by investigating the injectiveness and the surjective-
ness of the map κ∗.
Deﬁnition 5.3 Let (Ω,μ0) be a set with a σ-ﬁnite measure. A statistic κ :
(Ω,B) →(Ω′,B′) is called measure-injective if for all disjoint measurable subsets
A,B ⊆Ω the intersection κ(A) ∩κ(B) is a null set w.r.t. κ∗μ0 in Ω′. It is called
measure-surjective if Ω′\κ(Ω) is a μ0-null set. Moreover, it is called measure-
bijective if it is both measure-injective and measure-surjective.
Proposition 5.4 Let κ : (Ω,μ0) →(Ω′,μ′
0) be a statistic, and let κ∗: S(Ω,μ0) →
S(Ω′,μ′
0) be the push-forward map from above. Then the following hold:
(1) κ is measure-injective if and only if κ∗is injective.
(2) κ is measure-surjective if and only if κ∗is surjective.
(3) κ is measure-bijective if and only if κ∗is bijective.
Proof Assume that κ is measure-injective, and let ν ∈ker(κ∗). Let φ := dν/dμ0 ∈
L1(Ω,μ0). Decompose φ = φ+ −φ−into its non-negative and non-positive part
and let Ω± be the support of φ±. Furthermore, let Ω′
± := κ(Ω±) ⊆Ω′. Since κ∗is
measure-injective and Ω± are disjoint, it follows that Ω′
+ ∩Ω′
−is a μ′
0-null set. On
the other hand,
0 = κ∗(φμ0) = κ∗(φ+μ0) −κ∗(φ−μ0),
and since κ∗(φ±μ0) is supported on Ω′
± and Ω′
+ ∩Ω′
−is a null set, it follows that
κ∗(φ±μ0) = 0.
However, since φ± ≥0, κ∗(φ±μ0) = 0 implies that φ± = 0 and hence, φ = 0
μ0-a.e. and hence, ν = φμ0 = 0.
Conversely, suppose that κ∗is injective, and let A,B ⊆Ω be disjoint sets. We
let φ′
A,φ′
B ∈L1(Ω′,μ′
0) be characterized by
κ∗(χA μ0) = φ′
Aμ′
0
and
κ∗(χB μ0) = φ′
Bμ′
0.
Now we calculate by (5.11)
κ∗

κ∗φ′
BχA μ0

= φ′
Bκ∗(χA μ0) = φ′
Aφ′
Bμ′
0 = κ∗

κ∗φ′
AχB μ0

.
The injectivity of κ∗implies that κ∗φ′
BχA = κ∗φ′
AχB = 0, as these functions have
disjoint support. Thus, κ∗φ′
B vanishes on A, whence φ′
B vanishes on κ(A). On the
other hand, φ′
B > 0 a.e. on κ(B), whence κ(A) ∩κ(B) is a null set. This shows that
κ is measure-injective.
For the second statement, let Ω′
1 := κ(Ω) ⊆Ω′. From the deﬁnition of κ∗it is
clear that for any ν ∈S(Ω,μ0), κ∗ν is supported on Ω′
1. Thus, χΩ′\Ω′
1 does not lie
in the image of κ∗, whence κ∗is surjective only if Ω′\Ω′
1 is a null set, i.e., if κ is
measure-surjective. Conversely, if κ is measure-surjective, so that Ω′
1 = Ω′, then for
φ′ ∈L1(Ω′,μ′
0), we have φ′μ′
0 = κ∗(κ∗φ′μ0), which shows the surjectivity of κ∗.
The third statement is immediate from the ﬁrst two.
□

5.1
Congruent Embeddings and Sufﬁcient Statistics
253
5.1.2 Markov Kernels and Congruent Markov Embeddings
In this section, we shall deﬁne the notion of Markov morphisms. We refer e.g. to
[43] for a detailed treatment of this notion.
Deﬁnition 5.4 (Markov kernel) A Markov kernel between two measurable spaces
(Ω,B) and (Ω′,B′) is a map K : Ω →P(Ω′) associating to each ω ∈Ω a proba-
bility measure on Ω′ such that for each ﬁxed measurable A′ ⊆Ω′ the map
Ω −→[0,1],
ω −→K(ω)

A′
=: K

ω;A′
is measurable for all A′ ∈B′.
Furthermore, we call a Markov kernel dominated by a measure μ′
0 on Ω′ if K(ω)
on Ω′ is dominated by μ′
0 for all ω ∈Ω.
Remark 5.2 In some references, the terminology Markov transition is used instead
of Markov kernel, e.g., [25, Deﬁnition 4.1].
If a Markov kernel K is dominated by μ′
0, then there is a density function of K
w.r.t. μ′
0, K : Ω × Ω′ →ˆR, such that
K

ω;A′
=

A′ K

ω;ω′
μ′
0

ω′
for all ω ∈Ω and A′ ∈B′.
(5.21)
We shall denote both the Markov kernel and its density function by K if this will
not cause confusion. Also, observe that for a ﬁxed ω ∈Ω, the function K(ω;·) is
deﬁned only up to changes on a μ0-null set.
Evidently, the density function satisﬁes the properties
K

ω;ω′
≥0
and

Ω′ K

ω;ω′
dμ′
0

ω′
= 1
for all ω ∈Ω.
(5.22)
Example 5.4
(1) (Markov kernels between ﬁnite spaces) Consider the ﬁnite sets I = {1,...,m}
and I ′ = {1,...,n}. Any measure on I ′ is dominated by the counting measure
μ0 := 	
i′∈I ′ δi′ which has no non-empty null sets. Thus, there is a one-to-one
correspondence between Markov kernels between I and I ′ and density func-
tions K : I × I ′ →R, (i,i′) →Ki
i′ satisfying
Ki
i′ ≥0
and

i′
Ki
i′ = 1
for all i.
In this case, the probability measure Ki = K(i) on I ′ associated to i ∈I is
given as
Ki =

i′
Ki
i′δi′,

254
5
Information Geometry and Statistics
whence for i ∈I and A′ ⊆I ′ we have
K

i;A′
= Ki
A′
=

i′∈A′
Ki
i′.
(2) (Markov kernels induced by statistics) Let κ : Ω →Ω′ be a statistic. Then there
is an associated Markov kernel Kκ, associating to each ω ∈Ω the Dirac mea-
sure on Ω′ supported at κ(ω) ∈Ω′. That is, we deﬁne
Kκ(ω) := δκ(ω),
so that Kκ
ω;A′
:= χκ−1(A′)(ω).
(5.23)
Remark 5.3 If Ω′ is countable, then any probability measure on Ω′ is dominated by
a positive measure. Namely, if Ω′ = {ωn : n ∈N}, then we may deﬁne the measure
μ′
0 := 	
n∈N anδn, where δn is the Dirac measure supported at ωn and (an)n∈N is
a sequence with an > 0 and 	
n an < ∞. Then there is no non-empty μ′
0-null set
in Ω′, so that any measure on Ω′ is dominated by μ0.
Thus, any Markov kernel K : Ω →P(Ω′) with Ω′ countable can always be
represented by a density function as in (5.21).
On the other hand, the Markov kernel Kκ is not dominated by any measure on Ω′
unless κ(Ω) ⊆Ω′ is countable. Namely, if Kκ is dominated by such a measure μ′
0,
then μ′
0({κ(ω)}) > 0 as Kκ(ω)({κ(ω)}) = 1, hence the singleton sets {κ(ω)} are
atoms. Since the ﬁnite measure μ′
0 can have only countably many atoms, the count-
ability of κ(Ω) ⊆Ω′ follows.
For instance, the Markov kernel induced by the identity κ : Ω →Ω for an un-
countable set Ω is not dominated by any measure (cf. [61, p. 511]).
Deﬁnition 5.5 (Markov morphism) Let Ω,Ω′ be measure spaces. A Markov mor-
phism is a linear map of the form
K∗: S(Ω) −→S

Ω′
,
K∗μ

A′
:=

Ω
K

ω;A′
dμ(ω),
(5.24)
where K : Ω →P(Ω′) is a Markov kernel. In this case, we say that K∗is induced
by K.
In analogy to the notation for K we shall often denote
K∗

μ;A′
:= K∗μ

A′
.
Observe that we can recover the Markov kernel K from K∗using the relation
K(ω) = K∗δω
for all ω ∈Ω.
(5.25)
If K is dominated by the measure μ′
0 on Ω′, so that there is a density function
K : Ω × Ω′ →ˆR, then (5.24) can be rewritten as
K∗

μ;A′
=

Ω×A′ K

ω,ω′
d

μ × μ′
0

.
(5.26)

5.1
Congruent Embeddings and Sufﬁcient Statistics
255
The map ω →K(ω;A′) is measurable and bounded, whence the integral in
(5.24) converges. Moreover, since K(ω;A′) ≥0, it follows that K∗is monotone,
i.e., it maps non-negative measures on Ω to non-negative measures on Ω′.
Since K(ω) is a probability measure, we have K(ω;Ω′) = 1 for all ω ∈Ω,
whence
K∗

μ;Ω′
=

Ω
K

ω;Ω′
(
)*
+
=1
dμ = μ(Ω).
(5.27)
If μ ∈M(Ω) and hence K∗μ ∈M(Ω′), then ∥K∗μ∥T V = K∗μ(Ω) and ∥μ∥T V =
μ(Ω), whence
∥K∗μ∥T V = ∥μ∥T V
for all μ ∈M(Ω).
(5.28)
For a general measure μ ∈S(Ω), (5.24) implies that |K∗(μ;A′)| ≤K∗(|μ|;A′)
for all A′ ∈B′ and hence,
∥K∗μ∥T V ≤
K∗|μ|

T V = ∥μ∥T V
for all μ ∈S(Ω),
so that K∗: S(Ω) →S(Ω′) is a bounded linear map.
Remark 5.4 From (5.24) it is immediate that K∗preserves dominance of measures,
i.e., if μ1 dominates μ2, then K∗μ1 dominates K∗μ2. Thus, for each μ ∈M(Ω)
there is a restriction
K∗: S(Ω,μ) −→S

Ω′,μ′
,
(5.29)
where μ′ := K∗μ. Thus, just as in the case of statistics (5.10), we obtain induced
bounded linear maps
Kμ
∗: L1(Ω,μ) −→L1
Ω′,μ′
,
K∗(φμ) = Kμ
∗(φ)μ′.
(5.30)
Example 5.5
(1) (Markov kernels between ﬁnite spaces) Consider the Markov kernel between
the ﬁnite sets I = {1,...,m} and I ′ = {1,...,n}, determined by the (n × m)-
matrix (Ki
i′)i,i′ as described in Example 5.4. Following the deﬁnition, we see
that
K

δi
= Ki =

i′
Ki
i′δi′,
whence by linearity,
K∗

i
xiδi

=

i,i′
Ki
i′xiδi′.

256
5
Information Geometry and Statistics
Thus, if we identify the spaces S(I) and S(I ′) of signed measures on I and I ′
with Rm and Rn, respectively, by the correspondence
Rm ∋(x1,...,xm) −→

i
xiδi
and
Rn ∋(y1,...,yn) −→

i′
yi′δi′,
then the Markov morphism K∗: S(I) →S(I ′) corresponds to the linear map
Rm →Rn given by multiplication by the (n × m)-matrix (Ki
i′)i,i′.
(2) (Markov kernels induced by statistics) Let Kκ : Ω →P(Ω′) be the Markov
kernel associated to the statistic κ : Ω →Ω′ described in Example 5.4. Then
for each signed measure μ ∈S(Ω) we have
Kκ
∗μ

A′
=

Ω
K

ω;A′
dμ(ω)
(5.23)
=

κ−1(A′)
dμ = κ∗μ

A′
,
whence Kκ
∗: S(Ω) →S(Ω′) coincides with the induced map κ∗from (5.6).
Due to the identity Kκ
∗= κ∗we shall often write the Markov kernel Kκ
shortly as κ if there is no danger of confusion.
Deﬁnition 5.6 (Composition of Markov kernels) Let (Ωi,Bi), i = 1,2,3, be mea-
surable spaces, and let Ki : Ωi →P(Ωi+1) for i = 1,2 be Markov kernels. The
composition of K1 and K2 is the Markov kernel
K2K1 : Ω1 −→P(Ω3),
ω −→(K2)∗K1(ω).
Since ∥(K2)∗K1(ω)∥T V = ∥K1(ω)∥T V = 1 by (5.28), (K2)∗K1(ω) is a proba-
bility measure, hence this composition indeed yields a Markov kernel. Moreover, it
is straightforward to verify that this composition is associative, and for the induced
Markov morphism we have
(K2K1)∗= (K2)∗(K1)∗.
(5.31)
Deﬁnition 5.7 (Congruent Markov kernels, Markov morphisms) A Markov kernel
K : Ω′ →P(Ω) is called congruent if there is a statistic κ : Ω →Ω′ such that
κ∗K

ω′
= δω′
for all ω′ ∈Ω′,
or, equivalently, K∗is a right inverse of κ∗, i.e.,
κ∗K∗= IdS(Ω′) : S

Ω′
−→S

Ω′
.
(5.32)
In this case, we also say that K is κ-congruent or congruent w.r.t. κ, and the induced
Markov morphism K∗: S(Ω′) →S(Ω) is also called (κ-)congruent.
The notions of congruent Markov morphism from Deﬁnition 5.7 and that of con-
gruent embeddings from Deﬁnition 5.1 are closely related, but not identical. In fact,
we have the following

5.1
Congruent Embeddings and Sufﬁcient Statistics
257
Theorem 5.1 Let κ : Ω →Ω′ be a statistic and μ′ ∈M(Ω′) be a measure.
(1) If K : Ω′ →P(Ω) is a κ-congruent Markov kernel, then the restriction of K∗
to S(Ω′,μ′) ⊆S(Ω′) is a κ-congruent embedding and hence,
K∗

φ′μ′
=

κ∗φ′
K∗μ′
for all φ′ ∈L1
Ω′,μ′
.
(5.33)
In particular, Kμ′
∗(φ′) = κ∗φ′.
(2) Conversely, if K∗: S(Ω′,μ′) →S(Ω) is a κ-congruent embedding, then the
following are equivalent:
(a) K∗is the restriction of a κ-congruent Markov morphism to S(Ω′,μ′) ⊆
S(Ω′).
(b) μ := K∗μ′ ∈S(Ω) admits transverse measures w.r.t. κ.
This explains on a more conceptual level why the notions of congruent Markov
morphisms and congruent embeddings coincide for ﬁnite measure spaces Ω,Ω′
(cf. Example 5.2). Namely, any measure μ ∈M(Ω) admits a transverse measure
for any statistic κ : Ω →Ω′ if Ω is a ﬁnite measure space.
Proof By (5.32), the restriction of K∗is a κ-congruent Markov morphism in the
sense of Deﬁnition 5.1. Thus, by Proposition 5.2, (5.33) holds.
For the second, note that again by Proposition 5.2 any κ-congruent embedding
is of the form Kμ : S(Ω′,μ′) →S(Ω) given in (5.15) for some measure μ with
κ∗μ = μ′ and μ = K∗μ′.
If we assume that Kμ is the restriction of a κ-congruent Markov morphism in-
duced by the κ-congruent Markov kernel K : Ω′ →P(Ω), then we deﬁne the mea-
sures
μ⊥
ω′ := K

ω′
κ−1(ω) ∈M

κ−1
ω′
.
Note that for ω′ ∈Ω′
K

ω′;Ω\κ−1
ω′
=

Ω\κ−1(ω′)
dK

ω′
=

Ω′\ω′ d

κ∗K

ω′
(5.32)
=

Ω′\ω′ dδω′ = 0.
That is, K(ω′) is supported on κ−1(ω′) and hence, for an arbitrary set A ⊆Ω we
have
K

ω′;A

= K

ω′;A ∩κ−1
ω′
= μ⊥
ω′

A ∩κ−1
ω′
=

κ−1(ω′)
χA dμ⊥
ω′.

258
5
Information Geometry and Statistics
Substituting this into the deﬁnition of K∗and using K∗μ′ = Kμ(μ′) = μ, we obtain
for a subset A ⊆Ω

Ω
χA dμ = μ(A) = K∗

μ′;A
 (5.24)
=

Ω′ K

ω′;A

dμ′
ω′
=

Ω′

κ−1(ω′)
χA dμ⊥
ω′

dμ′
ω′
,
showing that (5.18) holds for φ = χA. But then, by linearity (5.18) holds for any
step function φ, and since these are dense in L1(Ω,μ), it follows that (5.18) holds
for all φ, so that the measures μ⊥
ω′ deﬁned above indeed yield transverse measures
of μ.
Conversely, suppose that μ := K∗μ′ admits transverse measures μ⊥
ω′, and by
Proposition 5.3 we may assume w.l.o.g. that μ⊥
ω′ ∈P(κ−1(ω′)). Then we deﬁne the
map
K : Ω′ −→P(Ω),
K

ω′;A

:= μ⊥
ω′

A ∩κ−1
ω′
=

κ−1(ω′)
χA dμ⊥
ω′.
Since for ﬁxed A ⊆Ω the map ω′ →

κ−1(ω′) χA dμ⊥
ω′ is measurable by the deﬁni-
tion of transversal measures, K is indeed a Markov kernel. Moreover, for A′ ⊆Ω′
κ∗K

ω′
A′
= K

ω′;κ−1
A′
= μ⊥
ω′

κ−1
A′
∩κ−1
ω′
= χA′
ω′
,
so that κ∗K(ω′) = δω′ for all ω′ ∈Ω′, whence K is κ-congruent. Finally, for any
φ′ ∈L1(Ω′,μ′) and A ⊆Ω we have
Kμ

φ′μ′
(A)
(5.15)
= κ∗φ′μ(A) =

Ω
χAκ∗φ′ dμ
(5.18)
=

Ω′

κ−1(ω′)
χAκ∗φ′ dμ⊥
ω′

dμ′
ω′
=

Ω′

κ−1(ω′)
χA dμ⊥
ω′

φ′
ω′
dμ′
ω′
=

Ω′ K

ω′;A

d

φ′μ′
ω′ (5.24)
= K∗

φ′μ′
(A).
Thus, Kμ(ν′) = K∗ν′ for all ν′ = φ′μ′ ∈S(Ω′,μ′), so that the given congru-
ent embedding Kμ coincides with the restriction of the Markov morphism K∗to
S(Ω′,μ′), and this completes the proof.
□
Let us summarize our discussion to clarify the exact relation between the notions
of statistics, Markov kernels, congruent embeddings and congruent Markov kernels:
1. Every statistic induces a Markov kernel (5.23), but not every Markov kernel is
given by a statistic. In fact, a Markov kernel K is induced by a statistic iff each
K(ω) is a Dirac measure supported at a point.

5.1
Congruent Embeddings and Sufﬁcient Statistics
259
2. To every Markov kernel there is an associated Markov morphism, cf. Deﬁni-
tion 5.5. From this Markov morphism we can rediscover the Markov kernel
(5.25), so that there is a 1:1 correspondence between Markov kernels and Markov
morphisms.
3. A congruent embedding K∗: S(Ω,μ) →S(Ω′) is a linear map which is the
right inverse of a statistic κ : Ω′ →Ω, cf. Deﬁnition (5.1). Its image is called
a congruent subspace. Congruent embeddings are always of the form (5.15),
whence every congruent subspace is of the form (5.16).
4. A congruent Markov kernel (congruent Markov morphism, respectively) is a
Markov kernel (Markov morphism, respectively) which is the right inverse of
some statistic, cf. Deﬁnition 5.7.
5. The restriction of a congruent Markov morphism to S(Ω,μ) is always a con-
gruent embedding, whence the image of S(Ω,μ) under a congruent Markov
morphism is always a congruent subspace. However, not every congruent em-
bedding is induced by a congruent Markov morphism, but rather this is the case
iff the corresponding measure admits transverse measures, cf. Theorem 5.1.
6. Since every statistic between ﬁnite sets admits transverse measures, there is no
difference between congruent embeddings and congruent Markov morphisms in
this case, explaining why this careful distinction was not necessary in the discus-
sion in Sect. 2.6 or in the original work of Chentsov [65].
As we saw in Proposition 5.1, the map κμ
∗: L1(Ω,μ) →L1(Ω′,μ′) induced by
a statistic κ : Ω →Ω′ preserves k-integrability for 1 ≤k ≤∞and is an isometry
if κ is congruent. We wish to generalize this result to the map Kμ
∗: L1(Ω,μ) →
L1(Ω′,μ′) (5.30) for a Markov kernel K : Ω →P(Ω′) with μ′ := K∗μ.
If K is κ-congruent for some statistic κ : Ω′ →Ω, then μ = κ∗K∗μ = κ∗μ′
and, by Theorem 5.1, Kμ
∗(φ) = κ∗φ. Thus, (5.14) implies that for 1 ≤k ≤∞and
φ ∈Lk(Ω,μ)
Kμ
∗(φ)

k =
κ∗φ

k = ∥φ∥k,
(5.34)
where the norms are taken in Lk(Ω,μ) and Lk(Ω′,μ′), respectively. In particular,
Kμ
∗(φ) ∈Lk(Ω,μ), so that Lk-regularity of functions is preserved both by congru-
ent Markov kernels and by statistics.
Theorem 5.2 (Cf. [25, Theorem 4.10])
Any Markov kernel K : Ω →P(Ω′) can
be decomposed into a statistic and a congruent Markov kernel. That is, there is
a Markov kernel Kcong : Ω →P( ˆΩ) which is congruent w.r.t. some statistic κ1 :
ˆΩ →Ω and a statistic κ2 : ˆΩ →Ω′ such that
K = (κ2)∗Kcong.
Proof Let ˆΩ := Ω × Ω′ and let κ1 : ˆΩ →Ω and κ2 : ˆΩ →Ω′ be the canonical
projections. We deﬁne the Markov kernel
Kcong : Ω −→P( ˆΩ),
Kcong(ω) := δω × K(ω),

260
5
Information Geometry and Statistics
i.e., Kcong(ω; ˆA) := K(ω;κ2( ˆA ∩({ω} × Ω′))) for
ˆA ⊆ˆΩ. Then evidently,
(κ1)∗(Kcong(ω)) = δω, so that Kcong is κ1-congruent, and (κ2)∗Kcong(ω) = K(ω),
so the claim follows.
□
Corollary 5.1
Let K : Ω →P(Ω′) be a Markov kernel, let μ ∈M(Ω), μ′ :=
K∗μ ∈M(Ω′) and Kμ
∗: L1(Ω,μ) →L1(Ω′,μ′) be the map from (5.30). Then
Kμ
∗maps Lk(Ω,μ) to Lk(Ω′,μ′) for 1 ≤k ≤∞, and satisﬁes
Kμ
∗(φ)

k ≤∥φ∥k,
where the norms are taken in Lk(Ω,μ) and Lk(Ω′,μ′), respectively.
Proof By Theorem 5.2 we can decompose K = κ∗Kcong, where Kcong : Ω →
P( ˆΩ) is congruent w.r.t. some statistic ˆκ : ˆΩ →Ω, and with a statistic κ : ˆΩ →Ω′.
Then it follows that K∗= κ∗Kcong
∗
, and whence,
Kμ
∗= κ ˆμ
∗

Kcong
∗
μ,
where ˆμ := Kcong
∗
μ ∈M( ˆΩ). But (Kcong
∗
)μ : Lk(Ω,μ) →Lk( ˆΩ, ˆμ) is an isome-
try by (5.34), whereas κ ˆμ
∗: Lk( ˆΩ, ˆμ) →Lk(Ω′,μ′) does not increase the norm by
(5.13), whence
Kμ
∗(φ)

k =
κ ˆμ
∗

Kcong
∗
μ(φ)

k ≤

Kcong
∗
μ(φ)

k = ∥φ∥k,
proving the assertion.
□
Remark 5.5
Corollary 5.1 can be interpreted in a different way. Namely, given
a Markov kernel K : Ω →P(Ω′) and r ∈(0,1], one can deﬁne the map Kr
∗:
Sr(Ω) →Sr(Ω′) by
Kr
∗˜μr := ˜πr(K∗μ)
for μ ∈S(Ω),
(5.35)
with the signed rth power ˜μr deﬁned before. Since ˜πr and π1/r are both continuous
by Proposition 3.2, the map Kr
∗is continuous, but it fails to be C1 for r < 1, even
for ﬁnite Ω.
Let μ ∈M(Ω) and μ′ := K∗μ ∈M(Ω′), so that Kr
∗(μr) = μ′r. If there
was a derivative of Kr
∗at μr, then it would have to be a map between the tan-
gent spaces TμrM(Ω) and Tμ′rM(Ω′), i.e., according to Proposition 3.1 between
Sr(Ω,μ) and Sr(Ω′,μ′). Let k := 1/r > 1, φ ∈Lk(Ω,μ) ⊆L1(Ω,μ), so that
φ′ := Kμ
∗(φ) ∈Lk(Ω′,μ′) by Corollary 5.1. Then by Proposition 3.2 and the chain
rule, we obtain
d

˜πkKr
∗

μr

φμr
= kμ′1−r · d

Kr
∗

μr

φμr
,
d

K∗˜πk
μr

φμr
= kK∗(φμ) = kφ′μ′,

5.1
Congruent Embeddings and Sufﬁcient Statistics
261
and these should coincide as ˜πkKr
∗= K∗˜πk by (5.35). Since d(Kr
∗)μr(φμr) ∈
Sr(Ω′,μ′), we thus must have
d

Kr
∗

μr

φμr
= φ′μ′r,
where φ′ = Kμ
∗(φ).
(5.36)
Thus, Corollary 5.1 states that this map is a well-deﬁned linear operator with op-
erator norm ≤1. The map d(Kr
∗)μr : Sr(Ω,μ) →Sr(Ω′,μ′) from (5.36) is called
the formal derivative of Kr
∗at μr.
5.1.3 Fisher–Neyman Sufﬁcient Statistics
Given a parametrized measure model (M,Ω,p), for a statistic κ : Ω →Ω′ the
induced model (M,Ω′,p′) given by p′(ξ) := κ∗p(ξ) usually contains less informa-
tion than the original model. If no information is lost when passing from (M,Ω,p)
to (M,Ω′,p′), then κ is called a sufﬁcient statistic for the model. This concept was
ﬁrst introduced by Fisher who stated that
“. . . the criterion of sufﬁciency, which latter requires that the whole of the relevant informa-
tion supplied by a sample shall be contained in the statistics calculated.” [96, p. 367]
We shall give a precise deﬁnition of sufﬁciency of a statistic later in Deﬁni-
tion 5.10, but in this section, we shall concentrate on the following most important
class of examples.
Deﬁnition 5.8 (Fisher–Neyman sufﬁcient statistics [202])
Let (M,Ω,p) be a
parametrized measure model and κ : Ω →Ω′ a statistic, and suppose that there
is a μ ∈M(Ω) such that
p(ξ) = p′
κ(·);ξ

μ
for some p′(·;ξ) ∈L1(Ω′,μ′) and hence,
p′(ξ) = κ∗p(ξ) = p′(·;ξ)μ′,
where μ′ = κ∗μ. Then κ is called a Fisher–Neyman sufﬁcient statistic for the model
(M,Ω,p).
Evidently, by (5.16) this is equivalent to saying that p(ξ) ⊆Cκ,μ for all ξ ∈M
and some ﬁxed measure μ ∈M(Ω).
In this section, we shall give several reformulations of the Fisher–Neyman sufﬁ-
ciency of a statistic in the presence of a density function.
Proposition 5.5 Let (M,Ω,μ0,p) be a parametrized measure model given by the
density function
p(ξ) = p(ω;ξ)μ0,

262
5
Information Geometry and Statistics
let κ∗: Ω →Ω′ be a statistic with the induced parametrized measure model
(M,Ω′,μ′
0,p′) where p′(ξ) = κ∗p(ξ) and μ′
0 = κ∗μ0, given by
p′(ξ) = p′
ω′;ξ

μ′
0.
Then κ is a Fisher–Neyman sufﬁcient statistic for (M,Ω,μ0,p) if and only if there
is a function r ∈L1(Ω,μ0) such that p,p′ can be chosen such that
p(ω;ξ) = r(ω)p′
κ(ω);ξ

.
(5.37)
Proof For a given ξ ∈M, let
Aξ :=

ω ∈Ω : p(ω;ξ) = 0

and
Bξ :=

ω ∈Ω : p′
κ(ω);ξ

= 0

.
Then
p(ξ)(Bξ) ≤p(ξ)

κ−1κ(Bξ)

= p′(ξ)

κ(Bξ)

= 0,
since p′(ω′;ξ) vanishes on κ(Bξ) by deﬁnition. Thus, p(ξ)(Bξ) = 0 and hence,
Bξ\Aξ ⊆Ω is a μ0-null set. After setting p(ω;ξ) ≡0 on this set, we may assume
Bξ\Aξ = ∅, i.e., Bξ ⊆Aξ, and with the convention 0/0 =: 1 we can therefore deﬁne
the measurable function
r(ω,ξ) :=
p(ω;ξ)
p′(κ(ω);ξ).
In order to prove the assertion, we have to show that r is independent of ξ and
has a ﬁnite integral if and only if κ is a Fisher–Neyman sufﬁcient statistic.
Let us assume that κ is as in Example 5.8. In this case, there is a measure
μ ∈M(Ω,μ0) such that p(ξ) = ˜p′(κ(ω);ξ)μ for some ˜p′ : Ω′ × M →R with
˜p′(·;ξ) ∈L1(Ω′,μ′) for all ξ. Moreover, we let μ′ := κ∗μ. Thus,
p′(ξ) = κ∗p(ξ) = κ∗

κ∗
˜p′(·;ξ)μ

= ˜p′(·;ξ)μ′
by (5.11). Since p(ξ) is dominated by both μ and μ0, we may assume w.l.o.g. the
μ is dominated by μ0 and hence, μ′ is dominated by μ′
0, and we let
ψ0 := dμ
dμ0
∈L1(Ω,μ0)
and
ψ′
0 := dμ′
dμ′
0
∈L1
Ω′,μ′
0

.
Then
p(ω,ξ) = dp(ξ)
dμ0
= dp(ξ)
dμ
dμ
dμ0
(ω;ξ) = ˜p′
κ(ω);ξ

ψ0(ω),
and similarly,
p′
ω′,ξ

= ˜p′
ω′;ξ

ψ′
0

ω′
.

5.1
Congruent Embeddings and Sufﬁcient Statistics
263
Therefore,
r(ω;ξ) =
p(ω;ξ)
p′(κ(ω);ξ) =
˜p′(κ(ω);ξ)ψ0(ω)
˜p′(κ(ω);ξ)ψ′
0(κ(ω)) =
ψ0(ω)
ψ′
0(κ(ω)),
which is independent of ξ ∈M, i.e., r(ω;ξ) = r(ω) as asserted. Moreover, r ∈
L1(Ω,μ0) since

Ω
r(ω) dμ0 =

Ω
ψ0(ω)
ψ′
0(κ(ω)) dμ0 =

Ω
κ∗

1
ψ′
0(ω′)

dμ
=

Ω′
1
ψ′
0(ω′) dμ′ =

Ω′ dμ′
0 = μ′
0

Ω′
= μ0(Ω) < ∞.
Conversely, if (5.37) holds for r ∈L1(Ω,μ0), then for μ := rμ0 ∈M(Ω,μ0)
p(ξ) = p(·;ξ)μ0 = p′
κ(·);ξ

μ,
which coincides with Example 5.8.
□
This statement yields the following characterization of Fisher–Neyman sufﬁcient
statistics, which is essentially a reformulation of the previous result.
Theorem 5.3 (Fisher–Neyman characterization) Let (M,Ω,μ0,p) be a parame-
trized measure model given by the density function
p(ξ) = p(ω;ξ)μ0,
and let κ : Ω →Ω′ be a statistic. Then κ is Fisher–Neyman sufﬁcient for the
parametrized measure model if and only if there exist a function s : Ω′ × M →R
and a function t ∈L1(Ω,μ0) such that for all ξ ∈M we have s(ω′,ξ) ∈
L1(Ω′,κ∗μ0) and
p(ω;ξ) = s

κ(ω);ξ

t(ω)
μ0-a.e.
(5.38)
Proof If (5.38) holds, let μ := t(ω)μ0 ∈M(Ω,μ0). Then we have for all ξ ∈M
p(ξ) = p(ω;ξ)μ0 = s

κ(ω);ξ

μ,
whence κ is a Fisher–Neyman sufﬁcient statistic for p.
Conversely, if κ is a sufﬁcient statistic for p, then we let t := r and s := p′ in
(5.37). Then (5.38) follows.
□
5.1.4 Information Loss and Monotonicity
Given a parametrized measure model (statistical model, respectively) (M,Ω,p)
and a Markov kernel K : Ω →P(Ω′) which induces the Markov morphism K∗:

264
5
Information Geometry and Statistics
M(Ω) →M(Ω′) as in (5.24), we obtain another parametrized measure model (sta-
tistical model, respectively) (M,Ω′,p′) by deﬁning p′(ξ) := K∗p(ξ).
It is the purpose of this section to investigate the relation between these two
models in more detail. In particular, we shall show that k-integrability is preserved,
and we shall quantify the information loss and hence deﬁne sufﬁcient statistics. We
begin with the following result.
Theorem 5.4 (Cf. [26])
Let (M,Ω,p), K : Ω →P(Ω′) and (M,Ω′,p′) be as
above, and suppose that (M,Ω,p) is k-integrable for some k ≥1. Then (M,Ω′,p′)
is also k-integrable, and
∂V logp′(ξ)

k ≤
∂V logp(ξ)

k
for all V ∈TξM,
(5.39)
where the norms are taken in Lk(Ω,p(ξ)) and Lk(Ω′,p′(ξ)), respectively. If K is
congruent, then equality in (5.39) holds for all V .
Moreover, if K is given by a statistic κ : Ω →Ω′ and k > 1, then equality in
(5.39) holds iff ∂V logp(ξ) = κ∗(∂V logp′(ξ)).
Proof Since K∗is the restriction of a bounded linear map, it is obvious that p′ :
M →M(Ω′) is again differentiable, and in fact,
dξp′(V ) = K∗

dξp(V )

,
(5.40)
for all V ∈TξM, ξ ∈M.
Let μ := p(ξ) and μ′ := p′(ξ) = K∗μ, and let φ := ∂V logp(ξ) and φ′ :=
∂V logp′(ξ), so that dξp(V ) = φμ and dξp′(V ) = φ′μ′. By (5.40) we thus have
K∗(φμ) = φ′μ′,
so that φ′ = Kμ
∗(φ) is the expectation value of φ given K. If p is k-integrable, then
φ = ∂V logp(ξ) ∈Lk(Ω,μ), whence φ′ ∈Lk(Ω′,μ′), and ∥φ′∥k ≤∥φ∥k by Corol-
lary 5.1. In fact, φ′ = d(Kr
∗)p(ξ)r(φ) by (5.36), and since d(Kr
∗)p(ξ)r is a bounded
linear map, depending continuously on ξ, it follows that ∥φ′∥k = ∥∂V logp′(ξ)∥k
depends continuously on V ∈T M and therefore, p′ is k-integrable as well and
(5.39) holds.
If K is congruent, then ∥φ′∥k = ∥φ∥k by (5.34).
If k > 1 and K is given by a statistic κ, then equality in (5.39) occurs iff φ = κ∗φ′
by Proposition 5.1.
□
Since the Fisher metrics gF of (M,Ω,p) and g′F of (M,Ω′,p′) are deﬁned as
g(V,V ) =
∂V logp(ξ)
2
2
and
g′(V,V ) =
∂V logp′(ξ)
2
2
by (3.41), Theorem 5.4 immediately implies the following.

5.1
Congruent Embeddings and Sufﬁcient Statistics
265
Theorem 5.5 (Monotonicity theorem (cf. [16, 25, 27, 164])) Let (M,Ω,p) be a k-
integrable parametrized measure model for k ≥2, let K : Ω →P(Ω′) be a Markov
kernel, and let (M,Ω′,p′) be given by p′(ξ) = K∗p(ξ). Then
g(V,V ) ≥g′(V,V )
for all V ∈TξM and ξ ∈M.
(5.41)
Remark 5.6 Note that our approach allows us to prove the Monotonicity Theo-
rem 5.5 with no further assumption on the model (M,Ω,p). In order for (5.41)
to hold, we can work with arbitrary Markov kernels, not just statistics κ. Even if K
is given by a statistic κ, we do not need to assume that Ω is a topological space with
its Borel σ-algebra as in [164, Theorem 2], nor do we need to assume the existence
of transversal measures of the map κ (e.g., [16, Theorem 2.1]), nor do we need to
assume that all measures p(ξ) have the same null sets (see [25, Theorem 3.11]). In
this sense, our statement generalizes these versions of the monotonicity theorem, as
it even covers a rather peculiar statistic as in Example 5.3.
In [16, p. 98], the difference
g(V,V ) −g′(V,V ) ≥0
(5.42)
is called the information loss of the model under the statistic κ, a notion which is
highly relevant for statistical inference. This motivates the following deﬁnition.
Deﬁnition 5.9 Let (M,Ω,p) be k-integrable for some k > 1. Let K : Ω →P(Ω′)
and (M,Ω′,p′) be as above, so that (M,Ω′,p′) is k-integrable as well. Then for
each V ∈TξM we deﬁne the kth order information loss under K in direction V as
∂V logp(ξ)
k
k −
∂V logp′(ξ)
k
k ≥0,
where the norms are taken in Lk(Ω,p(ξ)) and Lk(Ω′,p′(ξ)), respectively.
That is, the information loss in (5.42) is simply the special case k = 2 in Deﬁni-
tion 5.9. Observe that due to Theorem 5.4 the vanishing of the information loss for
some k > 1 implies the vanishing for all k > 1 for which this norm is deﬁned. That
is, the kth order information loss measures the same quantity by different means.
For instance, if (M,Ω,p) is k-integrable for 1 < k < 2, but not 2-integrable, then
the Fisher metric and hence the classical information loss in (5.42) is not deﬁned.
Nevertheless, we still can quantify the kth order information loss of a statistic of this
model.
Observe that for k = 2n an even integer τ 2n
(M,Ω,p)(V,...,V ) = ∥∂V logp(ξ)∥2n
2n,
whence the difference
τ 2n
(M,Ω,p)(V,...,V ) −τ 2n
(M,Ω′,p′)(V,...,V ) ≥0
represents the (2n)th order information loss of κ in direction V . This gives an inter-
pretation of the canonical 2n-tensors τ 2n
(M,Ω,p).

266
5
Information Geometry and Statistics
Having quantiﬁed the information loss, we now can deﬁne sufﬁcient statistics,
making precise the original idea of Fisher [96] which was quoted on page 261.
Deﬁnition 5.10 (Sufﬁcient statistic)
Let (M,Ω,p) be a parametrized measure
model which is k-integrable for some k > 1. Then a statistic κ : Ω →Ω′ or, more
general, a Markov kernel K : Ω →P(Ω′) is called sufﬁcient for the model if the
kth order information loss vanishes for all tangent vectors V , i.e., if
∂V logp′(ξ)

k =
∂V logp(ξ)

k
for all V ∈TξM,
where p′(ξ) = κ∗p(ξ) or p′(ξ) = K∗p(ξ), respectively.
Again, in this deﬁnition it is irrelevant which k > 1 is used, as long as k-
integrability of the model is satisﬁed.
In fact, the Fisher–Neyman sufﬁcient statistics from Deﬁnition 5.8 are sufﬁcient
in this sense and, under further assumptions, the only sufﬁcient statistics, as the
following result shows.
Proposition 5.6 ([202]) Let (M,Ω,μ,p) be a parametrized measure model, where
M is a connected manifold, with a positive regular density function p : Ω × M →
(0,∞), and let κ : Ω →Ω′ be a statistic which induces the model (M,Ω′,μ′,p′)
given by p′(ξ) := κ∗p(ξ). Then κ is a sufﬁcient statistic for the model if and only if
it is a Fisher–Neyman sufﬁcient statistic.
Proof If p(ξ) = p(·;ξ)μ0 where p is positive and differentiable in the ξ-
variable, then logp(·;ξ) and logp′(·;ξ) are well-deﬁned functions on Ω × M and
Ω′ × M, respectively and differentiable in ξ. In particular, κ∗(∂V logp′(·;ξ)) =
∂V (logp′(κ(·);ξ)), so that by Theorem 5.4 equality in (5.39) holds for k > 1 iff
∂V log
p(·;ξ)
p′(κ(·);ξ) = ∂V

logp(·;ξ) −

logp′
κ(·);ξ

= 0.
If M is connected, then this is the case for all V ∈T M iff the positive function
r(·) :=
p(·;ξ)
p′(κ(·);ξ) does not depend on ξ ∈M, that is, iff (5.37) is satisﬁed. Thus, by
Proposition 5.5 this is the case iff κ is Fisher–Neyman sufﬁcient for the model.
□
Observe that the proof uses the positivity of the density function p in a crucial
way. In fact, without this assumption the conclusion is false, as the following exam-
ple shows.
Example 5.6
Let Ω := (−1,1) × (0,1), Ω′ := (−1,1) and κ : Ω →Ω′ be the
projection onto the ﬁrst component. For ξ ∈R we deﬁne the statistical model p on
Ω as p(ξ) := p(s,t;ξ) dsdt, where
p(s,t;ξ) :=
⎧
⎪⎨
⎪⎩
h(ξ)
for ξ ≥0 and s ≥0,
2h(ξ)t
for ξ < 0 and s ≥0,
1 −h(ξ)
for s < 0,

5.1
Congruent Embeddings and Sufﬁcient Statistics
267
with h(ξ) := exp(−|ξ|−1) for ξ ̸= 0 and h(0) := 0. Then p(ξ) is a probability mea-
sure, and
p′(ξ) := κ∗p(ξ) = p′(s;ξ) ds
with
p′(s;ξ) :=

1 −h(ξ)

χ(−1,0)(s) + h(ξ)χ[0,1)(s),
and thus,
∂ξ logp(s,t;ξ)

k =
∂ξ logp′(s;ξ)

k
= k

d
dξ h(ξ)1/k

k
+

d
dξ

1 −h(ξ)
1/k

k1/k
,
where the norm is taken in Lk(Ω,p(ξ)) and Lk(Ω′,p′(ξ)), respectively. Since this
expression is continuous in ξ for all k, the models (R,Ω,p) and (R,Ω′,p′) are
∞-integrable, and there is no information loss of kth order for any k ≥1, so that
κ is a sufﬁcient statistic of the model in the sense of Deﬁnition 5.10. Thus, κ is a
sufﬁcient statistic for the model.
Indeed, this model is Fisher–Neyman sufﬁcient when restricted to ξ ≥0 and to
ξ ≤0, respectively; in these cases, we have
p(ξ) = p′(s;ξ)μ±,
with the measures μ+ := ds dt for ξ ≥0 and μ−:= (χ(−1,0)(s) + 2tχ[0,1)(s)) ds dt
for ξ ≤0, respectively.
However, since μ+ ̸= μ−, κ is not Fisher–Neyman sufﬁcient. This does not con-
tradict Proposition 5.6 since p(s,t;ξ) is not positive a.e. for ξ = 0. But it shows that
in general, there are sufﬁcient statistics other than those of Fisher–Neyman type.
Remark 5.7
(1) The reader might be aware that most texts use Fisher–Neyman sufﬁciency and
sufﬁciency synonymously, e.g., [96], [25, Deﬁnition 3.1], [16, (2.17)], [53, The-
orem 1, p. 117]. In fact, if the model is given by a regular positive density
function, then the two notions are equivalent by Proposition 5.6, and this is an
assumption that has been made in all these references.
(2) Theorem 5.4 no longer holds for signed k-integrable parametrized measure
models (cf. Sect. 3.2.6). In order to see this, let Ω := {ω1,ω2} and Ω′ :=
{ω′}, and let K be induced by the statistic κ : Ω →Ω′. Consider the signed
parametrized measure model (M,Ω,p) with M = (0,∞) × (0,∞), given by
p(ξ) := ξ1δω1 −ξ2δω2,
ξ := (ξ1,ξ2).
Then (M,Ω,p) has a logarithmic derivative (cf. Deﬁnition 3.11), as p(ξ1,ξ2)
has no non-empty null sets. Furthermore, p is ∞-integrable, since for each

268
5
Information Geometry and Statistics
k ≥1, the map ˜p1/k(ξ) = ξ1/k
1
(δω1)1/k −ξ1/k
2
(δω2)1/k is a C1-map, as
ξ1,ξ2 > 0.
On the other hand, p′(ξ) = K∗p(ξ) = (ξ1 −ξ2)δω′ does not even have a loga-
rithmic derivative. Indeed, dξp′(∂ξi) = (−1)i+1δω′ ̸= 0, whereas p′(ξ1,ξ1) = 0.
That is, if ξ1 = ξ2, then the measure dξp′(∂ξi) ̸= 0 is not dominated by
|p′(ξ)| = 0.
Thus, in general a signed parametrized model with logarithmic derivative
does not induce a model with logarithmic derivative, whence none of the con-
cepts related to k-integrability such as Theorem 5.4 can be generalized to the
case of signed parametrized measure models.
5.1.5 Chentsov’s Theorem and Its Generalization
In the preceding sections we saw how a Markov kernel K : Ω →P(Ω′) (e.g.,
a statistic κ : Ω →Ω′) induces a bounded linear map K∗: S(Ω) →S(Ω′) which
maps M(Ω) and P(Ω) to M(Ω′) and P(Ω′), respectively. This map may be
used to obtain from a parametrized measure model (statistical model, respec-
tively) (M,Ω,p) a new parametrized measure model (statistical model, respec-
tively) (M,Ω′,p′) by setting p′(ξ) := K∗p(ξ). In particular, this process preserves
k-integrability of (M,Ω,p) by Theorem 5.4.
Given covariant n-tensors Θ on S(Ω) and Θ′ on S(Ω′), we may deﬁne the
induced pullbacks p∗Θ and p′∗Θ′ on M. If Θ = K∗Θ′, then, as p′ = K ◦p, it
follows that p∗Θ = p′∗Θ′, and in this case, we call the tensor p∗Θ on M compatible
with K.
It is the aim of this section to characterize those tensors which are compatible
with congruent Markov morphisms K. In the case of ﬁnite Ω, we showed in Theo-
rem 2.3 that the space of tensors which are invariant under congruent Markov mor-
phisms is algebraically generated by the canonical n-tensors Ln
I for ﬁnite sets I, and
we shall discuss the generalization of this to arbitrary measure spaces Ω.
Observe, however, that the canonical n-tensors Ln
Ω, deﬁned in (3.87) are not
deﬁned on S(Ω), but rather on S1/n(Ω), and the canonical n-tensor τ n
M of the
parametrized measure model (M,Ω,p) from (3.44) is the pullback of Ln
Ω under
the differentiable map p1/n : M →M1/n(Ω) ⊆S1/n(Ω) by (3.43).
Thus, given a Markov kernel K∗: S(Ω) →S(Ω′), we need to deﬁne an induced
map Kr : Sr(Ω) →Sr(Ω′) for all r ∈(0,1] which is differentiable in the appro-
priate sense, so that it allows us to deﬁne the pullback K∗
r Θ′ for each covariant
n-tensor on Sr(Ω′). This is the purpose of the following
Deﬁnition 5.11 Let K : Ω →P(Ω′) be a Markov kernel with the induced Markov
morphism K∗: S(Ω) →S(Ω′). For 0 < r ≤1 we deﬁne the map
Kr : Sr(Ω) −→Sr
Ω′
,
νr −→˜πrK∗˜ν1/r

5.1
Congruent Embeddings and Sufﬁcient Statistics
269
with the signed power from (3.60) and the map ˜πr from Proposition 3.2. Moreover,
the formal derivative of Kr at μr ∈Sr(Ω) is deﬁned as
dμrKr : Sr(Ω,μ) −→Sr
Ω′,μ′
,
dμrKr

φμr
= d{K∗(φμ)}
dμ′
μ′r.
(5.43)
Evidently, as ˜π1 = IdS(Ω), we have K1 = K∗. Moreover, Kr maps Mr(Ω) to
Mr(Ω′), and since by (3.61) and (5.28) Kr preserves the norm, i.e.,
Kr(μr)

1/r = ∥μr∥1/r
for all μr ∈Sr(Ω),
(5.44)
Kr also maps Pr(Ω) to Pr(Ω′). We can characterize Kr as the unique map for
which
˜πkKr = K∗˜πk,
where k := 1/r ≥1,
(5.45)
and as we observed in Remark 5.5, the formal derivative dμrKr is uniquely charac-
terized by the equation
dKr(μr) ˜πk ◦dμrKr = K∗◦dμr ˜πk,
(5.46)
which would follow from (5.45) by the chain rule if Kr was a C1 map. Thus, even
though Kr will not be a C1-map in general, we still can deﬁne its formal derivative
in this way.
It thus follows that the formal derivatives
dKr : T Mr(Ω) −→T Mr
Ω′
and
dKr : T Pr(Ω) −→T Pr
Ω′
yield continuous maps between the tangent ﬁbrations described in Proposition 3.1.
Remark 5.8
(1) If K : Ω →P(Ω′) is κ-congruent for some statistic κ : Ω′ →Ω, then (5.43)
together with (5.33) implies for μ ∈M(Ω) and φ ∈L1/r(Ω,μ)
dμrKr

φμr
= κ∗φ(K∗μ)r.
(5.47)
(2) The notion of the formal derivative of a power of a Markov kernel matches
the deﬁnition of the derivative of dp1/k : T M →T M1/k(Ω) of a k-integrable
parametrized measure model from (3.68). Namely, if K : Ω →P(Ω′) is a
Markov kernel and (M,Ω′,p′) with p′ := K∗p is the induced k-integrable
parametrized measure model, then p′1/k = K1/kp1/k, and for the derivatives
we obtain
dp(ξ)1/kK1/k

dξp1/k(V )
 (3.68)
=
1
k dp1/k(ξ)K1/k

∂V logp(ξ)p(ξ)1/k
(5.43)
=
1
k
d{K∗(∂V logp(ξ)p(ξ))}
d{p′(ξ)}
p′(ξ)1/k

270
5
Information Geometry and Statistics
=
1
k
d{K∗(dξp(V ))}
d{p′(ξ)}
p′(ξ)1/k
=
1
k
d{dξp′(V )}
d{p′(ξ)} p′(ξ)1/k
=
1
k ∂V logp′(ξ) p′(ξ)1/k = dξp′1/k(V ),
so that the formal derivatives also satisfy the chain rule
p′1/k = K1/kp1/k =⇒dp′1/k = dK1/k dp1/k.
(5.48)
Our deﬁnition of formal derivatives is just strong enough to deﬁne the pullback
of tensor ﬁelds on the space of probability measures in analogy to (3.90).
Deﬁnition 5.12 (Pullback of tensors by a Markov morphism) Let K : Ω →P(Ω′)
be a Markov kernel, and let Θn be a covariant n-tensor on Pr(Ω′) (on Mr(Ω′),
respectively), cf. Deﬁnition 3.9. Then the pull-back tensor under K is deﬁned as the
covariant n-tensor K∗
r Θn on Pr(Ω) (on Mr(Ω), respectively) given as
K∗
r Θn(V1,...,Vn) := Θn
dKr(V1),...,dKr(Vn)

with the formal derivative dKr : T Mr(Ω) →T Mr(Ω′) from (5.43).
Evidently, K∗Θn is again a covariant n-tensor on Pr(Ω) and Mr(Ω), respec-
tively, since dKr is continuous. Moreover, (5.48) implies that for a parametrized
measure model (M,Ω,p) and the induced model (M,Ω′,p′) with p′ = K∗p we
have the identity
p′∗Θn = p∗K∗Θn
for any covariant n-tensor ﬁeld Θn on Pr(Ω) or Mr(Ω), respectively.
Our main interest in this section is to specify those families of covariant n-tensors
which are preserved under pull-backs of congruent Markov morphisms, generaliz-
ing the question addressed in Sect. 2.6 in the case of ﬁnite measure spaces Ω. In
order to do this, we make the following deﬁnition.
Deﬁnition 5.13 (Congruent families of tensors)
Let r ∈(0,1], and let (Θn
Ω;r) be
a collection of covariant n-tensors on Pr(Ω) (on Mr(Ω), respectively) for each
measure space Ω.
This collection is said to be a congruent family of n-tensors of regularity r if for
any congruent Markov kernel K : Ω′ →Ω we have
K∗
r Θn
Ω;r = Θn
Ω′;r.
As an example, we prove that the canonical tensors satisfy this invariance prop-
erty.

5.1
Congruent Embeddings and Sufﬁcient Statistics
271
Proposition 5.7
The canonical n-tensors τ n
Ω;r on Pr(Ω) (on Mr(Ω), respec-
tively) for 0 < r ≤1/n from Deﬁnition 3.10 form a congruent family of n-tensors of
regularity r.
Proof Let K : Ω →P(Ω′) be a κ-congruent Markov kernel w.r.t. the statistic κ :
Ω′ →Ω. Let μ ∈M(Ω) and μ′ := K∗μ, so that κ∗μ′ = μ.
We begin with the case r = 1/n, i.e., we show that K∗
1/nLn
Ω′ = Ln
Ω. For this
let Vi ∈TμM1/n(Ω) = S1/n(Ω,μ), so that Vi = φiμ1/n for some φi ∈Ln(Ω,μ).
Then
K∗
1/nLn
Ω′(V1,...,Vn) = Ln
Ω′

dK1/n

φ1μ1/n
,...,dK1/n

φnμ1/n
(5.47)
= Ln
Ω′

κ∗φ1μ′1/n,...,κ∗φnμ′1/n
= nn

Ω′ d

κ∗φ1μ′1/n
···

κ∗φnμ′1/n
= nn

Ω′ κ∗(φ1 ···φn) dμ′
= nn

Ω
φ1 ···φn dμ
= nn

Ω
d

φ1μ1/n
···

φnμ1/n
= Ln
Ω′(V1,...,Vn),
so that
K∗
1/nLn
Ω = Ln
Ω′.
(5.49)
But then, for any 0 < r ≤1/n
K∗
r τ n
Ω;r
(3.92)
= K∗
r

˜π1/nr∗Ln
Ω
(3.91)
=

˜π1/rnKr
∗Ln
Ω
(5.45)
=

K1/n ˜π1/rn∗Ln
Ω
(3.91)
=

π1/rn∗K∗
1/nLn
Ω
(5.49)
=

π1/rn∗Ln
Ω′
(3.92)
= τ n
Ω′;r,
showing the claim.
□
We can construct from these covariant n-tensors for any partition P = {P1,...,
Pl} ∈Part(n) of the index set {1,...,n}, as long as |Pi| ≤1/r for all i. Namely, we
deﬁne on Sr(Ω) the covariant n-tensor
τ P
Ω;r(V1,...,Vn) :=
l0
i=1
τ ni
Ω,r(VπP(i,1),...,VπP(i,ni)),
(5.50)

272
5
Information Geometry and Statistics
where πP : &
i∈I({i} × {1,...,ni}) −→{1,...,n} with ni := |Pi| is the map from
(2.79).
Just as in the case of a ﬁnite measure space Ω = I, the proof of Proposition 2.7
carries over immediately, so that the class of congruent families of n-tensors of
regularity r is closed under taking linear combinations, permutations and tensor
products.
Deﬁnition 5.14 The class of congruent families of covariant n-tensors of regularity
r which are algebraically generated by the canonical tensors {τ n
Ω,n ≤1/r} are the
congruent families of n-tensors on Mr(Ω) given by
 ˜Θn
Ω;r

μr =

P
aP
μ1/r
r

T V

τ P
Ω;r

μr,
(5.51)
where the sum is taken over all partitions P = {P1,...,Pl} ∈Part(n) with |Pi| ≤
1/r for all i, and where aP : (0,∞) →R are continuous functions, and by the ten-
sors on Pr(Ω) given by
Θn
Ω;r =

P
cPτ P
Ω;r,
(5.52)
where the sum is taken over all partitions P = {P1,...,Pl} ∈Part(n) with 1 <
|Pi| ≤1/r for all i, and where the cP ∈R are constants.
The proof of Proposition 2.9 carries over to show that this is the smallest
class of congruent families which is closed under taking linear combinations, per-
mutations and tensor products which contains the families of canonical tensors
{τ n
Ω;r : Ω a measure space} for 0 < r ≤1/n, justifying this terminology. In par-
ticular, all families of the form (5.51) and (5.52), respectively, are congruent.
Recall that in Theorem 2.3 we showed that for ﬁnite Ω, the only congruent fam-
ilies of covariant n-tensors on M+(Ω) and P+(Ω), respectively, are of the form
(5.51) and (5.52), respectively. This result can be generalized to the case of arbi-
trary Ω as follows.
Theorem 5.6 (Classiﬁcation of congruent families) For 0 < r ≤1, let (Θn
Ω;r) be a
family of covariant n-tensors on Mr(Ω) (on Pr(Ω), respectively) for each measure
space Ω. Then the following are equivalent:
(1) (Θn
Ω;r) is a congruent family of covariant n-tensors of regularity r.
(2) For each congruent Markov morphism K : I →P(Ω) for a ﬁnite set I, we have
K∗
r Θn
Ω;r = Θn
I;r.
(3) Θn
Ω;r is of the form (5.51) (of the form (5.52), respectively) for uniquely deter-
mined continuous functions aP (constants cP, respectively).
In the light of Deﬁnition 5.13, we may reformulate the equivalence of the ﬁrst
and the third statement as follows:

5.1
Congruent Embeddings and Sufﬁcient Statistics
273
Corollary 5.2 The space of congruent families of covariant n-tensors on Mr(Ω)
and Pr(Ω), respectively, is algebraically generated by the canonical n-tensors τ n
Ω;r
for n ≤1/r.
Proof of Theorem 5.6 We already showed that the tensors (5.51) and (5.52), respec-
tively, are congruent families, hence the third statement implies the ﬁrst. The ﬁrst
immediately implies the second by the deﬁnition of congruency of tensors. Thus, it
remains to show that the second statement implies the third.
We shall give the proof only for the families (Θn
Ω;r) of covariant n-tensors on
Mr(Ω), as the proof for the restricted families on Pr(Ω) is analogous.
Observe that for ﬁnite sets I, the space Mr
+(I) ⊆Sr(I) is an open subset and
hence a manifold, and the restrictions πα : Mr
+(I) →Mrα
+ (I) are diffeomorphisms
not only for α ≥1 but for all α > 0. Thus, given the congruent family (Θn
Ω;r), we
deﬁne for each ﬁnite set I the tensor
Θn
I :=

˜πr∗Θn
I;r
on M+(I).
Then for each congruent Markov kernel K : I →P(J) with I, J ﬁnite we have
K∗Θn
J
= K∗
˜πr∗Θn
J;r =

˜πrK∗
∗Θn
J;r
(5.45)
=

Kr ˜πr∗Θn
J;r =

˜πr∗K∗
r Θn
J;r =

˜πr∗Θn
I;r
= Θn
I .
Thus, the family (Θn
I ) on M+(I) is a congruent family of covariant n-tensors
on ﬁnite sets, whence by Theorem 2.3

Θn
I

μ =

P∈Part(n)
aP

∥μ∥T V

τ P
I

μ
for uniquely determined functions aP, whence on Mr
+(I),
Θn
I;r =

˜π1/r∗Θn
I
=

P∈Part(n)
aP
μ1/r
r

T V

˜π1/r∗τ P
I
=

P∈Part(n)
aP
μ1/r
r

T V

τ P
I;r.
By our assumption, Θn
I;r must be a covariant n-tensor on M(I), whence it must
extend continuously to the boundary of M+(I).
But by (3.94) it follows that τ ni
I;r has a singularity at the boundary of M(I),
unless ni ≤1/r. From this it follows that Θn
I;r extends to all of M(I) if and only if
aP ≡0 for all partitions P = {P1,...,Pi} where |Pi| > 1/r for some i.

274
5
Information Geometry and Statistics
Thus, Θn
I;r must be of the form (5.51) for all ﬁnite sets I. Let
Ψ n
Ω;r := Θn
Ω;r −

P
aP
μ1/r
r

T V

τ n
Ω;r
for the previously determined functions aP, so that (Ψ n
Ω;r) is a congruent family of
covariant n-tensors, and Ψ n
I;r = 0 for every ﬁnite I.
We assert that this implies that Ψ n
Ω;r = 0 for all Ω, which shows that Θn
Ω;r is of
the form (5.51) for all Ω, which will complete the proof.
To see this, let μr ∈Mr(Ω) and μ := μ1/r
r
∈M(Ω). Moreover, let Vj =
φjμr ∈Sr(Ω,μr), j = 1,...,n, such that the φj are step functions. That is, there
is a ﬁnite partition Ω = &
i∈I Ai such that
φj =

i∈I
φi
jχAi
for φi
j ∈R and mi := μ(Ai) > 0.
Let κ : Ω →I be the statistic κ(Ai) = {i}, and K : I →P(Ω), K(i) :=
1/miχAiμ. Then clearly, K is κ-congruent, and μ = K∗μ′ with μ′ := 	
i∈I miδi ∈
M+(I). Thus, by (5.43)
dμ′Kr

i∈I
φi
jmr
i δr
i

=

i∈I
φi
jχAiμr = φjμr = Vj,
whence if we let V ′
j := 	
i∈I φi
jmr
i δr
i ∈Sr(I), then
Ψ n
Ω;r(V1,...,Vn) = Ψ n
Ω;r

dKr

V ′
1

,...,dKr

V ′
n

= K∗
r Ψ n
Ω;r

V ′
1,...,V ′
n

= Ψ n
I;r

V ′
1,...,V ′
n

= 0,
since by the congruence of the family (Ψ n
Ω;r) we must have K∗
r Ψ n
Ω;r = Ψ n
I;r, and
Ψ n
I;r = 0 by assumption as I is ﬁnite.
That is, Ψ n
Ω;r(V1,...,Vn) = 0 whenever Vj = φjμr ∈Sr(Ω,μr) with step func-
tions φj. But the elements Vj of this form are dense in Sr(Ω,μr), hence the conti-
nuity of Ψ n
Ω;r implies that Ψ n
Ω;r = 0 for all Ω as claimed.
□
As two special cases of this result, we obtain the following, see also Remark 5.10
below.
Corollary 5.3 (Generalization of Chentsov’s theorem)
(1) Let (Θ2
Ω) be a congruent family of 2-tensors on P1/2(Ω). Then up to a constant,
this family is the Fisher metric. That is, there is a constant c ∈R such that for
all Ω,
Θ2
Ω = c gF .

5.1
Congruent Embeddings and Sufﬁcient Statistics
275
In particular, if (M,Ω,p) is a 2-integrable statistical model, then
p∗Θ2
Ω = c gM
is—up to a constant—the Fisher metric of the model.
(2) Let (Θ3
Ω) be a congruent family of 3-tensors on P1/3(Ω). Then up to a constant,
this family is the Amari–Chentsov tensor. That is, there is a constant c ∈R such
that for all Ω,
Θ3
Ω = c T.
In particular, if (M,Ω,p) is a 3-integrable statistical model, then
p∗Θ3
Ω = c TM
is—up to a constant—the Amari–Chentsov tensor of the model.
Corollary 5.4 (Generalization of Campbell’s theorem)
Let (Θ2
Ω) be a congru-
ent family of 2-tensors on M1/2(Ω). Then there are continuous functions a,b :
(0,∞) →R such that

Θ2
Ω

μ1/2(V1,V2) = a

∥μ∥T V

gF (V1,V2) + b

∥μ∥T V

τ 1
Ω;1/2(V1)τ 1
Ω;1/2(V2).
In particular, if (M,Ω,p) is a 2-integrable parametrized measure model, then
p∗
Θ2
Ω

ξ(V1,V2) = a
p(ξ)

T V

Ω
∂V1 logp(ξ) ∂V2 logp(ξ) dp(ξ)
+ b
p(ξ)

T V

∂V1
p(ξ)

T V

∂V2
p(ξ)

T V

.
While the above results show that for small n there is a unique family of con-
gruent n-tensors, this is no longer true for larger n. For instance, for n = 4 The-
orem 5.6 implies that any restricted congruent family of invariant 4-tensors on
Pr(Ω), 0 < r ≤1/4, is of the form
Θ4
Ω(V1,...,V4) = c0τ 4
Ω;r(V1,...,V4)
+ c1τ 2
Ω;r(V1,V2)τ 2
Ω;r(V3,V4)
+ c2τ 2
Ω;r(V1,V3)τ 2
Ω;r(V2,V3)
+ c3τ 2
Ω;r(V1,V4)τ 2
Ω;r(V2,V4),
so that the space of congruent families on Pr(Ω) is already 4-dimensional in this
case. Evidently, this dimension rapidly increases with n.
Remark 5.9 Let us make some remarks on the history of the terminology and the
results in this section. Morse–Sacksteder [189] called a statistical model a statistical
system, which does not have to be a differentiable manifold, or a set of dominated

276
5
Information Geometry and Statistics
probability measures, and it could be parametrized, i.e., deﬁned by a map from a set
to the space of all probability measures on a measurable space (Ω,A). They deﬁned
the notion of a statistical operation T , which coincides with the notion of Markov
kernel introduced in Deﬁnition 5.4 if T is “regular.” Thus their concept of a statis-
tical operation is slightly more general than our concept of a Markov kernel. The
latter coincides with Chentsov’s notion of a transition measure [62] and the notion
of a Markov transition in [25, Deﬁnition 4.1]. Morse–Sacksteder also mentioned
Blackwell’s similar notion of a stochastic transformation. In our present book, we
considerably extend the setting of our work [25] by no longer assuming the exis-
tence of a dominating measure for a Markov kernel. This extension incorporates the
previous notion of a Markov transition kernel which we now refer to as a dominated
Markov kernel.
Morse–Sacksteder deﬁned the notion of a statistical morphism as the map in-
duced by a statistical operation as in Deﬁnition 5.5. In this way, their notion of
a statistical morphism is slightly wider than our notion of a Markov morphism.
They also mentioned LeCam’s similar notion of a randomized map. The Morse–
Sacksteder notion of a statistical morphism has also been considered by Chentsov
in [65, p. 81].
Chentsov deﬁned a Markov morphism as a morphism that is induced by transition
measures of a statistical category [62]. Thus, his notion is again slightly wider than
our notion in the present book.
In [25] a Markov morphism is deﬁned as a map M(Ω,Σ) →M(Ω′,Σ′), which
is often induced by Markov transition (kernels). Also, the notion of a restricted
Markov morphism was introduced to deal with smooth maps (e.g., reparametriza-
tions) between parameter spaces of parametrized measure models.
One of the novelties in our consideration of Markov morphisms in the present
book is our extension of Markov morphisms to the space of signed measures which
is a Banach space with total variation norm.
In [16, p. 31] Amari–Nagaoka also considered Markov kernels as deﬁning trans-
formations between statistical models. This motivated the decomposition theorem
[25, Theorem 4.10]. The current decomposition theorem (Theorem 5.2) is formu-
lated in a slightly different way and with a different proof.
The notion of a congruent Markov kernel introduced in [26], see also [25], stems
from Chentsov’s notion of congruence in Markov geometry [65]. Chentsov consid-
ered only congruences in Markov geometry associated with ﬁnite sample spaces.
Remark 5.10
(1) Corollaries 5.3, 5.4 also follow from the proof of the Main Theorem in [25].
(2) In [188, §5] Morozova–Chentsov also suggested a method to extend the
Chentsov uniqueness theorem to the case of non-discrete measure spaces Ω.
Their idea is similar to that of Amari–Nagaoka in [16, p. 39], namely they
wanted to consider a Riemannian metric on inﬁnite measure spaces as the limit
of Riemannian metrics on ﬁnite measure spaces. They did not discuss a con-
dition under which such a limit exists. In fact they did not give a deﬁnition
of the limit of such metrics. If the limit exists they called it ﬁnitely generated.

5.2
Estimators and the Cramér–Rao Inequality
277
They stated that the Fisher metric is the unique (up to a multiplicative constant)
ﬁnitely generated metric that is invariant under sufﬁcient statistics. We also re-
fer the reader to [164] for a further development of the Amari–Nagaoka idea
and another generalization of the Chentsov theorem. There the Fisher metric is
characterized as the unique metric (up to a multiplicative constant) on statistical
models with the monotonicity property, see [164, Theorem 1.4] for a precise
formulation.
5.2 Estimators and the Cramér–Rao Inequality
In this section we assume that Ω is a measurable space and (P,Ω,p) is a 2-
integrable statistical model.
5.2.1 Estimators and Their Bias, Mean Square Error, Variance
The task of statistical inference is to ﬁnd an element in P that is most appropriate
for describing the observations made in Ω. In this sense, we formulate
Deﬁnition 5.15 An estimator is a map
ˆσ : Ω →P.
Thus, an estimator associates to every observed datum x in Ω a probability dis-
tribution from the class P . As P can also be considered as a family of product
measures on ΩN (N ∈N), we can also associate to every tuple (x1,...,xN) of
observations an element of P . Of course, in statistical practice, one might take a
large N, and one therefore also considers the limit N →∞, but since the theory for
any ﬁnite N is the same as for N = 1 in the current framework, we only consider
the latter here.
In other words, samples taken from Ω are assumed to obey some unknown prob-
ability distribution p0 which one wishes to estimate on the basis of those samples. In
parametric statistics, one is conﬁned to a particular class of probability distributions,
those parametrized by ξ ∈P , for some statistical model (P,Ω,p) as in Deﬁni-
tions 3.4, 3.7. That model may or may not contain the actual probability distribution
p0 from which the samples are taken, but in any case, one wishes to approximate
it as closely as possible by some p(ξ) for ξ ∈P . Of course, since the information
contained in the samples is necessarily incomplete and does not determine the dis-
tribution p0, we can never tell whether our estimate is correct. But we can develop
criteria for its likelihood to be correct, or at least to be the best among all the p(ξ)
from our statistical model. Again, the Fisher metric will enter in a crucial manner,
as it tells us how sensitively the parameter ξ to be estimated will depend on the

278
5
Information Geometry and Statistics
observations x. The corresponding quantitative statement is the Cramér–Rao bound
to be derived below.
In the sequel, we shall consider expectation values of such estimators. Classi-
cally (see, e.g., [48, p. 2]) P is parametrized by a parameter θ where θ belongs to an
open domain D in a Euclidean space V (in this case we say P is a parametric statis-
tical model), or θ belongs to an open domain D in an inﬁnite-dimensional Banach
space V (in this case we say P is a nonparametric or semiparametric model).1 If θ is
a 1–1 differentiable map, then θ yields coordinates of P . In particular θ−1 : P →V
is a V -valued function on P , which gives full information of P . As we have empha-
sized above, there are few statistical models where we have a 1–1 parametrization
θ : V ⊇D →P . So it is more convenient to consider partial information of P , an
observation of P (also called a feature of P ), which is a vector-valued function on P .
The last condition is imposed because we have to integrate over such observations
for computing expectations. We shall now introduce a general formal framework to
treat this issue.
Given a statistical model (P,Ω,p), we set
L2
P (Ω) :=

ψ ∈L2
Ω,p(ξ)

for all ξ ∈P

.
Let V be a topological vector space. We denote by V P the vector space of all V -
valued functions on P . A V -valued function will stand for the coordinate functions
on P , or in general, a feature of P (cf. [48]). Since for computing expectation val-
ues (and variance values) we need to integrate, we consider the following subspace
of V P
L2
ˆσ(P,V ) :=

ϕ ∈V P  l ◦ϕ ◦ˆσ ∈L2
P (Ω) for all l ∈V ′
.
(5.53)
(The functions l are simply coordinate functions, and if ϕ were real valued, we could
dispense of them.)
Thus, we have the sequence
Ω
ˆσ
−→P
ϕ
−→V
l
−→R,
and we now insert diffeomorphisms h : P →P in its middle. As p(ξ) is a measure
on Ω and ξ is simply a parameter, we obviously have

Ω
l ◦ϕ ◦h ◦ˆσ dp(ξ) =

Ω
l ◦ϕ ◦ˆσ dp(hξ)
for all l ∈V ′,
and we obtain immediately from (5.53) the following
Lemma 5.1 The subspace L2
ˆσ(P,V ) is invariant under the action of Diff(P).
1More precisely, a semiparametric model is a statistical model that has both parametric and non-
parametric components. (This distinction is only meaningful when the parametric component is
considered to be more important than the nonparametric one.)

5.2
Estimators and the Cramér–Rao Inequality
279
Any function ϕ ∈L2
ˆσ(P,V ) induces a V ′′-valued function ϕˆσ ∈(V ′′)P by com-
puting the expectation of the composition ϕ ◦ˆσ as follows

ϕˆσ(ξ),l

:= Ep(ξ)(l◦ϕ ◦ˆσ) =

Ω
l◦ϕ ◦ˆσ dp(ξ)
for all l ∈V ′.
(5.54)
Deﬁnition 5.16 The difference
bϕ
ˆσ := ϕˆσ −ϕ ∈

V ′′P
(5.55)
will be called the bias of the estimator ˆσ w.r.t. the map ϕ.
The bias compares ϕ(ξ) with the expectation value

Ω ϕ ◦ˆσ dp(ξ) w.r.t. the mea-
sure p(ξ) corresponding to ξ.
Deﬁnition 5.17
Given ϕ ∈L2
ˆσ(P,V ) the estimator ˆσ is called ϕ-unbiased if
ϕˆσ = ϕ, equivalently, bϕ
ˆσ = 0.
Thus, an estimator is unbiased if Ep(ξ)(ϕ ◦ˆσ) = ϕ(ξ). (The unbiasedness of
an estimator depends on the choice of “coordinates” ϕ, but we shall often simply
speak of an unbiased estimator.) In other words, when we assume that p is the
“correct” probability distribution describing the datum, the expected “coordinates”
ϕ of the estimator are those of p itself. Thus, if we assign weights to our data
according to p, then the averaged coordinates of the estimator should be those of p.
The equality ϕˆσ = ϕ does not depend on any speciﬁc observations. It rather says
that if the observations should happen to be such that if the hypothesis p is true,
then this hypothesis is conﬁrmed by the estimator.
For ϕ ∈V P and for l ∈V ′ we denote the composition l ◦ϕ by ϕl. This should be
considered as the lth coordinate of ϕ. Given ϕ ∈L2
ˆσ(P,V ), we deﬁne the ϕ-mean
square error of an estimator ˆσ : Ω →P to be the quadratic form MSEϕ
p(ξ)[ˆσ] on V ′
such that for each l,k ∈V ′ we have
MSEϕ
p(ξ)[ˆσ](l,k) := Ep(ξ)
-
ϕl ◦ˆσ −ϕl ◦p(ξ)

·

ϕk ◦ˆσ −ϕk ◦p(ξ)
,
.
(5.56)
We also deﬁne the variance of ˆσ w.r.t. ϕ to be the quadratic form V ϕ
p(ξ)[ˆσ] on V ′
such that for all l,k ∈V ′ we have
V ϕ
p(ξ)[ˆσ](l,k) := Ep(ξ)
-
ϕl ◦ˆσ −Ep(ξ)

ϕl ◦ˆσ

·

ϕk ◦ˆσ −Ep(ξ)

ϕk ◦ˆσ
,
. (5.57)
The RHSs of (5.56) and (5.57) are well-deﬁned, since ϕ ∈L2
ˆσ(P,V ).
We shall also use the following relation:
MSEϕ
p(ξ)[ˆσ](l,k) = V ϕ
p(ξ)[ˆσ](l,k) +

bϕ
ˆσ(ξ),l

·

bϕ
ˆσ(ξ),k

(5.58)
for all ξ ∈P and all l,k ∈V ′. Since for a given ξ ∈P the LHS and RHS of (5.58)
are symmetric bilinear forms on V ′, it sufﬁces to prove (5.58) in the case k = l. We

280
5
Information Geometry and Statistics
write
ϕl ◦ˆσ −ϕl ◦p(ξ) =

ϕl ◦ˆσ −Ep(ξ)

ϕl ◦ˆσ

+

Ep(ξ)

ϕl ◦ˆσ

−ϕl ◦p(ξ)

=

ϕl ◦ˆσ −Ep(ξ)

ϕl ◦ˆσ

+

bϕ
ˆσ(ξ),l

.
Taking into account that p(ξ) is a probability measure, we obtain
MSEϕ
p(ξ)[ˆσ](l,l) = V ϕ
p(ξ)[ˆσ](l,l) +

bϕ
ˆσ(ξ),l
2
+ 2

Ω

ϕl ◦ˆσ −Ep(ξ)

ϕl ◦ˆσ

·

bϕ
ˆσ(ξ),l

dp(ξ).
(5.59)
Since ⟨bϕ
ˆσ(ξ),l⟩does not depend on our integration variable x, the last term in the
RHS of (5.59) vanishes. As we have noted this proves (5.58).
A natural possibility to construct an estimator is the maximum likelihood method.
Here, given x, one selects that element of P that assigns the highest weight to x
among all elements in P . We assume for simplicity of the discussion that this ele-
ment of P is always unique. Thus, the maximum likelihood estimator
pML : Ω →P
is deﬁned by the implicit condition that, given some base measure μ(dx), so that we
can write any measure in P as p(x)μ(dx), i.e., represent it by the function p(x),
then
pML(x) = max
p∈P p(x).
(5.60)
pML does not depend on the choice of a base measure μ because changing μ only
introduces a factor that does not depend on p.
To understand this better, let us consider an exponential family
p(y;ϑ) = exp

γ (y) + fi(y)ϑi −ϕ(ϑ)

.
In fact, as we may put exp(γ (y)) into the base measure μ, we can consider the
simpler family
p(y;ϑ) = exp

fi(y)ϑi −ϕ(ϑ)

.
(5.61)
Now, for the maximum likelihood estimator, we want to choose ϑ as a function
of x such that
p

x;ϑ(x)

= max
ϑ
p(x;ϑ)
(5.62)
which yields the necessary condition
d
dϑ p(x;ϑ) = 0
at ϑ = ϑ(x),
(5.63)

5.2
Estimators and the Cramér–Rao Inequality
281
hence
fi(x) −∂ϕ(ϑ(x))
∂ϑi
= 0
for i = 1,...,n.
(5.64)
If we use the above expectation coordinates (see (4.74), (4.75))
ηi(ϑ) = ∂ϕ
∂ϑi (ϑ) = Ep(fi),
(5.65)
then, since ηi(ϑ(x)) = fi(x) by (5.64),
Ep

ηi

ϑ(x)

= Ep(fi) = ηi

ϑ(x)

.
(5.66)
We conclude
Proposition 5.8 The maximum likelihood estimator of an exponential family is un-
biased w.r.t. the expectation coordinates.
Of course, for the present analysis of the maximum likelihood estimator, the
exponential coordinates only played an auxiliary role, while the expectation coor-
dinates look like the coordinates of choice. Namely, we are considering a situation
where we have n independent observables fi whose values fi(x) for a datum x we
can record. The possible probability distributions are characterized by the expec-
tation values that they give to the observables fi, and so, we naturally uses those
values as coordinates for the probability distributions. Having obtained the values
fi(x) for the particular datum x encountered, the maximum likelihood estimator
then selects that probability distribution whose expectation values for the fi coin-
cide with those recorded values fi(x).
We should note, however, that the maximum likelihood estimator only evalu-
ates the density function p(x) at the observed point x. Therefore, depending on
the choice of the family P , it needs not be robust against observation errors for x.
In particular, one may imagine other conceivable selection criteria for an estima-
tor than maximum likelihood. This should depend crucially on the properties of the
family P . A tool to analyze this issue is the Cramér–Rao inequality that has been
ﬁrst derived in [67, 219].
5.2.2 A General Cramér–Rao Inequality
In this section, we shall formulate a general version of the Cramér–Rao inequality.
First we introduce the following regularity condition on estimators ˆσ : Ω →P .
Deﬁnition 5.18
Assume that ˆσ : Ω →P is an estimator and ϕ ∈L2
ˆσ(P,V ). We
call ˆσ a ϕ-regular estimator, if for all l ∈V ′ the function ξ →||ϕl ◦ˆσ||L2(Ω,p(ξ)) is
locally bounded, i.e., for all ξ0 ∈P
lim
ξ→ξ0
sup
ϕl ◦ˆσ

L2(Ω,p(ξ)) < ∞.

282
5
Information Geometry and Statistics
Lemma 5.2 (Differentiation under the integral sign)
Assume that ˆσ is ϕ-regular.
Then the V ′′-valued function ϕˆσ on P is Gateaux-differentiable. For all X ∈TξP
and for all l ∈V ′ we have
∂X(l ◦ϕˆσ)(ξ) =

Ω
l ◦ϕ ◦ˆσ · ∂X logp(ξ)p(ξ).
(5.67)
Proof Let X ∈TξP be a tangent vector to a curve γ (t) ⊆P , t ∈I, at γ (0). To prove
Lemma 5.2 it sufﬁces to show that
lim
ε→0
1
ε

Ω
l◦ϕ ◦ˆσ
-
p

γ (ε)

−p

γ (0)
,
=

Ω
l ◦ϕ ◦ˆσ · ∂X logp(ξ)p(ξ).
(5.68)
Here p(γ (ε)) −p(γ (0)) is a ﬁnite signed measure. Since p is a C1-map we have
p

γ (ε)

−p

γ (0)

=
 ε
0
∂X logp

γ (t)

p

γ (t)

dt.
(5.69)
Here, abusing notation, we also denote by X the vector ﬁeld along the curve γ (t)
whose value at γ (0) is the given tangent vector X ∈TξP .
Using Fubini’s theorem, noting that ϕl ◦ˆσ ∈L2
P (Ω) and (P,Ω,p) is 2-
integrable, we obtain from (5.69)
1
ε

Ω
l◦ϕ ◦ˆσ
-
p

γ (ε)

−p

γ (0)
,
= 1
ε
 ε
0

Ω
l ◦ϕ ◦ˆσ · ∂X logp

γ (t)

p

γ (t)

dt.
(5.70)
Claim The function
F(t) :=

Ω
ϕl ◦ˆσ · ∂X logp

γ (t)

p

γ (t)

is continuous.
Proof of Claim By Proposition 3.3, there is a ﬁnite measure μ0 dominating p(γ (t))
for all t ∈I. Hence there exist functions pt,qt ∈L1(Ω,μ0) such that
p

γ (t)

= ptμ0
and
dp

˙γ (t)

= ∂tp

γ (t)

= qtμ0.
Now we deﬁne
qt;2 :=
qt
2 p1/2
t
χ{pt>0} ∈L2(Ω,μ0),
so that dp1/2
˙γ (t)

= qt;2μ1/2
0 .
We also use the short notation ⟨f ;g⟩for the L2-product of f,g ∈L2(Ω,μ0). Then
we rewrite
F(t) =

ϕl ◦ˆσ · p1/2
t
;qt;2

.
(5.71)

5.2
Estimators and the Cramér–Rao Inequality
283
We compute

ϕl ◦ˆσ · p1/2
t
;qt;2

−

ϕl ◦ˆσ · p1/2
0
;q0;2

=

ϕl ◦ˆσ · p1/2
t
;qt;2 −q0;2

+

ϕl ◦ˆσ · p1/2
t
−ϕl ◦ˆσ · p1/2
0
;q0;2

.
(5.72)
We now proceed to estimate the ﬁrst term in the RHS of (5.72). By Hölder’s in-
equality, using notations in Sect. 3.2.3,

ϕl ◦ˆσ · p1/2
t
;qt;2 −q0;2

≤
ϕl ◦ˆσ · p1/2
t

2∥qt;2 −q0;2∥2
t→0
−−→0,
(5.73)
since
∥qt;2 −q0;2∥2 =
dp1/2
˙γ (t)

−dp1/2
˙γ (t)0
S1/2(Ω) →0
by the 2-integrability of p and hence the continuity of dp1/2, and since ∥ϕl ◦ˆσ ·
p1/2
t
∥2 = ∥ϕl ◦ˆσ∥L2(Ω,p(γ (t))) is bounded by the ϕ-regularity of ˆσ.
Next we proceed to estimate the second term in the RHS of (5.72). By Hölder’s
inequality,
ϕl ◦ˆσ ·

p1/2
t
−p1/2
0

1 ≤
ϕl ◦ˆσ

2
p1/2
t
−p1/2
0

2 ≤
ϕl ◦ˆσ

2
(pt −p0)1/2
2
=
ϕl ◦ˆσ

2∥pt −p0∥1/2
1
t→0
−−→0
(5.74)
as ∥pt −p0∥1 = ∥p(γ (t)) −p(γ (0))∥S(Ω) →0. Furthermore,
limsup
t→0
ϕl ◦ˆσ · p1/2
t

2 = limsup
t→0
ϕl ◦ˆσ

L2(Ω,p(γ (t))) < ∞
by the ϕ-regularity of ˆσ, which together with (5.74) implies that ϕl ◦ˆσ · p1/2
t
⇀
ϕl ◦ˆσ · p1/2
0
in L2(Ω,μ0) and therefore,

ϕl ◦ˆσ · p1/2
t
−ϕl ◦ˆσ · p1/2
0
;q0;2
 t→0
−−→0.
(5.75)
Using (5.71), (5.72), (5.73) and (5.75), we now complete the proof of Claim.
Continuation of the proof of Lemma 5.2 Since F(t) is continuous, (5.70) then yields
(5.68) for ε →0 and hence (5.67). This proves Lemma 5.2.
□
To derive a general Cramér–Rao inequality, we introduce several vector bundles
on a 2-integrable statistical model (P,Ω,p).
• As in [164] we denote the space of all pairs {(f,μ)|μ ∈M(Ω) and f ∈
Lk(Ω,μ)} by Lk
1(Ω). For a map p : P →M(Ω) we denote by p∗(Lk
1(Ω)) the
pull-back “ﬁbration” (also called the ﬁber product) P ×M(Ω) Lk
1(Ω).

284
5
Information Geometry and Statistics
• The quotient bundle ˆT P of the tangent bundle T P over the kernel of dp will be
called the essential tangent bundle. Likewise, the ﬁber ˆTξP of ˆT P is called the
essential tangent space. (Note that the dimension of this ﬁber at different points ξ
of P may vary.)
• As a consequence of the construction, the Fisher metric g descends to a non-
degenerate metric ˆg, which we shall call the reduced Fisher metric, on the ﬁbers
of ˆT P . Its inverse ˆg−1 is a well-deﬁned quadratic form on the ﬁbers of the dual
bundle ˆT ∗P , which we can therefore identify with ˆT P .
Deﬁnition 5.19 Let h be a Gateaux-differentiable function on P . A section P →
p∗(L2
1(Ω)), ξ →∇hξ ∈L2(Ω,p(ξ)), is called a pre-gradient of h if for all ξ ∈P
and X ∈TξP we have
∂X(h) = Ep

(∂X logp) · ∇hξ

.
Let us explain how the pre-gradient is related to the gradient w.r.t. the Fisher
metric. If the Fisher metric g is non-degenerate then there is a clear relation between
a pre-gradient ∇hξ and the gradient gradg(h)|ξ of a function h at ξ w.r.t. the Fisher
metric. From the relations deﬁning the gradient (B.22) and the Fisher metric (3.23),
∂X(h) = g

X,gradg(h)|ξ

= Ep

(∂X logp) · ∇hξ

we conclude that the image of the gradient gradg(h)|ξ via the (point-wise) linear
map TξP →L2(Ω,p(ξ)), X →∂X logp(ξ), can be obtained from its pre-gradient
by an orthogonal projection to the image d(dξ p)
dξ
(TξP) ⊆L2(Ω,p(ξ)). If the Fisher
metric is degenerate at ξ then we have to modify the notion of the gradient of a
function h, see (5.79) below.
Lemma 5.3 Let (P,Ω,p) be a 2-integrable statistical model and ϕ ∈L2
ˆσ(P,V ).
Assume that ˆσ is ϕ-regular. Then the section of the pullback ﬁbration p∗(L2
1(Ω))
deﬁned by ξ →(ϕl ◦ˆσ) −Ep(ξ)(ϕl ◦ˆσ) ∈L2(Ω,p(ξ)) is a pre-gradient of the
Gateaux-differentiable function ϕl
ˆσ for any l ∈V ′.
Proof Let X ∈TξP . By Lemma 5.2 we have
∂X

ϕl
ˆσ

=

Ω
∂X

ϕl ◦ˆσ dp(x;ξ)

=

Ω

ϕl ◦ˆσ

· ∂X logp(x;ξ)dp(x;ξ)
=

Ω

ϕl ◦ˆσ(x) −Ep(ξ)

ϕl ◦ˆσ

· ∂X logp(x;ξ)dp(x;ξ). (5.76)
Note that the last equality in (5.76) holds, since by (5.67)

Ω
∂X logp(x;ξ)dp(x;ξ) = 0.
Clearly (5.76) implies Lemma 5.3.
□

5.2
Estimators and the Cramér–Rao Inequality
285
We also see from this argument that a pre-gradient is only determined up to a
term that is L2-orthogonal to the image dp(TξP) ⊆L2(Ω,p(ξ)).
Lemma 5.3 implies
Corollary 5.5 Assume that ˆσ is ϕ-regular. If ∂X logp(ξ) = 0 ∈L2(Ω,p(ξ)), then
∂Xϕl
ˆσ = 0. Hence the differential dϕl
ˆσ descends to an element, also denoted by dϕl
ˆσ ,
in the dual space ˆT ∗P .
Proposition 5.9
Let (P,Ω,μ,p) be a ﬁnite-dimensional 2-integrable statistical
model and ϕ ∈L2
ˆσ(P,V ). Assume that ˆσ is ϕ-regular. Then for any l ∈V ′ and any
ξ ∈P we have
V ϕ
p(ξ)[ˆσ](l,l) := Ep(ξ)

ϕl ◦ˆσ −Ep(ξ)

ϕl ◦ˆσ
2 ≥
dϕl
ˆσ
2
ˆg−1(ξ).
Proof For any ξ ∈P the map
e : ˆTξP →L2
Ω,p(ξ)

,
X →∂X logp(·;ξ),
(5.77)
is an embedding. This embedding is an isometric embedding w.r.t. the Fisher met-
ric g on ˆTξP and the L2-metric in L2(Ω,p(ξ)), according to the deﬁnition of the
reduced Fisher metric. Thus we shall write e( ˆTξP, ˆg) to emphasize that e is an iso-
metric embedding. Since e( ˆTξP, ˆg) is a closed subspace in L2(Ω,p(ξ)), we have
the orthogonal decomposition
L2
Ω,p(ξ)

= e( ˆTξP, ˆg) ⊕

e( ˆTξP, ˆg)
⊥.
(5.78)
It is an important observation that we shall have to consider only the ﬁrst summand
in this decomposition. This will lead us to a more precise version of the Cramér–Rao
inequality. We shall now work this out. Denote by Πe( ˆTξ P) the orthogonal projection
L2(Ω,p(ξ)) →e( ˆTξP) according to the above decomposition. For a function ϕ ∈
L2
ˆσ(P,V ), let gradˆg ϕl
ˆσ ∈ˆT P denote the gradient of ϕl
ˆσ w.r.t. the reduced Fisher
metric ˆg on ˆT P , i.e., for all X ∈TξP we have
dϕl
ˆσ(X) = ˆg

pr(X),gradˆg ϕl
ˆσ

,
(5.79)
where pr denotes the projection TξP →ˆTξP . This gradient is well-deﬁned because
of Corollary 5.5. By Lemma 5.3 ϕl ◦ˆσ −Ep(ξ)(ϕl ◦ˆσ) is a pre-gradient of ϕl
ˆσ and
therefore, using (5.79), we have
Πe( ˆTξ P)

ϕl ◦ˆσ −Ep(ξ)

ϕl ◦ˆσ

= e

gradˆg ϕl
ˆσ

,
(5.80)
for ϕ ∈L2
ˆσ(P,V ). Using (5.57) and the decomposition (5.78), we obtain
V ϕ
p(ξ)[ˆσ](l,l) ≥
Πe( ˆTξ P)

ϕl ◦ˆσ −Ep(ξ)

ϕl ◦ˆσ
2
L2(Ω,p(ξ)).
(5.81)

286
5
Information Geometry and Statistics
Combining (5.81) with (5.80), we derive Proposition 5.9 immediately from the fol-
lowing obvious identity
gradˆg ϕl
ˆσ
2
ˆg(ξ) =
dϕl
ˆσ
2
ˆg−1(ξ).
□
We regard ||dϕl
ˆσ||2
ˆg−1(ξ) as a quadratic form on V ′ and denote the later one by
(ˆgϕ
ˆσ)−1(ξ), i.e.,
ˆgϕ
ˆσ
−1(ξ)(l,k) :=

dϕl
ˆσ,dϕk
ˆσ

ˆg−1(ξ).
Thus we obtain from Proposition 5.9
Theorem 5.7 (Cramér–Rao inequality)
Let (P,Ω,p) be a ﬁnite-dimensional 2-
integrable statistical model, ˆσ : Ω →P an estimator and ϕ ∈L2
ˆσ(P,V ). Assume
that ˆσ is ϕ-regular. Then the difference V ϕ
p(ξ)[ˆσ] −(ˆgϕ
ˆσ)−1(ξ) is a positive semi-
deﬁnite quadratic form on V ′ for any ξ ∈P .
This is the general Cramér–Rao inequality.
5.2.3 Classical Cramér–Rao Inequalities
(A) Assume that V is ﬁnite-dimensional and ϕ is a coordinate mapping. Then
ˆg = g,dϕl = dξl, and using (5.55), abbreviating bϕ
ˆσ as b, we write

gϕ
ˆσ
−1(ξ)(l,k) =

i
∂ξl
∂ξi + ∂bl
∂ξi

dξi,

j
∂ξk
∂ξj + ∂bk
∂ξj

dξj
 
g−1(ξ). (5.82)
Let B(ξ) be the linear transformation of V whose matrix coordinates are
B(ξ)l
k := ∂bl
∂ξk .
Using (5.82) we rewrite the Cramér–Rao inequality in Theorem 5.7 as follows:
Vξ[ˆσ] ≥

E + B(ξ)

g−1(ξ)

E + B(ξ)
T .
(5.83)
The inequality (5.83) coincides with the Cramér–Rao inequality in [53, Theo-
rem 1A, p. 147]. The condition (R) in [53, pp. 140, 147] for the validity of the
Cramér–Rao inequality is essentially equivalent to the 2-integrability of the (ﬁnite-
dimensional) statistical model with positive density function under consideration,
more precisely Borovkov ignores/excludes the points x ∈Ω where the density func-
tion vanishes for computing the Fisher metric. Borovkov also uses the ϕ-regularity
assumption, written as Eθ((θ∗)2) < c < ∞for θ ∈Θ, see also [53, Lemma 1,
p. 141] for a more precise formulation.

5.2
Estimators and the Cramér–Rao Inequality
287
(B) Assume V = R and ϕ is a coordinate mapping. Then
E + B(ξ) = 1 + b′
ˆσ,
(5.84)
where bˆσ is short for bϕ
ˆσ . Using (5.84) and (5.58), we derive from (5.83)
Eξ(ˆσ −ξ)2 ≥[1 + b′
ˆσ(ξ)]2
g(ξ)
+ bˆσ(ξ)2.
(5.85)
Note that (5.85) is identical with the Cramér–Rao inequality with a bias term in [66,
(11.290) p. 396, (11.323) p. 402].
(C) Assume that V is ﬁnite-dimensional, ϕ is a coordinate mapping and ˆσ is
ϕ-unbiased. Then the general Cramér–Rao inequality in Theorem 5.7 becomes the
well-known Cramér–Rao inequality for an unbiased estimator (see, e.g., [16, Theo-
rem 2.2, p. 32])
Vξ[ˆσ] ≥g−1(ξ).
(D) Our generalization of the Cramér–Rao inequality (Theorem 5.7) does not
require the non-degeneracy of the (classical) Fisher metric or the positivity of the
density function of statistical models. It was ﬁrst derived in [142], where we also
prove the general Cramér–Rao inequality for inﬁnite-dimensional statistical models
and analyze the ϕ-regularity condition in detail.
The point that we are making here is to consider the variance and the Fisher met-
ric as quadratic forms. In directions where dp vanishes, the Fisher metric vanishes,
and we cannot control the variance there, because observations will not distinguish
between measures when the measures don’t vary. In other directions, however, the
Fisher metric is positive, and therefore its inverse is ﬁnite, and this, according to our
general Cramér–Rao inequality, then leads to a ﬁnite lower bound on the variance.
In the next section, we’ll turn to the question when this bound is achieved.
5.2.4 Efﬁcient Estimators and Consistent Estimators
Proposition 5.8 asserts that the maximum likelihood estimator of an exponential
family is unbiased w.r.t. the expectation coordinates. We have also seen in (4.84)
that the covariance matrix of the expectation values of an exponential family yields
the Fisher metric. Therefore, by (5.66), the covariance matrix for the maximum
likelihood estimator of an exponential family is the inverse of the Fisher metric.
Therefore, the maximum likelihood estimator achieves equality for the Cramér–Rao
inequality.
Deﬁnition 5.20 Given ϕ ∈L2
ˆσ(P,V ), the estimator ˆσ : Ω →P is called ϕ-efﬁcient
if ˆσ is ϕ-regular and the Cramér–Rao inequality in Theorem 5.7 is an equality, i.e.,
V ϕ
p(ξ) = (gϕ
ˆσ)−1 for all ξ ∈P .

288
5
Information Geometry and Statistics
Thus, the maximum likelihood estimator of an exponential family is ϕ-efﬁcient.
Now we shall show that the converse statement is also valid, under a mild condi-
tion.
Theorem 5.8
Assume that (P,Ω,μ,p) is a connected ﬁnite-dimensional 2-
integrable statistical model with positive regular density function and p : P →
M(Ω) is an immersion. Let ˆσ : Ω →P be a ϕ-efﬁcient estimator for a V -valued
function ϕ on P where dimV = dimP . If the quadratic form (gϕ
ˆσ)−1 is non-
degenerate for all ξ and P has no boundary point, then P has an afﬁne structure in
which P is an exponential family.
Proof The idea of the proof of Theorem 5.8 consists in the following. Since ˆσ is
ϕ-efﬁcient, by (5.80) the pre-gradient of the function ϕl
ˆσ deﬁned by Lemma 5.3
coincides with the image of its gradient via the embedding e deﬁned in (5.77). Using
this observation we study the gradient lines of ϕl
ˆσ and show that these lines deﬁne
an afﬁne coordinate system on P in which we can ﬁnd an explicit representation of
an exponential family for P .
Assume that ˆσ : Ω →P is ϕ-efﬁcient. Since the quadratic form (gϕ
ˆσ)−1 is non-
degenerate for all ξ and dimV = dimP , the map V ′ →TξP, l →gradg ϕl(ξ), is
an isomorphism for all ξ ∈P . By (5.80), and recalling that ϕ ∈L2
ˆσ(Ω), for each
l ∈V ′ \ {0} we have
e

gradg ϕl
ˆσ(ξ)

= ϕl ◦ˆσ −Ep(ξ)

ϕl ◦ˆσ

∈L2
P (Ω).
(5.86)
We abbreviate gradϕl
ˆσ as X(l). Recalling the deﬁnition of the embedding e in (5.77),
we rewrite (5.86) as
∂X(l) logp(·;ξ) = e

gradg ϕl
ˆσ(ξ)

= ϕl ◦ˆσ −Ep(ξ)

ϕl ◦ˆσ

.
(5.87)
Note that ϕl ◦ˆσ does not depend on ξ and the last term on the RHS of (5.87),
the expectation value Ep(ξ)(ϕl ◦ˆσ), is differentiable in ξ by Lemma 5.2. Hence
the vector ﬁeld X(l) on P is locally Lipschitz continuous, since p : P →M(Ω)
is a C1-immersion. For a given point ξ ∈P we consider the unique integral curve
αl[ξ](t) ⊆P for X(l) starting at ξ; this curve satisﬁes
dαl[ξ](t)
dt
= X(l)
and
αl[ξ](0) = ξ.
(5.88)
We abbreviate
fl(x) := ϕl ◦ˆσ(x), ηl(ξ) := Ep(ξ)

ϕl ◦ˆσ

(5.89)
and set
ψl[ξ](t) :=
 t
0
ηl

αl[ξ](τ)

dτ.
(5.90)

5.2
Estimators and the Cramér–Rao Inequality
289
Using (5.88) and (5.89), the restriction of Eq. (5.87) to the curve αl[ξ](t) has the
following form:
d
dt logp

x,αl[ξ](t)

= fl(x) −ηl

αl[ξ](t)

.
(5.91)
Using (5.90), (5.88), and regarding (5.91) as an ODE for the function F(t) :=
logp(x,αl[ξ](t)) with the initial value F(0) = logp(x,ξ), we write the unique
solution to (5.91) as
logp

x,αl[ξ](t)

= fl(x) · t + logp(x,ξ) −ψl[ξ](t).
(5.92)
We shall now express the function ψl[ξ](t) in a different way. Using the equation

Ω
p

x,αl[ξ](t)

dμ = 1
for all t,
which by (5.92) is equivalent to

Ω
exp

fl(x) · t −ψl[ξ](t)

· p(x,ξ)dμ = 1,
we obtain
ψl[ξ](t) = log

Ω
exp

fl(x) · t

· p(x,ξ)dμ.
(5.93)
Now we deﬁne a map Φ : V ′ × P →P by
Φ(l,ξ) = αl[ξ](1).
We compute
d(0,ξ)Φ(l,0) = d
dt |t=0αtl[ξ](1) = X(l)|ξ.
(5.94)
The map Φ may not be well deﬁned on the whole space V ′ × P . For a given ξ ∈P
we denote by V ′(ξ) the maximal subset in V ′ where the map Φ(l,ξ) is well-deﬁned
for l ∈V ′(ξ). Since ξ is an interior point of P , V ′(ξ) is open.
Lemma 5.4 Given a point ξ ∈P , l ∈V ′(ξ), assume that l′ ∈V ′(Φ(l,ξ)) and l +
l′ ∈V ′(ξ). Then we have
Φ

l′,Φ(l,ξ)

= Φ

l′ + l,ξ

.
(5.95)
Proof Choose a small neighborhood U(ξ) ⊆P of ξ such that the restriction of p
to U(ξ) is an embedding. Now we choose V ′
ε ⊆V ′(ξ) such that the LHS and the
RHS of (5.95) belong to p(U(ξ)) for any l,l′ ∈V ′
ε such that l + l′ ∈V ′
ε. Since

290
5
Information Geometry and Statistics
p : U(ξ) →M(Ω) is an embedding, to prove (5.95), it sufﬁces to show that for all
x ∈Ω we have
logp

x,Φ

l′,Φ(l,ξ)

= logp

x,Φ

l′ + l,ξ

.
(5.96)
Using (5.92) we have
logp

x,Φ

l′,Φ(l,ξ)

= logp

x,Φ(l,ξ)

+ fl′(x) + N

l′,l,ξ

,
(5.97)
where N(l′,l,ξ) is the normalizing factor such that p in the LHS of (5.97) is a
probability density, cf. (5.93). (This factor is also called the cumulant generating
function or the log-partition function, see (4.69).) Expanding the RHS of (5.97),
using (5.92), we obtain
logp

x,Φ

l′,Φ(l,ξ)

= logp(x,ξ) + fl(x) + N(l,ξ) + fl′(x) + N

l′,l,ξ

.
(5.98)
Since fl(x) = ⟨l,ϕ ◦ˆσ(x)⟩, we obtain
fl(x) + fl′(x) = fl+l′(x).
Using this, we deduce from (5.98) that
logp

x,Φ

l′,Φ(l,ξ)

= logp(x,ξ) + fl+l′(x) + N1

l,l′,ξ

,
(5.99)
where N1(l,l′,ξ) is the log-partition function. By (5.92) the RHS of (5.99) coin-
cides with the RHS of (5.96). This proves (5.96) and hence (5.95). This proves
Lemma 5.4 for the case l,l′,l + l′ ∈V ′
ε ⊆V ′(ξ).
Now assume that l,l′ ∈V ′(ε) but l +l′ ∈V ′(ξ)\V ′
ε. Let c be the supremum of all
positive numbers c′ in the interval [0,1] such that (5.95) holds for c′ · l,c′ · l′. Since
Φ is continuous, it follows that c is the maximum. We just proved that (5.95) holds
for an open small neighborhood of any point ξ. Hence c = 1, i.e., (5.95) holds for
any l,l′ ∈V ′(ε). Repeating this argument, we complete the proof of Lemma 5.4. □
Completion of the proof of Theorem 5.8 Choosing a local isomorphism
Φξ : V ′ →P, l →Φ(l,ξ)
around a given point ξ ∈P , using (5.89), we conclude from (5.92) that
p

x,Φξ(l)

= p(x,ξ) · exp

l ◦ϕ ◦ˆσ(x)

· ˜N(l,ξ),
(5.100)
where ˜N(l,ξ) is the log-partition function. From (5.100), and remembering that
the RHS of (5.87) does not vanish, we conclude that the map Φξ : V ′(ξ) →P is
injective. By (5.94), Φξ is an immersion at ξ. Hence Φξ deﬁnes afﬁne coordinates
on V (ξ) where by (5.100) Φξ(V ′(ξ)) is represented as an exponential family.
We shall show that Φξ can be extended to ΦD on some open subset D ⊆V ′
such that ΦD provides global afﬁne coordinates on P = ΦD(D) in which P is
represented as an exponential family.

5.2
Estimators and the Cramér–Rao Inequality
291
Assume that ξ′ ∈V ′(ξ) \ {0}. Denote by V ′(ξ′) + ξ′ the translation of the subset
V (ξ′) in the afﬁne space V ′ by the vector ξ′. Let W := V ′(ξ) ∪[V ′(ξ′) + ξ′]. We
extend Φξ to ΦW on W via
ΦW(l) = Φξ(l)
for l ∈V ′(ξ),
(5.101)
ΦW

l′ + ξ′
= Φξ′
l′
for l′ ∈V ′
ξ′
.
(5.102)
We claim that ΦW is well deﬁned. It sufﬁces to show that if l′ + ξ′ ∈V ′(ξ) then
Φξ

l′ + ξ′
= Φξ′
l′
.
(5.103)
Clearly, (5.103) is a consequence of Lemma 5.4. This shows that ΦW provides a
global afﬁne coordinate system on ΦW(W ′) ⊆P . Repeating this argument, we pro-
vide a global afﬁne coordinate system on P provided by the map ΦD : D →P .
Finally, we need to show that P is an exponential family in the constructed afﬁne
coordinates, i.e., for any θ ∈D we have
p

x,ΦW(θ)

= p(x,ξ) · exp

θ,ϕ ◦ˆσ(x)

· exp

N(θ)

,
(5.104)
where N(θ) is the log-partition function. Representing
ΦW(θ) = Φ

ln,Φ

ln1,...,Φ(l1,ξ)

and using Lemma 5.4 and (5.100), we obtain immediately (5.104). This completes
the proof of Theorem 5.8.
□
Remark 5.11
(1) Under the additional condition that ϕ deﬁnes a global coordinate system on P ,
the second part of Theorem 1A in [53, p. 147] claims that P is a generalized
exponential family. Borovkov used a different coordinate system for P , hence
the representation of his generalized exponential system is different from ours.
Under the additional conditions that ϕ deﬁnes a global coordinate system on P ,
ϕ is unbiased and P is a curved exponential family ([16, Theorem 2.5, p. 36]).
Theorem 3.12 in [16, p. 67] also states that P is an exponential family.
(2) We refer the reader to [16, §3.5], [159, §4], [39, §9.3] for discussions on the
existence and uniqueness of unbiased efﬁcient estimators on ﬁnite-dimensional
exponential families, and to [143] for a discussion on the existence of unbiased
efﬁcient estimators on singular statistical models, which shows the optimality of
our general Cramér–Rao inequality, and the existence of biased efﬁcient estima-
tors on inﬁnite-dimensional exponential families as well as for a generalization
of Theorem 5.8.
Corollary 5.6 Assume the condition of Theorem 5.8. If ˆσ is an (ϕ-)unbiased esti-
mator, then ˆσ is the MLE, i.e.,
∂V log(x;ξ)|ξ=ˆσ(x) = 0,
for all x ∈Ω and all V ∈Tˆσ(x)P .

292
5
Information Geometry and Statistics
Proof Let V = X(l). From (5.87) we have
∂X(l) logp(x;ξ)|ξ=ˆσ(x) = ϕl ◦ˆσ(x) −Ep(ˆσ(x))

ϕl ◦ˆσ

=

l,bϕ
ˆσ

ˆσ(x)

= 0
since ˆσ is ϕ-unbiased. Since {X(l)|l ∈V ′} form a basis of TξP we obtain immedi-
ately Corollary 5.6.
□
Of course, in general when one selects an estimator, the desire is to make the
covariance matrix as small as possible. The Cramér–Rao inequality tells us that
the lower bound gets weaker if the Fisher metric tensor g becomes larger, i.e., if
our variations of the parameters in the family P generate more information. Thus,
the (un)reliability of the estimates provided by the estimator ˆσ, i.e., the expected
deviation (this is not a precise mathematical term, but only meant to express the
intuition) between the estimate and its expectation is controlled from below by the
inverse of the information content of our chosen parametrized family of probability
distributions. If, as in the maximum likelihood estimator, we choose the parameter
ϑ so as to maximize logp(x;ϑ), then the Fisher metric tells us by which amount
this expression is expected to change when we vary this parameter near its optimal
value. So, if the Fisher information is large, this optimal value is well distinguished
from nearby values, and so, our estimate can be expected to be reliable in the sense
that the lower bound in the Cramér–Rao inequality becomes small.
In the remainder of this section we discuss the notion of consistency in the
asymptotic theory of estimation, in which the issue is the performance of an es-
timate in the limit N →∞. Asymptotic theory is concerned with the effect of
taking repeated samples from Ω. That means for a sequence of increasing natu-
ral numbers N we replace Ω by ΩN and the base measure by the product mea-
sure μ. Clearly, (P,ΩN,μN,pN) is a 2-integrable statistical model if (P,Ω,μ,p)
is a 2-integrable statistical model. Thus, as we have already explained in the be-
ginning of this section, our considerations directly apply. In the asymptotic the-
ory of estimation the important notion is consistency instead of unbiasedness. Let
{ˆσN : ΩN →P| N = 1,2,...} be a sequence of estimators. We assume that there
exists a number N0 ∈N such that the following space
L2
∞(P,V ) =
∞
9
i=N0
L2
ˆσi(P,V )
is non-empty. Let ϕ ∈L2
∞(P,V ). A sequence of estimators {ˆσN : ΩN →P | N =
1,2,...} is called ϕ-consistent if, for all ξ ∈P , for all l ∈V ′ and for all ε > 0, we
have
lim
N→∞p(ξ)

xN ∈ΩN  l◦˜bϕ|ˆσN

xN,ξ
 > ε

= 0,
(5.105)
where
˜bϕ|ˆσN (x,ξ) := ϕ

ˆσN(x)

−ϕ(ξ)

5.2
Estimators and the Cramér–Rao Inequality
293
is called the error function. In other words, for each ξ ∈P and for each l ∈V ′ the ϕl-
coordinates of the value of the estimators ˆσN converge in probability associated with
p(ξ) to the ϕl-coordinates of its true value ξ. To make the notion of convergence in
probability of ˆσN precise, we need the notion of an inﬁnite product of probability
measures on the inverse limit Ω∞of the Cartesian product ΩN. The sigma algebra
on Ω∞is generated by the subsets π−1
N (S), where πN : Ω∞→ΩN is the natural
projection and S is an element in the sigma-algebra in ΩN. Let μ be a probability
measure on Ω. Then we denote by μ∞the inﬁnite product of μ on Ω∞such that
μ∞(π−1
N (S)) = μN(S). We refer the reader to [87, §8.2], [50, §3.5] for more details.
Within this setting, σN is regarded as a map from Ω∞to P via the composition with
the projection πN : Ω∞→ΩN.
If ϕ deﬁnes a global coordinate system, we call a sequence of ϕ-consistent es-
timators simply consistent estimators. Important examples of consistent estimators
are maximum likelihood estimators of a ﬁnite-dimensional compact embedded 2-
integrable statistical model with regular density function [53, Corollary 1, p. 192].
Remark 5.12 Under the consistency assumption for a sequence of estimators ˆσN :
ΩN →P and using an additional regularity condition on σN, Amari derived the
asymptotic Cramér–Rao inequality [16, (4.11), §4.1] from the classical Cramér–
Rao inequality. The asymptotic Cramér–Rao inequality belongs to the ﬁrst-order
asymptotic theory of estimation, where the notion of asymptotic efﬁciency is closely
related to the asymptotic Cramér–Rao inequality. We refer the reader to [48, 53] for
the ﬁrst-order asymptotic theory of estimation, in which the maximum likelihood
method also plays an important role. We refer the reader to [7–9, 16] for the higher-
order asymptotic theory of estimation.

Chapter 6
Fields of Application of Information Geometry
Information geometry has many ﬁelds of application. The efﬁciency of the
information-geometric approach has been particularly demonstrated in terms of the
natural gradient method [12]. This method greatly improves classical gradient-
based algorithms by exploiting the natural geometry of the Fisher metric. A com-
prehensive overview of applications within the ﬁelds of neural networks, machine
learning, and signal processing is provided in [11]. In this chapter, we aim to com-
plement and extend these by various research directions that we consider worth-
while. The selection of these topics is motivated by and based on the authors’
research interests and works, and is by no means complete. The aim is to high-
light important problems that can be approached from the information-geometric
perspective and to stimulate further work within these research directions.
6.1 Complexity of Composite Systems
The most famous quote about complex systems is attributed to Aristotle and says
“The whole is more than the sum of its parts.”1 Complex systems are systems where
the collective behavior of their parts entails emergence of properties that can hardly,
if at all, be inferred from properties of the parts. Several different approaches have
been proposed to formalize this idea or some alternative intuitive characterization of
complexity. They usually capture some important aspects, but leave out others. Here,
building upon [18, 19, 22, 28, 32, 36, 179], we develop an information-geometric
approach that provides a formalization of Aristotle’s insight and generalizes and
uniﬁes several of the earlier proposals. Before doing so, however, let us ﬁrst give a
rough classiﬁcation of existing approaches to formal complexity concepts. We can
discern the following three guiding ideas:2 Complexity is the ...
1. ... minimal effort that one has to make, or minimal resources that one has to use,
in order to describe or generate an object. Examples are: algorithmic complexity
1In fact, this is an abbreviation of a longer reasoning of Aristotle [17] in his Metaphysics, Vol. VII,
1041b, 11ff and Vol. VIII 1045a, 9ff.
2Note that the references to particular theories will by no means be complete.
© Springer International Publishing AG 2017
N. Ay et al., Information Geometry, Ergebnisse der Mathematik und ihrer Grenzgebiete.
3. Folge / A Series of Modern Surveys in Mathematics 64,
DOI 10.1007/978-3-319-56478-4_6
295

296
6
Fields of Application of Information Geometry
(Kolmogorov [152]; Chaitin [58]; Solomonoff [239]), computational complexi-
ties [125, 211], entropy or entropy rate (Shannon [235]; Kolmogorov [151, 168];
Sinai [237]).
2. ... minimal effort that one has to make, or minimal resources that one has to
use, in order to describe or generate regularities or structure of an object. Ex-
amples are: Kolmogorov minimal sufﬁcient statistics and related notions (see
[168]), stochastic complexity (Rissanen [226]), effective complexity (Gell-Mann
and Lloyd [104, 105]; Ay, Müller, and Szkoła [30, 31]), statistical complex-
ity (Crutchﬁeld and Young [69]; Shalizi and Crutchﬁeld [234]), excess en-
tropy (Grassberger [111]; Shalizi and Crutchﬁeld [234]; Bialek, Nemenman, and
Tishby [47]).
3. ... the extent to which an object, as a whole, is more than the sum of its parts.
The extent to which the whole cannot be understood by analysis of the system
parts alone, but only by taking into account their interactions.
The starting point of our approach will be the third concept, the one closest to Aris-
totle’s original insight, which we shall formalize in a geometric manner. This will
allow us not only to put the theory of [18] (Sect. 6.1.2) into perspective, but also
to relate various known measures of complexity to a general class of information-
geometric complexity measures. In particular, we can interpret the work of Tononi,
Sporns, and Edelman [245] and the excess entropy geometrically (Sect. 6.1.3.3).
Finally, we study the complexity of interacting stochastic processes and relate it to
notions of information ﬂow, such as Kolmogorov–Sinai entropy, transfer entropy,
and directed information (Sect. 6.1.4).
6.1.1 A Geometric Approach to Complexity
In order to quantify the extent to which the system is more than the sum of its parts,
we start with a few general arguments sketching a geometric picture of this very
broad deﬁnition of complexity which is illustrated in Fig. 6.1. Assume that we have
given a set S of systems to which we want to assign a complexity measure. For any
system S ∈S, we have to compare S with the sum of its parts. This requires a no-
tion of system parts, which clearly depends on the context. The speciﬁcation of the
parts of a given system is a fundamental problem in systems theory. We do not ex-
plicitly address this problem here but consider several choices of system parts. Each
collection of system parts that is assigned to a given system S may be an element
of a set D that formally differs from S. We interpret the corresponding assignment
D : S →D as a reduced description of the system in terms of its parts. Having the
parts D(S) of a system S, we have to reconstruct S, that is, take the “sum of the
parts”, in order to obtain a system that can be compared with the original system.
The corresponding construction map is denoted by C : D →S. The composition
P(S) := (C ◦D)(S)
then corresponds to the sum of parts of the system S, and we can compare S with
P(S). If these two systems coincide, then we would say that the whole equals the

6.1
Complexity of Composite Systems
297
Fig. 6.1 Illustration of the projection P : S →N that assigns to each system S the sum of its parts,
the system P(S). The image of this projection constitutes the set N of non-complex systems
sum of its parts and therefore does not display any complexity. We refer to these
systems as non-complex systems and denote their set by N , that is, N := {S ∈
S : P(S) = S}. Note that the utility of this idea within complexity theory strongly
depends on the concrete speciﬁcation of the maps D and C. As mentioned above, the
right deﬁnition of D incorporates the identiﬁcation of system parts which is already
a fundamental problem.
The above representation of non-complex systems is implicit, and we now derive
an explicit representation. Obviously, we have
N ⊆im(P) =

P(S) : S ∈S

.
With the following natural assumption, we even have equality, which provides an
explicit representation of non-complex systems as the image of the map P. We as-
sume that the construction of a system from a set of parts in terms of C does not
generate any additional structure. More formally,
(D ◦C)

S′
= S′
for all S′ ∈D.
This implies
P2 = (C ◦D) ◦(C ◦D)
= C ◦(D ◦C) ◦D
= C ◦D
= P.
Thus, the above assumption implies that P is idempotent and we can interpret it as
a projection. This yields the explicit representation of the set N of non-complex
systems as the image of P.
In order to have a quantitative theory of complexity, one needs a divergence D :
S ×S →R which allows us to quantify how much the system S deviates from P(S),
the sum of its parts. We assume that D is a divergence in the sense that D(S,S′) ≥0,

298
6
Fields of Application of Information Geometry
and D(S,S′) = 0 if and only if S = S′ (see condition (4.97)). In order to ensure
compatibility with P, one has to further assume that D satisﬁes
C(S) := D

S, P(S)

= inf
S′∈N D

S,S′
.
(6.1)
Obviously, the complexity C(S) of a system S vanishes if and only if the system S
is an element of the set N of non-complex systems.
6.1.2 The Information Distance from Hierarchical Models
6.1.2.1 The Information Distance as a Complexity Measure
In this section we want to make precise and explore the general geometric idea
described above. We recall the formal setting of composite systems developed in
Sect. 2.9. In order to have a notion of parts of a system, we assume that the system
consists of a ﬁnite node set V and that each node v can be in ﬁnitely many states Iv.
We model the whole system by a probability measure p on the corresponding prod-
uct conﬁguration set IV =×v∈V Iv. The parts are given by marginals pA where A
is taken from a hypergraph S (see Deﬁnition 2.13). The decomposition of a proba-
bility measure p into parts, corresponding to D, is given by the map p →(pA)A∈S,
which assigns to p the family of its marginals. The reconstruction of the system from
this partial information, which corresponds to C, is naturally given by the maximum
entropy estimate ˆp of p. This deﬁnes a projection C ◦D = πS : p →ˆp onto the
hierarchical model ES which plays the role of the set N of non-complex systems.
If the original system p coincides with its maximum entropy estimate ˆp = πS(p)
then p is nothing but the “sum of its parts” and is interpreted as a non-complex
system.
In order to quantify complexity, we need a deviation measure that is compatible
with the maximum entropy projection πS. Information geometry suggests to use the
KL-divergence, and we obtain the following measure of complexity as one instance
of (6.1), which we refer to as S-complexity:
DKL(p ∥ES) := inf
q∈ES
DKL(p ∥q) = DKL

p
πS(p)

.
(6.2)
In many examples, this measure of complexity can be expressed in terms of basic
information-theoretic quantities (see Example 6.1). Let us recall the deﬁnition of
those quantities and corresponding rules, which we require for our derivations be-
low. Assume that we have a distribution p on the whole conﬁguration set IV , and
consider the canonical projection XA : IV →IA which simply restricts any con-
ﬁguration (iv)v∈V to the sub-conﬁguration iA = (iv)v∈A. We deﬁne the entropy or
Shannon entropy (see (2.179)) of XA with respect to p as
Hp(XA) := −

iA∈IA
p(iA)logp(iA).
(6.3)

6.1
Complexity of Composite Systems
299
(With the strict concavity of the Shannon entropy, as deﬁned by (2.179), and the
linearity of the map p →(p(iA))iA∈IA, we obtain the concavity of the Shannon
entropy Hp(XA) as a function of p.) With a second subset B of V , we can deﬁne
the conditional entropy of XB given XA (with respect to p):
Hp(XB |XA) := −

iA∈IA
p(iA)

iB∈IB
p(iB|iA)logp(iB|iA).
(6.4)
For a sequence A1,...,An of subsets of V , we have the following decomposition
of the joint entropy in terms of conditional entropies:
Hp(XA1,...,XAn) =
n

k=1
Hp(XAk |XA1,XA2,...,XAk−1).
(6.5)
It follows from the concavity of the Shannon entropy that for two subsets A and
B of V , the conditional entropy Hp(XB |XA) is smaller than or equal to the entropy
Hp(XB). This shows that the following important quantity, the mutual information
of XA and XB (with respect to p), is non-negative:
Ip(XA;XB) := Hp(XB) −Hp(XB |XA)
=

iA∈IA,iB∈IB
p(iA,iB)log p(iA,iB)
p(iA)p(iB).
(6.6)
With a third subset C of V , we have the conditional mutual information of XA and
XB given XC (with respect to p):
Ip(XA;XB |XC) := Hp(XB |XC) −Hp(XB |XA,XC)
=

iA∈IA,iB∈IB
p(iA,iB|iC)log
p(iA,iB|iC)
p(iA|iC)p(iB|iC).
(6.7)
Clearly, the conditional mutual information (6.7) reduces to the mutual information
(6.6) for C = ∅. The (conditional) mutual information is symmetric in A and B,
which follows directly from the representation
Ip(XA;XB |XC) = Hp(XA |XC) + Hp(XB |XC) −Hp(XA,XB |XC).
(6.8)
Given a sequence A0,A1,...,An of subsets of V , we have the following chain rule:
Ip(XA1,...,XAn;XA0) =
n

k=1
Ip(XAk;XA0 |XA1,XA2,...,XAk−1).
(6.9)
We can easily extend the (conditional) mutual information to more than two vari-
ables XA and XB using its symmetric version (6.8). More precisely, for a sequence

300
6
Fields of Application of Information Geometry
A0,A1,...,An of subsets of V , we deﬁne the conditional multi-information as
Ip

n
>
k=1
XAk
 XA0

:= Ip(XA1;...;XAn |XA0)
:=
n

k=1
Hp(XAk |XA0) −Hp(XA1,...,XAn |XA0).
(6.10)
Here, we prefer the notation with the wedge “∧” whenever the ordering of the sets
A1,...,An is not naturally given. In the case where A0 = ∅, this quantity is known
as multi-information:
Ip

n
>
k=1
XAk

:= Ip(XA1;...;XAn)
:=
n

k=1
Hp(XAk) −Hp(XA1,...,XAn).
(6.11)
We shall frequently use the multi-information for the partition of the set V into its
single elements. This quantity has a long history and appears in the literature under
various names. The term “multi-information” was introduced by Perez in the late
1980s (see [241]). The multi-information has been used by Studený and Vajnarová
[242] in order to study conditional independence models (see Remark 2.4).
After having introduced basic information-theoretic quantities, we now discuss a
few relations to the S-complexity (6.2). A ﬁrst relation is given by the fact that the
KL-divergence from the maximum entropy estimate πS(p) can be expressed as a
difference of entropies (see Lemma 2.11):
DKL

p
πS(p)

= HπS(p)(XV ) −Hp(XV ).
(6.12)
We can evaluate DKL(p ∥ES) if we know the maximum entropy estimate of p
with respect to S. In the following example, we use formula (6.12), in combination
with Lemma 2.13, in order to evaluate DKL(p ∥ES) for various simple cases.
Example 6.1
(1) Let us begin with the most simple hypergraph, the one that contains the empty
subset of V as its only element, that is, S = {∅}. In that case, the hierarchi-
cal model (2.207) consists of the uniform distribution on IV , and by (6.12) we
obtain (cf. (2.178))
DKL(p ∥E{∅}) = log|IV | −Hp(XV ).
(6.13)
(2) We now consider the next simple case, where S consists of only one subset A
of V . In that case, the maximum entropy estimate of p with respect to S is

6.1
Complexity of Composite Systems
301
given by
π{A}(p)(iA,iV \A) = p(iA)
1
|IV \A|.
This directly implies
DKL(p ∥E{A}) = log|IV \A| −Hp(XV \A |XA).
(6.14)
Note that this (6.14) reduces to (6.13) for A = ∅.
(3) Consider three disjoint subsets A, B, and C that satisfy V = A∪B ∪C, and de-
ﬁne S = {A∪C,B ∪C}. Then, by Lemma 2.13, the maximum entropy estimate
of a distribution p with respect to S is given by
π{A∪C,B∪C}(p)(iA,iB,iC) = p(iC)p(iA|iC)p(iB|iC).
(6.15)
The conditional distributions p(iA|iC) and p(iB|iC) on the RHS of (6.15) are
only deﬁned for p(iC) > 0. In line with a general convention, we choose ex-
tensions of the involved conditional probabilities to the cases where p(iC) = 0
and denote them by the same symbols. As we multiply by p(iC), the joint dis-
tribution is independent of this choice. In what follows, we shall apply this
convention whenever it is appropriate. Equation (6.15) implies
DKL(p ∥E{A∪C,B∪C}) = Ip(XA;XB |XC).
(6.16)
(4) Consider a sequence A0,A1,...,An of disjoint subsets of V that satisﬁes
/n
k=0 Ak = V , and deﬁne S = {Ak ∪A0 : k = 1,...,n}. Note that, for A0 = ∅,
this hypergraph reduces to the one considered in Example 2.6(1). The maximum
entropy estimate of p with respect to S is given as
πS(p)(iA0,iA1,...,iAn) = p(iA0)
n
0
k=1
p(iAk|iA0),
(6.17)
which extends the formula (6.15). As the distance from the hierarchical model
ES we obtain the conditional multi-information (6.10):
DKL(p ∥ES) = Ip(XA1;...;XAn |XA0).
(6.18)
(5) The divergence from the hierarchical model E(k) of Example 2.6(2) quantiﬁes
the extent to which p cannot be explained in terms of interactions of maxi-
mal order k. Stated differently, it quantiﬁes the extent to which the whole is
more than the sum of its parts of size k. In general, this quantity can be easily
evaluated only for k = 1. In that case, we recover the multi-information of the
individual variables Xv, v ∈V . We shall return to this example in a moment.

302
6
Fields of Application of Information Geometry
6.1.2.2 The Maximization of the Information Distance
We now wish to consider the maximization of the function
DKL(·∥ES) : P(IV ) →R,
p →DKL

p
πS(p)

,
(6.19)
which we have motivated as a measure of complexity, the S-complexity. As shown
in Proposition 2.15, this function is continuous and therefore attains its maximal
value. In order to highlight the geometric nature of this function, we shall also re-
fer to it as the distance or information distance from the model ES. The problem
of maximizing the information distance from an exponential family was initially
suggested and analyzed by Ay [19] and further studied in a series of articles by, in
particular, Matúš, Knauf, and Rauh [28, 175–177, 179, 180, 221]. In [254], previous
results of the classical setting have been extended to the context of quantum states.
Algorithms for the evaluation of the information distance as a complexity measure
for quantum states have been studied in [203]. In that context, the information dis-
tance is related to the entanglement of quantum systems as deﬁned in [251].
Let us ﬁrst approach the problem of maximizing (6.19) from an intuitive perspec-
tive. In Sect. 2.8.1, we have deﬁned exponential families, and in particular hierar-
chical models, as afﬁne subspaces of the open simplex P+(IV ). Therefore, it should
always be possible to increase the distance of a point p from such an exponential
family as long as p ∈P+(IV ). This means that any maximizer p of that distance
has to be located in the boundary P(IV ) \ P+(IV ) of the full simplex, which im-
plies a restriction of the support of p. This intuition turns out to be correct. The
corresponding support reduction is quite strong, depending on the dimension of the
exponential family.
Theorem 6.1 (Support bound for maximizers ([19, 179]))
Let S be a non-empty
hypergraph, and let p be a local maximizer of DKL(·∥ES). Then
supp(p)
 ≤dim(ES) + 1 =

A∈S
0
v∈A

|Iv| −1

.
(6.20)
Proof Let p be a local maximizer of DKL(·∥ES). With the abbreviation ˆp :=
πS(p), we consider the afﬁne hull Ap of the preimage πS−1( ˆp), the set Pp of
all distributions in P(IV ) that have the same support as p, and the intersection
Sp := Ap ∩Pp. Clearly, Sp is relatively open (that is, open in its afﬁne hull) and
contains p. The restriction of DKL(·∥ES) to the set Sp equals, up to an additive
constant, minus the Shannon entropy and is therefore strictly convex (see (6.12)).
As p is a local maximizer of this restriction, it has to be an extreme point of Sp,
which is only possible if it consists of the single point p, that is, Ap ∩Pp = {p}.
This also implies
Ap ∩affPp = {p},
(6.21)

6.1
Complexity of Composite Systems
303
where we denote by aff the afﬁne hull in the vector space S(IV ) of all signed mea-
sures. With the dimension formula for afﬁne spaces we obtain
|IV | −1 = dimP(IV )
= dimaffP(IV )
≥dimaff(Ap ∪affPp)
= dimAp + dimaffPp −dim(Ap ∩affPp)
(
)*
+
=0,by (6.21)
=

|IV | −1 −dimES

+
supp(p)
 −1

.
This proves the inequality in (6.20). The equality follows from (2.208).
□
Theorem 6.1 is one of several results that highlight some simplicity aspects of
maximizers of the information distance from a hierarchical model. In particular, one
can also show that each maximizer p of DKL(·∥ES) coincides with its projection
πS(p) on its support, that is:
p(i) = λπS(p)(i),
i ∈supp(p),
(6.22)
where λ is a normalization factor. This can be obtained by the evaluation of the
gradient of the distance DKL(·∥ES) within the open simplex of distributions that
have the same support as the local maximizer p. The property (6.22) then follows
from the condition that the gradient has to vanish in p. (We omit the explicit proof
here and refer to the articles [19, 176, 179].)
For binary nodes, the support reduction (6.20) is quite strong. In that case, the
inequality (6.20) becomes |supp(p)| ≤|S|. Let us return to the hierarchical models
E(k) introduced in Example 2.6(2) and consider the distance DKL(p ∥E(k)). This
distance quantiﬁes the extent to which the distribution p is more than the sum of
its parts of size k, that is its marginals of size k. For k = 1, this is nothing but the
multi-information (6.11) of the individual variables. Assume that we have N binary
nodes and recall the dimension of E(k), given by (2.215):
d(N,k) :=
k

l=1
N
l

.
(6.23)
According to the support reduction of Theorem 6.1, each local maximizer has a
support size that is upper bounded by d(N,k) + 1, which equals N + 1 for k = 1.
Compared with the maximal support size 2N of distributions on N binary nodes,
this is an extremely strong support reduction. The situation is illustrated in Fig. 6.2.
If we interpret the information distance from ES as a measure of complexity, it
is natural to address the following problem: What kind of parametrized systems are
capable of maximizing their complexity by appropriate adjustment of their param-
eters? In the context of neural networks, for instance, that would correspond to a

304
6
Fields of Application of Information Geometry
Fig. 6.2 The three-
dimensional simplex and its
two-dimensional subfamily of
product distributions. The
extreme points of the simplex
are the Dirac measures δ(i,j),
i,j ∈{0,1}. The
maximization of the distance
from the family of product
distributions leads to
distributions with support
cardinality two. In addition,
the maximizers have a very
simple structure
learning process by which the neurons adjust their synaptic strengths and threshold
values so that the overall complexity of the ﬁring patterns increases. Based on The-
orem 6.1, it is indeed possible to construct relatively low-dimensional families that
contain all the local maximizers of the information distance.
Theorem 6.2 (Dimension bound for exponential families)
There exists an expo-
nential family E of dimension at most 2(dimES + 1) that contains all the local
maximizers of DKL(·∥ES) in its closure.
This statement is a direct implication of the support bound (6.20) and the follow-
ing lemma.
Lemma 6.1 Let I be a ﬁnite set, and let s ∈N, 1 ≤s ≤|I|. Then there exists an
exponential family Es of dimension at most 2s such that every distribution with
support size smaller than or equal to s is contained in the closure Es of Es.
Proof Consider an injective function f : I →R (f (i) = f (j) implies i = j). We
prove that the functions f k ∈F(I), k = 1,...,2s, span an exponential family Es so
that each distribution with support size smaller than or equal to s is contained in Es
(here, f k(i) := (f (i))k). Let p ∈P(I) with support size smaller than or equal to s.
We deﬁne the function g : I →R,
i →g(i) :=
0
i′∈supp(p)

f (i) −f

i′2 =
2s

k=0
ϑ(0)
k
f k(i).
Obviously, g ≥0, and g(i) = 0 if and only if i ∈supp(p).

6.1
Complexity of Composite Systems
305
Furthermore, we consider the polynomial h : R →R, t →	s−1
k=0 ϑ(1)
k
tk, that
satisﬁes
s−1

k=0
ϑ(1)
k
f k(i) = h

f (i)

= logp(i),
i ∈supp(p).
Obviously, the one parameter family
pλ(i) =
eh(f (i))−λg(i)
	
i′∈I eh(f (i′))−λg(i′)
is contained in Es. Note that we can ignore the term for k = 0 because this gives a
constant which cancels out. Furthermore, it is easy to see that
lim
λ→∞pλ(i) = lim
λ→∞
eh(f (i))(e−g(i))λ
	
i′∈I eh(f (i′))(e−g(i′))λ = p(i).
(6.24)
This proves that p is in the closure of the model Es.
□
Applied to the hierarchical models E(k), Theorem 6.2 implies that there is an
exponential family E of dimension 2(d(N,k) + 1) that contains all the local max-
imizers of DKL(·∥E(k)) in its closure. Again, for k = 1, that means that we have a
2(N + 1)-dimensional exponential family E in the (2N −1)-dimensional simplex
P(IV ) that contains all local maximizers of the multi-information in its closure.
This is a quite strong dimensionality reduction. If N = 10, for instance, this means
a reduction from 210 −1 = 1023 to 2(10 + 1) = 22.
We now reﬁne our design of families that contain the local maximizers of com-
plexity by focusing on a particular class of families. The idea is to identify interac-
tion structures that are sufﬁcient for the maximization of complexity by appropriate
adjustment of the interaction strengths. More speciﬁcally, we concentrate on the
class E(m) of families, given by the interaction order m of the nodes, and ask the fol-
lowing question: What is the minimal order m of interaction so that all (local) maxi-
mizers of DKL(·∥E(k)) are contained in the closure of E(m)? For k = 1, this problem
has been addressed in [28]. It turns out that for N nodes with the same cardinality
of state spaces the interaction order m = 2 is sufﬁcient for generating the global
maximizers of the multi-information. More precisely, if p is a global maximizer of
the multi-information then p is contained in the closure of E(2). These are the dis-
tributions that can be completely described in terms of pairwise interactions, and
they include the probability measures corresponding to complete synchronization.
In the case of two binary nodes, the maximizers are the measures 1
2(δ(0,0) + δ(1,1))
and 1
2(δ(1,0) + δ(0,1)) in Fig. 6.2. Note, however, that in this low-dimensional case,
the closure of E(2) coincides with the whole simplex of probability measures on
{0,1} × {0,1}. Therefore, being contained in the closure of E(2) is a property of all
probability measures and not special at all. The situation changes with three binary
nodes, where the dimension of the simplex P({0,1}3) is seven and the dimension
of E(2) is d(3,2) =
3
1

+
3
2

= 6 (see dimension formula (6.23)). For general N,

306
6
Fields of Application of Information Geometry
we have d(N,2) =
N
1

+
N
2

= N(N+1)
2
. Considering again the case N = 10, we
obtain this time a dimensionality reduction from 210 −1 = 1023 to 10(10+1)
2
= 55.
Note, however, that in contrast to the above construction of an exponential family
E according to Theorem 6.2, here we require that only the global maximizers of
the multi-information are contained in the closure of the constructed exponential
family.
For general k, the above question can be addressed using the following result of
Kahle [145]:
supp(p)
 ≤s, m ≥log2(s + 1)
⇒
p ∈E(m).
(6.25)
If m ≥log2(d(N,k)+2) then, according to (6.25) and with the use of (6.20), all the
maximizers of DKL(·∥E(k)) will be in the closure of E(m). For instance, if N = 10,
all maximizers of the multi-information will be contained in the closure of E(4), and
we have a reduction from 210 −1 = 1023 to d(10,4) =
10
1

+
10
2

+
10
3

+
10
4

=
385. This means that by increasing the number of parameters from 55, the number
that we obtained above for pairwise interactions, to 385, we can approximate not
only the global but also the local maximizers of the multi-information in terms of
interactions of order four.
Let us explore the implications of the presented results in view of the interpreta-
tion of DKL(·∥ES) as a complexity measure. Clearly, they reveal important aspects
of the structure of complex systems, which allows us to understand and control
complexity. In fact, the design of low-dimensional systems that are capable of gen-
erating all distributions with maximal complexity was the initial motivation for the
problems discussed in this section. The resulting design principles have been em-
ployed, in particular, in the context of learning systems (see [21, 29, 185]). However,
despite this constructive use of the simple structural constraints of complexity, we
have to face the somewhat paradoxical fact that the most complex systems, with
respect to the information distance from a hierarchical model, are in a certain sense
rather simple. Let us highlight this fact further. Say that we have 1000 binary nodes,
that is, N = 1000. As follows from the discussion above, interaction of order ten is
sufﬁcient for generating all distributions with locally maximal multi-information
DKL(·∥E(1)) (10 = log2(1024) ≥log2(1000 + 2) = log2(d(1000,1) + 2)). This
means that a system of size 1000 with (locally) maximal distance from the sum
of its elements (parts of size 1) is not more than the sum of its parts of size 10. In
view of our understanding of complexity as the extent to which the system is more
than the sum of its parts of any size, it appears inconsistent to consider these maxi-
mizers of multi-information as complex (nevertheless, in the thermodynamic limit,
systems with high multi-information exhibit features of complexity related to phase
transitions [94]). One would assume that complexity is reﬂected in terms of inter-
actions up to the highest order (see [146], which analyzes coupled map lattices and
cellular automata from this perspective). Trying to resolve this apparent inconsis-
tency by maximizing the distance DKL(·∥E(2)) from the larger exponential family
E(2) instead of maximizing the multi-information DKL(·∥E(1)) does not lead very
far. We can repeat the argument and observe that interaction of order 19 is now suf-
ﬁcient for generating all the corresponding maximizers: A system of size 1000 with

6.1
Complexity of Composite Systems
307
maximal deviation from the sum of its parts of size two is not more than the sum
of its parts of size 19. Following the work [32], we now introduce a modiﬁcation of
the divergence from a hierarchical model, which can handle this problem. Further-
more, we demonstrate the consistency of this modiﬁcation with known complexity
measures.
Remark 6.1 Before embarking on that, let us embed the preceding into the general
perspective offered by Theorem 2.8. That theorem offers some options for reducing
uncertainty. On the one hand, we could keep the functions fk and their expecta-
tion values and thus the mixture family M (2.161) ﬁxed. This then also ﬁxes the
maximum entropy estimate ˆμ, and we could then search in M for the μ that maxi-
mizes the entropy difference, that is, the Kullback–Leibler divergence (2.180) from
the exponential family. That is, imposing additional structure on μ will reduce our
uncertainty. This is what we have just explored.
On the other hand, we could also introduce further observables fℓand record
their expectation values. That would shrink the mixture family M. Consequently,
the minimization in Theorem 2.8 will also get more constrained, and the projection
ˆμ may get smaller entropy. That is, further knowledge (about expectation values
of observables in the present case) will reduce our uncertainty. This is explored in
[134].
6.1.3 The Weighted Information Distance
6.1.3.1 The General Scheme
In order to best see the general principle, we consider a hierarchy of hypergraphs
S1 ⊆S2 ⊆··· ⊆SN−1 ⊆SN := 2V
(6.26)
and denote the projection πSk on ESk by p(k). Then the Pythagorean relation (4.62)
of the relative entropy implies
DKL

p(l) p(m)
=
l−1

k=m
DKL

p(k+1) p(k)
,
(6.27)
for l,m = 1,...,N −1, m < l. In particular, we have
DKL

p
p(1)
=
N−1

k=1
DKL

p(k+1) p(k)
.
(6.28)
We shall use the same notation p(k) for various hierarchies of hypergraphs. Although
being clear from the given context, the particular meaning of these distributions will
change throughout our presentation.

308
6
Fields of Application of Information Geometry
Equation (6.27) shows a very important principle. The projections in the fam-
ily (6.26) can be decomposed into projections between the intermediate levels. This
also suggests a natural generalization of the information distance from a hierarchical
model, to give suitable weights to those intermediate projections. As we shall see,
this gain of generality will allow us to capture several complexity measures pro-
posed in the literature by a unifying principle. Thus, instead of (6.27) we consider a
weighted sum with a weight vector α = (α1,...,αN−1) ∈RN−1 and set:
Cα(p) :=
N−1

k=1
αk DKL

p
p(k)
(6.29)
=
N−1

k=1
βk DKL

p(k+1) p(k)
,
(6.30)
with βk := 	k
l=1 αl. Specifying a particular complexity measure then simply
means to employ speciﬁc assumptions to set those weights for the contributions
DKL(p(k+1) ∥p(k)). We might decide to put some of these weights to 0, but they
should be at least nonnegative. As one sees directly, the sequence βk is (strictly)
increasing with k if and only if all the αk are nonnegative (positive).
We shall now explore several weight schemes.
6.1.3.2 The Size Hierarchy of Subsets: The TSE-Complexity
As hierarchy (6.26) of hypergraphs we choose
Sk =
V
k

,
k = 1,...,N.
(6.31)
The corresponding hierarchical models E(k) are the ones introduced in Exam-
ple 2.6(2). In order to explore the family (6.29) of weighted complexity measures
we have to evaluate the terms DKL(p ∥p(k)). For k = 1, this is the multi-information
of the individual nodes, which can be easily computed. However, the situation is
more difﬁcult for 2 ≤k ≤N −1. There is no explicit expression for p(k) in these
cases. The iterative scaling method provides an algorithm that converges to p(k)
(see [78, 79]). In the limit, this algorithm would allow us to approximate the term
DKL(p ∥p(k)) with any desired accuracy. In this section, however, we wish to follow
a different path. The reason is that we want to relate the weighted complexity mea-
sure (6.29) to a measure of complexity that has been proposed by Tononi, Sporns,
and Edelman [245]. We refer to this measure as TSE-complexity and aim to identify
appropriate weights in (6.29) for such a relation. In order to do so, we ﬁrst estimate
the entropy of p(k) and then use (6.12) for a corresponding estimate of the distance
DKL(p ∥p(k)). More precisely, we wish to provide an upper bound for the entropy

6.1
Complexity of Composite Systems
309
of p(k). In order to do so, we consider the average entropy of subsets of size k,
Hp(k) :=
1
N
k


A⊆V
|A|=k
Hp(XA),
(6.32)
and analyze the way this quantity depends on k, with ﬁxed probability measure p.
This analysis yields the following observation: if we scale up the average entropy
Hp(k) by a factor N/k, corresponding to the system size, we obtain an upper bound
of the entropy of p(k). This clearly provides the upper bound
C(k)
p
:= N
k Hp(k) −Hp(N)
(6.33)
of the distance DKL(p ∥p(k)). The explicit derivations actually imply a sharper
bound. However, C(k)
p
is of particular interest here as it is related to the TSE-
complexity. We shall ﬁrst derive the mentioned bounds, summarized below in The-
orem 6.3, and then come back to this measure of complexity.
As a ﬁrst step, we show that the difference between Hp(k) and Hp(k −1) can be
expressed as an average of conditional entropies, proving that Hp(k) is increasing
in k. In what follows, we will simplify the notation and neglect the subscript p
whenever appropriate.
Proposition 6.1
Hp(k) −Hp(k −1) = 1
N

v∈V
1
N−1
k−1


A⊆V \{v}
|A|=k−1
Hp(Xv|XA) =: hp(k).
(6.34)
Proof
hp(k) = 1
N

v∈V
1
N−1
k−1


A⊆V \{v}
|A|=k−1
Hp(Xv|XA)
= 1
N

v∈V
1
N−1
k−1


A⊆V \{v}
|A|=k−1

Hp(Xv,XA) −Hp(XA)

=
k
N
N−1
k−1


A⊆V
|A|=k
Hp(XA) −N −(k −1)
N
N−1
k−1


A⊆V
|A|=k−1
Hp(XA)
=
1
N
k


A⊆V
|A|=k
Hp(XA) −
1
 N
k−1


A⊆V
|A|=k−1
Hp(XA)
= Hp(k) −Hp(k −1).
□

310
6
Fields of Application of Information Geometry
As a next step, we now show that differences between the averaged conditional
entropies hp(k) and hp(k +1) can be expressed as an average of conditional mutual
informations.
Proposition 6.2
hp(k) −hp(k + 1) =
1
N(N −1)

v,w∈V
v̸=w
1
N−2
k−1


A⊆V \{v,w}
|A|=k−1
Ip(Xv;Xw|XA).
Proof
1
N(N −1)

v,w∈V
v̸=w
1
N−2
k−1


A⊆V \{v,w}
|A|=k−1
Ip(Xv;Xw|XA)
=
1
N(N −1)

v,w∈V
v̸=w
1
N−2
k−1


A⊆V \{v,w}
|A|=k−1

Hp(Xv|XA) −Hp(Xv|Xw,XA)

= 1
N

v∈V
N −k
(N −1)
N−2
k−1


A⊆V \{v}
|A|=k−1
Hp(Xv|XA)
−1
N

v∈V
k
(N −1)
N−2
k−1


A⊆V \{v}
|A|=k
Hp(Xv|XA)
= hp(k) −hp(k + 1).
□
Since conditional mutual informations are positive, we can conclude that
h(k + 1) ≤h(k), i.e., the H(k) form a monotone and concave sequence as shown in
Fig. 6.3. In particular, we have
Hp(N) ≤Hp(k) + (N −k) hp(k).
(6.35)
As the RHS of this inequality only depends on the entropies of the k- and (k −1)-
marginals, it will not change if we replace p by the maximum entropy estimate p(k)
on the LHS of this inequality. This gives us
Hp(k)(XV ) ≤Hp(k) + (N −k) hp(k).
(6.36)
This also provides an upper bound for the distance
DKL

p
p(k)
= Hp(k)(XV ) −Hp(XV )
(6.37)
as
DKL

p
p(k)
≤Hp(k) + (N −k) hp(k) −Hp(N).
(6.38)

6.1
Complexity of Composite Systems
311
Fig. 6.3 The average entropy of subsets of size k grows with k. C(k)
p
can be considered to be an
estimate of the system entropy Hp(N) based on the assumption of a linear growth through Hp(k)
The estimates (6.36) and (6.38) involve the entropies of marginals of size k as
well as k −1. In order to obtain an upper bound that only depends on the entropies
of k-marginals, we express H(k) as the sum of differences:
Hp(k) =
k

l=1
hp(l)
with the convention Hp(0) = 0. With Proposition 6.2 we immediately obtain
Hp(k) =
k

l=1
hp(l) ≥k hp(k) ≥k hp(k + 1).
(6.39)
The estimates (6.36) and (6.38) now imply estimates that are weaker but only
depend on marginal entropies of subsets of size k:
Theorem 6.3 For all k, 1 ≤k ≤N, we have
Hp(k)(XV ) ≤N
k Hp(k),
DKL

p
p(k)
≤N
k Hp(k) −Hp(N) = C(k)
p ,
where equality holds for k = 1 and k = N.
The second inequality of this theorem states that the C(k)
p
can be considered as
an upper estimate of those dependencies DKL(p ∥p(k)) that cannot be described in
terms of interactions up to order k. The following result shows that the C(k)
p
are also
monotonically decreasing.

312
6
Fields of Application of Information Geometry
Corollary 6.1
C(k)
p ≤C(k−1)
p
.
Proof
C(k)
p −C(k−1)
p
= N
k Hp(k) −Hp(N) −
N
k −1Hp(k −1) + Hp(N)
= N
k

Hp(k) −Hp(k −1) −
1
k −1Hp(k −1)

= N
k

hp(k) −
1
k −1Hp(k −1)

≤0
since Hp(k −1) ≥(k −1) hp(k).
□
After having derived the C(k)
p
as an upper bound of the distances DKL(p ∥p(k)),
we can now understand the TSE-complexity. It can be expressed as a weighted sum
Cp :=
N−1

k=1
k
N C(k)
p .
(6.40)
If we pursue the analogy between DKL(p ∥p(k)) and C(k)
p
further, we can interpret
the TSE-complexity as being one candidate within the general class (6.29) of com-
plexity measures with weights αk = k
N .
6.1.3.3 The Length Hierarchy of Subintervals: The Excess Entropy
In this section, we consider a stochastic process X = (X1,X2,...,XN,...) with
ﬁnite state set I and distribution P . Here, the node set V = N plays the role of time.
For each N ∈N we have the probability measure p ∈P(I N) deﬁned by
pN(i1,...,iN) := P(X1 = i1,...,XN = iN),
i1,...,iN ∈I.
With the time order, we can now consider the hypergraph of intervals of length k,
the connected subsets of size k. This is different from the corresponding hypergraph
of the previous section, the set of all subsets with size k. In particular, in contrast to
the previous section, it is now possible to explicitly evaluate the maximum entropy
estimate p(k) and the corresponding terms DKL(p ∥p(k)) and DKL(p(k+1) ∥p(k))
for the hypergraphs of intervals. In what follows, we ﬁrst present the correspond-
ing derivations in more detail. After that, we will relate the weighted complexity
(6.30) to the excess entropy, a well-known complexity measure for stochastic pro-
cesses (see [68]), by appropriate choice of the weights βk. The excess entropy is
natural because it measures the amount of information that is necessary to perform

6.1
Complexity of Composite Systems
313
an optimal prediction. It is also known as effective measure complexity [111] and as
predictive information [47].
We use the interval notation [r,s] = {r,r + 1,...,s} and X[r,s] = i[r,s] for
Xr = ir, Xr+1 = ir+1, ..., Xs = is. We consider the family of hypergraphs
SN,k+1 :=

[r,s] ⊆[1,N] : s −r = k

,
0 ≤k ≤N −1.
(6.41)
The corresponding hierarchical models ESN,k+1 ⊆P(I N) represent the Markov pro-
cesses of order k. As the following proposition shows, the maximum entropy pro-
jection coincides with the k-order Markov approximation of the process X[1,N].
Proposition 6.3
Let X1,X2,...,XN be random variables in a non-empty and ﬁ-
nite state set I with joint probability vector p ∈P(I N), and let p(k) denote the
maximum entropy estimate of p with respect to SN,k+1. Then
p(k+1)(i1,...,iN) = p(i1,...,ik+1)
N−k
0
l=2
p(ik+l|il,...,ik+l−1),
(6.42)
DKL

p
p(k+1)
=
N−k−1

l=1
Ip(X[1,l];Xk+l+1 |X[l+1,k+l]).
(6.43)
Proof The distribution in (6.42) factorizes according to the hypergraph SN,k+1.
Therefore, according to Lemma 2.13, we have to prove that the A-marginal of the
distribution in (6.42) coincides with the A-marginal of p for all maximal intervals
A ∈SN,k+1. Let s ≥k + 1 and r = s −k, that is, s −r = k.

i1,...,ir−1

is+1,...,iN
p(i1,...,ik+1)
N−k
0
l=2
p(ik+l|il,...,ik+l−1)
=

i1,...,ir−1
p(i1,...,ik+1)
s−k
0
l=2
p(ik+l|il,...,ik+l−1)
×

is+1,...,iN
N−k
0
l=s−k+1
p(ik+l|il,...,ik+l−1)
(
)*
+
=1
=

i2,...,ir−1

i1
p(i1,...,ik+1)
 s−k
0
l=2
p(ik+l|il,...,ik+l−1)
=

i2,...,ir−1
p(i2,...,ik+1)
s−k
0
l=2
p(ik+l|il,...,ik+l−1)

314
6
Fields of Application of Information Geometry
=

i2,...,ir−1
p(i2,...,ik+1)p(ik+2|i2,...,ik+1)
s−k
0
l=3
p(ik+l|il,...,ik+l−1)
=

i3,...,ir−1

i2
p(i2,...,ik+2)
 s−k
0
l=3
p(ik+l|il,...,ik+l−1)
=

i3,...,ir−1
p(i3,...,ik+2)
s−k
0
l=3
p(ik+l|il,...,ik+l−1)
...
...
=

ir−1
p(ir−1,...,ik+r−1)
 s−k
0
l=r
p(ik+l|il,...,ik+l−1)
= p(ir,...,ik+r−1)p(is|ir,...,ik+r−1)
= p(ir,...,is).
Equation (6.43) is a direct implication of (6.42).
□
We have the following special cases of (6.42):
p(1)(i1,...,iN) =
N
0
l=1
p(il),
p(2)(i1,...,iN) = p(i1)p(i2|i1)···p(iN|iN−1),
p(N)(i1,...,iN) = p(i1,...,iN).
Proposition 6.4 In the situation of Proposition 6.3 we have
DKL

p(k+1) p(k)
=
N−k

l=1
Ip(Xk+l;Xl |X[l+1,k+l−1]),
1 ≤k ≤N −1.
(6.44)
If the process is stationary, the RHS of (6.44) equals (N −k)Ip(X1;Xk+1 |X[2,k]).
Proof
DKL

p(k+1) p(k)
= DKL

p
p(k)
−DKL

p
p(k+1)
=
N−k

l=2
Ip(X[1,l];Xk+l|X[l+1,k+l−1]) + Ip(X1;Xk+1|X[2,k])

6.1
Complexity of Composite Systems
315
−
N−k−1

l=1
Ip(X[1,l];Xk+l+1|X[l+1,k+l])
=
N−k−1

l=1
Ip(X[1,l+1];Xk+l+1|X[l+2,k+l]) + Ip(X1;Xk+1|X[2,k])
−
N−k−1

l=1
Ip(X[1,l];Xk+l+1|X[l+1,k+l])
=
N−k−1

l=1

Hp(Xk+l+1|X[l+2,k+l]) −Hp(Xk+l+1|X[1,k+l])

−

Hp(Xk+l+1|X[l+1,k+l]) −Hp(Xk+l+1|X[1,k+l])

+ Ip(X1;Xk+1|X[2,k])
=
N−k−1

l=1
Ip(Xk+l+1;Xl+1|X[l+2,k+l]) + Ip(X1;Xk+1|X[2,k])
=
N−k−1

l=0
Ip(Xk+l+1;Xl+1|X[l+2,k+l])
=
N−k

l=1
Ip(Xk+l;Xl|X[l+1,k+l−1])
= (N −k)Ip(X1;Xk+1|X[2,k])
(if stationarity is assumed).
□
We now come back to the complete stochastic process X = (X1,X2,...,XN,...)
with time set N. In order to deﬁne the excess entropy of this process, we assume
that it is stationary in the sense that its distribution is invariant with respect to the
shift map (i1,i2,...) →(i2,i3,...). The uncertainty about the outcome of a single
variable XN is given by the marginal entropy Hp(XN). The uncertainty about the
outcome of the same variable, given that the outcomes of the past N −1 variables
are known, is quantiﬁed by the conditional entropy
hp(XN) := Hp(XN |X1,...,XN−1).
The stationarity of X implies that the sequence hp(XN) is decreasing in N, and we
can deﬁne the limit
hp(X) := lim
N→∞hp(XN) = lim
N→∞
1
N Hp(X1,...,XN),
(6.45)

316
6
Fields of Application of Information Geometry
which is called the entropy rate or Kolmogorov–Sinai entropy of the process X. The
excess entropy of the process with the entropy rate hp(X) is then deﬁned as
Ep(X) := lim
N→∞
N

k=1

hp(Xk) −hp(X)

=
lim
N→∞

Hp(X1,...,XN) −Nhp(X)

.
(6.46)
It measures the non-extensive part of the entropy, i.e., the amount of entropy of each
element that exceeds the entropy rate. In what follows, we derive a representation of
the excess entropy Ep(X) in terms of the general structure (6.30). In order to do so,
we employ the following alternative representation of the excess entropy [68, 111]:
Ep(X) =
∞

k=1
k Ip(X1;Xk+1 |X[2,k]).
(6.47)
We apply Proposition 6.4 to this representation and obtain
Ep(X) =
∞

k=1
k Ip(X1;Xk+1|X[2,k]) = lim
N→∞
N−1

k=1
k Ip(X1;Xk+1|X[2,k])
= lim
N→∞
N−1

k=1
k
N −k (N −k)Ip(X1;Xk+1|X[2,k])
= lim
N→∞
N−1

k=1
k
N −k DKL

pN (k+1) pN (k)
(
)*
+
=:Ep(XN)
.
Thus, we have ﬁnally obtained a representation of quantities Ep(XN) that have
the structure (6.30) and converge to the excess entropy. The corresponding weights
βk =
k
N−k are strictly increasing with k. This implies weights αk =
N
(N−k)(N−k+1)
for the representation (6.29).
Let us conclude with a summary of Sect. 6.1.3 and an outlook. We have proposed
a general structure of weighted complexity measures, through (6.29) and (6.30), as
an extension of the information distance that we have studied in Sect. 6.1.2. The
basic idea is to decompose the unweighted information distance in terms of a given
hierarchy (6.26) of hypergraphs using the Pythagorean relation and then to weight
the individual terms appropriately. This very general ansatz allows us to show that
the TSE-complexity and the excess entropy can be interpreted as examples within
that scheme. The corresponding weights are quite reasonable but do not reveal a
general principle for the right choice of the weights that would determine a natural
complexity measure, given a hierarchy of hypergraphs. One reason for that is the

6.1
Complexity of Composite Systems
317
fact that the two hierarchies considered in these examples have quite different prop-
erties. A principled study is required for a natural assignment of weights to a given
hierarchy of hypergraphs.
6.1.4 Complexity of Stochastic Processes
In this section, we extend our study of Sect. 6.1.2 to the context of interacting
stochastic processes. In order to motivate the information distance as a complex-
ity measure, we ﬁrst decompose the multi-information rate of these processes into
various directed information ﬂows that are closely related to Schreiber’s transfer
entropy [40, 232] and Massay’s directed information [170–172]. We then interpret
this decomposition in terms of information geometry, leading to an extension of S-
complexity to the context of Markov kernels. This section is based on the works
[18, 22].
6.1.4.1 Multi-Information Rate, Transfer Entropy, and Directed Information
In order to simplify the arguments we ﬁrst consider only a pair Xk,Yk, k = 1,2,...,
of stochastic processes which we denote by X and Y. Below we present the straight-
forward extension to more than two processes. Furthermore, we use the notation
Xk for the random vector (X1,...,Xk) and similarly ik for the particular outcome
(i1,...,ik).
For a given time n ∈N, we deﬁne the normalized mutual information as
I

Xn ∧Y n
:= 1
n I

Xn ∧Y n
= 1
n

H

Xn
+ H

Y n
−H

Xn,Y n
= 1
n
n

k=1

H

Xk
 Xk−1
+ H

Yk
 Y k−1
−H

Xk,Yk
 Xk−1,Y k−1
.
In the case of stationarity, which we assume here, it is easy to see that the following
limit exists:
I(X ∧Y) := lim
n→∞I

Xn ∧Y n
.
(6.48)
This mutual information rate coincides with the difference hp(X) + hp(Y) −
hp(X,Y) of Kolmogorov–Sinai entropies as deﬁned by (6.45). Even though it is
symmetric, it plays a fundamental role in information theory [235] as the rate of in-
formation transmission through a directed channel. We now wish to decompose the

318
6
Fields of Application of Information Geometry
mutual information rate into directed information ﬂows between the two processes.
In order to do so, we deﬁne the transfer-entropy terms [40, 232]
T

Y k−1 →Xk

:= I

Xk ∧Y k−1  Xk−1
,
(6.49)
T

Xk−1 →Yk

:= I

Yk ∧Xk−1  Y k−1
.
(6.50)
With these deﬁnitions, we have
I

Xn ∧Y n
= 1
n
n

k=1

I

Xk ∧Yk
 Xk−1,Y k−1
(6.51)
+ T

Y k−1 →Xk

+ T

Xk−1 →Yk

.
(6.52)
Let us consider various special cases, starting with the case of independent and
identically distributed random variables (i.i.d. process). In that case, the transfer
entropy terms in (6.52) vanish and the conditional mutual informations on the RHS
of (6.51) reduce to mutual informations I(Xk ∧Yk). For stationary processes these
mutual informations coincide and we obtain I(X ∧Y) = I(X1 ∧Y1). In this sense,
I(X ∧Y) generalizes the mutual information of two variables.
In general, the conditional mutual informations on the RHS of (6.51) quan-
tify the stochastic dependence of Xk and Yk after “screening off” the causes of
Xk and Yk that are intrinsic to the system, namely Xk−1 and Y k−1. Assuming
that all stochastic dependence is generated by causal interactions, we can interpret
I(Xk ∧Yk |Xk−1,Y k−1) as the extent to which external causes are simultaneously
acting on Xk and Yk. If the system is closed in the sense that we have the following
conditional independence
p

ik,jk
 ik−1,jk−1
= p

ik
 ik−1,jk−1
p

jk
 ik−1,jk−1
(6.53)
for all k, then the conditional mutual informations in (6.52) vanish and the transfer
entropies in (6.52) are the only contributions to I(Xn ∧Y n). They quantify infor-
mation ﬂows within the system. As an example, consider the term
T

Y k−1 →Xk

= H

Xk
 Xk−1
−H

Xk
 Xk−1,Y k−1
.
It quantiﬁes the reduction of uncertainty about Xk if the outcomes of Y1,...,Yk−1
are known, in addition to the outcomes of X1,...,Xk−1. Therefore, the transfer
entropy T (Y k−1 →Xk) has been used as a measure for the causal effect of Y k−1 on
Xk [232], which is closely related to the concept of Granger causality (see [40] for
a detailed discussion on this relation). The limits of the transfer entropies (6.49) and
(6.50) for k →∞have been initially introduced by Marko [170] as directed versions
of the mutual information, and later, in a slightly modiﬁed form, further studied
by Massey [171] as directed informations. The work [33] provides an alternative
measure of information ﬂows based on Pearl’s theory of causation [212].
All deﬁnitions of this section naturally generalize to more than two processes.

6.1
Complexity of Composite Systems
319
Deﬁnition 6.1 Let XV = (Xv)v∈V be stochastic processes with state space IV =
×v∈V Iv. We deﬁne the transfer entropy and the normalized multi-information in
the following way:
T

XV \vn−1 →Xv,n

:= I

Xv,n ∧XV \vn−1  Xvn−1
,
I
 >
v∈V
Xvn

:= 1
n
 
v∈V
H

Xvn
−H

XV n
= 1
n
n

k=1

I
 >
v∈V
Xv,k
 XV k−1

+

v∈V
T

XV \vk−1 →Xv,k

.
(6.54)
Furthermore, if stationarity is assumed, we have the multi-information rate
I
 >
v∈V
Xv

:= lim
n→∞I
 >
v∈V
Xvn

.
(6.55)
6.1.4.2 The Information Distance from Hierarchical Models of Markov
Kernels
In order to highlight this decomposition of the multi-information from the infor-
mation-geometric perspective, we again restrict attention to the two processes X
and Y. We can rewrite I(Xn ∧Y n) in terms of KL-divergences:
I

Xn ∧Y n
= 1
n

in,jn
p

in,jn
log p(in,jn)
p(in)p(jn)
= 1
n
n

k=1

ik−1,jk−1
p

ik−1,jk−1
×

ik,jk
p

ik,jk
 ik−1,jk−1
log
p(ik,jk |ik−1,jk−1)
p(ik |ik−1)p(jk |jk−1).
Consider an interesting special case of condition (6.53):
p

ik,jk
 ik−1,jk−1
= p

ik
 ik−1
p

jk
 jk−1
,
k = 1,2,....
(6.56)
This condition describes the situation where the two processes do not interact at all.
We refer to such processes as being split. If the two processes are split, the transfer
entropy terms (6.52) also vanish, in addition to the conditional mutual information
terms on the RHS of (6.51).

320
6
Fields of Application of Information Geometry
This proves that I(Xn ∧Y n) vanishes whenever the two processes are split. Let
us consider the case of a stationary Markov process. In that case we have
I

Xn ∧Y n
= 1
n
n

k=1

ik−1,jk−1
p(ik−1,jk−1)
×

ik,jk
p(ik,jk |ik−1,jk−1)log p(ik,jk |ik−1,jk−1)
p(ik|ik−1)p(jk|jk−1)
(6.57)
≤1
n
n

k=1

ik−1,jk−1
p(ik−1,jk−1)
×

ik,jk
p(ik,jk |ik−1,jk−1)log p(ik,jk |ik−1,jk−1)
p(ik|ik−1)p(jk|jk−1)
(6.58)
= 1
n

i1,j1
p(i1,j1)log p(i1,j1)
p(i1)p(j1)
+ n −1
n

i1,j1
p(i1,j1)

i2,j2
p(i2,j2 |i1,j1)log
p(i2,j2 |i1,j1)
p(i2 |i1)p(j2 |j1)
(6.59)
n→∞
→

i,j
p(i,j)

i′,j′
p

i′,j′  i,j

log
p(i′,j′ |i,j)
p(i′ |i)p(j′ |j).
(6.60)
Here, the ﬁrst equality (6.57) follows from the Markov property of the joint pro-
cess (Xn,Yn). Note that, in general, the Markov property of the joint process is
not preserved when restricting to the individual marginal processes (Xn) and (Yn).
Therefore, p(ik|ik−1) and p(jk|jk−1) will typically deviate from p(ik|ik−1) and
p(jk|jk−1), respectively. The above inequality (6.58) follows from the fact that the
replacement of the ﬁrst pair of conditional probabilities by the second one can en-
large the KL-divergence. The last equality (6.59) follows from the stationarity of the
process. The convergence in line (6.60) is obvious.
Obviously, the KL-divergence in (6.60) of the joint Markov process from the
combination of two independent Markov processes can be interpreted within the
geometric framework of S-complexities deﬁned by (6.2). In order to make this in-
terpretation more precise, we have to ﬁrst replace the space of probability measures
by a corresponding space of Markov kernels. Correspondingly, we have to adjust the
notion of a hypergraph, the set of parts, and the notion of a hierarchical model, the
set of non-complex systems, to the setting of Markov kernels. Such a formalization
of the complexity of Markov kernels has been developed and studied by Ay [18, 22].
To give an outline, consider a set V of input nodes and a set W of output nodes.
The analogue of a hypergraph in the context of distributions should be a set S of
pairs (A,B), where A is an arbitrary subset of V and B is a non-empty subset of W.

6.1
Complexity of Composite Systems
321
This deﬁnes the set KS of kernels K : IV × IW →[0,1] that satisfy
Ki
j =
exp(	
(A,B)∈S φA,B(i,j))
	
j′ exp(	
(A,B)∈S φA,B(i,j′)),
φA,B ∈FA×B.
(6.61)
Note that these kernels can be obtained, by conditioning, from joint distributions that
are associated with a corresponding hypergraph on the disjoint union of V and W.
All hyperedges on this joint space that have an empty intersection with W cancel
out through the conditioning so that they do not appear in (6.61). This is the reason
for considering only non-empty subsets B of W.
The set KS is the analogue of a hierarchical model (2.207), now deﬁned for
kernels instead of distributions. We deﬁne the relative entropy of two kernels K and
L with respect to a distribution μ as
Dμ
KL(K ∥L) :=

i
μi

j
Ki
j log
Ki
j
Li
j
.
(6.62)
This allows us to deﬁne the S-complexity of a Markov kernel K as
Dμ
KL(K ∥KS) := inf
L∈KS
Dμ
KL(K ∥L).
(6.63)
Given two kernels Q and R that satisfy Dμ
KL(K ∥Q)=Dμ
KL(K ∥R)=Dμ
KL(K ∥KS),
we have Qi
j = Ri
j for all i with μi > 0 and all j. We denote any such projection of
K onto KS by KS.
Let us now consider a natural order relation ≼on the set of all hypergraphs.
Given two hypergraphs S and S′, we write S ≼S′ if for any pair (A,B) ∈S
there is a pair (A′,B′) ∈S′ such that A ⊆A′ and B ⊆B′. By the corresponding
inclusions of the interaction spaces, that is FA×B ⊆FA′×B′, we obtain
S ≼S′
⇒
KS ⊆KS′,
(6.64)
and for the associated hierarchy of complexity measures we have
Dμ
KL(K ∥KS) ≥Dμ
KL(K ∥KS′).
(6.65)
More precisely, the Pythagorean relation yields the following equality.
Proposition 6.5 Consider two hypergraphs S and S′ satisfying S ≼S′. Then
Dμ
KL(K ∥KS) = Dμ
KL(K ∥KS′) + Dμ
KL(KS′ ∥KS).
(6.66)
(Here we can choose any version KS′.)
Proof It is easy to see that any KS can be obtained from the maximum entropy
projection ˆp(i,j) of p(i,j) := μiKi
j onto the hierarchical model deﬁned by the

322
6
Fields of Application of Information Geometry
following hypergraph 'S on the disjoint union of V and W: We ﬁrst include the set
V as one hyperedge of 'S. Then, for all pairs (A,B) ∈S, we include the disjoint
unions A ∪B as further hyperedges of 'S. In these deﬁnitions, we obtain
Dμ
KL(K ∥KS) = inf
L∈KS
Dμ
KL(K ∥L) = inf
q∈E'
S
DKL(p ∥q) = DKL(p ∥ˆp),
and
KSi
j = ˆp(j |i),
whenever μi > 0.
(6.67)
This yields the Pythagorean relation (6.66) as a direct implication of the correspond-
ing relation for joint distributions.
□
The following proposition provides an explicit formula for the S-complexity
(6.63) in the case where the sets Bk form a partition of W. This directly corresponds
to the situation of Example 6.1(4).
Proposition 6.6
Let S = {(A1,B1),...,(An,Bn)} where the Ak are subsets of
V and the Bk form a partition of W. More precisely, we assume that Bk ̸= ∅,
Bk ∩Bl = ∅for k ̸= l, and /n
k=1 Bk = W. Then, with p(i,j) := μi Ki
j, i ∈IV ,
j ∈IW , the following holds:
(1) Any projection KS of K onto KS satisﬁes
KSi
j =
n
0
k=1
p(jBk |iAk),
whenever μiAk > 0 for all k.
(6.68)
(2) The corresponding divergence has the entropic representation
Dμ
KL(K ∥KS) =
n

k=1
Hp(XBk |XAk) −Hp(XW |XV ).
(6.69)
(Recall that, for any subset S of V ∪W, we denote by XS the canonical restric-
tion (iv)v∈V ∪W →(iv)v∈S.)
Proof With Lemma 2.13, we can easily verify that the maximum entropy estimate
of p with respect to the hypergraph 'S, as deﬁned in the proof of Proposition 6.5, is
given by
ˆp(iV ,jW) = p(iV )
0
(A,B)∈S
p(jB|iA).
(6.70)
With (6.67), we then obtain (6.68). A simple calculation based on this explicit for-
mula for KS conﬁrms the entropic representation (6.69).
□
We evaluate the entropic representation (6.69) of the S-complexity for a few
simple but instructive examples.

6.1
Complexity of Composite Systems
323
Example 6.2
In this example, we restrict attention to the case where V = W and
denote the variable XW by X′
V = (X′
v)v∈V . We consider only hypergraphs S for
which the assumptions of Proposition 6.6 are satisﬁed:
(1) Stot = {(V,V )}. The set KStot consists of all strictly positive kernels from IV
to IV . This obviously implies Dμ
KL(·∥KStot) ≡0.
(2) Sind = {(∅,V )}. The set KSind can naturally be identiﬁed with the set of all
strictly positive distributions on the image set IV . The complexity coincides
with the mutual information between the input XV and the output X′
V :
Dμ
KL(K ∥KSind) = I

X′
V ;XV

.
It vanishes if and only if XV and X′
V are stochastically independent.
(3) Sfac = {(∅,{v}) : v ∈V } ≼Sind. The set KSfac can naturally be identiﬁed with
the strictly positive product distributions (factorized distributions) on the image
set IV . The complexity
Dμ
KL(K ∥KSfac) =

v∈V
H

X′
v

−H

X′
V

(6.71)
is nothing but the multi-information of the output X′
v, v ∈V (see Eq. (6.11)).
(4) Ssplit = {({v},{v}) : v ∈V }. The set KSsplit describes completely split systems,
corresponding to a ﬁrst-order version of (6.56). The complexity
Dμ
KL(K ∥KSsplit) =

v∈V
H

X′
v
Xv

−H

X′
V
XV

(6.72)
extends (6.60) to more than two units. If K ∈Sind, the complexity (6.72) re-
duces to the multi-information (6.71).
(5) Spar = {(V,{v}) : v ∈V } ≽Ssplit. The set KSpar describes closed systems
corresponding to (6.53). The complexity
Dμ
KL(K ∥KSpar) =

v∈V
H

X′
v
XV

−H

X′
V
XV

quantiﬁes the stochastic dependence of the output variables X′
v, v ∈V , after
“screening off” the input XV . Therefore, it can be interpreted as the external
causal inﬂuence on the output variables. If K ∈Spar, then
Dμ
KL(K ∥KSsplit) =

v∈V
I

X′
v;XV \{v}
Xv

.
Therefore, the complexity (6.72) reduces to a sum of ﬁrst-order versions of
transfer entropies.
(6) Consider a network of nodes modeled in terms of a directed graph G = (V,E),
where E ⊆V × V . With pa(v) := {u ∈V : (u,v) ∈E}, the parents of v, we de-
ﬁne Snet := {(pa(v),{v}) : v ∈V } ≼Spar. The hierarchical model KSnet con-
sists of all dynamics where the nodes v ∈V receive information only from their

324
6
Fields of Application of Information Geometry
individual parent sets pa(v) and update their states synchronously. We assume
that each node has access to its own state, that is v ∈pa(v). With this assump-
tion, we have Ssplit ≼Snet and for all K ∈Snet,
Dμ
KL(K ∥KSsplit) =

v∈V
I

X′
v;Xpa(v)
Xv

.
(6.73)
Each individual term I(X′
v;Xpa(v)|Xv) on the RHS of (6.73) can be interpreted
as the information that the parents of node v contribute to the update of its state
in one time step, the local information ﬂow in v. This means that the extent
to which the global transition X →X′ is more than the sum of the individual
transitions Xv →X′
v, v ∈V , equals the sum of the local information ﬂows
(RHS of (6.73)).
The following diagram summarizes the inclusion relations of the individual fam-
ilies considered in Example 6.2:
KStot
⊇
KSpar
⊇
KSnet
⊇
⊇
KSind
⊇
KSfac
⊆
KSsplit
(6.74)
The measure Dμ
KL(K ∥KSsplit) plays a particularly important role as a measure
of complexity. It quantiﬁes the extent to which the overall process deviates from a
collection of non-interacting processes, that is, the split processes. Therefore, it can
be interpreted as a measure of interaction among the individual processes. In order to
distinguish this measure from the strength of mechanistic or physical interactions,
as those considered in Sect. 2.9.1, Dμ
KL(K ∥KSsplit) has also been referred to as
stochastic interaction [18, 22]. Motivated by the equality (6.73), it has been studied
in the context of neural networks as a measure of information ﬂow among neurons
[20, 36, 255]. It turns out that the maximization of Dμ
KL(K ∥KSsplit) in the domain
of all Markov kernels leads to a strong determinism, that is, Ki
j > 0 for a relatively
small number of js, which corresponds to the support reduction (6.20).
Obviously, there are two ways to project any K ∈KStot from the upper left of
the inclusion diagram (6.74) onto KSfac. By the Pythagorean relation, we obtain
Dμ
KL(K ∥KSsplit) = I

X′
V ;XV

+ Dμ
KL(KSind ∥KSfac)
−Dμ
KL(KSsplit ∥KSfac)
≤I

X′
V ;XV

+ I
 >
v∈V
X′
v

.
(6.75)
In general, Dμ
KL(K ∥KSsplit) can be larger than the mutual information of X′
V
and XV , because it also incorporates stochastic dependencies of the output nodes,
quantiﬁed by the multi-information (second term on RHS of (6.75)). This is the

6.1
Complexity of Composite Systems
325
case, for instance, when X′
V and XV are independent, and the output variables X′
v,
v ∈V , have positive multi-information. In that simple example, the upper bound
(6.75) is attained.
Conceptually, Dμ
KL(K ∥KSsplit) is related to the notion of integrated informa-
tion which has been considered by Tononi as the essential component of a general
theory of consciousness [244]. Various deﬁnitions of this notion have been pro-
posed [37, 42]. Oizumi et al. postulated that any measure of information integration
should be non-negative and upper bounded by the mutual information of X′
V and
XV [206, 207] (see also Sect. 6.9 of Amari’s monograph [11]). Therefore, only
a modiﬁed version of Dμ
KL(K ∥KSsplit) can serve as measure of information inte-
gration as it does not satisfy the second condition. One obvious modiﬁcation can
be obtained by simply subtracting from Dμ
KL(K ∥KSsplit) the multi-information that
appears in the upper bound (6.75). This leads to a quantity that is sometimes referred
to as synergistic information and has been considered as a measure of information
integration in [42]:
Dμ
KL(K ∥KSsplit) −I
 >
v∈V
X′
v

= I

X′
V ;XV

−

v∈V
I

X′
v;Xv

.
(6.76)
This quantity is obviously upper bounded by the mutual information between X′
V
and XV , but in general it can be negative. Therefore, Amari [11] proposed a dif-
ferent modiﬁcation of Dμ
KL(K ∥KSsplit) which is still based on the general deﬁni-
tion (6.63) of the S-complexity. He extended the set of split kernels by adding
the hyperedge (∅,V ) to the hypergraph Ssplit, which leads to the new hypergraph
Ssplit := Ssplit ∪{(∅,V )}. This modiﬁcation of the set of split kernels implies the
following inclusions (see Example 6.2(2)):
KSind ⊆K
Ssplit ⊇KSsplit.
It follows directly from the Pythagorean relation that
Dμ
KL(K ∥K
Ssplit) ≤I

X′
V ;XV

,
and
Dμ
KL(K ∥K
Ssplit) ≤Dμ
KL(K ∥KSsplit).
The ﬁrst inequality shows that the information ﬂow among the processes cannot
exceed the total information ﬂow. In fact, any S-complexity is upper bounded by
the mutual information of X′
V and XV , whenever Sind = {(∅,V )} ⊆S. Although
being captured within the general framework of the S-complexity (6.63), the mod-
iﬁed measure does not satisfy the assumptions of Proposition 6.6, and its evaluation
requires numerical methods. Iterative scaling methods [78, 79] provide efﬁcient al-
gorithms that allow us to approximate Dμ
KL(K ∥K
Ssplit) with any desired accuracy.
The hierarchical decomposition of the mutual information between multiple in-
put nodes Xv, v ∈V , and one output node Xw has been formulated by Williams and
Beer [257] as a general problem (partial information decomposition). Information-
geometric concepts related to information projections turn out to be useful in ap-
proaching that problem [46, 213].

326
6
Fields of Application of Information Geometry
Fig. 6.4 Various measures are evaluated with respect to the stationary distribution μβ of the ker-
nel Kβ, where β is the inverse temperature. For the plots (1)–(4) within (A) and (B), respectively,
the pairwise interactions ϑvw ∈R, v,w ∈V , are ﬁxed. (1) mutual information Ipβ (X′
V ;XV ) (we
denote by pβ the distribution pβ(i,j) = μβiKβi
j ); (2) stochastic interaction Dμβ
KL(Kβ ∥KSsplit);
(3) integrated information Dμβ
KL(Kβ ∥K
Ssplit) (these plots are obtained by the iterative scaling
method); (4) synergistic information Dμβ
KL(Kβ ∥KSsplit) −Ipβ (?
v∈V X′
v)
Example 6.3 This example depicts the analysis of Kanwal within the work [149].
We compare some of the introduced measures in terms of numerical evaluations,
shown in Fig. 6.4, using a simple ergodic Markov chain in {−1,+1}V . We as-
sume that the corresponding kernel is determined by pairwise interactions ϑvw ∈R,
v,w ∈V , of the nodes and the inverse temperature β ∈[0,∞). Given a state vector
XV at time n, the individual states X′
w at time n + 1 are updated synchronously
according to
P

X′
w = +1
 XV

=
1
1 + e−β 	
v ϑvwXv ,
w ∈V.
(6.77)
For the evaluation of the measures shown in Fig. 6.4, the pairwise interactions are
initialized and then ﬁxed (the two sets of plots shown in Figs. 6.4(A) and (B), re-
spectively, correspond to two different initializations of the pairwise interactions).
The measures are then evaluated as functions of the inverse temperature β, which
determines the transition kernel Kβ by Eq. (6.77) and its stationary distribution μβ.
We conclude this section by highlighting an important point here. As a motivation
of the S-complexity, we presented various decompositions of the multi-information
rate of stationary stochastic processes (see, in particular, (6.60)). Here, stationary
distributions of Markov chains determine the information-geometric structures of
such processes. For instance, the distribution p(i,j) that appears in the relative en-
tropy (6.60) is the stationary distribution of the joint process (Xn,Yn). A similar
derivation holds for the Fisher metric on models of Markov chains (see (3.38) and

6.2
Evolutionary Dynamics
327
(3.39), and also [193]). It is important to note that we did not incorporate the station-
arity into our deﬁnitions (6.61) of a hierarchical model of Markov kernels and the
relative entropy (6.62). Also, there is no coupling between the distribution μ and the
kernel K in Propositions 6.5 and 6.6. In particular, we could assume that the input
space equals the output space, that is V = W and IV = IW , and choose μ to be a
stationary distribution with respect to K, as we did in the above Example 6.3. Note
that in general this does not imply the stationarity of the same distribution μ with
respect to a projection KS of K. (In the second term of the RHS of the Pythagorean
relation (6.66), for instance, μ is not necessarily stationary with respect to KS′.)
However, if we add the hyperedge (∅,V ) to a given hypergraph S, the stationarity
is preserved by the projection. For Ssplit, this is exactly the above-mentioned exten-
sion (see Sect. 6.9 of Amari’s monograph [11]). A general consistent coupling of
Markov kernels and their corresponding invariant distributions has been developed
in [118, 193], where a Pythagorean relation for stationary Markov chains is formu-
lated in terms of the canonical divergence of the underlying dually ﬂat structure (see
Deﬁnition 4.6).
We expect that the development of the information geometry of stochastic pro-
cesses, in particular Markov chains [10, 118, 193, 243] and related conditional mod-
els [29, 147, 165, 184], will further advance the information-geometric research on
complexity theory.
6.2 Evolutionary Dynamics
Evolutionary dynamics studies the change in time of the relative frequencies of var-
ious types in a population. Depending on the biological application, such types can
stand for species, strategies, alleles in a gene pool, etc. Their frequencies change
in response to or as a consequence of various effects. The most important and best
studied effects are selective differences, which one tries to capture by the notion of
ﬁtness, mutations, that is, transitions between the different types, random sampling,
recombination when sexual pairing is involved, location dependent properties, pop-
ulation effects, etc. Some of these effects are deterministic, some can be modeled
at least in a deterministic manner at the level of large populations, and some are
irreducibly stochastic. As this is a vast topic, there are various monographs, like
[95, 123, 124, 224], that cover some aspects from a mathematical perspective. In
line with the aim of the present book, here we want to develop and explore the con-
nections with information geometry. That connection comes from the phrase that
we have italicized above, relative frequencies. Formally, such relative frequencies
can be treated as probabilities, and thus, we are lead to studying dynamical systems
on the probability simplex, and as we shall see, these dynamical systems naturally
involve the natural metric on that simplex, the Fisher metric, which has been called
the Shahshahani metric [233] in evolutionary dynamics. We shall start with the ef-
fects of selection and mutation. These will lead us to systems of ordinary differential
equations, the so-called replicator equations. We shall then augment this by the ef-
fects of random sampling. This is an inherently stochastic effect. Mathematically, it

328
6
Fields of Application of Information Geometry
can be seen as a perturbation of the replicator equations by noise. As is well known,
the evolution of the resulting probabilities can be modeled by what the physicists
call Fokker–Planck equations and the mathematicians prefer to call Kolmogorov
equations. Whether or not the relative frequencies involved can be biologically in-
terpreted as probabilities will play no role for our mathematical analysis.
Since the type composition of the population unfolds in time, the treatment of
time is important. The simplest transitions would consist of discrete time steps,
corresponding, for instance, to the annual reproduction in a population, suppress-
ing such biological issues as overlapping generations, etc. In fact, the evolutionary
models as presented here inevitably operate at a very abstract level and suppress
most biological details. The advantage of such an abstract treatment might consist
in identifying the dominant, most important mechanisms at work in biological evo-
lution and investigating them with powerful mathematical tools. At a later stage,
these abstract models can then be augmented by further relevant biological details,
but here we conﬁne ourselves to work out the fundamental principles.
We shall, however, rather investigate models with continuous instead of discrete
time. Of course, continuous time dynamics can be obtained as limits of discrete
ones, and we shall also insert a short section looking at such transitions from an
information geometric perspective.
6.2.1 Natural Selection and Replicator Equations
In this section, we introduce the notion of a replicator equation, that is, a certain type
of dynamical system on the probability simplex, and explore it with the tools of in-
formation geometry. The analysis of evolutionary dynamics in terms of differential
geometry has a long history [2–4, 123]. In particular, there is the Shahshahani met-
ric [233] which we shall recognize as the Fisher metric. Further links to information
geometry are given by exponential families, which naturally emerge within evolu-
tionary biology. The exponential connection has also been used in [24], in order to
study replicator equations that admit a closed-form solution.
Since various biological interpretations are possible, we proceed abstractly and
consider a ﬁnite set I = {0,1,2,...,n} of types. For the moment, to be concrete,
one may think of the types of species, or as evolutionary strategies. In Sect. 6.2.3, the
types will be alleles at a genetic locus. We then consider a population, consisting of
Ni members of type i. The total number of members of that population is then given
by N = 	
j Nj. We shall assume, however, that the dynamics will depend only on
the relative frequencies Ni
N , i ∈I, which deﬁne a point in the n-dimensional simplex
Σn = P(I). Assuming large population sizes, that is N →∞, we can consider the
full simplex as the set of all possible populations consisting of n + 1 different types.
Concerning the dynamical issue, the type composition of the population is chang-
ing in time, because the different types reproduce at different rates. The quantities
that we want to follow are the
˙pi
pi ; this quantity expresses the reproductive suc-
cess of type i. This reproductive success can depend on many factors, but in order

6.2
Evolutionary Dynamics
329
to have a closed theory, at time t, it should depend only on the population vector
p = (p0,p1,...,pn) at t. Thus, it should be a function f i(p). In order to maintain
the normalization 	pi = 1, we need to subtract a normalizing term. This leads us
to the replicator equation (see [123] and the references therein)
˙pi = pi

f i(p) −

j
pj f j(p)

,
i ∈I.
(6.78)
In terms of the concept of natural selection of Darwinian evolutionary theory, we
may want to interpret this as
˙pi
pi
= ﬁtness of type i −average ﬁtness,
(6.79)
although “ﬁtness”, in fact, is a very subtle concept whose precise carrier is not al-
ways clear (see, e.g., the discussion in [133]). That is, we want to call f i(p) the
ﬁtness of type i.
Equation (6.78) deﬁnes a dynamical system on P+(I). In fact, any dynamical
system ˙pi = Xi(p), i ∈I, on P+(I) can be represented in this form by setting
f i(p) := Xi(p)
pi . Therefore, (6.78) can be arbitrarily complex, depending on the par-
ticular choice of the ﬁtness function f : p →f (p) = 	
i f i(p)ei. Let us recapitu-
late a bit on that. The vector X(p) deﬁned by the RHS of Eq. (6.78) does not change
if we add a constant vector c(p) to the ﬁtness f (p). Therefore, X(p) is uniquely
determined by the equivalence class f (p) + R. As we have shown in Sect. 2.1, the
assignment p →f (p) + R is a section in the cotangent bundle T ∗P+(I), that is
a covector ﬁeld. Using the Fisher metric, in order to translate this into a section in
the tangent bundle, we obtain the ﬁeld p →X(p). The ﬁtness, which drives the
selection process in the evolutionary dynamics, corresponds to a covector ﬁeld.
Let us ﬁrst consider frequency independent ﬁtness functions, that is, where the
individual f i are constant and do not depend on the population p. The correspond-
ing mean ﬁtness f , which assigns to a population p the value f (p) := p(f ) =
	
i∈I pif i, does depend on p, however. By Proposition 2.2, the gradient of the
mean ﬁtness with respect to the Fisher metric is
(gradp f )i = pi

f i −

j
pj f j

,
and we obtain the following differential equation (see also [116]):
˙p(t) = gradp(t) f ,
p(0) = p0.
(6.80)
For any solution of (6.80), we have
d
dt f

p(t)

=

i
˙pi(t)f i
=

i
pi(t)

f i −

j
pj(t)f j

f i

330
6
Fields of Application of Information Geometry
= Varp(t)(f )
> 0,
(6.81)
unless we have the trivial situation where the ﬁtness values f j are the same for all
types j with nonzero frequencies pj. This means that the mean ﬁtness is increasing
in time. This observation is sometimes referred to as Fisher’s Fundamental Theorem
of Natural Selection [97], which in words can be stated as follows:
“The rate of increase in the mean ﬁtness of any organism at any time ascribable to natural
selection acting through changes in gene frequencies is exactly equal to its genetic variance
in ﬁtness at that time.” [88]
As we shall see below, this theorem in general requires that the ﬁtness values f i
be constant (independent of p), as we have assumed here.
When we consider another vector g with (constant) components gi that model
trait values of the individual types, we obtain in the same manner
d
dt g

p(t)

=

i
˙pi(t)gi
=

i
pi(t)

f i −

j
pj(t)f j

gi
= Covp(t)(f,g).
(6.82)
This means that the mean value of a trait will increase in time if its covariance
with the ﬁtness function is positive. This is due to G.R. Price [217], and it plays an
important role in the mathematical theory of evolution, see [224].
The differential equation (6.80) has the solution
t →
etf
p(etf ) p.
This solution has the following limit points (see Fig. 6.5):
lim
t→−∞pi(t) =

pi
	
j∈argmin(f ) pj ,
if i ∈argmin(f ),
0,
otherwise,
i ∈I,
and
lim
t→+∞pi(t) =

μi
	
j∈argmax(f ) pj ,
if i ∈argmax(f ),
0,
otherwise,
i ∈I.
This means that only those types will survive in the inﬁnite future that have maximal
ﬁtness, and those that persist from the inﬁnite past had minimal ﬁtness. Importantly,
this happens at exponential rates. Thus, the basic results of Darwin’s theory of evolu-
tion are encapsulated in the properties of the exponential function. When the ﬁtness

6.2
Evolutionary Dynamics
331
Fig. 6.5 The solution curve
of Eq. (6.80). Mean ﬁtness is
maximized along the curve,
eventually converging to
those types that have maximal
ﬁtness and coming from those
with least ﬁtness
is frequency dependent, the mean ﬁtness will no longer increase in general. In fact,
for a frequency dependent ﬁtness f , we have to extend Eq. (6.81) as
d
dt f

p(t)

= Varp(t)

f

p(t)

+

i
pi(t) d
dt f i
p(t)

.
(6.83)
Thus, the change of ﬁtness is caused by two processes, corresponding to the two
terms on the RHS of (6.83). Only the ﬁrst term always corresponds to an increase
of the ﬁtness [98], and because of the presence of the second term, the mean ﬁtness
need not increase during evolution. For instance, it could happen that the ﬁtness
values f i decrease for those types i whose relative frequencies increase, and con-
versely. It might be advantageous to be in the minority. Fisher himself used such
a reasoning for the balance of sexes in a population; see, e.g., the presentation in
[141].
After having discussed the case of constant ﬁtness functions, we now move on to
consider other cases with a clear biological motivation:
1. Linear ﬁtness. We wish to include interactions between the types. The simplest
possibility consists in considering replicator equations with linear ﬁtness. With
an interaction matrix A = (aij)i,j∈I , we consider the equation
˙pi = pi

j
aijpj −

k
pk

j
akjpj

.
(6.84)
While a constant ﬁtness function could always be represented as a gradient ﬁeld,
in the linear case, by (2.38), we need the following condition:
aij + ajk + aki = aik + akj + aji.
(6.85)
In particular, this is satisﬁed if the matrix A is symmetric, and a potential function
then is
V (p) = 1
2

ij
aij pipj.

332
6
Fields of Application of Information Geometry
2. The replicator–mutator equation. The replicator equation (6.78) is based on se-
lection that acts proportionally to the ﬁtness values f i(p) of the types i within
the population p. Evolutionary dynamics, however, involves also other mecha-
nisms, such as mutation and recombination. Here, we shall not consider recom-
bination, referring, for instance, to [124] instead, but only mutations. In formal
terms, a mutation is a transition between types. We thus consider a mutation ma-
trix mij, i,j ∈I, which formalizes the probability for i to mutate into j and
therefore satisﬁes mij ≥0 and 	
j mij = 1. Here, mii expresses the probability
that i does not mutate into another type. In particular, when mji = δji, we have
perfect replication, and no mutations occur.
After the replication of all types j according to their ﬁtnesses f j(p), they
may then change their identities by mutations. We obtain as a modiﬁcation of the
replicator equation the so-called replicator–mutator equation [115, 210, 240]:
˙pi =

j∈I
pj f j(p)mji −pi f (p).
(6.86)
Note that since mii = 1 −	
j̸=i mij, (6.86) is equivalent to
˙pi = pif i(p) +

j̸=i
pj f j(p)mji −

j̸=i
pif i(p)mij −pi f (p),
(6.87)
which then has a positive contribution for the mutations i gains from other types
and a negative contribution for those mutations lost to other types.
One can relate Eq. (6.86) to the replicator equation (6.78) in two ways. In
the case of perfect replication, mji = δji, we are back to the original replica-
tor equation. In the general case, we may consider a replicator-mutator equa-
tion as a replicator equation with a new, effective, ﬁtness function f i
eff(p) =
1
pi
	
j∈I pj f j(p)mji that drives the selection process based on replication and
mutation, that is,
˙pi = pi
 1
pi

j∈I
pj f j(p)mji −f (p)

.
(6.88)
3. The quasispecies equation. In the special case where the original ﬁtness functions
f i are frequency independent, but where mutations are allowed, Eq. (6.86) is
called the quasispecies equation. Note that, due to mutation, even in this case the
effective ﬁtness f i
eff(p) is frequency dependent and deﬁnes a gradient ﬁeld only
for f i = f j and mji = mki for all i ̸= j,k. We’ll return to that issue below in
Sect. 6.2.3. Hofbauer [122] studied a different version of the replicator–mutator
equation in view of Fisher’s fundamental theorem of natural selection.

6.2
Evolutionary Dynamics
333
6.2.2 Continuous Time Limits
Dynamical systems that occur as models in evolutionary biology can be either time
continuous or time discrete. Often one starts with a discrete transition that describes
the evolution from one generation to the next. In order to apply advanced methods
from the theory of differential equations, one then takes a continuous time limit,
leading to the replicator equation (6.78) in Sect. 6.2.1 above or the Kolmogorov
equations in Sect. 6.2.3 below. The dynamical properties of those equations crucially
depend on the way this limit is taken. In the present section, we want to address this
issue in a more abstract manner with the concepts of information geometry and
propose a procedure that extends the standard Euler method.
Let us ﬁrst recall how the standard Euler method relates time continuous dynam-
ical systems to time discrete ones. Say that we have a vector ﬁeld X on P+(I) and
the corresponding differential equation
˙p(t) = X

p(t)

,
p(0) = p0.
(6.89)
In order to approximate the solution p(t) with the Euler method, we choose a small
step size δ > 0 and consider the iteration
pδ := p + δX(p).
(6.90)
Starting with p0, we can sequentially deﬁne the points pδ
k+1 := pδ
k +δX(pδ
k) as long
as they stay in the simplex P+(I). These points approximate the solution p(t) of
(6.89) at time points tk = kδ, that is, pδ
k ≈p(tk). Clearly, the smaller δ is, the better
this approximation will be, and, assuming sufﬁcient regularity, pδ
k will converge to
p(t) for δk →t. Furthermore, we have
lim
pδ
k+1 −pδ
k
δ
= limX

pδ
k

= X

p(t)

= ˙p(t)
for δk →t.
(6.91)
This idea is quite natural and has been used in many contexts, including evolu-
tionary biology [123].
Let us interpret the iteration (6.90) from the information-geometric perspective.
Obviously, we obtain the new point pδ by moving a small step along the mixture
geodesic that starts at p and has the velocity X(p). We can rewrite (6.90) in terms
of the corresponding exponential map as
pδ = exp(m)
p

δX(p)

.
(6.92)
Clearly, from the information-geometric perspective any α-connection can be used,
so that we can extend the iteration (6.92) to the following family of iterations:
pδ = exp(α)
p

δX(p)

.
(6.93)
Starting with p0, we can again sequentially deﬁne pδ
k+1 := exp(α)
pδ
k (δX(pδ
k)), but
this time applying the general iteration (6.93) for any α-connection. Accordingly, in

334
6
Fields of Application of Information Geometry
order to express the difference on the LHS of (6.91), we have to use the inverse of
the exponential map:
lim 1
δ exp(α)
pδ
k
−1
pδ
k+1

= limX

pδ
k

= X

p(t)

= ˙p(t)
for δk →t.
(6.94)
Consider now a discrete time dynamical system on P+(I)
p′ = F(p) = p + X(p),
(6.95)
where we set X(p) := F(p) −p. How can we obtain a differential equation from
this? Following a frequently used rule, one simply reinterprets differences as ve-
locities. More precisely, one considers the difference equation p′ −p = X(p) and
then replaces the difference p′ −p on the LHS by the time derivative ˙p. In what
follows, we justify this simple rule by reversing the Euler method and using it for
the deﬁnition of a continuous time limit. In order to do so, we follow the Euler
method step by step. First, we modify the dynamics (6.95) by not taking a full step
in the direction X(p) but a step of size δ. This leads to the iteration formula (6.90),
although, at this point, the vector ﬁeld X(p) is interpreted as a difference and not
as a velocity. We then deﬁne the sequence pδ
k, k = 0,1,2,..., which we interpret
as the Euler approximation of a path p(t) that we want to obtain in the continuous
time limit. In order to get the velocity of that path, we ﬁnally evaluate the limit on
the LHS of (6.91) which gives us the desired differential equation. Clearly, this pro-
cedure amounts to the above-mentioned commonly used simple rule by which one
interprets differences as velocities.
The same procedure also works with the α-connections. However, in this more
general setting, we have to replace the difference p′ −p by the inverse of the ex-
ponential map. This ﬁnally leads to the following correspondence between discrete
time and continuous time dynamics, deﬁned in terms of the α-connection:
p′ = F(p)
∇(α)
←→
˙p = exp(α)
p
−1
F(p)

.
(6.96)
Now let us have a closer look at this correspondence for replicator equations. Let us
ﬁrst consider the discrete time replicator equation
p′
i = pi
wi(p)
	
j pjwj(p),
(6.97)
where wi(p) is referred to as the Wrightian ﬁtness of type i within the population p.
Using the α-connection, we can associate a continuous time replicator equation
˙pi = pi

m(α)i(p) −

j
pj m(α)j(p)

,
(6.98)
where m(α)i(p) is known as the Malthusian ﬁtness of i within p. Clearly, the Wrigh-
tian ﬁtness function w of the discrete time dynamics translates into different Malthu-
sian ﬁtness functions m(α) for the corresponding continuous time replicator dynam-
ics, depending on the afﬁne connection that we use for this translation. For α ̸= ±1,

6.2
Evolutionary Dynamics
335
we can specify the continuous time replicator equation up to a factor c, depending
on p and w:
˙pi = c pi

wi(p)
	
j pjwj(p)
 1−α
2
−

k
pk

wk(p)
	
j pjwj(p)
 1−α
2 
.
(6.99)
This follows from (2.65), applied to νi
μi = p′
i
pi =
wi(p)
	
j pj wj (p) (see the discrete time
replicator equation (6.97)). Let us highlight the cases α = −1 and α = 1, where we
can be more explicit by using (2.104). For α = −1, we obtain as continuous time
replicator equation
˙pi = pi

wi(p)
	
j pjwj(p) −1

,
(6.100)
and for α = 1 we have
˙pi = pi

logwi(p) −

j
pj logwj(p)

.
(6.101)
For the corresponding Malthusian ﬁtness functions of the replicator equation (6.98)
this implies
m(−1)i =
wi(p)
	
j pjwj(p) + const.,
m(1)i = logwi(p) + const.
(6.102)
A number of important continuous time models have been derived and studied in
[123] based on the transformation with α = −1. Also the log transformation be-
tween Wrightian and Malthusian ﬁtness, corresponding to α = 1, has been high-
lighted in [208, 259]. Even though the two notions of ﬁtness can be translated into
one another, it is important to carefully distinguish them from each other. When
equations are derived, explicit evolutionary mechanisms have to be incorporated.
For instance, linear interactions in terms of a matrix A = (aij)ij∈I are frequently
used in order to model the interactions of the types. Clearly, it is important to spec-
ify the kind of ﬁtness when making such linearity assumptions. Finally, we want
to mention that replicator equations also play an important role in learning theory.
They are obtained, in particular, as continuous time limits of discrete time learning
algorithms within reinforcement learning [51, 231, 250].
In the next section, we shall model evolution as a stochastic process on the sim-
plex P+(I). In general, such a process will no longer be described in terms of an
ordinary differential equation. Instead, a partial differential equation, the Fokker–
Planck equation, will describe the time evolution of a density function on the sim-
plex. This extension requires a continuous time limit that is more involved than the
one we have considered above. In particular, the transition from discrete to contin-
uous time has to be coupled with the transition from ﬁnite to inﬁnite population
sizes.

336
6
Fields of Application of Information Geometry
6.2.3 Population Genetics
In this section, we shall describe how the basic model of population genetics, the
Wright–Fisher model,3 can be understood from the perspective of information ge-
ometry, following [124], to which we refer for more details. This model read-
ily incorporates the effects of selection and mutation that we have investigated in
Sect. 6.2.3, but its main driver is another effect, random sampling. Random sam-
pling is a stochastic effect of ﬁnite populations, and therefore we can no longer
work with a system of ordinary differential equations in an inﬁnite population. We
shall perform an inﬁnite population limit, nevertheless, but we shall need a careful
scaling relation between population size and the size of the time step when pass-
ing from discrete to continuous dynamics (cf. Sect. 6.2.2). Furthermore, the random
sampling will lead us to parabolic partial differential equations in place of ordi-
nary ones, and it will contribute a second-order term in contrast to the replicator
equations that only involved ﬁrst derivatives.
Different from [124], we shall try to avoid introducing biological terminology
as far as possible, in order to concentrate on the mathematical aspects and to facil-
itate applications in other ﬁelds of this rather basic model. In particular, we shall
suppress biological aspects like diploidy that are not relevant for the essence of the
mathematical structure. Also, the style will be more narrative than in the rest of this
book, because we only summarize the results with the purpose of putting them into
the perspective of information geometry.
In order to put things into that perspective, let us start with some formal remarks.
Our sample space will be the simplex Σn = P(I). That is, we have a state space
of n + 1 possible types (called alleles in the model). We then have a population,
ﬁrst consisting of N members, and then, after letting N →∞, of inﬁnitely many
members. At every time (discrete m in the ﬁnite population case, continuous t in the
inﬁnite case), every member of the population has some type, and therefore, the rela-
tive frequencies of the types in the population yield an element p(m) or p(t) of Σn.
Thus, as in Sect. 6.2.1, p ∈Σn is interpreted here as a relative frequency, and not as
a probability, but for the mathematical formalism, this will not make a difference.
The Fisher metric will be the metric on Σn that we have constructed by identifying
it with the positive spherical sector Sn
+. (Since we identify a point p in Σn with a
probability distribution, there is no need to introduce a parameter ξ as in Sect. 6.3
here. But the metric is simply the Fisher metric on the parameter space, as always.)
Also, the individual members of the population will play no role. Just the relative
frequencies of the types across the population will be of interest. There is a second
level, however. The Kolmogorov forward or Fokker–Planck equation to be derived
3The Fisher here is the same Ronald Fisher after whom the Fisher metric is named. It is an ironic
fact that the Fisher metric that emerged from his work in parametric statistics is also useful for
understanding a model that he contributed to in his work on population genetics, although he
himself apparently did not see that connection.

6.2
Evolutionary Dynamics
337
below (see (6.117)) will yield a (probability) density f over these p ∈Σn. This f
will be the prime target of our analysis. f will satisfy the diffusion equation (6.117),
a Fokker–Planck equation. Alternatively, we could consider a Langevin stochastic
differential equation for a sample trajectory on Σn. The random contribution, how-
ever, will not be the Brownian motion for the Fisher metric, as in Sect. 6.3.1. Rather,
the diffusion term will deviate from Brownian motion by a ﬁrst-order term. This has
the effect that, in the absence of mutation effects, the dynamics will always end up
in one of the corners of the simplex. In fact, in the basic model without mutation or
selection, when the dynamics starts at (p0,p1,...,pn) ∈Σn, it will end up in the
ith corner with probability pi (see, e.g., [124] for details).
Now, let us begin with the model. In each generation m, there is a population
of N individuals. Each individual has a type chosen from the n + 1 possible types
{0,1,...,n}. Let Yi(m) be the number of individuals of type i in generation m, and
put pi(m) = Yi(m)
N
. Thus
n

i=0
Yi(m) = N
and
n

i=0
pi(m) = 1.
(6.103)
Given generation m, generation m + 1 is created by random sampling with replace-
ment from generation m. That is, expressing it in a rather non-biological manner, for
each individual in generation m + 1, randomly a parent is chosen in generation m,
and the individual inherits its parent’s type. (Thus, formally, the pi(m) now become
probabilities whereas the relative frequencies pi(m + 1) in the next generation are
random variables.) The transition probability of the process from generation m to
generation m + 1 is given by the multinomial formula
P

Y(m + 1) = y
Y(m) = η

=
N!
y0!y1!···yn!
n
0
i=0
ηi
N
yi
,
(6.104)
because this gives the probability for the components yi of Y(m + 1), that is, that
each type i is chosen yi times given that in generation m type i occurred ηi times,
that is, had the relative frequency ηi
N . The same formula applies to the transition
probabilities for the relative frequencies, P(p(m + 1) = p|p(m) = π) for π = η
N .
Because of the normalizations (6.103), it sufﬁces to consider the evolution of the
components i = 1,...,n, because those then also determine the evolution of the 0th
component.
Because we draw N times independently from the same probability distri-
bution pi(m), we may apply (2.23), (2.24) to get the expectation values and
(co)variances
Ep(m)

pi(m + 1)

= pi(m),
(6.105)
Covp(m)

pi(m + 1)pj(m + 1)

= pi(m)

δij −pj(m)

.
(6.106)

338
6
Fields of Application of Information Geometry
Of course, we could also derive these expressions from the general identity
Ep(m)

p(m + 1)a
=

p
paP

p(m + 1) = p
p(m)

(6.107)
for a multi-index a and (6.104).4
We are interested in the dynamics of the type distribution across generations. The
Chapman–Kolmogorov equation
P(m + 1,y0,y)
=

y1,...,ym
P

Y(m + 1) = y
Y(m) = ym

P

Y(m) = ym
Y(m −1) = ym−1

···P

Y(1) = y1
Y(0) = y0

(6.108)
then yields the probabilities for ﬁnding the type distribution y in generation m + 1
when the process had started with the type distribution y0 in generation 0.
We now come to the important step, the continuum limit N →∞. In this limit,
the transition probabilities will turn into differential equations. In order to carry out
this limit, we also need to rescale time,
t = m
N ,
hence δt = 1
N
(6.109)
and also introduce the rescaled variables
Qt = p(Nt) = Y(Nt)
N
and
δQt = Qt+δt −Qt.
(6.110)
From (6.105)–(6.107), we get
E

δQi
t

= 0,
(6.111)
E

δQi
tδQj
t

= Qi
t

δij −Qj
t

δt,
(6.112)
E

δQa
t

= o(δt)
for a multi-index a with |a| ≥3.
(6.113)
In fact, there is a general theory about the passage to the continuum limit which
we shall now brieﬂy describe. We consider a process Q(t) = (Qi(t))i=1,...,n with
values in U ⊆Rn and t ∈(t0,t1) ⊆R, and from discrete time t we want to pass
to the continuum limit δt →0. We thus consider N →∞with δt = 1
N . We shall
assume the following general conditions
lim
δt→0
1
δt Eδt

δQiQ(t) = q

= bi(q,t),
i = 1,...,n
(6.114)
4Of course, we employ here the standard multi-index convention: For a = (a0,...,an), we have
pa = (pa0
0 ,...,pan
n ).

6.2
Evolutionary Dynamics
339
and
lim
δt→0
1
δt Eδt

δQiδQjQ(t) = q

= aij(q,t),
i,j = 1,...,n,
(6.115)
with a positive semideﬁnite and symmetric matrix aij for all (q,t) ∈U × (t0,t1). In
addition, we need that the higher order moments can be asymptotically neglected,
that is, for all (q,t) ∈U × (t0,t1) and all multi-indices a = (a1,...,an) with |a| =
	ai ≥3, we have
lim
δt→0
1
δt Eδt

(δQ)aQ(t) = q

= 0.
(6.116)
In the limit N →∞with δt = 1
N , that is, for an inﬁnite population evolving in con-
tinuous time, the probability density Φ(p,s,q,t) :=
∂n
∂q1···∂qn P(Q(t)≤q|Q(s) = p)
with s < t and p,q ∈Ω then satisﬁes the Kolmogorov forward or Fokker–Planck
equation
∂
∂t Φ(p,s,q,t) = 1
2
n

i,j=1
∂2
∂qi∂qj

aij(q,t)Φ(p,s,q,t)

−
n

i=1
∂
∂qi

bi(q,t)Φ(p,s,q,t)

=: LΦ(p,s,q,t)
(6.117)
and the Kolmogorov backward equation
−∂
∂s Φ(p,s,q,t) = 1
2
n

i,j=1
aij(p,s)
∂2
∂pi∂pj Φ(p,s,q,t)
+
n

i=1
bi(p,s) ∂
∂pi Φ(p,s,q,t)
=: L∗Φ(p,s,q,t).
(6.118)
Note that time is running backwards in (6.118), which explains the minus sign in
front of the time derivative in the Kolmogorov backward equation. While the proba-
bility density function Φ depends on two points (p,s) and (q,t), either Kolmogorov
equation only involves derivatives with respect to one of them. When considering
classical solutions, Φ needs to be of class C2 with respect to the relevant spatial
variables in U and of class C1 with respect to the relevant time variable.
Thus, in the situation of the Wright–Fisher model, that is, with (6.112), (6.111),
the coefﬁcients in (6.117) are
aij(q) = qi
δij −qj
and
bi(q) = 0,
(6.119)

340
6
Fields of Application of Information Geometry
and analogously for (6.118). In particular, the coefﬁcients of the second-order term
are given by the Fisher metric on the probability simplex. The coefﬁcients of the
ﬁrst-order term vanish in this case, but we shall now consider extensions of the
model that lead to nontrivial bi.
For that purpose, we replace the multinomial formula (6.104) by
P

Y(m + 1) = y
Y(m) = η

=
N!
y0!y1!···yn!
n
0
i=0
ψi(η)yi.
(6.120)
This simply means that before the sampling, the values η0,...,ηn are subjected to
certain effects that turn them into new values ψ0(η),...,ψn(η) from which the next
generation is sampled. This is intended to include the biological effects of mutation
and selection, as we shall now brieﬂy explain. We begin with mutation. Let μij
be the fraction of type Ai that randomly mutates into type Aj in each generation
before the sampling for the next generation takes place. We put μii = 0.5 Then ηi
N
in (6.104) needs to be replaced by
ψi
mut(η) :=
ηi −	n
j=0 μijηi + 	n
j=0 μjiηj
N
,
(6.121)
to account for the net effect of Ai changing into some other Aj and conversely,
for some Aj turning into Ai, as in (6.87). The mathematical structure below will
simplify when we assume
μij =: μj
for all i ̸= j,
(6.122)
that is, the mutation rate depends only on the target type. Equation (6.121) then
becomes
ψi
mut(η) =
(1 −	n
j=0,j̸=i μj)ηi + ϑi
	n
j=0 ηj
N
.
(6.123)
This thus is the effect of changing type before sampling. The other effect, selection,
is mathematically expressed as a sampling bias. This means that each type gets a
certain ﬁtness, and the types with higher ﬁtness are favored in the sampling process.
Thus, when type Ai has ﬁtness si, and when for the moment we assume that there
are no mutations, we have to work with the Wrightian ﬁtness
ψi
sel(η) :=
siηi
	n
j=0 sjηj .
(6.124)
When all si are the same, that is, when there are no ﬁtness differences, we are back
to (6.104).
5Note that this is different from the convention adopted in Sect. 6.2.1 where the mutation rates mij
satisﬁed mii = 1 −	
j̸=i mij .

6.2
Evolutionary Dynamics
341
For the scaling limit, we assume that the mutation rates satisfy
μij = O
 1
N

(6.125)
and that the selection coefﬁcients are of the form
si = 1 + σi
with σi = O
 1
N

.
(6.126)
With
mij := Nμij,
mi := Nμi
and
vi := Nσi
(6.127)
we then have6
ψi(η) = 1
N

ηi

1 + vi −

j
vjηj

−

j
mijηi +

j
mjiηj

+ o
 1
N

.
(6.128)
Equations (6.111)–(6.113) then turn into
E

δQi
t

= ψi(q) −q = 1
N

qi

vi −

j
vjqj

−

j
mijqi +

j
mjiqj

+ o
 1
N

=: 1
N bi(q) + o
 1
N

,
(6.129)
E

δQi
tδQj
t

= 1
N qi
δij −qj
+ o
 1
N

=: 1
N aij(q) + o
 1
N

,
(6.130)
and
E

δQa
t

= o(δt)
for a multi-index a with |a| ≥3.
(6.131)
Thus, the second and higher moments are asymptotically not affected by the effects
of mutation and selection, but the ﬁrst moments now are no longer 0. In any case,
we have the Kolmogorov equations (6.117) and (6.118).
In a ﬁnite population, it may happen that one of the types disappears from the
population when it is not chosen during the sampling process. And in the absence
of mutation, it will then remain extinct forever. Likewise, in the scaling limit, it may
happen that one of the components qi becomes 0 after some ﬁnite time, and, again
6Noting again that here we employ the convention mii = 0 which is different from that used in
Sect. 6.2.1.

342
6
Fields of Application of Information Geometry
in the absence of mutations, will remain 0 ever after. In fact, almost surely, after
some ﬁnite time, only one type will survive, while all others go extinct.
There is another approach to the Kolmogorov equations that does not start with
the second terms with coefﬁcients aij(q) and considers the ﬁrst-order terms with
coefﬁcients bi(q) as a modiﬁcation of the dynamics of the parabolic equation
∂
∂t Φ(p,s,q,t) = 1
2
	n
i,j=1
∂2
∂qi∂qj (aij(q,t)Φ(p,s,q,t)) or of the antiparabolic
equation −∂
∂s Φ(p,s,q,t) = 1
2
	n
i,j=1 aij(p,s)
∂2
∂pi∂pj Φ(p,s,q,t), but that rather
starts with the ﬁrst-order terms and adds the second-order terms as perturbations
caused by noise. This works as follows (see, e.g., [140, Chap. 9] or [141, Sect. 4.5]).
In fact, this starting point is precisely the approach developed in Sect. 6.2.1. We start
with the dynamical system
dqi(t)
dt
= bi(q)
for i = 1,...,n.
(6.132)
The density u(q,t) of q(t) then satisﬁes the continuity equation
∂
∂t u(q,t) = −
n

i=1
∂
∂qi

bi(q)u(q,t)

= −div(bu),
(6.133)
that is, (6.117) without the second-order term. We then perturb (6.132) by noise and
consider the system of stochastic ODEs
dqi(t) = bi(q)dt +

j
aij(q)dWj(t),
(6.134)
where dWj(t) is white noise, that is, the formal derivative of Brownian motion
Wj(t). The index j = 1,...,n only has the role to indicate that in (6.134), we have
n independent scalar Brownian motions whose combined effect then yields the term
	
j aij(q)dWj(t). In that case, the density u(q,t) satisﬁes
∂
∂t u(q,t) = 1
2
n

i,j=1
∂2
∂qi∂qj

aij(q,t)u(q,t)

−
n

i=1
∂
∂qi

bi(q,t)u(q,t)

,
(6.135)
that is, (6.117). Thus, the deterministic component with the law (6.132) causes the
ﬁrst-order term, also called a drift term in this context, whereas the stochastic com-
ponent leads to the second-order term, called a diffusion term. In this interpretation,
(6.135) is the equation for the probability density of a particle moving under the
combined inﬂuence of a deterministic law and a stochastic perturbation. When the
particle starts and moves in a certain domain, it may eventually hit the boundary of
that domain and disappear. In our case, the domain is the probability simplex, and
hitting the boundary means that one of the components qi, i = 0,...,n, becomes 0.
That is, in our model, one of the types disappears from the population. We can then
ask for such quantities as the expected time for a trajectory starting somewhere in

6.2
Evolutionary Dynamics
343
the interior to ﬁrst hit the boundary. It turns out that this expected exit time t(p)
for a trajectory starting at p satisﬁes an inhomogenous version of the stationary
Kolmogorov backward equation (6.118), namely
L∗Φ

t(p)

= −1.
(6.136)
This equation now has a natural solution from the perspective of information ge-
ometry. In fact, in the setting of Sect. 4.2, we had obtained the (Fisher) metric as
the second derivatives of a potential function, see (4.32). Therefore, we have, in the
notation of that equation,
n

i,j=1
gij∂i∂jψ = n.
(6.137)
Since the coordinates we are currently employing are the afﬁne coordinates pi, the
potential function is the negative entropy
S(p) =

k
pk logpk,
(6.138)
and, recalling (2.21), (2.22),
gij =
∂2
∂pi∂pj
S(p) =
δij
pi
+ 1
p0

,
gij = pi(δij −pj).
(6.139)
Thus, we ﬁnd
n

i,j=1
pi(δij −pj)
∂2
∂pi∂pj

k
pk logpk

= n
(6.140)
and hence from (6.136), (6.118), we obtain
Theorem 6.4 In the case bi(q) = 0, the expected exit time is
t(p) = −2
n

k
pk logpk,
(6.141)
that is, up to constant factor, the entropy of the original type distribution.
The approach can be iteratively reﬁned, and one can compute formulae not only
for the expected time t(p) when the ﬁrst type gets lost from the population, but also
for the expected times of subsequent allele losses. Likewise, formulae exist for the
case where the drift terms bi do not vanish. We refer to [95, 124] and the references
given there.
We shall now turn to the case of non-vanishing drift coefﬁcients bi and introduce
a free energy functional, following [124, 246, 247]. The key is to rewrite the Kol-
mogorov forward equation (6.117), or equivalently (6.135), in divergence form, that

344
6
Fields of Application of Information Geometry
is, in the form
∂
∂t u(q,t) = ∇·

A(q)∇u(q,t)

−∇·

A(q)u(q,t)∇γ (q)

,
(6.142)
with
∇=
 ∂
∂q1 ,..., ∂
∂qn

.
In order to bring (6.135) into this form, we put
A(q) =

Aij(q)
n
i,j=1 = 1
2

aij(q)
n
i,j=1,
and write
∂
∂t u(q,t) =
n

i=1
∂
∂qi

n

j=1
∂
∂qj
aij(q)
2
u(q,t)

−
n

i=1
∂
∂qi

bi(q)u(q,t)

=
n

i=1
∂
∂qi

n

j=1

Aij(q) ∂
∂qj u(q,t)

+
n

i=1
∂
∂qi

n

j=1
∂
∂qj Aij(q) −bi(q)

u(q,t)

=
n

i=1
∂
∂qi

n

j=1

Aij(q) ∂
∂qj u(q,t)

+
n

i=1
∂
∂qi
1 −(n + 1)qi
2
−bi(q)

u(q,t)

.
(6.143)
Comparing (6.142) and (6.143), we see that γ has to satisfy

A(q)∇γ (q)

i = 1 −(n + 1)qi
2
−bi(q)
and hence
∂
∂qi γ (q) = −
n

j=1
2
δij
qj + 1
q0
1 −(n + 1)qj
2
−bj(q)

= −1 −2bi(q)
qi
+ 1 −2b0(q)
q0
.
(6.144)
(Note that with (6.129), that is, bi(q) = qi(vi −	
j vjqj)−	
j mijqi +	
j mjiqj
for i = 0,...,n, we have 	n
i=0 bi(q) = 0, and of course also 	n
i=0 qi = 1.)

6.2
Evolutionary Dynamics
345
A necessary and sufﬁcient condition for such a γ to exist comes from the Frobe-
nius condition (see (B.24), (B.25))
∂
∂qi
∂
∂qj γ (q) =
∂
∂qj
∂
∂qi γ (q), that is,
βi(q) := 1 −2bi(q)
qi
has to satisfy
∂
∂qj

βi(q) −β0(q)

= ∂
∂qi

βj(q) −β0(q)

for all i ̸= j.
(6.145)
From (6.129), we have
bi(q) = qi

vi −

j
vjqj

−

j
mijqi +

j
mjiqj.
(6.146)
Equation (6.145) then becomes7
−mji
qi + m0i
qi + mj0
q0 = −mij
qj + m0j
qj + mi0
q0 ,
for all i ̸= j,
that is,
mji = m0i
for all i ̸= j = 0,...,n,
which was our assumption (6.122) which thus is needed to get (6.145). In that case,
we get from (6.145), (6.146), recalling q0 = 1 −	n
j=1 qj, that
γ (q) = −
n

i=0
(1 −2mi)logqi +
n

i=0
viqi.
(6.147)
We then have
eγ (q) =
n
0
i=0

qi1−2mi
n
0
j=0
evj qj .
(6.148)
The region over which q varies is the probability simplex
Σn =


q0,...,qn
: qi ≥0,
n

i=0
qi = 1

,
(6.149)
and from (6.148), we see that

Σn eγ (q) dq < ∞
⇔
mi > 0
for all i.
(6.150)
Thus, we need positive mutation rates, and we now proceed with that assumption.
7In contrast to the situation in (2.38), here the ﬁtness coefﬁcients vi are assumed to be constant,
and the Frobenius condition here becomes a condition for the mutation rates mij .

346
6
Fields of Application of Information Geometry
Analogously to (4.71), we have the free energy
F

u(·,t)

= log

Σn eγ (q) dq
= −

Σn γ (q)u(q,t)dq +

Σn u(q,t)logu(q,t)dq.
(6.151)
Its time derivative along the ﬂow (6.142) satisﬁes
d
dt F

u(·,t)

= −

Σn γ (q) ∂
∂t u(q,t)dq +

Σn logu(q,t) ∂
∂t u(q,t)dq
since

∂
∂t u(q,t)dq = d
dt

u(q,t)dq = 0
= −

γ ∇(A∇u) +

γ ∇(Au∇γ ) +

logu∇(A∇u)
−

logu∇(Au∇γ )
= 2

∇γ A∇u −

∇γ Au∇γ −
 1
u∇uA∇u
integrating by parts
= −

A
1
u∇u −∇γ
2
≤0.
(6.152)
We have shown
Theorem 6.5 The free energy (6.151) decreases along the ﬂow (6.142).
From (6.152), we also see that
d
dt F(u(·,t)) = 0 precisely if u(q,t) =: u∞(q)
with logu∞(q) = γ (q) + const., where the constant is determined by

u = 1, that
is,
u∞(q) = eγ (q)
Z
with Z =

Σn eγ (q) dq.
(6.153)
Thus, the equilibrium solution u∞is given by a Gibbs type distribution. It is the
minimizer of the free energy and the maximizer of the entropy.
In order to analyze the convergence of u(q,t) to that equilibrium distribution
u∞(q), we put
h := u
u∞
and shall investigate the rate of convergence of h to 1. We have
logu∞−γ = −logZ.
(6.154)

6.2
Evolutionary Dynamics
347
Since Z is independent of q, this implies
∇(logu −γ ) = ∇

log u
u∞

+ ∇(logu∞−γ ) = ∇(logh).
(6.155)
We shall now derive the Kolmogorov backward equation for h.
Lemma 6.2
∂
∂t h = ∇· (A∇h) −∇ψ · A∇h = L∗h.
(6.156)
Proof We have
∂
∂t h = u−1
∞
∂
∂t u
= u−1
∞∇

Au∇(logu −γ )

= u−1
∞∇

Au∞h∇(logh)

= ∇

Ah∇(logh)

+ u−1
∞∇(u∞)

Ah∇(logh)

= ∇(A∇h) + ∇(logu∞)∇(A∇h)
= ∇·

A(q)∇h

+ ∇γ · A(q)∇h.
(6.157)
□
We can now compute the decay rate of the free energy functional towards its
asymptotic limit along the evolution of the probability density function u. For sim-
plicity, we shall write F(t) in place of F(u(·,t)), and F(∞) for F(u∞(·)).
Lemma 6.3
F(t) −F(∞) = DKL(u∥u∞) ≥0.
(6.158)
Proof
F(t) =

Σn u(logu −γ )dq
=

Σn u(logu∞−γ )dq +

Σn u(logu −logu∞)dq
=

Σn u(−logZ)dq +

Σn ulog u
u∞
dq
= −logZ +

Σn ulog u
u∞
dq
= −logZ +

Σn hlogh u∞dq

348
6
Fields of Application of Information Geometry
and from (6.154)
F(∞) =

Σn u∞(logu∞−γ )dq = −logZ.
□
From these computations, we also get the relation
DKL(u∥u∞) =

Σn hlogh u∞dq =: Su∞dq(h),
(6.159)
that is, the negative of the entropy of h w.r.t. the equilibrium measure u∞dq. We
can now reinterpret (6.152).
Lemma 6.4
d
dt Su∞dq(h) = d
dt F(t) = −

Σn
A(q)∇h · ∇h
h
u∞dq ≤0.
(6.160)
Proof Equation (6.160) follows from (6.152) and (6.155) and the fact that u∞and
hence also F(u∞) is independent of t.
□
6.3 Monte Carlo Methods
In this section, we present the application of the Fisher metric to Monte Carlo sam-
pling developed by Girolami and Calderhead [108]. We ﬁrst recall Markov chain
Monte Carlo sampling. Although this is a standard method in statistics, for instance,
for computing Bayesian posteriors, we present the basic ideas here in some detail
because not all of our readers may be familiar with them. We don’t discuss, however,
any issues of practical implementation, for which we refer to standard textbooks like
[167, 227].
Monte Carlo methods have been developed for the stochastic sampling of proba-
bility distributions. This is, for example, needed when one has to compute the inte-
gral over a probability distribution. For instance, the Bayesian scheme for comput-
ing the posterior π(ξ|x) for the parameter ξ from the prior π(ξ) and the likelihood
p(x;ξ) of the observed datum x,
π(ξ|x) = p(x;ξ)π(ξ)
p(x)
(6.161)
requires the computation of the integral
p(x) =

p(x;η)π(η)dη.
(6.162)
(Since the base measure will not play a role in this section, we denote it simply by
dη.) Typically, the distribution π(η) is not explicitly known. In order to approximate

6.3
Monte Carlo Methods
349
this integral, one therefore needs to sample the distribution. And in the case where
π(η) is not explicitly known, it turns out to be best to use a stochastic sampling
scheme. The basic idea is provided by the Metropolis algorithm [183] that when the
current sample is ξ0, one randomly chooses ξ1 according to some transition density
q(ξ1|ξ0) and then accepts ξ1 as the new sample with probability
min

1, ˜π(ξ1)
˜π(ξ0)

(6.163)
where
˜π(ξ) = p(x;ξ)π(ξ),
(6.164)
noting that since in (6.163) we are taking a ratio, we don’t need to know the normal-
ization factor p(x) and can work with the non-normalized densities ˜π rather than
the normalized π. Thus, ξ1 is always accepted when its probability is higher than
that of ξ0, but only with the probability ˜π(ξ1)
˜π(ξ0) when its probability is lower. When ξ1
is not accepted, another random sample is taken to which the same criterion applies.
The procedure is repeated until a new sample is accepted. The Metropolis–Hastings
algorithm [117] replaces the criterion (6.163) by
min

1, ˜π(ξ1)q(ξ0|ξ1)
˜π(ξ0)q(ξ1|ξ0)

(6.165)
where q(ξ1|ξ0) is the transition density for ﬁnding the putative new sample ξ1
when ξ0 is given. In many standard cases, this transition density is symmetric, i.e.,
q(ξ1|ξ0) = q(ξ0|ξ1), and (6.165) then reduces to (6.163). For instance, it may be
given by a normal distribution
N

ξ1;ξ0,Λ

(6.166)
with mean ξ0 and covariance Λ (see (1.44)). In any case, such a sampling procedure
yields a Markov chain since the transition to ξ1 depends only on the current state
ξ0 and not on any earlier ones. Thus, one speaks of Markov chain Monte Carlo
(MCMC).
Now, so far the scheme has been quite general, and it did not use any particular
information about the probability densities concerned. Concretely, one might try to
make the scheme more efﬁcient by incorporating the aim to ﬁnd samples with high
values of p or ˜p together with the geometry of the family of distributions at the
current sample ξ0. The ﬁrst aspect is addressed by the Langevin and Hamiltonian
Monte Carlo methods, and the second one is then utilized in [108].
Combining these aspects, but formulating it in abstract terms, we have a target
function L(ξ) that we wish to minimize and a metric g(ξ) with respect to which
we can deﬁne the gradient of L or Brownian motion as a stochastic tool for our
sampling. Here, we take
L(ξ) = −logπ(ξ)
(6.167)

350
6
Fields of Application of Information Geometry
and
g(ξ) = g(ξ),
(6.168)
but the scheme to be described will also work for other choices of L and g.
6.3.1 Langevin Monte Carlo
Here, we shall describe the geometric version of [108] of what is called the Metropo-
lis adjusted Langevin algorithm (MALA) in the statistical literature. We start with
the following observation. Sampling from the d-dimensional Gaussian distribution
N(x,σ 2Id) (where Id is the d-dimensional unit matrix) can be represented by Brow-
nian motion. Indeed, the probability distribution at time 1 for a particle starting at
x at time 0 and moving under the inﬂuence of white noise of strength σ is given
by N(x,σ 2Id) (see, for instance, [135]). This can also be written as a stochastic
differential equation for the d-dimensional random variable Y,
dY(t) = σdWd(t),
(6.169)
where Wd(t) is standard Brownian motion (of unit strength) on Rd. Here, t stands
for time. The random variable Y(1) with Y(0) = x is then distributed according to
N(x,σ 2Id). More generally, for a constant covariance matrix Λ = (λij), we have
the associated stochastic differential equation
dY i(t) = σ i
jdW j(t)
for i = 1,...,d,
(6.170)
where Σ = (σ i
j) is the positive square root of the positive deﬁnite matrix Λ−1, and
the W j(t),j = 1,...,d, are independent standard scalar Brownian motions. Again,
when Y(0) = x, Y(1) is distributed according to the normal distribution N(x,Λ).
While (6.169) generates Brownian motion on Euclidean space, when we look
at a variable ξ parametrizing a probability distribution, we should take the corre-
sponding Fisher metric rather than the Euclidean one, and consider the associated
Brownian motion. In general, Brownian motion on a Riemannian manifold with
metric tensor (gij(ξ)) in local coordinates is generated by the Langevin equation
dΞi(t) = −
1
2√detg(Ξ)
∂
∂ξj

detg(Ξ)gij(Ξ)

dt + σ i
jdW j(t)
= −1
2gjk(Ξ)Γ i
jk(Ξ)dt + σ i
jdW j(t),
(6.171)
where (σ i
j) now is the positive square root of the inverse metric tensor (gij), detg
is the determinant of gij and
Γ i
jk = 1
2giℓ
 ∂
∂ξk gjℓ+
∂
∂ξj gkℓ−∂
∂ξℓgjk

(6.172)

6.3
Monte Carlo Methods
351
are the Christoffel symbols (see (B.48)). See [126], p. 87. Since the metric tensor in
general is not constant, in this equation in addition to the noise term dW, we also
have a drift term involving derivatives of the metric tensor.
In particular, we may apply this to the Fisher metric g and obtain the stochastic
process corresponding to the covariance matrix given by the Fisher metric. Again,
the Fisher metric here lives on the space of parameters ξ, that is, on the space of
probability distributions p(·;ξ) parametrized by ξ.
Also, when we have a function L(ξ) that we wish to minimize, as in (6.167),
then we can add the negative gradient of L to the ﬂow (6.171) and consider
dΞi(t) = −gradL(Ξ)dt −1
2gjk(Ξ)Γ i
jk(Ξ)dt + σ i
jdW j(t).
(6.173)
In this manner, when we take L as in (6.167) and g as the Fisher metric, we obtain a
stochastic process that incorporates both our wish to increase the log-probability of
our parameter ξ and the fact that the covariance matrix should be given by the Fisher
metric. We can then sample from this process and obtain geometrically motivated
transition probabilities q(ξ1|ξ0) for the Metropolis–Hastings algorithm as in (6.165).
We may also consider (6.173) as a gradient descent for L perturbed by Brownian
motion w.r.t. our Riemannian metric. The metric here appears twice, in the deﬁnition
of the gradient as well as in the deﬁnition of the noise. In our setting, working on a
space of probability distributions parametrized by our variable ξ, we have a natural
Riemannian metric, the Fisher metric on that space. The scheme presented here is
more general. It applies to the optimization of any function L and works with any
Riemannian metric, but the question may then arise which such metric to choose.
6.3.2 Hamiltonian Monte Carlo
Here, we shall describe the geometric version of [108] of what is called the Hamil-
tonian or hybrid Monte Carlo method. In Hamiltonian Monte Carlo, as introduced
in [85], in contrast to Langevin Monte Carlo as described in the previous section,
the stochastic variable is not ξ itself, but rather its momentum. That is, ξ evolves
according to a deterministic differential equation with a stochastically determined
momentum m. Again, we start with the simplest case without a geometric structure
given by a Riemannian metric g or an objective function L. Thus, we consider a
d-dimensional constant covariance matrix Λ. We deﬁne the Hamiltonian
H(ξ,m) := 1
2miλijmj + 1
2 log

(2π)d detΛ

.
(6.174)
(At this moment, H does not yet depend on ξ, but this will change below.) Here, the
last term, which is constant, simply ensures the normalization

exp

−H(ξ,m)

dm = 1.
(6.175)

352
6
Fields of Application of Information Geometry
The associated system of Hamilton’s equations is
dξ
dτ = ∂H
∂m = Λ−1m,
(6.176)
dm
dτ = −∂H
∂ξ = 0.
(6.177)
Here τ is a time variable; since the process is different from the Langevin process
which involved a ﬁrst-order stochastic differential equation for ξ, whereas here from
(6.176), (6.177), we get the second-order equation d2ξ
dτ 2 = 0. We denote time here by
a different letter, τ instead of t.
Thus, here we take a ﬁxed initial value for ξ, say ξ(0) = ξ0, whereas we sample
m from the Gibbs distribution with Hamiltonian H, that is,
π(m) = exp

−H(ξ,m)

.
(6.178)
(We shall denote the probability distributions for ξ and its momentum m by the
same letter π, although they are of course different from each other.)
Again, we now extend this scheme by replacing the constant covariance matrix
Λ by a Riemannian metric tensor G = (gij(ξ)), having again the Fisher metric g in
mind, as we wish to interpret ξ as the parameter for a probability distribution. We
introduce an objective function L(ξ), as in (6.167). We then have the Hamiltonian
H(ξ,m) := L(ξ) + 1
2migij(ξ)mj + 1
2 log

(2π)d detg(ξ)

(6.179)
and the Gibbs density
1
Z exp

−H(ξ,m)

(6.180)
with the normalization factor Z =

exp(−L(η))dη. We note that for the choice
(6.167), L(ξ) = −logπ(ξ), we have
Z =

π(η)dη = 1.
(6.181)
The associated Hamilton equations now are
dξi
dτ = ∂H
∂mi
= gijmj,
dmi
dτ = −∂H
∂ξi = −∂L(ξ)
∂ξi
−1
2tr

gjk(ξ)∂gjk(ξ)
∂ξi

+ 1
2mjgjr(ξ)∂grs(ξ)
∂ξi
gskmk,
or in abbreviated form
dξi
dτ = ∂H
∂mi
= G−1m,
(6.182)
dmi
dτ = −∂H
∂ξi = −∂L
∂ξi −1
2tr

G−1 ∂G
∂ξi

+ 1
2mTG−1 ∂G
∂ξi G−1m. (6.183)

6.3
Monte Carlo Methods
353
Hamiltonian systems have a couple of useful properties (for more details, see, for
instance, [144]):
1. The Hamiltonian H(ξ(τ),m(τ)) remains constant in time:
d
dτ H

ξ(τ),m(τ)

= ∂H
∂ξ
dξ
dτ + ∂H
∂m
dm
dτ = −dm
dτ
dξ
dτ + dξ
dτ
dm
dτ = 0
by (6.182), (6.183).
2. The volume form dξ(τ)dm(τ) of phase space remains constant. This follows
from the fact that the vector ﬁeld deﬁned by the Hamiltonian equations is diver-
gence free:
d
dξ
dξ
dτ + d
dm
dm
dτ = ∂2H
∂ξ∂m −∂2H
∂m∂ξ = 0.
3. More strongly, the ﬂow τ →(ξ(τ),m(τ)) is symplectic: With z = (ξ,m),
(6.182), (6.183) can be written as
dz
dτ = J∇H

z(τ)

(6.184)
with
J =
 0
Id
−Id
0

where 0 and Id stand for the d × d-dimensional zero and unit matrix.
4. In particular, the ﬂow τ →(ξ(τ),m(τ)) is reversible.
Again, one starts with ξ(0) = ξ0 and samples the initial values for m according to
the Gibbs distribution (6.180). One accepts the state (ξ(1),m(1)) with probability
min(1, exp(−H(ξ(1),m(1)))
exp(−H(ξ(0),m(0)))).
The stationary density ¯π(ξ,m) is then given by 1
Z exp(−H(ξ,m)). When we
choose L(ξ) = −logπ(ξ), that is, (6.167), and recall (6.181), the stationary density
for ξ, obtained by marginalization w.r.t. m is
¯π(ξ) =

exp

−H(ξ,m)

dm = exp

−L(ξ)

= π(ξ),
(6.185)
that is, it agrees with the original density π(ξ).
In order to compare the Langevin and the Hamilton approach, we differentiate
the ﬁrst of the Hamilton equations w.r.t. τ and then insert the second, that is,
d2ξi
dτ 2 = ∂gij
∂ξk
dξk
dτ mj + gij dmj
dτ
= −gij ∂L
∂ξj −1
2gijtr

gℓk ∂gℓk
∂ξj


354
6
Fields of Application of Information Geometry
−gij ∂gjk
∂ξs gkℓgsrmrml + 1
2gijgℓk ∂gks
∂ξj gsrmℓmr
= −(gradL)i −1
2gijtr

gℓk ∂gℓk
∂ξj

−Γ i
ksgkℓgsrmℓmr,
(6.186)
where we have repeatedly exchanged bound indices and used the symmetry
gkℓgsrmrml = gsℓgkrmrml, in order to make use of (6.172) (that is, (B.48)).
When we compare (6.186) with (6.173), we see that in (6.186), the gradient of
L enters into the equations for the second derivatives of ξ w.r.t. τ, but in (6.173), it
occurs in the equations for the ﬁrst derivatives of ξ w.r.t. t. Thus, the role of time,
t vs. τ, is different in the two approaches. (This was, in fact, already observed at
the beginning of this section, when we analyzed (6.176), (6.177).) Modulo this dif-
ference, the Christoffel symbols enter in the same manner into the two equations.
Also, the momenta m are naturally covectors, and that is why they carry lower in-
dices. vk = gkℓmℓthen are the components of a vector (see (B.21)).
6.4 Inﬁnite-Dimensional Gibbs Families
We now wish to put some information-geometric constructions, in particular those
discussed in Sect. 4.3, into the context of the Gibbs families in statistical mechan-
ics.8 This section has a more informal character than other sections in this book,
because it presents heuristic constructions that can only be made rigorous with tech-
niques different from those developed here.
In statistical mechanics, one wishes to maximize the entropy
H(p) (= −ϕ) = −

p(x)logp(x)
(6.187)
for a probability distribution, i.e.,

p(x) = 1
(6.188)
subject to the constraints that the expectation values of the functions fi, i = 1,...,n,
are ﬁxed,
Ep(fi) =

fi(x)p(x) = f 0
i =: ηi.
(6.189)
8It turns out that our sign conventions are different from those of statistical mechanics, as our
convex potential function φ is the negative of the entropy, which would therefore be concave.
Likewise, our ψ is the negative of the free energy of statistical mechanics. This then also makes
some subsidiary signs different. We shall not change our general sign conventions here, for reasons
of consistency.

6.4
Inﬁnite-Dimensional Gibbs Families
355
One then introduces Lagrange multipliers ϑi, i = 0,...,n, and looks for extrema
of
Hϑ(p) := −

p(x)logp(x) −
n

i=1
ϑi

fi(x)p(x) −ηi

−ϑ0

p(x) −1

.
(6.190)
The solution is the probability distribution
p(x;ϑ) = exp

fi(x)ϑi −ψ(ϑ)

=
1
Z(ϑ) exp

fi(x)ϑi
(6.191)
with
ψ(ϑ) := ϑ0 + 1 = log

exp

fi(x)ϑi
(6.192)
and the partition function
Z(ϑ) = expψ(ϑ) =

exp

fi(x)ϑi
,
(6.193)
and the values of the Lagrange multipliers ϑi, i = 1,...,n, being determined by
ηj =
∂
∂ϑj log

exp

fi(x)ϑi
=:
∂
∂ϑj ψ(ϑ).
(6.194)
In particular, the entropy of the maximizing distribution gets smaller, the more con-
straints are added, i.e., the more observations fi are to be reproduced (see for in-
stance [130]). In particular, if in addition to the expected values of these functions fi,
we also require to include the expected values of their products in the case where
they are correlated, this further decreases the entropy of the resulting Gibbs distri-
bution in line with what we have deduced above. Of course, the more observations
are available, the smaller the uncertainty in the worst possible case, i.e., the entropy
of the Gibbs distribution.
In the context of statistical mechanics, the function ψ is the negative of the free
energy. Thus, although it might be somewhat misleading to speak about the free
energy here, since we have several observables fi, none of them being distinguished
as the energy, we shall nevertheless call it by that name. In any case, that free energy
depends on the choice of observables. This is in contrast to many contexts in physics
where the potential energy can be assumed to be a physical quantity whose value or
meaning does not depend on any such choices.
In conclusion, we see that the entropy and the free energy are dual to each other.
The Lagrange multipliers ϑi are the derivatives of the negative entropy with respect
to the expectation values ηj, and the expectation values are the derivatives of the
negative free energy with respect to the Lagrange multipliers. When written in terms
of the Lagrange multipliers, our entropy maximizing distribution is an exponential
or Gibbs distribution, but when written in terms of the expectation values, it becomes
a linear or mixture family. In the simplest situation of statistical mechanics, we

356
6
Fields of Application of Information Geometry
have the energy as the only observable, and the role of the Lagrange multiplier is
then assumed by the inverse temperature β. Thus, our preceding duality relations
generalize the fundamental formula of statistical mechanics
H = βE + logZ = βE −βF,
(6.195)
where H is the entropy, E is the energy, and F = −1
β logZ is the free energy.9
It is also of interest to extend the preceding formalism to functional integrals
as occurring in quantum theories. In the present context, we can see the resulting
structure best by setting up a formal correspondence between the ﬁnite-dimensional
case so far considered and the inﬁnite-dimensional one underlying path integrals.
So, instead of a point x ∈Rn with coordinates x1,...,xn, we consider a function
x : U →R on some set U with values x(u). In other words, the point u ∈U now
takes over the role of the index i. In the ﬁnite-dimensional Euclidean case, a vector
V at x has components V i, and a vector ﬁeld is of the form v(x) and leads to the
transformations xi →xi + tV i(x). A metric is of the form hijdxidxj and leads to
the product 	
i,j hijV iW j between vectors. A vector at x in the present case then
has components V (x)(u), its operation on a function is x(u) →x(u) + tV (x)(u),
and a metric has components h(u,v)dudv, with the symmetry h(u,v) = h(v,u),
and the metric product of two vectors V,W at x is

h(u,v)V (u)W(v)dudv.
(6.196)
We note that here, in order to carry out the integral, we need some measure du in the
deﬁnition of the metric h. For instance, when U is a subset of some Rd, we could
take the Lebesgue measure. Let us point out again here that U is not our sample
space; the sample space Ω will rather be a space of functions on U, like L2(U,du).
Also, since we are considering here a metric at x, that measure on U could depend
on the function x. In order that the metric be positive deﬁnite, we need to require
that V (·) →

h(u,·)V (u)du has a positive spectrum (we need to clarify here on
which space of functions we are considering this operation, but we leave this issue
open for the moment). In the ﬁnite-dimensional case, a transformation x = x(y) is
differentiable when all derivatives ∂xi
∂yj exist (and are continuous, differentiable, etc.,
whatever the precise technical requirement). Here, we then need to require for a
transformation x = x(y) that all derivatives ∂x(u)
∂y(v) exist (and satisfy appropriate con-
ditions). We note that so far, we have not introduced any topology or other structure
on the set U (except for the measure du whose technical nature, however, we have
also left unspeciﬁed). The differentiability requirement is simply stated in terms of
derivatives of real functions (the real number x(u) as a function of the real number
y(v)). When we pass from the L2-metric for vector ﬁelds in the ﬁnite-dimensional
case,

hij(x)V i(x)W j(x)dx
(6.197)
9We are setting the Boltzmann constant = 1 here.

6.4
Inﬁnite-Dimensional Gibbs Families
357
(with the measure dx, for example, coming from some Riemannian structure) to the
present case, we obtain a functional integral
 
h(x)(u,v)V (x)(u)W(x)(v)dudv

dx
(6.198)
where we now need to deﬁne the measure dx. This is in general a nontrivial task,
and to see how this can be achieved, we now turn to a more concrete situation, the
inﬁnite-dimensional analogue of Gaussian integrals. Since this topic draws upon
different mathematical structures than the rest of this text, we cannot provide all the
background about such functional integrals, and we refer to [137] and the references
provided there for more details.
Here, we need to be more concrete about the function space and consider the
Sobolev space H 1,2(U) of L2-functions with square integrable (generalized) deriva-
tives on some domain U ∈Rd (or, more generally, in some Riemannian manifold).10
We consider the Dirichlet integral
L(x) := 1
2

U
Dx(u)
2 du.
(6.199)
Here, Dx is the (generalized) derivative of x; when x is differentiable, we have
L(x) = 1
2

U

α
 ∂x
∂uα
2
du.
(6.200)
We also note that for a twice differentiable x,
L(x) = −1
2

U
x(u)&x(u)du = −1
2(x,&x)L2
(6.201)
with the Laplacian &x = 	
α
∂2x
(∂uα)2 .
Our inﬁnite-dimensional Gaussian integral is then the functional integral
Z :=

exp

−L(x)

dx.
(6.202)
Here, the measure dx as such is in fact ill-deﬁned, but Wiener showed that the mea-
sure exp(−L(x))dx can be deﬁned on our function space. The functional integral
can then be taken on all of L2 because L(x) = ∞and therefore exp(−L(x)) = 0
when x is not in our Sobolev space. Actually, one slight technical point needs to be
addressed here. The operator & has a kernel, consisting of the constant functions,
and therefore, our Gaussian kernel is not positive deﬁnite, but only semideﬁnite.
This is easily circumvented, however, by requiring

U x(u)du = 0 to eliminate the
10Eventually, it will sufﬁce for our purposes to work with L2(U), and so, the notion of a Sobolev
space is not so crucial for the present purposes. In any case, we can refer readers to [136] or [140].

358
6
Fields of Application of Information Geometry
constants. On that latter space, the Gaussian kernel is positive deﬁnite. This can
also be expressed as follows. All eigenvalues λj of &, that is, real numbers with
nonconstant solutions xj ∈L2(U) of
&xj + λjxj = 0
(6.203)
are positive. −& is then invertible, and its inverse is the Green operator G. This
Green operator is given by a symmetric integral kernel G(u,v), that is, it operates
via x(·) →

G(·,u)x(u)du, i.e., by convolution with this kernel. G(u,v) has a
singularity at u = v of order −log|u −v| for d = 2 and of order |u −v|2−d for
d > 2.
We may then rewrite (6.201) as
L(x) = 1
2

x,G−1x

L2
(6.204)
and (6.202) as
Z =

exp

−1
2

x,G−1x

dx.
(6.205)
More generally, we may consider
I(&,ϑ) =

exp

−1
2

x,G−1x

+ (ϑ,x)

dx
(6.206)
with
(ϑ,x)L2 =

U
ϑ(u)x(u)du.
(6.207)
This expression is then fully analogous to our Gaussian integral (4.91). The analogy
with the above discussion of Gaussian integrals, obtained by replacing the coordi-
nate index i in (4.91) by the point u in our domain U, would suggest
Z = (detG)
1
2 ,
(6.208)
when we normalize
dx =
0
i
dxi
√
2π
(6.209)
to get rid of the factor (2π)n in (4.91). Here, the (xi) are an orthonormal basis of
the Hilbert space L2(U), for example, the (orthonormalized) eigenfunctions of & as
in (6.203).11 The determinant detG in (6.208) can be deﬁned as the inverse of the
renormalized product of the eigenvalues λj via zeta function regularization.
11See, e.g., [136] for details.

6.4
Inﬁnite-Dimensional Gibbs Families
359
We can then directly carry over the above discussion for ﬁnite-dimensional Gaus-
sian integrals to our functional integral. Thus, with
ψ(ϑ) := 1
2(ϑ,Gϑ) + 1
2 logdetG,
(6.210)
we obtain an exponential family
p(x;ϑ) = exp

−1
2

x,G−1x

+ (ϑ,x) −ψ(ϑ)

.
(6.211)
Analogously to (3.34), we have a metric
∂2
∂ϑ(u)∂ϑ(v)ψ = G(u,v)
(6.212)
which is, in fact, independent of ϑ. It can also be expressed in terms of moments

x(u1)···x(um)

: = Ep

x(u1)···x(um)

=

x(u1)···x(um) exp(−1
2(x,G−1x) + (ϑ,x))dx

exp(−1
2(x,G−1x) + (ϑ,x))dx
=
1
I(&,ϑ)
∂
∂ϑ(u1) ···
∂
∂ϑ(um)I(&,ϑ).
(6.213)
Again, we have the correlator

x(u)x(v)

−

x(u)

x(v)

= G(u,v),
(6.214)
as an analogue of (4.84). For ϑ = 0, the ﬁrst-order moments ⟨x(u)⟩vanish.
Thus, when viewed in the framework of information geometry, the Green func-
tion yields a Fisher metric on L2(U) (with the constants divided out) via

G(u,v)V (u)W(v)dudv.
(6.215)
Since all eigenvalues of G are positive, this expression is positive deﬁnite.
The question then emerges to what extent the preceding formal computations
lead to a structure that satisﬁes our requirements for a statistical model, that is, the
differentiability requirements of Deﬁnition 3.4 and the integrability requirements
of Deﬁnition 3.7. In order to obtain differentiability, one might restrict ϑ to be a
test function, that is, inﬁnitely often differentiable and rapidly decaying at inﬁnity.
Of course, the precise requirement will depend on the space of functions (or distri-
butions) x that one wants to work with. 2-integrability, however, is not satisﬁed in
general. The reason is that the 2-point function

x(u)x(v)

= Ep

x(u)x(v)

=

x(u)x(v)exp(−1
2(x,G−1x) + (ϑ,x))dx

exp(−1
2(x,G−1x) + (ϑ,x))dx
(6.216)

360
6
Fields of Application of Information Geometry
becomes ∞for u = v. Equivalently, the Green function G has a singularity at u = v,
G(u,u) = ∞.
(6.217)
In quantum ﬁeld theory, this issue is dealt with by such techniques as Wick ordering
or renormalization (see, for instance, [109, 229]), but this is outside the scope of the
present book.
The preceding considerations can be generalized to other functional integrals and
to observables other than the values x(u). As this is obvious and straightforward, we
do not carry this out here.

Appendix A
Measure Theory
In this appendix, we collect some results from measure theory utilized in the text.
Let M be a set. A measure μ is supposed to assign a non-negative number, or
possibly ∞, to a subset A of M. This value μ(A) then represents something like the
size of A with respect to the measure μ. Thus
(1) 0 ≤μ(A) ≤∞.
Also naturally
(2) μ(∅) = 0.
Finally, we would like to have
(3) μ(/
n An) = 	
n μ(An) for a countable family of pairwise disjoint subsets
An,n ∈N, of M.
This implies in particular
(4) μ(A) + μ(M\A) = μ(M) for a subset A.
It turns out, however, that this does not quite work for arbitrary subsets. Thus, one
needs to restrict these requirements to certain subsets of M; of course, the class of
these subsets should include those that are relevant for the structures that M may
possess. For example, when M is a topological space, these results should hold for
the open and closed subsets of M. The appropriate requirements for such a class of
subsets are encoded in
Deﬁnition A.1 A nonempty collection B of subsets of a set M is called a σ-algebra
if:
(S1) Whenever A ∈B, then also M\A ∈B.
(S2) Whenever (An)n∈N ⊂B, then also /∞
n=1 An ∈B.
One easily observes that for a σ-algebra on M, necessarily ∅∈B and M ∈B,
and whenever (An)n∈N ⊂B, then also @∞
n=1 An ∈B.
© Springer International Publishing AG 2017
N. Ay et al., Information Geometry, Ergebnisse der Mathematik und ihrer Grenzgebiete.
3. Folge / A Series of Modern Surveys in Mathematics 64,
DOI 10.1007/978-3-319-56478-4
361

362
A
Measure Theory
Remark Let us brieﬂy compare the requirements of a σ-algebra with those for the
collection U of open sets in Euclidean space Rd. The latter satisﬁes
(O1) ∅,Rd ∈U.
(O2) If U,V ∈U, then also U ∩V ∈U.
(O3) If Ui ∈U for all i in some index set I, then also
8
i∈I
Ui ∈U.
So, a ﬁnite union of open sets is open again, as in the requirement for a σ-algebra.
The complement of an open set, however, is not open (except for ∅,Rd), but closed.
Therefore, the collection of open sets does not constitute a σ-algebra. There is a
simple solution to cope with this difﬁculty, namely to let U generate a σ-algebra,
that is, take the collection B of all sets generated by the iterated application of the
processes (S1, 2) to sets in U. This is called the Borel σ-algebra, and its members
are called Borel sets.
In fact, these constructions extend to arbitrary topological spaces, that is, sets M
with a collection U of subsets satisfying (O1)–(O3) (with Rd replaced by M now
in (O1), of course). These sets are then deﬁned to be the open sets generating the
topology of M. Their complements are then called closed.
Deﬁnition A.2 Let M be a set with a σ-algebra B.
A function μ : B →R+ ∪{∞}, i.e.,
0 ≤μ(A) ≤∞
for every A ∈B,
with
μ(∅) = 0,
is called a measure on (M,B) if whenever the sets An ∈B,n ∈N, are pairwise
disjoint, then
μ
 ∞
8
n=1
An

=
∞

n=1
μ(An)
(countable additivity).
The elements of B then are called measurable (w.r.t. μ).
For our purposes, a normalization is usually important:
Deﬁnition A.3 A measure μ with
μ(M) = 1
is called a probability measure.

A
Measure Theory
363
According to our deﬁnition, a measure assigns to each measurable subset of M a
nonnegative number, possibly +∞, but for our purposes, we mostly consider only
those measures μ with 0 < μ(M) < ∞. Such a measure can be made into a proba-
bility measure by a simple rescaling.
Examples of measures are of course the standard Lebesgue measure on Rd, but
also Dirac measures on arbitrary spaces M. Given a point x ∈M, the associated
Dirac measure δx is deﬁned by
δx(A) :=

0
if x /∈A,
1
if x ∈A,
for any subset A of M. Another example is the following one on an uncountable
set M: we take the σ-algebra of the sets A that are countable or complements of
countable sets, and we let μ(A) be 0 when A is countable and 1 when its comple-
ment is countable.
In general, measures can be quite wild, and so, if one wants to derive more spe-
ciﬁc results, one needs to impose some additional restrictions. When M is a topolog-
ical space, a very useful class in this regard turns out to be that of Radon measures,
i.e., those with
• μ(K) < ∞for every compact set K,
• μ(U) = sup{μ(K) : K compact, K ⊂U} for every open set U and
• μ(A) = inf{μ(U) : U open, A ⊂U} for every measurable set A.
The null sets of a measure μ are those subsets A of M with μ(A) = 0. We say
that a property holds almost everywhere (abbreviated as a.e.) (w.r.t. the measure μ)
when it holds except possibly on a null set.
Deﬁnition A.4 ϕ : M →R is said to be a simple function if there exist disjoint
measurable sets B1,...,Bk ⊂M and c1,...,ck ∈R such that
ϕ =
k

j=1
cjχBj .
Here χB is the characteristic function of the set B, that is,
χB(x) :=

1
when x ∈B,
0
when x /∈B.
We then deﬁne the integral of such a simple function as

ϕ dμ :=
k

j=1
cjμ(Bj).

364
A
Measure Theory
The following statements are equivalent for a function f : M →R ∪{±∞}:
1. For every c ∈R, the set Ac := {x ∈M : f (x) ≥c} is measurable.
2. f is the pointwise limit of simple functions (the convergence thus occurs every-
where and not just almost everywhere).
Such an f is then called measurable, and its integral is deﬁned as

f dμ := lim
n→∞

ϕn dμ,
where the ϕn are simple functions converging to f . f is called integrable when this
integral is ﬁnite.
We can also recover the measure of a measurable set A from the integral of its
characteristic function:
μ(A) =

χA dμ.
(A set is measurable precisely if its characteristic function is integrable.) In this
sense, measure and integral are equivalent concepts because either of them can be
used to derive the other.
There is another useful perspective on this. Given an integrable function f , we
assign a scalar μ(f ) :=

f dμ to it. When we wish to perform this on classes of
functions and measures to see some functorial properties, we encounter the difﬁculty
that whether a function is integrable depends on the measure chosen. We can ask,
however, for which measures all members of some class of functions are integrable.
The natural class of functions is that of continuous functions on a topological space
(a function f : X →R on a topological space is continuous if for every open set U
in R, the preimage f −1(U) is open in X; in fact, we can more generally consider
mappings F : X →Y between topological spaces here and have the same deﬁnition
of continuity). The space C0(X) of continuous linear functions on X is equipped
with the topology of uniform convergence. This topology is derived from the norm
∥f ∥C0 := sup
x∈X
f (x)
.
The topology is generated by (that is, the open sets are obtained as ﬁnite inter-
sections and countable unions of) sets of the form {g ∈C0(X) : ∥f −g∥< ϵ} for
some f ∈C0(X) and some ϵ > 0. C0(X) thus becomes a topological space itself,
in fact a Banach space, because we have a norm, our supremum norm, with respect
to which the space is complete (because R is complete). The Riesz representation
theorem says that the continuous linear functionals on C0(X) are precisely given by
the signed Radon measures (here, “signed” means that we drop the requirement that
the measure be non-negative).
We thus see the duality between functions and measures: we can consider

f dμ
as μ(f ), that is, the measure applied to the function, or as f (μ), the function applied
to the measure. Thus, measures yield linear functionals on spaces of functions, and
functions yield linear functionals on spaces of (signed) measures (even though there

A
Measure Theory
365
are more such functionals than those given by functions, that is, the dual space of the
space of signed Radon measures is bigger than the space of continuous functions).
An easy example is given by a Dirac measure δx (x ∈X). Here,
δx(f ) = f (x),
an operation on functions. Of course, f (x) is also the result of the operation of f
on the points of X, and this evaluation of the function at a point then also becomes
an operation on the associated Dirac measure δx.
We now want to use maps to move measures around. Let μ be a measure on M
with the σ-algebra B of measurable sets, and let κ : L →M be a map. We then try
to deﬁne a pullback measure κ∗μ on L by deﬁning
κ∗μ(U) := μ

κ(U)

whenever κ(U) ∈B. However, the sets U with this property need not constitute a
σ-algebra when κ is not bijective. For example, the image of L itself need not be a
measurable subset of M. Also, the images of disjoint sets in general are not disjoint
themselves.
We now consider a map κ : M →N, and we try to deﬁne a pushforward measure
κ∗μ on N by deﬁning
κ∗μ(V ) := μ

κ−1(V )

whenever κ−1(V ) ∈B. The latter sets V constitute a σ-algebra; for instance, the
preimage of N is M, the preimage of a complement is the complement of that
preimage, and the union of preimages is the preimage of the union of the images.
Therefore, κ∗μ is a measure on N with this σ-algebra.
We can also naturally see from the duality between measures and functions why
the pushforward, but not the pullback, works for moving measures around. Namely,
for functions, the natural operation is the pullback of a function f deﬁned as
κ∗f (x) = f

κ(x)

.
Pushforward does not work here (unless f is bijective). Now, from the duality, we
can simply put
κ∗μ(f ) = μ

κ∗(f )

and everything transforms nicely.
We now assume that M is a differentiable manifold and consider a diffeomor-
phism κ : M →M. Then both the pushforward κ∗μ and the pullback κ∗μ are de-
ﬁned for a measure μ on M.
Since

V
1
|detdκ(x)|μ(d(κ(x))) =

κ−1V μ(dx) by the transformation formula, we
have
κ∗μ

κ(x)
det dκ(x)
 = μ(y)
for y = κ(x).

366
A
Measure Theory
When we work with the pullback measure, we have
κ∗μ(x) =
detdκ(x)
μ

κ(x)

.
Given a measure on M, we can also deﬁne an L2-product for measurable func-
tions f,g:
⟨f,g⟩μ :=

fg dμ,
(A.1)
and a measurable function f is said to be of class L2 when ⟨f,f ⟩μ < ∞. The space
L2(μ) of such functions then becomes a Hilbert space.
For the transformed measure κ∗(μ), we then obtain the Hilbert space L2(κ∗(μ))
from
⟨f,g⟩κ∗μ =

κ∗f,κ∗g

μ.
When κ is a diffeomorphism, we also have the pullback measure and we thus have
⟨f,g⟩μ =

κ∗f,κ∗g

κ∗μ.
Two measures are called compatible if they have the same null sets. If μ1, μ2
are compatible ﬁnite non-negative measures, they are absolutely continuous with
respect to each other in the sense that there exists a non-negative function ϕ that is
integrable with respect to either of them, such that
μ2 = ϕμ1
or, equivalently,
μ1 = ϕ−1μ2.
(ϕ is called the Radon–Nikodym derivative of μ2 with respect to μ1).
Being integrable, ϕ is ﬁnite almost everywhere (with respect to both μ1 and μ2)
on M, and since the situation is symmetric between ϕ and ϕ−1, ϕ is also positive al-
most everywhere. Thus, for any ﬁnite non-negative measure μ on M, we let Cμ(M)
be the space of integrable functions on M that are positive almost everywhere with
respect to μ.
Any other ﬁnite non-negative measure μ′ that is compatible with μ thus is of the
form
μ′ = ϕμ
for some
ϕ ∈Cμ(M)
and
Cμ′(M) = Cμ(M) =: C(M).
This space depends on the compatibility class, but not on the individual measure
within that class.

Appendix B
Riemannian Geometry
We collect here some basic facts and principles of Riemannian geometry as the
foundation for the presentation of Riemannian metrics, covariant derivatives, and
afﬁne structures in the text. For a more penetrating discussion and for the proofs
of various results, we need to refer to [139]. Classical differential geometry as ex-
pressed through the tensor calculus is about coordinate representations of geometric
objects and the transformations of those representations under coordinate changes.
The geometric objects are invariantly deﬁned, but their coordinate representations
are not, and resolving this contradiction is the content of the tensor calculus.
We consider a d-dimensional differentiable manifold M and start with some con-
ventions:
1. Einstein summation convention
aibi :=
d

i=1
aibi.
(B.1)
The content of this convention is that a summation sign is omitted when the same
index occurs twice in a product, once as an upper and once as a lower index. The
conventions about when to place an index in an upper or lower position will be
given subsequently. One aspect of this, however, is
2. When (hij)i,j is a metric tensor1 (a notion to be explained below) with indices
i,j, the inverse metric tensor is written as (hij)i,j, that is, by raising the indices.
In particular
hijhjk = δi
k :=

1
when i = k,
0
when i ̸= k,
(B.2)
the so-called Kronecker symbol.
1We use here the letter h to indicate a metric tensor, instead of the more customary g, because we
want to reserve g for a particular metric, the Fisher metric.
© Springer International Publishing AG 2017
N. Ay et al., Information Geometry, Ergebnisse der Mathematik und ihrer Grenzgebiete.
3. Folge / A Series of Modern Surveys in Mathematics 64,
DOI 10.1007/978-3-319-56478-4
367

368
B
Riemannian Geometry
3. Combining the previous rules, we obtain more generally
vi = hijvj
and
vi = hijvj.
(B.3)
A (ﬁnite-dimensional) manifold M is locally modeled after Rd. Thus, locally, it
can be represented by coordinates x = (x1,...,xd) taken from some open subset
of Rd. These coordinates, however, are not canonical, and we may as well choose
other ones, y = (y1,...,yd), with x = f (y) for some homeomorphism f . When the
manifold M is differentiable, we can cover it by local coordinates in such a manner
that all such coordinate transitions are diffeomorphisms where deﬁned. Again, the
choice of coordinates is non-canonical. The basic content of classical differential
geometry is then to investigate how various expressions representing objects on M
like tangent vectors transform under coordinate changes. Here and in the sequel,
all objects deﬁned on a differentiable manifold will be assumed to be differentiable
themselves. This is checked in local coordinates, but since coordinate transitions
are diffeomorphic, the differentiability property does not depend on the choice of
coordinates.
Remark For our purposes, it is convenient, and in the literature it is customary, to
mean by differentiability smoothness of class C∞, that is, assume that all objects are
inﬁnitely often differentiable. The ring of (inﬁnitely often) differentiable functions
on M is denoted as C∞(M).
A tangent vector for M at some point represented by x0 in local coordinates x is
an expression of the form
V = vi ∂
∂xi ;
(B.4)
this means that it operates on a function φ(x) in our local coordinates as
V (φ)(x0) = vi ∂φ
∂xi |x=x0
.
(B.5)
The tangent vectors at p ∈M form a vector space, called the tangent space TpM
of M at p. While, as should become clear subsequently, this tangent space and its
tangent vectors are deﬁned independently of the choice of local coordinates, the
representation of a tangent space does depend on those coordinates. The question
then is how the same tangent vector is represented in different local coordinates y
with x = f (y) as before. The answer comes from the requirement that the result of
the operation of the tangent vector V on a function φ, V (φ), should be independent
of the choice of coordinates. Always applying the chain rule here and in the sequel,
this yields
V = vi ∂yα
∂xi
∂
∂yα .
(B.6)

B
Riemannian Geometry
369
Thus, the coefﬁcients of V in the y-coordinates are vi ∂yα
∂xi . This is veriﬁed by the
following computation:
vi ∂yα
∂xi
∂
∂yα φ

f (y)

= vi ∂yα
∂xi
∂φ
∂xj
∂xj
∂yα = vi ∂xj
∂xi
∂φ
∂xj = vi ∂φ
∂xi ,
(B.7)
as required.
More abstractly, changing coordinates by f pulls a function φ deﬁned in the x-
coordinates back to f ∗φ deﬁned for the y-coordinates, with f ∗φ(y) = φ(f (y)). If
then W = wα ∂
∂yα is a tangent vector written in the y-coordinates, we need to push
it forward as f∗W = wα ∂xi
∂yα
∂
∂xi to the x-coordinates, to have the invariance
(f∗W)(φ) = W

f ∗φ

,
(B.8)
which is easily checked:
(f∗W)φ = wα ∂xi
∂yα
∂φ
∂xi = wα ∂
∂yα φ

f (y)

= W

f ∗φ

.
(B.9)
In particular, there is some duality between functions and tangent vectors here. How-
ever, the situation is not entirely symmetric because we need to know the tangent
vector only at the point x0 where we want to apply it, but we need to know the
function φ in some neighborhood of x0 because we take its derivatives.
A vector ﬁeld then is deﬁned as V (x) = vi(x) ∂
∂xi , that is, by having a tangent
vector at each point of M. As indicated above, we assume here that the coefﬁ-
cients vi(x) are differentiable. The vector space of vector ﬁelds on M is written as
Γ (T M). (In fact, Γ (T M) is a module over the ring C∞(M).)
Subsequently, we shall need the Lie bracket [V,W] := V W −WV of two vector
ﬁelds V (x) = vi(x) ∂
∂xi ,W(x) = wj(x) ∂
∂xj ; its operation on a function φ is
[V,W]φ(x) = vi(x) ∂
∂xi

wj(x) ∂
∂xj φ(x)

−wj(x) ∂
∂xj

vi(x) ∂
∂xi φ(x)

=

vi(x)∂wj(x)
∂xi
−wi(x)∂vj(x)
∂xi
∂φ(x)
∂xj .
(B.10)
In particular, for coordinate vector ﬁelds, we have
1 ∂
∂xi , ∂
∂xj
2
= 0.
(B.11)
Returning to a single tangent vector, V = vi ∂
∂xi at some point x0, we consider a
covector ω = ωidxi at this point as an object dual to V , with the rule
dxi
 ∂
∂xj

= δi
j
(B.12)

370
B
Riemannian Geometry
yielding
ωidxi

vj ∂
∂xj

= ωivjδi
j = ωivi.
(B.13)
This expression depends only on the coefﬁcients vi and ωi at the point under con-
sideration and does not require any values in a neighborhood. We can write this as
ω(V ), the application of the covector ω to the vector V , or as V (ω), the application
of V to ω.
We have the transformation behavior
dxi = ∂xi
∂yα
dyα
(B.14)
required for the invariance of ω(V ). Thus, the coefﬁcients of ω in the y-coordinates
are given by the identity
ωidxi = ωi
∂xi
∂yα
dyα.
(B.15)
The transformation behavior of a tangent vector as in (B.6) is called contravariant,
the opposite one of a covector as (B.15) covariant.
A 1-form then assigns a covector to every point in M, and thus, it is locally given
as ωi(x)dxi.
Having derived the transformation of vectors and covectors, we can then also de-
termine the transformation rules for other tensors. A lower index always indicates
covariant, an upper one contravariant transformation. For example, the metric ten-
sor, written as hijdxi ⊗dxj, with hij = ⟨∂
∂xi ,
∂
∂xj ⟩being the product of those two
basis vectors, operates on pairs of tangent vectors. It therefore transforms doubly
covariantly, that is, becomes
hij

f (y)
 ∂xi
∂yα
∂xj
∂yβ dyα ⊗dyβ.
(B.16)
The function of the metric tensor is to provide a Euclidean product of tangent vec-
tors,
⟨V,W⟩= hijviwj
(B.17)
for V = vi ∂
∂xi ,W = wi ∂
∂xi . As a check, in this formula, vi and wi transform con-
travariantly, while hij transforms doubly covariantly so that the product as a scalar
quantity remains invariant under coordinate transformations.
Let M carry a Riemannian metric h = ⟨·,·⟩. Let S be a (smooth) submanifold.
Then h induces a Riemannian metric on S, by simply restricting it to the corre-
sponding tangent spaces TpS ⊂TpM for p ∈S. For p ∈S the tangent space TpM
can be orthogonally decomposed into the tangent space TpS and the normal space
NpS,
TpM = TpS ⊕NpS with V ∈NpM ⇔⟨V,W⟩= 0 for all W ∈TpS,
(B.18)
and we therefore also have an orthogonal projection from TpM onto TpS.

B
Riemannian Geometry
371
A Riemannian metric also provides an isomorphism between a tangent space
TpM and the corresponding cotangent space T ∗
p M. For each ω ∈T ∗
p M, we can ﬁnd
a vector Vω ∈TpM with
⟨Vω,W⟩= ω(W)
for all W ∈TpM,
(B.19)
and conversely, every V ∈TpM deﬁnes a linear functional
W →⟨V,W⟩
(B.20)
on TpM which can then be represented by some covector ωV ∈T ∗
p M. In local coor-
dinates, the relations between the components of a covector ω and the corresponding
vector V are
vi = hijωj
and
ωi = hijvj.
(B.21)
In particular, if f is a function, the 1-form df is dual to a vector ﬁeld gradf , the
gradient of f , which satisﬁes

gradf (x),W

= W(f )(x) = df (W)(x)
for all x ∈M and W ∈TxM.
(B.22)
In local coordinates, the gradient is given by
grad(f )i = hij ∂f
∂xj .
(B.23)
The gradient of a function f is a vector ﬁeld, but not every vector ﬁeld V is the
gradient of a function of f . There is a necessary condition, the Frobenius condition.
The condition
vi = grad(f )i = hij ∂f
∂xj
is equivalent to
ωj = hjivi = ∂f
∂xj
(B.24)
and (as we are assuming that all our objects are smooth), since the second derivatives
of f commute,
∂2f
∂xj ∂xk =
∂2f
∂xk∂xj for all indices j,k, we get the necessary condition
∂ωj
∂xk = ∂ωk
∂xj
for all j,k.
(B.25)
Frobenius’ Theorem says that this condition is also sufﬁcient on a simply connected
domain, that is, a vector ﬁeld V is a gradient of some function f on such a domain
if and only if the associated 1-form ωj = hjivi (B.24) satisﬁes (B.25). Obviously,
it depends on the metric whether a vector ﬁeld is the gradient of a function, because
via (B.24), the condition (B.25) involves the metric hij.

372
B
Riemannian Geometry
So far, we have computed derivatives of functions. We have also talked about
vector ﬁelds V (x) = vi(x) ∂
∂xi as objects that depend differentiably on their ar-
guments x. Of course, we can do the same for other tensors, like the metric
hij(x)dxi ⊗dxj. This naturally raises the question of how to compute their deriva-
tives. This encounters the problem, however, that in contrast to functions, the repre-
sentation of such tensors depends on the choice of local coordinates, and we have
described in some detail that and how they transform under coordinate changes. Pre-
cisely because of that transformation, they acquire a coordinate invariant meaning;
for example, the operation of a vector on a function or the metric product between
two vectors is independent of the choice of coordinates.
It now turns out that on a differentiable manifold, in general there is no single
canonical way of taking derivatives of vector ﬁelds or other tensors in an invariant
manner. There are, in fact, many such possibilities, and they are called connections
or covariant derivatives. Only when we have additional structures, like a Rieman-
nian metric, can we single out a particular such covariant derivative on the basis
of its compatibility with the metric. For our purposes, however, we also need other
covariant derivatives, and therefore, we now develop that notion.
Let M be a differentiable manifold. We recall that Γ (T M) denotes the space of
vector ﬁelds on M. An (afﬁne) connection or covariant derivative on M is a linear
map
∇: Γ (T M) ⊗Γ (T M) →Γ (T M)
(V,W) →∇V W
satisfying
(i) ∇is tensorial in the ﬁrst argument:
∇V1+V2W = ∇V1W + ∇V2W
for all V1,V2,W ∈Γ (T M),
∇f V W = f ∇V W
for all f ∈C∞(M),V,W ∈Γ (T M),
(ii) ∇is R-linear in the second argument:
∇V (W1 + W2) = ∇V W1 + ∇V W2
for all V,W1,W2 ∈Γ (T M),
and it satisﬁes the product rule
∇V (f W) = V (f )W + f ∇V W
for all f ∈C∞(M),V,W ∈Γ (T M).
(B.26)
∇V W is called the covariant derivative of W in the direction V . By (i), for any
x0 ∈M, (∇V W)(x0) only depends on the value of V at x0, but by way of contrast,
it also depends on the values of W in some neighborhood of x0, as it naturally
should as a notion of a derivative of W. The example on which this is modeled is
the Euclidean connection given by the standard derivatives, that is, for V = V i ∂
∂xi ,

B
Riemannian Geometry
373
W = W j
∂
∂xj ,
∇eucl
V
W = V i ∂W j
∂xi
∂
∂xj .
However, this is not invariant under nonlinear coordinate changes, and since a gen-
eral manifold cannot be covered by coordinates with only linear coordinate trans-
formations, we need the above more general and abstract concept of a covariant
derivative.
Let U be a coordinate chart in M, with local coordinates x and coordinate vec-
tor ﬁelds
∂
∂x1 ,...,
∂
∂xd (d = dimM). We then deﬁne the Christoffel symbols of the
connection ∇via
∇∂
∂xi
∂
∂xj =: Γ k
ij
∂
∂xk .
(B.27)
If we change our coordinates x to coordinates y, then the new Christoffel symbols,
∇∂
∂yl
∂
∂ym =: ˜Γ n
lm
∂
∂yn ,
(B.28)
are related to the old ones via
˜Γ n
lm

y(x)

=

Γ k
ij(x)∂xi
∂yl
∂xj
∂ym +
∂2xk
∂yl∂ym
∂yn
∂xk .
(B.29)
In particular, due to the term
∂2xk
∂yl∂ym , the Christoffel symbols do not transform as
a tensor (unless we restrict our coordinate transformations to those for which this
term vanishes, that is, to the afﬁne linear ones—a point to which we shall return).
However, we have
Lemma B.1 Let 1∇, 2∇be two connections, with corresponding Christoffel sym-
bols 1Γ k
ij, 2Γ k
ij. Then the difference 1Γ k
ij −2Γ k
ij transforms as a tensor.
Proof When we take the difference of two connections, the inhomogeneous term
∂2xk
∂yl∂ym drops out, because it is the same for both. The rest is tensorial.
□
Expressed more abstractly, this means that the space of connections on M is an
afﬁne space. In particular, if we have two connections ∇(1), ∇(−1), then we may
generate a family of connections
∇(α) := 1 + α
2
∇(1) + 1 −α
2
∇(−1),
for −1 ≤α ≤1.
(B.30)
For a connection ∇, we deﬁne its torsion tensor via
Θ(V,W) := ∇V W −∇WV −[V,W]
for V,W ∈Γ (T M).
(B.31)

374
B
Riemannian Geometry
Inserting our coordinate vector ﬁelds
∂
∂xi as before, we obtain
Θij := Θ
 ∂
∂xi , ∂
∂xj

= ∇∂
∂xi
∂
∂xj −∇∂
∂xj
∂
∂xi
since coordinate vector ﬁelds commute,
i.e.,
1 ∂
∂xi , ∂
∂xj
2
= 0
=

Γ k
ij −Γ k
ji
 ∂
∂xk .
We call the connection ∇torsion-free or symmetric if Θ ≡0. By the preceding
computation, this is equivalent to the symmetry
Γ k
ij = Γ k
ji
for all i,j,k.
(B.32)
Let c(t) be a smooth curve in M, and let V (t) := ˙c(t) (= ˙ci(t) ∂
∂xi (c(t)) in local
coordinates) be the tangent vector ﬁeld of c. In fact, we should rather write V (c(t))
in place of V (t), but we consider t as the coordinate along the curve c(t). Thus, in
those coordinates ∂
∂t = ∂ci
∂t
∂
∂xi , and in the sequel, we shall frequently and implicitly
make this identiﬁcation, that is, switch between the points c(t) on the curve and
the corresponding parameter values t. Let W(t) be another vector ﬁeld along c, i.e.,
W(t) ∈Tc(t)M for all t. We may then write W(t) = μi(t) ∂
∂xi (c(t)) and form
∇˙c(t)W(t) = ˙μi(t) ∂
∂xi + ˙ci(t)μj(t)∇∂
∂xi
∂
∂xj
= ˙μi(t) ∂
∂xi + ˙ci(t)μj(t)Γ k
ji

c(t)
 ∂
∂xk
(the preceding computation is meaningful as we see that it depends only on the
values of W along the curve c(t), but not on other values in a neighborhood of a
point on that curve).
This represents a (non-degenerate) linear system of d ﬁrst-order differential op-
erators for the d coefﬁcients μi(t) of W(t). Therefore, for given initial values μi(0),
there exists a unique solution W(t) of
∇˙c(t)W(t) = 0.
This W(t) is called the parallel transport of W(0) along the curve c(t). We also say
that W(t) is covariantly constant along the curve c. We also write
W(t) = Πc,t

W(0)

.
Given such a parallel transport Π and a vector ﬁeld W in the vicinity of the point
c(0) for a smooth curve c, we can recover the covariant derivative of W at c(0) in

B
Riemannian Geometry
375
the direction of the vector ˙c(0) as the inﬁnitesimal version of parallel transport,
∇˙c(0)W

c(0)

= lim
t↘0
W(c(t)) −Πc;c(0),c(t)W(c(0))
t
= lim
t↘0
Πc;c(t),c(0)tW(c(t)) −W(c(0))
t
,
(B.33)
where Πc;p,q is parallel transport along the curve c from the point p to the point q
which both are assumed to lie on c. (Note that the object under the ﬁrst limit sits in
Tc(t)M, and that under the second in Tc(0)M.)
Let now W be a vector ﬁeld in a neighborhood U of some point x0 ∈M. W is
called parallel if for any curve c(t) in U, W(t) := W(c(t)) is parallel along c. This
means that for all tangent vectors V in U,
∇V W = 0,
i.e.,
∂
∂xi W k + W jΓ k
ij = 0
identically in U, for all i,k,
with W = W i ∂
∂xi in local coordinates.
This is now a system of d2 ﬁrst-order differential equations for the d coefﬁcients
of W, and so, it is overdetermined. Therefore, in general, such a W does not ex-
ist. If, however, we can ﬁnd coordinates ξ1,...,ξd such that the coordinate vector
ﬁelds
∂
∂ξ1 ,...,
∂
∂ξd are parallel on U—such coordinates then are called afﬁne coor-
dinates—then we say that ∇is ﬂat on U. An example is, of course, given by the
Euclidean connection.
With respect to afﬁne coordinates, the Christoffel symbols vanish:
Γ k
ji
∂
∂ξk = ∇∂
∂ξi
∂
∂ξj = 0
for all i,j.
(B.34)
In particular, ∇is torsion-free. Moreover, in this case also the curvature tensor R,
deﬁned via
R(V,W)Z := ∇V ∇WZ −∇W∇V Z −∇[V,W]Z,
(B.35)
or in local coordinates
Rk
lij
∂
∂xk := R
 ∂
∂xi , ∂
∂xj
 ∂
∂xl
(i,j,l = 1,...,d)
(B.36)
vanishes as well. In fact, the curvature tensor can be expressed in terms of the
Christoffel symbols and their derivatives via
Rk
lij = ∂
∂xi Γ k
jl −
∂
∂xj Γ k
il + Γ k
imΓ m
jl −Γ k
jmΓ m
il .
(B.37)

376
B
Riemannian Geometry
Conversely, if both the torsion and curvature tensor of a connection ∇vanish, then ∇
is ﬂat in the sense that locally afﬁne coordinates can be found. For details, see [139].
We also note that, as the name indicates, the curvature tensor R is, like the torsion
tensor Θ, but in contrast to the connection ∇represented by the Christoffel sym-
bols, a tensor in the sense that when one of its arguments is multiplied by a smooth
function, we may simply pull out that function without having to take a derivative of
it. Equivalently, it transforms as a tensor under coordinate changes; here, the upper
index k stands for an argument that transforms as a vector, that is, contravariantly,
whereas the lower indices l,i,j express a covariant transformation behavior.
A manifold that possesses an atlas of afﬁne coordinate charts, so that all coor-
dinate transitions are given by afﬁne maps, is called afﬁne or afﬁne ﬂat. Such a
manifold then possesses a ﬂat afﬁne connection; we can simply take local afﬁne
connections in the coordinate charts and transform them by our afﬁne coordinate
changes when going from one chart to another one. As can be expected from general
principles of differential geometry (see [150]), the existence of an afﬁne structure
imposes topological restrictions on a manifold, see, e.g., [34, 121, 238]. For exam-
ple, a compact manifold with ﬁnite fundamental group, or, as Smillie [238] showed,
more generally one whose fundamental group is obtained from taking free or direct
products or ﬁnite extensions of ﬁnite groups, cannot carry an afﬁne structure. The
universal cover of an afﬁne manifold with a complete2 afﬁne connection is the afﬁne
space Am, i.e., Rm with its natural afﬁne structure, and the fundamental group then
has to be a subgroup of the group of afﬁne motions of Am acting freely and properly
discontinuously, see [34, 121]. One should note, however, that an analogue of the
Bieberbach theorem for subgroups of the group of Euclidean motions does not hold
in the afﬁne case, and so, this is perhaps less restrictive than it might seem.
For a geometric analysis approach to afﬁne structures, see [138].
A curve c(t) in M is called autoparallel or geodesic if
∇˙c ˙c = 0.
In local coordinates, this becomes
¨ck(t) + Γ k
ij

c(t)

˙ci(t)˙cj(t) = 0
for k = 1,...,d.
(B.38)
This constitutes a system of linear second order ODEs, and given x0 ∈M, V ∈
Tx0M, there exists a maximal interval IV ⊂R with 0 ∈IV and a geodesic
cV : IV →M
with cV (0) = x0, ˙cV (0) = V . We can then deﬁne the exponential map expx0 on some
star-shaped neighborhood of 0 ∈Tx0M:
expx0 : {V ∈Tx0M : 1 ∈IV } →M
(B.39)
V →cV (1).
(B.40)
We then have expx0(tV ) = cV (t) for 0 ≤t ≤1.
2A notion to be deﬁned below.

B
Riemannian Geometry
377
A connection ∇is called complete if every geodesic c can be continued indef-
initely; this means that if c : I →M is deﬁned on some interval I and geodesic
for ∇, then it can be extended as a geodesic c : (−∞,∞) →M.
Contrary to what one might expect, an afﬁne connection on a compact manifold
need not be complete; the following example is quite instructive: On the real line
with coordinate x, we consider the metric
ds2 = exdx2;
(B.41)
like any one-dimensional metric it is ﬂat, but it is not complete as −∞is at ﬁnite
distance from the interior of the real line w.r.t. this metric. The translation x →x +1
multiplies ds2 by e and is therefore afﬁne. If we divide by these translations to obtain
a circle as quotient, we have a compact manifold with an afﬁne ﬂat connection that
is not complete. Also, in contrast to the situation for Riemannian manifolds, the
completeness of an afﬁne structure does not necessarily imply that any two points
can be connected by a geodesic.
A submanifold S of M is called autoparallel if for all x0 ∈S, V ∈Tx0S for which
expx0 V is deﬁned, we have
expx0 V ∈S.
The inﬁnitesimal condition needed for this property is that
∇V W(x) ∈TxS
(B.42)
for any vector ﬁeld W(x) tangent to S and V ∈TxS. We observe that in this case,
the connection ∇of M also deﬁnes a connection on S, by restricting ∇to vector
ﬁelds that are tangential to S.
Let now M carry a Riemannian metric h = ⟨·,·⟩, and let ∇be a connection on M.
Let S again be a submanifold. ∇then induces a connection ∇S on S by projecting
∇V W for V,W tangent to S at x ∈S from TxM onto TxS, according to the orthog-
onal decomposition (B.18). When S is autoparallel, then such a projection is not
necessary, according to (B.42), but for a general submanifold, we need a metric to
induce a connection.
For a connection ∇on M, we may then deﬁne its dual connection ∇∗via
Z⟨V,W⟩= ⟨∇ZV,W⟩+

V,∇∗
ZW

(B.43)
for all tangent vectors Z and vector ﬁelds V , W.
In local coordinates, with hij = ⟨∂
∂xi ,
∂
∂xj ⟩, this becomes
∂
∂xk hij = hjlΓ l
ki + hil∗Γ l
kj =: Γkij + ∗Γkji,
(B.44)
i.e.,
Γijk =

∇∂
∂xi
∂
∂xj , ∂
∂xk
 
.
(B.45)

378
B
Riemannian Geometry
We say that ∇is a Riemannian connection if it is self-dual, i.e.,
∇= ∇∗.
This means that ∇satisﬁes the metric product rule
Z⟨V,W⟩= ⟨∇ZV,W⟩+ ⟨V,∇ZW⟩.
(B.46)
Given any connection ∇, 1
2(∇+ ∇∗) becomes a Riemannian connection. For any
Riemannian metric h, there exists a unique torsion-free Riemannian connection, the
so-called Levi-Civita connection ∇h. It is given by

∇h
V W,Z

= 1
2

V ⟨W,Z⟩−Z⟨V,W⟩+ W⟨Z,V ⟩
−

V,[W,Z]

+

Z,[V,W]

+

W,[Z,V ]

,
(B.47)
and the Christoffel symbols of ∇h can be expressed through the metric,
Γ k
ij = 1
2hkl
 ∂
∂xj hil + ∂
∂xi hjl −∂
∂xl hij

,
(B.48)
or, equivalently,
∂
∂xk hij = hjlΓ l
ik + hilΓ l
jk = Γikj + Γjki.
(B.49)
In particular, if
1
2

∇+ ∇∗
= ∇h
(B.50)
for the Levi-Cività connection ∇h, then the connections ∇and ∇∗are dual to each
other.
Now let ∇and ∇∗again be dual connections with respect to the metric h = ⟨·,·⟩.
We then have for the curvatures R and R∗of ∇and ∇∗, recalling the formula (B.35),

R(V,W)Z,Y

= ⟨∇V ∇WZ,Y⟩−⟨∇W∇V Z,Y⟩−⟨∇[V,W]Z,Y⟩
= V ⟨∇WZ,Y⟩−

∇WZ,∇∗
V Y

−W⟨∇V Z,Y⟩+

∇V Z,∇∗
WY

−[V,W]⟨Z,Y⟩+

Z,∇∗
[V,W]Y

= V W⟨Z,Y⟩−V

Z,∇∗
WY

−W

Z,∇∗
V Y

+

Z,∇∗
W∇∗
V Y

−WV ⟨Z,Y⟩+ W

Z,∇∗
V Y

+ V

Z,∇∗
WY

−

Z,∇∗
V ∇∗
WY


B
Riemannian Geometry
379
−[V,W]⟨Z,Y⟩+

Z,∇∗
[V,W]Y

= −

Z,R∗(V,W)Y

.
(B.51)
Thus,
Lemma B.2 R vanishes precisely if R∗does.
Unfortunately, the analogous result for the torsion tensors does not hold in gen-
eral, because the term [V,W] in (B.31) does not satisfy a product rule w.r.t. the
metric.
If, however, the torsion and curvature tensors of a connection ∇and its dual ∇∗
both vanish, then we say that ∇and ∇∗are dually ﬂat. In fact, by the preceding, it
sufﬁces to assume that ∇and its dual ∇∗are both torsion-free and that the curvature
of one of them vanishes.

Appendix C
Banach Manifolds
In this section, we shall discuss the theory of Banach manifolds, which are a gen-
eralization of the ﬁnite-dimensional manifolds discussed in Appendix B. Most of
our exposition follows the book [157]. We also present some standard results in
functional analysis which will be used in this book.
Recall that a (real or complex) Banach space is a (real or complex) vector space
E with a complete norm ∥· ∥. We call a subset U ⊂E open if for every x ∈U there
is an ϵ > 0 such that the ball
Bϵ(x) =

y ∈E : ∥y −x∥< ε

is also contained in U. The collection of all open sets is called the topology of E.
Two norms ∥· ∥1 and ∥· ∥2 on E are called equivalent if there are constants
c1,c2 > 0 such that for all v ∈E
c1∥v∥1 ≤∥v∥2 ≤c2∥v∥1.
Equivalent norms induce the same topology on E, and since for the most part we
are only interested in the topology, it is useful not to distinguish equivalent norms
on E.
Given two Banach spaces E1,E2, we let Lin(E1,E2) be the space of bounded
linear maps φ : E1 →E2, i.e., linear maps for which the operator norm
∥φ∥Lin(E1,E2) :=
sup
x∈E1,x̸=0
∥φx∥E2
∥x∥E1
is ﬁnite. Then Lin(E1,E2) together with the operator norm ∥· ∥Lin(E1,E2) is itself a
Banach space.
We also deﬁne inductively the Banach spaces
Lin

E0
1,E2

:= E2
and
Lin

Ek
1,E2

= Lin

E1,Lin

Ek−1
1
,E2

.
© Springer International Publishing AG 2017
N. Ay et al., Information Geometry, Ergebnisse der Mathematik und ihrer Grenzgebiete.
3. Folge / A Series of Modern Surveys in Mathematics 64,
DOI 10.1007/978-3-319-56478-4
381

382
C
Banach Manifolds
A special case of this is the dual space
E′ := Lin(E,R).
In general, for a vector space V , we deﬁne the space of linear functions
φ : V →R (without the requirement of continuity) as the linear dual space and
denote this by
E∗:= Linalg(E,R).
If dimE < ∞, then any linear map is continuous, whence
E′ = E∗
for ﬁnite-dimensional vector spaces E.
A sequence (xn)n∈N ∈E is called weakly convergent to x ∈E if limα(xn) =
α(x) for all α ∈E′. Weak convergence is denoted by
xn ⇀x.
We call a function φ : U →E2 weakly continuous if φ(x) ⇀φ(x0) for x →x0.
We recall the following three standard results in functional analysis whose proofs
can be found, e.g., in [182].
Theorem C.1 (Hahn–Banach theorem) Let E be a Banach space and x ∈E. Then
there is an α ∈E′ such that
∥α∥E′ = 1
and
α(x) = ∥x∥E.
Theorem C.2 (Uniform boundedness principle) Let E1,E2 be Banach spaces and
F ⊂Lin(E1,E2) an arbitrary subset. If for all x ∈E there is a constant Cx ∈R
such that
∥T x∥E2 ≤Cx
for all T ∈F,
then there is another constant C independent of x ∈E1 such that also
∥T ∥Lin(E1,E2) ≤C
for all T ∈F.
Theorem C.3 (Radon–Riesz theorem)
Let (fn)n∈N ∈Lk(Ω,μ0) be a sequence,
1 ≤k < ∞. Then the following are equivalent for f ∈Lk(Ω,μ0).
(1) lim∥fn −f ∥k = 0, i.e., fn converges to f in the ordinary sense.
(2) fn ⇀f and lim∥fn∥k = ∥f ∥k.
As an application of these theorems, let us prove the following
Proposition C.1 Let E be a Banach space and let (xn)n∈N ∈E be a sequence such
that xn ⇀x0 for some x0 ∈E. Then
∥x0∥E ≤liminf∥xn∥E
and
limsup∥xn∥E < ∞.
(C.1)

C
Banach Manifolds
383
Proof For the ﬁrst estimate, pick α ∈E′ with ∥α∥E′ = 1 and α(x0) = ∥x0∥E which
is possible by the Hahn–Banach theorem. Then
0 = limα(xn −x0) = limα(xn) −∥x0∥E
≤liminf∥α∥E′∥xn∥E −∥x0∥E = liminf∥xn∥E −∥x0∥E,
showing the ﬁrst estimate. For the second, let F := {ˆxn ∈E′′ = Lin(E′,R) : n ∈N},
where ˆxn(α) := α(xn). Indeed, for α ∈E′ we have
limsup
ˆxn(α)
 ≤limsup
α(xn −x0)
 +
α(x0)
 = 0 +
α(x0)
 < ∞,
so that {|ˆxn(α)| : n ∈N} is bounded for each α ∈E′. Thus, by the uniform bound-
edness principle, we have ∥ˆxn∥E′′ ≤C, and by the Hahn–Banach theorem, there is
an αn ∈E′ with ∥αn∥= 1 and ∥xn∥E = αn(xn) = |ˆxn(αn)| ≤∥ˆxn∥E′′ ≤C.
Thus, ∥xn∥E ≤C, whence limsup∥xn∥E < ∞.
□
Deﬁnition C.1 ((Weakly) Differentiable maps and diffeomorphisms)
Let E1, E2
be (real or complex) Banach spaces and U ⊂E1 an open subset, and let φ : U →E2
be a map.
(1) Let u0 ∈U. Then for each v ∈E1 the Gâteaux-derivative of φ at u0 in the
direction v is deﬁned as the limit
∂vφ|u0 := d
dt

t=0
φ(u0 + tv) = lim
t→0
1
t

φ(u0 + tv) −φ(u0)

∈E2,
and φ is called Gâteaux-differentiable at u0 ∈U if this Gâteaux-derivative ex-
ists for all v ∈E1, and the map
du0φ : E1 −→E2,
v −→∂vφ|u0
is bounded linear. du0φ is called the differential of φ at u0.
(2) φ is called Fréchet differentiable in u0 ∈U if it is Gâteaux-differentiable at u0,
and
lim
v→0
∥φ(u0 + v) −φ(u0) −dφu0(v)∥E2
∥v∥E1
= 0.
(3) We call φ weakly Gâteaux-differentiable at u0 ∈U in the direction v if there is
an element, again denoted by ∂vφ|u0 ∈E2, such that
1
t

φ(u0 + tv) −φ(u0)

⇀∂vφ|u0
for t →0.
Again, if the map du0φ : v →∂vφ|u0 is bounded linear, then we call φ weakly
Gâteaux differentiable at u0 and call du0φ its weak differential at u0.

384
C
Banach Manifolds
(4) We call φ weakly Fréchet differentiable in u0 ∈U if
φ(u0 + v) −φ(u0) −dφu0(v)
∥v∥E1
⇀0
for v →0.
(5) We call φ (weakly) Gâteaux differentiable or (weakly) Fréchet differentiable,
respectively, if it has this property at every u0 ∈U.
(6) We call φ a (weak) Ck-map if it is (weakly) differentiable, and if the inductively
deﬁned maps d1φ := dφ : U →Lin(E1,E2), and
dr+1φ : U −→Lin

Er
1,E2

,
u −→d

drφ

u ∈Lin

Er−1
1
,E2

are (weakly) differentiable for r = 1,...,k −1 and (weakly) continuous for
r = k.
(7) Let V := φ(U) ⊂E2. If the C1-map φ : U →V is bijective, V ⊂E2 is open
and both φ : U →V ⊂E2 and φ−1 : V →U ⊂E1 are differentiable, then φ is
called a diffeomorphism of U and V , and it is called of class Ck if both φ and
φ−1 are Ck-maps.
Usually, when speaking of differentiable maps we refer to Fréchet differentiabil-
ity unless it is explicitly stated that weak differentiability or the Gâteaux differen-
tiability is meant.
In the case of ﬁnite-dimensional spaces E1 = Rn and E2 = Rm, the Fréchet
derivative duΦ is multiplication with the Jacobi matrix of the differentiable map
φ : Rn ⊃U →Rm. That is, the differential is the inﬁnite-dimensional analogue
of the total differential, whereas the Gâteaux derivative represents the directional
derivative.
Let us also point out that the difference between differentiability and weak dif-
ferentiability arises only in inﬁnite dimensions, as it is derived from weak conver-
gence. This should not be confused with notions of weak differentiability for map-
pings with ﬁnite-dimensional domains and targets, as usually employed in calculus,
which are deﬁned in terms of integration by parts properties; see, e.g., [136].
Just as in the ﬁnite-dimensional case, the relations between the notions that we
have deﬁned can be summarized as follows.
1. If φ is Gâteaux-differentiable (Fréchet differentiable, respectively) at u0 ∈U,
then it is also weakly Gâteaux-differentiable (weakly Fréchet differentiable, re-
spectively) at u0 ∈U.
2. If φ : U →E1 is (weakly) Fréchet differentiable at u0 ∈U, then it is (weakly)
Gâteaux-differentiable at u0 ∈U.
3. If φ is (weakly) Gâteaux-differentiable, and the map dφ : U →Lin(E1,E2),
u →duφ is continuous, then φ is (weakly) Fréchet differentiable and hence a
(weak) C1-map.
Evidently, these properties explain why in the case of a continuous differential
dφ, there is no distinction between Gâteaux- and Fréchet-differentiability. The fol-
lowing gives a criterion for weak C1-maps.

C
Banach Manifolds
385
Proposition C.2 Let φ : E1 ⊃U →E2 be as above. Then the following statements
hold:
(1) If φ is a weak C1-map, then for any α ∈E′
2, the map αφ : U →R is a C1-map,
and
∂vαφ = α∂vφ.
(C.2)
(2) Conversely, if αφ : U →R is a C1-map for every α ∈E′
2, and if for all v there is
an element ∂vφ ∈E2 such that (C.2) holds for all α, then φ is a weak C1-map.
The existence of the elements ∂vφ follows, e.g., if E2 is reﬂexive.
If this is the case, then
∥∂V0φ∥≤liminf
V →V0
∥∂V φ∥
and
limsup
V →V0
∥∂V0φ∥< ∞.
(C.3)
Proof If φ is Gâteaux-differentiable, then the deﬁnition implies that αφ is Gâteaux-
differentiable and (C.2) holds for any α ∈E′
2. Thus, if φ is a weak C1-map, then
αφ : U →R is a C1-map as well.
Conversely, suppose that αφ is a C1-map and for each v ∈E1 there is an
∂vφ ∈E2 such that (C.2) holds, then from the deﬁnition it now follows that
1
t

φ(u0 + tv) −φ(u0)

⇀∂vφ,
so that ∂vφ is the weak Gâteaux-derivative of φ. The uniform boundedness prin-
ciple now implies that the linear map v →∂vφ is bounded, so that φ is Gâteaux-
differentiable, and the weak continuity of the map dφ is immediate.
The two estimates in (C.3) follow immediately from (C.1).
□
As in the ﬁnite-dimensional case, this notion allows us to deﬁne the notion of a
Banach manifold which is a space modeled on open sets of a Banach space.
Deﬁnition C.2 (Banach manifold) A Banach manifold of class Ck is a set M with
a collection of pairs (Ui,φi)i∈I , called coordinate charts or parametrizations, sat-
isfying the following conditions:
(1) The sets Ui ⊂M cover M, i.e., /
i∈I Ui = M.
(2) φi : Ui →Ei is an injective map into some Banach space Ei, and the image
φi(Ui) ⊂Ei is open.
(3) For any i,j ∈I, φi(Ui ∩Uj) ⊂Ei is open, and the bijections
φj ◦φ−1
i
: φi(Ui ∩Uj) −→φj(Ui ∩Uj),
called the transition maps, are Ck-diffeomorphisms.
We deﬁne a topology on M by requiring that U ⊂M is open if and only if
φi(U ∩Ui) ⊂Ei is open for all i ∈I. In this topology, Ui ⊂M is open and φi :

386
C
Banach Manifolds
Ui →φi(Ui) is a homeomorphism. We shall usually assume that this topology is
Hausdorff.
From this deﬁnition it is clear that open subsets of a Banach manifold and prod-
ucts of Banach manifolds are again Banach manifolds.
For μ ∈M we deﬁne an equivalence relation on the disjoint union &
μ∈Ui Ei
by v ∈Ei ∼d(φj ◦φ−1
i
)φi(μ)(v) ∈Ej. Similarly, on the disjoint union &
μ∈Ui E′
i
we deﬁne the equivalence relation v′ ∈E′
i ∼d(φi ◦φ−1
j )∗
φi(μ)(v′) ∈E′
j. The equiv-
alence classes of these relations are called the tangent vectors at μ and cotangent
vectors at μ, respectively, and the set of all (co-)tangent vectors at μ is called the
(co-)tangent space of M at μ, denoted by TμM and T ∗
μM, respectively.
Evidently, for each i ∈I with μ ∈Ui each (co-)tangent vector has a unique rep-
resentative in Ei and E′
i, respectively, and since the equivalence relations above are
deﬁned via bounded Banach space isomorphism, it follows that on TμM and T ∗
μM
there is a unique structure of a Banach space such that the identiﬁcations Ei ∼= TμM
and E′
i ∼= T ∗
μM are Banach space isomorphisms.
As a next step, we deﬁne differentiable maps between Banach manifolds.
Deﬁnition C.3 (Differentiable maps)
Let M,
˜
M be Banach manifolds of class
Ck with the parametrizations (Ui,φi)i∈I and ( ˜Uj, ˜φj)j∈J . A map Φ : M →˜
M is
called a (weak) Ck-map if the following hold:
(1) Φ is continuous.
(2) For all i ∈I and j ∈J , the map Φj
i := ˜φj ◦Φ ◦φi : φi(Ui ∩Φ−1( ˜Uj)) →
φj(Uj) is a (weak) Ck-map.
Observe that because of the continuity of Φ, the sets φi(Ui ∩Φ−1( ˜Uj)) ⊂Ei are
open, so that the condition of differentiability in this deﬁnition makes sense.
It follows from Deﬁnition C.1 that Φ is a weak Ck-map if and only if αΦ :
M →R is a Ck-map for all α ∈E′
j, where Uj ⊂Ej.
For a differentiable map Φ : M →˜
M, let μ ∈M and ˜μ := Φ(μ) ∈˜
M. If i ∈I
and j ∈J are chosen such that μ ∈Ui and ˜μ ∈˜Uj, then the (weak) differential
(dΦj
i )φi(μ) : Ei →Ej and its dual (dΦj
i )∗
φi(μ) : E′
j →E′
i are bounded linear maps,
and from the deﬁnition of tangent and cotangent vectors it follows that this induces
well-deﬁned bounded linear maps
dΦμ : TμM −→T ˜μ ˜
M
and
dΦ∗
μ : T ∗
˜μ ˜
M −→T ∗
μM.

References
1. Aigner, M.: Combinatorial Theory. Springer, Berlin (1979)
2. Akin, E.: The Geometry of Population Genetics. Lecture Notes in Biomathematics, vol. 17,
pp. 30–31 (1979)
3. Akin, E.: Exponential families and game dynamics. Can. J. Math. XXXIV(2), 374–405
(1982)
4. Akin, E.: The differential geometry of population genetics and evolutionary games. In: Math-
ematical and Statistical Developments of Evolutionary Theory, vol. 299, pp. 1–94 (1990)
5. Aliprantis, C., Border, K.: Inﬁnite Dimensional Analysis. Springer, Berlin (2007)
6. Amari, S.: Theory of information spaces. A geometrical foundation of statistics. POST
RAAG Report 106 (1980)
7. Amari, S.: Differential geometry of curved exponential families curvature and information
loss. Ann. Stat. 10, 357–385 (1982)
8. Amari, S.: Differential-Geometric Methods in Statistics. Lecture Notes in Statistics, vol. 28.
Springer, Heidelberg (1985)
9. Amari, S.: Differential geometrical theory of statistics. In: Differential Geometry in Statis-
tical Inference, Institute of Mathematical Statistics, California. Lecture Notes–Monograph
Series, vol. 10 (1987)
10. Amari, S.: Information geometry on hierarchy of probability distributions. IEEE Trans. Inf.
Theory 47(5), 1701–1711 (2001)
11. Amari, S.: Information Geometry and Its Applications. Applied Mathematical Sciences,
vol. 194. Springer, Berlin (2016)
12. Amari, S.: Natural gradient works efﬁciently in learning. Neural Comput. 10, 251–276
(1998)
13. Amari, S., Armstrong, J.: Curvature of Hessian manifolds. Differ. Geom. Appl. 33, 1–12
(2014)
14. Amari, S., Ay, N.: Standard divergence in manifold of dual afﬁne connections. In: Geomet-
ric Science of Information, Proceedings of the 2nd International Conference on Geometric
Science of Information, Palaiseau, France, 28–30 October (2015)
15. Amari, S., Kurata, K., Nagaoka, H.: Information geometry of Boltzmann machines. IEEE
Trans. Neural Netw. 3, 260–271 (1992)
16. Amari, S., Nagaoka, H.: Methods of Information Geometry. Translations of Mathematical
Monographs, vol. 191. Am. Math. Soc./Oxford University Press, Providence/London (2000)
17. Aristotle: The Metaphysics, Books I–IX, translated by H. Tredennick. Harvard University
Press/William Heinemann Ltd., Cambridge/London (1933). Loeb Classical Library
18. Ay, N.: Information geometry on complexity and stochastic interaction. MPI MIS Preprint
95/2001
© Springer International Publishing AG 2017
N. Ay et al., Information Geometry, Ergebnisse der Mathematik und ihrer Grenzgebiete.
3. Folge / A Series of Modern Surveys in Mathematics 64,
DOI 10.1007/978-3-319-56478-4
387

388
References
19. Ay, N.: An Information-geometric approach to a theory of pragmatic structuring. Ann.
Probab. 30(1), 416–436 (2002)
20. Ay, N.: Locality of global stochastic interaction in directed acyclic networks. Neural Comput.
14(12), 2959–2980 (2002)
21. Ay, N.: Geometric Design Principles for Brains of Embodied Agents. Künstl. Intell. 29, 389–
399 (2015). doi:10.1007/s13218-015-0382-z
22. Ay, N.: Information geometry on complexity and stochastic interaction. Entropy 17(4), 2432–
2458 (2015). doi:10.3390/e17042432
23. Ay, N., Amari, S.: A novel approach to canonical divergences within information geometry.
Entropy 17, 8111–8129 (2015). doi:10.3390/e17127866
24. Ay, N., Erb, I.: On a notion of linear replicator equations. J. Dyn. Differ. Equ. 17, 427–451
(2005)
25. Ay, N., Jost, J., Lê, H.V., Schwachhöfer, L.: Information geometry and sufﬁcient statistics.
Probab. Theory Relat. Fields 162, 327–364 (2015)
26. Ay, N., Jost, J., Lê, H.V., Schwachhöfer, L.: Parametrized measure models. Bernoulli (2015).
To appear, arXiv:1510.07305
27. Ay, N., Jost, J., Lê, H.V., Schwachhöfer, L.: Invariant geometric structures on statistical
models. In: Nielsen, F., Barbaresco, F. (eds.) Proceedings of Geometric Science of Informa-
tion, Second International Conference, GSI 2015, Palaiseau, France, October 28–30, 2015.
Springer Lecture Notes in Computer Sciences, vol. 9389 (2015)
28. Ay, N., Knauf, A.: Maximizing multi-information. Kybernetika 42(5), 517–538 (2007)
29. Ay, N., Montúfar, G., Rauh, J.: Selection criteria for neuromanifolds of stochastic dynamics.
In: Yamaguchi, Y. (ed.) Advances in Cognitive Neurodynamics (III), pp. 147–154. Springer,
Dordrecht (2013)
30. Ay, N., Müller, M., Szkoła, A.: Effective complexity and its relation to logical depth. IEEE
Trans. Inf. Theory 56(9), 4593–4607 (2010)
31. Ay, N., Müller, M., Szkoła, A.: Effective complexity of stationary process realizations. En-
tropy 13, 1200–1211 (2011)
32. Ay, N., Olbrich, E., Bertschinger, N., Jost, J.: A geometric approach to complexity. Chaos
21, 037103 (2011)
33. Ay, N., Polani, D.: Information ﬂows in causal networks. Adv. Complex Syst. 11(1), 17–41
(2008)
34. Ay, N., Tuschmann, W.: Dually ﬂat manifolds and global information geometry. Open Syst.
Inf. Dyn. 9, 195–200 (2002)
35. Ay, N., Tuschmann, W.: Duality versus dual ﬂatness in quantum information geometry.
J. Math. Phys. 44(4), 1512–1518 (2003)
36. Ay, N., Wennekers, T.: Dynamical properties of strongly interacting Markov chains. Neural
Netw. 16, 1483–1497 (2003)
37. Balduzzi, D., Tononi, G.: Integrated information in discrete dynamical systems: motivation
and theoretical framework. PLoS Comput. Biol. 4, e1000091 (2008)
38. Balian, R., Alhassid, Y., Reinhardt, H.: Dissipation in many-body systems: a geometric ap-
proach based on information theory. Phys. Rep. 131, 1–146 (1986)
39. Barndorff-Nielsen, O.: Information and Exponential Families in Statistical Theory. Wiley Se-
ries in Probability and Mathematical Statistics. John Wiley & Sons, Ltd., Chichester (2014)
40. Bossomaier, T., Barnett, L., Harré, M., Lizier, J.T.: An Introduction to Transfer Entropy,
Information Flow in Complex Systems. Springer, Berlin (2017)
41. Barndorff-Nielsen, O.E., Jupp, P.E.: Approximating exponential models. Ann. Inst. Stat.
Math. 41, 247–267 (1989)
42. Barrett, A.B., Seth, A.K.: Practical measures of integrated information for time-series data.
PLoS Comput. Biol. 7(1), e1001052 (2011). doi:10.1371/journal.pcbi.1001052
43. Bauer, H.: Probability Theory. de Gruyter, Berlin (1996). Translated from the German by
R.B. Burckel

References
389
44. Bauer, M., Bruveris, M., Michor, P.: Uniqueness of the Fisher–Rao metric on the space
of smooth densities. Bull. Lond. Math. Soc. (2016). arXiv:1411.5577. doi:10.1112/blms/
bdw020
45. Bengtsson, I., ˙Zyczkowski, K.: Geometry of Quantum States. Cambridge University Press,
Cambridge (2006)
46. Bertschinger, N., Rauh, J., Olbrich, E., Jost, J., Ay, N.: Quantifying unique information.
Entropy 16, 2161 (2014). doi:10.3390/e16042161
47. Bialek, W., Nemenman, I., Tishby, N.: Predictability, complexity, and learning. Neural Com-
put. 13, 2409–2463 (2001)
48. Bickel, P.J., Klaassen, C.A.J., Ritov, Y., Wellner, J.A.: Efﬁcient and Adaptive Estimation for
Semiparametric Models. Springer, New York (1998)
49. Blackwell, D.: Equivalent comparisons of experiments. Ann. Math. Stat. 24, 265–272 (1953)
50. Bogachev, V.I.: Measure Theory, vols. I, II. Springer, Berlin (2007)
51. Börgers, T., Sarin, R.: Learning through reinforcement and replicator dynamics. J. Econ.
Theory 77, 1–14 (1997)
52. Borisenko, A.A.: Isometric immersions of space forms into Riemannian and pseudo-
Riemannian spaces of constant curvature. Usp. Mat. Nauk 56(3), 3–78 (2001); Russ. Math.
Surv. 56(3), 425–497 (2001)
53. Borovkov, A.A.: Mathematical Statistics. Gordon & Breach, New York (1998)
54. Brody, D.C., Ritz, A.: Information geometry of ﬁnite Ising models. J. Geom. Phys. 47, 207–
220 (2003)
55. Brown, L.D.: Fundamentals of Statistical Exponential Families with Applications in Statisti-
cal Decision Theory. Institute of Mathematical Statistics, Lecture Notes Monograph Series,
vol. 9 (1986)
56. Calin, O., Udri¸ste, C.: Geometric Modeling in Probability and Statistics. Springer, Berlin
(2014)
57. Campbell, L.L.: An extended Chentsov characterization of a Riemannian metric. Proc. Am.
Math. Soc. 98, 135–141 (1986)
58. Chaitin, G.J.: On the length of programs for computing binary sequences. J. Assoc. Comput.
Mach. 13, 547–569 (1966)
59. Cheng, S.Y., Yau, S.T.: The real Monge–Ampère equation and afﬁne ﬂat structures. In:
Chern, S.S., Wu, W.T. (eds.) Differential Geometry and Differential Equations. Proc. Bei-
jing Symp., vol. 1980, pp. 339–370 (1982)
60. Chentsov, N.: Geometry of the “manifold” of a probability distribution. Dokl. Akad. Nauk
SSSR 158, 543–546 (1964) (Russian)
61. Chentsov, N.: Category of mathematical statistics. Dokl. Akad. Nauk SSSR 164, 511–514
(1965)
62. Chentsov, N.: A systematic theory of exponential families of probability distributions. Teor.
Veroâtn. Primen. 11, 483–494 (1966) (Russian)
63. Chentsov, N.: A nonsymmetric distance between probability distributions, entropy and the
theorem of Pythagoras. Mat. Zametki 4, 323–332 (1968) (Russian)
64. Chentsov, N.: Algebraic foundation of mathematical statistics. Math. Oper.forsch. Stat., Ser.
Stat. 9, 267–276 (1978)
65. Chentsov, N.: Statistical Decision Rules and Optimal Inference, vol. 53. Nauka, Moscow
(1972) (in Russian); English translation in: Math. Monograph., vol. 53. Am. Math. Soc.,
Providence (1982)
66. Cover, T.M., Thomas, J.A.: Elements of Information Theory, 2nd edn. Wiley, New York
(2006)
67. Cramer, H.: Mathematical Methods of Statistics. Princeton University Press, Princeton
(1946)
68. Crutchﬁeld, J.P., Feldman, D.P.: Regularities unseen, randomness observed: levels of entropy
convergence. Chaos 13(1), 25–54 (2003)
69. Crutchﬁeld, J.P., Young, K.: Inferring statistical complexity. Phys. Rev. Lett. 63, 105–108
(1989)

390
References
70. Csiszár, I.: Eine informationstheoretische Ungleichung und ihre Anwendung auf den Beweis
der Ergodizität von Markoffschen Ketten. Magy. Tud. Akad. Mat. Kut. Intéz. Közl. 8, 85–108
(1963)
71. Csiszár, I.: A note on Jensen’s inequality. Studia Sci. Math. Hung. 1, 185–188 (1966)
72. Csiszár, I.: Information-type measures of difference of probability distributeions and indirect
observations. Studia Sci. Math. Hung. 2, 299–318 (1967)
73. Csiszár, I.: On topological properties of f -divergences. Studia Sci. Math. Hung. 2, 329–339
(1967)
74. Csiszár, I.: Information measures: a critical survey. In: Proceedings of 7th Conference on
Information Theory, Prague, Czech Republic, pp. 83–86 (1974)
75. Csiszár, I.: I-divergence geometry of probability distributions and minimization problems.
Ann. Probab. 3, 146–158 (1975)
76. Csiszár, I., Matúš, F.: Information projections revisited. IEEE Trans. Inf. Theory 49, 1474–
1490 (2003)
77. Csiszár, I., Matúš, F.: On information closures of exponential families: counterexample.
IEEE Trans. Inf. Theory 50, 922–924 (2004)
78. Csiszár, I., Shields, P.C.: Information theory and statistics: a tutorial. Found. Trends Com-
mun. Inf. Theory 1(4), 417–528 (2004)
79. Darroch, J.N., Ratcliff, D.: Generalized iterative scaling for log-linear models. Ann. Math.
Stat. 43, 1470–1480 (1972)
80. Darroch, J.N., Speed, T.P.: Additive and multiplicative models and interactions. Ann. Stat.
11(3), 724–738 (1983)
81. Dawid, A.P., Lauritzen, S.L.: The geometry of decision theory. In: Proceedings of the Second
International Symposium on Information Geometry and Its Applications, pp. 22–28 (2006)
82. Diaconis, P., Sturmfels, B.: Algebraic algorithms for sampling from conditional distributions.
Ann. Stat. 26(1), 363–397 (1998)
83. Dósa, G., Szalkai, I., Laﬂamme, C.: The maximum and minimum number of circuits and
bases of matroids. Pure Math. Appl. 15(4), 383–392 (2004)
84. Drton, M., Sturmfels, B., Sullivant, S.: Lectures on Algebraic Statistics (Oberwolfach Semi-
nars). Birkhäuser, Basel (2009)
85. Duane, S., Kennedy, A.D., Pendleton, B.J., Roweth, D.: Hybrid Monte Carlo. Phys. Lett. B
195, 216–222 (1987)
86. Dubrovin, B.: Geometry of 2D Topological Field Theories. LNM, vol. 1620, pp. 120–348.
Springer, Berlin (1996)
87. Dudley, R.M.: Real Analysis and Probability. Cambridge University Press, Cambridge
(2004)
88. Edwards, A.W.F.: The fundamental theorem of natural selection. Biol. Rev. 69, 443–474
(1994)
89. Efron, B.: Deﬁning the curvature of a statistical problem (with applications to second order
efﬁciency). Ann. Stat. 3, 1189–1242 (1975), with a discussion by C.R. Rao, Don A. Pierce,
D.R. Cox, D.V. Lindley, Lucien LeCam, J.K. Ghosh, J. Pfanzagl, Niels Keiding, A.P. Dawid,
Jim Reeds and with a reply by the author
90. Efron, B.: The geometry of exponential families. Ann. Stat. 6, 362–376 (1978)
91. Eguchi, S.: Second order efﬁciency of minimum contrast estimators in a curved exponential
family. Ann. Stat. 11, 793–803 (1983)
92. Eguchi, S.: A characterization of second order efﬁciency in a curved exponential family.
Ann. Inst. Stat. Math. 36, 199–206 (1984)
93. Eguchi, S.: Geometry of minimum contrast. Hiroshima Math. J. 22, 631–647 (1992)
94. Erb, I., Ay, N.: Multi-information in the thermodynamic limit. J. Stat. Phys. 115, 967–994
(2004)
95. Ewens, W.: Mathematical Population Genetics, 2nd edn. Springer, Berlin (2004)
96. Fisher, R.A.: On the mathematical foundations of theoretical statistics. Philos. Trans. R. Soc.
Lond. Ser. A 222, 309–368 (1922)
97. Fisher, R.A.: The Genetical Theory of Natural Selection. Clarendon Press, Oxford (1930)

References
391
98. Frank, S.A., Slatkin, M.: Fisher’s fundamental theorem of natural selection. Trends Ecol.
Evol. 7, 92–95 (1992)
99. Friedrich, Th.: Die Fisher-Information und symplektische Strukturen. Math. Nachr. 152,
273–296 (1991)
100. Fujiwara, A., Amari, S.I.: Gradient systems in view of information geometry. Physica D 80,
317–327 (1995)
101. Fukumizu, K.: Exponential manifold by reproducing kernel Hilbert spaces. In: Gibilisco,
P., Riccomagno, E., Rogantin, M.-P., Winn, H. (eds.) Algebraic and Geometric Methods in
Statistics, pp. 291–306. Cambridge University Press, Cambridge (2009)
102. Gauss, C.F.: Disquisitiones generales circa superﬁcies curvas. In: P. Dombrowski, 150 years
after Gauss’ “Disquisitiones generales circa superﬁcies curvas”. Soc. Math. France (1979)
103. Geiger, D., Meek, C., Sturmfels, B.: On the toric algebra of graphical models. Ann. Stat.
34(5), 1463–1492 (2006)
104. Gell-Mann, M., Lloyd, S.: Information measures, effective complexity, and total information.
Complexity 2, 44–52 (1996)
105. Gell-Mann, M., Lloyd, S.: Effective complexity. Santa Fe Institute Working Paper 03-12-068
(2003)
106. Gibilisco, P., Pistone, G.: Connections on non-parametric statistical models by Orlicz space
geometry. Inﬁn. Dimens. Anal. Quantum Probab. Relat. Top. 1(2), 325–347 (1998)
107. Gibilisco, P., Riccomagno, E., Rogantin, M.P., Wynn, H.P. (eds.): Algebraic and Geometric
Methods in Statistics. Cambridge University Press, Cambridge (2010)
108. Girolami, M., Calderhead, B.: Riemann manifold Langevin and Hamiltonian Monte Carlo.
J. R. Stat. Soc. B 73, 123–214 (2011)
109. Glimm, J., Jaffe, A.: Quantum Physics. A Functional Integral Point of View. Springer, Berlin
(1981)
110. Goldberg, D.E.: Genetic algorithms and Walsh functions: part I, a gentle introduction. Com-
plex Syst. 3, 153–171 (1989)
111. Grassberger, P.: Toward a quantitative theory of self-generated complexity. Int. J. Theor.
Phys. 25(9), 907–938 (1986)
112. Gromov, M.: Isometric embeddings and immersions. Sov. Math. Dokl. 11, 794–797 (1970)
113. Gromov, M.: Partial Differential Relations. Springer, Berlin (1986)
114. Günther, M.: On the perturbation problem associated to isometric embeddings of Riemannian
manifolds. Ann. Glob. Anal. Geom. 7, 69–77 (1989)
115. Hadeler, K.P.: Stable polymorphism in a selection model with mutation. SIAM J. Appl. Math.
41, 1–7 (1981)
116. Harper, M.: Information Geometry and Evolutionary Game Theory (2009). arXiv:0911.1383
117. Hastings, W.: Monte Carlo sampling methods using Markov chains and their applications.
Biometrika 57, 97–109 (1970)
118. Hayashi, M., Watanabe, S.: Information geometry approach to parameter estimation in
Markov chains. Ann. Stat. 44(4), 1495–1535 (2016)
119. Henmi, M., Kobayashi, R.: Hooke’s law in statistical manifolds and divergences. Nagoya
Math. J. 159, 1–24 (2000)
120. Hewitt, E.: Abstract Harmonic Analysis, vol. 1. Springer, Berlin (1979)
121. Hicks, N.: A theorem on afﬁne connections. Ill. J. Math. 3, 242–254 (1959)
122. Hofbauer, J.: The selection mutation equation. J. Math. Biol. 23, 41–53 (1985)
123. Hofbauer, J., Sigmund, K.: Evolutionary Games and Population Dynamics. Cambridge Uni-
versity Press, Cambridge (1998)
124. Hofrichter, J., Jost, J., Tran, T.D.: Information Geometry and Population Genetics. Springer,
Berlin (2017)
125. Hopcroft, J., Motvani, R., Ullman, J.: Introduction to Automata Theory, Languages, and
Computation. Addison-Wesley/Longman, Reading/Harlow (2001)
126. Hsu, E.: Stochastic Analysis on Manifolds. Am. Math. Soc., Providence (2002)
127. Ito, T.: A note on a general expansion of functions of binary variables. Inf. Control 12, 206–
211 (1968)

392
References
128. Jaynes, E.T.: Information theory and statistical mechanics. Phys. Rev. 106(4), 620–630
(1957)
129. Jaynes, E.T.: Information theory and statistical mechanics II. Phys. Rev. 108(2), 171–190
(1957)
130. Jaynes, E.T.: In: Larry Bretthorst, G. (ed.) Probability Theory: The Logic of Science. Cam-
bridge University Press, Cambridge (2003)
131. Jeffreys, H.: An invariant form for the prior probability in estimation problems. Proc. R. Soc.
Lond. Ser. A 186, 453–461 (1946)
132. Jeffreys, H.: Theory of Probability, 2nd edn. Clarendon Press, Oxford (1948)
133. Jost, J.: On the notion of ﬁtness, or: the selﬁsh ancestor. Theory Biosci. 121(4), 331–350
(2003)
134. Jost, J.: External and internal complexity of complex adaptive systems. Theory Biosci. 123,
69–88 (2004)
135. Jost, J.: Dynamical Systems. Springer, Berlin (2005)
136. Jost, J.: Postmodern Analysis. Springer, Berlin (2006)
137. Jost, J.: Geometry and Physics. Springer, Berlin (2009)
138. Jost, J., ¸Sim¸sir, F.M.: Afﬁne harmonic maps. Analysis 29, 185–197 (2009)
139. Jost, J.: Riemannian Geometry and Geometric Analysis, 7th edn. Springer, Berlin (2017)
140. Jost, J.: Partial Differential Equations. Springer, Berlin (2013)
141. Jost, J.: Mathematical Methods in Biology and Neurobiology. Springer, Berlin (2014)
142. Jost, J., Lê, H.V., Schwachhöfer, L.: Cramér–Rao inequality on singular statistical models I.
arXiv:1703.09403
143. Jost, J., Lê, H.V., Schwachhöfer, L.: Cramér–Rao inequality on singular statistical models II.
Preprint (2017)
144. Jost, J., Li-Jost, X.: Calculus of Variations. Cambridge University Press, Cambridge (1998)
145. Kahle, T.: Neighborliness of marginal polytopes. Beitr. Algebra Geom. 51(1), 45–56 (2010)
146. Kahle, T., Olbrich, E., Jost, J., Ay, N.: Complexity measures from interaction structures.
Phys. Rev. E 79, 026201 (2009)
147. Kakade, S.: A natural policy gradient. In: Advances in Neural Information Processing Sys-
tems, vol. 14, pp. 1531–1538. MIT Press, Cambridge (2001)
148. Kass, R., Vos, P.W.: Geometrical Foundations of Asymptotic Inference. Wiley, New York
(1997)
149. Kanwal, M.S., Grochow, J.A., Ay, N.: Comparing information-theoretic measures of com-
plexity in Boltzmann machines. Entropy 19(7), 310 (2017). doi:10.3390/e19070310
150. Kobayashi, S., Nomizu, K.: Foundations of Differential Geometry, vol. I. Wiley-Interscience,
New York (1963)
151. Kolmogorov, A.N.: A new metric invariant of transient dynamical systems and automor-
phisms in Lebesgue spaces. Dokl. Akad. Nauk SSSR (N.S.) 119, 861–864 (1958) (Russian)
152. Kolmogorov, A.N.: Three approaches to the quntitative deﬁnition on information. Probl. Inf.
Transm. 1, 4–7 (1965)
153. Krasnosel’skii, M.A., Rutickii, Ya.B.: Convex functions and Orlicz spaces. Fizmatgiz,
Moscow (1958) (In Russian); English translation: P. Noordfoff Ltd., Groningen (1961)
154. Kuiper, N.: On C1-isometric embeddings, I, II. Indag. Math. 17, 545–556, 683–689 (1955)
155. Kullback, S.: Information Theory and Statistics. Dover, New York (1968)
156. Kullback, S., Leibler, R.A.: On information and sufﬁciency. Ann. Math. Stat. 22(1), 79–86
(1951)
157. Lang, S.: Introduction to Differentiable Manifolds, 2nd edn. Springer, Berlin (2002)
158. Lang, S.: Algebra. Springer, Berlin (2002)
159. Lauritzen, S.: General exponential models for discrete observations. Scand. J. Stat. 2, 23–33
(1975)
160. Lauritzen, S.: Statistical manifolds. In: Differential Geometry in Statistical Inference, Insti-
tute of Mathematical Statistics, California. Lecture Note-Monograph Series, vol. 10 (1987)
161. Lauritzen, S.: Graphical Models. Oxford University Press, London (1996)
162. Lê, H.V.: Statistical manifolds are statistical models. J. Geom. 84, 83–93 (2005)

References
393
163. Lê, H.V.: Monotone invariants and embedding of statistical models. In: Advances in De-
terministic and Stochastic Analysis, pp. 231–254. World Scientiﬁc, Singapore (2007).
arXiv:math/0506163
164. Lê, H.V.: The uniqueness of the Fisher metric as information metric. Ann. Inst. Stat. Math.
69, 879–896 (2017). doi:10.1007/s10463-016-0562-0
165. Lebanon, G.: Axiomatic geometry of conditional models. IEEE Trans. Inf. Theory 51, 1283–
1294 (2005)
166. Liepins, G.E., Vose, M.D.: Polynomials, basis sets, and deceptiveness in genetic algorithms.
Complex Syst. 5, 45–61 (1991)
167. Liu, J.S.: Monte Carlo Strategies in Scientiﬁc Computing. Springer, Berlin (2001)
168. Li, M., Vitanyi, P.: An Introduction to Kolmogorov Complexity and Its Applications.
Springer, Berlin (1997)
169. Lovri´c, M., Min-Oo, M., Ruh, E.: Multivariate normal distributions parametrized as a Rie-
mannian symmetric space. J. Multivar. Anal. 74, 36–48 (2000)
170. Marko, H.: The bidirectional communication theory—a generalization of information theory.
IEEE Trans. Commun. COM-21, 1345–1351 (1973)
171. Massey, J.L.: Causality, feedback and directed information. In: Proc. 1990 Intl. Symp. on
Info. Th. and Its Applications, pp. 27–30. Waikiki, Hawaii (1990)
172. Massey, J.L.: Network information theory—some tentative deﬁnitions. DIMACS Workshop
on Network Information Theory (2003)
173. Matumoto, T.: Any statistical manifold has a contrast function—on the C3-functions tak-
ing the minimum at the diagonal of the product manifold. Hiroshima Math. J. 23, 327–332
(1993)
174. Matúš, F.: On equivalence of Markov properties over undirected graphs. J. Appl. Probab. 29,
745–749 (1992)
175. Matúš, F.: Maximization of information divergences from binary i.i.d. sequences. In: Pro-
ceedings IPMU 2004, Perugia, Italy, vol. 2, pp. 1303–1306 (2004)
176. Matúš, F.: Optimality conditions for maximizers of the information divergence from an ex-
ponential family. Kybernetika 43, 731–746 (2007)
177. Matúš, F.: Divergence from factorizable distributions and matroid representations by parti-
tions. IEEE Trans. Inf. Theory 55, 5375–5381 (2009)
178. Matúš, F.: On conditional independence and log-convexity. Ann. Inst. Henri Poincaré Probab.
Stat. 48, 1137–1147 (2012)
179. Matúš, F., Ay, N.: On maximization of the information divergence from an exponential fam-
ily. In: Vejnarova, J. (ed.) Proceedings of WUPES’03, University of Economics Prague,
pp. 199–204 (2003)
180. Matúš, F., Rauh, J.: Maximization of the information divergence from an exponential family
and criticality. In: Proceedings ISIT 2011, St. Petersburg, Russia, pp. 809–813 (2011)
181. Matúš, F., Studený, M.: Conditional independences among four random variables I. Comb.
Probab. Comput. 4, 269–278 (1995)
182. Megginson, R.: An Introduction to Banach Space Theory. Graduate Texts in Mathematics,
vol. 183. Springer, New York (1998)
183. Metropolis, M., Rosenbluth, A., Rosenbluth, M., Teller, A., Teller, E.: J. Chem. Phys. 21,
1087–1092 (1953)
184. Montúfar, G., Rauh, J., Ay, N.: On the Fisher metric of conditional probability polytopes.
Entropy 16(6), 3207–3233 (2014)
185. Montúfar, G., Zahedi, K., Ay, N.: A theory of cheap control in embodied systems. PLoS
Comput. Biol. 11(9), e1004427 (2015). doi:10.1371/journal.pcbi.1004427
186. Morozova, E., Chentsov, N.: Invariant linear connections on the aggregates of probability
distributions. Akad. Nauk SSSR Inst. Prikl. Mat. Preprint, no 167 (1988) (Russian)
187. Morozova, E., Chentsov, N.: Markov invariant geometry on manifolds of states. Itogi Nauki
Tekh. Ser. Sovrem. Mat. Prilozh. Temat. Obz. 6, 69–102 (1990)
188. Morozova, E., Chentsov, N.: Natural geometry on families of probability laws. Itogi Nauki
Tekh. Ser. Sovrem. Mat. Prilozh. Temat. Obz. 83, 133–265 (1991)

394
References
189. Morse, N., Sacksteder, R.: Statistical isomorphism. Ann. Math. Stat. 37, 203–214 (1966)
190. Moser, J.: On the volume elements on a manifold. Trans. Am. Math. Soc. 120, 286–294
(1965)
191. Moussouris, J.: Gibbs and Markov random systems with constraints. J. Stat. Phys. 10(1),
11–33 (1974)
192. Murray, M., Rice, J.: Differential Geometry and Statistics. Chapman & Hall, London (1993)
193. Nagaoka, H.: The exponential family of Markov chains and its information geometry. In: The
28th Symposium on Information Theory and Its Applications, SITA2005, Onna, Okinawa,
Japan, pp. 20–23 (2005)
194. Nagaoka, H., Amari, S.: Differential geometry of smooth families of probability distribu-
tions. Dept. of Math. Eng. and Instr. Phys. Univ. of Tokyo. Technical Report METR 82-7
(1982)
195. Nash, J.: C1-isometric imbeddings. Ann. Math. 60, 383–396 (1954)
196. Nash, J.: The imbedding problem for Riemannian manifolds. Ann. Math. 63, 20–64 (1956)
197. Naudts, J.: Generalised exponential families and associated entropy functions. Entropy 10,
131–149 (2008). doi:10.3390/entropy-e10030131
198. Naudts, J.: Generalized Thermostatistics. Springer, Berlin (2011)
199. Neveu, J.: Mathematical Foundations of the Calculus of Probability. Holden-Day Series in
Probability and Statistics (1965)
200. Neveu, J.: Bases Mathématiques du Calcul de Probabilités, deuxième édition. Masson, Paris
(1970)
201. Newton, N.: An inﬁnite-dimensional statistical manifold modelled on Hilbert space. J. Funct.
Anal. 263, 1661–1681 (2012)
202. Neyman, J.: Sur un teorema concernente le cosidette statistiche sufﬁcienti. G. Ist. Ital. Attuari
6, 320–334 (1935)
203. Niekamp, S., Galla, T., Kleinmann, M., Gühne, O.: Computing complexity measures for
quantum states based on exponential families. J. Phys. A, Math. Theor. 46(12), 125301
(2013)
204. Ohara, A.: Geometry of distributions associated with Tsallis statistics and properties of rela-
tive entropy minimization. Phys. Lett. A 370, 184–193 (2007)
205. Ohara, A., Matsuzoe, H., Amari, S.: Conformal geometry of escort probability and its appli-
cations. Mod. Phys. Lett. B 26(10), 1250063 (2012)
206. Oizumi, M., Amari, S., Yanagawa, T., Fujii, N., Tsuchiya, N.: Measuring integrated in-
formation from the decoding perspective. PLoS Comput. Biol. 12(1), e1004654 (2016).
doi:10.1371/journal.pcbi.1004654
207. Oizumi, M., Tsuchiya, N., Amari, S.: A uniﬁed framework for information integration based
on information geometry. In: Proceedings of National Academy of Sciences, USA (2016).
arXiv:1510.04455
208. Orr, H.A.: Fitness and its role in evolutionary genetics. Nat. Rev. Genet. 10(8), 531–539
(2009)
209. Pachter, L., Sturmfels, B. (eds.): Algebraic Statistics for Computational Biology. Cambridge
University Press, Cambridge (2005)
210. Page, K.M., Nowak, M.A.: Unifying evolutionary dynamics. J. Theor. Biol. 219, 93–98
(2002)
211. Papadimitriou, C.: Computational Complexity. Addison-Wesley, Reading (1994)
212. Pearl, J.: Causality: Models, Reasoning and Inference. Cambridge University Press, Cam-
bridge (2000)
213. Perrone, P., Ay, N.: Hierarchical quantiﬁcation of synergy in channels. Frontiers in robotics
and AI 2 (2016)
214. Petz, D.: Quantum Information Theory and Quantum Statistics (Theoretical and Mathemati-
cal Physics). Springer, Berlin (2007)
215. Pistone, G., Riccomagno, E., Wynn, H.P.: Algebraic Statistics: Computational Commutative
Algebra in Statistics. Chapman & Hall, London (2000)

References
395
216. Pistone, G., Sempi, C.: An inﬁnite-dimensional structure on the space of all the probability
measures equivalent to a given one. Ann. Stat. 23(5), 1543–1561 (1995)
217. Price, G.R.: Extension of covariance selection mathematics. Ann. Hum. Genet. 35, 485–590
(1972)
218. Prokopenko, M., Lizier, J.T., Obst, O., Wang, X.R.: Relating Fisher information to order
parameters. Phys. Rev. E 84, 041116 (2011)
219. Rao, C.R.: Information and the accuracy attainable in the estimation of statistical parameters.
Bull. Calcutta Math. Soc. 37, 81–89 (1945)
220. Rao, C.R.: Differential metrics in probability spaces. In: Differential Geometry in Statistical
Inference, Institute of Mathematical Statistics, California. Lecture Notes–Monograph Series,
vol. 10 (1987)
221. Rauh, J.: Maximizing the information divergence from an exponential family. Dissertation,
University of Leipzig (2011)
222. Rauh, J., Kahle, T., Ay, N.: Support sets in exponential families and oriented matroid theory.
Int. J. Approx. Reason. 52, 613–626 (2011)
223. Rényi, A.: On measures of entropy and information. In: Proceedings of the 4th Berkeley
Symposium on Mathematical Statistics and Probability, vol. 1, pp. 547–561. University of
California Press, Berkeley (1961)
224. Rice, S.H.: Evolutionary Theory. Sinauer, Sunderland (2004)
225. Riemann, B.: Über die Hypothesen, welche der Geometrie zu Grunde liegen. Springer, Berlin
(2013). Edited with a commentary J. Jost; English version: B. Riemann: On the Hypotheses
Which Lie at the Bases of Geometry, translated by W.K. Clifford. Birkhäuser, Basel (2016).
Edited with a commentary by J. Jost
226. Rissanen, J.: Stochastic Complexity in Statistical Inquiry. World Scientiﬁc, Singapore (1989)
227. Robert, C., Casella, G.: Monte Carlo Statistical Methods. Springer, Berlin (2004)
228. Rockafellar, R.T., Wets, R.J.B.: Variational Analysis. Springer, Berlin (2009)
229. Roepstorff, G.: Path Integral Approach to Quantum Physics. Springer, Berlin (1994)
230. Santacroce, M., Siri, P., Trivellato, B.: New results on mixture and exponential models by
Orlicz spaces. Bernoulli 22(3), 1431–1447 (2016)
231. Sato, Y., Crutchﬁeld, J.P.: Coupled replicator equations for the dynamics of learning in mul-
tiagent systems. Phys. Rev. E 67, 015206(R) (2003)
232. Schreiber, T.: Measuring information transfer. Phys. Rev. Lett. 85, 461–464 (2000)
233. Shahshahani, S.: A new mathematical framework for the study of linkage and selection. In:
Mem. Amer. Math. Soc, vol. 17 (1979)
234. Shalizi, C.R., Crutchﬁeld, J.P.: Computational mechanics: pattern and prediction, structure
and simplicity. J. Stat. Phys. 104, 817–879 (2001)
235. Shannon, C.E.: A mathematical theory of communication. Bell Syst. Tech. J. 27(3), 379–423
(1948)
236. Shima, H.: The Geometry of Hessian Structures. World Scientiﬁc, Singapore (2007)
237. Sinai, Ja.: On the concept of entropy for a dynamical system. Dokl. Akad. Nauk SSSR 124,
768–771 (1959) (Russian)
238. Smillie, J.: An obstruction to the existence of afﬁne structures. Invent. Math. 64, 411–415
(1981)
239. Solomonoff, R.J.: A formal theory of inductive inference. Inf. Control 7, 1–22 (1964). 224–
254
240. Stadler, P.F., Schuster, P.: Mutation in autocatalytic reaction networks—an analysis based on
perturbation theory. J. Math. Biol. 30, 597–632 (1992)
241. Studený, M.: Probabilistic Conditional Independence Structures. Information Science &
Statistics. Springer, Berlin (2005)
242. Studený, M., Vejnarová, J.: The multiinformation function as a tool for measuring stochas-
tic dependence. In: Jordan, M.I. (ed.) Learning in Graphical Models, pp. 261–298. Kluwer
Academic, Dordrecht (1998)
243. Takeuchi, J.: Geometry of Markov chains, ﬁnite state machines, and tree models. Technical
Report of IEICE (2014)

396
References
244. Tononi, G.: Consciousness as integrated information: a provisional manifesto. Biol. Bull.
215(3), 216–242 (2008)
245. Tononi, G., Sporns, O., Edelman, G.M.: A measure for brain complexity: relating functional
segregation and integration in the nervous systems. Proc. Natl. Acad. Sci. USA 91, 5033–
5037 (1994)
246. Tran, T.D., Hofrichter, J., Jost, J.: The free energy method and the Wright–Fisher model with
2 alleles. Theory Biosci. 134, 83–92 (2015)
247. Tran, T.D., Hofrichter, J., Jost, J.: The free energy method for the Fokker–Planck equation of
the Wright–Fisher model. MPI MIS Preprint 29/2015
248. Tsallis, C.: Possible generalization of Boltzmann–Gibbs statistics. J. Stat. Phys. 52, 479–487
(1988)
249. Tsallis, C.: Introduction to Nonextensive Statistical Mechanics: Approaching a Complex
World. Springer, Berlin (2009)
250. Tuyls, K., Nowe, A.: Evolutionary game theory and multi-agent reinforcement learning.
Knowl. Eng. Rev. 20(01), 63–90 (2005)
251. Vedral, V., Plenio, M.B., Rippin, M.A., Knight, P.L.: Quantifying entanglement. Phys. Rev.
Lett. 78(12), 2275–2279 (1997)
252. Wald, A.: Statistical Decision Function. Wiley, New York (1950)
253. Walsh, J.L.: A closed set of normal orthogonal functions. Am. J. Math. 45, 5–24 (1923)
254. Weis, S., Knauf, A., Ay, N., Zhao, M.J.: Maximizing the divergence from a hierarchical
model of quantum states. Open Syst. Inf. Dyn. 22(1), 1550006 (2015)
255. Wennekers, T., Ay, N.: Finite state automata resulting from temporal information maximiza-
tion. Neural Comput. 17, 2258–2290 (2005)
256. Whitney, H.: Differentiable manifolds. Ann. Math. (2) 37, 645–680 (1936)
257. Williams, P.L., Beer, R.D.: Nonnegative decomposition of multivariate information (2010).
arXiv:1004.2151
258. Winkler, G.: Image Analysis, Random Fields, and Markov Chain Monte Carlo Methods.
Springer, Berlin (2003)
259. Wu, B., Gokhale, C.S., van Veelen, M., Wang, L., Traulsen, A.: Interpretations arising from
Wrightian and Malthusian ﬁtness under strong frequency dependent selection. Ecol. Evol.
3(5), 1276–1280 (2013)
260. Zhang, J.: Divergence function, duality and convex analysis. Neural Comput. 16, 159–195
(2004)
261. Ziegler, G.M.: Lectures on Polytopes. GTM, vol. 152. Springer, Berlin (1994)

Index
Symbols
A-interaction, 101
C1-map, 150
Ck-map (weak), 384
L1-topology, 122
α-connection, 50
α-divergence, 73, 74
α-geodesic, 50
∞-integrable parametrized measure
model, 153
κ-congruent embedding, 248
S-complexity, 298, 321
ϕ-regular estimator, 281
ϕ-efﬁcient, 287
e-connection, 43, 45
e-convergence of measures, 170
e-convex, 81
e-exponential map, 44, 46
e-geodesic, 43, 45, 81
e-topology, 124, 181
f -divergence, 76
k-integrable parametrized measure model, 138,
153, 155
k-integrable signed parametrized measure
model, 169
k-th order information loss, 243
m-connection, 43, 45
m-convex, 81
m-convex exponential family, 82
m-exponential map, 44, 46
m-geodesic, 43, 45, 81
q-divergence, 78
q-exponential, 79
q-generalization, 78
q-logarithm, 79
2-point function, 359
3-symmetric tensor, 191, 198
A
Afﬁne connection, 43, 372
Afﬁne coordinate system, 194, 195
Afﬁne coordinates, 198, 375
Afﬁne ﬂat, 376
Afﬁne manifold, 376
Afﬁne space, 79
Algebraically generated families of tensors,
62, 272
Allele, 336
Amari–Chentsov tensor, 47, 48, 57, 137, 138,
167, 187, 190, 275
Annihilator, 157
Asymptotic Cramér-Rao inequality, 293
Asymptotic theory of estimation, 292
Atom, 143
Autoparallel submanifold, 377
Average ﬁtness, 329
B
Banach manifold, 124, 385
Banach space, 381
Base measure, 123
Bayesian scheme, 348
Bias, 279
Borel sigma-algebra, 362
Brownian motion, 350
C
Campbell, 49, 62, 275
Campbell’s theorem, generalization of, 275
Canonical basis, 25
Canonical divergence, 201, 210, 213, 214
Canonical n-tensor, 57, 271, 273
© Springer International Publishing AG 2017
N. Ay et al., Information Geometry, Ergebnisse der Mathematik und ihrer Grenzgebiete.
3. Folge / A Series of Modern Surveys in Mathematics 64,
DOI 10.1007/978-3-319-56478-4
397

398
Index
Canonical n-tensor of a measurable
space Ω, 165
Canonical pairing, 146
Canonical tensor, 58, 138, 167
Canonical tensor of a partition, 58
Causal information ﬂow, 318
Centered Cramer class, 177
Chapman–Kolmogorov equation, 338
Characteristic function, 363
Chentsov, 47, 62
Chentsov’s characterization, 35
Chentsov’s theorem, 36, 268
Chentsov’s theorem, generalization of, 274
Christoffel symbols, 373
Circuit, 90
Circuit basis, 90
Circuit vector, 90
Classiﬁcation of congruent families of
covariant n-tensors, 62, 272
Compatibility class, 122
Compatible measures, 121, 143, 170, 171, 366
Complete connection, 377
Complex systems, 295
Composition of Markov kernels, 256
Conditional entropy, 299
Conditional multi-information, 300
Conditional mutual information, 299
Congruent embedding, 54
Congruent family of tensors, 54, 57, 62, 270,
272
Congruent Markov kernel, 54, 256
Congruent Markov morphisms, 256
Connection, 372
Consistency, 292
Consistent estimator, 292, 293
Continuous time limit, 333
Continuum limit, 338
Contrast function, 78, 211
Contravariant, 370
Convex support, 85
Coordinate change, 367
Correlator, 359
Cotangent vector of Banach manifold, 386
Covariance matrix, 208
Covariant, 370
Covariant derivative, 43, 372
Covariant n-tensor, 32, 53, 164, 268, 270
Covariant n-tensor, permutation of, 56
Covariant n-tensor ﬁeld, 165
Covariantly constant, 374
Covector, 369
Cramer class, 177
Cramér–Rao inequality, 281, 286, 292
Curvature, 185
Curvature tensor, 375, 376
Curved exponential family, 198
D
Density function, 136, 150
Diffeomorphism, 384
Difference vector, 79
Differentiable map between Banach
manifolds, 386
Differential (weak), 383
Directed information, 296, 317
Dirichlet integral, 357
Divergence, 68, 78, 202, 203, 206, 211
Dominated model, 150
Dual basis, 26, 29
Dual connection, 377
Dual potential, 206
Dual space, 382
Dualistic manifold, 191
Dualistic space, 191
Dualistic structure, 187, 191
Duality, 43
Dually ﬂat, 379
Dually ﬂat structure, 195, 200
E
Efﬁcient estimator, 287
Energy, 356
Entropy, 205, 207, 298, 343, 348, 354, 355
Entropy rate, 316
Equivalent norms, 381
Error function, 293
Essential tangent bundle, 284
Estimator, 277
Euler method, 333
Evolutionary biology, 328
Evolutionary dynamics, 327
Excess entropy, 312, 316
Expectation coordinates, 281
Expectation value, 278, 354
Expected exit time, 343
Exponential connection, 194
Exponential distribution, 355
Exponential family, 79, 81, 132, 153, 194, 204,
208, 210, 280, 359
Exponential map, 43, 376
Exponential tangent space, 176
Extension of a covariant n-tensor, 53
Extrinsic perspective, 185
F
Fisher metric, 2, 31, 57, 136, 138, 167, 187,
189, 207, 274, 277, 284, 336, 351, 359
reduced Fisher metric, 284, 285

Index
399
Fisher–Neyman characterization, 242, 263
Fisher–Neyman sufﬁcient statistic, 242, 244,
261, 266, 267
Fitness, 329
Flat connection, 375, 376
Fokker–Planck equation, 336, 339
Formal derivative of Kr, 269
Fréchet-differentiable map (weak), 383
Free energy, 205, 346, 347, 355
Free energy functional, 343
Frobenius condition, 371
Frobenius manifold, 199
Frobenius theorem, 371
Functional integral, 356, 357, 359
Fundamental theorem of natural selection, 330
G
Gâteaux-differentiable map (weak), 383
Gaussian distribution, 22
Gaussian family, 209
Gaussian integral, 209, 357
Geodesic, 43, 203, 376
Gibbs distribution, 355
Gibbs–Markov equivalence, 100
Global Markov property, 113
Gradient, 284, 371
Gradient ﬁelds, 39
Gradient ﬂow, 209
Granger causality, 318
Graphical models, 112
Green function, 360
Green operator, 358
H
Hahn–Banach theorem, 164, 382, 383
Half-density, 128, 137
Hamiltonian Monte Carlo method, 351
Hamiltonian system, 352, 353
Hammersley–Clifford theorem, 115
Hausdorff dimension, 123
Hausdorff measure, 123
Hellinger distance, 34
Hessian structure, 197
Hierarchical model, 108
Hybrid Monte Carlo method, 351
Hyperedges, 109
Hypergraph, 109
I
Implicit description of an exponential family,
84
Information divergence, 71
Information geometry, 21
Information loss of a statistic or a Markov
kernel, 243
Integrable function, 364
Integral, 363, 364
Integrated information, 325
Interaction potential, 100
Interaction space, 101
Intrinsic perspective, 185
Invariance, 48
Invariant tensor, 54, 62
Inverse temperature, 356
Isometric embedding, 185
Isostatistical immersion, 221
Iterative scaling, 325
J
Jordan decomposition theorem, 139
K
Kähler afﬁne structure, 197
KL-divergence, 70
Kolmogorov backward equation, 339, 347
Kolmogorov forward equation, 336, 339, 343
Kolmogorov–Sinai entropy, 296, 316
Kullback–Leibler divergence, 70, 71, 206, 209
L
Lagrange multiplier, 355
Lebesgue measure, 123
Legendre transformation, 197, 199
Levi-Civita connection, 190, 198, 378
Lie bracket, 369
Lie group, 123
Linear ﬁtness, 331
Linear monotone invariant, 223
Linear statistical manifold, 223
Local Markov property, 113
Logarithmic derivative, 6, 138, 152, 168
M
Malthusian ﬁtness, 334
Markov chain Monte Carlo, 349
Markov kernel, 35, 241, 253
Markov morphism, 254, 270
Markov properties, 113
Markov transition, 253
Matroid theory, 91
Maximization of information divergence, 302
Maximum entropy method, 99
Maximum likelihood, 280
Maximum likelihood estimator, 280, 281, 287,
288, 293
Mean ﬁtness, 329
Mean square error, 279

400
Index
Measurable, 362
Measurable function, 364
Measure, 361, 362
Metric, 370
Metric tensor, 370
Metropolis adjusted Langevin algorithm, 350
Metropolis algorithm, 349
Metropolis–Hastings algorithm, 349
Mixture connection, 195
Mixture family, 195, 206, 355
Mixture geodesic, 43
Möbius inversion, 103
Monotone invariant, 223
Monotonicity formula, 244
Monte Carlo method, 348
Moser, 126
Multi-information, 300
Multi-information rate, 319
Multiindex, 63
Mutation, 340
Mutual information, 299
N
Nash embedding theorem, 185
Natural selection, 328
Non-complex systems, 297
Nonextensive statistical mechanics, 79
Normal distribution, 22, 151
Normal space, 370
Normalized potential, 108
Null set, 363
O
Observable, 206
One-form, 370
Orlicz space, 172
P
Pairwise Markov property, 113
Parallel transport, 42, 374
Parametrized measure model, 121, 135, 138,
150, 188
Partition function, 205
Partition induced by a multiindex, 63
Partition of ﬁnite set, 58
Permutation of a covariant n-tensor, 56
Pistone–Sempi structure, 170
Population, 336
Potential energy, 205
Potential function, 196
Power of probability measure, 137
Pre-gradient, 284
Probability measure, 362
Projection onto an exponential family, 91
Projective space, 127
Propagator, 210
Pull-back of a tensor, 53, 166
Pullback of a measure, 365
Pure A-interaction, 102
Push-forward of a measure, 126, 244, 365
Pythagoras relation, 203
Q
Quantum states, 302
Quasispecies equation, 332
R
Radon measure, 363
Radon–Nikodym derivative, 122, 138, 366
Radon–Riesz theorem, 157, 163, 382
Regular density function, 150, 244
Relative entropy, 70, 71
Relative frequencies, 327
Replicator equation, 329
Replicator-mutator equation, 332
Restriction of a covariant n-tensor, 53
Riemannian connection, 378
Riemannian geometry, 185, 367
Riemannian metric, 31, 370
Riesz representation theorem, 364
Root of (signed) measure, 144
S
Sampling of probability distributions, 348
Selection, 340
Self-dual, 378
Shahshahani metric, 328
Sigma-algebra, 361
Signed ﬁnite measure, 121
Signed measures, 26
Signed parametrized measure model, 135, 168
Similar measures, 178
Simple function, 363
Simplicial complex, 109
Split kernel, 323
Split process, 319
Statistic, 241
Statistical inference, 277
Statistical manifold, 187, 191, 220
Statistical model, 135, 139, 150, 189, 277
Statistical structure, 191
Stochastic interaction, 324
Strictly convex, 196
Subdivision of a partition, 58
Submanifold, 370
Sufﬁcient statistic, 261, 266, 267
Symmetric connection, 374
Synergistic information, 325

Index
401
T
Tangent double cone of a subset of a Banach
space, 141
Tangent ﬁbration of a subset of a Banach
space, 141
Tangent space, 368
Tangent vector, 141, 368
Tangent vector of Banach manifold, 386
Tensor product of covariant n- and
m-tensors, 56
Theoretical biology, 31
Torsion tensor, 373
Torsion-free, 211, 374
Total variation, 52, 121
Transfer entropy, 296, 317, 319
Transition map, 385
Transverse measure, 250
TSE-complexity, 308, 312
Type, 336
U
Unbiased, 279, 281, 287
Uniform boundedness principle, 165, 382, 383,
385
V
Vacuum state, 108
Variance, 279
Vector ﬁeld, 369
W
Walsh basis, 106
Weak convergence, 382
Weakly C1-map, 138
Weakly continuous function, 382
Weakly differentiable map between Banach
manifolds, 386
Weakly Fréchet-differentiable map, 384
Weakly Gâteaux-differentiable map, 159, 383
Weakly k-integrable parametrized measure
model, 138, 153, 155
Weighted information distance, 307
Wright–Fisher model, 336
Wrightian ﬁtness, 334, 340
Y
Young function, 172
Z
Zustandssumme, 205

Nomenclature
N
natural numbers, positive integers, page 7
N(·;x,Λ), N(x,Λ)
set of normal or
Gaussian distributions with center or
mean x and covariance matrix
Λ = (λij), see equation (1.44), page 23
I
ﬁnite set of elementary events, page 25
F(I)
algebra of functions on I, page 25
1I , 1
unity of the algebra F(I), page 25
ei, i ∈I
canonical basis of F(I), page 25
S(I)
dual space of F(I), space of signed
measures on I, page 26
δi, i ∈I
dual basis of ei, i ∈I, page 26
Sa(I)
set of signed measures on I with total
mass a, see equation (2.1), page 26
M(I)
set of positive measures on I, see
equation (2.1), page 26
M+(I)
set of strictly positive measures on
I, see equation (2.1), page 26
P(I)
set of probability measures on I,
page 26
P+(I)
set of strictly positive probability
measures on I, see equation (2.1),
page 26
TμS(I)
tangent space of S(I) in μ, see
equation (2.2), page 27
T ∗
μS(I)
cotangent space of S(I) in μ, see
equation (2.3), page 27
TμM+(I)
tangent space of M+(I) in μ, see
equation (2.5), page 27
T ∗
μM+(I)
cotangent space of M+(I) in μ,
see equation (2.5), page 27
TμP+(I)
tangent space of P+(I) in μ, see
equation (2.5), page 27
F(I)/R
space of functions modulo constant
functions, page 27
T ∗
μP+(I)
cotangent space of P+(I) at μ, see
equation (2.7), page 27
T S(I)
tangent bundle of S(I), see equation
(2.8), page 28
T ∗S(I)
cotangent bundle of S(I), see
equation (2.8), page 28
T M+(I)
tangent bundle of M+(I), see
equation (2.8), page 28
T ∗M+(I)
cotangent bundle of M+(I),
page 28
T P+(I)
tangent bundle of P+(I), see
equation (2.8), page 28
T ∗P+(I)
cotangent bundle of P+(I),
page 28
⟨f,g⟩μ
the L2-product of functions
f,g ∈F(I) with respect to μ, see
equation (2.9), page 29
f μ
measure with density f with respect to
μ, see equation (2.10), page 29
dν
dμ
Radon–Nikodym derivative of ν with
respect to μ, page 30
g
Fisher metric, page 31
ˆg
redused Fisher metric, page 284
dF
Fisher distance, see equation (2.26),
page 34
dH
Hellinger distance, see equation (2.28),
page 35
Kf
Markov kernel induced by a
deterministic map f , page 35
K∗
pushforward map of a Markov kernel K,
see equation (2.30), page 35
&
disjoint union, page 38
dμV
differential of V in μ, see equation
(2.33), page 39
gradμ V
gradient of V in μ, see equation
(2.37), page 40
© Springer International Publishing AG 2017
N. Ay et al., Information Geometry, Ergebnisse der Mathematik und ihrer Grenzgebiete.
3. Folge / A Series of Modern Surveys in Mathematics 64,
DOI 10.1007/978-3-319-56478-4
403

404
Nomenclature

Π(m)
μ,ν
mixture (m) parallel transport on
M+(I), see equation (2.39), page 42

Π(e)
μ,ν
exponential (e) parallel transport on
M+(I), see equation (2.40), page 42
∇(m)
m-connection on M+(I), see equation
(2.42), page 43
∇(e)
e-connection on M+(I), see equation
(2.42), page 43
.
exp(m)
m-connection on M+(I), see
equation (2.43), page 44
.
exp(e)
e-connection on M+(I), see equation
(2.43), page 44
∇(m)
m-connection on P+(I), see equation
(2.47), page 45
∇(e)
e-connection on P+(I), see equation
(2.47), page 45
T
Amari–Chentsov tensor, page 47
τ n
μ
canonical covariant n-tensor on M+(I),
see equation (2.52), page 48
∇(α)
α-connection on M+(I), page 50
.
exp(α)
exponential map of the α-connection
on M+(I), page 50
∇(α)
α-connection on P+(I), see equation
(2.60), page 51
δi
Dirac measure supported at i ∈I, page 52
πI
rescaling projection from M+(I) to
P+(I), page 52
Θn
I
covariant n-tensor on P+(I), page 53
×
n S
n-fold Cartesian product of the set S,
page 53
˜Θn
I
covariant n-tensor on M+(I), page 53
⃗i
multiindex of I, element of I n, page 53
θ⃗i
I;μ
component functions of a covariant
n-tensor on M+(I), page 53
δ⃗i
multiindex of Dirac distributions, page 53
σ
permutation of a ﬁnite set, page 56
( ˜Θn
I )σ , (Θn
I )σ
permutation of ˜Θn
I by σ,
page 56
˜Θn
I ⊗˜Ψ m
I , Θn
I ⊗Ψ m
I
tensor product of
covariant n- and m-tensors on I,
page 56
τ n
I
canonical n-tensor on M+(I), page 57
Part(n)
set of partitions of {1,...,n},
page 58
|P|
number of sets in a partition P ∈Part(n),
page 58
πP
map associated to a partition
P ∈Part(n), page 58
≤
partial ordering of partition of Part(n) by
subdivision, page 58
τ P
I
canonical n-tensor of a partition
P ∈Part(n), page 59
P(⃗i)
partition of I induced by a multiindex ⃗i,
page 63
X(q,p)
the “difference vector” satisfying
expq(X(q,p)) = p, page 69

X(m)(ν,μ)
inverse of the exponential map of
the m-connection on M+(I), page 70

X(e)(ν,μ)
inverse of the exponential map
with respect to the e-connection on
M+(I), see equation (2.97), page 70
X(m)(ν,μ)
inverse of the exponential map of
the m-connection on P+(I), see
equation (2.104), page 73
X(e)(ν,μ)
inverse of the exponential map
with respect to the e-connection on
P+(I), page 73
D(α)
α-divergence, page 74
Df
f -divergence, see equation (2.118),
page 76
vec(μ,ν)
difference vector between μ and ν
in M+(I), page 80
E(μ0,L)
exponential family with base
measure μ0 and tangent space L, see
equation (2.132), page 81
E(L)
exponential family with the counting
measure as base measure and tangent
space L, page 81
M(μA : A ∈S)
m-convex exponential
family, see equation (2.133), page 81
n+,n−
positive and negative part of a vector
n ∈F(I), page 85
cs(E)
convex support of an exponential
family E, page 85
E(μ0,L)
closure of the exponential family
E(μ0,L) with respect to the natural
topology on P(I) ⊆S(I), page 87
M(μ,L)
set of distributions that assign to
all f ∈L the same expectation values
as μ, page 94
H(ν)
Shannon entropy of ν, page 99
IA
Cartesian product×v∈A Iv, see
equation (2.181), page 101
XA
natural projections, page 101
p(iA) = pA(iA)
marginal distribution, see
equation (2.183), page 101
p(iB|iA)
conditional distribution, page 101
FA
algebra of real-valued functions
f ∈F(IV ) that only depend on A,
page 101

Nomenclature
405
f (iA)
A-marginal of a function f ∈F(IV ),
page 101
PA
orthogonal projection onto FA, see
equation (2.186), page 101

FA
space of pure A-interactions, see
equation (2.188), page 102

PA
orthogonal projection onto 
FA, page 102
eA, A ⊆V
Walsh basis, page 106
S
hypergraph, page 109
Smax
maximal elements of the hypergraph
S, page 109
S
simplicial complex of the hypergraph S,
page 109
FS
sum of spaces FA, A ∈S, see equation
(2.204), page 109
ES
hierarchical model generated by S,
page 109
Sk
hypergraph of subsets of cardinality k,
see equation (2.212), page 111
E(k)
hierarchical model generated by Sk,
page 111
V
k

set of subsets of V that have cardinality
k, page 112
v ∼w
v and w neighbors, page 112
bd(v)
boundary of v, page 112
bd(A)
boundary of A, page 112
cl(A)
closure of A, page 112
C(G)
set of cliques of the graph G, page 113
E(G)
graphical model of the graph G,
page 113
XA ⊥⊥XB |XC
conditional independence of
XA and XB given XC, page 113
M(G)
set of distributions that satisfy the
global Markov property, page 117
M(G)
+
set of strictly positive distributions
that satisfy the global Markov property,
page 117
M(L)
set of distributions that satisfy the
local Markov property, page 117
M(L)
+
set of strictly positive distributions that
satisfy the local Markov property,
page 117
M(P)
set of distributions that satisfy the
pairwise Markov property, page 117
M(P)
+
set of strictly positive distributions that
satisfy the pairwise Markov property,
page 117
E(G)
extended graphical model of the graph
G, page 117
M(F)
set of factorized distributions,
page 117
M(F)
+
set of strictly positive factorized
distributions, page 117
I (G)
ideal generated by the global Markov
property, page 118
I (L)
ideal generated by the local Markov
property, page 118
I (P)
ideal generated by the pairwise Markov
property, page 118
I (G)
ideal with variety E(G), page 118
IS
ideal with variety ES, page 119
B
σ-Algebra on a measure space Ω,
page 121
S(Ω)
space of signed bounded measures on
Ω, page 121
∥· ∥T V
total variation of a bounded signed
measure, page 121
M(Ω)
set of ﬁnite measures on Ω, page 121
P(Ω)
set of probability measures on Ω,
page 121
S(Ω,μ0)
set of signed measures dominated
by μ0, page 122
ican
canonical map identifying S(Ω,μ0) and
L1(Ω,μ0), page 122
F+ = F+(Ω,μ)
positive μ-integrable
functions on Ω, page 122
M+ = M+(Ω,μ)
compatibility class of
measures on Ω, page 122
Bμ
formal tangent cone of the measure μ,
page 123
TμF+
formal tangent space of the measure
μ, page 123
T ∞
μ M+
set of bounded formal tangent
vectors, page 125
T 2
μM+
set of formal tangent vectors of class
L2, page 125
D(Ω)
diffeomorphism group of a manifold
Ω, page 126
P1M(Ω)
projectivization of M(Ω),
page 127
H
hyperbolic half plane, page 133
Sr(Ω)
space of r-th powers of ﬁnite signed
measures on Ω, page 137
Pr(Ω)
space of r-th powers of probability
measures on Ω, page 137
Mr(Ω)
space of r-th powers of ﬁnite
measures on Ω, page 137
p1/k
k-th root of a parametrized measure
model, page 138
τ n
M
canonical n-tensor of a parametrized
measure M, page 138

406
Nomenclature
S0(Ω)
Banach space of signed ﬁnite
measures on Ω with expectation value
0, page 139
M(Ω,μ0)
set of ﬁnite measures on Ω
dominated by μ0, page 140
P+(Ω,μ0)
set of probability measures on Ω
compatible with μ0, page 140
P(Ω,μ0)
set of probability measures on Ω
dominated by μ0, page 140
μ1 ≤μ2
measure μ2 dominates the measure
μ1, page 143
ıμ1
μ2
inclusion L1/r(Ω,μ1) →L1/r(Ω,μ2)
for μ1 ≤μ2, page 144
Sr(Ω,μ)
elements of Sr(Ω) dominated by
μ, page 145
Mr(Ω,μ)
elements of Mr(Ω) dominated
by μ, page 145
Pr(Ω,μ)
elements of Pr(Ω) dominated by
μ, page 145
(·;·)
canonical pairing of Sr(Ω) and
S1−r(Ω), page 146
˜πk
k-th signed power of powers of signed
measures, page 148
πk
k-th absolute power of powers of signed
measures, page 148
(M,Ω,p)
parametrized measure model or
statistical model on Ω, page 150
(M,Ω,μ0,p)
parametrized measure model
dominated by μ0, page 150
p(·;ξ)
density function of a parametrized
measure model (M,Ω,μ0,p),
page 150
⇀
weak convergence in a (Banach) space,
page 158
Ln
Ω
canonical n-tensor on S1/n(Ω),
page 165
τ n
Ω;r
canonical n-tensor on Sr(Ω), page 165
Φ∗Θ
pull-back of a covariant n-tensor Θ by
Φ, page 166
logμ0
logarithm of measure w.r.t. base
measure μ0, page 170
Lφ(μ)
Orlicz space of the Young function φ
on (Ω,μ), page 172
∥· ∥φ,μ
Orlicz norm on Lφ(μ), page 173
ˆBμ(Ω)
image of the logarithm map logμ,
page 176
Bμ(Ω)
intersection of ˆBμ(Ω) and −ˆBμ(Ω),
page 176
B0
μ(Ω)
inner points of Bμ(Ω), page 176
TμM+
exponential tangent space of M+ at
μ, page 176
μ′ ⪯μ
μ′ = φμ for φ ∈Lp(Ω,μ) for some
p > 1, page 178
μ′ ∼μ
μ′ is similar to μ, i.e., μ′ ⪯μ and
μ ⪯μ′, page 178
∇(α)
α-connection, see equation (4.4),
page 190
Γ (α)
ijk
α-Christoffel symbols, see equation
(4.4), page 190
∂i :=
∂
∂ϑi
vector ﬁelds for ϑ-coordinates, see
equation (4.27), page 196
∂j
abbreviation for vector ﬁelds, see
equation (4.27), page 196
D(p∥q)
canonical divergence, page 201
Z(ϑ) := eψ(ϑ)
partition function
(zustandssumme), see equation (4.69),
page 205
I(A,ϑ)
Gaussian integral, see equation
(4.90), page 209
D(p∥q)
divergence or contrast function,
page 211
D(V1 ···Vn∥W1 ···Wn)(p)
partial
derivatives of the divergence D,
page 211
g(D)(V,W)
metric induced by the
divergence D, see equation (4.101),
page 211
∇(D)
X Y
afﬁne connection induced by the
divergence D, see equation (4.102),
page 212
∇(D∗)
X
Y
afﬁne connection induced by the
dual D∗of the divergence D, page 212
T (D)(X,Y,Z)
tensor induced by the
divergence D, see equation (4.104),
page 212
T0 = 	m
i=1 dx3
i
standard 3-tensor on Rm, see
equation (4.138), page 226
κ
statistic, page 244
κ∗μ
push-forward of μ by κ, page 244
κ∗φ′
pull-back of a measurable function φ′,
page 245
Kμ
congruent linear map, page 248
μ⊥
ω′
transverse measure on κ−1(ω′),
page 250
Kκ
Markov kernel induced by statistic κ,
page 254
K∗
Markov morphism induced by K,
page 254
Kr
map from Sr(Ω) to Sr(Ω′) induced by a
Markov kernel, page 268
dKr
formal derivative of Kr, page 269
ˆσ
estimator, page 277

Nomenclature
407
L2
P (Ω)
the space of functions on Ω that
belong to L2(Ω,p(ξ)) for all ξ ∈P ,
page 278
V ϕ
p(ξ)[ˆσ]
variance, page 279
DKL(p ∥ES)
divergence from the
hierarchical model ES, page 298
Hp(XA)
entropy of XA, see equation (6.3),
page 298
Hp(XB |XA)
conditional entropy of XB
given XA, see equation (6.4), page 299
Ip(XA;XB)
mutual information of XA and
XB, see equation (6.6), page 299
Ip(XA;XB |XC)
conditional mutual
information of XA and XB given XC,
page 299
Ip(?n
k=1 XAk |XA0), Ip(XA1;...;XAn |XA0)
conditional multi-information, see
equation (6.9), page 300
Ip(?n
k=1 XAk), Ip(XA1;...;XAn)
multi-information, see equation (6.11),
page 300
Hp(k)
average entropy of subsets of size k ,
see equation (6.32), page 309
Cp(k)
upper bound of DKL(p ∥p(k)) , see
equation (6.33), page 309
hp(k)
difference Hp(k) −Hp(k −1) of
average entropies , see equation (6.34),
page 309
SN,k+1
simplicial complex of intervals of
length ≤k , see equation (6.41),
page 313
Ep(X)
excess entropy , see equation (6.47),
page 316
I(X ∧Y)
mutual information rate, see
equation (6.48), page 317
T (Y k−1 →Xk), T (Xk−1 →Yk)
transfer
entropies, page 318
T (XV \vn−1 →Xv,n)
transfer entropy, see
equation (6.54), page 319
I(?
v∈V Xv)
mutual information rate , see
equation (6.55), page 319
Dμ(K ∥L)
KL-divergence between two
kernels K and L with respect to μ,
page 321
Dμ(K ∥KS)
KL-divergence of a kernel K
from an hierarchical model KS of
kernels, see equation (6.63), page 321
KStot, KSind, KSfac, KSsplit, KSpar, KSnet
special hierarchical kernel models,
page 322
μ
measure, page 361
B
σ-algebra, page 361
δx
Dirac measure, page 363
χB
characteristic function of set B, page 363
∥f ∥C0 := supx∈X |f (x)|
C0-norm, page 364
⟨f,g⟩μ :=

fg dμ
L2-product w.r.t.
measure μ, page 366
aibi := 	d
i=1 aibi
Einstein summation
convention, see equation (B.0),
page 367
(hij)i,j
inverse of metric tensor (hij)i,j ,
page 367
δi
k
Kronecker symbol, page 367
TpM
tangent space of M at p, page 368
Γ (T M)
vector space of vector ﬁelds on M,
page 369
[V,W] := V W −WV
Lie bracket, page 369
⟨V,W⟩= hijviwj
metric product of vectors
V,W, see equation (B.16), page 370
T ∗
p M
cotangent space of M at p, page 371
gradf
gradient of function f , see equation
(B.22), page 371
∇
connection, covariant derivative, page 372
Γ k
ij
Christoffel symbols , page 373
Θ(V,W) := ∇V W −∇W V −[V,W]
torsion
tensor, see equation (B.31), page 374
R(V,W)Z := ∇V ∇W Z −∇W ∇V Z −∇[V,W]Z
curvature tensor, see equation (B.35),
page 375
expx0
exponential map at x0, see equation
(B.38), page 376
Bϵ(x) = {y ∈E : ∥y −x∥< ε}
metric ball,
page 381
Lin(E1,E2)
space of bounded linear maps,
page 381
∥φ∥Lin(E1,E2) := supx∈E1,x̸=0
∥φx∥E2
∥x∥E1
operator norm, page 381
Lin(Ek
1,E2)
k-multilinear maps from E1 to
E2, page 381
E′ := Lin(E,R)
dual space, page 382
E∗:= Linalg(E,R)
linear dual space,
page 382
xn ⇀x
weak convergence of (xn)n∈N to x,
page 382
∂vφ|u0
Gâteaux-derivative of φ at u0 in
direction v, page 383
du0φ
(weak) differential of φ at u0, page 383
drφ
r-th (weak) derivative of φ, page 384

